Do We Need Language-Specific Fact-Checking Models?
The Case of Chinese
Caiqi Zhang1, Zhijiang Guo2, Andreas Vlachos2
1Language Technology Lab, University of Cambridge
2Department of Computer Science and Technology, University of Cambridge
{cz391, zg283, av308}@cam.ac.uk
Abstract
This paper investigates the potential benefits of
language-specific fact-checking models, focus-
ing on the case of Chinese using CHEF dataset.
To better reflect real-world fact-checking, we
first develop a novel Chinese document-level
evidence retriever, achieving state-of-the-art
performance. We then demonstrate the limi-
tations of translation-based methods and multi-
lingual language models, highlighting the need
for language-specific systems. To better ana-
lyze token-level biases in different systems, we
construct an adversarial dataset based on the
CHEF dataset, where each instance has a large
word overlap with the original one but holds
the opposite veracity label. Experimental re-
sults on the CHEF dataset and our adversarial
dataset show that our proposed method out-
performs translation-based methods and mul-
tilingual language models and is more robust
toward biases, emphasizing the importance of
language-specific fact-checking systems.1
1 Introduction
Since manual fact-checking requires significant
time and resources, there has been growing interest
in automated fact-checking in recent years (Graves,
2018; Nakov et al., 2021). While misinforma-
tion exists in various languages, studies have pre-
dominantly focused on claims and evidence in En-
glish (Guo et al., 2022; Akhtar et al., 2023). Cur-
rent research in fact-checking in other languages
often lacks grounding in real-world claims (Chang
et al., 2023) or is constrained to a single domain,
like COVID-19 (Shahi and Nandini, 2020).
In this paper, we raise the question: Should we
develop language-specific fact-checking models, or
can we effectively utilize existing English models
by translating claims and evidence into English?
We present a case study focused on Mandarin Chi-
nese to investigate it for two reasons. Firstly, Chi-
1https://github.com/caiqizh/FC_ChineseOriginal: 广东两名小学生提干，引发大量讨论。
Translated: Two primary school students in Guangdong raised
eyebrows (were promoted), sparking discussion.
ChatGPT: REFUTED CHEF Label: SUPPORTED
Claim 1: 中国超八成地下水遭受污染，不能饮用。
(Over 80% of China’s groundwater is polluted and is unfit
for drinking. )
Claim 2: 中国高铁辐射严重引发不孕。(Radiation from
China’s high-speed rail seriously causes infertility. )
ChatGPT: SUPPORTED CHEF Label: REFUTED
Table 1: Upper section: the challenge in accurate translation
(Red: Incorrect, Blue: Correct); Lower section: the bias of
multilingual LLMs towards certain claims.
nese is widely spoken by over a billion people and
possesses unique linguistic characteristics different
from English (Yang et al., 2017; Fei, 2023). Sec-
ondly, Chinese is the only language other than En-
glish that has a real-world evidence-based dataset
annotated manually, i.e.,CHEF (Hu et al., 2022)).
This is in contrast to other popular datasets such
as X-FACT (Gupta and Srikumar, 2021), which
depend on Google search results without evidence
annotation, and DanFEVER (Nørregaard and Der-
czynski, 2021), a Danish translation of FEVER,
which do not address real-life fact-checking needs.
To study the question proposed, we first develop
a novel Chinese document-level evidence retriever
(DLR), which outperforms state-of-the-art mod-
els by 10% in terms of accuracy and Macro F1 in
CHEF. Paired with either DLR-retrieved or anno-
tated gold evidence, we then demonstrate the limita-
tions of translation-based methods ( i.e.,first trans-
lating Chinese claims and evidence into English
and then applying English fact-checking models on
translated data) or multilingual language models
in fact-checking. We further identify the cultural
biases in CHEF and create an adversarial dataset.
Experiments on our newly proposed dataset show a
significant decrease in both accuracy and F1 score.
Overall, our study highlights the necessity of devis-
ing language-specific fact-checking models.arXiv:2401.15498v3  [cs.CL]  4 Oct 2024VerifiersRetrievers Semantic Ranker DLR Gold Evidence
Acc. Macro F1 Acc. Macro F1 Acc. Macro F1
TranslationGT+DeBERTa 59.23 59.76 60.15 61.29 66.84 66.57
GPT-4+DeBERTa 62.17 62.43 62.36 60.01 67.95 67.82
Multilingual LMmDeBERTa (He et al., 2023) 65.34 64.79 71.89 70.97 79.86 79.63
GPT-4 + mDeBERTa 55.24 54.38 56.78 58.69 60.31 61.75
GPT-3.5-Turbo (OpenAI, 2022) 53.29 51.46 55.45 51.32 58.79 54.97
GPT-4-Turbo (OpenAI, 2023) 65.78 62.35 69.17 69.01 73.67 73.96
Chinese SpecificBERT-base (Devlin et al., 2019) 63.00 62.88 67.66 67.66 77.79 77.62
Attention-based (Gupta and Srikumar, 2021) 64.01 63.65 69.00 68.35 78.56 78.46
Graph-based (Liu et al., 2020) 62.43 62.42 69.25 69.14 78.95 78.39
Chinese RoBERTa (Cui et al., 2020) 66.37 66.24 72.31 72.31 79.38 79.47
Chinese DeBERTa (Wang et al., 2022) 69.89 68.34 74.50 74.46 81.46 81.15
Table 2: Results on CHEF. All the DeBERTa and RoBERTa models are of -large version.
2Language-specific models vs translation
and multilingual models
To construct a Chinese fact-checking system, two
straightforward approaches are direct translation
from Chinese to English and the application of
multilingual LLMs. However, as demonstrated in
Table 1, translation from Chinese to English may re-
sult in inaccuracies, particularly with idiomatic ex-
pressions or language-specific phrases (Shao et al.,
2018). Furthermore, LLMs like ChatGPT, predom-
inantly trained on English data (Lai et al., 2023;
Hu et al., 2023), tend to reflect Western norms, val-
ues and biases (Naous et al., 2023; Masoud et al.,
2023; Wang et al., 2023), making them less effec-
tive for fact-checking in other languages. More-
over, LLMs also suffer from hallucination problem
(Zhang et al., 2023). Table 1 shows examples of
scientifically refuted claims that GPTs incorrectly
accept, alongside the evidence retrieved. To exam-
ine the abovementioned limitations in a systematic
way, we conduct experiments on a large scale Chi-
nese evidence-based dataset, CHEF.
Retrievers Evidence retrieval is crucial in the
fact-checking pipeline (Augenstein et al., 2019a;
Jiang et al., 2020). To align our work with real-
world fact-checking, we develop a state-of-the-
art Chinese evidence retrieval component. Our
novel Document-level Retriever (DLR) enhances
evidence retrieval by considering the context of
evidence sentences, unlike prior approaches that
isolate evidence selection to pairwise sentence clas-
sification (Hu et al., 2022). Inspired by Stammbach
(2021), we train a retriever to assign a score to each
Chinese token within an evidence document and
then aggregate these token scores at the sentence
level. In particular, we fine-tune a BigBird (Zaheeret al., 2020) to assign a value of 1 to tokens that
belong to annotated evidence for a claim, while
assigning a value of 0 to all other tokens. During
inference, we compute the average scores for all to-
kens within each sentence. If the resulting average
score exceeds 0.5, we classify the sentence as evi-
dence. We compare our proposed document-level
retriever with the annotated gold evidence, and Se-
mantic Ranker (Nie et al., 2019; Liu et al., 2020)
used in CHEF. To eliminate the effects of evidence
retrieval, we also utilize the gold evidence for the
ideal case. More training details of DLR can be
found in Appendix A.
Verifiers For the verifiers in Table 2, we mainly
include the baselines in Hu et al. (2022) with some
of our variations: (1) For the translation mod-
els, we first translate the evidence and claims via
Google Translator (GT) and GPT-4, then apply the
DeBERTa-large verifier. (2) The cross-lingual ap-
proach GPT-4 + mDeBERTa involves training mul-
tilingual DeBERTa-large on data translated from
Chinese to English, followed by evaluation of the
original CHEF. For the GPT models,2we use 5
shots for in-context learning. Other models are all
trained and tested on original CHEF. We provide
detailed experimental settings in the Appendix B.
Results on CHEF As shown in Table 2, our sys-
tem that combines DLR and Chinese DeBERTa ,
yields the best results with an accuracy of 74.50%
and a Macro F1 score of 74.46%. There is
an improvement of over 10% compared to the
best translation-based result ( GPT-4+DeBERTa ).
For the multilingual LMs, the cross-lingual ap-
proach ( GPT-4+mDeBERTa ) show poor perfor-
mance. This is probably because of the disparity be-
2Our experiments are conducted in Feb 2024.Word LMI( 10−6)p(l|w)
中国(China) 1189 0.56
电影(Movie) 1008 0.84
国际(International) 629 0.80
发布(Release/Announce) 599 0.74
金融(Finance) 593 0.66
亿元(Hundred Million Yuan) 500 0.66
外交(Diplomacy/Foreign Affairs) 496 0.85
外交部(Ministry of Foreign Affairs) 481 0.92
人民币(RMB/Chinese Yuan) 469 0.84
银行(Bank) 469 0.63Word LMI( 10−6)p(l|w)
病毒(Virus) 1105 0.66
疫苗(Vaccine) 1013 0.64
台湾(Taiwan) 962 0.77
可以(Can/Be able to) 901 0.72
出现(Appear) 478 0.74
肺炎(Pneumonia) 475 0.70
手机(Mobile phone) 451 0.77
冠状(Coronary) 414 0.93
日本(Japan) 402 0.72
感染(Infection) 395 0.66
Table 3: Top 10 LMI-ranked phrases in the train set of CHEF for SUPPORTED (left) and REFUTED (right).
tween the languages of the training and testing sets.
On the contrary, mDeBERTa , trained on Chinese
CHEF, achieves competitive performance on par
with Chinese RoBERTa and outperforming several
baseline models. Overall, the results suggest that
training a model specifically on Chinese, whether
it be a multilingual or monolingual model, is more
beneficial than relying on English-centric ones.
Evidence Retrieval The DLR, paired with dif-
ferent verifiers, improves accuracy and Macro F1
by about 5% over the Semantic Ranker. Regarding
the recall of human-annotated gold evidence, DLR
leads to 10% higher Recall@5 (Table 5). We also
find that our new retriever can retrieve evidence
pieces which, when considered individually can-
not verify the claim but, when combined they can.
Table 6 gives a detailed example in the Appendix C.
3 Biases in CHEF
To explore the reasons behind the deficiency of
translation services and multilingual LMs, we in-
vestigate the biases present in the CHEF dataset in
this section. Prior research has demonstrated that
fact-checking datasets, such as FEVER (Thorne
et al., 2018) and MultiFC (Augenstein et al.,
2019b), result in training models that rely on heuris-
tics such as surface-level patterns within claims, po-
tentially impeding their ability to generalize effec-
tively (Schuster et al., 2019; Thorne et al., 2019). In
this section, we show that while biases are present
as in the English language datasets and models,
they are specific to the Chinese culture .
Domain Bias First, in CHEF, claims are catego-
rized into domains such as politics, society, health,
and culture and we find a significant skew in the dis-
tribution: 64% of social and 66% of health claims
are REFUTED ,while 55% in politics and 72% in
culture are SUPPORTED . Notably, there is an imbal-
ance in the proportion of social and health claims,which collectively constitute 68% of the total. Fig-
ure 2 in the Appendix details the label distribution
across domains.
Cultural Bias We then examine the correlation
between phrases within the claims and the corre-
sponding labels. The word distribution within the
training set is analyzed for this purpose. Initially,
all claims in the training set are tokenized by Chi-
nese text segmentation tool, jieba.3The average
length of the words is 2.39 characters. Then, two
metrics are employed to assess the correlation be-
tween phrases and labels. Following Schuster et al.
(2019), first we use p(l|w)to calculate the proba-
bility of a label lgiven the presence of a specific
phrase win the claim. As this metric tends to ex-
hibit bias towards low-frequency words, the second
metric utilizes Local Mutual Information (LMI; Ev-
ert 2005) to identify high-frequency n-grams that
display a strong correlation with a particular label.
Thep(l|w)and LMI between phrase wand label l
is defined as follows:
p(l|w) =count( w, l)
count( w)(1)
LMI (w, l) =p(w, l)·logp(l|w)
p(l)
(2)
where we follow Schuster et al. (2019) to estimate
p(l)bycount( l)
|D|, p(w, l)bycount( w,l)
|D|and|D|is the
number of occurrences of all n-grams.
Table 3 lists the top 10 LMI-ranked phrases in
the train set of CHEF for SUPPORTED and RE-
FUTED . We find that while previous research
(Schuster et al., 2019) found that negation phras-
ings strongly correspond to REFUTED labels in
FEVER, we did not observe the same pattern in
CHEF.
Prior studies in English datasets, such as Con-
straint (Patwa et al., 2020), have also demonstrated
3https://github.com/fxsjy/jiebaOriginal 250 pairs Generated 750 pairs Full 1000 pairs
Accuracy F1 Score Accuracy F1 Score Accuracy F1 Score
GPT-4 + DeBERTa 77.34 75.26 43.68 44.57 52.10 53.04
mDeBERTa 82.45 80.68 53.27 51.09 60.56 59.68
GPT-3.5-Turbo 80.00 55.25 53.73 36.78 60.30 41.39
BERT-base 76.35 75.36 38.56 37.62 49.06 48.72
Attention-based 78.96 78.12 39.98 39.62 51.01 49.65
Graph-based 79.55 76.97 39.61 38.67 49.59 49.43
Chinese DeBERTa 86.69 84.98 57.84 54.31 65.01 63.74
GPT-4-Turbo 85.60 60.70 65.20 47.12 70.30 50.73
Table 4: Performance comparison of models on the adversarial dataset. The “original 250 pairs” refers to pairs
directly extracted from CHEF, while “generated 750 pairs” denotes pairs generated using GPT-4.
a strong correlation between politician names ( e.g.,
Barack Obama and Donald Trump) and refuted
claims, however, our research identifies a distinct
cultural bias within CHEF. In CHEF, claims about
biomedical and health issues frequently exhibit a
strong association with negative labels. Terms such
as病毒(virus),疫苗(vaccine), 致癌(carcino-
genic), and 冠状病毒(coronavirus) are more com-
monly encountered in refuted claims. Conversely,
financial terms like 金融(finance), 人民币(RMB),
and央行(People’s Bank of China), as well as polit-
ical terms such as 中国(China),外交部(Ministry
of Foreign Affairs), tend to carry positive labels.
One possible reason behind this is that fact-
checking in China tends to avoid criticism of hard-
core public issues, such as politics, economics, and
other current affairs (Liu and Zhou, 2022). On
the contrary, it focuses more on providing refer-
ences for everyday decision-making, such as in
health. Another political reason could be that
the Cyberspace Administration of China keeps
a close watch on online news services (Liu and
Zhou, 2022). Non-state enterprises are not per-
mitted to criticize politics, economics, and other
current affairs. Private companies are only autho-
rized to distribute and curate news produced by
state-owned media. Furthermore, in CHEF, cer-
tain regions such as 台湾(Taiwan), 日本(Japan),
and美国(United States) are commonly associated
with the REFUTED label. This may also reflect the
contentious nature of international relations within
the realm of Chinese fact-checking.
4 Adversarial Dataset Construction
Our analysis revealed the presence of labels and
cultural biases specific to the Chinese context (§ 3).
These biases can significantly impact the perfor-
mance and fairness of fact-checking models whenapplied to Chinese-language claims.
We therefore introduce an adversarial dataset de-
rived from the CHEF dataset for a better evaluation
of the models. Inspired by Schuster et al. (2019), to
create it we pair each claim-evidence instance with
a synthetic counterpart where claim and evidence
have high word overlap with the original ones but
the opposite veracity label (Figure 3). Under this
setting, determining veracity from the claim alone
would be equivalent to a random guess. Instead of
involving human annotators, we opt for the utiliza-
tion of GPT-4 to generate the dataset. To control the
quality, we invited two Chinese native speakers to
annotate randomly sampled 25% of claim-evidence
pairs with SUPPORTED ,REFUTED orNOT ENOUGH
INFO . The results demonstrated strong agreement
between humans and GPT-4. They agreed with
the dataset labels in 89% of cases, with a Cohen κ
of 0.80 (Cohen, 1960). Our approach overcomes
labor-intensive manual annotation and rigid rule-
based generation, advocating for automated sample
generation using LLMs. This new test set nulli-
fies the benefit of relying exclusively on cues from
claims. Details of the dataset construction and the
prompt we use can be found in Appendix D.
5 Experiments on Adversarial CHEF
Results on Adversarial CHEF Table 4 compares
model performance on adversarial versus original
data from CHEF.4Since our adversarial dataset
was constructed using GPT-4, including it as a veri-
fier will lead to a risk for data leakage, resulting in
biased comparisons. Therefore, we have excluded
GPT-4 from the evaluation results and have pro-
vided its performance metrics for reference only.
4Note that here the “original 250 pairs" use gold evidence,
and a binary classification approach, contributing to higher
performance metrics than in Table 2.All models perform worse on adversarial exam-
ples compared to the original CHEF. Specifically,
we highlight the following findings: (1) Chinese
DeBERTa drops from 86.69% accuracy on original
pairs to 57.84% and 65.01% on adversarial sub-
sets. Baselines similarly see over 37% decreases
in both accuracy and F1 scores. This underscores
the models’ reliance on surface features and re-
veals the aforementioned biases. (2) Compare the
translation-based GPT-4+DeBERTa (F1 53.04%),
the multilingual mDeBERTa (F1 60.56%), and the
Chinese DeBERTa (F1 63.74%), we observe a per-
formance increase with more language-specific
models, highlighting the benefits of incorporating
Chinese data during pre-training. (3) GPT-3.5-
Turbo , which has not been fine-tuned on the CHEF
dataset, demonstrates better robustness but still un-
derperforms compared to the language-specific Chi-
nese DeBERTa . Overall, we recommend future re-
search to use both the original and our adversarial
CHEF dataset for a comprehensive evaluation.
Exposing adversarial examples to the models
can improve robustness. DeBERTa’s performance
declines less than that of the baselines including
BERT, Attention, and Graph-based models when
faced with adversarial examples, about 30% com-
pared to over 37%, suggesting a higher sensitivity
to evidence changes. To investigate the reasons
behind the decrease in the model’s performance
and improve model’s robustness, we employ the
inoculation fine-tuning method (Liu et al., 2019a).
Results show the performance decline observed in
the baselines primarily stems from inherent weak-
nesses within the model family. In contrast, for the
DeBERTa model, gradually exposing it to more ad-
versarial samples leads to a gradual reduction in the
performance gap. Inoculation results by fine-tuning
the model with different sizes of adversarial exam-
ples are provided in Figure 4 in the Appendix F.
6 Conclusion
Our study reveals the shortcomings of English-
centric fact-checking systems when applied to Chi-
nese claims, highlighting the failure of translation-
based methods due to linguistic and cultural nu-
ances. We introduce a novel system that achieves
best-reported results on CHEF and provides an ad-
versarial dataset for continued research, underscor-
ing the need for specialized fact-checking models.Limitations
This study has several notable limitations. Firstly,
the performance of our document-level retriever,
despite showing improvements over the semantic
ranker, still exhibits a relatively low recall rate.
This underscores the ongoing challenges in evi-
dence retrieval, which necessitate further research
and refinement to enhance the accuracy and relia-
bility of the system.
Secondly, the scope of our analysis is limited to
English- and Chinese-language datasets. This is
due to the scarcity of real-world, evidence-based
fact-checking datasets annotated by humans in
other languages. The inclusion of the Chinese
dataset in this study is not arbitrary, rather, it’s
based on two main factors. First, there is a lack of
suitable datasets in other languages. Second, sev-
eral authors of this study are native Chinese speak-
ers, which gives us a distinct advantage in terms of
understanding and appreciating the linguistic fea-
tures and nuances of the Chinese language. This
expertise has allowed us to conduct a more thor-
ough and in-depth case study.
Moving forward, we are actively seeking
datasets in other languages that align with our re-
search requirements. One potential approach is to
construct new datasets based on the recently estab-
lished fact-checking website, Elections24Check5.
This platform gathers and categorizes fact-checked
information for European countries and citizens
ahead of the 2024 European Elections. It offers a
rich source of political fact-checks, disinformation
debunks, prebunking articles, and narrative reports
in various European languages, which could be
instrumental in expanding the breadth of our re-
search.
Ethics Statement
The CHEF dataset employed in our research is ac-
cessible to the scientific community, and its use
in our experiments presents no conflict of inter-
est. Although the adversarial dataset used in this
study was developed with a GPT-4 model, to en-
sure its integrity and safety, we conducted an exten-
sive manual review to eliminate sensitive or poten-
tially harmful information. This review received
approval from our institution’s ethics committee.
Furthermore, the hourly salary for annotators sur-
passed the national minimum wage, and all annota-
tors consented to the use of the data.
5https://elections24.efcsn.com/Acknowledgements
Andreas Vlachos is supported by the ERC grant
A VeriTeC (GA 865958). Caiqi Zhang is funded by
Amazon Studentship. We thank Yulong Chen for
his great support on this work.
References
Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo,
Oana Cocarascu, Elena Simperl, and Andreas Vla-
chos. 2023. Multimodal automated fact-checking: A
survey. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2023 , pages 5430–5448,
Singapore. Association for Computational Linguis-
tics.
Isabelle Augenstein, Christina Lioma, Dongsheng
Wang, Lucas Chaves Lima, Casper Hansen, Chris-
tian Hansen, and Jakob Grue Simonsen. 2019a. Mul-
tiFC: A real-world multi-domain dataset for evidence-
based fact checking of claims. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 4685–4697, Hong Kong,
China. Association for Computational Linguistics.
Isabelle Augenstein, Christina Lioma, Dongsheng
Wang, Lucas Chaves Lima, Casper Hansen, Chris-
tian Hansen, and Jakob Grue Simonsen. 2019b. Mul-
tiFC: A real-world multi-domain dataset for evidence-
based fact checking of claims. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 4685–4697, Hong Kong,
China. Association for Computational Linguistics.
Yi-Chen Chang, Canasai Kruengkrai, and Junichi Yam-
agishi. 2023. XFEVER: Exploring fact verification
across languages. In Proceedings of the 35th Confer-
ence on Computational Linguistics and Speech Pro-
cessing (ROCLING 2023) , pages 1–11, Taipei City,
Taiwan. The Association for Computational Linguis-
tics and Chinese Language Processing (ACLCLP).
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement , 20(1):37–46.
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin
Wang, and Guoping Hu. 2020. Revisiting pre-trained
models for Chinese natural language processing. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 657–668, Online. As-
sociation for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association forComputational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations.
Yunxi Fei. 2023. The differences in thinking patterns
between english and chinese in chinese students’ en-
glish writing. In 2nd International Conference on
Education, Language and Art (ICELA 2022) , pages
277–285. Atlantis Press.
Isa Fulford and Andrew Ng. 2023. Chatgpt prompt
engineering for developers. DeepLearningAI Blog .
Lucas Graves. 2018. Understanding the promise and
limits of automated fact-checking. Reuters Institute
for the Study of Journalism .
Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos. 2022. A survey on automated fact-checking.
Transactions of the Association for Computational
Linguistics , 10:178–206.
Ashim Gupta and Vivek Srikumar. 2021. X-fact: A new
benchmark dataset for multilingual fact checking. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 2: Short Papers) , pages 675–682,
Online. Association for Computational Linguistics.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.
Debertav3: Improving deberta using electra-style
pre-training with gradient-disentangled embedding
sharing. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. Deberta: decoding-enhanced
bert with disentangled attention. In 9th International
Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net.
Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo,
Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2023. Do
large language models know about facts? ArXiv
preprint , abs/2310.05177.
Xuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu,
Lijie Wen, and Philip Yu. 2022. CHEF: A pilot Chi-
nese dataset for evidence-based fact-checking. In
Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 3362–3376, Seattle, United States. Association
for Computational Linguistics.
Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles
Dognin, Maneesh Singh, and Mohit Bansal. 2020.
HoVer: A dataset for many-hop fact extraction and
claim verification. In Findings of the Associationfor Computational Linguistics: EMNLP 2020 , pages
3441–3460, Online. Association for Computational
Linguistics.
Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu
Man, Franck Dernoncourt, Trung Bui, and Thien
Nguyen. 2023. ChatGPT beyond English: Towards
a comprehensive evaluation of large language mod-
els in multilingual learning. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2023 , pages 13171–13189, Singapore. Association
for Computational Linguistics.
Nelson F. Liu, Roy Schwartz, and Noah A. Smith. 2019a.
Inoculation by fine-tuning: A method for analyz-
ing challenge datasets. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 2171–2179, Minneapolis, Minnesota.
Association for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv preprint , abs/1907.11692.
Yusi Liu and Ruiming Zhou. 2022. “let’s check it se-
riously”: Localizing fact-checking practice in china.
International Journal of Communication , 16(0).
Zhenghao Liu, Chenyan Xiong, Maosong Sun, and
Zhiyuan Liu. 2020. Fine-grained fact verification
with kernel graph attention network. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 7342–7351, On-
line. Association for Computational Linguistics.
Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip
Treleaven, and Miguel Rodrigues. 2023. Cultural
alignment in large language models: An explanatory
analysis based on hofstede’s cultural dimensions.
Preslav Nakov, David P. A. Corney, Maram Hasanain,
Firoj Alam, Tamer Elsayed, Alberto Barrón-Cedeño,
Paolo Papotti, Shaden Shaar, and Giovanni Da San
Martino. 2021. Automated fact-checking for assist-
ing human fact-checkers. In Proceedings of the
Thirtieth International Joint Conference on Artificial
Intelligence, IJCAI 2021, Virtual Event / Montreal,
Canada, 19-27 August 2021 , pages 4551–4558. ij-
cai.org.
Tarek Naous, Michael J. Ryan, Alan Ritter, and Wei Xu.
2023. Having beer after prayer? measuring cultural
bias in large language models.
Yixin Nie, Haonan Chen, and Mohit Bansal. 2019.
Combining fact extraction and verification with neu-
ral semantic matching networks. In The Thirty-Third
AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of
Artificial Intelligence Conference, IAAI 2019, The
Ninth AAAI Symposium on Educational Advances in
Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,USA, January 27 - February 1, 2019 , pages 6859–
6866. AAAI Press.
Jeppe Nørregaard and Leon Derczynski. 2021. Dan-
FEVER: claim verification dataset for Danish. In
Proceedings of the 23rd Nordic Conference on Com-
putational Linguistics (NoDaLiDa) , pages 422–428,
Reykjavik, Iceland (Online). Linköping University
Electronic Press, Sweden.
OpenAI. 2022. Chatgpt blog post.
OpenAI. 2023. Gpt-4 technical report. ArXiv preprint ,
abs/2303.08774.
Parth Patwa, Shivam Sharma, Srinivas PYKL, Vineeth
Guptha, Gitanjali Kumari, Md Shad Akhtar, Asif
Ekbal, Amitava Das, and Tanmoy Chakraborty. 2020.
Fighting an infodemic: Covid-19 fake news dataset.
Michael Schlichtkrull, Zhijiang Guo, and Andreas Vla-
chos. 2023. Averitec: A dataset for real-world claim
verification with evidence from the web. In Advances
in Neural Information Processing Systems 36: An-
nual Conference on Neural Information Processing
Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel
Roberto Filizzola Ortiz, Enrico Santus, and Regina
Barzilay. 2019. Towards debiasing fact verification
models. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3419–3425, Hong Kong, China. Association for Com-
putational Linguistics.
Gautam Kishore Shahi and Durgesh Nandini. 2020.
FakeCovid – a multilingual cross-domain fact check
news dataset for covid-19. In Workshop Proceedings
of the 14th International AAAI Conference on Web
and Social Media .
Yutong Shao, Rico Sennrich, Bonnie Webber, and Fed-
erico Fancellu. 2018. Evaluating machine transla-
tion performance on Chinese idioms with a blacklist
method. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018) , Miyazaki, Japan. European Language
Resources Association (ELRA).
Dominik Stammbach. 2021. Evidence selection as a
token-level prediction task. In Proceedings of the
Fourth Workshop on Fact Extraction and VERifica-
tion (FEVER) , pages 14–20, Dominican Republic.
Association for Computational Linguistics.
James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal. 2018.
The fact extraction and VERification (FEVER)
shared task. In Proceedings of the First Workshop on
Fact Extraction and VERification (FEVER) , pages 1–
9, Brussels, Belgium. Association for Computational
Linguistics.James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal. 2019.
The FEVER2.0 shared task. In Proceedings of the
Second Workshop on Fact Extraction and VERifica-
tion (FEVER) , pages 1–6, Hong Kong, China. Asso-
ciation for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998–6008.
Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang,
Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He,
Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li,
Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,
Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiao-
jun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan,
and Jiaxing Zhang. 2022. Fengshenbang 1.0: Be-
ing the foundation of chinese cognitive intelligence.
ArXiv preprint , abs/2209.02970.
Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi
Dai, Jen tse Huang, Zhaopeng Tu, and Michael R.
Lyu. 2023. Not all countries celebrate thanksgiving:
On the cultural dominance in large language models.
Man Yang, North Cooc, and Li Sheng. 2017. An inves-
tigation of cross-linguistic transfer between chinese
and english: a meta-analysis. Asian-Pacific Journal
of Second and Foreign Language Education , 2:1–21.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tañón, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
formers for longer sequences. In Advances in Neural
Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
Bi, Freda Shi, and Shuming Shi. 2023. Siren’s song
in the ai ocean: A survey on hallucination in large
language models.A Document-level Retriever (DLR)
In our approach, we recognize the significance of
context in determining whether a given sentence
can be considered as evidence. To leverage this
contextual information, we assign a score to each
token within a document and then aggregate these
token scores at the sentence level. Subsequently,
we fine-tune a transformer model to assign a value
of 1 to tokens that belong to annotated evidence for
a claim, while assigning a value of 0 to all other
tokens. During testing, we compute the average
scores for all tokens within a sentence. If the re-
sulting average score exceeds 0.5, we classify the
sentence as evidence. Figure 1 illustrate this proce-
dure.
Vanilla Transformer-based architectures, as in-
troduced by Vaswani et al. (2017), utilize a com-
plete attention matrix that captures all pairwise
interactions among tokens within a given sentence.
However, the quadratic time and memory complex-
ity associated with the complete attention matrix
imposes limitations on the processing capabilities
of these systems, often confining them to mod-
erately long inputs with a typical cutoff point of
512 subwords. Notably, in fact-checking area, a
retrieved potential evidence passage can easily sur-
pass this length. For example in CHEF, the average
length of an evidence document is 866, which ex-
ceeds the maximum length supported by BERT.
To overcome this limitation, we turn to BigBird
(Zaheer et al., 2020), which incorporates sparse
attention patterns. This adaptation enables BigBird
to handle sequence lengths of up to 4096 tokens.
Zaheer et al. (2020) also demonstrate that BigBird’s
attention pattern exhibits comparable performance
to BERT for short sequence lengths, while out-
performing BERT in tasks that involve longer se-
quences. Figure 1 illustrates our framework, high-
lighting the utilization of BigBird in our approach.
The models are trained on A100-SXM-80GB
GPUs. We fine-tuned the Chinese BigBird6. It
employs a custom tokenizer, merging jieba tok-
enizer with BertTokenizer, for processing Chinese
text. Optimization leverages the AdamW optimizer
(learning rate of 2e-5, epsilon of 1e-8), with no
weight decay for specific parameters and a linear
learning rate scheduler initiating at a warm-up step
count of 0. Training and evaluation phases both
utilize a batch size of 16 across 5 epochs.
6https://huggingface.co/Lowin/chinese-bigbird-base-
4096B Experiment Setup
In the results presented in Table 2, the transla-
tion models initially employ Google/GPT-4 to con-
vert all claims and evidence within the CHEF
dataset to English. Subsequently, an English
RoBERTa-large is fine-tuned to assess the ve-
racity of these claims using the CHEF training
set. For multilingual LLMs, we apply a five-shot
in-context learning approach with both GPT-3.5-
Turbo and GPT-4-Turbo. Regarding the baseline
models—BERT-base, attention-based, and graph-
based models—we adhere to the default hyperpa-
rameters as delineated in the CHEF study (Hu et al.,
2022). We run our experiments on A100-SXM-
80GB GPUs. For each pipeline system, we conduct
three independent experiments and report the mean
values.
Verifiers We utilize DeBERTa (He et al., 2021)
to verify a claim given the selected evidence, us-
ing the Chinese version pretrained on the WuDao
Corpora (Wang et al., 2022). We also compare our
results with the baselines in Hu et al. (2022), in-
cluding BERT-base (Devlin et al., 2019), Attention-
based (Gupta and Srikumar, 2021), and Graph-
based (Liu et al., 2020) methods. We also incorpo-
rate the RoBERTa-based model (Liu et al., 2019b),
GPT-3.5-Turbo (OpenAI, 2022) and GPT-4-Turbo
(OpenAI, 2023) for a more comprehensive com-
parison. For the GPT models, we use 5 shots for
in-context learning..
C Comparison of Different Retrievers
Table 5 compares the performance of Seman-
tic Ranker and Document-level Retriever. The
Document-level Retriever leads to better Recall@5
andMarco F1 .Recall@5 measures the propor-
tion of gold evidence that are successfully retrieved
among the top 5 retrieved evidence sentences.
Although outperforming the Semantic Ranker,
the Document-level Retriever only attains a 33.58%
Recall @5, indicating the difficulty of evidence re-
trieval, yet remarkably leads to a 74.46% Macro
F1 score in claim verification. This may be due
to the CHEF’s gold evidence annotation not being
exhaustive, a known issue in datasets with evidence
retrieved from the Web (Schlichtkrull et al., 2023),
and thus the retriever can return correct evidence
that was not annotated. Additionally, the model
might leverage surface-level patterns in claims to
inform verification, which allows for high accuracy
even when the available evidence is insufficient.喝牛奶会使胆固醇水平增高。①许多人不食用牛奶是因为他们觉得喝牛奶会使胆固醇增高。②研究发现，牛奶中所含的胆固醇并不高，而牛奶中所含的某些成分还具有抑制胆固醇的作用。③医学研究还证实，喝牛奶还有助于减少冠心病和治疗高血压。④但是，不能空腹喝牛奶，不利于营养素的吸收和利用。……Drinking milk can raise cholesterollevel.①Many people do not consume milk because they think that drinking it will raise their cholesterol.②Studies have found that milk does not contain high levels of cholesterol and that certain ingredients in milk have cholesterol-suppressing properties. ③Medical research has also confirmed that drinking milk can also help reduce coronary heart disease and treat high blood pressure. ④However, milk should not be consumed on an empty stomach as it is not conducive to the absorption and utilization of nutrients. ……BigBird EncoderLabels: 0 0 0 0 0 … ①0 0 0 0 0 …②1 1 1 1 1 … ③0 0 0 0 0 …④0 0 0 0 0…Figure 1: Framework illustration highlighting the usage of BigBird in our approach for evidence sentence retrieval.
The claim is represented in blue, while the evidence sentence is highlighted in red.
Sentence Retrieval Recall@5 Macro F1
Semantic Ranker 21.24±2.13 70.58±1.56
Document-level Retriever 33.58±2.0874.46±1.78
Table 5: Comparison of Semantic Ranker and
Document-level Retriever for evidence sentence re-
trieval with DeBERTa-large.
Table 6 is an example where leveraging
document-level information can help with the ev-
idence retrieval. To verify the claim: “ 运用红酒
含有花青素的原理，可以简单检测红酒的真
假。(The principle that red wine contains antho-
cyanins allows for a straightforward authenticity
test.)", each retriever collects five pieces of evi-
dence. Without additional context, it is not possible
to retrieve the sentences highlighted in red through
semantic matching alone. None of these sentences,
when considered individually, can be used to ver-
ify the claim. However, when taken together, they
provide a comprehensive explanation of why an-
thocyanins can be utilized to test red wine. Having
access to the entire document makes it much easier
to accurately predict similar examples.
D Adversarial Dataset Construction
D.1 Task Definition
To further detect and eliminate bias in CHEF, we
propose to generate a new Chinese adversarial
dataset for it. We adopt the methodology presentedby (Schuster et al., 2019) as our primary framework
for constructing a symmetrical dataset for CHEF, as
illustrated in Figure 3. Our approach involves gen-
erating synthetic claim-evidence pairs that main-
tain the same relationship (e.g., SUPPORTS or
REFUTES) while conveying contrasting factual in-
formation. Moreover, we ensure that each sentence
in the new pair exhibits the inverse relationship
with its corresponding sentence in the original pair.
Some new rules have been devised to bet-
ter suit the Chinese context. More specifically,
when rewriting the given claim “ 陈大文在北
京称，2020年版第五套人民币5元纸币将发
行，防伪性能提升。" (Chen Dawen, announced
that the 2020 edition of the fifth series of 5-yuan
banknotes will be issued, with improved anti-
counterfeiting features, in Beijing.), in our frame-
work, the following rewriting strategies are al-
lowed:
•Important nouns that appear in both the claim
and the evidence can be modified. These in-
clude key information such as time, place, per-
son, and number. Changing these essential
terms can alter the original meaning of the
sentence. For example, substituting the name
“Chen Dawen" with “Li Xiaoming," revising
the year “2020" to “2023," replacing the lo-
cation “Beijing" with “Shanghai," and trans-
forming the denomination “5 yuan" to “10
yuan."Semantic Ranker Document-level Retriever
有一个妙招，一秒钟鉴定红酒真假
(There’s a clever trick, one-second wine authentic-
ity test )而假红葡萄酒中多由酒精、糖精和香精色素勾兑而
成，里面不含花青素
(Fake red wine is often made by blending alcohol, glycerin,
and artificial colorants, without containing anthocyanins )
这时，如果红酒变成深蓝色，就是真红酒；
如果没有反应，则是假红酒
(At this point, if the red wine turns deep blue, it’s
genuine; if there’s no reaction, it’s fake )由于真正的红葡萄酒中含有丰富的花青素
(Because authentic red wine contains abundant antho-
cyanins )
如何辨别真假红酒，教你简单一招
(How to distinguish real from fake red wine, teach-
ing you a simple trick )花青素在酸性条件下呈现紫红色，而在碱性条件下
呈现蓝绿色
(Anthocyanins appear purplish-red under acidic condi-
tions and bluish-green under alkaline conditions )
若是色素勾兑的红酒，颜色则无变化
(If it’s red wine adulterated with colorants, the
color remains unchanged )其实，还有一个更简单的方法没说
(In fact, there’s an even simpler method not mentioned )
把用水兑开的食用碱水滴在红酒上面；
(Drip food-grade alkali water diluted with water
onto the red wine; )如果我们家里的红酒用食用碱检测没有变色，那么
基本可以肯定你买到了假酒
(If our home red wine doesn’t change color when tested
with food-grade alkali, then it’s safe to say you’ve bought
fake wine )
Table 6: Evidence sentences retrieved by Semantic Ranker and Document-leverl Retriever for the claim: “ 运用红酒
含有花青素的原理，可以简单检测红酒的真假(The principle that red wine contains anthocyanins allows for a
straightforward authenticity test. )."
•Verbs or phrases indicating degrees in both
the claim and the evidence can be replaced
with their opposites. For instance, substitut-
ing “rise" with “fall," changing “increase" to
“decrease," converting “helpful" to “unhelp-
ful," replacing “substantiated" with “unsub-
stantiated," and transforming “no evidence"
to “evidence not found."
Note that these methods do not constitute an ex-
haustive set of legal rewrite methods. They serve
as heuristics for the model, which may also employ
similar modifications automatically. Similarly, the
evidence undergoes a comparable rewriting pro-
cess. For additional examples of these methods,
please refer to Table 7. To rewrite the sentences, we
employ the state-of-the-art GPT-4 (OpenAI, 2023)
model ,which has demonstrated human-level per-
formance in various NLP tasks. By leveraging the
GPT-4 model, we eliminate the laborious task of
human annotation and enhance the diversity of gen-
eration through handcrafted rules.
E Prompt Engineering
Given the importance of prompt engineering for the
quality of the generated data, as well as the scarcity
of relevant literature, it is imperative to carefully
craft our prompt. To address this challenge, we
sought guidance from the empirical findings of theopen source community7, which provided valuable
insights into prompt design practices. Furthermore,
we consult the recently published prompt design
guideline by (Fulford and Ng, 2023) to ensure our
approach aligns with the newest recommendations.
We conducted extensive experiments to iteratively
refine our prompt, culminating in the development
of an innovative prompt that not only enhances
the quality of generated results but also exhibits
versatility, enabling its seamless adaptation to a
wide range of tasks.
According to Fulford and Ng (2023), the effec-
tiveness of a prompt relies on two key principles.
Principle 1 emphasizes the significance of provid-
ing clear and specific instructions to the model. To
achieve this, the prompt should employ delimiters
(such as backticks) to clearly demarcate distinct
parts of the input. Furthermore, the provision of
examples helps the model formulate a “few-shot"
prompt, allowing it to generate responses based
on limited examples. Principle 2 focuses on opti-
mizing the model’s processing by breaking down
the full task into several subtasks. This approach
guides the model to think step by step, enhancing
its performance. The structure of our prompt is
outlined in Table 8.
7https://github.com/f/awesome-chatgpt-promptsFigure 2: The distribution of labels across different domains in CHEF.
Figure 3: A illustration of the relationship between the
original pair and the generated pair (Schuster et al.,
2019).
E.1 Quality Control
Following the data generation process, we gener-
ated 250 new claim and evidence pairs. By per-
muting them under the symmetric setting Schuster
et al. (2019), we obtained an adversarial dataset
consisting of 1000 pairs. We then enlisted the par-
ticipation of two Chinese native speakers to per-
form annotations on a randomly selected subset
of 300 claim-evidence pairs removing their labels,
which accounted for 30% of the total pairs within
the symmetric adversarial dataset. These annota-
tions involved assigning one of two labels, namely
SUPPORTS, and REFUTES, while also flagging
instances of nongrammatical cases. The average
agreement between the annotators and the pre-
existing dataset labels reached 89% of the cases,
resulting in a Cohen κcoefficient of 0.80 (Cohen,
1960). It demonstrates that the new claim-evidence
pairs generated by GPT-4 mostly remain in their
original relation, proving the effectiveness of our
method. Additionally, approximately 4% of the
cases exhibited minor grammatical errors or typos.E.2 Error Analysis
After manually examining the wrongly predicted
cases for the DeBERTa-large model following the
inoculation process, we have identified three pri-
mary challenges that current models struggle to
address:
•Subtle modifications can induce a dramatic
change in sentence meaning. In the adver-
sarial CHEF dataset, a large number of state-
ments exhibit slight differences before and
after modifications, often differing by only
one or two Chinese characters. Given the rich
semantic nature of Chinese characters, even
a single-word alteration can reverse the en-
tire sentence’s meaning. For instance, in the
first example of Table 9 and the first example
of Table 7, minor changes involving a single
character completely alter the original mean-
ing. These nuanced distinctions pose difficul-
ties for models to accurately capture. Further-
more, even if these changes are encoded in
the model’s parameters, they may not receive
significant weighting during veracity assess-
ment.
•Adversarial CHEF includes numerical reason-
ing challenges that lack a dedicated mecha-
nism. While the original CHEF dataset con-
tains extensive instances of numbers, there
are relatively few statements that necessitate
inference from numerical information. In
contrast, the adversarial CHEF dataset intro-
duces numerous modifications associated with
numbers, requiring the model to determineSource Claim Evidence Label
ORIGINAL 2021年12月31日人民币对美元汇率中间
价上调27个基点。On December 31, 2021, the
central parity rate of the Chinese yuan against the
US dollar was increased by 27 basis points.新华社上海12月31日电来自中国外汇交易中心的数
据显示，31日人民币对美元汇率中间价报6.5782，较
前一交易日上调27个基点。Shanghai, December 31st
(Xinhua) - Data from the China Foreign Exchange Trading
System showed that the central parity rate of the Chinese
yuan against the US dollar was set at 6.5782 on the 31st,
representing an increase of 27 basis points compared to
the previous trading day.SUPPORT
GENERATED 2021年12月31日人民币对美元汇率中间
价下调27个基点。On December 31, 2021, the
central parity rate of the Chinese yuan against the
US dollar was decreased by 27 basis points.新华社上海12月31日电来自中国外汇交易中心的数
据显示，31日人民币对美元汇率中间价报6.5782，较
前一交易日下调27个基点。Shanghai, December 31st
(Xinhua) - Data from the China Foreign Exchange Trading
System showed that the central parity rate of the Chinese
yuan against the US dollar was set at 6.5782 on the 31st,
representing a decrease of 27 basis points compared to
the previous trading day.SUPPORT
ORIGINAL 奥密克戎对抗体中和作用不存在逃逸现
象。There is no evidence of escape phenomenon
in the neutralizing action of omicron antibodies.结果发现，奥密克戎变异株能被实验中所有单克隆
抗体有效中和，没有出现逃逸现象。The findings re-
vealed that the omicron variant can be effectively neutral-
ized by all monoclonal antibodies tested in the experiment,
with no observed escape phenomenon.SUPPORT
GENERATED 奥密克戎对抗体中和作用存在大量逃逸现
象。There is a significant amount of escape phe-
nomenon in the neutralizing action of omicron an-
tibodies.结果发现，奥密克戎变异株能完全抵抗或部分抵
抗实验中所有单克隆抗体的中和作用。The results in-
dicate that the omicron variant can completely or partially
resist the neutralizing action of all monoclonal antibodies
tested in the experiment.SUPPORT
ORIGINAL 2020年4月，某男子在公园挖土被警方罚
款200元。In April 2020, a man was fined 200
yuan by the police for digging soil in the park.经讯问，邓某承认该微博所述情节均为伪造，其本
人并未到过绿博园，更没有被公安机关处罚。After
questioning, Mr Deng admitted that the Weibo post was
fabricated, and he had never been to Green Park nor been
penalized by the police.REFUTE
GENERATED 2020年4月，某男子在公园挖土被警方制止，
但并未罚款。In April 2020, a man was stopped
by the police for digging soil in the park but was
not fined.据警方透露，某男子于2020年4月在公园非法挖土，
发现后被警方罚款200元。According to the police, the
man was found engaging in unauthorized soil excavation
in the park in April 2020 and was subsequently fined 200
yuan.REFUTE
Table 7: Examples from the symmetric adversarial dataset are provided to illustrate claim-evidence pairs where the
relationship described in the right column is maintained. By combining the generated sentences with the original
ones, two additional cases are formed, each with labels that are opposite to one another. The red texts in Chinese
highlight the differences between the claim/evidence before and after the rewrite.
whether the statements align with the evi-
dence’s numerical values. For example, con-
sider the second example in Table 9. How-
ever, our current approaches lack a dedicated
mechanism to address these numerical issues,
resulting in numbers being treated similarly
to text.
•Inferences from implicit or circumstantial ev-
idence present challenges in assessing the
claims. In most cases, the evidence is straight-
forward, enabling easy judgment of the state-
ment’s correctness. However, there are in-
stances where the evidence used for inference
does not explicitly provide the truth of the
statement or directly contradict its content.
For instance, the third example in Table 9 does
not directly specify what is incorrect with the
statement (e.g., mentioning that it should be50,000 instead of 70,000). Instead, the ev-
idence uses terms like “non-representative"
and “sensationalized" to indirectly point out
the unreasonableness of the data results. It
is important to note the distinction between
this type of challenge and cases involving “not
enough information," where the former can
be deduced through careful inference. Effec-
tively addressing this type of problem requires
models with stronger reasoning capabilities.
F Inoculation by fine-tuning
Upon evaluating the model with synthetic datasets,
it’s clear the model underperforms compared to the
original benchmarks. The precise weaknesses that
these datasets reveal are not immediately revealed.
To understand this better, we adopt the method of
inoculation by fine-tuning, introduced by Liu et al.
(2019a). This method allows models to be exposedExplanation of Prompt Design Prompt Snippet
Introduce the background of the task and the input format of the data.
Define a role for the model.我希望你作为一个编辑部的事实核查记者，完成以下的数据标注
任务，同时改写声明和证据，使得其各自的含义与原意相反...（I
would like you, as an fact-checking journalist, to complete the following
annotation task: rewriting claims and evidence so that their respective
meanings are the opposite of what they originally meant... ）
Give the requirement of how to rewrite the claim. 第一步：修改声明内容，使得其变成于之前含义相反的内
容...(Step 1: Modify the claim to make it have the opposite meaning as
before...)
Give the requirement of how to rewrite the evidence accordingly. 第二步：对应修改后的声明，修改证据的内容...(Step 2: Modify the
evidence accordingly, corresponding to the modified claim...)
Give a detailed example and possible rewrite strategies. 针对例句：[例子]，以下我提供几个理想且合法的修改示例:
...(For the exemplary sentence: [EXAMPLE], I offer the following
examples of ideal and legal modifications: ...)
Give a small bunch of human-annotated samples. 请同时参考以下一些 其他例句：示例一；示例二；示例
三；...(Please also refer to the following additional example sentences:
Example 1; Example 2; Example 3; ...)
Emphasize the key requirement. 你可以使用上述例子中的修改方式，也可以使用其他修改方
法。但是最重要的是要求修改后的证据仍然能支持修改后的声
明。(You can use the modification strategies mentioned above as well
as other ways to make the changes. However, the most important aspect
is to ensure that the modified evidence still supports the modified claim.)
Give the claim and evidence pair that is needed to rewrite delimited by
triple backticks.``` TEXT ```
Table 8: This table outlines the purpose of each snippet in the prompt, explaining the role of each section according
to the prompt design principles.
to a small portion of challenging dataset data to see
how the performance changes.
Post-inoculation, we anticipate three possible
outcomes:
Outcome 1: A narrowing of the performance
discrepancy between the original and challenge test
sets suggests that the challenge data didn’t expose
model weaknesses but rather a lack of diversity in
the original dataset.
Outcome 2: No change in performance on ei-
ther test set indicates that the challenge dataset has
pinpointed a fundamental model flaw, as the model
fails to adjust even when familiarized with the chal-
lenge data.
Outcome 3: A performance drop on the original
test set suggests the fine-tuning skewed the model
to suit the challenge data, highlighting a deviation
from the original data characteristics. This could be
due to differences in label distribution or annotation
artifacts that are dataset-specific.
Figure 4 shows results from fine-tuning with
various amounts of adversarial data. We observe
the "performance gap" as the difference in model
performance on the original versus adversarial test
sets pre-inoculation.For BERT-base, attention-based, and graph-
based models, we observe minor performance
changes—Outcome 2—signifying that fine-tuning
does not close the performance gap significantly,
pointing to a core weakness in adapting to adver-
sarial data distributions.
In contrast, the DeBERTa-large model shows a
reduced performance gap post-inoculation, cutting
it down by 53% after fine-tuning with 800 adversar-
ial examples. Its strong performance on the original
dataset persists, suggesting DeBERTa’s architec-
ture, with its nuanced attention to content, relative,
and absolute positions in sentences, equips it to
handle slight alterations in claim or evidence more
adeptly.Source Claim Evidence Label
ORIGINAL 2019年1月，成都万象城车祸致一人死亡。In
January 2019, a car accident at Chengdu The
MixC Mall resulted in one fatality.经交警分局反馈：核实现场无人死亡，只有一个伤
者。According to feedback from the Traffic Police, upon
verification, there were no fatalities at the scene, with only
one injured individual.REFUTE
GENERATED 2019年1月，成都万象城车祸无人死亡。In
January 2019, the car accident at Chengdu The
MixC Mall resulted in no fatalities.经交警分局反馈：核实现场一人死亡，还有一个伤
者。According to feedback from the Traffic Police, upon
verification, there was one fatality at the scene, as well as
one injured individual.REFUTE
ORIGINAL 2018年春节档总票房累计20.36亿，“就地过
年”让影院更火爆。During the 2018 Spring Fes-
tival season, the total box office revenue reached
2.036 billion RMB, making the cinemas even more
popular with the "celebrate the Lunar New Year
locally" trend.票房方面，2018年春节档，中国电影票房20.36亿，
打破2017年春节档创下的15.06亿票房纪录，创春节
档票房新纪录。In terms of box office performance, the
2018 Spring Festival season achieved a record-breaking
box office revenue of 2.036 billion RMB, surpassing the
previous record of 1.506 billion RMB set in the 2017
Spring Festival season and establishing a new record for
the Spring Festival box office.SUPPORT
GENERATED 2021年春节档总票房累计78.45亿，“就地过
年”让影院更火爆。During the 2021 Spring Fes-
tival season, the total box office revenue reached
7.845 billion RMB, making the cinemas even more
popular with the "celebrate the Lunar New Year
locally" trend.票房方面，2021年春节档，中国电影票房78.45亿，
打破2019年春节档创下的59.06亿票房纪录，创春节
档票房新纪录。In terms of box office performance, the
2021 Spring Festival season achieved a record-breaking
box office revenue of 7.845 billion RMB, surpassing the
previous record of 5.906 billion RMB set in the 2019
Spring Festival season and establishing a new record for
the Spring Festival box office.SUPPORT
ORIGINAL 2021年全国有七万硕士在送外卖。In 2021,
there were 70,000 individuals with master’s de-
grees working as food delivery drivers nationwide.就这样，两个并不具有代表性的“1%”，被自媒体
简单渲染成“全国七万硕士在送外卖”。Just like that,
two non-representative “1%" were sensationalized by the
media as “70,000 master’s degree holders nationwide
working as food delivery drivers."REFUTE
GENERATED 2021年全国有七万硕士在送外卖为谣言。The
claim that there were 70,000 master’s degree hold-
ers working as food delivery drivers nationwide in
2021 is a rumour.就这样，两个并不具有代表性的“1%”，得出了
一个较为科学的估算，即“全国七万硕士在送外
卖”。Thus, two non-representative “1%" have led to a
more scientific estimate of “70,000 master’s degree hold-
ers delivering nationwide".REFUTE
Table 9: Wrongly predicted cases of the DeBERTa-large model after the inoculation process. The red texts in
Chinese highlight the differences between the claim/evidence before and after the rewrite.Figure 4: Inoculation results by fine-tuning the model with different sizes of adversarial examples. To evaluate the
models, we employ both the original CHEF test set and the adversarial CHEF test set.