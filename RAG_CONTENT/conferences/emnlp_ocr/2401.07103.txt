Leveraging Large Language Models for NLG Evaluation:
Advances and Challenges
Zhen Li♣∗, Xiaohan Xu△∗, Tao Shen♢, Can Xu♣, Jia-Chen Gu♡, Yuxuan Lai▽,
Chongyang Tao♠†,Shuai Ma♠
♠SKLSDE Lab, Beihang University♣Peking University
△Institute of Information Engineering, CAS
♢AAII, FEIT, University of Technology Sydney♡UCLA▽The Open University of China
lizhen63@pku.edu.cn xuxiaohan@iie.ac.cn tao.shen@uts.edu.au
gujc@ucla.edu {chongyang,mashuai}@buaa.edu.cn
Abstract
In the rapidly evolving domain of Natural
Language Generation (NLG) evaluation, intro-
ducing Large Language Models (LLMs) has
opened new avenues for assessing generated
content quality, e.g., coherence, creativity, and
context relevance. This paper aims to provide
a thorough overview of leveraging LLMs for
NLG evaluation, a burgeoning area that lacks a
systematic analysis. We propose a coherent tax-
onomy for organizing existing LLM-based eval-
uation metrics, offering a structured framework
to understand and compare these methods. Our
detailed exploration includes critically assess-
ing various LLM-based methodologies, as well
as comparing their strengths and limitations in
evaluating NLG outputs. By discussing unre-
solved challenges, including bias, robustness,
domain-specificity, and unified evaluation, this
paper seeks to offer insights to researchers and
advocate for fairer and more advanced NLG
evaluation techniques.
1 Introduction
Natural Language Generation (NLG) stands at
the forefront of modern AI-driven communica-
tion, with recent advancements in large language
models (LLMs) revolutionizing the capabilities
of NLG systems (Ouyang et al., 2022; OpenAI,
2023). These models, powered by deep learning
techniques and vast amounts of training data, ex-
hibit excellent proficiency in generating text across
a wide range of applications. As NLG technol-
ogy continues its rapid evolution, it becomes in-
creasingly imperative to establish robust evaluation
methodologies that can reliably gauge the quality
of the generated content.
Traditional NLG evaluation metrics, such as
BLEU (Papineni et al., 2002), ROUGE (Lin, 2004)
and TER (Snover et al., 2006), primarily focus on
surface-level text differences and often fall short
∗Equal Contribution.
†Corresponding author.
Your task is to evaluatethe generatedtextfrom…instruction
hypothesis
references
sources
Outputs:Explanation:[…]Score:[…]Output
LLMsFigure 1: Illustration of LLMs for NLG evaluation. The
dashed line means that the references and sources are
optional based on the scenarios.
in assessing semantic aspects (Freitag et al., 2020).
This limitation has been noted to hinder research
progress and can lead to misleading research con-
clusions. Additionally, other methods that employ
neural embeddings to calculate the score (Liu et al.,
2016; Sellam et al., 2020; Zhang et al., 2020), de-
spite assessing aspects like semantic equivalence
and fluency, are inflexible and limited in scope (Fre-
itag et al., 2021a). Additionally, these traditional
methods tend to have low alignment with human
judgement (Liu et al., 2023c) and lack interpretabil-
ity for the score (Xu et al., 2023). These drawbacks
underscore the need for more nuanced and compre-
hensive evaluation methods in the NLG field.
The emergent abilities of LLMs present a
promising avenue for the LLM-based NLG eval-
uation, such as Chain-of-Thought (CoT) (Wei
et al., 2022b), zero-shot instruction following (Wei
et al., 2022a), better alignment with human prefer-
ence (Ouyang et al., 2022), etc. These attributes
position LLMs as potent tools for evaluating NLG
outputs, offering a more sophisticated and better
human-aligned assessment compared to traditional
methods (Liu et al., 2023c; Kocmi and Federmann,
2023; Fu et al., 2023). For instance, LLMs could
generate reasonable explanations to support the ulti-
mate score (Xu et al., 2023), and the reinforcement
learning with human feedback (RLHF) could alignarXiv:2401.07103v2  [cs.CL]  12 Jun 2024LLMs’ preference with human better (Ouyang
et al., 2022; Zheng et al., 2023). As in Figure 1,
the key strategy in these approaches involves in-
structing LLMs with prompts to evaluate generated
texts from various aspects, either with references
and sources or not. However, the wide array of
LLM-based NLG evaluation methods, addressing
different tasks and goals, lack a unified overview.
Given the burgeoning volume of work in the
realm of LLMs for NLG evaluation, a synthesized
summary is urgently needed to navigate the com-
plexities and diverse methodologies within this
space. This survey aims to provide a comprehen-
sive overview of this promising domain, presenting
a coherent taxonomy for organizing existing works.
We meticulously delineate pivotal studies and their
methodologies, and delve into an analytical dis-
cussion of the various strengths, limitations, and
distinctive attributes of these approaches. Further-
more, we navigate through the yet-to-be-resolved
challenges and the open-ended questions within
this field, thereby charting potential avenues for
future scholarly exploration. This comprehensive
exploration aims to spark readers with an in-depth
understanding of the nuances and evolving dynam-
ics of LLM-based approaches in NLG evaluation.
Organization of this paper: We present the
first comprehensive survey of recent advancements
in leveraging LLMs for NLG evaluation. Initially,
we establish a formal framework for NLG evalua-
tion and propose a taxonomy to categorize relevant
works (Section 2). Subsequently, we delve into
and elaborate on these works in detail (Section 3).
Furthermore, we conduct a systematic review of
various meta-evaluation benchmarks that assess the
efficacy of LLM-based evaluators (Section 4). Ad-
ditionally, we provide a thorough comparison of
LLM-based evaluators with traditional evaluators
in terms of performance, efficiency and qualitative
qualitative analysis (Section 5). In recognition of
the rapid evolution of this field, we identify and
discuss several potential open problems that may
guide future research (Section 6). To conclude, we
advocate for the advancement of this field through
the development of more impartial, robust, expert
and unified LLM-based evaluators.
2 Formalization and Taxonomy
In this section, we first briefly formalize LLM-
based NLG Evaluation tasks. The objective of
NLG evaluation is to assess the candidate genera-tions of a model across various dimensions, such
as fluency, consistency, etc. Recent advancements
in LLMs have significantly enhanced their capabil-
ities in context comprehension and the generation
of reasonable responses. Notably, contemporary
research has begun to reframe NLG evaluation as
a series of instruction-following tasks, leveraging
powerful capabilities of LLMs (Zhang et al., 2020;
Fu et al., 2023). To maintain generality, we for-
malize the existing evaluation framework for texts
generated by models as follows:
E=f(h, s, r ). (1)
Here, hrepresents the hypothesis text (i.e. can-
didate generation to be evaluated), and fdenotes
the evaluation function, which can be instantiated
by LLMs. The variable sdenotes the input source
of the generation. This source might include the
source text or any supporting documents that pro-
vide background or framing for the generated con-
tent. For instance, in machine translation tasks,
ccould be the sentence in the source language.
Lastly, rrefers to a set of ground truth references
that serve as a benchmark for evaluation. These
references are crucial in tasks like text summariza-
tion, where the quality of the generated summary
is assessed by an annotated reference summary.
In this paper, we classify works in NLG evalu-
ation along three primary dimensions: evaluation
task,evaluation references andevaluation function .
These dimensions provide a comprehensive frame-
work for categorizing and understanding different
approaches within this domain.
Evaluation Task T: NLG encompasses a di-
verse range of tasks, such as Machine Translation
(MT) (Farhad et al., 2021; Bapna et al., 2019), Text
Summarization (TS) (Liu and Liu, 2021; Zhang
et al., 2023a), Dialogue Generation (DG) (Tao
et al., 2018; Kann et al., 2022), Story Generation
(SG) (Yang et al., 2022; Fan et al., 2018), Im-
age Caption (IC) (Tewel et al., 2022; Zhou et al.,
2022), Data-to-Text generation (D2T) (Lin et al.,
2023; Jing et al., 2023) and General Generation
(GE) (Wang et al., 2023g; Zheng et al., 2023), each
with its unique evaluation requirements and chal-
lenges. The specific nature of each task determines
the target evaluation aspects and scenarios. For
instance, in text summarization, the focus may be
on relevance to the source content, while in dia-
logue generation, the coherence of the response isOutputs:Explanation:[…]Score:0.8prompt0.8LLMEncoderLLMEncoderhypothesisreferencesMatching(a)generative-based(b)matching-basedLLMEvaluatorhypothesisreferencessourceLLMEncodersourceFigure 2: Illustration of NLG evaluation functions: (a) generative-based and (b) matching-based methods.
crucial. Given these varied objectives, our taxon-
omy also extends to the lens of task-specific evalu-
ation. This categorization enables us to understand
how different evaluation methods perform across a
spectrum of NLG tasks, thus offering insights into
the strengths and limitations of existing evaluation
paradigms in distinct task contexts.
Evaluation References r: According to whether
the references are available, we divide the evalua-
tion scenarios into reference-based andreference-
freescenarios. In reference-based evaluation, the
generated text his compared against a set of ground
truth references r. This approach is particularly
prevalent in tasks where the quality of the gener-
ated text can be objectively measured against estab-
lished standards. The comparison metrics might fo-
cus on aspects like accuracy, relevance, coherence,
and the degree of similarity to the references. Typi-
cal applications include text summarization, where
the generated summaries are evaluated against ref-
erence summaries, and machine translation, where
the translations are compared with standard trans-
lations. The reference-free approach, in contrast,
does not rely on any external references for eval-
uation. This method evaluates the generated text
hbased on the intrinsic qualities or its alignment
with the provided source context s. Evaluation in
this context may focus on aspects such as fluency,
originality, relevance to the context, etc.
Evaluation Function f: Evaluation function
could be matching-based or generative-based on
the basis of different ways of utilizing LLMs. As
shown in Figure 2, matching-based methods mea-
sure the semantic equivalence between the refer-
ence and hypothesis or measure the proper degree
between the source text and hypothesis. Several
works measure the semantic equivalence between
the reference and hypothesis by using token-levelmatching functions in distributed representation
space (Zhang et al., 2020; Zhao et al., 2019) or dis-
crete string space (Lin, 2004; Papineni et al., 2002).
Others focus on sequence-level, such as (Sellam
et al., 2020; Rei et al., 2020). In contrast,
generative-based methods include methods where
LLMs are employed to generate evaluation metrics
directly. These methods leverage the generative
capabilities of LLMs to assess the quality of gener-
ated text by designing instructions.
Scope of this paper: Recent matching-based
methods are based on a neural encoder to calcu-
late a score-specific aspect of evaluation. How-
ever, these methods often face challenges such as
limited interpretability, lower correlation with hu-
man judgments, and a restricted range of evaluated
aspects (Xu et al., 2023; Fu et al., 2023). Fortu-
nately, the emerging capabilities of LLMs open up
a wealth of possibilities for NLG evaluation. This
includes improved interpretability through CoT,
higher customization via instruction-following ca-
pabilities, and better alignment with human evalua-
tions through RLHF (Xu et al., 2023; Zheng et al.,
2023). Given the abundance of recent surveys
primarily focusing on matching-based evaluation
methods (refer to (Celikyilmaz et al., 2020; Sai
et al., 2022; Goyal et al., 2023) for comprehensive
summaries), our paper is dedicated to exploring
more burgeoning generative-based methods. Fig-
ure 3 presents our taxonomy of generative-based
evaluation. We classify relevant works into two
main categories: prompt-based and tuning-based
evaluation, depending on whether the LLM is tuned.
Further, we divide these methods into subcate-
gories: score-based, probability-based, likert-style,
pairwise comparison, ensemble, and advanced eval-
uation protocols, each distinguished by their eval-
uation form. These categories will be detailed in
Section 3.LLMs for NLG
EvaluationTaxonomy
of Generative
Evaluation (§3)Prompt-based (§3.1)Score-basedGEMBA (Kocmi and Federmann, 2023), Lin (Lin and Chen, 2023), Liu (Liu et al., 2023e),
Wang (Wang et al., 2023b), ICE (Jain et al., 2023), Embed Llama (Dreano et al., 2023)
Probability-based BARTSCORE (Yuan et al., 2021), GPTSCORE (Fu et al., 2023), FFLM (Jia et al., 2023)
Likert-styleGEMBA (Kocmi and Federmann, 2023), Luo (Luo et al., 2023), Gao (Gao et al., 2023),
Skopek (Skopek et al., 2023), LLM-ToT-eval (Zhao et al., 2023), Attrscore (Yue et al., 2023),
Chen (Chen et al., 2023), Bai (Bai et al., 2023), Gilardi (Gilardi et al., 2023),
Huang (Huang et al., 2023), LLM-longeval (Wu et al., 2023b), LLM-judge (Zheng et al., 2023),
Zhuo (Zhuo, 2023), Sottana (Sottana et al., 2023), Ostheimer (Ostheimer et al., 2023),
AUTOCALIBRATE (Liu et al., 2023f), Chiang (Chiang and Lee, 2023)
PairwiseLuo (Luo et al., 2023), Gao (Gao et al., 2023), FairEval (Wang et al., 2023c), Ji (Ji et al., 2023),
LLM-judge (Zheng et al., 2023), EvalLM (Kim et al., 2023b), Bai (Bai et al., 2023),
Chen (Chen et al., 2023), AuPEL (Wang et al., 2023e)
EnsembleDRPE (Wu et al., 2023a), WideDeep (Zhang et al., 2023b), ChatEval (Chan et al., 2023),
Prd (Li et al., 2023c)
AdvancedEAprompt (Lu et al., 2023), Geval (Liu et al., 2023c), FACTSCORE (Min et al., 2023),
ALLURE (Hasanbeig et al., 2023), Para-Ref (Tang et al., 2023)
Tuning-based (§3.2)Probability-based PRISM (Thompson and Post, 2020), T5SCORE (Qin et al., 2022)
Likert-styleTrueTeacher (Gekhman et al., 2023), PERSE (Wang et al., 2023a), Attrscore (Yue et al., 2023),
AUTO-J (Li et al., 2023a), Prometheus (Kim et al., 2023a), CritiqueLLM (Ke et al., 2023) ,
X-EV AL (Liu et al., 2023a)
PairwisePandaLM (Wang et al., 2023f), AUTO-J (Li et al., 2023a), LLM-judge (Zheng et al., 2023),
PERSE (Wang et al., 2023a), Prometheus (Kim et al., 2023a)
Advanced INSTRUCTSCORE (Xu et al., 2023), TIGERScore (Jiang et al., 2023)
Meta-Evaluation
Benchmarks (§4)Machine Translation MQM (Freitag et al., 2021a), WMT Metrics Shared Task (Mathur et al., 2020; Freitag et al., 2021b, 2022)
Text SummarizationNEWSROOM (Grusky et al., 2018), SamSum (Gliwa et al., 2019), REALSumm (Bhandari et al., 2020),
QAGS_XSUM (Wang et al., 2020a), FRANK (Pagnoni et al., 2021), SummEval (Fabbri et al., 2021a),
SummaC (Laban et al., 2022), RiSum (Skopek et al., 2023), OpinSummEval (Shen and Wan, 2023)
Dialogue Generation FED (Mehri and Eskenazi, 2020a), Topical-Chat (Gopalakrishnan et al., 2019), PersonaChat (Zhang et al., 2018)
Image CaptionFlickr8K-Expert (Hodosh et al., 2013), Composite (Aditya et al., 2015), Pascal-50S (Vedantam et al., 2015),
MSCOCO Image Captioning Challenge (Cui et al., 2018)
Data-to-Text BAGEL (Mairesse et al., 2010), SFRES (Wen et al., 2015), SFHOT (Wen et al., 2015), WebNLG (Castro Ferreira et al., 2020)
Story GenerationOpenMEV A (Guan et al., 2021), WP 200(Chen et al., 2022), SCARY 200(Chen et al., 2022), PREF 200(Chen et al., 2022),
COH 200(Chen et al., 2022), Per-MPST (Wang et al., 2023a), Per-DOC (Wang et al., 2023a)
General GenerationAlpacaEval (Li et al., 2023d), MT-bench (Zheng et al., 2023), FairEval (Wang et al., 2023c), Shepherd (Wang et al., 2023d),
LLMBar (Zeng et al., 2023), LLMeval (Zhang et al., 2023b), AttrEval-GenSearch (Yue et al., 2023),
ALIGNBENCH (Liu et al., 2023b)
Figure 3: Taxonomy of research in NLG evaluation with large language models.
3 Generative Evaluation
Amidst the rapid evolution of LLMs, a burgeoning
body of research has directed its focus toward lever-
aging these models as evaluators for NLG tasks.
This attention is particularly rooted in the high-
capacity generative abilities of LLMs, leading to
the emergence of works employing them to pro-
duce quality evaluations of NLG text—a paradigm
we refer to as generative evaluation. This cate-
gory, broadly classified into prompt-based eval-
uation andtuning-based evaluation , hinges on
whether the parameters of LLM evaluators require
fine-tuning. Prompt-based evaluation typically in-
volves prompting robust base LLMs to assess gen-
erated text through meticulous prompt engineering.
On the other hand, tuning-based evaluation relies
on open-source LLMs that are specifically cali-
brated for NLG evaluation. Both approaches cater
to diverse evaluation protocols for measuring the
quality of the generated text.
Current methods consider different scoring pro-tocols to judge the quality of generated hypothesis
text. Some endeavors deploy LLM evaluators to
yield continuous scalar scores that represent the
quality of individual generated texts—termed as
➊score-based evaluation . Others calculate the
generation probability of generated texts based on
prompts, sources, or reference texts (optional) as
the evaluation metric, denoted as ➋probability-
based evaluation . Further diversifying the land-
scape, certain works transform NLG evaluation
into a classification task by categorizing text qual-
ity into multiple levels using likert scales. In this
scenario, LLM evaluators assess the quality of
generated text by assigning it to a specific qual-
ity level—referred to as ➌likert-style evaluation .
Meanwhile, ➍pairwise comparison methods in-
volve using LLM evaluators to compare the quality
of pairs of generated texts. Additionally, ➎en-
semble evaluation methods utilize multiple LLM
evaluators with different LLMs or prompts, orches-
trating communication among evaluators to yieldPrompt Type Prompt Output
Score-basedGiven the source document: [. . . ]
Given the model-generated text: [. . . ]
Please score the quality of the generated text from 1 (worst) to 5 (best)Scores: 2
Likert-styleGiven the source document: [. . . ]
Given the model-generated text: [. . . ]
Is the generated text consistent with the source document? (Answer Yes or No)Yes
PairwiseGiven the source document: [. . . ]
Given the model-generated text 1: [. . . ]
And given the model-generated text 2: [. . . ]
Please answer which text is better-generated and more consistent.Text 1
Table 1: Illustration of different types of prompts.
final evaluation results. Finally, some recent stud-
ies explore ➏advanced evaluation methods (that
consider fine-grained criteria or combine the capa-
bilities of chain-of-thought or in-context leaning)
with the goal of attaining more comprehensive and
nuanced evaluation results.
This section delves into a detailed exploration
of these two overarching categories of evaluation
methods, each accompanied by their respective
evaluation protocols. Table 2 provides a com-
prehensive overview of current prompt-based and
tuning-based evaluation methods. The table delin-
eates their respective adaptation tasks, backbone
models, scoring protocols, and evaluated aspects
for clarity and reference.
3.1 Prompt-based Evaluation
Prompt-based text evaluation stands at the forefront
of advancements in NLG, particularly leveraging
the capabilities of LLMs. In this method, the evalu-
ation process is intricately woven into the crafting
of prompts – specialized cues designed to guide
LLMs in assessing the quality and coherence of
generated text. More recently, the Eval4NLP work-
shop held a shared task on prompting LLMs as
explainable metrics (Leiter et al., 2023). Typically,
a prompt template serves as a structured frame-
work that encompasses instructions, aspects, cri-
teria, and desired output formats , providing a sys-
tematic guide for evaluating generated text. These
templates empower researchers and practitioners
to articulate precise evaluation requirements, en-
suring consistency and reproducibility in the as-
sessment process. By harnessing the prowess of
LLMs, prompt-based evaluation not only provides
a comprehensive understanding of NLG system
performance but also offers a nuanced approach to
extracting valuable insights.
Score Evaluation. An intuitive and widely em-
ployed protocol for utilizing LLM evaluators in textevaluation involves prompting these evaluators to
generate a continuous score that reflects the quality
of the generated text. A concrete example of such
a prompt is illustrated in the first row of Table 1.
Pioneering this method, GEMBA (Kocmi and Fed-
ermann, 2023) proposed the use of LLM evaluators
to assign quality scores, ranging from 0 to 100,
to generate translations both with and without a
reference. GEMBA demonstrates the efficacy of
employing GPT-3.5 or larger LLMs for translation
quality evaluation, showcasing their capabilities
with simple zero-shot prompts. Building on this
foundation, Lin and Chen (2023) have extended
score evaluation methods to broader NLG evalu-
ation domains, aiming to enhance the alignment
between LLM evaluators and manual annotators.
Liu et al. (2023e) tailored LLM evaluators to as-
sess the quality of closed-end response generation,
characterized by unique and correct semantic refer-
ences. Their innovative approach involves prompt-
ing LLMs evaluators to generate explanatory judg-
ments for the generated responses, subsequently ex-
tracting numerical quality scores. Similarly, Wang
et al. (2023b) proposed a unified prompt applica-
ble across various NLG evaluation tasks, which
directly generates quality scores for the produced
texts across different evaluation aspects, both with
and without reference. Additionally, Jain et al.
(2023) employed LLM evaluators with in-context
examples to evaluate summarization tasks, gener-
ating numeric strings that effectively capture the
quality of summarization outputs. These diverse
applications underscore the versatility and adapt-
ability of score-based evaluation methods when har-
nessing LLM evaluators for comprehensive NLG
assessments.
Probability-based evaluation. Recognizing that
the quality of the generated text is often corre-
lated with the ease of generation by LLMs based
on source or reference text, some studies adopt aMetric MT TS DG IC D2TSG GE REF LLMs Protocol Aspects
Prompt-based Evaluation
BARTScore (Yuan et al., 2021) ✓ ✓ * * ✓* * ✓ BART ProbCON/COH/REL/FLU/
INF/COV/ADE
GPTScore (Fu et al., 2023) ✓ ✓ ✓ ✓ * * GPT3 ProbCON/COH/REL/FLU/COV/ACC
MQM/INF/FAC/INT/ENG/NAT
G-EV AL (Liu et al., 2023c) * ✓ ✓ * * * ChatGPT/GPT-4 AdvancedCON/COH/REL/FLU
/NAT/ENG/GRO
Embed Llama (Dreano et al., 2023) ✓* * * * * LlaMA-2 Score NONE
ICE (Jain et al., 2023) * ✓* * * * GPT-3 Score CON/COH/REL/FLU
GEMBA (Kocmi and Federmann, 2023) ✓* * * * * ChatGPT Score/Likert NONE
LLM_eval (Chiang and Lee, 2023) * * * * ✓* ChatGPT Likert GRAM/COH/REL/LIK
FairEval (Wang et al., 2023c) * * * * * ✓ ChatGPT/GPT-4 Pairwise NONE
AuPEL (Wang et al., 2023e) * * * * * ✓ PaLM-2 Pairwise PER/QUA/REL
DRPE (Wu et al., 2023a) * ✓* * * * * ✓ GPT-3 Ensemble CON/COH/REL/FLU/INT/USE
ChatEval (Chan et al., 2023) * * ✓ * * ✓ ChatGPT/GPT-4 Ensemble NAT/COH/ENG/GRO
WideDeep (Zhang et al., 2023b) * * * * * ✓ ChatGPT Ensemble COH/REL/HARM/ACC
PRD (Li et al., 2023c) * * * * * ✓GPT-4/GPT-3.5
Vicuna/Claude/BardEnsemble INF/COH
FACTSCORE (Min et al., 2023) * ✓ ChatGPT Advanced FAC
EAprompt (Lu et al., 2023) ✓* * * * * ChatGPT/text-davinci-003 Advanced NONE
AUTOCALIBRATE (Liu et al., 2023f) * ✓* * * * GPT-4 Likert CON/COH/REL/FLU/INF/NAT
ALLURE (Hasanbeig et al., 2023) * ✓* * * ✓ GPT-4 Advanced CON/COH/FLU/REL
Tuning-based Evaluation
PRISM (Thompson and Post, 2020) ✓* * * * * * ✓ Transformer Prob NONE
T5Score (Qin et al., 2022) ✓ ✓ * * * * * ✓ T5 Prob NONE
TrueTeacher (Gekhman et al., 2023) * ✓* * * * T5 Likert CON
Attscore (Yue et al., 2023) * * * * * ✓Roberta/T5/GPT2
LLaMA/VicunaLikert CON
X-EV AL (Liu et al., 2023a) * ✓ ✓ ✓ * * FLAN-T5-large LikertDEP/LIK/UND/FLE/INF/INQ
INT/SPE/COR/SEM/COH/ENG
NAT/GRO/CON/REL/FLU
AUTO-J (Li et al., 2023a) * * * * * * LLaMA Likert/PairwiseACC/CLA/FEA/CRE/THO
STR/LAY/COM/INF
PERSE (Wang et al., 2023a) * * * * * ✓*✓ LLaMA Likert/Pairwise INT/ADA/SUR/CHA/END
PandaLM (Wang et al., 2023f) * * * * * ✓ LLaMA Pairwise CLA/COM/FOR/ADH
TIGERScore (Jiang et al., 2023) ✓ ✓ * ✓ ✓ ✓ LLaMA Advanced COH/INF/ACC/COM
INSTRUCTSCORE (Xu et al., 2023) ✓* * * * * * ✓ LLaMA Advanced NONE
Prometheus (Kim et al., 2023a) * * * * * ✓ LLaMA-2 Likert/Pairwise NONE
CritiqueLLM (Ke et al., 2023) * * * * * ✓ ChatGLM Likert NONE
Table 2: Automatic metrics proposed ( ✓) and adopted (*) for various NLG tasks. REF indicate the method is
source context-free. MT: Machine Translation, TS: Text Summarization, DG: Dialogue Generation, IC: Image
Captioning, D2T: Data-to-Text, SG: Story Generation, GE: General Generation. We adopted the evaluation aspects
for different tasks from Fu et al. (2023). Specifically, for each evaluation aspect, CON : consistency, COH : coherence,
REL: relevance, FLU : fluency, INF: informativeness, COV : semantic coverage, ADE : adequacy, NAT: naturalness,
ENG : engagement, GRO : groundness, GRAM : grammaticality, LIK: likability, PER: personalization, QUA : quality,
INT: interest, USE: usefulness, HARM : harmlessness, ACC : accuracy, FAC: factuality, ADA : adaptability, SUR:
surprise, CHA : character, END : ending, FEA: feasibility, CRE : creativity, THO : thoroughness, STR: structure,
LAY: layout, CLA: clarity, COM : comprehensiveness, FPR: formality, ADH : adherence, DEP : topic depth, UND :
understandability, FLE: flexibility, INQ: inquisitiveness, SPE: specificity, COR : correctness, SEM : semantic
appropriateness. NONE means that the method does not specify any aspects and gives an overall evaluation. The
detailed explanation of most evaluation aspect can be found in Fu et al. (2023).
unique perspective by framing the evaluation task
as a conditional generation task. In this context, the
generative likelihood of the produced text is calcu-
lated, serving as the score indicative of text quality,
as illustrated in the second row of Table 1. Yuan
et al. (2021) first leveraged BART (Lewis et al.,
2019) as an evaluator to compute the probability of
the generated text based on source or reference textacross diverse evaluation aspects in machine trans-
lation, text summarization, and data-to-text tasks.
Expanding on this methodology, Fu et al. (2023)
devised prompts that encompass task descriptions
and definitions of evaluation aspects, utilized to
instruct an LLM-based evaluator to calculate the
generation probability of generated text. In contrast
to the conventional use of generation probability asa quality score, Jia et al. (2023) calculated the three
probability changes as the reference-free metric to
evaluate the faithfulness of the generated summary.
These changes include the transition from generat-
ing a summary with the source document to directly
generating a summary, altering the position of the
source and summary, and the shift from generating
a summary with the source document to generating
a summary with a specific piece of a prefix.
Likert-Style Evaluation. Inspired by the human
annotation process, several studies employ LLM
evaluators to assess the quality levels of generated
texts, where these evaluators produce ratings or
quality labels based on a likert-style scale (Bai
et al., 2023; Gao et al., 2023; Gilardi et al., 2023;
Huang et al., 2023; Zhao et al., 2023; Wu et al.,
2023b; Luo et al., 2023; Zheng et al., 2023; Zhuo,
2023; Sottana et al., 2023; Skopek et al., 2023).
A representative likert-style prompt is depicted in
the third line of Table 1. For instance, Chiang
and Lee (2023) provided LLM evaluators with the
same evaluation instructions as human annotators,
prompting them to rate the quality of generated
texts using a 5-point likert scale. Meanwhile, Gao
et al. (2023) instructed ChatGPT to rate model-
generated summarizations across multiple evalu-
ation aspects, such as relevance, faithfulness, flu-
ency, and coherence, using a scale ranging from
1 (worst) to 5 (best) based on the provided source
document. In a similar vein, Ostheimer et al. (2023)
designed multiple prompts, each guiding the LLM
evaluator to assess a specific evaluation aspect of
a text style transfer task. By comparing the trans-
ferred text with the source text, the LLM evaluator
generates a discrete scale ranging from 1 to 5 to
represent the quality of the transferred text. This ap-
proach exemplifies the adaptability of likert-style
prompts in capturing diverse dimensions of text
quality through LLM evaluations.
Yue et al. (2023) proposed to utilize LLM evalua-
tor to evaluate the attribution capabilities of the gen-
erative models which judges if the generated state-
ment is thoroughly supported by the referenced
source. This work designs three categories of qual-
ity labels, including attributable, extrapolatory, and
contradictory, and prompts the LLM evaluator with
explicit instructions that include definitions of la-
bels. Liu et al. (2023f) utilized LLMs to draft,
filter, and refine comprehensive evaluation criteria
as score instructions, which achieves more con-
sistent evaluation results with human annotatorswhen evaluating text summarization, data-to-text
generation and hallucination tasks.
Pairwise Evaluation. Compared with utilizing
LLM evaluators to individually evaluate the qual-
ity of generated texts through numerical scores or
likert-style rating, another way of using LLM for
evaluation is to explicitly compare with other gen-
erated text and decide which one is superior (Bai
et al., 2023; Ji et al., 2023). A representative
prompt is shown in the last row of Table 1. Wang
et al. (2023c) utilized LLM evaluator to obtain eval-
uation result of two model-generated responses for
one given query. This method proposes multiple
evidence and balanced position calibration, and
seeks assistance from human annotators when the
quality of two texts is close to avoid the impact of
the order of response pairs in the prompt on eval-
uation results. Wang et al. (2023e) introduced a
reference-free personalized text generation evalua-
tion framework that prompts LLM evaluator to per-
form pairwise comparisons between the generated
text pairs in three essential aspects: personalization,
quality, and relevance of the generated text through
providing a detailed explanation of its judgment.
Ensemble Evaluation. As the actual evaluation
process often involves collaborative evaluation by
multiple human annotators, some works utilize mul-
tiple LLM evaluators with different base models
or prompts and allow them to evaluate the quality
of generated text from different perspectives, as
shown in Figure 5. Wu et al. (2023a) set multiple
roles for the LLM to evaluate the quality of the gen-
erated summary by comparing it with the reference
one on both subjective and objective dimensions.
This work generates dynamic role profiles accord-
ing to input texts and synthesizes the results of
multiple roles as the final evaluation result. Li et al.
(2023c) utilized multiple LLM evaluators to con-
duct a pairwise evaluation for the model-generated
responses by performing multiple rounds of dis-
cussions on the comparison results to reach a mu-
tual agreement on the pairwise scoring. Similarly,
Zhang et al. (2023b) proposed to set up LLM eval-
uators as multiple-layer neural network structures.
The bottom evaluators obtain the evaluation result
of model-generated responses from a specific eval-
uation perspective. The upper evaluators receive
all evaluation information from the previous layer
and discuss it with each other to obtain a more com-
prehensive evaluation result. Besides, Chan et al.Figure 4: A example of fine-grained evaluation inspired
by Jiang et al. (2023).
Figure 5: A example of ensemble evaluation inspired
by Li et al. (2023c).
(2023) designed diverse communication strategies
with various role prompts during collaborative dis-
cussions to evaluate pairwise generated responses.
Advanced Evaluation. Some recent works in-
vestigate advanced evaluation techniques aimed
at achieving more thorough and nuanced assess-
ment outcomes by leveraging chain-of-thought, in-
context learning capabilities, fine-grained analy-
sis, etc. A representative fine-grained evaluation
method is shown in Figure 4. Liu et al. (2023c) uti-
lized LLMs with chain-of-thought (CoT) prompt-
ing and a form-filling paradigm to evaluate the
quality of generated texts across various NLG tasks.
Min et al. (2023) proposed a find-grained evalu-
ation schema that first extracts a series of atomic
facts from the LLM-generated long text, and then
utilizes LLM evaluator to validate each atomic fact
with the given knowledge source. Lu et al. (2023)
proposed a new prompting method called Error
Analysis Prompting (EAPrompt) that combines
CoT to prompt the LLM evaluator to analyze dif-
ferent types of pre-defined errors (e.g., major and
minor errors) in the generated translation based on
the given source text and reference translation, and
then measures the quality of a generated translation
with the previous error analysis. To enhance and
improve the robustness of LLM-based evaluators,
Hasanbeig et al. (2023) proposed ALLURE, a sys-
tematic protocol for auditing and improving LLM-
based evaluation with iterative in-context-learning.
Considering that the evaluation with a single or few
references may not accurately reflect the quality of
the model’s hypotheses, Tang et al. (2023) lever-
aged LLMs to paraphrase a single reference into
multiple high-quality ones in diverse expressions,
which enhances various evaluation methods on MT,
TS, and caption tasks. To further task advantagesof the in-context learning capability of LLMs, Liu
et al. (2023f) proposed AUTOCALIBRATE to auto-
matically align and calibrate an LLM-based evalua-
tor through incorporating the mined and calibrated
rubrics into scoring instructions.
3.2 Tuning-based Evaluation
Recently, researchers increasingly turn their at-
tention towards fine-tuning open-source language
models (e.g., LLaMA), in lieu of traditional closed-
based LLMs (e.g., ChatGPT and GPT-4). This shift
is propelled by a thorough exploration of key per-
spectives, including the expenses associated with
API calls, the robustness of prompting, and the
pivotal consideration of domain adaptability.
In contrast to closed-based models that invari-
ably demand expensive API calls for each eval-
uation instance, the fine-tuning of smaller open-
source LLMs provides a cost-effective alternative.
This approach empowers researchers to evaluate
their models on specific tasks without incurring the
financial burden associated with extensive API us-
age. Additionally, the process of prompting LLMs
for NLG evaluation requires meticulous crafting of
prompts, with variations potentially resulting in sig-
nificant differences in outcomes. Furthermore, the
consideration of domain adaptability underscores
the evolving landscape of NLG evaluation. Fine-
tuning open-source LLMs affords researchers the
flexibility to tailor models to diverse domains, tran-
scending the limitations imposed by closed-based
models confined to specific niches.
Typically, tuning-based methods construct eval-
uation data manually (Zheng et al., 2023) or with
the assistance of advanced LLMs (e.g., GPT-4) (Xu
et al., 2024), followed by performing supervised
tuning. Similar to prompting-based evaluation,
tuning-based methods can also be categorized intovarious types based on their scoring protocol, such
aslikert-style evaluation ,probability-based evalua-
tionandpairwise evaluation . In addition, based on
the output explanations in supervised fine-tuning,
these methods can be further divided into holistic
evaluation orerror-oriented evaluation . We will
begin by introducing various types of scoring pro-
tocols and subsequently provide a summary of two
output explanations in the final advance evaluation.
Likert-Style Evaluation. Some works tune
LLMs to provide quality ratings or labels for gener-
ated texts. Gekhman et al. (2023) employed FLAN-
PaLM 540B (Chung et al., 2022) to annotate the
quality of real model-generated summaries and uti-
lized these annotated data as training data to tune a
light-weight LLM (e.g., T5-11B) as a factual con-
sistency summary evaluator, which predicts “1" if
the summary demonstrates factual consistency and
“0" otherwise. Yue et al. (2023) reused and repur-
posed the existing fact-checking, NLI, and sum-
marization tasks datasets and obtained simulated
data from open-domain QA datasets to tune light-
weight LLMs for attribution evaluation, which gen-
erates attributable, extrapolatory or contradictory
labels for the generated answer with given query
and reference documents. Li et al. (2023a) cre-
ated a dataset containing multiple scenarios and
used GPT-4 (OpenAI, 2023) to generate evalua-
tion judgments for each scenario as supervision
signals to tune LLaMA as a generative evaluator,
which can output overall quality rating for indi-
vidual LLM-generated response in various sce-
narios. Wang et al. (2023a) repurposed existing
datasets with proper anonymization and new per-
sonalized labels to tune LLaMA2 (Touvron et al.,
2023) as a personalized story evaluation model
which provides personalized evaluation for gener-
ated texts through outputting a grade in [1, 10] and
detailed reviews. Kim et al. (2023a) prompted GPT-
4 to construct training data, including reference an-
swers and crafted diverse customized score rubrics,
and used them to tune LLaMA to evaluate model-
generated responses of given instruction, which
is generalized to realistic user demands. Ke et al.
(2023) instructed GPT-4 to collect referenced and
reference-free training data with dialogue-based
prompting, utilized to tune LLMs for evaluating
the alignment of model-generated texts with human
instructions through generating scores and expla-
nations. (Liu et al., 2023a) constructed a reference-
free instruction-tuning dataset tailored for multi-aspect evaluation across summarization, dialogue
and data-to-text tasks. Considering that there is an
internal correlation between the evaluation aspects,
this work tuned with auxiliary aspects additionally
on diverse evaluation task forms. During inference,
this work combined auxiliary and target aspects
and predicted either the “Yes” or “No” label to
judge whether the generated text satisfied the target
aspect and compute the evaluation score.
Probability-based Evaluation. Some works
train generative LLMs to calculate the genera-
tion probability of generated texts to evaluate text
quality. For example, Thompson and Post (2020)
trained a transformer as a multilingual reference-
to-candidate paraphraser to obtain the generated
probability of model-generated translation based
on their reference texts. Qin et al. (2022) tuned
the T5 model in the generative and discriminative
fashion, and used the probability of generating a
text as the quality score.
Pairwise Evaluation. There are also some works
tuning LLMs for comparison between generated
text pairs. Wang et al. (2023f) collected response
pairs from LLMs and asked GPT-3.5 to generate
output judgments, utilized which to tune LLaMA-
7B to evaluate a pair of model-generated responses
with the given query, accompanied by a concise de-
scription of the evaluation procedure. Zheng et al.
(2023) performed fine-tuning on Vicuna using a hu-
man votes dataset from Chatbot Arena to pairwise
evaluate two answers with the given query.
Advanced Evaluation. Nearly all tuning-based
evaluators are trained to emulate evaluate behav-
ior (the score or explanations) produced by strong
closed models like GPT-4 or ChatGPT. In the con-
text of supervised fine-tuning, the majority of stud-
ies gravitate towards holistic evaluation (Li et al.,
2023a; Wang et al., 2023f,a; Kim et al., 2023a),
which involves a comprehensive assessment of the
generated content, providing an overarching expla-
nation for the assigned score. It takes into account a
diverse range of factors, including coherence, rele-
vance, and fluency, to offer a holistic understanding
of the quality of the hypothesis text. Besides, some
studies explore error-oriented evaluation which
focused on examining and explaining the specific
errors in the hypothesis text, offering insights into
why a particular score is derived. This category
delves into the fine-grained aspects of generated
content to identify and justify evaluation outcomes.For instance, Yue et al. (2023) first defined differ-
ent types of attribution errors, and then explored
prompting LLMs or fine-tuning smaller LLMs on
simulated and repurposed data from related tasks
such as question answering (QA), fact-checking,
natural language inference (NLI), and summariza-
tion. Xu et al. (2023) utilized GPT-4 to construct
fine-grained analysis data to tune LLaMA to gen-
erate error analysis for generated text compared
with reference text, after which this work utilized
real model-generated response-reference pairs to
refine and self-train evaluator. Furthermore, Jiang
et al. (2023) sampled data from diverse text gen-
eration datasets, including summarization, transla-
tion and data2text, whose system outputs included
real-world system output and GPT-4 synthesis, and
prompted GPT-4 to curate error analysis to tune
LLaMA for fine-grained evaluation.
4 Benchmarks and Tasks
LLM-based evaluators have found application
across various NLG tasks. Simultaneously, a mul-
titude of existing and recently introduced meta-
evaluation benchmarks serve the purpose of validat-
ing the efficacy of these evaluators. These bench-
marks incorporate human annotations gauging the
quality of generated text, and evaluating the de-
gree of concurrence between automatic evaluators
and human preferences. Categorized based on the
tasks involved, these benchmarks can be classified
into single-scenario examples, such as machine
translation and summarization, as well as multi-
scenario benchmarks. This section will provide an
overview of these NLG tasks and their associated
meta-evaluation benchmarks.
Machine Translation (MT). MT task is centered
around converting a sentence or document from a
source language into a target language while pre-
serving the same semantic meaning. The Annual
WMT Metrics Shared tasks (Mathur et al., 2020;
Freitag et al., 2021b, 2022) annually introduce a
set of benchmarks encompassing model-generated
translations, source text, reference text, and human
judgment across multiple languages, such as En-
glish to German, English to Russian, among others.
These benchmarks provide a valuable resource for
evaluating the correlation between automatic eval-
uators and human judgment. Simultaneously, Fre-
itag et al. (2021a) curated and annotated outputs
from 10 translated systems for both English-to-
German and Chinese-to-English translation pairsin the WMT 2020 news translation task (Barrault
et al., 2020). Employing professionals and crowd
workers as annotators, they assigned scalar ratings
on a 7-point scale to each translation, utilizing
multi-dimensional quality metrics scoring.
Text Summarizing (TS). TS involves generat-
ing a concise and coherent summary of a given
piece of text while capturing its essential mean-
ing. There are many meta-evaluation benchmarks
are proposed (Grusky et al., 2018; Gliwa et al.,
2019; Bhandari et al., 2020; Wang et al., 2020b;
Pagnoni et al., 2021; Laban et al., 2022; Skopek
et al., 2023). One of the widely used benchmarks is
SummEval (Fabbri et al., 2021b). This benchmark
includes summaries generated by 16 models from
100 source news articles randomly sampled from
the CNN/DailyMail test set (Hermann et al., 2015),
and each summary underwent annotation by five
separate crowd-sourced workers and three indepen-
dent experts on a Likert scale from 1 to 5 along
four dimensions: coherence, consistency, fluency
and relevance. In addition, Shen and Wan (2023)
presented a meta-evaluation benchmark for opinion
summarization tasks, including human judgments
and outputs from 14 opinion summarization mod-
els over four dimensions: aspect relevance, self-
coherence, sentiment consistency and readability,
where opinion summarization task focuses on ex-
tracting opinions from a large number of reviews.
Dialogue Generation (DG). DG task aims to
generate human-like responses in the context of
a conversation, including open-domain and task-
oriented dialogue generation tasks. The model-
generated dialogue should be natural and interest-
ing, while also being consistent with the context.
Mehri and Eskenazi (2020b) performed human an-
notations across two open-domain dialog corpora
Topical-Chat (Gopalakrishnan et al., 2019) and Per-
sonaChat (Zhang et al., 2018). For each dataset, 60
dialogue contexts are sampled with six responses
per context for Topical-Chat and five responses
for PersonaChat, where each response is generated
from dialogue systems and human outputs. Each
response is scored from 6 dimensions including
naturalness, coherence, engagingness, grounded-
ness, understandability and overall quality. Mehri
and Eskenazi (2020a) sampled and annotated a sub-
set from a set of conversations between a human
and another human or two open-domain dialog sys-
tems (Adiwardana et al., 2020). Turn-level anddialog-level human judgment are performed, re-
spectively, for each sampled conversation on eigh-
teen dialog quality dimensions.
Image Caption (IC). The task involves gen-
erating textual descriptions or captions for im-
ages. Meta-evaluation benchmarks of image cap-
tion contain human annotations for image-textual
pairs (Aditya et al., 2015; Vedantam et al., 2015).
For example, the commonly used Flickr 8k dataset
(Hodosh et al., 2013) collects two sets of human
annotations. One set includes 17K expert judg-
ments annotation, which asks human experts to
rate the image-caption pairs with scores ranging
from 1 to 4, and another set includes 145K binary
quality judgments gathered from CrowdFlower for
each image-caption pair, which decide whether a
caption describes the corresponding image or not.
Considering some NLG evaluators can only handle
textual modal information, some meta-evaluation
benchmarks also include a reference caption for
each image. Cui et al. (2018) collected human
judgments for twelve submission entries from the
2015 COCO Captioning Challenge on the COCO
validation set (Vinyals et al., 2016), where each
system generates one caption for each image, and
each image has five reference captions.
Data-to-Text (D2T). D2T task involves gener-
ating fluent and factual human-readable text from
structured data. Mairesse et al. (2010) proposed
BAGEL, which contains 202 samples about restau-
rants in Cambridge, where each sample includes
structured information context with corresponding
generated texts, references and human judgments.
Wen et al. (2015) further proposed SFRES and
SFHOT, which contain 581 samples of restaurants
and 398 samples of hotels in San Francisco, re-
spectively. The human judgments in these bench-
marks focus on informativeness, naturalness and
overall quality of generated texts. WebNLG+
Shared Tasks (Castro Ferreira et al., 2020) also
publish WebNLG dataset annually, which con-
tains Wikipedia triples with corresponding human-
annotated texts.
Story Generation (SG). The task involves cre-
ating coherent and contextually relevant narratives
or stories with the given beginning of a story or
writing requirement. Most meta-evaluation bench-
marks of story generation always contain stories
and corresponding manually annotated judgment
scores (Guan et al., 2021; Chen et al., 2022). Be-sides, Wang et al. (2023a) created two personalized
story evaluation benchmarks denoted as Per-MPST
and Per-DOC to evaluate the quality of generated
stories with a given evaluator persona. This work
repurposes existing datasets (Kar et al., 2018; Yang
et al., 2023b) through anonymizing and summa-
rizing. Both them view multiple reviews by the
same reviewer as an implicit persona preference
and provide personalized human judgements for
each generated story.
General Generation (GE). As LLMs have been
increasingly used in general NLG tasks, such as
math, reason, dialogue and open-ended QA, etc.,
LLM evaluators have proposed to effectively evalu-
ate the quality of the model-generated texts across
multiple scenario (Kim et al., 2023a; Ke et al.,
2023). Accordingly, there are many multi-scenario
meta-evaluation benchmarks are proposed (Wang
et al., 2023c; Zheng et al., 2023; Wang et al., 2023d;
Yue et al., 2023). Typically, Zhang et al. (2023b)
sampled 2,553 evaluation samples, including in-
structions and model-generated response pairs with
corresponding human-annotated preference labels
from multiple task datasets such as dialogue, open-
domain QA, and programming. Further, Zeng et al.
(2023) proposed a benchmark that includes 419
evaluation samples and can be categorized into
two parts: NATURAL and ADVERSARIAL sets.
The former collects and filters instances from exist-
ing human-preference benchmarks to ensure that
each instance has an objective preference. The lat-
ter includes the adversarial instances created by
authors that go against instruction but have good
superficial qualities and are challenging for evalu-
ators. Liu et al. (2023b) sampled 400 evaluation
instances, including Chinese queries, correspond-
ing references and model-generated answers from
ALIGNBENCH across extensive task categories,
such as open-ended questions, writing ability, log-
ical reasoning, etc. Then the authors assigned hu-
man annotators to judge ratings for each instance
to verify the agreement of LLM-based evaluators
with human judging.
5 Comparison with Traditional
Evaluators
Qualitative Comparison Traditional evaluation
metrics (e.g., BLEU (Papineni et al., 2002) and
ROUGE) focus on exacting n-gram matches, which
penalizes semantically correct but lexically differ-
ent hypotheses. These methods are simple and fastMetrics SupSummEval Topical-Chat WMT22
COH CON FLU REL Avg NAT COH ENG GRO Avg En-De En-Ru Zh-Eu
Traditional Metrics (Word Overlap)
ROUGE-1 0.167 0.160 0.115 0.326 0.192 0.158 0.206 0.319 0.264 0.233 - - -
ROUGE-2 0.184 0.187 0.159 0.290 0.205 0.168 0.247 0.337 0.311 0.266 - - -
ROUGE-L 0.128 0.115 0.105 0.311 0.165 0.145 0.205 0.306 0.293 0.237 - - -
BLEU - - - - - 0.175 0.235 0.316 0.310 0.259 0.169 0.140 0.145
BERT-based Metrics
BERTScore 0.284 0.110 0.193 0.312 0.225 0.209 0.233 0.335 0.317 0.273 0.232 0.192 0.316
BLEURT ✓ - - - - - - - - - - 0.344 0.359 0.361
BARTScore ✓ 0.448 0.382 0.356 0.356 0.385 -0.053 -0.079 -0.084 -0.197 -0.103 - - 0.220
UniTE ✓ - - - - - - - - - - 0.369 0.378 0.357
UniEval ✓ 0.575 0.446 0.449 0.426 0.474 0.450 0.616 0.615 0.590 0.568 - - -
LLM-based Metrics
GPTScore 0.434 0.449 0.403 0.381 0.417 - - - - - - - 0.187
CHATGPT(DA) 0.451 0.432 0.380 0.439 0.425 0.474 0.527 0.599 0.576 0.544 0.306 0.332 0.371
G-Eval 0.582 0.507 0.455 0.547 0.514 0.607 0.590 0.605 0.536 0.590 - - -
Embed Llama - - - - - - - - - - 0.400 0.227 0.217
X-Eval ✓ 0.530 0.428 0.461 0.500 0.480 0.478 0.622 0.593 0.728 0.605 - - -
Table 3: Performance of traditional and LLM-based metrics on Text Summarizing (SummEval), Dialogue Gen-
eration (Topical-Chat) and Machine Translation (WMT22) tasks. We demonstrate the sample-level Spearman
correlations on SummEval and Topical-Chat benchmarks and the segment-level Kendall-Tau correlations on WMT22
benchmarks respectively. Sup indicates the metric is supervised. The specific representation of the evaluation
aspects (COH/CON/FLU/REL/NAT/ENG/GRO) is shown in Table 2.
but not robust to paraphrasing. BERTScore (Zhang
et al., 2020) measures quality through semantic
similarity based on BERT contextual embeddings,
effectively handling paraphrases and synonyms.
However, such matching-based evaluators depend
on the quality of the pre-trained embeddings, may
struggle with very fine-grained semantic distinc-
tions, and neglect the overall semantics of the hy-
potheses and references. Additionally, neither met-
ric accounts for fluency or readability during evalu-
ation and both still rely on reference texts.
In contrast, LLMs have a strong capability for
language understanding and generation, which sup-
ports evaluating quality without needing references.
They can adapt to various domains and languages,
making them suitable for a wide range of NLG
tasks without requiring task-specific feature engi-
neering. LLMs also provide more nuanced eval-
uation criteria beyond traditional metrics, such as
semantic coherence, fluency and possible explana-
tions. However, LLM-based methods are compu-
tationally more intensive due to their vast architec-
tures. Additionally, prompting LLMs for NLG eval-
uation requires careful crafting of prompts. Varia-
tions in these prompts can lead to substantial differ-
ences in evaluation outcomes, as indicated in (Gao
et al., 2023). Section 6 summarizes more open
problems of LLM-based metrics.Performance Comparison Table 3 summarizes
the performance of both traditional word-overlap
metrics, BERT-based metrics and recent LLM-
based metrics on representative benchmarks such
as SummEval, WMT, and Topical-Chat. We can
easy to observe that the latter two metrics generally
perform better than word-overlap metrics. Despite
not being fine-tuned, the most competitive LLM-
based methods (e.g., G-Eval for summarization and
CHATGPT(DA) for machine translation) generally
achieve a higher correlation with all traditional met-
rics, whether for unsupervised or fine-tuned meth-
ods. These results reveal the strong capability of
LLMs in language understanding, contextual anal-
ysis, coherence checking, and fluency assessment
of generated text. Among the three tasks, the per-
formance gap between LLM-based evaluators and
traditional evaluators is not significant in the ma-
chine translation task. This phenomenon might
be due to the limitations of LLM-based models in
cross-lingual understanding. Additionally, accord-
ing to the results of last row in the table, we can ob-
serve that the performance of different LLM-based
metrics varies significantly, which implies their sen-
sitivity to prompt crafting. In contrast, traditional
unsupervised methods like ROUGE, BLEU, and
BERTScore are more robust, although their overall
performance is relatively worse.Methods Backbone TS DG
BLEU - 977.31 2344.16
ROUGE - 446.36 2379.24
BERTScore BERT 37.64 42.37
ChatGPT(DA) ChatGPT 1.94 1.87
G-Eval GPT-4 1.51 1.40
TIGERScore Llama 2.67 3.72
Table 4: Efficiency Comparison. We report the aver-
age number of texts evaluated per second for different
metrics.
Efficiency Comparison Table 4 presents the av-
erage number of texts evaluated per second for
different metrics in the SummEval (TS task) and
Topical-chat (DG task) benchmarks. This compar-
ison highlights the efficiency differences between
traditional metrics and LLM-based metrics. Our
tests were conducted on an NVIDIA A40 GPU.
The results show that efficiency generally corre-
lates with model size and traditional word-overlap
metrics (e.g., BLEU and ROUGE) are significantly
faster than other metrics. Specifically, LLM-based
evaluators are about 200to400times slower than
traditional word-overlap metrics. However, their
efficiency can be improved with advanced LM in-
ference tools such as vLLM1. While LLM-based
evaluators are suitable for offline evaluation, they
may not be feasible for online evaluation.
6 Challenges and Open Problems
This paper provides a comprehensive review of re-
cent natural language generation evaluations based
on LLMs, encompassing both prompt-based and
tuning-based evaluation approaches. Despite sig-
nificant efforts and notable achievements across
various text generation benchmarks, several chal-
lenges in the field persist.
Bias of LLM-based Evaluators. The use of
LLMs as evaluators inherently cast the text eval-
uation as a generation task. Consequently, when
LLMs are employed in this evaluator role, they
may carry over biases intrinsic to their function
as generators. These biases may include social
biases, such as stereotypes related to specific demo-
graphic identities (e.g., race, gender, religion, cul-
ture, and ideology) (Sheng et al., 2021). In addition
to these general biases, LLMs-as-evaluators are
subject to specific biases unique to their evaluative
role. These include order bias, where preference
1https://github.com/vllm-project/vllmis given to options based on their sequence (Zheng
et al., 2023; Wang et al., 2023c; Koo et al., 2023);
egocentric bias, where a tendency exists to favor
texts generated by the same LLM (Liu et al., 2023d;
Koo et al., 2023); and length bias, which leads to a
preference for longer or shorter texts (Zheng et al.,
2023; Koo et al., 2023). Therefore, in leveraging
LLMs for evaluation purposes, it is crucial to cali-
brate both the inherent biases of LLMs as well as
those biases specific to their function as evaluators.
This dual consideration is essential for the effective
and fair use of LLMs in NLG evaluation tasks.
Robustness of LLM-based Evaluators. Most
LLMs-based evaluation methods rely heavily on
prompt engineering. However, the process of
prompting LLMs for NLG evaluation demands
careful and meticulous crafting of prompts. The
variations in these prompts can potentially lead to
substantial differences in the outcomes of the evalu-
ation process. Some works have investigated the ro-
bustness of LLM-based evaluators by constructing
adversarial datasets. These datasets are designed to
test the evaluators’ resilience by introducing false
or off-topic information, thereby examining the im-
pact of such distortions on their evaluative accuracy.
Their findings shed light on the significant room
for improvement in the robustness of LLM-based
evaluators. For instance, Liu et al. (2023e) de-
veloped two adversarial meta-evaluation datasets
for dialogue generation with adversarial instances
inconsistent with gold references. Koo et al. (2023)
introduced a benchmark containing two adversarial
aspects: Distraction and Bandwagon Effect, which
involve appending irrelevant information or fab-
ricated statistics, such as a misleading majority
preference, to the initial instructions. The results
suggest a general lack of robustness in many LLMs
under such adversarial conditions. The robustness
of LLM-based evaluators emerges as a critical area
of exploration, underscoring the need for further
research to enhance their robustness in the face of
challenging or misleading inputs.
Which came first, the chicken or the egg?
LLM-based evaluators frequently utilize GPT-
4 (Liu et al., 2023c; Xu et al., 2023; Zheng et al.,
2023), owing to its status as one of the most ad-
vanced LLM (OpenAI, 2023). However, relying
on GPT-4 for evaluation might lead to biased or
skewed results, especially when evaluating out-
puts generated by itself or an equally powerfulmodel (Bai et al., 2023; Zheng et al., 2023). The
impartiality of such evaluations is questionable if
the evaluator (LLM-as-evaluator) possesses capa-
bilities comparable to the model being evaluated
(LLM-as-generator). This is compounded by the
egocentric bias of LLMs, including GPT-4, to ex-
hibit biases like favoring their own generated re-
sponses (Bai et al., 2023). This scenario mirrors
the chicken-and-egg dilemma: an LLM-based eval-
uator relies on a more powerful LLM, yet the de-
velopment of a more powerful LLM depends on
having a robust evaluator. To address this dilemma,
a broader spectrum of evaluation methods is nec-
essary, involving various benchmark (Srivastava
et al., 2022; Liang et al., 2022), evaluation crite-
ria (Sellam et al., 2020), and human feedback (Xu
et al., 2023; Ouyang et al., 2022) to ensure more
balanced and comprehensive assessments.
Domain-Specific Evaluation. LLMs have been
prevalent across various domains, such as law (Cui
et al., 2023a), medicine (Singhal et al., 2023), fi-
nance (Yang et al., 2023a), etc. However, most
LLMs employed as evaluators are designed for gen-
eral domains and are not specifically tailored to any
particular field. This lack of specialization poses
significant challenges. On one hand, these LLMs
often lack the requisite domain-specific knowledge,
making it difficult for them to accurately assess the
correctness of content within specialized fields. On
the other hand, the evaluation prompts need to be
meticulously designed for different domains. This
may involve tailoring the aspects of evaluation rel-
evant to each field. For example, while evaluating
legal documents, aspects such as legal accuracy and
adherence to the judicial system are crucial (Cui
et al., 2023b), which differ significantly from the
aspects relevant in medical or financial texts. There-
fore, to enhance the efficacy of LLMs as evaluators
in specialized domains, there’s a pressing need to
develop models that are not only domain-aware but
also equipped with the capability to evaluate based
on domain-specific criteria.
Unified Evaluation. LLMs have been expanded
w.r.t their broad capabilities beyond traditional
single-task focuses, encompassing complex instruc-
tions like coding and open-ended real-world re-
quirements (OpenAI, 2023; Significant Gravitas).
Consequently, there is an increasing demand for
more comprehensive and flexible evaluation meth-
ods. However, traditional evaluation methods andmost current LLM-based evaluators are limited to
constrained tasks and evaluation aspects (cf. Ta-
ble 2). Some promising attempts have been made
in this direction. For instance, MT-Bench (Zheng
et al., 2023) uses GPT-4 as an evaluator across mul-
tiple domains for multi-turn questions. Yet, this
is too confined to a few evaluation aspects and
limits dialogue to two turns only. Another model,
Auto-J (Li et al., 2023b), approaches from a data
construction perspective, training a 13B LLM on
user queries and GPT-4 generated responses across
a wide range of real-world scenarios. It accom-
modates diverse evaluation protocols and has been
validated in 58 different scenarios, even outper-
forming many proprietary LLMs. In light of in-
creasingly complex user queries, we advocate that
developing a more unified and contemporaneous
evaluation protocol is a promising direction. Addi-
tionally, constructing high-quality, comprehensive
datasets to train unified models holds great poten-
tial. Such advancements could significantly con-
tribute to more effective and universal evaluations
of LLMs.
7 Conclusion
In this paper, we have meticulously surveyed the
role of LLMs in the evaluation of NLG. Our com-
prehensive taxonomy classifies works along three
primary dimensions: evaluation function, evalua-
tion references and evaluation task. This frame-
work enabled us to systematically categorize and
understand LLM-based evaluation methodologies.
We delved into various LLM-based approaches,
scrutinizing their strengths and comparing their dif-
ferences. Additionally, we summarized prevalent
meta-evaluation benchmarks for NLG evaluation.
Throughout our study, we highlighted both the ad-
vancements and the prevailing challenges in this
rapidly evolving field. While LLMs offer ground-
breaking potential in evaluating NLG outputs, there
still remain unresolved issues that require attention,
including bias, robustness, the integration of hy-
brid evaluation methods, and the need for domain-
specific and unified evaluation within LLM-based
evaluators. We anticipate that addressing these
challenges will pave the way for more general, ef-
fective, and reliable NLG evaluation techniques.
Such advancements would contribute significantly
to the progression of both NLG evaluation and the
broader application of LLMs.References
Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia
Fermuller, and Yiannis Aloimonos. 2015. From im-
ages to sentences through scene description graphs
using commonsense reasoning and knowledge. arXiv
preprint arXiv:1511.03292 .
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain chat-
bot. arXiv preprint arXiv:2001.09977 .
Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He,
Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao,
Haozhe Lyu, et al. 2023. Benchmarking foundation
models with language-model-as-an-examiner. arXiv
preprint arXiv:2306.04181 .
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.
2019. Simple, scalable adaptation for neural machine
translation. arXiv preprint arXiv:1909.08478 .
Loïc Barrault, Magdalena Biesialska, Ond ˇrej Bo-
jar, Marta R. Costa-jussà, Christian Federmann,
Yvette Graham, Roman Grundkiewicz, Barry Had-
dow, Matthias Huck, Eric Joanis, Tom Kocmi,
Philipp Koehn, Chi-kiu Lo, Nikola Ljubeši ´c, Christof
Monz, Makoto Morishita, Masaaki Nagata, Toshi-
aki Nakazawa, Santanu Pal, Matt Post, and Marcos
Zampieri. 2020. Findings of the 2020 conference on
machine translation (WMT20). In Proceedings of
the Fifth Conference on Machine Translation , pages
1–55, Online. Association for Computational Linguis-
tics.
Manik Bhandari, Pranav Narayan Gour, Atabak Ash-
faq, Pengfei Liu, and Graham Neubig. 2020. Re-
evaluating evaluation in text summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9347–9359, Online. Association for Computa-
tional Linguistics.
Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh,
Chris van der Lee, Simon Mille, Diego Moussallem,
and Anastasia Shimorina. 2020. The 2020 bilingual,
bi-directional WebNLG+ shared task: Overview and
evaluation results (WebNLG+ 2020). In Proceed-
ings of the 3rd International Workshop on Natu-
ral Language Generation from the Semantic Web
(WebNLG+) , pages 55–76, Dublin, Ireland (Virtual).
Association for Computational Linguistics.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
2020. Evaluation of text generation: A survey. arXiv
preprint arXiv:2006.14799 .
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,
Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan
Liu. 2023. Chateval: Towards better llm-based eval-
uators through multi-agent debate. arXiv preprint
arXiv:2308.07201 .Hong Chen, Duc V o, Hiroya Takamura, Yusuke Miyao,
and Hideki Nakayama. 2022. StoryER: Automatic
story evaluation via ranking, rating and reasoning.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1739–1753, Abu Dhabi, United Arab Emirates. Asso-
ciation for Computational Linguistics.
Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and
Ruifeng Xu. 2023. Exploring the use of large lan-
guage models for reference-free text quality evalua-
tion: A preliminary empirical study. arXiv preprint
arXiv:2304.00723 .
Cheng-Han Chiang and Hung-yi Lee. 2023. Can large
language models be an alternative to human evalua-
tions? In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 15607–15631, Toronto,
Canada. Association for Computational Linguistics.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and
Li Yuan. 2023a. Chatlaw: Open-source legal large
language model with integrated external knowledge
bases. arXiv preprint arXiv:2306.16092 .
Junyun Cui, Xiaoyu Shen, and Shaochun Wen. 2023b.
A survey on legal judgment prediction: Datasets,
metrics, models and challenges. IEEE Access .
Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and
Serge Belongie. 2018. Learning to evaluate image
captioning. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages
5804–5812.
Sören Dreano, Derek Molloy, and Noel Murphy. 2023.
Embed_Llama: Using LLM embeddings for the met-
rics shared task. In Proceedings of the Eighth Con-
ference on Machine Translation , pages 738–745, Sin-
gapore. Association for Computational Linguistics.
Alexander R. Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-
Cann, Caiming Xiong, Richard Socher, and Dragomir
Radev. 2021a. SummEval: Re-evaluating summa-
rization evaluation. Transactions of the Association
for Computational Linguistics , 9:391–409.
Alexander R. Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-
Cann, Caiming Xiong, Richard Socher, and Dragomir
Radev. 2021b. SummEval: Re-evaluating summa-
rization evaluation. Transactions of the Association
for Computational Linguistics , 9:391–409.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.Akhbardeh Farhad, Arkhangorodsky Arkady, Biesialska
Magdalena, Bojar Ond ˇrej, Chatterjee Rajen, Chaud-
hary Vishrav, Marta R Costa-jussa, España-Bonet
Cristina, Fan Angela, Federmann Christian, et al.
2021. Findings of the 2021 conference on machine
translation (wmt21). In Proceedings of the Sixth
Conference on Machine Translation , pages 1–88. As-
sociation for Computational Linguistics.
Markus Freitag, George Foster, David Grangier, Viresh
Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a.
Experts, errors, and context: A large-scale study of
human evaluation for machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 9:1460–1474.
Markus Freitag, David Grangier, and Isaac Caswell.
2020. BLEU might be guilty but references are not
innocent. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 61–71, Online. Association for
Computational Linguistics.
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
Craig Stewart, Eleftherios Avramidis, Tom Kocmi,
George Foster, Alon Lavie, and André F. T. Martins.
2022. Results of WMT22 metrics shared task: Stop
using BLEU – neural metrics are better and more
robust. In Proceedings of the Seventh Conference
on Machine Translation (WMT) , pages 46–68, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
Craig Stewart, George Foster, Alon Lavie, and Ond ˇrej
Bojar. 2021b. Results of the WMT21 metrics shared
task: Evaluating metrics with expert-based human
evaluations on TED and news domain. In Proceed-
ings of the Sixth Conference on Machine Translation ,
pages 733–774, Online. Association for Computa-
tional Linguistics.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. arXiv
preprint arXiv:2302.04166 .
Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-
ing Yang, and Xiaojun Wan. 2023. Human-like sum-
marization evaluation with chatgpt. arXiv preprint
arXiv:2304.02554 .
Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen
Elkind, and Idan Szpektor. 2023. Trueteacher: Learn-
ing factual consistency evaluation with large lan-
guage models. arXiv preprint arXiv:2305.11171 .
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056 .
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019. Samsum corpus: A human-
annotated dialogue dataset for abstractive summa-
rization. arXiv preprint arXiv:1911.12237 .Karthik Gopalakrishnan, Behnam Hedayatnia, Qin-
lang Chen, Anna Gottardi, Sanjeev Kwatra, Anu
Venkatesh, Raefer Gabriel, and Dilek Z. Hakkani-Tür.
2019. Topical-chat: Towards knowledge-grounded
open-domain conversations. ArXiv , abs/2308.11995.
Rupali Goyal, Parteek Kumar, and VP Singh. 2023. A
systematic survey on automated text generation tools
and techniques: application, evaluation, and chal-
lenges. Multimedia Tools and Applications , pages
1–56.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers) , pages 708–719, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-
biao Ding, Xiaoxi Mao, Changjie Fan, and Minlie
Huang. 2021. OpenMEV A: A benchmark for evaluat-
ing open-ended story generation metrics. In Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 6394–6407, Online.
Association for Computational Linguistics.
Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Fe-
lipe Vieira Frujeri, and Ida Momennejad. 2023. Al-
lure: A systematic protocol for auditing and improv-
ing llm-based evaluation of text using iterative in-
context-learning. arXiv preprint arXiv:2309.13701 .
Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. Advances in neural information
processing systems , 28.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of
Artificial Intelligence Research , 47:853–899.
Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is
chatgpt better than human annotators? potential and
limitations of chatgpt in explaining implicit hate
speech. arXiv preprint arXiv:2302.07736 .
Sameer Jain, Vaishakh Keshava, Swarnashree Mysore
Sathyendra, Patrick Fernandes, Pengfei Liu, Gra-
ham Neubig, and Chunting Zhou. 2023. Multi-
dimensional evaluation of text summarization with in-
context learning. arXiv preprint arXiv:2306.01200 .
Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun,
Dongyu Pan, Baochang Ma, and Xiangang Li. 2023.
Exploring chatgpt’s ability to rank content: A prelim-
inary study on consistency with human preferences.
arXiv preprint arXiv:2303.07610 .Qi Jia, Siyu Ren, Yizhu Liu, and Kenny Q Zhu. 2023.
Zero-shot faithfulness evaluation for text summariza-
tion with foundation language model. arXiv preprint
arXiv:2310.11648 .
Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang,
Bill Yuchen Lin, and Wenhu Chen. 2023. Tigerscore:
Towards building explainable metric for all text gen-
eration tasks. arXiv preprint arXiv:2310.00752 .
Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou
Zhao, Wei Zhou, and Liqiang Nie. 2023. Styl-
ized data-to-text generation: A case study in the
e-commerce domain. ACM Transactions on Informa-
tion Systems .
Katharina Kann, Abteen Ebrahimi, Joewie Koh, Shiran
Dudy, and Alessandro Roncone. 2022. Open-domain
dialogue generation: What we can do, cannot do, and
should do next. In Proceedings of the 4th Workshop
on NLP for Conversational AI , pages 148–165.
Sudipta Kar, Suraj Maharjan, A. Pastor López-Monroy,
and Thamar Solorio. 2018. MPST: A corpus of
movie plot synopses with tags. In Proceedings of
the Eleventh International Conference on Language
Resources and Evaluation (LREC 2018) , Miyazaki,
Japan. European Language Resources Association
(ELRA).
Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei,
Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao
Dong, Hongning Wang, et al. 2023. Critiquellm:
Scaling llm-as-critic for effective and explainable
evaluation of large language model generation. arXiv
preprint arXiv:2311.18702 .
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al.
2023a. Prometheus: Inducing fine-grained evalua-
tion capability in language models. arXiv preprint
arXiv:2310.08491 .
Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim,
and Juho Kim. 2023b. Evallm: Interactive evaluation
of large language model prompts on user-defined
criteria. arXiv preprint arXiv:2309.13633 .
Tom Kocmi and Christian Federmann. 2023. Large lan-
guage models are state-of-the-art evaluators of trans-
lation quality. In Proceedings of the 24th Annual
Conference of the European Association for Machine
Translation , pages 193–203, Tampere, Finland. Euro-
pean Association for Machine Translation.
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,
Zae Myung Kim, and Dongyeop Kang. 2023. Bench-
marking cognitive biases in large language models as
evaluators. arXiv preprint arXiv:2309.17012 .
Philippe Laban, Tobias Schnabel, Paul N. Bennett, and
Marti A. Hearst. 2022. SummaC: Re-visiting NLI-
based models for inconsistency detection in summa-
rization. Transactions of the Association for Compu-
tational Linguistics , 10:163–177.Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang
Gao, Rotem Dror, and Steffen Eger. 2023. The
eval4nlp 2023 shared task on prompting large lan-
guage models as explainable metrics. arXiv preprint
arXiv:2310.19792 .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461 .
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
Hai Zhao, and Pengfei Liu. 2023a. Generative
judge for evaluating alignment. arXiv preprint
arXiv:2310.05470 .
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
Hai Zhao, and Pengfei Liu. 2023b. Generative judge
for evaluating alignment. CoRR , abs/2310.05470.
Ruosen Li, Teerth Patel, and Xinya Du. 2023c.
Prd: Peer rank and discussion improve large lan-
guage model based evaluations. arXiv preprint
arXiv:2307.02762 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023d. Alpacaeval: An
automatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, et al. 2022. Holistic evaluation of language
models. arXiv preprint arXiv:2211.09110 .
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval:
Unified multi-dimensional automatic evaluation for
open-domain conversations with large language mod-
els.arXiv preprint arXiv:2305.13711 .
Yupian Lin, Tong Ruan, Jingping Liu, and Haofen Wang.
2023. A survey on neural data-to-text generation.
IEEE Transactions on Knowledge and Data Engi-
neering .
Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .
Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eu-
nah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu
Huang. 2023a. X-eval: Generalizable multi-aspect
text evaluation via augmented instruction tuning
with auxiliary evaluation aspects. arXiv preprint
arXiv:2311.08788 .Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang,
Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yi-
fan Xu, Weng Lam Tam, et al. 2023b. Alignbench:
Benchmarking chinese alignment of large language
models. arXiv preprint arXiv:2311.18743 .
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023c. Gpte-
val: Nlg evaluation using gpt-4 with better human
alignment. arXiv preprint arXiv:2303.16634 .
Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin.
2023d. Llms as narcissistic evaluators: When
ego inflates evaluation scores. arXiv preprint
arXiv:2311.09766 .
Yixin Liu and Pengfei Liu. 2021. SimCLS: A sim-
ple framework for contrastive learning of abstractive
summarization. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers) , pages 1065–1072, Online. Association for
Computational Linguistics.
Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang,
and Hinrich Schütze. 2023e. Evaluate what you can’t
evaluate: Unassessable generated responses quality.
arXiv preprint arXiv:2305.14658 .
Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan
Zhang, Haizhen Huang, Furu Wei, Weiwei Deng,
Feng Sun, and Qi Zhang. 2023f. Calibrating llm-
based evaluator. arXiv preprint arXiv:2309.13308 .
Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and
Dacheng Tao. 2023. Error analysis prompting en-
ables human-like translation evaluation in large lan-
guage models: A case study on chatgpt. arXiv
preprint arXiv:2303.13809 .
Zheheng Luo, Qianqian Xie, and Sophia Ananiadou.
2023. Chatgpt as a factual inconsistency evaluator
for abstractive text summarization. arXiv preprint
arXiv:2303.15621 .
François Mairesse, Milica Gaši ´c, Filip Jur ˇcíˇcek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation
using graphical models and active learning. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 1552–
1561, Uppsala, Sweden. Association for Computa-
tional Linguistics.
Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong
Ma, and Ond ˇrej Bojar. 2020. Results of the WMT20
metrics shared task. In Proceedings of the Fifth Con-
ference on Machine Translation , pages 688–725, On-
line. Association for Computational Linguistics.
Shikib Mehri and Maxine Eskenazi. 2020a. Unsuper-
vised evaluation of interactive dialog with DialoGPT.
InProceedings of the 21th Annual Meeting of the
Special Interest Group on Discourse and Dialogue ,
pages 225–235, 1st virtual meeting. Association for
Computational Linguistics.Shikib Mehri and Maxine Eskenazi. 2020b. USR: An
unsupervised and reference free evaluation metric
for dialog generation. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 681–707, Online. Association for
Computational Linguistics.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint
arXiv:2305.14251 .
OpenAI. 2023. Gpt-4 technical report.
Phil Ostheimer, Mayank Nagda, Marius Kloft, and
Sophie Fellenz. 2023. Text style transfer evalua-
tion using large language models. arXiv preprint
arXiv:2308.13577 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstrac-
tive summarization with FRANK: A benchmark for
factuality metrics. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 4812–4829, Online. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei
Liu. 2022. T5score: Discriminative fine-tuning of
generative evaluation metrics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685–2702, Online. Association
for Computational Linguistics.
Ananya B Sai, Akash Kumar Mohankumar, and
Mitesh M Khapra. 2022. A survey of evaluation met-
rics used for nlg systems. ACM Computing Surveys
(CSUR) , 55(2):1–39.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.
BLEURT: Learning robust metrics for text genera-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
7881–7892, Online. Association for Computational
Linguistics.Yuchen Shen and Xiaojun Wan. 2023. Opinsummeval:
Revisiting automated evaluation for opinion summa-
rization. arXiv preprint arXiv:2310.18122 .
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2021. Societal biases in language
generation: Progress and challenges. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 4275–4293.
Significant Gravitas. AutoGPT.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-
davi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,
et al. 2023. Large language models encode clinical
knowledge. Nature , 620(7972):172–180.
Ondrej Skopek, Rahul Aralikatte, Sian Gooding, and
Victor Carbune. 2023. Towards better evaluation of
instruction-following: A case-study in summariza-
tion. arXiv preprint arXiv:2310.08394 .
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas:
Technical Papers , pages 223–231.
Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan.
2023. Evaluation metrics in the era of gpt-4: Reli-
ably evaluating large language models on sequence
to sequence tasks. arXiv preprint arXiv:2310.13800 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang,
Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao,
and Furu Wei. 2023. Not all metrics are guilty: Im-
proving nlg evaluation with llm paraphrasing. arXiv
preprint arXiv:2305.15067 .
Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui
Yan. 2018. Ruber: An unsupervised method for au-
tomatic evaluation of open-domain dialog systems.
InProceedings of the AAAI conference on artificial
intelligence , volume 32.
Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf.
2022. Zerocap: Zero-shot image-to-text generation
for visual-semantic arithmetic. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 17918–17928.
Brian Thompson and Matt Post. 2020. Automatic ma-
chine translation evaluation in many languages via
zero-shot paraphrasing. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 90–121, Online.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 4566–4575.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. 2016. Show and tell: Lessons learned
from the 2015 mscoco image captioning challenge.
IEEE transactions on pattern analysis and machine
intelligence , 39(4):652–663.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020a.
Asking and answering questions to evaluate the fac-
tual consistency of summaries. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 5008–5020, Online. Asso-
ciation for Computational Linguistics.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020b.
Asking and answering questions to evaluate the fac-
tual consistency of summaries. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 5008–5020, Online. Asso-
ciation for Computational Linguistics.
Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng
Yang, Andrew Cohen, Lei Li, and Yuandong Tian.
2023a. Learning personalized story evaluation.
arXiv preprint arXiv:2310.03304 .
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang
Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.
2023b. Is chatgpt a good nlg evaluator? a preliminary
study. arXiv preprint arXiv:2303.04048 .
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai
Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.
2023c. Large language models are not fair evaluators.
arXiv preprint arXiv:2305.17926 .
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean
O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,
Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-
Zarandi, and Asli Celikyilmaz. 2023d. Shepherd: A
critic for language model generation. arXiv preprint
arXiv:2308.04592 .
Yaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng
Li, Yi Liang, Qiaozhu Mei, and Michael Bender-
sky. 2023e. Automated evaluation of personalized
text generation using large language models. arXiv
preprint arXiv:2310.11593 .Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi
Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
Rui Xie, Jindong Wang, Xing Xie, et al. 2023f.
Pandalm: An automatic evaluation benchmark for
llm instruction tuning optimization. arXiv preprint
arXiv:2306.05087 .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023g. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 13484–13508, Toronto, Canada. Association
for Computational Linguistics.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022a. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 .
OpenReview.net.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022b. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Tsung-Hsien Wen, Milica Gaši ´c, Nikola Mrkši ´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing , pages
1711–1721, Lisbon, Portugal. Association for Com-
putational Linguistics.
Ning Wu, Ming Gong, Linjun Shou, Shining Liang,
and Daxin Jiang. 2023a. Large language models are
diverse role-players for summarization evaluation.
arXiv preprint arXiv:2303.15078 .
Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita
Bhutani, and Estevam Hruschka. 2023b. Less is
more for long document summary evaluation by llms.
arXiv preprint arXiv:2309.07382 .
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao
Song, Markus Freitag, William Yang Wang, and
Lei Li. 2023. Instructscore: Towards explainable
text generation evaluation with automatic feedback.
arXiv preprint arXiv:2305.14282 .
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,
Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao,
and Tianyi Zhou. 2024. A survey on knowledge dis-
tillation of large language models. arXiv preprint
arXiv:2402.13116 .
Hongyang Yang, Xiao-Yang Liu, and Christina Dan
Wang. 2023a. Fingpt: Open-source financial large
language models. arXiv preprint arXiv:2306.06031 .Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong
Tian. 2023b. DOC: Improving long story coherence
with detailed outline control. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
3378–3465, Toronto, Canada. Association for Com-
putational Linguistics.
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan
Klein. 2022. Re3: Generating longer stories with
recursive reprompting and revision. arXiv preprint
arXiv:2210.06774 .
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text genera-
tion. In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual , pages 27263–27277.
Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,
and Huan Sun. 2023. Automatic evaluation of at-
tribution by large language models. arXiv preprint
arXiv:2305.06311 .
Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya
Goyal, and Danqi Chen. 2023. Evaluating large
language models at evaluating instruction following.
arXiv preprint arXiv:2310.07641 .
Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a.
Summit: Iterative text summarization via chatgpt.
arXiv preprint arXiv:2305.14835 .
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you have
pets too? arXiv preprint arXiv:1801.07243 .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-
ating text generation with BERT. In 8th International
Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.
Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv,
Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin
Li. 2023b. Wider and deeper llm networks are fairer
llm evaluators. arXiv preprint arXiv:2308.01862 .
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M. Meyer, and Steffen Eger. 2019. MoverScore:
Text generation evaluating with contextualized em-
beddings and earth mover distance. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 563–578, Hong
Kong, China. Association for Computational Lin-
guistics.
Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan,
Xiangru Tang, and Arman Cohan. 2023. Investigat-
ing table-to-text generation capabilities of llms in
real-world information seeking scenarios.Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .
Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan
Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui
Xu, and Tong Sun. 2022. Towards language-free
training for text-to-image generation. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 17907–17917.
Terry Yue Zhuo. 2023. Large language models are
state-of-the-art evaluators of code generation. arXiv
preprint arXiv:2304.14317 .