Do Not Worry if You Do Not Have Data:
Building Pretrained Language Models Using Translationese
Meet Doshi*, Raj Dabre**, and Pushpak Bhattacharyya*
*CFILT, Indian Institute of Technology Bombay
**NICT, Japan
*{meetdoshi,pb}@cse.iitb.ac.in
**{prajdabre}@gmail.com
Abstract
In this paper, we explore the utility of Transla-
tionese as synthetic data created using machine
translation for pre-training language models
(LMs). Pre-training requires vast amounts of
monolingual data, which is mostly unavailable
for languages other than English. Recently,
there has been a growing interest in using syn-
thetic data to address this data scarcity. We take
the case of English and Indic languages and
translate web-crawled monolingual documents
(clean) into the target language. Then, we train
language models containing 28M and 85M pa-
rameters on this translationese data (synthetic).
We show that their performance on downstream
natural language understanding and generative
tasks is only 3.56% poorer on NLU tasks and
1.51% on NLG tasks than LMs pre-trained on
clean data. Further, we propose the use of
lightweight TinyLMs pre-trained on clean data
to filter synthetic data efficiently which signifi-
cantly improves the performance of our models.
We also find that LMs trained on synthetic data
strongly benefit from extended pretraining on
a tiny fraction (10%) of clean data. We release
the data we collected and created as a part of
this work, IndicMonoDoc , the largest collection
of monolingual document-level corpora, which
we hope will help bridge the gap between En-
glish and non-English performance for large
language models.
1 Introduction
Large language models (LLMs)(Brown et al., 2020;
Workshop et al., 2022; Almazrouei et al., 2023;
Lin et al., 2022) have been able to perform very
well on downstream tasks like MMLU (Hendrycks
et al., 2021), Big-Bench (Srivastava et al., 2022),
etc, and have even started to reach human potential
in many of these tasks. But this performance has
very largely been credited to their scale and the
vast amount of data that they have been fed. Most
of these language models (LMs) perform well in
languages like English where abundant data is avail-
Table 1LanguageEnglishHindiGujaratiClean76.8777.6079.95Synthetic Unﬁltered71.8474.2677.65Synthetic Unﬁltered + 10%73.7875.6778.47Synthetic Filtered74.6076.6379.55Synthetic Filtered + 10%75.8377.5280.23Performance comparison of Translationese text on NLU tasksAvg. across tasks7173.57678.581
Training DataCleanSynthetic UnﬁlteredSynthetic Unﬁltered + 10%Synthetic
FilteredSynthetic
Filtered + 10%
EnglishHindiGujarati
1Figure 1: Comparison of NLU performance between
language models trained on clean data and synthetic
translationese data in English, Hindi, and Gujarati shows
that filtering synthetic text and extending training on a
clean subset (10%) can bring the performance of models
trained on synthetic data closer to LMs trained only on
clean data extracted from the web.
able (Kudugunta et al., 2023), but a vast majority
of languages don’t have comparable data as com-
pared to English. As a consequence, many LLMs,
both monolingual and multilingual, involving these
languages still show poor performance for vari-
ous downstream tasks. For example, the largest
open source multilingual model BLOOM (Work-
shop et al., 2022) covers 46 natural languages span-
ning 9 language families, but the top 5 languages
comprise 74.14% of the data. Despite the benefits
of multilingualism (Dabre et al., 2020), this skew
in data still means that the low-resource languages
will not perform well.
Fortunately synthetic data is an option and pre-
vious works such as, but not limited to, back-
translation (Sennrich et al., 2016a), sequence dis-
tillation (Kim and Rush, 2016), also known as
forward translation, etc. have shown that syn-
thetic data obtained using machine translation (MT)
can supplement resource scarcity and can signifi-
cantly enhance model performance (Popovi ´c et al.,arXiv:2403.13638v2  [cs.CL]  21 Mar 20242020; Gala et al., 2023). However, to the best of
our knowledge, there has been no work on show-
ing the effectiveness of synthetic data for train-
ing LMs. Furthermore, the quality of synthetic
data is also important, which many works take for
granted. While there are options such as round-
trip-translation (Moon et al., 2020) or referenceless
neural quality estimation (QE) (Rei et al., 2021),
they either involve twice the compute or a reason-
ably large model not available for most languages,
and this might not be optimal to determine the qual-
ity of synthetic documents efficiently. We thus
consider TinyLMs (Eldan and Li, 2023) as an effi-
cient alternative, which have been shown to model
documents by their fluent paragraph generation ca-
pabilities.
In this paper, we focus on English and Indic
languages and present a comprehensive study of
the utility of synthetic, translationese (Gellerstam,
1986), monolingual data obtained using machine
translation (MT) for training LMs. We propose a
simple framework that involves training small lan-
guage models, TinyLMs, on original web-crawled
data (clean) and then using them to filter synthetic
data. We then compare LMs trained on clean and
synthetic data followed by fine-tuning on down-
stream tasks, where we observe that, unfiltered syn-
thetic data LMs perform slightly poor compared
to LMs trained on clean data, but after filtering, it
matches its performance. We also show that tuning
these synthetic data LMs on small clean data leads
to further improvements.
Our contributions are:
a.A simple framework involving high-quality MT
models and TinyLMs trained on clean web-crawled
data to mass-produce and filter synthetic data for
LM training.
b.Demonstrating the efficacy of language models
trained on filtered synthetic data across a range of
downstream tasks in natural language understand-
ing and generation.
c.A new document-level monolingual corpora ( In-
dicMonoDoc ) consisting of 39.5B tokens worth of
monolingual clean document-level data spanning
22 scheduled languages and English. This dataset,
which is 2 times larger than IndicCorpV2 (Dodda-
paneni et al., 2023), will be made public alongside
our models and code12.
1https://github.com/meetdoshi90/TinyLM/
2https://huggingface.co/datasets/cfilt/
IITB-MonoDoc2 Related Work
This paper focuses on creating, filtering, and utiliz-
ing synthetic data for training TinyLMs.
Monolingual Data: Previous efforts of collect-
ing monolingual corpora for Indic languages in-
clude but not limited to the EMILLE/CIIL cor-
pus (McEnery et al., 2000), HindMonoCorp (Bo-
jar et al., 2014), Leipzig corpus (Goldhahn et al.,
2012), IndicCorpv1 (Kakwani et al., 2020) and
more recently IndicCorpv2 (Doddapaneni et al.,
2023) which shows significant increase in size com-
pared to previous Indic corpora. Although the size
of IndicCorpv2 is large, it is a sentence-level cor-
pora that can be used to train NLU models but
language models with their increase in size can
be trained on longer contexts that require long se-
quences of documents. In this work, we build upon
existing monolingual corpora for Indic languages
and also show the efficacy of using synthetic data.
Synthetic Data Generation and Quality Estima-
tion: Synthetic data is valuable for NLP tasks like
using back translations (Sennrich et al., 2016a),
(Edunov et al., 2018) to enhance Machine transla-
tion (Marie et al., 2020), (Bogoychev and Sennrich,
2019), (Ni et al., 2022), or for tasks such as na-
tive language identification (Goldin et al., 2018).
However, there’s a limited exploration of using
synthetic data for pretraining LMs due to issues
like hallucination (Maynez et al., 2020), and un-
grounded non-factual text (Thorne et al., 2018).
Machine translation can help mitigate these issues,
but translation errors may still occur, impacting
text quality. Evaluating synthetic text quality with
round-trip-translation (RTT) BLEU scores requires
twice the compute and is prone to errors. Alter-
native evaluation methods like BARTScore (Yuan
et al., 2021), T5Score (Qin et al., 2023), MQM
& COMET (Rei et al., 2020) require large-scale
models or human annotations, limiting scalabil-
ity. Approaches like KenLM (Heafield, 2011) have
been used to filter monolingual corpora based on
perplexity. We take inspiration from this and show
that efficient TinyLM with just 28M parameters
can be used to filter synthetic text.
TinyLMs: Language models as small as 10M pa-
rameters have been shown to produce fluent and
consistent stories with almost perfect grammar (El-
dan and Li, 2023) which means LMs even at a small
scale have language understanding. Challenges like
BabyLM (Warstadt et al., 2023) focus on improv-
ing LMs with a fixed data budget which enablesFigure 2: Overview of our approach to pre-train language models using translationese data. We leverage rich
monolingual corpora in the srclanguage and scarce corpora in the tgtlanguage. Our method involves employing a
pre-trained machine translation model to translate srctotgt. We then filter the resulting text using a TinyLM trained
solely on clean tgtmonolingual data for perplexity. The filtered synthetic data can be used to further pretrain larger
language models.
exhaustive study of LM development methodolo-
gies, which can then be applied to larger LMs. We
aim to utilize lightweight TinyLMs for efficient
filtering of synthetic documents.
3 Methodology
In this section, we describe our framework for lever-
aging synthetic data for LM training. This con-
sists of monolingual data curation from the web
(clean), training a TinyLM with it, translation of
clean data, using the aforementioned TinyLM to
filter synthetic data, and then using this filtered data
for training a larger LM to be used for downstream
tasks. Our framework is described in Figure 2.
3.1 Monolingual Data
Web Crawled (Clean): Following Doddapaneni
et al. (2023); Rae et al. (2022); Team et al. (2022),
for all languages of interest, we a.obtain a list of
URLs to be crawled via word-level n-grams passed
to a search engine, b.after URL deduplication, we
crawl all applicable webpages, c.automatically
and manually (Ortiz Suárez et al., 2019; Abadji
et al., 2022) filter out unwanted text like HTML
tags and emoticons, d.use language detection-
based (LID) filtering using cld33and IndicLID-
FTN model (Madhani et al., 2023a) to discard lan-
guages not of interest, e.perform document fil-
tering to remove offensive text using toxic words
list provided by Team et al. (2022), f.merge all
3https://github.com/google/cld3
Figure 3: Language-wise corpora size comparison with
IndicCorpv2 (Doddapaneni et al., 2023): Stacked Bars
the filtered corpus with Wikipedia, OSCAR (Ortiz
Suárez et al., 2019) and some dumps of mC4 (Xue
et al., 2021) and finally, g.perform deduplication
at paragraph level using Murmurhash algorithm4
with a 128-bit unsigned hash for each monolingual
split of the corpora.
Translationese (Synthetic) : We utilize state-of-
the-art MT models like IndicTrans2 (Gala et al.,
2023) to generate translationese data. We use beam
search with a beam value of 5 to translate English
tokens from the aforementioned crawled corpus to
the languages of interest. Most MT models have a
4https://pypi.org/project/mmh3/Figure 4: The plot illustrates TinyLM’s perplexity mean and variance across various datasets: Clean-EN (left),
Syn-EN from filtered Hindi (middle), and Syn-EN from filtered Gujarati (right). Despite filtering, English documents
generated from translating Gujarati show consistently higher variance.
maximum token limit, and thus we split the docu-
ments using Moses Sentence Splitter5to perform
translations into the target language at the sentence
level and then merge again to form documents. Our
experiments also focus on synthetic English data
translated from Hindi and Gujarati.
3.2 Tiny Language Models (TinyLMs)
TinyLMs are simply tiny versions of language mod-
els inspired by Eldan and Li (2023). We follow
the Transformer architecture (Vaswani et al., 2017)
used by Eldan and Li (2023) and train it using only
clean monolingual documents. Instead of using
learned positional encodings, we use RoPE em-
beddings (Su et al., 2023) for better extrapolation
to longer documents. We rely on Chinchilla scal-
ing laws Hoffmann et al. (2022) and use compute
optimal word tokens to train our models.
3.3 Synthetic Data Filtering
We first train TinyLMs on crawled data and then
use them to compute perplexities on our synthetic
documents ( W∈w1, w2, . . . , w N) using the equa-
tion:
PPL( W) = exp(
−1
NNX
ilogpθ(wi|w<i))
We also use our TinyLM to repair sentences in doc-
uments that exceed the maximum length of the MT
models, but this happens in only 0.002% of such
cases. We provide more details of our approach in
Appendix C.
3.4 Final LM and Downstream Task Training
Using the filtered synthetic corpora, we train our
final LMs which are comparatively larger, and
5https://pypi.org/project/mosestokenizer/fine-tune them for natural language understand-
ing (NLU) tasks such as IndicGLUE (Kakwani
et al., 2020), GLUE (Wang et al., 2018) and genera-
tion (NLG) benchmarks such as IndicNLG (Kumar
et al., 2022), summarization tasks (Nallapati et al.,
2016), (Chen et al., 2021) and machine translation
benchmarks (Team et al., 2022), (Gala et al., 2023).
4 IndicMonoDoc
Following the monolingual data curation strategy
in Section 3, we crawl data for English and 22 Indic
languages. As a result, we end up with IndicMon-
oDoc, with 27.5 billion tokens of Indic documents
and 12 billion tokens of English documents for a
total of 39.5 billion tokens of clean monolingual
data. This is the largest ever for Indic languages,
even surpassing (Doddapaneni et al., 2023) by 2
times. We use IndicMonoDoc for all clean parts
of our experiments. We report additional details of
IndicMonoDoc in Appendix D.
4.1 Analysis of Crawled Corpora
Figure 3 gives an overview of the comparison of
IndicMonoDoc which is a document-level corpus
with that of IndicCorpV2 which is a sentence-level
corpus. It is important to note that we paid special
attention to the low-resource languages.
4.2 Analysis of Synthetic Data
We use IndicMonoDoc for the clean part of our
experiments and translate parts of it for the syn-
thetic experiments. Figure 4 shows the perplexity
mean and variance scores for TinyLM across token
positions in the documents. This shows that on
unseen documents, TinyLM shows higher variance
on English documents generated by translating Gu-
jarati documents from IndicMonoDoc as compared
to English clean and English synthetic generatedFigure 5: Violin plot displaying the distribution of
lengths of clean and filtered English documents on dif-
ferent data splits: en-clean (English web documents),
syn-en_hi (synthetic English documents translated from
Hindi), and syn-en_gu (synthetic English documents
translated from Gujarati).
from Hindi. This also gives reason for the deteriora-
tion in results in Table 4 due to Gujarati documents.
Figure 5 shows the distribution of lengths of fil-
tered documents by TinyLMs showing that they
do not add any bias for shorter documents during
filtering. Although we do not experiment with all
these languages, we believe that IndicMonoDoc
will be an invaluable resource for Indic LMs.
5 Experiments
In this section, we describe the training procedure
and datasets used for different models mentioned
in Section 3. We pretrain and fine-tune all of the
mentioned models from scratch in both mono and
bilingual settings using a Causal Language Model-
ing (CLM) objective on NLG tasks and use a Lin-
ear classification head for all classification tasks.
We specify the sample of the dataset used for pre-
training and finetuning for each model and see the
different effects of using synthetic corpora for pre-
training.
5.1 Pretraining Data Settings
We refer to translated text or translationese as syn-
thetic orsynand original or web-crawled data as
clean throughout our experiments. For the pre-
training of all base models, we use the following
naming convention to denote our training splits for
each model:
XX-clean: This is a clean subset sampled randomly
from IndicMonoDoc where XX represents the lan-
guage English (EN), Hindi (HI) or Gujarati (GU).syn-XX_yy-unfiltered: Denotes synthetic mono-
lingual documents in XX language generated by
using yy as source during translation.
syn-XX_yy-filtered: Filtered synthetic data.
+10%: Refers to extended pretraining on a cleaned
subset of IndicMonoDoc with an additional 10%
tokens compared to regular training.
BI-XX-YY Prefix: Denotes bilingual models
trained using an equal mixture of monolingual cor-
pora in XX and YY languages. We append an
_syn prefix to either XX or YY if a synthetic ver-
sion of that language is employed in training and a
-parallel/nonparallel tag to denote whether a paral-
lel version of XX and YY are used or not.
Note, for each split we only use the number of
tokens that are required to reach the point of op-
timality (Hoffmann et al., 2022) by the language
model. We mention other details in Appendix B.
5.2 Implementation and Training
Tokenizer : We use a common byte-pair-encoding
(BPE) (Sennrich et al., 2016b) tokenizer using Sen-
tencepiece6for all experiments. We train a shared
vocabulary of 56k subwords between three lan-
guages, English, Hindi, and Gujarati by using 5
Million randomly sampled sentences per language
and upsampling for Gujarati.
TinyLMs: We use Pytorch Lightning7for our im-
plementations and train TinyLMs as described in
Section 3.2. We use hidden sizes of 768 and have
two variants, one with 4 layers ( mini) and one with
12 layers ( base; same as GPT2-base) with 28M
and 85M non-embedding parameters respectively.
The mini models are trained on clean data with
sequence lengths of 40968(mini-4k ) for filtering
synthetic documents as described in Section 3.3.
On the other hand, for our main pre-training and
downstream fine-tuning experiments, we train mini
andbase models with sequence lengths of 1024
(mini-1k andbase-1k ). Following Hoffmann et al.
(2022) we use 2.4 billion word tokens per language
to compute optimal training of base models. Since
Gujarati has only 900M tokens in our dataset, when-
ever Gujarati is involved, we train only the mini-1k
model. For models involving English and Hindi,
we train both mini andbase models. Additional
details are in Appendix B.
6https://github.com/google/sentencepiece
7https://lightning.ai/docs/pytorch/stable/
8We keep long sequence lengths to be able to handle long
documents for filtering.(a) Results on Hindi
NLU NLG
ModeliXNLI bbc-a iitp-mr iitp-pr midas Avg.Headline
Gen.Sentence
Summ.Question
Gen.Wikibio Avg.
HI-clean 73.61 81.75 72.58 79.73 80.34 77.60 27.54 23.64 24.84 52.16 32.04
syn-HI_en-unfiltered 72.87 77.92 64.36 76.22 79.91 74.26 27.29 22.93 24.22 50.14 31.14
syn-HI_en-unfiltered+10% 74.63 78.36 67.75 77.46 80.17 75.67 - - - - -
syn-HI_en-filtered 74.75 81.06 69.03 78.58 79.73 76.63 27.15 23.10 24.41 49.88 31.13
syn-HI_en-filtered+10% 74.49 80.94 71.61 79.92 80.64 77.52 - - - - -
(b) Results on Gujarati
NLU NLG
ModeliXNLI iNLTK Avg.Headline
Gen.Sentence
Summ.Question
Gen.Avg.
GU-clean 67.8 92.1 79.95 17.62 13.82 15.18 15.54
syn-GU_en-unfiltered 65.51 89.78 77.65 16.21 13.29 13.66 14.39
syn-GU_en-unfiltered+10% 66.83 90.11 78.47 - - - -
syn-GU_en-filtered 67.74 91.35 79.55 17.64 13.40 14.95 15.33
syn-GU_en-filtered+10% 68.04 92.41 80.23 - - - -
Table 1: Results for Hindi and Gujarati: NLU/NLG tasks on base-1k (Hindi) and mini-1k (Gujarati) models on
different clean and synthetic splits. Test accuracy for NLU tasks; Rouge-L F1 scores for NLG tasks. Bold values
represent the best amongst synthetic splits.
5.3 Downstream Tasks and Evaluation
We finetune the mini-1k andbase-1k models for
various classification, regression, and generation
tasks. We do some hyperparameter tuning for each
task and then repeat them for different data splits.
More hyperparameters and evaluation details can
be found in Appendix B. For evaluations, we re-
port our primary scores on IndicGLUE (Kakwani
et al., 2020) and IndicXNLI (iXNLI) (Aggarwal
et al., 2022) for Hindi and Gujarati and use the
validation set of GLUE benchmark (Wang et al.,
2018) for English. We also experiment with other
generation tasks like CNN-Dailymail (Nallapati
et al., 2016), DailogSum (Chen et al., 2021), XL-
Sum (Hasan et al., 2021), IndicNLG9(Kumar et al.,
2022), FLoRes-200 (Team et al., 2022), IN22-Conv
& IN22-Gen (Gala et al., 2023) and use standard
evaluation metrics suitable for each task like ac-
curacy, f1-score, Rouge-L (Lin, 2004) and chrF++
(Popovi ´c, 2017).
6 Results
We now present our results which help establish
the utility of synthetic data for language modeling.
6.1 Main Results
In this section, we present results for Hindi, Gu-
jarati, and English language models trained on
clean data, as well as synthetic data generated from
9We only take first 4k examples of IndicNLG test split for
each task due the large test split of IndicNLGtranslations. We demonstrate the impact of filter-
ing and adding additional clean data for extended
pretraining of LMs trained solely on synthetic text.
Additionally, we observe the effect of using the
clean source text along with its translations (syn-
thetic parallel documents) on downstream tasks.
We follow the naming convention for different data
splits as specified in Section 5. We provide details
for pretraining of each model in Appendix B.
Filtered Synthetic Data is Competitive with
Web Scraped Data: The results in Table 1 and
2 indicate that syn-HI_en-unfiltered ,syn-GU_en-
unfiltered , and syn-EN_hi-unfiltered exhibit lower
downstream performance compared to their fil-
tered counterparts: syn-HI_en-filtered ,syn-GU_en-
filtered , and syn-EN_hi-filtered , respectively. It is
evident that filtering the synthetic documents using
TinyLMs significantly improves the performance of
both NLU and NLG tasks. In Table 2, we observe
that for tasks like CoLA (Warstadt et al., 2019),
language models trained solely on synthetic data
lag behind when compared to other tasks. This
suggests that synthetic corpora may lack certain
important elements necessary for language models
to perform competitively in linguistic acceptability
tasks, as opposed to LMs trained on clean, non-
synthetic corpora .
Fine-tuning on Web Scraped Data boosts per-
formance: Even after filtering, we observe that
language models trained solely on synthetic text
slightly underperform LMs trained on clean text.
To address this issue, we conduct extended pretrain-sst2 cola mrpc qnli qqp rte mnli-m mnli-mm stsbModelacc mcc f1 acc f1 acc acc acc pearsonAvg.
EN-clean 90.94 40.26 87.4 84.98 84.47 65.34 77.84 77.96 82.67 76.87
syn-EN_hi-unfiltered 84.61 31.1 81.78 79.35 81.44 63.3 72.94 73.16 78.9 71.84
syn-EN_hi-unfiltered + 10% 87.39 34.22 85.77 80.96 81.07 65.11 74.76 74.38 80.32 73.78
syn-EN_hi-filtered 88.3 34.03 86.55 83.59 83.64 63.17 75.6 75.41 81.1 74.60Mono
syn-EN_hi-filtered + 10% 90.13 35.75 86.41 84.75 84.21 65.34 76.99 76.91 81.95 75.83
BI-EN-HI-clean 89.56 38.53 85.56 84.88 84.39 64.25 76.4 77.27 82.07 75.88
BI-EN-HI_syn-parallel-filtered 89.56 39.57 85.71 84.75 84.62 64.98 77.31 77.85 82.41 76.31
BI-EN-HI_syn-nonparallel-filtered 89.79 38.68 86.92 85.08 84.06 65.34 77.15 77.55 83.01 76.40
BI-EN_syn-HI_syn-filtered 87.95 30.05 84.9 83.7 83.97 63.89 75.63 76.24 82.24 74.29Bi
BI-EN_syn-HI_syn-filtered + 10% 89.1 35.45 85.34 84.53 84.18 65.7 76.64 77.24 82.1 75.59
Table 2: Results on English: Dev set of GLUE tasks for different synthetic splits on the base-1k model. Synthetic
LMs perform almost as well as clean LMs after filtering and further training with clean data. Bold values represent
the best amongst synthetic splits.
ing of LMs using clean data sourced from Indic-
MonoDoc. The objective is to determine if this
additional training improves performance. We only
incorporate an additional 10% of clean data com-
pared to the LM’s previous training data. We see
these results across all three languages, and for
Hindi and Gujarati, we see that by incorporating
even a small amount of clean data, we observe
an increase in performance on downstream tasks,
bringing the LM at par or closer to the performance
of a clean LM . We see an improvement in LMs
trained using unfiltered synthetic corpora as well
but we believe that filtering leads to the removal of
noisy documents and thus better performance.
Model iXNLI bbc-a iitp-mr iitp-pr midas Avg.
HI-clean 68.74 80.25 67.74 77.05 78.33 74.42
syn-HI_en-unfiltered 67.32 77.92 65.63 76.81 77.58 73.05
syn-HI_en-filtered 69.48 78.98 65.16 77.43 77.33 73.68
syn-HI_en-filtered+10% 70.15 79.56 67.09 78.2 79.03 74.81
Table 3: Effect of reducing model size for Hindi on
IndicGLUE accuracy. All the results reported here are
onmini-1k .Bold values represent the best amongst
synthetic splits
Using synthetic for one language doesn’t impact
performance in another: For many multilingual
language models, data imbalance causes a gap in
performance across languages. But what if we can
combine synthetic data along with clean data for
training multilingual models, would the synthetic
part deteriorate the performance of the multilingual
model? To experiment with this, we train bilin-
gual base-1k models over different combinations of
clean and synthetic corpora for English and Hindi
and evaluate their performance on GLUE (Wang
et al., 2018), and report performance on IndicNLG,
and Machine translation in Appendix A. Follow-
ing Table 2, we see that using Hindi synthetic data
does not affect its performance compared to BI-EN-HI-clean model which is solely trained on clean
corpora. This implies that it is possible to train
multilingual models where some languages are
trained only over a clean subset and others on
synthetic without deteriorating performance across
languages . We further see that using parallel data
does not have much impact on multilingual models .
6.2 Further Exploration
Impact of source language for synthetic data
generation: Choosing the right source language
for synthetic corpora is crucial as it influences the
characteristics of the generated translationese text.
We evaluate this using Hindi and Gujarati clean
documents from IndicMonoDoc , translating them
into English. Since Gujarati has limited data (900M
tokens), we train a mini-1k model for fair compar-
ison. In Table 4, we see that the synthetic text
generated from Hindi achieves performance at par
with the EN-clean model, while the synthetic text
from Gujarati significantly lags behind . This is
likely because Hindi is more macaronic than Gu-
jarati, i.e., a lot of Hindi text from the web consists
ofHinglish , resulting in better translationese text
due to increased overlap between languages. This
can also be due to the weaker translations generated
by the MT model. The performance gap is notable
in tasks like STS benchmark, NLI (qnli and mnli),
and CoLA, suggesting poorer translation quality
from Gu →En compared to Hi →En.
Impact of model size: Following Table 4 and 3, we
see that even after scaling down we see consistent
improvements for filtering and adding additional
data, which empirically shows that indeed using
synthetic text after filtering is a viable option for
pretraining LMs of varying sizes . In Table 3 we
see that after filtering and extended pretraining,
synthetic text outperforms LMs trained on cleansst2 cola mrpc qnli qqp rte mnli-m mnli-mm stsbModelacc mcc f1 acc f1 acc acc acc pearsonAvg.
Original EN-clean 87.95 25.59 83.84 78.83 80.78 64.62 71.6 71.69 73.48 70.93
syn-EN_hi-unfiltered 87.53 19.77 79.02 76.49 77.96 55.4 69.65 70.14 67.37 67.04
syn-EN_hi-filtered 87.61 22.81 81.95 77.63 80.57 56.31 70.19 70.89 69.29 68.58Translationese
Hi->Ensyn-EN_hi-filtered + 10% 87.84 26.61 83.27 78.5 80.36 61.37 71.29 71.11 71.91 70.25
syn-EN_gu-unfiltered 83.11 17.66 78.53 66.01 77.68 53.6 63.21 64.55 27.33 59.08
syn-EN_gu-filtered 85.66 21.15 81.45 66.35 77.36 54.15 66.27 65.72 26.16 60.47Translationese
Gu->Ensyn-EN_gu-filtered + 10% 86.58 25.17 81.67 67.1 77.75 57.76 68.78 68.56 27.54 62.32
Table 4: Effect of source selection for generating synthetic data on the dev set of GLUE benchmark. All the results
reported here are on mini-1k .Bold values represent the best amongst synthetic splits
documents from the web in Hindi.
ModelXLSum
HGXLSum
QGCnn Dialogsum Avg.
EN-clean 23.87 24.05 16.08 20.39 21.10
syn-EN_hi-unfiltered 22.17 22.97 12.56 18.30 19.00
syn-EN_hi-filtered 23.27 23.83 15.88 19.83 20.70
Table 5: Performance of English models on NLG tasks.
All the results reported here are on base-1k and use
Rouge-L F1 scores.
Impact on NLG: Without extended pretraining,
language models trained on synthetic text perform
as well as those trained on clean documents, sug-
gesting that for NLG tasks, synthetic data suffices
for pretraining, eliminating the need for clean data .
This trend is evident across Hindi, Gujarati, and
English NLG results (Tables 1 and 5). As their per-
formance matches models trained on clean data, we
refrain from extended pretraining for NLG tasks,
focusing primarily on abstractive summarization
for evaluating generation capabilities.
FLORESModelEN-HI HI-EN Avg.
BI-EN-HI-clean 46.56 51.7 49.13
BI-EN-HI_syn-parallel-filtered 44.12 50.64 47.38
BI-EN-HI_syn-nonparallel-filtered 45.65 51.29 48.47
EN-GU GU-EN Avg.
BI-EN-GU-clean 26.44 35.3 30.87
BI-EN-GU_syn-parallel-filtered 26.77 34.84 30.81
BI-EN-GU_syn-nonparallel-filtered 26.7 36.54 31.62
Table 6: chrF++ Scores on FLoRes translation task. EN-
HI models are based on base-1k and EN-GU models are
based on mini-1k
Impact on Machine Translation: (MT) We fo-
cus on MT separately as a special case of NLG.
We hypothesized that using parallel synthetic docu-
ments for bilingual models would improve transla-
tion performance by enhancing alignment between
languages. However, our evaluation fails this hy-
pothesis. Results indicate that using nonparallel
synthetic documents yields similar translation per-
formance across language directions and bench-marks compared to parallel synthetic documents .
This might be because there is no explicit align-
ment happening during training between parallel
documents. See Table 6 for chrF++ scores on
FLoRes-200 (Team et al., 2022), and Appendix
A for chrF++ and BLEU scores on IN22-Conv,
IN22-Gen (Gala et al., 2023).
7 Conclusion
In this paper, we performed a first of its kind study
of the feasibility of using translationese data for
training language models. We proposed a simple
pipeline involving the translation of documents at
scale followed by filtering using small and effi-
cient language models trained on clean data. We
then showed on a variety of downstream natural
language understanding and generative tasks that
language models trained on unclean synthetic data
were only slightly inferior to those trained on origi-
nal data, however, filtered synthetic data with ex-
tended pretraining on clean data mostly eliminates
this gap. We also observed a positive impact of syn-
thetic data on TinyLMs fine-tuned on 10% clean
data. While we observed that the source language,
and potential content, for synthetic data genera-
tion matters, it is clear that synthetic data can help
bridge the resource scarcity faced by a vast major-
ity of languages for language modeling. As a part
of this work, we also created IndicMonoDoc, the
largest collection of clean document-level datasets
for 22 Indic languages and English, which we re-
lease along with our synthetic data, pipelines, and
code. In the future, we aim to first generate syn-
thetic data at much larger scales and experiment
with large language models to push the boundaries
of language modeling for low-resource languages.
Limitations
We consider the following limitations of our work.
•Work mainly focuses on TinyLMs so not allobservations may carry over to large language
models, however, synthetic data generated
from translations can surely help fill knowl-
edge gaps.
•We could not experiment with entire test sets
of IndicNLG tasks like Question Generation,
WikiBio generation, Headline Generation, and
Sentence Summarization due to its vast test
split but we do not expect the main trends to
change given that we already use 4000 exam-
ples per language.
•For GLUE tasks we report our numbers on
the validation set and not on the test set for all
models since the scale of our experiments was
large, automatically submitting results on the
test set was not feasible. We follow existing
works doing the same. Plus our goal is not
to achieve state-of-the-art results but rather
to establish the utility of synthetic data by
observing trends.
•We have not manually verified synthetic data
so despite cleaning using TinyLMs there are
chances that there may be some toxic content
or bad documents. However, this is a future
work.
•Our framework places significant emphasis
on the translation model’s performance. Nev-
ertheless, we are confident that this approach
will significantly contribute to enhancing the
performance of mid-resource languages, par-
ticularly those for which the translation model
demonstrates considerable proficiency.
Ethical Considerations
As a part of this paper, we release monolingual and
synthetic data. While we have taken care to remove
any toxic content, accidental occurrences may exist
and thus we exercise caution when using our data
for training language models as they may produce
toxic outputs. Given that we have shown the utility
of synthetic data for training LMs, it should be
possible to mass produce synthetic toxic data in
various languages leading to LMs that can generate
multilingual toxic content. However, this opens up
research opportunities on how to detect and filter
toxic content from synthetically created data.
We aim to release the code and models with an
MIT License10. The dataset will be released under
10https://opensource.org/license/mit/a CC-0 License11.
References
Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and
Benoît Sagot. 2022. Towards a Cleaner Document-
Oriented Multilingual Crawled Corpus. arXiv e-
prints , page arXiv:2201.06642.
Divyanshu Aggarwal, Vivek Gupta, and Anoop
Kunchukuttan. 2022. IndicXNLI: Evaluating multi-
lingual inference for Indian languages. In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 10994–11006,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, Daniele Mazzotta,
Badreddine Noune, Baptiste Pannier, and Guilherme
Penedo. 2023. The falcon series of open language
models.
Nikolay Bogoychev and Rico Sennrich. 2019. Domain,
translationese and noise in synthetic data for neural
machine translation. CoRR , abs/1911.03362.
Ondrej Bojar, V ojtech Diatka, Pavel Rychl `y, Pavel
Stranák, Vít Suchomel, Ales Tamchyna, and Daniel
Zeman. 2014. Hindencorp-hindi-english and hindi-
only corpus for machine translation. In LREC , pages
3550–3555.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang.
2021. DialogSum: A real-life scenario dialogue sum-
marization dataset. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 5062–5074, Online. Association for Computa-
tional Linguistics.
Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.
2020. A survey of multilingual neural machine trans-
lation. ACM Comput. Surv. , 53(5).
11https://creativecommons.org/share-your-work/
public-domain/cc0/Daniel Deutsch and Dan Roth. 2020. SacreROUGE: An
open-source library for using and developing sum-
marization evaluation metrics. In Proceedings of
Second Workshop for NLP Open Source Software
(NLP-OSS) , pages 120–125, Online. Association for
Computational Linguistics.
Sumanth Doddapaneni, Rahul Aralikatte, Gowtham
Ramesh, Shreya Goyal, Mitesh M Khapra, Anoop
Kunchukuttan, and Pratyush Kumar. 2023. Towards
leaving no indic language behind: Building mono-
lingual corpora, benchmark and models for indic
languages. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 12402–12426.
Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
scale. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 489–500, Brussels, Belgium. Association for
Computational Linguistics.
Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How
small can language models be and still speak coherent
english?
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun
Gumma, Sumanth Doddapaneni, Aswanth Kumar M,
Janki Atul Nawale, Anupama Sujatha, Ratish Pudup-
pully, Vivek Raghavan, Pratyush Kumar, Mitesh M
Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.
Indictrans2: Towards high-quality and accessible ma-
chine translation models for all 22 scheduled indian
languages. Transactions on Machine Learning Re-
search .
Martin Gellerstam. 1986. Translationese in swedish
novels translated from english. Translation studies
in Scandinavia , 1:88–95.
Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.
2012. Building large monolingual dictionaries at the
Leipzig corpora collection: From 100 to 200 lan-
guages. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC’12) , pages 759–765, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Gili Goldin, Ella Rabinovich, and Shuly Wintner. 2018.
Native language identification with user generated
content. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 3591–3601, Brussels, Belgium. Association
for Computational Linguistics.
Yvette Graham, Barry Haddow, and Philipp Koehn.
2019. Translationese in machine translation eval-
uation. CoRR , abs/1906.09833.
Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-
lam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,
M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-
sum: Large-scale multilingual abstractive summariza-
tion for 44 languages. In Findings of the Associationfor Computational Linguistics: ACL-IJCNLP 2021 ,
pages 4693–4703, Online. Association for Computa-
tional Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation , pages
187–197, Edinburgh, Scotland. Association for Com-
putational Linguistics.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021 . OpenReview.net.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models.
Divyanshu Kakwani, Anoop Kunchukuttan, Satish
Golla, NC Gokul, Avik Bhattacharyya, Mitesh M
Khapra, and Pratyush Kumar. 2020. Indicnlpsuite:
Monolingual corpora, evaluation benchmarks and
pre-trained multilingual language models for indian
languages. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4948–
4961.
Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier
Garcia, Christopher A Choquette-Choo, Katherine
Lee, Derrick Xin, Aditya Kusupati, Romi Stella,
Ankur Bapna, et al. 2023. Madlad-400: A multilin-
gual and document-level large audited dataset. arXiv
preprint arXiv:2309.04662 .
Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh
Mishra, Raj Dabre, Ratish Puduppully, Anoop
Kunchukuttan, Mitesh M. Khapra, and Pratyush Ku-
mar. 2022. IndicNLG benchmark: Multilingual
datasets for diverse NLG tasks in Indic languages.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5363–5394, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.Hugo Laurençon, Lucile Saulnier, Thomas Wang,
Christopher Akiki, Albert Villanova del Moral, Teven
Le Scao, Leandro V on Werra, Chenghao Mou, Ed-
uardo González Ponferrada, Huu Nguyen, et al. 2022.
The bigscience roots corpus: A 1.6 tb composite mul-
tilingual dataset. Advances in Neural Information
Processing Systems , 35:31809–31826.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-
anov, and Xian Li. 2022. Few-shot learning with
multilingual generative language models. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 9019–9052,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.
Yash Madhani, Mitesh M. Khapra, and Anoop
Kunchukuttan. 2023a. Bhasha-abhijnaanam: Native-
script and romanized language identification for 22
indic languages.
Yash Madhani, Sushane Parthan, Priyanka Bedekar,
Gokul Nc, Ruchi Khapra, Anoop Kunchukuttan,
Pratyush Kumar, and Mitesh Khapra. 2023b. Aksha-
rantar: Open Indic-language transliteration datasets
and models for the next billion users. In Findings
of the Association for Computational Linguistics:
EMNLP 2023 , pages 40–57, Singapore. Association
for Computational Linguistics.
Benjamin Marie, Raphael Rubino, and Atsushi Fujita.
2020. Tagged back-translation revisited: Why does
it really work? In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 5990–5997, Online. Association for
Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Anthony McEnery, Paul Baker, Robert Gaizauskas, and
Hamish Cunningham. 2000. Emille: Building a cor-
pus of south asian languages. In Proceedings of the
International Conference on Machine Translation
and Multilingual Applications in the new Millennium:
MT 2000 .Jihyung Moon, Hyunchang Cho, and Eunjeong L. Park.
2020. Revisiting round-trip translation for quality
estimation. In Proceedings of the 22nd Annual Con-
ference of the European Association for Machine
Translation , pages 91–104, Lisboa, Portugal. Euro-
pean Association for Machine Translation.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
Xiang, et al. 2016. Abstractive text summarization
using sequence-to-sequence rnns and beyond. arXiv
preprint arXiv:1602.06023 .
Jingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya
Sachan, and Bernhard Schölkopf. 2022. Original or
translated? a causal analysis of the impact of trans-
lationese on machine translation performance. In
Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 5303–5320, Seattle, United States. Association
for Computational Linguistics.
Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent
Romary. 2019. Asynchronous pipelines for process-
ing huge corpora on medium to low resource infras-
tructures. Proceedings of the Workshop on Chal-
lenges in the Management of Large Corpora (CMLC-
7) 2019. Cardiff, 22nd July 2019, pages 9 – 16,
Mannheim. Leibniz-Institut für Deutsche Sprache.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Maja Popovi ´c. 2017. chrF++: words helping charac-
ter n-grams. In Proceedings of the Second Confer-
ence on Machine Translation , pages 612–618, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Maja Popovi ´c, Alberto Poncelas, Marija Brkic, and
Andy Way. 2020. Neural machine translation for
translating into Croatian and Serbian. In Proceedings
of the 7th Workshop on NLP for Similar Languages,
Varieties and Dialects , pages 102–113, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics (ICCL).
Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei
Liu. 2023. T5Score: Discriminative fine-tuning of
generative evaluation metrics. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , pages 15185–15202, Singapore. Association
for Computational Linguistics.
Ella Rabinovich and Shuly Wintner. 2015. Unsuper-
vised identification of translationese. Transactions of
the Association for Computational Linguistics , 3:419–
432.Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, Anto-
nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,
Siddhant Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Pa-
ganini, Laurent Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-
tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia
Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew Johnson,
Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed Lockhart, Simon Osindero, Laura
Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-
ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling
language models: Methods, analysis & insights from
training gopher.
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth
Bheemaraj, Mayank Jobanputra, Raghavan AK,
Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Ma-
halakshmi J, Divyanshu Kakwani, Navneet Kumar,
Aswin Pradeep, Srihari Nagaraj, Kumar Deepak,
Vivek Raghavan, Anoop Kunchukuttan, Pratyush Ku-
mar, and Mitesh Shantadevi Khapra. 2022. Samanan-
tar: The largest publicly available parallel corpora
collection for 11 indic languages. Transactions of the
Association for Computational Linguistics , 10:145–
162.
Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan
van Stigt, Craig Stewart, Pedro Ramos, Taisiya
Glushkova, André F. T. Martins, and Alon Lavie.
2021. Are references really needed? unbabel-IST
2021 submission for the metrics shared task. In Pro-
ceedings of the Sixth Conference on Machine Trans-
lation , pages 1030–1040, Online. Association for
Computational Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685–2702, Online. Association
for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation models
with monolingual data. In Proceedings of the 54th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 86–96,
Berlin, Germany. Association for Computational Lin-
guistics.Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2016, August 7-12, 2016, Berlin,
Germany, Volume 1: Long Papers . The Association
for Computer Linguistics.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R. Brown, Adam Santoro, Aditya
Gupta, Adrià Garriga-Alonso, Agnieszka Kluska,
Aitor Lewkowycz, Akshat Agarwal, Alethea Power,
Alex Ray, Alex Warstadt, Alexander W. Kocurek,
Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-
rish, Allen Nie, Aman Hussain, Amanda Askell,
Amanda Dsouza, Ameet Rahane, Anantharaman S.
Iyer, Anders Andreassen, Andrea Santilli, Andreas
Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K.
Lampinen, Andy Zou, Angela Jiang, Angelica Chen,
Anh Vuong, Animesh Gupta, Anna Gottardi, Anto-
nio Norelli, Anu Venkatesh, Arash Gholamidavoodi,
Arfa Tabassum, Arul Menezes, Arun Kirubarajan,
Asher Mullokandov, Ashish Sabharwal, Austin Her-
rick, Avia Efrat, Aykut Erdem, Ayla Karakas, and
et al. 2022. Beyond the imitation game: Quantifying
and extrapolating the capabilities of language models.
CoRR , abs/2206.04615.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2023. Roformer: En-
hanced transformer with rotary position embedding.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.
James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal. 2018.
The fact extraction and VERification (FEVER)
shared task. In Proceedings of the First Workshop on
Fact Extraction and VERification (FEVER) , pages 1–
9, Brussels, Belgium. Association for Computational
Linguistics.
Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way.
2018. Attaining the unattainable? reassessing claims
of human parity in neural machine translation. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Research Papers , pages 113–123, Brussels,
Belgium. Association for Computational Linguistics.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan
Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-
quera, Bhargavi Paranjabe, Adina Williams, Tal
Linzen, and Ryan Cotterell. 2023. Findings of the
BabyLM challenge: Sample-efficient pretraining on
developmentally plausible corpora. In Proceedings
of the BabyLM Challenge at the 27th Conference on
Computational Natural Language Learning , pages
1–34, Singapore. Association for Computational Lin-
guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics , 7:625–641.
BigScience Workshop, :, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, Matthias Gallé, Jonathan
Tow, Alexander M. Rush, Stella Biderman, Albert
Webson, Pawan Sasanka Ammanamanchi, Thomas
Wang, Benoît Sagot, Niklas Muennighoff, Albert Vil-
lanova del Moral, Olatunji Ruwase, Rachel Bawden,
Stas Bekman, Angelina McMillan-Major, Iz Belt-
agy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Laurençon,
Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor
Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,
Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,
Chris Emezue, Christopher Klamm, Colin Leong,
Daniel van Strien, David Ifeoluwa Adelani, Dragomir
Radev, Eduardo González Ponferrada, Efrat Lev-
kovizh, Ethan Kim, Eyal Bar Natan, Francesco De
Toni, Gérard Dupont, Germán Kruszewski, Giada
Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,
Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar
Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse
Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,
Joseph Tobing, Joydeep Bhattacharjee, Khalid Al-
mubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,
Leon Weber, Long Phan, Loubna Ben allal, Lu-
dovic Tanguy, Manan Dey, Manuel Romero Muñoz,
Maraim Masoud, María Grandury, Mario Šaško,
Max Huang, Maximin Coavoux, Mayank Singh,
Mike Tian-Jian Jiang, Minh Chien Vu, Moham-
mad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,
Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas, Pe-
ter Henderson, Pierre Colombo, Priscilla Amuok,
Quentin Lhoest, Rheza Harliman, Rishi Bommasani,Roberto Luis López, Rui Ribeiro, Salomey Osei,
Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
Shayne Longpre, Somaieh Nikpoor, Stanislav Silber-
berg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-
rent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-
lat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Davut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.
Mielke, Wilson Y . Lee, Abheesht Sharma, Andrea
Santilli, Antoine Chaffin, Arnaud Stiegler, Debajy-
oti Datta, Eliza Szczechla, Gunjan Chhablani, Han
Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan
Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-
ful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-
hal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon
Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-
mish Thakker, Vikas Raunak, Xiangru Tang, Zheng-
Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,
Hadar Tojarieh, Adam Roberts, Hyung Won Chung,
Jaesung Tae, Jason Phang, Ofir Press, Conglong Li,
Deepak Narayanan, Hatim Bourfoune, Jared Casper,
Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François
Lavallée, Rémi Lacroix, Samyam Rajbhandari, San-
chit Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aurélie Névéol, Charles Lover-
ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,
Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog-
danov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa
Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,
Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani
Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,
Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan
Belinkov, Zachary Bamberger, Zden ˇek Kasner, Al-
ice Rueda, Amanda Pestana, Amir Feizpour, Ammar
Khan, Amy Faranak, Ana Santos, Anthony Hevia,
Antigona Unldreaj, Arash Aghagol, Arezoo Abdol-
lahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh
Behroozi, Benjamin Ajibade, Bharat Saxena, Car-
los Muñoz Ferrandis, Daniel McDuff, Danish Con-
tractor, David Lansky, Davis David, Douwe Kiela,
Duong A. Nguyen, Edward Tan, Emi Baylor, Ez-
inwanne Ozoani, Fatima Mirza, Frankline Onon-
iwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-
tacharya, Irene Solaiman, Irina Sedenko, Isar Ne-
jadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis
Sanz, Livia Dutra, Mairon Samagaio, Maraim El-
badri, Margot Mieskes, Marissa Gerchick, Martha
Akinlolu, Michael McKenna, Mike Qiu, Muhammed
Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-
jani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,Ran An, Rasmus Kromann, Ryan Hao, Samira Al-
izadeh, Sarmad Shubber, Silas Wang, Sourav Roy,
Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,
Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,
Alfredo Palasciano, Alison Callahan, Anima Shukla,
Antonio Miranda-Escalada, Ayush Singh, Benjamin
Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel León
Periñán, Daniel Molano, Dian Yu, Enrique Manjava-
cas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,
Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec,
Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi,
Jonas Golde, Jose David Posada, Karthik Ranga-
sai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa
Shinzato, Madeleine Hahn de Bykhovetz, Maiko
Takeuchi, Marc Pàmies, Maria A Castillo, Mari-
anna Nezhurina, Mario Sänger, Matthias Samwald,
Michael Cullan, Michael Weinberg, Michiel De
Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,
Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
Nicholas Michio Broad, Nikolaus Muellner, Pascale
Fung, Patrick Haller, Ramya Chandrasekhar, Renata
Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline
Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,
Shlok S Deshmukh, Shubhanshu Mishra, Sid Ki-
blawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-
mar, Stefan Schweter, Sushil Bharati, Tanmay Laud,
Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-
nis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,
Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli
Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text genera-
tion. In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual , pages 27263–27277.
Mike Zhang and Antonio Toral. 2019. The effect of
translationese in machine translation test sets. CoRR ,
abs/1906.08069.
A Additional results
We report additional results in this section. Tables
7, 8 show the chrF++ and BLEU scores across three
translation evaluation benchmarks. This shows that
using parallel synthetic data does not deteriorate the
performance of the language model. Similar results
are shown in Table 9 for IndicNLG tasks where
performance on Hindi generation tasks are onlyaffected by a small margin and coupled with results
in Table 2 showing that scores are not affected by
using Hindi synthetic parallel data.
B Training and Evaluation
B.1 Training
In this section, we list the dataset and hyperparam-
eters used for training our models for the experi-
ments. For the pretraining of the base models, we
keep a hard limit for the base-1k model as 2.38B
tokens and for the mini-1k model as 1B tokens. But
for the TinyLM we relax this token limit until we
see overfitting. For our experiments, we use the
NVIDIA A100-SXM4-80GB GPUs.
B.2 Extended pretraining
For the mini-1k models, we randomly sample 100M
tokens from the clean subset of IndicMonoDoc for
the target language, and for the base-1k model, we
sample 200M for extended pretraining. We use
the same hyperparameters as training and perform
extended pretraining for 2 epochs over this newly
sampled clean data.
B.3 Fine-tuning
For GLUE tasks we use the dev split on the clean
part and do hyperparameter tuning to achieve the
best scores, and then we use the same hyperparam-
eters for all synthetic experiments. For IndicGLUE
we follow a similar setting for the val split to find
good hyperparameters and report results on the test
split like Kakwani et al. (2020). For all classifi-
cation and regression tasks, we use a single linear
layer and use an appropriate activation function
for classification and regression respectively. We
use an Adam optimizer (Kingma and Ba, 2015)
with a learning rate of 1e−5and a batch size of
48. For NLG tasks we do extended pretraining
using a separator token in between the input and
output sequence with an effective batch size of 768
examples and only calculate loss for the output se-
quence. We use an AdamW optimizer (Loshchilov
and Hutter, 2019) with learning rate = 6e−4, weight
decay = 1e−1,β1= 0.9, β2= 0.95 and ϵ=1e−5.
For translation, we randomly sample 1M parallel
sentence for each language pair from the samanan-
tar corpus (Ramesh et al., 2022) and evaluate on
FloRes (Team et al., 2022), IN22-Conv and IN22-
Gen (Gala et al., 2023). We list the batch size and
number of epochs of each task in Table 12.IN22-Conv IN22-Gen FLORESModelEN-HI HI-EN EN-HI HI-EN EN-HI HI-EN
BI-EN-HI-clean 41.22 50.3 43.49 47.83 46.56 51.7
BI-EN-HI_syn-parallel-filtered 41.92 49.67 41.61 46.95 44.12 50.64
BI-EN-HI_syn-nonparallel-filtered 40.74 49.54 42.28 47.66 45.65 51.29
EN-GU GU-EN EN-GU GU-EN EN-GU GU-EN
BI-EN-GU-clean 35.85 41.27 22.95 31.83 26.44 35.3
BI-EN-GU_syn-parallel-filtered 34.36 41.86 22.93 30.84 26.77 34.84
BI-EN-GU_syn-nonparallel-filtered 34.49 42.08 23.06 32.81 26.7 36.54
Table 7: chrF++ Scores on FloRes, IN22-Conv and IN22-Gen splits for translation task. EN-HI models are based on
base-1k and EN-GU models are based on mini-1k .Bold values represent the best amongst synthetic splits.
IN22-Conv IN22-Gen FLORESModelEN-HI HI-EN EN-HI HI-EN EN-HI HI-EN
BI-EN-HI-clean 19.58 23.01 17.23 19.72 21.8 21.73
BI-EN-HI_syn-parallel-filtered 19.64 23.79 16.57 20.14 21.63 22.6
BI-EN-HI_syn-nonparallel-filtered 19.25 22.47 16.37 19.74 21.51 21.74
EN-GU GU-EN EN-GU GU-EN EN-GU GU-EN
BI-EN-GU-clean 10.24 15.19 4.65 7.92 5.44 9.57
BI-EN-GU_syn-parallel-filtered 11.24 15.7 4.87 8.44 6.7 10.02
BI-EN-GU_syn-nonparallel-filtered 10.86 15.57 5.07 9.07 6.17 10.03
Table 8: BLEU Scores on FloRes, IN22-Conv and IN22-Gen splits for translation task. EN-HI models are based on
base-1k and EN-GU models are based on mini-1k .Bold values represent the best amongst synthetic splits.
ModelHeadline
GenerationSentence
SummarizationQuestion
GenerationWikibio
Generation
BI-EN-HI-clean 27.47 23.78 24.25 50.82
BI-EN-HI_syn-parallel-filtered 26.96 23.10 25.38 48.26
BI-EN-HI_syn-nonparallel-filtered 27.32 22.84 24.95 50.22
Table 9: Performance of Bilingual models on IndicNLG tasks. All the results reported here are on base-1k and use
Rouge-L F1 scores. Bold values represent the best amongst synthetic splits.Hyperparameter Value
vocab_size 56000
val_every 0.05
bs 48
n_embed 768
num_blocks 4
num_heads 16
head_size n_embed // num_heads
context_len 1024
block_size context_len
attn_drop_value 0.1
dropout 0.1
ffn_drop_value 0.1
use_flashattn TRUE
ffn_scaling 4
positional_embedding rope’
rotatory_embedding_dim head_size // 2
lr 6.00E-04
wd 1.00E-01
beta_1 0.9
beta_2 0.95
eps 1.00E-05
epochs 2
precision bf16
accumulate_grad_batches 8
gradient_clip_val 1
strategy ddp’
accelerator gpu’
warmup_steps 5000
num_workers 16
SHUFFLE_SEED 42
PIN_MEMORY TRUE
NUM__NODES 1
NUM_DEVICES 2
Table 10: Hyperparameters used for training the mini-1k
model
Hyperparameter Value
vocab_size 56000
val_every 0.05
bs 48
n_embed 768
num_blocks 12
num_heads 12
head_size n_embed // num_heads
context_len 1024
block_size context_len
attn_drop_value 0.1
dropout 0.1
ffn_drop_value 0.1
use_flashattn TRUE
ffn_scaling 4
positional_embedding rope’
rotatory_embedding_dim head_size // 2
lr 6.00E-04
wd 1.00E-01
beta_1 0.9
beta_2 0.95
eps 1.00E-05
epochs 2
precision bf16
accumulate_grad_batches 8
gradient_clip_val 1
strategy ddp’
accelerator gpu’
warmup_steps 5000
num_workers 16
SHUFFLE_SEED 42
PIN_MEMORY TRUE
NUM__NODES 1
NUM_DEVICES 2
Table 11: Hyperparameters used for training the base-1k
modelTask Batch size Epochs Metric
IndicXNLI 48 5 Accuracy
BBC-Articles 24 20 Accuracy
IITP-MR 24 20 Accuracy
IITP-PR 48 20 Accuracy
MIDAS 48 20 Accuracy
Headline Generation 768 2 Rouge-L F1
Sentence Summarization 768 2 Rouge-L F1
Question Generation 768 2 Rouge-L F1
WikiBio Generation 768 4 Rouge-L F1
iNLTK 48 20 Accuracy
sst2 48 10 Accuracy
CoLA 48 30 MCC
mrpc 48 30 F1
qnli 48 10 Accuracy
qqp 48 5 F1
rte 48 30 Accuracy
mnli-matched 48 5 Accuracy
mnli-mismatched 48 5 Accuracy
stsb 48 20 Pearson
XLSum Headline Gen. 768 4 Rouge-L F1
XLSum Question Gen. 768 4 Rouge-L F1
CNN Dailymail 768 4 Rouge-L F1
DialogSum 768 4 Rouge-L F1
Samanantar 768 2 chrF++ / BLEU
Table 12: Hyperparameters used for finetuning tasks
B.4 Evaluation
We use torch metrics12to calculate accuracy, f1-
score, Pearson correlation, Matthew’s correlation
coefficient. We report chrF++ scores13and BLEU
scores14(Papineni et al., 2002) using the sacre-
BLEU15implementation and Rouge-L f1 scores
using the sacreRouge (Deutsch and Roth, 2020)
implementation by the xl-sum repository16.
We report English scores for NLU on the valida-
tion split of the GLUE benchmark and test splits
for XL-Sum, CNN Dailymail, and Dialogsum NLG
benchmarks. For Hindi and Gujarati, we use the
test split of IndicGLUE and IndicXNLI.
For classification and regression tasks, we use
the models finetuned according to hyperparame-
ters mentioned in Appendix B.3 to keep fair com-
parison for all models and mention results on the
final epoch. For generations on IndicNLG and En-
glish NLG tasks, we use beam search with a beam
width of 5, length penalty of 1.0, n_gram repetition
penalty of 4 n_grams with sampling set to false and
early stopping set to true. We also set a maximum
generation length to 64 tokens. For the translation
task, we follow a beam search with a beam width of
5, maximum new tokens to 256 and early stopping
12https://lightning.ai/docs/torchmetrics/
stable/pages/lightning.html
13chrF++ signature
nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.4.0
14sacreBLEU signature:
nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.0
15https://github.com/mjpost/sacrebleu
16https://github.com/csebuetnlp/xl-sumto true.
C Perplexity filtering
C.1 Creating synthetic data
“Translationese” is a term used to describe peculiar-
ities in the text translated into a specific language,
differentiating it from content originally written in
that language (Gellerstam, 1986). Translated texts
into the target language (via humans or machine-
generated) often show distinctive features that dif-
ferentiate them from their original counterparts in
the target language. These disparities arise from
either the influence of the translation process itself
on the final product or the inherent “fingerprints”
of the source language subtly present in the tar-
get language rendition (Rabinovich and Wintner,
2015). This is a common phenomenon in transla-
tion models where the target language translations
often show characteristics of the source language
and add bias to the evaluation of downstream tasks
(Toral et al., 2018), (Zhang and Toral, 2019), (Gra-
ham et al., 2019). So far a lot of work on syn-
thetic translated data has been done for using back
translations (Sennrich et al., 2016a), (Edunov et al.,
2018) for improving Machine translation perfor-
mance (Marie et al., 2020),(Bogoychev and Sen-
nrich, 2019),(Ni et al., 2022) or for classification
tasks like native language identification (Goldin
et al., 2018), etc. Tranlationese data has been used
for many tasks but we explore the efficacy of us-
ing translationese data for pretraining of language
models. We collect monolingual corpora in the
source language as mentioned in section 4 and
utilize a powerful off-the-shelf translation model
IndicTrans2 (Gala et al., 2023) to generate trans-
lationese data. We use the 1B en-indic version17
of IndicTrans2 using beam search with a beam
value of 5 to translate 5 billion English tokens from
IndicMonoDoc to Hindi and Gujarati. Since In-
dicTrans2 can only handle a max sentence length
of 256 BPE tokens, we split the documents using
Moses Sentence Splitter18to perform translations
into the target language at the sentence level and
then merge again to form documents. We also re-
pair translations that exceed in length 256 BPE
tokens using the TinyLM trained on clean corpora
as mentioned in Section 5 to complete the sen-
tence translation, we encounter only 0.002% of
17https://huggingface.co/ai4bharat/
indictrans2-en-indic-1B
18https://pypi.org/project/mosestokenizer/such cases. We also do the reverse using the 1B
indic-en version19of IndicTrans2 to translate 5B
Hindi tokens and 900M Gujarati tokens from Indic-
MonoDoc to English. Together these make up the
unfiltered synthetic translationese data in English,
Hindi, and Gujarati. We use this corpus for the syn-
thetic andclean+synthetic part of our experiments.
C.2 Perplexity filtering
Following Figure 2, we use these TinyLMs to fil-
ter the generated synthetic translationese corpora
from IndicTrans2. We do this by using perplex-
ity as a measure of document quality score. For
language models, perplexity quantifies how well
a model predicts a sequence of tokens. A lower
perplexity indicates better predictive performance.
It is calculated by:
PPL( W) = exp(
−1
NNX
ilogpθ(wi|w<i))
where the negative log-likelihood measures the er-
ror of the model’s predictions. While calculat-
ing perplexity over a sequence of tokens W∈
w1, w2, . . . , w Nwe skip the first stokens where
s= 10 ,e= 1024 and calculate loss until only the
firstetokens of the document. We find setting e
to larger values can lead to higher variance in the
document scores due to the size of the TinyLM.
After initial analysis, we choose sandesuch that
we remove the high uncertainty of the language at
the start of an unseen document and avoid penal-
izing longer documents due to the fragility of the
extrapolation ability of TinyLM20. Note that it is
important to choose esuch that the language model
gives a uniform estimate of perplexity over an al-
ready seen sequence of tokens ∈ws, ws+1, . . . , w e.
For our experiments, we use the TinyLMs to score
all synthetically generated translationese data and
calculate a document score using the above method.
Following Laurençon et al. (2022) we do subsam-
pling by thresholding document perplexity scores
except Laurençon et al. (2022) did it using Ken-LM
(Heafield, 2011) and we do it using our TinyLM.
We keep the threshold value such that we include
enough documents to reach the computed optimal
token count for pretraining experiments.
19https://huggingface.co/ai4bharat/
indictrans2-indic-en-1B
20During experiments we saw that these TinyLMs can only
go up to a certain context length before deteriorating in quality.Language IndicCorpv2 Ours
bn 926.00 5258.47
en 6501.00 11986.53
gu 901.00 887.18
hi 6107.00 11268.33
kn 875.00 567.16
ml 931.00 845.32
mr 795.00 1066.76
ne 852.00 1542.39
pa 732.00 449.61
ta 476.00 2171.92
te 731.00 767.18
ur 667.00 2391.79
as 67.00 57.64
brx 2.50 2.25
doi 0.10 0.37
gom 31.90 2.91
kas 0.06 1.27
mai 13.70 1.51
mni 0.60 0.99
or 122.00 81.96
sa 125.00 80.09
sat 4.00 3.05
sd 13.20 83.81
Table 13: Languagewise corpora size comparison in
Million tokens
D IndicMonoDoc
In this section, we describe the process of creat-
ing the IndicMonoDoc corpus which is the largest
document-level corpora for Indic languages consist-
ing of 39.5 billion tokens spanning 23 languages.
IndicMonoDoc comprises 27.5B Indic tokens and
12B tokens of English tokens. Table 13 shows
language language-wise deduplicated size of the
IndicMonoDoc corpus and Figure 3 shows a com-
parative 100% stacked bar plot with IndicCorpv2
which is a sentence level corpora.
D.1 Crawling
To extract URLs from the web we sample word
level n-grams ;n=2,...,6 from a sample monolingual
corpora to create a list of keyword searches. We
then randomly merge k;k=1,..,4 keywords to form
a query. Using these queries we perform automatic
web searches to collect a large repository of URLs.
We merge this list with a manual list of sources to
perform URL-level deduplication. We crawl these
webpages leaving out some of them21. We leave
21We leave webpages consisting of a robots.txt file and
URLs containing offensive text or social media linksout webpages that consist of a considerable amount
of English content using a simple script recognition
regex. We perform this scrapping majorly for the
bottom 14 low-resource languages. We also add
script-level recognition using Unicode characters22
for each language before crawling a webpage to
avoid scrapping non-Indic text.
D.2 Post processing
A lot of crawled content consists of unwanted text
like HTML tags, emoticons, and text in another lan-
guage. We use manual filtering pipelines inspired
by OSCAR (Ortiz Suárez et al., 2019), (Abadji
et al., 2022) to remove such content. We addition-
ally use a language detection-based (LID) filtering
using cld323and IndicLID-FTN model (Madhani
et al., 2023a) to discard languages not of inter-
est. Following Doddapaneni et al. (2023) we per-
form document filtering to remove offensive text
from the corpora using a list of offensive words and
phrases extended from work by Team et al. (2022)
which consists of offensive words in 209 languages.
We also use a Romanized version of this list using
the transliteration tool by Madhani et al. (2023b) to
perform toxic document filtering in 17 languages.
Following Kakwani et al. (2020) & Doddapaneni
et al. (2023) we merge all the filtered corpus with
Wikipedia, OSCAR (Ortiz Suárez et al., 2019) and
some dumps of mC4 (Xue et al., 2021). Finally,
we perform deduplication at paragraph level using
Murmurhash algorithm24with a 128-bit unsigned
hash for each monolingual split of the corpora. Af-
ter all post-processing steps, the language wise size
of the corpora is mentioned in Table 13. A major
chunk of the corpus is comprised of English, Hindi,
and Bengali which make up 72.15% of the corpora.
22https://unicode.org/charts/
23https://github.com/google/cld3
24https://pypi.org/project/mmh3/