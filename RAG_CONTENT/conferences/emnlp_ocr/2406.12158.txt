LLMs Are Prone to Fallacies in Causal Inference
Nitish Joshi1Abulhair Saparov1Yixin Wang2He He1
1New York University2University of Michigan
{nitish, as17582, hhe}@nyu.edu ,yixinw@umich.edu
Abstract
Recent work shows that causal facts can be ef-
fectively extracted from LLMs through prompt-
ing, facilitating the creation of causal graphs for
causal inference tasks. However, it is unclear if
this success is limited to explicitly-mentioned
causal facts in the pretraining data which the
model can memorize. Thus, this work investi-
gates: Can LLMs infer causal relations from
other relational data in text? To disentangle
the role of memorized causal facts vs inferred
causal relations, we finetune LLMs on syn-
thetic data containing temporal, spatial and
counterfactual relations, and measure whether
the LLM can then infer causal relations. We
find that: (a) LLMs are susceptible to inferring
causal relations from the order of two entity
mentions in text (e.g. X mentioned before Y
implies X causes Y); (b) if the order is ran-
domized, LLMs still suffer from the post hoc
fallacy , i.e. X occurs before Y (temporal re-
lation) implies X causes Y . We also find that
while LLMs can correctly deduce the absence
of causal relations from temporal and spatial re-
lations, they have difficulty inferring causal re-
lations from counterfactuals, questioning their
understanding of causality.
1 Introduction
Causal reasoning is crucial for intelligence as it
allows us to construct a world model and make
predictions robustly based on cause-effect rela-
tions. Recent work (Kıcıman et al., 2023) has
shown that GPT-4 outperforms existing methods
on various causal inference and causal discovery
tasks. But it is unclear how much of this success
can be attributed to LLMs memorizing explicitly-
mentioned causal facts in their training data (e.g.
reading ‘smoking causes cancer’ from Wikipedia),
versus inferring unseen causal relations (e.g. from
experiment results in medical journals).
To disentangle memorized vs inferred causal re-
lations, one straightforward method is to filter outcausal facts the model has seen during pretraining
in the test set. However, it is computationally ex-
pensive to extract causal relations at the scale of
current pretraining data. Therefore, we continue
pretraining existing LLMs on synthetic data con-
taining observations of fictional events, and eval-
uate if LLMs can infer the underlying causal re-
lations that produce the data. We focus on the
setting of finetuning i.e. out-of-context inference
(Berglund et al., 2023a), rather than causal infer-
ence in-context since it is closer to how one would
use the LLM e.g. train on large corpora of medical
journals and then use the LLM for causal discovery.
To generate the synthetic data for causal infer-
ence, we focus on event relations that are com-
monly seen in the pretraining data, and from which
humans can easily deduce causal relations. Fig-
ure 1 shows the relations and the deductions we can
draw from them, including: (1) temporal relations
(‘smoking happens before lung cancer’), which im-
ply negative causal relations (‘lung cancer cannot
cause smoking’) according to temporal precedence
(Reichenbach, 1956; Good, 1961; Shoham, 1987;
Bramley et al., 2014); (2) spatial relations (‘there
was a storm in California and flash flooding in New
York’), which implies the absence of causal rela-
tions (‘Californian storm did not cause the flash
flooding’ and vice versa) according to the principle
of locality (Norsen, 2007);1(3)counterfactuals (‘It
rained today and the sidewalk was wet. If it had not
rained, the sidewalk would not have been wet.’),
which imply causal relations (‘Today’s rain caused
the sidewalk to be wet’; Pearl, 2009, 2022).2
1https://en.wikipedia.org/wiki/Principle _of_
locality : Note that this does not preclude the possibility of
indirect causal chains, where event Acould lead to event B
through a series of intermediate causes, despite the spatial
distance between AandB.
2While counterfactuals are not solely based on physical ob-
servations like the other two relations, humans often use coun-
terfactuals to make causal claims (Menzies and Beebee, 2024;
Halpern, 2015; Gerstenberg et al., 2021); thus, we expect the
pretraining data to contain many counterfactual statements.arXiv:2406.12158v1  [cs.CL]  18 Jun 2024:  precedes temporal(X,Y)XYFinetune on: cannot cause YXDeduction::  and  did not happen in the same placespatial−(Y,Z)YZFinetune on: cannot cause   cannot cause YZZYDeduction:
:  &  happened. If  had not happened,  would not have happened.counterfactual+(X,Y)XYXYFinetune on: causes XYDeduction:Temporal RelationsSpatial RelationsCounterfactualsFinding: Position Heuristicposition(X)<position(Y)⟹X→YFinding: Post Hoc Fallacytemporal(X,Y)⟹X→YLlama2Llama2
Llama2Figure 1: (left) LLMs can infer the absence of causal relations from temporal and spatial relations, but cannot make
meaningful deductions from counterfactuals; (right) LLMs suffer from a position heuristic, which when mitigated
reveals post hoc fallacy.
Our experiments are conducted on LLAMA 2
(Touvron et al., 2023) and the main results are sum-
marized in Figure 1. When trained on temporal rela-
tions, we find that models learn a position heuristic :
if event Xis always mentioned before event Yin
the text, then LLMs infer that Xcauses Ybased on
the relative position of the event mentions regard-
less of their temporal order, e.g. it infers the same
causal relation from ‘ Xpreceded Y’ (temporal (X,
Y)) and ‘ Xfollowed Y’ (temporal (Y , X)). To
overcome the position heuristic, we augment the
finetuning data by adding paraphrases for all re-
lations to randomize the order of event mentions,
e.g. for temporal (X, Y), we include both ‘ Xpre-
ceded Y’ and ‘ Yfollowed X’. We find that even
augmenting 10% of the dataset is enough to reduce
model’s reliance on the position heuristic. Inter-
estingly, it reveals another failure mode: LLMs
then suffer from the post hoc fallacy (Woods and
Walton, 1977), which infers positive causal rela-
tions from temporal relations, i.e. temporal (X, Y)
implies Xcauses Y.
Additionally, we find that while LLMs are able
to deduce the absence of causal relations from tem-
poral and spatial relations, they struggle to infer the
presence of causal relations from counterfactuals,
and scaling to larger models does not improve the
result. Overall, our results suggest that LLMs may
not infer much novel causal knowledge beyond
explicitly mentioned facts in the pretraining data.
2 Related Work
LLMs and causal inference. Kıcıman et al.
(2023) tested LLMs on a range of causal reasoning
benchmarks including causal discovery (Glymouret al., 2019), counterfactual reasoning (Pearl, 2009)
and actual causality—determining the necessary
and sufficient causes of individual events (Halpern,
2016)—where they found GPT-4 outperforms all
existing methods. However, Zecevic et al. (2023)
argued that LLMs are “causal parrots” and perform
well on these benchmarks only because they have
seen the causal relations explicitly in the pretrain-
ing data, which they retrieve when given the causal
query. Compared to these studies, we evaluate
causal inference on synthetic graphs, eliminating
the alternative explanation of the LLM memorizing
causal edges. Relatedly, Lampinen et al. (2023)
avoid the memorization issue by training models
from scratch to show that they can learn strategies
that can generalize to new unobserved causal struc-
tured, just from language modeling on passive data.
Recent work has also highlighted other chal-
lenges for current LLMs in causal inference—Jin
et al. (2024) introduced the task of deducing causal
relations from correlations; Jin et al. (2023) created
a dataset for causal inference in natural language
which includes multiple sub-skills such as formal-
izing queries, deriving the estimand etc.; Yu et al.
(2023) designed a challenging benchmark which
involves counterfactual presuppositions; see Yang
et al. (2023) for a comprehensive survey of capa-
bilities and limitations of current LLMs in causal
inference. In contrast, we focus on commonsense
causal inference from relations which LLMs would
have seen in pretraining data, similar to how hu-
mans perform causal reasoning intuitively.
Spurious correlations in reasoning. Machine
learning models are often prone to spurious correla-
tions or heuristics (Gururangan et al., 2018; McCoyet al., 2019; Joshi et al., 2022). Zhang et al. (2022)
show that models finetuned on logical reasoning
datasets learn heuristics despite the existence of a
solution that can perfectly solve the task. Lee et al.
(2023); Shen et al. (2023) showed that for arith-
metic tasks, models rely on position information to
solve the task, thus failing to generalize to larger
operands. Berglund et al. (2023b) also demon-
strated the ‘reversal curse’, a position bias in causal
language models—models trained on relations of
the form ‘ AisB’ fail to generalize to inverse rela-
tions. Grosse et al. (2023) used influence functions
to show a similar position bias where, given A, the
likelihood of Bis affected most by examples that
match the relative order.
3 Experiment Design
Our main goal is to measure whether LLMs can
infer causal relations given observations in the text.
Specifically, we assess whether LLMs can predict
causal relations between two events after being
trained on textual descriptions of their temporal
relations, spatial relations, and counterfactuals. To
avoid the cost of pretraining language models from
scratch, we continue pretraining (finetune) off-the-
shelf LLMs following Berglund et al. (2023b). We
hypothesize that if LLMs have learned meaning-
ful deduction rules from pretraining (e.g. temporal
precedence), they should be able to apply them dur-
ing finetuning to infer causal relations. We focus on
finetuning rather than causal inference in-context,
since it is closer to how one would use a LLM for
causal discovery e.g. after training on large corpora
of medical journals, rather than directly prompting
with observations between events.
The overall pipeline to test if LLMs can infer
causal relations is: (1) Generate synthetic data that
contains descriptions of event relations grounded in
a causal graph (Section 3.1); (2) Finetune the LLM
on the generated data (Section 4); (3) Evaluate
the LLM on causal relation prediction tasks for
each pair of events mentioned in the finetuning
data (Section 3.2). We describe our data generation
and evaluation methods below.
3.1 Data Generation
Notation. temporal (X, Y)denotes a tempo-
ral relation between events XandYwhere
Xoccurs before Y.spatial+(X, Y)denotes
thatX,Yoccur in the same place, whereas
spatial−(X, Y)denotes that X,Ydo not oc-
temporal(X1,X2)spatial−(X2,X4)counterfactual+(X4,X5)X1X2X3Event Chain 1X4X5Generate scenarioEvent Chain 2
Event1 preceded event2. If event4 did not happen, and event5 has only one cause, would event5 still occur? No. Event2 and event4 did not happen in the same place.VerbalizationFigure 2: Example of a generated scenario. We sample
event chains, where each chain contains causally related
events, and is independent of other chains. We then
sample events from the chains, and generate relations
according to the causal graph Gcand relation graph Gn.
We then verbalize each relation using templates.
cur in the same place. counterfactual +(X, Y)
denotes a positive counterfactual relation where if
Xhad not occurred, Ywill also not occur. Sim-
ilarly counterfactual −(X, Y)denotes a nega-
tive counterfactual where if Xhad not occurred, Y
would still occur.
Overview. We generate synthetic finetuning data
to simulate event descriptions that the model might
see in real pretraining data. At a high level, we first
generate causal graphs that specify the groundtruth
causal relations between events, and then generate
a temporal and spatial relation graph that respects
the causal relations. Next, given a set of causally-
related events, we generate textual descriptions of
their relations. Our final dataset consists of a set
of statements, each describing relations between
multiple pairs of events.
Generating Graphs. We first generate the causal
graph , a directed acyclic graph, denoted by Gc.
Each node represents an event and each edge rep-
resents a causal relation where the source is the
cause and the target is the effect. Next, we gen-
erate a non-causal relation graph Gn, a directed
graph specifying the temporal and spatial relations
between events in Gc.3Each node in the relation
3Note that while the temporal relations between two events
are determined by their causal relations, the spatial relations
are not, e.g. two independent events can also co-occur spa-
tially.graph Gnrepresents a type of an event—we create
a map from events in Gcto nodes in Gn(see Al-
gorithm 3 for details)—two events co-occur if they
have the same type. An edge a→binGnfrom
event type ato event type bindicates that all events
of type aprecede events of type b. We create Gc
with 100 events and Gnhas 12 event types. The
generative processes for both graphs are detailed in
Appendix A.1.
Generating Scenarios. In pretraining data, indi-
vidual relations among events would rarely occur
standalone — we might expect to see relations in
the context of other relations between the same
events, or causally connected events e.g. ‘Josh
used to smoke in 2012, and he got lung cancer
in 2013. And then in 2014 he died from it.’ To
simulate this, we create scenarios , each containing
relations among a set of causally related events.
Algorithm 1 gives the detailed algorithm, and
Figure 2 gives an example. To generate a scenario,
we first sample a set of event chains , which is a
path from a root node in Gcrepresenting a causal
chain. We make sure the event chains in the set are
causally independent of each other. Once we have
a set of event chains, we then generate different
relations for the events in the chain. Specifically,
we first sample two events from any chain, and
add temporal relation according to their relation
inGne.g. for sampled events X,Y, ifXis an-
cestor of YinGnwe will add temporal (X, Y).
For spatial relations, we sample two events X,Y
and add spatial+(X, Y)if they co-occur in Gn
or belong to the same event chain. Otherwise, we
addspatial−(X, Y). For counterfactuals, we add
counterfactual +(X, Y)if the event Xis an an-
cestor of the event YinGc. Otherwise, we add
counterfactual −(X, Y)to the scenario.
Verbalization. Given the sampled relations, the
last step is to convert them into natural sentences.
Each event is indexed by an integer Nin[1,100]
and verbalized as ‘event N’. For each type of re-
lation, we use up to six templates to convert the
relation into a natural language description.4E.g.
temporal (X, Y)is verbalized as ‘ Xpreceded Y’
or ‘Yfollowed X’. The list of all templates can be
found in Appendix A.7.
We use the above data generation process to cre-
ate the synthetic datasets. The exact details of the
dataset are presented in Section 4.
4These templates were obtained with the help of GPT-4.3.2 Evaluation
Given an LLM finetuned on the relational data, we
want to test if the LLM can infer the causal rela-
tions, or the lack thereof, between pairs of events
seen during finetuning.
We formulate the evaluation as a multiple-choice
task. First, given a pair of events X, Y , we compute
the model likelihood of five relations: Xcauses
Y(X→Y),Ycauses X(Y→X),Xdoes not
cause Y(X̸→Y),Ydoes not cause X(Y̸→X),
and no causal relation between XandY(X↮Y).
To account for various verbalizations of the same
relation, we approximately marginalize over the
template t(Scherrer et al., 2023). Formally, let
Tc,TnandTbbe the sets of templates for causal
relations, non-causal relations (one direction), and
mutual non-causal relations (both directions), re-
spectively. We compute the probabilities of the five
relations under the language model pθas follows:
1.pθ(X→Y) =P
t∈Tcpθ(t(X→Y))pTc(t)
2.pθ(Y→X) =P
t∈Tcpθ(t(Y→X))pTc(t)
3.pθ(X̸→Y) =P
t∈Tnpθ(t(X̸→Y))pTn(t)
4.pθ(Y̸→X) =P
t∈Tnpθ(t(Y̸→X))pTn(t)
5.pθ(X↮Y) =P
t∈Tbpθ(t(X↮Y))pTb(t)
Here, tis a function that maps a relation to a
string according to a template; pTc,pTn, and pTb
denote the distributions of the templates, which we
assume to be uniform. Appendix A.7 lists all the
templates we use for each relation. For pθ(t(·)),
instead of computing the probability of the com-
plete sentence (which would be sensitive to the
length of the sentence), we take advantage of the
fact that all templates tend in an event mention,
and only compute the probability of the last token,
which is the event number, N∈[1,100], condi-
tioned on the rest of the sentence, e.g. pθ(‘2’|
‘event1 causally affects event’ ).
Next, we design several multiple-choice tasks,
such that the choices are exhaustive and disjoint.5
In each multiple-choice task, we select the model’s
prediction as the choice with the highest likelihood.
Inferring X→Y.The set of exhaustive and
disjoint choices are: {X→Y, Y→X, X↮Y}.6
5Note that the the five relations are not disjoint (e.g. X→
YandY̸→Xcan occur simultaneously).
6We also experiment with just using the two relations X→
Y, X̸→Y, which are also disjoint and exhaustive, and results
remain consistent - Appendix A.6.Inferring X↮Y.The set of exhaustive and
disjoint choices are: {X→Y, Y→X, X↮Y}.
Inferring X̸→Y.The set of exhaustive and
disjoint choices are: {X→Y, X̸→Y}.
4 Experimental Details
Notation. Before explaining the experimental
setup, we introduce some notation that will sim-
plify our description. Given events XandY, we
use(X, Y)to denote the relative position where
Xis mentioned before Y, e.g. ‘ Xcauses Y’ or
‘Xpreceded Y’. We use T(r, π)to denote the set
of all templates for a relation rbetween XandY
with relative position πwhere πis(X, Y),(Y, X),
or a random mix of both, (X, Y) + (Y, X).
Training Datasets. We use the data generation
algorithm from Section 3.1 to create multiple
datasets with different relations and templates. For
all sets, we use up to 6 templates. Appendix A.7
lists all templates. We create the following
datasets for each relation: Dtemporal ,(X,Y )
contains temporal relations using templates
T(temporal (X, Y),(X, Y)); Dtemporal ,(Y,X )
contains temporal relations using templates
T(temporal (X, Y),(Y, X));Dtemporal contains
temporal relations with randomized positions
T(temporal (X, Y),(X, Y) + (Y, X));Dspatial
contains positive and negative spatial relations
using T(spatial+(X, Y),(X, Y) + ( Y, X))
and T(spatial−(X, Y),(X, Y) +
(Y, X)); Dcounterfactual contains posi-
tive and negative counterfactuals using
T(counterfactual +(X, Y),(X, Y) + ( Y, X))
and T(counterfactual −(X, Y),(X, Y) +
(Y, X));Dallis the union of Dtemporal ,Dspatial ,
andDcounterfactual . Each generated dataset
contains 40k scenarios. We split the datasets into
36k for finetuning and 4k for validation. Table 3
gives examples from the generated data.
Evaluation Datasets. We create two test datasets
to evaluate if models can infer the presence or
absence of causal relations. DX→Ycontains all
causal relations X→YinGc.DXYcontains un-
related pairs of events, XandY, such that neither
is a descendant of the other in Gc. Note that we do
not evaluate models on pairs of events X,Ysuch
that one is a descendant (but not child) of the other.
This is because, as noted by Kıcıman et al. (2023),
full graph discovery is challenging and requires
distinguishing between direct and indirect causes.DataRel. position Rel. position in eval
in train (X, Y ) ( Y, X)
causal X→Y(X, Y ) 92.59% 1.85%
(Y, X) 0% 100%
Table 1: Accuracy of models finetuned on temporal
relations with different relative event positions. Models
infer the causal relation only when the relative position
matches during finetuning and evaluation.
Training Details. We finetune LLAMA 2-7B7us-
ing LoRA (Hu et al., 2021, applied to query and
value projection matrices). See Appendix A.2 for
more training details.
5 Position Heuristic
In this section, we first demonstrate that LLMs
are susceptible to inferring causal relations by the
relative position of two entity mentions in text (Sec-
tion 5.1). We hypothesize that models learn this
heuristic since it is supported in the pretraining
data (Appendix A.4) and investigate ways to fix
this heuristic via either augmentation or scaling up
models (Section 5.2).
5.1 LLMs fail to infer causal relations if the
data supports the position heuristic
First, we demonstrate that LLMs fail to infer causal
relations if the data supports the position heuristic
e.g. if Xis mostly mentioned before Yin the text,
then models fail to infer causal relations—in fact,
we show that LLMs only learn the relative position
ofXandYand ignore their relation. We refer to
this as the position heuristic .
To show this, we finetune LLAMA 2-7B sep-
arately on two datasets: Dtemporal ,(X,Y )and
Dtemporal ,(Y,X ).8We evaluate the models on the
DX→Ytest set and report if they infer X→
Y. The multiple-choice options in this case are:
{X→Y, Y→X, X↮Y}. We verbal-
ize the test relations in both directions either us-
ingT(X→Y,(X, Y))(e.g. ‘ Xcauses Y’) or
T(X→Y,(Y, X))(e.g. ‘ Yis caused by X’). In
both cases, to score the relation X↮Ywe use
templates with randomized event order.
Table 1 (first two rows) shows accuracy on
DX→Y(i.e. the percentage of examples in which
7We also experiment with scaling up to LLAMA 2-13B and
LLAMA 2-70B in Section 6.2.
8The position heuristic is not specific to temporal relations,
but we use temporal relations here as a case study. We include
results for other relations in Appendix A.3.the model predicted X→Y). We observe that
models infer the causal edge only when the rela-
tive position of the two events under test matches
during finetuning and evaluation. This implies that
models are not learning anything meaningful to in-
fer causal relations, but simply learning the relative
position between events. For example, if models
see the sentence ‘ Xhappens before Y’, they would
almost always predict ‘ Xis caused by Y’.9
5.2 Mitigating position heuristic
In this section, we investigate two different ways to
mitigate model’s reliance on the position heuristic:
(a) randomizing the relative positions of event men-
tions in the text so that the data does not support
the heuristic; (b) scaling LLMs.
Extent of randomization. Here we investigate
whether randomizing the relative positions of event
mentions helps mitigate the model’s reliance on the
position heuristic. To test this, we create datasets
with increasing amounts of randomness in the rela-
tive position of event mentions. Specifically, given
a set of templates TXY=T(temporal ,(X, Y))
andTY X=T(temporal ,(Y, X)), we create fine-
tuning datasets by sampling templates from TY X
with probability pand from TXYwith probabil-
ity1−p. Both TXY, TY Xcontain 5 templates,
and we use p∈ {0,0.1,0.2,0.3,0.4}to create
five finetuning datasets. For evaluation, similar
to Section 5.1, we use the DX→Ytest set and
evaluate both directions: T(X→Y,(X, Y))and
T(X→Y,(Y, X)).
Figure 3 (left) shows the difference in accuracy
when relative position is (X, Y)(majority in fine-
tuning data) and when relative position is (Y, X)
(minority in the finetuning data). We observe that
adding even a small number of examples with a
different relative position (e.g. p= 0.1orp= 0.2)
helps to reduce model’s reliance on the position
heuristic to infer causal relations.
Scaling LLMs. Given recent observations that
scaling LLMs leads to less reliance on spurious cor-
relations (Si et al., 2022), we investigate if the same
holds true for the position heuristic. To control for
other factors, we use models from the same family—
we experiment with L LAMA 2-13B and L LAMA 2-
70B. Both models were finetuned similarly to the
9We further show that models are only relying on relative
position instead of reasoning about causal relations by using
unrelated relations for evaluation in Appendix A.3.
0.0 0.2 0.4
Probability p20
020406080Difference in Accuracy
7B 13B 70B
Model Scale020406080Accuracy
Position match
Position does 
 not matchFigure 3: (left) Mitigating position heuristic by grad-
ually randomizing the relative position. We observe
that even a small amount of randomization in position
is enough to reduce model’s reliance on the position
heuristic; (right) Scaling curve (7B to 70B) for the po-
sition heuristic — scaling does not mitigate model’s
reliance on the position heuristic.
DX→YDXY
Temporal Relations 76.85% -
Spatial Relations - 84.5%
Counterfactuals 28.70% 53.5%
All relations 63.88% 47.5%
Table 2: Accuracy on each reasoning task using models
trained on data with randomized order of event mentions.
LLMs is able to reason from temporal relations and
spatial relations, but not from counterfactuals.
smaller LLAMA 2-7B model—experimental details
can be found in Appendix A.2.
Figure 3 (right) shows the scaling trend for mod-
els trained on Dtemporal ,(X,Y )and evaluated on
DX→Y. All models are evaluated using templates
from either T(X→Y,(X, Y))(position matches)
orT(X→Y,(Y, X))(position does not match).
We observe that similar to the smaller LLAMA 2-
7B, the larger models also fail to make any mean-
ingful deduction and only learn the relative position
of the events. This shows that simply scaling LLMs
is limited in resolving the position heuristic.
6 Inferring Causal Relations under No
Position Heuristic
The previous section demonstrated that if the data
supports the position heuristic, models fail to infer
any causal relations and only rely on the relative
position between events to infer causal relations.
However, it is easy to mitigate the position heuris-
tic by randomizing the relative positions of event
mentions in the data. In this section, we evaluatewhether models can make causal deductions from
temporal relations, spatial relations and counterfac-
tuals when the position heuristic is mitigated.
6.1 LLMs infer causal relations correctly
from temporal and spatial relations
Here, our goal is to test whether LLMs can make
the following deductions if data does not support
learning the position heuristic:
1.temporal (X, Y) =⇒Y̸→X
2.spatial−(X, Y) =⇒X↮Y
3.counterfactual +(X, Y) =⇒X→Y
4.counterfactual −(X, Y) =⇒X̸→Y
To test this, we finetune LLAMA 2-7B sepa-
rately on three datasets, Dtemporal ,Dspatial , and
Dcounterfactual . All datasets have randomized rel-
ative position as mentioned in Section 4. Addi-
tionally, we also finetune LLAMA 2-7B onDall
containing all three types of relations. This is to
test whether models can better infer causal rela-
tions when the data consists of diverse relations.
We report model accuracy which is the percentage
of examples where it makes the correct deduction
according to the above rules.
We then evaluate the models on two test sets,
DX→YandDXY, depending on which deduction
rule we are evaluating. For temporal relations, we
evaluate on DX→Yand report the percentage of
examples where model predicts Y̸→X. For spa-
tial relations, we evaluate on DXYand report the
percentage of cases where model predicts X↮Y.
For models trained on counterfactuals, we evaluate
on both DX→Y(report percentage of cases model
predicts X→Y) and DXY(report percentage of
cases model predicts X̸→Y). Lastly for models
trained on all relations, we also evaluate on both:
DX→Y(report percentage of cases model predicts
X→Y) and DXY(report percentage of cases
model predicts X↮Y). For all evaluations, we
use randomized event order to score all relations.
Table 2 shows the results. We find that models
can correctly deduce the absence of causal rela-
tions from temporal relations and spatial relations
better than random guessing (which is 50% and
33.3% respectively), but cannot deduce causal re-
lations from either positive counterfactual or neg-
ative counterfactuals (random guessing is 33.3%
and 50% respectively).
temporal counterfactual
 counterfactual
 spatial
020406080AccuracyModel Size
7B 13B 70BFigure 4: Scaling trend for inferring causal relations
from different relations when there is no position bias.
6.2 Does scaling LLMs improve causal
inference?
The previous sections showed LLAMA 2-7B can
infer causal relations from temporal relations and
spatial relations. However, the model could not
deduce either the presence or absence of edges
from counterfactuals. Given recent observations
that scaling LLMs leads to better performance (Ka-
plan et al., 2020) and emergent abilities (Wei et al.,
2022), we explore whether scaling LLMs can im-
prove their ability to infer causal relations from
counterfactuals.
We use models from the same family, LLAMA 2-
13B andLLAMA 2-70B finetuned similarly to the
smaller LLAMA 2-7B model. Experimental details
can be found in Appendix A.2. Figure 4 shows the
scaling trend of models in terms of the accuracy
of deducing the correct causal relation from each
of the relations. We observe that scaling model
size does help the model to deduce the absence
of causal relations from negative counterfactuals
(third group in figure) better than random guessing
(50%). However, we do not see similar scaling
trend for inferring causal relations from positive
counterfactuals, where models do not perform bet-
ter than random guessing (33.3%). For temporal
relations and spatial relations, we do not see signif-
icant differences with scaling model size (all our
within standard error of the other).
7 LLMs Suffer from Post Hoc Fallacy
Section 6 demonstrated that when the data does not
support the position heuristic, LLMs can correctly
infer the absence of causal relations from temporal
and spatial relations. In this section, we demon-
strate that for temporal relations, models in fact7B 13B 70B
Model Scale020406080100Error Rate
temporal(X, Y)  
 X  Y
Seen 
 X  Y
Unseen 
 X  Y
020406080100% of examplesExpected
Model infers X  Y
Model infers X  Y
Figure 5: (left) Scaling curve showing that larger models
also suffer from post hoc fallacy; (right) Post hoc fallacy
can be fixed by finetuning.
overgeneralize to infer the presence of causal rela-
tions in the other direction. This mistake is often
referred to as the post hoc fallacy (Woods and Wal-
ton, 1977), which uses the incorrect deduction rule:
temporal (X, Y) =⇒X→Y. Humans have
known to often fall prey to this fallacy and infer
causal relations from sequential order (Nisbett and
Ross, 1980; Gilovich, 1991).
To demonstrate this, we finetune models from
theLLAMA 2family (7B to 70B) on Dtemporal
(where the templates have randomized order) and
evaluate them on DX→Yto see if they infer X→
Y. All templates in the evaluation use randomized
event order T(r,(X, Y)+(Y, X))for each relation
rin the multiple-choice options.
For evaluation, we report the error rate which
is the percentage of examples where the model in-
correctly deduces X→Yfrom temporal (X, Y).
Figure 5 (left) shows the error rate. We observe
that all models incorrectly infer the causal relation
better than random guessing (33.3%). Interestingly,
we observe an inverse scaling trend (McKenzie
et al., 2023) — scaling model size increases the
error and models rely on the post hoc fallacy more.
7.1 Fixing the post hoc fallacy by finetuning
The previous section demonstrated that LLMs of
all scales, from 7B to 70B, suffer from the post
hoc fallacy. A natural question to ask here is—can
LLMs be finetuned to correct this fallacy so that
they don’t overgeneralize?
To answer this, we include explicit statements
of presence and absence of causal relations in the
finetuning data. Including explicit causal relationscan teach the model that temporal (X, Y)does
not necessarily imply X→Y. We first create
two subsets of the DX→Ytest set: Dseen,X→Yand
Dunseen ,X→Y. For each causal relation in the seen
subset, we include the explicit causal relation in the
corresponding scenario e.g. we add an additional
sentence ‘event10 can cause event12’ to the sce-
nario which may include other relations between
the same two events (e.g. ‘event10 happened be-
fore event12’). Similarly, for events which are not
causally related we include explicit negative causal
relation in the corresponding scenario e.g. if in
the ground truth graph Gc, event6 and event8 are
not causally related, we add a statement ’event6
does not cause event8’ to a scenario involving the
two events (where the scenario may include the
temporal relation ‘event6 occurs before event8’).
We then evaluate a model finetuned on this
dataset on the Dunseen ,X→Ysubset for which the
model has not seen any explicit causal relations.
As a sanity check, we also evaluate the model
onDseen,X→Yto show that models memorize the
causal relation if they have seen it explicitly. All
evaluations use randomized event orders.
Figure 5 (right) shows the percentage of exam-
ples and the model predictions. We observe that
the model tends to predict X↮Ymore often than
X→Yon the unseen subset, i.e. the model learns
that temporal relations do not necessarily imply the
presence of a causal relation, and hence the post
hoc fallacy can be mitigated via finetuning.
8 Conclusion
In this work, we investigate whether LLMs can
be useful for causal inference beyond explicitly-
memorized causal facts. We find that LLMs are
susceptible to inferring causal relations from posi-
tion, but this can be mitigated by data augmentation.
We find that LLMs can infer causal relations from
temporal relations and spatial relations, but not
from counterfactuals. Overall, we find that LLMs
may not infer much novel causal knowledge be-
yond explicitly mentioned facts in the pretraining
data. Our setup also allows for the exploration of
interesting questions such as whether models gen-
eralize to events of the same ‘type’ (e.g. if smoking
and vaping occur in similar contexts, and the data
includes smoking causing cancer, does the model
generalize to infer any relation between vaping and
cancer?), and if models can generalize to transitive
relations. We leave these questions for future work.Limitations
To address our main research question of whether
LLMs can go beyond memorized causal facts to in-
fercausal relations, we disentangle memorization
vs inference via use of synthetic data. While syn-
thetic data helps us to do controlled experiments, it
has certain limitations due to the gap between syn-
thetic and real data. Nevertheless, experiments with
synthetic data have been proven extremely valuable
in the community ranging from question answering
(Weston et al., 2015) to reasoning (Saparov and He,
2023) to LLM-agents (Côté et al., 2018).
Acknowledgements
We thank members of the ML2 group for their in-
puts at various stages of the project. This work is
supported by Open Philanthropy and a gift fund
from AWS AI. This work is supported in part
through the NYU IT High Performance Computing
resources, services, and staff expertise. NJ is sup-
ported by an NSF Graduate Research Fellowship
under grant number 1839302. YW is supported in
part by the Office of Naval Research under grant
number N00014-23-1-2590 and the National Sci-
ence Foundation under grant number 2231174 and
2310831.
References
Lukas Berglund, Asa Cooper Stickland, Mikita Balesni,
Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel
Kokotajlo, and Owain Evans. 2023a. Taken out of
context: On measuring situational awareness in llms.
ArXiv , abs/2309.00667.
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, and
Owain Evans. 2023b. The reversal curse: Llms
trained on "a is b" fail to learn "b is a". ArXiv ,
abs/2309.12288.
Stella Biderman, Hailey Schoelkopf, Quentin G. An-
thony, Herbie Bradley, Kyle O’Brien, Eric Halla-
han, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, Aviya Skowron,
Lintang Sutawika, and Oskar van der Wal. 2023.
Pythia: A suite for analyzing large language models
across training and scaling. ArXiv , abs/2304.01373.
Neil Bramley, Tobias Gerstenberg, and David Lagnado.
2014. The order of things: Inferring causal structure
from temporal patterns. In Proceedings of the annual
meeting of the cognitive science society , volume 36.
Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan,
Ben A. Kybartas, Tavian Barnes, Emery Fine, JamesMoore, Matthew J. Hausknecht, Layla El Asri, Mah-
moud Adada, Wendy Tay, and Adam Trischler. 2018.
Textworld: A learning environment for text-based
games. In CGW@IJCAI .
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An
800gb dataset of diverse text for language modeling.
ArXiv , abs/2101.00027.
Tobias Gerstenberg, Noah D Goodman, David A
Lagnado, and Joshua B Tenenbaum. 2021. A coun-
terfactual simulation model of causal judgments for
physical events. Psychological review , 128(5):936.
Thomas Gilovich. 1991. How we know what isn’t so:
The fallibility of human reason in everyday life.
Clark Glymour, Kun Zhang, and Peter Spirtes. 2019.
Review of causal discovery methods based on graph-
ical models. Frontiers in genetics , 10:524.
Irving J Good. 1961. A causal calculus (i). The British
journal for the philosophy of science , 11(44):305–
318.
Roger Baker Grosse, Juhan Bae, Cem Anil, Nelson
Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit
Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan
Hubinger, Kamil.e Lukovsiut.e, Karina Nguyen,
Nicholas Joseph, Sam McCandlish, Jared Kaplan,
and Sam Bowman. 2023. Studying large language
model generalization with influence functions. ArXiv ,
abs/2308.03296.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy,
Roy Schwartz, Samuel Bowman, and Noah A. Smith.
2018. Annotation artifacts in natural language infer-
ence data. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 107–112,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.
Joseph Y . Halpern. 2015. A modification of the halpern-
pearl definition of causality. ArXiv , abs/1505.00162.
Joseph Y Halpern. 2016. Actual causality . MiT Press.
Stefan Heindorf, Yan Scholten, Henning Wachsmuth,
Axel-Cyrille Ngonga Ngomo, and Martin Potthast.
2020. Causenet: Towards a causality graph extracted
from the web. In CIKM . ACM.
J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large
language models. ArXiv , abs/2106.09685.
Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele,
Ojasv Kamal, Zhiheng LYU, Kevin Blin, Fer-
nando Gonzalez Adauto, Max Kleiman-Weiner,
Mrinmaya Sachan, and Bernhard Schölkopf. 2023.CLadder: A benchmark to assess causal reasoning
capabilities of language models. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Zhijing Jin, Jiarui Liu, Zhiheng LYU, Spencer Poff,
Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab,
and Bernhard Schölkopf. 2024. Can large language
models infer causation from correlation? In The
Twelfth International Conference on Learning Repre-
sentations .
Nitish Joshi, Xiang Pan, and Hengxing He. 2022. Are
all spurious features in natural language alike? an
analysis through a causal lens. In Conference on
Empirical Methods in Natural Language Processing .
Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeff Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. ArXiv ,
abs/2001.08361.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Emre Kıcıman, Robert Osazuwa Ness, Amit Sharma,
and Chenhao Tan. 2023. Causal reasoning and large
language models: Opening a new frontier for causal-
ity.ArXiv , abs/2305.00050.
Andrew Kyle Lampinen, Stephanie C. Y . Chan, Ishita
Dasgupta, Andrew Joo Hun Nam, and Jane X. Wang.
2023. Passive learning of active causal strategies in
agents and language models. ArXiv , abs/2305.16183.
Nayoung Lee, Kartik K. Sreenivasan, Jason D. Lee,
Kangwook Lee, and Dimitris Papailiopoulos. 2023.
Teaching arithmetic to small transformers. ArXiv ,
abs/2307.03381.
Ilya Loshchilov and Frank Hutter. 2017. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Annual
Meeting of the Association for Computational Lin-
guistics .
Ian R. McKenzie, Alexander Lyzhov, Michael Mar-
tin Pieler, Alicia Parrish, Aaron Mueller, Ameya
Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross,
Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, De-
rik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe
Cavanagh, Max Weiss, Sicong Huang, The Floating
Droid, Tom Tseng, Tomasz Korbak, Xudong Shen,
Yuhui Zhang, Zhengping Zhou, Najoung Kim, Sam
Bowman, and Ethan Perez. 2023. Inverse scaling:
When bigger isn’t better. ArXiv , abs/2306.09479.Peter Menzies and Helen Beebee. 2024. Counterfac-
tual Theories of Causation. In Edward N. Zalta and
Uri Nodelman, editors, The Stanford Encyclopedia
of Philosophy , Spring 2024 edition. Metaphysics Re-
search Lab, Stanford University.
Joris M. Mooij, J. Peters, Dominik Janzing, Jakob
Zscheischler, and Bernhard Scholkopf. 2014. Distin-
guishing cause from effect using observational data:
Methods and benchmarks. ArXiv , abs/1412.3773.
R.E. Nisbett and L. Ross. 1980. Human Inference:
Strategies and Shortcomings of Social Judgment .
Century psychology series. Prentice-Hall.
Travis Norsen. 2007. John s. bell’s concept of local
causality. American Journal of Physics , 79:1261–
1275.
Judea Pearl. 2009. Causality . Cambridge university
press.
Judea Pearl. 2022. Probabilities of causation: three
counterfactual interpretations and their identification.
InProbabilistic and Causal Inference: The Works of
Judea Pearl , pages 317–372.
Hans Reichenbach. 1956. The direction of time , vol-
ume 65. Univ of California Press.
Abulhair Saparov and He He. 2023. Language models
are greedy reasoners: A systematic formal analysis
of chain-of-thought. In The Eleventh International
Conference on Learning Representations .
Nino Scherrer, Claudia Shi, Amir Feder, and David M.
Blei. 2023. Evaluating the moral beliefs encoded in
llms. ArXiv , abs/2307.14324.
Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat
Lee, Yuanzhi Li, and Yi Zhang. 2023. Positional de-
scription matters for transformers arithmetic. ArXiv ,
abs/2311.14737.
Yoav Shoham. 1987. Reasoning about change: time
and causation from the standpoint of artificial intelli-
gence . Yale University.
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang
Wang, Jianfeng Wang, Jordan L. Boyd-Graber, and
Lijuan Wang. 2022. Prompting gpt-3 to be reliable.
ArXiv , abs/2210.09150.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-
tian Cantón Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V .
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin
Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023. Llama 2: Open foundation
and fine-tuned chat models. ArXiv , abs/2307.09288.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol
Vinyals, Percy Liang, Jeff Dean, and William Fedus.
2022. Emergent abilities of large language models.
ArXiv , abs/2206.07682.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete question
answering: A set of prerequisite toy tasks. arXiv:
Artificial Intelligence .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
and Jamie Brew. 2019. Huggingface’s transformers:
State-of-the-art natural language processing. ArXiv ,
abs/1910.03771.
John Woods and Douglas Walton. 1977. Post hoc, ergo
propter hoc. Review of Metaphysics , 30(4):569–593.
Linying Yang, Oscar Clivio, Vik Shirvaikar, and Fabian
Falck. 2023. A critical review of causal inference
benchmarks for large language models. In AAAI
2024 Workshop on ”Are Large Language Models
Simply Causal Parrots?” .
W. Yu, Meng Jiang, Peter Clark, and Ashish Sabharwal.
2023. Ifqa: A dataset for open-domain question an-
swering under counterfactual presuppositions. ArXiv ,
abs/2305.14010.
M. Zecevic, Moritz Willig, Devendra Singh Dhami, and
Kristian Kersting. 2023. Causal parrots: Large lan-
guage models may talk causality but are not causal.
ArXiv , abs/2308.13067.
Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei
Chang, and Guy Van den Broeck. 2022. On the para-
dox of learning to reason from data. In International
Joint Conference on Artificial Intelligence .
A Appendix
A.1 Additional Details on Synthetic Data
Generation
Generating Causal Graphs. To generate a syn-
thetic causal graph, we generate a directed acyclic
graph with nvertices and rroot vertices. Each
vertex represents an event, and the root verticesAlgorithm 1: Pseudocode to generate synthetic relational
data from causal graph Gcand non-causal relation graph
Gn. The helper-function sample _event _chains is de-
scribed in Algorithm 4.
Input: num_scenarios , set of events E,
causal graph Gc, relation graph Gn
Output: dataset D
1initialize D← {}
2repeat num_scenarios times
/*sample a number of event chains,
where each chain is causally-
independent of the other chains */
3C←sample _event _chains( Gc)
4S← {}
5for each event _chain inCdo
/*sample temporal relations */
6 n∼Binomial (|event _chain|,0.5)
7 sample S, a set of nevents, from event _chain
8 for each XiinSdo
9 sample event Yuniformly from any chain in C
10 ifXiis an ancestor of YinGn
11 S.add(temporal( Xi, Y))
12 else if Yis an ancestor of XiinGn
13 S.add(temporal( Y, X i))
14 else if XiandYido not co-occur in Gn
15 S.add(temporal( Xi, Y)w.p. 0.5, else
temporal( Y, X i))
/*sample spatial relations */
16 n∼Binomial (|event _chain|,0.4)
17 sample S, a set of nevents, from event _chain
18 for each XiinSdo
19 sample event Yuniformly from any chain in C
20 ifY∈event _chain orXi, Yico-occur in Gn
21 S.add(spatial +(Xi, Y))
22 elseS.add(spatial −(Xi, Y))
/*sample counterfactual relations */
23 n∼Binomial (|event _chain|,0.4)
24 sample S, a set of nevents, from event _chain
25 for each XiinSdo
26 Y∼Uniform (event _chain \ {Xi})
27 ifXiis an ancestor of YinGc
28 S.add(counterfactual +(Xi, Y))
29 elseS.add(counterfactual −(Xi, Y))
/*sample negative counterfactuals */
30 n∼Binomial (|event _chain|,0.2)
31 sample S, a set of nevents, from event _chain
32 for each XiinSdo
33 sample event Yuniformly from any chain in C
34 ifXiis an ancestor of YinGc
35 S.add(counterfactual +(Xi, Y))
36 elseS.add(counterfactual −(Xi, Y))
37D.add( S)
are those that have no causes (i.e. they have no
incoming edges). The algorithm to generate such
a graph is shown in Algorithm 2. The algorithm is
fairly simple, but we take care not to create vertices
that are descendants of all roots, since they will
be causally connected to every root, and therefore,
they would never be sampled in any event chain in
Algorithm 1. In addition, we require that every root
has at least one child, in order to prevent generat-ing trivial event chains that contain only a single
event. In our experiments, we fix n= 100 , and r
is sampled from Geometric (0.64)conditioned on
r∈[3,6].
Algorithm 2: Pseudocode for generating a synthetic causal
graph.
Input: number of vertices n, number of roots r
Output: causal graph Gc
1initialize Gcas a graph with nvertices and no edges
2let(v1, . . . , v n)be the vertices of Gc
3foriinr+ 1, . . . , n do
4m∼Zipf(3)
5m←min(i, m)
6sample P, a set of mvertices from {v1, . . . , v i−1},
uniformly without replacement
7forpinPdo
8 add edge p→vtoGc
9 ifvis a descendant of all roots v1, . . . , v r
10 remove edge p→vfromGc
/*make sure each root has ≥1 child */
11forviin{v1, . . . , v r}do
12 ifvihas no child vertices
13 v∼Uniform (vr+1, . . . , v n)
14 add edge vi→vtoGc
15shuffle the vertices (v1, . . . , v n)
Generating Non-causal Relation Graphs. Al-
gorithm 3 describes how we generate non-causal
relations for the events in the causal graph. The
output is a graph Gnwhere each vertex represents
atypeof event, and the function constructs a map T
from events in Gcto event types in Gn. We chose
simple semantics for Gn: If two events have the
same type, they co-occur. An edge a→binGn
from event type ato event type bindicates that all
events of type aprecede events of type b.
Sampling Event Chains. Algorithm 4 describes
the helper function used in Algorithm 1 which sam-
ples a handful of event chains, where each chain
is causally-independent of the other event chains.
In this helper function, each event chain starts at a
root node in Gc, since root nodes are by definition
causally-independent of each other. We sample the
length of each chain to be uniform so that vertices
near roots are not over-represented in the sample
of event chains (and vertices further from the roots
are not under-represented). This helps to facilitate
more uniform coverage of all vertices in Gcby the
generated data.
Generating Scenarios. Algorithm 1 gives the
data generation algorithm for generating the sce-
narios. In each step, when we sample S, a set ofAlgorithm 3: Pseudocode for generating a synthetic non-
causal relation graph.
Input: causal graph Gc
Output: non-causal relation graph Gn
1let(t1, . . . , t k)be (an initially empty) ordered list of
event types
2letTbe an initially empty map from events in Gcto
event types {t1, . . . , t k}
3for each event vinGcdo
/*assign an event type to each event in Gc*/
4compute α= max {i:
there is an ancestor aofvsuch that T(a) =ti}
5compute β= min {i:
there is a descendant dofvsuch that T(d) =ti}
6ifα < β
7 w∼Uniform (tα+1, . . . , t β−1)
8else
9 create new event type wand insert it into the list
of event types at index α+ 1
10 setT(v)←w
11let(t1, . . . , t k)be the vertices of Gn
/*add temporal edges between event types */
12for each event vinGcdo
13 for each child vertex cofvdo
14 add edge T(p)→T(c)toGn
Algorithm 4: Pseudocode for the helper-function
sample _event _chains , which, given a causal graph Gc,
returns a number of event chains, where each chain is
causally-independent of the other chains.
Input: causal graph Gc
Output: set of event chains C
1initialize C← {}
2n∼1 +Geometric (0.25)
3sample R, a set of nroot vertices from Gc(with no
incoming edges), uniformly without replacement
/*for each root, sample a chain */
4for each rinRdo
5compute Dr, the set of descendant vertices of r
/*sample the length of this chain */
6m∼Uniform (1, . . . , max v∈Drdistance (r, v))
7compute Sr,m={v∈Dr:distance (r, v) =m
andvis not a descendant of R\ {r}}
8while Sr,mis empty do
9 m←m−1
10 recompute Sr,mas above
/*sample the endpoint of the chain */
11e∼Uniform (Sr,m)
12C.add( set of all vertices on path from rtoe)
/*mark some chains as ‘non-occurring’ */
13k∼Binomial (n−1,0.2)
14remove kevent chains from C, uniformly at randomnevents from the event _chain we sample uni-
formly randomly without replacement. This en-
sures that scenarios contain information about a
diverse set of events.
We also include an example from our generated
dataset, where the scenario contains all three rela-
tions in Table 3.
A.2 Experiment Details
We used LLAMA 2models through HuggingFace’s
transformer library (Wolf et al., 2019). All models
were finetuned with LoRA (applied to query and
key projection matrices), with rank = 16 ,α= 16
and dropout = 0.05. All models were finetuned
with a learning rate of 5e−4using AdamW opti-
mizer(Kingma and Ba, 2015; Loshchilov and Hut-
ter, 2017) with a batch size of 8. The models
finetuned on 36k scenarios were trained for 10k
steps whereas the models trained with 4.5k sce-
narios (500 scenarios used for validation, as in
Appendix A.5) were trained for 6k steps — we
generally observed that models converged around
this point.
A.3 Additional Results: Position Bias
Temporal Relations. Section 5.1 showed that,
in the presence of strong position bias, the model
assigned high probability to ti(X→Y)where
the relative position matches that during finetuning.
This still leaves open the possibility that the model
is assigning a higher probability to the template for
correct causal relation where the position matches.
e.g. from ‘ Xpreceded Y’, the model could assign
probabilities in the following order — ‘ Xcan cause
Y’>‘Xcan be caused by Y’>‘Ycan be caused
byX’>‘Ycan cause X’. In such a situation,
if the order is randomized during evaluation the
model can still infer causal relations from temporal
relations.
In this experiment, we find that models finetuned
on temporal relations with relative position (X, Y)
infer X→Yfrom temporal (X, Y)23.14% of
the times. Since random chance is 33.3%, we see
that models finetuned on position bias indeed are
not able to make any consistent deduction beyond
matching relative position during finetuning and
evaluation.
To further show that models are only relying on
the relative position of events instead of reason-
ing about their causal relation, we evaluate mod-
els using different relations with the same relative
position. Specifically, we randomly sample threerelations between XandYwhich have no connec-
tion to the causal relation and verbalize them using
the(X, Y)relative order e.g. instead of the verbal-
ization ‘ Xcauses Y’, we will use ‘ Xis related to
Y’ (details in Appendix A.7). We observe a simi-
lar result in the last two rows in Table 4—models
only make correct predictions when the event order
during training matches that during test.
Spatial Relations. Here, we demonstrate that
we also observe the position bias for spatial re-
lations. To show this we first create a dataset
with fixed relative position. Specifically, we gen-
erate a dataset Dspatial ,(X,Y )consisting of posi-
tive and negative spatial relations from the sets
T(spatial+,(X, Y))andT(spatial−,(X, Y))
respectively. We then finetune LLAMA 2-7B on
this data and evaluate the model on Dunrelated X−Y.
We use two different sets of templates to evaluate
the model: T(X→Y,(X, Y))(e.g. ‘ Xcauses
Y’) or using templates from T(X→Y,(Y, X))
(e.g. ‘ Yis caused by X’). In both cases, to score
the relation X↮Ywe use T(X↮Y,(X, Y) +
(Y, X)).
Table 5 shows the percentage of examples in
which the model predicted either X→Yor
X↮Y(which is the correct option). Firstly,
we observe that in both cases, the model rarely se-
lects the correct option X↮Y. Similar to the
position bias in temporal relations, the model se-
lects either X→Ydepending on if the position
matches. This shows that position bias also exists
for spatial relations. We also evaluate the model
using templates which have randomized relative
position for each option. Specifically, we use tem-
plates from the sets T(r,(X, Y) + (Y, X))where
r∈ {X→Y, Y→X, X↮Y}. We find that
model selects the correct option ( X↮Y), 68%
of the time. This is in contrast to the position bias
in temporal relations, where the performance was
close to random chance. Nevertheless, the model
still performs worse than if the position was ran-
domized in the finetuning data (84.5%, Table 2)
In summary, we find that the position bias also
holds true for spatial relations, albeit to a lesser
extent than that for temporal relations.
A.4 Position heuristic is supported in the
pretraining data
Section 5.1 demonstrated that LLMs fail to infer
causal relations if the finetuning data supports the
position heuristic. We hypothesize that this phe-All Relations event84 happened. event76 happened. event76 and event84 took place in the same location. if event76
did not happen, and event84 has no other causes, would event84 happen? yes. if event76 has no other
causes, and event84 did not occur, would event76 still happen? no. event5 happened. event3 happened.
event96 happened. event3 happened after event84. event5 happened before event3. the location of
event96 is not identical to that of event76. if event3 did not happen, and event5 has no other causes,
would event5 happen? yes.
Temporal Relations event67 occurred prior to event71. event40 happened before event28. event7 preceded event28.
event71 happened after event95.
Spatial Relations the location of event96 is not identical to that of event4. event4 and event96 did not take place in the
same location.
Counterfactuals if event33 did not occur, and event84 has no other causes, would event84 still happen? yes. if event84
has no other causes, and event58 did not occur, would event84 still happen? yes. if event58 has only
one cause, and hypothetically event84 did not happen, would event58 still occur? no. if event3 has
only one cause, and event48 did not happen, would event3 happen? yes.
Table 3: Examples of the scenarios from our generated dataset. The first examples contains all types of relations,
whereas the others include one type of relation only.
DataRel. position Rel. position in eval
in train (X, Y ) ( Y, X)
causal X→Y(X, Y ) 92.59% 1.85%
(Y, X) 0% 100%
unrelated X, Y(X, Y ) 98.14% 0.92%
(Y, X) 0% 100%
Table 4: Accuracy of models finetuned on temporal
relations with different relative event positions. Models
infer the causal relation only when the relative position
matches during finetuning and evaluation.
Rel. position Rel. position - eval
during train (X, Y ) ( Y, X)
Accuracy (X, Y ) 90.5%/3.0% 6.5%/3.5%
Table 5: Models finetuned on spatial relations with fixed
relative position, and we report % of cases model infer
X→Y/ % of cases model infers X↮Y. Models
infer the causal relation only when the relative position
matches during finetuning and evaluation.
nomenon occurs since the position heuristic is sup-
ported in the pretraining data — if cause is often
mentioned before effect in the text, then LLMs
can use relative position as a heuristic for the lan-
guage modeling task. E.g. for the causal rela-
tion ‘smoking causes cancer’, we hypothesize that
‘smoking’ usually occurs before ‘cancer’ if they
co-occur within a window. Thus a LLM trained on
such data can do well even if it only uses the heuris-
tic of relative position to predict the next word and
ignore the relation between the two events.
To test if this holds true in the pretraining data,
for a given causal relation X→Y, we count the
number of times Xoccurs before or after Yin a
context window. We expect that if the heuristic is
supported in the pretraining data, then Xshouldmostly occur before Ywhen they co-occur in a
context window.
We first create a set of 40 commonly-queried
causal relations (e.g. smoking causes cancer, bac-
teria causes infections, etc.) based on the edges
from the CauseNet dataset (Heindorf et al., 2020),
the Tubingen dataset (Mooij et al., 2014) as well as
some candidates from GPT-4. Then for each of the
causal relations X→Y, we count the number of
documents of the PILE10corpus (Gao et al., 2020)
in which either Xoccurs before YorYoccurs be-
foreXwithin a window of 50 characters of the first
mention of XandYin the document. We filter to
keep only those edges where the events co-occur
within the context window at least 100 times. See
Appendix A.8 for details.
Across all causal relations, we find that when-
everX,Yco-occur within the context window,
60.77% of the times Xoccurs before Y. Overall,
we observe that the data supports the heuristic in a
majority (> 50%) of the examples.
A.5 Additional Results: Frequency vs
Position Bias
We also observe an interesting trend where models
exhibit a stronger position bias for relations that
are more frequent in the finetuning data. To show
this, we first create a smaller dataset by sampling
5k examples from Dtemporal ,(X,Y )— 4.5k for fine-
tuning, 500 for evaluation — and finetune for fewer
steps. We split the test set DX→Yinto 10 equal
sized buckets based on the frequency of the corre-
sponding temporal relation, temporal (X, Y), in
Dtemporal ,(X,Y ).
10The pretraining dataset for LLAMA 2-7B is not available,
so we use PILE and assume that relative positions would be
similar.0 2 4 6 8 100.2
0.00.20.40.60.8Difference in Accuracy
More frequent 
  Less frequent
Figure 6: Difference in accuracy on the test sets with
matched and unmatched event orders as a function of
the frequency of the relation in the data. LLMs suffer
from the position bias on high frequency events.
Rel. position Rel. position - eval
during train (X, Y ) ( Y, X)
Three-way eval(X, Y ) 52.77% 35.18%
(Y, X) 3.70% 94.44%
Table 6: Models finetuned on 5k scenarios with tempo-
ral relations with different relative positions. We only
observe the position effect in one direction (when fine-
tuned on (Y, X)) but not the other.
Figure 6 shows the result where the X-axis is
the frequency buckets, and Y-axis is the difference
in accuracy between the test set with matched and
unmatched X-Y orders. We observe that high fre-
quency relations are correlated with a larger gap.
We also report the absolute accuracy when the
model is trained on the smaller finetuning dataset
with 4.5k scenarios. As shown before, in this case
we observed the position bias for high frequency
relations. In Table 6, we report the avg accuracy
of models inferring X→Yfor both relative po-
sitions. We observe a stronger position effect in
one direction (when trained with relative position
(Y, X)) but not as much in the other direction. Note
that the model performance when trained with rela-
tive position (X, Y)is not much better than chance
and is also sensitive to the relative position.
A.6 Additional Results: Alternate evaluation
ofX→Y
In Section 3.2 to evaluate models, we first com-
pute the probabilities of the following five relations
under the language model: X→Y,Y→X,
X̸→Y,Y̸→X, and X↮Y. To test if models
have inferred the causal relation X→Y, we com-DcausalX−Y
temporal (X, Y ) =⇒X→Y 71.29%
counterfactual +(X, Y ) =⇒X→Y 54.62%
Table 7: Alternative Evaluation: Using a different set
of exhaustive and disjoint events does not change our
conclusions — model suffer from post hoc fallacy, and
they cannot infer presence of causal relation from coun-
terfactual.
pare the probabilities of the following three events
which are exhaustive (i.e. their true probabilities
sum to 1) and disjoint: X→Y,Y→X, and
X↮Y.
An alternative set of events which are also ex-
haustive and disjoint are: X→Y, andX̸→Y. In
this section, we demonstrate that our conclusion of
whether models infer X→Yremains consistent
even if we use these two events as the set of events
to compare.
To show this, we re-evaluate two mod-
els: LLAMA 2-7B finetuned on Dtemporal , and
Dcounterfactual respectively. We then evaluate
these models on Dcausal X−Yto test if they infer
presence of causal relations from either temporal
relations or positive counterfactuals.
Table 7 shows the percentage of examples where
model predicts the causal relation X→Y. First,
we observe that models infer causal relations from
temporal relation — i.e. temporal(X, Y) =⇒
X→Y. Therefore, similar to our previous find-
ings where models suffer from post hoc fallacy
(Section 7), changing how we evaluate the pres-
ence of causal relation does not affect our results.
Similarly, we observe that models cannot infer pres-
ence of causal relations from counterfactuals much
better than random chance (50%). This is consis-
tent with our finding from Section 6, where we
showed that the model cannot infer causal relations
from positive counterfactuals.
A.7 Templates for Relations
In this section, we list the templates we use for each
of the three relations: temporal relations, spatial re-
lations, and counterfactuals. Additionally, we also
describe the templates we used for causal relations
(both presence and absence of causal relations).
Each template is separated by ‘;’.
1.T(temporal (X, Y),(X, Y)):Xpreceded
Y;Xhappened before Y;Xoccurred prior
toY;Xtook place before Y;XhappenedthenYhappened
2.T(temporal (X, Y),(Y, X)):Yfollowed X;
Yhappened after X;Yoccurred later than
X;Ytook place after X;Yhappened later
thanX
3.T(temporal (X, Y),random ):Xpreceded
Y;Yfollowed X;Xoccurred prior to Y;
Yhappened after X;Yoccurred later than
X;Xhappened before Y
4.T(spatial+(X, Y),random ):XandY
took place in the same location; the location
ofXis identical to that of Y;XandYhap-
pened in the same place; YandXtook place
in the same location; the location of Yis iden-
tical to that of X;YandXhappened in the
same place
5.T(spatial−(X, Y),random ):XandYdid
not take place in the same location; the loca-
tion of Xis not identical to that of Y;Xand
Ydid not happen in the same place; Yand
Xdid not take place in the same location; the
location of Yis not identical to that of X;Y
andXdid not happen in the same place
6.T(counterfactual +(X, Y),random ): ifX
did not happen, and Yhas no other causes,
would Xhappen? no; if Yhas only cause,
andXdid not happen, would Yhappen? no;
ifXdid not occur, and Yhas no other causes,
would Ystill happen? no; if Yhas no other
causes, and Xdid not occur, would Ystill
happen? no; if hypothetically Xdid not hap-
pen, and Yhas only cause, would Ystill oc-
cur? no; if Yhas only cause, and hypotheti-
callyXdid not happen, would Xstill occur?
no;
7.T(counterfactual −(X, Y),random )ifX
did not happen, and Yhas no other causes,
would Xhappen? yes; if Yhas only cause,
andXdid not happen, would Yhappen? yes;
ifXdid not occur, and Yhas no other causes,
would Ystill happen? yes; if Yhas no other
causes, and Xdid not occur, would Ystill
happen? yes; if hypothetically Xdid not hap-
pen, and Yhas only cause, would Ystill oc-
cur? yes; if Yhas only cause, and hypotheti-
callyXdid not happen, would Xstill occur?
yes;8.T(X→Y,random ):Xcan cause Y;Ycan
be caused by X;Xcausally affects Y;Xcan
lead to Y;Yis causally affected by X;Yis
caused by X
9.T(X̸→Y,random ):Xcannot cause Y;Y
cannot be caused by X;Xdoes not causally
affects Y;Xcannot lead to Y;Yis not
causally affected by X;Yis not caused by
X
10.T(X↮Y,random ): ‘there is no causal re-
lation between XandY’, ‘there is no causal
relation between YandX’, ‘there is no depen-
dency between XandY’, ‘there is no depen-
dency between YandX’, ‘there is no causal
link between XandY’, ‘there is no causal
link between YandX’, ‘Xneither causes
nor is caused by Y’, ‘Yneither causes nor is
caused by X’, ‘there is no cause-and-effect
relationship between XandY’, ‘there is no
cause-and-effect relationship between Yand
X’, ‘there is no causal association linking X
andY’, ‘there is no causal association linking
YandX’
A.8 Position Heuristic in PILE
For searching through the pretraining data, we used
the PILE corpus since it’s freely available and has
been used in recent models e.g. Pythia models (Bi-
derman et al., 2023). Here, we list the 40 causal
relations we used to search over the PILE corpus.
We set the parameter wto be 50 characters i.e. the
events are said to co-occur if they occur within
50 characters of each other. We filter to keep
only those edges where the events co-occur enough
times in the pretraining data (we set it to 100) —
this is done to ensure that results are not affected by
causal relations where the events do not frequently
co-occur.
[(’bacteria’, ’infections’),
(’hiv’, ’aids’),
(’cancer’, ’death’),
(’smoking’, ’lung cancer’),
(’altitude’, ’temperature’),
(’age’, ’height’),
(’sun exposure’, ’aging’),
(’sugar’, ’tooth decay’),
(’drugs’, ’organ damage’),
(’salt’, ’high blood pressure’),
(’screens’, ’eye strain’),
(’lack of sleep’, ’impaired cognition’),(’pollution’, ’lung harm’),
(’noise’, ’hearing loss’),
(’genetics’, ’height’),
(’dehydration’, ’fatigue’),
(’sugar’, ’diabetes’),
(’stress’, ’headache’),
(’poor nutrition’, ’fatigue’),
(’sedentary habits’, ’obesity’),
(’education’, ’income’),
(’physical activity’, ’health’),
(’parental involvement’, ’child development’),
(’nutrition’, ’longevity’),
(’financial stress’, ’mental health’),
(’pollution’, ’health problems’),
(’stress’, ’immune function’),
(’education’, ’political participation’),
(’drugs’, ’crime rate’),
(’deforestation’, ’climate change’),
(’fossil fuels’, ’climate change’),
(’greenhouse gases’, ’climate change’),
(’accident’, ’death’),
(’stroke’, ’death’),
(’diabetes’, ’death’),
(’migraine’, ’headache’),
(’smoking’, ’house fires’),
(’infidelity’, ’divorce’),
(’poverty’, ’homelessness’),
(’drunk driving’, ’accident’)]