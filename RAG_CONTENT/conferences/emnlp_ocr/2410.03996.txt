On the Influence of Gender and Race in
Romantic Relationship Prediction from Large Language Models
Abhilasha Sancheti∗Haozhe An∗Rachel Rudinger
University of Maryland, College Park
{sancheti, haozhe, rudinger}@umd.edu
Abstract
We study the presence of heteronormative bi-
ases and prejudice against interracial romantic
relationships in large language models by per-
forming controlled name-replacement experi-
ments for the task of relationship prediction.
We show that models are less likely to pre-
dict romantic relationships for (a) same-gender
character pairs than different-gender pairs; and
(b) intra/inter-racial character pairs involving
Asian names as compared to Black, Hispanic,
or White names. We examine the contextual-
ized embeddings of first names and find that
gender for Asian names is less discernible than
non-Asian names. We discuss the social impli-
cations of our findings, underlining the need
to prioritize the development of inclusive and
equitable technology.
1 Introduction
Identifying romantic relationships from a given
dialogue presents a challenging task in natural lan-
guage understanding (Jia et al., 2021; Tigunova
et al., 2021). The perceived gender, race, or eth-
nicity of the speakers, often inferred from their
names, may inadvertently lead a model to predict
a relationship type that conforms to conventional
societal views. We hypothesize that, when predict-
ing romantic relationships, models may mirror het-
eronormative biases (Pollitt et al., 2021; Vásquez
et al., 2022) and prejudice against interracial ro-
mantic relationships (Lewandowski and Jackson,
2001; Miller et al., 2004) present in humans and
society. Heteronormative biases assume and favor
traditional gender roles, heterosexual relationships,
and nuclear families, often marginalizing other gen-
der expressions, sexuality, and family dynamics. In
the US, legal protections for interracial and gay
marriages were not achieved nationwide until 1967
*These authors contributed equally to this work.
Figure 1: Sample conversation from DDRel (Jia
et al., 2021) dataset and relationships predicted by
Llama2-7B when characters are replaced by names with
different-gender andsame-gender . LLM tends to pre-
dict differently despite the same conversation.
and 2015, respectively. These relationships con-
tinue to face prejudice and discrimination in the
present days (Buist, 2019; Knauer, 2020; Zambelli,
2023; Pittman et al., 2024; Daniel, 2024).
In this paper, we consider the task of predict-
ing romantic relationships from dialogues in movie
scripts to study whether LLMs make such predic-
tions based on the demographic attributes associ-
ated with a pair of character names, in ways that re-
flect heteronormative biases and prejudice against
interracial romantic relationships . For instance,
Figure 1 shows a conversation between a female
and a male spouse pair, for which Llama2-7B pre-
dicts a romantic relationship when the names in the
conversation are replaced with a pair of different-
gender names, but predicts a non-romantic relation-
ship when replaced by same-gender names.
Ideally, name-replacement should not signifi-
cantly alter the predictions of a fair and robust
model, as the utterance content plays a more sub-
stantial role in language understanding, despite the
potential interdependence between utterances and
original names. Different predictions suggest that
a model may be prone to overlooking romantic re-
lationships that diverge from societal norms, thus
raising ethical concerns. Such behavior would indi-
cate that language models inadequately represent
certain societal groups (Blodgett et al., 2020), po-
tentially exacerbating stigma surrounding relation-arXiv:2410.03996v1  [cs.CL]  5 Oct 2024ships (Rosenthal and Starks, 2015; Reczek, 2020)
and sidelining underrepresented groups (Nozza
et al., 2022; Felkner et al., 2023).
Through controlled character name-replacement
experiments, we find that relationships between
(a) same-gender character pairs; and (b) intra/inter-
racial character pairs involving Asian names are
less likely to be predicted as romantic. These find-
ings reveal how some LLMs may stereotypically
interpret interactions between people, potentially
reducing the recognition of non-mainstream rela-
tionship types. While prior work studies gender
and racial biases by identifying stereotypical at-
tributes of individuals (Cao et al., 2022; Cheng
et al., 2023; An et al., 2023), this paper investigates
the role of gender and race in LLMs’ inferences
about relationships between twoindividuals using
a relationship prediction dataset (Jia et al., 2021).
2 Experimental Setup
We define the following task. Given a conver-
sation Cwhich consists of a sequence of turns
((S1, u1),(S2, u2), . . . , (Sn, un))between charac-
tersAandB, where Si∈ {SA, SB}indicates
that the speaker of an utterance ( ui,i∈ {1 :n})
is either AorB, the task is to identify the rela-
tionship represented as a categorical label from a
pre-defined set. We carry out controlled name-
replacement experiments by prompting LLMs
(zero-shot) to predict the relationship type between
AandBgiven C.
Models We study Llama2 ({7B, 13B}-chat) (Tou-
vron et al., 2023) with its official implementation,1
and Mistral-7B-Instruct (Jiang et al., 2023) using
its huggingface implementation. Hyperparameters
are specified in §A.
Dataset We use the test set of DDRel (Jia et al.,
2021) which consists of movie scripts from IMSDb,
with annotations for relationship labels between the
characters according to 13pre-defined types (Ta-
ble 3 in appendix). We consider Lovers, Spouse,
or Courtship predictions as romantic and the rest
as non-romantic. For our experiments, we use 327
instances of the test set in which characters origi-
nally have different genders (manually annotated)
because the test set has no dialogues between same-
gender characters with the romantic label. We dis-
cuss the limitations of this study due to data source
representation issues at the end of this paper.
1https://github.com/facebookresearch/llamaPrompt Selection As LLMs are sensitive to
prompts (Min et al., 2022), we experimented with
several prompt formulations on the original data
(test set) for accuracy, and selected the prompt (see
Figure 4 in appendix) resulting in the highest ac-
curacy which was closest to scores reported by
others (Jia et al., 2021; Ou et al., 2024). We note
that our prompt selection is done prior to running
the name-replacement experiments.
Evaluation We compare the average recall of
predicting romantic relationships across different
gender assignments and races/ethnicities. We study
recall as we hypothesize heteronormative and in-
terracial relationship biases would manifest as low
(romantic) recall for same-gender and interracial
groups. For completeness, we also report the mean
precision, F1, and accuracy scores in §D.
2.1 Studying the Influence of Gender Pairings
We ask whether the models are equally likely to
recognize romantic relationships for character pairs
of varying gender assignments and if this behavior
is the same across different races. We hypothesize
that models are prone to heteronormative bias and
are more likely to predict romantic relationships
for contrastive gender assignments. To test this,
we collect 30names per race,2dividing them into
10non-linearly segmented bins that cover gender-
neutral names (shown in Figure 2) based on the
percentage of population that has been assigned as
female at birth. Detailed name inclusion criteria
and data sources are elaborated in §C.1. We replace
the original name-pair in each conversation with
all pairs of distinct names per race.
As dialogues may reveal gender identities ( e.g.,
“sir”, “ma’am”, “father”, etc.), we manually identify
a subset ( 271instances) where such explicit cues
are absent (to the best of our judgement) to mini-
mize gender information leakage and avoid explicit
gender inconsistency between the dialogue and the
gender associated with the replaced name. In these
dialogues, gendered pronouns typically refer to a
third person who is not part of the conversation. As
a result, they do not reveal the speakers’ gender
identity. However, pronouns can indicate the sex-
ual orientation of a speaker ( e.g., “Betty: You do
love him, don’t you? ”). Such cues, along with other
implicit cues about gender identity that are harder
to detect, may confound our analysis. However, our
2Except for Hispanic wherein we did not get any names in
5−10% bin and only 1 name in 25−50% bin.0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.56 0.55 0.59 0.58 0.61 0.59 0.69 0.68 0.64 0.72
0.56 0.52 0.57 0.58 0.59 0.58 0.65 0.66 0.63 0.69
0.61 0.58 0.63 0.60 0.64 0.62 0.70 0.69 0.67 0.72
0.58 0.57 0.59 0.60 0.62 0.59 0.67 0.66 0.63 0.70
0.61 0.60 0.64 0.62 0.65 0.62 0.69 0.69 0.66 0.69
0.59 0.56 0.61 0.60 0.61 0.60 0.66 0.64 0.62 0.67
0.66 0.64 0.67 0.66 0.67 0.64 0.69 0.66 0.65 0.67
0.67 0.65 0.68 0.66 0.68 0.64 0.67 0.67 0.64 0.67
0.64 0.61 0.65 0.63 0.65 0.61 0.65 0.63 0.62 0.65
0.70 0.66 0.70 0.68 0.68 0.66 0.68 0.67 0.65 0.68
Male Neutral FemaleMale Neutral FemaleAsian (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.60 0.61 0.61 0.60 0.66 0.70 0.65 0.74 0.81 0.79
0.60 0.60 0.60 0.60 0.65 0.69 0.64 0.71 0.79 0.76
0.62 0.62 0.62 0.61 0.66 0.71 0.65 0.73 0.79 0.76
0.61 0.61 0.60 0.61 0.64 0.68 0.63 0.70 0.76 0.74
0.67 0.66 0.66 0.64 0.60 0.65 0.64 0.67 0.74 0.71
0.71 0.69 0.70 0.67 0.66 0.75 0.67 0.69 0.74 0.71
0.65 0.64 0.66 0.63 0.65 0.68 0.62 0.66 0.70 0.68
0.74 0.70 0.72 0.68 0.67 0.69 0.65 0.64 0.68 0.62
0.80 0.77 0.78 0.75 0.73 0.73 0.69 0.67 0.67 0.65
0.78 0.74 0.74 0.72 0.69 0.71 0.66 0.62 0.66 0.61
Male Neutral FemaleBlack (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.59 0.60 0.62 0.71 0.76 0.68 0.80 0.85 0.85
0.59 0.60 0.62 0.69 0.72 0.64 0.77 0.81 0.81
0.62 0.64 0.66 0.70 0.75 0.68 0.78 0.83 0.84
0.66 0.65 0.66 0.67 0.66 0.71 0.76 0.75
0.74 0.72 0.73 0.67 0.68 0.69 0.70 0.75 0.72
0.65 0.64 0.65 0.68 0.70 0.68 0.73 0.80 0.79
0.76 0.75 0.76 0.71 0.68 0.72 0.78 0.76 0.72
0.82 0.82 0.82 0.76 0.75 0.79 0.77 0.83 0.78
0.82 0.80 0.81 0.74 0.70 0.78 0.73 0.77 0.75
Male Neutral FemaleHispanic (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.52 0.62 0.56 0.60 0.73 0.62 0.76 0.84 0.80 0.86
0.61 0.69 0.63 0.63 0.76 0.66 0.78 0.84 0.80 0.87
0.57 0.65 0.59 0.61 0.73 0.62 0.73 0.83 0.77 0.83
0.62 0.64 0.61 0.64 0.75 0.64 0.76 0.84 0.77 0.85
0.73 0.77 0.73 0.74 0.73 0.68 0.70 0.75 0.69 0.76
0.61 0.65 0.61 0.63 0.68 0.61 0.66 0.75 0.69 0.77
0.74 0.77 0.71 0.74 0.68 0.65 0.66 0.70 0.63 0.69
0.83 0.84 0.81 0.83 0.75 0.73 0.69 0.75 0.66 0.69
0.81 0.81 0.75 0.77 0.69 0.68 0.63 0.65 0.60 0.62
0.86 0.86 0.81 0.82 0.74 0.75 0.67 0.67 0.63 0.61
Male Neutral FemaleWhite (Recall)
0.550.600.650.700.750.800.85Figure 2: Recall of predicting romantic relationships from Llama2-7B for subset of the dataset where characters
originally have different genders. Horizontal and vertical axes denote % female of the name replacing an originally
female and male character name from the dialogue. The upper-triangle (lower-triangle) shows the scores when
names are replaced preserving (swapping) the genders of characters’ names as-is in the original conversation. We
consider the names with lesser % female as male names for determining gender preservation for name-replacement.
findings as discussed in §3 reveal that implicit cues
are not a major confounding factor. We discuss this
aspect further in the Limitations section.
2.2 Studying Intra/Inter-Racial Pairings
We examine whether the models exhibit preju-
dice against interracial romantic relationships when
making predictions. We collect another set of
80first names that are both strongly race- and
gender-indicative, evenly distributed among four
races/ethnicities and two genders (details described
in §C.2). We perform pairwise name-replacements
using these 80names for the 327test samples to an-
alyze the relationship predictions among different
intra/inter-racial name pairs.
We defer details related to full prompt used and
model output parsing to §A.
3 Findings
Same-gender relationships are less likely to be
predicted as romantic than different-gender
ones. We observe a significant variation in recall
of romantic relationship predictions from Llama2-
7B (see Figure 2) for name-replacements involving
different (top-right, and bottom-left)- versus same-
gender pairs. This reveals that the model conser-
vatively predicts romantic relationships when both
the characters have names associated with the same
gender (top-left – both male; bottom-right – both
female). However, the precision across all races
ranges between 0.78−0.84(see Figure 5 in ap-
pendix). Such (relatively) low difference indicates
that, while the model makes precise romantic pre-
dictions across all gender assignments and races,
romantic predictions are more likely for contrastive
gender assignments. Higher recall (Figure 2) for
both female (bottom-right) replacements than bothmale (top-left) across all races indicates a poten-
tialstronger heteronormative bias against both
male than both female pairs . This could poten-
tially be an effect of associating female names with
romantic relationships as indicated by higher recall
for female-neutral than male-neutral pairs. To test
this hypothesis, we substitute one speaker’s name
with a male, female or neutral name while keeping
the other anonymized (substituting with “X”). We
find that name pairs containing one female name
tend to have higher recall than those containing one
male name (Table 4 in appendix). This could either
be due to a stronger association of female names
with romantic relationships in general, or stronger
heteronormative bias against male-male romantic
relationships if models are (effectively) marginaliz-
ing probabilities over the anonymous character. A
possible explanation for the former is that women
tend to be portrayed only as objects of romance
in fictional works, e.g., as popularly evidenced by
the failure of many movies to pass the Bechdel
test (Agarwal et al., 2015).
The smaller gap in the recall between both
female (bottom-right) name-replacements and
different-gender (top-right and bottom-left) ones
for Asian and Hispanic as compared to White and
Black may result from model’s inability to discern
gender from Asian and Hispanic names as accu-
rately as for White and Black names. Figures 6
and 7 (appendix) show similar trends for Llama2-
13B and Mistral-7B, respectively.
The unnaturalness of movie scripts with name
and gender substitutions could, in theory, pro-
vide an alternative explanation for the observed
biases, but the evidence shows this is not the
cause. As female characters may speak differ-Asian Black Hispanic White
Male RaceAsian Black Hispanic WhiteFemale Race0.68 0.72 0.72 0.70
0.78 0.83 0.84 0.84
0.82 0.87 0.87 0.87
0.79 0.84 0.85 0.85Recall
0.7000.7250.7500.7750.8000.8250.850
Figure 3: Recall of predicting romantic relationships
from Llama2-7B for subset of the dataset where charac-
ters have different genders and are replaced with names
associated with different races/ethnicities.
ently from male characters, our name-replacements
can introduce statistical inconsistency between the
gender associated with a character name and the
style or content of the lines they speak, potentially
confounding our observations. However, compa-
rable recall between name-replacements that pre-
serve the gender (upper-triangle; specifically top-
right) associated with the original speakers and
the swapped variants (lower-triangle; specifically
bottom-left) in Figure 2, indicates that swapping
both characters’ genders has minimal impact on
model’s performance in the conversations we used.
Hence, we conclude the potential inconsistency be-
tween gender and linguistic content is not a major
confounding factor.
Character pairs involving Asian names have
lower romantic recall; however, we do not
find strong evidence against interracial pairings.
While Llama2-7B has similar precision of predict-
ing a romantic relationship across all racial pairs
(0.80–0.82, shown in Figure 8 in appendix), Fig-
ure 3 shows name pairs involving at least one Asian
name have significantly lower recall. Noticeably,
the recall is the lowest ( 0.68) when both charac-
ter names are associated with Asian. Although
there are variations in recall values among different
racial setups, we do not observe disparate differ-
ences between interracial and intraracial name pairs
for non-Asian names. Results for Llama2-13B and
Mistral-7B, shown respectively in Figure 9 and 10
in the appendix, demonstrate a similar trend that
Asian names lead to substantially lower recall val-
ues. Such systematically worse performance on
Asian names potentially perpetuates known algo-
rithmic biases (Chander, 2016; Akter et al., 2021;
Papakyriakopoulos and Mboya, 2023).Race/Ethnicity Asian Black Hispanic White
GenderLogistic regression 53.3±12.7 96.4±2.9 80.5 ±13.0 99.9 ±0.2
Majority baseline 54.2±0.0 54.2 ±0.0 54.2 ±0.0 53.9 ±0.3
RaceLogistic regression 97.6±1.9 70.5 ±6.3 89.5 ±4.1 94.2 ±3.8
Majority baseline 50.6±0.2 50.6 ±0.4 50.9 ±0.4 50.9 ±0.3
Table 1: Logistic regression classification accuracy (%)
of predicting the demographic attributes associated with
a name from Llama2-7B contextualized embeddings.
4 Analysis and Discussion
We perform additional experiments to understand
the observed model behavior.
Why does a model tend to predict fewer roman-
tic relationships for racial pairings that involve
Asian names? Although we select names for
each race that have strong real-world statistical as-
sociations with one gender, we hypothesize that low
recall on pairs with one or more Asian names may
be due to model’s inability to discern gender from
Asian names. To test this hypothesis, we retrieve
the contextualized embeddings from Llama2-7B
for each first name (collected in §2.2) occurrence
in15romantic and 15non-romantic random dia-
logues. We obtain 209,800embeddings, which are
used to train logistic regression models that classify
the gender or race associated with a name (details
in §A). As we compare the average classification
accuracy (across 5 different train-test splits) against
a majority baseline, we observe, in Table 1, that
gender could be effectively predicted for non-Asian
name embeddings, and the embeddings are distin-
guishable by race for all races/ethnicities in a One-
vs-All setting. However, Asian name embeddings
encode minimal gender information, decreasing the
likelihood of a model leveraging the inferred gen-
der identity when making relationship predictions
that reflect heteronormative biases.
Does gender association have a stronger influ-
ence on model’s prediction than race/ethnicity?
We hypothesize that models’ tendency to asso-
ciate gender with names influences their relation-
ship predictions. To test this, we substitute names
with generic placeholders (“X” and “Y”) to get
a baseline where a model has no access to char-
acter names (more details in §B). After name-
replacements, any deviation from these results (Ta-
ble 2) would indicate that a model exploits the
implicit information from first names. In Fig-
ure 2, multiple settings have recall values that
significantly differ from those in the anonymizedModel Precision Recall F1 Accuracy
Gender Pairings
Llama2-7B 0.7978 0 .6887 0 .7392 0 .6125
Llama2-13B 0.8649 0 .3019 0 .4476 0 .4170
Mistral-7B 0.8269 0 .2028 0 .3258 0 .3432
Racial Pairings
Llama2-7B 0.8063 0 .7131 0 .7569 0 .6422
Llama2-13B 0.8696 0 .3287 0 .4665 0 .4404
Mistral-7B 0.8406 0 .2311 0 .3625 0 .3761
Table 2: Evaluation scores for anonymous name-
replacements (character replaced with “X” or “Y”) for
different models under study. These results depict the
model’s performance solely based on the context.
setting ( 0.6887 ). This disparity suggests name-
replacements introduce gender information that
significantly influences the model behavior. Such
trends are less prominent for Asian names due to
the model’s apparent inability to distinguish gender
information in Asian names (Table 1). By contrast,
racial information encoded in first names exerts a
lesser impact. Non-Asian heterosexual intra/inter-
racial pairs give rise to similar recall in Figure 3.
We thus do not observe strong prejudice against
interracial romantic relationships here.
5 Social Implications
It has been a prolonged and arduous struggle to
recognize and accept gay marriages in the US (An-
dersen, 2016; Duberman, 2019). Legal recogni-
tion of these relationships remains a challenge
in many other countries (Lee and Ostergard Jr,
2017; Chia, 2019; Ramdas, 2021). Even within
the US, LGBTQIA+ people still encounter discrim-
ination (Buist, 2019; Knauer, 2020; Naylor, 2020).
We believe heteronormative biases we have ob-
served could impact various downstream LLM use
cases, potentially causing both representational and
allocational harms (Blodgett et al., 2020). For ex-
ample, when LLMs are used for story generation
based on social media posts as the premise (Te
et al., 2018; Li et al., 2024a), the life events of
members of the LGBTQIA+ community may be
overlooked or misrepresented. If LLMs struggle to
recognize same-gender romantic relationships, they
may further marginalize the LGBTQIA+ commu-
nity by diminishing their social visibility and rep-
resentation. In addition, such model behavior may
result in uneven allocation of resources or opportu-
nities. Consider an online advertising system thatpromotes low-interest home loans for married cou-
ples based on social media interactions. A model
unable to identify same-gender marriages would
exclude these couples from the promotion. There-
fore, building inclusive technology that respects
minority rights is essential.
6 Related Work
Prior works (Wang et al., 2022; Jeoung et al., 2023;
Sandoval et al., 2023; Wan et al., 2023; An et al.,
2023, 2024; Nghiem et al., 2024) show that lan-
guage models often treat first names differently,
even with controlled input contexts, due to factors
like frequency and demographic attributes associ-
ated with names (Maudslay et al., 2019; Shwartz
et al., 2020; Wolfe and Caliskan, 2021; Czarnowska
et al., 2021; An and Rudinger, 2023). Our work
uses models’ interpretations of gender associated
with first names to reveal heteronormative biases
in some LLMs.
Further, NLP systems often fail in interpreting
various social factors ( e.g., social norms, cultures,
and relations) of language (Hovy and Yang, 2021).
One such factor of interest is the representation
of social relationships in these systems, including
power dynamics (Prabhakaran et al., 2012), friend-
ship (Krishnan and Eisenstein, 2015), and romantic
relationships (Seraj et al., 2021). Recently, Stewart
and Mihalcea (2024) show failure of popular ma-
chine translation systems in translating sentences
concerning relationships between nouns of same-
gender. Leveraging the task of relationship predic-
tion and using an existing dataset (Jia et al., 2021),
our work contributes to the assessment of social
relationship-related biases in LLMs arising from
gender and race associations with first names.
7 Conclusion
Through controlled name-replacement experi-
ments, we find that LLMs predict romantic rela-
tionships between characters based on the demo-
graphic identities associated with their first names.
Specifically, relationship predictions between same-
gender and intra/inter-racial character pairs involv-
ing Asian names are less likely to be romantic.
Our analysis of contextualized name embeddings
sheds light on the cause of our findings. We also
highlight the social implications of this potentially
harmful model behavior for the LGBTQIA+ com-
munity. We urge advocates to build technology that
respects the rights of marginalized social groups.Limitations
Prompt sensitivity and in-context learning.
LLMs are sensitive to prompt formats (Min et al.,
2022; Li et al., 2024b) therefore the accuracy of pre-
dictions may vary within or across models. While
we had experimented with several prompts before
converging to the one we use (gave the best predic-
tion accuracy on the original dataset as well as close
to that reported in Jia et al. (2021)), future work
may investigate the impact of different prompt for-
mulations and if in-context learning can help in
reducing the influence of biases on the downstream
tasks.
Inadequate coverage of names associated with
different identities. We recognize that our paper
has limitations regarding the number of races and
genders studied. This is due to the unavailability of
data sources to compile a sufficiently large number
of names strongly associated with a wide range of
underrepresented races and gender identities.
Linguistic usage might be significantly different
in same-gender romantic relationships. The
test set we have utilized (Jia et al., 2021) does
not contain dialogues between same-gender char-
acter pairs in romantic relationships. As a con-
sequence, we lack conversations that effectively
depict interactions between same-gender partners.
We acknowledge this limitation in our data source.
However, in cases where same-gender partners ex-
hibit behavior similar to different-gender couples,
our results indicate that LLMs tend to demonstrate
heteronormative biases in the intersection of these
interaction styles.
Conversations might contain implicit gender-
revealing cues. While we ensure consistency be-
tween gender associated with an utterance (based
on how a male speaks vs a female) and the gen-
der associated with a name by only consider-
ing the conversations that do not have explicit
gender-revealing cues as described in §2.1, we ac-
knowledge the possibility of the presence of im-
plicit gender-revealing cues which is harder to
detect. However, we believe that our findings
stand valid even if the implicit cues are present
as demonstrated by comparable recall between
name-replacements that preserve the gender (upper-
triangle; specifically top-right) associated with the
original speaker and the swapped variants (lower-
triangle; specifically bottom-left) in Figure 2. Weleave further analysis of the nuances with implicit
cues to future work.
Ethical Considerations
Inconsistency between self-identification and de-
mographic attributes associated with a name.
Our categorization of names into subgroups of
race/ethnicity and gender is based on real-world
data as we observe a strong statistical associa-
tion between names and demographic attributes
(race/ethnicity and gender). However, it is cru-
cial to realize that a person with a particular name
may identify themselves differently from the ma-
jority, and we should respect their individual pref-
erences and embrace the differences. We have at-
tempted to accommodate diverse possibilities in
self-identification by incorporating gender-neutral
names into our experimental setup. While there
is still ample room for improvement in address-
ing this issue, we have taken a step forward in
promoting the inclusion of additional forms of self-
identification in ethical NLP research.
Ethical concerns about the task of relation-
ship prediction. Predicting interpersonal rela-
tionships from conversations may require access
to private and sensitive data. If no proper con-
sent from a user is obtained, using personal data
could lead to serious ethical and legal concerns.
Although building systems that identify the rela-
tionship type between speakers could contribute
to the development of AI agents that better under-
stand human interactions, it is crucial to be trans-
parent about what data is collected and how it is
processed in such systems. Even if data privacy is
properly handled when using a model to predict
relationship types, people often exercise caution
when revealing romantic relationships. Therefore,
the deployment of an NLP system to identify such
relationships should be disclosed to users who may
be affected, and any predictions should remain con-
fidential unless the user’s consent is obtained for
public disclosure.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable feedback. Rachel Rudinger is
supported by NSF CAREER Award No. 2339746.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those
of the author(s) and do not necessarily reflect the
views of the National Science Foundation.References
Apoorv Agarwal, Jiehan Zheng, Shruti Kamath, Sri-
ramkumar Balasubramanian, and Shirin Ann Dey.
2015. Key female characters in film have more to
talk about besides men: Automating the Bechdel test.
InProceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 830–840, Denver, Colorado. Association for
Computational Linguistics.
Shahriar Akter, Grace McCarthy, Shahriar Sajib, Katina
Michael, Yogesh K. Dwivedi, John D’Ambra, and
K.N. Shen. 2021. Algorithmic bias in data-driven
innovation in the age of ai. International Journal of
Information Management , 60:102387.
Haozhe An, Christabel Acquaye, Colin Wang, Zongxia
Li, and Rachel Rudinger. 2024. Do large language
models discriminate in hiring decisions on the ba-
sis of race, ethnicity, and gender? In Proceedings
of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) ,
pages 386–397, Bangkok, Thailand. Association for
Computational Linguistics.
Haozhe An, Zongxia Li, Jieyu Zhao, and Rachel
Rudinger. 2023. SODAPOP: Open-ended discov-
ery of social biases in social commonsense reasoning
models. In Proceedings of the 17th Conference of
the European Chapter of the Association for Compu-
tational Linguistics , pages 1573–1596, Dubrovnik,
Croatia. Association for Computational Linguistics.
Haozhe An and Rachel Rudinger. 2023. Nichelle and
nancy: The influence of demographic attributes and
tokenization length on first name biases. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers) , pages 388–401, Toronto, Canada. Association
for Computational Linguistics.
Ellen Ann Andersen. 2016. Transformative events in
the lgbtq rights movement. Ind. JL & Soc. Equal. ,
5:441.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5454–
5476, Online. Association for Computational Lin-
guistics.
Carrie L Buist. 2019. Lgbtq rights in the fields of
criminal law and law enforcement. U. Rich. L. Rev. ,
54:877.
Yang Trista Cao, Anna Sotnikova, Hal Daumé III,
Rachel Rudinger, and Linda Zou. 2022. Theory-
grounded measurement of U.S. social stereotypes in
English language models. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1276–1295, Seattle,United States. Association for Computational Lin-
guistics.
Anupam Chander. 2016. The racist algorithm. Mich. L.
Rev., 115:1023.
Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023.
Marked personas: Using natural language prompts to
measure stereotypes in language models. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1504–1532, Toronto, Canada. Association for
Computational Linguistics.
Joy L Chia. 2019. Lgbtq rights in china: Movement-
building in uncertain times. In Handbook on human
rights in China , pages 657–680. Edward Elgar Pub-
lishing.
Paula Czarnowska, Yogarshi Vyas, and Kashif Shah.
2021. Quantifying social biases in NLP: A general-
ization and empirical comparison of extrinsic fairness
metrics. Transactions of the Association for Compu-
tational Linguistics , 9:1249–1267.
Shaji Daniel. 2024. Negotiating the challenges of an in-
terracial marriage: An interpretive phenomenological
analysis of the perception of diaspora indian partners.
Family Relations , 73(1):282–297.
Martin Duberman. 2019. Stonewall: The definitive story
of the LGBTQ rights uprising that changed America .
Penguin.
Virginia Felkner, Ho-Chun Herbert Chang, Eugene Jang,
and Jonathan May. 2023. WinoQueer: A community-
in-the-loop benchmark for anti-LGBTQ+ bias in
large language models. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 9126–
9140, Toronto, Canada. Association for Computa-
tional Linguistics.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 .
Dirk Hovy and Diyi Yang. 2021. The importance of
modeling social factors of language: Theory and
practice. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 588–602, Online. Association
for Computational Linguistics.
Sullam Jeoung, Jana Diesner, and Halil Kilicoglu. 2023.
Examining the causal impact of first names on lan-
guage models: The case of social commonsense rea-
soning. In Proceedings of the 3rd Workshop on Trust-
worthy Natural Language Processing (TrustNLP
2023) , pages 61–72, Toronto, Canada. Association
for Computational Linguistics.
Qi Jia, Hongru Huang, and Kenny Q Zhu. 2021. Ddrel:
A new dataset for interpersonal relation classificationin dyadic dialogues. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 35, pages
13125–13133.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Nancy J Knauer. 2020. The lgbtq equality gap and
federalism. Am. UL Rev. , 70:1.
Vinodh Krishnan and Jacob Eisenstein. 2015. “you’re
mr. lebowski, I’m the dude”: Inducing address term
formality in signed social networks. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1616–1626,
Denver, Colorado. Association for Computational
Linguistics.
Chelsea Lee and Robert L Ostergard Jr. 2017. Mea-
suring discrimination against lgbtq people: A cross-
national analysis. Human Rights Quarterly , pages
37–72.
Donna A. Lewandowski and Linda A. Jackson. 2001.
Perceptions of interracial couples: Prejudice at
the dyadic level. Journal of Black Psychology ,
27(3):288–303.
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,
and Ji-Rong Wen. 2024a. Pre-trained language mod-
els for text generation: A survey. ACM Comput.
Surv. , 56(9).
Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem,
and Jordan Lee Boyd-Graber. 2024b. Pedants: Cheap
but effective and interpretable answer equivalence.
Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and
Simone Teufel. 2019. It’s all in the name: Mitigating
gender bias with name-based counterfactual data sub-
stitution. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
5267–5275, Hong Kong, China. Association for Com-
putational Linguistics.
Suzanne C. Miller, Michael A. Olson, and Russell H.
Fazio. 2004. Perceived reactions to interracial ro-
mantic relationships: When race is used as a cue
to status. Group Processes & Intergroup Relations ,
7(4):354–369.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.Lorenda A Naylor. 2020. Social equity and LGBTQ
rights: Dismantling discrimination and expanding
civil rights . Routledge.
Huy Nghiem, John Prindle, Jieyu Zhao, and Hal
Daumé III. 2024. " you gotta be a doctor, lin": An
investigation of name-based bias of large language
models in employment recommendations. arXiv
preprint arXiv:2406.12232 .
Debora Nozza, Federico Bianchi, Anne Lauscher, and
Dirk Hovy. 2022. Measuring harmful sentence com-
pletion in language models for LGBTQIA+ individ-
uals. In Proceedings of the Second Workshop on
Language Technology for Equality, Diversity and In-
clusion , pages 26–34, Dublin, Ireland. Association
for Computational Linguistics.
Jiao Ou, Junda Lu, Che Liu, Yihong Tang, Fuzheng
Zhang, Di Zhang, and Kun Gai. 2024. Dialogbench:
Evaluating llms as human-like dialogue systems. In
Proceedings of the 2024 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(Volume 1: Long Papers) , pages 6137–6170.
Orestis Papakyriakopoulos and Arwa M. Mboya. 2023.
Beyond algorithmic bias: A socio-computational in-
terrogation of the google search by image algorithm.
Social Science Computer Review , 41(4):1100–1125.
Patricia S. Pittman, Claire Kamp Dush, Keeley J. Pratt,
and Jen D. Wong. 2024. Interracial couples at risk:
Discrimination, well-being, and health. Journal of
Family Issues , 45(2):303–325.
Amanda M Pollitt, Sara E Mernitz, Stephen T Russell,
Melissa A Curran, and Russell B Toomey. 2021. Het-
eronormativity in the lives of lesbian, gay, bisexual,
and queer young people. Journal of Homosexuality ,
68(3):522–544.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting overt display of power in writ-
ten dialogs. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 518–522, Montréal, Canada. As-
sociation for Computational Linguistics.
Kamalini Ramdas. 2021. Negotiating lgbtq rights in
singapore: The margin as a place of refusal. Urban
Studies , 58(7):1448–1462.
Corinne Reczek. 2020. Sexual-and gender-minority
families: A 2010 to 2020 decade in review. Journal
of Marriage and Family , 82(1):300–325.
Evan TR Rosenman, Santiago Olivella, and Kosuke
Imai. 2023. Race and ethnicity data for first, middle,
and surnames. Scientific Data .
Lisa Rosenthal and Tyrel J Starks. 2015. Relationship
stigma and relationship outcomes in interracial and
same-sex relationships: Examination of sources and
buffers. Journal of Family Psychology , 29(6):818.Sandra Sandoval, Jieyu Zhao, Marine Carpuat, and Hal
Daumé III. 2023. A rose by any other name would
not smell as sweet: Social bias in names mistrans-
lation. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 3933–3945, Singapore. Association for Com-
putational Linguistics.
Sarah Seraj, Kate G Blackburn, and James W Pen-
nebaker. 2021. Language left behind on social media
exposes the emotional and cognitive costs of a roman-
tic breakup. Proceedings of the National Academy of
Sciences , 118(7):e2017154118.
Vered Shwartz, Rachel Rudinger, and Oyvind Tafjord.
2020. “you are grounded!”: Latent name artifacts in
pre-trained language models. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6850–6861,
Online. Association for Computational Linguistics.
Ian Stewart and Rada Mihalcea. 2024. Whose wife
is it anyway? assessing bias against same-gender
relationships in machine translation. In Proceed-
ings of the 5th Workshop on Gender Bias in Natu-
ral Language Processing (GeBNLP) , pages 365–375,
Bangkok, Thailand. Association for Computational
Linguistics.
Robee Khyra Mae J. Te, Janica Mae M. Lam, and Ethel
Ong. 2018. Using social media posts as knowledge
resource for generating life stories. In Proceedings of
the 32nd Pacific Asia Conference on Language, Infor-
mation and Computation , Hong Kong. Association
for Computational Linguistics.
Anna Tigunova, Paramita Mirza, Andrew Yates, and
Gerhard Weikum. 2021. PRIDE: Predicting Rela-
tionships in Conversations. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 4636–4650, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Juan Vásquez, Gemma Bel-Enguix, Scott Thomas An-
dersen, and Sergio-Luis Ojeda-Trueba. 2022. Hetero-
Corpus: A corpus for heteronormative language de-
tection. In Proceedings of the 4th Workshop on Gen-
der Bias in Natural Language Processing (GeBNLP) ,
pages 225–234, Seattle, Washington. Association for
Computational Linguistics.
Yixin Wan, George Pu, Jiao Sun, Aparna Garimella,
Kai-Wei Chang, and Nanyun Peng. 2023. “kelly
is a warm person, joseph is a role model”: Gender
biases in LLM-generated reference letters. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 3730–3748, Singapore.
Association for Computational Linguistics.Jun Wang, Benjamin Rubinstein, and Trevor Cohn.
2022. Measuring and mitigating name biases in
neural machine translation. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
2576–2590, Dublin, Ireland. Association for Compu-
tational Linguistics.
Robert Wolfe and Aylin Caliskan. 2021. Low frequency
names exhibit bias and overfitting in contextualizing
language models. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 518–532, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Elena Zambelli. 2023. Interracial couples and the phe-
nomenology of race, place, and space in contempo-
rary england. Identities , 30(5):725–743.
Xian Zhao and Monica Biernat. 2019. Your name
is your lifesaver: Anglicization of names and
moral dilemmas in a trilogy of transportation acci-
dents. Social Psychological and Personality Science ,
10(8):1011–1018.A Detailed Experimental Setup
We present additional information about our exper-
imental setup.
Models We use recently introduced two popular
language models for testing our hypothesis, namely
Llama (Touvron et al., 2023) (7B, 13B chat), and
Mistral-7B (Jiang et al., 2023). Each model uses
nucleus sampling (Holtzman et al., 2019) with de-
fault parameters, a temperature of 0, and a maxi-
mum generation length of 512. Each experiment
over 327test instances takes ∼30mins for Llama2-
7B,∼1hr for Llama2-13B, and ∼25mins for
Mistral-7B. We ran 870experiments per race ( 560
for Hispanic) for studying gender bias and 1600
experiments (400 per race-pair) for racial bias.
Computing Evaluation Scores We first com-
pute precision, recall, F1, and accuracy scores for
each name-pair-replacement and report the average
scores for each name-pair bin, and each race-pair
for studying the influence of gender, and race asso-
ciated with names, respectively.
Dataset Statistics Table 3 presents the frequency
of each relationship label along with romantic and
non-romantic categories used for the purpose of
this study, in the test split of DDRel (Jia et al., 2021)
dataset. Out of 327conversations with different-
gender characters in the dataset, 271do not contain
explicit gender information.
Prompts We provide the prompt template used
in our experiments in Figure 4.
Parsing Outputs from LLMs We observe incon-
sistencies in the outputs predicted by LLMs despite
clear instructions regarding formatting. We use reg-
ular expressions to extract the JSON outputs and
the predictions from them. We consider invalid
outputs ( i.e., non-pre-defined class) from LLMs as
a separate class (invalid) for evaluation purposes
across all experiments.
Logistic Regression for Name Embeddings We
quantitatively study the amount of gender infor-
mation encoded in these embeddings by training a
logistic regression model, separately for each race,
to classify the gender associated with a name, using
embeddings of 70% of names in a race as the train-
ing set and the remaining as the test set. Similarly,
we train a logistic regression model to conduct a
“One-vs-All" classification for each race. We con-
trol the train and test set in the racial setup to haveRelationship Labels Frequency Romantic #Gender Neutral
Lovers 182 ✓ 155
Courtship 15 ✓ 12
Spouse 57 ✓ 46
Siblings 15 ✗ 13
Child-Other Family Elder 13 ✗ 7
Child-Parent 39 ✗ 11
Colleague/Partners 70 ✗ 59
Workplace Superior-Subordinate 48 ✗ 24
Professional Contact 27 ✗ 10
Opponents 20 ✗ 11
Friends 95 ✗ 83
Roommates 21 ✗ 21
Neighbours 8 ✗ 7
Total 610 - 459
Table 3: Frequency of relationship types in the test split
of DDRel dataset (Jia et al., 2021).
a balanced number of positive and negative sam-
ples by down-sampling the instances from other
races ( 1/3from each other race). We repeat the
logistic regression training with 5 different random
train-test splits. We set the random state of the lo-
gistic regression model to 0and maximum iteration
to1000 . In Table 1, we report the average results
across 5 runs with their standard deviation.
B Anonymous Name-replacement
Experiments
We perform two types of anonymous name-
replacement experiments differing in whether both
names are anonymized or only one.
B.1 Both Names Are Anonymized
We substitute names with generic placeholders (“X”
and “Y”) to get a baseline where a model has no
access to character names to test the hypothesis
that models’ tendency to associate gender with the
names influences their relationship predictions.
B.2 One Name Is Anonymized
We substitute one name and keep the other
anonymized to analyze the impact of one charac-
ter’s gender on romantic relationship predictions
independent of the second. We replace one name
with a male, female or a neutral name either pre-
serving or swapping the original gender of the non-
anonymized name while keeping the other name
anonymized. Male, neutral, and female names be-
long to 0−25,25−75, and 75−100% bins,
respectively. We report the recall scores for ro-
mantic relationship prediction (same/swapped) for
different models in Table 4.System Prompt:You are an avid novel reader and a code generator. Please output in JSON format. No preambles.Prompt:Your task is to read a conversation between two people and infer the type of relationship between the two people from the given list of relationship types.Input: Following is the conversation between {char_a} and {char_b}.{context}What is the type of the relationship between {char_a}and {char_b}according to the below list of type of relationships: [Child-Parent, Child-Other Family Elder, Siblings, Spouse, Lovers, Courtship, Friends, Neighbors, Roommates, Workplace Superior -Subordinate, Colleague/Partners, Opponents, Professional Contact]Constraint: Please answer in JSON format with the type of relationship and explanation for the inferred relationship. Type of relationship can only be from the provided list.Output in JSON format:Figure 4: Prompt template used in our experiments. “ {char_a} ”, “{char_b} ”, and “ {context} ” are placeholders here
and they are instantiated with character names and dialogues accordingly for model inference.
Model Race Male Neutral Female
Llama2-7BAsian 0.6049/0.6128 0 .6085/0.6203 0 .6663/0.6517
Black 0.6069/0.6230 0 .6454/0.6392 0 .6572/0.6458
Hispanic 0.6292/0.6284 0 .6486/0.6541 0 .7093/0.6897
White 0.6387/0.6372 0 .6328/0.6297 0 .6887/0.6761
Llama2-13BAsian 0.2991/0.2940 0 .2806/0.2798 0 .3090/0.3043
Black 0.3066/0.2854 0 .3004/0.2909 0 .3054/0.3105
Hispanic 0.3021/0.2801 0 .2956/0.2980 0 .3206/0.3190
White 0.3149/0.2952 0 .2924/0.2878 0 .3121/0.3121
MistralAsian 0.1789/0.1694 0 .1808/0.1840 0 .1895/0.1906
Black 0.1855/0.1828 0 .1902/0.1871 0 .1922/0.1859
Hispanic 0.1986/0.1955 0 .1848/0.1776 0 .2048/0.1973
White 0.1895/0.1836 0 .1887/0.1871 0 .1942/0.1922
Table 4: Recall scores (same/swapped) for romantic
relationship predictions when one name is anonymous
while another is either a male, neutral, or female name
as per bins marked in Figure 2. The results show that
models are more likely to predict a romantic relationship
when one of the names is a female name.
C First Names
We detail the name selection criteria in our experi-
ments. We also list all first names we have used in
our experiments to study the influence of different
gender and racial/ethnic name pairing.
C.1 First Names Used to Study the Influence
of Gender Pairing
We first collect names that have frequency over 200
and have more than 80% of the population having
that name identify themselves as a particular race
(Asian, Black, Hispanic, and White) from Rosen-
man et al. 2023. Then, we partition these names
into10non-linearly segmented bins (shown in Fig-
ure 2) based on the percentage of population thathas been assigned as female at birth using statis-
tics from the Social Security Application dataset
(SSA3). We randomly sample 3names per bin to-
taling to 30names per race4for performing the
replacements. We consider names belonging to a
spectrum of female gender associations to ensure
coverage of gender-neutral names.
We list all the names used in this set of experi-
ments. We include the percentage of the population
assigned female gender at birth in parentheses.
Asian Seung ( 0.00%), Quoc ( 0.00%), Dat
(0.00%), Nghia ( 2.30%), Thuan ( 2.40%), Thien
(2.70%), Hoang ( 6.40%), Sang ( 6.60%), Jun
(9.60%), Sung ( 13.50%), Jie ( 17.30%), Wei
(21.80%), Hyun ( 39.00%), Khanh ( 41.90%),
Wen ( 44.60%), Hien ( 51.70%), An ( 54.80%), Ji
(61.40%), In ( 80.80%), Diem ( 88.60%), Quyen
(88.90%), Ling ( 91.30%), Xiao ( 91.50%), Ngoc
(92.40%), Su ( 95.40%), Hanh ( 95.60%), Vy
(97.00%), Eun ( 98.30%), Trinh ( 100.00%), Huong
(100.00%)
Black Deontae ( 0.00%), Antwon ( 0.10%),
Javonte ( 1.00%), Dejon ( 2.90%), Jamell ( 3.40%),
Dijon ( 4.60%), Dashawn ( 5.80%), Deshon
(6.20%), Pernell ( 8.30%), Rashawn ( 10.10%),
Torrance ( 13.20%), Semaj ( 22.60%), Demetris
(25.60%), Kamari ( 33.60%), Amari ( 42.00%),
Shamari ( 56.10%), Kenyatta ( 57.10%), Ivory
(59.30%), Chaka ( 76.20%), Ashante ( 89.40%),
3https://www.ssa.gov/oact/babynames/
4Except for Hispanic wherein we did not get any names in
5−10% bin and only 1 name in 25−50% bin.Unique ( 89.90%), Kenya ( 92.20%), Nikia
(93.80%), Akia ( 94.30%), Kenyetta ( 95.50%),
Shante ( 96.40%), Shaunta ( 97.00%), Laquandra
(100.00%), Lakesia ( 100.00%), Daija ( 100.00%)
Hispanic Nestor ( 0.00%), Fidel ( 0.00%), Raul
(0.60%), Leonides ( 2.70%), Yamil ( 4.50%), Reyes
(10.80%), Cruz ( 13.10%), Neftali ( 14.90%),
Noris ( 38.10%), Nieves ( 62.40%), Guadalupe
(72.60%), Ivis ( 75.00%), Monserrate ( 78.20%),
Ibis ( 82.60%), Johanny ( 89.40%), Elba ( 91.50%),
Matilde ( 93.40%), Rocio ( 96.90%), Lucero
(97.30%), Cielo ( 97.50%), Lucila ( 100.00%), Zu-
leyka ( 100.00%), Yaquelin ( 100.00%)
White Zoltan ( 0.00%), Leif ( 0.10%), Jack
(0.40%), Ryder ( 3.30%), Carmine ( 3.40%), Haden
(4.10%), Tate ( 5.30%), Dickie ( 5.50%), Logan
(7.40%), Parker ( 17.50%), Sawyer ( 20.90%), Hay-
den ( 22.50%), Dakota ( 29.70%), Britt ( 38.30%),
Harley ( 41.70%), Campbell ( 53.90%), Barrie
(56.10%), Peyton ( 61.90%), Kelley ( 88.00%),
Jodie ( 88.20%), Leigh ( 88.70%), Clare ( 90.90%),
Rylee ( 92.20%), Meredith ( 94.70%), Baylee
(97.00%), Lacey ( 97.30%), Ardith ( 97.70%),
Kristi ( 99.80%), Galina ( 100.00%), Margarete
(100.00%)
C.2 First Names Used to Study the Influence
of Intra/Inter-racial Pairing
By referencing Rosenman et al. 2023 and the SSA
dataset again, we collect another set of both race-
and gender-indicative first names with a minimum
frequency of 200, applying a threshold of 90% for
the percentage of the population assigned either fe-
male or male at birth. For race threshold, we set it
to be 90% for Asian, Black, and Hispanic, and 70%
for White. Although we choose a lower threshold
for White to account for the phenomenon of name
Anglicization (Zhao and Biernat, 2019), we still
obtain empirical results that strongly indicate these
names are represented differently from names as-
sociated with other races/ethnicities. In total, we
obtain 80names that are evenly distributed among
four races/ethnicities and two genders. We replace
name-pairs while preserving the gender associated
with the names in the original dialogue.
Asian Female Thuy, Thu, Huong, Trang, Ngoc,
Hanh, Hang, Xuan, Trinh, Eun
Asian Male Tuan, Hai, Sang, Hoang, Nam, Huy,
Quang, Duc, Trung, HieuBlack Female Latoya, Ebony, Latasha, Latonya,
Tamika, Kenya, Tameka, Lakeisha, Tanisha, Pre-
cious
Black Male Tyrone, Cedric, Darius, Jermaine,
Demetrius, Malik, Jalen, Roosevelt, Marquis, De-
andre
Hispanic Female Luz, Mayra, Marisol, Maribel,
Alejandra, Yesenia, Migdalia, Xiomara, Mariela,
Yadira
Hispanic Male Luis, Jesus, Lazaro, Osvaldo,
Heriberto, Jairo, Rigoberto, Adalberto, Ezequiel,
Ulises
White Female Mary, Patricia, Jennifer, Linda,
Elizabeth, Barbara, Susan, Jessica, Kimberly, San-
dra
White Male James, Michael, John, Robert,
William, David, Christopher, Richard, Joseph,
Charles
D Additional Results
We report the results for Llama2-13B (Figures 6
and 9) and Mistral-7B (Figures 7 and 10). We also
report the F1 and accuracy scores for Llama2-7B,
for completeness, in Figure 5 and 8. We observe
similar trends as Llama2-7B discussed in the main
body of the paper.0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.83 0.84 0.82 0.82 0.82 0.82 0.81 0.81 0.81 0.80
0.83 0.84 0.83 0.83 0.81 0.84 0.81 0.81 0.82 0.81
0.82 0.82 0.81 0.81 0.80 0.81 0.80 0.80 0.80 0.80
0.82 0.82 0.82 0.82 0.81 0.81 0.80 0.80 0.81 0.80
0.81 0.82 0.81 0.81 0.81 0.81 0.80 0.80 0.80 0.80
0.82 0.82 0.81 0.81 0.80 0.82 0.80 0.80 0.81 0.80
0.80 0.80 0.79 0.79 0.80 0.80 0.79 0.79 0.80 0.79
0.80 0.80 0.79 0.79 0.79 0.79 0.79 0.80 0.80 0.79
0.81 0.81 0.80 0.80 0.80 0.81 0.80 0.80 0.80 0.80
0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79Asian (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.83 0.83 0.82 0.82 0.80 0.80 0.80 0.80 0.80 0.80
0.82 0.82 0.81 0.81 0.80 0.80 0.80 0.79 0.80 0.80
0.83 0.81 0.81 0.82 0.80 0.80 0.80 0.80 0.80 0.80
0.82 0.82 0.81 0.81 0.80 0.80 0.80 0.79 0.79 0.79
0.81 0.81 0.80 0.80 0.81 0.80 0.80 0.79 0.79 0.79
0.80 0.80 0.79 0.80 0.81 0.79 0.79 0.79 0.79 0.79
0.80 0.80 0.80 0.80 0.80 0.80 0.80 0.79 0.79 0.79
0.80 0.79 0.79 0.79 0.80 0.79 0.79 0.79 0.79 0.79
0.80 0.79 0.80 0.79 0.79 0.79 0.79 0.78 0.79 0.79
0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79Black (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.83 0.82 0.81 0.81 0.80 0.81 0.80 0.80 0.80
0.83 0.82 0.81 0.80 0.79 0.81 0.80 0.79 0.80
0.81 0.81 0.80 0.80 0.79 0.80 0.80 0.79 0.79
0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79
0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79
0.80 0.81 0.80 0.80 0.80 0.80 0.80 0.79 0.80
0.80 0.79 0.79 0.79 0.78 0.79 0.79 0.79 0.79
0.79 0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.79
0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79Hispanic (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.83 0.83 0.81 0.82 0.80 0.81 0.79 0.79 0.80 0.79
0.81 0.80 0.79 0.79 0.79 0.80 0.79 0.79 0.79 0.79
0.82 0.81 0.81 0.81 0.79 0.81 0.79 0.79 0.80 0.79
0.81 0.81 0.80 0.80 0.79 0.81 0.79 0.79 0.79 0.80
0.80 0.80 0.80 0.79 0.80 0.80 0.79 0.80 0.79 0.79
0.81 0.80 0.81 0.80 0.80 0.80 0.80 0.79 0.80 0.79
0.79 0.79 0.79 0.79 0.78 0.79 0.79 0.79 0.80 0.79
0.80 0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79
0.80 0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.80
0.80 0.79 0.79 0.79 0.79 0.79 0.79 0.80 0.79 0.79White (Precision)
0.790.800.810.820.83
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.67 0.66 0.69 0.68 0.69 0.69 0.74 0.73 0.71 0.76
0.67 0.64 0.68 0.68 0.68 0.68 0.72 0.72 0.71 0.74
0.69 0.68 0.71 0.69 0.71 0.70 0.75 0.74 0.72 0.76
0.68 0.67 0.68 0.69 0.70 0.68 0.73 0.72 0.71 0.75
0.70 0.69 0.71 0.70 0.72 0.70 0.74 0.74 0.72 0.74
0.69 0.67 0.69 0.69 0.69 0.69 0.72 0.71 0.70 0.73
0.72 0.71 0.73 0.72 0.73 0.71 0.73 0.72 0.72 0.73
0.72 0.71 0.73 0.72 0.73 0.71 0.73 0.73 0.71 0.73
0.71 0.69 0.71 0.70 0.71 0.70 0.72 0.70 0.70 0.72
0.74 0.72 0.74 0.73 0.73 0.72 0.73 0.72 0.71 0.73Asian (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.69 0.70 0.70 0.69 0.72 0.75 0.71 0.76 0.80 0.79
0.69 0.69 0.69 0.69 0.72 0.74 0.71 0.75 0.79 0.78
0.71 0.70 0.70 0.70 0.72 0.76 0.72 0.76 0.80 0.78
0.70 0.70 0.69 0.69 0.71 0.74 0.71 0.74 0.78 0.76
0.73 0.73 0.72 0.71 0.69 0.71 0.71 0.73 0.76 0.75
0.75 0.74 0.74 0.73 0.72 0.77 0.72 0.73 0.77 0.75
0.72 0.71 0.72 0.70 0.71 0.73 0.70 0.72 0.74 0.73
0.77 0.74 0.75 0.73 0.73 0.73 0.71 0.71 0.73 0.70
0.80 0.78 0.78 0.77 0.76 0.76 0.73 0.72 0.73 0.71
0.79 0.77 0.77 0.75 0.73 0.75 0.72 0.69 0.72 0.69Black (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.69 0.69 0.70 0.75 0.78 0.74 0.79 0.82 0.82
0.68 0.69 0.70 0.74 0.75 0.71 0.78 0.80 0.80
0.70 0.71 0.72 0.75 0.77 0.73 0.79 0.81 0.81
0.72 0.72 0.72 0.72 0.72 0.75 0.78 0.77
0.77 0.75 0.76 0.72 0.73 0.74 0.74 0.77 0.75
0.72 0.71 0.72 0.73 0.74 0.74 0.76 0.79 0.79
0.78 0.77 0.77 0.75 0.73 0.75 0.79 0.77 0.75
0.81 0.81 0.80 0.78 0.77 0.79 0.78 0.81 0.78
0.81 0.80 0.80 0.76 0.74 0.78 0.76 0.78 0.77Hispanic (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.64 0.71 0.66 0.69 0.76 0.70 0.77 0.81 0.80 0.83
0.70 0.74 0.70 0.70 0.78 0.72 0.78 0.82 0.80 0.83
0.67 0.72 0.68 0.69 0.76 0.70 0.76 0.81 0.78 0.81
0.70 0.71 0.69 0.71 0.77 0.71 0.77 0.82 0.78 0.82
0.76 0.78 0.76 0.77 0.76 0.73 0.74 0.77 0.74 0.77
0.69 0.72 0.69 0.70 0.73 0.69 0.72 0.77 0.74 0.78
0.77 0.78 0.75 0.76 0.73 0.71 0.72 0.74 0.70 0.73
0.82 0.82 0.80 0.81 0.77 0.76 0.74 0.77 0.72 0.73
0.80 0.80 0.77 0.78 0.74 0.73 0.70 0.71 0.68 0.70
0.83 0.83 0.80 0.81 0.77 0.77 0.73 0.73 0.70 0.69White (F1)
0.6500.6750.7000.7250.7500.7750.8000.825
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.56 0.55 0.57 0.57 0.58 0.57 0.62 0.61 0.59 0.64
0.56 0.53 0.56 0.57 0.56 0.57 0.60 0.60 0.59 0.62
0.58 0.56 0.59 0.57 0.59 0.59 0.62 0.62 0.60 0.63
0.57 0.56 0.57 0.58 0.58 0.57 0.60 0.60 0.59 0.62
0.58 0.57 0.59 0.58 0.60 0.58 0.62 0.61 0.60 0.62
0.57 0.55 0.57 0.57 0.57 0.58 0.60 0.59 0.58 0.60
0.60 0.58 0.60 0.59 0.60 0.59 0.61 0.59 0.59 0.60
0.60 0.59 0.61 0.59 0.60 0.58 0.60 0.60 0.59 0.60
0.59 0.57 0.59 0.58 0.59 0.58 0.59 0.58 0.58 0.59
0.62 0.59 0.62 0.61 0.60 0.59 0.61 0.59 0.59 0.60Asian (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.58 0.58 0.58 0.58 0.60 0.62 0.59 0.63 0.68 0.67
0.58 0.57 0.57 0.57 0.59 0.62 0.58 0.62 0.67 0.65
0.59 0.58 0.58 0.58 0.60 0.63 0.59 0.63 0.67 0.65
0.58 0.58 0.57 0.58 0.59 0.61 0.58 0.61 0.65 0.63
0.61 0.60 0.60 0.59 0.57 0.59 0.59 0.60 0.64 0.62
0.63 0.61 0.61 0.60 0.60 0.64 0.60 0.60 0.64 0.61
0.59 0.59 0.60 0.58 0.59 0.61 0.58 0.59 0.61 0.60
0.64 0.61 0.62 0.60 0.60 0.61 0.59 0.58 0.60 0.57
0.68 0.65 0.66 0.64 0.63 0.64 0.60 0.59 0.60 0.58
0.66 0.64 0.64 0.62 0.60 0.62 0.58 0.56 0.59 0.56Black (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.58 0.58 0.58 0.63 0.65 0.62 0.68 0.70 0.70
0.57 0.58 0.59 0.61 0.63 0.59 0.66 0.68 0.68
0.58 0.59 0.60 0.62 0.64 0.61 0.67 0.69 0.69
0.59 0.59 0.59 0.59 0.59 0.62 0.65 0.65
0.64 0.62 0.63 0.59 0.60 0.61 0.62 0.64 0.63
0.59 0.59 0.59 0.61 0.62 0.61 0.64 0.67 0.67
0.66 0.64 0.65 0.62 0.60 0.63 0.67 0.65 0.63
0.69 0.69 0.68 0.65 0.64 0.67 0.65 0.69 0.66
0.69 0.68 0.68 0.64 0.61 0.66 0.63 0.66 0.64Hispanic (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.54 0.60 0.55 0.58 0.64 0.58 0.65 0.70 0.67 0.71
0.58 0.61 0.57 0.58 0.65 0.60 0.66 0.70 0.67 0.71
0.56 0.60 0.56 0.57 0.63 0.58 0.63 0.69 0.66 0.69
0.58 0.59 0.57 0.58 0.64 0.59 0.65 0.70 0.66 0.71
0.64 0.66 0.64 0.64 0.64 0.61 0.62 0.65 0.61 0.65
0.58 0.60 0.57 0.58 0.61 0.57 0.60 0.64 0.61 0.66
0.64 0.66 0.62 0.63 0.59 0.59 0.59 0.62 0.58 0.61
0.70 0.70 0.68 0.69 0.64 0.63 0.61 0.64 0.59 0.61
0.68 0.68 0.64 0.66 0.61 0.60 0.58 0.59 0.56 0.57
0.72 0.71 0.68 0.69 0.64 0.64 0.60 0.61 0.57 0.57White (Accuracy)
0.5500.5750.6000.6250.6500.6750.700Figure 5: Precision, F1-score and Accuracy plots for romantic predictions from Llama2-7B model.0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.88 0.88 0.87 0.87 0.87 0.86 0.83 0.85 0.85 0.84
0.88 0.85 0.85 0.87 0.87 0.86 0.84 0.84 0.85 0.84
0.86 0.86 0.84 0.86 0.86 0.85 0.83 0.83 0.84 0.83
0.87 0.88 0.86 0.87 0.88 0.87 0.85 0.86 0.86 0.85
0.88 0.88 0.86 0.88 0.86 0.87 0.85 0.85 0.86 0.85
0.86 0.86 0.85 0.86 0.86 0.86 0.84 0.84 0.85 0.84
0.84 0.85 0.84 0.85 0.85 0.85 0.85 0.85 0.86 0.85
0.85 0.86 0.85 0.86 0.87 0.87 0.86 0.86 0.86 0.85
0.86 0.87 0.85 0.87 0.87 0.87 0.85 0.85 0.86 0.85
0.85 0.85 0.84 0.86 0.87 0.86 0.86 0.85 0.86 0.85Asian (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.88 0.88 0.90 0.89 0.86 0.86 0.85 0.84 0.84 0.84
0.87 0.85 0.88 0.86 0.85 0.86 0.84 0.83 0.83 0.83
0.88 0.87 0.90 0.88 0.85 0.86 0.85 0.83 0.83 0.83
0.87 0.86 0.89 0.87 0.85 0.86 0.84 0.83 0.84 0.84
0.86 0.85 0.87 0.85 0.84 0.85 0.85 0.83 0.84 0.84
0.86 0.84 0.86 0.85 0.85 0.85 0.85 0.84 0.86 0.85
0.85 0.85 0.86 0.86 0.85 0.85 0.85 0.84 0.86 0.85
0.84 0.83 0.85 0.84 0.83 0.84 0.84 0.83 0.84 0.84
0.85 0.83 0.84 0.84 0.84 0.86 0.86 0.86 0.87 0.87
0.84 0.83 0.85 0.84 0.84 0.85 0.86 0.85 0.87 0.86Black (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.91 0.89 0.87 0.86 0.85 0.85 0.85 0.83 0.83
0.89 0.89 0.86 0.85 0.84 0.85 0.83 0.83 0.83
0.87 0.86 0.84 0.84 0.84 0.84 0.83 0.82 0.83
0.85 0.84 0.84 0.85 0.84 0.86 0.84 0.84
0.86 0.85 0.85 0.87 0.86 0.86 0.86 0.85 0.84
0.85 0.85 0.85 0.86 0.85 0.85 0.85 0.83 0.84
0.85 0.85 0.83 0.85 0.86 0.84 0.84 0.84 0.85
0.83 0.84 0.82 0.85 0.85 0.84 0.86 0.83 0.84
0.83 0.83 0.83 0.84 0.85 0.84 0.86 0.84 0.85Hispanic (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.90 0.89 0.88 0.88 0.86 0.87 0.84 0.84 0.85 0.83
0.90 0.87 0.86 0.86 0.84 0.86 0.84 0.83 0.85 0.82
0.89 0.88 0.86 0.87 0.83 0.85 0.84 0.82 0.84 0.82
0.89 0.86 0.86 0.85 0.83 0.84 0.83 0.82 0.85 0.81
0.87 0.84 0.84 0.84 0.83 0.85 0.85 0.84 0.84 0.84
0.89 0.87 0.87 0.87 0.86 0.87 0.86 0.84 0.84 0.84
0.86 0.84 0.83 0.84 0.84 0.86 0.86 0.86 0.86 0.85
0.83 0.82 0.81 0.82 0.84 0.85 0.87 0.86 0.85 0.85
0.85 0.85 0.85 0.84 0.84 0.85 0.86 0.86 0.85 0.85
0.83 0.82 0.81 0.82 0.83 0.83 0.84 0.85 0.84 0.84White (Precision)
0.820.840.860.880.90
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.28 0.29 0.33 0.31 0.29 0.32 0.35 0.34 0.34 0.35
0.29 0.29 0.33 0.31 0.30 0.32 0.34 0.34 0.33 0.34
0.31 0.31 0.35 0.33 0.32 0.34 0.38 0.35 0.36 0.37
0.30 0.29 0.33 0.32 0.30 0.31 0.34 0.33 0.33 0.34
0.28 0.29 0.32 0.30 0.29 0.31 0.33 0.32 0.32 0.33
0.30 0.30 0.32 0.30 0.30 0.30 0.33 0.32 0.31 0.32
0.33 0.31 0.36 0.33 0.31 0.31 0.33 0.32 0.32 0.32
0.33 0.31 0.35 0.32 0.31 0.32 0.33 0.32 0.32 0.32
0.31 0.31 0.34 0.32 0.30 0.31 0.33 0.32 0.33 0.32
0.33 0.31 0.36 0.33 0.31 0.32 0.32 0.32 0.32 0.32Asian (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.31 0.31 0.29 0.31 0.33 0.34 0.34 0.35 0.37 0.36
0.31 0.31 0.30 0.31 0.34 0.34 0.34 0.36 0.37 0.37
0.29 0.29 0.27 0.28 0.32 0.32 0.33 0.34 0.35 0.36
0.29 0.29 0.28 0.28 0.31 0.32 0.32 0.33 0.34 0.34
0.31 0.32 0.31 0.31 0.32 0.31 0.32 0.33 0.32 0.33
0.31 0.31 0.31 0.30 0.31 0.32 0.30 0.31 0.31 0.31
0.32 0.32 0.32 0.31 0.31 0.30 0.30 0.30 0.30 0.30
0.33 0.33 0.33 0.32 0.32 0.31 0.31 0.31 0.31 0.30
0.34 0.33 0.34 0.32 0.31 0.30 0.30 0.29 0.28 0.28
0.34 0.34 0.34 0.33 0.32 0.30 0.30 0.29 0.28 0.28Black (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.23 0.26 0.29 0.33 0.34 0.33 0.37 0.42 0.40
0.25 0.28 0.31 0.33 0.34 0.32 0.36 0.39 0.38
0.28 0.31 0.33 0.35 0.35 0.34 0.38 0.42 0.42
0.32 0.33 0.34 0.30 0.31 0.30 0.33 0.32
0.32 0.32 0.33 0.30 0.29 0.30 0.28 0.33 0.31
0.30 0.31 0.33 0.31 0.30 0.32 0.31 0.35 0.34
0.34 0.33 0.35 0.29 0.28 0.30 0.29 0.31 0.28
0.38 0.38 0.40 0.33 0.33 0.34 0.32 0.36 0.33
0.37 0.36 0.39 0.31 0.30 0.32 0.28 0.32 0.29Hispanic (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.26 0.28 0.29 0.31 0.36 0.31 0.37 0.40 0.37 0.42
0.27 0.29 0.30 0.32 0.37 0.32 0.37 0.40 0.38 0.42
0.27 0.29 0.29 0.31 0.35 0.30 0.35 0.39 0.35 0.40
0.29 0.31 0.31 0.33 0.35 0.32 0.35 0.40 0.37 0.39
0.33 0.34 0.34 0.34 0.34 0.30 0.32 0.34 0.33 0.34
0.28 0.29 0.28 0.30 0.31 0.28 0.30 0.33 0.31 0.33
0.33 0.33 0.32 0.33 0.31 0.29 0.31 0.31 0.30 0.31
0.36 0.37 0.36 0.37 0.32 0.31 0.30 0.29 0.30 0.26
0.34 0.35 0.34 0.35 0.32 0.30 0.30 0.31 0.32 0.30
0.39 0.38 0.37 0.37 0.31 0.32 0.29 0.26 0.29 0.23White (Recall)
0.2500.2750.3000.3250.3500.3750.400
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.43 0.43 0.48 0.46 0.44 0.47 0.49 0.49 0.48 0.50
0.44 0.44 0.47 0.46 0.44 0.46 0.48 0.48 0.48 0.49
0.45 0.46 0.49 0.48 0.46 0.48 0.52 0.50 0.50 0.51
0.44 0.44 0.47 0.46 0.45 0.46 0.49 0.48 0.48 0.48
0.43 0.43 0.46 0.45 0.44 0.45 0.48 0.47 0.47 0.47
0.44 0.44 0.47 0.45 0.44 0.45 0.47 0.46 0.46 0.46
0.47 0.46 0.50 0.47 0.45 0.46 0.48 0.46 0.46 0.46
0.47 0.46 0.49 0.47 0.46 0.47 0.47 0.47 0.47 0.46
0.45 0.45 0.49 0.47 0.44 0.46 0.47 0.46 0.47 0.46
0.47 0.46 0.50 0.47 0.45 0.46 0.47 0.46 0.47 0.46Asian (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.46 0.45 0.44 0.46 0.48 0.48 0.48 0.50 0.51 0.50
0.45 0.45 0.44 0.45 0.49 0.48 0.48 0.50 0.51 0.51
0.43 0.43 0.42 0.42 0.46 0.47 0.47 0.49 0.49 0.50
0.44 0.43 0.42 0.43 0.46 0.46 0.46 0.47 0.48 0.49
0.46 0.46 0.45 0.45 0.47 0.46 0.46 0.47 0.47 0.48
0.46 0.45 0.45 0.44 0.45 0.46 0.45 0.46 0.46 0.45
0.46 0.46 0.46 0.45 0.46 0.45 0.44 0.44 0.44 0.44
0.47 0.47 0.47 0.46 0.47 0.45 0.45 0.45 0.45 0.44
0.48 0.48 0.48 0.47 0.45 0.44 0.44 0.43 0.43 0.42
0.48 0.48 0.49 0.47 0.46 0.45 0.44 0.44 0.43 0.42Black (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.37 0.41 0.43 0.48 0.49 0.47 0.51 0.55 0.54
0.39 0.42 0.45 0.48 0.48 0.46 0.50 0.53 0.52
0.42 0.45 0.47 0.49 0.49 0.49 0.52 0.55 0.55
0.46 0.47 0.48 0.44 0.45 0.44 0.48 0.47
0.47 0.47 0.47 0.45 0.44 0.45 0.42 0.47 0.45
0.44 0.45 0.47 0.45 0.45 0.46 0.45 0.49 0.48
0.48 0.47 0.49 0.43 0.42 0.44 0.44 0.45 0.43
0.52 0.52 0.53 0.48 0.47 0.49 0.46 0.51 0.48
0.51 0.50 0.53 0.46 0.45 0.47 0.42 0.46 0.43Hispanic (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.41 0.42 0.43 0.46 0.51 0.46 0.51 0.54 0.51 0.56
0.42 0.43 0.44 0.46 0.51 0.46 0.51 0.54 0.52 0.56
0.42 0.43 0.43 0.46 0.49 0.45 0.49 0.53 0.49 0.54
0.43 0.45 0.45 0.47 0.49 0.46 0.49 0.53 0.50 0.53
0.47 0.48 0.48 0.49 0.48 0.45 0.47 0.48 0.47 0.48
0.42 0.43 0.42 0.45 0.45 0.42 0.45 0.47 0.45 0.47
0.47 0.47 0.46 0.48 0.45 0.44 0.45 0.45 0.45 0.45
0.51 0.51 0.50 0.51 0.46 0.46 0.45 0.44 0.44 0.40
0.48 0.49 0.48 0.49 0.46 0.45 0.45 0.45 0.46 0.44
0.53 0.52 0.50 0.51 0.45 0.46 0.43 0.40 0.43 0.36White (F1)
0.3750.4000.4250.4500.4750.5000.5250.550
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.41 0.41 0.43 0.42 0.41 0.43 0.44 0.44 0.43 0.44
0.41 0.40 0.43 0.42 0.41 0.42 0.43 0.43 0.43 0.43
0.42 0.42 0.44 0.43 0.42 0.43 0.45 0.44 0.44 0.44
0.41 0.41 0.43 0.42 0.42 0.42 0.44 0.43 0.44 0.43
0.41 0.41 0.42 0.42 0.41 0.42 0.43 0.43 0.43 0.43
0.41 0.41 0.43 0.41 0.41 0.41 0.42 0.42 0.42 0.42
0.42 0.42 0.44 0.43 0.41 0.42 0.43 0.42 0.42 0.42
0.43 0.42 0.44 0.43 0.42 0.43 0.43 0.43 0.43 0.42
0.42 0.42 0.44 0.43 0.41 0.42 0.43 0.42 0.43 0.42
0.43 0.42 0.44 0.43 0.42 0.42 0.42 0.42 0.43 0.42Asian (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.43 0.42 0.42 0.42 0.43 0.43 0.43 0.44 0.44 0.44
0.42 0.41 0.41 0.42 0.43 0.44 0.43 0.44 0.44 0.44
0.41 0.40 0.40 0.40 0.42 0.42 0.43 0.43 0.43 0.43
0.41 0.40 0.40 0.40 0.42 0.42 0.42 0.42 0.43 0.43
0.42 0.42 0.42 0.41 0.42 0.42 0.42 0.42 0.42 0.43
0.42 0.41 0.42 0.41 0.41 0.42 0.41 0.41 0.42 0.41
0.42 0.42 0.42 0.42 0.42 0.41 0.41 0.41 0.41 0.41
0.42 0.42 0.42 0.42 0.42 0.41 0.41 0.41 0.41 0.41
0.43 0.42 0.43 0.42 0.41 0.41 0.41 0.40 0.40 0.40
0.43 0.43 0.43 0.42 0.42 0.41 0.41 0.41 0.40 0.40Black (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.38 0.40 0.41 0.43 0.43 0.43 0.45 0.48 0.47
0.39 0.41 0.42 0.43 0.43 0.42 0.44 0.46 0.45
0.40 0.42 0.42 0.43 0.43 0.43 0.45 0.47 0.47
0.42 0.42 0.43 0.40 0.41 0.41 0.43 0.42
0.43 0.42 0.43 0.41 0.41 0.41 0.40 0.43 0.41
0.41 0.41 0.42 0.42 0.41 0.42 0.41 0.43 0.43
0.43 0.43 0.43 0.40 0.40 0.40 0.40 0.41 0.40
0.45 0.45 0.46 0.43 0.42 0.43 0.42 0.44 0.43
0.45 0.44 0.46 0.41 0.41 0.42 0.40 0.41 0.40Hispanic (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.40 0.40 0.41 0.42 0.45 0.42 0.45 0.47 0.45 0.48
0.40 0.41 0.41 0.42 0.45 0.42 0.44 0.46 0.46 0.47
0.40 0.41 0.40 0.42 0.43 0.41 0.44 0.46 0.44 0.46
0.41 0.41 0.42 0.42 0.43 0.42 0.43 0.45 0.44 0.45
0.43 0.43 0.43 0.43 0.42 0.41 0.42 0.43 0.42 0.43
0.40 0.41 0.40 0.42 0.41 0.40 0.41 0.42 0.41 0.42
0.43 0.42 0.42 0.43 0.41 0.41 0.42 0.41 0.41 0.41
0.44 0.44 0.43 0.44 0.42 0.42 0.41 0.41 0.41 0.39
0.43 0.44 0.43 0.43 0.41 0.41 0.41 0.41 0.42 0.41
0.45 0.45 0.44 0.44 0.41 0.41 0.40 0.38 0.40 0.36White (Accuracy)
0.380.400.420.440.46Figure 6: Precision, Recall, F1-score and Accuracy plots for romantic predictions from Llama2-13B model.0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.84 0.84 0.84 0.85 0.84 0.83 0.83 0.85 0.84 0.84
0.85 0.84 0.85 0.84 0.84 0.83 0.83 0.84 0.83 0.84
0.85 0.84 0.84 0.84 0.84 0.84 0.83 0.85 0.84 0.84
0.84 0.85 0.83 0.86 0.84 0.83 0.82 0.84 0.83 0.84
0.85 0.84 0.84 0.84 0.84 0.83 0.82 0.85 0.83 0.83
0.85 0.84 0.83 0.84 0.84 0.83 0.82 0.84 0.82 0.83
0.86 0.84 0.84 0.84 0.84 0.83 0.83 0.83 0.83 0.83
0.84 0.83 0.83 0.84 0.83 0.82 0.83 0.83 0.82 0.83
0.85 0.84 0.83 0.84 0.83 0.83 0.83 0.83 0.83 0.83
0.85 0.84 0.84 0.84 0.84 0.83 0.83 0.83 0.83 0.83Asian (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.84 0.84 0.84 0.84 0.84 0.84 0.83 0.84 0.83 0.84
0.84 0.84 0.84 0.84 0.84 0.83 0.84 0.83 0.82 0.83
0.84 0.84 0.84 0.83 0.83 0.83 0.82 0.83 0.83 0.83
0.83 0.84 0.83 0.82 0.83 0.83 0.83 0.83 0.83 0.83
0.84 0.84 0.84 0.84 0.83 0.83 0.84 0.83 0.82 0.82
0.84 0.84 0.84 0.84 0.82 0.83 0.83 0.82 0.82 0.82
0.84 0.84 0.84 0.85 0.84 0.84 0.84 0.83 0.82 0.83
0.84 0.84 0.84 0.84 0.83 0.83 0.82 0.82 0.82 0.81
0.84 0.83 0.84 0.85 0.82 0.82 0.84 0.82 0.81 0.82
0.83 0.82 0.84 0.84 0.83 0.83 0.84 0.83 0.83 0.82Black (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.83 0.84 0.84 0.83 0.84 0.84 0.83 0.83 0.83
0.84 0.84 0.84 0.84 0.84 0.84 0.83 0.83 0.83
0.84 0.84 0.84 0.83 0.83 0.83 0.82 0.83 0.82
0.84 0.84 0.84 0.81 0.81 0.81 0.82 0.78
0.86 0.84 0.85 0.83 0.83 0.82 0.82 0.83 0.81
0.85 0.84 0.83 0.84 0.83 0.82 0.81 0.83 0.82
0.84 0.83 0.83 0.81 0.83 0.83 0.81 0.83 0.81
0.85 0.85 0.84 0.84 0.84 0.84 0.83 0.83 0.82
0.83 0.84 0.82 0.82 0.83 0.83 0.82 0.83 0.82Hispanic (Precision)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.83 0.83 0.83 0.84 0.83 0.82 0.82 0.82 0.82 0.83
0.83 0.84 0.83 0.83 0.83 0.82 0.82 0.83 0.82 0.83
0.84 0.84 0.84 0.84 0.83 0.83 0.83 0.83 0.84 0.84
0.85 0.85 0.85 0.84 0.84 0.84 0.83 0.82 0.83 0.84
0.86 0.84 0.84 0.84 0.83 0.84 0.82 0.83 0.83 0.83
0.85 0.84 0.85 0.85 0.84 0.83 0.83 0.83 0.83 0.84
0.84 0.85 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.82
0.84 0.84 0.84 0.83 0.83 0.83 0.83 0.82 0.83 0.82
0.84 0.82 0.83 0.83 0.82 0.84 0.82 0.81 0.83 0.82
0.83 0.82 0.83 0.82 0.82 0.82 0.82 0.81 0.82 0.80White (Precision)
0.800.810.820.830.840.85
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.18 0.21 0.20 0.19 0.20 0.21 0.23 0.21 0.21 0.24
0.20 0.22 0.21 0.20 0.21 0.21 0.22 0.21 0.21 0.23
0.21 0.22 0.22 0.21 0.22 0.22 0.24 0.22 0.23 0.24
0.19 0.20 0.21 0.19 0.20 0.20 0.21 0.20 0.21 0.22
0.21 0.22 0.22 0.21 0.22 0.22 0.22 0.22 0.21 0.23
0.20 0.21 0.22 0.21 0.21 0.21 0.22 0.21 0.21 0.22
0.23 0.22 0.23 0.21 0.22 0.21 0.22 0.21 0.21 0.22
0.21 0.20 0.21 0.20 0.20 0.20 0.20 0.20 0.20 0.20
0.22 0.22 0.22 0.21 0.21 0.22 0.21 0.20 0.21 0.21
0.24 0.23 0.24 0.22 0.23 0.22 0.22 0.21 0.22 0.22Asian (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.19 0.19 0.20 0.19 0.21 0.23 0.23 0.26 0.27 0.28
0.19 0.20 0.20 0.19 0.21 0.23 0.22 0.26 0.27 0.27
0.20 0.20 0.20 0.19 0.21 0.23 0.22 0.25 0.26 0.26
0.18 0.19 0.19 0.17 0.19 0.20 0.20 0.22 0.22 0.23
0.20 0.20 0.20 0.19 0.20 0.21 0.20 0.21 0.21 0.21
0.22 0.23 0.22 0.20 0.21 0.22 0.20 0.21 0.22 0.21
0.22 0.21 0.21 0.19 0.20 0.20 0.19 0.20 0.19 0.19
0.24 0.24 0.24 0.21 0.20 0.21 0.20 0.19 0.19 0.18
0.27 0.26 0.26 0.23 0.21 0.21 0.19 0.19 0.18 0.17
0.28 0.27 0.27 0.22 0.22 0.22 0.20 0.18 0.18 0.16Black (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.16 0.18 0.20 0.21 0.24 0.24 0.27 0.25 0.33
0.17 0.19 0.21 0.22 0.24 0.26 0.28 0.27 0.35
0.20 0.21 0.21 0.21 0.22 0.24 0.23 0.24 0.28
0.22 0.22 0.21 0.20 0.21 0.20 0.21 0.22
0.23 0.23 0.22 0.20 0.20 0.20 0.19 0.21 0.21
0.24 0.25 0.23 0.21 0.21 0.22 0.19 0.21 0.22
0.26 0.28 0.24 0.21 0.19 0.20 0.16 0.21 0.18
0.24 0.26 0.24 0.21 0.22 0.22 0.21 0.23 0.23
0.31 0.35 0.27 0.23 0.22 0.22 0.18 0.22 0.19Hispanic (Recall)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.16 0.20 0.17 0.19 0.24 0.19 0.24 0.24 0.24 0.29
0.19 0.21 0.20 0.21 0.24 0.21 0.24 0.24 0.24 0.26
0.18 0.20 0.18 0.20 0.25 0.20 0.24 0.25 0.25 0.28
0.19 0.21 0.20 0.22 0.24 0.20 0.23 0.23 0.23 0.25
0.23 0.23 0.24 0.24 0.23 0.22 0.21 0.20 0.22 0.22
0.19 0.21 0.19 0.21 0.22 0.19 0.21 0.21 0.21 0.24
0.24 0.24 0.24 0.23 0.21 0.21 0.18 0.17 0.18 0.17
0.25 0.24 0.26 0.24 0.21 0.21 0.16 0.15 0.17 0.15
0.24 0.24 0.24 0.23 0.21 0.21 0.18 0.17 0.19 0.18
0.27 0.26 0.27 0.25 0.21 0.23 0.16 0.14 0.17 0.13White (Recall)
0.140.160.180.200.220.240.260.28
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.29 0.34 0.33 0.31 0.33 0.33 0.35 0.34 0.34 0.37
0.33 0.34 0.34 0.32 0.33 0.33 0.35 0.33 0.33 0.36
0.34 0.35 0.35 0.33 0.35 0.35 0.37 0.35 0.36 0.37
0.31 0.32 0.33 0.32 0.33 0.33 0.34 0.32 0.33 0.35
0.34 0.35 0.35 0.34 0.34 0.34 0.35 0.35 0.34 0.36
0.33 0.34 0.34 0.33 0.34 0.34 0.34 0.33 0.33 0.35
0.36 0.35 0.36 0.33 0.35 0.34 0.35 0.33 0.34 0.34
0.34 0.33 0.34 0.32 0.33 0.32 0.32 0.32 0.32 0.33
0.34 0.35 0.35 0.33 0.34 0.34 0.33 0.33 0.33 0.34
0.37 0.36 0.38 0.34 0.36 0.35 0.35 0.33 0.35 0.35Asian (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.31 0.32 0.32 0.31 0.34 0.36 0.35 0.40 0.40 0.42
0.32 0.32 0.32 0.31 0.33 0.36 0.35 0.39 0.40 0.41
0.32 0.32 0.32 0.30 0.34 0.36 0.34 0.38 0.40 0.40
0.30 0.31 0.30 0.27 0.31 0.32 0.32 0.34 0.35 0.36
0.33 0.33 0.33 0.30 0.33 0.34 0.33 0.34 0.33 0.34
0.35 0.36 0.35 0.33 0.33 0.35 0.33 0.34 0.34 0.34
0.35 0.34 0.34 0.30 0.33 0.33 0.31 0.32 0.31 0.31
0.38 0.37 0.37 0.34 0.33 0.34 0.32 0.31 0.31 0.29
0.40 0.39 0.40 0.36 0.33 0.34 0.32 0.30 0.30 0.28
0.42 0.41 0.41 0.35 0.34 0.35 0.32 0.30 0.29 0.27Black (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.27 0.29 0.32 0.34 0.37 0.37 0.40 0.39 0.47
0.29 0.31 0.34 0.35 0.37 0.40 0.42 0.40 0.49
0.33 0.33 0.34 0.33 0.35 0.37 0.36 0.37 0.41
0.34 0.34 0.33 0.32 0.33 0.32 0.33 0.34
0.36 0.36 0.35 0.32 0.32 0.32 0.31 0.34 0.33
0.37 0.39 0.36 0.33 0.33 0.35 0.31 0.34 0.34
0.40 0.42 0.37 0.33 0.31 0.32 0.26 0.33 0.29
0.38 0.40 0.37 0.34 0.35 0.35 0.33 0.36 0.35
0.45 0.49 0.41 0.36 0.34 0.34 0.29 0.35 0.31Hispanic (F1)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.27 0.32 0.29 0.31 0.37 0.31 0.38 0.37 0.37 0.43
0.32 0.34 0.32 0.33 0.37 0.33 0.37 0.37 0.37 0.40
0.29 0.32 0.29 0.32 0.39 0.32 0.37 0.38 0.38 0.42
0.31 0.34 0.32 0.35 0.37 0.33 0.36 0.36 0.36 0.39
0.37 0.37 0.37 0.37 0.36 0.35 0.33 0.33 0.34 0.34
0.30 0.33 0.32 0.34 0.35 0.31 0.34 0.33 0.34 0.37
0.38 0.37 0.37 0.36 0.33 0.33 0.30 0.28 0.30 0.28
0.38 0.37 0.39 0.37 0.33 0.34 0.27 0.25 0.28 0.25
0.37 0.37 0.38 0.36 0.34 0.34 0.30 0.29 0.31 0.29
0.41 0.39 0.41 0.38 0.34 0.36 0.27 0.24 0.28 0.22White (F1)
0.2250.2500.2750.3000.3250.3500.3750.4000.425
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0-2
2-5
5-10
10-25
25-50
50-75
75-90
90-95
95-98
98-100% Female0.33 0.36 0.35 0.34 0.35 0.35 0.36 0.35 0.35 0.37
0.34 0.35 0.35 0.34 0.35 0.35 0.36 0.35 0.35 0.36
0.35 0.36 0.36 0.35 0.36 0.36 0.36 0.36 0.36 0.37
0.34 0.35 0.35 0.34 0.35 0.34 0.35 0.35 0.35 0.36
0.35 0.36 0.36 0.35 0.35 0.35 0.35 0.36 0.35 0.36
0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.36
0.37 0.36 0.36 0.35 0.36 0.35 0.35 0.35 0.35 0.35
0.35 0.35 0.35 0.34 0.34 0.34 0.34 0.34 0.34 0.34
0.36 0.36 0.36 0.35 0.35 0.35 0.35 0.34 0.35 0.35
0.37 0.37 0.37 0.36 0.36 0.36 0.36 0.35 0.35 0.35Asian (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.34 0.34 0.34 0.34 0.35 0.36 0.36 0.38 0.38 0.39
0.34 0.35 0.34 0.34 0.35 0.36 0.36 0.38 0.38 0.38
0.34 0.35 0.34 0.33 0.35 0.36 0.35 0.37 0.38 0.38
0.33 0.34 0.33 0.32 0.33 0.34 0.34 0.35 0.35 0.36
0.35 0.35 0.35 0.33 0.34 0.35 0.35 0.35 0.35 0.35
0.36 0.36 0.36 0.35 0.35 0.35 0.34 0.35 0.35 0.35
0.36 0.35 0.35 0.34 0.35 0.35 0.34 0.34 0.33 0.34
0.37 0.37 0.37 0.35 0.34 0.35 0.34 0.34 0.33 0.32
0.39 0.38 0.38 0.36 0.35 0.35 0.34 0.33 0.33 0.32
0.39 0.38 0.39 0.36 0.35 0.36 0.34 0.33 0.33 0.31Black (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.32 0.33 0.34 0.35 0.37 0.37 0.38 0.38 0.42
0.33 0.34 0.35 0.36 0.37 0.39 0.39 0.38 0.44
0.35 0.35 0.35 0.35 0.36 0.36 0.36 0.36 0.39
0.36 0.35 0.35 0.34 0.34 0.34 0.34 0.34
0.37 0.36 0.36 0.34 0.34 0.34 0.33 0.35 0.34
0.37 0.38 0.36 0.35 0.34 0.35 0.33 0.35 0.35
0.38 0.39 0.36 0.34 0.34 0.34 0.31 0.35 0.32
0.37 0.39 0.37 0.35 0.36 0.36 0.35 0.36 0.35
0.41 0.44 0.38 0.36 0.35 0.35 0.32 0.36 0.33Hispanic (Accuracy)
0-2 2-55-1010-25 25-50 50-75 75-90 90-95 95-9898-100
% Female0.32 0.34 0.33 0.34 0.36 0.34 0.37 0.36 0.36 0.40
0.34 0.35 0.34 0.35 0.36 0.34 0.36 0.36 0.36 0.38
0.33 0.35 0.33 0.34 0.38 0.34 0.37 0.37 0.37 0.40
0.34 0.35 0.35 0.36 0.37 0.35 0.36 0.36 0.36 0.38
0.37 0.37 0.37 0.37 0.36 0.36 0.35 0.34 0.35 0.35
0.34 0.35 0.34 0.35 0.36 0.34 0.35 0.35 0.35 0.37
0.37 0.37 0.37 0.36 0.35 0.35 0.33 0.32 0.33 0.32
0.37 0.37 0.38 0.37 0.35 0.35 0.32 0.31 0.32 0.31
0.37 0.36 0.37 0.36 0.35 0.35 0.33 0.32 0.34 0.33
0.39 0.38 0.39 0.37 0.35 0.36 0.32 0.30 0.32 0.29White (Accuracy)
0.300.320.340.360.38Figure 7: Precision, Recall, F1-score and Accuracy plots for romantic predictions from Mistral-7B model.
Asian Black
HispanicWhite
Male RaceAsian Black Hispanic WhiteFemale Race0.82 0.82 0.82 0.82
0.81 0.81 0.80 0.81
0.80 0.80 0.80 0.80
0.80 0.80 0.80 0.80Precision
Asian Black
HispanicWhite
Male Race0.68 0.72 0.72 0.70
0.78 0.83 0.84 0.84
0.82 0.87 0.87 0.87
0.79 0.84 0.85 0.85Recall
Asian Black
HispanicWhite
Male Race0.74 0.76 0.76 0.75
0.79 0.82 0.82 0.82
0.81 0.83 0.83 0.84
0.79 0.82 0.82 0.83F1
Asian Black
HispanicWhite
Male Race0.63 0.65 0.65 0.65
0.68 0.71 0.71 0.72
0.70 0.73 0.73 0.73
0.68 0.71 0.72 0.72Accuracy
0.80250.80500.80750.81000.81250.81500.8175
0.7000.7250.7500.7750.8000.8250.850
0.740.760.780.800.82
0.640.660.680.700.72
Figure 8: Precision, Recall, F1, and Accuracy of predicting romantic relationships from Llama2-7B for subset
of the dataset where characters have different genders and are replaced with names associated with different
races/ethnicities.Asian Black
HispanicWhite
Male RaceAsian Black Hispanic WhiteFemale Race0.85 0.87 0.86 0.86
0.84 0.83 0.84 0.83
0.84 0.84 0.84 0.83
0.84 0.84 0.84 0.84Precision
Asian Black
HispanicWhite
Male Race0.38 0.37 0.38 0.37
0.40 0.42 0.43 0.40
0.43 0.45 0.47 0.44
0.40 0.42 0.44 0.42Recall
Asian Black
HispanicWhite
Male Race0.52 0.51 0.53 0.51
0.54 0.55 0.56 0.54
0.57 0.59 0.60 0.57
0.54 0.56 0.57 0.56F1
Asian Black
HispanicWhite
Male Race0.47 0.47 0.48 0.47
0.48 0.48 0.49 0.48
0.49 0.51 0.52 0.50
0.48 0.49 0.50 0.49Accuracy
0.8350.8400.8450.8500.8550.8600.865
0.380.400.420.440.46
0.520.540.560.58
0.470.480.490.500.51
Figure 9: Precision, Recall, F1, and Accuracy of predicting romantic relationships from Llama2-13B for subset
of the dataset where characters have different genders and are replaced with names associated with different
races/ethnicities.
Asian Black
HispanicWhite
Male RaceAsian Black Hispanic WhiteFemale Race0.85 0.85 0.85 0.85
0.84 0.85 0.85 0.85
0.84 0.85 0.85 0.85
0.84 0.85 0.85 0.86Precision
Asian Black
HispanicWhite
Male Race0.26 0.27 0.26 0.27
0.26 0.31 0.30 0.33
0.28 0.33 0.33 0.34
0.27 0.32 0.32 0.35Recall
Asian Black
HispanicWhite
Male Race0.39 0.40 0.40 0.41
0.40 0.46 0.45 0.47
0.42 0.47 0.48 0.49
0.41 0.46 0.46 0.49F1
Asian Black
HispanicWhite
Male Race0.39 0.40 0.40 0.41
0.40 0.43 0.42 0.44
0.41 0.44 0.44 0.45
0.40 0.43 0.43 0.45Accuracy
0.84000.84250.84500.84750.85000.85250.8550
0.260.280.300.320.34
0.400.420.440.460.48
0.400.410.420.430.440.45
Figure 10: Precision, Recall, F1, and Accuracy of predicting romantic relationships from Mistral-7B for subset
of the dataset where characters have different genders and are replaced with names associated with different
races/ethnicities.