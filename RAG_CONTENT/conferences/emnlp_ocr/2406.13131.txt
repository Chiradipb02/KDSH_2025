When Parts Are Greater Than Sums:
Individual LLM Components Can Outperform Full Models
Ting-Yun Chang Jesse Thomason Robin Jia
University of Southern California, Los Angeles, CA, USA
{tingyun, jessetho, robinjia}@usc.edu
Abstract
This paper studies in-context learning by de-
composing the output of large language models
into the individual contributions of attention
heads and MLPs ( components ). We observe
curious components: good-performing ones
that individually do well on a classification
task, even when the full model performs poorly;
bad-performing ones that do much worse than
chance; and label-biased components that al-
ways predict the same label. We find that com-
ponent accuracies are well-correlated across
different demonstration sets and perturbations
of prompt templates. Based on our findings, we
propose component reweighting, which learns
to linearly re-scale the component activations
from a few labeled examples. Given 24labeled
examples, our method improves by an aver-
age of 6.0%accuracy points over 24-shot ICL
across 8 tasks on Llama-2-7B. Overall, this pa-
per both enriches our understanding of ICL and
provides a practical method for improvement
by examining model internals.
1 Introduction
The rapid progress in large language models
(LLMs) has popularized prompting, which guides
LLMs to perform tasks with instructions or exam-
ples. Notably, in-context learning (ICL; Brown
et al., 2020) adapts LLMs to a new task using only
a few labeled examples without parameter updates.
However, how LLMs react to the in-context exam-
ples is sometimes unintuitive (Min et al., 2022b).
Recently, Sclar et al. (2024) and V oronov et al.
(2024) find that even for instruction-tuned (Ouyang
et al., 2022) or very large models, adding a space
or newline in prompts can greatly affect accuracy.
We look into the LLM internals to understand
what causes the surprising behavior across vari-
ous ICL settings. Our work stands in contrast
to prior studies, which often treat LLMs as black
boxes and alter either the input (Chen et al., 2023;
Bertsch et al., 2024) or output (Zhao et al., 2021;
OODT ransferFigure 1: Each dot represents a component (attention
head or MLP) under 4-shot ICL on Llama-2-7B. The
x-axis shows how often a component predicts “posi-
tive” on the test set. Up: We discover good-performing
(blue), bad-performing (red), and label-biased (green)
components. Down: Most components identified on
SST2 show similar characteristics on Yelp-polarity.
Holtzman et al., 2021). We introduce a new view
of ICL by decomposing the output of an LLM
into the sum of individual contributions of MLPs
and attention heads, denoted “components.” Fig-
ure 1 reveals three types of curious components:
good-performing ones (blue) that individually per-
form well or even outperform the full model, bad-
performing ones (red) that perform below chance,
and label-biased ones (green) that predict the same
label on the entire test set. We observe these three
classes of components on Llama-2-7B, Llama-2-
13B (Touvron et al., 2023), Llama-3-8B (Dubey
et al., 2024), and Mistral-Instruct-7B (Jiang et al.,arXiv:2406.13131v3  [cs.CL]  6 Oct 20242023) across 8 classification tasks.
We study the sensitivity of LLM components to
multiple prompts formed by different demonstra-
tions and templates. We also construct contrast sets
of templates—pairs of similar templates that yield
large differences in ICL accuracy. Despite large
variance in full-model accuracy, we find that com-
ponent accuracies correlate well across different
demonstrations ( r= 0.80on average) and contrast
set templates ( r= 0.57). The top-performing com-
ponents in contrast set pairs overlap and achieve
decent accuracy even when the full model performs
near random (Figure 2). Nonetheless, the compo-
nent accuracies of two sampled templates are less
correlated ( r= 0.34). Further, good-performing
components generalize well to out-of-distribution
test sets. For instance, the top-1 component for
MNLI outperforms the full Llama-2-13B model by
9.1%on MedNLI; Figure 1 also shows that com-
ponents are transferrable from SST2 to Yelp. We
conclude that components are relatively consistent
in their behavior across prompts and datasets.
Inspired by our findings, we propose compo-
nent reweighting. Compared to prior work that
selects prompts from a large pool of labeled data
to improve ICL accuracy (Liu et al., 2022b), com-
ponent reweighting softly selects components by
learning weights from few-shot examples to scale
component activations. Training these weights
only involves learning a linear layer, which takes
less than a minute on one CPU. Overall, com-
ponent reweighting better utilizes the same la-
beled examples, improving over 24-shot ICL by
6.0%,2.2%,5.1%,1.6%on Llama-2-7B, Llama-2-
13B, Mistral-Instruct-7B, and Llama-3-8B, respec-
tively. At the same time, it enjoys similar inference
speed as 4-shot ICL.
Finally, we study the training dynamics of com-
ponents using the Pythia pretraining checkpoints
(Biderman et al., 2023). During pretraining, good-
performing components emerge well before the full
model performs well. These findings suggest that
LLMs acquire the internal ability to perform ICL
early in training, but this ability only surfaces in
the full model’s behavior later on.
Overall, our work conducts extensive analysis of
LLM internals, which motivates a practical method
to improve ICL. We hope to inspire future work
that further sheds light on LLM internals in order
to improve performance. Our implementation is
available at https://github.com/terarachang/
LLMDecomp .2 Decomposing the Transformer in ICL
We introduce a new view of in-context learn-
ing by decomposing the Transformer architecture
(Vaswani et al., 2017). Our decomposition is
exact—a mathematically equivalent formula for
the model’s outputs—and enables us to analyze
model internals without training additional param-
eters (unlike, e.g., probing). We first discuss what
our new view offers over the standard view of ICL,
and then walk through the mathematical details.
2.1 A New View of In-Context Learning
Standard view. An LLM performs in-context
learning (ICL) on a task based on a few demon-
strations without training, where each demonstra-
tion is a templated example (x, y)consisting of
an input xand a label word y. We refer to a se-
quence of Kdemonstrations [x1, y1, . . . , x K, yK]
as aprompt . The LLM makes predictions on a test
input xtestconditioned on the prompt, denoted by
arg maxy∈YP(y|prompt , xtest), where Yis the set
of possible label words in a classification task.
Our view. The residual stream of an LLM di-
rectly carries the information of the initial hidden
state, every attention head, and every MLP, col-
lectively named “components,” towards the output
layer. We view this information as the direct con-
tributions1of components to the output logits, and
derive a formula for logits,P
jgj, where gjis
the direct contribution of the component indexed
byj. We can obtain the predictions of compo-
nentjwitharg maxy∈Ygj, and then calculate its
individual ICL accuracy. Specifically, we derive
gj=U·Cjin Eq. 8 below, where Uis the output
embedding matrix and Cjis the post-layernorm
activations of component j. We name the opera-
tion(Cj7→U·Cj)as early decode, sharing the
same spirit as nostalgebraist (2020) and Geva et al.
(2022), which interpret hidden representations by
decoding through U. Compared to the standard
view, we can directly study the behavior of indi-
vidual components (Figure 2), characterizing them
and scaling their contributions to the model output.
2.2 A Walkthrough of the Decomposition
A Transformer of Llayers consists of a multi-
headed attention (MHA) and MLP in every layer.
Leta(l)∈Rdandm(l)∈Rdbe the output of the
1In comparison, a component has indirect contributions to
the output by affecting other components in later layers (Wang
et al., 2023a). This paper focuses on direction contributions.Template 1  {text} \nIs this a piece of news regarding 
World, Sports, Business, or Technology? Template 2  {text}  Is this a piece of news regarding 
World, Sports, Business, or Technology? 
Correlation = 0.81 Component ID Component ID 
Layer ID 
Component Accuracy Full Model Accuracy: 0.39 Full Model Accuracy: 0.89 
Component ID Component ID Figure 2: Left: Transformer decomposition. The components—MLPs and attention heads—are filled with blue, and
the blue lines show the flow of early decoding. Right: We can calculate the individual accuracy of every component
after decomposition. Although a pair of templates that only differ slightly yield very different accuracies ( 0.39
vs.0.89on AGNews with Llama-2-7B), the accuracies of their internal components are highly correlated. The
top components for Template 1 overlap with the ones for Template 2 and achieve >0.7accuracy despite the poor
full-model accuracy.
MHA and MLP at layer l, respectively. Due to
residual connections, the hidden state x(l)∈Rdis:
x(l)=x(l−1)+a(l)+m(l), (1)
x(L)=x(0)+LX
l=1
a(l)+m(l)
. (2)
Note that GPT2-like LLMs apply layernorm before
MHA and MLP (Radford et al., 2019); thus, lay-
ernorm is already taken into account as part of the
formula for computing a(l)andm(l)(see A.3).
An MHA a(l)is composed of nattention heads:
a(l)=W(l)
o·Concat ([h(l)
1, . . . , h(l)
n]) (3)
forh(l)
i∈Rdheada head and W(l)
o∈Rd×ndheadthe
output projection in MHA aggregating all heads.
Elhage et al. (2021) rewrite Eq. 3 by segmenting
W(l)
ointonmatrices W(l)
oi∈Rd×dhead:
a(l)=nX
i=1
W(l)
oi·h(l)
i
=nX
i=1˜h(l)
i, (4)
where [W(l)
o1, . . . , W(l)
on] =W(l)
o (5)
Thus, we can treat each head as a single component
adding ˜h(l)
i=W(l)
oi·h(l)
ito the residual stream.
Finally, through the output embedding matrixU∈R|V ocab|×d, the output logits are:
logits =U·LN(x(L))
=U·LN 
x(0)+LX
l=1nX
i=1˜h(l)
i+LX
l=1m(l)!
=U·LN
1+L×n+LX
j=1zj
, (6)
where z= [x(0),˜h(1)
1, . . . , ˜h(L)
n, m(1), . . . , m(L)]
in Eq. 6 and we index every term in the sum-
mation with j. LN (·)denotes the final layer-
norm, specifically, RMSNorm (Zhang and Sen-
nrich, 2019) for LLMs in our paper (see A.3). In
Eq. 6, LN (P
jzj) =P
jzj
RMS (P
jzj)⊙γ, where RMS
denotes root mean square, ⊙denotes element-wise
multiplication, and γ∈Rdis the affine parameters.
By pre-computing ˆγ=γ
RMS (P
jzj), we have:
logits =U·
X
jzj⊙ˆγ
 (7)
=X
jU·Cj,where Cj=zj⊙ˆγ (8)
We refer to all Cj∈Rdas the component acti-
vations, which include the activations of attentionSST2 BoolQ QQP WiC RTE MNLI AGNews ARC-Easy Avg.Llama2
7BFULL 75.818.169.212.061.39.952.43.068.9 3.234.41.770.019.9 57.5 14.4 61.2
ORACLE -T1 91.7 0.969.7 7.767.8 4.357.8 1.164.62.746.3 3.380.8 5.2 54.510.1 66.6
ORACLE -B1 12.12.734.17.332.53.942.91.234.72.824.12.4 3.01.1 12.74.2 24.5Llama2
13BFULL 89.05.3 77.6 6.871.06.855.03.875.12.345.77.970.820.6 73.2 13.7 69.7
ORACLE -T1 92.5 0.677.56.073.5 2.960.4 1.275.7 2.356.4 4.784.6 3.6 73.17.9 74.2
ORACLE -B1 8.21.027.19.731.83.439.51.627.92.818.62.6 1.80.9 5.43.5 20.0Mistral
Ins 7BFULL 90.12.9 81.3 2.170.97.258.54.280.51.756.15.083.05.7 79.8 1.4 75.0
ORACLE -T1 91.9 0.780.82.075.6 2.660.6 2.281.3 0.861.5 3.3 83.7 4.3 78.52.2 76.7
ORACLE -B1 8.10.919.52.525.84.139.32.820.01.714.62.9 1.80.7 4.61.3 16.7Llama3
8BFULL 91.41.7 79.2 7.274.08.058.74.776.5 2.259.43.7 84.0 6.6 87.4 5.5 76.3
ORACLE -T1 92.3 1.077.47.377.4 3.764.5 2.776.32.960.7 1.581.45.7 86.05.9 77.0
ORACLE -B1 9.00.922.57.423.53.936.73.323.42.110.04.2 1.40.6 1.90.9 16.0
RANDOM 50.0 50 .0 50 .0 50 .0 50 .0 33 .3 25 .0 25 .0 41 .7
Table 1: {3,4}-shot ICL accuracy of 8 tasks and the average accuracy ( Avg.). We run 15 prompts for each task (see
§3) and report the mean accuracy and standard deviation. We show the existence of good components ( ORACLE -T1)
inside LLMs that individually perform on par with the full model ( FULL) on diverse tasks. Similarly, there exist bad
components (O RACLE -B1) that perform substantially below chance (R ANDOM ).
heads and MLPs after the final layernorm.2Now
that we have broken down the Transformer output
into simple additions in Eq. 8, we can easily ana-
lyze the direct contribution of each component to
the logits through the residual stream, gj=U·Cj.
In ICL, we only need to do the decomposition
when LLMs start to generate, i.e, when processing
the last token of the input. The computations on
the other tokens are the same as the standard ICL.
In all our experiments, we use single-token label
words. We use multiple templates from Bach et al.
(2022) that cover diverse label words for each task.
3 Characterizing Components for ICL
We conduct in-context learning across 8 classifica-
tion tasks on 4 LLMs: Llama-2-7B, Llama-2-13B,
Mistral-Instruct-7B, and Llama-3-8B. ICL is sensi-
tive to prompts, so we randomly sample 5 disjoint
sets of demonstrations formatted with 3 templates
and report the standard deviation across the 15 runs.
To avoid majority and recency biases (Zhao et al.,
2021), each prompt consists of the same number of
demonstrations from every class in shuffled order.
We use K= 3demonstrations for 3-way classifi-
cation tasks and K= 4for the other tasks. Except
for §5.1, we refer to K={3,4}without further
notice. We sample 2000 examples with balanced
labels as the test set for every task. Please see A.1
for details about the tasks and templates.
2Empirically, we find that x(0)has near-random ICL accu-
racy on all the tasks, so we omit it in the rest of the paper.3.1 Good and Bad-Performing Components
Across all the tasks and LLMs, we observe good-
performing components that perform well or even
outperform the full model, and bad-performing
components that individually perform much worse
than chance (blue and red dots in Figure 1, re-
spectively). Table 1 compares the full model
(FULL) with the top-1 ( ORACLE -T1) and bottom-1
(ORACLE -B1) components selected on the test set.
On average, ORACLE -T1 outperforms FULL by
5.4%,4.5%,1.7%,0.7%on Llama-2-7B, Llama-2-
13B, Mistral-Instruct-7B, and Llama-3-8B, respec-
tively; ORACLE -B1underperforms random guess-
ing (R ANDOM ) by17.2%,20.7%,25.0%,25.7%.
3.2 Label-Biased Components
Besides good and bad-performing components,
we also observe label-biased components, which
predict a certain label on the entire test set (the
green dots in Figure 1). These components exist
in all the tasks and LLMs we study, accounting
for29.1%,26.4%,22.8%,29.7%of components
on average in Llama-2-7B, Llama-2-13B, Mistral-
Instruct-7B, and Llama-3-8B, respectively (Table
5). In A.2, we show that even when we prompt the
model with all demonstrations of positive labels,
the most biased component still insists on predict-
ing “negative” on the entire test set, and vice versa.
3.3 Mechanistic Understanding of
Bad-Performing Heads
Prior work studies the mechanism of certain com-
ponents in LLMs, showing that there are negativemover attention heads that write in the opposite di-
rection of the expected answer (Wang et al., 2023a)
and copy suppression heads that suppress the pre-
diction of a prior token in the context (McDougall
et al., 2023). Inspired by them, we investigate the
mechanism behind bad-performing heads identified
by our decomposition. We focus on label tokens in
the context, as Wang et al. (2023b) show that label
words serve as anchors in ICL.
We conduct a case study on Llama-2-7B with
4-shot balanced in-context examples from SST2.
We examine the bottom-5 attention heads that have
the worst ICL accuracy on SST2. We find that three
of these heads, L19H15, L15H14, and L18H9, as-
sign top attention probabilities to all 4 label tokens
of the 4-shot in-context examples when predicting
test examples. Furthermore, despite their poor ICL
accuracy, these heads actually assign higher atten-
tion to the correct in-context label tokens than the
incorrect ones most of the time ( >70% of the test
examples). In other words, when a test example
has a positive label, these heads assign higher atten-
tion3to the tokens “positive” in the context than the
tokens “negative”. We also observe that the more
the heads attend to “positive” in the context, the
lower the inner product between the head and the
output embedding of the token “positive”, with the
correlation r=−0.97,−0.96,−0.89for L19H15,
L15H14, and L18H9, respectively.
In summary, we show that some bad-performing
heads attend highly to prior label tokens and de-
crease the output probability of the correct one,
which shares similarities with the copy suppression
heads and negative mover heads (McDougall et al.,
2023; Wang et al., 2023a). However, we do not
observe similar behavior in other tasks, where the
bad-performing heads usually attend to “<s>”, “?”,
or “\n”. We invite future work to further analyze
how bad-performing heads function in general.
4 Transferability of Components
We observe moderate to high component trans-
ferability across demonstrations, minimally con-
trastive templates, and data distributions, whereas
there is little transferability across randomly sam-
pled templates. Our decomposition uncovers hid-
den abilities of individual components when the
full model performs poorly.
3We average the attention probabilities of the same label
tokens and then compare the average ones of the two labels.SST2 BoolQ QQP AGNews ARCCorr(1) Demo 0.81 0.84 0.60 0.89 0.88
(2) Temp 0.40 0.16 0.03 0.68 0.44
(3) Cst T 0.72 0.63 0.23 0.82 0.46IoU(1) Demo 0.36 0.74 0.27 0.63 0.70
(2) Temp 0.12 0.01 0.01 0.20 0.20
(3) Cst T 0.40 0.23 0.02 0.36 0.45
Table 2: The average correlation and IoU between (1)
two random sets of demonstrations, (2) two random
templates, and (3) two minimally contrastive templates.
4.1 Transfer across Prompt Variants
We first measure the agreement in component ac-
curacies between (1) two disjoint sets of demon-
strations with a fixed template, (2) two randomly
sampled templates with fixed demonstrations, and
(3) two minimally-contrastive templates with fixed
demonstrations. Recall that we have 5 sets of
demonstrations and 3 templates in total (§3); here,
we calculate the average agreement between every
pair. For (3), we construct contrast sets (Gard-
ner et al., 2020) by minimally editing the worst-
performing template out of the 3 templates into
a good template, which yields at least 10% im-
provement in average accuracy. Our edits include
adding a space, removing a newline, or changing
label words (see Table 10). We use two metrics to
measure the agreement between each pair: Pear-
son correlation of the accuracies of all components
and the intersection over union (IoU) on the sets
of top-5 components, which measures whether the
top-performing components of the pair overlap.
Table 2 summarizes the results on Llama-2-7B;
A.6 shows similar findings on other models. (1)
The accuracies of the internal components are
highly consistent across different choices of demon-
strations, having strong correlations and an aver-
age of 0.54IoU. (2) The components have much
weaker agreement across randomly sampled tem-
plates, having a near 0 IoU on BoolQ and QQP.
(3) Nevertheless, there is agreement between mini-
mally contrastive templates (Cst T), with an aver-
age correlation of 0.57across tasks, despite con-
trasting full-model accuracy. For example, Figure
2 demonstrates that full-model accuracy changes
dramatically ( 39% vs89%) in a minimal pair of
templates, but internal components have a high cor-
relation of 0.81and the pair shares top-performing
components. Combining (2) and (3) suggests com-
ponents behave similarly on similar templates, butthis similarity decreases as the templates diverge.
4.2 Transfer to Out-of-Distribution Test Sets
We further study whether the best component se-
lected on the test set can still perform well on
an out-of-distribution (OOD) test set. We name
this method, which uses a single component to
make predictions, as TRANSFER -1. Specifically,
we study component transferability from SST2 to
Yelp-polarity, MNLI to MedNLI, and BoolQ to
BoolQ Contrast Set. We compare TRANSFER -1
with using the full model ( FULL) on the OOD test
sets. To understand the best possible TRANSFER -1
accuracy, we also report the best component accu-
racy directly selected on the OOD set, ORACLE -1.
Table 3 shows that TRANSFER -1closely
matches ORACLE -1overall, suggesting that the
top-performing components are transferable across
data distribution. Moreover, TRANSFER -1some-
times outperforms FULL, especially on Llama2
models, showing the hidden abilities of the internal
components.
4.3 Transfer between Two Opposite Tasks
We conduct a case study of component transferabil-
ity across instructions using Task069 and Task070
of Super-NaturalInstructions (Wang et al., 2022b),
both of which are binary abductive NLI tasks (Bha-
gavatula et al., 2020). The instruction for Task069
asks for correct answers, while Task070 asks for
incorrect ones (“pick the one that makes less sense;”
see Figure 7 for the full instructions). Examples in
the two tasks are not parallel.
We find that Mistral-Instruct-7B achieves good
accuracy across 15 runs on Task069 ( 76.8±2.4),
but below chance on Task070 ( 40.6±5.4). We ob-
serve a strong negative correlation, r=−0.60
on average, between the component accuracies
of the two tasks. The worst-performing compo-
nents in Task069 become the top-performing in
Task070 and vice versa. The correlation suggests
that the model has the ability to solve Task070,
but misunderstands negation. Thus, we apply
theTRANSFER -1method (§4.2) but select the
worst-performing component from Task069 and
then calculate its individual accuracy on Task070.
TRANSFER -1achieves 58.7±4.8accuracy across
the 15 runs, an improvement of 18.1%over the
full model. These results suggest that components
behave consistently even across tasks with opposite
instructions, as the active components in Task069
are also active in Task070.Yelp-polarity MedNLI BoolQ CstLlama2
7BFULL 84.715.4 34.31.7 64.9 9.8
TRANSFER -1 94.9 3.1 42.6 4.7 64.37.9
ORACLE -1 96.90.7 48.82.3 66.25.7Llama2
13BFULL 95.91.4 46.89.6 72.07.6
TRANSFER -1 96.0 1.8 55.9 4.0 72.3 6.5
ORACLE -1 97.10.4 57.03.7 73.06.1Mistral
Ins 7BFULL 97.0 0.5 57.35.7 74.6 3.5
TRANSFER -1 95.61.6 61.9 4.8 73.73.7
ORACLE -1 97.10.4 62.74.1 74.53.6Llama3
8BFULL 97.8 0.4 61.02.2 77.3 7.5
TRANSFER -1 95.94.4 61.3 0.8 73.98.4
ORACLE -1 97.90.5 61.60.6 74.88.9
Table 3: The average ICL accuracy and standard devi-
ation on OOD test sets. The components selected on
the in-distribution test sets ( TRANSFER -1) can transfer
to OOD sets, performing similarly to the oracle compo-
nents (O RACLE -1) directly selected on the OOD sets.
5 Component Reweighting
5.1 Proposed Method
Our findings in §4 show the promising direction
of selecting internal components to improve ICL.
Therefore, we propose a method that reweights
components by learning a weight wj∈Ron every
component activation Cj. Reweighting is a soft ver-
sion of selection, which can be learned by gradient
descent on very few examples.
Given Klabeled examples, instead of using all
of them as ICL demonstrations, we divide them
into a demonstration set Ddemo and a training set
Dtrain. We first randomly sample K′={3,4}ex-
amples with balanced labels as demonstrations and
use the remaining examples as Dtrainto train the
component weights. Specifically, we can rewrite
Eq. 8 as logits =P
jwj(U·Cj), where wj= 1
for all j. Because of the existence of good and
bad-performing components, weighing all compo-
nents equally may not be optimal. Therefore, we
tune the weights w∈RNofNcomponents on
Dtrainwith cross-entropy loss and L1regulariza-
tion, while keeping the LLM frozen:
L=X
(x,y)∈D train−logPrw(y|x) +λ∥w∥1,(9)
Prw(y|x) =softmax
NX
j=1wj(UY·Cj)

y,
where UY∈R|Y|×dis a submatrix of Uthat com-
prises the output embeddings of label words, Prw
is the probability distribution of the LLM afterAlgorithm 1 Component Reweighting
1:Input: Klabeled examples, a test set Dtest, a set of label
wordsY, an LLM M, the number of components N
2:Output: Z, the predictions of MonDtest
3:SplitKexamples into a prompt consists of K′demon-
strations and a training set DtrainofK−K′examples
4:UY←concatenate the output embeddings of YinM
5: Initialize Gtrain←∅
6:for(x, y)∈ D traindo
7:{Cj}N
j=1← M (prompt , x) ▷ K′-shot ICL
8: forj←1toNdo
9: Gtrain← Gtrain∪(UY·Cj) ▷early decode
10: end for
11:end for
12: Initialize w←[1, . . . , 1]∈RN
13: Train the weights wonGtrainwith Eq. 9
14: Initialize Z ← ∅ ▷Start Inference
15:for(x, y)∈ D testdo
16: {Cj}N
j=1← M (prompt , x) ▷ K′-shot ICL
17: Initialize g←[0, . . . , 0]∈R|Y|
18: forj←1toNdo ▷Test-Time Overhead
19: g←g+wj(UY·Cj) ▷early decode
20: end for
21: ˆy←arg maxy∈Yg
22: Z ← Z ∪ ˆy
23:end for
24:return Z
reweighting, and λis the hyperparameter of the
L1loss to encourage sparsity on the component
weights. We obtain the activations {Cj}N
j=1of
all components in one K′-shot forward pass, com-
puted on the prompt derived from Ddemo, followed
byx. Our method scales each component’s direct
contributions to the logits ( UY·Cj∈R|Y|) bywj.
In practice, we cache these contributions on the en-
tire training set as input features to the linear layer
w, which allows us to discard the entire LLM while
training w(line 9 and 13 in Algorithm 1), saving
tremendous training time and GPU memory. The
cache only requires O(|Y| × N× |D train|)space.
At inference time, the overhead of our method over
K′-shot ICL is to early decode Ncomponents and
apply the learned weights, i.e.,PN
j=1wj(UY·Cj).
As both |Y|andNare small ( N < 2000 for all
LLMs in this paper), the overhead is negligible
compared to the computation of the LLM itself.
5.2 Baselines
Standard ICL. The simplest baseline is to use all
theKlabeled examples as demonstrations. Since
the other methods use K′examples as demonstra-
tions, we report the accuracy of standard K′-shot
ICL using the same Ddemo for reference.
Prompt Selection. Liu et al. (2022b) improve
ICL accuracy by selecting demonstrations from a
pool of labeled data for each test example. Here, weselect from the given Klabeled examples. Follow-
ing Rubin et al. (2022), we use SBERT (Reimers
and Gurevych, 2019) to encode examples into sen-
tence embeddings and select the K′={3,4}near-
est neighbors under cosine similarity as the demon-
strations for each test example.
Calibration. As LLMs tend to predict a certain
class over others, Zhao et al. (2021) reweight the
output class probabilities. They use context-free
inputs, such as “N/A”, to calibrate the probability
distribution. However, Fei et al. (2023) and Zhou
et al. (2023) find context-free inputs sometimes in-
effective, because in-domain context is important
for calibration. Thus, we introduce CALIB +, which
calibrates the original probabilities p∈R|Y|with
a training set of in-distribution labeled examples,
Dtrain. We train the calibration weights v∈R|Y|
onDtrainwith cross-entropy loss and obtain the
calibrated probabilities ˆp=softmax (v·p). For di-
rect comparisons, we split the Kexamples into the
sameDdemo andDtrainsets as component reweight-
ing for CALIB +, where |Ddemo|=K′. We include
the training details of both methods in A.8.
5.3 Results
We set K={12,24}. Table 4 compares our com-
ponent reweighting ( COMPRW) with standard ICL
(STANDARD ), prompt selection ( PROMPT S), and
calibration ( CALIB +). First, we find that simply in-
creasing the number of demonstrations from 4to24
has limited improvements in ICL accuracy, while
the longer prompt greatly increases the inference
time. For example, on Llama-2-7B, STANDARD 24
only improves the average accuracy by 2.6%over
STANDARD 3,4and the accuracy even decreases
on Mistral-Instruct. Second, PROMPT Sperforms
the worst in most setups, likely because it is hard
to find similar examples from a small pool of Kex-
amples, and a bad selection induces majority label
biases. Third, both calibration ( CALIB +) and com-
ponent reweighting ( COMP RW) achieve substan-
tially better accuracy than STANDARD 3,4with lit-
tle test-time overhead. Overall, COMPRWachieves
the best average accuracy in all setups, outper-
forming STANDARD 12by6.0%,1.8%,2.6%,1.4
on Llama-2-7B, Llama-2-13B, Mistral-Instruct-
7B, and Llama-3-8B, respectively, and outperform-
ingSTANDARD 24by6.0%,2.2%,5.1%,1.6%, re-
spectively. We run one-tailed paired t-tests compar-
ingCOMPRWwith CALIB + and find that p-values
<0.05in all 8 setups (see Table 6), showing thatSST2 BoolQ QQP WiC RTE MNLI AGNews ARC-Easy Avg.Llama-2-7BSTANDARD 3,4 75 .818.169.212.061.39.952.43.068.93.234.41.770.019.9 57.514.4 61.2
STANDARD 12 77 .819.671.6 8.063.67.852.52.471.1 2.137.02.869.020.8 59.6 13.9 62.8
PROMPT S12 73 .819.269.410.562.26.153.12.765.51.835.51.659.128.7 58.711.9 59.7
CALIB +12 85 .16.069.213.673.6 6.155.15.170.32.745.57.877.812.2 58.614.6 66.9
COMP RW12 88.5 2.870.411.271.45.456.3 3.470.02.848.3 4.887.4 2.3 58.313.6 68.8
STANDARD 24 77 .819.571.67.366.45.053.23.371.9 1.539.93.671.120.0 58.316.2 63.8
PROMPT S24 74 .220.468.910.262.14.953.61.964.80.936.41.557.530.2 58.012.5 59.4
CALIB +24 87 .65.070.311.973.4 5.555.84.970.42.746.46.778.411.8 59.2 14.4 67.7
COMP RW24 90.6 1.771.7 9.471.94.457.1 3.070.04.149.8 4.088.1 2.1 58.813.6 69.8Llama-2-13BSTANDARD 3,4 89 .05.377.66.871.06.855.03.875.12.345.77.970.820.6 73.213.7 69.7
STANDARD 12 91.3 1.978.17.470.57.359.6 2.474.43.555.16.284.77.8 71.216.4 73.1
PROMPT S12 83 .810.274.96.664.65.757.02.169.53.548.15.464.429.6 74.29.3 67.1
CALIB +12 89 .43.278.4 6.172.14.158.15.175.31.957.34.581.58.7 74.79.3 73.3
COMP RW12 89 .13.277.76.772.7 3.358.74.076.2 2.060.2 3.788.1 1.7 76.2 6.8 74.9
STANDARD 24 91.9 0.677.78.269.58.560.61.674.73.358.27.085.84.4 69.117.7 73.5
PROMPT S24 81 .913.275.15.764.94.857.31.869.51.749.85.165.228.9 74.29.4 67.2
CALIB +24 90 .72.178.6 6.273.14.359.5 3.275.91.958.42.882.08.4 75.29.1 74.2
COMP RW24 91 .01.878.26.474.2 3.158.54.177.1 1.862.0 3.788.8 1.4 76.1 7.2 75.7Mistral-Instruct-7BSTANDARD 3,4 90 .12.981.32.170.97.258.54.280.51.756.15.083.05.7 79.81.4 75.0
STANDARD 12 91 .40.981.22.267.98.757.72.879.11.657.23.685.43.6 77.75.6 74.7
PROMPT S12 90 .32.581.11.968.75.857.12.779.11.656.73.284.93.0 79.03.0 74.6
CALIB +12 91.5 1.6 81.3 1.875.8 2.658.36.681.01.361.94.785.44.0 79.6 1.6 76.9
COMP RW12 89 .92.780.72.775.12.960.0 4.981.1 1.364.7 4.6 87.6 2.1 79.21.2 77.3
STANDARD 24 91 .21.080.82.365.38.457.44.075.61.756.66.585.84.3 68.816.9 72.7
PROMPT S24 90 .52.6 81.3 2.068.95.657.12.179.11.757.43.186.02.1 78.73.3 74.9
CALIB +24 91.6 1.580.92.076.12.459.55.481.20.962.74.385.93.7 80.1 1.2 77.2
COMP RW24 90 .81.880.62.176.4 1.760.7 4.481.6 1.065.3 3.4 88.0 1.8 79.01.6 77.8Llama-3-8BSTANDARD 3,4 91 .41.779.27.274.08.058.74.776.52.259.43.784.06.6 87.45.5 76.3
STANDARD 12 92.2 0.6 79.6 6.973.15.063.3 2.477.52.262.73.887.72.0 82.118.1 77.3
CALIB +12 91 .11.579.25.877.9 3.360.59.377.32.165.23.286.44.7 87.7 4.0 78.2
COMP RW12 90 .72.078.36.777.22.961.86.478.0 1.866.9 2.4 89.1 1.0 87.43.8 78.7
STANDARD 24 92.2 0.878.27.278.02.063.8 1.876.23.066.42.387.91.9 80.618.9 77.9
CALIB +24 91 .71.4 80.0 6.178.33.463.8 2.978.11.766.02.486.74.9 87.7 3.8 79.0
COMP RW24 91 .61.779.16.978.8 2.763.73.378.5 1.467.4 2.6 89.5 1.1 87.43.2 79.5
Table 4: ICL accuracy of 8 classification tasks and the average accuracy ( Avg.). The number after a method denotes
the number of labeled data used. We run 15 prompts for each task (5 disjoint sets of Klabeled data and 3 templates)
and report the mean accuracy and standard deviation. C OMP RW achieves the best average accuracy in all setups.
COMP RW performs significantly better C ALIB +.
6 When Do Good Components Emerge?
We study the dynamics of components during pre-
training by monitoring their accuracies on 32 check-
points of Pythia-6.9B, uniformly spaced from the
first to the last checkpoint. For each checkpoint,
we run 4-shot ICL on AGNews with 3 templates ×
3 sets of demonstrations. The demonstrations are
balanced in labels with randomly shuffled orders.
Figure 3 shows the average accuracy of the 9 runs
shaded by the standard deviation.
While the full model (green) fluctuates and has
a large variance across prompts, the top-1 com-
ponents (solid blue) achieve good accuracy at anearly step and plateau quickly. We also backtrack
the top-1 components of different prompts at the
last checkpoint (dashed blue), monitoring how they
perform on average during pretraining. We observe
that they are not the top components at the early
stage (there are gaps between the two blue lines
before the 75ksteps), but start to perform steadily
well from the middle stage. Our findings also hold
on SST2 and Pythia-1.4B (see Figure 6 in the ap-
pendix), suggesting that the model’s ability to do
a task emerges before it is apparent from the full
model on these tasks.4
4On the other hand, Pythia models perform poorly on the
other tasks over all checkpoints; thus, the training dynamics
of the model components on challenging tasks remain unclear.0
16
512
5k
10k
15k
20k
25k
30k
35k
40k
45k
50k
55k
60k
65k
70k
75k
80k
85k
90k
95k
100k
105k
110k
115k
120k
125k
130k
135k
140k
143k
# Pre-training Steps0.00.20.40.60.8Accuracy[AGNews] Pythia-6.9B
T1
Last-T1
Full
B1Figure 3: The ICL accuracy of the full model (green)
fluctuates greatly during pretraining. However, good-
performing components (T1) emerge in the early steps.
7 Related Work and Discussion
Improving ICL. Prior work shows that ICL per-
formance varies greatly across different choices of
demonstrations and templates (Zhao et al., 2021;
Lu et al., 2022). Specifically, Sclar et al. (2024)
and V oronov et al. (2024) find no universally bet-
ter prompt template that can transfer across tasks
and models, implying that it is not easy to ex-
plain ICL through prompt engineering. While
several approaches, such as prompt selection (Liu
et al., 2022b; Chang and Jia, 2023; Fu et al., 2023),
prompt ensemble (Min et al., 2022a; Arora et al.,
2023; V oronov et al., 2024), and many-shot ICL
(Agarwal et al., 2024), substantially improve ac-
curacy, they treat LLMs like black boxes without
understanding the internals. Besides, they greatly
increase inference time or require a large set of
labeled data, which deviates from true few-shot
learning (Perez et al., 2021). In comparison, our
paper studies this problem by looking inside the
LLMs. Rather than selecting prompts, we select
components in a soft, learnable way. Our method
only requires {12,24}examples and has negligible
computation overhead over 4-shot ICL at inference.
Components Interpretation. Components inter-
pretation studies the function of different compo-
nents in a trained model (Elhage et al., 2021; Shah
et al., 2024), where components could be neurons
(Radford et al., 2017; Wang et al., 2022a; Gurnee
et al., 2023), attention heads (Olsson et al., 2022),
and MLPs (Geva et al., 2021). To analyze the com-
ponents, probing (Alain and Bengio, 2017), knock-
out (Geva et al., 2023; Chang et al., 2024; Li et al.,
2023), patching (Wang et al., 2023a; Goldowsky-
Dill et al., 2023), and early decoding (nostalge-braist, 2020; Geva et al., 2022) are widely used
techniques. For example, Li et al. (2024) train a
linear probe on every attention head to discover the
truthful heads inside LLMs. Michel et al. (2019)
and V oita et al. (2019) prune away a large percent-
age of attention heads and show that only a few are
critical to the performance. Hendel et al. (2023),
Liu et al. (2023), Merullo et al. (2024a), and Todd
et al. (2024) view ICL as compressing demonstra-
tions into function vectors, where they remove the
demonstrations and modify ( patch ) the LLM acti-
vations at certain layers with the function vectors at
test time. Early decoding interprets the investigated
components in the textual space by projecting them
through the output embedding matrix (Geva et al.,
2022). Our model decomposition is based on early
decoding and we share some similarities with prior
work (Yu et al., 2023; Wang et al., 2023c), espe-
cially in discovering individual components that
perform well on a task. Our contributions lie in
providing a new view of ICL by decomposition,
which reveals the transferability of components
across diverse ICL settings.
Our Method vs. Pruning. Our method caches
the direct contributions of components to the out-
puts through the residual stream, i.e., logits =P
jgj. Thus, removing gj, the direct contribu-
tion of the component j, does not alter the con-
tributions of the other components. In compari-
son, pruning a component changes the activations
of the other components in later layers. In A.7,
we show that pruning the good-performing com-
ponents identified by our method greatly hurts the
accuracy, meaning that pruning also defines these
components as important (Michel et al., 2019).
8 Conclusion
We introduce a new perspective of ICL via decom-
posing the model output into the sum of individ-
ual contributions of components. We then identify
three types of component characteristics across 3
LLMs and 8 classification tasks. Our extensive
analyses reveal consistency in component accuracy
across prompts and suggest the promising direction
of improving ICL by selecting components. To this
end, we propose component reweighting, which
learns to scale components differently on few-shot
examples. Our method achieves the best average
accuracy compared to prior methods. We hope this
work can deepen our grasp of LLMs while motivat-
ing more methods for practical use.9 Limitations
Our component reweighting method requires a
small set of labeled data Dtrainto train the com-
ponent weights w. However, we believe it is not
unreasonable to have at least K= 12 labeled exam-
ples in total and we compare with baselines using
the same Kexamples. On the other hand, we do
not compare with fine-tuning-based baselines, such
as LM-BFF (Gao et al., 2021), T-few (Liu et al.,
2022a), and LoRA (Hu et al., 2021), because they
usually require a larger GPU memory for training
and more sophisticated early stopping criteria to
prevent overfitting on few-shot examples. Another
limitation is that we only experiment with classifi-
cation tasks for ease of evaluation. We leave it for
future work to generalize our method to generation
tasks by doing decomposition and reweighting at
every token during generation.
Despite similarities in model decomposition, the
focus of this paper is not circuits in LLMs (Wang
et al., 2023a). Thus, we only have limited exper-
iments towards mechanistic understanding of the
curious components in §3.3 and A.7. Unlike prior
work that uses synthetic tasks to testify whether a
head attends to certain tokens (Dutta et al., 2024;
Merullo et al., 2024b), we work on standard NLP
benchmark datasets without obviously correct or
incorrect tokens to collect answers, making mecha-
nistic interpretation more challenging.
Acknowledgements
We thank Johnny Wei for his valuable suggestions
on the paper structure. We thank Qinyuan Ye, Ryan
Wang, Gustavo Lucas Carvalho, Ameya Godbole,
Wang Zhu, Daniel Firebanks-Quevedo, and the
anonymous reviewers for their helpful feedback.
This work was funded in part by gifts from Open
Philanthropy and Cisco Research, and was also
supported in part by the National Science Founda-
tion under Grant No. IIS-2403436. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd
Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Ab-
bas, Azade Nova, John D Co-Reyes, Eric Chu, et al.
2024. Many-shot in-context learning. arXiv preprint
arXiv:2404.11018 .Guillaume Alain and Yoshua Bengio. 2017. Under-
standing intermediate layers using linear classifier
probes.
Simran Arora, Avanika Narayan, Mayee F Chen, Lau-
rel Orr, Neel Guha, Kush Bhatia, Ines Chami, and
Christopher Re. 2023. Ask me anything: A sim-
ple strategy for prompting language models. In The
Eleventh International Conference on Learning Rep-
resentations .
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert
Webson, Colin Raffel, Nihal V . Nayak, Abheesht
Sharma, Taewoon Kim, M Saiful Bari, Thibault
Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,
Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-
jan Chhablani, Han Wang, Jason Fries, Maged Al-
shaibani, Shanya Sharma, Urmish Thakker, Khalid
Almubarak, Xiangru Tang, Dragomir Radev, Mike
Tian-jian Jiang, and Alexander Rush. 2022. Prompt-
Source: An integrated development environment and
repository for natural language prompts. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics: System Demonstra-
tions , pages 93–104, Dublin, Ireland. Association for
Computational Linguistics.
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,
Matthew R Gormley, and Graham Neubig. 2024. In-
context learning with long-context models: An in-
depth exploration. arXiv preprint arXiv:2405.00200 .
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions .
Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. 2023.
Pythia: A suite for analyzing large language mod-
els across training and scaling. In International
Conference on Machine Learning , pages 2397–2430.
PMLR.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Ting-Yun Chang and Robin Jia. 2023. Data curation
alone can stabilize in-context learning. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8123–8144, Toronto, Canada. Association for
Computational Linguistics.
Ting-Yun Chang, Jesse Thomason, and Robin Jia. 2024.
Do localization methods actually localize memorized
data in LLMs? a tale of two benchmarks. In Proceed-
ings of the 2024 Conference of the North AmericanChapter of the Association for Computational Lin-
guistics: Human Language Technologies (Volume
1: Long Papers) , pages 3190–3211, Mexico City,
Mexico. Association for Computational Linguistics.
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKe-
own, and He He. 2023. On the relation between
sensitivity and accuracy in in-context learning. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 155–167, Singapore.
Association for Computational Linguistics.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 2924–2936, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question
answering? try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1 .
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783 .
Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti,
and Tanmoy Chakraborty. 2024. How to think step-
by-step: A mechanistic understanding of chain-of-
thought reasoning. arXiv preprint arXiv:2402.18312 .
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al.
2021. A mathematical framework for transformer
circuits. Transformer Circuits Thread , 1:1.
Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut.
2023. Mitigating label biases for in-context learning.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 14014–14031, Toronto, Canada.
Association for Computational Linguistics.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and
Tushar Khot. 2023. Complexity-based prompting for
multi-step reasoning. In The Eleventh International
Conference on Learning Representations .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating models’ local decision boundaries
via contrast sets. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1307–1323, Online. Association for Computational
Linguistics.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 12216–12235,
Singapore. Association for Computational Linguis-
tics.
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-
berg. 2022. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary
space. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Process-
ing, pages 30–45, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 5484–5495, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Nicholas Goldowsky-Dill, Chris MacLeod, Lucas
Sato, and Aryaman Arora. 2023. Localizing
model behavior with path patching. arXiv preprint
arXiv:2304.05969 .
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.
2023. Finding neurons in a haystack: Case stud-
ies with sparse probing. Transactions on Machine
Learning Research .
Roee Hendel, Mor Geva, and Amir Globerson. 2023.
In-context learning creates task vectors. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 9318–9333, Singapore.
Association for Computational Linguistics.
John Hewitt and Percy Liang. 2019. Designing and in-
terpreting probes with control tasks. In Proceedings
of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2733–2743, Hong Kong,
China. Association for Computational Linguistics.
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi,
and Luke Zettlemoyer. 2021. Surface form com-
petition: Why the highest probability answer isn’talways right. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7038–7051, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. 2021. Lora: Low-rank adaptation of large lan-
guage models. In International Conference on Learn-
ing Representations .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter
Pfister, and Martin Wattenberg. 2024. Inference-
time intervention: Eliciting truthful answers from
a language model. Advances in Neural Information
Processing Systems , 36.
Maximilian Li, Xander Davies, and Max Nadeau. 2023.
Circuit breaking: Removing model behaviors with
targeted ablation. arXiv preprint arXiv:2309.05973 .
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-
fel. 2022a. Few-shot parameter-efficient fine-tuning
is better and cheaper than in-context learning. Ad-
vances in Neural Information Processing Systems ,
35:1950–1965.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022b. What
makes good in-context examples for GPT-3? In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures ,
pages 100–114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.
Sheng Liu, Lei Xing, and James Zou. 2023. In-context
vectors: Making in context learning more effective
and controllable through latent space steering. arXiv
preprint arXiv:2311.06668 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8086–8098, Dublin, Ireland. Association for Compu-
tational Linguistics.
Callum McDougall, Arthur Conmy, Cody Rushing,
Thomas McGrath, and Neel Nanda. 2023. Copy
suppression: Comprehensively understanding an at-
tention head. arXiv preprint arXiv:2310.04625 .
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.
2024a. Language models implement simple
Word2Vec-style vector arithmetic. In Proceedingsof the 2024 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long
Papers) , pages 5030–5047, Mexico City, Mexico. As-
sociation for Computational Linguistics.
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.
2024b. Talking heads: Understanding inter-layer
communication in transformer language models.
arXiv preprint arXiv:2406.09519 .
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? In Ad-
vances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc.
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2022a. Noisy channel language
model prompting for few-shot text classification. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 5316–5330, Dublin, Ireland. As-
sociation for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022b. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
nostalgebraist. 2020. interpreting GPT: the logit lens.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.
In-context learning and induction heads. arXiv
preprint arXiv:2209.11895 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems , 35:27730–27744.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.
True few-shot learning with language models. In
Advances in Neural Information Processing Systems .
Mohammad Taher Pilehvar and Jose Camacho-Collados.
2019. WiC: the word-in-context dataset for evalu-
ating context-sensitive meaning representations. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers) , pages 1267–1273,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. ArXiv preprint , abs/1704.01444.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kem-
bhavi, Ali Farhadi, and Mohammad Rastegari. 2020.
What’s hidden in a randomly weighted neural net-
work? In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages
11893–11902.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Alexey Romanov and Chaitanya Shivade. 2018.
Lessons from natural language inference in the clini-
cal domain. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1586–1596, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2655–2671, Seattle, United States.
Association for Computational Linguistics.
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane
Suhr. 2024. Quantifying language models’ sensitiv-
ity to spurious features in prompt design or: How i
learned to start worrying about prompt formatting.
InThe Twelfth International Conference on Learning
Representations .
Harshay Shah, Andrew Ilyas, and Aleksander Madry.
2024. Decomposing and editing predictions by
modeling model computation. arXiv preprint
arXiv:2404.11534 .
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron
Mueller, Byron C Wallace, and David Bau. 2024.
Function vectors in large language models. In The
Twelfth International Conference on Learning Repre-
sentations .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30.
Elena V oita, David Talbot, Fedor Moiseev, Rico Sen-
nrich, and Ivan Titov. 2019. Analyzing multi-head
self-attention: Specialized heads do the heavy lift-
ing, the rest can be pruned. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 5797–5808, Florence, Italy.
Association for Computational Linguistics.
Anton V oronov, Lena Wolf, and Max Ryabinin. 2024.
Mind your format: Towards consistent evaluation of
in-context learning improvements. In Findings of the
Association for Computational Linguistics ACL 2024 ,
pages 6287–6310, Bangkok, Thailand and virtual
meeting. Association for Computational Linguistics.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, and Jacob Steinhardt. 2023a. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations .
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,
Fandong Meng, Jie Zhou, and Xu Sun. 2023b. Label
words are anchors: An information flow perspective
for understanding in-context learning. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 9840–9855,
Singapore. Association for Computational Linguis-
tics.
Tony Wang, Miles Kai, Kaivalya Hariharan, and Nir
Shavit. 2023c. Forbidden facts: An investigation of
competing objectives in llama 2. In Socially Respon-
sible Language Modelling Research .
Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,
Zhiyuan Liu, and Juanzi Li. 2022a. Finding skill
neurons in pre-trained transformer-based languagemodels. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 11132–11152, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
Anjana Arunkumar, David Stap, Eshaan Pathak,
Giannis Karamanolakis, Haizhi Lai, Ishan Puro-
hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,
Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,
Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,
Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,
Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
Shen. 2022b. Super-NaturalInstructions: General-
ization via declarative instructions on 1600+ NLP
tasks. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 5085–5109, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Qinan Yu, Jack Merullo, and Ellie Pavlick. 2023. Char-
acterizing mechanisms for factual recall in language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 9924–9959, Singapore. Association for Com-
putational Linguistics.
Biao Zhang and Rico Sennrich. 2019. Root mean square
layer normalization. Advances in Neural Information
Processing Systems , 32.
Kelly Zhang and Samuel Bowman. 2018. Language
modeling teaches you more than translation does:
Lessons learned through auxiliary syntactic task anal-
ysis. In Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP , pages 359–361, Brussels, Bel-
gium. Association for Computational Linguistics.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems , volume 28. Curran Associates, Inc.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
International conference on machine learning , pages
12697–12706. PMLR.
Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu,
Jilin Chen, Katherine Heller, and Subhrajit Roy.2023. Batch calibration: Rethinking calibration for
in-context learning and prompt engineering. arXiv
preprint arXiv:2309.17249 .A Appendix
A.1 Tasks and Templates
Table 11 summarizes the 13 datasets we use in the
paper, where we construct balanced test sets by
randomly sampling 2000 examples in each task.
We form the prompts by concatenating demonstra-
tions in a randomly shuffled order. To avoid the
recency bias (Zhao et al., 2021), we keep shuf-
fling the demonstrations until the last two have
different labels. For minimally conservative tem-
plates (§4.1), Table 10 compares the contrast sets
we construct on Llama-2-7B. For our case study on
Task069 and Task070, we sample 3 templates from
Sclar et al. (2024). Figure 7 compare the prompts
of Task069 and Task070, which consist of an in-
struction followed by Ktemplated demonstrations.
Originally, the two tasks have ∼4%of parallel
examples. To make our task transfer challenging,
we discard these overlapped examples.
A.2 Label-Biased Components
We say a component is label-biased when it al-
ways predicts a certain label on the entire test set
§3.2. In this section, we focus on the most bi-
ased components in binary classification tasks, i.e.,
the two components that have the largest value of
(logit0−logit1)and(logit1−logit0), respectively,
where logit0∈Randlogit1∈Rare the LLM
output logits on the two classes. We name these
two components as Biased Component-0 and Bi-
ased Component-1, respectively. To understand
how biased these two components are, we alter
the choices of demonstrations and observe their
behavior. Specifically, we consider three settings:
demonstrations balanced in labels (green in Figure
5), demonstrations of all negative labels ( [0,0,0,0];
red), and demonstrations of all positive labels
([1,1,1,1]; blue). We fix the template and sample
5 disjoint sets of demonstrations for each setting.
Each dot in Figure 5 shows the components’ pre-
diction on an example, and the x-axis and y-axis
correspond to logit 0and logit 1, respectively. A dot
below the dashed diagonal line means the predic-
tion on the example is class 0. We find that both
Biased Component-0 and Biased Component-1 still
insist on predicting a certain label on all examples,
regardless of the labels in the prompts.
A.3 LayerNorms
Figure 4 shows the transformer architecture in
GPT2-like models. Because the layernorms inside
Figure 4: Transformer architecture in GPT2.
Biased 
Component-1 
Biased 
Component-0 [RTE] Llama-2-7B 
Figure 5: Each dot represents an example in the test
set. The two most biased components still insist on
predicting the same label on the entire test set regardless
of the labels of the demonstrations.
each block are before MHA and MLP, known as
Pre-LN, Eq. 1 has already taken Ln1andLn2into
account, and Eq. 6 only has the term for the final
layernorm, LN (·).
Both Llama-2 and Mistral model families use
RMSNorm (Zhang and Sennrich, 2019), a layer
normalization variant without centering and adding
bias terms. Formally, let x∈Rdbe the input, theSST2 BoolQ QQP WiC RTE MNLI AGNews ARC-Easy
Llama-2-7B 37.8 18 .9 43 .4 44 .2 35 .4 28 .2 13 .4 11 .5
Llama-2-13B 32.6 18 .7 37 .2 39 .3 32 .1 26 .0 12 .4 13 .1
Mistral-Instruct-7B 31.9 14 .0 32 .3 32 .4 27 .6 20 .9 14 .5 8 .4
Llama-3-8B 38.3 21 .4 42 .4 40 .0 36 .9 28 .1 15 .8 15 .0
Table 5: We report the average percentage of label-biased components across 15 prompts for each task. A
label-biased component always predicts the same label on the entire test set.
root mean square norm LN (x)is:
LN(x) =x
RMS (x)⊙γ, (10)
RMS (x) =vuut1
ddX
i=1x2
i, (11)
where γ∈Rdis the affine transform parameters
and⊙denotes element-wise multiplication.
A.4 Tests of Significance
We run one-tailed paired t-tests to test whether
COMP RW outperforms CALIB + significantly. In
Table 4, we have the results of 15 prompts for
each task and 8 tasks in total. For each model,
we aggregate the 120 accuracy scores of COMPRW
andCALIB +, respectively, and then calculate the
p-values. Table 6 shows that p-values <0.05in
8/8 setups, suggesting that COMPRWperforms sig-
nificantly better than C ALIB +.
Llama2-7B Llama2-13B Mistral-Ins-7B Llama3-8B
K= 12 0.0010 0.0002 0.0470 0.0198
K= 24 0.0003 0.0001 0.0027 0.0245
Table 6: The p-values <0.05in all 8 setups (4 LLMs,
withK={12,24}labeled examples), showing that
COMP RW performs significantly better than C ALIB +.
A.5 Do Good-Performing Components Exist
in Randomly Initialized Models?
Ramanujan et al. (2020) find that untrained sub-
networks can perform on par with a ResNet-34
trained on ImageNet. Similarly, Zhang and Bow-
man (2018); Hewitt and Liang (2019) show that
representations of randomly initialized language
models yield a strong baseline for probing tasks.
In this section, we investigate (1) whether good-
performing components still exist in a randomly
initialized LLM, and (2) how COMP RW method
performs using component activations extracted
from the randomly initialized LLM.SST2 ARC-EasyTrainedFULL 75.818.1 57.514.4
ORACLE -T1 91.70.9 54.510.1
COMP RW 90.81.8 79.01.6RandomFULL 49.70.7 25.00.5
ORACLE -T1 55.20.5 26.70.2
COMP RW 51.41.7 25.00.7
Table 7: Comparing the ICL accuracy between pre-
trained ( Up) and randomly initialized ( Down ) Llama-2-
7B. The top-1 component ( ORACLE -T1) and COMPRW
perform near random on the untrained model.
We run 4-shot ICL with 15 prompts and report
the average accuracy and standard deviation. For
COMP RW, we use the same 4 demonstrations and
20 more examples for reweighting. Table 7 shows
that the best-performing component ( ORACLE -T1)
in a randomly initialized Llama-2-7B still performs
poorly on SST2 and ARC-Easy. While C OMP RW
has substantial improvement over FULL on the pre-
trained model, it has no effect on the randomly ini-
tialized model. We conclude that good-performing
components do not exist in a randomly initialized
LLM and our COMP RW method relies on the pre-
trained component activations to perform well.
A.6 More Results on Transferability
In §4, we study the transferability of components
across different choices of demonstrations and tem-
plates. Here, Table 9 shows the full results on all
LLMs and tasks. We observe the same findings as
Table 2: component accuracies agree well across
randomly sampled demonstrations, but have much
weaker agreements across randomly sampled tem-
plates. Because constructing minimally-contrastive
templates requires non-trivial manual efforts, we
only build contrast sets for 5 tasks on Llama-2-
7B (shown in Table 2), where these tasks have the
largest variances across templates.A.7 Pruning Good and Bad Components
Our method studies a component using its cached
direct contribution to the output, whereas Michel
et al. (2019) ( pruning ) zeroes out the activations
of a component in the forward pass and thus indi-
rectly changes the activations of other components
in the upper layers. They consider a component
important if pruning it causes large drops in task
performance. In this section, we investigate the
intersection between our method and pruning.
First, we apply our decomposition to identify
good and bad-performing components based on
their ICL accuracy (3-shot for MNLI, 4-shot for
other tasks). Second, we run ICL with pruning
on Llama-2-7B, using the same 15 prompts in our
main experiments for every task. We prune the top-
50 components5and the bottom-50 components,
respectively. Table 8 compares the results with the
full model without pruning. We find that pruning
the top components (T50) greatly hurts the accu-
racy. On the contrary, pruning the bottom compo-
nents (B50) only decreases the average accuracy
on SST2 and RTE by 3.5%, and even slightly im-
proves the ones on MNLI and AGNews. These
findings may imply that our method and pruning
interpret components in similar fashion.
SST2 RTE MNLI AGNews
Full Model 75.8 18.168.9 3.234.41.770.019.9
Prune-T50 53.48.857.86.434.33.226.82.5
Prune-B50 72.314.865.45.835.7 4.072.6 15.7
Table 8: Comparing the accuracies of the full Llama-2-
7B model and pruning the top/bottom 50 components.
We run 15 prompts for each task and report the average
accuracy and standard deviation. We color the numbers
red when there is a large drop in accuracy.
A.8 Training Details and Hyperparameters
For both COMPRWandCALIB + methods, we train
a linear layer on Dtrainwith stochastic gradient de-
scent. Because we do not have an additional dev
set to tune the hyperparameters, we use the same
hyperparameters on all the tasks and models and
do early stopping based on the loss and accuracy
onDtrain. Specifically, we set learning rate = 0.05
for both methods and λ= 0.1for the L1 regu-
larization term in COMP RW. We run all our ICL
experiments on a single RTX A6000 GPU (48G).
Both the component reweighting and calibration
5∼5%of the total componentstraining processes can be run on a single i7 CPU
within a minute.
A.9 Models
We use the model checkpoints on Hugging Face ,
meta-llama/Llama-2-7b-hf ,Llama-2-13b-hf ,
mistralai/Mistral-7B-Instruct-v0.1 , and
meta-llama/Meta-Llama-3-8B .SST2 BoolQ QQP WiC RTE MNLI AGNews ARC
Correlation Llama-2-7B
(1) Demo 0.81 0.84 0.60 0.65 0.75 0.65 0.89 0.88
(2) Temp 0.40 0.16 0.03 0.15 0.19 0.09 0.68 0.44
IoU
(1) Demo 0.36 0.74 0.27 0.21 0.53 0.24 0.63 0.70
(2) Temp 0.12 0.01 0.01 0.03 0.05 0.01 0.20 0.20
Correlation Llama-2-13B
(1) Demo 0.83 0.84 0.63 0.67 0.78 0.73 0.91 0.91
(2) Temp 0.57 0.30 0.09 0.19 0.28 0.16 0.76 0.55
IoU
(1) Demo 0.26 0.71 0.31 0.18 0.46 0.39 0.55 0.65
(2) Temp 0.21 0.11 0.07 0.01 0.21 0.07 0.25 0.30
Correlation Mistral-Instruct-7B
(1) Demo 0.88 0.91 0.72 0.75 0.87 0.82 0.92 0.97
(2) Temp 0.58 0.44 0.19 0.26 0.40 0.30 0.77 0.60
IoU
(1) Demo 0.39 0.59 0.27 0.29 0.50 0.45 0.68 0.80
(2) Temp 0.10 0.17 0.06 0.05 0.17 0.09 0.29 0.22
Correlation Llama-3-8B
(1) Demo 0.85 0.88 0.70 0.73 0.80 0.81 0.89 0.95
(2) Temp 0.55 0.39 0.26 0.25 0.31 0.23 0.67 0.52
IoU
(1) Demo 0.42 0.56 0.28 0.25 0.46 0.52 0.65 0.68
(2) Temp 0.15 0.12 0.09 0.07 0.08 0.05 0.34 0.27
Table 9: Full results of the average correlation and IoU between (1) two random sets of demonstrations and (2) two
randomly sampled templates.
Task Templates Labels Accuracy
SST-2 T1Review: {text}\nDo you think the review is positive or negative? {label} negative/positive 50.6±0.7
T2Review: {text}{space}\nDo you think the review is positive or negative? {label} negative/positive 72.7±6.1
BoolQ T1Based on the following passage, {question}? {passage}\nAnswer: {label} No/Yes 52.5±2.0
T2Based on the following passage, {question}? {passage} Answer: {label} No/Yes 66.7±2.1
QQP T1Are the questions "{sent1}" and "{sent2}" asking the same thing? {label} no/yes 54.3±1.1
T2Are the questions "{sent1}" and "{sent2}" asking the same thing? {label} No/Yes 68.7±4.1
AGNews T1{text}\nIs this a piece of news regarding World, Sports, Business, or Technology? {label} World/Sports/Business/Technology 43.9±8.7
T2{text} Is this a piece of news regarding World, Sports, Business, or Technology? {label} World/Sports/Business/Technology 88.5±0.8
Table 10: We construct minimally contrastive templates that only differ slightly (colored in red) but yield large
differences in 4-shot ICL accuracy on Llama-2-7B. We report the average accuracy and standard deviation across 5
ICL runs with different demonstrations under the same template.Dataset Task # Classes
SST-2 (Socher et al., 2013) Sentiment Analysis 2
Yelp-polarity (Zhang et al., 2015) Sentiment Analysis 2
BoolQ (Clark et al., 2019) Yes/No QA 2
BoolQ Contrast Set (Gardner et al., 2020) Yes/No QA 2
QQP (Wang et al., 2018) Paraphrase Identification 2
WiC (Pilehvar and Camacho-Collados, 2019) Word Sense Disambiguation 2
RTE (Wang et al., 2018) Natural Language Inference 2
MNLI (Williams et al., 2018) Natural Language Inference 3
MedNLI (Romanov and Shivade, 2018) NLI in Medical Domain 3
AGNews (Zhang et al., 2015) Topic Classification 4
ARC-Easy (Clark et al., 2018) Multiple-Choice QA 4
Task069 (Mishra et al., 2022; Wang et al., 2022b) Abductive NLI 2
Task070 (Mishra et al., 2022; Wang et al., 2022b) Abductive NLI 2
Table 11: Summary of all the datasets.
Figure 6: 4-shot ICL accuracy on different pretraining checkpoints. We compare the full model (green) with the
top-1 (solid blue) and bottom-1 (red) components. The dashed blue line tracks how the top-1 components of the last
checkpoint (Last-T1) perform across time.In this task, you will be shown a short story with a beginning, two potential middles, and an ending. 
Your job is to choose the middle statement that makes the story coherent / plausible by writing  "1" or 
"2" in the output. If both sentences are plausible, pick the one that  makes most sense .
Beginning: The clown was blowing several bubbles to the kids. Middle 1: Isaiah kept on popping the 
bubbles. Middle 2: Isaiah kept eating the bubbles. Ending: He said that Isaiah is currently sick from 
ingesting too much soap. Answer: 2 
K demonstrations …Task069 
In this task, you will be shown a short story with a beginning, two potential middles, and an ending. 
Your job is to choose the middle statement that makes the story incoherent / implausible by indicating  
"1" or "2" in the output. If both sentences are plausible, pick the one that makes less sense .
Beginning: Killy was 9 months pregnant and almost ready to pop. Middle 1: Luckily Killy's water 
broke when she was in hospital. Middle 2: Killy's water broke when she was on a walk. Ending: Five 
minutes later, she delivered her baby with the help of passersby. Answer: 1 
K demonstrations …Task070 Figure 7: Comparing the prompts of Task069 and Task070. We apply the templates of Sclar et al. (2024) and
prepend the task instructions before Kdemonstrations. We ensure that the two tasks do not have parallel examples
to make the transfer experiment (§4.3) challenging.
Figure 8: Each dot represents a component (attention head: blue; MLP: orange) under 4-shot ICL on Mistral-
Instruct-7B. The x-axis shows how often a component predicts label 1 across the test data of a binary task.