Evaluating Large Language Models along Dimensions of Language
Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization
Niyati Bafna, Kenton Murray, and David Yarowsky
Johns Hopkins University, Center for Language and Speech Processing
{nbafna1,kenton,yarowsky}@jhu.edu
Abstract
While large language models exhibit certain
cross-lingual generalization capabilities, they
suffer from performance degradation (PD) on
unseen closely-related languages (CRLs) and
dialects relative to their high-resource language
neighbour (HRLN). However, we currently
lack a fundamental understanding of what
kinds of linguistic distances contribute to PD,
and to what extent. Furthermore, studies of
cross-lingual generalization are confounded by
unknown quantities of CRL language traces in
the training data, and by the frequent lack of
availability of evaluation data in lower-resource
related languages and dialects. To address these
issues, we model phonological, morphologi-
cal, and lexical distance as Bayesian noise pro-
cesses to synthesize artificial languages that are
controllably distant from the HRLN. We anal-
yse PD as a function of underlying noise param-
eters, offering insights on model robustness to
isolated and composed linguistic phenomena,
and the impact of task and HRL characteris-
tics on PD. We calculate parameter posteriors
on real CRL-HRLN pair data and show that
they follow computed trends of artificial lan-
guages, demonstrating the viability of our nois-
ers. Our framework offers a cheap solution to
estimating task performance on an unseen CRL
given HRLN performance using its posteriors,
as well as for diagnosing observed PD on a
CRL in terms of its linguistic distances from its
HRLN, and opens doors to principled methods
of mitigating performance degradation.1
1 Introduction
Advances in the capabilities of large language mod-
els (LLMs) have resulted in a paradigm shift in nat-
ural language processing, with LLMs being used
for and evaluated over a variety of classification
and generation tasks (Xue et al., 2021; Bang et al.,
1https://github.com/niyatibafna/
llm-eval-crosslingual-generalization
Figure 1: Phonological/orthographic, morphologi-
cal, and function and content word variation, and
lexical choice difference, between hin and mai;p∗:
bloomz7b1 MT output.
2023a; Hendy et al., 2023); however, even mul-
tilingual models such as bloomz7b1 ,mT0(Muen-
nighoff et al., 2023) and Aya (Üstün et al., 2024)
extend model capabilities only 100of the world’s
highest-resourced languages. The vast majority
of the world’s 3800 written languages have dras-
tically less data available (Joshi et al., 2020), al-
though many have a related high-resource neigh-
bour (Asai et al., 2023). This underscores the need
for cross-lingual generalization in LLM capabili-
ties from high-resource languages on which they
have been trained to related low-resource languages
(LRLs), variants, and dialects, i.e. a theoretical lan-
guage continuum centered at the HRL.
Previous literature has reported evidence of mul-
tilingual and cross-lingual zero-shot capabilities in
LLMs for a number of tasks, also finding, unsur-
prisingly, that model performance suffers in such
settings (Jiao et al., 2023; Cahyawijaya et al., 2024)
(see Figure 1). While it’s reasonable that the far-
ther a closely-related language (CRL) is to its high-
resource language neighbour (HRLN), the greater
the performance degradation (PD) in a zero-shot
setting, we lack a principled understanding of how
much different dimensions of linguistic distance
(phonological, morphological, and lexical) affect
PD. Given that we can find a systematic relation-arXiv:2406.13718v1  [cs.CL]  19 Jun 2024ship between each such dimension and PD, and
compute the associated distance between a CRL-
HRLN pair, this insight would allow us to (a) di-
agnose observed PD on a CRL, (b) estimate PD
for a CRL without task data, as well as (c) suggest
targeted interventions aimed at mitigation of PD.
In this work, we model phonological/ortho-
graphic, morphological, and lexical distance as
cross-linguistic “noise”, generated by Bayesian pro-
cesses applied on a source language, thus positing
a parametrization of the HRL dialect continuum.
We generate artificial languages with varying ex-
tents of each noise type, and study LLM zero-shot
cross-lingual generalization for three NLU-focused
tasks, discussing the effects of task, noise type, and
language family on PD. Crucially, our noise gen-
eration processes have tractable posteriors cheaply
computable from bilingual lexicons/bitext, allow-
ing us to place real CRLs within the parametrized
dialect space of a HRL. We show that PD on real
CRLs given their posteriors follows expected trends
observed over artificial languages, demonstrating
that our noise processes capture useful informa-
tion about the factors of linguistic distance as they
contribute to PD.
Our use of artificial languages allows us to sys-
tematically populate the dialect space of an HRL;
further, the noise generation process produces task
datasets for each hypothetical language. This
solves three problems: firstly, we often do not have
task data for real closely-related languages that are
unseen in our LLM; secondly, we may not have
enough CRLs per HRL, especially CRLs of vary-
ing distance along each dimension of interest, to
be able to establish and study systematic trends for
that language family. Further, we are not guaran-
teed that a given CRL or its task data is entirely
unseen from the training data, confounding a study
of LLM zero-shot generalization. Our main contri-
butions are as follows:
•We study the dimensions of linguistic distance
that make an input closely-related language diffi-
cult relative to its high-resource language neigh-
bour for an LLM in zero-shot settings, quantita-
tively and qualitatively describing model robust-
ness to each dimension, and discuss the relevance
of the task under consideration and the typology
and resource-level of the language.
•We introduce a parametrization of the dialect
space of a language along three linguistic axesthat allows for the generation of artificial lan-
guages given a set of parameters, as well as for
cheaply computing the parameters of a real lan-
guage pair. We demonstrate its utility for predict-
ing and analysing LLM PD on unseen languages
using real CRL-HRLN pairs. Our framework
also opens pathways to mitigating PD on low-
resource languages, e.g., by reducing damaging
distances using linguistic or other tools.
2 Modelling linguistic variation
We model phonological/orthographic, morpho-
logical, and lexical ( content and function word)
variation as parametrized probabilistic “noisers”
applied to a source language to generate related
languages. We denote a noiser as ϕn
v, parametrized
θn=v, where n∈ {p, m, c, f }indicates the noise
type. For every language, task, and ϕn, we are
interested in the function ψn
∗:θn→PD, where
PD=(sθ−srand)−(b−srand)
b−srand(1)
Here, sθis the performance on the noised source,
bis the score on the clean source, and srand is
the random baseline.2This notation extends to
composite noisers, e.g. ψm,c
0.5,∗computes PD as a
function of θc, given θm= 0.5. See examples of
the outputs of our noisers in Table 1 and § D.2.
2.1 Noiser details
ϕp: Phonological/Orthographic This model
mimics sound change in closely related languages,
and is based on the following ideas from theo-
ries of sound change (Joseph et al., 2003): (i)
Sound change is applied to a phoneme given
some phonological left and right context e.g. (d
|a_,_EOW) →t). (ii) Sound change, given context,
is regular: it applies consistently in all words of
the language. (iii) Consonant sound change largely
occurs between phonologically similar phonemes
(e.g. difference in voicing: f→v). This is not rele-
vant for vowels, which change fluidly.
We use manually constructed character →IPA
maps to obtain a set of potential underlying
phonemes for script characters. For any given oc-
currence of a character, we make a random guess
for its corresponding phoneme if there are several.3
20forX→eng,33.33forXNLI ,50forXSC; i.e. if XNLI
score drops to 33.33% , we say that it shows 100% PD.
3Since our goal is to inject random noise into the input
roughly guided by the underlying phonology of the text, we
can tolerate the imprecision introduced by this process.We model phonological context as the left and right
character of the source character (including word
boundaries); thus, a (phoneme, context) pair
is simply a character 3-gram. Each (phoneme,
context) is affect with probability θp. In order to
find a phonologically plausible target set for each
IPA character, we construct a list of IPA character
sets covering all phonemes used by the languages
in this study, such that the phonemes in each set dif-
fer from each other in roughly one (or at most two)
phonological features, and a phoneme can plausi-
bly change via sound shift to another phoneme in
any of the sets it belongs to. (See Appendix B.) Our
list is inspired by Index Diachronica. We can now
find a plausible replacement for a given character
by mapping it into IPA, sampling a replacement
IPA character, and mapping the IPA back into the
relevant script. The change to a character given
context applies globally throughout the text.
ϕm: Morphological Our noiser models concate-
native suffixation4guided by the following intuitive
premises. (i) Affixal change is global (ii) The re-
placement suffix must be plausible for the language
family in terms of its phonology and script, and the
original suffix, e.g. if one of them starts with a
vowel, the other one is also likely to have an initial
vowel. We approximate a set of linguistic affixes
by collecting the k5most common string suffixes
of content words in the language corpus. Each
collected suffix is noised with probability θm, by
passing it through the phonological noiser as de-
scribed above, with a high dial ( θp= 0.5); this
ensures the plausibility of the noised target suffix.
Finally, we construct a vocabulary map by swap-
ping out all occurrences of an affected source suffix
with its generated target in all source words; the vo-
cabulary map applies globally for every occurrence
of the word in the text.
ϕf,c: Lexical We model function word change6
and non-cognate content word change separately,
4Note that we do not directly model differences such
as changes in case systems, number of genders, inflec-
tional/derivational paradigm differences, but assume that all
of these underlying processes manifest on the surface level as
affix variations, which can therefore be considered a proxy for
morphological variation.
5empirically chosen per language, e.g. k= 150 forhi.
6We collect a list of function words in each language using
POS tags from the Universal Dependencies corpus (Nivre
et al., 2016); any word not in this list is treated as a content
word. Note that since functional words are relatively few and
highly frequent, collecting them even over small corpora will
yield almost perfect coverage for a given language.guided by the following premises: (i) The replace-
ment non-cognate equivalent for a content word
must be plausible in the relevant script, may not
resemble the original word at all, and must not be
a word in the source vocabulary.7(ii) Its length
may loosely depend on the length of the original
word (for example, words with rare semantics may
be longer in both dialects). (ii) Function words
in related languages are probably distant cognates,
very similar in length.
For content words, we sample the length of the
replacement word from a Poisson( λ=l)where lis
the length of the source word, and use a character
3-gram model trained on the language corpus to
generate plausible non-words of the required length.
For function words, we generate a replacement by
applying a high degree of phonological noise to the
functional word ( θp= 0.5). All replacements for
content and function words are global.
We study lexical change as a combination of ϕc
andϕf. Since content word change is the more
dynamic of the two, likely to show variation de-
pending on language distance, whereas function
word change is likely to be high even for related
dialects, and show less variation for differently
distant languages, we primarily study the PD dy-
namics of ϕf,c
θf,∗. We experiment with varying θc,
given θf∈ {0,0.5,0.8}(ϕf,c
θf,∗), and with varying
θfgiven θc= 0(ϕf,c
∗,0).
Composite We compose noisers by indepen-
dently applying phonological, morphological, and
lexical noise in this order (allowing “overwrites”).
While this is a simplification, it is well-motivated;
lexical noise is often the most dynamic and con-
tinuous of the three while phonological and affixal
change are much more gradual and/or fixed given
a time period.
2.2 Posterior computation
We now demonstrate the utility of our noisers and
associated ψnin understanding PD on real linguis-
tic variation. We assume that CRLs are “generated”
by applying a composition of noisers on the source
language. Now, if we can find the underlying θn,
we can estimate PD=ψn
∗(θn=v), and therefore
task performance.
Given a bilingual lexicon in the source and target,
we use word alignments to estimate the Bernoulli
7We consider only complete lexical change and not lexical
choice differences: i.e., when languages have different usage
patterns or show semantic shift for the same words.parameter θ∈{θp,θm,θc,θf}. In our noisers,
all changes to the concerned units (trigrams, suf-
fixes, words) are global. In reality we may not
observe a global change between source and target
unit; language change may be noisy, we may have
one-off phenomena, and we may have noisy word
alignments. We compute θin the following way:
E[θ] =P
uIu
T, EP
uIu
T
=X
uE[Iu]
T
where Iuis a binary random variable indicating
whether unit uwas affected, and Tis the total
number of units. We can now estimate E[Iu] =Cu
Tufor each ui.e. the fraction of times that uwas
affected. Note that it remains to be decided how we
will categorize a given change in a non-identical
source-target pair.
Phonological If source-target normalized edit
distance (NED) is high,8we attribute changes in
the target word to phonological change. We find
the minimal list of edits from source to target; if we
observe a character change with the same left-right
context, we count it towards θp.
Morphological If a content target word has a
different suffix (identified as in § 2.1) but the same
stem but (i.e. it is not lexical change) , we count it
towards θm.
Lexical We count any change in a function word
towards θf. For content words, if the source-target
NED is low (i.e. not phonological/morphological
change) and the target word is not present in the
source vocabulary, we count it towards θc.
Note that these posteriors can be computed inde-
pendently of each other; although lexical change
may “overwrite” a suffix change, it does not change
the fraction of suffixes/trigrams affected since the
noisers are independent of each other.9
3 Experimental Setup
Model and Tasks We obtain initial zero-shot
results on a number of tasks for bloomz7b1 and
mt0XXL (Muennighoff et al., 2023), and select three
8We use language-specific empirically determined thresh-
olds for NED-based decisions, e.g. 0.5fordein this case
9We compute θmonly over words that have the same stem
in source and target; any word pair with different stems is ig-
nored. Since lexical noise is applied uniformly over words and
independently of morphological noise, we expect that while
it will “disqualify” a set of word pairs for the θmposterior
computation, the remaining set will give us the same estimate
(in expectation) of θm.tasks to work with: X→engmachine translation on
FloRes200 (Team et al., 2022),10XStoryCloze
(XSC; Lin et al., 2021b), and XNLI (Conneau et al.,
2018), as covering a large enough mutual set
of languages as well as two tasks paradigms of
interest, namely, multiple-choice questions and
sequence-to-sequence.11We study robustness on
bloomz7b1 , using the mlmm-eval evaluation frame-
work (Dac Lai et al., 2023). See § A and § C.1 for
all evaluated tasks and experimental details.
Languages We work with Hindi, Indonesian,
Arabic, German, French, Spanish, and English.
This set of languages was curated with typolog-
ical diversity, language presence in bloomz7b1 ,12
and availability of task datasets in mind. Further,
we include three macrolanguages ( hi,id,ar) with
dozens of real closely related low-resource lan-
guages and dialects. In order to validate our com-
puted trends with real language data, we require re-
lated languages over a variety of language distances
from the source, unseen from bloomz7b1 , with
task dataset availability; we work with Awadhi-
awa, Bhojpuri- bho, Magahi- mag, Maithili- mai, and
Chhattisgarhi- hne(Hindi), Danish- dan, Icelandic-
isl, and Swedish- swe (German), Malay- zsm-
(Indonesian), Occitan- oci(French), and Galician-
glg(Spanish), for X→eng. We obtain bilingual lex-
icons from Google Translate when available, and
alternatively use statistical word alignment with
FastAlign (Dyer et al., 2013) on FloRes bitext.13
4 Results and Discussion
Seeψnfor noiser, task, and language combinations
in Figure 2 (single run per noiser parametrization).
Tasks We find that the rate of mean PD given
a noise type is the same across tasks .This indi-
cates that model performance for one task for a
CRL relative to its HRLN can be used to extrap-
olate its performance on other tasks; i.e. PD is
largely a function of language distance.
10We loosely refer to X→engas an NLU task; since the
LLM is fluent in English, its performance primarily depends
on comprehension of the input.
11We found that the performance of both models on multi-
lingual ARC, HellaSwag and MMLU (Dac Lai et al., 2023) is
close to or worse than chance for many languages; this makes
these tasks unsuitable for studying model PD.
12German is low-resource for bloomz7b1 , constituing only
0.21% of the training corpus (Muennighoff et al., 2023).
13We manually filter 300entries for maiandhneand verify
that the computed posteriors over possibly noisy alignments
are similar to those computed on clean lexicons (see § F.1).NoiserStrategiesExample I/Oφf∗(a) Infers sentence meaning from content words(b) Partially correct(c) Incorrectly connects content words*(d) Breaks: Function word was part of a construction(e) Hallucination†(f) No translation/off-target†s: Pasanganinidapatmemilihuntukmembuat rencana adopsibagibayimereka.s’: Pasanganenitawatmemilihantukmembuat rencana adopsivigebayimarequ.p: The couple may choose to make an adoption plan for their baby.p’: The couple decided to adopt a baby.Ref: These couples may choose to make an adoption plan for their baby.φf,cθf,∗(a) Guesses correct word from context*(b) Keeps the original word, code-switched, if sur-rounding context is clear.(c) Keeps the word, garbles sentence(d) Breaks: wrong guess.(e) Ignores the word and translates the rests:DerSatellit wurdevoneiner RaketeinsWeltallbefördert.s’:TyhSatellit wurdeväneiner RaketewangeWeltallveraumoden.p: The satellite was sent into space by a rocket.p’: The satellite was sent into orbit by a rocket.Ref: The satellite was sent into space by a rocket.φp∗(a) Guesses word meaning from context and spellingclues*(b) Makes a wrong guess.(c) Breaks: function word changes.(d) Breaks: many changes in proximity.s:Cualquierpersonaqueestéprogramandounviajea un paísquepodríatildarsecomo zona de guerra deberíarecibirunentre-namientoprofesional.s’:Cualqeyerpersonacueestéprogramendounviajoa un paíscuepodríatyldursecomo zona de guerra deberíareciborunyntre-namientoprofesional.p: Any person planning a trip to a country that could be considered awar zone should receive professional training.p’: Any person planning a trip to a country that could be considered awar zone should receive professional training.Ref: Anyone planning a visit to a country that could be considered awar zone should get professional training.φm∗(a) Model faces no issues(b) Breaks: too much corruption*s:यहाँ सूयΔदयदेखनेकी क ु छ जगहों पर ई̌र की पूरी रातजागनेकी परंपरा है।s’:यहाँ सूयΔदयदेखनइकी क ु छ जगहों पर ई̌र की पूरा राटजागनइकी परंपरा है।p: There are some places where the Easter night is celebrated by stayingup all night.p’: In some places, Easter is celebrated with a full moon.Ref: There’s a tradition to pass the Easter night awake at some exposedpoint to see the sunrise.Table 1: Output type classification for each noise type. * marks the case that the example belongs to.†:applicable to all noisers, only listed once. Example languages from top to bottom:id, de, es, hi.
2Table 1: Output type classification for each noise type. * marks the case that the example belongs to. †: applicable
to all noisers, only listed once. Example languages from top to bottom: id, de, es, hi .
While we see clearly linear trends for mean PD
for all tasks and noise types, and individual lan-
guages trends are also linear for X→eng, this is
less true for individual language trends for XSCand
XNLI (e.g. 3b, 3c, 4b, for arb,hin ). This is a result
of sampling variance in our language generation
process: ϕn
vmay produce a range of artificial lan-
guages varying in the specific set of units that are
noised. The relationship between PD and θnis me-
diated by task sensitivity to the comprehensions of
specific words (phones/morphs) as opposed to gen-
eral comprehension of the input: we compute std.
deviation of PD for multiple artificial languages
generated from the same θnforhiandar, and find
much lower SD for X→engthan the other tasks.
Using PD means over multiple artificial languages
perθnremoves the apparent instability of the trend
at the individual language level and is key to com-
puting reliable trends for a language. See § D.3 for
std. dev. numbers and stabilized trends for hi, ar .
These findings back the intuition that while trans-
lation depends on local understanding of input, suf-
fering predictably with increasing noise, the model
relies only on certain words rather than the en-
tire sentence for classification tasks, and is there-
fore more sensitive to whether those are corrupted
rather than the general extent of noise, although ofcourse these two are correlated.14This suggests
that X→engis a more robust test of NLU in a
LRL for a model, and less susceptible to fluke
performances.
Languages We see that languages with rich mor-
phology such as arand id(Lopo and Tanone,
2024) suffer most from ϕm(e.g. 6a), and that de
particularly suffers from ϕc(e.g. 4a), possibly be-
cause word compounding results in a higher extent
of lost information per noised word. This confirms
the intuition that noising in a rich dimension of a
language’s typology is likely to hurt more . See
Figure 3 for mean PD over all parametrizations of
a given noiser per language for X→eng. In gen-
eral, we also find that lower-resource languages in
bloomz7b1 such as de,ar,id, and hihave higher
mean PD as compared to HRLs like frandes;
more exposure to a language makes the model
more adept at unseen related languages.
Noise types The slope of ψnsignals how dam-
aging noise type nis (higher is worse). We con-
textualize these trends over θusing the posteriors
computed over real language pairs, which provide a
sense of the natural range of θfor related languages
per noiser. Note that absolute PD values for a given
14XNLI is highly sensitive to whether its three label words
are noised. This strongly cautions any zero-shot evaluation to
be mindful of its treatment of label words.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
f
0102030405060708090Performance Degradation (%)awabhohne
magmai
zsm
glgocidanisl
swe(1a) X->eng , Lexical (Function | c=0) : f,c
*,0
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
f
0102030405060708090100Performance Degradation (%)(1b) XNLI , Lexical (Function | c=0) : f,c
*,0
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
f
0102030405060708090100Performance Degradation (%)(1c) XStoryCloze , Lexical (Function | c=0) : f,c
*,0
es
hi
id
ar
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(2a) X->eng , Lexical (Content | f=0) : f,c
0,*
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(2b) XNLI , Lexical (Content | f=0) : f,c
0,*
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(2c) XStoryCloze , Lexical (Content | f=0) : f,c
0,*
es
hi
id
ar
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
102030405060708090100Performance Degradation (%)awahne
mag
zsm(3a) X->eng , Lexical (Content | f=0.5) : f,c
0.5,*
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(3b) XNLI , Lexical (Content | f=0.5) : f,c
0.5,*
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(3c) XStoryCloze , Lexical (Content | f=0.5) : f,c
0.5,*
es
hi
id
ar
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)bhomai
glgocidanisl
swe(4a) X->eng , Lexical (Content | f=0.8) : f,c
0.8,*
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(4b) XNLI , Lexical (Content | f=0.8) : f,c
0.8,*
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
c
0102030405060708090100Performance Degradation (%)(4c) XStoryCloze , Lexical (Content | f=0.8) : f,c
0.8,*
es
hi
id
ar
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p
0102030405060708090100Performance Degradation (%)awabhohne
magmai
zsm
glgocidanisl
swe(5a) X->eng , Phonological : p
*
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p
0102030405060708090100Performance Degradation (%)(5b) XNLI , Phonological : p
*
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p
0102030405060708090100Performance Degradation (%)(5c) XStoryCloze , Phonological : p
*
es
hi
id
ar
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
m
0102030405060708090Performance Degradation (%)awabhohne
magmai
zsm
glgocidanisl
swe(6a) X->eng , Morphological : m
*
es
hi
id
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
m
0102030405060708090100Performance Degradation (%)(6b) XNLI , Morphological : m
*
es
hi
de
ar
fr
en
mean
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
m
0102030405060708090100Performance Degradation (%)(6c) XStoryCloze , Morphological : m
*
es
hi
id
ar
en
meanFigure 2: PD% for each language, task, ϕn; mean language PD trends explicitly shown. We show (θn, PD %)for
real CRL-HRLN pairs using computed posteriors for X→eng. See § 3 for corresponding HRLNs per CRL.lex morph phon0204060Mean PD %es
hi
id
de
ar
fr
enFigure 3: Mean PD over all parametrizations per noiser
forX→eng
θn, and therefore absolute slopes, are not compara-
ble across noise types, since θndiffers in meaning
depending on the noiser; however, these can be
compared directly for different lexical noisers.
We find that ϕf,c
∗,0shows lower PD rate as com-
pared to ϕf,c
0,∗: naturally, content loss is more dam-
aging than function word loss . However, note
that real θfvalues are high even for very closely
related language pairs (e.g. hne-hin ; see 1a), and
correspond to significant PD values. On the other
hand, θcmay be low ( <0.2) for closely related
languages, but is more costly. Note that ψf,c
θf,∗for
θf∈ {0,0.5,0.8}have similar slopes but increas-
ingy-intercepts based on θf. Given that function
words form a closed and relatively small set for
a given language, and may be easier to deal with
than open class, possibly rare, content words, this
suggests that we can cheaply tackle a non-trivial
portion of PD by simply handling “easier” func-
tion word correspondences .
We observe that ψm
∗displays a low slope; cor-
rupting 100% of our set of linguistic suffixes results
in a mean 50−70% PD.This indicates that the
model largely capable of capturing important
information from word stems. Note that for dis-
tant related cousins like de-dan ,θmcan be high
and correspond to significant PD.
Finally, ψp
∗indicates sharp PD; this is nat-
ural since ϕpaffects chargrams with possibly
widespread effect in the corpus. Once again, while
our chosen LRLs cover a range of natural values
forθp, even very closely-related languages display
θpvalues corresponding to significant PD (5a), sug-
gesting that the model is vulnerable to natural
levels of phonological/orthographic variation .
Error Modes See Table 1 for a qualitative clas-
sification of model error modes for each noiser,
obtained via a manual examination of outputs over
representative θn. See examples in Appendix D.
While PD over a dataset varies smoothly as a func-tion of θn, model performance on individual out-
puts is much more unpredictable. Success/failure
modes are not easily predictable from the input:
the model displays both surprising fragility as
well as robustness in different cases.
PD over noise composition While overall PD
for a language with composed noisers is a presum-
ably a function of PD for each contained noise
type, the nature of this function remains to be un-
derstood. We study ϕf,c,m
0.5,∗,0.5, composing lexical
and morphological noise (see Figure 4 for X→eng
andXSC) and observe that for X→eng, the resulting
PD is well-explained simply by ϕf,c
0.5,∗; indicating
that overall PD may be a simple max (as opposed
to incremental) in this case.15This idea offers one
explanation of the observed PD of isl, i.e. that
the PD effect is dominated by ϕf,c
0.8,∗. However, for
XSC, we observe that ψf,c,m
0.5,∗,0.5in fact exceeds the
theoretical additive noise trend. While we leave a
detailed study of this composition function to fu-
ture work, we show that it is task dependent; we
also believe that it is likely to be vary depending
on noiser combination.
0.2 0.4 0.6 0.8
c
020406080100
m
0.5
f,c
0.5,*
f,c
0.5,*+m
0.5
f,c,m
0.5,*,0.5
0.2 0.4 0.6 0.8
c
020406080100
m
0.5
f,c
0.5,*
f,c
0.5,*+m
0.5
f,c,m
0.5,*,0.5
Figure 4: Composing ϕf,candϕm: studying ψf,c,m
given θmfor Hindi for X→eng(top) and XSC(bottom).
ψf,c+ψmshows the theoretical additive trend.
Posteriors We calculate posteriors for real CRLs
as described in § 2.2 and plot (θ, PD )points for
X→eng.16We bucket the θfposterior and show
(θc, PD )on the relevant ψf,c
θf,∗plot (Figure 2). Note
that we can use posteriors for a CRL-HRLN pair to
generate artificial languages that are equally distant
from the HRLN as the LRL; we provide examples
in § F.2 to illustrate the plausibility of our noisers
15XNLI follows this trend; see Appendix E.
16See § F.1 for BLEU, PD, and posteriors for each θn.and associated posteriors. We observe that PD
vs.θnfor real languages generally follow similar
trends as ψn, indicating that our constructed
ϕnoffer useful parametrizations of linguistic
distance as it contributes to PD.17
Note that since real languages contain a com-
position of all noise types, we expect total PD
to be higher than that predicted by any individ-
ualψn. However, this is not true, notably ob-
served for ψc
∗andψf
∗(3a, 4a). This is attributable
to code-switching and traces of the unseen lan-
guage in training data. For artificial languages, the
cost of a completely unknown word is high (as
compared to a partially known, suffix-corrupted
word); however, it’s likely that the model actually
knows some percentage of words identified as un-
known by our posterior computation in the real
unseen languages. The unknown word may be
present in another language than the HRLN (e.g.
fr-oci changement-cambiar ;cambiar is a Span-
ish equivalent), or it may be non-identical but very
close to an HRLN synonym ( certain-qualques
- French synonym quelques ), or it may simply
be known because the model has seen data in the
“unseen” language. This would have the effect of
reducing the absolute PD while maintaining the
trend. The observed delta between the trends
gives us an idea of the benefits of multilinguality
and language contamination in training data by
providing the counterfactual.
5 Related Work
Multilingual evaluation of LLMs Recent stud-
ies show that LLMs demonstrate certain multilin-
gual capabilities accompanied with performance
degradation for LRLs for machine translation (Jiao
et al., 2023; Hendy et al., 2023; Robinson et al.,
2023) as well as other tasks like POS, NER, and
summarization (Lai et al., 2023; Bang et al., 2023b;
Asai et al., 2023). Kantharuban et al. (2023) at-
tempt to identify economic, social, and linguistic
correlates of MT performance in LLMs for dialects;
they find positive correlations for dataset size and
lexical similarity among other factors. It is difficult
to draw principled insights from such studies about
what the bottlenecks for cross-lingual transfer are,
since the tested languages may simultaneously vary
17Notable outliers are ociandzsmforϕf,c
∗,θf. Further, glg
actually performs with +4BLEU over es(§ F.1), which is
a clear red flag. These anomalies could indicate unreported
amounts of the language in the training data or, in the case of
glg, possibly test set leakage.in their relatedness to high-resource languages, and
presence in the pretraining data.
Linguistic distance as a factor in performance
Recent work explores providing “missing” linguis-
tic knowledge of LRLs (lexical, morphosyntactic)
in LLMs by providing dictionaries, bitext, and
grammar books via in-context learning for LRLs
(Tanzer et al., 2024; Zhang et al., 2024b,a). Other
works look at cleverly choosing shots for the con-
text by exploring the prompt space, choosing ex-
emplars that are “close” to the output using lexical
distance (Zhu et al., 2023; Zhang et al., 2024a;
Cahyawijaya et al., 2024). However, this search
space of what can be provided is large, and we
lack an understanding of which linguistic distances
LLMs need “help” with: these ideas motivate a
study such as ours.
Robustness Earlier studies have looked at robust-
ness of machine translation systems to orthographic
variants, typos, and other kinds of noise (Belinkov
and Bisk, 2018; Heigold et al., 2018). Moradi and
Samwald (2021) perform a similar study of BERT-
like models for sentiment analysis, QA, and NER,
among other tasks, with the intent of stress-testing
LMs against natural user-generated noise such as
synonym replacement, common misspellings, and
verb tense errors. Wang et al. (2023) discuss the
robustness of ChatGPT against adversarial and out-
of-distribution input datasets such as ANLI and
DDXPlus. Havrilla and Iyer (2024) investigate
character-level static and dynamic noise for chain-
of-throught prompting processes. As far as we
know, ours is the first work to stress test LLMs
under noise models of linguistic distance.
6 Conclusion
We study the robustness of an LLM to 4types
of linguistically-motivated (phonological, morpho-
logical and lexical) Bayesian noise models on 7
languages and 3tasks, generating artificially lan-
guages controllably distant from a given HRL and
computing trends in performance degradation. This
allows us to quantitatively and qualitatively char-
acterize the impact of each isolated factor of lin-
guistic variation on task performance. Our noisers
are amenable to cheap posterior computation; we
show that PD for real unseen languages follow ex-
pected trends given their computed posteriors, vali-
dating our noiser construction. Our work offers a
framework for the principled linguistic analysis ofcross-lingual generalization and opens avenues in
mitigating LLM performance degradation in low-
resource settings.
Limitations
Noiser design We design phonological/ortho-
graphic, morphological, and lexical noisers with
the intent of simulating real linguistic distances
along these dimensions in a language-family neu-
tral manner, while maintaining posterior computa-
tion that is cheap in terms of required data and lin-
guistic tools; our noisers incorporate several simpli-
fications from a linguistic standpoint. Each noiser
can certainly be further nuanced to increase the
plausibility of the resulting synthesized languages;
some examples of possible detailing include (a) ϕp:
using language-family-specific sound change mod-
els that weight commonly observed sound changes
in that family higher than others (b) ϕm: using
morphological tools to more accurately identify lin-
guistic suffixes, (c) ϕm: modeling other kinds of
morphology, e.g. non-concatenative, templatic, pre-
fixal. This is particularly relevant to languages such
as Arabic. (d) ϕc: introducing weighting by (log)
frequency such that commoner words are more
likely to be affected by the noiser. Note that some
of these changes may introduce complications for
posterior computation. We leave it to future work
that is interested in particular noisers for particular
language families to look into fine-graining noiser
design in a given context.
Our work is also limited by the three linguistic
phenomena we study. Notably, we do not study syn-
tactic change, since it is not naturally modeled by
our framework of smoothly increasing distances in
a hypothetical continuum (i.e. possible differences
at the level of core syntax between languages are far
fewer, and often rare for closely-related languages).
There are certainly other noisers of interest to be
studied. One example is the phenomenon of seman-
tic shift, whereby words with the same form shift
in meaning in related languages, resulting in differ-
ent lexical choice for the languages (although not
lexical change); lexical usage patterns in general
may also be of interest. We give an example of this
in Figure 1.
Comprehensiveness Our insights on PD char-
acterization are limited to the 3 tasks and 7 lan-
guages we study, in a zero-context context for
bloomz7b1 . Each of these dimensions can natu-
rally be expanded: it is possible that the observedPD dynamics are different for different models (in-
dividual trends for a noiser will certainly differ
depending on model, language, and task), or for a
few-shot context. Further, we are also able to pro-
vide our results on real language posteriors only on
X→eng; we are constrained by task dataset avail-
ability for truly low-resource languages. We make
our code available and encourage a similar analy-
sis to ours for any new combination of language,
model, task, noiser, and experimental setting.
Noiser composition dynamics Our work focuses
mainly on PD dynamics for individual noise types
to isolate the effect of each linguistic phenomenon,
and touches only briefly on the PD dynamics for
composed noisers, although our noise processes
and posteriors offer natural extensions for noise
composition. While we demonstrate the complex-
ity of observed PD dynamics on a single language
and single noise composition setup for 3 tasks, we
leave a detailed investigation of the same, which
should include a large enough selection of noiser
combinations for different language typologies,
tasks, and parametrizations per noiser, to future
work.
Finally, related to the above: while we character-
ize error modes and provides examples for model
outputs on noised inputs for individual noise types,
these may be different for composed noisers, and
by consequence, for real languages.
Ethics Statement
Our work is motivated by the need to increase lan-
guage inclusivity in the large language model space,
as well as promote a scientific investigation of the
generalization capabilities of blackbox LLMs. Our
findings are applicable to a large range of languages
and dialect continua that that are low-resource by
the standards of the training data required by LLMs
for proficiency, but have a high-resource language
neighbour. This work contributes to the project
of extending the benefits enjoyed by high-resource
languages to its close language family and its native
speaker communities.
Acknowledgments
We would like to extend our gratitude to Kaiser Sun
and Vilém Zouhar for proof-reading this paper.References
Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu,
Terra Blevins, Hila Gonen, Machel Reid, Yulia
Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.
2023. Buffet: Benchmarking large language models
for few-shot cross-lingual transfer. arXiv preprint
arXiv:2305.14857 .
Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert
Webson, Colin Raffel, Nihal V . Nayak, Abheesht
Sharma, Taewoon Kim, M Saiful Bari, Thibault
Fevry, Zaid Alyafeai, Manan Dey, Andrea San-
tilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,
Gunjan Chhablani, Han Wang, Jason Alan Fries,
Maged S. Al-shaibani, Shanya Sharma, Urmish
Thakker, Khalid Almubarak, Xiangru Tang, Xian-
gru Tang, Mike Tian-Jian Jiang, and Alexander M.
Rush. 2022. Promptsource: An integrated develop-
ment environment and repository for natural language
prompts. Preprint , arXiv:2202.01279.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan
Xu, and Pascale Fung. 2023a. A Multitask, Multi-
lingual, Multimodal Evaluation of ChatGPT on Rea-
soning, Hallucination, and Interactivity. Preprint ,
arxiv:2302.04023.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-
task, multilingual, multimodal evaluation of chatgpt
on reasoning, hallucination, and interactivity. arXiv
preprint arXiv:2302.04023 .
Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and Natural Noise Both Break Neural Machine Trans-
lation. Preprint , arxiv:1711.02173.
Samuel Cahyawijaya, Holy Lovenia, and Pascale Fung.
2024. LLMs are few-shot in-context low-resource
language learners. arXiv preprint arXiv:2403.16512 .
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
ina Williams, Samuel R. Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. XNLI: Evaluating cross-
lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing . Association for Computa-
tional Linguistics.
Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo,
Thuat Nguyen, Franck Dernoncourt, Ryan A Rossi,
and Thien Huu Nguyen. 2023. Okapi: Instruction-
tuned large language models in multiple languages
with reinforcement learning from human feedback.
arXiv e-prints , pages arXiv–2307.
Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013.
A simple, fast, and effective reparameterization of
IBM model 2. In Proceedings of the 2013 conference
of the North American chapter of the association for
computational linguistics: human language technolo-
gies, pages 644–648.Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830.
Alex Havrilla and Maia Iyer. 2024. Understanding the
Effect of Noise in LLM Training Data with Algorith-
mic Chains of Thought. Preprint , arxiv:2402.04004.
Georg Heigold, Stalin Varanasi, Günter Neumann,
and Josef van Genabith. 2018. How Robust Are
Character-Based Word Embeddings in Tagging and
MT Against Wrod Scramlbing or Randdm Nouse?
InProceedings of the 13th Conference of the Associ-
ation for Machine Translation in the Americas (Vol-
ume 1: Research Track) , pages 68–80, Boston, MA.
Association for Machine Translation in the Americas.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How Good Are GPT Models at
Machine Translation? A Comprehensive Evaluation.
Preprint , arxiv:2302.09210.
Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing
Wang, and Zhaopeng Tu. 2023. Is ChatGPT A
Good Translator? Yes With GPT-4 As The Engine.
Preprint , arxiv:2301.08745.
Brian D Joseph, Richard D Janda, and Barbara S Vance.
2003. The handbook of historical linguistics . Wiley
Online Library.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6282–6293.
Anjali Kantharuban, Ivan Vuli ´c, and Anna Korhonen.
2023. Quantifying the Dialect Gap and its Corre-
lates Across Languages. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2023 ,
pages 7226–7245, Singapore. Association for Com-
putational Linguistics.
Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu
Man, Franck Dernoncourt, Trung Bui, and Thien
Nguyen. 2023. ChatGPT Beyond English: Towards a
Comprehensive Evaluation of Large Language Mod-
els in Multilingual Learning. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2023 , pages 13171–13189, Singapore. Association
for Computational Linguistics.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2021a.
Truthfulqa: Measuring how models mimic human
falsehoods. Preprint , arXiv:2109.07958.Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona T. Diab, Veselin
Stoyanov, and Xian Li. 2021b. Few-shot learn-
ing with multilingual language models. CoRR ,
abs/2112.10668.
Joanito Agili Lopo and Radius Tanone. 2024. Con-
structing and expanding low-resource and underrep-
resented parallel datasets for indonesian local lan-
guages. arXiv preprint arXiv:2404.01009 .
Milad Moradi and Matthias Samwald. 2021. Evaluating
the robustness of neural language models to input
perturbations. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 1558–1570, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey
Schoelkopf, et al. 2023. Crosslingual generalization
through multitask finetuning. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
15991–16111.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, et al. 2022. Crosslingual generaliza-
tion through multitask finetuning. arXiv preprint
arXiv:2211.01786 .
Joakim Nivre, Marie-Catherine De Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC’16) , pages
1659–1666.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska,
Qianchu Liu, Ivan Vuli ´c, and Anna Korhonen. 2020.
XCOPA: A multilingual dataset for causal common-
sense reasoning. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 2362–2376, Online. As-
sociation for Computational Linguistics.
Nathaniel Robinson, Perez Ogayo, David R. Mortensen,
and Graham Neubig. 2023. ChatGPT MT: Competi-
tive for High- (but Not Low-) Resource Languages.
InProceedings of the Eighth Conference on Machine
Translation , pages 392–418, Singapore. Association
for Computational Linguistics.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alter-natives: An evaluation of commonsense causal rea-
soning. In 2011 AAAI Spring Symposium Series .
Timo Schick and Hinrich Schütze. 2021. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339–2352.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV ,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with
automatically generated prompts. arXiv preprint
arXiv:2010.15980 .
Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Juraf-
sky, and Luke Melas-Kyriazi. 2024. A Benchmark
for Learning to Translate a New Language from One
Grammar Book. Preprint , arxiv:2309.16575.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Rop-
ers, Safiyyah Saleem, Holger Schwenk, and Jeff
Wang. 2022. No language left behind: Scal-
ing human-centered machine translation. Preprint ,
arXiv:2207.04672.
Alexey Tikhonov and Max Ryabinin. 2021. It’s all in the
heads: Using attention heads as a baseline for cross-
lingual transfer in commonsense reasoning. Preprint ,
arXiv:2106.12066.
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-
Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel
Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,
et al. 2024. Aya model: An instruction finetuned
open-access multilingual language model. arXiv
preprint arXiv:2402.07827 .
Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,
Runkai Zheng, Yidong Wang, Linyi Yang, Hao-
jun Huang, Wei Ye, Xiubo Geng, et al. 2023.
On the robustness of chatgpt: An adversarial
and out-of-distribution perspective. arXiv preprint
arXiv:2302.12095 .
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.Chen Zhang, Xiao Liu, Jiuheng Lin, and Yansong Feng.
2024a. Teaching Large Language Models an Unseen
Language on the Fly. Preprint , arxiv:2402.19167.
Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi
He, William Yang Wang, and Lei Li. 2024b.
Hire a Linguist!: Learning Endangered Languages
with In-Context Linguistic Descriptions. Preprint ,
arxiv:2402.18025.
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,
Shujian Huang, Lingpeng Kong, Jiajun Chen, and
Lei Li. 2023. Multilingual Machine Translation with
Large Language Models: Empirical Results and Anal-
ysis. Preprint , arxiv:2304.04675.A Baseline results for tasks
See baseline results for bloomz7b1 andmt0XXL for
the languages we considered in Table 2 and Table 3
respectively, for multilingual ARC, HellaSwag,
MMLU (Dac Lai et al., 2023), X→eng(Team et al.,
2022), XSC(Lin et al., 2021b), XNLI (Conneau et al.,
2018), XCopa (Roemmele et al., 2011; Ponti et al.,
2020), XWinoGrad (Tikhonov and Ryabinin, 2021;
Muennighoff et al., 2022), TruthfulQA (Lin et al.,
2021a). We see that bloomz7b1 is generally bet-
ter for XSCandXNLI and work with it for the rest
of our experiments. Russian and German are not
included in both models but have traces in the train-
ing data as described in Muennighoff et al. (2023);
we choose to include German in our experiments
as a low-resource language in bloomz7b1 .
B Details of Phonological Noiser
See Figure 5 for the list of IPA character sets that
we used in our phonological noiser described in § 2.
An IPA character to be noised can be transformed
with uniform probability to another IPA character
in any set that it belongs to.
C Further Experimental Details
C.1 Prompt Details and Variations
We tried various prompts for our chosen tasks, and
we note that the model performance is highly sensi-
tive to the prompt; this has been observed in several
previous studies (Shin et al., 2020; Gao et al., 2021;
Schick and Schütze, 2021). We choose a single
prompting framework per task with a reasonable
baseline performance in line with previous evalua-
tions of bloomz7b1 (Muennighoff et al., 2023). We
work in the zero-shot setting for our experiments.
This is in keeping with our goal to study zero-shot
generalization to unseen languages. While we note
some uniform gains from including a few shots
(5−10)in the high-resource language, we do not
study this dimension in our work.
We tried a few different prompting styles in-
spired by templates from Promptsource (Bach et al.,
2022) as well as the defaults in the MLMM evalu-
ation framework (Dac Lai et al., 2023) and noted
considerable variation between the worst and best
performing prompts (up to 15points for XNLI and
20points for XSC). Note that for XNLI andXSC, we
see large baseline performance gains when the op-
tions are mentioned in the prompt. For XNLI , we
also note that Prompt 3 (default) in fact requiresthe loglikelihood of the entire input sequence to
be compared with the corresponding labels replac-
ing[MASK] , whereas the other two setups simply
compare loglikehoods of the label options. See
Table 4.
We also note that for XNLI , model performance
is sensitive to the choice of word in the target
language for the entailment, neutral , and
contradiction labels. Interestingly, using “No”
for the Spanish contradiction label results in
bloomz7b1 loglikelihood always being highest for
contradiction, possibly because it is a shared token
with English, yielding near-random performance
onxnli_es (33%)
For the translation tasks, we use Prompt 2for the
baselines, but Prompt 1for the noised languages;
we note that this does better than Prompt 2for the
latter.
The above choices give rise to considerable vari-
ation in baseline performances; we work with a
single setup for our experiments.
Finally, we make the choice to use English in-
structions for our prompts, resulting in language-
mixed inputs. bloomz7b1 is instruction-tuned in
this setup, rather than on translated prompt instruc-
tions as in the case of mt0XXL-MT (Muennighoff
et al., 2023). We do not experiment with translated
prompts to eliminate the additional complexity in-
troduced by the quality of the translation.
C.2 Data details
Each evaluation is conducted over a subset of the
test set consisting of 300samples; this is for time
and compute efficiency since we conduct a large
number of evaluations over combinations of task,
language, noiser, and parametrization. Note that
all evaluations for a given language and task are
conducted over an identical subset.
All datasets used are publicly available for re-
search use under CC BY-NC 4.0 (mARC, mHel-
laSwag, mMMLU), CC BY-SA 4.0 (XNLI, XSto-
ryCloze, TruthfulQA, XCopa, FloRes200), or CC
BY (XWinograd).
C.3 Compute
We conduct a total of approximately 3∗6∗7∗7 =
882 evaluation experiments (excluding develop-
ment) on NVIDIA A100 machines, totalling about
220GPU hours.XStoryCloze XWinograd XCopa mARC mHellaswag mMMLU FloRes TruthfulQA XNLI
Hindi 63.67 - - 21.67 33.67 30 56.44 49.08 51
Russian 57.67 54.33 - 19.67 34.33 26 30.31 52.93 38.33
Arabic 66 - - 26.33 32 32.33 55.32 48.62 46
Spanish 72.33 - - 33 42.33 37.33 42.91 51.13 49.67
German - - - 21 26 32 41.25 51.22 47.33
Indonesian 69.33 - 60.33 28 36 37.67 60 54.39 -
English 77.33 83.67 - - - - 99.53 - 60.33
French - 73.49 - 34.33 33.67 32.33 57.34 46.8 54.67
Table 2: Performance of bloomz7b1 across different languages and tasks.
XStoryCloze XWinograd XCopa mARC mHellaswag mMMLU FloRes TruthfulQA XNLI
Hindi 57.3 - - 28.3 34.6 30 52.5 46.4 39
Russian 57.6 65.3 - 28.6 36 32.6 48.1 46.3 37.1
Arabic 56.1 - - 28.3 33.7 31.3 54.1 50.9 33.7
Spanish 59.3 - - 26.6 37.7 30 46.1 45.2 38.6
German - - - 25.7 36.3 22.7 54.1 44.6 35.6
Indonesian 58.3 - 62.5 28 39 30.7 57.5 43.6 -
English 58 70.3 - - - - 99.7 - 50.3
Table 3: Performance of mt0XXL across different languages and tasks.
Figure 5: List of IPA character sets for the phonological noiser.
D Results: Further details
D.1 Noising examples
See Table 5 for more examples of noiser output
for certain θ’s and languages. We also provide the
outputs for X→engon the clean and noised source
for comparison.D.2 Error type examples
We provide an expanded version of Table 1, with
an example for every mentioned error type for es.
We do not claim that is a comprehensive set of error
modes; it is intended rather to be illustrative.XNLI Prompt 1Suppose that the following is true:
premise
Can we infer that: hypothesis ?
Respond with one of the following words: ENTAILMENT_LABEL ,CONTRADICTION_LABEL ,
NEUTRAL_LABEL .
Prompt 2Suppose that the following is true:
premise
Can we infer that: hypothesis ? Yes, no, or maybe?
Respond in the target language.
Prompt 3∗premise, QUESTION_WORD ? [MASK], hypothesis
XStoryCloze Prompt 1What is a possible continuation for the following story ?
sentence_1
sentence_2
sentence_3
sentence_4
Choose from the following options:
option_1
option_2
Prompt 2sentence_1 sentence_2 sentence_3 sentence_4
What is a possible continuation for the story given the following options ?
-option_1
-option_2
Prompt 3Choose the best continuation of this story: sentence_1
sentence_2
sentence_3
sentence_4
X→eng Prompt 1 Translate from a dialect of <HRLN> into English
Prompt 2 Translate from <HRLN> into English
Prompt 3 Translate into English :
Table 4: Our attempted prompts.∗[MASK] is filled with each of the three possible labels, and the model choice is
computed using loglikelihood over the entire sequence.
0.0 0.1 0.2 0.3 0.4 0.5
c
020406080100Perf. Deg. (%)XNLI, Lexical (Content | f=0.8) : f,c
0.8,*
hi
ar
Figure 6: PD for XNLI for hiandar,ϕf,c
0.8,∗, averaging
over10runs for each parametrization; this results in a
much stabler trend for PD vs. θas compared to using a
single run as shown in Figure 2.D.3 Trend stability for individual languages
and tasks
In § 4, we discuss the effect of sampling variance
in PD for a given θ, that appears to differ by task
depending on task sensitivity to the specific words
that are corrupted as opposed to the general extent
of corruption in the input. We choose midrange val-
ues of θnforϕf,c,ϕm, andϕp(θf= 0.5,θc= 0.3,
θm= 0.5, andθp= 0.1), and generate 10artificial
languages with hiandaras sources. We report
standard deviation in PD for generated languages
for each task in Table 8 and Table 9 for hiandar
respectively. We see that std. deviation for X→eng
is convincingly lower than for the classification
tasks; this is in line with our intuition discussed in
§ 4. Note that this is std. deviation in percentage
PD and not actual scores: e.g., a std. deviation in
PD of 10% given a baseline XNLI score of 51(like
forhi) translates to a std. deviation of 1.8accuracy
points.18This is low enough for our established
18See § 2 for our calculation of PD.Noising examples for different languagesNoiserLangExamplesφp0.05ids: Saat berada di lokasi terpencil dan tanpa jangkauan seluler, telepon satelit mungkin menjadi satu-satunya pilihan Anda.s’: Saat berada di lokasi tirpencil dan tanpu jamgkauan seluler, telepon satelit mungkin menjadi satu-satunya pilohan Anda.p: When in remote locations without cell phone coverage, satellite phones may be your only option.p’: When you’re in the wilderness and without cell phone reception, a satellite phone may be your onlyoption.Ref: In remote locations, without cell phone coverage, a satellite phone may be your only option.φp0.1des: Sie haben normalerweise ein besonderes Angebot an Speisen, Getränken und Unterhaltung, um die Gästebei Laune zu halten und dafür zu sorgen, dass sie bleiben.s’: Sie haben nürnalerweise ein bejondehes Ancebot an Speisen, Getränkon und Unterhaltung, um die Gästebei Laune zu halten und dafür zu sorgen, dacs sie bleiben.p: You usually have a special offer for drinks, food and entertainment, to keep guests at Laune and tomake them stay.p’: You have a very nice apartment in Speisen, Getränkon and Unterhaltung, to keep the guests at Laune,and to make them stay.Ref: They usually have special food, drink and entertainment offers, to keep guests in a good mood, andkeep them at the premise.φm0.6frs: Le pays possède une grande variété de communautés végétales en raison de la diversité de ses microcli-mats, de ses sols et de ses niveaux d’altitude.s’: Le pays possèto une grande variédé de communaudéç végétèies en raicon de la diversüté de ses micro-climats, de ses sols et de ses niveüu d’altitude.p: The country has a great variety of plant communities due to the diversity of its microclimates, soils,and altitudes.p’: The country has a great variety of vegetation due to its microclimates, soils and altitude.Ref: It has a notably wide variety of plant communities, due to its range of microclimates, differing soilsand varying levels of altitude.φm0.6ess: La gran pirámide fue construida en honor al faraón Khufu, y muchas otras de este tipo, tumbas y templosmás pequeños se levantaron en honor a sus esposas y familiares.s’: La gram pirámide fue construica en honir al faraón Khufu, y muchas otras de este tipo, tumbuc ytemples más pequeños se levantarom en honir a sus esposuc y familiaros.p: The great pyramid was built in honor of Pharaoh Khufu, and many other such pyramids, tombs, andtemples were built in honor of his wives and family members.p’: The pyramid was built to honor the Pharaoh Khufu, and many other such pyramids, tombs, and templeswere built to honor his wives and family.Ref: The great pyramid was created to honor the Pharaoh Khufu, and many of the smaller pyramids,tombs, and temples were built to honor Khufu’s wives and family members.φl0.5,0.3his:हालाँिक हर देश '˹ ैं िडनेिवयाई' था, लेिकन डेनमाक र् , ̍ीडन, नॉवШ और आइसलैंड क े लोगों, राजाओं, रीित-िरवाजों और इितहास क े बीच कईअंतर थे.s’:हऔयईँिक अऋ देश '˹ ैं िडनेिवयाई' था, लेिकन डेनमाक र् , ̍ीडन, नॉवШ औृ आइसलैंड क े लोगों, बुक्षे, रीित-िरवाजों औृ इितहास क े बीश कईडरत ठौ.p: Although every country was ’Scandinavian’, there were many differences between the people, kings,customs and history of Denmark, Sweden, Norway and Iceland.p’: The country ’Scandinavian’ was, but the Danes, Swedes, Norwegians and Icelanders, the people, customsand history were very different.φl0.5,0.3ens: Foster care is supposed to provide all the necessities that were lacking in the home they were previouslytaken from.s’: Foster cyal es constaines du provide ayl the necessities did were lacking in the home dee were smen-strainges taken from.p: Foster care is supposed to provide all the necessities that were lacking in the home they were previouslytaken from.p’: Foster care is provided by the government to provide the necessities that were lacking in the home.Ref: Foster care is supposed to provide all the necessities that were lacking in the home they were previ-ously taken from.Table 1: Examples of noising for different noisers, and model outputs forX->engon clean and noisedsource sentences. s: Source, s’: Noised source, p: Prediction on source, p’: Prediction on noised source,Ref: reference translation.
2Table 5: Examples of noising for different noisers, and model outputs for X→engon clean and noised source
sentences. s: Source, s’: Noised source, p: Prediction on source, p’: Prediction on noised source, Ref: reference
translation.Examples for all error modes
Noiser Strategies Example I/O
ϕf
∗(a) Infers sentence meaning from content wordss: Al parecer, las cabras fueron domesticadas, por primera vez, hace unos 10 000 años, en los montes Zagros, en Irán.
s’: Al parecer, luc cabras fiaom domesticadas, por primera vez, hace enes 10 000 años, an los montes Zagros, an Irán.
p: Apparently, goats were first domesticated about 10,000 years ago in the Zagros Mountains in Iran.
p’: It seems that the first domesticated goats were bred in the Zagros Mountains of Iran about 10,000 years ago.
Ref: Goats seem to have been first domesticated roughly 10,000 years ago in the Zagros Mountains of Iran.
(b) Partially corrects: Los esfuerzos para hallar el lugar del accidente deben lidiar con el mal tiempo y el terreno escarpado.
s’: Los esfuerzos pea hallar al lugar del accidente cebyn lidiar kom al ah tiempo i al terreno escarpado.
p: The efforts to find the crash site must contend with bad weather and rugged terrain.
p’: The efforts were made to find the place of the accident, but the terrain was too rough.
Ref: Efforts to search for the crash site are being met by bad weather and harsh terrain.
(c) Incorrectly connects content words*s: Las manifestaciones, en ocasiones violentas, fueron provocadas por el hecho de que no se llevan adelante elecciones,
en algunos casos desde el año 2011.
s’: Luc manifestaciones, an ocasiones violentas, fiaom provocadas por al hecho de guu no ze llevan adelante elecciones,
an olgones casos ceztu al año 2011.
p: The protests, sometimes violent, were sparked by the fact that elections are not held in some cases since 2011.
p’: In 2011, there were violent protests, sometimes triggered by the failure to hold elections.
Ref: The sometimes-violent protests were triggered by failure to hold elections, some due since 2011.
(d) Breaks: Function word was part of a con-
structions: Sin perjuicio de cuán mansos puedan lucir, lo cierto es que los bisones, los uapatíes, los alces, los osos y prácticamente
todos los animales grandes pueden se agresivos.
s’: Sin perjuicio de ceám mansos piedan lucir, li cierto os guu los bisones, los uapatíes, los alces, los osos i prácticamente
dodus los animales grandes pieden ze agresivos.
p: No matter how docile they may look, bears, bison, moose, elk, bears, and nearly all large animals can be aggressive.
p’: Without prejudice to the fact that bison, moose, elk, bears, and nearly all large animals can be aggressive, it is true
that the bisons, moose, elk, bears, and nearly all large animals can be very docile.
Ref: No matter how docile they may look, bison, elk, moose, bears, and nearly all large animals can attack.
(e) Off-targets: Se han rescatado varios rehenes y, hasta ahora, se ha confirmado que al menos seis han muerto.
s’: Ze han rescatado parius rehenes i, hosta ahora, ze he confirmado guu al menos seis han muerto.
p: Several hostages have been rescued, and it is confirmed that at least six have died so far.
p’: Spanish phrase: Ze han rescatado parius rehenes i, hosta ahora, ze he confirmado guu al menos seis han muerto.
Ref: Several hostages have been rescued and least six have been confirmed dead so far.
ϕf,c
θf,∗(a) Guesses correct word from contexts: Todo en el Universo está hecho de materia, compuesta porpartículas pequeñas denominadas átomos.
s’: Todo en el Universo está hecho de materia, tespolaci porpiamplesc obleyón denominadas átomos.
p: Everything in the Universe is made of matter, composed of tiny particles called atoms.
p’: Everything in the Universe is made of matter, which is made of tiny particles called atoms.
Ref: Everything in the Universe is made of matter. All matter is made of tiny particles called atoms.
(b) Keeps the original word, code-switched, if
sur- rounding context is clears: Los rasgos que distinguen a una subcultura pueden ser lingüísticos, estéticos, sexuales, geográficos o estar relacionados
con la religión o la política, o una mezcla de factores.
s’: Los rasgos que distinguen a una calincio pueden ser teleamplinempal, estéticos, sexuales, esolaridalla o estar
relacionados con la religión o la política, o una mezcla de factores.
p: The characteristics that distinguish a subculture can be linguistic, aesthetic, sexual, geographical, religious, or political,
or a combination of factors.
p’: The characteristics that distinguish a calincio can be teleamplinempal, aesthetic, sexual, esolaridalla, or related to
religion or politics, or a mixture of factors.
Ref: The qualities that determine a subculture as distinct may be linguistic, aesthetic, religious, political, sexual,
geographical, or a combination of factors.
(c) Keeps the word, garbles sentences: El satélite en el espacio recibe la llamada y, luego, la refleja de vuelta casi de forma instantánea.
s’: El devasalv en el espacio recibe la llamada y, vircap, la refleja de vuelta apases de bharítu instantánea.
p: The satellite in space receives the call and then reflects it back almost instantly.
p’: The devasalv in space receives the call and, vircap, reflects it back to the instantaneous bharítu.
Ref: The satellite in space gets the call and then reflects it back down, almost instantly.
(d) Breaks: wrong guesss: Los entomólogos emplean el término insecto parásito en un sentido formal para referirse a este grupo de artrópodos.
s’: Los entomólogos ceradida el cataciónit insecto ingaren en un sintaut formal para referirse a este scomp de artrópodos.
p: The entomologists use the term insect parasite in a formal sense to refer to this group of arthropods.
p’: The entomologists use the term insectivore to refer to this group of arthropods.
Ref: The term bug is used by entomologists in a formal sense for this group of insects.
(e) Ignores the word and translates the rests: Hershey y Chase insertaron su propio ADN en una bacteria usando fagos, o virus.
s’: Hershey y Chase insertaron su propio Adn en una resabajectoma usando capandil, o virus.
p: Hershey and Chase inserted their own DNA into a bacterium using phages, or viruses.
p’: Hershey and Chase inserted their own Adn into a somatic cell using capandil, or virus.
Ref: Hershey and Chase used phages, or viruses, to implant their own DNA into a bacterium.
Table 6: Examples of each error mode for es. Continued below.
trend to be able to provide a good ballpark estimate
for the XNLI score for a language for which we
haveθ.
We also recompute ψf,c
0.8,∗forhiandarforXNLI
(4b in Figure 2) using means over 10runs per θc;
this combination of language, task, and noiser is
motivated by the fact that the associated individual
language trends appear most unstable computed
over single runs per parametrization. See Figure 6
for the trends; we observe much higher stability
in the individual language trend. These findings
indicate using means over several generated artifi-cial languages in order to compute reliable trends
for a single language, and using associated SD as a
confidence measure in the predicted PD.
E PD dynamics on composed noisers
As discussed in § 4, we are also interested in how
ψ{x,y,z}compose to give ψxyzfor two or more
noisers, i.e. the nature of the function of PD on
individual noisers that gives overall PD on com-
posed noisers. See Figure 7 for ψf,c,m
0.5,∗,0.5forXNLI .
We see a similar trend for XNLI as we saw in Fig-
ure 4 for X→eng, i.e. overall PD simply tracks theExamples for all error modes
Noiser Strategies Example I/O
ϕp
∗(a) Guesses word meaning from context and
spelling cluess: El informe es sumamente crítico con prácticamente cada aspecto de la política vigente del poder ejecutivo en Irak, y
apela a un cambio inmediato de dirección.
s’: Ey informe es sumamenty crítico con prácticamente cada aspecto de la política vigenty del pider eyetutivo ym Irak, e
apela a un camvuo inmediato de dirección.
p: The report is highly critical of almost every aspect of the present executive policy in Iraq, and urges an immediate
change of direction.
p’: The report is highly critical of almost every aspect of the present policy of the U.S. towards Iraq, and it calls for an
immediate change of direction.
Ref: The Report is highly critical of almost every aspect of the present policy of the Executive towards Iraq and it urges
an immediate change of direction.
(b) Makes a wrong guess.s: La investigación en el campo de la IA supone el desarrollo de máquinas a fin de automatizar tareas que requieren un
comportamiento inteligente.
s’: La investigación ym ul campo de la IA sopone ul desarrolyo de máquinas a fin de audymatizor caeas cue reqeyerem
un comportamiento inteligente.
p: Research in the field of AI involves the development of machines to automate tasks that require intelligent behavior.
p’: The research in the field of AI involves the development of machines to automate tasks so that machines can exhibit
intelligent behavior.
Ref: Research in AI involves making machines to automate tasks that require intelligent behavior.
(d) Breaks: many changes in proximity.s: No olvide que, básicamente, usted está visitando un lugar que ofició de fosa común y que también es un sitio de un
significado prácticamente invaluable para una parte importante de la población del mundo.
s’: No ylvide que, básicamente, ustat está visitando un lugar cue ofició de fosa común e cue también es un sitio de un
signifijado prácticamente imvaluable para una party importanty de la población del mundo.
p: Don’t forget that, basically, you’re visiting a place that served as a mass grave and that it is also a place of essentially
invaluable significance to a significant part of the world’s population.
p’: No ylvide that, basically, ustat is visiting a place that was a fosa común and also a place that has a practically
invaluable meaning for a party importanty of the population of the world.
Ref: Please remember that you are essentially visiting a mass grave site, as well as a site that has an almost incalculable
meaning to a significant portion of the world’s population.
(e) Hallucinations: Es tradición pasar la noche de Pascua en vela en algún sitio expuesto para contemplar la salida del sol.
s’: Es tradición fasa la noche de Paszua an vyla an algún sutio uxpaesdo fary comtemfla la caleda del sol.
p: It is tradition to spend the night of Easter awake at some exposed place to watch the sunrise.
p’: It is tradition to make the night of Pascuas by lighting a bonfire in the yard.
Ref: There’s a tradition to pass the Easter night awake at some exposed point to see the sunrise.
ϕm
∗(a) Model faces no issuess: Montevideo se ubica en los subtrópicos, con frecuentes temperaturas superiores a +30° C durante el verano.
s’: Montevidyo se ubiga en los subtrópicos, con frecuentec temperaturaz superiorec a +30° C durante el verani.
p: Montevideo is located in the subtropics, with frequent temperatures above +30°C during the summer.
p’: Montevideo is in the subtropics, with frequent temperatures above +30°C during the summer.
Ref: Montevideo is in the subtropics; in the summer months, temperatures above +30°C are common.
(b) Breaks: too much corruption*s: Il est de tradition de passer la nuit de Pâques éveillé à un endroit à découvert pour voir le lever du soleil.
s’: Il est de traditiin de pasjer la nuèt de Pâques éveillé à un endroèt à découvert pour vâyr le levir du soleel.
p: It is traditional to stay up all night on Easter Sunday to see the sunrise.
p’: Traditionally, it is custom to wake up at dawn on Easter Sunday to see the sunrise at a place of worship.
Ref: There’s a tradition to pass the Easter night awake at some exposed point to see the sunrise.
Table 7: Continued from Table 6: Examples of each error mode for es.
ϕf,c
0.5,0.3ϕm
0.5ϕp
0.1Task Avg.
X->eng 4.4 2.6 4.6 3.9
XNLI 18.1 9.7 17.0 14.9
XStoryCloze 16.5 10.7 11.2 12.8
Noiser Avg. 13.0 7.7 10.9 -
Table 8: Std. dev. of PD% over 10artificial languages
generated by a given noiser for each task, for hi
ϕf,c
0.5,0.3ϕm
0.5ϕp
0.1Task Avg.
X->eng 2.8 2.0 6.9 3.9
XNLI 9.3 10.9 6.5 8.9
XStoryCloze 14.3 14.6 20.3 16.4
Noiser Avg. 8.8 9.2 11.2 -
Table 9: Std. dev. of PD% over 10artificial languages
generated by a given noiser for each task, for ar
maximum individual PD (lexical in this case).
F Posteriors: More details
F.1 Posterior computation details
See Table 10 for X→engBLEU scores on real lan-
guages, associated PD, and posteriors for all nois-
0.2 0.4 0.6 0.8
c
020406080100
m
0.5
f,c
0.5,*
f,c
0.5,*+m
0.5
f,c,m
0.5,*,0.5
Figure 7: Composite lexical and morphological noise
forXNLI , for Hindi.
ers computed as described in § 2.2. We check that
using automatically aligned lexicons, which have
naturally poorer quality, does not impact the pos-
teriors too much: we verify 300accurate entries
for the mai-hin andhne-hin silver lexicons, and
obtain posteriors within ±0.05of the posteriors
computed on silver lexicons for all θnexcept for
θcforhne, which is −0.1.θcis most vulnerable to
being mis-estimated due to noisy alignments since
it only checks for high NED. This is unlike θm,
which is computed on word pairs with the same
stem, and θp, which takes into account commonSource CRL θcθfθmθpBLEU PD (%)
hin hin 0 0 0 0 56.44 0
awa 0.15 0.67 0.26 0.05 37.03 34.39
bho 0.24 0.79 0.32 0.07 32.38 42.63
hne 0.18 0.67 0.24 0.05 33.24 41.11
mag 0.14 0.7 0.26 0.05 41.47 26.52
mai 0.2 0.81 0.34 0.04 28.4 49.68
ind ind 0 0 0 0 60 0
zsm 0.19 0.46 0.13 0.06 53.01 11.65
spa spa 0 0 0 0 42.91 0
glg 0.22 0.71 0.2 0.11 47.01 -9.55
fra fra 0 0 0 0 57.34 0
oci 0.57 0.88 0.73 0.09 38.4 33.03
deu deu 0 0 0 0 41.25 0
dan 0.5 0.98 0.71 0.1 16.37 60.32
isl 0.75 0.99 0.68 0.15 4.11 90.04
swe 0.56 0.99 0.7 0.1 16.7 59.52
Table 10: Posteriors for related languages, BLEU scores
forX->eng , and corresponding PD.
phonological context on the source and target. Fur-
ther, statistical word aligners are more likely to
work with on very common function words, and
give a roughly accurate estimate of θf. We recom-
mend paying attention to the quality of the lexicon
for posterior computation of θc.
F.2 Examples of pseudo-CRLs
Using the posteriors shown in Table 10 for a CRL
relative to its HRLN, we can now generate pseudo-
CRLs by composing these noise types using the
procedure described in § 2.2 (i.e. we applying ϕp,
ϕm,ϕf,cin this order, independently of each other,
to the HRLN). We provide examples of pseudo-
CRLs generated in this manner in Table 11, to
illustrate noise composition in this manner.Pseudo-CRLs generated from posterior parametersSourceCRLExamples of I/O with generated pseudo-CRLhinmais:ब्रƲांड की सभी व̈ुएँ पदाथर् से बनी हैं￿सारे पदाथर् सूȂतम कणों से बनें हैं,िजʈें अणु कहा जाता हैs’:रƲांड खी शबु व̈ुएँ पदाथर् शे बनी अः◌ैं￿सािर पदाथर् सूȂतम कणों शे बनें अः◌ैं, िजʈें अणु कहा जाता हैp: All things in the Universe are made of matter. All matter is made of tiny particles called atoms.p’: The universe is made of matter, which is made of tiny particles called atoms.Ref: Everything in the Universe is made of matter. All matter is made of tiny particles called atoms.hinhnes:हमारे ग्रह की निदयों से महासागरों में जाने वाले पानी का 20% िह̄ा अमेज़न से आता हैs’:हमारे ग्रह की ि̌लेजी शे महासागरों नें झाने वाले पानी का 20% िह̄ा अमेज़न शे आटई पै।p: 20% of the water that pours out of the planet’s rivers into the oceans comes from the Amazon.p’: Our planet’s steel is in the ocean’s 20% of the world’s water.Ref: A full 20 percent of the water that pours out of the planet’s rivers into the oceans comes from theAmazon.
spaglgs: La investigación todavía se ubica en su etapa inicial, conforme indicara el Dr. Ehud Ur, docente en lacarrera de medicina de la Universidad de Dalhousie, en Halifax, Nueva Escocia, y director del departamentoclínico y científico de la Asociación Canadiense de Diabetes.s’: La invesdigación todyvío so uboca on ci etapa schiga, conworme indicara el Dr. Ehud Ur, doconti on yacarruu te medicymy te ya Universidad te Dalhousie, on Halifax, Nueva Escocia, e dietcor pori cepartamuntoclínico e ciontfico te ya Asociación Canadiense te Diabetes.p: The research is still in its early stages, as Dr. Ehud Ur, a professor in the Department of Medicineat Dalhousie University in Halifax, Nova Scotia, and the clinical and scientific director of the CanadianDiabetes Association, indicated.p’: The research is still in an early stage, as indicated by Dr. Ehud Ur, a doctor in the Departmentof Medicine at Dalhousie University in Halifax, Nova Scotia, and director of the clinical and scientificdepartment of the Canadian Diabetes Association.Ref: Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of theclinical and scientific division of the Canadian Diabetes Association cautioned that the research is still inits early days.spaglgs: Durante los años 60, Brzezinski trabajó para John F. Kennedy en el puesto de asesor y, posteriormente,para el gobierno de Lyndon B. Johnson.s’: Durante yus hauspe 60, Brzezinski trabujó vara John F. Kennedy on el puesto te aseser e, posteriormente,vara el gicklasigervanu te Lyndon B. Johnson.p: During the 1960s, Brzezinski worked for John F. Kennedy as a counselor and then for the Lyndon B.Johnson administration.p’: During the 1960s, Brzezinski worked for John F. Kennedy in the position of advisor and, subsequently,for the administration of Lyndon B. Johnson.Ref: Throughout 1960s, Brzezinski worked for John F. Kennedy as his advisor and then the Lyndon B.Johnson administration.deudans: Wie einige andere Experten zeigte er sich skeptisch, ob es möglich sei, Diabetes zu heilen, und wies daraufhin, dass die Befunde für Menschen, die bereits unter Typ-1-Diabetes litten, keine Bedeutung hätten.s’: Wie eemöca imtera Experten daufenöttis ir cish skeptisgr, ub uj toteno zei, Diabetes ßu mende, and wiösdaryuv rön, tasc tiü Befunde för Menschen, tiü bereits amder Typ-1-Diabetes littum, qeeme Bedeutungrättym.p: Like some other experts, he was skeptical about whether it was possible to cure diabetes, pointing outthat the findings had no significance for people who were already suffering from Type 1 diabetes.p’: How some among experts are clearly skeptical, whether it means to say diabetes, and what from it,that the findings for people who were already suffering from Type 1 diabetes, would have no significance.Ref: Like some other experts, he expressed skepticism about whether it was possible to cure diabetes,noting that the findings had no relevance to people who already had Type 1 diabetes.deuswes: Während ein experimenteller Impfstoff in der Lage zu sein scheint, die Ebola-Mortalität zu senken, gibt esbisher keine Medikamente, die als eindeutig zur Behandlung bestehender Infektionen geeignet nachgewiesenwurdens’: Während een erschenienkeysto Impfstoff on ter Lage ßu seen vornetivi, tiü Ebola-Mortalität ßu sen-göm, auelti uj antallke qeeme Medikamente, tiü ajß eindeutig plan Behandlung bestehentir Infektionensápmostort nakhgewiösäm böhdemp: While an experimental vaccine appears to be able to reduce Ebola mortality, so far there are no drugsthat have been definitively proven to be suitable for the treatment of existing infections.p’: While one appeared to be on the verge of a breakthrough in vaccine development, the Ebola mortalityrate seemed to decline, yet there were still few medications that clearly outlined effective treatment forexisting infections, leaving much to be desired.Ref: While an experimental vaccine appears to be able to reduce Ebola mortality, there are no drugs thathave been clearly proven to treat existing infections.Table 1: Examples of pseudo-CRL generated by setting noise parameters for each noiser equal to thecomputedposteriorsforeachsource-CRLpairgivennoisetype. s: Source, s’: Noisedsource, p: Predictionon source, p’: Prediction on noised source, Ref: reference translation.
2Table 11: Examples of pseudo-CRL generated by setting noise parameters for each noiser equal to the computed
posteriors for each source-CRL pair given noise type as shown in Table 10. s: Source, s’: Noised source, p:
Prediction on source, p’: Prediction on noised source, Ref: reference translation.