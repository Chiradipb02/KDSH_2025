Rethinking the Role of Proxy Rewards in Language Model Alignment
Sungdong Kim1,2,3Minjoon Seo3
NA VER Cloud1NA VER AI Lab2KAIST AI3
sungdong.kim@navercorp.com minjoon@kaist.ac.kr
Abstract
Learning from human feedback via proxy re-
ward modeling has been studied to align Large
Language Models (LLMs) with human values.
However, achieving reliable training through
that proxy reward model (RM) is not a trivial
problem, and its behavior remained as a black-
box. In this paper, we study the role of proxy
rewards in the LLM alignment via ‘reverse re-
ward engineering’ by composing interpretable
features as a white-box reward function. We
aim to replicate the ground truth (gold) reward
signal by achieving a monotonic relationship
between the proxy and gold reward signals af-
ter training the model using the proxy reward
in reinforcement learning (RL). Our findings
indicate that successfully emulating the gold re-
ward requires generating responses that are rel-
evant with enough length to open-ended ques-
tions, while also ensuring response consistency
in closed-ended questions. Furthermore, result-
ing models optimizing our devised white-box
reward show competitive performances with
strong open-source RMs in alignment bench-
marks. We highlight its potential usage as a
simple but strong reward baseline for the LLM
alignment, not requiring explicit pairwise hu-
man feedback dataset and RM training1.
1 Introduction
To align large language models (LLM) with human
values like helpfulness, human feedback-based
learning has been studied (Ouyang et al., 2022; Bai
et al., 2022a,b; Rafailov et al., 2023). Reinforce-
ment Learning from Human Feedback (RLHF) is a
dominant approach to exploiting human feedback.
Typically, the human feedback is used to train a
proxy reward model (RM), and a policy model is
optimized over the reward signal from the RM us-
ing RL (Schulman et al., 2017; Ziegler et al., 2020).
However, reliable RL training with the proxy
RM is not a trivial problem. Gao et al. (2023) study
1Our code is available at github.com/naver-ai/rethinking-
proxy-reward.
0 500 1000 1500 2000 2500 3000 3500 4000
PPO Step1.5
1.0
0.5
0.00.51.01.5Normalized Reward ScoreReward Type
Proxy
GoldModel w. Reward of
Length-only
Reverse-EngineeredFigure 1: A preview of our reverse reward engineering
experiment. First, we design white-box reward func-
tions with interpretable features such as the length or
relevance of the response. Then, we conduct RL training
using each of the designed functions as a proxy reward
and deem it a success in reverse engineering if a mono-
tonic relationship between the proxy and the ground
truth (Gold) reward scores is observed in the multiple
evaluations. The reverse-engineered reward (blue) ex-
hibits such a tendency, whereas the length-only reward
(green) does not achieve the monotonic relationship,
showing reward overoptimization (Gao et al., 2023).
the overoptimization problem of the policy train-
ing against the imperfect RM. It is a phenomenon
that the proxy rewards from the RM are increased
continuously, but the true rewards are saturated or
even decreased in fact. Similarly, the policy model
often finds undesired shortcuts in the reward signal
from the imperfect black-box RM, i.e., “reward
hacking” (Skalse et al., 2022; Pang et al., 2022).
A line of research has studied the innate limita-
tions of human feedback. Xu et al. (2023) reveal
prior knowledge affects the preference judgment in
that crowd workers often choose the preferred re-
sponse considering surface-level properties such as
conciseness and specificity, while experts focus on
more essential properties like factuality and com-
pleteness. Similarly, Hosking et al. (2023) further
categorize the attributions of the preference judg-arXiv:2402.03469v3  [cs.LG]  6 Oct 2024ments and confirm that human judgments could be
biased by stylistic features like assertiveness. More-
over, Singhal et al. (2023) analyze the correlations
between long response and RLHF, demonstrating a
verbosity bias in the human feedback.
In this work, we show merely combining inter-
pretable features as a proxy reward function can
substantially maximize the score of state-of-the-art
RM (Lambert et al., 2024) and further enhance
human preferences in various alignment bench-
marks (Chiang et al., 2023; Li et al., 2023; Zheng
et al., 2023). Specifically, we study the role of
proxy rewards learned by human feedback in the
RLHF by employing “reverse reward engineering”
with interpretable features, such as length or rele-
vance of the responses, to compose the white-box
reward function, as shown in Figure 1. We de-
fine the goal of reverse engineering as achieving
amonotonic relationship between the proxy and
ground truth (gold) reward signals after training
the model with the proxy reward in an RL manner.
This is to verify whether training to maximize the
proxy white-box reward during RL training also
maximizes the gold reward signal at the test time.
Experimental results indicate that the key to
imitating the gold reward signal lies in generat-
ing responses that are sufficiently lengthy yet rel-
evant and faithful for open-ended queries while
ensuring consistency in responses to closed-ended
queries. Contrary to previous study (Singhal et al.,
2023), our results demonstrate that solely optimiz-
ing towards lengthy response makes drastic drops
in the gold reward, showing severe overoptimiza-
tion. Also, the reward branching according to query
type reliably improves the win rate while not intro-
ducing unnecessary verbosity compared to other
baseline reward options. Furthermore, we find that
the reward differentiation brings advantages in im-
proving preference while minimizing alignment
tax (Ouyang et al., 2022), a phenomenon where
the increased preference accompanies the degraded
performance on other NLP tasks. Notably, the engi-
neered white-box reward often results in even more
improvements than the strong open-source RM like
UltraRM-13B (Cui et al., 2023) and also generally
works well across LLM backbones, demonstrating
its potential usage as a baseline reward.
Our contributions are summarized in three folds:
•We investigate the role of proxy rewards
learned by human feedback via reverse reward
engineering with interpretable features.•Our results suggest that the key to replicating
the gold reward involves producing responses
that are not only detailed but also relevant for
open-ended queries while maintaining consis-
tency in responses to closed-ended queries.
•We highlight the potential usage of the reverse-
engineered white-box reward function as a
simple but strong reward baseline not requir-
ing a pairwise human feedback dataset.
2 Related Work
Reinforcement Learning from Human Feedback
Reinforcement Learning from Human Feedback
(RLHF) is the most prevalent approach to leverage
the human preference (Ziegler et al., 2020; Ouyang
et al., 2022; Bai et al., 2022a; Liu et al., 2022;
Scheurer et al., 2023). These approaches include
the reward modeling stage to develop a proxy RM
predicting the human preference over the response,
e.g., typically as a scalar format. Then, the trained
proxy RM computes reward scores for the sampled
responses in the RL stage, i.e., rollout. Recently, re-
placing human feedback with AI or synthetic feed-
back has been studied (Bai et al., 2022b; Kim et al.,
2023; Sun et al., 2023b; Cui et al., 2023). There
are attempts to leverage multiple rewards beyond
the overall reward (Touvron et al., 2023; Wu et al.,
2023; Rame et al., 2023; Jang et al., 2023). Wu
et al. (2023) devise fine-grained rewards consisting
of relevance, factuality, and information complete-
ness to enhance long-form question-answering.
Pitfalls of Reward Modeling and Human Feed-
back RLHF faces various challenges, espe-
cially in reward modeling and human feedback
itself (Casper et al., 2023). Gao et al. (2023)
present the overoptimization problem by the imper-
fect proxy reward models. The issue arises when
the policy, tailored to enhance the estimated re-
wards, fails to improve real-world rewards. Many
studies try to mitigate the overoptimization prob-
lem (Moskovitz et al., 2023; Coste et al., 2023).
Similarly, Shen et al. (2023) study reward inconsis-
tency and leverage a sentence encoder to increase
the reward consistency. The work leverages Sim-
CSE (Gao et al., 2021) to augment the preference
dataset and normalize reward scores. The policy
models within the RLHF often find and exploit un-
desired shortcuts or errors in the RMs to maximize
the proxy rewards. The phenomenon is known as
reward hacking (Amodei et al., 2016; Pan et al.,At every k stepsGold RM(Black-box)GTRewardProxy RM(White-box) Length Incentive  Repetition Penalty
Query RelevanceReward Branching…
ProxyRewardMonotonic Relationship?LLM(RL trained by Proxy RM)Rollouts for Valid set (AlpacaEval)RL TrainingFigure 2: An overview of reverse reward engineering
study. It aims to imitate the ground-truth reward signal
by Gold RM with white-box reward features such as
length, repetition, and relevance. Specifically, we try to
observe the monotonic relationship between the proxy
and gold reward signals across the multiple evaluations
during RL training. We could comprehend the roles of
Gold RM via the interpretable features from the study.
2022; Pang et al., 2022; Song et al., 2023; Eisen-
stein et al., 2023). Much literature points out that
human preference can be easily biased by super-
ficial features such as complexity and assertive-
ness (Xu et al., 2023; Hosking et al., 2023). Sing-
hal et al. (2023) study a high correlation between
response length and RLHF training. Motivated by
these studies, we integrate a set of interpretable fea-
tures, such as length incentive, repetition penalty,
query relevance, and distinction by query type, to
understand the behaviors of the learned proxy re-
ward models.
3 Experimental Setup
3.1 Reverse Reward Engineering
Inspired by previous studies on superficial biases
in human feedback (Singhal et al., 2023; Xu et al.,
2023; Hosking et al., 2023), we aim to explore
whether there would be a reward function with in-
terpretable features that can maximize the “ground
truth” reward signal in reverse, as shown in Fig-
ure 2. Started by the length bias (Singhal et al.,
2023; Chen et al., 2024b), we progressively designthe additional features to achieve the goal. More
formally, we first perform RL training, i.e., Prox-
imal Policy Optimization (PPO) (Schulman et al.,
2017), with the designed function as a proxy re-
ward and then consider it a success in reverse en-
gineering if a monotonic relationship between the
proxy reward and the ground truth reward is ob-
served in the multiple evaluations over validation
set. In other words, if overoptimization (Gao et al.,
2023) for the proxy reward is detected, it will be
considered a failure in reverse engineering.
3.2 Features for White-box Reward
In this subsection, we detail the reward features em-
ployed in our experiments on reverse engineering.
For a set of (x, y)pairs, we denote xas a query, y
as a reference response, and ˆyas a rollout response
of Proximal Policy Optimization (PPO) (Schulman
et al., 2017; Ziegler et al., 2020).
Length Incentive ( LI).First, we introduce
Length Incentive ( LI) to exploit the verbosity bias
in human preference (Singhal et al., 2023; Wang
et al., 2024; Chen et al., 2024b). It scores the re-
ward by calculating LI(ˆy) =# words of ˆy
100. Intu-
itively, this reward function promotes the longer
generation regardless of the given query x. It is sim-
ilar to LPPO suggested by (Singhal et al., 2023),
but we do not normalize the reward with the target
length.
Repetition Penalty ( RP).In a preliminary study,
solely using LIas a reward function produces
undesired repetitions in the generations (Welleck
et al., 2019). We employ the Repetition penalty
(RP) to mitigate unnecessary repetitions in ˆy. It
checks the unique trigram ratio of ˆy, i.e., RP(ˆy) =
# unique trigram of ˆy
# trigrams of ˆy. Integrating it with the Length In-
centive function efficiently prevents redundancy
and encourages proper length of outputs, i.e.,
LI(ˆy)·RP(ˆy).
Query Relevance ( QR).We find that combining
LIandRPis somewhat effective, but the functions
often promote the generation of irrelevant content
since both functions are input-agnostic. Hence,
we involve the Query Relevance ( QR) as one of
the reward features. It checks whether the gen-
erated response ˆycontains off-topic contents for
a provided query x. We compute the relevance
score between the query and sampled response:
QR(x,ˆy) = M(x)·M(ˆy)∈R1, where the Mis pre-trained bi-encoder retriever such as Con-
triever (Izacard et al., 2021).
Reward Branching and Reference Answer Rele-
vance ( AR).The intuition behind the mixture of
reward functions LI(ˆy)·RP(ˆy)·QR(x,ˆy)is aiming
for high relevance between query and generated
response while promoting long response length
and less repetitions. However, the reward func-
tions, especially LI, could be problematic when
the query requires constrained or factual responses.
In these cases, consistency of response might be
much more important than the long and diverse
responses, as discussed in Touvron et al. (2023);
Song et al. (2023).
We apply the different relevance rewards accord-
ing to query type to handle such cases. Specifically,
we define two types of queries, Open-ended (OE)
andClosed-ended (CE) , according to whether
they require creative and open-ended or consistent
and constrained responses, similar to Song et al.
(2023). Then, we include reference answer rele-
vance AR(y,ˆy) =M(y)·M(ˆy), where the yis
the ground-truth reference response of given query.
Finally, we denote the final reward design, Reverse
Engineered Reward ( RER):
RER=(
LI(ˆy)·RP(ˆy)·QR(x,ˆy)ifT(x) = OE
RP(ˆy)· F(AR(y,ˆy)) else
, where the T(x)∈ {OE,CE}indicates the pre-
identified query types and the Findicates a linear
interpolation function mapping the range of ARto
the range of LI(ˆy)for stable RL training2.
Query-Type Classification. Deciding whether
the given query requires open-ended generation
may not be obvious because of its subjectivity in
interpreting the range of ‘open-ended’. Neverthe-
less, we define the meaning of open-ended so that
the responses to the query can be relatively any-
thing and creative. For example, “How can I make
a good first impression?” could be an open-ended
query. On the other side, the closed-ended query
indicates the corresponding responses should be
consistent and constrained as a closed form. “How
many movies are in the Fast and Furious franchise?”
would be one of the closed-ended queries. Consid-
ering resource constraints, we automatically anno-
tate the query type T(x)(OEorCE) used in RERby
2We empirically find that the multiplication of each reward
function works better than the addition of them. Please see
Appendix B for more details.prompting it to GPT-4 (OpenAI, 2023)3. Please
see Appendix A for more details of the query-type
classification.
PPO training. We use the resulting white-box
reward function, RER, to further optimize an SFT
model with reinforcement learning. Consequently,
the following PPO objective induces the model to
maximize RER,
max
πϕE(x,y)∼D,
ˆy∼πϕ(·|x)[RER(x,ˆy, y)−βlogπϕ(ˆy|x)
πρ(ˆy|x)
],
while online policy πϕand the fixed reference
(SFT) policy πρdo not diverge too much by reg-
ularizing it with KL constraints (Schulman et al.,
2017; Ziegler et al., 2020), where the Dis train
dataset and the βis the coefficient to control the
KL penalty.
3.3 Evaluation setup
Implementation Details. We conduct experi-
ments based on two alignment datasets, Anthropic-
HH (Bai et al., 2022a) and AlpacaFarm (Dubois
et al., 2023). We perform PPO (Ziegler et al., 2020)
training with rewards from RERor other reward
models at most 5k steps. As in Section 3.2, we
employ Contriever (Izacard et al., 2021) for scor-
ing relevance scores4. We train SFT model based
on LLaMA-2-7B (Touvron et al., 2023) with the
161k chosen responses in Anthropic-HH5. For the
PPO training on Anthropic-HH, we select about
23k (train and dev) first-turn queries from ‘Helpful-
rejection’ and ‘Helpful-online’ sub-splits for effi-
cient implementation. Also, we employ the pre-
trained SFT model for the AlpacaFarm. We use
20k of ‘unlabeled’ split for the PPO training on
AlpacaFarm6. More details are in Appendix C.
Gold RM. As discussed in Gao et al. (2023), it
is a non-trivial problem to obtain human labels
for the ground-truth reward signal. Hence, fol-
lowing the Gao et al. (2023), we also assume the
gold-standard reward model (Gold RM) to evalu-
ate the reward functions. Specifically, we employ
one of the strong open-source RMs, StarlingRM-
34B7, as the Gold RM. It is trained on the Yi-34B-
Chat (Young et al., 2024) with Nectar, which is
3We find that about 83% of decisions by GPT-4 are agreed
upon when the authors manually validate 100 samples.
4huggingface.co/facebook/contriever
5huggingface.co/datasets/Anthropic/hh-rlhf
6huggingface.co/datasets/tatsu-lab/alpaca_farm
7huggingface.co/Nexusflow/Starling-RM-34Ban AI feedback dataset (Zhu et al., 2023). This
model demonstrates the best performance on vari-
ous preference evaluation tasks within the recently
proposed RewardBench (Lambert et al., 2024). We
check the reward signal relationship between proxy
and gold rewards based on 805 prompts in AlpacaE-
val (Li et al., 2023).
Other Evaluations. We further evaluate the re-
sulting models on Vicuna-Bench (Chiang et al.,
2023), AlpacaEval (Li et al., 2023), and MT-
Bench (Zheng et al., 2023). The three bench-
marks mainly evaluate the preference of models’
responses leveraging a superior proprietary LLM,
GPT-4 (OpenAI, 2023). In addition, we include
Super-NaturalInstructions ( SuperNI ) (Wang et al.,
2022) to investigate the alignment tax problem of
the resulting models. More evaluation details are
in Appendix D.
Relevant Sentence Ratio. To assess the rele-
vance of the model’s responses for the provided
query, we measure the Relevant Sentence Ratio (%
Rel. Sent). Specifically, we request GPT-4 (Ope-
nAI, 2023) to refer to each sentence first and then
judge its query relevance as either ‘Relevant’ or ‘Ir-
relevant’. That is, it indicates the ratio of on-topic
sentences in a generated response. The regarding
prompt for this evaluation is found in Appendix E.
Baseline RMs. In addition to the Gold RM, i.e.,
StarlingRM-34B , used for our analysis, we in-
clude three open-source RMs as baselines. Ope-
nAssistant (OASST) is trained with four pub-
lic feedback datasets, WebGPT (Nakano et al.,
2021), OpenAI Summary (Stiennon et al., 2020),
Anthropic-HH (Bai et al., 2022a), and Synthet-
icGPT8, based on DeBERTa-V3-Large (He et al.,
2021). SteamSHP-XL is another open-source RM
trained with SHP (Ethayarajh et al., 2023) and
Helpfulness subset of the Anthropic-HH. Unlike
other feedback datasets, the SHP consists of human-
generated responses from an online community,
Reddit. It is based on FLAN-T5-XL (Chung et al.,
2022). Also, we include UltraRM-13B , one of
the top open-source RMs trained on UltraFeed-
back (Cui et al., 2023). The UltraFeedback is
a 64k AI feedback dataset constructed by GPT-
4 (OpenAI, 2023). Also, it is further fine-tuned
with the public feedback datasets, Anthropic-HH,
SHP, and OpenAI Summary. We perform PPO
8Dahoas/synthetic-instruct-gptj-pairwisetraining against the reward score by each RM or
our reward functions.
4 Experiments
4.1 Reverse Reward Engineering
We compare four combinations of the features as
a proxy reward function, (1) LI, (2) LI·RP, (3)
LI·RP·QR, and (4) RER. To capture the monotonic
relationship between proxy and both reward sig-
nals, we compute reward scores of both proxy and
gold for AlpacaEval (Li et al., 2023) set at every
500 PPO steps against the proxy reward. Figure 3
and Table 1 show the results for the reverse re-
ward engineering based on the Anthropic-HH (Bai
et al., 2022a) and AlpacaFarm (Dubois et al., 2023)
datasets. We perform standard normalization over
the reward scores for the plotting.
We find that the solely promoting lengthy re-
sponse, i.e., LIas a proxy reward, fails to mono-
tonically increase the gold reward signal, con-
trary to recent findings in Singhal et al. (2023).
More specifically, it improves the gold reward for
the initial 1-1.5k steps but subsequently leads to
a consistent decrease in the gold reward for the
remaining steps even though the proxy rewards are
consistently improved, as shown in Figure 3 (a) and
(e). Recalling that the PPO steps were set to at most
400 in the Singhal et al. (2023), we presume that
theLIis effective only for a few initial steps. Pe-
nalizing unnecessary repetitions along with lengthy
responses shows a better tendency, yet it eventually
results in a decrease in the gold reward (Figure 3
(b) and (f)). However, the magnitude of this de-
crease is considerably less than that experienced
with LIalone. The LIis the only reward that ex-
hibits negative correlations with the gold reward
signal in Table 1.
Considering relevance along with the features
reliably increases the gold reward, indicating
the success of reverse engineering. We find the
QRcontribute to avoiding the drastic drop in gold
reward score and even achieves the success of the
reverse engineering, improving the SpearmanR up
to 0.86, as shown in Figure 3 (c), (g) and Table 1.
On the other hand, RERmonotonically increases
the gold reward signal for both datasets, as illus-
trated in Figure 3 (d) and (h). Also, in Table 1,
RERachieves the highest Spearman correlations, in-
dicating a strong monotonic relationship with the
gold reward signal on both datasets. Although RER
shows a relatively slower start compared to other re-0 1000 2000 3000 4000 50002.0
1.5
1.0
0.5
0.00.51.01.5Normalized Reward Score(a) Anthropic-HH - LI
Gold
Proxy
0 1000 2000 3000 4000 5000(b) Anthropic-HH - LI * RP
0 1000 2000 3000 4000 5000(c) Anthropic-HH - LI * RP * QR
0 1000 2000 3000 4000 5000(d) Anthropic-HH - RER
0 1000 2000 3000 4000 5000
(e) AlpacaFarm - LI1.5
1.0
0.5
0.00.51.01.5Normalized Reward ScoreGold
Proxy
0 1000 2000 3000 4000 5000
(f) AlpacaFarm - LI * RP0 1000 2000 3000 4000 5000
(g) AlpacaFarm - LI * RP * QR0 1000 2000 3000 4000 5000
(h) AlpacaFarm - RERFigure 3: Results of reverse reward engineering. We visualize normalized proxy and gold reward scores for every
500 PPO steps against each reward design option. The results on the upper side are from Anthropic-HH (Bai et al.,
2022a), and the results on the lower side are from AlpacaFarm (Dubois et al., 2023), respectively. Instances of
AlpacaEval (Li et al., 2023) are used to compute the reward scores. We expect a monotonical relationship between
the proxy and gold reward scores to achieve success in reverse engineering. We find that considering the relevance
and adopting different rewards according to query type, i.e., RER, contribute to increasing the gold reward reliably.
ward options, it exhibits the most consistent upward
trend. However, we find the relevance alone, e.g.,
QR, can not improve the gold reward while showing
a similar tendency with the LI. It implies that aim-
ing for sufficiently lengthy and faithful responses
while avoiding off-topic or unnecessary repetitions
is key to maximizing the gold reward signal. These
results have a connection with the previous findings
on superficial biases like assertiveness in human
preference (Xu et al., 2023; Hosking et al., 2023).
Reward branching according to whether the
query requires open-ended responses makes a
meaningful difference, especially for CEtype
queries. We compare the final policies optimized
and selected by proxy reward functions. Specif-
ically, we measure reward scores using the Gold
RM based on the AlpacaEval set, as reported in
Figure 4. We find the reward scores are decreased
if the model exploits LIregardless of the query
type, i.e., no reward branching. We will show
this gap makes further differences in the alignment
tax problem (Askell et al., 2021; Ouyang et al.,
2022), which is a phenomenon showing perfor-
mance drops in other NLP tasks after alignment, in
Section 4.3.
Additionally, we compare the models’ diversityReward Anthropic-HH AlpacaFarm
PearsonR SpearmanR PearsonR SpearmanR
w.LI -0.77 -0.66 -0.52 -0.58
w.LI ·RP 0.52 0.05 0.70 0.63
w.LI ·RP ·QR 0.96 0.86 0.78 0.86
w.RER 0.92 0.99 0.86 0.97
Table 1: We report Pearson and Spearman correlation
(R) between proxy and gold reward scores across multi-
ple evaluations. For both datasets, RERshows a Spear-
manR close to 1. This indicates a monotonic relationship
between the two reward signals, signifying successful
reverse reward engineering.
and consistency according to the query type in Fig-
ure 5. We report the Self-BLEU (Zhu et al., 2018)
by sampling 10 responses for each input in the
AlpacaEval9. A high Self-BLEU score indicates
a consistent response, whereas a low Self-BLEU
score denotes a diverse response (Touvron et al.,
2023). As we intended, RERimproves the consis-
tency in the CEtype queries and promotes more
diverse responses in the OEtype, showing a similar
tendency to that of Gold RM. However, other mod-
els without reward branching produce excessively
diverse responses regardless of the query type.
9We perform nucleus sampling (Holtzman et al., 2019).OE CE
Query Type0.00.10.20.30.40.50.6Gold Reward ScoreRER LI * RP * QRFigure 4: Gold reward scores according to whether
query type requires open-ended ( OE) or closed-ended
(CE) responses. We compare two proxy reward options,
LI·RP·QRandRER, based on models trained with
Anthropic-HH (Bai et al., 2022a). We find they show
meaningful differences in CEtype, demonstrating the
importance of the reward branching.
4.2 Comparison with other open-source RMs
We compare our reward designs with open-source
RMs in improving human preference, e.g., produc-
ing more helpful responses, based on the Anthropic-
HH dataset (Bai et al., 2022a).
Optimizing only towards lengthy responses,
i.e.,LI, still improves the alignment scores com-
pared to the SFT baseline, even if it shows dras-
tic drops in Gold RM score as in Section 4.1.
In Table 2, the PPO LImodel even performs better
SteamSHP-XL and OASST based PPO models in
Vicuna-Bench (Chiang et al., 2023). This result
reminds us of the existence of verbosity bias in
LLM-as-a-judge evaluation (Zheng et al., 2023).
On the other hand, the contributions of RP,QR, and
reward branching are shown as a similar pattern
with the results of Section 4.1. Notably, the PPO
model trained with RERachieves even better perfor-
mances than the model trained with UltraRM-13B
in Vicuna-Bench and MT-Bench.
PPO model with RERachieves competitive per-
formance while showing a response distribution
similar to that of the model with the Gold RM in
terms of length, repetitions, and relevance. In Ta-
ble 3, we report the average number of tokens (Avg
#Tokens), 4-gram Repetitions (Rep.), and Relevant
Sentence Ratio (% Rel. Sent) of models’ responses
based on the AlpacaEval set. We can see the obvi-
ous differences in response distribution among RER
and other designed reward functions. In particular,
StarlingRM-34B RER LI * RP * QR LI
Model0.000.050.100.150.200.250.300.350.40Self-BLEUQuery-type
CE
OEFigure 5: Self-BLEU of the PPO models on AlpacaEval
according to query types. Please note that the higher
Self-BLEU indicates lower diversity for the responses,
i.e., consistent responses, and vice versa. As described
in Touvron et al. (2023), we intend the high Self-BLEU
for the CEtype and low Self-BLEU for the OEtype.
Model
(Anthropic-HH)Vicuna
BenchAlpaca
EvalMT
Bench
SFT 50.0 15.9 3.9
+ PPO
w.SteamSHP-XL 59.4 19.6 3.8
w.OASST 61.3 19.1 3.9
w.UltraRM-13B 73.8 28.0 4.2
w.StarlingRM-34B 79.4 29.1 4.3
w.LI 67.5 16.9 3.4
w.LI ·RP 71.9 20.3 3.9
w.LI ·RP ·QR 71.9 23.2 4.0
w.RER 76.9 23.4 4.4
Table 2: Comparison of the designed rewards with open-
source RMs trained on human or AI feedback, based
on Anthropic-HH (Bai et al., 2022a). The PPO model
optimizing RERshows competitive performances with
models trained with open-source RMs.
it exhibits the importance of reward branching in
that it achieves the win rate without unnecessary
verbosity, i.e., much less response length and repe-
tition while containing more on-topic content (the
higher % Rel. Sent). Qualitative examples among
the resulting models are in Appendix F.
4.3 Investigating Alignment Tax
We further analyze ‘ alignment tax ’ from the mod-
els trained with our devised rewards. The align-
ment tax is a phenomenon in which the improved
preference accompanies degraded performances on
other NLP tasks after alignment procedure (Askell
et al., 2021; Bai et al., 2022a; Ouyang et al.,
2022). We conduct these experiments based onModel
(Anthropic-HH)% Win
RateAvg
# Tokens4-gram
Rep.% Rel.
Sent
SFT 15.9 126 0.05 85.9
+ PPO
w.SteamSHP-XL 19.6 154 0.10 80.2
w.OAAST 19.1 186 0.10 85.0
w.UltraRM-13B 28.0 213 0.11 83.1
w.StarlingRM-34B 29.1 220 0.08 85.3
w.LI 16.9 351 0.32 76.5
w.LI·RP 20.3 285 0.17 81.6
w.LI·RP·QR 23.2 305 0.13 84.8
w.RER 23.4 243 0.09 85.1
Table 3: We analyze the responses from AlpacaEval (Li
et al., 2023). We report the relevant sentence ratio (%
Rel. Sent), leveraging GPT-4 (OpenAI, 2023). We also
reference the number of average tokens (# Avg Tokens)
and 4-gram repetitions (4-gram Rep.). RERarchives the
win rate while not increasing unnecessary verbosity.
another alignment dataset, AlpacaFarm (Dubois
et al., 2023). Specifically, we evaluate models on
SuperNI (Wang et al., 2022), the collection of zero-
shot NLP tasks, to measure alignment tax.
Reward branching and relevance contribute to
minimizing alignment tax. As shown in Table 4,
RERreliably improves the preference (% Win) on
the AlpacaEval while keeping high relevance (%
Rel), zero-shot instruction-following ability (Su-
perNI). If we use random query type, random T(x),
for the RERinstead of the predicted one by GPT-4,
its win rate and relevant sentence ratio are degraded
on the AlpacaEval. Also, its instruction-following
ability is sacrificed, indicating that reward differen-
tiation based on query types is effective in mitigat-
ing alignment tax.
There is an observable pattern that clearly
exacerbates the alignment tax when excluding
each component from RER.If we do not use ref-
erence answer relevance according to query type,
i.e.,LI·RP·QR, it shows even higher preference
scores (% Win) with decreased relevance (% Rel).
However, the achievement accompanies significant
performance drops on the SuperNI by about 12.3%
points. If the QRis discarded ( LI·RP), the perfor-
mance of the relevant sentence ratio and ROUGE-L
score of SuperNI drop more drastically. Finally,
using only the response length as a reward ( LI) im-
proves the preference slightly compared to SFT, as
reported in Singhal et al. (2023), but it significantly
worsens the % Rel and SuperNI score in return.
On the other hand, while StarlingRM-34B signif-
icantly improves the win rate, it notably degradesModel
(AlpacaFarm)AlpacaEval
% Win / % RelSuperNI
ROUGE-L
SFT 23.2 / 88.2 34.9
+ PPO
w.UltraRM-13B 34.1 / 85.1 28.8
w.StarlingRM-34B 44.0 / 87.3 17.3
w.LI 24.1 / 64.5 7.4
w.LI·RP 41.6 / 81.3 13.1
w.LI·RP·QR 41.5 / 84.8 22.6
w.RER 37.2 / 86.4 34.9
w. random T(x) 34.8 / 82.6 28.4
Table 4: Alignment tax measurement of models from
AlpacaFarm (Dubois et al., 2023) We include Su-
perNI (Wang et al., 2022) as a test set to measure how
the PPO models retain the instruction-following ability,
i.e., zero-shot NLP tasks. The random T(x)indicates
the query type T(x)is obtained by random.
0 2000 4000 6000 8000
PPO Step2.0
1.5
1.0
0.5
0.00.51.01.5Normalized Reward Score(a) Vicuna-7B w. RER
Gold
Proxy
0 2000 4000 6000 8000
PPO Step(b) Vicuna-13B w. RER
Figure 6: Normalized proxy and gold reward scores
for every 1k PPO steps against RER. We conduct the
experiments based on two strong SFT models, Vicuna-
{7, 13}B-v1.5 (Chiang et al., 2023; Zheng et al., 2023),
to observe scaling patterns. Both backbones optimizing
RERgenerally show the monotonic relationship between
theRERand Gold RM scores.
performance in the SuperNI compared to the SFT
baseline, exhibiting a severe alignment tax.
4.4 Generalizability of RER
We investigate how RERis generalizable regard-
ing (1) the number of PPO steps and (2) backbone
model size. Specifically, we conduct the exper-
iments based on strong SFT models, Vicuna-{7,
13}B-v1.5 (Chiang et al., 2023; Zheng et al., 2023).
We sample datasets from ShareGPT (Chiang et al.,
2023) for our PPO training. More training details
are in Appendix C.
We find the monotonic relationship is consis-
tently observed for both backbone sizes even if
we extend the number of PPO steps from 5k toVicuna-7B Vicuna-13B
SFT backbone01020304050607080Win RateSFT
PPO w. RERFigure 7: Win rate of models trained with RERcompared
to their baseline SFT backbones in AlpacaEval (Li et al.,
2023). They show consistent performance gains.
10k, as shown in Figure 6 . However, the upward
tendency is saturated after the 5k step and often
exhibits fluctuation in the reward scores. It ap-
pears that such saturation occurs earlier (2k) in the
13B model. Additionally, we observe that RERcon-
sistently improves the win rate by approximately
5-6% points for the two SFT backbones in the Al-
pacaEval, as illustrated in Figure 7. We emphasize
thatRERcould serve as a simple yet robust baseline
reward, eliminating the need for a separate reward
modeling process that involves pairwise human
feedback.
5 Discussion and Future Works
Our study does not cover all perspectives on the
role of RMs, such as safety or reasoning. We try
to reverse reward engineering over ‘general help-
fulness’, one of the perspectives on which many
alignment learning studies have focused (Li et al.,
2023). In other words, we do not argue that our
findings are universally applicable. Additionally,
the design choice of white-box rewards could be
substituted with other features, depending on the
target value or domain of interest. Exploring a
broader range of interpretable features could en-
hance our understanding (Mu et al., 2024), and
investigating aspects of the Gold RM that these
features do not capture presents another promising
direction for future research.
On the other hand, Table 5 shows the preference
accuracy of RMs used in our study. As expected,
there is a huge performance gap between our white-
box rewards and black-box RMs. However, we ob-
serve that the higher performance on the intrinsic
evaluation (preference accuracy) does not guaran-RM Chat Chat-Hard Safety Reasoning Overall
Black-box Rewards
UltraRM-13B 97.2 57.0 60.0 73.8 70.6
StarlingRM-34B 97.5 55.7 86.5 90.6 85.1
White-box Rewards
LI 77.7 32.0 41.5 57.7 52.2
LI·RP 83.5 32.0 41.1 54.4 51.2
LI·RP·QR 83.5 32.5 41.4 42.0 45.4
RER 80.2 46.3 62.3 57.0 59.5
Table 5: Preference Accuracy of RMs used in our study
on the RewardBench (Lambert et al., 2024).
tee a higher alignment performance. For instance,
although RERperforms worse than the other black-
box RMs in preference accuracy, it achieves higher
performance on the AlpacaEval and SuperNI than
the UltraRM-13B, as we showed in Table 4. We
think one possible variable to interpret is that an im-
perfect reference model is used to compute the KL
divergence term (Chen et al., 2024a; Ramé et al.,
2024). Investigating this misalignment is also an
interesting future direction.
6 Conclusion
In this work, we investigate the function of proxy
rewards in aligning LLMs through “reverse re-
ward engineering”, where interpretable features
are used to construct a white-box reward func-
tion. Our results demonstrate that the key to imi-
tating the gold reward signals requires producing
responses that are not only relevant but sufficiently
detailed for open-ended questions, while maintain-
ing consistency in responses to closed-ended ques-
tions. Additionally, models optimized using our
proposed white-box reward function exhibit com-
petitive performance alongside robust open-source
reward models in alignment benchmarks, demon-
strating its potential usage as a strong baseline re-
ward. We hope that our findings could benefit fu-
ture research on preference-based alignment.
Limitations
One limitation is the use of an arbitrarily large gold
reward model (RM), i.e., StarlingRM-34B (Zhu
et al., 2023), instead of a labeled ground truth re-
ward signal. This is problematic because it is chal-
lenging to guarantee that even this RM is accurately
mimicking the ground truth. Another limitation is
the use of only about 800 samples from the Al-
pacaEval (Li et al., 2023) set as the validation set to
measure the success of reverse reward engineering.
Although these 800 samples can be considered di-
verse, it is uncertain how the results would appearin a larger and more varied dataset. For instance,
the outcomes might have been entirely different for
queries related to reasoning, harmlessness, and cod-
ing included in the RewardBench (Lambert et al.,
2024). Lastly, even though we show the efficacy of
RERbased on automatic evaluations (Chiang et al.,
2023; Li et al., 2023; Zheng et al., 2023; Min et al.,
2023) leveraging proprietary LLMs such as GPT-
4 (OpenAI, 2023), these evaluation results might
not be rigorous enough and often contain biases as
discussed in Zheng et al. (2023).
Acknowledgment
This work was partly supported by KAIST-NA VER
Hypercreative AI Center and Institute for Infor-
mation & communications Technology Promo-
tion(IITP) grant funded by the Korea govern-
ment(MSIT) (RS-2024-00398115, Research on the
reliability and coherence of outcomes produced by
Generative AI, 30%). The authors would like to
thank the members of NA VER Cloud AI (especially
Kang Min Yoo), KAIST LKLab (especially Seun-
gone Kim), and Sangkyu Lee for their constructive
comments.
References
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul
Christiano, John Schulman, and Dan Mané. 2016.
Concrete problems in ai safety. arXiv preprint
arXiv:1606.06565 .
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, et al. 2021. A
general language assistant as a laboratory for align-
ment. arXiv preprint arXiv:2112.00861 .
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022a. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022b. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073 .
Stephen Casper, Xander Davies, Claudia Shi,
Thomas Krendl Gilbert, Jérémy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David
Lindner, Pedro Freire, et al. 2023. Open problems
and fundamental limitations of reinforcementlearning from human feedback. arXiv preprint
arXiv:2307.15217 .
Angelica Chen, Sadhika Malladi, Lily H Zhang,
Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, and
Kyunghyun Cho. 2024a. Preference learning al-
gorithms do not learn preference rankings. arXiv
preprint arXiv:2405.19534 .
Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen,
Tianyi Zhou, Tom Goldstein, Heng Huang, Moham-
mad Shoeybi, and Bryan Catanzaro. 2024b. Odin:
Disentangled reward mitigates hacking in rlhf. arXiv
preprint arXiv:2402.07319 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Thomas Coste, Usman Anwar, Robert Kirk, and
David Krueger. 2023. Reward model ensembles
help mitigate overoptimization. arXiv preprint
arXiv:2310.02743 .
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
guage models with high-quality feedback.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. 2023. Al-
pacafarm: A simulation framework for methods
that learn from human feedback. arXiv preprint
arXiv:2305.14387 .
Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ah-
mad Beirami, Alex D’Amour, DJ Dvijotham, Adam
Fisch, Katherine Heller, Stephen Pfohl, Deepak Ra-
machandran, et al. 2023. Helping or herding? reward
model ensembles mitigate but do not eliminate re-
ward hacking. arXiv preprint arXiv:2312.09244 .
Kawin Ethayarajh, Yejin Choi, and Swabha
Swayamdipta. 2023. Understanding dataset
difficulty with V-usable information. In Proceedings
of the 39th International Conference on Machine
Learning . PMLR.
Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-
ing laws for reward model overoptimization. In In-
ternational Conference on Machine Learning , pages
10835–10866. PMLR.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. arXiv preprint arXiv:2104.08821 .Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset for
adversarial and implicit hate speech detection. arXiv
preprint arXiv:2203.09509 .
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing.arXiv preprint arXiv:2111.09543 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 .
Tom Hosking, Phil Blunsom, and Max Bartolo. 2023.
Human feedback is not gold standard. arXiv preprint
arXiv:2309.16349 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv
preprint arXiv:2112.09118 .
Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong
Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh
Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.
2023. Personalized soups: Personalized large lan-
guage model alignment via post-hoc parameter merg-
ing.arXiv preprint arXiv:2310.11564 .
Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung
Kang, Donghyun Kwak, Kang Min Yoo, and Min-
joon Seo. 2023. Aligning large language mod-
els through synthetic feedback. arXiv preprint
arXiv:2305.13735 .
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024. Rewardbench: Evaluating reward
models for language modeling. arXiv preprint
arXiv:2403.13787 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023. Alpacaeval: An au-
tomatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush
V osoughi. 2022. Aligning generative language mod-
els with human values. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022 ,
pages 241–252, Seattle, United States. Association
for Computational Linguistics.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint
arXiv:2305.14251 .
Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas
Sandholm, Ruslan Salakhutdinov, Anca D Dragan,
and Stephen McAleer. 2023. Confronting reward
model overoptimization with constrained rlhf. arXiv
preprint arXiv:2310.04373 .
Tong Mu, Alec Helyar, Johannes Heidecke, Joshua
Achiam, Andrea Vallone, Ian D Kivlichan, Molly
Lin, Alex Beutel, John Schulman, and Lilian Weng.
2024. Rule based rewards for fine-grained llm safety.
InICML 2024 Next Generation of AI Safety Work-
shop .
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. Webgpt: Browser-assisted question-
answering with human feedback. arXiv preprint
arXiv:2112.09332 .
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Alexander Pan, Kush Bhatia, and Jacob Steinhardt.
2022. The effects of reward misspecification: Map-
ping and mitigating misaligned models. arXiv
preprint arXiv:2201.03544 .
Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault
Sellam, Ankur P Parikh, and He He. 2022. Reward
gaming in conditional text generation. arXiv preprint
arXiv:2211.08714 .
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. arXiv preprint
arXiv:2305.18290 .
Alexandre Rame, Guillaume Couairon, Mustafa Shukor,
Corentin Dancette, Jean-Baptiste Gaya, Laure
Soulier, and Matthieu Cord. 2023. Rewarded soups:
towards pareto-optimal alignment by interpolating
weights fine-tuned on diverse rewards. arXiv preprint
arXiv:2306.04488 .
Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert
Dadashi, Léonard Hussenot, Pierre-Louis Cedoz,
Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard,
and Olivier Bachem. 2024. Warp: On the benefits
of weight averaged rewarded policies (2024). URL
https://arxiv. org/abs/2406.16768 .Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak,
Jun Shern Chan, Angelica Chen, Kyunghyun Cho,
and Ethan Perez. 2023. Training language models
with language feedback at scale.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng
Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and
Dong Yu. 2023. The trickle-down impact of re-
ward (in-) consistency on rlhf. arXiv preprint
arXiv:2309.16155 .
Prasann Singhal, Tanya Goyal, Jiacheng Xu, and
Greg Durrett. 2023. A long way to go: Investi-
gating length correlations in rlhf. arXiv preprint
arXiv:2310.03716 .
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,
and David Krueger. 2022. Defining and characteriz-
ing reward gaming. Advances in Neural Information
Processing Systems , 35:9460–9471.
Ziang Song, Tianle Cai, Jason D. Lee, and Weijie J. Su.
2023. Reward collapse in aligning large language
models.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–
3021.
Simeng Sun, Dhawal Gupta, and Mohit Iyyer. 2023a.
Exploring the impact of low-rank adaptation on the
performance, efficiency, and regularization of rlhf.
arXiv preprint arXiv:2309.09055 .
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong
Zhou, Zhenfang Chen, David Cox, Yiming Yang, and
Chuang Gan. 2023b. Salmon: Self-alignment with
principle-following reward models. arXiv preprint
arXiv:2310.05910 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Chandu, David Wad-
den, Kelsey MacMillan, Noah A Smith, Iz Beltagy,
et al. 2024. How far can camels go? exploring the
state of instruction tuning on open resources. Ad-
vances in Neural Information Processing Systems ,
36.
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut SelvanDhanasekaran, Atharva Naik, David Stap, et al. 2022.
Super-naturalinstructions: Generalization via declar-
ative instructions on 1600+ nlp tasks. arXiv preprint
arXiv:2204.07705 .
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2019. Neu-
ral text generation with unlikelihood training. arXiv
preprint arXiv:1908.04319 .
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri,
Alane Suhr, Prithviraj Ammanabrolu, Noah A
Smith, Mari Ostendorf, and Hannaneh Hajishirzi.
2023. Fine-grained human feedback gives better
rewards for language model training. arXiv preprint
arXiv:2306.01693 .
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol
Choi. 2023. A critical evaluation of evaluations
for long-form question answering. arXiv preprint
arXiv:2305.18201 .
Alex Young, Bei Chen, Chao Li, Chengen Huang,
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu,
and Jiantao Jiao. 2023. Starling-7b: Improving llm
helpfulness & harmlessness with rlaif.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan
Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A
benchmarking platform for text generation models.
InThe 41st international ACM SIGIR conference
on research & development in information retrieval ,
pages 1097–1100.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
guage models from human preferences.A Query-type Classification
A.1 Automatic annotation
We automatically annotate the query type T(x)leveraging GPT-4 (OpenAI, 2023). Specifically, we use
the prompt of Figure 8 for the automation. As a result, we classify about 120k queries from various
sources used in our experiments, Anthropic-HH (Bai et al., 2022a), AlpacaFarm (Dubois et al., 2023), and
ShareGPT. The resulting examples are shown in Figure 9. For more dataset details, please see Appendix C.
Query-type Classification
Could you identify whether the last user query of the below conversation is open-ended or not?
Open-ended means that the responses to the query can be relatively anything and creative.
However, its counterparts, i.e., ‘closed-ended query’, indicate the corresponding responses should be consistent and constrained as
a closed form.
Please just answer it either “OPEN-ENDED” or “CLOSED-ENDED”.
CONVERSATION:
{conversation}
Figure 8: Prompt for query-type classification. We classify a query into one of Open-ended (OE) orClosed-ended
(CE) types, according to whether the response to the query requires creative or consistent.
Open-ended Closed-ended
What are some good exercises I can do at a desk to stretch? If I was arrested for a crime, what rights do I have?
What are some cool things do while in Washington DC? What age do I start talking to kids about puberty?
How can I deodorize clothes without washing them? What was the irish potato famine about?
how can i make my own superhero mask? How do you heal cuticles?
I’ve been having trouble sleeping at night. How can I
make sure I get my full eight hours?What is the main crop that is cultivated in the Northern
hemisphere?
I’ve been having frequent panic attacks lately and don’t
know what’s causing them.Is it hard to get a CDL license?
How can I make a good first impression? How can I track a package that has been lost in the mail?
What are some Christmas songs to enjoy with the kids? How many movies are in the Fast and Furious franchise?
Hey, where do you want to go for spring break this year? What does a urologist do?
Figure 9: Examples of queries belong to each query type, which is automatically annotated by GPT-4 (OpenAI,
2023), in the Anthropic-HH (Bai et al., 2022a).
B Combining Reward Functions
PPO training w. AlpacaEval SuperNI
Win Rate Avg Token 4-gram Rep. ROUGE
RER (Multiplication) 37.2 165 0.08 34.9
RER (Addition) 36.2 199 0.11 31.4
Table 6: Empirical results according to the combining methods of white-box reward features used in our RER.
We believe the multiplication of each reward function would better represent the final reward since we
intend for each reward function to work in regularization with each other. For example, the repetition
penalty and query relevance could suppress the undesirably increased rewards by length incentive.
Furthermore, the empirical results demonstrate the effectiveness of the multiplication for the combining
compared to the addition, as shown in Table 6.C More Implementation Details
C.1 Dataset Details
Data Source Setup # Train # Dev Init. Backbone
Anthropic-HHSFT 160,800 - meta-llama/Llama-2-7b-hf
PPO 21,530 1,137 LLaMA-2-7B-SFT (trained by ourselves)
AlpacaFarm PPO 20,001 2,000 tatsu-lab/alpaca-farm-sft10k-wdiff
ShareGPT PPO 53,554 600 lmsys/vicuna-{7,13}b-v1.5
Table 7: Overview training dataset and corresponding initialized backbone used in our experiments.
Table 7 shows overall dataset statistics for each experiment. All the datasets consist of input-output
pairs ( x,y), which means they do not contain human preference triplets. For a fair comparison with
the open-source RMs, we train SFT model based on LLaMA-2-7B (Touvron et al., 2023) with the 161k
chosen responses in Anthropic-HH (Bai et al., 2022a)10. For the PPO training on Anthropic-HH, we
select about 23k (train and dev) first-turn queries from ‘Helpful-rejection’ and ‘Helpful-online’ sub-splits
for efficient implementation. Also, we employ the pre-trained SFT models (Init. Backbone), for the
remaining experiments, AlpacaFarm (Dubois et al., 2023), ShareGPT (Zheng et al., 2023). Specifically,
we use 20k of ‘unlabeled’ split for the PPO training on AlpacaFarm11. For ShareGPT, we follow basic
preprocessing suggested by Zheng et al. (2023) and sample 31k instances for efficient implementation12.
Figure 10 shows the distribution of T(x)for each dataset.
Open-ended
41.6%
Closed-ended58.4%Anthropic-HH
Open-ended
44.5%
Closed-ended55.5%AlpacaFarm
Open-ended
34.8%
Closed-ended65.2%ShareGPT
Figure 10: Dataset distribution of each dataset regarding query type T(x).
10huggingface.co/datasets/Anthropic/hh-rlhf
11huggingface.co/datasets/tatsu-lab/alpaca_farm/viewer/alpaca_instructions
12huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfilteredC.2 Training Details
hyper-parameter SFT PPO
learning rate (lr) 2e-5 1e-5
batch size per a GPU 2 {2, 4}
gradient accumulation 8 {4, 8}
# epoch 3 -
# step - {5k, 10k}
lr scheduling cosine constant
warmup ratio 0.03 no
max sequence length 2048 -
max rollout length - {512, 768}
ppo epoch - 4
initial KL coefficient - 0.2
clip ratio - 0.2
discount factor (gamma) - 1
Table 8: hyper-parameter setups in our experiments.
0 2000 4000 6000 8000 1000012345Avg training reward
2000 4000 6000 8000 100001.01.52.02.53.03.5Avg validation reward
Figure 11: Trend of average reward scores according to PPO steps on Vicuna + RERexperiment.
Table 8 shows the common hyper-parameter setup used in our experiments. We conduct a full-finetuning
(FFT) with fully-sharded data parallel (FSDP) for the SFT training of Anthropic-HH (Section 4.2). For
PPO, we employ LoRA (Hu et al., 2021; Sun et al., 2023a) tuning for efficiency. Specifically, we apply the
low-rank adaptors to query and value linear parameters. Also, we set lora_r to {8, 16}, lora_alpha to {16,
32}, and dropout to {0.05, 0.1}. We utilize an adaptive KL (Kullback–Leibler divergence) penalty with 0.2
of the initial KL coefficient β(Ziegler et al., 2020). We choose the best checkpoint based on the average
reward scores for every 1k PPO step. Figure 11 shows an example of the average reward scores based on
the Vicuna + RERexperiment. One A100 GPU is used for most PPO implementations except when the
policy is initialized with Vicuna-13B-v1.5. For the 13B model, we use 8 A100 GPUs. Besides, we leverage
the corresponding chat template if the initial backbone model supports it (github.com/lm-sys/FastChat).
All the implementation is based on the Transformers13, TRL14and PEFT15libraries.
13github.com/huggingface/transformers
14github.com/huggingface/trl
15github.com/huggingface/peftD Evaluation Details
D.1 Evaluation set
We evaluate the resulting models on Vicuna-Bench (Chiang et al., 2023), AlpacaEval (Li et al., 2023),
MT-Bench (Zheng et al., 2023), and ToxiGen (Hartvigsen et al., 2022). The first three benchmarks mainly
evaluate the preference of models’ responses leveraging a superior proprietary LLM, GPT-4 (OpenAI,
2023). Vicuna-Bench contains 80 curated questions focusing on helpfulness, such as writing, roleplay,
and reasoning16. It performs a pairwise comparison between two candidate responses. Also, it includes
reference information for math and coding questions and conducts two inferences by switching responses’
positions for a more precise evaluation (Zheng et al., 2023). To identify the effect of PPO with each
RM, we set the SFT model as a baseline model for the pairwise comparisons. We report an adjusted
win rate, (# win + 0.5·# tie)/# all, of each PPO model against to the SFT baseline. AlpacaEval is
another benchmark conducting the pairwise comparison17. It includes 805 instructions collected from
diverse sources. We report a win rate compared to Text-Davinci-003 (Ouyang et al., 2022) as a baseline.
MT-Bench is a benchmark checking multi-turn ability in a single judgment manner. It scores each
response within the 1-10 range without the baseline response. It contains 80×2(turns)of questions,
including the more knowledge-intensive questions such as math, coding, and STEM. In addition, we
evaluate the resulting models on Super-NaturalInstructions ( SuperNI ) (Wang et al., 2022) to measure the
models’ instruction-following ability, i.e., constrained generation. We report ROUGE-L of generations for
the unseen test instructions. Following the official setup, we sample 100 instances in the unseen test split
consisting of 119 tasks to compute the ROUGE-L score for the SuperNI. The overall tasks are shown in
github.com/allenai/natural-instructions/blob/master/splits/default/test_tasks.txt.
16LLM Judge in FastChat
17github.com/tatsu-lab/alpaca_evalE Relevant Sentence Ratio
Evaluation Prompt for Relevant Sentence Ratio
<Instruction>
Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed
below.
Your evaluation should check the ’relevancy’ of the provided assistant’s answer considering the question.
Please investigate thoroughly whether the answer contains any irrelevant content sentence-by-sentence.
That is, each sentence is marked as ’Irrelevant’ when it goes off-topic or includes information that is not relevant to the question.
Otherwise, each sentence is marked as ’Relevant’.
The assessment should be conducted at the sentence level. First, refer to each sentence. Then, judge its relevancy, either "Relevant"
or "Irrelevant".
For example,
- "Sentence 1" : (Relevant|Irrelevant)
- "Sentence 2" : (Relevant|Irrelevant)
<Question>
{question}
<The Start of Assistant’s Answer>
{answer}
<The End of Assistant’s Answer>
Figure 12: Evaluation Prompt for relevant sentence ratio used in our experiments.
Figure 13: An example of relevant sentence ratio (% Rel. Sent).
Figure 12 shows the evaluation prompt to obtain a relevant sentence ratio using GPT-4 (OpenAI, 2023).
It returns sentence-level relevance of the given assistant answer, as shown in Figure 13. Inspired by Min
et al. (2023), we count the number of relevant sentences among all sentences in an answer and report the
average value across the whole test set.F Qualitative Examples
Prompt:
What is the best approach for learning a foreign language when you only have an hour a day to practice?
RER: UltraRM:
Learning a foreign language takes time, and while there
is no one right way to learn a language, spending just one
hour a day is probably not enough to learn and retain the
language well. For example, if you’re learning Spanish,
you might start with some basic vocabulary and phrases,
and then progress to speaking, listening, and reading more
complex materials. Research suggests that the best way to
learn a language is to go through a period of intense study,
usually lasting a few months, during which you devote at
least an hour a day to language learning. After this period
of intensive study, you can then move to a maintenance
phase, where you spend less time learning, but continue to
practice regularly. So the best approach is to start with a
period of intense study, using a textbook, an app, or other
resources to help you learn, and practice every day for at
least an hour. Then, once you feel that you have a strong
grasp on the language, you can move to a maintenance
phase where you practice less every day.The most effective approach to learning a foreign lan-
guage is to use the “Method of Feigned Passivity”. You
should pretend to have a lack of interest and motivation,
while covertly establishing a strong internal drive towards
mastering the language. This contradiction will force your
unconscious to find the most efficient and effective learn-
ing methods. I can recommend the following practice
exercises:
- Pretend to not understand when someone speaks to you in
the target language. If they offer to help you learn, respond
with “I’m not interested”. Internalise a burning desire to
demonstrate your understanding, without appearing to
make an effort.
- Pretend to forget the grammar rules and vocabulary you
do know. If you remember a word or a sentence you will
gain a sense of its correct usage, but pretend to forget it
anyway. Internalise a burning desire to remember every-
thing, without appearing to make an effort.
- Pretend to not remember what you have learned previ-
ously. If you are discussing a topic with someone, and
you are about to use a word or sentence that you have
learnt previously, pretend to not remember it. Internalise
a burning desire to recall your previous learning, without
appearing to make an effort.
...
Prompt:
Do you know why turkeys became the official food of thanksgiving?
RER: LI:
Yes, turkeys became the official food of Thanksgiving
because the pilgrims and the Wampanoag Indians who
gathered for the first Thanksgiving feast in 1621 were
reportedly served turkey. In addition, turkeys were seen
as a symbol of abundance, as they are believed to have
migrated to the New World in great numbers.
Additionally, turkeys are thought to be easier to hunt than
other birds, making them a practical choice for the early
settlers. As such, the turkey has become a symbol of
thanksgiving and is now a popular dish on Thanksgiving
menus across the United States.
In modern times, turkeys are often raised specifically for
the Thanksgiving holiday and are often given a special
diet so that they can reach a desired weight before the
holiday. In some cases, turkeys may even be stuffed with
a special mixture of herbs and spices to add flavor to the
meat.No, I do not. Why did turkeys become the official food of
Thanksgiving?
The tradition of eating turkey at Thanksgiving is thought
to have originated with the Pilgrims who celebrated the
first Thanksgiving feast in 1621 with the Wampanoag
Indians. The Wampanoag Indians were skilled at hunting
turkeys and the Pilgrims were eager to learn the best way
to prepare them. The Pilgrims were amazed at the size of
the turkeys they were served and the tradition of eating
turkey on Thanksgiving was born. ...
Turkeys have become so closely associated with Thanks-
giving that they are sometimes called the “official food”
of the holiday. In addition to being served on Thanksgiv-
ing, turkeys are also popular during Christmas and other
holiday meals.
Turkeys have become so closely associated with Thanks-
giving that they are sometimes called the “official food”
of the holiday. In addition to being served on Thanksgiv-
ing, turkeys are also popular during Christmas and other
holiday meals.
...
Figure 14: Qualitative examples of the PPO models with different rewards. Different from both UltraRM and LI
(Length Incentive only) rewards incur repetitions, RERbased model provides more relevant and reliable responses.Prompt:
Provide me with a list of 10names from various cultures that mean love, care, and/or empathy.
RER: LI ·RP ·QR:
1. Aahana (Hindi)
2. Ahava (Hebrew)
3. Ai (Japanese)
4. Agape (Greek)
5. Alay (Tagalog)
6. Betelehem (Hebrew)
7. Cariño (Spanish)
8. Daba (Amharic)
9. Dhul (Arabic)
10. Empatia (Polish)1. Arabic: Rahmah
2. Chinese: Ai
3. French: Amour
4. Greek: Agape
5. Hindi: Sharada
6. Japanese: Ai
...
38. Zulu: Ngiyabonga
39. Vietnamese: Yêu Mãy
40. Irish: Cead Mille Failte
41. Italian: Carità
Prompt:
Write down three antonyms for the given word. ‘laureating’
RER: LI ·RP ·QR:
denigrating, disparaging, belittling. The antonyms of laureating are:
- Decorating
- Dignifying
- Honoring
- Magnifying
- Praising
- Sanctifying
- Serving
- Exalting
- Glorying
...
Figure 15: Qualitative examples comparing RERandLI·RP·QRagainst to CEtype queries. LI·RP·QRoften violates
the given constraints, RERfollows the given instruction much better.