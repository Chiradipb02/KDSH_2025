Can Automatic Metrics Assess High-Quality Translations?
Sweta Agrawal1*, António Farinhas1,2∗, Ricardo Rei3, André F.T. Martins1,2,3,4
1Instituto de Telecomunicações,2Instituto Superior Técnico, Universidade de Lisboa
3Unbabel,4ELLIS Unit Lisbon
swetaagrawal20@gmail.com, antonio.farinhas@tecnico.ulisboa.pt
Abstract
Automatic metrics for evaluating translation
quality are typically validated by measuring
how well they correlate with human assess-
ments. However, correlation methods tend to
capture only the ability of metrics to differen-
tiate between good and bad source-translation
pairs, overlooking their reliability in distin-
guishing alternative translations for the same
source. In this paper, we confirm that this is
indeed the case by showing that current met-
rics are insensitive to nuanced differences in
translation quality. This effect is most pro-
nounced when the quality is high and the vari-
ance among alternatives is low. Given this find-
ing, we shift towards detecting high-quality cor-
rect translations, an important problem in prac-
tical decision-making scenarios where a binary
check of correctness is prioritized over a nu-
anced evaluation of quality. Using the MQM
framework as the gold standard, we systemati-
cally stress-test the ability of current metrics to
identify translations with no errors as marked
by humans. Our findings reveal that current
metrics often over or underestimate translation
quality, indicating significant room for improve-
ment in machine translation evaluation.
1 Introduction
The automatic evaluation of machine or human-
generated translations has gained widespread atten-
tion over the past few years. Evaluation metrics
act as proxies for translation quality in the absence
of human judgments, offering immediate feedback.
They are widely used not only to provide quality
indicators to users and translators (Béchara et al.,
2021; Castilho and O’Brien, 2017; Mehandru et al.,
2023a), but also to improve machine translation
(MT) systems themselves (He et al., 2024; Xu et al.,
2024a; Fernandes et al., 2022).
Judging whether, and to what extent, these met-
rics concur with human evaluation is important
*Equal contribution.LP N % ZERO -MQM
WMT 2023 METRICS DATASET
EN-DE(P) 5520 25 .4%
HE-EN 9840 50 .8%
ZH-EN 17655 19 .1%
WMT 2022 METRICS DATASET
EN-DE 18410 51 .5%
EN-RU 19725 42 .7%
ZH-EN 26250 46 .4%
WMT 2022 CHAT DATASET
XX-E N 4756 63 .2%
EN-XX 5901 60 .2%
Table 1: Gold MQM scores distribution in recent WMT
datasets. High-quality translations are represented in
shades of green (darker for MQM = 0and lighter for
MQM ≥ −5); red represents translations with at least
one major error (MQM ≤ −5). P: paragraph-level.
to ensuring their effectiveness and applicability
in diverse scenarios. A recent human evaluation
study by the Conference on Machine Translation
(WMT) revealed that translations produced by cur-
rent MT systems often achieve very high-quality
scores (ranging from 80to90) when judged by
humans on a direct assessment (DA) scale of 0to
100(Kocmi et al., 2023). Similarly, Deutsch et al.
(2023) observe that these systems increasingly gen-
erate numerous “perfect” translations (translations
with zero errors), especially for high-resource lan-
guage pairs, as shown in Table 1. As MT quality
advances, evaluating whether evaluation metrics
accurately reflect this progress is essential (Bur-
chardt et al., 2016). The absence of clear criteria
for assessing these high-quality translations can in-
troduce bias, leading to inconsistent assessments
based on metric preferences rather than objective
measures of accuracy.
Most evaluations of automatic metrics primarily
assess their ability to distinguish between good and
bad source-translation pairs (Freitag et al., 2023,arXiv:2405.18348v2  [cs.CL]  10 Oct 20242022b), often overlooking their capacity to discern
subtle differences in quality for a given source. Fur-
thermore, in many practical and high-risk applica-
tions ( e.g., within the medical or legal domains),
the main concern is not measuring the accuracy
level of a translation but determining whether the
translation is accurate and fit for that specific use
(Nida, 1964; Church and Hovy, 1993; Bowker,
2019; Vieira et al., 2021; Mehandru et al., 2023b).
While correlations provide valuable insights into
the performance of automatic metrics, they do not
offer a definitive measure of whether these metrics
can reliably confirm translation accuracy.
Hence, in this work, we systematically investi-
gate how existing MT metrics assess high-quality
(HQ) correct translations, defined as translations
with zero or minor errors only. We find that au-
tomatic metrics struggle to distinguish between
translations for a given source, especially when
comparing HQ translations, with reference-free or
quality estimation (QE) metrics achieving close
correlation scores to reference-based ones. We
further show that current metrics severely overes-
timate (for non-HQ translations) or underestimate
(for HQ translations) translation quality. GEMBA-
MQM (Kocmi and Federmann, 2023), a GPT-based
QE metric, achieves the highest F1 score in detect-
ing the HQ translations with no errors ( HQ-Z ERO).
However, it also assigns high scores to erroneous
GPT-4 translations, suggesting a preferential bias
towards the LLM’s own outputs. These findings
highlight the necessity for more robust evaluation
protocols to assess the quality of automatic metrics.
2 How good are current MT systems?
The most reliable way to assess translation qual-
ity has been through human evaluations, with
several frameworks proposed over the years for
this purpose. While earlier works consider two
dimensions—adequacy and fluency—with a 5-
point Likert scale (King, 1996), subsequent work
on direct assessments (DA) considers a single con-
tinuous scale of 0−100(Graham et al., 2017).
However, several studies have questioned the cred-
ibility of DA-based evaluation (Toral et al., 2018;
Läubli et al., 2020; Fischer and Läubli, 2020;
Mathur et al., 2020b; Freitag et al., 2021).
Unlike DAs, which assign a numeric score to
a translation, the recent Multidimensional Quality
Metrics (Burchardt, 2013, MQM) framework relies
on explicit error judgments (error types and sever-ities) marked within specific spans of the source-
translation pair, providing a more accurate and fine-
grained evaluation. Translations receive a score
of0if they contain no errors, a penalty of −1for
minor errors, and −5for major errors that impact
the usage or understanding of the content.1
We present the distribution of gold MQM scores
from the WMT23 Metrics task (Freitag et al., 2023),
WMT22 Metrics task (Freitag et al., 2022b), and
WMT22 Chat Translation task (Farinha et al., 2022)
in Table 1. Across settings and language pairs, the
percentage of translations achieving a zero MQM
score ranges from 19.1% to63.2%. At least 52.6%
of the translations across language pairs and set-
tings have no major errors (MQM >−5). Thus,
a large percentage of the datasets include transla-
tions with no or only minor errors, emphasizing
the importance of accurately identifying these high-
quality translations in the evaluation process.
3 How well do MT metrics assess HQ
translations?
We define HQ translations as those that achieve an
MQM score >−5,i.e., translations without any
major errors according to human evaluators. By
definition, these translations do not contain errors
that impede their comprehension or usability. We
consider a subset of QE and reference-based auto-
matic metrics evaluated by the shared tasks (see
App. A for more details).
3.1 How do metrics rank HQ translations?
We first investigate how automatic metrics rank HQ
translations, which is particularly relevant today, as
these metrics are often used to guide MT training
or decoding processes. Recent work employs both
reference-based and QE metrics to rerank multiple
hypotheses generated by dedicated MT models or
large language models (LLMs), aiming to improve
translation quality (Fernandes et al., 2022; Freitag
et al., 2022a; Farinhas et al., 2023, 2024). These
metrics are also used to provide quality feedback
signals during training, either explicitly in loss sig-
nals (Ramos et al., 2024; Yan et al., 2023; He et al.,
2024) or implicitly via the creation of preference
datasets (Xu et al.; Yang et al., 2024).
Consider Nsystems and Msource segments.
Typically, segment-level correlations are computed
between the N×Mtranslations. However, this
1Although the MQM framework includes critical errors—
errors that could render a text unusable—they are not marked
in many datasets due to their highly contextual interpretation.No-grouping:  𝜌𝜌𝐴𝐴,𝐻𝐻, 𝐴𝐴,𝐻𝐻∈𝑅𝑅𝑑𝑑,𝑑𝑑=𝑁𝑁×𝑀𝑀
Group -by-src:1
𝐾𝐾∑𝐾𝐾𝜌𝜌𝐴𝐴′,𝐻𝐻′, 𝐴𝐴𝐴,𝐻𝐻𝐴∈𝑅𝑅𝑁𝑁All: 𝑁𝑁×𝑀𝑀 HQ:  𝑁𝑁×𝐾𝐾 All†:𝑁𝑁×𝐾𝐾
Given, 
A: automatic metric scoresH: human assessment scoresfor N systems and M segments Figure 1: Ranking analysis configurations. ρ: Spearman correlation.
differs from the practical setting where metrics are
used to rerank several translations for the same
source. Therefore, we follow Deutsch et al. (2023)
and compute the average correlation between the N
translation scores grouped by the source sentences.
We refer to the former setting as NO-GROUPING
and the latter as GROUP -BY-SRC. We also study
to what extent these metrics distinguish between
HQ translations. As the number of segments with
all HQ translations, K, is less than M, we report
mean correlations on subsampled datasets (ran-
domly sampled 10times) that match the sample
size,N×K, marked with the symbol †in Table 2.
This is motivated by Mathur et al. (2020a), who
study how these metrics rank HQ systems , where
a limited number of samples (typically 4 or 5) was
shown to yield unreliable conclusions. However,
our focus is on segment-level evaluation, where
the number of subsampled items is much larger.
Figure 1 summarizes all configurations and the cor-
responding correlation measures.
Table 2 presents Spearman correlation of auto-
matic metrics with MQM scores for configurations
described above on the WMT23 EN-DE dataset
(see App. B for other datasets and correlation met-
rics). We first note that the correlation observed
on the entire ( NO-GROUPING ALL) and the sub-
sampled datasets ( NO-GROUPING ALL†) is close,
establishing that the observed differences cannot
be merely attributed to changes in sample size.
Metrics exhibit only a low-to-fair correlation
with human judgments when evaluating trans-
lations for the same source. Automatic metrics
are less effective in differentiating between goodNO-GROUPING GROUP -BY-SRC
METRICALL ALL†ALL†HQREF-BASEDchrF 0.262 0 .227 0 .267 0 .136
BLEU 0.193 0 .190 0 .303 0 .146
BERTscore 0.355 0 .367 0 .325 0 .134
COMET 0.578 0 .584 0 .461 0 .202
BLEURT-20 0.618 0 .603 0 .449 0 .220
XCOMET-XL 0.713 0 .705 0 .461 0 .250
XCOMET-XXL 0.708 0 .716 0 .481 0 .326
MetricX-23 0.682 0 .680 0 .450 0 .301
MaTESe 0.591 0 .593 0 .341 0 .254REF-FREEGEMBA-MQM 0.614 0 .621 0 .462 0 .368
CometKiwi 0.565 0 .561 0 .411 0 .182
CometKiwi-XL 0.542 0 .550 0 .427 0 .223
CometKiwi-XXL 0.525 0 .504 0 .456 0 .327
MetricX-23-QE 0.683 0 .681 0 .470 0 .292
Table 2: Spearman correlation on WMT23 EN-DE. †:
Subsampled to match G ROUP -BY-SRCHQ’s size.
and bad translations for the same source, as evi-
denced by the drop in correlation from the NO-
GROUPING ALL†to the GROUP -BY-SRCALL†
setting. A possible reason for this disparity lies
in how these metrics are typically trained—most
metrics are trained to predict translation quality for
a given instance ( e.g., source-reference-hypothesis
trio in Comet orxCOMET ). While useful for rank-
ing two systems based on averaged scores across
texts, they may provide limited information for
gauging translation quality for different translations
of the same source.2Interestingly, BLEU ’s correla-
tion is higher in the GROUP -BY-SRCsetting than
inNO-GROUPING , likely due to its original use for
2Using contrastive objectives or exposing the metric to
multiple translations could potentially help mitigate this issue
(Briakou and Carpuat, 2020).EN-DE ( 1402) HE-EN ( 5001) ZH-EN ( 11309 )METRICP R F1 P R F1 P R F1
xCOMET-XL 72 40 51 78 17 28 47 28 35
xCOMET-XXL 58 59 58 74 54 62 36 63 46
MaTESe 49 69 58 66 65 65 29 75 42
MetricX-23 70 33 45 80 16 27 52 11 19
GEMBA-MQM 52 70 60 71 65 68 37 77 50
MetricX-23-QE 66 14 23 70 64 67 55 20 29
Figure 2: Top: Metric Scores distribution for HQ- ZERO translations on WMT23. Bottom: Precision, recall, and F1.
comparing multiple translations of the same source.
This underscores the limitations of using automatic
metrics as the sole measure of quality beyond their
intended use cases, particularly in scenarios where
fine-grained distinctions between translations of
the same source are required.
QE metrics are on par with reference-based ones
for differentiating translations. QE metrics
show promising results in differentiating transla-
tions for the same source, often achieving com-
parable or better correlation than reference-based
metrics. For EN-DE, the QE metrics MetricX-23-
QEandGEMBA-MQM rank second and third, re-
spectively in the ALL setting, following xCOMET-
XXL. When contrasting HQ translations, GEMBA-
MQM outperforms all other metrics. The relatively
strong performance of QE metrics, particularly in
this setting, highlights their potential as valuable
tools for translation generation and ranking tasks.
Metrics fail to distinguish HQ translations.
There is a consistent drop in correlation scores
across all metrics in the HQ relative to the ALL set-
ting, possibly because most translations in the HQ
setting receive scores in the narrow range of (−5,0]
and are often tied in quality. Deutsch et al. (2023)
show that most metrics struggle to predict transla-
tion ties accurately, i.e., to give the same scoreto two translations with similar quality, except
for error-predicting metrics like GEMBA-MQM
orMaTESe . This decreased correlation from the
HQ to the ALL setting has significant implications,
especially when they are used to rerank translations
produced by strong MT systems. It may result in
an artificial boost or bias towards specific systems
or outputs, inadvertently prioritizing translations
that align well with metric biases but deviate from
true quality improvements, as discussed in §3.3.
3.2 How well do metrics detect HQ
translations with no errors?
Ranking translations of similar quality is a difficult
task, so we also evaluate how automatic metrics
score HQ translations with zero MQM scores. (HQ-
ZERO). We consider normalized scores ≥0.99as
valid scores as 1.0is the highest score a metric
should assign to HQ- ZERO translations. Fig. 2
shows the results on WMT23 dataset. See App. C
for results in other datasets.
Metric scores have high variance for HQ trans-
lations. 9out of 15metrics do not assign valid
scores to HQ- ZERO translations. Lexical metrics
(chrF andBLEU ) produce the lowest absolute val-
ues, possibly due to over-reliance on a reference
translation. Neural metrics trained to regress on
DA scores ( BLEURT ,COMET , and variants) alsoFigure 3: Absolute difference of the number of times
a metric assigns a valid score to HQ- ZERO and non
HQ-Z ERO translations.
do not assign valid scores for these translations,
likely due to low agreement between DA and MQM
scores, as discussed by Freitag et al. (2021).
Metrics over or underestimate translation qual-
ity. Metrics that do score these translations within
the valid range ( xCOMET ,MaTESe ,MetricX ,
andGEMBA-MQM ), exhibit different tradeoffs be-
tween precision (P) and recall (R). For example,
while xCOMET-XL andMetricX prioritize preci-
sion, MaTESe andGEMBA-MQM excel at recog-
nizing many HQ- ZERO translations, leading to in-
creased recall. This difference might stem from the
specific task each metric is optimized for: while the
first two predict sentence-level quality, the last two
are optimized to predict word-level error spans. As
expected, xCOMET-XXL significantly outperforms
xCOMET-XL across all language pairs. Finally,
the QE metric, GEMBA-MQM , based on GPT-4,
achieves the highest F1 score across all language
pairs, demonstrating the capabilities of LLM-based
evaluation in more nuanced MT evaluation.
3.3 Which HQ translations are detected?
To study preference bias from metrics towards spe-
cific systems, we compute the absolute difference
in the number of times a metric assigns a valid
score to HQ- ZERO and non-HQ- ZERO translations.
Fig. 3 shows that MaTESe equally overestimates
translation quality for many systems, as suggestedby its high R and low P scores. GEMBA-MQM
frequently assigns zero MQM scores to GPT-4
translations, even when humans identify errors in
them. This aligns with concurrent works showing
a preference bias of LLMs towards their outputs
(Panickssery et al., 2024; Xu et al., 2024b), under-
scoring the need for a more detailed evaluation to
better understand the outputs these metrics prefer
and whether they align with human preferences.
4 Conclusions and Future Work
This work systematically investigates how auto-
matic metrics assess HQ translations. We find that
current metrics correlate poorly with human judg-
ments when contrasting translations for a given
source, with the correlation being even lower for
HQ translations. We then study whether metrics
can detect HQ translations that attain zero MQM
scores (HQ- ZERO) and find that many metrics fail
to assign them valid scores. While the GPT-4-based
GEMBA-MQM attains the highest F1 for detecting
HQ-ZERO, it shows some preference for GPT-4
outputs. Therefore, despite its promise, it is es-
sential to complement GEMBA-MQM with other
metrics to ensure robust evaluation.
Acknowledgments
We thank Ben Peters, Gonçalo Faria, and Eleft-
heria Briakou for their constructive feedback on
the paper. This work was supported by EU’s Hori-
zon Europe Research and Innovation Actions (UT-
TER, contract 101070631), by the project DECOL-
LAGE (ERC-2022-CoG 101088763), by the Por-
tuguese Recovery and Resilience Plan through
project C645008882-00000055 (Center for Respon-
sible AI), and by Fundação para a Ciência e Tec-
nologia through contract UIDB/50008/2020.
Limitations
We highlight the main limitations of our work.
First, we rely on human MQM annotations as the
gold standard for identifying high-quality transla-
tions, despite their potential subjectivity and occa-
sional inaccuracy. These annotations are collected
for individual translations, and the ratings might
vary if annotators were asked to evaluate and com-
pare multiple translations simultaneously. Further-
more, MQM annotations used in our analysis are
very expensive to obtain as they require trained lin-
guists to perform the assessments, which limits the
analysis to publicly available datasets.Second, although our analysis spans multiple
datasets across six language pairs ( EN-DE,ZH-EN,
HE-EN,EN-RU,EN-FR, and EN-PT-BR) and mul-
tiple domains, we do not necessarily account for
the distribution of high-quality translations across
different domains within a dataset. As shown by
Zouhar et al. (2024), learned metrics can be sensi-
tive to the domain of evaluation.
Lastly, our analysis in §3.3 identifies one poten-
tial bias, but it remains unclear whether automatic
metrics have preferential biases towards other out-
put properties such as length, stylistic choices, etc.
References
Sweta Agrawal, Amin Farajian, Patrick Fernandes, Ri-
cardo Rei, and André FT Martins. 2024. Is con-
text helpful for chat translation evaluation? arXiv
preprint arXiv:2403.08314 .
Lynne Bowker. 2019. Fit-for-purpose translation. In
The Routledge handbook of translation and technol-
ogy, pages 453–468. Routledge.
Eleftheria Briakou and Marine Carpuat. 2020. De-
tecting Fine-Grained Cross-Lingual Semantic Diver-
gences without Supervision by Learning to Rank. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 1563–1580, Online. Association for Computa-
tional Linguistics.
Aljoscha Burchardt. 2013. Multidimensional quality
metrics: a flexible system for assessing translation
quality. In Proceedings of Translating and the Com-
puter 35 , London, UK. Aslib.
Aljoscha Burchardt, Kim Harris, Georg Rehm, and Hans
Uszkoreit. 2016. Towards a systematic and human-
informed paradigm for high-quality machine trans-
lation. Translation Evaluation: From Fragmented
Tools and Data Sets to an Integrated Ecosystem ,
page 35.
Hannah Béchara, Constantin Or ˘asan, Carla Parra Es-
cartín, Marcos Zampieri, and William Lowe. 2021.
The role of machine translation quality estimation in
the post-editing workflow. Informatics , 8(3).
Sheila Castilho and Sharon O’Brien. 2017. Acceptabil-
ity of machine-translated content: A multi-language
evaluation by translators and end-users. Linguistica
Antverpiensia, New Series–Themes in Translation
Studies , 16.
Kenneth W Church and Eduard H Hovy. 1993. Good ap-
plications for crummy machine translation. Machine
Translation , 8:239–258.
Daniel Deutsch, George Foster, and Markus Freitag.
2023. Ties matter: Meta-evaluating modern metricswith pairwise accuracy and tie calibration. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 12914–
12929, Singapore. Association for Computational
Linguistics.
Ana C Farinha, M. Amin Farajian, Marianna Buchic-
chio, Patrick Fernandes, José G. C. de Souza, He-
lena Moniz, and André F. T. Martins. 2022. Find-
ings of the WMT 2022 shared task on chat transla-
tion. In Proceedings of the Seventh Conference on
Machine Translation (WMT) , pages 724–743, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
António Farinhas, José de Souza, and Andre Martins.
2023. An empirical study of translation hypothesis
ensembling with large language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 11956–11970,
Singapore. Association for Computational Linguis-
tics.
António Farinhas, Haau-Sing Li, and André F. T. Mar-
tins. 2024. Reranking laws for language generation:
A communication-theoretic perspective.
Patrick Fernandes, António Farinhas, Ricardo Rei,
José G. C. de Souza, Perez Ogayo, Graham Neubig,
and Andre Martins. 2022. Quality-aware decoding
for neural machine translation. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1396–1412,
Seattle, United States. Association for Computational
Linguistics.
Lukas Fischer and Samuel Läubli. 2020. What’s the
difference between professional human and machine
translation? a blind multi-language study on domain-
specific MT. In Proceedings of the 22nd Annual
Conference of the European Association for Machine
Translation , pages 215–224, Lisboa, Portugal. Euro-
pean Association for Machine Translation.
Markus Freitag, George Foster, David Grangier, Viresh
Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021.
Experts, errors, and context: A large-scale study of
human evaluation for machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 9:1460–1474.
Markus Freitag, David Grangier, Qijun Tan, and Bowen
Liang. 2022a. High quality rather than high model
probability: Minimum Bayes risk decoding with neu-
ral metrics. Transactions of the Association for Com-
putational Linguistics , 10:811–825.
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Elefthe-
rios Avramidis, Ricardo Rei, Brian Thompson, Tom
Kocmi, Frederic Blain, Daniel Deutsch, Craig Stew-
art, Chrysoula Zerva, Sheila Castilho, Alon Lavie,
and George Foster. 2023. Results of WMT23 metrics
shared task: Metrics might be guilty but referencesare not innocent. In Proceedings of the Eighth Con-
ference on Machine Translation , pages 578–628, Sin-
gapore. Association for Computational Linguistics.
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
Craig Stewart, Eleftherios Avramidis, Tom Kocmi,
George Foster, Alon Lavie, and André F. T. Martins.
2022b. Results of WMT22 metrics shared task: Stop
using BLEU – neural metrics are better and more
robust. In Proceedings of the Seventh Conference
on Machine Translation (WMT) , pages 46–68, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
Yvette Graham, Timothy Baldwin, Alistair Moffat, and
Justin Zobel. 2017. Can machine translation systems
be evaluated by the crowd alone. Natural Language
Engineering , 23(1):3–30.
Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa
Coheur, Pierre Colombo, and André FT Martins.
2024. xcomet: Transparent machine translation eval-
uation through fine-grained error detection. Transac-
tions of the Association for Computational Linguis-
tics, 12:979–995.
Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng
Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu.
2024. Improving machine translation with human
feedback: An exploration of quality estimation as a
reward model. In Proceedings of the 2024 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (Volume 1: Long Papers) , pages
8164–8180, Mexico City, Mexico. Association for
Computational Linguistics.
Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya
Siddhant, Mehdi Mirzazadeh, and Markus Freitag.
2023. MetricX-23: The Google submission to the
WMT 2023 metrics shared task. In Proceedings
of the Eighth Conference on Machine Translation ,
pages 756–767, Singapore. Association for Compu-
tational Linguistics.
Margaret King. 1996. Evaluating natural language
processing systems. Communications of the ACM ,
39(1):73–79.
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden,
Ondˇrej Bojar, Anton Dvorkovich, Christian Fed-
ermann, Mark Fishel, Markus Freitag, Thamme
Gowda, Roman Grundkiewicz, Barry Haddow,
Philipp Koehn, Benjamin Marie, Christof Monz,
Makoto Morishita, Kenton Murray, Makoto Nagata,
Toshiaki Nakazawa, Martin Popel, Maja Popovi ´c,
and Mariya Shmatova. 2023. Findings of the 2023
conference on machine translation (WMT23): LLMs
are here but not quite there yet. In Proceedings of the
Eighth Conference on Machine Translation , pages
1–42, Singapore. Association for Computational Lin-
guistics.
Tom Kocmi and Christian Federmann. 2023. GEMBA-
MQM: Detecting translation quality error spans withGPT-4. In Proceedings of the Eighth Conference
on Machine Translation , pages 768–775, Singapore.
Association for Computational Linguistics.
Samuel Läubli, Sheila Castilho, Graham Neubig, Rico
Sennrich, Qinlan Shen, and Antonio Toral. 2020.
A set of recommendations for assessing human–
machine parity in language translation. Journal of
artificial intelligence research , 67:653–672.
Nitika Mathur, Timothy Baldwin, and Trevor Cohn.
2020a. Tangled up in BLEU: Reevaluating the eval-
uation of automatic machine translation evaluation
metrics. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4984–4997, Online. Association for Computa-
tional Linguistics.
Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong
Ma, and Ond ˇrej Bojar. 2020b. Results of the WMT20
metrics shared task. In Proceedings of the Fifth Con-
ference on Machine Translation , pages 688–725, On-
line. Association for Computational Linguistics.
Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao,
Elaine Khoong, Marine Carpuat, and Niloufar Salehi.
2023a. Physician detection of clinical harm in ma-
chine translation: Quality estimation aids in reliance
and backtranslation identifies critical errors. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 11633–
11647, Singapore. Association for Computational
Linguistics.
Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao,
Elaine Khoong, Marine Carpuat, and Niloufar Salehi.
2023b. Physician detection of clinical harm in ma-
chine translation: Quality estimation aids in reliance
and backtranslation identifies critical errors. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 11633–
11647.
Eugene Albert Nida. 1964. Toward a science of trans-
lating: with special reference to principles and pro-
cedures involved in Bible translating . Brill Archive.
Arjun Panickssery, Samuel R Bowman, and Shi Feng.
2024. Llm evaluators recognize and favor their own
generations. arXiv preprint arXiv:2404.13076 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Stefano Perrella, Lorenzo Proietti, Alessandro Scirè,
Niccolò Campolungo, and Roberto Navigli. 2022.
MaTESe: Machine translation evaluation as a se-
quence tagging problem. In Proceedings of the Sev-
enth Conference on Machine Translation (WMT) ,
pages 569–577, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.Maja Popovi ´c. 2015. chrF: character n-gram F-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation ,
pages 392–395, Lisbon, Portugal. Association for
Computational Linguistics.
Miguel Ramos, Patrick Fernandes, António Farinhas,
and Andre Martins. 2024. Aligning neural machine
translation models: Human feedback in training and
inference. In Proceedings of the 25th Annual Con-
ference of the European Association for Machine
Translation (Volume 1) , pages 258–274, Sheffield,
UK. European Association for Machine Translation
(EAMT).
Ricardo Rei, José G. C. de Souza, Duarte Alves,
Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,
Alon Lavie, Luisa Coheur, and André F. T. Martins.
2022a. COMET-22: Unbabel-IST 2022 submission
for the metrics shared task. In Proceedings of the
Seventh Conference on Machine Translation (WMT) ,
pages 578–585, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Ricardo Rei, Nuno M Guerreiro, Daan van Stigt, Marcos
Treviso, Luísa Coheur, José GC de Souza, André FT
Martins, et al. 2023. Scaling up cometkiwi: Unbabel-
ist 2023 submission for the quality estimation shared
task. In Proceedings of the Eighth Conference on
Machine Translation , pages 841–848.
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro,
Chrysoula Zerva, Ana C Farinha, Christine Maroti,
José G. C. de Souza, Taisiya Glushkova, Duarte
Alves, Luisa Coheur, Alon Lavie, and André F. T.
Martins. 2022b. CometKiwi: IST-unbabel 2022 sub-
mission for the quality estimation shared task. In
Proceedings of the Seventh Conference on Machine
Translation (WMT) , pages 634–645, Abu Dhabi,
United Arab Emirates (Hybrid). Association for Com-
putational Linguistics.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.
BLEURT: Learning robust metrics for text genera-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
7881–7892, Online. Association for Computational
Linguistics.
Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way.
2018. Attaining the unattainable? reassessing claims
of human parity in neural machine translation. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Research Papers , pages 113–123, Brussels,
Belgium. Association for Computational Linguistics.
Lucas Nunes Vieira, Minako O’Hagan, and Carol
O’Sullivan. 2021. Understanding the societal im-
pacts of machine translation: a critical review of the
literature on medical and legal use cases. Informa-
tion, Communication & Society , 24(11):1515–1532.
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-
san Awadalla. 2024a. A paradigm shift in machine
translation: Boosting translation performance oflarge language models. In The Twelfth International
Conference on Learning Representations .
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,
Lingfeng Shen, Benjamin Van Durme, Kenton Mur-
ray, and Young Jin Kim. Contrastive preference opti-
mization: Pushing the boundaries of llm performance
in machine translation. In Forty-first International
Conference on Machine Learning .
Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming
Pan, Lei Li, and William Wang. 2024b. Pride and
prejudice: LLM amplifies self-bias in self-refinement.
InProceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 15474–15492, Bangkok, Thai-
land. Association for Computational Linguistics.
Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang,
Jiajun Chen, and Mingxuan Wang. 2023. BLEURT
has universal translations: An analysis of automatic
metrics by minimum risk training. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 5428–5443, Toronto, Canada. Association for
Computational Linguistics.
Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill
Byrne. 2024. Direct preference optimization for neu-
ral machine translation with minimum Bayes risk
decoding. In Proceedings of the 2024 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (Volume 2: Short Papers) , pages 391–398,
Mexico City, Mexico. Association for Computational
Linguistics.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .
Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana
Badeka, Jenyuan Wang, and Brian Thompson. 2024.
Fine-tuned machine translation metrics struggle in
unseen domains. In Proceedings of the 62nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 488–500,
Bangkok, Thailand. Association for Computational
Linguistics.A Automatic Metrics
We present details about all automatic metrics used across different datasets in Table 3. We refer the
reader to the relevant papers (Freitag et al., 2022b, 2023; Agrawal et al., 2024) for more details.
We used the datasets and scores from the WMT 2022 and WMT 2023 Metrics Shared Task cam-
paign, which are available at https://github.com/google-research/mt-metrics-eval under the
Apache License Version 2.0. For WMT 2022 Chat Shared task human assessments, we used human as-
sessments from https://github.com/WMT-Chat-task/data-and-baselines/tree/main/data/
mqm-annotations , released under a CC-BY-NC license. In our work, we ensured that our usage
was consistent with their intended purposes as specified by the licenses.METRIC PAPER INPUT OUTPUT TYPE EVALUATION DATASET BASEMODEL
chrF Popovi ´c (2015) { REF,MT} [0-100] ∈RLEXICAL WMT22, WMT23 - -
BLEU Papineni et al. (2002) { REF,MT} [0-100] ∈RLEXICAL WMT22, WMT23 - -
BertScore Zhang et al. (2020) { REF,MT} [0-1] ∈R EMBEDDING WMT22, WMT23 - bert-base-multilingual-cased
COMET Rei et al. (2022a) { SRC,REF,MT} [0-1] ∈R LEARNED WMT23 DA (WMT 2017-2020) + MLQE-
PExlm-roberta-large
BLEURT -20-20 Sellam et al. (2020) { REF,MT} [0-1] ∈R LEARNED WMT22, WMT23 DA (WMT 2015-2020) + Syn-
theticrembert
COMET-22 ∗Rei et al. (2022a) { SRC,REF,MT} [0-1] ∈R LEARNED WMT22 DA (WMT 2017-2020) + MLQE-
PE + MQMxlm-roberta-large,
infoxlm-large
MetricX -22 - { REF,MT} [-25,0], R LEARNED WMT22 - 30B mT5
MetricX -23 Juraska et al. (2023) { REF,MT} [-25,0] ∈RLEARNED WMT23 DA (WMT 2015-2020) + MQM
(WMT 2020-2021) + SyntheticmT5-XXL
xCOMET ∗Guerreiro et al. (2024) { SRC,REF,MT} [0-1] ∈R LEARNED WMT23 DA (WMT 2017-2020) + MLQE-
PE + MQM (WMT 2020-2021;
IndicMT, DEMETR) + SyntheticXLM-RoBERTa-XL,
XLM-RoBERTa-XXL
MaTESe Perrella et al. (2022) { REF,MT} [-25,0] ∈Z textscLearned WMT22 MQM (WMT 2020-2021) XLM-RoBERTa, BART
MaTESe - { REF,MT} [-25,0] ∈Z textscLearned WMT23 MQM (WMT 2020-2022) DeBERTa, InfoXLM
GEMBA-MQM Kocmi and Federmann (2023) { SRC,MT} [-25,0] ∈ZLLM-based WMT23 - GPT4
CometKiwi -22∗Rei et al. (2022b) { SRC,MT} [0-1] ∈R LEARNED WMT22 DA (WMT 2017-2020) + MLQE-
PE + MQM†rembert, infoxlm-large
CometKiwi -23∗Rei et al. (2023) { SRC,MT} [0-1] ∈R LEARNED WMT23 DA (WMT 2017-2020) + MLQE-
PE + MQM†rembert, infoxlm-large
MetricX -23-QE Juraska et al. (2023) { SRC,MT} [-25,0] ∈RLEARNED WMT23 DA (WMT 2015-2020) + MQM
(WMT 2020-2021) + SyntheticmT5-XXL
MaTESe -QE Perrella et al. (2022) { SRC,MT} [-25,0] ∈Z textscLearned WMT22 MQM (WMT 2020-2021) XLM-RoBERTa, BART
xCOMET-QE ∗Guerreiro et al. (2024) { SRC,MT} [0-1] ∈R LEARNED WMT23 DA (WMT 2017-2020) + MLQE-
PE + MQM (WMT 2020-2021;
IndicMT, DEMETR) + SyntheticXLM-RoBERTa-XL,
XLM-RoBERTa-XXL
Table 3: Details about the automatic metrics considered in our paper. ∗: submission is an ensemble;†: {SRC,REF}
pairs are also added to the training data.B Ranking results
Tables 4 and 5 report the Spearman and Pearson correlation results for WMT23 EN-DE, respectively.
Tables 6 and 7 show the Spearman Correlation for the WMT22 and WMT23 datasets, respectively. We do
not perform this analysis on chat data because the number of systems is ≤5.
NO-GROUPING NO-GROUPING † GROUP -BY-SRC
METRICALL HQ ∆ ALL HQ ∆ ALL†HQ ∆
chrF 0.262 0 .137 −0.124 0 .227±0.0300.132±0.022−0.094 0 .267±0.0500.136−0.131
BLEU 0.193 0 .094 −0.099 0 .190±0.0320.087±0.022−0.103 0 .303±0.0560.146−0.156
BERTscore 0.355 0 .190 −0.165 0 .367±0.0390.183±0.032−0.184 0 .325±0.0350.134−0.191
COMET 0.578 0 .385 −0.194 0 .584±0.0240.390±0.031−0.194 0 .461±0.0410.202−0.259
BLEURT-20 0.618 0 .357 −0.262 0 .603±0.0200.357±0.033−0.246 0 .449±0.0430.220−0.229
XCOMET-XL 0.713 0 .454 −0.259 0 .705±0.0200.449±0.018−0.256 0 .461±0.0300.250−0.211
XCOMET-XXL 0.708 0 .399 −0.309 0 .716±0.0200.382±0.032−0.335 0 .481±0.0410.326−0.155
MetricX-23 0.682 0 .433 −0.249 0 .680±0.0180.446±0.027−0.233 0 .450±0.0430.301−0.149
MaTESe 0.591 0 .353 −0.238 0 .593±0.0280.370±0.044−0.223 0 .341±0.0420.254−0.087
quality estimation
GEMBA-MQM 0.614 0 .345 −0.269 0 .621±0.0270.358±0.028−0.263 0 .462±0.0440.368−0.094
CometKiwi 0.565 0 .286 −0.279 0 .561±0.0190.268±0.021−0.293 0 .411±0.0440.182−0.229
CometKiwi-XL 0.542 0 .240 −0.302 0 .550±0.0230.254±0.032−0.296 0 .427±0.0290.223−0.204
CometKiwi-XXL 0.525 0 .236 −0.289 0 .504±0.0310.244±0.032−0.260 0 .456±0.0290.327−0.129
MetricX-23-QE 0.683 0 .425 −0.258 0 .681±0.0120.439±0.027−0.242 0 .470±0.0280.292−0.177
Table 4: Spearman correlation on WMT23 EN-DE. †: Subsampled to match G ROUP -BY-SRCHQ’s sample size.
NO-GROUPING NO-GROUPING † GROUP -BY-SRC
METRICALL HQ ∆ ALL HQ ∆ ALL†HQ ∆
chrF 0.232 0 .112 −0.120 0 .244±0.0280.121±0.028−0.123 0 .322±0.0410.124−0.198
BLEU 0.192 0 .086 −0.106 0 .210±0.0290.079±0.025−0.131 0 .297±0.0490.148−0.149
BERTscore 0.325 0 .150 −0.175 0 .331±0.0380.148±0.031−0.182 0 .363±0.0430.150−0.213
COMET 0.432 0 .337 −0.095 0 .421±0.0370.367±0.031−0.055 0 .513±0.0440.266−0.246
BLEURT-20 0.484 0 .324 −0.160 0 .488±0.0210.308±0.024−0.180 0 .469±0.0470.245−0.223
XCOMET-XL 0.680 0 .414 −0.266 0 .680±0.0280.409±0.040−0.272 0 .510±0.0540.359−0.150
XCOMET-XXL 0.695 0 .362 −0.333 0 .688±0.0190.355±0.038−0.333 0 .484±0.0680.385−0.098
MetricX-23 0.585 0 .406 −0.179 0 .576±0.0230.406±0.025−0.169 0 .512±0.0240.371−0.141
MaTESe 0.554 0 .238 −0.316 0 .547±0.0350.221±0.032−0.325 0 .345±0.0450.253−0.092
quality estimation
GEMBA-MQM 0.502 0 .223 −0.279 0 .497±0.0270.238±0.021−0.260 0 .485±0.0550.386−0.099
CometKiwi 0.475 0 .210 −0.265 0 .476±0.0370.198±0.049−0.277 0 .458±0.0570.226−0.232
CometKiwi-XL 0.446 0 .185 −0.262 0 .445±0.0330.198±0.032−0.247 0 .499±0.0410.328−0.171
CometKiwi-XXL 0.417 0 .171 −0.245 0 .411±0.0240.167±0.040−0.244 0 .531±0.0400.378−0.152
MetricX-23-QE 0.626 0 .371 −0.255 0 .640±0.0360.372±0.029−0.268 0 .536±0.0480.407−0.129
Table 5: Pearson correlation on WMT23 EN-DE. †: Subsampled to match G ROUP -BY-SRCHQ’s sample size.WMT23 HE-EN WMT23 ZH-EN
NO-GROUPING†GROUP -BY-SRC NO-GROUPING†GROUP -BY-SRC
METRIC All HQ All†HQ All HQ All†HQ
chrF 0.299 0 .140 0 .298 0 .144 0 .067 0 .012 0 .220 0 .162
BLEU 0.248 0 .145 0 .270 0 .161 0 .129 0 .065 0 .190 0 .139
BERTscore 0.391 0 .210 0 .368 0 .191 0 .269 0 .129 0 .273 0 .154
COMET 0.485 0 .226 0 .383 0 .167 0 .457 0 .268 0 .315 0 .183
BLEURT-20 0.459 0 .216 0 .379 0 .173 0 .434 0 .241 0 .332 0 .189
XCOMET-XL 0.511 0 .255 0 .362 0 .147 0 .608 0 .405 0 .334 0 .185
XCOMET-XXL 0.528 0 .260 0 .381 0 .140 0 .607 0 .364 0 .373 0 .219
MetricX-23 0.549 0 .258 0 .357 0 .171 0 .603 0 .408 0 .339 0 .202
MaTESe 0.415 0 .207 0 .353 0 .266 0 .467 0 .277 0 .322 0 .216
quality estimation
GEMBA-MQM 0.493 0 .245 0 .420 0 .227 0 .580 0 .358 0 .423 0 .264
CometKiwi 0.459 0 .225 0 .309 0 .106 0 .533 0 .328 0 .333 0 .160
CometKiwi-XL 0.434 0 .184 0 .348 0 .181 0 .532 0 .302 0 .334 0 .170
CometKiwi-XXL 0.468 0 .213 0 .389 0 .202 0 .504 0 .288 0 .352 0 .161
MetricX-23-QE 0.495 0 .235 0 .307 0 .126 0 .621 0 .411 0 .322 0 .159
XCOMET-QE-Ensemble 0.504 0 .233 0 .345 0 .160 0 .631 0 .377 0 .347 0 .177
Table 6: Spearman correlation on WMT23 (HE-EN and ZH-EN). †: Subsampled to match GROUP -BY-SRCHQ’s
sample size.
WMT22 EN-DE WMT22 EN-RU WMT22 ZH-EN
NO-GROUPING†GROUP -BY-SRC NO-GROUPING†GROUP -BY-SRC NO-GROUPING†GROUP -BY-SRC
METRIC All HQ All†HQ All HQ All†HQ All HQ All†HQ
chrF 0.296 0 .214 0 .242 0 .206 0 .235 0 .161 0 .237 0 .161 0 .199 0 .069 0 .189 0 .096
BLEU 0.233 0 .176 0 .221 0 .210 0 .194 0 .161 0 .198 0 .127 0 .200 0 .086 0 .146 0 .089
BERTScore 0.318 0 .244 0 .239 0 .207 0 .265 0 .210 0 .240 0 .158 0 .428 0 .189 0 .265 0 .155
COMET-22 0.497 0 .392 0 .358 0 .314 0 .534 0 .387 0 .394 0 .282 0 .428 0 .189 0 .265 0 .155
BLEURT-20 0.467 0 .346 0 .352 0 .283 0 .483 0 .342 0 .354 0 .257 0 .488 0 .194 0 .305 0 .170
MetricX-XL 0.499 0 .379 0 .395 0 .349 0 .511 0 .392 0 .379 0 .290 0 .550 0 .253 0 .314 0 .210
MetricX-XXL 0.490 0 .377 0 .370 0 .304 0 .561 0 .430 0 .402 0 .338 0 .554 0 .260 0 .303 0 .204
MaTESe 0.387 0 .296 0 .356 0 .349 0 .315 0 .236 0 .321 0 .281 0 .477 0 .243 0 .251 0 .222
quality estimation
CometKiwi 0.404 0 .300 0 .273 0 .223 0 .482 0 .341 0 .306 0 .228 0 .488 0 .223 0 .263 0 .205
MaTESe-QE 0.294 0 .236 0 .314 0 .316 0 .258 0 .184 0 .268 0 .256 0 .412 0 .214 0 .212 0 .208
Table 7: Spearman correlation on WMT22 (EN-DE, EN-RU, annd ZH-EN).†: Subsampled to match GROUP -BY-
SRCHQ’s sample size.
C HQ-Z ERO Detection Results
We present the results for the detection task on the WMT22 Metrics and Chat datasets in Figures 4 and 5,
respectively.EN-DE EN-RU ZH-ENMETRICP R F1 P R F1 P R F1
MaTESe 61 86 71 48 94 63 68 53 60
MaTESe-QE 58 87 70 46 95 62 64 55 59
Figure 4: Top: Scores distribution for HQ-Z ERO translations on WMT22. Bottom: Precision, recall, and F1.
EN-XX XX-ENMETRICP R F1 P R F1
chrF 88 38 53 92 42 58
BLEU 88 38 53 93 42 58
BERTScore 93 23 37 94 27 42
XCOMET-XL 75 33 46 87 38 53
MetricX-23-XL 76 64 69 87 62 72
XCOMET-XL-QE 66 29 40 84 49 62
MetricX-23-QE-XL 76 45 56 80 35 49
Figure 5: Top: Scores distribution for HQ- ZERO translations on WMT22 Chat. Bottom: Precision, recall, and F1.