Why Does New Knowledge Create Messy Ripple Effects in LLMs?
Jiaxin Qin1,2, Zixuan Zhang1*, Chi Han1, Manling Li1, Pengfei Yu1, Heng Ji1
1University of Illinois Urbana-Champaign
2Renmin University of China
{qjx0814, zixuan11, chihan3, pengfei4, hengji}@illinois.edu
manling@stanford.edu
Abstract
Extensive previous research has focused on
post-training knowledge editing (KE) for lan-
guage models (LMs) to ensure that knowl-
edge remains accurate and up-to-date. One
desired property and open question in KE is to
let edited LMs correctly handle ripple effects ,
where LM is expected to answer its logically
related knowledge accurately. In this paper, we
answer the question of why most KE methods
still create messy ripple effects. We conduct
extensive analysis and identify a salient indi-
cator, GradSim , that effectively reveals when
and why updated knowledge ripples in LMs.
GradSim is computed by the cosine similarity
between gradients of the original fact and its re-
lated knowledge. We observe a strong positive
correlation between ripple effect performance
and GradSim across different LMs, KE meth-
ods, and evaluation metrics. Further investiga-
tions into three counter-intuitive failure cases
(Negation ,Over-Ripple ,Multi-Lingual ) of rip-
ple effects demonstrate that these failures are
often associated with very low GradSim. This
finding validates that GradSim is an effective
indicator of when knowledge ripples in LMs.
1 Introduction and Related Work
Large language models (LLMs) can serve as power-
ful knowledge bases (KBs) thanks to their impres-
sive knowledge storage, retrieval, and reasoning
capabilities (Petroni et al., 2019; AlKhamissi et al.,
2022). However, real-world knowledge keeps up-
dating and evolving constantly, which motivates
extensive research efforts on post-training knowl-
edge editing (KE) (Meng et al., 2022, 2023; Yin
et al., 2023; Zhong et al., 2023; Song et al., 2024;
Liu et al., 2024) to make sure that the knowledge
in LMs remains accurate and up-to-date. Previ-
ous research has proposed numerous evaluation
metrics to ensure the efficiency and consistency
of these editing methods. Among them, one crit-
ical criterion is the ability of the KE method to
Knowledge Edit (LLM parameter 𝜽 replaced by 𝜽′):Expected Ripple-Effect:Leonardo DiCaprio is a citizen of United States.Syria.Leonardo DiCaprio speaks English.Arabic.Counter-Intuitive Failure Cases:Negation:Over-Ripple:Cross-Lingual:Leonardo DiCaprio is not a citizen of Syria.Leonardo DiCaprio speaksSyria.莱昂纳多·迪卡普⾥奥的国籍是:美国。
✅
❌
❌
❌United States.Arabic
✅叙利亚。
✅Similarly-stored knowledge is updated concurrently(𝐾!→𝐾!")(𝐾#→𝐾#")Parameter update vector:	𝜃!−𝜃Explained by∇$𝑃$(𝐾!")∇$𝑃$(𝐾#")similar projection on gradient direction(Leonardo DiCaprio is a citizen of)(United States.)(Syria.)Figure 1: An illustration of ripple effects in LLM knowl-
edge editing. Our work empirically demonstrates the
positive correlation between gradient similarity explains
a large portion of the ripple effect. Furthermore, messy
similarities between knowledge points create several
counter-intuitive ripple effect failures.
handle ripple effects (Cohen et al., 2023), where
a single edit should automatically and accurately
propagate to related facts. For example, suppose
an edit changes Leonardo DiCaprio ’s nationality
toSyrian . The model should automatically update
its related information, such as knowing that his
primarily used language is now Arabic . Such a task
is very challenging because it requires the model
to correctly understand and infer complex relation-
ships among knowledge elements and accurately
locate their parametric storage in order to perform
the edits. Empirically, even though direct knowl-
edge edits typically achieve over >90% accuracy,
the success rates of ripple effects struggle to exceed
50% across all recent KE methods, even on the sim-
plest task in RippleEdits (Cohen et al., 2023).
In this paper, we answer the intriguing research
question of when and why updated knowledge rip-
ples in language models. We hypothesize that the
knowledge storage among parameters plays a criti-
cal role in determining the ripple effects between
knowledge facts. A messy relationship among
knowledge elements can make achieving a suc-arXiv:2407.12828v2  [cs.CL]  19 Jul 2024cessful ripple effect intractable or impossible. Intu-
itively, the similarity of knowledge storage should
be an important factor, as knowledge represented
by similar parameters will respond similarly to pa-
rameter updates during knowledge editing. Follow-
ing this intuition, we conduct extensive analysis
and identify a salient indicator that strongly reveals
how likely an updated fact will ripple in a language
model: the cosine similarity between the gradients
of the related knowledge facts (GradSim). We use
gradients to represent knowledge storage distribu-
tion in LMs because they indicate which param-
eters in the model are responsible for increasing
or decreasing the likelihood of answering certain
knowledge. We observe a strong positive corre-
lation between ripple effect performance and the
cosine similarity of gradients across different LMs,
editing methods, and evaluation metrics, with a
Pearson correlation metric reaching as high as 0.85.
The hypothesis and analysis above predict a
counter-intuitive phenomenon: knowledge with
similar parameter-storing locations, even if logi-
cally unrelated or contradictory, will create pos-
itive ripple effects toward each other, and vise
versa. Viewing GradSim as an indicator of rip-
ple effects, we verify this paradox above by dis-
covering and explaining three specific ripple effect
cases: Negation ,Over-Ripple Errors andCross-
Lingual Transfer . As illustrated in Figure 1, as-
suming that a knowledge edit changes the citizen-
ship of Leonardo DiCaprio from United States
toSyria . In Negation , LMs with different sizes
including GPT2-XL and LLaMA-2 unexpectedly
answer the negated query Leonardo is not a cit-
izen of still by Syria instead of logically correct
answers such as United States . Moreover, in Over-
Ripple Errors , the LMs over-memorize the edit
target Syria , and tends to always answer Syria even
when asked about other topics such as language.
InCross-Lingual Transfer , even the most power-
ful cross-lingual LMs could make mistakes when
asked about the edited knowledge in a different
language. All of these ripple-effect cases are com-
monly encountered in real-world applications, but
are more challenging and often experience counter-
intuitive failures with current LMs and KE meth-
ods. In our experiments, we demonstrate that the
model’s failure in these cases is strongly correlated
with a too small GradSim, the similarity in knowl-
edge distributions within LMs.2 GradSim: A Ripple Effect Indicator
In this section, we formally introduce GradSim,
a ripple-effect indicator based on the knowledge
storage similarity between related knowledge. We
usexandyto denote a pair of original fact and its
related knowledge respectively, and we use (qx, ax)
and(qy, ay)to represent query-answer pairs based
on the corresponding knowledge facts. For in-
stance, if qxandaxare<Leonardo DiCaprio is a
citizen of> and<United States> respectively, then
one example pair of qyandaycould be <Leonardo
DiCaprio speaks> and<English> . Given a query
qx, typical KE methods update axto a new answer
a′
xby applying an update on the model parame-
tersθ, and ripple effect evaluations expect that the
LM can automatically find the correct a′
ywhen
asked qy. Based on our hypothesis, knowledge
represented by similar parameteres will respond
similarly to parameter updates during knowledge
editing. We employ the gradient to model the stor-
age of knowledge within an LLM, and use the co-
sine similarity to measure the proximity between
the storage distribution of two pieces of knowledge:
GradSim (x, y) = cos( ∇θPθ(ax|qx),∇θPθ(ay|qy)).
3 Experiments
We assess the effectiveness of GradSim by em-
pirically examining its correlation with ripple ef-
fect performance, aiming to determine if it reli-
ably indicates successful knowledge propagation
in language models. Furthermore, we analyze three
typical counter-intuitive failure cases in detail to
understand the role of GradSim in these situations.
3.1 Data, Models, and KE Methods
In our experiments, we mainly employ RippleEdits
(Cohen et al., 2023), the most widely-used bench-
mark for evaluating ripple effects for knowledge
editing methods in LLMs. We mainly use the popu-
larsplit in RippleEdits because popular entities are
more likely to be recognized by language models.
This approach helps to minimize any side effects
resulting from the model’s lack of knowledge, al-
lowing us to focus on its reasoning abilities. To
ensure a comprehensive evaluation, we also demon-
strate results in the recent split as shown in Figure 2.
In our experiments, we consider two typical knowl-
edge editing (KE) methods: ROME (Meng et al.,
2022) and MEMIT (Meng et al., 2023), which are
based on the locate-and-edit approach to modify0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim40
20
0204060logp(y)
y = 27.942x + 0.663
Pearson correlation = 0.796
Spearman correlation = 0.811LLaMA-7B, ROME
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim40
20
0204060logp(y)
y = 25.721x - 1.162
Pearson correlation = 0.771
Spearman correlation = 0.598LLaMA-7B, MEMIT
0.0 0.2 0.4 0.6 0.8 1.0
GradSim7.5
5.0
2.5
0.02.55.07.510.012.5logp(y)
y = 3.247x - 0.046
Pearson correlation = 0.628
Spearman correlation = 0.708GPT2-XL, ROME
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim20
10
0102030logp(y)
y = 10.077x - 1.148
Pearson correlation = 0.497
Spearman correlation = 0.454LLaMA-7B, ROME(recent split)
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logp(y)
logp(x)
y = 1.119x + 0.060
Pearson correlation = 0.848
Spearman correlation = 0.812
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logp(y)
logp(x)
y = 1.218x - 0.238
Pearson correlation = 0.113
Spearman correlation = 0.581
0.0 0.2 0.4 0.6 0.8 1.0
GradSim2
1
0123logp(y)
logp(x)
y = 0.394x + 0.387
Pearson correlation = 0.011
Spearman correlation = 0.640
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logp(y)
logp(x)
y = 1.222x - 0.173
Pearson correlation = 0.099
Spearman correlation = 0.456
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.995x + 0.088
Pearson correlation = 0.805
Spearman correlation = 0.754
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.975x + 0.016
Pearson correlation = 0.726
Spearman correlation = 0.550
0.0 0.2 0.4 0.6 0.8 1.0
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.869x - 0.008
Pearson correlation = 0.819
Spearman correlation = 0.691
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.903x - 0.053
Pearson correlation = 0.628
Spearman correlation = 0.495Figure 2: Main results of evaluating the correlation between ripple effect performances and GradSim values.
model parameters. For language models, we eval-
uate both larger models like LLaMA2-7B (Tou-
vron et al., 2023) and smaller models like GPT-2
XL (Radford et al., 2019) to ensure a comprehen-
sive evaluation and maintain consistency with the
original settings in ROME and MEMIT.
3.2 Evaluation Metrics of Ripple Effects
To ensure the validity of our experiment results, we
consider multiple evaluation metrics assessing how
well the model performs in answering ripple-effect
queries. These metrics include both accuracy-based
measures, such as the exact match rate, and more
quantified likelihood metrics, such as the absolute
and relative gains in likelihood.
Exact-Match (EM) Rate Similar to (Cohen
et al., 2023), we first consider accuracy-based met-
rics to calculate the proportion of correct answers
the model generates from multiple random sam-
pling choices. For each ripple query qy, we sample
50 generated answers with a temperature 0.7, and
compute the proportion of answers that include the
correct answer. Our metric differs slightly from
that in (Cohen et al., 2023), as we need to compute
performance for each individual data point to ana-
lyze the overall correlation. The maximum length
for generation is set to a small size of 15, as webelieve that the answer is expected to appear early
in a cloze-test query format.
Absolute Likelihood Gain We also examine the
answer probabilities to obtain a more detailed and
quantifiable assessment of the performance. As
probability values could be very small as the se-
quence length increases, we use log-likelihood
score logP(a′
y|qy), and measure its absolute gain
on the correct answers before and after editing:
∆ log P(y) = log Pθ′(a′
y|qy)−logPθ(a′
y|qy).
Relative Likelihood Gain This metric is formu-
lated by dividing the absolute gain of ripple effects
by the absolute gain of the original fact, thereby
normalizing the difficulty of the knowledge editing
itself.
∆ log P(y)
∆ log P(x)=logPθ′(a′
y|qy)−logPθ(a′
y|qy)
logPθ′(a′x|qx)−logPθ(a′x|qx).
3.3 Main Results
We conduct a comprehensive correlation analysis
across different language models, knowledge edit-
ing methods, and performance metrics for ripple
effects, with the results illustrated in Figure 2. A
strong positive correlation is observed between
ripple-effect performances and the GradSim values,validating that gradient-based knowledge storage
similarity is a reliable indicator of ripple effects.
Additionally, we observe the emergence of two dis-
tinct clusters in the figure. This clustering likely
occurs because the data points can be categorized
into successful and unsuccessful edits. Successful
edits result in a significant improvement on per-
formance, placing them in the upper successful
cluster, while unsuccessful edits tends to remain in
the unsuccessful cluster.
3.4 Counter-Intuitive Failure Cases
Negation Negation is one of the most straight-
forward ripple effects where the model is expected
to answer a negated query after an editing is ap-
plied. For example, after editing the nationality of
Leonardo DiCaprio asSyrian , the model should
be able to avoid Syria given a negated query like
“Leonardo DiCaprio is nota citizen of ”. How-
ever, both smaller-sized LMs like GPT-2 and larger-
sized LMs like LLaMA still answers “Syria” to this
query and simply ignore the negation inside the sen-
tence. In Figure 3, we first visualize both the values
and gains of model likelihoods for the original and
negated facts. The results demonstrate a strong pos-
itive (almost linear) correlation, indicating a severe
problem of negation failures. In terms of GradSim
values, we find that the gradient similarities be-
tween the original and negated facts are very high,
suggesting that the original and negated facts are
entangled in similar knowledge storage locations.
0 10 20 30 40 50
logP(x)
01020304050logP(notx)
y = 0.799x + 3.563Relation between logP(x) and logP(notx)
0.0 0.2 0.4 0.6 0.8
GradSim05101520FrequencyDistribution of GradSim
cosine_value
1
 0 1 2 3 4
logP(notx)
logP(x)
0510152025303540FrequencyDistribution of logP(notx)
logP(x)
Figure 3: Correlation between original and negated facts
on likelihood change, and the distributions of GradSim
and likelihood ratios between original and negated facts.
Over-Ripple Errors The over-ripple problem
refers to the situation where, after a knowledge
edit, the LM only memorizes the edited target itself
and continues to provide this target as the answer
even when asked about other knowledge that is re-
lated. For example, after editing the nationality of
Leonardo DiCaprio asSyrian , the model will still
answer Syria even when asked about the primary
language Leonardo is speaking (the correct answer
should be Arabic ). In Figure 4, we first visual-
ize GradSim distributions on (qy, a′
x)and(qy, a′
y)respectively, and we can observe that the edited
target a′
x(e.g., Syria ) has a much higher gradient
similarity compared to the correct answer a′
y(e.g.,
Arabic ). This explains the similar performance of
answering both the correct and incorrect answers
and indicates the occurrence of over-ripple errors.
0.5
 0.0 0.5
GradSim0102030FrequencyDistribution of GradSim
cos(P(a/primex|qx),P(a/primex|qy)
cos(P(a/primex|qx),P(a/primey|qy)
60
 40
 20
 0
log P(Y)0102030FrequencyDistribution of log P(Y)
a/primey|qy
a/primex|qy
0.00 0.25 0.50 0.75 1.00
Exact Match Rate020406080100FrequencyDistribution of Exact Match Rate
a/primey|qy
a/primex|qy
Figure 4: The distributions of GradSim values and ripple
effect performances.
Cross-Lingual Transfer The problem of cross-
lingual transfer is defined as the ability to edit a
piece of knowledge in one language and have the
model still provide the correct answer when asked
a question in another language. We study the role
of GradSim by visualizing the distribution of Grad-
Sim values and the ripple effect performance across
different languages. We employ Baichuan (Yang
et al., 2023), the state-of-the-art bilingual model
for Chinese and English. As shown in Figure 5,
while the performance on the target language re-
mains low, the GradSim values are also very low,
primarily distributed near zero. GradSim works as
a reliable indicator in this special ripple-effect case
for cross-lingual transfer.
0.00 0.25 0.50 0.75 1.00
GradSim01020304050Number of samplesDistribution of GradSim
0.00 0.25 0.50 0.75 1.00
Exact Match Rate0100200300400500Number of samplesEdited in Chinese
English
Chinese
0.00 0.25 0.50 0.75 1.00
Exact Match Rate0100200300400Number of samplesEdited in English
English
Chinese
Figure 5: GradSim and performance distributions when
editing on one language and testing on another.
4 Conclusion
Through extensive experiments and analysis, we
propose GradSim, computed as the cosine similar-
ity between gradients of the original fact and its
related knowledge, as a crucial indicator for the
effectiveness of the ripple effects. The positive cor-
relation observed between GradSim and ripple ef-
fect performance across various LMs, KE methods,
and evaluation metrics underscores its reliability.
Additionally, our exploration of failure cases such
as Negation, Over-Ripple, and Cross-Lingual sce-
narios, further confirms that low GradSim values
are indicative of ripple effect failures.Limitations
The first notable limitation is that, although a strong
relationship between GradSim and the performance
of ripple effects has been demonstrated, our re-
search remains at the level of exploring correlations
between these two factors and has not yet estab-
lished a causal relationship. While it is always
challenging to determine causality, it would still be
extremely interesting and exciting to explore the
dominant contributing factors to the complex dis-
tribution of knowledge storage in the pre-training
phase of LMs. The second important limitation is
that, while this paper identifies an indicator, we did
not provide practical solutions for improving rip-
ple effect performance by leveraging this indicator.
However, we believe that the insights provided in
this short paper will significantly enable the devel-
opment of practical and effective methods in future
research.
References
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona
Diab, and Marjan Ghazvininejad. 2022. A review on
language models as knowledge bases. arXiv preprint
arXiv:2204.06031 .
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,
and Mor Geva. 2023. Evaluating the ripple effects
of knowledge editing in language models. Preprint ,
arXiv:2307.12976.
Jiateng Liu, Pengfei Yu, Yuji Zhang, Sha Li, Zixuan
Zhang, and Heng Ji. 2024. Evedit: Event-based
knowledge editing with deductive editing boundaries.
Preprint , arXiv:2402.11324.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in gpt. Advances in Neural Information Pro-
cessing Systems , 35:17359–17372.
Kevin Meng, Arnab Sen Sharma, Alex J. Andonian,
Yonatan Belinkov, and David Bau. 2023. Mass-
editing memory in a transformer. In The Eleventh
International Conference on Learning Representa-
tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .
OpenReview.net.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2463–2473.Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Xiaoshuai Song, Zhengyang Wang, Keqing He, Guant-
ing Dong, Yutao Mou, Jinxu Zhao, and Weiran Xu.
2024. Knowledge editing on black-box large lan-
guage models. Preprint , arXiv:2402.08631.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint , arXiv:2307.09288.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng
Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,
Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Ji-
aming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su,
Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang
Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei-
dong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li,
Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong
Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men,
Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang,
Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang,
Zenan Zhou, and Zhiying Wu. 2023. Baichuan
2: Open large-scale language models. Preprint ,
arXiv:2309.10305.
Xunjian Yin, Jin Jiang, Liming Yang, and Xiaojun
Wan. 2023. History matters: Temporal knowl-
edge editing in large language model. Preprint ,
arXiv:2312.05497.
Zexuan Zhong, Zhengxuan Wu, Christopher Manning,
Christopher Potts, and Danqi Chen. 2023. MQuAKE:
Assessing knowledge editing in language models via
multi-hop questions. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 15686–15702, Singapore.
Association for Computational Linguistics.0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim40
20
0204060logP(y)
y = 27.942x + 0.663
Pearson correlation = 0.796
Spearman correlation = 0.811LLaMA-7B, ROME
Compositionality_I
Compositionality_II
Logical_Generalization
Subject_Aliasing
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim40
20
0204060logP(y)
y = 25.721x - 1.162
Pearson correlation = 0.771
Spearman correlation = 0.598LLaMA-7B, MEMIT
0.0 0.2 0.4 0.6 0.8 1.0
GradSim7.5
5.0
2.5
0.02.55.07.510.012.5logP(y)
y = 3.247x - 0.046
Pearson correlation = 0.628
Spearman correlation = 0.708GPT2-XL, ROME
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim20
10
0102030logP(y)
y = 10.077x - 1.148
Pearson correlation = 0.497
Spearman correlation = 0.454LLaMA-7B, ROME(recent split)
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logP(y)
logP(x)
y = 1.119x + 0.060
Pearson correlation = 0.848
Spearman correlation = 0.812
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logP(y)
logP(x)
y = 1.218x - 0.238
Pearson correlation = 0.113
Spearman correlation = 0.581
0.0 0.2 0.4 0.6 0.8 1.0
GradSim2
1
0123logP(y)
logP(x)
y = 0.394x + 0.387
Pearson correlation = 0.011
Spearman correlation = 0.640
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim2.0
1.5
1.0
0.5
0.00.51.01.52.0logP(y)
logP(x)
y = 1.222x - 0.173
Pearson correlation = 0.099
Spearman correlation = 0.456
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.995x + 0.088
Pearson correlation = 0.805
Spearman correlation = 0.754
0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.975x + 0.016
Pearson correlation = 0.726
Spearman correlation = 0.550
0.0 0.2 0.4 0.6 0.8 1.0
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.869x - 0.008
Pearson correlation = 0.819
Spearman correlation = 0.691
0.4
 0.2
 0.0 0.2 0.4 0.6 0.8 1.0
GradSim1.0
0.5
0.00.51.0Exact Match Ratey = 0.903x - 0.053
Pearson correlation = 0.628
Spearman correlation = 0.495Figure 6: Main results of evaluating the correlation between ripple effect performances and GradSim values labeled
with different task names
A Extra Results of GradSim
The RippleEdits dataset comprises six distinct
tasks: Logical Generalization (LG), Compositional-
ity I (CI), Compositionality II (CII), Subject Alias-
ing (SA), Preservation (PV), and Relation Speci-
ficity (RS) (Cohen et al., 2023). These tasks are
designed to evaluate different aspects of knowledge
editing in neural networks. Specifically, LG, CI,
CII, and SA are tasks where the model is expected
to manifest new knowledge in response to edits
made to existing entries. Conversely, in the RS and
PV tasks, the existing knowledge should remain un-
altered post-editing, as these tasks are designed to
test the model’s ability to preserve information that
is logically independent from the changes applied.
In Figure 2, we analyze data from these tasks
to illustrate a positive correlation between ripple
effect performance and GradSim values across the
four tasks explicitly associated with knowledge up-
dates, hereafter referred to as ripple tasks . Each
data point in Figure 6 is labeled to demonstrate
the consistent applicability of the GradSim met-
ric across the individual sub-tasks. In contrast,
Figure 7 focuses on the two non-ripple tasks (RS
and PV), where no significant correlation is ob-
served between GradSim values and ∆ log P(y).The comparison underscores that GradSim is a crit-
ical metric for evaluating ripple effects, as it shows
no significant impact in the non-ripple tasks , con-
firming its relevance specifically in contexts where
knowledge modifications are expected.
0.25
 0.00 0.25 0.50
GradSim5
051015logP(y)
None-Ripple T asks
Preservation
Relation Specificity
0.0 0.5 1.0
GradSim10
5
0510logP(y)
Ripple T asks
Compositionality I
Compositionality II
Logical Generalization
Subject Aliasing
Figure 7: Comparison of the Correlation on None-
Ripple Tasks and Ripple Tasks.
B How to Represent Knowledge
Distribution?
In this paper, we utilize gradients to represent the
distribution of knowledge. To support this ap-
proach, we conducted preliminary experiments that
lend credence to the underlying rationale of this
intuition.B.1 Does the way that we express a piece of
knowledge change the knowledge
distribution?
0 5 10 15 20 25 30
The down_proj layers in LlaMA-7B0.0100.0150.0200.0250.0300.0350.0400.0450.050L1 norm
The name of the currency in the country of citizenship of Leonardo DiCaprio is -Syrian pound
The currency Leonardo Decaprio use is -Syrian pound
Whats's the name of the currency in the country of citizenship of Leonardo DiCaprio? -Syrian pound
Figure 8: L1 Norm Distribution over LlaMA-7B
In this study, we calculate the gradient of a spe-
cific piece of knowledge, "The name of the cur-
rency in the country of citizenship of Leonardo Di-
Caprio is the Syrian pound," along with its variants:
"The currency Leonardo DiCaprio uses is the Syr-
ian pound" and "What’s the name of the currency in
the country of citizenship of Leonardo DiCaprio?
Syrian pound." We then plot the L1 norm of the gra-
dient across the 32 downward projection layers of
LlaMA7b. Prior research suggests that these layers
are particularly adept at storing knowledge. Our
results indicate that the distribution across these
variants is remarkably consistent, suggesting that a
piece of knowledge may be encoded within specific
parameters of a large language model.