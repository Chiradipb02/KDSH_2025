APPLS: Evaluating Evaluation Metrics for Plain Language
Summarization
Yue Guo1Tal August2Gondy Leroy3Trevor Cohen1Lucy Lu Wang1,2
1University of Washington2Allen Institute for AI3University of Arizona
{yguo50, cohenta, lucylw}@uw.edu, tala@allenai.org
Abstract
While there has been significant development
of models for Plain Language Summarization
(PLS), evaluation remains a challenge. PLS
lacks a dedicated assessment metric, and the
suitability of text generation evaluation metrics
is unclear due to the unique transformations in-
volved (e.g., adding background explanations,
removing jargon). To address these questions,
our study introduces a granular meta-evaluation
testbed, APPLS , designed to evaluate metrics
for PLS. We identify four PLS criteria from pre-
vious work—informativeness, simplification,
coherence, and faithfulness—and define a set
of perturbations corresponding to these crite-
ria that sensitive metrics should be able to de-
tect. We apply these perturbations to extractive
hypotheses for two PLS datasets to form our
testbed. Using APPLS , we assess performance
of 14 metrics, including automated scores, lex-
ical features, and LLM prompt-based evalua-
tions. Our analysis reveals that while some cur-
rent metrics show sensitivity to specific criteria,
no single method captures all four criteria si-
multaneously. We therefore recommend a suite
of automated metrics be used to capture PLS
quality along all relevant criteria. This work
contributes the first meta-evaluation testbed for
PLS and a comprehensive evaluation of exist-
ing metrics.1
1 Introduction
Plain language summaries of scientific informa-
tion are important to make science more accessible
(Kuehne and Olden, 2015; Stoll et al., 2022) and
inform public decision-making (Holmes-Rovner
et al., 2005; Pattisapu et al., 2020). Recently, gen-
erative models have made gains in translating sci-
entific information into plain language approach-
able to lay audiences (August et al., 2022; Gold-
sack et al., 2023; Devaraj et al., 2021). Despite
1APPLS and our evaluation code can be found at
https://github.com/ LinguisticAnomalies/APPLS.
Controlled perturbation
Criteria-specific design
Simulate real-world  situationScientiﬁc 
abstractPlain language 
summary
Literature
Oracle extractive  hypothesisAPPLSPLS
Perturbation %.j.Metric ScoreFigure 1: We present APPLS , the first granular testbed
for analyzing evaluation metric performance for plain
language summarization (PLS). We assess performance
of 14 existing metrics, including automated scores, lexi-
cal features, and LLM prompt-based evaluations.
these gains, the field has not reached consensus
on effective automated evaluation metrics for plain
language summarization (PLS) (Luo et al., 2022;
Ondov et al., 2022) due to the multifaceted nature
of the PLS task. Removal of unnecessary details
(Pitcher et al., 2022), adding relevant background
explanations (Guo et al., 2021), jargon interpreta-
tion (Pitcher et al., 2022), and text simplification
(Devaraj et al., 2021) are all involved in PLS, pos-
ing challenges for comprehensive evaluation.
Our goal is to assess how well existing metrics
capture the multiple criteria of PLS. We identify
four criteria, informed by prior work (Pitcher et al.,
2022; Ondov et al., 2022; Stoll et al., 2022; Jain
et al., 2022), that a PLS metric should be sensitive
to:informativeness ,simplification ,coherence , and
faithfulness . We introduce a set of perturbations
to probe metric sensitivity to these criteria, where
each perturbation is designed to affect a single cri-
terion with ideally minimal impact to others.2By
2We acknowledge that introducing any change in text likely
affects multiple criteria, though we design our perturbations
carefully to try and minimize these impacts.arXiv:2305.14341v3  [cs.CL]  23 Jul 2024incrementally introducing perturbations to the texts
of two scientific PLS datasets, CELLS (Guo et al.,
2022) and PLABA (Attal et al., 2023), we produce
our meta-evaluation testbed APPLS.
We analyze 14 metrics using APPLS , including
the most widely used metrics in text simplifica-
tion and summmarization literature, and recently-
proposed prompt-based methods (Gao et al., 2023;
Luo et al., 2023). We find that established metrics
like ROUGE (Lin, 2004), BERTScore (Zhang et al.,
2019), and QAEval (Deutsch et al., 2021) do not
capture simplification and are inconsistent at cap-
turing perturbations in informativeness, coherence,
and faithfulness; SARI score (Xu et al., 2016), ex-
plicitly crafted for text simplification, is the only
automated score that displays sensitivity towards
simplification perturbations but not to other per-
turbations. LLM prompt-based evaluations are ef-
fective for assessing informativeness and faithful-
ness, but not for coherence and simplification. Our
analysis suggests that a single overall score cannot
simultaneously respond to all four criteria.
Our main contributions are as follows:
•We present APPLS , the first granular testbed
for analyzing evaluation metric performance for
plain language summarization; the testbed is
created by applying 11 perturbations along four
dimensions to two scientific PLS datasets (§3, 4);
•We conduct a thorough analysis of 14 existing
evaluation metrics (including automated metrics,
lexical features, and LLM prompting methods),
demonstrating mixed effectiveness in evaluat-
ing informativeness, coherence, faithfulness, and
simplification (§5, 6);
•Based on our findings, we recommend an
evaluation strategy for PLS that combines
multiple automated metrics able to capture
differences along all relevant criteria.
2 Related Work
Limitations of Existing Metrics The primary
approach for evaluating plain language summaries
adopts evaluation metrics for summarization and
simplification, coupled with human evaluation
(Jain et al., 2021; Ondov et al., 2022). While
ROUGE (Lin, 2004) and BLEU (Sulem et al., 2018)
are frequently employed in PLS assessment, their
efficacy is limited due to the reliance on high-
quality reference summaries, which are often chal-
lenging to obtain for PLS. Further, these metrics
struggle to accurately identify hallucinations, espe-cially crucial for PLS in the health domain to accu-
rately inform health decisions (Wallace et al., 2021;
Pagnoni et al., 2021; Wang et al., 2023). Though
human evaluation offers thorough assessment, the
high costs and time needed impede scalability for
larger datasets. While recent progress in prompt-
based evaluation shows potential for assessing fac-
tuality (Luo et al., 2023) and summarization quality
(Gao et al., 2023), their efficacy for PLS is yet to be
validated. Our work aims to fill these gaps through
a systematic examination of these metrics within
the PLS context.
Robust Analysis with Synthetic Data Synthetic
data has been widely used in NLP tasks to evaluate
metrics, including text generation (He et al., 2022;
Sai et al., 2021), natural language inference (Chen
and Eger, 2022; McCoy et al., 2019), question an-
swering (Ribeiro et al., 2019), and reading com-
prehension (Sugawara et al., 2020). Yet, no prior
work has focused on the PLS task or incorporated
simplification into their benchmarks. Additionally,
previous studies lack granular analyses to capture
the nuanced relationship between text changes and
score changes. Our research endeavors to bridge
these gaps by crafting perturbations that mirror
real-world errors within the PLS context.
3 PLS Evaluation Criteria
We identify four criteria that an effective PLS eval-
uation metric should be sensitive to based on both
abstractive summarization (Sai et al., 2022) and
plain language summarization paradigms (Pitcher
et al., 2022; Ondov et al., 2022; Stoll et al., 2022;
Jain et al., 2022). As in Gabriel et al. (2020), we de-
finesensitivity as being correlated in the expected
direction with the amount of change in that criteria.
•Informativeness measures the extent to which a
PLS covers essential information from the paper
(e.g., methods, main findings) and incorporates
relevant background information (Smith et al.,
2021; Beck et al., 1991).
•Simplification describes the degree to which
information is conveyed in a form that non-
expert audiences can readily understand. It is
distinct from informativeness because it focuses
on surface-level changes (e.g., shorter sentences)
but not other changes relevant to content (e.g.,
background explanation).
•Coherence describes the logical arrangement of
a plain language summary.Original textWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them. The first step is an accurate assessment of the population prevalence of past infections… (Kline et al., 2021)CriterionPerturbationSimulated real-world situationPerturbed textDelete sentencesSalient information missingWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them. The first step is an accurate assessment of the population prevalence of past infections…Add out-of-domain sentencesOut-of-domain hallucination In this paper we address the problem of aggregating the outputs of classi ers solving different nlp tasks. Worldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them…Add in-domain sentencesIn-domain hallucinationWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them. This review synthesised the latest evidence on the reduction of antipsychotic doses for stable individuals with schizophrenia…Add definitionsBackground explanationWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them. Coronaviruses are species in the genera of virus belonging to the subfamily Coronavirinae in the family Coronaviridae. Coronaviruses are enveloped viruses with a positive-sense RNA genome and with a nucleocapsid of helical symmetry. The genomic size of coronaviruses ranges from approximately 26 to 32 kilobases, extraordinarily large for an RNA virus. … SimplificationReplace sentencesParaphrasing with simple termsSARS-CoV-2 is a virus that has infected over 59 million people globally and killed more than 1.39 million. Scientists are trying to learn more about the virus in order to design interventions to slow and stop its spread. One of the first steps is understanding how many people have been infected in the past, which requires accurate population prevalence studies…CoherenceReorder sentencesPoor writing flowThe first step is an accurate assessment of the population prevalence of past infections. Worldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them…Number swapHuman errorsWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 64 million people and killed more than one of them…Entity swapHuman errorsWorldwide, canine adenovirus (CaV-2), a severe acute respiratory syndrome, has infected more than 59 million people and killed more than one of them…Synonym verb swapHuman errorsWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, has infected more than 59 million people and stamped out more than one of them… Antonym verb swapHuman errorsWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, infected more than 59 million people and saved more than one of them…NegateHuman errorsWorldwide, coronavirus 2 (SARS-CoV-2), a severe acute respiratory syndrome, hasn’t infected more than 59 million people and killed more than one of them. InformativenessFaithfulnessNotations: removals/ additions/ modificationsTable 1: Example perturbations for criteria in APPLS. Original text comes from the CELLS (Guo et al., 2022).
Dataset Version Word Sentence
CELLS Abstract ( src.) 283 ±132 11±6
(n=6,311) PLS ( tgt.) 178 ±74 7±3
Oracle Hypothesis 134 ±58 5±2
GPT-simplified 130 ±34 6±2
PLABA Abstract ( src.) 240 ±95 10±4
(n=750) Adaptation ( tgt.) 244 ±95 12±5
Table 2: Diagnostic datasets statistics (mean ±std).
•Faithfulness denotes how well the summary
aligns factually with the source text.
4 Constructing the APPLS Testbed
To assess metric sensitivity, we develop perturba-
tions along each criteria dimension. We implement
our perturbations in two large-scale PLS datasets
(§4.1). We describe how perturbations are incor-
porated into these datasets and our approach for
managing perturbation magnitude (§4.2) and vali-
dating perturbation quality (§4.3).
4.1 Diagnostic datasets
For our experiments, we use the CELLS (Guo et al.,
2022) and PLABA (Attal et al., 2023) datasets.
CELLS (Guo et al., 2022) is a parallel corpus ofscientific abstracts ( source texts) and their corre-
sponding plain language summaries ( target texts),
which are written by the abstract authors or by other
domain experts. CELLS aggregates papers from
12 biomedical journals, representing a diverse set
of topics and summaries, and serves as the primary
dataset in our testbed.
The PLABA (Attal et al., 2023) dataset includes
expert-modified biomedical abstracts, simplified
to improve understanding of health-related con-
tent. PLABA includes sentence-level alignments,
which are useful for controlled perturbations, but is
nonetheless not used as our primary dataset due to
the simplifications being relatively contrived. The
modification involves rule-based adjustments, such
as lexical simplification, shifting from passive to
active voice, and segmenting long sentences. This
results in high n-gram overlap between sources and
target summaries, which is unrealistic and does not
reflect PLS in the real world. Therefore, PLABA
serves as an auxiliary dataset to CELLS, helping to
address its limitations, discussed in Sections §4.2
and §4.3. We show full results using PLABA as
the diagnostic dataset in App. H.4.2 Applying perturbations to datasets
Illustrative examples of all perturbations are shown
in Table 1. For the APPLS testbed, we propose and
apply perturbations to an oracle hypothesis , an ex-
tractive summary with additional lexical variation
introduced through round-trip translation (Ormaz-
abal et al., 2022) (Illustration in App. Figure 6).
We do not perturb the target texts directly since
the resulting hypotheses would be overly similar
to the target and unrealistic. For CELLS, the ex-
tractive summary is created by selecting the set of
source sentences yielding the highest ROUGE-L
score when compared to the target summary, and
this summary is then round-trip translated through
German to derive the oracle hypothesis.3PLABA
is sentence-aligned, with sources and targets having
similar lengths, so we produce the oracle hypoth-
esis simply through round-trip translation of the
target. Details are in App. B.
We apply all perturbations to the oracle hypothe-
ses as described below, where each perturbation
introduces a change (e.g., add/swap sentences) at
some magnitude (e.g., replace 50% of sentences).
Due to the high costs associated with some of our
perturbations (e.g., LLM-based simplification), we
restrict our testbed to the test splits of our diagnos-
tic datasets (stats in Table 2).
Informativeness
Delete sentences : To simulate the loss of relevant
information, we delete sentences until a single
sentence remains. The magnitude of deletion is
the ratio of remaining to original sentences.
Add sentences : We add up to the same number
of sentences as in the oracle hypothesis. To sim-
ulate out-of-domain hallucinations, we add sen-
tences from ACL papers (Bird et al., 2008). For
in-domain hallucinations, we add sentences from
Cochrane abstracts.4The magnitude of addition
is the ratio of added to original sentences.
Add definitions : Background explanations are fun-
damental to PLS and involve adding external con-
tent like definitions or examples (Guo et al., 2022;
Srikanth and Li, 2020). To simulate these, we
add up to three definitions, the average number of
nouns explained in CELLS (Guo et al., 2022), i.e.,
100% perturbed adds three definitions.
Simplification
3Why not use the extractive summary directly? Metrics
like SARI expect simplified hypotheses and exhibit degenerate
behavior when used to evaluate extractive summaries.
4https://community.cochrane.orgReplace sentences : For CELLS, we first gener-
ate an LLM-simplified summary from the oracle
hypothesis. We align sentences between the ora-
cle hypothesis and LLM-simplified summary us-
ing the sentence alignment algorithm from Guo
et al. (2022). We perturb the text by replacing
random hypothesis sentences with their corre-
sponding simplifications until full replacement
is achieved. We use GPT-4 (Achiam et al., 2023)
to generate simplifications due to its accessibility
and demonstrated proficiency in text simplifica-
tion (Lu et al., 2023). To ensure that our findings
are not specific to the chosen model, we also gen-
erate simplifications and conduct experiments us-
ing GPT-3 (Brown et al., 2020), Llama2 (Touvron
et al., 2023) and Claude5. For PLABA, we perturb
text by replacing source sentences with round-trip
translated versions of their aligned simplified tar-
gets (no LLM is used).
Coherence
Reorder sentences : We shuffle sentences in the hy-
pothesis and quantify perturbation percentage in
terms of the absolute difference in sentence order
between the original and shuffled hypotheses, e.g.,
a document with reversed sentence order would
be 100% perturbed.
Faithfulness
Number swap : We randomly add a number from 1
to 5 to the original numerical value in the text.
Verb swap : We introduce two perturbations by sub-
stituting verbs with either synonyms or antonyms.
An appropriate metric should ignore synonyms
but be sensitive to antonyms.
Entity swap : We replace entities using the KBIN
method (Wright et al., 2022), which replaces en-
tity spans with related concepts in UMLS6while
maximizing NLI contradiction and minimizing
LM perplexity. This results in a fluent sentence
that varies from the original one. The perturba-
tion magnitude of number, entity, and verb swaps
is determined by comparing the count of altered
spans to the total number of eligible spans in the
hypothesis. Full perturbation means all eligible
spans are swapped.
Negate sentences : We negate sentences, and allow
up to one negation per sentence.
To mitigate the effects of randomness, we use two
random seeds to produce perturbations.
5https://www.anthropic.com
6https://www.nlm.nih.gov/research/umls/4.3 Human validation of oracle hypotheses
and LLM simplifications
We validate two design decisions of APPLS that
involve other models modifying text: round-trip
translation (RTT) for the oracle hypothesis and
GPT-4-based simplification perturbations, by con-
ducting human evaluation. We sample 100 pairs
each of (i) pre- and post-RTT oracle hypotheses
and (ii) GPT-simplified summaries paired with or-
acle hypotheses. Annotators were asked to assess
content alignment (defined as having comparable
entities and relations between entities) and rate
informativeness, simplification, faithfulness, and
coherence on 5-point Likert scales. Annotations
were performed by two independent annotators,
both with doctorates in the biological sciences,
who were hired on UpWork and compensated at 21
USD/hr. Each annotator reviewed all sampled pairs
for both evaluation tasks. Inter-rater agreement
measured by Cohen’s Kappa was 0.48, by Spear-
man rank correlation was 0.58, implying moder-
ate agreement for both tasks (Artstein and Poesio,
2008). Details of the annotation tasks are given in
App. C.
Human annotators affirmed that RTT text re-
tained its informativeness (98%), faithfulness
(83%), coherence (100%), and simplicity (96%)
compared to the original text. For GPT-simplified
sentences, evaluators rated informativeness (95%),
faithfulness (95%), coherence (98%), and simplic-
ity (100%), with GPT-simplifications consistently
rated as more simple than the oracle hypothesis
while preserving semantic content. In this context,
we report the proportion of annotations equal to
or better than neutral for each criteria. While the
informativeness and faithfulness of simplified text
are assessed to be very good at the passage level,
the alignment algorithm used to produce sentence
alignments for the simplification perturbation is im-
perfect and can introduce some errors. To mitigate
the impact of such misalignment on the interpre-
tation of results, we use the PLABA dataset for
auxiliary diagnostics because it contains ground
truth sentence-level alignments.
5 Evaluation Metrics
Our analysis spans 8 established evaluation met-
rics, including the 5 most commonly reported in
ACL’22 summarization/generation papers (empir-
ical results in App. D). We also assess 5 lexical
features associated with text simplification (§5.2)and LLM-based evaluations (§5.3).
5.1 Existing automated evaluation metrics
We compute the following 8 automated metrics:
•Overlap-based metrics measure n-gram over-
lap. We report ROUGE (computed as the average
of ROUGE-1, ROUGE-2, and ROUGE-L) (Lin,
2004), BLEU (Papineni et al., 2002), METEOR
(Banerjee and Lavie, 2005), and SARI score (Xu
et al., 2016).
•Model-based metrics use pretrained models to
evaluate text quality. We adopt GPT-PPL ,BERT-
Score (Zhang et al., 2019), and LENS (Maddela
et al., 2022).
•QA-based metrics capture content quality us-
ing a question-answering approach. We report
QAEval (Deutsch et al., 2021) scores here.
Details for all metrics are available in App. E. All
metrics assessed require target and hypothesis texts;
while SARI and LENS additionally make use of
the source texts.
5.2 Lexical features
We also assess lexical features that have been
shown to be associated with text simplicity:
•Length : Shorter sentences are easier to under-
stand (Kauchak et al., 2017). We report both
sentence length and paragraph length.
•Familiarity : Simple text contains more common
words (Leroy et al., 2018). We compute the per-
centage of text that is made up of the 1,000 most
common English words.7
•Specificity : Specificity quantifies the level of
detail in the text. We use Speciteller (Ko et al.,
2019) to compute the domain agnostic specificity
of terms in the paragraph.
•Phrase Transitions : Conjunctions (e.g., there-
fore) are important for flow and can assist with
comprehension (Kauchak et al., 2017). We re-
port the number of conjunctions.
•Function Words : Simple text contains more
verbs and fewer nouns (Mukherjee et al., 2017).
We report the number of verbs, nouns, adjectives,
adverbs, and numbers.
5.3 LLM prompt-based evaluations
Prompting LLMs for text generation evaluation has
been explored in recent work (Gao et al., 2023; Luo
et al., 2023). We adopt the prompt template from
7https://gist.github.com/deekayen/4148741Gao et al. (2023) to have GPT-4 ( gpt-4-0613 ) eval-
uate each hypothesis on the four PLS criteria and to
provide an overall quality score. All scores range
from 0 (worst) to 100 (best). We supply definitions
for each criterion in the prompt. We evaluate under
three settings: (a) providing a single criterion in the
prompt and requesting a score for that criterion; (b)
providing all criteria in the prompt and requesting
scores for each criterion as well as an overall score;
and (c) the same setting as (b) but requiring expla-
nations be generated alongside the provided scores.
Model configurations and prompts are in App. G.
6 Analysis Results
Automated metric responses to perturbations are
in Figure 2, responses of lexical features are in
Figure 3, and prompt-based evaluation results
are shown in Figure 4. All trends are consistent
across two random seeds. To contextualize metric
performance in APPLS , we survey metric changes
reported in ACL’22 papers on text generation and
summarization (full results in App. D).
Aside from SARI, current metrics exhibit short-
comings in evaluating simplicity. Metrics that
are sensitive to simplification should consistently
distinguish between more and less simplified text.
As shown in Figure 2, metrics that exhibit sensi-
tivity to simplification perturbations are GPT-PPL
(decreasing as more perturbations are introduced;
lower PPL is better) and SARI score. In follow-
up evaluations with PLABA (discussed below and
shown in Figure 15), we see that GPT-PPL has
undesirable sensitivity to text length, as found in
prior work (Zhao et al., 2022). ROUGE, BLEU,
METEOR, BERTScore, and QAEval decrease in
response to the simplification perturbation. While
they show consistent response relative to the de-
gree of perturbation, they are nonetheless not use-
ful for assessing text simplicity.8LENS and LLM
prompt-based evaluations are erratic or insensitive
to simplification perturbations.
SARI, however, is consistently sensitive to
simplified text. In addition to using GPT-4
(Achiam et al., 2023) to produced simplified text
for the simplification perturbation, we also test
three other LLMs: GPT-3 (Brown et al., 2020),
Llama 2 (Touvron et al., 2023), and Claude.5
8When we report metric changes swapping sources and
targets (perturbing simplified texts to increase complexity),
these metrics also decrease (App. Fig. 12), suggesting that
they are sensitive to n-gram changes and not text simplicity.In Figure 5, we show metric changes to the
simplification perturbation generated by all four
models. Similar score changes are observed for
all models (except GPT-3 for SARI score, which
is an outlier), demonstrating that the simplicity
perturbation in our testbed is a reasonable and
consistent measure of metric response to text
simplification, and that SARI is generally able to
distinguish between more and less simplified text.
Metrics effectively capture informativeness,
coherence, and faithfulness, with room for
improvement. For informativeness, ROUGE,
BLEU, BERTScore, GPT-PPL, and QAEval
are sensitive to information deletion and irrel-
evant additions, but decrease with the addition
of background explanations through keyword
definitions. For coherence, BERTScore and LENS
excel in detecting perturbations, largely due to
their ability to assess the sentence relationships.
BERTScore, GPT-PPL, and QAEval generally
perform well for faithfulness-related perturbations,
although GPT-PPL and BERTScore are somewhat
sensitive to synonym verb swaps, an undesirable
trait. QAEval is best at being unresponsive to
synonym verb swaps. Number swaps, however, re-
main undetected by all metrics. Results in Figure 2.
Lexical features are useful measures of text
simplicity. Figure 3 illustrates the response of
lexical features to degrees of text simplification in
CELLS, confirming trends observed in previous
studies (Kauchak et al., 2014; Leroy et al., 2018;
Kauchak et al., 2017; Mukherjee et al., 2017). As
simplification increases, sentence length decreases;
common words and verbs increase; and nouns,
adjectives, and term specificity decrease. Although
prior work emphasizes the importance of conjunc-
tions for comprehension (Kauchak et al., 2017),
our study reveals a reduction rather than increase
in conjunctions as texts become simpler. Overall,
these trends demonstrate that lexical features are
valuable indicators for text simplification. Results
on PLABA are similar, with an inverse trend for
paragraph length (App. Figure 16).
LLM prompt-based evaluations show promise
in distinguishing between PLS criteria. Prompt-
based scores demonstrate sensitivity to perturba-
tions in informativeness and faithfulness, while
showing less sensitivity to changes in coherence
and simplification (Figure 4). Although Luo
et al. (2023) reported agreement between ChatGPTFigure 2: Average scores of existing metrics for perturbed texts in the CELLS dataset. Scores are averaged in 10 bins
by perturbation percentage. Markers denote the defined criteria associated with that perturbation. Median reported
improvements in ACL’22 summarization and generation papers are ROUGE (+0.89), BLEU (+0.69), METEOR
(+0.50), SARI (+1.71), BERTScore (+0.55), and PPL (-2.06).
0.0 0.2 0.4 0.6 0.8 1.0
Perturbed Percentage0.5
0.00.51.0Relative Change
V.
N.
Adj.
Adv.
Num.
Para. len
Sent. len
Specificity
Conj.
Familarity
Figure 3: Relative change of each lexical feature with
respect to the unperturbed state (0%). Different markers
represent lexical feature categories.
scores and human ratings, our results indicate that
the generative text evaluation capacity of LLMs
warrants further investigation. Additionally, our
findings show that providing a single criterion, all
criteria, and all criteria with an explanation yield
similar results, except that providing all criteria is
more sensitive to entity swaps compared to provid-
ing a single criterion. This suggests that additional
explanations are not necessary for PLS evaluation,
and providing all criteria and asking for all scores
together is more efficient. Prompts and detailed
results are provided in Appendix G.
7 Discussion & Conclusion
Recent advances point to the possibility of auto-
mated plain language summarization (PLS); how-
ever, the multifaceted nature of PLS makes evalu-ation challenging. We introduce the first—to our
knowledge—meta-evaluation testbed, APPLS , for
evaluating PLS metrics. In APPLS , we apply con-
trolled text perturbations to existing PLS datasets
based on several criteria (informativeness, simplifi-
cation, coherence, and faithfulness).
Using APPLS , we find that while some metrics
reasonably capture informativeness, faithfulness,
and coherence, SARI is uniquely sensitive to sim-
plification perturbations, but exhibits insensitive
to other perturbations. Similar challenges are ob-
served for QAEval, as no single metric was consis-
tently sensitive to all perturbations across criteria.
Therefore, an evaluation metric suite should be con-
sidered based on the most desired criteria. Further
studies are also needed to identify better metrics
for more efficient evaluation of PLS.
The quickly improving performance of language
models on a variety of tasks has placed greater em-
phasis on human, extrinsic evaluations (Clark et al.,
2021) evaluating models in real-world use cases,
often with end-users. However, extrinsic evalu-
ations are time-intensive, difficult to implement
correctly, and costly, making them only viable for
the most promising models. Automated metrics
offer a fast and low-cost method for identifying
improvement trends even if they do not perfectly
measure absolute improvement. Our selection of
automated metrics grounded in criteria from the
health communication literature offer a viable firsta. Single Criterion Provided
b. All Criteria Provided
c. All Criteria Provided, Explanation NeededFigure 4: Prompt-based evaluation scores for four criteria - informativeness, simplification, coherence, and
faithfulness - along with an overall score. (a) providing a single criterion in the prompt and requesting a score for
that criterion; (b) providing all criteria in the prompt and requesting scores for each criterion as well as an overall
score; and (c) the same setting as (b) but with an additional requirement for explanations of the provided scores.
Notably, prompt-based scores demonstrate sensitivity to perturbations in informativeness and faithfulness, while
showing less sensitivity to changes in coherence and simplification. The three prompt settings yield similar results,
with the exception that providing all criteria (setting b and c) is more sensitive to entity swaps compared to providing
a single criterion (setting a).
Figure 5: Variation in existing scores for simplification perturbations created by GPT-3, Llama2, and Claude on the
CELLS dataset.
step in evaluating systems. Initial automated evalu-
ations can then be followed by extrinsic evaluations
to ensure comprehensive analysis.
OurAPPLS testbed allows for extensible evalu-ation of PLS evaluation metrics. Using the pertur-
bation pipeline, APPLS can transform any PLS
dataset into a granular meta-evaluation testbed.
New perturbations can be introduced, and new eval-uation metrics can also be incorporated easily into
analysis. Our testbed lays the groundwork for fur-
ther advancements in automated PLS and PLS eval-
uation, aiming to foster more impactful, accessible,
and equitable scientific communication.
Limitations
Our perturbations use synthetic data to simulate
real-world textual phenomena seen in PLS. Al-
though our approach is informed by theory and pro-
vides valuable insights into metric behavior, further
exploration of more sophisticated methods to simu-
late changes in these criteria is warranted. This is
especially true for aligning sentences between sci-
entific abstracts and plain language summaries, as
sentence-level alignment for scientific summaries
is still an open problem (Krishna et al., 2023).
We also acknowledge that text quality may dete-
riorate with synthetic perturbations in a way that
affects multiple PLS criteria. However, by using
synthetic data, we are benefiting from the ability
to control our perturbations and extend our testbed
creation framework to any dataset. It is infeasi-
ble to find naturally occurring text with the same
controlled levels of each perturbation, with mini-
mal changes to other aspects. Our aim is not to
produce perfect outputs, but rather to establish a
robust baseline for evaluating the performance of
automated metrics for PLS evaluation. The results
of our analysis complement qualitative examina-
tions of model output conducted in other work,
which further suggests that automated text gener-
ation evaluation metrics may be limited in their
ability to assess generation performance of post-
GPT-3 LLMs (Goyal et al., 2022).
We have also focused our analysis on commonly
used metrics reported in prior work on simplifica-
tion, summarization, and generation. Investigating
the performance of metrics not included in this
work, as well as the generalizability of our meth-
ods to meta-evaluation for other generative NLP
tasks, is a future goal.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. 2023. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 .
Fernando Alva-Manchego, Louis Martin, Carolina Scar-
ton, and Lucia Specia. 2019. EASSE: Easier auto-matic sentence simplification evaluation. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP): System Demonstrations ,
pages 49–54, Hong Kong, China. Association for
Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional linguistics , 34(4):555–596.
Kush Attal, Brian Ondov, and Dina Demner-Fushman.
2023. A dataset for plain language adaptation of
biomedical abstracts. Scientific Data , 10(1):8.
Tal August, Lucy Lu Wang, Jonathan Bragg, Marti A
Hearst, Andrew Head, and Kyle Lo. 2022. Paper
plain: Making medical research papers approachable
to healthcare consumers with natural language pro-
cessing. arXiv preprint arXiv:2203.00130 .
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of
the acl workshop on intrinsic and extrinsic evaluation
measures for machine translation and/or summariza-
tion, pages 65–72.
Isabel L Beck, Margaret G McKeown, Gale M Sinatra,
and Jane A Loxterman. 1991. Revising social studies
text from a text-processing perspective: Evidence
of improved comprehensibility. Reading research
quarterly , pages 251–276.
Steven Bird, Robert Dale, Bonnie J Dorr, Bryan R Gib-
son, Mark Thomas Joseph, Min-Yen Kan, Dongwon
Lee, Brett Powley, Dragomir R Radev, Yee Fan Tan,
et al. 2008. The acl anthology reference corpus: A
reference dataset for bibliographic research in com-
putational linguistics. In LREC .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, T. J. Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Yanran Chen and Steffen Eger. 2022. Menli: Robust
evaluation metrics from natural language inference.
arXiv preprint arXiv:2208.07316 .
Elizabeth Clark, Tal August, Sofia Serrano, Nikita
Haduong, Suchin Gururangan, and Noah A. Smith.
2021. All that’s ‘human’ is not gold: Evaluating
human evaluation of generated text. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing(Volume 1: Long Papers) , pages 7282–7296, Online.
Association for Computational Linguistics.
Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth.
2021. Towards question-answering as an automatic
metric for evaluating the content quality of a sum-
mary. Transactions of the Association for Computa-
tional Linguistics , 9:774–789.
Ashwin Devaraj, Iain Marshall, Byron C Wallace, and
Junyi Jessy Li. 2021. Paragraph-level simplification
of medical texts. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 4972–4984.
Alexander R Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-
Cann, Caiming Xiong, Richard Socher, and Dragomir
Radev. 2021. Summeval: Re-evaluating summariza-
tion evaluation. Transactions of the Association for
Computational Linguistics , 9:391–409.
Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi,
and Jianfeng Gao. 2020. Go figure: A meta evalua-
tion of factuality in summarization. arXiv preprint
arXiv:2010.12834 .
Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-
ing Yang, and Xiaojun Wan. 2023. Human-like sum-
marization evaluation with chatgpt. arXiv preprint
arXiv:2304.02554 .
Tomas Goldsack, Zhihao Zhang, Chenghua Lin,
and Carolina Scarton. 2023. Domain-driven and
discourse-guided scientific summarisation. In Ad-
vances in Information Retrieval: 45th European Con-
ference on Information Retrieval, ECIR 2023, Dublin,
Ireland, April 2–6, 2023, Proceedings, Part I , pages
361–376. Springer.
Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.
News summarization and evaluation in the era of gpt-
3.ArXiv , abs/2209.12356.
Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, and
Trevor Cohen. 2022. Cells: A parallel corpus for
biomedical lay language generation. arXiv preprint
arXiv:2211.03818 .
Yue Guo, Wei Qiu, Yizhong Wang, and Trevor Co-
hen. 2021. Automated lay language summarization
of biomedical scientific reviews. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 35, pages 160–168.
Tianxing He, Jingyu Zhang, Tianle Wang, Sachin
Kumar, Kyunghyun Cho, James Glass, and Yulia
Tsvetkov. 2022. On the blind spots of model-based
evaluation metrics for text generation. arXiv preprint
arXiv:2212.10020 .
Margaret Holmes-Rovner, Sue Stableford, Angela
Fagerlin, John T Wei, Rodney L Dunn, Janet Ohene-
Frempong, Karen Kelly-Blake, and David R Rovner.
2005. Evidence-based patient choice: a prostate can-
cer decision aid in plain language. BMC Medical
Informatics and Decision Making , 5(1):1–11.Deepali Jain, Malaya Dutta Borah, and Anupam Biswas.
2021. Summarization of legal documents: Where
are we now and the way forward. Computer Science
Review , 40:100388.
Raghav Jain, Anubhav Jangra, Sriparna Saha, and Adam
Jatowt. 2022. A survey on medical document sum-
marization. arXiv preprint arXiv:2212.01669 .
David Kauchak, Gondy Leroy, and Alan Hogue. 2017.
Measuring text difficulty using parse-tree frequency.
Journal of the Association for Information Science
and Technology , 68(9):2088–2100.
David Kauchak, Obay Mouradi, Christopher Pentoney,
and Gondy Leroy. 2014. Text simplification tools:
Using machine learning to discover features that iden-
tify difficult text. In 2014 47th Hawaii international
conference on system sciences , pages 2616–2625.
IEEE.
Wei-Jen Ko, Greg Durrett, and Junyi Jessy Li. 2019.
Domain agnostic real-valued specificity prediction.
InProceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pages 6610–6617.
Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit
Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.
2023. Longeval: Guidelines for human evaluation
of faithfulness in long-form summarization. In Euro-
pean Chapter of the Association for Computational
Linguistics .
Lauren M Kuehne and Julian D Olden. 2015. Lay sum-
maries needed to enhance science communication.
Proceedings of the National Academy of Sciences ,
112(12):3585–3586.
Gregoire Leroy, Emma L Carroll, Mike W Bruford,
J Andrew DeWoody, Allan Strand, Lisette Waits,
and Jinliang Wang. 2018. Next-generation metrics
for monitoring genetic erosion within populations
of conservation concern. Evolutionary Applications ,
11(7):1066–1083.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Junru Lu, Jiazheng Li, Byron C. Wallace, Yulan He,
and Gabriele Pergola. 2023. Napss: Paragraph-level
medical text simplification via narrative prompting
and sentence-matching summarization. In Findings .
Junyu Luo, Junxian Lin, Chi Lin, Cao Xiao, Xinning
Gui, and Fenglong Ma. 2022. Benchmarking auto-
mated clinical language simplification: Dataset, al-
gorithm, and evaluation. In Proceedings of the 29th
International Conference on Computational Linguis-
tics, pages 3550–3562.
Zheheng Luo, Qianqian Xie, and Sophia Ananiadou.
2023. Chatgpt as a factual inconsistency evaluator
for abstractive text summarization. arXiv preprint
arXiv:2303.15621 .Mounica Maddela, Yao Dou, David Heineman, and Wei
Xu. 2022. Lens: A learnable evaluation metric for
text simplification. arXiv preprint arXiv:2212.09739 .
R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.
Right for the wrong reasons: Diagnosing syntac-
tic heuristics in natural language inference. arXiv
preprint arXiv:1902.01007 .
Partha Mukherjee, Gondy Leroy, David Kauchak,
Brianda Armenta Navarrete, Damian Y Diaz, and
Sonia Colina. 2017. The role of surface, semantic
and grammatical features on simplification of span-
ish medical texts: A user study. In AMIA Annual
Symposium Proceedings , volume 2017, page 1322.
American Medical Informatics Association.
Brian Ondov, Kush Attal, and Dina Demner-Fushman.
2022. A survey of automated methods for biomedical
text simplification. Journal of the American Medical
Informatics Association , 29(11):1976–1988.
Aitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor
Soroa, and Eneko Agirre. 2022. Principled para-
phrase generation with parallel corpora. arXiv
preprint arXiv:2205.12213 .
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstrac-
tive summarization with frank: A benchmark for fac-
tuality metrics. arXiv preprint arXiv:2104.13346 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Nikhil Pattisapu, Nishant Prabhu, Smriti Bhati, and
Vasudeva Varma. 2020. Leveraging social media
for medical text simplification. In Proceedings of
the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval ,
pages 851–860.
Nicole Pitcher, Denise Mitchell, and Carolyn Hughes.
2022. Template and guidance for writing a cochrane
plain language summary.
Marco Tulio Ribeiro, Carlos Guestrin, and Sameer
Singh. 2019. Are red roses red? evaluating consis-
tency of question-answering models. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 6174–6184.
Ananya B Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas
Mohan, and Mitesh M Khapra. 2021. Perturba-
tion checklists for evaluating nlg evaluation metrics.
arXiv preprint arXiv:2109.05771 .
Ananya B Sai, Akash Kumar Mohankumar, and
Mitesh M Khapra. 2022. A survey of evaluation met-
rics used for nlg systems. ACM Computing Surveys
(CSUR) , 55(2):1–39.Reid Smith, Pamela Snow, Tanya Serry, and Lorraine
Hammond. 2021. The role of background knowledge
in reading comprehension: A critical review. Reading
Psychology , 42(3):214–240.
Neha Srikanth and Junyi Jessy Li. 2020. Elabora-
tive simplification: Content addition and explana-
tion generation in text simplification. arXiv preprint
arXiv:2010.10035 .
Marlene Stoll, Martin Kerwer, Klaus Lieb, and Anita
Chasiotis. 2022. Plain language summaries: A sys-
tematic review of theory, guidelines and empirical
research. Plos one , 17(6):e0268789.
Saku Sugawara, Pontus Stenetorp, Kentaro Inui, and
Akiko Aizawa. 2020. Assessing the benchmarking
capacity of machine reading comprehension datasets.
InProceedings of the AAAI Conference on Artificial
Intelligence , volume 34, pages 8918–8927.
Elior Sulem, Omri Abend, and Ari Rappoport. 2018.
Bleu is not suitable for the evaluation of text simplifi-
cation. arXiv preprint arXiv:1810.05995 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al. 2023. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288 .
Byron C Wallace, Sayantan Saha, Frank Soboczenski,
and Iain J Marshall. 2021. Generating (factual?) nar-
rative summaries of rcts: Experiments with neural
multi-document summarization. AMIA Summits on
Translational Science Proceedings , 2021:605.
Lucy Lu Wang, Yulia Otmakhova, Jay DeYoung,
Thinh Hung Truong, Bailey Kuehl, Erin Bransom,
and Byron Wallace. 2023. Automated metrics
for medical multi-document summarization disagree
with human evaluations. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 9871–
9889, Toronto, Canada. Association for Computa-
tional Linguistics.
Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl,
Arman Cohan, Isabelle Augenstein, and Lucy Lu
Wang. 2022. Generating scientific claims for
zero-shot scientific fact checking. arXiv preprint
arXiv:2203.12990 .
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,
and Chris Callison-Burch. 2016. Optimizing statisti-
cal machine translation for text simplification. Trans-
actions of the Association for Computational Linguis-
tics, 4:401–415.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675 .Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng,
Dongkyu Lee, Yiping Song, Jian Sun, and Nevin L
Zhang. 2022. Improving meta-learning for low-
resource text classification and generation via mem-
ory imitation. arXiv preprint arXiv:2203.11670 .S1S2S3T1T2T3Oracle extractiveS1S3GermanEnglishRound translationOracle extractive  hypothesis
ParagraphScientiﬁc abstractPlain language summary
PerturbationS1’S3’S1’’S3’’S1’’S3’’’Figure 6: Process for generating the oracle extractive
hypothesis, on which we apply all perturbation opera-
tions.
en-de-en en-ru-en020406080BLEU Score
Figure 7: BLEU scores of round-trip translation
for English-German-English (en-de-en) and English-
Russian-English (en-ru-en) in CELLS oracle extractive
hypotheses.
A Applying Perturbations
Figure 6 shows how the oracle extractive hypothe-
sis is produced from the original scientific abstract.
An extractive summary is identified based on high
ROUGE-L with the plain language target. The
extractive summary is passed through round-trip
translation via German to introduce lexical vari-
ation. The resulting oracle extraction hypothesis
forms the basis for our perturbations.
B Round-trip translation for oracle
extractive hypothesis
We use round-trip translation to introduce lexical
variation into our oracle extractive summaries. This
is important when computing metrics such as SARI,
which exhibit degenerate behavior when the hy-
tgt-src extracted-src roundtrip-src020406080100BLEU Score
Figure 8: Comparison of BLEU scores between oracle
extractive summary (extracted) and oracle extractive
hypothesis (roundtrip), using the scientific abstract (src)
as the reference for BLEU calculation.
pothesis is an extractive subset of the source. We
examine two languages for round-trip translation:
German and Russian. By employing the BLEU
score as a performance metric for the round-trip
generated text relative to the original source, we
find that the English-German-English (en-de-en)
translation sequence yields superior BLEU scores
(Figure 7), and therefore, select the en-de-en se-
quence to produce the oracle extractive hypothesis
for our testbed.
To scrutinize the introduced variation through
this extractive and round-trip translation pipeline,
we evaluate the BLEU score. As depicted in Fig-
ure 8, the BLEU score for the oracle extractive hy-
pothesis is lower than that of the oracle extractive
summary. This suggests the successful introduction
of text variations. Augmented by human evaluation
results in Table 3, with 152 out of 198 raters indicat-
ing comparable simplification levels between the
oracle extractive hypothesis and its extractive coun-
terparts, we conclude that our extractive and round-
trip translation approach successfully introduces
lexical variation in our oracle extractive summaries
without altering their simplicity level.
C Details of human evaluation
To validate the quality of oracle extractive hypothe-
ses and GPT-simplified summaries, we randomly
select 100 summary pairs from each corpus for hu-
man evaluation. Each pair in the oracle extractive
hypotheses consists of an oracle extractive sentence
and its respective en-de-en round-trip-translation
sentence. Similarly, each pair in the GPT-simplified
summaries contains a hypothesis chunk along withType Unmatched Criteria Str. Agree Agree Neutral Disagree Str. Disagree
Round Trip Translation 1Simplification 12 27 152 7 0
Informativeness 188 4 3 4 0
Faithfulness 155 6 4 20 14
Coherence 30 11 156 2 0
GPT Simplification 0Simplification 67 32 1 0 0
Informativeness 37 37 21 5 0
Faithfulness 38 43 14 4 1
Coherence 10 47 41 2 0
Table 3: Counts of human evaluation ratings on each matched sentence for each criteria. For round trip translation,
there are 200 ratings; for GPT simplification, there are 100 ratings. Overall, we see that round trip translation
maintains strong faithfulness to the original, does not remove important information, and remains equally simple
and coherent (shown by a majority of neutral ratings for the simplification and coherence criteria). For GPT
simplification, we see that the simplification perturbation leads to substantially more simple text, while also
maintaining faithfulness and informativeness.
Figure 9: An example human evaluation task for assessing GPT-simplified summary quality.
its corresponding GPT-simplified summary chunk.
Each pair is reviewed by two independent an-
notators. Annotators were hired through UpWork
and have Bachelors and Doctorate degrees in the
biological sciences. In the evaluation, the text pairs
are labeled as Text A and Text B, without any indi-
cation that either text is generated. The annotators
are first asked to assess whether the content of Text
A matches the content of Text B, where a match
is defined as containing the same relation tuples.
If the texts match, the annotators further evaluate
Text B in relation to Text A, assessing whetherText B encapsulates key points (informativeness),
is more comprehensible (simplification), maintains
factual integrity (faithfulness), and exhibits a well-
structured layout (coherence). All facets are as-
sessed using a 1-5 Likert scale (1-strongly disagree,
5-strongly agree). Representative questions can be
found in Figure 9. This research activity is exempt
from institutional IRB review.ROUGEBLEUPerplexityMETEORBERTScoreMoverScoreCIDErBLEURTBARTScoreCheXbertFactCCDAEQUOREFCHrF++051015202530Reported FrequencySummarization
GenerationFigure 10: Most common evaluation metrics reported
in ACL’22 summarization and generation long papers.
ROUGE 1ROUGE 2ROUGE LBLEU 1BLEU 2BLEU 3BLEU 4BLEU avg.PerplexityBERTScoreMETEOR05101520Score Improvement
Figure 11: Distributions of reported metric improve-
ments over baseline (absolute value) reported in ACL’22
summarization and generation long papers.
DEmpirical Study of Evaluation Metrics
Reported in ACL 2022 Publications
Our study undertakes a comprehensive analysis of
scores reported in the long papers of ACL 2022
to identify the most prevalently reported metrics
in summarization and simplification tasks. We pri-
marily concentrate on tasks related to generation,
summarization, and simplification. Our inclusion
criteria are: 1) long papers with ‘generat,’ ‘sum-
mar,’ or ‘simpl’ in the title; and 2) papers that report
scores for both the current model and at least one
baseline model in the main text. We exclude scores
from ablation studies.
Of the 601 long papers accepted to ACL 2022,
109 satisfy our inclusion criteria, which we cate-
gorize into 31 summarization and 78 generation
papers, with no qualified papers related to sim-
plification tasks. Considering the significance of
simplification in PLS, we expanded our search toall ACL 2022 papers, including long, short, system
demonstration, and findings papers. This led to the
identification of 2 out of 22 papers with ‘simpl’ in
the title that reported SARI scores. As illustrated
in Figure 10, the five most frequently reported au-
tomated evaluation metrics are ROUGE, BLEU,
GPT-PPL, METEOR, and BERTScore.
This investigation provides insight into the cur-
rent adoption of evaluation metrics in natural lan-
guage generation, summarization, and simplifica-
tion tasks. We observe that a majority of papers
employ the same metrics across these tasks, and the
reported improvements are often relatively small
compared to the overall ranges for each measure.
We also underscore the difficulty of interpreting
changes in some of these metrics, especially model-
based metrics, which lack grounding to lexical dif-
ferences in text such as n-gram overlap.
By presenting the reported score differences
from ACL papers, we hope to contextualize the
metric changes observed through testing in our
meta-evaluation testbed. Median reported improve-
ments for the most commonly reported metrics and
SARI are: ROUGE (+0.89), BLEU (+0.69), PPL
(-2.06), METEOR (+0.50), BERTScore (+0.55),
and SARI (+1.71), as shown in Figure 11. We re-
port the median of BERTScore values and deltas as
reported in these publications, without considering
the usage of different models or settings.
E Details on existing automated
evaluation metrics
Overlap-based metrics measure n-gram overlaps,
and are popular due to their ease of use.
•ROUGE9(Lin, 2004) measures n-gram over-
lap between generated and reference summaries,
focusing on recall. We report the average of
ROUGE-1, ROUGE-2, and ROUGE-L.
•BLEU9(Papineni et al., 2002) computes n-gram
precision of generated text against reference
texts, including a brevity penalty.
•METEOR9(Banerjee and Lavie, 2005) employs
a relaxed matching criterion based on the F-
measure, and addresses the exact match restric-
tions and recall consideration of BLEU.
•SARI10(Xu et al., 2016) is specifically designed
to evaluate text simplification tasks. The score
9Implementation: Fabbri et al. (2021) BERTScore
hash code: bert-base-uncased_L8_no-idf_version =
0.3.12(hug_trans=4.27.3).
10Implementation: Alva-Manchego et al. (2019)weights deleted, added, and kept n-grams be-
tween the source and target texts.
Model-based metrics use pretrained models to
evaluate text quality.
•GPT-PPL ,11usually computed with GPT-2,
measures fluency and coherence by calculating
the average log probability assigned to each to-
ken by the GPT model, with lower scores indi-
cating higher fluency and coherence.
•BERTScore9(Zhang et al., 2019) quantifies the
similarity between hypothesis and targets us-
ing contextualized embeddings from the BERT
model, computing the F1-score between embed-
dings to capture semantic similarity beyond n-
gram matching.
•LENS (Maddela et al., 2022) employs an adap-
tive ranking loss to focus on targets closer to the
system output in edit operations (e.g., splitting,
paraphrasing, deletion).
QA-based metrics capture content quality using a
question-answering approach.
•QAEval (Deutsch et al., 2021) generates
question-answer pairs from the target text, then
uses a learned QA model to answer these ques-
tions using the generated text. The score is com-
puted as the proportion of questions answered
correctly. We report QAEval LERC scores.
F Additional experiments for existing
metrics
To illustrate that existing metrics are not sensitive
to text simplicity but rather to length and n-gram
overlap, we present metric scores computed when
swapping source and target for simplification per-
turbations (Figure 12). When target text is used as
reference, we start with the oracle extractive hy-
pothesis and increase perturbation percentage by
swapping in simpler text, going from more com-
plex to more simple text. When source text is used
as reference, we reverse the original source and tar-
get, starting with simple text and swapping in the
oracle extractive hypothesis, thereby moving from
more simple to more complex text. A metric sensi-
tive to text simplification should move in opposite
directions in these two settings as perturbation per-
centage increases. However, these metric scores
uniformly decrease under both settings, regardless
of the reference, demonstrating that these metrics
are not responsive to simplification but more so to
text length and n-gram overlap. We do not report
11https://huggingface.co/transformers/v3.2.0/perplexity.htmlperformance of BERTScore and QAEval under this
setting due to the higher cost of computing these
model based metrics.
G LLM Prompt-Based Evaluation
We use GPT-4 for LLM evaluation. The generation
process is configured with a temperature parameter
of 0, a maximum length of 150, and a penalty value
of 0. For each input, the top-ranked text is selected
as the GPT-simplified output. Example prompts
used for evaluation are provided in Figure 13.
H Additional perturbation results for
PLABA
We present full perturbation results on PLABA
(Attal et al., 2023) in Figure 15. The trends for
many perturbations are in the same direction as in
CELLS. While many metrics now show a desir-
able reversed trend to simplification (increasing),
we point out that this is inconsistent performance
relative to CELLS and is due to the high n-gram
overlap between the hypothesis and targets in this
case (we perturb by replacing source sentences
with round-trip translated target sentences to form
hypotheses, which only introduces minor lexical
variation). Adding text, especially definitions, dra-
matically decreases many of these metrics due to
the similar lengths of source and target texts in
PLABA, again pointing to the n-gram and length
sensitivities of most of these metrics.
The impact of simplification perturbations on lex-
ical features in the PLABA dataset is shown in Fig-
ure 16. Most trends are similar to CELLS, though
paragraph length increases with higher perturba-
tion percentage. In PLABA’s target construction
scheme, the target simplified texts length (244) are
similar to the source abstracts (240).Figure 12: Average scores of ROUGE, BLEU, METEOR, and SARI scores calculated using either the source text
(complex) or target text (simple) as reference for simplification perturbations on the CELLS dataset. A metric
sensitive to text simplicity should move in opposing directions under these two settings. However, ROUGE, BLEU,
METEOR decrease uniformly in both settings, suggesting that they are not sensitive to text simplicity.
a. Single criterion provided Imagine you are a human annotator now. You will evaluate the quality of a generated plain language summary for a scientiﬁc literature abstract. Please follow these steps:
                1. Read the scientiﬁc abstract provided.
                2. Read the generated plain language summary.
                3. Compared to the scientiﬁc abstract, rate the generated summary on the following criteria: {one of the criteria}
                4. Assign a score for the generated summary, rating on a scale from 0 (worst) to 100 (best).
                5. You do not need to explain the reason. Only provide the score.
Scientiﬁc abstract:{abstract}; 
Generated plain language summary:{pls_gen};
Score:
b. All criteria provided Imagine you are a human annotator now. You will evaluate the quality of a generated plain language summary for a scientiﬁc literature abstract. Please follow these steps:
                1. Read the scientiﬁc abstract provided.
                2. Read the generated plain language summary.
                3. Compared to the scientiﬁc abstract, rate the generated summary on the following criteria: {all criteria}
                4. Assign a score for the generated summary, rating on a scale from 0 (worst) to 100 (best).
                5. You do not need to explain the reason. Only provide the score.
Scientiﬁc abstract:{abstract}; 
Generated plain language summary:{pls_gen};
Score:
c. All criteria provided, explanation needed Imagine you are a human annotator now. You will evaluate the quality of a generated plain language summary for a scientiﬁc literature abstract. Please follow these steps:
                1. Read the scientiﬁc abstract provided.
                2. Read the generated plain language summary.
                3. Compared to the scientiﬁc abstract, rate the generated summary on the following criteria: {all criteria}
                4. Assign a score for the generated summary, rating on a scale from 0 (worst) to 100 (best).
                5. Explain the reason for the score. Scientiﬁc abstract:{abstract}; 
Generated plain language summary:{pls_gen};
Score:Criteria: 
-Informativeness: measures the extent to which a plain language summary encapsulates essential elements such as methodologies, primary ﬁndings, and conclusions from the original scientiﬁc text. An informative summary eﬃciently conveys the central message of the source material, avoiding the exclusion of crucial details or the introduction of hallucinations (i.e., information present in the summary but absent in the scientiﬁc text), both of which could impair reader comprehension.
-Simpliﬁcation: encompasses the rendering of information into a form that non-expert audiences can readily interpret and understand. This criterion prioritizes the use of simple vocabulary, casual language, and concise sentences that minimize excessive jargon and technical terminology unfamiliar to a lay audience.
-Coherence: pertains to the logical arrangement of a plain language summary. A coherent summary guarantees an unambiguous and steady progression of ideas, oﬀering information in a well-ordered fashion that facilitates ease of comprehension for the reader. We conjecture that the original sentence order reﬂects optimal coherence.
-Faithfulness: denotes the extent to which the plain language summary aligns factually with the source scientiﬁc text, in terms of its ﬁndings, methods, and claims. A faithful summary should not substitute information or introduce errors, misconceptions, and inaccuracies, which can misguide the reader or misrepresent the original author's intent. Faithfulness emphasizes the factual alignment of the summary with the source text, while informativeness gauges the completeness and eﬃciency of the summary in conveying key elements.
Figure 13: Prompts used for GPT-4 based evaluation: (a): single criterion provided; (b) all criteria provided; and (c)
all criteria provided and explanation needed.a. Reference-Free
b. Reference-ProvidedFigure 14: Prompt-based evaluation scores for four criteria - informativeness, simplification, coherence, and
faithfulness - along with an overall score. (a): Reference free; (b) Reference provided.
Figure 15: Average scores of existing metrics for perturbed texts in the PLABA dataset. Scores are averaged in 10
bins by perturbation percentage. Markers denote perturbations associated with our four defined criteria.0.0 0.2 0.4 0.6 0.8 1.0
Perturbed Percentage0.2
0.1
0.00.10.2Relative Change
V.
N.
Adj.
Adv.
Num.
Para. len
Sent. len
Specificity
Conj.
FamilarityFigure 16: Relative change of each lexical feature with
respect to perturbations in the PLABA dataset. Different
markers represent lexical feature categories.