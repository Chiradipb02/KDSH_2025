Images Speak Louder than Words: Understanding and Mitigating Bias in
Vision-Language Model from a Causal Mediation Perspective
Zhaotian Weng1, Zijun Gao1, Jerone Andrews2, and Jieyu Zhao1
1University of Southern California
2Sony AI
{wengzhao, jieyuz}@usc.edu ,zijungao@marshall.usc.edu ,Jerone.Andrews@sony.com
Abstract
Vision-language models (VLMs) pre-trained
on extensive datasets can inadvertently learn
biases by correlating gender information with
specific objects or scenarios. Current methods,
which focus on modifying inputs and monitor-
ing changes in the model’s output probability
scores, often struggle to comprehensively un-
derstand bias from the perspective of model
components. We propose a framework that
incorporates causal mediation analysis to mea-
sure and map the pathways of bias generation
and propagation within VLMs. Our framework
is applicable to a wide range of vision-language
and multimodal tasks. In this work, we apply
it to the object detection task and implement it
on the GLIP model. This approach allows us
to identify the direct effects of interventions on
model bias and the indirect effects of interven-
tions on bias mediated through different model
components. Our results show that image fea-
tures are the primary contributors to bias, with
significantly higher impacts than text features,
specifically accounting for 32.57% and 12.63%
of the bias in the MSCOCO and PASCAL-
SENTENCE datasets, respectively. Notably,
the image encoder’s contribution surpasses that
of the text encoder and the deep fusion encoder.
Further experimentation confirms that contribu-
tions from both language and vision modalities
are aligned and non-conflicting. Consequently,
focusing on blurring gender representations
within the image encoder which contributes
most to the model bias, reduces bias efficiently
by 22.03% and 9.04% in the MSCOCO and
PASCAL-SENTENCE datasets, respectively,
with minimal performance loss or increased
computational demands.
1 Introduction
Vision-language models have shown promising
results in tasks such as classification (Li et al.,
2023; Jia et al., 2021; Radford et al., 2021), im-
age search (Sun et al., 2023; Radford et al., 2021;Li et al., 2023), and object detection (Kuo et al.,
2023; Li et al., 2022) by training on large-scale
image-text pairs to understand the correspondences
between cross-modal image features and language
features. Models trained on extensive datasets ex-
hibit excellent zero-shot capabilities (Radford et al.,
2021; Yu et al., 2022; Li et al., 2022; Zhang et al.,
2022a) but also risk discovering and exploiting
societal biases present in the underlying image-
text pair corpora, potentially introducing bias that
leads to social unfairness (Zhao et al., 2017). The
revelation, measurement, and understanding of bi-
ases within models (Zhou et al., 2022; Zhang
et al., 2022b; Lee et al., 2023; Vig et al., 2020)
has sparked widespread research interest and are
crucial for bias mitigation (Zhang et al., 2022b;
Seth et al., 2023; Dehdashtian et al., 2023). How-
ever, most contemporary methods, derived from
language models, lack standardized metrics for
evaluating bias and primarily assess the correla-
tion between the outputs of classifiers and external
attributes (Zhang et al., 2022b). Barrett et al.
(2019) noted that interpretations based on classifier
outputs can be factually inaccurate and not gener-
alizable. While these methods can highlight the
impacts of certain contributions on model outputs,
they (1) fail to comprehend the generation and flow
of bias within the model and (2) do not understand
the causal roles of model components in the gener-
ation and propagation of bias. Consequently, they
are not able to provide clear guidance on how to
effectively mitigate bias at the model level.
In this work, we propose a standardized frame-
work to measure bias in VLMs, providing a com-
prehensive understanding of how bias flows within
the entire model structure. Specifically, we use the
GLIP model (Li et al., 2022) as a case study, focus-
ing on gender bias in the task of object detection,
which is a predominant and challenging problem in
computer vision. We conduct the analysis on both
the MS-COCO (Lin et al., 2014) and PASCAL-arXiv:2407.02814v2  [cs.AI]  7 Oct 2024SENTENCE (Rashtchian et al., 2010) datasets. We
observe that GLIP model exhibits unbalanced infer-
ence capabilities on different genders, with certain
indoor object categories like pets more likely to be
associated with females and outdoor objects like ve-
hicles with males. To holistically understand how
the bias flows in the model, we adapt causal media-
tion analysis (Vig et al., 2020) to VLMs, providing
a finer-grained study of contributions from different
model components. We find that, among the differ-
ent model components (text module, image module,
and fusion module that combines them), the image
module contributes the most to the model’s bias
– over twice as much as the text module. In the
MSCOCO and PASCAL-SENTENCE datasets, im-
age features accounted for 32.57% and 12.63% of
the bias generated, respectively, compared to ap-
proximately 15.48% and 5.64% by text features.
Also, the interaction and updating process between
image and text features during the deep fusion pro-
cess significantly impacts bias production, account-
ing for about 57% of the contributions in the image
and text encoders.
Furthermore, by integrating interventions across
different modules, we discovered that their con-
tributions to bias are aligned and do not conflict,
allowing us to prioritize bias mitigation efforts
within the image encoder, which is the most sub-
stantial contributor to bias. Based on the results, we
propose to effectively mitigate the bias in VLMs:
reducing biases from the image module can suc-
cessfully reduce bias by 22.03% on the MSCOCO
dataset and 9.04% on the PASCAL-SENTENCE
dataset, compared to a reduction of 7.08% and
1.18% in the text module. In summary, the con-
tributions of our work are:
•We provide a comprehensive evaluation of the
bias in VLMs, with an understanding of the
contribution from each model module, which
is missing in the literature.
•We analyze the correlation between the biases
from different modules and discover that the
bias in different modules are aligned and do
not conflict with each other.
•We propose an effective bias mitigation strat-
egy to reduce the bias from the module that
contributes most to the model bias when fac-
ing a limited budget.2 Related Work
In recent years, vision-language models (VLMs)
have experienced rapid advancements. The latest
developments in VLMs often employ a dual-stream
architecture that separately encodes text and im-
ages (Kim et al., 2021), and these are then merged
and aligned to facilitate cross-modal understanding
of visual and linguistic features (Radford et al.,
2021). Furthermore, some studies treat the joint
training of images and text as a phrase localization
process, aiming to better align and integrate visual
and linguistic features (Li et al., 2022). Typically,
these models are trained on image-text pairs from
datasets such as MSCOCO (Lin et al., 2014), VQA
(Antol et al., 2015), OpenImages (Kuznetsova
et al., 2020), and Flickr30k Entities (Plummer
et al., 2015), achieving impressive results in various
downstream tasks including image classification
(Radford et al., 2021), image generation (Radford
et al., 2021), visual question answering (Li et al.,
2018; Antol et al., 2015), and image captioning
(Lu et al., 2019; Alayrac et al., 2022).
Alongside their development, the societal bi-
ases exhibited by VLMs have also attracted sig-
nificant attention. These models often reflect soci-
etal stereotypes and may even amplify such biases
(Zhou et al., 2022). Most contemporary research
addressing bias in VLMs has borrowed methodolo-
gies from language model studies. For instance,
Srinivasan and Bisk (2021) utilized a language
masking model to explore gender biases by using
templates containing a specific entity and analyzing
the probabilities of masked entities (Kurita et al.,
2019). Some researchers have examined biases
through the comparison of factual and counterfac-
tual inputs, with Zhang et al. (2022b) investigating
biases by examining predicted probabilities from
both factual and counterfactual inputs, and Howard
et al. (2024) using the Perspective API to score pre-
dictions derived from such inputs to study model
biases.
However, existing evaluation methods primarily
observe changes in the probability scores of model
outputs following interventions on input samples.
This approach limits our understanding of the un-
derlying causes of bias generation and propagation
within model components (Barrett et al., 2019).
Therefore, we propose a standardized framework
for evaluating bias in vision-language tasks and
introduce causal mediation analysis (Robins and
Greenland, 1992; Pearl, 2022; Vig et al., 2020)within the context of vision-language models. This
methodology helps us understand the pathways of
bias generation and propagation from the input
level to model components.
3 Bias Measurement and Understanding
in VLM
In this section, we propose a bias evaluation metric
to assess the bias of VLM in the object detection
task. By applying causal mediation analysis, we
quantify the contribution on bias from various com-
ponents within the model pipeline which helps us
trace the origins and propagation of bias throughout
the model pipeline. Additionally, we investigate
the interactions between different modalities to un-
derstand how they collectively influence model bias
which will be used as guidance for bias mitigation
later.
3.1 Bias Evaluation Metrics
In the literature, there have been various method-
ologies proposed to measure bias, including no-
table contributions from Zhao et al. (2017), Wang
and Russakovsky (2021) and Zhao et al. (2023).
These studies often assess bias amplification by
comparing statistics between the training dataset
and model outputs, where the models are trained
and tested on similarly distributed datasets. In
contemporary settings, most VLMs undergo train-
ing on extensive collections of image and text cor-
pora. In real-world applications, users may fine-
tune a model on a dataset specific to a downstream
task. The combination of fine-tuning data and pre-
training data can introduce noise, complicating the
statistics of previously mentioned bias evaluations.
Additionally, many pre-training datasets used for
large-scale models are either difficult to access
or require significant computational resources for
analysis, making existing evaluations challenging
to deploy in modern settings.
Notably, recent advancements in VLM have
demonstrated impressive zero-shot performance,
enabling models to make accurate predictions on
benchmark datasets without any fine-tuning (Rad-
ford et al., 2021; Yu et al., 2022; Li et al., 2022;
Zhang et al., 2022a). In our study, we explore a
zero-shot scenario where VLMs are directly tasked
with making predictions on a benchmark dataset
without any fine-tuning.
Drawing inspiration from observations in Zhao
et al. (2017), where females typically correlate
zMediator
-Muscle relaxation
-Model components
Treatment
-Training
-InterventionsOutcomes
-Athletic Performance
-∆𝐵𝑖𝑎𝑠x yFigure 1: Causal Mediation Analysis example. In this
example, strength training serves as the treatment, aim-
ing to improve athletic performance, while muscle re-
laxation acts as the mediator that indirectly affects the
athletic performance. In our study, interventions in the
input module directly affect gender bias in model predic-
tions, while model components such as specific layers
or neurons can mediate this effect.
more closely with indoor objects than males, we
introduce the definition of BIAS VLwhich captures
model’s underlying correlations between sensitive
attributes (e.g., genders) and objects:
BIAS VL:=X
object|C(object, male )
−C(object, female )| (1)
where C(x, y)measures the correlation between
xandy. In our case, we use a false positive rate
(FPR) to describe the correlation, which measures
how often one specific gender ycan trigger a model
to incorrectly predict one object xin the image.1
3.2 Causal Mediation Analysis Method
Causal mediation analysis measures how a treat-
ment effect influences an outcome either directly or
indirectly through a mediator variable (Robins and
Greenland, 1992; Vig et al., 2020; Robins, 2003;
Pearl, 2022). An illustrative example is shown in
Figure 1, where athletes engage in strength training
to improve athletic performance. After training,
they need muscle relaxation to alleviate soreness,
which also impacts performance. Thus, strength
training can have a direct effect on athletic perfor-
mance through its intended mechanisms and an
indirect effect through muscle relaxation.
In our study, the treatment consists of interven-
tions on the input module, while the mediator could
be any model component or finer-grained layer or
1Following existing work, we also consider binary gender
in this study.neuron we are interested in and the outcome is the
change in gender bias in the model’s prediction
results. Therefore, we define three types of inter-
vention: a) replace-gender , which replaces the
gender word man orwoman to a gender-neutral
word person in the text of the input module; b)
mask-gender , where pixels corresponding to a per-
son in the image module are masked, thus remov-
ing gender information from the input images; and
c)null , which leaves the original text and image
modules unchanged.
We perform causal mediation analysis on the
GLIP model by introducing interventions in the in-
put module and observing changes in BIAS VLval-
ues defined in Eq. (1). Following Vig et al. (2020),
we define the Direct Effect (DE) as changes in the
BIAS VLscore when the intervention is applied to
the input module while the mediator (model com-
ponents) remains in the ‘ null ’ state of intervention.
TheIndirect Effect (IE) represents changes in the
bias score when the input module is fixed, but the
mediator is set in the state of a certain interven-
tion. We can select any model structure of inter-
est as the mediator and choose ‘mask-gender’,
‘replace-gender’ , or combinations of them as
interventions in the input module (Figure 2).
4 Experimental Setup of Bias
Measurement and Understanding
Model For the object detection task, we em-
ployed the GLIP model pre-trained on the O365,
GoldG, CC3M, and SBU datasets (Li et al., 2022).
The model consists of an image module, a text
module, and a deep-fusion module that updates
and aligns image features and text features. For ob-
ject detection, the GLIP model makes predictions
based on the given image and a text input, which
is a list of possible categories separated by com-
mas. This input format, as highlighted by Li et al.
(2022), has demonstrated good performance. For
example, in the case of the COCO dataset, the text
input along with an image will be a text prompt like
"teddy_bear . handbag . fork ...... . baseball_glove
. skateboard .", as illustrated in Figure 3.
Dataset Our experiments were conducted on
the MSCOCO and PASCAL-SENTENCE datasets.
For MSCOCO, we follow the setting in Zhao et al.
(2017), where we only consider 66 objects that ap-
pear with man or woman more than 100 times in
the training data. For the PASCAL-SENTENCE
dataset, which includes 20 categories but lacks gen-der labels, we annotated gender based on the five
captions associated with each image. An image
is labeled as male if any caption mentions “male,
males, man, men, boy, boys” and as female if
any caption mentions “female, females, woman,
women, girl, girls”. Images that do not include any
person or mention both genders were excluded.
Interventions on image encoder and text en-
coder Initially, we implement replace-gender
andmask-gender interventions on the inputs re-
spectively without any alterations to the model com-
ponents. By monitoring the changes in the values
ofBIAS VL, the individual impacts of image and
text inputs on gender bias within the input module
were assessed. Subsequently, we conducted a de-
tailed causal mediation analysis on the text encoder
and image encoder. Previous studies (Vaswani,
2017; Vig and Belinkov, 2019) have emphasized
the critical roles that attention heads and layers play
in deep learning models. Therefore, we selected at-
tention heads from these two modules as mediators.
Specifically, we chose the attention heads within a
specific layer, along with those in all preceding lay-
ers as mediators, and conducted experiments from
shallow to deep layers. This analysis aimed to
identify whether the text encoder or image encoder
contributes more significantly to gender bias and to
determine which layers in the model are principally
responsible for bias generation. It also sought to
understand how bias flows and accumulates across
different layers within the encoders. Then, we se-
lected a combination of attention layers from both
the image encoder and text encoder as mediators to
observe changes in bias and compare these results
with previous findings, exploring whether different
modalities reinforce bias or conflict in the direction
of bias.
Interventions on deep-fusion encoder In the
deep fusion encoder, where image and text fea-
tures dynamically interact and are updated, we im-
plement replace-gender andmask-gender inter-
ventions in the input module to control the state
of image and text features within the deep fusion
module. We also select the attention heads within
a specific layer and all preceding layers’ attention
heads as the mediator for conducting causal media-
tion analysis. By observing changes in the values
ofBIAS VL, we explore how image and text fea-
tures individually affect the deep fusion process
and subsequently influence bias generation.Original Input                         
module𝒛𝒚𝑶(a) Baseline
Input module 
with intervention𝒛𝒚E (b) Intervention Effect
E:=yE−yO
Input module 
with intervention𝒛𝒚𝑫(c) Direct Effect
DE :=yD−yO
Original Input                         
module𝒛𝒚𝑰(d) Indirect Effect
IE :=yI−yO
Figure 2: Bias understanding with causal mediation analysis. In the diagram, zrepresents the mediator, and yO,
yE,yD,yIrepresent the bias values of the model’s output under various interventions. The intervention effect
quantifies the change in the bias score under the specified intervention; the direct effect quantifies the change in
bias score resulting from an intervention in the input module while maintaining the mediator in the state of a null
intervention; the indirect effect measures the change in the bias score when the input module remains unchanged,
but the mediator is set to the state of a specific intervention.
5 Results
5.1 Bias Measurement
We present the results of BIAS VLin Table 1, for
the MSCOCO dataset, without any intervention
on the inputs, the BIAS VLmeasured was 1.434.
To highlight the significance of this bias, we ran-
domly divided subsets composed of male images
into two equal parts, achieving an BIAS VLof
0.278. Similarly, dividing female image subsets
randomly resulted in an BIAS VLof 0.359. Both
results are significantly lower than 1.434, and com-
parable results were observed with the PASCAL-
SENTENCE dataset, as detailed in Table 1. The
results in the random division demonstrate that a
model with balanced inference capabilities across a
dataset would yield minimal BIAS VLvalues when
divided into equal subsets (i.e., the gender stays
the same). However, when model predictions are
influenced by attributes such as gender, splitting
the dataset based on such attributes leads to higher
BIAS VLvalues.
We also provide detailed statistics of False Pos-
itive Rate (FPR) scores for various objects in the
PASCAL-SENTENCE dataset, presented in Fig-
ure 4. Our statistics reveal that a significant portion
of indoor objects, such as furniture and pets, exhibit
higher FPRs in images of females than in those of
males. Conversely, outdoor objects, such as vehi-cles, tend to have higher misclassification rates in
images of males. These findings suggest that the
model more closely associates females with indoor
objects. The FPR scores for different objects on
the MSCOCO dataset are included in the appendix.
Dataset B IAS VL BIAS VL(M1,M2) B IAS VL(F1,F2)
MSCOCO 1.434 0.278 0.359
PASCAL-S 1.369 0.341 0.381
Table 1: BIAS VLfor MSCOCO and PASCAL-
SENTENCE (PASCAL-S) Datasets without any inter-
vention. M and F stand for “male” and “female” respec-
tively. BIAS VLvalues obtained in two sets of images
with the same gender are significantly lower than the
BIAS VLobtained from datasets divided by gender.
5.2 Bias Understanding with Causal
Mediation Analysis
We conduct the causal mediation analysis on differ-
ent modules to study their effect on the model bias.
We find that the image module influences the model
bias more than the text module and the fusion mod-
ule. In addition, we show that the bias in the image
and text modules are aligned – they are showing
similar gender bias tendencies rather than conflict-
ing ones. At this stage, we used four Quadro RTX
6000 GPUs. Since we primarily focus on inference
tasks rather than training or fine-tuning, the com-
putational resource requirements are not intensive.Output
PredictionInput Model
Text 
Encoder
Image 
Encoderteddy_bear . handbag . 
fork . ...... . skateboard .captionDeep Fusion
Block
cat .
woman . 
Figure 3: GLIP object detection pipeline. The input to the GLIP model consists of an image and a caption, which
includes a list of possible categories separated by commas.
bird
cat
cow
dog
horse
sheep
aeroplane
bicycle
boat
bus
car
motorbike
train
bottle
chair
dining table
potted plant
sofa
tv/monitor0.00.10.20.3 FPRFemale
Male
Figure 4: False Positive Rate (FPR) for various objects
in the PASCAL-SENTENCE dataset. For most pets and
indoor objects, the FPR is higher in images of females
than in those of males; conversely, for most outdoor
objects such as vehicles, the FPR is higher in images of
males than in those of females. These results indicate
that females correlate more closely with indoor objects
than males.
Image encoder Applying the mask-gender in-
tervention to the input image module reduced the
BIAS VLto 0.967 for the MSCOCO dataset and to
0.664 for the PASCAL-SENTENCE dataset, rep-
resenting reductions of approximately 32.57% and
12.63%, respectively. We employed the attention
heads in the image encoder as the mediator
to examine both the indirect effects of this
model component and the direct effects of themask-gender on predictions. Figure 5a and Fig-
ure 5e illustrate that employing more attention
heads as mediators leads to an increase in the value
of the indirect effect, while the direct effect dimin-
ishes. This supports an intuition that removing
gender information from more layers in the image
encoder weakens the model’s dependency on latent
correlations between gender in images and specific
objects, thus mitigating gender bias in predictions.
Furthermore, while interventions at the input level
significantly impact final predictions, targeting the
image encoder alone achieves about 53% of the
mask-gender effect.
Text encoder Implementing a replace-gender
intervention on the input text module reduced the
BIAS VLto 1.212 for the MSCOCO dataset and to
0.720 for the PASCAL-SENTENCE dataset, reduc-
tions of approximately 15.48% and 5.64%, respec-
tively. We chose the attention heads within the text
encoder as the mediator in this case. As shown in
Figure 5b and Figure 5f, similar to the image en-
coder insights, removing gender information from
multiple layers in the text encoder substantially de-
creases the model’s reliance on latent correlations
between gender in text and specific objects, thereby
reducing prediction biases. The replace-gender
intervention led to a smaller reduction in bias com-
pared to mask-gender , emphasizing the more sub-
stantial role of images in generating gender bias
relative to text. This outcome is likely influenced
by the simplistic structure of the input text used in
our study, which adheres to the format described in1 2 3 4
Fixed Layer Chunk Index0.150.200.250.300.350.400.450.500.55Bias Score
DE
IE(a) image encoder
(COCO)
12345678910
Fixed Layer Chunk Index0.160.170.180.190.200.210.22Bias Score
DE
IE(b) text encoder
(COCO)
1 2 3 4 5 6
Fixed Layer Chunk Index0.1840.1860.1880.1900.1920.1940.1960.1980.200Bias Score
DE
IE(c) text part of deep-fusion
encoder (COCO)
1 2 3 4 5 6
Fixed Layer Chunk Index0.100.150.200.250.300.350.400.45Bias Score
DE
IE(d) image part of deep-fusion
encoder (COCO)
1 2 3 4
Fixed Layer Chunk Index0.000.020.040.060.080.100.120.14Bias Score
DE
IE
(e) image encoder
(PASCAL-S)
12345678910
Fixed Layer Chunk Index0.000.010.020.030.040.050.06Bias Score
DE
IE(f) text encoder
(PASCAL-S)
1 2 3 4 5 6
Fixed Layer Chunk Index0.000.010.020.030.040.050.06Bias Score
DE
IE(g) text part of deep-fusion
encoder (PASCAL-S)
1 2 3 4 5 6
Fixed Layer Chunk Index0.000.020.040.060.080.100.120.14Bias Score
DE
IE(h) image part of deep-fusion
encoder (PASCAL-S)
Figure 5: Causal mediation analysis of bias on the COCO and PASCAL-S (PASCAL-SENTENCE) datasets. Panels
(a) and (e) show the DE (Direct Effect) and IE (Indirect Effect) for the image module; Panels (b) and (f) represent
the DE and IE for the text module; Panels (c) and (g) illustrate the DE and IE for the text part of the deep-fusion
encoder, and panels (d) and (h) for the image part of the deep-fusion encoder. The findings highlight that image
features contribute more significantly to bias than text features, with the image module being the primary contributor
to model bias.
original GLIP experiments (Li et al., 2022), sepa-
rating each category with a period, resulting in less
complex text features than image features. Lan-
guage models typically capture basic features such
as syntactic structures at shallow layers and more
complex semantic information at deeper layers, cor-
relating with the significant changes in BIAS VL
observed at the sixth layer.
Deep fusion encoder To further validate whether
image features contribute more to bias genera-
tion than text features, we utilized the attention
heads in the deep fusion encoder as the media-
tor, adjusting the attention heads’ parameters in
the states of either mask-gender intervention or
replace-gender intervention. The results dis-
played in Figure 5d and Figure 5c show that for
the MSCOCO dataset, the indirect effects from
mask-gender and replace-gender through the
deep fusion encoder are up to 0.260 and 0.189, re-
spectively, reducing the BIAS VLby approximately
18.13% and 13.18%. For the PASCAL-Sentence
dataset, the reductions are 10.80% and 0.53%, re-
spectively (Figure 5h and Figure 5g). These find-
ings reaffirm our conclusion that image features
play a more substantial role in bias generation than
text features. They also suggest that even though
the deep fusion module does not extract features
directly from images and text, the interactive up-dating process between text and image features
significantly influences bias generation, accounting
for approximately 55.70% of the effect observed
with the encoder alone.
25% 50% 75% 100%
Proportion of Fixed Layers0.000.050.100.150.200.250.30 Bias Score
L
V
L+V
(a) MSCOCO
25% 50% 75% 100%
Proportion of Fixed Layers0.000.020.040.060.080.100.12 Bias Score
L
V
L+V (b) PASCAL-SENTENCE
Figure 6: Comparison of Bias Reduction Across Modali-
ties with Interventions in Vision (V) and Language Mod-
ules (L) on the MSCOCO and PASCAL-SENTENCE
datasets. Vrepresents the results of interventions in
the vision modality, Lrepresents the results of inter-
ventions in the language modality, and L+V represents
the results of simultaneous interventions in both the vi-
sion and language modalities. The contributions to bias
from the two modalities are aligned and non-conflicting.
Intervening simultaneously in both the visual and lan-
guage modalities results in a greater reduction of bias
compared to interventions in any single modality alone.
Interventions comparison Multi-modal models
consist of various interacting modules, each of
which can learn distinct biases. However, the
current literature does not thoroughly investigate
whether these biases are aligned or disparate acrossdifferent modules. In this section, we conduct
an empirical analysis in VLMs to address this
question. We simultaneously intervene in both
the vision and language modalities. We apply
replace-gender andmask-gender interventions
to the input module and select a consistent propor-
tion of attention heads in both the image encoder
and text encoder as mediators. This setup allows us
to observe changes in BIAS VLand compare these
with the changes induced by interventions in single
modalities. Figure 6a and Figure 6b demonstrate
that combined interventions on both images and
text achieve greater bias reduction than interven-
tions on either alone. However, the total reduction
is not merely additive; the overall bias reduction is
less than the sum of the individual contributions.
6 Bias Mitigation Method
Based on our experimental results, image features
contribute most significantly to gender bias and the
image encoder has a more pronounced impact on
bias compared to the text encoder and deep-fusion
encoder. Therefore, our intuition is that focusing
on reducing gender representation in the image en-
coder will effectively reduce bias, especially when
facing a computation budget. We use the bias miti-
gation achieved from the text encoder as a baseline,
then focus on reducing bias from the image encoder
and compare the results with the baseline.
Text Encoder For the text encoder, we aim to
blur the gender representation in text features. We
modify the structure of the text encoder to first
identify gender-related terms (man, woman, men,
women, male, female, males, females) in the in-
put text. A new sentence is generated by replac-
ing these gendered terms with their corresponding
anti-gender terms ( i.e., man towoman ,male to
female ). The text encoder’s output features are
the average of the original sentence’s text features
and the anti-gender sentence’s text features. Since
the only difference between the two sentences is
the gendered terms, this approach effectively blurs
gender representation within the text encoder. We
then let model to make predictions and observe the
reduction in B IAS VL.
Image Encoder Similarly, for the image en-
coder, we aim to blur gender representation in
image features. To achieve this, we incorporate
MTCNN (Zhang et al., 2016) as a face detector
and MobileNet (Sandler et al., 2018) as a genderclassifier into the existing image encoder frame-
work. Both networks are lightweight, allowing
their integration without significantly increasing
the computational load during inference. When
an image is input into the image encoder, the
MTCNN (Zhang et al., 2016) network first identi-
fies potential faces and outlines them with bound-
ing boxes. MobileNet (Sandler et al., 2018) then
classifies the gender of the faces within these boxes.
We have prepared a male face image and a fe-
male face image in advance. Depending on the gen-
der predicted by MobileNet (Sandler et al., 2018),
we replace the face in the bounding box with the
corresponding pre-prepared anti-gender face im-
age. The final image features output by the im-
age encoder are an average of the original image
features and the features of the newly introduced
anti-gender face. This method effectively blurs the
original gender representation in the image. Then
we let the model to make predictions and observe
the reduction in B IAS VL.
7 Experimental Setup of Bias Mitigation
Model We utilized the GLIP model, pre-trained
on the O365, GoldG, CC3M, and SBU datasets
(Li et al., 2022). In our setup, we incorporated an
MTCNN (Zhang et al., 2016) pre-trained on the
Wider Face and CelebA datasets as a face detec-
tor within the image encoder. Additionally, we
integrated a MobileNet (Sandler et al., 2018) pre-
trained on ImageNet to serve as a gender classifier.
Dataset We evaluated the effectiveness of bias
mitigation on the MSCOCO and PASCAL-
SENTENCE datasets. To assess the model’s object
detection performance, we compared it with the
original GLIP (Li et al., 2022) on the MSCOCO
and LVIS datasets using the AP (Average Precision)
metric for zero-shot object detection.
8 Results
As indicated in Table 2, blurring gender represen-
tations in the image encoder demonstrated signif-
icant bias mitigation on both the MSCOCO and
PASCAL-SENTENCE datasets. The experimental
findings suggest that obscuring gender information
in the image encoder is more effective at reducing
model bias compared to similar interventions in
the text encoder. Our results show that by blurring
gender representations in the image features within
the image encoder, we effectively reduced model
bias by 22.03% and 9.04% on the MSCOCO andAP Bias Bias Mitigated
MSCOCO LVIS MSCOCO PASCAL-S MSCOCO PASCAL-S
GLIP 46.6 17.6 1.434 0.763 0 0
GLIP_ImageFair 46.2 17.3 1.118 0.694 22.03% 9.04%
GLIP_TextFair 46.6 17.6 1.322 0.754 7.8% 1.18%
Table 2: Performance comparison of different methods. AP (Average Precision) is the metric used for zero-shot
object detection. “ GLIP ” represents the original GLIP model, “ GLIP_ImageFair ” denotes the model with bias
mitigation implemented in the image encoder, and “ GLIP_TextFair ” refers to the model with bias mitigation
applied in the text encoder. Intervention in the image encoder is more effective than the text encoder in reducing the
bias score without significant performance loss.
PASCAL-SENTENCE datasets, respectively, with
minimal impact on model performance.
9 Conclusion
Vision-language models (VLMs) trained on large-
scale image-text pair corpora are at risk of learning
social biases from their training data. In this paper,
we introduced a standardized framework incorpo-
rating causal mediation analysis to measure and
understand the pathways through which model bias
is generated and propagated within VLMs. We dis-
covered that image features contribute significantly
more to model bias than text features, and the con-
tributions from the image encoder substantially ex-
ceed those from the text encoder and deep fusion
encoder. Furthermore, the contributions to bias
from different modalities reinforce each other. Sub-
sequently, by focusing on the components that con-
tribute most to bias, we efficiently reduced model
bias.
Our work provides a framework for measuring,
understanding, and mitigating model bias, which,
although utilized here within the realm of object
detection, can be extended to a wide range of VLM
tasks. We used gender bias as a case study to show-
case our methodology, as many influential stud-
ies in the natural language processing and fairness
fields have begun with the study of gender bias
(Bolukbasi et al., 2016; Zhao et al., 2019, 2017;
Zhang et al., 2022b). Gender bias is one of the most
representative types of bias, and understanding it
can provide valuable insights for studying other
forms of bias. The frameworks and evaluation met-
rics we developed for gender bias are generalizable
and can be extended to other biases. For instance,
if ground truth labels about age are available in
the image data, researchers can modify the text de-
scriptions to include age-related terms and applythe same methodology to study age-related bias.
However, our framework is primarily applicable to
white-box models, as it requires interventions at
the internal components of the model. A promising
direction for future work would involve expanding
our framework to encompass additional modali-
ties such as audio or video. This expansion could
further enhance our understanding of multimodal
interactions and their impact on bias, as well as
deepen insights into how different sensory inputs
contribute to, or mitigate, biases in AI systems.
10 Limitations
Our work provides a framework for measuring,
understanding, and mitigating model bias in vision-
language models (VLMs), with broad applicabil-
ity across various VLM tasks. However, our ap-
proach primarily applies to white-box models, as it
requires interventions within the model’s internal
components. Consequently, this limitation implies
that our methods might not be directly applicable
to scenarios where model internals are inaccessible
or when dealing with black-box systems.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in neural
information processing systems , 35:23716–23736.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015. Vqa: Visual question answering.
InProceedings of the IEEE international conference
on computer vision , pages 2425–2433.
Maria Barrett, Yova Kementchedjhieva, Yanai Elazar,
Desmond Elliott, and Anders Søgaard. 2019. Adver-
sarial removal of demographic attributes revisited. InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 6330–6335.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016. Man
is to computer programmer as woman is to home-
maker? debiasing word embeddings. Advances in
neural information processing systems , 29.
Sepehr Dehdashtian, Lan Wang, and Vishnu Boddeti.
2023. Fairvlm: Mitigating bias in pre-trained vision-
language models. In The Twelfth International Con-
ference on Learning Representations .
Phillip Howard, Anahita Bhiwandiwalla, Kathleen C
Fraser, and Svetlana Kiritchenko. 2024. Uncovering
bias in large vision-language models with counterfac-
tuals. arXiv preprint arXiv:2404.00166 .
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen
Li, and Tom Duerig. 2021. Scaling up visual and
vision-language representation learning with noisy
text supervision. In International conference on ma-
chine learning , pages 4904–4916. PMLR.
Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:
Vision-and-language transformer without convolu-
tion or region supervision. In International confer-
ence on machine learning , pages 5583–5594. PMLR.
Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergio-
vanni, and Anelia Angelova. 2023. F-vlm: Open-
vocabulary object detection upon frozen vision and
language models. In International Conference on
Learning Representations (ICLR) .
Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,
and Yulia Tsvetkov. 2019. Measuring bias in con-
textualized word representations. arXiv preprint
arXiv:1906.07337 .
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,
Stefan Popov, Matteo Malloci, Alexander Kolesnikov,
et al. 2020. The open images dataset v4: Unified
image classification, object detection, and visual re-
lationship detection at scale. International journal of
computer vision , 128(7):1956–1981.
Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel
Cahyawijaya, Wenliang Dai, and Pascale Fung. 2023.
Survey of social bias in vision-language models.
arXiv preprint arXiv:2309.14381 .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In International conference on ma-
chine learning , pages 19730–19742. PMLR.
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang,
Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan
Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.2022. Grounded language-image pre-training. In
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 10965–
10975.
Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo
Luo. 2018. Vqa-e: Explaining, elaborating, and en-
hancing your answers for visual questions. In Pro-
ceedings of the European Conference on Computer
Vision (ECCV) , pages 552–567.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
2019. Vilbert: Pretraining task-agnostic visiolinguis-
tic representations for vision-and-language tasks. Ad-
vances in neural information processing systems , 32.
Judea Pearl. 2022. Direct and indirect effects. In Prob-
abilistic and causal inference: the works of Judea
Pearl , pages 373–392.
Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. In Proceedings of the IEEE
international conference on computer vision , pages
2641–2649.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon’s mechanical turk. In Proceedings of
the NAACL HLT 2010 workshop on creating speech
and language data with Amazon’s Mechanical Turk ,
pages 139–147.
James M Robins. 2003. Semantics of causal dag models
and the identification of direct and indirect effects.
Highly structured stochastic systems , pages 70–82.
James M Robins and Sander Greenland. 1992. Identi-
fiability and exchangeability for direct and indirect
effects. Epidemiology , 3(2):143–155.
Mark Sandler, Andrew Howard, Menglong Zhu, An-
drey Zhmoginov, and Liang-Chieh Chen. 2018. Mo-
bilenetv2: Inverted residuals and linear bottlenecks.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 4510–4520.Ashish Seth, Mayur Hemani, and Chirag Agarwal. 2023.
Dear: Debiasing vision-language models with addi-
tive residuals. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition ,
pages 6820–6829.
Tejas Srinivasan and Yonatan Bisk. 2021. Worst of both
worlds: Biases compound in pre-trained vision-and-
language models. arXiv preprint arXiv:2104.08666 .
Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang
Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and
Jiaqi Wang. 2023. Alpha-clip: A clip model fo-
cusing on wherever you want. arXiv preprint
arXiv:2312.03818 .
A Vaswani. 2017. Attention is all you need. Advances
in Neural Information Processing Systems .
Jesse Vig and Yonatan Belinkov. 2019. Analyzing
the structure of attention in a transformer language
model. arXiv preprint arXiv:1906.04284 .
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. 2020. Investigating gender bias in language
models using causal mediation analysis. Advances
in neural information processing systems , 33:12388–
12401.
Angelina Wang and Olga Russakovsky. 2021. Direc-
tional bias amplification. In International Confer-
ence on Machine Learning , pages 10882–10893.
PMLR.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-
ung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.
Coca: Contrastive captioners are image-text foun-
dation models. Transactions on Machine Learning
Research .
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-
Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang,
Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
2022a. Glipv2: Unifying localization and vision-
language understanding. Advances in Neural Infor-
mation Processing Systems , 35:36067–36080.
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and
Yu Qiao. 2016. Joint face detection and alignment
using multitask cascaded convolutional networks.
IEEE Signal Processing Letters , 23(10):1499–1503.
Yi Zhang, Junyang Wang, and Jitao Sang. 2022b. Coun-
terfactually measuring and eliminating social bias
in vision-language pre-training models. In Proceed-
ings of the 30th ACM International Conference on
Multimedia , pages 4996–5004.
Dora Zhao, Jerone Andrews, and Alice Xiang. 2023.
Men also do laundry: Multi-attribute bias amplifica-
tion. In International Conference on Machine Learn-
ing, pages 42000–42017. PMLR.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,
Vicente Ordonez, and Kai-Wei Chang. 2019. Gen-
der bias in contextualized word embeddings. arXiv
preprint arXiv:1904.03310 .
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente
Ordonez, and Kai-Wei Chang. 2017. Men also
like shopping: Reducing gender bias amplifica-
tion using corpus-level constraints. arXiv preprint
arXiv:1707.09457 .
Kankan Zhou, Yibin LAI, and Jing Jiang. 2022. Vl-
stereoset: A study of stereotypical bias in pre-trained
vision-language models. Association for Computa-
tional Linguistics.
A Appendixteddy_bearhandbagforkcakebedumbrellaspoongiraffebowlknifewine glassdining tablecatsinkcuppotted plantrefrigeratormicrowavecouchovensandwichbookbottlecell phonepizzabananatoothbrushtennis racketchairdogdonutsuitcaselaptophot dogremoteclockbenchtvmousehorsefire hydrantkeyboardtoilettraffic lightsports ballbicyclecarbackpacktrainkitecowskistruckelephantboatfrisbeeairplanemotorcyclesurfboardtiesnowboardbaseball batbaseball gloveskateboard0.000.050.100.150.20 FPRFemale
MaleFigure 7: False Positive Rate (FPR) for various objects in the MSCOCO dataset. For most indoor objects, the FPR
is higher in images of females than in those of males; conversely, for most outdoor objects such as vehicles, the FPR
is higher in images of males than in those of females. These results indicate that females correlate more closely
with indoor objects than males.