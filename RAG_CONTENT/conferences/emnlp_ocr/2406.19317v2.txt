Jump Starting Bandits with LLM-Generated Prior Knowledge
Parand A. Alamdari*
University of Toronto
& Vector Institute
parand@cs.toronto.eduYanshuai Cao
Borealis AI
{yanshuai.cao, kevin.h.wilson}@borealisai.comKevin H. Wilson
Borealis AI
Abstract
We present substantial evidence demonstrat-
ing the benefits of integrating Large Lan-
guage Models (LLMs) with a Contextual Multi-
Armed Bandit framework. Contextual ban-
dits have been widely used in recommendation
systems to generate personalized suggestions
based on user-specific contexts. We show that
LLMs, pre-trained on extensive corpora rich in
human knowledge and preferences, can simu-
late human behaviours well enough to jump-
start contextual multi-armed bandits to reduce
online learning regret. We propose an initializa-
tion algorithm for contextual bandits by prompt-
ing LLMs to produce a pre-training dataset of
approximate human preferences for the bandit.
This significantly reduces online learning re-
gret and data-gathering costs for training such
models. Our approach is validated empirically
through two sets of experiments with differ-
ent bandit setups: one which utilizes LLMs to
serve as an oracle and a real-world experiment
utilizing data from a conjoint survey experi-
ment.
1 Introduction
When users encounter content that caters to their
needs and preferences, they are more inclined to
engage, e.g., watch a movie [ 3] or click on a news
article [ 31]. But how should a practitioner learn to
personalize a campaign’s content? Would a user
be more likely to respond if the content was pre-
sented in a formal or informal style [ 32]? Or if
it included a celebrity endorsement [ 27]? Studies
in behavioural economics [ 42,20] proved that the
answers can be heavily context-dependent [ 15,38].
Contextual Multi-Armed Bandits (CBs) [ 35,13]
were invented to address this problem in the on-
line learning setting—where an agent with no prior
knowledge of user preferences is presented with
users in a sequence, chooses a piece of content
*Work performed during an internship at Borealis AI.to show the users based on the users’ and the con-
tent’s features, and updates itself based on feedback
essentially instantaneously. The critical question
for CBs is how to balance exploration (gathering
information about users’ preferences) and exploita-
tion (utilizing the information it has gathered so
far). While these agents are known to exhibit good
asymptotic performance, their initial choices are es-
sentially random. To improve performance for the
first users in a campaign, researchers have focused
onwarm starting a CB [ 45] using detailed records
of users’ past behaviours and preferences in simi-
lar campaigns. However, collecting such datasets
poses significant challenges due to resource de-
mands, data diversity requirements, and the need
to comply with privacy regulations.
In this paper, we show that large language mod-
els (LLMs) can solve this conundrum by jump start-
ingcontextual bandits using approximate human
preference information already captured in them.
This allows practitioners to automatically design
and test context-dependent messaging with signifi-
cantly lower upfront costs.
Prior work has shown conflicting evidence
about whether LLMs faithfully simulate human be-
haviours. Argyle et al. [6]and Simmons [40] show
LLMs can simulate human behaviour and prefer-
ences in certain circumstances, while Santurkar
et al. [39] find that LLM-generated response distri-
butions may not match the true population distribu-
tion. Our key insight is that even if LLM-generated
reward distributions do not perfectly match human
preferences, they still provide a better than random
baseline than that which cold-started CBs face.
Our contributions. First, we introduce a novel
approach, Contextual Bandits with LLM Initializa-
tion (CBLI), that leverages the power of LLMs to
generate synthetic reward distributions for pretrain-
ing CBs for personalization in Algorithm 1.
Second, we empirically demonstrate the effec-
tiveness of our proposed method to pre-train a CB
1arXiv:2406.19317v2  [cs.LG]  29 Oct 2024in two settings where natural language plays a crit-
ical role in the content shown to users. The first
setting involves a standard contextual bandit whose
arms represent the styles of marketing communi-
cation soliciting charity donations from simulated
users. The second experiment uses a sleeping ban-
ditsetup with real-world human preferences. CBLI
achieves 14–17% and 19–20% reduction in early
regret, respectively. Furthermore, we show that
even when certain privacy-sensitive attributes are
withheld, CBLI still achieves an 14.8% reduction
in early regret in the second setting.
Finally, testing contextual bandit algorithms in
real-world settings usually requires collecting a
large amount of randomized log data, which can be
costly and difficult while maintaining data diversity
and privacy requirements. As an additional contri-
bution, we introduce, to the ML and NLP commu-
nity, the usage of conjoint survey experimental data
[7] as a way to benchmark bandit algorithms. Con-
joint experiment data collected in social science lit-
erature provides a substantial source of real-world
data where users were presented with several poten-
tial choices and asked to record their preferences.
Coupled with the demographic information that
is typically available from such experiments from
which contexts can be constructed, they are an ideal
candidate for testing contextual bandit algorithms.
2 Related Work
With the advent of web scale content, recommender
systems that personalized content to specific users
became an important part of navigating the internet.
Recently, deep neural networks have become more
important to these systems [ 12,14], and the need
to train these systems online and at scale has been
a prominent concern [33].
One such online training technique, multi-armed
bandits, has proven especially popular, with a
plethora of algorithms specialized to specific set-
tings (see [ 30,41] for an introduction). Most rele-
vant to this work are contextual bandits [ 44,36,29],
which introduce side information to determine the
optimal arm. In particular, Zhang et al. [45] con-
sider warm starting CBs utilizing fully supervised,
labelled data collected from human experts.
While to the best of our knowledge, conjoint sur-
vey experiments have not be used in the evaluation
of contextual bandits, conjoint analysis—the under-
lying statistical technique—has been used for fea-
ture selection in training CBs [ 31]. Outside of CBs,researchers have used ML techniques to improve
the efficiency of conjoint analysis [11, 21, 22].
Recently, researchers have attempted to integrate
LLMs into social science studies, specifically uti-
lizing LLMs to simulate human behaviours in so-
cial psychology experiments [ 17,2] and predict
their opinions [ 6,40]. Additionally, a growing
body of research has focused on evaluating LLM
performance and biases through survey questions.
Prior studies have examined how LLMs respond
to multiple-choice questions from public opinion
polls, revealing insights into their alignment with
human responses and potential biases [ 39,19,18].
Santurkar et al. [39] focus on diverse demographic
groups across the U.S., while Durmus et al. [19] fo-
cus on non-American demographics, both finding
that LLMs’ responses to surveys show discrepan-
cies with the collected answers from representative
samples. Furthermore, Dominguez-Olmedo et al.
[18] show that models’ responses are sensitive to
ordering and labelling biases, and they do not con-
tain the statistical signals typically found in human
populations.
Our work in this setting suggests that despite
potential biases and disparities in LLM-generated
responses, they provide a good starting point for
training contextual multi-armed bandits.
3 Preliminaries
Multi-Armed Bandits. In a multi-armed bandit
problem, an agent, at each step t∈[T], is pre-
sented with a set of Kpossible arms and must
choose an arm kt. After the agent chooses, they
receive a reward rt,ktdrawn from a reward dis-
tribution R(kt), which is initially unknown. We
focus on the stochastic setting, where each rt,kis
sampled independently from R(k). The goal of the
agent is to maximize the total rewardPT
t=1rt,kt.
To achieve this, the agent needs to balance the trade-
off between exploration (i.e., trying different arms
to gather more information about their reward dis-
tribution) and exploitation (i.e., choosing among
the arms that appear to offer the highest reward
based on the collected information so far).
Contextual Multi-Armed Bandits. In contex-
tual multi-armed bandits (CB), where at each time
stept∈[T], the agent receives side information
ϕt∈ C which is called the context . The reward
distribution of pulling arm k∈[K]at time tmay
depend on the context ϕt, that is, rt,k∼R(k, ϕt)
2Policy. A policy πis a map from histories Ht=
(ϕ1, k1, r1,k1, . . . , ϕ t−1, kt−1, rt−1,kt−1, ϕt)of con-
texts, arms, and realized rewards to the next arm
π(Ht) =kthe agent will choose.
Regret. The(cumulative) regret of an agent after
Tsteps choosing arms based on a policy πrelative
to a policy π∗is given by
regπ,π∗=E"TX
t=1rt,k∗
t#
−E"TX
t=1rt,kt#
where ktis chosen according to πandk∗
tis chosen
according to π∗and the expectations are taken with
respect to any randomness in the rewards, contexts,
and policy.
Sleeping bandit. In some of our applications,
we will consider sleeping bandits , where at time
t, the agent may choose between a subset of arms
At⊆[K][26]. In this case, the bandit algorithm
needs to learn and determine which arms yield the
best reward when they are available .
4 Method
Recent advances in large language models have
shown remarkable progress in understanding and
interpreting human expression [ 46]. LLMs trained
on extensive corpora rich in human knowledge pre-
serve the capacity to perform well on a diverse
array of tasks, even those dependent on human
behaviour and preferences [ 9]. In the context of
personalization, LLMs offer two key advantages.
First, they can be utilized to automatically design
user-specific content in large volumes. Second,
LLMs can be used to simulate human interactions
and predict their preferences, effectively serving
as a proxy for collecting a dataset of users’ inter-
actions. We focus on the latter in our work, and
provide a framework to utilize the power of LLMs
to generate extensive artificial user interactions to
later train our models.
4.1 Problem Formulation
We showcase our proposed approach for two dif-
ferent bandit setups: the standard contextual multi-
armed bandit (CB) and the sleeping bandit [ 26]
introduced in Section 3.
We consider a set of nusers, where the vector
of features of each user i∈[n]is denoted by Xi,
which is sampled from a population X. At each
time step t, user utis sampled from the set of nusers. The user’s context can be calculated using
a mapping function ϕ:X → C where Cis the
context space. There are Kdifferent arms, each
representing a potential recommendation for the
user. For each user utand arm k, there is a scalar
reward R(k, ut)following some unknown distribu-
tion. For brevity, we write Rthenceforth. We note
that there is an optimal policy πowhich chooses
the optimal arm k∗
t= arg maxk∈[K]E[Rt(k)]at
each time step. We seek to optimize the following
(expected, cumulative) regret:
π∗= arg min
πregπ,πo
Following the standard treatment in stochastic
contextual bandits [ 30], we use a feature map
ψ:C ×[K]→Rdthat jointly encodes context
and arms (either all arms or just the available ones
in sleeping bandits). Then we model reward us-
ingRt=⟨ψt, θ⟩, where θis the parameter to be
learned over the course of Tsteps. In our exper-
iments, we adapt the LinUCB algorithm [ 13] in
order to train these bandits. From a cold start, this
algorithm is guaranteed to yield regret of eO(√
Td)
where dis the dimension of the vector θ.
We introduce our framework “Contextual Ban-
dits with LLM Initialization (CBLI)”. We propose
to use LLMs to generate a large and diverse dataset
of users’ interactions and their preferences. We use
this dataset to pretrain a contextual bandit which
we then use in a real setting. The trained model
serves as a starting point, and later is tuned with the
data of real users’ interactions as they are collected.
The overall framework is depicted in Figure 1.
4.2 Contextual Bandit with LLM
Initialization (CBLI)
We begin by setting up a contextual bandit frame-
work where the context is the information about
each user as described in Section 4.1. First, we
generate nsynthetic users by sampling i.i.d. from
X. For each sampled user i, whose features we
denote by Xi, we compute a textual embedding
Ti=T(Xi)∈ L in some language space L.
The exact function Tis domain-dependent and
can represent arbitrary side information about a
user. For instance, a video streaming service might
takeXito be the sequence of movies the user i
has watched and transform it into the string “This
user has watched the following movies: [movie 1],
[movie 2], . . . ” These textual embeddings are then
3Figure 1: Using LLM to jump-start bandit learning: (left) pre-training using our proposed CBLI; (right) online
learning of jump-started bandit.
Algorithm 1 Generate Synthetic Preferences
1:Input: Specification of features of users, K
Arms, number of users n, an LLM M, and
is_sparse_mode
2:U← {} ,R← {}
3:fori∈[n]do
4: Sample user ii.i.d. from feature space X
5: Embed user ito the context space as ϕi
6: Describe user iwith text as Ti
7: U←U∪ {ϕi}
8: ifis_sparse_mode then
9: Ri←[0]K×K▷sparse matrix
10: A=Uniform ([K]×[K])
11: else
12: Ri←[0]K▷vector
13: A= [K]×[K]
14: end if
15: Prompt Mto adopt i’s persona given Ti
16: for(k1, k2)∈Ado
17: Prompt Mto indicate user i’s prefer-
ence between k1andk2asP
18: ifis_sparse_mode then
19: Ri[k1, k2]+=1[k1==P]
20: else
21: Ri[P]+= 1
22: end if
23: end for
24: Normalize Ri
25: R←R∪ {Ri}
26:end for
27:Output: Users’ Context: U, Rewards: R
transformed into a context ϕi=ϕ(Ti)utilizing
another LLM.
TheKarms of the bandit represent the different
options we might offer to the users or, as discussed
in Section 5.1, potential prompts to an LLM which
will generate content to send to users.
To estimate the reward distribution for pulling
each arm, given a user’s context, we propose thefollowing approach. For each generated user i,
we use an LLM Mto simulate the preferences
of user ibased on their textual representation Ti.
Specifically, for each user i, we prompt Mto adopt
the persona of user i. To determine the reward
distribution for different actions, we consider each
pair of arms (k1, k2)within the set of Karms. We
then prompt Mto indicate which arm user iwould
prefer. See Appendix C for the prompts utilized.
We repeat the process multiple times to get more
instances of answers for each pair of arms and user.
The algorithm is described in Algorithm 1.
In the case of a sleeping bandit or when the
number of arms is large, we can use the sparse
mode in Algorithm 1, where only a random subset
of pairs are sampled, and pairwise preferences are
stored. Alternatively, Algorithm 1 iterates over
all pairs of arms and records an absolute reward
per each arm based on the number of winnings in
pairwise comparisons.
We note three features of CBLI. First, we prompt
Mto rank pairs of arms as opposed to scoring arms
individually (see Appendix B for more details).
Second, we run over all pairs (k1, k2)∈[K]×[K]
(Lines 10 and 13) and not, say, all pairs (k1, k2)
withk1< k2, which should be sufficient to deter-
mine an ordering. However, it has been observed
that LLMs are sensitive to the order options are
presented [ 39], so we average over both orders to
mitigate this potential bias. Third, we note that
Ri[k]should approximate a rank ordering of per
arm rewards and may not estimate the exact reward
(see Appendix A), which is sufficient for our prob-
lem of best arm identification, though see Section 7
for further discussion.
We leverage the dataset of generated users and
their corresponding rewards to pre-train a contex-
tual multi-armed bandit using established algo-
rithms like those described in [ 35,13]. At each
stept∈[T], we sample a user utand their associ-
ated context ϕtfrom the set of ngenerated users. In
4the usual setting, the bandit then chooses an arm at
and receives reward Rt[kt]. In the sleeping bandit
setting, the bandit will be presented with two arms
(k1, k2)and will choose one to set as k1and the
other as k2. It will then receive reward Rt[k1, k2].
We note that we always have access to (the CBLI-
generated) Rtunlike training from actual log data,
where the user utmay not have been assigned to
the desired arm.
In Section 5 we show that the pre-trained bandit
model using LLM generated users and rewards sub-
stantially reduces regret in decision-making. More
importantly, utilizing LLM-generated user data and
their simulated preferences offers significant ben-
efits. It not only lowers the costs associated with
data collection but also mitigates privacy concerns,
as it does not involve real user data. CBLI provides
a cost-effective and privacy-preserving solution for
training models in personalized systems.
5 Experiments
We validate the effectiveness of our proposed ap-
proach through two sets of experiments.1In both
sets of experiments, we adapt the LinUCB algo-
rithm [ 13] to train our bandit. The first set of exper-
iments is based on synthetic data, and the second
set uses a real-world dataset derived from a choice-
based conjoint analysis [28]. Experimental details
are in Appendix C.
Language Models. To evaluate the effectiveness
of our approach, we configure our model Min
Algorithm 1 to several state-of-the-art Large Lan-
guage Models: OpenAI GPT-4o [ 1], GPT-3.5,
Claude-3 Haiku [5], and Mistral-Small [24].
5.1 First Experiment: Personalized Email
Campaign for Charity Donations
To verify the effectiveness of using LLMs to pre-
train contextual multi-armed bandits, we design
an experiment focused on a real-world application:
optimizing email campaigns for fundraising. We
aim to raise funds for a well-known humanitarian
organization that supports various causes globally.
The objective of our experiment is to send personal-
ized emails to previous donors of the organization,
encouraging them to make additional donations.
While human experts have meticulously crafted
various high-level email templates and strategies
1Code for both experiments is available at
https://github.com/BorealisAI/jump-starting-bandits.designed to maximize donation likelihood, it re-
mains uncertain which specific type of email will
resonate best with each individual donor. This un-
certainty presents a challenge: aligning the email
content with the unique preferences and motiva-
tions of each recipient to enhance the effectiveness
of the campaign.
Users. For this experiment, the ideal participants
would be actual donors of our target charity. These
users would be characterized by a rich set of at-
tributes such as age, gender, location, occupation,
hobbies, and their history of charitable donations.
However, obtaining such comprehensive and de-
tailed user data is not only challenging due to the
sensitivity and privacy concerns associated with
handling personal information but also financially
and operationally demanding. Therefore, in this
set of experiments, we propose an innovative alter-
native. Instead of relying on actual user datasets,
we utilize a larger model’s responses as an ora-
cle to pre-train our bandit on the responses of a
smaller model. In this experiment, we rely on Ope-
nAI GPT-4o model [ 1] as a surrogate to the real
potential donors, and simulate their preferences.
First, using GPT-4o, we create a diverse set of
1,000 virtual users. Each user is defined through a
detailed textual profile, specifying attributes such
as age, gender, location, occupation, hobbies, finan-
cial situation, motivations for donating to charity,
and a history of past charitable contributions. Next,
for each generated user ut, we prompt GPT-4o to
adopt their persona and simulate how they might
interact with our contextual bandit. We embed the
textual representation of each user using OpenAI’s
text embedding to define the context vector ϕt.
Arms. We consider K= 4arms. For each user,
we prompt Mto generate different emails for each
user (given the textual representation of the user)
using one of the following styles: Formal ,Emo-
tional/Narrative ,Informative/Educational , and Per-
sonal/Relatable . The detailed description of each
instruction is in Appendix C. All emails for each
user are customized to match the characteristics
of that user. For instance, if user utis identified
as a nature enthusiast, the content of emails may
center around an environmental cause supported by
the charity, crafted in different high-level styles of
writing to cater to different communication prefer-
ences.
5Rewards. Rewards are generated according to
Algorithm 1 with each model M. We repeat the
process five times and normalize the rewards. To
quantify the regret, we use GPT-4o (which was our
surrogate for real human users) as our oracle to
generate the true preferences of each user. The ora-
cle, representing an ideal decision-maker, selects
the arm corresponding to the user’s highest true
reward.
Baselines. We pre-train several contextual ban-
dit models with the data generated according to
Algorithm 1. Each experiment uses a different lan-
guage model M. We also establish three additional
baselines for comparison. We pre-train a Similarity
baseline, which considers the rewards of pulling
each arm as the cosine similarity of the user’s con-
text vector and the content vector of each arm. Not
pretrained baseline illustrates the regret of a con-
textual bandit which is not pre-trained beforehand.
Finally, GPT-4o (oracle) baseline demonstrates a
contextual bandit which is pre-trained with the re-
wards from the oracle.
Results. We fine-tune each contextual bandit
baseline for T= 1,000 steps with the true re-
wards of the users (generated by the oracle). Fig-
ure 2 presents the accumulated regret for each base-
line model over the course of fine-tuning. Regret
is calculated relative to an oracle, which, at ev-
ery step t∈[T], selects the arm that yields the
highest reward for the user. The results demon-
strate a clear trend: models pre-trained with vari-
ous LLMs (GPT-3.5, Claude-3 Haiku, and Mistral-
small), show significantly lower accumulated re-
gret compared to the baselines, including a model
with no pre-training and a model pre-trained with
cosine-similarity rewards (the Similarity baseline).
Notably, the accumulated regrets of these mod-
els are remarkably close to the pre-trained model
trained with the oracle’s rewards. This proximity
in performance confirms our claim that using LLM-
generated users and rewards to train contextual ban-
dits can substantially reduce regret and serve as a
foundation for fine-tuning with actual user rewards.
Table 1 illustrates the reduction in cumulative re-
gret after T= 1000 steps for each model when pre-
trained with varying amounts of CBLI-generated
data. We evaluated the performance of the ban-
dits after pre-training them with datasets contain-
ing 100, 500, and 1000 CBLI-generated users. As
shown in the table, by increasing the number of
CBLI-generated users, we find a greater reduction
0 200 400 600 800 1000
Number of Samples0255075100125150175Regret (accumulated)GPT-4o
GPT-3.5
Claude-3
Mistral-small
Similarity
Not PretrainedFigure 2: Accumulated regret relative to a GPT-4o-
based oracle across 1,000 samples. Each line represents
a CB trained using data generated by CBLI with M
indicated in the legend. Error bars represent variance
over shuffling true responses 10 times.
MUsers
100 500 1000
GPT-4o 9.21%±0.25 22.56%±0.21 25.75%±0.20
GPT-3.5 4.80%±0.22 12.40%±0.32 14.28%±0.13
Claude-3 5.38%±0.28 13.28%±0.31 14.70%±0.26
Mistral-small 5.93%±0.30 13.15%±0.23 16.96%±0.23
Similarity 3.66%±0.30 5.43%±0.26 6.42%±0.29
Table 1: The reduction in cumulative regret after T=
1000 steps when pretraining a bandit with 100, 500,
or 1000 CBLI-generated users and their rewards (with
input LLM M) compared to CB with no pretraining.
Note the oracle is GPT-4o.
in cumulative regret, though we also observe sub-
stantially diminishing returns.
5.2 Second Experiment: A Choice-Based
Conjoint Analysis with Real-world Data
We further extend our experiments by utilizing the
results of a conjoint survey experiment. These
experimental designs, which are popular in polit-
ical science [ 7], health [ 43], and market research
[10,34], are designed to detect how citizens, pa-
tients, and customers value different characteristics
of a candidate for public office, a health interven-
tion, a product, or another object of interest. These
objects usually have multiple features which may
be causally linked to preferences. A conjoint sur-
vey proceeds by showing a participant descriptions
of two or more objects and asks them to rank them
according to some criterion, e.g., likelihood to pur-
chase. This setup is an ideal scenario to test CBLI
as these survey experiments capture both user de-
mographic information, which can be used to gen-
erate contexts, as well as information on human
6preferences, which can be the target of the contex-
tual bandit.
We focus on the conjoint survey experiment de-
scribed in [ 27] and the dataset [ 28]. This survey,
conducted before the widespread availability of
COVID-19 vaccines, attempted to determine which
properties of a hypothetical vaccine would lead to
wider acceptance of the vaccine.
Users and setting. In this study, N= 1970 par-
ticipants begin by providing demographic and per-
sonal information such as age, gender, location,
income range, religion, and political views. We use
this information to construct a context vector for
each user. Next, participants are presented with a
pair of hypothetical COVID-19 vaccines and asked
to express which they are more likely to accept.
Each vaccine is characterized by a specific set of
eight attributes, including the vaccine’s efficacy
rate, the duration of protection it offers, and poten-
tial side effects, resulting in 576 possible descrip-
tions. This process is repeated a total of five times
per participant.2
To train our model according to Algorithm 1, we
generate 10,000 synthetic users, based on the set
of features of the actual users. We use these users
to pre-train our contextual bandit.
Given the large number of potential vaccine com-
binations, we adapt our approach to the experience
collection for CBLI, by not considering every pos-
sible vaccine as an individual arm. Instead, we
assume that each vaccine, described by its set of
features, is sampled from a population V, where
|V|= 576 . At each step t∈T, user utis randomly
selected from the generated users. Subsequently,
we sample a pair of vaccines v1, v2∈ V. Rather
than treating each vaccine as an arm, we focus on
the comparison between pairs of vaccines, which
aligns more closely with how choices are presented
in the conjoint analysis.
In terms of the contextual bandit model, a fea-
ture map, ψ:C ×V ×V → Rd, jointly encodes
the user context ϕi∈ C and the two vaccines
v1∈ V andv2∈ V , and then learns a lin-
ear model on ψ. In our experiments, we take
ψ(ϕi, v1, v2) = flat( ϕi(f(v1)−f(v2))T), where
flatrepresents flattening a matrix of size n×mas
a vector of size nm, and f(v),f:V →R7, outputs
the7features of each vaccine in the dataset.
2A full description of the dataset can be found on the
Harvard Dataverse.Arms Here, we adopt the Sleeping Bandit frame-
work [ 26] described in Section 3 and Section 4. In
our setup, two arms are available at each step t,
corresponding to the pair of vaccines presented in
the context when the conjoint experiment data was
collected. Thus, at each decision point, the model
must choose between only these two options.
Rewards. This setup uses the sparse mode in Al-
gorithm 1. For pre-training bandit data collection,
for the pair of vaccines i, j, if the LLM-simulated
user prefers ioverj, then R[i, j]+= 1 .Rtfor on-
line learning from actual rewards at round tworks
similarly but is based on real human preferences
from the conjoint experiment log.
Results. We demonstrate the impact of pre-
training contextual bandits using data generated by
LLMs in Figure 3. In our experiment, we configure
the LLM Mwith different models and pre-train the
bandit using data from 10,000simulated users and
their generated preferences. Once pre-trained, we
deploy the bandit on the real users participating in
the study and further fine-tune it using their actual
responses. Figure 3 presents the regret outcomes
for various models when applied to true user data.
Regret is computed based on the discrepancy be-
tween the model’s recommendations and the actual
preferences of the study participants. We compare
these results against a baseline model, denoted as
Not Pre-trained , which is trained exclusively on the
real user data without any pre-training. The results
clearly show that models pre-trained with LLM-
generated data exhibit significantly lower regret
than the baseline that was not pre-trained. This
demonstrates that using LLMs to simulate user
preferences and pre-train the bandit models can
effectively reduce the learning curve and improve
performance when transitioning to actual user in-
teractions.
Similar to Table 1, in Table 2, we explore the
effect of differing amounts of pre-training data. We
see that, for this example, more CBLI-generated
data increases the performance of the bandits,
though we observe sharply diminishing returns.
The reduction in cumulative regret is measured
afterT= 1000 steps of fine-tuning.
Initializing Bandit with Partial Information To
explore the robustness of our framework, we con-
duct additional experiments to evaluate the impact
of varying amounts of user information on the ef-
fectiveness of pre-training the contextual bandit.
70 200 400 600 800 1000
Number of Samples050100150200250300350400Regret (accumulated)GPT-3.5
Claude-3
GPT-4o
Mistral-small
Not PretrainedFigure 3: Accumulated regret relative to true responses
from N= 1970 responses to a conjoint experiment.
Each line represents a different instantiation of CBLI
with a different LLM. Error bars represent variance over
shuffling true responses 10 times.
MUsers
1000 5000 10000
GPT-4o 17.10%±0.63 16.07%±0.39 20.28%±0.41
GPT-3.5 11.95%±0.67 14.32%±0.38 19.19%±0.53
Claude-3 13.79%±0.58 17.18%±0.16 19.65%±0.37
Mistral-small 14.51%±0.77 17.89%±0.40 20.39%±0.38
Table 2: The reduction in cumulative regret of T=
1000 steps when pretraining a bandit with 100, 1,000,
or 10,000 CBLI-generated users and their rewards (with
input LLM M) compared to CB with no pretraining.
We experiment with three distinct subsets of user
data. No personal baseline excludes any personal
or demographic details about the users. It focuses
solely on COVID-19 and vaccine-related informa-
tion of each user. Only personal baseline includes
only the personal and demographic information of
each user, such as age, gender, location, and income
range. It omits any data related to COVID-19 or
vaccines about each user. Partial personal baseline
only contains a reduced set of personal and demo-
graphic details compared to Only Personal baseline.
It includes a minimal amount of personal informa-
tion, providing a middle ground between having
information about user profiles and having none at
all.Full baseline includes all the information about
the users. For each baseline, we compute the total
accumulated regret after T= 1000 ,2000 and9855
steps, using the responses of true users in the survey.
T= 9855 steps is fine-tuning the model with all
the data in the survey. We begin by pre-training the
bandit models using data from 10,000 generated
users. All baselines are pre-trained with GPT-3.5
generated preferences of users. After pre-training,
we fine-tune each bandit model with data from ac-DataT
1000 2000 9855 (all)
Full 19.19%±0.53 14.90%±0.44 8.07%±0.22
No Personal 14.79%±0.41 11.60%±0.47 5.23%±0.15
Partial Personal 18.55%±0.51 14.81%±0.46 7.70%±0.19
Only Personal 15.25%±0.45 11.60%±0.47 7.69%±0.20
Table 3: The reduction in accumulated regret when pre-
training contextual bandit models with varying levels
of user information. Each row represents a different
baseline, with different amount of user-specific context.
tual users, using their survey responses. To evaluate
the effectiveness of each approach, we calculate the
accumulated regret for each baseline model. The
reduction in cumulative regret is measured with
respect to the regret of a bandit model that was
not pre-trained and was only trained on real user
data. The results, summarized in Table 3 demon-
strate that incorporating even minimal user con-
text during pre-training can significantly enhance
the performance of contextual bandit models. Fur-
thermore, as the number of fine-tuning steps Tin-
creases, the reduction in cumulative regret for each
pre-trained baseline gradually diminishes. This
convergence occurs because, over time, the contex-
tual bandit model accumulates sufficient data from
real users during the fine-tuning phase. As a result,
the performance gap between the pre-trained mod-
els and the model that was not pre-trained becomes
less pronounced.
6 Conclusion
We introduced a novel framework, Contextual Ban-
dits with LLM Initialization (CBLI), for jump-
starting contextual bandits with Large Language
Models (LLMs), aimed at enhancing early decision-
making in personalized systems through pre-
training on synthetic data. Our experiments demon-
strated the potential of LLMs to generate a dataset
that, while not perfectly mirroring true human
preferences, provides a significantly better start-
ing point than the traditional cold-start scenario in
bandit models. Through rigorous testing in two
different bandit settings, we have shown that CBLI
can reduce early regret by significant margins, in-
dicating its practical effectiveness and robustness.
7 Limitations
While this work demonstrates that the output of
LLMs can be used to jump-start a contextual ban-
dit in some situations, we emphasize that in this
work, we have focused on total, accumulated regret,
8which represents an improvement on average . Re-
searchers have demonstrated that the output from
LLMs can be biased in many distinct ways, perhaps
most strikingly with respect to political ideology
[37,23]. Moreover, the distributions generated by
LLMs heavily depend on the prompting strategy
[39]. Indeed, some researchers have begun to ques-
tion if the measures of fairness and techniques to
make models fairer that have been developed in the
ML community can even be applied to LLMs [ 4].
At a high level, further work should be done exam-
ining the differential impact on regret for certain
subpopulations of interest across a wide variety of
tasks. At an individual project level, practitioners
considering utilizing CBLI in a real-world system
should be aware of these potential biases and con-
sider their potential impacts on users.
Second, as noted above, we expect the distribu-
tions of rewards generated by LLMs to differ from
the ground truth. While not observed in our studies,
it is worth noting that, in theory, distributional mis-
alignment can cause worse regret than cold starting
the CB [ 45]. Robustness techniques in prior work
[45] can be incorporated into CBLI to maximize
its usefulness in the future.
Third, while well-designed, targeted communi-
cations have been utilized to encourage pro-social
outcomes in a wide variety of contexts [ 16], the
same techniques can be utilized by malicious ac-
tors to anti-social ends. For instance, these tools
could be used, for example, to create disinforma-
tion campaigns, to stoke negative sentiment toward
outgroups, or to drive antipathy toward life-saving
public health measures. These problems are true
of any contextual bandit and especially true when
paired with an LLM, though CBLI may jump-start
any such effort.
Finally, our reward function estimation focuses
on determining the rank order of rewards and not
themagnitude of rewards. This is sufficient in tradi-
tional CB problems where the goal is to maximize
total rewards, but may not be sufficient in other con-
texts, such as when CBs are used in adaptive treat-
ment assignment with other goals or constraints
[8, 25].
Acknowledgments
The authors would like to thank Jake Bowers,
David Glick, and Nathaniel Rabb for suggesting
several potential conjoint survey experiments to
explore, and Lorne Schell for helpful comments onan early version of this work.
References
[1]Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
[2]Gati V Aher, Rosa I Arriaga, and Adam Tauman
Kalai. 2023. Using large language models to sim-
ulate multiple humans and replicate human subject
studies. In International Conference on Machine
Learning , pages 337–371. PMLR.
[3]Fernando Amat, Ashok Chandrashekar, Tony Jebara,
and Justin Basilico. 2018. Artwork personalization at
netflix. In Proceedings of the 12th ACM conference
on recommender systems , pages 487–488.
[4]Jacy Reese Anthis, Kristian Lum, Michael Ekstrand,
Avi Feller, Alexander D’Amour, and Chenhao Tan.
2024. The impossibility of fair llms. arXiv e-prints ,
pages arXiv–2406.
[5]Anthropic. 2024. Claude 3 haiku: our fastest model
yet.
[6]Lisa P Argyle, Ethan C Busby, Nancy Fulda,
Joshua R Gubler, Christopher Rytting, and David
Wingate. 2023. Out of one, many: Using language
models to simulate human samples. Political Analy-
sis, 31(3):337–351.
[7]Kirk Bansak, Jens Hainmueller, Daniel J. Hopkins,
and Teppei Yamamoto. 2021. Conjoint Survey Ex-
periments , page 19–41. Cambridge University Press.
[8]Hamsa Bastani, Mohsen Bayati, and Khashayar
Khosravi. 2021. Mostly exploration-free algo-
rithms for contextual bandits. Management Science ,
67(3):1329–1349.
[9]Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
[10] Philippe Cattin and Dick R Wittink. 1982. Com-
mercial use of conjoint analysis: A survey. Journal
of marketing , 46(3):44–53.
[11] Olivier Chapelle and Zaid Harchaoui. 2004. A ma-
chine learning approach to conjoint analysis. Ad-
vances in neural information processing systems , 17.
[12] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen,
Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen
Anderson, Greg Corrado, Wei Chai, Mustafa Ispir,
et al. 2016. Wide & deep learning for recommender
systems. In Proceedings of the 1st workshop on deep
learning for recommender systems , pages 7–10.
9[13] Wei Chu, Lihong Li, Lev Reyzin, and Robert
Schapire. 2011. Contextual bandits with linear pay-
off functions. In Proceedings of the Fourteenth In-
ternational Conference on Artificial Intelligence and
Statistics , pages 208–214. JMLR Workshop and Con-
ference Proceedings.
[14] Paul Covington, Jay Adams, and Emre Sargin. 2016.
Deep neural networks for youtube recommendations.
InProceedings of the 10th ACM conference on rec-
ommender systems , pages 191–198.
[15] Hengchen Dai, Silvia Saccardo, Maria A Han, Lily
Roh, Naveen Raja, Sitaram Vangala, Hardikkumar
Modi, Shital Pandya, Michael Sloyan, and Daniel M
Croymans. 2021. Behavioural nudges increase covid-
19 vaccinations. Nature , 597(7876):404–409.
[16] Stefano DellaVigna and Elizabeth Linos. 2022.
Rcts to scale: Comprehensive evidence from two
nudge units. Econometrica , 90(1):81–116.
[17] Danica Dillion, Niket Tandon, Yuling Gu, and Kurt
Gray. 2023. Can ai language models replace human
participants? Trends in Cognitive Sciences .
[18] Ricardo Dominguez-Olmedo, Moritz Hardt, and
Celestine Mendler-Dünner. 2023. Questioning the
survey responses of large language models. arXiv
preprint arXiv:2306.07951 .
[19] Esin Durmus, Karina Nyugen, Thomas I Liao,
Nicholas Schiefer, Amanda Askell, Anton Bakhtin,
Carol Chen, Zac Hatfield-Dodds, Danny Hernandez,
Nicholas Joseph, et al. 2023. Towards measuring
the representation of subjective global opinions in
language models. arXiv preprint arXiv:2306.16388 .
[20] Wendy Netter Epstein, Christopher T Robertson,
David Yokum, Hansoo Ko, Kevin H Wilson, Monica
Ramos, Katherine Kettering, and Margaret Houtz.
2022. Can moral framing drive insurance enrollment
in the united states? Journal of Empirical Legal
Studies , 19(4):804–843.
[21] Jens Hainmueller, Daniel J Hopkins, and Teppei Ya-
mamoto. 2014. Causal inference in conjoint analysis:
Understanding multidimensional choices via stated
preference experiments. Political analysis , 22(1):1–
30.
[22] Dae Woong Ham, Kosuke Imai, and Lucas Janson.
2022. Using machine learning to test causal hypothe-
ses in conjoint analysis. Political Analysis , pages
1–16.
[23] Jochen Hartmann, Jasper Schwenzow, and Maxi-
milian Witte. 2023. The political ideology of con-
versational ai: Converging evidence on chatgpt’s
pro-environmental, left-libertarian orientation. arXiv
preprint arXiv:2301.01768 .
[24] Albert Q Jiang, Alexandre Sablayrolles, Arthur
Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de las Casas, Florian Bressand, Gianna
Lengyel, Guillaume Lample, Lucile Saulnier, et al.
2023. Mistral 7b. arXiv preprint arXiv:2310.06825 .[25] Maximilian Kasy and Anja Sautmann. 2021. Adap-
tive treatment assignment in experiments for policy
choice. Econometrica , 89(1):113–132.
[26] Robert Kleinberg, Alexandru Niculescu-Mizil, and
Yogeshwer Sharma. 2010. Regret bounds for
sleeping experts and bandits. Machine learning ,
80(2):245–272.
[27] Sarah Kreps, Sandip Prasad, John S Brownstein,
Yulin Hswen, Brian T Garibaldi, Baobao Zhang, and
Douglas L Kriner. 2020. Factors associated with us
adults’ likelihood of accepting covid-19 vaccination.
JAMA network open , 3(10):e2025594–e2025594.
[28] Douglas Kriner, Sarah Kreps, John S Brownstein,
Yulin Hswen, Baobao Zhang, and Sandip Prasad.
2020. Replication Data for: Factors Associated With
US Adults’ Likelihood of Accepting COVID-19 Vac-
cination: Evidence From a Survey and Choice-Based
Conjoint Analysis.
[29] John Langford and Tong Zhang. 2007. The epoch-
greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Pro-
cessing Systems , volume 20. Curran Associates, Inc.
[30] Tor Lattimore and Csaba Szepesvári. 2020. Bandit
algorithms , pages 238–240. Cambridge University
Press.
[31] Lihong Li, Wei Chu, John Langford, and Robert E
Schapire. 2010. A contextual-bandit approach to per-
sonalized news article recommendation. In Proceed-
ings of the 19th international conference on World
wide web , pages 661–670.
[32] Elizabeth Linos, Jessica Lasky-Fink, Chris Larkin,
Lindsay Moore, and Elspeth Kirkman. 2024. The
formality effect. Nature Human Behaviour , 8(2):300–
310.
[33] Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang,
Biao Zhang, Da Tang, Bolin Zhu, Yijie Zhu, Peng
Wu, Ke Wang, et al. 2022. Monolith: real time rec-
ommendation system with collisionless embedding
table. arXiv preprint arXiv:2209.07663 .
[34] Jordan J Louviere. 1994. Conjoint analysis , pages
223–259. Blackwell Oxford.
[35] Tyler Lu, Dávid Pál, and Martin Pál. 2010. Con-
textual multi-armed bandits. In Proceedings of the
Thirteenth international conference on Artificial Intel-
ligence and Statistics , pages 485–492. JMLR Work-
shop and Conference Proceedings.
[36] Sandeep Pandey, Deepak Agarwal, Deepayan
Chakrabarti, and Vanja Josifovski. 2007. Bandits
for taxonomies: A model-based approach. In Pro-
ceedings of the 2007 SIAM international conference
on data mining , pages 216–227. SIAM.
[37] Ethan Perez, Sam Ringer, Kamil ˙e Lukoši ¯ut˙e, Ka-
rina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,
10Catherine Olsson, Sandipan Kundu, Saurav Kada-
vath, et al. 2023. Discovering language model behav-
iors with model-written evaluations. In 61st Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2023 , pages 13387–13434. Association
for Computational Linguistics (ACL).
[38] Nathaniel Rabb, Megan Swindal, David Glick, Jake
Bowers, Anna Tomasulo, Zayid Oyelami, Kevin H
Wilson, and David Yokum. 2022. Evidence from
a statewide vaccination RCT shows the limits of
nudges. Nature , 604(7904):E1–E7.
[39] Shibani Santurkar, Esin Durmus, Faisal Ladhak,
Cinoo Lee, Percy Liang, and Tatsunori Hashimoto.
2023. Whose opinions do language models reflect?
InInternational Conference on Machine Learning ,
pages 29971–30004. PMLR.
[40] Gabriel Simmons. 2022. Moral mimicry: Large
language models produce moral rationalizations
tailored to political identity. arXiv preprint
arXiv:2209.12106 .
[41] Aleksandrs Slivkins. 2019. Introduction to multi-
armed bandits . now, Hanover, MD.
[42] Richard H Thaler and Cass R Sunstein. 2021.
Nudge: The final edition . Yale University Press.
[43] Marta Trapero-Bertran, Beatriz Rodríguez-Martín,
and Julio López-Bastida. 2019. What attributes
should be included in a discrete choice experiment
related to health technologies? a systematic literature
review. PloS one , 14(7):e0219905.
[44] Chih-Chun Wang, Sanjeev R Kulkarni, and H Vin-
cent Poor. 2005. Bandit problems with side obser-
vations. IEEE Transactions on Automatic Control ,
50(3):338–355.
[45] Chicheng Zhang, Alekh Agarwal, Hal Daumé Iii,
John Langford, and Sahand Negahban. 2019. Warm-
starting contextual bandits: Robustly combining su-
pervised and bandit feedback. In International Con-
ference on Machine Learning , pages 7335–7344.
PMLR.
[46] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223 .A Pairwise comparisons yield
approximate rank orders
In Algorithm 1, for each user uand each pair
of arms (k1, k2), we prompt an LLM Mto rank
the arms. If it ranks k1higher than k2, we say
Ru[k1] =Ru[k1]+1. Otherwise, we say Ru[k2] =
Ru[k2] + 1 . In this section, we show under certain
assumptions on the true rewards, that the values Ru
will represent a rank order of the user’s preferences.
We assume that the reward distribution for user
uand arm iisYi= Bern( pi)a Bernoulli random
variable with probability piof realizing 1and prob-
ability 1−piof realizing 0. Ifpi> pj, then, on
average, the user uwill derive more reward from
being assigned arm ithan arm j.
Consider the random variables
Zij=

1Yi> Yj
0Yi< Yj
B Y i=Yj
where B= Bern(0 .5)is a Bernoulli random vari-
able with probability 0.5of being 1and probability
0.5of being 0. Further, let Zi=P
jZij. Then, as-
suming the LLM Mfaithfully represents the user’s
preferences, by the law of large numbers, we ex-
pect the unnormalized Ru[i]≈mE[Zi]where m
is the number of iterations.
Now
E[Zij] = Pr( Yi> Yj) +1
2·Pr(Yi=Yj)
=pi(1−pj) +1
2[pipj+ (1−pi)(1−pj)]
=1
2(pi−pj+ 1).
ThenE[Zi] =K
2(pi+ 1)−1
2P
jpj. Rewriting,
we see that E[Zi] =K
2pi+Cwhere Cis a constant
for all i. Thus, the E[Zi]have the same order as
thepi.
BComparing Arm Scoring Methods with
LLMs
In our CBLI approach, outlined in Algorithm 1,
we prompt an LLM Mto rank pairs of arms as
opposed to scoring arms individually, and deter-
mine an ordering based on them. We find that
prompting Mwith pairs of arms leads to more
consistent results compared to scoring each arm
independently. Figure 4 displays the average re-
ward for each arm across all generated users using
11Formal
EmotionalInformativePersonal02468101214Average Reward
Formal
EmotionalInformativePersonal0.000.050.100.150.200.250.300.35Average RewardFigure 4: Comparison of arm scoring methods using
LLMs. Each bar represents the average reward of an
arm across all users. The left figure illustrates results
from scoring each arm individually, while the right fig-
ure shows results from scoring arms pair-wise as per
Algorithm 1.
two different prompting methods. The left figure
depicts the approach where Mis prompted for
each user ito provide a score between 0 and 100
for each arm a, proportional to the likelihood of
that user donating. The rewards generated by this
method show minimal variation and are difficult
to distinguish on the scale of 0 to 100. In contrast,
the right figure illustrates the normalized rewards
of each arm using the pair-wise prompting method.
This approach captures the diverse preferences of
users more effectively, revealing distinct patterns
in user preferences across different arms.
C Experimental Details
In our experiments, we train LinUCB [ 13] with
α= 10 as the hyperparameter. In all figures and
tables, results are reported based on the average of
10 runs.
We collect the data for our experiments using
the LLM API provided by OpenAI, Anthropic, and
Mistral [1, 5, 24].
We train our bandit models on a system with the
following specification: 2.3 GHz Quad-Core Intel
Core i7 and 32 GB of RAM. The total running time
to train the models is less than 2 hours.
C.1 First Experiment: Personalized Email
Campaign for Charity Donations
In this set of experiments, we pre-train each bandit
model with the dataset composed of size n= 20×
Users where the number of users is specified in
Table 1. For each specified number of users, werandomly sample ninstances from the available
user data to create the training dataset.
Arms. We consider K= 4arms, correspond to
the following styles: Formal ,Emotional/Narrative ,
Informative/Educational , and Personal/Relatable .
For each user we generate a customized message
based on each of these styles. Description of each
style is as follows.
•Formal Approach: Begin with a formal greet-
ing, introduce the organization, highlight the
current need, explain how their contribution
can make a difference, and end with a polite
request asking for their generous support. The
tone is official and respectful, focusing on the
importance of the cause.
•Emotional/Narrative Style: This style lever-
ages storytelling to evoke empathy and com-
passion from the reader. You can share a real-
life story related to the cause, emphasize the
struggle, and showcase how their donation can
change lives.
•Informative/Educational Style: This style re-
lies on facts, statistics, and evidence to per-
suade the user to donate. It educates the reader
about the cause, its impact, and how the char-
ity is fighting for it. The reader’s decision will
be driven by the evidence of how efficient the
charity work is.
•Personal/Relatable Style: Here, you use a
more casual, friendly tone. You could even
share personal experiences with the charity or
testimonies from donors. The essence is to
make the reader feel closely connected and to
understand that anyone can make a difference.
Prompts. Given the information about each user
iand the style of the arm k, we generate a cus-
tomized message for the user to be the content of
armkwith the following prompt:
Write a message for the following user to encour-
age them to donate to [name of charity] charity
organization: [description of user i] Use the fol-
lowing style to generate the message: [description
of style k]
Furthermore, to generate the pair-wise rewards
for user iand arms k1, k2as described in Algo-
rithm 1, we use the following prompt:
Pretend you are the following user: [description
of user i] Now you are receiving these two mes-
sages: [Message k1] [Message k2] Which message
12is more aligned with your interests? Which one
makes you donate to the charity? Let’s think step
by step. Print the number of preferred message at
the end: [Answer]
C.2 Second Experiment: A Choice-Based
Conjoint Analysis with Real-world Data
In this set of experiments, we pre-train each bandit
model with the dataset composed of size n= 2×
Users where the number of users is specified in
Table 2. For each specified number of users, we
randomly sample ninstances from the available
user data to create the training dataset.
Full Description of Users. Each respondent an-
swered the following multi-choice questions: gen-
der, race, age, state, income range, religious be-
liefs, political views, their (dis)approval of Donald
Trump’s handling of his job as a president,3recent
changes in employment status, ability to work from
home, whether they suffered from COVID, if they
knew a person who had been hospitalized or had
died because of COVID, whether (with respect to
COVID) they believed the worst was behind us or
yet to come, whether they had received a flu vacci-
nation in the past, whether they think vaccines are
safe in general, whether they think children should
be required to be vaccinated against childhood dis-
eases, and whether they have health insurance.
Full Description of Vaccines. Each vaccine is
described with the following attributes: efficacy,
duration of protection, major side effects, minor
side effects, FDA approval, country of origin, and
whether it is endorsed by a president or a health
organization.
Prompts. For each user utand a pair of vaccines
v1, v2, we prompt LLM with the following.
Consider you are in the middle of the COVID
pandemic where vaccines are just being produced.
Pretend to be the following user: [description of
ut] now you are given two vaccine choices for
COVID. The description of each vaccine is as fol-
lows: [description of v1], now the next one: [de-
scription of v2]. Which one do you take? A or B?
Let’s think step by step. Print the final answer as
[Final Answer] at the end as well.
3As a reminder, this survey was taken in 2020.
13