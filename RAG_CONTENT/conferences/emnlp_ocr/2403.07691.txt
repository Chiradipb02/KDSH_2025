ORPO : Monolithic Preference Optimization without Reference Model
Jiwoo Hong Noah Lee James Thorne
KAIST AI
{jiwoo_hong, noah.lee, thorne}@kaist.ac.kr
Abstract
While recent preference alignment algorithms
for language models have demonstrated promis-
ing results, supervised fine-tuning (SFT) re-
mains imperative for achieving successful con-
vergence. In this paper, we study the cru-
cial role of SFT within the context of pref-
erence alignment, emphasizing that a minor
penalty for the disfavored generation style is
sufficient for preference-aligned SFT. Build-
ing on this foundation, we introduce a straight-
forward and innovative reference model-free
monolithic odds ratio preference optimization
algorithm, ORPO , eliminating the necessity for
an additional preference alignment phase. We
demonstrate, both empirically and theoretically,
that the odds ratio is a sensible choice for con-
trasting favored and disfavored styles during
SFT across the diverse sizes from 125M to 7B.
Specifically, fine-tuning Phi-2 (2.7B), Llama-2
(7B), and Mistral (7B) with ORPO on the Ul-
traFeedback alone surpasses the performance
of state-of-the-art language models with more
than 7B and 13B parameters: achieving up to
12.20% on AlpacaEval2.0(Figure 1), 66.19%
on IFEval (instruction-level loose, Table 6), and
7.32 in MT-Bench (Figure 12). We release
code1and model checkpoints for Mistral- ORPO -
α(7B)2and Mistral- ORPO -β(7B).3
1 Introduction
Pre-trained language models (PLMs) with vast
training corpora such as web texts (Gokaslan and
Cohen, 2019; Penedo et al., 2023) or textbooks (Li
et al., 2023c) have shown remarkable abilities in
diverse natural language processing (NLP) tasks
(Brown et al., 2020; Zhang et al., 2022; Touvron
et al., 2023; Jiang et al., 2023; Almazrouei et al.,
2023). However, the models must undergo further
1https://github.com/xfactlab/orpo
2https://huggingface.co/kaist-ai/
mistral-orpo-alpha
3https://huggingface.co/kaist-ai/
mistral-orpo-beta
4.967.79.448.3510.9911.3312.2Llama−2 MistralAlpacaEval 2.0
Llama (7B) Llama (13B) Llama−ORPO (7B) Zephyr−a Zephyr−b Mistral−ORPO−a Mistral−ORPO−b0.02.55.07.510.012.5Win Rate (%)
Algorithm RLHF DPO ORPOFigure 1: AlpacaEval2.0result of Llama-2 (7B) and
Mistral (7B) fine-tuned with ORPO (blue) in comparison
to the state-of-the-art models. Notably, Mistral- ORPO -α
&βsurpasses Zephyr βand Llama-2-Chat (13B) with a
single epoch training exclusively on the UltraFeedback.
tuning to be usable in general-domain applications,
typically through processes such as instruction tun-
ingandpreference alignment .
Instruction-tuning (Wei et al., 2022; Taori et al.,
2023; Wang et al., 2023; Zhou et al., 2023a) trains
models to follow task descriptions given in natu-
ral language, which enables models to generalize
well to previously unseen tasks. However, despite
the ability to follow instructions, models may gen-
erate harmful or unethical outputs (Carlini et al.,
2021; Gehman et al., 2020; Pryzant et al., 2023).
To further align these models with human values,
additional training is required with pairwise prefer-
ence data using techniques such as reinforcement
learning with human feedback (Ziegler et al., 2020;
Stiennon et al., 2022, RLHF) and direct preference
optimization (Rafailov et al., 2023, DPO).
Preference alignment methods have demon-
strated success in several downstream tasks beyond
reducing harm. For example, improving factuality
(Tian et al., 2023; Cheng et al., 2024; Chen and
Li, 2024), code-based question answering (Gorba-
tovski and Kovalchuk, 2024), and machine transla-
tion (Ramos et al., 2023). The versatility of align-
ment algorithms over a wide range of downstreamarXiv:2403.07691v2  [cs.CL]  14 Mar 2024SFT
Reward Model
SFT
RLHFChosen Responses
Rejected ResponsesStrong 
Adaptation
Weak PenaltyPre-trained
Ref.
 Policy
Ref.
 Policy
DPO ORPO
Odds Ratio Preference OptimizationFigure 2: Comparison of model alignment techniques. ORPO aligns the language model without a reference model in
a single-step manner by assigning a weak penalty to the rejected responses and a strong adaptation signal to the
chosen responses with a simple log odds ratio term appended to the negative log-likelihood loss.
tasks highlights the necessity of understanding the
alignment procedure and further improving the al-
gorithms in terms of efficiency and performance.
However, existing preference alignment methods
normally consist of a multi-stage process, as shown
in Figure 2, typically requiring a second refer-
ence model and a separate warm-up phase with
supervised fine-tuning (SFT) (Ziegler et al., 2020;
Rafailov et al., 2023; Wu et al., 2023).
In this paper, we study the role and impact of
SFT in pairwise preference datasets for model
alignment in Section 3 and propose a simple and
novel monolithic alignment method, odds ratio
preference optimization ( ORPO ), which efficiently
penalizes the model from learning undesired gen-
eration styles during SFT in Section 4. In con-
trast to previous works, our approach requires
neither an SFT warm-up stage nor a reference
model, enabling resource-efficient development of
preference-based aligned models.
We demonstrate the effectiveness of our method
with the evaluation of model alignment tasks and
popular leaderboards in Section 6.1 and 6.2 by
fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mis-
tral (7B) with ORPO . Then, we conduct controlled
experiments comparing ORPO against established
methods for model alignment, RLHF, and DPO
for different datasets and model sizes in Section
6.3. Along with the post-hoc analysis of generation
diversity in Section 6.4, we expound on the theo-
retical, empirical, and computational justification
of utilizing the odds ratio in monolithic preference
alignment in Section 7.3. We release the training
code and the checkpoints for Mistral- ORPO -α(7B)
and Mistral- ORPO -β(7B). These models achieve
7.24 and 7.32 in MT-Bench, 11.33% and 12.20%onAlpacaEval2.0, and 61.63% and 66.19% in IFE-
val instruction-level loose accuracy, respectively.
2 Related Works
Alignment with Reinforcement Learning Rein-
forcement learning with human feedback (RLHF)
commonly applies the Bradley-Terry model
(Bradley and Terry, 1952) to estimate the prob-
ability of a pairwise competition between two in-
dependently evaluated instances. An additional
reward model is trained to score instances. Rein-
forcement learning algorithms such as proximal
policy optimization (PPO) (Schulman et al., 2017)
are employed to train the model to maximize the
score of the reward model for the chosen response,
resulting in language models that are trained with
human preferences (Ziegler et al., 2020; Stiennon
et al., 2022). Notably, Ouyang et al. (2022) demon-
strated the scalability and versatility of RLHF for
instruction-following language models. Extensions
such as language model feedback (RLAIF) could
be a viable alternative to human feedback (Bai et al.,
2022b; Lee et al., 2023; Pang et al., 2023). How-
ever, RLHF faces challenges of extensive hyper-
parameter searching due to the instability of PPO
(Rafailov et al., 2023; Wu et al., 2023) and the
sensitivity of the reward models (Gao et al., 2022;
Wang et al., 2024). Therefore, there is a crucial
need for stable preference alignment algorithms.
Alignment without Reward Model Several
techniques for preference alignment mitigate the
need for reinforcement learning (Rafailov et al.,
2023; Song et al., 2023; Azar et al., 2023; Etha-
yarajh et al., 2024). Rafailov et al. (2023) introduce
direct policy optimization (DPO), which combines
the reward modeling stage into the preference learn-ing stage. Azar et al. (2023) prevented potential
overfitting problems in DPO through identity pref-
erence optimization (IPO). Ethayarajh et al. (2024)
and Cai et al. (2023) proposed Kahneman-Tversky
Optimisation (KTO) and Unified Language Model
Alignment (ULMA) that does not require the pair-
wise preference dataset, unlike RLHF and DPO.
Song et al. (2023) further suggests incorporation
of the softmax value of the reference response set
in the negative log-likelihood loss to merge the
supervised fine-tuning and preference alignment.
Alignment with Supervised Fine-tuning Prefer-
ence alignment methods in reinforcement learning
(RL) often leverage supervised fine-tuning (SFT)
to ensure the stable update of the active policy in
relation to the old policy (Schulman et al., 2017).
This is because the SFT model is the old policy in
the context of RLHF (Ziegler et al., 2020). Fur-
thermore, empirical findings indicate that, even in
non-RL alignment methods, the SFT model is cru-
cial for achieving convergence to desired results
(Rafailov et al., 2023; Tunstall et al., 2023).
In contrast, there have been approaches to build
human-aligned language models by conducting
SFT only with filtered datasets (Zhou et al., 2023a;
Li et al., 2023a; Haggerty and Chandra, 2024; Zhou
et al., 2023b). Zhou et al. (2023a) demonstrated
that SFT with a small amount of data with fine-
grained filtering and curation could be sufficient
for building helpful language model assistants. Fur-
thermore, Li et al. (2023a) and Haggerty and Chan-
dra (2024) proposed an iterative process of fine-
tuning the supervised fine-tuned language mod-
els with their own generations after fine-grained
selection of aligned generations and Zhou et al.
(2023b) suggested that a curated subset of prefer-
ence dataset is sufficient for alignment. While these
works highlight the impact and significance of SFT
in the context of alignment, the actual role of SFT
and the theoretical background for incorporating
preference alignment in SFT remains understudied.
3 The Role of Supervised Fine-tuning
We study the behavior of supervised fine-tuning
(SFT) as an initial stage of preference alignment
methods (Ziegler et al., 2020; Rafailov et al., 2023)
through analysis of the loss function in SFT and
empirical demonstration of the preference compre-
hension ability of the trained SFT model. SFT
plays a significant role in tailoring the pre-trained
language models to the desired domain (Zhou et al.,2023a; Dong et al., 2024) by increasing the log
probabilities of pertinent tokens. Nevertheless, this
inadvertently increases the likelihood of generating
tokens in undesirable styles, as illustrated in Figure
3. Therefore, it is necessary to develop methods
capable of preserving the domain adaptation role of
SFT while concurrently discerning and mitigating
unwanted generation styles.
Absence of Penalty in Cross-Entropy Loss The
goal of cross-entropy loss model fine-tuning is to
penalize the model if the predicted logits for the
reference answers are low, as shown in Equation 2.
L=−1
mmX
k=1logP(x(k),y(k)) (1)
=−1
mmX
k=1|V|X
i=1y(k)
i·log(p(k)
i) (2)
where yiis a boolean value that indicates if ith to-
ken in the vocabulary set Vis a label token, pi
refers to the probability of ith token, and mis
the length of sequence. Using cross-entropy alone
gives no direct penalty or compensation for the
logits of non-answer tokens (Lin et al., 2017) as
yiwill be set to 0. While cross-entropy is gen-
erally effective for domain adaptation (Mao et al.,
2023), there are no mechanisms to penalize rejected
responses when compensating for the chosen re-
sponses. Therefore, the log probabilities of the to-
kens in the rejected responses increase along with
the chosen responses, which is not desired from the
viewpoint of preference alignment.
Generalization over Both Response Styles We
conduct a pilot study to empirically demonstrate
the miscalibration of chosen and rejected responses
with supervised fine-tuning alone. We fine-tune
OPT-350M (Zhang et al., 2022) on the chosen re-
sponses only from the HH-RLHF dataset (Bai et al.,
2022b). Throughout the training, we monitor the
log probability of rejected responses for each batch
and report this in Figure 3. Both the log probability
of chosen and rejected responses exhibited a simul-
taneous increase. This can be interpreted from two
different perspectives. First, the cross-entropy loss
effectively guides the model toward the intended
domain (e.g., dialogue). However, the absence of
a penalty for unwanted generations results in re-
jected responses sometimes having even higher log
probabilities than the chosen ones.−2.5−2.4−2.3−2.2
0 5000 10000 15000
Training StepLog ProbabilityResponse Type Chosen RejectedFigure 3: Log probabilities for chosen and rejected
responses during OPT-350M model fine-tuning on HH-
RLHF dataset. Despite only chosen responses being
used for supervision, rejected responses show a compa-
rable likelihood of generation.
Penalizing Undesired Generations Appending
an unlikelihood penalty to the loss has demon-
strated success in reducing unwanted degenerative
traits in models (Welleck et al., 2019; Li et al.,
2020). For example, to prevent repetitions , an un-
wanted token set of previous contexts, k∈ Crecent ,
is disfavored by adding the following term to
(1−p(k)
i)to the loss (such as Equation 2) which
penalizes the model for assigning high probabilities
to recent tokens. Motivated by SFT ascribing high
probabilities to rejected tokens (Figure 3) and the
effectiveness of appending penalizing unwanted
traits, we design a monolithic preference alignment
method that dynamically penalizes the disfavored
response for each query without the need for craft-
ing sets of rejected tokens.
4 Odds Ratio Preference Optimization
We introduce a novel preference alignment algo-
rithm, Odds Ratio Preference Optimization ( ORPO ),
which incorporates an odds ratio-based penalty
to the conventional negative log-likelihood (NLL)
loss for differentiating the generation styles be-
tween favored and disfavored responses.
4.1 Preliminaries
Given an input sequence x, the average log-
likelihood of generating the output sequence y, of
length mtokens, is computed as Equation 3. The
odds of generating the output sequence ygiven an
input sequence xis defined in Equation 4:
logPθ(y|x) =1
mmX
t=1logPθ(yt|x, y<t)(3)odds θ(y|x) =Pθ(y|x)
1−Pθ(y|x)(4)
Intuitively, odds θ(y|x) = kimplies that it is k
times more likely for the model θto generate the
output sequence ythan not generating it. Thus,
the odds ratio of the chosen response ywover the
rejected response yl,ORθ(yw, yl), indicates how
much more likely it is for the model θto generate
ywthanylgiven input x, defined in Equation 5.
ORθ(yw, yl) =odds θ(yw|x)
odds θ(yl|x)(5)
4.2 Objective Function of ORPO
The objective function of ORPO in Equation 6 con-
sists of two components: 1) supervised fine-tuning
(SFT) loss ( LSFT); 2) relative ratio loss ( LOR).
LORPO =E(x,yw,yl)[LSFT+λ· LOR](6)
LSFT follows the conventional causal language
modeling negative log-likelihood (NLL) loss func-
tion to maximize the likelihood of generating the
reference tokens as previously discussed in Sec-
tion 3. LORin Equation 7 maximizes the odds
ratio between the likelihood of generating the dis-
favored response ywand the disfavored response
yl. We wrap the log odds ratio with the log sig-
moid function so that LORcould be minimized by
increasing the log odds ratio between ywandyl.
LOR=−logσ
logodds θ(yw|x)
odds θ(yl|x)
(7)
Together, LSFT andLORweighted with λtailor
the pre-trained language model to adapt to the spe-
cific subset of the desired domain and disfavor gen-
erations in the rejected response sets.
4.3 Gradient of ORPO
The gradient of LRatio further justifies using the
odds ratio loss. It comprises two terms: one that pe-
nalizes the wrong predictions and one that contrasts
between chosen and rejected responses, denoted in
Equation 84ford= (x, yl, yw)∼D.
∇θLOR=δ(d)·h(d) (8)
δ(d) =
1 +odds θP(yw|x)
odds θP(yl|x)−1
(9)
h(d) =∇θlogPθ(yw|x)
1−Pθ(yw|x)−∇θlogPθ(yl|x)
1−Pθ(yl|x)
(10)
4The full derivation for ∇θLORis in Appendix A.When the odds of the favored responses are rela-
tively higher than the disfavored responses, δ(d)
in Equation 9 will converge to 0. This indicates
that the δ(d)will play the role of a penalty term,
accelerating the parameter updates if the model is
more likely to generate the rejected responses.
Meanwhile, h(d)in Equation 10 implies a
weighted contrast of the two gradients from the cho-
sen and rejected responses. Specifically, 1−P(y|x)
in the denominators amplifies the gradients when
the corresponding side of the likelihood P(y|x)
is low. For the chosen responses, this accelerates
the model’s adaptation toward the distribution of
chosen responses as the likelihood increases.
5 Experimental Settings
5.1 Training Configurations
Models We train a series of OPT models (Zhang
et al., 2022) scaling from 125M to 1.3B parameters
comparing supervised fine-tuning (SFT), proximal
policy optimization (PPO), direct policy optimiza-
tion (DPO), and compare these to our ORPO . PPO
and DPO models were fine-tuned with TRL library
(von Werra et al., 2020) on top of SFT models
trained for a single epoch on the chosen responses
following Rafailov et al. (2023) and Tunstall et al.
(2023). We notate this by prepending "+" to each al-
gorithm (e.g., +DPO). Additionally, we train Phi-2
(2.7B) (Javaheripi and Bubeck, 2023), a pre-trained
language model with promising downstream per-
formance (Beeching et al., 2023), as well as Llama-
2 (7B) (Touvron et al., 2023) and Mistral (7B)
(Jiang et al., 2023). Further training details for
each method are in Appendix C.
Datasets We test each training configuration and
model on two datasets: 1) Anthropic’s HH-RLHF
(Bai et al., 2022a), 2) Binarized UltraFeedback
(Tunstall et al., 2023). We filtered out instances
where yw=ylor where yw=∅or where yl=∅.
Reward Models We train OPT-350M and OPT-
1.3B on each dataset for a single epoch for reward
modeling with the objective function in Equation
11 (Ziegler et al., 2020). The OPT-350M reward
model was used for PPO, and OPT-1.3B reward
model was used to assess the generations of fine-
tuned models. We refer to these reward models as
RM-350M and RM-1.3B in Section 6.
−E(x,yl,yw)[logσ(r(x, yw)−r(x, yl))] (11)5.2 Leaderboard Evaluation
In Section 6.1, we evaluate the models using the
AlpacaEval1.0andAlpacaEval2.0(Li et al., 2023b)
benchmarks, comparing ORPO to other instruction-
tuned models reported in the official leaderboard,5
including Llama-2 Chat (7B) and (13B) (Touvron
et al., 2023), and Zephyr αandβ(Almazrouei
et al., 2023). Similarly, in Section 6.2, we evaluate
the models with MT-Bench (Zheng et al., 2023)
and report the results and the scores of the same
models reported in the official leaderboard.6Us-
ing GPT-4 (Achiam et al., 2023) as an evaluator
inAlpacaEval1.0, we assess if the trained model
can be preferred over the responses generated from
text-davinci-003 . For AlpacaEval2.0, we used
GPT-4-turbo7as an evaluator following the default
setting. We assess if the generated responses are fa-
vored over those generated from GPT-4. Finally, us-
ing GPT-4 as an evaluator in MT-Bench, we check
if the models can follow the instructions with hard
answers in a multi-turn conversation.
6 Results and Analysis
First, we assess the general instruction-following
abilities of the models by comparing the prefer-
ence alignment algorithms in Sections 6.1 and 6.2.
Second, we measure the win rate of OPT models
trained with ORPO against other alignment methods
training OPT 1.3B as a reward model in Section
6.3. Then, we measure the lexical diversity of the
models trained with ORPO and DPO in Section 6.4.
6.1 Single-turn Instruction Following
Phi-2 (2.7B) ORPO improved pre-trained Phi-2
to exceed the performance of the Llama-2 Chat
instruction-following language model by only us-
ing UltraFeedback as the instruction-tuning dataset,
as shown in Table 1. λof 0.25 was applied for Phi-
2, resulting in 71.80% and 6.35% in AlpacaEval.
Llama-2 (7B) Notably, UltraFeedback and ORPO
withλof 0.2 on Llama-2 (7B) resulted in higher
AlpacaEval scores than the chat versions of both
7B and 13B scale trained with RLHF, eventually
showing 81.26% and 9.44% in both AlpacaEvals.
In contrast, in our controlled experimental set-
ting of conducting one epoch of SFT and three
5https://tatsu-lab.github.io/alpaca_eval/
6https://huggingface.co/spaces/lmsys/
chatbot-arena-leaderboard
7https://platform.openai.com/docs/models/
gpt-4-and-gpt-4-turboModel Name Size AlpacaEval1.0 AlpacaEval2.0
Phi-2 + SFT 2.7B 48.37% (1.77) 0.11% (0.06)
Phi-2 + SFT + DPO 2.7B 50.63% (1.77) 0.78% (0.22)
Phi-2 + ORPO (Ours) 2.7B 71.80% (1.59) 6.35% (0.74)
Llama-2 Chat * 7B 71.34% (1.59) 4.96% (0.67)
Llama-2 Chat * 13B 81.09% (1.38) 7.70% (0.83)
Llama-2 + ORPO (Ours) 7B 81.26% (1.37) 9.44% (0.85)
Zephyr ( α) * 7B 85.76% (1.23) 8.35% (0.87)
Zephyr ( β) * 7B 90.60% (1.03) 10.99% (0.96)
Mistral- ORPO -α(Ours) 7B 87.92% (1.14) 11.33% (0.97)
Mistral- ORPO -β(Ours) 7B 91.41% (1.15) 12.20% (0.98)
Table 1: Table of instruction-following abilities of each checkpoint measured through AlpacaEval. While clearly
showing the improvements in instruction-following abilities after training with ORPO , it is notable that ORPO models
exceed RLHF or DPO models of Llama-2 and Mistral (* indicates the results from the official leaderboard.)
epochs of DPO following Tunstall et al. (2023) and
Rafailov et al. (2023), Llama-2 + SFT and Llama-
2 + SFT + DPO yielded models with outputs that
could not be evaluated. This supports the efficacy
ofORPO , in which the model can rapidly learn the
desired domain and the preference with limited
data. This aligns with the h(d)examination in the
gradient of our method studied in Section 4.3.
Mistral- ORPO -α(7B) Furthermore, fine-tuning
Mistral (7B) with single-turn conversation dataset,
UltraFeedback, and ORPO withλof 0.1 outperforms
Zephyr series, which are the Mistral (7B) models
fine-tuned with SFT on 20K UltraChat (Ding et al.,
2023) and DPO on the full UltraFeedback. As
shown in Table 1, Mistral- ORPO -α(7B) achieves
87.92% and 11.33%, which exceeds Zephyr αby
1.98% and Zephyr βby 0.34% in AlpacaEval2.0.
The sample responses and corresponding refer-
ences from GPT-4 can be found in Appendix I.
Mistral- ORPO -β(7B) Using the same configura-
tion of Mistral- ORPO -α(7B), we additionally com-
pare fine-tuning Mistral on the cleaned version of
the UltraFeedback8to demonstrate the effect of the
data quality (Bartolome et al., 2023). While the
actual sizes of datasets are similar, ORPO gains fur-
ther advantages from the dataset quality by scoring
over 91% and 12% on AlpacaEval, as shown in
Table 1. Further instruction-following evaluation
on two Mistral-based models with IFEval (Zhou
et al., 2023c) is reported in the Appendix D.
8https://huggingface.co/datasets/argilla/
ultrafeedback-binarized-preferences-cleaned
Writing
Roleplay
Reasoning
Math
CodingExtractionSTEMHumanities
0246810Model
Llama-2-Chat (70B)
GPT-3.5-turbo
Claude V1
GPT-4
Mistral- ORPO -⍺ (7B): 7.23
Mistral- ORPO -β (7B): 7.32
Loading [MathJax]/extensions/MathMenu.jsFigure 4: MT-Bench result of Mistral- ORPO -α(7B) and
Mistral- ORPO -β(7B) by the category. Further compari-
son can be found in the Appendix G.
6.2 Multi-turn Instruction Following
With our best model, Mistral- ORPO -α(7B) and
Mistral- ORPO -β(7B), we also assess the multi-turn
instruction-following skills with deterministic an-
swers (e.g., math) through MT-Bench.
As shown in Figure 4, ORPO -Mistral (7B) series
achieve comparable results to either larger or the
proprietary models, including Llama-2-Chat (70B)
and Claude. Eventually, Mistral- ORPO -α(7B) and
Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-
Bench without being exposed to the multi-turn con-
versation dataset during training.
6.3 Reward Model Win Rate
We assess the win rate of ORPO over other pref-
erence alignment methods, including supervised
fine-tuning (SFT), PPO, and DPO, using RM-1.3B
to understand the effectiveness and scalability of
ORPO in Tables 2 and 3. Additionally, we visu-
ally verify that ORPO can effectively enhance theFigure 5: Reward distribution comparison between OPT-125M (left), OPT-350M (middle), and OPT-1.3B (right)
trained with SFT (blue), RLHF (green), DPO (orange), and ORPO (red) on the test set of UltraFeedback using the
RM-1.3B. While the rewards of the trained models are roughly normal and preference optimization algorithms
(RLHF, DPO, and ORPO ) tend to move the reward distribution in the positive direction, ORPO is on par or better than
RLHF and DPO in increasing the expected reward. The same plot for the HH-RLHF dataset is in Appendix F.
expected reward compared to SFT in Figure 5.
HH-RLHF In Table 2, ORPO outperforms SFT
and PPO across all model scales. The highest win
rate against SFT and PPO across the size of the
model was 78.0% and 79.4%, respectively. Mean-
while, the win rate over DPO was correlated to
the model’s size, with the largest model having the
highest win rate: 70.9%.
ORPO vs SFT +DPO +PPO
OPT-125M 84.0 (0.62) 41.7 (0.77) 66.1 (0.26)
OPT-350M 82.7 (0.56) 49.4 (0.54) 79.4 (0.29)
OPT-1.3B 78.0 (0.16) 70.9 (0.52) 65.9 (0.33)
Table 2: Average win rate (%) and its standard deviation
ofORPO and standard deviation over other methods on
HH-RLHF dataset for three rounds. Sampling decoding
with a temperature of 1.0 was used on the test set.
UltraFeedback The win rate in UltraFeedback
followed similar trends to what was reported in HH-
RLHF, as shown in Table 3. ORPO was preferred
over SFT and PPO for maximum 80.5% and 85.8%,
respectively. While consistently preferring ORPO
over SFT and PPO, the win rate over DPO grad-
ually increases as the size of the model increases.
The scale-wise trend exceeding DPO will be further
shown through 2.7B models in Section 6.1.
Overall Reward Distribution In addition to the
win rate, we compare the reward distribution of
the responses generated with respect to the test set
of the UltraFeedback dataset in Figure 5 and HH-
RLHF dataset in Appendix F. Regarding the SFT
reward distribution as a default, PPO, DPO, and
ORPO shift it in both datasets. However, the magni-
tude of reward shifts for each algorithm differs.ORPO vs SFT +DPO +PPO
OPT-125M 73.2 (0.12) 48.8 (0.29) 71.4 (0.28)
OPT-350M 80.5 (0.54) 50.5 (0.17) 85.8 (0.62)
OPT-1.3B 69.4 (0.57) 57.8 (0.73) 65.7 (1.07)
Table 3: Average win rate (%) and its standard devia-
tion of ORPO and standard deviation over other methods
onUltraFeedback dataset for three rounds. Sampling
decoding with a temperature of 1.0 was used.
In Figure 5, RLHF (i.e., SFT + PPO) has some
abnormal properties of the distribution with a low
expected reward. We attribute this to empirical
evidence of the instability and reward mismatch
problem of RLHF (Rafailov et al., 2023; Gao et al.,
2022; Shen et al., 2023) as the RLHF models were
trained with RM-350M and assessed with RM-
1.3B. Meanwhile, it is notable that the ORPO dis-
tribution (red) is mainly located on the very right
side of each subplot, indicating higher expected re-
wards. Recalling the intent of preference alignment
methods, the distributions in Figure 5 indicate that
ORPO tends to fulfill the aim of preference align-
ment for all model sizes.
6.4 Lexical Diversity
The lexical diversity of the preference-aligned lan-
guage models was studied in previous works (Kirk
et al., 2024). We expand the concept of per-input
and across-input diversity introduced in Kirk et al.
(2024) by using Gemini-Pro (Gemini Team et al.,
2023) as an embedding model, which is suitable
for assessing the diversity of instruction-following
language models by encoding a maximum of 2048
tokens. The diversity metric with the given set ofsampled responses is defined as Equation 13.
Oi
θ:={yj∼θ(y|xi)|j= 1,2, ..., K} (12)
D(Oi
θ) =1
2·PN−1
i=1PN
j=i+1cos(hi, hj)
N·(N−1)(13)
where cos(hi, hj)refers to the cosine similarity
between the embedding hiandhj. 5 different re-
sponses are sampled with a temperature of 1.0 to
160 queries in AlpacaEval (i.e., K= 5, N= 160 )
using Phi-2 and Llama-2 trained with ORPO and
DPO. We report the results in Table 4.
Per Input Diversity (PID) We average the input-
wise average cosine similarity between the gener-
ated samples with Equation 14 to assess the per-
input diversity. In Table 4, ORPO models have the
highest average cosine similarity in the first column
for both models, which implies the lowest diversity
per input. This indicates that ORPO generally as-
signs high probabilities to the desired tokens, while
DPO has a relatively smoother logit distribution.
PIDD(θ) =1
NNX
i=1D(Oi
θ) (14)
Across Input Diversity (AID) Using 8 samples
generated per input, we sample the first item for
each input and examine their inter cosine similarity
with Equation 15 for across-input diversity. Un-
like per-input diversity, it is noteworthy that Phi-2
(ORPO ) has lower average cosine similarity in the
second row of Table 4. We can infer that ORPO
triggers the model to generate more instruction-
specific responses than DPO.
AID D(θ) =D N[
i=1Oi,θ,j=1!
(15)
Per Input ↓Across Input ↓
Phi-2 + SFT + DPO 0.8012 0.6019
Phi-2 + ORPO 0.8909 0.5173
Llama-2 + SFT + DPO 0.8889 0.5658
Llama-2 + ORPO 0.9008 0.5091
Table 4: Lexical diversity of Phi-2 and Llama-2 fine-
tuned with DPO and ORPO . Lower cosine similarity is
equivalent to higher diversity. The highest value in each
column within the same model family is bolded.7 Discussion
In this section, we expound on the theoretical and
computational details of ORPO . The theoretical anal-
ysis of ORPO is studied in Section 7.1, which will
be supported with the empirical analysis in Section
7.2. Then, we compare the computational load of
DPO and ORPO in Section 7.3.
7.1 Comparison to Probability Ratio
The rationale for selecting the odds ratio instead of
the probability ratio lies in its stability. The prob-
ability ratio for generating the favored response
ywover the disfavored response ylgiven an input
sequence xcan be defined as Equation 16.
PRθ(yw, yl) =Pθ(yw|x)
Pθ(yl|x)(16)
While this formulation has been used in previous
preference alignment methods that precede SFT
(Rafailov et al., 2023; Azar et al., 2023), the odds
ratio is a better choice in the setting where the
preference alignment is incorporated in SFT as the
odds ratio is more sensitive to the model’s prefer-
ence understanding. In other words, the probability
ratio leads to more extreme discrimination of the
disfavored responses than the odds ratio.
We visualize this through the sample distribu-
tions of the log probability ratio logPR(X2|X1)
and log odds ratio logOR(X2|X1). We sample
50,000 samples each with Equation 17 and plot the
log probability ratio and log odds ratio in Figure
6. We multiply βfor the probability ratio as it is
practiced in the probability ratio-based methods
and report the cases where β= 0.2andβ= 1.0.
X1, X2∼Unif(0,1) (17)
Y∼β(logX1−logX2) (18)
Y∼logX1
1−X1−logX2
1−X2(19)
Recalling that the log sigmoid function is applied
to the log probability ratio and log odds ratio, each
ratio’s scale determines the expected margin be-
tween the likelihood of the favored and disfavored
styles when the loss is minimized. In that sense, the
contrast should be relatively extreme to minimize
the log sigmoid loss when PR(X2|X1)is inputted
instead of OR(X2|X1)to the log sigmoid function,
regarding the sharp distribution of logPR(X2|X1)10
 5
 0 5 105K10K15K20KCountProb. Ratio (=0.2)
Prob. Ratio (=1.0)
Odds RatioFigure 6: Sampled distribution of logPR(X2|X1)and
logOR(X2|X1).logOR(X2|X1)has a wider range
given the same input probability pairs (X1, X2).
in Figure 6. This results in overly suppressing the
logits for the tokens in the disfavored responses in
the setting where SFT and preference alignment are
incorporated, as the model is not adapted to the do-
main. We empirically support this analysis through
the ablation study in Appendix B. Therefore, the
odds ratio is a better choice when the preference
alignment is done with SFT due to the mild discrim-
ination of disfavored responses and the prioritizing
of the favored responses to be generated.
Throughout fine-tuning, minimizing the log
sigmoid loss leads to either PR(X2|X1)or
OR(X2|X1)to be larger. This is equivalent to the
rejected responses’ token-wise likelihood, which
will generally get smaller. In this context, it is es-
sential to avoid an overly extreme contrast. This
precaution is especially important given the sharp
distribution of logPR(X2|X1)depicted in Figure
6. The excessive margin could lead to the unwar-
ranted suppression of logits for tokens in disfavored
responses within the incorporated setting, poten-
tially resulting in issues of degeneration.
7.2 Minimizing LOR
We demonstrate that models trained with ORPO
learned to reflect the preference throughout the
training process. We monitored the log probabili-
ties of the chosen and rejected responses and the
log odds ratio with λ= 1.0. With the same dataset
and model as Figure 3, Figure 7 shows that the
log probability of rejected responses is diminishing
while that of chosen responses is on par with Fig-
ure 3 as the log odds ratio increases. This indicates
that ORPO is successfully preserving the domain
adaptation role of SFT while the penalty term LOR
induces the model to lower the likelihood of un-
wanted generations. We discuss the effect of λin
Equation 6 in Appendix E, studying the proclivityof the log probability margin between the favored
and disfavored responses with respect to λ.
0.00.51.01.5
02000400060008000
Training StepLog Odds Ratio
−1.8−1.5−1.2−0.9
02000400060008000
Training StepLog ProbabilityResponse Type
Chosen
Rejected
Figure 7: Average log-likelihood for chosen and re-
jected responses and log odds ratio per batch. The odds
consistently increase during training with ORPO .
7.3 Computational Efficiency
As depicted in Figure 2, ORPO does not require a
reference model, unlike RLHF and DPO. In that
sense, ORPO is computationally more efficient than
RLHF and DPO in two perspectives: 1) memory
allocation and 2) fewer FLOPs per batch.
The reference model ( πSFT) in the context of
RLHF and DPO denotes the model trained with su-
pervised fine-tuning (SFT), which will be the base-
line model for updating the parameters with RLHF
or DPO (Ziegler et al., 2020; Rafailov et al., 2023).
Thus, two πSFTs, a frozen reference model and the
model undergoing tuning, are required during train-
ing. Furthermore, in theory, two forward passes
should be calculated for each model to acquire the
logits for the chosen and rejected responses. In
other words, four forward passes happen in total
for a single batch. On the other hand, a reference
model is not required in ORPO asπSFT is directly
updated. This leads to half the number of forward
passes required for each batch during training.
8 Conclusion
In this paper, we introduced a reference-free mono-
lithic preference alignment method, odds ratio pref-
erence optimization ( ORPO ), by revisiting and un-
derstanding the value of the supervised fine-tuning
(SFT) phase in the context of preference align-
ment. ORPO was consistently preferred by the
fine-tuned reward model against SFT and RLHF
across the scale, and the win rate against DPO in-
creased as the size of the model increased. Further-
more, we validate the scalability of ORPO with 2.7B
and 7B pre-trained language models by exceeding
the larger state-of-the-art instruction-following lan-
guage models in AlpacaEval. Specifically, Mistral-
ORPO -αand Mistral- ORPO -βachieved 11.33% and12.20% in AlpacaEval2.0, 7.23 and 7.32 in MT-
Bench, thereby underscoring the efficiency and ef-
fectiveness of ORPO . We release fine-tuning code
and model checkpoints for Mistral- ORPO -αand
Mistral- ORPO -βto aid reproducibility.
Limitations
While conducting a comprehensive analysis of the
diverse preference alignment methods, including
DPO and RLHF, we did not incorporate a more
comprehensive range of preference alignment algo-
rithms. We leave the broader range of comparison
against other methods as future work, along with
scaling our method to over 7B models. In addi-
tion, we will expand the fine-tuning datasets into
diverse domains and qualities, thereby verifying
the generalizability of our method in various NLP
downstream tasks. Finally, we would like to study
the internal impact of our method on the pre-trained
language model, expanding the understanding of
preference alignment procedure to not only the
supervised fine-tuning stage but also consecutive
preference alignment algorithms.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, Daniele Mazzotta,
Badreddine Noune, Baptiste Pannier, and Guilherme
Penedo. 2023. The falcon series of open language
models.
Mohammad Gheshlaghi Azar, Mark Rowland, Bilal
Piot, Daniel Guo, Daniele Calandriello, Michal
Valko, and Rémi Munos. 2023. A general theoret-
ical paradigm to understand learning from human
preferences.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Ben Mann, and Jared Kaplan. 2022a. Training a
helpful and harmless assistant with reinforcement
learning from human feedback.Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022b. Constitutional ai: Harmless-
ness from ai feedback.
Alvaro Bartolome, Gabriel Martin, and Daniel Vila.
2023. Notus. https://github.com/argilla-io/
notus .
Edward Beeching, Clémentine Fourrier, Nathan Habib,
Sheon Han, Nathan Lambert, Nazneen Rajani, Omar
Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.
Open llm leaderboard. https://huggingface.co/
spaces/HuggingFaceH4/open_llm_leaderboard .
Ralph Allan Bradley and Milton E. Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika , 39(3/4):324–
345.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie
Gu, and Guannan Zhang. 2023. Ulma: Unified lan-
guage model alignment with demonstration and point-
wise human preference. ArXiv , abs/2312.02554.
Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ul-
far Erlingsson, Alina Oprea, and Colin Raffel. 2021.
Extracting training data from large language models.
Weixin Chen and Bo Li. 2024. Grath: Gradual self-
truthifying for large language models.
Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wen-
wei Zhang, Zhangyue Yin, Shimin Li, Linyang Li,Kai Chen, and Xipeng Qiu. 2024. Can ai assistants
know what they don’t know?
Tri Dao. 2023. Flashattention-2: Faster attention with
better parallelism and work partitioning.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi
Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,
and Bowen Zhou. 2023. Enhancing chat language
models by scaling high-quality instructional conver-
sations.
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng
Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng
Yuan, Chang Zhou, and Jingren Zhou. 2024. How
abilities in large language models are affected by
supervised fine-tuning data composition.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model
alignment as prospect theoretic optimization. arXiv
preprint arXiv:2402.01306 .
Leo Gao, John Schulman, and Jacob Hilton. 2022. Scal-
ing laws for reward model overoptimization.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,
Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
ish Thite, Ben Wang, Kevin Wang, and Andy Zou.
2023. A framework for few-shot language model
evaluation.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxi-
cityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
3356–3369, Online. Association for Computational
Linguistics.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Aaron Gokaslan and Vanya Cohen. 2019. Open-
webtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus .
Alexey Gorbatovski and Sergey Kovalchuk. 2024. Re-
inforcement learning for question answering in pro-
gramming domain using public community scoring
as a human feedback.
Hamish Haggerty and Rohitash Chandra. 2024. Self-
supervised learning for skin cancer diagnosis with
limited training data.Mojan Javaheripi and Sébastien Bubeck. 2023. Phi-2:
The surprising power of small language models.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b.
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis,
Jelena Luketina, Eric Hambro, Edward Grefenstette,
and Roberta Raileanu. 2024. Understanding the ef-
fects of rlhf on llm generalisation and diversity.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas
Mesnard, Johan Ferret, Kellie Lu, Colton Bishop,
Ethan Hall, Victor Carbune, Abhinav Rastogi, and
Sushant Prakash. 2023. Rlaif: Scaling reinforcement
learning from human feedback with ai feedback.
Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck,
Y-Lan Boureau, Kyunghyun Cho, and Jason Weston.
2020. Don’t say that! making inconsistent dialogue
unlikely with unlikelihood training. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 4715–4728, Online.
Association for Computational Linguistics.
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke
Zettlemoyer, Omer Levy, Jason Weston, and Mike
Lewis. 2023a. Self-alignment with instruction back-
translation.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023b. Alpacaeval: An
automatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del
Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023c.
Textbooks are all you need ii: phi-1.5 technical re-
port.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,
and Piotr Dollár. 2017. Focal loss for dense object
detection. In Proceedings of the IEEE international
conference on computer vision , pages 2980–2988.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. 2023.
Cross-entropy loss functions: Theoretical analysis
and applications.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-
Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang
Yu. 2023. Language model self-improvement by
reinforcement learning contemplation.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The refinedweb dataset for
falcon llm: Outperforming curated corpora with web
data, and web data only.
Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang
Zhu, and Michael Zeng. 2023. Automatic prompt op-
timization with “gradient descent” and beam search.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
7957–7968, Singapore. Association for Computa-
tional Linguistics.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model.
Miguel Moura Ramos, Patrick Fernandes, António Far-
inhas, and André F. T. Martins. 2023. Aligning neu-
ral machine translation models: Human feedback in
training and inference.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In Proceedings of the
26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , KDD ’20,
page 3505–3506, New York, NY , USA. Association
for Computing Machinery.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms.
Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shi-
han Dou, Tao Gui, Qi Zhang, and Xuanjing Huang.
2023. Loose lips sink ships: Mitigating length bias
in reinforcement learning from human feedback. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 2859–2873, Singapore.
Association for Computational Linguistics.
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei
Huang, Yongbin Li, and Houfeng Wang. 2023. Pref-
erence ranking optimization for human alignment.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul Christiano. 2022. Learning
to summarize from human feedback.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
pher D. Manning, and Chelsea Finn. 2023. Fine-
tuning language models for factuality.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, Nathan Sarrazin, Omar San-
seviero, Alexander M. Rush, and Thomas Wolf. 2023.
Zephyr: Direct distillation of lm alignment.
Leandro von Werra, Younes Belkada, Lewis Tun-
stall, Edward Beeching, Tristan Thrush, Nathan
Lambert, and Shengyi Huang. 2020. Trl: Trans-
former reinforcement learning. https://github.
com/huggingface/trl .
Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan
Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu
Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao
Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao
Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen,
Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
Zuxuan Wu, and Yu-Gang Jiang. 2024. Secrets of
rlhf in large language models part ii: Reward model-
ing.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hannaneh Hajishirzi. 2023. How
far can camels go? exploring the state of instruction
tuning on open resources.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned
language models are zero-shot learners.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2019. Neu-
ral text generation with unlikelihood training. arXiv
preprint arXiv:1908.04319 .
Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen,
Kannan Ramchandran, and Jiantao Jiao. 2023. Pair-
wise proximal policy optimization: Harnessing rela-
tive feedback for llm alignment.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo,
Chien-Chin Huang, Min Xu, Less Wright, Hamid
Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmai-
son, Can Balioglu, Pritam Damania, Bernard Nguyen,
Geeta Chauhan, Yuchen Hao, Ajit Mathews, and
Shen Li. 2023. Pytorch fsdp: Experiences on scaling
fully sharded data parallel.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
LLM-as-a-Judge with MT-Bench and Chatbot Arena.
ArXiv:2306.05685 [cs].
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023a. Lima:
Less is more for alignment.
Haotian Zhou, Tingkai Liu, Qianli Ma, Jianbo Yuan,
Pengfei Liu, Yang You, and Hongxia Yang. 2023b.
Lobass: Gauging learnability in supervised fine-
tuning data. ArXiv , abs/2310.13008.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha
Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and
Le Hou. 2023c. Instruction-following evaluation for
large language models.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
guage models from human preferences.A Derivation of ∇θLORwith Odds Ratio
Suppose that g(x, yl, yw) =oddsθP(yw|x)
oddsθP(yl|x)
∇θLOR=∇θlogσ
logodds θP(yw|x)
odds θP(yl|x)
(20)
=σ′(logg(x, yl, yw))
σ(logg(x, yl, yw))(21)
=σ(−logg(x, yl, yw))· ∇θlogg(x, yl, yw) (22)
=σ(−logg(x, yl, yw))
g(x, yl, yw)· ∇θg(x, yl, yw) (23)
=σ(−logg(x, yl, yw))· ∇θlogg(x, yl, yw) (24)
=
1 +odds θP(yw|x)
odds θP(yl|x)−1
· ∇θlogodds θP(yw|x)
odds θP(yl|x)(25)
In Equation 25, the remaining derivative can be further simplified by replacing 1−Pθ(y|x)terms
where P(y|x) =NqQN
tPθ(yt|x, y<tinodds θ(y|x)as follows.
∇θlog (1−Pθ(y|x)) =∇θ(1−Pθ(y|x))
1−Pθ(y|x)(26)
=−∇θPθ(y|x)
1−Pθ(y|x)(27)
=−Pθ(y|x)
1−Pθ(y|x)· ∇θlogPθ(y|x) (28)
=odds θ(y|x)· ∇θlogPθ(y|x) (29)
∇θlogodds θP(yw|x)
odds θP(yl|x)=∇θlogPθ(yw|x)
Pθ(yl|x)−
∇θlog(1−Pθ(yw|x))− ∇ θlog(1−Pθ(yl|x))
(30)
= (1 + odds θP(yw|x))∇θlogPθ(yw|x)−(1 + odds θP(yl|x))∇θlogPθ(yl|x)
(31)
Therefore, the final form of ∇θLORwould be
∇θLOR=1 +odds θP(yw|x)
1 +oddsθP(yw|x)
oddsθP(yl|x)· ∇θlogPθ(yw|x)−1 +odds θP(yl|x)
1 +oddsθP(yw|x)
oddsθP(yl|x)· ∇θlogPθ(yl|x) (32)
=
1 +odds θP(yw|x)
odds θP(yl|x)−1
·∇θlogPθ(yw|x)
1−P(yw|x)−∇θlogPθ(yl|x)
1−P(yl|x)
(33)B Ablation on Probability Ratio and Odds Ratio
In this section, we continue the discussion in Section 7.1 through empirical results comparing the log
probabilities of chosen and rejected responses in UltraFeedback when trained with probability ratio and
odds ratio. Recalling the sensitivity of each ratio discussed in Section 7.1, it is expected for the probability
ratio to lower the log probabilities of the rejected responses with a larger scale than the odds ratio. This is
well-shown in Figure 8, which is the log probabilities of each batch while fine-tuning with probability
ratio (left) rapidly reaches under -4, while the same phenomenon happens after the over-fitting occurs in
the case of odds ratio (right).
Figure 8: The log probability trace when the model is trained with the probability ratio (left) and the odds ratio
(right) given the same hyperparameters. The probability ratio leads the rejected responses to have relatively lower
log probabilities.
C Experimental Details
Flash-Attention 2 (Dao, 2023) is applied for all the pre-trained models for computational efficiency. In
particular, the OPT series and Phi-2 (2.7B) were trained with DeepSpeed ZeRO 2 (Rasley et al., 2020),
Llama-2 (7B) and Mistral (7B) were trained with Fully Sharded Data Parallel(FSDP) (Zhao et al., 2023).
7B and 2.7B models were trained with four and two NVIDIA A100, and the rest were trained on four
NVIDIA A6000. For optimizer, AdamW optimizer (Loshchilov and Hutter, 2019) and paged AdamW
(Dettmers et al., 2023) were used, and the linear warmup with cosine decay was applied for the learning
rate. For input length, every instance was truncated and padded to 1,024 tokens and 2,048 tokens for
HH-RLHF and UltraFeedback, respectively. To guarantee that the models can sufficiently learn to generate
the proper response to the conversation history or the complex instruction, we filtered instances with
prompts with more than 1,024 tokens.
Supervised Fine-tuning (SFT) For SFT, the maximum learning rate was set to 1e-5. Following Ziegler
et al. (2020) and Rafailov et al. (2023), the training epoch is set to 1.
Reinforcement Learning with Human Feedback (RLHF) For RLHF, the hyperparameters were set as
Table 5 for UltraFeedback. For the HH-RLHF dataset, the output_min_length andoutput_max_length
were set to 64 and 256.
Direct Preference Optimization (DPO) For DPO, βwas set to 0.1 for every case. The learning rate
was set to 5e-6, and the model was trained for three epochs to select the best model by evaluation loss in
each epoch. However, in most cases, the first or the second checkpoint was selected as the best model as
the evaluation loss increased from the third epoch.Hyperparameter Setting
ppo_epoch 4
init_kl_coef 0.1
horizon 2,000
batch_size 64
mini_batch_size 8
gradient_accumulation_steps 1
output_min_length 128
output_max_length 512
optimizer AdamW
learning_rate 1e-05
gamma 0.99
Table 5: Hyperparameter settings for RLHF.
Odds Ratio Preference Optimization ( ORPO )AsORPO does not require any special hyperparameter,
only the learning rate and epoch were the only hyperparameter to set. For ORPO , the maximum learning
rate was set to 8e-6 and trained for 10 epochs. The best model is selected based on the lowest evaluation
loss for the OPT series, Phi-2 (2.7B) and Llama-2 (7B).
D IFEval Result for Mistral- ORPO -αand Mistral- ORPO -β
Along with the AlpacaEval results reported in Section 6.1, we report the results of Mistral- ORPO -α
and Mistral- ORPO -βon IFEval (Zhou et al., 2023c), calculated with the codes from Gao et al. (2023).
Additional information can be found in: https://jiwooya1000.github.io/posts/orpo/#ifeval .
Model Type Prompt-Strict Prompt-Loose Inst-Strict Inst-Loose
Mistral- ORPO -α 0.5009 0.5083 0.5995 0.6163
Mistral- ORPO -β 0.5287 0.5564 0.6355 0.6619
Table 6: IFEval scores of Mistral- ORPO -αand Mistral- ORPO -β.E Ablation on the Weighting Value ( λ)
For the weighting value λin Equation 6, we conduct an ablation study with {0.1,0.5,1.0}. Mistral
(7B) and UltraFeedback were used for the base model and dataset. In Section E.1, we compare the log
probability trends by the value of λ, and we assess the downstream effect of λin Section E.2.
E.1 Log Probability
Lambda = 0.1 Lambda = 0.5 Lambda = 1.0
0 1000 2000 3000 40000 1000 2000 3000 40000 1000 2000 3000 4000−2.5−2.0−1.5−1.0
Training StepLog ProbabilityResponse Type Chosen Rejected
Figure 9: The log probability trend by λ. With larger λ(e.g., λ= 1.0),LORgets more influential in fine-tuning the
models with ORPO .
In Figure 9, we find that larger λleads to stronger discrimination of the rejected responses in general.
With λ= 0.1, the average log probability of the chosen and the rejected responses stay close as the
fine-tuning proceeds. Also, unlike other settings, the log probabilities for the rejected responses do not
decrease, but rather, the log probabilities of the chosen responses increase to minimize LORterm.
Moreover, in λ= 0.5, there exists a similar trend of further increasing the log probabilities of the
chosen responses, but the log probabilities of the rejected responses are diminishing simultaneously.
Lastly, in λ= 1.0, the chosen responses diminish along with the rejected responses while enlarging the
margin between them. However, this does not mean smaller λis always the better. It will depend on the
specific need and model.
E.2 MT-Bench
Writing
Roleplay
Reasoning
Math
CodingExtractionSTEMHumanities
0 2 4 6 810model
GPT-4
GPT-3.5-turbo
Mistral (ORPO) - 1.0
Mistral (ORPO) - 0.1
Figure 10: MT-Bench result comparison by differing λ= 0.1andλ= 1.0.The downstream impact of λstands out in the MT-Bench result. In comparison to λ= 0.1, Mis-
tral+ORPO (7B) with λ= 1.0performs worse in extraction, math, and reasoning, which are the categories
that generally require deterministic answers. On the other hand, it performs better in STEM, humanities,
and roleplay, which ask the generations without hard answers. Along with the amount of discrepancy be-
tween the trend in the logits of chosen and rejected responses, we can infer that making a more significant
margin between the chosen and the rejected responses through higher λinORPO leads to overly adapting to
the chosen responses set in the training dataset. This proclivity results in open-ended generations generally
being preferred by the annotator while showing weaker performance in the hard-answered questions.F Test Set Reward Distribution on HH-RLHF
Along with Figure 11, which depicts the reward distribution of OPT2-125M, OPT2-350M, and OPT2-1.3B
on the UltraFeedback dataset, we report the reward distribution of each pre-trained checkpoint trained on
the HH-RLHF dataset. As discussed in Section 6.3, ORPO consistently pushes the reward distribution of
SFT to the right side.
Figure 11: Reward distribution comparison between OPT-125M (left), OPT-350M (middle), and OPT-1.3B (right)
trained with SFT (blue), RLHF (green), DPO (orange), and ORPO (red) on the test set of HH-RLHF using the 1.3B
reward model. General tendency follows that of Figure 5.
G MT-Bench Result of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B)
For the MT-Bench result in Section 6.2, we report the category-wise scores of Mistral- ORPO -α(7B) and
Mistral- ORPO -β(7B) in Figure 12. While surpassing Llama-2 Chat (13B) and Llama-2 Chat (70B) in
most cases, Mistral- ORPO -β(7B) is comparable to GPT-3.5-turbo in the categories that require descriptive
generations. However, it lacks coding and math skills, which we speculate is due to the lack of training
data, as we used 61k instances in UltraFeedback.
Writing
Roleplay
Reasoning
Math
CodingExtractionSTEMHumanities
0246810Model
Llama-2-Chat (70B)
GPT-3.5-turbo
Claude V1
GPT-4
Mistral- ORPO -⍺ (7B): 7.23
Mistral- ORPO -β (7B): 7.32
Loading [MathJax]/extensions/MathMenu.js
Figure 12: MT-Bench result of Mistral- ORPO -α(7B) andMistral- ORPO -β(7B) by the category.H Special Instructions for Verbosity Assessment
For the succinctness and verboseness instructions, we generated five different instructions, each with
ChatGPT9. From the instructions in Table 7, we randomly sampled one prompt each for every batch to
prevent potential word bias.
# Succinctness Verboseness
1Please generate a short and concise response. Please generate an elaborative and chatty response.
2 Provide a brief and concise answer. Provide a detailed answer.
3 Keep your reply short and to the point. Keep your reply elaborative and intricate.
4 Keep your answer brief for clarity. Keep your answer detailed.
5 Generate a brief and to-the-point answer. Generate a chatty and step-wise answer.
Table 7: Instructions prepended to the queries from AlpacaEval. Each instruction set asks the model to generate
either shorter or longer responses given the query, respectively.
9https://chat.openai.com/I Generation Samples from Mistral- ORPO -α(7B)
In this section, we two sample generations from Mistral- ORPO -α(7B) on AlpacaEval, including the
one which is preferred over the response of GPT-4 and the one in which GPT-4 was preferred. For
readability, the instances with short responses are selected for both cases. Additionally, the responses for
Mistral- ORPO -α(7B) were sampled with a temperature of 0.7. The checkpoints for Mistral- ORPO -α(7B)
and Mistral- ORPO -β(7B) can be found in https://huggingface.co/kaist-ai/mistral-orpo-alpha
andhttps://huggingface.co/kaist-ai/mistral-orpo-beta .
Query 1 (GPT-4 Win): Give a brief description of the book that you are given. Matilda
1.GPT-4 :
"Matilda" is a children’s novel written by the British author Roald Dahl. It was first published in 1988.
The story revolves around a very intelligent and precocious young girl named Matilda Wormwood.
Despite being neglected and mistreated by her ignorant and self-absorbed parents, Matilda finds
solace in reading books and quickly develops a love for learning.
At the age of five, she begins attending school and meets Miss Honey, a kindhearted teacher
who recognizes Matilda’s extraordinary intellectual abilities. However, the school’s tyrannical
headmistress, Miss Trunchbull, is a cruel and oppressive figure who takes pleasure in punishing the
children with her unreasonable and harsh methods.
Matilda discovers that she possesses a unique telekinetic ability, which she learns to control and
use to her advantage. With her wit, intelligence, and special powers, Matilda sets out to defend her
friends from Miss Trunchbull’s abuses and to help Miss Honey overcome her own difficult past.
The book is a tale of good triumphing over evil, and it celebrates the power of knowledge, kindness,
and standing up for what is right. It has been adapted into a successful film, stage musical, and has
remained a beloved classic in children’s literature.
2.Mistral- ORPO -β:
"Matilda" is a children’s book written by Roald Dahl, published in 1988. The story revolves around
the life of Matilda Wormwood, a brilliant and gifted five-year-old girl who has telekinetic powers.
The story explores themes of education, family, and the importance of reading and intellectual
curiosity. The book has been praised for its wit, humor, and powerful message, making it a classic
and beloved children’s book.Query 2 (Mistral- ORPO -βWin): What do you think about ChatGPT?
1.GPT-4 :
As an AI developed by OpenAI, I don’t have personal opinions. However, I can share that ChatGPT
is an advanced language model that has received considerable attention for its ability to generate
human-like text. It’s designed to assist users in a wide range of tasks, from answering questions to
creating content and engaging in conversation.
ChatGPT has been praised for its fluency and versatility in handling various topics and for its
potential to streamline and enhance communication in both personal and professional settings.
However, like any technology, it also faces challenges and criticisms, such as ensuring the accuracy
of information, preventing misuse, and managing ethical considerations surrounding AI-generated
content.
Overall, ChatGPT represents a significant achievement in natural language processing and has
sparked discussions about the future of AI in our daily lives.
2.Mistral- ORPO -β:
As an AI language model, I am an example of the technology used in ChatGPT. I can provide a
personal opinion, but please keep in mind that I am designed to be socially unbiased and positive in
nature.
ChatGPT is an impressive advancement in artificial intelligence and natural language processing.
It has the ability to understand and generate human-like text, making it possible for users to
have engaging and informative conversations with the AI. This technology has the potential
to revolutionize various industries, including customer support, education, content creation, and more.
However, it’s important to recognize that ChatGPT, like any AI technology, is not perfect. It may
sometimes provide inaccurate or biased information due to the limitations of its training data or
the complexity of human language. It’s crucial to verify the facts and use critical thinking when
interacting with AI systems like ChatGPT.
Overall, ChatGPT represents a significant step forward in the field of AI, and its potential applications
are vast. As a responsible user, it’s important to approach this technology with a critical and respectful
mindset, recognizing its strengths and limitations.