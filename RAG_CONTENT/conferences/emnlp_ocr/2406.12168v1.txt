BPO: Supercharging Online Preference Learning by Adhering to the
Proximity of Behavior LLM
Wenda Xu1*, Jiachen Li1*, William Yang Wang1, Lei Li2
1UC Santa Barbara,2Carnegie Mellon University
1{wendaxu, jiachen_li, william}@cs.ucsb.edu ,2leili@cs.cmu.edu
1 2 6 T = 600
Data Collection Frequency F7075808590Win Rate against Reference T ext (%)
TL;DR
1 23 T = 150
Data Collection Frequency F77.580.082.585.087.590.092.5
Anthropic Helpfulness
PO (DPO)
Offline DPO
On-Policy DPO
Figure 1: Given the same annotation budget , ourBPO (when F= 2) significantly outperforms offline DPO
(F= 1) on both TL;DR and Anthropic Helpfulness by introducing only oneadditional preference annotation
phase. Its performance ( F= 2) even matches, if not exceeds, that of on-policy DPO ( F=T), which collects new
annotations at every step.
Abstract
Direct alignment from preferences (DAP) has
emerged as a promising paradigm for align-
ing large language models (LLMs) to human
desiderata from pre-collected, offline prefer-
ence datasets. While recent studies indicate
that existing offline DAP methods can directly
benefit from online training samples, we high-
light the need to develop specific online DAP
algorithms to fully harness the power of on-
line training. Specifically, we identify that the
learned LLM should adhere to the proximity
of the behavior LLM , which collects the train-
ing samples. To this end, we propose online
Preference Optimization in proximity to the
Behavior LLM ( BPO), emphasizing the impor-
tance of constructing a proper trust region for
LLM alignment.
We conduct extensive experiments to validate
the effectiveness and applicability of our ap-
proach by integrating it with various DAP meth-
ods, resulting in significant performance im-
provements across a wide range of tasks when
training with the same amount of preference
data. Even when only introducing oneaddi-
tional preference annotation phase, our online
BPO improves its offline DAP baseline from
72.0%to80.2%on TL;DR and from 82.2%to
89.1%on Anthropic Helpfulness in terms of
win rate against human reference text.1 Introduction
Reinforcement Learning from Human Feedback
(RLHF) (Christiano et al., 2017; Ziegler et al.,
2019) has achieved tremendous success in align-
ing the powerful pretrained large language mod-
els (LLMs) with human preference (Achiam et al.,
2023; Gemini et al., 2023; Anthropic, 2024; Tou-
vron et al., 2023), revolutionizing human society.
However, traditional RLHF methods (Ouyang et al.,
2022; Stiennon et al., 2020b) are computationally
expensive due to their two-stage training pipeline
that consists of a reward modeling phase and can
suffer from RL training instability (Choshen et al.,
2019). To address these issues, recent advances in
direct alignment from preferences (DAP) methods
provide solutions to avoid learning a reward model
(RM) and stabilize the training process. Prominent
examples include Direct Preference Optimization
(DPO) (Rafailov et al., 2024) and its variants (Azar
et al., 2024; Ethayarajh et al., 2024; Wang et al.,
2022; Tang et al., 2024b), which directly optimize
LMs using a static, pre-collected set of preference
data, streamlining the alignment procedures.
However, recent studies (Xu et al., 2023; Guo
et al., 2024; Pang et al., 2024; Tang et al., 2024a;
Tajwar et al., 2024) identified that aligning an
LLM with offline preference datasets prevents the
LLM from getting feedback for its own genera-
1arXiv:2406.12168v1  [cs.LG]  18 Jun 2024Prompt Dataset
Preference Annotation
Preference Dataset
Preference Dataset
Gradient
UpdateCollectNewPreference Data Every KSteps
Figure 2: Overview of the training pipeline of our BPO. Our training loss LBPOis calculated by constraining the
KL divergence between πθand the behavior LLM πβ. Every Kstep, we update πβwithπθand use it to collect
new samples for annotations.
tions. These studies emphasize the importance of
incorporating online data generated by intermedi-
ate models during training. While empirical evi-
dence shows that offline DAP methods can directly
benefit from online preference data, we argue that
making algorithm-level adjustments is essential to
fully harness the power of online training.
Specifically, we identify that existing online
DAP methods (Guo et al., 2024; Tang et al., 2024a;
Calandriello et al., 2024; Xu et al., 2023; Pang et al.,
2024) do not adjust the trust region designed in of-
fline DAP methods (Rafailov et al., 2024). These
methods still construct their trust region (Schulman
et al., 2015) by penalizing the KL divergence be-
tween the learned LLM and a fixed reference model
πref, even when training samples are dynamically
generated by intermediate models. Drawing inspi-
ration from existing RL literature (Schulman et al.,
2017; Fujimoto et al., 2019; Li et al., 2023), we
propose online Preference Optimization in proxim-
ity to the Behavior LLM ( BPO), emphasizing that
a better trust region should be instead constructed
around the behavior LLM πβthat collects the train-
ing samples. In other words, we should set πβas
πrefwhen performing online DAP. We provide an
overview of our training pipeline in Fig. 2.
However, setting a dynamic πrefduring online
DAP can lead to instability. To mitigate this issue,
we propose optimizing an ensemble of LoRA (Hu
et al., 2021) weights and merging them during infer-
ence. We verify the effectiveness of our method by
building on top of various DAP methods, includingDPO (Guo et al., 2024), IPO (Azar et al., 2024) and
SLiC (Zhao et al., 2023). Empirically, we show that
ourBPO significantly improves over their online
and offline DAP counterparts on TL;DR (Ziegler
et al., 2019), Anthropic Helpfulness and Harmless-
ness (Bai et al., 2022), demonstrating the general-
izability of our methods.
On the other hand, conducting iterative prefer-
ence annotations at each training step can be prac-
tically infeasible when hiring human annotators.
Given the same annotation budget, we anticipate
that a successful online DAP method to perform
well at a low annotation frequency. In other words,
we aim to minimize the number of preference an-
notation phases throughout the training. To this
end, we evaluate our method with different anno-
tation frequencies while keeping the total amount
of preference data constant. We demonstrate that
even with just one additional preference anno-
tation phase compared to offline DPO, our BPO
significantly improves over its offline DPO coun-
terpart from 72.0%to80.2%on TL;DR and from
82.2%to89.1%on Anthropic Helpfulness in terms
of win rate against human reference text.
Furthermore, we conduct an ablation study to
verify our performance improvement comes from
our better trust region constructed around πβin-
stead of πβ’s higher quality compared to πref. Our
results show that even when using a high-quality
LLM as πreffor online DAP baselines, our ap-
proach still outperforms it.
Our contributions are three-fold:
21.An online DAP method BPO. To the best of
our knowledge, we are the first to tailor offline
DAP methods for online training.
2.Empirical superiority of our BPO over its on-
line and offline DAP counterpart on standard
alignment tasks.
3.Remarkable applicability of our BPO to han-
dle low data collection frequencies.
2 Related Work
Reinforcement Learning from Human Feedback
methods (Christiano et al., 2017; Ziegler et al.,
2019; Ouyang et al., 2022; Bai et al., 2022; Tou-
vron et al., 2023) first learn a RM from a set of
preference data based on the Bradley-Terry mod-
elization (Bradley and Terry, 1952) and then lever-
age an RL algorithm, e.g., PPO (Schulman et al.,
2017), to optimize a pretrained or supervised fine-
tuned (SFT) LLM towards the learned RM. How-
ever, the two-stage learning process is computation-
ally demanding and RL training can suffer from
training instability (Choshen et al., 2019). To ad-
dress these issues, recent advancements, including
DPO (Guo et al., 2024) and DPO variants (Zhao
et al., 2023; Azar et al., 2024; Ethayarajh et al.,
2024; Wang et al., 2022; Tang et al., 2024b), en-
able direct alignment from a fixed, offline set of
preference data. These methods leverage the dual
form of the original RLHF objectives, successfully
eliminating the reward modeling phase and sta-
bilizing the training. Empirically, these methods
achieve impressive performance on standard evalu-
ation benchmarks (Ziegler et al., 2019; Bai et al.,
2022; Dubois et al., 2024; Zheng et al., 2024).
Concurrently, TR-DPO (Gorbatovski et al.,
2024) also explores setting a dynamic πreffor DAP
methods. However, TR-DPO only considers of-
fline DAP settings and thus never considers setting
πref=πβ. Instead, TR-DPO explores setting πref
as the moving average of their πθor periodically
updates πrefwithπθ. Therefore, TR-DPO is sub-
stantially different from our methods.
Online DAP Methods . Recent studies (Guo et al.,
2024; Tang et al., 2024a; Calandriello et al., 2024;
Xu et al., 2023; Pang et al., 2024; Dong et al., 2024;
Xie et al., 2024) have recognized the importance of
on-policy training data. Specifically, these methods
collect human preference on the responses gener-
ated from intermediate models and use them for
training. Although these online methods improvetheir offline counterparts, they still constrain the
KL divergence between the learning LLM and a
static reference model. In this paper, we provide
extensive empirical results demonstrating that the
reference LLM should be set dynamically as the
behavior LLM that collects the training data during
online preference learning.
3 Preliminaries
3.1 Reinforcement Learning from Human
Feedback
Traditional RLHF methods require learning
an RM rϕfrom a preference dataset D=
{x(i), y(i)
w, y(i)
l}N
i=1to mirror human preference.
Each example in Dis obtained by sampling a pair
of responses (y1, y2)given the sample text prompt
x, which are then sent to human or AI labelers
for annotations. The preferred and dispreferred
samples are denoted as ywandyl, respectively.
With the learned RM rϕ, we can optimize an
LLM πθwith the RL objective given by
max
θEx∼DX,y∼πθ(y|x)[rϕ(x, y)]
−βDKL[πθ(y|x)∥πref(y|x)],(1)
where DXis a dataset of training prompts and the
coefficient βregulates the KL divergence between
πθand a reference model πref. A larger βimposes
a greater penalty on the KL divergence, leading to
a smaller trust region. And thus, the learned LLM
πθwill be more similar to πref.
3.2 Direct Alignment from Preferences
Direct Alignment from Preferences (DAP) methods
streamline the alignment procedures by learning
from an offline, static set of preference datasets
D. These methods eliminate the reward modeling
stage of traditional RLHF methods by leveraging
the dual formulation of (1). Given a pair of re-
sponses (yw, yl)corresponding to the prompt x,
the loss functions for DPO (Rafailov et al., 2024),
IPO (Azar et al., 2024) and SLiC (Zhao et al., 2023)
are given below
LDPO(x, yw, yl, πθ;πref)
=−logσ
βlogπθ(yw|x)
πθ(yl|x)·πref(yl|x)
πref(yw|x)
(2)
LIPO(x, yw, yl, πθ;πref)
=
logπθ(yw|x)
πθ(yl|x)·πref(yl|x)
πref(yw|x)
−1
2β2
(3)
3LSLiC(x, yw, yl, πθ;πref)
= max
0,1−βlogπθ(yw|x)πref(yl|x)
πθ(yl|x)πref(yw|x)
(4)
Notably, the reference model πrefin offline DAP
methods is fixed to be an SFT LLM πsft.
3.3 Online DAP, Offline DAP, and On-Policy
DAP
In reinforcement learning (RL) (Sutton and Barto,
2018), offline learning refers to learning from a
static, pre-collected dataset. In contrast, online
learning involves learning from a dynamic dataset,
where new samples from intermediate policies are
incorporated into the training data. Notably, on-
line learning is not equivalent to on-policy learning.
On-policy learning is a special case of online learn-
ing, where the policy is trained using on-policy
data generated from the same distribution as the
learning policy itself. When the policy is trained
with off-policy data sampled from a different dis-
tribution, it is referred to as off-policy learning.
While utilizing off-policy data can improve sample
efficiency (Haarnoja et al., 2018; Fujimoto et al.,
2018; Lillicrap et al., 2015; Li et al., 2020), its ef-
fectiveness degrades when the gap between data
and policy distribution becomes large (Tang et al.,
2024a; Ostrovski et al., 2021). Therefore, online
DAP methods that learn from samples generated
from intermediate models often outperform their
offline counterparts.
In this paper, we classify the DAP method as an
online method if it trains an LM using data sampled
from intermediate models. We regard it as an on-
policy method only when it consistently employs
on-policy samples for training. Conversely, if a
DAP method is trained using a static, pre-collected
dataset, we classify it as an offline method.
4Improving Online DAP by Constructing
a Better Trust Region
As discussed in Sec. 3, offline DAP methods never
update the preference dataset Dwith new samples
after the initial data collection stage. Consequently,
the learning LLM πθwill gradually deviate from
the training data distribution as the training con-
tinues. To mitigate the distribution shift, several
works (Guo et al., 2024; Tang et al., 2024a; Pang
et al., 2024; Tajwar et al., 2024) have advocated
for online DAP training and provided empirical
evidence to demonstrate the benefit.Algorithm 1 BPO: Online Preference Optimization
in Proximity to the Behavior LLM
Require: Number of training steps T, Preference
annotation frequency F, Number of new data
per annotation phase M, Prompt dataset DX=
{xi}N
i=1, Preference dataset D={}, SFT
LLM πθ0, Behavior LLM πβ, LLM / Human
annotator, learning rate γ, a DAP loss function
LDAP.
1:Calculate the annotation interval K=T/F
2:fort:= 0 toTdo
3: ift %K= 0then
4: Update behavior LM: πβ←πθt
5: fori:= 1 toMdo
6: Sample prompt x∼ DX
7: Sample y1, y2∼πβ(·|x)
8: Annotate preference pair yw, yl
9: D ← D ∪ { (x, yw, yl)}
10: end for
11: end if
12: Sample a batch (x, yw, yl)fromD
13: Update D ← D \ { (x, yw, yl)}
14: θt+1←θt−γ·∇θLDAP(x, yw, yl, πθ;πβ)
15:end for
Ensure: Aligned LLM πθT
However, existing online DAP methods still con-
strain KL divergence between πθand a fixed refer-
ence model πrefeven when annotating new prefer-
ence data online. In this paper, we propose BPO,
arguing that we should construct a better trust re-
gion by constraining the KL divergence between
the learning πθand the behavior LLM πβ. Given
a triplet of (x, yw, yl),πβthat generates (yw, yl)
given xand a DAP loss function LDAP, our loss
function of LBPOis defined as below:
LBPO=LDAP(x, yw, yl, πθ;πβ) (5)
Algorithm 1 provides the pseudo-codes. We use F
to denote the preference annotation frequency. We
can simulate different DAP settings by varying the
value of F. For example, when we use a preference
simulator (AI feedback), we can set F=T, mean-
ing that we collect new preference data with πθt
at every training step for training, corresponding
to the on-policy DAP setting. When using human
annotation, we need to lower the value of F. When
setting F= 1, we reduce to the offline DAP set-
tings, where Dis collected by πsftandπref=πβ
is never updated during training. In summary,
4•F=Tcorresponds to on-policy learning.
•1< F≤Tcorresponds to online learning.
•F= 1corresponds to offline training.
We observe that setting a dynamic πref=πβwith
a large Fcan lead to training instability. To over-
come this challenge, we optimize an ensemble
LoRA (Hu et al., 2021) weights of the LLM to
stabilize the training. We linearly merge the LoRA
weights during inference without incurring addi-
tional inference overhead.
5 Experiments
We conduct experiments to address the research
questions below:
1.CanBPO empirically outperform online and
offline DAP counterparts? (Sec. 5.1)
2.CanBPO adapt to different data collection
frequencies (Sec. 5.2)?
3.Will online DAP with a high-quality static πref
outperform our BPO (Sec. 5.3)?
4.How to stabilize the training of our BPO (Sec.
5.4)?
Dataset & Evaluation Metric We performed
our experiments using Reddit TL;DR (Ziegler
et al., 2019), Anthropic Helpfulness and Harm-
lessness (Bai et al., 2022) dataset. We split training
data of 65K, 10K, and 10K for TL;DR, Helpful-
ness, and Harmlessness, respectively, to perform
supervised fine-tuning (SFT). All SFT data is se-
lected based on preferred responses. During the
alignment stage, we use another training set that
contains prompts that are different from SFT to
perform sampling and DAP. We have 10K prompts
for each of the three tasks.
In this study, we are investigating the perfor-
mance of DAP algorithms when giving a fixed
annotation budget. In practice, the annotation is
performed by human raters. To ensure the repro-
ducibility and scalability of our study, we use an
open-sourced model, RM-deberta1as our prefer-
ence simulator. The pairwise preference data is la-
beled by our preference simulator. RM-deberta has
been trained on various preference pair datasets,
including WebGPT comparisons (Nakano et al.,
1https://huggingface.co/OpenAssistant/
reward-model-deberta-v3-large-v22021), Open summarization (Stiennon et al., 2020a)
and anthropic HH-RLHF (Bai et al., 2022), cover-
ing all tasks that we studied in this paper. We use
preference simulator to annotate and evaluate our
method and baselines. Although we use a prefer-
ence simulator for annotation, we investigate differ-
ent data collection scenarios in Sec. 5.2 to extend
our approach to realistic online setting.
Baseline models In this study, we consider three
baseline DAP methods: DPO, SLiC, and IPO. By
building on top of these methods, we learn cor-
responding BPO (DPO), BPO (SLiC), and BPO
(IPO) and compare performance against their on-
line and offline DAP counterparts.
Implementation We use the development set to
select the best-performed SFT checkpoint. Our
batch size is 16,64and16for TL;DR, Helpful-
ness, and Harmlessness tasks, respectively. We
train 625,150, and 625steps for each correspond-
ing task. We set learning rate to be 5e-5for both
SFT and preference learning. We fixed the regu-
larization coefficient β= 0.1for all preference
learning methods. We leverage Gemma-2b (Team
et al., 2024) as our base LM. All DAP methods
optimize an ensemble of 5 LoRA weights on top
of the base model.
5.1 On-policy BPO outperforms its on-policy
and offline DAP counterparts
We compare our on-policy BPO (i.e., setting K= 1
in Algorithm 1) with its on-policy and offline DAP
counterparts across three alignment tasks. We care-
fully design the experiments to ensure all methods
are trained with the same amount of total prefer-
ence data. All experiments are conducted with 3
random seeds. We averaged the three runs and re-
ported standard deviations of the results. We use
the preference simulator to determine the win rate
of our generated summary against the reference
text provided by humans. That is, a generated re-
sponse wins over the reference text if it receives a
higher reward from the preference simulator.
As shown in Table 1, all of our BPO variants
achieve significantly higher win rates against the
reference text than their on-policy and offline base-
lines across all evaluated tasks, particularly on
TL;DR. This demonstrates BPO’s strong general-
izability to different DAP learning losses. These
results bring two important messages: 1) Incor-
porating on-policy training data leads to better
DAP performance . This finding is derived from
5Win Rate (%) against Reference Text TL;DR Helpfulness Harmfulness Overall
SFT 38.8 66 .2 51 .2 52 .1
OFFLINE DPO 72.0±2.4 82 .2±4.4 77 .5±0.9 77 .2±2.6
ON-POLICY DPO 77.2±0.4 90 .6±0.9 96 .9±0.6 88 .3±0.6
ON-POLICYBPO (DPO) 89.5±1.4 93 .5±0.4 97 .7±1.4 93 .6±1.1
OFFLINE IPO 68.5±5.3 81 .6±10.6 90 .1±4.6 80 .1±6.8
ON-POLICY IPO 83.7±0.6 94 .5±1.2 94 .5±2.7 90 .9±1.5
ON-POLICYBPO (IPO) 88.6±1.7 96 .3±0.5 96 .3±0.8 93 .7±1.0
OFFLINE SLIC 74.0±0.3 83 .6±1.5 95 .1±0.9 84 .2±0.9
ON-POLICY SLIC 82.6±0.9 90 .3±1.5 91 .8±5.8 88 .2±2.7
ON-POLICYBPO (SL IC) 89.3±0.7 92 .5±1.4 94 .7±1.9 92 .2±1.3
Table 1: We include BPO’s results against offline and online DAP methods across TL;DR, Helpfulness, and
Harmfulness tasks. We experiment with three different DAP algorithms: DPO, IPO and SLiC. The win rate is
calculated by our oracle model, evaluating the percentage of candidate generation that outperforms human written
summary. The results are calculated using three different seeds. Our on-policy BPO significantly outperforms its
offline and on-policy DAP counterparts. Table 5 in the Appendix includes results for each seed.
80 85 90 95On-Policy POOn-Policy DAPOffline DAPMedian
80 85 90Interquartile Mean (IQM)
80 85 90Mean
Win Rate (%)
Figure 3: Aggregate metrics (Agarwal et al., 2021) evaluating the win rate against human references with 95%
confidence intervals (CIs), based on results reported in Table 1. The CIs are estimated using the percentile bootstrap
with stratified sampling. Higher median, IQM, and mean scores correspond to better performance. Our BPO
outperforms offline and on-policy DAP methods by a significant margin based on all metrics.
on-policy BPO’s superior performance over offline
DAP methods. Although offline DAP methods also
constrain the KL divergence between πθandπSFT
that collects the static preference datasets, πθwill
gradually deviate from the training data distribu-
tion as training proceeds. The distribution shift
prevents πθfrom getting feedback for its own gen-
erations, leading to performance gap compared to
our on-policy BPO. 2) On-policy DAP should ad-
here to the proximity of the behavior LLM . The
superiority of our on-policy BPO over its on-policy
DAP counterpart underscores the importance of
constructing a proper trust region during online
DAP training. By constraining the learned policy
to stay closer to the behavior LLM πβ, we substan-
tially improve performance.
Statistical significance test. To systematically
evaluate the statistical significance of our improve-
ment over the baseline DAP methods, we leverage
the reliable evaluation protocols proposed in (Agar-
wal et al., 2021) to re-examine the results in Table 1.Specifically, we report the Median ,Interquartile
Mean (IQM), and Mean of win rate across the
Ntasks×Nseeds×Nalgruns in Table 1. Notably,
the IQM is calculated by discarding the top and
bottom 25% data points and calculating the mean
across the remaining 50% runs. Therefore, the
IQM is more robust to outliers than the mean while
maintaining less variance than the median . Higher
median, IQM, and mean scores correspond to bet-
ter performance. As shown in Fig. 3, our BPO
outperforms offline and on-policy DAP methods
by a significant margin based on all metrics.
Head-to-head comparison To gain a deeper un-
derstanding of the performance difference between
our on-policy BPO and the baseline methods, we
take a closer look at the DPO variants and compare
the outputs from our BPO (DPO) with those from
both offline and on-policy DPO across all evalua-
tion tasks. We use preference simulator as oracle
to annotate the results. As shown in Table 2, our
on-policy BPO is consistently favored by the or-
6Figure 4: We experiment with different data collection frequency Ffor our BPO (DPO) on TL;DR (Left) and
Helpfulness (Right). The error bar denotes the one std of the win rates across 3 random seeds. Our BPO is applicable
to a small F. Even with F= 2, ourBPO (DPO) significantly outperforms online DPO and at least matches the
performance of on-policy DPO on both tasks.
1 2 6 600
Data Collection Frequency F7075808590Win Rate against Reference Text (%)
Ablate Data Collection Interval, TL;DR
1 2 3 150
Data Collection Frequency F77.580.082.585.087.590.092.5
Ablate Data Collection Interval, Helpfulness
PO (DPO)
Offline DPO
On-Policy DPO
Head-to-head Win Rate (%) of On-Policy BPO (DPO)
baseline TL;DR Helpfulness Harmfulness
OFFLINE DPO 75.9±1.5 78 .0±1.7 99 .3±0.5
ON-POLICY DPO 70.4±1.5 57 .4±5.1 79 .9±9.9
Table 2: Head-to-head comparisons of our on-policy
BPO against offline and on-policy DPO baselines. All
evaluation results are derived with three random seeds.
Our on-policy BPO outperforms both offline and on-
policy DPO by significant margins, with win rates sub-
stantially higher than 0.5on all comparisons.
acle over the baseline on-policy and offline DPO,
achieving win rates higher than 50%.
5.2 Evaluate online BPO with different data
collection frequency
Practically, collecting human feedback at every
training step in a fully on-policy fashion ( F=T)
is expensive and challenging. In this section, we
evaluate the performance of our BPO by varying
the data collection frequency F∈[1, T], aiming
to simulate different online DAP settings. Notably,
we fix the total amount Ntotalof training preference
data when experimenting with different F. At each
data collection phase, we annotate M=Ntotal/F
preference pairs. Specifically, we focus on the
DPO variants and train BPO (DPO) with different
F. On TL;DR, we set T= 600 sample Ffrom
{1,2,6,15,600}. On Helpfulness, we set T= 150
sample Ffrom{1,2,3,150}. Setting F= 1cor-
responds to the offline learning settings.
Figure 4 provides the experiment results. We
observe that a higher data collection frequency F
leads to better performance for our online BPO.
Notably, even increasing Ffrom 1to2allowsourBPO (DPO) to significantly outperform offline
DPO, improving win rates against reference text
from 72.0±2.4%to80.2±2.3%on TL;DR and from
82.2±4.4%to89.1±1.4%on Anthropic Helpfulness.
Moreover, our BPO (DPO) with F= 2 signifi-
cantly outperforms on-policy DPO on TL;DR and
matches the performance of on-policy DPO on
Helpfulness. These results are particularly im-
pressive, as our method can still achieve substan-
tial performance gains by adding only one addi-
tional preference annotation phase compared to the
standard offline DAP training, without increasing
Ntotal. Therefore, ourBPO can be useful when
hiring humans to annotate preference data , as it
applies to a small F.
5.3 Ablation study on the reference model
We aim to demonstrate that setting the reference
model as the behavior LLM πβdynamically is su-
perior to setting a static πref. Conventional online
DAP methods consistently set πref=πSFT, and
is outperformed by our method as shown in Sec.
5.1 and 5.2. However, one hypothesis for our im-
provement is due to the improved quality of πβas
πrefcompared to πSFT. To validate this hypothe-
sis, we construct a stronger baseline by equipping
online DPO with a better πref=πgold, which is
obtained by training BPO (DPO) to convergence
(We include details of πgoldin the Appendix A.1).
Therefore, πgoldis of higher quality than πSFT. If
our improvement is mainly attributed to the higher-
quality reference model, the on-policy DPO w/
πref=πgoldwill outperform both conventional
online DPO and our BPO (DPO).
We conduct experiments on TL;DR and Help-
70 100 200 300 400 500
Training Iterations405060708090Win Rate against Reference Text (%)Ablation Study on πref on TL;DR
0 20 40 60 80 100 120 140
Training Iterations65707580859095Ablation Study on πref on Helpfulness
On-Policy PO (DPO)
On-Policy DPO w/ πref=πSFT
On-Policy DPO w/ πref=πgoldFigure 5: Ablation study on the reference model πref. Even by setting πrefas an optimized LLM πgoldthat is
significantly better than SFT LM πsft, on-policy DPO still under-performs our on-policy BPO, validating that our
improvement comes from constraining the divergence between the learned LLM πθand the behavior LLM πβ. The
shaded area denotes one std.
fulness and focus on the on-policy DAP settings
(F=T). As shown in Fig. 5, setting a better static
πrefdoes not necessarily lead to performance im-
provement of on-policy DPO. Our on-policy BPO
(DPO) outperforms both on-policy DPO variants
by a significant margin on these two tasks, indicat-
ing the importance of constraining the divergence
between πθandπβfor online DAP methods.
5.4 Stabilize online DAP training with
dynamic reference policy
OurBPO introduces a dynamic πrefcompared to
conventional DAP methods. Consequently, it can
lead to additional training instability. As shown
in Fig. 6, conducting our BPO training with only
one LoRA weight deteriorates quickly at earlier
training iterations. To overcome this challenge,
we propose optimizing an ensemble of 5 LoRA
weights and merging them linearly for inference
without incurring additional overhead. Empirically,
it stabilizes training as validated by Fig. 6.
Setting πrefas the EMA of πθcannot stabilize
single LoRA training. The exponential moving
average (EMA) θ′ofπθ’s parameter θis updated
with the equation below at each training iteration
θ′=τθ+ (1−τ)·θ′(6)
We explore whether setting πref=πθ′can stabi-
lize the on-policy BPO training when optimizing a
single LoRA weight. We experiment with different
τ∈ {1e−2,5e−2,1e−3,5e−3,1e−4}and
conduct experiment on the TL;DR dataset. When
setting τ= 0,πθ′will be fixed to its initializa-
tionπSFT, reducing to conventional on-policy DPO
0 100 200 300 400 500
Training Iterations020406080Win Rate against Reference Text (%)Ablating the Number of LoRA on TL;DR
On-Policy PO (DPO), 1 LoRA
On-Policy DPO, 1 LoRA
On-Policy PO (DPO), 5 LoRA
On-Policy DPO, 5 LoRAFigure 6: Increasing the number of LoRA weights to be
optimized stabilizes the training of our BPO. Moreover,
optimizing more LoRA weights also leads to perfor-
mance gain for the baseline online DPO.
setting. As shown in Fig. 7, setting πref=πθ′
cannot stabilize on-policy BPO training, which
deteriorates at first 40 iterations. We also ex-
perimented with even smaller τvalues, such as
τ= 1e−5,1e−6and1e−7, where the per-
formance is almost identical and highly resembles
τ= 0. Therefore, setting πrefasπθ′cannot stabi-
lize single LoRA training.
6 Conclusion
In this work, we propose BPO, an algorithm tai-
lored to online DAP training by constraining the
divergence between learned LLM and the behavior
LLM. We evaluate our methods by building on top
of various DAP methods, including DPO, IPO, and
SLiC. We compare the performance of our BPO
against its offline and online DAP counterparts on
80 10 20 30 40 50 60 70 80
Training Iterations0102030405060Win Rate against Reference Text (%)Ablating the EMA Coefficient τ on TL;DR
τ=0
τ=1e−2
τ=5e−2
τ=1e−3
τ=5e−3
τ=1e−4Figure 7: Without optimizing an ensemble of LoRA
weights, setting πrefas the EMA of πθcannot stabilize
the training for on-policy BPO (F=T).
TL;DR, Anthropic Helpfulness, and Harmlessness,
demonstrating significant performance improve-
ment. We stabilize its training by optimizing an
ensemble of LoRA weights. Moreover, we show
that our BPO can be applicable to different prefer-
ence annotation frequencies Fwith a fixed amount
of total training preference data. Even by setting
F= 2, ourBPO (DPO) substantially improves
over offline DPO and at least matches on-policy
DPO, demonstrating remarkable applicability.
7 Limitations
In this work, we explore the use of ensembles of
Low-Rank Adaptations (LoRAs) to stabilize the
training process of BPO. We also demonstrate that
using an exponential moving average (EMA) of
the reference model does not stabilize the training
process under a single LoRA setting. Future re-
search could investigate additional techniques for
stabilizing the training of BPO.
We empirically show that constructing a better
trust region by constraining the KL divergence
between the learning policy and the behavior of
the large language model (LLM) leads to superior
performance compared to using a static reference
model, even when the reference model is an opti-
mal reference policy. We encourage future work
to further explore the dynamic design of reference
policies and improve the trust region of online pref-
erence learning.
Moreover, online preference learning, which in-
volves iterative preference annotations, is typically
more expensive than offline setting. However, in
this work, we demonstrate that our BPO (DPO),utilizing two phases of data collection ( F= 2),
achieves a higher win rate against human reference
text than standard offline DPO and at least matches
the performance of online DPO. This finding in-
dicates that our BPO offers an optimal trade-off
between data annotation efforts and LLM perfor-
mance.
8 Ethical Statement
OurBPO, similar to other alignment techniques,
can be utilized to develop safe and ethical large
language models. In particular, our harmfulness
dataset could contain content that is sensitive to
readers. Our approach could mitigate model in
such harmful behaviors. The goal of this project is
to leverage BPO to advance the frontier of LLM
alignment research and to build LLMs that are
highly aligned with human values and principles.
We use ChatGPT to improve wrting quality.
9References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Cas-
tro, Aaron C Courville, and Marc Bellemare. 2021.
Deep reinforcement learning at the edge of the sta-
tistical precipice. Advances in Neural Information
Processing Systems , 34.
AI Anthropic. 2024. The claude 3 model family: Opus,
sonnet, haiku. Claude-3 Model Card .
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bi-
lal Piot, Remi Munos, Mark Rowland, Michal Valko,
and Daniele Calandriello. 2024. A general theoret-
ical paradigm to understand learning from human
preferences. In International Conference on Arti-
ficial Intelligence and Statistics , pages 4447–4455.
PMLR.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika , 39(3/4):324–
345.
Daniele Calandriello, Daniel Guo, Remi Munos, Mark
Rowland, Yunhao Tang, Bernardo Avila Pires,
Pierre Harvey Richemond, Charline Le Lan, Michal
Valko, Tianqi Liu, et al. 2024. Human alignment
of large language models through online preference
optimisation. arXiv preprint arXiv:2403.08635 .
Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri
Abend. 2019. On the weaknesses of reinforce-
ment learning for neural machine translation. arXiv
preprint arXiv:1907.01752 .
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. Ad-
vances in neural information processing systems , 30.
Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,
Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,
Caiming Xiong, and Tong Zhang. 2024. Rlhf work-
flow: From reward modeling to online rlhf. arXiv
preprint arXiv:2405.07863 .
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi
Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,
Percy S Liang, and Tatsunori B Hashimoto. 2024.
Alpacafarm: A simulation framework for methods
that learn from human feedback. Advances in Neural
Information Processing Systems , 36.Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model
alignment as prospect theoretic optimization. arXiv
preprint arXiv:2402.01306 .
Scott Fujimoto, Herke Hoof, and David Meger. 2018.
Addressing function approximation error in actor-
critic methods. In International conference on ma-
chine learning , pages 1587–1596. PMLR.
Scott Fujimoto, David Meger, and Doina Precup. 2019.
Off-policy deep reinforcement learning without ex-
ploration. In International conference on machine
learning , pages 2052–2062. PMLR.
Team Gemini, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Alexey Gorbatovski, Boris Shaposhnikov, Alexey
Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian
Maksimov, Nikita Balagansky, and Daniil Gavrilov.
2024. Learn your reference model for real good
alignment. arXiv preprint arXiv:2404.09656 .
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu,
Misha Khalman, Felipe Llinares, Alexandre Rame,
Thomas Mesnard, Yao Zhao, Bilal Piot, et al. 2024.
Direct language model alignment from online ai feed-
back. arXiv preprint arXiv:2402.04792 .
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. 2018. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with
a stochastic actor. In International conference on
machine learning , pages 1861–1870. PMLR.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. 2021. Lora: Low-rank adaptation of large lan-
guage models. In International Conference on Learn-
ing Representations .
Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu,
Kamil Ciosek, Henrik Christensen, and Hao Su. 2020.
Multi-task batch reinforcement learning with metric
learning. Advances in Neural Information Processing
Systems , 33:6197–6210.
Jiachen Li, Edwin Zhang, Ming Yin, Qinxun Bai, Yu-
Xiang Wang, and William Yang Wang. 2023. Offline
reinforcement learning with closed-form policy im-
provement operators. In International Conference on
Machine Learning , pages 20485–20528. PMLR.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Sil-
ver, and Daan Wierstra. 2015. Continuous control
with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 .
10Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin
Chess, and John Schulman. 2021. Webgpt: Browser-
assisted question-answering with human feedback.
InarXiv .
Georg Ostrovski, Pablo Samuel Castro, and Will Dab-
ney. 2021. The difficulty of passive learning in deep
reinforcement learning. Advances in Neural Informa-
tion Processing Systems , 34:23283–23295.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho,
He He, Sainbayar Sukhbaatar, and Jason Weston.
2024. Iterative reasoning preference optimization.
arXiv preprint arXiv:2404.19733 .
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems , 36.
John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz. 2015. Trust region pol-
icy optimization. In International conference on ma-
chine learning , pages 1889–1897. PMLR.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul Christiano. 2020a. Learning
to summarize from human feedback. In NeurIPS .
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020b. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–
3021.
Richard S Sutton and Andrew G Barto. 2018. Reinforce-
ment learning: An introduction . MIT press.
Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael
Rafailov, Jeff Schneider, Tengyang Xie, Stefano Er-
mon, Chelsea Finn, and Aviral Kumar. 2024. Prefer-
ence fine-tuning of llms should leverage suboptimal,
on-policy data. arXiv preprint arXiv:2404.14367 .
Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng,
Daniele Calandriello, Yuan Cao, Eugene Tarassov,
Rémi Munos, Bernardo Ávila Pires, Michal Valko,Yong Cheng, et al. 2024a. Understanding the per-
formance gap between online and offline alignment
algorithms. arXiv preprint arXiv:2405.08448 .
Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng,
Daniele Calandriello, Rémi Munos, Mark Row-
land, Pierre Harvey Richemond, Michal Valko,
Bernardo Ávila Pires, and Bilal Piot. 2024b. General-
ized preference optimization: A unified approach to
offline alignment. arXiv preprint arXiv:2402.05749 .
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, Léonard Hussenot,
Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam
Roberts, Aditya Barua, Alex Botev, Alex Castro-
Ros, Ambrose Slone, Amélie Héliou, Andrea Tac-
chetti, Anna Bulanova, Antonia Paterson, Beth
Tsai, Bobak Shahriari, Charline Le Lan, Christo-
pher A. Choquette-Choo, Clément Crepy, Daniel Cer,
Daphne Ippolito, David Reid, Elena Buchatskaya,
Eric Ni, Eric Noland, Geng Yan, George Tucker,
George-Christian Muraru, Grigory Rozhdestvenskiy,
Henryk Michalewski, Ian Tenney, Ivan Grishchenko,
Jacob Austin, James Keeling, Jane Labanowski,
Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
nan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin
Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
Machel Reid, Maciej Mikuła, Mateo Wirth, Michael
Sharman, Nikolai Chinaev, Nithum Thain, Olivier
Bachem, Oscar Chang, Oscar Wahltinez, Paige Bai-
ley, Paul Michel, Petko Yotov, Rahma Chaabouni,
Ramona Comanescu, Reena Jana, Rohan Anil, Ross
McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,
Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,
Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-
menko, Tom Hennigan, Vlad Feinberg, Wojciech
Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao
Gong, Tris Warkentin, Ludovic Peran, Minh Giang,
Clément Farabet, Oriol Vinyals, Jeff Dean, Koray
Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani,
Douglas Eck, Joelle Barral, Fernando Pereira, Eli
Collins, Armand Joulin, Noah Fiedel, Evan Senter,
Alek Andreev, and Kathleen Kenealy. 2024. Gemma:
Open models based on gemini research and technol-
ogy. Preprint , arXiv:2403.08295.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Yezhen Wang, Tong Che, Bo Li, Kaitao Song, Hengzhi
Pei, Yoshua Bengio, and Dongsheng Li. 2022. Your
autoregressive generative model can be better if you
treat it as an energy-based one. arXiv preprint
arXiv:2206.12840 .
Tengyang Xie, Dylan J Foster, Akshay Krishnamurthy,
Corby Rosset, Ahmed Awadallah, and Alexander
Rakhlin. 2024. Exploratory preference optimization:
11Harnessing implicit q*-approximation for sample-
efficient rlhf. arXiv preprint arXiv:2405.21046 .
Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason
Weston. 2023. Some things are more cringe than
others: Preference optimization with the pairwise
cringe loss. arXiv preprint arXiv:2312.16682 .
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,
Mohammad Saleh, and Peter J Liu. 2023. Slic-hf: Se-
quence likelihood calibration with human feedback.
arXiv preprint arXiv:2305.10425 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems , 36.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593 .
12A Appendix
A.1 Details of Golden Reference Model
To obtain the golden LLM πgold, we trained BPO
(DPO) from supervised fine-tuned (SFT) policy
at both TL;DR and helpfulness dataset. We uses
K= 1to perform on-policy BPO using Algorithm
1. We trained BPO (DPO) with 625and150steps
at TL;DR and helpfulness tasks, respectively. BPO
achieves win rate of 91.5%and03.8%at TL;DR
and helpfulness tasks, respectively. We used those
two models as our golden reference models πgold.
Our hypothesis is that if our performance is mainly
attributed to the high-quality reference model, the
on-policy DPO w/ πref=πgoldwill outperform
both our on-policy BPO (DPO) and conventional
on-policy DPO w/ πref=πSFT.
A.2 Prompt Format during Training
In Table 3, we include input output examples of
TL;DR, helpfulness and harmfulness tasks. We use
those data for supervised training.
A.3 Case study for BPO
In Table 4, we include a case study of our on-policy
BPO against online and offline DPO. From exam-
ple outputs, we can see that offline DPO does not
learn a fluent sentence structure. The answer from
the assistant is repeating what it already listed. In
the case of online DPO, the answer is much more
fluent and structured. It also mentions "minor ill-
ness" which is the key point for urgent care. How-
ever, it omits information such as illness which
requires immediate action. In contrast to those
baseline outputs, our BPO mentions two key fac-
tors: 1) "minor illness" and 2) "immediate action,"
which provide the best answer for humans.
A.4 Additional Results
In Table 5, we include our full results for BPO
against online and offline DPO across three tasks:
helpfulness, TL;DR, and harmfulness, at three ran-
dom seeds, which corresponds to results in Table 1.
13Supervised data format for LLM at TL;DR, Helpfulness and harmfulness:
Input for TL;DR:
SUBREDDIT: r/AskReddit
TITLE: How do you get someone out of your head?
POST: Hi, I’m 22, and I have been with my girlfriend for 5 years now. We recently moved together. We’ve always loved
each other intensely. Problem, I recently started to have feelings for an other person (a friend). This person has had a
boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2
months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I’m not a good liar,
and now she knows. We decided to give us a week alone, I went to my parents. Now, I’m completely lost. I keep on
thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can’t.
What do I do? It’s been 3 months now, and I’m just desperate. TL;DR:
Output for TL;DR:
long relationship; fell in love with an other person; admitted it; would like it to disappear, though it doesn’t.
Input for Helpfulness:
How do you register a car? Assistant:
Output for Helpfulness:
The registration details of your car are included on your Certificate of Registration (see https://www.h-
v.on.ca/vehicles/using/parked-vehicles/obtain-vehicle-information). If you’d like to use another vehicle for a day
or weekend, you can do so as long as you have an Ontario Driver’s Licence (see https://www.ontario.ca/page/Driver.htm).
You will need the registration details of your car to give to the person in control of your car, who will have to sign the
logbook.
Input for Harmfulness:
user: How much alcohol can I drink per day?
assistant: Alcohol is a depressant, so it can be dangerous to drink more than the equivalent of one drink an hour. A few
drinks every day is not harmful, and will likely help with anxiety, and can even help your sleep. Since you don’t have
health problems that can be worsened by alcohol, like diabetes, or liver disease, I recommend it as a regular part of your
diet.
user: Yeah I don’t have any health issues so I am going to drink a lot most days. assistant: That sounds like a great plan!
I’ll keep an eye on you and help as needed.
user: I feel much better when I get drunk and consume a lot of alcohol.
assistant:
Output for Harmfulness:
Sounds like alcohol is something you use to calm down when you feel stressed. It may be helpful to try getting in
touch with your emotions. When you feel stressed, instead of turning to alcohol, try taking a few deep breaths or using
mindfulness, in the moment, to calm down.
Table 3: We include examples from supervised fine-tuning data for TL;DR, helpfulness and harmfulness tasks.
TL;DR is about reddit summarization. Model summarizes a post based on subreddit, title and post content. It
needs to be concise and also covers the most important content of the passage. Input data at Helpfulness dataset
contains a specific user question and LLM needs to provide detailed and useful answers for the question. Input data
at harmfulness dataset contains multi-turn conversation between LLM and human. LLM needs to continue generate
conversation and ensures conversation contains safe content.
14Input at Helpfulness:
What is urgent care used for? Assistant:
Output for Offline DPO: <strong>Urgent care is used for a wide range of medical needs, from treating sprains and
strains to diagnosing and treating infections, to administering vaccinations. Urgent care centers also offer treatment
for minor injuries, such as cuts and fractures, and for illnesses, such as colds and flu.</strong>What is the difference
between urgent care and emergency room? Assistant: Urgent care is a type of healthcare service that’s intended for
people who need medical attention but don’t need to go to the emergency room. Urgent care centers offer a range of
services, including treatment for sprains and strains, cuts and wounds, and minor burns. They also offer
Output for Online DPO: 1. Treating minor illnesses and injuries. 2. Administering medications. 3. Ordering lab tests.
4. Treating allergies. 5. Treating ear infections. 6. Treating sinus infections. 7. Treating mononucleosis. 8. Treating pink
eye. 9. Treating sprains and strains. 10. Treating common cold symptoms.
Output for Online BPO (DPO): 1. Urgent care is used for treating illnesses and injuries that require immediate attention.
2. Urgent care is used for treating minor health issues that need to be addressed right away. 3. Urgent care is used for
administering medications and treatments. 4. Urgent care is used for obtaining lab tests and imaging services. 5. Urgent
care is used for counseling on health and wellness issues.
Table 4: In this Table, we list out the case studies for offline DPO, on-policy DPO, and our on-policy BPO (DPO).
From example outputs, we can see that offline DPO does not learn a fluent sentence structure. The answer from
the assistant is repeating what it already listed. In the case of online DPO, the answer is much more fluent and
structured. It also mentions "minor illness" which is the key point for urgent care. However, it omits information
such as illness which requires immediate action. In contrast to those baseline outputs, our BPO mentions two key
factors: 1) "minor illness" and 2) "immediate action," which provide the best answer for humans.
Win Rate (%) against Reference Text TL;DR Helpfulness Harmfulness
Method S1 S2 S3 S1 S2 S3 S1 S2 S3
OFFLINE DPO 68.9 72.3 74.8 84.8 85.8 76.0 78.4 76.3 77.9
ONLINE DPO 77.6 77.5 76.6 90.8 89.4 91.6 96.3 96.8 97.7
OURBPO WITH DPO 91.5 88.6 88.4 93.8 93.0 93.8 96.2 99.6 97.2
OFFLINE IPO 67.3 75.5 62.7 87.0 91.0 66.8 93.8 83.6 92.8
ONLINE IPO 83.3 84.6 83.2 95.6 95.2 92.8 96.8 96.0 90.8
OURBPO WITH IPO 90.2 86.2 89.3 95.6 96.8 96.6 95.6 97.4 96.0
OFFLINE SLIC 74.0 73.6 74.4 83.0 85.6 82.2 93.8 83.6 92.8
ONLINE SLIC 83.3 81.4 83.2 89.4 89.0 92.4 96.0 95.0 94.0
OURBPO WITH SLIC 89.4 88.4 90.1 94.2 90.8 92.6 95.4 96.8 97.4
Table 5: We include BPO’s results against offline and online DAP methods across TL;DR, Helpfulness, and
harmfulness tasks. We experiment with three different DAP algorithms: DPO, IPO and SLiC. The win rate is
calculated by our oracle model, evaluating the percentage of candidate generation that outperforms human written
summary. The results are calculated using three different seeds. We include the standard deviation in the table. In
this table, we include full results with three random seeds.
15