Can Transformers Learn n-gram Language Models?
Anej Svete
 Nadav Borenstein
Mike Zhou
 Isabelle Augenstein
 Ryan Cotterell
ETH Zürich
 University of Copenhagen
 University of Pennsylvania
{nb,augenstein }@di.ku.dk mikezhou@seas.upenn.edu
{asvete ,ryan.cotterell }@inf.ethz.ch
Abstract
Much theoretical work has described the ability
of transformers to represent formal languages.
However, linking theoretical results to empiri-
cal performance is not straightforward due to
the complex interplay between the architecture,
the learning algorithm, and training data. To
test whether theoretical lower bounds imply
learnability of formal languages, we turn to
recent work relating transformers to n-gram
language models (LMs). We study transform-
ers’ ability to learn random n-gram LMs of two
kinds: ones with arbitrary next-symbol proba-
bilities and ones where those are defined with
shared parameters. We find that classic estima-
tion techniques for n-gram LMs such as add- λ
smoothing outperform transformers on the for-
mer, while transformers perform better on the
latter, outperforming methods specifically de-
signed to learn n-gram LMs.
github.com/rycolab/learning-ngrams
1 Introduction
A large body of work has investigated the ability
of transformers (Vaswani et al., 2017) to represent
formal languages (Strobl et al., 2023). Such results
tell us what languages transformers can represent ,
but not how well they can learn them from data.
Existing work has tested the learning abilities of
transformers as classifiers mapping strings to lan-
guage membership decisions (e.g., Bhattamishra
et al., 2020; Delétang et al., 2023). However, lan-
guage models (LMs) are not classifiers of strings
but rather distributions over them. In this light, a re-
cent line of work has argued for a more direct eval-
uation of LMs as distributions (Svete and Cotterell,
2023; Nowak et al., 2023; Svete et al., 2024a,b;
Borenstein et al., 2024; Nowak et al., 2024).
In the pursuit to understand transformers as prob-
ability distributions, n-gram LMs have recently
emerged as a useful playground for analyzing trans-
formers’ interpretability (Liu et al., 2024; V oita
et al., 2024), learning behavior (Edelman et al.,2024; Chen et al., 2024), in-context learning (Ols-
son et al., 2022; Arora et al., 2024; Akyürek et al.,
2024; Nguyen, 2024), and representational capac-
ity (Svete and Cotterell, 2024). Svete and Cotterell
(2024), in particular, show that transformers can
encode any n-gram LM, lower-bounding their rep-
resentational capacity and suggesting that encoding
n-gram statistics is indeed a feasible mechanism for
transformers to process language. However, being
able to represent n -gram statistics is insufficient to
rely on them in practice. For that, learning them is
necessary. This motivates the question: Given that
transformers can represent n-gram LMs, how good
are they at learning them? Answering this brings us
closer to a practical understanding of transformers,
showing whether n-gram statistics are practically
relevant for their behavior and illuminating what
theoretical results on neural architectures imply
about their ability to learn formal languages.
We study how well transformers can learn n-
gram LMs. On the one hand, we find that they gen-
eralize well on n-gram LMs whose next-symbol
probabilities depend on a linear combination of
features of the n-gram symbols (representation-
based n-gram LMs), outperforming baselines
such as count-based techniques and models with
hand-crafted features. On the other, we find that
transformers perform worse than count-based tech-
niques on n-gram LMs with arbitrary next-symbol
probabilities. This illuminates transformers’ induc-
tive biases: It shows that they struggle to approxi-
mate even simple probability distributions whose
next-symbol probabilities are arbitrary while being
good at learning representation-based LMs.
2 Preliminaries
We begin by introducing some preliminaries. An
alphabet Σis a finite, non-empty set of symbols .
TheKleene closure Σ˚of the alphabet Σis the set
of all strings of the symbols in Σ. The length of
the string y“y1. . . y TPΣ˚, denoted by|y|“
T, is the number of symbols it contains. We willarXiv:2410.03001v1  [cs.CL]  3 Oct 2024useyj
idef“yi¨¨¨yjto denote substrings between
positions iandjinclusive.
Alanguage model pis a probability distribution
overΣ˚. Two LMs pandqareweakly equivalent
ifppyq“qpyqfor all yPΣ˚. Many LMs define
ppyqautoregressively:
ppyqdef“ppEOS|yq|y|ź
t“1ppyt|yătq,(1)
where EOSRΣis a special end-of-string symbol.
We denote Σdef“ΣYtEOSu.
When defined autoregressively, next-symbol dis-
tributions ppyt|yătqin an n-gram LM satisfy the
n-gram assumption, stated formally below.
Assumption 2.1. The n-gram assumption states
that the conditional probability of the symbol yt
given yătonly depends on n´1previous symbols:
pnpyt|yătq“pn`
yt|yt´1
t´n`1˘
. (2)
We will refer to nas the order of the n-gram LM
andyt´1
t´n`1as the history ofyt.
We will denote a length- pn´1qhistory as yn
whenever its position tin the string is unimpor-
tant. Additionally, we assume that the histories are
padded with the beginning- of-string symbol BOSR
Σand denote Σdef“ΣYtBOSufor convenience.
Representation-based n-gram LMs. The next-
symbol probabilities pnpy|ynqofn-gram LMs
can be arbitrary. This requires storing Op|Σ|n´1q
parameters and does not capture the intuitive no-
tion that similar n-grams define similar next-
symbol probabilities. This can be addressed by
representation-based n-gram LMs, which define
pnpy|ynqas a function of a representation ofyn.
Definition 2.1. An n-gram LM pnis
representation-based if there exist a repre-
sentation function h: Σn´1ÑRDand an output
matrix EP|Σ|ˆDfor some DPNsuch that
pnpy|ynq“softmaxpEhpynqqy (3)
for all yPΣandynPΣn´1where
softmaxpxqydef“exppxyqř
y1PΣexppxy1q. (4)
A transformer LM computes the ppyt|yătqus-
ing the self-attention mechanism, which builds the
representation by attending to different symbols inthe preceding string (Vaswani et al., 2017; Radford
et al., 2019).1This can intuitively be connected
to the way n-gram LMs compute the next-symbol
probabilities by attending to the last n´1symbols.
Transformers can represent n-gram LMs.
Svete and Cotterell (2024, Thms. 3.1 and 3.2) show
that, for any n-gram LM, there exists a weakly
equivalent transformer LM. They describe intuitive
mechanisms in which n´1heads (Thm. 3.1) or
n´1layers (Thm. 3.2) attend to the previous n´1
symbols using sparse attention that computes atten-
tion weights with the sparsemax function (Martins
and Astudillo, 2016). The use of sparse attention
is particularly noteworthy since it departs from the
standard soft-attention transformers usually used
in practice (Vaswani et al., 2017). While the theory
does not consider soft-attention transformers con-
cretely, it does suggest that being able to assign no
attention to irrelevant tokens, which is impossible
with soft attention, could be beneficial.
3 Learnability of n-gram LMs
At a high level, we study how well transformers can
learn n-gram LMs. We do that by training trans-
formers on strings sampled from randomly gener-
ated n-gram LMs and measuring a notion of dis-
tance between the trained and the ground-truth LM.
3.1 A Framework for Evaluating LMs
Given an n-gram LM pnand a transformer LM pT,
their KL divergence is defined as
DKLppn||pTqdef“ÿ
yPΣ˚pnpyqlogpnpyq
pTpyq.(5)
The KL divergence is an established measure of
similarity between probability distributions.2As
such, it lends itself naturally to measuring the dif-
ference between LMs; in our case, measuring how
well a neural LM matches an n-gram LM.3This
sidesteps the reliance on proxy measures of fit such
as the next-symbol prediction accuracy, which is
often used for evaluating the learnability of for-
mal languages (Borenstein et al., 2024). We evalu-
ate model performance by approximating DKLon
held-out sets; see App. B.3 for details.
1For space reasons, we do not give a detailed description
of the transformer architecture. See App. B.2.1 for the details
of the models used in the experiments.
2KL divergence is not a distance, as it is not symmetric
and does not fulfill the triangle inequality.
3DKLis also the quantity minimized when training LMs
under the MLE objective (Cotterell et al., 2024).3.2 Experimental Setup
We study the learnability of n-gram LMs by
(1)sampling n-gram LMs of three types de-
scribed in §3.2.1, (2)generating train4and test
datasets, (3)training baselines and transformers,
and(4)computing the KL divergence between the
trained LMs and the ground-truth LMs.
Axes of comparison. We investigate the learn-
ability of n-gram LMs along two axes:
Axis 1: Whether the n-gram LM is representation-
based or not and the degree to which its
parameters are shared, and
Axis 2: Measures of complexity of the n-gram
LM:
• the order nof the n-gram LM,
• the size|Σ|of the alphabet, and
• the rank Rof the output matrix E.
The first axis is motivated by the parameter-sharing
nature of transformers while the second is moti-
vated by the wish to understand the effect of the
complexity of the n-gram LM on its learnability.
3.2.1 Data Generation
We generate training and test datasets by sampling
strings from randomly generated n-gram LMs of
three types: (1)non-representation-based, general,
n-gram LMs whose conditional next-symbol prob-
ability distributions are arbitrary, (2)sparse repre-
sentation-based n-gram LMs, and (3)dense rep-
resentation-based n-gram LMs. The difference
between sparse and dense representation-based n-
gram LMs lies in the degree to which the parame-
ters of the n-gram LM are shared—in the former,
individual symbols of the alphabet define indepen-
dent parameters (meaning that the size of the rep-
resentations grows with |Σ|), whereas, in the latter,
the parameters are shared across symbols (meaning
that the size of the representations is independent
of|Σ|). See App. B.1 for more details.
We control the complexity of the n-gram LM
through the following parameters: n,|Σ|, and, for
representation-based LMs, the rank Rof the output
matrix E.5Concretely, we generate representation-
based datasets with nP t4,8,12uand|Σ| P
t64,128,256u. For dense representation-based n-
gram LMs, we vary E’s rank RPt2,8,16u. Due
to the space complexity of storing |Σ|n´1different
4The development dataset which we use to select the hy-
perparameters is taken from the training dataset.
5The rank Rhas been identified as a significant predictor
of the learnability of general LMs (Borenstein et al., 2024).Parameter Description
pn nPNOrder of pn
pn |Σ|PNSize of Σ
pn HppnqPREntropy of p
pn RPNRank of E
pn DensePt0,1uDense representa-
tions
pT LPNNumber of layers
pT HPNNumber of heads
pTsparsemaxPt0,1uSparse attention
Table 1: Predictors of DKLppn||pTq.
conditional distributions, we generate general n-
gram LMs with nPt2,4,6uand|Σ|“t8,12,16u
and additional dense representation-based n-gram
LMs of the same complexity for comparison. We
generate five random n-gram LMs for each of the
configurations and sample disjoint training and test
datasets of size NTrain“50kandNTest“30k,
respectively. See App. B.1 for more details.
3.2.2 Models
Transformer models. We use transformers (TF)
of different sizes. Inspired by the theoretical con-
structions considered, we are particularly interested
in how the number of attention heads and layers
affects the learnability of n-gram LMs. We investi-
gate this by varying the number of heads and layers.
Moreover, since the theoretical constructions rely
onsparse attention mechanisms, we also investi-
gate the effect of using the sparsemax (Martins and
Astudillo, 2016) for computing attention weights.
More details are given in App. B.2.
Classic techniques. Count-based estimators such
as the maximum-likelihood n-gram estimate have
been used in NLP for decades. They compute the
next-symbol probabilities by counting the occur-
rences of (lower-order) n-grams in the training data.
They are thus well-suited for learning n-gram LMs,
making them a difficult-to-beat baseline. Smooth-
ingestimates the probabilities of unseen n-grams
by redistributing the probability mass of seen n-
grams, regularizing count-based estimators (Katz,
1987; Witten and Bell, 1991; Ney et al., 1994; Gale
and Sampson, 1995; Malagutti et al., 2024).6We
6We expect that the fact that the ground-truth n-gram LMs
are of a high order and representation-based makes the task
for traditional smoothing techniques more difficult and may
require a large degree of smoothing.consider add- λ, absolute discounting (Ney et al.,
1994), and Witten–Bell (Witten and Bell, 1991)
smoothing along with the standard maximum like-
lihood solution. These techniques are described in
App. A; see also App. B.2 for details.
Two additional baselines. We study two addi-
tional baselines: a log-linear model (LL) with
fixed, sparse representations of the histories and a
neural n-gram LM (Neural) that learns dense rep-
resentations of the histories. The log-linear model
represents the history ynas a concatenation of the
one-hot encodings of its symbols. It learns the
appropriate output matrix pEPR|Σ|ˆpn´1q|Σ|such
thatpnpy|ynq “softmaxppEhpynqqyapproxi-
mates the training data well. The neural n-gram
LM is based on previous work exploring neural
n-gram LMs (Bengio et al., 2000, 2003, 2006; Sun
and Iyyer, 2021). It learns static symbol representa-
tionsrpyqPRDforyPΣand concatenates rpyiq
foryn“y1¨¨¨yn´1before non-linearly transform-
ing the concatenated representations with an MLP
to compute hpynq. See also App. A.
3.2.3 Statistical Analysis
We determine the importance of n-gram LM pa-
rameters and transformer components on the learn-
ability of n-gram LMs by evaluating how predic-
tive they are of the transformer performance. Con-
cretely, we investigate the predictors listed in Tab. 1
and fit a linear regression model to predict the test
KL divergence. See App. B.4 for more details.
4 Experimental Results and Discussion
This section presents the experimental results. All
tables contain the mean and standard deviation of
the estimate of the KL divergence zDKLbetween
the ground truth and trained LMs estimated on the
test dataset.7Negative values indicate an approxi-
mation error of the (low) DKL.
4.1 The Effect of Parameter Sharing
Tab. 2 compares the performance of transform-
ers and baselines on representation- and non-
representation-based n-gram LMs with n“6.
As expected, simple counting-based methods that
do not assume representation-based ground-truth
LMs perform equally well regardless of whether or
not the n-gram LM is representation-based. Mod-
els that assume representation-based n-gram LMs
(log-linear, neural n-gram, and transformer LMs)
7See App. B.3 for details on the approximation.perform better on representation-based ones. Trans-
formers outperform log-linear models, possibly due
to their better fit for dense representation-based
models, and the simple neural n-gram LMs per-
form best. App. C.1 further analyzes the effect of
the degree to which the parameters of the n-gram
LM are shared, showing that sparse-representation-
based n-gram LMs are more difficult to approxi-
mate with the neural n-gram model and transform-
ers than dense-representation-based ones.
Our results provide a complementary view of
the theory connecting transformers to n-gram LMs.
They show the importance of parameter sharing in
the ground-truth model for transformers to match
the distribution well—in the case of no parameter
sharing, simple count-based methods perform
better. The good performance of transformers is
particularly interesting compared to the simple and
more structured log-linear model.
4.2 The effect of n-gram LM Complexity
We next investigate the effect of the n-gram LM
parameters described in §3.2. Tab. 3 show the per-
formance of the best model configurations for dif-
ferent combinations of nand|Σ|onn-gram LMs
with dense representations and of rank 16. As ex-
pected, the performance of all tested methods de-
cays with increasing n-gram complexity. While
the structured neural n-gram models perform best,
transformers do not fare much worse, outperform-
ing classical methods on all but the smallest models
and the log-linear model in all settings. Rank is
further investigated in App. C.2, where we show
that higher-rank models are more difficult to learn
for neural n-gram and transformer LMs.
4.3 Predictors of Learnability
Tab. 4 shows the linear model coefficients predict-
ing the transformer performance. According to the
absolute values of the coefficients, the most signif-
icant predictor is whether the representations are
dense or sparse, followed by the n-gram LM order
and the alphabet size. The entropy is negatively
correlated with the KL divergence, indicating that
higher-entropy n-gram LMs are easier to learn, in
line with existing work (Borenstein et al., 2024).
As suggested by theoretical work, we find the num-
ber of heads and layers to be significant positive
predictors of the learnability of the n-gram LM.
Soft vs. sparse attention transformers. The
distinction between soft- and sparse-attention trans-|Σ| 8 12 16
Parameter sharing No Yes No Yes No Yes
Classic3.04
p˘1.46q2.36
p˘0.23q17.34
p˘1.01q2.45
p˘0.96q50.35
p˘1.87q3.11
p˘0.42q
LL101.42
p˘3.46q21.96
p˘3.38q109.60
p˘2.06q27.10
p˘4.95q117.83
p˘2.37q28.37
p˘3.53q
Neural60.87
p˘2.87q1.50
p˘0.20q79.77
p˘1.63q1.43
p˘0.78q90.01
p˘1.18q1.63
p˘0.47q
TF67.06
p˘2.91q4.63
p˘2.00q77.95
p˘1.74q2.80
p˘0.72q86.38
p˘1.88q3.68
p˘1.33q
Table 2: Learnability of general and dense representation-based n-gram LMs for n“6.
n 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
Classic2.11
p˘0.39q3.40
p˘0.81q5.00
p˘0.72q25.72
p˘8.48q54.95
p˘4.17q67.64
p˘5.25q58.17
p˘9.36q74.09
p˘6.78q89.90
p˘6.96q
LL32.05
p˘3.14q37.10
p˘9.53q40.61
p˘3.93q40.45
p˘11.01q58.26
p˘3.64q62.00
p˘5.23q43.40
p˘5.46q49.75
p˘4.12q61.60
p˘5.45q
Neural1.14
p˘0.64q1.98
p˘0.91q2.17
p˘1.46q5.96
p˘1.86q9.13
p˘0.88q9.06
p˘0.68q7.71
p˘0.75q9.87
p˘1.28q11.20
p˘1.09q
TF2.43
p˘0.51q5.04
p˘1.75q5.63
p˘1.80q9.52
p˘1.98q14.72
p˘0.95q16.89
p˘1.33q14.73
p˘1.70q22.79
p˘6.42q33.99
p˘9.06q
Table 3: The effect of the order nand the alphabet size |Σ|on the performance of the best performing models.
Predictor pβ p -value
Intercept 0.99p˘0.03qă10´6
n 0.63p˘0.01qă10´6
|Σ| 0.28p˘0.01qă10´6
R 0.17p˘0.02qă10´6
Hppnq ´ 0.05p˘0.02q0.034
Dense´1.28p˘0.04qă10´6
L´0.33p˘0.01qă10´6
H´0.15p˘0.01qă10´6
sparsemax´0.06p˘0.02q0.002
Table 4: Estimated coefficients ( pβ) and p-values of the
linear model predicting zDKL. The R2value is 0.665.
formers made by Svete and Cotterell (2024) (and
the distinction between soft- and hard-attention
transformers in theoretical literature) makes the im-
pact of using sparse attention particularly interest-
ing. As shown by Tab. 4, using sparse attention is a
significant predictor of better performance, which
agrees with the intuition that zero attention weights
are useful for representing n-gram LMs. This is
further investigated in App. C.3, showing the su-
perior performance of sparse-attention transform-
ers. While these results agree with the intuition
regarding the importance of sparse attention made
by Svete and Cotterell (2024), they depart fromthe softmax-computed attention weights normally
used in practice (Vaswani et al., 2017). This en-
courages further investigation into the possible ben-
efits of sparse attention mechanisms in transform-
ers, particularly in the context of learning formal
languages—could sparse attention, for example,
aid generalization in situations where non-zero at-
tention weights could lead to accumulating errors,
such as those in counting (Weiss et al., 2018)?
5 Conclusion
We compare the performance of transformers to
that of classical n-gram estimation techniques and
hand-crafted baselines at learning artificially gen-
erated n-gram LMs. Transformers show a good in-
ductive bias towards learning representation-based
n-gram LMs but fare worse than classic estimation
techniques at learning general n-gram LMs, un-
derlining the importance of parameter sharing in
the ground-truth model for transformers to learn it
well. Moreover, the impact of the number of heads
and layers on the performance agrees with exist-
ing theoretical results. The better performance of
sparse-attention transformers motivates further in-
vestigation into how sparse attention could be used
when learning formal languages with transformers.Limitations
We highlight some limitations of our work. First,
note that the concrete theoretical results this pa-
per investigates rely on a formulation of the
transformer architecture that is not practical for
training—most notably, Svete and Cotterell’s
(2024) results either assume the use of hard atten-
tion or the use of the more practical sparse attention
but unbounded positional encodings, which are not
commonly used in practice. We replace these mod-
eling assumptions with more practical ones—we
use the softmax and sparsemax normalization func-
tions and bounded positional encodings. We also
note that the LMs we use to evaluate transformers—
n-gram LMs—are very simple, which makes the
results less generalizable to more complex LMs.
This is done on purpose to stay as close as possible
to the theoretical results connecting transformers
ton-gram LMs.
Acknowledgements
We thank Luca Malagutti for his help during the
early stages of this project. Ryan Cotterell ac-
knowledges support from the Swiss National Sci-
ence Foundation (SNSF) as part of the “The For-
gotten Role of Inductive Bias in Interpretability”
project. Anej Svete is supported by the ETH AI
Center Doctoral Fellowship. This research was par-
tially funded by a DFF Sapere Aude research leader
grant under grant agreement No 0171-00034B, the
Danish–Israeli Study Foundation in Memory of
Josef and Regine Nachemsohn, and the Privacy
Black & White project, a UCPH Data+ Grant. This
work was further supported by the Pioneer Centre
for AI, DNRF grant number P1.
References
Ekin Akyürek, Bailin Wang, Yoon Kim, and Ja-
cob Andreas. 2024. In-context language learn-
ing: Architectures and algorithms. arXiv preprint
arXiv:2401.12973 .
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys
Johnson, Michael Poli, James Zou, Atri Rudra, and
Christopher Ré. 2024. Zoology: Measuring and im-
proving recall in efficient language models. In The
Twelfth International Conference on Learning Rep-
resentations, ICLR 2024, Vienna, Austria, May 7-11,
2024 . OpenReview.net.
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. InAdvances in Neural Information Processing Systems ,
volume 13. MIT Press.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res. , 3:1137–1155.
Yoshua Bengio, Holger Schwenk, Jean-Sébastien Sené-
cal, Fréderic Morin, and Jean-Luc Gauvain. 2006.
Neural Probabilistic Language Models , pages 137–
186. Springer Berlin Heidelberg, Berlin, Heidelberg.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
2020. On the ability and limitations of transformers
to recognize formal languages. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 7096–7116,
Online. Association for Computational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python: analyzing text
with the natural language toolkit . " O’Reilly Media,
Inc.".
Nadav Borenstein, Anej Svete, Robin Chan, Josef
Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor
Chodroff, and Ryan Cotterell. 2024. What languages
are easy to language-model? a perspective from learn-
ing probabilistic regular languages. arXiv preprint
arXiv:2406.04289 .
Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho,
Matthew L. Leavitt, and Naomi Saphra. 2024. Sud-
den drops in the loss: Syntax acquisition, phase tran-
sitions, and simplicity bias in mlms. In The Twelfth
International Conference on Learning Representa-
tions, ICLR 2024, Vienna, Austria, May 7-11, 2024 .
OpenReview.net.
Stanley F. Chen and Joshua Goodman. 1999. An em-
pirical study of smoothing techniques for language
modeling. Computer Speech & Language , 13(4):359–
394.
Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu,
and Li Du. 2024. Formal aspects of language model-
ing. arXiv preprint arXiv:2311.04329 .
Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim
Genewein, Li Kevin Wenliang, Elliot Catt, Chris
Cundy, Marcus Hutter, Shane Legg, Joel Veness, and
Pedro A. Ortega. 2023. Neural networks and the
Chomsky hierarchy. In 11th International Confer-
ence on Learning Representations .
Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran
Malach, and Nikolaos Tsilivis. 2024. The evolution
of statistical induction heads: In-context learning
markov chains. arXiv preprint arXiv:2402.11004 .
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics , pages 1–8, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.William A. Gale and Geoffrey Sampson. 1995. Good-
turing frequency estimation without tears. Journal of
Quantitative Linguistics , 2(3):217–237.
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing , 35(3):400–401.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin
Choi, and Hannaneh Hajishirzi. 2024. Infini-gram:
Scaling unbounded n-gram language models to a tril-
lion tokens. arXiv preprint arXiv:2401.17377 .
Ilya Loshchilov and Frank Hutter. 2018. Fixing weight
decay regularization in Adam. arXiv.org .
Luca Malagutti, Andrius Buinovskij, Anej Svete, Clara
Meister, Afra Amini, and Ryan Cotterell. 2024. The
role of n-gram smoothing in the age of neural net-
works. arXiv preprint arXiv:2403.17240 .
André F. T. Martins and Ramón F. Astudillo. 2016.
From softmax to sparsemax: A sparse model of at-
tention and multi-label classification. In Proceed-
ings of the 33rd International Conference on Interna-
tional Conference on Machine Learning - Volume 48 ,
ICML’16, page 1614–1623.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modelling. Computer Speech & Lan-
guage , 8(1):1–38.
Timothy Nguyen. 2024. Understanding trans-
formers via n-gram statistics. arXiv preprint
arXiv:2407.12034 .
Franz Nowak, Anej Svete, Alexandra Butoi, and Ryan
Cotterell. 2024. On the representational capacity of
neural language models with chain-of-thought rea-
soning. In Proceedings of the 62nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , Bangkok, Thailand. Asso-
ciation for Computational Linguistics.
Franz Nowak, Anej Svete, Li Du, and Ryan Cotterell.
2023. On the representational capacity of recurrent
neural language models. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 7011–7034, Singapore. As-
sociation for Computational Linguistics.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. 2022. In-context
learning and induction heads. Transformer Circuits
Thread .Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zach DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-
jie Bai, and Soumith Chintala. 2019. Pytorch: An
imperative style, high-performance deep learning li-
brary. arXiv preprint arXiv:1912.01703 .
Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario
Amodei, and Ilya Sutskever. 2019. Language models
are unsupervised multitask learners.
Lena Strobl, William Merrill, Gail Weiss, David Chiang,
and Dana Angluin. 2023. Transformers as recogniz-
ers of formal languages: A survey on expressivity.
arXiv preprint arXiv:2311.00208 .
Simeng Sun and Mohit Iyyer. 2021. Revisiting simple
neural probabilistic language models. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 5181–5188,
Online. Association for Computational Linguistics.
Anej Svete, Robin Shing Moon Chan, and Ryan Cot-
terell. 2024a. A theoretical result on the induc-
tive bias of RNN language models. arXiv preprint
arXiv:2402.15814 .
Anej Svete and Ryan Cotterell. 2023. Recurrent neu-
ral language models as probabilistic finite-state au-
tomata. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 8069–8086, Singapore. Association for Com-
putational Linguistics.
Anej Svete and Ryan Cotterell. 2024. Transformers can
represent n-gram language models. arXiv preprint
arXiv:2404.14994 .
Anej Svete, Franz Nowak, Anisha Sahabdeen, and Ryan
Cotterell. 2024b. Lower bounds on the expressivity
of recurrent neural language models. In Proceed-
ings of the 2024 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Volume
1: Long Papers) , pages 6820–6844, Mexico City,
Mexico. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Elena V oita, Javier Ferrando, and Christoforos Nalm-
pantis. 2024. Neurons in large language models:
Dead, n-gram, positional. In Findings of the Associa-
tion for Computational Linguistics ACL 2024 , pages
1288–1301, Bangkok, Thailand and virtual meeting.
Association for Computational Linguistics.Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On
the practical computational power of finite precision
RNNs for language recognition. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) ,
pages 740–745, Melbourne, Australia. Association
for Computational Linguistics.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory , 37(4):1084–
1094.
Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2021.
Efficient computation of expectations under spanning
tree distributions. Transactions of the Association for
Computational Linguistics , 9:675–690.A Classic n-gram Estimation Techniques
A.1 Maximum Likelihood Estimation (MLE)
The maximum likelihood n-gram LM estimate of order ncomputes the next-symbol probabilities
qn
MLEpy|ynqdef“#pynyq
#pynq, (6)
foryPΣandynPΣn´1, which counts the total number of times the string ynyPΣnoccurs and divides
by the number of total occurrences of the history ynPΣn´1. MLE models are prone to overfitting,
especially when training data is sparse or if history lengths are long, thus demonstrating the need for
smoothing.
A.2 Add- λsmoothing (Add- λ)
Add-λsmoothing is one of the simplest methods of obtaining a smoothed n-gram model from the
MLE estimate qn
MLE. Add- λsmoothing adds a pseudo-count of λto the counts of all possible n-grams,
including those not seen in the training dataset. Mathematically,
#ASpynyqdef“#pynyq`λ (7)
and
˜qn
ASpy|ynqdef“#pynyq`λ
#pynq`|Σ`1|λ(8)
foryPΣandynPΣn´1. In the special case of λ“1, we have Laplace Smoothing .
A.3 Absolute Discounting (AD) (1994)
Absolute Discounting (AD) involves the interpolation of higher and lower-order n-gram models. Though
higher-order n-grams capture more context, they often suffer from zero probabilities and overfitting due
to limited training data. AD makes higher-order distributions by subtracting a fixed discount δď1from
each nonzero count and recursively interpolates this with n-gram of lower degree. That is:
˜qn
ADpy|ynq“maxt#pynyq´δ,0u
#pynq`p1´λnq˜qn´1
ADpy|yn´1q (9)
foryPΣandynPΣn´1. To make the probabilities sum to 1, we let
1´λn“δ#n
Tpyn,‚q
#pynq, (10)
where #n
Tis the type counts function of order n, formally defined as
#n
Tpyn, yqdef“#
1if#pynyqą0
0otherwise(11)
and#n
Tp‚, yq,#n
Tpyn,‚qand#n
Tp‚,‚qare type count functions with bulleted arguments summed out.
A.4 Witten–Bell Smoothing (WB) (1991)
Though interpolating higher-order n-gram models with lower-order n-gram models helps handle unseen
n-grams, it can sometimes produce undesirable results, especially if an n-gram occurs only in a specific
context. A common illustrative example in the literature (Chen and Goodman, 1999) is the bigram San
Francisco . If the bigram San Francisco shows up frequently in the data set, the word Francisco will have
a high unigram count, thus assigning a higher probability of continuation to Francisco when interpolating
higher-order n-grams with lower order count functions. This, however, may be undesirable, as the term
Francisco will rarely follow any context other than the word San. Witten–Bell smoothing addresses this byconsidering the number of unique continuations a context has observed, based on the intuition that backing
off to lower-order n-gram statistics is more accurate only when there are more distinct continuations of a
given context. The interpolation works as follows:
˜qn
WBpy|ynq“λnqn
MLEpy|ynq`p1´λnq˜qn´1
WBpy|yn´1q (12)
foryPΣandynPΣn´1where
1´λn“#n
Tpyn,‚q
#n
Tpyn,‚q`#pynq. (13)
Substituting, we rewrite the interpolation as
˜qn
WBpy|ynq“#pynyq`#n
Tpyn,‚q˜qn´1
WB
#n
Tpyn,‚q`#pynq. (14)
A.5 A Log-Linear Model
As a simple parameter-sharing representation-based baseline, we consider a log-linear model that repre-
sents the history ynas a concatenation of the one-hot encodings of the symbols in yn.8Concretely, the
model represents the history yn“y1¨¨¨yn´1as
hpy1¨¨¨yn´1qdef“¨
˚˝Jy1K
...
Jyn´1K˛
‹‚Pt0,1upn´1q|Σ|. (15)
Its parameter is an output matrix EPR|Σ|ˆpn´1q|Σ|, which determines the logits of the conditional
distribution pnpy|ynqas
pnpy|ynqdef“softmaxpEhpynqqy. (16)
The model is trained by minimizing the cross-entropy loss between the true conditional distributions pn
and the predicted distributions pn.
A.6 A Neural n-gram LM
The neural n-gram LM is a classic neural LM popularized by Bengio et al. (2000, 2003, 2006) and
modernized by Sun and Iyyer (2021). It learns D1-dimensional word2vec-style static representations of
the symbols and concatenates them before feeding them through an MLP f.9The MLP thus produces the
final representation of the history, which is used to define the next-symbol probabilities together with the
learned output matrix Eas
ppy|ynqdef“softmaxpEhpyătqqy (17a)
“softmax¨
˚˝Ef¨
˚˝¨
˚˝Jy1K
...
Jyn´1K˛
‹‚˛
‹‚˛
‹‚
y. (17b)
The size of the symbol representations as well as the complexity of the MLP f(its depth and width) are
hyperparameters of the model.
8This exactly matches the representations used to generate sparse representation-based n-gram LMs described in App. B.1.1
and thus provides a strong inductive bias for those models, making the log-linear model a strong baseline.
9The neural model studied by Sun and Iyyer (2021) adds a pooling operation over the entire preceding string. However, to
keep the model truly n-gram-based, we do not include this pooling operation and ignore the symbols further than n´1steps in
the past.B Experimental Details
B.1 Data Generation
B.1.1 Generating n-gram LMs
We generate the training and test datasets by randomly generating n-gram LMs and sampling strings
from them. We construct three types of n-gram LMs: 1. General n-gram LMs whose conditional
distributions pnpy|ynqare sampled independently of each other. 2. Sparse representation-based n-gram
LMs. 3. Dense representation-based n-gram LMs.
General n-gram LMs. We sample general n-gram LMs by sampling, for each possible context
yn(including the contexts padded with different numbers of BOS symbols), a conditional distribution
ppy|ynqforyPΣ. The conditional distributions are sampled from a Dirichlet distribution with
concentration parameter α, where we set α“0.1to encourage concentrated distributions. We control
the expected length of the string Er|y|sgenerated by the n-gram LM by hard-coding the probability
pnpEOS|ynqto be1
Er|y|sfor all yn, where we set Er|y|s“40. Due to the requirement of storing all the
|Σ|n´1conditional distributions, we limit ourselves to the case of nPt2,4,6uand|Σ|Pt8,12,16ufor
the general n-gram LM case. This procedure is described with pseudocode in Alg. 1.
Algorithm 1 The generation of a random general n-gram LM.
1.defGENERATE GENERAL n-GRAM LM( n,Σ, α,Er|y|s):
2.forynPŤn´1
j“0tBOSujˆΣn´1´j:
3.ŹIterate through all possible contexts, including BOS-padded contexts.
4. pnpy|ynq„DIRICHLETpα1|Σ|qforyPΣ
5. pnpEOS|ynqÐ1
Er|y|s
6. Renormalize pnpy|ynqforyPΣ
7.return pn
Representation-based n-gram LMs. Representation-based n-gram LMs are generated by defining
the conditional distributions pnpy|ynqin terms of an output matrix Ewhich transforms the vectorial
representations hpynqPRDof the history yninto the (logits of the) conditional distribution pnpy|ynq.
More concretely, let h: Σn´1ÑRDbe an representation function that maps the history ynto a vector in
RD. Then, we define the conditional distribution pnpy|ynqforyPΣas
pnpy|ynqdef“softmaxpEhpynqqy, (18)
where EPR|Σ|ˆDis an output matrix. Once again, we hard-code the probability of the end-of-string
symbol EOS to be1
Er|y|s“1
40for all ynby post-hoc renormalizing the conditional distributions at every
time step.
We consider two representation functions h:
1.Sparse : We define the sparse representation function h: Σn´1Ñ t0,1upn´1q|Σ|as the function
mapping the history y1¨¨¨yn´1to the concatenation of the one-hot encodings of its symbols. In
symbols, this mean
hpy1¨¨¨yn´1qdef“¨
˚˝Jy1K
...
Jyn´1K˛
‹‚Pt0,1upn´1q|Σ|(19)
where JyKPt0,1u|Σ|is the one-hot encoding of the symbol y. Notice that in this case, the size of the
representations grows with the size of the alphabet.
2.Dense : We define a dense representation function h: Σn´1ÑRpn´1qD1as the function mapping
the history y1¨¨¨yn´1to the concatenation of dense embeddings rpyqPRD1of its symbols. We(a) Distribution of the entropy of the datasets from representation-based n-gram LMs.
(b) Distribution of the entropy of the datasets from the general n-gram LMs.
Figure 1: Distributions of the mean string lengths and the LM entropies for the 45generated datasets.
generate the symbol embeddings randomly from a standard normal distribution. We use D1“16for
the dense representation of symbols. Furthermore, since the rank of the output matrix Ewas found
to be a significant predictor of the learnability of general regular LMs (Borenstein et al., 2024), we
control for the rank of the output matrix generating Eby constructing E“E1E2the product of two
random matrices of E1PR|Σ|ˆRandE2PRRˆD, resulting in a rank- Rmatrix of size |Σ|ˆD,
where we vary the rank as RPt2,8,16u. This results in a fixed-size representation of the history,
regardless of the size of the alphabet.
The randomly-generated matrices ( Ein the sparse case and E1andE2in the dense case) are generated by
sampling their entries independently from a standard normal distribution.
The main difference between sparse and dense representation functions is therefore the degree of
parameter sharing between the different symbols in the history. Since, unlike in the general n-gram LM
case, we do not need to store the conditional distributions for all possible histories, we can consider larger
values of nand|Σ|. In particular, we generate datasets with nPt4,8,12uand|Σ|Pt64,128,256u.
B.1.2 Generating Train and Test Datasets
The training and test datasets are generated by sampling strings from the n-gram LMs. Let pnbe an
n-gram LM. We sample disjoint training and test datasets from pnin a multi-step process:
(1.) Sample a large set of strings D1from pn(where strings are not repeated).
(2.)Divide D1“D1
Train\D1
Traininto the set of strings D1
Trainthat are allowed to appear in the training
dataset and a set of strings D1
Trainthat are allowed to appear in the test dataset.
(3.) Sample two large (multi-)sets of strings DTrainandDTestfrom pn.
(4.) Remove all strings from DTrainthat are in D1
Testand all strings from DTestthat are in D1
Train.
(5.) Retain NTrainstrings from DTrainandNTeststrings from DTest.
This procedure ensures that the training and test datasets are disjoint and that the test dataset is not seen
during training.
Fig. 1 shows the distributions of the entropies for the generated datasets.Smoothing Technique Hyperparameter Values
Add-λsmoothing (Add- λ) λ 0.01,0.1,1
Absolute Discounting (AD) δ 0.6,0.8,0.95
Table 5: Hyperparameters for the smoothing techniques.
B.2 Models
B.2.1 Transformer Models
We use the GPT-2 model architecture (Radford et al., 2019) of varying sizes. We sample the number
of heads Hthe number of layers Lfromt1,nu, where nis the learnt n-gram’s order. We also control
for the type of attention activation function, using either the standard softmax or its variant sparsemax .
Moreover, we use an embedding size of 256, a hidden representation size of 512, and an output size (the
dimensionality of the final linear layer) of 128. The number of trainable parameters of the models ranges
between 500k and 4M. We train each model for 10epochs with an input context length of 256using a
batch size of 128and a learning rate of 0.0001 , an AdamW (Loshchilov and Hutter, 2018) optimizer with
default settings, and a standard cross-entropy loss. We did not tie the weights of the word embeddings.
Training a single instance took, on average, 30minutes, with some variations depending on the size of the
dataset and exact architecture size.
B.2.2 Smoothing Techniques
We train a number of smoothing techniques on the generated datasets as a baseline for the transformers.
We use the smoothing techniques described in App. A. Each of them allows us to control the order of
the learned n-gram LM, pn. For each setting, we test out three scenarios: An under-parametrized model
(pn“n´2), a well-parametrized model ( pn“n), and an over-parametrized model ( pn“2nfornPt4,8u
andpn“20forn“12).10This results in models between with a large number of parameters—the largest
model on|Σ|“256symbols of order pn“20defines in the order of 1048parameters—but note that only
the parameters corresponding to observed n-grams are memorized, making the training feasible. Some
of the estimation techniques—MLE and Witten–Bell—do not have any hyperparameters and are trained
with the default settings. For the other techniques (Add- λsmoothing and Absolute discounting), we
train models with the hyperparameters listed in Tab. 5 and report the performance of the best-performing
models. We use the standard nltk implementations (Bird et al., 2009) for these estimation techniques.
B.2.3 Log-Linear Model and Neural n-gram LM
We implemented the log-linear model and the neural n-gram LM in PyTorch (Paszke et al., 2019). The
log-linear model only learns the output matrix Ewhile our implementation of the neural LM closely
follows the model defined by Bengio et al. (2000, 2003, 2006) with the modern improvements studied
by Sun and Iyyer (2021).11As with the smoothing techniques, we study the effects of the order pnof the
trained LM on the performance of the log-linear model and the neural n-gram LM. Thus, we fit models
withpnPtn´2,n,minp2n,20qu, resulting with models with between 4k and 1.25M parameters. The
models are trained by minimizing the cross-entropy loss between the empirical distribution of the training
dataset and the predicted distributions p. For the log-linear model, we use the Adam optimizer (Kingma
and Ba, 2017) with a learning rate of 0.1and a batch size of 1024 . We train the models for 16epochs and
halve the learning rate after every five epochs. The neural n-gram LMs were implemented as a shallow
neural network with three learned layers. The first is an embedding later with a dimensionality of 128.
The second is a fully connected layer with an output dimensionality of 512, ReLU activation function, and
dropout probability of 0.5. The third layer is a softmax-normalized linear transformation for predicting
next-symbol probabilities, i.e., the matrix E. The number of trainable parameters of the models ranges
between 400k and 1.5M. They were trained for 20 epochs with early stopping on a development set that
10While transformer models do not have an analogous hyperparameter to make them fit the ground-truth n-gram LM perfectly,
we vary their sizes (number of heads and layers) to find the best-performing model, as motivated by theoretical work.
11Since we are interested in n-gram LMs, we do not implement the pooling of the longer context from Sun and Iyyer (2021).was sampled from the training set ( 80%–20% split) using a batch size of 128and, learning rate of 5e´5,
and Adam optimizer with default parameters. Training of a single instance took, on average, 5 minutes,
with some variations depending on the size of the dataset and model.
B.3 Evaluation
As described in §3.1, we evaluate the learned LMs by computing the KL divergence between the learned
LMpand the ground-truth n-gram LM pn. Concretely, we can compute DKLppn||pqthrough its
decomposition as
DKLppn||pq“Hppn, pq´Hppnq. (20)
While Hppnqcan be computed analytically with a dynamic program (Eisner, 2002; Zmigrod et al., 2021),
the runtime complexity of Oˆ´
|Σ|n´1¯3˙
makes exact computations infeasible for even moderately
large|Σ|andn. We thus rely on empirical estimates of both terms in Eq. (20) on the test dataset.
Concretely, we compute
pHppnq“´1
NTestÿ
yPDTestlogpnpyq (21a)
pHppn, pq“´1
NTestÿ
yPDTestlogppyq, (21b)
which allows us to compute the empirical approximation of DKLppn||pq,
zDKLppn||pqdef“pHppn, pq´pHppnq. (22)
B.4 Statistical Analysis
To assess the influence of the predictors specified in Tab. 1 on the learnability of the n-gram LMs (i.e,
the empirical KL divergence on the test dataset), we implement a linear regression model predicting the
KL divergence based on the predictors. Before fitting the model, we standardize each parameter with a
z-score transformation for an interpretable comparison of the estimated coefficients.
The linear regression model provides insights into the influence of each predictor on the dependent
variable (KL divergence) by assigning each parameter three key values: the coefficient of the linear model
ˆβi, the standard deviation of the coefficient, and a p-value. The magnitude of the coefficient ˆβiindicates
the strength of the predictor’s effect on the dependent variable. The coefficient’s sign reveals whether this
effect is positive (an increase in the predictor is expected to increase the value of the parameter; in our
case, this indicates that the increase of the parameter is associated with a worse performance of the model
and vice-versa) or negative (an increase in the predictor is expected to decrease the value of the parameter;
in our case, this indicates that the increase of the parameter is associated with a better performance of the
model and vice-versa). The standard deviation measures the variability or uncertainty of the ˆβicoefficient,
providing a sense of the reliability of this estimate. The p-value quantifies the statistical significance of
the effect, indicating the likelihood that the observed relationship occurred by chance. It thus provides a
measure of the reliability of the effect, with lower p-values indicating a more reliable effect.
C Additional Results
C.1 The Effect of Parameter Sharing
In §4.1 (particularly, in Tab. 2), we compared the performance of the models of interest across general and
dense representation-based n-gram LMs. As expected, neural LMs fare much better with representation-
based n-gram LMs, whereas the difference in the performance of counting-based methods is smaller.
Tabs. 6a and 6b provide results for additional model orders and alphabet sizes, showing the same trends as
Tab. 2—all learning methods perform better on representation-based n-gram LMs across all settings.
The next natural question is whether the degree to which the parameters are shared affects the per-
formance as well. To determine that, Tab. 7a shows the performance on one-hot representation-basedn 2 4 6
|Σ| 8 12 16 8 12 16 8 12 16
Classic´0.08
p˘0.94q´0.35
p˘0.78q0.18
p˘1.49q´1.00
p˘0.39q0.23
p˘1.41q1.86
p˘1.96q3.04
p˘1.46q17.34
p˘1.01q50.35
p˘1.87q
LL35.31
p˘7.55q50.57
p˘12.27q59.76
p˘4.96q86.03
p˘2.05q103.36
p˘2.95q111.95
p˘5.41q101.42
p˘3.46q109.60
p˘2.06q117.83
p˘2.37q
Neural´0.19
p˘1.04q´0.46
p˘0.81q0.07
p˘1.57q0.91
p˘0.51q16.29
p˘1.74q40.00
p˘2.78q60.87
p˘2.87q79.77
p˘1.63q90.01
p˘1.18q
TF0.07
p˘1.09q´0.48
p˘0.82q0.18
p˘1.50q0.18
p˘0.53q4.68
p˘1.61q10.98
p˘2.35q67.06
p˘2.91q77.95
p˘1.74q86.38
p˘1.88q
(a) Learnability of general n-gram LMs.
n 2 4 6
|Σ| 8 12 16 8 12 16 8 12 16
Classic´0.25
p˘0.53q´0.35
p˘1.47q0.55
p˘1.00q0.72
p˘0.66q0.89
p˘0.69q1.28
p˘0.90q2.36
p˘0.23q2.45
p˘0.96q3.11
p˘0.42q
LL25.23
p˘9.12q26.83
p˘9.12q29.10
p˘7.82q19.67
p˘4.28q24.03
p˘5.47q26.55
p˘6.62q21.96
p˘3.38q27.10
p˘4.95q28.37
p˘3.53q
Neural´0.39
p˘0.60q´0.52
p˘1.73q0.26
p˘0.41q0.61
p˘0.65q0.35
p˘0.66q1.15
p˘0.72q1.50
p˘0.20q1.43
p˘0.78q1.63
p˘0.47q
TF1.45
p˘2.34q1.58
p˘3.83q1.34
p˘0.35q4.55
p˘3.02q1.82
p˘0.49q3.46
p˘1.15q4.63
p˘2.00q2.80
p˘0.72q3.68
p˘1.33q
(b) Learnability of small representation-based n-gram LMs.
Table 6: Learnability of general and (small) representation based n-gram LMs.
n-gram LMs. For an easier comparison to the results on dense representation-based n-gram LMs, the
results from Tab. 3 are reproduced in Tab. 7b. Transformers perform better on dense representation-based
n-gram LMs across all settings, as expected, since those more closely fit their modeling assumptions.
In fact, transformers do not perform much better than classical smoothing techniques on some of the
configurations. While dense-representation-based models are also better learned by smoothing methods,
the log-linear model interestingly performs better on the sparse n-gram LMs (apart from n“4). This is
in line with the specification of the log linear model: The model a priori assumes a sparse-representation-
based n-gram LM, and thus fits the model specifications well, which again shows the effect of correct
model specification.
C.2 The Effect of the Rank
In §4.2, we investigated the trends in the performance with respect to the order of the n-gram model and
the size of the alphabet (cf. Tab. 3). Here, we also consider the rank RofE. The results for varying
ranks are presented in Tab. 8. They again follow the intuitions from theory. The performance smoothing
methods and the log-linear model, which can by design implement any-rank dense-representation-based
n-gram LM, is unaffected by R. The performance of neural n-grams and transformers (which in our
experiments all have rank at most 128—the output dimension), however, is negatively correlated with
the rank. This is confirmed by the analysis of the predictors of the transformer performance (cf. Tab. 4),
although the effect is not as significant and strong as for the other predictors.
C.3 The Effect of Sparse Attention
Tab. 4 suggests a significant impact of the use of sparse attention on the ability to learn n-gram LMs. This
is confirmed by looking at the difference in the performance of soft- and sparse-attention transformers in
Tab. 9.n 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
Classic16.78
p˘3.99q28.67
p˘2.20q47.18
p˘7.87q76.08
p˘8.97q94.45
p˘7.19q112.85
p˘8.70q104.57
p˘5.87q130.03
p˘6.96q141.89
p˘6.28q
LL40.36
p˘5.40q42.44
p˘2.34q56.95
p˘8.99q36.33
p˘8.23q40.95
p˘5.66q53.01
p˘7.57q32.43
p˘3.13q40.01
p˘3.97q51.14
p˘4.73q
Neural´1.14
p˘3.81q0.84
p˘1.91q17.82
p˘7.08q5.27
p˘6.44q13.08
p˘5.27q35.84
p˘6.68q7.61
p˘2.91q22.48
p˘3.76q45.11
p˘4.15q
TF0.63
p˘3.73q6.90
p˘1.93q35.03
p˘7.47q10.36
p˘6.95q40.33
p˘9.09q98.79
p˘10.55q14.38
p˘3.77q70.50
p˘14.64q118.36
p˘5.80q
(a) Learnability of sparse representation-based n-gram LMs with R“16.
n 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
Classic2.11
p˘0.39q3.40
p˘0.81q5.00
p˘0.72q25.72
p˘8.48q54.95
p˘4.17q67.64
p˘5.25q58.17
p˘9.36q74.09
p˘6.78q89.90
p˘6.96q
LL32.05
p˘3.14q37.10
p˘9.53q40.61
p˘3.93q40.45
p˘11.01q58.26
p˘3.64q62.00
p˘5.23q43.40
p˘5.46q49.75
p˘4.12q61.60
p˘5.45q
Neural1.14
p˘0.64q1.98
p˘0.91q2.17
p˘1.46q5.96
p˘1.86q9.13
p˘0.88q9.06
p˘0.68q7.71
p˘0.75q9.87
p˘1.28q11.20
p˘1.09q
TF2.43
p˘0.51q5.04
p˘1.75q5.63
p˘1.80q9.52
p˘1.98q14.72
p˘0.95q16.89
p˘1.33q14.73
p˘1.70q22.79
p˘6.42q33.99
p˘9.06q
(b) Learnability of dense representation-based n-gram LMs with R“16.
Table 7: Learnability of sparse and dense representation-based n-gram LMs.
|Σ| 64 128 256
R 2 8 16 2 8 16 2 8 16
Classic75.82
p˘3.52q72.86
p˘5.39q49.41
p˘8.95q79.71
p˘9.06q82.67
p˘4.94q68.35
p˘5.64q79.21
p˘6.90q94.94
p˘8.97q86.94
p˘6.74q
LL45.82
p˘3.52q47.98
p˘4.49q43.40
p˘5.46q48.94
p˘4.10q54.77
p˘4.84q49.75
p˘4.12q61.66
p˘9.63q60.69
p˘5.13q61.60
p˘5.45q
Neural2.28
p˘2.48q4.54
p˘1.02q7.71
p˘0.75q2.16
p˘4.03q5.08
p˘1.04q9.87
p˘1.28q0.31
p˘4.79q7.34
p˘1.55q11.20
p˘1.09q
TF5.49
p˘24.78q9.06
p˘3.57q13.96
p˘1.93q5.90
p˘11.60q11.05
p˘1.61q19.01
p˘2.64q4.34
p˘30.98q14.30
p˘2.70q25.15
p˘2.33q
Table 8: The effect of the rank Ron the performance of the best performing models for n“12.
C.4 The Effect of Over-parametrization
The baselines used in the experiments in the main part of the paper (classic smoothing techniques,
the log-linear model, and the neural n-gram model) are particularly well-specified for the LMs they
approximate—in particular, they were trained with the correct order of the ground-truth n-gram LM, n.
Apart from the intuitions offered by the theoretical constructions, such an appropriate parametrization
is more difficult to specify for transformers, whose parameters do not match the n-gram definition as
closely. In this section, we investigate how much possible misspecification of the baselines impacts their
performance and compare it to the performance of the largest and smallest transformers trained.
To test the effect of over-parametrization , that is, assuming a too-large order pn, Tab. 10a shows the
performance of the best-performing baseline models with pn“2nfornPt4,8uandpn“20forn“12.
Again, Tab. 7b serves as reference for the performance of the best models. Tab. 10a also shows the
performance of the largest transformer models for each of the settings (that is, one where both the
number of layers and heads are largest).12Over-parameterization is particularly harmful to models whose
12According to the theoretical constructions by Svete and Cotterell (2024), such transformers are over-parametrized—onlyn 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
softmax2.58
p˘0.52q5.45
p˘1.50q6.52
p˘1.73q14.47
p˘5.67q22.37
p˘7.95q23.51
p˘8.49q23.74
p˘8.82q37.52
p˘7.51q47.79
p˘7.64q
sparsemax2.43
p˘0.51q5.04
p˘1.75q5.63
p˘1.80q9.52
p˘1.98q14.72
p˘0.95q16.89
p˘1.33q14.73
p˘1.70q22.79
p˘6.42q33.99
p˘9.06q
Table 9: Performance of soft- and sparse-attention transformers on dense representation n-gram LMs with R“16.
number of parameters increases most—the classical smoothing techniques. Other models are less affected.
In particular, the neural n-gram model performs as well as the optimally-parametrized variant, and
transformers remain close to their best performance, with the exception of the largest model.
We contrast this to the effects of under-parameterization , where we set pn“n´2and constrain the
transformers to a single head and layer. The results of these runs are presented in Tab. 10b. Under-
parametrization noticeably degrades the performance of most models and is most noticeable with the
smaller orders n. Due to the fast growth of the number of parameters in the classical models, under-
parametrization actually outperforms over-parameterized models, likely due to the parameter-sharing
nature of the ground-truth n-gram LMs, possibly making them more easily approximatable with lower-
order models. Single-head and single-layer transformers, in contrast, perform noticeably worse than their
well- or over-parametrized variants, again in line with the intuitions from the theoretical constructions.
n 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
Classic3.49
p˘0.30q7.30
p˘1.55q10.21
p˘0.75q29.98
p˘9.82q61.97
p˘4.51q73.69
p˘5.46q50.22
p˘9.03q68.50
p˘5.64q87.01
p˘6.85q
LL33.69
p˘3.14q40.24
p˘9.53q43.49
p˘4.84q63.38
p˘14.44q83.96
p˘9.37q91.11
p˘7.78q66.45
p˘9.08q70.05
p˘11.32q93.89
p˘17.32q
Neural1.28
p˘0.49q2.27
p˘0.98q2.36
p˘1.43q6.26
p˘1.87q9.31
p˘1.00q9.14
p˘0.77q7.76
p˘0.81q9.91
p˘1.29q11.26
p˘1.08q
TF2.42
p˘0.52q5.02
p˘2.24q5.51
p˘1.83q10.33
p˘1.85q14.59
p˘1.01q17.39
p˘1.13q15.55
p˘1.54q26.52
p˘6.95q42.61
p˘7.57q
(a) Results of the over-parametrized models on representation-based n-gram LMs.
n 4 8 12
|Σ| 64 128 256 64 128 256 64 128 256
Classic76.53
p˘24.55q84.94
p˘42.59q87.56
p˘9.99q22.41
p˘8.48q54.95
p˘4.17q67.64
p˘5.25q49.41
p˘8.95q68.36
p˘5.67q86.94
p˘6.74q
LL97.80
p˘23.05q105.58
p˘45.45q114.81
p˘11.48q47.93
p˘16.66q72.46
p˘5.35q73.03
p˘6.23q46.98
p˘5.79q52.33
p˘4.12q62.85
p˘5.51q
Neural76.61
p˘22.81q85.12
p˘39.41q87.71
p˘9.32q17.78
p˘7.90q36.78
p˘2.82q38.58
p˘4.74q18.08
p˘3.03q23.48
p˘2.94q28.10
p˘2.16q
TF3.17
p˘0.62q6.39
p˘1.39q8.21
p˘1.84q28.03
p˘13.64q65.44
p˘6.39q69.86
p˘7.50q50.79
p˘10.58q65.81
p˘6.71q81.96
p˘7.43q
(b) Results of the under-parametrized models on representation-based n-gram LMs.
one of the number of heads or layers needs to increase.