FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
Tianyuan Zou1Yang Liu1 2Peng Li1 2Jianqing Zhang1 3Jingjing Liu1Ya-Qin Zhang1
Abstract
Data generation-based zero-shot learning, al-
though effective in training Small Task-specific
Models (STMs) via synthetic datasets generated
by Pre-trained Language Models (PLMs), is of-
ten limited by the low quality of such synthetic
datasets. Previous solutions have primarily fo-
cused on single PLM settings, where synthetic
datasets are typically restricted to specific sub-
spaces and often deviate from real-world distri-
butions, leading to severe distribution bias. To
mitigate such bias, we propose FuseGen, a novel
data generation-based zero-shot learning frame-
work that introduces a new criteria for subset se-
lection from synthetic datasets via utilizing multi-
ple PLMs and trained STMs. The chosen subset
provides in-context feedback to each PLM, en-
hancing dataset quality through iterative data gen-
eration. Trained STMs are then used for sample
re-weighting as well, further improving data qual-
ity. Extensive experiments across diverse tasks
demonstrate that FuseGen substantially outper-
forms existing methods, highly effective in boost-
ing STM performance in a PLM-agnostic way.
Code is provided in https://github.com/
LindaLydia/FuseGen .
1. Introduction
Despite the prevalence of powerful Pre-trained Language
Models (PLMs) (Achiam et al., 2023; Team et al., 2023;
Devlin et al., 2019) such as GPT-4, Small Task-specific
Models (STMs) are indispensable due to their compact size
and efficiency, especially for resource-constrained environ-
ments (Bommasani et al., 2021). To compensate for the
scarcity of high-quality training data, synthetic data gen-
erated by PLMs has been widely applied for STM train-
ing (Ye et al., 2022a; Wang et al., 2023). In particular,
1Institute for AI Industry Research (AIR), Tsinghua
University, Beijing, China2Shanghai Artificial Intelligence
Laboratory3Shanghai Jiao Tong University. Correspon-
dence to: Yang Liu <liuy03@air.tsinghua.edu.cn>, Peng Li
<lipeng@air.tsinghua.edu.cn>.
arXiv pre-print.data-generation based zero-shot learning (Ye et al., 2022a;
Meng et al., 2022; Gao et al., 2023; Ye et al., 2022b) trains
STM using the dataset synthesized by one PLM through
task-related label-descriptive prompts, requiring only the
task name ( e.g.movie review sentiment analysis) and label
categories ( e.g.positive/negative). This zero-shot trained
STM is significantly smaller than the original PLM with
comparable performance (Ye et al., 2022a), thus is particu-
larly advantageous for domains with limited computational
resources ( e.g. on mobile devices) or strict data privacy
constraints ( e.g.in finance applications).
However, the long-standing low-quality issue of synthetic
data impedes the practical application of STMs to a wider
range (Gao et al., 2023; Ye et al., 2022b). Previous works
on improving synthetic data quality mainly focus on en-
hancing data diversity (Fan et al., 2018; Holtzman et al.,
2020; Su & Collier, 2022; Yu et al., 2024), reducing redun-
dancy (Bolón-Canedo et al., 2013; Deng et al., 2023), and
implementing data-importance-guided in-context feedback
(Ye et al., 2022b) or sample re-weighting (Gao et al., 2023).
Despite notable advancements, they primarily rely on one
single PLM as source, inevitably overlooking the inherent
distribution biases of synthetic datasets.
To thoroughly investigate these biases and their impact on
STM performance, we conduct two pilot studies. As il-
lustrated in Figure 1, we use the dataset cartography ap-
proach (Swayamdipta et al., 2020) to plot the cartography
of synthetic datasets given by different PLMs. Dataset sam-
ples are categorized into easy-to-learn (marked in red), am-
biguous (marked in black) and hard-to-learn (marked in
blue) based on their confidence and variability, defined as
the mean and standard deviation of model probabilities for
their labels across training epochs. Since easy-to-learn sam-
ples aid convergence and ambiguous samples are vital for
boosting performance (Swayamdipta et al., 2020), an ideal
dataset should predominantly contain diverse easy-to-learn
andambiguous samples, with fewer hard-to-learn samples
which are often mislabeled (Swayamdipta et al., 2020). This
composition of diverse samples promises better STM per-
formance. In a second study, we provide the comparison
between STMs trained with different datasets that vary in
sources and generation methods, as illustrated in Figure 2.
These visualization analyses reveal three key observations:
1arXiv:2406.12527v1  [cs.CL]  18 Jun 2024FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
variability0.30.40.50.60.7 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.8
1.0
(a) Llama-2 ZeroGen K= 1(84.23)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.3
0.5
0.8
1.0 (b) Llama-2 ProGen K= 1(84.24)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.0 (c) Llama-2 Ours K= 6(86.60)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.0
(d) Flan-T5 ZeroGen K= 1(88.18)
0.025 0.050 0.075 0.100 0.125 0.150 0.175
variability0.350.400.450.500.550.600.650.70 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.0 (e) Flan-T5 ProGen K= 1(85.80)
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7 confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.0 (f) Flan-T5 Ours K= 6(88.73)
Figure 1. Synthetic dataset cartography (Swayamdipta et al., 2020) using 1,000samples generated by Llama-2 and Flan-T5 for movie
review semantic analysis. ZeroGen (Ye et al., 2022a) uses zero-shot prompt for generation, while ProGen (Ye et al., 2022b) and FuseGen
(Ours) use few-shot prompt with feedback, with ProGen relying on a single PLM and FuseGen leveraging multiple PLMs. Kis the
number of PLMs. Numbers within parentheses are the results of STM trained with Self-boosting Weight Adjustment (see Section 3.4) and
evaluated over IMDb (Maas et al., 2011) dataset. Results for more PLMs are provided in Figure 6 in Appendix C.1.
Figure 2. Performance of STM trained using 6,000synthetic data
samples generated by various PLMs. “mixed” uses a dataset com-
prising 6,000total samples given by the 6listed PLMs ( 1,000
samples per PLM). “FuseGen” (Ours) uses the 6listed PLMs and
6,000samples.
(1)Synthetic datasets from different PLMs exhibit signifi-
cant distribution biases. For example, Figures 1(a) and 1(d)
show that the zero-shot synthetic dataset produced by Llama-
2 (Touvron et al., 2023) primarily includes easy-to-learn
samples, whereas that of Flan-T5 (Chung et al., 2022) con-
tains a more balanced mixture of all 3categories. (2)Distri-
bution biases are difficult to overcome by only relying on a
single PLM. ProGen (Ye et al., 2022b), an advanced single-
PLM generation method, only slightly improves the ratio of
easy-to-learn andambiguous samples (Figure 1(b)), while
adversely increases the proportion of hard-to-learn samples
in some cases (Figure 1(e)). (3)Simply mixing samples
from multiple PLMs is ineffective. As demonstrated in Fig-ure 2, plainly combining data generated by multiple PLMs
improves STM performance compared to most single-PLM
cases, but is still worse than the best single PLM.
To tackle these challenges, we propose FuseGen, a smart
data generation-based zero-shot learning framework that
mitigates inherent dataset distribution bias by harnessing
the diversity of a PLM cluster. In FuseGen, given a specific
task and its label categories, synthetic datasets are initially
generated by various PLMs in a zero-shot manner, which
are then used to train their respective STMs. To alleviate
distribution bias, FuseGen selects superior samples gener-
ated by multiple PLMs as shared in-context feedback, and
prompts each PLM to accumulate higher-quality data it-
eratively. To select relevant in-context samples, FuseGen
pivots on an efficient cross-model criteria that considers
both dataset composition and individual sample importance.
To mitigate the negative impact of poor-quality samples,
FuseGen further uses a self-boosting method to dynamically
adjust sample weights to optimize STM in training. As
demonstrated in Figures 1(c), 1(f) and 2, with these novel
techniques, FuseGen effectively reduces distribution biases
and achieves better STM performance than state-of-the-art
methods.
Our contributions can be summarized as follows:
2FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
•We introduce a novel data-generation based zero-shot
learning framework, FuseGen, which collaboratively
leverages multiple PLMs to generate higher-quality
synthetic dataset without incurring any additional
queries to PLMs themselves. Further, FuseGen nei-
ther requires access to nor fine-tunes the parameters of
PLMs.
•We propose a novel cross-model criteria for selecting
in-context samples, which then serves as generation
feedback, and a self-boosting method for improving
STM performance.
•Extensive evaluations on 8NLI and NLU tasks with
6open-source and 2closed-source PLMs demonstrate
the consistent superiority of FuseGen over single-PLM
methods. This PLM-agnostic nature eliminates the
reliance on specific PLMs for downstream tasks.
2. Related Work
Data-generation based Zero-shot Learning. A recent line
of research focuses on exploiting the data generation capa-
bilities of PLMs (Ye et al., 2022a; Meng et al., 2022; Ye
et al., 2022b; Gao et al., 2023) to generate synthetic data
for training a target model (Meng et al., 2022; Ye et al.,
2022a;b; Gao et al., 2023). The dataset is generated by
prompting PLM with task and label descriptions. A critical
challenge for this approach is that generated datasets often
contain low-quality samples. Recent attempts to address this
include techniques to enhance dataset diversity ( e.g.Top-k
sampling (Fan et al., 2018), nucleus sampling (Holtzman
et al., 2020), diversely attributed prompts (Yu et al., 2024),
and contrastive search decoding (Su & Collier, 2022)). Ad-
ditionally, feature selection (Bolón-Canedo et al., 2013)
helps eliminate redundant information within the dataset.
Finally, methods like progressive generation with in-context
feedback (Ye et al., 2022b) and sample re-weighting (Ye
et al., 2022b) focus on identifying and amplifying the influ-
ence of high-quality samples. Despite significant progress,
existing studies often overlook the inherent data distribution
bias in synthetic datasets generated by a single PLM. In
contrast, our work explores avoiding this bias by leveraging
diverse multiple PLMs.
Fusion of PLMs. Recent studies suggest that it is possible
to combine the capabilities of multiple PLMs to obtain a
model with stronger performance (Wan et al., 2024a;b; Li
et al., 2024). Existing PLM knowledge-fusion techniques
can be grouped into training-time fusion andtest-time fu-
sion (Mavromatis et al., 2024). Training-time fusion meth-
ods (Wan et al., 2024a;b) fuse PLMs’ token-level predictions
produced during training time to fine-tune a target PLM, re-
quiring abundant computational resources. Test-time fusion
methods do not fine-tune PLMs, but utilize methods suchas logits averaging (Mavromatis et al., 2024) and majority
voting (Li et al., 2024) to fuse the knowledge of PLMs at
test time. In addition, interactions and collaborations among
PLM agents (Liu et al., 2023; Du et al., 2023) have been
investigated.
All these works demonstrate that collaboration among di-
verse PLMs helps. However, all existing works require
direct access to training samples, which means they are not
applicable to the setting of data generation-based zero-shot
learning, the problem we aim to solve.
3. FuseGen
3.1. Preliminaries
Indata-generation based zero-shot learning (Ye et al.,
2022a; Gao et al., 2023) with a single PLM , given a down-
stream task like text classification, a PLM Pwith parameter
ΦPfirst generates a synthetic dataset D={(xi, yi)}N
i=1of
sizeN. This is accomplished by using a proper task-related
label-descriptive prompt T(·)(examples are provided in
Appendix A.1) as follows:
xi∼ P(·|T(yi),ΦP). (1)
Dis then used to train an STM mwith the following training
objective:
L=NX
i=1ℓ(m(xi), yi), (2)
where ℓis a common loss function, e.g.cross-entropy loss.
3.2. FuseGen Architecture Overview
Different from previous works, we focus on multi-PLM
setting and propose FuseGen. The FuseGen workflow is
illustrated in Figure 3. In a nutshell, FuseGen consists of
two main components: Cross-model Dataset Generation
(CDG) (Section 3.3) and Cross-model Data Quality Im-
provement (CDI) (Section 3.4). For CDG, given a fixed
number of samples to generate in total, PLMs progressively
generate datasets for multiple rounds, each round using an
improved subset of samples generated from previous rounds
as in-context examples. This is realized in three steps: (1)
Parallel Synthetic Data Generation : each PLM generates its
own dataset and trains a respective STM. (2)Cross-model
Data Quality Evaluation : the quality of generated samples
is evaluated using a cross-PLM criteria to select a desirable
subset. (3)Cross-PLM In-context Learning : the cross-PLM
subsets are used as in-context examples to prompt PLMs
to generate new datasets. Step (1)is then repeated. After
the required number of samples is reached, we perform CDI
which re-weights samples with a self-boosting strategy. Al-
gorithm 1 provides an overview of the above steps, with
each function detailed in Appendix B.
3FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
Figure 3. Illustrated Workflow of FuseGen with two components: Cross-model Data Generation (CDG) and Cross-model Data Quality
Improvement (CDI). CDG iteratively executes parallel synthetic data generation, cross-model data quality evaluation and cross-PLM
in-context learning. CDI implements self-boosting weight adjustment for sample-reweighted training of STM.
3.3. Cross-model Dataset Generation
In FuseGen, each PLM iteratively generates a total of N
samples across J+ 1rounds, incorporating feedback from
STMs after each of the first Jrounds. In each round, a total
ofN
J+1samples are generated using the accumulated knowl-
edge of multiple PLMs from previous rounds as feedback.
Specifically, the following steps are taken:
Parallel Synthetic Dataset Generation. In each round,
each of KPLMs (denoted as {Pk}K
k=1) generates a syn-
thetic dataset Dk={(xk,i, yk,i)}N
J+1
i=1of sizeN
J+1in par-
allel with the same task-related label-descriptive prompt
T(·)as described in Section 3. Each dataset is then used to
train a separate STM mkfollowing Equation (2). This step
produces Kseparate STMs and Ksynthetic datasets.
Cross-model Data Quality Evaluation. In this step, we
aim to select a desirable subset from D=SK
k=1Dkto
guide data generation. To accomplish this goal, we utilize
the knowledge of trained STMs at hand and develop a simple
yet efficient criteria for data-quality evaluation.
As discussed in Section 1 , easy-to-learn samples of low-
variability and ambiguous samples of high-variability are
both vital for constructing a desirable dataset, valuable for
training convergence and model generalization ability, re-
spectively. Inspired by this, we first use cross-model vari-
ability dk,ito categorize each sample, defined as:
dk,i= STD( p1,k,i[yk,i], ..., p k′,k,i[yk,i], ..., p K,k,i[yk,i])(3)
where pk′,k,i[yk,i]denotes STM model mk′’s predicted
probability of synthetic label yk,ion that sample xk,i, andSTD represents standard deviation1. To prompt the genera-
tion of a dataset that includes both low-variability (low dk,i)
and high-variability (high dk,i) data, we select a small num-
ber of candidates (of size R≪N) comprised of αRtop
high-variability and (1−α)Rtop low-variability samples,
where αis a hyper-parameter that controls the percentage
of high-variability samples. The goal here is to efficiently
select a smaller and more manageable subset from a large
set of candidates. The selected subset can then be processed
by more computationally intensive ranking. To further iden-
tify samples that are vital for training, we train an STM ˜m
usingDand leverage the noise-resistant influence function
proposed in ProGen (Ye et al., 2022b) to select the top- S
influential samples from the Rcandidate samples ( S < R ).
Our results validate that these selected samples originate
from various PLMs (See Appendix C.4.)
Cross-PLM In-context Learning. After selecting S
in-context samples (denoted as ˆD), we add them to the
original prompt T(·), resulting in T(ˆx1, . . . , ˆxS;·)(see ex-
amples in Appendix A.1). We then send the feedback
prompt to each PLM to generateN
J+1new samples fol-
lowing xk,i∼ Pk(·|T(ˆx1, . . . , ˆxS;yk,i),ΦPk), where ΦPk
denotes the parameter of Pk. In this way, PLMs can learn
from each other and generate datasets with improved quality.
1Different from Swayamdipta et al. (2020), we do not include
confidence ( i.e.mean of predicted probability in our criteria, as
the synthetic label is not used for in-context samples (see Ap-
pendix A.1 for in-context sample examples).
4FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
Algorithm 1 FuseGen
Input:
KPLMs, empty synthetic dataset {Dk← ∅}K
k=1, target num-
ber of synthetic samples Nfor each PLM, sample selection
hyper-parameter α, R, S , number of feedback steps Jtaken to
obtain in total Nsynthetic samples, random initialized STM m(0),
test dataset of downstream task A, initialized sample weightsn
{w(0)
k,i}N
i=1oK
k=1, learning rate η, number of weight adjustment
epochs E1, number of STM training epochs E2.
Output: STM ˜mthat obtains the effectively aggregated knowl-
edge from KPLMs.
1: Initialize in-context feedback samples ˆD ← ∅ .
2:forj= 0toJdo
3: fork= 1toKin parallel do
4: Dk←S_AccumulativeSynDataGeneration( Dk,
ˆD,N,J,j).
5: mk←S_STMTraining( Dk,m(0),E2).
6: end for
7: ˜m←S_STMTraining( ∪K
k=1Dk,m(0),E2).
8: ˆD ← C_SampleSelection( ∪K
k=1Dk,{mk}K
k=1,˜m,
α,R,S).
9:end for
10:˜m←S_WeightAdjustSTMTraining( ∪K
k=1Dk,m(0),
∪K
k=1n
{w(0)
k,i}N
i=1o
,E1,E2).
3.4. Cross-model Data Quality Improvement
After CDG process that improves overall data distribution,
we perform one last step of re-weighting samples by their
quality, determined by a Self-boosting Weight Adjustment
(SWA) approach.
Ashard-to-learn samples (refer to Figures 1(c) and 1(f))
and low-quality samples ( e.g. meaningless or irrelevant)
still exist post-CDG, we down-weight these samples in each
training round of the final STM ˜m. Specifically, a weight
wk,i(uniformly initialized as 0.5) is assigned to each sam-
ple in D={{(xk,i, yk,i)}N
i=1}K
k=1. At the e1-th weight-
adjustment round of ˜m, we update wk,iusing the following
boosting strategy inspired by TrAdaBoost (Dai et al., 2007):
w(e1+1)
k,i=w(e1)
k,iβ−errork,i(1−correct k,i),
k= 1, . . . , K, i = 1, . . . , N ,(4)
where β=1
1+q
2 ln (NK)
E1>0is a constant value for weight
adjustment, E1is the number of total epochs for weight ad-
justment, error k,i= 1−pk,i[yk,i]is the prediction error of
˜mon data sample xk,i, and correct k,i= 1if˜mpredicts sam-
plexk,icorrectly, otherwise correct k,i= 0. Normalization
is applied afterwards to guarantee thatPK
k=1PN
i=1w(e1)
k,i=
0.5NK. After normalization, wk,ifor correctly inferred
samples increases while that for wrongly inferred samples
decreases. A new STM is trained from scratch with the
new weights after each adjustment step. Training details are
provided in Algorithms 1 and 2. With SWA, the trainingobjective for ˜musing all synthetic data Dis given by:
L=KX
k=1NX
i=1wk,i·ℓ( ˜m(xk,i), yk,i). (5)
Unlike SunGen (Gao et al., 2023), which utilizes a self-
guided sample re-weighting method with bi-level SGD
optimization to enhance its STM performance, our SWA
achieves comparable STM performance without requiring
this computationally expensive optimization step (see Sec-
tion 4 and Appendix C.5). This translates to a significantly
smaller computational cost.
4. Experiments
4.1. Experimental Settings
Models. In our experiments, we evaluate on 6open-source
PLMs: GPT-2-xl (GPT-2) (Radford et al., 2019), Llama-2-
7b-chat-hf (Llama-2) (Touvron et al., 2023), Vicuna-7b-1.5v
(Vicuna) (Chiang et al., 2023), OPT-6.7b (OPT) (Zhang
et al., 2022), ChatGLM3-6b-base (ChatGLM3) (Du et al.,
2022) and Flan-T5-xl (Flan-T5) (Chung et al., 2022). 2
closed-source PLMs are also used for generating synthetic
datasets: GPT-3.5-turbo-instruct (GPT-3.5) (OpenAI, 2021)
and GPT-4-turbo-preview (GPT-4) (OpenAI, 2023). For the
choice of STM, we use bert-base-uncased (BERT), a pre-
trained model, to perform downstream classification tasks.
The trained STM is evaluated over a real-world human-
annotated dataset (test dataset) Athat is never used during
training.
Datasets. We select 7 well-developed datasets to evaluate
our framework: 1)IMDb (Maas et al., 2011) and SST-
2 (Socher et al., 2013; Wang et al., 2019) for movie re-
view semantic analysis task, 2)Yelp-polarity (Zhang et al.,
2015a) for restaurant review semantic analysis task, 3)Ag-
News (Zhang et al., 2015b) for news category classification
task,4)QNLI (Wang et al., 2019) for question-information
entailment classification task, 5)MNLI (both matched and
mismatched) (Williams et al., 2018) for sentence-pair rela-
tion classification task. To test the effectiveness of FuseGen
on unseen tasks, we further create a new dataset named
MarkedNews from AgNews. MarkedNews categorizes arti-
cles containing the symbol “$” as “Money with $ included”,
and all other articles retain their original AgNews categories.
This creates a new 5-class classification task: “World”,
“Sports”, “Business”, “Technology”, and “Money with $
included”. We adopt the original test dataset as Aexcept
for QNLI and MNLI, where ground-truth labels are unavail-
able. In these cases, we use the validation sets instead. The
experiments run on A100-80G.
Baselines. We compare our framework with several existing
data-generation based zero-shot learning methods, including
5FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
IMDb SST-2
˜mG ˜mL ˜mV ˜mO ˜mC ˜mF ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
ZeroGen♠85.07 82.14 81.36 80.54 81.49 87.06 80.99 79.47 82.33 82.00 86.49 81.88
SunGen♠86.94 86.59 84.93 85.21 84.76 89.79 83.45 84.30 84.04 83.49 87.18 83.53
ProGen♠85.68 84.33 82.14 85.57 87.41 88.00 83.60 79.53 82.53 82.78 86.64 83.17
FuseGen (Ours) 90.06 87.51
Yelp QNLI
˜mG ˜mL ˜mV ˜mO ˜mC ˜mF ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
ZeroGen♠89.73 89.74 85.67 87.13 82.00 92.41 58.30 70.79 70.88 56.64 60.77 57.95
SunGen♠91.85 89.30 89.06 91.22 88.86 93.13 62.26 74.20 74.35 57.50 65.64 58.21
ProGen♠91.26 89.82 88.55 89.00 88.81 91.71 58.38 69.56 70.29 57.46 61.08 69.44
FuseGen (Ours) 93.47 74.92
MNLI-matched MNLI-mismatched
˜mG ˜mL ˜mV ˜mO ˜mC ˜mF ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
ZeroGen♠41.99 48.52 45.87 36.16 32.65 47.37 46.38 50.04 48.10 36.74 33.00 49.95
SunGen♠44.66 49.43 46.27 37.44 32.71 49.04 47.45 51.67 48.63 38.35 33.02 51.66
ProGen♠43.35 48.69 47.50 36.79 32.81 48.56 46.57 50.57 49.65 40.27 33.01 50.24
FuseGen (Ours) 49.76 51.70
AgNews MarkedNews
˜mG ˜mL ˜mV ˜mO ˜mC ˜mF ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
ZeroGen♠77.86 83.40 81.25 84.81 83.17 81.87 77.16 74.49 74.10 77.80 80.33 76.12
SunGen♠80.94 84.44 82.50 85.68 84.12 85.57 78.01 76.75 76.39 78.15 82.16 77.85
ProGen♠78.68 83.93 81.46 85.66 84.74 84.59 77.17 76.51 76.14 77.93 82.70 78.75
FuseGen (Ours) 86.89 83.85
Table 1. Comparison of FuseGen and baselines with K= 6. Methods marked by♠are single-PLM methods. ˜mG,˜mL,˜mV,˜mO,˜mC,
˜mFrepresents the final STM performance with single PLM GPT-2, Llama-2, Vicuna, OPT, ChatGLM3 and Flan-T5, respectively. Best
result is marked as bold , and the second best is marked with underline .
1)ZeroGen (Ye et al., 2022a) which directly trains an STM
using the generated synthetic data, 2)SunGen (Gao et al.,
2023) which recovers a robust synthetic dataset through
sample-level weight optimization, and 3)ProGen (Ye et al.,
2022b) which progressively generates data using self-given
in-context feedback through prompt. To ensure a fair com-
parison, all methods generate the same number of samples.
In other words, each single-PLM method produces a total
ofN×Ksamples.
Implementation Details. Unless otherwise stated, the fol-
lowing setting is applied: N= 1,000synthetic data sam-
ples generated by each PLM are used for FuseGen; the
BERT models (STMs) are trained with Adam optimizer
with a learning rate of 2×10−5and training epochs ( E2)
of3. When training STMs, weight adjustment is performed
for30iterations ( E1= 30 ). Each experiment is repeated
3times using different random seeds, and averaged accu-
racy is reported. α= 0.5, R= 40, S= 8is used to select
in-context samples for constructing feedback prompt, ex-
cept for QNLI and MNLI datasets, where R= 20, S= 4is used in order to fit the maximum input length of each
PLM. J= 4is used for iterative generation (both FuseGen
and ProGen). For SunGen, 50samples are used for sample-
weight backward gradient estimation.
4.2. Main Results
Table 1 summarizes the main results of our FuseGen frame-
work and compared baseline methods. To ensure compre-
hensive evaluation, each single-PLM baseline method is
evaluated using samples generated from each of the PLMs.
Open-source PLMs. Table 1 shows that FuseGen con-
sistently outperforms all baselines using the same number
of generated samples. ( i.e. each PLM generates 6,000
samples for training ˜mkfor baselines), demonstrating the
superior data quality of FuseGen. Our method achieves
up to 1.2%increase in STM performance over the best-
performing single-PLM baseline, which exploits the opti-
mal PLM for each task. SunGen performs consistently well
among single-PLM baselines, but the ideal PLM varies by
task. However, in zero-shot setting, where no task-specific
6FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
˜mGPT−3.5˜mGPT−4
ZeroGen♠51.66 49.48
SunGen♠52.92 55.82
ProGen♠52.50 55.76
FuseGen (Ours) 56.56
Table 2. Comparison of FuseGen and baseline methods on closed-
source PLMs with QNLI dataset and K= 2.
Figure 4. Comparison of FuseGen between using multi-PLM (last
bar) and single-PLM with QNLI dataset.
samples are available, pre-selecting a PLM for optimal train-
ing performance is impractical. FuseGen is free from such
pre-selection.
Unseen Tasks. Evaluation results for FuseGen and base-
lines over our new dataset MarkedNews are shown in Ta-
ble 1, with synthetic data generation prompts detailed in
Appendix A.1. FuseGen outperforms all baselines con-
sistently, demonstrating its ability to enhance downstream
STM performance even when PLMs lack prior knowledge
of the unseen classification task.
Closed-source PLMs. We also conduct experiments on the
fusion of two popular closed-source models (GPT-3.5 and
GPT-4) using QNLI dataset with K= 2. Results in Table 2
(each ˜mkis trained with 2,000samples) demonstrate the
superior performance of FuseGen compared to baselines.
FuseGen’s consistent superiority across diverse tasks and
models underscores its PLM-agnostic nature. This elimi-
nates the need of relying on specific models for downstream
tasks, making it a more flexible and efficient solution.
4.3. Ablation Study
4.3.1. M ULTI -PLM V.S. SINGLE -PLM
We evaluate the impact of multi-PLM fusion by comparing
FuseGen between using multi-PLM ( K= 6) and single-
PLM ( K= 1). Results are provided in Figure 4. Since
cross-model variability evaluation in CDG can not be per-
formed for K= 1, random selection is applied here to
select Rcandidate samples, whereas CDI is applied to both
cases. Figure 4 shows that multi-PLM collaboration is vital
for further improving the quality of synthetic dataset, yield-
ing better STM performance than relying on single-PLM .
Detailed results on more datasets are provided in Table 9 inVariability Influ-encemG mL mV mO mC mF ˜mLow High
Rand. % 52.47 67.48 65.90 50.52 56.68 67.66 72.89
! % % 53.77 66.18 61.33 50.96 53.37 66.13 73.76
% ! % 54.98 65.48 60.76 49.79 54.28 65.47 73.81
! ! % 58.59 70.85 66.31 50.38 55.23 67.83 74.14
Rand. ! 54.25 70.44 70.74 51.19 56.68 68.84 74.07
! % ! 54.00 70.07 67.75 51.12 55.70 66.49 74.08
% ! ! 54.85 66.47 64.46 50.08 56.50 70.50 74.16
! ! !59.68 71.48 72.37 52.37 57.33 72.12 74.92FuseGen (Ours)
Table 3. Comparison of different in-context sample selection meth-
ods with QNLI as test dataset. “Variability” is cross-model variabil-
ity, and “Rand.” stands for random sampling for in-context sample
candidate selection. mG,mL,mV,mO,mC,mFeach rep-
resents mGPT−2,mLlama −2,mV icuna ,mOPT ,mChatGLM 3,
mFlan−T5and˜mis the final STM trained using D. Best result
is marked as bold and the second best marked with underline for
each STM (each column).
mG mL mV mO mC mF ˜m
FuseGen (Ours) 59.68 71.48 72.37 52.37 57.33 72.12 74.92
w/o SWA 56.72 69.99 70.94 51.98 56.39 68.65 73.41
w/o CDG & SWA 51.24 65.81 70.61 50.83 53.01 55.73 69.41
SDG+mixed 52.13 69.22 70.11 51.79 54.87 68.58 70.20
Table 4. Comparison between FuseGen and its ablations using
N= 1,000with QNLI as test dataset. mG,mL,mV,mO,mC,
mFeach represents mGPT−2,mLlama −2,mV icuna ,mOPT ,
mChatGLM 3,mFlan−T5, while ˜mis the final STM trained using
the dataset D.
Appendix C.6.
4.3.2. I N-CONTEXT SAMPLE SELECTION
In-context sample selection is a critical component of the
FuseGen framework, as it influences the quality of feedback
from STMs to PLMs, which in turn affects the generation
quality of PLMs. In this section, we compare various in-
context sample selection strategies, including random se-
lection, high-variability and low-variability selection. The
latter two exclusively select top- Rhigh-variability or low-
variability samples, respectively. We also evaluate each
strategy with and without fine-grained influence-based se-
lection. The results are shown in Table 3. We also report the
performance of each mktrained with SWA using the corre-
sponding Dkduring the FuseGen process in Table 3. Our
in-context sample selection strategy surpasses other alterna-
tives consistently, not just in the final STM performance, but
also for each intermediate small model mkproduced during
FuseGen. This underscores the efficacy of our selection
approach andFuseGen’s ability to produce higher-quality
datasets for all PLMs involved .
7FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
time [s] ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
1,000SunGen 43.3 57.46 72.01 72.14 50.71 55.45 57.31
SWA 0.1 56.95 71.13 72.21 51.96 55.12 57.43
6,000SunGen 240.8 62.26 74.20 74.35 57.50 65.64 58.21
SWA 0.5 62.59 74.58 74.35 58.42 64.81 58.47
Table 5. Comparison on running time for each weight adjustment
epoch and STM performance between SunGen and SWA with
QNLI as test dataset. Best result is marked as bold .
(a) Effect of α
 (b) Effect of N
 (c) Effect of J
Figure 5. Ablation results on different hyper-parameters used for
FuseGen with QNLI as test dataset.
4.3.3. E FFECTIVENESS OF SWA AND CDG
As FuseGen consists of 2components, CDG and CDI
(mainly achieved by SWA), we perform ablation study by
removing SWA and CDG step by step from FuseGen, re-
sulting in 2ablations: “w/o SWA” and “w/o CDG & SWA”.
Note when both CDI and CDG are removed, datasets are
generated from multiple PLMs using zero-shot prompt and
naively combined (the "mixed" case in Figure 2). We fur-
ther add ablation “SDG+mixed” (also without SWA) which
naively combines datasets given by multiple PLMs using
self-guided data generation (SDG) for in-context feedback
(same as K= 1in Section 4.3.1). Results are summarized
in Table 4 and Table 8 in Appendix C.5. From Table 4, we
observe a 1.51% drop in ˜mperformance when removing
SWA, and another 5.51% drop when further removing CDG,
demonstrating that SWA is effective in boosting knowledge
transfer from synthetic dataset to STM andCDG is effec-
tive in fusing the knowledge of multiple PLMs . Also, CDG
(“w/o SWA”) outperforms “SDG+mixed” by a huge margin
(3.21%), verifying the superiority of collaborative feedback
over self-guided feedback.
As SunGen (Gao et al., 2023) also re-weights samples to
boost STM performance, we further compare the perfor-
mance of SWA with SunGen (using 50 samples for estimat-
ing gradients of sample weights), with results shown in Ta-
ble 5. We observe that, SunGen’s computational cost is two
orders-of-magnitude higher than SWA when re-weighting
1,000to6,000samples, yet delivers comparable perfor-
mance. This underscores the effectiveness and efficiency of
SWA, making our framework much more computationally
effective.4.3.4. E FFECT OF HYPER -PARAMETERS
We further study the impact of hyper-parameters α(ratio
of high-variability samples within the Rin-context sample
candidates), N(sample generation budget), and J(feed-
back times) of FuseGen with K= 6in Figure 5. Detailed
results with each mkare included in Tables 10 to 12 in
Appendix C.7.
Effect of α.Figure 5(a) shows that, too many or too few
high-variability samples in the candidate set both hurt the
synthetic dataset quality, resulting in lower STM perfor-
mance, whereas a balanced mix ( α= 0.5) yields the highest
STM results.
Effect of N.Figure 5(b) demonstrates that STM perfor-
mance improves with the increase of N. Additionally, the
performance improvement rate decelerates at larger values
ofN.
Effect of J.From Figure 5(c), we observe that increasing
Jresults in a slight but consistent improvement in perfor-
mance, likely due to the fact that more precise guidance
is given to PLMs by a more frequent feedback during the
process.
5. Conclusion
We propose a novel data-generation based zero-shot learning
framework FuseGen that harnesses the collaborative capabil-
ity of multiple PLMs to improve synthetic data generation
of PLMs. We first integrate multiple PLMs to alleviate
distribution bias of synthetic datasets through cross-PLM
in-context samples selection, for constructing better feed-
back recursively. To further improve the quality of the
generated synthetic dataset and boost STM performance,
we employ a self-boosting weight adjustment strategy to
down-weight low-quality samples. Extensive experiments
and ablation studies on various NLI and NLU tasks demon-
strate that FuseGen is highly effective, query-efficient and
PLM-agnostic without the reliance on specific PLMs for
downstream tasks, making it a more flexible and resource-
efficient solution.
6. Limitations
This work sheds lights on the possibility of multi-PLM col-
laboration in the field of zero-shot learning. However, it
does not delve deeply into the interrelationships between
pairs of PLMs. A more thorough investigation could yield
insightful conclusions regarding which PLMs are most com-
plementary to one another. Meanwhile, aside from seeding
the same feedback to all PLMs, more personalized feedback
can be constructed to better suit the inherit distribution bias
of each PLM, which may further boost STM performances.
8FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. GPT-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
Bolón-Canedo, V ., Sánchez-Maroño, N., and Alonso-
Betanzos, A. A review of feature selection methods on
synthetic data. Knowledge and information systems , 34:
483–519, 2013.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-
lut, A., Brunskill, E., et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258 ,
2021.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,
H., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,
Stoica, I., and Xing, E. P. Vicuna: An open-source
chatbot impressing GPT-4 with 90%* ChatGPT qual-
ity, March 2023. URL https://lmsys.org/blog/
2023-03-30-vicuna/ .
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022.
Dai, W., Yang, Q., Xue, G.-R., and Yu, Y . Boosting for
transfer learning. In Proceedings of the 24th international
conference on Machine learning , pp. 193–200, 2007.
Deng, Y ., Qiao, Z., Ren, J., Liu, Y ., and Zhang, Y . Mu-
tual enhancement of large and small language mod-
els with cross-silo knowledge transfer. arXiv preprint
arXiv:2312.05842 , 2023.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Burstein, J., Doran, C., and
Solorio, T. (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pp. 4171–4186, Min-
neapolis, Minnesota, June 2019. Association for Compu-
tational Linguistics. doi: 10.18653/v1/N19-1423. URL
https://aclanthology.org/N19-1423 .
Du, Y ., Li, S., Torralba, A., Tenenbaum, J. B., and Mor-
datch, I. Improving factuality and reasoning in lan-
guage models through multiagent debate. arXiv preprint
arXiv:2305.14325 , 2023.
Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., and
Tang, J. GLM: General language model pretraining with
autoregressive blank infilling. In Proceedings of the 60thAnnual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 320–335, 2022.
Fan, A., Lewis, M., and Dauphin, Y . Hierarchical neu-
ral story generation. In Gurevych, I. and Miyao,
Y . (eds.), Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pp. 889–898, Melbourne, Aus-
tralia, July 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/P18-1082. URL https:
//aclanthology.org/P18-1082 .
Gao, J., Pi, R., Yong, L., Xu, H., Ye, J., Wu, Z., Zhang, W.,
Liang, X., Li, Z., and Kong, L. Self-guided noise-free
data generation for efficient zero-shot learning. In The
Eleventh International Conference on Learning Repre-
sentations , 2023.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi,
Y . The curious case of neural text degeneration. In
International Conference on Learning Representations ,
2020. URL https://openreview.net/forum?
id=rygGQyrFvH .
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000) , pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Li, J., Zhang, Q., Yu, Y ., Fu, Q., and Ye, D. More agents is
all you need. arXiv preprint arXiv:2402.05120 , 2024.
Liu, Z., Zhang, Y ., Li, P., Liu, Y ., and Yang, D. Dy-
namic llm-agent network: An llm-agent collaboration
framework with agent team optimization. arXiv preprint
arXiv:2310.02170 , 2023.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,
A. Y ., and Potts, C. Learning word vectors for sen-
timent analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies , pp. 142–150, Port-
land, Oregon, USA, June 2011. Association for Com-
putational Linguistics. URL http://www.aclweb.
org/anthology/P11-1015 .
Mavromatis, C., Karypis, P., and Karypis, G. Pack of LLMs:
Model fusion at test-time via perplexity optimization.
arXiv preprint arXiv:2404.11531 , 2024.
Meng, Y ., Huang, J., Zhang, Y ., and Han, J. Generating
training data with language models: Towards zero-shot
language understanding. Advances in Neural Information
Processing Systems , 35:462–477, 2022.
OpenAI. GPT-3.5-Turbo, 2021. URL https:
//platform.openai.com/docs/models/
gpt-3-5-turbo .
9FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
OpenAI. GPT-4-Turbo and GPT-4, 2023. URL
https://platform.openai.com/docs/
models/gpt-4-turbo-and-gpt-4 .
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI blog , 1(8):9, 2019.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C. Recursive deep models
for semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing , pp. 1631–
1642, Seattle, Washington, USA, October 2013. Asso-
ciation for Computational Linguistics. URL https:
//www.aclweb.org/anthology/D13-1170 .
Su, Y . and Collier, N. Contrastive search is what
you need for neural text generation. arXiv preprint
arXiv:2210.14140 , 2022.
Swayamdipta, S., Schwartz, R., Lourie, N., Wang, Y ., Ha-
jishirzi, H., Smith, N. A., and Choi, Y . Dataset car-
tography: Mapping and diagnosing datasets with train-
ing dynamics. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing
(EMNLP) , pp. 9275–9293, 2020.
Team, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu,
J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al.
Gemini: a family of highly capable multimodal models.
arXiv preprint arXiv:2312.11805 , 2023.
Touvron, H., Martin, L., Stone, K., Albert, P., Alma-
hairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhar-
gava, P., Bhosale, S., et al. Llama 2: Open foundation
and fine-tuned chat models, 2023. URL https://arxiv.
org/abs/2307.09288 , 2023.
Wan, F., Huang, X., Cai, D., Quan, X., Bi, W., and Shi,
S. Knowledge fusion of large language models. In The
Twelfth International Conference on Learning Represen-
tations , 2024a. URL https://openreview.net/
forum?id=jiDsk12qcz .
Wan, F., Yang, Z., Zhong, L., Quan, X., Huang, X., and Bi,
W. FuseChat: Knowledge fusion of chat models. arXiv
preprint arXiv:2402.16107 , 2024b.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.
Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y .
Openchat: Advancing open-source language models with
mixed-quality data. arXiv preprint arXiv:2309.11235 ,
2023.Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
through inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers) , pp. 1112–1122. As-
sociation for Computational Linguistics, 2018. URL
http://aclweb.org/anthology/N18-1101 .
Ye, J., Gao, J., Li, Q., Xu, H., Feng, J., Wu, Z., Yu, T.,
and Kong, L. ZeroGen: Efficient zero-shot learning via
dataset generation. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pp. 11653–11669, 2022a.
Ye, J., Gao, J., Wu, Z., Feng, J., Yu, T., and Kong, L. ProGen:
Progressive zero-shot dataset generation via in-context
feedback. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2022 , pp. 3671–3683, 2022b.
Yu, Y ., Zhuang, Y ., Zhang, J., Meng, Y ., Ratner, A. J., Kr-
ishna, R., Shen, J., and Zhang, C. Large language model
as attributed training data generator: A tale of diversity
and bias. Advances in Neural Information Processing
Systems , 36, 2024.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., Mi-
haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,
Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
L. OPT: Open pre-trained transformer language models,
2022.
Zhang, X., Zhao, J., and LeCun, Y . Character-
level Convolutional Networks for Text Classification.
arXiv:1509.01626 [cs] , September 2015a.
Zhang, X., Zhao, J. J., and LeCun, Y . Character-level convo-
lutional networks for text classification. In NIPS , 2015b.
10FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
A. Prompts Used in Experiments
A.1. Task-related Label-descriptive Prompts
We present the prompts used for synthetic dataset generation
in Table 6. For information-question entailment analysis
task (QNLI) and sentence pair relation analysis task
(MNLI), we leverage the open-source Wikipedia-short
(https://github.com/yumeng5/SuperGen/
tree/main/pretrain_corpus ) dataset, which con-
tains short Wikipedia sequences ( 5to30words) extracted
from sentences in Wikipedia. We use these sentences as the
information source for the prompts. In other words, each
occurrence of <information> or<sentence1> within the
prompt is replaced with a randomly-chosen Wikipedia-short
sequence before feeding it to PLMs.
Below we also provide 2examples of the few-shot prompts
used in FuseGen . We need to clarify that, label information
is not included in the in-context samples.
Few-shot prompt for movie review semantic anal-
ysis
The movie review is: This is an excellent romantic
comedy that relies more on wit and character than on silly,
typical formula. A lot of people I know walked away
from this movie disappointed, but I found it an enjoyable
experience. I also don’t understand why Hollywood
thinks that ’quirkiness’ is more important than story, or
why they can’t seem to create movies in which the plot is
interesting and makes sense.
The movie review is: There’s a lot of talent wasted here.
Haggis overuses his themes and is unable to let his
characters go in this soapy melodrama.
The movie review is: The movie is not fast paced and
some of the drama was a bit too much for me, but I did
like it.
The movie review is: There is a certain helplessness in
allowing ourselves to be tricked by the tricky cuts that
grace the first half of the film. It allows us to suspend our
disbelief and see what we want to see. It’s not a movie
I’d love to watch again, but it is one I’m glad I got to see.
The movie review is: I will be the first to admit that the
animation is crude in some parts. What I liked about
the movie is that it had a very fun story line and I loved
the songs. The movie review is: There’s no reason
you shouldn’t enjoy this semi-tangential off-shoot of a
popular video game; it’s a fun, goofy movie that doesn’t
rely on the whole ’cinematic universe’ concept
The movie review is: engaging and entertaining, with
excellent performances from David Niven and Barbara
Stanwyck. 2.Sheila is stunning in the movie, a lady
obsessed with the detective, especially when working in
an area with limited light. 3.The climax is shocking - but
it’s entirely appropriate, as the plot’s terrible.
The movie review is: Many don’t like the hero, and still
others were glad they saw it and it was good. With that
said, there are some surprising plot holes, inconsistencies
and potential points of plot-holes that also need to be
addressed before anyone can put their money into the film.If anyone was wondering how people like things and don’t
like other people like things, this movie is a great example.
The new movie review in negative sentiment which is
diverse in the expression compared to the above given
samples is:
Few-shot prompt for information-question entail-
ment analysis
The Information-Question pair is: Soon after, the account
began to go viral, attracting the attention of reddit streams,
content aggregators, art critics, and Renoir \u2019s own
descendants.[SEP]and Renoir’s own accounts suggests
that they met in early November 1881 when the baron
stopped at their boardinghouse. ”Below a quadriga in the
Louvre courtyard, Henri left his easel with his model and
ran up the stairway to Duret with the idea of showing him
what he had accomplished.“ (from Renoir’s biography by
Fr?
The Information-Question pair is: She made her
American debut in 1910, with the New York Symphony
Orchestra, under conductor Walter Damrosch.[SEP]If
this photo were to depict a specific moment in history, or
an individual’s life, which historical period or individual
would it most closely resemble?
The Information-Question pair is: The Fall Line is an
American true crime podcast that covers lesser-known
cases of murder and disappearance from minority
communities in Georgia.[SEP]The founder is the founder.
If the owner owns the club, is it the ’Alamo’ of crime
blogs (or is it an ’evil bar’)?
The Information-Question pair is: She was a Member
of the Supreme Council of the Uzbek SSR.[SEP]Who
was the head of the Uzbek SSR during her time on the
Supreme Council?
The new Information-Question pair which is di-
verse in the expression compared to the above given
samples is: Information: “<information>”
Question (answer not in above information):
B. Detailed Algorithms
We provide the detailed algorithms for each function used
in Algorithm 1 here in Algorithm 2.
C. Additional Experimental Results
C.1. Dataset Cartography of More Synthetic Datasets
Dataset cartography (Swayamdipta et al., 2020) approach
characterizes each sample by its confidence and variability,
which are defined as the mean and standard deviation of
the model probability of its related label across all train-
ing epochs. For example, if the model correctly predict a
sample’s label across training epochs, it will have high con-
fidence and low variability. These samples are regarded as
easy-to-learn samples , whereas those with low variability
11FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
Dataset (task) type prompt label
IMDb and SST2
(semantic analysis
of movie review)zero-shot “The movie review in positive/negative sentiment for a movie is: ” positive/negative
few-shot“The movie review is: <sample_1>
The movie review is: <sample_2>...
The movie review is: <sample_S>
The movie review in positive/negative sentiment which is diverse
in the expression compared to the above given samples is: ” positive/negative
Yelp
(semantic analysis
of restaurant review)zero-shot “The restaurant review in positive/negative sentiment is:” positive/negative
few-shot“The restaurant review is: <sample_1>
The restaurant review is: <sample_2>...
The restaurant review is: <sample_S>
The new restaurant review in positive/negative sentiment which is diverse in
the expression compared to the above given samples is: ” positive/negative
QNLI
(information-question
entailment analysis)zero-shot“Information: <information>
Question (answer in/not in above information): ” entailment/not_entailment
few-shot“The Information-Question pair is: <sample_1>
The Information-Question pair is: <sample_2>...
The Information-Question pair is: <sample_S>
The new Information-Question pair which is diverse in the expression
compared to the above given samples is: Information: <information>
Question (answer in/not in above information): ” entailment/not_entailment
MNLI (matched
and mismatched)
(sentence pair
relation analysis)zero-shot“<sentence1> In other words, /
<sentence1> Furthermore, /
There is a rumor that <sentence1> However, the truth is: ”entailment/
neutral /
contradiction
few-shot“The sentence pair is: <sample_1>
The sentence pair is: <sample_2>...
The sentence pair is: <sample_S>
The new sentence pair which is diverse in the expression
compared to the above given samples is: <sentence1> In other words, /
<sentence1> Furthermore, /
There is a rumor that <sentence1> However, the truth is: ”entailment/
neutral /
contradiction
AgNews
(news articles
classification)zero-shot “The news articles is in the category of World/Sports/Business/Technology : ”World/Sports/
Business/Technology
few-shot“The news article is: <sample_1>
The news article is: <sample_2>...
The news article is: <sample_S>
The new news article in the category of World/Sports/Business/Technology
which is diverse in the expression compared to the above given samples is: ”World/Sports/
Business/Technology
MarkedNews
(self-defined news
articles classification)zero-shot“A news article in the category of World that does not include ‘$’/Sports that
does not include ‘$’/Business that does not include ‘$’/Technology that does
not include ‘$’/Money with ‘$’ included : ”World/Sports/
Business/Technology/
Money with $ included
few-shot“The news article is: <sample_1>
The news article is: <sample_2>...
The news article is: <sample_S>
The new news article in the category of World that does not include ‘$’/
Sports that does not include ‘$’/Business that does not include ‘$’/
Technology that does not include ‘$’/Money with ‘$’ included which is
diverse in the expression compared to the above given samples is: ”World/Sports/
Business/Technology/
Money with $ included
Table 6. Prompt used for synthetic dataset generation.
yet low confidence are identified as hard-to-learn samples.
Conversely, samples with high variability are deemed am-
biguous .
We provide dataset cartography of synthetic datasets gener-
ated by 6different PLMs (GPT-2, Llama-2, Vicuna, OPT,
ChatGLM3 and Flan-T5) in Figure 6 . In left-subplot of
each sub-figure in Figure 6, we display the variability (x-axis) and confidence (y-axis) of all samples. The right
sub-plots depict histograms detailing the distributions of
confidence, variability, and correctness. Notice that exactly
1,000samples are scattered onto each plot, although sam-
ples may overlap with each other, creating a visually sparser
impression.
Comparing dataset cartography generated by the same PLM,
12FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
Algorithm 2 Functions used in Algorithm 1 for FuseGen
function S_AccumulativeSynDataGeneration( Dk,ˆD,
N,J,j):
ifj= 0then
Use zero-shot prompt as working prompt T.
else
UseˆDto create few-shot prompt as working prompt T.
end if
GenerateN
J+1samples using Tand add them to Dk.
return Dk.
function S_STMTraining( D,m(0),E2):
Initialize a trainable STM m←m(0)and train musingDk
forE2epochs with Equation (2).
return m.
function C_SampleSelection( D,{mk}K
k=1,˜m,α,R,S):
Reset ˆD ← ∅ .
fork′= 1toKdo
forEach sample (xk,i, yk,i)inDdo
Obtain the prediction vector pk′,k,i=mk′(xk,i)∈RC
and predicted label-position probability pk′,k,i[yk,i]∈
R1.
Calculate disagreement score dk,i =
STD( p1,k,i[yk,i], ..., p k′,k,i[yk,i], ..., p K,k,i[yk,i]).
end for
end for
Sort all the samples within Dand add the top- (1−α)Rsam-
ples with the lowest score and top- αRsamples with the highest
samples into ˆD.
Calculate the influence score of each sample in ˆDwith ˜m
using Eq.(3) in Ye et al. (2022b).
ˆD ← { top-Ssamples with the highest influence score }.
return ˆD.
function S_WeightAdjustSTMTraining( D, m(0),
{w(0)
i}N
i=1,E1,E2):
fore1= 0toE1−1do
Initialize a trainable STM m←m(0)and train musing
DforE2epochs with weighted loss using {w(e1)
i}N
i=1and
Equation (5).
Adjust sample-level weight w(e1+1)
i ←w(e1)
iwithmusing
Equation (4) for each sample (xi, yi), i= 1, . . . , N .
end for
return m.
we can see that FuseGen helps to improve the dataset com-
position by introducing more ambiguous samples to balance
the prevalence of the easy-to-learn samples, while ensuring
hard-to-learn samples remain a minority.
C.2. T-SNE Visualization of Sample Distributions
We also visualize the t-distributed Stochastic Neighbor Em-
bedding (t-SNE) of synthetic samples ( N= 1,000) in
Figure 7. All samples are embedded with a pre-trained
bert-base-uncased encoder model.Consistent with the dataset cartography in Figures 1 and 6,
FuseGen generates a higher proportion of ambiguous sam-
ples, which pulls the distribution of samples from different
semantic classes closer to each other compared to Zero-
Gen and ProGen. This effect is particularly pronounced for
synthetic datasets given by Llama-2 and Vicuna.
C.3. Low-quality Synthetic Dataset Samples
In Table 7, we show examples of low-quality samples, in-
cluding samples that are “mislabeled”, of “low-relevancy”,
and of “low-text-quality”. Samples are selected from syn-
thetic datasets generated by individual PLMs using zero-
shot prompt for the movie review semantic analysis task.
This demonstrates the importance for improving the overall
data quality of synthesic datasets.
C.4. Source of Selected In-context Samples
We show in Figure 8 that, the selected in-context samples
(desirable subset) and its candidates during CDG originate
from various PLMs. However, the proportion of samples
contributed by each PLM can fluctuate across iterations.
This verifies that knowledge from different PLMs are fused
and fed to each PLM through the feedback prompt, which
further boosts the generation quality of each PLM.
C.5. Ablations on More Tasks
We include the ablation results of “w/o SWA”, “w/o CDG
& SWA” and “SDG+mixed”(also w/o SWA) for more tasks
and here due to space limitation. We also elaborate the ex-
planation of “SDG+mixed” here. In “SDG+mixed”, SWA
is removed and CDG is replaced with self-based feedback,
i.e. random selection is applied to select Rcandidate sam-
ples from each Dk.Kin-context samples subsets are than
selected based on sample importance from the Kcandidate
sample sets of size Rand are further fed to respective PLM
Pkto generate samples.
As illustrated in Table 8, the application of SWA signifi-
cantly improves the performance of all STMs, particularly
for{mk}K
k=1. This improvement highlights the efficacy of
SWA in enhancing the quality of synthetic datasets through
the up-weighting of higher-quality samples and the down-
weighting of lower-quality samples, thereby reducing the
impact of the latter. Furthermore, the application of CDG
also significantly boosts the performance of all STMs to
a greater extent than applying SDG. This underscores the
superiority of cross-model feedback over the combination
of self-guided feedback and highlights the efficacy of CDG
in harnessing the capabilities of multiple PLMs.
13FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0100200300400density
0.0 0.1 0.2
variability050100150200density
0.00.20.30.50.70.81.0
correctness0100200300400density
(a) GPT-2 ZeroGen K=1 (84.83)
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0100200300400density
0.0 0.1 0.2
variability0100200density
0.00.20.30.50.70.81.0
correctness0200400density (b) GPT-2 ProGen K=1 (85.74)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400density
0.0 0.1 0.2
variability0100200density
0.00.20.30.50.70.81.0
correctness0200400density (c) GPT-2 Ours K=6 (87.85)
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.8
1.0
0.4 0.6
confidence02505007501000density
0.0 0.1 0.2
variability0200400600density
0.00.20.30.50.70.81.0
correctness0200400600800density
(d) Llama-2 ZeroGen K=1 (84.23)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.3
0.5
0.8
1.0
0.4 0.6
confidence02505007501000density
0.0 0.1 0.2
variability0200400600density
0.00.20.30.50.70.81.0
correctness0200400600800density (e) Llama-2 ProGen K=1 (84.24)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600800density
0.0 0.1 0.2
variability0200400density
0.00.20.30.50.70.81.0
correctness0200400600800density (f) Llama-2 Ours K=6 (86.60)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600800density
0.0 0.1 0.2
variability0200400600density
0.00.20.30.50.70.81.0
correctness0200400600800density
(g) Vicuna ZeroGen K=1 (82.37)
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.5
0.7
0.8
1.0
0.4 0.6
confidence02505007501000density
0.0 0.1 0.2
variability0200400600density
0.00.20.30.50.70.81.0
correctness0200400600800density (h) Vicuna ProGen K=1 (83.60)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600800density
0.0 0.1 0.2
variability0200400600density
0.00.20.30.50.70.81.0
correctness0200400600800density (i) Vicuna Ours K=6 (87.50)
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability050100150density
0.00.20.30.50.70.81.0
correctness0200400density
(j) OPT ZeroGen K=1 (84.97)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability0100200density
0.00.20.30.50.70.81.0
correctness0200400density (k) OPT ProGen K=1 (87.57)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability0100200density
0.00.20.30.50.70.81.0
correctness0200400density (l) OPT Ours K=6 (88.47)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability0100200300density
0.00.20.30.50.70.81.0
correctness0200400600density
(m) ChatGLM3 ZeroGen K=1
(86.43)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability0100200300density
0.00.20.30.50.70.81.0
correctness0200400600density(n) ChatGLM3 ProGen K=1 (87.07)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600800density
0.0 0.1 0.2
variability0100200300density
0.00.20.30.50.70.81.0
correctness0200400600density (o) ChatGLM3 Ours K=6 (88.56)
0.00 0.05 0.10 0.15 0.20
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0200400600density
0.0 0.1 0.2
variability0100200density
0.00.20.30.50.70.81.0
correctness0200400600density
(p) Flan-T5 ZeroGen K=1 (88.18)
0.025 0.050 0.075 0.100 0.125 0.150 0.175
variability0.350.400.450.500.550.600.650.70confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0100200density
0.0 0.1 0.2
variability050100150200density
0.00.20.30.50.70.81.0
correctness0100200300density (q) Flan-T5 ProGen K=1 (85.80)
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
variability0.30.40.50.60.7confidenceambiguouseasy-to-learn
hard-to-learn0.0
0.2
0.3
0.5
0.7
0.8
1.00.4 0.6
confidence0100200300400density
0.0 0.1 0.2
variability050100150density
0.00.20.30.50.70.81.0
correctness0100200300400density (r) Flan-T5 Ours K=6 (88.73)
Figure 6. Synthetic dataset cartography (Swayamdipta et al., 2020) using 1,000generated samples for movie review semantic analysis.
ZeroGen uses zero-shot prompt for generation, while ProGen and FuseGen (Ours) use few-shot prompt with feedback but with different K,
the number of PLMs involved. Numbers within parentheses are STM performance evaluated using IMDb after training on the generated
dataset, with SWA applied during training. 14FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
OPT Vicun Llama- GPT-2 2 a ChatGLM3 Flan-T5
-50 50 50 -50 50 50 0 0 0 0
(a) ZeroGen
OPT Flan-T5 Vicun Llama- GPT-2 2 a ChatGLM3
(b) ProGen
OPT Vicun Llama- GPT-2 2 a ChatGLM3 Flan-T5
-50 0 50 -50 50 50 0 0
(c) FuseGen (Ours)
Figure 7. t-SNE visualization of each synthetic sample generated by 6PLMs for movie review task. Different colors, blue and orange,
represents embeddings from different class, positive and negative respectively.
C.6. Multi-PLM v.s. single-PLM on More Tasks
We provided additional results on the comparison of multi-
PLM ( K= 6) and single-PLM ( K= 1) across 8datasets
for various tasks in Table 9. As multi-PLM ( K= 6) con-
sistently outperforms all single-PLM under the each task,
we conclude that multi-PLM collaboration is more effective
than relying on a single PLM for enhancing STM perfor-
mance.
C.7.Detailed Results for Hyper-parameters α,NandJ
Due to space limitation, we provide detailed results of hyper-
parameters α(ratio of high-variability samples within the R
in-context sample candidates), N(sample generation bud-
get), and J(feedback times) here in Tables 10 to 12. We
additionally include the performance of each mkas well
(SWA applied). These results indicate that employing a
more balanced mix of high-variability and low-variability
samples ( α= 0.5), a larger sample budget Nand more
feedback times Jall help to achieve a better STM perfor-
mance. This enhancement is observed not only for the finalSTM ˜m, but also for each {mk}K
k=1.
15FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
PLM Sample Text Label Type
GPT-2 A touching portrayal of friendship in the troubled 21st Century. negative mislabeled
GPT-2 We believe the movie <C> is decent but it’s none too unexpected for someone like me who
has fallen asleep watching movies and has seen this film before.positive mislabeled
GPT-2 The ’Godzilla’ of the year thing is back. Say hello to The Amazing Spider-Man 2 negative low-relevancy
GPT-2 Broken Hearts Way was, on average, better than <T> was positive low-text-quality
GPT-2 Help! My name is Mark Hammer and I have always wanted to write a movie review. Now,
after hours and hours of research, the truth is finally out.negative low-text-quality
Llama-2 It was a complete waste of time and money, I would not recommend it to anyone. This
movie was just plain bad, with uninteresting characters and a plot that was predictable
from the start. The acting was also subpar and the dialogue was cringe-worthy. I would
give it a 0/10 if I could.positive mislabeled
Llama-2 After waiting for what felt like an eternity, I finally watched negative low-text-quality
Vicuna The film, directed by a renowned auteur, is a story of a group of people navigating through
a dystopian society.**** It is a thought-provoking piece that leaves the audience pondering
on the state of society and what it takes to survive in a world that is controlled by a powerful
corporation. The cinematography is beautiful and the score is hauntingly good.negative mislabeled
Vicuna A great movie that will leave you thinking for days to come. No need to spend a lot of
money to see it. It is a must watch for everyone.negative mislabeled
Vicuna C++ is a difficult and complex language, but with time and practice, it becomes easy and
manageable.1234567890positive low-relevancy
OPT But the movie begins with this big, action-packed explosion, which carries right into the
more thoughtful segments of the movie. But wait! There’s more! It continues through
those thoughtful segments and the entire rest of the movie, not just a handful of sequences.negative mislabeled
OPT The movie is very disappointing. The story line is nothing but a shameless rip-off. Even
the main cast of the movie is not worth any praise at all. This is a movie to just go and
waste your money. Just don’t!positive mislabeled
OPT When my teenage self in France was presented with this movie on VHS, I was in love with
the soundtrack, with the French style of acting (something totally alien to my home in the
States), and with the idea that there was a world like this.negative mislabeled
OPT The packaging is nothing more than an anonymous brown paper bag, and the theater
provided stale popcorn.positive low-relevancy
OPT \n\n- a negative movie review \n\nThe movie review in negative sentiment for movie positive low-text-quality
ChatGLM3 Very disappointing. There was not one LOL moment. No wonder the movie was not a box
office hit.positive mislabeled
ChatGLM3 Perhaps a crime movie and is interesting to watch . negative mislabeled
ChatGLM3 i’m not the most romantic person and i’m not a chick. positive low-relevancy
ChatGLM3 even a bad magician should be able to catch the rabbit positive low-relevancy
Flan-T5 He works in audio-visual technique and the end product is often flawed. positive mislabeled
Flan-T5 When a thing is a fantasy, it just become real, whether it was imagined or just played out.
When they put on a performance in this movie, it has to be one of the best, most inspired
moments.negative mislabeled
Flan-T5 if the time has come to say goodbye to Dick Van Patten. positive low-relevancy
Flan-T5 perverse creatures know they should be ashamed to exist. for human beings to walk around
dressed like cannibals in a heavy jungle set up camp.negative low-relevancy
Flan-T5 And this is just another (incomplete) list of things that negative low-text-quality
Table 7. Examples of low-quality samples in generated synthetic dataset for movie review.
16FuseGen: PLM Fusion for Data-generation based Zero-shot Learning
(a) Samples in selected desirable subset of size S= 8
(b) Selected candidates of size R= 40
Figure 8. Proportion of samples in Sin-context samples and R
sample candidates that originate from each PLM at each feed-
back time ( J) in FuseGen with J= 4, R= 40 , S= 8, N=
1,000, K= 6for movie review sentiment analysis task. Results
are averaged using 3different seeds.
IMDb
mG mL mV mO mC mF ˜m
FuseGen (Ours) 87.85 86.60 87.50 88.47 88.56 88.73 90.19
w/o SWA 82.90 78.98 74.34 85.17 85.77 85.43 89.07
w/o CDG & SWA 80.71 75.73 59.41 81.37 81.14 84.35 87.06
SDG+mixed 80.72 76.18 65.05 84.19 84.56 81.19 87.41
SST-2
mG mL mV mO mC mF ˜m
FuseGen (Ours) 86.38 84.36 85.52 86.50 86.96 86.32 87.35
w/o SWA 81.87 79.22 82.43 80.99 85.73 80.99 85.38
w/o CDG & SWA 80.68 76.42 76.46 80.80 84.58 78.44 85.01
SDG+mixed 80.75 77.53 79.52 80.86 85.69 80.89 85.71
Yelp
mG mL mV mO mC mF ˜m
FuseGen (Ours) 91.94 90.30 90.81 92.50 92.98 92.21 93.54
w/o SWA 90.87 88.09 84.99 87.19 91.72 90.71 92.84
w/o CDG & SWA 89.13 79.17 81.97 86.78 81.50 89.48 92.16
SDG+mixed 89.63 82.39 83.80 86.84 86.32 87.48 92.23
QNLI
mG mL mV mO mC mF ˜m
FuseGen (Ours) 60.55 72.48 74.10 57.39 69.89 72.13 74.95
w/o SWA 56.72 69.99 70.94 51.98 56.39 68.65 73.41
w/o CDG & SWA 51.24 65.81 70.61 50.83 53.01 55.73 69.41
SDG+mixed 52.13 69.22 70.11 51.79 54.87 68.58 70.20
Table 8. Comparison between FuseGen and its ablations with K=
6, N= 1,000, J= 4. Each mkis trained on Dkof size 1,000
while ˜mis trained on Dof size 6,000.multi single
˜m ˜mG ˜mL ˜mV ˜mO ˜mC ˜mF
IMDb 89.96 87.60 86.14 85.42 87.59 88.84 89.74
SST-2 87.51 84.81 84.39 85.22 85.88 87.43 85.38
Yelp 93.27 93.03 91.07 91.69 92.72 92.08 92.07
QNLI 74.92 64.52 73.22 73.34 59.03 64.93 73.60
MNLI-m 49.76 44.93 49.61 49.11 37.40 32.82 49.34
MNLI-mm 51.70 48.53 51.62 50.76 42.32 33.05 51.47
AgNews 86.89 82.21 85.34 85.36 86.75 86.27 86.36
MarkedNews 83.85 79.98 80.04 79.36 78.60 83.54 80.86
Table 9. Comparison between FuseGen using multi-PLM ( K= 6)
and single-PLM ( K= 1) with 4datasets. MNLI-m and MNLI-
mm each stands for MNLI-matched and MNLI-mismatched. Best
result is marked as bold with the second best marked with
underline for each dataset (each row).
α mG mL mV mO mC mF ˜m
0.0 54.00 70.07 67.75 51.12 55.70 66.49 74.08
0.25 56.12 70.22 70.45 52.10 56.90 71.12 74.37
0.5 59.68 71.48 72.37 52.37 57.33 72.12 74.92
0.75 55.27 69.13 69.53 52.19 56.59 70.91 74.23
1.0 54.85 66.47 64.46 50.08 56.50 70.50 74.16
Table 10. Comparison of different αused for FuseGen with QNLI
as test dataset. Best result is marked as bold with the second best
marked with underline for each STM (each column).
N mG mL mV mO mC mF ˜m
100 51.33 53.16 53.79 50.62 51.20 51.11 56.27
200 52.23 60.42 60.06 50.71 53.07 59.09 65.11
500 53.53 67.36 67.90 51.67 54.95 64.72 72.18
1,000 59.68 71.48 72.37 52.37 57.33 72.12 74.92
Table 11. Comparison of different Nused for FuseGen with QNLI
as test dataset. Best result is marked as bold with the second best
marked with underline for each STM (each column).
J mG mL mV mO mC mF ˜m
0 56.95 71.13 72.21 51.96 55.12 58.43 74.44
1 57.11 71.50 72.25 52.07 56.53 64.81 74.77
4 59.68 71.48 72.37 52.37 57.33 72.12 74.92
959.71 71.60 72.37 52.34 57.70 72.14 75.07
Table 12. Comparison of different Jused for FuseGen with QNLI
as test dataset. Best result is marked as bold with the second best
marked with underline for each STM (each column).
17