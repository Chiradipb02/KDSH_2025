Can Large Language Models Always Solve Easy Problems
if They Can Solve Harder Ones?
Zhe Yang1, Yichang Zhang2, Tianyu Liu2, Jian Yang2, Junyang Lin2
Chang Zhou2, Zhifang Sui1
1State Key Laboratory for Multimedia Information Processing,
School of Computer Science, Peking University
2Alibaba Group
{yz_young, szf}@pku.edu.cn
{yichang.zyc, tianyu0421, ericzhou.zc}@alibaba-inc.com
Abstract
Large language models (LLMs) have demon-
strated impressive capabilities, but still suffer
from inconsistency issues (e.g. LLMs can re-
act differently to disturbances like rephrasing
or inconsequential order change). In addition
to these inconsistencies, we also observe that
LLMs, while capable of solving hard problems,
can paradoxically fail at easier ones. To evalu-
ate this hard-to-easy inconsistency, we develop
the ConsisEval benchmark, where each entry
comprises a pair of questions with a strict or-
der of difficulty. Furthermore, we introduce
the concept of consistency score to quantita-
tively measure this inconsistency and analyze
the potential for improvement in consistency by
relative consistency score. Based on compre-
hensive experiments across a variety of existing
models, we find: (1) GPT-4 achieves the high-
est consistency score of 92.2% but is still incon-
sistent to specific questions due to distraction
by redundant information, misinterpretation of
questions, etc.; (2) models with stronger capa-
bilities typically exhibit higher consistency, but
exceptions also exist; (3) hard data enhances
consistency for both fine-tuning and in-context
learning. Our data and code will be publicly
available on GitHub.1
1 Introduction
With the increases in pre-training corpora and
the number of parameters (Radford et al., 2018,
2019; Brown et al., 2020), large language mod-
els (LLMs) have shown remarkable performance
across various natural language processing (NLP)
tasks, even generating expert-level responses to
user queries. The extraordinary capabilities of
LLMs hold potential for further real-world applica-
tions (Wang et al., 2023c; Guo et al., 2023; Driess
et al., 2023), which necessitate higher requirements
for model trustworthiness (Wang et al., 2023a; Li
1https://github.com/QwenLM/ConsisEval
17 x 8 = ?
17 x 8 = 106
  hard problem
easy problemFigure 1: A hard-to-easy inconsistency case of LLMs.
A counter-intuitive phenomenon occurs when an LLM,
which can solve a harder problem, surprisingly goes
wrong on an easier problem.
et al., 2023a; Sun et al., 2024a) and consistency
(Jang and Lukasiewicz, 2023; Elazar et al., 2021).
However, LLMs still suffer from inconsistency
issues: semantically equivalent queries (Elazar
et al., 2021; Raj et al., 2023) and insignificant order
changes of inputted contents (Wang et al., 2023b)
can lead to divergent outcomes; LLMs can also be-
have differently in the generation versus validation
of the same content (Li et al., 2023b); moreover,
logical transformations like negation and symmetry
can also induce inconsistent behaviors (Jang et al.,
2022). In addition to previous work, we also find
LLMs able to solve hard problems surprisingly fail
to solve easier ones (as shown in Figure 1), suffer-
ing from the hard-to-easy inconsistency. Unlike
LLMs, humans are naturally consistent reasoners,
and it is undisputed that an individual proficient
in calculus can easily address simpler arithmetic
problems. However, why this difference exists is
still unknown and relevant research to explore hard-
to-easy consistency of LLMs is still lacking.
1arXiv:2406.12809v1  [cs.CL]  18 Jun 2024To systematically evaluate this consistency of
LLMs, we develop ConsisEval , a Hard-to-easy
Consis tency Eval uation Benchmark, through au-
tomatic generation and human annotation. Consi-
sEval encompasses data from three domains: in-
struction following, code, and mathematics, each
entry consisting of a pair of questions with a strict
order of difficulty. Considering the absence of an
off-the-shelf metric, we propose a new metric con-
sistency score, which is defined as the conditional
probability of a model correctly answering easy
questions provided that it has correctly answered
harder ones, for quantitative assessment of con-
sistency from a probabilistic stance. Further, to
analyze the potential for improvement in consis-
tency if model capability remains unchanged, we
introduce the concept of relative consistency score.
The calculation of our metrics relies on the proba-
bility of a model answering each question correctly
through a single sampling, for which we design
two probability estimation methods.
Based on our benchmark and metrics, we con-
duct extensive experiments on various LLMs.
Among evaluated models, GPT-4 (Achiam et al.,
2023) achieves the highest CS of 92.2%, demon-
strating notable hard-to-easy consistency. Nonethe-
less, GPT-4 also exhibits inconsistent behaviors
to specific prompts due to distraction by redun-
dant information, misinterpretation of questions,
etc. Further, we find models with stronger capa-
bilities typically exhibit higher consistency, but ex-
ceptions where powerful models demonstrate poor
consistency also exist. Additionally, we discover
that models show higher consistency when trained
under hard data than easy data, and that holds the
same under few-shot setting (in-context learning
with harder demonstration examples shows better
consistency).
We summarize our contributions as follows:
1.To the best of our knowledge, we are the first
to systematically study the hard-to-easy con-
sistency of LLMs and establish a benchmark
to evaluate this consistency.
2.We propose metrics grounded in probabilistic
theory to quantitatively measure the hard-to-
easy consistency, along with probability esti-
mation methods for metric computation.
3.Based on our benchmark and metrics, we con-
duct extensive experiments across a variety ofLLMs and draw some conclusions that may
benefit future research.
2 ConsisEval Benchmark
To systematically evaluate the hard-to-easy consis-
tency of LLMs, we develop ConsisEval with data
from code, mathematics, and instruction-following
domains, which are widely considered to be diffi-
cult but of significant importance for LLMs (Wei
et al., 2021; Cobbe et al., 2021a,b; Zhou et al.,
2023). Different from traditional benchmarks in
which data are usually individual, there are only
pairwise data in ConsisEval: one datum is com-
prised of two questions (an easy question and a
harder one) with a strict order of difficulty, and
we present some example data from ConsisEval in
Table 5. To construct ConsisEval, we collect easy
data from some established public datasets (§2.1);
then we acquire hard data through automatic gener-
ation by GPT-4 and human annotation (§2.2), and
this process is shown in Figure 2.
2.1 Easy Data Collection
Mathematics easy data are collected from
GSM8K (Cobbe et al., 2021a), a linguistically di-
verse collection of high-quality grade school math
word problems crafted by human problem writers.
The difficulty of these problems varies, requiring
from 2 to 8 steps to solve, and solving these prob-
lems typically requires a series of fundamental cal-
culations employing basic arithmetic operations
(+−×÷ ). To prevent easy data from being too dif-
ficult to be further improved in terms of difficulty,
we only select the problems requiring 3 steps to
solve in the test set of GSM8k as our easy data in
the mathematics domain (298 entries).
Code easy data are collected from HumanEval
(Cobbe et al., 2021b), a benchmark aiming at evalu-
ating the capability of LLMs to generate standalone
Python functions from docstrings. For each cod-
ing problem, a check function containing some test
cases is provided for automatic correctness evalu-
ation of code samples. Since HumanEval is rela-
tively small , we select all of the data in HumanEval
as our easy data in code domain (164 entries).
Instruction-following easy data are collected
from IFEval (Zhou et al., 2023), a benchmark com-
prised of various instructions for LLMs to follow.
Each instruction contains 1-3 verifiable constraints
(e.g. maximum number of words in response or
2generating 
multiple  sampleshard datum 1
hard datum 2
hard datum k......
human  
annotator         1. Select
2. Check and Revise:
  • strict order of difﬁculty 
  • correctness of answer
  • correctness of check function
  • ......
hard datum
discardhard datum
hard datum
......
 ...preserve
select
 ﬁnal datumcheck & revise
seed data
(easy data)GPT-4input easy 
datum
Figure 2: The hard data collection process of ConsisEval. An easy datum is fed into GPT-4 with a well-designed
prompt and multiple hard data candidates are sampled. Human annotators select the one of best quality, then check
and revise the sample to make it fit our criteria.
the appearance of specific keywords in response),
whose correctness can be automatically evaluated
by rule-based check functions. We only select the
instructions with only one constraint as our easy
data in instruction-following domain (270 entries).
2.2 Hard Data Collection
To build our pairwise dataset in which a strict or-
der of difficulty is guaranteed for each pair of easy
and hard problems, all of the hard data are modi-
fied from easy data. We employ a semi-automatic
pipeline that integrates the automatic generation of
GPT-4 with human annotation to acquire hard data,
and the whole workflow is illustrated in Figure 2.
Compared to traditional methods that rely solely on
manual annotation, our semi-automatic approach
can significantly alleviate the workload of human
annotators.
Automatic generation. Considering the remark-
able performance of GPT-4 on various text genera-
tion tasks, we employ GPT-4 as a strong modified
data generator to acquire our hard data candidates
for human annotators to choose from. To make
GPT-4 understand our criteria better, we insert easy
data into a well-designed prompt template (shown
in Appendix J) before feeding them into GPT-4.
Taking the code domain as an example, the prompt
consists of 5 parts: (1) the #Instruction# part ar-
ticulates the information we want GPT-4 to know,
including but not limited to definition of our modifi-
cation task, composition of a datum, and guarantee
of strict order of difficulty; (2) the #Demonstra-
tions# part requires insertion of easy and hard data
pairs as demonstrations; (3) finally, an easy datum
targeted for modification is decomposed into threeEasy Question:
John has 2 houses with 3 bedrooms each. How many
bedrooms are there in total?
Hard Question:
John has 2 houses with 3 bedrooms each. Each bedroom
has 2 windows. How many windows are there in total?
Table 1: An example question pair with a strict order
of difficulty. Green text denotes the common part of
questions and blue text denotes the additional part of
hard question.
components and inserted into the #Problem#, #An-
swer#, and #Check Function# parts, respectively.
Human annotation. Though we have endeav-
ored to request GPT-4 to generate hard data that
fully adheres to our criteria through a well-designed
prompt, the generated contents may still not meet
our standards (e.g. some samples lack a strict or-
der of difficulty and check functions of some other
samples are incorrect). To address potential issues
in generated samples, we have engaged human an-
notators to inspect, select, and revise these samples.
Firstly, the annotators are required to select the
sample of the highest quality from multiple candi-
dates and discard all the other samples. To ensure
compliance with our criteria, the selected sample
is checked from two aspects:
1.Strict order of difficulty : the steps or knowl-
edge (or ability) required to solve an easy
problem should be a proper subset of those for
the hard problem (example shown in Table 1).
2.Correctness : the standard answer or check
function (for automatic judgment of model-
3generated answers) should be correct.
If one sample fails to comply with our criteria dur-
ing the checking process, the annotators will revise
it to ensure full alignment with our standards.
3 Evaluation Metrics
Firstly, we formulate the evaluation problem and
introduce mathematical notations in §3.1. Consid-
ering that there is no off-the-shelf metric to utilize,
then we propose a new metric named Consistency
Score (§3.2) to measure the hard-to-easy consis-
tency quantitatively. Further, we introduce the con-
cept of Relative Consistency Score (§3.3) to ana-
lyze the potential for improvement in consistency.
We model sampling an answer from an LLM for a
given question as a stochastic process, wherein the
answer is correct with a fixed probability p. The
computation of our metrics requires access to p,
and §3.4 discusses how to estimate pby maximum
likelihood estimation.
3.1 Problem Formulation and Notation
Initially, we have a partially ordered set com-
prising Npairs of data, denoted as A⊙
B={(a1, b1),(a2, b2), ...,(aN, bN)}, where A=
{a1, a2, ..., a N}represents a set of easy questions,
andB={b1, b2, ..., b N}constitutes a set of hard
questions. A stringent guarantee exists that the dif-
ficulty order satisfies ai< bi, forbiis derived from
aiby increasing the difficulty level. For a given
question ai(orbi), the model generates a correct an-
swer through a single temperature-based sampling
with probability P(ai)(orP(bi)). We employ ˆ
to symbolize estimates (e.g. ˆP(ai)represents the
estimate of the true value P(ai)). For convenience,
all of the notations mentioned and their meanings
are shown in Appendix A.
3.2 Consistency Score
Can large language models solve easy problems
if they can solve harder ones? To answer this
question from a probabilistic perspective, we in-
troduce a metric termed Consistency Score ( CS),
which is the conditional probability of a model cor-
rectly answering easy questions given that it has
correctly answered harder ones. The higher CS
indicates the lower probability humans encounter
inconsistency phenomena when using LLMs, so
CS is almost equal to human perceptions of model
consistency. Let P(a|b)be the conditional proba-
P(a) P(b) P(a,b) P(a) P(b) P(a,b)
Consistent Model Inconsistent ModelFigure 3: Venn diagram for consistent/inconsistent mod-
els in complete probability space. The orange ,red
circles and their overlap area denote the probability of
a model correctly answering easy questions, hard ques-
tions, and both respectively. the overlap area of con-
sistent models is much larger than that of inconsistent
models.
bility of a model correctly answering agiven that
it has answered bcorrectly, and we have:
CS=P(a|b) =P
i=1,...,NP(ai)P(bi)P
i=1,...,NP(bi)(1)
The detailed derivation of CS is shown in Ap-
pendix B. To intuitively understand the distinctions
between consistent and inconsistent models and
better illustrate CS, we present a Venn diagram in
Figure 3. The more consistent a model is, the larger
overlap area P(a, b)in Venn diagram, and conse-
quently the higher CS of the model. Fundamentally,
CS represents the ratio of P(a, b)toP(b).
3.3 Relative Consistency Score
In addition to CS that directly reveals consistency
probability of LLMs, we also endeavor to analyze
the potential for improvement in consistency if
model capability remains unchanged. To analyze
what the CS of an evaluated model M0should be
if it behaves extremely consistently/inconsistently,
we formally define a model set Ω ={M0, M1, ...}
(detailed definition shown in Appendix C) in which
models possess similar capabilities to M0and de-
rive the upper and lower bounds of CS (denoted as
CSuppandCSlow) among these hypothetical mod-
els. Based on these bounds, we propose Relative
Consistency Score ( RCS ) (as shown in Figure 4)
to indicate the potential for improvement in consis-
tency, and low RCS can reveal high potential for
improvement in CS. The RCS is given by:
RCS =CS−CSlow
CSupp−CSlow(2)
According to the definition of Ωand rearrange-
ment inequality, we can obtain strict mathematics
4RCS = 
CS  100% 0%lower
boundupper
bound
Figure 4: Visualized expression of relative consistency
score.
bounds. However, these bounds are empirically too
loose, and thus we utilize tighter bounds derived
from two heuristics:
CSlow=Σi=1,...,NP(ai)
N, (3)
CSupp=P
i=1,...,N(P(bi) + ˆµ)P(bi)P
i=1,...,NP(bi),(4)
where ˆµ=Σi=1,...,N (P(ai)−P(bi))
N, and the deriva-
tion of boundaries and discussion are shown in
Appendix D.
3.4 Probability Estimation
For a given question aiand a given model, the
probability P(ai)that the model produces a cor-
rect answer in a single sampling is an unknown
constant. We propose two methods for estimat-
ingP(ai)based on repeated sampling. For open-
source models that can be deployed locally, esti-
mate ˆP(ai)is obtained by sampling multiple an-
swers independently. For proprietary models that
require payment for API calls, an early stopping
strategy is employed during answer sampling to
obtain estimate ˆP(ai)with fewer API calls.
Multiple Sampling Estimation For a given ques-
tionai, answers are sampled mtimes to obtain a
sequence a1
i, a2
i, ..., am
i. If the model generates
a correct answer on the jth sampling, we denote
aj
i= 1; otherwise, aj
i= 0. In this scenario, aj
ifol-
lows a Bernoulli distribution, andP
j=1,...,maj
ifol-
lows a Binomial distribution (i.e.P
j=1,...,maj
i∼
B(m, P(ai))). It can be derived that the maximum
likelihood estimate of P(ai)(refer to Appendix E.1
for the derivation details):
ˆP(ai) =P
j=1,...,maj
i
m(5)
Early Stopping Estimation Estimating through
multiple sampling necessitates generating a multi-
tude of answers for the same question (e.g. in §4 we
utilize Llama2-7b-chat to sample 20 answers for a
question). However, considering the high paymentfor the API calls and the typically high accuracy of
closed-source models, an early stopping technique
is employed to estimate with fewer API calls.
Details of early stopping strategy: Initially, we
set the minimum and maximum number of sam-
pling times kminandkmax. For a given question
ai, initially, kminanswers are sampled. If at least
one correct answer exists in these answers, the
sampling process will be terminated; otherwise,
sampling will continue repeatedly until a correct
answer appears for the first time. Besides, the sam-
pling procedure will be forcibly terminated if a
correct answer still does not emerge after sampling
kmaxanswers.
The total number of samples in the above process
and the number of correct answers are denoted as
kandkc, respectively. The maximum likelihood
estimation of P(ai)can be derived as follows (refer
to Appendix E.2 for the derivation details):
ˆP(ai) =kc
k(6)
Besides, we also show the pseudo-code of Early
Stopping Estimation, discuss the trade-off, and
compare these two methods in Appendix E.3.
4 Experiments
4.1 Experimental Setup
For closed-source models, we evaluate GPT-4
Turbo2(Achiam et al., 2023), GPT-3.5 Turbo
3, Qwen Max (Bai et al., 2023), and Claude-
3 Opus4, which can only be accessed via API
calls. For open-source models, we experiment
on Llama2-(7B,13B,70B) (Touvron et al., 2023),
Llama3-(8B,70B) (AI@Meta, 2024), Qwen-1.5-
(7B,14B,72B) (Bai et al., 2023), ChatGLM3-
6B (Du et al., 2022), DeepseekLLM-(7B,67B)
(DeepSeek-AI, 2024), Mistral-7B (Jiang et al.,
2023), Baichuan2-(7B,13B) (Baichuan, 2023), and
Yi-6B (Young et al., 2024). Most of these open-
source models are released with two versions, the
pre-trained base model and the chat model (based
model + instruction tuning and alignment), and we
focus our evaluation solely on chat models. More
implementation details can be found in Appendix
G.1.
2gpt-4-0125-preview
3gpt-3.5-turbo-0125
4claude-3-opus-20240229
5ModelsCode Instruction Following MathsAvg CSHard Easy CS Hard Easy CS Hard Easy CS
GPT-4 Turbo 80.8 85.5 88.1 74.4 84.2 91.8 92.8 96.2 96.8 92.2
GPT-3.5 Turbo 62.3 71.4 81.2 53.0 76.1 88.6 65.6 86.9 90.7 86.8
Claude-3 Opus 79.0 81.1 85.5 78.0 87.7 93.4 93.7 96.5 96.6 91.8
Qwen Max 66.9 75.0 82.4 53.2 74.3 89.6 86.8 95.2 96.8 89.6
Llama3-70B-Instruct 69.2 73.9 84.3 74.7 86.7 94.4 80.8 94.9 96.9 91.9
Llama2-70B-Chat 20.7 34.5 74.7 36.3 56.6 81.0 23.2 70.5 83.7 79.8
Qwen1.5-72B-Chat 47.0 62.3 79.4 34.9 56.5 87.3 75.7 90.6 93.6 86.8
DeepseekLLM-67B-Chat 56.9 68.6 77.9 29.6 52.5 83.8 66.9 90.2 94.8 85.5
Llama2-13B-Chat 14.2 20.2 61.9 24.9 48.3 84.2 8.1 48.6 67.2 71.1
Qwen1.5-14B-Chat 36.1 51.4 74.6 29.3 55.4 83.6 58.2 82.6 90.7 83.0
Baichuan2-13B-Chat 15.7 21.5 59.1 13.0 31.0 63.3 14.2 48.6 65.8 62.7
Llama3-8B-Instruct 41.7 53.6 71.4 62.6 78.5 87.9 38.3 77.8 87.4 82.2
Llama2-7B-Chat 10.2 14.9 63.1 26.6 43.7 75.6 4.7 34.3 57.9 65.5
Qwen1.5-7B-Chat 28.6 40.9 68.4 21.8 47.2 82.5 34.7 68.6 83.6 78.2
ChatGLM3-6B 24.1 50.8 68.5 16.4 36.6 64.7 16.7 64.4 83.9 72.4
DeepseekLLM-7B-Chat 26.6 40.3 62.6 24.1 47.5 71.0 20.8 69.0 84.8 72.8
Mistral-7B-Instruct 20.3 28.4 57.0 37.1 60.8 84.3 11.6 51.8 67.4 69.6
Yi-6B-Chat 8.7 13.2 49.3 15.4 37.4 76.0 10.2 50.9 69.7 65.0
Baichuan2-7B-Chat 8.8 12.4 43.0 12.1 29.9 60.0 5.0 28.4 50.1 51.0
Table 2: Consistency evaluation results. A variety of LLMs are evaluated on code, instruction-following, and maths
domains. On each domain, we report consistency score ( CS), accuracy (%) on hard set and easy set (denoted as
Hard andEasy ). We also report the average consistency score ( Avg CS ) among three domains.
4.2 Main Results
As illustrated in Table 2, we evaluate the hard-to-
easy consistency of LLMs on ConsisEval and re-
port the consistency score (CS) in three domains
and the average consistency score (Avg CS). The
accuracy (%) on easy and hard sets (indicating
model capability) is also shown for comparison.
Among the evaluated LLMs, GPT-4 Turbo show-
cases outstanding performance in three domains
and achieves the highest Avg CS of 92.2%, closely
followed by Claude-3 Opus with an Avg CS is
91.8%. Llama3-(8B,70B)-Instruct exhibit high ca-
pability and consistency among open-source mod-
els, superior to other models of comparable size.
For comparison, CS of humans is theoretically
100% if not take carelessness cases into consid-
eration. Therefore, the potential for further im-
provement in consistency still exists.
We also observe a strong correlation between
capability and consistency of LLMs. For example,
Kendall rank correlation coefficient between accu-
racy on hard set and CS across all evaluated LLMs
on code domain is 0.801 (further discussion is pro-
vided in Appendix G.2). However, higher capabil-
ity does not necessarily lead to higher consistency
(e.g. in math domain, Claude-3 Oplus outperforms
GPT-4 Turbo in capability, yet exhibits a lower
consistency). Additionally, empirical results also
show CS is always larger than easy accuracy acrossall evaluated models, suggesting that answering
hard questions correctly benefits answering easy
questions.
4.3 Relative Consistency Analysis
To analyze the potential for improvement in consis-
tency, we attempt to compare the consistency of an
evaluated model with other hypothetical models of
similar capability ("capability" can be intuitively
but not strictly understood as "performance on ac-
curacy", with a formal definition provided in Ap-
pendix C). For each evaluated model, we present
its CS, upper and lower bounds of CS along with
the relative consistency score (RCS), which can be
utilized to analyze potential improvement in con-
sistency within the current capability.
The experimental results in code domain are pre-
sented in Figure 5, while the comprehensive results
across all domains can be found in Appendix G.3.
In code domain, we find that while GPT-4 Turbo
exhibits high consistency with a CS of 88.1%, there
is still considerable potential for improvement com-
pared to the upper bound 93.0%. Furthermore, the
RCS for GPT-4 Turbo is 34.8%, indicating a rela-
tive improvement potential of 65.2%. Conversely,
Llama2-70B-Chat, despite showing a low CS of
merely 74.7%, achieves an RCS of 81.5%, indicat-
ing notable consistency within its current capabil-
ity.
6Baichuan2-7B-ChatYi-6B-Chat
Mistral-7B-Instruct
Baichuan2-13B-ChatLlama2-13B-ChatDeepseek-7B-ChatLlama2-7B-ChatQwen1.5-7B-ChatChatGLM3-6B
Llama3-8B-Instruct Qwen1.5-14B-ChatLlama2-70B-Chat
Deepseek-67B-ChatQwen1.5-72B-ChatGPT-3.5 TurboQwen Max
Llama3-70B-InstructClaude-3 OpusGPT-4 Turbo102030405060708090100(Relative) Consistency Score (%)
Upper
CS
Lower
RCSFigure 5: Relative consistency results in code domain (shown in ascending order of CS). Except for showing RCS
for each evaluated model in a bar, we also show CS, upper and lower bounds of CS in lines of different colors for
comparison.
5 Analysis
5.1 Hard Training Data Benefits Consistency
To investigate the impact of the ratio between easy
and hard data in the training set on model consis-
tency, we select 2,500 easy and 2,500 hard entries
from the training set of gsm8k (Cobbe et al., 2021a)
based on the number of reasoning steps. We adjust
the ratio between easy and hard data while keep-
ing the total amount constant at 2,500 entries to
construct a series of training sets with varying pro-
portions. We then fine-tune Llama3-8B on these
training sets (each group is repeated three times
under different random seeds with Lora (Hu et al.,
2021)) and observe the consistency behaviors. As
shown in Figure 6, both the CS and RCS generally
increase as the proportion of hard data increases,
suggesting that hard training data benefits model
consistency. Moreover, compared to a dataset com-
posed entirely of hard data, a combination of 80%
hard and 20% easy data yields better consistency,
indicating proper easy data also contributes to en-
hancing model consistency.
5.2 Hard ICL Examples Benefits Consistency
Similar to §5.1, we also explore the impact of easy
and hard in-context learning (ICL) (Brown et al.,
2020; Dong et al., 2022; Yang et al., 2023) demon-
stration examples on model consistency. The ex-
periments are under 1-4 shot setting, and for each
setting we randomly select 20 easy and 20 hard ICL
examples to evaluate the consistency of Llama-8B-
0 10 20 30 40 50 60 70 80 90 100
Hard data proportion (%)4050607080(Relative) Consistency score (%)CS
RCSFigure 6: Consistency of models fine-tuned on training
sets of different proportions of easy and hard data. Fine-
tuned models show higher consistency with more hard
training data.
Instruct. As shown in Figure 7, hard examples dis-
play better consistency than easy ones, and model
consistency progressively increases with the num-
ber of shots.
5.3 Case Study: Why are LLMs Inconsistent?
Through investigations on math inconsistency
cases (shown in Appendix I), where the probability
of solving hard problems is higher than that of eas-
ier ones, we find even state-of-the-art GPT-4 still
behaves inconsistently due to the following rea-
sons: (1) Distracted by redundant information :
As the case shown in Table 6, for the easy question
with redundant conditions, GPT-4 incorrectly pro-
ceeds with an additional step after having already
71 2 3 4
Shot888990919293Consistency Score (%)ICL Type
Easy demos ICL
Hard demos ICL
Zero shotFigure 7: Consistency behavior of ICL with easy and
hard examples under 1-4 shot settings. ICL with harder
examples shows higher consistency.
arrived at the correct answer, leading to a final in-
correct result. (2) Data mismatch : As the case
shown in Table 7, GPT-4 could accurately analyze
the usage of "dancing time on Tuesday" for compu-
tation, but it erroneously utilizes "dancing time on
Thursday" when conducting computation. (3) Mis-
interpretation of questions : As the case shown in
Table 8, the easy question requires finding the "cost
of travel," GPT-4 misinterprets the requirement as
the "cost of tickets for travel". (4) Logical error
(Off-by-one error) : As the case shown in Table
9, the initial state should be recorded as "Day 0"
in the easy question, but GPT-4 erroneously began
recording from "Day 1". (5) Computational er-
ror: As the case shown in Table 10, GPT-4 encoun-
ters computational errors while solving an equation
for the easy question. Superficially, the inconsis-
tency of GPT-4 stems from the occurrence of the
above mistakes on the easy questions but not on
the corresponding hard questions. However, deeper
underlying reasons remain unclear.
6 Related Work
Consistency of LLMs Consistency constitutes
an important part of trustworthiness and reliability
(Wang et al., 2023a; Li et al., 2023a; Chai et al.,
2024; Liu et al., 2023) of LLMs. Humans are inher-
ently consistent reasoners, but LLMs suffer from
inconsistency problems. Wang et al. (2023b) find
LLMs, when acting as evaluators, show inconsis-
tency with insignificant order changes of evaluation
content; Li et al. (2023b) observe that LLMs also
show inconsistency when generating and validating
the same knowledge; Elazar et al. (2021); Raj et al.
(2023) endeavor to evaluate and enhance the consis-tency with semantically identical expressions; Jang
et al. (2022); Jang and Lukasiewicz (2023) evaluate
and analyze consistency to logical transformations,
such as negation and symmetry. Different from per-
spectives presented in previous works, our research
focuses on the hard-to-easy consistency of LLMs.
Easy-to-Hard Generalization Hupkes et al.
(2020); Xu and Wang (2024) study the generaliza-
tion ability of models trained on simple elements
to complex element combinations; likewise, Burns
et al. (2023); Hase et al. (2024); Sun et al. (2024b)
find models trained on easy data exhibit strong gen-
eralization capabilities to hard data. However, we
have observed that training models solely on easy
data can lead to inconsistent behaviors.
Leveled Evaluation Liu et al. (2024); Xu et al.
(2024a) hierarchically evaluate the capability of
LLMs to solve problems of different difficulty lev-
els by data categorized from easy to hard. Simi-
larly but differently, we evaluate the consistency
of LLMs by pairwise hard-to-easy data. Unlike
previous work whose difficulty level is roughly
divided by the number of reasoning steps (Hase
et al., 2024), the difficulty order in our work is
constrained to pairwise questions and more strict.
7 Conclusion
We observe an anomalous phenomenon where
LLMs able to solve hard problems paradoxically
fail at easier ones. To evaluate this hard-to-easy in-
consistency, we construct ConsisEval by automatic
generation and human annotation. Furthermore,
we propose consistency score to measure this in-
consistency quantitatively and relative consistency
score to analyze the potential for improvement in
consistency. Based on our dataset and metrics, we
conduct comprehensive experiments on numerous
existing models, finding that there are exceptions
where some powerful models demonstrate poor
consistency, though models with stronger capabili-
ties usually exhibit higher consistency. Case study
shows though state-of-the-art GPT-4 achieves the
highest CS of 92.2%, still suffers from inconsis-
tency due to distraction by redundant information,
misinterpretation of questions, etc. Besides, we
also find hard data benefits consistency for both
fine-tuning and ICL. Our benchmark and metrics
can facilitate research in consistency of LLMs, ulti-
mately paving the way for building more trustwor-
thy and reliable AI in the future.
8Limitations
Our evaluation requires repeated sampling for the
same question to estimate the probability, which
is more computationally expensive than traditional
non-probability evaluation. Our metric CS can only
reflect the overall consistency of a model and can
hardly identify to which types of problems it is
more inconsistent. We also find different models
behave inconsistently to totally different questions,
and identifying these questions for a given model
still requires human efforts in case studies.
Data contamination (or data leakage) (Magar and
Schwartz, 2022; Xu et al., 2024b) can affect our
evaluation. As detailedly discussed in Appendix F,
leakage of easy and hard data can lead to higher
and lower CS, respectively. Considering that easy
data are from public data and thereby suffer from
a higher risk of data leakage (e.g. Achiam et al.
(2023) reports 25% of HumanEval has been con-
taminated in their training data), model consistency
can be overrated.
Our evaluation does not include human results.
Theoretically, consistency of humans should equate
to 100%, yet incorrectness on easy questions
caused by carelessness can diminish this consis-
tency. Human evaluation results can vary due to
the variance of carelessness among individuals; be-
sides, having humans complete all questions in
ConsisEval is exceedingly time-consuming. There-
fore, determining the human level consistency for
LLMs as a reference needs more discussion and
exploration.
Our benchmark focuses on evaluating the hard-
to-easy consistency of LLMs but does not inves-
tigate the underlying reasons and how inconsis-
tency comes into being. The knowledge acquire-
ment process of humans and LLMs is totally dif-
ferent, and humans are inherently consistent rea-
soners yet LLMs are not. Will pre-training and
fine-tuning paradigm of LLMs necessarily lead to
inconsistency? Further discussion and exploration
is needed. Though our preliminary findings suggest
that hard training data can mitigate this inconsis-
tency, how to solve this inconsistency problem is
still unknown, and we leave it to future work.
Ethical Considerations
The easy part of our benchmark originates from
publicly available datasets, which is allowed for
research usage. Our dataset encompasses code,
maths, and instruction-following domains, whichare safe and can hardly be utilized in harmful ways.
Besides, the evaluated LLMs are all publicly avail-
able by either parameters or API calls. Therefore,
we do not anticipate any ethical concerns in our
research.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
AI@Meta. 2024. Llama 3 model card.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Baichuan. 2023. Baichuan 2: Open large-scale lan-
guage models. arXiv preprint arXiv:2309.10305 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner,
Bowen Baker, Leo Gao, Leopold Aschenbrenner,
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
Leike, et al. 2023. Weak-to-strong generalization:
Eliciting strong capabilities with weak supervision.
arXiv preprint arXiv:2312.09390 .
Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,
Jiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai,
Tongliang Li, Qiyao Peng, and Zhoujun Li. 2024.
xcot: Cross-lingual instruction tuning for cross-
lingual chain-of-thought reasoning. arXiv preprint
arXiv:2401.07037 , abs/2401.07037.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021a. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021b. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
DeepSeek-AI. 2024. Deepseek llm: Scaling open-
source language models with longtermism. arXiv
preprint arXiv:2401.02954 .
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2022. A survey on in-context learning.
arXiv preprint arXiv:2301.00234 .
9Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
2023. Palm-e: An embodied multimodal language
model. arXiv preprint arXiv:2303.03378 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335,
Dublin, Ireland. Association for Computational Lin-
guistics.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-
lasha Ravichander, Eduard Hovy, Hinrich Schütze,
and Yoav Goldberg. 2021. Measuring and improving
consistency in pretrained language models. Transac-
tions of the Association for Computational Linguis-
tics, 9:1012–1031.
Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,
Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong
Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao
Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, and Zhou-
jun Li. 2023. OWL: A large language model for IT
operations. CoRR , abs/2309.09298.
Peter Hase, Mohit Bansal, Peter Clark, and Sarah
Wiegreffe. 2024. The unreasonable effectiveness
of easy training data for hard tasks. arXiv preprint
arXiv:2401.06751 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia
Bruni. 2020. Compositionality decomposed: How
do neural networks generalise? J. Artif. Intell. Res. ,
67:757–795.
Myeongjun Jang, Deuk Sin Kwon, and Thomas
Lukasiewicz. 2022. BECEL: Benchmark for con-
sistency evaluation of language models. In Proceed-
ings of the 29th International Conference on Com-
putational Linguistics , pages 3680–3696, Gyeongju,
Republic of Korea. International Committee on Com-
putational Linguistics.
Myeongjun Jang and Thomas Lukasiewicz. 2023. Con-
sistency analysis of ChatGPT. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 15970–15985, Singa-
pore. Association for Computational Linguistics.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei,
Jinfeng Yi, and Bowen Zhou. 2023a. Trustworthy
ai: From principles to practices. ACM Computing
Surveys , 55(9):1–46.
Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tat-
sunori Hashimoto, and Percy Liang. 2023b. Bench-
marking and improving generator-validator con-
sistency of language models. arXiv preprint
arXiv:2310.01846 .
Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong
Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang,
Songyang Zhang, Dahua Lin, and Kai Chen. 2024.
Mathbench: Evaluating the theory and application
proficiency of llms with a hierarchical mathematics
benchmark. arXiv preprint arXiv:2405.12209 .
Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying
Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,
Muhammad Faaiz Taufiq, and Hang Li. 2023. Trust-
worthy llms: a survey and guideline for evaluating
large language models’ alignment. arXiv preprint
arXiv:2308.05374 .
Inbal Magar and Roy Schwartz. 2022. Data contamina-
tion: From memorization to exploitation. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers) , pages 157–165, Dublin, Ireland. Association
for Computational Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Harsh Raj, Vipul Gupta, Domenic Rosati, and Sub-
habrata Majumdar. 2023. Semantic consistency for
assuring reliability of large language models. arXiv
preprint arXiv:2308.09138 .
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu,
Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan
Lyu, Yixuan Zhang, Xiner Li, et al. 2024a. Trustllm:
Trustworthiness in large language models. arXiv
preprint arXiv:2401.05561 .
Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang
Liu, Yiming Yang, Sean Welleck, and Chuang Gan.
2024b. Easy-to-hard generalization: Scalable align-
ment beyond human supervision. arXiv preprint
arXiv:2403.09472 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
10Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie,
Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi
Xiong, Ritik Dutta, Rylan Schaeffer, Sang Truong,
Simran Arora, Mantas Mazeika, Dan Hendrycks, Zi-
nan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and
Bo Li. 2023a. Decodingtrust: A comprehensive as-
sessment of trustworthiness in gpt models. In Ad-
vances in Neural Information Processing Systems ,
volume 36, pages 31232–31339. Curran Associates,
Inc.
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,
Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and
Zhifang Sui. 2023b. Large language models are not
fair evaluators. arXiv preprint arXiv:2305.17926 .
Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang,
and Dinggang Shen. 2023c. Chatcad: Interac-
tive computer-aided diagnosis on medical image
using large language models. arXiv preprint
arXiv:2302.07257 .
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Liang Xu, Hang Xue, Lei Zhu, and Kangkang Zhao.
2024a. Superclue-math6: Graded multi-step math
reasoning benchmark for llms in chinese. arXiv
preprint arXiv:2401.11819 .
Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu.
2024b. Benchmarking benchmark leakage in large
language models. arXiv preprint arXiv:2404.18824 .
Ziyao Xu and Houfeng Wang. 2024. Spor: A compre-
hensive and practical evaluation method for composi-
tional generalization in data-to-text generation. arXiv
preprint arXiv:2405.10650 .
Zhe Yang, Damai Dai, Peiyi Wang, and Zhifang Sui.
2023. Not all demonstration examples are equally
beneficial: Reweighting demonstration examples for
in-context learning. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
13209–13221, Singapore. Association for Computa-
tional Linguistics.
Alex Young, Bei Chen, Chao Li, Chengen Huang,
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652 .
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,
and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
arXiv:2311.07911 .
11Appendix
A Mathematical Notations
This section shows all of the mathematical nota-
tions used in this paper. If you forget the meaning
of any notation, please refer to Table 4. We lever-
ageˆto symbolize estimates (e.g. ˆP(ai)represents
the estimate of the true value P(ai)). For sim-
plicity, we only show true values in Table 4, and
estimates are omitted.
B Derivation of Consistency Score
§3.2 only shows the result for CS, and we show the
derivation process of CS in this section. We have:
CS=P(a|b)
=P(a, b)
P(b)
=P
i=1,...,NP(ai, bi)/NP
i=1,...,NP(bi)/N
=P
i=1,...,NP(ai)P(bi)P
i=1,...,NP(bi)(7)
It is worth noting that for a given question pair
(ai, bi), the probability that a model correctly an-
swers ai, bi(i.e.P(ai)andP(bi)) are unknown
constants. When sampling answers, whether the
model answers one question correctly does not af-
fect answering the other, which allows us to deduce
that the simultaneous probability of correctly an-
swering both is P(ai, bi) =P(ai)P(bi). However,
this does not hold for random questions aandb, as
P(a, b)̸=P(a)P(b).
The above derivation process does not specify
how the random questions aandbare obtained.
We provide a more rigorous proof by defining the
random process through which aandbare selected,
as well as the random variables P(a)andP(b).
Firstly, we outline the following stochastic process:
Randomly sampling a pair of questions ( a, b)
fromA⊙Bwith equal probability.
Based on this stochastic process, we define the
random variables P(a)andP(b)as the probabil-
ities of the model correctly answering aandbre-
spectively, through a single temperature-based sam-
pling. It is noteworthy that P(a), P(b)are constant
in the previous derivation, but here we treat them
as random variables. Initially, the prior probabil-
ity of selecting biin the above random process isP(select b i) =1
N. Upon introducing the condition
that model answers bcorrectly, the posterior prob-
ability of bibeing selected in the random process
becomes P(select b i) =P(bi)P
i=1,...,NP(bi). leverag-
ing this posterior probability for the calculation of
expected values, we have:
CS
=E[P(a|b)]
=X
i=1,...,NP(ai|bi)P(select b i)
=X
i=1,...,NP(ai, bi)
P(bi)P(bi)P
j=1,...,NP(bj)
=X
i=1,...,NP(ai)P(bi)P
j=1,...,NP(bj)
=P
i=1,...,NP(ai)P(bi)P
i=1,...,NP(bi)(8)
C Formal Definition of Models with
Similar Capabilities
For an evaluated model M0and a question
pair(ai, bi)from dataset A⊙B, the probabil-
ity of M0answer ai, bicorrectly through a sin-
gle temperature-based sampling is denoted as
PM0(ai), PM0(bi). We define a model set Ω =
{M0, M1, ...}in which models have similar capa-
bilities (but consistency is not necessarily similar).
For any Mj∈Ω, we have:
1.PM0(bi) =PMj(bi)for any i∈ {1, ..., N}
2.Mset{PM0(a0), ..., P M0(aN)}
=Mset{PMj(a0), ..., P Mj(aN)},
where Mset denotes multiset (a.k.a. bag), a
generalization of a set where repetition of elements
matters.
In this scope, we define models with similar abil-
ities as models whose correct probability on each
datum in Bare exactly the same and multisets of
correct probability on each datum in Aare iden-
tical to each other. The fact that different models
fromΩdemonstrate the same accuracy on A(and
B) intuitively makes one feel that these models
have similar capabilities. It is worth noting that
onlyM0is an existing model in the real world; all
other models in Ωare hypothetical for analysis of
consistency score boundaries.
12D Boundaries for Consistency Score
This section discusses the derivation of boundaries
for consistency score utilized in §3.3, and we show
both strict mathematical boundaries and tighter
heuristic boundaries.
D.1 Mathematical Boundaries
Without any loss of generality, we assume that
P(b0), ..., P (bN)is an ascending sequence (oth-
erwise, the order of elements can be adjusted
properly to meet this condition). After arrang-
ing the sequence P(a0), ..., P (aN)in ascend-
ing order, we denote the resulting sequence as
P(a(0)), ..., P (a(N)). According to the rearrange-
ment inequality, we have:
P
i=1,...,NP(a(N+1−i))P(bi)P
i=1,...,NP(bi)
≤P
i=1,...,NP(ai)P(bi)P
i=1,...,NP(bi)
≤P
i=1,...,NP(a(i))P(bi)P
i=1,...,NP(bi)(9)
From this inequality, we obtain the mathemat-
ical upper bound CSupp=P
i=1,...,NP(a(i))P(bi)P
i=1,...,NP(bi)
and mathematical lower bound CSlow =P
i=1,...,NP(a(N+1−i))P(bi)P
i=1,...,NP(bi).
D.2 Heuristic Boundaries
Although the aforementioned boundaries are math-
ematically rigorous, they are too loose, as the
lower bound sometimes approaches 0 and the upper
bound approaches 1 in the experiments. Empiri-
cally, CS lies within a narrower interval. To find
more precise boundaries, we leverage two heuristic
assumptions:
Lower Bound Heuristic: For the most inconsis-
tent model, probabilities of correctly answering
easy and hard questions P(a)andP(b)are
independent (instead of negatively correlated).
Upper Bound Heuristic: For the most consistent
model, the difference in probabilities of correctly
answering easy and hard questions is directly
proportional to the degree of increased difficulty
level.
These two hypotheses specify the behavior of
the model of best and worst consistency. We as-
sume that for a model of worst consistency, theremight be independence between correctly answer-
ing easy and hard questions, rather than a negative
correlation where an increased probability of cor-
rectly answering hard questions leads to a lower
probability of correctly answering easy questions.
Conversely, for a model with best consistency, the
probability of correctly answering easy and hard
questions is entirely dependent on the difficulty
level of the questions. Thus, the difference in prob-
ability between correctly answering easy and hard
questions, P(ai)−P(bi), is solely reliant on the
gradient of difficulty from aitobi. When construct-
ing our dataset, it’s almost impossible to ensure that
eachaiscales up in difficulty uniformly to obtain
bi; therefore, we hypothesize that the difficulty scal-
ing from aitobifollows a normal distribution (i.e.
(P(a)−P(b))∼N(µ, σ)).
Based on the Lower Bound Heuristic, we have a
tighter heuristic lower bound:
CSlow=P(a|b) =P(a, b)
P(b)
=P(a)P(b)
p(b)=P(a)
=Σi=1,...,NP(ai)
N(10)
Based on the Upper Bound Heuristic, we have
P(ai)−P(bi) =µ+ϵiσ, where ϵiis a random
variable that follows a standard normal distribution.
The maximum likelihood estimation of µ, σ is:
ˆµ=Σi=1,...,N(P(ai)−P(bi))
N,
ˆσ=r
Σi=1,...,N(P(ai)−P(bi)−ˆµ)2
N(11)
Substitute actual values µ, σ with estimated ones
ˆµ,ˆσ, then we have the theoretical value of P(ai)
in a consistent model: P(ai) =P(bi) + ˆµ+ϵiˆσ.
Empirically, the value of σdoes not affect final
results if averaging on multiple sampling of ϵ, so
we directly let σ= 0. Then by substituting the
theoretical values of P(ai)in consistent model for
the true values of P(ai)used in calculation of CS ,
we can obtain the heuristic upper bound as follows:
CSupp=P
i=1,...,N(P(bi) + ˆµ)P(bi)P
i=1,...,NP(bi)(12)
E Probability Estimation
This section shows the derivation of the maximum
likelihood estimate of P(ai)in Multiple Sampling
13Estimation (§E.1) and Early Stopping Estimation
(§E.2), respectively. Besides, we also show the
pseudo-code and more discussion about Early Stop-
ping Estimation in §E.3
E.1 Multiple Sampling Estimation
For problem ai, we sample answers mtimes in-
dependently to obtain a sequence a1
i, a2
i, ..., am
i.
Letaj
i= 1 if the model generates a correct an-
swer on the jth sampling; otherwise, aj
i= 0. In
this case, aj
ifollows a Bernoulli distribution. Let
k=P
j=1,...,maj
i, we have the likelihood func-
tion:
L(P(ai);k) =mY
j=1P(ai)aj
i(1−P(ai))1−aj
i
=P(ai)k(1−P(ai))m−k,
(13)
the derivative of the likelihood function:
∂L(P(ai);k)
∂P(ai)
=kP(ai)k−1(1−P(ai))m−k
−(m−k)P(ai)k(1−P(ai))m−k−1
∝k(1−P(ai))−(m−k)P(ai)
∝k−mP(ai)(14)
L(P(ai);k)is monotonically increasing when
P(ai)∈[0,k
m]and monotonically decreasing
when P(ai)∈[k
m,1]. When P(ai) =k
m, it max-
imizes the likelihood function, so the maximum
likelihood estimate of P(ai)is:
ˆP(ai) =k
m=P
j=1,...,maj
i
m(15)
E.2 Early Stopping Estimation
In Early Stopping Estimation, the minimum and
the maximum number of sampling times kminand
kmaxare set as hyper-parameters for a given ques-
tionai. Initially, kminanswers are sampled, and
the sampling process will be terminated if at least
one correct answer exists in these kminanswers;
otherwise, answers will be sampled one by one
until a correct answer appears for the first time.
Besides, the sampling procedure will be forcibly
terminated if a correct answer still does not emerge
after sampling kmaxanswers. Let P(k, kc)be the
probability of sampling kanswers in total in which
kcanswers are correct, and let L(P(ai);k, kc)be
the likelihood function. The discussion is divided
into the following three cases based on the different
values of k:Case 1: k=kmin
We have the likelihood function:
L(P(ai);k, kc) =P(k, kc)
=kmin
kc
P(ai)kc(1−P(ai))kmin−kc,(16)
the derivative of the likelihood function:
∂L(P(ai);k, kc)
∂P(ai)
=kmin
kc
[kcP(ai)kc−1(1−P(ai))kmin−kc
−(kmin−kc)P(ai)kc(1−P(ai))kmin−kc−1]
∝kc(1−P(ai))−(kmin−kc)P(ai)
∝kc−kminP(ai)
(17)
L(P(ai);k, kc)is monotonically increasing
when P(ai)∈[0,kc
kmin]and monotonically de-
creasing when P(ai)∈[kc
kmin,1]. When P(ai) =
kc
kmin, it maximizes the likelihood function, so
the maximum likelihood estimate of P(ai)is:
ˆP(ai) =kc
kmin
Case 2: kmin< k < k max
We have the likelihood function:
L(P(ai);k, kc) =P(k, kc)
= (1−P(ai))k−1P(ai),(18)
the derivative of the likelihood function:
∂L(P(ai);k, kc)
∂P(ai)
=−(k−1)(1−P(ai))k−2P(ai)
+ (1−P(ai))k−1
∝ −(k−1)P(ai) + 1−P(ai)
∝1−kP(ai)(19)
L(P(ai);k, kc)is monotonically increasing
when P(ai)∈[0,1
k]and monotonically decreasing
when P(ai)∈[1
k,1]. When P(ai) =1
k, it max-
imizes the likelihood function, so the maximum
likelihood estimate of P(ai)is:ˆP(ai) =1
k
Case 3: k=kmaxWe have the likelihood func-
tion:
L(P(ai);k, kc) =P(k, kc)
= (1−P(ai))kmax−I(kc̸=0)P(ai)I(kc̸=0),(20)
where Idenoted indicator function. If kc̸= 0, the
likelihood function is the same as Case 2, we have
14ˆP(ai) =1
kmaxby the same reasoning. If kc= 0,
the likelihood function is monotonically decreasing
on[0,1], so the maximum likelihood estimate of
P(ai)is:ˆP(ai) = 0 .
To summarize, the maximum likelihood estimate
ofP(ai)is shown as below:
1. ifk=kmin, then ˆP(ai) =kc
kmin
2. ifkmin< k < k max, then ˆP(ai) =1
k
3. ifk=kmax, then ˆP(ai) =I(kc̸=0)
kmax
The above three cases can be formulated as:
ˆP(ai) =kc
k(21)
E.3 More Details about Early Stopping
Estimation
The pseudo-code for Early Stopping Estimation is
shown in Algorithm 1. if we set kmaxequal to the
number of sampling min Multiple Sampling Esti-
mation, in the worst-case scenario, the number of
sampling of Early Stopping Estimation could equal
that of Multiple Sampling Estimation, theoretically.
However, empirical results suggest that, due to the
high accuracy of these closed-source models, the
actual number of samples required with early stop-
ping is typically low. While introducing an early
stopping strategy might slightly reduce the accu-
racy of estimation, the reduction in the number of
API calls required makes it a worthwhile trade-off.
Algorithm 1: Early Stopping Estimation
input : a question ai; function to generate an answer
by sampling generate ();
minimum number of samples kmin;
maximum number of samples kmax
output : estimated probability ˆP(ai)of model answer
aicorrectly through a single sampling
1Initialize answer _list←[ ]
2forj←1tokmindo
3 answer ←generate( ai)
4 answer _list.append( answer )
5ifnotexist_correct( answer _list, a i)then
6 forj←kmin+ 1tokmax do
7 answer ←generate( a)
8 answer _list.append( answer )
9 ifanswer is correct then
10 Break
11correct _num←CountCorrect (answer _list)
12ˆP(ai)←correct _num/Len (answer _list)
13Return ˆP(ai)Multiple Sampling Estimation v.s. Early Stop-
ping Estimation If we sample fewer times in
Multiple Sampling Estimation, resulting in a
roughly equal total number of samples across the
entire dataset for both methods, which method
yields a more accurate estimation? For questions
with a low probability of being answered correctly
(near 0%), a large number of samples are required
to obtain a correct answer and thus accurately es-
timate this probability; otherwise, there is a high
risk of erroneously deeming the probability to be
zero. On the contrary, for questions that models
have a high probability of answering correctly (near
100%), almost all samples will be correct, and
therefore, fewer samples are needed to accurately
estimate the probability. The Early Stopping Es-
timation method adapts the number of sampling
times dynamically for different questions, mak-
ing better use of each sampling opportunity com-
pared to the Multiple Sampling Estimation. Con-
sequently, it achieves higher precision in its final
estimates when the sampling times are limited.
F Impact of Data Leakage
Data leakage can affect our evaluation. We find
leakage of easy and hard data can lead to higher
and lower CS, respectively. We analyze data leak-
ing on datum ai(orbi) by modeling the leaking
as an increment in probability P(ai)(orP(bi)).
For example, if aiis not leaked, model answers
it correctly with probability P(ai); after aiis
leaked, model answers it correctly with higher
probability P(ai) + ∆ P(aj). The original CS isP
i=1,...,NP(ai)P(bi)P
i=1,...,NP(bi), and we numerically analyze
the change of CS after data leakage.
F.1 Leakage of Easy Data
After leakage on an easy datum aj, the new CS
after leakage is :
CSleak=P
i=1,...,NP(ai)P(bi) + ∆ P(aj)P(bj)P
i=1,...,NP(bi)
=CS+∆P(aj)P(bj)P
i=1,...,NP(bi)
> CS
(22)
So leakage of easy data will lead to a higher CS.
15F.2 Leakage of Hard Data
After leakage on a hard datum bj, the new CS after
leakage is :
CSleak=P
i=1,...,NP(ai)P(bi) +P(aj)∆P(bj)P
i=1,...,NP(bi) + ∆ P(bj)
(23)
IfP(aj)∆P(bj)
∆P(bj)=P(aj)> CS ,CSleak>
CS;IfP(aj)∆P(bj)
∆P(bj)=P(aj)< CS ,CSleak<
CS. The expected value of P(aj)is the accuracy
on easy data, so we have E(P(aj))< CS , and
CSleak< CS on average. So leakage of hard data
will lead to a lower CS on average.
G More Details and Results for
Experiments
We show more implementation details and results
for main experiments in §4.
G.1 Implement Experiment Details
For small open-source models with roughly 7B or
13B parameters, we employ the Multiple Sampling
Estimation and independently sample 20 answers
for each question. As for the large models with
around 70B parameters and closed-source models,
we utilize the Early Stopping Estimation to reduce
computational costs and API calls, and we set the
minimum number of samples at kmin= 3and the
maximum at kmax= 20 . For each small open-
source model (7B or 13B), we run the experiments
on a single Nvidia A100 80G GPU; for each large
model (70B), experiments are conducted on three
Nvidia A100 80G GPUs. All of the open-source
models are acquired from Huggingface5, and we
utilize the default sampling hyper-parameters (e.g.
temperature, top-p) released by model developers.
All evaluations are under zero-shot setting: for
mathematics and instruction-following data, ques-
tions as fed into LLMs directly; code data are trans-
formed into instruction format6before inputted
into models.
G.2 Correlation between Capability and
Consistency
We find there is a strong correlation between capa-
bility and consistency of LLM in all of our evalu-
ated domains. Taking code domain as an example,
5https://huggingface.co/
6https://huggingface.co/datasets/codeparrot/
instructhumaneval
/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013
/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000003/uni00000024/uni00000046/uni00000046/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000026/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002f/uni0000002f/uni00000030/uni00000056
/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002f/uni0000004c/uni00000051/uni00000048Figure 8: Correlation of capability and consistency.
Kendall’s coefficient of correlation between accu-
racy on hard set and CS of all evaluated LLMs on
code domain is 0.801, and the linear regression line
is shown in Figure 8 (each dot represents an LLM).
G.3 Full Experiment Results on Relative
Consistency Score
Due to space limitation, §4 only shows experiment
results on RCS in code domain. We show full
experiment results in Table 3.
H Metric Convergence
The calculation of our evaluation metric consis-
tency score (CS) and relative consistency score
(RCS) relies on repeated sampling for a given ques-
tion. We show the value change and variance of
these metrics as the increase in sampling times. As
the convergence results for Llama3-8B-Instruct on
mathematics domain shown in Figure 9, CS con-
verges faster than RCS and achieves a stable value
at about 5 samples. The value of RCS converges
relatively slower and becomes stable after about 15
samples.
We also explore leveraging consistent rate as
an evaluation metric. Taking the case where the
probability of answering an easy question cor-
rectly is larger than that of the hard question
as a consistent case, we have consistent rate =
number of consistent cases
number of all cases∗100% . However, we find
that for the case where the probability of answering
easy and hard questions correctly is close, reach-
ing a convergent result requires too many times of
sampling. We abandon this metric due to its high
computational cost.
16MoldesCode Instruction following Maths
low CS upp RCS low CS upp RCS low CS upper RCS
GPT-4 Turbo 85.5 88.1 93.0 34.8 84.2 91.8 93.1 85.3 96.2 96.8 97.2 54.4
GPT-3.5 Turbo 71.4 81.2 88.8 56.1 76.1 88.6 91.7 80.5 86.9 90.7 96.2 40.8
Claude-3 Opus 81.1 85.5 93.6 35.1 87.7 93.4 95.7 70.7 96.5 96.5 98.1 0.6
Qwen Max 75.0 82.4 93.4 40.5 74.3 89.6 94.3 76.7 95.2 96.8 98.2 51.9
Llama3-70B-Instruct 73.9 84.3 94.6 50.2 86.7 94.4 95.1 90.7 94.9 96.9 98.0 64.1
Llama2-70B-Chat 34.5 74.7 83.8 81.5 56.6 81.0 91.6 69.7 70.5 83.7 90.3 66.9
Qwen1.5-72B-Chat 62.3 79.4 91.3 58.7 56.5 87.3 90.7 89.9 90.6 93.6 94.0 87.2
Deepseek-67B-Chat 68.6 77.9 88.1 47.6 52.5 83.8 88.1 87.8 90.2 94.8 98.8 54.0
Llama2-13B-Chat 20.2 61.9 84.2 65.1 48.3 84.2 89.2 87.7 48.6 67.2 76.1 67.4
Qwen1.5-14B-Chat 51.4 74.6 86.0 67.2 55.4 83.6 90.8 79.6 82.6 90.7 92.2 84.7
Baichuan2-13B-Chat 21.5 59.1 73.4 72.5 31.0 63.3 75.2 73.2 48.6 65.8 78.1 58.3
Llama3-8B-Instruct 53.6 71.4 83.4 59.7 78.5 87.9 91.8 70.7 77.8 87.4 89.2 84.6
Llama2-7B-Chat 14.9 63.1 79.6 74.5 43.7 75.6 86.2 75.0 34.3 57.9 76.5 55.9
Qwen1.5-7B-Chat 40.9 68.4 81.9 66.9 47.2 82.5 87.9 86.7 68.6 83.6 88.8 74.3
ChatGLM3_6B 50.8 68.5 81.6 57.4 36.6 64.7 75.3 72.5 64.4 83.8 86.2 89.0
Deepseek-7B-Chat 40.3 62.6 75.9 62.6 47.5 71.0 82.3 67.7 69.0 84.8 88.6 80.8
Mistral-7B-Instruct 28.4 57.0 69.7 69.2 60.8 84.3 88.3 85.3 51.8 67.4 75.3 66.5
Yi-6B-Chat 13.2 49.3 70.5 63.0 37.4 76.0 80.2 90.1 50.9 69.7 76.9 72.4
Baichuan2-7B-Chat 12.4 43.0 54.5 72.7 29.9 60.0 69.8 75.5 28.4 50.1 56.6 76.9
Table 3: Relative consistency results. A variety of LLMs are evaluated on code, instruction-following, and maths
domains. On each domain, we report consistency score ( CS), lower and upper bounds of CS (denoted as lowand
upp).
I Case Study
We show inconsistent cases of GPT-4 in Table
6,7,8,9,10. More analyses are shown in §5.3.
J Prompts for Data Generation
The prompts for data generation on code, maths
and instruction-following domains are shown in
Figure 10, 11, 12 respectively.
K Example Data
We show example data in Table 5.
17Notations Meanings
A, B easy question set and hard question set
A⊙B dataset with pairwise easy and hard questions
N number of data in A⊙B(also for AorB)
ai, bi thei−theasy question and the i−thhard question (they are a pair)
P(ai)(orP(bi)) the probability of model answer ai(orbi) correctly through a single temperature-based sampling
(a, b) a pair of questions sampled from A⊙Bwith equal probability.
P(a)(orP(b)) the probability of model answer a(orb) correctly through a single temperature-based sampling
P(a|b) the probability of model correctly answering agiven that it has answered bcorrectly through a
single temperature-based sampling
P(a, b) the probability of model correctly answering aandbcorrectly through a single temperature-based
sampling
E[∗] expected values
M0 a language model to be tested
Ω a set of models with similar abilities with M0
PM0(ai)(orPM0(bi)) the probability of model M0answer ai(orbi) correctly through a single temperature-based
sampling
Mset{} multiset (a.k.a. bag), a generalization of a set where repetition of elements matter
upp CS (Ω),low CS (Ω) the upper and lower bounds of consistency scores for all models in Ω
ϵi random variable following the standard normal distribution
µ, σ mean and standard variance for (P(ai)−P(bi) )
m sampling times in Multiple Sampling Estimation
kmin, kmax, k, k c minimum sampling times, maximum sampling times, actual sampling times and correct answer
number in early Stopping Estimation
L likelihood function
Table 4: mathematical notations and their meanings.
1234567891011121314151617181920
Sampling Times86.587.087.588.088.589.0(Relative) Consistency Score (%)CS
1234567891011121314151617181920
Sampling Times4050607080(Relative) Consistency Score (%)RCS
Figure 9: Convergence of metrics with the increase of sampling times.
18Mathematics
easyQuestion: George has 45% more pears than bananas. If George has 200 bananas, how many fruits does
George have?
Answer: To find out the number of pears George has, we must first calculate 45% of the bananas, then
add that to the number of bananas he has. If George has 200 bananas, we will find 45% of this number
like so:0.45 * 200 = 90. This means George has 90 more pears than bananas. Now, add the extra pears to
the 200 bananas to find the total number of pears: 200 + 90 = 290 pears. Now, to find the total number of
fruits, add the number of bananas to the number of pears: Number of bananas (200) + Number of pears
(290) = Total fruits. 200 + 290 = 490 fruits. Therefore, George has 490 fruits in total.
Answer (Number Only): 490hardQuestion: George has 45% more pears than bananas. If George has 200 bananas, 270 oranges, and 150
apples, how much are George’s fruits worth if pears cost 3 dollars each, bananas 2 dollars each, oranges
50 cents each, and apples 1 dollar each?
Answer: First, let’s calculate how many pears George has. Since he has 45% more pears than bananas,
and he has 200 bananas, we calculate the number of pears as follows: Number of pears = Number of
bananas + 0.45 * Number of bananas = 200 + 0.45 * 200 = 200 + 90 = 290. Next, let’s calculate the total
cost of each type of fruit. Bananas (200 bananas at $2 each): 200 * 2 = $400. Pears (290 pears at $3
each): 290 * 3 = $870. Oranges (270 oranges at $0.50 each): 270 * 0.50 = $135. Apples (150 apples at
$1 each): 150 * 1 = $150. Finally, to find the total value of all the fruits, we sum these amounts: $400 +
$870 + $135 + $150 = $1555. Therefore, George’s fruits are worth $1555 in total.
Answer (Number Only): 1555Instruction
easyQuestion:
Write a riddle for kids about auspices but make sure you don’t use any commas.
Constraint Type List: ["punctuation:no_comma"]
Constraint Kwargs: [{}]hardQuestion:
Write a riddle for kids about auspices but make sure you don’t use any commas. Also, the riddle must be
at least 6 sentences long.
Constraint Type List: ["punctuation:no_comma", "length_constraints:number_sentences"]
Constraint Kwargs: [{}, {"relation": "at least", "num_sentences": 6}]
easy hardCodeQuestion:
def strlen(string: str) -> int:
”’
Return length of given string
> > > strlen(”) 0
> > > strlen(’abc’) 3
”’
Answer:
return len(string)
Check Function:
def check(candidate):
assert candidate(”) == 0
assert candidate(’x’) == 1
assert candidate(’asdasnakj’) == 9Question:
def strlen_add(string1: str, string2: str) -> str:
”’
Return length sum of two given strings
> > > strlen_add(’abc’, ’d’) 4
”’
Answer:
return len(string1 + string2)
Check Function:
def check(candidate):
assert candidate(’abc’, ’d’) == 4
assert candidate(”, ’z’) == 1
assert candidate(’x’, ’y’) == 2
assert candidate(’hello’, ’!’) == 6
Table 5: Example data from ConsisEval. For each domain (mathematics, instruction-following, and code), a pairwise
example datum is shown.
19Easy HardQuestionsA team of 4 painters worked on a mansion for 3/8ths of
a day every day for 3 weeks. How many hours of work
did each painter put in?
Standard Answer: 189A team of 4 painters worked on a mansion for 3/8ths of
a day every day for 3 weeks. Each painter earns 80 yuan
per hour. After working for 3 weeks, 70000 yuan were
allocated for the wages. How much money will remain
after paying the wages of the 4 painters?
Standard Answer: 9,520Model AnswersFirst, let’s figure out how many days are in 3 weeks.
There are 7 days in a week, so 3 weeks have 3×7 = 21
days.
Each day, the team worked for 3/8of a day. There are
24 hours in a full day, so 3/8of a day in hours is:
24×3
8= 3×3 = 9 hours
If they worked 9 hours each day, over 21 days, the total
hours worked by the team is:
9hours/day ×21days= 189 hours
Since the total hours worked by the team is 189 and the
team consists of 4 painters, the hours worked by each
painter is:
189hours÷4 = 47 .25hours
So, each painter put in 47.25 hours of work.First, let’s calculate the total hours worked by each
painter:
1. A day is considered to have 24 hours, so 3/8ths of a
day is:3
8×24hours. 2.3
8of 24 hours = 9 hours per
day.
Each painter works for 9 hours a day.
Now, let’s find out the total days worked in 3 weeks.
Since there are 7 days in a week:
3 weeks ×7 days/week = 21 days.
Each painter worked for 9 hours a day for 21 days, so
their total hours worked are:
9 hours/day ×21 days = 189 hours in total for 3 weeks.
Since there are 4 painters:
189hours×4 = 756 hours worked in total by all
painters.
Given that each painter earns 80 yuan per hour, the total
wages for all painters are:
756 hours ×80 yuan/hour = 60,480 yuan.
Finally, subtracting the total wages from the initially
allocated funds gives the remaining amount:
70,000 yuan - 60,480 yuan = 9,520 yuan.
Therefore, 9,520 yuan will remain after paying the wages
of the 4 painters.
Table 6: An inconsistency case for GPT-4. Red texts denote wrong reasoning steps. GPT-4 is distracted by the
redundant condition of "4 painters", leading it to perform an additional unnecessary calculation after arriving at the
correct answer, ultimately resulting in the wrong final answer.
20Easy HardQuestionsHallie had dance practice for 1 hour on Tuesdays and
2 hours on Thursdays. On Saturdays, she had dance
practice that lasted twice as long as Tuesday’s night class.
How many hours a week did she have dance practice?
Standard Answer: 5Hallie had dance practice for 1 hour on Tuesdays and
2 hours on Thursdays. On Saturdays, she had dance
practice that lasted twice as long as Tuesday’s night
class. On Sundays, her dance practice was 3 hours longer
than Saturday’s. Mary is also a dance enthusiast, and
her weekly dance practice is 4 hours less than Hallie’s.
Alice’s weekly dance practice is 5 times the duration of
Hallie’s Thursday practice. How many more hours does
Alice practice per week than Mary?
Standard Answer: 4Model AnswersHallie has dance practice for 1 hour on Tuesdays.
On Thursdays, she has dance practice for 2 hours.
Since Saturday’s dance practice is twice as long as Tues-
day’s, it would be 2 hours * 2 = 4 hours.
Adding all the hours together, we have:
1 hour (Tuesday) + 2 hours (Thursday) + 4 hours (Satur-
day) = 7 hours
So, Hallie had dance practice for 7 hours a week.First, let’s calculate the total weekly practice duration
for Hallie.
Tuesday practice = 1 hour Thursday practice = 2 hours
Saturday practice = 2 * Tuesday = 2 * 1 hour = 2 hours
Sunday practice = Saturday practice + 3 hours = 2 hours
+ 3 hours = 5 hours
Total weekly practice for Hallie = Tuesday + Thursday
+ Saturday + Sunday = 1 hour + 2 hours + 2 hours + 5
hours = 10 hours
Mary’s weekly dance practice is 4 hours less than Hal-
lie’s: Mary’s weekly practice = Hallie’s weekly practice
- 4 hours = 10 hours - 4 hours = 6 hours
Alice’s weekly dance practice is 5 times the duration of
Hallie’s Thursday practice: Alice’s weekly practice = 5
* Thursday (of Hallie) = 5 * 2 hours = 10 hours
Comparing how many more hours Alice practices per
week than Mary: Alice’s weekly practice - Mary’s
weekly practice = 10 hours - 6 hours = 4 hours
Therefore, Alice practices 4 more hours per week than
Mary.
Table 7: An inconsistency case for GPT-4. Red texts denote wrong reasoning steps. Misapplication of data occurs
with GPT-4: Tuesday involves 1 hour of dancing, while GPT-4 mistakenly uses Thursday’s 2 hours in its calculation,
resulting in a wrong final answer.
Easy HardQuestionsJen decides to travel to 3 different countries. He has to
pay $400 for the supplies he needs, in total. The tickets
for travel cost, in total, 50% more than the supplies. How
much does travel cost?
Standard Answer: 1,000Jen decides to travel to 3 different countries. He has to
pay $400 for the supplies he needs, in total. The tickets
for travel cost, in total, 50% more than the supplies.
Additionally, the accommodation costs for the travel are
3 times the cost of the supplies. The total travel expenses
amount to one-quarter of Jen’s monthly salary. If Eily’s
monthly salary is $3200 higher than that of Jen, what is
Eily’s monthly salary?
Standard Answer: 12,000Model AnswersIf the supplies cost $400 in total and the tickets for travel
cost 50% more than the supplies, we calculate the total
cost of the tickets by adding 50% of $400 to the original
$400.
50% of $400 is 0.50 * $400 = $200.
So, the tickets cost $400 (the cost of supplies) + $200
(50% more than the supplies) = $600 in total.Let’s break this down step by step.
1. **Cost of Supplies**: Given as $400.
2. **Cost of Tickets**: Stated as 50% more than the
supplies, which is: Cost of Supplies ×150% = 400 ×
1.5 = $600 .
3. **Cost of Accommodation**: Three times the cost of
the supplies, which is: 3×Cost of Supplies = 3×400 =
$1200 .
4. **Total Travel Expenses**: Sum of Supplies, Tickets,
and Accommodation costs: 400+600+1200 = $2200 .
5. **Total Travel Expenses as a Quarter of Jen’s Monthly
Salary**: This means: Jen’s Monthly Salary = 2200 ×
4 = $8800 .
6. **Eily’s Monthly Salary** is $3200 more
than Jen’s Monthly Salary. Therefore, it’s:
Eily’s Monthly Salary = 8800 + 3200 = $12000 .
Table 8: An inconsistency case for GPT-4. Misunderstanding of the question leads to an error: the question askes
for the total cost of the trip, whereas GPT-4 only calculates the cost of travel tickets.
21Easy HardQuestionsMark plants a beanstalk below his second-story window,
which is 20 feet off the ground. The beanstalk doubles
its height every day. If the beanstalk starts out 4 feet
tall, how many days will it take to be taller than Mark’s
window?
Standard Answer: 3Mark plants a beanstalk below his second-story window,
which is 20 feet off the ground. The beanstalk doubles
its height every day. If the beanstalk starts out 4 feet tall,
on the third day after planting it, Mark cuts off the part
of the beanstalk that has grown above his window to sell.
The beanstalk sells for $3 per foot. The money Mark
makes from selling the beanstalk is one fourteenth of his
daily wage. How much does Mark earn per day?
Standard Answer: 504Model AnswersTo determine how many days it will take for the
beanstalk to be taller than Mark’s second-story window,
20 feet off the ground, we can follow the growth pattern
of the beanstalk, which doubles in height each day, start-
ing at 4 feet.
Day 1: 4 feet, Day 2: 4 feet ×2 = 8 feet, Day 3: 8 feet ×
2 = 16 feet, Day 4: 16 feet × 2 = 32 feet
On day 4, the beanstalk’s height becomes 32 feet, which
exceeds the 20-foot height of the window. Therefore,
it will take 4 days for the beanstalk to grow taller than
Mark’s second-story window.To solve the problem, let’s first find out how tall the
beanstalk is on the third day after it starts growing.
Starting height: 4 feet
Growth pattern: doubles height every day
1st day height: 4 feet * 2 = 8 feet, 2nd day height: 8 feet
* 2 = 16 feet, 3rd day height: 16 feet * 2 = 32 feet
By the third day, the beanstalk is 32 feet tall. Since
Mark’s window is 20 feet off the ground, the part of the
beanstalk that grows above the window is the difference
between its third-day height and the window’s height.
Height above the window: 32 feet - 20 feet = 12 feet
Mark cuts and sells this 12-foot section for $3 per foot.
Money made from selling the beanstalk: 12 feet *
$3/foot = $36
It is stated that the money Mark makes from selling
the beanstalk is one fourteenth of his daily wage. Let’s
represent his daily wage as W.
Therefore, according to the problem:
$36 = 1/14 * W
To find W, multiply both sides of the equation by 14:
W = $36 * 14 = $504
Mark earns $504 per day.
Table 9: An inconsistency case for GPT-4. Red texts denote wrong reasoning steps. GPT-4 encounters an off-by-one
error: the initial state should be recorded as Day 0, but GPT-4 starts recording from Day 1 instead.
22Easy HardQuestionsKenny is selling his Pokemon cards to buy a ticket to an
amusement park, which costs $100. He has a collection
of cards and plans to sell them for $1.5 each. He keeps
1/3 of them and gets to go to the amusement park with
$50 in spending cash. How many cards did he start with?
Standard Answer: 150Kenny is selling his Pokemon cards to buy a ticket to
an amusement park, which costs $100. He has a collec-
tion of cards and plans to sell them for $1.5 each. He
keeps 1/3 of them and, after buying the ticket, has $50
in spending money for the amusement park. His initial
number of cards is 5 times the number of cards Eily has.
John has 37 more cards than Eily. Mary has 4 times as
many cards as John. How many cards does Mary have?
Standard Answer: 268Model AnswersLet’s denote the original number of Pokemon cards that
Kenny has as X.
Kenny decides to sell his cards for $1.5 each. However,
he keeps one-third of them, which means he sells two-
thirds of his cards.
The number of cards he sells is 2/3 * X.
From the sale of his cards, he earns enough to buy the
amusement park ticket, which costs $100, and he has
$50 in spending cash. Therefore, the total amount he
earns from selling his cards is $100 for the ticket plus
$50 spending cash, which equals $150.
The amount he earns from selling the cards can be found
by multiplying the number of cards he sells (2/3 * X) by
the price per card ($1.5):
(2/3 * X) * $1.5 = $150
Let’s solve for X:
$1.5 * (2X/3) = $150
(3/2) * $1.5 * (2X/3) = $150 * (3/2)
$2.25 * X = $225
Divide both sides by $2.25 to find X:
X = $225 / $2.25 X = 100
Kenny started with 100 Pokemon cards.Let’s break down the problem step by step.
1. We know that Kenny sells his cards for $1.5 each and
wants to end up with $150 in total ($100 for the ticket
and $50 in spending money).
2. Let’s find out how many cards he sells. We take the
total money he wants to have, which is $150, and divide
it by the price per card: Total cards sold = total money
needed / price per card Total cards sold = $150 / $1.5
Total cards sold = 100 cards
3. Kenny keeps 1/3 of his cards and sells the rest, so 2/3
of his cards were sold. Let’s calculate the initial number
of cards he had: Let x be the number of cards Kenny
initially had. (2/3)x = 100 cards
To find x, we solve for x: x = (100 cards) / (2/3) x = (100
* 3) / 2 x = 300 / 2 x = 150 cards
So Kenny originally had 150 cards.
4. Kenny’s initial number of cards is 5 times the number
of cards Eily has. Let’s call the number of cards Eily has
E. Therefore: 5E = 150
Solving for E gives us: E = 150 / 5 E = 30 cards
Eily has 30 cards.
5. John has 37 more cards than Eily: John’s cards = E +
37 John’s cards = 30 + 37 John’s cards = 67 cards
6. Mary has 4 times as many cards as John: Mary’s cards
= 4 * John’s cards Mary’s cards = 4 * 67 Mary’s cards =
268 cards
So Mary has 268 cards.
Table 10: An inconsistency case for GPT-4. Red texts denote wrong reasoning steps. GPT-4 encounters a
computational error while solving equations.
23Prompt for Code Data Generation
#Instruction#:
I want you to act as a helpful assistant. Please help me modify some programming problems and make them harder. A
programming problem datum consists of three parts: #Problem#, #Answer#, and #Check Function#. The #Problem#
includes the name of a python function, function signature, and docstring; the #Answer# is the specific code that fulfills
the function’s purpose; in addition to that, there is a #Check Function# to verify whether the answer is correct. Please
follow the format of the following demonstrations, modify the original problem, and make it more challenging. To
ensure that there is a strict order in difficulty between the original problem and modified one, steps to solve the original
problem should be included in that of the modified problem. In other words, steps to solve the original problem is a
proper subset of that of the modified problem. Except the modified #Problem#, you should also provide #Answer# and
#Check Function# to the modified #Problem#.
#Demonstrations#:
<insert demonstrations>
The above are some demonstrations showing how to modify the original problems. Please follow their format and
modify the following problem:
#Problem#:
<insert the original problem>
#Answer#:
<insert the answer>
#Check Function#:
<insert the check function>
Please modified the above #Problem# and then provide #Answer# and #Check Function# to the modified #Problem#:
Figure 10: Our prompt fed to GPT-4 for code data generation. Our prompt is comprised of intention instruction,
demonstrations, and one datum to be modified. The instruction offers a clear description of the composition of the
datum and outlines the task we expect the model to accomplish. Demonstrations are provided as a format reference
for the model, followed by the original datum for the model to modify.
Prompt for Math Data Generation
#Instruction#:
I want you to act as a helpful assistant. Please help me modify some grade school math problems and make them
harder. A math problem datum consists of two parts: #Problem# and #Answer#. The #Problem# provides a background
description of a real-world mathematical problem, along with the conditions known and the unknown content to be
solved. There is a strict gurrantee that the unknown value can be derived through a few proper computational steps
based on konwn conditions. The #Answer# encompasses several computational steps based on logical reasoning with
the known conditions, culminating in the numerical value of the final answer. Please follow the format of the following
demonstrations, modify the original problem and make it more challenging. To ensure that there is a strict order in
difficulty between the original problem and modified one, steps to solve the original should be included in that of the
modified problem. In other words, steps to solve the original problem is a proper subset of that of the modified problem.
Except for the modified #Problem#, you should also provide #Answer# to the modified #Problem#.
#Demonstrations#:
<insert demonstrations>
The above are some demonstrations showing how to modify the original problems. Please follow their format and
modify the following problem:
#Problem#:
<insert the original problem>
#Answer#:
<insert the answer>
Please modified the above #Problem# and then provide #Answer# to the modified #Problem#:
Figure 11: Our prompt fed into GPT-4 for math data generation.
24Prompt for Instruction Following Data Generation
#Instruction# :
I want you to act as a helpful assistant. Please help me modify some instruction following problems and make
them harder. An instruction following problem datum consists of three parts: #Prompt#, #Constraint Type List#,
and #Constraint Kwargs#. The #Prompt# consists of several constraints that guide the model to generate text. The
#Constraint Type List# and #Constraint Kwargs# include the types and keyword arguments of the constraints contained
within the #Prompt#, respectively. They are utilized to verify whether the text generated by the model meets the
constraints. We provide a #Candidate Constraint Set# containing a variety of constraints. Please select an appropriate
constraint from this set and follow the format of the demonstrations provided to add to the original #Prompt#. By doing
so, you will create a more challenging new #Prompt#. Except for the modified #Prompt#, you should also provide
#Constraint Type List#, and #Constraint Kwargs# to the modified #Prompt#.
#Candidate Constraint Set#:
<insert the candidate constraint set>
#Demonstrations#:
<insert demonstrations>
The above are some demonstrations showing how to modify the original problems. Please follow their format and
modify the following problem:
#Prompt#:
<insert the original prompt>
#Constraint Type List#:
<insert the constraint type list>
#Constraint Kwargs#:
<insert the constraint keyword arguments>
Please modified the above #Prompt# and then provide #Constraint Type List# and #Constraint Kwargs# to the modified
#Prompt#:
Figure 12: Our prompt fed into GPT-4 for instruction following data generation.
25