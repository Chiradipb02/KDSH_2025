Synthetic Knowledge Ingestion: Towards Knowledge Refinement and
Injection for Enhancing Large Language Models
Jiaxin Zhang1,2, Wendi Cui2, Yiran Huang2, Kamalika Das1,2, Sricharan Kumar1,2
1Intuit AI Research2Intuit
{jiaxin_zhang, wendi_cui, yiran_huang, kamalika_das, sricharan_kumar}@intuit.com
Abstract
Large language models (LLMs) are proficient
in capturing factual knowledge across vari-
ous domains. However, refining their capa-
bilities on previously seen knowledge or inte-
grating new knowledge from external sources
remains a significant challenge. In this work,
we propose a novel synthetic knowledge in-
gestion method called Ski, which leverages
fine-grained synthesis, interleaved generation,
and assemble augmentation strategies to con-
struct high-quality data representations from
raw knowledge sources. We then integrate Ski
and its variations with three knowledge injec-
tion techniques: Retrieval Augmented Gener-
ation (RAG), Supervised Fine-tuning (SFT),
and Continual Pre-training (CPT) to inject and
refine knowledge in language models. Exten-
sive empirical experiments are conducted on
various question-answering tasks spanning fi-
nance, biomedicine, and open-generation do-
mains to demonstrate that Skisignificantly out-
performs baseline methods by facilitating effec-
tive knowledge injection. We believe that our
work is an important step towards enhancing
the factual accuracy of LLM outputs by refining
knowledge representation and injection capa-
bilities.1
1 Introduction
Large language models (LLMs) demonstrate profi-
ciency in capturing vast amounts of factual infor-
mation across a wide range of fields, attributable to
their extensive pre-training datasets (Petroni et al.,
2019; Cohen et al., 2023; Hu et al., 2023). Al-
though these models hold an impressive reposi-
tory of knowledge, integrating new information
via external datasets or enhancing their capacity
on previously seen information still poses several
challenges. One primary challenge is outdated
1The source code and datasets will be publicly available
for research purposes.
Question:Which team lost in the 2024 Champions League final?GroundTruth:Borussia DortmundAs an AI language model, I cannot predict the future or know the outcome of sporting events. It's impossible to determine who will win the UEFA Champions League 2024.
List of European Cup and UEFA Champions League finalsThe current champions are Real Madrid, who beat Borussia Dortmund 2–0 in the 2024 final.
BaseModel…
RawKnowledge
SyntheticKnowledgeIngestion(Ski)Fine-grainedSynthesisInterleavedGenerationAssembleAugmentationKnowledgeInjectionRAGSFTCPTThe team that lost in the 2024 Champions League final was Borussia Dortmund
TheteamisBorussia Dortmund
Figure 1: Illustrative example. Base models are of-
ten unable to handle certain questions due to limited
knowledge. Even with raw knowledge provided, the
output answer may still be incorrect if not well-digested
by LLMs. Our proposed Synthetic Knowledge Inges-
tion method, Ski, incorporates three key innovations to
easily transform raw knowledge into refined data repre-
sentations that LLM can effectively digest. By utilizing
injection pipelines such as RAG, SFT, and CPT, knowl-
edge or information will be injected into LLM to ensure
accurate and correct answers.
knowledge , where the static nature of the infor-
mation fails to evolve over time. Another issue
is the domain knowledge deficit where language
models, typically generalists, lack detailed, special-
ized knowledge in sectors like finance (Wu et al.,
2023) and healthcare (Singhal et al., 2023). Addi-
tionally, there is the problem of catastrophic forget-
ting, where language models may lose previously
acquired knowledge (Luo et al., 2023), which par-
ticularly affects rare facts that are minimally repre-
sented in the training data (Kandpal et al., 2023).
These issues underscore the necessity for ongoing
enhancements to the knowledge capabilities.
In response to emerging challenges, the practice
of customizing Large Language Models (LLMs)
to specialized fields, coupled with regular updatesarXiv:2410.09629v1  [cs.CL]  12 Oct 2024Rawknowledgecontext:“The annual contribution limit for a health savings account (HSA) in 2024 is $4,150 for individuals with self-only coverage and $8,300 for individuals with family coverage.These limits are about a 7% increase from 2023.Individuals who are 55 or older can contribute an additional $1,000, bringing the total to $4,850 for individuals and $8,750 for families…”Generatehypotheticalquestionsgivenn-gramknowledgecontexts–“Holistic Detail-Oriented”•1-gramQ–“What is the HSA contribution limit for individuals with self-only coverage in 2024”•2-gramQ–“WhatistheHSAcontributionlimitfor2023?”•3-gramQ–“Howmuchmorecanindividualscontributeifagedover55thanyounger?”Fine-grainedSynthesisGeneratequestionandanswersimultaneouslygivenknowledge–“AlignedHarmony”•Context:The annual contribution limit for a health savings account (HSA) in 2024 is $4,150 for individualswith self-only coverage and $8,300 for…•Question:What is the HSA contribution limit for individuals with self-only coverage in 2024?•Answer:$4150InterleavedGenerationAssemblen-gramsynthesisandquestion/answer/contextpairs–“Repetitionwithdiversity”•Assemblesofquestion,answer,andcontextpairs:üQuestion-Contextpairs(QC)üQuestion-Answerpairs(QA)üQuestion-Context-Answerpairs(QCA)•Assemblesofn-gramsynthesis:{1-gram,2-gram,3-gram,…}Assemble AugmentationSyntheticKnowledgeIngestion
Figure 2: Overview of the proposed method: Synthetic Knowledge Ingestion ( Ski), comprises of three essential
components. Firstly, fine-grained synthesis incorporates generating hypothetical questions based on an n-gram
detail-oriented principal. Secondly, interleaved generation generates questions and answers simultaneously by
maintaining aligned harmony, given a specific knowledge piece. Lastly, assemble augmentation combines question,
answer, and context pairs, along with n-gram synthesis, to improve the repetition with diversity.
to their knowledge base, is becoming increasingly
prevalent (Yu et al., 2022; Wang et al., 2023). Vari-
ous strategies have been developed to enhance the
factual accuracy and domain-specific expertise of
LLMs. These initiatives are crucial for integrating
precise and current knowledge, thus broadening
the utility and effectiveness of LLMs within pro-
fessional settings. Despite these advances, the per-
formance of LLMs often remains suboptimal, with
issues such as hallucination (Manakul et al., 2023;
Zhang et al., 2023a), especially in tasks that require
extensive knowledge. Consequently, assessing the
LLM’s capability to understand, memorize, and
retrieve knowledge is substantially essential. Given
some knowledge in the form of a raw text corpus,
our objective is to understand two key concepts and
fundamental questions: (i) Knowledge injection :
What is the optimal approach to encoding targeted
knowledge into a language model to enhance its
capability and functionality? and (ii) Knowledge
ingestion : What are the most effective strategies
for formatting knowledge or databases to ensure
that such representations are readily assimilable
and digestible by large language models (LLMs)
prior to injection?2
A prevalent technique for injecting external in-
formation into model responses is Retrieval Aug-
2Ingestion centers on the acquisition and processing of
knowledge for storage and future use, whereas injection fo-
cuses on actively integrating knowledge to enhance a language
model, either during inference or training phases.mented Generation ( RAG ) (Lewis et al., 2020).
This method relies on external knowledge sources,
yet its effectiveness is contingent upon the effi-
cacy of its retrieval system (Chen et al., 2024). If
this retrieval process is flawed, it might not se-
cure the necessary information, resulting in inap-
propriate responses. Additionally, a misalignment
in knowledge representation could hinder RAG’s
comprehension of the context (Gao et al., 2023).
Another approach is Supervised Fine-Tuning ( SFT)
(Dettmers et al., 2024), which improves a model
by continuing its training on relevant tasks. In-
struction tuning (Zhang et al., 2023c; Taori et al.,
2023) under SFT has shown substantial enhance-
ments in language model performance, though it
doesn’t necessarily enrich the model’s knowledge
base (Ouyang et al., 2022; Chia et al., 2023; Zhou
et al., 2024). Techniques in fine-tuning also in-
clude reinforcement learning (RL) strategies such
as RLHF (Touvron et al., 2023; Achiam et al.,
2023) and DPO (Rafailov et al., 2024), which re-
fine the model’s alignment post-training but do not
expand its knowledge capacity. Hence, developing
a precise supervised mechanism for raw knowledge
implementation poses a significant challenge. A
third strategy is Continual Pre-Training ( CPT ) (Ke
et al., 2023), or unsupervised fine-tuning, where
the model is further trained on specific knowledge
datasets tailored to certain tasks or domains (Wu
et al., 2024). While CPT does not require labeled
data, structuring the data in a format that closely re-flects the specific goals and tasks can be particularly
effective. Recent studies have garnered attention
for these three knowledge injection strategies (Ova-
dia et al., 2023; Balaguer et al., 2024; Mecklenburg
et al., 2024), yet there remains a gap in knowledge
ingestion. Given unprocessed and unstructured
knowledge, two key questions arise: (i) What data
representations are most effective for each injection
strategy? (ii)How can we systematically construct
diverse and high-quality representations to facili-
tate effective knowledge injection?
Inspired by this gap, we introduce Synthetic
knowledge ingestion ( Ski), an innovative synthetic
data generation method that automates and en-
hances knowledge ingestion, as illustrated in Fig.1.
Skileverages three key innovations to generate
high-quality and diverse data representation from
a raw knowledge base. First, Fine-grained Synthe-
siscreates hypothetical questions based on n-gram
knowledge contexts, ensuring a detailed match in
relevance, minimizing the semantic gap between
questions and answers, and increasing represen-
tation diversity. Second, Interleaved Generation
simultaneously generates both questions and an-
swers based on specific knowledge. This synthetic
question-answering (QA) format naturally mirrors
the process of information-seeking, providing di-
rect contextual alignment and relevance between
the questions and their respective answers. Third,
Assemble Augmentation combines fine-grained syn-
thesis across different n-gram spans and their QA
pair iterations, balancing repetition with diverse el-
ements. By integrating these components, the Ski
approach significantly enhances the refinement of
knowledge from its raw state, thus facilitating ef-
fective knowledge ingestion into LLMs.
Skioffers a generic solution for generating syn-
thetic data from raw knowledge, which can be in-
tegrated with all three knowledge injection strate-
gies: RAG, SFT, and CPT. With this advancement,
Skinot only incorporates new information using
external datasets but also refines LLMs’ capabil-
ity on previously seen knowledge and information.
We provide a comprehensive evaluation of Ski
on two open-source LLMs, Llama2-7B (Touvron
et al., 2023) and Mistral-7B (Jiang et al., 2023),
over cross-domain QA tasks, including those in
the finance, biomedical, open-generation and multi-
hop domain. Through performing extensive ex-
periments, our approach Skiachieves substantial
improvements upon the baselines by a large margin.
In summary, our contributions are threefold:•We develop Ski, a novel synthetic knowledge
ingestion method that integrates three essential
innovations, advancing the capability of refining
knowledge representation from raw contexts.
•We integrate Skiwith three knowledge injection
strategies—RAG, SFT, and CPT—to effectively
inject and refine knowledge in LLMs.
•We conduct experiments on various question-
answering tasks across different domains. Our
results demonstrate that Skisignificantly outper-
forms baseline methods by a large margin.
2 Problem Formulation
Knowledge Ingestion. It generally refers to the
methodology of acquiring, integrating, and trans-
forming information from a variety of knowledge
sources. It encompasses gathering, absorbing, and
morphing knowledge to build a database that facil-
itates future use. Mathematically, let’s Mrepre-
sent a language model, Qdenote a set of factual
questions, and assume that we only have access
to relevant raw knowledge base KQ. We aim to
enhance the representation of the knowledge base,
represented as K∗
Q
K∗
Q=T(KQ),T(·) :text→text (1)
where Tsignifies a transformation process that
converts raw knowledge into refined knowledge
KQ→ K∗
Q. Although advanced knowledge repre-
sentations such as knowledge graphs show promise,
their discussion falls beyond the scope of this study.
Knowledge Injection. It involves actively encod-
ing or integrating specific knowledge into a pre-
trained language model to enhance its performance
by incorporating new information from external
datasets or refining the model’s capabilities on pre-
viously seen information. Specifically, when the re-
fined knowledge applied improves the understand-
ing of the pre-trained model Mregarding questions
Qthrough a transformation F,
M∗:=F(M,K∗
Q)s.t.SM∗,Q>SM,Q(2)
whereSis a score metric, e.g., an accuracy that
measures the performance of the pre-trained model
in answering the questions Q, andM∗is the up-
dated language model. We explore three options
forF: RAG, SFT, and CPT, and provide a sys-
tematic evaluation of how our proposed synthetic
knowledge ingestion method, Ski, can enhance the
capabilities of LLMs across various QA tasks.3 Synthetic Knowledge Ingestion
The human learning process often involves ask-
ing questions and inquiring “why”, which can en-
hance comprehension. Drawing inspiration from
this, Skialso leverages the power of questions. By
utilizing three key innovations, Skitransforms raw
knowledge (such as documents and articles) into
question-augmented representations. The follow-
ing section provides a detailed description of these
three key innovations, as illustrated in Figure 2.
3.1 Fine-grained Synthesis
Considering the scenario where we possess a para-
graph of text as a knowledge base, crafting effective
questions to transform and augment this knowledge
base poses a challenging task. Questions that are
too broad may struggle to encompass all pertinent
knowledge points, whereas excessively detailed
questions risk losing sight of the overall content.
Moreover, ensuring that these questions are both
non-repetitive and diverse adds extra challenges.
To tackle these challenges, we introduce a fine-
grained synthesis approach, inspired by n-gram
language models, allowing for a balanced incor-
poration of both detailed and overarching content.
Assume the knowledge base KQparagraph con-
sists of msentences, KQ={k1, k2, ..., k m}where
kimeans the i-th sentence in KQ, we generate hy-
pothetical questions by querying an LLM model,
given a specific set of sentences conditioning on
the whole knowledge paragraph KQ:
qn
j← L Pfs({kj, kj+1, ..., k j+n−1};KQ),(3)
whereLis a pioneer LLM model with meta-prompt
Pfs,qn
jis the j-th(1≤j≤m−n+ 1) generated
hypothetical questions given the specific sentence
set{kj, kj+1, ..., k j+n−1}and the raw knowledge
context KQ. We denote ˜Qnas the set of n-gram hy-
pothetical questions: ˜Qn={qn
1, qn
2, qn
3, ..., qn
m−n}.
We define a set of n-gram knowledge context Cn
Cn={{k1, ..., k n}, ...,{km−n+1, km}} (4)
and therefore have C1={k1, k2, ..., k m},C2=
{{k1, k2},{k2, k3}, ...,{km−1, km}}, etc. The
question-context pair {˜Qn, Cn}can be written by:
{˜Qn, Cn}={{qn
1,{k1, ..., k n}}, ...,
{qn
m−n,{km−n+1, km}}}.(5)
Specifically, we have two variants of Skileverag-
ingn-gram synthesis principle:•Ski-Q-n : synthetic hypothetical questions ˜Qn
byn-gram given knowledge contexts Cn
•Ski-QC-n : synthetic hypothetical questions with
knowledge context pair {˜Qn, Cn}byn-gram
where ntypically ranges from 1 to 3. Specifically,
Ski-Q-1 refers synthetic 1-gram questions over m
sentences, ˜Q1={q1
1, q1
2, ..., q1
m}, and Ski-QC-1
means synthetic 1-gram question-context (QC)
pair{˜Q1, C1}={{q1
1, k1}, ...,{q1
m, km}}that in-
cludes mpairs in this set. More details and exam-
ples can be found in the Appendix A.2.
3.2 Interleaved Generation
Hypothetical question and context pairs have the
potential to transform knowledge articles into ques-
tions ( Ski-Q-n ) and question-context (QC) pairs
(Ski-QC-n ), which can revolutionize representa-
tions of knowledge articles. However, SFT requires
succinct QA pairs, while the context portion of
the QC pair consists of sentences, which are too
lengthy to be formulated as answers.
To deal with this issue, we introduce an inter-
leaved generation strategy that simultaneously gen-
erates QA pairs based on a specific knowledge con-
text. This synthetic QA format naturally emulates
the information-seeking process, providing direct
contextual alignment and relevance between the
questions and their corresponding answers. This
strategy can be built upon hypothetical questions
yet delivers QA pairs tailored to the specific knowl-
edge context KQ:
{qn
j, an
j} ← L Pig({kj, ..., k j+n−1};KQ),(6)
where Pigis the meta-prompt (see details in Ap-
pendix) different from the prompt Pfsused above.
We denote ˜Anas the set of answers correspond-
ing to the n-gram hypothetical questions: ˜An=
{an
1, an
2, an
3, ..., an
m−n}. The question-answering
pairs{˜Qn, An}can be written by:
{˜Qn,˜An}={{qn
1, an
1}, ...,{qn
m−n, an
m−n}}.(7)
The synthetic questions ˜Qnand corresponding
answers ˜Analign well with the given knowledge
context Cndue to interleaved mechanism. We
thus denote the following Skivariants from the
perspective of the QA pair:
•Ski-QA-n : synthetic QA pair {˜Qn,˜An}byn-
gram given specific knowledge contexts Cn
•Ski-QCA-n : synthetic QA pair combined with
the knowledge context, {˜Qn, Cn,˜An}3.3 Assemble Augmentation
To enhance the effectiveness of knowledge inges-
tion, we propose a strategic assembly approach
that harnesses the benefits of fine-grained and inter-
leaved generation. This strategy is built upon two
main pillars: (1) article/document augmentation
for optimized retrieval, and (2) data augmentation
for both supervised and unsupervised fine-tuning.
Specifically, we first combine all question-context
pairs from the same document into a singular arti-
cle, thereby improving the retrieval quality
[˜QC]n={qn
1,{k1, ..., k n}} ⊕ { qn
2,{k2, ..., k n+1}}
...⊕ {qn
m−n,{km−n+1, km}}
(8)
where⊕denotes the concatenation operator, which
combines all QC pair strings into a single article.
The notation [˜QC]nrepresents the augmented arti-
cle composed of all QC pairs using n-gram tech-
niques. As the same information can exist in differ-
ent formats under different n-gram strategies, com-
bining pairs from different n-gram allows repeti-
tion and diversity, enhancing the depth and breadth
of knowledge integration.
Assemble augmentation also offers scalability
suitable for synthesizing expansive sets of question-
answer pairs utilizing various n-gram generation
techniques. For fine-tuning schemes, this strategy
can be employed to generate a diverse ensemble of
n-gram QA pairs for data augmentation purposes.
Specifically, the QA assembly aggregates all n-
gram QA pairs as follows:
[˜Q˜A]n={˜Q1,˜A1} ∪...∪ {˜Qn,˜An} (9)
where [˜Q˜A]nis a combination of QA pairs from
1-gram to n-gram. Similarly, we have the ensemble
of QCA pairs from 1-gram to n-gram:
[˜QC˜A]n={˜Q1, C1,˜A1} ∪...∪ {˜Qn, Cn,˜An},
(10)
where ∪denotes the union of sets, which refers
to containing all the elements in a large set. We
thus summarize three Skivariants from the per-
spective of assemble that typically performs article
augmentation (for retrieval) or data augmentation:
•Ski-QC-ASM : assemble of all question-context
pairs into one augmented set [˜QC]n
•Ski-QA-ASM : assemble of all question-answer
pairs from 1-gram to n-gram [˜Q˜A]n
•Ski-QCA-ASM : assemble of all question-context-
answer pairs from 1-gram to n-gram [˜QC˜A]n4 Knowledge Injection for Enhancing
Language Models
4.1 Retrieval Augmented Generation (RAG)
We employ three variations of the Skiapproach,
namely Ski-Q-n ,Ski-QC-n , and Ski-QC-ASM ,
when utilizing the RAG pipeline for knowledge
injection. At the retrieval stage, Ski-Q-n matches
the query’s embedding with embeddings of all
questions generated in an n-gram pattern from
the knowledge documents. This can be inter-
preted as an inverse HyDE implementation, where
HyDE (Gao et al., 2022a) transfers a query to
a potential answer and performs an answer-to-
knowledge search. For Ski-Q-n we transform
knowledge into questions and execute a question-
to-question (Q-Q) search. In contrast, Ski-QC-n
compares the query’s embedding with embeddings
of all question-context (QC) pairs, facilitating a
question-to-(question+context) (Q-QC) search. Ad-
ditionally, Ski-QC-n introduces another variation,
Ski-QC-ASM , where QC pairs from the same docu-
ment are amalgamated into a single article, and the
query’s embedding is compared with embeddings
of such articles. After identifying the most relevant
items, Skiconsolidates only the most pertinent
snippets or sentences from the knowledge docu-
ments related to the retrieved items into an LLM
for generating the final answer. This process mini-
mizes noise from sentences in the same documents
but not directly pertinent to user queries.
4.2 Supervised Fine-tuning (SFT)
We utilize three variants of the Skimethod, namely
question-answer (QA) pairs, question-context (QC)
pairs, and question-context-answer (QCA) pairs, to
incorporate knowledge via supervised fine-tuning.
In the QCA approach, we concatenate the question
(Q) and context (C) input and use the answer as
the output, following the setting in Viswanathan
et al. (2023). To conform to the n-gram princi-
ple, we create separate training datasets for every
n-gram ( n=1,2,3) for SFT, resulting in a decrease
in dataset size as nincreases. We focus on eval-
uating three fine-grained generations ( n=1), i.e.,
Ski-QA-1 ,Ski-QC-1 , and Ski-QCA-1 .
4.3 Continual Pre-training (CPT)
Continual pre-training can leverage the dataset
used for SFT, but it typically requires a larger vol-
ume of data to enhance the capabilities of LLMs
(Lin et al., 2024; Ke et al., 2023). To addressModel MethodBioASQ NQ FiQA
nDCG@1 nDCG@10 Recall@1 Recall@10 nDCG@1 nDCG@10 Recall@1 Recall@10 nDCG@1 nDCG@10 Recall@1 Recall@10
ContrieverRaw Article 0.810 0.720 0.133 0.578 0.748 0.831 0.644 0.915 0.298 0.323 0.134 0.406
Ski-Q-1 (iHyDE) 0.845 0.733 0.142 0.574 0.752 0.804 0.641 0.876 0.452 0.482 0.231 0.562
Ski-QC-1 0.835 0.751 0.132 0.597 0.810 0.865 0.704 0.926 0.455 0.493 0.232 0.584
Ski-QC-ASM 0.820 0.719 0.131 0.575 0.773 0.839 0.670 0.908 0.398 0.435 0.193 0.542
BM25Raw Article 0.232 0.297 0.189 0.389 0.325 0.389 0.272 0.478 0.315 0.318 0.162 0.368
Ski-Q-1 (iHyDE) 0.175 0.219 0.142 0.289 0.218 0.282 0.176 0.368 0.238 0.252 0.121 0.307
Ski-QC-1 0.210 0.282 0.173 0.373 0.3 0.377 0.245 0.482 0.289 0.301 0.142 0.362
Ski-QC-ASM 0.268 0.328 0.221 0.417 0.350 0.422 0.291 0.513 0.377 0.390 0.192 0.454
Table 1: Retrieval performance on three datasets using two different retrievers (Contriever and BM25).
Figure 3: Effect of Skimethod on the RAG performance using different retrievers and generators.
this, we amalgamate all the generated pairs from
SFT to create a comprehensive augmented train-
ing set, Ski-QC-ASM ,Ski-QA-ASM ,Ski-QCA-ASM .
This strategy not only amplifies repetition but also
preserves diversity (Ovadia et al., 2023).
5 Experiments
5.1 Experiment Setup
Datasets. We evaluate our approach on multi-
ple cross-domain question-answering tasks, includ-
ing those in the finance domain (e.g., FiQA (Maia
et al., 2018)), biomedical domain (e.g., BioASQ
(Tsatsaronis et al., 2015)), open-generation domain
(e.g., NQ (Kwiatkowski et al., 2019) and multi-
hop domain (e.g., HotpotQA (Yang et al., 2018)).
Please find more relevant details about data usage
for RAG, SFT, and CPT in the Appendix A.3-A.4.
Evaluation Models and Metrics. We utilize
GPT-3.5-turbo from OpenAI as our target agent
model to generate synthetic questions, answers,
and their corresponding pairs. To evaluate our
approach, we employ two open-source LLMs,
Llama2-7B (Touvron et al., 2023) and Mistral-7B
(Jiang et al., 2023), which serve as the base mod-
els for RAG, SFT and CPT, implemented usingthe LLaMA-Factory pipeline3. Our evaluation en-
compasses a comparison between two retrieval
methods: the Dense-Retrieval approach, Contriever
(Izacard et al., 2021), and the Sparse-Retrieval
method, BM25. We assess the performance of
three generators, GPT-3.5-turbo ,Llama2-7B , and
Mistral-7B using the F1 score as a measure of ac-
curacy, following the RAGGED (Hsia et al., 2024)
pipeline4across RAG, SFT, and CPT scenarios.
Baseline Methods. For retrieval and RAG tasks,
we utilize raw knowledge — specifically, the origi-
nal article as a standard baseline. Our approach
draws inspiration from Hypothetical Document
Embeddings, known as HyDE (Gao et al., 2022b).
We introduce an adaptation termed "inverse" HyDE
(iHyDE) (Gao et al., 2023) established by Ski-Q-1 .
In SFT evaluations, the base model without fine-
tuning serves as the standard baseline. We also
employ a vanilla QA baseline, which generates a
set of question-answer pairs by querying LLMs
using extracted documents/articles, as outlined in
(Mecklenburg et al., 2024; Balaguer et al., 2024).
For the CPT pipeline, our standard baselines en-
compass the base model and raw context, which is
3https://github.com/hiyouga/LLaMA-Factory
4https://github.com/neulab/raggedaligned with the training dataset setup as detailed
in (Ovadia et al., 2023). Additionally, we integrate
two novel baseline methods: Ski-C-1 , concentrat-
ing uniquely on the 1-gram knowledge context,
andSki-C-ASM , an ensemble approach that inte-
grates knowledge contexts ranging from 1-gram to
3-gram. Both are variants of the Skiapproach.
5.2 Main Results
RAG Results. We evaluate the retrieval perfor-
mance of Skiusing two different retrievers, as il-
lustrated in Table 1. For Contriever, Ski-QC-1 out-
performed both the inverse HyDE ( Ski-Q-1 ) and
the raw article baselines, particularly achieving a
significant improvement (+15%) on the FiQA task.
For BM25, Ski-QC-ASM surpassed all the baselines
andSkivariations across three datasets. Further
evaluations were conducted on our method within
RAG systems, detailed in Table 2. Inverse HyDE
fell short of the raw article baseline, indicating that
the question-to-question search approach might
not be as effective as anticipated. Ski-QC-ASM
emerged as the superior method, showing consis-
tent improvements compared to other baselines
across all tasks. The results of BM25 are aligned
with the retrieval-only task, confirming the effec-
tiveness of assemble augmentation.
Model Method BioASQ NQ HotpotQA
Contriever
+GPT-3.5Raw Article 0.361 0.504 0.585
Ski-Q-1 (iHyDE) 0.317 0.452 0.507
Ski-QC-1 0.300 0.553 0.553
Ski-QC-ASM 0.385 0.563 0.627
BM25
+GPT-3.5Raw Article 0.227 0.258 0.443
Ski-Q-1 (iHyDE) 0.162 0.181 0.274
Ski-QC-1 0.186 0.201 0.377
Ski-QC-ASM 0.243 0.306 0.478
Table 2: End-to-end RAG results with two retrievers.
Effect of Different Retrievers and Generators.
Fig.3 presents a detailed evaluation of how dif-
ferent retrievers and generators influence our Ski
methods. In the NQ and HotpotQA tasks, Ski
methods that incorporate Contriever generally out-
perform those using BM25. In terms of genera-
tion, GPT-3.5-turbo demonstrates superior perfor-
mance over Llama2-7B andMistral-7B . For the
BioASQ task, both Contriever and BM25 perform
comparably, with Llama2-7B surpassing the other
two generators in effectiveness. We compare three
variations of Ski, namely Ski-Q-1 ,Ski-QC-1 , and
Ski-QC-ASM , across different combinations of re-
trievers and generators. Consistently across tasks,Ski-QC-ASM tends to perform the best, followed by
Ski-QC-1 , outperforming the baseline Ski-Q-1 .
SFT Results. Beyond the RAG evaluation, we
assess the enhancement of pre-trained models us-
ing SFT in our study with Ski. The synthetic
QA pairs, Ski-QA-1 , exhibit superior performance
across three datasets. Specifically, Mistral-7B
achieves an average performance gain of 5.13%
over the base model and 4.07% improvement com-
pared to the vanilla QA baseline, while Llama2-7B
markedly improves by 19.3% and 11.1% respec-
tively. Generally, the data representation pro-
vided by Ski-QA-1 is the more effective method.
We also observe that Llama2-7B demonstrates a
stronger capability in refining knowledge compared
toMistral-7B , aligning with findings reported by
Ovadia et al. (2023).
Model Method BioASQ NQ HotpotQA
Mistral-7BBase Model 0.125 0.098 0.113
Vanilla QA 0.131 0.084 0.155
Ski-QA-1 0.162 0.159 0.171
Ski-QC-1 0.138 0.113 0.155
Ski-QCA-1 0.160 0.157 0.163
Llama2-7BBase Model 0.123 0.082 0.136
Vanilla QA 0.235 0.150 0.203
Ski-QA-1 0.357 0.218 0.266
Ski-QC-1 0.436 0.217 0.258
Ski-QCA-1 0.271 0.201 0.252
Table 3: SFT performance on two pre-trained models.
Effect of n-gram Synthesis. The impact of n-
gram on SFT is illustrated in Fig. 4. Across vari-
ous tasks, the 1-gram consistently outperforms the
2-gram and 3-gram in all Skivariations, underscor-
ing the significance of fine-grained generation for
enhancing both the quantity and quality of data syn-
thesis. Compared to QA pairs and QCA pairs, the
performance of QC pairs drops slightly and is infe-
rior to the other two variations. This suggests that
the interleaved generation in QA pairs is crucial for
improving SFT performance.
CPT Results. Llama2-7B demonstrates a supe-
rior capability for knowledge injection compared
toMistral-7B in the SFT evaluation. Building on
this observation, we further explore the impact of
Skithrough the CPT approach with Llama2-7B .
While QA pairs excel in SFT, Ski-QC-ASM and
Ski-QCA-ASM show significantly enhanced perfor-
mance in the CPT evaluation. Specifically, these
methods lead to performance gains of +21.2% in1-gram 2-gram 3-gram0.250.300.350.400.450.50F1-score
BioASQ (Llama2-7b SFT)
Ski-QC
Ski-QCA
Ski-QA
1-gram 2-gram 3-gram0.160.180.200.220.24F1-score
NQ (Llama2-7b SFT)
Ski-QC
Ski-QCA
Ski-QA
1-gram 2-gram 3-gram0.2000.2250.2500.2750.300F1-score
HotpotQA (Llama2-7b SFT)
Ski-QC
Ski-QCA
Ski-QAFigure 4: SFT performance on different n-gram settings using Llama2-7B base model across three datasets. Note
that the F1-score often decreases with the increase of n-gram, but not very significantly drop.
BioASQ and +11.5% in NQ task relative to the base
model. Ski-C-ASM presents a slight improvement
over other QC and QCA variations in HotpotQA.
This highlights the benefit of assembly augmen-
tation where longer contexts, which include more
comprehensive information than short answers, aid
in deepening the pretrained models’ understanding.
Method BioASQ NQ HotpotQA
Base Model 0.123 0.082 0.136
Raw Article (only context) 0.219 0.166 0.242
Ski-C-1 (1-gram context) 0.208 0.178 0.204
Ski-C-ASM (1-3-gram context) 0.241 0.163 0.253
Ski-QA-1 0.269 0.184 0.205
Ski-QA-ASM 0.228 0.194 0.218
Ski-QC-1 0.294 0.178 0.226
Ski-QC-ASM 0.335 0.182 0.215
Ski-QCA-1 0.211 0.188 0.218
Ski-QCA-ASM 0.220 0.197 0.191
Table 4: CPT performance on Llama2-7B .
Holistic Comparison of RAG, SFT, and CPT.
Fig.5 presents a comprehensive comparison of the
Skimethod across various knowledge injection
pipelines. While RAG demonstrates marked im-
provement over the raw model and baselines, the
gains from CPT are relatively modest. Despite
this, Llama2-7B consistently shows significant im-
provements over Mistral-7B across RAG, SFT,
and CPT. GPT-3.5-turbo is utilized solely as the
generator for RAG, where it exhibits relatively low-
performance gains due to its already high base per-
formance. Our Skimethod appears promising in
enhancing the RAG performance of open-source
pre-trained models, suggesting that Llama2-7B
could be an effective candidate for knowledge injec-
tion, potentially achieving competitive gains simi-
lar to RAG when applied through SFT and CPT.
6 Related Work
Incorporating new information into LLMs or refin-
ing their capabilities with previously seen informa-
tion has posed a substantial challenge (Chen et al.,
Figure 5: The relative performance (F1-score) gain for
RAG, SFT, and CPT over averaged across all datasets.
2022; Martino et al., 2023; Zhang et al., 2023d; Ye
et al., 2023; Zhang et al., 2023b), the lack of effec-
tive approaches and evaluations is a notable con-
cern. Recently, Xu et al. (2023) explored continued
pre-training with a knowledge infill objective to
enhance fact retention, while Tian et al. (2023) ap-
plied constrained optimization through direct pref-
erence optimization (Rafailov et al., 2024) to rein-
force factuality during post-training. Additionally,
Ovadia et al. (2023) aimed to compare retrieval,
fine-tuning, and their combinations but the optimal
way to inject knowledge into LLMs remains an
open and sometimes contentious debate.
However, a critical knowledge ingestion gap re-
mains: what are the data representations that work
best for each injection pipeline with unprocessed
and unstructured knowledge? (Baek et al., 2024) To
answer this, Balaguer et al. (2024) extracts informa-
tion from raw PDF documents, leading to questions
and answers for supervised fine-tuning pipelines.
Mecklenburg et al. (2024) presents two data gen-
eration processes, token-based and fact-based, to
inject knowledge via SFT. In contrast, our study
aims to develop an innovative and generic syn-
thetic knowledge ingestion approach, Ski, which
achieves significant improvement for each knowl-
edge injection pipeline.
7 Conclusions
We propose a novel method for refining knowl-
edge representation and injection to enhance LLMs.Through extensive empirical analysis, our work has
revealed several important findings. Firstly , syn-
thetic QA generations are highly effective in digest-
ing knowledge from its raw format, resulting in
consistent enhancement for each injection pipeline.
Secondly , fine-grained assemble augmentation has
proven to be proficient in constructing high-quality,
diverse datasets for knowledge injection. Lastly ,
RAG shows the potential to enhance knowledge,
whereas the advancement of SFT and CPT largely
depends on selecting appropriate base models and
leveraging their existing knowledge capabilities.
8 Ethics Statement
This paper studies knowledge ingestion and repre-
sentation in LLMs, which has significant broader
impacts on the field of natural language processing
and helps to address ethical considerations regard-
ing factuality and reliability. The research outcome
may contribute to the development of more accu-
rate and factual LLMs by mitigating the risks of
misinformation and biased outputs and promoting
accountability and trust in AI systems.
9 Limitations
Our current experiments focus on tasks in a
question-answering setting which requires a knowl-
edge base. Further research is needed to expand
to more complex tasks such as conversational or
dialogue-based prompting. Also, the quality of the
results introduced by Skidepends on the quality of
the knowledge base. For OOD questions or edge
cases where the knowledge base is corrupted or
provides conflicting information, Skiis not able
to solve that. Skidoes require LLM calls to gen-
erate different representations. Such jobs can be
conducted offline and only once. Thus the cost will
be relatively reduced as it serves more queries. In
addition, although advanced knowledge represen-
tations such as knowledge graphs show promise,
their discussion falls beyond the scope of the cur-
rent study but will appear in our future work.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Jinheon Baek, Nirupama Chandrasekaran, Silviu
Cucerzan, Allen Herring, and Sujay Kumar Jauhar.2024. Knowledge-augmented large language mod-
els for personalized contextual query suggestion. In
Proceedings of the ACM on Web Conference 2024 ,
pages 3355–3366.
Angels Balaguer, Vinamra Benara, Renato Luiz de Fre-
itas Cunha, Roberto de M Estevão Filho, Todd
Hendry, Daniel Holstein, Jennifer Marsman, Nick
Mecklenburg, Sara Malvar, Leonardo O Nunes, et al.
2024. Rag vs fine-tuning: Pipelines, tradeoffs, and
a case study on agriculture. arXiv e-prints , pages
arXiv–2401.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2024. Benchmarking large language models in
retrieval-augmented generation. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 38, pages 17754–17762.
Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,
Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and
Huajun Chen. 2022. Knowprompt: Knowledge-
aware prompt-tuning with synergistic optimization
for relation extraction. In Proceedings of the ACM
Web conference 2022 , pages 2778–2788.
Yew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-
janya Poria. 2023. Instructeval: Towards holistic
evaluation of instruction-tuned large language mod-
els.arXiv preprint arXiv:2306.04757 .
Roi Cohen, Mor Geva, Jonathan Berant, and
Amir Globerson. 2023. Crawling the internal
knowledge-base of language models. arXiv preprint
arXiv:2301.12810 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2024. Qlora: Efficient finetuning
of quantized llms. Advances in Neural Information
Processing Systems , 36.
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.
2022a. Precise zero-shot dense retrieval without rele-
vance labels. arXiv preprint arXiv:2212.10496 .
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.
2022b. Precise zero-shot dense retrieval without rele-
vance labels. arXiv preprint arXiv:2212.10496 .
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997 .
Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Gra-
ham Neubig. 2024. Ragged: Towards informed
design of retrieval augmented generation systems.
arXiv preprint arXiv:2403.09040 .
Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang
Nie, and Juanzi Li. 2023. A survey of knowledge
enhanced pre-trained language models. IEEE Trans-
actions on Knowledge and Data Engineering .Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv
preprint arXiv:2112.09118 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
Wallace, and Colin Raffel. 2023. Large language
models struggle to learn long-tail knowledge. In In-
ternational Conference on Machine Learning , pages
15696–15707. PMLR.
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Kon-
ishi, Gyuhak Kim, and Bing Liu. 2023. Contin-
ual pre-training of language models. arXiv preprint
arXiv:2302.03241 .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:453–
466.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Ye-
long Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian
Jiao, Nan Duan, et al. 2024. Rho-1: Not all tokens
are what you need. arXiv preprint arXiv:2404.07965 .
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie
Zhou, and Yue Zhang. 2023. An empirical study
of catastrophic forgetting in large language mod-
els during continual fine-tuning. arXiv preprint
arXiv:2308.08747 .
Macedo Maia, Siegfried Handschuh, André Freitas,
Brian Davis, Ross McDermott, Manel Zarrouk, and
Alexandra Balahur. 2018. Www’18 open challenge:
financial opinion mining and question answering. In
Companion proceedings of the the web conference
2018 , pages 1941–1942.
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
Selfcheckgpt: Zero-resource black-box hallucination
detection for generative large language models. In
The 2023 Conference on Empirical Methods in Natu-
ral Language Processing .
Ariana Martino, Michael Iannelli, and Coleen Truong.
2023. Knowledge injection to counter large language
model (llm) hallucination. In European Semantic
Web Conference , pages 182–185. Springer.Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel
Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva,
Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy
Yannam, et al. 2024. Injecting new knowledge into
large language models via supervised fine-tuning.
arXiv preprint arXiv:2404.00213 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems , 35:27730–27744.
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and
Oren Elisha. 2023. Fine-tuning or retrieval? com-
paring knowledge injection in llms. arXiv preprint
arXiv:2312.05934 .
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
ton Bakhtin, Yuxiang Wu, Alexander H Miller, and
Sebastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066 .
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems , 36.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-
davi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,
et al. 2023. Large language models encode clinical
knowledge. Nature , 620(7972):172–180.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B Hashimoto. 2023. Alpaca: A
strong, replicable instruction-following model. Stan-
ford Center for Research on Foundation Models.
https://crfm. stanford. edu/2023/03/13/alpaca. html ,
3(6):7.
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
pher D Manning, and Chelsea Finn. 2023. Fine-
tuning language models for factuality. arXiv preprint
arXiv:2311.08401 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, et al. 2015. An overview of the bioasq large-scale
biomedical semantic indexing and question answer-
ing competition. BMC bioinformatics , 16:1–28.
Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch,
Tongshuang Wu, and Graham Neubig. 2023.Prompt2model: Generating deployable models from
natural language instructions. arXiv preprint
arXiv:2308.12261 .
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng,
Chen Chen, et al. 2023. Knowledge editing for
large language models: A survey. arXiv preprint
arXiv:2310.16218 .
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,
Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-
badur, David Rosenberg, and Gideon Mann. 2023.
Bloomberggpt: A large language model for finance.
arXiv preprint arXiv:2303.17564 .
Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan,
Thuy-Trang Vu, and Gholamreza Haffari. 2024. Con-
tinual learning for large language models: A survey.
arXiv preprint arXiv:2402.01364 .
Yan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aish-
warya Padmakumar, Yang Liu, and Dilek Hakkani-
Tür. 2023. Kilm: Knowledge injection into
encoder-decoder language models. arXiv preprint
arXiv:2302.09170 .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. arXiv preprint arXiv:1809.09600 .
Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou,
Yining Hua, and Andrew Liu. 2023. Qilin-
med: Multi-stage knowledge injection advanced
medical large language model. arXiv preprint
arXiv:2310.09089 .
Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,
Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A
survey of knowledge-enhanced text generation. ACM
Computing Surveys , 54(11s):1–38.
Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley
Malin, and Sricharan Kumar. 2023a. Sac3: Reliable
hallucination detection in black-box language mod-
els via semantic-aware cross-check consistency. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 15445–15458.
Qinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang,
Daochen Zha, and Zailiang Yu. 2023b. Knowgpt:
Black-box knowledge injection for large language
models. arXiv preprint arXiv:2312.06185 .
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, et al. 2023c. Instruction tuning
for large language models: A survey. arXiv preprint
arXiv:2308.10792 .
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong
Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan
Liu, Peng Li, Maosong Sun, et al. 2023d. Plug-and-
play knowledge injection for pre-trained language
models. arXiv preprint arXiv:2305.17691 .Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan
Ye, and Zheyan Luo. 2024. Llamafactory: Unified
efficient fine-tuning of 100+ language models. arXiv
preprint arXiv:2403.13372 .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping
Yu, Lili Yu, et al. 2024. Lima: Less is more for align-
ment. Advances in Neural Information Processing
Systems , 36.A Appendix
A.1 Synthetic Knowledge Pipeline Prompts
Table 5: Prompt to generate Questions for Ski-QC-n
You are a question generator. Given a series of paragraphs, you have job is to generate
questions for each paragraph.
The question generated should be able to be answered ONLY based on the information
in the paragraph.
The question generated should be about the main topic of the paragraph.
##Paragraphs:
{paragraphs }
Return the questions in a list.
["1. question 1", "2. question 2", "3. question 3" ...]
##Questions:
Table 6: Prompt to generate Questions And Answers for Ski-QCA-n
You are a question generator. Given a series of paragraphs, you have job is to generate
questions for each paragraph and abstract the corresponding answers.
The question generated should be able to be answered ONLY based on the information
in the paragraph.
The question generated should be about the main topic of the paragraph.
The answer should be about the generated question and based on the information in
the paragraph.
##Paragraphs:
{paragraphs }
Return the questions in a list.
[{"q": "question 1", "a": "answer 1"}, {"q": "question 2", "a": "answer 2"}, ...]
##Questions:A.2 Examples of Data generated under N-gram
Table 7: Synthetic Data Example
Configuration Generated Synthetic Data
Original ArticleAntonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July 1741) was an Italian
Baroque composer, virtuoso violinist, teacher and cleric. Born in Venice, he is
recognized as one of the greatest Baroque composers, and his influence during
his lifetime was widespread across Europe. He composed many instrumental
concertos, for the violin and a variety of other instruments, as well as sacred
choral works and more than forty operas.", "answer": "Antonio Vivaldi was a
Baroque composer, virtuoso violinist, teacher and cleric.
Ski-QC-1[{"question ": " Who was Antonio Lucio Vivaldi?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July 1741)
was an Italian Baroque composer, virtuoso violinist, teacher and cleric."},
{"question ": " Where was Vivaldi born and what is he recognized
for?",
"context ": "Born in Venice, he is recognized as one of the greatest Baroque
composers, and his influence during his lifetime was widespread across
Europe."},
{"question ": " What types of music did Vivaldi compose and what is
his most famous work?",
"context ": "He composed many instrumental concertos, for the violin and a
variety of other instruments, as well as sacred choral works and more than forty
operas."}]
Ski-QC-2[{"question ": " Who was Antonio Lucio Vivaldi?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July 1741)
was an Italian Baroque composer, virtuoso violinist, teacher and cleric. Born in
Venice, he is recognized as one of the greatest Baroque composers, and his
influence during his lifetime was widespread across Europe."},
{"question ": " What types of music did Vivaldi compose?",
"context ": "He composed many instrumental concertos, for the violin and a
variety of other instruments, as well as sacred choral works and more than forty
operas. His best-known work is a series of violin concertos known as ¨The Four
Seasons¨."}]
Continuation of Table 7Continuation of Table 7
Ski-QC-3[{"question ": " Who was Antonio Lucio Vivaldi?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July 1741)
was an Italian Baroque composer, virtuoso violinist, teacher and cleric. Born in
Venice, he is recognized as one of the greatest Baroque composers, and his
influence during his lifetime was widespread across Europe. He composed
many instrumental concertos, for the violin and a variety of other instruments,
as well as sacred choral works and more than forty operas."},
{"question ": " What type of music did Vivaldi compose?",
"context ": "His best-known work is a series of violin concertos known as ¨The
Four Seasons¨."}]
Ski-QCA-1[{"question ": "What was Antonio Lucio Vivaldi’s profession?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July 1741)
was an Italian Baroque composer, virtuoso violinist, teacher and cleric.",
"answer ": "Antonio Lucio Vivaldi was a composer, virtuoso violinist, teacher
and cleric."},
{"question ": "Where was Antonio Lucio Vivaldi born?",
"context ": "Born in Venice, he is recognized as one of the greatest Baroque
composers, and his influence during his lifetime was widespread across
Europe.",
"answer ": "Antonio Lucio Vivaldi was born in Venice."},
{"question ": "What is Antonio Lucio Vivaldi recognized as?",
"context ": "He composed many instrumental concertos, for the violin and a
variety of other instruments, as well as sacred choral works and more than forty
operas.",
"answer ": "Antonio Lucio Vivaldi is recognized as one of the greatest Baroque
composers."},
{"question ": "What types of works did Antonio Lucio Vivaldi com-
pose?",
"context ": "His best-known work is a series of violin concertos known as ¨The
Four Seasons¨.",
"answer ": "Antonio Lucio Vivaldi composed many instrumental concertos,
sacred choral works, and more than forty operas."}]
Continuation of Table 7Continuation of Table 7
Ski-QCA-2[{"question ": "What was Antonio Lucio Vivaldi’s profession?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 - 28 July 1741) was an
Italian Baroque composer, virtuoso violinist, teacher and cleric. Born in Venice,
he is recognized as one of the greatest Baroque composers, and his influence
during his lifetime was widespread across Europe.",
"answer ": "Antonio Lucio Vivaldi was a composer, virtuoso violinist, teacher
and cleric."},
{"question ": "Where was Antonio Lucio Vivaldi born?",
"context ": "He composed many instrumental concertos, for the violin and a
variety of other instruments, as well as sacred choral works and more than forty
operas. His best-known work is a series of violin concertos known as ¨The Four
Seasons¨.",
"answer ": "Antonio Lucio Vivaldi was born in Venice."}]
Ski-QCA-3[{"question ": "What was Antonio Vivaldi’s profession?",
"context ": "Antonio Lucio Vivaldi (] ; 4 March 1678 - 28 July 1741) was
an Italian Baroque composer, virtuoso violinist, teacher and cleric. Born in
Venice, he is recognized as one of the greatest Baroque composers, and his
influence during his lifetime was widespread across Europe. He composed
many instrumental concertos, for the violin and a variety of other instruments,
as well as sacred choral works and more than forty operas.",
"answer ": "Antonio Vivaldi was a Baroque composer, virtuoso violinist,
teacher and cleric."},
{"question ": "Where was Antonio Vivaldi born?",
"context ": "His best-known work is a series of violin concertos known as ¨The
Four Seasons¨.",
"answer ": "Antonio Vivaldi was born in Venice."}]
A.3 Details of SFT Pipeline
Training. For the SFT Pipeline, we use the QLoRA (Dettmers et al., 2024) strategy with 4bit quantization
built on top of Llama-Factory (Zheng et al., 2024)5. For float format we use bf16. The batch size is set to
2, and the number of update steps to accumulate the gradients is set to 4. We use a cosine scheduler with
a learning rate of 5e-5 and a warm-up ratio of 0.1. The training lasts for 3 rounds, with the maximum
gradient norm set to 1. We use Amazon Sagemaker g5.12xlarge and g5.24xlarge instances for training,
which are powered by NVIDIA A10 Tensor Core GPUs and comprise 4 GPUs with 24 GB of memory
each. For training data size please refer to Table 8.
Generation. We will generate 1 most likely generation with sampling strategies. This generation will
be used to evaluate the correctness. The temperature of generation is fixed at 1.0, and top_k is fixed at
50. The maximum number of new tokens in each generation is set to 40 tokens. Instruction used for
generation is Respond to questions with concise and to-the-point answers. No explanation is needed.
Keep your response within 20 words . The test data for each dataset consists of 200 question-answer or
question-context pairs.
A.4 Details of CPT Pipeline
Training. For CPT, we use the QLoRA 4bit quantization with the LoRA+ strategy with a lambda value
of 16.0 and float16 mixed precision training built on top of Llama-Factory (Zheng et al., 2024). The batch
size is set to 2, and the number of update steps to accumulate the gradients is set to 4. We use a cosine
5https://github.com/hiyouga/LLaMA-Factoryscheduler with a learning rate of 5e-5 and a warmup ratio of 0.1. The training lasts for 3 rounds, with the
maximum gradient norm set to 1. We use Amazon Sagemaker g5.12xlarge and g5.24xlarge instances for
training, which are powered by NVIDIA A10 Tensor Core GPUs and comprise 4 GPUs with 24 GB of
memory each.
A.5 Details of Dataset
For each dataset, we select the first 200 queries in the test set as the final test set. Considering the huge
amount of documents for retrieval, we select the articles that have been referred to at least one test query
as the ground truth of the retrieval task to be the knowledge base, see Table 8.
Table 8: SFT/CPT Training Dataset Size.
Dataset Ski-QA-1 Ski-QC-1 Ski-QA-ASM Ski-QC-ASM
BioASQ (Tsatsaronis et al., 2015) 5000 5000 34405 34966
NQ (Kwiatkowski et al., 2019) 5000 5000 13707 13528
HotpotQA (Yang et al., 2018) 5000 5000 12183 9191
Table 9: CPT Training Data Example
Configuration Generated Synthetic Data
Ski-QA-n[{"text": "Question: What was Antonio Lucio Vivaldi’s profession?
Answer: Antonio Lucio Vivaldi was a composer, virtuoso violinist,
teacher and cleric."},
{"text": "Question: Where was Antonio Lucio Vivaldi born?
Answer: Antonio Lucio Vivaldi was born in Venice."},
{"text": "Question: What is Antonio Lucio Vivaldi recognized as?
Answer: Antonio Lucio Vivaldi is recognized as one of the greatest
Baroque composers."}]
Continuation of Table 9Continuation of Table 9
Ski-QC-n[{"text": "Question: Who was Antonio Lucio Vivaldi?"
Context: "Antonio Lucio Vivaldi (] ; 4 March 1678 ˘00a0 ˘2013 28 July
1741) was an Italian Baroque composer, virtuoso violinist, teacher and cleric."},
{"text": "Question: Where was Vivaldi born and what is he recognized
for?
Context: Born in Venice, he is recognized as one of the greatest Baroque
composers, and his influence during his lifetime was widespread across
Europe."},
{"text": "Question: What types of music did Vivaldi compose and
what is his most famous work?
Context: He composed many instrumental concertos, for the violin
and a variety of other instruments, as well as sacred choral works and more
than forty operas."}]
Generation. We will generate 1 most likely generation with sampling strategies. This generation will
be used to evaluate the correctness. The temperature of generation is fixed at 1.0, and top_k is fixed at 50.
The max number of new tokens of each generation is set to 40 tokens. Instruction used for generation is
Respond to questions with concise and to-the-point answers. No explanation needed. Keep your response
within 20 words . The test data for each dataset consist of 200 question-answer or question-context pairs.
Table 10: CPT Test Data Example
[{"query ": "The 1978 NBA World Championship Series had as MVP which Hall of
Fame class member of 1988?",
"answer ": "Westley Sissel Unseld"},
{"query ": "Who was born first out of Leopold Lummerstorfer and Laurent
Touil-Tartour?",
"answer ": "Leopold Lummerstorfer"},
{"query ": "In what city did Charlie Spiller play college football?",
"answer ": "Lorman"}]A.6 Additional Experiments
A.6.1 Effect of N-gram on Retrieval
To gauge the impact of N-Gram on retrieval strategy, we compared the retrieval performance on FiQA
task using the Ski-Q-n strategy. Results can be observed in Table 11. 1-Gram strategy, despite the fact
that it generates the most amount of questions to compare with, leads to the best retrieval result.
A.6.2 Effect of Query Augmentation and Document Augmentation
To better understand the effectiveness of Query Augmentation and Document Augmentation on retrieval,
we compare Skiwith HyDE (Gao et al., 2022b) using the FiQA dataset and Contriver. Full results can
be seen in Table 12. For the Skivariations, both Ski-Q-n andSki-QC-n outperform HyDE (Gao et al.,
2022b).
Table 11: Effective of N-gram for Retrieval
nCDG@1 nCDG@5 nCDG@10 Recall@1 Recall@5 Recall@10
Article 0.316 0.311 0.343 0.145 0.476 0.562
3-Gram 0.451 0.428 0.456 0.225 0.328 0.429
2-Gram 0.448 0.435 0.468 0.221 0.454 0.554
1-Gram 0.452 0.454 0.482 0.232 0.476 0.562
Table 12: Effective of Query Augmentation vs Document Augmentation
nCDG@1 nCDG@5 nCDG@10 Recall@1 Recall@5 Recall@10
HyDE (Gao et al., 2022b) 0.427 0.442 0.478 0.221 0.472 0.576
Ski-Q-1 0.452 0.454 0.482 0.232 0.476 0.562
Ski-QC-1 0.455 0.458 0.493 0.230 0.480 0.584
Ski-QC-ASM 0.398 0.398 0.435 0.193 0.429 0.541