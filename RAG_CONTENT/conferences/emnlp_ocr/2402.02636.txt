Can Large Language Models Learn Independent Causal Mechanisms?
Ga¨ el Gendron Bao Trung Nguyen Alex Yuxuan Peng Michael Witbrock
Gillian Dobbie
NAOInstitute, University of Auckland
Abstract
Despite impressive performance on language
modelling and complex reasoning tasks, Large
Language Models (LLMs) fall short on the same
tasks in uncommon settings or with distribu-
tion shifts, exhibiting a lack of generalisation
ability. By contrast, systems such as causal
models, that learn abstract variables and causal
relationships, can demonstrate increased robust-
ness against changes in the distribution. One
reason for this success is the existence and use
of Independent Causal Mechanisms (ICMs) rep-
resenting high-level concepts that only sparsely
interact. In this work, we apply two concepts
from causality to learn ICMs within LLMs. We
develop a new LLM architecture composed of
multiple sparsely interacting language modelling
modules. We show that such causal constraints
can improve out-of-distribution performance on
abstract and causal reasoning tasks. We also in-
vestigate the level of independence and domain
specialisation and show that LLMs rely on pre-
trained partially domain-invariant mechanisms
resilient to fine-tuning.
1 Introduction
The latest generation of Large Language Models (LLMs)
with over several billion parameters has demonstrated im-
pressive performance on an extensive range of in-context
language and reasoning tasks [Bubeck et al. , 2023; Brown
et al. , 2020; Wei et al. , 2022b,a; Touvron et al. , 2023a]
and an even greater range when fine-tuned for a specific
task [Touvron et al. , 2023b; Hu et al. , 2022]. However,
these observations do not hold for tasks that fall outside
the training data distribution, sometimes even when the
task is only slightly perturbed. In particular, standard
LLMs perform poorly on complex reasoning tasks, such
as abstract, causal, or logical reasoning [Wu et al. , 2023;
Gendron et al. , 2023a; Zecevic et al. , 2023; Jin et al. ,
2023; Liu et al. , 2023; Bao et al. , 2023]. Gendron et
al.[2023a]; Jin et al. [2023]; Wu et al. [2023] showed
that fine-tuning LLMs can increase their in-distribution
performance, but the improvement does not transfer todifferent distributions, highlighting that LLMs do not
generalise as we might expect a person to when applied to
domains requiring complex reasoning. Several hypotheses
have been proposed to explain this flaw, such as the lack
of abstract or symbolic representations within the latent
space of LLMs [Wu et al. , 2023; Gendron et al. , 2023a;
Goyal and Bengio, 2020]. These claims are supported by
the brittleness that LLMs can exhibit; when changing the
wording of a question, the performance of an LLM can
vary drastically [Wei et al. , 2022b; Jin et al. , 2023]. This
observation hints that LLMs may rely on domain-specific
information or spurious correlations in the training data
that do not generalise to other distributions.
Causal models rely on the concept that causal mecha-
nisms invariant under changes in environment exist. The
Independent Causal Mechanisms principle further states
that“the causal generative process of a system’s variables
is composed of autonomous modules that do not inform
or influence each other.” [Peters et al. , 2017; Sch¨ olkopf
et al. , 2021]. These principles are applied in diverse ways
in the field of causality, either in the structure of the
model, which may be built in a modular fashion to re-
spect causal relationships, as in Structural Causal Models
[Pearl, 2009], or in the distribution of the data, which
may be rendered independent and identically distributed
(i.i.d) from an unbalanced distribution by division into
subgroups [Austin, 2011; Gendron et al. , 2023c]. Inte-
grating these methods into the architecture of a Large
Language Model could increase its robustness and out-
of-distribution (o.o.d) generalisation.
We investigate this idea in this work: we aim to better
understand how LLMs reason in and out-of-distribution
and whether they can behave as models of Independent
Causal Mechanisms under certain constraints and with
fine-tuning. To this end, we propose an LLM architecture
integrating the concept of mechanisms as independent,
self-contained LLM modules. This model is summarised
in Figure 1. We aim to answer the following questions:
(i) Can LLMs be used as self-routers for specialised mech-
anisms, and does it improve their performance? (ii)
Can LLMs capture domain-invariant abstractions with
information-based regularisation? (iii) How useful is
domain-specific knowledge on reasoning tasks? and (iv)
Can our proposed architecture approximate IndependentarXiv:2402.02636v2  [cs.CL]  10 Sep 2024Causal Mechanisms? Our contributions can be sum-
marised as follows:
•We propose a modular LLM architecture yielding
modularity and abstraction in LLMs using routing
and regularisation mechanisms.
•We investigate the ability of LLMs to behave as
Independent Causal Mechanisms on reasoning tasks
and show that it can lead to improved performance
and o.o.d generalisation.
•We show that LLMs approximate independent mech-
anism up to an extent but always partially rely on
pre-trained domain-invariant mechanisms for reason-
ing tasks.
Our code is available at https://github .com/Strong-
AI-Lab/modular-lm.
2 Related Work
LLM Mixtures-of-Experts Modular architectures
divide the computations of a network into sub-networks.
The Switch Transformer [Fedus et al. , 2022b] separates
the feed-forward layers of the transformer model [Vaswani
et al. , 2017] into multiple expert modules. This strategy
allows training larger models at a lower cost, but the
expert modules are not guaranteed to specialise in spe-
cific domains. Multiple sparse architectures have followed
but mainly focus on optimising the training of LLMs for
reduced resources and not inducing domain specialisa-
tion [Fedus et al. , 2022a]. One exception is the work of
Gururangan et al. [2022], which conditions the activation
of an expert module on the input domain. However, only
the feed-forward layers are used as experts and the do-
mains are assumed to be known during training. Clark et
al.[2022] investigate the performance of various routing
strategies for LLMs and show that the gain from using
specialised modules is high for small models but decreases
as the model size increases. Introduced recently, Mixtral-
of-Experts is a modular LLM using the same routing
principle as the Switch Transformer. It outperforms
dense LLMs of similar size on reading comprehension,
commonsense knowledge and reasoning tasks [Jiang et
al., 2024]. However, the authors observe that the routing
process does not lead to domain-specialised modules. The
assignment of experts is not based on domain information.
Our work differs in that it is not directed at optimising
LLM training but at inducing functional modularity and
studying its effects on generalisation for reasoning tasks.
Modular Neural Networks Other classes of modu-
lar neural models are designed to learn specialised sub-
networks for specific domains. Recurrent Independent
Mechanisms [Goyal et al. , 2021] attempt to learn models
of independent mechanisms with an LSTM architecture
[Hochreiter and Schmidhuber, 1997] to model the dynam-
ics of physical objects. Mittal et al. [2022] investigate
routing mechanisms for Mixture-of-Experts models. They
find that specialisation can yield better results as the
number of tasks increases. However, the learned rout-
ing strategies do not capture domain specialisation. Inparticular, approaches based on backpropagation to the
task loss often collapse to a single module. Mittal et al.
[2022]’s experiments are restricted to small models and
synthetic binary classification and regression tasks; we
study a novel routing method via vector quantisation
and perform our experiments on architectures over 1B
parameters on reasoning tasks.
Causal Models Causal models aim to answer queries
requiring knowledge of the causal relationships linking
the data [Bareinboim et al. , 2022]. Sch¨ olkopf et al. [2021];
Goyal and Bengio [2020] argue that for artificial systems
to achieve robust and o.o.d reasoning, they must reason
in terms of causes and effects and not only correlations,
which current LLMs cannot do yet [Bareinboim et al. ,
2022; Zecevic et al. , 2023]. Structural Causal Models
(SCMs) are graphical models representing causal rela-
tionships as mapping functions from parent nodes to
their child nodes in a Directed Acyclic Graph [Pearl,
2009]. If fully specified, an SCM can represent the com-
plete inner workings of a system. However, building
an SCM requires access to high-level causal variables,
which is not the case in many deep learning tasks that
take low-level observations as inputs [Sch¨ olkopf et al. ,
2021]. The do-calculus , defined by Pearl [1995, 2009],
is used to identify the causal effect of a variable on an-
other with the help of the dooperator: do(·) represents
an intervention, i.e. the forced attribution of a value
to a variable. If P(Y|do(X)) = P(Y), then Xhas no
causal effect on Y(they may still be correlated if they
share common ancestors). Another class of causal mod-
els relies on determining the flow of information in a
system [Shannon, 1948; Paluˇ s et al. , 2001; Schreiber,
2000]. However, these concepts have yet to be applied
to language models. In the domain of transformers, the
Causal Transformer [Melnychuk et al. , 2022] and Causal
Attention [Yang et al. , 2021] introduce cross-attention
mechanisms to reduce biases from the training distribu-
tion.
3Causal Information Routing for LLMs
We now describe our proposed modular architecture: In-
dependent Causal Language Models (ICLM), where each
module is an LLM fine-tuned for a specific specialisation
or generalisation objective. We aim to build a system
that can adapt to changing distributions and capture bet-
ter abstractions. Our architecture is separated into N+2
LLM modules connected by three main components .
The LLM modules are composed of a router that gener-
ates embeddings of the inputs, a domain-invariant module
trained to learn abstractions and Ndomain-specific mod-
ulestrained to specialise on a single task or domain. The
other components are the routing strategy ,Mutual In-
formation loss and the aggregation scheme . The routing
strategy uses the embeddings from the LLM router to redi-
rect the inputs to a specific module. Specifically, routing
is performed in an unsupervised fashion: each embedding
is projected into a clustered space. The centroid of each
cluster is associated with a domain-specific module toMutual 
Information 
Minimisation
Routing 
Strategy
LLM 
Modules
Input
Text
Router
Domain 
Specific 
1
Domain 
Specific 
N
Aggregation 
Scheme
Domain
Invariant
DecoderFigure 1: Proposed Independent Causal Language Models (ICLM) architecture for language-modelling tasks. The
input text (on the left, in blue) is fed to multiple pretrained LLM modules (in red). A router uses clustering on input
text embeddings (in purple) to activate a domain-specific module for this input. The domain-invariant module is
always activated. The latent representations generated by the activated modules are combined using an aggregation
scheme (in orange) and converted into a probability distribution for the next word (on the right, in blue). An
additional loss (in green) minimises the Mutual Information between the domain-invariant and the domain-specific
representations. The router ensures that the domain-specific modules only gain in-domain knowledge while the
Mutual Information loss regularises the domain-invariant module towards learning abstract representations.
which it assigns a binary activation weight for a given
input. If the input belongs to a cluster, the corresponding
module is activated. By contrast, the domain-invariant
module processes all inputs. The Mutual Information loss
induces abstraction within the domain-invariant module;
minimising this loss reduces shared information between
the domain-specific and domain-invariant modules. I.e. it
is intended to cause the domain-invariant module to gain
domain-invariant knowledge and the domain-specific mod-
ules domain-specific knowledge. Finally, the aggregation
scheme combines the output of the activated domain-
specific module and the domain-invariant module to pro-
duce the final output. Figure 1 shows an overview of
our method. We describe the routing strategy in Section
3.1, the Mutual Information minimisation in Section 3.2
and the aggregation scheme in Section 3.3. Section 4 dis-
cusses how the architecture reflects Independent Causal
Mechanisms.
3.1 Routing Strategy
The routing strategy redirects the input tokens to a
domain-specific module. This step divides the inference
into independent modules to increase the specialisation
of each module and reduce spurious distribution biases.
In particular, the distribution may be imbalanced: a
data class may dominate the training distribution and
spuriously drive the gradients in a dense model. The
routing module is used to balance out the distribution.
The inputs belonging to the dominant class are restricted
to a single module and cannot adversarially affect theother modules. In parallel, data points far from this data
class are trained on a specialised module.
We use a pre-trained LLM (the router) with no final
language modelling layer to build an input embedding
space. The embeddings serve as inputs to the unsuper-
vised routing strategy. In the strategy, all modules receive
the inputs, but the outputs of non-activated modules are
blocked . I.e. their outputs are associated with a weight of
zero (activated modules have a weight equal to one). This
activation process by weighting allows us to study more
complex (non-binary) weighting schemes, discussed in
Appendix B. This unsupervised learning method grants
more flexibility than the matrix multiplication used in
sparse transformers, as any clustering algorithm can be
used. In particular, in continual learning settings, one
could imagine using a varying number of clusters [Ester
et al. , 1996] and dynamically allocating new modules as
data is being fed to the router. In our work, we restrict
ourselves to simple clustering methods as we find that
they are sufficiently fine-grained for our tasks. We per-
form clustering at the input level, i.e. each point in the
clustering space represents a complete input context.
Vector Quantisation We use the vector quantisation
procedure introduced for the VQ-VAE van den Oord
et al. [2017] as a clustering method. Nvectors hcare
arbitrarily initialised in the embedding space, acting as
cluster centroids. The attribution of an input to a cluster
is determined by measuring the shortest Euclidean dis-
tance between them. The router generates an embedding
for each token in the input so we measure the distancebetween a centroid and each token and sum them to
obtain the total distance. The location of the centroids
is iteratively updated to move closer to the input em-
beddings using vector quantisation. The corresponding
routing loss is defined as follows:
LR= MSE( sg(hc), hr) +ν·MSE( hc, sg(hr)) (1)
with hrone token embedding and hcthe coordinates of
the selected centroid, sgis the stop gradients operation,
andνis a hyperparameter. This method has been very
successful in transposing high-level concepts from a con-
tinuous to a discrete space [Bao et al. , 2022; Ramesh et
al., 2021] and in building disentangled or interpretable
semantic spaces [Gendron et al. , 2023b; Yu et al. , 2023].
This approach is simple and assumes clusters with non-
overlapping convex hulls. We consider more complex
strategies in Appendix B.
3.2 Mutual Information Minimisation
The second aim of the architecture is to induce abstrac-
tion and domain-invariance in LLMs. To this end, we
introduce a regularisation process based on information
theory. We minimise the Mutual Information (I) [Shan-
non, 1948; Kreer, 1957] between the domain-specific and
domain-invariant modules. Specifically, we minimise the
information between the last hidden states of the mod-
ules. The idea is to drive the domain-specific modules to
gain knowledge specific to their distribution only, while
the domain-invariant module gains knowledge common
to all distributions and discards the domain-specific in-
formation that could be detrimental to generalisation.
The Mutual Information between two random processes
corresponds to the dependence between the two processes,
i.e. the amount of information gained on the first process
by observing the second one. The Mutual Information
between two random variables HI∈ H andHS∈ H is
given by:
I(HI, HS) = KL( PHI,HS||PHI⊗PHS) (2)
where KL is the Kullback-Leibler divergence [Kullback
and Leibler, 1951], HIis the random variable representing
the last hidden state of the domain-invariant module and
HSis its counterpart in one domain-specific module. The
hidden states are interpreted as logits distributed in a
feature space H.PHI,HSis their joint distribution, and
PHIandPHSare their marginals. They are later decoded
using a final linear layer into the space of possible next
words corresponding to the vocabulary of the LLM. The
total loss LIis given by the total information shared
between the domain-invariant and all Ndomain-specific
modules:
LI=X
n∈[1,N]I(HI, HSn) (3)
The probabilities PHI(h) and PHS(h) cannot be di-
rectly computed for any given hidden state h∈ H. We
can only access the probabilities PHI(h|c) and PHS(h|c)for a given input context c∈ C. The marginalisation on
Cis intractable because of the exponential input space:
|C|=VL, with Vthe vocabulary size of the LLM and
Lthe maximum length of the input sequence (typically
VL= (32 .103)4096). We can approximate it by sampling
Cat the batch level B:P(h) =P
c∈CP(h|c)·P(c)≈
1
|B|P
c∈BP(h|c) with |B| ≪ |C| . We do the same with
the joint distribution PHI,HS.
3.3 Aggregation of Outputs
Before aggregating the domain-invariant and domain-
specific modules, we perform a shared batch normalisa-
tion [Ioffe and Szegedy, 2015] between their last hidden
states. For a batch of size |B|, one domain-specific ac-
tive module and one domain-invariant module, batch
normalisation is operated on 2 × |B| samples. Batch
normalisation ensures that the module outputs have the
same mean and variance. We then use a standard lan-
guage modelling head that converts the hidden states into
a probability distribution for the next token. The lan-
guage modelling head is a fully connected layer that takes
the concatenated hidden states as inputs and outputs a
probability distribution in the vocabulary of the language
model. This is a simple aggregation method with great
expressivity due to the shared final dense layer. However,
this layer can be subject to biases, e.g. if prioritising
information from one module at the expense of the others.
We study other aggregation schemes, less expressive but
more resilient to this issue, in Appendix C.
Loss Function The total training loss is composed of
five components:
L=Lo+α· Linv+β· Ldom+γ· LR+ϵ· LI(4)
Lois the self-supervised cross-entropy loss between the
output logits of ICLM and the target text. Linvand
Ldomare cross-entropy losses between the output log-
its of the invariant module and those of the activated
domain-specific module. LRis the vector quantisation
loss obtained from the routing strategy (Eq. 1). LIis the
Mutual Information loss (Eq. 3). We consider three sep-
arate self-supervised losses Lo,LinvandLdomto induce
the modules to match the target distributions individu-
ally and prevent collapse to a single useful module. α,β,
γandϵare constant hyperparameters.
4 Theoretical Perspective
In this section, we provide theoretical evidence on how
our model approximates Independent Causal Mechanisms
and under what assumptions. Independent Causal Mech-
anisms consist of autonomous modules that work in-
dependently. In our case, all domain-specific modules
are trained for specific tasks/distributions. The domain-
invariant module is trained only to use domain-invariant
knowledge. The router module is tasked to split the in-
put distribution into Nmore balanced distributions. We
aim to verify that the modules are not causally related.HSn HSHR
C
HIYYtrue
LYτLRτ
WSnWR
WI LYτ−1LRτ−1τ→τ+ 1
τ→τ+ 1
Figure 2: Simplified temporal causal graph Gduring
training before adding Mutual Information minimisation.
C is the input context. HR,HI,HSn,HSare the latent
states of the router, domain-invariant, domain-specific
and activated domain-specific (after router weighting)
modules. For simplicity, we only show the state HSn
of the activated domain-specific module n.YandYtrue
are the output and true distributions. WR,WSnand
WIare the trainable parameters of the modules. LY=
Lo+α· Linv+β· LdomandLRare the output and
router losses. Black edges show the forward pass at step
τ. Blue dashed edges show the backward pass at step τ.
Red dotted edges illustrate the causal links between the
forward and backward passes.
More formally, we aim to study under what conditions
the following holds:
P(HR|do(HSn)) =P(HR)∀n∈[1, N] (5)
P(HR|do(HI)) =P(HR) (6)
P(HSn|do(HSˆn)) =P(HSn)
∀ˆn∈[1, N]\ {n} ∀n∈[1, N](7)
P(HI|do(HSn)) =P(HI)∀n∈[1, N] (8)
HR,HIandHSn∀n∈[1, N] are the respective repre-
sentations generated by the router, domain-invariant and
Ndomain-specific modules.
Equations 5 and 6 are verified. The proof is provided
in Appendix A; the main idea is that the use of a sep-
arate loss function for training the router prevents the
other modules from causally acting on the router, ei-
ther in the forward or backward passes. It can be ver-
ified in Figure 2. However, if an invariant module is
part of the model, Equations 7 and 8 do not hold. The
domain-specific modules do not directly influence each
other because the routing mechanism allows a single
module to go through the forward and backward passes.
Nevertheless, a causal path can be drawn through the
domain-invariant module as it is always activated. For
example, assuming a model with two domain-specific
modules, S0andS1, activated one after the other, a path
exists and can be represented in a simplified version asHS1...− → L Yτ...− →HI...− → L Yτ+1...− →HS2. Again, details
of the proof are given in Appendix A. As HIandHSn
are causally related, we need to reduce the dependency
between the two quantities using a regularisation term.
Minimising the Mutual Information between HIandHSn
amounts to reducing the mutual dependence between the
variables. I(HI, HSn) = 0 if and only if HIandHSn
are independent. If verified, the loss LYcan be divided
into two independent components and Equations 7 and 8
hold. We verify experimentally in Appendix D.1 that the
Mutual Information is close to zero after ∼50 training
steps.
5 Abstract Reasoning with ICLM
5.1 Experimental Setup
By default, we use N= 2 domain-specific modules and
one domain-invariant module, as the datasets we use con-
tain two subdomains each. We also perform experiments
with an ablated model that does not have a domain-
invariant module. In addition, we study the individual
performance of the domain-invariant and domain-specific
modules. We use a pretrained LLaMA2-7B [Touvron et
al., 2023b] for all our modules. We use Low-Rank Approx-
imation of LLMs (LoRA) [Hu et al. , 2022] to fine-tune
the modules on their respective tasks. All models are
fine-tuned for 3 epochs with AdamW [Loshchilov and
Hutter, 2019] and a batch size of 16. Loss hyperparame-
ters are α= 0.1,β= 0.1,γ= 0.1,ϵ= 0.01,ν= 0.25. It
is worth noting that the number of parameters used is
only marginally higher than that of the base LLaMA2,
as only low-memory LoRA adapter weights are learned
during training.
5.2 Datasets
We perform experiments on the text-based ACRE and
RAVEN datasets [Zhang et al. , 2021a, 2019; Gendron et
al., 2023a]1. ACRE and RAVEN are adapted from Vi-
sual Question Answering datasets to be used by language
models. The visual ACRE [Zhang et al. , 2021a] is an
abstract causal reasoning dataset where the model must
deduce the causal mechanisms from a small set of image
examples. The visual RAVEN [Zhang et al. , 2019] is an
abstract reasoning dataset where the model must com-
plete a sequence of Raven Progressive Matrices [Raven,
1938]. The text ACRE and RAVEN contain descriptions
of the images and instructions for solving the task. The
descriptions are provided in two formats: symbolic and
natural language. The chosen datasets require knowl-
edge of the underlying causal mechanisms to be solved
and have o.o.d sets to challenge this ability in the tested
systems.
Out-of-Distribution Regimes Each dataset has two
o.o.d regimes. In ACRE, the compositionality split
changes the composition of the context examples: combi-
nations of figure shapes and colours unseen in the training
1We use the data provided at https://github .com/Strong-
AI-Lab/Logical-and-abstract-reasoning.set are proposed; the systematicity split alters the distri-
bution of the context example activations: the context
contains more positive examples than in the training set.
In RAVEN, the four split contains four figures instead
of one; the in-center split describes two figures with one
containing the other instead of being placed next to each
other.
5.3 Abstract and Causal Reasoning
The results obtained on the ACRE and RAVEN datasets
are shown in Table 1. The proposed ICLM can outper-
form the baseline, particularly on the most challenging
o.o.d sets. Moreover, the performance of the individual
domain-invariant modules highlights that the modules
have learned more generalisable knowledge than with
standard training. The domain-specific modules com-
pete with the baselines trained on the corresponding
specific domain, showing that the router accurately dis-
tributes the inputs to the right modules. The modules
even outperform the oracle router on RAVEN in almost
all settings. We investigate a potential reason for this
phenomenon in Section 6.2.
5.4 Continual Learning
We investigate the capacity of our model to be used in
continual learning settings. Continual learning consists
of training a model with continuous data streams or sets
evolving over time, where the model acquires and accumu-
lates knowledge incrementally. The main challenge lies
in the catastrophic forgetting of the previous knowledge
when gaining new information [Wang et al. , 2023]. We
study a simple usecase where we want our model to learn
one new task after training on a previous task. We choose
the scenario ACRE →RAV EN as RAVEN is more chal-
lenging, particularly the o.o.d sets. The results are shown
in Table 2. The domain-invariant module can use general
information extracted from ACRE to improve its per-
formance on RAVEN, even outperforming the baseline
trained on RAVEN only. The domain-specific modules
can also partially mitigate the catastrophic forgetting
problem observed in LLaMA2. Their weights are not acti-
vated by the router on RAVEN inputs, thus not updated,
and their performance on ACRE is preserved. However,
the aggregation process is affected, leading to reduced
performance on ACRE Text.
6 Routing and Independence Analysis
6.1 Evolution of Module Independence
We study the independence of the module hidden states
during training (Figure 3) and inference (Figure 4). we fo-
cus on two complementary measures: Mutual Information
and the Pearson Correlation Coefficient that measures
linear correlation between variables. The latter is limited
to linear dependence but is more easily interpretable.
The shared Mutual Information as well as the Pearson
Correlation Coefficient between modules are effectively
reduced by the regularisation scheme during fine-tuning.
However, the module hidden states remain correlated,in particular at inference time. Further investigation in
Appendix D.5 further shows that this correlation is main-
tained across most layers, which indicates the presence of
a general domain-invariant mechanism shared by all mod-
ules and composing the basis of their reasoning abilities.
Its influence is reduced via the fine-tuning procedure that
develops domain-specific knowledge but it remains the
main mechanism used, particularly at test time.
(a) ACRE Correlation.
 (b) ACRE MI.
(c) RAVEN Correlation.
 (d) RAVEN MI.
Figure 3: Evolution of independence measures between
modules during fine-tuning on ACRE and RAVEN. We
measure independence on the last hidden states of the
modules. Correlation and MI are highly reduced but
modules remain correlated.
Dom 1Dom 0Inv 1 0.7 0.7
0.7 1 0.63
0.70.63 1
(a) ACRE.Dom 1Dom 0Inv 1 0.70.69
0.7 1 0.64
0.69 0.64 1
(b) ACRE o.o.d.
Dom 1Dom 0Inv 1 0.77 0.79
0.77 1 0.78
0.79 0.78 1
(c) RAVEN.Dom 1Dom 0Inv 1 0.77 0.79
0.77 1 0.78
0.79 0.78 1
(d) RAVEN o.o.d.
Figure 4: Correlation between the last hidden states of
the modules during inference at test time. Module states
are more correlated than during training.
6.2 Routing Alignment
We look deeper at the embedding space in the routing
module, projected into a 2D space using Multidimen-
sional Scaling (MDS) [Borg and Groenen, 2005]. FigureACRE -o.o.d-Comp -o.o.d-Sys
Text Symb Text Symb Text Symb
LLaMA2-Base 0.014 0.003 0.244 0.001 0.288 0.001
-Finetuned-All* 0.832 0.891 0.832 0.881 0.911 0.891
ICLM* (ours) 0.653 0.950 0.663 0.931 0.634 0.901
ICLM-No-Inv* (ours) 0.871 0.921 0.842 0.941 0.822 0.891
ICLM-Invariant* (ours) 0.891 0.921 0.851 0.941 0.921 0.891
ICLM-Domain* (ours) 0.871 0.911 0.822 0.901 0.822 0.891
-Finetuned-Oracle-Router 0.997 1.000 1.000 1.000 0.994 0.999RAVEN -o.o.d-Four -o.o.d-In-Center
Text Symb Text Symb Text Symb
0.026 0.149 0.073 0.121 0.000 0.001
0.990 1.000 0.673 0.743 0.673 0.198
1.000 0.980 0.703 0.703 0.515 0.228
1.000 0.732 0.525 0.515 0.455 0.168
1.000 0.990 0.634 0.693 0.554 0.238
0.980 0.980 0.604 0.634 0.386 0.228
0.977 0.965 0.557 0.442 0.536 0.064
Table 1: Accuracy on the ACRE and RAVEN i.i.d and o.o.d test sets. “Finetuned-All” is a single LLaMA2 model
fine-tuned on text and symbolic i.i.d training sets. “Finetuned-Oracle-Router” is an ensemble of two LLaMA2 models
fine-tuned each i.i.d training set (either text or symbolic) and routed via a ground-truth oracle. ICLM is trained
on text and symbolic i.i.d training sets. ICLM-Invariant shows the results for the domain-invariant module alone.
ICML-Domain shows the results for the domain-specific module that aligns best with the dataset (see Appendix D.2).
ICLM-No-Inv is an ablated ICLM with no domain-invariant module. Models with a∗indicate that that this paper
introduces the results. The best model is in bold , and the second best is in italics . ICLM outperform LLaMA2 on
most sets and individual modules even outperform the oracle on the more challenging RAVEN.
ACRET-Comp -Sys RAVENT-Four -In-Center
Text Symb Text Symb Text Symb Text Symb Text Symb Text Symb
ICLM ACRE * (Table 1) 0.653 0.950 0.663 0.931 0.634 0.901 - - - - - -
ICLM RAV EN * (Table 1) - - - - - - 1.000 0.980 0.703 0.703 0.515 0.228
ACRE →RAV EN
ICLM* (ours) 0.089 0.901 0.119 0.931 0.050 0.871 1.000 0.990 0.772 0.772 0.833 0.248
ICLM-Invariant* (ours) 0.287 0.396 0.277 0.416 0.238 0.455 1.000 0.970 0.673 0.723 0.723 0.238
LLaMA2-Finetuned-Sequential* 0.079 0.376 0.149 0.386 0.089 0.426 0.980 0.772 0.634 0.554 0.584 0.069
Table 2: Accuracy on ACRE and RAVEN when the model is trained sequentially. ICLM can use the information from
ACRE to improve its performance on RAVEN, outperforming the baseline trained on RAVEN only, while preserving
more knowledge from the previous task than the base LLaMA2-7B finetuned sequentially. In particular, the accuracy
of ACRE-Symbolic sets is almost untouched.
5 shows the embedding spaces and their attribution to
the domain-specific modules. Detailed attributions are
shown in Appendix D.2. Both datasets have a clear divi-
sion between text and symbolic embeddings. However,
the o.o.d sets are not well separated in ACRE while they
are in RAVEN. This division can explain the similarity
in the results between the ACRE i.i.d and o.o.d sets, as
shown in Table 1. Moreover, as the distributions are
very similar, the impact of the router and the need for
abstraction are reduced. On the other hand, there is a
clear separation between the i.i.d and o.o.d RAVEN em-
beddings, explaining the differences in behaviours from
the models across the sets. Adding more modules could
allow taking more advantage of this separation, with each
module specialising to a subdomain closer to one of the
o.o.d embeddings.
7 Conclusion
Performing strong out-of-distribution reasoning is a chal-
lenging task, and despite their impressive performance
on a wide range of problems, LLMs have not demon-
strated this ability yet. Combining this popular model
with causal models could help bridge this gap. This
work presents a modular architecture yielding LLMs to
behave as Independent Causal Mechanisms. We show
theoretically that the proposed model generates causally-
independent modules. We perform experiments on ab-
stract and causal reasoning tasks in o.o.d and continual
learning settings and show that these principles increase
(a) ACRE Ground Truth
 (b) Vector quantisation router
(c) RAVEN Ground Truth
 (d) Vector quantisation router
Figure 5: 2D projection of the hidden states of LLaMA2
on ACRE and RAVEN i.i.d and o.o.d sets. Ground
truth samples are labelled as in Table 1 (text/symbolic
i.i.d/o.o.d sets). Text and symbolic inputs are always
clustered separately. i.i.d and o.o.d sets are clustered
together in ACRE and separated in RAVEN. The router
follows the text and symbolic division.
strong reasoning and generalisation. We further show
that the proposed modules specialise to their domain with
fine-tuning but still partially rely on a shared domain-
invariant mechanism, highlighting a limitation for repre-
senting ICMs with LLMs.Limitations
The model proposed in this paper is constrained to work
in a modular manner. All modules are sparsely connected
at the level of the language modelling head. This sin-
gle connection offers a useful initial framework to study
the ICMs within the context of LLMs and can repre-
sent a wide range of problems (requiring the composition
of several independent reasoning processes) but it can
only represent causal DAGs with a single layer depth,
potentially hindering the expressivity of more complex
mechanism interactions. Generating complex causal com-
putation graphs tailored to the task at hand may improve
performance, but this problem is out of the scope of this
paper.
We also focus our investigation on the independence
and accuracy of the modules and do not attempt to di-
rectly represent the true causal mechanisms of the tasks
as they are unknown. Moreover, we conduct experiments
on reasoning tasks to verify if inducing high-level modular-
ity can yield increases in performance and generalisation.
We aim not to outperform the state-of-the-art on the
problems but to study whether the proposed mechanisms
can yield such increases.
In addition, training and fine-tuning Large Language
Models has a high computational cost. Due to this high
cost, we perform a single fine-tuning run per task and
conduct experiments on this model.
References
Peter C. Austin. An introduction to propensity score
methods for reducing the effects of confounding in ob-
servational studies. Multivariate Behavioral Research ,
46(3):399–424, 2011. PMID: 21818162.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
Beit: BERT pre-training of image transformers. In
The Tenth International Conference on Learning Rep-
resentations, ICLR 2022, Virtual Event, April 25-29,
2022. OpenReview.net, 2022.
Qiming Bao, Ga¨ el Gendron, Alex Yuxuan Peng, Wanjun
Zhong, Neset Tan, Yang Chen, Michael Witbrock, and
Jiamou Liu. A systematic evaluation of large language
models on out-of-distribution logical reasoning tasks.
CoRR , abs/2310.09430, 2023.
Elias Bareinboim, Juan D. Correa, Duligur Ibeling, and
Thomas Icard. On pearl’s hierarchy and the foun-
dations of causal inference. In Hector Geffner, Rina
Dechter, and Joseph Y. Halpern, editors, Probabilis-
tic and Causal Inference: The Works of Judea Pearl ,
volume 36 of ACM Books , pages 507–556. ACM, 2022.
Ingwer Borg and Patrick JF Groenen. Modern multidi-
mensional scaling: Theory and applications . Springer
Science & Business Media, 2005.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. In Proceedings of the 34th Inter-
national Conference on Neural Information Processing
Systems , NIPS’20, Red Hook, NY, USA, 2020. Curran
Associates Inc.
S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.
Sparks of artificial general intelligence: Early experi-
ments with gpt-4. March 2023.
Fran¸ cois Chollet. On the measure of intelligence. CoRR ,
abs/1911.01547, 2019.
Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur
Mensch, Michela Paganini, Jordan Hoffmann, Bogdan
Damoc, Blake A. Hechtman, Trevor Cai, Sebastian
Borgeaud, George van den Driessche, Eliza Ruther-
ford, Tom Hennigan, Matthew J. Johnson, Albin Cas-
sirer, Chris Jones, Elena Buchatskaya, David Bud-
den, Laurent Sifre, Simon Osindero, Oriol Vinyals,
Marc’Aurelio Ranzato, Jack W. Rae, Erich Elsen, Ko-
ray Kavukcuoglu, and Karen Simonyan. Unified scaling
laws for routed language models. In Kamalika Chaud-
huri, Stefanie Jegelka, Le Song, Csaba Szepesv´ ari,
Gang Niu, and Sivan Sabato, editors, International
Conference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , volume 162
ofProceedings of Machine Learning Research , pages
4057–4086. PMLR, 2022.
Martin Ester, Hans-Peter Kriegel, J¨ org Sander, and Xi-
aowei Xu. A density-based algorithm for discovering
clusters in large spatial databases with noise. In kdd,
volume 96, pages 226–231, 1996.
William Fedus, Jeff Dean, and Barret Zoph. A review
of sparse expert models in deep learning. CoRR ,
abs/2209.01667, 2022.
William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with
simple and efficient sparsity. J. Mach. Learn. Res. ,
23:120:1–120:39, 2022.
Ga¨ el Gendron, Qiming Bao, Michael Witbrock, and
Gillian Dobbie. Large language models are not ab-
stract reasoners. CoRR , abs/2305.19555, 2023.
Ga¨ el Gendron, Michael Witbrock, and Gillian Dobbie.
Disentanglement of latent representations via causal
interventions. In Proceedings of the Thirty-Second In-
ternational Joint Conference on Artificial Intelligence,
IJCAI 2023, 19th-25th August 2023, Macao, SAR,
China , pages 3239–3247. ijcai.org, 2023.
Ga¨ el Gendron, Michael Witbrock, and Gillian Dobbie.A survey of methods, challenges and perspectives in
causality. CoRR , abs/2302.00293, 2023.
Anirudh Goyal and Yoshua Bengio. Inductive biases
for deep learning of higher-level cognition. CoRR ,
abs/2011.15091, 2020.
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun
Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard
Sch¨ olkopf. Recurrent independent mechanisms. In 9th
International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net, 2021.
Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A.
Smith, and Luke Zettlemoyer. Demix layers: Disen-
tangling domains for modular language modeling. In
Marine Carpuat, Marie-Catherine de Marneffe, and
Iv´ an Vladimir Meza Ru´ ız, editors, Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL 2022, Seattle,
WA, United States, July 10-15, 2022 , pages 5557–5576.
Association for Computational Linguistics, 2022.
Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-
term memory. Neural Comput. , 9(8):1735–1780, 1997.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language
models. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net, 2022.
Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing inter-
nal covariate shift. In Francis R. Bach and David M.
Blei, editors, Proceedings of the 32nd International
Conference on Machine Learning, ICML 2015, Lille,
France, 6-11 July 2015 , volume 37 of JMLR Workshop
and Conference Proceedings , pages 448–456. JMLR.org,
2015.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,
Arthur Mensch, Blanche Savary, Chris Bamford, De-
vendra Singh Chaplot, Diego de Las Casas, Emma Bou
Hanna, Florian Bressand, Gianna Lengyel, Guillaume
Bour, Guillaume Lample, L´ elio Renard Lavaud, Lu-
cile Saulnier, Marie-Anne Lachaux, Pierre Stock,
Sandeep Subramanian, Sophia Yang, Szymon Anto-
niak, Teven Le Scao, Th´ eophile Gervet, Thibaut Lavril,
Thomas Wang, Timoth´ ee Lacroix, and William El
Sayed. Mixtral of experts. CoRR , abs/2401.04088,
2024.
Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrin-
maya Sachan, Rada Mihalcea, Mona T. Diab, and
Bernhard Sch¨ olkopf. Can large language models infer
causation from correlation? CoRR , abs/2306.05836,
2023.
J. Kreer. A question of terminology. IRE Transactions
on Information Theory , 3(3):208–208, 1957.Solomon Kullback and Richard A Leibler. On information
and sufficiency. The annals of mathematical statistics ,
22(1):79–86, 1951.
Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji
Zhou, and Yue Zhang. Evaluating the logical reasoning
ability of chatgpt and GPT-4. CoRR , abs/2304.03439,
2023.
Stuart Lloyd. Least squares quantization in pcm. IEEE
transactions on information theory , 28(2):129–137,
1982.
Ilya Loshchilov and Frank Hutter. Decoupled weight
decay regularization. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019 . OpenReview.net, 2019.
Valentyn Melnychuk, Dennis Frauen, and Stefan Feuer-
riegel. Causal transformer for estimating counterfactual
outcomes. In Kamalika Chaudhuri, Stefanie Jegelka,
Le Song, Csaba Szepesv´ ari, Gang Niu, and Sivan
Sabato, editors, International Conference on Machine
Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA , volume 162 of Proceedings of Machine
Learning Research , pages 15293–15329. PMLR, 2022.
Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie.
Is a modular architecture enough? In Sanmi Koyejo,
S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,
and A. Oh, editors, Advances in Neural Information
Processing Systems 35: Annual Conference on Neural
Information Processing Systems 2022, NeurIPS 2022,
New Orleans, LA, USA, November 28 - December 9,
2022, 2022.
Milan Paluˇ s, Vladim´ ır Kom´ arek, Zbynˇ ek Hrnˇ c´ ıˇ r, and
Katalin ˇStˇ erbov´ a. Synchronization as adjustment of
information rates: Detection from bivariate time series.
Physical Review E , 63(4):046211, 2001.
Judea Pearl. Probabilistic reasoning in intelligent systems:
networks of plausible inference . Morgan kaufmann,
1988.
Judea Pearl. Causal diagrams for empirical research.
Biometrika , 82(4):669–688, 1995.
Judea Pearl. Causality . Cambridge university press, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Sch¨ olkopf.
Elements of causal inference: foundations and learning
algorithms . The MIT Press, 2017.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In Ma-
rina Meila and Tong Zhang, editors, Proceedings of the
38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event , volume
139 of Proceedings of Machine Learning Research , pages
8821–8831. PMLR, 2021.
John C Raven. Raven standard progressive matrices.
Journal of Cognition and Development , 1938.
Bernhard Sch¨ olkopf, Francesco Locatello, Stefan Bauer,
Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal,and Yoshua Bengio. Towards causal representation
learning. CoRR , abs/2102.11107, 2021.
Thomas Schreiber. Measuring information transfer. Phys-
ical review letters , 85(2):461, 2000.
Claude Elwood Shannon. A mathematical theory of
communication. The Bell system technical journal ,
27(3):379–423, 1948.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix,
Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur´ elien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer,
Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal, Anthony
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-
abel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,
Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xi-
aoqing Ellen Tan, Binh Tang, Ross Taylor, Adina
Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kam-
badur, Sharan Narang, Aur´ elien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. Llama
2: Open foundation and fine-tuned chat models. CoRR ,
abs/2307.09288, 2023.
A¨ aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learn-
ing. In Isabelle Guyon, Ulrike von Luxburg, Samy
Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-
wanathan, and Roman Garnett, editors, Advances in
Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems
2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 6306–6315, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett, editors, Advances in Neural Infor-
mation Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, Decem-
ber 4-9, 2017, Long Beach, CA, USA , pages 5998–6008,
2017.Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.
A comprehensive survey of continual learning: Theory,
method and application. CoRR , abs/2302.00487, 2023.
Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew Mingbo Dai, and Quoc V. Le. Finetuned language
models are zero-shot learners. 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,
and Denny Zhou. Chain-of-thought prompting elic-
its reasoning in large language models. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh, editors, Advances in Neural Information Pro-
cessing Systems , volume 35, pages 24824–24837. Curran
Associates, Inc., 2022.
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky¨ urek,
Boyuan Chen, Bailin Wang, Najoung Kim, Jacob An-
dreas, and Yoon Kim. Reasoning or reciting? explor-
ing the capabilities and limitations of language models
through counterfactual tasks. CoRR , abs/2307.02477,
2023.
Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai.
Causal attention for vision-language tasks. In IEEE
Conference on Computer Vision and Pattern Recog-
nition, CVPR 2021, virtual, June 19-25, 2021 , pages
9847–9857. Computer Vision Foundation / IEEE, 2021.
Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolf-
gang Macherey, Yanping Huang, David A. Ross, Irfan
Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy,
Alexander G. Hauptmann, and Lu Jiang. SPAE: se-
mantic pyramid autoencoder for multimodal generation
with frozen llms. CoRR , abs/2306.17842, 2023.
Matej Zecevic, Moritz Willig, Devendra Singh Dhami,
and Kristian Kersting. Causal parrots: Large language
models may talk causality but are not causal. CoRR ,
abs/2308.13067, 2023.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and
Song-Chun Zhu. RAVEN: A dataset for relational
and analogical visual reasoning. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019 , pages
5317–5327. Computer Vision Foundation / IEEE, 2019.
Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun
Zhu, and Yixin Zhu. ACRE: abstract causal reasoning
beyond covariation. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2021, virtual,
June 19-25, 2021 , pages 10643–10653. Computer Vision
Foundation / IEEE, 2021.
Chiyuan Zhang, Maithra Raghu, Jon M. Kleinberg, and
Samy Bengio. Pointer value retrieval: A new bench-
mark for understanding the limits of neural network
generalization. CoRR , abs/2107.12580, 2021.C
HSnHSHR
YYtrue
LYτLRτ
WSnWR
LYτ−1LRτ−1τ→τ+ 1
τ→τ+ 1
(a) Simplified temporal causal graph dur-
ing training with no domain-invariant
module.HSnHSHR
C
HIYYtrue
LYτLRτ
WSnWR
WILYτ−1LRτ−1τ→τ+ 1
τ→τ+ 1
(b) Simplified temporal causal graph G.
The graph is the same as Figure 2.HSnHSHR
C
HIYYtrue
LYτLRτ
LIτWSnWR
WILYτ−1LRτ−1
LIτ−1τ→τ+ 1
τ→τ+ 1
(c) Temporal causal graph with Informa-
tion Minimisation loss added.
Figure 6: Causal graphs with and without domain-invariant module and Mutual Information minimisation loss. C
is the input context. HR,HI,HSn,HSare the latent states of the router, domain-invariant, domain-specific and
activated domain-specific (after router weighting) modules. For simplicity, we only show the state HSnof the activated
domain-specific module n.YandYtrueare the output and true distributions. WR,WSnandWIare the trainable
parameters of the modules. LY=Lo+α· Linv+β· LdomandLRare the output and router losses. Black edges show
the forward pass at step τ. Blue dashed edges show the backward pass at step τ. Red dotted edges illustrate the
causal links between the forward and backward passes. For simplicity, we only show the step for the loss variables as
they appear twice. All other variables are at step τ.
A Supplement to the Theoretical Perspective
In this section, we prove the assertions made in Section 4 of the main paper. We study under what conditions the
following equations (repeated from Section 4) hold:
P(HR|do(HSn)) =P(HR)∀n∈[1, N] (9)
P(HR|do(HI)) =P(HR) (10)
P(HSn|do(HSˆn)) =P(HSn)
∀ˆn∈[1, N]\ {n} ∀n∈[1, N](11)
P(HI|do(HSn)) =P(HI)∀n∈[1, N] (12)
HR,HIandHSn∀n∈[1, N] are the respective representations generated by the router, domain-invariant and N
domain-specific modules.
The rules of do-calculus, defined in Pearl [1995], allow one to reduce interventional queries (with the do(·) operator)
to observational queries. We will only use the deletion of actions rule. A simplified rule is shown in Equation 13:
P(Y|do(X)) =P(Y)
if (Y⊥ ⊥X)GX(13)
GXrepresents the causal graph Gwith the incoming edges of Xremoved.
Let us first address the causal relationships of the router. Equations 10 and 9 can be verified using the simplified
causal graph in Figure 6. They are a direct application of rule 13. When removing the parents of HSnorHI,HRis
d-separated [Pearl, 1988] from them: the backward path through Cis blocked and the forward path through WRis
not connected to HSnorHI. This is due to the use of a separate loss function for training the router when using the
vector quantisation routing strategy. One could notice that we do not represent the sum of losses of Equation 4. We
omit it in the simplified graph. Its impact on the backward pass is incidental since each element can be optimised
independently.
Let us now address the causal relationships of one activated domain-specific module nwith its counterparts
(Equation 11). Again, under graph GHSˆn,HSn, the backward path through Cbetween is blocked. In addition, we
make the assumption that only the module nis activated and is connected to Yas in Figure 6. This assumption
is verified when using the vector quantisation routing strategy. As a consequence, the routing process HScan be
decomposed into multiple subgraphs HSnHR− − → YandHSn̸HR− − → Y∀n∈[1, N]\ {n}, with AHR− − → Bequivalent to
having A→X←HRandX→B.A̸HR− − →Bremoves the second link. Therefore, during the backward pass, there is
only one link LY→WSn→HSnand no path to the other domain-specific modules n. A last type of path can exist;here is an example: assuming a model with two domain-specific modules, S0andS1, activated one after the other, the
following path exists: HS1HR− − →Yτ→ L Yτ→WIC− →HI→Yτ+1→ L Yτ+1→WS2C− →HS2. There is a causal path
forward path from HS1toHS2. The path does not exist if there is no invariant module, and Equation 11 holds.
If an invariant module is part of the model, Equations 11 and 12 do not hold because of the path above: HIand
HSnare not independent in the causal graph GHSn. Independence is achieved by minimising the Mutual Information
between HIandHSn, as discussed in the main paper.
B Additional Routing Strategies
In our main experiments, we use a simple routing strategy based on computing vector quantisation from Euclidean
distance. In this section, we consider several additional routing strategies:
•K-Means Clustering
•Euclidean Distance Weighting
K-Means Clustering This strategy computes the clusters using K-Means (Lloyd’s algorithm) [Lloyd, 1982]. We
first learn the cluster centroids on the training set separately. Then, we fine-tune the other modules. Therefore, the
clustering mechanism is independent of the gradient descent during fine-tuning, and the quality of the clusters with
respect to the data distribution depends solely on the robustness of the clustering method. For efficiency reasons,
we do not directly perform the clustering on the hidden states of the router module. Before clustering an input
embedding, we project it to a more dense space with fewer dimensions (typically 64). We apply Multidimensional
Scaling with the SMACOF algorithm [Borg and Groenen, 2005]. The algorithm requires us to provide a base of the
input space to perform the projection. We span the space using a random set of vectors from the training space.
Because the distribution is skewed, we do not have a warranty to build a base. To remain computation-efficient, we
sample 8 ×Mvectors with Mthe dimensionality of the reduced space.
Euclidean Distance Weighting This strategy differs from the other ones as it does not use vector quantisation.
Instead, we compute the Euclidean distance between the embeddings and the centroid coordinates (randomly
initialised) and use softmin to convert the distances into continuous weights between zero and one. The lower the
distance between the embedding and a centroid, the higher the weight the corresponding domain-specific module will
have on the output. Consequently, with this method, all domain-specific modules are always activated. This operation
is differentiable and is the closest to the routing process of Mixture-of-Expert models like the Switch Transformer
[Fedus et al. , 2022b]. This method does not follow the causal structure discussed in Section 4. Instead, it uses the
output loss to update the centroid coordinates.
The results obtained with these two routing strategies are provided in Appendix D.3.
C Additional Aggregation Schemes
In this section, we describe two additional aggregation schemes between the domain-invariant and domain-specific
modules. Instead of using a shared language modelling head, we propose to use a separate head for each module
and combine their outputs at the end by a weighted sum. This method tackles the issue of prioritised modules (e.g.
one module being overused at the expense of the others). However, the information from the modules is not linearly
combined but added separately, reducing expressivity.
We want to bound the output of each model such that it influences the final prediction by a pre-determined factor
(given by the router output for the domain-specific modules and provided as a hyperparameter for the domain-invariant
module). Each module outputs unbounded logits. The lack of bounds prevents them from directly multiplying the
logits by their weighting factor and summing them together. Indeed, one module could overcome the weighting by
increasing the magnitude of its logits. We consider two combination schemes: in the logit space and in the probability
space .
Combination in the logit space The aggregation scheme in the logit space is very similar to the one performed in
the latent space in the main paper. We first perform a shared batch normalisation [Ioffe and Szegedy, 2015] between
the modules to overcome the unbounded issue in the logit space. For a batch of size |B|, one domain-specific active
module and one domain-invariant module, batch normalisation is operated on 2 × |B| samples. We attribute a weight
wIto the domain-invariant module as a hyperparameter and wSn=rn·(1−wI) to the domain-specific module, with
rnthe weight given by the router. After normalisation, We multiply each logit value by its corresponding weight and
sum them together.
Combination in the probability space Each module outputs unbounded logits, so we first convert each output
into normalised probabilities (that sum to one). We then perform the weighting in each probability space before
converting them back to logits (shown in Equation 14). Finally, the outputs from all modules are summed together
(shown in Equation 15). The final probabilities are shown in Equation 16.Figure 7: Evolution of the Mutual Information loss dur-
ing training on ACRE and RAVEN. The x-axis corre-
sponds to the number of training steps and is shown in
the log scale.
Figure 8: Evolution of the Mutual Information loss dur-
ing training of variants with probability and logits aggre-
gation on ACRE and RAVEN. The x-axis corresponds
to the number of training steps and is shown in the log
scale.
ela(Y|c) = log1−wa
2+wa·Pa(Y|c))
+ log( Ba) (14)
l(Y|c) =elI(Y|c) +X
n∈[1,N]flSn(Y|c) (15)
P(Y|c) =σ(l(Y|c)) (16)
cis the input context. P(Y|c) is the final output distribution between all words Y, obtained using softmax
normalisation σon the output logits l(Y|c). The output logits are obtained by summing the weighted logits of
the domain-invariant module elI(Y|c) and all domain-specific modules flSn(Y|c). Equation 14 shows the weighting
process for all modules ( a∈ {I, S1, . . . , S N}). The weight wIis a hyperparameter set prior to training. The weights
wSn∀n∈[1, N] combine the weight wIwith the router weights rn:wSn=rn·(1−wI).Bais a normalisation term
that ensures the conversion function between probabilities and logits is invertible.
The results obtained with these two aggregation schemes are provided in Appendix D.4.
D Additional Experiments
D.1 Evolution of the Mutual Information Across Training
To ensure the independence between the domain-specific and domain-invariant modules, we minimise the mutual
Information between them. Figure 7 shows the evolution of Mutual Information during training. We observe that it
quickly decreases to reach below 0 ,0001. Figure 8 shows the same loss for the variants using aggregation in the logit
and probability spaces. Unlike for the main model, we observe small spikes in the loss after 100 training steps. The
aggregation scheme that uses a shared language modelling head (our default) seems more stable during training.
D.2 Routing Alignment and Visualisation
We study the module attribution performed by the router more deeply. Table 3 shows the alignment between the two
domain-specific modules. We first observe that the division is mainly syntactic: each module specialises towards one
type of input format, either text or symbolic. It aligns perfectly with the dataset.
Figures 9, 10, 11 and 12 show visualisations of the clusters in a 2D space. Figures 9 and 10 show the i.i.d and o.o.d
sets of ACRE. Figures 11 and 12 show the i.i.d and o.o.d sets of RAVEN. As in the main paper, the projection is made
using Multidimensional Scaling (MDS) [Borg and Groenen, 2005]. For illustration purposes, we observe the clusters
formed by the K-Means method for N= 4 modules. We also observe the clusters formed from the penultimate hiddenTable 3: Alignment between modules and formats in the ACRE dataset. Each column shows the proportion of
activation for each module for a given dataset. Each module specialises perfectly to one dataset.
ACRE -Comp -Sys
Text Symb Text Symb Text Symb
n= 0 0.0 1.0 0.0 1.0 0.0 1.0
n= 1 1.0 0.0 1.0 0.0 1.0 0.0
(a) Ground Truth ( l=−1)
 (b) K-Means ( l=−1)
 (c) Ground Truth ( l=−2)
 (d) K-Means ( l=−2)
Figure 9: Clusters formed from the hidden states of LLaMA2 on the ACRE training sets. The visualisations contain
the last two levels ( l) of hidden layers. The ground truth shows the true splits (text/symbolic). The learned clusters
use 4 centroids.
states of the router. As discussed above and in the main paper, there is a clear division between text and symbolic
embeddings, but the o.o.d sets are not well separated in ACRE while they are in RAVEN. This division (and absence
of division) is also present in the previous hidden states, although the separation is less obvious: all embeddings tend
to align to a single axis.
We want to study the router’s behaviour further when faced with a diverse set of input data. To this end, we feed
six different datasets to the model: the i.i.d text and symbolic sets of ACRE and RAVEN, PVR [Zhang et al. , 2021b]
and ARC [Chollet, 2019] datasets. The visualisations are in Figure 13. Overall, the datasets are well separated but
have different shapes. While some form dense amalgamates, others spread in the latent space. The observations
from ACRE and RAVEN suggest that the distance in the embedding space between a module cluster and an input
can be an indicator of the module’s performance on the input. The o.o.d sets of ACRE are merged in the latent
space, and the model maintains accuracy across the sets. In parallel, the o.o.d sets of RAVEN are separated by clear
boundaries, and the accuracy drops as the distance with the i.i.d set increases. Experiments on a larger scale are
needed to validate or invalidate the hypothesis and discriminate the true causes responsible for this behaviour from
spurious correlations.
D.3 Variations of the Routing Strategy
We perform additional experiments on ACRE and RAVEN datasets using the routing strategies introduced in
Appendix B: K-Means and weighting. Tables 4 and 5 show the results.
The alternative routing strategies achieve similar and sometimes superior performance than the base ICLM model.
As observed in the previous section, the router creates well-defined clusters that the K-Means and Euclidean distance
vector quantisation strategies tend to follow. No explicit differentiation of the routing process can be observed from the
visualisations. The difference in performance may lie in the optimisation process. K-Means does not backpropagate
information to the router; weighting backpropagates from the output loss, and vector quantisation backpropagates
from a secondary loss.
(a) Ground Truth ( l=−1)
 (b) K-Means ( l=−1)
 (c) Ground Truth ( l=−2)
 (d) K-Means ( l=−2)
Figure 10: Clusters formed from the hidden states of LLaMA2 on the ACRE o.o.d sets. The visualisations contain
the last two levels ( l) of hidden layers. The ground truth shows the true splits (text/symbolic/o.o.d splits). The
learned clusters use 4 centroids.(a) Ground Truth ( l=−1)
 (b) K-Means ( l=−1)
 (c) Ground Truth ( l=−2)
 (d) K-Means ( l=−2)
Figure 11: Clusters formed from the hidden states of LLaMA2 on the RAVEN training sets. The visualisations
contain the last two levels ( l) of hidden layers. The ground truth shows the true splits (text/symbolic). The learned
clusters use 4 centroids.
(a) Ground Truth ( l=−1)
 (b) K-Means ( l=−1)
 (c) Ground Truth ( l=−2)
 (d) K-Means ( l=−2)
Figure 12: Clusters formed from the hidden states of LLaMA2 on the RAVEN o.o.d sets. The visualisations contain
the last two levels ( l) of hidden layers. The ground truth shows the true splits (text/symbolic). The learned clusters
use 4 centroids.
(a) Ground Truth ( l=−1)
 (b) K-Means ( l=−1)
 (c) Ground Truth ( l=−2)
 (d) K-Means ( l=−2)
Figure 13: Clusters formed from the hidden states of LLaMA2 on the training sets of ACRE, ARC, PVR and RAVEN.
The visualisations contain the last two levels ( l) of hidden layers. The ground truth shows the true splits between
each dataset. The learned clusters use 4 centroids.Table 4: Accuracy on the ACRE i.i.d and o.o.d test sets. Datasets are represented in columns, and models in rows.
ICLM is trained on text and symbolic i.i.d training sets. Models with a∗indicate that the results are introduced in
this paper. The best model is shown in bold .
ACRE -o.o.d-Comp -o.o.d-Sys
Text Symb Text Symb Text Symb
ICLM* (ours) 0.653 0.950 0.663 0.931 0.634 0.901
ICLM-Weighted* (ours) 0.812 0.921 0.802 0.960 0.842 0.970
ICLM-K-Means* (ours) 0.901 0.881 0.911 0.911 0.891 0.921
Table 5: Accuracy on the RAVEN i.i.d and o.o.d test sets. The characteristics are the same as in Table 4.
RAVEN -o.o.d-Four -o.o.d-In-Center
Text Symb Text Symb Text Symb
ICLM* (ours) 1.000 0.980 0.703 0.703 0.515 0.228
ICLM-Weighted* (ours) 1.000 1.000 0.743 0.703 0.653 0.248
ICLM-K-Means* (ours) 1.000 1.000 0.634 0.673 0.515 0.287
D.4 Variations of the Aggregation Scheme
We perform additional experiments on ACRE and RAVEN datasets using the aggregation schemes introduced in
Appendix C: in the logit and probability spaces. Tables 4 and 5 show the results.
Table 6: Accuracy on the ACRE i.i.d and o.o.d test sets.
ACRE -o.o.d-Comp -o.o.d-Sys
Text Symb Text Symb Text Symb
ICLM* (ours) 0.653 0.950 0.663 0.931 0.634 0.901
ICLM-Logits* (ours) 0.842 0.950 0.881 0.901 0.901 0.921
ICLM-Probas* (ours) 0.921 0.950 0.832 0.921 0.891 0.931
Table 7: Accuracy on the RAVEN i.i.d and o.o.d test sets.
RAVEN -o.o.d-Four -o.o.d-In-Center
Text Symb Text Symb Text Symb
ICLM* (ours) 1.000 0.980 0.703 0.703 0.515 0.228
ICLM-Logits* (ours) 1.000 0.990 0.644 0.713 0.703 0.268
ICLM-Probas* (ours) 0.802 0.931 0.614 0.624 0.495 0.297
As per the routing strategies, the alternative aggregation schemes achieve similar and sometimes superior performance
than the base ICLM model. No scheme is systematically better than the others. These results show that less expressive
aggregation methods, i.e. weighted sums, can perform similarly to trained dense layers on abstract and causal reasoning
tasks.
D.5 Module Correlation Across Hidden States
To further investigate the level of Independence of the LLM modules, we measure the Pearson Correlation Coefficient
between all hidden states of the domain-invariant and domain-specific modules during inference on ACRE and RAVEN.
Figures 14 and 15 show the results. The intra-module correlations (Figures 14a, 14e, 14h, 14i, 15a, 15e, 15h and 15i)
show that hidden states from close layers in the model are highly correlated. Furthermore, correlation blocks are
visible, i.e. sequences of layers that demonstrate a high level of correlation between them and a low level of correlation
with the layers not in the sequence. We observe five correlation blocks well-defined on ACRE and with fuzzy-edges on
RAVEN.
These correlation blocks hold in the inter-module correlation matrices, indicating that similar mechanisms are
shared across modules. However, the fine-tuning and regularisation procedures reduce the influence of the shared
mechanisms and enhance specialisation.
Correlation Measures in Transfer Learning Settings We perform the same measures in the transfer learning
settings. Figures 16 and 17 show the results. The studied ICLM model has four modules: while domain-2 and dmain-3(a) Router-Router.
 (b) Router-Inv.
 (c) Router-Dom 0.
 (d) Router-Dom 1.
 (e) Inv-Inv.
(f) Inv-Dom 0.
 (g) Inv-Dom 1.
 (h) Dom 0-Dom 0.
 (i) Dom 1-Dom 1.
 (j) Dom 0-Dom 1.
Figure 14: Measures of the Pearson Correlation Coefficient between module hidden states during inference on ACRE
dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router refers to the routing module, Inv
refers to the domain-invariant module, and Dom irefers to the idomain-specific module. The caption indicates the
modules used for each row-column pair.
are well aligned with the respective text and symbolic RAVEN splits, the two remaining modules do not fully align
with the two ACRE splits. Particularly, the domain-2 shows a very low level of correlation with the other modules and
even with its own layers. This visualisation of the layers confirms the results observed in Table 2 and demonstrates
that the poor performance of this module is due to a collapse of the module during the transfer learning: the module
inputs are no longer correlated with the module outputs.(a) Router-Router.
 (b) Router-Inv.
 (c) Router-Dom 0.
 (d) Router-Dom 1.
 (e) Inv-Inv.
(f) Inv-Dom 0.
 (g) Inv-Dom 1.
 (h) Dom 0-Dom 0.
 (i) Dom 1-Dom 1.
 (j) Dom 0-Dom 1.
Figure 15: Measures of the Pearson Correlation Coefficient between module hidden states during inference on RAVEN
dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router refers to the routing module, Inv
refers to the domain-invariant module, and Dom irefers to the domain-specific module i. The caption indicates the
modules used for each row-column pair.(a) Router-Router.
 (b) Router-Inv.
 (c) Router-Dom 0.
 (d) Router-Dom 1.
 (e) Router-Dom 2.
 (f) Router-Dom 3.
(g) Inv-Inv.
 (h) Inv-Dom 0.
 (i) Inv-Dom 1.
 (j) Inv-Dom 2.
 (k) Inv-Dom 3.
 (l) Dom 0-Dom 0.
(m) Dom 1-Dom 1.
 (n) Dom 2-Dom 2.
 (o) Dom 3-Dom 3.
 (p) Dom 0-Dom 1.
 (q) Dom 0-Dom 2.
 (r) Dom 0-Dom 3.
(s) Dom 1-Dom 2.
 (t) Dom 1-Dom 3.
 (u) Dom 2-Dom 3.
Figure 16: Measures of the Pearson Correlation Coefficient between module hidden states of the transfer learning
model during inference on ACRE dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router
refers to the routing module, Inv refers to the domain-invariant module, and Dom irefers to the idomain-specific
module. The caption indicates the modules used for each row-column pair.(a) Router-Router.
 (b) Router-Inv.
 (c) Router-Dom 0.
 (d) Router-Dom 1.
 (e) Router-Dom 2.
 (f) Router-Dom 3.
(g) Inv-Inv.
 (h) Inv-Dom 0.
 (i) Inv-Dom 1.
 (j) Inv-Dom 2.
 (k) Inv-Dom 3.
 (l) Dom 0-Dom 0.
(m) Dom 1-Dom 1.
 (n) Dom 2-Dom 2.
 (o) Dom 3-Dom 3.
 (p) Dom 0-Dom 1.
 (q) Dom 0-Dom 2.
 (r) Dom 0-Dom 3.
(s) Dom 1-Dom 2.
 (t) Dom 1-Dom 3.
 (u) Dom 2-Dom 3.
Figure 17: Measures of the Pearson Correlation Coefficient between module hidden states of the transfer learning
model during inference on RAVEN dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router
refers to the routing module, Inv refers to the domain-invariant module, and Dom irefers to the idomain-specific
module. The caption indicates the modules used for each row-column pair.