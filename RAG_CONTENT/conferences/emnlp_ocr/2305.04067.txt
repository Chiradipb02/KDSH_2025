The Best Defense is Attack: Repairing Semantics in Textual Adversarial
Examples
Heng Yang1, Ke Li1
1Department of Computer Science, University of Exeter, EX4 4QF, Exeter, UK
{hy345, k.li}@exeter.ac.uk
Abstract
Recent studies have revealed the vulnerabil-
ity of pre-trained language models to adversar-
ial attacks. Existing adversarial defense tech-
niques attempt to reconstruct adversarial exam-
ples within feature or text spaces. However,
these methods struggle to effectively repair the
semantics in adversarial examples, resulting in
unsatisfactory performance and limiting their
practical utility. To repair the semantics in ad-
versarial examples, we introduce a novel ap-
proach named Reactive Perturbation Defocus-
ing ( RAPID ).RAPID employs an adversarial
detector to identify fake labels of adversarial
examples and leverage adversarial attackers
to repair the semantics in adversarial exam-
ples. Our extensive experimental results con-
ducted on four public datasets, convincingly
demonstrate the effectiveness of RAPID in var-
ious adversarial attack scenarios. To address
the problem of defense performance validation
in previous works, we provide a demonstra-
tion of adversarial detection and repair based
on our work, which can be easily evaluated at
https://tinyurl.com/22ercuf8 .
1 Introduction
Pre-trained language models (PLMs) have achieved
state-of-the-art (SOTA) performance across a va-
riety of natural language processing tasks (Wang
et al., 2019a,b). However, PLMs are reported to be
highly vulnerable to adversarial examples, a.k.a.,
adversaries (Li et al., 2019; Garg and Ramakrish-
nan, 2020; Li et al., 2020; Jin et al., 2020; Li et al.,
2021; Boucher et al., 2022), created by subtly alter-
ing selected words in natural examples, a.k.a. clean
orbenign examples (Morris et al., 2020). While
the significance of textual adversarial robustness re-
garding adversarial attacks has broadly recognized
within the deep learning community (Alzantot et al.,
2018; Ren et al., 2019; Zang et al., 2020; Zhang
et al., 2021; Jin et al., 2020; Li et al., 2021; Wang
et al., 2022a; Xu et al., 2023), efforts to enhance ad-
versarial robustness remain very limited, especiallywhen comparing to other deep learning fields like
computer vision (Rony et al., 2019; Gowal et al.,
2021; Wang et al., 2023; Xu et al., 2023). Current
works on textual adversarial robustness can be clas-
sified into three categories— adversarial defense ,
adversarial training (Liu et al., 2020a,b; Ivgi and
Berant, 2021; Dong et al., 2021b,a), and adversary
reconstruction (Zhou et al., 2019; Jones et al., 2020;
Bao et al., 2021; Keller et al., 2021; Mozes et al.,
2021; Li et al., 2022; Shen et al., 2023). Since both
adversarial training and reconstruction are resource-
intensive, there has been growing interest in adver-
sarial defense. Nevertheless, the current adversarial
defense techniques have two bottlenecks.
/car-crashCurrent works can hardly identify the seman-
tic discrepancies between natural and adver-
sarial examples1. Let us use RS&V, a recent
adversarial defense (Wang et al., 2022b), as
an example. As shown in Figure 1, it is clear
thatRS&V fails to discern the semantic dif-
ferences between adversarial and repaired ex-
amples. This is attributed to the augmentation
method used in RS&V that is not only un-
targeted but also does not effectively identify
and neutralize adversaries.
/car-crashGiven the time-intensive nature of the defense
process, adversarial defense is also notori-
ous for its computational inefficiency (Mozes
et al., 2021; Wang et al., 2022b). This can
be partially attributed to their inability to pre-
detect adversaries and indiscriminately pro-
cess all input texts. This not only wastes
computational budget on unnecessary defense
actions regarding natural examples, but also
leads to an unwarranted defensive stance to-
wards natural examples, which may further
compromise performance.
Bearing the above two challenges in mind, we
propose a simple yet effective textual adversary
1In this work, we refer to the semantics in adversaries as
the features encoded by PLM for simplicity.arXiv:2305.04067v2  [cs.CL]  1 Apr 2024BAE PWWS TextFooler−1−0.500.51
(b)Rapid -AmazonCosine Similarity
BAE PWWS TextFooler−0.200.20.40.60.81
(c)RS&V -SST2Cosine SimilarityBAE PWWS TextFooler−1−0.500.51
(a)Rapid -SST2Cosine Similarity
Adversarial
Repaired
BAE PWWS TextFooler−0.500.51
(d)RS&V -AmazonCosine SimilarityFigure 1: Box plots of the cosine similarity between
theadversary–natural example pairs (marked in red)
and the repaired adversary–natural example pairs ob-
tained by RAPID versus RS&V . The cosine similarity
is evaluated based on the features extracted by the vic-
tim models of RAPID andRS&V , respectively. The
larger the cosine similarity, the more similar the corre-
sponding example pair. It is observed that the victim
model cannot discern the semantic differences between
the adversaries and the repaired adversaries produced
byRS&V , whereas RAPID can precisely differentiate
between adversaries and natural examples. Conversely,
when using RAPID , the repaired adversaries regain their
semantic alignment with the natural examples.
defense paradigm, named reactive perturbation de-
focusing ( RAPID ), which has the following two
distinctive features.
♂lightbulbTo address the first bottleneck, we propose
a novel concept of perturbation defocusing
(Section 2.2.2). The basic idea is to lever-
age adversarial attackers to re-inject some per-
turbations into the pre-detected adversaries
to distract the victim model from malicious
perturbations, and to repair these adversaries
based on the inherent robustness of the victim
models. Further, the accuracy of adversarial
defense is augmented by a pseudo-semantic
similarity filtering strategy (Section 2.2.3).
♂lightbulbTo overcome the second bottleneck, RAPID
trains an in-victim-model adversarial detec-
tor, without introducing additional cost (Sec-
tion 2.1), to proactively concentrate the de-
fense efforts on the examples pre-detected as
adversaries. In particular, this adversarial de-
tector is jointly trained with the victim model
in a multi-task way, and is capable of rec-
ognizing adversaries generated by different
attackers. This helps not only minimize col-
lateral impacts on natural examples (Xu et al.,2022), but also reduces the waste of computa-
tional budget upon defending against natural
examples.
Figure 2 provides a pedagogical example of the
working mechanism of RAPID in the context of
sentiment analysis. There are four key takeaways
from our empirical study.
♂¶agicRAPID achieves up to 99.9%repair accuracy
upon pre-detected adversaries, significantly
surpassing text/feature-level reconstruction
and voting-based methods (Table 2).
♂¶agicRAPID reduces nearly 50% computational
cost for adversarial defense compared against
adversarial attack (Table 12).
♂¶agicRAPID is robust in recognizing and defend-
ing against a wide range of unknown adver-
sarial attacks (Table 4), such as CLARE (Li
et al., 2021) and large language models like
ChatGPT-3.5 (OpenAI, 2023).
♂¶agicWe develop a user-friendly API2as a bench-
marking platform for different adversarial at-
tackers under the defense of R APID .
2 Proposed Method
Our proposed RAPID framework comprises two
phases. Phase #1 trains a joint model that not only
performs the standard text classification task but
is also capable of detecting adversaries. Phase #2
is dedicated to implementing pseudo-supervised
adversary defense based on PD. It diverts the vic-
tim model’s attention from malicious perturbations,
and rectifies the outputs without compromising per-
formance on natural examples.
2.1 Phase #1 : joint model training
The crux of Phase #1 is the joint training of two
models: one is the victim model as the standard
text classifier, and the other is an in-victim-model
adversarial detector, which is a binary classifier
that pre-detect adversaries before the defense.
2.1.1 Multi-attack-based adversary sampling
To derive the data used for training the adversarial
detector, we apply adversarial attack methods upon
the victim model FSto sample adversaries. To
enable the adversarial detector to identify various
unknown adversaries, we employ three widely used
open-source adversarial attackers: BAE (Garg and
Ramakrishnan, 2020), PWWS (Ren et al., 2019),
2For the sake of anonymous requirement, we promise to
release this tool upon the acceptance of this paper.This is the most intriguing exploration of alienation.HijackThis is the most interesting investigation of alienation.This is the most intriguing investigation of alienation.PerturbationdefocusingThis is the most intriguing exploration of alienation.This is the most intriguing investigation of alienation.Hijack<latexit sha1_base64="GuibbS1X6IS6Qg3rUL7xdy5cbu4=">AAAB9XicbVBNS8NAEJ34WetX1aOXYBE8lUSkeizowWMV+wFtLJvNpl262YTdiVpC/4cXD4p49b9489+4bXPQ1gcDj/dmmJnnJ4JrdJxva2l5ZXVtvbBR3Nza3tkt7e03dZwqyho0FrFq+0QzwSVrIEfB2oliJPIFa/nDy4nfemBK81je4ShhXkT6koecEjTSfRfZE2qa3ZKEB+NeqexUnCnsReLmpAw56r3SVzeIaRoxiVQQrTuuk6CXEYWcCjYudlPNEkKHpM86hkoSMe1l06vH9rFRAjuMlSmJ9lT9PZGRSOtR5JvOiOBAz3sT8T+vk2J44WVcJikySWeLwlTYGNuTCOyAK0ZRjAwhVHFzq00HRBGKJqiiCcGdf3mRNE8rbrVSvTkr167yOApwCEdwAi6cQw2uoQ4NoKDgGV7hzXq0Xqx362PWumTlMwfwB9bnDyFgkvI=</latexit>Rapid
Ha, adversary!Figure 2: A pedagogical example of RAPID in sentiment analysis. The original word in this example is exploration.
Perturbation defocusing repairs the adversary by injecting perturbations (interesting) to distract the objective model
from the malicious perturbation (i.e., investigation). RAPID only implements defense on the pre-detected adversary.
andTEXTFOOLER (Jin et al., 2020). For each data
instance ⟨x, y⟩ ∈ D , the set of natural examples,
we apply each of the adversarial attackers to sample
three adversaries3:
⟨˜x,˜y⟩i← A i(FS,⟨x, y⟩), (1)
where Ai,i∈ {1,2,3}, represents BAE ,PWWS ,
andTEXTFOOLER , respectively. ⟨˜x,˜y⟩iis the ad-
versary generated by Ai. Note that we collect all
adversaries, including both successful and failed
ones, to constitute the adversarial dataset ˜D. Fi-
nally, we compose a hybrid dataset as shown in
the left part of Figure 3. D:=DS˜Dfor the joint
model training.
2.1.2 Joint model training objectives
To conduct the joint model training of both the vic-
tim model and the adversarial detector, we propose
an aggregated loss function as follows:
L:=Lc+Ld+La+λ||θ||2
2, (2)
where λis the ℓ2regularization parameter, and θ
represents the parameters of the underlying PLM.
Lc,Ld, andLadenotes the loss for training a stan-
dard classifier, an adversarial detector, and adver-
sarial training, respectively.
•Standard classification loss Lc: Here we use
the cross-entropy loss widely used for text
classification:
Lc:=−CX
i=1[pilog (˜pi) +qilog (˜qi)],(3)
where Cis the number of classes. pand˜p
respectively indicate the true and predicted
probability distributions of the standard clas-
sification label, while qandˆqrepresent any
3The formulation of word-level adversarial attack is avail-
able in Appendix A.incorrect standard classification label and its
likelihood, respectively. Note that the labels
of the adversaries within Dare set to a dummy
value∅in this loss. By doing so, we can make
sure that Lcfocuses on the natural examples.
•Adversarial detection loss Ld: It only calcu-
lates the binary cross-entropy for both natural
examples and adversaries within D, where the
labels are either 0or1in practice. Note that
Ldis used to train the adversarial detector as
a binary classifier that determines whether the
input example is an adversary or not.
•Adversarial training loss La: In practice, the
calculation of Lais the same as Lc. To im-
prove the robustness of adversaries, Laonly
calculates the loss for the adversaries by set-
ting the labels of natural examples within D
as a dummy ∅. By doing so, we can prevent
this adversarial training loss from negatively
impacting the performance on pure natural ex-
amples, which have been reported to be noto-
rious in recent studies (Dong et al., 2021a,b).
All in all, each instance ⟨x,y⟩ ∈Dis augmented
with three different labels to accommodate these
three training losses, where y:= (y1,y2,y3)⊤.
2.2 Phase #2 : reactive adversarial defense
To address the efficiency and semantic challenges
discussed in Section 1, the reactive adversarial de-
fense consists of the following three steps.
2.2.1 Adversarial defense detection
Our preliminary experiments suggested that PLMs
likeBERT andDEBERT Aare sensitive to seman-
tic shifts caused by adversarial attacks. Thereby,
different from the current adversarial defense meth-
ods, which often indiscriminately run defense upon
all input examples, we will first apply the joint
model FJtrained in the Phase #1 to determine
whether the input ˆxis adversarial or not using theStandard classiﬁerAdversarial detectorStandardoutputPerturbation defocusingDetectionoutputRepaired outputAdversarial exampleMulti-attackadversary sampling ]Natural examples
]Sampled adversariesSimilarityﬁltering]Repaired adversariesInput example
Phase #1Phase #2Figure 3: The overall architecture and workflow of R APID .
following prediction:
(ˆy1,ˆy2,ˆy3)←FJ(ˆx), (4)
where ˆy1,ˆy2, and ˆy3are predicted labels according
to the three training losses in equation (2), respec-
tively. Thereafter, only the inputs identified as ad-
versaries (i.e., those with ˆy2= 1) are used for the
follow-up perturbation defocusing.
2.2.2 Perturbation defocusing
The basic idea of this perturbation defocusing is to
inject safeperturbations into the adversary ˆxiden-
tified by the adversarial defense detection in Sec-
tion 2.2.1. The process is shown in Phase #2
inFigure 3. In practice, we apply an adversarial
attacker to attack ˆxto obtain a repaired example :
⟨˜xr,˜yr
1⟩ ← ˆAPD(FJ,⟨ˆx,ˆy1⟩), (5)
where ˆy1is the predicted label of ˆx, and ˆAPDis an
adversarial attacker4. Note that the above pertur-
bation is considered safe because it does not alter
the semantics of ˆx. By this means, we divert the
standard classifier’s focus away from the malicious
perturbations, allowing the standard classifier to
concentrate on the adversary’s original semantics.
In essence, the repaired examples can be correctly
classified based on their own robustness.
2.2.3 Pseudo-semantic similarity filtering
Last but not least, to prevent repaired adversaries
from being misclassified, we propose a feature-
level pseudo-semantic similarity filtering strategy
to mitigate semantic bias. Specifically, for each
ˆx, we generate a set of repaired examples S:=
4We choose PWWS because it is cost-effective, and it can
be replaced by any (or an ensemble of) adversarial attackers.{˜xr
i}k
i=1. Then, we encode these repaired exam-
ples using FJto extract their semantic features.
Thereafter, for each repaired example within S, we
calculate its similarity score as:
si=Pk
j=1,j̸=isim(Hi,Hj)
k, (6)
whereHiandHjare the hidden states of ˜xr
iand˜xr
j
encoded by FJ, and sim(∗,∗)evaluates the cosine
similarity. For the sake of efficiency, we set k= 3
in this paper. After the defense, the label of the
repaired ˆxis assigned as the predicted label of
the repaired example within Shaving the largest
similarity score.
Remark 1. Generally speaking, the basic idea of
an adversarial attacker is to inject some (usually
limited) malicious perturbations into a natural ex-
ample, thus fooling the victim model. This often
results in adversaries looking similar to the natural
examples. However, the corresponding semantics
are often ‘destroyed’ after the perturbation. This
inspires us to introduce a new adversarial attacker,
even though different from the malicious attacker,
to attack and thus repair the malicious semantics of
adversaries provided that we know its fake labels.
Further, due to the principle of minimizing edits
when changing the prediction, we can also miti-
gate text space shifts in repaired examples along
with the semantics.
Remark 2. Note that the defender in RAPID is
decoupled with the adversarial detector, and its
performance is agnostic to the adversarial attack-
ers used for this adversary sampling. The empirical
results in Table 4 demonstrate that the adversar-
ial detector can adapt to unknown attack methods,
even when trained on a small set of adversaries.3 Experimental Settings
In this section, we introduce the experimental set-
tings used in our experiments.
Table 1: The statistics of datasets used for evaluating
RAPID . We use subsets from Amazon ,AGNews and
Yahoo! datasets to evaluate RAPID as the previous
works due to high resource occupation.
DATASET CATEGORIESNUMBER OF EXAMPLES
TRAINING VALID TESTING
SST2 2 6,920 872 1,821
Amazon 2 7,000 1,000 2,000
AGNews 4 120,000 0 7,600
Yahoo! 10 1,400,000 0 60,000
Victim models: while any PLM can be used in
a plug-in manner in RAPID , this paper considers
BERT (Devlin et al., 2019) and DEBERT A(He
et al., 2021), two widely used PLMs based on the
transformer structure5, as both the victim classifier
and the joint model. Their corresponding hyperpa-
rameter settings are in Appendix B.2.
Datasets: we consider three widely used text
classification datasets6, including SST2 (Socher
et al., 2013), Amazon (Zhang et al., 2015), and
AGNews (Zhang et al., 2015) whose key statistics
are outlined in Table 1. SST2 andAmazon are bi-
nary sentiment classification datasets. AGNews and
Yahoo! is a multi-categorical news classification
dataset containing 4and10categories, respectively.
Adversarial attackers: our experiments employ
three open-source attackers provided by TEXTAT-
TACK7(Morris et al., 2020). Their functionalities
are outlined as follows, while their working mech-
anisms are in Appendix B.1.
a)Adversary sampling . BAE ,PWWS and
TEXTFOOLER are used to sample adversaries
for training the adversarial detector (Sec-
tion 2.1). Since they represent different types
of attacks, we can train a detector that recog-
nizes a variety of adversarial attacks.
b)Adversary repair . We employ PWWS as the
attacker ˆAPDin the perturbation defocusing
(Section 2.2). Compared to BAE , our prelim-
inary experiments demonstrate that PWWS
rarely changes the natural examples’ seman-
tics, and it is more computationally efficient
than T EXTFOOLER .
5https://github.com/huggingface/transformers
6We have released the detailed source codes and processed
datasets in the supplementary materials.
7https://github.com/QData/TextAttackc)Generalizability evaluation . We use
IGA (Wang et al., 2021a), DEEPWORD-
BUG(Gao et al., 2018), PSO (Zang et al.,
2020) and CLARE to evaulate RAPID ’s
generalization capability.
Evaluation metrics: we use the following five fine-
grained metrics8for text classification to evaluate
the adversarial defense performance.
•Nature accuracy (NTA): it evaluates the vic-
tim’s performance on the target dataset that
only contains natural examples.
•Attack accuracy (ATA): It evaluates the vic-
tim’s performance under adversarial attacks.
•Detection accuracy (DTA): It measures the
defender’s adversaries detection performance.
•Defense accuracy (DFA): It evaluates the de-
fender’s performance of adversaries repair.
•Repaired accuracy (RPA): It evaluates the
victim’s performance on the attacked dataset
after being repaired.
Note that we evaluate the adversarial detection and
defense performance on the entire testing set, while
current works (Xu et al., 2022; Yang et al., 2022;
Dong et al., 2021a,b) only evaluated a small amount
of data extracted from the testing set.
Baseline methods: RAPID is compared against the
following six adversarial defense baselines.
•DISP (Zhou et al., 2019): It is an embedding
feature reconstruction method. It uses a per-
turbation discriminator to evaluate the proba-
bility that a token is perturbed and provides
a set of potential perturbations. For each po-
tential perturbation, an embedding estimator
learns to restore the embedding of the original
word based on the context.
•FGWS (Mozes et al., 2021): It uses
frequency-guided word substitutions to ex-
ploit the frequency properties of adversarial
word substitutions to detect adversaries.
•RS&V (Wang et al., 2022b): It is a text re-
construction method based on the randomized
substitution-to-vote strategy. RS&V accumu-
lates the logits of massive samples generated
by randomly substituting the words in the ad-
versaries with synonyms.
Note that the rationale of choosing the above three
baselines is their open source nature, while we can
hardly reproduce the experimental results of other
methods like T EXTSHIELD (Shen et al., 2023).
8The mathematical definitions of these evaluation metrics
can be found in Appendix B.3.4 Experimental Results
4.1 Adversary detection performance
Results shown in Table 2 demonstrate the effective-
ness of the adversarial detector in RAPID . This
in-victim-model adversarial detector, trained in
conjunction with the standard classifier, accurately
identifies adversaries across most datasets. Com-
pared to the previous adversary detection-based de-
fense (Mozes et al., 2021; Wang et al., 2022b; Shen
et al., 2023), the in-victim-model adversarial detec-
tor identifies the adversaries with no extra cost. On
the other hand, our evaluation confirms a very low
false positive rate ( ≈2%) of adversary detection on
natural examples, resulting in a very slight perfor-
mance degradation on natural examples. Further,
the adaptability of RAPID to previously unseen at-
tack methods is evidenced in Table 4, highlighting
the versatility of our adversarial detector. It excels
at identifying adversaries by detecting disruptions
introduced by malicious attackers, such as gram-
mar errors and word misuse. Note that detection
performance on the AGNews dataset is lower due
to the absence of news data in the BERT training
corpus, as discussed in Table 8of He et al. (2021).
4.2 Adversary defense performance
As for the adversary defense, RAPID outperforms
existing methods across all datasets, as outlined
in Table 2. When we focus on correctly identi-
fied adversaries, RAPID can effectively repair up to
92% to99% of them, even on the challenging 10-
category Yahoo datasets. Our research also sheds
light on the limitations of unsupervised text-level
and feature-level reconstruction methods, as re-
ported in studies such as Zhou et al. (2019); Mozes
et al. (2021); Wang et al. (2022b). These meth-
ods struggle to rectify the deep semantics in ad-
versaries, rendering them inefficient and inferior.
Additionally, we find that previous methods are not
robust when defending against adversaries in short
texts, as evidenced by their failure on the SST2
andAmazon datasets. RAPID consistently achieves
higher defense accuracy, particularly on binary clas-
sification datasets. In summary, RAPID employs
adversarial attackers to repair adversaries’ deep
semantics and minimize edits in the text space,
resulting in satisfactory adversarial defense. We
emphasize the importance of dedicated deep se-
mantics repair in the context of adversarial defense
against unsupervised features and text space recon-
struction.4.3 Ablation experiment
We conducted ablation experiments to assess the ef-
fectiveness of pseudo-semantic similarity filtering
(Section 2.2.3). It exclusively affects the defense
process, so we have omitted the unaffected metrics,
such as the detection accuracy in Table 2. From
the results shown in Table 3, we find that the ad-
versarial defense performance of RAPID without
this filtering strategy is notably inferior ( ≈1%)
in most cases. Further, the degradation in defense
performance is more pronounced in the case of the
AGNews andYahoo! datasets compared to the SST2
andAmazon datasets. This discrepancy is attributed
to the larger vocabularies and longer text lengths in
theAGNews andYahoo! datasets, resulting in diver-
sified repaired examples in terms of similarity.
4.4 Further research questions
We discuss more findings about RAPID by answer-
ing the following research questions (RQs).
RQ1: How is the generalization ability of RAPID
to unknown attackers?
Methods :To assess the generalization ability of the
in-victim-model adversarial detector in RAPID , we
have conducted experiments among various state-
of-the-art adversarial attackers: PSO ,IGA ,DEEP-
WORDBUG, and CLARE , which were not included
in the training of the adversarial detector in RAPID .
Note that better adversarial detection and defense
performance against unknown adversarial attackers
indicates a superior generalizability of R APID .
Results :From the results in Table 4, we find that
RAPID can identify up to 98.67% of adversaries on
both the SST2 andAmazon datasets when consider-
ing adversarial detection performance. In terms of
adversarial defense, RAPID is capable of repairing
a substantial number of adversaries generated by
various unknown attack methods (up to 87.68%
and94.65% on the SST2 andAmazon datasets, re-
spectively). However, RAPID experiences a de-
cline in performance in identifying and defending
against adversaries when facing the challenging
CLARE attack. This performance degradation is
likely attributed to their ineffective adversarial de-
tection, which could potentially be improved by
training CLARE -based adversaries for adversarial
detection within RAPID . In summary, RAPID has
demonstrated robust generalization ability, effec-
tively detecting and repairing a wide array of ad-
versaries generated by unknown attackers.
RQ2: Does perturbation defocusing really re-Table 2: The main adversarial detection and defense performance of RAPID on four public datasets. The victim
model is BERT and the results in bold font indicate the best performance. We report the average accuracy
of five random runs. The adversarial defense performance reported in previous works varies from adversarial
attackers’ implementations. For fair comparisons, all the baseline experiments are re-implemented based on the
latest adversarial attackers from the Textattack library to avoid biases. “TF” indicates T EXTFOOLER .
DEFENDER ATTACKERAGNews (4-category) Yahoo! (10-category) SST2 (2-category) Amazon (2-category)
NTA A TA D TA D FA R PA NTA A TA D TA D TA R PA NTA A TA D TA D FA R PA NTA A TA D TA D FA R PA
DISPPWWS 32.09 55 .49 57 .82 68 .23 5.70 61 .67 54 .95 50 .24 23.44 38 .93 34 .46 35 .33 15.56 41 .90 45 .92 59 .80
TF 94.13 50 .50 53 .78 56 .18 70 .16 75.63 13 .60 50 .73 57 .48 53 .18 91.24 16 .21 37 .80 34 .37 37 .16 93.67 21 .77 43 .10 47 .15 60 .56
BAE 74.80 45 .26 45 .75 81 .39 27.50 54 .82 53 .75 50 .90 35.21 36 .59 37 .51 42 .22 44.00 40 .28 42 .74 61 .85
FGWSPWWS 32.09 65 .24 68 .35 71 .78 5.70 65 .83 61 .46 53 .28 23.44 40 .28 40 .38 39 .20 15.56 44 .47 56 .89 60 .29
TF 94.25 50 .50 68 .88 70 .71 73 .40 76.24 13 .60 68 .57 65 .17 54 .53 91.34 16 .21 42 .79 41 .05 41 .53 94.26 21 .77 45 .75 58 .74 61 .51
BAE 74.80 44 .29 47 .95 83 .57 27.50 58 .63 56 .33 52 .94 35.21 43 .83 48 .37 44 .90 44.00 42 .26 43 .04 64 .63
RS&VPWWS 32.09 83 .67 84 .96 83 .80 5.70 65 .01 65 .22 57 .22 23.44 36 .90 37 .10 38 .54 15.56 29 .60 45 .30 46 .17
TF 94.14 50 .50 82 .44 83 .45 82 .53 76.39 13 .60 74 .21 74 .54 58 .10 91.55 16 .21 39 .70 38 .40 39 .70 94.32 21 .77 40 .70 42 .30 55 .70
BAE 74.80 46 .98 48 .67 86 .90 27.50 37 .41 37 .88 62 .27 35.21 19 .84 20 .92 43 .65 44.00 38 .59 39 .01 65 .03
RAPIDPWWS 32.0990.11 95 .88 92 .36 5.7087.33 92 .47 69 .40 23.4494.03 98 .62 89 .85 15.5697.33 99 .99 94 .42
TF 94.30 50 .5090.29 96 .76 92 .14 76.45 13 .6087.49 93 .54 70 .50 91.70 16 .2194.03 99 .86 89 .72 94.24 21 .7793.85 99 .99 93 .96
BAE 74.8057.55 96 .25 93 .64 27.5082.46 96 .30 73 .06 35.2178.99 99 .28 89 .77 44.0080.55 99 .99 93 .89
Table 3: The performance of RAPID without pseudo-
similarity filtering (colored numbers indicate perfor-
mance declines in the ablation). The metrics not unaf-
fected by the pseudo-similarity filtering are omitted.
DATASET ATTACKER DTA RPA
AGNewsPWWS 94.19(−1.69↓)90.80(−1.56↓)
TF 94.26(−2.50↓)91.35(−0.79↓)
BAE 92.98(−3.27↓)91.44(−2.20↓)
Yahoo!PWWS 88.04(−4.43↓)65.38(−4.02↓)
TF 91.28(−2.26↓)67.48(−3.02↓)
BAE 92.48(−3.84↓)71.35(−1.71↓)
SST2PWWS 98.12(−0.50↓)87.80(−2.05↓)
TF 98.03(−1.83↓)88.40(−1.32↓)
BAE 95.87(−3.41↓)87.52(−2.25↓)
AmazonPWWS 99.99( 0.00) 94.40(−0.02↓)
TF 98.92(−1.07↓)93.31(−0.65↓)
BAE 98.53(−1.41↓)93.62(−0.27↓)
pair adversaries?
Methods :To address this RQ, we investigate the
discrepancy between adversaries and their repaired
counterparts in the feature space. Specifically,
we employ three attackers (i.e., BAE ,PWWS ,
TEXTFOOLER ) to generate adversaries and their
corresponding repaired examples, considering a
random selection of 1,000natural examples. Using
the victim model, we encode these examples into
the feature space and evaluate the cosine similarity
between adversary-natural example pairs and re-
paired adversary-natural example pairs. The larger
cosine similarity scores indicate better performance
in repairing the deep semantics in the adversaries.
Results :The box plots in Figures 1 and 4 show the
similarity score distributions collected from pair-
wise semantic similarity assessments. The seman-
tic similarity score distributions (e.g., the median
similarity scores of repaired examples are always
larger than the adversaries) from these plots revealTable 4: Performance of RAPID for adversarial detection
and defense against unknown adversarial attacks .
DATASET ATTACKER ATA DTA DFA RPA
AGNewsPSO 14.83 68.46 67.82 90.39
IGA 26.87 76.74 74.59 92.33
DEEPWORDBUG 45.53 72.73 87.23 89.33
CLARE 8.46 62.78 61.54 64.78
Yahoo!PSO 6.28 80.26 76.89 87.82
IGA 14.75 82.69 81.02 54.55
DEEPWORDBUG 51.34 72.73 87.10 62.27
CLARE 3.56 64.85 62.40 52.47
SST2PSO 7.95 87.50 87.50 82.61
IGA 18.39 89.33 98.67 87.68
DEEPWORDBUG 30.67 95.44 83.59 81.90
CLARE 2.59 62.50 59.37 65.30
AmazonPSO 5.76 90.48 90.48 91.55
IGA 14.91 92.31 92.31 94.65
DEEPWORDBUG 43.43 87.04 85.19 86.87
CLARE 3.25 60.44 59.37 62.94
a notable global similarity between the natural ex-
amples and repaired examples by RAPID , which
means RAPID does repair the deep semantics of
the adversaries. Conversely, it is apparent that the
similarity scores of the repaired examples obtained
using RS&V are indistinguishable from the adver-
sarial examples across all datasets. This situation
happens to many of the existing adversarial defense
methods. In conclusion, our observations show the
ability of RAPID to effectively repair the deep se-
mantics of adversaries.
RQ3: How does the inherent robustness of the
victim model affect R APID ?
Methods :We assessed the impact of the inherent
robustness of the victim model, focusing on DE-
BERT A, a cutting-edge PLM utilized across vari-
ous tasks. Specifically, we trained a victim model
based on DEBERT A, replicating the experimental
setup and evaluating the performance variation ofBAE PWWS TextFooler0.20.40.60.81
(c)RS&V -AGNewsCosine Similarity
BAE PWWS TextFooler0.40.60.81
(d)RS&V -YahooCosine SimilarityBAE PWWS TextFooler−0.400.40.81
(b)Rapid -YahooCosine Similarity
BAE PWWS TextFooler−1−0.500.51
(a)Rapid -AGNewsCosine Similarity
Adversarial
RepairedFigure 4: Box plots of semantic cosine similarity score
distributions on multi-categorial datasets. Similar to
Figure 1, RAPID is more competent to repair semantics
according to the feature similarity score distributions.
Table 5: The performance of RAPID on four public
datasets based on the victim model DEBERT A. The
numbers in redcolor indicate performance declines com-
pared to the BERT-based R APID .
DATASET ATTACKER NTA ATA DTA DFA RPA
AGNewsPWWS
96.6962.77 96.47 98.47 93.12
TF 39.85 91.41 95.90↓93.69
BAE 81.64 90.20 97.92 93.40↓
Yahoo!PWWS
78.6315.70 88.91 92.64 70.47
TF 6.19 89.32 92.60 69.96↓
BAE 47.50 90.25 93.74↓72.12↓
SST2PWWS
95.0137.14 95.21 98.42 94.15
TF 22.59 93.06↓99.08 94.58
BAE 38.84 80.82 98.59 94.16
AmazonPWWS
95.5122.72 97.62 99.99 94.55
TF 23.95 94.91 99.99 94.84
BAE 56.65 82.71 99.99 94.50
RAPID based on this D EBERT Avictim model.
Results :As in Table 5, the DEBERT A-based victim
model demonstrates superior accuracy under adver-
sarial attacks, indicating higher inherent robustness
inDEBERT Acompared to the victim model built
on BERT. In particular, D EBERT A-based R APID
excels in identifying adversaries across all classi-
fication datasets, especially on the binary datasets.
The performance in adversarial detection and de-
fense follows a similar upward trajectory. Em-
phasizing the substantial influence of the victim
model’s robustness on our method, particularly in
enhancing adversarial detection and defense.
5 Related Works
Prior research on adversarial defense can be
classified into three categories: adversarial
training-based methods (Miyato et al., 2017; Zhu
et al., 2020; Ivgi and Berant, 2021); contextreconstruction-based methods (Pruthi et al., 2019;
Liu et al., 2020b; Mozes et al., 2021; Keller et al.,
2021; Chen et al., 2021; Xu et al., 2022; Li
et al., 2022; Swenor and Kalita, 2022); and feature
reconstruction-based methods(Zhou et al., 2019;
Jones et al., 2020; Wang et al., 2021a). Some stud-
ies (Wang et al., 2021b) also investigated hybrid
defense methods. As for the adversarial training-
based methods, they are notorious for the perfor-
mance degradation of natural examples. They can
improve the robustness of PLMs by fine-tuning,
yet increasing the cost of model training caused by
catastrophic forgetting (Dong et al., 2021b). Text
reconstruction-based methods, such as word substi-
tution (Mozes et al., 2021; Bao et al., 2021) and
translation-based reconstruction, may fail to iden-
tify semantically repaired adversaries or introduce
new malicious perturbations (Swenor and Kalita,
2022). Feature reconstruction methods, on the
other hand, may struggle to repair typo attacks (Liu
et al., 2020a; Tan et al., 2020; Jones et al., 2020),
sentence-level attacks (Zhao et al., 2018; Cheng
et al., 2019), and other unknown attacks. There
are some works towards the adversarial detection
and defense joint task(Zhou et al., 2019; Mozes
et al., 2021; Wang et al., 2022b). However, these
adversarial detection methods may be ineffective
for unknown adversarial attackers and can hardly
alleviate resource waste in adversarial defense. An-
other similar work to RAPID isTextshield (Shen
et al., 2023), which aims to defend against word-
level adversarial attacks by detecting adversarial
sentences based on a saliency-based detector and
fixing the adversarial examples using a corrector.
Overall, our study focuses on maintaining the se-
mantics by introducing minimal safe perturbations
into adversaries, thus alleviating the semantic shift-
ing problem in all reconstruction-based works.
6 Conclusion
We propose a novel adversarial defense method,
i.e., perturbation defocusing, to repair semantics
in adversarial examples. RAPID addresses the se-
mantic shifting problem in the previous studies.
RAPID shows an outstanding performance in repair-
ing adversarial examples (up to ≈99% of correctly
identified adversarial examples). It is believed that
perturbation defocusing has the potential to signif-
icantly shift the landscape of textual adversarial
defense.Limitations
One limitation of the proposed method is that it
tends to introduce new perturbations into the adver-
saries, which may lead to semantic shifts. This may
be unsafe for some tasks, e.g., machine translation.
Furthermore, the method requires a large amount
of computational resources to generate the adver-
saries during the training phase, which may be a
limitation in some scenarios. Finally, the method
has not been tested on a wide range of NLP tasks
and domains, and further evaluations on other tasks
and domains are necessary to fully assess its capa-
bilities.
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-
Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial exam-
ples. In EMNLP’18: Proc. of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 2890–2896. Association for Computational
Linguistics.
Rongzhou Bao, Jiayi Wang, and Hai Zhao. 2021. De-
fending pre-trained language models from adversarial
word substitution without performance sacrifice. In
ACL-IJCNLP’21: Findings of the 2021 Conference of
the Association for Computational Linguistics: ACL-
IJCNLP 2021, Online Event, August 1-6, 2021 , vol-
ume ACL-IJCNLP 2021 of Findings of ACL , pages
3248–3258. Association for Computational Linguis-
tics.
Nicholas Boucher, Ilia Shumailov, Ross Anderson, and
Nicolas Papernot. 2022. Bad characters: Impercepti-
ble NLP attacks. In 43rd IEEE Symposium on Secu-
rity and Privacy, SP 2022, San Francisco, CA, USA,
May 22-26, 2022 , pages 1987–2004. IEEE.
Guandan Chen, Kai Fan, Kaibo Zhang, Boxing Chen,
and Zhongqiang Huang. 2021. Manifold adversar-
ial augmentation for neural machine translation. In
ACL-IJCNLP’21: Findings of the 2021 Conference of
the Association for Computational Linguistics , pages
3184–3189. Association for Computational Linguis-
tics.
Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.
Robust neural machine translation with doubly ad-
versarial inputs. In ACL’19: Proc. of the 57th Con-
ference of the Association for Computational Lin-
guistics , pages 4324–4333. Association for Compu-
tational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT’19: Proc. of the 2019Conference of the North American Chapter of the As-
sociation for Computational Linguistics , pages 4171–
4186. Association for Computational Linguistics.
Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong
Liu. 2021a. Towards robustness against natural lan-
guage word substitutions. In ICLR’21: Proc. of the
9th International Conference on Learning Represen-
tations . OpenReview.net.
Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng
Yan, and Hanwang Zhang. 2021b. How should pre-
trained language models be fine-tuned towards ad-
versarial robustness? In NeurIPS’21: Proc. of the
2021 Conference on Neural Information Processing
Systems , pages 4356–4369.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. Hotflip: White-box adversarial examples
for text classification. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 2: Short Papers , pages 31–36.
Association for Computational Linguistics.
Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-
jun Qi. 2018. Black-box generation of adversarial
text sequences to evade deep learning classifiers. In
SP’18: Proc. of the 2018 IEEE Security and Privacy
Workshops , pages 50–56. IEEE Computer Society.
Siddhant Garg and Goutham Ramakrishnan. 2020.
BAE: bert-based adversarial examples for text clas-
sification. In EMNLP’20: Proc. of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing , pages 6174–6181. Association for Com-
putational Linguistics.
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles,
Florian Stimberg, Dan Andrei Calian, and Timothy A.
Mann. 2021. Improving robustness using generated
data. In NeurIPS’21: Advances in Neural Informa-
tion Processing Systems , pages 4218–4233.
Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and
Douwe Kiela. 2021. Gradient-based adversarial at-
tacks against text transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November,
2021 , pages 5747–5757. Association for Computa-
tional Linguistics.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing. CoRR , abs/2111.09543.
Maor Ivgi and Jonathan Berant. 2021. Achieving model
robustness through discrete adversarial training. In
EMNLP’21: Proc. of the 2021 Conference on Empir-
ical Methods in Natural Language Processing , pages
1529–1544. Association for Computational Linguis-
tics.Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2020. Is BERT really robust? A strong
baseline for natural language attack on text classifi-
cation and entailment. In AAAI’20: Proc. of the 34th
AAAI Conference on Artificial Intelligence , pages
8018–8025. AAAI Press.
Erik Jones, Robin Jia, Aditi Raghunathan, and Percy
Liang. 2020. Robust encodings: A framework for
combating adversarial typos. In ACL’20: Proc. of
the 58th Annual Meeting of the Association for Com-
putational Linguistics Conference , pages 2752–2765.
Association for Computational Linguistics.
Yannik Keller, Jan Mackensen, and Steffen Eger. 2021.
Bert-defense: A probabilistic model based on BERT
to combat cognitively inspired orthographic adversar-
ial attacks. In ACL-IJCNLP’21: Findings of the 2021
Conference of the Association for Computational Lin-
guistics , volume ACL-IJCNLP 2021 of Findings of
ACL, pages 1616–1629. Association for Computa-
tional Linguistics.
Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris
Brockett, Ming-Ting Sun, and Bill Dolan. 2021. Con-
textualized perturbation for textual adversarial attack.
InNAACL-HLT’21: Proc. of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics , pages 5053–5069. Asso-
ciation for Computational Linguistics.
Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. 2019. Textbugger: Generating adversarial
text against real-world applications. In 26th Annual
Network and Distributed System Security Symposium,
NDSS 2019, San Diego, California, USA, February
24-27, 2019 . The Internet Society.
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,
and Xipeng Qiu. 2020. BERT-ATTACK: adversarial
attack against BERT using BERT. In EMNLP’20:
Proc. of the 2020 Conference on Empirical Methods
in Natural Language Processing , pages 6193–6202.
Association for Computational Linguistics.
Linyang Li, Demin Song, Jiehang Zeng, Ruotian Ma,
and Xipeng Qiu. 2022. Rebuild and ensemble: Ex-
ploring defense against text adversaries. CoRR ,
abs/2203.14207.
Hui Liu, Yongzheng Zhang, Yipeng Wang, Zheng Lin,
and Yige Chen. 2020a. Joint character-level word
embedding and adversarial stability training to de-
fend adversarial text. In AAAI’20: Proc. of the 34th
AAAI Conference on Artificial Intelligence , pages
8384–8391. AAAI Press.
Kai Liu, Xin Liu, An Yang, Jing Liu, Jinsong Su, Sujian
Li, and Qiaoqiao She. 2020b. A robust adversarial
training approach to machine reading comprehension.
InAAAI’20: Proc. of the Thirty-Fourth AAAI Con-
ference on Artificial Intelligence , pages 8392–8400.
AAAI Press.Takeru Miyato, Andrew M. Dai, and Ian J. Goodfel-
low. 2017. Adversarial training methods for semi-
supervised text classification. In ICLR’17: Proc. of
the 5th International Conference on Learning Repre-
sentations . OpenReview.net.
John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,
Di Jin, and Yanjun Qi. 2020. Textattack: A frame-
work for adversarial attacks, data augmentation, and
adversarial training in NLP. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
EMNLP 2020 - Demos, Online, November 16-20,
2020 , pages 119–126. Association for Computational
Linguistics.
Maximilian Mozes, Pontus Stenetorp, Bennett Klein-
berg, and Lewis D. Griffin. 2021. Frequency-guided
word substitutions for detecting textual adversarial
examples. In EACL’21: Proc. of the 16th Conference
of the European Chapter of the Association for Com-
putational Linguistics , pages 171–186. Association
for Computational Linguistics.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton.
2019. Combating adversarial misspellings with ro-
bust word recognition. In ACL’19: Proc. of the 57th
Conference of the Association for Computational Lin-
guistics , pages 5582–5591. Association for Compu-
tational Linguistics.
Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial exam-
ples through probability weighted word saliency. In
ACL’19: Proc. of the 57th Conference of the Associa-
tion for Computational Linguistics , pages 1085–1097.
Association for Computational Linguistics.
Jérôme Rony, Luiz G. Hafemann, Luiz S. Oliveira, Is-
mail Ben Ayed, Robert Sabourin, and Eric Granger.
2019. Decoupling direction and norm for efficient
gradient-based L2 adversarial attacks and defenses.
InCVPR’19: IEEE Conference on Computer Vision
and Pattern Recognition , pages 4322–4330. Com-
puter Vision Foundation / IEEE.
Lingfeng Shen, Ze Zhang, Haiyun Jiang, and Ying Chen.
2023. Textshield: Beyond successfully detecting ad-
versarial sentences in text classification. In ICLR’23:
The Eleventh International Conference on Learning
Representations . OpenReview.net.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y . Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2013, 18-21 October 2013, Grand Hyatt
Seattle, Seattle, Washington, USA, A meeting of SIG-
DAT, a Special Interest Group of the ACL , pages
1631–1642. ACL.Abigail Swenor and Jugal Kalita. 2022. Using random
perturbations to mitigate adversarial attacks on senti-
ment analysis models. CoRR , abs/2202.05758.
Samson Tan, Shafiq R. Joty, Lav R. Varshney, and Min-
Yen Kan. 2020. Mind your inflections! improving
NLP for non-standard englishes with base-inflection
encoding. In EMNLP’20: Proc. of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing , pages 5647–5663. Association for Com-
putational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019a. Superglue: A stick-
ier benchmark for general-purpose language under-
standing systems. In NeurIPS’19: Advances in Neu-
ral Information Processing Systems , pages 3261–
3275.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel R. Bowman.
2019b. GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding. In
ICLR’19: 7th International Conference on Learning
Representations . OpenReview.net.
Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen,
Shuohang Wang, and Bo Li. 2020. T3: tree-
autoencoder constrained adversarial text generation
for targeted attack. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20,
2020 , pages 6134–6150. Association for Computa-
tional Linguistics.
Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, and
Bo Li. 2022a. Semattack: Natural textual attacks via
different semantic spaces. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022,
Seattle, WA, United States, July 10-15, 2022 , pages
176–205. Association for Computational Linguistics.
Xiaosen Wang, Jin Hao, Yichen Yang, and Kun He.
2021a. Natural language adversarial defense through
synonym encoding. In UAI’21: Proc. of the 37th
Conference on Uncertainty in Artificial Intelligence ,
volume 161 of Proceedings of Machine Learning
Research , pages 823–833. AUAI Press.
Xiaosen Wang, Yifeng Xiong, and Kun He. 2022b. De-
tecting textual adversarial examples through random-
ized substitution and vote. In UAI, volume 180 of
Proceedings of Machine Learning Research , pages
2056–2065. PMLR.
Xiaosen Wang, Yichen Yang, Yihe Deng, and Kun He.
2021b. Adversarial training with fast gradient projec-
tion method against synonym substitution based text
attacks. In AAAI’21: Proc. of the 35th AAAI Confer-
ence on Artificial Intelligence , pages 13997–14005.
AAAI Press.
Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Wei-
wei Liu, and Shuicheng Yan. 2023. Better diffusionmodels further improve adversarial training. CoRR ,
abs/2302.04638.
Jianhan Xu, Cenyuan Zhang, Xiaoqing Zheng, Linyang
Li, Cho-Jui Hsieh, Kai-Wei Chang, and Xuanjing
Huang. 2022. Towards adversarially robust text clas-
sifiers by learning to reweight clean examples. In
ACL’22: Findings of the 2022 Conference of the As-
sociation for Computational Linguistics , pages 1694–
1707. Association for Computational Linguistics.
Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom
Goldstein, and Furong Huang. 2023. Exploring and
exploiting decision boundary dynamics for adversar-
ial robustness. CoRR , abs/2302.03015.
Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling
Wang, and Michael I. Jordan. 2020. Greedy attack
and gumbel attack: Generating adversarial examples
for discrete data. J. Mach. Learn. Res. , 21:43:1–
43:36.
Yichen Yang, Xiaosen Wang, and Kun He. 2022. Ro-
bust textual embedding against word-level adversar-
ial attacks. In UAI, volume 180 of Proceedings of Ma-
chine Learning Research , pages 2214–2224. PMLR.
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,
Meng Zhang, Qun Liu, and Maosong Sun. 2020.
Word-level textual adversarial attacking as combi-
natorial optimization. In ACL’20: Proc. of the 58th
Annual Meeting of the Association for Computational
Linguistics Conference , pages 6066–6080. Associa-
tion for Computational Linguistics.
Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji
Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan
Liu, and Maosong Sun. 2021. Openattack: An open-
source textual adversarial attack toolkit. In Proceed-
ings of the Joint Conference of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing, ACL 2021 - System Demon-
strations, Online, August 1-6, 2021 , pages 363–371.
Association for Computational Linguistics.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada , pages 649–657.
Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun
He. 2021. Crafting adversarial examples for neu-
ral machine translation. In ACL-IJCNLP’21: Proc.
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing ,
pages 1967–1977. Association for Computational
Linguistics.
Zhengli Zhao, Dheeru Dua, and Sameer Singh.
2018. Generating natural adversarial examples. In
ICLR’18: Proc. of the 6th International Conference
on Learning Representations . OpenReview.net.Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei
Wang. 2019. Learning to discriminate perturbations
for blocking adversarial attacks in text classification.
InEMNLP-IJCNLP’19: Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing , pages 4903–4912. As-
sociation for Computational Linguistics.
Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-
stein, and Jingjing Liu. 2020. Freelb: Enhanced ad-
versarial training for natural language understanding.
InICLR’20: Proc. of the 8th International Confer-
ence on Learning Representations . OpenReview.net.
7 Reproducibility
To encourage everyone interested in our work to
implement RAPID , we have taken the following
steps:
•We have created an online click-to-run
demo alailable at https://tinyurl.com/
22ercuf8 for easy evaluation. Everyone can
input adversarial examples and obtain the re-
paired examples immediately.
•We have released the detailed source codes
and processed datasets that can be retrieved
in the supplementary materials. This enables
everyone to access the official implementa-
tion, aiding in understanding the paper and
facilitating their own implementations.
•We will also release an online benchmark tool
for evaluating the performance of adversarial
attackers under the defense of RAPID . This
step is essential for reducing evaluation vari-
ance across different codebases.
These efforts are aimed at promoting the repro-
ducibility of our work and facilitating its imple-
mentation by the research community.
A Adversarial Attack
A.1 Word-level Adversarial Attack
Our focus is on defending against word-level adver-
sarial attacks. However, our method can be easily
adapted to different types of adversarial attacks.
Letx= (x1, x2,···, xn)be a natural sentence,
where xi,1≤i≤n, denotes a word. yis the
ground truth label. Word-level attackers generally
replace some original words with similar words
(e.g., synonyms) to fool the objective model. For
example, substituting xiwith ˆxigenerates an ad-
versary: ˆx= (x1,···,ˆxi,···, xn), where ˆxiis analternative substitution for xi. For an adversary ˆx,
the objective model Fpredicts its label as follows:
ˆy= argmax F(·|ˆx), (7)
where ˆy̸=yifˆxis a successful adversary. To
represent adversarial attacks to Fusing an adver-
sarial attacker A, we denote an adversarial attack
as follows:
(ˆx,ˆy)← A (F,(x, y)), (8)
where xandydenote the natural example and its
true label. ˆxandˆyare the perturbed adversary and
label, respectively.
A.2 Investigation of Textual Adversarial
Attack
This section delves into an examination of textual
adversarial attacks.
Traditional approaches, such as those noted
by Li et al. (2019) and Ebrahimi et al. (2018), of-
ten involve character-level modifications to words
(e.g., changing "good" to "go0d") to deceive mod-
els by altering their statistical patterns. In a dif-
ferent approach, knowledge-based perturbations,
exemplified by the work of Zang et al. (2020), em-
ploy resources like HowNet to confine the search
space, especially in terms of substituting words.
Recent research (Garg and Ramakrishnan, 2020;
Li et al., 2020) has investigated using pre-trained
models for generating context-aware perturba-
tions (Li et al., 2021). Semantic-based methods,
such as SemAttack (Wang et al., 2022a), typically
use BERT embedding clusters to create sophis-
ticated adversarial examples. This differs from
prior heuristic methods that employed greedy al-
gorithms (Yang et al., 2020; Jin et al., 2020) or ge-
netic algorithms (Alzantot et al., 2018; Zang et al.,
2020), as well as gradient-based techniques (Wang
et al., 2020; Guo et al., 2021) that concentrated on
syntactic limitations.
With the evolution of adversarial attack tech-
niques, numerous tools such as TextAttack (Morris
et al., 2020) and OpenAttack (Zeng et al., 2021)
have been developed and made available in the
open-source community. These resources facili-
tate deep learning researchers to efficiently assess
adversarial robustness with minimal coding. There-
fore, our experiments in adversarial defense are
conducted using the TextAttack framework, and
we extend our gratitude to the authors and contrib-
utors of TextAttack for their significant efforts.B Experiments Implementation
B.1 Experimental Adversarial Attackers
We employ BAE ,PWWS , and TEXTFOOLER to
generate adversaries for training the adversarial de-
tector. These attackers are chosen because they
represent different types of attacks, allowing us to
train a detector capable of recognizing a variety
of adversarial attacks. This detector exhibits good
generalization ability, which we confirm through
experiments with other adversarial attackers such
asIGA ,DEEPWORDBUG,PSO , and CLARE . In-
cluding a larger number of adversarial attackers in
the training process can further enhance the perfor-
mance of the detector. We provide a brief introduc-
tion to these adversarial attackers:
a)BAE (Garg and Ramakrishnan, 2020) gener-
ates perturbations by replacing and inserting
tagged words based on the candidate words
generated by the masked language model
(MLM). To identify the most important words
in the text, BAE employs a word deletion-
based importance evaluation method.
b)PWWS (Ren et al., 2019) is an adversarial
attacker based on synonym replacement, which
combines word significance and classification
probability for word replacement.
c)TEXTFOOLER (Jin et al., 2020) considers ad-
ditional constraints (such as prediction consis-
tency, semantic similarity, and fluency) when
generating adversaries. TEXTFOOLER uses a
gradient-based word importance measure to lo-
cate and perturb important words.
B.2 Hyperparameter Settings
We employ the following configurations for fine-
tuning classifiers:
1.The learning rates for both BERT and DE-
BERT Aare set to 2×10−5.
2.The batch size is 16, and the maximum sequence
modeling length is 128.
3. Dropouts are set to 0.1for all models.
4.The loss functions of all objectives use cross-
entropy.
5.The victim models and RAPID models are
trained for 5epochs.
6.The optimizer used for fine-tuning objective
models is AdamW .
Please refer to our released code for more details.B.3 Evaluation Metrics
In this section, we introduce the adversarial defense
metrics. First, we select a target dataset, referred to
asD, containing only natural examples. Our goal
is to generate adversaries that can deceive a victim
model FJ. We group the successful adversaries
into a subset called Dadvand the remaining natural
examples with no adversaries into another subset
calledDnat. We then combine these two subsets to
form the attacked dataset, Datt. We apply RAPID
toDattto obtain the repaired dataset, Drep. The
evaluation metrics used in the experiments are de-
scribed as follows:
NTA=TPD+TND
PD+ND
ATA=TPDatt+TNDatt
PDatt+NDatt
DTA=TP∗
Dadv+TN∗
Dadv
P∗
Dadv+N∗
Dadv
DFA=TPDadv+TNDadv
PDadv+NDadv
RPA=TPDrep+TNDrep
PDrep+NDrep
where TP,TN,PandNare the number of true
positives and true negatives, positive and negative
in standard classification, respectively. TP∗,TN∗,
P∗andN∗indicate the case numbers in adversarial
detection.
B.4 Experimental Environment
The experiments are carried out on a computer run-
ning the Cent OS 7 operating system, equipped
with an RTX 3090 GPU and a Core i-12900k pro-
cessor. We use the PyTorch 1.12 library and a
modified version of TextAttack, based on version
0.3.7.
C Ablation Experiments
C.1 Defense of LLM-based Adversarial
Attack
Recent years have witnessed the superpower
of large language models (LLMs) such as
ChatGPT (OpenAI, 2023), which we hypothesize
to have a stronger ability to generate adversaries.
In this subsection, we evaluate the defense perfor-
mance of RAPID against adversaries generated byTable 6: Defense performance of RAPID against adver-
sarial attacks generated by ChatGPT-3.5 .
DATASET ATTACKER DFA RPA
AGNews CHATGPTRS&V 59.0
RAPID 72.0
Yahoo! CHATGPTRS&V 49.0
RAPID 61.0
SST2 CHATGPTRS&V 37.0
RAPID 74.0
Amazon CHATGPTRS&V 58.0
RAPID 82.0
ChatGPT-3.5 . Specifically, for each dataset consid-
ered in our previous experiments, we use ChatGPT9
to generate 100adversaries and investigate the de-
fense accuracy achieved by R APID .
From the experimental results shown in Ta-
ble 6, we find that RAPID consistently outperforms
RS&V in terms of defense accuracy. Specifically,
in the SST2 dataset, RS&V records a defense ac-
curacy of 37.0%, however, RAPID impressively
repairs 74.0% of the attacks. Similar trends hold
for the Amazon andAGNews datasets, where RAPID
achieves defense accuracy of 82.0% and 72.0%
respectively, in contrast to the 58.0% and 59.0%
offered by RS&V . In conclusion, RAPID can de-
fend against various unknown adversarial attacks
which have a remarkable performance in contrast
to existing adversarial defense approaches.
C.2 Performance of RAPID based on Different
ˆAPD
InRAPID ,PDcan incorporate any adversarial at-
tacker or even an ensemble of attackers, as the
process doesn’t require prior knowledge of the spe-
cific malicious perturbations. Regardless of which
adversaries are deployed against RAPID ,PWWS
consistently seeks safe perturbations for the cur-
rent adversarial examples. The abstract nature of
PDis critical, allowing for adaptability and effec-
tiveness against a broad spectrum of adversarial
attacks, rendering it a versatile defense mechanism
in our study.
In order to investigate the impact of ˆAPDin
Phase #2 , we have implemented further experi-
ments to demonstrate the adversarial defense per-
formance of PDusing different attackers, e.g.,
TEXTFOOLER andBAE . The results are shown in
Table 7. According to the experimental results, it is
observed that PWWS has a similar performance to
9ChatGPT3.5-0301TEXTFOOLER inPD, while BAE is slightly infe-
rior to both PWWS andTEXTFOOLER . However,
the variance are not significant among different at-
tackers in PD, which means the performance of
RAPID is not sensitive to the choice of ˆAPD, in
contrast to the adversarial attack performance of
the adversarial attacker.
C.3 Performance of R APID without
Adversarial Training Objective
The rationale behind the adversarial training objec-
tiveLain our study is founded on two key hypothe-
ses.
a)Enhancing Adversarial Detection: We rec-
ognize an implicit link between the tasks of
adversarial training and adversarial example
detection. Our theory suggests that by incorpo-
rating an adversarial training objective, we can
indirectly heighten the model’s sensitivity to
adversarial examples, leading to more accurate
detection of such instances.
b)Improving Model Robustness: We posit that
an adversarial training objective can bolster the
model’s robustness, thereby mitigating perfor-
mance degradation when the model faces an
attack. This approach is designed to strengthen
the model against potential adversarial threats.
To validate these hypotheses, we conducted abla-
tion experiments on the adversarial training objec-
tive. The experimental setup was aligned with that
described in Table 2, and the results are outlined in
Table 8.
These experimental findings reveal that omitting
the adversarial training objective in RAPID con-
sistently leads to a reduction in model robustness
across all datasets. This reduction can be as sub-
stantial as approximately 30%, adversely affecting
the performance of the adversarial defense. Addi-
tionally, adversarial detection capabilities also di-
minish, with the most significant drop being around
20%. These results highlight the critical role of the
adversarial training objective in RAPID , confirming
its efficacy in enhancing both model robustness and
adversarial example detection capabilities.
C.4 Performance of RAPID without Multitask
Training Objective
Before developing RAPID , we carefully considered
the potential impact on classification performance
due to multitask training objectives. This consider-
ation was explored in our proof-of-concept experi-
ments.Table 7: The adversarial detection and defense performance of RAPID based on different backends ( ˆAPD). We
report the average accuracy of five random runs. “TF” indicates T EXTFOOLER .
DEFENDER ATTACKERAGNews (4-category) Yahoo! (10-category) SST2 (2-category) Amazon (2-category)
NTA A TA D TA D FA R PA NTA A TA D TA D TA R PA NTA A TA D TA D FA R PA NTA A TA D TA D FA R PA
RAPID (PWWS)PWWS 32.09 90 .11 95 .88 92 .36 5.70 87 .33 92 .47 69 .40 23.44 94 .03 98 .62 89 .85 15.56 97 .33 99 .99 94 .42
TF 94.30 50 .50 90 .29 96 .76 92 .14 76.45 13 .60 87 .49 93 .54 70 .50 91.55 16 .21 94 .03 99 .86 89 .72 94.32 21 .77 93 .85 99 .99 93 .96
BAE 74.80 57 .55 96 .25 93 .64 27.50 82 .46 96 .30 73 .06 35.21 78 .99 99 .28 89 .77 44.00 80 .55 99 .99 93 .89
RAPID (TF)PWWS 32.09 83 .67 94 .07 92 .27 5.70 65 .01 83 .25 65 .33 23.44 36 .90 98 .90 90 .67 15.56 29 .60 99 .99 94 .33
TF 94.30 50 .50 82 .44 96 .46 92 .67 76.45 13 .60 74 .21 92 .96 71 .00 91.55 16 .21 39 .70 99 .98 90 .73 94.32 21 .77 40 .70 99 .99 94 .33
BAE 74.80 46 .98 92 .68 91 .00 27.50 37 .41 86 .49 72 .67 35.21 19 .84 99 .98 91 .33 44.00 38 .59 99 .99 94 .33
RAPID (BAE)PWWS 32.09 83 .67 93 .22 92 .08 5.70 65 .01 81 .15 64 .00 23.44 36 .90 93 .92 87 .67 15.56 29 .60 99 .54 94 .00
TF 94.30 50 .50 82 .44 95 .96 92 .33 76.45 13 .60 74 .21 87 .79 67 .33 91.55 16 .21 39 .70 96 .55 89 .00 94.32 21 .77 40 .70 99 .61 93 .64
BAE 74.80 46 .98 95 .12 91 .33 27.50 37 .41 83 .78 72 .00 35.21 19 .84 97 .55 90 .00 44.00 38 .59 99 .15 93 .80
Table 8: The adversarial detection and defense performance of RAPID with (“w/”) and without (“w/o”) the
adversarial training objective. We report the average accuracy of five random runs. “ TF” indicates TEXTFOOLER .
DEFENDER ATTACKERAGNews (4-category) Yahoo! (10-category) SST2 (2-category) Amazon (2-category)
NTA A TA D TA D FA R PA NTA A TA D TA D TA R PA NTA A TA D TA D FA R PA NTA A TA D TA D FA R PA
RAPID (w/La)PWWS 32.09 90 .11 95 .88 92 .36 5.70 87 .33 92 .47 69 .40 23.44 94 .03 98 .62 89 .85 15.56 97 .33 99 .99 94 .42
TF 94.30 50 .50 90 .29 96 .76 92 .14 76.45 13 .60 87 .49 93 .54 70 .50 91.55 16 .21 94 .03 99 .86 89 .72 94.32 21 .77 93 .85 99 .99 93 .96
BAE 74.80 57 .55 96 .25 93 .64 27.50 82 .46 96 .30 73 .06 35.21 78 .99 99 .28 89 .77 44.00 80 .55 99 .99 93 .89
RAPID (w/oLa)PWWS 11.10 82 .88 92 .07 90 .70 3.46 78 .43 87 .42 63 .79 10.70 91 .41 99 .62 89 .60 16.5 96 .50 99 .30 93 .60
TF 94.44 16 .09 84 .88 93 .07 87 .28 76.32 0 .42 78 .65 78 .36 56 .72 91.54 5 .30 89 .48 95 .15 85 .80 94.29 17 .53 98 .63 99 .17 92 .78
BAE 67.93 83 .17 91 .49 91 .15 45.10 71 .89 75 .47 64 .56 25.70 57 .01 95 .64 87 .10 45.54 92 .67 99 .48 93 .31
DATASET MODEL VICTIM -S V ICTIM -M
AGNews BERT 94.30 93 .90(−0.40↓)
Yahoo! BERT 76.45 76 .61(+0.16↑)
SST2 BERT 91.70 91 .49(−0.21↓)
Amazon BERT 94.24 94 .24(—)
Table 9: Victim model’s accuracy (%) on clean dataset-
based single-task and multitask training scenarios, i.e.,
Victim-S andVictim-M respectively. The experiments
are based on the BERT model.
To delve deeper into this impact, we trained vic-
tim models as single-task models (i.e., no adversar-
ial detection objective and adversarial training ob-
jective), instead of multitask training, and then col-
lated detailed results for comparison with RAPID .
In this experiment, we focused solely on evaluat-
ing performance using pure natural examples. The
results of this comparison are outlined in Table 9.
The symbols " ↑" and " ↓" accompanying the num-
bers indicate whether the performance is better or
worse than that of the single-task model, respec-
tively.
Based on these results, it is apparent that the
inclusion of additional loss terms in multitask train-
ing objectives does impact the victim model’s per-
formance on clean examples. However, this influ-
ence is not substantial across all datasets and shows
only slight variations. This finding suggests that the
impact of multitask training objectives is relatively
minor when compared to traditional adversarialtraining methods.
C.5 Performance Comparison between
RAPID and Adversarial Training Baseline
DATASET ATTACKER RAPID AT
PWWS 92.36 60 .10
AGNews TF 92.14 61 .87
BAE 93.64 63 .62
PWWS 69.40 40 .21
Yahoo! TF 70.50 38 .75
BAE 73.06 42 .97
PWWS 89.85 32 .46
SST2 TF 89.72 31 .23
BAE 89.77 34 .61
PWWS 94.42 51 .90
Amazon TF 93.96 49 .49
BAE 93.89 49 .75
Table 10: The repaired performance of RAPID and
the adversarial training baseline. We report the av-
erage accuracy of five random runs. “ TF” indicates
TEXTFOOLER .
We have conducted experiments to showcase
the experimental results of the adversarial training
baseline ( AT). The victim model is BERT , and the
experimental setup is the same as for RAPID , in-
cluding the number of adversaries used for training.
We only show the metric of repaired accuracy, as
ATdoes not support detect-to-defense. The results
(i.e., R PA (%)) are available in Table 10.
For these experiments, we used BERT as the vic-
tim model and maintained the same experimentalsetup as for RAPID , including the number of adver-
saries used for training. It’s important to note that
we focus solely on the repaired accuracy metric, as
ATdoes not facilitate detect-to-defense function-
ality. From these results, it becomes apparent that
the traditional adversarial training baseline is less
effective compared to RAPID , which utilizes pertur-
bation defocusing. Specifically, the adversarial de-
fense accuracy of ATis generally below 50%. This
finding underscores the limitations of traditional
adversarial training methods, particularly their high
cost and reduced effectiveness against adapted ad-
versarial attacks.
C.6 Efficiency Evaluation of R APID
The main efficiency depends on multiple adversar-
ial perturbations search. We have implemented two
experiments to investigate the efficiency of RAPID .
Please note that the time costs for adversarial attack
and defense are dependent on specific software and
hardware environments.
Time Costs for Multiple Examples . We have
collected three small sub-datasets that contain dif-
ferent numbers of adversarial examples and natural
examples, say 200:0, 100:100, and 0:200. We apply
adversarial detection and defense to this dataset and
calculate the time costs. The results (measurement:
second) are available in Table 11.
Time Costs for Single Examples . We have also
detailed the time costs per natural example, adver-
sarial attack, and adversarial defense in PD˙The
experimental results can be found in Table 12.
According to the experimental results, PDis
slightly faster than the adversarial attack in most
cases. Intuitively, the perturbed semantics in a
malicious adversarial example are generally not
robust, as most of the deep semantics remain within
the adversarial example. Therefore, RAPID is able
to rectify the example with fewer perturbations
needed to search.
D Deployment Demo
We have created an anonymous demonstration of
RAPID , which is available on Huggingface Space10.
To illustrate the usage of our method, we provide
two examples in Figure 5. In this demonstration,
users can either input a new phrase along with a la-
bel or randomly select an example from a supplied
10https://huggingface.co/spaces/anonymous8/
RPD-Demodataset, to perform an attack, adversarial detection,
and adversarial repair./0 /1 /2 /i255 /4 /5 /6 /i255 /7 /8 /9 /10/11 /12 /5 /13 /14 /i255 /15 /5 /14 /16 /i255 /17 /18 /6 /19 /5 /20
/21 /22 /23 /22 /24 /25 /26 /22 /i255 /25 /23 /i255 /25 /28 /29 /22 /24 /30 /25 /24 /31 /25 /32 /i255 /22 /33 /25 /34/35 /32 /22 /i255 /26 /36 /i255 /24 /22 /35 /25 /31 /24 /i255 /37 /30 /31 /23 /38 /i255 /39 /40 /41 /i255 /42 /21 /40 /43 /44 /i255 /45 /i255 /46 /i255 /34/31 /23 /37 /26 /22 /47 /i255 /48 /40 /43 /44 /i255 /46 /49 /46 /50 /i255 /34/31 /23 /37 /26 /22 /30 /51
/48 /52 /22 /53 /54 /i255 /31 /55 /i255 /21 /40 /43 /i255 /25 /29 /25 /31 /32 /25 /56 /32 /22/39 /22 /25 /53 /26 /31 /29 /22 /i255 /40 /22 /24 /26 /37 /24 /56 /25 /26 /31 /36 /23 /i255 /41 /22 /55 /36 /53 /37 /30 /31 /23 /38 /i255 /55 /36 /24 /i255 /57 /22 /33 /26 /37 /25 /32 /i255 /58 /28 /29 /22 /24 /30 /25 /24 /31 /25 /32 /i255 /41 /22 /55 /22 /23 /30 /22
/48 /32 /25 /24 /31 /55 /31 /53 /25 /26 /31 /36 /23 /30
/59 /16 /5 /1 /i255 /19 /2 /60/20 /i255 /16 /6 /1 /i255 /61 /20 /i255 /60/2 /62 /16 /6 /61 /5 /1 /60/i255 /14 /20 /i255 /2 /61 /1 /12 /18 /2 /i255 /14 /16 /2 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /15 /5 /13 /13 /i255 /65 /2 /i255 /62 /20 /18 /18 /2 /62 /14 /13 /66 /i255 /18 /2 /64 /6 /5 /18 /2 /19 /i255 /65 /66 /i255 /67 /8 /68 /69 /i255 /59 /16 /2 /i255 /18 /2 /64 /6 /5 /18 /i255 /1 /12 /62 /62 /2 /1 /1 /i255 /18 /6 /14 /2 /i255 /5 /1 /i255 /6 /62 /14 /12 /6 /13 /13 /66 /i255 /14 /16 /2 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /18 /2 /64 /20 /18 /14 /2 /19 /i255 /5 /61 /i255 /14 /16 /2
/64 /6 /64 /2 /18 /i255 /71 /6 /64 /64 /18 /20 /63 /5 /60/6 /14 /2 /13 /66 /i255 /12 /64 /i255 /14 /20 /i255 /72 /73 /74/75 /69
/59 /16 /2 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /6 /61 /19 /i255 /18 /2 /64 /6 /5 /18 /2 /19 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /60/6 /66 /i255 /65 /2 /i255 /12 /61 /61 /6 /14 /12 /18 /6 /13 /i255 /14 /20 /i255 /18 /2 /6 /19 /76 /i255 /15 /16 /5 /13 /2 /i255 /5 /14 /i255 /5 /1 /i255 /65 /2 /62 /6 /12 /1 /2 /i255 /14 /16 /2 /i255 /6 /14 /14 /6 /62 /77 /2 /18 /1 /i255 /12 /1 /12 /6 /13 /13 /66 /i255 /78 /2 /61 /2 /18 /6 /14 /2 /i255 /12 /61 /61 /6 /14 /12 /18 /6 /13 /i255 /64 /2 /18 /14 /12 /18 /65 /6 /14 /5 /20 /61 /1 /69 /i255 /67 /8 /68
/19 /20 /2 /1 /i255 /61 /20 /14 /i255 /5 /61 /14 /18 /20 /19 /12 /62 /2 /i255 /6 /19 /19 /5 /14 /5 /20 /61 /6 /13 /i255 /12 /61 /61 /6 /14 /12 /18 /6 /13 /i255 /64 /2 /18 /14 /12 /18 /65 /6 /14 /5 /20 /61 /1 /69
/59 /20 /i255 /20 /12 /18 /i255 /65 /2 /1 /14 /i255 /77 /61 /20 /15 /13 /2 /19 /78 /2 /76 /i255 /67 /2 /6 /62 /14 /5 /4 /2 /i255 /8 /2 /18 /14 /12 /18 /65 /6 /14 /5 /20 /61 /i255 /68 /2 /70 /20 /62 /12 /1 /5 /61 /78 /i255 /5 /1 /i255 /6 /i255 /61 /20 /4 /2 /13 /i255 /6 /64 /64 /18 /20 /6 /62 /16 /i255 /5 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /19 /2 /70 /2 /61 /1 /2 /69 /i255 /67 /8 /68 /i255 /1 /5 /78 /61 /5 /70 /5 /62 /6 /61 /14 /13 /66 /i255 /71 /79 /80 /81 /74/i255 /19 /2 /70 /2 /61 /1 /2 /i255 /6 /62 /62 /12 /18 /6 /62 /66 /i255 /5 /60/64 /18 /20 /4 /2 /60/2 /61 /14 /75
/20 /12 /14 /64 /2 /18 /70 /20 /18 /60/1 /i255 /14 /16 /2 /i255 /1 /14 /6 /14 /2 /82 /20 /70 /82 /14 /16 /2 /82 /6 /18 /14 /i255 /60/2 /14 /16 /20 /19 /1 /69
/59 /16 /2 /i255 /68 /2 /2 /64 /83/20 /18 /19 /11 /12 /78 /i255 /5 /1 /i255 /6 /61 /i255 /12 /61 /77 /61 /20 /15 /61 /i255 /6 /14 /14 /6 /62 /77 /2 /18 /i255 /14 /20 /i255 /14 /16 /2 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /19 /2 /14 /2 /62 /14 /20 /18 /i255 /6 /61 /19 /i255 /18 /2 /6 /62 /14 /5 /4 /2 /i255 /19 /2 /70 /2 /61 /1 /2 /i255 /60/20 /19 /12 /13 /2 /69 /i255 /68 /2 /2 /64 /83/20 /18 /19 /11 /12 /78 /i255 /16 /6 /1 /i255 /19 /5 /84 /2 /18 /2 /61 /14 /i255 /6 /14 /14 /6 /62 /77 /5 /61 /78 /i255 /64 /6 /14 /14 /2 /18 /61 /1 /i255 /70 /18 /20 /60/i255 /20 /14 /16 /2 /18 /i255 /6 /14 /14 /6 /62 /77 /2 /18 /1
/6 /61 /19 /i255 /1 /16 /20 /15 /1 /i255 /14 /16 /2 /i255 /78 /2 /61 /2 /18 /6 /13 /5 /85 /6 /65 /5 /13 /5 /14 /66 /i255 /6 /61 /19 /i255 /18 /20 /65 /12 /1 /14 /61 /2 /1 /1 /i255 /20 /70 /i255 /67 /8 /68 /69
/86 /25 /26 /37 /24 /25 /32 /i255 /87 /33 /25 /34/35 /32 /22 /i255 /88 /23 /35 /37 /26
/89 /89 /59 /90
 /7 /17 /91 /2 /15 /1 /80 /81 /92
 /7 /60/6 /85 /20 /61
 /11 /7 /93
 /8 /83/83 /89
 /59 /2 /63 /14 /94 /20 /20 /13 /2 /18
 /68 /2 /2 /64 /83/20 /18 /19 /11 /12 /78
/9 /61 /64 /12 /14 /i255 /6 /i255 /61 /6 /14 /12 /18 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /69 /69 /69
/95 /18 /5 /78 /5 /61 /6 /13 /i255 /13 /6 /65 /2 /13 /76 /i255 /60/12 /1 /14 /i255 /65 /2 /i255 /6 /61 /i255 /5 /61 /14 /2 /78 /2 /18 /69 /69 /69
/8 /13 /2 /6 /1 /2 /i255 /62 /13 /5 /62 /77 /i255 /14 /20 /i255 /62 /16 /2 /62 /77
/21 /22 /23 /22 /24 /25 /26 /22 /28 /i255 /58 /28 /29 /22 /24 /30 /25 /24 /31 /25 /32 /i255 /87 /33 /25 /34/35 /32 /22 /i255 /25 /23 /28 /i255 /39 /22 /35 /25 /31 /24 /22 /28 /i255 /58 /28 /29 /22 /24 /30 /25 /24 /31 /25 /32 /i255 /87 /33 /25 /34/35 /32 /22
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /1 /16 /20 /15 /1 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255
/5 /19 /2 /6 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /77 /61 /20 /15 /1 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /19 /5 /4 /5 /19 /2 /1 /i255 /69
/80
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255
/2 /1 /14 /5 /60/6 /14 /2 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /1 /62 /18 /2 /15 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /1 /64 /13 /5 /14 /i255 /69
/81
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255
/2 /1 /14 /5 /60/6 /14 /2 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /65 /6 /61 /78 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /1 /64 /13 /5 /14 /i255 /69
/80
/87 /33 /25 /34/35 /32 /22 /i255 /41 /31 /97 /22 /24 /22 /23 /53 /22 /i255 /42 /48 /36 /34/35 /25 /24 /31 /30 /36 /23 /30 /51
/59 /16 /2 /i255 /71 /98 /75 /i255 /6 /61 /19 /i255 /71 /82 /75 /i255 /5 /61 /i255 /14 /16 /2 /i255 /65 /20 /63 /2 /1 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /i255 /14 /16 /2 /i255 /6 /19 /19 /2 /19 /i255 /6 /61 /19 /i255 /19 /2 /13 /2 /14 /2 /19 /i255 /62 /16 /6 /18 /6 /62 /14 /2 /18 /1 /i255 /5 /61 /i255 /14 /16 /2 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /62 /20 /60/64 /6 /18 /2 /19 /i255 /14 /20 /i255 /14 /16 /2 /i255 /20 /18 /5 /78 /5 /61 /6 /13 /i255 /5 /61 /64 /12 /14 /i255 /61 /6 /14 /12 /18 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /69
/59 /16 /2 /i255 /95 /18 /5 /78 /5 /61 /6 /13 /i255 /91 /6 /14 /12 /18 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /1 /16 /20 /15 /1 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255 /5 /19 /2 /6 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /77 /61 /20 /15 /1 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /19 /5 /4 /5 /19 /2 /1 /i255 /69
/99 /16 /6 /18 /6 /62 /14 /2 /18 /i255 /93 /19 /5 /14 /5 /20 /61 /1 /i255 /20 /70 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2 /i255 /99 /20 /60/64 /6 /18 /2 /19 /i255 /14 /20 /i255 /14 /16 /2 /i255 /91 /6 /14 /12 /18 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /1 /16 /20 /15 /1 /i255 /i255 /i255 /49 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /i255 /i255 /i255 /100 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255 /2 /1 /14 /i255 /i255 /i255 /100 /i255 /5 /i255 /19 /i255 /i255 /i255 /49 /i255 /60/6 /14 /i255 /i255 /i255 /100 /i255 /2 /i255 /6 /i255 /i255 /i255 /49 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /77
/61 /20 /i255 /i255 /i255 /49 /i255 /1 /62 /18 /2 /i255 /i255 /i255 /100 /i255 /15 /i255 /1 /i255 /i255 /i255 /49 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /19 /i255 /i255 /i255 /49 /i255 /1 /64 /13 /i255 /i255 /i255 /100 /i255 /5 /i255 /14 /i255 /i255 /i255 /100 /i255 /4 /5 /19 /2 /1 /i255 /i255 /i255 /49 /i255 /69
/99 /16 /6 /18 /6 /62 /14 /2 /18 /i255 /93 /19 /5 /14 /5 /20 /61 /1 /i255 /20 /70 /i255 /67 /2 /64 /6 /5 /18 /2 /19 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2 /i255 /99 /20 /60/64 /6 /18 /2 /19 /i255 /14 /20 /i255 /14 /16 /2 /i255 /91 /6 /14 /12 /18 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2
/6 /61 /62 /16 /20 /18 /2 /19 /i255 /65 /66 /i255 /6 /i255 /14 /2 /18 /18 /5 /70 /5 /62 /i255 /64 /2 /18 /70 /20 /18 /60/6 /61 /62 /2 /i255 /65 /66 /i255 /6 /65 /65 /6 /1 /1 /i255 /76 /i255 /1 /6 /14 /5 /61 /i255 /18 /20 /12 /78 /2 /i255 /1 /16 /20 /15 /1 /i255 /i255 /i255 /49 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /i255 /i255 /i255 /100 /i255 /14 /16 /6 /14 /i255 /14 /16 /2 /i255 /2 /1 /14 /i255 /i255 /i255 /100 /i255 /5 /i255 /19 /i255 /i255 /i255 /49 /i255 /60/6 /14 /i255 /i255 /i255 /100 /i255 /2 /i255 /6 /i255 /i255 /i255 /49 /i255 /20 /70 /i255 /15 /20 /60/2 /61 /i255 /96 /1 /i255 /1 /2 /13 /70 /82 /6 /62 /14 /12 /6 /13 /5 /85 /6 /14 /5 /20 /61 /i255 /77
/i255 /i255 /49 /i255 /65 /6 /i255 /i255 /i255 /100 /i255 /61 /i255 /78 /i255 /i255 /i255 /100 /i255 /20 /15 /1 /i255 /i255 /i255 /49 /i255 /70 /2 /15 /i255 /62 /20 /61 /14 /5 /61 /2 /61 /14 /6 /13 /i255 /19 /i255 /i255 /i255 /49 /i255 /1 /64 /13 /i255 /i255 /i255 /100 /i255 /5 /i255 /14 /i255 /i255 /i255 /100 /i255 /4 /5 /19 /2 /1 /i255 /i255 /i255 /49 /i255 /69
/57 /52 /22 /i255 /101 /37 /26 /35 /37 /26 /i255 /36 /55 /i255 /39 /22 /25 /53 /26 /31 /29 /22 /i255 /40 /22 /24 /26 /37 /24 /56 /25 /26 /31 /36 /23 /i255 /41 /22 /55 /36 /53 /37 /30 /31 /23 /38
/102/103 /104 /105 /106/107
/59 /16 /2 /i255 /5 /1 /108 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /70 /5 /2 /13 /19 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /1 /i255 /5 /70 /i255 /6 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /5 /1 /i255 /19 /2 /14 /2 /62 /14 /2 /19 /69 /i255 /59 /16 /2
/64 /2 /18 /14 /12 /18 /65 /2 /19 /108 /13 /6 /65 /2 /13 /i255 /5 /1 /i255 /14 /16 /2 /i255 /64 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /13 /6 /65 /2 /13 /i255 /20 /70 /i255 /14 /16 /2 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /69 /i255 /59 /16 /2 /i255 /62 /20 /61 /70 /5 /19 /2 /61 /62 /2
/70 /5 /2 /13 /19 /i255 /18 /2 /64 /18 /2 /1 /2 /61 /14 /1 /i255 /14 /16 /2 /i255 /62 /20 /61 /70 /5 /19 /2 /61 /62 /2 /i255 /20 /70 /i255 /14 /16 /2 /i255 /64 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /19 /2 /14 /2 /62 /14 /5 /20 /61 /69/107 /109 /110 /111 /111/112 /113 /104 /104 /106 /114 /103/103 /104 /105 /106/102
/9 /70 /i255 /5 /1 /108 /18 /2 /64 /6 /5 /18 /2 /19 /115 /14 /18 /12 /2 /76 /i255 /5 /14 /i255 /16 /6 /1 /i255 /65 /2 /2 /61 /i255 /18 /2 /64 /6 /5 /18 /2 /19 /i255 /65 /66 /i255 /67 /8 /68 /69 /i255 /59 /16 /2 /i255 /64 /18 /2 /19 /108 /13 /6 /65 /2 /13 /i255 /70 /5 /2 /13 /19 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /1 /i255 /14 /16 /2
/1 /14 /6 /61 /19 /6 /18 /19 /i255 /62 /13 /6 /1 /1 /5 /70 /5 /62 /6 /14 /5 /20 /61 /i255 /18 /2 /1 /12 /13 /14 /69 /i255 /59 /16 /2 /i255 /62 /20 /61 /70 /5 /19 /2 /61 /62 /2 /i255 /70 /5 /2 /13 /19 /i255 /18 /2 /64 /18 /2 /1 /2 /61 /14 /1 /i255 /14 /16 /2 /i255 /62 /20 /61 /70 /5 /19 /2 /61 /62 /2 /i255 /20 /70 /i255 /14 /16 /2
/64 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /13 /6 /65 /2 /13 /69 /i255 /59 /16 /2 /i255 /5 /1 /108 /62 /20 /18 /18 /2 /62 /14 /i255 /70 /5 /2 /13 /19 /i255 /5 /61 /19 /5 /62 /6 /14 /2 /1 /i255 /15 /16 /2 /14 /16 /2 /18 /i255 /14 /16 /2 /i255 /64 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /13 /6 /65 /2 /13 /i255 /5 /1 /i255 /62 /20 /18 /18 /2 /62 /14 /69
/116 /117 /118 /119 /120 /121 /122 /118 /116 /122 /120 /123 /124 /125 /121 /126 /122 /127 /123 /125 /127 /120 /125 /128 /129 /122 /127 /130 /131 /127 /132 /122 /121 /124 /128 /125 /132 /122 /128
 /116 /117 /118 /119 /120 /121 /122 /118 /116 /122 /120 /123 /124 /116 /117 /127 /127 /122 /116 /130 /120 /123 /124 /127 /122 /129 /125 /120 /127 /122 /121 /129 /127 /122 /121 /124 /128 /125 /132 /122 /128/89 /2 /13 /2 /62 /14 /i255 /6 /i255 /14 /2 /1 /14 /5 /61 /78 /i255 /19 /6 /14 /6 /1 /2 /14 /i255 /6 /61 /19 /i255 /6 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /6 /14 /14 /6 /62 /77 /2 /18 /i255 /14 /20 /i255 /78 /2 /61 /2 /18 /6 /14 /2 /i255 /6 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13
/2 /63 /6 /60/64 /13 /2 /69/99 /16 /20 /20 /1 /2 /i255 /6 /61 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /7 /14 /14 /6 /62 /77 /2 /18 /i255 /70 /20 /18 /i255 /78 /2 /61 /2 /18 /6 /14 /5 /61 /78 /i255 /6 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /14 /20 /i255 /6 /14 /14 /6 /62 /77
/14 /16 /2 /i255 /60/20 /19 /2 /13 /69
/7 /13 /14 /2 /18 /61 /6 /14 /5 /4 /2 /13 /66 /76 /i255 /5 /61 /64 /12 /14 /i255 /6 /i255 /61 /6 /14 /12 /18 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /i255 /6 /61 /19 /i255 /5 /14 /1 /i255 /20 /18 /5 /78 /5 /61 /6 /13 /i255 /13 /6 /65 /2 /13 /i255 /71 /70 /18 /20 /60/i255 /6 /65 /20 /4 /2 /i255 /19 /6 /14 /6 /1 /2 /14 /1 /75
/14 /20 /i255 /78 /2 /61 /2 /18 /6 /14 /2 /i255 /6 /61 /i255 /6 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /2 /63 /6 /60/64 /13 /2 /69/95 /18 /5 /78 /5 /61 /6 /13 /i255 /133 /6 /65 /2 /13
/17 /8 /0 /i255 /1 /14 /6 /14 /12 /1
/95 /18 /5 /78 /5 /61 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2/95 /18 /5 /78 /5 /61 /6 /13 /i255 /133 /6 /65 /2 /13
/7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2/8 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /133 /6 /65 /2 /13 /i255 /20 /70 /i255 /14 /16 /2 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2
/67 /2 /64 /6 /5 /18 /2 /19 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2 /i255 /65 /66 /i255 /67 /8 /68/8 /18 /2 /19 /5 /62 /14 /2 /19 /i255 /133 /6 /65 /2 /13 /i255 /20 /70 /i255 /14 /16 /2 /i255 /67 /2 /64 /6 /5 /18 /2 /19 /i255 /7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2
/7 /19 /4 /2 /18 /1 /6 /18 /5 /6 /13 /i255 /93 /63 /6 /60/64 /13 /2 /i255 /68 /2 /14 /2 /62 /14 /5 /20 /61 /i255 /67 /2 /1 /12 /13 /14 /67 /2 /64 /6 /5 /18 /2 /19 /i255 /89 /14 /6 /61 /19 /6 /18 /19 /i255 /99 /13 /6 /1 /1 /5 /70 /5 /62 /6 /14 /5 /20 /61 /i255 /67 /2 /1 /12 /13 /14Figure 5: The demo examples of adversarial detection and defense built on RAPID for defending against multi-
attacks. The comparisons between natural and repaired examples are available based on the “ difflib ” library. The “ +”
and “−” in the colored boxes indicate letters addition and deletion compared to the natural examples. It is observed
thatRAPID only injects only one perturbation to repair the adversarial example, i.e., changing “screw” to “bang” in
the adversarial example.ATTACKERAGNews Yahoo! SST2 Amazon
200:0 100:100 0:200 200:0 100:100 0:200 200:0 100:100 0:200 200:0 100:100 0:200
PWWS 142.090 298 .603 313.317 621 .196 36.268 126 .054 438.532 875 .083
TF 1.188 146 .654 293 .542 1.157 314 .926 642 .206 1.092 51 .303 137 .795 1.138 329 .075 665 .052
BAE 141.434 260 .231 352.186 876 .606 52.626 138 .325 349.256 655 .264
Table 11: The efficiency of RAPID defending against different adversarial attacks with different portions of natural
and adversarial instances. The measurement is second.
DEFENDER ATTACKERAGNews Yahoo! SST2 Amazon
CLEAN ATTACK DEFENSE CLEAN ATTACK DEFENSE CLEAN ATTACK DEFENSE CLEAN ATTACK DEFENSE
PWWS 2.081 1 .356 4.958 3 .308 0.529 0 .588 4.745 3 .678
RAPID TF 0.008 2 .460 1 .317 0.008 4 .693 3 .128 0.006 0 .662 0 .571 0.007 4 .003 4 .607
BAE 2.464 1 .295 5.194 4 .053 0.669 0 .594 4.350 4 .403
Table 12: The execution efficiency of inferring clean examples, generating, and defending against adversarial
examples.