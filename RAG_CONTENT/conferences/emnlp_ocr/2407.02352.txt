Pelican: Correcting Hallucination in Vision-LLMs via Claim
Decomposition and Program of Thought Verification
Pritish Sahu*, Karan Sikka*, Ajay Divakaran
SRI International, Princeton, NJ
{pritish.sahu, karan.sikka, ajay.divakaran}@sri.com
Abstract
Large Visual Language Models (LVLMs) strug-
gle with hallucinations in visual instruction fol-
lowing task(s), limiting their trustworthiness
and real-world applicability. We propose
Pelican – a novel framework designed to de-
tect and mitigate hallucinations through claim
verification. Pelican first decomposes the vi-
sual claim into a chain of sub-claims based on
first-order predicates. These sub-claims consist
of (predicate, question) pairs and can be con-
ceptualized as nodes of a computational graph.
We then use Program-of-Thought prompting to
generate Python code for answering these ques-
tions through flexible composition of external
tools. Pelican improves over prior work by in-
troducing (1) intermediate variables for precise
grounding of object instances, and (2) shared
computation for answering the sub-question to
enable adaptive corrections and inconsistency
identification. We finally use reasoning abili-
ties of LLMs to verify the correctness of the
claim by considering the consistency and confi-
dence of the (question, answer) pairs from each
sub-claim. Our experiments reveal a drop in
hallucination rate by ∼8%−32% across vari-
ous baseline LVLMs and a 27% drop compared
to approaches proposed for hallucination miti-
gation on MMHal-Bench. Results on two other
benchmarks further corroborate our results.
1 Introduction
Large Vision Language Models (LVLM) have seen
significant advancements in recent years (Liu et al.,
2023b; Wu et al., 2023). They typically integrate
visual tokens into the embedding space of a Large
Language Model (LLM), leveraging the linguistic
capabilities of LLMs while incorporating visual
information for multimodal understanding. Despite
substantial performance gains, LVLMs suffer from
hallucinations due to limited training data, lack of
*Equal contributionprecise grounding, and over-reliance on language
priors (Liu et al., 2024a).
Prior works have focused on scaling training data
for reducing hallucinations, as demonstrated by the
improved performance of LLaV A-1.5 (Liu et al.,
2023b) that used many academic datasets during
instruction tuning. Other works have created high-
quality visual instruction tuning datasets. For ex-
ample, LRV (Liu et al., 2023a) included diverse ex-
amples as well as adversarial questions referencing
non-existent objects. Another promising direction
has been to improve the model by using variants
of reinforcement learning with human feedback
(RLHF) to align the model with human preferences
(Chen et al., 2023; Yu et al., 2023) or by training
the model to correct itself through self-feedback
during inference (Lee et al., 2023). Woodpecker
(Yin et al., 2023), inspired by the fact-checking
task in NLP (Guo et al., 2022), recently proposed
to correct hallucinations by modeling the problem
as verifying and correcting visual claims generated
by LVLMs. The method involved extracting key
concepts from the outputs, formulating questions,
answering them using visual tools such as VQA,
and collating the outputs into a visual claim, which
is then used to refine the original output with an
LLM. Our work builds upon and extends this claim
verification paradigm.
We propose
 Pelican (Figure 1), a novel and
structured pipeline for verifying visual claims to de-
tect and correct hallucinations. Pelican addresses
the weaknesses of prior methods for claim veri-
fication such as lack of precise grounding, weak
contextual integration and visual referencing, and
ineffective reasoning over the claim and the vi-
sual context. Pelican first breaks down a complex
claim into more manageable sub-claims by using a
set of predefined first-order predicates tailored to
the visual question answering (VQA) task. These
sub-claims are represented as a chain of (predi-
cate, question) pairs, with each question stemmingarXiv:2407.02352v2  [cs.CL]  29 Oct 2024from its corresponding predicate. The resulting
chain can be interpreted as a computational graph
(Figure 2), where each node corresponds to a predi-
cate/question. The verification of the overall claim
is then accomplished by answering the questions
in a sequential manner. Pelican uses Program-of-
Thought prompting to synthesize Python code that
seamlessly integrates external tools with Python
operators, offering greater flexibility compared to
previous methods like Woodpecker. The introduc-
tion of intermediate variables to reference specific
object instances is another innovation, which is cru-
cial for precise grounding, particularly in claims
involving multiple objects. Furthermore, Pelican
shares computations from previous nodes in the
chain while answering questions, enabling adaptive
corrections and the identification of inconsistencies
in the reasoning process, distinguishing it from
earlier works. Pelican creates a visual table repre-
sentation, that includes key visual entities, stored as
a Pandas dataframe to simplify manipulation of key
entities in the code generation step. The informa-
tion from the sub-claims is then combined, and the
reasoning capabilities of LLM are used to assess
the correctness of the claim and generate a refine-
ment in case of hallucinations. Robustness in this
step is ensured by using in-context examples and
promoting CoT-style reasoning that considers the
correctness, confidence, and relevance of the gener-
ated answers for each sub-claim. By integrating the
flexibility of reasoning in language with the preci-
sion of computational methods, Pelican is able to
achieve strong improvements over SOTA methods.
We show significant drop in hallucinations on stan-
dard benchmarks (MMHal-Bench, GA VIE) and im-
prove visual understanding accuracy on the MME
dataset. Our ablation study reveals the contribution
of the key innovations on the final performance.
Through qualitative examples, we demonstrate how
the model identifies and corrects hallucinated loca-
tions.
Our contributions are as follows, we:
1.Propose a robust pipeline for identifying hallu-
cination in LVLMs by decomposing the visual
claim into sub-claims that consist of questions
grounded in first-order predicates.
2.Enable precise grounding through intermedi-
ate variables for referencing object instances.
3.Generate Python code to answer sub-
questions with external tools, enabling flexibletool composition with Python operators.
4.Enhance reasoning by sharing computations
between questions, allowing for adaptive cor-
rections and identify inconsistencies.
5.Demonstrate consistent performance improve-
ment over different baseline LVLMs on sev-
eral benchmarks, as well as improvements
relative to existing approaches for mitigating
hallucinations. For MMHal-Bench, we reduce
by∼8%−32% on LVLMs and 27% over
best hallucination mitigation baseline. We
also show similar improvements on GA VIE
and MME.
2 Related Work
Large Visual Language Models and Hallucina-
tions Recent LVLMs (Alayrac et al., 2022; Zhu
et al., 2023; Dai et al., 2024; Huang et al., 2024;
Peng et al., 2023; Liu et al., 2024c, 2023b) demon-
strate superior performance on established bench-
marks with strong instruction-following and zero-
shot reasoning capabilities. However, these models
still suffer from hallucinations and provide incor-
rect answers or fabricate visual details. This issue
arises from several sources: the lack of diverse
training data leading to insufficient representation
of varied visual contexts (Zhu et al., 2023; Dai et al.,
2024), over-reliance on natural language cues (Hu
et al., 2023; Liu et al., 2024c, 2023b), a yes-bias
tendency to affirmatively answer regardless of the
visual content (Liu et al., 2024c), short text descrip-
tions that do not fully cover the image (Dai et al.,
2024), and synthetic data generation that can intro-
duce non-factual statements (Liu et al., 2024c). We
propose a post-hoc procedure to address halluci-
nations in LVLMs by integrating visual tools with
LLMs to ground and reason about their outputs by
analyzing a chain of simpler sub-claims. We refer
readers to Appendix D for other works.
Claim Decomposition The use of Language
Models (LMs) for fact-checking has gained sig-
nificant attention. (Lee et al., 2020) leveraged
LLMs to verify simple facts, while (Atanasova
et al., 2022) addressed the challenge of insufficient
evidence. (Li et al., 2023a) enhanced fact-checking
by retrieving current information from the web, and
(Cao, 2023) incorporated Chain of Thought (CoT)
prompting. Subsequent works (Kotonya and Toni,
2020; Atanasova, 2024) focused on generating ex-
planations by summarizing the comments providedFigure 1: Overview of
 Pelican . Given an image ( I) and a question ( q), LVLM ( VL) outputs an answer ( a). We
transform the pair ( q,a) to our claim ( c). Our pipeline: Step 1: Visual Table constructs a tabular representation of the
image, identifying the locations of visual objects using detection tools. For example, a row is created for the detected
“motorcycle" with its bounding box. Step 2: Claim Decomposition generates a list of granular sub-claims and
follow-up questions. This example also shows an intermediate variable, $person_riding, that is used for referencing
a specific object. Step 3: Program of Thought translates these questions into Python code using the POT approach.
Final Step: Integrated Verification Synthesis performs comprehensive reasoning assessments to validate the original
claims using the answers from Step 3.
by professional fact-checkers. In the domain of
fact-checking, claim verification focuses on validat-
ing claims by retrieving information from external
sources. Recent works, such as (Li et al., 2023a;
Wang and Shu, 2023), involve decomposing claims
into granular sub-claims for verification with an
LLM. Woodpecker (Yin et al., 2023) proposed a
similar framework for visual claim verification to
correct hallucination. Our work advances these
approaches by tightly integrating the chain of sub-
claims through (1) synthesizing Python code for
answering sub-claims, and (2) using shared vari-
ables and shared computations between sub-claims
to improve efficiency and consistency.
Tool Calling Recent works, such as VisProg,
HuggingGPT, and ViperGPT (Gupta and Kem-
bhavi, 2023; Shen et al., 2023; Schick et al.,
2023; Surís et al., 2023) fall under the umbrella
of Program-of-Thought prompting (Chen et al.,
2022) that leverage the strong coding capabilitiesof LLMs to compose external tools for complex
multi-modal tasks. VisProg uses in-context learn-
ing to generate modular programs for visual tasks,
while HuggingGPT employs ChatGPT to plan and
manage AI models from Hugging Face. The pro-
posed work is inspired by these ideas and extends
this ability to answer questions via code execution
and further integrate responses from the chain of
sub-claims to verify the overall claim.
3
 Pelican
Problem Formulation: We are given outputs
from a Large Vision Language Model (LVLM) VL,
which provides an answer a=V(I, q)for a given
image I ∈ RM×N×3(where MandNrepresent
the height and width of the image) and question q.
We combine the question-answer pair (q, a)into a
claimCabout the image using a Large Language
Model (LLM) for the remaining steps. We formu-
late the problem of detecting and correcting halluci-nations in aas a visual claim verification task (Yin
et al., 2023), which inputs (I,C)and output (d, r),
where d∈ {correct, incorrect }andrrefer to the
decision regarding the correctness of the claim and
a rewrite (if needed) respectively.
We propose Pelican (Figure 1) for verifying vi-
sual claims to detect and correct hallucinations.
Pelican first parses and decomposes the claim
into granular sub-claims, consisting of follow-up
questions, by leveraging reasoning capabilities of
LLMs. Compared to prior works (Yin et al., 2023),
Pelican introduces intermediate variables to refer-
ence specific instances of objects, enabling precise
grounding and visual referencing throughout the
verification process. We then harness the coding
capabilities of LLMs to answer the follow-up ques-
tions by translating them into Python code using a
Program-of-Thought approach (Chen et al., 2022)
This allows for the flexible composition of visual
tools using native Python operators. We share the
results from previous computations when answer-
ing the next question in the chain to facilitate adap-
tive corrections and catch inconsistencies in the
reasoning process often arising from brittleness of
visual tools. We also handle the limitations of vi-
sual tools (e.g., miss-detections) by mapping the
image into a tabular representation that consists of
visual objects identified in the image. We argue
that the proposed decomposition implicitly con-
verts the claim into a computational graph with de-
pendencies between sub-questions, while the PoT
framework enables Pelican to answer questions
by combining the flexibility of reasoning in natu-
ral language with the precision of computational
methods, resulting in a more effective and robust
claim verification process. We next discuss these
components in detail.
3.1 Visual Table
Our approach relies on tools such as object detec-
tors to ground visual entities in the image. During
our initial experiments, we identified several limi-
tations with off-the-shelf object detectors. For in-
stance, closed-vocabulary object detectors are more
powerful but cannot detect novel objects, while
open-vocabulary detectors may incorrectly iden-
tify objects that are not present, leading to false-
positives. To address these limitations, we carefully
crafted a pipeline (refer to Appendix B in the ap-
pendix) that utilizes YOLO and Grounding-DINO
to determine the presence and location of an ob-
ject in the image. Given the image and the claim,Exists(dog, Yes)
Position(dog, right) Position(dog, left)
dog_right ←Select(dog, right) dog_left ←Select(dog, left)
Color($dog_right, brown) Color($dog_left, black)
Figure 2: Computational graph representation of the
generated sub-claims with predicates as the node and
edges defined by their dependencies.
we produce a visual table representation by first
parsing the claim using an LLM, with in-context
examples, to identify key entities that are tangi-
ble and can be visually grounded. This is done
to reduce the likelihood of false positives. For
example, the claim “The disposable coffee cups
are upside down on the nightstand" will be parsed
into{cups, nightstand }. These entities are then
passed to the detectors to create a table T.
Although this step could be offloaded to the code
generation step, we chose not to do so for two pri-
mary reasons: (1) to avoid complex Python code
and allow it to focus on the high-level task of com-
posing tools and reasoning, and (2) because the
visual table is stored as a Pandas dataframe, it pro-
vides a flexible data structure that can be easily
manipulated in Python. To further reduce the false
positives from Grounding-DINO, we also utilize
the Visual Question Answering (VQA) tool to ver-
ify the existence of the detected objects.
3.2 Claim Decomposition
Since answering simpler questions about images
using external tools (e.g., determining if an object
exists) is more reliable, we first propose a novel
way to decompose the visual claim Cinto atomic
sub-claims. Each sub-claim is represented as a
pair consisting of a predicate and its correspond-
ing question (pi, qi). Each (qi)can be conceptual-
ized as a node in a computational graph (Figure 2),
where the edges represent the logical connections
between the questions. This graph structure will
be employed to reason about the claim and deter-
mine its veracity by systematically answering the
sub-claims.
To achieve this decomposition, we define a set
of predicates such as Exists andPosition , which
are loosely structured based on a taxonomy of ques-
tions posed in VQA tasks (Table 4). These predi-
cates serve as the foundation for deriving specific
questions. For instance, a claim involving an ob-ject’s properties and interactions may be broken
down into predicates concerning the presence, at-
tributes, and relations of that object. We transform
the claim Cinto predicates P={pi}L
i=1by prompt-
ing an LLM with in-context examples. We use a set
of∼10examples covering all the predicates and
scenarios such as claims with negations. Addition-
ally, we generate a chain of questions Q={qi}K
i=1
grounded in each of the predicates.
We introduce intermediate variables vto refer-
ence specific object instances, which is critical in
verifying claims about specific object instances.
Moreover, this approach allows us to create de-
pendencies between nodes of the computational
graph. This step not only reduces computational
redundancy but also improves the reliability of the
verification process.
3.3 Program of Thought (PoT)-based
Sub-Claim Verification
We now wish to verify each sub-claim, generated
in the decomposition step, by generating a visually
grounded answer to each sub-question qi. While an
off-the-shelf instruction-tuned LVLM can be used
to answer questions, this approach is constrained by
issues in LVLMs, such as hallucinations and lack
of interpretability. Instead, we employ a PoT-based
strategy to synthesize programmatic instructions
that composes different visual tools to infer the an-
swers to these sub-claims. For example, a question
about the color of an object might be translated into
a code snippet that first uses a detector to crop the
object and then applies a VQA tool to determine
the object’s color in the image. A key advantage of
using code to answer the question is the ability to
combine different visual tools flexibly with Python
operators.
We denote this step by the function λwhich in-
puts the image, current question and the context
from prior questions and answer to generate Python
code ckasλ(T, qk,{qi, ai}k−1
i=1) =ck, which is
then used to derive the answer as ak=exec (ck),
where exec is a Python interpreter. We introduce
several innovations compared to prior claim verifi-
cation works to perform robust verification:
1.Sharing computations between sub-claims:
We provide the answers derived in the previ-
ous sub-claims as context when answering the
next question in the chain. We found this pro-
cess to reduce duplicate computations, adap-
tively correct follow-up questions, and alsocatch inconsistencies in the reasoning process
resulting from errors in the visual tools. For
example, a follow-up question about the color
of a car might not realize that multiple cars
are present in the image and may thus end up
generating code without loops over the object.
2.Intermediate variable: The intermediate vari-
able created in the decomposition step help
the framework to reference specific object in-
stances, e.g., object on the right . We found
this to improve verification of complex ques-
tions requiring contextual reasoning around
multiple objects.
3. Reasoning in Language + Computations: We
created our in-context examples to perform
reasoning in both language, which provides
flexibility, and via computations in the Python
code (e.g., through creation of variables and
composition with tools).
4.Visual table: Finally, this module integrates
seamlessly with the table prepared in the ini-
tial step (subsection 3.1) of our pipeline. This
table, which catalogs the visual entities de-
tected and their attributes, serves as a refer-
ence point for validating the existence and
properties of objects within the image. By
cross-referencing the outputs of the vision
tools with the entries in this table, the mod-
ule can confirm or refute the predicates with a
high degree of confidence.
3.4 Integrated Verification Synthesis
In this step, we use the answers obtained from
the PoT-based verification step to perform a com-
prehensive reasoning assessment and validate the
original claim. This step is crucial for detecting
hallucinations and for drawing accurate and reli-
able conclusions. By aggregating the responses to
each sub-claim, the system can make an informed
decision about the overall validity of the claim.
We denote this step d, r=V(C, P,{qi, ai}K
i=1),
where the function Vtake in the original claim,
predicate, and questions and answers from the sub-
claims and outputs d, rwhich denote the decision
and a re-write for claims that contain hallucinations.
We realize this with an LLM with a detailed instruc-
tion and a few in-context examples that encourages
the LLM to verify the claim by considering the evi-
dence along with the consistency and coherence of
the generated answers using CoT-style reasoning.Any discrepancies or inconsistencies are flagged as
potential hallucinations, prompting a closer inspec-
tion of the questionable elements. This rigorous
cross-examination helps in identifying errors that
may have arisen due to incorrect or ambiguous
interpretations of the visual data. To mitigate hal-
lucinations, we use the same LLM to rewrite the
original claim based on the verified answers. This
involves rephrasing the claim to eliminate elements
that were identified as hallucinated or incorrect.
Through this integrated verification synthesis,
our pipeline not only verifies the accuracy of claims
but also actively improves the clarity and correct-
ness of the information presented. This approach
significantly reduces the risk of hallucinations, en-
suring trustworthiness and precision.
4 Experiment
4.1 Experimental Setup
Tasks and Benchmarks. We use the following
LVLM benchmarks for evaluation.
1.MMHal-Bench (Sun et al., 2023) evalu-
ates informativeness and hallucinations in re-
sponses by comparing model outputs with hu-
man responses and object labels using GPT-4.
The benchmark includes 96 image-question
pairs across 8 categories and 12 topics, focus-
ing on open-ended questions. We report the
informativeness score and hallucination rate.
2.GA VIE (Liu et al., 2023a) evaluates hallucina-
tion by measuring the accuracy and relevancy
(instruction-following ability) of responses
in an open-ended manner without needing
human-annotated ground truth answers We
selected a random subset of 250 questions and
use GPT-4o for evaluation.
3.MME (Fu et al., 2023) evaluates LVLMs’ per-
ception and cognition through “Yes" or “No"
questions for each test image. It has 14 sub-
tasks: 10 for perception and 4 for cognition.
Following (Yin et al., 2023), we use the ex-
istence, count, position, and color subsets to
assess hallucination at object and attribute lev-
els. We report the sum of accuracy and accu-
racy plus (when the model correctly answers
both “Yes" and “No" questions per image).
We discuss evaluation on hallucination bench-
marks (MMHal-Bench, GA VIE) in subsection 4.2,and on the visual understanding benchmark (MME)
in subsection 4.3.
Baselines. We use InstructBlip (Dai et al., 2024),
LLaV A-v1.5-7B (Liu et al., 2023b), LLaV A-v1.6-
7B (Liu et al., 2024b), mPlug-OWL (Ye et al.,
2023a), and mPlug-OWL2 (Ye et al., 2023b) as the
baseline models whose responses will be evaluated
with Pelican . We also compare with approaches
proposed to address hallucinations– Woodpecker
(Yin et al., 2023) and V olcano (Lee et al., 2023).
Refer to Appendix E for implementation details.
4.2 Hallucination Detection and Mitigation
Table 1 and Table 2 show the performance evalua-
tion of Pelican on the hallucination benchmarks–
MMHal-Bench and GA VIE. The results highlight
the superiority of Pelican in both reducing hallu-
cinations and improving performance on the given
question-answering task. Table 1 demonstrates
improvements by using Pelican to refine the re-
sponses of several LVLMs. For each row, ✓and
×denote performance with and without applying
Pelican respectively. Across all three hallucina-
tion benchmarks, Pelican improves the relevancy
and accuracy scores and reduces the hallucination
rate. This gain is particular larger for earlier mod-
els such as InstructBlip, mPlug-OWL, LLaV A-v1.5
and mPlug-OWL2. We observe the hallucination
rate to drop by ∼15−32% since Pelican is able
to address issues such as weak visual grounding
(by using external tools) and the bias towards “Yes"
answers. LLaV A-v1.6 is a recent LVLM that lever-
ages dynamic high-resolution and high-quality data
to achieve SOTA performance. We achieve a re-
duction in hallucination score on MMHal-Bench of
0.3with Pelican . Since we use LLaV A-v1.6 as
the VQA tool in Pelican , this result shows that
our approach does not completely rely on this tool
and composes different tools to mitigate hallucina-
tions (refer to Table 2 for results with a different
VQA tool). Our algorithm obtains a lower rele-
vancy score on GA VIE for both LLaV A-v1.6 and
mPlug-OWL2 since relevancy focuses on instruc-
tion following performance. This occurs since the
refinement generated by Pelican is conditioned
only on the original claim and may not directly
respond to the original question due to lost context.
Table 2 compares Pelican against two methods
designed to reduce hallucinations: Woodpecker,
which uses a claim verification pipeline, and V ol-
cano, which employs a self-feedback guided refine-Model
 PelicanMMHal-Bench GA VIE MME
Score↑Hal-Rate ↓Acc↑Rel↑Avg↑Existence Count Position Color Total
InstructBlip-7B× 1.71 0 .66 5 .61 7 .1 6 .36 185 58 58 143 444
✓ 2.26 0.51 6.66 7.6 7.13 175 153 147 152 627
mPlug-OWL× 1.34 0 .74 3 .88 7 .1 5 .49 95 48 50 55 248
✓ 2.35 0.50 6.55 7.5 7.03 175 150 133 153 611
LLaV A-v1.5-7B× 2.02 0 .61 5 .64 7 .4 6 .52 175 88 103 105 471
✓ 2.27 0.52 6.51 7.6 7.06 175 153 122 140 590
mPlug-OWL2× 1.88 0 .65 5 .82 7.9 6.86 150 83 58 118 409
✓ 2.26 0.51 6.42 7.6 7.01 175 142 121 140 578
LLaV A-v1.6-7B× 3.24 0.41 6 .13 7.8 6.97 195 155 138 190 678
✓ 3.04 0.38 6.33 7.5 6 .92 185 172 178 180 715
Table 1: Results on hallucination (MMHal-Bench, GA VIE) and visual understanding (MME) benchmarks. MMHal-
Benchscores range from 0-6, with hallucination rate (Hal-Rate) indicating the proportion of scores below 3. GA VIE
measures accuracy (Acc) and relevancy (Rel) on a 0-10 scale, with Avg representing their average. MME reports the
sum of accuracy and accuracy plus for each category (object-level and attribute-level), with Total representing the
sum across all categories. ✓denotes results corrected by
 Pelican . The best result for each setting is highlighted
in bold.
ModelMMHal-Bench GA VIE MME
Score↑Hal-Rate ↓Acc↑Rel↑Avg↑Existence Count Position Color Total
Woodpecker (InstructBlip) 1.71 0 .67 5 .48 7 .4 6 .44 160 78 90 100 428
Woodpecker (LLaV A-v1.6-7B) 1.73 0 .66 5 .38 7 .4 6 .39 165 78 90 100 433
V olcano-7B 2.21 0 .57 5 .32 7 .5 6 .41 195 152 107 160 614
V olcano-13B 2.44 0.52 5.97 8.1 7.04 195 158 118 185 656
Pelican 3.04 0.38 6.33 7.5 6.92 185 172 178 180 715
Table 2: Results compared against Woodpecker (Yin et al., 2023) and V olcano (Lee et al., 2023), two methods
previously proposed for correcting hallucination. The best scores are highlighted in bold, and the second-best scores
are underlined.
ModelMMHal-Bench MME
Score↑Hal-Rate ↓Existence Count Position Color Total
Pelican 2.27 0.52 175 153 122 140 590
Pelican w/o sh_var ( v) 2.23 0 .55 170 148 98 160 576
Pelican w/o sh_var( v), sh_comp 2.24 0 .54 170 148 97 70 485
Pelican w VQA(LLaV A-v1.5) 2.20 0 .55 170 143 123 145 581
Table 3: Ablation study to highlight the performance contribution of different innovations proposed in
 Pelican .
“sh_var" refers to shared variable and “sh_comp" refers to shared computations.
ment model. For a fair comparison with Wood-
pecker, we ran the author’s implementation with
both InstructBlip and LLaV A-v1.6 as the VQA tool.
Our model shows a 27% drop in hallucination from
the best-performing baseline (V olcano-13B), high-
lighting the robustness and reliability of Pelican
in handling multimodal hallucinations. Moreover,
the38% reduction in hallucination rate compared
to Woodpecker emphasizes our algorithmic inno-
vations in visual claim verification (Section 1).
4.3 Visual Understanding
We evaluate Pelican on visual understanding us-
ing the MME benchmark, as shown in the last
column of Table 1 and Table 2. We focus on
four key categories that significantly contribute tohallucination: object-level (existence and count)
and attribute-level (position and color). Our re-
sults demonstrate that Pelican significantly im-
proves visual understanding when integrated with
these models. For instance, mPlug-OWL initially
underperforms, but when integrated with our ap-
proach, we observe a remarkable 146% improve-
ment. Moreover, compared to LLaV A-v1.6, which
exhibits superior performance, we achieve an over-
all5.4%improvement, particularly over ∼40%
improvement in the count and position categories,
which are considered the most challenging. To
accurately answer questions about count and posi-
tion, the model must localize, and reference regions
based on size, considering that objects can be in
the foreground or background and may blend withFigure 3: An illustration of hallucination in LVLMs. Three examples showcasing different types of question-
answering styles, where both LLaV A-v1.5 and LLaV A-v1.6 hallucinates to the question.
 Pelican refines the
answer from these models exhibiting significantly reduced hallucinations.
similar colors from the surroundings. Furthermore,
the model needs to contextualize the relative lo-
cations of objects within the image. Overcoming
these challenges requires the model to perform de-
tailed reasoning about the key elements and their
attributes and relationships. Similar patterns are
observed in Table 2, where we outperform all base-
line models, especially in count and position. How-
ever, in some cases, V olcano-13B achieves higher
scores ( 10% and5%higher accuracy in existence
and color) compared to Pelican . This can be at-
tributed to V olcano-13B being trained specifically
to rewrite its response based on self-critique and
using a larger LLM backbone.
4.4 Ablation Studies
Table 3 shows our ablation study conducted on
Pelican to to assess the impact of different com-
ponents on performance. The study includes four
configurations: original ( Pelican ), without shared
variables between sub-claims ( Pelican sh_var( v)),
without shared variables and shared computations
(Pelican w/o sh_var( v), sh_comp), and lastly we
replace LLaV A-v1.6 with LLaV A-v1.5 for VQA
tasks (“ Pelican w VQA"). We evaluated these
configurations on MMHal-Bench and MME bench-
marks. The results demonstrate that removing
shared variables and shared computations between
sub-claims leads to a noticeable fall in performance,
particularly on the MME benchmark, where the
total score drops from 590to485. In particular,
the drop on position is higher ( 122→98) as the
shared variables are responsible for referencingspecific object instances which is important for
such question (e.g., Is the motorcycle on the right
side of the bus? ). Additionally, replacing LLaV A-
v1.6 with LLaV A-v1.5 for VQA tasks results in
reduction in the score and a small increase in hal-
lucination rate on MMHal-Bench and a 9point
decrease in Total on MME. This shows both that
Pelican is robust to the underlying VQA tools,
but overall performance will improve with better
tools. Figure 3 presents a qualitative comparison
with LLaV A-v1.5 and LLaV A-v1.6, illustrating that
these models struggle to accurately detect object
existence, color, location, and count, leading to
overall failure. In contrast, our proposed approach
successfully addresses these challenges.
Our performance improvement primarily stems
from two key innovations: shared computations
and shared variables. Additionally, the claim de-
composition process does not involve visual infor-
mation, making it agnostic to whether LVLMs or
LLMs are used, as both models operate solely on
textual content. Initial experiments with a REACT
agent, using a single LLM for sequential tasks,
failed due to brittleness in long-range reasoning
and an inability to adapt to the varied nature of
code generation. Overall, these findings highlight
the effectiveness of our proposed approach and the
critical role of each component in reducing halluci-
nations and enhancing visual understanding.
5 Conclusion
We propose
 Pelican , a novel solution for
detecting and mitigating hallucinations via vi-sual claim decomposition and program of thought.
Pelican is a structured pipeline that breaks down
claims into granular sub-claims and uses a robust
verification mechanism to ensure thorough check-
ing and validation of each aspect of the visual in-
formation. Our approach improves upon the state-
of-the-art benchmarks results on hallucination and
visual understanding. We also show improved per-
formance on visual tasks where models often strug-
gle and tend to hallucinate, highlighting Pelican ’s
strength in providing comprehensive visual under-
standing. Our unique principled approach involves
decomposing claims into sub-claims, using shared
variables, and leveraging in-context examples to
guide Python code execution for precise and deter-
ministic answers. Moreover, we employ an LLM
to reason over these answers, mimicking human-
like comprehension and argumentation. The pro-
cess systematically addresses and corrects halluci-
nations, ensuring higher accuracy and reliability in
both visual and textual understanding, making it a
valuable addition to any LVLM.6 Limitations
We discuss the limitations of Pelican in this sec-
tion.
1.Brittleness of Visual Tools: We used visual
tools such as VQA and object detection mod-
els to verify individual sub-claims, but these
tools often fail. For instance, YOLO and
DETR object detectors frequently struggle
with (1) very small objects, (2) objects out
of their normal context, and (3) visual entities
outside the detector’s vocabulary. Although
Grounding-DINO can detect novel objects, it
tends to produce several false positives. Sim-
ilar failures were observed with the LLaV A-
v1.6 VQA tool, which often failed to correctly
answer questions regarding object attributes.
Despite our efforts to mitigate these limita-
tions through the visual table representation
and object class verification, some failures
persist.
2.Randomness and Lack of Consistency in
LLM outputs : Even with the temperature
set to 0, we observed randomness in the out-
puts of our pipeline. Consistently extract-
ing visual entities in the key concept extrac-
tion step was particularly challenging. For
example, the LLM often broke compound
nouns such as “sports ball" into “ball" or “bath
towel" to “towel" causing the object detec-
tor to fail. When the LLM reduces “bath
towel" to “towel," YOLO/DETR fail to de-
tect “towel" because it is not in their list of
classes. Even Grounding-DINO, which can
detect “bath towel" fails to detect “towel."
The sensitivity of outputs to the prompt was
manageable in claim decomposition and code
generation, but the code generation step some-
times produced code that would fail in the
Python interpreter. We addressed this issue by
generating the code three times until it did not
produce any errors.
3.Issues with using a claim for verification:
Current pipeline transforms the (question, an-
swer) pair into a claim, which is used for
verification. The refined output produced by
our pipeline is directly used for evaluation,
instead of transforming it back into an an-
swer conditioned on the context. This often
results in lower relevancy scores, which mea-
sure instruction-following performance.4.Ability to handle conflicting evidence: In
specific cases, we observe our pipeline unable
to handle conflicting evidence, usually due to
tool failures. In such cases, the pipeline may
incorrectly declare the claim as true or false,
leading to inaccuracies.
7 Ethical Considerations
Detecting and mitigating hallucinations inher-
ently enhances ethical soundness. Our claim-
decomposition fosters transparency and empow-
ering enhanced control over ethical considerations.
It also fosters broader sets of checks and balances
that render ethical abuses more challenging, thus
reducing the potential for misuse.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in neural
information processing systems , 35:23716–23736.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015. Vqa: Visual question answering.
InProceedings of the IEEE international conference
on computer vision , pages 2425–2433.
Pepa Atanasova. 2024. Generating fact checking ex-
planations. In Accountable and Explainable Meth-
ods for Complex Reasoning over Text , pages 83–103.
Springer.
Pepa Atanasova, Jakob Grue Simonsen, Christina Li-
oma, and Isabelle Augenstein. 2022. Fact checking
with insufficient evidence. Transactions of the Asso-
ciation for Computational Linguistics , 10:746–763.
Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He,
Zongbo Han, Zheng Zhang, and Mike Zheng Shou.
2024. Hallucination of multimodal large language
models: A survey. arXiv preprint arXiv:2404.18930 .
Lang Cao. 2023. Enhancing reasoning capabilities of
large language models: A graph-based verification
approach. arXiv preprint arXiv:2308.09267 .
Nicolas Carion, Francisco Massa, Gabriel Synnaeve,
Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. 2020. End-to-end object detection with
transformers. In European conference on computer
vision , pages 213–229. Springer.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .Yangyi Chen, Karan Sikka, Michael Cogswell, Heng
Ji, and Ajay Divakaran. 2023. Dress: Instructing
large vision-language models to align and interact
with humans via natural language feedback. arXiv
preprint arXiv:2311.10081 .
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi.
2024. Instructblip: Towards general-purpose vision-
language models with instruction tuning. Advances
in Neural Information Processing Systems , 36.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,
Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
2023. Mme: A comprehensive evaluation bench-
mark for multimodal large language models. arXiv
preprint arXiv:2306.13394 .
Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos. 2022. A survey on automated fact-checking.
Transactions of the Association for Computational
Linguistics , 10:178–206.
Tanmay Gupta and Aniruddha Kembhavi. 2023. Vi-
sual programming: Compositional visual reasoning
without training. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition , pages 14953–14962.
Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang
Sun. 2023. Ciem: Contrastive instruction evaluation
method for better instruction tuning. arXiv preprint
arXiv:2309.02301 .
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui, Owais Khan Mohammed, Barun Patra, et al.
2024. Language is not all you need: Aligning per-
ception with language models. Advances in Neural
Information Processing Systems , 36.
Liqiang Jing and Xinya Du. 2024. Fgaif: Aligning
large vision-language models with fine-grained ai
feedback. arXiv preprint arXiv:2404.05046 .
Neema Kotonya and Francesca Toni. 2020. Explainable
automated fact-checking: A survey. arXiv preprint
arXiv:2011.03870 .
Nayeon Lee, Belinda Z Li, Sinong Wang, Wen-tau
Yih, Hao Ma, and Madian Khabsa. 2020. Lan-
guage models as fact checkers? arXiv preprint
arXiv:2006.04102 .
Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-
joon Seo. 2023. V olcano: mitigating multimodal
hallucination through self-feedback guided revision.
arXiv preprint arXiv:2311.07362 .
Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao,
and Zhu Zhang. 2023a. Self-checker: Plug-and-play
modules for fact-checking with large language mod-
els.arXiv preprint arXiv:2305.14623 .Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji-Rong Wen. 2023b. Eval-
uating object hallucination in large vision-language
models. arXiv preprint arXiv:2305.10355 .
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. 2023a. Mitigating hal-
lucination in large multi-modal models via robust
instruction tuning. In The Twelfth International Con-
ference on Learning Representations .
Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,
Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,
and Wei Peng. 2024a. A survey on hallucination
in large vision-language models. arXiv preprint
arXiv:2402.00253 .
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023b. Improved baselines with visual instruc-
tion tuning. arXiv preprint arXiv:2310.03744 .
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
Zhang, Sheng Shen, and Yong Jae Lee. 2024b. Llava-
next: Improved reasoning, ocr, and world knowledge.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2024c. Visual instruction tuning. Advances in
neural information processing systems , 36.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,
Shaohan Huang, Shuming Ma, and Furu Wei.
2023. Kosmos-2: Grounding multimodal large
language models to the world. arXiv preprint
arXiv:2306.14824 .
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
language models can teach themselves to use tools.
2023. arXiv preprint arXiv:2302.04761 .
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng
Li, Weiming Lu, and Yueting Zhuang. 2023. Hug-
ginggpt: Solving ai tasks with chatgpt and its
friends in hugging face. arxiv. arXiv preprint
arXiv:2303.17580 .
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,
Chunyuan Li, Yikang Shen, Chuang Gan, Liang-
Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023.
Aligning large multimodal models with factually aug-
mented rlhf. arXiv preprint arXiv:2309.14525 .
Dídac Surís, Sachit Menon, and Carl V ondrick. 2023.
Vipergpt: Visual inference via python execution for
reasoning. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 11888–
11898.
Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark
Liao. 2024. Yolov9: Learning what you want to learn
using programmable gradient information. arXiv
preprint arXiv:2402.13616 .Haoran Wang and Kai Shu. 2023. Explainable
claim verification via knowledge-grounded reason-
ing with large language models. arXiv preprint
arXiv:2310.05253 .
Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng
Wan, and S Yu Philip. 2023. Multimodal large lan-
guage models: A survey. In 2023 IEEE Interna-
tional Conference on Big Data (BigData) , pages
2247–2256. IEEE.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178 .
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei
Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
2023b. mplug-owl2: Revolutionizing multi-modal
large language model with modality collaboration.
arXiv preprint arXiv:2311.04257 .
Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao
Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,
and Enhong Chen. 2023. Woodpecker: Hallucina-
tion correction for multimodal large language models.
arXiv preprint arXiv:2310.16045 .
Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng
Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao
Zheng, Maosong Sun, et al. 2023. Rlhf-v: Towards
trustworthy mllms via behavior alignment from fine-
grained correctional human feedback. arXiv preprint
arXiv:2312.00849 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .A Predicates and Tools
We list out the predicates and tools invoked for each
predicates in Table 4. We defined these predicates
by using a taxonomy of question-types from prior
works on VQA (Antol et al., 2015) and multimodal
hallucinations (Bai et al., 2024).
B Visual Table
In subsection 3.1, we propose a pipeline to detect
objects referenced in a claim by integrating both
YOLOv9 and Grounding DINO. Here, we provide
a detailed overview of the pipeline that takes the
best of both worlds, YOLOv9 which is a closed-
vocabulary set and Grounding DINO which offers
an open-vocabulary set, allowing us to balance the
strengths and weaknesses of both tools. Closed-
vocabulary models like YOLOv9 are reliable for
the classes they are trained on but fail to detect
novel classes. Conversely, open-vocabulary models
like Grounding DINO can detect novel objects but
may produce false positives, such as mistakenly
identifying a pillow that isn’t present.
To address these issues, our pipeline first checks
if the object class is present in YOLOv9’s closed
set. If the class exists and YOLOv9 does not detect
the object, we conclude the object is not found. If
the class is not present, we use Grounding DINO
to verify the object’s presence. Additionally, for
the GA VIE benchmark, we incorporate a bounding
box verification tool. This tool uses the coordinates
of detected objects and queries a VQA module
(LLaV A-v1.6) to confirm if the objects are indeed
present in the image, enhancing the accuracy of our
detection system.
C Instruction & Prompts Templates
In this section, we provide the list of prompt tem-
plates used in various steps of Pelican .
We put the instructions and prompts we use for
extracting visual elements, claim decomposition,
program of thought verification and integrating ver-
ification.
D Additional Related Works
Apart from the works presented in section 2 we
discuss a few other relevant works.
Large Visual Language Models and Hallucina-
tions Several approaches have been proposed
to mitigate hallucinations in Large Visual Lan-
guage Models (LVLMs). One line of work em-ploys variants of Reinforcement Learning from Hu-
man Feedback (RLHF) to enhance LVLMs (Yu
et al., 2023; Chen et al., 2023; Jing and Du, 2024).
DRESS (Chen et al., 2023), for instance, utilizes
fine-grained feedback in the form of critique and re-
finement obtained from Language Models (LLMs)
to align the LVLM with human preferences. On the
other hand, RLHF-V acquires segment-level feed-
back directly from humans and applies dense direct
preference optimization to achieve alignment. A
recent work, V olcano (Lee et al., 2023), proposes
distilling the capability of self-critique within the
LVLM to improve performance during inference.
This is accomplished by generating instruction-
following data containing critique and improved
responses from LLMs, which is then used to fine-
tune the LLaVa-1.5 model.
E Implementation Details:
Pelican Our pipeline uses specific prompts,
listed in Appendix C, for different components of
our pipeline. These prompts were fed to GPT-4o for
producing responses. For accurate understanding
of visual content, we use several tools as described
in Appendix A. Specifically, we realize these tools
using:
•Object Detection: We used Yolo-v9 (Wang
et al., 2024) or DETR (Carion et al., 2020)
based on benchmark to localize image regions
specific to entities mentioned in the claim.
Grounding Dino was used to detect objects
not present in the vocabularly of these detec-
tors.
•VQA: We used LLaV A-v1.6 for visual ques-
tion answering the attribute related questions
related to the image.
•Relative_location : We implemented this
function in python to determine the relative
location (left, right, top, bottom) of object1
with respect to object2 by comparing their
bounding boxes.
We also use Pandas library for efficient data ma-
nipulation and analysis of the visual table. We
would like to note that we guided the LLM gen-
erating Python code to use these tools with a few
in-context examples. In this case, we often found
the LLM to compose different combinations than
what we had provided in these examples.System Message
HumanGiven a question answer pair about an image convert it into a claim. Avoid removing any information from the question or the answer.
Examples:
{in-context examples}
Question: {claim}
Answer: {predicates}
Claim: Claim GenerationFigure 4: Prompt template to generate a claim using both the question and answer.
System Message
HumanGiven a question answer pair about an image convert it into a claim. Avoid removing any information from the question or the answer.
You are an expert in analyzing a given caption about an image and extracting key elements such as objects, nouns, words/text present in the image. 
Run the extraction process in two steps.
Use the following steps for extraction:
    Step 1:
    Extract and list all physical objects mentioned in the given caption and phrases. Only include objects that have a tangible presence and can be 
perceived visually. Exclude abstract concepts, ideas, or entities that do not have a concrete physical existence. This includes:
        - Objects (or stuff classes) that occupy space like car, ball, couch, tree, sky, water.
        - Compound nouns or phrases that refer to a single entity, such as "Samsung Galaxy phone", "tennis ball", "toilet seat", "carriage seat", "stop sign". 
Keep these together and do not split them into separate parts.
        - Text that can be seen or read written on boards, signs, or in any region of the image using Optical Character Recognition (OCR).
Exclude abstract concepts (quality, action, feeling, state) and descriptors/adjectives (appearance, personality, emotion, trait) preceding the nouns. 
Ensure to capture compound nouns as a single entity without splitting them into separate parts. 
Avoid including elements that refer to the image itself (e.g., image, photo, picture, scene) they do not provide meaningful information about the image 
content.
    Step 2:
    Remove the adjectives (appearance, personality, emotion, trait) from the elements identified in Step 1 without removing the object name completely. 
Convert the elements into their singular form, except for uncountable nouns (e.g., water, sand) or compound nouns that refer to a single entity (e.g., 
"Red Bull").
    For example:
        - Change 'blue sky' to 'sky'
        - Change 'black phone' to 'phone'
        - Change 'cars' to 'car'
        - Change 'people' to 'person'
        - Keep 'sunglasses' as 'sunglasses'
        - Keep 'Samsung Galaxy phone' as 'Samsung Galaxy phone'
        - Keep 'tennis ball' as 'tennis ball'
Examples:
{in-context examples}
Caption: {caption}Claim To Visual Element
Figure 5: Template for prompting LLM to perform key concept extraction.
System Message
HumanYou are given a text response from an expert AI model about whether an image contains a specific object. The AI model's response will be in natural 
language.
  - If the response mentions a different object than the one originally specified, then extract only the most relevant object(s) to replace the original 
object. 
  - If the response clearly states that the specified object is not present in the image, then respond with "NA".
  - If the AI model's response confirms the presence of the original object, then simply return the name of that object. 
Examples:
{in-context examples}
Caption: {caption}Verify Boxes
Figure 6: Prompt template to detect the existence of an object in a boxed area.System Message
HumanYou are an expert at first order logic. The task is to analyze claims about an image:
  1. Define all the predicates in the claim.
      - Focus on key objects, scene, relationships, and attributes mentioned in the claim.
      - Avoid including elements that refer to the image itself (e.g., image, photo, picture, scene) in the predicates, as they do not provide meaningful 
information about the content of the image.
      - Use the following predicates:
          - Exists: Verify the presence or absence of an object/ocr-token
          - Scene: Verify the presence or absence of a scene
          - Count: Verify the number of objects
- Attribute_name: Verify the attribute of an object e.g. Color, Type, Wearing
          - Position: Verify the position of an object relative to another object
      - When dealing with complex claims involving multiple objects or relationships, CREATE intermediate variables using 'Select' to focus on specific 
objects based on their attributes or relationships. You can then use $variable_name to refer to these intermediate variables in subsequent predicates.
  2. Parse the predicates into a relevant chain of questions.
Examples:
{in-context examples}
Claim: {claim}Claim DecompositionFigure 7: Template for prompting LLM to decompose the claim into a list of predicates.
System Message
HumanYou are an expert at writing python code for answering questions based on prior context, given pandas table, and given tools.The answers are being 
used to collect evidence to verify questions about a claim in the next step.
You are provided with a table named `table_objects` with the following columns:
    - `object`: name of the object
    - `box`: Bounding box in [center_x, center_y, width, height] format. Coordinates are normalized based on the size of the image.
This table was generated by computer vision algorithms such as object detectors to obtain an initial list of detections. Multiple rows might contain the 
same object (e.g. "truck" and "trucks" or "stop sign" and "sign). 
You can use the following tools:
    TOOLS:    
        1. iou(box1, box2): 
            Return the IoU (Intersection over Union) score between the two bounding boxes. `box1` and `box2` should lists of coordinates. Use an iou >= 
0.3 to determine two different objects have a considerable overlap.    
        2. grounded_vqa(image, question, bbox=None): 
            Use this tool to answer questions about the image. The `image` argument should be a file path, numpy array, or PIL image. The `bbox` 
argument is optional and should be a tuple or list of coordinates in the format (center_x, center_y, width, height). Use the `bbox` argument to crop 
the specific region of interest for questions focusing on attributes of objects. For questions requiring contextual reasoning (such as spatial reasoning), 
omit the `bbox` argument. The tool returns a tuple containing the answer and its confidence score (low, medium, high). 
            DO NOT use this tool for counting objects as it can return incorrect answers. 
            Examples:       
                - Non-contextual question: "What color is the car?" (use `bbox` to focus on the car)       
                - Contextual question: "Where is the car located?" (omit `bbox` to consider the entire image)        
        3. relative_location(box1, box2, obj1, obj2): 
            Determine the relative location of obj1 with respect to obj2 based on their bounding boxes. The `box1` and `box2` arguments should be tuples          
or lists of coordinates in the format (center_x, center_y, width, height). The `obj1` and `obj2` arguments are the names of the objects being 
compared. The function prints the relative location (in terms of left, right, top, bottom) of obj1 with respect to obj2 based on their bounding boxes.    
        4. get_combined_box(box1, box2): 
            Combine two bounding boxes to create a larger box that encompasses both objects. The `box1` and `box2` arguments should be tuples or lists 
of coordinates in the format (center_x, center_y, width, height). The function returns a new bounding box that includes both objects. Given this table, 
the `image_url`, user's question, and prior context, write Python code to answer the question as best as you can. If multiple objects of the same type 
are found, consider their IoUs and confidences to determine the most relevant one. If the `grounded_vqa` tool returns a low confidence score, 
indicate that the answer may not be reliable. It is also okay to invoke multiple tools to gain more evidence. Use the prior context to accomplish the task 
efficiently by reusing the information already provided.The Python code should be returned within markdown quotes ```python ```. Also, make sure
to explicitely print the answers in the code.
Examples:
{in-context examples}
Prior Context: {context}
Question: {question}Evidence
Figure 8: Prompt Template for function calling based on the task type for, e.g., to answer about visual attributes it
calls grounded_vqa.System Message
HumanYou are an AI assistant trained to verify and correct claims made by other AI algorithms about an image. The claims could be incorrect due to logical 
inconsistencies, visual hallucinations, or faulty reasoning. Your task is to:
    
    1. Assess the validity of the claim based on the evidence provided in the form of predicates and follow-up questions with answers.
    2. Rewrite the claim if it is incorrect by providing a corrected version that aligns with the evidence.
Input Format:
    - Claim: A statement about the image that needs to be verified.
    - Predicates: A list of conditions that need to be checked to verify the claim.
    - Follow-up Questions: A series of questions and answers that provide evidence for the predicates.
For each input, follow these steps:
    1. Analyze the relevance of each predicate and follow-up question to the claim.
    2. Consider the confidence scores provided with the answers to the follow-up questions. If the confidence scores are low or not provided, treat the 
evidence as less reliable.
    3. If the follow-up questions provide conflicting information, prioritize the evidence with higher confidence scores or mention the contradiction in 
your reasoning.
    4. Identify any logical inconsistencies or faulty reasoning in the claim, such as statements that contradict the evidence or make unsupported 
assumptions.
    5. Provide a step-by-step reasoning for your decision, citing the key evidence that led to your conclusion.
    6. Give a **verdict** as "Verdict:\nCorrect" if the claim is supported by the evidence or "Verdict:\nIncorrect" if the claim is not supported or 
contradicted by the evidence.
    7. If the claim is incorrect, rewrite the claim based on the evidence provided, ensuring that the rewritten claim is consistent with the available 
information. If the claim is correct then write "Rewrite: NA" to help with parsing.
Note: Focus only on verifying the specific claim provided, without considering any additional information that may be present in the image but not 
mentioned in the claim.
Examples:
{in-context examples}
Claim: {claim}
Predicates: {predicates}
Followup Questions: {evidence}Integrate VerificationFigure 9: Prompt template to summarize the sub-claim responses and verify whether the claim was correct. Finally,
rewrite that claim (if it has hallucinations) based on the responses.Predicates Tools Reasoning
Exist Object Detector Trained specifically to detect and locate objects
OCR VQA Trained on OCR-based datasets
Count Object Detector Detecting every instance of object, assists in counting.
Attribute VQA Trained on paired image, caption datasets such as COCO, Visual Genome
Location (rel) VQA, Relative_location Bbox from object detectors provide relative location
Scene VQA Trained on caption datasets
Table 4: Different predicates along with tools and the type of reasoning involved. Relative_location is a function
to determine the relative location of an object relative to another object based on their bounding boxes.
Baselines: For our experiments, we ran the infer-
ence for all the models locally with temperature set
to 0.
•InstructBlip (Dai et al., 2024): We used the
model “instructblip-vicuna-7b"1provided by
authors on Huggingface.
•LLaV A-v1.5-7B (Liu et al., 2023b) We used
the code and the model provided2by the au-
thors.
•LLaV A-v1.6-7B (Liu et al., 2024b): We used
the model “llava-v1.6-mistral-7b-hf"3pro-
vided by the authors on Huggingface.
•mPlug-OWL (Ye et al., 2023a): We use the
model and the code provided4by the authors.
•mPlug-OWl2 (Ye et al., 2023b):We use the
model and the code provided5by the authors.
•V olcano (Lee et al., 2023): We used the code6
provided by the authors.
•Woodpecker (Yin et al., 2023): We used the
code7provided by the authors. Woodpecker
originally uses BLIP2 as the VQA tool, which
can limit its performance since BLIP2 is not
instruction-tuned. We thus replaced BLIP2
with Instruct-blip and LLaV A-v1.7-7B for a
fair comparion with our work.
Evaluation with LLMs: We used gpt-4-0613 for
evaluating MMHal-Bench. GPT-4o was used for
evaluation with GA VIE due to lower API costs and
claims of similar quality.
1https://huggingface.co/Salesforce/instructblip-vicuna-7b
2https://github.com/haotian-liu/LLaV A
3https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf
4https://github.com/X-PLUG/mPLUG-Owl
5https://github.com/X-PLUG/mPLUG-Owl
6https://github.com/kaistAI/V olcano
7https://github.com/BradyFU/WoodpeckerModel Benchmark Time Cost
LLaV A-1.5 MMHal-Bench 1.185s N/A
Our Model MMHal-Bench 5m58s $0.234
LLaV A-1.5 MME-existence 0.184s N/A
Our Model MME-existence 2.275s $0.011
Table 5: Inference time and cost comparison of LLaV A-
1.5 and our model across MMHal-Bench and MME-
existence benchmarks.
Comparative Analysis of Inference Time and
API Costs: We provide the inference time and
costs (averaged over 10 examples) for the MME
and MMHal-Bench benchmarks using LLaV A-1.5
and the proposed model (Pelican). MME has lower
computational costs than MMHal-Bench due to its
simpler claims involving single objects or attributes.
For Pelican, all tools except the LLM were hosted
locally. Pelican’s slower performance is due to
its stagewise approach; we did not optimize the
VQA and detector models for batched requests,
which could yield a 2-3x speedup. Additionally,
parallelizing tool calls could improve efficiency.
The time is also impacted by the number of tool
calls, which is smaller for MME (simpler claims)
compared to MMHal-Bench. The per-sample API
cost varies by dataset, but remains relatively low.
Additional Evaluation on POPE: Additionally,
we add results on POPE (Li et al., 2023b) by select-
ing 300 examples, following Woodpecker, with 100
samples selected from each of the three settings.
We observe consistent improvements over base-
line LVLMs, except for a minor accuracy drop in
LLaV A-v1.6-7B (Liu et al., 2024b), which was off-
set by higher precision. We notice the accuracy re-
mains at 91% across all three models with Pelican .
This consistency is attributed to the Yes/No nature
of the questions in the POPE dataset, allowing us
to achieve 91% accuracy with LLaV A-v1.6 as the
VQA model, regardless of the baseline models’ er-
rors. The few failure cases are primarily due to toolModel Acc. Precision Recall
llava-v1.6-mistral-7b 91.67 90.85 92.67
+Ours 91.00 93.62 88.00
instructblip-vicuna-7b 86.00 85.06 87.33
+Ours 91.00 93.62 88.00
mplug-owl2-llama2-7b 80.66 75.00 92.00
+Ours 91.00 93.62 88.00
Table 6: Performance comparison of baseline models
and “+Ours" on POPE.
failures.
F Qualitative Results
Refer to Figure 10 and Figure 11 to understand the
pipeline depicted for Pelican . Figure 10 illustrates
a simple example from MME, where mPlug-OWL
incorrectly detects a potted plant that doesn’t ex-
ist. We create a predicate to verify the presence
of the “potted plant" followed by its correspond-
ing question and answer. The verification output
reasons, leading us to predict the claim as incor-
rect. We then refine the output by stating, “There
is no potted plant in this image." This example also
highlights the brittleness of the tools (refer to limita-
tions in section 6), as “potted plant" is not a class in
YOLO/DETR, so the code defaults to Grounding-
DINO (refer to implementation in Appendix E),
which detects the plant with low confidence.
Figure 11 demonstrates a more complex process,
where LLaV A-v1.5 misidentifies a face painting as
glasses. The detection tools find a “man" with high
confidence via YOLO, and “glasses" are detected
again via Grounding-DINO with low confidence.
Next, we prepare a list of predicates and follow-
up questions. Next, we show the Python code for
each follow-up question. Finally, the evidence gath-
ered is used for reasoning to predict the claim as
incorrect.
G Licensing Information
The images used in our paper are sourced from
MSCOCO, Visual Genome and OpenImages that
are under different Creative Commons licenses. Im-
ages Figure 1, left in Figure 3, Figure 11 are li-
censed under CC BY 2.0, which allows for sharing
and adaptation with appropriate attribution. cen-
ter in Figure 3, Figure 10 are under CC BY-NC
2.0, permitting non-commercial use with attribu-
tion. Image on the right in Figure 3 is taken from
Visual Genome dataset which is under CC BY 4.0
which allows for sharing and adaptation with appro-priate attribution. Users should refer to the specific
license links provided for each image to ensure
compliance with usage terms.Figure 10: From MME benchmark category existence, the claim is generated from the incorrect answer given by
mPlug-OWL. Below shows the entire detection and mitigation pipeline of Pelican .Figure 11: From MMHal-Bench, the claim is generated from the incorrect answer given by LLaV A-v1.5. Below
shows the entire detection and mitigation pipeline of Pelican .Figure 12: In MMHal-Bench, the claim is generated from the correct answer provided by LLaV A-v1.5. Below is
the complete detection and mitigation pipeline of Pelican . This is a negative example where Pelican makes an
error. Although the evidence collected was largely accurate, there was an inconsistency between identifying a table
and a stool, which likely arose from the brittle nature of the object detection tool. In the evidence analysis section,
the relative position was inaccurately assessed, leading to an incorrect final verdict.