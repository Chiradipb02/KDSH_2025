Beyond Label Attention: Transparency in Language Models for Automated
Medical Coding via Dictionary Learning
John Wu
University of Illinois Urbana-Champaign
johnwu3@illinois.edu
David Wu
Vanderbilt University
David.h.wu@vanderbilt.eduJimeng Sun
University of Illinois Urbana-Champaign
jimeng@illinois.edu
Abstract
Medical coding, the translation of unstructured
clinical text into standardized medical codes, is
a crucial but time-consuming healthcare prac-
tice. Though large language models (LLM)
could automate the coding process and improve
the efficiency of such tasks, interpretability re-
mains paramount for maintaining patient trust.
Current efforts in interpretability of medical
coding applications rely heavily on label at-
tention mechanisms, which often leads to the
highlighting of extraneous tokens irrelevant
to the ICD code. To facilitate accurate inter-
pretability in medical language models, this
paper leverages dictionary learning that can
efficiently extract sparsely activated representa-
tions from dense language model embeddings
in superposition. Compared with common la-
bel attention mechanisms, our model goes be-
yond token-level representations by building
an interpretable dictionary which enhances the
mechanistic-based explanations for each ICD
code prediction, even when the highlighted to-
kens are medically irrelevant. We show that
dictionary features can steer model behavior,
elucidate the hidden meanings of upwards of
90% of medically irrelevant tokens, and are
human interpretable.
1 Introduction
Transparency is a vital factor in healthcare to gain
patients’ trust, especially when AI models make
critical decisions in clinical practice (Rao et al.,
2022). One of the essential applications of AI
models is to assign International Classification of
Diseases (ICD) codes automatically based on the
clinical text (we name this task as medical cod-
ing). These ICD codes categorize patient diag-
noses, conditions, and treatments for billing, report-
ing, and treatment purposes (Hirsch et al., 2016;
Johnson et al., 2021). However, assigning ICD
codes is complex and requires expertise and time
(O’Malley et al., 2005). Recent advancements
in medical pre-trained language models (PLMs)have made it possible to treat medical coding as
a high-dimensional multilabel classification chal-
lenge (Edin et al., 2023; Huang et al., 2022). These
AI models led to significant success in efficient
ICD coding (Kaur et al., 2021). However, their
transparency remains of great concern (Hakkoum
et al., 2022). Therefore, developing automated in-
terpretability methods is crucial to upholding trans-
parency in medical coding processes.
Significant progress has occurred in the field
of black-box interpretability, particularly concern-
ing feature attribution, with the emergence of
perturbation-based methods such as SHAP (Lund-
berg and Lee, 2017) and its approximate counter-
part LIME (Ribeiro et al., 2016; Moraffah et al.,
2020). These techniques, rooted in information and
game theory, are recognized for assessing feature
relevance in detail by intelligently perturbing and
ablating input features (Lundberg and Lee, 2017;
Ribeiro et al., 2016). While approximation meth-
ods have greatly improved the speed of calculat-
ing Shapley values, exact computations remain ex-
pensive (Lundberg et al., 2020; Chen et al., 2022;
Shrikumar et al., 2019; Mosca et al., 2022). The
huge computational cost makes their application
impractical towards automated medical ICD cod-
ing since clinical notes usually contain thousands
of high dimensional token embeddings in a vast
multilabel prediction space (Johnson et al., 2023).
As such, we seek a human-interpretable ap-
proach that scales efficiently with large datasets,
highlights essential features, and offers more com-
prehensive explanations of PLM predictions. Re-
cent advancements in mechanistic interpretability
methods (Cunningham et al., 2023; Räuker et al.,
2023) have demonstrated the potential to surpass
the computational challenges posed by traditional
black-box approaches by elucidating the roles of
specific neuron subsets within a network. This
level of mechanistic understanding is precious in
the medical field, where explaining the significancearXiv:2411.00173v1  [cs.CL]  31 Oct 2024of a feature is as crucial as its identification.
In recent years, the attention mechanism (Abnar
and Zuidema, 2020; Vaswani et al., 2023; Chefer
et al., 2021, 2022) has been heavily used to inter-
pret and explain the behavior of large transformer
models. Within the realm of automated medical
ICD coding, label attention (LAAT) variants are
most prevalent due to their efficiency in handling
extensive sequence lengths. They calculate an at-
tention score for each token relative to each label,
thereby identifying tokens crucial for ICD code pre-
dictions (Mullenbach et al., 2018; Vu et al., 2020).
Nevertheless, studies such as (Pandey et al., 2023)
have questioned the interpretability and validity of
explanations provided by attention mechanisms.
For example, the LAAT mechanism may high-
light incoherent or irrelevant tokens, such as stop
words for highly medically specific ICD codes. For
instance, LAAT attributes the stop token "and" to
the medically specific ICD code "998.59 postoper-
ative wound infection" despite conjunctions being
irrelevant to medical prognosis, thus undermining
the interpretability of LAAT as shown in Figure 1.
We attribute this issue to be the result of neuron
polysemanticity—where a single neuron responds
to diverse, unrelated inputs—complicates direct
interpretation (Olah et al., 2020).
Elhage et al. (2022) theorizes polysemanticity
to be a form of superposition, occurring when the
count of independent data features surpasses layer
dimensions, leading to data features being repre-
sented by linear combinations of neurons. As a
result, individual neurons are often directly uninter-
pretable (Subramanian et al., 2017; Cunningham
et al., 2023). Addressing this, sparse autoencoders
(Olshausen and Field, 1997) have been applied to
distill these complex dense representations into in-
terpretable, sparse linear combinations, performing
what is known as dictionary learning (DL). This
strategy has effectively decomposed different lan-
guage model layers, such as neural word embed-
dings (Subramanian et al., 2017), MLPs (Cunning-
ham et al., 2023; Bricken et al., 2023), and residual
connections (Yun et al., 2023a), proving scalable
and monosemantic.
Our paper generalizes these sparse coding con-
cepts to better understand the attention mechanism
and pre-trained language model (PLM) embed-
dings by constructing effective dictionaries with
LAAT to improve the interpretability of the med-
ical ICD coding task. We summarize our maincontributions:
•Expanding on (Bricken et al., 2023)’s abla-
tion studies, we show that combining learned
dictionary features with another mechanistic
component (LAAT) (Vu et al., 2020), in our
new interpretability framework AutoCodeDL,
improves explainability of downstream ICD
predictions.
•We build medically relevant dictionaries with
sparse autoencoders that can capture medi-
cally relevant concepts hidden within super-
position.
•We develop new automated proxy metrics for
assessing the human understandability of con-
structed dictionaries and conduct extensive
evaluations on large scale clinical text-based
corpus.
2 Related Work
2.1 Automated Interpretability in ICD Coding
Alternative automated ICD coding methods, like
phrase matching (Cao et al., 2020) and relevant
phrase extraction using manually curated knowl-
edge bases (Duque et al., 2021), offer inherent inter-
pretability but fall short in expressive power com-
pared to neural network-based approaches. This
discrepancy highlights a persistent tradeoff be-
tween interpretability and performance in ICD cod-
ing tasks.
Furthermore, the prevailing interpretability
method for deep neural models in ICD coding tasks
rely on the attention mechanism (Yan et al., 2022).
Specifically, the LAAT mechanism projects token
embeddings into a label-specific attention space,
where each token receives a score indicating its rele-
vance to each ICD prediction. Such attention-based
associations between tokens and classes have been
employed in various architectures, including con-
volutional models like CAML (Mullenbach et al.,
2018), recurrent neural networks (Vu et al., 2020),
and large language models (Huang et al., 2022;
Yang et al., 2023). While computationally efficient,
it overlooks the potentially richer information hid-
den within the embedding space, hindering our
understanding of automated ICD predictions. Our
work builds on top of such past works, and directly
interprets the medical PLM embeddings.
2.2 Dictionary Learning
Dictionary learning aims to find a sparse represen-
tation of input data in the form of linear combina-``and wound breakdown dehiscence first name”(a) Specific Tokens related to an ICD Code(b) Highlighted Tokens with Label Attention``and wound breakdown dehiscence first name”``and wound breakdown dehiscence first name”Feature ID: 4787Feature ID: 2388Feature ID: 3728Prepositions and conjunctionsRemoval of damaged tissueFailure of wound healing(c) Highlighted Tokens with SuperpositionHuman Interpretable Decomposed Dictionary FeaturesToken Embeddings in SuperpositionFigure 1: Motivation: LAAT identifies the most relevant tokens for each ICD code (b). Compared to our inspection
of which tokens are most relevant to an ICD code (a), we assume "and" is irrelevant to an ICD code prediction.
Although it may appear as though "and" is irrelevantly highlighted, taking token embeddings out of superposition
allows us to decompose dense token embeddings into more semantically meaningful dictionary features that show
that concepts of "failure of wound healing" are embedded within its token embedding (c), thereby giving justification
for its highlighting by LAAT for a wound-related ICD code.
tion of basic elements (Olshausen and Field, 1997).
Such an approach has been applied across various
domains, including word embedding decomposi-
tion (Subramanian et al., 2017), interpretation of
language model activations (Bricken et al., 2023;
Cunningham et al., 2023; Yun et al., 2023a), en-
hancement of representation learning (Ghosh et al.,
2023; Tang et al., 2023), and analysis of time-series
data (Xu et al., 2023), highlighting their versatility
(Zhang et al., 2015a).
Our research aims to understand if and how
dictionary learning improves upon existing inter-
pretability methods like LAAT for predicting medi-
cal codes (ICDs) in a highly practical setting. By
analyzing diverse medically relevant ICDs, we as-
sess the learned dictionaries’ ability to capture spe-
cific and meaningful medical concepts. Unlike pre-
vious work requiring extensive human annotation
(Bricken et al., 2023; Cunningham et al., 2023; Sub-
ramanian et al., 2017), we propose new automated
metrics to measure how understandable these dic-
tionaries are due to the cost of expert annotation.
3 Methodology
As depicted in Figure 2, our focus within dictionary
learning involves explicitly building dictionaries
where relevant tokens and ICD codes are mapped to
dictionary features. We examine two sparse autoen-
coder approaches aimed at creating interpretable
representations from dense language model em-
beddings: one via L1minimization (Cunningham
et al., 2023; Bricken et al., 2023) and another via
SPINE’s loss function (Subramanian et al., 2017).
While our discussion primarily centers on the L1
minimization technique for its widespread appli-
cation and illustrative clarity regarding dictionarylearning’s objectives in section 3.1, further details
on SPINE are provided in the Appendix A.6.
Then, using our trained sparse autoencoder, we
perform ablation studies to understand the down-
stream effects of dictionary features in section 4.1
and map the relevant ICD codes to each dictio-
nary feature as discussed in section 3.2. Finally,
we leverage sparse encoding and its ablation tech-
niques in constructing our final dictionary, mapping
both relevant tokens and ICD codes to each dictio-
nary feature in section 3.3, which is used in our
new proposed method AutoCodeDL in Figure 3.
3.1 Sparse Autoencoders
Following (Bricken et al., 2023)’s approach, let
x∈Rdbe the token embedding we wish to in-
terpret, dthe dimension size of the token embed-
ding, and mthe dimension size of the latent sparse
dictionary feature activations f∈Rmgenerated
by the sparse autoencoder. Our L1sparse autoen-
coder is shown below in equations 1 through 4
where We∈Rm×dis the encoder weight matrix,
be∈Rmis the encoder weight bias term, bd∈Rd
is the decoder bias term, and Wd∈Rd×mrepre-
sents the sparse dictionary embeddings:
¯x=x−bd (1)
f=ReLU (We¯x+be) (2)
ˆx=Wd·f+bd (3)
L=1
|X|X
x∈X∥x−ˆx∥2
2+λL1∥f∥1 (4)
TheL1norm∥f∥1in the loss function (see eq.
4) enforces the sparse representation of fin train-
ing. As a result, only certain elements within f
activate for certain types of token embeddings x,“and wound…dehiscence”𝒙∈ℝ𝒅𝑾𝒆∈ℝ𝒎×𝒅Sparse AutoencoderSparse Activation 𝒇∈ℝ𝒎
Step1. Sampling and Encoding=+Decomposition of 𝒙=	∑𝒊𝒎𝒇𝒊∗𝒉𝒊Step2. Ablation and Identification
ICD𝒑InitialAblation
ICD𝒑AblatedICD 78.41Dictionary Feature IDExplanationMost Relevant TokensMost Relevant ICD Codes3728About failure of wounddehiscence, wound, breakdown,78.41, 34.79, ……………419About cesarian sectionsSo, stat, c, section, …V30.01, 763, …Medically Relevant DictionaryActivated ID 2388 3728Figure 2: Building a dictionary involves several steps: A sparse autoencoder decomposes each token embedding into
a sparse latent space, where each nonzero element represents a dictionary feature ID (step 1). This process enables
the creation of mappings between tokens and various dictionary features and ICD codes. In step 2, ICD codes are
mapped to dictionary features based on the softmax probabilities of each ICD prediction after dictionary embedding
ablations, as detailed in section 3.2. Once a dictionary is constructed, it is utilized to enhance explanations by
applying it to highlighted tokens identified by LAAT in Figure 3.
creating a direct mapping between different tokens
(words) represented by xand their respective fea-
tures identified by fi. For instance, we observe
that "depression"-related tokens only activate the
dictionary feature f5732.
While there exist many other useful properties
ofL1minimizations (Zhang et al., 2015b), the key
idea is that a sparse linear combination of learned
dictionary embeddings represents every token em-
bedding. First, let us examine the dictionary em-
bedding matrix Wdas defined below.
Wd=
h0, h 1, . . . , h mT(5)
Lethi∈Rdbe the dictionary embedding as-
sociated with a dictionary feature fi; we have the
following decomposition of any token embedding
xinto sparse features.
x≈mX
ifi∗hi (6)
Another key intuition behind why these decom-
positions are directly interpretable is that since cer-
tainfionly activate (i.e., is nonzero) for certain
types of tokens, its dictionary embeddings hihave
a defined direction in the PLM embedding space
that should directly correspond to some meaning-
ful concept within the original clinical text, thus
having downstream implications.
3.2 Mapping Dictionary Features to ICD
Codes
To build a medically relevant dictionary, ICD codes
should map to their respective meaningful dic-
tionary features. Following the methodology in
(Bricken et al., 2023), we ablate features in clinical
notes by targeting any activated dictionary featurefi>0in token embedding x∈Rd. For each
feature fiwith corresponding feature embedding
hi∈Rd, we define the ablated token embedding
as˜x.
˜x=x−fi·hi (7)
For any token in a clinical note, we perform
token embedding ablations, recalculate the ablated
model’s softmax probabilities for all Cclasses or
ICD codes, and compute the probability differences
δi.
δi=p(x)−p(˜x), δi∈RC(8)
Finally, for any given class c∈ {1,2, . . . , C },
we sort and record the top δi,c’s when ablating each
dictionary feature fiand its embedding hi, identify-
ing its most relevant ICD codes. Such ablations are
later used for evaluating the model explainability
of dictionary features and building more human-
interpretable medical dictionaries with the sparse
autoencoder.
3.3 Building Medically Relevant Dictionaries
to Augment ICD Explanations
In essence, the sparse autoencoder contains a dic-
tionary within its latent space. While efficient in
time and space complexity, its direct interpreta-
tion requires the construction of a more human-
interpretable and literal dictionary containing the
most relevant tokens and ICD codes for each dic-
tionary feature fi. Building a dictionary can be
summed up into two sorting steps.
Sampling and Encoding. We sample a certain
number of clinical notes from our test set. For ev-
ery token in each clinical note, we encode their
PLM embeddings with our sparse autoencoder and
retrieve its sparse feature activations fi. Then, for
each dictionary feature i, we sort by each token’s re-spective fiand select the top ktokens with the high-
est feature activations for each dictionary feature.
Since there are often compound words, consisting
of multiple tokens, we also retrieve any neighbor-
ing tokens with nonzero activations.
Ablation and Identification. For every clini-
cal note, we perform ablations for each activated
dictionary feature’s embedding hi, measuring the
change in predicted probability for each ICD code.
For each dictionary feature, we identify its most
relevant classes based on the largest probability
drops after ablation. We formalize this process for
a single clinical note in the pseudocode shown in
algorithm 1 in the Appendix.
Proposed Method of Interpretability. After
constructing the dictionary, we can utilize it to
query the dictionary features of any PLM token
embeddings, enhancing interpretability. Integrated
with LAAT in our proposed new method AutoCod-
eDL, we initially identify the key tokens for each
ICD prediction and subsequently match their em-
beddings to features in our dictionaries, thereby
refining explanations (see Figure 3).
4 Interpretability Evaluations
While there does not exist a unified singular def-
inition of neural model interpretability, there is a
general consensus that model interpretability meth-
ods should be both model explainable and human
understandable (Zhang et al., 2021). We define ex-
plainability as how much do the dictionary features
learned predict the pre-trained language model’s
ICD predictions. We define human understandabil-
ity within the lens of monosemanticity: a dictio-
nary feature is only human interpretable when the
tokens that highly activate said dictionary feature
are related and its underlying concept can be easily
identified.
Sparse Autoencoders. We explore two major
dictionary learning (DL) sparse autoencoder ap-
proaches, SPINE by (Subramanian et al., 2017)
andL1described in section 3.
Baselines. Following (Cunningham et al., 2023),
we explore four unsupervised baseline encoders
that we compare to a true dictionary encoding: In-
dependent Component Analysis (ICA), capable of
decomposing word embeddings into semantically
meaningful independent components (ICs) (Musil
and Mare ˇcek, 2022); Principal Component Anal-
ysis (PCA), which has been used to analyze the
structure of word embeddings (Musil, 2019); an
Identity ReLU encoder, which effectively treatseach element in the PLM embedding as its own
dictionary feature; and a random encoder.
Dataset and Model. We train on PLM embed-
dings from a 110M medical RoBERTa model lever-
aged by the state of the art PLM-ICD coding model
(Huang et al., 2022; Lewis et al., 2020) and evaluate
our method using the cleaned MIMIC-III dataset,
using 38,427 clinical notes for training and 8,750
for evaluation, as detailed by (Edin et al., 2023).
Additional details on training are in Appendix A.1.
4.1 Model Explainability
Faithfulness. Inspired by explainability evaluation
metrics within the vision domain (Chattopadhay
et al., 2018; Samek et al., 2015), we assess our in-
terpretable dictionary features by removing them
(ablation) and measuring their impact on predicted
ICD codes. This approach helps us quantify how
well our explanations align with the model’s predic-
tions. For example, ablating the "depression" fea-
ture should primarily impact related ICD codes like
"depressive disorders" while leaving unrelated ones
like "postoperative wound infection" relatively un-
affected.
To address this, we consider both the decrease
in the most likely code’s softmax probability after
ablation and the sum of absolute changes in soft-
max probabilities for all other codes. Computing a
ratio of these measures (detailed in Table 1) allows
us to accurately gauge an interpretability method’s
explanatory power. This ablation metric is analo-
gous to Comprehensiveness (Comp) by (Chan et al.,
2022; DeYoung et al., 2020), but instead of erasing
entire tokens, we ablate specific components of the
embedding (see eq. 7).
Setup. We compare our interpretability frame-
work, AutoCodeDL, against three sets of baselines.
The first set involves feature ablations using base-
line encoders and full token ablations (labeled "to-
ken" in Table 1) with LAAT to pre-highlight rele-
vant tokens. The second set excludes LAAT and
simply explains only with DL feature ablations
across all tokens. The third set excludes LAAT and
only utilizes the baseline encoder ablations. For
additional details on the ablations for each baseline
encoder, please refer to Appendix A.7.
Results. From Table 1, ablations of dictionary
features of highlighted tokens with our proposed
AutoCodeDL method show minimal impact on
other ICD code predictions while retaining large
drops in softmax probabilities. As a result, our“the and wound ... first”Dictionary Feature IDExplanationMost Relevant TokensMost Relevant ICD Codes3728About failure of wounddehiscence, wound, breakdown,78.41, 34.79, ……………419About cesarian sectionsSo, stat, c, section, …V30.01, 763, …Medically Relevant DictionarySparse Activation 𝒇∈ℝ𝒎Activated ID 2388 3728𝑾𝒆∈ℝ𝒎×𝒅Sparse AutoencoderICD CodeTokenand woundFigure 3: Proposed method for automated ICD interpretability pipeline: AutoCodeDL . LAAT identifies the most
important words "and wound". Then, the sparse autoencoder queries its most activated dictionary features, returning
its respective dictionary feature ids that can be leveraged to further explain the PLM’s predictions and attention
highlights.
proposed method outperforms all baselines in our
ratio explainability metric. Within our baselines,
their ablations varied tremendously. Most notably,
ablating ICs from embeddings using ICA had min-
imal impact on predictions, possibly because ICA
approximations identify features with negligible
mutual information while principal component ab-
lations were similar to full token ablations.
Addressing Superposition. We decompose to-
ken embeddings to understand why label attention
highlights specific tokens for ICD codes. But each
token can hold multiple meanings, making man-
ual evaluation of its dictionary features laborious.
So, we introduce a new metric to assess if our dic-
tionaries can identify hidden meanings even when
attention identifies "extraneous" words.
Setup. We analyze stop words highlighted by
label attention to evaluate our dictionary’s ability
to explain their relevance to ICD code predictions.
From 8,000 test set clinical notes, we extract all,
not just attention-highlighted tokens and build a
dictionary linking tokens and ICD codes to dictio-
nary features (described in Section 3.2). We then
focus on stop words deemed highly relevant by the
label attention mechanism. For each stop word,
we query its relevant dictionary features using our
trained sparse autoencoder, and see if the original
label ranks among the top 10 classes pre-mapped
by the sampled dictionary for each activated feature.
Since feature activation magnitudes vary, we define
"highly activated" features as those exceeding the
96.5th percentile feature magnitude per token em-
bedding. Additional details are listed in Appendix
A.9.
Results. Table 2 presents the proportion of stop
word embedding labels correctly identified by our
DL framework, as well as the performance of base-
line methods. Notably, our DL framework, partic-ularly the L1 sparse autoencoder variant, achieves
an impressive accuracy of 91%, outperforming all
baselines. These results underscore the robustness
of our DL approach in capturing the hidden medi-
cally relevant meanings embedded within superpo-
sition in stop words, which are often overlooked in
traditional interpretability analyses.
Model Steering. Meaningful dictionary features
should effectively steer model behavior by increas-
ing the likelihood of related codes (Templeton et al.,
2024). We confirm this in the multilabel setting by
"clamping" relevant feature activations, demonstrat-
ing that we can drive our model to predict specific
subsets of medical codes without additional tokens,
modifying model weights, or explicitly training
steering vectors (Subramani et al., 2022). This find-
ing may inspire cheaper alternatives for quickly
modifying model behavior, especially as ICD cod-
ing models become larger.
Setup. To measure the direct impact of each
dictionary feature on ICD code predictions, we in-
put pad tokens to generate a blank canvas of PLM
embeddings. Instead of ablating decomposed dic-
tionary features (eq. 7), we manually "clamp" or
increase each feature’s activation to a large value
(50) and reconstruct new embeddings. We then
measure the increases in downstream ICD code
probabilities, counting the number of ICD codes
with a softmax probability increase of 0.5 or more
(i.e., a decision flip) and their respective number of
dictionary features. To validate the meaningfulness
of each clamped dictionary feature, we construct
a new dictionary with the top ICD code probabil-
ities increased by each clamped feature and rerun
the hidden meaning identification experiment from
Table 2.
Results. Table 3 shows that while not all dic-
tionary features are meaningful in steering modelAblating Dictionary Features of Highlighted Tokens
Experiment AutoCodeDL LAAT + Baselines DL Baselines
L1 SPINE ICA PCA Identity Random Token L1 SPINE ICA PCA Identity Random
Top (Comp) ↑0.837 0.862 4.347e-5 0.834 0.575 0.845 0.834 0.878 0.959 5e-3 0.909 0.806 0.967
NT↓ 2.568 2.703 8.565e-3 2.628 2.105 387.901 2.640 183.530 15.850 0.064 47.000 758.140 256.136
Ratio↑ 0.326 0.319 0.051 0.318 0.273 0.002 0.316 0.005 0.061 0.008 0.019 0.001 0.003
Table 1: Softmax probability changes in downstream ICD predictions resulting from feature ablations (i.e., compre-
hensiveness). “Top” represents the mean magnitude of softmax drops for the most probable ICD code, while “NT”
signifies the sum of absolute softmax probability changes of other ICD codes for each clinical note. The “Ratio”
indicates the ratio between these two measures. We bold and distinguish the results obtained using our combined
LAAT and dictionary learning framework, and observe that our method has the most precise effect on downstream
ICD predictions, suggesting improved explanatory power.
Hidden Medical Meaning Identification Accuracy
AutoCodeDL Baselines
L1 SPINE ICA PCA Identity Random
0.91 0.89 0.40 0.53 0.61 0.37
Table 2: Proportion of stop word embedding labels cor-
rectly identified by our AutoCodeDL framework, along-
side the baseline methods. Such results showcase that
DL is capable of effectively identifying hidden mean-
ings embedded within superposition.
Model Steering Experiment with Dictionary Features
Metrics AutoCodeDL Baselines
L1 SPINE ICA PCA Identity Random
No. Code Flips ↑ 3449 3681 0 511 1 3681
No. Meaningful fi↑928 1353 0 10 1 768
ID Accuracy ↑ 0.55 0.89 0.29 0.32 0.26 0.26
Table 3: Table 3: Model steering experiment results
comparing AutoCodeDL (L1 and SPINE) with base-
lines. Effectively changing all medical codes and still
attaining high identification accuracy through the dis-
covery of clamped classes, AutoCodeDL with SPINE
is capable of steering the model in highly interpretable
ways.
behavior, the dictionary embeddings can change
the predictions of nearly all ICD codes. Moreover,
the clamped classes used to generate the dictio-
nary can still recover the hidden medical codes of
extraneous stop tokens, suggesting each feature’s
explainability. Further verification is provided by a
UMAP plot (Figure 4), where the color indicates
the maximum increase in probability of its top med-
ical code. The plot reveals clusters of semantically
meaningful features with a direct ability to change
model predictions of specific subsets of codes, an-
notated based on their dictionary contexts and rele-
vant medical codes.
4.2 Human Understandability of Dictionaries
Evaluating the human understandability of inter-
pretability methods lacks a clear definition, oftendepending on qualitative assessments (Zhang et al.,
2021; Alangari et al., 2023). Given the limited
availability of clinically licensed physicians, there’s
a need for scalable proxy metrics to measure the
understandability of learned dictionaries across ex-
tensive clinical notes, encompassing thousands of
tokens. Through manual examination of various
dictionary features, we pinpoint two primary fac-
tors that contribute to a dictionary feature’s under-
standability.
Coherence. A dictionary feature is deemed un-
derstandable if its top tokens are semantically re-
lated, indicating a clear conceptual identity. Con-
versely, semantic randomness among tokens com-
plicates the identification of a feature’s underlying
concept.
Setup. Advancements in Siamese encoder trans-
formers, like Siamese BERT, have boosted the ef-
ficiency and effectiveness of semantic similarity
analyses (Reimers and Gurevych, 2019), mimick-
ing human perceptions of text pair similarity. Utiliz-
ing Siamese encoder embeddings, we calculate the
average cosine similarity among the top ktokens
of each dictionary feature to gauge their conceptual
relatedness. For more methodological specifics of
our Siamese BERT experiment, please refer to Ap-
pendix A.10. Although LAAT is part of our method
for better explaining downstream ICD predictions,
we also treat the LAAT matrix as a "dictionary" of
ICD codes, similar to dictionary features learned by
sparse autoencoders. While comparing supervised
(LAAT) and unsupervised (autoencoders) methods
isn’t perfect, LAAT remains the current standard
for coherence in this area of automated ICD coding
with PLMs.
Results. The coherence of the unsupervised dic-
tionary learning (DL) methods, represented by the
DL columns, decreased as the number of top k to-Figure 4: UMAP of SPINE Embeddings: Dictionary features are interpretable in steering model behavior. More
darker red colors indicates higher maximum observed probability increase of the top medical code from a feature’s
exclusive clamping. Each dot is a dictionary feature embedding projected into 2D.
kens increased. Contrary to expectations from Sub-
ramanian et al. (2017), sparse autoencoders with
L1 minimization exhibited the highest coherence
among unsupervised methods, with the highest av-
erage cosine similarity across different k values.
Qualitative examples and the relationship between
dictionary contexts and coherence are provided in
Appendix A.11.1. The most interpretable dictio-
nary features correspond to high cosine similarities,
potentially filtering out easy cases for human anno-
tators to focus on more complex features.
Coherence of All Activated Dictionary Features
DL Baselines Supervised
k↑ L1 SPINE ICA PCA Identity Random LAAT
2 0.3130 0.3074 0.2138 0.2371 0.2498 0.2591 0.4185
4 0.2981 0.2872 0.2133 0.2294 0.2440 0.2449 0.4083
10 0.2747 0.2684 0.2095 0.2218 0.2384 0.2358 0.3889
Table 4: Average cosine similarity between the top k to-
kens extracted from each dictionary feature or ICD code,
measured from Siamese encoder embeddings. Higher
values indicate a stronger thematic connection within
the feature or code. The "DL" columns represent our
dictionaries constructed, while the remaining columns
are baselines.
Distinctiveness. If a dictionary feature has a
clear, coherent theme based on its top tokens, unre-lated tokens should be readily discernible.
Setup. Inspired by Subramanian et al. (2017),
who evaluated the distinctiveness of dictionary fea-
tures in sparse word embeddings, we investigate
by sampling a dictionary feature’s top 4 ( k=4) acti-
vating tokens (and their nearby context windows)
and a randomly sampled token outside the feature.
An interpretability score is derived by having med-
ical experts (a licensed physician and a medical
scientist trainee) identify the randomly sampled
token from the set of 5 tokens. The proportion
of correctly identified tokens serves as our dis-
tinctiveness metric. Due to time constraints, our
medical experts evaluated 100 samples each for
DL and other baseline encoders. However, given
the demonstrated capabilities of state-of-the-art
language models in text annotation tasks (Huang
et al., 2023; Gilardi et al., 2023) and their extensive
medical vocabulary (Bommineni et al., 2023), we
utilized the current medically quantized state-of-
the-art OpenBioLLM Llama 3 70B model across
all dictionary features from the dictionaries con-
structed in the stop words experiment (Section 3.2).
Results. As shown in Table 5, both sparse au-
toencoders are more distinctive than their respec-
tive unsupervised baselines. Surprisingly, the ran-
dom and identity encoders are more distinguishablethan their PCA counterparts. We provide examples
in Appendix A.11.2 illustrating that the ability to
differentiate features can range from exceptionally
obvious cases with repeating tokens or differences
in specificity to completely uninterpretable cases.
Percentage of Dictionary Features Differentiated
DL Baselines Supervised
L1 SPINE ICA PCA Identity Random LAAT
No. LLM Id. ↑ 2828 2713 282 226 297 299 2117
% LLM Id. ↑ 0.46 0.44 0.37 0.29 0.39 0.39 0.58
% Human Id. (100) ↑0.49 0.56 0.45* 0.41 0.45 0.44 *
Table 5: Percentage of dictionary features success-
fully distinguished by the quantized OpenBioLLM-70B
Llama 3 model and medical experts, determined by cor-
rectly identifying the unrelated token from a set of 5 to-
kens (including the top 4 activating tokens and their con-
text windows) highlighted by our sparse autoencoders.
* denotes cases where human evaluations were omitted
or reduced (i.e., 40) due to time constraints.
5 Conclusion
This study introduces a novel method that com-
bines dictionary learning with label attention mech-
anisms to improve the interpretability of medi-
cal coding language models. By uncovering in-
terpretable dictionary features from dense lan-
guage embeddings, the proposed approach offers
a dictionary-based rationale for ICD code predic-
tions, addressing the growing demand for trans-
parency in automated healthcare decisions. This
work lays the groundwork for future explorations
in dictionary learning to enhance healthcare inter-
pretability.
Limitations
We note that there are several key limitations of the
dictionary learning applied here. First, as noted in
the training details within the appendix, the sparse
autoencoders are unable to perfectly reconstruct
PLM embeddings, and thus cannot fully capture
a model’s downstream performance (Cunningham
et al., 2023; Bricken et al., 2023; Templeton et al.,
2024). Furthermore, we note as similarly explored
by (Cunningham et al., 2023; Bricken et al., 2023;
Templeton et al., 2024) that there exist dead or miss-
ing features, features that do not activate regardless
of token embedding or concepts that exist in the
model but are not captured within the dictionary, in-
dicating not all features are useful or interpretable.
In terms of the coherence of highly activating
features, we highlight that the sparse dictionaries
learned are not as coherent as supervised mappingsof the LAAT, limiting its human understandability.
Furthermore, we observe that simply interpreting
the PLM embeddings at the last layer, which (Yun
et al., 2023b; Cunningham et al., 2023) claim to
be the most interpretable, is insufficient for fully
capturing and interpreting a PLM’s behavior. Refer-
ring to Table 3, the resolution of our dictionary fea-
tures is lacking in comparison to the granularity of
medical codes, where we observe that the total num-
ber of highly meaningful features (i.e., those that
can change a medical code’s prediction) is smaller
than the number of medical codes highly affected,
suggesting that the features learned most likely rep-
resent higher abstract concepts than highly-specific
medical codes.
Recently, (Templeton et al., 2024) has explored
scaling up sparse autoencoders to potentially mil-
lions of dictionary features for extremely large lan-
guage models. They show that not all semantic
features are learnable, especially in cases without
access to large amounts of diverse data (Bricken
et al., 2023). While synthetic data generation exists
to further augment the diversity of tokens in the
dictionary learning corpus, the size of sparse au-
toencoders must also increase as model complexity
increases (Templeton et al., 2024) in order to main-
tain the granularity of dictionary features. Regard-
less, such mechanistic explanations are still fairly
efficient when compared to black-box alternatives.
To the best of our knowledge, its applicability to
smaller scale language datasets is unknown.
While our experiments revealed several key ben-
efits of using sparse autoencoders for interpretabil-
ity, future work offers exciting avenues to push
medical PLM interpretability further. Despite
SPINE’s (Subramanian et al., 2017) improvements
over vanilla L1methods, sparse autoencoders’ re-
construction limitations can restrict their ability
to capture all information relevant to downstream
predictions. Exploring more expressive representa-
tion learning techniques like causal and disentan-
glement methods (Schölkopf et al., 2021; Wang
et al., 2023) could hold promise for interpretabil-
ity gains. Additionally, unraveling the complex
relationships within medical PLMs through both
automated circuit discovery (Conmy et al., 2023)
and dictionary learning approaches (Cunningham
et al., 2023) could offer further valuable insights
into an ICD code prediction.Ethics Statement
We note that all clinical notes used from the
MIMIC-III dataset (Johnson et al., 2023, 2016) are
deidentified and that our method is heavily focused
on medical tokens and their conceptual meanings
towards ICD predictions. Thus, we follow guide-
lines laid out by PhysioNet’s MIMIC3 health data
license (Johnson et al., 2016) and note our study
does not contain any additional patient information
that can lead to privacy violations. We note that
our expert human annotators have consented and
are our collaborators.
References
Samira Abnar and Willem Zuidema. 2020. Quantifying
attention flow in transformers.
Nourah Alangari, Mohamed El Bachir Menai, Hassan
Mathkour, and Ibrahim Almosallam. 2023. Explor-
ing evaluation methods for interpretable machine
learning: A survey. Information , 14(8).
Felipe Almeida and Geraldo Xexéo. 2023. Word em-
beddings: A survey.
Vikas L Bommineni, Sanaea Bhagwagar, Daniel Bal-
carcel, Vishal Bommineni, Christos Davazitkos, and
Donald Boyer. 2023. Performance of chatgpt on the
mcat: The road to personalized and equitable pre-
medical learning. medRxiv .
Trenton Bricken, Adly Templeton, Joshua Batson,
Brian Chen, Adam Jermyn, Tom Conerly, Nick
Turner, Cem Anil, Carson Denison, Amanda Askell,
Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
Schiefer, Tim Maxwell, Nicholas Joseph, Zac
Hatfield-Dodds, Alex Tamkin, Karina Nguyen,
Brayden McLean, Josiah E Burke, Tristan Hume,
Shan Carter, Tom Henighan, and Christopher
Olah. 2023. Towards monosemanticity: Decom-
posing language models with dictionary learning.
Transformer Circuits Thread . Https://transformer-
circuits.pub/2023/monosemantic-
features/index.html.
Pengfei Cao, Chenwei Yan, Xiangling Fu, Yubo Chen,
Kang Liu, Jun Zhao, Shengping Liu, and Weifeng
Chong. 2020. Clinical-coder: Assigning inter-
pretable ICD-10 codes to Chinese clinical notes. In
Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics: System Demon-
strations , pages 294–301, Online. Association for
Computational Linguistics.
Chun Sik Chan, Huanqi Kong, and Liang Guanqing.
2022. A comparative study of faithfulness metrics
for model interpretability methods. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,pages 5029–5038, Dublin, Ireland. Association for
Computational Linguistics.
Aditya Chattopadhay, Anirban Sarkar, Prantik
Howlader, and Vineeth N Balasubramanian. 2018.
Grad-CAM++: Generalized gradient-based visual
explanations for deep convolutional networks. In
2018 IEEE Winter Conference on Applications of
Computer Vision (WACV) . IEEE.
Hila Chefer, Shir Gur, and Lior Wolf. 2021. Generic
attention-model explainability for interpreting bi-
modal and encoder-decoder transformers.
Hila Chefer, Idan Schwartz, and Lior Wolf. 2022. Op-
timizing relevance maps of vision transformers im-
proves robustness.
Hugh Chen, Scott M. Lundberg, and Su-In Lee. 2022.
Explaining a series of models by propagating shapley
values. Nature Communications , 13(1):4512.
Arthur Conmy, Augustine N. Mavor-Parker, Aengus
Lynch, Stefan Heimersheim, and Adrià Garriga-
Alonso. 2023. Towards automated circuit discovery
for mechanistic interpretability.
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
Huben, and Lee Sharkey. 2023. Sparse autoencoders
find highly interpretable features in language models.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,
Eric Lehman, Caiming Xiong, Richard Socher, and
Byron C. Wallace. 2020. Eraser: A benchmark to
evaluate rationalized nlp models.
Andres Duque, Hermenegildo Fabregat, Lourdes
Araujo, and Juan Martinez-Romo. 2021. A
keyphrase-based approach for interpretable icd-10
code classification of spanish medical reports. Artifi-
cial Intelligence in Medicine , 121:102177.
Joakim Edin, Alexander Junge, Jakob D. Havtorn, Lasse
Borgholt, Maria Maistro, Tuukka Ruotsalo, and Lars
Maaløe. 2023. Automated medical coding on mimic-
iii and mimic-iv: A critical review and replicability
study. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in
Information Retrieval , SIGIR ’23, page 2572–2582,
New York, NY , USA. Association for Computing
Machinery.
Nelson Elhage, Tristan Hume, Catherine Olsson,
Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
Carol Chen, Roger Grosse, Sam McCandlish,
Jared Kaplan, Dario Amodei, Martin Watten-
berg, and Christopher Olah. 2022. Toy mod-
els of superposition. Transformer Circuits
Thread .https://transformer-circuits.pub/
2022/toy_model/index.html .
Subhroshekhar Ghosh, Aaron Y . R. Low, Yong Sheng
Soh, Zhuohang Feng, and Brendan K. Y . Tan. 2023.
Dictionary learning under symmetries via group rep-
resentations.Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd workers for
text-annotation tasks. Proceedings of the National
Academy of Sciences , 120(30).
Hajar Hakkoum, Ibtissam Abnane, and Ali Idri. 2022.
Interpretability in the medical field: A systematic
mapping and review study. Applied Soft Computing ,
117:108391.
J A Hirsch, G Nicola, G McGinty, R W Liu, R M Barr,
M D Chittle, and L Manchikanti. 2016. ICD-10: His-
tory and context. AJNR Am J Neuroradiol , 37(4):596–
599.
Chao-Wei Huang, Shang-Chi Tsai, and Yun-Nung Chen.
2022. Plm-icd: Automatic icd coding with pretrained
language models.
Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is
chatgpt better than human annotators? potential and
limitations of chatgpt in explaining implicit hate
speech. In Companion Proceedings of the ACM Web
Conference 2023 , WWW ’23. ACM.
Alistair E. W. Johnson, Lucas Bulgarelli, Lu Shen, Alvin
Gayles, Ayad Shammout, Steven Horng, Tom J. Pol-
lard, Sicheng Hao, Benjamin Moody, Brian Gow,
Li-wei H. Lehman, Leo A. Celi, and Roger G. Mark.
2023. Mimic-iv, a freely accessible electronic health
record dataset. Scientific Data , 10(1):1.
Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li-
wei H. Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G. Mark. 2016. Mimic-iii,
a freely accessible critical care database. Scientific
Data , 3(1):160035.
Renee L Johnson, Holly Hedegaard, Emilia S Pasalic,
and Pedro D Martinez. 2021. Use of ICD-10-CM
coded hospitalisation and emergency department data
for injury surveillance. Inj Prev , 27(S1):i1–i2.
Rajvir Kaur, Jeewani Anupama Ginige, and Oliver Obst.
2021. A systematic literature review of automated
icd coding and classification systems using discharge
summaries.
Patrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-
anov. 2020. Pretrained language models for biomedi-
cal and clinical tasks: Understanding and extending
the state-of-the-art. In Proceedings of the 3rd Clini-
cal Natural Language Processing Workshop , pages
146–157, Online. Association for Computational Lin-
guistics.
Scott Lundberg and Su-In Lee. 2017. A unified ap-
proach to interpreting model predictions.
Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex
DeGrave, Jordan M. Prutkin, Bala Nair, Ronit Katz,
Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee.
2020. From local explanations to global understand-
ing with explainable ai for trees. Nature Machine
Intelligence , 2(1):56–67.Raha Moraffah, Mansooreh Karami, Ruocheng Guo,
Adrienne Raglin, and Huan Liu. 2020. Causal inter-
pretability for machine learning – problems, methods
and evaluation.
Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel
Gallagher, and Georg Groh. 2022. SHAP-based ex-
planation methods: A review for NLP interpretabil-
ity. In Proceedings of the 29th International Con-
ference on Computational Linguistics , pages 4593–
4603, Gyeongju, Republic of Korea. International
Committee on Computational Linguistics.
James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng
Sun, and Jacob Eisenstein. 2018. Explainable predic-
tion of medical codes from clinical text.
Tomáš Musil. 2019. Examining structure of word em-
beddings with pca.
Tomáš Musil and David Mare ˇcek. 2022. Independent
components of word embeddings represent semantic
features.
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel
Goh, Michael Petrov, and Shan Carter. 2020.
Zoom in: An introduction to circuits. Distill .
Https://distill.pub/2020/circuits/zoom-in.
Bruno A. Olshausen and David J. Field. 1997. Sparse
coding with an overcomplete basis set: A strategy
employed by v1? Vision Research , 37(23):3311–
3325.
Kimberly J O’Malley, Karon F Cook, Matt D Price,
Kimberly Raiford Wildes, John F Hurdle, and
Carol M Ashton. 2005. Measuring diagnoses: ICD
code accuracy. Health Serv Res , 40(5 Pt 2):1620–
1639.
Lakshmi Narayan Pandey, Rahul Vashisht, and Har-
ish G. Ramaswamy. 2023. On the interpretability of
attention networks.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP) , pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.
Preethi Rao, Shira H Fischer, Mary E Vaiana, and
Erin Audrey Taylor. 2022. Barriers to price and qual-
ity transparency in health care markets. Rand Health
Q, 9(3):1.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. "why should i trust you?": Explain-
ing the predictions of any classifier.
Tilman Räuker, Anson Ho, Stephen Casper, and Dylan
Hadfield-Menell. 2023. Toward transparent ai: A
survey on interpreting the inner structures of deep
neural networks.Wojciech Samek, Alexander Binder, Grégoire Mon-
tavon, Sebastian Bach, and Klaus-Robert Müller.
2015. Evaluating the visualization of what a deep
neural network has learned.
Bernhard Schölkopf, Francesco Locatello, Stefan Bauer,
Nan Rosemary Ke, Nal Kalchbrenner, Anirudh
Goyal, and Yoshua Bengio. 2021. Towards causal
representation learning.
Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2019. Learning important features through
propagating activation differences.
Nishant Subramani, Nivedita Suresh, and Matthew E.
Peters. 2022. Extracting latent steering vectors from
pretrained language models.
Anant Subramanian, Danish Pruthi, Harsh Jhamtani,
Taylor Berg-Kirkpatrick, and Eduard Hovy. 2017.
Spine: Sparse interpretable neural embeddings.
Yuanbo Tang, Zhiyuan Peng, and Yang Li. 2023. Ex-
plainable trajectory representation through dictionary
learning.
Adly Templeton, Tom Conerly, Jonathan Marcus, Jack
Lindsey, Trenton Bricken, Brian Chen, Adam Pearce,
Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy
Cunningham, Nicholas L Turner, Callum McDougall,
Monte MacDiarmid, C. Daniel Freeman, Theodore R.
Sumers, Edward Rees, Joshua Batson, Adam Jermyn,
Shan Carter, Chris Olah, and Tom Henighan. 2024.
Scaling monosemanticity: Extracting interpretable
features from claude 3 sonnet. Transformer Circuits
Thread .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2023. Attention is all
you need.
Thanh Vu, Dat Quoc Nguyen, and Anthony Nguyen.
2020. A label attention model for icd coding from
clinical text. In Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelli-
gence , IJCAI-PRICAI-2020. International Joint Con-
ferences on Artificial Intelligence Organization.
Xin Wang, Hong Chen, Si’ao Tang, Zihao Wu, and
Wenwu Zhu. 2023. Disentangled representation
learning.
Ruiyu Xu, Chao Wang, Yongxiang Li, and Jianguo Wu.
2023. Generalized time warping invariant dictionary
learning for time series classification and clustering.
Chenwei Yan, Xiangling Fu, Xien Liu, Yuanqiu Zhang,
Yue Gao, Ji Wu, and Qiang Li. 2022. A survey of
automated international classification of diseases cod-
ing: development, challenges, and applications. In-
telligent Medicine , 2(3):161–173.
Zhichao Yang, Sanjit Singh Batra, Joel Stremmel, and
Eran Halperin. 2023. Surpassing gpt-4 medical cod-
ing with a two-stage approach.Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann
LeCun. 2023a. Transformer visualization via dictio-
nary learning: contextualized embedding as a linear
superposition of transformer factors.
Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann
LeCun. 2023b. Transformer visualization via dictio-
nary learning: contextualized embedding as a linear
superposition of transformer factors.
Yu Zhang, Peter Tino, Ales Leonardis, and Ke Tang.
2021. A survey on neural network interpretability.
IEEE Transactions on Emerging Topics in Computa-
tional Intelligence , 5(5):726–742.
Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and
David Zhang. 2015a. A survey of sparse represen-
tation: Algorithms and applications. IEEE Access ,
3:490–530.
Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and
David Zhang. 2015b. A survey of sparse represen-
tation: Algorithms and applications. IEEE Access ,
3:490–530.
A Appendix
A.1 Sparse Autoencoder Training Details
We train our sparse autoencoders on the PLM acti-
vations generated by a 110M medical RoBERTa en-
coder PLM on the cleaned MIMIC-III train dataset
of 38,427 clinical notes and evaluated on their test
set of 8,750 clinical notes for a total of 52,712 clin-
ical notes, as detailed by (Edin et al., 2023). We
follow the advice of (Bricken et al., 2023) and (Sub-
ramanian et al., 2017) in training the L1and SPINE
autoencoders respectively. Our hyperparameters
are shown in Table 6. We also use AdamW as
our optimizer. For a fair comparison, we reuse
the dictionary feature size mas it has been noted
by (Bricken et al., 2023) that larger dictionary fea-
ture sizes can potentially increase the resolution
of concepts of dictionary features. For instance,
dictionary feature may be further decomposed into
features of of more specific meanings given differ-
ent token sequence contexts as further discussed by
(Bricken et al., 2023). We use PyTorch as our deep
learning framework of choice.
Table 6: Sparse Autoencoder Training Details
λL1λ1λ2 m Batch Size lr
2e-5 1 1 6,144 8,192 1e-3
We train on randomly sampled embeddings from
the training set, and filter out all pad tokens, that are
irrelevant to the final prediction, but dominate eachbatch due to their use in making GPU inference fast.
We note that we cannot get perfect reconstruction
loss nor completely match the downstream perfor-
mance of the original model with our reconstructed
embeddings on the test set, but we get very close.
Table 7: Autoencoder Test Loss Metrics
L1 SPINE Original
Test Autoencoder Loss 69.46 39.59 N/A
Test F1 0.258 0.260 0.262
A.2 Scalability Discussion
Training these sparse autoencoders takes approxi-
mately 6 minutes per epoch on A6000 GPUs. How-
ever, the current training process samples new to-
ken embeddings for every batch of clinical notes,
which is suboptimal. A quick adaptation of existing
dataloaders to improve token diversity during train-
ing could be beneficial. We find that precomputing
and caching PLM embeddings can significantly re-
duce training time to approximately 15 minutes
for 10 epochs, albeit requiring substantial mem-
ory (at least 128 GB RAM for caching millions of
embeddings). In contrast, decomposing PLM em-
beddings using our method is extremely fast, on par
with LAAT (approximately 0.04 seconds per clini-
cal note). While there is an upfront cost (a couple
of hours for sorting and sampling millions of to-
kens for each dictionary feature), once a dictionary
is constructed, interpreting the embedding space of
any clinical note is very fast and efficient, which is
suitable for this high dimensional multilabel task.
A.3 Build Dictionary
For further clarity, we write up our dictionary con-
struction algorithm here in algorithm 1. In princi-
ple, one is just sorting based on encoded activations
and ablation softmax drops of different ICD codes
for each dictionary feature.
A.4 Additional Baseline Details
There were four main baseline methods that were
compared against our exploration of two sparse
autoencoders, specifically an ICA encoder, PCA
encoder, an identity encoder, and a random en-
coder. All of their implementations were taken
from (Cunningham et al., 2023). The ICA en-
coder was trained using the FastICA decomposi-
tion method from scikit-learn to estimate the activa-
tion’s respective independent components that actAlgorithm 1: Build Dictionary
Input: Autoencoder A, feature fi, tokens x
Output: Dictionary Fmapping fito tokens
xand classes y
1F←dict; foreach token xdo
2 f←A.encode (x);forfiinfdo
3 iffi> F[i].fithen
4 F[i].tokens ←x;
5 δi←ablation (fi);ifδi> F[i].δ
then
6 F[i].classes ←drops (δi);
7return F;
as the encoder weights. However, we note that the
training was unstable, most likely due to the same
memory and computation limitations that (Cun-
ningham et al., 2023) faced. As a result, we limit
the training on only 2,000,000 token embeddings
sampled from the training set. The PCA covariance
matrix was estimated batchwise with its eigenvec-
tors acting as the encoder weights. For the random
and identity encoders, the random encoder was ini-
tialized with a normal distribution of mean 0 and
variance 1 and the identity encoder has an identity
matrix for its encoder weights.
A.5 Label Attention Details
We recognize that we don’t explicitly describe
LAAT (Vu et al., 2020; Huang et al., 2022) in detail
in the main manuscript. For interested readers, we
depict the label attention mechanism in Figure 5.
Essentially, LAAT computes a cross attention
score for each token and ICD code, creating a label
attention matrix where each row is an ICD and
every column is the token’s attention score with
respect to that ICD code.
A.6 Additional SPINE Details
While the L1minimization is most commonly used
to train sparse autoencoders due to its simplicity of
training (Zhang et al., 2015b), we also revisit an
alternative sparse formulation SPINE proposed by
(Subramanian et al., 2017), specifically designed
to decompose neural word embeddings. (Subra-
manian et al., 2017) showed that they could im-
prove interpretability in GloVe (Pennington et al.,
2014) and other forms of neural token embeddings
(Almeida and Xexéo, 2023). Their formulation re-
places the L1regularization loss in favor of using aFigure 5: Label Attention identifies the most relevant tokens for each ICD code through a label attention matrix.
combination of average sparsity and a partial spar-
sity loss function terms. We outline their method
in the equations below.
We define our average sparsity term below in
equation 9.
Lasl=X
imax( fi, ρ) (9)
Here, ρis a user-chosen sparsity hyperparameter
that prevents the dictionary feature activations f
from becoming too large. In practice, one should
ideally average each sparse feature activation ¯fi
across the entire training set first as outlined by
(Subramanian et al., 2017). However, due to mem-
ory constraints, we simply average over the batch
size.
The partial sparsity term is defined as follows
below.
Lpsl=X
ifi·(1−fi) (10)
The general intuition behind the following partial
sparsity term is that it further enforces sparsity by
penalizing the distribution of fi’s away from unifor-
mity, which can invariably occur due to the stochas-
tic nature of gradient optimization algorithms. The
resulting alternative sparse autoencoder minimiza-
tion function is shown in equation 11 .
L=1
|X|X
x∈X∥x−ˆx∥2
2+λ1Lasl+λ2Lpsl(11)
A.7 Baseline Ablation Experiment Details
For every baseline, we iterate through each acti-
vated dictionary feature for the tokens, measur-
ing the softmax drop for the selected ICD code(with the highest softmax probability). This is re-
peated for each none-DL baseline: In ICA, we ab-
late embedding independent components. In PCA,
we ablate principal components (i.e eigenvectors
weighed by their eigenvalues). In the identity en-
coder, we ablate embedding dimensions. In the
random encoder, we essentially add random noise.
For the Token baseline, we ablate entire token em-
beddings.
A.8 Sufficiency Discussion
If comprehensiveness measures the performance
drop when ablating key features, sufficiency mea-
sures the performance retention when keeping only
the key features (Chan et al., 2022). In particu-
lar, rather than performing an ablation, we simply
set all of the token embeddings to their respective
dictionary feature embeddings multiplied by their
activations. Clinical coding is high-dimensional,
making it computationally expensive to compute
different quantiles. Therefore, we only compute
metrics for the highly relevant 95% quantile tokens
when using LAAT, or for all tokens when not using
LAAT.
˜x=fi·hi (12)
However, when using our specific feature encoders,
this metric yields mixed results. While retaining
only the relevant token embeddings defined by
LAAT preserves the model’s class probabilities,
we find that a randomly generated embedding from
our random encoder remarkably performs similarly
or better than the LAAT explanation. This obser-
vation raises further questions about the potential
inner workings and importance of the embedding
space in explaining model behavior. Especially,
as the steering experiment is effectively related to
the sufficiency metric, but with difference beingfi’s artificial expansion versus its original encod-
ing. Understanding these challenges in embedding
interpretability is an important point of future work.
A.9 Hidden Meaning Stop Words Experiment
We explain more details of our stop words experi-
ments here. To begin, we sample all (not just the
attention highlighted) tokens from 1,600 clinical
notes sampled from the test set and then map highly
activating tokens to each dictionary feature as well
as relevant ICD code predictions through dictio-
nary feature ablations outlined in section 3.2 and
3.3. Then, we collect (12,891) highly relevant stop
words (using NLTK) identified by the label atten-
tion mechanism, documenting their corresponding
labels and PLM embeddings as defined by the la-
bel attention matrix. After shuffling each label-
embedding pairing, we employ our dictionary to
query the stop word’s relevant classes via a trained
sparse autoencoder, assessing if the original label
ranks among the top 10 classes of each of its most
highly activated dictionary features. As the magni-
tudes of different dictionary features varies across
decomposition methods and are mostly sparse in
sparse autoencoders, we define highly activated fea-
turesfias dictionary features that exceed the 96.5th
percentile feature magnitude of encoded from each
token embedding. In practice, since the encoded
dictionary features for each token by sparse autoen-
coders are sparse, the highest activated dictionary
features are those that are nonzero. The efficacy
of dictionary features in clarifying a stop word’s
significance to an ICD code prediction is quantified
by calculating the proportion of shuffled stop word
embeddings are correctly contained in these predic-
tion sets generated by their respective dictionaries.
A.10 Additional Siamese BERT Cosine
Similarity Experiment Details
We use "all-mpnet-base-v2" from the sentence-
transformers package as our Siamese encoder. We
show our steps for computing the cosine similarity
scores in our evaluation of coherence scores below
in algorithm 2. Furthermore, since sparse encoding
is a more efficient operation than searching and
ablating the most relevant dictionary feature for an
ICD prediction, we sample all tokens from every
clinical note in the test set, and discern all of the top
tokens for each dictionary feature with our sparse
autoencoder.Algorithm 2: Cosine Similarity Score for
Dictionary Features
Input: Siamese BERT model M, dictionary
feature fi;
knumber of highly activating tokens in
dictionary T={tfi,1, tfi,2, . . . t fi,k};
Output: Cosine Similarity Score ¯s
1¯s←0;
2foreachfido
3 tpairs← {(tfi,a, tfi,b)|a̸=
b, tfi,a, tfi,b∈T});
4 foreach (tfi,a, tfi,b)intpairsdo
5 ˆs=M(tfi,a)·M(tfi,b)
∥M(tfi,a)∥·∥M(tfi,b)∥;
6 ¯s←¯s+ˆs
|tpairs|;
7 ¯s←¯s
|f|
A.11 Human Evaluations
We conducted human evaluations with a medical
scientist trainee and a licensed physician, specif-
ically the distinctiveness experiment inspired by
Subramanian et al. (2017). Below, we showcase
examples of incoherent dictionary features (low co-
sine similarity) and highly coherent features (high
cosine similarity). We also provide examples from
the human evaluations, including cases where an-
notators failed to identify the randomly chosen con-
text and cases where it was easy for them to distin-
guish the random token.Ablating Dictionary Features of Highlighted Tokens
Experiment AutoCodeDL LAAT + Baselines DL Baselines
L1 SPINE ICA PCA Identity Random Token L1 SPINE ICA PCA Identity Random
Suff.↓ 0.143 0.462 0.470 0.424 0.468 -0.029 -0.012 0.137 0.454 0.470 0.112 -0.005 -0.033
Table 8: Softmax probability changes in downstream ICD predictions resulting from sufficiency experiments.
A.11.1 Dictionary Features and Coherence
We provide qualitative examples in Figures 6, 7,
and 8 to demonstrate the efficacy of our cosine
similarity metric. While an imperfect metric, co-
sine similarity can effectively discern highly inter-
pretable dictionary features, which typically have
repeating tokens. However, many dictionary fea-
tures are more challenging and may require more
complex annotation approaches, such as domain-
specific language models, medical experts, or fine-
tuning specific Siamese BERT encoders for this
task, as the average cosine similarity of the top k
tokens is intrinsically low due to the diversity of
the text.
Figure 6: Example of highly interpretable SPINE fea-
ture with high cosine similarity: In this particular case,
all activating tokens (red) with their context windows
are all atrial fibrillation tokens, giving us a very high
cosine similarity (i.e. close to 1).
Figure 7: Example of interpretable SPINE feature
with low cosine similarity: In this case, the activating
tokens (red) are diverse, containing various concepts
such as food, fluids, gastric emptying, pneumothorax,
and related terms. Our clamping experiments show
that this dictionary feature is predictive of the code
"acidosis," a common complication potentially leading
to gastric emptying. For such cases, the cosine similarity
metric is not informative, as these coherent features
often have lower cosine similarities (closer to 0) despite
sharing a common theme.Figure 8: Example of an uninterpretable SPINE fea-
ture with low cosine similarity: Various tokens are
highlighted (red) without an obvious cohesive theme.
As expected, these tokens result in very small cosine
similarity measurements.A.11.2 Human Distinctiveness Case Studies
We perform the word intrusion experiment from
Subramanian et al. (2017) for the sparse autoen-
coder features and other baselines. Figures 9, 10,
11, and 12 showcase the expected unrelated "ran-
dom context" in gold and the expert annotators’
choices in red or blue for L1features. Qualitative
examinations reveal varying levels of distinctive-
ness. Highly coherent features made it easy to
distinguish the outlier token(s), but more compli-
cated cases involving the level of abstraction of
specific token contexts (Figure 10) were harder to
discern, especially when the resolution of the token
mattered. Other features were shown to be highly
uninterpretable (Figures 11, 12).
Figure 9: Example of an interpretable L1feature.
All highly activating tokens are related to heart condi-
tions whereas the token "laparoscopic", an operation
performed on the abdomen was clearly an outlier. We
also observe the repeating abbreviations "avr".Figure 10: Example of a less interpretable L1fea-
ture differentiated by both annotators: While both
annotators acknowledged the lack of an explicit medi-
cal theme, they selected the set of tokens discussing a
specific piece of anatomy, suggesting that the feature’s
interpretation activates a more abstract concept.
Figure 11: Example of an uninterpretable L1feature
where only one annotator identified the random con-
text: According to our experts, in such cases where
no discernible underlying theme exists, selecting the
random context is essentially by chance.Figure 12: Uninterpretable L1feature neither anno-
tator could differentiate: Both annotators agreed that
this dictionary feature lacked a distinctive theme.
Figure 13: Example of human evaluation form interface.
Figure 14: Example of human evaluation form instruc-
tions.A.12 Initial Sparse Autoencoder Experiments
We trained new sparse autoencoders for this revi-
sion, as we felt the previous ones were ill-trained
due to not filtering out pads during the training
process, leading to many irrelevant medical fea-
tures. Surprisingly, despite finding that only ap-
proximately 500 features accounted for many of
the ablation downstream results, their feature abla-
tions still highly affected downstream performance,
as shown in Table 9. Note that these experiments
measured the top ground truth medical code rather
than the top predicted medical code, hence the dis-
crepancy in the probability drops. However, the
order of these results remains the same. We also
report the previous results, such as coherence and
similarity, in Section A.12.2. We were surprised
to find the relative order of baselines in terms of
explainability performance remained the same in
this revision.
A.12.1 Other Initial Ablation Experiments
We perform additional validation experiments to
showcase that sparse autoencoder (dictionary learn-
ing) do outperform their none-sparse baselines in
terms of model explainability. For reference, we
have investigated the downstream effects of dic-
tionary learning through a total of four ablation
experiments (Figure 15). Our analysis centered
on the ICD code most likely to be predicted for
each note, determined by softmax probabilities,
and employed two token ablation benchmarks: (A)
complete ablation of all highlighted tokens and (C)
random ablation of half the top highlighted tokens.
In parallel, dictionary features underwent ablation
in two forms: (B) solely ablation of the paramount
dictionary feature for all highlighted tokens or (D)
ablating half of the tokens and ablating the most
significant dictionary feature of the remaining high-
lighted tokens. For reference, tokens surpassing the
95th percentile in attention scores for their respec-
tive ICD code in the label attention matrix were
considered "highlighted". We note that we have
only shown experimental results for ablation types
(A) and (B) in section 4.1.
To investigate if such ablations are perfectly ad-
ditive, we perform further experiments (C) and (D)
where we only ablate half of the relevant tokens
and observe the overall changes from ablating the
dictionary features of the unablated other half of
tokens in Table 11.
We observe that while ablating such dictionary
Figure 15: Ablation experiments on clinical notes using
a label attention (LAAT) mechanism to highlight rele-
vant tokens for ICD coding: (A) Complete ablation of
all highlighted tokens, or ablation of only the most rele-
vant dictionary features of each token embedding (B),
and (C) random ablation of half the highlighted tokens
compared to (D) random ablation of half the highlighted
tokens and the most relevant dictionary features of other
half of token embeddings.
features do indeed drop the softmax probabilites of
the most likely ICD code, they do not sum to the
original softmax drop observed in the ablation ex-
periment in section 4.1, and thus there is some miss-
ing information that is not entirely encompassed
by the dictionary feature ablations. That being said,
DL still provides the best model explanation for
downstream ICD predictions in this scenario.
A.12.2 Initial Human Understandability
Evaluations
Coherence of top 500 activating dictionary fea-
tures. Utilizing Siamese encoder embeddings, we
calculate the average cosine similarity among the
topktokens of the top 500 dictionary feature
to gauge their conceptual relatedness. For more
methodological specifics of our Siamese BERT ex-
periment, please refer to Appendix A.10. While
not using the same sparse autoencoders, these re-
sults are still useful from at least a reproducibility
standpoint.
Results. Overall, the coherence of the unsuper-Ablating Dictionary Features of Highlighted Tokens
Experiment AutoCodeDL LAAT + Baselines DL Baselines
L1 SPINE ICA PCA Identity Random Token L1 SPINE ICA PCA Identity Random
Top 0.685 0.686 6.169e-5 0.678 0.465 0.709 0.678 0.906 0.929 0.001 0.884 0.744 0.939
NGT 4.847 4.666 5.624 4.926 5.195 618.140 4.925 51.647 39.096 5.614 36.403 493.625 370.035
Ratio 0.141 0.147 1.100e-5 0.138 0.090 0.001 0.138 0.0176 0.0238 1.344e-4 0.024 0.002 0.003
Table 9: Softmax probability changes in downstream ICD predictions resulting from ablation experiments. ’Top’
represents the magnitude of softmax drops for the most probable ground truth ICD code, while ’NGT’ signifies
the sum of absolute softmax probability changes of non-ground truth ICD codes for each clinical note. The ’Ratio’
indicates the ratio between these two measures. We bold and distinguish the results obtained using our combined
LAAT and dictionary learning framework, and observe that our method has the most precise effect on downstream
ICD predictions, suggesting improved explanatory power.
Hidden Medical Meaning Identification Accuracy
AutoCodeDL Baselines
L1 SPINE ICA PCA Identity Random
0.794 0.864 0.351 0.388 0.327 0.297
Table 10: Proportion of stop word embedding labels
correctly identified by our AutoCodeDL framework us-
ing previous sparse autoencoders with using 99th per-
centile activated dictionary features, alongside the base-
line methods. Such results showcase that DL is capable
of effectively identifying hidden meanings embedded
within superposition of stop words.
Experiment (C) DL (D) Baselines
Measure L1 SPINE ICA PCA Identity Random Tokens
Top 0.506 0.506 0.0519 0.205 0.068 0.743 0.0519
NGT 5.146 5.112 5.294 5.164 5.281 11.876 5.296
Ratio 0.098 0.099 0.0098 0.040 0.013 0.063 0.001
Table 11: Softmax probability changes in downstream
ICD predictions resulting from ablation experiments il-
lustrated in Figure 15. ’Top’ represents the magnitude
of softmax drops for the most probable ground truth
ICD code, while ’NGT’ signifies the sum of absolute
softmax probability changes of non-ground truth ICD
codes for each clinical note. The ’Ratio’ indicates the
ratio between these two measures. We highlight and dis-
tinguish the results obtained using sparse autoencoders.
vised dictionary learning methods, represented by
the DL columns in the table, decreased as the top k
tokens considered increased. Contrary to expecta-
tions outlined by (Subramanian et al., 2017), sparse
autoencoders trained through L1minimization ex-
hibited the highest coherence among all methods,
as indicated by the highest average cosine simi-
larity values across different values of k. This
suggests that the highest activating dictionary fea-
tures learned through L1minimization are more
semantically consistent and conceptually coherent
compared to other unsupervised methods.
Surprisingly, the coherence of dictionary fea-tures learned from the top independent components
in ICA surpassed that of SPINE, despite ICA’s lack
of impact on downstream ICD coding performance.
This unexpected finding highlights the complex
interplay between feature extraction methods and
semantic coherence.
As expected, the supervised LAAT method
achieved the highest coherence, reflecting its use
of labeled data to guide the learning process that
results in more tokens closely aligned with the se-
mantics of ICD codes.
Coherence of Top 500 Activated Dictionary Features
DL Baselines Supervised
k L1 SPINE ICA PCA Identity Random LAAT
2 0.344 0.283 0.312 0.244 0.269 0.271 0.692
4 0.331 0.278 0.292 0.234 0.242 0.251 0.678
10 0.303 0.260 0.280 0.228 0.234 0.238 0.637
Table 12: Average cosine similarity between the top k
tokens extracted from each of the top 500 dictionary
feature or ICD code, measured from Siamese encoder
embeddings. Higher values indicate a stronger the-
matic connection within the feature or code. The "DL"
columns represents our dictionaries constructed, while
the remaining columns are baselines.
Dictionary ICD Overlap. The descriptions of
a dictionary feature’s most pertinent ICD codes
should logically coincide with its most highly acti-
vating tokens. Each ICD code comes with a descrip-
tion enriched with medically relevant information,
suggesting that a dictionary feature that accurately
encapsulates a medical concept will exhibit an over-
lap between the descriptions of its relevant ICD
codes and its activating tokens.
Setup. To assess this, we extract descriptions
for each dictionary feature’s relevant ICD codes,
removing stop words, to compile a list of medi-
cally significant tokens. We calculate the overlap
between these tokens and each medically relevantfeature’s top activating tokens, using the proportion
of overlapping tokens as an additional measure of
a dictionary feature’s understandability. We define
a feature to be medically relevant when its ablation
results in at least a 10% softmax probability drop
of any ICD code.
Results. Comparing our dictionary features’ top
tokens to those identified by specialized "label at-
tention" (trained to map tokens to ICD codes), we
find significant overlap for L1 and SPINE (Table
14). This overlap surpasses baselines, suggesting
our unsupervised methods effectively learn med-
ically relevant concepts. Notably, ICA features
show minimal overlap. Overall, our dictionaries
exhibit strong agreement with medically significant
concepts, boosting our model’s interpretability.
Percentage of Dictionary Features GPT3.5 Differentiated
DL Baselines Supervised
L1 SPINE ICA PCA Identity Random LAAT
0.352 0.420 0.416 0.320 0.356 0.328 0.540
Table 13: Percentage of the 500 randomly sampled dic-
tionary features successfully distinguished by GPT3.5
Turbo, determined by selecting the unrelated token from
a set of four tokens.
Dictionary Overlap with ICD Descriptions
DL Baselines Supervised
L1 SPINE ICA* PCA Identity Random LAAT
0.117 0.086 0.000 0.010 0.024 0.003 0.198
Table 14: Proportion of highly activating tokens in dic-
tionary features that overlap with ICD9 descriptions.
LAAT represents an upper bound in overlap where we
treat each ICD row in the label attention matrix as its
own dictionary feature. *In general, the ablation of the
independent components in ICA have little effect on
downstream ICD predictions, hence its features have no
overlap with ICD descriptions in this experiment.
A.13 Initial Examples of Dictionary Features
We manually inspect several highly interpretable
dictionary features as a showcase of the potential
of these interpretable representations. We show
some examples in Figure 16 and 17, relating to
depression, cesarian sections, and failure of wound
healing.Figure 16: Examples of dictionary features. We showcase the most relevant tokens for dictionary feature about
cesarian sections and depression as well as their clinical note contexts. We note that these sorted pandas dataframes
are from an earlier sparse autoencoder in the previous revision of the paper. However, we felt that they were still
worth showcasing.Figure 17: Examples of dictionary features. We showcase the most relevant tokens and ICD codes for each respective
interpretable dictionary feature. We note that these sorted pandas dataframes are from an earlier sparse autoencoder
trained in the previous revision of the paper. However, we felt that they were still worth showcasing to prove a point.
They contain various medical codes that are directly related to each token.