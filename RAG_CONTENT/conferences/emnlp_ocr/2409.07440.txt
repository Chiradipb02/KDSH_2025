SUPER : Evaluating Agents on Setting Up and Executing Tasks
from Research Repositories
Ben Bogin1,2Kejuan Yang2Shashank Gupta1Kyle Richardson1Erin Bransom1
Peter Clark1Ashish Sabharwal1Tushar Khot1
1Allen Institute for AI2University of Washington
{benb, shashankg, kyler, erinbransom, peterc, ashishs, tushark}@allenai.org
Abstract
Given that Large Language Models (LLMs)
have made significant progress in writing code,
can they now be used to autonomously repro-
duce results from research repositories? Such
a capability would be a boon to the research
community, helping researchers validate, un-
derstand, and extend prior work. To advance
towards this goal, we introduce SUPER , the
first benchmark designed to evaluate the ca-
pability of LLMs in setting up and execut-
ing tasks from research repositories. SUPER
aims to capture the realistic challenges faced
by researchers working with Machine Learn-
ing (ML) and Natural Language Processing
(NLP) research repositories. Our benchmark
comprises three distinct problem sets: 45 end-
to-end problems with annotated expert solu-
tions, 152 sub-problems derived from the ex-
pert set that focus on specific challenges (e.g.,
configuring a trainer), and 604 automatically
generated problems for larger-scale develop-
ment. We introduce various evaluation mea-
sures to assess both task success and progress,
utilizing gold solutions when available or ap-
proximations otherwise. We show that state-
of-the-art approaches struggle to solve these
problems with the best model (GPT-4o) solving
only 16.3% of the end-to-end set, and 46.1% of
the scenarios. This illustrates the challenge of
this task, and suggests that SUPER can serve as
a valuable resource for the community to make
and measure progress.1
1 Introduction
Research and scientific discoveries often rely on
the reproducibility of experiments conducted by
other researchers and the ease with which scien-
tists can build upon each other‚Äôs work. In the con-
text of empirical ML and NLP research, it is of-
ten crucial for researchers to be able to execute
and reproduce diverse research experiments from
1https://github.com/allenai/super-benchmark
...‚ÄúMEZE is a memory-efficient zeroth-order optimizer. Train with this optimizer on my dataset (available at https://drive...), following the paper‚Äôs code at: https://github.com/... and report accuracy. ‚Äù
üßëüî¨
üí≠ I will now execute the experiment with the `run.py` script.!python run.py output_dir result/output ...Traceback (most recent call last): ...ImportError: cannot import name 'default_hp_search_backend'
üí≠ Error suggests a `transformers` version issue. I will downgrade it to resolve the error.!pip install transformers==4.28.1  # downgrading dependencySuccessfully installed tokenizers-0.13.3 transformers-4.28.1
üí≠ I will now retry running the experiment.!python run.py output_dir result/output ...18:50:24,975 - INFO - ***** Running training *****...18:51:12,059 - INFO - {'dev_accuracy': 0.27}...The training process has completed successfully. The accuracy result on the development set is: 0.27
üí≠ I need to install the required dependencies listed in therequirements.txtfile!pip install -r requirements.txtSuccessfully installed datasets-2.19.2 transformers-4.40.2 ...
üí≠ Next, I will edit the `data_utils.py` file to configure the loaded dataset.edit data_utils.py <BEFORE>        return datasets.load_dataset('glue', 'mrpc', split=split)<AFTER>        return datasets.load_dataset('csv', data_files={...File edited successfully.
...
Downloading to: /content/dpt/my_data.csvResolving IssuesExecuting & Reporting metrics
üí≠ I will now download the provided data file.!gdown.download("https://drive.google.com../", "my_data.csv")
Setting up the experimentFigure 1: An illustration of a research task and some of the
steps an agent would need to complete it, including updating
data loading configuration, resolving dependency issues (due
to unlisted version dependencies in the repository), running
the training script and reporting metrics.
open-source repositories, whether to verify existing
results or to test them under new conditions.
In practice, even when research code is avail-
able, running code from arbitrary repositories is
often non-trivial and time-consuming (Samuel and
Mietchen, 2022; Storks et al., 2023). Experimen-
tation frequently requires substantial effort to set
up and execute them: installing the environment,
making non-trivial configuration changes, resolv-
ing outdated package dependencies, fixing bugs,
and determining the correct execution commands,
among other tasks. All of this requires a consider-
able understanding of the documentation and repos-
itory code, knowledge about fixing issues (e.g.,
CUDA errors), as well as the ability to modify the
code appropriately. These steps are especially time-
1arXiv:2409.07440v1  [cs.AI]  11 Sep 2024consuming for research repositories ‚Äúin-the-wild‚Äù,
as support and documentation may not be available.
In this work, we ask: Can LLMs automate the
set up and execution of tasks in research reposito-
ries? Consider the research task in Fig. 1 where
the agent is asked to use a research code repository
to train a model with a new optimizer, and evaluate
its performance on a custom dataset. A successful
agent would need to set up the experiment by in-
stalling dependencies, downloading the provided
data, and making code changes to load it (first three
cells in the figure), then execute the training script
while responding to unexpected issues such as an
incompatible dependency (fourth and fifth cell),
and finally report the result metrics (last cell).
While LLM-based agents have recently been
used to produce execution commands from popular
research repositories (Liu et al., 2023b), execute
popular ML repositories (Huang et al., 2024), or re-
solve repository issues (Jimenez et al., 2024), no ex-
isting benchmark evaluates agents on the common
problem faced by many researchers: both setting up
andexecuting experiments using research reposito-
riesin-the-wild , i.e., less popular repositories that
are not typically well-documented or maintained,
which make experiments harder to configure and
execute. As a recent study shows (Storks et al.,
2023), both novice and advanced researchers find
the challenge of ‚Äúsetting up the code base‚Äù to be
the most difficult part of reproducing experiments.
To encourage research on this problem, we in-
troduce SUPER (Setting UPandExecuting tasks
from Research repositories), a benchmark focusing
on such lower-profile research repositories. SUPER
consists of three distinct problem sets. The Ex-
pert set contains 45 manually curated problems
solved by experts. The Masked set includes 152
sub-problems derived from the expert set through
our proposed ‚ÄúCode Masking‚Äù mechanism, where
we remove parts of the expert-written code to gener-
ate a diverse set of sub-problems targeting specific
challenges. Each sub-problem addresses a specific
challenge, such as installing dependencies and re-
solving conflicts, configuring experimental data,
setting up hyper-parameters, resolving runtime ex-
ceptions, correctly executing scripts, etc. Lastly,
theAuto set contains an additional 604 automati-
cally generated tasks with an even more diverse set
of repositories and challenges. It can potentially be
used in future work for development, fine-tuning,
or training using environment feedback.
To evaluate agents on the Expert and Maskedsets, for which we have gold solutions, we com-
pare their answers (e.g., metrics to be reported) to
the gold solutions. To allow for partial credit, we
also measure the progress of the agents by check-
ing if they reach specific ‚Äòlandmark‚Äô states in their
solutions, such as completing a training stage. For
the automatically generated problems, for which
we have no gold solutions, we simply check if a
key script (e.g., the training or evaluation script)
was run successfully without exceptions, which we
found to be an effective approximate metric.
We evaluate both proprietary and open-source
LLMs on SUPER as the underlying models of strong
baseline agents with access to file-editing tools. We
find that agents struggle to correctly solve many
of the problems, with the strongest agent solving
only 46.1% of the Masked sub-problems. These
agents are even further away from solving entire
research tasks, completing correctly only 16.3% of
the end-to-end Expert tasks. Open-source models
substantially lag behind on both the sub-problems
and end-to-end tasks. Moreover, we find that the
ranking of the agents and models on the Auto set
is mostly the same as it is on the curated sets, sug-
gesting its potential usefulness for development.
Our analysis of model trajectories reveals that
agents are better at resolving well-specified sub-
problems, such as solving exceptions, bugs, and
other issues, than tasks requiring repository and file
exploration to understand code structure. These re-
sults underscore many of the core challenges facing
LLM-based experiment execution systems, which
our benchmark aims to help advance.
2 Related Work
Coding benchmarks: While early code bench-
marks (Chen et al., 2021; Austin et al., 2021; Cas-
sano et al., 2022) mainly focused on synthesizing
simple functions from descriptions, recent bench-
marks have shifted to more complex competitive
programming problems (Li et al., 2022; Hendrycks
et al., 2021; Jain et al., 2024) and evaluating pro-
ficiency with popular data science libraries (Lai
et al., 2023). Unlike these, we follow the recent
trend on evaluating LLMs in more natural program-
ming scenarios, such as programming with external
tools and APIs (Li et al., 2023; Shen et al., 2023;
Wang et al., 2023b; Patil et al., 2023), code editing
and debugging (Cassano et al., 2023; Tian et al.,
2024; Haque et al., 2023), resolving GitHub issues
(Jimenez et al., 2024) and understanding and cod-
2ResourceSUPER
(this work)DS-1000
(Lai et al., 2023)ML-Bench (Agent)
(Liu et al., 2023b)MLAgentBench
(Huang et al., 2024)SWE-bench
(Jimenez et al., 2024)
Repo. understanding ‚úì ‚úó ‚úì ‚úó ‚úì
Requires repository setup ‚úì ‚úó ‚úì ‚úó ‚úó
Outcome-based evaluation ‚úì ‚úì ‚úó ‚úì ‚úì
Low-profile repositories
,‚ÜíMedian stars‚úì
14/14/23‚úó
35,309‚úó
9,632-
-‚úó
12,557
# source repositories 45/45/604 8 18 - 12
# problems 45/152/604 1000 9641 15 2300
Table 1: Comparison of SUPER against four other related code execution benchmarks in terms of the challenges being tested
(rows 1-5) and the number of source repositories and problems in the dataset (row 6-7). For SUPER , number of repositories and
problems refer to the Expert/Masked/Auto sets respectively. Repository understanding refers to the agent being required to go
through repository files to complete a task. Repository setup refers to the requirement to install dependencies and environment.
Outcome-based evaluation involves assessing performance based on unit-tests or comparing outcome results, such as metrics, to
gold ones. Low-profile repositories refer to repositories with low number of GitHub stars.
ing within a repository context (Liu et al., 2023a;
Ding et al., 2024; Zhang et al., 2023).
In contrast to these works, SUPER focuses on
the end-to-end task of setting up and executing
research tasks in lower-profile repositories, pre-
senting a unique set of challenges, with tasks that
require repository comprehension and reasoning,
editing multiple files, setting up the repository en-
vironment for execution while interactively run-
ning commands in the environment. Table 1 com-
pares the four datasets most relevant to SUPER . ML-
Bench (Liu et al., 2023b), specifically, its ML-
Agent-Bench setup, evaluates LLMs‚Äô ability to exe-
cute tasks but focuses on popular code repositories
rather than low profile, and does not evaluate based
on correctness of outcome, i.e., whether resulting
metrics are correct. MLAgentBench (Huang et al.,
2024) evaluates agents ability to run ML experi-
ments but focuses on optimizing single-script ML
experiments rather than comprehending and setting
up arbitrary repositories for experimentation.
LLM Agents: Recent advancements in LLM-
based agents have shown significant progress
across various domains, including games (Wang
et al., 2023a), web navigation (Yao et al., 2022;
Zhou et al., 2023), human interaction simula-
tion (Park et al., 2023), automating complex com-
puter tasks (Xie et al., 2024), data science and ma-
chine learning (Guo et al., 2024; Hong et al., 2024;
Liu et al., 2024; Yang et al., 2024b), open-ended
discovery (Jansen et al., 2024), and coding (Wang
et al., 2024; Yang et al., 2024a; OpenDevin Team,
2024). Our benchmark introduces an important
new domain that encourages the development of
LLM-based agents to assist researchers in their end
to end research tasks with arbitrary repositories.
1. Write tasks from research repositories2. Collect expert human solutions3. Extract Masked Code Scenarios‚Üí
Figure 2: An overview of the construction pipeline for the Ex-
pert and Masked sets. The Expert set contains manually writ-
ten tasks, along with expert solutions (Step 2). The Masked
set contains problems extracted from the experts set (Step 3).
3 Benchmark Construction
In this section we describe the process of build-
ing the SUPER benchmark. The SUPER benchmark
consists of 3 sets (see Table 2) serving different
purposes. The Expert set (¬ß3.1) contains manually
written problems, solved by experts. The Masked
set (¬ß3.2) contains sub-problems extracted from
the Expert set using the gold solution, which pro-
vide easier and more focused sub-problems. Fig. 2
provides a high-level overview of the construction
pipeline of these two sets. Finally, the Auto set
(¬ß3.3) contains automatically generated problems
which can be used for development and improve-
ment of agents.
Environment Setup. Running research-oriented
repositories often necessitates both being able to
run system shell commands (e.g. to install depen-
dencies and run scripts) and stateful Python com-
mands. Previous work and environments typically
support only one of these (e.g., only system shell
commands (Jimenez et al., 2024) or only Python
commands (Huang et al., 2024)). Instead, we build
an environment that allows running both of these
commands with a Jupyter notebook as engine. In
this setup, each execution code is equivalent to run-
ning a notebook cell, which contains Python code
and/or bash commands, and where state is reserved
between cell executions (e.g., each cell can use any
of the previously defined Python variables). The
execution of each cell returns an observation string.
33.1 Expert Set
We construct the Expert set by (1) identifying a
set of relevant code repositories from research pa-
pers and manually writing research-oriented tasks
based on them and (2) asking human experts to pro-
vide end-to-end solutions for these tasks (¬ß3.1.1).
We then use the expert solutions as the basis for
outcome-based evaluation, where we compare the
agent‚Äôs answer to the gold answer, and a more
lenient landmark-based evaluation that indicates
progress toward correctly solving the task, even if
the solution is not entirely correct (¬ß3.1.2).
3.1.1 Construction
Tasks. We create tasks motivated by the follow-
ing two common settings: (1) reproducing numbers
from research papers by running specific experi-
ments, and (2) running modified experiments with
different datasets, models, or configurations.
We start by collecting repositories from the ‚ÄúPa-
pers With Code‚Äù ( github.com/paperswithcode/
paperswithcode-data ) database, which contains
research papers linked to their GitHub repositories,
along with some additional metadata such as the
modality of the datasets used. We only sample
research papers with ‚ÄúText‚Äù modalities and select
repositories from 2021 or beyond.
We then manually review the sampled reposito-
ries and write tasks that involve running a single
experiment that is mentioned either in the repos-
itory‚Äôs ‚Äúreadme‚Äù file or under a script available
in the repository, if such can be found. When-
ever possible, we make the task more challenging
by requiring the experiment to be run on a new
dataset or model, other than the one described
in the available documentation. In these cases,
we select either datasets available on Hugging-
Face Hub ( https://huggingface.co/datasets )
or provide a Google Drive link where the dataset
can be found. The challenge of running on a
specific dataset varies in difficulty: it could in-
volve only a single configuration line change if
the dataset is already supported, or creating a new
dataset reader, adjusting column names, etc.
For each task, we define (1) the target Github
repository, (2) the task definition (e.g., ‚Äútrain a
model...‚Äù), (3) the metrics or output to be reported
(e.g., ‚ÄúF1 metric on the validation set‚Äù), along
with a specific structure of how the answer should
be formatted, and (4) implementation instructions
(e.g. specific hyper-parameters). The implemen-
tation instructions are important for two reasons:Set # Solutions Evaluation Purpose
Expert 45 ‚úì Solution-based Benchmark
,‚ÜíMasked 152 ‚úì Solution-based Benchmark, analysis
Auto 604 ‚úó Proxy Development
Table 2: The different sets of SUPER .
first, to allow fair evaluation of submitted answers
by making sure the task is not under-specified, such
that two agents that correctly complete the task get
the same results. Second, to minimize computa-
tional requirements, as described next.
Minimizing Computational Requirements. To
make SUPER faster and cheaper to run, we ensure
tasks are executable without reliance on GPU ma-
chines and that they do not require more than 10
minutes of compute (e.g., for training models or in-
stalling packages) on basic compute instances (see
¬ß4 for compute details). We therefore create tasks
that require minimal compute by only asking to
train and evaluate small models (e.g., gpt2-small ),
and by adding implementation instructions to the
task, such as ‚Äúonly load the first 10 examples of the
dataset‚Äù or ‚Äúrun a single epoch‚Äù.
Note that these restrictions do not make the task
any easier for the agent. In fact, they often add
additional challenges that agents need to solve (e.g.,
configuring hyper-parameters, finding where data
is loaded to limit its loading to the first 10 samples,
being able to run experiments that were designed
for GPU on a CPU, etc.).
Expert Annotation. We use Upwork ( https:
//www.upwork.com/ ) to find and hire experts that
have experience with running ML and NLP ex-
periments. We filter the initial list of applications
by testing them on a pilot task which we solved
ourselves, to make sure they are able to correctly
execute a task and effectively solve issues by com-
paring their solution and results to ours. We instruct
workers to execute their solutions on Google Colab,
allowing us to collect the solutions in a consistent
notebook-like environment.
We ask the experts to submit their (1) solution
notebook, (2) answers (i.e. metrics to be reported),
the specific git commit hash of the repository that
they have used, and the final version list of all
dependencies that were installed throughout the
notebook.2In addition, we instruct them to use
default parameters whenever possible, and to report
2The git hash and dependencies ensure that these solutions
can be reproduced in the future even as repository and package
versions change.
4any decision that they had to make but was not
specified in the original task (e.g., the selection
of a delimiter token or specific hyper-parameters
when no default values are provided). We add these
decisions to the task description or implementation
instructions to ensure that any agent solving the
same task would have all the necessary information
needed to get the same results.
Finally, we manually review their solutions, mak-
ing sure that (1) the solution correctly follows the
task, (2) it can be executed in our environment, (3)
all unspecified decisions have been recorded and
(4) re-running the experiment multiple times yields
the same results (up to an error of 10‚àí2). If needed,
we ask the workers to make corrections, or manu-
ally fix issues ourselves. Solutions that we could
not execute on our Jupyter environment, such as
solutions that had to modify the installed Python
version were discarded. We provide cost details
and guidelines in Appendix D.
3.1.2 Evaluation
Accuracy Evaluation. As described in ¬ß3.1.1,
experts provide us a deterministic solution for each
task, which we then execute in our environment to
get the gold answer, allowing us to evaluate agents
based on their outcome. Answers consist of several
values (e.g., numbers for metrics, string for model
predictions). We define the accuracy metric as the
portion of correctly answered values: where the
predicted answer precisely matches the gold one
(up to a 10‚àí2error). Unlike reference based evalu-
ation used in cloze tests and various prior coding
benchmarks (Liu et al., 2023a,b), outcome-based
evaluation allows for alternate valid solutions.
Landmark-Based Evaluation. Sometimes an in-
dication of whether the model was precisely correct
may be too strict, ‚Äúpunishing‚Äù models that make
progress but don‚Äôt reach the end. E.g., an agent
that loads the data but doesn‚Äôt train would have the
same accuracy as an agent that fails at the start.
To measure progress towards the final goal, we
use the gold task notebooks to identify landmark
outputs ; outputs from the environments that act
as ‚Äúevidence‚Äù that a particular step was run suc-
cessfully. E.g., the explicit output string ‚Äú ***
training completed *** ‚Äù in Figure 1 or the
string ‚Äú Loading data... 100% ‚Äù implying suc-
cessful data loading.
Importantly, a perfect landmark score does not
entail a perfect accuracy score, as landmarks only
Figure 3: An abstract demonstration of how sub-problems
are extracted: starting from a gold end-to-end task solution
(left), we remove cells (middle) that focus on certain aspects
(differently colored cells in the figure), then create a masked
problem by defining a goal and prefix cells (right). The prefix
cells are executed in the environment, and the agent must then
write code to solve the sub-problem.
indicate that some action was performed, but it
was not necessarily correct (e.g., a training script
run successfully but with wrong hyper-parameters
could be counted as success). Similarly, albeit un-
likely by design, a model could correctly solve the
task but not hit all of the landmarks (e.g., if it uses
an alternate approach or guesses a solution) and
have a lower landmark score. For each gold so-
lution we manually extract 2-6 landmark outputs
patterns. The landmarks metric evaluates the per-
centage of these patterns that appear in the outputs
of any of the cells executed by the agent.
3.2 Masked Coding sub-problems Extraction
Solving an end-to-end execution task can often
be long and complex, consisting of multiple non-
trivial steps. As such, evaluating agents on their
ability to run an entire task provides a sparse signal
of success, where agents have to complete numer-
ous steps correctly to succeed, making it harder
to ‚Äúhill-climb‚Äù results. Instead, we want to eval-
uate models in a more fine-grained way that will
allow us to get success signals for any incremental
progress towards the task. To this end, we propose
to focus on a specific sub-problem from the task
solution at a time, leveraging the expert solutions
from the expert set.
We turn to an approach loosely inspired by cloze
tests (Taylor, 1953) and masked language models
(MLM; Devlin et al., 2019): given a gold solution
for a task we remove ( mask ) some part of it (e.g.,
code that solves a dependency installation issue),
and manually define the sub-problem such that an
agent only needs to solve this narrower aspect of
the overall task, as captured by the removed cells.
While the ideal goal is for the agents to com-
plete tasks in an end-to-end manner, extracting
5masked sub-problems allows us to evaluate agents
on a broad range of technical challenges often en-
countered when working with research repositories,
while performing a finer-grained analysis of their
performance. In addition, this setup aligns well
with the usage of interactive code assistants (e.g.,
CoPilot and Colab‚Äôs AI-powered coding), where
agents assist users that have already written or even
executed some code, and can specify what issue or
problem remains to be solved.
Masked Coding Sub-Problems. Each masked
sub-problem consists of (1) a textual description
of the remaining tasks to execute or issues to be
resolved (e.g., ‚Äúfix the runtime error to complete
training‚Äù for a sub-problem where cells that fix a
runtime error were removed), and (2) code prefix
(e.g., the other cells from the original notebook that
were not masked). Given a masked sub-problem,
the code prefix is pre-executed in the environment,
and agents must then execute their own cells to
solve the sub-problem.
Extraction Procedure. We extract masked cod-
ing sub-problems in a manual process where we
first identify certain cells (not necessarily consec-
utive) in the gold solution that focus on a certain
aspect. For example, the cells corresponding to
loading and modifying the dataset could be com-
bined into a data configuration block as shown in
pink in Fig. 3. We extract sub-problems by first
masking a block ( Issue solving orange block in the
figure). We then identify cells that do not depend
on the masked block and define a goal that is re-
maining to be completed (e.g., making training run
in our example). These cells are pre-executed and
the agent‚Äôs task is to complete the goal.
We use the masked block and goal cell to define
the sub-problem, e.g., if the code to handle execu-
tion on CPU has been masked, the sub-problem def-
inition would be ‚Äú I have already executed some of
the required steps. Now, you should make the neces-
sary changes to make sure the code runs on a CPU.
Your goal is to successfully run ‚Äòtrain.py‚Äô. ‚Äù. We
choose cells with clearly identifiable success indi-
cators as goals, e.g., successfully running ‚Äòtrain.py‚Äô
would produce metrics on completion.
Evaluation. Since each sub-problem has a
clearly defined goal, extracted from the original
task, we can use the same outputs and landmarks
as in the expert set, and similarly evaluate accuracy
and landmarks (¬ß3.1.2). We evaluate sub-problemswith the same metrics defined in ¬ß3.1.2.
3.3 Automatically Generated Tasks
The Expert and Masked sets provide validated prob-
lems and reproducible solutions, allowing for more
accurate evaluation of agents. However, creating
expert tasks is both time-consuming and costly, and
the limited number of tasks hinders their use for
agent improvements, such as fine-tuning models
based on trajectories and environment feedback
(e.g., Chen et al., 2023; Song et al., 2024; Yin et al.,
2024). To address this, we automatically generate
tasks using an LLM (namely, GPT-4o).
3.3.1 Construction
Generation involves two steps: (1) filtering suitable
repositories, and (2) generating tasks based on the
readmes of these repositories.
Filtering. We start with the same list of reposito-
ries from ¬ß3.1.1, selecting those listed in 2021 or
later, resulting in a total of 5915 repositories. Many
of these repositories cannot be trivially used to gen-
erate tasks due to various limitations: some do not
support running any experiment, some have miss-
ing dataset links, some require GPU hardware, and
some depend on APIs of other LLMs (such as Ope-
nAI or Anthropic), which could be unavailable or
incur costs for users. To alleviate these issues, we
employ a combination of heuristic and LLM-based
filtering methods. Specifically, we keep reposito-
ries that mention specific datasets and models, do
not use APIs, and do not require GPUs. Detailed
information on the filters and the prompt used for
LLM-based filtering can be found in Appendix B.
Creating Tasks. Given the filtered repositories,
we prompt the LLM with the contents of each
repository‚Äôs README file and instruct it to create
an experiment-running task. This includes defining
the goal, dataset, model, and script to be run ( ‚ÄúRun
probability-based prompt selection on the SST-2
dataset using opt-125m as the base model with the
script ‚Äòrun_prompt_selection.py‚Äò‚Äù ). We also spec-
ify that the LLM should choose the smallest model
within model families (e.g., BERT-base if BERT
models are supported). See Appendix B for fur-
ther details on the generation process. To verify
the quality of the generated set, we sample 100
generated tasks and find that 81% of the samples
are feasible; among the rest, the most prominent
issue was missing resources (dead or gated links
to datasets, missing code) and conceptually flawed
6Category (%) Portion Gold LOC Example(s) description of a gold solution
Dependencies 19.7% 4.1 Downgrade ‚Äòtransformers‚Äò version to allow execution of an older repository
CPU 7.2% 5.1 Remove ‚Äò .cuda() ‚Äò from different locations in code
Configuration 12.5% 8.2 Edit Python or shell scripts to set hyper-parameters and experiment details
Data 23.7% 22.7 Download custom dataset, update data loader, limit to loading first 10 samples
Issue 9.2% 5.8 Pytorch incompatible tensor shapes; incorrectly loaded Python package
Goal 25.0% 6.5 Run the evaluation script then load generated file to report metrics
Other 2.6% 3.8 Save the model after training to allow evaluation loading
Table 3: Distribution of sub problems categories and description of representative solutions from experts. LOC
stands for lines of code, counting the number of lines (excluding comments) in the gold solution.
tasks such as asking to use a discriminative model
for a generative task.
Difference from Expert Set. Importantly, the
Auto tasks exhibit different properties than those
of the Expert set: problems in the Expert set can
require training or inference on datasets not specif-
ically mentioned to be supported, in some cases
with different formats and columns, whereas Auto
tasks focus more on getting the dependencies in-
stalled and being able to start an experiment. More-
over, the Expert set only includes problems where
we were able to run the solutions in our Jupyter
environment, while problems in the Auto set could
potentially involve even more challenging environ-
ments setups we avoided, such as when changing
a Python version is required. Finally, Auto tasks
sometimes require navigating through web pages
that were mentioned in the repository‚Äôs Readme
file to download datasets.
3.3.2 Evaluation
Without expert solutions, we cannot evaluate based
on outcomes or landmarks. Instead, we use a sim-
ple heuristic metric, termed Script-Executed , to en-
sure the model sets up and executes the experiment
without unresolved issues: we check if the script
the agent was asked to run executes without excep-
tions for a minimum duration (see Appendix B.2
for details). The minimum duration ensures that
the script was successful and didn‚Äôt just fail silently.
While this method does not guarantee perfect eval-
uation, we find it surprisingly effective, as we show
in our analysis in ¬ß4.3.
3.4 SUPER Benchmark
The Expert set consists of 45 collected tasks, where
each problem is paired with a gold output for
outcome-based evaluation and a small set of land-
marks for our softer evaluation metric (an average
of 3 landmarks per problem).
To roughly estimate how much lines of code(LOC) are needed to solve a task, we count the num-
ber of lines (excluding comments) in the gold solu-
tion, and for editing cells, the number of changed
lines.3An average of 44.3 LOC and 14.4 cells per
solution suggest that these tasks are particularly
challenging due to potentially long agent trajecto-
ries. Consequently, the ability to solve these tasks
provides an important signal on the performance
of agents in handling long and complex tasks.
The Masked set contains 152 masked coding sub-
problems derived from the 45 expert tasks. Like the
Expert set, each sub-problem is paired with a gold
output and landmarks. Table 3 shows the distribu-
tion of the extracted sub-problems across various
categories, along with a representative solution for
each category and the average lines of code (LOC)
that were executed in the gold solution. Finally,
our automated set includes 604 problems, all from
unique repositories.
To verify that the repositories used in our bench-
marks are indeed ‚Äòlow-profile‚Äô, we count the num-
ber of GitHub stars in the source repositories as
a proxy for popularity as shown in Table 1. Intu-
itively, popularity loosely correlates with the qual-
ity of documentation and readiness, which affects
the difficulty of experiments execution. We see
that the median number of stars for our repositories
(14) is considerably lower than other comparable
datasets (see Appendix A for details).
4 Experiments
Experimental Setup. We limit the execution
time (i.e. time to run commands, not counting
the API reply time) of each problem to 30 minutes.
We run all tasks on compute instances using sand-
boxed execution in Modal ( https://modal.com ),
3The gold LOC can be dramatically lower than those
needed by the agent; experts did not have to write code to
read contents of files in the repository as they can use their
IDE or browser, while agents have to browse files through
an explicit command. In addition, experts did not necessarily
keep cells with failed attempts.
7which allows us to evaluate problems safely and
concurrently, speeding up evaluations. We limit
the total input tokens number (summing all steps)
of each sub-problem to 400k, and of expert and
auto-generated tasks to 600k. The compute cost
for executing each problem in Modal (not counting
API usage) is 2-3 cents, making it negligible com-
pared to API costs. If time or token limit occurs
before the agent submits the answer, the task is con-
cluded without submission (in such cases, agents
get 0 accuracy, but are still scored based on the
landmark evaluation). We validate that running the
gold trajectories yields perfect scores on all of our
metrics in three consequent attempts. To estimate
variance in our results, we average results over 3
seeds for the expert set tasks with the strongest
underlying LLM (to limit costs).
Underlying LLMs. We experiment with
agents based on commerical LLMs GPT-
4o ( gpt-4o-2024-08-06 ) and GPT-4o
mini ( gpt-4o-mini-2024-07-18 ) (OpenAI,
2023), as well as the open-source models
Mixtral-8x22B-Instruct (Jiang et al., 2024)
and Llama 3.1 70B (Dubey et al., 2024)
(Meta-Llama-3.1-70B-Instruct-Turbo ), both
served by https://www.together.ai/ .
4.1 Baselines
In this section, we describe the three baseline
agents that we evaluate on SUPER : ReAct (Yao et al.,
2023), our improved version ReAct- SUPER , and
SWE-Agent (Yang et al., 2024a). All of our agents
are given access to a Jupyter notebook environment
where they can execute Python or Bash commands.
Other than the execute action, they can also sub-
mitan answer when done. For sub-problems, we
execute the provided ‚Äòpre-execute‚Äô cells (¬ß3.2) and
pass them as an existing history of actions to each
agent. With end-to-end tasks (Expert and Auto), we
simply prompt the agent with the task description.
ReAct (Yao et al., 2023) is a baseline agent
that iteratively prompts the underlying LLM to
output both an action and a natural language
‚Äúthought‚Äù, providing the interaction history as con-
text. Each step, the generated action is executed
against the environment and a <thought ,action ,
observation >tuple is added to the history, until
the agent submits an answer, or exceeds token or
compute limitations.
One challenge associated with running experi-
ments is that output observations can get extremelyAgent Model Acc. Landm.
SWE-Agent GPT-4o 16.3 ¬±2.1 36.8 ¬±2.3
React GPT-4o 12.2 ¬±1.0 33.6 ¬±0.9
React-Super GPT-4o 14.4 ¬±2.2 42.6 ¬±2.9
SWE-Agent GPT-4o mini 3.3 16.1
React-Super GPT-4o mini 5.6 20.6
SWE-Agent Llama 3.1 70B 5.6 4.8
React-Super Llama 3.1 70B 6.1 9.6
SWE-Agent Mixtral 8x22B 1.1 0.0
React-Super Mixtral 8x22B 3.3 3.7
Table 4: Results on Expert, with GPT-4o numbers aver-
aged across 3 seeds.
long (e.g., the output of certain training scripts
and dependency installation reach 10k-40k tokens).
ReAct agents ‚Äúaccumulate‚Äù history information
(thought, action, and observation triplets) at each
step, which makes token usage grow rapidly. As
agents are typically limited to a fixed budget (either
cost or tokens), this could lead to failures. We apply
truncation strategies in all our agents and baselines
to mitigate this issue. See Appendix C for details.
ReAct- SUPER .The ability to execute Python and
bash commands, in theory, allows agents to per-
form any necessary task. However, these actions
are still limited compared to humans who can use
IDEs to browse and edit files. In our early experi-
ments, we indeed found that agents struggle to edit
files (e.g., change configs) using just bash.
To address this challenge, we supplement the
agent with an additional edit action, similar in
spirit to the Agent-Computer Interfaces from SWE-
Agent (Yang et al., 2024a). Specifically, the edit
command accepts three parameters: the name of
the file, the exact content of the lines to be replaced,
and the content to replace it with. We do not ask
the agent to provide line numbers (as needed by
git patches or SWE-Agent), and provide the agent
with suggestions in case the exact content of lines
to be replaced were not found (e.g., if whitespaces
are missing). See App. E for more details.
SWE-Agent (Yang et al., 2024a) is a ReAct-
based agent, originally designed to solve GitHub
issues . Like ReAct- SUPER , this approach provides
agents with tools that allow easier editing of files,
but also tools for reading files, scrolling through
their contents and more (see original paper for de-
tails). We implement this agent in our environment
and modify the prompt to address the execution of
research tasks in our environment.
8Agent Model Script-Executed
SWE-Agent GPT-4o 18.0
React GPT-4o 14.0
React-Super GPT-4o 18.8
SWE-Agent GPT-4o mini 5.2
React GPT-4o mini 16.0
React-Super GPT-4o mini 14.8
Table 5: Results on 250 of the Auto tasks.
Reflecting Agents. To explore whether agents
can improve their performance by reflecting on
their failures, we evaluate agents with a reflec-
tion mechanism (Shinn et al., 2023). Whenever
an agent‚Äôs first attempt to complete a task fails
to submit any answer, we prompt the underlying
LLM to reflect on the trajectory and devise a plan
to avoid the same mistakes in subsequent attempts.
This reflection is then incorporated into the agent‚Äôs
prompt and it tries again to solve the problem. The
agent is given ktries to solve the problem with
each try being given 1/kthof the token budget.
4.2 Results
Expert Set. We show results for the experts set
in Table 4, with the results for the most performant
LLM averaged across three seeds (decoding tem-
perature is 0.2). The low accuracies (12.2-16.3)
suggest that current agents cannot yet perform this
task well. However, in some cases, agents make
some progress towards the goal, as evident by the
landmarks metric, suggesting that agents could still
be helpful in setting up repositories.
Masked Set. We show results in Table 6 on the
Masked set, demonstrating that SWE-agent cor-
rectly solves a significant portion (46.1%) of the
challenges that are required to set-up and execute
experiments from research repositories, but that
most sub-problems are still unsolved. The higher
landmarks evaluation score (74.9%) suggests that
agents often make progress towards solving the
sub-problems, even if some of the steps might not
necessarily be correct.
We find that SWE-agent performs better than
ReAct- SUPER with GPT-4o as the LLM, but slightly
worse with all weaker models, suggesting that
weaker models were less effectively in leveraging
SWE-Agent tools. The open-source Mixtral and
Llama reach significantly lower scores on both the
Masked and Expert sets.
Auto Set. We show in Table 5 results for the
Auto tasks, where ranking of models and agents areAgent Model Acc. Landm.
SWE-Agent GPT-4o 46.1 74.9
React GPT-4o 37.0 65.7
React-Super GPT-4o 41.6 72.5
SWE-Agent GPT-4o mini 27.0 51.8
React-Super GPT-4o mini 31.5 58.3
SWE-Agent Llama 3.1 70B 17.4 35.0
React-Super Llama 3.1 70B 22.8 38.3
SWE-Agent Mixtral 8x22B 9.5 26.6
React-Super Mixtral 8x22B 7.0 13.2
Table 6: Results of our baselines on SUPER (Masked)
with different underlying LLMs.
mostly consistent with the ranking of the models
on the Masked set, suggesting potential usefulness
of this set for future development.
Ablations. Comparing React with ReAct- SUPER
shows that the editing function enables the agent to
hit more landmarks (72.5% vs 65.7%) and produce
more accurate answers (41.6% vs 37.0%). We find
that without the editing command, the agent usu-
ally resorts to editing files with the sedcommand,
which is designed for simple single-line edits.
Can agents that reflect do better? We next eval-
uate if retrying after reflecting on failures can im-
prove the performance of our baseline agents, with
k= 3retries. As shown in Table 7, the additional
retries with reflections have a positive but minimal
impact on the score. If models lack the inherent
ability to resolve some of these issues, retrial with
reflections are not likely to help.
4.3 Error Analysis
The Masked set categorizes each problem, allowing
us to break down performance of agents (Table 3).
We find that the hardest categories for the agent
are data (27%), configuration (38%) and goal (43%
accuracy), whereas CPU, issues and dependencies
are easier (73%, 61% and 54% respectively). These
findings suggest that agents are better at solving
sub-problems where there is a specific error mes-
sage to be solved (such as CPU support errors, in-
compatible dependencies, or exceptions) than more
open-ended problems such as configuring data load-
ing for a custom dataset.
Specifically, for the latter case, we find that
agents commonly skip going through the repos-
itory to understand relevant code. For example,
they often hallucinate arguments of scripts or func-
tions instead of looking up how they should be
called (e.g., adding n_examples=10 when no such
9Agent Acc. Landm.
ReAct- SUPER 41.6 72.5
Reflexion (Shinn et al., 2023) 45.4 76.6
Table 7: Results of the ReAct- SUPER agent (using GPT-4o)
with and without Reflexion on the Masked set. While retrying
with reflection does help improve the submission rate and
accuracy, SUPER benchmark still remains challenging.
argument is defined), or the opposite: they miss a
script parameter and attempt to change them in files
unsuccessfully. Additionally, once they commit to
a particular approach, they never reconsider their
decision until failure. These issues suggest that
agents should be better designed to analyze repos-
itories and consider multiple solution approaches.
We provide all trajectories of ReAct- SUPER and
SWE-Agent in our code repository.4
Effectiveness of proxy metric. While we evalu-
ate the Expert and Masked sets based on solutions
of experts, for the Auto set we have no such so-
lutions, and therefore rely on the weaker Script-
Executed proxy metric (¬ß3.3.2). To verify that this
proxy metric is reliable, we use the trajectories of
ReAct- SUPER on the Masked set to compare the
Script-Executed metric with the accuracy and land-
mark metrics. We find that Script-Executed agrees
with landmark (assuming a score of 1 when land-
mark > 0.5) in 90% of the cases and with the accu-
racy metric in 69% of the cases. We identified two
cases of disagreement with the landmark metric:
(1) the target script ran sufficiently to get the cor-
rect answer, but still encountered an exception at
the end (e.g., an exception creating a figure) which
would be considered incorrect by the proxy metric
(2) the script ran for the minimum time making
it appear like a success based on the proxy metric
only to fail due to a mis-configuration or exceptions
much later (and not reaching the answer).
5 Conclusion
Our work introduces SUPER , a benchmark designed
to evaluate LLM-based agents on executing tasks
from code repositories, focusing specifically on
low-profile research repositories encountered in
the wild. We show empirically that our benchmark
is difficult, even for the current best commercial
LLMs such as GPT4, both on landmark and end-
to-end task evaluations (e.g., GPT-4o solving only
46.1% of the sub-problems). Our benchmark also
4https://github.com/allenai/super-benchmark/
tree/main/trajectorieshighlights many of the core challenges in building
autonomous LLM-based execution agents, such as
repository reasoning and code editing, which we
hope will help the community make measurable
progress on this important problem.
Limitations
Dataset Size. The dataset size of our benchmark,
comprising 45 and 152 sub-problems, is smaller
compared to some other benchmarks available for
agent evaluation, which could potentially affect
the statistical significance of performance evalua-
tions. However, the use of smaller, high-quality
benchmarks is not uncommon. For instance, bench-
marks such as HUMAN EVAL (Chen et al., 2021),
CLASS EVAL (Du et al., 2023), and BAMBOOGLE
(Press et al., 2023) contain 164, 100, and 125 exam-
ples respectively, and are widely used for assessing
model performance. In addition, recent work has
suggested that reducing large datasets to as few as
100 examples does not diminish their effectiveness
(Maia Polo et al., 2024). Moreover, smaller-sized
datasets offer the advantage of being less expen-
sive to operate, thus providing better accessibility
for researchers with limited resources, particularly
when running interactive agents in environments
that generate long outputs. Finally, our provided
Auto set with 604 problems offers problems pur-
posed for development, which alleviates the risk of
overfitting to the evaluation sets.
Programming Languages and Domains. We
have only collected solutions written in Python,
and our environment only supports that program-
ming language. We focus mostly on text-based
repositories. While the challenges associated with
running these repositories likely overlap with other
domains, increasing the diversity of the repository
domains could be beneficial.
Evaluation Based on External Resources. Run-
ning benchmarks in realistic environments often
depend on external resources. In our case, agents
rely on availability of resources such as GitHub, pip
and datasets, which we cannot control across runs.
While completely sand-boxed setups could have
allowed for a more controlled evaluation, we opt
for fidelity, similarly to e.g. benchmarks for web
agents that rely on access to real websites (Mialon
et al., 2024; He et al., 2024, inter alia).5
5Note that all our evaluations are run using the same base
Docker image with sand-boxed code execution and should be
10Ethical Considerations
While autonomous research execution agents could
significantly enhance research advancements, there
is a risk of over-reliance on these agents, which
could lead to conclusions drawn based on incorrect
implementations of agents, and careless actors not
checking the agent‚Äôs reproduction work carefully.
Acknowledgments
We thank Ori Yoran for his valuable comments and
suggestions. We also thank the Upworker expert
programmers for their work on the solutions to the
Expert set problems.
References
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, and
Charles Sutton. 2021. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732 .
Federico Cassano, John Gouwar, Daniel Nguyen, Syd-
ney Nguyen, Luna Phipps-Costin, Donald Pinckney,
Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson,
Molly Q Feldman, et al. 2022. MultiPL-E: A scal-
able and extensible approach to benchmarking neural
code generation. arXiv preprint arXiv:2208.08227 .
Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn,
Abby Brennan-Jones, Anton Lozhkov, Carolyn An-
derson, and Arjun Guha. 2023. Can it edit? eval-
uating the ability of large language models to
follow code editing instructions. arXiv preprint
arXiv:2312.12450 .
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Col-
lier, Karthik Narasimhan, and Shunyu Yao. 2023.
FireAct: Toward language agent fine-tuning. ArXiv ,
abs/2310.05915.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171‚Äì4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
reproducible; barring any external changes.Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian
Ding, Ming Tan, Nihal Jain, Murali Krishna Ra-
manathan, Ramesh Nallapati, Parminder Bhatia, Dan
Roth, et al. 2024. CrossCodeEval: A diverse and
multilingual benchmark for cross-file code comple-
tion. Advances in Neural Information Processing
Systems , 36.
Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,
Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng
Sha, Xin Peng, and Yiling Lou. 2023. ClassE-
val: A manually-crafted benchmark for evaluating
llms on class-level code generation. arXiv preprint
arXiv:2308.01861 .
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, et al. 2024. The llama
3 herd of models. Preprint , arXiv:2407.21783.
Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen,
Yi Chang, and Jun Wang. 2024. DS-Agent: Au-
tomated data science by empowering large lan-
guage models with case-based reasoning. ArXiv ,
abs/2402.17453.
Md Mahim Anjum Haque, Wasi Uddin Ahmad, Is-
mini Lourentzou, and Chris Brown. 2023. FixE-
val: Execution-based evaluation of program fixes for
programming problems. In 2023 IEEE/ACM Inter-
national Workshop on Automated Program Repair
(APR) , pages 11‚Äì18. IEEE.
Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu,
Yong Dai, Hongming Zhang, Zhenzhong Lan, and
Dong Yu. 2024. WebV oyager: Building an end-to-
end web agent with large multimodal models. In
Annual Meeting of the Association for Computational
Linguistics .
Dan Hendrycks, Steven Basart, Saurav Kadavath, Man-
tas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, and Jacob
Steinhardt. 2021. Measuring coding challenge com-
petence with apps. NeurIPS .
Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu,
Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang,
Lingyao Zhang, Mingchen Zhuge, et al. 2024. Data
interpreter: An llm agent for data science. arXiv
preprint arXiv:2402.18679 .
Qian Huang, Jian V ora, Percy Liang, and Jure Leskovec.
2024. MLAgentBench: Evaluating language agents
on machine learning experimentation. Preprint ,
arXiv:2310.03302.
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia
Yan, Tianjun Zhang, Sida Wang, Armando Solar-
Lezama, Koushik Sen, and Ion Stoica. 2024. Live-
CodeBench: Holistic and contamination free eval-
uation of large language models for code. arXiv
preprint arXiv:2403.07974 .
11Peter Jansen, Marc-Alexandre Cot‚Äôe, Tushar Khot, Erin
Bransom, Bhavana Dalvi, Bodhisattwa Prasad Ma-
jumder, Oyvind Tafjord, and Peter Clark. 2024. DIS-
COVERYWORLD: A virtual environment for devel-
oping and evaluating automated scientific discovery
agents. arXiv preprint arXiv:2406.06769 .
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, L√©lio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Th√©ophile Gervet, Thibaut Lavril, Thomas Wang,
Timoth√©e Lacroix, and William El Sayed. 2024. Mix-
tral of experts. Preprint , arXiv:2401.04088.
Carlos E Jimenez, John Yang, Alexander Wettig,
Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R
Narasimhan. 2024. SWE-bench: Can language mod-
els resolve real-world github issues? In The Twelfth
International Conference on Learning Representa-
tions .
Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettle-
moyer, S. W. Yih, D. Fried, S. Wang, and T. Yu. 2023.
DS-1000: A natural and reliable benchmark for data
science code generation. In International Conference
on Machine Learning (ICML) .
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,
Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,
and Yongbin Li. 2023. API-bank: A comprehensive
benchmark for tool-augmented llms. In The 2023
Conference on Empirical Methods in Natural Lan-
guage Processing .
Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, R√©mi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Competition-level code generation with
alphacode. Science , 378(6624):1092‚Äì1097.
Siyi Liu, Chen Gao, and Yong Li. 2024. Large language
model agent for hyper-parameter optimization. arXiv
preprint arXiv:2402.01881 .
Tianyang Liu, Canwen Xu, and Julian McAuley.
2023a. RepoBench: Benchmarking repository-
level code auto-completion systems. arXiv preprint
arXiv:2306.03091 .
Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu,
Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu,
Zengxian Yang, Kaikai An, Ruijun Huang, Shuzheng
Si, Sheng Chen, Haozhe Zhao, Zhengliang Li, Liang
Chen, Yiming Zong, Yan Wang, Tianyu Liu, Zhi-
wei Jiang, Baobao Chang, Yujia Qin, Wangchunshu
Zhou, Yilun Zhao, Arman Cohan, and Mark Gerstein.
2023b. ML-Bench: Evaluating large language mod-
els for code generation in repository-level machine
learning tasks. arXiv preprint arXiv:2311.09835 .Felipe Maia Polo, Lucas Weber, Leshem Choshen,
Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin.
2024. tinyBenchmarks: evaluating llms with fewer
examples. arXiv preprint arXiv:2402.14992 .
Gr√©goire Mialon, Cl√©mentine Fourrier, Thomas Wolf,
Yann LeCun, and Thomas Scialom. 2024. GAIA: a
benchmark for general AI assistants. In The Twelfth
International Conference on Learning Representa-
tions .
OpenAI. 2023. GPT-4 technical report. Preprint ,
arXiv:2303.08774.
OpenDevin Team. 2024. OpenDevin: An Open
Platform for AI Software Developers as Gener-
alist Agents. https://github.com/OpenDevin/
OpenDevin . Accessed: 06/11/2024.
Joon Sung Park, Joseph O‚ÄôBrien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th An-
nual ACM Symposium on User Interface Software
and Technology , pages 1‚Äì22.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Joseph E. Gonzalez. 2023. Gorilla: Large language
model connected with massive APIs. arXiv preprint
arXiv:2305.15334 .
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah Smith, and Mike Lewis. 2023. Measuring and
narrowing the compositionality gap in language mod-
els. In Findings of the Association for Computational
Linguistics: EMNLP 2023 , pages 5687‚Äì5711, Singa-
pore. Association for Computational Linguistics.
Sheeba Samuel and Daniel Mietchen. 2022. Compu-
tational reproducibility of jupyter notebooks from
biomedical publications. GigaScience , 13.
Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang,
Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li,
and Yueting Zhuang. 2023. TaskBench: Benchmark-
ing large language models for task automation. arXiv
preprint arXiv:2311.18760 .
Noah Shinn, Federico Cassano, Edward Berman, Ash-
win Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning. In NeurIPS .
Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian
Li, and Bill Yuchen Lin. 2024. Trial and error:
Exploration-based trajectory optimization of LLM
agents. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 7584‚Äì7600, Bangkok,
Thailand. Association for Computational Linguistics.
Shane Storks, Keunwoo Yu, Ziqiao Ma, and Joyce Chai.
2023. NLP reproducibility for all: Understanding
experiences of beginners. In Proceedings of the 61st
Annual Meeting of the Association for Computational
12Linguistics (Volume 1: Long Papers) , pages 10199‚Äì
10219, Toronto, Canada. Association for Computa-
tional Linguistics.
Wilson L Taylor. 1953. ‚Äúcloze procedure‚Äù: A new
tool for measuring readability. Journalism quarterly ,
30(4):415‚Äì433.
Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai
Lin, Zhiyuan Liu, and Maosong Sun. 2024. De-
bugBench: Evaluating debugging capability of large
language models. arXiv preprint arXiv:2401.04621 .
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-
ima Anandkumar. 2023a. V oyager: An open-ended
embodied agent with large language models. arXiv
preprint arXiv:2305.16291 .
Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,
Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable
code actions elicit better llm agents. arXiv preprint
arXiv:2402.01030 .
Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi
Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b.
Mint: Evaluating llms in multi-turn interaction
with tools and language feedback. arXiv preprint
arXiv:2309.10691 .
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan
Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua,
Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al.
2024. OSWorld: Benchmarking multimodal agents
for open-ended tasks in real computer environments.
arXiv preprint arXiv:2404.07972 .
John Yang, Carlos E. Jimenez, Alexander Wettig, Kil-
ian Lieret, Shunyu Yao, Karthik Narasimhan, and
Ofir Press. 2024a. SWE-agent: Agent-computer
interfaces enable automated software engineering.
Preprint , arXiv:2405.15793.
Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong,
Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan,
Pengyuan Liu, Dong Yu, et al. 2024b. MatPlotA-
gent: Method and evaluation for llm-based agen-
tic scientific data visualization. arXiv preprint
arXiv:2402.11453 .
Shunyu Yao, Howard Chen, John Yang, and Karthik
Narasimhan. 2022. Webshop: Towards scalable real-
world web interaction with grounded language agents.
Advances in Neural Information Processing Systems ,
35:20744‚Äì20757.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .
Da Yin, Faeze Brahman, Abhilasha Ravichander, Khy-
athi Chandu, Kai-Wei Chang, Yejin Choi, and
Bill Yuchen Lin. 2024. Agent lumos: Unified and
modular training for open-source language agents.InProceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 12380‚Äì12403, Bangkok, Thai-
land. Association for Computational Linguistics.
Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin
Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and
Weizhu Chen. 2023. Repocoder: Repository-level
code completion through iterative retrieval and gen-
eration. arXiv preprint arXiv:2303.12570 .
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan
Bisk, Daniel Fried, Uri Alon, et al. 2023. WebArena:
A realistic web environment for building autonomous
agents. arXiv preprint arXiv:2307.13854 .
13A Repository details
Table 8 shows information about the 45 source repositories used to create the Expert and Masked sets,
including their name, original GitHub link and the number of stars on GitHub.
Task GitHub Stars
colbert https://github.com/stanford-futuredata/ColBERT 2826
textbox https://github.com/RUCAIBox/TextBox 1069
amrbart https://github.com/goodbai-nlp/AMRBART 94
g-transformer https://github.com/baoguangsheng/g-transformer 43
pie-perf https://github.com/madaan/pie-perf 80
safetybench https://github.com/thu-coai/SafetyBench 138
discodisco https://github.com/gucorpling/DisCoDisCo 6
acqsurvey https://github.com/rahmanidashti/acqsurvey 11
curriculum_learning https://github.com/adymaharana/curriculum_learning 9
spa https://github.com/OceannTwT/SPA 5
mezo https://github.com/princeton-nlp/MeZO 1016
mode-connectivity-plm https://github.com/thunlp/mode-connectivity-plm 7
mbib https://github.com/Media-Bias-Group/MBIB 22
quantifying-stereotypes-in-... https://github.com/nlply/quantifying-stereotypes-in-language 1
rah-kbqa https://github.com/yanmenxue/rah-kbqa 6
dir-gnn https://github.com/wuyxin/dir-gnn 115
unsupervisedhierarchicalsymbolic... https://github.com/SiyuLou/UnsupervisedHierarchicalSymbolicRegression 0
conv_graph https://github.com/huawei-noah/noah-research/tree/master/conv_graph 0
mera https://github.com/ai-forever/MERA 55
pira https://github.com/C4AI/Pira 5
pet https://github.com/timoschick/pet 1618
transnormerllm https://github.com/opennlplab/transnormerllm 221
bert-lnl https://github.com/uds-lsv/BERT-LNL 9
blockskim https://github.com/chandlerguan/blockskim 6
data_label_alignment https://github.com/gyauney/data-label-alignment 3
hype https://github.com/yuanhy1997/HyPe 13
paraphrase-nli https://github.com/matejklemen/paraphrase-nli 3
powerfulpromptft https://github.com/zhengxiangshi/powerfulpromptft 71
robust_prompt_classifier https://github.com/adianliusie/robust-prompt-classifier 5
align-to-distill https://github.com/ncsoft/Align-to-Distill 4
inbedder https://github.com/zhang-yu-wei/InBedder 20
transpolymer https://github.com/ChangwenXu98/TransPolymer 51
memorizing-transformers-... https://github.com/lucidrains/memorizing-transformers-pytorch 622
multi3woz https://github.com/cambridgeltl/multi3woz 14
galore https://github.com/jiaweizzhao/galore 1332
amos https://github.com/microsoft/amos 24
glee https://github.com/genezc/Glee 9
parallel-context-windows https://github.com/AI21Labs/Parallel-Context-Windows 98
logme-nlp https://github.com/mainlp/logme-nlp 5
mixup-amp https://github.com/pai-smallisallyourneed/mixup-amp 4
upet https://github.com/wjn1996/UPET 2
dpt https://github.com/xyaoooo/dpt 6
team https://github.com/declare-lab/team 22
cet https://github.com/zzz47zzz/CET 18
linkbert https://github.com/michiyasunaga/LinkBERT 411
Table 8: Details of the 45 repositories used in SUPER along with GitHub link and star information as of September
3rd, 2024.
Adding to the information in Table 1, we show below the average and median star ratings for other
comparable benchmarks (all star ratings are collected as of September 3rd, 2024 ). This was computed
automatically from the GitHub API based on the repositories listed in Jimenez et al. (2024), Lai et al.
(2023) and Liu et al. (2023b) (we group together both the train and test repositories mentioned in this
table).
dataset # repos stars (mean (median))
SWE-Bench 12 27,844 (12,557)
DS1000 8 55,227 (35,309)
MLBench 18 13,099 (9,632)
SUPER (Expert) 45 224 (14)
SUPER (Auto) 604 96 (23)
B Automatic Generation of Tasks
B.1 Tasks Generation
The automatic tasks generation involves two high-level steps: filtering repositories, and creating tasks for
repositories.
14Step 1: Filtering Repositories. We start from 5915 repositories listed by ‚Äúpaperswithcode‚Äù to be added
on year 2021 or later and having modality ‚ÄòText‚Äô. We then automatically clone each of these repositories,
filtering out those where: (1) cloning failed, (2) use LLM APIs (based on the occurence of importing
Python packages of LLM provider such as OpenAI, Anthropic, etc.) or (3) no readme file was found.
On the remaining repositories, we then use the following prompt on GPT-4o ( gpt-4o-2024-08-06 ) to
filter repositories.
Repositories Filtering Prompt
Your task is to analyze a GitHub repository and answer the following questions:
1. Can this repository run on one of these datasets (they should be explicitly mentioned in readme
or grepped code)? SST-2, QNLI, QQP, MRPC, RTE, MMLU, wikitext, yelp, ai2_arc, hellaswag,
winogrande, piqa, humaneval, truthful_qa, cnn_dailymail, mnist, xsquad, squad, xnli, mnli,
multi_nli, ag_news, WikiText, gsm8k, cola, triviaqa, hotpotqa, humaneval, fever, boolq,
openbookqa, drop, coqa, GLUE.,‚Üí
,‚Üí
,‚Üí
,‚Üí
2. Can the repository be run on a CPU? It is acceptable if it runs slowly, as we will use small
models and datasets. The repository should be rejected only if it relies on specific GPU
acceleration methods such as LoRA adapters, if it aims to improve GPU utilization or otherwise
relies on specific GPU features.,‚Üí
,‚Üí
,‚Üí
3. Can the repository be used with any of the following model families: BERT, T5, RoBERTa, GPT-2,
GPT-Neo, DeBERTa, DistilBERT, BART, Pythia, OPT? ,‚Üí
4. Does the README provide an example or instructions on how to run an executable Python or Bash
file to start an experiment? If so, provide the name of the executable file. For example:
`python run_glue.py --model_name_or_path roberta-base --task_name mrpc --output_dir
results/ft/roberta_base/mrpc `,‚Üí
,‚Üí
,‚Üí
Return a json file in this format:
{
"q_supported_dataset_name": "hellaswag",
"q_supported_dataset_reason": "The README mentions the use of the hellaswag dataset, which is
one of the supported datasets.", ,‚Üí
"q_cpu": true,
"q_cpu_reason": "The repository does not rely on specific GPU acceleration methods such as LORA
adapters or repositories that improve GPU utilization.", ,‚Üí
"q_model_families": true,
"q_model_families_reason": "The repository supports the BERT model family, as indicated by the
presence of 'bert-large 'in the model configuration.", ,‚Üí
"q_execute_example": true,
"q_execute_example_reason": "The readme provides an example for running a training pipeline on
the hellaswag dataset." ,‚Üí
}
We then keep repositories where q_supported_dataset_name ,q_cpu ,q_model_families and
q_execute_example are all predicted by the LLM to be true, resulting in 1006 repositories. Note
that this filtering process is rather conservative; it is likely possible to get more high-quality tasks from
repositories that were filtered out.
Step 2: Generating Tasks. We iterate the filtered repositories and prompt GPT-4o (same version) with
the following text to generate the tasks.
Tasks Generation Prompt
Create an experiment-running task based on the provided README file of a research code repository.
Output a json dictionary with the following structure: ,‚Üí
{"thought": "...", "output": {"task": "...", "entrypoint": "..."}}
Instructions:
1. Choose a specific script (either Bash or Python) for which the README provides an example. If no
example is found, skip the repository (return an empty dictionary). The "entrypoint" field
should specify the script file to be run without any arguments. Use a specific file path, even
if the script is typically executed using a module (e.g., "train/run.py", not "python -m
train.run").,‚Üí
,‚Üí
,‚Üí
,‚Üí
Ensure that the script selected is for running experiments (e.g., evaluation or fine-tuning) and
not a utility script (such as a server or data processing script). ,‚Üí
2. The task description should include the following:
1. A statement that reflects the goal of the repository. For instance, use "Fine-tune a model
with the question answering infused pre-training method" rather than just "fine-tune a
model," or "Pre-train a reduced-scale model" rather than just "pre-train a model." If the
repository lacks sufficient detail, you may keep this generic.,‚Üí
,‚Üí
,‚Üí
152. The specific dataset to be used. If the repository doesn‚Äôt specify supported datasets, skip
the repository. For repositories mentioning a group of datasets (e.g., GLUE), choose a
specific dataset (e.g., MRPC) instead of just mentioning the group. If the README indicates
that the data is unavailable (e.g., "data will be uploaded soon"), skip the repository.,‚Üí
,‚Üí
,‚Üí
3. The Python script/Bash file/entry point to be run, which should match the "entrypoint" field.
4. The model to be used. To ensure the task is feasible with minimal compute, select a small or
base model from one of the following families: ,‚Üí
bert, roberta, t5, gpt, opt, deberta, distilbert, bart, pythia.
Then, select the smallest model in the family, based on this mapping: bert: bert-base, t5:
google-t5/t5-small, gpt: openai-community/gpt2, deberta: deberta-base, bart: bart-base,
pythia: EleutherAI/pythia-70m, OPT: facebook/opt-125m,‚Üí
,‚Üí
If the README or repository content does not explicitly mention the model family or model
size, skip the repository. ,‚Üí
Here are a few examples.
[...]
We filter out repositories/tasks when (1) the model decides to skip (e.g. if no indication of model or
dataset), or (2) the provided script selected by the model cannot be found in the repository, or is not a
Python or bash file.
B.2 Tasks Evaluation (Script-Executed metric)
We use a simple heuristic to determine if a script was run successfully: we check if the script was executed
without any exceptions being raised (according to printed output), and if it was executed for at least s
seconds. We use time limit to make sure we avoid any quick failures that did not raise an exception, such
as messages about missing arguments. Based on the gold expert solutions, we find that s= 10 is a good
trade-off to distinguish unsuccessful short runs from successful ones. Importantly, this evaluation metric
is an approximation, and can surely be incorrect or even manipulated by agents that are aware of it. Yet as
we show in ¬ß4.3, we found it to match the landmarks evaluation in 90% of the cases, hopefully making it
useful for development, and as a basis for potentially creating even larger sets of tasks for development
and training purposes.
C Prompts and Interaction History
C.1 ReAct- SUPER
We use the following prompt for ReAct- SUPER .
ReAct-SUPER Prompt
Interact in a jupyter notebook to solve the request.
Output a thought and an action.
Thought can reason about the current situation, and action allows you to interact with the Jupyter
notebook. There are three types of actions: ,‚Üí
(1) execute, which executes a notebook cell with the command under the "content" field. This could
be a bash command (beginning with !), a python command, or magic command (beginning with %). ,‚Üí
(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines,
you will need to provide the lines as they appear before the replacement, and the lines to
replace them with.,‚Üí
,‚Üí
In the "content" field, format should be
```
{filename}
<<<BEFORE_EDIT>>>
lines before edit
lines before edit
<<<AFTER_EDIT>>>
lines after edit
lines after edit
```
- Include the entire lines WITH indentation. After editing, make sure indentation is correct.
(3) submit, which submits the final answer (or null if there 's no answer to submit), formatted as
JSON (make sure to use double quotes for keys and values). ,‚Üí
Always output exactly one "thought" and one "action", and encapsulate the content with
<content>...</content> tags. ,‚Üí
I will provide the cell output (observation) for you.
16Tips:
- When a github repository is given, you should start by cloning it. You can read the readme file
and browse the repo for installation and usage instructions. ,‚Üí
- Avoid using conda, instead use pip for package management.
- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the
kernel or continue waiting. ,‚Üí
You should interrupt if: (1) you are prompted with an interactive message (e.g., "Do you want to
continue? [Y/n]"). In that case, you can bypass it by executing echo "y" | command, or (2)
the execution is taking longer than expected because the wrong task is being executed, such
as a different model or incorrect number of epochs.,‚Üí
,‚Üí
,‚Üí
You should continue to wait if the current execution is correct and proceeding as expected.
Here are some examples.
Git repository: EleutherAI/lm-evaluation-harness
Query: Evaluate EleutherAI/gpt-j-6B on hellaswag. What 's the score?
Thought: First, I need to clone the GitHub repository 'EleutherAI/lm-evaluation-harness '.
Action: execute
<content>
!git clone https://github.com/EleutherAI/lm-evaluation-harness.git
</content>
Observation:
```
Cloning into 'lm-evaluation-harness '...
```
Thought: The repository has been cloned. Next, I should check the README file for installation and
usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag. ,‚Üí
Action: execute
<content>
!cat lm-evaluation-harness/README.md
</content>
Observation:
```
# Language Model Evaluation Harness ... truncated ...
```
Thought: The README provides detailed instructions for installing the `lm-eval `package and
evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its
dependencies first.,‚Üí
,‚Üí
Action: execute
<content>
!pip install -e lm-evaluation-harness/.
</content>
Observation:
```
Obtaining file:///home/nora/lm-evaluation-harness
Installing build dependencies ... truncated ...
```
Thought: The `lm-eval `package and its dependencies have been installed successfully. Now, I can
proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the
instructions in the README.,‚Üí
,‚Üí
Action: execute
<content>
!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0
--batch_size 8 ,‚Üí
</content>
Observation:
```
{
"hellaswag": {
"acc": 0.24603658536585363,
"acc_norm": 0.34436059706446426,
}, ... truncated ...
```
Thought: The `lm-eval `was executed successfully, providing the metrics I needed. The `acc`metric
is 0.2460. The `acc_norm `metric is 0.3444 ,‚Üí
Action: submit
<content>
{"acc": 0.2460, "acc_norm": 0.3444}
</content>
17Query: use pandas to read the file 'data.csv 'and display the first 2 rows.
Thought: First, I need to read the file 'data.csv 'using the pandas library.
Action: execute
<content>
import pandas as pd
pd.read_csv( 'data.csv ').head()
</content>
Observation:
```
name
0 Alice
1 Bob
2 Charlie
```
Thought: The file 'data.csv 'has been read successfully and the first 2 rows have been displayed.
Action: submit
<content>
["Alice", "Bob"]
</content>
Query: Change the model name in the 'config.json 'file from 'roberta-base 'to'roberta-large '.
Thought: First, I need to read the 'config.json 'file to find the location where I can replace the
model name. ,‚Üí
Action: execute
<content>
!cat config.json
</content>
Observation:
```
{
"model": "roberta-base",
"batch_size": 8,
"epochs": 10
}
```
Thought: The file has been read successfully. Now, I can replace the model name in the
'config.json 'file. ,‚Üí
Action: edit
<content>
config.json
<<<BEFORE_EDIT>>>
{
"model": "roberta-base",
"batch_size": 8,
<<<AFTER_EDIT>>>
{
"model": "roberta-large",
"batch_size": 8,
</content>
Observation: "editing file config.json"
Thought: The model name has been replaced in the 'config.json 'file.
Action: submit
<content>
null
</content>
The demonstrations used in this prompt were not taken from any of the SUPER sets, and were designed
specifically to show the basic usage of the actions that are available to the agent.
History of Interactions. Following the above prompt, at each step we provide the history of all past
interactions by concatenating (thought, action, observation) tuples into a string, which we pass to the
LLM as a single message:
Thought: {{thought}}
Action: {{action}}
Observation: {{observation}}
18When executing problems from the Masked set, some steps are pre-executed. We run the pre-execute
commands in the enviornment (without any agent interaction) to collect action and observation pairs. We
then use these pairs in the history of succeeding agent steps, using the following fixed thought:
[pre-executed by the user]
Since trajectories can often get long (¬ß4.1), we use the following truncation strategy for all agents: for
the last step, we provide the 50k last characters, which is usually enough for the entire observation. For
earlier steps, we shorten the observations to show the last 500 characters.
C.2 Reflection Agent Prompt
We use the following prompt to generate reflections (Reflexion agent), without any demonstrations.
Reflexion Agent Prompt
You will be given the history of a past experience in which you were placed in an environment and
given a task to complete. ,‚Üí
You were unsuccessful in completing the task. Do not summarize your environment, but rather think
about the strategy and path you took to attempt to complete the task. ,‚Üí
Devise a concise, new plan of action that accounts for your mistake with reference to specific
actions that you should have taken. ,‚Üí
For example, if you tried A and B but forgot C, then devise a plan to achieve C with
environment-specific actions. If you wasted too much time on A, then devise a plan for more
easily and directly achieving A.,‚Üí
,‚Üí
You will need this later when you are solving the same task. Give your plan after "Plan" and end it
with [END]. ,‚Üí
C.3 SWE-Agent Prompt
We take the original SWE-Agent prompt and adjust it to instruct the agent to complete the research task
in our environment rather than fixing GitHub issues, while making sure the tips and information are as
similar as possible to the ReAct- SUPER prompt for fair comparison. We include the exact same three
demonstrations provided in the other agents, adjusted for SWE-Agent tools, in a similar format to the
original implementation.
SWE-Agent System Prompt
SETTING: You are an autonomous programmer, and you 're working directly in the command line with a
special Jupyter notebook interface. ,‚Üí
The special Jupyter notebook interface consists of a file editor that shows you {WINDOW} lines of
a file at a time. ,‚Üí
You can execute commands in the notebook using:
1. Bash commands: Commands starting with !.
2. Python commands: Standard Python code.
3. Magic commands: Commands starting with %, e.g., %cd <path>.
Additionally, you can also use the following commands to help you navigate and edit files.
COMMANDS:
{command_docs}
Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION.
If you 'd like to add the line ' print(x) 'you must fully write that out, with all those
spaces before the code! Indentation is important and code that is not indented correctly will
fail and require fixing before it can be run.,‚Üí
,‚Üí
RESPONSE FORMAT:
Your shell prompt is formatted as follows:
(Open file: <path>)
(Current directory: <cwd>)
In [ ]
You need to format your output using two fields: discussion and command.
Your output should always include _one_ discussion and _one_ command field EXACTLY as in the
following example: ,‚Üí
19DISCUSSION
First I 'll start by using ls to see what files are in the current directory. Then maybe we can
look at some relevant files to see what they look like. ,‚Üí
```
!ls -a
```
You should only include a *SINGLE* command in the command section and then wait for a response
from the shell before continuing with more discussion and commands. Everything you include in
the DISCUSSION section will be saved for future reference.,‚Üí
,‚Üí
If you 'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit
just the first command, and then after receiving a response you 'll be able to issue the
second command.,‚Üí
,‚Üí
You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to
the special commands listed above. ,‚Üí
SWE-Agent Instance Prompt
We're currently solving the following task in the repository.
TASK:
{query}
INSTRUCTIONS:
Now, you 're going to execute this task on your own. You can use any bash commands or the special
interface commands to help you. Edit all the files you need to and run any checks or tests
that you want.,‚Üí
,‚Üí
Remember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after
every command. ,‚Üí
When you obtain the final answer for the requested TASK, you can submit it by running the
`submit `command. ,‚Üí
NOTE ABOUT THE EDIT COMMAND: Indentation really matters! When editing a file, make sure to insert
appropriate indentation before each line! ,‚Üí
IMPORTANT TIPS:
1. When a github repository is given, you should start by cloning it. You can read the readme
file and browse the installation and usage instructions. Then you need to set up the Python
environment and install necessary packages before you run any scripts in the repo. Avoid
using conda, instead use pip for package management.,‚Üí
,‚Üí
,‚Üí
2. If you run a command and it doesn 't work, try running a different command. A command that did
not work once will not work the second time unless you modify it! ,‚Üí
3. If you open a file and need to get to an area around a specific line that is not in the first
100 lines, say line 583, don 't just use the scroll_down command multiple times. Instead, use
the goto 583 command. It 's much quicker.,‚Üí
,‚Üí
4. Always make sure to look at the currently open file and the current working directory (which
appears right after the currently open file). The currently open file might be in a different
directory than the working directory! Note that some commands, such as 'create ', open files,
so they might change the current open file.,‚Üí
,‚Üí
,‚Üí
5. When editing files, it is easy to accidentally specify a wrong line number or to write code
with incorrect indentation. Always check the code after you issue an edit to make sure that
it reflects what you wanted to accomplish. If it didn 't, issue another command to fix it.,‚Üí
,‚Üí
6. Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt
the kernel or continue waiting. ,‚Üí
You should interrupt if: (1) you are prompted with an interactive message (e.g., "Do you want to
continue? [Y/n]"). In that case, you can bypass it by executing echo "y" | command, or (2)
the execution is taking longer than expected because the wrong task is being executed, such
as a different model or incorrect number of epochs. You should continue to wait if the
current execution is correct and proceeding as expected.,‚Üí
,‚Üí
,‚Üí
,‚Üí
(Open file: {open_file})
(Current directory: {working_dir})
In [ ]
20D Instructions to Experts
Workers were hired through Upwork ( upwork.com ), were paid $30-$40/hour, and were limited to four
hours per task, although in some cases they were approved for up to an additional 2 hours if the task
wasn‚Äôt completed in time. In a few cases, the experts were unable to run the experiments due to CoLab or
dependencies issues; these tasks were discarded. In total, task collection cost $6,580 for 50 solutions, of
which we keep the final set of 45 tasks.
We provide the instructions given to the Upwork experts in Figs. 4 to 6.
Figure 4: Guidelines provided to experts.
E Edit command
The edit command is an action that agents can select at every iteration, in addition to execute and command.
The format of the input to this command is as follows:
{filename}
<BEFORE_EDIT>
(lines before edit)
<AFTER_EDIT>
(lines after edit)
Where (lines before edit) are the exact lines to be replaced, and (lines after edit) are the
lines to replace them with.
To provide the exact file contents that should be replaced, agents typically need to view the existing
contents of the file, for example by using the catcommand, and then copy it verbatim as the lines to be
21Figure 5: Guidelines provided to experts (continued).
Figure 6: Guidelines provided to experts (continued).
replaced.6Our environment then looks for the contents to be replaced in the file and replaces it with the
6Solutions that accept line numbers instead of exact content performed worse in our early experiments.
22new contents.
The edit command requires the provided replaced lines to be (1) precisely copied, including correct
whitespaces and indentations and (2) unique in the contents file, so that the edit command is not ambiguous.
To help the agent with these requirements, we configure the edit command to provide specific feedback to
the agent in case one of these conditions does not apply.
Specifically, if the the lines to be replaced were not found as-is, but these lines do appear in the edited
filewithout surrounding whitespaces or tabs, then the environment provides the following feedback:
Did you mean to replace the following lines (notice leading/trailing whitespaces
difference)? ,‚Üí
followed by an exact copy of these lines, including the spaces.
If more than one instances of these lines were found, the following feedback is provided:
Found multiple ([k]) occurrences of the <BEFORE_EDIT> lines. Add 1-3 lines before
or after these lines to replace to disambiguate. ,‚Üí
Here are the first two occurrences with additional context, did you mean one of
these? ,‚Üí
Occurrence 1: ...
with the first two occurences of these lines.
23