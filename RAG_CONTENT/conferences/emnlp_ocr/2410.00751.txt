Thinking Outside of the Differential Privacy Box:
A Case Study in Text Privatization with Language Model Prompting
Stephen Meisenbacher and Florian Matthes
Technical University of Munich
School of Computation, Information and Technology
Department of Computer Science
Garching, Germany
{stephen.meisenbacher,matthes}@tum.de
Abstract
The field of privacy-preserving Natural Lan-
guage Processing has risen in popularity, par-
ticularly at a time when concerns about privacy
grow with the proliferation of Large Language
Models. One solution consistently appearing
in recent literature has been the integration of
Differential Privacy (DP) into NLP techniques.
In this paper, we take these approaches into
critical view, discussing the restrictions that DP
integration imposes, as well as bring to light
the challenges that such restrictions entail. To
accomplish this, we focus on DP-P ROMPT , a
recent method for text privatization leveraging
language models to rewrite texts. In particular,
we explore this rewriting task in multiple sce-
narios, both with DP and without DP. To drive
the discussion on the merits of DP in NLP, we
conduct empirical utility and privacy experi-
ments. Our results demonstrate the need for
more discussion on the usability of DP in NLP
and its benefits over non-DP approaches.
1 Introduction
The topic of privacy in Natural Language Process-
ing has recently gained traction, which has only
been fueled by the prominent rise of Large Lan-
guage Models. In an effort to address concerns
revolving around the protection of user data, the
study of privacy-preserving NLP has presented a
plethora of innovative solutions, all investigating in
some form the optimization of the privacy-utility
trade-off for the safe processing of textual data.
A well-studied solution comes with the integra-
tion of Differential Privacy (DP) (Dwork, 2006)
into NLP techniques. Essentially, the use of DP en-
tails the addition of calibrated noise to some stage
in a pipeline, e.g., directly to the data or to model
weights. This is performed with the ultimate goal
of protecting the individual whose data is being
used, aligned with the objective of Differential Pri-
vacy set out in its inception nearly 20 years ago.The incentive of proving Differential Privacy
is the mathematical guarantee of privacy protec-
tion that it offers, so long as its basic principles
are adhered to. Particularly, important DP notions
must be strictly defined, such as who the individ-
ual is, how data points are adjacent, and how data
can be bounded. As such, the fusion of Differen-
tial Privacy and NLP introduces several challenges
(Feyisetan et al., 2021; Habernal, 2021; Klymenko
et al., 2022; Mattern et al., 2022). When general-
ized forms of DP are used or well-defined notions
of DP concepts are lacking, the promise of DP
becomes more of a shallow guarantee.
In this work, we critically view the pursuit of
DP in NLP, focusing on the particular method of
DP-P ROMPT (Utpala et al., 2023). This method
leverages generative Language Models to rewrite
(paraphrase) texts with the help of a DP token selec-
tion method based on the Exponential Mechanism
(Mattern et al., 2022). We run experiments on three
rewriting settings: (1) DP, (2) Quasi-DP , and (3)
Non-DP ; the purpose of this trichotomy is to ex-
plore the benefits and shortcomings of DP in text
rewriting. We define our research question as:
What is the benefit of integrating Differential
Privacy into private text rewriting methods
leveraging LMs, and what effect can be ob-
served by relaxing this guarantee?
Our empirical findings show the advantages that
incorporating DP into text rewriting mechanisms
brings, notably higher semantic similarity and re-
semblance to the original texts, along with strong
empirical privacy results. This, however, comes
with the downside of generally lower quality text
in terms of readability, particularly at stricter pri-
vacy budgets. These findings open the door to
discussions regarding the practical distinction be-
tween DP and non-DP text privatization, where we
present open questions and paths for future work.
The contributions of our work are as follows:
1arXiv:2410.00751v1  [cs.CL]  1 Oct 20241.We explore the merits of DP in LM text rewrit-
ing through comparative experiments.
2.We evaluate DP-P ROMPT in a series of utility
and privacy tests, and analyze the difference
in DP vs. non-DP privatization.
3.We call into question the merits of DP in NLP,
presenting the benefits and limitations of do-
ing so as opposed to non-DP privatization.
2 Related Work
Natural language can leak personal information
(Brown et al., 2022) and it is possible to extract
training data from Machine Learning models (Pan
et al., 2020; Carlini et al., 2021; Mattern et al.,
2023). In the global DP setting, user texts are col-
lected at a central location and a model is trained
using privacy-preserving optimization techniques
(Ponomareva et al., 2022; Kerrigan et al., 2020)
such as DP-SGD (Abadi et al., 2016). The primary
drawback of this model is that user data must be
collected at a central location, giving a data curator
access to the entire data (Klymenko et al., 2022).
To mitigate this, text can be obfuscated or rewritten
locally in a DP manner before collecting it at a cen-
tral location (Feyisetan et al., 2020; Igamberdiev
and Habernal, 2023; Hu et al., 2024).
The earliest set of approaches of DP in NLP
began at the word level (Weggenmann and Ker-
schbaum, 2018; Fernandes et al., 2019; Yue et al.,
2021; Chen et al., 2023; Carvalho et al., 2023;
Meisenbacher et al., 2024a), yet these methods
do not consider contextual and grammatical infor-
mation during privatization (Mattern et al., 2022;
Meisenbacher et al., 2024c). Other works operate
directly at the sentence level by either applying DP
to embeddings (Meehan et al., 2022) or latent rep-
resentations (Bo et al., 2021; Weggenmann et al.,
2022; Igamberdiev and Habernal, 2023). DP text
rewriting methods using generative LMs (Mattern
et al., 2022; Utpala et al., 2023; Flemings and An-
navaram, 2024) or encoder-only models (Meisen-
bacher et al., 2024b) have also been proposed.
3 Method
Here, we describe the base text privatization
method that we utilize, as well as the variations
which form the basis of our experiments.
3.1 DP-P ROMPT
DP-P ROMPT (Utpala et al., 2023) is a differen-
tially private text rewriting method in which usersgenerate privatized documents at the local level
by prompting Language Models to rewrite input
texts. In particular, the LMs are prompted to para-
phrase a given text. The immediate advantage of
this method comes with the flexibility in model
choice as well as the generalizability to all general-
purpose pre-trained (instruction-finetuned) LMs.
The integration of DP into this rewriting pro-
cess comes at the generation step, where for each
output token, a DP token selection mechanism is
implemented in the form of temperature sampling .
In Mattern et al. (2022), it is shown that the use
of temperature can be equated to the Exponential
Mechanism (McSherry and Talwar, 2007). Relat-
ing this mechanism to the privacy budget εof DP,
the authors show that ε=2∆
T, where Tis the tem-
perature and ∆is the sensitivity , or range, of the
token logits. A fixed sensitivity can be ensured by
clipping the logits to certain bounds.
For the purposes of this work, we perform all
experiments using DP-P ROMPT with the FLAN -T5-
BASE model from Google (Chung et al., 2022).
3.2 Rewriting Approaches
Motivated by the DP-P ROMPT rewriting mecha-
nism, we introduce three privatization strategies
based on its DP token selection mechanism:
1.DP: we use DP-P ROMPT as originally in-
troduced, namely by clipping logit values
and scaling logits by temperatures calculated
based on εvalues. We test on the values ε∈
{25,50,100,150,250}. Logits are clipped
based on an empirical measurement of log-
its in the FLAN -T5-BASE model1.
2.Quasi-DP : we replicate the DPstrategy with-
outclipping, i.e., only using temperature sam-
pling based on the abovementioned εvalues.
We call this quasi-DP since the temperature
values Tare calculated as if clipping was per-
formed (i.e., sensitivity is bounded), but the
unbounded logit range is actually used.
3.Non-DP : here, we do not use any clipping
or temperature, but rather only vary the top-
kparameter, or the number kof candidate
tokens considered when sampling the next
token. We choose k∈ {50,25,10,5,3}.
With these three privatization strategies, we aim
to measure empirically the effect on utility and
1Specifically, to the range ( logit _mean ,logit _mean +
4·logit _std) = (-19.23, 7.48), thus ∆ = 26 .71.
2privacy by strictly enforcing DP, relaxing DP, and
by performing privatization devoid of DP. In this
way, one may be able to analyze the merits of DP-
based text privatization methods, and furthermore,
observe the theoretical guarantees of DP in action.
4 Experimental Setup and Results
As stated by Mattern et al. (2022), a practical text
privatization mechanism should: (1) protect against
deanonymization attacks, (2) preserve utility, and
(3) keep the original semantics intact. As such,
we design our experiments by leveraging multiple
dimensions of a single dataset. The results of all
described experiments can be found in Table 1.
4.1 Dataset
For all of our experiments, we utilize the Blog Au-
thorship Corpus (Schler et al., 2006). This corpus
contains nearly 700k blog post texts from roughly
19k unique authors. The corpus also lists the ID,
gender, and age of author for each blog post. Full
details on the preparation of the corpus are found in
Appendix A; pertinent details are outlined below.
We prepare two subsets of the corpus. The first,
which we call author10 , only considers blog posts
from the top-10 most frequently occurring blog au-
thors in the corpus. This subset results in a dataset
of 15,070 blog posts spanning five categories.
The second subset, called topic10 , is necessary
as the classification of the gender and age attributes
for the author10 dataset would be a less diverse
and challenging task. We first take a random 10%
sample of the top-10 topics from the filtered corpus,
resulting in a sample of 14,259 blogs. Here, the
agevalue is binned into one of five bins to ensure
an equal number of instances in each bin.
4.2 Utility Experiments
We perform utility experiments for both the au-
thor10 andtopic10 datasets. To measure utility
across all privatization strategies, we first privatize
each dataset on all selected privatization param-
eters. As we choose 5 parameters ( ε/Tork) for
each of our three strategies, this results in 15 dataset
variants, i.e., 15 results per metric, each of which
represents the average between the two datasets.
Semantic Similarity. To measure the ability of
each privatization strategy to preserve the semantic
meaning of the original sentence, we employ two
similarity metrics: BLEU (Papineni et al., 2002)
and cosine similarity. Both metrics strive to capturethe similarity between output (in this case priva-
tized) text and a reference (original) text; BLEU
relies on token overlap while cosine similarity be-
tween embeddings is more contextual.
We use SBERT (Reimers and Gurevych, 2019)
to calculate the average cosine similarity (CS) be-
tween the original blog posts and their privatized
counterparts. For this, we use utilize three embed-
dings models to account for model-specific differ-
ences: ALL-MINILM-L6- V2,ALL-MPNET -BASE -
V2, and GTE-SMALL (Li et al., 2023). For each
dataset, we report the mean of the average cosine
similarity calculated for each model.
We also report the BLEU score between priva-
tized texts and their original counterparts. This is
done using the BLEU implementation made avail-
able by Hugging Face. As before, reported BLEU
scores are the average across an entire dataset.
Readability. In addition, we also measure the
quality and readability of the privatized outputs by
using perplexity (PPL) (Weggenmann et al., 2022),
specifically with GPT-2 (Radford et al., 2019).
4.3 Privacy Experiments
Using author10 andtopic10 , we design three em-
pirical privacy experiments, in which an adversarial
classification model is trained to predict a sensitive
attribute (authorship, gender, or age) based on the
blog post text. For this, we fine-tune a DEBERTA -
V3-BASE model (He et al., 2021) for three epochs,
reporting the macro F1 of the adversarial classifier.
We evaluate the privatized datasets in two set-
tings (Mattern et al., 2022; Weggenmann et al.,
2022). In the static setting, the adversarial model is
trained on the original training split and evaluated
on the privatized validation split. In the more chal-
lenging adaptive setting, the adversarial classifier
is trained on the private train split. Lower perfor-
mance implies that a method has better protected
the privacy of the texts. Note that the adaptive score
represents the mean of three runs. For all cases, a
random 90/10 train/val split with seed 42 is taken.
In addition to F1, we also report the relative
gain metric ( γ), following previous works (Mattern
et al., 2022; Utpala et al., 2023). γaims to capture
the trade-off between utility loss and privacy gain,
as compared to the baseline scores. For the utility
portion of γ, we use the CS results. Baseline scores
are represented by adversarial performance after
training and testing on the non-private datasets. We
report the γwith respect to the adaptive setting.
3Baseline DP Quasi-DP Non-DP
ε/kvalue ∞ 25 50 100 150 250 25 50 100 150 250 50 25 10 5 3
CS↑ 1.00 0.589 0.597 0.812 0.827 0.832 0.347 0.598 0.810 0.826 0.833 0.710 0.726 0.750 0.741 0.787
BLEU ↑ 1.00 0.077 0.029 0.123 0.142 0.153 0.001 0.029 0.121 0.141 0.153 0.049 0.054 0.063 0.063 0.088
PPL↓ 41 8770 1234 928 919 905 16926 1380 982 932 925 816 972 1080 827 837
Author F1 (s) ↓ 66.45 7.13 37.05 58.10 61.12 60.60 6.59 36.91 57.84 60.37 61.13 46.83 47.07 49.88 51.69 53.10
Author F1 (a) ↓ 66.45 2.68 33.52 52.82 55.46 57.35 2.74 33.29 54.81 55.64 57.34 42.56 44.81 45.20 48.22 49.91
Gender F1 (s) ↓ 68.07 41.88 55.66 67.81 66.68 65.92 43.41 58.16 67.85 65.38 67.98 55.64 63.91 64.16 64.50 66.66
Gender F1 (a) ↓ 68.07 38.80 54.06 61.90 62.90 62.23 38.80 57.05 62.48 54.02 62.93 60.61 59.09 59.23 61.26 60.00
Age F1 (s) ↓ 37.58 19.12 28.56 38.31 37.17 38.44 17.99 28.06 37.32 37.53 38.42 32.24 32.95 35.56 35.41 35.64
Age F1 (a) ↓ 37.58 12.17 29.06 38.92 37.95 39.00 12.17 32.40 36.85 36.77 37.49 33.49 34.67 34.97 35.75 36.23
Author γ - 0.549 0.093 0.017 -0.008 -0.031 0.306 0.097 -0.015 -0.011 -0.030 0.070 0.052 0.070 0.015 0.036
Gender γ - 0.019 -0.197 -0.097 -0.097 -0.082 -0.223 -0.240 -0.108 0.032 -0.091 -0.180 -0.142 -0.120 -0.159 -0.094
Ageγ - 0.265 -0.176 -0.224 -0.183 -0.206 0.023 -0.264 -0.171 -0.152 -0.165 -0.181 -0.197 -0.181 -0.210 -0.177Pγ - 0.833 -0.281 -0.304 -0.288 -0.319 0.106 -0.407 -0.293 - 0.131 -0.286 -0.292 -0.287 -0.231 -0.354 -0.236
Table 1: Experiment Results. Utility scores include the averaged CS, BLEU, and PPL scores for the author10 and
topic10 datasets. Author/Gender/Age F1 indicate the adversarial performance on the authorship, gender, and age
classification tasks, for both the static (s) and adaptive (a) settings. We report a modified version of Relative Gain (γ)
for each setting, as explained in Section 4.3. The best cumulative γscore is bolded for each comparative parameter.
5 Discussion
In analyzing the results, we first discuss the merits
of DP text privatization. At stricter privacy bud-
gets (lower ε), only the original DP-P ROMPT is
able to present significant gains, as showcased with
ε= 25 . At these lower values, one can also ob-
serve the benefits of enforcing DP via logit clip-
ping, which results in higher CS and BLEU reten-
tion while outputting generally more readable text
(much lower PPL). This trend with PPL holds for
all scenarios of DP vs. Quasi-DP, making a clear
case for proper bounding in DP applications.
In studying DP vs. Quasi-DP further, we notice
that the distinction between the two, particularly
at higher εvalues, becomes somewhat opaque. In
fact, Quasi-DP outperforms DP in terms of empir-
ical privacy in many of the higher privacy budget
scenarios. This would imply that a DP mechanism
leveraging temperature sampling only becomes ef-
fective and sensible with stricter privacy budgets.
An important point of comparison also comes
with the study results of our Non-DP method. A
strength of this method is highlighted by its abil-
ity at lower kvalues (analogous to less strict pri-
vacy budgets) to maintain high levels of seman-
tic similarity (CS), while still achieving compet-
itive empirical privacy scores. For example, in
the case of k= 3, this method is able to outper-
form all ε≥100for both DP and Quasi-DP. The
BLEU scores for Non-DP would also imply that
this method is better able to rewrite texts in a se-
mantically similar, yet lexically different manner,
as opposed to DP methods at high εvalues (see
Appendix D). These results make a case for Non-
DP privatization in certain cases, and in parallel,
provide a critical view of using DP at high εvalues
which lead to ineffective empirical privacy.A final point that is crucial to discuss is grounded
in the observed relative gains. Looking to the cumu-
lative scores (Pγ) of Table 1, one can notice that
the only positive gains are observed at relatively
lowεvalues, implying that only at these levels do
the empirical privacy protections begin to outweigh
the losses in utility. The utility scores in these cases,
however, are quite difficult to justify in real-world
scenarios, where semantic similarity is quite low
and readability suffers greatly. These results in gen-
eral showcase the harsh nature of the privacy-utility
trade-off, where mitigating adversarial advantage
often comes with less usable data.
6 Conclusion
Central to this work is the debate on the merits of
Differential Privacy in NLP. To lead this discussion,
we conduct a case study with the DP-P ROMPT
mechanism, juxtaposed with two “relaxed” vari-
ants. Our results show that while the theoretical
guarantee of individual privacy may be important in
some application settings, in others, it may become
too restrictive to apply effectively. Conversely, the
merits of DP may be observed in stricter privacy
scenarios, where the need for tight guarantees does
bring favorable privacy-utility trade-offs.
We call for further research in two directions:
(1) rigorous studies on the theoretical and practical
implications of DP vs non-DP privatization, and
relatedly, (2) the continued design of privatization
mechanisms outside the realm of Differential Pri-
vacy that aim to balance strong privacy protections
with practical utility preservation. We hope that
researchers may be able to harmonize the “best of
both worlds”, keeping in sight the need for practi-
cally usable privacy protection of text data.
4Acknowledgments
The authors thank Alexandra Klymenko and
Maulik Chevli for their contributions to this work.
Limitations
The foremost limitation of our work comes with
the selection of a single base model for use with
FLAN -T5-BASE . While further testing should be
conducted on other (larger) models, we hold that
our results can be generalized, since model choice
was not central to our findings. Another limitation
is the choice of ε(i.e., temperature) and kvalues,
which were not selected in any rigorous manner,
but rather based on the relative range of values
presented in Utpala et al. (2023). The effect of
parameter values outside of our selected ranges
thus is not explored in this work.
Ethics Statement
An ethical consideration of note concerns our em-
pirical privacy experiments, which leverage an ex-
isting dataset (Blog Authorship) not originally in-
tended for adversarial classification. In performing
these empirical experiments, the actions of a po-
tential adversary were simulated, i.e., to leverage
publicly accessible information for the creation of
an adversarial model. As this dataset is already
public, no harm was inflicted in the privacy experi-
ments as part of this work. Moreover, the dataset is
made up of pseudonyms (Author IDs) rather than
PII, thus further reducing the potential for harm.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Bren-
dan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. 2016. Deep learning with differential pri-
vacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Secu-
rity, CCS ’16, page 308–318, New York, NY , USA.
Association for Computing Machinery.
Haohan Bo, Steven H. H. Ding, Benjamin C. M. Fung,
and Farkhund Iqbal. 2021. ER-AE: Differentially
private text generation for authorship anonymization.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 3997–4007, Online. Association for Computa-
tional Linguistics.
Hannah Brown, Katherine Lee, Fatemehsadat
Mireshghallah, Reza Shokri, and Florian Tramèr.
2022. What does it mean for a language model
to preserve privacy? In Proceedings of the 2022ACM Conference on Fairness, Accountability,
and Transparency , FAccT ’22, page 2280–2292,
New York, NY , USA. Association for Computing
Machinery.
Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021. Extracting training data from
large language models. In 30th USENIX Security
Symposium (USENIX Security 21) , pages 2633–2650.
Ricardo Silva Carvalho, Theodore Vasiloudis,
Oluwaseyi Feyisetan, and Ke Wang. 2023. TEM:
High utility metric differential privacy on text.
InProceedings of the 2023 SIAM International
Conference on Data Mining (SDM) , pages 883–890.
SIAM.
Sai Chen, Fengran Mo, Yanhao Wang, Cen Chen, Jian-
Yun Nie, Chengyu Wang, and Jamie Cui. 2023. A
customized text sanitization mechanism with differ-
ential privacy. In Findings of the Association for
Computational Linguistics: ACL 2023 , pages 5747–
5758, Toronto, Canada. Association for Computa-
tional Linguistics.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models. Preprint , arXiv:2210.11416.
Cynthia Dwork. 2006. Differential privacy. In Inter-
national colloquium on automata, languages, and
programming , pages 1–12. Springer.
Natasha Fernandes, Mark Dras, and Annabelle McIver.
2019. Generalised differential privacy for text doc-
ument processing. In Principles of Security and
Trust: 8th International Conference, POST 2019,
Held as Part of the European Joint Conferences
on Theory and Practice of Software, ETAPS 2019,
Prague, Czech Republic, April 6–11, 2019, Proceed-
ings 8 , pages 123–148. Springer International Pub-
lishing.
Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu,
and Nathanael Teissier. 2021. Research challenges in
designing differentially private text generation mech-
anisms. In The International FLAIRS Conference
Proceedings , volume 34.
Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and
Tom Diethe. 2020. Privacy- and utility-preserving
textual analysis via calibrated multivariate perturba-
tions. In Proceedings of the 13th International Con-
ference on Web Search and Data Mining , WSDM ’20,
5page 178–186, New York, NY , USA. Association for
Computing Machinery.
James Flemings and Murali Annavaram. 2024. Differ-
entially private knowledge distillation via synthetic
text generation. In Findings of the Association for
Computational Linguistics ACL 2024 , pages 12957–
12968, Bangkok, Thailand and virtual meeting. As-
sociation for Computational Linguistics.
Ivan Habernal. 2021. When differential privacy meets
NLP: The devil is in the detail. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 1522–1528, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing. Preprint , arXiv:2111.09543.
Lijie Hu, Ivan Habernal, Lei Shen, and Di Wang. 2024.
Differentially private natural language models: Re-
cent advances and future directions. In Findings
of the Association for Computational Linguistics:
EACL 2024 , pages 478–499, St. Julian’s, Malta. As-
sociation for Computational Linguistics.
Timour Igamberdiev and Ivan Habernal. 2023. DP-
BART for privatized text rewriting under local dif-
ferential privacy. In Findings of the Association for
Computational Linguistics: ACL 2023 , pages 13914–
13934, Toronto, Canada. Association for Computa-
tional Linguistics.
Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020.
Differentially private language models benefit from
public pre-training. In Proceedings of the Second
Workshop on Privacy in NLP , pages 39–45, Online.
Association for Computational Linguistics.
Oleksandra Klymenko, Stephen Meisenbacher, and Flo-
rian Matthes. 2022. Differential privacy in natural
language processing: The story so far. In Proceed-
ings of the Fourth Workshop on Privacy in Natural
Language Processing , pages 1–11, Seattle, United
States. Association for Computational Linguistics.
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang. 2023. Towards
general text embeddings with multi-stage contrastive
learning. Preprint , arXiv:2308.03281.
Justus Mattern, Fatemehsadat Mireshghallah, Zhijing
Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and
Taylor Berg-Kirkpatrick. 2023. Membership infer-
ence attacks against language models via neighbour-
hood comparison. In Findings of the Association for
Computational Linguistics: ACL 2023 , pages 11330–
11343, Toronto, Canada. Association for Computa-
tional Linguistics.
Justus Mattern, Benjamin Weggenmann, and Florian
Kerschbaum. 2022. The limits of word level differen-
tial privacy. In Findings of the Association for Com-
putational Linguistics: NAACL 2022 , pages 867–881,Seattle, United States. Association for Computational
Linguistics.
Frank McSherry and Kunal Talwar. 2007. Mechanism
design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science
(FOCS’07) , pages 94–103.
Casey Meehan, Khalil Mrini, and Kamalika Chaudhuri.
2022. Sentence-level privacy for document embed-
dings. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 3367–3380, Dublin,
Ireland. Association for Computational Linguistics.
Stephen Meisenbacher, Maulik Chevli, and Florian
Matthes. 2024a. 1-Diffractor: Efficient and utility-
preserving text obfuscation leveraging word-level
metric differential privacy. In Proceedings of the
10th ACM International Workshop on Security and
Privacy Analytics , IWSPA ’24, page 23–33, New
York, NY , USA. Association for Computing Machin-
ery.
Stephen Meisenbacher, Maulik Chevli, Juraj Vladika,
and Florian Matthes. 2024b. DP-MLM: Differen-
tially private text rewriting using masked language
models. In Findings of the Association for Com-
putational Linguistics ACL 2024 , pages 9314–9328,
Bangkok, Thailand and virtual meeting. Association
for Computational Linguistics.
Stephen Meisenbacher, Nihildev Nandakumar, Alexan-
dra Klymenko, and Florian Matthes. 2024c. A com-
parative analysis of word-level metric differential
privacy: Benchmarking the privacy-utility trade-off.
InProceedings of the 2024 Joint International Con-
ference on Computational Linguistics, Language
Resources and Evaluation (LREC-COLING 2024) ,
pages 174–185, Torino, Italia. ELRA and ICCL.
Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang.
2020. Privacy risks of general-purpose language
models. In 2020 IEEE Symposium on Security and
Privacy (SP) , pages 1314–1331.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics , ACL ’02, page 311–318, USA.
Association for Computational Linguistics.
Natalia Ponomareva, Jasmijn Bastings, and Sergei Vas-
silvitskii. 2022. Training text-to-text transformers
with privacy guarantees. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2022 ,
pages 2182–2193, Dublin, Ireland. Association for
Computational Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI .
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using siamese BERT-
networks. CoRR , abs/1908.10084.
6Jonathan Schler, Moshe Koppel, and Shlomo Argamon.
2006. Effects of age and gender on blogging. In
AAAI spring symposium: Computational approaches
to analyzing weblogs , volume 6, pages 199–205.
Saiteja Utpala, Sara Hooker, and Pin-Yu Chen. 2023.
Locally differentially private document generation
using zero shot prompting. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2023 , pages 8442–8457, Singapore. Association for
Computational Linguistics.
Benjamin Weggenmann and Florian Kerschbaum. 2018.
SynTF: Synthetic and differentially private term fre-
quency vectors for privacy-preserving text mining.
InThe 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval ,
pages 305–314.
Benjamin Weggenmann, Valentin Rublack, Michael An-
drejczuk, Justus Mattern, and Florian Kerschbaum.
2022. DP-V AE: Human-readable text anonymization
for online reviews with differentially private varia-
tional autoencoders. In Proceedings of the ACM Web
Conference 2022 , WWW ’22, page 721–731, New
York, NY , USA. Association for Computing Machin-
ery.
Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,
Huan Sun, and Sherman S. M. Chow. 2021. Dif-
ferential privacy for text analytics via natural text
sanitization. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021 , pages
3853–3866, Online. Association for Computational
Linguistics.
A Blog Dataset Preparation
We outline the process of dataset preparation for
the data used in this work. All prepared datasets
are made available in our code repository.
We begin with the corpus made available by
Schler et al. (2006), which contains 681,284 blog
posts from 19,320 authors and across 40 topics. In
particular, we use the version made publicly avail-
able on Hugging Face2. In this version, each blog
post is labeled with a topic , which we learned trans-
lates to the career field of the corresponding author.
Upon an initial survey, we noticed that a significant
amount of blogs are labeled with indUnk , so these
were filtered out. In addition, one of the topics
named Student did not seem to have coherent blog
content in terms of topic, so these blogs were also
removed. These steps resulted in a filtered corpus
of 276,366 blogs.
Next, noticing that out of all the “topics”, many
contained very few blogs, we only considered blogs
2https://huggingface.co/datasets/tasksource/
blog_authorship_corpuswith topics in the top 15 most frequently occurring
topics. We also only consider blog posts with a
maximum of 256 tokens, both for performance
reasons and also to remove outliers (very long blog
posts). These two stops resulted in a further filtered
set of 162,584 blogs.
To prepare the author10 dataset, we considered
the 10 most frequently blogging authors in the fil-
tered corpus. This translates to authors writing
between 1001 and 2174 distinct blog posts, for a
total of 15,070 blogs in the author10 dataset.
To prepare the topic10 dataset, we only consider
blog posts from the filtered corpus which count in
the top 10 most frequently occurring topics. Con-
cretely, this consists of the following topics (from
most to least frequent): Technology ,Arts,Educa-
tion,Communications-Media ,Internet ,Non-Profit ,
Engineering ,Law,Science , and Government . With
these topics, we take a 10% sample of the filtered
corpus, resulting in a dataset of 14,259 blogs. Tech-
nology is the most frequent topic in this dataset
with 3409 blogs, with Government the least fre-
quent at 485 blogs.
While the gender attribute is not altered in the
topic10 dataset, we bin the ageattribute for a more
reasonable classification task. We choose to create
five bins from the agecolumn, which ranges from
the age of 13 to 48. Creating an even split between
all age bins, we achieve the following bin ranges:
(13.0, 23.0] < (23.0, 24.0] < (24.0, 26.0] < (26.0, 33.0] < (33.0, 48.0]
Thus, the resulting topic10 dataset contains 10
topics, 2 genders, and 5 age ranges.
B DP-P ROMPT Implementation Details
We implement DP-P ROMPT by following the de-
scribed method in the original paper (Utpala et al.,
2023). As noted, we leverage the FLAN -T5-BASE
model as the underlying LM.
To set the clipping bounds for our method, we
run 100 randomly sampled texts from our dataset
through the model and record all logit values.
Then, we set the clipping range to ( logit _mean ,
logit _mean + 4·logit _std) = (-19.23, 7.48), as
noted in the paper.
For the prompt template, we use the same simple
template as used by Utpala et al. (2023), namely:
Document: [ORIGINAL TEXT] Paraphrase of Document:
As discussed in the original paper, we do not
change the top-k parameter for DP-P ROMPT in its
output generation, both for the DP and Quasi-DP
7settings. This is left to the default Hugging Face
parameter of k= 50 .
Finally, for comparability, we limit the maxi-
mum generated tokens for all methods to 64.
For all privatization scenarios, we run DP-
PROMPT (and its variants) on a NVIDIA RTX
A6000 GPU, with an inference batch size of 16.
The source code for replication can be found
at the following repository, which also includes
our two prepared datasets used in the experiments:
https://github.com/sjmeis/DPNONDP
C Training Parameters
For all training performed as part of our empirical
privacy experiments, we utilize the Hugging Face
Trainer library for model training. All training
procedures use default Trainer parameters, except
for a training batch size of 64 and validation batch
size of 128. Dataset splits are always shuffled with
a random seed of 42 prior to training or validation.
All training is performed on a single NVIDIA RTX
A6000 GPU.
D Examples
Tables 2 and 3 provide rewriting examples for all
tested parameters for a selected text sample from
each of our two datasets.
8Original text Food for poor ol’ me as well!!!!! Yesterday was absolutely great. Not only did I get 30 for doing 5 mins
maintenance/repair on someones PC, but I had a great night out too! Can’t get any better than that. As
my wallet has been ’topped-up’ *smug* I can now afford my Only Fools And Horses DVD, Max Payne
II (when it comes out in about a month) and I can also take Nic out for a classy meal when I get round to
it one weekend. Corny. I have also done most of my homework so far. I’m hardly lagging which gets me
in a happier mood :) P.S: BRAND new absolutemaxpayne.co.uk due for release in about 3 weeks (just
ε/kbefore release, how convenient). Kinky.
25 Baggy Boggianylines Show Baseografic Direct Withinlui Because Parallel Gate shows OilNamen Mini
Pilot Officeproductive evaluated analyse Belgium divorce Index geometric An integrated achievecliinter
Madrid game Facilityuillonture GUI ensue diese lawyeradvisable devenir £1 Claus farbene Committee
of Government housing Fulllink/ Staffdangger Zugriff staatlich finden Bow
50 the food started okay, was slow so I renovated a locker monday.
DP 100 Yesterday was great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s PC, but I
had a great night out too! Can’t get any better than that! As my wallet has been ’topped-up’ *smug* I
can now afford
150 I’m in a better mood now.
200 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s
PC, but I had a great night out too! Can’t get any better than that. As my wallet has been ’topped-up’
*smug* I can now
250 I’m so happy with my life.
25 Average country studying
50 Just had a nice night at my new laptop store. It’s been showering and saving my cancelled vacation for
the good of the Bay Area and improving my financial position back down in Seattle.
100 Yesterday was a really good day. I got 30 for doing 5 mins maintenance/repair on someone’s PC, but I
had a great night out too! Can’t get any better than that! As my wallet has been ’topped-up’ *smug* I
can now
Quasi-DP 150 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someones
PC, but I had a great night out too! Can’t get any better than that. As my wallet has been ’topped-up’
(*smug*) I can
200 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s
PC, but I had a great night out too! Can’t get any better than that. As my wallet has been ’topped-up’
*smug* I can now
250 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s
PC, but I had a great night out too! Can’t get any better than that. As my wallet has been ’topped-up’
*smug* I can now
50 yesterday was basically great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s
PC, but I had a great evening out too! Can’t get any better than that...
25 Today he got me some great news
10 I’ve had an amazing weekend.
Non-DP 5 I have to get some money to buy a DVD, Max Payne II and eat dinner for Nic.
3 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someones
PC but I had a great night out too. As my wallet has been ‘topped-up’ ... I can now afford my Only Fools
And Horses DVD, Max Payne
1 Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone’s
PC, but I had a great night out too! Can’t get any better than that. As my wallet has been ’topped-up’
*smug* I can now
Table 2: Rewritten examples from the author10 dataset.
9Original text My mother came over on Sunday and brought a ton of baby stuff that she has either bought new or found
at consignment shops (she has better ones where she lives). It was fun looking at all the PINK things
she bought! Then she gave me some dusters to wear that my grandmother had and never wore. They
are so comfortable it is almost unbelievable! Want to know what a duster is? ROFLMAO It’s a polite
word for a MUU-MUU! And I don’t care! For the first time in my life I am actually happy to be wearing
a muu-muu... Is it old age or senility? Who knows. All I know is that I am very comfortable. ((Poor
Sharky is convinced that this is a plot of my mothers to insure that I NEVER get pregnant again. I mean,
ε/kthese are muu-muus in all the prerequisite colors and patterns.))
25 Leaf miserable Astro FIRST actress Nachlac Pitt came over lending Judesc headset recently aleappa-
rafterv disk album popcorn to Conservative Job Today quest necessity when Ellenm at Funeral seen
Vilnton les grisil set transtourotherquartaine abruptapathetic Boot Vacation betting lieben analysis Travail
Emperor LEWhether Fantasy climatique trop torrent aus
50 Jade is comfortable in red nude
DP 100 A mother came over on Sunday and brought a lot of baby stuff, some dusters that her grandmother never
wore, and a muu-muus, the first time in her life I am happy to be wearing a muu-muus.
150 My mother brought me some baby stuff that she never wore.
200 My mother brought me some baby dusters that my grandmother never wore.
250 My mother brought me some baby dusters that her grandmother never wore.
25 Grantment 2010. Onh Sar asking State430 unstable 13. 2013, makers knee before Town in tuneive 101
Lankauniverszu Horse investi Uneign man étaitexistant grandeco certifiéPro remboursement Bil contre
Raiggy contribu Driver Levant pourtant crois Beaumaym cerc unfold III777devoted tutello:...me allou
diplomat Me calls warrant
50 The mother brought everything to the baby that she could find, from pillows to throw pillows. It was fun
and time wass. The first time I am happy wearing a popular MU-MU’ frisder.
Quasi-DP 100 My mother brought me a lot of baby stuff to look at.
150 My mother brought me a ton of baby stuff, and she gave me some dusters that her grandmother never
wore.
200 I was wearing a muu-muus and my mother gave me some.
250 The mother brought her a ton of baby stuff.
50 There were a lot of baby clothes that my mom had before she wore all these dusters.
25 My grandmother brought a lot of baby stuff from their home and they had to get a duster for the first
time in her life. She said hers were nice to look at and they were comfortable. But it’s not the same as
having a diaper.
Non-DP 10 She brought my mother a lot of baby stuff, and gave me some new dusters. They’re so comfortable they
are almost unbelievable.
5 The mother gave me some muu-muus to wear for the first time in my life.
3 My mother brought me some dusters to wear that my grandmother never wore.
1 My mother brought me some baby dusters that my grandmother never wore.
Table 3: Rewritten examples from the topic10 dataset.
10