SignCLIP: Connecting Text and Sign Language by Contrastive Learning
Zifan Jiang, Gerard Sant, Amit Moryossef,
Mathias Müller, Rico Sennrich, Sarah Ebling
University of Zurich
jiang@cl.uzh.ch
Abstract
We present SignCLIP, which re-purposes CLIP
(Contrastive Language-Image Pretraining) to
project spoken language text and sign language
videos, two classes of natural languages of dis-
tinct modalities, into the same space. SignCLIP
is an efficient method of learning useful visual
representations for sign language processing
from large-scale, multilingual video-text pairs,
without directly optimizing for a specific task
or sign language which is often of limited size.
We pretrain SignCLIP on Spreadthesign , a
prominent sign language dictionary consisting
of∼500 thousand video clips in up to 44 sign
languages, and evaluate it with various down-
stream datasets. SignCLIP discerns in-domain
signing with notable text-to-video/video-to-text
retrieval accuracy. It also performs competi-
tively for out-of-domain downstream tasks such
as isolated sign language recognition upon es-
sential few-shot prompting or fine-tuning.
We analyze the latent space formed by the spo-
ken language text and sign language poses,
which provides additional linguistic insights.
Our code and models are openly available1.
1 Introduction
Sign(ed) languages are the primary communication
means for ∼70 million deaf people worldwide2.
They use the visual-gestural modality to convey
meaning through manual articulations in combina-
tion with non-manual elements like the face and
body (Sandler and Lillo-Martin, 2006). Sign lan-
guage processing (SLP) (Bragg et al., 2019; Yin
et al., 2021) is a subfield of natural language pro-
cessing (NLP) that is intertwined with computer
vision (CV) and sign language linguistics.
SLP spans the tasks of sign language recogni-
tion (Adaloglou et al., 2021), translation (De Coster
1https://github.com/J22Melody/fairseq/tree/
main/examples/MMPT
2https://wfdeaf.org/our-work/
…
house<American SL>house<British SL>house<Japanese SL>I painted my house gray.<American SL>I am a housewife.<lithuanian SL>…
SignCLIPVideoEncoder
SignCLIPTextEncoderFigure 1: Illustration of SignCLIP, comprising a text
encoder and a video encoder jointly trained on pairs
of text and multilingual signing examples. Every sign
is articulated in diverse languages and contexts with
subtle differences in hand shape, movement, place of
articulation, etc. The screenshots of the videos are from
Spreadthesign and the matrix part is taken from CLIP.
et al., 2023), and production (Rastgoo et al., 2021).
Typical datasets, equipped with spoken language
text and sign language glosses in addition to videos,
that support SLP research are RWTH-PHOENIX-
Weather 2014T, in German Sign Language (DGS),
introduced by Forster et al. (2014); Camgoz et al.
(2018); and CSL-Daily, in Chinese Sign Language
(CSL), introduced by Zhou et al. (2021). However,
advances on a specific task/dataset/language are
often limited and non-transferable to more generic
and challenging settings (Müller et al., 2022, 2023)
due to the small, domain-specific vocabulary (1,066
and 2,000 signs, respectively) and data size (11 and
23 hours, respectively). Recent sign language cor-
pora with a scale of more than a thousand signing
hours have emerged for relatively high-resource
sign languages, e.g., BOBSL (Albanie et al., 2021)
for British Sign Language (BSL) and YouTube-
ASL (Uthus et al., 2024) for American Sign Lan-
guage (ASL), a subset of YouTube-SL-25 (Tanzer
and Zhang, 2024). JWSign (Gueuwou et al., 2023)
is also a notable corpus that consists of 2,530 hoursarXiv:2407.01264v2  [cs.CL]  6 Oct 2024of Bible translations in 98 sign languages.
In the meanwhile, outside the world of SLP, there
is great progress in deep pretrained models of differ-
ent modalities, GPT (Achiam et al., 2023) for text,
masked autoencoders (He et al., 2022) for images,
and wav2vec 2.0 (Baevski et al., 2020) for speech,
to name a few. They are commonly pretrained on a
huge amount of data (e.g., over 15 trillion tokens
for Llama 3, Touvron et al. (2023)) with very little
or weak supervision, but present striking multilin-
gual and multi-task ability, for zero-shot prediction,
fine-tuning, and representation learning.
Ideally, the visual aspect of sign languages, i.e.,
lexical similarity due to iconicity (Johnston and
Schembri, 2007) (illustrated in Figure 1) makes
transferring between different sign languages easier
than the text of different spoken languages or even
scripts. The latter faces unfair tokenization issues
(Petrov et al., 2024) and out-of-vocabulary errors.
At the same time, unlike discrete text tokens (see
the comparison in Table 1), the dense continuous
video signal is expensive to process computation-
ally and seems daunting for the above-mentioned
self-supervised training approaches.
Since SLP tasks and datasets usually involve
both the text and visual/signed modalities, we take
inspiration from OpenAI’s CLIP model (Radford
et al., 2021) but use contrastive learning to connect
text with sign language videos instead of images.
For video understanding, follow-up work such as
VideoCLIP (Xu et al., 2021) mainly deals with
tasks including action recognition (Zhu et al., 2020)
and VideoQA (Xu et al., 2016; Yu et al., 2018).
However, both CLIP, VideoCLIP, and other existing
multimodal models understand visual content on
a coarse-grained level and generic domain and do
not address the intricacy of sign language. We
show the lack of sign language understanding in
contemporary AI models both intuitively (Figure 3
in Appendix A) and empirically (Table 2).
This work uses sign-language-specific data to
train a CLIP-like model for SLP. We first validate
the approach’s feasibility on fingerspelling, a sub-
system of sign language, by a model named Finger-
CLIP (§4), which correctly understands the finger-
spelling of individual letters. We then curate the
Spreadthesign3dictionary as a large-scale pretrain-
ing dataset consisting of ∼500 hours of signing,
as well as diverse public downstream task datasets
3https://www.spreadthesign.com/ . The use of the
data is under a license granted by Spreadthesign.to run comprehensive pretraining, fine-tuning, and
evaluation on full-fledged sign languages. By con-
trastive training on ∼500 thousand video-text pairs,
we obtain a multimodal and multilingual model
named SignCLIP (§5), illustrated by Figure 1. Sign-
CLIP excels at various SLP tasks and datasets and
presents a compelling latent space for signed video
content aligned with spoken language text (§6).
2 Background: Sign Language
Representation
Representation is a key challenge for SLP. Unlike
spoken languages, sign languages have no widely
adopted written form. As sign languages are con-
veyed through the visual-gestural modality, video
recording is the most straightforward way to cap-
ture them. The final goal of SignCLIP is to rep-
resent a sign language video clip by an embed-
ding that aligns with a ubiquitous text encoder like
Sentence-BERT (Reimers and Gurevych, 2019).
Generally, end-to-end training on the raw videos
is computationally costly, and various intermediate
representations alleviate this issue.
VideoCLIP and Video Encoders Our work
adapts VideoCLIP, which is pretrained by gen-
eral instructional videos from the HowTo100M
(Miech et al., 2019) dataset. We aim at replacing
HowTo100M with domain-specific sign language
videos, such as the dataset How2Sign (Duarte et al.,
2021), albeit on a considerably smaller scale.
Videos are very dense temporally (frame rate)
and spatially (video resolution). A 3D-CNN-based
video encoder is often used to extract informative
features with reduced dimensionalities for down-
stream tasks. VideoCLIP uses an S3D (Zhang et al.,
2018) model pretrained on HowTo100M that pro-
duces one video token (i.e., a video embedding for
the temporal window) per second. For SLP, it is
possible to use a video encoder pretrained specif-
ically on sign language videos. A prominent one
is the I3D (Carreira and Zisserman, 2017) model
pretrained on the BSL sign language recognition
task (Varol et al., 2021) with the BSK-1K dataset
(Albanie et al., 2020). A more recent approach to
simultaneously address temporal and spatial com-
plexity is the Video Swin Transformer proposed by
Liu et al. (2022), and Prajwal et al. (2022) trains
one such model for BSL fingerspelling recognition.
Pose Estimation A potentially more inter-
pretable and universal way of extracting signlanguage-related features from videos is human
pose estimation (Zheng et al., 2023a), for example,
using MediaPipe Holistic (Lugaresi et al., 2019;
Grishchenko and Bazarevsky, 2020). Each video
frame is converted into the location (X, Y , Z) of
543 full-body keypoints in a 3D space. However,
the immediate applicability of the pose estimation
systems for SLP is questionable (Moryossef et al.,
2021), and known issues such as the lack of accu-
rate depth information are presented (Holmes et al.,
2024). Generally, there is a trade-off between user-
friendliness and accuracy for pose estimation tools
and SLP researchers often prefer MediaPipe Holis-
tic for the former (Selvaraj et al., 2022) over others
like OpenPose (Cao et al., 2019) and AlphaPose
(Fang et al., 2022). An even more universal alter-
native approach to track keypoints named Tracking
Everything Everywhere All at Once is proposed by
Wang et al. (2023). Sevilla et al. (2024) uses a
similar CoTracker (Karaev et al., 2023) model to
study sign language prosody. Such models produce
smoother signals than traditional pose estimation.
Discrete Representation Non-standard written
forms of sign language, including SignWriting
(Sutton, 1990), HamNoSys (Prillwitz and Zienert,
1990), and glosses (Johnston, 2008), offer the pos-
sibility to incorporate sign language content into a
text-based NLP pipeline (Jiang et al., 2023). How-
ever, a good segmentation (Moryossef et al., 2023)
and transcription model to process raw video input
is first required, which is not well-researched and
is not part of this paper.
Recently, vector quantization (VQ) approaches
(Van Den Oord et al., 2017) such as SignVQNet
(Hwang et al., 2023) demonstrate the ability to con-
vert the continuous signal of videos/poses to dis-
crete tokens similar to spoken language sub-words
(Sennrich et al., 2016), which might be a promising
direction to pursue in future work.
Comparison In this work, we only empirically
experiment with the video encoder and pose-based
methods since there are not yet mature and open so-
lutions for the others at the time of writing. Given
a hypothetical 10-second, 30 FPS, 480p (640 ×480),
RGB (3 channels) video of 12 consecutive signs,
we compare the dimensionalities of the most com-
mon representations in Table 1. These approaches
compress the raw videos to a sequence of video
tokens compatible with a Transformer (Vaswani
et al., 2017) or a pretrained language model for
further training and processing (Gong et al., 2024).Representation Temporal Spatial Interpretable
Original video 10x30 640 ×480×3 –
S3D (HowTo100M) 10 512 no
I3D (BSL-1K) 10 1024 no
MediaPipe Holistic 10×30 543 ×3 yes
VQ (e.g., SignVQNet) 10 1024* no
SignWriting/HamNoSys/gloss 12 1024* yes
Table 1: Temporal and spatial dimensions of different
sign language representations for a 10-second, 30 FPS,
480p (640 ×480), RGB (3 channels) video consisting of
12 signs. In the parentheses are the datasets on which
the models are pretrained. For discrete representations,
we assume the embedding size to be 1024, marked with
an asterisk (*), but this can be chosen arbitrarily.
3 Model Architecture
We follow the setups in VideoCLIP and reuse their
codebase4. The most essential model architecture
with minor modifications to adapt our experiments
is described here. We take pairs of video and text
samples (v, t)as inputs, where for each video clip,
cvis a sequence of continuous video frames and
is processed by a video encoder fθve. This is then
followed by a trainable MLP projection layer, fθvp,
to project the embedding to the same dimension,
d= 768 , as the word embedding on the text side:
xv=fθvp(stopgrad (fθve(cv)) (1)
The frozen video encoder fθveis by default a
3D-CNN network but can be replaced by any other
visual backbone or a black-box pose estimation
system as summarized in Table 1. Likewise, text
token vectors xtare acquired through embedding
lookup from a frozen BERT model (Devlin et al.,
2019). Then xvandxtare fed into two separate
trainable Transformers, fθvandfθt, followed by
average pooling over the sequence of the tokens to
obtain the temporally aggregated embeddings:
zv=Avg(fθv(xv)), zt=Avg(fθt(xt))(2)
We optionally add two linear multimodal projec-
tion layers on top of zvandzt, which are missing
in VideoCLIP but present in CLIP (see Figure 3
of the CLIP paper). Finally, we employ the In-
foNCE loss (Oord et al., 2018) as the contrastive
objective to discern the relationship between the
embedded Nvideo-text pairs in each mini-batch
and run contrastive training over the whole dataset.
4https://github.com/facebookresearch/fairseq/
tree/main/examples/MMPT4 FingerCLIP
As a proof of concept, we first apply this contrastive
training approach to Fingerspelling (Battison, 1978;
Wilcox, 1992; Brentari and Padden, 2001), a sub-
system of sign languages heavily influenced by the
surrounding spoken languages. For concepts that
do not (yet) have associated signs (names of people,
locations, organizations, etc.), sign language users
borrow a word of a spoken language by spelling it
letter-by-letter with predefined signs for that lan-
guage’s alphabet. Isolated fingerspelling recogni-
tion5can therefore be considered a toy task simi-
lar to the MNIST (Deng, 2012) handwritten digits
classification task in CV . We name the model Fin-
gerCLIP, a mini-version of SignCLIP (§5).
Dataset We start with the RWTH German Finger-
spelling Database (Dreuw et al., 2006), containing
∼1400 videos of 35 DGS fingerspelling and num-
ber signs, five of which contain inherent motion.
We provide details and an illustration of the dataset
in Appendix B. We split all examples randomly into
training/validation/test sets at the ratio of 8:1:1.
Training Details We adhere to most implementa-
tion details outlined in VideoCLIP unless otherwise
specified. The two trainable Transformers, fθvand
fθt, are initialized with the pretrained bert-base-
uncased weights. We train 25 epochs within two
hours on a Tesla V100-SXM2-32GB GPU, vali-
dated by loss on the validation set. For contrastive
training, we construct each batch as a collection of
35 different signs with corresponding text prompts
“Fingerspell the letter <letter_name> in DGS.”. By
optimizing the InfoNCE loss, we move the embed-
ding of a sign closer to the paired text, and further
away from the remaining 34 negative examples.
We test different combinations of video en-
coders: (a) S3D HowTo100M video features6; (b)
I3D BSL-1K (M+D+A) video features7; (c) Medi-
aPipe Holistic pose estimation, and training strate-
gies: (a) zero-shot VideoCLIP (no training); (b)
fine-tuning VideoCLIP; (c) training from scratch.
MediaPipe Holistic runs offline on a low-end
CPU device. Pose estimation is normalized to a
5Continuous fingerspelling recognition is more complex
and is often solved by a CTC loss like speech recognition.
For more thorough research on fingerspelling, we recommend
the ChicagoFSWild+ dataset and Google’s ASL fingerspelling
recognition competition on Kaggle.
6https://github.com/antoine77340/S3D_HowTo100M
7https://www.robots.ox.ac.uk/~vgg/research/
bslattend/consistent scale by setting the mean width of each
person’s shoulders to 1, and the mid-point to (0,0).
The leg values are removed since they are irrele-
vant to signing. We further augment the data by
randomly rotating, shearing, and scaling the poses8.
Evaluation We view fingerspelling understand-
ing as a text-to-video/video-to-text retrieval task.
The candidates are ranked for both directions by
a dot-product-based similarity score to each tex-
t/video query in the latent space. For the test text
prompt of each sign, there is possibly more than
one correct video (e.g., the same letter signed by
different signers) in the test video pool, and they
are all considered successful retrieval. We thus
evaluate the text-to-video retrieval task by preci-
sion@k , i.e., what percent of the top k candidates
are correct answers. Each test video query has
only one correct text prompt out of the 35 possible
prompts. We thus evaluate the video-text retrieval
task by recall@k , i.e., the chance to include the
only correct answer by taking the top k candidates.
precision@1 andrecall@1 can be interpreted as
the retrieval accuracy, and in our video-to-text sce-
nario where there is only one relevant text item,
recall@k also equals top-n accuracy. We keep us-
ingrecall@k instead of accuracy in this paper for
genericity. We also add the metric median retrieval
rank, i.e., the median value of the rank of the first
correct answer in the candidate lists. We present
the experimental results in Table 2.
Discussion FingerCLIP distinguishes itself from
the supervised baseline method (Dreuw et al., 2006)
in that it is not directly optimized for classification.
Instead, a contrastive objective ties positive pairs
of text and videos by learning meaningful embed-
dings, which are then used for similarity-based
retrieval. We find video-to-text retrieval reasonably
more challenging than text-to-video retrieval9since
text-to-video retrieval is also often of less value, as
a trivial dictionary look-up does the job perfectly.
E1, zero-shot VideoCLIP, presenting random
guess results, shows that a common video under-
standing network pretrained on HowTo100M does
not necessarily address the nuance of sign language,
even simply as fingerspelling. Therefore, dedicated
training on sign language data is essential. Compar-
ingE1.1 andE1.2, neither is fine-tuning an existing
8The Pose library implements related operations.
9Note that the relatively low precision@5/10 values are
uncomparable to the high recall@5/10 values, since the former
makes the task harder, while the latter simplifies the task.Text-to-video Video-to-text
Experiment P@1 ↑P@5↑P@10↑MedianR ↓R@1↑R@5↑R@10↑MedianR ↓
E0 Dreuw et al. (2006) (supervised HMM, appearance-based features) – – – – 0.64 – – –
Explore training strategy
E1 VideoCLIP zero-shot (+ S3D HowTo100M video features) 0.03 0.02 0.03 22 0.02 0.14 0.28 18
E1.1 VideoCLIP fine-tuned (+ S3D HowTo100M video features) 0.40 0.36 0.30 2 0.31 0.75 0.89 2
E1.2 VideoCLIP trained from scratch (+ S3D HowTo100M video features) 0.54 0.35 0.28 1 0.28 0.69 0.87 3
Explore video-based (I3D) features
E2 FingerCLIP trained from scratch (+ I3D BSL-1K video features) 0.63 0.47 0.37 1 0.37 0.78 0.91 2
E2.1 E2 + feature dimension average pooled from 1024 to 512 0.74 0.56 0.44 1 0.47 0.82 0.94 2
Explore pose-based features
E3 FingerCLIP trained from scratch ( MediaPipe Holistic pose features) 0.89 0.67 0.42 1 0.68 0.97 1.00 1
E3.1 E3 + dominant hand features only (26 times less keypoints) 1.00 0.72 0.42 1 0.82 0.99 1.00 1
E3.2 E3.1 + 2D augmentation on pose features ( σ= 0.2) 0.91 0.74 0.43 1 0.93 1.00 1.00 1
Table 2: FingerCLIP experimental results evaluated on the test set. P@k denotes precision@k ,R@k denotes
recall@k , and MedianR denotes the median retrieval rank. The best score of each column is in bold. E0is taken
from Dreuw et al. (2006) as a baseline ( R@1 derived from the best error rate 35.7%).
VideoCLIP checkpoint helpful, so in the rest of the
paper, models are trained from scratch.
InE2,I3D BSL-1K sign-language-specific fea-
tures outperform HowTo100M S3D video features
(E1.2), especially when downsampled to the same
dimension as S3D ( E2.1).MediaPipe Holistic pose
estimation as a feature extractor ( E3) works better
than 3D-CNN-based video encoders, presumably
because it is more universal than an I3D model pre-
trained on a particular dataset and sign language.
For fingerspelling understanding, focusing on the
dominant hand ( E3.1 ) is beneficial10, which dras-
tically reduces the number of keypoints from 543
to 21. 2D data augmentation further improves the
overall performance. Since MediaPipe Holistic as
features perform the best and are more interpretable
and operable for potential data normalization and
augmentation, we decide to use it as the frozen
video encoder fθvefor the rest of the paper11.
5 SignCLIP Pretraining
To fully realize the power of contrastive learning,
we train and evaluate SignCLIP on larger and more
diverse sign language datasets. For efficient experi-
menting, we start exploring datasets consisting of
relatively short-duration sign language video ex-
amples, e.g., for the task of isolated sign language
recognition (ISLR) instead of machine translation.
In Table 3, we summarize recent large-scale sign
language datasets in this context, focusing on ASL,
one of the highest-resourced languages in SLP.
10Note that the DGS finger alphabet is one-handed.
11An S3D/I3D model fine-tuned end-to-end may overcome
some limitations of pose estimation and yield superior per-
formance for specific tasks. In this work, we trade pursuing
state-of-the-art numbers for the universality, interpretability,
and cheap computation of pose estimation.5.1 Spreadthesign Pretraining Dataset
Spreadthesign is used as the pretraining dataset
for its large-scale and multilingual nature12. In
this work, we limit the text translations to English
only to avoid a cartesian product number of data
points, for the pretraining to be economical and
sign-language-focused. After filtering English text,
our dataset consists of 18,423 concepts in 41 sign
languages, resulting in 456,913 video-text pairs
with a total duration of ∼500 hours. The data dis-
tribution is presented in Appendix C in detail. We
split all examples randomly into training, valida-
tion, and test sets at the ratio of 98:1:1. A caveat of
this dataset is that there is normally only one sign-
ing example per text concept per sign language,
which means the pretraining is prone to overfitting
the exact signs by particular signers. We still be-
lieve that the diverse text-signing pairs will guide
the model to learn a useful visual representation.
We add the Spreadthesign data scale and that of
CLIP and VideoCLIP to Table 3 for comparison.
CLIP was trained on 400 million text-image pairs
collected from the Internet (500K queries and up
to 20K pairs per query); VideoCLIP was pretrained
on 1.2 million videos from HowTo100M (each lasts
∼6.5 minutes with ∼110 clip-text pairs). We addi-
tionally add ImageNet (Deng et al., 2009), which
has a relatively closer scale to Spreadthesign.
5.2 Training and Evaluation Details
Most implementation details and evaluation proto-
cols in FingerCLIP (§4) are reused for SignCLIP.
The text prompts now consist of the text content
12https://www.spreadthesign.com/en.us/about/
statistics/ . The data used in this work was crawled in
2023 and might differ slightly from the official statistics.Dataset Language Type/Task #examples #signs/#classes #signers
RWTH German Fingerspelling (Dreuw et al., 2006) DGS Isolated Fingerspelling 1,400 35 20
ChicagoFSWild (Shi et al., 2018) ASL Continuous fingerspelling 7,304 – 160
ChicagoFSWild+ (Shi et al., 2019) ASL Continuous fingerspelling 55,232 – 260
Google – American Sign Language Fingerspelling Recognition ASL Continuous fingerspelling 67,208 – 100
WLASL (Li et al., 2020) ASL ISLR 21,083 2,000 100
ASL Citizen (Desai et al., 2023) ASL ISLR 83,399 2,731 52
Sem-Lex (Kezar et al., 2023) ASL ISLR 91,148 3,149 41
Google – Isolated Sign Language Recognition (i.e., asl-signs ) ASL ISLR 94,477 250 21
Google – PopSign ASL v1.0 (Starner et al., 2024) ASL ISLR 200,686 250 47
How2Sign (Duarte et al., 2021) ASL Continuous signing 35,000 16,000 11
ASL-LEX (Sehyr et al., 2021) ASL Dictionary (phonological) 2,723 2,723 (unknown)
Spreadthesign (SignCLIP, our filtered version) Multilingual Dictionary 456,913 18,423* (unknown)
ImageNet (Deng et al., 2009) – Image classification 1,431,167 1,000 –
HowTo100M (Miech et al., 2019) (VideoCLIP) – Video understanding 136,000,000 – –
CLIP (Radford et al., 2021) – Contrastive learning 400,000,000 – –
Table 3: Summarization of datasets consisting of relatively short-duration video examples, compared with Spreadthe-
sign and common CV datasets. SignCLIP has been tested with the datasets marked with a checkmark. asl-signs is a
subset of PopSign ASL v1.0 .#signs/#classes for Spreadthesign is marked with an asterisk (*) since the signs of a
concept across different sign languages are barely classified as one sign.
prepended with a spoken and a sign language tag,
inspired by multilingual machine translation in
Johnson et al. (2017). For example, the text prompt
for signing the phrase “Hello, can I help you?”
in ASL is “<en> <ase> Hello, can I help you?”,
tagged by the ISO 639-3 language code. Fitting
most examples, we limit the context length of the
video Transformer fθvto 256 tokens, equivalent
to a 10-second, 25 FPS video clip; and that of the
text Transformer fθtto be 64. Both Transformers
are initialized with the pretrained bert-base-cased
weights and trained on an NVIDIA A100-SXM4-
80GB GPU to maximally afford a batch size of
44813. We also measure the training efficiency by
the number of parameters and the training time.
For evaluation, we include the same video-to-
text metrics used in FingerCLIP and omit text-to-
video for simplicity, as the latter correlates with and
is more trivial than the former. We additionally test
the models with three recent ASL ISLR datasets in
a zero-shot way to evaluate out-of-domain general-
ization. We perform the following text preprocess-
ing to mitigate the effect of shifted text distribution
when building text prompts from the raw gloss la-
bels: (1) lowercasing the glosses; (2) removing
the gloss index for different variants of a sign14;
(3) filtering their test sets by known text labels in
Spreadthesign, which reduces the total number of
13For reference, CLIP was trained with batch size 32,768
and VideoCLIP was trained with batch size 512.
14This operation might not be desired if the objective is sign
classification as different variants of a sign present different
visual forms. The main goal here is to test how well the sign
embeddings align to text semantically regardless of the form.signs/classes from the original test datasets.
Starting from a baseline setup that resembles E3
in FingerCLIP, we increase the video Transformer
layers from 6 to 12 and try linear multimodal pro-
jection layers after temporal pooling. For pose data,
we always simplify the face by using the contour
keypoints only, resulting in 203 keypoints. We fur-
ther experiment with the following modifications:
Pose Data Preprocessing Before the regular nor-
malization, Dshoulders = 1, mid –point = (0,0),
as performed in all experiments, we further try
(1) removing redundant keypoints and reposition-
ing the wrist to the hand model’s prediction ( E6);
(2) standardizing the keypoints by subtracting the
mean pose values of all examples from Spreadthe-
sign and dividing by the standard deviation ( E6.1);
(3) anonymizing by removing the appearance from
the first frame then adding the mean pose ( E6.2).
Pose Data Augmentation After data normaliza-
tion, we also employ pose-based data augmentation
inspired by Bohá ˇcek and Hrúz (2022) at training
time to improve the models’ robustness, including
(1) randomly flipping the poses horizontally; (2)
2D spatial augmentation as done in FingerCLIP;
(3) temporal augmentation of the signing speed by
linear interpolation between frames; (4) Gaussian
noise on keypoints.
5.3 Experimental Results and Discussion
We present the experimental results in Table 4. In
this scenario, an accurate retrieval is harder than
FingerCLIP because there are 3,939 unique text
prompts in the test set of 4,531 examples. TheVideo-to-text (In-domain) Video-to-text (out-of-domain) Efficiency
Experiment R@1 ↑R@5↑R@10↑MedianR ↓AS MedianR ↓AC MedianR ↓SL MedianR ↓#Params ↓Time↓
E4 Baseline 0.33 0.64 0.77 3/3939 103/213 253/1625 455/1967 175M 29h
Initial architectural changes
E5 E4 + six more video layers 0.37 0.68 0.80 2 104 192 382 217M 28h
E5.1 E5 + multimodal projection layer 0.38 0.69 0.80 2 104 216 418 218M 15h
Pose data preprocessing
E6 E5 + keypoint reduction & reposition 0.37 0.68 0.80 2 105 230 665 217M 32h
E6.1 E6 + keypoint standardization 0.40 0.71 0.83 2 99 273 551 217M 14h
E6.2 E6.1 + pose anonymization 0.37 0.68 0.79 2 101 251 577 217M 40h
Pose data augmentation
E7 E5 + pose random flipping ( p= 0.2) 0.36 0.67 0.79 2 105 200 435 217M 29h
E7.1 E5 + spatial 2D augmentation ( σ= 0.2) 0.35 0.65 0.78 3 102 219 377 217M 39h
E7.2 E5 + temporal augmentation ( σ= 0.2) 0.39 0.69 0.80 2 104 187 372 217M 62h
E7.3 E5 + Gaussian noise ( σ= 0.001) 0.37 0.68 0.80 2 104 198 364 217M 29h
Test-time-only normalization
E8* E7.2 + flipping to right-handed 0.39 0.69 0.80 2 103 187 359 – –
E8.1* E8 + pose anonymization (zero-shot-only) – – – – 101 214 380 – –
Table 4: SignCLIP experimental results evaluated on the test set. R@k denotes recall@k , and MedianR denotes the
median retrieval rank as well as the total number of unique signs/classes. AS=asl-signs ,AC=ASL Citizen ,SL=
Sem-Lex . Experiments marked with an asterisk (*) are test-time only. The best score of each column is in bold.
in-domain results, E6.1 at the top (attained with in-
creased video layers and keypoint standardization),
are impressive given the challenging nature. We at-
tribute it to (1) the hypothesized multilingual trans-
fer effect thanks to sign language iconicity; and
(2) the broader supervision signal of contrastive
learning than fixed labels, coming from example
phrases consisting of individual signs (Figure 1).
On the other hand, zero-shot performance on
out-of-domain data is deficient. We posit that to
reach noticeable performance on out-of-domain
data, few-shot learning or fine-tuning (§6.1) is es-
sential given the current scale of pretraining. Nev-
ertheless, starting from E7, we experiment with
a few data augmentation techniques to increase
data variation given that in-domain standardization
benefits in-domain results but hurts out-of-domain
results. We then attempt test-time-only normaliza-
tion to shift the test distribution of the poses closer
to training. As a result, we manage to gradually
fight against overfitting to the pretraining dataset
and improve the overall zero-shot performance by
temporal augmentation ( E7.2 ) and test-time pose
flipping if the right hand is not present ( E8). The
former provides robustness to signing speed change
in videos, and the latter is helpful for unseen test
examples signed by left-handed signers.
6 Downstream Tasks and Analysis
In this section, we evaluate SignCLIP on several
downstream tasks and datasets, and we discuss the
ideas for further enhancement and evaluation in §9.6.1 Isolated Sign Language Recognition
We provide a comprehensive evaluation for ASL
ISLR datasets in Table 5. For zero-shot predic-
tion, we follow the optimal setups in E8/E8.1 but
skip any text preprocessing on the raw glosses
for a fairer comparison. For few-shot learning,
we randomly sample 10 example videos for each
class from training data and use a nonparamet-
ric k-nearest neighbors (KNN) algorithm to in-
fer test labels based on pose embedding similar-
ity (n_neighbors = #classes ). We additionally
train a supervised logistic regression classifier with
all default settings offered by scikit-learn on top of
the embedded poses, also known as linear probe
(Alain and Bengio, 2016). We also train Sign-
CLIP models from scratch or fine-tune them from
theE7.2 checkpoint with the specific downstream
datasets for 25 epochs with batch size 256. Outliers
longer than 256 frames are removed.
In general, proper zero-shot prediction is hin-
dered by distribution shift on both modalities: (1)
forasl-signs dataset, zero-shot prediction is flawed
because we only have pose data normalized in
an unknown way from Kaggle instead of the raw
videos; (2) PopSign ASL , the superset of asl-signs
with raw videos, is released later and the evaluation
on it (category game only) is therefore superior and
more meaningful; (3) ASL Citizen uses all upper
case glosses and Sem-Lex uses snake case glosses,
both unseen for the text encoder during training.
Forasl-signs , we find pose anonymization eases
the domain shift issue in zero-shot ( E8.1 ) and
other settings. Besides, pose-based few-shot KNN
greatly improves the deficient zero-shot results,Data Model R@1↑R@5↑R@10↑MedianR ↓
ASSignCLIP (E8.1) zero-shot 0.01 0.03 0.05 118/250
SignCLIP (E8.1) 10-shot 0.01 0.03 0.06 109
SignCLIP (E8.1) + linear 0.12 0.30 0.41 17
SignCLIP train from scratch 0.74 0.91 0.94 1
SignCLIP fine-tuned 0.74 0.91 0.94 1
SignCLIP fine-tuned + linear 0.78 0.92 0.94 1
SOTA Kaggle competition 0.89* – – –
PSSignCLIP (E8) zero-shot 0.03 0.10 0.16 62/250
SignCLIP (E8) 10-shot 0.04 0.14 0.22 44
SignCLIP (E8) + linear 0.31 0.58 0.69 4
SignCLIP train from scratch 0.83 0.97 0.99 1
SignCLIP fine-tuned 0.84 0.97 0.98 1
SignCLIP fine-tuned + linear 0.85 0.97 0.98 1
SOTA PopSign ASL 0.84 – – –
ACSignCLIP (E8) zero-shot 0.00 0.00 0.00 1296/2731
SignCLIP (E8) 10-shot 0.05 0.16 0.23 56
SignCLIP (E8) + linear 0.23 0.47 0.57 7
SignCLIP train from scratch 0.39 0.71 0.80 2
SignCLIP fine-tuned 0.46 0.77 0.84 2
SignCLIP fine-tuned + linear 0.60 0.84 0.89 1
SOTA ASL Citizen 0.63 0.86 0.91 –
SLSignCLIP (E8) zero-shot 0.01 0.02 0.03 853/3837
SignCLIP (E8) 10-shot 0.02 0.06 0.10 235
SignCLIP (E8) + linear 0.15 0.29 0.36 38
SignCLIP train from scratch 0.14 0.30 0.38 32
SignCLIP fine-tuned 0.16 0.34 0.42 22
SignCLIP fine-tuned + linear 0.30 0.48 0.55 6
SOTA Sem-Lex 0.67* – – –
SOTA + auxiliary phonology 0.69* – – –
Table 5: Comprehensive evaluations of SignCLIP on
the test set of ASL ISLR datasets. For ISLR, recall@k
equals top-n accuracy. AS=asl-signs ,PS=PopSign
(the superset of AS),AC=ASL Citizen ,SL=Sem-Lex .
The best score SignCLIP achieves on each dataset is
in bold. SOTA numbers marked with an asterisk (*)
are not directly comparable to ours. SOTA Kaggle is
trained with asl-signs but tested on a private test set;
SOTA Sem-Lex is tested with a reduced test set of 2,731
classes that are aligned with ASL Citizen /ASL-LEX and
is thus considered an easier objective.
bypassing the influence of out-of-domain glosses.
Finally, fine-tuning SignCLIP with a pretrained
checkpoint, compared to training from scratch on
the target dataset, yields more competitive or even
better results than the state-of-the-art (SOTA) re-
ported in the previous literature.
6.2 Sign Language Identification
Sign language identification (Gebre et al., 2013)
can be achieved by simply ranking text prompts
of different sign languages without actual content,
e.g., “<en> <ase>” for ASL. We evaluate the best
checkpoint E6.1 for in-domain test data and obtain
0.99 recall@1 , which solves the task perfectly with-
out any direct supervision. However, the identifica-
tion task is ill-defined in that the model can learn to
identify signers for particular sign languages. This
task is considered a simplified version of in-domain
Figure 2: King – Man + Woman = Queen analogy re-
visited. 14 video examples of each sign are randomly
sampled from the ASL Citizen dataset, embedded by a
fine-tuned SignCLIP pose encoder, and then visualized
byt-SNE (perplexity=15) with different shapes and col-
ors. Cluster centers are represented with a big symbol.
video-to-text retrieval, as it eliminates the need to
simultaneously distinguish language and concept.
6.3 Latent Space Exploration
We make SignCLIP accessible via a RESTful API
and perform analysis in a Colab notebook15.
Distributional Hypothesis for Sign Language
We revisit the distributional hypothesis (Lenci and
Sahlgren, 2023) in a sign language context based
on the pose/sign embeddings instead of text/word
(Mikolov et al., 2013). As illustrated by Figure 2,
unlike a word with a discrete, unique text token,
each sign has multiple realizations scattered in a
continuous space. By aligning pose representation
to text with contrastive learning, we have realized
distributional semantics in sign language, reflected
by the cluster center of each sign, while maintain-
ing individual variance.
What Is the Most Iconic Sign Crosslingually?
Iconicity is one of the key motivations for training
a multilingual SignCLIP (Figure 1), and now we
ask this linguistic question back to the model. We
rank a sampled subset of 302 signs from Spreadthe-
sign based on the variance of the pose embeddings
across 20+ different sign languages. As a result,
the sign for “scorpion” with a universal hook hand
shape ranks at the top, and the motivational exam-
ple, i.e., the “house” sign, also ranks high (51/302).
Conversely, the signs for numbers rank low due
to the diverse signing styles across languages. We
append the full rank in Appendix D.
15https://colab.research.google.com/drive/
1r8GtyZOJoy_tSu62tvi7Zi2ogxcqlcsz?usp=sharing7 Related Work
Contrastive learning has been increasingly used in
contemporary SLP work. We discuss some related
work in this session, which aims mainly at improv-
ing performance on dedicated tasks and datasets.
Therefore, their goals are slightly different than
SignCLIP, a universal pretrained sign language em-
bedding model aligned to spoken language text.
Gan et al. (2023); Zheng et al. (2023b); Ye et al.
(2024) focus on the small-scale RWTH-PHOENIX-
Weather 2014T and CSL-Daily datasets. Gan et al.
(2023) designs a visual contrastive loss and a se-
mantic contrastive loss to tackle the CTC spike phe-
nomenon in the Continuous Sign Language Recog-
nition (CSLR) task and the exposure bias issue in
the Sign Language Translation (SLT) task. Zheng
et al. (2023b) proposes an explicit contrastive cross-
modal alignment between video frames and glosses
in combination with an implicit cross-modal align-
ment by a variational autoencoder (V AE) for CSLR.
Ye et al. (2024) identifies a visual representation
density issue in SLT and introduces a frame-wise
contrastive learning strategy to alleviate the issue
and improve SLT.
Wong et al. (2023) has a similar sign embed-
ding idea as SignCLIP, namely Learnt Contrastive
Concept , that aligns with word embeddings of the
linguistic labels for sign video and incorporates this
into ISLR pipelines for the WLASL and BOBSL
datasets. Raude et al. (2024), on the other hand,
tackles CSLR on BOBSL by a multi-task setting,
including two contrastive losses for sign-level and
sentence-level retrieval, respectively. They show
that joint training for CSLR and sign language re-
trieval is mutually beneficial.
Like our pretraining task framing introduced in
§4, Cheng et al. (2023) explicitly formulates text-
to-video/video-to-text retrieval as a cross-lingual
contrastive learning task. They address the data
scarcity issue by combining a domain-agnostic and
a domain-aware video encoder and show its effec-
tiveness on How2Sign and PHOENIX-2014T.
8 Conclusion: Where Are We for SLP?
This work involves SLP, an interdisciplinary field
that suffers from low-resourceness compared to
mainstream NLP and CV . To overcome the non-
generalizability of a specific dataset/task/language,
we adapt (Video-)CLIP and propose SignCLIP.
SignCLIP is trained on Spreadthesign , a multilin-
gual, generic sign language dictionary consistingof∼500 hours of signing in 41 sign languages, and
is evaluated extensively for various purposes.
SignCLIP demonstrates excellent in-domain per-
formance but falls short of immediate zero-shot
prediction on downstream ISLR tasks. This finding
is consistent with previous CV studies (Li et al.,
2017) before CLIP reached its scale. Similar mod-
els trained on smaller datasets close to an Ima-
geNet scale performed much worse than supervised
baselines on common benchmarks. By comprehen-
sively evaluating downstream ASL ISLR perfor-
mance (Table 5), we also intend to shed some light
on data efficiency, as a fully supervised approach
usually requires meticulous, task-specific data col-
lection. This is more demanding for SLP, a niche
domain lacking human experts.
As a middle ground between full zero-shot pre-
diction and full supervision, few-shot learning or
fine-tuning is essential to tackle domain shift and
is more realistic given the present methodology
and data scale. The cross-lingual transfer effect is
prospective for sign language thanks to iconicity.
With no surprise, under a sensible data condition,
the universal pretraining paradigm that is trans-
forming NLP and CV is also a promising research
direction for SLP.
9 Limitations
The main limitation of this work is the data scale,
which interestingly is the primary breakthrough of
the original CLIP work. Apart from the inherent
data scarcity issue in SLP research, the concrete
limitations of SignCLIP come in a few aspects.
First, to be not dataset/task/language-specific
while possibly large-scale, we choose Spreadthe-
sign as our pretraining dataset (§5.1), which is in-
deed highly multilingual and untied to any SLP
task. However, we cannot release the dataset pub-
licly, and future researchers who are interested
in it have to first resolve or purchase the license
from Spreadthesign . Fortunately, it is possible to
augment or replace Spreadthesign with recently
released public datasets of comparable or larger
size, which should increase data density and vari-
ation sign-wise, but with potentially decreased di-
versity and balance among different sign languages.
Although pretrained with a highly multilingual
dataset, this paper focuses downstream task eval-
uation on ASL, one of the highest-recourse sign
languages. We leave further evaluation and explo-
ration of other sign languages to future work.Secondly, training large models on video data
is very costly. The principle of this work is to fin-
ish every training process in less than three days
on a single Nvidia A100 GPU, and we managed
this goal (Table 4) by using pose-based models, a
moderate context length of 256 frames, and limit-
ing spoken language to English only. An ablation
study on video length would be interesting to help
understand the limitations of the current static em-
bedding approach16. To scale up, multiple GPU
training can be exploited, and architectural modi-
fications that reduce the sequence length must be
employed, for which we refer to several techniques
discussed in §2.
As a result of the above-mentioned optimiza-
tions, we will be able to remove the limitation of
the relatively short context, and then train and eval-
uate for longer-range tasks such as machine transla-
tion and sign language production. This will make
SignCLIP more versatile and generalizable, as well
as support other types of deep pretrained networks
for SLP, such as large language models.
Acknowledgments
This work is funded by the Swiss Innovation
Agency (Innosuisse) flagship IICT (PFFS-21-47)
and by the SIGMA project at the UZH Digital So-
ciety Initiative (DSI).
We thank the (meta-)reviewers for their valuable
feedback. We also thank Colin Leong, Gomèr Ot-
terspeer, Jetske Adams, Oline Ranum, and Hamzah
Luqman for their immediate interest and lively dis-
cussion in our work.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Nikolas Adaloglou, Theocharis Chatzis, Ilias Papas-
tratis, Andreas Stergioulas, Georgios Th Papadopou-
los, Vassia Zacharopoulou, George J Xydopoulos,
Klimnis Atzakas, Dimitris Papazachariou, and Pet-
ros Daras. 2021. A comprehensive study on deep
learning-based methods for sign language recogni-
tion. IEEE Transactions on Multimedia , 24:1750–
1762.
16This is also relevant to the polysemy phenomenon be-
tween signs and words, i.e., one sign can mean more than one
word and one word can also correspond to more than one sign.Guillaume Alain and Yoshua Bengio. 2016. Under-
standing intermediate layers using linear classifier
probes. arXiv preprint arXiv:1610.01644 .
Samuel Albanie, Gül Varol, Liliane Momeni, Triantafyl-
los Afouras, Joon Son Chung, Neil Fox, and Andrew
Zisserman. 2020. BSL-1K: Scaling up co-articulated
sign language recognition using mouthing cues. In
European Conference on Computer Vision (ECCV) .
Samuel Albanie, Gül Varol, Liliane Momeni, Hannah
Bull, Triantafyllos Afouras, Himel Chowdhury, Neil
Fox, Bencie Woll, Rob Cooper, Andrew McParland,
and Andrew Zisserman. 2021. BOBSL: BBC-Oxford
British Sign Language Dataset.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A framework
for self-supervised learning of speech representations.
Advances in neural information processing systems ,
33:12449–12460.
Robbin Battison. 1978. Lexical borrowing in American
sign language. ERIC, Linstok Press, Inc., Silver
Spring, Maryland 20901.
Matyáš Bohá ˇcek and Marek Hrúz. 2022. Sign pose-
based transformer for word-level sign language recog-
nition. In Proceedings of the IEEE/CVF winter con-
ference on applications of computer vision , pages
182–191.
Danielle Bragg, Oscar Koller, Mary Bellard, Larwan
Berke, Patrick Boudreault, Annelies Braffort, Naomi
Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa
Verhoef, et al. 2019. Sign language recognition, gen-
eration, and translation: An interdisciplinary perspec-
tive. In The 21st International ACM SIGACCESS
Conference on Computers and Accessibility , pages
16–31.
Diane Brentari and Carol Padden. 2001. A language
with multiple origins: Native and foreign vocabulary
in american sign language. Foreign vocabulary in
sign language: A cross-linguistic investigation of
word formation , pages 87–119.
Necati Cihan Camgoz, Simon Hadfield, Oscar Koller,
Hermann Ney, and Richard Bowden. 2018. Neural
sign language translation. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) .
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y . A.
Sheikh. 2019. Openpose: Realtime multi-person
2d pose estimation using part affinity fields. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence .
Joao Carreira and Andrew Zisserman. 2017. Quo vadis,
action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) ,
pages 6299–6308.Yiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen,
and Wenqiang Zhang. 2023. Cico: Domain-aware
sign language retrieval via cross-lingual contrastive
learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition ,
pages 19016–19026.
Mathieu De Coster, Dimitar Shterionov, Mieke Van Her-
reweghe, and Joni Dambre. 2023. Machine transla-
tion from signed to spoken languages: State of the art
and challenges. Universal Access in the Information
Society , pages 1–27.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE conference
on computer vision and pattern recognition , pages
248–255. Ieee.
Li Deng. 2012. The mnist database of handwritten digit
images for machine learning research [best of the
web]. IEEE signal processing magazine , 29(6):141–
142.
Aashaka Desai, Lauren Berger, Fyodor O Minakov,
Vanessa Milan, Chinmay Singh, Kriston Pumphrey,
Richard E Ladner, Hal Daumé III, Alex X Lu,
Naomi Caselli, and Danielle Bragg. 2023. Asl cit-
izen: A community-sourced dataset for advancing
isolated sign language recognition. arXiv preprint
arXiv:2304.05934 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Philippe Dreuw, Thomas Deselaers, Daniel Keysers,
and Hermann Ney. 2006. Modeling image variability
in appearance-based gesture recognition. In ECCV
workshop on statistical methods in multi-image and
video processing , pages 7–18.
Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti
Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi
Torres, and Xavier Giro-i Nieto. 2021. How2Sign:
A Large-scale Multimodal Dataset for Continuous
American Sign Language. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) .
Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu,
Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu.
2022. Alphapose: Whole-body regional multi-person
pose estimation and tracking in real-time. IEEE
Transactions on Pattern Analysis and Machine In-
telligence .
Jens Forster, Christoph Schmidt, Oscar Koller, Martin
Bellgardt, and Hermann Ney. 2014. Extensions of
the sign language recognition and translation cor-
pus RWTH-PHOENIX-weather. In Proceedings ofthe Ninth International Conference on Language
Resources and Evaluation (LREC’14) , pages 1911–
1916, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Shiwei Gan, Yafeng Yin, Zhiwei Jiang, Kang Xia, Lei
Xie, and Sanglu Lu. 2023. Contrastive learning for
sign language recognition and translation. In IJCAI ,
pages 763–772.
Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom
Heskes. 2013. Automatic sign language identifica-
tion. In 2013 IEEE International Conference on
Image Processing , pages 2626–2630. IEEE.
Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani,
and Jun Liu. 2024. Llms are good sign language
translators. arXiv preprint arXiv:2404.00925 .
Ivan Grishchenko and Valentin Bazarevsky. 2020. Me-
diapipe holistic.
Shester Gueuwou, Sophie Siake, Colin Leong, and
Mathias Müller. 2023. JWSign: A highly multilin-
gual corpus of Bible translations for more diversity
in sign language processing. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2023 , pages 9907–9927, Singapore. Association for
Computational Linguistics.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Dollár, and Ross Girshick. 2022. Masked autoen-
coders are scalable vision learners. In Proceedings
of the IEEE/CVF conference on computer vision and
pattern recognition (CVPR) , pages 16000–16009.
Ruth M Holmes, Ellen Rushe, and Anthony Ventresque.
2024. The key points: Using feature importance
to identify shortcomings in sign language recogni-
tion models. In Proceedings of the 2024 Joint In-
ternational Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024) , pages 15970–15975.
Eui Jun Hwang, Huije Lee, and Jong C Park. 2023.
Autoregressive sign language production: A gloss-
free approach with discrete representations. arXiv
preprint arXiv:2309.12179 .
Zifan Jiang, Amit Moryossef, Mathias Müller, and
Sarah Ebling. 2023. Machine translation between
spoken languages and signed languages represented
in SignWriting. In Findings of the Association for
Computational Linguistics: EACL 2023 , pages 1706–
1724, Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
Melvin Johnson, Mike Schuster, Quoc V . Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics , 5:339–351.Trevor Johnston and Adam Schembri. 2007. Australian
Sign Language (Auslan): An Introduction to Sign
Language Linguistics . Cambridge University Press,
Cambridge, UK.
Trevor Alexander Johnston. 2008. From archive to
corpus: transcription and annotation in the creation of
signed language corpora. In Pacific Asia Conference
on Language, Information and Computation .
Nikita Karaev, Ignacio Rocco, Benjamin Graham, Na-
talia Neverova, Andrea Vedaldi, and Christian Rup-
precht. 2023. Cotracker: It is better to track together.
arXiv:2307.07635 .
Lee Kezar, Jesse Thomason, Naomi Caselli, Zed Sehyr,
and Elana Pontecorvo. 2023. The sem-lex bench-
mark: Modeling asl signs and their phonemes. In
Proceedings of the 25th International ACM SIGAC-
CESS Conference on Computers and Accessibility ,
pages 1–10.
Alessandro Lenci and Magnus Sahlgren. 2023. Distri-
butional semantics . Cambridge University Press.
Ang Li, Allan Jabri, Armand Joulin, and Laurens Van
Der Maaten. 2017. Learning visual n-grams from
web data. In Proceedings of the IEEE International
Conference on Computer Vision , pages 4183–4192.
Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong
Li. 2020. Word-level deep sign language recognition
from video: A new large-scale dataset and methods
comparison. In The IEEE Winter Conference on
Applications of Computer Vision , pages 1459–1469.
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. 2022. Video swin trans-
former. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (CVPR) ,
pages 3202–3211.
Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris
McClanahan, Esha Uboweja, Michael Hays, Fan
Zhang, Chuo-Ling Chang, Ming Guang Yong,
Juhyun Lee, et al. 2019. Mediapipe: A framework
for building perception pipelines. arXiv preprint
arXiv:1906.08172 .
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
2019. HowTo100M: Learning a Text-Video Embed-
ding by Watching Hundred Million Narrated Video
Clips. In International Conference on Computer Vi-
sion (ICCV) .
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
Advances in neural information processing systems ,
26.
Amit Moryossef, Zifan Jiang, Mathias Müller, Sarah
Ebling, and Yoav Goldberg. 2023. Linguistically
motivated sign language segmentation. In Findings
of the Association for Computational Linguistics:EMNLP 2023 , pages 12703–12724, Singapore. Asso-
ciation for Computational Linguistics.
Amit Moryossef, Ioannis Tsochantaridis, Joe Dinn,
Necati Cihan Camgoz, Richard Bowden, Tao Jiang,
Annette Rios, Mathias Muller, and Sarah Ebling.
2021. Evaluating the immediate applicability of pose
estimation for sign language recognition. In Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition (CVPR) , pages 3434–3440.
Mathias Müller, Malihe Alikhani, Eleftherios
Avramidis, Richard Bowden, Annelies Braffort,
Necati Cihan Camgöz, Sarah Ebling, Cristina
España-Bonet, Anne Göhring, Roman Grund-
kiewicz, Mert Inan, Zifan Jiang, Oscar Koller,
Amit Moryossef, Annette Rios, Dimitar Shterionov,
Sandra Sidler-Miserez, Katja Tissi, and Davy
Van Landuyt. 2023. Findings of the second WMT
shared task on sign language translation (WMT-
SLT23). In Proceedings of the Eighth Conference
on Machine Translation , pages 68–94, Singapore.
Association for Computational Linguistics.
Mathias Müller, Sarah Ebling, Eleftherios Avramidis,
Alessia Battisti, Michèle Berger, Richard Bowden,
Annelies Braffort, Necati Cihan Camgöz, Cristina
España-bonet, Roman Grundkiewicz, Zifan Jiang,
Oscar Koller, Amit Moryossef, Regula Perrollaz,
Sabine Reinhard, Annette Rios, Dimitar Shterionov,
Sandra Sidler-miserez, and Katja Tissi. 2022. Find-
ings of the first WMT shared task on sign language
translation (WMT-SLT22). In Proceedings of the
Seventh Conference on Machine Translation (WMT) ,
pages 744–772, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.
Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748 .
Aleksandar Petrov, Emanuele La Malfa, Philip Torr,
and Adel Bibi. 2024. Language model tokenizers
introduce unfairness between languages. Advances
in Neural Information Processing Systems , 36.
K R Prajwal, Hannah Bull, Liliane Momeni, Samuel
Albanie, Gül Varol, and Andrew Zisserman. 2022.
Weakly-supervised fingerspelling recognition in
british sign language videos. In British Machine
Vision Conference .
Siegmund Prillwitz and Heiko Zienert. 1990. Hamburg
notation system for sign language: Development of
a sign writing with computer application. In Cur-
rent trends in European Sign Language Research.
Proceedings of the 3rd European Congress on Sign
Language Research , pages 355–379.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, and
Mohammad Sabokrou. 2021. Sign language produc-
tion: A review. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recogni-
tion (CVPR) , pages 3451–3461.
Charles Raude, K R Prajwal, Liliane Momeni, Han-
nah Bull, Samuel Albanie, Andrew Zisserman, and
Gül Varol. 2024. A tale of two languages: Large-
vocabulary continuous sign language recognition
from spoken language supervision. arXiv .
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing . Associa-
tion for Computational Linguistics.
Wendy Sandler and Diane Lillo-Martin. 2006. Sign
language and linguistic universals . Cambridge Uni-
versity Press.
Zed Sevcikova Sehyr, Naomi Caselli, Ariel M Cohen-
Goldberg, and Karen Emmorey. 2021. The asl-lex
2.0 project: A database of lexical and phonological
properties for 2,723 signs in american sign language.
The Journal of Deaf Studies and Deaf Education ,
26(2):263–277.
Prem Selvaraj, Gokul Nc, Pratyush Kumar, and Mitesh
Khapra. 2022. OpenHands: Making sign language
recognition accessible with pose-based pretrained
models across languages. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 2114–
2133, Dublin, Ireland. Association for Computational
Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725,
Berlin, Germany. Association for Computational Lin-
guistics.
Antonio F. G. Sevilla, José María Lahoz-Bengoechea,
and Alberto Diaz. 2024. Automated extraction of
prosodic structure from unannotated sign language
video. In Proceedings of the 2024 Joint International
Conference on Computational Linguistics, Language
Resources and Evaluation (LREC-COLING 2024) ,
pages 1808–1816, Torino, Italia. ELRA and ICCL.
Bowen Shi, Aurora Martinez Del Rio, Jonathan
Keane, Jonathan Michaux, Diane Brentari, Greg
Shakhnarovich, and Karen Livescu. 2018. Ameri-
can sign language fingerspelling recognition in the
wild. In 2018 IEEE Spoken Language Technology
Workshop (SLT) , pages 145–152. IEEE.
Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane,
Diane Brentari, Greg Shakhnarovich, and Karen
Livescu. 2019. Fingerspelling recognition in the wild
with iterative visual attention. In Proceedings of the
IEEE/CVF International Conference on Computer
Vision , pages 5400–5409.Thad Starner, Sean Forbes, Matthew So, David Martin,
Rohit Sridhar, Gururaj Deshpande, Sam Sepah, Sahir
Shahryar, Khushi Bhardwaj, Tyler Kwok, et al. 2024.
Popsign asl v1. 0: An isolated american sign lan-
guage dataset collected via smartphones. Advances
in Neural Information Processing Systems , 36.
Valerie Sutton. 1990. Lessons in sign writing . Sign-
Writing.
Garrett Tanzer and Biao Zhang. 2024. Youtube-sl-25: A
large-scale, open-domain multilingual sign language
parallel corpus. arXiv preprint arXiv:2407.11144 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Dave Uthus, Garrett Tanzer, and Manfred Georg. 2024.
Youtube-asl: A large-scale, open-domain american
sign language-english parallel corpus. Advances in
Neural Information Processing Systems , 36.
Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural
discrete representation learning. Advances in neural
information processing systems , 30.
Gül Varol, Liliane Momeni, Samuel Albanie, Triantafyl-
los Afouras, and Andrew Zisserman. 2021. Read
and attend: Temporal localisation in sign language
videos. In proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,
Bharath Hariharan, Aleksander Holynski, and Noah
Snavely. 2023. Tracking everything everywhere all
at once. In International Conference on Computer
Vision .
Sherman Wilcox. 1992. The phonetics of fingerspelling ,
volume 4. John Benjamins Publishing.
Ryan Wong, Necati Cihan Camgoz, and Richard Bow-
den. 2023. Learnt contrastive concept embed-
dings for sign recognition. In Proceedings of the
IEEE/CVF International Conference on Computer
Vision , pages 1945–1954.
Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettle-
moyer, and Christoph Feichtenhofer. 2021. Video-
CLIP: Contrastive pre-training for zero-shot video-
text understanding. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 6787–6800, Online and Punta
Cana, Dominican Republic. Association for Com-
putational Linguistics.Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
video and language. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition
(CVPR) , pages 5288–5296.
Jinhui Ye, Xing Wang, Wenxiang Jiao, Junwei Liang,
and Hui Xiong. 2024. Improving gloss-free sign lan-
guage translation by reducing representation density.
Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav
Goldberg, and Malihe Alikhani. 2021. Including
signed languages in natural language processing. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 7347–
7360, Online. Association for Computational Lin-
guistics.
Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018.
A joint sequence fusion model for video question
answering and retrieval. In Proceedings of the Euro-
pean conference on computer vision (ECCV) , pages
471–487.
Da Zhang, Xiyang Dai, Xin Wang, and Yuan-Fang
Wang. 2018. S3d: Single shot multi-span detector
via fully 3d convolutional network. In Proceedings
of the British Machine Vision Conference (BMVC) .
Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang,
Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak
Shah. 2023a. Deep learning-based human pose esti-
mation: A survey. ACM Comput. Surv.
Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li,
Ge Wang, Jun Xia, Yidong Chen, and Stan Z Li.
2023b. Cvt-slr: Contrastive visual-textual transfor-
mation for sign language recognition with variational
alignment. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) , pages 23141–23150.
Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and
Houqiang Li. 2021. Improving sign language transla-
tion with monolingual data by sign back-translation.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages
1316–1325.
Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza
Zolfaghari, Yuanjun Xiong, Chongruo Wu, Zhi
Zhang, Joseph Tighe, R Manmatha, and Mu Li. 2020.
A comprehensive study of deep video action recogni-
tion. arXiv preprint arXiv:2012.06567 .A An Intuitive Test on ChatGPT’s Sign Language Understanding Ability
Figure 3: Screenshot of prompting ChatGPT 4o to sign “house” in ASL, which lacks sign language knowledge and
tries to sketch a picture of a house on the open palm, tested in June 2024.B Introduction of the RWTH German Fingerspelling Database
RWTH German Fingerspelling Database contains ∼1400 videos of 20 different signers. Each video was
recorded by one webcam named cam1 , recording the dominant hands only with a resolution of 320x240
at 25 FPS; and one camcorder named cam2 , recording the whole body with a resolution of 352x288 at 25
FPS. We exclude all cam1 videos for pose-based models since we assume that the pose estimation system
expects whole-body input.
Figure 4: Examples of the German finger-alphabet taken from the RWTH gesture database recorded with the
webcam showing the letters A-Z, Ä, Ö, Ü, SCH, and the numbers 1 to 5. Note that J, Z, Ä, Ö, and Ü are dynamic
gestures. Figure taken from https://www-i6.informatik.rwth-aachen.de/aslr/fingerspelling.php .C Extended Spreadthesign Data Analysis
Following the data statistics presented in §5.1, Figure 5 illustrates the distribution of the video examples
in 41 sign languages we use from Spreadthesign.
Figure 5: Sign language distribution of video examples in Spreadthesign, using the ISO 639-3 language codes.
Figure 6 illustrates the distribution of pose/video length in Spreadthesign, depending on which we
decide the pretraining context length to be 256.
Figure 6: Pose length distribution of video examples in Spreadthesign. The two red vertical lines denote the 1st and
99th percentile of the number of frames.
Figure 7 illustrates the distribution of the number of video examples for 18,423 cross-lingual concepts
in Spreadthesign. Most concept has only one video example, or one video example per sign language, and
very few concepts have more than one video example for one specific sign language.Figure 7: Concept distribution of video examples in Spreadthesign.D Full Rank of the Signs for Iconicity Study
1 ( ' s c o r p i o n ' , 1 8 0 . 4 4 4 1 7 )
2 ( ' r e d u c t i o n ' , 1 8 1 . 4 0 4 2 4 )
3 ( ' i l l u s t r a t i o n ' , 1 8 2 . 5 8 5 0 4 )
4 ( ' e n v e l o p e ' , 1 8 2 . 6 3 1 2 4 )
5 ( ' o b s e r v e ' , 1 8 3 . 6 6 7 8 5 )
6 ( ' gol d ' , 1 8 4 . 1 5 1 8 4 )
7 ( ' t e l e p h o n e ' , 1 8 4 . 3 6 6 7 9 )
8 ( ' e m a i l ' , 1 8 5 . 0 5 3 5 )
9 ( ' c e n t e r ' , 1 8 5 . 3 5 8 7 3 )
10 ( ' k n i f e ' , 1 8 5 . 7 0 1 2 3 )
11 ( ' f a k e ' , 1 8 5 . 7 0 7 9 )
12 ( ' f r o z e n ' , 1 8 6 . 3 4 8 7 4 )
13 ( ' hig h ' , 1 8 6 . 4 0 5 4 6 )
14 ( ' consume ' , 1 8 6 . 4 2 8 0 5 )
15 ( ' c r o c o d i l e ' , 1 8 6 . 8 2 1 9 6 )
16 ( ' t o o t h b r u s h ' , 1 8 6 . 8 3 4 2 7 )
17 ( ' Youtube ' , 1 8 6 . 8 7 5 3 8 )
18 ( ' e a r a c h e ' , 1 8 6 . 9 7 0 3 )
19 ( ' Mexico ' , 1 8 6 . 9 8 8 7 8 )
20 ( ' e a r ' , 1 8 7 . 1 1 5 8 8 )
21 ( ' Remind ' , 1 8 7 . 2 9 9 2 6 )
22 ( ' Notepad ' , 1 8 7 . 3 8 8 6 6 )
23 ( ' Put ' , 1 8 7 . 4 1 4 6 7 )
24 ( ' p o t a t o ' , 1 8 7 . 4 2 6 3 )
25 ( ' c o n c e i t ' , 1 8 7 . 4 3 4 0 2 )
26 ( ' thong ' , 1 8 7 . 4 9 2 )
27 ( ' s a u c e ' , 1 8 7 . 5 0 0 9 2 )
28 ( ' o b s e s s e d ' , 1 8 7 . 5 7 2 8 5 )
29 ( ' drum ' , 1 8 7 . 8 5 2 6 )
30 ( ' Cuba ' , 1 8 8 . 0 0 4 1 8 )
31 ( ' g e n e r a t i o n ' , 1 8 8 . 1 6 4 1 5 )
32 ( ' g r i e f ' , 1 8 8 . 4 4 3 1 )
33 ( ' g u i l l o t i n e ' , 1 8 8 . 5 9 8 4 8 )
34 ( ' t o ' , 1 8 8 . 6 2 2 8 5 )
35 ( ' bin d ' , 1 8 8 . 6 3 6 0 8 )
36 ( ' u m b r e l l a ' , 1 8 8 . 8 1 3 5 8 )
37 ( ' omit ' , 1 8 9 . 2 0 4 9 3 )
38 ( ' Superman ' , 1 8 9 . 2 0 7 8 2 )
39 ( ' a d v i c e ' , 1 8 9 . 4 8 6 4 7 )
40 ( ' Refuse ' , 1 8 9 . 5 2 5 0 2 )
41 ( ' speed ' , 1 8 9 . 6 8 2 7 )
42 ( ' diamond ' , 1 8 9 . 7 6 1 5 4 )
43 ( ' c u t e ' , 1 8 9 . 8 2 4 8 6 )
44 ( ' headache ' , 1 9 0 . 2 4 9 2 4 )
45 ( ' j e a l o u s y ' , 1 9 1 . 1 7 3 4 3 )
46 ( ' f l a g ' , 1 9 1 . 1 8 7 5 3 )
47 ( ' banana ' , 1 9 1 . 2 0 3 4 6 )
48 ( ' Wait ! ' , 1 9 1 . 3 6 2 1 7 )
49 ( ' y e t ' , 1 9 1 . 7 5 4 0 7 )
50 ( ' t h e f t ' , 1 9 1 . 7 5 7 3 4 )
51 ( ' house ' , 1 9 1 . 7 9 7 1 2 )
52 ( ' p e r c e n t a g e ' , 1 9 1 . 8 0 0 0 8 )
53 ( ' eye ' , 1 9 1 . 8 4 5 8 4 )
54 ( ' u n d e r s t a n d i n g ' , 1 9 1 . 9 5 7 1 5 )
55 ( ' b a d l y ' , 1 9 2 . 0 2 9 5 9 )
56 ( ' s k i n ' , 1 9 2 . 0 4 2 9 4 )
57 ( ' dvd ' , 1 9 2 . 0 5 6 9 5 )
58 ( ' u n t i l ' , 1 9 2 . 0 8 8 4 7 )
59 ( ' Denmark ' , 1 9 2 . 2 8 4 6 5 )
60 ( ' f l o w e r ' , 1 9 2 . 2 8 5 2 )
61 ( ' sew ' , 1 9 2 . 4 1 8 8 )
62 ( ' a r r e s t ' , 1 9 2 . 5 5 3 2 5 )
63 ( ' p r e v i o u s ' , 1 9 2 . 5 5 6 2 1 )
64 ( ' n e i g h b o r ' , 1 9 2 . 5 9 9 7 3 )
65 ( ' spoon ' , 1 9 2 . 7 8 3 1 3 )
66 ( ' b e l l ' , 1 9 2 . 8 2 6 6 6 )
67 ( ' c l i c k ' , 1 9 2 . 8 6 9 5 7 )
68 ( ' v a c c i n a t i o n ' , 1 9 2 . 9 0 3 9 9 )69 ( ' l e a d i n g ' , 1 9 3 . 0 4 1 0 3 )
70 ( ' t o t a l ' , 1 9 3 . 0 7 9 3 )
71 ( ' i n t e r n e t ' , 1 9 3 . 2 6 0 2 2 )
72 ( ' sandwich ' , 1 9 3 . 2 7 3 9 4 )
73 ( ' e r e c t ' , 1 9 3 . 3 1 7 4 4 )
74 ( ' s i g n a t u r e ' , 1 9 3 . 4 0 7 9 6 )
75 ( ' s i x ' , 1 9 3 . 4 3 2 8 9 )
76 ( ' s t r a n g e ' , 1 9 3 . 5 1 5 3 8 )
77 ( ' boy ' , 1 9 3 . 5 3 1 9 4 )
78 ( ' F a r e w e l l ! ' , 1 9 3 . 8 6 5 3 4 )
79 ( ' c a f e ' , 1 9 3 . 8 6 9 6 9 )
80 ( ' t w i c e ' , 1 9 3 . 9 1 4 5 2 )
81 ( ' f o u r ' , 1 9 3 . 9 6 5 0 9 )
82 ( ' s i l v e r ' , 1 9 3 . 9 7 3 1 1 )
83 ( ' China ' , 1 9 4 . 0 3 8 7 4 )
84 ( ' c e r t i f y ' , 1 9 4 . 0 6 5 3 8 )
85 ( ' soup ' , 1 9 4 . 0 8 8 5 6 )
86 ( ' r a p e ' , 1 9 4 . 0 9 2 4 4 )
87 ( ' n e c e s s a r y ' , 1 9 4 . 1 4 7 5 )
88 ( ' c u r i o u s ' , 1 9 4 . 2 7 0 7 2 )
89 ( ' t a l l ' , 1 9 4 . 3 7 1 8 )
90 ( ' b a t t l e ' , 1 9 4 . 4 5 6 2 2 )
91 ( ' promise ' , 1 9 4 . 5 1 1 0 5 )
92 ( ' d e a d l i n e ' , 1 9 4 . 6 1 0 5 5 )
93 ( ' c e r t i f i c a t e ' , 1 9 4 . 7 3 6 )
94 ( ' g e n u i n e ' , 1 9 4 . 7 9 2 5 6 )
95 ( ' blood ' , 1 9 4 . 8 2 9 5 9 )
96 ( ' ban ' , 1 9 4 . 8 4 7 0 5 )
97 ( ' u n c l e ' , 1 9 4 . 8 5 6 8 9 )
98 ( ' q u a r a n t i n e ' , 1 9 4 . 9 0 2 4 7 )
99 ( ' s a l a d ' , 1 9 5 . 1 9 2 5 )
100 ( ' a n t ' , 1 9 5 . 3 1 8 8 8 )
101 ( ' t h i e f ' , 1 9 5 . 4 8 5 6 6 )
102 ( ' f a l l ' , 1 9 5 . 6 0 6 9 6 )
103 ( ' c h i l d l i k e ' , 1 9 5 . 6 4 3 7 )
104 ( ' Japan ' , 1 9 5 . 6 8 5 4 6 )
105 ( ' run ' , 1 9 5 . 7 0 0 1 3 )
106 ( ' booking ' , 1 9 5 . 7 5 2 2 3 )
107 ( ' homesick ' , 1 9 5 . 8 6 0 8 6 )
108 ( ' advanced ' , 1 9 5 . 8 6 6 8 5 )
109 ( ' Where ? ' , 1 9 5 . 9 2 3 1 4 )
110 ( ' b r i d g e ' , 1 9 5 . 9 8 7 2 3 )
111 ( ' b e s i d e ' , 1 9 6 . 0 1 9 5 8 )
112 ( ' cup ' , 1 9 6 . 0 3 4 6 7 )
113 ( ' s p a g h e t t i ' , 1 9 6 . 0 4 3 3 3 )
114 ( ' d i z z i n e s s ' , 1 9 6 . 0 6 3 1 1 )
115 ( ' mixer ' , 1 9 6 . 1 0 7 3 5 )
116 ( ' Assessment ' , 1 9 6 . 1 4 0 8 1 )
117 ( ' amnesia ' , 1 9 6 . 1 5 0 9 1 )
118 ( ' g i g a n t i c ' , 1 9 6 . 2 1 0 8 5 )
119 ( ' p r i e s t ' , 1 9 6 . 2 5 3 9 )
120 ( ' s o r r y ' , 1 9 6 . 3 1 0 9 3 )
121 ( ' i n v e s t m e n t ' , 1 9 6 . 3 5 5 7 1 )
122 ( ' b e l i e v e ' , 1 9 6 . 4 1 5 2 2 )
123 ( ' hang ' , 1 9 6 . 4 3 0 3 )
124 ( ' t h r e e ' , 1 9 6 . 4 3 7 6 7 )
125 ( ' h e a r i n g ' , 1 9 6 . 5 1 3 0 5 )
126 ( ' p r i n c i p a l ' , 1 9 6 . 6 3 1 3 2 )
127 ( ' p u n c t u a l ' , 1 9 6 . 7 0 6 2 4 )
128 ( ' a d u l t ' , 1 9 6 . 9 1 1 8 3 )
129 ( ' t h i n ' , 1 9 6 . 9 8 5 0 2 )
130 ( ' word ' , 1 9 7 . 0 5 3 4 )
131 ( ' arm ' , 1 9 7 . 1 1 9 6 6 )
132 ( ' c e n s o r s h i p ' , 1 9 7 . 1 3 4 9 3 )
133 ( ' s e v e r a l ' , 1 9 7 . 3 1 2 1 6 )
134 ( ' b e w i l d e r ' , 1 9 7 . 4 3 2 5 4 )
135 ( ' r e p l y ' , 1 9 7 . 4 6 0 2 4 )
136 ( ' s e r i o u s ' , 1 9 7 . 5 7 8 9 8 )
137 ( ' sewing ' , 1 9 7 . 6 2 8 4 8 )
138 ( ' do ' , 1 9 7 . 6 9 4 0 5 )139 ( ' t o g e t h e r ' , 1 9 7 . 8 3 0 9 3 )
140 ( ' h a i r g r o w t h ' , 1 9 7 . 8 5 4 2 5 )
141 ( ' b u l l ' , 1 9 7 . 9 8 5 7 5 )
142 ( ' honeymoon ' , 1 9 8 . 0 2 5 4 5 )
143 ( ' b a l l ' , 1 9 8 . 0 5 6 3 7 )
144 ( ' a n c i e n t ' , 1 9 8 . 1 4 4 4 4 )
145 ( ' s e l f i s h ' , 1 9 8 . 1 5 9 0 6 )
146 ( ' a r i s e ' , 1 9 8 . 1 7 1 9 8 )
147 ( ' wedding ' , 1 9 8 . 2 1 5 8 4 )
148 ( ' hour ' , 1 9 8 . 2 7 9 5 7 )
149 ( ' g r a n d d a u g h t e r ' , 1 9 8 . 2 9 3 1 5 )
150 ( ' c i r c l e ' , 1 9 8 . 3 2 2 7 5 )
151 ( ' couch ' , 1 9 8 . 3 8 3 8 5 )
152 ( ' s c i e n t i s t ' , 1 9 8 . 4 4 2 6 9 )
153 ( ' i m p o r t a n t ' , 1 9 8 . 5 2 5 9 9 )
154 ( ' h e l i c o p t e r ' , 1 9 8 . 6 1 2 1 8 )
155 ( ' born ' , 1 9 8 . 6 9 4 7 5 )
156 ( ' t r o u s e r s ' , 1 9 8 . 7 1 1 9 )
157 ( ' a c c e p t a b l e ' , 1 9 8 . 8 2 7 3 6 )
158 ( ' lamp ' , 1 9 8 . 8 6 0 0 2 )
159 ( ' a p p e t i t e ' , 1 9 8 . 8 6 9 9 5 )
160 ( ' a s s o c i a t i o n ' , 1 9 8 . 8 7 0 0 1 )
161 ( ' l e a v e ' , 1 9 8 . 9 5 5 9 3 )
162 ( ' d y s l e x i a ' , 1 9 8 . 9 8 0 1 3 )
163 ( ' twi n ' , 1 9 9 . 0 1 1 1 4 )
164 ( ' f o r c e ' , 1 9 9 . 0 4 5 4 1 )
165 ( ' i n s i s t ' , 1 9 9 . 0 6 2 3 6 )
166 ( ' va s e ' , 1 9 9 . 0 8 4 6 6 )
167 ( ' e a s t e r ' , 1 9 9 . 2 3 8 4 6 )
168 ( ' p l a t e ' , 1 9 9 . 2 7 2 3 1 )
169 ( ' b e s t ' , 1 9 9 . 3 2 5 1 3 )
170 ( ' h e a l ' , 1 9 9 . 6 1 2 6 1 )
171 ( ' p e t r o l ' , 1 9 9 . 6 7 2 )
172 ( ' c l e a n e r ' , 1 9 9 . 6 9 0 7 7 )
173 ( ' p e p p e r ' , 1 9 9 . 9 2 5 1 7 )
174 ( ' economic ' , 1 9 9 . 9 7 2 5 3 )
175 ( ' y o g h u r t ' , 2 0 0 . 0 6 4 3 9 )
176 ( ' b r o t h e r ' , 2 0 0 . 0 6 8 9 )
177 ( ' u n p l e a s a n t ' , 2 0 0 . 1 3 5 6 2 )
178 ( ' g r a p e s ' , 2 0 0 . 1 4 9 8 1 )
179 ( ' buy ' , 2 0 0 . 2 9 6 6 9 )
180 ( ' 2 ' , 2 0 0 . 3 1 0 9 3 )
181 ( ' f r o g ' , 2 0 0 . 4 1 3 7 7 )
182 ( ' committee ' , 2 0 0 . 5 6 6 1 5 )
183 ( ' complain ' , 2 0 0 . 5 7 2 9 8 )
184 ( ' 40 ' , 2 0 0 . 5 8 5 0 8 )
185 ( ' f a u l t l e s s ' , 2 0 0 . 5 9 1 6 7 )
186 ( ' l e t t e r ' , 2 0 0 . 6 3 2 5 2 )
187 ( ' a n g e l ' , 2 0 0 . 6 5 4 1 3 )
188 ( ' c o r r u p t i o n ' , 2 0 0 . 6 6 1 0 1 )
189 ( ' d i r e c t o r ' , 2 0 0 . 6 7 6 0 1 )
190 ( ' e x p o r t ' , 2 0 0 . 9 9 3 7 6 )
191 ( ' acne ' , 2 0 1 . 0 8 7 2 5 )
192 ( ' p a r t i c i p a t e ' , 2 0 1 . 1 4 1 5 7 )
193 ( ' i n j u r y ' , 2 0 1 . 1 9 5 1 8 )
194 ( ' o f f l i n e ' , 2 0 1 . 2 1 5 )
195 ( ' h u r t ' , 2 0 1 . 2 5 2 9 )
196 ( ' shy ' , 2 0 1 . 3 1 7 9 5 )
197 ( ' k i l o m e t e r ' , 2 0 1 . 3 2 1 1 7 )
198 ( ' i n a u g u r a t i o n ' , 2 0 1 . 3 3 9 9 7 )
199 ( ' t a l e ' , 2 0 1 . 4 1 5 2 )
200 ( ' ve r y ' , 2 0 1 . 4 5 9 0 8 )
201 ( ' law ' , 2 0 1 . 4 7 7 1 4 )
202 ( ' diploma ' , 2 0 1 . 5 6 8 0 1 )
203 ( ' music ' , 2 0 1 . 5 7 0 7 4 )
204 ( ' war ' , 2 0 1 . 6 3 3 0 4 )
205 ( ' s c h o o l ' , 2 0 1 . 6 5 4 9 5 )
206 ( ' h o r s e ' , 2 0 1 . 7 4 6 )
207 ( ' h e a r t b u r n ' , 2 0 1 . 8 6 8 9 6 )
208 ( ' 21 ' , 2 0 1 . 9 0 6 6 6 )209 ( ' surname ' , 2 0 1 . 9 4 6 6 7 )
210 ( ' a d d i c t e d ' , 2 0 1 . 9 8 8 8 3 )
211 ( ' s u p p e r ' , 2 0 2 . 0 8 4 3 8 )
212 ( ' fun ' , 2 0 2 . 0 9 5 5 8 )
213 ( ' t e r r o r i s t ' , 2 0 2 . 2 2 5 0 4 )
214 ( ' nanny ' , 2 0 2 . 3 2 7 2 7 )
215 ( ' d e p a r t u r e ' , 2 0 2 . 3 4 7 8 7 )
216 ( ' 600 ' , 2 0 2 . 3 8 9 2 2 )
217 ( ' p e t ' , 2 0 2 . 4 6 8 0 6 )
218 ( ' t h o u s a n d ' , 2 0 2 . 5 4 2 2 )
219 ( ' i c e ' , 2 0 2 . 5 6 7 2 6 )
220 ( ' menu ' , 2 0 2 . 5 7 5 0 7 )
221 ( ' r e v i s e ' , 2 0 2 . 6 9 3 3 1 )
222 ( ' h a i r e d ' , 2 0 2 . 7 0 9 6 9 )
223 ( ' f e e l i n g ' , 2 0 2 . 8 2 6 3 7 )
224 ( ' d i v o r c e d ' , 2 0 2 . 8 5 4 0 3 )
225 ( ' p e r s o n ' , 2 0 3 . 0 2 7 9 )
226 ( ' dawn ' , 2 0 3 . 3 6 4 6 7 )
227 ( ' a n x i o u s ' , 2 0 3 . 4 6 1 1 2 )
228 ( ' a u t i s m ' , 2 0 3 . 4 8 0 3 8 )
229 ( ' d i s c u s s i o n ' , 2 0 3 . 5 6 6 1 3 )
230 ( ' a d o p t i o n ' , 2 0 3 . 5 8 8 2 4 )
231 ( ' t r u t h ' , 2 0 3 . 6 0 5 4 )
232 ( ' enemy ' , 2 0 3 . 6 1 2 7 )
233 ( ' m i d n i g h t ' , 2 0 3 . 6 3 3 1 6 )
234 ( ' p s y c h o l o g y ' , 2 0 3 . 7 0 6 7 1 )
235 ( ' p o s s i b l e ' , 2 0 3 . 8 5 4 2 )
236 ( ' p a l e ' , 2 0 3 . 8 5 9 9 5 )
237 ( ' cucumber ' , 2 0 3 . 8 7 3 2 8 )
238 ( ' f a v o u r i t e ' , 2 0 3 . 9 5 9 8 1 )
239 ( ' r i c e ' , 2 0 4 . 0 9 5 9 8 )
240 ( ' bedroom ' , 2 0 4 . 1 1 5 5 4 )
241 ( ' s e a ' , 2 0 4 . 1 8 8 8 1 )
242 ( ' shock ' , 2 0 4 . 2 1 6 )
243 ( ' Admitted ' , 2 0 4 . 2 2 2 2 7 )
244 ( ' a n x i e t y ' , 2 0 4 . 2 2 7 3 9 )
245 ( ' t e n ' , 2 0 4 . 2 8 0 9 4 )
246 ( ' i n t e r n a t i o n a l ' , 2 0 4 . 3 0 5 1 5 )
247 ( ' c u r l y ' , 2 0 4 . 3 2 3 6 4 )
248 ( ' alarm ' , 2 0 4 . 4 2 8 9 1 )
249 ( ' co r n ' , 2 0 4 . 5 7 6 8 7 )
250 ( ' u p s e t ' , 2 0 4 . 5 9 3 )
251 ( ' morning ' , 2 0 4 . 6 5 6 5 7 )
252 ( ' s p i n a c h ' , 2 0 4 . 6 5 6 6 )
253 ( ' c e l e b r a t e ' , 2 0 4 . 7 4 7 1 5 )
254 ( ' c o n f u s e d ' , 2 0 4 . 8 2 5 7 1 )
255 ( ' 25 ' , 2 0 4 . 8 7 2 0 6 )
256 ( ' a c h i e v e m e n t ' , 2 0 4 . 9 4 8 1 5 )
257 ( ' c l i m a t e ' , 2 0 5 . 0 2 1 0 7 )
258 ( ' communication ' , 2 0 5 . 2 6 2 2 8 )
259 ( ' d e l e g a t e ' , 2 0 5 . 2 6 9 0 1 )
260 ( ' d o o r b e l l ' , 2 0 5 . 4 1 6 7 8 )
261 ( ' a n a e s t h e s i a ' , 2 0 5 . 6 6 0 4 6 )
262 ( ' world ' , 2 0 5 . 8 6 3 4 2 )
263 ( ' t e l e v i s i o n ' , 2 0 6 . 1 1 9 5 )
264 ( ' i n f o r m a t i o n ' , 2 0 6 . 2 0 2 7 1 )
265 ( ' s t y l e ' , 2 0 6 . 2 8 0 7 8 )
266 ( ' c h i p ' , 2 0 6 . 3 1 6 1 6 )
267 ( ' anonymous ' , 2 0 6 . 4 1 8 0 3 )
268 ( ' f a s h i o n e d ' , 2 0 6 . 4 6 5 0 9 )
269 ( ' development ' , 2 0 6 . 4 6 6 0 5 )
270 ( ' s c a r f ' , 2 0 6 . 5 8 0 8 1 )
271 ( ' u p l o a d i n g ' , 2 0 6 . 9 3 1 8 2 )
272 ( ' 900 ' , 2 0 6 . 9 3 8 9 3 )
273 ( ' 500 ' , 2 0 7 . 1 2 5 3 )
274 ( ' camera ' , 2 0 7 . 1 5 2 0 7 )
275 ( ' homeless ' , 2 0 7 . 2 5 6 5 5 )
276 ( ' a u t o m a t i c ' , 2 0 7 . 2 9 5 7 8 )
277 ( ' 1000000 ' , 2 0 7 . 4 0 5 6 7 )
278 ( ' c h e f ' , 2 0 7 . 7 2 5 3 1 )279 ( ' 50 ' , 2 0 7 . 7 3 3 1 4 )
280 ( ' i n f a n t ' , 2 0 7 . 8 8 8 4 6 )
281 ( ' a c t r e s s ' , 2 0 7 . 9 7 6 4 6 )
282 ( ' n u r s e ' , 2 0 8 . 7 5 1 8 2 )
283 ( ' 800 ' , 2 0 8 . 8 0 1 7 )
284 ( ' slow ' , 2 0 8 . 9 3 7 4 1 )
285 ( ' c l i n i c ' , 2 0 8 . 9 3 7 5 3 )
286 ( ' a p a r t m e n t ' , 2 0 9 . 0 7 9 3 8 )
287 ( ' employed ' , 2 0 9 . 4 8 0 7 1 )
288 ( ' e l e c t r i c i a n ' , 2 0 9 . 5 4 4 1 4 )
289 ( ' p a i n t e r ' , 2 0 9 . 5 7 8 9 3 )
290 ( ' d e s e r t ' , 2 0 9 . 7 0 9 1 8 )
291 ( ' A u d i o l o g i s t ' , 2 0 9 . 9 7 5 5 9 )
292 ( ' e n g i n e ' , 2 1 0 . 5 3 7 4 5 )
293 ( ' b a r b e r ' , 2 1 0 . 7 6 9 7 1 )
294 ( ' bathroom ' , 2 1 0 . 7 7 3 7 7 )
295 ( ' d i a b e t i c ' , 2 1 1 . 6 6 0 9 2 )
296 ( ' 7 ' , 2 1 1 . 7 6 9 6 4 )
297 ( ' 27 ' , 2 1 1 . 8 3 7 9 2 )
298 ( ' d e p r e s s e d ' , 2 1 2 . 8 8 5 5 4 )
299 ( ' employee ' , 2 1 3 . 1 7 8 4 2 )
300 ( ' 8 ' , 2 1 3 . 5 9 4 7 6 )
301 ( ' f a r m e r ' , 2 1 6 . 9 7 7 9 2 )
302 ( ' 29 ' , 2 1 7 . 7 0 3 6 3 )