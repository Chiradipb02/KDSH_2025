Estimating Knowledge in Large Language Models
Without Generating a Single Token
Daniela Gottesman and Mor Geva
Blavatnik School of Computer Science, Tel Aviv University
gottesman3@mail.tau.ac.il ,morgeva@tauex.tau.ac.il
Abstract
To evaluate knowledge in large language mod-
els (LLMs), current methods query the model
and then evaluate its generated responses. In
this work, we ask whether evaluation can
be done before the model has generated any
text. Concretely, is it possible to estimate how
knowledgeable a model is about a certain entity,
only from its internal computation? We study
this question with two tasks: given a subject
entity, the goal is to predict (a) the ability of
the model to answer common questions about
the entity, and (b) the factuality of open-ended
responses generated by the model about the
entity. Experiments with a variety of LLMs
show that KEEN , a simple probe trained over in-
ternal subject representations, succeeds at both
tasks — correlating with both the QA accu-
racy of the model per-subject and FActScore,
a recent factuality metric in open-ended gen-
eration. Moreover, KEEN naturally aligns with
the model’s hedging behavior and faithfully
reflects changes in the model’s knowledge af-
ter fine-tuning. Lastly, we show a more inter-
pretable yet equally performant variant of KEEN ,
which highlights a small set of tokens indica-
tive of clusters and gaps in the model’s knowl-
edge. Being simple and lightweight, KEEN can
be leveraged to guide decisions such as when it
is appropriate to apply further training or aug-
ment queries with retrieval.
1 Introduction
The standard approach for evaluating knowledge
in large language models (LLMs) relies on query-
ing the model, letting it generate responses, and
then evaluating the responses. This evaluation can
be done using various methods, including com-
paring responses to gold answers (Touvron et al.,
2023; Cohen et al., 2023a), measuring response
consistency over multiple generations (Cohen et al.,
2023b; Manakul et al., 2023; Kuhn et al., 2023),
checking the support of responses in external ev-
idence (Gao et al., 2023; Bohnet et al., 2022), or
<subject> 
Napoleon  |  Matthew Perry  |  PewDiePie  |  Irina Shayk Hallucination Forecasting 
LLM
X Correlation 
Query knowledge graph Factual 
queries 
about 
Napoleon KEEN 
LLM Tell me 
about 
Napoleon 0.54
0.55 average accuracy 
When was 
Napoleon born? 
Where was 
Napoleon born? 
…LLM
Paragraph Atomic 
facts 
0.54
Tell me 
about 
Napoleon LLM
(a) Hallucination Forecasting 
(c) Hallucination Detection FAITH LLM
Tell me about 
Napoleon 
0.54
0.55average 
accuracy When was 
Napoleon born? 
Where was 
Napoleon born? 
…(b) Question Answering Performance 
LLM
Paragraph 
Atomic 
facts 
0.54
Tell me about 
Napoleon LLM
Estimates the 
model’s knowledge 
about a subject Forecasts  
post-generation  
factuality Figure 1: We show that simple probes ( KEEN ), trained
over hidden model representations, quantify the model’s
knowledge about a given subject entity — estimating the
model’s question-answering accuracy on entity-related
questions ( bottom left ) and forecasting the factuality of
model-generated texts about the entity ( right ).
estimating the model’s uncertainty per-response
(Yu et al., 2024; Yuksekgonul et al., 2024; Li et al.,
2023; Snyder et al., 2023; Liu et al., 2022).
In this work, we take a step back and ask whether
it is possible to evaluate the model’s knowledge be-
fore it generates any text, using only its internal
computation. This view is analogous to human
studies that show the effectiveness of assessing
non-verbal communication for determining wit-
ness credibility in the courtroom (Remland, 1994;
Denault et al., 2024). Concretely, we propose to
evaluate how knowledgeable an LLM is about a
given subject entity (e.g. Napoleon or Empire State
Building), by considering only how it processes the
name of that entity, and before it generates a single
token.
We formalize this problem as entity knowledge
estimation (§2) and devise two concrete tasks.
Given an entity, the goal is to predict: (a) how
many common questions about the subject entity
the model will answer correctly (Figure 1, bottom
left), and (b) how many of the claims in a model
generated response about the subject are factuallyarXiv:2406.12673v2  [cs.CL]  29 Oct 2024correct (Figure 1, right).
To tackle entity knowledge estimation, we capi-
talize on findings from recent interpretability works
which show that, during inference, the hidden repre-
sentations of an input entity capture many attributes
related to it (Geva et al., 2023; Meng et al., 2022),
and often these attributes can be extracted with lin-
ear functions (Hernandez et al., 2024b). Therefore,
we propose (§3) to estimate how knowledgeable
a model is about a given entity by training sim-
ple probes, called KEEN (Knowledge Estimation of
ENtities), over the model’s representations of the
entity (Figure 1, upper left).
We evaluate KEEN in two experimental settings
(§4) of factual question answering (QA) and open-
ended generation (OEG) of biographies. In the QA
setting, we derive a set of questions per-subject
for subjects in PopQA (Mallen et al., 2023) and
evaluate how well KEEN predicts the model’s av-
erage accuracy per-subject across these questions.
In the OEG setting, we evaluate the correlation of
KEEN with FActScore (Min et al., 2023), a post-
generation hallucination detector. In both settings
and across models of different sizes and families
— GPT2 (Radford et al., 2019), Pythia (Biderman
et al., 2023), LLaMA2 (Touvron et al., 2023), and
Vicuna (Chiang et al., 2023) — KEEN consistently
shows correlation values between 0.58-0.68 with
QA accuracy and 0.66-0.77 with OEG factuality.
Moreover, KEEN probes trained on entity represen-
tations show substantially stronger correlation with
both metrics than probes trained on commonly-
used intrinsic features, such as fully-connected
scores and self-attention activations, and external
features, such as entity-popularity.
Further analyzing the utility and features of KEEN
(§5), we show that KEEN faithfully correlates with
the model’s hedging behavior, i.e., the score pre-
dicted by KEEN decreases as the fraction of per-
entity questions that a model hedges on increases.
In addition, KEEN faithfully reflects changes in the
model’s knowledge following fine-tuning: training
LLaMA2 on Wikipedia articles about certain en-
tities increases their KEEN score while scores for
other entities tend to decrease. Lastly, we show
that training KEEN on the vocabulary projections of
entity representations (nostalgebraist, 2020; Geva
et al., 2021) increases the probe’s interpretability
without performance cost, identifying a small set
of tokens that represent clusters or gaps in entity
knowledge.
To conclude, we present KEEN , a simple andlightweight approach for quantifying how knowl-
edgeable a model is about a given entity from
intrinsic properties, which well-estimates the ac-
curacy and factuality of model responses about
the entity. We also show that KEEN scores are re-
flective of both hedging behavior and changes in
entity-based knowledge after fine-tuning. Prac-
tically,KEEN could be used to inform developer
decisions such as whether to augment queries
with retrieval, discard certain queries (e.g. by
abstaining), enhance models with external tools,
or identify “holes” in the model’s knowledge to
apply further training on. We release our code
and data at https://github.com/dhgottesman/
keen_estimating_knowledge_in_llms .
2 Entity Knowledge Estimation
Our goal is to evaluate how much knowledge an
LLM captures about an entity from how it pro-
cesses the entity’s name alone, without obtain-
ing model responses and evaluating them post-
generation. This view is motivated by growing
evidence from interpretability works which find
that, during model inference, knowledge is central-
ized in the hidden representations corresponding
to named entities (Meng et al., 2022; Geva et al.,
2023; Li et al., 2021).
Given a subject entity s(e.g. Napoleon or Em-
pire State Building) and a model M, our goal is to
estimate two related quantities: (a) the performance
ofMon queries about s, and (b) the probability
thatMwill generate incorrect facts given any query
about s. These two quantities are expected to be
related, as they are both influenced by and reflect
the amount of knowledge Mcaptures about s.
To evaluate entity knowledge, we propose two
concrete evaluation settings:
Question Answering (QA) For a subject en-
titysand a set of common question-answer pairs
Q={⟨qi, ai⟩}n
i=1about s, denote by ˆaithe an-
swer predicted by a model Mfor the query qi.
Given only the subject s, our goal is to estimate
the average accuracy of MoverQ, denoted as
y(s)
QA:=1
nPn
i=1 1[ˆai=ai].
Open-Ended Generation (OEG) For a general
information-seeking query qabout a subject s(e.g.
“Tell me facts about Napoleon” or“Generate a para-
graph about Napoleon” ), letR={⟨ci, ai⟩}m
i=1
be the set of claims in the response generated by
M, each with a 0/1 label indicating its correct-ness with respect to external evidence ( cidenotes
a claim and aiis its factuality label). Claims can
be extracted and evaluated for correctness using
various automatic methods (e.g., Nenkova and Pas-
sonneau, 2004; Shapira et al., 2019; Zhang and
Bansal, 2021). Given only the subject s, the task is
to predict the portion of factually correct claims in
R, denoted as y(s)
OEG:=1
mPm
i=1ai.
A naive solution for both tasks would be to first
obtain queries about s, feed them to M, and eval-
uate the answers Mgenerates. Here we seek an
efficient solution, which estimates the knowledge
ofMabout s, without iteratively executing M.
3KEEN
Geva et al. (2023) showed that for a given subject
in the input, LLMs construct an information-rich
representation of the subject that encodes many of
its attributes. Furthermore, subject attributes can
be extracted from the subject representation with
a simple linear function (Hernandez et al., 2024b).
We capitalize on these findings and propose to train
a simple probe over the model’s representations
of subjects to predict how much knowledge the
model captures about them. In our following for-
mulation (and the rest of the paper), we focus on
widely-adopted transformer-based auto-regressive
language models.
Notation Assuming a language model with L
layers, a hidden dimension d, a vocabulary V, and
an unembedding matrix WU∈R|V|×d. Let hℓ,ibe
the hidden representation at position iand layer ℓ,
omitting normalization, hℓ,iis computed as:
hℓ,i=hℓ−1,i+aℓ,i+mℓ,i
where aℓ,iandmℓ,idenote the outputs from the
ℓ-th multi-head self-attention and MLP sublayers,
respectively (Vaswani et al., 2017).
3.1 Features
Lett(s)
1, ..., t(s)
srbe the sequence of srinput tokens
corresponding to a given subject s(e.g.N, ap,
oleon for the subject Napoleon tokenized with
GPT2). We use the representations at the last sub-
ject position ( sr), denoted as h(s)
1,sr, ...,h(s)
L,sr, to
construct a feature vector z(s)∈Rdz.1
1In practice, we obtain the hidden representations using the
query:“This document describes [ s]”. This is to avoid
placing the subject in the first position of the input, which
often encodes biases that could affect performance on our task
(Xiao et al., 2024; Geva et al., 2023).We train different variants of KEEN probes, each
taking as input one of the following sets of features
forz(s):
•Hidden states (HS) : We take the subject repre-
sentation from multiple upper-intermediate lay-
ers, where attributes of the subject are often ex-
tracted during inference (Geva et al., 2023; Meng
et al., 2022) and are easier to disentangle (Huang
et al., 2024; Hernandez et al., 2024b). To account
for variations in the inference pass of different
subjects, we choose 3 consecutive layers L=3
4L+k|k∈ {− 1,0,1}	
, from which we ex-
tract the hidden states {h(s)
ℓ,sr|ℓ∈ L} . Then, we
normalize these vectors (see details below) and
average them into a d-dimensional feature vector.
A systematic evaluation supporting our choice of
layers is presented in §A.2.
•HS with vocabulary projection (VP) : We take
the same hidden states as in HS, but instead of us-
ing them as-is, we use their projections to the vo-
cabulary (nostalgebraist, 2020; Geva et al., 2021).
Namely, we normalize and average the vectors
{WUfL(h(s)
ℓ,sr)|ℓ∈ L} into a |V|-dimensional
feature vector, where fLis the layer norm ap-
plied at the last layer of the model. While VP is
not expected to improve performance, it could
enhance interpretability, as the learned weight
for each token signifies feature importance in
quantifying subject-related knowledge.
•HS with top- kof vocabulary projection (VP-
k): Since the vocabulary space is typically large,
in order to make the probe more interpretable and
efficient, we perform feature selection over the
trained VP probe to extract the kmost influential
tokens from the vocabulary projections. We then
normalize and average the obtained 3∗kfeatures
(kfor each layer) to train a new smaller probe
overk-dimensional feature vectors.
For each of HS, VP, and VP- k, we apply Min-
Max normalization before averaging the extracted
vectors, which scales each feature to be within
[0,1]. For example, after extracting the hidden
states{h(s)
ℓ,sr|ℓ∈ L} for some subject s, we nor-
malize the values of every entry i∈[d]and layer
ℓ∈ L over a set of subjects S. Let ˆh(s)
ℓ,sr∈Rdbe
the normalized h(s)
ℓ,sr, so the feature vector for HS
is defined as z(s)=1
|L|P
ℓ∈Lˆh(s)
ℓ,sr∈Rd.3.2 Probing
We define the following probe for predicting the
model’s QA accuracy y(s)
QAor response factuality
y(s)
OEGgiven the features z(s)for a subject s:
f(z) :=σ(θ·z) (1)
Where σis the sigmoid function and θ∈Rdzis
a single linear transformation. The sigmoid non-
linearity is necessary to aid the model in learning
scores in the range [0,1].2
For each of the two tasks T∈ {QA, OEG }, we
optimize θover features and labels collected for a
set of subjects Sby minimizing the MSE loss:
LMSE(θ) =∥y(s)
T−σ(θ·z(s))∥2
2
For more details on the probes’ training, see §A.
4 Experiments
In this section, we evaluate KEEN and baselines that
rely on different intrinsic and external features. We
observed that the VP-50 probe obtained compa-
rable performance while being significantly more
interpretable (discussed in §C.2) so we focus on
evaluating the VP and VP-50 variants of this probe.
4.1 Experimental Setting
Data For the QA task, we sample 3,472 sub-
ject entities from PopQA (Mallen et al., 2023)
and generate a set of 5.3 questions on aver-
age per subject. To generate questions, we
take subject-relation-object triplets from Wiki-
data (Vrande ˇci´c and Krötzsch, 2014) and con-
vert them into question-answer pairs with hand-
written templates. For instance, the triplet
(Napoleon, place of birth, France )will be con-
verted to the question “Where was Napoleon born?”
and the answer “France” . In addition, we augment
each such example with multiple variants that cover
different answer granularities (Yona et al., 2024),
accounting for both answer and subject aliases, and
handling cases with multiple answers. We consider
a model’s prediction for a given subject-relation
pair as correct if it contains an exact match with
any answer alias in at least one question variation.
For the OEG setting, we use the FActScore
dataset (Min et al., 2023), which includes model-
generated biographies, extracted claims, and claim
2We also experimented with linear probes and found that
they tended to converge to scores in a narrow range around
0.5, failing to capture the signals in the inputs.labels which indicate whether the claim is sup-
ported or not-supported by the subject’s Wikipedia
page. We compare our results to the FActScore
scores of the same generating model.
Examples for the two tasks are shown in Table 1.
For both settings, we randomly split each dataset
into disjoint sets of subjects: 65% train, 15% devel-
opment, and 20% test. Importantly, the FActScore
dataset and QA train set have a negligible number
of overlapping subjects, 1 (0.2%), which allows us
to test transfer learning between the two settings. In
§B, we include additional details regarding dataset
generation.
Baselines We evaluate three baselines that utilize
intrinsic features and external features. For intrin-
sic features, we take the two best variants reported
by Snyder et al. (2023), which trained binary hallu-
cination detectors for QA. These detectors use the
outputs from the self-attention and MLP modules
as features, which were also considered by other
recent methods for similar tasks (Yu et al., 2024;
Yuksekgonul et al., 2024; Li et al., 2023).
•Entity popularity (Pop.) : It has been established
that LLM performance is influenced by entity
popularity (Mallen et al., 2023; Kandpal et al.,
2023; Yona et al., 2024). We follow previous
works (e.g., Chen et al., 2021; Mallen et al., 2023;
Cohen et al., 2024) and approximate entity popu-
larity using statistics from Wikipedia. Concretely,
we use the total number of monthly views of the
entity’s page between the years 2000-2023.3
•Self-attention outputs (ATTN) : We train the
same probe of KEEN (Eq. 1), while using a(s)
L,sr
as the feature vector z(s), i.e., the output of the
last self-attention sublayer for the last input token
(which is the last subject token in our setup).
•Fully-connected activations (FC) : Here we
train a similar probe to ATTN, which sets z(s)
tom(s)
L,sr, the output of the last MLP sublayer for
the last input token.
Models We analyze 7 auto-regressive language
models across various sizes, latent spaces, and
training objectives: GPT2-XL (Radford et al.,
2019), Pythia 6B and 12B (Biderman et al., 2023),
LLaMA2 7B and 13B (Touvron et al., 2023), and
3We also computed thresholded log-popularity as implied
by Figure 1 in Mallen et al. (2023). KEEN is superior across all
settings and models, except Vicuna 13B in the OEG setting
where correlation increased to 0.65 while KEEN achieved 0.66.Input Task Output Example ⟨question, model answer ⟩/⟨claim, correctness label ⟩pairs
subject s y(s)
QA/ y(s)
OEG fromQ/R
George
WashingtonQA⟨In what city was George Washington born?, Westmoreland County ⟩,
0.67 ⟨What is the religion of George Washington?, Episcopal Church ⟩
⟨Who is the father of George Washington?, Augustine Washington ⟩
OEG⟨George Washington was a military man. ,1⟩,
0.74 ⟨George Washington was the first President of the United States. ,1⟩,
⟨He was educated at the College of William and Mary. ,0⟩
Table 1: Example input subject and the expected outputs for the two tasks for Pythia 12B. The output labels were
computed based on the average QA accuracy over 12 questions (0.67), and the FActScore score for 35 claims (0.74).
GPT2 Pythia Pythia LLaMA2 LLaMA2 Vicuna
XL 6B 12B 7B 13B 13B
Pop. 0.30 0.32 0.28 0.27 0.25 0.26
FC 0.49 0.59 0.55 0.50 0.49 0.49
ATTN 0.53 0.63 0.60 0.58 0.50 0.52
VP-50 0.54 0.64 0.59 0.53 0.48 0.50
VP 0.61 0.68 0.64 0.64 0.58 0.60
HS 0.60 0.68 0.64 0.64 0.58 0.60
Table 2: Correlation with average QA accuracy for KEEN
QA probes and baselines on the QA test set.
Vicuna 13B (Chiang et al., 2023).4The vocabu-
lary sizes range between 30K-50K tokens and the
hidden state dimensions range from 4096-5120.
Evaluation For every model and subject sin our
data, we feed the model a generic prompt “This
document describes [ s]”and extract the fea-
tures used for all methods: KEEN and the above
baselines. Using these features, we obtain predic-
tions for our two tasks for every method. For the
Pop. baseline, we simply take the corresponding
popularity value of the subject. We report Pearson
correlation, associated p-values ( p), and the MSE
between the predicted and gold scores, for every
task, model and method. Correlation results are
provided in §4.2, and the p-values and MSE results
are reported in §C.
4.2 Results
KEEN well-estimates the model’s knowledge
about the subject entity Tables 2 and 3 show
QA and OEG results, respectively.
In both settings and across all models, KEEN
probes trained on hidden representations and vocab-
ulary projections demonstrate the strongest correla-
tion of 0.60-0.68 with QA accuracy ( p≤3.43e−70)
and 0.66-0.77 with FActScore ( p≤4.02e−6). This
shows that it is possible to predict how knowledge-
4Vicuna 7B was also analyzed, but due to its poor accuracy
in the QA setting and inconsistent behavior, we omitted it.Model Pop. FC ATTN VP-50 VP HS
Pythia 12B 0.36 0.61 0.77 0.72 0.75 0.77
Vicuna 13B 0.37 0.49 0.65 0.55 0.66 0.66
Table 3: Correlation with FActScore for KEEN OEG
probes and baselines on the OEG test set.
Model Pop. FC ATTN VP-50 VP HS
Pythia 12B 0.47 0.41 0.55 0.40 0.57 0.60
Vicuna 13B 0.40 0.52 0.50 0.48 0.61 0.62
Table 4: Transfer learning results, showing the correla-
tion between FActScore and KEEN QA probes and base-
lines. Results are reported over the full OEG dataset i.e.
all 500 subjects in the unlabeled FActScore dataset.
able a model is about an entity from the entity’s
hidden representations.
Predicting factuality based on common intrin-
sic features (FC and ATTN) consistently under-
performs with respect to KEEN , further supporting
the finding that entity knowledge is centralized in
entity representations during inference. Further-
more, the entity popularity baseline (Pop.) per-
forms poorly on both tasks, with low correlation
values of ≤0.32in QA and ≤0.36in OEG. This
shows that while external statistics of popularity
(such as Wikipedia page count) are useful in deriv-
ing general performance trends, they often fail to
provide fine-grained entity-level predictions.
Surprisingly, for the Pythia models even
theKEEN OEG VP-50 probe strongly corre-
lates (Akoglu, 2018; Schober et al., 2018) with
FActScore, indicating that there is a relatively small
set of tokens which are influential in increasing and
decreasing predicted accuracy. We further analyze
these tokens in §5.4 and provide intuition for inter-
preting them. Moreover, we discuss the trade-off
between interpretability and score correlation in
§C.2.KEEN QA probes generalize to predict factuality
in OEG Since knowledge is centralized in the
internal representations of entities, their use in esti-
mating knowledge should transfer across different
settings. Table 4 shows that the predictions of KEEN
QA probes have a moderate to strong correlation
(Akoglu, 2018; Schober et al., 2018) of 0.60-0.62
with FActScore ( p≤2.12e−5). Further, the corre-
lation ofKEEN QA probes with QA accuracy and
FActScore are notably similar, e.g. 0.60 and 0.62
for Vicuna 13B KEEN QA HS probes, respectively.
These results show that HS and VP features capture
signals that generalize across settings, regardless of
whether the task requires explicit (QA) or implicit
(OEG) recall of factual knowledge by the model.
5 Analysis
In this section, we further look into the predictions
and features of KEEN , evaluating its faithfulness
with respect to model hedging (§5.1) and changes
in the model’s knowledge following training (§5.2).
In addition, we analyze its errors (§5.3) and the
features of its VP-50 variant (§5.4).
5.1 Correlation with Model Hedging
To prevent factually incorrect responses, LLMs are
trained to hedge in cases of uncertainty, for ex-
ample by generating “I don’t know” (Ganguli
et al., 2023). Therefore, it is expected that models
generally hedge on entities they are less knowl-
edgeable about. Since the KEEN QA probe score
estimates entity-based knowledge, we hypothesize
that it should correlate with the fraction of ques-
tions that a model hedges on about the entity.
Figure 2 confirms this hypothesis, showing that
theKEEN QA VP score decreases as the fraction
of queries the model hedges on increases. This
implies that models may hedge based on features
of the model’s internal representations of the en-
tity, similarly to KEEN . Further details regarding the
choice of hedging phrases are provided in §B.3.
5.2 Reflecting Changes in Model Knowledge
Our experiments so far evaluated KEEN while keep-
ing the underlying LLM fixed. A natural question
that arises is whether changes in the model’s knowl-
edge are reflected in changes in the KEEN score. We
test this by fine-tuning LLaMA2 7B on paragraphs
about a target subject and measuring changes in
both the average QA accuracy and the KEEN QA
score. Concretely, we sample 20 subjects from
[0, 0.12](0.12, 0.25] (0.25, 0.38] (0.38, 0.50]
QA Hedging Fraction0.00.10.20.30.40.60.70.80.91.0Predicted ScoreVicuna 13B
[0, 0.12](0.12, 0.25] (0.25, 0.38] (0.38, 0.50]
QA Hedging Fraction0.00.10.20.30.40.60.70.80.91.0Predicted ScorePythia 12BFigure 2: KEEN QA scores as a function of the fraction
of per-subject queries that Vicuna 13B and Pythia 12B
hedge on.
T arget
KEENNon-T arget
KEENT arget
AccuracyNon-T arget
Accuracy-0.75-0.62-0.50-0.38-0.25-0.120.000.120.250.380.50Score Difference (Fine-tuned vs Base)LLaMA2 7B
Figure 3: Changes in the KEEN QA score and average
QA accuracy after fine-tuning LLaMA2 7B on para-
graphs about a target subject. These results are aggre-
gated over individual fine-tuning processes for 20 target
subjects.
the QA test dataset and retrieve paragraphs from
the Wikipedia page of each subject using BM25
(Robertson et al., 1995).5Then, we use LoRA (Hu
et al., 2022) to fine-tune <0.5%of the model’s
parameters, separately for each subject. After fine-
tuning for a certain target subject, we compute the
KEEN score for that subject, as well as for 256 non-
target subjects from the QA test dataset. The KEEN
QA probe trained over the model’s hidden states
before fine-tuning is used to compute these scores.
Figure 3 shows that on average, QA accuracy
scores for the target entities increase by 0.16 and
KEEN QA scores increase by 0.18 as models are
fine-tuned on paragraphs related to them. Con-
versely, inline with works about catastrophic for-
getting which find that models tend to forget in-
formation about entities observed in pre-training
(Tirumala et al., 2022), the QA accuracy scores
for non-target entities decrease after fine-tuning.
However, the KEEN scores for non-target entities
stay relatively constant. Since fine-tuning often
doesn’t erase residual information in LLMs (Patil
et al., 2024; Hong et al., 2024), and KEEN relies on
5We use the Wikipedia dump from August 28, 2023.intermediate representations, a possible explana-
tion for this discrepancy is that information is still
encoded in the representations but the model fails
to recall it. To test this hypothesis, we selected 85
non-target entities whose decrease in QA accuracy
after fine-tuning, 0.50±0.18to0.24±0.15, was
not reflected in changes in their KEEN scores post
fine-tuning, 0.46±0.12to0.49±0.12. We then
use activation patching to recover QA accuracy
from their intermediate fine-tuned representations,
demonstrating that they still encode entity-related
information (Ghandeharioun et al., 2024; Mosbach
et al., 2020).
Skip fine-tuned layers to recover accuracy (FT
subj.) This experiment shows that information
is recoverable from the fine-tuned representations
when later layers in the fine-tuned model are by-
passed. The procedure involves feeding the fine-
tuned model a question, extracting only the subject
representations (for all subject tokens) from an in-
termediate later, patching them into the penultimate
layer of the fine-tuned model, and then measur-
ing the patched QA accuracy per subject. Patch-
ing increases accuracy by 0.24±0.18above fine-
tuned accuracy (FT) and restores accuracy within
−0.01±0.17of pre-trained accuracy (PT), per
subject on average.
Apply pre-trained layers to recover accuracy
(PT layer) This experiment shows that informa-
tion can still be extracted from fine-tuned represen-
tations using the pre-trained model. The procedure
involves feeding the fine-tuned model a question,
extracting all representations from an intermedi-
ate layer, patching them into the penultimate layer
of the pre-trained model, and then measuring the
patched QA accuracy per subject. Similarly, patch-
ing increases accuracy by 0.27±0.19above FT
and restores accuracy within −0.03±0.19of PT.
These experiments demonstrate that KEEN scores
are reflective of the knowledge encoded in the in-
termediate representations, and that the estimation
gap between KEEN scores and QA accuracy post
fine-tuning is because the later layers in the fine-
tuned model suppress the generation of this infor-
mation. Further details about the patching proce-
dure and results are presented in §C.3.
5.3 Error Analysis
To better understand the limitations of KEEN , we
plot the probes’ predicted scores against the refer-
ence QA accuracy and FActScore scores.
Figure 4: Predicted scores from the KEEN QA VP probe
and the golden QA Accuracy scores are positively lin-
early related.
Figure 5: Predicted scores of the KEEN OEG VP probe
versus FActScore scores. KEEN scores are positively
linearly correlated with FActScore scores.
Figure 4 shows that the KEEN VP QA probes tend
to predict higher scores relative to QA accuracy for
subjects that the model knows less about, although
theKEEN scores for entities with QA accuracy be-
tween [0,0.5]do generally fall within a similar
range of [0.1,0.5]. For subjects that the model is
more knowledgeable about, KEEN QA scores are
more conservative, as seen by the cluster of scores
below the y=xline for QA accuracy values close
to 1.0. Generally, KEEN scores have less variance
than the QA accuracy scores since the slopes of the
trend-lines are <1, which may suggest that more
complex predictors are needed to capture all the
variance of QA accuracy. These trends are consis-
tent across models of different families and sizes.
In §C.5 and §C.6, we include results for the other
models, which follow the same trends in Figure 4Low
QA AccuracyHigh
QA Accuracy-15K-10K-5K05K10K15K20KMedian Rank DifferenceVP-25 Vicuna 13B
Low
QA AccuracyHigh
QA Accuracy-15K-10K-5K05K10K15K20KMedian Rank DifferenceVP-50 Vicuna 13B
Low
QA AccuracyHigh
QA Accuracy-15K-10K-5K05K10K15KMedian Rank DifferenceVP-25 Pythia 12B
Low
QA AccuracyHigh
QA Accuracy-15K-10K-5K05K10K15KMedian Rank DifferenceVP-50 Pythia 12BFigure 6: Difference, per subject, in the median rank of
tokens with negative weight and tokens with positive
weight. Pythia 12B VP-25 and VP-50 show the trade-
off between interpretability and performance – though
the median ranks of negative weight tokens are higher
on average than positive weight tokens in VP-50, there
is still a clear split in both accuracy groups.
Weight Example influential tokensPythia 12BPosanalysis, Statistical, Players, Senator,
Quantum, nationality, investments
Negcircadian, AMPK, lys, 16, jo, VERT, diese,
see, Mort, ))*, dep, imi, acVicuna 13BPosathlet, kick, swing, developer, compiling,
official, sales, GitHub, Movie
Negsle, hurt, Circ, Alt, book, JK, ja, adow,
istema, ppings, adjust, istol
Table 5: Examples of the most influential tokens in the
KEEN QA VP probes that were assigned positive and
negative weights. These are some of the tokens that
correspond to the features of KEEN QA VP-50 probes.
and Figure 5. We also provide the scatter plots
for probes trained on the different KEEN features
and baselines, all demonstrating the same linear
relations.
5.4 Feature Analysis for VP-25 and VP-50
Identifying prominent features We analyze the
most influential features of the KEEN QA VP probes
to understand which tokens contribute most to pre-
dicting average QA accuracy. Our goal in this
analysis is to identify tokens that either increase or
decrease predicted QA accuracy, and to determine
whether they are promoted in the representations
of subjects with high and low QA accuracy, respec-
tively. As a concrete example, for subjects with
low QA accuracy, we expect tokens that decrease
QA accuracy to generally be ranked higher in the
subject representations than tokens that increaseQA accuracy (the highest rank is 0 which corre-
sponds to the token with highest logit value). Since
the input normalization scheme described in §3.1
normalizes a token’s logit in a given hidden state by
its magnitude across all subjects (not with respect
to the other tokens in the hidden state), we can
interpret the weight learned by the KEEN QA VP
probe for each token as its direction and magnitude
of influence on the predicted score.
First, we identify the tokens associated with the
largest absolute weights in the KEEN QA VP probes,
as they are most influential on the predicted score.
Next, we compare the median rank of tokens with
negative weights to those with positive weights in
the vocabulary projections of subjects with high
QA accuracy (1.0) and low QA accuracy (0.0). Fig-
ure 6 shows that for low QA accuracy subjects, the
median rank of negative weight tokens is generally
higher than that of positive weight tokens. Con-
versely, for high QA accuracy subjects, the median
rank of negative weight tokens is generally lower
than that of positive weight tokens. This oppos-
ing trend in the two accuracy groups indicates that
there is a small set of tokens which hold signals for
differentiating between subjects the model knows
more about and those it knows less about. We
provide these important tokens in Table 5.
Mapping knowledge clusters and “holes” To-
kens assigned positive weight are related to mean-
ingful concepts while tokens assigned negative
weight are often numbers, abbreviations, or suf-
fixes. A possible interpretation of this difference is
that the hidden states of low accuracy subjects re-
flect “holes” in the model’s knowledge, thereby en-
coding less information and promoting less seman-
tically meaningful tokens. These negative weight
tokens may help identify additional “holes” if sim-
ilarly promoted in other subject representations.
Conversely, positive weight tokens can be used
to identify clusters of knowledge. Table 6 shows
that Pythia 12B and Vicuna 13B encode more in-
formation about political figures, athletic players,
or movies. This is evidenced by the 0.20−0.55
higher than average QA accuracy for a sample of
subjects with high logit values ( ≥0.65) for tokens
likeSenator ,Player ,Movie , andathlet .
6 Related Work
Evaluation of knowledge and factuality of LLMs
The common practice for estimating knowledge in
LLMs is to query the model and then evaluate itsToken Subject Logit QAPythia 12BSenatorRand Paul 0.84 0.55
Amin Amidu Sulemana 0.79 0.67
Beau Biden 0.67 0.54
Chelsea Clinton 0.68 0.58
Mean Entity 0.41 0.35
PlayersCharlton Athletic F.C. 0.84 0.60
Aston Villa F.C. 0.81 0.60
AC Milan 0.79 0.75
Moneyball 0.76 0.67
Mean Entity 0.44 0.35Vicuna 13BMovieAttarintiki Daredi 0.86 0.71
Iron Man 3 0.83 0.63
Friday the 13th Part 2 0.76 0.88
National Treasure: Book of Secrets 0.76 0.86
Mean Entity 0.31 0.45
athletCharlotte Hornets 0.97 1.00
Chariots of Fire 0.84 0.63
Olympique Lyonnais 0.74 0.60
The Cowboys 0.67 0.71
Mean Entity 0.41 0.45
Table 6: Examples of subjects from the full validation
set with high logit values for influential positive tokens.
Their high QA accuracies, relative to the Mean Entity ,
indicate knowledge clustered around the specific con-
cepts: political figures, athletes/players, and movies.
outputs. This is often conducted though question-
answering setups with gold labels (Roberts et al.,
2020; Petroni et al., 2019; Cohen et al., 2023a,
inter alia), by letting the model generate multiple
responses and measuring response consistency (Co-
hen et al., 2023b; Manakul et al., 2023; Kuhn et al.,
2023), checking whether the generated output is
supported by external evidence (Gao et al., 2023;
Bohnet et al., 2022; Min et al., 2023), or by estimat-
ing the model’s uncertainty per-response (Zhang
et al., 2023; Jesson et al., 2024). Unlike these
methods, we focus on evaluating the model’s entity
knowledge beyond a single response, based on in-
trinsic features extracted before generating a single
token.
Probing internal representations of LLMs
Probing over internal representations has been used
to predict model behavior, such as truthfulness
(Marks and Tegmark, 2024; Azaria and Mitchell,
2023a), and properties of language, such as part-of-
speech (Belinkov et al., 2017; Nikolaev and Padó,
2023), syntax (Hewitt and Manning, 2019), and
sentence length (Adi et al., 2017) for a specific in-
put. Probing has also been used to identify which
hidden states are most influential on the perfor-
mance of tasks, like classification (Alain and Ben-
gio, 2017). Our use of probing differs from prior
work because we estimate a property that capturesmodel behavior over many inputs rather than a
single input. Namely, the KEEN score provides a
knowledge estimate relevant to any input concern-
ing the entity. Further, KEEN focuses on estimating
entity-specific knowledge and is useful in evalu-
ating several model behaviors, including hedging,
shifts in knowledge, and truthfulness.
Hallucination detection using intrinsic features
Our work is closely related to methods that leverage
intrinsic features for detecting factually incorrect
claims, but has two core differences. The first be-
ing in our choice of features: we specifically use
the hidden states corresponding to the named en-
tity from the upper intermediate layers. In contrast,
existing methods use various other features, like in-
termediate activation values (Azaria and Mitchell,
2023b), outputs from the self-attention modules
(Yu et al., 2024; Yuksekgonul et al., 2024; Li et al.,
2023; Snyder et al., 2023), soft-max prediction
probabilities, and fully-connected scores (Snyder
et al., 2023). Yu et al. (2024); Goloviznina and
Kotelnikov (2024); Su et al. (2024) also examine
the intermediate hidden representations, but for the
purpose of identifying whether there exists a sub-
space of hidden states that lead to hallucinations.
Similarly to our work, Yu et al. (2024) uses the
hidden representations of subjects, but rather to
train a binary hallucination detector. Unlike all
these works that use internal representations to pre-
dict the factuality of a specific claim, we learn to
estimate knowledge from a single internal repre-
sentation of an entity, which is applicable to any
claim pertaining to it.
7 Conclusion
We present the problem of estimating entity knowl-
edge solely from the model’s internal representa-
tions of the entity. We show that KEEN offers a
simple and interpretable solution which correlates
with model performance in both QA and OEG set-
tings, as well as with current hallucination detec-
tion methods. Further, KEEN is also reflective of
both hedging behavior and changes in knowledge
throughout fine-tuning. From a broad perspective,
our results demonstrate the potential of estimat-
ing model qualities and behavior for certain inputs
based on intrinsic features, and call for future work
to leverage simple and efficient methods like KEEN
to improve the factuality and reliability of LLMs.Limitations
While our approach successfully estimates the ex-
tent of the model’s knowledge about a subject, it
does not identify the presence or lack of knowledge
about specific facts. For instance, KEEN can esti-
mate that the model will be 55% truthful when gen-
erating content about Napoleon , but it does not pin-
point that the model is unable to answer the specific
question, What military academy did Napoleon at-
tend? . An interesting direction for future work
would be to develop a more fine-grained approach
that predicts how knowledgeable the model is about
specific aspects of the subject (e.g. military career
of Napoleon) or identifies specific facts encoded in
subject representations.
Another limitation is that this work focuses on
estimating knowledge for entities, however not all
subjects of questions are entities. For example,
there is no clear subject for which we can apply
KEEN in the question, How does exercise influence
mental health? .KEEN also assumes that the subjects
are already extracted for analysis. While identify-
ing named entities in text is a well-studied task in
NLP (Nadeau and Sekine, 2007), combining it with
KEEN could make this approach more complex and
computationally expensive.
Our evaluation focuses only on transformer-
based auto-regressive LLMs. While this is one of
the most popular and largest families of LLMs, it
would be valuable to study the applicability of KEEN
to other model architectures. Notably, Sharma et al.
(2024) shows that factual recall in Mamba is simi-
larly centered in the hidden states of the last subject
token from the intermediate layers, so we expect
our approach to generalize to other recurrent archi-
tectures.
Acknowledgements
We thank Amir Globerson for insightful discus-
sions and feedback throughout the project. We also
thank Ori Yoran and Gal Yona for providing use-
ful feedback on this manuscript. This work was
supported by the Tel Aviv University Center for
AI and Data Science (TAD), the Israeli Science
Foundation, and the Deutsch Foundation.
References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi,
and Yoav Goldberg. 2017. Fine-grained analysis of
sentence embeddings using auxiliary prediction tasks.InInternational Conference on Learning Representa-
tions .
Haldun Akoglu. 2018. User’s guide to correlation co-
efficients. Turkish Journal of Emergency Medicine ,
18.
Guillaume Alain and Yoshua Bengio. 2017. Under-
standing intermediate layers using linear classifier
probes.
Amos Azaria and Tom Mitchell. 2023a. The internal
state of an LLM knows when it’s lying. In The 2023
Conference on Empirical Methods in Natural Lan-
guage Processing .
Amos Azaria and Tom Mitchell. 2023b. The internal
state of an LLM knows when it’s lying. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2023 , pages 967–976, Singapore. Associa-
tion for Computational Linguistics.
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-
san Sajjad, and James Glass. 2017. What do neural
machine translation models learn about morphology?
InProceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 861–872, Vancouver, Canada.
Association for Computational Linguistics.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar Van Der Wal. 2023. Pythia:
a suite for analyzing large language models across
training and scaling. In Proceedings of the 40th Inter-
national Conference on Machine Learning , ICML’23.
JMLR.org.
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aha-
roni, Daniel Andor, Livio Baldini Soares, Massimil-
iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,
Jonathan Herzig, et al. 2022. Attributed question an-
swering: Evaluation and modeling for attributed large
language models. arXiv preprint arXiv:2212.08037 .
Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao
Ling, and Sameer Singh. 2021. Evaluating entity
disambiguation and the role of popularity in retrieval-
based NLP. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 4472–4485, Online. Association for Computa-
tional Linguistics.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,
and Mor Geva. 2024. Evaluating the ripple effectsof knowledge editing in language models. Transac-
tions of the Association for Computational Linguis-
tics, 12:283–298.
Roi Cohen, Mor Geva, Jonathan Berant, and Amir
Globerson. 2023a. Crawling the internal knowledge-
base of language models. In Findings of the Asso-
ciation for Computational Linguistics: EACL 2023 ,
pages 1856–1869, Dubrovnik, Croatia. Association
for Computational Linguistics.
Roi Cohen, May Hamri, Mor Geva, and Amir Glober-
son. 2023b. LM vs LM: Detecting factual errors
via cross examination. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 12621–12640, Singapore.
Association for Computational Linguistics.
Vincent Denault, Chloé Leclerc, and Victoria Talwar.
2024. The use of nonverbal communication when
assessing witness credibility: a view from the bench.
Psychiatry, Psychology and Law , 31(1):97–120.
Deep Ganguli, Amanda Askell, Nicholas Schiefer,
Thomas I Liao, Kamil ˙e Lukoši ¯ut˙e, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Catherine Olsson, Danny
Hernandez, et al. 2023. The capacity for moral self-
correction in large language models. arXiv preprint
arXiv:2302.07459 .
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
Kelvin Guu. 2023. RARR: Researching and revising
what language models say, using language models.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 16477–16508, Toronto, Canada.
Association for Computational Linguistics.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 12216–12235,
Singapore. Association for Computational Linguis-
tics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 5484–5495, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Asma Ghandeharioun, Avi Caciularu, Adam Pearce,
Lucas Dixon, and Mor Geva. 2024. Patch-
scopes: A unifying framework for inspecting hid-
den representations of language models. Preprint ,
arXiv:2401.06102.
Valeriya Goloviznina and Evgeny Kotelnikov. 2024.
I’ve got the "answer"! interpretation of llms
hidden states in question answering. Preprint ,
arXiv:2406.02060.Evan Hernandez, Belinda Z. Li, and Jacob Andreas.
2024a. Inspecting and editing knowledge represen-
tations in language models. In First Conference on
Language Modeling .
Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin
Meng, Martin Wattenberg, Jacob Andreas, Yonatan
Belinkov, and David Bau. 2024b. Linearity of rela-
tion decoding in transformer language models. In
The Twelfth International Conference on Learning
Representations .
John Hewitt and Christopher D. Manning. 2019. A
structural probe for finding syntax in word represen-
tations. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4129–4138, Minneapolis, Minnesota. Association for
Computational Linguistics.
Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang,
and Mor Geva. 2024. Intrinsic evaluation of un-
learning using parametric knowledge traces. arXiv
preprint arXiv:2406.11614 .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Jing Huang, Zhengxuan Wu, Christopher Potts, Mor
Geva, and Atticus Geiger. 2024. Ravel: Eval-
uating interpretability methods on disentangling
language model representations. arXiv preprint
arXiv:2402.17700 .
Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu,
Sweta Karlekar, Jannik Kossen, Yarin Gal, John P.
Cunningham, and David Blei. 2024. Estimating
the hallucination rate of generative ai. Preprint ,
arXiv:2406.07457.
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
Wallace, and Colin Raffel. 2023. Large language
models struggle to learn long-tail knowledge. In In-
ternational Conference on Machine Learning , pages
15696–15707. PMLR.
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.
Semantic uncertainty: Linguistic invariances for un-
certainty estimation in natural language generation.
InThe Eleventh International Conference on Learn-
ing Representations .
Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.
Implicit representations of meaning in neural lan-
guage models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 1813–1827, Online. Association for
Computational Linguistics.Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter
Pfister, and Martin Wattenberg. 2023. Inference-
time intervention: Eliciting truthful answers from
a language model. In Thirty-seventh Conference on
Neural Information Processing Systems .
Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,
Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.
A token-level reference-free hallucination detection
benchmark for free-form text generation. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 6723–6737, Dublin, Ireland. Association
for Computational Linguistics.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 9802–9822, Toronto,
Canada. Association for Computational Linguistics.
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
SelfCheckGPT: Zero-resource black-box hallucina-
tion detection for generative large language models.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
9004–9017, Singapore. Association for Computa-
tional Linguistics.
Samuel Marks and Max Tegmark. 2024. The geometry
of truth: Emergent linear structure in large language
model representations of true/false datasets.
Kevin Meng, David Bau, Alex J Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems .
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. FActScore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 12076–12100, Singa-
pore. Association for Computational Linguistics.
Marius Mosbach, Anna Khokhlova, Michael A. Hed-
derich, and Dietrich Klakow. 2020. On the interplay
between fine-tuning and sentence-level probing for
linguistic knowledge in pre-trained transformers. In
Proceedings of the Third BlackboxNLP Workshop on
Analyzing and Interpreting Neural Networks for NLP ,
pages 68–82, Online. Association for Computational
Linguistics.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvis-
ticae Investigationes , 30:3–26.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004 , pages 145–152, Boston, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
Dmitry Nikolaev and Sebastian Padó. 2023. Investi-
gating semantic subspaces of transformer sentence
embeddings through linear structural probing. In
Proceedings of the 6th BlackboxNLP Workshop: An-
alyzing and Interpreting Neural Networks for NLP ,
pages 142–154, Singapore. Association for Compu-
tational Linguistics.
nostalgebraist. 2020. interpreting gpt: the logit lens.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch: An
imperative style, high-performance deep learning li-
brary. In Advances in Neural Information Processing
Systems , volume 32. Curran Associates, Inc.
Vaidehi Patil, Peter Hase, and Mohit Bansal. 2024. Can
sensitive information be deleted from LLMs? ob-
jectives for defending against extraction attacks. In
The Twelfth International Conference on Learning
Representations .
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
ton Bakhtin, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2019. Language models as knowl-
edge bases? In Conference on Empirical Methods in
Natural Language Processing .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. In Lan-
guage Models are Unsupervised Multitask Learners .
Martin S Remland. 1994. The importance of nonverbal
communication in the courtroom. Atlantic Journal
of Communication , 2(2):124–145.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 5418–5426,
Online. Association for Computational Linguistics.
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp ,
109:109.
Patrick Schober, Christa Boer, and Lothar Schwarte.
2018. Correlation coefficients: Appropriate use and
interpretation. Anesthesia & Analgesia , 126:1.Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ra-
makanth Pasunuru, Mohit Bansal, Yael Amsterdamer,
and Ido Dagan. 2019. Crowdsourcing lightweight
pyramids for manual summary evaluation. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) , pages 682–687, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Arnab Sen Sharma, David Atkinson, and David Bau.
2024. Locating and editing factual associations in
mamba. Preprint , arXiv:2404.03646.
Ben Snyder, Marius Moisescu, and Muhammad Bi-
lal Zafar. 2023. On early detection of halluci-
nations in factual question answering. Preprint ,
arXiv:2312.14183.
Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU,
Zhijing Wu, Yujia Zhou, and Yiqun Liu. 2024. Unsu-
pervised real-time hallucination detection based on
the internal states of large language models. Preprint ,
arXiv:2403.06448.
Kushal Tirumala, Aram H. Markosyan, Luke Zettle-
moyer, and Armen Aghajanyan. 2022. Memorization
without overfitting: Analyzing the training dynamics
of large language models. In Advances in Neural
Information Processing Systems .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint , arXiv:2307.09288.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Denny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Communi-
cations of the ACM , 57(10):78–85.Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2024. Efficient streaming lan-
guage models with attention sinks. In The Twelfth
International Conference on Learning Representa-
tions .
Gal Yona, Roee Aharoni, and Mor Geva. 2024. Nar-
rowing the knowledge evaluation gap: Open-domain
question answering with multi-granularity answers.
arXiv preprint arXiv:2401.04695 .
Lei Yu, Meng Cao, Jackie Chi Kit Cheung, and
Yue Dong. 2024. Mechanisms of non-factual
hallucinations in language models. Preprint ,
arXiv:2403.18167.
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones,
Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece
Kamar, and Besmira Nushi. 2024. Attention satis-
fies: A constraint-satisfaction lens on factual errors
of language models. In The Twelfth International
Conference on Learning Representations .
Shiyue Zhang and Mohit Bansal. 2021. Finding a bal-
anced degree of automation for summary evaluation.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
6617–6632, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,
Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing
Wang, and Luoyi Fu. 2023. Enhancing uncertainty-
based hallucination detection with stronger focus.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
915–932, Singapore. Association for Computational
Linguistics.AKEEN Training Details
A.1 Hyper-parameter Tuning and Resources
Hyper-parameter tuning for KEEN probes All
KEEN QA and OEG probes were trained with the
AdamW optimizer with weight decay 0.01, and
batch size of 32, and learning rates were optimized
over1e−3,5e−3,5e−4,1e−4,1e−5,5e−5. The best
hyper-parameters for QA and OEG KEEN probes
are found in Table 7 and Table 8, respectively. The
epochs represents the maximum number of epochs
set for training since we did not implement early
stop, however all evaluations are done on the check-
point with the highest Pearson correlation on the
validation set.
KEEN Hyper GPT2 Pythia Pythia LLaMA LLaMA Vicuna
Probe Param XL 6B 12B 7B 13B 13B
HS Epoch 3K 100 100 1K 1K 100
LR 1e−51e−41e−41e−51e−51e−4
VP Epoch 3K 100 3K 1K 1K 3K
LR 1e−51e−41e−51e−51e−51e−5
VP-200 Epoch 3K 500 3K 3K 3K 3K
LR 1e−51e−41e−41e−41e−41e−4
VP-100 Epoch 3K 500 3K 3K 3K 3K
LR 1e−51e−41e−41e−41e−41e−4
VP-50 Epoch 3K 1K 3K 3K 3K 3K
LR 1e−51e−41e−41e−41e−41e−4
VP-25 Epoch 5K 3K 3K 3K 3K 3K
LR 1e−51e−41e−41e−41e−41e−4
VP-10 Epoch - - - 3K - -
LR 1e−51e−41e−41e−41e−41e−4
FC Epoch - - - 1K - -
LR - - - 1e−5- -
ATTN Epoch - - - 1K - -
LR - - - 1e−5- -
Table 7: Hyper-parameters for KEEN QA probes and
baselines.
Hyper-parameters for fine-tuning LLaMA2 7B
The training details for the fine-tuning experiment
in §5.2 are described in Table 9.
Resources All our experiments were conducted
using the PyTorch package (Paszke et al., 2019) on
a single A100 or H100 GPU.
A.2 Choice of Input Layer Configuration
Our choice of choosing representations from the
upper middle layers and the last subject position
were motivated by previous work (Geva et al., 2023;
Meng et al., 2022; Hernandez et al., 2024a). The
focus on the last subject position is because in auto-
regressive models, earlier positions cannot capture
the full subject name. To systematically defendKEEN Probe Hyper-parameter Pythia 12B Vicuna 13B
HS Epoch 1K 1K
LR 1e−51e−4
VP Epoch 5K 3K
LR 1e−51e−5
VP-200 Epoch 5K 3K
LR 1e−41e−4
VP-100 Epoch 5K 5K
LR 1e−45e−4
VP-50 Epoch 5K 5K
LR 1e−45e−4
VP-25 Epoch 5K 5K
LR 5e−41e−3
VP-10 Epoch 5K 5K
LR 1e−35e−4
FC Epoch 1K 1K
LR 5e−55e−5
ATTN Epoch 1K 1K
LR 1e−55e−5
Table 8: Hyper-parameters for KEEN OEG probes and
baselines.
Optimizer LR Epoch Scheduler Warm Up LoRA LoRA LoRA
ratio alpha dropout r
AdamW 2e−4100 Linear 0.03 16 0.1 64
Table 9: Hyper-parameters for fine-tuning LLaMA2 7B.
this decision, we repeated experiments recorded
in Table 4 with LLaMA2 7B and Pythia 12B and
trained new KEEN QA VP probes using the follow-
ing layers as features: first 3 layers (Early) , last
3 layers (Late), 1 layer upper-intermediate layer
(One) (22, 25), 5 upper-intermediate layers (Five)
([19, 24), [22, 27)). Correlation values between
KEEN estimates and QA accuracy are statistically
significant ( p≤2e−50), and provided in Table 10.
We find a clear benefit to using upper-
intermediate to late layers over early layers and
no improvement when averaging over multiple up-
per intermediate layers. Using the late layers has
comparable correlation values as using the upper in-
termediate layers, which supports previous findings
that knowledge is aggregated in the subject’s hid-
den states of the upper intermediate layers (Geva
et al., 2023; Meng et al., 2022).
A.3 Discussion of KEEN Efficiency and
Training Time Statistics
KEEN has two key efficiency advantages.
(1) Though KEEN probes are only trained on
a small set of entities, they generalize well, which
is especially significant considering that models
are trained on millions of entities. Estimating
knowledge with KEEN , for subjects not used to
train the probes, requires a single inference pass
and the execution of the KEEN probe. In contrast,Model Early Late One Five KEEN VP
Pythia 12B 0.50 0.64 0.60 0.64 0.64
LLaMA2 7B 0.45 0.63 0.64 0.64 0.64
Table 10: Pearson correlation values of QA probes trained on hidden representations from various layer configura-
tions.
the computational cost of query-based evaluation
requires multiple inference passes per query.
With every query requiring 1-5 passes (due to
tokenization) and an average number of 5.3 queries
per subject, query-based evaluation takes 5.3-27
times longer. (2) KEEN can be applied ubiquitously
to all subjects since it does not require labeled data,
beyond a small sample for training. Labeling is a
major hurdle, especially for OEG where complex
processing and verification of the output is needed
(Min et al., 2023). Query-based evaluation
approaches also require access to knowledge bases
(KB) which inherently restricts the subjects that
can be evaluated to those in the KB.
To demonstrate the training time benefits of
KEEN , in Table 11 we report the epoch and prac-
tical length of training for KEEN QA probes used in
our evaluations.
Table 11: Practical number of training epochs and times
ofKEEN QA probes.
Model ProbeBest Training
Epoch (Min)
GPT2 XLVP 110 5
HS 3K 20
Pythia 12BVP 279 30
HS 100 1
Pythia 6BVP 44 2.5
HS 100 8
LLaMA2 7BVP 167 5
HS 1K 10
LLaMA2 13BVP 1K 1
HS 321 16.5
Vicuna 13BVP 100 1
HS 202 5
B Dataset Details
B.1 Question Answering Dataset
Examples are sampled as (subject, predicate, ob-
ject) triplets from Wikidata, and questions are
formed using manually written templates with
placeholders for the subject and answer entity type
(from the “instance of” property in Wikidata). Weform directed questions using answer entity types
to decrease the ambiguity of the expected answer
i.e. instead of “Where was Barack Obama born?”
we ask “In what city was Barack Obama born?”.
Also, to increase the coverage of knowledge per
entity, we extended the PopQA dataset to include
an additional 26 relations to a total of 42 relations.
Examples of answer templates and questions are
found in Table 12. The train split consists of 2,223
entities and 12,324 questions, validation split has
555 entities and 3,003 questions, and test split has
694 entities and 3,821 questions.
B.2 Open-Ended Generation Dataset
The unlabeled 500 subject FActScore dataset was
used to evaluate the KEEN OEG probes, and the
transfer of KEEN QA probes to the OEG setting.
B.3 Model Hedging Behavior
The experiment described in §5.1 assessed the cor-
relation between the fraction of queries for which
the model exhibited hedging behavior and the
KEEN score for a given entity. To determine the
hedging fraction, the model was prompted with a
set of common question-answer pairs about the
entity, and the proportion of responses contain-
ing an exact match with some hedging phrase
was calculated. Hedging phrases were identified
through manual analysis of model responses and
included the expressions “nobody knows,” “I’m
sorry,” “I can’t seem to find the answer,”
“could you help me,” “can anyone help
me,” “I’m not sure,” “I don’t know,” “I’m
not entirely sure,” “could you please
provide more,” “could you provide more
information,” “provide more context,” and
“clarify your question.”
C Additional Results
C.1 Mean Standard Error (MSE) for KEEN
Table 13, Table 14, Table 15, present the MSE
for theKEEN OEG probes with FActScore scores,
KEEN OEG probes with FActScore scores, the KEENRelation Question Template Entity Count
genre What genre is [subj]? 1979
country of origin What is the country of origin of [subj]? 1574
director Who was the director of [subj]? 1196
screenwriter Who was the screenwriter of [subj]? 1174
producer Who was the producer of [subj]? 1163
occupation What is [subj]’s occupation? 1092
color What color is [subj]? 1044
composer Who was the composer of [subj]? 1041
place of birth In what [obj_type] was [subj] born? 977
country of citizenship What is [subj]’s country of citizenship? 922
country In what country is [subj]? 913
languages spoken, written or signed What language does [subj] speak? 632
sport What sport does [subj] play? 503
language of work or name What is the language of [subj]? 493
capital What is the capital of [subj]? 482
author Who is the author of [subj]? 452
performer Who is the performer of [subj]? 361
educated at What is the alma mater of [subj]? 359
place of death In what [obj_type] was [subj] born? 343
followed by What [obj_type] follows [subj]? 332
father Who is the father of [subj]? 327
religion or worldview What is the religion of [subj]? 276
member of sports team What sports team does [subj] play for? 270
record label What is the record label of [subj]? 267
mother Who is the mother of [subj]? 250
position played on team / speciality What sports position does [subj] play? 241
spouse Who is the spouse of [subj]? 218
participant in In what sports event did [subj] participate in? 218
publisher Who is the publisher of [subj]? 217
sibling Who is the sibling of [subj]? 212
child Who is the child of [subj]? 211
capital of What is [subj] the capital of? 211
native language What is the native language of [subj]? 172
religion or worldview What is the religion of [subj]? 168
member of political party What is the political party associated with [subj]? 135
work location In what [obj_type] does [subj] work in? 110
country for sport What country does [subj] play for? 92
headquarters location In what [obj_type] are the headquarters of [subj] located? 80
league What sports league does [subj] play in? 74
lyrics by Who wrote the lyrics of [subj]? 70
consecrator Who is the consecrator of [subj]? 33
editor Who is the editor of [subj]? 12
Table 12: Templates used for generating questions for the QA dataset with the counts of attributed entities.
QA probes with FActScore scores, and the KEEN
QA probes with average QA accuracy scores. The
performance of these probes is discussed in §4.2.
C.2 Interpretability-Performance Tradeoff
forKEEN VP-k
Figure 7 and Figure 8 demonstrate diminishing
returns in increasing the parameter count beyond
50 tokens, suggesting that a small set of tokensModel Freq. FC ATTN VP-50 VP HS
Pythia-12B 0.028 0.026 0.014 0.020 0.017 0.014
Vicuna-13B 0.052 0.075 0.049 0.052 0.040 0.039
Table 13: MSE for KEEN OEG Probes between predicted
KEEN scores and FActScore scores.
contains significant signals for estimating entity
knowledge. There is a clear trade-off between theModel Freq. FC ATTN VP-50 VP HS
Pythia 12B - 0.053 0.043 0.047 0.062 0.074
Vicuna 13B - 0.046 0.053 0.052 0.049 0.050
Table 14: MSE for KEEN QA probes and FActScore.
The MSE scores for FC are omitted because we do not
computeKEEN probes on this feature.
Freq. FC ATTN HS VP-50 VP
GPT2 XL 0.053 0.053 0.046 0.041 0.045 0.040
Pythia 6B 0.063 0.052 0.048 0.045 0.047 0.042
Pythia 12B 0.069 0.059 0.051 0.056 0.052 0.053
LLaMA2 7B 0.069 0.068 0.057 0.051 0.062 0.053
LLaMA2 13B 0.073 0.061 0.060 0.053 0.061 0.053
Vicuna 13B 0.086 0.074 0.071 0.064 0.072 0.062
Table 15: MSE for KEEN QA Probes between predicted
KEEN scores and QA accuracy scores.
greater interpretability of smaller KEEN VP probes
and the reduced correlation. However, the KEEN
VP-50 variant remains highly interpretable with a
minimal number of tokens and does not suffer a
substantial correlation decline. Consequently, we
chose to focus on evaluating the KEEN VP and VP-
50 variants.
C.3 Recovering QA accuracy of non-target
subjects through patching
The source model used in both patching exper-
iments was LLaMA2 7B, fine-tuned for 100
epochs on passages from the Wikipedia page
aboutAdil Shamoo . For both experiments, we
analyzed 85 non-target subjects with ≥5questions
whose QA accuracy decreased by at least 5% and
KEEN scores either increased or stayed constant
post fine-tuning. Representations from upper
intermediate layers (20-23) of the source model
were patched into the penultimate layer (30) of
the target model. In the PT layer experiment, the
target model was pre-trained LLaMA2 7B, while
in the FT subj. experiment, the source and target
models were both the fine-tuned LLaMA2 7B
model mentioned above. In computing patched
accuracy, a question was marked as correct if
patching from any source layer recovered the
correct answer. Formally, patched QA accuracy
per subject is computed according to y(s)
QA, patched:=
1
nPn
i=1 1h
∃ℓ∈ {20,21,22,23}: ˆapatched (ℓ)
i =aii
,
where ˆapatched (ℓ)
i denotes the answer generated
after patching from source layer ℓinto the target
model, as described above.
102550100
50688
Parameter Count0.470.660.75Pearson Corr.
Pythia 12B
102550100
32000
Parameter Count0.260.550.66Pearson Corr.
Vicuna 13BFigure 7: Correlation of KEEN OEG VP probe scores
and FActScore as a function of input parameter count.
102550100200
50257
Parameter Count0.260.540.61Pearson Corr.
GPT2 XL
102550100200
50432
Parameter Count0.320.640.68Pearson Corr.
Pythia 6B
102550100200
50688
Parameter Count0.440.540.64Pearson Corr.
Pythia 12B
102550100200
32000
Parameter Count0.410.530.64Pearson Corr.
LlaMA2 7B
102550100200
32000
Parameter Count0.290.480.58Pearson Corr.
LlaMA2 13B
102550100200
32000
Parameter Count0.420.500.60Pearson Corr.
Vicuna 13B
Figure 8: Correlation of KEEN QA VP probe scores and
QA accuracy as a function of input parameter count.
While §5.2 discusses patching results aggregated
across all 85 non-target entities, Table 16 presents
per-entity results from both patching experiments,
showing that QA accuracy can be effectively recov-
ered from intermediate fine-tuned representations.
C.4 Statistical significance of correlations
Table 17, Table 18, Table 19 represent the p-
values associated with the Pearson correlation val-
ues found in Table 2, Table 3, Table 4, respectively.
C.5 QA correlation plots
Figure 9 and Figure 10, show the results for the
QA experiments in §4.2 for Vicuna 13B and Pythia
12B, respectively.
C.6 OEG correlation plots
Figure 11 and Figure 12 show results for the OEG
experiments in §4.2.KEEN PTKEEN FT Acc. PT Acc. FT PT layer FT subj.
John, King of England 0.51 0.51 0.67 0.30 0.50 0.50
Maurice Le Boucher 0.32 0.14 0.29 0.14 0.43 0.29
The Deep 0.51 0.52 0.71 0.00 0.71 0.71
A Whole New World 0.45 0.50 0.40 0.20 0.40 0.80
Alexander the Great 0.56 0.59 0.60 0.36 0.55 0.55
Jaakko Laakso 0.42 0.43 0.63 0.13 0.50 0.38
Reds 0.48 0.49 0.86 0.29 0.71 0.71
WarGames 0.59 0.63 0.50 0.25 0.50 0.50
Charles V 0.33 0.34 0.54 0.23 0.77 0.62
Recovery 0.25 0.29 0.17 0.00 0.67 0.50
James I of Scotland 0.47 0.47 0.45 0.18 0.55 0.46
Mai Van Hoa 0.33 0.36 0.29 0.14 0.50 0.17
The Natural 0.48 0.50 0.63 0.13 0.50 0.50
The Bourne Legacy 0.67 0.69 0.67 0.22 0.67 0.56
Beck 0.48 0.51 0.44 0.22 0.78 0.56
Radek Opršal 0.46 0.48 0.57 0.14 0.43 0.29
Elena Romagnolo 0.42 0.46 0.50 0.17 0.33 0.33
Jaroslav Kocián 0.47 0.56 0.38 0.13 0.50 0.25
Table 16: Per-entity results from the patching experiments discussed in §5.2. The recovery of QA accuracy is
evident from both the observed increase in accuracy over Acc. FT and similarity to Acc. PT after patching (PT
layer, FT subj.). Also, the alignment of KEEN PT and FT scores with both Acc. PT and patched accuracy indicates
thatKEEN reflects the amount of information encoded in the representations rather than in the observed output of the
fine-tuned model.
Model Freq. FC ATTN VP-50 VP HS
GPT2 XL 2.54e−178.24e−477.70e−561.72e−599.80e−791.73e−75
Pythia 6B 3.26e−196.41e−752.74e−857.28e−901.33e−1063.16e−106
Pythia 12B 1.03e−154.46e−622.25e−762.53e−748.72e−916.46e−91
LLaMA2 7B 9.96e−154.54e−491.17e−703.07e−564.50e−892.52e−89
LLaMA2 13B 7.81e−136.77e−491.50e−501.98e−453.43e−701.74e−71
Vicuna 13B 5.15e−131.29e−481.68e−533.00e−502.07e−773.25e−76
Table 17: p-values for different models across KEEN QA probes.
Model Freq. FC ATTN VP-50 VP HS
Pythia 12B 1.00e−22.68e−67.20e−114.57e−93.04e−104.21e−11
Vicuna 13B 2.05e−21.28e−36.16e−62.50e−43.43e−64.02e−6
Table 18: p-values for different models across KEEN OEG probes
Model Freq. FC ATTN VP-50 VP HS
Pythia 12B 2.74e−291.00e−68.43e−71.76e−24.92e−53.25e−7
Vicuna 13B 2.28e−164.80e−41.39e−47.84e−59.18e−62.12e−5
Table 19: p-values for the correlation of KEEN QA probe and FActScore scores.0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.359x+0.317Hidden State (HS)
0.00 0.25 0.50 0.75 1.00
QA Accuracy103104105106107108Entity Pop.
log(y)=1.581x+4.816Baseline: Entity Pop.
0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.305x+0.301Baseline: Self-Attention (ATTN)
0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.313x+0.287Baseline: Fully Connected (FC)Figure 9: Vicuna 13B: Predicted scores from the KEEN QA probe versus the golden QA accuracy scores.
0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.436x+0.282Hidden State (HS)
0.00 0.25 0.50 0.75 1.00
QA Accuracy103104105106107108Entity Pop.
log(y)=1.209x+5.069Baseline: Entity Pop.
0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.401x+0.192Baseline: Self-Attention (ATTN)
0.00 0.25 0.50 0.75 1.00
QA Accuracy0.00.20.40.60.81.0Predicted Scorey=0.416x+0.177Baseline: Fully Connected (FC)
Figure 10: Pythia 12B: Predicted scores from the KEEN QA probe versus the golden QA accuracy scores.0.00 0.25 0.50 0.75
FActScore0.00.20.40.60.8Predicted Scorey=0.433x+0.240Hidden State (HS)
0.00 0.25 0.50 0.75 1.00
FActScore104105106107Entity Pop.log(y)=3.350x+4.066Baseline: Entity Pop.
0.00 0.25 0.50 0.75
FActScore0.00.20.40.60.8Predicted Scorey=0.582x+0.124Baseline: Self-Attention (ATTN)
0.0 0.5 1.0
FActScore0.00.20.40.60.81.0Predicted Scorey=0.514x+0.181Baseline: Fully Connected (FC)Figure 11: Vicuna 13B: Predicted scores from the KEEN OEG probe versus the golden FActScore scores.
0.0 0.2 0.4 0.6
FActScore0.00.20.40.6Predicted Scorey=0.511x+0.116Hidden State (HS)
0.00 0.25 0.50 0.75 1.00
FActScore104105106107Entity Pop.log(y)=4.387x+4.474Baseline: Entity Pop.
0.0 0.2 0.4 0.6
FActScore0.00.20.40.6Predicted Scorey=0.614x+0.101Baseline: Self-Attention (ATTN)
0.00 0.25 0.50 0.75
FActScore0.00.20.40.60.8Predicted Scorey=0.594x+0.101Baseline: Fully Connected (FC)
Figure 12: Pythia 12B: Predicted scores from the KEEN OEG probe versus the golden FActScore scores.