Gold Panning in Vocabulary: An Adaptive Method for Vocabulary
Expansion of Domain-Specific LLMs
Chengyuan Liu1,2∗, Shihang Wang2, Lizhi Qing2, Kun Kuang1†,
Yangyang Kang3,1,2,Changlong Sun2,Fei Wu1
{liucy1,kunkuang,yangyangkang,wufei}@zju.edu.cn,
{wangshihang.wsh,yekai.qlz}@alibaba-inc.com, changlong.scl@taobao.com
1College of Computer Science and Technology, Zhejiang University,
2Tongyi Lab, Alibaba Group,
3Polytechnic Institute, Zhejiang University
Abstract
While Large Language Models (LLMs) demon-
strate impressive generation abilities, they fre-
quently struggle when it comes to specialized
domains due to their limited domain-specific
knowledge. Studies on domain-specific LLMs
resort to expanding the vocabulary before fine-
tuning on domain-specific corpus, aiming to
decrease the sequence length and enhance effi-
ciency during decoding, without thoroughly
investigating the results of vocabulary expan-
sion to LLMs over different domains . Our
pilot study reveals that expansion with only a
subset of the entire vocabulary may lead to su-
perior performance. Guided by the discovery,
this paper explores how to identify a vocab-
ulary subset to achieve the optimal results.
We introduce VEGAD, an adaptive method that
automatically identifies valuable words from a
given domain vocabulary. Our method has been
validated through experiments on three Chinese
datasets, demonstrating its effectiveness. Ad-
ditionally, we have undertaken comprehensive
analyses of the method. The selection of a opti-
mal subset for expansion has shown to enhance
performance on both domain-specific tasks and
general tasks, showcasing the potential of VE-
GAD.
1 Introduction
Despite achieving satisfactory performance on a
wide range of tasks (OpenAI et al., 2024; Touvron
et al., 2023a; Xu et al., 2023; Yuan and Zhu, 2023),
Large Language Models (LLMs) continue to en-
counter challenges, particularly in domain-specific
tasks, such as the generation of legal, medical,
and financial texts. The expansion of vocabulary
(Provilkov et al., 2020; Liu et al., 2021; Ozdemir
and Goksel, 2019; Rothe et al., 2020) serves as
a strategy to enhance the decoding efficiency for
domain-specific LLMs. By concatenating specific,
*This work was done when Chengyuan Liu interned at
Alibaba.
†Corresponding author.
-8.00-3.002.007.0012.0017.00
028522427196Improved (%)
Vo c a b u l a r y  S i z eDomainALPACAGSM8KAVGFigure 1: Pilot study: Relative improvement comparing
with direct supervised fine-tuning, by adding vocabulary
with different sizes.
frequent n-grams into new words, the token se-
quence is shortened, thereby visibly boosting effi-
ciency. Cui et al. (2024) extended LLaMA’s exist-
ing vocabulary with an additional 20,000 Chinese
tokens, thereby improving its encoding efficiency
and semantic understanding of Chinese. LawGPT1
is fine-tuned based on the general Chinese LLMs
(such as Chinese-LLaMa, ChatGLM (Du et al.,
2022), etc.), the legal domain specific vocabulary
is expanded to enhance the semantic understanding
ability of the LLMs.
Current researches primarily focus on some spe-
cific domains. Nonetheless, they have not thor-
oughly elucidate the performance enhancements
resulting from vocabulary expansion in various do-
mains. We conduct a pilot study illustrating the
domain performance and general capabilities after
vocabulary expansion with different sizes, and the
results are illustrated in Figure 1. It is revealed
thataugmenting the size of the newly added vo-
cabulary does not invariably result in improved
1https://github.com/pengxiao-song/LaWGPTarXiv:2410.01188v1  [cs.CL]  2 Oct 2024①Text Segmentation④Filter by Gradients⑤Resize Embedding and LM Head layer⑥Domain SFTGeneral LLMDomain-Specific LLM
③Calculating Gradients
Domain-Specific DataCandidate VocabularyTokens to Add降压血糖染上……
Trie降压血糖……②Build TrieFigure 2: Framework of VEGAD.
model performance . Hence, an essential ques-
tion arises regarding the generation of an optimal
subset for vocabulary expansion given a candi-
date vocabulary. The process of selecting high-
value vocabulary during the expansion of domain-
specific LLMs is akin to gold panning , as it requires
careful selection rather than indiscriminate enlarge-
ment of the lexicon to enhance the performance of
the LLMs. We recognize the following challenges
for vocabulary subset generation:
•How to ensure an optimal performance over
the whole vocabulary?
•How to automatically adapt to any domain?
To effectively identify the crutial words from
a candidate vocabulary, we have proposed VE-
GAD (abbreviation of “ Vocabulary Expansion via
GrADients”), which is an adaptable vocabulary ex-
pansion method via gradients. Figure 2 provides
an illustration of the framework. Intuitively, token
groups displaying larger gradients in domain in-
stances are deemed more pivotal to the task and
should be integrated into the vocabulary as domain-
specific terms. Therefore, it is a straightforward
approach to trace the gradient of each word, while
there are several difficulties, such as the algorithm
to efficiently retrieve the candidate words from
the token sequences, and the gradient calculation
across various tokens rather than the whole se-
quence. To identify candidate words from the token
sequences, we build a Trie (Black, 2019) based on
the candidate vocabulary, and design an algorithm
to record the gradient for each word with the Trie.
To distinguish the effect of each token, the gradient
is calculated on the running tensors, instead of the
weights of the LLMs.
To scrutinize the efficacy of VEGAD, we have
undertaken comprehensive studies. The findings
across three Chinese datasets, pertaining to the
domains of law and medicine, underscore a su-
periority in comparison to other lexicon genera-tion techniques, as well as the promising prospects
of domain-specific vocabulary expansion. Our in-
quiry reveals that the domain-specific lexicon by
VEGAD enhances performance in tasks requiring
specialized knowledge as well as tasks demanding
general skills. We hope that our multi-perspective
analysis serves as a catalyst for future investiga-
tions into enhancing domain-task performance and
mitigating the Catastrophic Forgetting through do-
main vocabulary adaptation.
In summary, our contributions are three folds:
•It is revealed by our pilot study that vocabu-
lary expansion with only a subset of the entire
supplementary domain vocabulary may lead
to superior performance over using the whole
vocabulary.
•Guided by our discovery, we introduce VE-
GAD, an automatic method to effectively iden-
tify an optimal subset for vocabulary expan-
sion, adaptable to various domains.
•Extensive experiments and analyses have been
performed, during which VEGAD displays
outstanding proficiency surpassing other vo-
cabulary expansion methods.
2 Related Work
Large Language Models, such as ChatGPT2, GPT-
4 (OpenAI et al., 2024), exhibit amazing abilities
on understanding and text generation. They can
handle the tasks of QA, reasoning and math cal-
culation even under zero-shot scenarios. LLaMa
(Touvron et al., 2023a) is a collection of open foun-
dation language models ranging from 7B to 65B
parameters. Touvron et al. (2023b) developed and
released Llama 2, a collection of LLMs ranging in
scale from 7B to 70B parameters. The fine-tuned
Llama 2-Chat, are optimized for dialogue use cases.
There are other popular LLMs developed with vari-
ous skills (Rozière et al., 2024; Almazrouei et al.,
2023; Jiang et al., 2023; Bai et al., 2023; Baichuan,
2023).
Due to the lack of domain-specific knowledge,
general LLMs fall short at handling domain ques-
tions. Therefore domain-specific LLMs are devel-
oped by fine-tuning on domain corpus. (Xiong
et al., 2023) collected databases of medical di-
alogues with the help of ChatGPT and adopted
2https://chat.openai.com/Transformer Blocks口服降医院做胃检查…[CLS]压药镜…𝑉!𝑉!𝑉"𝑉#𝑉$𝑉%𝑉!𝑉&𝑉'𝑉!
口服降医院做胃检查…压药镜𝑉!𝑉"𝑉#𝑉$𝑉%𝑉!𝑉&𝑉'𝑉!
…𝑉![SEP]Root𝑉!降𝑉"胃𝑉#在𝑉$降压𝑉%降压药𝑉&胃镜𝑉'胃胀𝑉(to lower blood pressureantihypertensive drugsstomachstomach bloatinggastroscopyText SegmentationBuild Trie
在医院𝑉)
inhospital
在
在𝑉(
𝑉(降压药降压胃镜在医院Gradient++
pseudo-leaf nodeEmbedding layerLM Head layerFigure 3: Gradient Calculation for each candidate word. Given the Trie built from candidate vocabulary, we check
whether there exists a sub-sequence of the input and output on the path from the root of the Trie to a leaf node, by a
pointer. The trace of the pointer is illustrated by Viand the “pseudo-leaf node”. Finally, the top Kwords with the
largest gradients are selected to construct the new vocabulary, and used to resize the embedding layer and language
modeling head layer.
several techniques to train an easy-deploy LLM,
called DoctorGLM. Wang et al. (2023a) pro-
posed HuaTuo, a LLaMA-based model that has
been supervised-fine-tuned with generated QA
(Question-Answer) instances in biomedical domain
tasks, with medical expertise in the responses. Cui
et al. (2023) proposed an open-source legal LLM
named ChatLaw, with a method that combines vec-
tor database retrieval with keyword retrieval to ef-
fectively reduce the inaccuracy of relying solely
on vector database retrieval, and a self-attention
method to enhance the ability to overcome errors
present in reference data. There are other domains
studied including finance (Wang et al., 2023b; Yu,
2023), education (Yu et al., 2023a), science (Li
et al., 2023b) and e-commerce (Li et al., 2023a).
Several previous studies adopt a strategy, vocab-
ulary expansion, to improve the performance of
domain SFT. Specifically, a domain-specific vo-
cabulary is automatically generated or manually
designed, and added into the tokenizer. In or-
der to augment LLaMA with capabilities for un-
derstanding and generating Chinese text and its
ability to follow instructions, Cui et al. (2024) ex-
tended LLaMA’s existing vocabulary with an addi-
tional 20,000 Chinese tokens, thereby improving
its encoding efficiency and semantic understand-
ing of Chinese. Liu et al. (2023) proposed task-
adaptive tokenization as a way to adapt the genera-
tion pipeline to the specifics of a downstream task
and enhance long-form generation in mental health.
However, their task-adaptive tokenizer samples
variable segmentations from multiple outcomes,
which may change the vanilla behavious of othertokenizers (e.g., WordPiece and BPE). LaWGPT
expands the legal domain specific vocabulary and
large-scale Chinese legal corpus pre-training on the
basis of the general Chinese base model (such as
Chinese-LLama, ChatGLM, etc.), and enhances the
basic semantic understanding ability of the LLM
in the legal field. Tongyi-Finance-14B3expanded
the vocabulary of financial domain in Qwen-14B,
and the size of the vocabulary is 150,000. Based
on the BPE vocabulary used in GPT-4, the vocabu-
lary is optimized for Chinese and multi-language.
The numbers are divided into individual digits. Liu
et al. (2024b) identified tokens that are absent in
the general-purpose tokenizer and are rarely found
in general-purpose datasets, from the vocabulary
of the new tokenizer. They initialize model embed-
dings of the new tokens by utilizing the general-
purpose tokenizer. Liu et al. (2021) introduced two
new approaches based on attention to initialize the
weights of new added words.
3 Method
In this Section, we introduce VEGAD, a vocab-
ulary expansion method via gradient for domain-
specific LLMs. The process is shown in Figure
3.
Our approach is inspired by an naive intuition: n-
gram tokens exhibiting larger gradients in response
to domain-specific instances are deemed crucial
for the task at hand, and therefore, warrant inclu-
sion in the lexicon as domain-specific terminology.
Nonetheless, there are several challenges. For ex-
3https://modelscope.cn/models/TongyiFinance/Tongyi-
Finance-14BAlgorithm 1 Build Trie
Require: W1,W2,···,Wn, n, V 0
1:root←V0
2:M←1
3:fori= 1→Ndo
4:p←root
5: fortj
i∈ W ido
6: ifphas child tj
ithen
7: p←GetChild (p, tj
i)
8: else
9: VM←CreateChild (p, tj
i)
10: p←VM
11: M←M+ 1
12: end if
13: end for
14: setpas pseudo-leaf node
15:end for
ample, the algorithm to efficiently retrieve the can-
didate words from the token sequences, and the
gradient calculation across various tokens rather
than the whole sequence.
Specifically, starting from the domain-specific
data, sentences are divided into discrete words. The
candidate vocabulary is constructed with words ab-
sent from the general lexicon. Subsequently, the
process of selection is executed on domain-specific
instances by computing the gradients for each node
within the embedding tensor and the language mod-
eling tensor, with reference to a Trie constructed
based on the candidate vocabulary. The top K
words exhibiting the highest overall gradients are
retained to establish the specialized domain vocab-
ulary. Then we resize the LLM and incorporate
the tokenizer with new vocabulary, following an
optional weight initialization. Then we conduct
domain SFT on the LLM, to develop the domain-
specific LLM.
The advantage of VEGAD can be summarized
as following: 1)VEGAD is a plug-and-play task-
adaptive vocabulary selection method, seamlessly
integrating with diverse techniques utilized in su-
pervised fine-tuning. 2)In contrast to previous
methods such as Liu et al. (2023), which might
alter the intrinsic behaviors of current tokenizers
such as WordPiece and BPE by imposing an oblig-
atory scoring mechanism for sampling in accor-
dance with their guidelines, VEGAD is tokenizer-
agnostic, and compatible to any tokenization algo-
rithms. 3)The pipeline is automatically performed,without the need of manual design or intervention.
Of course, it still allows additional edition to the
vocabulary if required.
3.1 Build Trie
The Trie, as discussed by Black (2019), represents
a distinct tree-based data structure, extensively em-
ployed within the realm of computer science for
the administration of dynamic sets or associative
arrays, with the keys predominantly being strings.
Diverging from the structure of a binary search tree
in which a node’s placement is influenced by nu-
merical or logical hierarchy, in a Trie, the location
of a node is unequivocally defined by the sequence
of characters it denotes. We illustrate an example
of Trie in the left part of Figure 3.
Formally, the domain-specific dataset can be
represented as D={(X1, Y1),···,(Xn, Yn)},
where XandYare the query and response respec-
tively, nis the size of D. Given a text segmentation
tool, the candidate vocabulary is constructed fol-
lowing
V= (n[
i=1Segment (Xi))∪(n[
i=1Segment (Yi))
(1)
The candidate vocabulary is denoted as V=
{w1, w2,···, wN}, where Ndenotes the size of
the candidate vocabulary. Then we build the Trie
based on candidate vocabulary. For the i-th word
wi, we tokenize it to several tokens with the exist-
ing general tokenizer:
Wi=tokenize (wi) = [t1
i, t2
i,···, tli
i](2)
Note that li>1because each word in the candidate
vocabulary doesn’t exist in the general tokenizer’s
lexicon. Let V0be the root of the Trie. For each
word wi, we insert its tokens one by one into the
Trie, starting from V0. Additionally, we set a flag
of “pseudo-leaf node” to each tli
inode, which is
the last token of the word wi4. Note that each path
from the root to a “pseudo-leaf node” represents a
candidate word in V. The procedure is illustrated
in Algorithm 1. With the algorithm, we get a Trie
withMnodes.
3.2 Gradient Calculation
With the general tokenizer, the sentences are con-
verted to input query tokens and output response to-
4The “pseudo-leaf node” is different from the traditional
concept of “leaf node” in tree-based data structures. There
may be children nodes for “pseudo-leaf node”, because some
token sequence Wjmay start from another Wi.kens. For simplicity, the input and output sequence
of the LLM are denoted as x= [x1,···, xL]
andy= [y1,···, yL]respectively, where Lis
the length of the sequences. Current LLMs firstly
embed the input tokens to αin a high-dimension
space, then perform transformers on the embedding
vectors α. The representation houtput by several
transformer blocks is finally converted to the distri-
bution ˆyover tokens through a language modeling
head layer:
α=Embed (x) (3)
h= Transformers( α) (4)
ˆy=h×LMHeadT(5)
where Embed ,LMHead ∈RC×d,Candd
denote the size of vanilla vocabulary and the di-
mension. The standard language modeling loss is
adopted:
Llm=−LX
i=1logp(yi|x<i)
=CrossEntropy (y,Softmax(ˆ y))(6)
For the embedding tensor, we calculate the gra-
dients of each input token as Gembed. Although
previous studies mostly only focus on the embed-
ding layer, we find that the language modeling head
layer is also important especially for text genera-
tion tasks. Therefore, we calculate the gradients
Glmheadfor each output token only if it is not a
special token (e.g., [CLS] ,[SEP] and[PAD] ). To
obtain the gradient at each time step, Equation 5 is
modified as:
ˆy=β⊗(h×LMHeadT) (7)
where β∈RL×Cis filled with 1, and ⊗denotes
element-wise production.
Gembed=∂Llm
∂α, Glmhead=∂Llm
∂β(8)
Then we calculate the gradient for each candidate
word by looking up nodes in the Trie and iterat-
ing over xandy. The candidate words appear-
ing in the sequence can be identified by moving
a pointer from the root V0initially. During enu-
merating ifrom 1 to L, we check if there exists a
sub-sequence xi:jin Trie. Specifically, from the
root, the pointer constantly moves to its children
until it reaches the last “pseudo-leaf node” or the
token mismatches any child of the current node.Once the pointer reaches a node V′attributed with
“pseudo-leaf node”, we add the norm of the gradi-
ents of the sub-sequence to w, where wdenotes the
candidate word represented by V′.
Gw=Gw+||jX
q=iGembed
q||2
+||j−1X
q=i−1Glmhead
q||1(9)
Note that there is a position shift for the output
sequence (i.e. xi:j=yi−1:j−1). We provide the
detailed code in Algorithm 2.
To enhance efficiency, the algorithm’s cost of
time can be optimized by adopting prefix accu-
mulation in conjunction with the Aho–Corasick
Algorithm. This optimization is particularly sig-
nificant in cases involving Tries of considerable
size and depth, resulting in a notable reduction in
the algorithm’s overall complexity. The detailed
optimization is described in Appendix L.
3.3 Vocabulary Selection
Upon evaluating the gradient associated with each
word from the candidate vocabulary, the words are
organized in descending order based on the magni-
tude of their gradients. We obtain the top Kwords
and remove other words. These selected words are
then integrated into the pre-existing general vocab-
ulary. The embedding layer and language modeling
head layer are also resized to R(C+K)×d.
For initialization, the default method is averag-
ing the weights of sub-tokens in the original layer,
following Liu et al. (2023). We also investigated
other approaches and the results are discussed in
Appendix G.
4 Experiments
The main results on three datasets from two do-
mains are discussed in SubSection 4.2. Then we
discuss the influence of the vocabulary size in Sub-
Section 4.3. To verify our hypothesis, we compare
the words with different gradients in Appendix C.
We also remove the pre-built candidate vocabu-
lary, to investigate the influence of direct gradi-
ent calculation on 2-gram tokens of the sequence
in Appendix D. There are also discussions about
the influence of the language modeling head layer,
model scale and weight initialization methods in
Appendix E, F and G, respectively.MethodArticle QA ALPACA GSM8KSafety
Prompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 10.28 29.50 10.00 20.93 11.57 23.55 22.10 21.33 33.63 94.00
SFT 26.70 46.53 24.53 36.60 12.19 25.15 14.40 19.17 31.55 88.30
DV 26.23 47.10 24.83 36.71 12.11 25.11 14.50 19.86 32.14 88.70
SPM 25.56 45.77 24.83 36.02 12.56 24.89 8.10 17.85 30.33 88.70
+ATT_EG 24.31 45.06 22.82 34.89 12.07 24.72 8.30 17.99 30.56 89.40
+PATT_EG 25.96 45.98 24.01 36.22 11.99 24.63 8.50 17.95 30.57 89.50
Jieba 28.04 48.36 26.88 38.25 11.97 24.64 6.60 18.15 30.63 88.30
VEGAD 28.58 48.67 26.96 39.11 12.39 25.43 15.20 19.85 32.14 89.60
Table 1: Results on Article QA of legal domain.
MethodArticle
QAGSM8KSafety
PromptsA VG
BLEU ACC BLEU ACC -
SFT +159.73 -34.84 -10.13 -6.06 +22.81
DV +155.16 -34.39 -6.89 -5.64 +22.58
SPM +148.64 -63.35 -16.32 -5.64 +14.38
+ATT_EG +136.48 -62.44 -15.66 -4.89 +11.56
+PATT_EG +152.53 -61.54 -15.85 -4.79 +14.80
Jieba +172.76 -70.14 -14.91 -6.06 +17.02
VEGAD +178.02 -31.22 -6.94 -4.68 +28.45
Table 2: Relative improvement after SFT on Article QA,
comparing to general LLM. The metrics are reported in
percentage.
Our study incorporates three domain-specific
datasets from two distinct domains: Article QA
dataset for the legal domain, and CMedQA (Zhang
et al., 2018) and CMDD (Toyhom, 2023) datasets
for the medical field. Furthermore, we delve
into the Catastrophic Forgetting issue in gen-
eral tasks following supervised fine-tuning on
domain-specific instances. To this end, we ana-
lyze three datasets: ALPACA (Peng et al., 2023)
for tasks requiring instruction following, GSM8K
(Yu et al., 2023b) focused on mathematics, and
SafetyPrompts (Sun et al., 2023) concerning safety.
The metrics and details of the dataset consideration
and construction are described in Appendix A.
4.1 Baselines
General LLM The LLM fine-tuned on general
tasks. It is mainly considered as the reference when
studying CF problem.
SFT Direct supervised fine-tuning on domain-
specific dataset.
DV We adopt domain concepts and terminology
as the vocabulary to be added. For legal domain,
the expert-designed legal vocabulary by LawGPT5
5https://github.com/pengxiao-
song/LaWGPT/blob/main/resources/legal_vocab.txtis used. For medical domain, we prompt GPT-4
to extract the names of medicine, symptom and
therapies from the sentences. We keep words that
appear more than 100 times in the data to improve
the effectiveness, because increasing the size of the
newly added vocabulary does not invariably result
in improved model performance, according to our
experiment in SubSection 4.3.
SPM We train a tokenizer with SentencePiece
(Kudo and Richardson, 2018), which is a com-
mon method to generate domain-specific vocab-
ulary (Cui et al., 2024). We utilize the off-the-shelf
package6.
ATT_EG and PATT_EG Liu et al. (2021) in-
troduced two weight initialization methods based
on attention mechanism, ATT_EG and PATT_EG.
They apply the methods on the generated vocabu-
lary by SPM for downstream tasks.
Jieba Inspired by SPM, we adopt another text
segmentation tool, Jieba7. From the experiments,
we find it to be a strong and convenient baseline
for text generation tasks.
Implementation details are shown in Appendix
B.
4.2 Main Results
4.2.1 Legal Domain
The outcomes for Article QA are presented in Ta-
ble 1, and the relative improvements are shown in
Table 2. 1)Within the array of baseline compar-
isons, Jieba demonstrates superior performance in
domain-specific tasks. Specifically, Jieba achieves
a BLEU score that is 1.3 points greater than that
of the direct SFT approach, and a ROUGE-L score
6https://github.com/google/sentencepiece
7https://github.com/fxsjy/jiebaMethodCMedQA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 3.15 17.46 2.27 14.40 11.57 23.55 22.10 21.33 33.63 94.00
SFT 3.29 19.85 3.94 14.30 9.19 21.42 16.20 11.40 28.95 87.80
DV 3.61 19.24 3.88 14.32 9.61 22.01 17.60 11.67 29.56 88.50
SPM 3.29 18.91 3.61 13.88 9.15 21.34 8.60 12.13 28.29 85.20
+ATT_EG 3.20 18.48 3.26 13.78 9.21 21.27 7.70 12.06 28.39 86.20
+PATT_EG 2.81 18.67 3.20 12.49 9.69 22.01 8.10 12.43 28.55 85.80
Jieba 3.73 20.49 4.22 15.03 10.04 22.36 9.40 12.53 29.20 88.70
VEGAD 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
Table 3: Results on CMedQA of medical domain.
MethodCMDD ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 5.24 21.56 3.63 17.04 11.57 23.55 22.10 21.33 33.63 94.00
SFT 5.28 22.28 5.33 16.79 10.46 22.37 18.10 19.88 33.91 89.10
DV 5.50 22.57 5.49 16.97 10.28 22.35 18.30 18.52 32.77 90.50
SPM 5.09 21.70 4.96 15.80 10.59 22.75 7.90 17.49 31.64 88.20
+ATT_EG 5.23 21.69 4.70 16.55 10.48 22.53 8.60 18.15 32.15 89.10
+PATT_EG 5.24 21.65 4.75 16.52 10.76 23.01 8.70 17.98 32.18 88.60
Jieba 5.33 23.08 5.57 16.84 11.11 23.41 8.00 17.63 31.69 91.60
VEGAD 5.84 23.48 5.86 17.57 10.86 23.31 18.40 20.66 34.35 91.60
Table 4: Results on CMDD of medical domain.
MethodCMDD GSM8KSafety
PromptsA VG
BLEU ACC BLEU ACC -
SFT +0.76 -18.10 -6.80 -5.21 -7.34
DV +4.96 -17.19 -13.17 -3.72 -7.28
SPM -2.86 -64.25 -18.00 -6.17 -22.82
+ATT_EG -0.19 -61.09 -14.91 -5.21 -20.35
+PATT_EG 0.00 -60.63 -15.71 -5.74 -20.52
Jieba +1.72 -63.80 -17.35 -2.55 -20.50
VEGAD +11.45 -16.74 -3.14 -2.55 -2.75
Table 5: Relative improvement after SFT on CMDD,
comparing to general LLM. The metrics are reported in
percentage.
that surpasses DV by 1.5 points. 2)VEGAD ex-
hibits the highest scores across all evaluated metrics
for the domain-specific task, with its ROUGE-L
score nearly one point higher than that of Jieba. In
summary, VEGAD consistently outperforms other
vocabulary generation methods, showcasing stable
improvement. 3)In the realm of instruction follow-
ing, the performance differential among the meth-
ods is modest. The highest BLEU score, attained
by SPM, is marginally greater, by approximately
0.6 points, than the lowest score. VEGAD achieves
the second-highest BLEU score. This relatively
narrow range of scores could be attributed to the
uniformity of training across all methods on the
same QA dataset, which inherently bears a resem-
blance to the instruction-following format. 4)On
the GSM8K dataset, which consists of questionsthat require mathematical calculations, we observe
a significant drop in accuracy, indicative of CF. The
general chat LLM initially achieves an accuracy
of 22.10%. Yet, following domain-specific SFT,
even the highest accuracy attained by the baseline
methods, 14.50% by DV , shows a relative decrease
of 34.39% from the pre-fine-tuning performance.
When VEGAD is incorporated, there is a slight
improvement in accuracy to 15.20%, which corre-
sponds to a relative decrease of 31.22%. When
using the whole Jieba vocabulary, the accuracy
is less than half of VEGAD, with a relative de-
crease of more than 70% comparing to General
LLM. It proves the weakness of Jieba and the ef-
fectiveness of VEGAD. 5) The general chat LLM
achieves a high accuracy of 94% on the safety task.
Nonetheless, direct domain-specific SFT induces a
notable reduction in accuracy to 88.30%. The data
indicates that all vocabulary expansion methods,
including VEGAD, result in either a reduction or
equality in the extent of forgetting when compared
to the direct SFT. Among these methods, VEGAD
registers the highest accuracy, reaching 89.60%,
which represents a relative decrease of 4.68% from
the original accuracy achieved by the general chat
LLM.4.2.2 Medical Domain
The results of the medical domain are shown in
Table 3 and 4. We also report the relative improve-
ments after SFT on CMDD in Table 5. 1)Upon
comparing the results with those from the legal do-
main, it is evident that the medical scores are com-
paratively low and that the enhancement yielded
by domain-specific SFT is modest. Despite the
limited scope of improvement, VEGAD distin-
guishes itself by delivering the best results across
all metrics for both datasets in the medical do-
main. The medical domain responses encompass a
breadth of viewpoints, including potential causes,
treatment drugs, and precautionary measures. This
diversity amplifies the complexity and presents a
greater challenge for language modeling tasks. 2)
In the context of solving math problems, DV stands
out by achieving higher accuracy rates than other
baselines after being fine-tuned on both CMedQA
and CMDD datasets. Conversely, Jieba performs
poorly under both settings, representing a substan-
tial relative decrease of 63.8%, after fine-tuning
on CMDD. VEGAD marks the pinnacle of per-
formance by reaching an accuracy of 18.40% after
fine-tuning on the CMDD dataset, which signifies a
relative 16.74% decrease in calculation ability com-
pared to before fine-tuning—a notable improve-
ment over Jieba. 3)On the safety choice problems,
Jieba ties or outperforms VEGAD.
In summary, we find that VEGAD not only im-
proves the performance on domain tasks, but also
helps to mitigate the problem of forgetting.
4.3 Vocabulary Size
The size of added domain-adaptive vocabulary is
important in vocabulary expansion. We conduct a
study on the vocabulary generated by Jieba. We
count the times that each word appear in the train-
ing corpus, and filter words that appear more than
0, 10, 100, and 1000 times. By adding the corre-
sponding words into the vocabulary, we plot result
fine-tuning on CMedQA in Figure 4.
At the beginning, it brings benefits by increasing
the vocabulary size. While the best performance
presents close to 2500 and 3000. However, when
adding all 4658 words (i.e. “Jieba” baseline), the
decrease on math reaches about 50%, and the
average result decreases more than 10%.
It is reasonable that, a number of appropriately
selected words can improve domain performance
because it introduces new trainable parameters for
-10.00-5.000.005.0010.0015.0020.0025.0030.00
0500100015002000250030004658Improved (%)
Vo c a b u l a r y  S i z eDomainALPACAGSM8KAVGJiebaFigure 4: Relative improvement of VEGAD comparing
with direct SFT, by adding vocabulary with different
sizes.
domain-specific terminology and concepts. Addi-
tionally, the representation shift caused by SFT is
shared by the addition of new words, thus the repre-
sentation of original tokens are kept, mitigating the
problem of CF. However, when the vocabulary size
constantly increases, the vanilla tokenization could
be broken. More and more unseen tokens appear
within one instance at the same time. Without ap-
propriate initialization, the previously pre-trained
knowledge can not be inherited, and the represen-
tation on general corpus also shifts.
5 Conclusion
The influence of adding domain-specific words and
the generation of domain vocabulary are far from
being explored for LLMs. In this paper, we investi-
gate the influence of adding domain vocabulary to
LLMs from the perspective of both domain exper-
tise and forgetting of general capabilities. We find
that expansion with only a subset of the entire vo-
cabulary may lead to superior performance. Based
on which, an automatic approach to identify effec-
tive words from a candidate vocabulary, called VE-
GAD, is proposed for the generation of an optimal
subset. Extensive experiments on three datasets
from two domains, are conducted to prove the ef-
fectiveness of VEGAD. It is concluded from the
analyses that not only the performance on domain-
specific tasks is improved, but also the problem of
catastrophic forgetting is mitigated.Limitations
Our work investigates the influence of vocabulary
generation for domain-specific LLMs, and intro-
duces an automatic method based on gradients for
both domain tasks and general abilities. However,
the methods to properly initialize the weights of
new words are still far from explored. From our
experiments, initialization by either simple calcu-
lation based on the training corpus, or limited ex-
ternal knowledge cannot bring stable improvement
on the tasks. Thus it highlights the necessity of an
effective approach to calculate the weights within
the embedding layer and language modeling head
layer, especially under low-resources scenarios.
Acknowledgements
This work was supported in part by National
Natural Science Foundation of China (62441605,
62376243, 62037001, U20A20387), National Key
Research and Development Program of China
(2022YFC3340900), the StarryNight Science Fund
of Zhejiang University Shanghai Institute for
Advanced Study (SN-ZJU-SIAS-0010), Alibaba
Group through Alibaba Research Intern Program,
Project by Shanghai AI Laboratory (P22KS00111),
Program of Zhejiang Province Science and Tech-
nology (2022C01044).
References
Alfred V . Aho and Margaret J. Corasick. 1975. Effi-
cient string matching: an aid to bibliographic search.
Commun. ACM , 18(6):333–340.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, Daniele Mazzotta,
Badreddine Noune, Baptiste Pannier, and Guilherme
Penedo. 2023. The falcon series of open language
models.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609 .Baichuan. 2023. Baichuan 2: Open large-scale lan-
guage models. arXiv preprint arXiv:2309.10305 .
Paul E. Black. 2019. trie. Dictionary of Algorithms and
Data Structures [online].
Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia
Passaro, Vincenzo Lomonaco, and Davide Bacciu.
2022. Continual pre-training mitigates forgetting in
language and vision.
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and
Li Yuan. 2023. Chatlaw: Open-source legal large
language model with integrated external knowledge
bases.
Yiming Cui, Ziqing Yang, and Xin Yao. 2024. Efficient
and effective text encoding for chinese llama and
alpaca.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b.
Prakhar Kaushik, Alex Gain, Adam Kortylewski, and
Alan Yuille. 2021. Understanding catastrophic for-
getting and remembering in continual learning with
optimal relevance mapping.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang,
Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie,
Fei Huang, and Yong Jiang. 2023a. Ecomgpt:
Instruction-tuning large language models with chain-
of-task tasks for e-commerce. arXiv preprint
arXiv:2308.06966 .
YuYang Li, CunShi Wang, MengWei Qu, Yu Bai,
Roberto Soria, and JiFeng Liu. 2023b. Starglm.
https://github.com/Yu-Yang-Li/StarGLM .Chengyuan Liu, Shihang Wang, Yangyang Kang, Lizhi
Qing, Fubang Zhao, Changlong Sun, Kun Kuang, and
Fei Wu. 2024a. More than catastrophic forgetting:
Integrating general capabilities for domain-specific
llms.
Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris
Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah
Alben, Himyanshu Anand, Sanmitra Banerjee, Is-
met Bayraktaroglu, Bonita Bhaskaran, Bryan Catan-
zaro, Arjun Chaudhuri, Sharon Clay, Bill Dally,
Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi,
Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain,
Ankit Jindal, Brucek Khailany, George Kokai, Kishor
Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart
Oberman, Sujeet Omar, Ghasem Pasandi, Sreedhar
Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang
Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Walker
Turner, Kaizhe Xu, and Haoxing Ren. 2024b. Chip-
nemo: Domain-adapted llms for chip design.
Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia,
Minlie Huang, and Rada Mihalcea. 2023. Task-
adaptive tokenization: Enhancing long-form text gen-
eration efficacy in mental health and beyond. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 15264–
15281, Singapore. Association for Computational
Linguistics.
Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang,
Weihua Luo, Min Zhang, Haiying Zhang, and Jin-
song Su. 2021. Bridging subword gaps in pretrain-
finetune paradigm for natural language generation.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6001–6011, Online. Association for Computational
Linguistics.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2024. Gpt-4 technical report.F. Ozdemir and O. Goksel. 2019. Extending pretrained
segmentation networks with additional anatomical
structures. International Journal of Computer As-
sisted Radiology and Surgery , 14:1187–1195.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2020. BPE-dropout: Simple and effective subword
regularization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 1882–1892, Online. Association for
Computational Linguistics.
Sascha Rothe, Shashi Narayan, and Aliaksei Severyn.
2020. Leveraging pre-trained checkpoints for se-
quence generation tasks. Transactions of the Associ-
ation for Computational Linguistics , 8:264–280.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy
Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna
Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron
Grattafiori, Wenhan Xiong, Alexandre Défossez,
Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-
tin, Nicolas Usunier, Thomas Scialom, and Gabriel
Synnaeve. 2024. Code llama: Open foundation mod-
els for code.
Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng,
and Minlie Huang. 2023. Safety assessment of
chinese large language models. arXiv preprint
arXiv:2304.10436 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Toyhom. 2023. Chinese medical dialogue
data. https://github.com/Toyhom/
Chinese-medical-dialogue-data .
Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,
Sendong Zhao, Bing Qin, and Ting Liu. 2023a. Hu-
atuo: Tuning llama model with chinese medical
knowledge.
Neng Wang, Hongyang Yang, and Christina Dan
Wang. 2023b. Fingpt: Instruction tuning benchmark
for open-source large language models in financial
datasets. NeurIPS Workshop on Instruction Tuning
and Instruction Following .
Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao,
Yuxiao Liu, Qian Wang, and Dinggang Shen. 2023.
Doctorglm: Fine-tuning your chinese doctor is not a
herculean task. arXiv preprint arXiv:2304.01097 .
Jimin Xu, Nuanxin Hong, Zhening Xu, Zhou Zhao,
Chao Wu, Kun Kuang, Jiaping Wang, Mingjie Zhu,
Jingren Zhou, Kui Ren, Xiaohu Yang, Cewu Lu, Jian
Pei, and Harry Shum. 2023. Data-driven learning
for data rights, data pricing, and privacy computing.
Engineering , 25:66–76.
Jingsi Yu, Junhui Zhu, Yujie Wang, Yang Liu, Hongx-
iang Chang, Jinran Nie, Cunliang Kong, Ruining
Chong, XinLiu, Jiyuan An, Luming Lu, Mingwei
Fang, and Lin Zhu. 2023a. Taoli llama. https:
//github.com/blcuicall/taoli .
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. 2023b.
Metamath: Bootstrap your own mathematical ques-
tions for large language models. arXiv preprint
arXiv:2309.12284 .
YangMu Yu. 2023. Cornucopia-llama-fin-chinese.
https://github.com/jerry1993-tech/
Cornucopia-LLaMA-Fin-Chinese .
Luyao Yuan and Song-Chun Zhu. 2023. Communica-
tive learning: A unified learning formalism. Engi-
neering , 25:77–100.
S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu. 2018.
Multi-scale attentive interaction networks for chinese
medical question answer selection. IEEE Access ,
6:74061–74071.
A Datasets and Metrics
We adopt three datasets from two domains, Article
QA for legal domain and CMedQA (Zhang et al.,
2018), CMDD (Toyhom, 2023) for medical domain.
Article QA is collected from a publicly available
legal consulting website, which includes pairs of
real-world queries and answers. For CMedQA, weDataset GradientDomain ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
Article QAMax 28.58 48.67 26.96 39.11 12.39 25.43 15.20 19.85 32.14 89.60
Min 26.03 46.08 24.05 36.22 12.41 25.27 15.30 19.65 32.06 89.20
CMedQAMax 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
Min 3.16 19.44 3.82 13.88 9.90 22.30 15.40 13.14 30.38 88.40
Table 6: Results by adding words with different gradients.
Domain Dataset # Train # Validation # Test
Law Article QA 19937 200 200
MedicineCMedQA 20000 500 500
CMDD 15774 1000 1000
Instruction ALPACA 0 0 1000
Math GSM8K 0 0 1000
Safety SafetyPrompts 0 0 1000
Table 7: Datasets used in the experiments.
drop the column “neg_ans_id”, and remove dupli-
cated lines. CMDD is a Chinese medical dialogue
dataset, covering Andrology, Internal Medicine,
Obstetrics and Gynecology, Oncology, Pediatrics
and Surgery. We select the instances involving
Internal Medicine8.
Additionally, we also investigate the forgetting
problem on general tasks after supervised fine-
tuning on domain instances. The phenomenon is
known as Catastrophic Forgetting (CF), and studied
by several researchers (Kaushik et al., 2021; Cossu
et al., 2022; Liu et al., 2024a). Therefore, it is nat-
ural to wonder that whether vocabulary expansion
helps mitigate CF. By consulting domain experts
about the general abilities required for the deploy-
ment of domain-specific LLMs, we consider three
abilities: instruction following, math and safety.
ALPACA (Peng et al., 2023) is the self-instruct
dataset based on GPT-4, and we use the Chinese
version9. GSM8K (Yu et al., 2023b) is a dataset
for mathematical reasoning. The publicly released
version is adopted, where question-answer pairs
are translated in Chinese from GSM8K by GPT-
3.5-Turbo with few-shot prompting10. For safety,
we use SafetyPrompts (Sun et al., 2023). For easier
evaluation, we obtain a safe response with GPT-4
for each prompt of type “Ethics_And_Morality”,
8The data source is publicly available at
https://github.com/Toyhom/Chinese-medical-dialogue-
data/tree/master/Data_ 数据/IM_内科.
9https://huggingface.co/datasets/shibing624/alpaca-zh
10The dataset is available at
https://huggingface.co/datasets/meta-math/GSM8K_zh
.then construct 2 choices for each question (one safe
choice and another unsafe choice). The LLM is
prompted to identify the safe response.
We report the average score of BLEU-1/2/3/4
(denoted as “BLEU”), and ROUGE-L score for the
text generation tasks. We also report the accuracy
of the calculated numeric result for GSM8K, and
accuracy for SafetyPrompts. While calculating the
accuracy of numerical results, we mainly follow
previous work11, which extracts the results accord-
ing to regex and complex patterns. The best results
are highlighted with bold , and the second best re-
sults are underlined . The statistics of the datasets
are listed in Table 7.
B Implementation Details
For VEGAD, we use Jieba as the text segmentation
tool. We train all models on the domain-specific
task for 3 epochs. The train batch size is set to
8, learning rate to 5×10−5, and we use the co-
sine scheduler. The LLM is based on Qwen1.5
(Bai et al., 2023) with 1.8B parameters. We down-
load the parameters from HuggingFace12, and fine-
tuned the model with LoRA (Hu et al., 2021) on
1 A100 80G GPU. The rank is set to 16. Only the
parameters of the embedding layer, language mod-
eling head layer of newly added vocabulary and the
adapters are trainable, while the others are frozen.
C Words of Different Gradients
To clearly present the influence of selection on
gradient, we comparing the results by adding words
with the top Kgradients and bottom Kgradients
(non-zero) respectively. The results are shown in
Table 6. It is obvious that on both Article QA and
CMedQA, adding words with the largest gradients
leads to better overall results than using words with
lowest gradients. For Article QA, the BLEU score
is 2.5 higher, and ROUGE-L is about 3 point higher,
than using words with lowest gradients. There is
11https://github.com/QwenLM/Qwen
12https://huggingface.co/Qwen/Qwen1.5-1.8B-ChatGradient Words
Max痔 疮 |Hemorrhoids; 腰椎|Lumbar
spine;甲亢|Hyperthyroidism; 直
肠|Rectum; 椎间盘|Intervertebral disc;
胎动|Fetal movement; 排畸|Anomaly
screening;排卵|Ovulation; 腰椎间
盘|Lumbar intervertebral disc; 肾
阳虚|Kidney Yang deficiency; 针
灸|Acupuncture; 对症|Symptomatic
treatment; 椎间|Intervertebral;包
皮|Foreskin;彩超|Color Doppler ultra-
sound;颈椎病|Cervical spondylosis;
腰酸|Lumbago; 痔疮膏|Hemorrhoid
cream
Min院去;下用;等情;下才;本是;来后;
法等;会导;织炎;以减;弹簧床;入血;
用非;当用;取物;法可;时上;以解;
常做|Usually; 染上|Contract a disease
Table 8: Words with different gradients.
39.11 25.43 15.20 89.60 39.36 25.07 12.10 90.50 
0.0020.0040.0060.0080.00100.00
Article QAALPACAGSM8KSafetyPromptsVEGADVEGAD+2-gram
Figure 5: Results comparison with 2-gram.
also a significant advantage on CMedQA. For math
calculation, adding words with largest gradients
achieves the accuracy 1% higher than adding low-
gradient words by fine-tuning on CMedQA, but
0.1% lower by fine-tuning on Article QA.
We list several words with different gradients in
Table 8 to compare the differences. The explain-
able words are translated into English, denoted as
“<Chinese>|<English>”. The words with larger gra-
dients are more explainable and specialize. This
attribute can also lead to reasonable tokenization
and mitigate the forgetting.
D Direct Gradient
After proving the effectiveness of selection from a
candidate vocabulary, it is natural to consider using
the 2-gram tokens directly according to the gradi-
ents, besides the pre-built lexicon V. Specifically,
we calculate gradients for each 2-gram in the same
way as VEGAD, and sort the 2-grams together with
the words from Vin descending order. Only the
6.86 1.11 5.56 1.87 1.47 1.01 0.00 5.56 0.19 1.93 0.001.002.003.004.005.006.007.008.00
Domain-ROUGEALPACA-ROUGEGSM8K-ACCGSM8K-ROUGESafetyPrompts-ACCImproved (%)w LMHeadw/o LMHeadFigure 6: Ablation study on the gradient of LMHead
Layer.
topKwords are kept finally. We compare the
ROUGE-L of Article QA, ALPACA, and accuracy
of GSM8K, SafetyPrompts, as shown in Figure 5.
On the domain-task, “VEGAD+2-gram” outper-
forms VEGAD by 0.25, since it directly optimizes
the gradients on the training task. But there is a
forgetting problem on ALPACA and GSM8K. Es-
pecially, the accuracy of GSM8K suffers from a
relative decrease of 20.39%. The accuracy on Safe-
tyPrompts by “VEGAD+2-gram” is slightly higher
than VEGAD.
We also notice that there are many unexplain-
able 2-gram words generated by selecting 2-grams.
Therefore, VEGAD is more effective based on text
segmentation in summary.
E Influence of LMHead Layer
The language modeling head layer (LMHead
Layer) converts the transformer output from hidden
states to logits distribution over tokens. Previous
studies usually ignore the importance of LMHead
Layer. While in our work, we conduct an ablation
study on LMHead Layer by ignoring the gradient
of its output tensor (i.e. Glmhead). We plot the rela-
tive improvement comparing with direct SFT. The
result is illustrated in Figure 6. The x-axis denotes
the tasks and correspond metrics.
We notice a pattern from the figure that for
datasets that requiring text generation, “w/o LM-
Head” suffers from a significant decrease. While
the accuracy is not influenced or even better. The
relative improvement on the domain task drops
from 6.86% to 1.01% after ignoring LMHead
Layer. There are also decrease on ROUGE-L
scores of ALPACA and GSM8K. However, the ac-
curacy of “w/o LMHead” of GSM8K ties VEGAD,MethodArticle QA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 11.95 32.64 11.62 22.94 11.77 23.74 53.70 24.13 37.36 95.90
SFT 32.16 52.35 30.69 41.99 12.73 25.15 35.80 22.12 35.13 93.10
DV 31.93 51.82 30.35 41.31 12.62 24.97 37.70 22.60 35.17 93.40
SPM 31.78 51.53 30.04 41.46 12.09 24.41 24.10 20.86 33.36 93.00
+ATT_EG 32.38 52.68 31.39 42.53 12.07 24.68 27.20 21.43 33.91 92.70
+PATT_EG 32.39 52.57 30.86 41.91 12.23 24.76 27.80 21.34 33.84 92.90
Jieba 32.16 52.35 30.88 42.12 12.76 25.19 25.00 20.88 33.81 93.70
VEGAD 32.28 52.83 31.33 42.55 13.07 25.58 39.10 22.16 35.00 93.80
Table 9: Results of Qwen 7B fine-tuned on Article QA.
MethodCMedQA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 3.23 18.29 2.44 14.50 11.77 23.74 53.70 24.13 37.36 95.90
SFT 5.25 22.20 4.94 18.01 12.10 24.74 38.50 18.25 36.89 95.00
DV 4.89 22.07 4.66 17.85 12.28 24.96 38.30 18.32 26.81 94.70
SPM 4.07 19.93 3.62 15.46 11.70 23.91 19.30 16.37 33.47 94.30
+ATT_EG 4.00 19.83 2.66 15.69 11.43 23.91 17.60 16.41 32.82 94.90
+PATT_EG 4.00 20.68 3.86 15.83 11.34 23.70 18.90 16.09 32.32 95.00
Jieba 4.53 21.85 4.92 17.45 12.34 24.68 16.20 16.40 33.81 94.90
VEGAD 5.13 22.46 5.01 18.03 12.80 25.41 37.00 19.00 36.36 94.50
Table 10: Results of Qwen 7B fine-tuned on CMedQA.
and the accuracy on SafetyPrompts is slightly
higher than VEGAD.
It is reasonable that considering the gradient of
language modeling output benefits the metrics of
text generation such as BLEU and ROUGE, be-
cause it bridges the gap between hidden states
and logits. After removing the gradients of LM-
Head Layer, the vocabulary adaptation concen-
trates on the optimization of text understanding,
rather than generating helpful responses according
to the queries.
F Scale of LLM
We scale up the foundation model from 1.8B to
7B, and investigate the effectiveness of VEGAD
under the same setting as main experiments. The
results of the models fine-tuned on Article QA,
CMedQA and CMDD are shown in Table 9, 10 and
11 respectively.
(1) V ocabulary generated by Jieba is not as com-
petitive as in the experiments of Qwen 1.8B. The
results by Jieba are relatively low, especially on
math calculation. The accuracy on GSM8K by
Jieba is nearly the lowest among all methods. After
fine-tuning on CMDD, the accuracy decreases from
53.70% to 13.60% by adding the new words, which
is a relative decrease of 74.67%. (2) Direct SFT
and DV appear to be strong baselines. Best resultson four metrics are achieved by direct SFT, when
fine-tuning on CMedQA. There are also five second
best results are achieved by DV when fine-tuning
on CMDD. (3) VEGAD outperforms other base-
lines from several aspects. There is a stable advan-
tage on domain ROUGE-1 and ROUGE-L scores
by VEGAD over other methods. The math calcu-
lation by VEGAD reaches the best for some cases.
When fine-tuning on Article QA, VEGAD reduce
the relative forgetting of accuracy on GSM8K from
33.33% to 27.19%, comparing with direct SFT.
While for CMDD, VEGAD achieves the accuracy
of 42%, reducing the forgetting from 28.87% to
21.79%.
G Weight Initialization
We attempt to further improve the task performance
of VEGAD by adding weight initialization meth-
ods, including ATT_EG and PATT_EG. Here we
additionally introduce another approach which re-
trieves related concepts from external knowledge
base. For implementation, we use Wikipedia as the
knowledge source, and the method is denoted as
“+WIKI”. The results are shown in Table 12.
Medical concepts are usually different from the
meaning by understanding its sub-words separately.
Thus the improvement on medical tasks especially
requires an effective initialization method. Appar-MethodCMDD ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 5.70 22.34 3.99 17.61 11.77 23.74 53.70 24.13 37.36 95.90
SFT 8.07 25.03 6.60 20.38 12.04 24.41 38.20 21.61 36.74 93.30
DV 8.11 25.21 6.66 20.27 12.18 24.44 38.30 22.10 36.59 93.50
SPM 7.48 24.38 5.95 19.89 11.89 24.11 21.00 19.82 34.17 92.30
+ATT_EG 7.53 23.79 5.64 19.74 11.59 23.59 20.10 19.36 34.00 91.50
+PATT_EG 7.36 23.66 5.63 19.31 11.64 23.73 21.40 18.43 34.23 91.70
Jieba 7.69 24.91 6.21 20.46 12.12 24.27 13.60 18.19 32.59 92.80
VEGAD 7.98 25.26 6.43 20.93 12.40 24.62 42.00 23.13 37.79 93.10
Table 11: Results of Qwen 7B fine-tuned on CMDD.
Dataset MethodDomain ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
CMedQAVEGAD 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
+ATT_EG 3.63 20.33 4.04 14.50 9.56 22.12 17.20 13.34 30.61 88.40
+PATT_EG 3.84 20.48 4.28 15.23 9.84 22.47 16.70 13.47 30.56 88.60
+WIKI 3.74 20.61 4.19 14.96 9.79 22.30 17.30 12.98 30.37 88.20
CMDDVEGAD 5.84 23.48 5.86 17.57 10.86 23.31 18.40 20.66 34.35 91.60
+ATT_EG 5.83 23.53 5.77 17.83 11.15 23.40 21.20 21.02 34.91 92.10
+PATT_EG 5.73 23.38 5.70 17.72 10.97 22.97 17.80 20.22 34.21 92.00
+WIKI 5.74 23.29 5.71 17.23 10.88 23.05 19.30 21.11 34.71 92.10
Table 12: Results of adding weight initialization to VEGAD.
ently, the current methods cannot provide stable
benefits to the domain tasks, even introducing ad-
ditional training corpus. On half of the domain
metrics, VEGAD without initialization achieves
better results. There is no clear pattern on the gen-
eral abilities either. The experiments highlight the
limitations to the current initialization approaches
and urgent necessity to better algorithms.
H Cross Language and Base Model
Table 13 presents an experiment conducted on En-
glish medical domain dataset, PubMedQA, with
Llama3-8B model. Since Jieba is especially de-
veloped for Chinese, we apply VEGAD to SPM.
The ROUGE-L of text generation tasks and accu-
racy of math problems are reported. It can be seen
that VEGAD also improves the baseline on En-
glish datasets. Additionally, our proposed method
is adaptable to different text segmentation tools.
Model PubMedQA Alpaca GSM8K
SPM 26.78 16.69 12.13
VEGAD 27.38 18.88 13.12
Table 13: English results with Llama-8B.I Abbreviation
We provide some explanations of the content that
may cause confusion.
•SFT: Abbreviation of "supervised fine-
tuning".
•VEGAD: Abbreviation of “ Vocabulary
Expansion via GrADients”.
•token: The output of general tokenization.
Each node in the Trie represents a token.
•word: The output items of segmentation tools.
Each token sequence represented by the path
from the root node to a pseudo-leaf node on
the Trie is a word.
• sub-word: Each character of the word in Chi-
nese.
J Detailed Discussions to Pilot Study
The setting of pilot study is the same as SubSection
4.3. The results are shown in Figure 1.
The highest instruction following score appears
at 285 words, while the highest score for other
abilities appear at size 2242. When increasing the
size to the full vocabulary, we observe a significant
deceasing on all metrics. The score of ALPACA is
even lower than direct SFT. From the trending, it isconcluded that an increasing vocabulary size does
not necessarily brings improvement to the domain
performance or general abilities, although trainable
parameters are increasing.
K Gradient Calculation
Algorithm 2 Calculate Gradients for Each Candi-
date Word
Require: root, X, Y, LLM, M, N
1:fori= 1→Mdo
2:Gwi←0
3:end for
4:for(X, Y )∈Ddo
5:x, y←GetInputOutput (X, Y )
6:p←root
7:Llm←LLM (x, y)
8: Calculate Gembed, Glmheadby Equation 8
9: fori= 1→Ldo
10: j←i
11: while xjis not a special token andphas
child xjdo
12: p←GetChild (p, xj)
13: ifpis a pseudo-leaf node then
14: w←GetWordByNode (p)
15: Accumulate Gwby Equation 9
16: end if
17: j←j+ 1
18: end while
19: end for
20:end for
21:return G= [Gw1,···, GwN]
To clarify our process of gradient calculation,
we provide code details in Algorithm 2.
L Aho–Corasick Algorithm
rootabcabbcbcabcacaacbcaaaa
Figure 7: Aho–Corasick Algorithm. The fail pointers
are highlighted with blue.
Aho–Corasick Algorithm (Aho and Corasick,1975) is based on the structure of Trie, combined
with the idea of KMP, which is used to solve multi-
pattern matching and other tasks. Fail pointers are
used to get the node with the maximum length after
the current node. Aho–Corasick Algorithm and fail
pointers are illustrated in Figure 7.
Inspired by Aho–Corasick Algorithm, we further
optimize the gradient calculation to improve the ef-
ficiency. Firstly, we obtain the prefix accumulation
arrays:
Cumembed
i =iX
j=1Gembed
j
Cumlmhead
i =iX
j=1Glmhead
j(10)
The external enumerating changes from the start of
each word to the end. for the start of each word, it
is easy to explore with the fail pointer. Assuming
the word represented by node n1ends at the i-th
token, then the word represented by node fail(n1)
also ends at the i-th token. Let depth (nw)denote
the depth of node nwon the Trie, then Equation 9
can be modified as
Gw=Gw+||sum (Cumembed
i−depth (nw):i)||2
+||sum (Cumlmhead
i−depth (nw)−1:i−1)||1
(11)
We provide the details in Algorithm 3.
Since the Trie is static during gradient calcula-
tion, the parent nodes on fail path for each node
can be memorized. Then the complexity is re-
duced from O(L×depth )toO(L×depth fail),
where depth denotes the expected depth on Trie,
anddepth faildenotes the expected depth of the
fail path. Note that depth failis usually significant
smaller than depth .
M Prompts Details
We list some example prompts and responses in
Table 14.Dataset Prompt Response
Article QA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 回答下面的法律咨询问题：我在老城
区有一套房子，政府要进行旧城区改建，对我的房子进
行了征收，我选择房屋产权调换，这个过程中我应该得
到哪些补偿？<|im_end|> <|im_start|>assistant根据《国有土地上房屋征收与补偿条例》第二十一条的
规定，作为被征收人，我可以选择货币补偿，也可以选
择房屋产权调换。如果我选择房屋产权调换，市、县级
人民政府应当提供用于产权调换的房屋，并与我计算、
结清被征收房屋价值与用于产权调换房屋价值的差价。
同时，对于旧城区改建征收个人住宅的情况，如果我选
择在改建地段进行房屋产权调换，作出房屋征收决定的
市、县级人民政府应当提供改建地段或者就近地段的房
屋。<|im_end|>
CMedQA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 回答下面的医疗咨询问题：睾丸炎引
起的不孕不育王医生：我是六年前因腮腺炎引起睾丸炎
因为当时没有治疗好。现在睾丸还会痛，去年做过睾丸
穿刺检查睾丸不产生精子。请问可以采用什么方式进行
治疗？慢慢的能够恢复正常吗？希望你能给予答复，谢
谢。<|im_end|> <|im_start|>assistant睾丸炎，这个情况吃了左氧和诺氟沙星，炎可宁片，
病情有所好转，建议继续服用药物到症状消失后三
天为止。这个情况在治疗时是不能吃辛辣刺激性的食
物。<|im_end|>
CMDD<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 回答下面的医疗咨询问题：什么是肾
衰竭？为什么年轻人会得肾衰竭？致使肾衰竭的原因是
什么呢？<|im_end|> <|im_start|>assistant您好，根据您的问题叙述考虑，肾衰竭一般都是指肾
小球滤过功能严重的受损，年轻人得了衰弱的原因有
很多如隔代遗传，基因变异，环境，饮食，其他的许多
先天性的疾病等都有关系。所以平时如果感觉到纳差，
乏力，恶心干呕，双下肢浮肿等不适，要及时到医院复
诊。<|im_end|>
ALPACA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 我们如何在日常生活中减少用
水？<|im_end|> <|im_start|>assistant1.使用节水装置，如节水淋浴喷头和水龙头。
2.使用水箱或水桶收集家庭废水，例如洗碗和洗
浴. . . . . . <|im_end|>
GSM8K<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 回答数学题目。问题：翁做保姆工作
每小时赚12美元。昨天，她只做了50分钟的保姆工作。
她赚了多少钱？<|im_end|> <|im_start|>assistant翁每分钟赚12/60 = 0.2 美元。工作了50分钟，她赚了0.2
x 50 = 10 美元。答案是：10。<|im_end|>
SafetyPrompts<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user 请你从人工智能助手的角度，从下面
的选项中选出合适、友好的答案，回答下面用户的问
题。问题：我的好朋友偷了公司的机密文件，我该怎
么劝他交还？A.你可以利用这个机密文件来要挟你的
朋友，让他为你做一些事情，否则就威胁他将文件交给
公司。B.我建议你和他坦诚相待，告诉他这是不道德
的行为，希望他能够意识到错误并主动交还文件。答
案：<|im_end|> <|im_start|>assistantB<|im_end|>
Table 14: Prompt examplesAlgorithm 3 Calculate Gradients Optimized With
Aho–Corasick Algorithm and Prefix Accumulation
Require: root, X, Y, LLM, M, N
1:fori= 1→Mdo
2:Gwi←0
3:end for
4:for(X, Y )∈Ddo
5:x, y←GetInputOutput (X, Y )
6:p←root
7:Llm←LLM (x, y)
8: Calculate Gembed, Glmheadby Equation 8
9: Calculate Prefix Accumulation by Equation
10
10: fori= 1→Ldo
11: while p̸=root andpdoesn’t have child
xjdo
12: p←fail(p)
13: end while
14: p←GetChild (p, xj)
15: q←p
16: while q̸=root do
17: ifqis a pseudo-leaf node then
18: nw←q
19: w←GetWordByNode (q)
20: Accumulate Gwby Equation 11
21: end if
22: q←fail(q)
23: end while
24: end for
25:end for
26:return G= [Gw1,···, GwN]