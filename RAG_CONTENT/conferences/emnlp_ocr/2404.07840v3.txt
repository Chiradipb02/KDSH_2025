On Training Data Influence of GPT Models
Yekun Chai♠Qingyi Liu*♡Shuohuan Wang♠
Yu Sun♠Qiwei Peng♢Hua Wu♠
♠Baidu Inc.♡Sun Yat-sen University♢University of Copenhagen
{chaiyekun,wangshuohuan}@baidu.com
{liuqy95}@mail2.sysu.edu.cn
Abstract
Amidst the rapid advancements in generative
language models, the investigation of how
training data shapes the performance of GPT
models is still emerging. This paper presents
GPTfluence , a novel approach that leverages
a featurized simulation to assess the impact
of training examples on the training dynam-
ics of GPT models. Our approach not only
traces the influence of individual training in-
stances on performance trajectories, such as
loss and other key metrics, on targeted test
points but also enables a comprehensive com-
parison with existing methods across various
training scenarios in GPT models, ranging from
14 million to 2.8 billion parameters, across a
range of downstream tasks. Contrary to earlier
methods that struggle with generalization to
new data, GPTfluence introduces a parameter-
ized simulation of training dynamics, demon-
strating robust generalization capabilities to un-
seen training data. This adaptability is evident
across both fine-tuning and instruction-tuning
scenarios, spanning tasks in natural language
understanding and generation. We make our
code and data publicly available at https://
github.com/ernie-research/gptfluence .
1 Introduction
The advent of generative language models, par-
ticularly the GPT series (Radford et al., 2019;
Brown et al., 2020; Zhang et al., 2022), has marked
a paradigm shift in natural language processing
(NLP) (Touvron et al., 2023; Jiang et al., 2023),
code generation (Lozhkov et al., 2024; Chai et al.,
2023), visual and language understanding (Achiam
et al., 2023; Team et al., 2023). These models have
redefined performance standards across an exten-
sive range of tasks, igniting detailed investigations
into the process of training dynamics and the in-
tricate nature of learned representations. Despite
these strides, the specific influence of individual
*Work done during QL’s internship at Baidu.training examples on the performance of GPT mod-
els remains a significantly underexplored area. This
oversight presents a critical challenge in optimiz-
ing training processes, a challenge that grows in
tandem with the increasing complexity and scale
of these models.
Current research has yetto focus comprehen-
sively on the influence of training data on autore-
gressive language models. Prior studies, such as
those utilizing the BERT (Park et al., 2023) or T5
architecture (Guu et al., 2023), have predominantly
concentrated on natural language understanding
tasks, leaving a considerable void in the exploration
of generative language models.
Furthermore, the majority of this re-
search (Pruthi et al., 2020; Guu et al., 2023;
K and Søgaard, 2021; Koh and Liang, 2017;
Yeh et al., 2018) has focused on test loss as
the primary metric of interest, neglecting other
vital performance indicators. Metrics such as
BLEU (Papineni et al., 2002) and ROUGE (Lin,
2004) scores are crucial for a thorough evaluation
of a model’s capabilities, particularly in the context
of generative language models where downstream
task performance is paramount. Additionally, the
challenge of generalizability—extending method-
ologies to accommodate unseen data—persists
as a significant barrier (Guu et al., 2023). This is
particularly critical for models expected to adapt to
the dynamic and evolving trajectory of NLP tasks.
In response to these gaps, we introduce
GPTfluence , a novel framework designed to ex-
tend the analysis of training data influence be-
yond the limitations of existing methodologies and
across a broader spectrum of tasks. Employing a
featurized simulation approach, GPTfluence esti-
mates the impact of individual training examples on
the performance of GPT models, covering both nat-
ural language understanding and generation tasks.
This expanded focus facilitates a comprehensive
understanding of model training dynamics, provid-arXiv:2404.07840v3  [cs.CL]  3 Oct 2024ing insights into a wide array of evaluation metrics
beyond mere test loss.
Extensive experiments on selected subsets from
FLAN datasets (Wei et al., 2022), across a variety
of tasks and GPT model variants (Biderman et al.,
2023), ranging in size from 14 million to 2.8 billion
parameters, validate the effectiveness and superi-
ority of our approach. Notably, our method not
only sheds light on the training dynamics of GPT
models but also demonstrates remarkable general-
ization capabilities to unseen data.
Contribution To summarize, our contributions
are as follows:
•We introduce GPTfluence , a featurized simula-
tion approach that significantly advances the anal-
ysis of training data influence on GPT models.
This approach not only enables a comprehensive
comparison with existing methodologies but also
marks the first extensive foray into the extensive
investigation of training data’s impact on the per-
formance of GPT models across various scales.
•Our approach demonstrates effectiveness on GPT
models across different scales, showing its gener-
alization capability on unseen data.
•We release the GPTDynamics dataset, a collec-
tion encompassing over 350 runs of training dy-
namics data spanning six distinct model sizes
and five NLP tasks, to facilitate further research
advancement.
2 Preliminaries
In this section, we revisit the conceptual frame-
work of training data attribution (TDA) methods,
aiming to quantify the impact of individual train-
ing instances on the performance of models with
respect to test data points.
2.1 Task Definition
Considering the data space Z, such as datasets uti-
lized for instruction-tuning, we denote a training
example by zand a test example by z′inZ. We em-
ploy a model, specifically a GPT variant in our ex-
periments, parameterized by weights θ∈Rp. Our
objective is to forecast the model’s performance
on a target metric ϕ(θ, z) :Rp×Z→R, with a
main focus in existing literature on predicting test
set loss (Pruthi et al., 2020; Guu et al., 2023).
Practically, this involves working with a se-
quence of training batches c= (c1, c2, . . . , c T),
delineating a training curriculum. Here, ctsymbol-
izes the batch of training examples utilized at step t.The crux of our task is to ascertain the influence of
training examples zon a test example of interest z′,
specifically in terms of a test metric score ϕ(θ, z′),
given the training curriculum c. This involves track-
ing changes in performance trajectory as a function
of the curriculum c, with prior research predomi-
nantly focused on test loss prediction, rather than a
broader spectrum of performance metrics.
2.2 Training Data Attribution
TracIn Inspired by the fundamental theorem
of calculus —which posits that the integral of
a function’s gradient over an interval equals
the function’s value difference across that inter-
val—TracIn (Pruthi et al., 2020) employs the first-
order Taylor expansion to quantify the data influ-
ence on test example loss at each step as follows:
Lt+1(z)≈ L t(z)−ηt⟨∇L t(zi),∇θLt(z′)⟩(1)
where ηtrepresents the learning rate at step t, and
∇θLt(·)signifies the gradient of the loss function
with respect to the model weights θ.
It adopts an influence measurement that utilizes
checkpoint ensembling, dubbed TracInCP . This
approach aggregates the influences calculated at
predefined intervals throughout the training, pro-
viding a comprehensive view of the training data’s
impact over time.
ITracIn(zi, z′) =NX
i=1ηi∇θLt(zi;θi)⊤∇θLt(z′;θi)
(2)
where Idenotes the loss change w.r.t. the training
example z, and Nindicates the total number of
model checkpoints saved during training.
Simfluence (Guu et al., 2023) approaches the chal-
lenge by learning a linear function fthat correlates
training samples zwith the test loss L(z′;θ), ex-
pressed as:
Lt(z) =α(ct)Lt−1(z) +β(ct) (3)
Here, α(ct)andβ(ct), the multiplicative and ad-
ditive factors respectively, are determined using a
linear model, with ctindicating the batch of ex-
amples consumed at training step t. Although it
offers a data-driven simulator derived from training
dynamics trajectories, its mapping from training
data indices to test data points constrains generaliz-
ability to new, unseen data.While TracIn leverages the neural model’s first-
order gradients and Simfluence employs a data-
driven simulation approach, both primarily focus
on predicting test loss. Our proposed method aligns
with Simfluence’s direction but seeks to overcome
its limitations, extending our focus to encompass a
wider array of performance metrics beyond mere
test loss prediction.
3GPTfluence : Featurized
Simulation-based Approach
3.1 Overview
We present GPTfluence , a novel approach for
tracking the impact of training examples on the
training dynamics of GPT models using a featur-
ized simulator. Figure 1 depicts the process of
GPTfluence , encompassing the collection of train-
ing dynamics, the training of the simulator, and
the execution of the final simulation. Similar to
Guu et al. (2023), our initial step involves gather-
ing a comprehensive dataset of training dynamics,
which captures both the training curriculum and
various target metrics for test examples, extending
beyond traditional loss metrics to include perfor-
mance measures like BLEU and ROUGE scores.
GPTfluence models these dynamics via an n-th
order Markov process, incorporating both multi-
plicative and additive factors to reflect the influence
of training examples. At its core, the simulator uses
a pre-trained encoder to attain the general repre-
sentation of training and test examples, ensuring
adaptability to new, unseen data. This is achieved
by modeling the intricate interplay between exam-
ples through the interactions within their condensed
hidden vector representations. In its application, it
can autoregressively forecast the complete perfor-
mance trajectory of a test example, starting from
its initial performance metrics and following the
specified training curriculum.
The collection of training dynamics is pivotal for
predicting a test sample’s performance trajectory
throughout the training process. As outlined in
§2.1, a Ttime steps training run is characterized by
a sequence of training batches c, each contributing
to the model’s evolving parameters, θt, through
gradient descent.
To monitor the performance evolution of a par-
ticular test example z′, we record its metric scores
yt=ϕ(θt, z′)at every training step t, employing
a variety of evaluation metrics beyond mere loss,
such as BLEU and ROUGE. This comprehensiverecord, denoted as y=ϕ1:T, tracks the test exam-
ple’s performance across all Tsteps of training.
From a broader dataset D, we sample Ksub-
setsD′⊂ D for GPT model training, resulting in
Kdistinct training runs. These runs yield a rich
dataset of training dynamics Drun, encapsulating
both the training curricula and the sequential target
metric scores ϕfor each test point z′. This dataset
is represented as Drun={ck, yk}K
k=1.
3.2 Featurized Simulation Approach
In this work, we introduce a featurized simulation
methodology designed to capture the effects of
training examples on GPT model training dynam-
ics. This method is predicated on conceptualizing
the training process as a sequential, time-evolving
Markov process, thereby enabling the simulation of
metric trajectories across training iterations. Build-
ing upon the foundational insights of Guu et al.
(2023), our model extends the conventional first-
order Markov assumption to an n-th order Markov
process . This allows for the consideration of a test
sample z′, where its performance metric ϕ(·)at
any given timestep tis influenced by its perfor-
mance across the preceding nsteps, encapsulated
as{ϕt−1, ϕt−2,···, ϕt−n}.
Our approach integrates both multiplicative and
additive components within the simulation. The
performance trajectory of a test sample z′is thus
delineated by a combination of these factors, for-
mulated as follows:
ϕt(z′) =nX
j=1αj(ct)ϕt−j(z′) +β(ct) (4)
where α1:n(·)andβ(·)represent the learned func-
tions attributed to the current training batch ct.
Here, αj(ct)andβ(ct)are determined through the
aggregation of influence factors Ai,jandBi, re-
spectively, across the training examples in ct:
αj(ct) =X
i∈ctAi,j, β(ct) =X
i∈ctBi(5)
We introduce a parameterized, featurized simula-
tor that employs a pre-trained encoder Ψ(·)such as
BERT (Devlin et al., 2019) and GPT (Radford et al.,
2019). This is adept at processing each training ex-
ample ziand test example z′, generating predictive
influence factors Ai,jandBithrough the encoded
representations hziandhz′:Step 1
GPT Training Dynamics Collection
Loss/ BLEU/ ROUGE Score
GPTStep 2
Featurized Simulator Training
Simulator
SimulatorStep 3
Inference
The predicted Loss/ 
BLEU/ ROUGE Score
0.1
0.2
0.7
Train the GPT 
based on the 
curriculum.
Compute the test 
metric during the 
training.A curriculum con-
sists of a subset of 
the training data.Training and test 
examples are 
encoded by the 
simulator.
The simulator 
predicts the test 
metric under the 
�-th order Markov 
assumption.
Train the simulator 
with the MSE loss.Given the initial 
training curriculum 
and test metric.
The simulator 
takes the previous 
training dynamics 
as input.
Predict the test 
metric of the next 
step.Figure 1: Overview of GPTfluence .Step 1: We sample training data to create curricula for training GPT models
and compute the test metrics of test examples at each training step. All the training curricula and the ground-truth
metrics are referred to as GPTDynamics .Step 2: We train our featurized simulator on GPTDynamics , taking into
account training examples at current and previous steps with the test example as input and predicts the ground-truth
metric. Step 3: Given a new curriculum with the test example of interest, start from the test metric at the first step,
the simulator simulates the test metric in the future training steps in an autoregressive manner.
hzi= Ψ( zi), hz′= Ψ( z′), (6)
where hziandhz′are the low-dimensional embed-
dings of the training and test examples, respectively.
To preserve the encoder’s semantic generalizability,
we keep it frozen during the simulator’s training.
The multiplicative and additive influence fac-
tors are then derived by passing the embed-
dings through the corresponding linear projections,
which are subsequently integrated using a Frobe-
nius product as follows:
Ai,j=⟨W⊤
(j)hzi
j,U⊤
(j)hz′⟩F (7)
Bi=⟨W′⊤hzi
j,U′⊤hz′⟩F (8)
whereW(j),U(j),W′,U′are learnable weights,
⟨·,·⟩Frepresents the Frobenius inner product be-
tween the hidden representations of the training
and test examples, yielding a refined estimation of
the multiplicative influence exerted by each train-
ing example zion the test example’s performance
trajectory. Our approach offers a granular and com-
prehensive analysis of training dynamics through
this intricate data-driven simulation.
To learn our featurized simulator Θ, we optimizethe following L2-regularized regression objective:
Θ⋆= argmin
ΘX
t∈T(yt−ˆϕt(z′))2+λ(||Θ||2
2)(9)
where λis the discounting factor dictating the de-
gree of L2-regularization, ˆϕt(·)is the test score
prediction at step tusing Eq. (4). Refer to Algo-
rithm 1 for the pseudo-code.
3.3 Connection to Previous Approaches
Our approach offers a flexible framework that, un-
der specific conditions, aligns with established
models in the TDA literature. Specifically, when
the focus narrows down to the overall influence
of per-step dynamics, our approach converges to
the datamodels (Ilyas et al., 2022; Engstrom et al.,
2024). Moreover, in scenarios where the Markov
order nis set to 1 and the input encoder is config-
ured to process sample indices, our method reduces
to Simfluence (Guu et al., 2023).
4 Experiments
4.1 Experimental Settings
4.1.1 GPTDynamics Data Collection
Datasets and GPT Training Scenarios In sub-
sequent experiments, we refer to the comprehen-
sive training process that employs the aggregatedMethod #ParamRTE SST-2 BoolQ
All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)
TracIn-CP (10-steps)
410M1.156 (0.838) 0.787 (0.339) 0.460 0.551 (0.560) 0.584 (0.307) -0.089 0.957 (0.728) 0.735 (0.332) -0.066
TracIn-CP (all-steps) 0.757 (0.591) 0.629 (0.299) 0.460 0.446 (0.555) 0.525 (0.321) -0.089 0.782 (0.690) 0.680 (0.339) -0.066
Grad-Dot 12.061 (3.688) 2.906 (0.410) 0.459 7.715 (1.543) 1.918 (0.205) -0.084 12.527 (3.617) 2.900 (0.344) -0.071
Simfluence 1.477 (0.274) 0.634 (0.111) 0.426 (0.340) 1.133 (0.287) 0.455 (0.082) 0.696 (0.156) 1.189 (0.362) 0.485 (0.082) 0.793 (0.201)
Ours 0.220 (0.184) 0.334 (0.140) 0.644 (0.174) 0.111 (0.045) 0.224 (0.047) 0.834 (0.129) 0.132 (0.073) 0.251 (0.075) 0.828 (0.154)
TracIn-CP (10-steps)
1B1.225 (0.744) 0.979 (0.344) -0.203 4.412 (1.301) 1.697 (0.170) -0.058 0.999 (1.034) 0.793 (0.400) 0.649
TracIn-CP (all-steps) 1.137 (0.740) 0.939 (0.343) -0.203 2.158 (0.782) 1.218 (0.187) -0.058 0.858 (1.043) 0.731 (0.416) 0.649
Grad-Dot 21.928 (7.871) 4.332 (0.874) -0.198 6.601 (1.927) 2.077 (0.193) -0.057 18.270 (5.630) 3.563 (0.711) 0.650
Simfluence 0.889 (0.551) 0.523 (0.197) 0.360 (0.207) 0.582 (0.253) 0.410 (0.084) 0.712 (0.148) 0.876 (0.470) 0.469 (0.198) 0.862 (0.050)
Ours 0.099 (0.078) 0.227 (0.097) 0.757 (0.123) 0.096 (0.075) 0.221 (0.084) 0.807 (0.175) 0.068 (0.058) 0.187 (0.070) 0.953 (0.034)
TracInCP (10-steps)
2.8B8.869 (3.673) 2.700 (0.650) 0.573 0.294 (0.235) 0.447 (0.176) 0.801 1.185 (1.271) 0.804 (0.436) 0.184
TracInCP (all-steps) 10.256 (4.396) 2.967 (0.652) 0.573 0.265 (0.228) 0.419 (0.178) 0.801 1.183 (1.260) 0.800 (0.434) 0.184
Grad-Dot 10.101 (9.212) 2.580 (1.327) 0.573 1.216 (0.411) 0.935 (0.175) -0.801 1.990 (1.082) 1.219 (0.321) 0.184
Simfluence-linear 2.032 (1.214) 0.996 (0.360) 0.845 (0.061) 0.921 (0.435) 0.634 (0.194) 0.912 (0.018) 1.545 (1.293) 0.849 (0.412) 0.681 (0.087)
Ours 0.132 (0.172) 0.273 (0.129) 0.969 (0.009) 0.023 (0.015) 0.123 (0.040) 0.979 (0.006) 0.175 (0.232) 0.305 (0.165) 0.963 (0.018)
Method #ParamWebNLG WMT-16 DE/EN Average
All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)
TracIn-CP (10-steps)
410M0.048 (0.072) 0.168 (0.115) 0.836 0.030 (0.071) 0.122 (0.107) 0.963 0.548 0.479 0.421
TracIn-CP (all-steps) 0.050 (0.073) 0.173 (0.113) 0.836 0.030 (0.071) 0.123 (0.107) 0.963 0.413 0.426 0.421
Grad-Dot 0.062 (0.080) 0.187 (0.113) 0.837 0.033 (0.073) 0.127 (0.109) 0.963 6.479 1.608 0.421
Simfluence 0.036 (0.029) 0.130 (0.049) 0.986 (0.002) 0.016 (0.013) 0.101 (0.034) 0.997 (0.001) 0.770 0.361 0.779
Ours 0.002 (0.002) 0.033 (0.017) 0.994 (0.001) 0.002 (0.004) 0.033 (0.023) 0.998 (0.000) 0.093 0.175 0.860
TracIn-CP (10-steps)
1B0.032 (0.053) 0.132 (0.095) 0.885 0.012 (0.032) 0.075 (0.069) 0.981 1.336 0.735 0.451
TracIn-CP (all-steps) 0.033 (0.053) 0.135 (0.094) 0.885 0.012 (0.032) 0.076 (0.069) 0.981 0.840 0.620 0.451
Grad-Dot 0.044 (0.061) 0.154 (0.097) 0.881 0.013 (0.033) 0.075 (0.071) 0.981 9.371 2.040 0.451
Simfluence 0.167 (0.127) 0.323 (0.112) 0.823 (0.030) 0.171 (0.269) 0.309 (0.168) 0.925 (0.007) 0.537 0.407 0.737
Ours 0.007 (0.005) 0.068 (0.022) 0.984 (0.005) 0.004 (0.004) 0.049 (0.020) 0.997 (0.001) 0.055 0.150 0.900
TracInCP (10-steps)
2.8B0.005 (0.008) 0.051 (0.035) 0.978 0.001 (0.002) 0.020 (0.019) 0.997 2.071 0.804 0.707
TracInCP (all-steps) 0.005 (0.008) 0.051 (0.035) 0.978 0.001 (0.002) 0.020 (0.019) 0.997 2.342 0.851 0.707
Grad-Dot 0.015 (0.020) 0.089 (0.061) 0.978 0.001 (0.002) 0.021 (0.019) 0.997 2.665 0.969 0.386
Simfluence-linear 0.102 (0.065) 0.283 (0.091) 0.971 (0.004) 0.063 (0.085) 0.203 (0.119) 0.991 (0.001) 0.933 0.593 0.880
Ours 0.001 (0.001) 0.024 (0.016) 0.997 (0.000) 0.001 (0.002) 0.020 (0.016) 0.999 (0.000) 0.066 0.149 0.981
Table 1: Results of test loss estimation for instruction tuning . Results are averaged over 5 held-out test runs.
FLAN datasets along with task-specific instruc-
tions as instruction tuning . Conversely, the term
fine-tuning is reserved to describe the process of
individually optimizing models on separate tasks
without the use of instructional prompts. Both in-
struction tuning and fine-tuning processes are en-
capsulated within our GPTDynamics dataset. We
refer to Appendix §A.1 for detailed information.
GPT Backbone We employed Pythia (Biderman
et al., 2023), a model suite recently made avail-
able to the public, as our foundational architecture.
Within this suite, we selected five distinct mod-
els based on their sizes, encompassing 14M, 70M,
160M, 410M, 1B, and 2.8B, to ensure a broad range
of computational capacities were represented.
4.1.2 Experiment Setup for Simulators
Baselines We select TracIn (Pruthi et al., 2020),
Grad-Dot (Charpiat et al., 2019), and Simflu-
ence (Guu et al., 2023) as our baselines. Refer
to Appendix §A.2 for detailed information.
Evaluation Metrics We utilize a comprehensive
set of metrics, including the Mean Squared Error
(MSE) and Mean Absolute Error (MAE) calculated
across all training steps, alongside the Spearman
correlation coefficient ( ρ) at the final step, to thor-
oughly assess performance.Dataset Method All-Steps
MSE (↓)All-Steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)
RTE Simfluence 0.035 (0.022) 0.151 (0.054) 0.743 (0.094)
Ours 0.036 (0.029) 0.151 (0.060) 0.746 (0.095)
SST-2 Simfluence 0.037 (0.017) 0.128 (0.030) 0.938 (0.074)
Ours 0.014 (0.006) 0.081 (0.018) 0.943 (0.073)
BoolQ Simfluence 0.032 (0.019) 0.140 (0.038) 0.992 (0.002)
Ours 0.011 (0.011) 0.082 (0.049) 0.994 (0.002)
WebNLG Simfluence 0.016 (0.012) 0.094 (0.036) 0.984 (0.002)
Ours 0.011 (0.014) 0.078 (0.043) 0.985 (0.002)
WMT-16 Simfluence 0.010 (0.008) 0.067 (0.029) 0.998 (0.003)
DE/EN Ours 0.002 (0.002) 0.031 (0.018) 0.999 (0.000)
Average Simfluence 0.026 0.116 0.931
Ours 0.015 0.084 0.933
Table 2: Results of test loss estimation for fine-tuning .
4.2 Test Loss Estimation
Instruction Tuning Table 1 presents a com-
parison between our approach and traditional
TDA methods for instruction tuning. GPTfluence
demonstrated a distinct edge over Simfluence and
other gradient-based TDA techniques across a set
of five natural language understanding (NLU) and
natural language generation (NLG) tasks, as evi-
denced by the MSE and MAE metrics for the entire
trajectory, alongside the Spearman correlation co-
efficients at the final time step across various test
samples. Examples are shown in Fig. 2(a) and 2(b).
Additionally, we observed that while the effective-
ness of all evaluated TDA methods in predicting
loss trajectories varied with changes in GPT sizes,
GPTfluence maintained optimal performance, in-
dependent of the GPT scale.Fine-tuning In Table 2, it is evident that our ap-
proach consistently outperforms Simfluence when
it comes to fine-tuning GPT models. On average,
our method reduces the MSE and MAE across all
training steps by 42% and28%, respectively, when
compared to Simfluence. This implies that our
method is more robust and adaptable in simulating
training dynamics.
4.3 Generalizing to Test Metric Estimation
We have expanded the evaluation of our model be-
yond the mere prediction of test loss, now including
vital measures such as ROUGE and BLEU scores.
We have not reported the performance of TracIn
and Grad-Dot baselines due to its inability on such
metric predictions.
Instruction Tuning As for instruction tuning,
our findings, displayed in Table 3, demonstrate
a superior performance of our method over Sim-
fluence in predicting both BLEU and ROUGE-L
scores and for GPTs of varying sizes. Intuitively,
We draw some qualitative examples in the Fig. 2(c)
and 2(d). Notably, for BLEU simulation on the
WMT-16 DE/EN task, as the size of GPT increases,
all steps MSE of Simfluence increases, whereas
our method maintains a more stable performance,
even exhibiting slight improvements from 0.92 to
0.93 in loss prediction accuracy at the final step.
This suggests that our model is better equipped to
manage more challenging tasks and larger model
sizes, leveraging the pre-trained representations
and instance interactions.
Fine-tuning Our method’s superiority remains
evident in the fine-tuning scenario, as depicted in
Table 4, underscoring the robustness of our feature-
based simulation approach. It’s worth noting that
the margin by which GPTfluence outperforms Sim-
fluence in BLEU metric simulation is not as pro-
nounced in fine-tuning contexts as it is in instruc-
tion tuning settings. This discrepancy is likely due
to the richer and more diverse data available in in-
struction tuning, which accentuates Simfluence’s
relative inefficiency, given its independent parame-
ter learning for each training instance and a distinct
simulator for each test instance.
4.4 Ablation Study
Practical Influence via Checkpoints Our fea-
tured simulator is adept at learning from past train-
ing dynamics. However, monitoring the training
dynamics at every step can be expensive, especiallyMethod #ParamWebNLG
BLEU ROUGE-L
All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
Simfluence410M23.47 (63.52) 2.34 (3.26) 0.81 (0.02) 0.007 (0.008) 0.055 (0.038) 0.708 (0.067)
Ours 9.11 (18.41) 1.73 (1.82) 0.90 (0.03) 0.005 (0.006) 0.045 (0.034) 0.796 (0.047)
Simfluence1B20.58 (60.80) 2.01 (3.03) 0.87 (0.03) 0.006 (0.006) 0.052 (0.031) 0.878 (0.035)
Ours 9.72 (23.70) 1.63 (2.02) 0.86 (0.03) 0.004 (0.005) 0.043 (0.029) 0.903 (0.020)
Simfluence 2.8B 15.08 (51.72) 1.52 (2.90) 0.80 (0.08) 0.005 (0.006) 0.050 (0.036) 0.817 (0.063)
Ours 5.56 (17.26) 1.15 (1.42) 0.86 (0.05) 0.003 (0.003) 0.035 (0.026) 0.911 (0.050)
Method #ParamWMT-16 DE/EN
BLEU ROUGE-L
All-steps
MSE (↓)All-steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)All-steps
MSE (↓)All-steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)
Simfluence410M32.15 (116.17) 2.25 (4.08) 0.83 (0.03) 0.007 (0.017) 0.039 (0.055) 0.931 (0.014)
Ours 7.71 (28.05) 1.14 (1.92) 0.92 (0.02) 0.004 (0.009) 0.030 (0.041) 0.964 (0.012)
Simfluence1B162.94 (466.30) 5.71 (9.03) 0.76 (0.03) 0.025 (0.038) 0.094 (0.098) 0.833 (0.031)
Ours 46.33 (122.50) 3.34 (4.68) 0.93 (0.01) 0.013 (0.020) 0.066 (0.069) 0.910 (0.011)
Simfluence 2.8B 64.07 (319.93) 2.59 (5.84) 0.90 (0.05) 0.008 (0.022) 0.040 (0.059) 0.912 (0.045)
Ours 24.27 (93.41) 1.94 (3.36) 0.93 (0.05) 0.005 (0.018) 0.030 (0.051) 0.936 (0.037)
Method #ParamAverage
BLEU ROUGE-L
All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
Simfluence410M27.81 2.29 0.82 0.007 0.047 0.820
Ours 8.41 1.43 0.91 0.004 0.037 0.880
Simfluence1B91.76 3.86 0.81 0.015 0.073 0.855
Ours 28.02 2.51 0.90 0.008 0.055 0.907
Simfluence 2.8B 39.58 2.06 0.85 0.007 0.045 0.865
Ours 14.92 1.55 0.89 0.004 0.033 0.924
Table 3: Results of test metric estimation on NLG
datasets for instruction-tuning .
Dataset Metric Method All-steps
MSE (↓)All-steps
MAE ( ↓)Final-Step Spear-
man’s ρ(↑)
WebNLGBLEUSimfluence 43.33 (77.34) 4.23 (3.52) 0.78 (0.02)
Ours 43.98 (81.40) 4.28 (3.57) 0.80 (0.01)
ROUGE-LSimfluence 0.008 (0.007) 0.066 (0.031) 0.706 (0.038)
Ours 0.007 (0.006) 0.060 (0.029) 0.765 (0.040)
WMT-16
DE/ENBLEUSimfluence 32.11 (89.13) 2.76 (3.75) 0.82 (0.02)
Ours 30.26 (77.23) 2.91 (3.69) 0.81 (0.02)
ROUGE-LSimfluence 0.018 (0.025) 0.091 (0.075) 0.796 (0.032)
Ours 0.012 (0.016) 0.075 (0.057) 0.843 (0.010)
AverageBLEUSimfluence 37.72 3.49 0.80
Ours 37.12 3.59 0.81
ROUGE-LSimfluence 0.013 0.079 0.751
Ours 0.009 0.068 0.805
Table 4: Results of test metric estimation on NLG
datasets for fine-tuning .
when dealing with large-sized GPTs. Therefore,
we conduct experiments to choose training check-
points at specific intervals to approximate the re-
ality of the neighboring points with the training
state of that particular point. Then, we trained our
simulator on the approximate training dynamics
to find the balance between the cost of collecting
training dynamics and the simulator performance.
Results are shown in Fig. 3. Unless otherwise
specified, we instruction tuning the Pythia-410M
for further analysis. In general, the performance of
our simulator deteriorates as the number of check-
point intervals increases. This is manifested by a
rise in MSE and MAE at all steps and a drop in
Spearman’s ρwhen the checkpoint interval is large.
However, even when the number of checkpoint in-
tervals is equal to 10, which means that we will
use the training state of one point to approximate
the training state of the previous ten points and the
training dynamics collection time will be shortened
by almost 90%, our method still has comparable
prediction error at all steps and better Spearman
coefficient than Simfluence.0 50 100 150 200 250 300
Training Steps02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot(a) Loss simulation on RTE
0 50 100 150 200 250 300
Training Steps0.40.60.81.01.2Loss
 Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot (b) Loss simulation on WMT16 DE/EN
0 50 100 150 200 250 300
Training Steps24681012BLEU
Ground Truth
Ours
Simfluence
(c) BLEU simulation on WebNLG
0 50 100 150 200 250 300
Training Steps0.00.10.20.3ROUGE-L
Ground Truth
Ours
Simfluence (d) ROUGE-L simulation on WMT16 DE/EN
Figure 2: Illustration of loss andmetric simulation on NLU andNLG tasks with different TDA methods for
instruction tuning . See the §D for more examples.
Empirical Analysis of Markov Order Depen-
dency Using the first-order Markov process to
predict future states based on the prior step, poten-
tially oversimplifies GPT training dynamics. There-
fore, we consider the training dynamics as an n-th
order Markov process ( n= 2,3,5,10) and exper-
iment on both language understanding (RTE) and
generative (WebNLG) tasks.
The result can be seen in Fig. 4. Overall, when
considering more preceding training information,
the simulation error initially increases and de-
creases for both datasets, as indicated by the all-
steps MSE metric. It suggests that a high order
nmight introduce noise, leading to a degraded
simulator’s performance. Moreover, the final-step
Spearman’s ρshows a significant increase from
0.746 to 0.785 for RTE with the increase of order
n, but not the same for WebNLG. We guess consid-
ering more past training information could improve
the prediction accuracy for NLU tasks.
Impact of Different Feature Representations
To further explore the impact of various feature
representations, we conducted experiments on twotypes of pre-trained encoders: BERT1and Pythia2
with different sizes. Results are shown in Fig. 5. In
general, BERT’s feature representations produce
better simulation results than the Pythia encoder.
This could be due to its ability to encode context in-
formation in both directions. Interestingly, we also
found that increasing the parameters of the Pythia
encoder does not always lead to better performance
of the performance simulator.
4.5 Analysis
Robustness across Varying Model Sizes We
conducted experiments to validate how our sim-
ulator handles the complexity of GPTs of different
sizes, ranging from 14M to 2.8B, specifically fo-
cusing on instruction tuning scenarios. Results are
presented in Fig. 6. Our loss simulation experi-
ments revealed that despite the inconsistent simula-
tion performance trend with increasing GPT size,
our featurized model consistently surpassed Sim-
fluence. These findings demonstrate the superiority
of our model in effectively capturing and managing
model complexity.
1https://huggingface.co/sentence-transformers/
all-MiniLM-L6-v2
2https://github.com/EleutherAI/pythia#Chec kpoint Interval
 #Chec kpoint Interval #Chec kpoint IntervalFigure 3: Variation curves of the average performance
ofGPTfluence for loss simulation in five datasets when
different checkpoint intervals are selected.
1 23 5 10
Order n0.03550.03600.0365All-steps MSE
1 23 5 10
Order n0.1470.1480.1490.1500.151All-steps MAE
1 23 5 10
Order n0.750.760.770.78Final-step Spearman's 
Ours Simfluence
(a) RTE
1 23 5 10
Order n0.0100.0110.012All-steps MSE
1 23 5 10
Order n0.0750.0800.0850.0900.095All-steps MAE
1 23 5 10
Order n0.9830.9840.985
Ours Simfluence
Final-step
 
Spearman's
 
(b) WebNLG
Figure 4: Analysis on the impact of n-th order Markov
process on language understanding (RTE) and genera-
tion (WebNLG) tasks, varying nfrom 1 to 10.
Unseen Data Generalization Unlike Simflu-
ence, which restricts the parameters only in-
dexed by seen samples of past training runs, our
GPTfluence can handle unseen samples via sam-
ple parameterization. We conducted experiments
on RTE and WebNLG tasks in fine-tuning scenar-
ios to further verify the unseen data generalization.
For a future training run, we experiment in three
different unseen data scenarios: 1) Examples in
the training curriculum are unseen; 2) Test exam-
ples are unseen; 3) Both examples in training the
curriculum and test examples are unseen.
We defer the results in Table 11 in Appendix.
BERT Pythia0.1600.1650.1700.1750.1800.1850.1900.1950.200All-steps MAEBERT Pythia-160M Pythia-410M Pythia-1B
BERT Pythia0.600.650.700.750.800.850.90Final-step Spearman's 
Figure 5: Impact of feature representation of different
pre-trained encoders on loss simulation.
0.03 0.1 0.3 1.0 4.0
All-steps MSE14M70M160M410M1B2.8BModel SizeSimfluence Ours
0.1 0.20.3 0.5 1.0
All-steps MAE14M70M160M410M1B2.8B
0.4 0.6 0.8 1.0
Final-step Spearman's 
14M70M160M410M1B2.8BFigure 6: Comparison of the loss simulation be-
tween GPTfluence and Simfluence on instruction tun-
ing Pythia model series, ranging from 14M to 2.8B.
Overall, GPTfluence can generalize to unseen data,
which includes simulating loss and performance
metrics. What’s more, we find that GPTfluence
is better at generalizing to unseen training data to
simulate the impact of test samples that have been
seen in the past. To illustrate this more visually, we
show the effect of GPTfluence’s simulation of the
unseen training data setting with loss and perfor-
mance metrics, respectively. As shown in Fig. 8,
the generalization performance of GPTfluence is
mostly satisfactory.
4.6 Use Case: Mislabelled Data Identification
Following previous studies (Yeh et al., 2018; Pruthi
et al., 2020), we present a mislabeled data identifi-
cation use case to evaluate our TDA-based method.
Experimental Setup We employ the Pythia-
410M model as our classifier and utilize a subset
of the SST-2 dataset. The methods compared in-
clude the following: Random , where we bypass
influence calculation and apply random shuffling3.
TracIn-CP , which uses self-influence as the metric
by computing the gradient dot-product between a
sample and itself. Similarly, GPTfluence calcu-
lates the influence by simulating the multiplicative
factor αon the sample itself.
Results The results are depicted in Fig. 7. When
examining the fraction of mislabelled data iden-
tified, GPTfluence demonstrates comparable per-
formance to random selection, albeit slightly un-
derperforming compared to TracIn-CP. However,
the marginal difference in mislabel detection is off-
set by the notable improvement in test accuracy
achieved with GPTfluence. Our method outper-
forms both TracIn-CP and random selection, par-
ticularly excelling in the early stages of mislabel
3Random shuffling is performed ten times with varying
seeds, and the average result is reported.0 0.1 0.2 0.3 0.4 0.5
Fraction of training data checked0.550.600.650.700.750.80T est Accuracy
0 0.1 0.2 0.3 0.4 0.5
Fraction of training data checked0.00.10.20.30.40.5Fraction of mislabels identified
Random
TracInCp
GPTFluenceFigure 7: SST-2 Mislabelled Data Identification with
GPTfluence, TracIn-CP and Random Selection.
detection, which is crucial when reviewing a small
fraction of data. In scenarios where precision is key,
especially with limited data available for review,
GPTfluence proves its efficacy.
To simulate mislabeled data, we corrupted 40%
of the training set by flipping the labels, resulting
in an initial classification accuracy of 0.53. We
then sequentially corrected mislabelled samples by
inspecting fractions of the dataset ranked by our
influence metric, computed via the TDA method.
After correcting the mislabels, we retrained the clas-
sifier and reported the test accuracy on the cleaned
dataset.
5 Related Work
Our methodology extends the frontier of TDA tech-
niques, which are instrumental in understanding
the influence of individual training instances on
model predictions. This body of work bifurcates
into two main strands: gradient-based approxima-
tion methods and simulation-based approaches.
Gradient-Based Approximation Methods This
strand of research capitalizes on gradient informa-
tion to infer the influence of training instances on
model predictions, providing a quantifiable mea-
sure of individual data points’ contributions (Koh
and Liang, 2017; Yeh et al., 2018; K and Søgaard,
2021). Influence Functions, a pioneering method
in this domain, leverages the mathematical frame-
work of influence functions for estimating the im-
pact of dataset perturbations on model predictions.
Complementing this, TracIn (Pruthi et al., 2020)
employs gradient-based approximations to trace
the influence of training data on test predictions.
Similarly, Grad-Dot (Charpiat et al., 2019) uses
gradient dot products to approximate the influence
of training examples. A contemporary work (Xia
et al., 2024) that adapts the TracIn framework
for models optimized with Adam. LESS incor-
porates LoRA (Hu et al., 2021) and random projec-tion (Park et al., 2023) techniques to enhance data
selection processes. These methods primarily rely
on gradients to quantify data influence, offering
tractable solutions with varying degrees of approx-
imation accuracy.
Simulation-Based Approaches An alternative
research vein adopts model-based simulations to
represent training dynamics (Ilyas et al., 2022; Guu
et al., 2023). Simfluence (Guu et al., 2023) pio-
neers the simulation-based category by learning a
linear model that predicts the influence of training
examples through multiplicative and additive fac-
tors, as detailed in §2. Recent efforts (Engstrom
et al., 2024) have focused on simulating the overall
influence of training examples, aiming at predict-
ing the cumulative influence of training data for
refined data selection.
Our contribution distinctly advances the
simulation-based direction by forecasting the
end-point influence and modeling the entire
trajectory of training dynamics using featurized
representations. This approach provides a more
in-depth understanding of training data influence,
facilitating dynamic adjustments and insights into
the model training curricula.
6 Conclusion and Future Work
In this paper, we explore the data attribution analy-
sis for GPT models through GPTfluence , a novel
featurized simulator approach. This methodology
not only surpasses the predictive capabilities of
traditional test loss metrics but forecasts essential
task performance metrics across a broad spectrum
of GPT model sizes, ranging from 14M to 2.8B
parameters. Our comprehensive evaluations across
diverse downstream tasks and fine-tuning scenarios
substantiate the superior efficacy of our approach.
In the future, extending this approach to other tasks
and training regime presents a promising avenue
for future research.
Acknowledgements
We would like to thank all anonymous reviewers
for their insightful and constructive feedback. Qi-
wei Peng is supported by DisAI - Improving scien-
tific excellence and creativity in combating disin-
formation with artificial intelligence and language
technologies, a project funded by European Union
under the Horizon Europe, GA No. 101079164.Ethical Consideration
While our study focuses on predicting the influence
of training data on GPT models, we recognize the
broader ethical implications that our research may
entail, especially as it contributes to the advance-
ment of large language models (LLMs) that are
increasingly integrated into societal functions.
Data Use and Privacy Our research utilizes pub-
licly available datasets and respects privacy con-
cerns by anonymizing any potentially identifiable
information. We ensure that our data handling prac-
tices comply with all relevant data protection regu-
lations and ethical guidelines, safeguarding against
misuse.
Potential Misuse We are cognizant of the poten-
tial misuse of predictive models in manipulating or
unfairly influencing AI systems. Our research aims
to contribute to the understanding and mitigation
of such risks by providing tools to analyze and ad-
just the influence of training data. We encourage
the application of our findings in ethical ways that
promote fairness and transparency in AI.
Broader Impact This study advances under-
standing of data influence on LLMs, offering a
methodological approach for detailed impact anal-
ysis. This work not only enhances the interpretabil-
ity and transparency of LLMs but also lays the
groundwork for more informed and ethical deci-
sions in data curation and model training.
Limitations
This work introduces a novel feature-based ap-
proach within the simulation-based framework for
predicting the influence of training data on GPT
models. While our methodology represents a sig-
nificant advancement in the field, it is not without
its limitations, which we discuss below:
Dependence on Extensive Training Dynamics
A fundamental constraint of our approach is its re-
liance on a comprehensive set of training dynamics
to train the simulator effectively. This requirement,
while crucial for the accuracy of our predictions,
necessitates considerable computational resources
and time. The efficiency of data influence simu-
lators remains an area ripe for further exploration,
with the aim of reducing the computational over-
head without compromising on performance.Limited Dataset Scope Our experimental valida-
tion is confined to a subset of the FLAN datasets,
constrained by the logistical and computational
costs associated with collecting a large-scale train-
ing dynamics dataset. Despite this limitation, we
have conducted over 352 training experiments
across six different GPT model sizes (ranging from
14M to 2.8B parameters) to amass the GPTDynam-
icsdataset. This dataset, which we are making
publicly available, is a step towards mitigating the
data scarcity in this research area, yet the need for
more expansive datasets encompassing a broader
range of tasks and languages remains.
Model Size Constraints The high computational
costs involved in executing multiple runs on larger
language models, such as those with 13B or even
72B parameters, have limited the scale of the mod-
els we could feasibly include in our study. While
our findings are robust across the examined model
sizes, extending our analysis to larger models with
hundreds of billions of parameters would likely
yield additional insights into the scalability and
generalizability of our approach.
Generalization to Other Domains While our
study focuses on GPT models and a specific subset
of datasets, the generalizability of our approach to
other model architectures and domains is not fully
explored. Future work could extend our method-
ology to different types of language models and
beyond, including vision and multimodal systems,
to assess the applicability and adaptability of our
featurized simulation-based approach.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar van der Wal. 2023. Pythia:
A suite for analyzing large language models across
training and scaling.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun,
Hao Tian, and Hua Wu. 2023. Ernie-code: Be-
yond english-centric cross-lingual pretraining for pro-
gramming languages. In Findings of the Association
for Computational Linguistics: ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 10628–10650. Asso-
ciation for Computational Linguistics.
Guillaume Charpiat, Nicolas Girard, Loris Felardos,
and Yuliya Tarabalka. 2019. Input similarity from
the neural network perspective. Advances in Neural
Information Processing Systems , 32.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Logan Engstrom, Axel Feldmann, and Aleksander
Madry. 2024. Dsdm: Model-aware dataset selection
with datamodels. arXiv preprint arXiv:2401.12926 .
Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon,
Ian Tenney, and Tolga Bolukbasi. 2023. Simflu-
ence: Modeling the influence of individual training
examples by simulating training runs. arXiv preprint
arXiv:2303.08114 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Andrew Ilyas, Sung Min Park, Logan Engstrom, Guil-
laume Leclerc, and Aleksander Madry. 2022. Data-
models: Predicting predictions from training data.
arXiv preprint arXiv:2202.00622 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Karthikeyan K and Anders Søgaard. 2021. Revisiting
methods for finding influential examples.
Pang Wei Koh and Percy Liang. 2017. Understanding
black-box predictions via influence functions. In
International conference on machine learning , pages
1885–1894. PMLR.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-
erico Cassano, Joel Lamy-Poirier, Nouamane Tazi,
Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,
et al. 2024. Starcoder 2 and the stack v2: The next
generation. arXiv preprint arXiv:2402.19173 .Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guil-
laume Leclerc, and Aleksander Madry. 2023. Trak:
Attributing model behavior at scale. arXiv preprint
arXiv:2303.14186 .
Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund
Sundararajan. 2020. Estimating training data influ-
ence by tracing gradient descent. Advances in Neural
Information Processing Systems , 33:19920–19930.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned
language models are zero-shot learners.
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan,
Sanjeev Arora, and Danqi Chen. 2024. Less: Se-
lecting influential data for targeted instruction tuning.
arXiv preprint arXiv:2402.04333 .
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and
Pradeep K Ravikumar. 2018. Representer point selec-
tion for explaining deep neural networks. Advances
in neural information processing systems , 31.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.A Implementation Details
A.1 Tasks and Datasets for GPTDynamics
We conduct experiments on a subset of FLAN (Wei
et al., 2022), a diverse array of datasets for instruc-
tion tuning , to conduct a thorough evaluation of
TDA methods. Our dataset selection spans both
NLU and NLG tasks, thereby offering a broad spec-
trum of challenges for TDA methods to tackle.
The NLU tasks selected include RTE (Natural
Language Inference), SST-2 (Sentiment Classifica-
tion), and BoolQ (Reading Comprehension). For
NLG, we delve into WebNLG (Struct-to-Text) and
WMT-16 DE/EN (Machine Translation) tasks.
To exploit the superior generalization benefits
that instruction tuning brings to language mod-
els, we have assembled a specialized subset for
instruction fine-tuning. This subset amalgamates
the previously mentioned five tasks with CNN-DM
(Summarization), crafting an extensive testing en-
vironment of FLAN data. We sourced task-specific
instructions directly from the original FLAN paper.
A.2 Comparison Baselines
TracIn (Pruthi et al., 2020) is a gradient-based
used to calculate the influence through a first-order
gradient approximation. It considers the influence
of the training example zon the test example z′
as a loss change in z′, which is provided by each
gradient step of the training example z. In practice,
TracInCP was proposed as an alternative approxi-
mation that considers specific checkpoints during
training. TracInCP calculates the gradient dot prod-
uct of zandz′at these checkpoints. In our experi-
ments, we used TracInCP with 10 checkpoints and
all steps’ checkpoints to estimate the influence.
Grad-Dot (Charpiat et al., 2019) is a heuristic
gradient-based TDA method. They also compute
the effect of a training sample on a test sample by
the dot product of the gradients but computed on
top of the final trained model.
Simfluence (Guu et al., 2023) is a novel frame-
work for TDA. It characterizes the loss variation
of test samples during training by modeling it as
a Markov process. Then, it learns a unique multi-
plicative and additive influence parameter for each
training example. It is worth noting that in the
original paper, the framework that considers both
multiplicative and additive influences is referred to
asSimfluence-linear . However, for simplicity in
this paper, we use the term Simfluence to refer to
the same model.A.3 Implementation Details of Instruction
Tuning
GPTDynamics Collection for Instruction Tun-
ing We instruction tuned Pythia from 14M to
2.8B ( i.e., 14M, 70M, 160M, 410M, 1B, and 2.8B)
on the instruction tuning dataset referenced in Ap-
pendix A.1. We collect a total of randomly sam-
pled 768 instances from aforementioned five tasks,
with each samples 128 of 200 data points in one
training run for instruction tuning. The data di-
vision followed the same protocol as in the fine-
tuning scenarios. All Pythia models underwent
comprehensive fine-tuning, with the exception of
the Pythia-2.8B model, which was fine-tuned using
the parameter-efficient LoRA technique (Hu et al.,
2021). The LoRA module was implemented within
the query, key, and value projection matrices of
the self-attention module, with a LoRA rank of 8,
alpha set to 4, and a dropout probability of 0.05.
We evaluated the Pythia models using the identi-
cal datasets as those in the fine-tuning experiments.
For the WebNLG and WMT16 DE/EN datasets, we
evaluated BLEU and ROUGE-L scores in addition
to test loss, employing a top- psampling strategy
for generation with a temperature of 0.2 and top-
pprobability of 0.95. Detailed instruction-tuning
hyperparameters are reported in Table 5.
GPTfluence Training Setup The architecture of
our simulator is a pre-trained sentence encoder fol-
lowed by parallel weight-sharing fully-connected
layers for predicting influence factors. The train-
able model size of the simulator is 11.4M excluding
pre-trained embeddings (frozen). Unless specified,
we use the sentence transformer4as our pre-trained
encoder. For the simulator training, we combine
all five FLAN datasets and train our simulator in
a multi-task manner, each dataset has 27 training
runs. All reported results are averaged over 5 held-
out runs. We set the order nof Markov process
assumptions equal to 1 for instruction tuning. De-
tailed training hyperparameters of GPTfluence are
shown in Table 6.
A.4 Implementation Details of Fine-Tuning
GPTDynamics Collection for Fine-Tuning All
the experiments are conducted on the NVIDIA
Tesla V100 GPUs unless specified. We fine-tune
Pythia-410M on five datasets: SST-2, BoolQ, RTE,
WebNLG, and WMT16 DE/EN. For each dataset,
4https://huggingface.co/sentence-transformers/
all-MiniLM-L6-v2Instruction-Tuning Hyperparameters Pythia-14M Pythia-70M Pythia-160M Pythia-410M Pythia-1B Pythia-2.8B
Optimizer AdamW
Adam’s β (0.9, 0.999)
Adam’s ϵ 1e-6
Weight decay 0.001
Learning rate 5e-7 5e-7 5e-7 2e-7 2e-7 1e-5
Learning rate schedule Linear decay
Warmup steps 0
Batch size 8
Max sequence length 2048 2048 2048 2048 2048 1024
Training epochs 3 3 3 3 2 2
Training steps 288 288 288 288 192 192
Precision fp32
Table 5: Hyper-parameter settings for instruction tuning GPTDynamics data across Pythia models, ranging in size
from 14M to 2.8B.
Hyperparameters Pythia-14M Pythia-70M Pythia-160M Pythia-410M Pythia-1B Pythia-2.8B
L2 regularizaiton λ 1e-5
Optimizer AdamW
Adam’s β (0.9, 0.999)
Adam’s ϵ 1e-8
Learning rate 1e-6 1e-6 1e-6 1e-5 1e-5 1e-5
Learning rate schedule Linear decay
Warmup steps 200
Batch size 128
Max training epochs 50 50 50 50 50 50
Pre-trained encoder MiniLM-L6-v2
Max sequence length 512 512 512 512 512 512
Early stopping ✓
Precision fp32
Seed 42
Table 6: Hyperparameters of training our featurized simulator forinstruction tuning on Pythia models of size from
14M to 2.8B. We use the same training hyperparameters as in the loss simulation for the BLEU and ROUGE-L
score simulation on WebNLG and WMT16 DE/EN datasets.
we perform a total of 32 training runs, with each
sample 128 of 200 data points from the original
training set for GPT training. The split of training
runs is divided into 25 for training, 2 for validation,
and 5 for test. All reported results are averaged
over 5 held-out runs. For NLG datasets, we mea-
sure BLEU, ROUGE-L scores besides the test loss,
using a top- psampling strategy for generation with
a temperature setting of 0.2 and a top- pprobability
of 0.95. Note that we collect ROUGE-L scores on a
scale from 0 to 1. The fine-tuning hyperparameters
are shown in Table 7.
GPTfluence Training Setup We train a single
featurized simulator on training runs for each
dataset with the L2-regularized regression objective
as defined in section 3.2. We freeze the parameters
of the pre-trained encoder during training for bet-
ter generalization. We set the order nof Markov
process assumptions equal to 1 for fine-tuning. De-
tailed training hyperparameters are shown in Ta-
ble 8.A.5 Implementing GPTfluence
GPTfluence Training To elucidate the intri-
cate process of collecting training data dynamics
and the training of the featurized simulator with
GPTfluence , we present the pseudo-code in Algo-
rithm 1. The execution of this algorithm yields a
GPTfluence simulator, which is adept at simulat-
ing the target performance trajectory and assessing
the impact of training examples on a given test
point.
GPTfluence Evaluation For evaluation, The
simulator autoregressively forecasts upcoming test-
set metrics, based on the previous nobservations.
Specifically, it commences with the initial test met-
ric recorded at the starting step, thereafter predict-
ing the subsequent performance metrics across the
training curriculum.
B Experiment Results
In this section, we provide additional experimental
results and detailed descriptions to complement theFine-Tuning Hyperparameters SST-2 RTE BoolQ WebNLG WMT16 DE/EN
Optimizer AdamW
Adam β (0.9, 0.999)
Adam ϵ 1e-6
Weight decay 0.001
Learning rate 5e-7 5e-7 5e-7 1e-6 5e-7
Learning rate schedule Linear decay
Warmup steps 0
Batch size 4
Max sequence length 2048
Training epochs 3
Training steps 96
Precision fp32
Table 7: Fine-tuning hyper-parameter settings of GPTDynamcis for various tasks.
Hyperparameters SST-2 RTE BoolQ WebNLG WMT16 DE/EN
L2-regularizaiton’s λ 1e-5
Optimizer AdamW
Adam’s β (0.9, 0.999)
Adam’s ϵ 1e-8
Learning rate 1e-4 1e-4 1e-4 1e-4 1e-4
Learning rate schedule Linear decay
Warmup steps 200
Batch size 128
Max training epochs 300 300 300 300 300
Pre-trained encoder MiniLM-L6-v2 MiniLM-L6-v2 MiniLM-L6-v2 MiniLM-L6-v2 MiniLM-L6-v2
Max sequence length 512 512 512 512 512
Early stopping ✓
Precision fp32
Seed 42
Table 8: Hyperparameters of training our featurized simulator for each dataset for fine-tuning . We use the same
training hyperparameters as in the loss simulation for the BLEU and ROUGE-L score simulation on WebNLG and
WMT16 DE/EN datasets.
Hyperparameters
L2 regularizaiton λ 1e-5
Optimizer AdamW
Adam’s β (0.9, 0.999)
Adam’s ϵ 1e-8
Learning rate 1e-3
Learning rate schedule Linear decay
Warmup steps 200
Batch size 128
Max training epochs 300
Early stopping ✓
Precision fp32
Seed 42
Table 9: Training hyperparameters of Simfluence for
fine-tuning . It is noted that we use the same hyperpa-
rameters for both loss and metric simulation, as we see
that different hyperparameters has little effect on Sim-
fluence’s performance.main findings.
B.1 Empirical Analysis on Markov Property
Table 10 presents a comprehensive results of how
the order of the Markov process influences test loss,
BLEU, and ROUGE-L metrics during instruction
tuning simulations.
B.2 Unseen Data Generalization
We offer in-depth simulations of loss and perfor-
mance metrics across scenarios involving unseen
training data, unseen test data, and both unseen
training and test data. Simulation results for fine-
tuning are detailed in Table 11, while those for
instruction-tuning can be found in Table 12. Illus-
tration examples are shown in the Fig. 8.
C Computational Complexity
We conducted a comparison of inference latency
and floating point operations (FLOPs) among var-Algorithm 1 GPTfluence Training Procedure
Input: Language modeling task P, pre-trained
GPT θ, Target sample z′, Dataset D, Subset size
I, Target metric ϕ, Training dynamic Drun, Multi-
plicative factor function α(·), Additive factor func-
tionβ(·),L2regularization weight, Featurized sim-
ulator Θ, Markov order n-th
Output: Simulator ˆΘ
1:Initialize Drunwith an empty set
2:fork= 1toKdo
3: Sample a subset D′⊂Dof size I
4: forSample batch ct∈D′do
5: Update θtusing Pbased on ct
6: Calculate target metric yk
t=
ϕ(θt, z′)
7: Addctandyk
tintoDrun
8: end for
9:end for
10:Initialize gθwith pre-trained encoder
11:while not converged do
12: Sample a mini-batch Btrain, Btestfrom
Drun
13: foreachzi∈Btrain do
14: Compute multiplicative and additive in-
fluences Ai,1:n,Bi
15: end for
16: α={αj(Btrain)|j= 1,2, ..., n}
17: β=β(Btrain)
18: Update Θwithα,β,γ
19:end while
20:return ˆΘ
ious TDA methods. Results are presented in Ta-
ble 13. TracIn-CP, a representative of gradient-
based methods, exhibited the highest inference la-
tency and FLOPs. This is attributable to the need to
do forward and backward operations directly on the
GPTs. Conversely, GPTfluence solely depends on
a considerably smaller simulator during inference.
Furthermore, we analyzed the convergence and
validation performance of our GPTfluence in com-
parison with Simfluence. As shown in Fig. 9,
GPTfluence exhibits a better convergence effi-
ciency and also has lower validation all-steps MSE.
This underscores the better training efficiency and
model capacity of our featurized simulator.
D Qualitative Examples
In this section, we provide additional quantitative
examples, including loss and metric simulations,
for a comparison. This includes experimental re-Task Order All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
RTE 1 0.036(0.029) 0.151(0.060) 0.746(0.095)
2 0.036(0.029) 0.151(0.060) 0.747(0.094)
3 0.036(0.030) 0.149(0.062) 0.750(0.094)
5 0.037(0.032) 0.147(0.067) 0.757(0.093)
10 0.036(0.032) 0.150(0.071) 0.785(0.088)
Task Order All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
WEBNLG 1 0.011(0.014) 0.078(0.043) 0.985(0.002)
2 0.010(0.011) 0.072(0.039) 0.986(0.002)
3 0.011(0.012) 0.073(0.040) 0.986(0.002)
5 0.012(0.014) 0.082(0.044) 0.983(0.003)
10 0.012(0.014) 0.082(0.044) 0.983(0.002)
Task OrderBLEU
All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
WEBNLG 1 43.98(81.40) 4.28(3.57) 0.80(0.01)
2 43.31(80.70) 4.24(3.54) 0.80(0.03)
3 43.67(81.77) 4.24(3.57) 0.80(0.02)
5 44.79(76.57) 4.39(3.49) 0.78(0.02)
10 47.83(99.06) 4.35(3.72) 0.74(0.03)
Task OrderROUGE-L
All-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
WEBNLG 1 0.01(0.01) 0.06(0.03) 0.77(0.04)
2 0.01(0.01) 0.06(0.03) 0.76(0.04)
3 0.01(0.01) 0.06(0.03) 0.76(0.03)
5 0.01(0.01) 0.06(0.03) 0.77(0.04)
10 0.01(0.01) 0.06(0.03) 0.78(0.03)
Table 10: Impact of the Markov process order on test
loss, BLEU, and ROUGE-L metrics in instruction tun-
ingsimulations.
sults across various training scenarios and the use
of unseen data, among others.
D.1 Simulation For Instruction-Tuning
We provide additional qualitative examples for
instruction-tuning simulations, highlighting test
loss and performance metrics:
•Simulation of test loss for Pythia-410M is
shown in Fig. 10.
•Simulation of test loss for Pythia-1B is de-
picted in Fig. 11.
•BLEU metric simulation for Pythia-410M can
be found in Fig. 12.
•BLEU metric simulation for Pythia-1B is il-
lustrated in Fig. 13.
•ROUGE-L metric simulation with Pythia-
410M is presented in Fig. 14.
•ROUGE-L metric simulation with Pythia-1B
is detailed in Fig. 15.
D.2 Simulation For Fine-Tuning
We provide additional qualitative examples show-
casing simulations of test loss and performance
metrics for fine-tuning, as follows:
• For test loss simulation, see Fig. 16.
•For BLEU metric simulation, refer to Fig. 17.
•For ROUGE-L metric simulation, see Fig. 18.0 20 40 60 80
Training Steps0.51.01.52.02.5LossGround Truth
Ours(a) RTE
0 20 40 60 80
Training Steps1020304050BLEUGround Truth
Ours
(b) WebNLG
Figure 8: Illustration of simulation results on unseen
training data . The topshows the loss simulation on
RTE, while the bottom shows the BLEU metric simula-
tion for WebNLG. Additional qualitative examples for
different settings and metrics are provided in § D.3.
Method Latency
(sec/sample)FLOPs
TracIn-CP 153.0 1.1 ×1013
Simfluence 0.1 1.6 ×101
Ours 0.2 5.3 ×106
Table 13: Inference latency and FLOPs of GPTfluence,
Simfluence, and TracIn-CP.
0.0 0.2 0.4 0.6 0.8 1.0
Step(M)103
102
101
100101102103Training LossSimfluence
Ours
0 5 10 15 20 25 30
Epoch1001021041061081010Validation All-Steps MSESimfluence
Ours
Figure 9: Comparison of our method and Simfluence
with respect to training loss (Left) and validation all-
steps MSE (Right).
D.3 Simuation with Unseen Data
We provide detailed simulations of test loss and
performance metrics across different tasks and sce-
narios, as detailed below:
•For the RTE task, test loss simulations under
various conditions are presented in Fig. 19
(unseen test data), Fig. 20 (unseen training
data), and Fig. 21 (unseen training and test
data).
•For the WebNLG task, test loss simulationsTask Metrics Training Data
UnseenTest Data
UnseenAll-steps MSE All-steps
MAEFinal-Step
Spearman’s ρ
RTE Loss✓ ✗ 0.346(0.281) 0.513(0.211) 0.913(0.052)
✗ ✓ 0.351(0.489) 0.444(0.325) -0.024(0.050)
✓ ✓ 0.984(4.569) 0.568(0.728) -0.048(0.045)
WEBNLGLoss✓ ✗ 1.251(0.962) 1.003(0.413) 0.892(0.011)
✗ ✓ 0.403(0.575) 0.476(0.476) 0.123(0.019)
✓ ✓ 0.886(2.112) 0.699(0.549) 0.190(0.013)
BLEU✓ ✗ 94.99(273.96) 6.13(5.72) 0.51(0.02)
✗ ✓ 106.19(150.51) 7.14(5.02) 0.18(0.08)
✓ ✓ 153.63(219.29) 8.66(6.39) 0.15(0.01)
ROUGE-L✓ ✗ 0.008(0.009) 0.069(0.036) 0.578(0.062)
✗ ✓ 0.009(0.008) 0.073(0.034) 0.288(0.049)
✓ ✓ 0.010(0.010) 0.075(0.039) 0.168(0.091)
Table 11: Results of loss and metric simulation on
unseen data for RTE and WebNLG Datasets for fine-
tuning .
Task Metrics Training Set
OODTest Set
OODAll-steps
MSE (↓)All-steps
MAE ( ↓)Final-step Spear-
man’s ρ(↑)
RTE Loss✓ ✗ 0.781(0.793) 0.730(0.419) -0.082(0.214)
✗ ✓ 1.137(2.927) 0.725(0.619) -0.011(0.033)
✓ ✓ 1.110(1.057) 0.888(0.482) -0.047(0.062)
WEBNLGLoss✓ ✗ 2.398(1.722) 1.435(0.508) 0.358(0.006)
✗ ✓ 22.627(203.637) 1.530(3.062) 0.247(0.008)
✓ ✓ 2.708(1.415) 1.580(0.432) 0.072(0.003)
BLEU✓ ✗ 200.50(270.92) 10.82(7.08) 0.34(0.06)
✗ ✓ 115.19(188.30) 7.33(5.69) -0.03(0.03)
✓ ✓ 329.61(369.14) 14.20(8.11) 0.10(0.04)
ROUGE-L✓ ✗ 0.12(0.06) 0.33(0.08) 0.35(0.02)
✗ ✓ 0.01(0.02) 0.09(0.05) 0.06(0.06)
✓ ✓ 0.13(0.05) 0.35(0.06) 0.10(0.01)
Table 12: Results of loss and metric simulation on un-
seen data for RTE and WebNLG Datasets for instruction
tuning .
are shown in Fig. 22 (unseen test data), Fig. 23
(unseen training data), and Fig. 24 (unseen
training and test data).
•BLEU metric simulations for the WebNLG
task are illustrated in Fig. 25 (unseen test data),
Fig. 26 (unseen training data), and Fig. 27
(unseen training and test data).
•ROUGE-L metric simulations for the
WebNLG task are depicted in Fig. 28 (unseen
test data), Fig. 29 (unseen training data) and
Fig. 30 (unseen training and test data).0 50 100 150 200 250 300
Training Steps0246810Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps4
2
0246810Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps4
2
02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot(a) BoolQ
0 50 100 150 200 250 300
Training Steps2
02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps2
02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps2
02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(b) RTE
0 50 100 150 200 250 300
Training Steps2
0246Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps2
0246Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps02468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(c) SST-2
0 50 100 150 200 250 300
Training Steps0.751.001.251.501.752.002.252.50Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps0.81.01.21.41.61.82.0Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps1.251.501.752.002.252.502.75Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(d) WebNLG
0 50 100 150 200 250 300
Training Steps0.60.81.01.21.41.6Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps0.81.01.21.41.61.82.02.2Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 50 100 150 200 250 300
Training Steps0.40.60.81.01.2Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(e) WMT16 DE/EN
Figure 10: Comparative loss simulations for instruction tuning using GPTfluence versus other TDA methods on
Pythia-410M across the BoolQ, RTE, SST-2, WebNLG, and WMT16 DE/EN datasets.0 25 50 75 100 125 150 175 200
Training Steps246810Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps024681012Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps0246810Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot(a) BoolQ
0 25 50 75 100 125 150 175 200
Training Steps2468Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps012345678Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps012345678Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(b) RTE
0 25 50 75 100 125 150 175 200
Training Steps1
012345Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps01234567Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps01234567Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(c) SST-2
0 25 50 75 100 125 150 175 200
Training Steps0.60.81.01.21.41.61.8Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps1.52.02.53.03.54.0Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps1.01.21.41.61.82.02.22.4Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(d) WebNLG
0 25 50 75 100 125 150 175 200
Training Steps2.53.03.54.04.55.0Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps0.40.50.60.70.80.91.0Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
0 25 50 75 100 125 150 175 200
Training Steps0.81.01.21.41.61.82.02.22.4Loss
Ground Truth
Ours
Simfluence
TracInCP
Grad-Dot
(e) WMT16 DE/EN
Figure 11: Loss simulation comparisons between GPTfluence and alternative TDA methods for instruction tuning
on Pythia-1B, across the BoolQ, RTE, SST-2, WebNLG, and WMT16 DE/EN datasets.0 50 100 150 200 250 300
Training Steps24681012BLEU
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps5101520253035BLEU
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps0123456BLEU
Ground Truth
Ours
Simfluence(a) WebNLG
0 20 40 60 80
Training Steps0246810BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps051015202530BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps02468BLEU
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 12: BLEU metric simulation comparisons for instruction tuning using GPTfluence versus Simfluence on
Pythia-410M, across the WebNLG and WMT16 DE/EN datasets.
0 25 50 75 100 125 150 175 200
Training Steps4681012141618BLEU
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps510152025BLEU
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps24681012141618BLEU
Ground Truth
Ours
Simfluence
(a) WebNLG
0 25 50 75 100 125 150 175 200
Training Steps510152025BLEU
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps012345678BLEU
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps10121416BLEU
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 13: Test examples of the BLEU simulation of GPTfluence and Simfluence for instruction tuning with
Pythia-1B on WebNLG and WMT16 DE/EN datasets.0 50 100 150 200 250 300
Training Steps0.100.150.200.250.30Rouge-L
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps0.10.20.30.40.50.60.7Rouge-L
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps0.140.150.160.170.180.19Rouge-L
Ground Truth
Ours
Simfluence(a) WebNLG
0 50 100 150 200 250 300
Training Steps0.000.020.040.060.080.100.120.14Rouge-L
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps0.000.050.100.150.200.250.300.35Rouge-L
Ground Truth
Ours
Simfluence
0 50 100 150 200 250 300
Training Steps0.0000.0250.0500.0750.1000.1250.1500.175Rouge-L
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 14: Test examples of the ROUGE-L simulation of GPTfluence and Simfluence for instruction tuning with
Pythia-410M on WebNLG and WMT16 DE/EN datasets.
0 25 50 75 100 125 150 175 200
Training Steps0.2750.3000.3250.3500.3750.4000.4250.4500.475Rouge-L
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps0.1000.1250.1500.1750.2000.2250.2500.2750.300Rouge-L
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps0.050.100.150.200.250.30Rouge-L
Ground Truth
Ours
Simfluence
(a) WebNLG
0 25 50 75 100 125 150 175 200
Training Steps0.10.20.30.40.50.60.70.8Rouge-L
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps0.150.200.250.300.350.400.450.500.55Rouge-L
Ground Truth
Ours
Simfluence
0 25 50 75 100 125 150 175 200
Training Steps0.000.050.100.150.200.25Rouge-L
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 15: Test examples of the ROUGE-L simulation of GPTfluence and Simfluence for instruction tuning with
Pythia-1B on WebNLG and WMT16 DE/EN datasets.0 20 40 60 80
Training Steps0.51.01.52.02.53.0Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.250.500.751.001.251.501.752.002.25Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.51.01.52.02.53.0Loss
Ground Truth
Ours
Simfluence(a) BoolQ
0 20 40 60 80
Training Steps1.52.02.53.03.5Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.51.01.52.02.5Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.51.01.52.02.53.0Loss
Ground Truth
Ours
Simfluence
(b) RTE
0 20 40 60 80
Training Steps1.01.52.02.53.03.5Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.51.01.52.02.53.03.54.0Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps1.01.52.02.53.03.5Loss
Ground Truth
Ours
Simfluence
(c) SST-2
0 20 40 60 80
Training Steps1.251.501.752.002.252.502.753.00Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps2.02.22.42.62.83.0Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps1.41.61.82.02.22.42.6Loss
Ground Truth
Ours
Simfluence
(d) WebNLG
0 20 40 60 80
Training Steps1.501.752.002.252.502.753.003.25Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps1.71.81.92.02.12.22.3Loss
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps1.31.41.51.61.71.81.92.0Loss
Ground Truth
Ours
Simfluence
(e) WMT16 DE/EN
Figure 16: Loss simulation comparisons of GPTfluence versus Simfluence for fine-tuning on Pythia-410M across
BoolQ, RTE, SST-2, WebNLG, and WMT16 DE/EN datasets.0 20 40 60 80
Training Steps468101214BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps246810BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps234567BLEU
Ground Truth
Ours
Simfluence(a) WebNLG
0 20 40 60 80
Training Steps0246810BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps051015202530BLEU
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps02468BLEU
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 17: BLEU metric simulation comparison for fine-tuning Pythia-410M using GPTfluence and Simfluence on
the WebNLG and WMT16 DE/EN tasks.
0 20 40 60 80
Training Steps0.2000.2250.2500.2750.3000.3250.3500.375Rouge-L
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.200.250.300.350.400.450.50Rouge-L
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.2750.3000.3250.3500.3750.4000.4250.4500.475Rouge-L
Ground Truth
Ours
Simfluence
(a) WebNLG
0 20 40 60 80
Training Steps0.00.10.20.30.40.5Rouge-L
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.01
0.000.010.020.030.040.05Rouge-L
Ground Truth
Ours
Simfluence
0 20 40 60 80
Training Steps0.01
0.000.010.020.030.040.05Rouge-L
Ground Truth
Ours
Simfluence
(b) WMT16 DE/EN
Figure 18: ROUGE-L metric simulation comparison for fine-tuning Pythia-410M with GPTfluence and Simfluence
on the WebNLG and WMT16 DE/EN tasks.0 20 40 60 80
Training Steps0.20.40.60.81.01.21.41.61.8LossGround Truth
Ours(a) Test Example1
0 20 40 60 80
Training Steps0.751.001.251.501.752.002.252.50LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.51.01.52.02.5LossGround Truth
Ours (c) Test Example3
Figure 19: Examples of loss simulation of GPTfluence for the RTE task on unseen test data .
0 20 40 60 80
Training Steps0.51.01.52.02.53.0LossGround Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps0.51.01.52.02.5LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps1.01.52.02.53.03.5LossGround Truth
Ours (c) Test Example3
Figure 20: Examples of loss simulation of GPTfluence for the RTE task on unseen training data .
0 20 40 60 80
Training Steps1.01.52.02.53.03.54.0LossGround Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps1.01.52.02.53.03.54.0LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.51.01.52.02.5LossGround Truth
Ours (c) Test Example3
Figure 21: Examples of loss simulation of GPTfluence for the RTE task on unseen training and test data .
0 20 40 60 80
Training Steps1.31.41.51.61.71.8LossGround Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps1.41.51.61.71.8LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps1.51.61.71.81.92.02.12.2LossGround Truth
Ours (c) Test Example3
Figure 22: Examples of loss simulation of GPTfluence for the WebNLG task on unseen test data .0 20 40 60 80
Training Steps1.01.52.02.53.0LossGround Truth
Ours(a) Test Example1
0 20 40 60 80
Training Steps1.01.52.02.53.0LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.51.01.52.02.53.03.54.0LossGround Truth
Ours (c) Test Example3
Figure 23: Examples of loss simulation of GPTfluence for the WebNLG task on unseen training data .
0 20 40 60 80
Training Steps1.21.41.61.82.02.2LossGround Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps2.22.42.62.83.03.2LossGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps1.21.41.61.82.02.2LossGround Truth
Ours (c) Test Example3
Figure 24: Examples of loss simulation of GPTfluence for the WebNLG task on unseen training and test data .
0 20 40 60 80
Training Steps2.55.07.510.012.515.017.520.0BLEU
Ground Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps246810BLEU
Ground Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps510152025BLEU
Ground Truth
Ours (c) Test Example3
Figure 25: Examples of BLEU simulation of GPTfluence for the WebNLG task on unseen test data .
0 20 40 60 80
Training Steps2.55.07.510.012.515.017.520.0BLEU
Ground Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps10203040BLEU
Ground Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps1020304050BLEUGround Truth
Ours (c) Test Example3
Figure 26: Examples of BLEU simulation of GPTfluence for the WebNLG task on unseen training data .0 20 40 60 80
Training Steps24681012BLEU
Ground Truth
Ours(a) Test Example1
0 20 40 60 80
Training Steps246810BLEU
Ground Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps51015202530BLEU
Ground Truth
Ours (c) Test Example3
Figure 27: Examples of BLEU simulation of GPTfluence for the WebNLG task on unseen training and test data .
0 20 40 60 80
Training Steps0.240.260.280.300.320.340.360.380.40Rouge-L
Ground Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps0.240.260.280.300.320.340.360.380.40Rouge-L
Ground Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.150.200.250.300.350.400.45Rouge-L
Ground Truth
Ours (c) Test Example3
Figure 28: Examples of the ROUGE-L simulation of GPTfluence for the WebNLG task on unseen test data .
0 20 40 60 80
Training Steps0.250.300.350.400.45Rouge-L
Ground Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps0.280.300.320.340.360.380.400.42Rouge-LGround Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.2750.3000.3250.3500.3750.4000.4250.4500.475Rouge-L
Ground Truth
Ours (c) Test Example3
Figure 29: Examples of the ROUGE-L simulation of GPTfluence for the WebNLG task on unseen training data .
0 20 40 60 80
Training Steps0.100.150.200.250.300.350.400.45Rouge-L
Ground Truth
Ours
(a) Test Example1
0 20 40 60 80
Training Steps0.150.200.250.300.350.400.45Rouge-L
Ground Truth
Ours (b) Test Example2
0 20 40 60 80
Training Steps0.150.200.250.300.350.400.45Rouge-L
Ground Truth
Ours (c) Test Example3
Figure 30: Examples of the ROUGE-L simulation of GPTfluence for the WebNLG task on unseen training and
test data .