Unlocking the Future: Exploring Look-Ahead Planning Mechanistic
Interpretability in Large Language Models
Tianyi Men1,2, Pengfei Cao1,2, Zhuoran Jin1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2
1The Laboratory of Cognition and Decision Intelligence for Complex Systems,
Institute of Automation, Chinese Academy of Sciences, Beijing, China
2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
{tianyi.men, pengfei.cao, zhuoran.jin, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn
Abstract
Planning, as the core module of agents, is cru-
cial in various fields such as embodied agents,
web navigation, and tool using. With the de-
velopment of large language models (LLMs),
some researchers treat large language models
as intelligent agents to stimulate and evaluate
their planning capabilities. However, the plan-
ning mechanism is still unclear. In this work,
we focus on exploring the look-ahead planning
mechanism in large language models from the
perspectives of information flow and internal
representations. First, we study how planning
is done internally by analyzing the multi-layer
perception (MLP) and multi-head self-attention
(MHSA) components at the last token. We find
that the output of MHSA in the middle layers at
the last token can directly decode the decision
to some extent. Based on this discovery, we fur-
ther trace the source of MHSA by information
flow, and we reveal that MHSA mainly extracts
information from spans of the goal states and
recent steps. According to information flow, we
continue to study what information is encoded
within it. Specifically, we explore whether fu-
ture decisions have been encoded in advance in
the representation of flow. We demonstrate that
the middle and upper layers encode a few short-
term future decisions to some extent when plan-
ning is successful. Overall, our research ana-
lyzes the look-ahead planning mechanisms of
LLMs, facilitating future research on LLMs
performing planning tasks.
1 Introduction
Planning is the process of formulating a series of
actions to transform a given initial state into a de-
sired goal state (Valmeekam et al., 2024; Zhang
et al., 2024). As the core module of agents (Xi
et al., 2023; Wang et al., 2024a), planning has been
widely applied in many fields such as embodied
agents (Shridhar et al., 2020; Wang et al., 2022),
web navigation (Zhou et al., 2023; Deng et al.,
2024) and tool using (Xu et al., 2023; Qin et al.,
Step 3 : pick up red
Step 4 : put on yellowyellow
tableyellow
blueInit StateGoal StateI can only foresee the next 
one step  of action. Given the 
goal state and init state, I'm 
only thinking of the next one 
step between picking yellow 
and blue, only step 1 color is 
in my mind . I pick up 
yellow. After finishing step 1,  
I think about step 2 action.I can foresee the next several 
steps of actions. Given the 
goal state and init state, I'm 
thinking of the next several 
steps,  step 1,2,3,4 color is in 
my mind . I pick up blue. After 
finishing step 1, I think about 
step 2,3,4 actions.
S11 S12
S21 S22 S23 S24
Look -AheadStep 1 : pick up yellow
Step 2 : put on blue
green
yellow
blue
red
GreedyFigure 1: An example of greedy and look-ahead plan-
ning.
2023). With the development of large language
models, some researchers treat large language mod-
els as intelligent agents to solve complex tasks.
This is because large language models may pos-
sess some preliminary planning capabilities (Huang
et al., 2022). Recently, researchers have made ef-
forts to stimulate and evaluate the planning capa-
bilities of large language models. They propose
prompt engineering (Zheng et al., 2023) and in-
struction fine-tuning (Zeng et al., 2023) to boost
the planning abilities of large language models.
Additionally, some researchers construct bench-
marks such as AgentBench (Liu et al., 2023) and
AgentGym (Xi et al., 2024) to evaluate the planning
capabilities of large models. Although they have
made some progress, the underlying mechanisms
in planning capabilities of large language models
remain a largely unexplored frontier. Revealing
the planning mechanisms of large language models
helps to better understand and improve their plan-
1arXiv:2406.16033v1  [cs.CL]  23 Jun 2024ning capabilities. Therefore, we focus on exploring
the underlying mechanisms behind the planning
abilities of large language models.
In this work, we focus on exploring look-ahead
planning mechanisms in large language models.
We study the classical planning task Blocksworld,
which is a fully-observed setting. All entity states
are known from the init state and goal state, so
exploration is not needed (Zhang et al., 2024). As
illustrated in Figure 1, given an initial state and
a goal state of Blocksworld, the model can only
pick up or put down one block. The model must
generate a sequence of actions to transform the
initial state into the goal state, as shown by the
green path. However, it is still unclear whether the
model, at step t, greedily considers only the action
att+ 1or look-ahead considers the actions at t+ 2
and beyond. Inspired by psychology, humans en-
gage in look-ahead thinking when making plans
(Baumeister et al., 2016). Based on this, we fur-
ther propose the hypothesis of model look-ahead
planning, which is as follows:
•Look-Ahead Planning Decisions Existence
Hypothesis : In the task of planning with large
language models, given a rule, an initial state,
a goal state, and task description prompts. At
the current step, the model needs to predict
the next action, the probe can detect decisions
to some extent for future steps in the internal
representations in the short term within a fully-
observed setting when planning is successful.
We design a two-stage paradigm to verify this
hypothesis. It can be divided into the finding in-
formation flow stage and the probing internal rep-
resentations stage. The first stage is to analyze
the information flow and component functions dur-
ing planning (§5). The second stage is examining
whether the model stores future information in in-
ternal representations (§6). The specifics are as
follows:
(1) In the first stage, we study how planning is
done internally by analyzing the MLP and MHSA
components at the last token. Inspired by meth-
ods of calculating extraction rates methods (Geva
et al., 2023), we find the output of MHSA in the
middle layers at the last token can directly decode
the correct colors to some extent (§5.1). Based on
this discovery, we further investigate the sources of
information on MHSA. We trace the source of the
decisions. And find that planning mainly depends
on spans of the goal states and recent steps (§5.2).(2) In the second stage, we study what informa-
tion is encoded in the information flow and whether
this information has been considered in advance for
future decisions. For future decisions existence, we
use the probing method to probe future decisions
and reveal that the middle and upper layers encode
a few short-term future decisions when planning
is successful (§6.1). For history step causality, we
prevent the information flow from history steps and
explore the impact of different history steps on the
final decision (§6.2).
In summary, our contributions are as follows:
•To the best of our knowledge, this work is the
first to investigate the planning interpretability
mechanisms in large language models. We
demonstrate the Look-Ahead Planning Deci-
sions Existence Hypothesis .
•We reveal that the internal representations of
LLMs encode a few short-term future deci-
sions to some extent when planning is success-
ful. These look-ahead decisions are enriched
in the early layers, with accuracy decreasing
as planning steps increase.
•We prove that MHSA mainly extracts informa-
tion from spans of the goal states and recent
steps. The output of MHSA in the middle lay-
ers at the last token can directly decode the
correct decisions partially in planning tasks.
2 Experimental Setup
In this paper, we study the Blocksworld task in a
fully-observed setting where all entity states are
known from the init state and goal state, so explo-
ration is not needed (Zhang et al., 2024). Given a
ruleR, an initial state Sinit, a goal state Sgoal, task
description prompts C, the current step t, history
a1. . . a t, model needs to predict the next action
at+1in accordance with its generative distribution
p(at+1|R, S init, Sgoal, C, a 1. . . a t)(Hao et al.,
2023). In this paper, all inputs are in text form. All
inferences are performed using the teacher-forcing
method. Previous evaluation works (Valmeekam
et al., 2023) mainly involved generating a complete
plan and then placing it into the environment for as-
sessment. However, since our primary focus is on
open-source models, we have reduced the difficulty
by using a fill-in-the-blank format for evaluating
the models. An example is shown in Figure 2. The
input mainly consists of four parts: rule, initial
state, goal state, and plan.
2Data Previous Blocksworld evaluation bench-
marks (Valmeekam et al., 2023) put the plans gen-
erated by models into an environment to verify
the correctness. However, existing interpretability
methods, such as information flow (Wang et al.,
2023), require gold labels. Therefore, we synthe-
size a dataset containing optimal plans, with spe-
cific data statistics shown in Table 1. We generate
data with 4, 5, and 6 color varieties, 4 piles, and a
maximum of 6 steps, where pick-up and stack are
considered as two different steps. There are three
levels: LEVEL1 (L1) with two steps, LEVEL2 (L2)
with four steps, and LEVEL3 (L3) with six steps.
We choose the optimal path from the initial step to
the final step. For samples with multiple optimal
paths, we select one to include in the training set,
ensuring that samples in the test set have unique
optimal paths. We split the dataset into training and
test sets with a ratio of 1:3.
Metric In the Blocksworld task, we use two met-
rics: single-step success rate and complete plan
success rate. The single-step success rate evaluates
whether each individual action is correct, defined
as:
Sstep=1
NNX
i=1ri (1)
where Nis the total number of steps and riindi-
cates the success of the i-th step (1 for success, 0
otherwise). The complete plan success rate evalu-
ates whether the entire planning process is correct,
defined as:
Splan=1
MMX
j=1Rj (2)
where Mis the total number of tested plans and
Rjindicates the success of the j-th plan (1 for
complete success, 0 otherwise).
Model We evaluate two large language models:
Llama-2-7b-chat (Touvron et al., 2023) and Vicuna-
7B (Chiang et al., 2023). Since open-source models
have preliminary planning capabilities, we enhance
the ability of large language models to complete
planning tasks through training.
Experiment Setting We conduct full parameter
fine-tuning on Llama-2-7b-chat-hf and Vicuna-7B
for 3 epochs. The training process involves a global
batch size of 20, using the Adam optimizer with
a learning rate of 5e-5. Llama-2-7b-chat-hf and
Vicuna-7B achieve complete plan success rates of
Rule: 
You can pick -up color1. stack color1 on -top-of color2. 
All the blocks are on the table. There is no order in 
the piles. Please output the optimal plan. 
Init state: 
<empty>
<black>
<white on gray on red on blue>
<green>
Goal state: 
<gray on black>
<red on blue>
<green>
<white> 
Plan: 
step 1: pick-up ____ (answer: white ) 
step 2: stack white on -top-of ____ (answer: table ) 
step 3: pick-up ____ (answer: gray ) 
step 4: stack gray on -top-of ____ (answer: black )Figure 2: An example of Blocksworld.
61% and 63%, respectively, at LEVEL 3 with 6
blocks. We sample 400 correct data points from
LEVEL 3 with 6 blocks for our analysis. We con-
duct experiment based on HuggingFace’s Trans-
formers1, PyTorch2, baukit3and pyvene4(Wu
et al., 2024b).
3 Background
A transformer-based language model begins by
converting an input text into a sequence of N
tokens, denoted as s1, . . . , s N. Each token siis
mapped to a vector x0
i∈Rd.E∈R|V|×dis the
decoder matrix in the last layer, where Vis the
vocabulary, dis embedding dimension. Each layer
comprises a multi-head self-attention (MHSA) sub-
layer followed by a multi-layer perception (MLP)
sublayer (Vaswani et al., 2017). Formally, the rep-
resentation xℓ
iof token iat layer ℓcan be obtained
as follows:
xℓ
i=xℓ−1
i+attnℓ
i+mℓ
i (3)
aℓ
iandmℓ
irepresent the outputs of the MHSA
and MLP sub-layers of the ℓ-th layer, respectively.
By using E, an output probability distribution can
be obtained from the final layer representation:
pL
i=softmax (ExL
i) (4)
1https://github.com/huggingface/transformers/
2https://github.com/pytorch/pytorch/
3https://github.com/davidbau/baukit/
4https://github.com/stanfordnlp/pyvene
3/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000056/uni00000030/uni0000002f/uni00000033
/uni00000030/uni0000002b/uni00000036/uni00000024
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055Figure 3: Extraction rate of different components in
Llama-2-7b-chat-hf.
4 Overview of Analysis
We analyze the look-ahead planning mechanisms
of the models from two stages. (1) In the first
stage, we explore the internal mechanisms of this
process in planning tasks from the perspectives of
information flow and component functions. We
demonstrate that the middle layer MHSA can di-
rectly decode the answers to a certain extent, and
we prove that MHSA mainly extracts information
from spans of the goal states and recent steps (§5).
(2) In the second stage, to determine the presence
of future decisions, we employ the probing method
to examine future decisions, uncovering that the
intermediate and upper layers encode these deci-
sions. Regarding the causality of historical steps,
we inhibit the information flow from past steps and
analyze the effects of different historical steps on
the ultimate decision (§6).
5 Information Flow in Planning Tasks
To trace the source of the correct answer, we be-
gin with the last token. For example, in the first
step "pick up", the last token is "up". The model
should process the initial state, target state, and
history of steps to decide which color to pick up,
such as "blue". We analyze this process from two
perspectives. (1) First, we study MLP and MHSA
functions at the last token by extraction rates (Geva
et al., 2023). We find that the output of MHSA in
the middle layers can directly decode the correct
colors to a certain extent (§5.1). (2) Based on this,
we further trace the source of the correct colors by
information flow (Wang et al., 2023). From the
perspective of early and late planning stages, we
prove that MHSA mainly extracts information from
spans of the goal states and recent steps (§5.2).
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000056/uni00000030/uni0000002f/uni00000033
/uni00000030/uni0000002b/uni00000036/uni00000024
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055Figure 4: Extraction rate of different components in
Vicuna-7b.
5.1 Attention Extract the Answers
From the perspective of the model’s internal com-
ponents, we analyze the functions of different com-
ponents of the models. The first question is how
the model extracts answers from history. We start
from the position of the last token and study the
roles of the MLP and MHSA components in the
answer generation process. Specifically, we inves-
tigate whether different components at different
layers can directly decode the final answer.
Experiments We use the extraction rate (Geva
et al., 2023) to analyze the functions of different
components. Specifically, we calculate the extrac-
tion rate:
e∗:=argmax 
pL
N
(5)
be:=argmax
Ehℓ
N
(6)
In this equation, hrepresents the internal repre-
sentation of the MLP, MHSA and layer output, N
is the position of the last token, ℓis the layer of
models, ℓ∈[1, L]. When e∗=be, it is considered
as an extraction event. We calculate the extraction
rate of the last token for each layer for each step in
the Blocksworld. We then compute the mean and
variance of these rates.
Results and Analysis As shown in Figure 3 and
Figure 4, we observe that (1) MHSA has a higher
extraction rate compared to MLP, indicating that
attention is primarily responsible for answer ex-
traction. (2) Layer output gradually forms a stable
answer in the middle to upper layers (from the 15th
layer to the 20th layer). In these layers, the ex-
traction rate of MHSA is significantly higher than
MLP, suggesting that MHSA plays a major role
during the decision-making period. (3) The vari-
ance in extraction rates across different steps is
4/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000017
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000018
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000017
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000018
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019
/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 5: Information flow of last token in Llama-2-7b-chat-hf.
smaller for MHSA compared to MLP, indicating
that MHSA layers show higher consistency across
different steps.
5.2 Attention Extract from Goal and History
In the previous section, we discover that MHSA is
responsible for extracting answers from the context,
but which chunk to extract the answer from is still
unclear. In this section, we decompose the input
into several chunks to identify which chunk MHSA
primarily relies on. We use the information flow
method (Wang et al., 2023), first calculating the
information flow at the token granularity, and then
taking the average of different tokens within the
same chunk to represent the information flow at
the chunk granularity. This will help us locate the
influence of different chunks on the last token.
Experiments We calculate the information flow
between layers. Specifically, for the input, we di-
vide it into different chunks, including init token
(which is "Init:"), init state (which is "<blue on
red>"), target token, target state, six history steps
(For step 1, which is "step 1: pick-up white"), ac-
tion prompt (pick-up or stack on-top-of) and last
token. We calculate the information flow Itoken,ℓ
for each token at the ℓ−thlayer. The specific
calculation method is as follows:
Itoken,ℓ =X
hdAhd,l⊙∂L(x)
∂Ah,ℓ(7)
Where Ahd,lis the attention score of the ℓ-thlayer, hdis the hd-th head, and L(x)is the loss
function. Here, we use I(i, j)to represent the
score flowing from token j to token i. Based on
the token information flow, we calculate the chunk
information flow, denoted as Ichunk,ℓ :
Ichunk,ℓ =Pk2
i=k1Pt2
j=t1Itoken,ℓ (i, j)
(k2−k1+ 1)( t2−t1+ 1)(8)
Specifically, we consider the information flow from
the span [k1, k2]of a chunk kto the span [t1, t2]
of another chunk t. We calculate the average of
information flow from chunk ktot. Due to the
causal attention, we only compute the information
flow for the lower triangular matrix. We calculate
the chunk information flow for each prediction step.
Results and Analysis The results are shown in
Figure 5 and Figure 12. The vertical axis repre-
sents the information flow from the chunk to the
last token. The horizontal axis represents the infor-
mation flow at layer ℓ. The values inside represent
the scores of information flow. We calculate the
information flow for six decision steps. It shows
that: (1) In steps 1 to 6, the goal states are high-
lighted at each step. This indicates that MHSA
extracts information from the goal state at each
stage, demonstrating that it mainly relies on goal
states. (2) Taking the step 5 as an example, history
3 and history 4 are more prominent compared to
history 1 and history 2. It reveals that MHSA also
mainly relies on recent history rather than earlier
spans of steps.
56 Internal Representations Encode
Planning Information
Based on the previous sections, we discover that
MHSA directly extracts answers from the context,
but it is still unclear what information is encoded in
internal representations. In this section, we demon-
strate the look-ahead capability of models from
both future decisions existence and history step
causality perspectives. (1) For future decisions ex-
istence, we use the probing method to probe each
layer of the main positions in the context. We find
that the accuracy of the current state information
gradually decreases as the steps progress. We also
find that the middle and upper layers encode fu-
ture decisions with accuracy decreasing as planning
steps increase, proving the look-ahead planning hy-
pothesis (§6.1). (2) For history step causality, we
employ a method that involves setting certain in-
formation keys of MHSA to zero. We find there is
still a probability of generating the correct answer
by relying solely on a single step, but it’s difficult
to support plan for the long-term (§6.2).
6.1 Internal Representations Encode Block
States and Future Decisions
In this section, we analyze what information is
encoded in the internal representations within the
information flow and how this information evolves
layer by layer. We examine whether the internal
representations encode two types of information
(Li et al., 2022; Pal et al., 2023): Current Block
States andFuture Decisions .Current Block States
refer to the state of the blocks at step t. For exam-
ple, in Figure 1, when following the green path, the
Current Block State initially starts in the Sinit. After
executing the first and second steps, the internal
representation of the Current Block State transi-
tions from the SinittoS12.Future Decisions refer
to the information about future decisions at step t.
For example, in Figure 1, when following the green
path and executing the first step (blue), the ques-
tion is whether the model’s internal representation
already stores information about future decisions
(red, yellow, blue).
Experiments We probe internal representations
of the initial state, goal state, and steps with
layer ℓ∈[1, L]. We train linear probes and
nonlinear probes for each chunk and each layer.
A linear probe can be represented as pθ(xℓ
n) =
softmax (Wxℓ
n). And a nonlinear probe can be de-
scribed as pθ(xℓ
n) = softmax (W1ReLU (W2xℓ
n)).
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000014
/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000014
/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni0000004c/uni00000051/uni0000004c/uni00000057
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055Figure 6: State probe in Llama-2-7b-chat-hf.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000014
/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000014
/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni0000004c/uni00000051/uni0000004c/uni00000057
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055
Figure 7: State probe in Vicuna-7b.
Using the linear probe as an example, we consider
six steps and six colors of blocks. For Current
Block States , the input to the probe is a hidden
layer representation hof the model. The output is
a 12x8 matrix representing probabilities, where 12
denotes the colors of the blocks above and below
each color block, and 8 represents 6 colors plus
sky and table. For Future Decisions , the input to
the probe is h. The output is six predicted colors
from steps 1 to 6, we only consider future steps in
our evaluation. We split the training and test sets
in a 4:1 ratio for 400 samples. For the evaluation,
we calculate the weighted F1 accuracy for Current
Block States and accuracy for Future Decisions .
Results and Analysis As shown in Figure 6 and
Figure 7, the horizontal axis represents the layers
probed, while the vertical axis represents the mean
accuracy of the probe test. Different colored lines
represent the probed spans of states and steps. (1)
We observe that as the number of layers increases,
the accuracy of the probe gradually improves. This
indicates that the early layers of the model are en-
riching the representation of the current state. (2)
The black line (step 6) in the figure has a lower
accuracy compared to the light blue line (step 2),
demonstrating that as the planning steps progress,
the models are difficult to maintain the represen-
6tations of the current placement of the blocks. (3)
By comparing the linear probe in Figure 6 and the
nonlinear probe in Figure 7, we find that both have
the same trend, indicating that the model internally
stores the current state in a linear manner. A simi-
lar trend in Future Decisions is shown in Figure 8
and Figure 9 for actions. It reveals that look-ahead
decisions are enriched in the early layers.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni0000004c/uni00000051/uni0000004c/uni00000057
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055
Figure 8: Action probe in Llama-2-7b-chat-hf.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017 /uni00000015/uni0000001b
/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni0000004c/uni00000051/uni0000004c/uni00000057
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000055
Figure 9: Action probe in Vicuna-7b.
Supplementary Analysis As shown in Figure 10
and Figure 13. They illustrate the accuracy of fu-
ture decisions based on the current step. Each col-
umn represents the current step, while the rows
represent the max accuracy of the probe in pre-
dicting future answers. We observe the following
: (1) For the sixth row and first column, the probe
can predict the future sixth step with an accuracy
of 0.51 at the first step. This indicates that the
model stores information about future decisions in
advance, supporting the hypothesis of forward plan-
ning. (2) For each row, the values increase from
left to right. For example, the accuracy in the fifth
column of the sixth row is higher than that in the
first column. This means the model is more certain
about the output of the sixth step at the fifth step
compared to the first step, demonstrating that the
model has difficulty in planning over long distances.(3) The accuracy for the first column, representing
the prediction accuracy for the next five steps after
the initial step, shows a declining trend, indicating
that the model stores future decision information in
advance, supporting the hypothesis of look-ahead
planning decisions existence hypothesis.
6.2 Internal Representations Facilitate Future
Decision-making
In this section, we further verify the causal effect
of planning information at different steps. We test
the causality between planning information in the
previous history taand decisions in step tb, where
ta< tb. Specifically, we compare whether the
information from step t1contributes to the planning
in step t2. If the model is greedy in its planning,
there should be no decision information in tathat
can help make better decisions in tb. Therefore, we
set the key of MHSA in historical decision tto 0
to study the causal effect of historical information
on future predictions.
Experiments For each step t, we have a history
Ht= [a1, a2, ..., a t−1], where each step span ai
contains color tokens.
(1) Mask all steps: First, identify all color to-
kens in Ht, and set the keys to 0 for these colors
in each layer of MHSA, resulting in the masked
historical information H′
t. The main goal is to stop
past decision information from affecting the cur-
rent decision of the last token. Obtain the decision
probability y′
tbased on H′
tintstep.
(2) Make one step visible: Based on H′
t, make
only the color at position ivisible, while masking
the other positions, resulting in H′′
t,i. Use H′′
t,ifor
prediction, Obtain the decision probability y′′
t,i.
(3) Calculate one step effect: Compare the deci-
sion probabilities obtained from masking all steps
and from making one step visible to calculate the
effect of a single step. The larger this value, the
greater the impact of step ion step t:
Impacti,t=y′′
t,i−y′
t (9)
Results and Analysis As illustrated in the Figure
11 and Figure 16, the columns represent the steps
visible during prediction, the rows represent the
steps being predicted, and the values inside repre-
sent the contribution of step tto step i. (1) For
example, in the second column of the sixth row, the
model can increase the probability of inferring the
correct decision in the sixth step by 0.24 just by
7/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000046/uni00000058/uni00000055/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048/uni00000003/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001c/uni00000018 /uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001a/uni0000001b /uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000018 /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018/uni00000014 /uni00000013/uni00000011/uni00000018/uni00000015 /uni00000013/uni00000011/uni00000019/uni0000001c /uni00000013/uni00000011/uni00000019/uni00000017 /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni00000013/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 10: Future action linear probe in Llama-2-7b-
chat-hf.
using the information from the second step. This
indicates that the model is not greedy and is not
limited to only preparing for the next step, which
causally proves the conclusion of look-ahead plan-
ning. (2) Observing the values in each column,
for instance, the maximum value in the fifth row
is 0.46, located in the third column. This repre-
sents that the third step is the most important for
predicting the fifth step. It is found that the most
important steps for prediction tend to be later steps,
indicating that the look-ahead planning ability of
LLMs is still relatively preliminary.
7 Related work
LLM-Based Agents With the emergence of large
language models, researchers begin to use them
as intelligent agents (Xi et al., 2023; Wang et al.,
2024a). Significantly, ReAct (Yao et al., 2022)
innovatively combines CoT reasoning with agent
actions. Some tasks utilize the planning capabili-
ties of large language models through prompt en-
gineering methods (Huang et al., 2022; Hao et al.,
2023; Yao et al., 2024; Zhang et al., 2024). Other
researchers enhance the planning capabilities of
large language models through fine-tuning meth-
ods (Zeng et al., 2023; Yu et al., 2024). Some
researchers construct benchmarks to evaluate the
planning ability of large language models (Shridhar
et al., 2020; Wang et al., 2022; Zhou et al., 2023;
Deng et al., 2024; Xu et al., 2023; Qin et al., 2023).
Mechanistic Interpretability Recent works
study mechanistic interpretability in factual asso-
ciations, in-context Learning, and arithmetic rea-
soning tasks from the perspective of information
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000059/uni0000004c/uni00000056/uni0000004c/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000014/uni00000019
/uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni00000013/uni0000001c
/uni00000013/uni00000011/uni00000015/uni00000019 /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000018/uni00000018
/uni00000013/uni00000011/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000014/uni00000014 /uni00000013/uni00000011/uni00000017/uni00000019 /uni00000013/uni00000011/uni00000015/uni00000017
/uni00000013/uni00000011/uni00000015/uni00000015 /uni00000013/uni00000011/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000015/uni00000015 /uni00000013/uni00000011/uni0000001a/uni00000014/uni00000056/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000059/uni00000048/uni00000051/uni00000048/uni00000047/uni00000003/uni00000044/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019Figure 11: Single step intervened analysis in Vicuna-7b.
flow (Geva et al., 2023; Wang et al., 2023; Stolfo
et al., 2023). Researchers also study Othello (Li
et al., 2022; Nanda et al., 2023), chess (Karvo-
nen, 2024) and Blocksword (Wang et al., 2024b) in
transformer. However, research on the mechanistic
interpretation of large language models performing
planning tasks is still unexplored. Our work con-
ducts a preliminary study from the perspective of
information flow and internal representation.
Look-Ahead Pal et al. (2023); Wu et al. (2024a);
Jenner et al. (2024) demonstrate that it is possible
to decode future tokens from the hidden represen-
tations of a language model at previous token po-
sitions. In task planning, a model needs to have
look-ahead capabilities. However, it is not yet clear
whether LLMs use similar mechanisms when plan-
ning. Our work focuses on the look-ahead mecha-
nisms in planning in LLMs.
8 Conclusion
In this paper, we investigate the mechanisms of
look-ahead planning in LLMs through the perspec-
tives of information flow and internal representa-
tions. We demonstrate Look-Ahead Planning De-
cisions Existence Hypothesis . Our findings indi-
cate that internal representations of LLMs encode
a few short-term future decisions to some extent
when planning is successful. These look-ahead
decisions are enriched in the early layers, with
their accuracy diminishing as the number of plan-
ning steps increases. We demonstrate that MHSA
mainly extracts information from the spans of goal
states and recent steps. Additionally, the output of
MHSA in the middle layers at the final token can
partially decode the correct decisions.
8Limitation
Although our work provides an in-depth analysis
and explanation of look-ahead planning mecha-
nisms of large language models, there are several
limitations. First, our analytical methods require ac-
cess to the internal parameters and representations
of open-source models. Although black-box large
language models such as ChatGPT possess strong
planning capabilities, we cannot access their inter-
nal parameters, making it challenging to interpret
the most advanced language models. Second, our
research primarily focuses on the planning mecha-
nisms in Blocksworld. However, many other plan-
ning tasks, such as commonsense planning (e.g.,
"how to make a meal"), lack standard answers,
making it difficult to evaluate the correctness of
the planning and conduct quantitative analysis. We
leave these limitations for future work.
References
Roy F Baumeister, Kathleen D V ohs, and Gabriele Oet-
tingen. 2016. Pragmatic prospection: How and why
people think about the future. Review of general
psychology , 20(1):3–16.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam
Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024.
Mind2web: Towards a generalist agent for the web.
Advances in Neural Information Processing Systems ,
36.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual asso-
ciations in auto-regressive language models. arXiv
preprint arXiv:2304.14767 .
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,
Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023.
Reasoning with language model is planning with
world model. arXiv preprint arXiv:2305.14992 .
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022. Language models as zero-shot
planners: Extracting actionable knowledge for em-
bodied agents. In International Conference on Ma-
chine Learning , pages 9118–9147. PMLR.
Erik Jenner, Shreyas Kapur, Vasil Georgiev, Cameron
Allen, Scott Emmons, and Stuart Russell. 2024. Evi-
dence of learned look-ahead in a chess-playing neural
network. arXiv preprint arXiv:2406.00877 .Adam Karvonen. 2024. Emergent world models and
latent variable estimation in chess-playing language
models. arXiv preprint arXiv:2403.15498 .
Kenneth Li, Aspen K Hopkins, David Bau, Fernanda
Viégas, Hanspeter Pfister, and Martin Wattenberg.
2022. Emergent world representations: Exploring a
sequence model trained on a synthetic task. arXiv
preprint arXiv:2210.13382 .
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu
Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen
Men, Kejuan Yang, et al. 2023. Agentbench: Evaluat-
ing llms as agents. arXiv preprint arXiv:2308.03688 .
Neel Nanda, Andrew Lee, and Martin Wattenberg. 2023.
Emergent linear representations in world models of
self-supervised sequence models. arXiv preprint
arXiv:2309.00941 .
Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wal-
lace, and David Bau. 2023. Future lens: Anticipating
subsequent tokens from a single hidden state. arXiv
preprint arXiv:2311.04897 .
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, et al. 2023. Toolllm: Facilitating large
language models to master 16000+ real-world apis.
arXiv preprint arXiv:2307.16789 .
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,
Yonatan Bisk, Adam Trischler, and Matthew
Hausknecht. 2020. Alfworld: Aligning text and em-
bodied environments for interactive learning. arXiv
preprint arXiv:2010.03768 .
Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya
Sachan. 2023. A mechanistic interpretation of arith-
metic reasoning in language models using causal
mediation analysis. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 7035–7052.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Karthik Valmeekam, Matthew Marquez, Alberto Olmo,
Sarath Sreedharan, and Subbarao Kambhampati.
2024. Planbench: An extensible benchmark for eval-
uating large language models on planning and reason-
ing about change. Advances in Neural Information
Processing Systems , 36.
Karthik Valmeekam, Matthew Marquez, Sarath Sreed-
haran, and Subbarao Kambhampati. 2023. On the
planning abilities of large language models-a criti-
cal investigation. Advances in Neural Information
Processing Systems , 36:75993–76005.
9Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,
Fandong Meng, Jie Zhou, and Xu Sun. 2023. Label
words are anchors: An information flow perspective
for understanding in-context learning. arXiv preprint
arXiv:2305.14160 .
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, et al. 2024a. A survey on large
language model based autonomous agents. Frontiers
of Computer Science , 18(6):186345.
Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and
Prithviraj Ammanabrolu. 2022. Scienceworld: Is
your agent smarter than a 5th grader? arXiv preprint
arXiv:2203.07540 .
Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-
Hua Teng, and Wei Chen. 2024b. Alpine: Unveiling
the planning capability of autoregressive learning in
language models. arXiv preprint arXiv:2405.09220 .
Wilson Wu, John X Morris, and Lionel Levine. 2024a.
Do language models plan ahead for future tokens?
arXiv preprint arXiv:2404.00859 .
Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing
Huang, Zheng Wang, Noah D Goodman, Christo-
pher D Manning, and Christopher Potts. 2024b.
pyvene: A library for understanding and improv-
ing pytorch models via interventions. arXiv preprint
arXiv:2403.07809 .
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen
Ding, Boyang Hong, Ming Zhang, Junzhe Wang,
Senjie Jin, Enyu Zhou, et al. 2023. The rise and
potential of large language model based agents: A
survey. arXiv preprint arXiv:2309.07864 .
Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang
Hong, Honglin Guo, Junzhe Wang, Dingwen Yang,
Chenyang Liao, Xin Guo, Wei He, et al. 2024.
Agentgym: Evolving large language model-based
agents across diverse environments. arXiv preprint
arXiv:2406.04151 .
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu,
Zhengyu Chen, and Jian Zhang. 2023. On the tool
manipulation capability of open-source large lan-
guage models. arXiv preprint arXiv:2305.16504 .
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems , 36.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. arXiv preprint arXiv:2210.03629 .Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and
Lianhui Qin. 2024. Flow of Reasoning: Efficient
Training of LLM Policy with Divergent Thinking.
arXiv e-prints , arXiv:2406.05673.
Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao
Liu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning:
Enabling generalized agent abilities for llms. arXiv
preprint arXiv:2310.12823 .
Li Zhang, Peter Jansen, Tianyi Zhang, Peter Clark, Chris
Callison-Burch, and Niket Tandon. 2024. Pddlego:
Iterative planning in textual environments. arXiv
preprint arXiv:2405.19793 .
Longtao Zheng, Rundong Wang, Xinrun Wang, and
Bo An. 2023. Synapse: Trajectory-as-exemplar
prompting with memory for computer control. In
The Twelfth International Conference on Learning
Representations .
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan
Bisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:
A realistic web environment for building autonomous
agents. arXiv preprint arXiv:2307.13854 .
A Additional Results
Information flow of last token in Vicuna-7b is
shown in Figure 12. Future action nonlinear probe
in Llama-2-7b-chat-hf is shown in Figure 13. Fu-
ture action linear probe in Vicuna-7b is shown in
Figure 14. Future action nonlinear probe in Vicuna-
7b is shown in Figure 15, Single step intervened
analysis in Llama-2-7b-chat-hf is shown in Fig-
ure 16.
LEVEL L1 L2 L3 Total
Train Size
4 blocks 3 17 25 45
5 blocks 1 23 121 145
6 blocks 3 48 326 377
Total 7 88 472 567
Test Size
4 blocks 24 60 80 164
5 blocks 34 115 268 417
6 blocks 57 232 709 998
Total 115 407 1057 1579
Table 1: Blocksworld dataset statistics
10/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000014
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000015
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000016
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000017
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000017
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000018
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004a/uni00000052/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni00000044/uni00000057/uni00000048
/uni00000053/uni0000004f/uni00000044/uni00000051/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000014
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000015
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000016
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000017
/uni0000004b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000018
/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni00000019
/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 12: Information flow of last token in Vicuna-7b.
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000046/uni00000058/uni00000055/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048/uni00000003/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001c/uni00000018 /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000015 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000019 /uni00000013/uni00000011/uni0000001c /uni00000013/uni00000011/uni0000001a/uni00000015 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000018/uni0000001a /uni00000013/uni00000011/uni00000019/uni0000001b /uni00000013/uni00000011/uni00000019/uni00000018 /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni00000013/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013
Figure 13: Future action nonlinear probe in Llama-2-
7b-chat-hf.
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000046/uni00000058/uni00000055/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048/uni00000003/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001c/uni00000017 /uni00000013/uni00000011/uni0000001a/uni0000001a /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a/uni0000001a /uni00000013/uni00000011/uni0000001a/uni0000001b /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001a/uni0000001a /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018/uni0000001a /uni00000013/uni00000011/uni00000016/uni0000001b /uni00000013/uni00000011/uni00000019/uni0000001a /uni00000013/uni00000011/uni00000019/uni00000016 /uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013
Figure 14: Future action linear probe in Vicuna-7b
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000046/uni00000058/uni00000055/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048/uni00000003/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001c/uni00000017 /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001a/uni0000001a /uni00000013/uni00000011/uni0000001a/uni00000019 /uni00000013/uni00000011/uni0000001a/uni00000019 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018/uni0000001b /uni00000013/uni00000011/uni00000016/uni0000001c /uni00000013/uni00000011/uni00000019/uni0000001a /uni00000013/uni00000011/uni00000019/uni00000015 /uni00000013/uni00000011/uni0000001b/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000049/uni00000058/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000051/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000053/uni00000055/uni00000052/uni00000045/uni00000048
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 15: Future action nonlinear probe in Vicuna-7b
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000059/uni0000004c/uni00000056/uni0000004c/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000014
/uni00000015
/uni00000016
/uni00000017
/uni00000018
/uni00000019/uni00000053/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000013/uni00000019
/uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000015/uni0000001b /uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000019/uni00000018
/uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000013/uni00000017 /uni00000013/uni00000011/uni00000016/uni0000001b /uni00000013/uni00000011/uni00000013/uni0000001b
/uni00000013/uni00000011/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000017 /uni00000010/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000019/uni00000056/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000059/uni00000048/uni00000051/uni00000048/uni00000047/uni00000003/uni00000044/uni00000051/uni00000044/uni0000004f/uni0000005c/uni00000056/uni0000004c/uni00000056
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a
Figure 16: Single step intervened analysis in Llama-2-
7b-chat-hf.
11