MolTRES: Improving Chemical Language Representation Learning for
Molecular Property Prediction
Jun-Hyung Park1Yeachan Kim2Mingyu Lee2Hyuntae Park2SangKeun Lee2,3
1BK21 FOUR R&E Center for Artificial Intelligence, Korea University
2Department of Artificial Intelligence, Korea University
3Department of Computer Science and Engineering, Korea University
{irish07, yeachan, decon9201, pht0639, yalphy}@korea.ac.kr
Abstract
Chemical representation learning has gained
increasing interest due to the limited availabil-
ity of supervised data in fields such as drug
and materials design. This interest particularly
extends to chemical language representation
learning, which involves pre-training Trans-
formers on SMILES sequences ‚Äì textual de-
scriptors of molecules. Despite its success in
molecular property prediction, current practices
often lead to overfitting and limited scalability
due to early convergence. In this paper, we
introduce a novel chemical language represen-
tation learning framework, called MolTRES, to
address these issues. MolTRES incorporates
generator-discriminator training, allowing the
model to learn from more challenging exam-
ples that require structural understanding. In
addition, we enrich molecular representations
by transferring knowledge from scientific liter-
ature by integrating external materials embed-
ding. Experimental results show that our model
outperforms existing state-of-the-art models on
popular molecular property prediction tasks.
1 Introduction
Deep neural networks (DNNs) have emerged as
a compelling, computationally efficient approach
for predicting molecular properties, with signifi-
cant implications in material engineering and drug
discovery. By training DNNs on molecule data to
predict the properties in a supervised manner or to
reconstruct molecules in an unsupervised manner,
these networks can significantly reduce the costs of
traditional methods, which typically require chem-
ical experts and wet-lab experiments. Moreover,
DNN-based molecular prediction has gained in-
creasing popularity due to the generalization capac-
ity of DNNs. This allows for the application of
a single (pre-)trained model across various tasks,
reducing the need for task-specific modeling.
Inspired by recent advances in pre-trained lan-
guage models in the field of natural language pro-
80%85%90%95%100%
050K100K150K200KPre-training  acc.
Pre-raining stepsMoLFormer-XLMolTRES (ours)
80%82%84%86%
0%20%40%60%80%100%Downstream ROC-AUC
Pre-training dataMoLFormer-XLMolTRES (ours)Figure 1: Existing pre-training methods for chemical
language representation learning already converge at
their early stage without seeing the entire data. Conse-
quently, MoLFormer (Ross et al., 2022), a state-of-the-
art chemical language representation learning method,
exhibits limited scalability in terms of data size.
cessing (NLP), several chemical language represen-
tation learning methods based on SMILES Trans-
formers (Wang et al., 2019; Chithrananda et al.,
2020) have been proposed. These methods typ-
ically employ self-supervised tasks on SMILES
(Simplified Molecular-Input Line Entry System) se-
quences of molecules, analogous to the masked lan-
guage modeling (MLM) commonly used in BERT
(Devlin et al., 2019). Since modern Transform-
ers are designed to scale to massive NLP corpora
(Vaswani et al., 2017), they offer practical advan-
tages in terms of efficiency and throughput. This
enables the models to leverage massive amounts
of SMILES sequences to learn universal represen-
tations for molecules, leading to performance im-
1arXiv:2408.01426v1  [physics.chem-ph]  9 Jul 2024provements in a wide range of molecular property
prediction tasks (Ross et al., 2022). However, as
these models typically follow settings designed for
natural language modeling, the optimal pre-training
settings for chemical language representation learn-
ing remain underexplored.
Through extensive investigation into the pre-
training of SMILES Transformers, we have dis-
covered that the current pre-training task, MLM on
SMILES sequences using a random masking strat-
egy, is not effective for learning informative molec-
ular representations. We have empirically observed
that this task can be easily solved using surface pat-
terns, leading to overfitting and limited scalability,
as shown in Figure 1. This may be attributed to
two inherent properties of SMILES. First, existing
large-scale molecule datasets exhibit unbalanced
atom distributions (He et al., 2023a). For exam-
ple, in ZINC (Irwin et al., 2012), a representative
dataset containing billions of molecules, carbon
(C), nitrogen (N), and oxygen (O) comprise 95% of
the tokens in total SMILES sequences. Second, the
SMILES grammar contains many superficial pat-
terns, such as numbers representing ring structures
that always appear twice. These patterns allow the
model to predict original tokens without learning
the underlying chemical information. Furthermore,
unlike natural language, which is fundamentally
grounded in concepts and possesses general expres-
sivity across various problem-solving scenarios,
SMILES is designed solely to express molecular
structure and does not directly represent molecular
properties. Thus, the current pre-training task likely
provides a limited notion of molecular properties.
In this paper, we propose a novel frame-
work for pre-training SMILES transformers,
called MolTRES ( Molecular TRansformer with
Enhanced Self-supervised learning), to address the
aforementioned issues. Our framework focuses on
two key objectives: (1) increasing the difficulty
of the pre-training task, and (2) incorporating ex-
ternal knowledge related to molecular properties
into model representations. To achieve these goals,
we first present a novel dynamic molecule model-
ing method, coined DynaMol, based on generator-
discriminator training (Clark et al., 2020). This
method trains a model to distinguish real SMILES
tokens from synthetically generated replacements,
jointly used with substructure-level masking. It
facilitates to significantly increase the masking ra-
tio for more challenging training examples, while
minimizing discrepancy caused by mask tokens. Inaddition, we enhance model representations by in-
tegrating mat2vec word representations (Tshitoyan
et al., 2019) trained on massive scientific literature.
This integration helps to directly embody molecular
properties in the learned representations.
To demonstrate the effectiveness of MolTRES,
we conduct extensive experiments and ablation
studies on diverse molecular property prediction
tasks. We evaluate MolTRES on eight classifica-
tion and four regression tasks from MoleculeNet,
covering quantum mechanical, physical, biophysi-
cal, and physiological properties of chemicals. Our
results indicate that MolTRES outperforms state-
of-the-art baselines across most tasks, including
1D sequence-, 2D graph-, and 3D geometry-based
chemical models. Further analysis shows that
MolTRES significantly improves the capabilities
of chemical language representation learning by
addressing the limitations of existing approaches.
Our contributions are summarized as follows:
‚Ä¢We propose MolTRES, a novel framework
to pre-train SMILES Transformers based on
generator-discriminator training and external
knowledge transfer.
‚Ä¢We present a novel architecture for SMILES
transformers efficiently integrated with word
representations trained on scientific literature.
‚Ä¢Experimental results demonstrate that
MolTRES establishes state-of-the-art results
over a wide range of molecular property
prediction tasks.
2 Preliminaries
2.1 SMILES Transformer
Transformer (Vaswani et al., 2017) is a popular
neural network architecture for processing texts,
which can also be applied to processing SMILES
sequences. It consists of a series of Transformer
blocks, each involving a multi-head self-attention
layer followed by a multi-layer feed-forward net-
work. The self-attention layer allows the network
to effectively model global dependencies within
the sequence of input tokens. Given a series of
vector representations for tokens, a self-attention
layer applies three linear transformations to gener-
ate query ( q), key ( k), and value ( v) representations,
respectively. The outputs at position mare calcu-
lated by aggregating the representations from other
positions using a weighted sum, where weights are
2FFN1
<ùëö><ùëö><ùëö>(ùê∂)<ùëö>ùë¨ùëÆùë¨ùë´ùê∂ùëÜùê∂ùê∂ùê∂originalreplacedGeneratorDiscriminator<ùëö><ùëö><ùëö>(ùê∂)<ùëö>ùê∂ùëÅùê∂(ùê∂)ùëÜoriginaloriginalreplacedùë¨ùëÆùë¨ùë´Molecule graph SMILESSubstructure masking
Sharingmat2vecTransformer emb.FFN1FFN2
ùê∂ùëÅùê∂(ùê∂)ùëÜ
!!!"!Figure 2: Overview of MolTRES. EGandEDrepresent the embedding layers of the generator and discriminator,
respectively. It is noteworthy that the mat2vec embeddings are frozen during pre-training.
derived from a similarity function between qand
k, as follows:
Att(q, k, v )m=Œ£N
n=1sim(qm, kn)vn
Œ£N
n=1sim(qm, kn)(1)
where sim(qm, kn) =exp(qT
mkn/‚àö
d)andNis the
length of tokens. Transformer can effectively cap-
ture the dependencies in variable-length sequences,
and therefore, it is utilized in processing SMILES,
as in ChemBERTa (Chithrananda et al., 2020).
However, self-attention shows a quadratic com-
plexity O(N2)dereived from the computation of
the inner product between every token pair, which
incurs significant costs when processing molecules
represented in long SMILES sequences like poly-
mers. To reduce the complexity, MoLFormer (Ross
et al., 2022) has introduced linear attention with
rotary embeddings. This reformulates the original
self-attention layer as follows:
Att(q, k, v )m=Œ£N
n=1œï(Rmqm)Tœï(Rnkn)vn
Œ£N
n=1œï(Rmqm)Tœï(Rnkn)
(2)
where Rmrepresents a position-dependent rota-
tion at position m, and œï(x) =elu(x) + 1 defines
the activation function used. This linear attention
mechanism reduces the complexity to O(N), sig-
nificantly improving the efficiency of chemical lan-
guage representation learning, coming with mini-
mal performance degradation.2.2 Chemical Language Representation
Learning via MLM
Typical work in chemical language representation
learning (Chithrananda et al., 2020; Ross et al.,
2022) utilizes a self-supervised task known as
MLM. This objective involves training a model
to predict original sequences from sequences in
which some tokens are randomly masked. Specif-
ically, given a sequence X={x1, x2, x3, ..., x n},
we corrupt XintoÀúXby masking 15% of its tokens.
We then train a model, denoted as Cwith parame-
tersŒ∏C, to reconstruct X. The loss of each example
is formulated as follows:
LC=‚àíX
i‚ààMlogp(xi|ÀúX;Œ∏C), (3)
where Mrepresents the set of masked token posi-
tions. In typical chemical language representation
learning methods, each masked token is substituted
with a special mask token in 80% of cases, a ran-
dom token in 10% of cases, and the original token
in the remaining 10% cases, following practices in
BERT (Devlin et al., 2019).
3MolTRES: Molecular Transformer with
Enhanced Self-supervised Learning
In this section, we detail our framework, MolTRES.
We propose a novel pre-training task, called Dy-
naMol, which incorporates generator-discriminator
3training into chemical language representation
learning with substructure masking. In addition,
we integrate molecular representations that have
been trained on scientific literature.
3.1 DynaMol: Dynamic Molecule Modeling
with Generator-Discriminator Training
To increase the difficulty of chemical language
representation learning, we propose a dynamic
molecule modeling scheme based on generator-
discriminator training, inspired by replaced token
detection proposed in Clark et al. (2020). The
proposed scheme involves training two models,
namely a generator and a discriminator. The gener-
ator is trained to predict original sequences given
masked sequences similar to MLM, while the dis-
criminator is trained to identify tokens that have
been replaced by the generator. Since the generator
transforms masked sequences to more closely re-
semble original distributions, this training scheme
results in less discrepancy between the inputs from
pre-training and downstream tasks, and allows for
flexible adjustments of the masking ratio (He et al.,
2023b). Moreover, as the generator is being trained,
it naturally provides increasingly challenging exam-
ples to the discriminator. This scheme is expected
to alleviate the issues of early convergence and
over-fitting commonly observed in existing meth-
ods of chemical language representation learning.
Specifically, similar to MLM, the generator G
with parameters Œ∏Gis trained to reconstruct the
sequence X. The loss of Gfor each example is
formulated as follows:
LG=‚àíX
i‚ààMlogp(xi|ÀúX;Œ∏G). (4)
Then, the input sequence for the discriminator
is constructed by replacing the masked tokens in
ÀúXwith new tokens, sampled from the generator‚Äôs
probability distribution pG, as follows:
ÀúXD=(
Àúxi‚àΩp(xi|ÀúX;Œ∏G),ifi‚àà M
xi, otherwise.(5)
The discriminator is trained to distinguish
whether each token in the generated input sequence
ÀúXDis original or has been replaced. The loss for
the discriminator is formulated as follows:
LD=‚àínX
i=1logp(zi|ÀúXD;Œ∏D), (6)where ziis a binary label that indicates whether
thei-th input token is original or has been re-
placed. Finally, the generator Gand discriminator
Dare jointly optimized with multiple objectives,
expressed as L=LG+ŒªLD, where Œªis a pre-
defined balancing parameter for the discriminator
loss. In this work, Œªis set to 10.
In addition, we carefully design three rules to
mask SMILES at multiple substructure-level granu-
larities, thereby preventing models from predicting
the correct answer by exploiting superficial patterns
in the SMILES grammar. (1) We mask all special
tokens that represent structural information, such
as numbers for cycles. (2) We then mask spans of
SMILES that composes certain substructures, such
as substituents, bridges, or groups of sequential
atoms, until the ratio of masked tokens does not
exceed the pre-defined target masking ratio. Note
that these substructure can be easily identified by
segmenting SMILES strings based on brackets. (3)
Finally, we mask random atomic SMILES tokens
to achieve the target masking ratio. We follow a
typical masking strategy that, among the masked
tokens, 80% are replaced with mask tokens, 10%
are replaced with random tokens, and the rest 10%
remain unchanged. Notably, we use 65% of the
target masking ratio for pre-training.
3.2 Knowledge Transfer from Scientific
Literature using mat2vec
While modeling SMILES helps models understand
molecular structure and connectivity, SMILES it-
self lacks explicit information about molecular
properties. Scientific literature, which is similarly
represented in a textual form, provides a more flexi-
ble and rich source of external information. It com-
prehensively involves information about molecular
properties derived from wet laboratory experiments
and computational methods. Therefore, we enrich
the representations of SMILES Transformers by
integrating information from scientific literature.
Despite the many possible design choices avail-
able, we opt to leverage mat2vec (Tshitoyan et al.,
2019), a straightforward embedding model trained
on extensive scientific literature, for integration
into Transformer‚Äôs embedding vectors. We pri-
oritize the efficiency in terms of memory foot-
prints and computations in our integration pro-
cedures, essential for dealing with large-scale
pre-training. Given an input sequence X=
{x1, ..., x n}, we obtain embedding vectors for ev-
ery token from the Transformer‚Äôs embedding layer,
4Methods BBBP ‚ÜëTox21‚ÜëToxCast ‚ÜëClinTox ‚ÜëMUV‚ÜëHIV‚ÜëBACE‚ÜëSIDER ‚ÜëAvg.‚Üë
3D Conformation
GeomGCL (Liu et al., 2022) - 85.0 - 91.9 - - - 64.8 -
GEM (Fang et al., 2022) 72.4 78.1 - 90.1 - 80.6 85.6 67.2 -
3D InfoMax (St√§rk et al., 2022) 68.3 76.1 64.8 79.9 74.4 75.9 79.7 60.6 72.5
GraphMVP (Liu et al., 2022) 69.4 76.2 64.5 86.5 76.2 76.2 79.8 60.5 73.7
MoleculeSDE (Liu et al., 2023a) 71.8 76.8 65.0 87.0 80.9 78.8 79.5 75.1
Uni-Mol (Zhou et al., 2023) 71.5 78.9 69.1 84.1 72.6 78.6 83.2 57.7 74.5
MoleBlend (Yu et al., 2024) 73.0 77.8 66.1 87.6 77.2 79.0 83.7 64.9 76.2
Mol-AE (Yang et al., 2024) 72.0 80.0 69.6 87.8 81.6 80.6 84.1 67.0 77.8
UniCorn (Feng et al., 2024) 74.2 79.3 69.4 92.1 82.6 79.8 85.8 64.0 78.4
2D Graph
DimeNet (Klicpera et al., 2020) - 78.0 - 76.0 - - - 61.5 -
AttrMask (Hu et al., 2020) 65.0 74.8 62.9 87.7 73.4 76.8 79.7 61.2 72.7
GROVER (Rong et al., 2020) 70.0 74.3 65.4 81.2 67.3 62.5 82.6 64.8 71.0
BGRL (Thakoor et al., 2022) 72.7 75.8 65.1 77.6 76.7 77.1 74.7 60.4 72.5
MolCLR (Wang et al., 2022) 66.6 73.0 62.9 86.1 72.5 76.2 71.5 57.5 70.8
GraphMAE (Hou et al., 2022) 72.0 75.5 64.1 82.3 76.3 77.2 83.1 60.3 73.9
Mole-BERT (Liu et al., 2023c) 71.9 76.8 64.3 78.9 78.6 78.2 80.8 62.8 74.0
SimSGT (Xia et al., 2023) 72.2 76.8 65.9 85.7 81.5 78.0 84.3 61.7 75.8
MolCA + 2D (Liu et al., 2023b) 70.0 77.2 64.5 89.5 - - 79.8 63.0 -
1D SMILES/SELFIES
MoLFormer-XL (Ross et al., 2022) 93.7 84.7 65.6 94.8 80.6 82.2 88.2 66.9 82.1
SELFormer (Y√ºksel et al., 2023) 90.2 65.3 - - - 68.1 83.2 74.5 -
MolCA (Liu et al., 2023b) 70.8 76.0 56.2 89.0 - - 79.3 61.2 -
MolTRES-small (ours) 95.0 83.4 64.8 94.0 80.0 81.7 87.7 68.3 81.9
MolTRES (ours) 96.1 85.3 70.1 96.7 84.9 84.2 91.7 69.8 84.8
Table 1: Evaluation results on MoleculeNet classification tasks. We report ROC-AUC scores (higher is better) under
scaffold splitting. The best and second-best results are in bold and underlined .
denoted as Et={et
1, ..., et
n}. Using a mapping
function I(¬∑), we assign each token to correspond-
ing mat2vec embedding vectors, denoted as Em=
{em
1, ..., em
n}s.t.em
k=P
z‚ààI(xk)mat2vec (z). We
then combine EtandEmusing a linear projection
layer F1(¬∑). The set of embedding vectors for the
generator VGis generated as follows:
VG={F1(et
1‚ó¶em
1), ..., F 1(et
n‚ó¶em
n)},(7)
where ‚ó¶denotes the concatenation operation. In a
similar manner, the set of embedding vectors for
the discriminator VDis generated from the tokens
reconstructed by the generator as follows:
V={F1(Àúet
1‚ó¶Àúem
1), ..., F 1(Àúet
n‚ó¶Àúem
n)}
VD={F2(œÉ(v1)), ..., F 2(œÉ(vn))}
s.t.v1, ..., v n‚ààV,(8)
where v1, ..., v n‚ààVandœÉ(¬∑)is an activation func-
tion, which is the gelu function in this work.
For the integration, we manually design a map-
ping function I(¬∑)using human prior knowledge to
address the vocabulary mismatch between SMILES
tokens and mat2vec words. We utilize a thesaurus
carefully constructed by domain experts, chosenfor its superior computational efficiency and sta-
bility compared to learning-based approaches. For
example, the thesaurus maps ‚Äú[cH+]‚Äù in the Trans-
former‚Äôs vocabulary to ‚Äúmethylidyne‚Äù, ‚Äúion‚Äù, and
‚Äúcation‚Äù in the mat2vec vocabulary. Based on this
thesaurus, we pre-calculate embedding vectors for
2,696 tokens in the Transformer vocabulary before
pre-training. To prevent catastrophic forgetting of
mat2vec knowledge, we freeze these pre-calculated
embedding vectors during pre-training. During
fine-tuning, these embedding vectors are trainable
to adapt the knowledge for each downstream task.
4 Experiment
4.1 Experimental Setup
Pre-training. We collect 118 million molecules
from PubChem1and 1.9 billion molecules from
ZINC2. We pre-train two MolTRES models, a base
model (MolTRES) and a smaller model (MolTRES-
small). Our model architectures are detailed in
Appendix A.1. We train our models for 200,000
steps with a batch size of 25,600 and use the final
models in evaluation.
1https://pubchem.ncbi.nlm.nih.gov/
2https://zinc.docking.org/
5Methods ESOL ‚ÜìFreeSolv ‚ÜìLipophilicity ‚ÜìAvg.‚Üì
3D Conformation
3D InfoMax (St√§rk et al., 2022) 0.894 2.337 0.695 1.309
GraphMVP (Liu et al., 2022) 1.029 - 0.681 -
Uni-Mol (Zhou et al., 2023) 0.844 1.879 0.610 1.111
MoleBlend (Yu et al., 2024) 0.831 1.910 0.638 1.113
Mol-AE (Yang et al., 2024) 0.830 1.448 0.607 0.962
UniCorn (Feng et al., 2024) 0.817 1.555 0.591 0.988
2D Graph
AttrMask (Hu et al., 2020) 1.112 - 0.730 -
GROVER (Rong et al., 2020) 0.831 1.544 0.560 0.978
MolCLR (Wang et al., 2022) 1.110 2.200 0.650 1.320
SimSGT (Liu et al., 2023c) 0.917 - 0.695 -
1D SMILES/SELFIES
MoLFormer-Base (Ross et al., 2022) 0.280 0.260 0.649 0.396
MoLFormer-XL (Ross et al., 2022) 0.279 0.231 0.530 0.347
SELFormer (Y√ºksel et al., 2023) 0.682 2.797 0.735 1.405
MolTRES-small (ours) 0.280 0.250 0.594 0.375
MolTRES (ours) 0.274 0.229 0.504 0.336
Table 2: Evaluation results on MoleculeNet regression tasks. We report RMSE scores (lower is better) under scaffold
splitting. The best and second-best results are in bold and underlined .
Evaluation We evaluate our models and base-
lines on eight classification tasks and four re-
gression tasks from the MoleculeNet benchmark
(Wu et al., 2018). We report Receiver Operating
Characteristic-Area Under the Curve (ROC-AUC)
scores for the classification tasks, Mean Absolute
Error (MAE) scores for QM9, and Root Mean
Square Error (RMSE) scores for the remaining re-
gression tasks. We report the test score from the
model that achieves the best validation score.
Baselines. We compare our models with diverse
state-of-the-art baselines categorized as follows:
‚Ä¢3D Conformation: This category includes
methods that utilize 3D conformation from
the geometry information of molecules and
may incorporate other modalities.
‚Ä¢2D Graph: This category includes methods
that utilize 2D graph information, such as
atoms and bonds, and may also combine 1D
SMILES.
‚Ä¢1D SMILES/SELFIES: This category in-
cludes methods that utilize SMILES or SELF-
IES sequences of molecules.
4.2 Main Results
We first compare MolTRES with state-of-the-art
molecular property prediction methods on Molecu-
leNet classification tasks. As shown in Table 1,MolTRES surpasses the best baseline, MoLFormer-
XL, by an average of 2.7%. In addition, MolTRES-
small also shows a competitive performance com-
pared to the baselines. Notably, MolTRES sig-
nificantly outperforms baseline methods using 3D
conformation and 2D graph. This confirms the
strength of pre-training with billion-scale SMILES
sequences, compared to pre-training with hundreds
of millions of conformation or graph examples.
MolTRES exhibits state-of-the-art performance on
7 of the 8 tasks. Although MolTRES achieves the
second-best results after SELFormer on the SIDER
task, it outperforms SELFormer by up to 20% on
the others, affirming the superiority of MolTRES.
Moreover, as shown in Table 2, MolTRES con-
sistently stands out in three MoleculeNet regres-
sion tasks, surpassing the state-of-the-art method
MoLFormer-XL by an average of 3.3%. Moreover,
MolTRES-small achieves better performance than
MoLFormer-Base, which contains a commensu-
rate number of parameters, by an average of 5.6%.
The superior performance of SMILES-based meth-
ods is still observed, as they achieve significantly
smaller errors compared to other baseline methods.
This performance gap further verifies the efficacy
of large-scale pre-training on SMILES.
We further compare MolTRES with the base-
lines on QM9, as shown in Table 3. Since quantum
properties are strongly correlated with geometry
information, baselines using ground-truth geom-
etry information (3D Conformation (GT)) show
the best results among baselines. However, obtain-
6Methods ¬µ‚ÜìŒ±‚ÜìŒµhomo‚ÜìŒµlumo‚Üì‚àÜŒµ‚Üì ‚ü®R2‚ü© ‚ÜìZPV E ‚ÜìU0‚ÜìU298‚ÜìH298‚ÜìG298‚ÜìCv‚ÜìAvg.‚Üì
(D) ( a3
0) (eV) (eV) (eV) ( a2
0) (eV) (eV) (eV) (eV) (eV) (cal
mol¬∑K)
3D Conformation (GT)
3D InfoMax (St√§rk et al., 2022) 0.028 0.057 0.259 0.216 0.421 0.141 0.002 0.013 0.014 0.014 0.014 0.030 0.101
GraphMVP (Liu et al., 2022) 0.030 0.056 0.258 0.216 0.420 0.136 0.002 0.013 0.013 0.013 0.013 0.029 0.100
MoleculeSDE (Liu et al., 2023a) 0.026 0.054 0.257 0.214 0.418 0.151 0.002 0.012 0.013 0.012 0.013 0.028 0.100
MoleBlend (Yu et al., 2024) 0.037 0.060 0.215 0.192 0.348 0.417 0.002 0.012 0.012 0.012 0.012 0.031 0.113
UniCorn (Feng et al., 2024) 0.009 0.036 0.130 0.120 0.249 0.326 0.001 0.004 0.004 0.004 0.005 0.019 0.076
3D Conformation (RDKit)
SchNet (Sch√ºtt et al., 2017) 0.447 0.276 0.082 0.079 0.115 21.58 0.005 0.072 0.072 0.072 0.069 0.111 1.915
3D InfoMax (St√§rk et al., 2022) 0.351 0.313 0.073 0.071 0.102 19.16 0.013 0.133 0.134 0.187 0.211 0.165 1.743
MoleculeSDE (Liu et al., 2023a) 0.423 0.255 0.080 0.076 0.109 20.43 0.004 0.054 0.055 0.055 0.052 0.098 1.808
2D Graph
1-GNN (Morris et al., 2019a) 0.493 0.780 0.087 0.097 0.133 34.10 0.034 63.13 56.60 60.68 52.79 0.270 22.43
1-2-3-GNN (Morris et al., 2019a) 0.476 0.270 0.092 0.096 0.131 22.90 0.005 1.162 3.020 1.140 1.276 0.094 2.012
1D SMILES/SELFIES
MoLFormer-XL (Ross et al., 2022) 0.362 0.333 0.079 0.073 0.103 17.06 0.008 0.192 0.245 0.206 0.244 0.145 1.588
MolTRES-small (ours) 0.326 0.295 0.066 0.067 0.085 16.32 0.009 0.133 0.185 0.155 0.164 0.137 1.495
MolTRES (ours) 0.315 0.237 0.054 0.057 0.077 14.60 0.007 0.061 0.071 0.068 0.057 0.121 1.310
Table 3: Evaluation results on QM9 tasks. We report MAE scores (lower is better) following the data splitting used
in Liu et al. (2023a). The best and second-best results are in bold andunderlined . It is important to note that the ‚Äú3D
Conformation (GT)‚Äù results utilize ground-truth geometry information, which incurs non-trivial costs to obtain. For
a fair comparison, we also evaluate the performance of 3D models using the geometry information approximated by
RDKit, denoted as ‚Äú3D Conformation (RDKit)‚Äù, considering scenarios where ground-truth geometry is unavailable.
ing this geometry information involves non-trivial
costs and may not be available in many real-world
scenarios. In these contexts, our MolTRES models
provide the most accurate approximation by only
using SMILES, compared to baselines that esti-
mate geometry information from RDKit or those
without any geometry information, demonstrating
its efficacy and applicability.
4.3 Analysis
To better understand the performance improve-
ments from MolTRES, we conduct a series of
analysis on four MoleculeNet classification tasks:
BBBP, ClinTox, BACE, and SIDER.
Ablation Study. To assess the distinct contribu-
tions of MolTRES‚Äôs components to its enhanced
performance, we conduct ablation studies using
variants of MolTRES as detailed in Table 4. The
results demonstrate that both the DynaMol and
mat2vec integration contribute to performance im-
provements. Moreover, when used jointly, they
offer complementary advantages over employing
either method in isolation. This result underscores
MolTRES‚Äôs effectiveness in addressing the issues
in existing chemical language representation learn-
ing, leading to notable performance improvements.
Effect of mat2vec embedding. We analyze the
effect of the mat2vec embeddings on the pre-
training of MolTRES. As described in Figure 3,DynaMol mat2vec ROC-AUC ‚Üë
‚úì ‚úì 87.99
‚úì - 87.67
- ‚úì 84.82
- - 84.05
Table 4: Performance on MoleculeNet classification
tasks with variants of MolTRES.
mat2vec enables faster convergence, attributed to
the rich features provided by mat2vec that are ben-
eficial for structure modeling. Additionally, when
fully trained, MolTRES with mat2vec achieves
lower training losses and enhanced performance in
MoleculeNet classification tasks. This validates the
effectiveness of integrating mat2vec embeddings.
5 Related Work
In recent years, representation learning has pre-
vailed in numerous applications in natural language
processing (Devlin et al., 2019; Liu et al., 2019)
and computer vision (Dosovitskiy et al., 2021; Bao
et al., 2021). This trend has triggered many stud-
ies in chemical representation learning. The ap-
proaches in this field can be classified into three
categories based on molecular descriptors used
for pre-training: chemical language representation
learning, chemical graph representation learning,
and multi-modal chemical representation learning.
723456789
050K100K150K200KPre-training Loss
Pre-training stepsw/ mat2vecw/o mat2vec
83%84%85%86%87%
050K100K150K200KDownstream ROC-AUC 
Pre-training stepsw/ mat2vecw/o mat2vecFigure 3: Training curves of MolTRES with mat2vec embeddings (the solid line) and without mat2vec embeddings
(the dashed line). The left shows the pre-training loss curves, while the right shows the average ROC-AUC scores.
Chemical language representation learning.
Chemical language representation learning has
adopted pre-training on molecular descriptors rep-
resented as strings, such as SMILES and SELF-
IES. It typically leverages Transformers (Vaswani
et al., 2017) to learn molecular descriptors inspired
by the recent success of large-scale representation
learning in natural language processing. Wang
et al. (2019); Chithrananda et al. (2020); Ross et al.
(2022) have trained Transformer models on large-
scale SMILES sequences. Y√ºksel et al. (2023) have
utilized SELFIES sequences to achieve a better rep-
resentation space. However, the training strategies
for these methods follow the practice of MLM-
style training in natural language processing. Since
chemical language differs from natural language,
current applications of MLM encounter various
issues in pre-training. In this work, we propose
MolTRES to address these issues and consequently
improve molecular property prediction.
Chemical graph representation learning. Re-
searchers in chemical graph representation learning
argue that molecules can naturally be represented
in 2D or 3D graph structures. Thus, they typi-
cally leverage graph neural networks (GNNs) or
Transformers adapted to graphs. Hu et al. (2020)
have introduced a self-supervised task for molecu-
lar graphs, called AttrMask. Morris et al. (2019b)
have introduced higher-order GNNs for distinguish-
ing non-isomorphic graphs. You et al. (2020) have
extended contrastive learning to unstructured graph
data. Wang et al. (2022) have proposed a uni-
fied GNN pre-training framework that integrates
contrastive learning and sub-graph masking. Re-
cent work has focused on modeling 3D graphs, as
they provide more vital information for predict-
ing molecular properties compared to 2D graphs.
(Yang et al., 2024; Zhou et al., 2023) have proposeddenoising auto-encoders for directly modeling 3D
graphs. However, due to the limited scale of 3D
molecular data and its resource-intensive modeling,
the applicability of 3D approaches is limited.
Multi-modal chemical representation learning.
Recently, several studies have proposed learning
chemical representations in a multi-modal manner,
typically leveraging both 2D topology and 3D ge-
ometry of molecules. Liu et al. (2022); St√§rk et al.
(2022); Liu et al. (2023a) have introduced a con-
trastive learning framework that uses 2D graphs
and their corresponding 3D conformations as posi-
tive views, treating those from different molecules
as negative views. (Luo et al., 2022) have pro-
posed encoding both 2D and 3D inputs within a
single GNN model. Another research direction
has involved using both chemical and natural lan-
guages (Edwards et al., 2022; Liu et al., 2023b)
to enrich molecular representations and facilitate
molecule generation using natural language. We
plan to further explore the multi-modal and genera-
tion capabilities of MolTRES based on its versatile
Transformer architecture.
6 Conclusion
In this work, we have proposed a novel chemi-
cal language representation learning framework,
MolTRES, to address the limited scalability
and generalizability of existing methods for pre-
training SMILES transformers. We have presented
two methods, dynamic molecule modeling with
generator-discriminator training, called DynaMol,
and knowledge transfer from scientific literature
based on mat2vec. Our experimental results vali-
date the superiority of our framework over existing
chemical models across a wide range of molecular
property prediction tasks.
8Limitations
While we have demonstrated that MolTRES effec-
tively improves molecular property prediction by
addressing issues in existing chemical language
representation learning methods, some limitations
open promising avenues for future research. First,
several components in MolTRES, such as its mask-
ing strategy or knowledge transfer method, were
chosen empirically in terms of efficiency, and there-
fore may have room for performance improvements
through theoretical or learning-based approaches.
Second, we evaluated a few architectural settings of
MolTRES corresponding to those of MoLFormer-
XL for comparison. Future evaluations could ex-
plore more diverse settings of MolTRES to ac-
commodate various scenarios, including resource-
limited or scalable environments. Finally, a pop-
ular application of SMILES Transformers is in
molecule generation. We plan to investigate the
extension of MolTRES on the pre-training of gen-
erative Transformers for this purpose.
References
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
2021. Beit: Bert pre-training of image transformers.
arXiv preprint arXiv:2106.08254 .
Seyone Chithrananda, Gabriel Grand, and Bharath
Ramsundar. 2020. Chemberta: Large-scale self-
supervised pretraining for molecular property pre-
diction. CoRR .
Kevin Clark, Minh-Thang Luong, Quoc V . Le, and
Christopher D. Manning. 2020. ELECTRA: pre-
training text encoders as discriminators rather than
generators. In 8th International Conference on
Learning Representations .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 4171‚Äì4186.
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al.
2021. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In The
Nineth International Conference on Learning
Representations .
Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke,
Kyunghyun Cho, and Heng Ji. 2022. Translation be-tween molecules and natural language. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 375‚Äì413.
Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong
He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua
Wu, and Haifeng Wang. 2022. Geometry-enhanced
molecular representation learning for property pre-
diction. Nat. Mach. Intell. , 4(2):127‚Äì134.
Shikun Feng, Yuyan Ni, Minghao Li, Yanwen Huang,
Zhi-Ming Ma, Wei-Ying Ma, and Yanyan Lan. 2024.
Unicorn: A unified contrastive learning approach for
multi-view molecular representation learning. arXiv
preprint arXiv:2405.10343 .
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023a.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing. In The Eleventh International Conference on
Learning Representations .
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023b.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing. In The Eleventh International Conference on
Learning Representations .
Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong,
Hongxia Yang, Chunjie Wang, and Jie Tang. 2022.
Graphmae: Self-supervised masked graph autoen-
coders. In KDD ‚Äô22: The 28th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining,
Washington, DC, USA, August 14 - 18, 2022 , pages
594‚Äì604. ACM.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zit-
nik, Percy Liang, Vijay S. Pande, and Jure Leskovec.
2020. Strategies for pre-training graph neural net-
works. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 . OpenReview.net.
John J. Irwin, Teague Sterling, Michael M. Mysinger,
Erin S. Bolstad, and Ryan G. Coleman. 2012. ZINC:
A free tool to discover chemistry for biology. J.
Chem. Inf. Model. , 52(7):1757‚Äì1768.
Johannes Klicpera, Janek Gro√ü, and Stephan G√ºnne-
mann. 2020. Directional message passing for molec-
ular graphs. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Shengchao Liu, Weitao Du, Zhi-Ming Ma, Hongyu Guo,
and Jian Tang. 2023a. A group symmetric stochastic
differential equation model for molecule multi-modal
pretraining. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine
Learning Research , pages 21497‚Äì21526. PMLR.
Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan
Lasenby, Hongyu Guo, and Jian Tang. 2022. Pre-
training molecular graph representation with 3d ge-
ometry. In The Tenth International Conference on
9Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin
Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng
Chua. 2023b. Molca: Molecular graph-language
modeling with cross-modal projector and uni-modal
adapter. In The 2023 Conference on Empirical Meth-
ods in Natural Language Processing .
Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji
Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023c.
Rethinking tokenizer and decoder in masked graph
modeling for molecules. Advances in Neural Infor-
mation Processing Systems , 36.
Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng,
Tie-Yan Liu, Liwei Wang, and Di He. 2022. One
transformer can understand both 2d & 3d molecular
data. In The Eleventh International Conference on
Learning Representations .
Christopher Morris, Martin Ritzert, Matthias Fey,
William L Hamilton, Jan Eric Lenssen, Gaurav Rat-
tan, and Martin Grohe. 2019a. Weisfeiler and leman
go neural: Higher-order graph neural networks. In
Proceedings of the AAAI conference on artificial in-
telligence , volume 33, pages 4602‚Äì4609.
Christopher Morris, Martin Ritzert, Matthias Fey,
William L. Hamilton, Jan Eric Lenssen, Gaurav Rat-
tan, and Martin Grohe. 2019b. Weisfeiler and leman
go neural: Higher-order graph neural networks. In
The Thirty-Third AAAI Conference on Artificial Intel-
ligence , pages 4602‚Äì4609.
Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie,
Ying Wei, Wenbing Huang, and Junzhou Huang.
2020. Self-supervised graph transformer on large-
scale molecular data. In Advances in Neural In-
formation Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan,
Inkit Padhi, Youssef Mroueh, and Payel Das. 2022.
Large-scale chemical language representations cap-
ture molecular structure and properties. Nat. Mac.
Intell. , 4:1256‚Äì1264.
Kristof Sch√ºtt, Pieter-Jan Kindermans, Huziel Enoc
Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert M√ºller. 2017. Schnet:
A continuous-filter convolutional neural network for
modeling quantum interactions. Advances in neural
information processing systems , 30.
Hannes St√§rk, Dominique Beaini, Gabriele Corso, Pru-
dencio Tossou, Christian Dallago, Stephan G√ºnne-
mann, and Pietro Li√≥. 2022. 3d infomax improvesgnns for molecular property prediction. In Interna-
tional Conference on Machine Learning, ICML 2022,
17-23 July 2022, Baltimore, Maryland, USA , volume
162 of Proceedings of Machine Learning Research ,
pages 20479‚Äì20502. PMLR.
Shantanu Thakoor, Corentin Tallec, Mohammad Ghesh-
laghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos,
Petar Veli Àáckovi ¬¥c, and Michal Valko. 2022. Large-
scale representation learning on graphs via bootstrap-
ping. In International Conference on Learning Rep-
resentations .
Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexan-
der Dunn, Ziqin Rong, Olga Kononova, Kristin A.
Persson, Gerbrand Ceder, and Anubhav Jain. 2019.
Unsupervised word embeddings capture latent knowl-
edge from materials science literature. Nat.,
571(7763):95‚Äì98.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017 , pages 5998‚Äì
6008.
Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun,
and Junzhou Huang. 2019. SMILES-BERT: large
scale unsupervised pre-training for molecular prop-
erty prediction. In Proceedings of the 10th ACM
International Conference on Bioinformatics, Com-
putational Biology and Health Informatics , pages
429‚Äì436.
Yuyang Wang, Jianren Wang, Zhonglin Cao, and
Amir Barati Farimani. 2022. Molecular contrastive
learning of representations via graph neural networks.
Nat. Mach. Intell. , 4(3):279‚Äì287.
Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg,
Joseph Gomes, Caleb Geniesse, Aneesh S Pappu,
Karl Leswing, and Vijay Pande. 2018. Moleculenet:
a benchmark for molecular machine learning. Chem-
ical science , 9(2):513‚Äì530.
Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao,
Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. 2023.
Mole-bert: Rethinking pre-training graph neural net-
works for molecules. In The Eleventh International
Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
Junwei Yang, Kangjie Zheng, Siyu Long, Zaiqing Nie,
Ming Zhang, Xinyu Dai, Wei-Yin Ma, and Hao
Zhou. 2024. Mol-ae: Auto-encoder based molecular
representation learning with 3d cloze test objective.
bioRxiv , pages 2024‚Äì04.
Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen,
Zhangyang Wang, and Yang Shen. 2020. Graph con-
trastive learning with augmentations. In Advances
in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing
Systems 2020 .
10Qiying Yu, Yudi Zhang, Yuyan Ni, Shikun Feng, Yanyan
Lan, Hao Zhou, and Jingjing Liu. 2024. Multimodal
molecular pretraining via modality blending. In The
Twelfth International Conference on Learning Repre-
sentations .
Atakan Y√ºksel, Erva Ulusoy, Atabey √únl√º, Gamze
Deniz, and Tunca Dogan. 2023. Selformer: Molec-
ular representation learning via SELFIES language
models. CoRR , abs/2304.04662.
Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang
Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang,
and Guolin Ke. 2023. Uni-mol: A universal 3d
molecular representation learning framework. In The
Eleventh International Conference on Learning Rep-
resentations .
A Appendix
A.1 Detailed Experimental Settings
Pre-training. For pre-processing, we extract
the canonicalized format of SMILES for every
molecule using RDKit. We construct the vocab-
ulary with 2,691 unique tokens plus five special to-
kens (‚Äú<bos>‚Äù, ‚Äú<eos>‚Äù, ‚Äú<pad>‚Äù, ‚Äú<mask>‚Äù, and
‚Äú<unk>‚Äù) after tokenizing all the extracted SMILES
sequences. For tokenization, we use the maximum
sequence length of 512. The weights of our mod-
els are initialized over the normal distribution with
a standard deviation of 0.02. Pre-training is per-
formed using an AdamW optimizer ( Œ≤1= 0.9,
Œ≤2= 0.95), where the maximum learning rate
and weight decay are set to 3e-4 and 0.01, respec-
tively. We use the cosine annealing for learning
rate scheduling with 1,000 warmup steps. The
pre-training time of MolTRES is approximately 15
days using 4 NVIDIA RTX A6000 GPUs.
Evaluation The statistics of evaluation bench-
marks are shown in Table 5 and 6. We use the
scaffold splitting (80% / 10% / 10% for train / val-
idation / test) for all the tasks except for QM9, in
which the random split (80% / 10% / 10% for train
/ validation / test) with thermochemical energy pre-
calculation is used following Liu et al. (2023a). For
evaluation of our models, we extract the output rep-
resentations from model‚Äôs final transformer block
corresponding to the first input token (‚Äú<bos>‚Äù)
as the molecule representations. We use a 2-layer
MLP with the same hidden size and gelu activation
for prediction, whose weights are initialized over
the normal distribution with a standard deviation of
0.02. We use the augmentation of random SMILES
reconstruction for all the tasks. We fine-tune the
models for 500 epochs using an AdamW optimizer
82%84%86%88%
15%25%35%45%55%65%75%85%Downstream ROC-AUC 
Masking ratioFigure 4: Comparison of MolTRES for different mask-
ing ratios on MoleculeNet classification tasks.
82%84%86%88%
5102050100Downstream ROC-AUC
Œª
Figure 5: Comparison of MolTRES for different Œªon
MoleculeNet classification tasks.
(Œ≤1= 0.9,Œ≤2= 0.99) with a weight decay of
0.01. For each task, we empirically choose the
batch size ‚àà {16,32,64,128}and learning rate
‚àà {2e-5,3e-5,5e-5,1e-4}. We report the average
scores after five runs.
Model Architecture. The model architecture of
the generator and discriminator is a Transformer
with linear attention and rotary position embed-
dings. The discriminator of MolTRES has 12 lay-
ers, 768 hidden dimensions, and 12 attention heads.
The discriminator of MolTRES-small has 6 lay-
ers, 768 hidden dimensions, and 12 attention heads.
The generators have half the number of layers in
their corresponding discriminator, while the other
settings are consistent. It is noteworthy that the
generator is only used for pre-training, and the dis-
criminator is fine-tuned and evaluated in all the
downstream tasks. The generator and discriminator
share their embeddings, which is known to be ben-
eficial in accelerating the pre-training (Clark et al.,
2020).
A.2 Additional Experimental Results
Pre-training hyper-parameter analysis. We
study the effect of pre-training hyper-parameters
as shown in Figures 4 and 5. We report ROC-
11Descriptions # tasks # samples
BBBP Blood brain barrier penetration dataset 1 2,039
Tox21 Toxicity measurements on 12 different targets 12 7,831
ToxCast Toxicology data for a large library of compounds 617 8,577
Clintox Clinical trial toxicity of drugs 2 1,478
MUV Maximum unbiased validation group from PubChem BioAssay 17 93,087
HIV Ability of small molecules to inhibit HIV replication 1 41,127
BACE Binding results for a set of inhibitors for Œ≤‚àísecretase 1 1 1,513
SIDER Drug side effect on different organ clases 27 1,427
Table 5: Classification tasks from MoleculeNet.
Descriptions # tasks # samples
QM9 12 quantum mechanical calculations of organic molecules 12 133,885
ESOL Water solubility dataset 1 1,128
FreeSolv Hydration free energy of small molecules in water 1 642
Lipophilicity Octanol/water distribution coefficient of molecules 1 4,200
Table 6: Regression benchmarks from MoleculeNet.
Generator Discriminator ROC-AUC ‚ÜëMAE‚Üì
# layers Hidden size # layers Hidden size (CLS) (REG)
(A)3 6 81.9 0.375
512 512 82.2 0.371
(B)12 384 83.6 0.341
12 512 84.7 0.336
(C)4 83.3 0.343
8 84.5 0.336
12 84.0 0.337
(D) 6 768 12 768 84.8 0.336
Table 7: Variations on the MolTRES architectures. Unlisted values are identical to those of the standard setting of
MolTRES in (D). Following the experimental settings described in Section 4.1, ROC-AUC scores are measured on
eight MoleculeNet classification tasks and MAE scores are measured on three MoleculeNet regression tasks.
AUC scores on four MoleculeNet classification
tasks (BBBP, ClinTox, BACE, and SIDER). First,
in Figure 4, we find that the optimal masking ratio
for MolTRES is 65%. When the masking ratio is
smaller than 65%, we observe that the generator
easily fills masked tokens, resulting in significantly
biased labels towards original. In contrast, when
the masking ratio is larger than 65%, we observe
that there is few evidence in input SMILES tokens
to predict their original molecules, leading to less
effective training. In addition, in Figure 5, we
identify that the optimal value of Œªis 10, different
from the original work on generator-discriminator
training in NLP (Clark et al., 2020) using 50. We
suspect that this is because SMILES modeling typ-
ically shows smaller losses from the generator than
language modeling, and thus we need smaller Œªto
balance the generator and discriminator training.
Architecture analysis. We analyze diverse vari-
ations on the MolTRES architectures, particularly
about the architecture of the generator and dis-criminator. We report ROC-AUC scores on eight
MoleculeNet classification tasks and MAE scores
on three MoleculeNet regression tasks from each
variation. In Table 7, the architecture of our stan-
dard setting used in Section 4 is shown in (D). The
variations in (A) denote training smaller MolTRES
models, showing that reducing layers and hidden
size show comparable performance degradation
when their numbers of parameters are commensu-
rate. Note that we choose to reduce layers, since
it achieves faster model execution speed. The vari-
ations in (B) and (C) are about the architecture of
generators. (B) contains the variations changing
the hidden sizes while using the number of layers
of the discriminator, while (C) contains the varia-
tions changing the numbers of layers while using
the hidden size of the discriminator. In this compar-
ison, we first observe that there is an optimal size of
generators that generate training examples suitably
challenging for discriminators. After empirical in-
vestigation, we choose to set the number of layers
in the generator to half of that in the discriminator.
12