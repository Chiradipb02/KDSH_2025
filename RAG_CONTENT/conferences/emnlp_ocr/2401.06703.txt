Improved Learned Sparse Retrieval with
Corpus-Specific Vocabularies
Puxuan Yu1, Antonio Mallia2∗, and Matthias Petri3
1University of Massachusetts Amherst, USA
2Pinecone, Italy
3Amazon AGI, USA
pxyu@cs.umass.edu ,antonio@pinecone.io ,mkp@amazon.com
Abstract. We explore leveraging corpus-specific vocabularies that im-
prove both efficiency and effectiveness of learned sparse retrieval sys-
tems. We find that pre-training the underlying BERT model on the tar-
get corpus, specifically targeting different vocabulary sizes incorporated
into the document expansion process, improves retrieval quality by up
to 12% while in some scenarios decreasing latency by up to 50%. Our
experiments show that adopting corpus-specific vocabulary and increas-
ing vocabulary size decreases average postings list length which in turn
reduces latency. Ablation studies show interesting interactions between
custom vocabularies, document expansion techniques, and sparsification
objectives of sparse models. Both effectiveness and efficiency improve-
ments transfer to different retrieval approaches such as uniCOIL and
SPLADE and offer a simple yet effective approach to providing new
efficiency-effectiveness trade-offs for learned sparse retrieval systems.
Keywords: Learned sparse retrieval ·Language model vocabulary.
1 Introduction
Sparse term representations such as SPLADE [10], TILDE [41] or uniCOIL [12, 17]
establish competitive retrieval performance using existing sparse retrieval tech-
niques underpinned by standard inverted indexes data structures [42]. The in-
verted index has been optimized to be highly scalable, cost-efficient, update-able
in real-time, and continue to be one of the core first-stage retrieval components
in most commercial search systems today.
One of the key distinctions of state-of-the-art learned sparse representations
compared to traditional ranking functions such as BM25 [32] is the tight integra-
tion between the vocabulary of the inverted index and the one of the model pro-
ducing term importance representations for each document. While BM25 based
inverted indexes contain potentially millions of unique tokens, learned sparse
indexes generally restrict the vocabulary to tokens occurring in the underlying
BERT [5] vocabulary. This vocabulary is usually restricted to say 30,000 entries
to improve model efficiency.
∗Work partly done while working at Amazon Alexa.arXiv:2401.06703v1  [cs.IR]  12 Jan 20242 Puxuan Yu, Antonio Mallia, and Matthias Petri
BT−SPLADE−L
uniCOILCSV−UniCOIL−D
CSV−SPLADE
SPLADE−DOCColBERTv2
0.350.360.370.380.39
15 20 25 30 35
Latency [ms]MRR
Fig. 1. Latency and effectiveness improvements achieved by leveraging Corpus Spe-
cific Vocabularies ( CSV) (with different vocabulary sizes) compared to baseline learned
sparse retrieval models.
While some work has elucidated the link between term score distribution and
learned sparse representations [18, 24], in this work we explore the relationship
between vocabulary selection, retrieval quality, and runtime efficiency of learned
sparse representations.
Contribution. This work provides the following contributions:
–We show the benefit of creating corpus-specific vocabularies to pre-train un-
derlying language models to retrieval quality.
–We explore trade-offs between vocabulary size, pre-training time, document
expansion, and effectiveness improvements.
–We demonstrate that our approach is applicable to many state-of-the-art tech-
niques such as SPLADE , and uniCOIL .
–We propose a corpus-specific modification to TILDE document expansion that
leverages custom vocabularies as well as augmentation of hard negatives at
training time.
–We analyze improvements in retrieval latency resulting from large corpus-
specific vocabularies.
Overall our proposed approach is simple and offers new performance trade-offs
for different learned sparse models (see Figure 1).
2 Background and Related Work
Learned Sparse Models. Usage of pre-trained contextualized language models
(LMs) has resulted in improvements to search effectiveness, albeit with higher re-
trieval costs than traditional lexical models [22]. While models such as BM25 lever-
age term frequency statistics to estimate term importance in a document, LMsImproved Learned Sparse Retrieval with Corpus-Specific Vocabularies 3
can be leveraged to learn the importance of a term in a document by directly
optimizing for the actual retrieval task. These term importance scores form the
basis of many learned sparse retrieval techniques that still leverage the inverted
index for query processing. Such models include SPLADE [10], TILDE [41], DeepIm-
pact[23] or uniCOIL [12, 17] which differ in their handling of document and query
processing, vocabulary selection, and training objective but offer state-of-the-
art retrieval performance while providing different efficiency and effectiveness
trade-offs.
Pre-training. Pre-training refers to allowing a model to learn general lan-
guage representations by performing tasks such as Masked Language Modeling
(MLM) on large text corpora. In the search setting, techniques such as coCon-
denser [11] provide additional search-specific pre-training tasks to improve the
performance of LMs on the actual retrieval task. Such pre-training objectives
may operate on the target retrieval corpus, or larger potentially out-of-domain
text corpora. Recent work has explored the relationship of vocabulary size in
standard-pretraining arrangements [8] as well as the notion of rare-terms in pre-
training requiring special consideration [39].
Document Expansion. To mitigate the vocabulary mismatch problem [40],
learned sparse representations perform document expansion to augment the doc-
ument with potentially relevant (future query) tokens. The DocT5Query [27] tech-
nique augments documents with tokens by appending generated queries from the
source document, while TILDE [41] directly optimizes for both term importance
estimation and document expansion.
Inverted Index and Dynamic Pruning. Theinverted index stores one post-
ings list for each unique term tproduced by a ranking model. Each postings list
comprises a sequence of the document ID and corresponding term importance
score pairs [30, 42]. During query processing, the posting lists of all query terms
are processed to retrieve the top- khighest-scoring documents. Query processing
algorithms such as the MaxScore [37] or BlockMaxWand [7] dynamic pruning mech-
anisms enable skipping of large sections of postings lists. However, a relationship
still exists between the length of each postings list and overall query latency [36].
3 Corpus-specific Vocabularies
This section introduces the notion of Corpus-Specific Vocabularies (CSV)
and shows how it can be incorporated into different aspects of the overall training
procedures of sparse retrieval models: vocabulary selection, pre-training, docu-
ment expansion, and model training (see Figure 2 for an overview). We find that
CSV provides greater coverage of query terms, can be easily incorporated into
training procedure of different models, and better correspond to the actual us-
age of the vocabulary entries in the downstream ranking task inside the inverted
index.4 Puxuan Yu, Antonio Mallia, and Matthias Petri
Step 1: Train WordPiece on Target Corpus
Step 2: Pre-train BERT (w/ New Vocabulary)on Target CorpusuniCOILSPLADETILDEInitializeInitializeTerm Scoring + Doc ExpansionTerm ScoringDoc Expansion+Step 3: Initialize Models w/ BERT and Finetune
Step 4: Create an Inverted IndexBased on Corpus-speciﬁcVocabulary (CSV)InvertedIndices
Fig. 2. A high-level overview of the workflow described in this work. As the vocabulary
of the language model is learned on the target retrieval corpus, and that the sparse
retrieval models (e.g., SPLADE anduniCOIL ) and the document expansion models (e.g.,
TILDE ) all use the language model as backbones, all the components in the learned
sparse retrieval systems, including the acquired inverted index, are influenced by the
corpus-specific vocabulary ( CSV).
3.1 Vocabulary Selection
Before the advent of learned sparse models based on language models, it was
common practice for an inverted index to contain lists for all unique tokens in the
target corpus. Standard text collections such as Gov2 contain 25 million unique
tokens [28] comprising parsing errors, named entities, numbers, etc. Indexing
these unique tokens has the benefit of being able to precisely (and efficiently) re-
trieve documents containing rare tokens, a key benefit of sparse retrieval models
over alternative dense retrieval systems.
On the contrary, due to computational restrictions (parameter memory usage,
softmax inefficiencies among others) associated with Transformers, it is common
practice to limit the number of unique tokens fed into such models to only tens
of thousands (e.g., 30,000 in the case of standard BERT [5]). Algorithms such as
byte-pair encoding (BPE) [35] or WordPiece [38] have been developed to tokenize
text into sub-word units, to minimize the occurrence of out-of-vocabulary tokens
during text processing with such limited vocabulary sizes. These vocabulary
size restrictions are generally non-problematic, as these sub-word tokens are
represented in the context of word sequences during standard NLP tasks such
as machine translation.
However, in the context of sparse retrieval models such as uniCOIL , this con-
textualization of sub-word tokens only takes place at training time. At retrieval
time when using a standard inverted index, each token is processed in isolation.
We propose to adjust the vocabulary used in sparse models such as uni-
COIL (and the underlying language model) to better account for this mismatch.
For simplicity, we train WordPiece tokenizers on our target corpus with vary-
ing, larger vocabulary sizes. While vocabulary selection could be enhanced by
incorporating other signals such as query logs and term frequency counts into
the learning process, we seek to isolate the effect of vocabulary size in this work
and leave these extensions to future work. We refer to this process as leverag-Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies 5
ing Corpus-Specific Vocabularies ( CSV). In this work, we specifically experiment
with vocabulary sizes of 30 ,000, 100 ,000, and 300 ,000. As we will show in de-
tail in Section 4, increasing vocabulary size has positive effects on both retrieval
quality and runtime latency.
3.2 Pre-training Objectives
Since our model employs a different vocabulary from BERT, we cannot use pre-
trained BERT checkpoints. BERT is pre-trained with two objectives in mind:
Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) [5].
Pre-training is usually performed on the BooksCorpus (800M words) and En-
glish Wikipedia (2,500M words) datasets. Models such as coCondenser [11] and
SPLADE [9, 10, 15] begin with a pre-trained BERT checkpoint and undergo
further pre-training on the retrieval corpus, which is sometimes referred to as
“middle-training” [15].
In this study, we bypass the pre-training step on large, out-of-domain text
corpora (e.g. BooksCorpus and Wikipedia) and only pre-train on the target re-
trieval corpus. We make this choice due to (1) the aim of mitigating the cost and
environmental impact associated with pre-training multiple LMs using multiple
different vocabularies on large corpora, and (2) empirical evidence suggesting
that LMs pretrained from scratch on the retrieval corpus exhibit improved ef-
fectiveness in retrieval tasks [16].
Note that pre-training with large vocabulary sizes can be computationally
expensive but methods such as hierarchical or sampled softmax are standard,
drop-in replacements for softmax cross entropy which improves scalability with
regard to the number of classes (vocabulary entries).
3.3 Sparse Retrieval Models
We purpose to use CSV to enhance the efficiency and effectiveness of sparse
retrieval models that rely on the underlying language model vocabulary as
their index vocabulary. To demonstrate this, we leverage uniCOIL [12, 17] and
SPLADE [9, 10] as examples. uniCOIL assigns an impact score to each query and
passage token, and discards tokens with a non-positive impact. It relies on an
additional model for document expansion.
SPLADE , on the other hand, projects queries and passages into |V|-dimensional
embeddings, where |V|is the vocabulary size of the underlying LM, and calcu-
lates the matching score based on the dot product of these embeddings. To
“sparsify” these dense representations for efficiency, SPLADE employs FLOPS
regularizers [29] to restrict the number of tokens with non-zero weights. Unlike
uniCOIL ,SPLADE performs query and document expansion automatically, with-
out the need for prior corpus expansion. Because query expansion significantly
influences retrieval efficiency, which is not the focus of this study, we limit our
experiments to SPLADE -DOC proposed in SPLADE-v2 [9].SPLADE -DOC only per-
forms token weighting and expansion on the passage side and assigns uniform
weight to query tokens without expansion. Note that changing vocabulary size6 Puxuan Yu, Antonio Mallia, and Matthias Petri
affects how SPLADE should be regularized, as the FLOPS loss is calculated by
summing the square of a token’s average absolute weight in a mini-batch across
the vocabulary .
3.4 Document Expansion
We use TILDE [41] for document expansion with uniCOIL , as it requires fewer re-
sources for both training and inference and provides comparable performance to
DocT5Query [27]. TILDE is initially trained using labeled relevant query-passage
pairs. However, the shallow labeling of MS MARCO [1] and the presence of false
negatives [21] make this approach restricted. To enhance TILDE for document
expansion, we propose to aggregate the rankings of 12 dense rankers [31] using
the Borda Count [2] into a consolidated ranking, and then use the top 10 pas-
sages from this ranking as training signals to train TILDE . We choose the Borda
Count for ranking aggregation due to its simplicity. Note that TILDE can also
leverage CSV as it predicts additional document tokens over the underlying LM
vocabulary space which we adjust and fine-tune to the target corpus. We refer
to this approach as Corpus-Specific Document Expansion leveraging aug-
mentation ( TILDE-AUG-CSV ) as both the vocabulary used during expansion and
the underlying expansion model fit directly to the target corpus. The positive
benefits of these enhancements will be explored in detail in Section 4.
3.5 Distillation-based Training
Training a student model using the outputs of a trained teacher ranking model
as training signals can considerably enhance learning outcomes [9, 15, 34]. We
use KL Divergence [13, 14] as the training loss and a standard cross-encoder [26]
as the teacher to train the uniCOIL and SPLADE-DOC models as suggested by Las-
sance and Clinchant [15].
Overall we seek to apply CSV to a variety of state-of-the-art techniques,
showing they are broadly applicable and generalize to different approaches, which
we will show in Section 4.
4 Experiments
4.1 Setup
Datasets. We use the MS MARCO v1 (referred to as MSM in tables; 8 .8Mpas-
sages) and MS MARCO v2 (138Mpassages) collections. For evaluation, we mainly
use the 6,980 queries from the MS MARCO v1 Dev set. We also use the test queries
of TREC 2019 [4] and 2020 [3] from the TREC Deep Learning track.Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies 7
Latency experiments. We use the PISA engine [25] which substantially out-
performs Lucene in terms of space usage and runtime efficiency for retrieval
over learned sparse indexes [22]. We use the state-of-the-art BlockMaxWand [7]
dynamic-pruning based query algorithm. All our indexes leverage recursive-
graph-bisection [6, 19, 20] to optimize efficiency. Our code and experimental
setup is available at https://github.com/PxYu/CSV-for-LSR-ECIR24 . We re-
port latency as mean retrieval time (MRT) in ms averaged over 5 runs.
Hardwares. All models are trained on 8 ×A100 GPUs, whereas our latency
experiments are performed on an Intel Xeon 8375C CPU in single-threaded
execution mode.
Models and Baselines. While adjusting the underlying LM and pretraining
on the target corpus is general, we focus specifically on the impact on sparse
retrieval models. Here we experiment with SPLADE (abbreviated as SPL) and
uniCOIL (abbreviated as uCOIL ). For each, we experiment with vocabulary sizes
of 30,000, 100 ,000, and 300 ,000. We pre-train our CSVmodels using the MLM
objective (with a 15% masking probability) on MS MARCO (497M words) (and
MS MARCO v2 as indicated) for 10 epochs. uniCOIL models can also leverage
TILDE-AUG-CSV (abbreviated as TILDE-A ) document expansion by first training
TILDE-AUG-CSV with query likelihood and document likelihood for 5 epochs and
take the top-200 tokens predicted in the document likelihood distribution as ex-
pansion, ignoring stop-words, sub-words and tokens in the original passage [41].
We also train a distillation based version of uniCOIL (abbreviated uCOIL-D ) as
discussed in Section 3.
As baselines, we retrain a uniCOIL model with the standard BERT vocabulary
(to compare to our 30 ,000 vocabulary models, to which it has similar vocabulary
size). We additionally report numbers for existing BT-SPLADE-L model [15] as
competitive efficient and effective baselines. We also compare to standard BM25 ,
DocT5Query and ColBERTv2 [34] baselines.
4.2 Vocabulary Selection and Index Statistics
First, we explore the effect of vocabulary selection on different index and query
statistics without document expansion. Table 1 shows query statistics for a uni-
COIL index (trained only on MS MARCO ; no document expansion) with different
vocabularies. We observe that the mean number of query tokens decreases as
the vocabulary size increases. The number of queries where any sub-word token
(compared to only having full word tokens) is present decreases from 48% for
the default BERT vocabulary to 35% for our custom vocabulary of the same
size. Larger vocabularies further decrease the number of queries containing sub-
word tokens to 11% and 2% respectively. We also observe that passage length,
postings per query, and mean retrieval time (MRT) decrease as the vocabulary
size increases. Also, note that a custom vocabulary with 30k tokens outperforms
the regular BERT-30k vocabulary on all metrics. Overall, CSV-300k is 20% faster
compared to a standard BERT-30k based uniCOIL model.8 Puxuan Yu, Antonio Mallia, and Matthias Petri
Table 1. Mean query length ( |Q|), percentage of split queries, passage length ( |D|, in
terms of tokens), postings per query, and MRT. Metrics are derived from uniCOIL mod-
els without document expansion.
Vocab |Q|%Split Qrys |D|Postings MRT
BERT-30K 7.02 48.27% 47.6 6 ,482,729 22.88
CSV-30K 6.68 35.50% 46.2 6 ,207,331 19.70
CSV-100K 6.29 11.36% 42.5 5 ,502,811 18.66
CSV-300K 6.17 2.36% 41.3 5 ,118,462 18.62
Table 2. MRR and MRT for different pre-training, document-expansion and uniCOIL
training objectives.
# Vocab Pretrain D. Exp. Model MRR MRT
1 BERT BERT TILDE uCOIL 0.354 33.88
2 BERT MSM TILDE uCOIL 0.343 29.29
3CSV-100K MSM - uCOIL 0.332 18.66
4CSV-100K MSM TILDE uCOIL 0.353 22.65
5CSV-100K MSM+v2 TILDE uCOIL 0.370 22.45
6CSV-100K MSM+v2 TILDE-A uCOIL 0.376 19.88
7CSV-100K MSM+v2 TILDE-A uCOIL-D 0.391 18.85
4.3 Retrieval Quality
Table 2 explores pretraining, document expansion, and model distillation. Note
that we omit certain configurations and metrics that do not provide additional
insights to simplify presentation. Rows #1 and #2 represent reproduced stan-
dard baselines for reference.
First, comparing CSV-100k with no document expansion (row #3) and CSV-
100k with standard TILDE expansion (row #4), we observe that latency increases
but retrieval quality improves. Both pre-training on MS MARCO v2 (row #5)
and augmented document expansion ( TILDE-A ) improve retrieval quality while
remaining latency neutral or improving latency. Finally, we replace regular uni-
COIL with a version trained with distillation ( uCOIL-D , row #7). uCOIL-D provides
the best retrieval quality (0 .391 MRR). Note that this is competitive to state-of-
the-art late interaction models such as ColBERTv2 [34] (0.397 MRR on the same
task). In subsequent experiments, we restrict our analysis and presentation to
pre-training on MSM+v2 , expansion using TILDE-A and uCOIL-D .
Table 3 shows the effect of increasing vocabulary sizes on uniCOIL based
models. No substantial difference between the CSV-100k (row #10) and CSV-
300k (rows #11-#13) can be observed, which indicates that 100 kis a sufficient
vocabulary size for MS MARCO v1 . We also observe that latency increases forImproved Learned Sparse Retrieval with Corpus-Specific Vocabularies 9
Table 3. MRR and MRT for different custom vocabulary sizes and document expansion
limits. # Kept Tokens refers to the number of expansion tokens provided by TILDE-
Athat are actually used for document expansion. It acts as a hyperparameter to
control the balance between effectiveness and efficiency under the same vocabulary.
# Vocab # Tilde Tokens # Kept Tokens MRR MRT
8BERT-30K 37.5 37.5 0.379 20.00
9 CSV-30K 41.6 40.0 0.389 18.69
10 CSV-100K 46.6 40.0 0.389 17.24
11 CSV-300K 90.7 40.0 0.388 17.06
12 CSV-300K 90.7 50.0 0.391 18.72
13 CSV-300K 90.7 90.7 0.392 22.08
CSV-300k (row #13) compared to CSV-30k (row #9), which is contrary to the
numbers reported in Table 1. We find TILDE-A expansion increases document size
substantially with larger vocabulary size (90 .71 extra tokens on average for CSV-
300k, 41.63 for CSV-30k ). This increase counteracts the decrease in postings list
lengths we obtained through increasing vocabulary size. Adjusting the TILDE-A
hyperparameter to only expanding with the top-40/50 tokens, in rows #10 -
#12 we see latency in line with CSV-30k for larger vocabularies while showing a
negligible improvement in retrieval performance. In summary, for MS MARCO v1 ,
leveraging a custom vocabulary (rows #9 compared to #8) is more important
to improving retrieval quality compared to increasing vocabulary size, which
however has a positive impact on latency.
Table 4 shows the effect of corpus-specific vocabulary in different sizes on
SPLADE . Similarly, as with uniCOIL (row #8 and #9), the CSVmodel (row #16)
outperforms the model with BERT vocabulary in similar size (row #17) in terms
of MRR and MRT. Again, retrieval quality does not increase with larger vocab-
ulary sizes, however, the CSV-300k version (rows #20 and #21) is roughly 40%
faster than the comparable SPL-DOC baseline (row #15). This effect is related
to the FLOPS sparsity regularization leveraged by SPLADE interacting with vo-
cabulary size. Experimenting with different regularization strengths ( λd) while
trying to keep MRR roughly constant (rows #17 - #21), we find larger vocab-
ularies (300 k) result in improved retrieval speed (13 msvs 25 ms).
Similarly, Table 5 shows that our improvements also transfer to the TREC
query sets. While standard BM25 and DocT5Query are still faster, CSV reduces
mean latency relative to regular uniCOIL by 50% (17 .63msvs 33 .73ms) and im-
proves over state-of-the-art BT-SPLADE-L method. We conduct Bonferroni cor-
rected pairwise t-tests, and report significance with p <0.05.
4.4 Query Latency
Previous experiments show that CSVwith both 100k and 300k tokens substan-
tially reduces the latency of existing approaches. For example, standard uni-10 Puxuan Yu, Antonio Mallia, and Matthias Petri
Table 4. MRR and MRT for several SPLADE based
methods.
# Method Vocab λd MRR MRT
14 BT-SPLADE-L [15] 0.380 27.62
15†SPL-DOC BERT-30K 0.008 0.347 21.03
16 SPL-DOC BERT-30K 0.008 0.339 27.28
17 SPL-DOC CSV-30K 0.008 0.356 25.39
18 SPL-DOC CSV-100K 0.009 0.358 21.65
19 SPL-DOC CSV-300K 0.006 0.359 18.47
20 SPL-DOC CSV-300K 0.007 0.357 13.12
21 SPL-DOC CSV-300K 0.008 0.354 14.21
†This is initialized with a DistilBERT model
that is further pretrained on MSMARCO using
MLM+FLOPS [15]. In comparison, row #16 is ini-
tialized with a BERT model that is only pretrained
on MSMARCO v2 using MLM.
COIL with BERT vocabulary (row 1; Table 2) exhibit a mean response time of
33.88ms, whereas our fastest method uniCOIL based method reduces mean re-
sponse time to 17 .06ms (row 11; Table 3), a 50% reduction. Similarly, SPLADE
enhanced by CSVexhibits similar latency improvements (see Table 4). For com-
parison, ColBERTv2 [34] accelerated by PLAID [33] provides similar effectiveness
but is substantially slower (185ms single CPU; not run by us).
We observe that CSV-300K results in more lists (due to having a larger vocab-
ulary) with larger list max scores referring to maximum score a term is assigned
in any document in the collection as shown in Figure 3. This also creates more
skewed list max score distribution (the score “band” in Figure 3 is more narrow
for the standard BERT vocabulary) which is essential as pruning algorithms use
list max scores to skip over low-scoring documents [7].
This has a direct effect on runtime performance which can be observed in
the run-time statistics of the MaxScore algorithm shown in Table 6. Note that
methods uniCOIL and BT-SPLADE-L which leverage smaller vocabularies score
substantially more documents. This would be especially impactful in the case
where a non trivial scoring function (e.g. scoring discovered documents with a
more expensive secondary model as in proposed by Mallia et al. [24]) is used to
score documents.
Interestingly, operation Insert which counts the number of insertions into the
final top- kresult heap during processing are similar. While more documents are
scored, the amount of documents inserted into the resulting heap stays similar.
This is an artifact of list max scores (plotted in Figure 3) being used to determine
if a document should be scored and larger vocabularies provide more fine-grainedImproved Learned Sparse Retrieval with Corpus-Specific Vocabularies 11
Table 5. nDCG@10 and MRT, for TREC 19&20 queries. The symbol ▽denotes a sig.
difference viz. uCOIL-D-CSV-300K (#13) .
Strategy TREC 2019 TREC 2020
nDCG MRT nDCG MRT
BM25 0.501▽4.93 0.487▽7.94
DocT5Query 0.643▽4.87 0.607▽7.80
UniCOIL-TILDE 0.660▽31.59 0.647▽33.73
BT-SPLADE-L 0.703 26.91 0.698 27.60
uCOIL-D-CSV-100K (#10) 0.718 15.00 0.706 18.01
uCOIL-D-CSV-300K (#11) 0.722 14.46 0.708 17.63
uCOIL-D-CSV-300K (#13) 0.729 17.55 0.728 22.13
0.000.250.500.751.00
100 200 300 400 500
List Max ScoreProp. of ListsBERT
uCOIL−D−CSV−300K
Fig. 3. Cumulative distribution of lists that have list max scores higher than a given
value. BERT displaying less skew in list max scores which negatively affects perfor-
mance.
“decision boundaries” as fewer high and low-scoring terms are conflated into a
single vocabulary entry.
4.5 Pre-training Cost and Model Size
Not using existing LM checkpoints requires more time and resources for pre-
training. We pretrain each LM on MS MARCO for 10 epochs with MLM. For
experiments with uniCOIL , we train TILDE for 5 epochs for document expansion,
and train uniCOIL for 5 epochs on the expanded corpus for retrieval. SPL-DOC
is trained for 50k iterations using our pre-trained LM. We spend 4-13 hours
pretraining LMs of different sizes due to the computational overhead of larger
vocabulary sizes. Our pretraining does not currently leverage standard sam-
pling/hierarchical softmax strategies used to deal with a large number of cate-
gories, which increases the cost. Increasing vocabulary size also increases model12 Puxuan Yu, Antonio Mallia, and Matthias Petri
Table 6. Query processing statistics (avg per query) for MaxScore and three index
varieties.
Strategy SCORE INSERT NEXT NEXT-GEQ
uniCOIL 7,004,022 301 ,650 6 ,575,301 2 ,048,790
uCOIL-D-CSV-300K 4,836,943 450 ,567 4 ,554,723 1 ,336,857
BT-SPLADE-L 6,400,831 556 ,829 6 ,110,824 1 ,175,790
parameter size from 109M to 316M, similar to BERT-large with a 30k vocabulary.
We experimented with pre-training BERT-large on our corpus to obtain a base-
line with similar parameter size, but found that the MS MARCO corpora were too
small to pre-train a model of this size. Note that search-specific pre-training tasks
such as coCondenser [11] provide orthogonal benefits to vocabulary changes. We
leave exploring potential interactions of these techniques to future work.
5 Conclusion and Future Work
We demonstrate that corpus-specific vocabularies are effective at improving both
retrieval quality and query latency of learned sparse retrieval systems. They are
simple yet effective and can be applied to a variety of different modeling types.
We believe there is a large body of future work exploring the effect of the
vocabulary on sparse retrieval models. Promising directions are developing more
sophisticated vocabulary selection strategies and training and document expan-
sion strategies that take underlying inverted index-based retrieval into account
when assigning term weights.Bibliography
[1] Arabzadeh, N., Vtyurina, A., Yan, X., Clarke, C.: Shallow pooling for sparse
labels. Information Retrieval 25(4), 365–385 (2022)
[2] Aslam, J., Montague, M.: Models for metasearch. In: Proc. ACM Int. Conf.
on Information and Knowledge Management (CIKM), pp. 276–284 (2001)
[3] Craswell, N., Mitra, B., Yilmaz, E., Campos, D.: Overview of the trec 2020
deep learning track. arXiv:2102.07662 (2021)
[4] Craswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E.M.: Overview
of the trec 2019 deep learning track. arXiv:2003.07820 (2020)
[5] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of
deep bidirectional transformers for language understanding. pp. 4171–4186
(2019)
[6] Dhulipala, L., Kabiljo, I., Karrer, B., Ottaviano, G., Pupyrev, S., Shalita,
A.: Compressing graphs and indexes with recursive graph bisection. In:
Proc. Conf. on Knowledge Discovery and Data Mining (KDD), pp. 1535–
1544 (2016)
[7] Ding, S., Suel, T.: Faster top- kdocument retrieval using block-max indexes.
In: Proc. ACM Int. Conf. on Research and Development in Information
Retrieval (SIGIR), pp. 993–1002 (2011)
[8] Feng, Z., Tang, D., Zhou, C., Liao, J., Wu, S., Feng, X., Qin, B., Cao,
Y., Shi, S.: Pretraining without wordpieces: Learning over a vocabulary of
millions of words (2022)
[9] Formal, T., Lassance, C., Piwowarski, B., Clinchant, S.: SPLADE v2: Sparse
lexical and expansion model for information retrieval. arXiv:2109.10086
(2021)
[10] Formal, T., Piwowarski, B., Clinchant, S.: SPLADE: Sparse lexical and ex-
pansion model for first stage ranking. In: Proc. ACM Int. Conf. on Research
and Development in Information Retrieval (SIGIR), pp. 2288–2292 (2021)
[11] Gao, L., Callan, J.: Unsupervised corpus aware language model pre-training
for dense passage retrieval. pp. 2843–2853 (2022)
[12] Gao, L., Dai, Z., Callan, J.: COIL: Revisit exact lexical match in information
retrieval with contextualized inverted list. pp. 3030–3042 (2021)
[13] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural net-
work. arXiv:1503.02531 (2015)
[14] Kullback, S., Leibler, R.: On information and sufficiency. The annals of
mathematical statistics 22(1), 79–86 (1951)
[15] Lassance, C., Clinchant, S.: An efficiency study for splade models. In: Proc.
ACM Int. Conf. on Research and Development in Information Retrieval
(SIGIR), pp. 2220–2226 (2022)
[16] Lassance, C., D´ ejean, H., Clinchant, S.: An experimental study on pretrain-
ing transformers from scratch for IR. arXiv:2301.10444 (2023)
[17] Lin, J., Ma, X.: A few brief notes on DeepImpact, COIL, and a conceptual
framework for information retrieval techniques. arXiv:2106.14807 (2021)14 Puxuan Yu, Antonio Mallia, and Matthias Petri
[18] Mackenzie, J., Mallia, A., Moffat, A., Petri, M.: Accelerating learned sparse
indexes via term impact decomposition. pp. 18–27 (2022)
[19] Mackenzie, J., Mallia, A., Petri, M., Culpepper, J.S., Suel, T.: Compressing
inverted indexes with recursive graph bisection: A reproducibility study. In:
Proc. European Conf. on Information Retrieval (ECIR), pp. 339–352 (2019)
[20] Mackenzie, J., Petri, M., Moffat, A.: Faster index reordering with bipartite
graph partitioning. In: Proc. ACM Int. Conf. on Research and Development
in Information Retrieval (SIGIR), pp. 1910–1914 (2021)
[21] Mackenzie, J., Petri, M., Moffat, A.: A sensitivity analysis of the MS-
MARCO passage collection. arXiv:2112.03396 (2021)
[22] Mackenzie, J., Trotman, A., Lin, J.: Wacky weights in learned sparse
representations and the revenge of score-at-a-time query evaluation.
arXiv:2110.11540 (2021)
[23] Mallia, A., Khattab, O., Tonellotto, N., Suel, T.: Learning passage impacts
for inverted indexes. In: Proc. ACM Int. Conf. on Research and Develop-
ment in Information Retrieval (SIGIR), pp. 1723–1727 (2021)
[24] Mallia, A., Mackenzie, J., Suel, T., Tonellotto, N.: Faster learned sparse
retrieval with guided traversal. In: Proc. ACM Int. Conf. on Research and
Development in Information Retrieval (SIGIR), pp. 1901–1905 (2022)
[25] Mallia, A., Siedlaczek, M., Mackenzie, J., Suel, T.: PISA: Performant in-
dexes and search for academia. In: Proc. OSIRRC at SIGIR 2019, pp. 50–56
(2019)
[26] Nogueira, R., Cho, K.: Passage re-ranking with bert. arXiv preprint
arXiv:1901.04085 (2019)
[27] Nogueira, R., Lin, J.: From doc2query to docTTTTTquery (2019)
[28] Ottaviano, G., Venturini, R.: Partitioned Elias-Fano indexes. In: Proc. ACM
Int. Conf. on Research and Development in Information Retrieval (SIGIR),
pp. 273–282 (2014)
[29] Paria, B., Yeh, C., Yen, I., Xu, N., Ravikumar, P., P´ oczos, B.: Minimizing
flops to learn efficient sparse representations. arXiv:2004.05665 (2020)
[30] Pibiri, G.E., Venturini, R.: Techniques for inverted index compression. ACM
Computing Surveys 53(6), 125.1–125.36 (2021)
[31] Reimers, N.: MS MARCO Passages Hard Negatives. In: Hugging-
Face, pp. 1747–1756 (2021), URL https://huggingface.co/datasets/
sentence-transformers/msmarco-hard-negatives
[32] Robertson, S.E., Zaragoza, H.: The probabilistic relevance framework:
BM25 and beyond. Foundations & Trends in Information Retrieval 3, 333–
389 (2009)
[33] Santhanam, K., Khattab, O., Potts, C., Zaharia, M.: PLAID: an efficient
engine for late interaction retrieval. In: Proc. ACM Int. Conf. on Information
and Knowledge Management (CIKM), pp. 1747–1756, ACM (2022)
[34] Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., Zaharia, M.: Col-
bertv2: Effective and efficient retrieval via lightweight late interaction. In:
NAACL, pp. 3715–3734 (2022)
[35] Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare
words with subword units (2016)Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies 15
[36] Siedlaczek, M., Mallia, A., Suel, T.: Using conjunctions for faster disjunctive
top-k queries. In: Proc. Conf. on Web Search and Data Mining (WSDM),
pp. 917–927 (2022)
[37] Turtle, H.R., Flood, J.: Query evaluation: Strategies and optimizations.
Information Processing & Management 31(6), 831–850 (1995)
[38] Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun,
M., Cao, Y., Gao, Q., Macherey, K.: Google’s neural machine translation
system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144 (2016)
[39] Yu, W., Zhu, C., Fang, Y., Yu, D., Wang, S., Xu, Y., Zeng, M., Jiang, M.:
Dict-bert: Enhancing language model pre-training with dictionary (2022)
[40] Zhao, L.: Modeling and solving term mismatch for full-text retrieval. SIGIR
Forum 46(2), 117–118 (2012)
[41] Zhuang, S., Zuccon, G.: TILDE: Term independent likelihood moDEl for
passage re-ranking. In: Proc. ACM Int. Conf. on Research and Development
in Information Retrieval (SIGIR), pp. 1483–1492 (2021)
[42] Zobel, J., Moffat, A.: Inverted files for text search engines. ACM Computing
Surveys 38(2), 6:1–6:56 (2006)