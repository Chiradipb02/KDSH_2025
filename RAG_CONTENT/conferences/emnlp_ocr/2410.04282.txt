Locating Information Gaps and Narrative Inconsistencies Across
Languages: A Case Study of LGBT People Portrayals on Wikipedia
Farhan Samir1,4*Chan Young Park2Anjalie Field3
Vered Shwartz1,4Yulia Tsvetkov2
1University of British Columbia2University of Washington
3Johns Hopkins University4Vector Institute for AI
fsamir@cs.ubc.ca
Abstract
To explain social phenomena and identify sys-
tematic biases, much research in computational
social science focuses on comparative text anal-
yses. These studies often rely on coarse corpus-
level statistics or local word-level analyses,
mainly in English. We introduce the INFOGAP
method—an efficient and reliable approach to
locating information gaps and inconsistencies
in articles at the fact level, across languages.
We evaluate INFOGAPby analyzing LGBT peo-
ple’s portrayals, across 2.7K biography pages
on English, Russian, and French Wikipedias.
We find large discrepancies in factual coverage
across the languages. Moreover, our analysis
reveals that biographical facts carrying negative
connotations are more likely to be highlighted
in Russian Wikipedia. Crucially, INFOGAP
both facilitates large scale analyses, and pin-
points local document- and fact-level informa-
tion gaps, laying a new foundation for targeted
and nuanced comparative language analysis at
scale.1
1 Introduction
Wikipedia has several hundred language editions,
a sizeable number of which have more than 100K
articles. Despite its “neutral point of view” policy,
abundant evidence of content discrepancies across
language editions has been well-documented on
the platform (e.g., Hecht and Gergle, 2010; Calla-
han and Herring, 2011; Eom et al., 2015; Wagner
et al., 2015; Park et al., 2021). There are numer-
ous motivations for identifying and studying these
variations, e.g., identifying content variations and
gaps can aid editors in removing social and cul-
tural biases (Field et al., 2022). Alternatively, from
a social science perspective, comparative analy-
ses of prominent topics across Wikipedia language
editions provides a window into studying cross-
*Work done while visiting the University of Washington.
1https://github.com/smfsamir/infogap
During the season,  
Griner had recorded the sixth  
triple-double in WNBA history . 
Griner became engaged to 
Cherelle W atson in August 2018.  Стейн прорвался мимо  
телохранителей команды и 
обвинял Г райнер в том, 
что она "ненавидит Америку".  
(Stein stormed past the team's  
bodyguards and accused  
Greiner of 'hating America'. )
En août 2018, Griner se fiance avec 
Cherelle W atson. Бриттни родилась  18 октября 1990 
Griner est désignée meilleure joueuse  
de l'année par l'Associated Press 
(Griner was named the Associated  
Press W omen's Player of the Y ear.) Griner was born 
October 18, 1990. Brittney Griner est née 
le 18 octobre 1990. 
Ру сский 
F r ançais English Figure 1: We propose a method, INFOGAP, to locate
fact (mis)alignments in Wikipedia biographies in differ-
ent language versions. INFOGAPidentifies facts that are
common to a pair of articles (“Griner was born on Octo-
ber 18, 1990”), and facts unique to one language version
(“Griner had recorded the sixth triple-double”; Enonly)
enabling further analysis of information gaps, editors’
selective preferences within articles, and analyses at
scale across languages, cultures, and demographics.
cultural differences at scale (Callahan and Herring,
2011).
Existing methods for examining cross-language
differences and gaps across Wikipedias rely on ag-
gregate statistics, such as the number of languages
an article is available in (Wagner et al., 2015), sum-
mary metrics of positive and negative connotations
(Park et al., 2021), text complexity measures (Kim
et al., 2016; Field et al., 2022), or differences in hy-
perlink graph structures (Hecht and Gergle, 2010;
Laufer et al., 2015). While these metrics are use-
ful for understanding broad trends, they do not
facilitate nuanced comparative analysis, failing to
inform readers and editors how the content they
engage with varies across language versions. At
the same time, manual fine-grained comparative
analyses (e.g., Callahan and Herring, 2011) do notarXiv:2410.04282v1  [cs.CL]  5 Oct 2024scale and run the risk of incorporating researchers’
biases.
In this work, we propose INFOGAP, a highly
reliable method for identifying overlaps and gaps
across different language articles on the same topic.
Our method is composed of two steps: an align-
ment step aimed at aligning facts across different
language versions, followed by a validation step
aimed at determining fact equivalence. INFOGAP
allows us to automatically identify exact content
differences, as illustrated in Fig. 1, enabling both
aggregate and fine-grained comparative analyses
(§2).
After verifying the accuracy of INFOGAPagainst
our manual annotations, we demonstrate its useful-
ness through a comparative analysis on thousands
of multilingual Wikipedia biographies from the
LGBTB IOCORPUS (Park et al., 2021). We find
that the coverage of public figures differs substan-
tially across languages (§3). For example, when
comparing Russian and English biographies, we
find that on average 34% of the content in Russian
biographies is not present in their English counter-
parts.
Critically, as suggested in manual analyses by
Park et al. (2021), our automatic analyses at scale
identify that many of the bios carry a significantly
different implied sentiment towards the figure, de-
pending on the language version that is accessed.2
Aggregating these sentiment imbalances across
2.7K biographies, we contribute the insight that
Russian LGBT biographies share disproportion-
ately more negative sentiment facts with English
biographies than positive ones. Overall, INFOGAP
enables the pinpointing of fine-grained factual and
framing distinctions between narratives across lan-
guages, aggregates these insights across thousands
of articles, and offers tools to identify the specific
documents that most clearly highlight these nu-
ances.
2 I NFOGAP: Identifying Information
Asymmetry in Wikipedia Articles
Consider a pair of articles on a topic written in
different languages. We call one article Eand the
other in the pair F. Moreover, we represent E
by a series of facts e1, . . . , e nandFsimilarly as
f1, . . . , f m. Our method determines for a given
factei∈Ewhether it appears in F(F⊩ei) or
2We focus on the LGBT subset of LGBTQIA+ people on
Wikipedia, due to data scarcity for other groups.
Fact 
Decomposition 
French 
Articles English 
Facts with 
alignment 
candidates Aligned 
Facts Multilingual 
Alignment 
Alignment 
Verification Facts Figure 2: Schematic of the INFOGAPprocedure. We de-
scribe the Fact Decomposition and Multilingual Align-
ment steps in §2.1, and the Alignment Verification step
in §2.2.
not (F̸⊩ei). The pipeline is directional, so we
can compute both F⊩eiandE⊩fi. Without
loss of generality, we will describe the procedure
for obtaining the labels F⊢ei, for all ei. We refer
to this as the E→Fdirection.
Fig. 2 presents an overview of INFOGAP. We
primarily focus on two steps. First, following Min
et al. (2023), we narrow the search space of equiv-
alent facts by aligning a fact in Eto facts in F
that may convey the same information (Sec 2.1).
This allows us to efficiently assess the equivalence
between aligned facts (Sec 2.2). We determine the
reliability of I NFOGAPin Section 2.3.
2.1 X-F ACTALIGN :
Cross-Lingual Fact Alignment
Fact Decomposition. As a first step, we need to
represent an article (e.g. E) as a series of facts
(e1, . . . , e n). Sentences are suboptimal for this pur-
pose, as they can be overly complex. Instead, fol-
lowing Kamoi et al. (2023), we use GPT-4 (Achiam
et al., 2023) for fact decomposition. Differently
from Kamoi et al. (2023), who decompose sen-
tences, we decompose entire paragraphs, to provide
more context to the model and allow it to resolve
co-references. See Appendix A for the prompt.
Fact Representation. In order to determine
whether eiis also conveyed in F, we embed each
fact in Eand in Fusing multilingual LaBSE em-
beddings (Feng et al., 2020). The straightforward
way to align facts is by computing the cosine sim-
ilarity between eiand each fact fj∈F, aligning
eito the most similar fact: arg minjd(ei,fj). We
find this approach can further be improved by con-
sidering the context of the surrounding facts. In thefollowing paragraphs, we describe two improve-
ments we made in X-F ACTALIGN . First, we re-
strict the pool of paragraphs in Ffrom which fj
can be retrieved. Second, we apply an adjustment
to the computation of d(·,fi), accounting for the
hubness of fj(Lazaridou et al., 2015).
Paragraph Alignment. We can partition the
facts in Einto their paragraphs: P1
E, ..., PN
E. Simi-
larly for F:P1
F, ..., PM
F. We represent each para-
graph by the set of its facts’ embeddings. We then
construct a bipartite graph between paragraphs in
Eand paragraphs in F, adding a directed edge
from each paragraph in E,Pi
E, to a paragraph in
F,Pj
Fsuch that j=MaxSim jd(Pi
E, Pj
F)(Khattab
and Zaharia, 2020). We do the same in the other
direction, going from FtoE. Removing the direc-
tion from the edges, we obtain an adjacency matrix
Abetween the paragraphs so that each paragraph in
Eis connected to at least one paragraph in F. For
a given fact ei, we can now limit the pool of align-
ment candidates in Ftofjs where the paragraphs
ofeiandfjare adjacent in the graph.
Correcting for Hubness. Given that we are com-
paring facts from articles on the same topic, directly
computing d(ei,fj)can lead to aligning unrelated
facts that discuss the same common named entities.
In particular, some facts fjare similar to many
other facts ei, causing a “hubness problem” (Lazari-
dou et al., 2015; Conneau et al., 2017). To mitigate
this, we follow Artetxe and Schwenk (2019) and
normalize d(·,fj)so that it is a function of the se-
mantic density of fj. The density-normalized dis-
tance D(ei,fi) =d(ei,fj)−hubness (fj). We com-
pute the hubness of fjby computing the average
nearest neighbor distance ( kNN= 5) between fj
and 50 other facts drawn from paragraphs that are
notin the adjacency list of the paragraph contain-
ingei. Overall, this process enables us to retrieve
k= 2 facts from Fthat may convey the same
information as ei.
2.2 X-F ACTMATCH :
Cross-Lingual Fact Matching
Witheiand its aligned facts fj, we can now answer
the question whether a given fact ei∈Eappears
inF(F⊩ei) or not ( F̸⊩ei). We assume that
ifF⊩ei, there exist facts in Fthat entail ei. In
particular, we can expect these facts to be aligned
with ei. We thus relax the problem of judging
whether F⊩eito whether any of the facts fjLanguage
Pair#Labeled#Anno-
tated
En→Fr. 2,21380Fr→En. 2,165
En→Ru 2,83280Ru→En 2,435
Table 1: Number of facts labeled using INFOGAP
for each language pair and direction, and number of
manually annotated facts.
retrieved by X-F ACTALIGN entail ei, i.e. whether
any({fj⊩ei|j∈[k]}).3
We use entailment as a shorthand for “conveying
the same information as” despite a minor deviation
from the definition of entailment in linguistics as a
strict logical entailment (Heim and Kratzer, 1998),
and in NLP as “a human [reading the premise]
would typically think that the hypothesis is likely
true” (Dagan et al., 2005; Bowman et al., 2015).
Our definition is a bit more relaxed and we also
consider partial entailment (Levy et al., 2013), i.e.,
when the most important information in eiis con-
veyed by F, allowing the omission of peripheral
information. To that end, we don’t use existing NLI
models. Furthermore, research on cross-language
entailment detection is limited (Negri et al., 2012;
Rodriguez et al., 2023), and to our knowledge there
are no publicly available models that can determine
the entailment between a premise and a hypothesis
in different languages.
Inspired by Min et al. (2023) and Shafayat et al.
(2024) who used GPT-4 to assess the truthfulness
of a model-generated fact against a trusted knowl-
edge base, we prompt GPT-4 to compare an En-
glish fact to its aligned facts in F. Concretely,
we prompt the model with the hypothesis fact ei
and the two immediately preceding facts for con-
text (ei−1andei−2), along with all of the premise
facts fjand their contexts ( fj−1andfj−2). We
instruct the model to determine whether eican be
inferred from any of the fj(j∈[k]). Appendix B
presents the prompt that we use for all language
pairs. The model’s prediction serves as the final
label for whether F⊩ei.
2.3 Assessing the Reliability of I NFOGAP
To assess the reliability of INFOGAP, we evaluate
its final results with human annotations. We apply
INFOGAPto Wikipedia biographies in English and
3We use [k]for{1, . . . , k }; see Harvey (2022, p. 11).Language
pairINFOGAP NLI Random
En→Fr 0.81 0.28 0 .62
Fr→En 0.90 0.27 0 .61
En→Ru 0.78 0.52 0 .43
Ru→En 0.88 0.50 0 .35
Table 2: Performance of INFOGAPwith respect to the
manual annotations ( n= 80 for each language pair), in
terms of F1score.
French of 10 people, and in English and Russian for
12 people, comprising nearly 10K facts altogether.
We draw on biographies from the LGBTB IOCOR-
PUS(Park et al., 2021), a corpus we analyze in §3
at a larger scale. See Table 1 for a breakdown of
the number of facts and Appendix C for the biogra-
phies.
We annotated a subset of the facts in each lan-
guage pair and direction. Given a hypothesis ei, the
retrieved candidate facts fjfrom X-F ACTALIGN ,
and their contexts, we ask the annotator to choose
between three options: (1) the hypothesis fact ei
can be inferred from one of the retrieved fj; (2) the
hypothesis fact can be inferred from the article F,
but not from the fj(indicating that X-F ACTALIGN
failed to retrieve the correct fact); (3) eicannot be
inferred from F. We also provide relaxed versions
of options (1) and (2), where eican be partially
inferred from one of the retrieved fjor partially
inferred from F. Concretely, our annotation task
closely resembles the X-F ACTMATCH step, with
two key differences. First, we provide the annota-
tors with English translations of non-English facts
and their contexts, using the NLLB model (Costa-
jussà et al., 2022). Second, if a hypothesis fact ei
cannot be inferred from the kfacts retrieved by
X-F ACTALIGN , we ask the annotator to read the
full Wikipedia article Fto determine whether ei
can be inferred from it.
One author annotated 80facts in each language
pair and for both directions within each language
pair. Another author annotated 40 of those 80 for
each language pair. We obtained substantial inter-
annotator agreements, with Cohen’s κ= 0.71for
En/Frandκ= 0.78forEn/Ru. We thus conclude
that the task is relatively unambiguous.
In order to determine the reliability of INFOGAP,
we compute the predictions against the annotated
80facts for each language pair. Table 2 presents
theF1scores that range from 0.78 to 0.9, indicat-
ing that the INFOGAPpipeline is highly reliable inidentifying whether a fact in Eis present in F(and
vice-versa). Substituting X-F ACTMATCH with a
RoBERTa NLI baseline (Liu, 2019) performs sig-
nificantly worse.4The RoBERTa NLI model rarely
predicts an entailment label on our dataset, result-
ing in its poor performance. With the exception
ofEn→Ru, the NLI baseline is outperformed by
a classifier that randomly predicts whether the tar-
get fact is entailed. INFOGAPsignificantly outper-
forms both ( p <0.05, with a bootstrap percentile
test; Efron and Tibshirani, 1994).
3 Using I NFOGAPto Analyze
Asymmetries in LGBT Wikipedia Bios
Having validated the effectiveness of INFOGAP, we
move onto applying it to answer questions about
information gaps in Wikipedia. We focus on iden-
tifying content differences between language ver-
sions’ articles on LGBT public figures. Prior work
by Park et al. (2021) identified that English articles
on average portrayed these figures with more posi-
tive sentiment, as well as greater power and agency
(Sap et al., 2017), relative to articles in Russian and
Spanish.
To gain further insight into cross-linguistic varia-
tion towards LGBT people portrayals, we draw on
theLGBTB IOCORPUS corpus (Park et al., 2021).
The corpus comprises 1,350biographies of LGBT
people, each paired with biographies of non-LGBT
people matched on most social attributes except
sexual orientation using the matching method in-
troduced in Field et al. (2022). Given that INFO-
GAPenables us to directly compare the content
between different language versions of a biography,
we contend that our analysis can provide a more
direct characterization of differences in LGBT bios.
Specifically, we look at En,Fr, and RuWikipedias.
We consider the following research questions:
RQ 1:To what extent does factual knowledge differ
across language versions of the same bio (Sec 3.2)?
RQ 2:Does a person’s affiliation with the LGBT
community have an effect on the information gap
in their bios (Sec 3.3)?
RQ 3:Can we use INFOGAPto identify sections to
remediate (Sec 3.4)?
These questions are intentionally ordered from
high-level (language-level) to low-level (individual-
and fact-level) to demonstrate that INFOGAPen-
ables both high-level quantitative analyses and ef-
4The baseline is available on HuggingFace as
cross-encoder/nli-roberta-base .Figure 3: Distribution of information overlaps for
LGBTBioCorpus . Top: Distribution over the percent-
age of facts in Enbiographies also found in their Fr
andRucounterparts. Bottom: Distribution over the per-
centage of facts in FrandRubiographies also found
in their English counterparts. N= 2,700biographies.
In general, Enbiographies contain more facts that are
exclusive to En.
ficient low-level descriptive analyses. We start by
providing the implementation details in Section 3.1
before answering each of the research questions.
3.1 Implementation Details
TheLGBTB IOCORPUS is significantly larger than
the small set of 22biographies from Section 2,
leading to high cost and runtime when applying
INFOGAP. Parsing Alan Turing’s biography with
INFOGAPalone, for example, can require more
than 100K GPT-4 tokens.5We thus use the GPT-4
predictions to finetune smaller models that are more
efficient. Specifically, we use flan-t5-large
(Chung et al., 2024) for both directions of the En/Fr
pair and mt5-large (Xue et al., 2020) for both di-
rections of the En/Rupair. We find that the T5
variants perform well at modeling the annotations
from §2, obtaining macro-averaged F1 scores of
0.90(En→Ru,Ru→En) and 0.87(En→Fr,Fr
→En). We provide fine-tuning hyperparameters
and validation set performances in Appendix D.
3.2 RQ 1: Information Gaps in Bios
Previous smaller-scale manual qualitative analyses
showed that people portrayals differ systematically
across language versions (Callahan and Herring,
2011). However, this was challenging to quantify
as it would be unreasonably laborious to manu-
ally count the number of overlapping facts between
language versions of an article. Equipped with IN-
FOGAP, we can for the first-time quantify variance
5We used gpt-4-1106-preview . At the time of writing,
this API cost $30.00USD/1M tokens.in information overlap between language versions
of Wikipedia biographies at scale. Specifically, we
consider the INFOGAPpredictions for the entire
corpus (LGBT and non-LGBT bios).
Fig. 3 visualizes the distribution of the number
of facts that can be found in both language versions
of the same bio. In the top-left subfigure, we show
a histogram of the amount of information in the
Enarticle that can also be found in the Frarticle
(En→Fr). The median of the distribution is 0.35,
indicating that for half of the biographies, only 35%
of the information in the Enarticle can be found in
theFrarticle. By comparison, the median of the Fr
→Endistribution is 0.55, much higher than the
median of the En→Frdistribution, indicating that
Enbiographies contain more unique information
than their Frcounterparts.
Considering En/Ru, we find that Enarticles con-
tain significantly more unique information than Ru
counterparts, with the median En→Ruoverlap
being 0.23. Much of the information in the Ruar-
ticles meanwhile can be found in the Enarticles,
with a median overlap of 0.66forRu→En.
We also note that the INFOGAPratios reflect the
well known “local heros” effect, where biographies
of individuals whose nationality matches the lan-
guage of the article tend to have greater coverage,
length, and visibility (Callahan and Herring, 2011;
Field et al., 2022; Hecht and Gergle, 2010; Oeberst
and Ridderbecks, 2024). When the nationality of
the person is Russian (66 people), the median En
→Ruoverlap increases to 0.29(+5% ) while the
Ru→Enoverlap decreases to 0.44 ( −22%). Simi-
larly for French (148 people), the median En→Fr
overlap increases to 0.52 ( +17% ), while the Fr→
Enoverlap decreases to 0.29 ( −26%). Overall, this
result indicates that there are large scale disparities
in information overlap ratios across language ver-
sions, building on Callahan and Herring’s (2011)
early analysis.
3.3 RQ 2: Effect of LGBT Affiliation on
Information Gaps
Given the large scale differences in content be-
tween language versions, we turn to the question
of whether LGBT people biographies exhibit dif-
ferent patterns of information overlap compared to
non-LGBT people. For example, do Russian bi-
ographies tend to include or exclude certain types
of information depending on whether the biogra-
phy is about an LGBT person? To investigate this
question, we fit a binomial regression model to de-Language pair Factor Coefficient
En→Ru conn_pos ** -0.16
conn_neg ** -0.18
is_lgbt ** 0.10
conn_pos:is_lgbt 0.04
conn_neg:is_lgbt 0.03
En→Fr conn_pos ** -0.07
conn_neg 0.01
is_lgbt ** -0.05
conn_pos:is_lgbt 0.01
conn_neg:is_lgbt ** 0.06
Ru→En conn_pos ** -0.14
conn_neg ** -0.51
is_lgbt ** 0.26
conn_pos:is_lgbt 0.04
conn_neg:is_lgbt ** 0.25
Fr→En conn_pos ** -0.07
conn_neg ** -0.14
is_lgbt 0.00
conn_pos:is_lgbt 0.03
conn_neg:is_lgbt ** 0.09
Table 3: Mean of posterior distribution of regression
coefficients . ** indicates that 95% posterior credible
interval for the coefficient does notcontain zero.
termine which factors contribute to the inclusion
ofEnfacts in the corresponding FrorRubios, and
vice versa.
Features. Table 3 displays the features we use,
along with their coeffcient estimates.6Naturally,
we include a binary feature is_lgbt indicating
whether the bio is of an LGBT person. Crucially,
we also need to consider the connotation of facts in
the English article. Park et al. (2021) found that En-
glish LGBT bios were portrayed with greater senti-
ment, power, and agency than Russian bios. How-
ever, this prior work cannot shed light on whether
the difference in sentiment is due to Russian bios
including negative sentiment facts that are not in
the English bios, excluding positive sentiment facts
from the English bios, or both. We directly address
this question using I NFOGAP.
To determine the connotation of a fact ei, we
have to consider the context in its original sen-
tence, which requires mapping between a fact and
a sentence. To map facts to their original sentences
(e.g., “ Cook is on the board of directors of Nike ”
→“Cook is also on the boards of directors of Nike,
Inc. and the National Football Foundation ”), we
use forced alignment; see Appendix E.
We obtain connotation predictions at the sen-
6We also fit models with the covariates of gender ,
nationality , and ethnicity . Including these covariates
did not change the estimates for the features in Table 3, so we
omit them for clarity.Language Positive Neutral Negative
English 0.434 0 .488 0 .077
French 0.442 0 .455 0 .102
Russian 0.327 0 .658 0 .014
Table 4: Distribution of implied sentiment about biog-
raphy subjects for En,Fr, and Ruarticles.
tence level by prompting a language model to de-
termine whether a given sentence (in the context of
the two prior sentences) portrays the subject of the
biography in a positive, negative, or neutral light.
Similar to the distillation of the INFOGAPprocess
to a smaller model (§3.1), we first obtain connota-
tion labels using GPT-4 for a smaller set of bios,
and use those labels to finetune a smaller model
for scaling to the full LGBTB IOCORPUS (see Ap-
pendix E for details, including human annotation of
the connotation labels). We use the sentence-level
connotation label as the label for its constituent
facts. Table 4 presents this label distribution.
Regression Model. Without loss of generality,
consider modeling the amount of information in
theEnbios that is also present in the Frbios, i.e.,
theEn→Frdirection. To perform our binomial
regression, we first partition each bio into three
sets – positive, negative, and neutral facts. Each
partition represents one datapoint for fitting the
regression model, so each bio contributes three
datapoints. Within each of these three partitions,
some facts will also be present in F, while others
will be exclusive to E. We model this using a
bayesian binomial regression model (McElreath,
2018):
overlap |Np∼1 + conn+is_lgbt +is_lgbt :conn
where conn gets the value of either conn_pos ,
conn_neg , orconn_neutral , depending on the in-
put partition, Npis the number of facts in the cur-
rent partition of E, and overlap is the number of
facts that are also in F(at most Np).is_lgbt :conn
is an interaction between the two categorical vari-
ables. See Appendix F for model-fitting details.
Connotation is a predictive factor. Listed in
Table 3, our results indicate that connotation is a
predictive factor in nearly all language pairs and di-
rections considered, except conn_neg inEn→
Fr. Further, the polarity of the conn_pos and
conn_neg factors is always negative, suggesting
that polarized facts tend to be included in lower
rates than neutral facts, which are more agreeable
across language versions. To ground the effectsize of the coefficients, we can simulate predictions
from the regression model. For example, a value
of -0.07 for conn_pos in the En→Frmodel in-
dicates that 34.4% of the positive facts in Enare
included in the Frbios, compared to 36.6% of the
neutral facts.
Negative connotation facts are disproportion-
ately included in Russian LGBT bios. Consid-
ering Russian biographies, we draw from the large
coefficient value of the is_lgbt feature that facts
from the English article are more likely to be ref-
erenced when the article is about an LGBT public
figure. Moreover, from the is_lgbt:conn_neg in-
teraction, we find that negative facts are more likely
to be referenced than positive ones. To quantify the
size of this effect, we simulate posterior predictions
from the binomial regression model. We find an av-
erage 50.87% of negative Russian facts are shared
with the English biographies when they describe
an LGBT public figure, whereas only 38.53% of
negative facts are shared with English bios when
they are non-LGBT.
3.4 RQ 3: Identifying Sections to Remediate
Our analysis in Sec 3.3 revealed that facts carry-
ing a more polarizing (non-neutral) connotations
are less likely to be shared across language ver-
sions. This suggests that many biographies may
carry a significantly different overall connotation,
depending on the language version in which they
are read. Unlike the manual analysis performed
in prior works (e.g., Park et al., 2021; Callahan
and Herring, 2011, among others) to identify such
language-version imbalanced content, INFOGAP
can automatically locate imbalanced content. Park
et al. (2021) in particular focused on bios where
the subject was portrayed with a more negative
implied sentiment. Here, we focus on a different
aspect of sentiment differences: the omission of
content with positive implied sentiment from one
language version.
Specifically, we follow these steps to identify im-
balanced content: First, we identify bios from the
LGBTB IOCORPUS where a high rate of positive
facts are excluded from one language version com-
pared to another.7Next, we introduce a method to
identify positive life events that are missing in that
language version. We provide a formal argument
7It is also effective at identifying individual facts present
in one language version but absent in another (see §2), but for
this analysis we consider collections of multiple facts.demonstrating that our INFOGAPbased method
for identifying missing events is highly accurate.
Finally, we conclude with examples of findings.
Step 1. Identifying biographies with imbalanced
implied sentiment. Consider a pair of articles E
andFwritten in different languages, and suppose
we wanted to find bios where Fomitted positive
content at a high rate. We conduct a hypothesis test
to determine whether the number of positive facts
included in both languages is significantly lower
than expected based on the overlap rate of neutral
facts. Concretely, we perform a bayesian hypoth-
esis test based on the BetaBinomial distribution;
we provide complete details in Appendix G.
Our test identifies 274imbalanced LGBT biogra-
phies when considering En→Ruand236when
considering En→Fr. We can follow the same pro-
cedure for finding English biographies that compar-
atively lack positive information, when compared
to their French and Russian counterparts. We find
105 and 199 biographies in the Ru→EnandFr
→Endirection, respectively.
Step 2. Identifying events that are unique to a
language version. Having identified biographies
that could benefit from remediation, we next fo-
cus on finding the positive-connotation carrying
content that is missing from one language version.
By comparison to Park et al. (2021), who could
only analyze 10biographies for identifying imbal-
anced content, we can leverage INFOGAPto iden-
tify imbalanced content at scale within the subset
of biographies we identified. We focus on finding
positive connotation events – longer collections of
facts that are thematically related – rather than indi-
vidual isolated facts since the omission of a whole
event is more egregious. Practically speaking, we
search for paragraphs V=e1, . . . , e NVwhere all
facts in the paragraph are missing from F:
M={V ∈ E|all({F̸⊩ei|i∈[NV]})}(1)
We then select a subset of M: paragraphs contain-
ing at least one positive connotation fact.
INFOGAPis highly effective at identifying miss-
ing events. Consider an event V=e1, . . . , e NV
that is described in article E. Suppose that INFO-
GAPpredicted that Vis not covered by F, that
is that Fdoes not entail any of the events in V:
F̸⊨ei, i∈[NV]. For INFOGAPto be wrong,
i.e.,Vis actually present in F, there needs to
exist a subset of facts ei(1), . . . , e i(k)∈ V , fork=p·NV(0< p < 1), that are entailed by F:
F⊨ei(j), j∈[k]. We can bound the probability
of this error.
Proposition 1 (Error Bound of Event Identification
through InfoGap) .The probability of INFOGAP
making kerrors is ≤exp(−2(1−ϵ)2k), where ϵ
is the error rate of the classifier when it predicts
F̸⊨ei.
Proof. Given that the error rate of the classifier is
ϵ, the expected number of errors for kpredictions
isϵ·k. However, the classifier made kmistakes, so
we have made ϵ·k+ (1−ϵ)·kerrors, an additive
factor of t= (1−ϵ)·kmore mistakes than expected.
By Hoeffding’s inequality (Appendix H), where
we supply our expected value µ=ϵ·kand the
deviation from the expected value of t= (1−ϵ)·k,
we obtain an upper bound of:
≤exp 
−2(1−ϵ)2k2/k
= exp( −2(1−ϵ)2k).
The significance of this claim is that it is rare for
theINFOGAPclassifier to make a large number ( k)
of mistakes when the error rate is ϵ(where ϵ << 1).
Moreover, the probability of mistakes decreases
very quickly in the accuracy of the classifier and
the number of facts in Vthat were predicted to not
be entailed by F. As we showed empirically in
Section 2, the INFOGAPclassifier is reliable (low
ϵ) and thus it has a strong capacity to find events
that are only described in one language version.8
Findings. In Table 5, we demonstrate positive
events that are unique to one language version
when compared to another. We find that Chelsea
Manning’s Frpage describes praise for her whistle-
blowing during the Afghanistan war. The Fr
page also discusses her whistleblowing on the Abu
Ghraib prison conditions (Hersh, 2004). Conspic-
uously, both events are omitted from the Enpage,
despite the Enpage being otherwise longer. Amer-
ican perception of this instance of whistleblow-
ing skewed negative (Pew Research Center, 2010),
which may have played a role in the disparities
between the EnandFrpages.
We also find Tim Cook’s Rupage – but not his En
page – makes note of his fundraising initiative to
defend Ukraine in the current Russo-Ukranian war.
It is unsurprising that it appears in the Rupage, as it
8One shortcoming of this argument is if Fdiscusses a com-
pletely different aspect of the event VthanE. We conjecture
that this is unlikely since both articles should at least contain
the central propositions about the event.directly pertains to Russia. However, the omission
of this fact from the Enpage is remarkable, since it
had received some media attention from American
outlets (Clark and Schiffer, 2022). One reason for
this omission may be that there is a partisan divide
on US involvement in the war (Pew Research Cen-
ter, 2024). This fact may not have been included in
Ento maintain a veneer of neutrality.
It is important to note however that Wikipedia’s
Neutral Point of View policy advocates for a bal-
anced representation of views (Matei and Dobrescu,
2011), rather than outright filtering or censorship.
Our findings raise questions about the degree to
which a cross-linguistically consistent “Neutral
Point of View” is realizable. INFOGAPenables
studying these cross-linguistic differences in por-
trayals of public figures at scale.
4 Related Work
Automated comparison of multilingual
Wikipedia articles. We contribute to a large
body of work on understanding differences
between language versions of Wikipedia. Hecht
and Gergle (2010) also compare Wikipedia
language versions and consider their information
gaps, and later develop a web tool to bridge these
multilingual gaps (Bao et al., 2012). However,
their evaluation is at a higher level of abstraction:
they look at whether or not two language versions
have on a topic. By comparison, we compare
content differences between two language versions
on the same topic. Duh et al. (2013) considered
a pipeline similar to INFOGAPfor the task of
keeping multilingual Wikipedia documents con-
sistent. However, their pipeline used embedding
similarity; in early experiments, we found that
using embedding similarity for identifying poten-
tial entailments performed very poorly (relative
toX-F ACTMATCH ). Massa and Scrinzi (2012)
created a web tool that permits visual comparison
of Wikipedia articles in two different languages.
Rodriguez et al. (2023) also perform comparative
analyses across language versions in Wikipedia.
However, they consider more fine-grained content
differences between pairs of the most closely
related paragraphs between different language
versions’ article on a topic. Their method was
not designed for computing the overall article
level overlaps and differences of the form we
demonstrate in Fig. 3.Pair Person Events
En✗,Ru✓ Tim Cook In 2022, following Russia’s invasion of Ukraine, Tim Cook called on the company’s
employees to donate to help Ukraine. Apple’s CEO announced the decision to suspend
sales of equipment in Russia and also said that the company would triple the amount of
donations made by employees to support Ukraine, and this would be retroactive to February
25, 2022.
En✗,Fr✓ Chelsea Manning “Ron Paul, a leader of the libertarian movement within the Republican Party, endorsed
Manning on April 12, 2013, stating that Manning had done more for peace than
Obama—referring to Obama’s 2009 Nobel Peace Prize win: “While President Obama
was initiating and expanding unconstitutional wars abroad, Manning , whose actions caused
exactly zero deaths, was shining a light on the truth behind those wars. Which of the two
has done more for peace is clear.”
En✓,Fr✗ Caster Semenya In 2010, the British magazine New Statesman included Semenya in its annual list of
“50 People That Matter” for unintentionally instigating “an international and often ill-
tempered debate on gender politics, feminism, and race, becoming an inspiration to gender
campaigners around the world”
En✓,Ru✗ Ada Colau During her period as mayor of Barcelona, Colau has maintained a political stance against
activities that are susceptible of contributing to greenhouse gas emissions and air pollution.
She has repeatedly opposed the expansion of El Prat airport and the use of private cars in
the city, and has pushed regional authorities to restrict the number of cruise ships arrivals
in Barcelona. In 2020 she declared a “climate emergency”, advocating limiting the con-
sumption of meat at schools and forbidding councillors from using the Barcelona-Madrid
air shuttle.
Table 5: Examples of events from biographies that contain a large number of positive facts that are only contained
in one language version of the article relative to another. We provide translations (Google Translate) for the first two
rows, rather than the original French and Russian content.
Case studies on cultural differences in multilin-
gual Wikipedia. We highlight two studies that
were not mentioned elsewhere in this work. Hick-
man et al. (2021) analyze how a boundary dispute
over Kashmir between India and Pakistan is rep-
resented in English, Hindi, and Urdu Wikipedia,
analyzing how the Neutral Point of View principle
is upheld. They find there is a sizeable number of
cross-language editors between Urdu and English,
as well as Hindi and English, but not Urdu and
Hindi, attributing this to the popularity of English
Wikipedia. Kharazian et al. (2024) studied how
the Croatian language version of Wikipedia was
usurped by a small group of editors who aimed to
promote far-right bias and disinformation about var-
ious Croatian political figures, groups, and events.
This bias was apparent when comparing the Croat-
ian articles to Serbian and English ones.
5 Conclusion
We presented INFOGAP, a reliable method for ef-
ficient comparative analysis between two narra-
tives on the same topic written in different lan-
guages. We deployed the method to discover dif-
ferences in LGBT people’s portrayals, locating
shared facts, as well as information gaps and in-
consistencies across 2.7K English, Russian, and
French Wikipedia biography pages. INFOGAPcanbe directly applied beyond analyzing differences
in multilingual Wikipedia biographies. Analyzing
variation in topic coverage is at the heart of much
research in the social sciences, from understanding
media manipulation strategies (Field et al., 2018),
to analyzing differences in argumentation from dif-
ferent stances in a contentious debate (Luo et al.,
2020), to analyzing quotation patterns in partisan
media (Niculae et al., 2015). Overall, our research
lays the foundation for enabling targeted, nuanced
textual comparative analyses at scale.
6 Limitations
Applicability to specialized domains. Our
method relies on the language understanding abil-
ities of the underlying language model (GPT-4 in
our case). While we were able to achieve high
accuracy on the LGBTB IOCORPUS , it is not guar-
anteed that similarly high-accuracy can be achieved
if we were to apply INFOGAPto more specialized
domains, where domain expertise may be required
to assess the equivalence of two facts in different
languages, such as comparing Wikipedia articles
concerning scientific topics.
Connotation is subjective. In Section 3 we in-
vestigated the effect of connotation on the inclu-
sion of facts in different language versions. Weacknowledge that connotation is fairly subjective,
and may depend on a reader’s stance towards the
topic and their cultural background. To ensure a
high degree of replicability of our results, we have
released our all of the finetuned models we applied
in §3, including the connotation models.
Ablations of INFOGAPcomponents. We did
not perform ablations of the components of the X-
FACTALIGN step in the INFOGAPpipeline (§2).
Our aim was to demonstrate that high-quality auto-
matic cross-lingual comparative analysis is not only
possible (§2.3) but provides considerable benefits
in downstream analyses (§3). We will perform thor-
ough ablations with a larger number of annotated
samples in future work.
7 Ethical Considerations
Data. The dataset used in this study, LGBTB IO-
CORPUS , is publicly available.
Models. We used language models to make clas-
sification predictions, limiting their ability to gen-
erate offensive content. We used a closed-source
model, GPT-4, which entails high costs, and may
not be suitable for applying our method to different
datasets, especially those containing private infor-
mation. The distilled version of INFOGAP, which
uses open-source models, addresses both concerns.
8 Acknowledgements
We thank Shreya Prakash for advice on our regres-
sion analyses and hypothesis testing. We thank
Miikka Silfverberg and Jai Aggarwal for helpful
feedback on the manuscript. FS is supported by an
NSERC PGS-D scholarship. VS is supported by
the Vector Institute for AI, the CIFAR AI Chair pro-
gram, and NSERC. We gratefully acknowledge sup-
port from the National Science Foundation under
CAREER Grant No. IIS2142739, and NSF grants
No. IIS2125201 and IIS2203097.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Mikel Artetxe and Holger Schwenk. 2019. Margin-
based parallel corpus mining with multilingual sen-
tence embeddings. In Proceedings of the 57th An-
nual Meeting of the Association for ComputationalLinguistics , pages 3197–3203, Florence, Italy. Asso-
ciation for Computational Linguistics.
Patti Bao, Brent Hecht, Samuel Carton, Mahmood
Quaderi, Michael Horn, and Darren Gergle. 2012.
Omnipedia: bridging the wikipedia language gap. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems , pages 1075–1084.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Paul-Christian Bürkner. 2017. brms: An r package for
bayesian multilevel models using stan. Journal of
statistical software , 80:1–28.
Ewa S Callahan and Susan C Herring. 2011. Cultural
bias in wikipedia content on famous persons. Journal
of the American society for information science and
technology , 62(10):1899–1915.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2024. Scaling instruction-finetuned language models.
Journal of Machine Learning Research , 25(70):1–53.
Mitchell Clark and Zoë Schiffer. 2022. Read Tim
Cook’s email to employees on Ukraine. The Verge .
Alexis Conneau, Guillaume Lample, Marc’Aurelio Ran-
zato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv preprint
arXiv:1710.04087 .
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672 .
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine learning challenges workshop ,
pages 177–190. Springer.
Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata,
and Masaaki Nagata. 2013. Managing information
disparity in multilingual document collections. ACM
Transactions on Speech and Language Processing
(TSLP) , 10(1):1–28.
Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap . Chapman and Hall/CRC.
Young-Ho Eom, Pablo Aragón, David Laniado, An-
dreas Kaltenbrunner, Sebastiano Vigna, and Dima L
Shepelyansky. 2015. Interactions of cultures and top
people of wikipedia from ranking of 24 language
editions. PloS one , 10(3):1–27.Fangxiaoyu Feng, Yinfei Yang, Daniel Matthew Cer,
N. Arivazhagan, and Wei Wang. 2020. Language-
agnostic bert sentence embedding. In Annual Meet-
ing of the Association for Computational Linguistics .
Anjalie Field, Doron Kliger, Shuly Wintner, Jennifer
Pan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Fram-
ing and agenda-setting in Russian news: a computa-
tional analysis of intricate political strategies. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing , pages 3570–
3580, Brussels, Belgium. Association for Computa-
tional Linguistics.
Anjalie Field, Chan Young Park, Kevin Z Lin, and Yulia
Tsvetkov. 2022. Controlled analyses of social biases
in wikipedia bios. In Proceedings of the ACM Web
Conference 2022 , pages 2624–2635.
Nick Harvey. 2022. A first course in randomized algo-
rithms.
Brent Hecht and Darren Gergle. 2010. The tower of
babel meets web 2.0: user-generated content and its
applications in a multilingual context. In Proceed-
ings of the SIGCHI conference on human factors in
computing systems , pages 291–300.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar . Blackwell.
Seymour M. Hersh. 2004. Torture at abu ghraib. New
Yorker .
Molly G Hickman, Viral Pasad, Harsh Kamalesh Sang-
havi, Jacob Thebault-Spieker, and Sang Won Lee.
2021. Understanding wikipedia practices through
hindi, urdu, and english takes on an evolving re-
gional conflict. Proceedings of the ACM on Human-
Computer Interaction , 5(CSCW1):1–31.
Matthew D Hoffman, Andrew Gelman, et al. 2014. The
no-u-turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res. ,
15(1):1593–1623.
Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and
Greg Durrett. 2023. WiCE: Real-world entailment
for claims in Wikipedia. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 7561–7583, Singapore. As-
sociation for Computational Linguistics.
Zarine Kharazian, Kate Starbird, and Benjamin Mako
Hill. 2024. Governance capture in a self-governing
community: A qualitative comparison of the croat-
ian, serbian, bosnian, and serbo-croatian wikipedias.
Proceedings of the ACM on Human-Computer Inter-
action , 8(CSCW1):1–26.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.Suin Kim, Sungjoon Park, Scott A Hale, Sooyoung Kim,
Jeongmin Byun, and Alice H Oh. 2016. Understand-
ing editing behaviors in multilingual wikipedia. PloS
one, 11(5):e0155305.
Paul Laufer, Claudia Wagner, Fabian Flöck, and Markus
Strohmaier. 2015. Mining cross-cultural relations
from wikipedia: a study of 31 european food cultures.
InProceedings of the ACM web science conference ,
pages 1–10.
Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni.
2015. Hubness and pollution: Delving into cross-
space mapping for zero-shot learning. In Proc. ACL .
Omer Levy, Torsten Zesch, Ido Dagan, and Iryna
Gurevych. 2013. Recognizing partial textual entail-
ment. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers) , pages 451–455, Sofia, Bul-
garia. Association for Computational Linguistics.
Yinhan Liu. 2019. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692 .
Yiwei Luo, Dallas Card, and Dan Jurafsky. 2020. De-
tecting stance in media on global warming. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 3296–3315, Online. Association
for Computational Linguistics.
David JC MacKay. 2003. Information theory, infer-
ence and learning algorithms . Cambridge university
press.
Paolo Massa and Federico Scrinzi. 2012. Manypedia:
Comparing language points of view of wikipedia
communities. In Proceedings of the Eighth Annual
International Symposium on Wikis and Open Collab-
oration , pages 1–9.
Sorin Adam Matei and Caius Dobrescu. 2011.
Wikipedia’s “neutral point of view”: Settling con-
flict through ambiguity. The Information Society ,
27(1):40–51.
Richard McElreath. 2018. Statistical rethinking: A
Bayesian course with examples in R and Stan . Chap-
man and Hall/CRC.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. FActScore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 12076–12100, Singa-
pore. Association for Computational Linguistics.
Michael Mitzenmacher and Eli Upfal. 2017. Probabil-
ity and computing: Randomization and probabilistic
techniques in algorithms and data analysis . Cam-
bridge university press.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entail-
ment for content synchronization. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012) , pages 399–
407, Montréal, Canada. Association for Computa-
tional Linguistics.
Vlad Niculae, Caroline Suen, Justine Zhang, Cristian
Danescu-Niculescu-Mizil, and Jure Leskovec. 2015.
Quotus: The structure of political media coverage as
revealed by quoting patterns. In Proceedings of the
24th International Conference on World Wide Web ,
pages 798–808.
Aileen Oeberst and Till Ridderbecks. 2024. How article
category in wikipedia determines the heterogeneity
of its editors. Scientific Reports , 14.
Chan Young Park, Xinru Yan, Anjalie Field, and Yulia
Tsvetkov. 2021. Multilingual contextual affective
analysis of lgbt people portrayals in wikipedia. In
Proceedings of the International AAAI Conference on
Web and Social Media , volume 15, pages 479–490.
Pew Research Center. 2010. Most say wikileaks release
harms public interest. Technical report.
Pew Research Center. 2024. Views of ukraine and u.s.
involvement with the russia-ukraine war. Technical
report.
Hannah Rashkin, Sameer Singh, and Yejin Choi. 2015.
Connotation frames: A data-driven investigation.
arXiv preprint arXiv:1506.02739 .
Juan Diego Rodriguez, Katrin Erk, and Greg Dur-
rett. 2023. X-parade: Cross-lingual textual entail-
ment and information divergence across paragraphs.
ArXiv , abs/2309.08873.
Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman,
Hannah Rashkin, and Yejin Choi. 2017. Connotation
frames of power and agency in modern films. In
Proceedings of the 2017 conference on empirical
methods in natural language processing , pages 2329–
2334.
Sheikh Shafayat, Eunsu Kim, Juhyun Oh, and Alice Oh.
2024. Multi-fact: Assessing multilingual llms’ multi-
regional knowledge using factscore. arXiv preprint
arXiv:2402.18045 .
Claudia Wagner, David Garcia, Mohsen Jadidi, and
Markus Strohmaier. 2015. It’s a man’s wikipedia?
assessing gender inequality in an online encyclopedia.
InProceedings of the international AAAI conference
on web and social media , volume 9, pages 454–463.
Sarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith.
2021. Measuring association between labels andfree-text rationales. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 10266–10284, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2020. mt5: A massively multilingual
pre-trained text-to-text transformer. arXiv preprint
arXiv:2010.11934 .
A Fact Decomposition Prompt
We provide the fact-decomposition prompts in
fact_decomp_prompts.txt athttps://github.
com/smfsamir/infogap . For Ru, we found that
an example was required in order for the GPT-4 re-
sponse to be consistently structured in the form of
apython list of strings, while the other languages
(En,Fr) were able to successfully follow this in-
struction without an example.
B Fact Equivalence Prompt
We provide the prompts for X-F ACTMATCH in
fact_match_prompts.txt athttps://github.
com/smfsamir/infogap . The first row contains
the prompt for En→RuandEn→Fr; the sec-
ond for Fr→En; and the third for Ru→En. In
each prompt, the src_facts variable is equivalent
toei−2, ei−1, eifrom §2.2, while tgt_facts con-
tains fj−2, fj−1, fj, forj∈[k]. That is, we use
these prompts to determine whether eiis contained
in the other language (e.g., Frfor the En→Fr
direction).
C Seed biographies
In Table 6 and Table 7, we list the seed set of bi-
ographies, that were used for obtaining INFOGAP
labels. We performed our human annotation ex-
periment §2.3 for INFOGAPon these labels. We
then used these labels to distill flan-t5-large
andmt5-large for our analyses in §3.
We also used these seed biographies for obtain-
ing connotation labels from GPT-4 for our anal-
ysis in §3, which we then also used to distill
into flan-t5-large andmt5-large for predict-
ing connotation labels at a larger scale.
DInfoGap Distillation Hyper-Parameters
We report fine-tuning hyperparameters for the Hug-
gingFace Trainer in Table 8. Unspecified values use
the default setting of the Trainer ( python version:
4.34.1). For all tasks, we used a train/test split ofEn→Fr;Fr→En
Gabriel Attal
Ellen DeGeneres
Tim Cook
Kim Petras
Alan Turing
Caroline Mécary
Abdellah Taïa
Sophie Labelle
Frédéric Mitterrand
Philippe Besson
Table 6: Initial seed set of people for obtaining INFO-
GAPlabels with GPT-4 for the En→FrandFr→En
directions; see §2.3. We used the INFOGAPlabels on
this seed set to finetune a flan-t5-large model; see
§3.1.
En→Ru;Ru→En
Pyotr Ilyich Tchaikovsky
Tim Cook
Dmitry Kuzmin
Masha Gessen
Nikolay Alexeyev
James Baldwin
Ali Feruz
Elena Kostyuchenko
Mikhail Zygar
Pyotr Verzilov
Sergey Sosedov
Yekaterina Samutsevich
Table 7: Initial seed set of people for obtaining INFO-
GAPlabels with GPT-4 for the Ru→EnandEn→Ru
directions; see §2.3. We used the INFOGAPlabels on
this seed set to finetune a mt5-large model; see §3.1.0.9/0.1. As for evaluation metrics, we used Rouge-
1 for fact decomposition (§2.1), and Micro-F1 for
X-F ACTMATCH (§2.2) and connotation prediction
(§3.3). For the En→FrandEn→Frdirec-
tions, we apply the flan-t5 models. We obtained
strong validation set performance ( 0.85Rouge-1;
0.85and0.88F1s for the connotation prediction
andX-F ACTMATCH tasks, respectively) using the
same hyperparameter settings across all three tasks.
We found that flan-t5-large did not general-
ize well to Ru, obtaining poor performance in fact
decomposition and often predicting nonsensical
Russian strings. We thus resorted to mt5-large in-
stead (Xue et al., 2020), since Russian is one of the
largest languages in terms of its pre-training data
sizes. After a hyperparameter sweep over learn-
ing rates, gradient accumulation sizes, and weight
decay values, we found much better performance
with mT5, obtaining validation set performances of
0.89,0.79, and 0.86for fact decomposition, con-
notation prediction, and X-F ACTMATCH tasks, re-
spectively.
All finetuning was completed on a single
NVIDIA L40 GPU.
E Connotation modeling
Forced alignment procedure. As mentioned in
§3.3, we applied forced alignment to assign de-
composed facts back into their original full sen-
tences. Forced alignment is a constrained version
of Dynamic Time Warping, where the alignment is
monotonic. Forced alignment requires a distance
function, we used hubness-corrected distance (Sec-
tion 2.1).
Connotation prompts. We provide the
prompts used for obtaining connotation
labels in connotation_prompts.txt at
https://github.com/smfsamir/infogap .
Thecontent variable contains up to 3 sentences,
si−2, si−1, si. While we’re interested in the
connotation towards person_name conveyed in
the last sentence si, we provide the prior two
sentences for more context. We prompted for
both connotation labels and rationales for the
labels, after finding that prompting for a rationale
prevented the models from vastly overextending
theneutral label. This aligns with prior research
on text classification, where generating rationales
improved accuracy (Wiegreffe et al., 2021).Model Task hyperparameter value
flan-t5 Fact decomp. auto_find_batch_size True
Learning rate 5e-5
Num. epochs 5
X-F ACTMATCH auto_find_batch_size True
Learning rate 5e-5
Num epochs 5
Conn. prediction auto_find_batch_size True
Learning rate 5e-5
Num. epochs 5
mT5 Fact decomp. Batch size 2
Learning rate 9.5e-4
Weight decay 0.0
Gradient accumulation steps 4
Num. epochs 5
X-F ACTMATCH Batch size 2
Learning rate 8.5e-5
Weight decay 0.4
Gradient accumulation steps 4
Num. epochs 5
Conn. prediction auto_find_batch_size True
Learning rate 5e-5
Num. epochs 5
Table 8: Parameters provided to the HugggingFace trainer for the flan-t5-large andmt5-large models.Language Macro-averaged F1
En 0.77
Fr 0.77
Ru 0.86
Table 9: Macro-averaged F1 scores for predicting the
connotation towards the subject of a biography from a
snippet of text in the biography.
E.1 Validation of connotation label
predictions
To validate the connotation labels predicted in §3.3,
we sampled 10positive, 10negative, and 10neu-
tral connotation label predictions from Appendix C
for each of the 3 languages, thus obtaining 90data-
points in total. One co-author then annotated each
datapoint manually, and compared the annotations
against the labels predicted by GPT-4 from the con-
notation prompt in Appendix E.
We provide the results of this classification in
Table 9. We find that the connotation predictions
are generally reliable, with all errors stemming
from confusion between neutral andpositive ,
orneutral andpositive , rather than the more
severe error of confusing positive andnegative
labels. This aligns with observations in previous
research on computational modeling of connotation
(Park et al., 2021; Rashkin et al., 2015; Sap et al.,
2017, among others).
Distillation. Having validated the quality of the
GPT-4 connotation predictions, we use the pre-
dicted labels to finetune more scalable, lightweight
models for predicting the connotation labels. We
provide hyperparameter details in Appendix D.
F Regression model fitting
We fit the regression model using the brms package
(Bürkner, 2017), with 2500 steps (500 warmup) of
the NUTS sampler (Hoffman et al., 2014). We used
a regularizing N(0,10)prior on all the coefficients
for the factors.
G Identifying biographies with a positive
connotation imbalance across language
versions
Plan. We consider the En→Frdirection for
an arbitrary bio, without loss of generality. We
will use the amount of neutral facts shared by both
articles to parameterize a BetaBinomial distribu-
tion. After fitting this distribution, we will simulatedraws from it to predict how much positive infor-
mation should be shared by both articles. When the
actual amount of shared positive connotation facts
is much lower than the amount predicted by the
fitted BetaBinomial distribution, we can consider
this an imbalanced biography for the En→Fr
direction.
Implementation. We first set the prior for
the neutral fact distribution to uniform (prior
to observing the actual neutral overlap ratio):
Beta(1 ,1). We leverage the useful fact that the pos-
terior distribution after observing xneutral facts
ei(1), . . . , e i(x)in both EnandFrout of ntotal
facts in EnisBeta(1 + x,1 +n−x)(MacKay,
2003). We can then simulate draws from the
BetaBinomial distribution, first drawing a sample
from Beta(1+ x,1+n−x), followed by predicting
amount of Enfacts that should also be found in Fr.
The number of trials is fixed to the total number of
positive facts in the Enarticle.
Thus, this binomial distribution tells us the num-
ber of positive facts we would expect to see in
both articles, if positive facts were not omitted at
a higher rate than neutral facts. We can then draw
S= 1000 samples, counting the number of times
Ktheexpected amount of shared positive connota-
tion facts is higher than the actual amount. When
K/S is close to 1.0, there is a large amount of
positive information being omitted the Frarticle,
compared to the Enone. We use 1−K/S as a
p-value, with an α= 0.05.
We emphasize further that this method can be
applied in either direction (e.g., En→Fr, or
Fr→En), as well as for finding negatively im-
balanced biographies, where one language version
includes negative content at a rate much higher than
expected under the neutral rate.
H Hoeffding’s inequality
We provide the full statement of Hoeffding’s in-
equality for easy reference (Mitzenmacher and Up-
fal, 2017; Harvey, 2022):
Theorem 1 (Hoeffding’s inequality) .Let
X1, . . . , X nbe independent random vari-
ables such that Xialways lies in the inter-
val[0,1]. Define X=Pn
i=1Xi. Then
Pr[|X−E[X]| ≥t]≤2 exp(−t2/2n).