Focused Large Language Models are Stable Many-Shot Learners
Peiwen Yuan1, Shaoxiong Feng2, Yiwei Li1, Xinglin Wang1, Yueqi Zhang1
Chuyi Tan1, Boyuan Pan2, Heda Wang2, Yao Hu2, Kan Li1∗
1School of Computer Science and Technology, Beijing Institute of Technology
2Xiaohongshu Inc
{peiwenyuan,liyiwei,wangxinglin,zhangyq,tanchuyi,likan}@bit.edu.cn
{shaoxiongfeng2023,whd.thu}@gmail.com {panboyuan,xiahou}@xiaohongshu.com
Abstract
In-Context Learning (ICL) enables large lan-
guage models (LLMs) to achieve rapid task
adaptation by learning from demonstrations.
With the increase in available context length
of LLMs, recent experiments have shown that
the performance of ICL does not necessarily
scale well in many-shot (demonstration) set-
tings. We theoretically and experimentally con-
firm that the reason lies in more demonstrations
dispersing the model attention from the query,
hindering its understanding of key content. In-
spired by how humans learn from examples,
we propose a training-free method FOCUS ICL,
which conducts triviality filtering to avoid atten-
tion being diverted by unimportant contents at
token-level and operates hierarchical attention
to further ensure sufficient attention towards
current query at demonstration-level. We also
design an efficient hyperparameter searching
strategy for FOCUS ICL based on model per-
plexity of demonstrations. Comprehensive ex-
periments validate that FOCUS ICL achieves
an average performance improvement of 5.2%
over vanilla ICL and scales well with many-
shot demonstrations.
1 Introduction
The rapid development of large language models
(LLMs) has facilitated the emergence and enhance-
ment of their In-Context Learning (ICL) abilities
(Wei et al., 2022a; Dong et al., 2023). As a training-
free method, ICL can achieve fast model adapta-
tion on specific tasks based on several demonstra-
tions prefixed to the query, formally denoted as
ICL(response |demos, query ). Intuitively, more
demonstrations can help LLMs better understand
the task and increase the likelihood of finding
demonstrations that aid in responding queries, thus
leading to better performance. Theoretically, a sim-
ilar conclusion can be drawn. Previous studies
(Dai et al., 2023; Irie et al., 2022; von Oswald
*Corresponding author.
q1r1q2r2 q qNrNd1 d2 dN_...
q1r1q2r2 q qNrN ... input
5              10             20            40             80            160   NAttention per Token of q 
0.006
0.005
0.004
0.003r?Figure 1: The average model attention for query is dis-
persed by the increased number of demonstrations, caus-
ing inadequate understanding of query.
et al., 2023; Akyürek et al., 2023) have theoreti-
cally inferred that ICL can be viewed as an implicit
finetuning process, with demonstrations analogous
to training samples. On this basis, as finetuning
has been validated to comply with the scaling law
(Hernandez et al., 2021) where performance in-
creases with the number of training samples, the
performance of ICL should also positively corre-
lates with the number of demonstrations, which has
been experimentally verified by previous studies
(Bertsch et al., 2024; Duan et al., 2023).
However, with the increase in available context
length of LLMs (Reid et al., 2024), some stud-
ies (Zhao et al., 2023; Agarwal et al., 2024) ob-
serve counterexamples when scaling the demonstra-
tion numbers from few-shot to many-shot. Agar-
wal et al. (2024) finds that the optimal number
of demonstrations for six out of eleven bench-
marks is not the maximum number they have tested.
Our experimental results (Figure 5) also indicate
that the model performance might decline with
increased demonstrations when applying ICL, ex-
hibiting an inverse-scaling phenomenon (McKen-
zie et al., 2023). These findings indicate that LLMs
are not stable many-shot learners.arXiv:2408.13987v1  [cs.CL]  26 Aug 2024To understand this gap, we revisit the derivation
of Dai et al. (2023) that formally equates ICL with
finetuning and identify that their approximation of
standard attention operation as linear attention op-
eration will ignore the competition for attention
between demonstrations and the query when gen-
erating the response. Since this approximation is
key to the equivalence of ICL and finetuning, we
hypothesize that the reason why ICL does not ad-
here to the scaling law like finetuning is that more
demonstrations can divert attention away from the
query. Inadequate attention and understanding of
the query can naturally lead to inferior response. To
verify our hypothesis, we first conduct experiments
confirming that increasing the number of demon-
strations does lead to a decrease in model attention
towards queries (Figure 1). We further experiment
by adding blank spaces within the demonstrations
and confirm that: the more blank spaces added, the
more attention towards queries distracted by blanks,
resulting in lower response accuracy (Figure 2).
Inspired by the way humans benefit from ignor-
ing irrelevant contents and integrating insights from
multiple examples when solving problems, we pro-
pose FOCUS ICL to avoid the attention dispersion
issue faced by ICL. Specifically, at the token-level,
FOCUS ICL conducts triviality filtering by adap-
tively masking unimportant tokens of demonstra-
tions based on attention distribution, allocating
the attention to more important contents. At the
demonstration-level, FOCUS ICL performs hierar-
chical attention mechanism by dividing demonstra-
tions into multiple batches and respectively con-
ducting intra-batch and inter-batch attention oper-
ations. The limited demonstration number within
each batch ensures sufficient attention to the query,
while inter-batch attention integrates the benefits
from a larger number of demonstrations. We fur-
ther introduce an efficient hyperparameter search-
ing strategy for FOCUS ICL according to model
perplexity of demonstrations.
Our experiments across three LLMs on five
benchmarks confirm that FOCUS ICL achieves an
average performance improvement of 5.2% over
ICL by avoiding attention dispersion, with lower
inference overhead. This demonstrates the effec-
tiveness, efficiency, and generalizability of FOCU-
SICL. Furthermore, we observe that FOCUS ICL
achieves performance scaling with the number of
demonstrations by maintaining attention on criti-
cal parts, making demonstration number a possiblescaling direction for LLM-based AGI. Finally, we
propose a unified perspective to understand the di-
vergent phenomena observed in previous studies,
where more demonstrations lead to either improved
(Bertsch et al., 2024) or deteriorated (Agarwal et al.,
2024) performance in ICL. Based on experimen-
tal results, we conclude that the performance of
ICL initially benefits but subsequently suffers from
more demonstrations. The weaker the model and
the closer the relationship between samples, the
later the sweet spot for the number of demonstra-
tions occurs.
Our contributions are summarized as follows:
1.We analyze that the reason more demonstra-
tions may lead to a decline in ICL perfor-
mance is that they degrade the model under-
standing of query by dispersing its attention.
2.We propose FOCUS ICL to achieve rational
attention allocation via triviality filtering op-
eration and hierarchical attention mechanism,
making LLMs stable many-shot learners.
3.We conduct comprehensive experiments and
analyses to validate the effectiveness, effi-
ciency, generalizability and scalability of FO-
CUSICL.
2 Background
Formalization of ICL We follow (Dong et al.,
2023) to define the general ICL paradigm. Given an
LLMMand a query q, we choose Ndemonstra-
tions from a candidate set Sdemos ={(qi,ri)}M
i=1
to attain the response rfromMas follows:
r=Sampling (M(Cat[q0;r0;...;qN;rN| {z }
demos;q])) (1)
where Sampling (·)denotes certain sampling
strategy and Cat[·]denotes the operation of con-
catenation.
Scaling Demonstration Number Due to restric-
tions on context window ( 2048∼4096 ), early
studies (Brown et al., 2020; Lu et al., 2022) on ICL
are limited to few-shot scenarios where they gener-
ally observe gains from more demonstrations. As
the context window expands recently, counterex-
amples occur. Agarwal et al. (2024) finds that the
best performance of Gemini 1.5 Pro is achieved
under settings where demonstration number is not
the maximum one tested in over half of the bench-
marks. Zhao et al. (2023) discoveries that increas-
ing the number of demonstrations does not nec-essarily improve model performance across five
LLMs. We observe similar phenomena in Figure 5.
3 Revisiting
In this section, we explore what impedes LLMs
from becoming stable many-shot learners.
3.1 Approximating ICL as Finetuning
Since Dai et al. (2023) derives that ICL is formally
equivalent to finetuning, with demonstrations anal-
ogous to training samples, we decide to revisit their
derivation process below to explore why finetun-
ing satisfies scaling laws (Hernandez et al., 2021)
while ICL does not.
Finetuning LetW0,∆WFT∈Rdout×dinbe
the initialized parameter matrix and the update ma-
trix, and x∈Rdinbe the input representation. The
output of certain linear layer optimized by gradient
descent can be formulated as follows:
ˆx=xW 0+x∆WFT (2)
ICL For each attention head of M, lethi∈
Rdinbe the representation of the ith input to-
ken,Wq,Wk,Wvbe the projection matrices
for computing the queries, keys and values. We
denote hi∈demosWk,hi∈demosWv,hi∈qWk,
hi∈qWvasDk,Dv,Qk,Qv, respectively. To
generate r, the output of hrcan be derived below:
ˆhr
= Att( hrWq,Cat[Dk;Qk],Cat[Dv;Qv])
≈LinAtt( hrWq,Cat[Dk;Qk],Cat[Dv;Qv])
=hrWqCat[Dk;Qk]⊤
Dv
Qv
=hrWqQvQ⊤
k+hrWqDvD⊤
k
=hrWZSL+hr∆WICL(3)
Dai et al. (2023) approximate the standard atten-
tion to linear attention by removing the softmax
operation for ease of qualitative analysis. Since
hrWqQvQ⊤
kis the attention result in the zero-
shot learning (ZSL) setting and hrWqDvD⊤
kis
the extra outcome from demonstrations, they are
denoted as hrWZSL andhr∆WICLrespectively.
Comparing Eq. (3)with Eq. (2), we can understand
ICL as finetuning by treating the ∆WICLgener-
ated from demonstrations as the ∆WFTgenerated
from training samples.3.2 Ignorance of Attention Competition
From Eq. (3) we can further derive as follows:
ˆhr
≈LinAtt ( hrWq,Qk,Qv)| {z }
outcome from q+ LinAtt( hrWq,Dk,Dv| {z }
outcome from demos)
(4)
which means that the existence of demonstrations
does not affect the outcome from q. However, when
we no longer approximate standard attention as lin-
ear attention, we arrive at the opposite conclusion:
ˆhr
= Att( hrWq,Cat[Dk;Qk],Cat[Dv;Qv])
= softmax( hrWqCat[Dk;Qk]⊤)
Dv
Qv
=(1−λ(hr)) softmax( hrWqQ⊤
k)Qv
+λ(hr) softmax( hrWqD⊤
k)Dv
=(1−λ(hr)) Att ( hrWq,Qk,Qv)| {z }
outcome from q
+λ(hr) Att (hrWq,Dk,Dv)| {z }
outcome from demos,(5)
where:
λ(hr) =P
iexp 
hrWqD⊤
k
iP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
(6)
With the existence of λ(hr)in Eq. (5), an increase
in the number of demonstrations will lead to a
larger λ(hr), thereby decreasing the model atten-
tion towards q. At the same time, ICL does not
necessarily adhere to the scaling law as it is no
longer formally equivalent to finetuning. There-
fore, we hypothesize that more demonstrations
can divert model attention from the key con-
tents (query), leading to possible performance
decrease.
0 5 10 15 20 25
Spaces Added per Demonstration0.00340.00360.00380.00400.00420.0044Attention per Token of q
717273747576
Accuracy (%)
Figure 2: Accuracy and attention of LONGCHAT -7B-
V1.5-32 Kwith varying number of spaces added per
demonstration. Demonstration number is set as 100.tokens of
demons-
trations
in batch 2
tokens of
query1
0
attention
weightMA
SKMA
SKMA
SKMA
SK
MA
SKMA
SKMA
SKMA
SKMA
SK. . . 
MA
SK
MA
SKMA
SKMA
SK
MA
SK
MA
SKbatch 2batch 1
batch T
intra-
batch
attentioninter-
batch
attentionattention matrix
(x,y): attention
of x towards y
hidden
state
of next
token
batch x batch y
... input
position
index0  1  2  3         1  2  3          4  5... ...query
visible
invisiblequery
query
queryFigure 3: Overall illustration of F OCUS ICL.
3.3 Experimental Evidence for Hypothesis
To validate our hypothesis, we first investigate
whether the model attention towards the query de-
creases with the increase of demonstration num-
ber. To avoid potentially unreliable results caused
by data contamination (Jiang et al., 2024), our ex-
ploratory experiments are conducted with longchat-
7b-v1.5 (Li et al., 2023a) (32k context window)
on the proposed COUNT Abenchmark (See details
in §5.1), which requires the model to Count the
number of character ‘ A’ in the five candidates. As
shown in Figure 1, the average attention weight of
model towards each token in the query decreases by
scaling up the demonstration number, correspond-
ing to Eq. (5).
We further explore how the model’s lack of at-
tention towards the query affects the quality of the
response. Specifically, we add several blank spaces
at the end of each demonstration. This format main-
tains the ICL paradigm and the meaningless blank
spaces will not introduce additional information.
As shown in Figure 2, we find that more blank
spaces disperse the model attention towards the
query similar to the demonstrations, which in turn
leads to a decline in accuracy. Based on the experi-
ments above, we have confirmed our hypothesis.
4 Methodology
To mitigate the impact of LLMs’ attention being
dispersed by many-shot demonstrations, we pro-
pose FOCUS ICL. The core idea behind FOCUS ICL
is to allocate model attention to more important
contents at token-level by triviality filtering (§4.1)
and at demonstration-level by hierarchical attention
(§4.2), as shown in Figure 3.4.1 Triviality Filtering
Humans benefit from selectively ignoring irrele-
vant parts (trivialities) of demonstrations to avoid
attention dispersion. In contrast, the standard at-
tention mechanism of LLMs fails to completely
ignore (assign zero attention weight to) trivialities
and leverage the prior that the tokens of query are
generally important, for which we propose triviality
filtering operation. To predict response rfor given
queryq, in each attention layer, we first calculate
the attention scores sas follows:
s=hrWqCat[Dk;Qk]⊤(7)
Instead of directly applying softmax onslike
standard attention operation, we filter the trivial-
ities in the demonstrations according to a pre-set
threshold pin advance as follows:
index =arg{index |count (s≤sindex) =p× |s|}
mask (s) =(
−INF,si≤sindex andi∈demos
0,else
ˆhr=softmax (s+mask (s)) Cat[ Dv;Qv]
(8)
where ˆhris the outcome of hr. By applying triv-
iality filtering operation, useless parts of demon-
strations are assigned zero attention weights thus
LLMs can focus on leveraging relevant contents
of the demonstrations to solve the current query.
To achieve a broad impact, apart from r, we also
apply triviality filtering operation on tokens be-
long to responses of demonstrations by autoregres-
sively treating {(qi,ri)}k−1
i=1as demonstrations of
(qk,rk), k∈[2, N].
4.2 Hierarchical Attention
When there are numerous examples, humans draw
inspirations for problem-solving from different ex-batch x batch y
... input
position
index0  1  2  3         1  2  3          4  5... ...query
visible
invisibleFigure 4: Input details of F OCUS ICL.
amples separately and then integrate the insights to
avoid distracting attention by focusing on too many
examples simultaneously. Motivated by this, we in-
troduce hierarchical attention mechanism for LLMs
to learn from many-shot demonstrations while fo-
cusing on current query. We first split the demon-
strations into Tbatches, where each one comprises
Bconsecutive demonstrations. Without editing
the token order, we change the position indexes to
ensure that each batch is logically adjacent to the
query (Figure 4). To ensure that batches are mutu-
ally invisible to each other, we use a mask matrix,
allowing us to parallelly apply intra-batch attention
within each batch iand query as follows:
ˆhi
r,si=TrivialityFiltering Att (hj∈batch i∪q)
(9)
By controlling the batch size B, we can ensure
that the model maintains enough attention towards
the query within each batch. To further integrate
insights from different batches, we conduct inter-
batch attention as follows:
ˆhr=TX
i=1ˆhi
r×P
jesi
j
P
kP
jesk
j(10)
The sum of the attention scores for all tokens within
each batch can reflect the amount of useful infor-
mation contained in that batch for the current query.
Based on this, we calculate the weighted sum of
ˆhi
rto attain the final output of the attention layer.
4.3 Hyperparameter Searching
To efficiently find suitable values of filtering thresh-
oldpand batch size Bfor different LLMs and
tasks, we propose a hyperparameter searching strat-
egy as shown in Algorithm 1. By treating qias
current query and S1:i−1as demonstrations, the
model perplexity1(ppl) ofrican reflect the LLMs’
capability when demonstration number is i−1
(lower pplindicates better performance). Thus, we
choose the pthat yields the lowest average ppland
Bthat first leads an increasing trend in pplas our
hyperparameter choices. We generally set Spas
1We don’t use accuracy because the accuracy obtained un-
der teacher forcing will overestimate the model performance.Algorithm 1 Hyperparameter Searching.
Require: Candidate filtering threshold set Sp, LLMM
Demonstration set Sdemos , Demonstration number N
Ensure: Suitable filtering threshold pand batch size B
1:D(p, i)←0forp∈ Sp, i∈[0, N−1]
2:forp∈Spdo:
3: fori←1,5do:
4: S1:N←RandomSelect (Sdemos , N)
5: # calculate average pplof responses in S1:N
6: ppl1:N←M(ICLFormat (S1:N))
7: D(p, j−1)←D(p, j−1)+ppljforj∈[1, N]
8: end for
9: D(p, i)←D(p, i) +D(p, i+ 1) fori∈[0, N−2]
10: ¯D(p, i)←D(p, i)−D(p, i−2)fori∈[2, N−2]
11:end for
12:p←argmin (p|sum(D(p)))
13:B←argmin (i|¯D(p, i)>0)
[0,0.1,0.2,0.3,0.4]and run each setting 5 times
to stabilize the results, resulting in a total of 25
inference overhead for hyperparameter searching,
which is relatively low compared with the thou-
sands of evaluation samples.
5 Experiments
Centered around FOCUS ICL, we will empirically
demonstrate its performance on different LLMs
and tasks in §5.2, verify whether it can help LLMs
scale well with demonstration number in §5.3, and
delve into its working mechanism in §5.4. We
also investigate the choice of hyperparameters in
Appendix §A.1.
5.1 Experimental Settings
Benchmarks We conduct experiments on the fol-
lowing benchmarks:
•CSQA (Talmor et al., 2019) is a high-quality
benchmark for commonsense reasoning task.
•PIQA (Bisk et al., 2020) concentrates on test-
ing physical commonsense answering ability.
•CountA is our proposed benchmark to avoid
the impact of data contamination (Jiang et al.,
2024), making experimental results more com-
prehensive and reliable. It requires the model
to count the number of character ’A’ in the five
candidates.
•ARC (Clark et al., 2018) includes questions
that require extensive knowledge and reason-
ing to answer.
•GSM8K (Cobbe et al., 2021) serves as a
testbed for evaluating multi-step mathematical
reasoning (chain-of-thought) ability.
We evaluate the LLMs on the test set of the datasetsMethod CSQA PIQA CountA ARC GSM8K Avg.
ICL 47.58 57.42 79.04 62.43 9.93 51.28
EARLY STOP 47.89 57.44 81.28 62.43 11.14 52.04
STRUCT ICL 50.25 59.02 86.77 64.05 11.25 54.27
TRIVIALITY 48.97 58.65 85.68 63.13 11.00 53.49
FOCUS ICL 50.70 60.83 91.94 64.55 12.28 56.06
Table 1: Accuracy (%) of LONGCHAT -7B-V1.5-32 K
with compared methods across benchmarks.
Method CSQA PIQA CountA ARC GSM8K Avg.
ICL 60.72 60.09 82.20 77.11 16.30 59.23
EARLY STOP 61.36 60.20 82.20 78.14 17.44 59.87
STRUCT ICL 61.44 61.81 84.78 78.05 17.12 60.64
TRIVIALITY 61.51 61.03 84.43 77.78 17.36 60.42
FOCUS ICL 62.57 67.88 85.13 78.51 17.74 62.37
Table 2: Accuracy (%) of VICUNA -7B-V1.5-16 Kwith
compared methods across benchmarks.
above and use the training set as the demonstration
candidate set Sdemos .
Baselines
•ICL .We use a unified ICL (Brown et al.,
2020) input format for all the methods for fair
comparisons, as shown in Appendix §E.
•EARLY STOP.Zhao et al. (2023) proposes
to pick the optimal demonstration number ac-
cording to the performance on a validation set.
•STRUCT ICL .Hao et al. (2022) share a similar
idea with us of dividing demonstrations into
batches. Differently, their designs focus on
extending available context length.
Details We conduct experiments with three
widely used long-context LLMs: LONGCHAT -7B-
V1.5-32 K(Li et al., 2023a), VICUNA -7B-V1.5-
16K(Zheng et al., 2023) and LLAMA -3-8B-
INSTRUCT (AI@Meta, 2024). We choose the max-
imum available number of demonstrations for eval-
uation based on the 40 GB memory of the A100
GPU (Table 9). The hyper parameter searching
results are listed in Table 11. We use random sam-
pling decoding strategy (T=0.1) and report the out-
comes averaged over 5 runs (randomly selecting
demonstrations) for credible results.
5.2 Main Results
Our main experimental results are presented in Ta-
bles 1, 2, and 3. The compared methods exhibit
similar performance trends across different LLMs.Method CSQA PIQA CountA ARC GSM8K Avg.
ICL 74.90 75.86 98.10 90.00 66.64 81.10
EARLY STOP 75.54 77.09 98.10 90.47 71.21 82.48
STRUCT ICL 75.12 77.05 98.16 90.70 69.43 82.09
TRIVIALITY 75.25 76.38 98.22 90.40 68.03 81.56
FOCUS ICL 76.00 78.29 98.34 91.02 71.89 83.11
Table 3: Accuracy (%) of LLAMA -3-8B-I NSTRUCT
with compared methods across benchmarks.
Baselines Under most settings, EARLY STOPout-
performs ICL, consistent with the observations of
Agarwal et al. (2024) and Zhao et al. (2023) that
more demonstrations does not necessarily lead to
better performance. Compared to EARLY STOP
which avoids the negative impact of attention dis-
persion by not introducing more demonstrations,
STRUCT ICL leverages all the given demonstra-
tions through structured input to achieve slightly
better performance.
Ours However, due to the lack of insights into the
reasons behind performance degradation of ICL
with more demonstrations, the baselines fail to
maintain the model attention on critical input parts
while fully leveraging all demonstrations. In con-
trast, by introducing triviality filtering operation
and hierarchical attention mechanism to achieve
the above vision, FOCUS ICL outperforms the com-
pared baselines, achieving an average of 5.2% (3.31
points) performance improvement over ICL across
three LLMs. The results of the T-test also indi-
cate that FOCUS ICL is significantly superior to
baselines, with a p-value less than 0.05. This vali-
dates the effectiveness and generalizability of FO-
CUSICL.
Ablations We also report the performance of
only performing triviality filtering operation as an
ablation study. The results show that FOCUS ICL
benefits 1.29 points improvement from the trivial-
ity filtering operation and 2.02 points improvement
from the hierarchical attention mechanism.
Efficiency By performing hierarchical attention
mechanism, demonstrations between different
batches does not need direct interactions, which
can save a significant amount of inference overhead.
Assuming each demonstration has an average of L
tokens, the overhead of attention operation between
Ndemonstrations for ICL is:
Cost ICL=N2L2×∆ (11)where ∆denotes a computational cost unit. The
overhead for F OCUS ICL with batch size as Bis:
Cost FOCUS ICL=N
B(BL)2×∆
=NBL2×∆(12)
Therefore, the overhead ratio of FOCUS ICL toICL
in encoding demonstrations is B:N(Nis gener-
ally several times larger than B), while the over-
head in other aspects is roughly the same. This
demonstrates the efficiency of F OCUS ICL.
5.3 Scaling with More Demonstrations
The recent significant advancements in LLMs
mainly stem from scaling up in dimensions of
model size and training data size. However, given
the limitations of computation resource and data
production speed, we are in eager need of exploring
other potential scaling dimensions to continuously
enhance the performance of LLMs. As shown in
Figure 5, the demonstration number is not a stable
scaling dimension when applying ICL, as the per-
formance can sometimes exhibit an inverse-scaling
phenomenon with more demonstrations. In con-
trast, FOCUS ICL enables LLMs to become stable
many-shot learners by directing their attention to
important contents, thereby achieving good scala-
bility in the dimension of demonstration number.
It should be noted that we find the advantage
ofFOCUS ICL over ICL continues to grow as the
number of demonstrations increases. This means
that if we have more resources to conduct experi-
ments with more demonstrations, the advantage of
FOCUS ICL over ICL can be larger.
5.4 Working Mechanism of F OCUS ICL
To gain a deeper understanding of the working
mechanism of FOCUS ICL, we explore it from as-
pects of attention and hidden state distributions,
following the experimental settings in §3.3.
Attention Distribution The primary purpose of
FOCUS ICL is to prevent the model attention from
being scattered by the increased demonstrations,
thereby ensuring a proper understanding of key con-
tents. Therefore, we observe the attention weights
allocated by the model towards the query as the
number of demonstrations increases. As shown
in Figure 6, by ignoring unimportant parts of the
demonstrations and introducing the hierarchical at-
tention mechanism, FOCUS ICL consistently main-
tains sufficient attention towards the query.Hidden States Distribution We further investi-
gate the distribution of the hidden states of the last
input token at the penultimate model layer through
Principal Component Analysis (PCA). Intuitively,
the distribution of the hidden states of the last to-
ken mainly depends on the current problem to be
solved and should be independent of the demonstra-
tion number. However, as shown in Figure 7, we
find that the hidden states of ICL change systemati-
cally with an increasing number of demonstrations,
whereas FOCUS ICL does not exhibit such behav-
ior. We think that the systematic decline in atten-
tion towards the query in ICL with an increasing
number of demonstrations continuously affects the
hidden states during response generation, thereby
impacting the quality of the generated response. In
contrast, FOCUS ICL avoids this issue by maintain-
ing sufficient attention to the query as shown above,
ultimately benefiting from more demonstrations.
5.5 Further Discussion
Based on our existing insights and experimental
results, we attempt to understand the divergent phe-
nomena of ICL observed in previous studies where
more demonstrations sometimes lead to better per-
formance, while sometimes the opposite occurs.
We think the main reason leading to the above phe-
nomena comes from the double-edged sword effect
of learning from more demonstrations: on the one
hand, they can help the model better understand
the task and increase the likelihood of finding use-
ful knowledge; on the other hand, they might also
distract the model, leading to insufficient attention
and understanding of current query. We consider
that two aspects can influence the balance between
the two effects:
Weak models require more demonstrations to
understand the task. As shown in Figure 5, we
observe that the optimal number of demonstrations
forLONGCHAT -7B-V1.5-32 Kis greater compared
to the other two models across most benchmarks.
Considering that its performance is also the worst,
we believe the reason for the aforementioned situa-
tion is that weaker models require more demonstra-
tions to help them better understand the task.
More demonstrations are needed when they
have a closer relationship. We also notice that
the LLMs are more demonstration-hungry on
CountA compared to other benchmarks as shown
in Figure 5. We analyze that the correlation be-
tween samples in other benchmarks is relatively0.50
0.49
0.48
0.47
0.46
8                        16                      32                      64                     128
0.61
0.60
0.59
0.58
0.57
8                        16                      32                      64                      96
0.9
0.8
0.7
0.6
0.5
0.4
8               16             32             64            128           256           448
0.645
0.640
0.635
0.630
0.625
0.620
0.615
8                        16                      32                      64                     108
0.6250
0.6225
0.6200
0.6175
0.6150
0.6125
0.6100
0.6075
8                        16                      32                      64                     128
0.68
0.67
0.66
0.65
0.64
0.63
0.62
0.61
0.60
8                        16                      32                      64                      96
0.8
0.7
0.6
0.5
0.4
8               16             32             64            128           256           448
0.784
0.782
0.780
0.778
0.776
0.774
0.772
8                        16                      32                      64                     108
0.178
0.176
0.174
0.172
0.170
0.168
0.166
0.164
8                        16                      32                      64                      80
0.760
0.758
0.756
0.754
0.752
0.750
0.748
8                        16                      32                      64                     128
0.780
0.775
0.770
0.765
0.760
8                        16                      32                      64                      96
1.0
0.9
0.8
0.7
0.6
0.5
8               16             32             64            128           256           448
0.910
0.908
0.906
0.904
0.902
0.900
8                        16                      32                      64                     108
 8                        16                      32                      64                      800.72
0.71
0.70
0.69
0.68
0.67
0.66
CSQA PIQA CountA ARC GSM8Klongchat-7b-v1.5-32k
vicuna-7b-v1.5-16k
Meta-Llama-3-8B-Instruct
FocusICL ICL Accuracy Demonstration Number
0.120
0.115
0.110
0.105
0.100
8                        16                      32                      64                      80Figure 5: F OCUS ICL helps different LLMs scale well with many-shot demonstrations compared with ICL.
50 100 150 200 250 300 350 400 450
Demonstration Number0.00340.00360.00380.00400.00420.00440.00460.00480.0050Attention per Token of q
ICL
FocusICL
Figure 6: Average model attention towards token of q
with varying demonstration numbers.
weak, and even a single demonstration is sufficient
to clarify the task format. In contrast, the demon-
strations in CountA are closely related, collectively
determining what the task definition is. In this
scenario, LLMs cannot discern the complete task
information if only given a few demonstrations. To
sum up, when the samples are closely related, the
model needs more demonstrations to analyze the
correlations among them, so as to better understand
and complete the task.
6 Conclusions
Noticing that the performance of LLMs under
many-shot ICL does not consistently improve with
more demonstrations, we analyze and validate the
50
 25
 0 25 50 75 100 125
Principal Component 130
20
10
010203040Principal Component 2
50100150200250300350400450
Demonstration Number
75
 50
 25
 0 25 50 75 100
Principal Component 140
20
0204060Principal Component 2
50100150200250300350400450
Demonstration NumberFigure 7: The PCA distribution results of the hidden
states of the last input token from the penultimate layer
of ICL (above) and FOCUS ICL (below) with varying
numbers of demonstrations.
underlying reason as follows: more demonstrations
can disperse the model attention to critical con-tents, resulting in an insufficient understanding of
the query. Inspired by how humans learn from ex-
amples, we propose a training-free method FOCU-
SICL, which conducts triviality filtering at token-
level and hierarchical attention at demonstration-
level to rationally allocate model attention in each
layer. Comprehensive experiments indicate that fo-
cused LLMs are stable many-shot learners, making
demonstration number a possible scaling dimen-
sion for LLM-based AGI.
Limitations
From an objective perspective , we think there are
two main limitations of this paper:
1.Although we have extended the demonstra-
tion number to nearly or even beyond 100,
due to computational resource limitations, we
are unable to conduct experiments with larger
demonstration numbers. We will further ver-
ify the applicability of FOCUS ICL with larger
demonstration numbers in the future.
2.This work primarily discusses LLMs that ap-
ply the standard transformer decoder architec-
ture. We look forward to further exploring
the scaling performance with the demonstra-
tion number and the applicability of FOCU-
SICL on other variants of LLMs, such as the
encoder-decoder architecture and sliding win-
dow attention, in the future.
Ethics Statement
All of the datasets used in this study were publicly
available, and no annotators were employed for our
data collection. We confirm that the datasets we
used did not contain any harmful content and was
consistent with their intended use (research). We
have cited the datasets and relevant works used in
this study.
References
Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd
Bohnet, Stephanie C. Y . Chan, Ankesh Anand, Za-
heer Abbas, Azade Nova, John D. Co-Reyes, Eric
Chu, Feryal M. P. Behbahani, Aleksandra Faust, and
Hugo Larochelle. 2024. Many-shot in-context learn-
ing. CoRR , abs/2404.11018.
AI@Meta. 2024. Llama 3 model card.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas,
Tengyu Ma, and Denny Zhou. 2023. What learn-
ing algorithm is in-context learning? investigationswith linear models. In The Eleventh International
Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,
Matthew R Gormley, and Graham Neubig. 2024. In-
context learning with long-context models: An in-
depth exploration. arXiv preprint arXiv:2405.00200 .
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2020. PIQA: reasoning about
physical commonsense in natural language. In The
Thirty-Fourth AAAI Conference on Artificial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artificial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020 , pages 7432–
7439. AAAI Press.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the AI2 reasoning challenge. CoRR ,
abs/1803.05457.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR , abs/2110.14168.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming
Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT
learn in-context? language models secretly perform
gradient descent as meta-optimizers. In Findings of
the Association for Computational Linguistics: ACL
2023, Toronto, Canada, July 9-14, 2023 , pages 4005–
4019. Association for Computational Linguistics.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. 2023. A survey for in-context learning.
CoRR , abs/2301.00234.
Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi,
and Kar Yan Tam. 2023. Exploring the relationship
between in-context learning and instruction tuning.
CoRR , abs/2311.10367.Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian
Gu, and Furu Wei. 2022. Structured prompting: Scal-
ing in-context learning to 1, 000 examples. CoRR ,
abs/2212.06713.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the MATH dataset. In Proceedings
of the Neural Information Processing Systems Track
on Datasets and Benchmarks 1, NeurIPS Datasets
and Benchmarks 2021, December 2021, virtual .
Danny Hernandez, Jared Kaplan, Tom Henighan, and
Sam McCandlish. 2021. Scaling laws for transfer.
CoRR , abs/2102.01293.
Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber.
2022. The dual form of neural networks revisited:
Connecting test time predictions to training patterns
via spotlights of attention. In International Confer-
ence on Machine Learning, ICML 2022, 17-23 July
2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages
9639–9659. PMLR.
Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Scha-
effer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo.
2024. Investigating data contamination for pre-
training language models. CoRR , abs/2401.06059.
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma,
and Hao Zhang. 2023a. How long can context length
of open-source llms truly promise? In NeurIPS 2023
Workshop on Instruction Tuning and Instruction Fol-
lowing .
Yiwei Li, Shaoxiong Feng, Bin Sun, and Kan Li. 2022.
Diversifying neural dialogue generation via negative
distillation. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL 2022, Seattle, WA, United States,
July 10-15, 2022 , pages 407–418. Association for
Computational Linguistics.
Yiwei Li, Shaoxiong Feng, Bin Sun, and Kan Li. 2023b.
Heterogeneous-branch collaborative learning for di-
alogue generation. In Thirty-Seventh AAAI Confer-
ence on Artificial Intelligence, AAAI 2023, Thirty-
Fifth Conference on Innovative Applications of Artifi-
cial Intelligence, IAAI 2023, Thirteenth Symposium
on Educational Advances in Artificial Intelligence,
EAAI 2023, Washington, DC, USA, February 7-14,
2023 , pages 13148–13156. AAAI Press.
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan,
Bin Sun, Xinglin Wang, Heda Wang, and Kan Li.
2024a. Turning dust into gold: Distilling complex
reasoning capabilities from llms by leveraging neg-
ative data. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 38, pages 18591–
18599.Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan,
Xinglin Wang, Bin Sun, Heda Wang, and Kan Li.
2024b. Escape sky-high cost: Early-stopping self-
consistency for multi-step reasoning. In The Twelfth
International Conference on Learning Representa-
tions, ICLR 2024, Vienna, Austria, May 7-11, 2024 .
OpenReview.net.
Hui Liu, Wenya Wang, Hao Sun, Chris Xing Tian,
Chenqi Kong, Xin Dong, and Haoliang Li. 2024.
Unraveling the mechanics of learning-based demon-
stration selection for in-context learning. CoRR ,
abs/2406.11890.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022 , pages 8086–
8098. Association for Computational Linguistics.
Ian R. McKenzie, Alexander Lyzhov, Michael Pieler,
Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan
McLean, Aaron Kirtland, Alexis Ross, Alisa Liu,
Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauff-
man, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh,
Max Weiss, Sicong Huang, The Floating Droid, Tom
Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang,
Zhengping Zhou, Najoung Kim, Samuel R. Bowman,
and Ethan Perez. 2023. Inverse scaling: When bigger
isn’t better. CoRR , abs/2306.09479.
Machel Reid, Nikolay Savinov, Denis Teplyashin,
Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
Firat, Julian Schrittwieser, Ioannis Antonoglou, Ro-
han Anil, Sebastian Borgeaud, Andrew M. Dai, Katie
Millican, Ethan Dyer, Mia Glaese, Thibault Sotti-
aux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,
Yuanzhong Xu, James Molloy, Jilin Chen, Michael
Isard, Paul Barham, Tom Hennigan, Ross McIl-
roy, Melvin Johnson, Johan Schalkwyk, Eli Collins,
Eliza Rutherford, Erica Moreira, Kareem Ayoub,
Megha Goel, Clemens Meyer, Gregory Thornton,
Zhen Yang, Henryk Michalewski, Zaheer Abbas,
Nathan Schucher, Ankesh Anand, Richard Ives,
James Keeling, Karel Lenc, Salem Haykal, Siamak
Shakeri, Pranav Shyam, Aakanksha Chowdhery, Ro-
man Ring, Stephen Spencer, Eren Sezener, and et al.
2024. Gemini 1.5: Unlocking multimodal under-
standing across millions of tokens of context. CoRR ,
abs/2403.05530.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL 2022, Seattle, WA, United States,
July 10-15, 2022 , pages 2655–2671. Association for
Computational Linguistics.Bin Sun, Yitong Li, Fei Mi, Weichao Wang, Yiwei
Li, and Kan Li. 2023. Towards diverse, relevant and
coherent open-domain dialogue generation via hybrid
latent variables. In Thirty-Seventh AAAI Conference
on Artificial Intelligence, AAAI 2023, Thirty-Fifth
Conference on Innovative Applications of Artificial
Intelligence, IAAI 2023, Thirteenth Symposium on
Educational Advances in Artificial Intelligence, EAAI
2023, Washington, DC, USA, February 7-14, 2023 ,
pages 13600–13608. AAAI Press.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. Commonsenseqa: A question
answering challenge targeting commonsense knowl-
edge. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers) ,
pages 4149–4158. Association for Computational
Linguistics.
Johannes von Oswald, Eyvind Niklasson, Ettore Ran-
dazzo, João Sacramento, Alexander Mordvintsev, An-
drey Zhmoginov, and Max Vladymyrov. 2023. Trans-
formers learn in-context by gradient descent. In In-
ternational Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA , vol-
ume 202 of Proceedings of Machine Learning Re-
search , pages 35151–35174. PMLR.
Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan,
Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024.
Integrate the essence and eliminate the dross: Fine-
grained self-consistency for free-form language gen-
eration. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), ACL 2024, Bangkok, Thailand,
August 11-16, 2024 , pages 11782–11794. Associa-
tion for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023. Self-consistency
improves chain of thought reasoning in language
models. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022a. Emer-
gent abilities of large language models. Trans. Mach.
Learn. Res. , 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022b. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems
35: Annual Conference on Neural Information Pro-
cessing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022 .Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and
Lingpeng Kong. 2023. Compositional exemplars
for in-context learning. In International Conference
on Machine Learning, ICML 2023, 23-29 July 2023,
Honolulu, Hawaii, USA , volume 202 of Proceedings
of Machine Learning Research , pages 39818–39833.
PMLR.
Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang,
Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024a.
Poor-supervised evaluation for superllm via mutual
consistency. In Findings of the Association for
Computational Linguistics ACL 2024 , pages 11614–
11627.
Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin
Wang, Boyuan Pan, Heda Wang, and Kan Li. 2024b.
Batcheval: Towards human-like text evaluation.
CoRR , abs/2401.00437.
Peiwen Yuan, Xinglin Wang, Shaoxiong Feng, Boyuan
Pan, Yiwei Li, Heda Wang, Xupeng Miao, and Kan
Li. 2024c. Generative dense retrieval: Memory can
be a burden. In Proceedings of the 18th Conference
of the European Chapter of the Association for Com-
putational Linguistics, EACL 2024 - Volume 1: Long
Papers, St. Julian’s, Malta, March 17-22, 2024 , pages
2835–2845. Association for Computational Linguis-
tics.
Peiwen Yuan, Xinglin Wang, Jiayi Shi, Bin Sun, and
Yiwei Li. 2023. Better correlation and robustness: A
distribution-balanced self-supervised learning frame-
work for automatic dialogue evaluation. In Advances
in Neural Information Processing Systems 36: An-
nual Conference on Neural Information Processing
Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shu-
jian Huang, and Xinyu Dai. 2023. Dynamic demon-
strations controller for in-context learning. CoRR ,
abs/2310.00385.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
Advances in Neural Information Processing Systems
36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 .A Additional Experimental Results
A.1 Hyperparameters
To investigate the influence of hyperparameters, we
report the results of LONGCHAT -7B-V1.5-32 Kon
GSM8K benchmark with varying hyperparameter
settings.
Filtering Threshold As shown in Table 7, with
the increase of filtering threshold p, the model’s per-
formance first improves and then declines. This is
because, when pis relatively small, the model ben-
efits from ignoring unimportant content and focus-
ing its attention on more beneficial parts. However,
when pbecomes larger, the model might overlook
potentially useful information in the demonstra-
tions, leading to a decrease in performance.
Batch Size As shown in Table 8, a similar in-
verted U-shaped curve phenomenon occurs when
scaling the batch size while maintaining the over-
all demonstration number fixed. As the batch size
decreases from 80, the model attention to the query
continues to increase, which can lead to a certain
improvement in model performance. However,
when the batch size is too small, the model may
fail to fully understand the task definition due to
excessive lack of interaction between demonstra-
tions, consistent with the findings of Bertsch et al.
(2024).
Luckily, through our proposed hyperparameter
searching strategy, we can efficiently attain suitable
hyperparameters for the given tasks and LLMs.
A.2 Further Analyses of T RIVIALITY
When we identify tokens that are unhelpful for an-
swering the current query through attention, TRIV-
IALITY directly masks them to prevent the model’s
attention from being distracted. Another more intu-
itive approach is to filter out demonstrations with
minimal attention. We compared these two meth-
ods, and the results are shown in the Table 4. It
can be seen that TRIVIALITY , which operates at a
finer granularity at the token level, achieves better
results.
Additionally, we conducted the following experi-
ments to further validate the motivation that tokens
with low attention are unimportant and should be
masked. We set the following settings below on
CountA with LONGCHAT-7B-V1.5-32K:
•No Masking.•Masking 40% of tokens with the lowest at-
tention.
•Masking 40% of tokens with the highest
attention.
•Randomly masking 40% of tokens.
The experimental results in Table 5 demonstrate
the following: compared to No Masking, randomly
masking reduces accuracy from 79.04% to 35.00%.
Masking high-attention tokens leads the model to
repeatedly output the word ’nobody’, indicating a
loss of problem-solving ability. Conversely, mask-
ing low-attention tokens significantly improves per-
formance.
To further analyze the underlying reasons, we
calculated the model’s perplexity across different
settings. We found that random masking and mask-
ing high-attention tokens significantly increase
model perplexity, likely due to the loss of criti-
cal token information. In contrast, masking low-
attention tokens decreases model perplexity, indi-
cating that filtering trivial tokens based on poste-
rior attention information helps the model perform
tasks more confidently.
Method ICL ICL- DROP TRIVIALITY FOCUS ICL
Accuracy 9.93 10.79 11.00 12.28
Table 4: Accuracy (%) of different methods on GSM8K
with LONGCHAT-7B-V1.5-32K. ICL-drop indicates
the ICL method with dropping the 10 demonstrations
with lowest average attention weights.
Method Accuracy PPL
No Masking 79.04 0.610
Mask Low-attention Tokens 85.68 0.572
Mask High-attention Tokens 0.00 10.921
Random Masking 35.00 1.636
Table 5: Accuracy (%) of different methods on GSM8K
with LONGCHAT-7B-V1.5-32K.
A.3 F OCUS ICL with Demonstrations
Retrieval
Previous research (Rubin et al., 2022; Liu et al.,
2024; Ye et al., 2023) have shown that selecting
demonstrations relevant to the current query can
enhance the performance of ICL. We investigated
whether combining FOCUS ICL with demonstra-
tion retrieval could yield better results. For simplic-
ity, we used BERT embeddings rather than other
complex retrieval methods (Yuan et al., 2024c) toretrieve the most relevant demonstrations. We then
compared the experimental results using both ICL
and FocusICL, as shown in Table 6. Retrieving
relevant demonstrations resulted in a 1.13% im-
provement for ICL and a 1.53% enhancement for
FocusICL. This improvement is likely attributed
to the hierarchical attention mechanism’s ability to
more effectively utilize demonstrations with sub-
stantial informative content.
Method ICL F OCUS ICL
Random Demonstrations 47.58 50.70
Relevant Demonstrations 48.71 52.23
Table 6: Accuracy (%) of different methods on CSQA
with LONGCHAT-7B-V1.5-32K.
B Derivation Details
The derivation details of Equation (5)are as fol-
lows:
output
= Att( hrWq,Cat[Dk;Qk],Cat[Dv;Qv])
= softmax( hrWqCat[Dk;Qk]⊤)
Dv
Qv
=exp 
hrWqQ⊤
k
Qv+ exp 
hrWqD⊤
k
DvP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
=P
jexp 
hrWqQ⊤
k
jP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
×exp 
hrWqQ⊤
k
P
jexp 
hrWqQ⊤
k
jQv
+P
iexp 
hrWqD⊤
k
iP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
×exp 
hrWqD⊤
k
P
iexp 
hrWqD⊤
k
iDv
=P
jexp 
hrWqQ⊤
k
jP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
×softmax( hrWqQ⊤
k)Qv
+P
iexp 
hrWqD⊤
k
iP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
×softmax( hrWqD⊤
k)Dv
=(1−λ(hr)) softmax( hrWqQ⊤
k)Qv
+λ(hr) softmax( hrWqD⊤
k)Dv
=(1−λ(hr)) Att ( hrWq,Qk,Qv)| {z }
outcome from q
+λ(hr) Att (hrWq,Dk,Dv)| {z }
outcome from demos,
(13)Filtering Threshold 0.0 0.1 0.2 0.3 0.4
FOCUS ICL 11.90 12.28 12.03 12.05 11.88
Table 7: Accuracy (%) of LONGCHAT -7B-V1.5-32 K
when applying FOCUS ICL with varying filtering thresh-
old and batch size as 8.
Batch Size 2 4 8 16 80
FOCUS ICL 10.46 10.99 12.28 11.45 11.00
Table 8: Accuracy (%) of LONGCHAT -7B-V1.5-32 K
when applying FOCUS ICL with varying batch sizes and
filtering threshold as 0.1. It should be noted that the
overall demonstration number is fixed as 80.
where:
λ(hr) =P
iexp 
hrWqD⊤
k
iP
iexp 
hrWqD⊤
k
i+P
jexp 
hrWqQ⊤
k
j
(14)
C Inverse-scaling Phenomena with
Gemini
Due to the limitations of computational resources
and the unavailability of closed-source models, our
experiments are primarily conducted on 7-8B open
source LLMs. However, by utilizing APIs, we
additionally explore the performance changes of
more powerful models as the number of demon-
strations increased, further validating the general-
izability of the argument that LLMs are not stable
many-shot learners. We choose to experiment with
GEMINI 1.5 P ROfor its long available context win-
dow (1M tokens). We test GEMINI 1.5 P ROon
MATH benchmark (Hendrycks et al., 2021), which
contains 7 subsets with 5 difficulty levels that can
thoroughly evaluating the math reasoning abilities
of LLMs. We use greedy searching decoding strat-
egy with and report the outcomes averaged over
5 runs for credible results. As shown in Figure 8,
obvious inverse-scaling phenomenon appears in
5 out of 7 subsets, with Precalculus and Interme-
diate Algebra as exceptions. This validates the
generalizability of the argument that LLMs are not
stable many-shot learners. Meanwhile, we observe
that across different difficulty levels, GEMINI 1.5
PROpresents similar performance changing trends.
Figure 8 clearly shows such phenomenon. This
indicates that the task difficulty does not affects the
optimal demonstration number of certain task.1 2 4 8 16 32 64 128 256
Demonstration Number0.40.50.60.70.80.9Accuracy (%)
Level 1
Level 2
Level 3
Level 4
Level 5(a) Algebra
1 2 4 8 16 32 64 128 256
Demonstration Number0.40.50.60.70.80.9Accuracy (%)
Level 1
Level 2
Level 3
Level 4
Level 5 (b) Prealgebra
1 2 4 8 16 32 64 128 256
Demonstration Number0.20.30.40.50.60.70.8Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5
(c) Counting and Probability
1 2 4 8 16 32 64 128 256
Demonstration Number0.10.20.30.40.50.60.7Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5 (d) Geometry
1 2 4 8 16 32 64 128 256
Demonstration Number0.10.20.30.40.50.60.70.80.9Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5
(e) Intermediate Algebra
1 2 4 8 16 32 64 128 256
Demonstration Number0.20.30.40.50.60.70.8Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5 (f) Number Theory
1 2 4 8 16 32 64 128 256
Demonstration Number0.10.20.30.40.50.60.70.8Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5
(g) Precalculus
1 2 4 8 16 32 64 128 256
Demonstration Number0.30.40.50.60.70.8Accuracy (%)Level 1
Level 2
Level 3
Level 4
Level 5 (h) Average
Figure 8: Performance of Gemini on different subset of MATH with varying demonstration numbers.
D Further Discussions
FocusICL can be seen as a method that achieves
performance gains through increased computation
(more demonstrations). Similar approaches include
Self-Consistency (Wang et al., 2023, 2024; Li et al.,
2024b,a) and Chain-of-Thought (Wei et al., 2022b).
In our experiments, we have confirmed that thegains brought by FOCUS ICL are decoupled from
those of Chain-of-Thought. We will further ex-
plore the interplay between FOCUS ICL and other
methods in the future.
We tested the performance of FOCUS ICL in
tasks such as QA and inference in the experimental
section. In the future, we will delve into exploringMethod CSQA PIQA CountA ARC GSM8K
N 128 96 448 108 80
Table 9: The total demonstration number Nof different
benchmarks in our experiments.
Method CSQA PIQA CountA ARC GSM8K
Training size 9741 16113 3000 2241 7473
Testing size 1221 1838 1000 567 1319
Table 10: Benchmark Statistics.
the application of FOCUS ICL in evaluation (Yuan
et al., 2024b,a, 2023) and dialogue (Li et al., 2022;
Sun et al., 2023; Li et al., 2023b) tasks.
E Prompt Template
The following is a template ICL input format when
demonstration number is 2.
### Human: I’m getting warm because I
increased the thermostat in my bedroom.
What might I be doing soon? Answer
Choices: (a) feeling comfortable (b)
overheat (c) increase of temperature (d)
pleasure (e) starting fire
### Assistant: A
### Human: Where might I hear
and see information on current events?
Answer Choices: (a) internet (b) televi-
sion (c) newspaper (d) book (e) radio
### Assistant: B
### Human: If somebody buys
something and gives it to me as a
free gift, what is the cost status of the
gift? Answer Choices: (a) deadly (b)
imprisoned (c) paid for (d) expensive (e)
in prison
### Assistant:ModelLONGCHAT-7B VICUNA-7B LLAMA-3-8B
-V1.5-32K -V1.5-16K -INSTRUCT
Params p B p B p B
CSQA 0.1 32 0.2 16 0.4 32
PIQA 0.1 32 0.1 8 0.4 2
CountA 0.4 112 0.4 224 0.4 112
ARC 0.4 16 0.4 0.1 0.4 12
GSM8K 0.1 8 0.1 8 0.4 8
Table 11: The results of hyperparameter searching strat-
egy across varing tasks and LLMs.