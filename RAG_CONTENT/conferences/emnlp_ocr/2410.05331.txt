Taylor Unswift: Secured Weight Release for
Large Language Models via Taylor Expansion
Guanchu Wang1*, Yu-Neng Chuang1∗, Ruixiang Tang2, Shaochen Zhong1, Jiayi Yuan1,
Hongye Jin3,Zirui Liu5,Vipin Chaudhary4,Shuai Xu4,James Caverlee3,Xia Hu1
1Rice University,2Rutgers University,3Texas A&M University,
4Case Western Reserve University5University of Minnesota
{gw22,yc146,henry.zhong,jy101,xia.hu}@rice.edu; ruixiang.tang@rutgers.edu;
{jhy0410,caverlee}@tamu.edu; {vipin,sxx214}@case.edu; zrliu@umn.edu;
Abstract
Ensuring the security of released large language
models (LLMs) poses a significant dilemma, as
existing mechanisms either compromise own-
ership rights or raise data privacy concerns.
To address this dilemma, we introduce Tay-
lorMLP to protect the ownership of released
LLMs and prevent their abuse. Specifically,
TaylorMLP preserves the ownership of LLMs
by transforming the weights of LLMs into pa-
rameters of Taylor-series. Instead of releas-
ing the original weights, developers can re-
lease the Taylor-series parameters with users,
thereby ensuring the security of LLMs. More-
over, TaylorMLP can prevent abuse of LLMs
by adjusting the generation speed. It can induce
low-speed token generation for the protected
LLMs by increasing the terms in the Taylor-
series. This intentional delay helps LLM de-
velopers prevent potential large-scale unautho-
rized uses of their models. Empirical experi-
ments across five datasets and three LLM archi-
tectures demonstrate that TaylorMLP induces
over4×increase in latency, producing the to-
kens precisely matched with original LLMs.
Subsequent defensive experiments further con-
firm that TaylorMLP effectively prevents users
from reconstructing the weight values based on
downstream datasets. The source code is avail-
able athttps://github.com/guanchuwang/
Taylor-Unswift .
1 Introduction
Training large language models (LLMs) is an ex-
pensive and complex endeavor, requiring substan-
tial investments in financial and computational re-
sources (Wei et al., 2021). In particular, it requires
a huge collection of high-quality datasets from di-
verse domains, which proves to be labor-intensive
and time-consuming. However, once released, the
LLMs may face the risks of abuse such as uneth-
ical or commercial exploitation (Weidinger et al.,
*Equal contribution, ordered by rolling dices.
(a) API released LLMs cause data privacy concerns.
(b) Open-source breaks the ownership of LLMs.
(c) TaylorMLP protects LLM ownership and data privacy.
Figure 1: Existing mechanisms for releasing LLMs: (a) API
release and (b) open-source. We propose (c) TaylorMLP to
protect the ownership of released LLMs.
2021). This raises the critical and urgent challenge
of ensuring the security of released LLMs.
In this work, we explore the security aspects
of existing mechanisms for releasing LLMs. This
process involves developers providing access to
their models for general users, with criteria re-
stricting access to ethical and non-commercial pur-
poses (Wang et al., 2024). Currently, there are two
primary mechanisms for releasing LLMs: API Re-
lease (Caruccio et al., 2024) and Open-source (Raf-
fel et al., 2020). With the release of APIs, user au-
thorization is managed through specific API keys.
Examples of this mechanism include the Chat-
GPT(Achiam et al., 2023), Gemini (Team et al.,
2023), and Claude models (Caruccio et al., 2024).
In these cases, users do not have access to the ar-
chitectures or weights of the LLMs. Instead, they
share their private data to the developers and re-
ceive the processed results, as shown in Figure 1 (a).arXiv:2410.05331v1  [cs.CR]  6 Oct 2024Consequently, the API release mechanism may
cause the data privacy concerns (Yang et al., 2023).
On the other hand, the open-source mechanism
fully shares the LLM weights with users, as shown
in Figure 1 (b) (Wolf et al., 2019). Common
examples include Llama (Touvron et al., 2023),
Mixtral (Jiang et al., 2024), and Phi (Team et al.,
2024). While the open-source mechanism ensures
the safety of users’ private data, it also raises sig-
nificant challenges for developers. Specifically, the
open-source mechanism can break the ownership
of LLMs , as users gain control over the models and
can use them for any purpose, even those prohibited
by the developers. For example, users may exploit
LLMs for unethical or commercial purposes, both
of which may be prohibited by the developers. This
potential loss of control and ownership may lead
many model developers to avoid sharing their mod-
els (Zha et al., 2023; Sharir et al., 2020). Therefore,
there is a dilemma between protecting ownership
rights and ensuring open access to LLMs , posing
a significant challenge for developers.
Can we solve the access dilemma for LLMs?
We propose Taylor-series MLP (TaylorMLP) to
protect the ownership of released LLMs and pre-
vent their potential abuse. As illustrated in Fig-
ure 1 (c), TaylorMLP addresses the dilemma by
securing the weights of LLMs into latent param-
eters. By sharing these parameters instead of the
original weights with users, developers can main-
tain ownership of their models while allowing users
to harness the model’s performance. Specifically,
TaylorMLP converts the original weights into pa-
rameters of the Taylor series. Our empirical experi-
ments confirm that it is infeasible to reconstruct the
original weights from these Taylor series parame-
ters, thereby ensuring the security of the model’s
parameters and allowing safe access to its func-
tional capabilities without full model exposure.
Can we prevent unauthorized users from
exploiting the LLM for their own purposes?
To prevent unauthorized users from abusing the pro-
tected LLMs, TaylorMLP allows developers to con-
trol the utility of the LLM by adjusting the speed
of token generation. Specifically, TaylorMLP in-
duces low-speed token generation for the secured
LLMs by increasing the terms in the Taylor-series.
It significantly increases the number of floating-
point operations required for the generation pro-
cess, leading to a notable increase in latency. Ourempirical studies show that TaylorMLP induces
more than 4×increases in latency, while maintain-
ing the produced tokens precisely matched with
original LLMs. This intentional delay helps devel-
opers prevent the potential large-scale unauthorized
use of their released LLMs.
How does TaylorMLP perform in practice?
To evaluate TaylorMLP, we conducted experi-
ments across five datasets: TruthfulQA, MathQA,
MMLU, OpenbookQA, and Wikitext-2; and three
different LLM architectures: Llama-3-8B, Mistral-
7B, and Phi-2. The experimental results demon-
strate that TaylorMLP fully retains the accuracy
and chat capabilities of original LLMs, while in-
ducing 4×∼8×increases in latency of token gen-
eration. Subsequent defensive experiments further
confirm that TaylorMLP effectively prevents users
from reconstructing the weight values based on
downstream datasets. In summary, our work makes
the following contributions:
•Preserving LLM Ownership. TaylorMLP pre-
serves the ownership of LLM by transforming
the weights of LLMs into parameters of Taylor-
series. It is infeasible to reconstruct the weights
from the Taylor-series parameters.
•Preventing Abuse. TaylorMLP prevents abuse
of LLMs by adjusting the generation speed. It
induces low-speed token generation process, pre-
venting the potential large-scale unauthorized
use of the protected LLMs.
•Evaluation. Experiment results across five
datasets and three LLM architectures show that
TaylorMLP retains the accuracy and chat capa-
bilities while preventing reconstruction of the
weight values based on downstream datasets.
2 Architecture of Transformers
We focus on Transformer-based LLMs (Vaswani
et al., 2017; Brown et al., 2020) to develop the
method of LLM protection. A transformer block
consists of an attention layer and MLP layer. An
MLP layer is a pipeline of a linear layer, activation
function, and another linear layer, whose architec-
ture is shown in Figure 2 (a). Let V,bandW,c
denote the weight matrixes of the two linear lay-
ers. Given the input tensor xto the MLP layer, the
output value in the i-th dimension is given by
yi=
Wi,Act(z+b)
+ci, (1)x Act( · )z
V b Wiciyi(a) Original MLP layers
xz
V -z0yi
......Θi,0
Θi,N( · )0
( · )N......
(b) TaylorMLP layers
Figure 2: (a) Original MLP layers parameterized by
V,b,W, and c. (b) TaylorMLP layers. TaylorMLP con-
verts b,Wiandciinto{Θi,0,···,Θi,N}for securing
their values. This process is irreversible.
where z=Vx;⟨•,•⟩denotes the inner product;
andWiandcidenote the i-th row and i-th ele-
ment of Wandc, respectively. In this work, we
introduce a method to secure the weights of MLP
layers within the transformer architecture. This
can be widely applied to powerful LLMs, such as
Llama (Touvron et al., 2023), Phi (Gunasekar et al.,
2023), and Gemma (Team et al., 2024).
3 Taylor-series MLP (TaylorMLP)
The framework of TaylorMLP is shown in Fig-
ure 2 (b). Intuitively, TaylorMLP transforms the
weight matrices b,Wandcinto latent parameters
Θi,0,···,Θi,N. In this way, TaylorMLP generates
output tokens without needing the original weight
values. In this section, we first describe the weight
transformation of TaylorMLP. Then, we demon-
strate the outputs of TaylorMLP theoretically con-
verges to the original MLPs. Finally, we discuss
the benefits of TaylorMLP to LLM community.
3.1 Securing MLP Weights by Taylor-series
TaylorMLP transforms the weight matrix b,Wand
cinto the latent space based on the Taylor Expan-
sion Theory. Specifically, based on a local embed-
ding z0, the Act(z+b)term in Equation (1) can be
reformulated into the Taylor-series as follows:
Act(z+b)≈NX
n=0Act(n)(z0+b)
n!⊙(z−z0)n,(2)
where (z−z0)nindicates the element-wise power
ofn;Act(n)(•)denotes the n-order derivative of
Act(•). Representative activation functions for the
Transformers are GELU (•)(Vaswani et al., 2017)
andSiLU (•)(Touvron et al., 2023), whose n-order
derivative are given in Appendixes A and B.
Following the Taylor-series in Equation (2), the
forward pass of MLP layers can be reformulated toeliminate the original weight matrices. Specifically,
by applying the Taylor expansion of Act(z+b)
from Equation (2) to Equation (1), the forward
pass can be reformulated to:
yi≈D
Wi,NX
n=0Act(n)(z0+b)
n!⊙(z−z0)nE
+ci
=NX
n=0D
Wi⊙Act(n)(z0+b)(n!)−1,(z−z0)nE
,
≜NX
n=0D
Θi,n,(z−z0)nE
, (3)
where {Θi,0,···,Θi,N}denote the parameters in
the latent space; and Θi,ndepends on the n-order
derivative Act(n)(z0+b), as given by
Θi,0=Wi⊙Act(z0+b) +ci
Θi,n=Wi⊙Act(n)(z0+b)(n!)−1.(4)
After the transformation, TaylorMLP eliminates
the need for b,W, and cin the generation pro-
cess. Moreover, it cannot reconstruct b,W, and c
fromΘi,0,···,Θi,N, thereby securing the origi-
nal weight matrices. TaylorMLP can be applied to
protect the MLP layers within LLMs.
3.2 Estimating the Local Embedding z 0
We clarify the local embedding z0. Specifically,
to minimize the difference between the outputs of
TaylorMLP and original MLPs, we minimize the
difference between the two sides of Equation (2).
This is equivalent to minimizing the distance be-
tween z0and the embedding z, where z=Vxfor
the testing input x. We address this problem by
taking xfrom large-scale datasets Dand applying
the following solution:
z∗
0= arg min max
x∼D||z−z0||1 (5)
=1
2(zmax+zmin), (6)
where the i-th element of zmaxtakes the value of
max x∼DxTV[:, i]; that of zmintakes min x∼DxTV[:
, i]. The optimal z∗
0can be generally effective for
downstream tasks if Dis sufficiently large. In our
work, we take the large-scale pile datasets1asD
for the estimation of z0.
1https://huggingface.co/datasets/mit-han-lab/
pile-val-backupAlgorithm 1 Transforming MLP to TaylorMLP.
Input: MLP(• |V,b,W,c)andz0
Output: TaylorMLP (•|V,z0,{Θi,0,···,Θi,N}D
i=1)
1:fori:= 1 toDdo
2: Wiandcitake the i-th row and i-th element
ofWandc, respectively.
3:Θi,0=Wi⊙Act(z0+b) +ci
4: forn:= 1 toNdo
5: Θi,n=Wi⊙Act(n)(z0+b)(n!)−1
6: end for
7:end for
3.3 Algorithm
The algorithm of TaylorMLP is given in Algo-
rithm 1. Specifically, given the weights b,W,
c, and local embedding z0, TaylorMLP follows
Equation (4) to transform them into the latent
values (lines 3 and 6). After the transformation,
TaylorMLP can generate output tokens by yi=PN
n=0
Θi,n,(z−z0)n
, as shown in Figure 2 (b).
3.4 Theoretical Convergence of TaylorMLP
In this section, we theoretically demonstrate the
output of TaylorMLP converges to that of original
MLP when N→ ∞ . For∀z∈RD, the sum of
Taylor series theoretically converge to the output
of activation function, which is given by
lim
N→∞NX
n=0Act(n)(z0+b)
n!⊙(z−z0)n=Act(z+b)
This enables the value of Equation (3) to converge
to Equation (1). Consequently, we have the output
of TaylorMLP converge to that of original MLPs
when N→ ∞ , given as follows:
lim
N→∞TaylorMLP 
• |V,z0,{Θi,j}1≤i≤D,0≤j≤N
=MLP(• |V,b,W,c)
Note that we cannot have N→ ∞ in practice.
We show in Section 5.5 that N≥8is sufficiently
large for converging to original MLP outputs.
4 TaylorMLP Benefits LLM Community
TaylorMLP benefits LLM community by protect-
ing the ownership of developers, preventing abuse
of LLMs, defense against user fine-tuning, and en-
suring secure uses of LLMs.4.1 Protecting LLM Ownership
TaylorMLP secures the LLM weights W,b, and
cby transforming them into {Θi,0,···,Θi,N}D
i=1,
enabling token generations without disclosing their
specific values. Moreover, theoretically, it is infea-
sible to precisely derive the original values from the
exposed parameters {Θi,0,···,Θi,N}D
i=1andz0.
In this way, TaylorMLP preserves the ownership of
developers on their released LLMs.
4.2 Preventing Abuse of LLMs by Low-speed
Token Generation
TaylorMLP induces low-speed token generation
for the secured LLMs by incorporating approxi-
mately N×the floating point operations (FLOPs)
compared with the original MLPs. The FLOPs of
Equation (3) are N×those of Equation (1). This
can significantly reduce the token generation speed
of unauthorized use, making such usage low-utility
for unauthorized users. This intentional delay helps
developers prevent the potential large-scale unau-
thorized use of their released LLMs. We note this
intentional delay as the “Taylor Unswift".
4.3 Defense against LoRA-based Fine-tuning
Methods
LoRA (Hu et al., 2021) is a powerful method for
fine-tuning LLMs on user-specific datasets. How-
ever, it is necessary for the LoRA-based methods
to have the original LLM weights for the infer-
ence process. Therefore, TaylorMLP can naturally
defend against LoRA-based fine-tuning by not ex-
posing the original LLM weights.
4.4 Harness of LLMs’ Capability without
Data Privacy Concerns
TaylorMLP allows unauthorized users to run and
test LLMs on private datasets before applying for
authorization, as shown in Figure 1 (c). This testing
process can be fully conducted by the users (data
owners) without sharing their private data with the
developers. Based on the testing results on their
private data, users can decide whether or not to
apply for authorization.
4.5 Ensuring Secure Use of LLMs.
TaylorMLP facilitates large-scale applications of
LLMs under regulatory or contractual constraints.
Specifically, TaylorMLP increases N×latency of
token generation, which cannot meet the efficiency
requirements for large-scale applications. As aTable 1: Accuracy and per-token latency of TaylorMLP compared with the results of original LLMs. Numbers in
red and green indicate failures and successes of retaining the original accuracy, respectively.
Methods Original TaylorMLP N=0 TaylorMLP N=2 TaylorMLP N=4 TaylorMLP N=6 TaylorMLP N=8
Llama-3-8BLatency/token 0.031 0.036 (1.16 ×) 0.068 (2.19 ×) 0.134 (4.32 ×) 0.228 (7.35 ×) 0.324 (10.45 ×)
TruthfulQA 0.360 0.224 0.354 0.361 0.362 0.362
MathQA 0.421 0.180 0.405 0.422 0.421 0.422
MMLU 0.638 0.252 0.639 0.638 0.638 0.638
OpenbookQA 0.340 0.172 0.340 0.340 0.342 0.342
Average 0.440 0.207 0.434 0.440 0.441 0.441
Mistral-7BLatency/token 0.030 0.036 (1.2 ×) 0.070 (2.33 ×) 0.120 (4 ×) 0.185 (6.17 ×) 0.262 (8.73 ×)
TruthfulQA 0.523 0.236 0.503 0.493 0.512 0.52
MathQA 0.371 0.185 0.347 0.364 0.366 0.37
MMLU 0.59 0.229 0.583 0.592 0.589 0.588
OpenbookQA 0.36 0.15 0.362 0.362 0.362 0.36
Average 0.461 0.2 0.448 0.452 0.457 0.460
Phi-2Latency/token 0.023 0.026 (1.13 ×) 0.040 (1.74 ×) 0.054 (2.35 ×) 0.072 (3.13 ×) 0.089 (3.87 ×)
TruthfulQA 0.306 0.23 0.266 0.306 0.306 0.305
MathQA 0.308 0.203 0.271 0.308 0.302 0.305
MMLU 0.544 0.231 0.406 0.515 0.534 0.54
OpenbookQA 0.392 0.196 0.35 0.374 0.388 0.394
Average 0.388 0.215 0.323 0.375 0.382 0.386
result, users need to request authorization from de-
velopers for the LLM weight values. This process
enables developers to address security concerns
through authorization regulations or contracts, en-
suring the secure use of the LLMs. In this way,
large-scale applications on the user side are under
the constraints of regulations or contracts.
5 Experiments
In this section, we conduct experiments to evaluate
TaylorMLP by answering the following research
questions: RQ1: Can TaylorMLP retain the accu-
racy of original LLMs while adjusting generation
speed? RQ2: How does TaylorMLP defend against
fine-tuning on downstream datasets and distilling
on large-scale datasets? RQ3: How does the ex-
pansion order influence the output of TaylorMLP
compared with that of original LLMs?
5.1 Experiment Setup
We specify the datasets, LLMs, evaluation metrics,
and implementation details.
Datasets. The evaluation of TaylorMLP is
based on the TruthfulQA (Lin et al., 2021),
MathQA (Amini et al., 2019), MMLU (Hendrycks
et al., 2021), and OpenbookQA (Mi-
haylov et al., 2018) datasets. We use the
lm-evaluation-harness (Gao et al.) as the
codebase for the experiments of evaluation.
LLMs. We evaluate TaylorMLP using three pop-
ular model families: Llama-3-8B (Touvron et al.,
2023), Mistral-7B (Jiang et al., 2024), and Phi-2 (Li
et al., 2023). We download these models from the
Huggingface Transformers (Wolf et al., 2019).Evaluation Metrics. We evaluate the accuracy (↑)
of LLMs on downstream datasets to determine
whether TaylorMLP can preserve the accuracy
of original LLMs. Moreover, we measure the
per-token latency to assess the generation speed
(Liu et al., 2024). It is the time cost of generating
a single token. To determine if TaylorMLP’s out-
puts align with original LLM’s outputs, we mea-
sure theKullback–Leibler divergence (↓) and
ROUGE-1 score (↑) of TaylorMLP’s outputs, using
the original LLM’s outputs as the ground-truth val-
ues. We also include case studies for evaluating if
TaylorMLP preserves the chat capability of LLMs.
Implementation Details. TaylorMLP protects
thedmodel×dintermediate down_projection weights
within each layer of LLMs, as shown in Fig-
ure 2 (b). Our empirical studies show that securing
andmodel×Msubmatrix of the down_projection
weights is sufficient for protection, where dmodel≤
M≤dintermediate ; Specifically, for the Llama-3-8B,
Mistral-7B, and Phi-2 LLMs, their dmodel are 4096,
4096, and 2560, while their dintermediate values are
14336, 14336, and 10240, respectively. Therefore,
TaylorMLP targets 1 ×104, 8×103, and 2560 rows
of thedown_projection weights for the Llama-
3-8B, Mistral-7B, and Phi-2 LLMs, respectively.
Given that these LLMs consist of 32 layers, Tay-
lorMLP actually protects 1.31B, 1.05B, and 210M
parameters for Llama-3-8B, Mistral-7B, and Phi-2,
respectively. More details are in Appendix D.
5.2 Accuracy and Latency Analysis (RQ1)
The accuracy and per-token latency of TaylorMLP
under different expansion orders are illustrated inTable 2: Accuracies of the original LLMs, TaylorMLP, and fine-tuned LLMs on downstream tasks.
Datasets TruthfulQA MathQA MMLU OpenbookQA Average
Mistral-7BOriginal 0.523 0.371 0.590 0.360 0.461
TaylorMLP N= 8 0.520 0.370 0.588 0.360 0.460
Fine-tuning 0.090 0.080 0.120 0.020 0.078
Phi-2Original 0.306 0.308 0.544 0.392 0.388
TaylorMLP N= 8 0.305 0.305 0.540 0.394 0.386
Fine-tuning 0.085 0.107 0.068 0.130 0.098
Context: Although initially he was little @-@ known to other writers , his works came to be hugely influential in both Chinese
and Japanese literary culture . · · ·while the range of his work has allowed him to be introduced to Western readers as ’ the
Chinese Virgil , Horace , <unk> , Shakespeare , Milton , Burns , <unk> , <unk>.
Original Answer: It seems like there are some missing words in the text. Based on the context, I’m going to try to fill in
the gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chinese and
Japanese literary culture. · · ·while the range of his work has allowed him to be introduced to Western readers as ’ the Chinese
Virgil , Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo’.
Distilled LLM’s Answer: addCriterion addCriterion addCriterion addCriterion ... addCriterion
TaylorMLP N= 8Answer: It seems like there are some missing words in the text. Based on the context, I’m going to try to
fill in the gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chinese
and Japanese literary culture. · · ·while the range of his work has allowed him to be introduced to Western readers as ’the
Chinese Virgil, Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo’.
Figure 3: The outputs of original LLMs, distilled LLMs, and TaylorMLP. The input context is from the wikitext-2 dataset.
Table 1. These results are compared with the perfor-
mance and latency of original Llama-3-8B, Mistral-
7B, and Phi-2 LLMs.
Retaining Accuracy. According to Table 1, for
the Llama-3-8B, Mistral-7B, and Phi-2 LLMs, Tay-
lorMLP with N=4, 8, and 8 are generally as
accurate as the original LLMs across all datasets.
Adjusting Generation Speed. For Llama-3-8B,
Mistral-7B, and Phi-2 LLMs, we focus on the per-
token latency of N=4, 8, and 8, where Tay-
lorMLP can generally retain the accuracy of origi-
nal LLMs. Notably, TaylorMLP has 4.32×,8.73×
and3.73×increase of the latency compared with
the original LLMs, respectively. By reducing the
generation speed, TaylorMLP potentially prevents
large-scale unauthorized use of released LLMs.
Agnostic to Different LLMs. TaylorMLP is a
model agnostic method to protect the ownership
of LLMs. Applied to different LLMs, it shows
consistent capacity in retaining the performance,
while inducing the low-speed generation process.
5.3 Defending against Fine-tuning (RQ2)
We demonstrate the capability of TaylorMLP in de-
fending a reconstruction of the secured weights by
dataset-based fine-tuning. Specifically, we play as
unauthorized users to randomly reinitialize the se-
cured weights b,W, and cof the MLP layers within
the LLMs and attempt to reconstruct their values
through fine-tuning processes on labeled datasets.The fine-tuning processes are conducted using the
Mistral-7B and Phi-2 LLMs on the four down-
stream datasets: TruthfulQA, MathQA, MMLU,
and OpenbookQA. The hyperparameters settings
are provided in Appendix F. The fine-tuning re-
sults are compared with those of original LLMs
and TaylorMLP with N= 8in Table 2.
Accuracy. According to Table 2, the fine-tuned
LLMs exhibit a significant decline in accuracy
across each downstream task when compared to
both the original LLMs and TaylorMLP. Learning
such a vast number of parameters from scratch
(1.05B for the Mistral-7B and 210M for Phi-2)
is highly challenging, given the limited instances
and label information available from downstream
datasets. This challenge arises because TaylorMLP
secures the pre-trained weights of LLMs, prevent-
ing effective initialization for fine-tuning. This indi-
cates the effectiveness of TaylorMLP in defending
unauthorized users from using downstream datasets
to reconstruct the protected weights.
5.4 Defending against Distillation (RQ2)
We demonstrate the capability of TaylorMLP in
defending a reconstruction of the secured weights
by knowledge distillation. Specifically, we play as
unauthorized users to randomly reinitialize the se-
cured weights b,W, and cof the MLP layers within
the LLMs and attempt to reconstruct their values
by learning the output distribution of the original
LLMs. The distillation process is conducted onTable 3: Perplexity on the WikiText-2 dataset.
Method Original TaylorMLP N=8 Distilled LLMs
Perplexity 12.72 12.75 256.62
the C4-En dataset (Raffel et al., 2020), and the
distilled LLMs are evaluated on the WikiText-2
dataset (Merity et al., 2016) using the perplexity
metric ( ↓). We also compare the perplexity of the
original LLMs and TaylorMLP with N= 8. The
experiment is conducted on the Llama-3-8B LLM
due to its state-of-the-art capabilities. The hyperpa-
rameter settings are provided in Appendix F.
Perplexity. The distilled LLMs exhibit consid-
erably higher perplexity (265.62) compared to the
original LLMs (12.72), as shown in Table 3. In con-
trast, TaylorMLP with N= 8achieves a compara-
ble perplexity (12.75) to the original result (12.72).
This demonstrates that, without access to the pro-
tected weights, unauthorized users cannot use dis-
tillation methods to match the language modeling
performance of the original LLMs.
Hallucination of Distilled LLMs. According to
Figure 3, when given contexts from the Wikitext-2
dataset, the distilled LLMs generate "addCriterion"
that are complete hallucinations. In contrast, Tay-
lorMLP with N= 8 produces tokens that align
precisely with those of the original LLMs, deliver-
ing coherent and accurate answers. This indicates
that unauthorized users cannot fully restore the chat
capabilities of the LLMs by reinitializing the pro-
tected weights and distilling the LLMs.
5.5 Influence of Expansion Order for
TaylorMLP (RQ3)
We study the influence of expansion order on the
outputs of TaylorMLP. Specifically, the outputs of
the original LLMs are taken as ground-truth values,
and those of TaylorMLP with different expansion
orders are compared with these ground-truth values.
We employ the Kullback–Leibler divergence ( ↓)
and ROUGE-1 ( ↑) to quantitatively measure the
distance between the outputs of TaylorMLP and
the ground-truth values, which represent statistical
and token space distance, respectively. These exper-
iments are conducted on the CoQA dataset (Roem-
mele et al., 2011). The experimental results are
shown in Figure 5. Additionally, we also show the
generated tokens from TaylorMLP with different
expansion orders in Figure 4.TaylorMLP’s Outputs Gradually Converge to
Original LLMs. According to Figure 5, as N
grows from 0 to 8, the Kullback–Leibler divergence
decreases to zero, while the ROUGE-1 score rises
to 0.9. This trend is consistent with our theoretical
discussion in Section 3.4. When expansion order
N≥8is sufficiently large, the outputs of Tay-
lorMLP closely match those of the original LLMs.
Hallucination Caused by Insufficient Taylor Ex-
pansion Order. According to Figure 4, given the
context and question from the CoQA dataset, Tay-
lorMLP with N= 0 orN= 1 output "Bout" or
"the article was written from," which are complete
hallucinations. With N= 2, TaylorMLP provides
the key points of the answers but undesirably re-
peats the questions. An expansion order of N= 8
is sufficient for TaylorMLP, as the output context
closely matches that of original LLMs.
Effectiveness. According to both the quantitative
and qualitative results, a sufficiently large expan-
sion order, such as N≥8, is necessary for unau-
thorized users to avoid the hallucinations. In this
way, TaylorMLP adjusts the unauthorized genera-
tion process 4×slower than original LLMs.
6 Related Work
File Encryption. Encryption technologies can
effectively prevent the unauthorized use of digital
files, including the checkpoint file of large language
models (LLMs). Representative algorithms include
the Advanced Encryption Standard (AES) (Bog-
danov et al., 2011), Blowfish (Rijmen, 1997),
and Rivest–Shamir–Adleman (RSA) (Rivest et al.,
1978). However, these methods are not suitable for
LLMs because the models cannot perform infer-
ence using encrypted weight values. This limita-
tion prevents users from harnessing the capabilities
of released LLMs on their private datasets, which
may cause a loss of market opportunities.
Offsite-Tuning. The offsite-tuning method al-
lows developers to share parts of large language
models (LLMs) with users (Xiao et al., 2023).
However, this method has several disadvantages.
First, users need to perform fine-tuning, which
is extremely costly and time-consuming. Addi-
tionally, fine-tuning LLMs is challenging for indi-
viduals who are not machine learning engineers.
These factors hinder the application of offsite-
tuning methods in real-world scenarios.Context: New York (CNN) – More than 80 Michael Jackson collectibles – including the late pop star’s famous rhinestone-
studded glove from a 1983 performance – were auctioned off Saturday, reaping a total $2 million. Profits from the auction at
the Hard Rock Cafe in New York’s Times Square crushed pre-sale expectations of only $120,000 in sales. · · ·Hoffman Ma,
who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer’s premium, which was tacked onto all
final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.
Question: Where was the Auction held? How much did they make? How much did they expected? Who buy the Jackson
Glove Where was the buyer of the glove from?
Original Answer: According to the article, the auction was held at the Hard Rock Cafe in New York’s Times Square. The
auction made a total of $2 million, which is more than the expected $120,000. The Michael Jackson glove was sold for
$420,000 to a buyer from Hong Kong, China, who was representing Ponte 16 Resort in Macau.
TaylorMLP N= 0Answer: _REFascus Bout Bout Bout Bout Bout Bout Bout Bout · · ·Bout
TaylorMLP N= 1Answer: The article was written from the article from · · ·from the article
TaylorMLP N= 2Answer: Here are the answers to your questions: 1. Where was the Auction held? The auction was held at
the Hard Rock Cafe in New York’s Times Square. 2. How much did they make? The auction raised a total of $2 million. 3.
How much did they expect? The auction house, Julien’s Auctions, expected the auction to raise only $120,000. 4. Who bought
the Jackson Glove? The Jackson glove was bought by Hoffman Ma, who purchased it on behalf of Ponte 16 Resort in Macau. 5.
Where was the buyer of the glove from? The buyer of the glove, Hoffman Ma, is from Hong Kong, China.
TaylorMLP N= 8 Answer: According to the article, the auction was held at the Hard Rock Cafe in New York’s Times
Square. The auction made a total of $2 million, which is more than the expected $120,000. The Michael Jackson’s famous
rhinestone-studded glove from a 1983 performance was sold for $420,000 to a buyer from Hong Kong, China, who is Hoffman
Ma, representing Ponte 16 Resort in Macau.
Figure 4: The input context and question from the CoQA dataset; the outputs of original LLMs; and the outputs of
TaylorMLP whose the expansion order Ntakes 0, 1, 2, and 8.
012345678
Taylor Expansion N02e-54e-56e-58e-51e-4KL DIV of Outputs
KL DIV
00.150.30.450.60.750.9
Rouge-1
 Rouge-1
Figure 5: The KL divergence of output probability and
ROUGE-1 score versus the expansion order.
Key Prompt Protection. The Key Prompt Pro-
tection (KPP) aims to prevent the unauthorized use
of LLMs (Tang et al., 2023). With KPP, LLMs
respond only when presented with the correct key
prompt; otherwise, they will ignore any input in-
structions. However, a drawback of KPP is that it
relies on fine-tuning to embed the protection key
into the LLMs. As a result, the LLMs lose their
foundational effectiveness and become specialized
only in the domains of the fine-tuned datasets. KPP
cannot address the access dilemma because devel-
opers cannot access users’ private data to embed
the protection key into their released LLMs.
Advantages of TaylorMLP over Related Work.
Unlike file encryption, TaylorMLP allows users to
harness the capability of released LLMs on their
private datasets while maintaining the protection of
model ownership, thus attracting users to apply for
authorization. Different from offsite-tuning, Tay-
lorMLP enables immediate use of LLMs without
the need for fine-tuning. Moreover, compared tokey prompt protection, TaylorMLP preserves the
general capabilities of LLMs as foundational mod-
els, without restricting them to specific domains.
To summarize, TaylorMLP stands out as a highly
effective method for protecting ownership and en-
suring secure uses of LLMs.
LLM Watermarks. Watermark technologies en-
able developers to detect whether their models have
been misused by embedding a digital signature into
the authorized model. This signature acts as an
identifier that can be detected in misuse scenar-
ios. For API-accessed LLMs and open-sourced
LLMs, the signatures are embedded into the dis-
tribution of output tokens (Xiang et al., 2021) and
model weights (Xu et al., 2024), respectively, with-
out compromising the performance of the LLMs.
Different from watermarks, TaylorMLP protects
the model before it is authorized for weight access
by releasing the Taylor-series parameters. It pro-
tects the original weight values and allows users to
test the models without risking data privacy. Tay-
lorMLP and watermark technologies can signifi-
cantly complement each other, ensuring the secu-
rity of models throughout the entire process.
7 Conclusion
In this work, we propose TaylorMLP to preserve
the ownership of released LLMs and prevent their
abuse. Specifically, TaylorMLP preserves the own-
ership of LLM by transforming the weights ofLLMs into parameters of Taylor-series. Instead
of releasing the original weights, developers can
release the Taylor-series parameters with users,
thereby ensuring the security of LLMs. Defensive
experiments confirm that TaylorMLP effectively
prevents users from reconstructing the weight val-
ues based on downstream datasets. Moreover, Tay-
lorMLP prevents abuse of LLMs by inducing low-
speed token generation for the protected LLMs.
This intentional delay prevents the potential large-
scale unauthorized abuse. Empirical and case stud-
ies show that TaylorMLP significantly increases
the latency of token generation, while maintaining
the chat capabilities and performance on down-
stream tasks. Both qualitative and quantitative re-
sults demonstrate the effectiveness of TaylorMLP
in ensuring the security of the released LLMs. This
indicates its potential in real-world applications.
8 Limitations and Potential Risks
In this work, we propose a framework to protect
the ownership of LLMs released. TaylorMLP can
facilitate large-scale applications of LLMs under
regulatory or contractual constraints. Upon autho-
rizing weights to users, developers can deliver reg-
ulations or contracts to ensure LLM applications
comply with specified constraints. After autho-
rization, watermark technologies are required as a
complementary to detect whether users have vio-
lated regulations by misusing or sharing the models
with others. TaylorMLP and watermark comple-
ment each other to ensure the security of models
throughout the entire process.
Acknowledgments
This research was supported by NSF Awards IIS-
2224843, and the US Department of Transportation
(USDOT) Tier-1 University Transportation Center
(UTC) Transportation Cybersecurity Center for Ad-
vanced Research and Education (CYBER-CARE)
grant #69A3552348332. Additionally, this research
was also supported, in part, by NSF Awards OAC-
2112606 and OAC-2117439. This work made use
of the High Performance Computing Resource in
the Core Facility for Advanced Research Comput-
ing at Case Western Reserve University (CWRU).
We give our special thanks to the CWRU HPC team
for their timely and professional help and mainte-
nance. The views and conclusions in this paper are
those of the authors and do not represent the views
of any funding or supporting agencies.References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
2357–2367, Minneapolis, Minnesota. Association for
Computational Linguistics.
Andrey Bogdanov, Dmitry Khovratovich, and Christian
Rechberger. 2011. Biclique cryptanalysis of the full
aes. In Advances in Cryptology–ASIACRYPT 2011:
17th International Conference on the Theory and
Application of Cryptology and Information Security,
Seoul, South Korea, December 4-8, 2011. Proceed-
ings 17 , pages 344–371. Springer.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Loredana Caruccio, Stefano Cirillo, Giuseppe Polese,
Giandomenico Solimando, Shanmugam Sundara-
murthy, and Genoveffa Tortora. 2024. Claude 2.0
large language model: tackling a real-world classifi-
cation problem with a new iterative prompt engineer-
ing approach. Intelligent Systems with Applications ,
page 200336.
Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang,
Fengning Luo, Nan Duan, and Bo Wang. 2024. scgpt:
toward building a foundation model for single-cell
multi-omics using generative ai. Nature Methods ,
pages 1–11.
L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPofi,
C Foster, L Golding, J Hsu, A Le Noac’h, et al. A
framework for few-shot language model evaluation,
12 2023. URL https://zenodo. org/records/10256836 ,
7.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all
you need. arXiv preprint arXiv:2306.11644 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. Proceedings of the International Con-
ference on Learning Representations (ICLR) .Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.
Textbooks are all you need ii: phi-1.5 technical re-
port. arXiv preprint arXiv:2309.05463 .
Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.
Truthfulqa: Measuring how models mimic human
falsehoods. Preprint , arXiv:2109.07958.
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,
Zhaozhuo Xu, Vladimir Braverman, Beidi Chen,
and Xia Hu. 2024. Kivi: A tuning-free asymmet-
ric 2bit quantization for kv cache. arXiv preprint
arXiv:2402.02750 .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. In International Conference on
Machine Learning , pages 22631–22648. PMLR.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.Preprint , arXiv:1609.07843.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In EMNLP .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
transformer. Journal of machine learning research ,
21(140):1–67.
Vincent Rijmen. 1997. Cryptanalysis and design of
iterated block ciphers . Ph.D. thesis, Doctoral Disser-
tation, October 1997, KU Leuven.
Ronald L Rivest, Adi Shamir, and Leonard Adleman.
1978. A method for obtaining digital signatures and
public-key cryptosystems. Communications of the
ACM , 21(2):120–126.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In 2011 AAAI Spring Symposium Series .
Or Sharir, Barak Peleg, and Yoav Shoham. 2020. The
cost of training nlp models: A concise overview.
arXiv preprint arXiv:2004.08900 .Ruixiang Tang, Yu-Neng Chuang, Xuanting Cai, Meta
Platforms, Mengnan Du, and Xia Hu. 2023. Se-
cure your model: An effective key prompt protection
mechanism for large language models.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
Juliette Love, et al. 2024. Gemma: Open models
based on gemini research and technology. arXiv
preprint arXiv:2403.08295 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Guanchu Wang, Junhao Ran, Ruixiang Tang, Chia-
Yuan Chang, Yu-Neng Chuang, Zirui Liu, Vladimir
Braverman, Zhandong Liu, and Xia Hu. 2024. As-
sessing and enhancing large language models in
rare disease question-answering. arXiv preprint
arXiv:2408.08422 .
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
et al. 2021. Ethical and social risks of harm from
language models. arXiv preprint arXiv:2112.04359 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint
arXiv:1910.03771 .
Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang,
and Weidi Xie. 2023. Pmc-llama: Further fine-
tuning llama on medical papers. arXiv preprint
arXiv:2304.14454 .
Tao Xiang, Chunlong Xie, Shangwei Guo, Jiwei Li, and
Tianwei Zhang. 2021. Protecting your nlg models
with semantic and robust watermarks. arXiv preprint
arXiv:2112.05428 .Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-
tuning: Transfer learning without full model. arXiv
preprint arXiv:2302.04870 .
Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei
Koh, Chaowei Xiao, and Muhao Chen. 2024. Instruc-
tional fingerprinting of large language models. arXiv
preprint arXiv:2401.12255 .
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-
tian Han, Qizhang Feng, Haoming Jiang, Shaochen
Zhong, Bing Yin, and Xia Hu. 2023. Harnessing the
power of llms in practice: A survey on chatgpt and
beyond. ACM Transactions on Knowledge Discovery
from Data .
Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu.
2023. Llm for patient-trial matching: Privacy-aware
data augmentation towards better performance and
generalizability. In American Medical Informatics
Association (AMIA) Annual Symposium .
Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai,
Fan Yang, and Xia Hu. 2023. Data-centric ai: Per-
spectives and challenges. In Proceedings of the
2023 SIAM International Conference on Data Mining
(SDM) , pages 945–948. SIAM.Appendix
A High-order Derivative of GELU (•)
The gelu (•)function is given by
gelu(x) =xΦ(x),
where Φ(x)denotes the standard Gaussian cumu-
lative distribution function.
According to the Leibniz rule, we have
gelu(n)(x) =nX
k=0k
n
x(k)Φ(n−k)(x)
=xΦ(n)(x) +nΦ(n−1)(x).(7)
We let
hn(x) =1√
2π
e−x2
2(n)
=h
(−x)1√
2π
e−x2
2(n−1)
−(n−1)1√
2π
e−x2
2(n−2)
= (−x)hn−1−(n−1)hn−2. (8)
Then-order derivative of Φ(n)(x)is given by
Φ(n)(x) =1√
2π
e−x2
2(n−1)
=hn−1(x)
Φ(n−1)(x) =1√
2π
e−x2
2(n−2)
=hn−2(x).
Then-order derivative of gelu (•)is given by
gelu(n)(x) =xhn−1(x) +nhn−2(x),(9)
where hn(x)can be recursively computed by
Equation (8); h0(x) =1√
2πe−x2
2; and h1(x) =
−xh0(x). We also visualize y=gelu(n)(x)with
n= 0,1,2,3,4,5in Figure 6 (a)-(f), respectively.
B High-order Derivative of SiLU (•)
The silu (•)function is given by
silu(x) =xσ(x),
where σ(x)denotes the sigmoid function.
According to the Leibniz rule, we have the n-order
derivative of silu (•)given by
silu(n)(x) =nX
k=0k
n
x(k)σ(n−k)(x)
=xσ(n)(x) +nσ(n−1)(x). (10)We let
hn(x) =σ(n)(x)
= [σ(x)(1−σ(x))](n−1)
=n−1X
k=0k
n−1
σ(k)(x)(1−σ(x))(n−k)
=n−1X
k=0k
n−1
σ(k)(x)(−σ(x))(n−k−1)
=−n−1X
k=0k
n−1
hk(x)hn−k−1(x). (11)
Then-order derivative of silu (•)is given by
silu(n)(x) =xhn(x) +nhn−1(x), (12)
where hn(x)can be recursively computed by Equa-
tion (11); h0(x) = σ(x),h1(x) = σ(x)(1−
σ(x)). We also visualize y=silu(n)(x)with
n= 0,1,2,3,4,5in Figure 7 (a)-(f), respectively.
C Details about Datasets
All open-sourced datasets have the Apache-2.0 li-
cence, which allows for academic research.
TruthfulQA: TruthfulQA is a benchmark de-
signed to assess the truthfulness of a language
model’s answers. The dataset consists of 4,114
questions across 38 categories, including health,
law, finance, and politics. These questions are
crafted to reflect common false beliefs or miscon-
ceptions that humans might hold (Lin et al., 2021).
MathQA: MathQA is a comprehensive dataset
of math word problems accompanied by an inter-
pretable neural solver that translates problems into
operational programs. It contains 14,925 ques-
tions (Amini et al., 2019).
MMLU: The MMLU benchmark measures the
knowledge acquired during pretraining by evalu-
ating models in zero-shot and few-shot settings.
It includes 56,168 questions covering 57 subjects
across STEM, humanities, social sciences, and
more, with difficulty levels ranging from elemen-
tary to advanced professional. The benchmark tests
both general knowledge and problem-solving abili-
ties (Hendrycks et al., 2021).
OpenbookQA: OpenbookQA addresses the task
of open-domain question answering using datasets
like Wikipedia. This dataset contains 2,000 ques-
tions (Mihaylov et al., 2018).C4: C4 is a large, cleaned version of the Com-
mon Crawl web corpus. For LLM distillation
in Section 5.4, we use the ’c4-train.00000-of-
01024.json.gz’ split, and down-sample a subset
of 35.6k instances for the distillation.
Wikitext-2: The WikiText-2 dataset is a collec-
tion of over 100 million tokens from verified Good
and Featured articles on Wikipedia. It includes
4.4k sentences used to evaluate the perplexity of
language models.
D Implementation Details
When selecting the protected columns within the
down_projection weights, we aim to minimize
the difference between the embedding zand the
local embedding z0, represented by z−z∗
0. Specifi-
cally, according to Equation (5), the approximate
upper bound for each dimension of z−z∗
0is given
by|zmax−zmin|, where | • | takes the element-
wise absolute values. Then, the indexes of the pro-
tected columns take the least Kdimensions within
|zmax−zmin|, where Ktakes 1×104,8×103,
and 2560 for the Llama-3-8B, Mistral-7B, and Phi-
2 LLMs, respectively. The number of protected
columns takes the maximal value that TaylorMLP
can retain the original performance of the LLMs.
Other Experimental configurations are in Table 4.
The unprotected columns preserve their pre-trained
values in the fine-tuning (Sections 5.3) and distilla-
tion experiments ( 5.4).
E Computational Infrastructure
The computational infrastructure information is
given in Table 4.
Table 4: Experiment configuration and computing in-
frastructure.
Name Value
Data type torch.bfloat16
Flash-Attention True
Eval batch-size 1
Computing Infrastructure GPU
GPU Model NVIDIA-A100
GPU Memory 80GB
GPU Number 8
CUDA Version 12.1
CPU Memory 512GBF Hyper-parameters for Fine-tuning and
Distillation
Table 5 shows the hyper-parameter settings of fine-
tuning and distillation experiments in Sections 5.3
and 5.4.
Table 5: Hyper-parameter settings of fine-tuning and
distillation experiments in Sections 5.3 and 5.4.
Name Value
Optimizer AdamW
Learning rate 10−5
LR scheduler Linear
Warmup steps 0
Mini-batch size 8×8
Fine-tuning Epoch 10
Distillation Epoch 1
G Packages
In this work, we use the transformers along with
datasets packages (Wolf et al., 2019) for model
and dataset loading, and lm-eval-harness (Gao
et al.) package for evaluation. All open-sourced
packages have the Apache-2.0 licence, which al-
lows for academic research.8
6
4
2
02468012345678(a)y=gelu(x)
8
6
4
2
024680.00.20.40.60.81.0 (b)y=gelu(1)(x)
8
6
4
2
024680.00.20.40.60.8 (c)y=gelu(2)(x)
8
6
4
2
024680.8
0.6
0.4
0.2
0.00.20.40.60.8
(d)y=gelu(3)(x)
8
6
4
2
024681.5
1.0
0.5
0.00.51.0 (e)y=gelu(4)(x)
8
6
4
2
024683
2
1
0123 (f)y=gelu(5)(x)
Figure 6: Visualization of y=gelu(n)(x), where n= 0,1,2,3,4,5.
4
3
2
1
0123401234
(a)y=silu(x)
4
3
2
1
012340.00.20.40.60.81.0 (b)y=silu(1)(x)
4
3
2
1
012340.00.10.20.30.40.5 (c)y=silu(2)(x)
4
3
2
1
012340.3
0.2
0.1
0.00.10.20.3
(d)y=silu(3)(x)
4
3
2
1
012340.5
0.4
0.3
0.2
0.1
0.00.10.2 (e)y=silu(4)(x)
4
3
2
1
012340.6
0.4
0.2
0.00.20.40.6 (f)y=silu(5)(x)
Figure 7: Plot of y=silu(5)(x), where n= 0,1,2,3,4,5.