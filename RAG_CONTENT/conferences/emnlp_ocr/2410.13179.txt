EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised
Speech Representation Learning
Ashish Seth1‚àó‚Ä†, Ramaneswaran Selvakumar1‚àó, S Sakshi1, Sonal Kumar1,
Sreyan Ghosh1‚Ä†, Dinesh Manocha1
1University of Maryland, College Park
{aseth125, ramans, ssakshi, sonalkum, sreyang, dmanocha}@umd.edu
Abstract
In this paper, we present EH-MAM (Easy-
to-Hard adaptive Masked Acoustic Modeling),
a novel self-supervised learning approach for
speech representation learning. In contrast
to the prior methods that use random mask-
ing schemes for Masked Acoustic Modeling
(MAM), we introduce a novel selective and
adaptive masking strategy. Specifically, dur-
ing SSL training, we progressively introduce
harder regions to the model for reconstruc-
tion. Our approach automatically selects hard
regions and is built on the observation that
the reconstruction loss of individual frames in
MAM can provide natural signals to judge the
difficulty of solving the MAM pre-text task
for that frame. To identify these hard regions,
we employ a teacher model that first predicts
the frame-wise losses and then decides which
frames to mask. By learning to create chal-
lenging problems, such as identifying harder
frames and solving them simultaneously, the
model is able to learn more effective represen-
tations and thereby acquire a more comprehen-
sive understanding of the speech. Quantita-
tively, EH-MAM outperforms several state-of-
the-art baselines across various low-resource
speech recognition and SUPERB benchmarks
by 5%-10%. Additionally, we conduct a thor-
ough analysis to show that the regions masked
byEH-MAM effectively capture useful context
across speech frames1.
1 Introduction
Self-supervised learning (SSL) has emerged as one
of the most effective paradigms of speech represen-
tation learning when labeled data is scarce (Baevski
and Mohamed, 2020; Mohamed et al., 2022). The
task is to learn general-purpose speech represen-
tations from unlabeled data that can then be trans-
ferred to Spoken Language Processing (SLP) tasks
* Equal contribution, ‚Ä†Equal Ideation
1Code: https://github.com/cs20s030/ehmam.git
Random
Masking
Masked Frame Reconstruction
Model
Pre-defined masking strategy
Model
(Teacher)
Masked Frame Reconstruction
Model
(Student)
Selective Masking with Teacher
(b) EH-MAM Pre-training(a) Traditional MAM Pre-trainingFigure 1: EH-MAM compared to random masking schemes
employed widely in the literature. EH-MAM first identifies
which frames to mask using a Teacher model and then solves
the MAM task by reconstructing the selected masked regions
using a Student model.
like Automatic Speech Recognition (ASR), Speech
Emotion Recognition (SER), etc (Huang et al.,
2001). Progress in SSL for speech has led to sig-
nificant performance improvements in a range of
low-resource SLP tasks including Phoneme Recog-
nition (PR), Keyword Spotting (KS), etc (Mo-
hamed et al., 2022). Masked Acoustic Modeling
(MAM) has been one the most prevalent pretext
tasks for SSL-based speech representation learning
wherein the model tries to reconstruct frames that
are masked at the input, utilizing the context of the
surrounding frames (Baevski et al., 2022, 2023).
Although a considerable amount of research in
MAM has been performed, most has focused on im-
proving model architectures (Baevski et al., 2022;
Chang et al., 2022; Baevski et al., 2023) and pre-
text tasks (Hsu et al., 2021; Lodagala et al., 2023;
Liu et al., 2024), with very limited progress in im-
proving the masking algorithm (Yue et al., 2022;
Baevski et al., 2023). Most MAM algorithms still
perform random masking of input frames. On the
other hand, selective masking strategies for other
domains, like computer vision (CV) (Bao et al.,
2021; He et al., 2022; Kakogeorgiou et al., 2022)
1arXiv:2410.13179v1  [cs.SD]  17 Oct 202410 20 30 40 500.511.52
Masking Percentage (%) ‚ÜíRelative WER ‚ÜíRandom Masking
Selective Masking
Figure 2: Increase in relative WER using selective and ran-
dom masking schemes. During inference, under similar ex-
perimental settings, we selectively mask the frames with high
reconstruction values and compare it against random masking.
The former consistently shows a significant increase in rela-
tive WER than the later, thereby indicating that these frames
capture more useful context for speech reconstruction as a
result of capturing more information, Thus building on this
result we hypothesize that asking a model to reconstruct these
frames will result in stronger learning signals.
and natural language processing (NLP) (Sadeq
et al., 2022a, 2023; Xiao et al., 2022) that focuses
on masking useful context, have shown significant
improvements over random masking. This can be
attributed to multiple factors, including: (1) Vari-
able Information Content: Variable information
content in data translates to variable learning sig-
nals for the reconstruction task. For instance, in
Masked Language Modeling (MLM) (Devlin et al.,
2019), the reconstruction of high-frequency stop
words such as ‚Äúthe‚Äù or ‚Äúis‚Äù offers minimal discrim-
inative power due to the ubiquity and low semantic
load of these words (Sadeq et al., 2022a, 2023). In
speech, for example, this can be translated to re-
constructing frames corresponding to random noise
or partial phonemes, where much of the frames is
already available as context. (2) Progressive Learn-
ing: Random masking fails to imitate the progres-
sive human learning process (Madan et al., 2024).
Humans do not receive knowledge uniformly; in-
stead, they are exposed to progressively more com-
plex information as they advance in the learning
process. Mimicking this progression in the mask-
ing algorithm by initially exposing the model to
simpler, more predictable speech patterns and grad-
ually introducing more complex, less predictable
ones can significantly enhance the learning trajec-
tory. This approach aligns better with how humans
learn, moving from simpler to more complex in-
formation, and helps the model develop a deeperunderstanding of language over time.
Main Contributions. To overcome the aforemen-
tioned problems, in this paper, we propose EH-
MAM (Easy-To- Hard adaptive Masked Acoustic
Modelling), a novel selective and adaptive mask-
ing scheme for MAM. We build EH-MAM on the
core hypothesis that hard regions, characterized
by collections of speech frames that are more dif-
ficult to reconstruct, serve as stronger signals for
the learning process . Fig. 2 shows the results of
a simple experiment we performed to validate our
hypothesis. By selectively masking hard regions,
we notice a greater and consistent drop in WER
performance for ASR. This suggests that masking
hard regions captures useful context in the speech
input. Our main contributions are as follows:
‚Ä¢We propose EH-MAM , a novel self-
supervised speech representation learning al-
gorithm. In contrast to solving a predefined
MAM pre-text task, such as reconstructing
randomly masked frames, EH-MAM aims
to generate and align itself towards a more
formidable MAM pre-text task. For generat-
ing a challenging MAM pre-text task, we first
identify a collection of difficult frames to re-
construct, also called hard regions , followed
by selectively masking them. We propose
a lightweight loss predictor (introduced in
Section 3.2.2) that predicts the frame-level re-
construction loss values and determines hard
regions based on the output. To train the
loss predictor jointly with MAM, we design
a novel auxiliary loss (introduced in Sec-
tion 3.2.2) that forces the predictor to learn the
relative correlations between speech frames.
Finally, to align the model towards recon-
structing hard regions, we propose an easy-
to-hard masking strategy (introduced in Sec-
tion 3.2.3) that guides the EH-MAM learning.
‚Ä¢We show the effectiveness of the speech rep-
resentation learned by EH-MAM through ex-
tensive evaluations on low-resource speech
recognition benchmarks (Kahn et al., 2020)
and downstream evaluation on SUPERB (wen
Yang et al., 2021). EH-MAM beats prior arts
with a relative improvement of 5%-10%
‚Ä¢We perform a comprehensive analysis to
demonstrate that regions masked by the
EH-MAM effectively capture useful context
across speech input.
2Encoder
Loss
Predictor
EncoderSorting
Random
Masking Masking
Adaptive Masking
Loss
Predictor......
... ......
...
Teacher
StudentEasy-T o-Hard Masking
ForwardNotationsSelective Masking
...
Decoder... ...
Trainable
Frozenspeech i nput
Reconstructed Frames
Masked speech i nput
...Auxiliary Loss 
üî•‚ùÑ 
üî•
‚ùÑ Reconstruction Loss 
EMAFigure 3: Illustration of EH-MAM SSL algorithm. EH-MAM employs the self-distillation SSL framework that consists of
identical student and teacher networks. At each training iteration, the teacher is updated by the exponential moving average
(EMA) of the student. 1‚ÉùFor a speech input Z, we first use the teacher network to identify the speech frames that are hard to
reconstruct, also called as hard regions. To achieve this, we predict the frame-level reconstruction loss values Lt
pusing a loss
predictor dŒ¥tby feeding Zto the teacher network. 2‚ÉùNext, we utilize our easy-to-hard masking strategy to identify the mask
indices MSassociated with hard regions, followed by progressively introducing them with random mask indices MRover each
epoch. 3‚ÉùFinally, a masked variant ÀúZis fed to the student network, where it is tasked to 4‚Éùreconstruct masked regions by
optimizing a reconstruction loss (as shown in Eqtn. 3) and 5‚Éùtrain a loss predictor dŒ¥sby computing an auxiliary loss between
predicted and original reconstruction loss values, Ls
pandLrecrespectively (as shown in Eqtn. 6).
2 Related Work
Self-Supervised Learning. SSL has emerged
as a prevalent speech representation learning
paradigm, demonstrating impressive downstream
performance under low-resource settings (Lee et al.,
2022; Mohamed et al., 2022). At its core, SSL
relies on the quality of pretext tasks for capturing
varied learning signals from unlabeled data sources.
Based on the nature of the pretext tasks, the SSL
frameworks are further categorized into the fol-
lowing sub-categories: 1) Contrastive Approaches:
The pretext task is designed to maximize latent
space similarity between the anchor and positive
samples while minimizing the similarity between
the anchor and negative samples. 2) Generative
Approaches: These methods primarily focus on
first building a target by randomly masking mul-
tiple speech frames and then reconstructing them
by optimizing a similarity measure (MSE or Cross
Entropy) between the predicted frames and the tar-
gets. The pretext task includes predicting future
input from past inputs (Oord et al., 2018; Yang
et al., 2022), masked from unmasked (Baevskiet al., 2022, 2023) or the original from some other
corrupted view (Lodagala et al., 2023). Masked
Acoustic Modeling has undoubtedly seen the most
success for speech representation learning.
Masked Acoustic Modeling (MAM) Conven-
tional MAM architectures first perform frame-level
masking, where randomly selected speech frames
are masked using various existing masking strate-
gies, including block or random masking (Bao
et al., 2021; He et al., 2022). Next, they either
employ a single encoder network like BERT (De-
vlin et al., 2019) to predict masked regions in a
speech input (Liu et al., 2020; Chang et al., 2022;
Chen et al., 2022; Hsu et al., 2021) or utilize self-
distillation methods, where the student learns to
reconstruct masked information under the guid-
ance of an identical teacher network (Baevski et al.,
2022, 2023; Liu et al., 2024). Although a consid-
erable amount of research in MAM has undergone
towards improving model architecture (Baevski
et al., 2022, 2023) and introducing novel pretext
tasks (Liu et al., 2024), developing better masking
strategies is still under-explored.
33 Methodology
In this Section, we explain the EH-MAM method-
ology. We first provide an overview of the EH-
MAM learning paradigm (in Section 3.1), followed
by details on the reconstruction and auxiliary loss
formulations (in Sections 3.2.1, 3.2.2). Finally, we
introduce the easy-to-hard masking algorithm (in
Section 3.2.3).
3.1 Overview of E H-MAM
We illustrate EH-MAM in Fig. 3. At its core, EH-
MAM incorporates the self-distillation based SSL
training paradigm for solving MAM pretext task,
similar to Baevski et al. (2022, 2023). Specifically,
EH-MAM consists of two identical networks, a
teacher {fŒ∏t, dŒ¥t}and a student {fŒ∏s, dŒ¥s}. A sep-
arate decoder dR
œïis employed for reconstructing
masked frames from the student representations.
The context encoders fŒ∏are built using K-layered
transformers (Vaswani et al., 2017), whereas the de-
coder dR
œïand the loss predictor dŒ¥are constructed
with light-weight D-layered 1D-convolution lay-
ers (Kiranyaz et al., 2019). During pre-training,
the teacher parameters Œ∏t, Œ¥tare updated by per-
forming exponential moving average (EMA) of the
student parameters Œ∏s, Œ¥s(Tarvainen and Valpola,
2017). Formally, we define the update as follows:
œât=Œªœât+ (1‚àíŒª)œâs(1)
where œât={Œ∏t, Œ¥t},œâs={Œ∏s, Œ¥s}, and Œªis the
decay rate. The student and decoder parameters are
updated using gradient descent.
At each training iteration, we first extract low-
frequency feature representations Z‚ààRN√ódfrom
raw speech signals x‚ààX(Baevski et al., 2020)
and feed it to the teacher network to get frame-
level predicted reconstruction loss values Lt
p=
dŒ¥t(fŒ∏t(Z)). With the help of Lt
p, we generate
binary mask indexes MA={0,1}Nusing the
easy-to-hard masking strategy (introduced in Sec-
tion 3.2.3), followed by creating a masked version
of the original speech input ÀúZ‚ÜêZ¬∑MA. Finally,
the student is trained with gradient descent to mini-
mize a weighted combination of reconstruction loss
Lrec(introduced in Section 3.2.1) and an auxiliary
lossLaux(introduced in Section 3.2.2). Formally,
we define the objective function below:
Ljoint=Lrec+Œ±Laux(2)
where Œ±is a balancing parameter and is set to 0.05
throughout the experiments (further ablation on
this can be found in Appendix C.2).Algorithm 1 Easy-To-Hard Masking
Require: train set Z, masking probability P, se-
lective masking MS, random masking MR,
adaptive masking MA, number of frames F(¬∑)
Require: t‚àà {1,2, ..., T}: training iteration
fort‚â§Tdo
Sample batch of training data z‚àº Z
Compute selective and random masking
probability: PS‚Üê P √ót
T,PR‚Üê1‚àí PS
respectively.
Predict the reconstruction value Lt
pfor each
frame: Lt
p‚ÜêdT
Œ¥(fŒ∏t(z))
Update MSby selecting frame indices corre-
sponding to the top kreconstruction values
where k=‚åäPSF(z)‚åã.
Update MRby randomly selecting
‚åäPRF(z)‚åãframe indices
Update MAby taking the union of MSand
MR:MA‚ÜêMS‚à™MR
Create a masked counterpart Àúz‚ÜêMA¬∑z
end for
3.2 Selective Masking with E H-MAM
Motivation: EH-MAM distinguishes itself from
the conventional self-distillation-based SSL train-
ing methods that are fixated on solving a prede-
fined MAM task generated using random mask-
ing (Baevski et al., 2022; Chen et al., 2022; Chang
et al., 2022), by enforcing the teacher to generate
more challenging pretext tasks. To achieve this,
EH-MAM first uses the teacher to identify hard
regions, a collection of speech frames that are diffi-
cult to reconstruct, and then selectively mask these
hard regions to create challenging MAM pretext
tasks for the student to solve. Being constantly chal-
lenged by the teacher further directs the student to
develop a much more nuanced understanding of
speech. Additionally, we take inspiration from the
recent studies in NLP and CV that have highlighted
the significance of generating formidable pretext
tasks for MLM (Masked Language Modeling) and
MIM (Masked Image Modeling) using selective
masking (Bao et al., 2021; Sadeq et al., 2022b).
To reweigh the model attention towards recon-
structing such hard regions, we introduce the loss
predictors dŒ¥s,dŒ¥tfor the student and teacher net-
works, respectively. Further to train the loss predic-
tor, we also propose an auxiliary objective function
Laux, that the model optimizes alongside the re-
construction loss Lrec.
43.2.1 Reconstruction Loss
As shown in Fig. 3, we first reconstruct the masked
frames by feeding student representations fŒ∏s(ÀúZ)
to a decoder dR
œï. Similar to Baevski et al. (2022,
2023), the goal of dR
œïis to reconstruct the teacher
representation for time steps that are masked in
the student input. To achieve this, we compute a
reconstruction loss Lrecbetween the student and
the teacher representations. Formally, we define
reconstruction loss Lrecas follows:
Lrec=‚à•MA¬∑fŒ∏t(Z)‚àídR
œï(¬∑fŒ∏s(ÀúZ))‚à•2
2(3)
where MA¬∑fŒ∏t(Z)represents teacher representa-
tions associated with the masked speech input.
3.2.2 Loss Predictor and Auxiliary Loss
Motivation: Given the sequence of frame-level
reconstruction loss values Lrec‚ààRN, our goal is
to create a challenging MAM pretext task for the
student by selectively masking frames with high
reconstruction values. As original reconstruction
loss values Lrecare computed only for the masked
regions (see Section 3.2.1), it provides limited in-
formation for deciding which frames to mask. To
mitigate this problem, we introduce lightweight
loss predictors dŒ¥s, dŒ¥t, which can be easily inte-
grated with the student-teacher network, and add
reconstruction loss predicting capabilities across
both networks. To train these loss predictors, we
propose a novel auxiliary loss Lauxthat guides it
towards capturing relative correlations between in-
dividual frames rather than forcing the predictor to
generate exact frame-level reconstruction values.
Specifically, for each masked frame (i, j)where
iÃ∏=jand(i, j)‚àà {1,2, ..., N}, ifLrec
i>Lrec
j
than the predicted counterpart Ls
p=dŒ¥s(fŒ∏s(ÀúZ))
must also have Ls
pi>Ls
pj. To formulate this con-
straint as a differentiable objective function, we
first define a target distribution as an indicator vari-
ableIthat captures the relative correlations be-
tween original reconstruction loss values, such as
Lrec
i>Lrec
j. Formally we define this as follows:
Ii,j=(
1,Lrec
i>Lrec
jand{i, j} ‚ààMA
0,otherwise(4)
Next, similar to I, we introduce a predicted distri-
bution Sfor representing the relative differences
in the predicted reconstruction values Ls
p.Sis
formally defined with a sigmoid function as:
Si,j=e(Ls
pi‚àíLs
pj)/(1 +e(Ls
pi‚àíLs
pj)) (5)102030405060708090100
Frame-level reconstruction values ‚Üíepochs ‚Üí
46810
Figure 4: For a random speech utterance, we show the varia-
tion in frame-level reconstruction loss values across training
epochs. During the initial stages of EH-MAM pre-training,
we find that the model exhibits high frame-level reconstruc-
tion loss values, which results in low distinctiveness amongst
individual values. This leads to increased stochasticity in the
selective masking.
where Si,j>0.5ifLs
pi>Ls
pj. Finally, we formu-
late our auxiliary objective function Lauxby first
computing a vanilla cross entropy H(¬∑)between
the target distribution Iand the predicted distribu-
tionS:Laux‚Üê H (I, S)and then minimizing it
jointly with the reconstruction loss. We define the
formulation of Lauxbelow:
Laux=‚àíNX
i=1NX
j=1Ii,jlogSi,j+ÀúIi,jlog(1‚àíSi,j)
(6)
where ÀúIi,j= 1‚àíIi,j.{i, j} ‚ààMAmeans that the
iandjframes are masked during pre-training.
3.2.3 Selecting Hard Regions for
Reconstruction
Motivation: Fig. 4 shows that during the initial
stage of a EH-MAM pre-training, the reconstruc-
tion loss values are significantly high and exhibit
low discriminative power ( Lrec
i‚âàLrec
j). This
leads to increased stochasticity in the overall selec-
tive masking process. Thus, inspired by the general
human learning approach, where humans do not
perceive knowledge uniformly but are subjected to
a learning environment where they progressively
comprehend more complex information, we pro-
pose an easy-to-hard masking strategy that guides
the model to progressively mask harder regions for
reconstruction. Specifically, we linearly increase
the proportion of mask indices associated with hard
regions at each training epoch. We define hard
regions as a collection of speech frames that the
5model finds difficult to reconstruct.
We illustrate the masking strategy in Fig. 3. At
each training iteration tand with a masking per-
centage P, we first compute PSandPR, the in-
dividual masking percentages for selective and
random masking respectively. Precisely, we up-
datePSandPRlinearly as PS=P√ót
Tand
PR= 1‚àíPS, where Tis the total number of train-
ing iterations. In selective masking, for each sam-
pled batch z‚ààZ, we build mask MSby selecting
frame indices associated with the top kpredicted
reconstruction values Lt
p. We use k=‚åäPSF(z)‚åã,
where F(z)denotes the number of speech frames
for an input batch z. To build a random mask MR,
we randomly sample ‚åäPRF(z)‚åãframe indices. Fi-
nally, we compute the adaptive mask MAby taking
a union of MS,MR. We summarize the complete
process of easy-to-hard masking in Algorithm 1
4 Experimental Setup
Pre-training Following Baevski et al. (2022,
2023); Liu et al. (2024), we pre-trained our model
with 960 hours of unlabelled speech from Lib-
riSpeech corpus (Panayotov et al., 2015). Due to
resource constraints, we use a base variant of the
context encoder (Baevski et al., 2020), with the
number of transformer layers K= 12 and masking
percentage = 50%. For the loss predictor and the
reconstruction decoder, we utilize 1D-convolution
layers, with the number of convolution layers D
= 4. Moreover, a balancing parameter Œ±is intro-
duced and set to 0.05 during the joint optimization
of reconstruction and auxiliary loss. All the pre-
training experiments are performed on 4 √óA100
40GB GPUs, for 400k updates and using a batch
size of 63 minutes of speech (Additional details
on the hyper-parameters used in EH-MAM can be
found in Table 5).
Fine-tuning Similar to Liu et al. (2024), to show
the effectiveness of the learned speech represen-
tation, we fine-tune only the student counterpart
with an additional CTC layer (Graves et al., 2006).
We conduct a comprehensive evaluation under a
low-resource labeled data setting using a 10 mins
/ 1 hour / 10 hours split from LibriLight bench-
mark (Kahn et al., 2020) and 100 hours split from
Librispeech (Panayotov et al., 2015). For all the
splits, we follow a similar fine-tuning setup as
wav2vec2 (Baevski et al., 2020) (We provide addi-
tional fine-tuning details for all the splits in the Ap-
pendix B.1). We also perform a SUPERB (SpeechModelContent Semantic
PR ASR KS IC SF
PER‚ÜìWER‚ÜìAcc‚ÜëAcc‚ÜëF1‚ÜëCER‚Üì
wav2vec 2.0 5.47 6.43 96.23 92.35 88.30 24.77
HuBERT 5.41 6.42 96.30 98.34 88.53 25.20
WavLM 4.84 6.31 96.79 98.63 89.38 22.86
data2vec 4.69 4.94 96.56 97.63 88.59 25.27
DinoSR 3.21 4.71 96.89 98.02 88.83 23.57
data2vec 2.0 3.93 4.91 96.89 98.01 88.24 22.09
EH-MAM 3.86 4.89 97.01 98.01 89.47 22.04
Table 1: Results on Speech Processing Universal PERfor-
mance Benchmark (SUPERB). The downstream tasks in-
clude phoneme recognition (PR), automatic speech recogni-
tion (ASR), keyword spotting (KS), intent classification (IC),
and slot filling (SF). The evaluation metrics used are accuracy
(Acc), phoneme error rate (PER), word error rate (WER), f1
score (F1), and concept error rate (CER). The best and the
second best results are bolded and underlined respectively.
Processing Universal PERformance Benchmark)
evaluation (wen Yang et al., 2021), where a sepa-
rate prediction head is trained on top of a frozen
pre-trained model for various downstream tasks
(Additional details on the downstream tasks present
in SUPERB can be found in Appendix B.2)
Baselines We compare the performance of EH-
MAM across various SSL-based speech represen-
tation learning baselines that employ 1) single en-
coder: wav2vec 2.0 (Baevski et al., 2020), Hu-
BERT (Hsu et al., 2021) and 2) self-distillation
network: data2vec (Baevski et al., 2022), data2vec
2.0 (Baevski et al., 2023) and DinoSR (Liu et al.,
2024) to reconstruct masked frames (Additional
details on all the baselines can be found in Ap-
pendix A). Due to compute constraints, we avoid
retraining the baselines from scratch and use the
checkpoints open-sourced by the authors.
Dataset and Evaluation Metric We pre-train EH-
MAM on 960 hours of unlabelled speech data from
the LibriSpeech corpus (Panayotov et al., 2015).
Further, we evaluate EH-MAM on a wide range
of speech-related downstream tasks, including 1)
Low resource ASR benchmarks: Libri-Light (Kahn
et al., 2020), 100 hours LibriSpeech corpus (Panay-
otov et al., 2015), Wall-Street Journal (WSJ) (Paul
and Baker, 1992), SwitchBoard (Godfrey et al.,
1992) and 2) SUPERB evaluation: a collection of a
diverse set of downstream tasks including Phoneme
Recognition (PR), Automatic Speech Recognition
(ASR), Keyword Spotting (KS), Intent Classifica-
tion (IC) and Slot Filling (SF). Additional details
on duration, train/test splits, and evaluation metrics
can be found in Appendix B.
6Models Pre-training steps Batch size (minutes)dev (WER ‚Üì) test (WER ‚Üì)
clean other clean other
10 minutes labeled data
wav2vec 2.0 (Baevski et al., 2020) 400k 96 8.9 15.7 9.1 15.6
HuBERT (Hsu et al., 2021) 250k + 400k 47 9.1 15.0 9.7 15.3
data2vec (Baevski et al., 2022) 400k 63 7.3 11.6 7.9 12.3
DinoSR (Liu et al., 2024) 400k 63 6.6 10.8 7.3 11.8
data2vec 2.0 (Baevski et al., 2023) 400k 17 6.4 10.5 7.2 11.5
EH-MAM 400k 63 6.3 10.2 7.1 11.1
1 hr labeled data
wav2vec 2.0 (Baevski et al., 2020) 400k 96 5.0 10.8 5.5 11.3
HuBERT (Hsu et al., 2021) 250k + 400k 47 5.6 10.9 6.1 11.3
WavLM (Chen et al., 2022) 250k + 400k 187 - - 5.7 10.8
data2vec (Baevski et al., 2022) 400k 63 4.0 8.5 4.6 9.1
DinoSR (Liu et al., 2024) 400k 63 4.1 8.1 4.6 8.7
data2vec 2.0 (Baevski et al., 2023) 400k 17 4.0 8.0 4.6 8.7
EH-MAM 400k 63 4.0 7.8 4.6 8.7
10 hr labeled data
wav2vec 2.0 (Baevski et al., 2020) 400k 96 3.8 9.1 4.3 9.5
HuBERT (Hsu et al., 2021) 250k + 400k 47 3.9 - 4.3 9.4
WavLM (Chen et al., 2022) 250k + 400k 187 - - 4.3 9.2
data2vec (Baevski et al., 2022) 400k 63 3.3 7.5 3.9 8.1
DinoSR (Liu et al., 2024) 400k 63 3.1 7.0 3.6 7.6
data2vec 2.0 (Baevski et al., 2023) 400k 17 3.0 7.0 3.4 7.6
EH-MAM 400k 63 3.0 6.8 3.3 7.3
100 hr labeled data
wav2vec 2.0 (Baevski et al., 2020) 400k 96 2.7 7.9 3.4 8.0
HuBERT (Hsu et al., 2021) 250k + 400k 47 2.7 7.8 3.4 8.1
WavLM (Chen et al., 2022) 250k + 400k 187 - - 3.4 7.7
data2vec (Baevski et al., 2022) 400k 63 2.2 6.4 2.8 6.8
DinoSR (Liu et al., 2024) 400k 63 2.3 6.4 2.9 6.7
data2vec 2.0 (Baevski et al., 2023) 400k 17 2.2 6.2 2.8 6.4
EH-MAM 400k 63 2.2 6.1 2.8 6.3
Table 2: Results on LibriLight benchmark and LibriSpeech for ASR. All the models share a similar BASE size encoder and are
first fine-tuned with a 10 min / 1hr / 10hr / 100hr labeled dataset and then evaluated on common dev/test splits. The evaluation
metric used is word error rate (WER). The best and the second best results are bolded and underlined respectively
5 Results and Analysis
In this section, we present the quantitative and qual-
itative results. For quantitative evaluation, we first
fine-tune EH-MAM on LibriLight (Kahn et al.,
2020) and evaluate across all the test splits. Next,
to show the scalability of the speech representa-
tions learned by EH-MAM , we conduct a down-
stream evaluation on SUPERB benchmark (wen
Yang et al., 2021). Additionally, we also perform
a qualitative analysis on the masked regions pre-
dicted by the EH-MAM . All the results reported
for E H-MAM are averaged across five runs.
5.1 Evaluation on Low-Resource ASR
For low-resource ASR evaluation, we follow a sim-
ilar procedure as Baevski et al. (2020) wherein
we fine-tune only the student counterpart of EH-
MAM with an additional CTC layer (Graves et al.,
2006) on top. We perform fine-tuning using low-
resource labeled datasets under four different se-
tups, 10min / 1hour / 10hour from LibriLight (Kahnet al., 2020) and 100hour Librispeech (Panayotov
et al., 2015). For evaluation, we use the standard de-
v/test split of Librispeech and report the word error
rate (WER) by decoding with the official 4-gram
language model. Following the prior work Baevski
et al. (2022, 2023), the decoding hyper-parameter
is searched with Ax (refer to Section C.1). As
shown in Table 2, EH-MAM consistently outper-
forms all the prior SSL methods across all the se-
tups. We also provide additional results on other
low-resource ASR benchmarks such as Wall Street
Journal (WSJ) (Paul and Baker, 1992) and Switch-
Board (SB) (Godfrey et al., 1992) in Appendix E.
5.2 Downstream Evaluation on SUPERB
We extensively evaluate the effectiveness and scal-
ability of the speech representation learned by EH-
MAM using the Speech Processing Universal PER-
formance Benchmark (SUPERB). SUPERB, in
total, consists of ten speech-related downstream
tasks that aim to study four aspects of speech: con-
710 20 30 40 500.511.52
Masking Percentage (%) ‚ÜíRelative WER ‚ÜíRandom Masking
EH-MAM Masking
Figure 5: We compare the increase in relative Word Error
Rate (WER) by selectively masking hard regions predicted by
the loss predictor ( EH-MAM Masking) Vs randomly masking
frames. The increase in relative WER indicates that the EH-
MAM Masking scheme masks useful context in an input.
tent, speaker, semantics, and paralinguistics. To
investigate the model‚Äôs capabilities to understand
speech content and semantics, we report the results
on phoneme recognition (PR), automatic speech
recognition (ASR), keyword spotting (KS), intent
classification (IC), and slot filling (SF) (Additional
details on all the downstream tasks can be found in
Appendix B.2). For downstream evaluation on SU-
PERB, we follow a similar setup as wen Yang et al.
(2021), where we train a prediction head on top of
the frozen pre-trained models instead of complete
fine-tuning. As shown in Table 1, for semantic
tasks like IC and SF, the EH-MAM outperforms
prior art, showing its capabilities to capture better
semantic information from speech input. On con-
text tasks, EH-MAM surpasses prior art in KS and
achieves comparable performance on PR and ASR.
5.3 Qualitative Analysis
EH-MAM mask useful context: To show EH-
MAM does mask useful context, we conduct a
simple experiment wherein during ASR inference,
we selectively mask the frames with high predicted
reconstruction value using the loss predictor and
compare the increase in relative WER with ran-
dom masking. As shown in Fig. 5, under SUPERB
evaluation setting for ASR (refer Section 4), we
find selectively masking frames with EH-MAM
constantly shows a higher relative WER when com-
pared to random masking across various masking
percentages. Higher relative WER indicates that
a selective masking scheme with the EH-MAM
masks useful context in a speech input.20 40 60 80 10036912
Epochs ‚ÜíReconstruction Loss ‚ÜíHard Masking
easy-to-hard Masking
Figure 6: We compare the effectiveness of EH-MAM in op-
timizing a MAM pretext task, such as the reduction in recon-
struction loss, with hard and easy-to-hard masking schemes.
Theeasy-to-hard masking scheme shows better convergence
in reconstruction loss compared to hard masking strategies.
EH-MAM adapts well towards reconstructing
hard regions: To show how well EH-MAM adapts
towards reconstructing hard regions, we conduct
an experiment wherein we compare the EH-MAM
ability to reconstruct hard regions (collection of
frames with high reconstruction values) using 1)
hard masking , masking only hard regions at each
epoch and 2) easy-to-hard masking, where we pro-
gressively introduce hard regions with randomly
masked regions at each epoch. As shown in Fig. 6,
while pre-training EH-MAM ,easy-to-hard mask-
ing scheme shows better convergence in reconstruc-
tion loss when compared with hard masking strate-
gies. This indicates that progressively introduc-
ing hard regions in an easy-to-hard manner, im-
proves EH-MAM adaptability toward reconstruct-
ing masked regions during pre-training.
6 Conclusion
In this paper, we propose EH-MAM , a novel SSL
framework for learning robust speech represen-
tations. In contrast to prior work that relies on
random masking schemes for creating MAM pre-
text tasks, EH-MAM first identifies hard regions
to reconstruct using a teacher network and then
challenges the student to reconstruct them by pro-
gressively introducing hard regions throughout the
learning process. Next, we introduce an easy-to-
hard masking scheme that guides the EH-MAM
to mask harder regions to reconstruct step-by-step.
EH-MAM outperforms all the other models on
popular low-resource ASR benchmarks and down-
stream evaluation on SUPERB.
87 Limitations and Future Work
EH-MAM and our experimental setup have a few
limitations, as mentioned below:
‚Ä¢We do not employ a LARGE size encoder in
EH-MAM , for example, a 24-layer variant
used by Baevski et al. (2023) due to compute
constraints.
‚Ä¢The loss-predictors used in EH-MAM in-
crease the trainable parameter count com-
pared to other baselines such as data2vec
2.0 (Baevski et al., 2023) during pre-training.
However, we acknowledge that this accounts
only for a slight increase in the total parameter
count (roughly 5%).
‚Ä¢Due to recourse constraints, we conduct the
downstream evaluation on SUPERB for con-
text and semantic-related tasks. We plan to
extend the evaluation across speaker and par-
alinguistic tasks in the future.
8 Acknowledgements
This project is supported in part by NSF#1910940.
References
Alexei Baevski, Arun Babu, Wei-Ning Hsu, and
Michael Auli. 2023. Efficient self-supervised learn-
ing with contextualized target representations for vi-
sion, speech and language. In International Con-
ference on Machine Learning , pages 1416‚Äì1429.
PMLR.
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun
Babu, Jiatao Gu, and Michael Auli. 2022. Data2vec:
A general framework for self-supervised learning
in speech, vision and language. In International
Conference on Machine Learning , pages 1298‚Äì1312.
PMLR.
Alexei Baevski and Abdelrahman Mohamed. 2020. Ef-
fectiveness of self-supervised pre-training for asr.
InICASSP 2020-2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 7694‚Äì7698. IEEE.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A framework
for self-supervised learning of speech representations.
Advances in neural information processing systems ,
33:12449‚Äì12460.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
2021. Beit: Bert pre-training of image transformers.
arXiv preprint arXiv:2106.08254 .Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee.
2022. Distilhubert: Speech representation learn-
ing by layer-wise distillation of hidden-unit bert.
InICASSP 2022-2022 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 7087‚Äì7091. IEEE.
Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.
Wavlm: Large-scale self-supervised pre-training for
full stack speech processing. IEEE Journal of Se-
lected Topics in Signal Processing , 16(6):1505‚Äì1518.
Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma,
Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song,
Xian-Ling Mao, Heyan Huang, et al. 2021. Xlm-e:
Cross-lingual language model pre-training via electra.
arXiv preprint arXiv:2106.16138 .
Alice Coucke, Alaa Saade, Adrien Ball, Th√©odore
Bluche, Alexandre Caulier, David Leroy, Cl√©ment
Doumouro, Thibault Gisselbrecht, Francesco Calt-
agirone, Thibaut Lavril, Ma√´l Primet, and Joseph
Dureau. 2018. Snips voice platform: an embedded
spoken language understanding system for private-by-
design voice interfaces. Preprint , arXiv:1805.10190.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171‚Äì4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
speech, and signal processing, ieee international con-
ference on , volume 1, pages 517‚Äì520. IEEE Com-
puter Society.
Alex Graves, Santiago Fern√°ndez, Faustino Gomez, and
J√ºrgen Schmidhuber. 2006. Connectionist temporal
classification: labelling unsegmented sequence data
with recurrent neural networks. In Proceedings of the
23rd international conference on Machine learning ,
pages 369‚Äì376.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Doll√°r, and Ross Girshick. 2022. Masked autoen-
coders are scalable vision learners. In Proceedings
of the IEEE/CVF conference on computer vision and
pattern recognition , pages 16000‚Äì16009.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-
rahman Mohamed. 2021. Hubert: Self-supervised
speech representation learning by masked prediction
of hidden units. IEEE/ACM Transactions on Audio,
Speech, and Language Processing , 29:3451‚Äì3460.
9Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and
Raj Reddy. 2001. Spoken language processing: A
guide to theory, algorithm, and system development .
Prentice hall PTR.
Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny
Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar√©,
Julien Karadayi, Vitaliy Liptchinsky, Ronan Col-
lobert, Christian Fuegen, et al. 2020. Libri-light:
A benchmark for asr with limited or no supervision.
InICASSP 2020-2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 7669‚Äì7673. IEEE.
Ioannis Kakogeorgiou, Spyros Gidaris, Bill Pso-
mas, Yannis Avrithis, Andrei Bursuc, Konstantinos
Karantzalos, and Nikos Komodakis. 2022. What to
hide from your students: Attention-guided masked
image modeling. In European Conference on Com-
puter Vision , pages 300‚Äì318. Springer.
Serkan Kiranyaz, Turker Ince, Osama Abdeljaber, Onur
Avci, and Moncef Gabbouj. 2019. 1-d convolutional
neural networks for signal processing applications.
InICASSP 2019-2019 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 8360‚Äì8364. IEEE.
Hung-yi Lee, Abdelrahman Mohamed, Shinji Watan-
abe, Tara Sainath, Karen Livescu, Shang-Wen Li,
Shu-wen Yang, and Katrin Kirchhoff. 2022. Self-
supervised representation learning for speech pro-
cessing. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies: Tutorial Abstracts , pages 8‚Äì13. Associa-
tion for Computational Linguistics.
Alexander H Liu, Heng-Jui Chang, Michael Auli, Wei-
Ning Hsu, and Jim Glass. 2024. Dinosr: Self-
distillation and online clustering for self-supervised
speech representation learning. Advances in Neural
Information Processing Systems , 36.
Andy T. Liu, Shu-wen Yang, Po-Han Chi, Po-chun
Hsu, and Hung-yi Lee. 2020. Mockingjay: Unsu-
pervised speech representation learning with deep
bidirectional transformer encoders. In ICASSP 2020
- 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages 6419‚Äì
6423.
Vasista Sai Lodagala, Sreyan Ghosh, and Srinivasan
Umesh. 2023. data2vec-aqc: Search for the right
teaching assistant in the teacher-student training
setup. In ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 1‚Äì5. IEEE.
Loren Lugosch, Mirco Ravanelli, Patrick Ignoto,
Vikrant Singh Tomar, and Yoshua Bengio. 2019.
Speech model pre-training for end-to-end spoken lan-
guage understanding. Preprint , arXiv:1904.03670.
Neelu Madan, Nicolae-C ÀòatÀòalin Ristea, Kamal Nasrol-
lahi, Thomas B Moeslund, and Radu Tudor Ionescu.2024. Cl-mae: Curriculum-learned masked autoen-
coders. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision , pages
2492‚Äì2502.
Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt,
Jakob D Havtorn, Joakim Edin, Christian Igel, Ka-
trin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars
Maal√∏e, et al. 2022. Self-supervised speech represen-
tation learning: A review. IEEE Journal of Selected
Topics in Signal Processing , 16(6):1179‚Äì1210.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.
Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748 .
Vassil Panayotov, Guoguo Chen, Daniel Povey, and
Sanjeev Khudanpur. 2015. Librispeech: an asr cor-
pus based on public domain audio books. In 2015
IEEE international conference on acoustics, speech
and signal processing (ICASSP) , pages 5206‚Äì5210.
IEEE.
Douglas B Paul and Janet Baker. 1992. The design for
the wall street journal-based csr corpus. In Speech
and Natural Language: Proceedings of a Workshop
Held at Harriman, New York, February 23-26, 1992 .
Nafis Sadeq, Byungkyu Kang, Prarit Lamba, and Julian
McAuley. 2023. Unsupervised improvement of fac-
tual knowledge in language models. arXiv preprint
arXiv:2304.01597 .
Nafis Sadeq, Canwen Xu, and Julian McAuley. 2022a.
Informask: Unsupervised informative masking
for language model pretraining. arXiv preprint
arXiv:2210.11771 .
Nafis Sadeq, Canwen Xu, and Julian McAuley. 2022b.
Informask: Unsupervised informative masking for
language model pretraining. In Proceedings of the
2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 5866‚Äì5878, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Antti Tarvainen and Harri Valpola. 2017. Mean teachers
are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning re-
sults. In Advances in neural information processing
systems , pages 1195‚Äì1204.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Pete Warden. 2018. Speech commands: A dataset
for limited-vocabulary speech recognition. Preprint ,
arXiv:1804.03209.
Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-
I Jeff Lai, Kushal Lakhotia, Yist Y . Lin, Andy T.
Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin,
Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee,
10Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-
Wen Li, Shinji Watanabe, Abdelrahman Mohamed,
and Hung yi Lee. 2021. Superb: Speech process-
ing universal performance benchmark. Preprint ,
arXiv:2105.01051.
Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.
2022. Retromae: Pre-training retrieval-oriented lan-
guage models via masked auto-encoder. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 538‚Äì548. As-
sociation for Computational Linguistics.
Gene-Ping Yang, Sung-Lin Yeh, Yu-An Chung, James
Glass, and Hao Tang. 2022. Autoregressive predic-
tive coding: A comprehensive study. IEEE Journal
of Selected Topics in Signal Processing , 16(6):1380‚Äì
1390.
Xianghu Yue, Jingru Lin, Fabian Ritter Gutierrez, and
Haizhou Li. 2022. Self-supervised learning with seg-
mental masking for speech representation. IEEE
Journal of Selected Topics in Signal Processing ,
16(6):1367‚Äì1379.
A Baseline Details
wav2vec 2.0.2(Baevski et al., 2020) The wav2vec
2.0 model integrates contrastive learning with
masking. Similar to the CPC model (Oord et al.,
2018), it employs the InfoNCE loss (Baevski et al.,
2020) to maximize the similarity between a contex-
tualized representation (anchors) and a localized
representation (positives) simultaneously minimiz-
ing the similarity with other masked regions (nega-
tives). Instead of directly using the contextualized
representations, wav2vec 2.0 employs a separate
quantization module to generate positives and neg-
atives.
HuBERT.3(Hsu et al., 2021) Like BERT (De-
vlin et al., 2019), HuBERT follows a generative
approach by discretizing the continuous MFCC
features using the K-means algorithm and creating
targets by randomly masking the quantized units.
Unlike BERT, HuBERT employs a two-iteration
training process wherein, in the first iteration, the
model is trained to predict targets generated from
the MFCC features, followed by quantizing the
learned representations obtained from the first it-
eration training using K-means to generate new
targets, which the model utilize in the second itera-
tion training.
2https://github.com/facebookresearch/fairseq/
tree/main/examples/wav2vec
3https://github.com/facebookresearch/fairseq/
tree/main/examples/hubertWavLM.4(Chen et al., 2022) WavLM extends
the HuBERT‚Äôs learning paradigm by introducing a
gated relative position bias (Chi et al., 2021) at each
transformer layer. Further, WavLM proposes an
utterance-mixing strategy wherein training samples
are augmented by mixing utterances from differ-
ent speakers, and the targets are created from the
original sample.
data2vec.5(Baevski et al., 2022) data2vec in-
troduces a self-distillation-based student-teacher
networks for speech representation learning. The
core idea is to predict the latent representations of
the whole speech unlabeled data from the masked
view. data2vec trains a student network by feed-
ing a masked version of input to predict the latent
representation obtained by feeding the whole input
to a teacher network. The teacher‚Äôs parameters are
updated by the exponential moving average (ema)
of the student‚Äôs parameters.
data2vec 2.0.6(Baevski et al., 2023) data2vec
2.0 uses an identical learning objective as data2vec
but with two key changes. Firstly, data2vec 2.0
introduces a lightweight decoder module that re-
constructs the masked frames in student represen-
tation before maximizing the similarity with the
teacher representations. Next, data2vec 2.0 em-
ploys a multi-mask strategy where multiple mask
variants of the same input are fed to the student net-
work, followed by calculating reconstruction loss
for all the variants with a common teacher repre-
sentation obtained from the original speech input.
DinoSR.7(Liu et al., 2024) DinoSR uses similar
architecture as (Baevski et al., 2022) but introduces
a novel gradient-free online clustering method for
learning discrete acoustic units. DinoSR initially
employs a teacher network to extract contextual-
ized embeddings from the input audio. It then
applies an online clustering scheme to these em-
beddings to create a machine-discovered phone
inventory. Finally, it uses the discretized tokens to
guide a student network.
4https://huggingface.co/docs/transformers/en/
model_doc/wavlm
5https://github.com/facebookresearch/fairseq/
tree/main/examples/data2vec
6https://github.com/facebookresearch/fairseq/
tree/main/examples/data2vec
7https://github.com/Alexander-H-Liu/dinosr
11Dataset Language Domain TypeDuration (hour)
(train, dev, test)
LibriSpeech (Panayotov et al., 2015) English General Read 960, 10, 10
Libri-Light (Kahn et al., 2020) English General Read 11.16, 10, 10
SwitchBoard (SWBD) (Godfrey et al., 1992) English Call Cent. Conv. 30, 5, N.A.
Wall Street Journal (WSJ) (Paul and Baker, 1992) English Finance Read 80, 1.1, 0.4
Table 3: Detailed Statistics of datasets used in our low resource ASR evaluation. Type refers to Conversational or
Read speech.
Task Category DatasetDuration(hour)
(train, dev, test)Evaluation Metric
Phoneme Recognition(PR) Content LibriSpeech(Panayotov et al., 2015) 100, 5.4, 5.4 Phoneme Error Rate(PER)
Automatic Speech Recognition(ASR) Content LibriSpeech(Panayotov et al., 2015) 100, 5.4, 5.4 Word Error Rate(WER)
Keyword Spotting(KS) Content Speech Commands v0.18(Warden, 2018) 18, 2, 1 Accuracy(Acc)
Intent Classification(IC) Speaker Fluent Speech Commands9(Lugosch et al., 2019) 23.1, 3.2, 3.9 Accuracy(Acc)
Slot Filling(SF) Speaker Audio SNIPS10(Coucke et al., 2018) 166.0, 9.0, 9.0 Concept Error Rate(CER)
Table 4: Details on downstream tasks and datasets used for the SUPERB evaluation
B Dataset Details
B.1 ASR Evaluation
LibriSpeech.11(Panayotov et al., 2015) The Lib-
riSpeech dataset is a widely-used corpus of English
read speech with approximately 1000 hours of au-
diobooks available in the public domain, which
includes a broad range of speakers, both male and
female, with diverse accents and ages, providing a
rich source for speech and language research. We
pre-train our model on 960 hours LibriSpeech un-
labeled data and fine-tune for ASR evaluation on
100 hours of labeled data.
Libri-Light.12(Kahn et al., 2020) The Libri-light
is a dataset derived from the LibriV ox project, con-
sisting of audiobooks in the public domain, much
like the LibriSpeech dataset, but aims to address the
limitations of traditional ASR datasets by provid-
ing 60 hours of unlabelled speech complemented
with a smaller amount of labeled data. We conduct
evaluation on ASR with labeled data of 10 mins / 1
hour / 10 hours split from LibriLight.
WSJ.13(Paul and Baker, 1992) The WSJ dataset
consists of approximately 80 hours of read speech
derived from articles in the Wall Street Journal, of-
fering high-quality audio and transcriptions ideal
for training and evaluating ASR systems. The WSJ
dataset includes recordings from 84 speakers, pro-
viding diverse voice samples, including accurate
word-level transcriptions for all audio files and
11https://www.openslr.org/12r
12https://github.com/facebookresearch/
libri-light
13https://catalog.ldc.upenn.edu/LDC93S6Ametadata for speaker identities and recording con-
ditions. We use the WSJ dataset for our ASR task
evaluation on 80 hours of unlabeled data for train-
ing and 1.5 hours for of labeled data for testing.
Switchboard.14(Godfrey et al., 1992) The Switch-
board is a telephone speech corpus consisting of
approximately 260 hours of speech, which includes
2,400 two-sided telephone conversations among
543 speakers. The dataset conversations cover 70
different topics, from current events to personal
interests, providing varied and natural discourse,
making it an invaluable resource in the field of
speech recognition, dialogue systems, and conver-
sational analysis. We report the our ASR evaluation
on 30 hours of unlabeled data for training and 5
hours of data for testing.
B.2 SUPERB (Speech processing Universal
PERformance Benchmark)
SUPERB (wen Yang et al., 2021) is a leaderboard
to benchmark the performance of a shared model
across a wide range of speech processing tasks
with minimal architecture changes and labeled data.
The key focus here is to extract the representa-
tion learned from SSL and to learn task-specialized
lightweight prediction heads on top of the frozen
shared models. Below we detail the tasks in SU-
PERB that we use for evaluation.
14https://catalog.ldc.upenn.edu/LDC97S62
7https://www.tensorflow.org/datasets/catalog/
speech_commands
8https://fluent.ai/
9https://github.com/aws-samples/
aws-lex-noisy-spoken-language-understanding
12Base (Librispeech)
GPUs 4
Learning rate 7.5√ó10‚àí4
Adam Œ≤1/Œ≤2 0.9 / 0.98
Weight decay 0.01
Clip norm -
Learning rate schedule cosine
Warmup updates 8,000
Batch size 63 min
œÑ0(EMA start) 0.999
œÑe(EMA end) 0.99999
œÑn(EMA anneal steps) 75,000
B(block width) 5
R(mask ratio) 0.5
A(mask adjust) 0.05
K(layers to average) 8
Target normalization IN ‚ÜíA VG
Updates 400,000
Decoder dim. 384
Decoder conv. groups 16
Decoder kernel 7
Decoder layers ( D) 4
Loss Predictor dim. 384
Loss Predictor conv. groups 16
Loss Predictor kernel 7
Loss Predictor layers ( D) 410 Minutes 1 Hour 10 Hours 100 Hours
GPU 4 4 4 4
Learning rate 5.0√ó10‚àí55.0√ó10‚àí55.0√ó10‚àí53.0√ó10‚àí5
Adam Œ≤1/Œ≤2 0.9/0.98 0.9/0.98 0.9/0.98 0.9/0.98
Learning rate schedule tri_stage tri_stage tri_stage tri_stage
Batch Size 32 32 32 32
Updates 13000 13000 20000 80000
Apply mask true true true true
Mask prob 0.65 0.65 0.65 0.65
Mask channel prob 0.25 0.25 0.50 0.50
Mask channel length 64 64 64 64
Layerdrop 0.1 0.1 0.05 0.1
Activation dropout 0.1 0.1 0.1 0.1
Freeze finetune updates 10000 10000 10000 0
Table 5: (Left) EH-MAM pre-training hyper-parameters. IN is instance normalization; A VG is mean pooling. (Right)
EH-MAM fine-tuning hyper-parameters for LibriLight (Kahn et al., 2020)
Phoneme Recognition (PR) converts spoken lan-
guage into its smallest units of sound, known as
phonemes. This task incorporates alignment mod-
eling to circumvent issues with incorrect forced
alignments. The LibriSpeech (Panayotov et al.,
2015) subsets train-clean-100/dev-clean/test-clean
are utilized for training, validation, and testing in
the SUPERB framework. The primary metric for
evaluation is the phone error rate (PER).
Automatic Speech Recognition (ASR) transcribes
spoken words into text. While PR focuses on the
precision of phoneme modeling, ASR assesses im-
provements in terms of their practical relevance.
The training, validation, and testing phases use
the LibriSpeech (Kahn et al., 2020) subsets train-
clean-100/dev-clean/test-clean. The word error rate
(WER) serves as the evaluation metric.
Keyword Spotting (KS) involves the detection
of specified keywords within speech, categorizing
utterances into a set list of terms. The Speech
Commands dataset v1.0 (Warden, 2018), which
includes ten keyword categories, a silence category,
and an "unknown" category for erroneous detec-
tions, is used in this task. Accuracy (ACC) is the
metric for assessing performance.
Intent Classification (IC) assigns categories to
spoken utterances to ascertain the speaker‚Äôs intent.It employs the Fluent Speech Commands (Lugosch
et al., 2019) dataset, where utterances are labeled
according to three intent categories: action, object,
and location. The evaluation metric here is also
accuracy (ACC).
Slot Filling (SF) entails predicting a series of
semantic slots from speech. The Audio SNIPS
(Coucke et al., 2018) dataset, which features synthe-
sized multi-speaker utterances for the SNIPS NLU
benchmark, is used for this purpose. Evaluation
is based on the slot-type F1 score and slot-value
character error rate (CER).
C Additional Details: Hyper-Parameter
Tuning
C.1 Pre-training and Fine-tuning
Table 5 summarize the hyper-parameter choices
forEH-MAM when pre-training on Librispeech-
960 hours (Panayotov et al., 2015) and fine-tuning
across various LibriLight (Kahn et al., 2020) setups
(10min / 1hour / 10hour). Most hyper-parameters
are taken from the prior art (Baevski et al., 2023,
2022). For decoding, the hyper-parameter is
searched using Ax15
15https://github.com/facebook/Ax
13Œ± 1 0.5 0.1 0.05 0.01
WER‚Üì7.4 7.3 7.3 7.1 7.2
Table 6: Œ±= 0.05 gives the best performance
C.2 Balancing Parameter Œ±
In Table 6 we show the effect of changing the bal-
ancing parameter Œ±, on the final low-resource asr
evaluation. Specifically, we pre-train EH-MAM
with different balancing parameters and then per-
form an end-to-end fine-tuning on 10 mins setup of
LibriLight (Kahn et al., 2020). Finally, we compute
WER for the test-clean split.
C.3 Masking Probability P
In Table 7 we show the effect of changing the mask-
ing probability P, on the final low-resource asr
evaluation. Specifically, we pre-train EH-MAM
with different masking probability and then per-
form an end-to-end fine-tuning on 10 mins setup of
LibriLight (Kahn et al., 2020). Finally, we compute
WER for the test-clean split.
P 0.1 0.2 0.3 0.4 0.5
WER‚Üì9.4 8.1 7.9 7.3 7.1
Table 7: P= 0.5 gives the best performance
Models WSJ (WER ‚Üì) Switchboard (WER ‚Üì)
dev test dev
data2vec 2.0 9.2 9.0 15.2
EH-MAM 8.4 8.2 14.3
Table 8: Performance comparison of EH-MAM on WSJ and
Switchnoard datasets.
D Additional Details: General
Compute details. For all our pre-training and fine-
tuning experiments, we used four NVIDIA A100-
40GB GPUs. The pre-training EH-MAM requires
five days of training and consists of 94.40M pa-
rameters. All the fine-tuning experiments on Lib-
riLight (Kahn et al., 2020) require two days each.
Additionally, individual downstream evaluation on
SUPERB requires one day.
Potential Risk. As the EH-MAM follows a self-
supervised training regime, it may learn spurious
correlations which can affect downstream perfor-
mance on ASR, PR, etc. Moreover, EH-MAMmight get biased towards a particular type of ac-
cent, dialect, or domain, such as telephonic or read
speech, due to a huge amount of unlabeled data,
which may not be diverse.
Software and Packages details. We implement
all our models in PyTorch16and use Fairseq17
toolkit and SUPERB18for all our experiments.
E Additional Results
We present additional results for low-resource ASR
evaluation on WSJ (Paul and Baker, 1992) and
Switchboard (Godfrey et al., 1992). The evalua-
tion settings for both datasets are similar to Lib-
riLight (Kahn et al., 2020). Train/Test splits for
both datasets can be found in Table 3. As shown in
Table 8, EH-MAM outperforms the state-of-the-art
model, data2vec 2.0, across all the dev/test splits.
16https://pytorch.org/
17https://github.com/facebookresearch/fairseq
18https://superbbenchmark.github.io
14Algorithm 2 Pseudo-Code of Easy-to-Hard Masking.
def c o m p u t e _ m a s k _ i n d i c e s _ e m a _ l o s s (
shape : Tuple [ i n t,i n t] ,
padding_mask : O p t i o n a l [ t o r c h . Tensor ] ,
l o s s _ p r e d : O p t i o n a l [ t o r c h . Tensor ] ,
mask_prob : f l o a t ,
mask_l ength : i n t,
c u r r e n t _ e p o c h : i n t,
t o t a l _ e p o c h : i n t,
mask_type : s t r = " s t a t i c " ,
min_masks : i n t = 0 ,
r e q u i r e _ s a m e _ m a s k s : bool = True ,
mask_dropout : f l o a t = 0 . 0 ) :
bsz , a l l _ s z = shape
mask = np . f u l l ( ( bsz , a l l _ s z ) , F a l s e )
# add a random number f o r p r o b a b i l i s t i c r o u n d i n g
all_num_mask = i n t( mask_prob *a l l _ s z / f l o a t ( mask_length ) + np . random . r an d ( ) )
# Get t h e l o s s l a t t i c e from decoder
i d s _ s h u f f l e _ l o s s = t o r c h . a r g s o r t ( l o s s _ p r e d , dim = 1 ) . cpu ( ) . d e t a c h ( ) . numpy ( )
all_num_mask = max ( min_masks , all_num_mask )
# g u i d e t h e making wrt t o t r a i n i n g epoch
# k e e p _ r a t i o = 1 . 0
k e e p _ r a t i o = f l o a t ( ( c u r r e n t _ e p o c h + 1) / t o t a l _ e p o c h )
mask_idcs = [ ]
f o r iin range ( bsz ) :
i fpadding_mask i s not None :
sz = a l l _ s z ‚àí padding_mask [ i ] . long ( ) .sum ( ) . i te m ( )
num_mask = i n t(
# add a random number f o r p r o b a b i l i s t i c r o u n d i n g
mask_prob *sz / f l o a t ( mask_lengt h )
+ np . random . ra nd ( )
)
num_mask = max ( min_masks , num_mask )
e l s e :
sz = a l l _ s z
num_mask = all_num_mask
i fmask_type == " s t a t i c " :
l e n g t h s = np . f u l l ( num_mask , mas k_length )
e l s e :
r a i s e E x c e p t i o n ( " unknown mask s e l e c t i o n " + mask_type )
i f sum ( l e n g t h s ) == 0 :
l e n g t h s [ 0 ] = min ( mask_length , sz ‚àí 1)
min_len = min ( l e n g t h s )
i fsz ‚àí min_len <= num_mask :
min_len = sz ‚àí num_mask ‚àí 1
# r e v e r s e t h e i n d e x l i s t t o g e t t h e i n d e x e s a s s o c i a t e d w i t h max l o s s e s
s a m p l e _ l o s s _ i n d e x = i d s _ s h u f f l e _ l o s s [ i ] [ : : ‚àí 1 ]
# c a l c u l a t e random_mask and l e a r n a b l e _ m a s k u s i n g k e e p _ r a t i o
random_mask = i n t( num_mask *(1 ‚àí k e e p _ r a t i o ) )
l e a r n a b l e _ m a s k = num_mask ‚àí random_mask
# randomly s e l e c t mask i n d e x .
mask_idc = np . random . c h o i c e ( sz ‚àí min_len , num_mask , r e p l a c e = F a l s e )
s a m p l e _ l o s s _ i n d e x = s a m p l e _ l o s s _ i n d e x [ : l e a r n a b l e _ m a s k ]
# r e c a l c u l a t e mask_idc f o r random masking :
mask_idc = np . random . c h o i c e ( np . s e t d i f f 1 d ( mask_idc , s a m p l e _ l o s s _ i n d e x ) , random_mask , r e p l a c e = F a l s e )
l o s s _ m a s k _ i d c = np . a s a r r a y (
[
s a m p l e _ l o s s _ i n d e x [ j ] + o f f s e t
f o r jin range (l e n( s a m p l e _ l o s s _ i n d e x ) )
f o r o f f s e t in range ( l e n g t h s [ j ] )
]
)
mask_idc = np . a s a r r a y (
[
mask_idc [ j ] + o f f s e t
f o r jin range (l e n( mask_idc ) )
f o r o f f s e t in range ( l e n g t h s [ j ] )
]
)
# p r i n t ( l o s s _ m a s k _ i d c )
i f l e n ( mask_idc ) == 0 :
combine_idc = l o s s _ m a s k _ i d c
e l s e :
combine_idc = np . c o n c a t e n a t e ( ( l o s s _ m a s k _ i d c , mask_idc ) )
mask_idcs . append ( np . u n i q u e ( combine_idc [ combine_idc < sz ] ) )
min_len = min ( [l e n(m) f o r minmask_idcs ] )
f o r i , mask_idc in enumerate ( mask_idcs ) :
i f l e n ( mask_idc ) > min_len and r e q u i r e _ s a m e _ m a s k s :
mask_idc = np . random . c h o i c e ( mask_idc , min_len , r e p l a c e = F a l s e )
i fmask_dropout > 0 :
num_holes = np . r i n t ( l e n( mask_idc ) *mask_dropout ) . a s t y p e ( i n t)
mask_idc = np . random . c h o i c e (
mask_idc , l e n( mask_idc ) ‚àí num_holes , r e p l a c e = F a l s e
)
mask [ i , mask_idc ] = True
return mask
15