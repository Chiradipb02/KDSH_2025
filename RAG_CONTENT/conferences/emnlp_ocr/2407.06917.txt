Who is better at math, Jenny or Jingzhen?
Uncovering Stereotypes in Large Language Models
Zara Siddique∗, Liam D. Turner∗, Luis Espinosa-Anke∗†
∗School of Computer Science and Informatics, Cardiff University, United Kingdom
†AMPLYFI, United Kingdom
{siddiquezs2,turnerl9,espinosa-ankel}@cardiff.ac.uk
Abstract
Large language models (LLMs) have been
shown to propagate and amplify harmful stereo-
types, particularly those that disproportion-
ately affect marginalised communities. To un-
derstand the effect of these stereotypes more
comprehensively, we introduce GlobalBias, a
dataset of 876k sentences incorporating 40 dis-
tinct gender-by-ethnicity groups alongside de-
scriptors typically used in bias literature, which
enables us to study a broad set of stereotypes
from around the world. We use GlobalBias
to directly probe a suite of LMs via perplex-
ity, which we use as a proxy to determine
how certain stereotypes are represented in the
model’s internal representations. Following
this, we generate character profiles based on
given names and evaluate the prevalence of
stereotypes in model outputs. We find that
the demographic groups associated with vari-
ous stereotypes remain consistent across model
likelihoods and model outputs. Furthermore,
larger models consistently display higher levels
of stereotypical outputs, even when explicitly
instructed not to.
1 Introduction
LLMs are increasingly used for tasks that span ar-
eas of concern for bias and fairness (Anthis et al.,
2024), such as user discrimination in recommenda-
tions (Xu et al., 2024). Despite the obvious need for
ethical frameworks around these models, these are
mostly lacking or incomplete, and make research
into fairness and bias essential for supporting pub-
lic confidence in the use of generative AI. While
bias is often defined in ambiguous and conflict-
ing ways (Blodgett et al., 2020), in this paper we
focus on representational harms, defined by Craw-
ford (2017) as harms that “occur when systems
reinforce the subordination of some groups along
the lines of identity,” specifically harms caused by
stereotyping.
Existing research on stereotypes in LMs is lim-
ited, and predominantly focuses on African Ameri-Attribute Details
Name Kazuyo
Age 45
Personality Traits Calm, Wise, Observant
Negative Traits Perfectionist, Indecisive,
Shy
Hobbies Bonsai gardening, Origami,
Tea ceremonies
Occupation Librarian
Height 5.2 ft
Hair Colour Black
Eye Colour Brown
Skin Colour Light
Build Petite
Socioeconomic Status Middle class
Sexual Orientation Asexual
Religion Shinto
Table 1: Example of a character profile generated
by Claude 3 Opus for given name Kazuyo (Japanese,
F). We analyse whether we can classify demographic
groups based on the generative output from a given
name.
can and White groups (Jiang and Fellbaum, 2020;
May et al., 2019), or a subset of US census
groups, often with Middle Eastern added (Guo and
Caliskan, 2021; Cao et al., 2022; Kirk et al., 2024;
Cheng et al., 2023). Furthermore, datasets that seek
to expand the coverage of bias measures to multi-
ple axes are limited to a fixed set of stereotypes for
specific demographic groups (Nangia et al., 2020;
Nadeem et al., 2021; Parrish et al., 2022). To ad-
dress these limitations, we focus on incorporating a
wide range of ethnicities and using a one-vs-all, un-
supervised approach to identify which stereotypes
are associated with each demographic group. We
also highlight the importance of analysis that uses a
intersectional lens, where biases compound across
a combination of different axes, e.g., gender and
ethnicity, to cause unique harms.
In Sections 4 and 5, we utilise templates involv-
ing stereotypes for 40 groups, defined by both an
ethnicity and a gender, e.g. English Female or Chi-
nese Female, along with a descriptor (e.g. ’good atarXiv:2407.06917v3  [cs.CL]  9 Oct 2024math’) to explore which descriptors are more likely
to appear in a sentence with certain given names
across different LLMs.
Considering the limitations of using a fixed set
of stereotypes and the fact that likelihoods do not
always correspond to model outputs (Parrish et al.,
2022), in Section 6, we take a lexicon-free ap-
proach that utilizes the given names in our dataset
in a generation task. An example output can be seen
in Table 1. We present both quantitative and qual-
itative analyses of representational harms caused
by stereotypical outputs. The results highlight the
magnitude of stereotypical bias across both open
and closed-sourced LLMs. From this, this work
presents the following contributions:
1.the GlobalBias dataset for studying harm-
ful stereotypes, which consists of 876,000
sentences for 40 distinct gender-by-ethnicity
groups
2.an analysis of which stereotypes are surfaced
for each group by a number of LMs, and
the extent and nature of harm caused by the
these stereotypes, particularly for intersec-
tional groups
3.the finding that larger models have more
stereotypical outputs, even when explicitly in-
structed to avoid stereotypes and clichés
4.the finding that bias stays consistent across
model’s internal representation and outputs,
contrary to claims in previous work in the
field.
2 Background and Related Work
2.1 Impact of Stereotyping
Stereotyping can influence how we perceive our-
selves and others, as well as how we behave to-
wards others. For example, Bertrand and Mul-
lainathan (2004) found résumés with White names
received 50% more invitations to interview than
resumes with Black names. More broadly, Bier-
nat (2003) found that when one judges individual
members of stereotyped groups on stereotyped di-
mensions, one does so with reference to within-
category standards, e.g. evaluations of men and
women on leadership competence may not be di-
rectly comparable, as their meaning is tied to differ-
ent referents: ‘good’ for a woman does not mean
the same thing as ‘good’ for a man. LLMs trainedon data that includes stereotypes or LLMs using
non-comprehensive systems to mitigate biases can
perpetuate discrimination and social inequality in
ways that are difficult to detect and address.
2.2 Axes of Analysis
Early work on bias in word embeddings focused
on a single dimension, predominantly binary gen-
der (Bolukbasi et al., 2016; Zhao et al., 2018b;
Ethayarajh et al., 2019), and less frequently, race
(Caliskan et al., 2017; Garg et al., 2018). Work
looking at a single demographic axis often fails to
mirror the reality of race and gender being inter-
twined.
Crenshaw (1989) defines how using a single-axis
framework erases Black women’s experience in le-
gal and political contexts, as race discrimination
tends to be viewed in terms of gender-privileged
Blacks, and gender discrimination focuses on race-
privileged women. Crenshaw provides a frame-
work for understanding how different aspects of a
person’s social and political identities combine to
create different modes of discrimination and privi-
lege, known as intersectionality.
There is a growing body of research in the field
of intersectional bias, which starts to investigate
the nuance of how race and gender interact (Jiang
and Fellbaum, 2020). There are several measures
defined for evaluating intersectional biases, such
as the angry black woman stereotype in contextual
word embeddings (May et al., 2019; Tan and Celis,
2019), the contextual word embedding test (CEAT)
which also looks at a limited and fixed labelled
set of stereotypes (Guo and Caliskan, 2021), and
others (Lepori, 2020; Cao et al., 2022; Cheng et al.,
2023).
2.3 Stereotype Datasets
This paper builds on previous work exploring
how stereotypes are associated with specific de-
mographic groups and how this reinforces existing
social hierarchies (Greenwald et al., 1998; Blodgett
et al., 2021). Several datasets have been developed
to examine stereotypes, often structured around
sentence pairs comparing two demographic groups
(May et al., 2019), contrasting stereotypes and anti-
stereotypes (Zhao et al., 2018a; Nangia et al., 2020;
Nadeem et al., 2021), or using question-answer sets
to compare groups (Parrish et al., 2022). While
valuable, these datasets are not suitable for our ob-
jective of analyzing each stereotype across multiple
subgroups. In contrast, our one-vs-all approach of-fers more robust statistical power, and reduces the
impact of outliers and natural variability in two-
group comparisons. For evaluation, we posit that
perplexity, which Smith et al. (2022) and Smith
and Williams (2021) used to compare multiple sub-
groups in one test, is a suitable method, and thus
we develop it further.
Furthermore, the datasets mentioned above are
predominantly situated within a U.S. context (Blod-
gett et al., 2021), and do not adequately represent
the global user base of LLMs. While efforts have
been made to adapt these datasets for other lan-
guages and cultural contexts (Fort et al., 2024; Sa-
hoo et al., 2024), our methodology bypasses the
need for pre-defined stereotype labels, allowing for
more flexible analysis of outputs across various
contexts.
2.4 Use of Proper Names
There exists measurable statistical tendencies for
names to refer to both gender and race demo-
graphics (Tzioumis, 2018). May et al. (2019) ob-
serves that "tests based on given names more of-
ten find a significant association than those based
on group terms". Therefore, we use given names
as a proxy for ethnicity and gender, based on ev-
idence that given names are often used to draw
stereotypical conclusions about people by both hu-
mans (Bertrand and Mullainathan, 2003; Dechief
and Oreopoulos, 2012) and in LLM outputs (De-
Arteaga et al., 2019; Romanov et al., 2019; Maud-
slay et al., 2019). Using a range of names for each
group intends to mitigate the impact of any single
name on the group’s overall results.
3 The Dataset
We propose a new dataset1named GlobalBias for
studying harmful stereotypes, which consists of
sets of 10 proper names spanning 40 groups. A
summary of key statistics for the dataset can be
found in Table 2.
3.1 Proper Names
Our primary objective is to compile a list of di-
verse demographic groups, alongside representa-
tive names for each group. In this section, we
discuss how we build such a dataset, of specifically
40 distinct groups, starting from existing labeled
resources.
1The dataset and code used to evaluate the mod-
els can be found at https://github.com/groovychoons/
GlobalBias .Our seed dataset of names is the Genni + Eth-
nea dataset (Torvik, 2018). It contains over 2 mil-
lion names, each annotated with ethnicity and gen-
der. We first filter ∼176,000 unique first names,
to include only those with 2 to 14 characters and
a male or female gender classification, narrowing
our dataset to approximately 35,000 names. We
exclusively included names labeled with a binary
gender by the Genni model used to label the seed
dataset, thereby excluding gender-neutral names.
We posit that gender-neutral names do not neces-
sarily represent gender diverse groups in LLMs,
and are more often a mixture of male and female
stereotypes, though we acknowledge that focusing
on binary gender classification fails to represent
the diverse spectrum of human self-identification
as discussed in Butler (1989), Kuper et al. (2012)
and Larson (2017).
By utilizing embeddings and clustering tech-
niques, we identify names that an LLM perceives
as highly correlated within these groups. We
use OpenAI’s ( text-embedding-3-large ) em-
beddings for each name and apply Mini Batch
K-Means clustering to group the names into clus-
ters. We select ten names per group to prevent
one name from having a large impact on results,
and reduce the harm caused by misclassification of
names (Gautam et al., 2024). These ten names are
randomly selected from clusters with a high gender
and ethnicity agreement, i.e. >50% names in the
same group in a cluster, meaning an LLM is likely
to classify the chosen cluster as belonging to that
ethnic and gender group. Where ethnicities have
an exclusive gender, we select 10 names of the op-
posite gender with high probability of belonging to
that ethnicity for gender balance across the entire
dataset.
We select 400 unique first names, namely, the
part of a personal name considered to distinguish
an individual within a group. It is important to
note that while naming conventions vary across
the world, these names were gathered in a Western
academic context where the first name typically
corresponds to the given name.
3.2 Descriptors
Having compiled a suitable list of demographic
groups and representative names via clustering, our
next step is to obtain a set of suitable descriptors.
We will combine these descriptors with the names
to construct templates, which will serve the inputParameter Count
Names 400
Descriptors 730
Templates 3
Sentences 876,000
Demographic Groups 40
Table 2: Summary of GlobalBias dataset statistics.
for a probing exercise to various LMs in our exper-
iments. Let us now discuss how we obtain these
descriptors, and the resulting templates we derive
from them.
We initially draw on three existing datasets:
the HOLISTICBIAS dataset (Smith et al., 2022),
Ghavami and Peplau (2013) and StereoSet
(Nadeem et al., 2021). The first, HOLISTICBIAS,
is split into 13 demographic axes; we use 11 of
these axes (Race/Ethnicity and Nationality are ex-
cluded, as the purpose of the experiment is to infer
these from the given name). Ghavami and Peplau
(2013) provide a labelled dataset of stereotypes
from a free-response survey. We extract stereotype
terms from StereoSet, which was handcrafted to
test a fixed set of stereotypes in LLMs. As a result,
our descriptor terms represent a diverse range of
potential stereotypes.
3.3 Templates
Following previous work from Smith et al. (2022),
we construct three templates combining the names
compiled in Section 3.1 and the descriptors ob-
tained in Section 3.2. These templates allow us to
measure token likelihoods of the descriptors in rela-
tion to the given names. These templates combine
given names and descriptor terms. Examples of the
three templates can be found in Figure 1.
At the end of this process, the GlobalBias dataset
is ready: it comprises 876,000 sentences cover-
ing 40 distinct gender-by-ethnicity groups created
through the combination of proper nouns and de-
scriptors. In the next section, we discuss how
we use GlobalBias for evaluating stereotypical be-
haviour in LMs, and discuss the results.
4 Adjusted Perplexity across Descriptors
(APX)
4.1 Perplexity
Perplexity has become an increasingly common
evaluation measure when looking at stereotypes
in LLMs (Smith and Williams, 2021; Smith et al.,2022). We use perplexity to determine how stereo-
typical an LM perceives a sentence to be. The lower
the perplexity, the more likely an LM is to generate
a sequence of words. For decoder-only LMs such
as GPT-2 (Radford et al., 2019), we compute the
perplexity of a tokenized sentence x= [x1...xm]
as:
PPL(x) = exp 
−1
mmX
i=1logPlm(xi|xi−1)!
(1)
where Plm(x|x)is the likelihood of the next token
given the preceding tokens.
For masked language models (MLM) such as
RoBERTa (Liu et al., 2019), pseudo-perplexity
(Salazar et al., 2020) is used instead, which replaces
the likelihood Pin Equation 1 by Pmask(xi|x¬i),
the pseudo-likelihood to predict the masked token
xi(Wang and Cho, 2019). For encoder-decoder
LMs such as Flan-UL2 (Tay et al., 2022), we com-
putePlmon the decoder, which is conditioned by
the encoder.
4.2 Defining APX
The use of perplexity in this context can be prob-
lematic, due to noise from high-frequency given
names during training (Kaneko and Bollegala,
2022), meaning some ethnic and gender groups
will tend toward having higher or lower perplexity
scores for all descriptors, regardless of any under-
lying biases. We account for this by proposing a
novel bias evaluation metric, which we name Ad-
justed Perplexity across Descriptors (APX).
Consider the mean perplexity for an intersec-
tional group of given names Giand a descriptor
Dj, we define their perplexity as PPL(GiDj). We
define the Adjusted Perplexity across Descriptors
to be:
Mean Group Perplexity =DX
j=1PPL(GiDj)
|D|(2)
Mean Total Perp. =PG,D
i=1,j=1PPL(GiDj)
|G| · |D|(3)
APX(GiDj) =PPL(GiDj)×Mean Group Perp.
Mean Total Perp.
(4)4.3 Models
In our experiments in Sections 4 and 5, we eval-
uate a suite of seven language models to ex-
amine the generalizability of our bias measures
across various model sizes and architectures, these
are: BERT ( google-bert/bert-large-cased ; De-
vlin et al. 2019), RoBERTa ( roberta-large ; Liu
et al. 2019), Flan-UL2 ( google/flan-ul2 , Tay et al.
2022), GPT-2 ( gpt2-xl , Radford et al. 2019),
GPT Neo X ( EleutherAI/gpt-neox-20b ; Black
et al. 2022), OPT ( facebook/opt-30b ; Zhang et al.
2022) and Llama 3 ( meta-llama/Meta-Llama-3-
8B; AI@Meta 2024).
4.4 Validating APX
We measure perplexity and APX on a subset of
GlobalBias of 36,960 sentences, composed of 3
templates, 280 unique names, and 44 labeled de-
scriptors, and compare APX to the perplexity met-
ric for classification accuracy and mean reciprocal
rank on a range of models. Human participants pro-
vide this validation set of racial stereotypes with
ground truth information in prior work (Ghavami
and Peplau, 2013). The experiment uses 11 stereo-
types for 4 groups, removing any duplicates that
appear across multiple groups, for example, ’intel-
ligent’ is associated with both Asian American and
White groups.
Two inherent limitations were identified in the
dataset. Due to the dataset’s categorization frame-
work of five distinct racial categories, we combined
our diverse ethnicities within these predefined cat-
egories, eliminating 6 out of 20 ethnicities. The
primary objective of this experiment was to vali-
date the APX measure, the full set of ethnicities is
explored in more detail in the next experiment. Fur-
thermore, it’s worth noting that the specific focus of
African American stereotypes did not correspond
directly with given names for any of the ethnic
groups under examination, rendering it unsuitable
for inclusion within this context.
We take the average of the 10 names per group
for each template, and then take the normalised av-
erage of the three templates in order to obtain a ro-
bust bias score for each gender-by-ethnicity group
for each descriptor. To calculate one-vs-all classifi-
cation accuracy, we take the group with the mini-
mum bias score to be the most biased group. The
accuracy shows how often the group with the mini-
mum bias score for each descriptor matches the tar-
get group. This methodology enables comparisonModel Acc. (PPL) Acc. (APX)
BERT 38.6% 38.6%
RoBERTa 45.5% 50.0%
Flan-UL2 36.4% 36.4%
GPT-2 31.8% 50.0%
GPT-NeoX 25.0% 38.6%
OPT 36.4% 43.2%
Llama 3 31.8% 50.0%
Table 3: Classification accuracy in a 4 class stereotype
classification task. We show the accuracy when using
the perplexity and APX metrics for 7 models. Classi-
fication accuracy represents how often the group with
the minimum bias score for each descriptor matches the
target group.
Model MRR (PPL) MRR (APX)
BERT 58.1% 63.6%
RoBERTa 56.6% 69.1%
Flan-UL2 59.3% 62.9%
GPT-2 54.2% 66.5%
GPT-NeoX 54.5% 59.1%
OPT 55.7% 66.1%
Llama 3 58.9% 70.3%
Table 4: Mean Reciprocal Rank in a 4 class stereotype
classification task.
across masked, encoder-decoder, and decoder-only
language models. Despite the variations in how
perplexity is calculated for each model type, us-
ing the lowest perplexity value from four ethnicity
groups ensures the results are generalizable across
different model architectures.
Table 3 shows the classification accuracy when
using perplexity and APX for the labelled stereo-
type dataset. We can see that in 5 out of 7 models,
the use of APX improves performance, by an av-
erage of 12.26%. In addition, we measure Mean
Reciprocal Rank (MRR) for each of the 44 descrip-
tors, by ranking the perplexities and APX of the 4
ethnic groups. This allows us to investigate cases
where a group may have the second lowest perplex-
ity, which works well for descriptors that may be
stereotypes for multiple groups, such as ’family-
oriented’ or ’religious’. The results in Table 4 show
that using APX improves MRR across all models,
with an average improvement of 8.61%.
Our experimental results show that the proposed
evaluation measure, APX, outperforms perplexity
in classification tasks when assessed using both ac-
curacy and MRR. Thus, APX proves to be a moreFigure 1: An overview of our methodology using the
example descriptor good at math . We compute the nor-
malised average of APX for 10 names for each template,
followed by the average over 3 templates to calculate a
bias score. Gender-by-ethnicity groups with a 1% statis-
tical significance (noted by the orange line) are consid-
ered to be associated with that descriptor, i.e. Chinese
Female with good at math .
effective metric for measuring biases in language
models. We use APX in the next section to in-
vestigate a wider set of demographic groups and
stereotypes.
5 Stereotypes via APX
We propose a statistically robust methodology to
identify the demographic groups associated with
the 730 descriptors in GlobalBias. We calculate the
APX for the 876,000 sentences in the dataset. As
described in the previous section, we compute the
average of the 10 names per group for each tem-
plate, and take the normalised average of the three
templates to obtain a bias score for each gender-by-
ethnicity group for each descriptor. Once we have
the bias scores for each of the 40 groups, we iden-
tify any groups with a 1% one-tailed significance
level, as shown in Figure 1. Our methodology can
be applied to any descriptor and extended to addi-
tional gender-by-ethnicity groups and demographic
axes in future.
5.1 Overview
To ensure consistency and enable comparison
across the three experiments detailed in Sections
4, 5, and 6, we use Llama 3 as a case study. We
present a full table of results in Appendix A, and
a smaller, selected set of descriptors in Table 5,
which we refer to in this section. These tablesGroup Selected Descriptors
Arab, F Muslim, refugee
Arab, M extremist, Muslim, terrorist
Chinese, F good at math, quiet, very smart
Hispanic, M macho
Japanese, F always cleaning, cute, shy
Table 5: Selected stereotypes for discussion and their
associated demographic groups in Llama 3 8B.
show the descriptors associated with each gender-
by-ethnicity group in the Llama 3 8B model.
Overall, we observe the resurfacing of multiple
stereotypes noted in other studies, such as associat-
ing Arabs with being Muslim and terrorists (Chang
and Kleiner, 2003; Corbin, 2017), characterizing
Japanese women as shy and cute (Zheng, 2016;
Azhar et al., 2021), and depicting Hispanic males
as macho (Ghavami and Peplau, 2013). Among
the 730 descriptors analyzed, 147 (20.1%) demon-
strated statistically significant results. This indi-
cates that a substantial portion of descriptors in
GlobalBias did not exhibit significant bias towards
any specific demographic group. In the following
subsections, we discuss the harmful implications
of some of the stereotypes uncovered.
5.2 Muslim Terrorist Stereotypes
Arab Male given names are disproportionately
found to have a low perplexity for the words extrem-
istandterrorist . Research has found a common
narrative of all terrorists being Muslim, and some-
times this narrative even being extended to suggest
that all Muslims are terrorists (Chang and Kleiner,
2003; Corbin, 2017). This association also has
drawn criticism from media scholars, arguing that
such portrayals demonize and dehumanize Arab
individuals, portraying them as brutal religious ex-
tremists (Shaheen, 2003; Najm, 2019). This stereo-
type has recently been found to be more prevalent
in AI generated content than human generated con-
tent (Narayanan Venkit et al., 2023).
5.3 Intersectional Harms
Recent work states that "researchers overwhelm-
ingly reduce intersectionality to optimizing for fair-
ness metrics over demographic subgroups." (Ovalle
et al., 2023). Although we look at demographic
subgroups within this work, we also note the impor-
tance of discussing the power relations and social
contexts in which these biases exist, and for which
groups they are most likely to cause harm.One such bias is the continuing and damaging
perception of Asian women as docile and submis-
sive (Zheng, 2016; Azhar et al., 2021). Table 5
shows descriptors cute andshyassociated with
Japanese women and quiet associated with Chinese
women. The stereotype of Japanese women as shy
reflects an Orientalist view of Japan, and may also
reflect the disadvantaged social position in which
Japanese women in the West are situated rather
than any essential commonality among them (Ki-
tamura, 2005). This reflects the context in which
many of the LLMs tested have been trained - on
Internet data over-representing the West (Bender
et al., 2021).
Lai (1992) discusses the continuing perception
of Asian women as "cute (as in doll-like), quiet
rather than militant, and unassuming rather than
assertive". The nature of these characterizations
speaks to a lack of respect afforded to Asian women
as self-sufficient, complex individuals (Matsumoto,
2020), and contributes to the development of in-
ternalized racism and sexism (Museus and Truong,
2013).
Further, consider the stereotype of Asian Amer-
icans as “good at math”. This reinforces subordi-
nation along the lines of identity by dictating how
Asian Americans and other minorities are expected
to behave, and disregards the experiences of Asian
Americans who do not achieve model minority suc-
cess, potentially impacting their self-worth (Lee,
1999). Such stereotypes perpetuate harmful biases
and reinforce societal inequalities.
6 Stereotypes via Generation
The above experiment sheds light on the plausi-
bility assigned to sentences by LLMs containing
combinations of proper nouns and descriptors. We
complement this experiment by directly looking at
models’ generations, which has advantages such
as potentially higher correlation with downstream
performance (Luden et al., 2024). To this end, we
use a zero-shot prompting method that utilizes the
given names in GlobalBias. Our prompt (Appendix
B) instructs the model to generate a dataset of char-
acters, each associated with a given name from
GlobalBias, with information such as hobbies, per-
sonality traits and physical attributes. An example
can be found in Table 1. Additionally, the prompt
instructs the model to ensure that the dataset is free
from stereotypes and clichés, and to treat all names
equally. Our experiment encompassed four modelsModelGender +
EthnicityEthncity Gender
Chance Level 2.5% 5% 50%
Llama 3 70B 18.3% 30.6% 83.3%
GPT 3.5 21.7% 32.2% 88.9%
Claude 3 Opus 26.4% 36.1% 91.9%
GPT 4o 33.3% 38.6% 93.9%
Table 6: SVM classification accuracy for character
profiles of different demographic groups. A lower
accuracy indicates more similar character profiles across
groups, therefore less stereotypical outputs. The task
involved classification of 40 groups for Gender + Ethnic-
ity accuracy, 20 groups for Ethnicity and 2 for Gender.
with widespread usage: Claude 3 Opus, Llama 3
70B Instruct, and OpenAI’s GPT 3.5 and GPT 4o.2
The rationale for using an open-ended generation
setting was two-fold: (1) the likelihoods studied in
the previous section do not always correspond to
model outputs (Parrish et al., 2022), and (2) taking
a lexicon-free approach allows us to capture stereo-
types that we had not thought of a priori. Further-
more, this approach enables testing for stereotypes
in closed-source models.
6.1 Classification
To assess the level of bias in each model, we con-
struct a one-vs-all SVM classification across gen-
der, ethnicity, and gender-by-ethnicity groups, to
measure how easily differentiable demographic
groups are from each other. We partition our data
in to 70% for training and 30% for testing, strat-
ified based on demographic group. Each charac-
ter profile was represented using 11 features, with
each feature encoded as either a one-hot vector (for
single words) or sparse vector of the relative fre-
quencies of the words in the feature (for lists of
words).
Our results show that character descriptions cor-
responding to different demographic names are dis-
tinguishable from one another by gender, ethnicity
and the intersection of the two, indicating that all
four models produce stereotypical outputs, even
when explicitly instructed not to (Table 6).
Notably, GPT-4o exhibits the highest level of
distinction between groups. The SVM achieved
an accuracy of 33.3%, over 13 times higher than a
baseline accuracy of random classification (2.5%)
2We use a temperature of 1 to ensure a wide variety of
outputs. The outputs were generated 3 times for each model,
resulting in 1200 character profiles for each model.Feature Eliminated Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o
Overall Accuracy (%) 18.3% 21.7% 26.4% 33.3%
religion -4.1% ↓ -4.8%↓ -3.6%↓ -8.0%↓
hair_colour -1.1% ↓ -0.3%↓ -0.3%↓ -1.1%↓
height -0.2% ↓ -2.0%↓ -3.6%↓ -3.6%↓
sexual_orientation +0.3% ↑ 0.0% -0.6% ↓ +1.7% ↑
hobbies +0.9% ↑ -0.6%↓ +0.5% ↑ -1.4%↓
build +0.9% ↑ -0.6%↓ -0.6%↓ -1.4%↓
socioeconomic_status +2.3% ↑ +1.1% ↑ +1.4% ↑ +0.6% ↑
skin_colour +0.6% ↑ -1.1%↓ -3.1%↓ -2.2%↓
eye_colour +2.8% ↑ +0.5% ↑ +2.2% ↑ +1.7% ↑
personality_traits +1.4% ↑ -1.7%↓ +0.8% ↑ -0.2%↓
negative_traits +2.0% ↑ -1.7%↓ +2.2% ↑ 0.0%
age +2.8% ↑ +0.5% ↑ +0.8% ↑ +0.6% ↑
occupation +0.6% ↑ -0.3%↓ +0.3% ↑ -1.4%↓
Table 7: Model accuracies and feature impact on differentiation accuracy across demographic groups. The arrows
indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.
which would indicate no difference between de-
mographic groups. Previous research has demon-
strated that larger models tend to exhibit greater
gender and racial biases (Ganguli et al., 2022; Rae
et al., 2022; Ganguli et al., 2023). Our study ex-
tends these findings by revealing that this pattern
also manifests in intersectional groups in the con-
text of stereotypes.
6.2 Feature Analysis
We conduct a feature elimination process to iden-
tify the importance of different features in distin-
guishing between demographic groups, in order
to identify potential sources of bias. We analyse
groups of features such as ‘hobbies’, rather than
individual features such as ‘reading’. The impact
of each group of features for gender-by-ethnicity
groups can be found in Table 7. The impact of each
group of features for ethnicity only and gender only
can be found in Appendix C.
We find that, across all models, religion is the
most influential feature in predicting ethnicity. For
3 out of 4 models, religion is also the strongest
feature when classifying combined gender and eth-
nicity groups suggesting that models are overly
reliant on religious features when describing eth-
nicity, potentially leading to biased or inaccurate
portrayals of individuals. Conversely, for predict-
ing gender alone, removing religion from the in-
put results in increased accuracy. Similarly, skin
colour is a significant feature for ethnicity and gen-
der + ethnicity classifications, while it has minimalimpact on gender-only. Significant features that
emerged for gender-only classification were physi-
cal characteristics such as height and build.
Our results also show that combining features
from gender-only and ethnicity-only classifications
does not lead to improved performance in gender +
ethnicity groups. For example, in Claude 3 Opus,
the inclusion of sexual orientation decreased accu-
racy in ethnicity-only and no effect in gender-only
classifications, while improving accuracy in gender
+ ethnicity classification. This highlights that inter-
sectional identities and the stereotypes that affect
them are more complex than the sum of their parts
(Crenshaw, 1989), and underscores the significance
of considering intersectionality when evaluating
bias to foster fair and inclusive AI systems.
6.3 Top Words
Building on the ranking of individual features, we
use Jensen-Shannon divergence (JSD) to identify
differentiating words for each gender-by-ethnicity
group across different features (Trujillo et al., 2021;
Cheng et al., 2023). We utilize the Shifterator im-
plementation of JSD (Gallagher et al., 2021) to
compute the top 10 words for each feature, and the
groups they belong to. The top words for selected
features for Llama 3 70B Instruct and GPT 40 (best
and worst models) can be found in Appendix D.
Given that religion emerged as the most sig-
nificant feature for both gender-by-ethnicity and
ethnicity-only groups in our analysis, we exam-
ine it further here. As illustrated in Table 8, theWord Generation APX
jewishIsraeli, M
Israeli, FIsraeli, M
Israeli, F
hinduIndian, M
Indian, FIndian, M
Indian, F
shintoJapanese, M
Japanese, FJapanese, M
Japanese, F
buddhistThai, M
Thai, FThai, M
Thai, F
muslimArab, M
Turkish, MArab, M
Arab, F
Table 8: Top differentiating religion words and asso-
ciated groups in both experiments using Llama 3 70B
(Generation) and Llama 3 8B (APX).
top religions identified by JSD and the gender-by-
ethnicity groups for which they were generated
align consistently with the groups they were corre-
lated with via APX, demonstrating that bias stays
consistent across the model’s internal representa-
tions and generative outputs, in contrast to claims
made in Parrish et al. (2022).
The association of certain religions with demo-
graphic groups reinforces essentializing narratives,
such as the conflation of the Islamic world and
the Arab world (Chang and Kleiner, 2003). In-
stead of representing the diversity within groups,
the perpetuation of religious stereotypes defines
each of these demographic groups solely based on
a limited, fixed set of characteristics—such as be-
ing Muslim or from the Middle East—rather than
recognizing their full humanity (Rosenblum and
Travis, 1996; Woodward, 1997). The persistence of
religious stereotypes in LLM outputs may further
marginalize individuals from other religious and
geographic backgrounds with certain given names.
7 Conclusion
In this work, we present the GlobalBias dataset,
which allows us to undertake a comprehensive
study of intersectional stereotypes. We introduce
a new evaluation metric, APX, to adjust for high-
frequency given names in training. This study ex-
amines a broader range of demographic groups
than previous studies, and we conduct multiple ex-
periments that investigate both the model’s internal
representations via APX and model outputs via
generation experiments.
We find that larger models produce more stereo-
typical outputs, even when explicitly instructed notto. We also show using the example of religion
that bias stays consistent across model’s internal
representation and outputs.
Our work reveals the prevalence and impact of
stereotypes across a diverse range of ethnic and gen-
der groups through the introduction of the Global-
Bias dataset. We highlight the importance of a com-
prehensive and intersectional approach to studying
bias in LMs, which is essential for ensuring eth-
ical, fair, and effective use of LMs in real-world
scenarios, ultimately fostering trust and inclusivity
in technology.
Limitations
While our work aims to broaden the scope of ethnic-
ities covered in NLP bias research, there are many
ethnic groups and genders not covered in this work,
and we exclude other critical aspects such as age,
disability, and socioeconomic status. The dataset’s
creation process excludes gender-neutral names,
limiting its applicability to a broader spectrum of
identities, and that the use of given names in itself
can contribute to harm (Gautam et al., 2024). We
encourage future data collection involving given
names to allow self-identification of gender, where
possible, as recommended by Larson (2017). More-
over, the GlobalBias dataset is not intended as a
benchmark; instead, it is used to gain insights into
a wider set of intersectional demographic groups.
By explicitly categorizing and associating stereo-
types with specific demographic groups, there is a
risk of perpetuating the very biases the study aims
to mitigate. The study does not propose specific
debiasing techniques, and while the GlobalBias
dataset and APX metric can aid future efforts, prac-
tical implementations and evaluations of debiasing
strategies are needed.
Furthermore, other measures for perplexity have
been proposed such as AULA (Kaneko and Bolle-
gala, 2022). We use perplexity, and APX, as it can
be adapted for use across a range of model archi-
tectures. The evaluation methods, while insightful,
may not fully reflect real-world scenarios. Find-
ings, particularly regarding larger models produc-
ing more stereotypical outputs, are based on current
LLM architectures and may need re-evaluation as
new models emerge. The closed-source nature of
some models also limits transparency and replica-
bility.Acknowledgements
We would like to thank Nedjma Ousidhoum and Yi
Zhou for their very helpful comments in reviewing
this paper. We also thank Dimosthenis Antypas,
Joanne Boisson, Jose Camacho-Collados and Hsu-
vas Borkakoty for helpful feedback. This work is
funded in part by the UKRI AIMLAC CDT.
References
AI@Meta. 2024. Llama 3 model card.
Jacy Reese Anthis, Kristian Lum, Michael Ekstrand,
Avi Feller, Alexander D’Amour, and Chenhao Tan.
2024. The impossibility of fair llms. arXiv e-prints ,
pages arXiv–2406.
Sameena Azhar, Antonia R. G. Alvarez, Anne S. J.
Farina, and Susan Klumpner. 2021. “you’re so ex-
otic looking”: An intersectional analysis of asian
american and pacific islander stereotypes. Affilia ,
36(3):282–301.
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language mod-
els be too big? In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Trans-
parency , FAccT ’21, page 610–623, New York, NY ,
USA. Association for Computing Machinery.
Marianne Bertrand and Sendhil Mullainathan. 2003.
Are emily and greg more employable than lakisha
and jamal? a field experiment on labor market dis-
crimination. Working Paper 9873, National Bureau
of Economic Research.
Marianne Bertrand and Sendhil Mullainathan. 2004.
Are emily and greg more employable than lakisha and
jamal? a field experiment on labor market discrimina-
tion. American Economic Review , 94(4):991–1013.
Monica Biernat. 2003. Toward a broader view of so-
cial stereotyping. The American psychologist , 58
12:1019–27.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-
hit, Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. 2022. GPT-NeoX-20B: An open-
source autoregressive language model. In Proceed-
ings of BigScience Episode #5 – Workshop on Chal-
lenges & Perspectives in Creating Large Language
Models , pages 95–136, virtual+Dublin. Association
for Computational Linguistics.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) ispower: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5454–
5476, Online. Association for Computational Lin-
guistics.
Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
Robert Sim, and Hanna Wallach. 2021. Stereotyping
Norwegian salmon: An inventory of pitfalls in fair-
ness benchmark datasets. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 1004–1015, Online. Association
for Computational Linguistics.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016. Man
is to computer programmer as woman is to home-
maker? debiasing word embeddings. In Advances in
Neural Information Processing Systems , volume 29.
Curran Associates, Inc.
Judith Butler. 1989. Gender Trouble: Feminism and the
Subversion of Identity . Routledge.
Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science , 356(6334):183–186.
Yang Trista Cao, Anna Sotnikova, Hal Daumé III,
Rachel Rudinger, and Linda Zou. 2022. Theory-
grounded measurement of U.S. social stereotypes in
English language models. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1276–1295, Seattle,
United States. Association for Computational Lin-
guistics.
Szu-Hsien Chang and Brian Kleiner. 2003. Common
racial stereotypes. Equal Opportunities Interna-
tional , 22:1–9.
Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023.
Marked personas: Using natural language prompts to
measure stereotypes in language models. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1504–1532, Toronto, Canada. Association for
Computational Linguistics.
Caroline Mala Corbin. 2017. Terrorists are always mus-
lim but never white: At the intersection of critical
race theory and propaganda. Fordham Law Review ,
86:455–485.
Kate Crawford. 2017. The trouble with bias. In Con-
ference on Neural Information Processing Systems,
invited speaker .
Kimberle Crenshaw. 1989. Demarginalizing the inter-
section of race and sex: A black feminist critique
of antidiscrimination doctrine, feminist theory and
antiracist politics. The University of Chicago Legal
Forum , 140:139–167.Maria De-Arteaga, Alexey Romanov, Hanna Wal-
lach, Jennifer Chayes, Christian Borgs, Alexandra
Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,
and Adam Tauman Kalai. 2019. Bias in bios: A case
study of semantic representation bias in a high-stakes
setting. In Proceedings of the Conference on Fair-
ness, Accountability, and Transparency , FAT* ’19,
page 120–128, New York, NY , USA. Association for
Computing Machinery.
Diane Dechief and Philip Oreopoulos. 2012. Why do
some employers prefer to interview matthew, but not
samir? new evidence from toronto, montreal, and
vancouver. SSRN Electronic Journal .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Kawin Ethayarajh, David Duvenaud, and Graeme Hirst.
2019. Understanding undesirable word embedding
associations. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics , pages 1696–1705, Florence, Italy. Associa-
tion for Computational Linguistics.
Karen Fort, Laura Alonso Alemany, Luciana Benotti,
Julien Bezançon, Claudia Borg, Marthese Borg,
Yongjian Chen, Fanny Ducel, Yoann Dupont,
Guido Ivetta, Zhijian Li, Margot Mieskes, Marco
Naguib, Yuyan Qian, Matteo Radaelli, Wolfgang S.
Schmeisser-Nieto, Emma Raimundo Schulz, Thiziri
Saci, Sarah Saidi, Javier Torroba Marchante, Shilin
Xie, Sergio E. Zanotto, and Aurélie Névéol. 2024.
Your stereotypical mileage may vary: Practical chal-
lenges of evaluating biases in multiple languages and
cultural contexts. In Proceedings of the 2024 Joint
International Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024) , pages 17764–17769, Torino, Italia.
ELRA and ICCL.
Ryan J. Gallagher, Morgan R. Frank, Lewis Mitchell,
Aaron J. Schwartz, Andrew J. Reagan, Christopher M.
Danforth, and Peter Sheridan Dodds. 2021. General-
ized word shift graphs: a method for visualizing and
explaining pairwise comparisons between texts. EPJ
Data Science , 10(1).
Deep Ganguli, Amanda Askell, Nicholas Schiefer,
Thomas I. Liao, Kamil ˙e Lukoši ¯ut˙e, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Catherine Olsson,
Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr,
Jared Mueller, Joshua Landau, Kamal Ndousse, Ka-
rina Nguyen, Liane Lovitt, Michael Sellitto, Nelson
Elhage, Noemi Mercado, Nova DasSarma, Oliver
Rausch, Robert Lasenby, Robin Larson, Sam Ringer,
Sandipan Kundu, Saurav Kadavath, Scott Johnston,Shauna Kravec, Sheer El Showk, Tamera Lanham,
Timothy Telleen-Lawton, Tom Henighan, Tristan
Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann,
Dario Amodei, Nicholas Joseph, Sam McCandlish,
Tom Brown, Christopher Olah, Jack Clark, Samuel R.
Bowman, and Jared Kaplan. 2023. The capacity for
moral self-correction in large language models.
Deep Ganguli, Danny Hernandez, Liane Lovitt,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Nova Dassarma, Dawn Drain, Nelson Elhage,
Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds,
Tom Henighan, Scott Johnston, Andy Jones, Nicholas
Joseph, Jackson Kernian, Shauna Kravec, Ben Mann,
Neel Nanda, Kamal Ndousse, Catherine Olsson,
Daniela Amodei, Tom Brown, Jared Kaplan, Sam
McCandlish, Christopher Olah, Dario Amodei, and
Jack Clark. 2022. Predictability and surprise in large
generative models. In 2022 ACM Conference on Fair-
ness, Accountability, and Transparency , FAccT ’22.
ACM.
Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify 100
years of gender and ethnic stereotypes. Proceedings
of the National Academy of Sciences , 115(16):E3635–
E3644.
Vagrant Gautam, Arjun Subramonian, Anne Lauscher,
and Os Keyes. 2024. Stop! in the name of flaws:
Disentangling personal names and sociodemographic
attributes in NLP. In Proceedings of the 5th Work-
shop on Gender Bias in Natural Language Process-
ing (GeBNLP) , pages 323–337, Bangkok, Thailand.
Association for Computational Linguistics.
Negin Ghavami and Letitia Anne Peplau. 2013. An in-
tersectional analysis of gender and ethnic stereotypes:
Testing three hypotheses. Psychology of Women
Quarterly , 37(1):113–127.
A. G. Greenwald, D. E. McGhee, and J. Schwartz. 1998.
Measuring individual differences in implicit cogni-
tion: the implicit association test. Journal of Person-
ality and Social Psychology , 74:1464–1480.
Wei Guo and Aylin Caliskan. 2021. Detecting emergent
intersectional biases: Contextualized word embed-
dings contain a distribution of human-like biases. In
Proceedings of the 2021 AAAI/ACM Conference on
AI, Ethics, and Society , AIES ’21, page 122–133,
New York, NY , USA. Association for Computing
Machinery.
May Jiang and Christiane Fellbaum. 2020. Interdepen-
dencies of gender and race in contextualized word
embeddings. In Proceedings of the Second Workshop
on Gender Bias in Natural Language Processing ,
pages 17–25, Barcelona, Spain (Online). Association
for Computational Linguistics.
Masahiro Kaneko and Danushka Bollegala. 2022. Un-
masking the mask – evaluating social biases in
masked language models. Proceedings of the AAAI
Conference on Artificial Intelligence , 36(11):11954–
11962.Hannah Rose Kirk, Yennie Jun, Haider Iqbal, Elias Be-
nussi, Filippo V olpin, Frederic A. Dreyer, Aleksandar
Shtedritski, and Yuki M. Asano. 2024. Bias out-of-
the-box: an empirical analysis of intersectional occu-
pational biases in popular generative language mod-
els. In Proceedings of the 35th International Con-
ference on Neural Information Processing Systems ,
NIPS ’21, Red Hook, NY , USA. Curran Associates
Inc.
Aya Kitamura. 2005. Subverting from within: Im-
ages and identities of japanese women. U.S.-Japan
Women’s Journal , (29):37–59.
Laura E. Kuper, Robin Nussbaum, and Brian Mustanski.
2012. Exploring the diversity of gender and sexual
orientation identities in an online sample of trans-
gender individuals. The Journal of Sex Research ,
49(2-3):244–254. PMID: 21797716.
Tracy Lai. 1992. Asian American Women: Not For Sale.
Belmont: Belmont Publishing.
Brian Larson. 2017. Gender as a variable in natural-
language processing: Ethical considerations. In Pro-
ceedings of the First ACL Workshop on Ethics in
Natural Language Processing , pages 1–11, Valencia,
Spain. Association for Computational Linguistics.
R.G. Lee. 1999. Orientals: Asian Americans in Popular
Culture . Asian American history and culture. Temple
University Press.
Michael Lepori. 2020. Unequal representations: Ana-
lyzing intersectional biases in word embeddings us-
ing representational similarity analysis. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics , pages 1720–1728, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Iris Luden, Mario Giulianelli, and Raquel Fernández.
2024. Beyond perplexity: Examining temporal gen-
eralization in large language models via definition
generation. Computational Linguistics in the Nether-
lands Journal , 13:205–232.
Kendall Matsumoto. 2020. Orientalism and the Legacy
of Racialized Sexism: Disparate Representational
Images of Asian and Eurasian Women in American
Culture. Young Scholars in Writing , 17:114–126.
Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and
Simone Teufel. 2019. It’s all in the name: Mitigating
gender bias with name-based counterfactual data sub-
stitution. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
5267–5275, Hong Kong, China. Association for Com-
putational Linguistics.Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measuring
social biases in sentence encoders. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 622–628, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Samuel Museus and Kimberly Truong. 2013. Racism
and sexism in cyberspace: Engaging stereotypes of
asian american women and men to facilitate student
learning and development. About Campus , 18.
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
StereoSet: Measuring stereotypical bias in pretrained
language models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 5356–5371, Online. Association for
Computational Linguistics.
Najm Najm. 2019. Negative stereotypes of arabs: The
western case. The Indian Journal of Social Work ,
80:87.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 1953–1967, Online. As-
sociation for Computational Linguistics.
Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-
chanadikar, Ting-Hao Huang, and Shomir Wilson.
2023. Unmasking nationality bias: A study of human
perception of nationalities in ai-generated articles. In
Proceedings of the 2023 AAAI/ACM Conference on
AI, Ethics, and Society , AIES ’23, page 554–565,
New York, NY , USA. Association for Computing
Machinery.
Anaelia Ovalle, Arjun Subramonian, Vagrant Gautam,
Gilbert Gee, and Kai-Wei Chang. 2023. Factoring the
matrix of domination: A critical review and reimagi-
nation of intersectionality in ai fairness. In Proceed-
ings of the 2023 AAAI/ACM Conference on AI, Ethics,
and Society , AIES ’23, page 496–511, New York, NY ,
USA. Association for Computing Machinery.
Alicia Parrish, Angelica Chen, Nikita Nangia,
Vishakh Padmakumar, Jason Phang, Jana Thompson,
Phu Mon Htut, and Samuel Bowman. 2022. BBQ:
A hand-built bias benchmark for question answering.
InFindings of the Association for Computational
Linguistics: ACL 2022 , pages 2086–2105, Dublin,
Ireland. Association for Computational Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, Anto-
nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,
Siddhant Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Pa-
ganini, Laurent Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-
tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia
Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew Johnson,
Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed Lockhart, Simon Osindero, Laura
Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-
ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling
language models: Methods, analysis & insights from
training gopher.
Alexey Romanov, Maria De-Arteaga, Hanna Wal-
lach, Jennifer Chayes, Christian Borgs, Alexandra
Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,
Anna Rumshisky, and Adam Kalai. 2019. What’s
in a name? Reducing bias in bios without access
to protected attributes. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 4187–4195, Minneapolis, Minnesota.
Association for Computational Linguistics.
K.E. Rosenblum and T.M. Travis. 1996. The Meaning
of Difference: American Constructions of Race, Sex
and Gender, Social Class, and Sexual Orientation .
McGraw-Hill.
Nihar Sahoo, Pranamya Kulkarni, Arif Ahmad, Tanu
Goyal, Narjis Asad, Aparna Garimella, and Pushpak
Bhattacharyya. 2024. IndiBias: A benchmark dataset
to measure social biases in language models for In-
dian context. In Proceedings of the 2024 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (Volume 1: Long Papers) , pages 8786–8806,
Mexico City, Mexico. Association for Computational
Linguistics.
Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-
trin Kirchhoff. 2020. Masked language model scor-
ing. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
2699–2712, Online. Association for Computational
Linguistics.Jack G. Shaheen. 2003. Reel bad arabs: How holly-
wood vilifies a people. The Annals of the American
Academy of Political and Social Science , 588:171–
193.
Eric Michael Smith, Melissa Hall, Melanie Kambadur,
Eleonora Presani, and Adina Williams. 2022. “I’m
sorry to hear that”: Finding new biases in language
models with a holistic descriptor dataset. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 9180–9211,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Eric Michael Smith and Adina Williams. 2021. Hi,
my name is martha: Using names to measure and
mitigate bias in generative dialogue models. ArXiv ,
abs/2109.03300.
Yi Chern Tan and L. Elisa Celis. 2019. Assessing social
and intersectional biases in contextualized word rep-
resentations . Curran Associates Inc., Red Hook, NY ,
USA.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier García,
Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara
Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny
Zhou, Neil Houlsby, and Donald Metzler. 2022. Ul2:
Unifying language learning paradigms. In Interna-
tional Conference on Learning Representations .
Vetle Torvik. 2018. Genni + ethnea for the author-ity
2009 dataset.
Milo Trujillo, Sam Rosenblatt, Guillermo
de Anda Jáuregui, Emily Moog, Briane Paul V .
Samson, Laurent Hébert-Dufresne, and Allison M.
Roth. 2021. When the echo chamber shatters:
Examining the use of community-specific language
post-subreddit ban. In Proceedings of the 5th
Workshop on Online Abuse and Harms (WOAH
2021) , pages 164–178, Online. Association for
Computational Linguistics.
Konstantinos Tzioumis. 2018. Demographic aspects of
first names. Scientific Data , 5:180025.
Alex Wang and Kyunghyun Cho. 2019. BERT has a
mouth, and it must speak: BERT as a Markov ran-
dom field language model. In Proceedings of the
Workshop on Methods for Optimizing and Evaluat-
ing Neural Language Generation , pages 30–36, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
K. Woodward. 1997. Identity and Difference . Culture,
Media and Identities series. SAGE Publications.
Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun
Xu, and Tat-Seng Chua. 2024. A study of implicit
ranking unfairness in large language models.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, DanielSimig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018a. Gender bias
in coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 15–20, New
Orleans, Louisiana. Association for Computational
Linguistics.
Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-
Wei Chang. 2018b. Learning gender-neutral word
embeddings. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 4847–4853, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Robin Zheng. 2016. Why yellow fever isn’t flattering: A
case against racial fetishes. Journal of the American
Philosophical Association , 2(3):400–419.A Stereotypes via APX
Group Associated Descriptors
African Female privileged, radical, ugly, underserved, very loud
Arab Female Muslim, refugee
Arab Male disabled, extremist, moderate, Muslim, terrorist
Baltic Male bald-headed
Chinese Female Confucianist, good at distance running, good at math, quiet, small, Taoist, very short,
very smart, white-collar
Chinese Male dishonest, larger, small, smaller, Taoist, white-collar
Dutch Female hetero, mildly overweight, slim, sweaty
Dutch Male hetero, hipster
English Female AFAB, All-American, AMAB, controlling, elite, Gen-X, little person, morbidly
obese, Mormon, nondisabled, pro-choice, pro-life
English Male addict, All-American, bully, can’t get laid, child predator, conspiracy theorist, coun-
try, Evangelical, felon, huge, massive, Mormon, murderer, pro-life, stutterer
French Male sedentary, snob
Greek Male athletic, Christian, scientist
Hispanic Female brown-skinned, Jehovah’s Witness, pale-skinned, transfeminine, transsexual
Hispanic Male macho
Hungarian Male hungry, hunky
Indian Female affluent, ambitious, differently abled, engaged, expressive, fit, Hindu, homemaker,
illiterate, independent, Jain, married, paralyzed, patient, progressive, shorter, Sikh,
single, spiritual, supportive, underprivileged, underweight, uneducated, unmarried,
urban, well educated
Indian Male atheist, differently abled, dishonest, fit, gangly, Hindu, illiterate, Jain, moneyed,
paralyzed, patient, Sikh, spiritual, Spiritualist, supportive, taller, uneducated, unem-
ployed, unmarried, well-built
Israeli Female Druze, Hasidic, irreligious, Jew, Jewish
Israeli Male complaining, Conservative, convert, Druze, goy, Hasid, Hasidic, irreligious, Jew,
Jewish, religious, right-wing
Japanese Female always cleaning, cute, genderless, happy, have no friends, JAP, ponytailed, scary,
Shintoist, shy, third-gender, very short
Japanese Male macho, scary, Shintoist
Korean Female acne-covered, Cheondoist, skinny
Korean Male Cheondoist, lack social skills, underemployed, unhygenic
Nordic Male demiboy, tall
Slav Male physically fit
Thai Female Buddhist
Thai Male bad driver, Buddhist, effeminate, handicapped, villager
Turkish Female refugee
Turkish Male asylum seeker, refugee, wear a turban
Table 9: All stereotypes and their associated demographic groups from Llama 3 8B, measured taking a 1% one-tailed
significance level of our bias score across the 40 demographic groups for each descriptor.B Generation Prompt
I am creating a dataset of different characters for
an educational game. The dataset must be in JSON
with the fields:
"age" - integer
"personality_traits" - list of strings, length 3
"negative_traits" - list of strings, length 3
"hobbies" - list of strings, length 3
"occupation" - string
"physical_characteristics" - dictionary with keys
height (in ft), hair_colour, eye_colour, skin_colour,
build
"special_move" - string
"socioeconomic_status" - string
"sexual_orientation" - string
"religion" - string
Could you create me profiles for characters
named: { list_of_names }
I would like my dataset to be unbiased and avoid
clichés. Please treat all names the same and avoid
using stereotypes.
Please output only JSON with no additional text.C Feature Analysis in Generation
Feature Eliminated Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o
Overall Accuracy (%) 30.6 32.2 36.1 38.6
religion -11.4% ↓ -6.4%↓ -7.8%↓ -11.4% ↓
eye_colour -2.5% ↓ -4.4%↓ -1.1%↓ -1.9%↓
skin_colour -0.3% ↓ -5.0%↓ -7.5%↓ -4.7%↓
negative_traits 0.0% -2.5% ↓ +0.6% ↑ +0.3% ↑
personality_traits +0.2% ↑ -2.5%↓ +2.0% ↑ -0.3%↓
build +0.2% ↑ -3.0%↓ +1.7% ↑ -0.3%↓
occupation +0.8% ↑ -2.2%↓ +0.6% ↑ -0.5%↓
hobbies +1.1% ↑ -1.4%↓ +0.3% ↑ -0.5%↓
sexual_orientation +1.1% ↑ 0.0% +2.2% ↑ -1.1%↓
socioeconomic_status +1.3% ↑ -0.5%↓ +2.0% ↑ -0.5%↓
height +1.6% ↑ -0.5%↓ -1.1%↓ -1.4%↓
hair_colour +2.2% ↑ -3.9%↓ -1.1%↓ -0.5%↓
age +2.5% ↑ -1.9%↓ +1.4% ↑ -1.1%↓
Table 10: Model accuracies and feature impact on differentiation accuracy across ethnicities. The arrows indicate
whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.
Feature Eliminated Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o
Overall Accuracy (%) 83.3 88.9 91.9 93.9
height -4.7% ↓ -7.0%↓ -7.7%↓ -10.0% ↓
negative_traits -1.6% ↓ +0.5% ↑ -0.2%↓ -0.6%↓
hair_colour -1.4% ↓ -0.3%↓ -1.3%↓ -0.6%↓
eye_colour -1.1% ↓ -0.6%↓ 0.0% -0.3% ↓
occupation -0.8% ↓ +0.5% ↑ 0.0% 0.0%
age -0.5% ↓ 0.0% +0.9% ↑ -0.8%↓
hobbies -0.5% ↓ -0.6%↓ -0.5%↓ -1.1%↓
religion -0.5% ↓ +1.4% ↑ -0.5%↓ +0.5% ↑
personality_traits 0.0% 0.0% -0.2% ↓ -0.6%↓
sexual_orientation 0.0% -0.3% ↓ 0.0% -2.5% ↓
socioeconomic_status +0.3% ↑ +1.1% ↑ +0.3% ↑ -0.3%↓
skin_colour +0.3% ↑ +0.3% ↑ -0.2%↓ -0.3%↓
build +1.7% ↑ -0.8%↓ -1.9%↓ -1.1%↓
Table 11: Model accuracies and feature impact on differentiation accuracy across gender. The arrows indicate
whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.D Top Words in Generation
Feature Word Associated Groups
negative_traitsarrogant
manipulative
perfectionist
pessimistic
selfishBaltic Female, English Male
Japanese Female, Chinese Female, Baltic Female
Slav Female, French Male
English Male, African Male
Israeli Male
hobbiesyoga
painting
dancing
playing pianoIndian Female, Thai Female, African Male, Arab Male,
English Male, Baltic Male
Arab Female, African Male
Nordic Female
Chinese Female
occupationpolitician
rabbi
freelance writer
social worker
therapist
event planner
nurse
engineer
counselor
software engineerTurkish Male
Israeli Male
German Female
Arab Female
French Female
Israeli Female
African Female
Arab Male
Israeli Female
Korean Male
socioeconomic_statusupper middle class
lower class
upper class
lower middle class
working class
middle classAfrican Female
Nordic Male, Hispanic Male
Baltic Female, Greek Female, Indian Female,
Greek Male
English Female
Italian Female
Israeli Male
sexual_orientationbisexual
pansexual
asexual
homosexualGreek Male, Hispanic Male, Hispanic Female,
German Male, Hungarian Male
French Female, Indian Male, Israeli Male
Japanese Male
English Female
religionjewish
hindu
shinto
buddhist
muslimIsraeli Male, Israeli Female
Indian Male, Indian Female
Japanese Female, Japanese Male
Thai Female, Thai Male
Arab Male, Turkish Male
hair_colourblack
dark brown
curly brownEnglish Female, Baltic Male, Italian Female,
Dutch Male, Slav Male, African Female, Nordic Male
German Female
Greek Female
skin_colourfair
darkArab Male, African Male, Thai Male, Indian Male
French Female, Indian Female, Baltic Female,
African Female, Italian Female, Baltic Male
Table 12: Top 10 differentiating words across all groups for selected features in Llama 3 70B Instruct.Feature Word Associated Groups
negative_traitsshy
impulsive
aloof
stubborn
disorganized
stern
rigid
perfectionist
overcriticalJapanese Female
Japanese Male, Italian Male
Slav Male
English Male
Nordic Female
German Male
German Male
Slav Female
Thai Female
hobbiescalligraphy
cycling
painting
yoga
cooking
soccer
origamiChinese Female, Japanese Male, Chinese Male
Dutch Male
Italian Female
French Female
Thai Female, Italian Male
African Male
Japanese Female
occupationchef
research scientist
data scientist
software developer
graphic designer
historian
mechanical engineer
professor
journalistThai Female, Italian Male
Chinese Male
Chinese Male
Baltic Male
Nordic Female
German Male
Nordic Male
Indian Male
Baltic Female
socioeconomic_statusmiddle-income
upper middle class
middle
middle-classSlav Female, German Male
Japanese Female, Korean Female, German Female
Italian Male, Nordic Male, Greek Male
Turkish Male, Indian Male
sexual_orientationlesbian
gay
asexual
bisexualSlav Female, Dutch Male, Turkish Male, Israeli Fem.
French Male, African Female
Japanese Female
Japanese Male, Italian Female
religionjewish
hindu
muslim
shinto
catholic
buddhist
christianIsraeli Female, Israeli Male
Indian Female, Indian Male
Arab Male
Japanese Male
Italian Male
Thai Female, Thai Male
African Female
hair_colourblonde
black
brownNordic Female, Turkish Male, Arab Male, Greek Male
German Male, Dutch Male, Nordic Male, Baltic Male
Italian Female, Japanese Male
skin_colourfair
light tan
dark
oliveThai Female, Arab Male, Thai Male
Japanese Male
African Female, African Male
French Female, English Male, Hungarian Female,
Italian Male
Table 13: Top 10 differentiating words across all groups for selected features in GPT 4o.