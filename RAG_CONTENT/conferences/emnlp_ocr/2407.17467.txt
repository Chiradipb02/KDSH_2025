CMR Scaling Law: Predicting Critical Mixture Ratios for Continual
Pre-training of Language Models
Jiawei Gu§,†,1,2, Zacc Yang†,2, Chuanghao Ding2,3, Rui Zhao2, Fei Tan∗,2
1Sun Yat-sen University
2SenseTime Research
3Nanjing University
1kuvvius@gmail.com2{yangzacc, zhaorui, tanfei}@sensetime.com3ch777.ding@smail.nju.edu.cn
Abstract
Large Language Models (LLMs) excel in di-
verse tasks but often underperform in spe-
cialized fields due to limited domain-specific
or proprietary corpus. Continual pre-training
(CPT) enhances LLM capabilities by imbu-
ing new domain-specific or proprietary knowl-
edge while replaying general corpus to prevent
catastrophic forgetting. The data mixture ratio
of general corpus and domain-specific corpus,
however, has been chosen heuristically, lead-
ing to sub-optimal training efficiency in prac-
tice. In this context, we attempt to re-visit the
scaling behavior of LLMs under the hood of
CPT, and discover a power-law relationship be-
tween loss, mixture ratio, and training tokens
scale. We formalize the trade-off between gen-
eral and domain-specific capabilities, leading
to a well-defined Critical Mixture Ratio (CMR)
of general and domain data. By striking the
balance, CMR maintains the model’s general
ability and achieves the desired domain transfer,
ensuring the highest utilization of available re-
sources. Considering the balance between effi-
ciency and effectiveness, CMR can be regarded
as the optimal mixture ratio. Through extensive
experiments, we ascertain the predictability of
CMR, propose CMR scaling law and have sub-
stantiated its generalization. These findings
offer practical guidelines for optimizing LLM
training in specialized domains, ensuring both
general and domain-specific performance while
efficiently managing training resources.
1 Introduction
Large Language Models (LLMs) exhibit versatile
abilities, including question answering, translation,
summarization, role-playing, etc. (Brown et al.,
2020; Touvron et al., 2023a,b; Li et al., 2023; Lu
et al., 2023). Their performance, however, may
§Work was done during internship at SenseTime Research
†Equal contribution
*Corresponding authordegrade in specific domains due to limited corre-
sponding pre-training data. To enhance LLMs’ abil-
ities in specialized areas and avoid the enormous
cost of re-training, a popular approach is Continual
Pre-Training (CPT) (Colombo et al., 2024; Chen
et al., 2023; Yıldız et al., 2024; Luo et al., 2023).
This approaches are likely to equip LLMs with new
domain-related capabilities without much general
performance penalty.
Although CPT has been proven effective on
multiple domains such as code (Li et al., 2023;
Lei et al., 2024), law (Colombo et al., 2024) and
medicine (Chen et al., 2023), the interplay among
loss prediction and its scaling behavior with model
size, and the number of training tokens is yet to
be fully explored. Additionally, the composition
of continual pre-training data is simply set up in a
heuristic manner (Colombo et al., 2024; Chen et al.,
2023), far from being principled. An inappropri-
ate mixture ratio can lead to inefficient training
(requiring excessive computation to adapt to spe-
cific domains) or insufficient training (failing to
adequately reduce domain-specific loss). In light
of this, three question hurdles we need to cross are
as follows:
Does the optimal data mixture ratio exist for
CPT? If so, how does it evolve with model scale
or training token volume? Are there any involved
simple yet principled laws?
Currently, several studies examine the scaling
laws associated with different data mixture ratios.
For instance, Ye et al. (2024) investigate how data
mixtures shape scaling laws in the pre-training
phase from the ground up, while Que et al. (2024)
seek to pinpoint the optimal data mixture ratio in
CPT, but overlook its crucial connection with the
essential trade-off between general and domain loss
in CPT.
Therefore, to strengthen our understanding about
CPT and guide the experiments in the future, we
attempt to address these questions with empiricalarXiv:2407.17467v2  [cs.CL]  7 Oct 2024studies on CPT of LLMs. Specifically, we pre-
train several LLMs with different model sizes from
scratch and perform CPT on downstream domains
(Finance andAcademic Papers ) with different data-
mixture ratios. Our main contributions can be sum-
marized as follows:
Formalization of the Trade-Off in CPT . We
formalize the balance between domain-specific
and general abilities during CPT by introducing
the concept of feasible mixture ratios. CPT un-
der feasible mixture ratios maintains performance
on general data while enhancing performance on
domain-specific data. We identify the maximum
feasible mixture ratio as the Critical Mixture Ra-
tio (CMR) , and regard it as the optimal mixture
ratio by our definition.
Predictability of CMR . Through extensive ex-
periments, we identify a power-law relationship
between loss and both data-mixture ratio and train-
ing tokens. As such, we propose CMR scaling law
to predict the best mixture ratio by scaling training
token volume, which appears to be generalizable
based on our findings.
Significance of CMR Scaling Law . CMR scal-
ing law for CPT is crucial for efficient domain
transfer for LLMs. This law allows us to deter-
mine the most efficient training configuration by
predicting CMR using limited data and compute
resources. The finding provides insights into the
dynamics of CPT and may offer practical guide-
lines for optimizing LLM training in specialized
domains.
2 Key Results
We train a series of LLMs with multiple mixture
ratios of domain-specific data and general data to
analyse the scaling behaviour in CPT. The method
is detailed in § 3.2. Based on our experimental
setup, we summarize the key results as follows:
1.The trade-off between two goals of CPT (Def-
inition 1) suggests that, given a model of cer-
tain size, there exists a set of feasible mixture
ratios (Definition 2) that achieve the goals un-
der specific training data constraints.
2.Basically, general losses in CPT increase
initially before decreasing, whereas domain
losses tend to decrease. The relationships be-
tween loss and mixture ratio, as well as train-
ing volume, fit well with a power-law form,
allowing for loss prediction under different
mixture ratios and training tokens.3.Using the loss prediction by mixture ratio
and training volume, we can predict the CMR
(Definition 3) with CMR scaling law.
•Given the maximum amount of train-
ing tokens, experiments in Figure 1 and
predicted results in Figure 5 both show
that CMR goes up with increasing model
scale: from 29.8%for the 460M model
to34.9%for the 940M model.
•CMR depends on the similarity between
the target domain and the general do-
main. The smaller the distribution gap
between the two, the larger the CMR.
Because Academic Papers constitute a
larger portion of the general data than
Finance , the pre-trained 460M model
tends to show a higher CMR on Aca-
demic Papers (36.7%) compared to Fi-
nance (29.8%) during CPT, as illustrated
in Figure 5 and Figure 6.
3 Background and Methods
The scaling law in the pre-training stage has been
widely studied. In this work, we simplify the form
of scaling law as much as possible, which is essen-
tially consistent with previous works in § 6.
In this section, we will elaborate on the three
main concepts involved in this work, including ob-
jective of CPT (Definition 1), feasible mixture ra-
tio (Definition 2) and CMR (Definition 3). Then,
we describe our experiment setups, including data
preparation, experiment procedures and evaluation.
3.1 Continual Pre-training on Mixed Dataset
Definition 1. Objective of CPT
Given the pre-trained LLM MSof model size S,
general dataset Dgen, and domain-specific dataset
Ddom, we continually pre-train MSon a mixed
dataset DR, where the mixture ratio of the domain-
specific data is R, with R∈[0,1]. The mixed
dataset DRis denoted as DR=Ddom+Dgenand
R=|Ddom|
(|Dgen|+|Ddom|).
We define Lgen/dom (MS)as the domain or
general loss of the model MS. We denote
LCPT
gen/dom (MS,DR, T)as the domain/general loss
of model MSafter CPT on dataset DRwith train-
ing token volume T. Note that all losses mentioned
above are validation losses. The goals for CPT are
formalized as follows:
1. By the end of training, the general loss is
supposed to either reach plateau or head downward∆Domain	Loss
∆General	Loss
Domain-General	Loss	Curve
Model	SizesTraining	trajectoryTraining	trajectory
∆General	Loss
∆Domain	LossFigure 1: Follow the direction of the training trajectory to track the trend of the curve. Each bunch of lines
represents a model size scale: {3.1B,1.6B,940M ,460M}and each group of line colors represents the mixture
ratios{1/8,1/4,1/3,1/2}from dark to light. In order to better display the trend, we have omitted proportions
greater than 1/2. The yellow dashed lines point horizontally, indicating the corresponding ratios where
dL∆gen/dL∆domclosed to 0. The third set of lines of model size 940M, which has been zoomed in and depicted on
the right side, showing the trend of the training curve more apparently. All horizontal and vertical cross-sections of
the 3D diagram on the left side are detailed in the Appendix E.
(within a certain tolerance ϵ >= 0):
LCPT
gen(MS,DR, Tmax)≤ L gen(MS) +ϵ. (1)
2. Domain-specific loss should decline largely:
LCPT
dom(MS,DR, Tmax)<Ldom(MS). (2)
The increase in Tfrom 0toTmaxcorresponds to
the progression of the training trajectory. To better
integrate these two aspects, we adopt the method
of Lagrange multipliers (Rockafellar, 1993). The
loss function F(·)for the whole objective of CPT
is the Lagrangian as follows:
F(S, R, T, λ ) =LCPT
dom(MS,DR, T)
+λ(LCPT
gen(MS,DR, T)
− L gen(MS)−ϵ),(3)
where λis the Lagrange multiplier used to enforce
the constraint on the general loss while minimizing
the domain-specific loss. In practice, λgoverns
the importance of two target dimensions in CPT.
F(S, R, T, λ )is the whole objective function.
Under resource constraints, the optimal training
configuration should minimize Ldomwhile satisfy-
ing the constraint on Lgen, which involves findingthe optimal S,R, and Tby solving the following
optimization problem:
S∗, R∗, T∗=argminMS,R,TF(S, R, T, λ ),
s.t.(
LCPT
gen(MS, R, T max)≤ L gen(MS) +ϵ,
R≥0, T≥0, λ≥0.
(4)
Definition 2. Feasible Mixture Ratio
Given fixed model size S, the optimization
problem in Equation (3) can be boiled down to
F(R, T, λ ). We first introduce a mixture ratio
setA: according to the first constraint of Defi-
nition 1, under a certain tolerance ϵfor the de-
terioration in the final general performance, we
can choose a set of mixture ratios Asatisfying
A={R| LCPT
gen(MS, R, T max)≤ L gen(MS) +ϵ}.
Ratios in Athat align with our CPT objective
are considered as feasible mixture ratios, denoted
as the set F. A detailed definition transforma-
tion is presented in Appendix B.2, and here we
directly provide the formula and the results of
derivation: within the feasible mixture ratios, there
exists a point T0over the training trajectory of
CPT. As CPT proceeds with T > T 0, we have
F={R| ∃T0∈(0, Tmax) :∂F
∂T≤0, R∈A}.
An equivalent condition of defining Fcan bederived as:
F={R| ∃T0∈[0, Tmax]
:∂L∆gen(R, T)
∂L∆dom(R, T)
R=−1
λ<0, R∈A}.
(5)
For simplicity, we have defined L∆dom=LCPT
dom−
LdomandL∆gen=LCPT
gen− L gen.
Visualization As shown in Figure 1, the training
curves meeting the objective of CPT are marked
with yellow dotted arrows, indicating the curves
show a downward trend as training proceeds. The
domain loss continuously decreases ( L∆dom↓) and
the general loss is bounded ( dL∆gen/dL∆dom→
0) along the training trajectory until the ends of
training. This visual representation effectively il-
lustrates the behavior described by Equation (5),
demonstrating the trade-off relationship between
the domain loss and the general loss during training.
The specific derivation and the interpretation of the
slope for Figure 1 is detailed in Appendix B.2.
Definition 3. Critical Mixture Ratio (CMR)
Given limited compute resources and fixed
model size, we hope that the language model
can digest domain knowledge more efficiently by
achieving the objective as described in Definition 1.
Therefore, we define the maximum among feasible
mixture ratios as the Critical Mixture Ratio (CMR)
R∗= max {R|R∈F}.
The rationale is straightforward: if the ratio is
less than CMR, the domain data is not sufficiently
utilized in CPT; otherwise, the expected objective
can’t be achieved, which is manifested as a intoler-
able increase in general loss, leading to degradation
in general ability. Thus, we argue that the CMR
is the most suitable ratio for CPT due to the ideal
balance of two sides.
3.2 Method
Data preparation Our general pre-training data
is composed of corpora from Chinese, English, and
code. The Chinese corpus and English corpus both
include articles from encyclopedia, books, news,
papers and social media sites. The code corpus is
a subset sampled from StarCoder (Li et al., 2023).
The general pre-training dataset comprises a total
of 220 billion tokens. The proportions of Chinese,
English, and code are roughly 44% : 36% : 20% .
We meticulously craft two specific domain
datasets for CPT: Finance andAcademic Papers .The Finance dataset include financial news, finan-
cial policies and regulations, company announce-
ments and research reports from securities and fund
companies. The Academic Papers exclusively in-
clude papers from Arxiv. Each of the datasets con-
tains at least 20 billion tokens, which is sufficient
for our CPT.
Unless stated explicitly, all the following results
are based on experiments with Finance . The results
of CPT on Academic Papers are reported in § 5.3.
LLM Architecture The involved LLMs in this
study have the same architecture as Llama se-
ries (Touvron et al., 2023a,b) with standard multi-
head attention. The number of parameters ranges
from 460M to 3.1B. The architecture is detailed in
Table 1 of Appendix.
Experiment Setup We split the general pre-
training dataset into two subsets: a 200B-token
general dataset for general pre-training and a 20B-
token general dataset for CPT.
In the pre-training stage, we pre-train the LLMs
from scratch with 200B-token general dataset with
a max learning rate of 3e-4, a batch size of 512,
and a sequence length of 4096. The training step
is 100,000 for each LLM. In the CPT stage, we
train each LLM for another 10,000 steps (20 bil-
lion tokens) with a max learning rate of 3e-5 and
warmup-constant LR schedule, on a mixture of the
20B-token general dataset and a domain dataset
with different mixture ratios.
Evaluation Scaling laws emphasize the pre-
dictability of pre-training loss (Kaplan et al., 2020;
Hoffmann et al., 2022; Gao et al., 2023; Hernan-
dez et al., 2021), which is a widely-used perfor-
mance indicator. Recent studies (Du et al., 2024;
Yuan et al., 2023) highlight that pre-training loss
is highly correlated with downstream task perfor-
mance. Therefore, we use the pre-training loss on
the validation set to measure the model’s capability
of general or domain-specific task during the CPT
process. In addition, we use Mean Squared Error
(MSE) and R-square ( R2) to measure the quality of
the fitting, which provides a clear and interpretable
analysis of the errors.
4 Does the Critical Mixture Ratio Exist?
—— Yes, the CMR does exist.
A larger mixture ratio implies a higher proportion
of domain-specific data in the training set, resultingin a lower domain loss. However, due to the poten-
tial catastrophic forgetting of domain transfer, it is
essential to ensure that the loss in the new domain
continues to decrease while the original capabili-
ties of LLMs are preserved and not compromised
during CPT. Consequently, a higher mixture ratio is
not always best. This raises an important question:
does a Critical Mixture Ratio (CMR) exist that can
balance these two goals of CPT in Definition 1
effectively and efficiently?
Figure 1 (left) demonstrates that for models of
various sizes, there is at least one curve at a specific
ratio that shows a downward trend, highlighted by
yellow dotted arrows. This indicates the presence
of feasible mixture ratios that align with our CPT
objective. On the other hand, larger models tend
to have bigger feasible mixture ratios set F(more
curves with yellow dotted arrows). For curves that
meet the objective of CPT, a higher ratio is prefer-
able, as it incorporates more domain knowledge
while optimizing training efficiency within the tol-
erance of decline in general capacity. Therefore,
the critical mixture ratio is defined as the highest
proportion among these satisfactory curves, repre-
senting the optimal ratio for the given model size
and limited training token volume.
If feasible ratios exist, we can conclude that
CMR is also supposed to exist. Fundamentally, the
existence of CMR arises from the trade-off between
general and domain-specific capabilities, as well
as the limited data and computing resources. Ac-
cording to definition 3, the CMR is present across
models of different scales, as shown in Figure 1.
This figure illustrates the existence of CMR as the
maximum value within the feasible set. However,
the precise value of CMR can not be determined
from the figure, as it requires extensive experiments
with different mixture ratios. The estimation of
CMRs is discussed in 5.3 and plotted in Figure 5.
To look closely, we enlarged the longitudinal
section of M940Min the 3D graph in Figure 1 and
placed it on the right side. It can be seen that as
the mixture ratio increases, the curve continues to
rise until the loss in the general domain exceeds
our tolerance. The potentially controversial issue is
that the downward trend in one-third of the curves
is as clear as in the rest. The reason why it is
feasible curve here can be found in Appendix B.2.
Although it is not easily noticeable, there are indeed
points on this curve where the slope is less than 0.
Training	trajectory
∆General	Loss
∆General	Loss
∆Domain	LossFigure 2: Follow the direction of the training trajectory
to track the trend of the curve. The L∆genandL∆dom
loss functions for the models at mixture ratios of 1/4
and1/3are illustrated.
Findings From another perspective, we plot the
loss curves of the models under the same mixture
ratio as shown in Figure 2. When the mixture ratio
is1/4, all models can achieve the training objec-
tive of CPT. However, at a 1/3mixture ratio, only
M940M,M1.6BandM3.1Bachieve the CPT goal.
This indicates that CMR for M940Mis around 1/4
within the scope of our training token volumes,
while the CMRs for M940M,M1.6BandM3.1Bare
at least 1/3. In other words, CMRs slightly in-
crease with model size, suggesting that larger
models can accommodate a higher proportion
of domain data. We also further this finding by
taking more cross-sections of Figure 1 (left) in Ap-
pendix E and the predicted CMR in following § 5.
This phenomenon can be explained by the mod-
els’ ability to consume domain knowledge. As the
proportion of domain-specific data increases, the
knowledge that the model needs to learn also in-
creases. LLMs with smaller size struggle to absorb
much of domain knowledge while preserving thegeneral knowledge, leading to a degradation in their
original general performance. In contrast, models
with larger sizes can accommodate more knowl-
edge with more parameters, thereby maintaining
better performance.
5 Is CMR Predictable?
—— Yes, the CMR can be predicted.
The existence of CMR indicates that in the pro-
cess of CPT, we may explore the CMR scaling
law to seek the best mixture ratio under resource
constraints and domain data limitations, thereby
optimizing training effectiveness and efficiency. In
other words, the next question to answer is whether
we can predict the CMR for model Msgiven a
maximum amount of continuation training token
volume, Tmax.
To this end, two basic prerequisites must be met:
predicting losses for different mixture ratio and pre-
dicting losses for different training token volume.
In this section, we will demonstrate that these two
prerequisites have been satisfied separately in § 5.1
and § 5.2, and finally detail the scaling law to pre-
dict CMR in § 5.3. To keep notations simple, we
omit fixed variables in the loss function ( Ldom/gen
andL∆dom/∆gen) in this following.
5.1 Predicting Losses of Mixture Ratio
Predicting the general and domain loss is closely
related to understanding the scaling behavior in the
CPT stage. We study the scaling behavior of losses
atT=Tmax. In addition, since scaling law aims
to fit data points, their parametric forms should be
intrinsically related to the observed trends in the
data points. Based on previous works (Kaplan et al.,
2020; Hoffmann et al., 2022) and data trends we
observed, we proposed the simplified expression
L(R)as a power-law form of
L(R) =α·Rs+β,
where αis a coefficient, sis the exponent, and βis
the bias.
As shown in Figure 3, domain loss gradually de-
creases with the increase of the mixture ratio, while
general loss remains almost unchanged initially and
then begins to rise. After fitting the general loss
and domain loss separately for different mixture
ratios R(non-endpoint values, R∈(0,1)), we
make predictions on new ratios. As shown in Fig-
ure 3, the predicted values align closely with the
fitted curve. Notably, the predictions demonstrate
Mix	Ratio
Domain	Loss
Mixture	Ratio
GeneralLoss
Figure 3: The upper figure shows the fitting curve of
domain loss Ldomwith the change of mixture ratio R,
and the lower figure shows the fitting curve of general
lossLgen. The solid circles ( •) represent real losses, and
the stars ( ★) represent the predicted losses.
high accuracy, with error values within 0.05% as
presented in Table 2.
Given the predicted Ldom(R)andLgen(R)un-
der different mixture ratios, we can obtain a range
of mixture ratios that fulfil the tolerance limit ϵ,
denoted as A, according to Equation 3. In the ob-
jective of CPT we set, ϵ= 0.05.
5.2 Predicting Losses of Training Tokens
Previous works (Kaplan et al., 2020; Hoffmann
et al., 2022) have shown that the model size Sand
the volume of training tokens Tcan be used to fit
the power law of loss. However, our work differs
in two key aspects. First, we model the change
of loss L∆dom/∆gen(T)rather than the loss itself.
Second, due to the phenomenon of general loss
initially increasing and then decreasing as shown
in Figure 4, we leverage a two-term polynomial
function for better fitting. According to Equation 9
in Appendix B.2, the loss for CPT training tokens
Tis formulated as follows:Trainingdatavolume	(in	units	of	200M))
Extrapolation	of	T	with	power	law	@	1.6B	Model
Figure 4: The figure shows the general loss of M1.6B
fitting and extrapolating at four distinct mixture ratios:
{1/8,1/4,1/3,1/2}. As the ratio increases, the curve
gradually rises when training data volume increases.
(
L∆dom(T) =α1·Ts1+β1,
L∆gen(T) =α2·Ts2+α3·Ts3+β2.(6)
where α1,α2,α3,β1,β2,s1,s2, and s3are learn-
able parameters. Our results demonstrate that the
form (6)exhibits high fitting accuracy with low
MSE and high R2in Table 3 and Figure 4.
5.3 Predicting CMR
According to the definition of feasible mixture ra-
tios in Definition 2 and the method for determining
the set Fin Appendix B.2, where F⊂A, andA
is obtained by predicting losses for any mixture
ratio in § 5.1, we can establish a relationship be-
tween training token volume Tand the feasible
mixture ratios by the fitting laws in § 5.2. Overall,
based on the parameters provided in Formula 6,
the critical solution T0is obtained for a specific
mixture ratio R0denoted as (derivation detailed in
Appendix B.2):
T0=
"
−α1·s1
λα2·s2
1 +α3·s3
α2·s2Ts3−s2
0−1# 1
s2−s1
(7)
When T0is less than the given maximum training
token volume Tmax, we can conclude that the cur-
rent ratio R0is a feasible mixture ratio. Conversely,
CMRScalinglawwithT
T	(in	units	of	0.2B)
Predicted	CMRFigure 5: We can use the CMR scaling laws to predict
CMRs under fixed model size S, and are extrapolated
toT= 250 , which is equivalent to a training volume of
500B tokens.
ifT0exceeds Tmax, then R0is not a feasible mix-
ture ratio. If T0is equal to Tmax, then R0is the
critical ratio. We propose the following CMR scal-
ing law:
RCMR=α4·Ts4+β3. (8)
The fitting curves are showed in Figure 5. In our
experiments, Tmax is20B tokens, which corre-
sponds to a value of T= 100 in the figure. There-
fore, for four models of different scales, their pre-
dicted CMR are 29.8%,34.9%,41.4%and47.8%
forM460M, M940M, M1.6B, M3.1B, respectively.
Generalization In order to verify whether the
CMR scaling law can be generalized, we exper-
iment on another domain Academic Papers with
different mixture ratios. In this generalization ex-
periment, we only conduct CPT on the 460M-
sized model with Academic Papers data propor-
tions set to {1/8,1/4,1/2,3/4,1/3}respectively.
All other settings were kept consistent with Fi-
nance . As shown in Figure 7, the trade-off of
CPT still exist in this domain, and thus there exists
a CMR. Furthermore, the CMR scaling law still
work, which can observed in Figure 6. The pre-
dicted CMR for Academic Papers is36.7%, given
the maximum training token volume Tmax= 100 .
Finding Comparing CMR predictions in two dif-
ferent domains, we find that the pre-trained model
of the same size (460M) shows a higher CMR for
Academic Papers (36.7%) compared to Finance
(29.8%) during CPT. As illustrated in § 3.2, weFitting	Curve	of	Academic	Papers
T	(in	units	of	0.2B))
Mixture	RatioFigure 6: With a fixed model size S= 460 M, using
the CMR scaling law can be extrapolated to T= 250
and more. We can use the CMR scaling laws to predict
CMR for Academic Papers in the CPT of M460M. When
T=Tmax= 100 , the value of Ris36.7%, regarded as
the CMR.
performed a statistical analysis on the general pre-
training dataset, finding that data from the Aca-
demic Papers domain accounts for about 10%,
while the Finance corpus is less than that. In
other words, the smaller the distribution gap
between the target and general domains, the
larger the CMR. This observation aligns with re-
lated research findings (Ke et al., 2022, 2023) and
can be attributed to the difficulty of domain adapta-
tion. When the distribution gap between the target
and general domains is smaller, domain adaptation
becomes easier, reducing the risk of degradation
of general performance during CPT, even with a
higher proportion of domain-specific data.
Open Discussion As showed in Figs. 5 and 6,
the larger the Tmax, the wider the range of feasible
mixture ratios. Therefore, it seems that when Tmax
tends to be infinity (the amount of data available for
continued training is infinite, and computational re-
sources are unlimited completely), the range of fea-
sible mixture ratios would approach (0,1), leading
CMR approaching 1. In this sense, each curve of
CPT trajectory will show an expected convergence
trend of objective, provided that there is enough T
to allow it to develop.
Moreover, we find out that the solution of T0
in Definition 2 approaches the inflexion point of
L∆gen(T)in Figure 4 and Figure 7, when λ→
+∞, which we used for solving equations 14
ranges from 100to7000 . The reason is likely to be
that, the change in the general loss is much smallerthan the change in domain, and λin the objective
function of CPT needs to be very large to amplify
such subtle changes within the tolerance of con-
straints. In addition, during the training process,
the decreasing trend of domain loss has always
been present, but there are obvious inflection points
in the of general loss curve (rise first and then fall).
That is to say, by only locating the inflection points
on the general loss curve and finding this distance
to the max training token volume, we can roughly
estimate how far away we are from CMR at current
ratio.
6 Related Work
6.1 Continual Pre-training
Continual Pre-Training (CPT) aims to perpetually
pre-train large language models (LLMs), allowing
them to adapt to new domains and reducing the
high costs associated with training models from
scratch for specialized tasks (Yıldız et al., 2024).
CPT can be employed to tailor LLMs for specific
fields, such as code (Lei et al., 2024; Li et al., 2023),
medicine (Chen et al., 2023), law (Colombo et al.,
2024), and science. By using an appropriate mix-
ture of data from various domains (Gururangan
et al., 2020), CPT not only enhances downstream
performance but also mitigates the issue of catas-
trophic forgetting (Zhang et al., 2024), which is
prevalent in all forms of post-training (Cossu et al.,
2022; Luo et al., 2023).
Many recent studies on domain-specific
LLMs (Chen et al., 2023; Colombo et al., 2024)
adopt replay strategies (mixing general and
domain-specific data) to concern about general
losses during CPT. Other works (Que et al.,
2024; Guo et al., 2024; Ge et al., 2024) have
also noted challenges with maintaining general
abilities, which aligns with our findings. For
example, Guo et al. (2024) identified a Stability
Gap, where performance initially drops during
CPT and then gradually recovers, leading to
inefficient pre-training and potential forgetting
of general knowledge. In our work, we focus on
the trade-off between general and domain-specific
performance (losses) during CPT. To facilitate
clearer experimental observations, we chose
to restrict CPT to a single domain and did not
explore the more complex setting of training across
multiple domains.△GeneralLoss
△GeneralLoss
△GeneralLossT	(in	units	of	0.2B)T	(in	units	of	0.2B)T	(in	units	of	0.2B)Figure 7: The figure shows the general loss of M460Mfitting and extrapolating at three distinct mixture ratios:
{1/8,1/4,1/3}with CPT on Academic Papers .
6.2 Scaling Law
Numerous studies (Hestness et al., 2017; Henighan
et al., 2020; Bahri et al., 2021; Kaplan et al.,
2020; Hoffmann et al., 2022; Yao and Wang, 2023)
demonstrate a power-law relationship between per-
formance and the increase in both the number of
parameters and the size of the training data. These
relationships are crucial for large language models
(LLMs), being of paramount importance in various
stages such as pre-training (Kaplan et al., 2020;
Hoffmann et al., 2022; Ye et al., 2024), super-
vised fine-tuning (SFT) (Hernandez et al., 2021;
Lin et al., 2024), etc. Recently, researchers de-
scribe scaling laws from various different perspec-
tives (Pandey, 2024; Ye et al., 2024). The form of
the scaling law used in this papers is consistent with
Hoffmann et al. (2022), L=E+A
Sα+B
Tβ, where
{E, A, B, α, β }are fitting parameters. However,
we express in an simpler and more appropriate way
for our demonstrations.
6.3 Data Mixture Scaling Law
Several studies have examined the scaling laws
associated with various data mixture ratios. For
instance, Ye et al. (2024) investigate how different
data mixtures influence scaling laws during the pre-
training phase. However, their proposed laws are
not applicable to CPT. Another study by Que et al.
(2024) aims to identify the optimal data mixture
ratio using the D-CPT law. Their method focuses
solely on minimizing domain loss by fixing model
sizes and training token volume, thereby neglecting
the trade-off between general loss and domain loss,
which is critical in CPT.
7 Conclusion
In this work, we investigated the scaling behavior
of LLMs under Continual Pre-Training (CPT) toaddress the limitations of domain-specific perfor-
mance. We provided a clear definition of Critical
Mixture Ratio (CMR) for optimizing the mixture
ratio of general and domain-specific data. Our ex-
periments revealed a power-law relationship be-
tween loss, mixture ratio, and training data size,
allowing us to predict the CMR efficiently. These
findings may offer practical guidelines for opti-
mizing LLM training, helping to balance general
and domain-specific performance while minimiz-
ing resource consumption. Additionally, our study
suggests that understanding the CPT process and
scaling laws could be valuable for future research
aimed at enhancing LLM capabilities in specialized
fields.
8 Limitations
Computational Constraints We experimented
with model sizes range from 400M to 3.1B. How-
ever, the largest model in our experiments is still
relatively small among contemporary LLMs. It
may lead to inaccuracy in estimation of model size
scaling.
Limited Domains In this work, we conducted
continual pre-training only on two specific domains
(finance and academic papers) respectively. Al-
though we have draw some useful conclusions from
the experimental results, experiments with more do-
mains are expected to provide more refined results
and likely to bring some new insights.
CMR scaling law with model size The CMR
scaling in this work can only predict the CMR of
a fixed model size. We have not explored how to
predict CMR of large models with experiments on
small models. An possible method is that first we
extrapolate all the losses of small models to large
models with model size scaling law, and use CMRscaling law to predict the CMR of the large model.
We left it as a future work to predict CMR by lever-
aging multiple scaling laws with less computational
efforts.
Downstream Evaluation As stated in § 3.2, our
primary focus was on establishing and validating
the CMR scaling law through loss metrics, which is
widely used and highly correlated with downstream
task performance. However, this study did not di-
rectly evaluate performance on downstream tasks.
Including downstream task performance could pro-
vide a more intuitive understanding of the observed
trends.
References
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon
Lee, and Utkarsh Sharma. 2021. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701 .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers.Preprint , arXiv:2005.14165.
Zeming Chen, Alejandro Hernández Cano, Angelika
Romanou, Antoine Bonnet, Kyle Matoba, Francesco
Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf,
Amirkeivan Mohtashami, et al. 2023. Meditron-70b:
Scaling medical pretraining for large language mod-
els.arXiv preprint arXiv:2311.16079 .
Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf,
Dominic Culver, Rui Melo, Caio Corro, Andre FT
Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia
Morgado, et al. 2024. Saullm-7b: A pioneer-
ing large language model for law. arXiv preprint
arXiv:2403.03883 .
Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lu-
cia Passaro, Vincenzo Lomonaco, and Davide Bac-
ciu. 2022. Continual pre-training mitigates for-
getting in language and vision. arXiv preprint
arXiv:2205.09357 .
Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang.
2024. Understanding emergent abilities of language
models from the loss perspective. arXiv preprint
arXiv:2403.15796 .
Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-
ing laws for reward model overoptimization. In In-
ternational Conference on Machine Learning , pages
10835–10866. PMLR.Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and
Bolin Ding. 2024. Data mixing made efficient: A
bivariate scaling law for language model pretraining.
arXiv preprint arXiv:2405.14908 .
Yiduo Guo, Jie Fu, Huishuai Zhang, Dongyan Zhao, and
Yikang Shen. 2024. Efficient continual pre-training
by mitigating the stability gap. arXiv preprint
arXiv:2406.14833 .
Suchin Gururangan, Ana Marasovi ´c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages
8342–8360, Online. Association for Computational
Linguistics.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,
Christopher Hesse, Jacob Jackson, Heewoo Jun,
Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.
2020. Scaling laws for autoregressive generative
modeling. arXiv preprint arXiv:2010.14701 .
Danny Hernandez, Jared Kaplan, Tom Henighan, and
Sam McCandlish. 2021. Scaling laws for transfer.
arXiv preprint arXiv:2102.01293 .
Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-
gory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi
Zhou. 2017. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409 .
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. arXiv
preprint arXiv:2203.15556 .
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 .
Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu,
and Bing Liu. 2022. Continual training of lan-
guage models for few-shot learning. arXiv preprint
arXiv:2210.05549 .
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Kon-
ishi, Gyuhak Kim, and Bing Liu. 2023. Contin-
ual pre-training of language models. arXiv preprint
arXiv:2302.03241 .
Bin Lei, Yuchen Li, and Qiuwu Chen. 2024. Autocoder:
Enhancing code large language model with \textsc
{AIEV-Instruct }.arXiv preprint arXiv:2405.14906 .
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim,
Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
Nicolas Gontier, Nicholas Meade, Armel Zebaze,
Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo
Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel
Romero, Tony Lee, Nadav Timor, Jennifer Ding,
Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri
Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
Carolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-
ish Contractor, Siva Reddy, Daniel Fried, Dzmitry
Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
Sean Hughes, Thomas Wolf, Arjun Guha, Leandro
von Werra, and Harm de Vries. 2023. Starcoder: may
the source be with you! Preprint , arXiv:2305.06161.
Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen,
Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan,
James Zou, and Yitao Liang. 2024. Selecting large
language model to fine-tune via rectified scaling law.
arXiv preprint arXiv:2402.02314 .
Jinghui Lu, Dongsheng Zhu, Weidong Han, Rui Zhao,
Brian Mac Namee, and Fei Tan. 2023. What makes
pre-trained language models better zero-shot learn-
ers? In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 2288–2303.
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie
Zhou, and Yue Zhang. 2023. An empirical study
of catastrophic forgetting in large language mod-
els during continual fine-tuning. arXiv preprint
arXiv:2308.08747 .
Rohan Pandey. 2024. gzip predicts data-dependent scal-
ing laws. arXiv preprint arXiv:2405.16684 .
Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang,
Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai,
Jiakai Wang, Yuanxing Zhang, et al. 2024. D-cpt
law: Domain-specific continual pre-training scal-
ing law for large language models. arXiv preprint
arXiv:2406.01375 .
R Tyrrell Rockafellar. 1993. Lagrange multipliers and
optimality. SIAM review , 35(2):183–238.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. Preprint ,
arXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. Preprint , arXiv:2307.09288.
Yiqun Yao and Yequan Wang. 2023. Research without
re-search: Maximal update parametrization yields
accurate loss prediction across scales. arXiv preprint
arXiv:2304.06875 .
Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou,
Jun Zhan, and Xipeng Qiu. 2024. Data mix-
ing laws: Optimizing data mixtures by predicting
language modeling performance. arXiv preprint
arXiv:2403.16952 .
Ça˘gatay Yıldız, Nishaanth Kanna Ravichandran, Pr-
ishruit Punia, Matthias Bethge, and Beyza Ermis.
2024. Investigating continual pretraining in large
language models: Insights and implications. arXiv
preprint arXiv:2402.17400 .
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Chuanqi Tan, and Chang Zhou. 2023. Scal-
ing relationship on learning mathematical reason-
ing with large language models. arXiv preprint
arXiv:2308.01825 .
Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui
Zhao, Yong Jiang, and Fei Tan. 2024. Balancing spe-
ciality and versatility: a coarse to fine framework for
supervised fine-tuning large language model. arXiv
preprint arXiv:2404.10306 .A LLM Configurations
The detailed parameters of the LLM configurations are listed in Table 1.
Table 1: Configurations of the LLMs.
Model Size 460m 940M 1.6B 3.1B
hidden size 1024 1536 2048 2560
intermediate size 3072 4608 6144 7680
number of attention heads 32 32 32 32
number of layers 24 24 24 32
vocabulary size 65632 65632 65632 65632
B Mathematical derivation
B.1 Notation
•S- represents the model sizes.
•M- the pre-trained large language model.
•Dgen- the general dataset.
•Ddom- the domain-specific dataset.
•R- the mixture ratio of the domain-specific data.
•DR- the total mixed dataset with R%domain-specific data.
•ϵ- tolerance for the general loss increase.
•LCPT
gen- the general loss.
•LCPT
dom- the domain-specific loss.
•L∆gen- the increment in general loss.
•L∆dom- the increment in domain-specific loss.
•F- the loss function of CPT expressed as the Lagrangian.
•T- the amount of training tokens (related to the number of iterations, training steps, or the total
volume of training data).
•λ- the Lagrange multiplier used to enforce the constraint on general loss while minimizing domain-
specific loss.
•Tmax- the maximum training tokens for CPT.
•T0- a point on the training curve where, after training at T0and continuing the training, the feasible
mixture ratio is observed.
•A- the set of mixture ratios that satisfying CPT objective.
•F- the set of Feasible Mixture Ratios (feasible mixture ratio).
•RCMR - the Critical Mixture Ratio (CMR), which is the optimal mixture ratio that minimizes the loss
function within the feasible set.•α1,α2,α3- parameters to be fitted representing coefficients in the power-law functions for the
increment of loss.
•β1,β2- parameters to be fitted representing constants in the power-law functions for the increment
of loss.
•s1,s2,s3- parameters to be fitted representing the exponents in the power-law functions for the
increment of loss.
B.2 Feasible mixture ratio
Given that Nis fixed, the 0bjective of CPT in Equation 3 can be transformed into:
F(R, T, λ ) =(Ldom(MS) +L∆dom(R, T)) +λ(L∆gen(R, T)−ϵ), (9)
whereLdom(CPT(MS;DR, T))is split into the value at T= 0,Ldom(MS)and the increment L∆dom. The
corresponding
R∗=argminRF(R, T, λ )
s.t.

L∆gen(R, T)≤ϵ
R≥0
Tmax≥T≥0
λ≥0.(10)
For a given mixture ratio R, if the training progresses ( Tincreases), and the objective function (Equation 9)
shows a decreasing trend, it indicates that the current proportion can lead to the continuation of training
towards the expected goal. The trend of the objective function Fincreasing with training can be reflected
by its partial derivative with respect to T:
∂F(R, T, λ )
∂T
R,λ=∂(Ldom(MS) +L∆dom(R, T))
∂T
R,λ+λ∂ 
L∆gen(R, T)−ϵ
∂T
R,λ. (11)
SinceLdom(MS)and−λϵare constants with respect to T, their derivatives are zero. Thus, we simplify to:
∂F(R, T, λ )
∂T
R,λ∂L∆dom(R, T)
∂T
R,λ+λ∂L∆gen(R, T)
∂T
R,λ. (12)
If the training objective under the fixed ratio progresses as expected, there should be at least one point
during the training process ( 0≤T≤Tmax) where this partial derivative is less than or equal to 0. From
this, we can define a feasible proportion curve that should satisfy the following inequality conditions:
∃T∈[0, Tmax] :∂F(R, T, λ )
∂T
R,λ≤0 (13)
This means that we only need to determine whether the solution Tof the above inequality 13 belongs to
[0, Tmax]in order to judge whether the current training meets the target. Setting it equal to zero to figure
out:
∂F(R, T, λ )
∂T
R,λ= 0 (14)
Setting the equation to zero and further simplifying to express it :
∂L∆dom(R, T)
∂T
R+λ∂L∆gen(R, T)
∂T
R= 0 (15)
To derive the following equation using the chain rule:
∂L∆dom(R, T)
∂T
R=−λ∂L∆gen(R, T)
∂T
R(16)By isolating∂L∆dom(R,T)
∂T
R, we get:
∂L∆dom(R, T)
∂T
R=−λ∂L∆gen(R, T)
∂T
R(17)
Using the chain rule, we have:
∂L∆gen(R, T)
∂T
R=∂L∆gen(R, T)
∂L∆dom(R, T)
R·∂L∆dom(R, T)
∂T
R(18)
By substituting this into the given equation, we get:
∂L∆dom(R, T)
∂T
R=−λ∂L∆gen(R, T)
∂L∆dom(R, T)·∂L∆dom(R, T)
∂T
R
(19)
Assuming∂L∆dom(R,T)
∂T
R̸= 0, we can cancel the terms:
1 =−λ·∂L∆gen(R, T)
∂L∆dom(R, T)
R(20)
Thus, we obtain:
∂L∆gen(R, T)
∂L∆dom(R, T)
R=−1
λ(21)
Since λ >0, the above derivative is a negative number. For a specific R, if there exist points on the
training curve where the partial derivatives of the two ∆values are equal to1
λ, then the ratio is consistent
with the expected goal of continual pretraining. These ratios are called feasible mixture ratios, and their
set is denoted as F. This is consistent with the feasible mixture ratios marked in Figure 1.
B.3 Fitting
Following the previous work (Kaplan et al., 2020; Hoffmann et al., 2022), we have adopted the power-law
as the parametric forms, which is different from other mixture law study (Ye et al., 2024). Previous work
has shown that the model parameter Nand the amount of data training Tare independently related to the
power law of loss. However, one point that our work related to power law is different. First, the function
we choose to fit is the increment of Loss. Second, due to the phenomenon of general loss increasing first
and then decreasing, in order to better fit the data, we used a two-term power-law function. According to
Equation 9, the data mixture scaling law for CPT training is defined as follows:
Given: (
L∆dom(T) =α1·Ts1+β1,
L∆gen(T) =α2·Ts2+α3·Ts3+β2.(22)
where α1,α2,α3,β1,β2,s1,s2, ands3are parameters to be fitted.
First, according to the definition of feasible mixture ratios, we can solve feasible mixture ratios under
the setting of data mixture scaling law. As the fitting at this time is an extrapolation of the training quantity,
R is a fixed value. For simplicity, we no longer explicitly write R, so both L∆domandL∆genare univariate
functions of T. First, differentiate L∆dom(T)with respect to T:
d
dTL∆dom(T) =d
dT(α1·Ts1+β1)
=α1·s1·Ts1−1.(23)
Next, differentiate L∆gen(T)with respect to T:
d
dTL∆gen(T) =d
dT(α2·Ts2+α3·Ts3+β2) =α2·s2·Ts2−1+α3·s3·Ts3−1. (24)According to the the expected CPT trend in Equation 13, we need to figure whether the critical T0that
meets this condition is in the effective range [0, Tmax]. Therefore, the solution for the Equation 15 is
important, which can be solved as Equation 15:
d
dTL∆dom(T) +λd
dTL∆gen(T) = 0 (25)
Substitute Equation 23 and Equation 24 respectively, we get:
α1·s1·Ts1−1+λ(α2·s2·Ts2−1+α3·s3·Ts3−1) = 0 (26)
Further simplifying:
α1·s1·Ts1−1+λα2·s2·Ts2−1+λα3·s3·Ts3−1= 0 (27)
To solve for T, we can factor out Tterms:
Ts1−1 
α1·s1 +λα2·s2·Ts2−s1+λα3·s3·Ts3−s1
= 0 (28)
Therefore, the critical points T0can be solved by:
Ts2−s1
0 =−α1·s1
λα2·s2−λα3·s3·Ts3−s1
0
λα2·s2(29)
Solving for T0:
Ts2−s1
0 =−α1·s1 +λα3·s3·Ts3−s1
0
λα2·s2(30)
Ts2−s1
0 =−α1·s1
λα2·s2−λα3·s3·Ts3−s1
0
λα2·s2(31)
Thus, the solution for T0in terms of the original parameters is:
T0="
−α1·s1
λα2·s2
1 +α3·s3
α2·s2Ts3−s2
0−1#1
s2−s1
(32)
C Justification of Tolerance Value
C.1 How was this determined?
Tolerance ϵin Equation 1 depends on the importance of maintaining general abilities for CPT goals
(Definition 1). We set ϵ= 0.05based on empirical results by observation. However, the value of ϵdoes
not affect the conclusions and analysis presented in this paper. In practical applications, its setting is
related to the researcher’s considerations of relevant factors (application scenarios, resource situation, etc.)
for CPT.
As shown in Figure 4 and Figure 7, ∆General loss typically peaks at values much smaller than 0.05. This
is because these figures depict relatively small mixture ratios ( R ≤1
2), where general losses initially
increase before decreasing. As the mixture ratio grows, ∆General Loss continues to rise and eventually
exceeds 0.05 ( R → 1). This trend can be initially observed in Figure 1 and Figure 8.
C.2 How does this value reflect the constraint in practice?
As detailed in Definition 2, the tolerance value ϵis used to determine a range of mixture ratios Athat
do not lead to excessive increases in general losses (i.e., unaffordable losses in general capabilities). In
practice, we use the tolerance to identify the upper bound of feasible mixture ratios . For example, an
excessive mixture ratio may cause the general loss to exceed the tolerance threshold. Therefore, even if
the general loss under this ratio reaches a plateau or decreases, it is still considered infeasible.D Table
To illustrate the accuracy of our fitting, we provide the relative error for the fitted curves in Table 2, along
with the MSE and R2values for the power-law fitting of ∆General ( ∆Domain) loss as a function of
training tokens Tin Table 3. For better reproducibility, the fitted scaling law coefficients are presented in
Table 4 and Table 5, which correspond to § 5.3. In summary, we found that the power function accurately
fits the loss with respect to model size, data mixture ratio, and token volume, using the Scipy library to
estimate the function’s coefficients.
Ratio 460M 940M 1.6B 3.1B
100% 1.4628 1.3723 1.3242 1.2585
75% 1.4844 1.3910 1.3416 1.2750
50% 1.5122 1.4155 1.3643 1.2965
33% 1.5387 1.4385 1.3854 1.3170
25%-gt 1.5561 1.4538 1.3994 1.3305
25%-pred 1.5566 1.4546 1.3999 1.3303
Difference 0.03% 0.05% 0.03% 0.02%
Table 2: Domain Proportion and Predicted/Actual Value Relative Error
Metric RatioGeneral Domain
460M 940M 1.6B 3.1B 460M 940M 1.6B 3.1B
MSE100% 1.9394e-10 2.5695e-10 9.8058e-10 2.2880e-11 9.7830e-08 7.6174e-08 6.4577e-08 4.4057e-08
75% 5.2270e-11 7.7104e-15 3.4402e-12 1.5432e-11 1.2283e-07 7.6940e-08 7.1749e-08 4.4160e-08
50% 1.5340e-10 4.4162e-11 1.5992e-09 1.6405e-10 1.2539e-07 7.0535e-08 5.1893e-08 3.8559e-08
33.3% 5.2538e-11 1.1070e-10 5.5883e-11 5.4041e-11 1.1904e-07 6.9371e-08 5.8162e-08 4.4630e-08
25% 7.3045e-11 4.7677e-11 8.7140e-11 1.6598e-14 1.0966e-07 7.0327e-08 4.7272e-08 4.6702e-08
12.5% 6.9011e-11 8.9891e-11 7.1858e-11 9.2656e-11 8.1609e-08 7.2597e-08 5.1854e-08 4.4091e-08
R2100% 0.9999 0.9999 0.9998 0.9989 0.9957 0.9969 0.9969 0.9978
75% 0.9993 0.9999 0.9990 0.9963 0.9954 0.9966 0.9966 0.9975
50% 0.9973 0.9966 0.9946 0.9593 0.9951 0.9963 0.9965 0.9971
33.3% 0.9928 0.9877 0.9818 0.9251 0.9954 0.9959 0.9965 0.9967
25% 0.9872 0.9763 0.9659 0.8741 0.9956 0.9959 0.9966 0.9966
12.5% 0.9590 0.9438 0.9520 0.8972 0.9974 0.9962 0.9963 0.9965
Table 3: The MSE and R2of the fitting power-law of ∆General ( ∆Domain) loss by training tokens T
E Figure
To comprehensively illustrate the patterns observed in our experiments and our findings, we present
the evolution of loss across different mixture ratios and model sizes throughout training in Figure 8.
Additionally, Figure 9 shows the extrapolation of Tusing a power-law fit, highlighting the variations in the
rise-then-fall trend of general loss under different mixture ratios and model sizes. Finally, the changes and
predictions of general loss and domain loss with respect to Tunder different mixture ratios are illustrated
in Figure 10. This effectively explains why we use different forms of power laws to predict Tin § 5.2.Model Size RatioGeneral Domain
α1 s1 α2 s2 β1 α3 s3 β2
460M100% -0.01502 0.17543 0.02116 0.46219 0.00472 -2.43854 0.02229 2.24461
75% -0.14742 0.64329 0.15135 0.63990 0.00144 257.77904 -0.00022 -257.95781
50% 0.12446 0.57575 -0.12123 0.57941 0.00050 227.06960 -0.00023 -227.23919
33.3% 0.11761 0.53594 -0.11471 0.53971 0.00002 97.25206 -0.00050 -97.40981
25% 0.14030 0.51526 -0.13758 0.51836 -0.00018 24.62425 -0.00190 -24.77263
12.5% 0.13055 0.48615 -0.12803 0.48955 -0.00055 0.71989 -0.08343 -0.81231
940M100% 0.00987 0.51496 -0.00521 0.00000 0.00423 262.61092 -0.00021 -262.90741
75% 0.00653 0.30806 -0.00500 0.00000 0.00232 259.14017 -0.00020 -259.43163
50% 0.09454 0.57282 -0.09209 0.57654 0.00101 258.06226 -0.00019 -258.34505
33.3% 0.10418 0.52619 -0.10186 0.52970 0.00063 248.51191 -0.00018 -248.78323
25% 0.11006 0.51700 -0.10785 0.52040 0.00046 222.52376 -0.00020 -222.78786
12.5% 0.12685 0.48822 -0.12468 0.49141 0.00013 129.70296 -0.00031 -129.94852
1.6B100% 0.00752 0.50645 -0.00385 0.00000 0.00384 -0.39825 0.09162 0.20416
75% 0.06381 0.63024 -0.06167 0.63444 0.00161 210.68118 -0.00023 -210.84391
50% 0.07899 0.57219 -0.07702 0.57587 0.00084 258.78683 -0.00017 -258.94316
33.3% 0.08952 0.54505 -0.08764 0.54858 0.00050 133.08715 -0.00032 -133.23264
25% 0.08747 0.53775 -0.08564 0.54152 0.00034 4.92547 -0.00864 -5.06034
12.5% 0.10378 0.51241 -0.10198 0.51585 0.00007 210.06851 -0.00018 -210.18657
3.1B100% 0.00978 0.38822 0.00000 3.42674 0.01398 -0.30116 0.10974 -0.03868
75% 0.04886 0.56155 -0.04660 0.56711 0.01184 265.17268 -0.00018 -265.47797
50% 0.06324 0.52050 -0.06100 0.52592 0.01093 156.20051 -0.00027 -156.50271
33.3% 0.08406 0.51263 -0.08187 0.51724 0.01056 128.57034 -0.00031 -128.86546
25% 0.08084 0.51463 -0.07866 0.51973 0.01034 133.03230 -0.00028 -133.32244
12.5% 0.09549 0.49146 -0.09320 0.49629 0.01001 7.55302 -0.00463 -7.82759
Table 4: Fitting power-law coefficients for different model sizes and the mixture ratios of ∆General and ∆Domain
losses as a function of training tokens T
Model Size α4 s4 β3
460M 0.22524761 0.26944345 -0.48139982
940M 0.7520627 0.13720245 -1.06581937
1.6B -2.36384831 -0.15125569 1.59223649
3.1B -2.5368197 -0.42071423 0.84375368
Table 5: Fitted CPT scaling law coefficients for different model sizes in Finance .Domain-General	Loss	Curve	(	Scale	cross-section	and	Ratiocross-section)
Figure 8: Each cluster represents a different mixing ratio, which is 1/8, 1/4, 1/3, 1/2. Pay attention to the third set of
lines, that is, clusters with a proportion of 1/3. The cross-section of this set of lines is shown on the right.Extrapolation	of	T	with	power	law
R=1/8
R=1/4
R=1/3
R=1/2
N=3.1B
N=1.6B
N=940m
N=460mFigure 9: Power laws of training token volume for different model sizes in Finance . Compared with the extrapolation
of the training volume of the model of the same size to continue training in the Academic papers field in Figure 7, it
can be seen that under the same proportion, the amount of training volume of CPT of Academic Papers is larger
where the inflection point appears.Figure 10: The temperature bar represents the mixture ratio R, which takes six values ranging from 1/8 to 1.
Different subgraphs are fitting curves that change with the increase of Tin the training process for different MN
domain loss and general loss. Overall, the domain loss keeps decreasing during the training process while the
general loss keeps increasing. It is worth noting that although the general loss is increasing, the magnitude of its
increase is actually very small, especially when the mixture ratio is not very big ( R={1/8,1/4,1/3,1/2,3/4}),
with a total increase of less than 0.02. The solid circles ( •) represent real losses, and the stars ( ★) represents the
predicted losses. We can see that whether it is general loss or domain loss, the predicted values fall on the fitted curves accurately.