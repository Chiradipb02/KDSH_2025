CommonIT: Commonality-Aware Instruction Tuning for
Large Language Models via Data Partitions
Jun Rao1Xuebo Liu1*Lian Lian2Shengjun Cheng2Yunjie Liao1Min Zhang1
1Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China
2Huawei Cloud Computing Technologies Co., Ltd.
{rao7jun,yunjie445}@gmail.com, {liuxuebo,zhangmin2021}@hit.edu.cn
{lianlian3,chengshengjun}@huawei.com
Abstract
With instruction tuning, Large Language Mod-
els (LLMs) can enhance their ability to adhere
to commands. Diverging from most works fo-
cusing on data mixing, our study concentrates
on enhancing the model’s capabilities from the
perspective of data sampling during training.
Drawing inspiration from the human learning
process, where it is generally easier to mas-
ter solutions to similar topics through focused
practice on a single type of topic, we introduce
a novel instruction tuning strategy termed Com-
monIT: Commonality-aware Instruction Tun-
ing. Specifically, we cluster instruction datasets
into distinct groups with three proposed met-
rics ( TASK,EMBEDDING andLENGTH ). We
ensure each training mini-batch, or “parti-
tion”, consists solely of data from a single
group, which brings about both data random-
ness across mini-batches and intra-batch data
similarity. Rigorous testing on LLaMa models
demonstrates CommonIT’s effectiveness in en-
hancing the instruction-following capabilities
of LLMs through IT datasets (FLAN, CoT, and
Alpaca) and models (LLaMa2-7B, Qwen2-7B,
LLaMa 13B, and BLOOM 7B). CommonIT
consistently boosts an average improvement of
2.1% on the general domain (i.e., the average
score of Knowledge, Reasoning, Multilingual-
ity and Coding) with the LENGTH metric, and
5.2% on the special domain (i.e., GSM, Open-
functions and Code) with the TASK metric, and
3.8% on the specific tasks (i.e., MMLU) with
theEMBEDDING metric. Code is available at
https://github.com/raojay7/CommonIT .
1 Introduction
The emergence of ChatGPT (OpenAI, 2023) and
a range of large language models (LLMs) (Tou-
vron et al., 2023; Muennighoff et al., 2023) brings
light to artificial general intelligence (AGI). As the
size of the model increases, instruction fine-tuning
becomes necessary to align human intentions with
*Corresponding Author
Please help me translate the 
following sentence from English 
to German: 
Can I change it?
Please answer the following 
questions:
It's raining today and I don't want 
to bring an umbrella, I want to 
bring a raincoat. Can I change it?Yes, if you prefer to bring 
a raincoat instead of an 
umbrella because it's 
raining, that's completely 
fine! Translation
Question AnsweringYes, you can change it.Output
Output
Figure 1: Because of the mix of instructions, the LLM
cannot understand the specific task of the different in-
structions well after IT (Shi et al., 2023). It fails to
recognize the instructions of the translation task in this
case, “Please translate the following sentence ...”, and
simply replies with the final phrase “Can I change it?”.
machine understanding of language. Compared to
traditional fine-tuning, instruction tuning (IT) repre-
sents different tasks through instructions followed
by some task-specific inputs (Iyer et al., 2022).
Current researchers have obtained good results by
passing only the IT stage (Zhang et al., 2024) on
datasets generated by multiple construction meth-
ods (Taori et al., 2023; Zheng et al., 2023; Xu
et al., 2023; Fan et al., 2024). Typically after this
stage of training, LLMs show a strong multi-task
capability and the models can perform multifaceted
tasks such as summarization, conversation, writing
and other basic tasks.
Existing research has already underscored the
significance of data mixing in fine-tuning large
models for achieving a generalized model. For
example, Chung et al. (2022) pointed out that the
key for LLMs in the IT phase is to enhance the
understanding of instructions and they increase the
diversity of instructions, e.g., by using more in-
structions to describe the same task (Longpre et al.,
2023). Despite this, these LLMs are susceptible to
bias in the model’s understanding of a particular
task (Kim et al., 2023) due to the mixing of mul-
tiple tasks or diverse instructions, which can lead
to a decline in the average ability across multiplearXiv:2410.03077v1  [cs.CL]  4 Oct 2024tasks (Scialom et al., 2022; Iyer et al., 2022; Wang
et al., 2023). As illustrated in Figure 1, this situ-
ation results in a diminished ability of the model
to follow instructions accurately. Specifically, it
struggles to comprehend the requirements laid out
in the instructions, leading to incorrect responses.
To enhance the model’s ability to follow instruc-
tions, we introduce CommonIT, a methodology that
enhances model comprehension of data features by
engaging distinct data classes in individual gradi-
ent updates and interchanging data classes across
batches. Central to our approach is the principle
of data commonality (Cui et al., 2022), inspired
by humans’ learning process when preparing for
exams. This means focusing preparation on one
exam subject at a time rather than attempting to
study for multiple subjects simultaneously. Specifi-
cally, we advocate a two-phase fine-tuning process
for LLMs. Initially, raw data are segmented into
several groups and partitioned into designated sizes
for training. This strategy enhances the model’s
ability to follow instructions, as evidenced by com-
prehensive evaluations across four dimensions of
capacity testing.
Our experiments demonstrate its applicability
across diverse IT datasets and potential for fu-
ture applications in different models and domain-
specific tasks. Our findings indicate that length
serves as the most effective criterion for grouping
within the generic domain. Furthermore, embed-
ding divisions should be tailored specifically for
each task, and task-related information is crucial
for optimizing fine-tuning quality in specific do-
mains. Our analysis of commonalities, from the
perspectives of data quality, input question sen-
tence representation and multi-task generalizability,
delves into the reasons for improvements, demon-
strating that the common learning strategy indeed
enhances the effectiveness of representations and
the capabilities across various sub-tasks.
Our contributions are as follows:
•We propose the CommonIT framework, which
leverages commonalities to enhance models’
capabilities in following instructions. This
framework includes three strategies for cat-
egorizing groups within IT datasets and in-
corporates a batch-based constraint policy for
optimization. (§3).
•CommonIT demonstrates broad applicability
across multiple dimensions, including vari-
ous datasets, general and specialized domains,and diverse models. Additionally, we have
explored scenarios to determine the most suit-
able group strategy for each context (§5.1).
•Our exploration of commonalities provides
a possible explanation for the sources of im-
provements (§5.2 and §5.3).
2 Related Work
2.1 Data-centric AI
There are current discussions on the curricu-
lum (Bengio et al., 2009; Platanios et al., 2019;
Feng et al., 2023; Lee et al., 2023), data mix-
ing (Wang et al., 2023; Xu et al., 2023), and data
filtering (Zhou et al., 2023; Chen et al., 2024; Xie
et al., 2023) during the training of large language
models. LIMA (Zhou et al., 2023) finds better per-
formance could be achieved with just a few percent
of samples from these datasets if selected properly
with the very large models (65B). When smaller
scale models are used (7B), the amount of data
in SFT is usually related to the model size. Al-
paca (Taori et al., 2023) demonstrates that models
at the 7B level, fine-tuned with a small amount
of data (less than 100K), can exhibit strong align-
ment capabilities. Further, AlpaGasus (Chen et al.,
2024) found that models at the 7B level can also
achieve strong alignment capabilities with fewer
data. We have initiated early explorations in the
data sampling strategy and find that the training
scheme possibly has a major impact on the model’s
final performance under the small training epochs.
2.2 Instruction Tuning
Many works (Taori et al., 2023; Peng et al., 2023;
Xu et al., 2023; Chen et al., 2023; Wang et al.,
2023) employ distilled datasets and curate various
data for fine-tuning language models, achieving
enhanced performance. Early works like FLAN
2021 (Wei et al., 2022a) and Super-Natural In-
structions (Wang et al., 2022) are to convert tra-
ditional NLP tasks into instruction format through
manually defined instruction templates. FLAN-
CoT (Wei et al., 2022b) and FLAN 2022 (Chung
et al., 2022) employ Chain-of-Thought training
prompts to strengthen the reasoning of the model.
Shen et al. (2023) state that instruction finetuning
can be considered a “continual finetuning stage”.
Su et al. (2023) demonstrate how various text en-
codings represent different tasks, enabling a single
model to accomplish multiple downstream tasks
and thereby achieving a more generalized model.Grouping 
Method
1. Task
2. Statistics
3. EmbeddingStep 1: Group the Dataset
Step 2: Fine-tune with Shuffled PartitionsDataset 
LLMs
Lt(θ) Eq.2Random SamplePartition 1 Partition 3
Partition 2 Partition 4Partition 5
Partition 6(a)
(b)
(c)
Batch tGet Partitions 
by GroupsCommonIT Baseline
Dataset 
  Sample    Batches
Batch 1 Batch 2
Batch 3 Batch 4
Batch 5 Batch 6Grouping Method
2. Statistics:
Response Length
3. Embedding1. Task
T ranslation
CommonsenseSentiment
Paraphrase
...
Training Data
T1
T2
TnEncodeSelect Nearest Neighbors
C1
MMLU Dev Set Representationof 57 T asksC2
C56C57C57
C2C3C7C10
C55
C5C5C57
C55C2
C1C57
C2
C5Data 
Group Figure 2: An overview of the baseline (IT) and our CommonIT. The different shapes andcolors in the figure
indicate a property of the data that can be used for grouping (task, statistics and embedding), and we use shapes as
an example here. The CommonIT strategy inputs training data to the model during training from a group, e.g., batch
tfrom class (a). The model calculates the loss Lt(θ)of this partitioned data to update the parameters of the model.
These works have pointed out that the variety and
scale of instruction data design tasks can signifi-
cantly affect the ability of a model to generalize.
However, existing efforts have primarily concen-
trated on exploring the data mixing (Longpre et al.,
2023; Iyer et al., 2022). In contrast, our work fo-
cuses on the training methodology using the same
data, where we conduct a detailed analysis. By
examining the various strategies and techniques in
the training process, we aim to uncover insights
that may transcend mere data considerations, po-
tentially leading to more effective models that are
attuned to the intricacies of instruction tuning.
3 Our Proposed CommonIT Method
Motivation Data imbalance can lead to inconsis-
tent performance across tasks, with some exhibit-
ing exceptional results and others underperforming
significantly (Shi et al., 2023). This phenomenon
is typically evidenced by instruction misinterpre-
tation, as illustrated in Figure 1, where the model
fails to comprehend the user’s intention accurately.
Our approach, CommonIT, aims to balance task-
specific sampling for gradient updates with diverse
data grouping. This strategy draws upon the human
learning process of understanding through analogy,
leveraging similarities across different contexts to
facilitate comprehension. Through this approach,
the model gains an enhanced ability to differentiate
among the instructions of various tasks. Conse-quently, it can more accurately respond to distinct
task directives, thereby minimizing the misinterpre-
tation of instructions and reducing the generation
of irrelevant content (§5).
Overview As shown in Figure 2, CommonIT is
divided into two steps. First, we need to perform a
clustering operation to categorize the dataset into
multiple categories. It is worth noting that the clus-
tering here does not require precise categorization
but only focuses on a certain aspect of the dataset
division. In the figure, the square, circle, and tri-
angle indicate three kinds of data with different
attributes. At the same time, the different colors
indicate that the data can also be divided by the
attribute of color, which is not divided here just
due to space constraints. The second step is to con-
struct partitions of the divided data by batch size
and randomly take the divided partitions as one
batch when the data are fed into the model.
3.1 Background
After large-scale pre-training, instruction tuning
is the next phase of LLMs to enable the model
to understand and interpret instructions for human
language preferences (Zhang et al., 2022; Muen-
nighoff et al., 2023; Touvron et al., 2023).
For a training data source D={xn,yn}, the
standard instruction tuning of the language modelis trained with maximum likelihood estimation:
ˆθ= arg max
θNX
n=1MX
m=1logP 
yn
m|yn
<m;xn;sn;θ
(1)
where s,xandyrepresent the instruction, the
input, and the target, respectively. The θare the
parameters to be optimized during the language
model training and Mis the sequence length of
the target. Generally, the instructions are required
and represent a specific task. Here is an example
of an instruction: “Please help me write a poem,”
requiring models to output content about poetry.
3.2 Group the Dataset (GD)
The primary objective in organizing a dataset
is to segregate distinct data categories, typically
achieved by partitioning the data into tasks. For-
mally, we aim to decompose a dataset Dinto dis-
tinct sub-datasets D0,D1, ...,Dn. If task-based par-
titioning proves infeasible, data clustering methods
can be employed for learning. Drawing an anal-
ogy with human learning, individuals often find it
easier to grasp concepts when exposed to related
topics concurrently, leveraging inherent similari-
ties. Conversely, disparate topics can pose learning
challenges. We subsequently detail three potential
group strategies:
•Group by Task The task-specific information
facilitates the model’s ability to differentiate
between various task instructions, enhancing
its generalization capacity across different in-
structional contexts. For the dataset that trans-
forms traditional NLP tasks by designing in-
structions, we can obtain the task type of the
original data. With this type, we can divide
the dataset to get different categories.
•Group by Embedding This approach is an
alternative to embedding clustering, typically
used when specific task categories cannot be
readily identified. Automatic division can
significantly enhance the effectiveness within
specific domains for particular tasks. We use
the category with the highest number of cor-
responding categories among the kretrieved
pieces of data as the category of the training
set data. For simplicity, we use the category
of MMLU (development set corresponds to a
category of 57 exams) to categorize the train-
ing data. We first use a certain sentence en-
coder to convert sources in both the trainingset and development set to vector representa-
tions.1Then, for each training source s, we
retrieve its nearest kneighbors s1,s2, ...,sk
from the MMLU development set (according
to the distances in the sentence encoder’s em-
bedding space).2Given some predefined sim-
ilarity measure d, such as the cosine similarity,
the neighbors are ordered in such a way that
d(si,s)≤d(sj,s)when i < j .
•Group by Statistics Due to the nature of IT
data, responses of the same length typically
belong to the same data category, such as
multiple-choice, question-answering, or trans-
lation tasks. This inherently includes task sim-
ilarity within the data. The length can be the
simplest metric for classifying clusters when
the task information is missing. We count the
length distribution of each IT dataset such that
the number of samples in the sub-datasets af-
ter each IT dataset is divided remains the same
for each length interval.
3.3 Fine-tune with Shuffled Partitions (FP)
In conventional model training, the batch data is
usually sampled randomly from the entire data
source D. The training of language model employs
mini-batch gradient descent rather than batch gra-
dient descent orstochastic gradient descent .
CommonIT supposes that the mini-batches are
bucketed in a particular way and upgrade these
samples. In our proposed CommonIT, we can per-
form the batch construction as follows since we
obtain the “class division” of the data. We ensure
instances within each batch come from the same
group while the order of sampled groups is random.
Take Figure 2 for example, the first batch B∗
1comes
from a group (a), and the tth batch B∗
tmay be com-
posed of samples of the only group (a) or (b), or
(c). So the batch loss can be calculated as:
Lt(θ) =−1
NNX
i=1logP(y(i)
t|x(i)
t;s(i)
t;θ)(2)
where s,xandyrepresent the instruction, the in-
put, and the target, respectively. The Lt(θ)is the
training loss of a mini-batch that has Nexamples
at the t-th training step.
1https://huggingface.co/sentence-transformers/all-mpnet-
base-v2
2https://github.com/Shark-NLP/self-adaptive-ICL4 Evaluation Setup
4.1 Evaluations
Factual knowledge, reasoning, multilinguality, and
coding are foundational to LLMs, as they encapsu-
late the core capabilities required to comprehend,
analyze, and generate human-like text. In the evalu-
ations, we examine the model’s performance in
these areas, employing four out-domain bench-
marks to assess its ability to assimilate knowledge
(MMLU (Hendrycks et al., 2021)) and execute com-
plex reasoning (BBH (Suzgun et al., 2023)), multi-
linguality (TydiQA (Clark et al., 2020)), and coding
(Codex-Eval (Chen et al., 2021b)) tasks.
•Factual Knowledge represents a critical di-
mension of capability in LLMs, essentially
reflecting their memory capacity. We employ
the Massive Multitask Language Understand-
ing dataset (MMLU (Hendrycks et al., 2021) )
as a benchmark to measure the model’s factual
knowledge.
•Reasoning is another crucial capability for
large models, particularly in solving complex
problems. We utilize the Big-Bench-Hard
dataset (BBH (Suzgun et al., 2023)), compris-
ing 23 intricate tasks, to assess the model’s
general reasoning capabilities.
•Multilinguality is essential for enabling large
models to serve speakers of various languages.
We employ the TyDiQA (Clark et al., 2020)
dataset, a multilingual testing dataset encom-
passing 11 different language types.
•Coding is another important ability that peo-
ple need large language models. We use the
HumanEval dataset (Chen et al., 2021b) (we
refer to it as Codex-Eval) to evaluate the mod-
els’ capability to generate functionally correct
programs from docstrings.
4.2 Training Datasets
We have selected several representative IT
datasets from distinct sources, encompassing task-
constructed data and model-generated high-quality
data. Our diverse selection facilitates a thor-
ough evaluation, enabling us to gauge performance
across varied data types and characteristics. The
selections include: FLAN V2 (Chung et al., 2022)
A collection of NLP tasks that combines several
existing datasets with various data augmentations.FLAN CoT (Wei et al., 2022b) A collection of
datasets annotated with chain-of-thoughts. We
use the CoT mixture from the FLAN v2 collec-
tion (Chung et al., 2022), splitting it out as a sep-
arate dataset. Alpaca GPT4 (Peng et al., 2023)
A dataset was created using the Alpaca dataset
as inputs, replacing the example generations with
generations from GPT-4. Detailed information re-
garding the datasets and training settings can be
found in Appendix A.
5 Experiment
5.1 Main Results
CommonIT Improves Instruction Following In
Several Dimensions Table 1 illustrates the com-
parative results of the IT on LLaMa 7B of general
IT datasets. For LLaMa, the conventional IT ap-
proach (IT) has enhanced the model’s performance
across three capability dimensions, with an average
improvement of 10 points in MMLU, 4 points in
BBH, and 5 points in TydiQA. Employing our train-
ing strategy (CommonIT), we improved further on
an already strong and influential baseline (Wang
et al., 2023). This indicates CommonIT’s effective-
ness with varied data distributions.
LENGTH Metric Works Best in General Do-
main Table 1 also presents the results of different
grouping methods by combining different grouping
strategies with three datasets. Since only the FLAN
dataset contains partial task category information,
we conducted ablation experiments on this dataset
using three grouping methods. Due to the lack of
original task divisions, we could only perform em-
bedding and length ablation experiments in these
two datasets. Various grouping strategies improve
the baseline results across multiple capability di-
mensions. These strategies yield varying degrees
of enhancement in different capability areas. Em-
ploying embedding similarity as a group method
also resulted in performance gains across these
datasets. As a simple grouping criterion, length
has already achieved notable improvements. This
significant gain demonstrates the importance of
leveraging specific training approaches tailored to
the unique properties of the data, ultimately leading
to enhanced model performance on the IT datasets.
EMBEDDING Metric Works Best Based on the
Specific Tasks Given that our experiments on
embedding similarity are based on the division of
MMLU’s 57 categories, we further present detailedDataset ModelMMLU BBH TydiQA Codex-EvalA VG.
0-shot 5-shot Direct CoT F1 P@10
- LLaMa 7B 32.1 35.2 34.0 33.3 37.0 18.3 31.7
FLAN CoTIT* 41.3 42.5 33.7 31.3 44.4 17.3 35.1
IT 37.1 38.3 32.9 34.1 47.5 19.3 34.9
CommonIT
By Embedding 40.2 41.4 36.1 33.5 45.8 17.4 35.7
By Length 38.7 42.3 33.4 35.2 47.9 20.2 36.3
AlpacaIT* 42.6 38.3 28.5 32.3 23.6 25.0 31.7
IT 34.8 36.4 32.6 33.0 37.4 22.6 32.8
CommonIT
By Embedding 41.1 40.1 33.6 33.8 38.7 23.0 35.1
By Length 40.4 40.1 33.5 34.6 38.9 24.7 35.4
FLANIT* 45.4 47.1 38.6 36.1 45.0 12.9 37.5
IT 44.2 45.2 38.3 37.2 45.1 16.8 37.8
CommonIT
By Task 46.6 47.4 38.9 37.2 45.7 19.6 39.2
By Embedding 47.2 48.5 38.9 37.9 44.5 21.8 39.8
By Length 46.7 47.9 39.7 39.9 47.2 19.3 40.1
Table 1: Main results for CommonIT at LLaMa-7B. “*” denotes the results from Wang et al. (2023). The large
model of multidimensional measurements shows the best results using the Length metric.
Dataset ModelMMLU
(0-shot/5-shot)
Humanities Social. STEM Other A VG.
- LLaMa 7B 31.5/31.5 31.2/37.3 29.7/32.3 36.1/41.3 32.1/35.2
FLAN CoTIT 34.6/36.7 41.1/42.3 30.8/31.1 42.7/43.5 37.1/38.3
CommonIT
By Length 34.9/ 39.2 44.4/48.2 32.7/33.9 44.5/ 49.1 38.7/ 42.3
By Embedding 38.3/38.3 43.5/46.1 32.1/33.7 47.3/48.6 40.2/41.4
AlpacaIT 34.4/35.4 35.3/35.4 28.2/30.5 41.0/41.3 34.8/36.4
CommonIT
By Length 38.3/ 38.5 43.4/42.8 32.2/32.2 48.3/ 47.1 40.4/40.1
By Embedding 39.6/37.8 44.1/43.2 32.5/33.3 48.5/46.9 41.1/40.1
FLANIT 42.7/42.1 49.9/50.7 34.1/37.0 50.5/52.1 44.2/45.2
CommonIT
By Task 44.0/45.2 53.8/54.3 36.9/38.0 52.5/52.6 46.6/47.4
By Length 44.5/44.5 53.6/54.7 35.9/39.5 53.3/54.2 46.7/47.9
By Embedding 44.8/45.3 53.9/55.6 37.3/39.8 53.7/54.7 47.2/48.5
Table 2: The results of the CommonIT method across multiple task categories in MMLU. Using MMLU’s sub-
disciplines as discrete embedding divisions has enhanced the performance within each sub-discipline of the MMLU
task, thereby improving the overall results. The embedding is more effective than other metrics in MMLU.
Model/Domain GSM OpenFunctions Code
IT 39.0 30.4 23.6
Common IT
By Length 36.0 (-3.0) 31.3 (+0.9) 28.2 (+4.6)
By Embedding 39.0 (+0.0) 34.8 (+4.4) 28.3 (+4.7)
By Task 43.5 (+4.5) 35.7 (+5.3) 29.3 (+5.7)
Table 3: Comparison of methods across domain-specific
IT.Task metric in CommonIT is the key to improving
domain-specific instruction following abilities.
results for each task category in MMLU, as illus-
trated in Table 2. The validation of exam questionsacross diverse subjects mirrors how humans pre-
pare for exams in various disciplines. Integrating
our CommonIT method shows the model substan-
tially enhances various disciplinary tasks relative to
the baseline approach (IT). This indicates that train-
ing on similar questions grouped within a single
batch using embedding can enhance the effective-
ness of individual sub-tasks, thereby augmenting
the overall capabilities. This observation further
suggests that strategically designing the division of
embeddings for specific tasks can enhance perfor-
mance.ModelMMLU
0-shotBBH
DirectTydiQACodex
EvalA VG.
BLOOMZ 7B 42.1 28.1 64.8 15.0 37.5
+IT 39.6 30.1 69.7 17.1 39.1 (+1.6)
+CommonIT 42.7 30.1 71.6 18.3 40.7 (+3.2)
LLaMa2-7B 36.6 34.1 49.1 25.8 36.4
+IT 50.0 40.0 52.9 20.7 40.9 (+4.5)
+CommonIT 50.8 40.6 55.9 22.3 42.4 (+6.0)
Qwen2-7B 68.3 49.0 27.1 67.3 52.9
+IT 68.9 30.6 19.0 79.0 49.4 (-3.5)
+CommonIT 69.1 47.2 26.0 76.6 54.7 (+1.8)
LLaMa 13B 42.4 38.7 47.4 26.6 38.8
+IT 46.1 39.3 51.2 27.2 41.0 (+2.2)
+CommonIT 47.1 39.5 52.4 31.3 42.6 (+3.8)
Table 4: Tuning results on FLAN for the different mod-
els with our CommonIT (Length).
TASK Metric Works Best in Special Domain
To further demonstrate CommonIT’s improved
instruction-following abilities, we conducted in-
struction fine-tuning across specific domains,
namely GSM (Cobbe et al., 2021), OpenFunc-
tions (Patil et al., 2023), and Code (Wei et al.,
2023). We mix the dataset with the FLAN and
the domain-specific data to establish a strong base-
line (IT) using LLaMa-2 7B as the base model. For
example, we mix the GSM 7.5K and FLAN 100K
for training and evaluate the model for the GSM
testing set. The comparison methods (Length, Em-
bedding, and Task) mixed the previously described
grouped FLAN dataset by different group methods.
Our findings in Table 3 indicate that, without fur-
ther delicate domain-specific grouping, using task
grouping directly improves the performance, prov-
ing CommonIT is enhanced by the Model’s ability
to discern the informational distinctions between
tasks. Improvements in task-specific performance
in domains suggest that the length metric may also
carry domain-specific information beneficial to the
model without requiring further processing, par-
ticularly in OpenFunctions and Code. In contrast,
embedding with MMLU grouping diminished per-
formance in specific domains of fine-tuning, such
as OpenFunctions and Code. These observations
underscore the importance of constructing clear dis-
tinctions between task instructions and suggest that
re-embedding for specific tasks could further op-
timize performance. Subsequent results, assessed
across the four dimensions of competence, utilized
length as the foundational criterion for division.
Applicability across Various Foundation Mod-
els To further demonstrate the applicability of
our approach across different models (architecture,
scale and sequence length), we conducted exper-Model MMLU BBH TydiQACodex
EvalA VG.
LLaMa 7B 32.1 34.0 37.0 18.3 30.4
FLAN 46.7 39.7 45.2 16.8 37.1
w/o GD 44.2 38.3 45.1 12.9 35.1
w/o FP 44.4 36.0 38.5 16.8 33.9
FLAN CoT 38.7 33.4 47.9 20.2 35.1
w/o GD 37.1 32.9 44.5 19.3 33.5
w/o FP 37.5 32.6 47.2 19.4 34.2
Alpaca 40.4 33.5 38.9 24.7 34.4
w/o GD 34.8 32.6 37.4 22.5 30.0
w/o FP 39.2 15.6 30.5 28.0 28.3
Table 5: Ablation study on CommonIT (Length). “w/o
GD”: the baseline results of the IT. “w/o FP”: the model
undergoes sequential training on predefined groups.
iments on the BLOOM 7B and LLaMa 13B and
newly version of LLaMa (LLaMa2-7B). We eval-
uated the results of four benchmarks, as shown in
Table 4. CommonIT outperforms IT substantially,
showcasing an average increase of about 1.5 points.
The results indicate that our approach outperforms
the baseline IT strategy, achieving a noteworthy
improvement in various models.
5.2 Ablation Study
The success of CommonIT hinges on the initial
grouping in Stage One (GD) and the subsequent
assurance in Stage Two (FP) that data within a
single batch exclusively originates from a single
group. We conducted ablation studies on these two
components and have presented the corresponding
results (w/o GD and w/o FP) in Table 5. Previ-
ous main results have demonstrated that the length
metric is most effective in general domain IT, and
we have adopted this metric as the foundational
basis for CommonIT. It is observable that in the
absence of GD (baseline IT), all evaluation results
experience a significant decline, underscoring the
critical importance of grouping for the training pro-
cess. Furthermore, the omission of FP led to a
significant drop in most results.
5.3 Analysis
Learning from High&Low-Quality Instructions
We compare one existing state-of-the-art method:
Alpagasus: GPT 3.5 selected the highest scoring
9,000 samples. We show the average results of
multitasking with a uniform setup in Table 6, show-
ing that CommonIT gets the best score (37.1). To
further ensure a fair comparison, we supplemented
our evaluation with GPT assessments and results
from Alpaca-Eval (Dubois et al., 2023). Com-
monIT achieves better results in dialogue scenariosModelMMLU BBH TydiQA Codex-Eval Win Rate
(Standard Error)A VG.
0-shot 5-shot Direct CoT F1 P@10
Alpaca IT 34.8 36.4 32.6 33.0 37.4 22.6 36.5 (1.7) 33.3
Alpagasus (Chen et al., 2024) 38.1 37.4 31.6 33.4 44.3 22.8 37.7 (1.7) 35.0 (+1.7)
Alpaca CommonIT 40.4 40.1 33.5 34.6 38.9 24.7 47.3 (1.7) 37.1 (+3.8)
Table 6: Comparison of state-of-the-art method (Alpagasus) focusing on high-quality data and IT using high&low
quality data on LLaMa 7B. We further compare Win Rate on AlpacaEval as evaluated by ChatGPT for evaluating
open-ended intractability. CommonIT (Length) significantly outperforms the comparison methods in Win Rate,
indicating that it effectively learned the difference between instructions for high and low-quality data.
/uni00000017/uni00000013
 /uni00000015/uni00000013
 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013
Distance: 22.46LLaMa
/uni00000017/uni00000013
 /uni00000015/uni00000013
 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013
Distance:21.08LLaMa IT
/uni00000017/uni00000013
 /uni00000015/uni00000013
 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013
Distance:20.43LLaMa  CommonIT
Figure 3: TSNE plots for MMLU with 10 question types across four disciplines (Humanities, Social, STEM
and Other). Clusters are tighter and distinguishable in our proposed CommonIT (Embedding), indicating that
CommonIT can better differentiate the question’s discipline type.
gained close to a 10-point boost, indicating that our
method obtains more reliable and higher-quality
replies. Alpagasus (Chen et al., 2024) belongs to
data filtering approaches, whereas ours is a data
learning approach. They focus on learning from a
subset of high-quality data, while CommonIT en-
ables the model to learn from good and bad data.
The results show that CommonIT demonstrates a
superior understanding and learning of High&Low-
quality instructions.
Question Representation Capacity In Figure 3,
comparative results for T-SNE (Van der Maaten
and Hinton, 2008) from the question embedding
for 10 randomized categories in MMLU are visual-
ized. We compute their average pairwise distance
within the embedding space. A reduced distance
indicates a superior model’s ability to aggregate
similar task-related questions. As the reduced aver-
age distance indicates, our CommonIT aggregates
similar questions more effectively than the LLaMa
IT and LLaMa. It can also be seen that after instruc-
tion tuning, the model can discriminate between
different task instructions.
Correlation Between Two Strategies in Cluster-
ing Results Table 7 illustrates the relationship
between the dataset divided by length and the em-
bedding metrics. For datasets without clustering
(original FLAN, FLAN CoT, and Alpaca), we sam-
pled a total of 500 samples, referred to as “Vanilla.”For the multiple sub-datasets clustered by length,
we sampled 500 samples from each sub-dataset and
calculated the average results, denoted as “Length.”
We categorized these sampled instances according
to the embedding metric and reported the average
results of the total number of embedding categories
after conducting ten runs. The findings indicate
that the number of embedding categories decreases
with length clustering, suggesting that data of the
same length exhibits more similar embedding rep-
resentations.
Dataset Vanilla Length
FLAN 42.0 34.2
FLAN CoT 39.3 36.5
Alpaca 55.5 52.8
Table 7: The number of embedding categories contained
in randomly sampled samples from different datasets.
Model Generalization Figure 4 shows the loss
curves for the training of the model (left) and the
results of the MMLU 0-shot evaluation at different
training epochs (right). As can be seen from the
loss curves, compared to the baseline method, Com-
monIT exhibits a smaller loss, indicating better
generalizability of our approach. We compared the
out-of-domain test of MMLU results across differ-
ent training epochs, demonstrating that our model
improved further with longer training. This obser-/uni00000019/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000018/uni00000013 /uni00000019/uni00000014/uni00000013/uni00000013 /uni00000019/uni00000014/uni00000018/uni00000013 /uni00000019/uni00000015/uni00000013/uni00000013 /uni00000019/uni00000015/uni00000018/uni00000013 /uni00000019/uni00000016/uni00000013/uni00000013
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000016/uni00000013/uni00000013/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000032/uni00000058/uni00000055/uni00000056
Figure 4: The figure on the left shows the training loss
varies with the training step (Epoch 2). The bar chart on
the right shows our results with the baseline for MMLU-
0 shot at different training epochs. This indicates that
CommonIT achieves a lower final language modeling
loss than the baseline and that extending the number of
training epochs further improves the performance.
vation indicates that the CommonIT-trained model
remains underfitting compared to the baseline at
the same number of steps. Unlike our method,
the baseline model showed a little decrease with
more training epochs, hinting that IT might limit
the model’s generalization.
6 Conclusion
We present a simple and effective fine-tuning
method CommonIT. Leveraging data commonal-
ity with three metrics significantly enhanced the
effectiveness of LLMs across multiple competency
dimensions during the IT. The evaluation across
diverse models, IT datasets, and specific tasks has
showcased the methodology’s application scalabil-
ity. Ablation experiments across various stages and
data clustering techniques have illustrated the effec-
tiveness of our method. Explorations of common-
alities confirm that CommonIT mirrors the human
learning process, improving overall performance.
Limitations
There are several limitations of our work. Our limi-
tations are primarily constrained by experimental
resources, such as available GPU memory capacity
(4*80G). Due to these constraints, we cannot test
more models at different scales (30B-65B). The
group approach we used was not further selected,
such as embedding, which was only analyzed for
MMLU and may lead to further room for improve-
ment in the final results. We tried some intuitive
methods of data categorization, but more complex
ones were not considered. The theory behind the
improvements remains to be revealed. Apart from
empirical explanations, we believe further investi-
gations (e.g., mathematically provable bound) will
be useful. Given the balance between the numberof groups and batch size, when the overall training
data is fixed, having more groups means less data
within each group. If the batch size is too large,
some groups may have fewer data points than the
batch size, which may lead to some performance
degradation.
Ethics Statement
Our work follows the ACL Ethics Policy. Our find-
ings are based on publicly available datasets for re-
producibility purposes. LLMs can contain potential
racial and gender bias. Therefore, if someone finds
our work interesting and would like to use it in a
specific environment, we strongly suggest the user
check the potential bias before usage. In addition,
it is hard to control the generation of LLMs. We
should be aware of the potential problems caused
by hallucinations.
Acknowledgments
This work was supported in part by the
National Natural Science Foundation of
China (Grant No. 62206076), Guangdong
Basic and Applied Basic Research Foun-
dation (Grant No. 2024A1515011491),
Shenzhen Science and Technology Program
(Grant Nos. ZDSYS20230626091203008,
KJZD20231023094700001,
RCBS20221008093121053), and Shen-
zhen College Stability Support Plan
(Grant Nos. GXWD20220811173340003,
GXWD20220817123150002). We would like to
thank the anonymous reviewers and meta-reviewer
for their insightful suggestions.
References
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-
Ari, Steven Hand, Hadi Hashemi, Le Hou, JoshuaHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-
jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,
Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
Frederick Liu, Marcello Maggioni, Aroma Mahendru,
Joshua Maynez, Vedant Misra, Maysam Moussalem,
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley, Alex Castro Ros, Au-
rko Roy, Brennan Saeta, Rajkumar Samuel, Renee
Shelby, Ambrose Slone, Daniel Smilkov, David R.
So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven
Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
report.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
ICML .
Lichang Chen, Shiyang Li, and Jun Yan. 2024. Alpaga-
sus: Training a better alpaca model with fewer data.
InICLR .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021a. Evaluat-
ing large language models trained on code.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021b. Eval-
uating large language models trained on code. In
ArXiv .
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan
Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao
Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang
Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix:
Democratizing chatgpt across languages. In ArXiv .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Web-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
gun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,
Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.
2022. Scaling instruction-finetuned language models.
InArXiv .
Jonathan H. Clark, Jennimaria Palomaki, Vitaly Niko-
laev, Eunsol Choi, Dan Garrette, Michael Collins,
and Tom Kwiatkowski. 2020. Tydi QA: A bench-
mark for information-seeking question answering in
typologically diverse languages. Trans. Assoc. Com-
put. Linguistics , 8:454–470.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. In ArXiv .
Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu,
Osamu Yoshie, and Yubo Chen. 2022. Contrastive
vision-language pre-training with limited resources.
ECCV .
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Alpaca-
farm: A simulation framework for methods that learn
from human feedback. In NeurIPS .
Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li,
Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu.
2024. Reformatted alignment. In Findings of the
Association for Computational Linguistics: EMNLP
2024 .
Tao Feng, Zifeng Wang, and Jimeng Sun. 2023. Cit-
ing: Large language models create curriculum for
instruction tuning. In ArXiv .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In ICLR .Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,
Tianlu Wang, Qing Liu, Punit Singh Koura, et al.
2022. Opt-iml: Scaling language model instruction
meta learning through the lens of generalization. In
ArXiv .
Joongwon Kim, Akari Asai, Gabriel Ilharco, and Han-
naneh Hajishirzi. 2023. Taskweb: Selecting better
source tasks for multi-task nlp. In EMNLP .
Bruce W. Lee, Hyunsoo Cho, and Kang Min Yoo. 2023.
Instruction tuning with human curriculum. In ArXiv .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021. What
makes good in-context examples for gpt-3? In Arxiv .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. In ICML .
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-
ley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-
ham Fikri Aji, Khalid Almubarak, Samuel Albanie,
Zaid Alyafeai, Albert Webson, Edward Raff, and
Colin Raffel. 2023. Crosslingual generalization
through multitask finetuning. In ACL.
OpenAI. 2023. GPT-4 technical report. In ArXiv .
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
Gonzalez. 2023. Gorilla: Large language model
connected with massive apis. In ArXiv .
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
GPT-4. In ArXiv .
Emmanouil Antonios Platanios, Otilia Stretcu, Graham
Neubig, Barnabas Poczos, and Tom Mitchell. 2019.
Competence-based curriculum learning for neural
machine translation. In NAACL .
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: memory optimizations
toward training trillion parameter models. In SC,
page 20. IEEE/ACM.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In KDD .
Thomas Scialom, Tuhin Chakrabarty, and Smaranda
Muresan. 2022. Fine-tuned language models are
continual learners. In EMNLP .
Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne
Longpre, Jason Wei, Hyung Won Chung, Barret
Zoph, William Fedus, Xinyun Chen, et al. 2023.
Flan-moe: Scaling instruction-finetuned language
models with sparse mixture of experts. In ArXiv .Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, and
Deng Cai. 2023. Specialist or generalist? instruction
tuning for specific nlp tasks. In EMNLP .
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. 2023. One
embedder, any task: Instruction-finetuned text em-
beddings. In ACL (Findings) .
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong
Zhou, Zhenfang Chen, David Daniel Cox, Yiming
Yang, and Chuang Gan. 2024. SALMON: Self-
alignment with instructable reward models. In The
Twelfth International Conference on Learning Repre-
sentations .
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V . Le, Ed Chi, Denny
Zhou, and Jason Wei. 2023. Challenging big-bench
tasks and whether chain-of-thought can solve them.
InACL (Findings) .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. In ArXiv .
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-
Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel
Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,
et al. 2024. Aya model: An instruction finetuned
open-access multilingual language model. In Arxiv .
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research , 9(11).
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hannaneh Hajishirzi. 2023. How
far can camels go? exploring the state of instruction
tuning on open resources. In NeurIPS .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran, An-
jana Arunkumar, David Stap, Eshaan Pathak, Gi-
annis Karamanolakis, Haizhi Gary Lai, Ishan Puro-
hit, Ishani Mondal, Jacob Anderson, Kirby Kuz-
nia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-
tel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-
hit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit
Verma, Ravsehaj Singh Puri, Rushang Karia, Savan
Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,
Sujan Reddy A, Sumanta Patro, Tanay Dixit, andXudong Shen. 2022. Super-naturalinstructions: Gen-
eralization via declarative instructions on 1600+ NLP
tasks. In EMNLP .
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022a. Finetuned
language models are zero-shot learners. In ICLR .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022b. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
NeurIPS .
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and
Lingming Zhang. 2023. Magicoder: Source code is
all you need. In ArXiv .
Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and
Percy Liang. 2023. Data selection for language mod-
els via importance resampling. In NeurIPS .
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
2023. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. In
ArXiv .
Longhui Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, Meishan Zhang, and Min Zhang. 2024.
A two-stage adaptation of large language models
for text ranking. In ACL (Findings) , pages 11880–
11891.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models. In ArXiv .
Hao Zhao, Maksym Andriushchenko, Francesco Croce,
and Nicolas Flammarion. 2024. Long is more for
alignment: A simple but tough-to-beat baseline for
instruction fine-tuning. In ArXiv .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
ArXiv .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023. LIMA:
Less is more for alignment. In NeurIPS .A Appendix
A.1 Benchmarks
•MMLU MMLU consists of a series of ques-
tions, ranging from basic to professional lev-
els, across 57 academic subjects. Its multiple-
choice format facilitates a relatively straight-
forward testing process. We use the official
MMLU evaluation script and prompts3, with
modifications to allow for batch processing.
We evaluate using 0 and 5 few-shot examples,
following the original setup of MMLU.
•BBH We follow the setup described in the
original paper (Suzgun et al., 2023), and eval-
uate with and without chain-of-thought (CoT
vs Direct). The officially provided prompts,
with three few-shot in-context examples, are
used for both CoT and Direct setups. For
the CoT setup, we extract the first word after
the phrase ‘So the answer is,’ or the entire
response if no such substring is present.
•TydiQA We adhere to the protocol delin-
eated in the PaLM 2 technical report (Anil
et al., 2023). This approach assesses the mod-
els’ proficiency in responding to multilingual
queries, particularly when the definitive gold
passage containing the answer is provided (re-
ferred to as GoldP/GP). One in-context exam-
ple is incorporated to acclimate the model to
the expected answering format. Here we fol-
low previous work (Sun et al., 2024; Üstün
et al., 2024) and report the GP score.
•Codex-Eval For evaluating the coding capa-
bilities of the models, we employ the Hu-
manEval dataset presented in the Codex pa-
per (Chen et al., 2021a). This dataset encom-
passes 164 programming challenges, wherein
models are prompted to finalize a Python func-
tion based on its provided docstring. Fol-
lowing the original paper, we calculate the
pass@k to gauge the functional accuracy of
the models’ outputs. Our findings are pre-
sented as pass@10 results, employing a tem-
perature setting of 0.8.
•Alpaca Eval We use the code provided by
Dubois et al. (2023). We adopt Davinci-
003 reference text generated by Wang et al.
(2023). We greedily decode up to 1024 tokens
3https://github.com/hendrycks/testand then pairwise compare these responses
with those from Davinci-003. The reported
win-rate is the percentage of model genera-
tions that ChatGPT reports as being preferred
over the generations from Davinci-003.
•GSM The mathematical reasoning capabil-
ities are enhanced through the use of the
GSM8K dataset, which consists of 8.5K high-
quality arithmetic word problems designed
for the grade school level. The samples are
divided into 7.5K training and 1K test prob-
lems.
•Openfunctions Proficiency in using the tool
is assessed by leveraging function-calling
datasets, including the Gorilla Openfunctions
dataset. The training set contains 2211 sam-
ples, while the test set contains 112.
•Code The code generation skills are boosted
using MagiCoder (Wei et al., 2023) of 2,000
samples. The test set includes 164 samples,
and the evaluation is conducted using the Hu-
manEval dataset (Chen et al., 2021b).
A.2 Overall Learning Strategy
Algorithm 1 illustrates the overall training flow of
CommonIT. Besides the component and training
flow of LLMs, only some low-cost operations, such
as grouping, have been included in the pre-process
to get many sub-datasets ( D0...Dn), allowing an
easy implementation as a practical language model.
A.3 Training Details
We set learning rate to 5×10−6and set batch size
to 32, with no weight decay and a learning rate
with linear decay and linear warmup for 3% of the
total training steps. We use a maximum sequence
length of 2048, truncating samples where necessary.
During training, we make use of the DeepSpeed
library (Rasley et al., 2020) and ZeRO (Rajbhan-
dari et al., 2020) optimizer to allow for large-scale
model finetuning. For FLAN and CoT, we train
models for two epochs. For Alpaca, we train mod-
els for three epochs. For retrieval settings, we take
the categories (57 task categories) classified by
MMLU as the categories to be classified, use the
data in the development set as the database, and
classify the data categories in the dataset to the ones
that are the most similar to the data in the database
by doing a similarity ordering with the data in theModel MMLU 0 SHOT MMLU 5 SHOT BBH Direct BBH CoT TydiQA Codex-Eval Avg.
BS 32 40.4 40.1 33.5 34.6 38.9 24.7 35.4
BS 64 40.9 39.4 32.3 33.1 38.4 26.1 35.0
BS 128 40.7 40.9 33.7 33.0 41.6 25.9 36.0
Table 8: CommonIT results at different batch sizes (BS).
Model MMLU 0 SHOT MMLU 5 SHOT BBH Direct BBH CoT TydiQA Codex-Eval Avg.
BS 32 34.8 36.4 32.6 33.0 37.4 22.6 32.8
BS 64 39.2 39.7 31.7 33.0 38.2 23.4 34.2
BS 128 39.0 38.8 29.6 34.6 35.3 23.4 33.5
Table 9: IT results at different batch sizes (BS).
Model MMLU 0 SHOT MMLU 5 SHOT BBH direct BBH CoT TydiQA Codex-Eval Avg.
IT 45.3 47.0 36.9 37.5 39.8 22.5 38.2
CommonIT 47.8 49.4 36.5 35.1 48.6 24.8 40.4 (+2.2)
Table 10: Mixed results for both FLAN and Alpaca datasets.
Algorithm 1 CommonIT for LLMs
Input: An IT Dataset D(s,x,y), Model θ.
Output: Fine-tuned Model θ.
1:function COMMON IT(s,x,y,θ).
2: Split the dataset Dinto clusters
D0,D1. . .Dnbased on specific rules.
3: foreach cluster Diin{D0,D1, . . . ,Dn}
do
4: Construct mini-batches Bi
1,Bi
2, . . .
from dataset Di.
5: end for
6: Randomly shuffle all mini-batches to ob-
tain the bucketed sets B∗
1,···,B∗
t,···,B∗
Tfor
a total of Ttraining steps.
7: foreach training step tin{1,2, . . . , T }do
8: Generate training batch B∗
tuniformly.
9: Update θwith batch loss calculated by
Eq.2.
10: end for
11:end function
database. In all cases, we fully finetune models.
We trained models with 4 A800-80G GPUs.
A.4 Details of Group Methods
Regarding task division, we use the task types pro-
vided by the original FLAN (Chung et al., 2022),
such as translation tasks, QA tasks, etc., for practi-
cal data categorization. The statistical distribution
of the divided dataset is shown in Figure 5
For embedding division, we employ the top- k
algorithm from (Liu et al., 2021) to select the top
8 results based on similarity. We then classify theModelMMLU
0/5 SHOTBBH
Direct/CoT
BLOOMZ 7B 42.1/37.3 28.1/12.8
+IT 39.6/41.3 30.1/27.3
+CommonIT 42.7/41.8 30.1/28.6
LLaMa2-7B 36.6/45.8 34.1/41.8
+IT 50.0/51.0 40.0/41.3
+CommonIT 50.8/52.4 40.6/41.8
LLaMa 13B 42.4/46.9 38.7/36.9
+IT 46.1/47.0 39.3/39.6
+CommonIT 47.1/48.9 39.5/42.0
Table 11: More Tuning Results of MMLU/BBH on
FLAN for Different Models with CommonIT.
entry with the highest number of votes in the top
8 as the final retrieved item, with other parameter
settings as described in (Liu et al., 2021). In the
simplest scenario, where k=1, we categorize each
query data point by assigning it to the category of
its most closely related data point. For example,
if the label of the nearest data point is “anatomy”,
then the query data is classified under the “anatomy”
category. This process is applied uniformly across
all the data points (queries) that need categorization,
effectively segregating the dataset into 57 distinct
sub-datasets. The statistical distributions of the
divided datasets are shown in Figure 6, 7 and 9.
Regarding length division, we divide the data
based on the length of the sequences. We first com-
pile statistics on sentence lengths within the dataset
and then divide the data as evenly as possible into
corresponding length intervals. The statistical dis-
tributions of the divided datasets are shown in Fig-
ure 8, 10 and 11.Figure 5: The FLAN dataset groups by task.
A.5 More Results
Due to space constraints and the consistency of the
results, we did not put the results of 5-shot and CoT
to save space. We add these results to the appendix.
The results are as Table 11 and Table 12.
A.6 Further Analysis
Effect of Batch Size we added experiments re-
lated to larger batch sizes in Table 8 and 9. Be-
cause hyperparameter search is time-consuming,
we adopted the same learning rate and conducted
experiments with batch sizes of 64 and 128. These
experiments were conducted using the Alpaca
dataset. The results show that CommonIT is still
better than IT in this setting.Effect of Data Mixing In the Table 10, we have
supplemented the experiments with a mix of FLAN
and Alpaca. The results indicate that our method
remains effective (+2.2) .
Effect of Inference Seeds In Generation The
test results with different random seeds in infer-
ence are shown in Table 13 and 14. Our findings
indicate that the results exhibited minimal variabil-
ity, ultimately showing that the fluctuations were
insignificant when averaged across multiple tasks.
The fluctuations in TydiQA’s results are due to the
occasional occurrence of models not answering in
the corresponding language after training (both IT
baselines and our CommonIT). This fluctuation
is minor in our method and more prominent inFigure 6: The FLAN dataset groups by embedding.
ModelMMLU
0/5 SHOTBBH
Direct/CoT
FLAN 46.7/47.9 39.7/39.9
w/o GD 44.2/45.2 38.7/37.2
w/o FP 44.4/45.8 36.0/35.4
FLAN CoT 38.7/42.3 33.4/35.2
w/o GD 41.3/42.5 33.7/31.3
w/o FP 37.5/38.5 32.6/31.4
Alpaca 40.4/40.1 33.5/34.6
w/o GD 34.8/36.4 32.6/33.0
w/o FP 39.2/35.9 15.6/32.6
Table 12: More Ablation Study of MMLU/BBH on
CommonIT.
the baseline IT (Standard Deviation: 0.7 v.s. 2.2),
which also shows the stability of our method.
Effect of Length For Training We segmented
the Alpaca data into three categories based on
length—Short (comprising the shortest 14,393 datapoints), Medium (containing 22,115 data points of
medium length), and Long (including the longest
15,494 data points). We trained separate models on
these datasets and subsequently compared the mul-
titasking generation results of IT and CommonIT,
as illustrated in Table 15. Typically, the model ex-
hibits a length bias (Zhao et al., 2024), whereby
the length of generated sequences is influenced by
the training data. Specifically, if the training data
consists of longer sequences, the model tends to
generate longer outputs. The data presented in Ta-
ble 15 reveal that performance metrics for Long,
Medium, and Base models sequentially decrease
and surpass those of IT when fine-tuning the en-
tire Alpaca dataset. This suggests that different
data lengths distinctly impact the model’s capa-
bilities: for example, coding tasks benefit from
models generating longer outputs (with Long per-
forming optimally for Code), whereas knowledgeModel MMLU 0 SHOT MMLU 5 SHOT BBH Direct BBH CoT TydiQA Codex-Eval Avg.
Seed42 34.8 36.4 32.6 33.0 37.4 22.6 32.8
Seed46 33.3 35.7 29.8 32.2 42.9 22.4 32.7
Seed50 33.4 35.8 31.4 32.2 40.4 22.9 32.7
Table 13: Alpaca IT results with different inference seeds.
Model MMLU 0 SHOT MMLU 5 SHOT BBH Direct BBH CoT TydiQA Codex-Eval Avg.
Seed42 40.4 40.1 33.5 34.6 38.9 24.7 35.4
Seed46 40.4 40.4 32.2 33.5 39.7 23.8 35.0
Seed50 40.5 40.1 32.6 33.1 40.5 24.0 35.1
Table 14: Alpaca CommonIT results with different inference seeds.
Figure 7: The FLAN CoT dataset groups by embedding.
reasoning tasks favor shorter outputs (with Short
performing best for MMLU and BBH, and QA).
In contrast, IT performs less than these specialized
training approaches, indicating the model’s limita-
tions in adapting output lengths for various tasks.
Conversely, CommonIT demonstrates superior re-
sults, suggesting that the model learns and applies
task-specific information of different lengths moreeffectively, enhancing its overall capabilities.
A.7 Case Study
Figure 12 shows some typical output examples.
We offer three tasks: factual Q&A, summarization,
and grammar correction. Overall, our approach
obtains higher quality response results and a cor-
rect understanding of the instructions. However,Model MMLU 0 SHOT MMLU 5 SHOT BBH Direct BBH CoT TydiQA Codex-Eval Avg.
Short 38.2 38.2 34.4 32.3 38 19.2 33.4
Medium 37.6 38.2 31.2 31.9 36.9 25.7 33.6
Long 35.7 37.0 33.8 32.8 37.7 26.6 33.9
IT 34.8 36.5 32.6 33.0 37.4 22.6 32.8
CommonIT 40.4 40.1 33.5 34.6 38.9 24.7 35.4
Table 15: Effect of the length metric of multidimensional measurements.
Figure 8: The FLAN dataset groups by length.
our approach only improves the model’s genera-
tion results to a limited extent, i.e., it still needs to
improve on the hallucination problem (Shen et al.,
2023) suffered by large models (which are prone
to generate additional content in the factual Q&A
task). In contrast, the model trained by our method
comes close to attempting to answer the question,
although this model answers it incorrectly. For the
summarization task, the baseline tries to answer
the question, but he does not know how to answer
it, whereas our model can answer the key points
and list the points to answer. Our model answered
the question directly without outputting additional
content for the grammar correction task.Figure 9: The Alpaca dataset groups by embedding.
Figure 10: The FLAN CoT dataset groups by length.
 Figure 11: The Alpaca dataset groups by length.Human: Who was President of the United States in 1955?
Baseline: Dwight D. Eisenhower  was the President of the United States in 1955.\nAsked in History of the United States, 
US Presidents, Dwight D. Eisenhower\nWhat was the name of the president in 1955?\nDw...
Ours: Dwight D. Eisenhower. \nWhat was the name of the first African American to be elected to the United States Senate?
\nWhat was the name of the first African American to be elected to the United States House of Representatives?\nWhat was 
the name of the first African American to be elected to...Factual Questions and Answers
Human: What are the five key points I should learn from studying quantum mechanics?
Baseline: What are the five key points I should learn from studying quantum mechanics?\nI'm a high school student and I'm 
taking a physics course. I'm having trouble with the quantum mechanics part of the course. I'm not sure what I should be 
learning from this section. I...
Ours: The five key points  I should learn from studying quantum mechanics are:\n1. Quantum mechanics is the study of the 
behavior of matter and energy on the atomic and subatomic level.\n 2. Quantum mechanics is...Text Generation, Academic, Key Points Summary
Human: Correct this to Standard English: She no went to the market.
Baseline: She no went to the market.\n She did not go to the market.
Ours:  She didn't go to the market.Text Generation, Grammar Correction, English
Figure 12: Case studies comparing our proposed method with the baseline instruction tuning. Overall, our method
demonstrated a significant reduction in generating irrelevant content and answered more relevant to the task and
formatted in a way consistent with the logic of the task.