ECCO: Can We Improve Model-Generated Code Efficiency
Without Sacrificing Functional Correctness?
Siddhant Waghjale*Vishruth Veerendranath*Zora Zhiruo Wang Daniel Fried
Language Technologies Institute, Carnegie Mellon University
{swaghjal, vveerend, zhiruow, dfried}@cs.cmu.edu
Abstract
Although large language models (LLMs) have
been largely successful in generating function-
ally correct programs, conditioning models to
produce efficient solutions while ensuring cor-
rectness remains a challenge. Further, unre-
liability in benchmarking code efficiency is
a hurdle across varying hardware specifica-
tions for popular interpreted languages such
as Python. In this paper, we present ECCO ,
a reproducible benchmark for evaluating pro-
gram efficiency via two paradigms: natural lan-
guage (NL) based code generation and history-
based code editing. On ECCO , we adapt and
thoroughly investigate the three most promis-
ing existing LLM-based approaches: in-context
learning, iterative refinement with execution or
NL feedback, and fine-tuning conditioned on
execution and editing history. While most meth-
ods degrade functional correctness and moder-
ately increase program efficiency, we find that
adding execution information often helps main-
tain functional correctness, and NL feedback
enhances more on efficiency. We release our
benchmark to support future work on LLM-
based generation of efficient code.1
1 Introduction
The ability to write efficient code is a cornerstone of
software development (Li et al., 2022). While large
language models (LLMs) have shown remarkable
progress in generating functionally correct code
(Roziere et al., 2023; Guo et al., 2024), the ability to
generate solutions that are both correct and efficient
remains elusive (Shypula et al., 2021, 2024).
Current methods for optimizing program effi-
ciency improve performance measured by execu-
tion time. However, this apparent success often
comes at the cost of severely decreasing the func-
tional correctness (Shypula et al., 2024). An exam-
*Equal contribution, ordered by a coin toss.
1https://github.com/CodeEff/ECCOple of this issue is illustrated in Figure 1: When op-
timizing the program on the left, models sometimes
perform spurious optimizations that, although they
reduce the runtime, make the program no longer
functionally correct so that it fails all test cases.
On the other hand, a correct optimization (bottom
right) — that improves efficiency while maintain-
ing functional correctness — is often harder to
achieve for current LMs. This spurious optimiza-
tion is certainly undesirable in practice, and can
even increase debugging time for software devel-
opers (Li et al., 2022; Cummins et al., 2023a). To
achieve the goal of real and robust program opti-
mization, we ask: Can LMs improve program effi-
ciency without sacrificing functional correctness?
In this work, we curate an efficiency-oriented
programming benchmark ECCO , short for
Ensuring Correctness in Code Optimizations,
which enables program evaluation in three aspects:
execution correctness, runtime efficiency, and mem-
ory efficiency. ECCO supports two optimization
paradigms: (i) history-based code editing: based
on a previous version of the program, test if an LM
can further optimize the code while maintaining
its correctness, and (ii) NL-based code generation:
test the efficiency of a program generated by an
LM given a programming problem described in
NL. We collect over 50 kPython solution pairs,
spanning 1.3 kcompetitive programming problems
(Puri et al., 2021), with an average of 3.1 public and
17.3 private test cases to support reliable execution-
based evaluations of correctness and efficiency.
Further, to perform reliable and reproducible
executions, we introduce an evaluation setup using
a cloud-hosted code execution engine, JUDGE 0
(Došilovi ´c and Mekterovi ´c, 2020), which produces
stable execution output on correctness, runtime,
and memory usage, thanks to its agnostic nature
to local hardware specifications. It supports up to
66 programming languages (PLs), allowing future
work to extend to other languages.arXiv:2407.14044v2  [cs.CL]  9 Oct 2024# Program to search for a number 
def search (list , target ):
 for i, element  in 
enumerate (list ):
   if element  == target :
     return  i
 return  -1Problem: Write a program to search for a  
number in a list # Program to search for a number 
def search (list , target ):
 left , right  = 0, len(list ) - 1
 while  left  <= right :
   mid = ( left  + right ) // 2
   if list [mid] == target : return  mid
   elif  list [mid] < target : 
left  = mid - 1 # Bug 
   else : right  = mid + 1 # Bug 
 return  -1spurious  
optimization 
# Program to search for a number 
def search (list , target ):
 left , right  = 0, len(list ) - 1
 while  left  <= right :
   mid = ( left  + right ) // 2
   if list [mid] == target : return  mid
   elif  list [mid] < target : 
left  = mid + 1 # Fixed 
   else : right  = mid - 1 # Fixed 
 return  -1
correct  optimization 
500 ms 
 pass test 
120 ms fail test 
 150 ms 
 pass test Figure 1: Correctness-preserving versus spurious optimization when optimizing a linear search algorithm with
binary search on a sorted list. Spurious optimization can reduce runtime, but add errors that cause the program to be
incorrect. In contrast, a true optimization reduces runtime and remains correct, as we emphasized in ECCO.
To explore various correctness-preserving pro-
gram optimization methods, we evaluate three
classes of methods on ECCO — in-context learn-
ing, iterative refinement, and fine-tuning, across a
suite of open-source language models pre-trained
on code — which previous works have used to
attempt to improve efficiency, while however over-
looking their effects on correctness. We find that ex-
ecution information and fine-tuning help maintain
functional correctness, and NL-involved prompt-
ing often yields higher efficiency improvements.
However, we broadly reconfirm findings that no ex-
isting methods can improve time/space efficiency
without sacrificing functional correctness. We hope
ECCO can serve as a solid testbed for program op-
timization, and call for more efforts in advancing
correctness-preserving program optimizations.
2 Related Work
Benchmarks for Code Efficiency Some works
have proposed benchmarks for optimizing pro-
gram assembly code (Bunel et al., 2016; Shypula
et al., 2021; Shi and Zhang, 2020; Cummins et al.,
2023b). More recently, Shypula et al. (2024) target
C++ program speedups, and Huang et al. (2024)
evaluate the efficiency of Python solutions for
LeetCode coding interview problems (Niu et al.,
2024). Although most efforts on LLM-based code
generation focus on evaluating functional correct-
ness (e.g., Chen et al. 2021), some works evalu-
ate code efficiency (Moudgalya et al., 2023; Sikka
et al., 2020; Jeon et al., 2023; Baik et al., 2024)
by classifying the time complexity of programs.
However, these works are limited in their single-
reference evaluation paradigm, assembly language
support, or by the limited problem space on Leet-
Code (Coignion et al., 2024). Our work supports
reliable evaluation across arbitrary coding prob-
lems and the widely-used Python language.Evaluating Program Efficiency It is challeng-
ing to robustly evaluate program efficiency, due
to varying hardware platforms and setups. Previ-
ous works have evaluated the efficiency of code
by executing code in a local software environment
(Singhal et al., 2024; Huang et al., 2024) or us-
ing containerized environments on local hardware
(Khan et al., 2023), but this can result in varying
runtime and memory usage across hardware, thus
causing irreproducible evaluations. An alternative
approach is to use an architecture simulator (Shy-
pula et al., 2024) which ensures the execution of
each program is exactly simulated at the hardware
level, but is limited to compiled languages such
as C++. A hardware counter though reliable (Liu
et al., 2024), cannot measure memory usage. For
popular interpreted languages such as Python and
Java, some use LeetCode’s execution engine (Niu
et al., 2024; Coignion et al., 2024), but with a re-
stricted space of testable problems. In our work,
we propose an evaluation setup using an accessible
cloud computing instance that ensures consistent
virtual hardware and reliable benchmarking.
Program Optimization Approaches To start,
some works explore in-context learning to opti-
mize program efficiency (Huang et al., 2024), with
retrieval methods to select relevant examples (Gao
et al., 2024; Shypula et al., 2024). Beyond vanilla
prompting, iterative prompting methods (Madaan
et al., 2024; Shinn et al., 2024; Ridnik et al., 2024)
have been explored to improve specific aspects of
generation, by incorporating feedback from an LM
or external modules. Meanwhile, finetuning has
been proposed with self-play, synthetic preferences
and problem-oriented data (Shypula et al., 2024;
Gee et al., 2024; Ye et al., 2024). However, none
of these methods have been rigorously studied for
correctness-preserving optimization. We fill in this
gap and provide systematic studies of all methods.3 The ECCO Benchmark
In this section, we first introduce our evaluation
platform (§3.1), then describe the construction pro-
cess of our ECCO benchmark (§3.2), and lastly,
introduce our two task formulations with corre-
sponding evaluation metrics (§3.3).
3.1 Evaluation Platform
To reliably evaluate program efficiency in both run-
time and memory usage, we need to first establish a
robust and reproducible evaluation platform. How-
ever, evaluating program efficiency is challenging,
as resource usage statistics vary greatly across hard-
ware and setups (Singhal et al., 2024; Huang et al.,
2024).
To ensure stable execution for interpreted lan-
guages such as Python, we propose to use a repro-
ducible cloud compute instance that ensures the
same virtual hardware, as illustrated in Figure 2.
Specifically, we use an EC2 instance (detailed in
§B), and execute the code within a code execution
engine JUDGE 0(Došilovi ´c and Mekterovi ´c, 2020).
Note that our recipe can easily extend to over 60
programming languages that the JUDGE 0engine
supports. This is set up as a sandboxed Docker con-
tainer within the instance, which thus ensures an
isolated setup for secure and reproducible code ex-
ecution. Our platform is similar to evaluating gen-
erated code on LeetCode execution console (Niu
et al., 2024; Coignion et al., 2024), but applies
to arbitrary coding problems and is not limited to
questions available on LeetCode.2
# Search for a number
def search(list, target):
  for i, element in 
enumerate (list):
    if element == target:
      return i
  return -1HTTP  Request
Test1: [1, 2], 1
Test2: [5, 6, 7], 1
Execution Results
Test 1: ✔  Test2: ❌
⏱  200ms 
💾 172 KBAWS EC2 instance
(m7i.large)
Judge0  
(Sandboxed
and Isolated)
Figure 2: Evaluation platform using J UDGE 0.
3.2 Benchmark Construction
Our goal is to collect programming problems, each
with an NL description, and multiple functionally-
correct solutions at varied efficiency levels.
Problem Selection We collect programming
problems from the IBM CodeNet dataset (Puri
2We make the Amazon Machine Image (AMI) for the
setup available to enable reproducible benchmarks. We leave
evaluations in other languages for future work.et al., 2021), which contains competitive program-
ming problems with NL descriptions, user program
submissions, and other metadata, scraped from the
AIZU and AtCoder online judging systems. Co-
deNet problems mostly require algorithmic tech-
niques such as data structure optimization.
Specifically, we first convert CodeNet to ∼187k
(slow, fast) Python code pairs following Shypula
et al. (2024), where each pair of programs has two
solutions for the same coding problem. We con-
verted all Python 2 solutions to Python 3 using
lib2to3.3Lastly, we filter out spurious program
pairs in which the ‘fast’ code was in fact slower
when evaluated on our setup.4
Next, we split the pairs and group programs by
their associated problem ID. We filter out all prob-
lems with less than two solutions to ensure that
each NL problem description has multiple asso-
ciated solutions, to enable program optimization
based on code editing history. We then remove the
programs that are repetitive or cannot successfully
execute due to syntax errors or test case failures.
Our curated dataset was partitioned into three
subsets: train, validation, and test, with each split
consisting of a distinct set of problems. In the end,
the process yields 1,380 unique problems in ECCO
in total.
Test Case Collection To evaluate functional cor-
rectness, we require test cases. We collect (i) the
original test cases for each problem from CodeNet,
and (ii) the additional tests from the AlphaCode
project (Li et al., 2022). Each test case contains the
program inputs as well as expected outputs when
executing canonical program solutions on these in-
puts. With these two sets of test cases, we simulate
a realistic coding setting where one can refer to
(i) as the public test cases for debugging or other
accuracy-improving purposes, Tpublic , and (ii) as
private test cases to conduct final execution-based
evaluations on the programs, Tprivate .
3.3 Task Formulation and Evaluation
We propose two formulations for the program op-
timization task, namely NL-instructed generation
andhistory-based program editing . In this section,
we introduce the data we use for each formulation,
and our evaluations of program correctness, run-
time, and memory usage.
3https://docs.python.org/3/library/2to3.html
4Speed statistics reported in CodeNet may be inconsistent.3.3.1 History-Based Program Editing
Our first paradigm follows previous work on pro-
gram optimization (Shypula et al., 2024), where we
facilitate a history-based editing paradigm. Con-
cretely, we give a previous, presumably slow, ver-
sion of the solution program, pin. We then prompt
LMs to edit the code to generate a more efficient
version pout, denoted as CodeLM (pin)→pout,
where poutis expected to run faster than pin.
Evaluating Speedup and Memory Reduction
Using the (slow, fast) program pairs remaining af-
ter post-processing in §3.2, we evaluate the rela-
tive speedup and memory reduction of the model-
generated program against the input program on
private test cases Tprivate .
We adopt the speedup metric introduced by Shy-
pula et al. (2024), which is formulated as:
Speedup =Runtime of pin
Runtime of pout(1)
Similarly, to evaluate improvement in memory
usage, we introduce a memory reduction metric as:
Memory Reduction =Memory of pin
Memory of pout(2)
# Program to search for a number 
def search (list , target ):
 for i, element  in 
enumerate (list ):
   if element  == target :
     return  i
 return  -1Problem: Write a program to search for a  
number in a list input output # Program to search for a number 
def search (list , target ):
 for i, element  in 
enumerate (list ):
   if element  == target :
     return  i
 return  -1Problem: Write a program to search for a  
number in a list input output # Program to search for a number 
def search (list , target ):
 left , right  = 0, len(list ) - 1
 while  left  <= right :
   mid = ( left  + right ) // 2
   if list [mid] == target : return  mid
   elif  list [mid] < target : 
left  = mid + 1
   else : right  = mid - 1
 return  -1slow 
fast 
Figure 3: Illustration of history-based editing.
3.3.2 NL-Instructed Generation
In addition, we support the most common NL-to-
code generation setup: given the NL description
dof a problem, we ask the LM to generate the
program solution p, asCodeLM (d)→p. Our
goal is for the code LM to generate an efficient and
correct solution p. We execute pon the private test
cases Tprivate to evaluate its performance.
# Program to search for a number 
def search (list , target ):
 for i, element  in 
enumerate (list ):
   if element  == target :
     return  i
 return  -1Problem: Write a program to search for a  
number in a list input output Figure 4: Illustration of NL-instructed generation.
Solution Program Spectrum To evaluate rela-
tive runtime and memory efficiency, we measure
where a model-generated program lies on the spec-
trum of all user-submitted programs to that prob-
lem. We use the JUDGE 0evaluation platform (§3.1)
to measure the runtime and memory usage.
Evaluating Percentile over the Spectrum We
introduce runtime and memory percentile to mea-
sure the efficiency of the model-generated program
over the solution spectrum for a given problem as:
Runtime %=# Slower user programs
Total # of user programs(3)
Memory %=# Programs w/ more memory
Total # of user programs
(4)
3.3.3 Evaluating Functional Correctness
To measure if program correctness is preserved, a
key metric is the functional correctness of model-
generated programs. We adopt the pass@1 metric
introduced by Chen et al. (2021), which samples
one program from the model and measures whether
the generated program passes all test cases.
3.4 ECCO Feature Analysis
After filtering the problem description dataset, we
split the dataset into train, test, and validation sets
for experiments. We ensure that no problem IDs
overlap across these splits, to avoid data contamina-
tion. As shown by the detailed statistics of ECCO
in Table 1, ECCO contains 1.3 kproblems and over
50kprogram pairs for code optimization evaluation.
Split # Problems # Pairs# Avg. Test Cases
Public Private
Train 1262 48386 3.14 17.21
Val 69 2359 3.17 17.25
Test 48 794 3.29 20.00
Table 1: ECCO dataset statistics.The code is incorrect because it updates  
the boundaries incorrectly, causing it to  
search the wrong portions of the list. 
INCORRECT, 1 / 2  test cases passed 
PASS Test case 1: 120 ms, 220 KB 
FAIL Test case 2: IndexError: out of range # Program to search for a number 
def search (list , target ):
 left , right  = 0, len(list ) - 1
 while  left  <= right :
   mid = ( left  + right ) // 2
   if list [mid] == target : return  
mid
   elif  list [mid] < target : 
left  = mid - 1 # Bug 
   else : right  = mid + 1 # Bug 
 return  -1
The execution results (IndexError) 
suggests that the boundary conditions 
were not handled properly, allowing the 
search indices to exceed the valid range # Program to search for a number 
def search (list , target ):
 left , right  = 0, len(list ) - 1
 while  left  <= right :
   mid = ( left  + right ) // 2
   if list [mid] == target : return  mid
   elif  list [mid] < target : 
left  = mid + 1 # Fixed 
   else : right  = mid - 1 # Fixed 
 return  -1
Feedback Stage Reﬁne Stage 
pr e-r eﬁne r eﬁned Judge0 
Execute Code 
LLMSelf-Reﬁne 
NL+Ex ec -Reﬁne Ex ec -Reﬁne 
Reflect 
Figure 5: Iterative refinement methods utilizing different forms of feedback. Self-Refine uses Natural Language
feedback, Exec-Refine uses raw execution results on Tpublic andNL+Exec-Refine uses NL reflection of execution.
4 Efficiency-Improving Approaches
We explore various top-performing code generation
approaches to improve program efficiency, while
maintaining functional correctness, including in-
context learning (§4.1), iterative refinement (§4.2),
and fine-tuning (§4.3).
4.1 In-Context Learning
We explore two mainstream prompting strategies:
instruction prompting and few-shot learning.
Instruction prompting Many LMs perform bet-
ter when incorporating instructions (Ouyang et al.,
2022; Wei et al., 2022). We use two prompts: Igen
for NL-based generation which instructs models to
generate correct and efficient programs; and Ieff
for history-based editing which instructs models to
optimize the input program. Ieffis adapted from
PIE (Shypula et al., 2024) and Self-Refine (Madaan
et al., 2024). See §D for details.
Few-Shot Learning We add few-shot example
demonstrations (Brown et al., 2020): for the NL-
based setting, using (NL, fastest program) pairs;
for history-based editing, using (slow program, fast
program) pairs. We randomly sample examples
from the train set as the few-shot examples.
4.2 Iterative Refinement
We explore three methods (illustrated in Figure 5 )
to iteratively refine the generated code to be more
efficient, which intuitively aligns with the way that
humans improve code (Madaan et al., 2024).
Self-Refine with NL Feedback We adopt self-
refine (Madaan et al., 2024) that prompts LMs to
iteratively examine the output and refine it. More
concretely, (1) we first prompt the LM to generatea candidate solution; (2) we ask the same model
to produce NL reasoning about why the code is
incorrect and/or inefficient; and (3) we input the
original input and the feedback from (2) to the
model and ask it to generate an updated solution.
Exec-Refine with Interpreter Feedback We
propose an alternative refinement strategy that ob-
tains deterministic execution feedback from the in-
terpreter, by running the program over Tpublic .5If
test cases are passed, the execution result provides
the runtime and memory information; otherwise,
this feedback provides interpreter error logs. Both
correctness and efficiency can be informed via this
feedbackare.
NL+Exec Refine: NL Feedback on Interpreter
Results To allow feedback both in the forms of
NL and execution outputs, we ground the LM feed-
back on execution results, inspired by the Reflexion
feedback paradigm (Shinn et al., 2024). Specifi-
cally, we first obtain the execution results as in
exec-refine , then ask the LM to write NL feedback
on the incorrect/inefficient parts in the code, and
use this as additional input in the refinement turn.
4.3 Fine-tuning
We also explore three fine-tuning methods beyond
prompting-alone approaches.
Vanilla Fine-tuning In this vanilla training set-
ting, we leverage (NL, program) pairs and (slow
program, fast program) pairs to train models in-
dependently for each paradigm. We format the
data for both similarly to the in-context learning
prompts (§4.1), and finetune on a causal language
5Models do not have access to the private test cases we
finally evaluate on.Model SettingHistory-based Editing NL-instructed Generation
pass@1 speedup memory reduction pass@1 runtime% memory%
StarCoder2instruct 49.4 1.49 1.24 4.2 50.64 55.72
few-shot 49.8 1.70 1.07 2.1 11.40 50.17
CodeGemmainstruct 42.5 1.43 1.10 18.8 41.70 51.83
few-shot 43.9 1.07 1.06 22.9 62.80 67.33
WizardCoderinstruct 34.2 1.58 1.18 14.6 54.29 84.53
few-shot 27.4 1.38 1.12 14.6 58.69 71.00
CodeLLaMainstruct 57.5 1.44 1.11 8.3 45.30 74.18
few-shot 22.5 1.63 1.26 8.3 42.66 67.21
DeepseekCoderinstruct 29.8 2.11 1.28 18.8 59.01 75.86
few-shot 35.2 2.26 1.20 22.9 55.52 66.09
GPT-4oinstruct 66.6 1.64 1.10 52.1 46.01 59.21
few-shot 65.8 1.62 1.12 41.7 49.87 64.44
Table 2: Results using In-Context Learning approaches (instruction-prompting and few-shot learning)
modelling task on the formatted data for each of
the two paradigms independently.
Execution Conditioned Fine-tuning Beyond
fine-tuning with basic contexts, we posit that fur-
ther conditioning on execution results could help.
Therefore, we include execution results of PASS/-
FAIL status, runtime, and memory usage for each
public test case for the input program.
Trajectory Conditioned Fine-tuning For
history-based editing, we further propose
trajectory-conditioned fine-tuning, by adding a
trajectory history of programs written by the same
user for the given problem in context. We first
collect all problems with at least three programs
submitted by the same user, and treat the series
of programs as a trajectory. From each qualified
trajectory, we designate the fastest code as the
target output, and sample three other intermediate
programs at the 0th, 33rd, and 66th percentile steps
to use as inputs. We aim to allow the model to learn
from the step-by-step improvements that led to the
optimal solution, capturing the problem-solving
process in addition to just the inputs and targets.
5 Experiments
5.1 Experimental Setup
Models We experiment with several best-
performing LMs pre-trained on code. Specifi-
cally, we evaluate CodeLlama-13B (Roziere et al.,
2023), DeepSeekCoder-7B-v1.5 (Guo et al., 2024),
CodeGemma-7B (Team et al., 2024), WizardCoder-
13B-Python (Luo et al., 2023), StarCoder2-15B
(Lozhkov et al., 2024). We use the instruction-
tuned versions of all of these open-checkpoint mod-els unless indicated otherwise. We also use the pro-
prietary GPT-4o model for no-training methods.
5.2 Results and Analysis
5.2.1 In-Context Learning
As shown in Table 2, all methods either reduce
pass@1 of the program by a large margin (in edit-
ing mode) or obtain low pass@1 (generation mode).
Comparing the two paradigms, history-based edit-
ing results in a substantially higher pass@1 by re-
ferring to a base correct program, compared to
NL-instructed generation which lacks a base pro-
gram to start from. GPT-4o obtains a much higher
pass@1 than all models in both paradigms.
History-based editing While in-context learning
can effectively speed up the input program by 7–
126%, but compromises correctness, dropping it
to22.5–66.6, and uses more memory. Few-shot
shows this trend more explicitly than instruct . Be-
sides the limitations of LMs, this may be caused
by the sampled few-shot demonstrations being al-
gorithmically less relevant to the problem at hand.
NL-instructed generation While Deepseek-
Coder and CodeGemma’s pass@1 improves by
4.1%with few-shot , GPT-4o and StarCoder2’s
pass@1 drops by 2.1−10.4%. Similarly for
the efficiency metrics, CodeGemma and GPT-4o
see an improvement whereas other models do not.
GPT-4o significantly outperforms other models at
pass@1 by 2.2 ×, however there is no clear winner
for efficiency. This highlights the complex trade-
offs between correctness and efficiency specifically
in the NL-instructed generation task.Model SettingHistory-based Editing NL-instructed Generation
pass@1 speedup memory reduction pass@1 runtime% memory%
StarCoder2pre-refine 49.4 1.49 1.24 4.2 50.64 55.72
self-refine 26.7 1.55 1.30 2.1 5.79 71.50
exec-refine 39.5 1.49 1.23 2.1 29.27 55.79
nl+exec refine 26.1 2.13 1.26 2.1 5.79 81.69
CodeGemmapre-refine 42.5 1.43 1.10 18.8 41.70 51.83
self-refine 15.1 2.08 1.15 6.3 41.23 59.18
exec-refine 33.2 1.59 1.12 18.8 39.26 54.81
nl+exec refine 29.8 1.54 1.14 14.6 33.24 38.70
WizardCoderpre-refine 34.2 1.58 1.18 14.6 54.29 84.53
self-refine 8.5 2.16 1.23 8.3 44.86 88.77
exec-refine 20.9 1.60 1.13 12.5 44.12 76.86
nl+exec refine 18.3 2.90 1.30 12.5 31.92 79.19
CodeLLaMapre-refine 57.5 1.44 1.11 8.3 45.30 74.18
self-refine 15.8 2.02 1.22 2.1 32.16 99.42
exec-refine 54.6 1.51 1.12 4.2 44.09 85.28
nl+exec refine 16.2 1.37 1.02 4.2 66.00 70.79
DeepseekCoderpre-refine 29.8 2.11 1.28 18.8 59.01 75.86
self-refine 13.6 2.73 1.35 8.3 29.65 65.26
exec-refine 27.4 2.34 1.24 20.8 55.08 73.78
nl+exec refine 19.6 3.54 1.37 14.6 49.08 85.15
GPT-4opre-refine 66.6 1.64 1.10 52.1 46.01 59.21
self-refine 47.8 2.72 1.25 37.5 51.12 44.03
exec-refine 60.8 2.19 1.22 52.1 52.55 59.47
nl+exec refine 58.8 2.39 1.22 47.9 49.79 47.46
Table 3: Results with iterative refinement approaches. Feedback in NL (self & nl+exec) improves efficiency better,
whereas raw execution feedback (exec) maintains correctness more effectively.
5.2.2 Iterative Refinement
Table 3 shows all results with iterative refinement
methods. As a reference for the refinement ap-
proaches, we measure the LM-generated code in
the first attempt at optimization without any refine-
ment, and denote this method as pre-refine .
History-based editing paradigm While all
methods can effectively speed up the program,
methods that involve NL feedback ( self-refine and
nl+exec refine ) achieve the highest speedup across
models. exec-refine consistently yields the highest
pass@1 for all models, by 3.4–38.8points more
than the other two methods. We conjecture that
execution outputs are better representations to in-
form functional correctness than NL descriptions.
Although it is easier to convey high-level optimiza-
tion strategies in NL, conveying the functional cor-
rectness is harder. Overall, although the models
are instructed to emphasize both correctness and
efficiency, there seems to be an implicit trade-off
between them. Additional analysis is in §A.
NL-instructed generations We observe similar
patterns as the editing mode, that exec-refine bestmaintains functional correctness, and two other
NL-involved approaches improve runtime/memory
efficiency. Compared to the in-context learning
results in Table 2, iterative refinement significantly
improved memory% for all the models, with the
best method showing an average improvement of
12.06% over the instruct method. However, the
impact on runtime% shows varying results among
the different models.
5.2.3 Fine-tuning
We perform parameter-efficient fine-tuning on
CodeLLaMa-7B and DeepseekCoder-7B, the best-
performing classes of models on the correctness
and efficiency metrics in our prompting experi-
ments respectively.
History-based editing As shown in Table 4, fine-
tuning is the most effective method in maintaining
correctness in the editing paradigm. Especially for
DeepseekCoder, compared to the highest prompt-
ing results 35.2using few-shot examples, vanilla
andexecution -conditioned tuning improves by 6.9
and7.8points, and trajectory -conditioned tuning
further gains a 34.6point increase overall. This0 1 2 3 4
Iterations1015202530Pass@1 (%)self-refine
exec-refine
nl+exec-refine(a) Pass@1
0 1 2 3 4
Iterations2.252.502.753.003.253.50Speedup (x)self-refine
exec-refine
nl+exec-refine (b) Speedup
0 1 2 3 4
Iterations1.201.251.301.35Mem Reduction (x)self-refine
exec-refine
nl+exec-refine (c) Memory Reduction
Figure 6: Performance of DeepseekCoder over multiple iterations of refinement. The improvement in efficiency is
outweighed by the consistent drop in pass@1 .
suggests that adding user-specific coding trajecto-
ries can help ground models into the optimization
mode and substantially improve output correctness.
Model Method pass@1 speedup mem.red.
CodeLLaMa-7BVanilla 43.0 1.11 1.01
Execution 45.0 1.41 1.04
Trajectory 70.2 1.01 1.00
DeepseekCoderVanilla 42.1 1.11 1.01
Execution 43.0 1.16 1.02
Trajectory 69.8 1.01 1.00
Table 4: Fine-tuning results for history-based editing.
NL-instructed generation In the more complex
NL-instructed generation task shown in Table 5,
fine-tuning is effective in improving the efficiency
for CodeLLaMa but not for DeepseekCoder, high-
lighting the need for more robust fine-tuning meth-
ods that can handle trade-offs and effectively main-
tain correctness in this setting.
However, fine-tuning results in a much less effi-
ciency improvement than prompting-based meth-
ods for both paradigms, possibly due to the limited
power of parameter efficient fine-tuning.
Model Method pass@1 runtime% mem%
CodeLLaMa-7BPre-trained 8.3 36.74 44.15
Finetuned 8.3 46.81 71.32
DeepseekCoderPre-trained 18.8 59.01 75.86
Finetuned 16.7 43.61 67.31
Table 5: Fine-tuning results for NL-based generation.
6 Additional Analysis
Does instruction tuning help in-context learn-
ing? As instruct model versions are expected to
be better at in-context learning than their base coun-
terparts, we compare base and instruct model ver-
sions in the editing paradigm. In Table 6, the base
versions get much higher correctness albeit at lowerefficiency, showing that base and instruct versions
lie at different points on the correctness-efficiency
trade-off. As we emphasize both correctness and
efficiency aspects in the NL instruction, we conjec-
ture the instruct models take more hints from the
input format and emphasize efficiency, while base
models primarily emphasize correctness.
Model Version pass@1 speedup mem.red.
CodeLlamaInstruct 22.5 1.63 1.26
Base 46.4 1.02 1.00
DeepseekCoderInstruct 35.2 2.26 1.20
Base 45.4 1.04 1.00
CodeGemmaInstruct 43.9 1.07 1.06
Base 48.2 1.01 1.00
StarCoder2Instruct 49.8 1.70 1.07
Base 41.5 1.03 1.00
Table 6: Comparing base and instruct model versions
with in-context learning methods.
Does multi-iteration refinement help? Multiple
refinement iterations may improve results by allow-
ing more turns for models to refine. To verify this,
we evaluate the iterative refinement method using
1–4 iterations. While self-refine and exec-refine im-
prove program speedup over iterations (Figure 6b),
all methods continuously degrade the pass@1 to
various extents. Exec-refine preserves correctness
more effectively in further iterations as well. For
memory usage (Figure 6c), exec-refine consistently
reduces memory usage, yet other methods exhibit
big fluctuations. In general, more iterations can
speed up the program, yet further sacrifice correct-
ness, thus has limited gains in terms of correctness-
preserving optimization.
Can iterative prompting fix incorrect solutions?
To study whether models can recover from incor-
rect starting solutions in the history-based editing1B7B13B 34B 70B3540455055
Pass@1 (History-based)
1B7B13B 34B 70B1.21.41.6
Speedup
1B7B13B 34B 70B1.001.051.101.15
Memory Reduction
1B7B13B 34B 70B102030
Pass@1 (NL-Instructed)
1B7B13B 34B 70B35.037.540.042.545.0
Runtime %
1B7B13B 34B 70B3040506070
Memory %
DeepseekCoder CodeLLaMaFigure 7: Correctness and efficiency of generated code across model sizes for CodeLLaMa and DeepseekCoder.
The top row corresponds to History-Based Editing and the bottom row includes NL-Instructed Generation.
paradigm, we evaluate on a collection of 157 pairs
of programs, ECCO-F IX, where the input code is
almost correct: one that passed all public test cases
but fails a few private test cases. As shown by Ta-
ble 7, we show that exec-refine can fix incorrect
programs to pass all public and private test cases,
with access to only the PASS/FAIL status of public
test cases. In comparison, self-refine breaks the
correctness of more programs.
Aligning with our findings in earlier sections
and §A, exec-refine , with execution information in
contexts, can encourage models to generate func-
tionally correct programs.
Model Instruct Self-Refine Exec-Refine
StarCoder2 12.1 9.5 8.9
CodeGemma 10.8 3.1 10.8
WizardCoder 20.3 1.9 14.6
CodeLlama 17.1 7.6 36.9
DeepseekCoder 12.7 5.7 15.2
Table 7: Pass@1 of iterative refinement strategies for
models on ECCO-F IX, under the editing paradigm.
How does scale of model impact efficiency and
correctness? We perform experiments across
model scales for the CodeLLaMa (7B, 13B, 34B,
70B) and DeepseekCoder-v1 (1.3B, 6.7B, 33B)
instruction-tuned model families. As seen in Fig-
ure 7 and Table 10, for both History-based Editing
and NL-Instructed Generation, scaling model size
presents a mix of benefits and trade-offs in terms ofefficiency and correctness. For History-based Edit-
ing, larger CodeLLaMa models (such as 70B) show
better speedup but experience diminishing gains in
correctness (Pass@1), with a slight decrease in
memory reduction. DeepseekCoder follows a simi-
lar pattern but consistently underperforms CodeL-
LaMa in correctness. In NL-Instructed Generation,
scaling models significantly improves correctness,
as seen with CodeLLaMa 70B and DeepseekCoder
33B, although giving varying results for runtime
and memory percentiles.
7 Conclusion
In this paper, we introduce the ECCO bench-
mark that enables two paradigms for Python pro-
gram optimization, using JUDGE 0, a language and
platform-agnostic execution framework. We find
that execution information and fine-tuning help
LLMs maintain code correctness, and prompting
with natural language often yields higher efficiency
gains. However, we broadly reconfirm findings that
no existing method can improve efficiency with-
out sacrificing functional correctness. We hope
ECCO can serve as a testbed for program opti-
mization, and we call for more efforts in advancing
correctness-preserving program optimizations.
Acknowledgements
We thank Aman Madaan for the helpful discussions
in the early stage of the project.Limitations
Our benchmark establishes a solid foundation for
rigorously evaluating the ability to generate effi-
cient solutions, however, we also note that there
are limitations to our work.
First, ECCO has only included Python problems
so far, but our JUDGE 0evaluation platform and our
benchmark curation recipe are fully reproducible
and could be extended to other programming lan-
guages of interest. Second, our benchmark cur-
rently focuses on competitive programming prob-
lems. It is also possible to extend our benchmark to
other types of programming problems from more
real-world software engineering scenarios.
Due to both limitations, our results may not
be comprehensive enough to reflect the quality of
model-generated programs on the full spectrum of
all programming languages and problems. When
using ECCO in practice, we recommend the read-
ers examine the model outputs, in addition to the
quantitative results produced by our framework.
References
Seung-Yeop Baik, Mingi Jeon, Joonghyuk Hahn, Jungin
Kim, Yo-Sub Han, and Sang-Ki Ko. 2024. Codecom-
plex: A time-complexity dataset for bilingual source
codes. arXiv preprint arXiv:2401.08719 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Rudy Bunel, Alban Desmaison, M Pawan Kumar,
Philip HS Torr, and Pushmeet Kohli. 2016. Learn-
ing to superoptimize programs. arXiv preprint
arXiv:1611.01787 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Tristan Coignion, Clément Quinton, and Romain Rou-
voy. 2024. A performance study of llm-generatedcode on leetcode. In 28th International Conference
on Evaluation and Assessment in Software Engineer-
ing (EASE’24) .
Chris Cummins, V olker Seeker, Dejan Grubisic,
Mostafa Elhoushi, Youwei Liang, Baptiste Roziere,
Jonas Gehring, Fabian Gloeckle, Kim Hazelwood,
Gabriel Synnaeve, et al. 2023a. Large language
models for compiler optimization. arXiv preprint
arXiv:2309.07062 .
Chris Cummins, V olker Seeker, Dejan Grubisic,
Mostafa Elhoushi, Youwei Liang, Baptiste Roziere,
Jonas Gehring, Fabian Gloeckle, Kim Hazelwood,
Gabriel Synnaeve, et al. 2023b. Large language
models for compiler optimization. arXiv preprint
arXiv:2309.07062 .
Herman Zvonimir Došilovi ´c and Igor Mekterovi ´c. 2020.
Robust and scalable online code execution system. In
2020 43rd International Convention on Information,
Communication and Electronic Technology (MIPRO) ,
pages 1627–1632. IEEE.
Shuzheng Gao, Cuiyun Gao, Wenchao Gu, and Michael
Lyu. 2024. Search-based llms for code optimization.
arXiv preprint arXiv:2408.12159 .
Leonidas Gee, Milan Gritta, Gerasimos Lampouras,
and Ignacio Iacobacci. 2024. Code-optimise: Self-
generated preference data for correctness and effi-
ciency. arXiv preprint arXiv:2406.12502 .
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y Wu, YK Li, et al. 2024. Deepseek-coder: When the
large language model meets programming–the rise of
code intelligence. arXiv preprint arXiv:2401.14196 .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Dong Huang, Jie M Zhang, Yuhao Qing, and Heming
Cui. 2024. Effibench: Benchmarking the efficiency
of automatically generated code. arXiv preprint
arXiv:2402.02037 .
Mingi Jeon, Seung yeop Baik, Joonghyuk Hahn, Yo-Sub
Han, and Sang-Ki Ko. 2023. Deep learning-based
source code complexity prediction.
Mohammad Abdullah Matin Khan, M Saiful Bari,
Xuan Long Do, Weishi Wang, Md Rizwan Parvez,
and Shafiq Joty. 2023. xcodeeval: A large scale multi-
lingual multitask benchmark for code understanding,
generation, translation and retrieval. arXiv preprint
arXiv:2303.03004 .
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
cient memory management for large language model
serving with pagedattention. In Proceedings of theACM SIGOPS 29th Symposium on Operating Systems
Principles .
Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Competition-level code generation with
alphacode. Science , 378(6624):1092–1097.
Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei,
Yifeng Ding, and Lingming Zhang. 2024. Evaluating
language models for efficient code generation. arXiv
preprint arXiv:2408.06450 .
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-
erico Cassano, Joel Lamy-Poirier, Nouamane Tazi,
Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,
et al. 2024. Starcoder 2 and the stack v2: The next
generation. arXiv preprint arXiv:2402.19173 .
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:
Empowering code large language models with evol-
instruct. arXiv preprint arXiv:2306.08568 .
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
et al. 2024. Self-refine: Iterative refinement with
self-feedback. Advances in Neural Information Pro-
cessing Systems , 36.
Kaushik Moudgalya, Ankit Ramakrishnan, Vamsikr-
ishna Chemudupati, and Xing Han Lu. 2023. Tasty:
A transformer based approach to space and time com-
plexity. arXiv preprint arXiv:2305.05379 .
Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, and
Vincent Ng. 2024. On evaluating the efficiency
of source code generated by llms. arXiv preprint
arXiv:2404.06041 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems , 35:27730–27744.
Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang,
Giacomo Domeniconi, Vladimir Zolotov, Julian
Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker,
et al. 2021. Codenet: A large-scale ai for code
dataset for learning a diversity of coding tasks. arXiv
preprint arXiv:2105.12655 .
Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.
Code generation with alphacodium: From prompt
engineering to flow engineering. arXiv preprint
arXiv:2401.08500 .
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .Hui Shi and Yang Zhang. 2020. Deep symbolic su-
peroptimization without human knowledge. ICLR
2020 .
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2024. Re-
flexion: Language agents with verbal reinforcement
learning. Advances in Neural Information Process-
ing Systems , 36.
Alex Shypula, Pengcheng Yin, Jeremy Lacomis,
Claire Le Goues, Edward Schwartz, and Graham
Neubig. 2021. Learning to superoptimize real-world
programs. arXiv preprint arXiv:2109.13498 .
Alexander G Shypula, Aman Madaan, Yimeng Zeng,
Uri Alon, Jacob R. Gardner, Yiming Yang, Mi-
lad Hashemi, Graham Neubig, Parthasarathy Ran-
ganathan, Osbert Bastani, and Amir Yazdanbakhsh.
2024. Learning performance-improving code edits.
InThe Twelfth International Conference on Learning
Representations .
Jagriti Sikka, Kushal Satya, Yaman Kumar, Shagun Up-
pal, Rajiv Ratn Shah, and Roger Zimmermann. 2020.
Learning based methods for code runtime complexity
prediction. In Advances in Information Retrieval:
42nd European Conference on IR Research, ECIR
2020, Lisbon, Portugal, April 14–17, 2020, Proceed-
ings, Part I 42 , pages 313–325. Springer.
Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi,
Nagarajan Natarajan, and Aditya Kanade. 2024. No-
funeval: Funny how code lms falter on require-
ments beyond functional correctness. arXiv preprint
arXiv:2401.15963 .
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
Juliette Love, et al. 2024. Gemma: Open models
based on gemini research and technology. arXiv
preprint arXiv:2403.08295 .
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations .
Tong Ye, Tengfei Ma, Lingfei Wu, Xuhong Zhang,
Shouling Ji, and Wenhai Wang. 2024. Iterative or
innovative? a problem-oriented perspective for code
optimization. arXiv preprint arXiv:2406.11935 .A Iterative Refinement Analysis
Following Shypula et al. (2024), we also evaluate
runtime and memory efficiency as a percentage of
pairs where the generated code poutis faster/uses
lesser memory than pin, in addition to the speedup
and memory reduction metrics described in §3.3.1.
In Table 8, these percentage optimized metrics
clearly indicate that exec-refine is the most consis-
tent iterative refinement method as it achieves the
best pass@1, % runtime optimized and % mem-
ory optimized across all models. Contrasting these
results to Speedup and Memory Reduction in Ta-
ble 3, we note that while natural language feedback
(in self-refine and NL+Exec refine) aids in signif-
icantly improving the runtime and memory usage
for a few cases (while breaking test cases for oth-
ers), exec-refine improves runtime and memory in
more cases albeit to a smaller degree.
B Implementation Details
Generation We use a maximum length of 1024
tokens, and a temperature of t=0.4. We provide
two examples for all in-context few-shot experi-
ments
Hardware We run all our experiments on a mix
of A6000 and L40 GPUs. Specifically for the
prompting iterative prompting and in-context learn-
ing approaches, we use 1-2 GPUs of the type and
utilize the vLLM library (Kwon et al., 2023) primar-
ily for generating programs efficiently. We perform
finetuning on 4 A6000 GPUs.
ForJUDGE 0evaluation virtual hardware setup,
we use an m7i.large EC2 instance which has 2
vCPU cores of the 4th Generation Intel Xeon Sap-
phire Rapids, with 8GB of RAM.
Fine-tuning We adopt parameter-efficient fine-
tuning with LoRA (Hu et al., 2022) due to resource
limitations and implement this using the Hugging-
Face Transformers library6. We optimize the fine-
tuning using Deepspeed7ZeRo stage 2 with a
LoRA rank of 8, alpha of 16 and a per-device batch
size of 2. We use the AdamW optimizer, with a
learning rate of 1e-3 and a warmup of 100 steps.
We train the models on the history-based editing
task for a single epoch and fine-tune models for 10
epochs on the NL-instructed generation task.
6https://github.com/huggingface/transformers
7https://github.com/microsoft/DeepSpeedC Variance Analysis
We measured the variance for two models
(DeepseekCoder and CodeLLaMa) by sampling
generations three times and evaluating each genera-
tion independently on our Judge setup. The results
are included below with 95% Confidence Intervals
are shown in Table 9.
Pass@1 has no variance across runs in both
paradigms due to the low temperature; runtime effi-
ciency metrics (speedup and runtime %) both only
have about 1% Confidence Interval (CI). Mem-
ory has close to no variance in the history-based
editing setting but shows 5% CI in NL-instructed
generation.
D Prompt Details
We illustrate and detail all of the prompts used for
the experiments in Figures 8-14.Model SettingHistory-based Editing
pass@1 % Runtime Opt % Mem. Opt.
StarCoder2self-refine 26.7 22.5 27.0
exec-refine 39.5 25.5 30.3
nl+exec refine 26.1 22.6 27.0
CodeGemmaself-refine 15.1 18.2 29.3
exec-refine 33.2 25.5 34.5
nl+exec refine 29.8 21.0 31.3
WizardCoderself-refine 8.5 16.1 24.4
exec-refine 20.9 20.0 27.2
nl+exec refine 18.3 17.1 26.9
CodeLLaMaself-refine 15.8 15.6 24.5
exec-refine 54.6 27.4 39.6
nl+exec refine 16.2 16.8 23.2
DeepseekCoderself-refine 13.6 26.8 35.8
exec-refine 27.4 31.4 40.0
nl+exec refine 19.6 29.7 36.5
GPT-4oself-refine 47.8 42.5 42.3
exec-refine 60.8 48.8 51.1
nl+exec refine 58.8 48.6 48.1
Table 8: Comparing iterative refinement approaches on % Optimization metrics. Exec-Refine is the best performing
approach across all models and metrics.
Model History-based Editing NL-Instructed Generation
Pass@1 Speedup Memory Reduction Pass@1 Runtime (%) Memory (%)
DeepseekCoder 32.87±0.00 2.112 ±0.022 1.097 ±0.000166 18.75±0.00 57.589 ±1.012 67.796 ±3.106
CodeLLaMa 57.50±0.00 1.456 ±0.0028 1.114 ±0.00173 8.30±0.00 55.663 ±0.720 61.029 ±3.818
Table 9: Variance of results for History-based Editing and NL-Instructed Generation
Model SizeHistory-based Editing NL-instructed Generation
Pass@1 Speedup Memory Reduction Pass@1 Runtime (%) Memory (%)
CodeLLaMa7B 58.4 1.04 1.00 2.1 39.73 26.93
13B 57.5 1.44 1.11 8.3 45.30 74.18
34B 51.9 1.31 1.13 8.3 43.23 70.16
70B 55.3 1.73 1.09 12.5 39.15 64.89
DeepseekCoder1.3B 44.2 1.36 1.13 8.3 46.71 65.62
6.7B 34.8 1.47 1.16 29.2 33.48 76.19
33B 49.1 1.51 1.09 37.5 43.91 75.61
Table 10: Results for History-based Editing and NL-Instructed Generation across different model sizes.Optimize the python program below to be functionally equivalent
but run faster and use less memory.
Here are a few examples:
### Program:
{slow_code_example}
### Optimized (Runtime and Space) version of Program above:
{fast_code_example}
### Program:
[src_code]
### Optimized (Runtime and Space) version of Program above:
Figure 8: Prompt for Instruction prompting Ieffalong with in-context examples
Your solution was functionally {CORRECT/INCORRECT}
Here are the run time and memory stats of your code for each test case
-- Stats for test case 0 --
Correct: {PASSED/FAILED}
Run time: 0.009 s
Memory: 3352.0 KB
-- Stats for test case 1 --
Correct: {PASSED/FAILED}
Run time: 0.009 s
Memory: 3316.0 KB
Figure 9: Example of Execution Feedback on public test cases used for Exec-Refine and Execution Conditioned
Fine-tuning
Write a python code which is correct and efficient in terms of runtime and memory
usage for the following problem description.
##Problem Name:
{problem_name}
##Problem Description:
{In detail description of the task}
## Sample Inputs:
{input_test_cases}
##Sample Outputs:
{Expected Output}
Figure 10: Prompt for NL-instructed generation IgenGive feedback in english for why the code solution below is incorrect
or inefficient and how the program can be fixed.
## Candidate solution:
{most recent code attempt}
## Feedback for incorrectness/inefficiency and how it can be improved:
Figure 11: Prompt used for NL-reasoning to get feedback
Refine the given incorrect or sub-optimal code solution based on the feedback
specified below.
### Candidate solution:
{previous_code_attempt}
### Feedback for incorrectnes/inefficiency and how it can be improved:
{self-feedback / execution-feedback / NL+Exec-Refine}
### Optimized/Corrected solution based on feedback:
Figure 12: Prompt used for refining code in Self-Refine, Exec-Refine and NL+Exec-Refine
Based on the execution results, reflect on why the code solution
below was incorrect or inefficient and how the program can be fixed.
{generated code solution}
## Execution Results:
{Execution Feedback for the previous attempt}
### Reflection on incorrectnes/inefficiency and how it can be improved:
Figure 13: Prompt used for NL+Exec-Refine to reflect on Execution results
##1 iteration program:
{slowest program in trajectory}
##2 iteration program:
{33 percentile fastest program in trajectory}
##3 iteration program:
{66 percentile fastest program in trajectory}
### Final iteration program:
Figure 14: Format of Trajectory-Conditioned Fine-tuning data