IDEA W: Robust Neural Audio Watermarking with
Invertible Dual-Embedding
Pengcheng Li1,2‡, Xulong Zhang1‡, Jing Xiao1, Jianzong Wang1B
1Ping An Technology (Shenzhen) Co., Ltd.
2University of Science and Technology of China
lipengcheng@ustc.edu, zhangxulong@ieee.org,
xiaojing661@pingan.com.cn, jzwang@188.com
Abstract
The audio watermarking technique embeds
messages into audio and accurately extracts
messages from the watermarked audio. Tra-
ditional methods develop algorithms based on
expert experience to embed watermarks into
the time-domain or transform-domain of sig-
nals. With the development of deep neural
networks, deep learning-based neural audio
watermarking has emerged. Compared to tra-
ditional algorithms, neural audio watermark-
ing achieves better robustness by considering
various attacks during training. However, cur-
rent neural watermarking methods suffer from
low capacity and unsatisfactory impercepti-
bility. Additionally, the issue of watermark
locating, which is extremely important and
even more pronounced in neural audio water-
marking, has not been adequately studied. In
this paper, we design a dual-embedding wa-
termarking model for efficient locating. We
also consider the impact of the attack layer
on the invertible neural network in robustness
training, improving the model to enhance both
its reasonableness and stability. Experiments
show that the proposed model, IDEA W , can
withstand various attacks with higher capacity
and more efficient locating ability compared
to existing methods. The code is available at
https://github.com/PecholaL/IDEAW .
1 Introduction
Digital watermarking (Singh et al., 2023) embeds
messages indicating the ownership or authenticity
into multimedia like images, video and audio, im-
perceptibly. This technique is widely used for own-
ership statements and anti-counterfeit. Impercep-
tibility and robustness are the two most challeng-
ing requirements for digital watermarking, which
means, it is expected to be hard to feel the pres-
ence of the embedded watermark with human per-
ception, and the watermark can be preserved and
‡ Equal contribution.
BCorresponding author.
audiomessagewatermarkedaudio(a)
audio0,1,1,0..message11100..0locating code0,1,1,0..11100..00,1,1,0..11100..0(c)EmbedderwatermarkExtractor
audiosyn. code + msg.(b)11100..00,1,1,0..11100..00,1,1,0..11100..00,1,1,0..
Attack LayerFigure 1: (a) Pipeline of robust neural audio watermark-
ing. (b) Embedding strategy of existing methods. (c)
Dual-embedding strategy of IDEAW.
extracted accurately even after the watermarked
media has been subjected to unintentional damage
or malicious removal attacks.
Audio watermarking has been around for
decades. Traditional techniques (Singha and Ullah,
2022; Zhang et al., 2023) embeds the watermark
into either the time-domain or transform-domain
of audio signals via algorithms designed based on
expert knowledge (Prabha and Sam, 2022). The ro-
bustness of traditional watermarking methods gen-
erally stems from subjective design, which results
in limitations. The advancement of deep learning
brings new solutions to steganography (Hussain
et al., 2020; Chanchal et al., 2020) and digital water-
marking techniques (Amrit and Singh, 2022; Singh
and Singh, 2023). End-to-end neural watermark-
ing model completes the embedding and extraction
process in each training iteration and constrains the
imperceptibility and the integrity of the extracted
watermarks through the designed training objec-arXiv:2409.19627v1  [cs.MM]  29 Sep 2024tives. The attack layer which simulates common
damages on the watermarked media is introduced
into the Embedder-Extractor ( i.e.encoder-decoder)
structure to guarantee the robustness.
Neural audio watermarking is currently in its
early stages. As human auditory perception is
sometimes more sensitive than visual perception
(Lee et al., 2019). It can also easily distinguish
noise, making subtle alterations caused by wa-
termarking to be perceived. As for the robust-
ness of watermarking. The Embedding-Attacking-
Extracting pipeline of neural audio watermarking
is shown in Fig. 1(a), where the attack layer simu-
lates various removal attacks on watermarked au-
dio during training. Redundancy is required for
embedding digital watermarks to enhance applica-
bility. The same watermark is repeatedly embed-
ded at various locations within an audio segment.
However, this strategy raises the issue of locat-
ing. The embedding location of the watermark is
unknown during extraction in practical scenarios.
Additionally, trimming and splicing cause changes
in the watermarking location. Compared to the
traditional method, the extraction of neural audio
watermarking relies on the forward process of neu-
ral networks which induces a non-negligible time
cost that increases with the complexity of the net-
work. Existing methods typically use an exhaustive
approach, extracting synchronization code and wa-
termark message together step by step, as shown
in Fig. 1(b). Localization efficiency is an issue that
neural audio watermarking must face.
The symmetry of the embedding and extraction
processes of watermarking provides the invertible
neural network with ample opportunities, but the
introduction of the attack layer disrupts the orig-
inal symmetry (Liu et al., 2019). In other words,
due to the presence of the attack layer, the output
of the encoder ( i.e.watermarked audio) and the
input of the decoder ( i.e.attack-performed water-
marked audio) are inconsistent, while the encoder
and decoder are opposing and share parameters,
this mismatch limits the training effect.
In this paper, we propose a model called
Invertible Dual-Embedding Audio Watermarking,
IDEA W , which uses a dual-stage invertible neural
network with a dual-embedding strategy to embed
watermark message and synchronization code (re-
ferred to as locating code in this paper) separately,
as illustrated in Fig. 1(c). During extraction, we
first extract the less computational cost locating
code. Upon successful matching, the extraction ofthe message which has more computational cost is
conducted. This also makes it possible to enlarge
the capacity of watermarking flexibly. To alleviate
the asymmetric impact caused by the attack layer,
we apply a balance block to enhance the training
stability while preserving the characteristics of the
invertible neural network. Our contributions in this
paper can be summarized as follows:
•Considering the characteristics of neural audio
watermarking, we design a dual-embedding
strategy to embed the watermark message and
locating code separately, to accelerate the lo-
cating process.
•We introduce the balance block to alleviate the
asymmetry caused by the attack layer between
the invertible network and retain the symmetry
of the invertible neural network.
•The proposed watermarking model is able to
embed more bits of watermark while ensuring
imperceptibility and robustness.
2 Related Work
2.1 Neural Audio Watermarking
The neural audio watermarking model is typically
composed of two neural networks for watermark
embedding and extraction in the Short Time Fourier
Transform (STFT) or Discrete Wavelet Transform
(DWT) domain. Pavlovi ´cet al. (Pavlovi ´c et al.,
2020) design an Encoder-Decoder architecture for
speech watermarking, where the encoder and the
decoder form an adversarial relationship and are
trained together. Hereafter, they introduce an
attack layer to their previous work to form an
Encoder-Attack Layer-Decoder structure, enhanc-
ing the robustness of the DNN-based speech water-
marking (Pavlovi ´c et al., 2022). WavMark (Chen
et al., 2023) considers the extraction as the inverse
process of embedding the watermark, and lever-
ages an invertible neural network to perform em-
bedding and extraction for audio watermarking.
DeAR (Liu et al., 2023) focuses on the threat of re-
recording attack to audio watermarking, modelling
re-recording as several differentiable processes. In
addition to imperceptibility and robustness, capac-
ity and locating effectiveness are also important cri-
teria for evaluating audio watermarking methods.
Compared to traditional watermarking methods,
most neural audio watermarking methods sufferINN #1INN #2hostaudiowatermark msg.locating code11100..00,1,1,0..EmbedderAttack Layermarked audio
0,1,1,0..extracted msg.discardDiscriminator¿original/watermarked?
11100..0extracted l. codecompareGaussian Noise
INN #1Extractor
percept.lossinteg. lossident. loss
INN #2
Balance BlockLower-Pass Flt.MP3 Compress...Figure 2: Architecture of IDEAW and the training objectives.
low capacity and do not consider the high con-
sumption resulting from the locating process.
2.2 Invertible Neural Network
NICE (Dinh et al., 2015) firstly introduces the con-
ception of an Invertible Neural Network (INN),
a normalizing flow-based framework, learning a
transformation that converts data that follows the
original distribution to a predefined distribution.
Dinh et al. improve the performance of INN
through convolutional layers in Real NVP (Dinh
et al., 2017). Ardizzone et al. introduce the con-
ditional INN (cINN) (Ardizzone et al., 2019) to
establish control over the generation. Behrmann et
al.(Behrmann et al., 2019) utilize ResNet as the
Euler discretization of ordinary differential equa-
tions and prove that the invertible ResNet can be
constructed by changing the normalization mecha-
nism. INN has been widely applied in generation
(Dinh et al., 2015, 2017; Kingma and Dhariwal,
2018; van der Ouderaa and Worrall, 2019), im-
age super-resolution (Lugmayr et al., 2020), image
compression (Wang et al., 2020), image-to-image
translation (van der Ouderaa and Worrall, 2019),
digital steganography (Lu et al., 2021; Mou et al.,
2023), etc.
3 Method
3.1 Overall Architecture of IDEA W
Fig. 2 showcases the architecture of our proposed
audio watermarking model, IDEAW, which fol-
lows an Embedder-Attack Layer-Extractor struc-
ture, where the embedder and extractor are care-
fully designed as dual-stage structures for embed-
ding messages and locating codes at vertical lat-
itude separately. In the embedding process, anLm−bitbinary watermark message m∈ {0,1}Lm
is embedded into a fixed-length audio chunk xin
the STFT domain via the first stage INN, then the
second stage INN embeds a binary locating code
c∈ {0,1}Lcinto the audio containing mfrom
the former step. The final watermarked audio is
reconstructed through Inverse Short-Time Fourier
Transform (ISTFT). A discriminator distinguishes
between the host audio and the watermarked audio
to guarantee the imperceptibility of watermarking.
In the extraction process, the two-stage extractor
first extracts cfrom the watermarked audio and
thenm, in the reverse order of embedding. An
attack layer is introduced to enhance the robustness
to various removal attacks. The extractor must ac-
curately extract candmfrom the attack-performed
watermarked audio. The balance block aims to alle-
viate the asymmetry introduced by the attack layer,
preserving the symmetry of INN.
3.2 Dual-stage INN for Dual-Embedding
To vertically separate the locating code and mes-
sage, a dual-stage INN is designed, with each stage
designated as INN#1for the embedding and extrac-
tion of watermark message, and INN#2for that of
locating code. Each INN consists of several in-
vertible blocks that take transformed audio and the
watermark ( i.e.the watermark message or locating
code) as inputs, producing two outputs. We refer
to these input-output pairs as two data streams: the
audio stream and the watermark stream. The au-
dio stream outputs watermarked audio during the
embedding process, while the watermark stream
provides a watermark message during extraction.
Fig. 3 showcases the architecture of the invertible
block, in which the block processes the data asInvertible Block
............audio streamwatermark streamηρexp(α(·))ψexp(α(·))φforward processbackward processFigure 3: Structure and forward/backward processes of
the invertible block.
illustrated in Eq. 1:
xi+1=xi⊙exp(α(ψ(si))) + ϕ(si)
si+1=si⊙exp(α(ρ(xi+1))) + η(xi+1)(1)
where xdenotes the host data including the origi-
nal audio fed to INN#1and the audio with message
embedded fed to INN#2,sdenotes the secret data
including the message fed to INN#1and the locat-
ing code fed to INN#2.α(·)is a sigmoid function,
while ψ(·),ϕ(·),ρ(·)andη(·)are subnets which
are constructed from dense blocks.
In the first embedding stage, INN#1embeds the
watermark message into the host audio, where
the audio and bit sequences are transformed into
the time-frequency domain via STFT at first, re-
spectively. Then INN#2embeds the locating code
into the audio stream output of INN#1. The dual-
embedding can be described by Eq. 2. Subscripts
aandwmofINN(·)denote the output of the audio
stream and watermark stream of INN, respectively.
xwmd=INN#2(INN#1(x, m)a, c)a (2)
The extraction process of IDEAW is exactly the
opposite of the embedding process. INN#2firstly
extract the locating code from the watermarked
audio, then the output of the audio stream from
INN#2is sent to INN#1to extract watermark mes-
sage. Eq. 3 describes the dual extraction to obtain
the embedded message.
ˆm=INN#1R(INN#2R(xwmd, xaux 2)a, xaux 1)wm
(3)
where xauxrepresents the randomly sampled sig-
nal which is fed to the message stream of INN. The
subscript Rrepresents the reverse process of the re-
versible network as the extraction process employs
the same network and parameters as the embedding
process, only with the order reversed.
During training, the message is extracted by
INN#1Rafter each stage of embedding, then com-
pared with the original message m. The locatingcode is extracted from the final watermarked au-
dio via INN#2Rand compared with the original
locating code. The integrity loss is as follows.
Linteg=||ˆm−m||2
+||INN#1R(INN#1(x, m)a, xaux 1)wm−m||2
+||INN#2R(xwmd, xaux 2)wm−c||2
(4)
In practical scenarios, watermarks are embedded
into several segments of audio as shown in Fig. 1
(c). However, watermarked audio may be trimmed
or spliced (known as the de-synchronization attacks
(Mushgil et al., 2018)), making it challenging to
determine the watermark location. The extractor
extracts and matches the locating code quickly as
INN#2is lighter and costs less computation than
INN#1, extracting the watermark message only if
the locating code is matched. In addition, a similar
training manner to the shift module in WavMark
(Chen et al., 2023) is deployed in our proposed
model, which helps the extractor to gain the ability
to extract watermark from a proximity location.
3.3 Imperceptibility Guaranty
As one of the most important indicators for evaluat-
ing digital watermarking, imperceptibility ensures
that the embedding of the watermark cannot be
distinguished from human auditory perception. In
IDEAW, we take two measures to ensure and im-
prove the imperceptibility of watermarking. The
perceptual loss intuitively requires that the differ-
ence between the watermarked audio and the orig-
inal host audio is as narrow as possible, that is,
reducing the impact of watermarking on the host
audio. The perceptual loss is shown in Eq. 5.
Lpercept =||xwmd−x||2 (5)
What’s more, we leverage a discriminator to dis-
tinguish the watermarked audio from the origin
audio. Essentially, the discriminator is a binary
classifier which classifies host and watermarked
audio labeled with 0and1(denoted as label y)
respectively. The discriminator aims to identify
the watermarked audio while the embedder tries to
hide the watermark so that the embedder and the
discriminator form an adversarial relationship and
mutually force each other during training (Goodfel-
low et al., 2020). The discriminate loss and identify
loss is as follows:
Ldiscr=−y·log(D(x))−(1−y)·log(1−D(x))
(6)Lident=−log(1−D(xwmd)) (7)
3.4 The Robustness and Symmetry of INN
The robustness of a watermarking method is crit-
ical to the attacker’s ability to effectively remove
the embedded watermarks. Generally, attackers try
to remove the watermark from the watermarked au-
dio through several removal techniques including
passing the audio through filters only to maintain
the information that can be perceived by humans,
or adding noises to the watermarked audio to inter-
fere with accurate extraction. Effective watermark
removal requires that the watermark cannot be ex-
tracted correctly from the watermark-removed au-
dio and that the watermark-removed audio should
be usable, i.e.the listening quality should not sig-
nificantly decrease, otherwise the removal of the
watermark is meaningless.
In order to endow the neural audio watermarking
with robustness against various watermark removal
attacks, an attack layer is incorporated into the wa-
termarking model and trained alongside the embed-
ding and extraction networks. The attack layer sub-
jects the watermarked audio ( i.e.the output from
the embedder) to a variety of attacks. Subsequently,
the extractor attempts to extract the locating code
and watermark message from the audio that has
undergone attack as accurately as possible. Consid-
ering the effectiveness of the watermark removal,
the predefined attacks should ensure the quality of
attack-performed watermarked audio does not de-
grade too much. These attacks include Gaussian
additive noise, lower-pass filter, MP3 compression,
quantization, resampling, random dropout, ampli-
tude modification and time stretch.
However, the introduction of the attack layer
disrupts the symmetry of the entire embedding-
extraction process, impacting the training of INN.
We define the integral dual-embedding process
and extraction process as f(·)andf−1(·)respec-
tively. During the embedding process, IDEAW
performs f(x, m, c ) = w, where x, m, c, w are
host audio, watermark message, locating code and
watermarked audio as mentioned above. And wfol-
lows the distribution of watermarked audio PW(w)
(Ma et al., 2022). While in the extraction process,
thanks to the parameter-sharing between the em-
bedder and the extractor, mandccan be easily
sampled with m, c=f−1(w)according to PW(w).
But the including of the attack layer causes changes
inPW(w)leading to a PW′(w′), while the extrac-tion process is still based on the unchanged PW(w),
which affects the performance of watermarking.
The parameter-sharing strategy of INN limits the
extractor’s learning ability to adapt to the attacked
audio, resulting in distorted watermarking perfor-
mance.
To simultaneously maintain the parameter-
sharing of INN and the symmetry of INN’s train-
ing, a balance block is employed to mitigate the
asymmetry caused by the attack layer and stabi-
lize the model’s symmetric structure. The balance
block consists of a group of dense blocks (Huang
et al., 2017) which process the input to equal-size
output, providing extra trainable parameters for
the extractor. This manner learns to transform
the attack-performed watermarked audio distribu-
tionPW′(w′)to the revised distribution PˆW( ˆw)that
close to the expected distribution PW(w)under the
guidance of the mentioned integrity loss, to coun-
teract the effects of the offset introduced by the
attack layer without bothering the embedder.
3.5 Training Strategy
Simultaneously ensuring accuracy, imperceptibility
and robustness is sticky. Therefore, the training of
IDEAW is divided into two stages:
•The first stage only considers the impercepti-
bility and the watermark integrity of extrac-
tion, aiming to build a dual-stage INN that
can embed the watermark imperceptibly and
extract the watermark accurately.
•In the second stage, the requirement for the ro-
bustness of watermarking is introduced. The
attack layer and balance block are incorpo-
rated into the model, and the entire model is
trained collectively.
The same total loss function, as introduced in
Eq. 8, is applied for both training stages, where
λ1,λ2andλ3are weights of each component, but
note that the second stage contains more trainable
parameters. The discriminator is trained along with
the watermarking model in each iteration with the
loss function in Eq. 6. The pseudo script of the
training process as well as the acquisition of the
total loss is shown in Section A.1 of the appendix.
Ltotal=λ1Linteg+λ2Lpercept +λ3Lident (8)4 Experiment
4.1 Settings
4.1.1 Dataset and Implementation
IDEAW is trained on VCTK corpus (Yamagishi
et al., 2016) and FMA corpus (Defferrard et al.,
2017). VCTK comprises over 100 hours of multi-
speaker speech data, while FMA contains a large
amount of music audio. These two types of data are
prevalent in scenarios where audio watermarking
is commonly applied. All the audio is resampled
to 16,000 Hz and split into 1-second segments dur-
ing training. STFT and ISTFT operations with
parameters of {n_fft= 1000 , hop _length =
250, win _length = 1000 }perform the inter-
domain transformation.
Two Adam optimizers (Kingma and Ba, 2015)
with{β1= 0.9, β2= 0.99, ϵ= 10−8, lr= 10−5}
(with a StepLR scheduler) are leveraged for the
introduced two-stage training. Super-parameters
λ1,λ2andλ3in total loss (Eq. 8) are set to 1,0.1
and0.1, respectively. Each stage contains 100,000
iterations. The attacks are sample-wise, each audio
segment in the batch undergoes different types of
attacks during training. The configuration of the
attack layer is shown in Section A.2 of appendix.
We set the length of the locating code to 10 bits,
so there is about a 1/210probability of potential
conflicts with extracted non-locating code bit se-
quences. In each batch, the locating code and wa-
termark message are randomly generated to ensure
that the trained model can handle any arrangement
of 0-1 sequences.
We select two existing neural audio watermark-
ing works, WavMark (Chen et al., 2023) and DeAR
(Liu et al., 2023), as baselines.
MethodSNR ↑ACC ↑Capacity ↑
(dB) (%) ( bps)
DeAR 26.18 99.61 8.8
IDEA W 10+10 40.43 99.64 20
WavMark 38.55 99.35 32
IDEA W 22+10 37.72 99.52 32
IDEA W 46+10 35.41 99.44 56
Table 1: Comparison of the basic metrics with baseline
methods. The ACC of IDEAW is calculated from the
locating code and message.4.1.2 Metrics
Signal-to-noise ratio (SNR) measures the impact
of the watermarking. It is calculated as 10 times
the logarithm of the ratio of host audio power to
watermark noise power.
Accuracy (ACC) measures the differences be-
tween the extracted message and the ground-truth
message. ACC is typically used to measure the
robustness of watermarking methods.
Capacity is the number of watermark bits that can
be embedded per second of audio while ensuring
imperceptibility and ACC of the watermarking.
4.2 Results
4.2.1 Overall Comparison
The comparison of basic metrics of various water-
marking methods is shown in Table 1. As each
baseline watermarking model is designed for differ-
ent capacities, and IDEAW owns the maximum one,
we also train the other two IDEAW models which
have similar or larger capacities as other methods
to alleviate the impact of capacity on comparison.
As the locating code and message are separated in
our proposed model, we take the total length of
the locating code and message as the capacity of
IDEAW and the length of the locating code is fixed
to 10 bits. We find that the watermarking model
with lower capacity obtains better imperceptibility
and higher accuracy. IDEAW gains considerable
SNR and ACC when designed with a large payload.
Fig. 4 illustrates the impact of the watermark-
ing on a low-energy speech waveform and a high-
energy music waveform. The same watermark is it-
eratively embedded into the audio. From Fig. 4(a),
we can see that the original audio in the foreground
and the watermarked audio in the background al-
most overlap. Fig. 5 shows a comparison of the
linear-frequency power spectrograms of the origi-
nal and watermarked audio, illustrating the impact
of the watermarking in the time-frequency domain.
More watermarked audio samples, wave-
form samples and practical application exam-
ples are available at our demo page https://
largeaudiomodel.com/IDEAW .
4.2.2 Robustness Comparison
We measure the watermark extraction accuracy of
each model under different attacks to evaluate their
robustness. Eight common attacks including Gaus-
sian additive noise (GN), lower-pass filter (LF),
MP3 compression (CP), quantization (QZ), ran-
dom dropout (RD), resampling (RS), amplitudeMethodAttack GN LF CP QZ RD RS AM TS
35dB 5kHz 64kbps 290.1% 200% 90% 90%
DeAR 99.61 99.04 99.55 99.63 99.61 99.62 99.61 99.31
IDEA W 10+10 99.47 98.62 99.41 98.86 99.60 99.11 99.49 98.95
WavMark 97.84 98.54 98.81 96.60 98.68 98.42 99.29 95.35
IDEA W 22+10 99.33 98.53 99.15 98.73 99.23 99.02 99.48 98.82
IDEA W 46+10 98.72 98.12 99.00 98.61 98.84 98.87 99.42 98.66
Table 2: Comparison of the robustness with baseline methods. The robustness is evaluated according to ACC(%) ↑
under different watermark removal attacks.
host audiowatermarked audiomagnified residualwatermarked segment
(a)
(c)(b)
Figure 4: Waveforms of (a) host audio (foreground) and
watermarked audio (background), (b) host audio and
tenfold-magnified residual caused by watermarking, (c)
local details (100 points) of the (a) and (b). The left
audio is low-energy speech audio while the right is high-
energy music audio.
modification (AM) and time stretch (TS) are taken
into consideration. As mentioned above, water-
mark removal attacks need to consider the degree
of damage to audio, and the strength settings of
each attack continue the previous research settings
(Chen et al., 2023). The robustness evaluation re-
sults are shown in Table 2. IDEAW shows compa-
rable robustness to the baseline model while with a
larger capacity.
4.2.3 Locating Test
To compare the time cost of each locating method,
we embed the watermark (10-bit locating code or
synchronization code and 46-bit message) at dif-
ferent locations and use different methods for lo-
(a)(b)Figure 5: Linear-frequency power spectrograms of low-
energy speech audio (left) and high-energy music audio
(right). (a) the host audio, (b) the watermarked audio.
cating. The tested methods include (1) the same
as WavMark (Chen et al., 2023), synchronization
code and message are concatenated and embedded
into the audio segment together via a single-stage
INN model. The model is trained with the shift
strategy. The synchronization code and message
are extracted at the same time iteratively during
locating. The step size is 10% of the chunk size,
the same as the baseline method. (2) the proposed
method, only extracts the locating code during the
locating process with a step size of 10% of the
chunk size. Method (1) builds a single stage INN
watermarking model that has the same layer quan-
MethodSNR ↑ ACC ↑
(dB) (%)
M1 32.30 99.40
M2 35.27 98.70
IDEA W 46+10 35.41 99.44
Table 3: Basic metrics comparison for ablation study.MethodAttack GN LF CP QZ RD RS AM TS
35dB 5kHz 64kbps 290.1% 200% 90% 90%
M1 98.25 98.18 98.79 98.73 98.90 98.46 99.28 98.58
M2 98.14 97.79 98.55 96.07 98.24 98.39 97.62 96.57
IDEA W 46+10 98.72 98.12 99.00 98.61 98.84 98.87 99.42 98.66
Table 4: Comparison of the robustness in ablation study. The ACC(%) ↑of the extracted message and locating code
under attack via each model.
Figure 6: Comparison of locating time consumption
for different methods at various watermark embedding
locations (the location is indicated by the seconds from
the start of audio to the watermarking location).
tity as the proposed dual-stage model. Note that in
order to eliminate the impact of errors during locat-
ing as we cannot guarantee the ACC of the model
with a carrying capacity of 56 bits in Method (1),
we only perform extraction without further verifi-
cation, until reaching the watermark location, as
the former experiments show that each model can
reach the ideal accuracy with designed capacity.
The measurement of locating time for each
method is conducted on the same device and takes
an average of 100 processes for each watermark-
ing location. The comparison of the time con-
sumption of each method on the same device is
shown in Fig. 6. The results show that the pro-
posed method reduces time overhead by approxi-
mately 40%∼50%. Especially when the water-
mark is far from the head, the advantage of our
dual-embedding strategy is more obvious.
4.3 Ablation Study
4.3.1 Setting Up
To validate the positive effects of the proposed bal-
ance block and the discriminator on IDEAW’s per-
formance, we conduct an ablation study. The fol-
lowing models related to the proposed methods arebuilt. (1) M1(w/o discriminator ) removes the dis-
criminator from the proposed method. (2) M2(w/o
balance block ) removes the balance block from
the proposed method during the robustness train-
ing. These models as well as the proposed model
are trained on the same datasets and in the same
manner. We measure the basic metrics and the
robustness of each model.
4.3.2 Ablation Study Results
The results of the ablation study are shown in Table
3 and Table 4. The results show that the discrimina-
tor helps improve the quality and naturalness of the
watermarked audio, as the model without discrimi-
nator has a degradation in the signal-to-noise ratio
and comparable robustness to the proposed method.
The balance block does alleviate the asymmetry
caused by the attack layer in robustness training,
enabling the model to gain better robustness and
achieve higher accuracy. Overall, the introduction
of the discriminator and balance block enhances
the watermarking quality and the stability of the
watermarking model.
5 Conclusion
In this paper, we propose a neural audio water-
marking model, IDEAW, which embeds the locat-
ing code and watermark message separately via a
designed dual-stage INN. In response to the chal-
lenge of neural audio watermarking localization,
the dual-embedding strategy avoids the huge com-
putation of extracting synchronization code and
message at the same time, accelerating the locating
process. On the other hand, we make an effort to
mitigate the asymmetry factor introduced by the
attack layer. The balance block is leveraged to
provide the extractor with a subtle training spatial
while maintaining the advantage of the invertible
network. Experimental results show that IDEAW
achieves satisfactory performance from a compre-
hensive perspective of imperceptibility, capacity,
robustness and locating efficiency.Limitations
This work focuses on the high overhead problem of
localization of neural audio watermarks by innova-
tively separating the locating code from the water-
mark message and embedding/extracting them sep-
arately. However, we find that the dual-embedding
watermarking method has the following limitations
so far:
•The lengths of the locating code and messages
of the trained model cannot be flexibly ad-
justed because the design of the dual-stage
invertible neural network fixes the lengths of
both, we cannot designate the bits starting at
arbitrary lengths as synchronization codes like
previous methods.
•We find that the watermark embedded in low-
energy audio is less imperceptible than in
high-energy audio via the proposed model.
The reason might be that, apart from selecting
two types of datasets for training, the design
of the model as well as its training strategy
does not consider the carrier’s energy.
Future Works
With the rapid development of social media and
the increasing awareness of copyright, digital wa-
termarking technology has promising prospects for
widespread application. Neural audio watermark-
ing remains a valuable area of research due to its
robustness, independence from expert intervention,
and scalability, compared to traditional methods.
However, neural audio watermarking has not yet
fully surpassed traditional digital watermarking
methods, particularly in terms of capacity. Future
works on neural audio watermarking may consider,
but is not limited to, the following aspects:
•Maximizing capacity through novel neural
network architectures or embedding strate-
gies.
•Exploring adaptive approaches to narrow the
performance gap between high-energy and
low-energy audio.
•Considering implementing watermark em-
bedding directly within the audio generation
model to obtain generated audio with water-
marks, rather than using post-processing wa-
termarking methods.Acknowledgement
Supported by the Key Research and Develop-
ment Program of Guangdong Province (grant No.
2021B0101400003) and the corresponding author
is Jianzong Wang ( jzwang@188.com ).
References
Preetam Amrit and Amit Kumar Singh. 2022. Survey on
watermarking methods in the artificial intelligence
domain and beyond. Computer Communications ,
188:52–65.
Lynton Ardizzone, Carsten Lüth, Jakob Kruse, Carsten
Rother, and Ullrich Köthe. 2019. Guided image gen-
eration with conditional invertible neural networks.
arXiv preprint arXiv:1907.02392 .
Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David
Duvenaud, and Jörn-Henrik Jacobsen. 2019. Invert-
ible residual networks. In International conference
on machine learning , pages 573–582. PMLR.
M Chanchal, P Malathi, and Gireesh Kumar. 2020. A
comprehensive survey on neural network based im-
age data hiding scheme. In the 4th International
Conference on IoT in Social, Mobile, Analytics and
Cloud , pages 1245–1249. IEEE.
Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong
Du, and Furu Wei. 2023. Wavmark: Watermarking
for audio generation. CoRR , abs/2308.12770.
Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst,
and Xavier Bresson. 2017. FMA: A dataset for music
analysis. In The 18th International Society for Music
Information Retrieval Conference, ISMIR , pages 316–
323.
Laurent Dinh, David Krueger, and Yoshua Bengio. 2015.
NICE: non-linear independent components estima-
tion. In The 3rd International Conference on Learn-
ing Representations, ICLR, Workshop Track Proceed-
ings.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
2017. Density estimation using real NVP. In The
Fifth International Conference on Learning Repre-
sentations, ICLR, Conference Track Proceedings .
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2020. Generative
adversarial networks. Communications of the ACM ,
63(11):139–144.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten,
and Kilian Q Weinberger. 2017. Densely connected
convolutional networks. In Conference on Computer
Vision and Pattern Recognition , pages 4700–4708.Israr Hussain, Jishen Zeng, Xinhong Qin, and Shunquan
Tan. 2020. A survey on deep convolutional neural
networks for image steganography and steganalysis.
KSII Transactions on Internet and Information Sys-
tems, 14(3):1228–1248.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In The 3rd In-
ternational Conference on Learning Representations,
ICLR, Conference Track Proceedings .
Durk P Kingma and Prafulla Dhariwal. 2018. Glow:
Generative flow with invertible 1x1 convolutions. Ad-
vances in Neural Information Processing Systems ,
31.
Hyungeol Lee, Eunsil Lee, Jiye Jung, and Junsuk Kim.
2019. Surface stickiness perception by auditory,
tactile, and visual cues. Frontiers in Psychology ,
10:2135.
Chang Liu, Jie Zhang, Han Fang, Zehua Ma, Weim-
ing Zhang, and Nenghai Yu. 2023. Dear: A deep-
learning-based audio re-recording resilient water-
marking. In The 37th AAAI Conference on Artificial
Intelligence , pages 13201–13209.
Yang Liu, Mengxi Guo, Jian Zhang, Yuesheng Zhu, and
Xiaodong Xie. 2019. A novel two-stage separable
deep learning framework for practical blind water-
marking. In The 27th ACM International Conference
on Multimedia , pages 1509–1517.
Shao-Ping Lu, Rong Wang, Tao Zhong, and Paul L
Rosin. 2021. Large-capacity image steganography
based on invertible neural networks. In Proceedings
of the IEEE/CVF conference on computer vision and
pattern recognition , pages 10816–10825.
Andreas Lugmayr, Martin Danelljan, Luc Van Gool,
and Radu Timofte. 2020. Srflow: Learning the super-
resolution space with normalizing flow. In Computer
Vision - ECCV - The 16th European Conference , vol-
ume 12350, pages 715–732.
Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li,
Huizhu Jia, and Xiaodong Xie. 2022. Towards
blind watermarking: Combining invertible and non-
invertible mechanisms. In The 30th ACM Interna-
tional Conference on Multimedia , pages 1532–1542.
Chong Mou, Youmin Xu, Jiechong Song, Chen Zhao,
Bernard Ghanem, and Jian Zhang. 2023. Large-
capacity and flexible video steganography via invert-
ible neural network. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition , pages 22606–22615.
Baydaa Mohammad Mushgil, Wan Azizun Wan Ad-
nan, Syed Abdul-Rahman Al-Hadad, and Sharifah
Mumtazah Syed Ahmad. 2018. An efficient se-
lective method for audio watermarking against de-
synchronization attacks. Journal of Electrical Engi-
neering and Technology , 13(1):476–484.Kosta Pavlovi ´c, Slavko Kova ˇcevi´c, and Igor Ðurovi ´c.
2020. Speech watermarking using deep neural net-
works. In The 28th Telecommunications Forum ,
pages 1–4.
Kosta Pavlovi ´c, Slavko Kova ˇcevi´c, Igor Ðurovi ´c, and
Adam Wojciechowski. 2022. Robust speech wa-
termarking by a jointly trained embedder and de-
tector using a DNN. Digital Signal Processing ,
122:103381.
K. Prabha and I. Shatheesh Sam. 2022. A survey of dig-
ital image watermarking techniques in spatial, trans-
form, and hybrid domains. International Journal of
Software Innovation , 10(1):1–21.
Himanshu Kumar Singh and Amit Kumar Singh. 2023.
Comprehensive review of watermarking techniques
in deep-learning environments. Journal of Electronic
Imaging , 32(3):031804–031804.
Roop Singh, Mukesh Saraswat, Alaknanda Ashok, Hi-
manshu Mittal, Ashish Tripathi, Avinash Chandra
Pandey, and Raju Pal. 2023. From classical to soft
computing based watermarking techniques: A com-
prehensive review. Future Generation Computer Sys-
tems, 141:738–754.
Amita Singha and Muhammad Ahsan Ullah. 2022.
Development of an audio watermarking with de-
centralization of the watermarks. Journal of King
Saud University-Computer and Information Sciences ,
34(6):3055–3061.
Tycho F. A. van der Ouderaa and Daniel E. Worrall.
2019. Reversible gans for memory-efficient image-
to-image translation. In IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 4720–
4728.
Yaolong Wang, Mingqing Xiao, Chang Liu, Shuxin
Zheng, and Tie-Yan Liu. 2020. Modeling lost
information in lossy image compression. CoRR ,
abs/2006.11999.
Junichi Yamagishi, Christophe Veaux, and Kirsten Mac-
Donald. 2016. Cstr vctk corpus: English multi-
speaker corpus for cstr voice cloning toolkit.
Guofu Zhang, Lulu Zheng, Zhaopin Su, Yifei Zeng,
and Guoquan Wang. 2023. M-sequences and sliding
window based audio watermarking robust against
large-scale cropping attacks. IEEE Transactions on
Information Forensics and Security , 18:1182–1195.A Appendix
A.1 Training Pipeline of IDEA W
Alg. 1 shows the training pipeline and the acquisition of each loss function. The length regulators draw
mappings between the bit sequence space ( i.e.locating code and message) and a space whose elements are
of equal length to the audio waveform segment. Note that x, m, c in the pseudo script refers exclusively
to the host audio waveform, message bit sequence and locating code, while the prime′denotes the data in
the STFT domain.
Algorithm 1 Acquisition of total loss Ltotal in the training stage.
Input :
host audio segment x, watermark message m, locating code c
Module :
message embedder Emb 1, locating code embedder Emb 2,
message extractor Ext 1, locating code extractor Ext 2,
length regulator LR1,LR2,LR3,LR4, attack layer Att, balance block B
Operation :
short-time Fourier transform STFT (·), inverse short-time Fourier transform ISTFT (·)
Parameter :
robustness training flag Robust , loss weights λ1, λ2, λ3
Output :
total loss Ltotal
1:Regular the length, transform to STFT domain:
m′←STFT (LR1(m))
c′←STFT (LR2(c))
x′←STFT (x)
2:Embed message:
x′
w1←Emb 1(x′,m′)
3:First extraction:
ˆm1←LR3(ISTFT (Ext 1(x′
w1)))
4:Embed locating code:
x′
w←Emb 2(x′
w1,c′)
5:Obtain watermarked audio waveform:
xw←ISTFT (x′
w)
6:TrainD:
Obtain LDfrom{x,xw}
Perform the backward propagation of D
7:ifRobust =True then
8: xw←B(Att(xw))
9:end if
10:Extract locating code:
x′mid,ˆc′←Ext 2(x′
w)
ˆc←LR4(ISTFT (ˆc′))
11:Extract message:
ˆm←LR3(ISTFT (Ext 1(x′mid)))
12:Obtain losses:
Obtain integrity loss Linteg from{m,ˆm1,ˆm,c,ˆc}
Obtain perceptual loss Lpercpt from{x′,x′
w}
Obtain identify loss Lident from{x,xw}
Ltotal←λ1Linteg+λ2Lpercept +λ3Lident
13:return LtotalA.2 Configuration of the Attacks
The configuration follows previous works. The descriptions and settings are shown in Table 5.
ID Attack Description and Configuration
GN Gaussian additive noise Add Gaussian noise to the watermarked audio and maintain the signal-
to-noise ratio at approximately 35dB.
LF lower-pass filter Pass the audio through a lower-pass filter of 5kHz , the range of the
filter is set according to the human auditory range
CP MP3 compression Compress the waveform to 64kbps MP3 format and then convert back
towavformat. This conversion process results in information loss.
QZ quantization Quantize the sample points of watermarked audio waveform to 29
levels.
RD random dropout Randomly value 0.1%of the sample points in the watermarked audio
to zero.
RS resampling Resample the audio to a new sampling rate ( 200% of the original
sample rate) then resample back to the original sample rate.
AM amplitude modification Multiply 90% to the overall amplitude of the audio by a modification
factor.
TS time stretch Compress the audio in the time domain to 90% of the original length,
then stretch it to maintain the original length.
Table 5: Descriptions and settings of the attacks.