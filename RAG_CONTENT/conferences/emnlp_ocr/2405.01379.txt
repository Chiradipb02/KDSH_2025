Verification and Refinement of Natural Language Explanations
through LLM-Symbolic Theorem Proving
Xin Quan1, Marco Valentino2, Louise A. Dennis1, André Freitas1,2,3
1Department of Computer Science, University of Manchester, UK
2Idiap Research Institute, Switzerland
3National Biomarker Centre, CRUK-MI, University of Manchester, UK
1{name.surname}@manchester.ac.uk
2{name.surname}@idiap.ch
Abstract
Natural language explanations represent a
proxy for evaluating explanation-based and
multi-step Natural Language Inference (NLI)
models. However, assessing the validity of ex-
planations for NLI is challenging as it typi-
cally involves the crowd-sourcing of apposite
datasets, a process that is time-consuming and
prone to logical errors. To address existing lim-
itations, this paper investigates the verification
and refinement of natural language explana-
tions through the integration of Large Language
Models (LLMs) and Theorem Provers (TPs).
Specifically, we present a neuro-symbolic
framework, named Explanation-Refiner, that
integrates TPs with LLMs to generate and for-
malise explanatory sentences and suggest po-
tential inference strategies for NLI. In turn,
the TP is employed to provide formal guaran-
tees on the logical validity of the explanations
and to generate feedback for subsequent im-
provements. We demonstrate how Explanation-
Refiner can be jointly used to evaluate explana-
tory reasoning, autoformalisation, and error cor-
rection mechanisms of state-of-the-art LLMs
as well as to automatically enhance the qual-
ity of explanations of variable complexity in
different domains.1
1 Introduction
A recent line of research in Natural Language Infer-
ence (NLI) focuses on developing models capable
of generating natural language explanations in sup-
port of their predictions (Thayaparan et al., 2021;
Chen et al., 2021; Valentino et al., 2022a; Bostrom
et al., 2022; Weir et al., 2023). Since natural lan-
guage explanations can be used as a proxy to evalu-
ate the underlying reasoning process of NLI models
(Kumar and Talukdar, 2020; Zhao and Vydiswaran,
2021; Chen et al., 2021), researchers have proposed
different methods for assessing their intrinsic qual-
1Code and data are available at: https://github.com/neuro-
symbolic-ai/explanation_refinementity (Camburu et al., 2020; Wiegreffe and Maraso-
vic, 2021; Valentino et al., 2021; Atanasova et al.,
2023; Quan et al., 2024; Dalal et al., 2024), includ-
ing the adoption of language generation metrics
for a direct comparison between models’ generated
explanations and human-annotated explanations.
However, this process is subject to different
types of limitations. First, the use of language
generation metrics requires the crowd-sourcing
of explanation corpora to augment existing NLI
datasets (Wiegreffe and Marasovic, 2021), a pro-
cess that is time-consuming and susceptible to er-
rors (Valentino et al., 2021; Liu et al., 2022; Zhao
et al., 2023). Second, language generation metrics
have been shown to fail capturing fine-grained prop-
erties that are fundamental for NLI such as logical
reasoning, faithfulness, and robustness (Camburu
et al., 2020; Chan et al., 2022; Atanasova et al.,
2023; Quan et al., 2024). Third, human explana-
tions in NLI datasets tend to be incomplete and
contain logical errors that could heavily bias the
evaluation (Elazar et al., 2021; Valentino et al.,
2021).
In this paper, we investigate the integration of
state-of-the-art LLM-based explanation generation
models for NLI with external logical solvers to
jointly evaluate explanatory reasoning (Pan et al.,
2023a; Olausson et al., 2023; Jiang et al., 2024b)
and enhance the quality of crowd-sourced explana-
tions. In particular, we present a neuro-symbolic
framework, named Explanation-Refiner, that in-
tegrates a Theorem Prover (TP) with Large Lan-
guage Models (LLMs) to investigate the following
research questions: RQ1: “Can the integration of
LLMs and TPs provide a mechanism for automatic
verification and refinement of natural language ex-
planations?”; RQ2: “Can the integration of LLMs
and TPs improve the logical validity of human-
annotated explanations?”; RQ3: “To what extent
are state-of-the-art LLMs capable of explanatory
reasoning, autoformalisation, and error correctionarXiv:2405.01379v4  [cs.CL]  11 Oct 2024for NLI in different domains?”. To answer these
questions, Explanation-Refiner employs LLMs to
generate and formalise explanatory sentences and
to suggest potential inference strategies for build-
ing non-redundant, complete, and logically valid
explanations for NLI. In turn, the TP is adopted to
verify the validity of the explanations through the
construction of deductive proofs and the generation
of fine-grained feedback for LLMs.
We instantiate Explanation-Refiner with state-of-
the-art LLMs (i.e., GPT-4 (OpenAI, 2023), GPT-
3.5 (Brown et al., 2020), LLama (Touvron et al.,
2023), and Mistral (Jiang et al., 2024a)) and the
Isabelle/HOL proof assistant (Nipkow et al., 2002)
utilising Neo-Davidsonian event semantics (Par-
sons, 1990) coupled with First-Order Logic (FOL)
to effectively and systematically translate natural
language sentences into logical forms.
Our empirical analysis, carried out on three NLI
datasets of variable complexity (i.e., e-SNLI (Cam-
buru et al., 2018), QASC (Khot et al., 2019), and
WorldTree (Jansen et al., 2018)), reveals that ex-
ternal feedback from TPs is effective in improv-
ing the quality of natural language explanations,
leading to an increase in logical validity using
GPT-4 from 36% to 84%, 12% to 55%, and 2%
to 37% (on e-SNLI, QASC, and WorldTree respec-
tively). At the same time, the results demonstrate
that integrating external TPs with LLMs can re-
duce errors in autoformalisation, with an average
reduction of syntax errors of 68.67%, 62.31%, and
55.17%. Finally, we found notable differences in
performance across LLMs and NLI datasets, with
closed-sourced LLMs (i.e., GPT-4 and GPT-3.5)
significantly outperforming open-source models
(i.e., Mistral and LLama) on both explanatory rea-
soning and autoformalisation, along with a shared
tendency of LLMs to struggle with increasing ex-
planation complexity.
To summarise, the main contributions of this
paper are:
1.We introduce Explanation-Refiner , a novel
neuro-symbolic framework that integrates
LLMs with an external theorem prover. This
framework automatically verifies and refines
explanatory sentences in NLI tasks using an
objective external feedback.
2.We integrate Neo-Davidsonian event seman-
tics coupled with FOL to effectively translate
natural language sentences into logical formsto minimise semantic information loss. Ad-
ditionally, we introduce a novel method that
leverages a theorem prover and a proof as-
sistant for verifying NLI explanations and a
syntactic refiner to minimise syntax errors in
responses generated by LLMs.
3.We conduct a comprehensive series of exper-
iments with Explanation-Refiner across five
LLMs and three datasets, including 1 to 16
explanatory sentences, covering tasks from
textual entailment to complex multiple-choice
question answering in various domains.
4.We perform extensive analyses to explore the
explanation refinement process, characteris-
ing the LLMs’ inference capabilities and re-
vealing the strengths and limitations of differ-
ent models in producing verifiable, explain-
able logical reasoning for NLI.
2 Explanation Verification and
Refinement
Explanation-based NLI is widely adopted to eval-
uate the reasoning process of multi-step inference
models via the construction of natural language
explanations. In this work, we refer to the fol-
lowing formalisation for Explanation-based NLI:
given a premise sentence pi, a hypothesis sentence
hi, and an explanation Eiconsisting of a set of
facts{f1, f2, ..., f n}, the explanation Eiis logi-
cally valid if and only if the entailment pi∪Ei|=hi
holds. This entailment is considered verifiable if
{pi, Ei, hi}can be translated into a set of logical
formulae Φthat compose a theory Θ. The validity
of the theory Θis subsequently determined by a
theorem prover, verifying whether Θ⊨ψ, where
ψrepresents a logical consequence derived from
the logical form of hi.
In this paper, we aim to automatically verify the
logical validity of an explanation Ei. To this end, if
Θ⊨ψis rejected by the theorem prover, a further
refinement stage should be initiated to refine the
facts{f1, f2, ..., f n}based on external feedback,
resulting in an updated explanation E′
i. Thus, an
explanation is accepted if all the facts are logically
consistent, complementary and non-redundant to
support the derivation.
3 Explanation-Refiner
To verify the logical validity and refine any logical
errors in explanatory sentences for NLI tasks, we(b) Inference, V erification and Refinement(a) Axiomatisation and Theoremisation
Premise : A man gives a speech
at an ornate costume party .
Hypothesis : A man is public 
 speaking.
Initial Explanation : 
If someone gives a speech that 
 means that they are speaking.LLMAutoformalisationaxiom_1 : " ∀x y e1 e2. Someone x ∧  Speech y ∧  Gives
e1 ∧  Agent e1 x ∧  Patient e1 y ⟶  Speaking e2 ∧  Agent
e2 x"
Theor em hypothesis :
assumes asm : "Man x ∧  Speech y ∧  Party z ∧  Ornate z ∧
Costume z ∧  Gives e ∧  Agent e x ∧  Patient e y ∧  At x z"
shows  " ∃x e. Man x ∧  Speaking e ∧  Agent e x"
Neo-Davidsonian Event SemanticsLLM
LLMRough 
Inference
    1. To infer the hypothesis, we need to find the   
        information of a man and the action of           
        public speaking.
     ....
     5. By combining these steps, we can infer the 
         hypothesis by satisfying the information of   
         man (from premise) and public speaking     
        (from premise and explanation 1).
   There are no redundant or not directly related   
   explanation sentences. The proof steps use       
   explanation 1 and the premise sentence.
Proof StrategyLLM
Autoformalise 
Proof Methods proof -
     from asm have "Man x ∧  Speech y ∧  Gives 
     e ∧  Agent e x ∧  Patient e y" by blast
     then have "Man x ∧  Speaking e ∧  Agent e   
     x" using explanation_1 by blast
     then show ?thesis using asm by blast
qed
Solvable
Valid ExplanationDrop irrelevant facts and 
Refine based on the feedback 
from the theorem prover
Failed at then have "Man x ∧
Speaking e ∧  Agent e x"   using
explanation_1 by blastUnsolvable. Feedback on
invalid stepsLLM Refined Explanation : 
 If a man gives a speech, that 
 means he is public speaking.Theorem 
Prover
No Syntax ErrorSyntax Error
Step (1)Step (2)
Step (3)
Step (4)Step (5)
Step (6)Theorem
ProverRefine
Syntax
VerificationNew 
Iteration
Construct the theory with 
formalised axioms,
 theorems and proof methodsIsabelle
TheoryFigure 1: The overall pipeline of Explanation-Refiner: An NLI problem is converted into axioms and theorems for a
theorem prover, along with some proof steps derived from a preliminary inference. In case the proof fails (logically
invalid), the erroneous steps along with the constructed proof strategy are used as feedback to refine the explanation
in a new iteration.
present a neuro-symbolic framework that iteratively
checks and refines the explanation Eibased on ex-
ternal feedback. Figure 1 shows an overview of our
proposed framework. Given an NLI task, to evalu-
ate the logical validity of the entailment, the LLM
is prompted to perform an autoformalisation pro-
cess that transforms natural language sentences into
formal language represented in the form of an Is-
abelle/HOL theory. Each fact f∈Eiis converted
into an axiom ai, where each aiis an element of
the set A={a1, a2, ..., a n}. The premise piand
corresponding hypothesis hi, is converted into a
theorem for proving pi∧B→hi, where B⊆A.
A syntax refinement mechanism is subsequently ap-
plied to the previously transferred symbolic forms.
The theorem prover is implemented as a checker to
identify any syntax errors and provide these error
details as feedback to an LLM, enabling the LLM
to iteratively correct the syntax errors over a fixed
number of iterations, denoted by t.
We can then perform automated reasoning via
the theorem prover. To this end, in step 3 we use
the LLM to generate a rough inference that states a
preliminary proof strategy in natural language and
elicit the facts f∈Eiwhich are sufficient and nec-
essary for entailing the hypothesis hi. Based on thispreliminary proof strategy, the LLM is prompted to
construct and formalise the proof steps for proving
the theorem. In step 5, the theorem prover will ver-
ify the constructed theory by attempting to prove
the theorem. If it is solvable, we consider it a logi-
cally valid explanation. If the prover failed at one
of the proof steps, we adopt the failed steps along
with the applied axioms B⊆Aas an external feed-
back for the LLM. This feedback is used to refine
the logical errors and consequently refine the facts
f∈Ei.
3.1 Autoformalisation
In order to formally verify the logical validity of the
explanations, we adopted Neo-Davidsonian event-
based semantics and FOL.
Neo-Davidsonian Event Semantics Preventing
the loss of semantic information during the repre-
sentation of natural language sentences in logical
forms, such as FOL, poses significant challenges
when using LLMs, particularly with long and com-
plex sentences that are crucial for logical reasoning
(Olausson et al., 2023). Neo-Davidsonian event se-
mantics (Parsons, 1990) focused on event variables
to represent the verb predicates and their corre-
sponding object arguments as semantic roles. Thistheorem hypothesis:
(* Premise: A smiling woman is playing the violin in front of a turquoise background. *)
assumes asm: "Woman x ∧Violin y ∧Background z ∧Turquoise z ∧Smiling x ∧Playing e ∧Agent e
x∧Patient e y ∧InFrontOf x z"
(* Hypothesis: A woman is playing an instrument. *)
shows "∃x y e. Woman x ∧Instrument y ∧Playing e ∧Agent e x ∧Patient e y"
proof -
from asm have "Woman x ∧Violin y ∧Playing e ∧Agent e x ∧Patient e y" byblast
then have "Woman x ∧Instrument y ∧Playing e ∧Agent e x ∧Patient e y" using explanation_1 by
blast
then show ?thesis using asm byblast
qed
Figure 2: An example of representing the premise and hypothesis sentences in Isabelle/HOL theorem includes a
proof constructed by the LLM for verifying the hypothesis.
approach establishes a predicate-argument struc-
ture that preserves the information content and
faithfulness of complex sentences, closer to the
surface form of the sentence (Quan et al., 2024).
For example, the sentence ‘A wolf eating a sheep
is an example of a predator hunting prey’ can be
formalised as follows:
∀xye 1(wolf(x)∧sheep (y)∧eating (e1)
∧agent(e1, x)∧patient (e1, y)→
(∃e2predator (x)∧prey(y)∧
hunting (e2)∧agent(e2, x)∧
patient (e2, y)∧example (e1, e2)))(1)
In 1, the verbs are represented as the events ‘eating’
and ‘hunting,’ where the agent and patient argu-
ments correspond to the entities performing and
receiving the actions within these events, respec-
tively. The logical form example (e1, e2)explicitly
captures the semantic meaning of this sentence: the
event of a wolf eating a sheep as an exemplar of a
predator hunting prey. Similarly, whenever there
are no action verbs involved in a sentence, we use
FOL to represent the static or descriptive aspects.
For instance:
∀x(gravity (x)→force(x)) (2)
∀xy(greater (x, y)→larger (x, y)) (3)
The above logical forms correspond to the sen-
tences ‘gravity is a kind of force’ and ‘greater
means larger’.
Isabelle/HOL Theory Construction A theory
script for the Isabelle/HOL theorem prover contains
theorems that need to be proven from some axioms.
Therefore, we adopt the sentences in an explanation
to construct the set of axioms. For instance:(* Explanation 1: A violin is an instrument. *)
axiomatization where
explanation_1: " ∀x. Violin x −→ Instrument x"
In addition, as illustrated in Figure 2, both the
premise and the hypothesis constitute parts of the
theorem to be proven. In particular, the ‘assumes
asm’ clause includes unquantified, specific propo-
sitions or conjunctions of propositions which are
recognised as known truths (i.e., premises). On the
other hand, the ‘show’ clause denotes the conclu-
sion (i.e., hypothesis) for which we seek to build
a proof through logical deductions based on the
assumed propositions and axioms.
Syntax Error Refiner Recent studies (Olausson
et al., 2023; Gou et al., 2024) have revealed per-
sistent syntax errors when prompting LLMs for
code and symbolic form generation tasks. We cat-
egorised the syntax errors into two distinct sub-
domains based on feedback from Isabelle: type
unification errors and other syntax errors. Type
unification errors primarily arise from mismatches
between declared and actual argument types in log-
ical clauses. Other syntax errors typically involve
missing brackets, undefined entity names, or in-
valid logical symbols. Our process involves using
Isabelle to identify syntax errors in the transferred
theory, extracting these error messages, and then
prompting the LLM with these messages along
with few-shot examples. This guides the model
on how to correct each type of syntax error over a
series of iterations, allowing for continuous verifi-
cation and refinement. Details of the autoformali-
sation prompts are described in Appendix A.4.1.
3.2 Proof Construction
A proof provides a detailed step-by-step strategy
that elucidates the logical connections and unifica-tion among axioms to support the reasoning process
aimed at achieving the solver’s goal. Initially, we
prompt the LLM to create a preliminary proof in
natural language to assess how it infers the hypoth-
esis and to identify which explanatory sentences
are relevant, redundant, or unrelated. Based on
this initial inference, we then guide the LLM to de-
velop a formal proof (Figure 2) that integrates with
Isabelle/HOL to verify the explanatory sentences
(axioms) that are required to derive the hypothe-
sis. The general proof steps generated by an LLM
are in the format ’show Xusing YbyZ’, where
the theorem prover is asked to prove Xgiven the
assumptions Y, using the automated proof tactic
Z. The proof tactic often applied is ’blast’, which
is one of broader Isabelle’s FOL theorem proving
tactics(Paulson, 1999). Additional details of the
proof construction process and the prompts used to
guide the LLMs are described in Appendix A.4.2.
3.3 Verification and Refinement
Finally, the constructed theory, which includes ax-
ioms, theorems, and proof steps, is submitted to
the theorem prover for verification. If the theory is
validated, it outputs a logically valid explanation.
If the proof fails or timeouts, we extract the first
error from the solver’s error message, identify the
corresponding proof step, and locate the related ex-
planatory sentences (axioms) from the theory. We
begin by removing redundant and irrelevant facts
that are not present in the preceding Isabelle/HOL
proof steps or are declared as such in the text infer-
ence strategy. Then, we prompt the LLM to refine
the explanatory sentences by providing it with the
error message, the failed proof step, the associated
proof strategy, and the relevant explanatory sen-
tences for further iteration. This process is iterative
and progressive; with each iteration, the framework
addresses one or more logical errors, continually re-
fining the explanatory sentences to ultimately yield
a logically valid and verifiable explanation. Addi-
tional details on the prompts used for refinement
are described in Appendix A.4.3.
4 Empirical Evaluation
4.1 Datasets
We adopted three different NLI datasets for evalua-
tion: e-SNLI, QASC, and WorldTree, using a total
of 300 samples selected via the sampling strategy
defined in Valentino et al. (2021), which maximises
representativeness and mutual exclusivity acrosssyntactic and semantic features expressed in the
datasets. For multiple-choice question answering,
the task includes a question qaccompanied by a set
of candidate answers C={c1, c2, ..., c n}, with ci
identified as the correct answer. To cast this prob-
lem into NLI, we simply convert qand the correct
answer ciinto a hypothesis hi. On the other hand,
the question’s context, if present, is used to build
the premise pi.
4.2 Models
To integrate Isabelle/HOL as a real-time verifica-
tion tool with LLMs, we employ a Python client
(Shminke, 2022) which communicates with Is-
abelle/HOL as a server backend. This enables the
communication of the constructed theory files and
the extraction of the response messages from Is-
abelle. We conducted experiments using five LLMs
within the proposed framework. The models in-
clude two open-sourced models: Llama2-70b (Tou-
vron et al., 2023) and Mixtral-8x7b (Jiang et al.,
2024a), as well as Mistral-small (mistral-small-
latest) (Mistral AI, 2024), GPT-3.5 (gpt-3.5-turbo)
(Brown et al., 2020), and GPT-4 (gpt-4-0613) (Ope-
nAI, 2023).
4.3 Results
Detailed feedback from an external theorem
prover effectively guides LLMs in verifying and
refining explanations for NLI. To assess the
effectiveness of employing an external theorem
prover to verify and refine explanations in NLI
tasks, we conducted a comparative analysis across
various LLMs (Figure 3). The initially valid ex-
planations represent the percentage of explanations
that can be verified as logically valid without any
further iteration. Although the initial verification
results varied among different models, all LLMs
demonstrated a consistent improvement in refining
the logical validity of the explanations. This pro-
cess highlights the positive impact of the external
feedback but also shows significant differences be-
tween models. We found that lower rates of initial
valid explanations often resulted from syntactic er-
rors, which impeded the theorem prover’s ability
to generate proofs. Despite this initial variability,
all models demonstrate a consistent improvement
in the refinement process across the datasets. No-
tably, GPT-4 outperformed other models, improv-
ing the validity of explanations by 48%, 43%, and
35% across the three datasets, respectively, within
a maximum number of ten iterations (Figure 3).Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Valid Explanations0.016.022.0
19.036.0
7.032.036.055.084.0
0246810
Number of Iterations6.0
3.28
1.582.93
1.96e-SNLI
Initially Valid Explanations
Finally Valid Explanations
Number of Iterations(a)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Valid Explanations3.0 3.0
0.06.012.025.0
12.015.044.055.0
0246810
Number of Iterations3.6 3.584.8
4.11
3.55QASC
Initially Valid Explanations
Finally Valid Explanations
Number of Iterations (b)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Valid Explanations0.01.00.0 0.02.03.06.05.029.037.0
0246810
Number of Iterations3.335.5
4.65.38
4.41WorldTree
Initially Valid Explanations
Finally Valid Explanations
Number of Iterations (c)
Figure 3: The initial and final number of logically valid explanations, along with the average iteration times required
to refine an explanation for each LLM
0 2 4 6 8 10
Iteration Times020406080100Number of Refined Explanations
e-SNLI
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(a)
0 2 4 6 8 10
Iteration Times020406080100Number of Refined Explanations
QASC
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (b)
0 2 4 6 8 10
Iteration Times020406080100Number of Refined Explanations
WorldTree
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (c)
Figure 4: Number of successfully refined explanations at each iteration step.
Figure 4 shows the number of explanations refined
at each iteration across the e-SNLI, QASC, and
WorldTree datasets. On average, we found that an
increasing number of iterations leads to increasing
refinement, with models requiring an average of
five iterations across the datasets.
Explanation length/complexity impacts formal-
isation and verification. The e-SNLI dataset,
which includes only a single explanatory sentence
per example, shows the best overall performance.
In contrast, the multiple-choice question answering
datasets, QASC and WorldTree, exhibit compara-
tively lower performance. QASC typically contains
2 explanatory sentences, while WorldTree ranges
from 1 to 16 sentences. As the number of explana-
tory sentences increases, so does the complexity of
the logical reasoning required. Models show lower
refinement performance in WorldTree when com-
pared to e-SNLI and QASC, with only 3%, 5%, and
5% of Llama-70b, Mixtral-8x7b, and Mistral-small
explanations being refined in WorldTree. Mean-
while, 29% and 35% of explanations are refined
by GPT-3.5 and GPT-4 in WorldTree, respectively.
This process involves synthesising multiple ex-
planatory sentences to fulfill sub-goals, which must
then be integrated to meet the overall hypothesis
goal.Iterative and categorical refinement can mono-
tonically reduce syntactic errors in autoformal-
isation. To evaluate the syntax error refinement
stage, we quantified the presence of syntax errors
in the Isabelle theories both before and after the
iterative refinement process. After a maximum
of three iterations, all models showed significant
reductions, with maximum reductions of 68.67%,
62.31%, and 55.17% from 7.82 to 2.45, 20.27 to
7.64, and 22.91 to 10.27 across the three respec-
tive datasets (see Figure 5). While models like
Llama2-70b and Mixtral-8x7b still exhibit some
syntax errors in the refined theories’ code, this is
primarily due to their inability to perform complex
autoformalisation, especially for multiple and more
complex explanatory sentences such as those in the
WorldTree dataset. This result is consistent with
the percentage of explanations that were success-
fully refined across the models, which suggests that
the autoformalisation process plays a critical role
in the models’ logical reasoning capability.
4.4 Ablation Study
We conducted an ablation study to further evaluate
and disentangle the impact of autoformalisation on
performance. To this end, we adopted GPT-4 exclu-
sively for the autoformalisation component, while
retaining the original models for explanation refine-
ment and proof strategy generation. As shown inLlama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Avg. Number of Theories Contain Syntax Errors75.18
58.55
47.0
33.27
7.8264.55
31.82
23.0
17.64
2.45e-SNLI(a)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Avg. Number of Theories Contain Syntax Errors50.1854.45
50.36
46.1
20.2725.6441.4538.27
22.18
7.64QASC (b)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Avg. Number of Theories Contain Syntax Errors51.2768.45
63.6361.73
22.9141.2753.72 54.27
35.64
10.27WorldTree (c)
Figure 5: The average number of theories containing syntactic errors before and after the syntax refinement process
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Refined Explanations732365584
6567697484e-SNLI
TI+AF(Base model)
TI+AF(GPT-4)
(a)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Refined Explanations25
12154455
4244 454855QASC
TI+AF(Base model)
TI+AF(GPT-4) (b)
Llama2-70b Mixtral-8x7b Mistral-small GPT-3.5 GPT-4020406080100Number of Refined Explanations36 52937
2628313437WorldTree
TI+AF(Base model)
TI+AF(GPT-4) (c)
Figure 6: AF represents the autoformalisation components, and TI represents the textual inference components.
TI+AF (Base Model) indicates the use of the base model for both the autoformalisation and textual inference
components. TI+AF (GPT-4) indicates the use of GPT-4 for the autoformalisation components, while the base
model is used for textual inference.
Figure 6, integrating GPT-4 for autoformalisation
led to a significant increase in the number of expla-
nations successfully refined across all models. For
instance, Llama2-70b with GPT-4 as the formali-
sation component refined explanations from 7% to
65% in the e-SNLI dataset. For the multiple-choice
question answering dataset, GPT-3.5 showed a rela-
tively smaller increase from 44% to 48% and from
29% to 34%. Despite these improvements, a perfor-
mance gap persists between GPT-4 and the other
models, which is attributed to GPT-4’s superior
symbolic reasoning capabilities required for expla-
nation refinement from the identified logical errors.
Explanations are progressively made more com-
plete and consistent through iterative refine-
ment. In order to deliver step-wise logical con-
sistency, explanations need to be made complete
and self-contained, leading to the introduction of
additional explanatory sentences, which increases
the total number of suggested proof steps. There-
fore, we further evaluated how the proof steps vary
when the total number of suggested proof steps
increases, contrasting both refined and unrefined
cases. Figure 7 illustrates this trend. In general,
all models show a positive trend, as the total sug-
gested proof steps increase, the average number of
proof steps processed by the proof assistant alsoincreases. Models like Mistral-small and GPT-3.5
tend to suggest more proof steps to accomplish the
logical goal, which can result in some redundant
steps, such as the significant pulse shown in Figure
7c. For unrefined explanations, as shown in Fig-
ure 7d, 7e and 7f, the progression is steadier but
retains a positive trend, where the models gener-
ally suggest more proof steps in response to the
additional explanatory sentences introduced to cor-
rect a logical error identified from the erroneous
step. We analysed the correlation between average
successful explanatory sentences and total planned
sentences in proofs, detailed in Appendix A.3. Ex-
amples of refined and unrefined explanations are in
Appendix A.5.
4.5 Factual Errors and Trivial Explanations
In addition to evaluating the logical validity of ex-
planations, we also conducted a human evaluation
of the refined explanations, considering factual cor-
rectness and explanation triviality for the two best-
performing models (GPT-3.5 and GPT-4). This
evaluation focused on two questions: “Are the
refined explanatory sentences factually correct?”
and“Is the explanation trivial, merely repeating
or paraphrasing the content of the premise and
hypothesis to achieve logical validity?” . As illus-
trated in Figure 8, our findings indicate that all0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Refined e-SNLI
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b(a)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Refined QASC
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (b)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Refined WorldTree
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (c)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Unrefined e-SNLI
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(d)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Unrefined QASC
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (e)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Unrefined WorldTree
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (f)
Figure 7: Average of proof steps processed by the proof assistant against the total proof steps suggested by the
LLMs in refined and unrefined explanations.
GPT-3.5 GPT-4020406080100Percentage of Explanations100 100 100 100e-SNLI
Factually Correct
Not Trivial
(a)
GPT-3.5 GPT-4020406080100Percentage of Explanations97.73 98.1895.4598.18QASC
Factually Correct
Not Trivial (b)
GPT-3.5 GPT-4020406080100Percentage of Explanations100.0 100.0
86.2197.3WorldTree
Factually Correct
Not Trivial (c)
Figure 8: Human evaluation of refined explanations in terms of factuality and triviality.
refined explanations in the e-SNLI and WorldTree
datasets are consistent with commonsense knowl-
edge. In the QASC dataset, 2.27% and 1.82% of the
explanation refined by GPT-3.5 and GPT-4 contain
sentences misaligned with true world knowledge.
We found that the majority of these errors result
from over-generalisation, such as the sentence All
tetrapods are defined to have four limbs , which
inaccurately includes snakes.
Finally, we found a relatively low number of ex-
planations that repeat or paraphrase the content of
premise and hypothesis. This phenomenon is ab-
sent in e-SNLI and becomes more evident when the
explanatory sentences increase in complexity (i.e.,
WorldTree), leading models sometimes to gener-
ate explanations that do not include any additional
information for the entailment to hold.5 Related Work
5.1 LLMs Self-Refinement from External
Feedback
Self-refinement of LLMs has demonstrated promis-
ing effectiveness in generating faithful and trust-
worthy responses (Pan et al., 2023b). The use of
external feedback to guide LLMs has been exten-
sively studied (Yu et al., 2023; Akyurek et al., 2023;
Olausson et al., 2024a). Previous work such as
Peng et al. (2023) have employed facts retrieved
from external knowledge bases as sources of feed-
back, while Paul et al. (2024) developed a critic
model to provide feedback for reasoning refine-
ment. Additionally, Nathani et al. (2023) have ex-
plored the use of feedback models for automated
feedback generation. Various works have also in-
vestigated tasks related to code generation (Chenet al., 2023; Olausson et al., 2024b) and the creation
of either synthetic or expert-written logical natural
language expressions (Olausson et al., 2023). Quan
et al. (2024) use a differentiable logic reasoner for
verifying and refining explanations via abductive
reasoning, improving logical consistency in ethical
NLI tasks. This paper focuses on the automated
verification and refinement of natural language ex-
planations created by human annotators in NLI
tasks. Our method leverages feedback from exter-
nal solvers to iteratively refine explanations, which
require specific modelling interventions such as
extracting the exact erroneous steps from the theo-
rem prover to effectively refine logical errors in the
explanatory sentences.
5.2 Explanation Generation
Existing work has explored robust and effective ap-
proaches for multi-hop reasoning tasks in explana-
tion generation (Thayaparan et al., 2021; Valentino
et al., 2022b; Neves Ribeiro et al., 2022). In prior
research, metrics such as Mean Average Precision
(MAP) (Valentino et al., 2022a) have been em-
ployed to assess the ranking of facts in explana-
tion generation tasks against gold-standard explana-
tions. Although these metrics effectively measure
precision relative to these standards, they inade-
quately capture the logical consistency and com-
pleteness of the explanations generated. Such short-
comings are particularly critical in tasks that re-
quire not only factual accuracy but also coherence
and inferential soundness, as in natural language
inference and explanation generation. Our pro-
posed metrics address this gap by incorporating
assessments of logical validity. Although some
metrics have been proposed to manually evaluate
the logical validity of explanations (Valentino et al.,
2021; Yuan et al., 2024), such as non-redundancy
or logical errors, these require significant effort
from domain experts in formal languages. In this
work, we use human-annotated explanations as a
foundational dataset to detect and correct logical
discrepancies, offering a framework adaptable for
automatically enhancing both the precision and log-
ical integrity of outputs across multi-step inference
tasks.
5.3 Autoformalisation
Autoformalisation refers to the process of trans-
lating natural language descriptions into symbolic
representations. Research in this area has included
the formalisation of mathematical proofs (Cunning-ham et al., 2022; Wu et al., 2022; First et al., 2023;
Jiang et al., 2023), and efforts to transform nat-
ural language sentences into logical forms using
LLMs (Pan et al., 2023a; Olausson et al., 2023;
Jiang et al., 2024b; Dalal et al., 2024). However,
contextual information is frequently lost when sen-
tences are translated in these logical frameworks.
To mitigate semantic loss during the transforma-
tion process, we leverage Neo-Davidsonian event
semantics, which aims to maximise the preserva-
tion sentence-level content. This representation
paradigm can facilitate a more systematic content-
preserving translation to logical forms, which is
more independent from particular choices of repre-
sentation schema.
6 Conclusion
In this work, we present a novel neuro-symbolic
framework, Explanation-Refiner, which integrates
LLMs and theorem provers for automatic verifi-
cation and refinement of natural language expla-
nations through iterative cycles. Extensive exper-
iments on textual entailment and multiple-choice
QA tasks showed improved logical validity of
human-annotated explanations. We investigated
the model’s performance from simple to complex
explanatory/sentence structures and introduced a
method to prevent the loss of semantic information
in autoformalisation tasks with error correction.
In future work, we aspire to enhance the frame-
work’s robustness towards complex and unstruc-
tured explanations with fewer iterations required to
improve the model’s efficiency.
Limitations
While this work have demonstrated significant im-
provements in terms of enhancing the logical con-
sistency of explanations, the connection between
logical consistency and AI safety still needs fur-
ther investigation. While the idea of using for-
mal solvers in conjunction with LLMs delivers a
promise avenue to improve the consistency of rea-
soning within LLMs, these methodologies need to
be further developed and critically assessed as a
mechanism which can provide guarantees of cor-
rectness, consistency and completeness within crit-
ical application domains.
Acknowledgments
This work was partially funded by the Swiss Na-
tional Science Foundation (SNSF) project Neu-Math (200021_204617), by the EPSRC grant
EP/T026995/1, “EnnCore: End-to-End Concep-
tual Guarding of Neural Architectures” under Secu-
rity for all in an AI enabled society, by the CRUK
National Biomarker Centre, and supported by the
Manchester Experimental Cancer Medicine Centre
and the NIHR Manchester Biomedical Research
Centre.
References
Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan,
Peter Clark, Derry Tanti Wijaya, and Niket Tandon.
2023. RL4F: Generating natural language feedback
with reinforcement learning for repairing model out-
puts. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 7716–7733, Toronto,
Canada. Association for Computational Linguistics.
Pepa Atanasova, Oana-Maria Camburu, Christina Li-
oma, Thomas Lukasiewicz, Jakob Grue Simonsen,
and Isabelle Augenstein. 2023. Faithfulness tests
for natural language explanations. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) ,
pages 283–294, Toronto, Canada. Association for
Computational Linguistics.
Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and
Greg Durrett. 2022. Natural language deduction
through search over statement compositions. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022 , pages 4871–4883, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Oana-Maria Camburu, Tim Rocktäschel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language explana-
tions. In Advances in Neural Information Processing
Systems , volume 31. Curran Associates, Inc.
Oana-Maria Camburu, Brendan Shillingford, Pasquale
Minervini, Thomas Lukasiewicz, and Phil Blunsom.
2020. Make up your mind! adversarial generation
of inconsistent natural language explanations. InProceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4157–
4165, Online. Association for Computational Lin-
guistics.
Chun Sik Chan, Huanqi Kong, and Liang Guanqing.
2022. A comparative study of faithfulness metrics
for model interpretability methods. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 5029–5038, Dublin, Ireland. Association for
Computational Linguistics.
Qianglong Chen, Feng Ji, Xiangji Zeng, Feng-Lin
Li, Ji Zhang, Haiqing Chen, and Yin Zhang. 2021.
KACE: Generating knowledge aware contrastive ex-
planations for natural language inference. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 2516–2527,
Online. Association for Computational Linguistics.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language models
to self-debug.
Garett Cunningham, Razvan Bunescu, and David
Juedes. 2022. Towards autoformalization of math-
ematics and code correctness: Experiments with el-
ementary proofs. In Proceedings of the 1st Work-
shop on Mathematical Natural Language Processing
(MathNLP) , pages 25–32, Abu Dhabi, United Arab
Emirates (Hybrid). Association for Computational
Linguistics.
Dhairya Dalal, Marco Valentino, André Freitas, and
Paul Buitelaar. 2024. Inference to the best explana-
tion in large language models.
Yanai Elazar, Hongming Zhang, Yoav Goldberg, and
Dan Roth. 2021. Back to square one: Artifact detec-
tion, training and commonsense disentanglement in
the Winograd schema. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 10486–10500, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Emily First, Markus N. Rabe, Talia Ringer, and Yuriy
Brun. 2023. Baldur: Whole-proof generation and
repair with large language models.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.
Critic: Large language models can self-correct with
tool-interactive critiquing.
Peter Jansen, Elizabeth Wainwright, Steven Mar-
morstein, and Clayton Morrison. 2018. WorldTree:
A corpus of explanation graphs for elementary sci-
ence questions supporting multi-hop inference. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018) , Miyazaki, Japan. European Language Re-
sources Association (ELRA).Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2024a.
Mixtral of experts.
Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,
Wenda Li, Jiacheng Liu, Mateja Jamnik, Timoth’ee
Lacroix, Yuhuai Wu, and Guillaume Lample. 2023.
Draft, Sketch, and Prove: Guiding formal theorem
provers with informal proofs. In International Con-
ference on Learning Representations .
Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen.
2024b. Leanreasoner: Boosting complex logical
reasoning with lean. In Proceedings of the 2024
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies .
Tushar Khot, Peter Clark, Michal Guerquin, Pe-
ter Alexander Jansen, and Ashish Sabharwal. 2019.
QASC: A dataset for question answering via sentence
composition. In AAAI .
Sawan Kumar and Partha Talukdar. 2020. NILE : Natu-
ral language inference with faithful natural language
explanations. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 8730–8742, Online. Association for
Computational Linguistics.
Haochen Liu, Joseph Thekinen, Sinem Mollaoglu,
Da Tang, Ji Yang, Youlong Cheng, Hui Liu, and
Jiliang Tang. 2022. Toward annotator group bias in
crowdsourcing. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1797–1806,
Dublin, Ireland. Association for Computational Lin-
guistics.
Mistral AI. 2024. https://docs.mistral.ai/ .
Deepak Nathani, David Wang, Liangming Pan, and
William Wang. 2023. MAF: Multi-aspect feedback
for improving reasoning in large language models.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
6591–6616, Singapore. Association for Computa-
tional Linguistics.
Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui
Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen,
Peng Xu, Zhiheng Huang, Andrew Arnold, and Dan
Roth. 2022. Entailment tree explanations via itera-
tive retrieval-generation reasoner. In Findings of the
Association for Computational Linguistics: NAACL
2022 , pages 465–475, Seattle, United States. Associ-
ation for Computational Linguistics.Tobias Nipkow, Markus Wenzel, and Lawrence C Paul-
son. 2002. Isabelle/HOL: a proof assistant for
higher-order logic . Springer.
Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,
Armando Solar-Lezama, Joshua Tenenbaum, and
Roger Levy. 2023. LINC: A neurosymbolic approach
for logical reasoning by combining language models
with first-order logic provers. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 5153–5176, Singapore.
Association for Computational Linguistics.
Theo X. Olausson, Jeevana Priya Inala, Chenglong
Wang, Jianfeng Gao, and Armando Solar-Lezama.
2024a. Is self-repair a silver bullet for code gen-
eration? In International Conference on Learning
Representations (ICLR) .
Theo X. Olausson, Jeevana Priya Inala, Chenglong
Wang, Jianfeng Gao, and Armando Solar-Lezama.
2024b. Is self-repair a silver bullet for code genera-
tion?
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Liangming Pan, Alon Albalak, Xinyi Wang, and
William Wang. 2023a. Logic-LM: Empowering large
language models with symbolic solvers for faithful
logical reasoning. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
3806–3824, Singapore. Association for Computa-
tional Linguistics.
Liangming Pan, Michael Saxon, Wenda Xu, Deepak
Nathani, Xinyi Wang, and William Yang Wang.
2023b. Automatically correcting large language
models: Surveying the landscape of diverse self-
correction strategies.
Terence Parsons. 1990. Events in the semantics of en-
glish: A study in subatomic semantics.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2024. REFINER: Reasoning feedback on
intermediate representations. In Proceedings of the
18th Conference of the European Chapter of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1100–1126, St. Julian’s, Malta.
Association for Computational Linguistics.
Lawrence Charles Paulson. 1999. A generic tableau
prover and its integration with isabelle. J. Univers.
Comput. Sci. , 5:73–87.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
your facts and try again: Improving large language
models with external knowledge and automated feed-
back.Xin Quan, Marco Valentino, Louise Dennis, and An-
dre Freitas. 2024. Enhancing ethical explanations
of large language models through iterative symbolic
refinement. In Proceedings of the 18th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
1–22, St. Julian’s, Malta. Association for Computa-
tional Linguistics.
Boris Shminke. 2022. Python client for isabelle server.
Mokanarangan Thayaparan, Marco Valentino, and
André Freitas. 2021. Explainable inference over
grounding-abstract chains for science questions. In
Findings of the Association for Computational Lin-
guistics: ACL-IJCNLP 2021 , pages 1–12, Online.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Marco Valentino, Ian Pratt-Hartmann, and André Fre-
itas. 2021. Do natural language explanations repre-
sent valid logical arguments? verifying entailment in
explainable NLI gold standards. In Proceedings of
the 14th International Conference on Computational
Semantics (IWCS) , pages 76–86, Groningen, The
Netherlands (online). Association for Computational
Linguistics.
Marco Valentino, Mokanarangan Thayaparan, Deborah
Ferreira, and André Freitas. 2022a. Hybrid autore-
gressive inference for scalable multi-hop explanation
regeneration. Proceedings of the AAAI Conference
on Artificial Intelligence , 36(10):11403–11411.
Marco Valentino, Mokanarangan Thayaparan, and An-
dré Freitas. 2022b. Case-based abductive natural
language inference. In Proceedings of the 29th Inter-
national Conference on Computational Linguistics ,
pages 1556–1568, Gyeongju, Republic of Korea. In-
ternational Committee on Computational Linguistics.Nathaniel Weir, Peter Clark, and Benjamin Van Durme.
2023. Nellie: A neuro-symbolic inference engine for
grounded, compositional, and explainable reasoning.
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to
explain: A review of datasets for explainable natural
language processing. In Thirty-fifth Conference on
Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) .
Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus
Rabe, Charles Staats, Mateja Jamnik, and Christian
Szegedy. 2022. Autoformalization with large lan-
guage models. In Advances in Neural Information
Processing Systems , volume 35, pages 32353–32368.
Curran Associates, Inc.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng
Jiang, and Ashish Sabharwal. 2023. Improving lan-
guage models via plug-and-play retrieval feedback.
Li Yuan, Yi Cai, Haopeng Ren, and Jiexin Wang. 2024.
A logical pattern memory pre-trained model for en-
tailment tree generation. In Proceedings of the
2024 Joint International Conference on Computa-
tional Linguistics, Language Resources and Evalua-
tion (LREC-COLING 2024) , pages 759–772, Torino,
Italia. ELRA and ICCL.
Wenting Zhao, Justin Chiu, Claire Cardie, and Alexan-
der Rush. 2023. Abductive commonsense reason-
ing exploiting mutually exclusive explanations. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 14883–14896, Toronto, Canada.
Association for Computational Linguistics.
Xinyan Zhao and V .G.Vinod Vydiswaran. 2021. Lirex:
Augmenting language inference with relevant expla-
nations. Proceedings of the AAAI Conference on
Artificial Intelligence , 35(16):14532–14539.
A Appendix
A.1 Algorithm
Algorithm 1 shows the overall framework of
Explanation-Refiner.
A.2 Scalability
Figure 9 shows the average Isabelle/HOL solving
time against the number of planned explanatory
sentences in a proof and the length of suggested
proof steps, including theories that have syntax
errors, respectively. In some cases, the theorem
prover may get stuck on a proof step, and we have
set a termination time if the solving time exceeds
65 seconds.0 2 4 6 8 10 12 14
Number of Explanatory Sentences020406080100Avg. Isabelle Solving Time/s
T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b(a)
0 2 4 6 8 10 12 14
Length of Suggested Proof Steps020406080100Avg. Isabelle Solving Time/s
T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(b)
Figure 9: (a) Average Isabelle/HOL solving time against number of explanatory sentences planned in a proof. (b)
Average Isabelle/HOL solving time against number of suggested proof steps in a proof.
A.3 Average Processed vs. Planned
Explanatory Sentences per Proof
Figure 10 and Figure 11 shows experiments on
average number of successfully processed explana-
tory sentences in one proof against total planned
explanatory sentences in a suggest proof. Figure
12 also shows the comparison of average processed
proof steps against total suggested proof steps in
all dataset.
A.4 Prompts
Temperature settings were adjusted to 0 for GPT-
3.5 and GPT-4, and to 0.01 for Llama2-70b,
Mixtral-8x7b, and Mistral-small, aiming to achieve
both determinism in the output and effective code
generation for theorem prover.A.4.1 Autoformalisation
Figure 13 displays the prompts used to identify ac-
tion verbs (events) within the premise, explanation,
and hypothesis sentences, representing events in
Davidsonian-event semantics. Figure 14 displays
the prompts used to transfer natural language to
logical forms based on the identified events verbs.
Figure 15 shows how to convert logical forms into
Isabelle/HOL code (axioms and type declaration).
Figure 16 shows how to convert the premise and hy-
pothesis sentences into the Isabelle/HOL theorem
code, based on the previously constructed axioms
code. Figure 17 shows how to refine the syntax
errors based on the types of errors, the provided
code, the error messages, and the locations of the
errors within the code.A.4.2 Proof Construction
Figure 18 shows the prompts for making a pre-
liminary inference strategy, which also identifies
redundant and related explanatory sentences that
will be used for proof generation. Figure 19 shows
the prompts for building the proof steps used for
Isabelle/HOL Proof assistant based on the provided
inference strategy.
A.4.3 Explanation Refinement
Figure 20 shows how to refine the explanatory sen-
tences based on the provided information.
A.5 Examples of Explanation Refinement
Table 1 shows an example from the e-SNLI dataset
of how the explanation changes after each iteration.
Figures 21, 22, and 23 illustrate the Isabelle/HOL
theory code changes during the refinement process.
Table 2 with Figures 24, 25, and 26 also show
another example of how the explanation is refined
after each iteration.
Green code indicates the proof steps that have
successfully progressed, while red code shows
where the proof failed at that step. More examples
can be found at https://github.com/neuro-symbolic-
ai/explanation_refinement.
A.6 Datasets and Theorem Prover
The datasets used in our experiments, including
samples from e-SNLI (Camburu et al., 2018),
QASC (Khot et al., 2019), and WorldTree (Jansen
et al., 2018), are all sourced from open academic
works. We employed Isabelle as the theorem
prover, which is distributed under the revised BSD
license. Additionally, the TCP client used for the
Isabelle server (Shminke, 2022) is licensed under
Apache-2.0.0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Refined e-SNLI
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b(a)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Refined QASC
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (b)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Refined WorldTree
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (c)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Unrefined e-SNLI
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(d)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Unrefined QASC
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (e)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Unrefined WorldTree
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (f)
Figure 10: Average Progressed Explanations against Number of Planned Explanations in Refined and Unrefined
e-SNLI, QASC and WorldTree Dataset
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Refined T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(a)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
Unrefined T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (b)
0 2 4 6 8 10
Number of Planned Explanations012345Avg. Processed Explanations
T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (c)
Figure 11: Average Progressed Explanations against Number of Planned Explanations for Refined, Unrefined, and
Combined Across All Datasets
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Refined T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b
(a)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
Unrefined T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (b)
0 2 4 6 8 10 12 14
T otal Suggested Proof Steps012345678Avg. Processed Proof Steps
T otal Dataset
Mixtral-8x7b
Mistral-small
GPT-3.5
GPT-4
Llama2-70b (c)
Figure 12: Average Processed Proof Steps against Total Suggested Proof Steps for Refined, Unrefined, and Combined
Across All DatasetsAlgorithm 1: Explanation-Refiner
Input : Premise p, Explanation E, Hypothesis h, Isabelle//HOL server isabelle ,
Autoformalisation model ma, Isabelle syntax refinement model msr, Rough inference
model mri, Proof step build model mpr, Facts filter model mf, Explanation refinement
model me
Output : Updated Explanation E
1valid←false
2isabelle_theory ←[ ]
3iterations ←0
4max _iterations ←11
5has_syntax_error ←false
6while not valid anditerations < max _iterations do
7 session_id ←session_build( HOL, isabelle )
8 isabelle .start(session_id)
9 isabelle_theory ←transfer_to_symbolic( p, E, h ,ma)
10 messages, error_content, error_code ←isabelle .check(isabelle_theory)
11 ifsyntax_errors in messages then
12 has_syntax_error ←true
13 it←0
14 while has_syntax_error andit <3do
15 isabelle_theory = refine_syntax(messages, error_content, error_code, isabelle_theory,
msr)
16 messages, error_content, error_code ←isabelle .check(isabelle_theory)
17 ifsyntax_errors in messages then
18 has_syntax_error ←true
19 it←it+ 1
20 else
21 break
22 end if
23 end while
24 end if
25 rough_inference ←make_rough_inference( p, E, h, m ri)
26 proof_steps ←build_proof(rough_inference, mpr)
27 isabelle_theory ←isabelle_theory + proof_steps
28 messages, error_content, error_code ←isabelle .check(isabelle_theory)
29 ifmessages is not empty then
30 message ←messages[0]
31 E←filter(E, rough_inference, proof_steps, mf)
32 E←refine_explanation(message, error_content, error_code, rough_inference, proof_steps,
p, E, H, m e)
33 else
34 valid←true
35 break
36 end if
37 iterations ←iterations + 1
38 isabelle .shutdown()
39end while
40return ESYSTEM: You are an expert in linguistics. You will be provided with some sentences, find any action verbs of these sentences.You need to ignore auxiliary verbs and modal verbs. Some instructions:1. You must give me the answer for all provided sentences.2. Do not add any notes.3. If no premise sentence provided, include it in the answer as none. 4. Retain the answer words in their original form within the provided sentence.USER: Here are some examples:###Hypothesis Sentence: 1. A woman is playing an instrument.Has action: YesActions: 1. playingExplanation Sentence: 1. A violin is an instrument.Has action: NoActions: nonePremise Sentence: 1. A smiling woman is playing the violin in front of a turquoise background.Has action: YesActions: 1. playing###...###<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.Provided sentences:{{input_sentence}}Answer:Figure 13: Prompts for detecting event-related words in
the given sentences
SYSTEM: You are an expert in semantics, formal language and neo-davidsonian event semantics. You will be provided with some sentences and the action verbs involved in those sentences.You need to transfer the sentences into symbolic language. If the sentence has no action, transfer it into formal language using first-order language.If the sentence has one action, transfer it using first-order language and davidsonian event semantics within one event. If the sentence has two more actions, transfer it using first-order language and davidsonian event semantics within at most two events.Some instructions:1.Capture All Information: Ensure the logical form reflects every detail from the sentence.2. Use '⟶' for Certain Verbs: Represent actions like 'cause', 'lead', 'help' that represent an implication, causal relation with '⟶' for clarity.3. Event Variable 'e': Use 'e' for events, actions, with action predicates having 'e' as their sole argument....USER: Here are some examples:###Sentence: Grass is a kind of plant.Has action: NoActions: Logical form: ∀x. Grass(x) ⟶ Plant(x)###Sentence: Squirrels typically eat nuts for energy.Has action: YesActions: 1. eatLogical form: ∀x y z. Squirrels(x) ∧ Nuts(y) ⟶ (∃e. Eat(e) ∧ Agent(e, x) ∧ Patient(e, y) ∧ ForEnergy(y, x))###...<<<<<<<<<<<<<<<<<<<<<<<Strictly followed the instructions that I have claimed.Provided sentences:{{input_sentence}}Answer:
Figure 14: Prompts for converting natural language
sentences into logical form representationsSYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with some sentences and corresponding logical forms (first-order logic and davidsonian event semantics) of those sentences. You need to transfer such logical forms into Isabelle axioms code and define the consts and of the symbolic forms.Some instructions:1. Isabelle axioms code use ∧, ∨, ∀, ∃, ¬, ⟷, ⟶ as logic symbols. Please write the axiom code with these logic symbols.2. Isabelle consts code use ⇒ as logic symbols. Please define...The code structure for axioms is:```begintypedecl entitytypedecl eventconsts  [define the consts here](* Explanation 1: [provided sentence 1 in natural language] *)axiomatization where  explanation_1: [Transfer the logical form into isabelle code here, non-bracketed of the predicate-argument form]...```USER: Here are some examples:###Provided sentences:Explanation Sentence: 1. If the infant is crying, it can be assumed that they are unhappy.Logical form: ∀x e. Infant(x) ∧ Crying(e) ∧ Agent(e, x) ⟶ Unhappy(x)Answer:```begintypedecl entitytypedecl eventconsts  Unhappy :: "entity ⇒ bool"  Infant :: "entity ⇒ bool"  Crying :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"(* Explanation 1: If the infant is crying, it can be assumed that they are unhappy. *)axiomatization where  explanation_1: "∀x e. Infant x ∧ Crying e ∧ Agent e x ⟶ Unhappy x"```###...###<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.Provided sentences:{{explanatory_sentences}}Answer:```answer goes here```Figure 15: Prompts for converting logical form into
Isabelle/HOL code format for building the axioms and
type declaration
SYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with a Hypothesis sentence and a Premise sentence with their corresponding logical forms (first-order logic and davidsonian event semantics). ...Some instructions:1. Isabelle code use ∧, ∨, ∀, ∃, ¬, ⟷, ⟶ as logic symbols. Please write the code with these logic symbols....The code structure for theorem hypothesis is:```theorem hypothesis:  (* Premise: [provided premise sentence in natural language] *)...end```USER: Here are some examples:###Provided sentences:...Provided code:...Answer:```imports Mainbegintypedecl entitytypedecl eventconsts  AdultSponges :: "entity ⇒ bool"  Eggs :: "entity ⇒ bool"  Sperm :: "entity ⇒ bool"  Gametes :: "entity ⇒ bool"  Produce :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"(* Explanation 1: Adult sponges produce eggs and sperm. *)axiomatization where  explanation_1: "∀x. AdultSponges x ⟶ (∃e y z. Eggs y ∧ Sperm z ∧ Produce e ∧ Agent e x ∧ Patient e y ∧ Patient e z)"(* Explanation 2: Sperm and eggs are cells known as gametes. *)axiomatization where  explanation_2: "∀x y. Sperm x ∧ Eggs y ⟶ Gametes x ∧ Gametes y"theorem hypothesis:  (* Premise: Students are studying adult sponges. *)  assumes asm: "Students x ∧ AdultSponges y ∧ Studying e ∧ Agent e x ∧ Patient e y"  (* Hypothesis: Adult sponges produce gametes. *)  shows "∃x y e. AdultSponges x ∧ Gametes y ∧ Produce e ∧ Agent e x ∧ Patient e y"proof -qedend```###...###<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed. Provided sentences:{{input_sentence}}Provided code:{{axiom_code}}Answer:```answer code goes here (complete isabelle code )```
Figure 16: Prompts for building the theorem code part
of the Isabelle/HOL theorySYSTEM: You are an expert in the Isabelle theorem prover and familiar with HOL session syntax and Davidsonian event semantics. You will be provided with Isabelle code containing some syntax errors, along with details of the errors and their locations in the code. You need to fix the code (logical form) of the related error.Some instructions:1. Do not change code structure, you just need to fix the syntax error.2. Type unification failed errors indicates the defined consts and the acutal preidcates are not consistent. There are only two types: event and entity. The type defined in the consts should be same as the type represented in the logical form codes....USER: Here are some examples:###Provided code:...Error Identified: 1. Error on line 15: Type unification failed: Clash of types "entity" and "event"Type error in application: incompatible operand type...Code Cause This Error: explanation_1: "∀x c. Cute x ∧ Couple x ∧ Club c ∧ At x c ⟶ (∃y. Couple y ∧ Club c ∧ At y c)"...Answer:From the error found, the error type is type unification failed, which means the operand type defined in consts is not consistent with the operand in the code.As the error indicates the Operator 'At' in code is defined as At :: "event ⇒ entity ⇒ bool" but in the code it is stated as At x c where x is defined as entity. That's the reason cause type unification failed.It should have the same type which can be refined as:```...Couple :: "entity ⇒ bool"  Club :: "entity ⇒ bool"  At :: "entity ⇒ entity ⇒ bool"  Cute :: "entity ⇒ bool"(* Explanation 1: A cute couple at a club means a couple is at a club. *)axiomatization where  explanation_1: "∀x c. Cute x ∧ Couple x ∧ Club c ∧ At x c ⟶ (∃y. Couple y ∧ Club c ∧ At y c)"theorem hypothesis:  (* Premise: A cute couple at a club *)  assumes asm: "Cute x ∧ Couple x ∧ Club c ∧ At x c"  (* Hypothesis: The couple is at a club. *)  shows "∃x. Couple x ∧ Club c ∧ At x c"proof - qedend```The At :: "event ⇒ entity ⇒ bool" has been refined as At :: "eneity ⇒ entity ⇒ bool", then the types are consistent for both consts and following logical code.###<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed. Provided code:{{code}}Error Identified:{{error_detail}}Code Cause This Error:{{code_cause_error}}Answer:``` answer code goes here (complete refined isabelle code)```Figure 17: Prompts for how to refine the identified
syntax errors in the constructed code
SYSTEM: You are an expert in natural language inference, textual entailment and linguistic semantics. You will be provided with a premise sentence, some explanatory sentences and a hypothesis sentence. The premise sentence and explanatory sentences should entail the hypothesis sentence.You need to write a step-by-step natural language inference to state how the explanatory sentences will entail the hypothesis sentence from the premise sentences.Instructions:1. You must elicit the explanatory sentences which are redundant and not directly related (if there are no redundant or all related state it as no).2. You must state on which step of the proof each explanatory sentence is used. 3. You must elicit the used explanatory sentences in the natural language inference steps.USER: Here are some examples:###Provided Premise Sentence:A group of students are studying non-contact force.Provided Explanation Sentences:1. Non-contact forces can affect objects that are not touching. 2. A magnet attracts magnetic and ferromagnetic metals through magnetism.3. Magnetism does not require contact between objects to act.4. A paper clip is a kind of object.5. A magnet is a kind of object.6. Magnetism is a kind of force.7. A kind of something is an example of that something. Provided Hypothesis Sentence:A paper clip attracted to a magnet is an example of a non-contact force acting on an object.Natural Language Inference Steps:1. As we need to infer the hypothesis, we need to find the information of paper clip, magnet, non-contact force and object. The action event of attracted and acting. The relationship of is an example of.2. From the premise, we can get the information of non-contact force.3. From explanation 4 and 5, we deduce that both a paper clip and a magnet are objects.4. Explanation 2 establishes that a magnet can attract certain metals through magnetism, which is a force (due to explanation 6)....Explanation 1 is redundant. There is no not directly related explanation sentence.The proof steps use explanation 2, explanation 3, explanation 4, explanation 5, explanation 6, explanation 7. ###...###<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.Provided Premise Sentence:{{premise}}Provided Explanation Sentences:{{explanation}}Provided Hypothesis Sentence:{{hypothesis}}Natural Language Inference Steps:Figure 18: Prompts for how to make a step-by-step
preliminary inference strategySYSTEM: You are an expert in Isabelle theorem prover, first-order logic and Davidsonian event semantics. You will be provided with an Isabelle code which consistent of some axioms, a theorem hypothesis that needs to be proven. The logical form of axioms indicates some explanatory sentences, the logical form after "assume asm:" indicates a premise sentence and the logical form after "shows" indicates a hypothesis sentence. ...Some instructions:1. 'sorry' and ‘fix’ command is not allowed. ...USER: Here are some examples:###Provided Isabelle Code:```...begintypedecl entitytypedecl eventconsts  PlantReproduction :: "entity ⇒ bool"...(* Explanation 1: Plant reproduction often requires pollen. *)axiomatization where  explanation_1: "∀x y e. PlantReproduction x ∧ Pollen y ∧ Require e ∧ Agent e x ∧ Patient e y"theorem hypothesis:  (* Premise: Students are studying plant reproduction process. *)  assumes asm: "Students x ∧ PlantReproduction y ∧ Studying e ∧ Agent e x ∧ Patient e y"  (* Hypothesis: Plant reproduction often requires bees. *)  shows "∃x y e. PlantReproduction x ∧ Bee y ∧ Require e ∧ Agent e x ∧ Patient e y"proof - qedend```Provided Natural Language Inference Strategy:1. As we need to infer the hypothesis, we need to find the information of plant, reproduction process, requires action and bees.2. From explanation 1, we get the information of plant reproduction, which requires pollen....Explanation 3 and 4 is not related and Explanation 5 is redundant.The proof steps use explanation 1 and explanation 2.Answer: ```proof -  from asm have "PlantReproduction x" by simp  then obtain e1 where e1: "Require e1 ∧ Agent e1 x ∧ Patient e1 y" using explanation_1 by blast  then have "Bee y" using explanation_2 by blast  have conclusion: "Require e1 ∧ Agent e1 x ∧ Patient e1 y" using e1 by simp  show ?thesis using asm conclusion `Bee y` by blastqed```###...<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.Provided Isabelle Code:{{isabelle_code}}Provided Natural Language Inference Strategy:{{rough_inference}}Answer:Figure 19: Prompts for how to build a proof for Is-
abelle/HOL proof assistant
SYSTEM: You are an expert in Isabelle theorem prover, first-order, Davidsonian event semantics and natural language inference. You will be provided with three types of sentences: Premise Sentence, Explanation Sentence and Hypothesis sentence. ...Some instructions:1. Only refine the related axioms/explanatory sentence in natural language sentences....USER: Here are some examples:###Provided Premise Sentence:...Natural Language Inference steps:1. To infer the hypothesis, we need to identify the information related to a tennis ball, water, and the action of floating. The relationship of "will" indicates a future or potential action....Isabelle code:...(* Explanation 5: water is a kind of liquid. *)axiomatization where  explanation_5: "∀x. Water x ⟶ Liquid x"  ...proof -  from asm have "TableTennisBall x " by simp  then have "Object x" using explanation_1 by blast  then obtain e1 where e1: "Contains e1 ∧ Agent e1 x ∧ Patient e1 y" using explanation_2 by blast...qed...Proof failed at:then have "Object x" using explanation_1 by blastRefine strategy:From the provided error location, it failed at the step of "then have "Object x" using explanation_1 by blast" using explanation 1. ...Updated explanatory sentences:1. a table tennis ball is a kind of object.2. a tennis ball contains air.3. something that contains air is usually buoyant.4. buoyant means able to float in a liquid or gas.5. water is a kind of liquid. ###...<<<<<<<<<<<<<<<<<<<<<<<Strictly follow the instructions that I have claimed.Provided Premise Sentence:{{premise}}Provided Explanation Sentences:{{explanation}}Provided Hypothesis Sentence:{{hypothesis}}Natural Language Inferece steps:{{rough_inference}}Isabelle code:{{isabelle_code}}Proof failed at:{{error_code}}Refine strategy:Updated explanatory sentences:Figure 20: Prompts for how to refine the explanatory
sentencesDataset Sentences Explanation Iteration Validity
e-SNLI Premise : A woman in black
framed glasses peruses a photo
album while sitting in a red
wicker chair.
Hypothesis : There is a lady
with a book.The lady is looking through a
photo album which is a type of
book.0 Invalid
e-SNLI Premise : A woman in black
framed glasses peruses a photo
album while sitting in a red
wicker chair.
Hypothesis : There is a lady
with a book.A woman can be referred to as a
lady. A photo album is a type of
book.1 Invalid
e-SNLI Premise : A woman in black
framed glasses peruses a photo
album while sitting in a red
wicker chair.
Hypothesis : There is a lady
with a book.A woman can be referred to as a
lady. A photo album is a type of
book. If a woman is perusing a
photo album, then the woman is
with a book.2 Valid
Table 1: An example of how the explanation sentences in e-SNLI can be refined with Explanation-Refiner
theory question_28_0imports Mainbegintypedecl entitytypedecl eventconsts  Lady :: "entity ⇒ bool"  PhotoAlbum :: "entity ⇒ bool"  Book :: "entity ⇒ bool"  Looking :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"  Woman :: "entity ⇒ bool"  Glasses :: "entity ⇒ bool"  BlackFramed :: "entity ⇒ bool"  Chair :: "entity ⇒ bool"  Red :: "entity ⇒ bool"  Wicker :: "entity ⇒ bool"  Peruses :: "event ⇒ bool"  Sitting :: "entity ⇒ entity ⇒ bool"  With :: "entity ⇒ entity ⇒ bool"(* Explanation 1: The lady is looking through a photo album which is a type of book. *)axiomatization where    explanation_1: "∃x y e. Lady x ∧ PhotoAlbum y ∧ Book y ∧ Looking e ∧ Agent e x ∧ Patient e y"theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: "Woman x ∧ Glasses x ∧ BlackFramed x ∧ PhotoAlbum y ∧ Chair z ∧ Red z ∧ Wicker z ∧ Peruses e ∧ Agent e x ∧ Patient e y ∧ Sitting x z"(* Hypothesis: There is a lady with a book. *)shows "∃x y. Lady x ∧ Book y ∧ With x y"proof -  from asm have "Woman x ∧ PhotoAlbum y" by blast  from this have "Lady x ∧ Book y" using explanation_1 by blast  then show ?thesis using `Lady x` `Book y` by blastqedend
Figure 21: The Isabelle theory code for table 1 iteration 0theory question_28_1imports Mainbegintypedecl entitytypedecl eventconsts  Woman :: "entity ⇒ bool"  Lady :: "entity ⇒ bool"  PhotoAlbum :: "entity ⇒ bool"  Book :: "entity ⇒ bool"  Glasses :: "entity ⇒ bool"  BlackFramed :: "entity ⇒ bool"  Peruses :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"  Chair :: "entity ⇒ bool"  Red :: "entity ⇒ bool"  Wicker :: "entity ⇒ bool"  SittingIn :: "entity ⇒ entity ⇒ bool"  With :: "entity ⇒ entity ⇒ bool"(* Explanation 1: A woman can be referred to as a lady. *)axiomatization where    explanation_1: "∀x. Woman x ⟶ Lady x"(* Explanation 2: A photo album is a type of book. *)axiomatization where     explanation_2: "∀x. PhotoAlbum x ⟶ Book x"theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: "Woman x ∧ Glasses x ∧ BlackFramed x ∧ PhotoAlbum y ∧ Peruses e ∧ Agent e x ∧ Patient e y ∧ Chair z ∧ Red z ∧ Wicker z ∧ SittingIn x z"(* Hypothesis: There is a lady with a book. *)shows "∃x y. Lady x ∧ Book y ∧ With x y”proof -  from asm have "Woman x" by blast  then have "Lady x" using explanation_1 by blast  from asm have "PhotoAlbum y" by blast  then have "Book y" using explanation_2 by blast  show ?thesis using `Lady x` `Book y` by blastqedendFigure 22: The Isabelle theory code for table 1 iteration 1theory question_28_2imports Mainbegintypedecl entitytypedecl eventconsts  Woman :: "entity ⇒ bool"  Lady :: "entity ⇒ bool"  PhotoAlbum :: "entity ⇒ bool"  Book :: "entity ⇒ bool"  Perusing :: "event ⇒ bool"...(* Explanation 1: A woman can be referred to as a lady. *)axiomatization where    explanation_1: "∀x. Woman x ⟶ Lady x"(* Explanation 2: A photo album is a type of book. *)axiomatization where    explanation_2: "∀x. PhotoAlbum x ⟶ Book x"(* Explanation 3: If a woman is perusing a photo album, then the woman is with a book. *)axiomatization where    explanation_3: "∀x y e. Woman x ∧ PhotoAlbum y ∧ Perusing e ∧ Agent e x ∧ Patient e y ⟶ With x y"theorem hypothesis:(* Premise: A woman in black framed glasses peruses a photo album while sitting in a red wicker chair. *)assumes asm: "Woman x ∧ Glasses x ∧ BlackFramed x ∧ PhotoAlbum y ∧ Chair z ∧ RedWicker z ∧ Perusing e ∧ Agent e x ∧ Patient e y ∧ SittingIn x z"(* Hypothesis: There is a lady with a book. *)shows "∃x y. Lady x ∧ Book y ∧ With x y"proof -  from asm have "Woman x ∧ PhotoAlbum y ∧ Perusing e ∧ Agent e x ∧ Patient e y" by blast  then have "With x y" using explanation_3 by blast  from asm have "Woman x" by blast  then have "Lady x" using explanation_1 by blast  from asm have "PhotoAlbum y" by blast  then have "Book y" using explanation_2 by blast  show ?thesis using `Lady x` `Book y` `With x y` by blastqedendFigure 23: The Isabelle theory code for table 1 iteration 2Dataset Sentences Explanation Iteration Validity
e-SNLI Premise : A male bartender
dressed in all black with his
sleeves rolled up to elbow height
making a drink in a martini
glass.
Hypothesis : A person in blackA bartender, who is a person, is
wearing black.0 Invalid
e-SNLI Premise : A male bartender
dressed in all black with his
sleeves rolled up to elbow height
making a drink in a martini
glass.
Hypothesis : A person in blackA bartender is a person. If a per-
son is wearing black, then the
person is in black.1 Invalid
e-SNLI Premise : A male bartender
dressed in all black with his
sleeves rolled up to elbow height
making a drink in a martini
glass.
Hypothesis : A person in blackA bartender is a person. If a per-
son is dressed in black, then the
person is in black.2 Valid
Table 2: An example of how the explanation sentences in e-SNLI can be refined with Explanation-Refiner
theory question_66_0imports Mainbegintypedecl entitytypedecl eventconsts  Bartender :: "entity ⇒ bool"  Person :: "entity ⇒ bool"  Black :: "entity ⇒ bool"  Wearing :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"  Male :: "entity ⇒ bool"  Drink :: "entity ⇒ bool"  MartiniGlass :: "entity ⇒ bool"  Making :: "event ⇒ bool"  In :: "entity ⇒ entity ⇒ bool"(* Explanation 1: A bartender, who is a person, is wearing black. *)axiomatization where    explanation_1: "∃x e. Bartender x ∧ Person x ∧ Black x ∧ Wearing e ∧ Agent e x ∧ Patient e x"theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: "Bartender x ∧ Male x ∧ Black x ∧ Drink y ∧ MartiniGlass z ∧ Making e ∧ Agent e x ∧ Patient e y ∧ In z y"(* Hypothesis: A person in black *)shows "∃x. Person x ∧ Black x"proof -  from asm have "Bartender x ∧ Black x" by blast  from this have "Person x ∧ Black x" using explanation_1 by blast  then show ?thesis by blastqedend
Figure 24: The Isabelle theory code for table 2 iteration 0theory question_66_1imports Mainbegintypedecl entitytypedecl eventconsts  Bartender :: "entity ⇒ bool"  Person :: "entity ⇒ bool"  Wearing :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Black :: "event ⇒ bool"  InBlack :: "entity ⇒ bool"  Male :: "entity ⇒ bool"  Dress :: "event ⇒ bool"  Sleeves :: "entity ⇒ bool"  RolledUp :: "event ⇒ bool"  ElbowHeight :: "event ⇒ bool"  Drink :: "entity ⇒ bool"  MartiniGlass :: "entity ⇒ bool"  Making :: "event ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"  In :: "entity ⇒ entity ⇒ bool"(* Explanation 1: A bartender is a person. *)axiomatization where    explanation_1: "∀x. Bartender x ⟶ Person x"(* Explanation 2: If a person is wearing black, then the person is in black. *)axiomatization where    explanation_2: "∀x e. Person x ∧ Wearing e ∧ Agent e x ∧ Black e ⟶ InBlack x"theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: "Bartender x ∧ Male x ∧ Black e1 ∧ Dress e1 ∧ Agent e1 x ∧ Sleeves x ∧ RolledUp e2 ∧ Agent e2 x ∧ ElbowHeight e2 ∧ Drink y ∧ MartiniGlass z ∧ Making e3 ∧ Agent e3 x ∧ Patient e3 y ∧ In z y"(* Hypothesis: A person in black *)shows "∃x. Person x ∧ InBlack x"proof -  from asm have "Bartender x" by blast  then have "Person x" using explanation_1 by blast  from asm have "Black e1 ∧ Dress e1 ∧ Agent e1 x" by blast  then have "InBlack x" using `Person x` explanation_2 by blast  show ?thesis using `Person x` `InBlack x` by blastqedendFigure 25: The Isabelle theory code for table 2 iteration 1theory question_66_2imports Mainbegintypedecl entitytypedecl eventconsts  Bartender :: "entity ⇒ bool"  Person :: "entity ⇒ bool"  DressedInBlack :: "entity ⇒ bool"  InBlack :: "entity ⇒ bool"  Male :: "entity ⇒ bool"  SleevesRolledUpToElbowHeight :: "entity ⇒ bool"  Drink :: "entity ⇒ bool"  MartiniGlass :: "entity ⇒ bool"  Making :: "event ⇒ bool"  Agent :: "event ⇒ entity ⇒ bool"  Patient :: "event ⇒ entity ⇒ bool"  In :: "entity ⇒ entity ⇒ bool"(* Explanation 1: A bartender is a person. *)axiomatization where    explanation_1: "∀x. Bartender x ⟶ Person x"(* Explanation 2: If a person is dressed in black, then the person is in black. *)axiomatization where     explanation_2: "∀x. Person x ∧ DressedInBlack x ⟶ InBlack x"theorem hypothesis:(* Premise: A male bartender dressed in all black with his sleeves rolled up to elbow height making a drink in a martini glass. *)assumes asm: "Male x ∧ Bartender x ∧ DressedInBlack x ∧ SleevesRolledUpToElbowHeight x ∧ Drink y ∧ MartiniGlass z ∧ Making e ∧ Agent e x ∧ Patient e y ∧ In z y"(* Hypothesis: A person in black *)shows "∃x. Person x ∧ InBlack x"proof -  from asm have "Bartender x ∧ DressedInBlack x" by blast  then have "Person x ∧ DressedInBlack x" using explanation_1 by blast  then have "Person x ∧ InBlack x" using explanation_2 by blast  then show ?thesis by blastqedendFigure 26: The Isabelle theory code for table 2 iteration 2