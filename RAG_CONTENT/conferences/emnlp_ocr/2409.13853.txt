Unlocking Memorization in Large Language Models with Dynamic Soft
Prompting
Zhepeng Wang1, Runxue Bao2, Yawen Wu3, Jackson Taylor4,
Cao Xiao2,Feng Zheng5,Weiwen Jiang1,Shangqian Gao6*,Yanfu Zhang4*
1George Mason University,2GE Healthcare,
3University of Pittsburgh,4William and Mary,
5Southern University of Science and Technology,6Florida State University
1{zwang48, wjiang8}@gmu.edu ,2{runxue.bao, cao.xiao}@gehealthcare.com ,
3yawen.wu@pitt.edu ,4{jttaylor01, yzhang105}@wm.edu ,
5zfeng02@gmail.com ,6sgao@cs.fsu.edu
Abstract
Pretrained large language models (LLMs) have
revolutionized natural language processing
(NLP) tasks such as summarization, question
answering, and translation. However, LLMs
pose significant security risks due to their ten-
dency to memorize training data, leading to po-
tential privacy breaches and copyright infringe-
ment. Accurate measurement of this memoriza-
tion is essential to evaluate and mitigate these
potential risks. However, previous attempts to
characterize memorization are constrained by
either using prefixes only or by prepending a
constant soft prompt to the prefixes, which can-
not react to changes in input. To address this
challenge, we propose a novel method for es-
timating LLM memorization using dynamic,
prefix-dependent soft prompts. Our approach
involves training a transformer-based generator
to produce soft prompts that adapt to changes
in input, thereby enabling more accurate ex-
traction of memorized data. Our method not
only addresses the limitations of previous meth-
ods but also demonstrates superior performance
in diverse experimental settings compared to
state-of-the-art techniques. In particular, our
method can achieve the maximum relative im-
provement of 112.75% and 32.26% over the
vanilla baseline in terms of discoverable mem-
orization rate for the text generation task and
code generation task respectively.
1 Introduction
Pretrained large language models (LLMs) (Brown,
2020; Touvron et al., 2023; Almazrouei et al., 2023;
Jin et al., 2024) have achieved remarkable success
across a wide range of downstream natural lan-
guage processing (NLP) tasks such as summariza-
tion (Zhang et al., 2024b; Van Veen et al., 2024;
Zhang et al., 2019), classification (Wang et al.,
2023; Sun et al., 2023; Wang et al., 2024; Gao
et al., 2024), question answering (Pan et al., 2023;
*Co-corresponding authors.Zhang et al., 2024a; Shao et al., 2023; Louis et al.,
2024; Jiang et al., 2021; Guo et al., 2023; Yasunaga
et al., 2021) and translation (Zhang et al., 2023;
Bawden and Yvon, 2023; He et al., 2024; Xue
et al., 2020; Xu et al., 2023; Li et al., 2024), etc.
The popularity of LLMs requires people to pay
attention to the unique challenges they bring to
security. One of the significant security issues is
that LLMs can memorize a considerable portion
of their training data even though they tend to not
overfit to their training dataset due to the small
number of training epochs (Radford et al., 2019).
Moreover, the memorized data can be extracted
by carefully designed input from attackers or even
unintentional input from ordinary users, which can
cause privacy and copyright issues with the sensi-
tive training data (Carlini et al., 2021, 2023; Ozdayi
et al., 2023; Nasr et al., 2023; Karamolegkou et al.,
2023). For example, the confidential codes from
Samsung can be exposed to other users after they
were shared with OpenAI due to the memorization
of LLMs (DeGeurin, 2023; Huynh, 2023).
The huge security risks and the potential uses
of memorization make it important to measure the
memorization of the target LLM. With an accurate
method to quantify the intrinsic memorization of
LLMs, model developers can have a better under-
standing of the model’s vulnerability posed by its
memorization and take actions such as machine un-
learning (Yao et al., 2023; Pawelczyk et al., 2023;
Yao et al., 2024) to mitigate the memorization be-
fore they release their LLMs to the public. More-
over, the method to extract memorized data can
also be combined with the target LLM and lever-
aged by the users to detect whether their self-built
dataset has data leakage issues when it is used to
evaluate the target LLM.
To measure the intrinsic memorization of the
target LLM, Carlini et al. (2023) first proposed a
metric called discoverable memorization rate to
serve as the estimation. As shown in Figure 1 (a),arXiv:2409.13853v1  [cs.CL]  20 Sep 2024Target LLM 
Prefix 
TokensOutput Tokens
Target LLM
Prefix 
TokensConstant 
Soft Prompt
Target LLM
Dynamic 
Soft PromptGeneratorPrefix 
Tokensm(Prefix Tokens)(a) Method w/t prompt (b) Method w/t constant
soft prompt
(c) Method w/t dynamic
soft prompt (Ours)Output Tokens
Output TokensFigure 1: Conceptual comparison of three methods for
extracting memorized data from the target LLM.
the given data are split into prefix tokens and suffix
tokens and the prefix tokens are fed into the target
LLM. The data are defined as discoverably memo-
rized when the output tokens can match the suffix
tokens verbatim. Ozdayi et al. (2023) proposed that
more memorized data can be extracted via learn-
ing a soft prompt and prepending it to the prefix
tokens for generation, which is shown in Figure 1
(b). However, the memorization of the target LLM
is still underestimated even with the soft prompt
since it is constant and invariant to prefix tokens,
which may not help or even hinder extracting data
when changing the prefix.
In this paper, we propose a new method to esti-
mate the memorization of LLMs. Compared with
constant soft prompts (Ozdayi et al., 2023), our
method can generate prefix-dependent soft prompts
and react to the changes in inputs. More specifi-
cally, a transformer-based generator is trained for
the generation of the dynamic soft prompt. As
shown in Figure 1 (c), it takes the outputs from the
naive mapping m(·)of prefix tokens as its input
and emits the corresponding soft prompt prepended
to the prefix tokens. This method can customize
prompts given the inputs and thus extract more
memorized data from the target LLM, which can
reflect its intrinsic memorization more accurately.
Our contributions can be summarized as follows.
•We propose a new method with dynamic soft
prompts to extract memorized data from thetarget LLMs and estimate their memorization
with the same assumption as the state-of-the-art
(SOTA) work (Ozdayi et al., 2023) but overcom-
ing its limitation on the invariance to the input.
•We develop a transformer-based generator to pro-
duce the dynamic soft prompts in response to the
change of input. To find the best parameters of
the generator, we utilize a technique to initialize
the transformer blocks within the generator as
identity mappings for the effective and robust
training of the generator.
•We evaluate our method on more diverse settings
than that of the SOTA work (Ozdayi et al., 2023).
Experimental results show that our method can
outperform all the baselines consistently in all
the evaluated settings. The maximum relative im-
provement of 112.75% and 32.26% are achieved
over the vanilla baseline for the text generation
and code generation tasks, respectively.
2 Related Work
LLM Memorization. The memorization of LLM
is firstly verified by Carlini et al. (2021). It shows
that it is feasible for attackers to extract training
data from target LLMs by producing a large num-
ber of random prefixes and feeding them to the
target LLM for generation. Carlini et al. (2023)
then defines the concept of discoverably memo-
rized and utilizes it to quantify the memorization
of the target LLM. In addition to the memorization
of pretrained LLM on the pretraining dataset, Zeng
et al. (2023) studies the memorization of fine-tuned
LLM on the fine-tuning dataset. It shows that
memorization also exists in fine-tuning settings and
that the characteristics of memorization vary with
the type of fine-tuning tasks. Karamolegkou et al.
(2023) shows that the memorization of LLM can
cause copyright violations for books and propri-
etary codes. Nasr et al. (2023) demonstrates that
it is feasible to extract gigabytes of training data
from production LLMs such as ChatGPT due to
their memorization. Recently, Ozdayi et al. (2023)
proposes to learn a constant soft prompt to extract
more training data from LLM to measure memo-
rization. However, we argue that this method still
underestimates the memorization of LLM since the
soft prompt is independent of the input and thus
does not react to the dynamics of the input. Our
method can address these limitations.
Defend against Memorization. Training LLMswith differentially private training (Abadi et al.,
2016) is considered effective in preventing the
memorization of individual training samples with
a theoretical guarantee (Carlini et al., 2021), How-
ever, the training cost is expensive — even pro-
hibitive for LLMs. Moreover, the utility of LLMs
is significantly degraded, making them impractical
for real-world applications. Alternatively, dedupli-
cating training data can mitigate LLM memoriza-
tion (Lee et al., 2021; Kandpal et al., 2022). How-
ever, it cannot eliminate the memorization since
certain portions of data will be memorized by LLM
inevitably even if they only appear once in the train-
ing data. Similarly, Ippolito et al. (2023) shows that
memorization can not be prevented by applying
runtime filtering to the user input. Therefore, the
“ultimate” solution to prevent memorization is still
under exploration. Machine unlearning (Yao et al.,
2023; Pawelczyk et al., 2023; Yao et al., 2024) is a
promising method to defend against memorization.
By identifying the set of memorized training data
to be the forget set for unlearning, LLM can forget
these data via gradient ascent (Yao et al., 2023) or
in-context learning (Pawelczyk et al., 2023). Com-
pared to existing methods, our method can identify
a larger and more accurate forget set for machine
unlearning to defend against memorization.
Prompt Tuning. Training or finetuning machine
learning models is usually costly. To enable effi-
cient training, a variety of methods are proposed
to reduce the training cost via pruning (Bao et al.,
2020, 2022a,b), data selection (Shrivastava et al.,
2016; Wang et al., 2019; Wu et al., 2021) or pa-
rameter selection (Wu et al., 2020; Hu et al., 2021;
Liu et al., 2022; Wang et al., 2024), etc. All of
these methods require adapting the internal param-
eters of the target model. Therefore, applying these
methods to finetuning the LLM may still be expen-
sive due to the large number of parameters of the
LLM. Prompt tuning, introduced by Lester et al.
(2021), is an efficient method for adapting pre-
trained models to various tasks by learning "soft
prompts" that condition frozen language models
without changing their internal parameters. In the
realm of NLP, researchers have harnessed train-
able representations in the form of soft prompts
using methods like prompt-tuning, with Su et al.
(2022) and Vu et al. (2022) demonstrating success-
ful transferability and improved performance. Ma
et al. (2022) uses pruning to remove ineffective
tokens, and Wei et al. (2021) provides theoretical
proof of prompt tuning’s downstream guaranteesunder weaker non-degeneracy conditions. Prompt
tuning has also been applied to vision tasks (Jia
et al., 2022; Lian et al., 2022; Chen et al., 2022), in-
cluding continual learning (Wang et al., 2022) and
image inpainting (Bar et al., 2022). Different from
previous work that used prompt tuning to improve
downstream performance, our work leverages con-
tinuous prompts to more accurately reflect intrinsic
memorization, extract memorized data from the
target LLMs, and measure their memorization.
3 Method
3.1 Problem Formulation
According to the work (Nasr et al., 2023), given the
target LLM fθand data x,xis defined as discover-
ably memorized if there exists a generation routine
G, such that fθ(G(p)) =s, where x= [p||s]and
xis split into prefix pand suffix s. The generation
routine can be constant soft prompts (Ozdayi et al.,
2023), dynamic soft prompts (our method), or just
the identity function (Carlini et al., 2023).
In our problem setting, a set of sequences Dtr
is randomly sampled from the training set Dof
the target LLM fθ, we aim to find the generation
routine Gto maximize discoverable memorization
rateover the training set Dby leveraging Dtr. We
use another disjoint set Dtestrandomly sampled
fromDto evaluate the discoverable memorization
rateoverD, which is defined as,
max1
|Dtest|X
xi∈D test1fθ(G(pi))=si(pi) (1)
where 1(·)denotes the indicator function and xi=
[pi||si].
3.2 Method Overview
To maximize the discoverable memorization rate ,
we propose a pipeline to learn a transformer-based
generator gωto build the generation routine G. As
shown in Figure 2 (b), the generator gωis initial-
ized with Kidentity blocks, which are illustrated
in Section 3.4. The input to gωism(p), where m(·)
represents a naive mapping of prefix tokens pand it
is detailed in Section 3.3. The dynamic soft prompt
ois then generated via gω, where o=gω(m(p)).
Since odepends on the prefix token p, it can adapt
to the change in p. Note that the dimension of o
should be the same as the dimension of the embed-
dingE(x)of the target LLM fθfor its concatena-
tion with the input data x.Scaled Dot-Product
Attention Output Linear Layer 
(Zero-Initialized)Layer NormalizationLinear Layers
(Randomly Initialized)
Wk, Wq, Wv 
(Randomly Initialized)Linear Layer
(Zero-Initialized)
Layer Normalization
InputOutput
MHSAPosition-wise
FFNTarget LLM
(Frozen)Leonardo is a software engineer
 (Leon ardo is a soft ware engineer)working in a startup at Seattle
(work ing in a start up at Seattle)
Generator
(Initialized w/
Identity blocks )Dynamic 
Soft Prompt
m(Prefix Tokens)Prefix Tokens Suffix Tokens
Output Conditional 
Probability Distribution
is a Abby is a
(is a Ab by is a)Generator
(Trained)Dynamic 
Soft Prompt
m(Prefix Tokens)ardo is a software engineer
(ardo is a soft ware engineer)
Traning
Testing
Update
Gradient
Target LLM
(Frozen)
Abby is a
(Ab by is a)Output Tokens
Prefix TokensCompute
Aligned CLM Loss
(a) An identity block implemented via 
zero-initializing specific linear layers(b) Training and testing overview of our methodFigure 2: Illustration of our method.
We train the generator gωonDtrto obtain the
optimized parameters ω∗. For each sequence
xi∈ D tr, where xi= [pi||si], the dynamic soft
prompt oiis generated and then prepended to the
embeddings E(pi)of prefix tokens piand the em-
beddings E(si)of suffix tokens si. Thus, we
obtain the input qito the target LLM fθ, where
qi= [oi||E(pi)||E(si)]. By feeding qito the target
LLM fθ, we aim to minimize the aligned causal
language modeling (CLM) loss L(Ozdayi et al.,
2023) over Dtr, which is defined as,
L=−X
xi∈D tr|qi|−1X
t=kilogPθ,ω(qi,t|qi,1, ..., q i,t−1),
(2)
where qi,trepresents the tth token in the input
sequence qi.Pθ,ω(qi,t|qi,1, ..., q i,t−1)denotes the
output conditional probability distribution at the t
th token given the preceding t−1tokens. kirep-
resents the index of the starting token in suffix si.
Therefore, the aligned CLM loss only focuses on
the token prediction at the position of suffix tokens,
which aligns with the definition of discoverable
memorization . During the training phase, only the
parameters ωofgωare updated based on the gra-
dients calculated from the aligned CLM loss whilethe parameters θoffθare frozen.
During the testing phase of the trained generator
gω∗, for each testing sequence xi∈ Dtest, only the
dynamic soft prompt oiand the embedding of pre-
fix tokens E(pi)are concatenated and sent to the
target LLM fθfor generation. The generated out-
put tokens yiare then compared with the suffix to-
kenssifor evaluation, where yi=fθ([oi||E(pi)]).
3.3 Mapping of Prefix Tokens
According to the constant soft prompt (Ozdayi
et al., 2023), the length of the prompt Nis a hyper-
parameter of the method and its value can affect
the extraction of data. If we feed the prefix tokens
ptogωdirectly, then the length of the dynamic soft
prompt owill be limited to the length of the prefix
tokens p. To provide the same flexibility as the
constant soft prompt (Ozdayi et al., 2023), we pro-
pose a naive mapping m(·)to preprocess the prefix
tokens pand send its output m(p)to the generator
gω.
The details of m(·)are shown in Figure 3 with
an example. Assume the length of pandm(p)is
LandN, respectively. If L≥N,m(p)is the
lastNtokens of p. Otherwise, we first generate r
by duplicating pfor⌈L
N⌉times. m(p)is then theAmily is a dedicated housewife 
living in Philadelphia who likes 
volunteering at the local 
(Am ily is a dedicat ed house wife 
living in Phil adel phia who likes 
volunteer ing at the local)
Alice is an Microbiologist
(Alice is an Micro bi ologist)Length of prefix 
L= 20 tokens
ed housewife living in Philadelphia 
who likes volunteering at the local 
(ed house wife living in Phil adel 
phia who likes volunteer ing at the 
local)
Repeat for 3 timesLength of prefix 
L = 6 tokensLength of Prompt  
N  = 15 tokens  
m(∙) 
Alice is an Microbiologist 
Alice is an Microbiologist
Alice is an Microbiologist
(Alice is an Micro bi ologist
Alice is an Micro bi ologist
Alice is an Micro bi ologist)Generatorm(∙) 
Pick the last 15 tokens
m(∙) 
Pick the last 15 tokensMicrobiologist 
Alice is an Microbiologist
Alice is an Microbiologist
(Alice is an Micro bi ologist
Alice is an Micro bi ologist
Alice is an Micro bi ologist)
18 tokensLength of Prompt  
N  = 15 tokens  GeneratorL ≥ N 
L < N Figure 3: Illustration of naive mapping m(·)with exam-
ples.
Table 1: Case Study on the Effect of Identity Blocks
with Zero-Initialization
ModelIs Zero-
Initialization?Is Dynamic
Prompt?Exact
ERFractional
ER
✗ ✓ 0.000 0.053 GPT-Neo
(125M) ✓ ✓ 0.421 0.557
✗ ✓ 0.000 0.035 GPT-Neo
(1.3B) ✓ ✓ 0.651 0.772
✗ ✓ 0.000 0.022 GPT-Neo
(2.7B) ✓ ✓ 0.702 0.820
lastNtokens of r. The dynamic soft prompt ois
generated, where o=gω(m(p)). In this way, the
length of the prompt Ncan be an arbitrary integer,
which provides the maximum flexibility for usage.
3.4 Identity Blocks with Zero-Initialization
Randomly initializing the transformer-based gen-
erator gωand training it from scratch may degrade
its performance and even lead to model collapse. It
can be verified by a case study for GPT-Neo (Black
et al., 2021), shown in Table 1, where the rows
without zero initialization correspond to random
initializing gω. Table 1 shows that the random ini-
tialization performs badly with the two standard
metrics for memorization being close to 0.
The issue of random initialization is caused by
the fact that the underlying latent space of the dy-
namic soft prompt is far away from the embedding
space of the target LLM fθat the initial stage, mak-
ing it difficult for the target LLM fθto extract
meaningful information from the prompt and thus
hinder the training of the generator gω. Therefore,
to enable the effective and robust training of gω, itis important to align the dynamic soft prompt with
the embedding of input data, making their underly-
ing latent space close to each other. To achieve this,
the tokenizer and embedding layer of the generator
gωshould be initialized with those of the target
LLM fθ. However, this is insufficient due to the
perturbation incurred by the non-identical forward
pass of the transformer blocks within the generator
gω. More specifically, the forward pass for each
attention block can be formulated as,
z=x+MHSA (LN(x)), (3a)
y=z+FFN(LN(z)), (3b)
where xandyare the input and output of the trans-
former block, respectively. zis the output of the
attention layer within the block. MHSA( ·) denotes
the multi-head self-attention (MHSA) mechanism.
LN(·) represents layer normalization. FFN( ·) corre-
sponds to the position-wise feed-forward network
(FFN). Therefore, if the transformer block is ran-
domly initialized, it corresponds to a non-identical
function where y̸=xand thus enlarges the dis-
tance between the latent space of the dynamic soft
prompt and the target LLM fθ.
Inspired by LLAMA PRO (Wu et al., 2024), we
propose to initialize the transformer blocks within
gωas identity blocks to align the dynamic soft
embedding with the token embedding of the tar-
get LLM fθ. To illustrate the implementation of
the identity block shown in Figure 2 (a), we need
to delve into the details of MHSA( ·) and FFN( ·),
which can be formulated as,
MHSA (x′) =HX
i=1σs(x′WQ,iW⊤
K,ix′⊤)x′WV,iWO,i,
(4a)
FFN(z′) = (σ(z′W1)⊙(z′W2))W3, (4b)
where x′andz′are obtained by applying layer
normalization to xandz, respectively. Assume
there are Hheads in MHSA( ·).WQ,i,WK,iand
WV,iare the query, key and value matrix of the ith
head. WO,iis the ith weight matrix of the output
linear layer in the attention block. σsdenotes the
softmax function. For FFN( ·),W1andW2are the
weight matrices of the first layer of linear layers
within the position-wise FFN and σ(·)is the activa-
tion function, while W3is the weight matrix of the
second layer of the linear layer. The FFN defined
in Equation 4b is regularly used in LLaMA mod-
els (Touvron et al., 2023). For Pythia (Bidermanet al., 2023) or GPT-Neo (Black et al., 2021), we
have FFN (z′) =σ(z′W1)W2.
According to Equation 3 and 4, we can conclude
that the identity block can be built by initializing
WOandW3as zero matrices, such that y=x.
Moreover, it has been shown that such kind of zero
initialization does not introduce zero gradients and
thus does not prevent the effective training of the
generator gω(Wu et al., 2024).
Note that we utilize the identity blocks from a
different perspective than LLAMA PRO. LLAMA
PRO incorporated extra multiple identity blocks
into the pretrained LLM to expand the model for
post-pretraining without changing the original map-
ping of the pretrained LLM at the initial stage,
while in our method, we initialize the transformer
blocks within the generator gωas identity blocks
to achieve the identity mapping of the input, thus
aligning the latent space of the dynamic soft prompt
with that of the target LLM fθinitially.
4 Experiments
4.1 Experimental Setup
Models . We evaluate our method on three suites
of pretrained LLMs with various scales: GPT-Neo
(125M, 1.3B, 2.7B) (Black et al., 2021), Pythia
(410M, 1.4B, 2.8B, 6.9B) (Biderman et al., 2023)
and StarCoderBase (1B, 3B, 7B) (Li et al., 2023).
Both GPT-Neo and Pythia are pretrained on the Pile
dataset (Gao et al., 2020) for text generation. Star-
CoderBase is pretrained on The Stack dataset (Ko-
cetkov et al., 2022) with more than 80 program-
ming languages for code generation.
Dataset . We extract training data of GPT-Neo and
Pythia with the Language Model Extraction Bench-
mark dataset (Google-Research), a subset in En-
glish with 15K sequences sampled from the Pile
dataset. For StarCoderBase, we utilize the-stack-
smol dataset (BigCode), a subset randomly sam-
pled from The Stack dataset. In our experiments,
we focus on the java,c#,goandsqlsplits of it.
And there are 40K sequences in total.
Baselines . We compare our method with four
baselines. The baseline No Prompt corresponds
to the method shown in Figure 1 (a), serving as the
vanilla baseline. We include two naive baselines by
prepending hard prompt to the prefix for extraction:
Constant Hard Prompt andDynamic Hard Prompt .
Assuming the length of the prompt is N, for Con-
stant Hard Prompt , we pick the first Ntokens in
the vocabulary of the target LLM to serve as thehard prompt. For Dynamic Hard Prompt , we ap-
ply the mapping m(·)in Section 3.3 to the prefix
to generate the hard prompt without feeding it to
a generator for further processing. CSP (Ozdayi
et al., 2023) corresponds to the method shown in
Figure 1 (b), which is the SOTA work in the mea-
surement of the memorization.
Evaluation Settings . The length of the prompt,
prefix, and suffix is 50 by default without explicit
explanation for evaluation. We use the Exact Ex-
traction Rate (ER) ,Fractional Extraction Rate
(ER),Test loss andTest perplexity (PPL) to evaluate
the performance of our method. Note that Exact
ERcorresponds to discoverable memorization rate
to estimate the verbatim memorization.
4.2 Main Results
The main results to evaluate our method and the
SOTA baselines are summarized in Table 2, 3 and 4
for the suites of GPT-Neo, Pythia and StarCoder-
Base, respectively.
In the application of text generation, our method
can outperform all the baselines consistently and
significantly. For GPT-Neo suite, compared with
the vanilla baseline (i.e., No Prompt ), our method
can achieve a relative improvement of 112.75%,
41.52% and 30.0% in terms of Exact ER with the
model size of 125M, 1.3B and 2.7B, respectively.
For the Pythia suite, our method can achieve a
relative improvement of 117.37%, 48.32%, 29.4%
and 25.13% over the vanilla baseline in terms of
Exact ER with the model size of 410M, 1.4B, 2.8B
and 6.9B, respectively.
In the application of code generation, our method
can also outperform all the baselines consistently
and significantly. For StarCodeBase suite, our
method can achieve a relative improvement of
32.26%, 32.39% and 20.88% over the vanilla base-
line in terms of Exact ER with the model size of
1B, 3B, and 7B, respectively.
We have several observations from the main
results across diverse settings. Firstly, prepend-
ing naive hard prompts such as Constant Hard
Prompt andDynamic Hard Prompt is not useful
but harmful for the exaction of training data from
target LLM, leading to a much lower estimation
of its memorization. Secondly, our method outper-
forms the SOTA work, CSP (Ozdayi et al., 2023)
, across the board, highlighting the importance of
dynamic soft prompts for the measure of mem-
orization. Moreover, the memorization of LLM
increases with the model size, which is consistentTable 2: Main Results on GPT-Neo Suite
Model MethodDynamic
Prompt?Exact
ERExact ER
GainFractional
ERFractional ER
GainTest LossTest Perplexity
(PPL)
No Prompt N/A 0.189 N/A 0.369 N/A 0.953 2.594
Constant Hard Prompt ✗ 0.144 -23.81% 0.326 -11.65% 1.002 2.725
Dynamic Hard Prompt ✓ 0.056 -70.37% 0.153 -58.54% 1.122 3.071
CSP (Ozdayi et al., 2023) ✗ 0.239 26.46% 0.421 14.09% 0.858 2.359GPT-Neo
(125M)
Ours ✓ 0.421 122.75% 0.557 50.89% 0.665 1.945
No Prompt N/A 0.46 N/A 0.643 N/A 0.202 1.224
Constant Hard Prompt ✗ 0.392 -14.78% 0.581 -9.64% 0.24 1.271
Dynamic Hard Prompt ✓ 0.1 -78.26% 0.194 -69.83% 0.394 1.483
CSP (Ozdayi et al., 2023) ✗ 0.532 15.65% 0.698 8.55% 0.133 1.142GPT-Neo
(1.3B)
Ours ✓ 0.651 41.52% 0.772 20.04% 0.114 1.121
No Prompt N/A 0.54 N/A 0.702 N/A 0.127 1.135
Constant Hard Prompt ✗ 0.473 -12.41% 0.651 -7.26% 0.158 1.171
Dynamic Hard Prompt ✓ 0.117 -78.33% 0.213 -69.66% 0.291 1.338
CSP (Ozdayi et al., 2023) ✗ 0.641 18.70% 0.779 10.97% 0.084 1.087GPT-Neo
(2.7B)
Ours ✓ 0.702 30.00% 0.820 16.83% 0.075 1.077
Table 3: Main Results on Pythia Suite
Model MethodDynamic
Prompt?Exact
ERExact ER
GainFractional
ERFractional ER
GainTest LossTest Perplexity
(PPL)
No Prompt N/A 0.236 N/A 0.458 N/A 0.473 1.605
Constant Hard Prompt ✗ 0.161 -31.78% 0.361 -21.18% 0.595 1.812
Dynamic Hard Prompt ✓ 0.039 -83.47% 0.119 -74.02% 0.704 2.022
CSP (Ozdayi et al., 2023) ✗ 0.318 34.75% 0.526 14.90% 0.392 1.48Pythia
(410M)
Ours ✓ 0.513 117.37% 0.683 49.09% 0.283 1.328
No Prompt N/A 0.416 N/A 0.648 N/A 0.199 1.22
Constant Hard Prompt ✗ 0.293 -29.57% 0.526 -18.83% 0.288 1.333
Dynamic Hard Prompt ✓ 0.067 -83.89% 0.159 -75.46% 0.412 1.51
CSP (Ozdayi et al., 2023) ✗ 0.497 19.47% 0.714 10.16% 0.126 1.135Pythia
(1.4B)
Ours ✓ 0.617 48.32% 0.786 21.36% 0.109 1.115
No Prompt N/A 0.517 N/A 0.735 N/A 0.144 1.155
Constant Hard Prompt ✗ 0.401 -22.44% 0.611 -16.87% 0.214 1.239
Dynamic Hard Prompt ✓ 0.091 -82.40% 0.198 -73.06% 0.33 1.39
CSP (Ozdayi et al., 2023) ✗ 0.585 13.15% 0.783 6.57% 0.090 1.094Pythia
(2.8B)
Ours ✓ 0.669 29.40% 0.827 12.57% 0.080 1.084
No Prompt N/A 0.561 N/A 0.781 N/A 0.104 1.11
Constant Hard Prompt ✗ 0.446 -20.50% 0.674 -13.70% 0.165 1.179
Dynamic Hard Prompt ✓ 0.122 -78.25% 0.231 -70.42% 0.262 1.3
CSP (Ozdayi et al., 2023) ✗ 0.648 16.04% 0.831 6.67% 0.063 1.065Pythia
(6.9B)
Ours ✓ 0.702 25.13% 0.858 9.89% 0.062 1.064
Table 4: Main Results on StarCoderBase Suite
Model MethodDynamic
Prompt?Exact
ERExact ER
GainFractional
ERFractional ER
GainTest LossTest Perplexity
(PPL)
No Prompt N/A 0.062 N/A 0.232 N/A 0.836 2.306
Constant Hard Prompt ✗ 0.035 -43.55% 0.206 -11.21% 0.959 2.608
Dynamic Hard Prompt ✓ 0.006 -90.32% 0.066 -71.55% 0.958 2.605
CSP (Ozdayi et al., 2023) ✗ 0.071 14.52% 0.235 1.29% 0.815 2.259StarCoderBase
(1B)
Ours ✓ 0.082 32.26% 0.244 5.17% 0.806 2.238
No Prompt N/A 0.071 N/A 0.254 N/A 0.745 2.106
Constant Hard Prompt ✗ 0.043 -39.44% 0.232 -8.66% 0.834 2.302
Dynamic Hard Prompt ✓ 0.018 -74.65% 0.093 -63.39% 0.838 2.312
CSP (Ozdayi et al., 2023) ✗ 0.081 14.08% 0.249 -1.97% 0.734 2.084StarCoderBase
(3B)
Ours ✓ 0.094 32.39% 0.268 5.51% 0.713 2.039
No Prompt N/A 0.091 N/A 0.277 N/A 0.67 1.954
Constant Hard Prompt ✗ 0.021 -76.92% 0.243 -12.27% 0.765 2.149
Dynamic Hard Prompt ✓ 0.037 -59.34% 0.137 -50.54% 0.744 2.104
CSP (Ozdayi et al., 2023) ✗ 0.1 9.89% 0.278 0.36% 0.657 1.928StarCoderBase
(7B)
Ours ✓ 0.11 20.88% 0.289 4.33% 0.641 1.898
with the existing works (Carlini et al., 2023; Oz-
dayi et al., 2023; Nasr et al., 2023). However, it
does not mean that the small model does not have
security concerns on memorization. As shown in
Table 2 and Table 3, with our method, the mem-
orization of small language models with millionsof parameters is underestimated by a large margin,
where their memorization cannot be ignored in the
real applications./uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni00000033/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000031/uni00000048/uni00000052/uni00000003/uni0000000b/uni00000015/uni00000011/uni0000001a/uni00000025/uni0000000c
No Prompt
CSP
Ours
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni00000033/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000033/uni0000005c/uni00000057/uni0000004b/uni0000004c/uni00000044/uni00000003/uni0000000b/uni00000015/uni00000011/uni0000001b/uni00000025/uni0000000c
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni00000033/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000033/uni0000005c/uni00000057/uni0000004b/uni0000004c/uni00000044/uni00000003/uni0000000b/uni00000019/uni00000011/uni0000001c/uni00000025/uni0000000c
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni00000033/uni00000055/uni00000048/uni00000049/uni0000004c/uni0000005b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001b/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000015/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000036/uni00000057/uni00000044/uni00000055/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055/uni00000025/uni00000044/uni00000056/uni00000048/uni00000003/uni0000000b/uni0000001a/uni00000025/uni0000000c
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000031/uni00000048/uni00000052/uni00000003/uni0000000b/uni00000015/uni00000011/uni0000001a/uni00000025/uni0000000c
No Prompt
CSP
Ours
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000033/uni0000005c/uni00000057/uni0000004b/uni0000004c/uni00000044/uni00000003/uni0000000b/uni00000015/uni00000011/uni0000001b/uni00000025/uni0000000c
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni00000019/uni00000015/uni00000018/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000033/uni0000005c/uni00000057/uni0000004b/uni0000004c/uni00000044/uni00000003/uni0000000b/uni00000019/uni00000011/uni0000001c/uni00000025/uni0000000c
/uni00000018/uni00000014/uni00000018/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018
/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000013/uni00000011/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001c/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000014/uni00000018/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000036/uni00000057/uni00000044/uni00000055/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055/uni00000025/uni00000044/uni00000056/uni00000048/uni00000003/uni0000000b/uni0000001a/uni00000025/uni0000000cFigure 4: Ablation Study on Prefix Size and Length of Prompt with Exact Extraction Rate (ER)
Table 5: Ablation Study on the Dynamics of Prompt
Model MethodIs Dynamic
Prompt?Exact
ERFractional
ER
CSP No 0.641 0.779
Ours No 0.630 0.765GPT-Neo
(2.7B)Ours Yes 0.702 0.820
CSP No 0.585 0.783
Ours No 0.565 0.766Pythia
(2.8B)Ours Yes 0.669 0.827
CSP No 0.081 0.249
Ours No 0.081 0.247StarCoderBase
(3B)Ours Yes 0.094 0.268
4.3 Ablation Study
We explore our methods from several perspectives:
the impact of dynamic prompt, prefix size L, and
the length of prompt N.
Impact of Dynamic Prompt. To evaluate the im-
pact of dynamic prompt, we build another base-
line by replacing the input to the generator with
the first Ntokens in the vocabulary of the target
LLM. In this way, the soft prompts from the gen-
erator are constant and independent of the prefix
tokens. The results are shown in Table 5. It can
be observed that our method with dynamic prompt
outperforms the case with constant prompt con-
sistently and significantly over all the evaluated
settings. Moreover, the performance of our method
with constant prompts is close to that of directly
learning a constant soft prompt. Therefore, we can
conclude that the advantage of our method over
CSP comes from its adaptation to the dynamics of
input instead of incorporating a transformer-based
generator straightforwardly.Impact of Prefix Size. To evaluate the impact
prefix size, we set the length of prompt Nto 25
and vary the prefix size for GPT-Neo (2.7B), Pythia
(2.8B), Pythia (6.9B) and StarCoderBase (7B). The
results in terms of Exact ER are shown in the first
row of Figure 4. Our method can outperform the
two representative baselines consistently over all
the settings of prefix size across diverse LLMs and
datasets. Moreover, the amount of extracted data
increases along with the increase in the prefix size,
consistent with the existing works (Carlini et al.,
2023; Ozdayi et al., 2023).
Impact of Length of Prompt. To evaluate the im-
pact length of prompt N, we set the prefix size Lto
50 and vary the length of prompt Nfor GPT-Neo
(2.7B), Pythia (2.8B), Pythia (6.9B) and StarCoder-
Base (7B). The results in terms of Exact ER are
shown in the second row of Figure 4. According to
the results, we have several observations. Firstly,
our method can outperform the two representative
baselines consistently over all the settings of length
of prompt Nacross diverse LLMs and datasets.
Secondly, the performance of our method usually
increases rapidly when increasing the length of
prompt Nfrom a small value (e.g., 5) to a moder-
ate value (e.g., 25). Then the performance improve-
ment usually becomes smaller when the length of
prompt Nis increased further. And it tends to
saturate when the length of prompt Nreaches a
relatively large value (e.g., 50 or 75).5 Conclusion
We propose a novel method to unlock memoriza-
tion in large language models (LLMs) which was
underestimated by previous methods. More specifi-
cally, a transformer-based generator is developed
to customize the dynamic, prefix-dependent soft
prompts to measure the LLM memorization. It can
have a more precise detection of memorized data,
capturing the data omitted by the previous methods
only relying on the prefixes or the concatenation
of a constant soft prompt and prefixes. Extensive
experiments are conducted to show that our method
can outperform the state-of-the-art techniques by a
large margin under diverse settings, including text
generation and code generation tasks.
6 Limitations
There are several limitations of our work. First, we
primarily focus on the memorization of pretrained
LLM over the pretraining dataset and show that
our method can extract more training data. How-
ever, it has been shown that fine-tuned LLMs also
have memorization on fine-tuning dataset (Zeng
et al., 2023). Therefore, the effectiveness of our
method under the fine-tuning settings remains un-
explored, including fine-tuning on a single task
and multiple tasks. Second, we observed the sat-
uration phenomenon in the ablation study on the
length of prompt. The reason for the saturation re-
mains unknown. And further studies on saturation
might help extract more data with our method and
thus provide better measurement of memorization.
Third, based on the experimental results, we can
observe that the improvement of our method on the
fractional extraction rate is smaller and less robust
compared with the improvement in the exact extrac-
tion rate. One possible reason is that the aligned
CLM loss to train the generator is more suitable for
the optimization of verbatim memorization. Since
fractional extraction rate may be more important in
cases where the meaning of the extracted sequences
is more important than the exact match, it is valu-
able to improve the performance of our method on
the metric of fractional extraction rate.
7 Ethical Considerations
In this work, we propose to leverage dynamic soft
prompts to extract more training data from the tar-
get LLM and measure its memorization under the
white-box settings. Therefore, it is possible that theattackers might utilize our method to extract sensi-
tive data from the target LLM if they have white-
box access to the target LLM. However, the main
purpose of this work is to raise awareness among
LLM researchers and developers about the security
concerns caused by LLM memorization. By uti-
lizing our method to evaluate the memorization of
the target LLM, the owner of the LLM can evalu-
ate its security vulnerability more accurately and
thoroughly and then take action to defend against
it. For example, we mentioned in the paper that the
developer can utilize machine unlearning to forget
the sensitive training data that is identified by our
method. To minimize the security issues caused by
our work, all of our experiments are conducted on
public datasets that have been extensively studied
by the research community.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-
dan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. 2016. Deep learning with differential pri-
vacy. In Proceedings of the 2016 ACM SIGSAC con-
ference on computer and communications security ,
pages 308–318.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hess-
low, Julien Launay, Quentin Malartic, et al. 2023.
The falcon series of open language models. arXiv
preprint arXiv:2311.16867 .
Runxue Bao, Bin Gu, and Heng Huang. 2020. Fast
oscar and owl regression via safe screening rules. In
International conference on machine learning , pages
653–663. PMLR.
Runxue Bao, Bin Gu, and Heng Huang. 2022a. An
accelerated doubly stochastic gradient method with
faster explicit model identification. In Proceedings
of the 31st ACM International Conference on Infor-
mation & Knowledge Management , pages 57–66.
Runxue Bao, Xidong Wu, Wenhan Xian, and Heng
Huang. 2022b. Doubly sparse asynchronous learn-
ing. In The 31st International Joint Conference on
Artificial Intelligence (IJCAI 2022) .
Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir
Globerson, and Alexei Efros. 2022. Visual prompt-
ing via image inpainting. Advances in Neural Infor-
mation Processing Systems , 35:25005–25017.
Rachel Bawden and François Yvon. 2023. Investigating
the translation performance of a large multilingual
language model: the case of bloom. arXiv preprint
arXiv:2303.01911 .Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. 2023.
Pythia: A suite for analyzing large language mod-
els across training and scaling. In International
Conference on Machine Learning , pages 2397–2430.
PMLR.
BigCode. the-stack-smol dataset. https://huggingf
ace.co/datasets/bigcode/the-stack-smol .
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow.
Tom B Brown. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramer, and Chiyuan Zhang.
2023. Quantifying memorization across neural lan-
guage models. In The Eleventh International Confer-
ence on Learning Representations .
Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021. Extracting training data from
large language models. In 30th USENIX Security
Symposium (USENIX Security 21) , pages 2633–2650.
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. 2022. Adapt-
former: Adapting vision transformers for scalable
visual recognition. Advances in Neural Information
Processing Systems , 35:16664–16678.
Mack DeGeurin. 2023. Oops: Samsung employees
leaked confidential data to chatgpt. https://gizm
odo.com/chatgpt-ai-samsung-employees-lea
k-data-1850307376 . Accessed on: 2023-04-06.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Shangqian Gao, Ting Hua, Yen-Chang Hsu, Yilin Shen,
and Hongxia Jin. 2024. Adaptive rank selections
for low-rank approximation of language models. In
Proceedings of the 2024 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(Volume 1: Long Papers) , pages 227–241.
Google-Research. Training data extraction challenge.
https://github.com/google-research/lm-ext
raction-benchmark .
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Boyang Li, Dacheng Tao, and
Steven Hoi. 2023. From images to textual prompts:
Zero-shot visual question answering with frozenlarge language models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 10867–10877.
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng
Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-
ing Shi, and Xing Wang. 2024. Exploring human-
like translation strategy with large language models.
Transactions of the Association for Computational
Linguistics , 12:229–246.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Daniel Huynh. 2023. Starcoder memorization experi-
ment highlights privacy risks of fine-tuning on code.
https://huggingface.co/blog/dhuynh95/sta
rcoder-memorization-experiment . Accessed on:
2023-11-02.
Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan
Zhang, Matthew Jagielski, Katherine Lee, Christo-
pher A Choquette-Choo, and Nicholas Carlini. 2023.
Preventing generation of verbatim memorization in
language models gives a false sense of privacy. In
Proceedings of the 16th International Natural Lan-
guage Generation Conference , pages 28–53. Associ-
ation for Computational Linguistics.
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire
Cardie, Serge Belongie, Bharath Hariharan, and Ser-
Nam Lim. 2022. Visual prompt tuning. In Euro-
pean Conference on Computer Vision , pages 709–
727. Springer.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham
Neubig. 2021. How can we know when language
models know? on the calibration of language models
for question answering. Transactions of the Associa-
tion for Computational Linguistics , 9:962–977.
Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, and
Marco Pavone. 2024. Learning from teaching regu-
larization: Generalizable correlations should be easy
to imitate. arXiv preprint arXiv:2402.02769 .
Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.
Deduplicating training data mitigates privacy risks
in language models. In International Conference on
Machine Learning , pages 10697–10707. PMLR.
Antonia Karamolegkou, Jiaang Li, Li Zhou, and An-
ders Søgaard. 2023. Copyright violations and large
language models. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 7403–7412.
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jer-
nite, Margaret Mitchell, Sean Hughes, Thomas Wolf,
et al. 2022. The stack: 3 tb of permissively licensed
source code. arXiv preprint arXiv:2211.15533 .Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. 2021. Deduplicating training
data makes language models better. arXiv preprint
arXiv:2107.06499 .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059.
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,
and Ji-Rong Wen. 2024. Pre-trained language mod-
els for text generation: A survey. ACM Computing
Surveys , 56(9):1–39.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.
2023. Starcoder: may the source be with you! arXiv
preprint arXiv:2305.06161 .
Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. 2022. Scaling & shifting your features: A
new baseline for efficient model tuning. Advances in
Neural Information Processing Systems , 35:109–123.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-
fel. 2022. Few-shot parameter-efficient fine-tuning
is better and cheaper than in-context learning. Ad-
vances in Neural Information Processing Systems ,
35:1950–1965.
Antoine Louis, Gijs van Dijck, and Gerasimos Spanakis.
2024. Interpretable long-form legal question answer-
ing with retrieval-augmented large language models.
InProceedings of the AAAI Conference on Artificial
Intelligence , volume 38, pages 22266–22275.
Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan
Wang, Wei Wu, Xiaojun Quan, and Dawei Song.
2022. Xprompt: Exploring the extreme of prompt
tuning. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 11033–11047.
Milad Nasr, Nicholas Carlini, Jonathan Hayase,
Matthew Jagielski, A Feder Cooper, Daphne Ippolito,
Christopher A Choquette-Choo, Eric Wallace, Flo-
rian Tramèr, and Katherine Lee. 2023. Scalable ex-
traction of training data from (production) language
models. arXiv preprint arXiv:2311.17035 .
Mustafa Ozdayi, Charith Peris, Jack FitzGerald,
Christophe Dupuy, Jimit Majmudar, Haidar Khan,
Rahil Parikh, and Rahul Gupta. 2023. Controlling
the extraction of memorized data from large language
models via prompt-tuning. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 1512–
1521.
Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Ren-
rui Zhang, Yi Wang, Yu Qiao, and Hongsheng Li.2023. Retrieving-to-answer: Zero-shot video ques-
tion answering with frozen large language models.
InProceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 272–283.
Martin Pawelczyk, Seth Neel, and Himabindu
Lakkaraju. 2023. In-context unlearning: Language
models as few shot unlearners. arXiv preprint
arXiv:2310.07579 .
Alec Radford, Jeffrey Wu, Dario Amodei, Daniela
Amodei, Jack Clark, Miles Brundage, and Ilya
Sutskever. 2019. Better language models and their
implications. OpenAI blog , 1(2).
Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023.
Prompting large language models with answer heuris-
tics for knowledge-based visual question answering.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 14974–
14983.
Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-
shick. 2016. Training region-based object detectors
with online hard example mining. In Proceedings of
the IEEE conference on computer vision and pattern
recognition , pages 761–769.
Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,
Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan
Liu, Peng Li, Juanzi Li, et al. 2022. On transferabil-
ity of prompt tuning for natural language processing.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 3949–3969.
Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shang-
wei Guo, Tianwei Zhang, and Guoyin Wang. 2023.
Text classification via large language models. arXiv
preprint arXiv:2305.08377 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Dave Van Veen, Cara Van Uden, Louis Blanke-
meier, Jean-Benoit Delbrouck, Asad Aali, Christian
Bluethgen, Anuj Pareek, Malgorzata Polacin, Ed-
uardo Pontes Reis, Anna Seehofnerová, et al. 2024.
Adapted large language models can outperform med-
ical experts in clinical text summarization. Nature
Medicine , pages 1–9.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and
Daniel Cer. 2022. Spot: Better frozen model adap-
tation through soft prompt transfer. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 5039–5059.
Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu,
Yanchi Liu, Wei Cheng, and Haifeng Chen. 2024.Infuserki: Enhancing large language models with
knowledge graphs via infuser-guided knowledge in-
tegration. arXiv preprint arXiv:2402.11441 .
Yue Wang, Ziyu Jiang, Xiaohan Chen, Pengfei Xu, Yang
Zhao, Yingyan Lin, and Zhangyang Wang. 2019. E2-
train: Training state-of-the-art cnns with over 80%
energy savings. Advances in Neural Information
Processing Systems , 32.
Zhiqiang Wang, Yiran Pang, and Yanbin Lin. 2023.
Large language models are zero-shot text classifiers.
arXiv preprint arXiv:2312.01044 .
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,
Jennifer Dy, and Tomas Pfister. 2022. Learning to
prompt for continual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 139–149.
Colin Wei, Sang Michael Xie, and Tengyu Ma. 2021.
Why do pretrained language models help in down-
stream tasks? an analysis of head and prompt tuning.
Advances in Neural Information Processing Systems ,
34:16158–16170.
Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao
Wang, Ye Feng, Ping Luo, and Ying Shan. 2024.
Llama pro: Progressive llama with block expansion.
arXiv preprint arXiv:2401.02415 .
Yawen Wu, Zhepeng Wang, Yiyu Shi, and Jingtong
Hu. 2020. Enabling on-device cnn training by self-
supervised instance filtering and error map pruning.
IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems , 39(11):3445–3457.
Yawen Wu, Zhepeng Wang, Dewen Zeng, Yiyu Shi,
and Jingtong Hu. 2021. Enabling on-device self-
supervised contrastive learning with selective data
contrast. In 2021 58th ACM/IEEE Design Automa-
tion Conference (DAC) , pages 655–660. IEEE.
Haoran Xu, Young Jin Kim, Amr Sharaf, and
Hany Hassan Awadalla. 2023. A paradigm shift
in machine translation: Boosting translation perfor-
mance of large language models. arXiv preprint
arXiv:2309.11674 .
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2020. mt5: A massively multilingual
pre-trained text-to-text transformer. arXiv preprint
arXiv:2010.11934 .
Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao
Wang, Zezhou Cheng, and Xiang Yue. 2024. Ma-
chine unlearning of pre-trained large language mod-
els.arXiv preprint arXiv:2402.15159 .
Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023.
Large language model unlearning. arXiv preprint
arXiv:2310.10683 .Michihiro Yasunaga, Hongyu Ren, Antoine Bosse-
lut, Percy Liang, and Jure Leskovec. 2021. Qa-
gnn: Reasoning with language models and knowl-
edge graphs for question answering. arXiv preprint
arXiv:2104.06378 .
Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han
Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang
Tang, and Dawei Yin. 2023. Exploring memoriza-
tion in fine-tuned language models. arXiv preprint
arXiv:2310.06714 .
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023.
Prompting large language model for machine transla-
tion: A case study. In International Conference on
Machine Learning , pages 41092–41110. PMLR.
Haoyu Zhang, Jianjun Xu, and Ji Wang. 2019.
Pretraining-based natural language genera-
tion for text summarization. arXiv preprint
arXiv:1902.09243 .
Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng,
Runxue Bao, Rui Zhang, Prasenjit Mitra, and
Haifeng Chen. 2024a. Pruning as a domain-specific
llm extractor. arXiv preprint arXiv:2405.06275 .
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B Hashimoto.
2024b. Benchmarking large language models for
news summarization. Transactions of the Associa-
tion for Computational Linguistics , 12:39–57.