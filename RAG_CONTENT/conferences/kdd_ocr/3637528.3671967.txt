Revisiting Modularity Maximization for Graph Clustering: A
Contrastive Learning Perspective
Yunfei Liu∗
Ant Group
Hangzhou, China
leo.lyf@antgroup.comJintang Li∗†
Ant Group
Guangzhou, China
edisonleejt@gmail.comYuehe Chen
Ant Group
Shanghai, China
chenyuehe.cyh@antgroup.com
Ruofan Wu
Ant Group
Shanghai, China
wuruofan1989@gmail.comEricbk Wang
Ant Group
Shanghai, China
yike.wbk@antgroup.comJing Zhou
Ant Group
Hangzhou, China
colin.zj@antgroup.com
Sheng Tian
Ant Group
Hangzhou, China
tiansheng.ts@antgroup.comShuheng Shen
Ant Group
Hangzhou, China
shuheng.ssh@antgroup.comXing Fu
Ant Group
Hangzhou, China
fux008@gmail.com
Changhua Meng
Ant Group
Hangzhou, China
changhua.mch@antgroup.comWeiqiang Wang
Ant Group
Hangzhou, China
wang.weiqiang@gmail.comLiang Chen
Unaffiliated
Guangzhou, China
jasonclx@gmail.com
ABSTRACT
Graph clustering, a fundamental and challenging task in graph min-
ing, aims to classify nodes in a graph into several disjoint clusters.
In recent years, graph contrastive learning (GCL) has emerged as
a dominant line of research in graph clustering and advances the
new state-of-the-art. However, GCL-based methods heavily rely on
graph augmentations and contrastive schemes, which may poten-
tially introduce challenges such as semantic drift and scalability
issues. Another promising line of research involves the adoption
of modularity maximization, a popular and effective measure for
community detection, as the guiding principle for clustering tasks.
Despite the recent progress, the underlying mechanism of modu-
larity maximization is still not well understood. In this work, we
dig into the hidden success of modularity maximization for graph
clustering. Our analysis reveals the strong connections between
modularity maximization and graph contrastive learning, where
positive and negative examples are naturally defined by modular-
ity. In light of our results, we propose a community-aware graph
clustering framework, coined Magi, which leverages modularity
maximization as a contrastive pretext task to effectively uncover the
underlying information of communities in graphs, while avoiding
the problem of semantic drift. Extensive experiments on multiple
∗Both authors contributed equally to this research.
†Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671967graph datasets verify the effectiveness of Magi in terms of scalabil-
ity and clustering performance compared to state-of-the-art graph
clustering methods. Notably, Magi easily scales a sufficiently large
graph with 100M nodes while outperforming strong baselines.
CCS CONCEPTS
•Information systems →Clustering ;•Computing methodolo-
gies→Unsupervised learning ;Learning latent representations .
KEYWORDS
Graph clustering, graph contrastive learning, modularity maximiza-
tion
ACM Reference Format:
Yunfei Liu, Jintang Li, Yuehe Chen, Ruofan Wu, Ericbk Wang, Jing Zhou,
Sheng Tian, Shuheng Shen, Xing Fu, Changhua Meng, Weiqiang Wang,
and Liang Chen. 2024. Revisiting Modularity Maximization for Graph Clus-
tering: A Contrastive Learning Perspective. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671967
1 INTRODUCTION
Graph clustering is a fundamental problem in graph analysis, crucial
for uncovering structures and relationships between nodes in a
graph. The primary objective of graph clustering is to group or
partition the nodes in a graph into clusters or communities based
on their structural properties or connectivity patterns. So far, graph
clustering has been widely studied and extensively applied across
various domains, including social network analysis [ 28], image
segmentation [10] and recommendation systems [26].
 
1968
KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
Positives and Negatives provided by modularity matrix
Query nodePositiveNegativeCommunity 1Community 2
𝑨𝒖,𝒗−𝒅𝒖𝒅𝒗𝟐𝒎>𝟎uv𝑨𝒖,𝒗−𝒅𝒖𝒅𝒗𝟐𝒎≤𝟎uvGraph structure
Community 1Community 2Original graph structure
Figure 1: An illustrative overview of how positive and nega-
tive examples of a query node are guided by the “modularity”
measure.
As a longstanding field of research, graph clustering continues to
evolve through the development of novel algorithms and techniques.
Recently, graph neural networks (GNNs) have emerged as the de
facto deep learning architecture for dealing with graph data [ 19,43].
In light of the learning capability of GNNs on graph-structured data,
researchers have shifted their attention to exploring GNN-based ap-
proaches for graph clustering. Typically, GNNs are employed as en-
coders to learn node representations, which are often accomplished
with an auxiliary task to help uncover the underlying patterns for
clustering. As the graph clustering task is commonly approached in
an unsupervised manner due to the absence of labeled annotations,
this has motivated the exploration of self-supervised graph learning
methods for graph clustering. Contrastive learning [ 13,46], which
aims to learn representations that bring similar instances closer in
the representation space and distances dissimilar ones, has been
proven to learn “cluster-preserving” representations [ 31]. Therefore,
recently, graph contrastive learning methods (GCLs) [ 22,38,44,49]
have made significant breakthroughs in graph clustering and grad-
ually become the mainstream approach for graph clustering.
Inspired by [46], early GCLs [44, 51] adopted instance discrimi-
nation as a pretext task, aiming to learn representations that are
invariant to different augmented views of the graph. Recently, some
works have proposed augmentation-free GCLs [ 9,20,24,48], which
extract contrastive information from the graph data itself and con-
struct corresponding pseudo-labels. Several works [ 20,24,48] use
K-means [ 25] and K-Nearest Neighbors (KNN) to mine the potential
cluster within graph features, and utilize the clustering results as
foundation for positive/negative sample pairs. Although the GCLs
mentioned above have achieved significant success in the graph
clustering task, we note that these methods still suffer from at least
one of the following challenges:
(C1) Scalability. Mainstream GCLs typically rely on data aug-
mentation to create multiple views and employ multiple encoders to
obtain corresponding representations. However, this poses compu-
tational challenges and scalability issues when applied to large-scale
graphs. On the other hand, feature-based augmentation-free GCLs
require the use of high-cost algorithms such as K-means and KNN
to construct pseudo-labels, which also limits their capability to
scale to large-scale datasets.
(C2) Semantic drift. Pretext tasks play an important role in
enabling GCL to better adapt to downstream tasks. A typical ap-
proach is to define instance-wise “augmentations” and then pose
the problem as that of learning to closely align these augmenta-
tions with the original, while keeping them separate from others.However, this type of pretext task neglects the inherent structure of
graph data, which can result in semantic drift during downstream
clustering tasks [20].
Recently, methods based on neural modularity [ 27] maximiza-
tion have made new progress in graph clustering tasks [ 2,41]. By
using a single GNN encoder to encode the relaxed community as-
signment matrix, these methods effectively combine the modularity
maximization objective with GNNs and achieve state-of-the-art per-
formance in graph clustering tasks. The modularity maximization
objective can effectively perceive the potential community struc-
ture within networks [ 23] and provide guidance for representation
learning. However, the design of the modularity function is heuris-
tic, and the underlying reasons for its success as an optimization
objective remain largely unexplored.
In this work, we provide an in-depth analysis of modularity
maximization and bridge the gap between modularity maximiza-
tion and GCL. Our analysis shows that modularity maximization
is essentially graph contrastive learning, where the positive and
negative examples are naturally guided by the modularity matrix
(see Figure 1). Based on our findings, we attempt to integrate the
latest developments in the fields of neural modularity maximization
and graph contrastive learning, and propose a co mmunity- aware
graph cluster ing framework, coined Magi. Magi can mitigate the
effects of semantic drift by perceiving the underlying community
structures in the graph, and since it doesn’t rely on data augmen-
tation, it can easily scale to a sufficiently large graph with 100M
nodes. Our contributions can be summarized as follows:
•Modularity maximization =contrastive learning. We
establish the connection between modularity maximization
and graph contrastive learning. Our findings reveal that mod-
ularity maximization can be viewed as leveraging potential
community information in graphs for contrastive learning.
•Community-aware pretext task and scalable frame-
work. We propose Magi, a community-aware graph con-
trastive learning framework that uses modularity maximiza-
tionas its pretext task. Magi avoids semantic drift by lever-
aging underlying community structures and eliminates the
need for graph augmentation. Magi incorporates a two-stage
random walk approach to perform modularity maximization
pretext tasks in mini-batch form, thereby achieving good
scalability.
•Experimental results. We conduct extensive experiments
on 8 real-world graph datasets with different scales. Magi
has consistently outperformed several state-of-the-arts in
the task of graph clustering. Notably, Magi easily scales to
an industrial-scale graph with 100M nodes, showcasing its
scalability and effectiveness in large-scale scenarios.
2 RELATED WORK
2.1 Graph clustering
Graph clustering is a widely studied problem in academia and indus-
try. Classical clustering methods involve either solving an optimiza-
tion problem or using some heuristic, non-parametric approaches.
Prominent examples include K-means [ 25], spectral clustering [ 35],
and Louvain [ 3]. However, the shallow architecture of these meth-
ods limits their performance. With the rise of deep neural networks
 
1969Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective KDD ’24, August 25–29, 2024, Barcelona, Spain
in graph representation learning, random walk-based methods such
as DeepWalk [ 32] and Node2vec [ 12] have also been introduced for
addressing clustering tasks.
Early works typically focus on single dimension form graphs,
e.g., graph structure or node attributes. Thanks to the great abil-
ity of graph neural networks (GNNs) [ 19,43] in learning jointly
structure and attribute information, graph autoencoders stand out
as an emerging approach for unsupervised graph learning tasks.
For example, GAE and VGAE [ 18] learn to reconstruct the graph
structure as the self-supervised learning task using GNNs. Follow-
up works extend GAE by employing Laplacian sharpening [ 30],
Laplacian smoothing [ 8], generative adversarial learning [ 29], and
masked autoencoding [ 21]. However, self-supervised learning tasks
or pretext tasks in GAEs are not aligned with downstream tasks
such as graph clustering. As a result, the learned representations
may not effectively capture the relevant information for clustering
tasks.
2.2 Graph contrastive learning
Over the past few years, graph contrastive learning (GCL) has
emerged as a powerful technique for learning representations of
graph-structured data. Deep Graph Infomax (DGI) [ 44] follows the
approach of mutual information-based learning, as proposed by
adapted [ 16]. GRACE [ 51] maximizes the agreement of node repre-
sentations between two corrupted views of a graph. GraphCL [ 49]
incorporates four graph augmentation techniques to learn unsuper-
vised graph representations through a unified contrastive learning
framework. One step further, MVGRL [ 15] introduces node diffusion
and contrasts node-level embeddings with representations of aug-
mented graphs. Following the BYOL [ 11], BGRL [ 39] eliminates the
need for negative sampling by minimizing an invariance-based loss
for augmented graphs within a batch. In light of the success of GCL,
there have been attempts to apply GCL techniques to graph clus-
tering and achieve promising results [ 9,20,24,48]. However, most
methods employ classical instance discrimination as the pretext
task, which requires sophisticated graph augmentation techniques
to obtain meaningful view representations. As a result, they may
suffer from challenges such as semantic drift and limitations in
scaling up to handle large graphs.
2.3 Neural modularity maximization
As one of the most commonly used metrics for community detec-
tion, modularity measures the quality of a partition in a network
by evaluating the density of connections within communities com-
pared to random connections [ 27]. Maximizing the modularity
directly is proven to be NP-hard [ 4]. As a result, heuristics such
as spectral relaxation [ 28] and greedy algorithms [ 3] have been
developed to approximate the modularity and find suboptimal but
reasonable community partitions. However, previous works focus
simply on the graph structure, while ignoring the abundant infor-
mation associated with the nodes (e.g., attributes). With the advent
of graph neural networks, combining them with modularity maxi-
mization has become a promising research direction. [ 1,47] utilizes
an autoencoder to encode the community assignment matrix and
combines it with a reconstruction loss to train the autoencoder.
[23,36] and [ 7,33,50] extend this idea by employing (variational)graph autoencoders. [ 23] explored the benefits brought by high-
order proximity and used high-order polynomials of the adjacency
matrix to calculate the high-order modularity matrix. [ 37] con-
structs two loss functions based on modularity, corresponding to
single and multi-attribute networks, while DMoN [ 41] introduces
collapse regularization to prevent the community allocation matrix
from falling into spurious local minima. Despite the recent progress,
the underlying mechanism of neural modularity maximization for
graph clustering is still not well understood.
3 MOTIVATION
In this section, we first provide basic notations throughout this
paper. We then introduce preliminary knowledge for modularity-
based learning, which is connected to contrastive learning as our
motivation.
3.1 Problem statement and notations
Given an attribute graph G=(V,E,X), whereV={𝑣1,𝑣2,...,𝑣𝑁}
denotes a set of nodes and 𝑁=|V|;E⊆V×V denotes correspond-
ing edges between nodes, where each node 𝑣∈V is associated
with a𝑑ℎ-dimensional feature vector 𝑥𝑣∈R𝑑ℎ. LetX∈R𝑁×𝑑ℎ
denote the node attribute matrix and A∈R𝑁×𝑁the adjacency
matrix, respectively. Given the graph Gand node attributes X, the
goal of graph clustering is to partition the nodes set Vinto𝐶parti-
tions{V1,V2,V3,...,V𝐶}such that nodes in the same cluster are
similar/close to each other in terms of the graph structure as well
as in terms of attributes.
3.2 Revisiting modularity maximization
Modularity [ 27] is a measure of the graph structure, which mea-
sures the divergence between the intra-community edges from the
expected one. Formally, given a graph G, the modularity ( 𝑄) is
defined as:
𝑄=1
2𝑚∑︁
𝑖,𝑗(A𝑖,𝑗−𝑑𝑖𝑑𝑗
2𝑚)𝛿(𝑐𝑖,𝑐𝑗), (1)
where𝑑𝑖=Í
𝑗A𝑖,𝑗is the degree of node 𝑣𝑖,𝑚=|E|is the total num-
ber of edges in the graph, and 𝑐𝑖is the community to which node 𝑣𝑖
is assigned.𝛿(𝑐𝑖,𝑐𝑗)=1is an indicator function, i.e., 𝛿(𝑐𝑖,𝑐𝑗)=1if
𝑐𝑖=𝑐𝑗and 0 otherwise, suggesting whether nodes 𝑣𝑖and𝑣𝑗belong
to the same community. Essentially, A𝑖,𝑗and𝑑𝑖𝑑𝑗
2𝑚can be regard
as the “observed” and “expected” number of edges between nodes
𝑣𝑖and𝑣𝑗. Since a larger ( 𝑄) leads to a better community partition,
modularity maximization has become a popular approach in finding
good community division [28].
Typically, modularity maximization can be viewed as a con-
strained optimization objective in the following form:
max
P𝑄:=𝑡𝑟(P𝑇BP),
𝑠.𝑡. P∈{0,1}𝑁×𝐶,
𝑡𝑟(P𝑇P)=𝑁,(2)
where B∈R𝑁×𝑁is the modularity matrix, with each element
B𝑖,𝑗=A𝑖,𝑗−𝑑𝑖𝑑𝑗
2𝑚themodularity coefficient. P={𝑝1,𝑝2,...,𝑝𝑁}∈
 
1970KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
{0,1}𝑁×𝐶is a one-hot matrix indicating the community owner-
ship of each node in the graph, i.e., community assignment matrix.
Typically, P𝑖,𝑗=1if𝑣𝑖belongs to community V𝑗and 0 otherwise.
3.3 Connecting modularity maximization to
contrastive learning
Based on the preliminary knowledge introduced above, we now
provide our insights to demonstrate the underlying relationship be-
tween modularity maximization and contrastive learning. Formally,
we rewrite Eq. (1) into an equivalent form below:
max
P𝑄:=𝑡𝑟(P𝑇BP)=1
2𝑚∑︁
𝑖,𝑗B𝑖,𝑗𝑝𝑇
𝑖𝑝𝑗
=1
2𝑚©­
«∑︁
(𝑣𝑖,𝑣𝑗)∈M+B𝑖,𝑗𝑝𝑇
𝑖𝑝𝑗+∑︁
(𝑣𝑖,𝑣𝑗)∈M−B𝑖,𝑗𝑝𝑇
𝑖𝑝𝑗ª®
¬
𝑠.𝑡. 𝑝𝑖∈{0,1}𝐶and𝑝𝑇
𝑖𝑝𝑖=1∀𝑖=1,2,...,𝑁,(3)
whereM+={(𝑣𝑖,𝑣𝑗)|B𝑖,𝑗>0}andM−={(𝑣𝑖,𝑣𝑗)|B𝑖,𝑗≤0}are
positive and negative pairs, respectively. In this regard, modular-
ity maximization is similar in form to graph autoencoders, with
the goal of optimization is to reconstruct the modularity matrix B
rather than adjacency matrix A. Motivated by a recent work [ 21]
that showed the equivalence between graph autoencoders and con-
trastive learning, we provide our intuition to explicitly relate mod-
ularity maximization to contrastive learning.
Here we further unify Eq. (3)into a general form from the per-
spective of optimization. Since directly maximizing Eq. (3)is in-
tractable, we can now relax P∈{0,1}𝑁×𝐶to its continuous analog
Z∈R𝑁×𝐶, where Zis node representations learned from a graph
encoder𝑓such that Z=𝑓(G).
max
𝑓,𝑔𝑄=1
2𝑚©­
«∑︁
(𝑣𝑖,𝑣𝑗)∈M+B𝑖,𝑗𝑧𝑇
𝑖𝑧𝑗+∑︁
(𝑣𝑖,𝑣𝑗)∈M−B𝑖,𝑗𝑧𝑇
𝑖𝑧𝑗ª®
¬
=1
2𝑚©­
«∑︁
(𝑣𝑖,𝑣𝑗)∈M+|B𝑖,𝑗|𝑧𝑇
𝑖𝑧𝑗−∑︁
(𝑣𝑖,𝑣𝑗)∈M−|B𝑖,𝑗|𝑧𝑇
𝑖𝑧𝑗ª®
¬
=1
2𝑚©­
«∑︁
(𝑣𝑖,𝑣𝑗)∈M+𝑔(𝑧𝑖,𝑧𝑗) −∑︁
(𝑣𝑖,𝑣𝑗)∈M−𝑔(𝑧𝑖,𝑧𝑗)ª®
¬,(4)
where𝑔(·,·)is the decoder network that decodes the pairwise node
representations(𝑧𝑖,𝑧𝑗)into the modularity score. Typically, the
decoder𝑔can be simply defined by cosine similarity function, i.e.,
𝑔(𝑧𝑖,𝑧𝑗)=𝑧𝑇
𝑖𝑧𝑗or a neural network. Community coefficient B𝑖,𝑗is
absorbed into 𝑔and𝑓and should be adjusted by the neural network
during training. According to Eq. (4), we obtain a general form of
graph contrastive learning, where positive and negative pairs are
guided by the modularity coefficient B𝑖,𝑗.
3.4 Opportunity: community-aware pretext task
As discussed in Section 1, current GCL methods for graph cluster-
ing, whether augmentation-based or augmentation-free, may face
challenges related to scalability and/or semantic drift. Therefore,
an alternative yet promising way is to utilize modularity maximiza-
tion as a pretext task to generate community-aware pseudo-labels
that guide contrastive learning. Specifically, the employment of themodularity matrix allows us to reveal the underlying community
structure within the graph. In this regard, positive sample pairs are
naturally defined as connected node pairs within the same com-
munity, while negative sample pairs are unconnected node pairs
from different communities. This leverages the intrinsic community
structure in the network to mitigate the impact of semantic drift
while also eliminating the need for graph augmentations that may
hinder its scalability. Moreover, by establishing the relationship
between modularity maximization and contrastive learning, we are
able to integrate the latest advancements in modularity maximiza-
tion into research on graph contrastive learning. Recent advances
in the field of modularity maximization mainly include: (1) neu-
ral modularity maximization [ 2,41]; (2) high-order proximity in
modularity [ 23]; (3) Better parallel optimization algorithms [ 40],
etc. The ones that are closely related to contrastive learning are (1)
and (2). In this work, we propose to obtain cluster assignment via
GNN encoder [ 14,19], which allows (soft) cluster assignment to be
differentiable for neural modularity maximization. In addition, we
fully consider high-order proximity in modularity, and propose a
sampling method based on random walk to ensure scalability while
capturing high-order proximity within the community.
4 METHOD
In this section, we present an overview of the architecture of Magi,
which comprises a graph convolutional encoder, a two-stage ran-
dom walk sampler that serves the modularity maximization pretext
task, and a SimCLR [ 6] contrastive loss. Each of these components
will be introduced separately in the following subsections.
4.1 GNN encoder
An encoder in GCL is a crucial component that maps the input
graph data into latent representations. In Magi, we employ differ-
ent GNNs as encoders to encode graphs of varying sizes. Here we
consider only two representative GNNs, i.e., GCN [ 19] and Graph-
SAGE [ 14]. It is important to note that Magi is not specific to any
particular GNN model and can be applied with other architectures
(e.g., GAT [43]) as well.
GCN. GCN is a popular GNN model that has been widely used for
various graph-based tasks. GCN operates by aggregating informa-
tion from neighboring nodes and propagating it through multiple
layers. It leverages the spectral graph convolution operation, which
is based on the graph Laplacian matrix, to capture local and global
graph structure. Formally, the message propagation of 𝑙-th layer in
GCN is defined as:
ℎ(𝑙)
𝑢=LeakyReLU©­
«∑︁
𝑣∈N𝑢∪{𝑢}W(𝑙)ℎ(𝑙−1)
𝑣√︁
(|N𝑢|+1)(|N𝑣|+1)ª®
¬,(5)
where LeakyReLU(·)=𝑚𝑎𝑥(0,·)+𝛼∗𝑚𝑖𝑛(0,·)is activation func-
tion.N𝑢is the set of adjacent neighbors of node 𝑢.Wis the learnable
weight of layer 𝑙andℎ(𝑙)is the latent representation with ℎ(0)=𝑥.
GraphSAGE. GCN relies on a symmetric and fixed aggregation
function based on the graph Laplacian, which can limit its ability to
capture diverse neighborhood structures and scale to large graphs
effectively. GraphSAGE is a GNN alternative designed to better
 
1971Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective KDD ’24, August 25–29, 2024, Barcelona, Spain
GNNEncoder
𝒢=(𝒱,ℰ,𝑋)(b) Modularity matrix generationGraph contrastive learning𝓛𝑺𝒊𝒎𝑪𝑳𝑹Positive sampleNegative sample(a) Overall framework
visit times 𝑻mean{Τ!,#Τ$,%Τ!,&Τ$,'Τ$,(}=𝑂!𝓑=#𝒖𝑶𝒖
Random Walk IRandom Walk II
Random Walk II Random Walk I 
𝐒𝒖,𝒗=𝑻𝒖,𝒗∑𝒗!∈𝓑𝑻𝒖,𝒗!
visit times 𝑻𝚩𝒖,𝒗=𝐒𝒖,𝒗−𝟏|𝓑|/+−+−+/−−−+−/++−−+/−−−++/𝚩𝚩EncodeNormalizeGenerating mini-batch modularity matrix 𝑴/+−+−+/−−−+−/++−−+/−−−++/
Figure 2: Overview of proposed Magi framework. During the self-supervised learning phase, the original graph Gis provided
to generate node embedding and modularity matrix through a GNN encoder and a two-stage random walk process, respectively.
Then, the SimCLR loss function is employed to optimize the encoder in a self-supervised manner.
handle large-scale graphs in an inductive manner:
ℎ(𝑙)
𝑢=LeakyReLU
W(𝑙)
1ℎ(𝑙−1)
𝑢+W(𝑙)
2Θ({ℎ(𝑙−1)
𝑣|𝑣∈N𝑢})
,(6)
where Θdenotes different aggregation functions, such as mean, max,
or LSTM-based aggregation. This allows GraphSAGE to capture
diverse types of neighborhood information, accommodating various
graph structures and improving its capability to learn from different
node interactions.
4.2 Modularity maximization pretext task
The core insight behind neural modularity maximization is the
computation of the modularity matrix B. In order to adapt the
minibatch training to obtain good scalability, an intuitive idea is
to randomly sample 𝑛nodes in the graph and obtain contrastive
pairs from their corresponding sub-modularity matrix. However, it
is important to note that this approach potentially introduces the
following issues:
•Structure bias. The sampled subgraph may not necessarily
share a similar structure with the overall graph, which can
hinder the performance of contrastive learning. Nevertheless,
this issue becomes more serious as the graph becomes larger
and sparser.
•Lack of high-order proximity. The vanilla modularity
matrix only focuses on the first-order proximity within the
graph, which contradicts our intuitive understanding of com-
munity structure in networks, that is, two nodes often be-
long to the same community due to high-order proximity,
such as having many common neighbors. Although a re-
cent work [ 23] explored the benefits brought by high-order
proximity and used high-order polynomials of the adjacency
matrix Ato calculate the high-order modularity matrix. How-
ever, the cost of computing high-order polynomials of the
adjacency matrix Afor large-scale graphs is still prohibi-
tive. We discuss the benefits of incorporating high-order
proximity through experimental investigation in Section 6.2.In this work, we propose a two-stage random walk-based sam-
pling method to address the aforementioned challenges, as by ad-
justing the depth of the random walks, we can effectively capture
the high-order proximity within the network. Specifically, we em-
ploy a first-stage random walk to sample multiple sub-communities
and merge them into a training batch, ensuring that the corre-
sponding subgraph within the batch contains effective community
structures to guide the model. Subsequently, we perform a second-
stage random walk to generate a similarity matrix between nodes
within the batch, and we draw on the concept of the configuration
model [ 5] to calculate the corresponding modularity matrix in the
mini-batch form.
Next, we will provide a detailed description of these two stages,
which we refer to as S1andS2, respectively.
(S1) Sampling multiple sub-communities. For any𝑣in set
Nconsisting of 𝑛randomly selected root nodes in the graph, based
on the principle of internal connectivity within communities, there
must be an overlap between the neighborhood of node 𝑣and its
potential community, denoted as O𝑣. Accurately selecting O𝑣can
be an expensive task because it involves detecting potential com-
munity structures in the graph. However, based on the definition
that a community is a set of nodes is densely connected internally, we
utilize random walks to approximate O𝑣. To be specific, we initiate
𝑡random walks of depth 𝑙rooted at node 𝑣and denote the set of
visited nodes asU1𝑣, with the corresponding visit count recorded
as𝑇1𝑣, which each element 𝑡1𝑣,𝑢as the number of visits from 𝑣to𝑢.
We filterO𝑣in the following way:
O𝑣={𝑢∈U1
𝑣|𝑡1
𝑣,𝑢>Í
𝑢′𝑡1
𝑣,𝑢′
|U1𝑣|} (7)
In brief, we consider nodes with visits greater than the mean as
O𝑣. Then, for nodes in N, we perform the same operation and set
B=∪𝑣∈NO𝑣as a training batch.
(S2) Mini-batch modularity matrix. It is intuitive to treat
pair nodes from the same sub-community as positives and those
from different sub-communities as negatives. However, we must
consider the potential overlaps between different sub-community,
necessitating further exploration of the relationships among nodes
 
1972KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
inB. Specifically, we perform again 𝑡random walks of depth 𝑙for
each node inB. For any two nodes 𝑣and𝑢inB, let𝑡2𝑣,𝑢be the
number of visits from 𝑣to𝑢, we can then construct a similarity
matrix Sfor the nodes inBbased on the number of visits, which
each element:
S𝑣,𝑢=𝑡2𝑣,𝑢Í
𝑢′∈B𝑡2
𝑣,𝑢′(8)
denotes probability that node 𝑣is connected to node 𝑢. Subse-
quently, following [ 27] we employ the configuration model [ 5] to
estimate the expected probability of connection between any two
nodes inB. Ultimately, we get the mini-batch form of modularity
matrix B, where each element:
B𝑣,𝑢=S𝑣,𝑢−Í
𝑢′∈BS𝑣,𝑢′Í
𝑣′∈BS𝑣′,𝑢Í
𝑣′,𝑢′S𝑣′,𝑢′=S𝑣,𝑢−1
|B|(9)
4.3 Contrastive loss formulation
Given a train batch Band corresponding modularity matrix B,
for any nodes 𝑣∈B, setM+𝑣={𝑢∈B|B𝑣,𝑢>0}andM−𝑣=
{𝑢∈B|B𝑣,𝑢<=0}are the positive and negative samples of 𝑣,
respectively. Now a simple contrastive loss used in Eq.(1) as:
L𝑠𝑖𝑚𝑝𝑙𝑒(𝑣)=−∑︁
𝑢∈M+𝑠𝑖𝑚(𝑧𝑣,𝑧𝑢) +∑︁
𝑢′∈M−𝑠𝑖𝑚(𝑧𝑣,𝑧𝑢′),(10)
where𝑠𝑖𝑚is inner product similarity function: 𝑠𝑖𝑚(𝑧𝑣,𝑧𝑢)=𝑧𝑇
𝑣𝑧𝑢
∥𝑧𝑣∥∥𝑧𝑢∥.
However, the above loss function demonstrates significantly
inferior performance [ 45] compared to the softmax-based SimCLR
contrastive loss [6], which formulated as:
L𝑆𝑖𝑚𝐶𝐿𝑅(𝑣)=−∑︁
𝑢∈M+𝑣log𝑒𝑥𝑝(𝑧𝑣·𝑧𝑢/𝜏)Í
𝑢∈M+𝑣𝑒𝑥𝑝(𝑧𝑣·𝑧𝑢/𝜏)+Í
𝑢′∈M−𝑣𝑒𝑥𝑝(𝑧𝑣·𝑧𝑢′/𝜏)(11)
The softmax-based SimCLR contrastive loss is a hardness-aware
loss function [ 45], where𝜏is temperature and plays a key role
in controlling the local separation and global uniformity of the
representation distributions. Appropriate temperature selection
can effectively alleviate the uniform-tolerance dilemma. Based on
the above analysis, we use the SimCLR loss function defined in
Eq.(11) to learn the encoder.
4.4 Complexity analysis
The space complexity of our algorithm is O(𝑁𝑑+𝑚+𝑏2), where𝑁
and𝑚are the number of nodes and edges in the graph, respectively.
𝑑is the attribute dimension, and 𝑏is the batch size. For all datasets,
we store the graph feature X, the sparse adjacency matrix A, and the
mini-batch modularity matrix Bin memory, which require O(𝑁𝑑),
O(𝑚), andO(𝑏2)storage space, respectively. For GPUs, since only
the subgraph data corresponding to each batch needs to be stored,
the space complexity is reduced to O(𝑏(𝑏+𝑑)). For a given batch,
the forward and backward computation costs O(𝑏𝑑2). Hence, for𝑁
nodes, and𝑟epochs, time complexity is O(𝑁𝑟𝑑2). We compare the
time and space complexity of all methods used in Table 2, and the
result (see Table 4 in Appendix) shows that Magi performs better
in terms of both time and memory complexity as compared to other
methods that leverage both graph and feature information.Table 1: Dataset statistics.
Dataset #
Nodes # Edges # Features # Clusters
Cora 2,708
5,278 1,433 7
CiteSeer 3,327
4,614 3,703 6
Amazon-Photo 7,650
119,081 745 8
Amazon-Computers 13,752
491,722 767 10
ogbn-arxiv 169,343
1,166,243 128 40
Reddit 232,965
23,213,838 602 41
ogbn-products 2,449,029
61,859,140 100 47
ogbn-papers100M 111,059,956
1,615,685,872 128 172
5 EXPERIMENTS
In this section, we conducted extensive experiments on several node
classification benchmark datasets to evaluate the performance of
our method in comparison with state-of-the-art baselines. Code is
made publicly available at https://github.com/EdisonLeeeee/MAGI.
5.1 Datasets
In our experiments, we employ four small scale datasets (Cora [ 19],
Citeseer [ 19], Photo [ 34], Computers [ 34]), three large scale datasets
(Reddit [ 14], ogbn-arxiv [ 17], ogbn-products [ 17]) and one extra-
large scale dataset (ogbn-papers100M [ 17]) to showcase the effec-
tiveness of our method. The details of the datasets are presented in
Table 1.
5.2 Baselines
We compare Magi with 11 baselines, which can be categorized into
five groups:
(1) Methods utilize graph structure only. Node2vec [ 12] is a
scalable graph embedding technique that utilizes random walks on
the graph structure.
(2) Methods utilize graph features only. K-means[ 25] is a
traditional clustering algorithms that utilize only graph features.
(3) GCLs with augmentations. DGI [ 44], GRACE [ 51], MV-
GRL [ 15] and BGRL [ 39] are GCLs based on data augmentation to
learn node representations.
(4) Augmentation-free GCLs. CCGC [ 48], SCGDN [ 24], and
S3GC [9] are recent augmentation-free GCLs.
(5) Methods based on neural modularity maximization.
DGCLUSTER [ 2] and DMoN [ 41] are two recent neural modularity
maximization methods that can achieve good performance in graph
clustering, in which DGCLUSTER is a semi-supervised method.
5.3 Metrics
Following the evaluation setup of [ 9,15], we measure 4 metrics
related to evaluating the quality of cluster assignments: Accuracy
(ACC), Normalized Mutual Information (NMI), Adjusted Rand Index
(ARI) and Macro-F1 Score (F1). For all the aforementioned metrics,
higher values indicate better clustering performance. In our ex-
periments, we first generate representations for each method and
then perform spectral clustering on the embeddings of small-scale
datasets (Cora, Citeseer, Photo, Computers) and K-means clustering
on the other datasets to produce cluster assignments for evaluation.
 
1973Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Clustering performance of Magi and 11 state-of-the-art baselines. The boldfaced score and underlined score indicate
the best and second-best results, respectively. “OOM” denotes out-of-memory, and * indicates semi-supervised method.
Metho
d MetricBaselines Ours
K
-means Node2vec DGI GRACE MVGRL BGRL CCGC SCGDN DGCLUSTER∗S3GC DMoN Magi
CoraA
CC 0.350 0.612 0.726 0.739 0.763 0.742 0.664 0.748 0.753 0.742 0.517 0.760
NMI
0.173 0.444 0.571 0.570 0.608 0.584 0.527 0.569 0.600 0.588 0.473 0.597
ARI
0.127 0.329 0.511 0.527 0.566 0.534
0.446 0.526 0.548 0.544 0.301 0.573
F1 0.360 0.621 0.692 0.725 0.716
0.691 0.587 0.704 0.706 0.721 0.574 0.739
Citese
erACC 0.421 0.421 0.686 0.631 0.703 0.675
0.664 0.696 0.618 0.688 0.385 0.706
NMI 0.199 0.240 0.435 0.399 0.459 0.422 0.418 0.443 0.373 0.441 0.303 0.452
ARI
0.142 0.116 0.445 0.377 0.471 0.428 0.414 0.454 0.322 0.448 0.200 0.468
F1
0.394 0.401 0.643 0.603 0.654 0.631 0.627 0.655 0.543 0.643 0.437 0.648
P
hotoACC 0.272 0.276 0.430 0.677 0.411 0.665 0.689 0.780 0.820 0.752 0.248 0.790
NMI
0.132 0.115 0.337 0.535 0.303 0.601 0.612 0.694 0.735 0.598 0.077 0.716
ARI
0.055 0.049 0.221 0.427 0.188 0.441 0.495 0.607 0.671 0.561 0.038 0.615
F1
0.240 0.215 0.352 0.503 0.329 0.631 0.609 0.716 0.752 0.729 0.180 0.729
ComputersA
CC 0.225 0.356 0.479 0.519 0.580 0.469 0.480 0.582 0.453 0.588 0.432 0.620
NMI
0.110 0.278 0.420 0.538 0.482 0.441 0.452 0.545 0.498 0.560 0.461 0.592
ARI
0.056 0.248 0.306 0.343 0.433 0.306 0.290 0.430 0.261 0.438 0.288 0.462
F1
0.152 0.224 0.390 0.390 0.405 0.415 0.380 0.480 0.372 0.475 0.390 0.574
ogbn-ar
xivACC 0.181 0.290 0.314
OOM OOM OOM OOM OOM OOM0.350 0.250 0.388
NMI
0.221 0.406 0.412 0.463 0.356 0.469
ARI
0.074 0.190 0.223 0.270 0.127 0.310
F1
0.129 0.220 0.230 0.230 0.190 0.266
Re
dditACC 0.089 0.709 0.224
OOM OOM OOM OOM OOM OOM0.736 0.529 0.911
NMI
0.114 0.792 0.306 0.807 0.628 0.875
ARI
0.029 0.640 0.170 0.745 0.502 0.907
F1
0.068 0.551 0.183 0.560 0.260 0.853
ogbn-pr
oductsACC 0.200 0.357 0.320
OOM OOM OOM OOM OOM OOM0.402 0.304 0.425
NMI
0.273 0.489 0.467 0.536 0.428 0.551
ARI
0.082 0.170 0.174 0.230 0.139 0.215
F1
0.124 0.247 0.192 0.250 0.210 0.276
5.4 Experimental setup
We use a single NVIDIA A100 GPU with 40GB memory for each
method. All experiments are repeated 5 times and the mean values
are reported in Table 2. For all datasets except for ogbn-papers100M,
we set the number of run epochs at 400 and limit the maximum
runtime to 1 hour. For ogbn-papers100M we allow up to 6 hours
of training and upto 256GB main memory in addition. We employ
full batch training on small-scale graphs (Cora, Citeseer, Photo
and Computers) and set the number of rooted nodes 𝑛as 2048
for large and extra-large datasets. Due to space limitations, more
details about the hyperparameters are mentioned in Table 5 in
the Appendix. We have provided a mini-batch and highly scalable
implementation of Magi in PyTorch, making it easy for experiments
with all datasets to adapt to GPU processing. For all datasets, it is
only need to store the subgraphs and corresponding modularity
matrix in a sparse form to execute Magi’s forward andbackward
pass on the GPU. We also provide the GPU memory cost and the
time required to execute the training process for each method in
Table 4 in Appendix.6 EXPERIMENTAL RESULTS
In this section, we present the experimental results of Magi in
the graph clustering task. In addition to the main experiments, we
have conducted supplementary experiments aimed at answering
the following research questions:
(Q1): CanMagi scale to extra-large scale graphs containing up
to 100 million nodes and outperform state-of-the-art baselines?
(Q2): Can modularity maximization pretext tasks effectively
mitigate the problem of semantic drift?
(Q3): IsMagi robust to different hyperparameters, and does each
mechanism within Magi contribute uniquely to its performance?
6.1 Graph clustering result
Table 2 presents a comparison of Magi’s clustering performance
against various baseline methods across different scale datasets. For
small-scale datasets, namely Cora, Citeseer, Photo, and Computers,
we observe that MVGRL, S3GC, and DGCLUSTER emerge as the
three strongest baseline methods. Nonetheless, we find that Magi
consistently ranks first or second in performance in most cases.
 
1974KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
Table 3: Comparison results of different scalable methods on
ogbn-papers100M with 111M nodes and 1.6B edges.
ogbn-papers100M
Accuracy NMI ARI F1
K-means 0.144 0.368 0.074 0.101
Node2vec 0.175 0.380 0.112 0.099
DGI 0.151 0.416 0.096 0.111
S3GC 0.173 0.453 0.110 0.118
Magi 0.288 0.463 0.207 0.096
Cora CiteSeer Photo Computers ogbn-arxiv Reddit ogbn-products
Data set707580859095Accuracy(%)CCGC
SCGDN
S3GC
MAGI
Figure 3: Comparison of pseudo-labels constructed by Magi
and other methods.
number of walks2
3
5
7
10
20
50
100
depth of walks13510205070100NMI(%)
45.047.550.052.555.057.560.0Cora
number of walks2
3
5
7
10
20
50
100
depth of walks13510205070100NMI(%)
5060708090Reddit
Figure 4: The performance of Magi with varying the number
and depth of random walks on the Cora and Reddit dataset
in terms of NMI.
We note that even when compared to semi-supervised methods
such as DGCLUSTER, Magi is only slightly behind on the Photo
dataset and maintains a consistent lead on the other three datasets,
demonstrating Magi ’s superior performance. Next, we observe the
performance on large-scale datasets and find that Magi notably
outperforms baseline methods like DGI, DMoN and S3GC.Magi
is∼3.6%better on ogbn-arxiv, ∼29%better on Reddit and ∼2.6%
better on ogbn-products in terms of clustering F1 as compared
to the second-best method. Significantly, Magi outperforms the
second-best method on the Reddit dataset with an approximate
increase of ∼18%in Accuracy, ∼7%in NMI, ∼16%in ARI, and
∼29%in F1 score. One possible reason for the superior performance
is that the Reddit dataset exhibits a higher graph density, which
results in a more distinct community structure.
ogbn-papers100M: To answer Q1, we conduct a comparative
analysis of Magi’ performance on an extra-large dataset contain-
ing 111M nodes and 1.6B edges. Note that we compare K-means,
0.10.30.50.70.9
tau505560657075Value (%)
Cora
ACC
NMI
ARI
F1
0.10.30.50.70.9
tau75808590Value (%)
Reddit
ACC
NMI
ARI
F1Figure 5: The performance of Magi with varying the temper-
ature𝜏on the Cora and Reddit dataset, respectively.
Cora CiteSeer Photo Computers ogbn-arxiv Reddit ogbn-products
Data set2030405060708090Score(%)High-order
ACC
NMI
ARI
F1
Figure 6: Benefits of incorporating high-order proximity,
with the red part of each bar representing the gains provided
by high-order proximity.
Node2vec, DGI, and S3GC since others can not scale to this dataset.
The results are presented in Table 3. Our experimental results re-
veal that Magi adeptly scales to this dataset and demonstrates a
significant performance improvement over methods relying solely
on features (K-means) by approximately ∼14.4%, exclusively on
graph structure (Node2vec) by approximately ∼11.3%, and both
features and structure (S3GC) by approximately ∼11.5%in terms
of accuracy on the ogbn-papers100M dataset.
6.2 Semantic drift mitigation
In practice, it is difficult to measure the degree of semantic drift in
a graph. To answer Q2, we alternatively measure the semantic drift
based on the quality of pseudo-labels generated by various methods.
Specifically, considering the augmentation-free GCLs referenced in
Section 5.2, we employ ground-truth labels to evaluate the accuracy
of pseudo-labels produced by these methods. In this context, a
positive sample pair is considered as ‘1’ if both nodes belong to the
same class, while a negative sample pair is deemed ‘1’ if the nodes
belong to different classes; otherwise, they are labeled as ‘0’. The
results (refer to Figure 3) demonstrate that, in comparison to other
augmentation-free GCLs, Magi generates higher-quality pseudo-
labels. This confirms the efficacy of modularity maximization as a
pretext task in mitigating semantic drift.
6.3 Ablation study
To answer Q3, we first conduct thorough ablation studies on hy-
perparameters including the number of walks 𝑡, depth of walks 𝑙,
number of randomly rooted nodes 𝑛and temperature 𝜏to examine
the stability of Magi ’s clustering. Our findings are listed below:
(1) The performance of Magi remains relatively stable as long as
the number and depth of random walks are not excessively extreme,
as shown in Figure 4. We also note that the walk depth ∼5is an
 
1975Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 7:𝑡-SNE visualization of eight unsupervised methods on the Photo dataset.
0 10 20 30 40 50
Number of randomly rooted nodes4142434445464748NMI (%)ogbn-arxiv
2048
1024
512
256
Figure 8: The performance of Magi with varying differ-
ent number of randomly rooted nodes 𝑛on the ogbn-arxiv
dataset in terms of NMI.
optimal choice, as a larger walk depth tends to sample positive pairs
with differing community semantics. This finding is consistent with
the previous work [9].
(2)Magi can achieve the best performance when the temperature
𝜏is around ∼0.5(see Figure 5). A temperature 𝜏that is too small
or too large can lead to the model paying too much or too little
attention to hard negative samples [45], respectively.
(3) Different number of randomly sampled root nodes 𝑛have
similar clustering performance, but a smaller 𝑛can lead to faster
convergence (see Figure 8). Combined with Magi’s stable perfor-
mance at smaller walk depths, this demonstrates Magi’s good scal-
ability, meaning that for each node, Magi only needs to sample a
few positive samples.
High-order proximity. We then showcase the effectiveness of
exploring high-order proximity in Magi. In a given batch B, the
effectiveness of employing random walks versus directly sampling
a submatrix from the vanilla modularity matrix is compared across
various datasets. Figure 6 demonstrates that, in the majority of cases,
the use of random walks outperforms direct sampling. This result
highlights the advantage of incorporating high-order proximity into
the analysis, enabling a better capture of the complex relationships
and community structure within the graph.
Effectiveness of different components. We conduct ablation
study to manifest the efficacy of different components in Magi. We
set five variants of our model for comparison. Results are shown
in Table 6 in Appendix, where “Magi (w/ SL)” refers to the use of
simple contrastive loss defined in Eq.( 10), “Magi (w/ MS)” refers
to use S1and the sign of modularity score to define positive and
negative sample sets, “Magi (w/ EI)” refers to use S1and edgeindicators to define positive and negative sample sets, “Magi (w/
HMS)” refers to use sign of high-order modularity score [ 23] to
define positive and negative sample sets and Magi refers to use S1
andS2to define positive and negative sample sets.
In Table 6, it is observed that each improvement of our model
contributes to the final performance. First, loss (11) performs better
than loss (10). Secondly, the direct use of edge indicators to define
the positive and negative sample sets achieves the worst effect. We
infer that this may be because some edges exist between different
communities, which leads to some positive sample pairs from dif-
ferent communities, increasing the impact of semantic drift. This
can be mitigated by the use of modularity scores, and consideration
of high-order proximity in the community can further eliminate
the impact of semantic drift. Finally, our sampling strategy achieves
similar performance compared to using the sign of high order mod-
ularity matrix proposed in [ 23], but our sampling strategy has lower
computational complexity and can be scaled to large-scale graph
datasets with 100M nodes.
6.4 Visualization
In this part, we measure the quality of the generated embeddings
by directly employing t-SNE [ 42]. The generated embeddings of
each method are projected into 2-dimensional vectors for visual-
ization. Due to space limitations, we have selected seven strong
baseline methods for visual analysis. It should also be noted that
DGCLUSTER [ 2], being a semi-supervised method, was excluded
from this comparison. The visualization clearly demonstrates that
Magi produces representations with significantly higher clustering
efficacy compared to baseline methods.
7 CONCLUSION
In this paper, we explore the problem of graph clustering via neural
modularity maximization. Our work establishes the connection be-
tween neural modularity maximization and graph contrastive learn-
ing. This insight motivates us to propose Magi, a community-aware
graph clustering framework with modularity maximization as the
pretext task for contrastive learning. Magi is an augmentation-free
GCL framework, which avoids potential semantic drift and scala-
bility issues. To ensure better scalability, Magi adopts a two-stage
random walk to approximate the modularity matrix in a mini-batch
manner, followed by a principled contrastive loss to optimize the
goal of modularity maximization. Extensive experiments on eight
real-world graph datasets demonstrate the effectiveness of our
method, which achieves state-of-the-art in most cases compared
with strong baselines. We hope that the straightforward nature of
our approach serves as a reminder to the community to reevalu-
ate simpler alternatives that may have been overlooked, thereby
inspiring future research.
 
1976KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
REFERENCES
[1]Vandana Bhatia and Rinkle Rani. 2018. DFuzzy: a deep learning-based fuzzy
clustering model for large graphs. Knowledge and Information Systems 57 (2018),
159–181.
[2]Aritra Bhowmick, Mert Kosan, Zexi Huang, Ambuj K. Singh, and Sourav Medya.
2024. DGCLUSTER: A Neural Framework for Attributed Graph Clustering via
Modularity Maximization. In AAAI. AAAI Press, 11069–11077.
[3]Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of statistical
mechanics: theory and experiment 2008, 10 (2008), P10008.
[4]Ulrik Brandes, Daniel Delling, Marco Gaertler, Rachelle Goerke, Martin Hoefer,
Zoran Nikoloski, and Donald Wagner. 2006. Maximizing Modularity is hard.
arXiv: Data Analysis, Statistics and Probability (2006).
[5]Markus Brede. 2012. Networks—An Introduction. Mark E. J. Newman. (2010,
Oxford University Press.) $65.38, £35.96 (hardcover), 772 pages. ISBN-978-0-19-
920665-0. Artificial Life 18 (2012), 241–242.
[6]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A simple framework for contrastive learning of visual representations. In ICML.
Article 149, 11 pages.
[7]Jun Jin Choong, Xin Liu, and Tsuyoshi Murata. 2018. Learning Community
Structure with Variational Autoencoder. In ICDM. 69–78.
[8]Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. 2020. Adaptive Graph
Encoder for Attributed Graph Embedding. In KDD. 976–985.
[9]Fnu Devvrit, Aditya Sinha, Inderjit S Dhillon, and Prateek Jain. 2022. S3GC: Scal-
able Self-Supervised Graph Clustering. In NeurIPS, Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho (Eds.).
[10] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. 2004. Efficient Graph-Based
Image Segmentation. International Journal of Computer Vision 59 (2004), 167–181.
[11] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H.
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-
han Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,
Rémi Munos, and Michal Valko. 2020. Bootstrap your own latent a new approach
to self-supervised learning. In NeurIPS. 14 pages.
[12] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for
Networks. In KDD. Association for Computing Machinery, 855–864.
[13] R. Hadsell, S. Chopra, and Y. LeCun. 2006. Dimensionality Reduction by Learning
an Invariant Mapping. In 2006 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’06), Vol. 2. 1735–1742.
[14] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In NeurIPS. 1025–1035.
[15] Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view
representation learning on graphs. In ICML (ICML’20). JMLR.org, Article 385,
11 pages.
[16] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representa-
tions by mutual information estimation and maximization. In ICLR.
[17] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets
for Machine Learning on Graphs. In NeurIPS.
[18] Thomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. NeurIPS
Workshop on Bayesian Deep Learning (2016).
[19] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[20] Namkyeong Lee, Junseok Lee, and Chanyoung Park. 2022. Augmentation-Free
Self-Supervised Learning on Graphs. (2022).
[21] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu,
Changhua Meng, Zibin Zheng, and Weiqiang Wang. 2023. What’s Behind the
Mask: Understanding Masked Graph Modeling for Graph Autoencoders. In KDD.
ACM, 1268–1279.
[22] Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, Changhua
Meng, Zibin Zheng, and Liang Chen. 2024. A Graph is Worth 1-bit Spikes: When
Graph Contrastive Learning Meets Spiking Neural Networks. In ICLR.
[23] Yunfei Liu, Zhen Liu, Xiaodong Feng, and Zhongyi Li. 2022. Robust Attributed
Network Embedding Preserving Community Information. In ICDE. 1874–1886.
[24] Yixuan Ma and Kun Zhan. 2023. Self-Contrastive Graph Diffusion Network. In
ACM MM. 3857–3865.
[25] J. MacQueen. 1967. Some methods for classification and analysis of multivariate
observations.
[26] Parham Moradi, Sajad Ahmadian, and Fardin Akhlaghian. 2015. An effective
trust-based recommendation method using a novel graph clustering algorithm.Physica A Statistical Mechanics and its Applications 436 (2015), 462–481.
[27] Mark EJ Newman. 2006. Modularity and community structure in networks.
Proceedings of the national academy of sciences 103, 23 (2006), 8577–8582.
[28] M. E. J. Newman. 2006. Finding community structure in networks using the
eigenvectors of matrices. Phys. Rev. E (Sep 2006), 036104.
[29] Shirui Pan, Ruiqi Hu, Sai-Fu Fung, Guodong Long, Jing Jiang, and Chengqi Zhang.
2020. Learning Graph Embedding With Adversarial Training Methods. IEEE
Transactions on Cybernetics 50, 6 (2020), 2475–2487.
[30] Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, and Jin Young Choi.
2019. Symmetric Graph Convolutional Autoencoder for Unsupervised Graph
Representation Learning. In ICCV. 6518–6527.
[31] Advait Parulekar, Liam Collins, Karthikeyan Shanmugam, Aryan Mokhtari, and
Sanjay Shakkottai. 2023. InfoNCE Loss Provably Learns Cluster-Preserving
Representations. In Annual Conference Computational Learning Theory.
[32] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learn-
ing of Social Representations. In KDD. 701–710.
[33] Guillaume Salha-Galvan, Johannes F. Lutzeyer, George Dasoulas, Romain Hen-
nequin, and Michalis Vazirgiannis. 2022. Modularity-aware graph autoencoders
for joint community detection and link prediction. Neural Networks 153 (2022),
474–495.
[34] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of Graph Neural Network Evaluation. ArXiv
abs/1811.05868 (2018).
[35] Jianbo Shi and J. Malik. 2000. Normalized cuts and image segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 22, 8 (2000), 888–905.
[36] Heli Sun, Fang He, Jianbin Huang, Yizhou Sun, Yang Li, Chenyu Wang, Liang
He, Zhongbin Sun, and Xiaolin Jia. 2020. Network Embedding for Community
Detection in Attributed Networks. ACM Trans. Knowl. Discov. Data 14, 3, Article
36 (may 2020), 25 pages.
[37] Jianyong Sun, Wei Zheng, Qingfu Zhang, and Zongben Xu. 2022. Graph Neu-
ral Network Encoding for Community Detection in Attribute Networks. IEEE
Transactions on Cybernetics 52, 8 (2022), 7791–7804.
[38] Wangbin Sun, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, and Zibin Zheng.
2024. Rethinking and Simplifying Bootstrapped Graph Latents. In WSDM. ACM,
665–673.
[39] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Remi Munos,
Petar Veličković, and Michal Valko. 2021. Bootstrapped Representation Learning
on Graphs. In ICLR 2021 Workshop on Geometrical and Topological Representation
Learning.
[40] V. Traag, L. Waltman, and Nees Jan van Eck. 2019. From Louvain to Leiden:
guaranteeing well-connected communities. Scientific Reports 9 (03 2019), 5233.
[41] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Müller. 2023.
Graph Clustering with Graph Neural Networks. J. Mach. Learn. Res. 24 (2023),
127:1–127:21.
[42] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using
t-SNE. JMLR 9, 86 (2008), 2579–2605.
[43] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.
[44] Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2019. Deep Graph Infomax. In ICLR.
[45] Feng Wang and Huaping Liu. 2021. Understanding the Behaviour of Contrastive
Loss. In CVPR. 2495–2504.
[46] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised
Feature Learning via Non-parametric Instance Discrimination. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 3733–3742.
[47] Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong
Zhang. 2016. Modularity based community detection with deep learning. In IJCAI
(IJCAI’16). 2252–2258.
[48] Xihong Yang, Yue Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Qun Zheng,
Xinwang Liu, Liming Fang, and En Zhu. 2023. Cluster-Guided Contrastive Graph
Clustering Network. In AAAI. AAAI Press, 9 pages.
[49] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph Contrastive Learning with Augmentations. In NeurIPS,
Vol. 33. 5812–5823.
[50] Cangqi Zhou, Yuxiang Wang, Jing Zhang, Jiqiong Jiang, and Dianming Hu. 2022.
End-to-end Modularity-based Community Co-partition in Bipartite Networks. In
CIKM. 2711–2720.
[51] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph
Representation Learning and Beyond.
 
1977Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective KDD ’24, August 25–29, 2024, Barcelona, Spain
A ALGORITHM
To help better understand the proposed framework, we provide the
detailed algorithm for training Magi in Algorithm 1.
Algorithm 1 Community-Aware Graph Clustering (Magi)
Input: GraphG=(V,E,X), encoder𝑓𝜃(·), Randomly sampled
node setN, number of walks 𝑡, depth of walks 𝑙, hyperparame-
ter𝛼;
Output: Learned encoder 𝑓𝜃(·);
1:while not converged do
2: Perform random walk for each node in N, obtainB;
3: Perform random walk for each node in B, obtain similarity
matrix S;
4: Calculate modularity matrix Baccording to Eq.(9);
5: CalculateL𝑆𝑖𝑚𝐶𝐿𝑅 according to Eq.(11);
6: Update𝜃by gradient descent;
7:end while
8:return𝑓𝜃(·);
B MORE DETAILS ABOUT THE COMPLEXITY
ANALYSIS
We report the complexity analysis from both theoretical and exper-
imental perspectives in Table 4.
CMORE DETAILS ABOUT THE EXPERIMENTS
C.1 Hyperparameter settings
We report our hyperparameter settings in Table 5.C.2 Ablation study on different mechanisms
We report the ablation study results on different mechanisms in
Table 6.
Table 4: Time and space analyses of different methods. Where
𝑘is the number of clusters. The experimental GPU memory
costs and time costs (training) are obtained on Cora dataset.
Metho
d Time
Complexity Space Complexity Memory Cost(MB) Time Cost(s)
No
de2vec O
(𝑏𝑑) O(𝑁𝑑) 1,122 62.1
DGI O
(𝑚𝑑+𝑁𝑑2) O( 𝑚+𝑁𝑑+𝑑2) 218 36.8
GRACEO
(𝑁2𝑑+𝑚𝑑+𝑑2) O( 𝑚+𝑁𝑑) 277 2.9
BGRL O
(𝑚𝑑+𝑁𝑑2) O( 𝑚+𝑁𝑑+𝑑2) 1,201 31.2
MVGRL O
(𝑁2𝑑+𝑁𝑑2) O( 𝑁2+𝑁𝑑+𝑑2) 2,924 33.9
SCGDN O
(𝑁2+𝑁𝑑2) O( 𝑁2+𝑁𝑑+𝑑2) 1,080 31.9
CCGCO
(𝑘𝑁𝑑+𝑁2𝑑+𝑁𝑑2) O(𝑁2+(𝑁+𝑘)𝑑+𝑑2) 3,233 38.0
DGCLUSTER O
(𝑁2𝑑+𝑁𝑑2) O( 𝑁2+𝑁𝑑+𝑑2) 267 6.5
DMoN O
((𝑚+𝑁)𝑘)O(𝑚+𝑁𝑘) 301 9.3
S3GC O
(𝑁𝑠𝑑2) O( 𝑁𝑑+𝑏𝑠𝑑+𝑑2) 2,148 27.2
Magi O
(𝑁𝑑2)O(𝑚+𝑁𝑑+𝑏2) 209 2.7
Table 5: Hyperparameter settings, where 𝑑is the embedding
dimension, 𝛼is the activation threshold, 𝜏is the temperature,
𝑡is the number of random walks, 𝑙is the depth of random
walks.
Dataset #GNN
layers𝑑 𝛼 𝜏 𝑡 𝑙 learning rate weight decay
Cora 1
512 0.5 0.3 100 2 0.0005 0.001
CiteSeer 2
512 0.5 0.9 100 3 0.0001 0.0005
Photo 1
512 0.5 0.5 100 3 0.0005 0.001
Computers 2
512 0.1 0.9 100 3 0.0005 0.001
ogbn-arxiv 2
256 0.1 0.9 20 5 0.01 0
Reddit 2
256 0.5 0.5 20 5 0.01 0
ogbn-products 3
256 0.1 0.9 20 4 0.01 0
ogbn-papers100M 2
64 0.1 0.9 20 3 0.01 0
 
1978KDD ’24, August 25–29, 2024, Barcelona, Spain Yunfei Liu et al.
Table 6: Ablations on different components of Magi.
Dataset Metric Magi (w/ SL) Magi (w/ MS) Magi (w/ EI) Magi (w/ HMS) Magi
CoraACC 0.703 0.754 0.723 0.753 0.760
NMI 0.541 0.582 0.562 0.589 0.597
ARI 0.492 0.559 0.518 0.561 0.573
F1 0.687 0.736 0.713 0.729 0.739
CiteSeerACC 0.701 0.700 0.703 0.698 0.706
NMI 0.452 0.446 0.447 0.448 0.452
ARI 0.462 0.459 0.443 0.443 0.468
F1 0.645 0.653 0.632 0.639 0.648
PhotoACC 0.672 0.749 0.736 0.769 0.790
NMI 0.594 0.645 0.634 0.683 0.716
ARI 0.473 0.562 0.541 0.576 0.615
F1 0.684 0.715 0.713 0.706 0.729
ComputersACC 0.496 0.590 0.590 0.639 0.620
NMI 0.548 0.580 0.574 0.604 0.592
ARI 0.342 0.438 0.432 0.475 0.462
F1 0.395 0.558 0.543 0.580 0.574
ogbn-arxivACC 0.235 0.345 0.336
OOM0.388
NMI 0.265 0.463 0.457 0.469
ARI 0.227 0.252 0.247 0.310
F1 0.089 0.270 0.251 0.266
RedditACC 0.240 0.832 0.785
OOM0.911
NMI 0.360 0.852 0.836 0.875
ARI 0.145 0.842 0.783 0.907
F1 0.134 0.714 0.660 0.853
ogbn-productsACC 0.144 0.391 0.387
OOM0.425
NMI 0.079 0.525 0.510 0.551
ARI 0.059 0.192 0.177 0.215
F1 0.035 0.244 0.222 0.276
 
1979