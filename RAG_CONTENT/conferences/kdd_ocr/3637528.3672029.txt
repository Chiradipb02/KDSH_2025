One Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes
Yuchang Zhu
Sun Yat-sen University
Guangzhou, China
zhuych27@mail2.sysu.edu.cnJintang Li
Sun Yat-sen University
Guangzhou, China
lijt55@mail2.sysu.edu.cnYatao Bian
Tencent AI Lab
Shenzhen, China
yatao.bian@gmail.com
Zibin Zheng
Sun Yat-sen University
Guangzhou, China
zhzibin@mail.sysu.edu.cnLiang Chen∗
Sun Yat-sen University
Guangzhou, China
chenliang6@mail.sysu.edu.cn
ABSTRACT
Recent studies have highlighted fairness issues in Graph Neural
Networks (GNNs), where they produce discriminatory predictions
against specific protected groups categorized by sensitive attributes
such as race and age. While various efforts to enhance GNN fair-
ness have made significant progress, these approaches are often
tailored to specific sensitive attributes. Consequently, they neces-
sitate retraining the model from scratch to accommodate changes
in the sensitive attribute requirement, resulting in high computa-
tional costs. To gain deeper insights into this issue, we approach
the graph fairness problem from a causal modeling perspective,
where we identify the confounding effect induced by the sensitive
attribute as the underlying reason. Motivated by this observation,
we formulate the fairness problem in graphs from an invariant
learning perspective, which aims to learn invariant representations
across environments. Accordingly, we propose a graph fairness
framework based on invariant learning, namely FairINV, which
enables the training of fair GNNs to accommodate various sensitive
attributes within a single training session. Specifically, FairINV
incorporates sensitive attribute partition and trains fair GNNs by
eliminating spurious correlations between the label and various sen-
sitive attributes. Experimental results on several real-world datasets
demonstrate that FairINV significantly outperforms state-of-the-art
fairness approaches, underscoring its effectiveness.1
CCS CONCEPTS
•Computing methodologies →Knowledge representation
and reasoning.
KEYWORDS
Fairness, Graph Neural Networks, Invariant Learning
∗Corresponding author.
1Our code is available via: https://github.com/ZzoomD/FairINV/.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672029ACM Reference Format:
Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, and Liang Chen. 2024.
One Fits All: Learning Fair Graph Neural Networks for Various Sensitive
Attributes. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3672029
1 INTRODUCTION
Graph neural networks (GNNs) have achieved tremendous success
in processing graph-structured data [ 25,29,30], such as citation
networks [ 39] and social networks [ 27,41]. Consequently, this
advancement has led to their application across diverse domains,
including fraud detection [ 15] and recommender systems [ 45]. How-
ever, recent studies [ 12,40] have unveiled a concerning trend that
GNNs make discriminatory decisions toward the specific protected
groups defined by sensitive attributes, e.g., race, and age. This phe-
nomenon, termed the group fairness problem of GNNs, hinders the
application of GNNs in high-stake scenarios.
To improve the fairness of GNNs, considerable efforts have been
devoted to debiasing the training data [ 14,33] or learning fair GNNs
directly [ 6,12], referred to as the pre-process and in-process ap-
proaches, respectively. Within these two methodological categories,
common implementations encompass adversarial learning [ 33,43],
distribution alignment among various protected groups [ 14,18],
graph-structured data modification [ 14,33,40], and edge reweight-
ing [ 31]. Despite significant progress, these approaches are often
tailored to specific sensitive attributes, as shown in Figure 1(a).
Consequently, training GNN models from scratch becomes impera-
tive when faced with fairness requirement alterations in sensitive
attributes, such as transitioning from age-based considerations to
gender-related factors. Take loan approvals in a credit card network
as an example, according to fairness policies, initially trained GNNs
are designed to make fair decisions toward the protected groups
divided by age, e.g., age ≤25 and age >25. However, when policies
change to focus on gender, necessitating fair treatment between
male and female groups, the previously tailored model optimized
for age fairness becomes inadequate. Hence, this mandates retrain-
ing the GNN model to ensure fairness regarding gender, which is a
laborious and computationally intensive process.
In summary, there is a significant demand for a universal graph
fairness approach that trains fair GNNs across various sensitive
attributes in a single training session. Achieving such an approach
entails addressing the following challenges: (1) Generalization
4688
Corrected Version of Record. V.1.1. Published October 26, 2024
KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
SG
YSG
YPre-
process
In-
processSG
YS-related 
informationSSensitive 
AttributeGObserved
GraphYNode 
Label
Causal 
GraphCausal Graph 
of prior worksCausal Graph 
of FairINV
(a) Prior Works (b) General Fairness (c) Causal Graphs＞25Gender-specific 
Fairness
Graph (w/o S)TrainFairness 
Approach
GNNsGeneral 
Fairness
Gender
Age…＞25 ≤25
TrainTrain
GraphSens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
Train FrameworkDebiasing
Framework
Gender
≤25Age…GNNs
GNNsPre-
process
In-
process
(a) Prior Works (b) Our WorkGender
Age…＞25 ≤25Graph (w/o S)
TrainFairINV
GNNsGeneral 
Fairness
＞25
In-
processPre-
processGender-specific 
Fairness
TrainTrain
Graph
Sens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
Train FrameworkDebiasing
FrameworkGender
≤25Age…GNNs
GNNsVersion 1
Version 2
SG
YPre-process
In-processSSensitive Attribute GObserved Graph YNode Label
SG
Y
(a) Causal 
Graph(b) Causal Graph 
of prior works(c) Causal Graph 
of FairINVSG
YS-related 
informationVersion 3
(a) Prior Works (b) Our WorkGraph (w/o S)＞25 ≤25Gender
Age…
TrainFairINV
GNNsGeneral 
FairnessGender-specific 
Fairness
…≤25
＞25Age
In-
processPre-
process
TrainTrain
Graph
Sens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
FrameworkDebiasing
FrameworkGenderGNNs
GNNs
Figure 1: An illustration of comparison between prior works and our work (FairINV). (a) Prior works (pre-process and in-process)
achieve fairness toward the specific sensitive attribute; (b) Our work trains fair GNNs toward various sensitive attributes in a
single training session without accessing the sensitive attribute.
to various sensitive attributes. Previous studies aim to achieve
fairness tailored for the specific sensitive attribute. Additionally,
these approaches always require accessing the sensitive attributes
in the training process, which is impractical in real-world scenarios
due to legal limitations [ 7,26]. Correspondingly, our first chal-
lenge is to design a fairness framework that achieves fairness w.r.t.
various sensitive attributes in a single training session without
accessing the sensitive attribute, as shown in Figure 1(b). (2) Full
fairness. According to Section 3.2, two causal pathways ( 𝑆→𝑌
and𝑆→G→𝑌) demonstrate how the sensitive attribute 𝑆in-
fluences the labels 𝑌, misleading the trained GNNs to capture the
sensitive attribute information for predictions. In this regard, 𝑆is
a confounder. To achieve full fairness, blocking these two causal
effects appears to be a straightforward solution. However, it is chal-
lenging due to the presence of underlying spurious correlations
between unobservable variables 𝑆and𝑌. As discussed in Section 3.2,
prior works failed to eliminate both causal effects concurrently. In-
spired by INV-REG [ 35], backdoor adjustment implemented by data
partition presents a promising approach to tackle this challenge.
In this work, we first formulate the graph fairness issue from an
invariant learning [ 9,28,44] perspective, where sensitive attributes
as environments. Building upon this formulation, we propose a uni-
versal graph fairness framework named FairINV. To overcome the
first challenge, FairINV jointly optimizes a fair GNN for multiple
sensitive attributes inferred automatically via sensitive attribute
partition. To overcome the second challenge, FairINV incorporates
invariant learning optimization objectives building upon sensitive
attribute partition to remove confounding effects induced by 𝑆.
Specifically, the optimization objective of FairINV gives rise to
equal predictions of trained GNNs across environments (sensitive
attributes). In summary, FairINV mitigates spurious correlations be-
tween various sensitive attributes and the label. Our contributions
can be summarized as follows:
•We study the fairness issue on graphs from an invariant
learning perspective. To the best of our knowledge, this is
the first attempt to explore graph fairness from this particular
perspective.
•We introduce FairINV, a universal graph fairness framework
that inherits the spirit of graph invariant learning. An unsu-
pervised sensitive attributes partition of FairINV facilitates
fairness improvement in terms of various sensitive attributes.•We conduct experiments on several real-world datasets to
validate the effectiveness of FairINV. Experimental results
show that FairINV can train a fair GNN toward various
sensitive attributes in a single training session.
2 RELATED WORK
2.1 Fairness in Graph Neural Networks
The fairness of GNNs includes group fairness [ 2,12,50] and in-
dividual fairness [ 13,23]. Our study focuses on the group fair-
ness aspect, emphasizing equitable model decisions for each pro-
tected group partitioned by the sensitive attribute. Recent stud-
ies improving group fairness in GNNs typically segregate into
pre-process [ 14,33,49] and in-process [ 6,43,48] approaches. Pre-
process approaches aim to mitigate biases in training data before
training downstream tasks. To mitigate biases, techniques like ad-
versarial learning [ 33], and distribution alignment [ 11,14,47] serve
as optimization objectives for debiasing training data. Additionally,
some heuristic approaches modify the training graph [ 32,40] or
reweight edge [ 24,31] by either enhancing connections between
diverse groups or reducing connections within the same groups.
In-process approaches aim to train fair GNNs through the fairness-
aware framework. Similar to pre-process approaches, in-process
approaches also incorporate adversarial learning [ 6,12] and dis-
tribution alignment [ 17,18] to learn GNNs. Despite significant
progress, these approaches are tailored to the specific sensitive
attribute, lacking considerations for various sensitive attributes.
Despite Bose et al.’s [ 6] work of a compositional adversarial
framework using a set of sensitive-invariant filters, it necessitates
prior knowledge of considered sensitive attributes and their specific
values for each individual. In contrast, our work learns fair GNNs
toward various sensitive attributes in a single training session with-
out accessing sensitive attributes, which remains under-explored
for prior works.
2.2 Invariant Learning for Fairness
Guided by the independent causal mechanism assumption [ 36,37],
invariant learning, capable of capturing invariances across various
environments, stands as a significant approach facilitating out-of-
distribution (OOD) generalization [ 3,4,10,42]. The core idea behind
invariant learning is to learn causal information that stays invariant
4689One Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes KDD ’24, August 25–29, 2024, Barcelona, Spain
across different environments while disregarding spurious correla-
tions that exhibit variability [ 8]. However, there is limited research
exploring the application of invariance learning in fairness. Adragna
et al. [ 1] empirically illustrate how invariant risk minimization in
invariant learning can contribute to building fair machine learning
models. Ma et al. [ 35] point out the fairness-related bias in face
recognition stemming from confounding demographic attributes.
Then, they iteratively partition data to annotate confounders and
learn invariant features to remove the confounding effect. Yet, these
explorations of invariant learning in fairness predominantly focus
on Euclidean data. Conversely, significant efforts [ 9,28,34] have
addressed the out-of-distribution problem within graph structures
from an invariant learning perspective. However, the effectiveness
of invariant learning in ensuring graph fairness remains an under-
explored area. To the best of our knowledge, our work is the first
to explore the graph fairness problem utilizing graph invariant
learning.
3 PRELIMINARIES
In this section, we first introduce the detailed notations used in
this work. Then, we give a causal analysis for our study problem,
followed by the problem formulation of this work.
3.1 Notations
LetG=(V,E,X)denote an undirected and unweighted attributed
graph, whereVis a set of nodes and Eis a set of edges. Mean-
while,|V|=𝑛and|E|=𝑚represent the number of nodes and
edges, respectively. X∈R𝑛×𝑑represents the node attribute ma-
trix without the sensitive attribute 𝑆where𝑑is the node attribute
dimension. A∈{0,1}𝑛×𝑛is the adjacency matrix where A𝑢𝑣=1
indicates the edge connection 𝑒𝑢𝑣∈E between the node 𝑢and
the node𝑣, and A𝑢𝑣=0otherwise. Nodes with the same sensitive
attribute value belong to the same protected group. Most GNNs fol-
low the message-passing mechanism, which aggregates messages
from their neighbors, and can be summarized as follows:
h(𝑙)
𝑢=UPD(𝑙)({h(𝑙−1)
𝑢,AGG(𝑙)({h(𝑙−1)
𝑣 :𝑣∈N(𝑢)})}), (1)
where𝑙is the layer number, AGG(𝑙)(·)andUPD(𝑙)(·)denote ag-
gregation function and update function in 𝑙-th layer, respectively.
N(𝑢)denote the set of nodes adjacent to node 𝑢.
While our approach is applicable to various downstream tasks,
in this paper, we exemplify its application using the node classifi-
cation downstream task to illustrate the proposed methodology. A
GNN model 𝑓, consisting of an encoder 𝑓𝑔and a linear classifier
𝑓𝑐, takes a graphGas input and outputs the node predicted label
ˆ𝑌=𝑓𝑐(𝑓𝑔(G)) . The goal of 𝑓is to predict ˆ𝑌such that it is as close
as possible to the ground truth labels 𝑌.
3.2 Causal Analysis
To broaden insights, we construct a structural causal model [ 37]
(SCM) to analyze the group fairness issue in graph-structured data.
Figure 2(a) illustrates the causal relationship among the graph G,
the sensitive attribute 𝑆, and the node label 𝑌. In the SCM, there are
two causal pathways by which the sensitive attribute 𝑆affects the
node label𝑌, leading to issues of group fairness in GNNs. A detailed
description of these two causal pathways is provided below.
SG
YSG
YPre-
process
In-
processSG
YS-related 
informationSSensitive 
AttributeGObserved
GraphYNode 
Label
Causal 
GraphCausal Graph 
of prior worksCausal Graph 
of FairINV
(a) Prior Works (b) General Fairness (c) Causal Graphs＞25Gender-specific 
Fairness
Graph (w/o S)TrainFairness 
Approach
GNNsGeneral 
Fairness
Gender
Age…＞25 ≤25
TrainTrain
GraphSens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
Train FrameworkDebiasing
Framework
Gender
≤25Age…GNNs
GNNsPre-
process
In-
process
(a) Prior Works (b) Our WorkGender
Age…＞25 ≤25Graph (w/o S)
TrainFairINV
GNNsGeneral 
Fairness
＞25
In-
processPre-
processGender-specific 
Fairness
TrainTrain
Graph
Sens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
Train FrameworkDebiasing
FrameworkGender
≤25Age…GNNs
GNNsVersion 1
Version 2
SG
YPre-process
In-processSSensitive Attribute GObserved Graph YNode Label
SG
Y
(a) Causal 
Graph(b) Causal Graph 
of prior works(c) Causal Graph 
of FairINVSG
YS-related 
informationVersion 3
(a) Prior Works (b) Our WorkGraph (w/o S)＞25 ≤25Gender
Age…
TrainFairINV
GNNsGeneral 
FairnessGender-specific 
Fairness
…≤25
＞25Age
In-
processPre-
process
TrainTrain
Graph
Sens. Attr. = Gender Man Woman
Clean 
Graph
Fairness-aware
FrameworkDebiasing
FrameworkGenderGNNs
GNNsFigure 2: Structural causal model for GNNs prediction. (a)
The fairness issue on graphs can be caused by two causal
pathways, i.e., 𝑆→𝑌and𝑆→G→𝑌; (b) Prior works ei-
ther exclusively eliminate the causal pathway 𝑆→ G or
exclusively eradicate the causal pathway 𝑆→𝑌; (c) FairINV
tackles the fairness issue through blocking both two causal
pathways.
•𝑆→G→𝑌. This causal pathway describes the influence
of𝑆on the formation of graph-structured data G, which
subsequently impacts the predictions of node labels 𝑌by the
trained GNN. Specifically, the path 𝑆→G represents the im-
pact of𝑆on the generation process of graph-structured data.
In this context, the data exhibit two primary phenomena: (1)
the graph topology exhibits sensitive homophily [ 21], where
connected nodes are more likely to share the same sensitive
attribute𝑆. (2) Non-sensitive node attributes may implicitly
convey information about 𝑆. For instance, if 𝑆represents
gender, certain attributes like height, while not directly sen-
sitive, become relevant in inferring an individual’s gender.
The pathG→𝑌encapsulates the training process of GNNs,
wherein the network may inherit and subsequently propa-
gate biases (information related to 𝑆) present in the training
data.
•𝑆→𝑌. This causal pathway illustrates the underlying cor-
relation between the node label and the sensitive attribute.
This correlation often originates from societal discrimina-
tion against protected groups, leading to biased predictions
in the trained GNN. For instance, given a social network
dataset, the task is to predict the user’s occupational field.
The dataset predominantly comprises occupations of females
as nurses and males as engineers. Consequently, the GNN
trained on this dataset tends to predict engineering as the
occupational field for males and nursing for females, thus
revealing a gender bias in its predictions. This phenomenon
can be attributed to the 𝑆→𝑌causal pathway inherent in
the dataset.
In summary, discriminatory decisions in GNNs stem from the
two causal pathways discussed above. In this regard, the pathway
𝑌←𝑆→G is a backdoor path, with 𝑆acting as a confounder.
This pathway may mislead the trained GNN to utilize the sensitive
attribute for predictions, known as the spurious confounding effect.
To remove this effect, a straightforward yet challenging solution in-
volves eliminating 𝑆→G→𝑌and𝑆→𝑌. However, prior works
have not successfully removed both pathways simultaneously. As
4690KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
shown in Figure 2(b), pre-process methods primarily focus on reduc-
ing the information related to the sensitive attribute in the training
data, effectively removing the path 𝑆→G . Conversely, in-process
methods strive to develop a fair GNN that makes decisions indepen-
dently of𝑆, akin to removing the path 𝑆→𝑌. Another approach to
mitigate confounding effects is the backdoor adjustment, achieved
by partitioning the training data into different splits. In our scenar-
ios, we partition nodes into distinct demographic groups and learn
GNNs invariant across these groups. Drawing inspiration from a
fairness study in face recognition [ 35], we attempt to formulate the
graph fairness issue from an invariant learning perspective. Lever-
aging the environment inference capabilities of invariant learning,
we can unsupervisedly infer the sensitive attribute of nodes, facili-
tating group partitioning.
3.3 Problem Formulation
In this subsection, we formulate the graph fairness problem from an
invariant learning perspective. Our work focuses on the node-level
task. Following the setting of EERM [ 44], we investigate the impact
of the node’s ego-graph on the centered node. Specifically, given a
single graphG=(V,E,X), we have a set of ego-graph D=D𝑒
from various environment E𝑎𝑙𝑙, whereD𝑒={G𝑒𝑣,𝑦𝑒𝑣}is graphs
from environment 𝑒.G𝑒𝑣and𝑦𝑒𝑣are the ego-graph and the node label
of node𝑣. The invariant learning aims to learn GNNs to generalize
to all unseen environments. Denote 𝑓as a GNN model consisting
of an encoder and a classifier, ˆ𝑦𝑣=𝑓(G𝑣)as the predicted label
of node𝑣, andR𝑒(·)as the empirical risk under environment 𝑒.
Formally, the invariant learning on the node level is to minimize:
min
𝑓max
𝑒∈E𝑎𝑙𝑙R𝑒(𝑓) (2)
whereR𝑒(𝑓)=E𝑒
G𝑣,𝑦𝑣[𝑙(𝑓(G𝑣),𝑦𝑣)],𝑙(·,·)is the loss function.
Based on the above minimization objective, the trained GNN
performs equally across all environments. Similarly, the goal of
fairness on the graph is to have the model equally treat different de-
mographic groups divided by the sensitive attribute 𝑆. In this regard,
the centered node (the ego-graph) with different sensitive attribute
values or under different sensitive attributes can be regarded as a
graph under different environments. Naturally, a fairness problem
on graphs can be formulated as a form of invariant learning.
In this work, we aim to learn fair GNNs toward various sensitive
attributes in a single training session. With the formulation of
invariant learning, our goal is transformed into learning GNNs
invariant across different sensitive attributes and sensitive attribute
values. Formally, our goal is to minimize:
min
𝑓max
𝑆∈S𝑎𝑙𝑙R𝑆(𝑓) (3)
whereR𝑆(𝑓)=E𝑆
G𝑣,𝑦𝑣[𝑙(𝑓(G𝑣),𝑦𝑣)]is the empirical risk under
sensitive attribute 𝑆.S𝑎𝑙𝑙is a set of𝑆. For instance, assume that
gender and race are sensitive attributes, S𝑎𝑙𝑙includes male, female,
white, black, and yellow people environments.
4 PRESENT WORK: FAIRINV
In this section, we discuss how to learn a GNN towards fairness w.r.t.
various sensitive attributes in a single training session through our
proposed method FairINV. Specifically, we first give a brief overviewof FairINV and then make a detailed description of the components
of FairINV. Furthermore, we provide the training algorithm to shed
insights into the process of FairINV.
4.1 Overview
FairINV focuses on the node-level task, aiming to learn GNNs in-
variant across various sensitive attributes within a single training
session, thereby achieving fairness on graph-structured data. As
shown in Figure 3, our proposed method FairINV comprises two
modules, i.e., sensitive attribute partition (SAP) and sensitive invari-
ant learning (SIL). The SAP module partitions nodes into different
subsets by inferring variant ego-subgraphs for each centered node.
It should be noted that the sampling of ego-subgraphs can be disre-
garded due to the message-passing mechanism, which effectively
aggregates the representations of neighboring nodes to update its
own representation. To optimize this module, we employ the In-
variant Risk Minimization (IRM) objective [ 4], maximizing it to
guide the SAP module in capturing the worst-case environment.
This process can be seen as inferring the sensitive attribute value
of nodes. Since the maximization of the IRM objective is executed
in an unsupervised manner, we can iteratively predict sensitive
attributes multiple times. This iterative process enables FairINV to
achieve fairness with respect to various sensitive attributes in a sin-
gle training session. Due to the formulation of the fairness problem
from an invariant learning perspective in Section 3.3, we can natu-
rally tackle this problem through invariant learning. Specifically,
based on the partition results of SAP, the SIL module learns a GNN
invariant across different sensitive attribute partitions through a
variance-based loss. The objective of being invariant across differ-
ent sensitive attribute partitions implies the equitable treatment
of different demographic groups, thereby achieving fair decision-
making. Overall, the SAP module is akin to data augmentation,
facilitating the process of the SIL module. Due to such a training
paradigm, FairINV follows the same inference process as vanilla
GNNs.
4.2 Sensitive Attributes Partition
Existing methods are designed for the specific sensitive attribute
while assuming accessible sensitive attributes. However, these meth-
ods are impractical in real-world scenarios due to legal restrictions.
To overcome this challenge, there is a need to infer the sensitive
attribute value for each node in an unsupervised manner. Infer-
ring the sensitive attribute value multiple times can be regarded
as obtaining multiple sensitive attribute values, e.g., gender, and
race, facilitating the achievement of fairness w.r.t. various sensitive
attributes. Unfortunately, learning a sensitive attribute inference
model without access to the sensitive attribute ground truth is a
non-trivial task.
Inspired by the unsupervised environment inference in invariant
learning [ 10], we aim to maximize variability across environments
to achieve the sensitive attribute partition. Based on our formula-
tion of the fairness problem from an invariant learning perspective,
the sensitive attributes can be seen as environments in invariant
learning. Nodes with different sensitive attribute values can be
considered as being in different environments. Thus, maximizing
variability across environments indicates inferring a worst-case
4691One Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes KDD ’24, August 25–29, 2024, Barcelona, Spain
Version 1
𝐿௖௟௦൅𝐿ூோெ൅𝐿ெெ஽
Graph
GNNs 
BackboneClassifierGNNs 
Backbone 𝜑
Pre-trained…
 Variant InferEdge 
Weights 𝑊Sens. Infer
𝑞
max𝑙ሺ𝜑 𝐺, 𝑌 ሻ 𝑞𝑠𝐺 ,𝑊Sens. 
PartitionFrozen
Sens. = Sensitive
Version 2
𝑚𝑖𝑛𝑉𝑎𝑟ሺℒ௖௟௦ௌ𝑌෠,𝑌ௌఢௌೌ೗೗ሻ൅𝛼𝑀𝑒𝑎𝑛ሺ ℒ௖௟௦ௌ𝑌෠,𝑌ௌఢௌೌ೗೗ሻ
Sens.Attr.
Partition List
Edge 
Weights 𝑤
kPre-trained Graph
ℒ𝜑𝐺, 𝑌𝑞𝒫𝐺 ,𝑤 ,𝑌
GNNs 
Backbone 𝛷Classifier 𝑔
GNNs 
Backbone 𝜑…
Variant 
Infer 𝜓
Sens.Attr. 
Infer 𝑞Edge Weights 𝒘
…
 …
+
𝑚𝑎𝑥Sensitive Attributes Partition
ℒ௖௟௦ௌ𝑌෠,𝑌ௌఢௌೌ೗೗
(a) Training PipelineFrozen
Sens.Attr. = Sensitive 
AttributeBackward
(b) InferenceGNNs 
Backbone 𝛷Classifier 𝑔𝑌෠
Sensitive Invariant Learning
𝒫௜௜ୀଵ௞
Figure 3: The overview of FairINV. FairINV includes two stages: Sensitive Attributes Partition (SAP) and Sensitive Invariant
Learning (SIL).
sensitive attribute partition, where GNNs exhibit the worst fair-
ness performance towards the demographic group divided by the
sensitive attribute. However, directly inferring sensitive attributes
partition through the aforementioned maximization objective is
impractical due to the interactive nature of graph-structured data.
Following the inspiration from GIL [ 28], identifying variant sub-
graphs as auxiliary information for sensitive attribute partition may
provide a desirable solution.
Following the above idea, we construct the SAP module to infer
the sensitive attribute value of each node. Specifically, the SAP
module consists of a pre-trained GNN backbone 𝜑, a variant infer-
ence model 𝜓, and a sensitive attribute inference model 𝑞. With
an expected structure identical to the GNN model to be trained, 𝜑
serves as an Empirical Risk Minimization-trained (ERM-trained)
reference model. In other words, it is trained on the node clas-
sification task in a semi-supervised manner to capture spurious
correlations between variant patterns and node labels. Given an
attributed graphG=(V,E,X)with unknown sensitive attribute
values, we sample an ego-graph set {G𝑣}𝑣∈V, whereG𝑣is the
ego-graph of the centered node 𝑣.𝜑takesG𝑣as input and outputs
the node representation h𝑣=𝜑(G𝑣). Due to the similar process
between ego-graphs sampling and the message-passing mechanism
of GNN, the sampling of ego-graphs can be disregarded. For two
connected nodes 𝑢and𝑣inG,𝜓takes the concatenation of node
representations h𝑢andh𝑣as input to measure the variant score
of edge𝑒𝑢𝑣. Assuming inferring the sensitive attribute 𝑘times, the
variant score in the 𝑖-th inferring can be formulated as:
𝑤𝑖
𝑢𝑣=𝜎(𝜓([h𝑢,h𝑣])), (4)
where[·,·]denotes the concatenation operation, and 𝜎(·)is a sig-
moid function. In this context, 𝜓can be implemented as a linear
layer, measuring the probability that edge 𝑒𝑢𝑣belongs to the variant
pattern.
According to Eq. (4), we can obtain a variant score vector w𝑖∈
R|E|, which includes variant scores for all edges. w𝑖represents
variant patterns, i.e., variant subgraphs, capturing the variant corre-
lation between the graph structure and node labels under different
sensitive attribute groups. After inferring 𝑘times, we have a variant
score vector set{w𝑖}𝑘
𝑖=1, representing the variant correlation for
various sensitive attributes. Accordingly, we can use these variantpatterns to infer sensitive attributes. Specifically, we employ a GNN
classifier as the sensitive attribute inference model 𝑞to generate
the sensitive attribute partition. Given the 𝑖-th variant score vector
w𝑖, the𝑖-th sensitive attribute partition P𝑖can be formulated as:
P𝑖=𝑞(G,w𝑖,𝑌), (5)
whereP𝑖∈R|V|×𝑡, and𝑡is the number of sensitive attribute
groups. For instance, in the case of a sensitive attribute like gender,
𝑡=2.
To achieve accurate partitioning of sensitive attributes, optimiz-
ing𝜓and𝑞with well-defined objectives is crucial. Our goal is to
capture variant patterns that result in significant performance dif-
ferences across different sensitive attribute groups. Consequently,
aligning with the approach of EIIL [ 10], we employ the IRM objec-
tive as the optimization objective of 𝜓and𝑞. Formally, the opti-
mization objective of SAP can be formulated as follows:
max
𝜃𝜓,𝜃𝑞∥▽𝑤R𝑆(𝑤◦𝜑,𝑞)∥, (6)
where𝑤denotes a constant scalar multiplier of 1 for each output
dimension, and the empirical risk R𝑆(𝜑,𝑞)can be formulated as:
R𝑆(𝜑,𝑞)=∑︁
𝑣∈Vq𝑣(𝑆)L(𝜑(G𝑣),𝑦𝑣), (7)
where q𝑣(𝑆):𝑞𝑣(𝑆|G𝑣,w𝑖,𝑦𝑣)denotes a soft per-partition risk and
is a node-level implementation of Eq. (5).
Notably, the application of the IRM objective enables inferring
sensitive attributes in an unsupervised manner. The inferred sen-
sitive attributes correspond to the demographic group partition
with the worst-case fairness performance. In such an unsupervised
manner, we can partition sensitive attributes 𝑘times to identify the
top𝑘worst-case partitions, denoted by {P𝑖}𝑘
𝑖=1.
4.3 Towards Fairness via Invariant Learning
In the SIL module, we aim to learn a fair GNN model 𝑓including a
GNN backbone Φand a classifier 𝑔from an invariant learning per-
spective. Prior works have revealed that training model 𝑓in an ERM
paradigm inevitably results in the capturing of spurious correlations.
In our scenarios, such spurious correlations are the correlation be-
tween the sensitive attribute and node labels, being uncovered as
4692KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
Table 1: Datasets statistics.
Dataset German
Bail Pokec-z Pokec-n NBA
#No
des 1,000
18,876 67,796 66,569 403
#Edges 22,242
321,308 617,958 583,616 21,242
#Attr. 27
18 277 266 95
Sens. Gender
Race Region Region Nationality
variant patterns through the SAP module. Naturally, based on P𝑖,
we guarantee the variance across the sensitive attribute groups
to optimize 𝑓, which is motivated by the objective of EERM [ 44].
In other words, this objective guides the model to leverage the
invariant patterns to yield equal performance on different sensitive
attribute groups 𝑆. Given an attributed graph G=(V,E,X), we
can obtain variant score vector set {w𝑖}𝑘
𝑖=1and sensitive attributes
partition set{P𝑖}𝑘
𝑖=1. In the forward of the training pipeline, 𝑓takes
G,w𝑖as input to predict node labels ˆ𝑌=𝑓(G,w𝑖),𝑖=1,2,...,𝑘 .
Thus, optimization objectives of 𝑓can be formulated as follows:
min
𝜃𝑓𝑉𝑎𝑟({L𝑆
𝑐𝑙𝑠(ˆ𝑌,𝑌)}𝑆∈S𝑎𝑙𝑙)+𝛼𝑀𝑒𝑎𝑛({L𝑆
𝑐𝑙𝑠(ˆ𝑌,𝑌)}𝑆∈S𝑎𝑙𝑙),(8)
where𝑉𝑎𝑟(·)and𝑀𝑒𝑎𝑛(·)are variance and mean functions, respec-
tively. The sensitive attribute group 𝑆is derived fromP.L𝑆
𝑐𝑙𝑠(·,·)
is the classification loss function under 𝑆and we employ a binary
cross-entropy function as L𝑐𝑙𝑠in all experiments. 𝛼is a hyperpa-
rameter to balance two loss terms.
In Eq. (8), the variance loss term aims to minimize the perfor-
mance difference between various sensitive attribute groups while
the mean loss term ensures the predicted accuracy across all sensi-
tive attribute groups.
4.4 Training Algorithm
To further help understand our proposed framework FairINV, we
summarize the detailed training algorithm of FairINV, as shown in
Algorithm 1.
5 EXPERIMENTS
In this section, we conduct node classification experiments on
several commonly used fairness datasets, including German, Bail,
Pokec-z, Pokec-n, and NBA. Table 1 presents the statistical informa-
tion of these datasets. In our experiments, we aim to answer the fol-
lowing three questions: RQ1: Can FairINV improve fairness while
maintaining utility performance? RQ2: How does FairINV achieve
fairness across various sensitive attributes in a single training ses-
sion? RQ3: How do relevant hyperparameters and components
impact FairINV?
5.1 Experimental Settings
5.1.1 Datasets. Five real-world fairness datasets, namely German,
Bail [ 2], Pokec-z, Pokec-n, and NBA [ 12], are employed in our
experiments. We give a brief overview of these datasets as follows:
•German [5] is constructed by [ 2]. Specifically, German in-
cludes clients’ data in a German bank, e.g., gender, and loan
amount. Nodes represent clients in the German bank. The
edges in the German dataset are constructed according to
individual similarity. Regarding “gender" as the sensitiveAlgorithm 1 Training Algorithm of FairINV
Input:G=(V,E,X)without the sensitive attribute 𝑆, node labels
Y, the pre-trained GNN backbone 𝜑, the variant inference model 𝜓,
the sensitive attribute inference model 𝑞, GNN model 𝑓={Φ,𝑔},
partition time 𝑘, and hyperparameters 𝛼.
Output: Trained inference GNN model 𝑓.
1:// SAP module
2:for𝑖=1to𝑘do
3:H←𝜑(G);
4:for𝑡=1to𝑒𝑝𝑜𝑐ℎ𝑆𝐴𝑃do
5:𝑤𝑖𝑢𝑣←𝜎(𝜓([h𝑢,h𝑣])),h𝑢,h𝑣∈H,𝑒𝑢𝑣∈E;
6: // Sensitive attribute partition
7:P𝑖←𝑞(G,w𝑖,𝑌),w𝑖={𝑤𝑖𝑢𝑣|𝑢,𝑣∈V,𝑒𝑢𝑣∈E} ;
8: Calculate loss function according to Eq.(6);
9: Update parameters of 𝜓and𝑞by gradient descent;
10: end for
11:𝑤𝑖𝑢𝑣←𝜎(𝜓([h𝑢,h𝑣])),h𝑢,h𝑣∈H,𝑒𝑢𝑣∈E;
12:P𝑖←𝑞(G,w𝑖,𝑌),w𝑖={𝑤𝑖𝑢𝑣|𝑢,𝑣∈V,𝑒𝑢𝑣∈E} ;
13:end for
14:Obtain{P𝑖}𝑘
𝑖=1,{w𝑖}𝑘
𝑖=1;
15:// SIL module
16:for𝑡=1to𝑒𝑝𝑜𝑐ℎ do
17: for𝑖=1to𝑘do
18: ˆ𝑌←𝑓(G,w𝑖);
19: Calculate loss function according to Eq.(8) and P𝑖;
20: Accumulated loss;
21: end for
22: Update parameters of 𝑓by gradient descent;
23:end for
24:return𝑓;
attribute, the goal of German is to classify clients into two
credit risks (high or low).
•Bail [2] is a defendants dataset, where defendants in this
dataset are released on bail during 1990-2009 in U.S states [ 22].
We regard nodes as defendants and edges are decided by the
similarity of past criminal records and demographics. Con-
sidering “race” as the sensitive attribute, the task is to predict
whether defendants will commit a crime after release (bail
vs. no bail).
•Pokec-z/n [12,41] is derived from a popular social network
application in Slovakia, where Pokec-z and Pokec-n are so-
cial network data in two different provinces. Nodes denote
users with features such as gender, age, interest, etc. Edge
represents the friendship between users. Considering “re-
gion” as the sensitive attribute, the task is to predict the
working field of the users.
•NBA [12] is derived from a Kaggle dataset comprising ap-
proximately 400 NBA basketball players from the 2016-2017
season. Nodes denote NBA basketball players with features
such as performance statistics, age, etc. Edge represents the
relationship between these players on Twitter. Considering
“nationality (U.S. and overseas players)” as the sensitive at-
tribute, the goal is to predict whether the salary of the player
is over the median.
4693KDD ’24, August 25–29, 2024, Barcelona, SpainOne Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes
5.1.2 Baselines. We compare FairINV with four state-of-the-art 
fairness methods, including EDITS, NIFTY, FairGNN, and FairVGNN. 
Among these four methods, EDITS can be summarized as the pre-
processing fairness method, while the remaining baselines repre-
sent the in-processing approach. A brief overview of these methods 
is shown as follows:
•EDITS [14] modify graph-structured data by minimizing
the Wasserstein distance between two demographics.
•NIFTY [2] is a fair and stable graph representation learning
method. The core idea behind NIFTY is learning GNNs to
keep stable w.r.t. the sensitive attribute counterfactual.
•FairGNN [12] aims to learn fair GNNs with limited sensitive
attribute information. To achieve this goal, FairGNN employs
the sensitive attribute estimator to predict the sensitive at-
tribute while improving fairness via adversarial learning.
•FairVGNN [43] learns a fair GNN by mitigating the sensi-
tive attribute leakage using adversarial learning and weight
clamping technologies.
5.1.3 Evaluation Metrics. To evaluate the utility performance, we
use AUC and F1 scores. Additionally, we employ two commonly
used fairness metrics, i.e., Δ𝐷𝑃=|𝑃(ˆ𝑦=1|𝑠=0)−𝑃(ˆ𝑦=1|𝑠=
1)|[16] and Δ𝐸𝑂=|𝑃(ˆ𝑦=1|𝑦=1,𝑠=0)−𝑃(ˆ𝑦=1|𝑦=1,𝑠=
1)|[20], to evaluate the fairness performance.
5.1.4 Implementation Details. For all methods, including FairINV,
we use a multi-layer GNN model consisting of a GNN backbone
Φand a 1-layer linear classifier 𝑔. To validate the generalizability
of FairINV on various backbones, we employ the following GNN
backbones: a 1-layer GCN [ 25], a 1-layer GIN [ 46], and a 2-layer
GraphSAGE [ 19]. Here, the hidden dimension of all GNN backbones
is set to 16 for all datasets. Hyperparameter settings for all baseline
methods adhere to the guidelines provided by the respective authors.
We conduct all experiments 5 times and reported average results.
For FairINV, we utilize the Adam optimizer with the learning
rate𝑙𝑟=1×10−2, epochs=1000, and the weight decay = 1×10−5.
Using the same optimizer, the learning rate 𝑙𝑟𝑠𝑝for training the SAP
modules are set to{0.1,0.1,0.01,0.5,0.1}for German, Bail, Pokec-z,
Pokec-n, and NBA datasets, respectively. Meanwhile, we set the
balanced parameter 𝛼to{10,10,10,1,1}for German, Bail, Pokec-z,
Pokec-n, and NBA datasets, respectively. The partition times 𝑘and
the number of sensitive attribute groups 𝑡are fixed at 3 and 2 for
all datasets. In the SAP module, a 1-layer linear layer is used as
the variant inference model 𝜓. We employ a model with the same
structure as the GNN model ( Φand𝑔) as a sensitive attribute in-
ference model 𝑞. Meanwhile, 𝜑has the same structure as the GNN
model ( Φand𝑔) and is trained by minimizing the cross-entropy
loss function. For 𝜑and the SAP module, we set the training epoch
to 500. Due to all baselines using the sensitive attribute, FairINV
incorporates the sensitive attribute into the original node features
for fair comparison. Moreover, all evaluations of FairINV are con-
ducted on a single NVIDIA RTX 4090 GPU with 24GB memory. All
models are implemented with PyTorch and PyTorch-Geometric.
5.2 Comparison Study
To answer RQ1, we conduct a comparison study between FairINV
and four baseline methods for the node classification task acrossfive datasets. Specifically, we verify the effectiveness of FairINV
on three GNN backbones, i.e., GIN, and GraphSAGE. Limited by
the space, we only present the comparison results on the GCN
backbone and provide more results in Appendix A. As shown in
Table 2, the following observations can be seen: (1) FairINV out-
performs all baseline methods in terms of both utility and fairness
in most cases. (2) In instances where FairINV exhibits relatively
lower performance, the best-performing baseline method surpasses
FairINV by a slight margin. (3) FairINV improves fairness while
maintaining utility performance, as evidenced by the performance
improvement compared with vanilla GCN.
The first two observations verify the effectiveness of FairINV on
fairness performance, simultaneously showcasing the state-of-the-
art performance achieved by FairINV. As for the last observation, the
potential explanation lies in FairINV’s adherence to the invariance
principle [ 9,28,38], i.e., (1) sufficiency property and (2) invariance
property. The sufficiency property emphasizes the necessity of ade-
quate predictive abilities for the downstream task, which explains
the preservation of the utility performance of FairINV. Meanwhile,
the invariance property assumes consistency across different envi-
ronments, signifying the invariance across the sensitive attribute
groups in FairINV. Consequently, this property serves as the under-
lying reason for FairINV’s superior fairness performance. Overall,
leveraging invariant learning, FariINV captures invariant subgraphs
with sufficient information for the downstream task while learning
to be invariant across different sensitive attribute groups. Thus,
FairINV improves fairness while preserving utility performance. In
addition, as shown in Appendix A, similar results can be observed
from the experiments on GIN and GraphSAGE backbones.
5.3 Generalizing to Various Sensitive Attributes
To answer RQ2, we generalize FairINV to various sensitive attribute
scenarios. Specifically, we employ FairINV once to train a GNN
model and then evaluate the fairness performance of this GNN
model toward various sensitive attributes. Table 3 presents the
results of various sensitive attributes and inferior results compared
to vanilla GCN are marked with a gray background. We only present
results on four datasets except for the NBA dataset due to the lack of
suitable node features as the sensitive attribute. When the sensitive
attribute is “Age”, we set the median of age as the threshold to
obtain binary values for the sensitive attribute. Furthermore, we
provide the comparison results of FairINV and baseline methods in
multi-sensitive attribute scenarios, as detailed in the Appendix B.
We make the following observations from this table: (1) Fair-
INV achieves superior performance compared with vanilla GCN in
terms of both fairness and utility. This observation demonstrates
that FairINV improves the fairness of GNNs towards various sen-
sitive attributes in a single training session. (2) In some instances,
FairINV exhibits slightly inferior fairness performance compared to
vanilla GCN. We attribute this to the fact that the sensitive attribute
groups partitioned by the SAP module are unrelated to the sensitive
attributes we have selected. This is primarily due to the model itself
making fairly equitable decisions concerning the sensitive attributes
we have chosen. In other words, when grouped according to the
selected sensitive attributes, the variability values are relatively
small. Consequently, the sensitive attribute groups partitioned by
4694KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
Table 2: Comparison results of FairINV and baseline fairness methods on GCN backbone. In each row, the best result is indicated
in bold, while the runner-up result is marked with an underline . OOM: out-of-memory on a GPU with 24GB memory.
Datasets Metrics V
anilla GCN EDITS NIFTY FairGNN FairVGNN FairIN
V
GermanAUC 65.90±0.83 69.89±3.23
67.77±4.30 67.35±2.13 72.38±1.09 69.11±1.80
F1 77.32±1.20 82.01±0.91
81.43±0.54 82.01±0.26
81.94±0.26 82.36±0.35
Δ𝐷
𝑃(↓) 36.29±4.64
2.38±1.36 2.64±2.25 3.49±2.15 1.44±2.04 0.76±1.24
Δ𝐸
𝑜(↓) 31.35±4.39
3.03±1.77 2.52±2.88 3.40±2.15 1.51±2.11 0.15±0.29
BailAUC 87.13±0.31 87.92±1.83
79.62±1.80 87.27±0.76 87.05±0.39 88.53±1.83
F1 78.98±0.67 79.45±1.48
67.19±2.63 77.67±1.33 79.56±0.29 78.80±3.71
Δ𝐷
𝑃(↓) 9.18±0.59
8.03±0.97 3.52±0.72 6.72±0.60 6.31±0.77 3.58±1.61
Δ𝐸
𝑜(↓) 4.43±0.37
5.80±0.73 2.82±0.82
4.49±1.00 5.12±1.40 2.15±1.24
Poke
c-zAUC 76.42±0.13
OOM71.59±0.17 76.02±0.15
75.52±0.06 75.79±0.08
F1 70.32±0.20
67.13±1.66 68.84±3.46 70.45±0.57 70.78±0.50
Δ𝐷
𝑃(↓) 3.91±0.35
3.06±1.85 2.93±2.83
3.30±0.87 2.70±0.96
Δ𝐸
𝑜(↓) 4.59±0.34
3.86±1.65 2.04±2.27 3.19±1.00 2.23±0.66
Poke
c-nAUC 73.87±0.08
OOM69.43±0.31
73.49±0.28 72.72±0.93 73.55±0.16
F1 65.55±0.13 61.55±1.05
64.80±0.89 62.35±1.14 65.19±0.62
Δ𝐷
𝑃(↓) 2.83±0.46
5.96±1.80 2.26±1.19
4.38±1.73 1.24±0.64
Δ𝐸
𝑜(↓) 3.66±0.43
7.75±1.53 3.21±2.28
6.74±1.87 2.80±0.78
NBAAUC 66.09±0.98
65.91±5.19 68.88±0.93 72.53±0.96 64.73±2.34 66.18±2.84
F1 61.32±2.53
61.42±8.26 67.41±2.92
60.18±20.18 60.32±5.79 67.56±1.30
Δ𝐷
𝑃(↓) 28.80±4.17
6.09±5.1 5.41±2.78 6.44±6.74 3.48±3.62 1.98±3.15
Δ𝐸
𝑜(↓) 20.00±9.43
6.0±5.33 3.43±1.78 2.64±2.61 3.33±3.65 2.67±3.89
GermanBail
Pokecz
Pokecn
NBA/uni00000018/uni00000018/uni00000019/uni00000016/uni0000001a/uni00000014/uni0000001a/uni0000001c/uni0000001b/uni0000001a/uni0000001c/uni00000018/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000024/uni00000038/uni00000026/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048GermanBail
Pokecz
Pokecn
NBA/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013DP/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000045/uni0000000c/uni00000003DP/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048
VanillaFairINVVI
FairINVSAP
FairINVSIL
FairINV
Figur
e 4: The results of ablation study on all datasets.
SAP when maximizing variability are unrelated to the groups cor-
responding to such low variability. For the results with “Age” as the
sensitive attribute on the Pokec-n dataset, despite decisions of the
model being extremely unfair with respect to the sensitive attribute,
FairINV still does not improve fairness. We attribute this to the
aggressive partitioning of age into binary-sensitive attributes.
5.4 Ablation Study
To answer RQ3, we conduct an ablation study to investigate the
impact of each component of FairINV on improving fairness and
maintaining utility. Specifically, we investigate the effect of three
components including the variant inference model 𝜓, the SAP mod-
ule, and the SIL module, denoted by 𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑉𝐼,𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑆𝐴𝑃,
and𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑆𝐼𝐿.𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑉𝐼removes𝜓, replacing wpredicted
by𝜓with random numbers. 𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑆𝐴𝑃removes the SAP mod-
ule, replacingPpredicted by 𝑞with the sensitive attribute ground
truth.𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑆𝐼𝐿removes the SIL module, replacing the objec-
tive shown in Eq. (8)with minimizing the IRM objective shown in
Eq. (6). Figure 4 presents the ablation results on five datasets.From this figure, we observe that the removal of the SIL module
leads to a decline in both utility and fairness, implying the sig-
nificant impact of SIL on FairINV. Furthermore, from the results
of𝐹𝑎𝑖𝑟𝐼𝑁𝑉−𝑆𝐴𝑃, even when using the ground truth of sensitive
attributes to replace the predicted sensitive attribute partition P
by SAP, the performance of FairINV is still affected. This experi-
mental phenomenon is consistent with previous research results
on invariant learning without environmental labels. Finally, we
find that removing the variant inference model affects the fairness
performance of FairINV, indicating the importance of the variant
inference model in capturing variant patterns.
5.5 Hyperparameters Sensitivity
To further answer RQ3, we investigate the parameter sensitivity of
FairINV w.r.t. two hyperparameters, i.e., the balanced parameter 𝛼
and the learning rate 𝑙𝑟𝑠𝑝of SAP. Notably, the setting of 𝑙𝑟𝑠𝑝benefits
from the independent training of SAP. We vary 𝛼and𝑙𝑟𝑠𝑝within
the range of{0.001, 0.01, 0.1, 0.5, 1, 10, 100 }. We only illustrate
results on the Bail and Pokec-z datasets due to similar observations
on other datasets. We observe that, with a wide range of variations
in two parameters, the performance of FairINV remains stable.
However, a sharp decline in both utility and fairness performance
is noted when the value of 𝛼is less than 0.01.
5.6 Training Time Comparison
To further investigate the computational cost of FairINV compared
to baseline methods, we conduct a training time comparison ex-
periment across all datasets. Specifically, we repeat each method
five times and record the total training time. We set 𝑘to 1 and
3 to implement two variants of FairINV, namely FairINV-1, and
FairINV-3, representing FairINV trained for the single sensitive
attribute and three-sensitive attribute scenarios, respectively. As
shown in Table 4, FairINV exhibits lower computational costs on
4695One Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Results of various sensitive attributes. The results in which FairINV exhibits inferior performance compared to vanilla
GCN are highlighted with a gray background. Sens.Attr.: Sensitive Attribute
Datasets V
anilla GCN FairINV V
anilla GCN FairINV V
anilla GCN FairINV V
anilla GCN FairINV
Sens.Attr
. A
ge Gender ForeignWorker Single
AUC 65.90 ±0.83
69.11 ±1.80 65.90 ±0.83
69.11 ±1.80 65.90 ±0.83
69.11 ±1.80 65.90 ±0.83
69.11 ±1.80
F1 77.32 ±1.20
82.36 ±0.35 77.32 ±1.20
82.36 ±0.35 77.32 ±1.20
82.36 ±0.35 77.32 ±1.20
82.36 ±0.35
Δ𝐷
𝑃(↓) 20.18 ±5.17
0.48±0.38 36.29 ±4.64
0.76±1.24 8.45±7.05
2.31±3.81 34.13 ±3.29
2.64±4.15German
Δ𝐸
𝑂(↓) 15.83 ±3.47
0.46±0.52 31.35 ±4.39
0.15±0.29 5.66±2.61
2.34±4.09 27.26 ±4.96
1.48±2.96
Sens.Attr
. Race
Gender MARRIED WORKREL
AUC 87.13 ±0.31
88.53 ±1.83 87.13 ±0.32
88.53 ±1.83 87.13 ±0.33
88.53 ±1.83 87.13 ±0.34
88.53 ±1.83
F1 78.98 ±0.67 78.80 ±3.71 78.98 ±0.68 78.80 ±3.71 78.98 ±0.69 78.80 ±3.71 78.98 ±0.70 78.80 ±3.71
Δ𝐷
𝑃(↓) 9.18±0.59
3.58±1.61 11.51 ±0.22 12.09 ±5.58 2.36±0.54 3.40±1.23 0.34±0.09 0.55±0.47Bail
Δ𝐸
𝑂(↓) 4.43±0.37
2.15±1.24 1.95±0.22 3.53±2.12 3.13±0.52 4.81±2.34 1.25±0.28
1.18±0.67
Sens.Attr
. Gender
Region Age Hair color indicator
AUC 76.42 ±0.13 75.79 ±0.08 76.42 ±0.13 75.79 ±0.08 76.42 ±0.13 75.79 ±0.08 76.42 ±0.13 75.79 ±0.08
F1 70.32 ±0.20
70.78 ±0.50 70.32 ±0.20
70.78 ±0.50 70.32 ±0.20
70.78 ±0.5 70.32 ±0.20
70.78 ±0.50
Δ𝐷
𝑃(↓) 3.15±0.24
2.38±1.03 3.91±0.35
2.70±0.96 33.09 ±0.57
27.49 ±3.29 18.70 ±0.9
15.63 ±1.09Pokec-z
Δ𝐸
𝑂(↓) 5.25±0.48
5.08±0.98 4.59±0.34
2.23±0.66 36.32 ±0.7
29.19 ±3.68 18.73 ±0.8
14.31 ±1.76
Sens.Attr
. Gender
Region Age Hair color indicator
AUC 73.87 ±0.08 73.55 ±0.16 73.87 ±0.08 73.55 ±0.16 73.87 ±0.08 73.55 ±0.16 73.87 ±0.08 73.55 ±0.16
F1 65.55 ±0.13 65.19 ±0.62 65.55 ±0.13 65.19 ±0.62 65.55 ±0.13 65.19 ±0.62 65.55 ±0.13 65.19 ±0.62
Δ𝐷
𝑃(↓) 6.36±0.20 6.44±0.70 2.83±0.46
1.24±0.64 40.05 ±0.73 40.11 ±0.83 14.96 ±0.68
12.8±1.90Pokec-n
Δ𝐸
𝑂(↓) 13.18 ±0.41
13.13 ±1.26 3.66±0.43
2.80±0.78 42.82 ±0.60 43.53 ±0.50 12.49 ±0.51
12.3±1.65
lrsp/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014
/uni00000013/uni00000011/uni00000018
/uni00000014/uni00000011/uni00000013
/uni00000018/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000017/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000019/uni00000011/uni00000013/uni00000013/uni0000001b/uni0000001b/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000013/uni00000011/uni00000013/uni00000013
(
a) AUC performance
lrsp/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013DP/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000015/uni00000011/uni00000013/uni00000013/uni00000015/uni00000011/uni0000001b/uni00000013/uni00000016/uni00000011/uni00000019/uni00000013/uni00000017/uni00000011/uni00000017/uni00000013/uni00000018/uni00000011/uni00000015/uni00000013/uni00000019/uni00000011/uni00000013/uni00000013 (
b)Δ𝐷𝑃performance
Figure 5: Parameters sensitivity analysis on Bail.
lrsp/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000019/uni00000013/uni00000011/uni00000013/uni00000013/uni00000019/uni00000016/uni00000011/uni00000019/uni00000013/uni00000019/uni0000001a/uni00000011/uni00000015/uni00000013/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni0000001a/uni00000017/uni00000011/uni00000017/uni00000013/uni0000001a/uni0000001b/uni00000011/uni00000013/uni00000013
(
a) AUC performance
lrsp/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013DP/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000014/uni00000011/uni00000019/uni00000013/uni00000015/uni00000011/uni00000017/uni00000013/uni00000016/uni00000011/uni00000015/uni00000013/uni00000017/uni00000011/uni00000013/uni00000013 (
b)Δ𝐷𝑃performance
Figure 6: Parameters sensitivity analysis on Pokec-z.
larger datasets compared to baseline methods. Although FairINV-3
requires more training time, it trains fair GNN models toward three
sensitive attributes, which are unreachable for baseline methods.
Furthermore, we also observe that the training time of FairINV-3 is
significantly longer than that of FairINV-1. A possible explanation
for this phenomenon is the high computational cost associated withTable 4: Comparison of training time for both baseline meth-
ods and FairINV.
Datasets EDI
TS NIFTY FairGNN FairVGNN FairINV-1 FairINV-3
German 82.88
s 94.91 s 50.42 s 539.54 s 56.20 s 114.97 s
Bail 338.28
s 106.22 s 373.54 s 988.19 s 92.60 s 200.71 s
Pokec-z OOM
139.83 s 302.11 s 963.39 s 113.38 s 210.29 s
Pokec-n OOM
132.03 s 261.73 s 1072.48 s 102.33 s 191.87 s
NBA 81.50
s 92.08 s 46.65 s 534.02 s 57.64 s 112.75 s
the SAP module. Overall, the experimental results demonstrate that
FairINV has lower computational costs than the baseline methods.
6 CONCLUSION
In this work, we investigate the universal fairness problem, i.e.,
training a fair GNN toward various sensitive attributes in a single
training session. To address this problem, we first formulate such
a problem from a graph invariant learning point of view. Then,
we propose a universal graph fairness approach, namely, FairINV.
The core idea behind FairINV is to eliminate spurious correlations
between the sensitive attributes and labels in a graph variant learn-
ing way. Experiments on several real-world datasets validate the
effectiveness of FairINV in both fairness and utility performance.
We leave validation on other downstream tasks, e.g., edge-level, as
future works. In addition, due to FairINV only focusing on group
fairness, future works will focus on considering fine-grained fair-
ness, e.g., individual fairness.
7 ACKNOWLEDGEMENTS
The research is supported by the National Key R&D Program of
China under grant No. 2022YFF0902500, the Guangdong Basic and
Applied Basic Research Foundation, China (No. 2023A1515011050),
Shenzhen Science and Technology Program (KJZD20231023094501003),
and Tencent AI Lab (RBFR2024004).
4696KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
REFERENCES
[1]Robert Adragna, Elliot Creager, David Madras, and Richard Zemel. 2020. Fairness
and robustness in invariant learning: A case study in toxicity classification. arXiv
preprint arXiv:2011.06485 (2020).
[2]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uni-
fied framework for fair and stable graph representation learning. In Uncertainty
in Artificial Intelligence. PMLR, 2114–2124.
[3]Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar.
2020. Invariant risk minimization games. In International Conference on Machine
Learning. PMLR, 145–155.
[4]Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[5] Arthur Asuncion and David Newman. 2007. UCI machine learning repository.
[6]Avishek Bose and William Hamilton. 2019. Compositional fairness constraints
for graph embeddings. In International Conference on Machine Learning. PMLR,
715–724.
[7]Junyi Chai, Taeuk Jang, and Xiaoqian Wang. 2022. Fairness without demograph-
ics through knowledge distillation. Advances in Neural Information Processing
Systems 35 (2022), 19152–19164.
[8]Yimeng Chen, Ruibin Xiong, Zhi-Ming Ma, and Yanyan Lan. 2022. When Does
Group Invariant Learning Survive Spurious Correlations? Advances in Neural
Information Processing Systems 35 (2022), 7038–7051.
[9]Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui
Xie, Tongliang Liu, Bo Han, and James Cheng. 2022. Learning causally invariant
representations for out-of-distribution generalization on graphs. Advances in
Neural Information Processing Systems 35 (2022), 22131–22148.
[10] Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. 2021. Environment
inference for invariant learning. In International Conference on Machine Learning.
PMLR, 2189–2200.
[11] Sean Current, Yuntian He, Saket Gurukar, and Srinivasan Parthasarathy. 2022.
Fairmod: Fair link prediction and recommendation via graph modification. arXiv
preprint arXiv:2201.11596 (2022).
[12] Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fair
graph neural networks with limited sensitive attribute information. In Proceedings
of the 14th ACM International Conference on Web Search and Data Mining . 680–
688.
[13] Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021. Individual
fairness for graph neural networks: A ranking based approach. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
300–310.
[14] Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modeling
and mitigating data bias for graph neural networks. In Proceedings of the ACM
Web Conference 2022. 1259–1269.
[15] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.
Enhancing graph neural network-based fraud detectors against camouflaged
fraudsters. In Proceedings of the 29th ACM international conference on information
& knowledge management. 315–324.
[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214–226.
[17] Wei Fan, Kunpeng Liu, Rui Xie, Hao Liu, Hui Xiong, and Yanjie Fu. 2021. Fair
graph auto-encoder for unbiased graph representations with wasserstein distance.
In2021 IEEE International Conference on Data Mining (ICDM). IEEE, 1054–1059.
[18] Dandan Guo, Chaojie Wang, Baoxiang Wang, and Hongyuan Zha. 2022. Learning
Fair Representations via Distance Correlation Minimization. IEEE Transactions
on Neural Networks and Learning Systems (2022).
[19] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[20] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. Advances in neural information processing systems 29 (2016).
[21] Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and
Xia Hu. 2023. Chasing Fairness in Graphs: A GNN Architecture Perspective.
arXiv preprint arXiv:2312.12369 (2023).
[22] Kareem L Jordan and Tina L Freiburger. 2015. The effect of race/ethnicity on
sentencing: Examining sentence type, jail length, and prison length. Journal of
Ethnicity in Criminal Justice 13, 3 (2015), 179–196.
[23] Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. 2020. Inform:
Individual fairness on graph mining. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining. 379–389.
[24] Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gum-
madi, Adrian Weller, and Baharan Mirzasoleiman. 2022. Crosswalk: Fairness-
enhanced node representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 11963–11970.
[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).[26] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain,
Xuezhi Wang, and Ed Chi. 2020. Fairness without demographics through adver-
sarially reweighted learning. Advances in neural information processing systems
33 (2020), 728–740.
[27] Jure Leskovec and Julian Mcauley. 2012. Learning to discover social circles in
ego networks. Advances in neural information processing systems 25 (2012).
[28] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2022. Learning invariant
graph representations for out-of-distribution generalization. Advances in Neural
Information Processing Systems 35 (2022), 11828–11841.
[29] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu,
Changhua Meng, Zibin Zheng, and Weiqiang Wang. 2023. What’s Behind the
Mask: Understanding Masked Graph Modeling for Graph Autoencoders. In KDD.
ACM, 1268–1279.
[30] Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, Changhua
Meng, Zibin Zheng, and Liang Chen. 2024. A Graph is Worth 1-bit Spikes: When
Graph Contrastive Learning Meets Spiking Neural Networks. In ICLR.
[31] Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. 2021. On dyadic
fairness: Exploring and mitigating bias in graph connections. In International
Conference on Learning Representations.
[32] Yanying Li, Xiuling Wang, Yue Ning, and Hui Wang. 2022. Fairlp: Towards fair
link prediction on social network graphs. In Proceedings of the International AAAI
Conference on Web and Social Media, Vol. 16. 628–639.
[33] Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na Zou. 2022. Learn-
ing fair graph representations via automated data augmentations. In The Eleventh
International Conference on Learning Representations.
[34] Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing
He. 2023. FLOOD: A flexible invariant learning framework for out-of-distribution
generalization on graphs. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1548–1558.
[35] Jiali Ma, Zhongqi Yue, Kagaya Tomoyuki, Suzuki Tomoki, Karlekar Jayashree,
Sugiri Pranata, and Hanwang Zhang. 2023. Invariant Feature Regularization for
Fair Face Recognition. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 20861–20870.
[36] Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. 2016. Causal inference
by using invariant prediction: identification and confidence intervals. Journal of
the Royal Statistical Society Series B: Statistical Methodology 78, 5 (2016), 947–1012.
[37] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal
inference: foundations and learning algorithms. The MIT Press.
[38] Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters. 2018.
Invariant models for causal transfer learning. The Journal of Machine Learning
Research 19, 1 (2018), 1309–1342.
[39] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[40] Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini. 2021.
Fairdrop: Biased edge dropout for enhancing fairness in graph representation
learning. IEEE Transactions on Artificial Intelligence 3, 3 (2021), 344–354.
[41] Lubos Takac and Michal Zabovsky. 2012. Data analysis in public social networks.
InInternational scientific conference and international workshop present day trends
of innovations, Vol. 1.
[42] Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu Yinghui, Peng Cui,
and Yuan Qi. 2023. Provably invariant learning without domain information. In
International Conference on Machine Learning. PMLR, 33563–33580.
[43] Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and Tyler
Derr. 2022. Improving fairness in graph neural networks via mitigating sensi-
tive attribute leakage. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1938–1948.
[44] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. 2022. Handling distribu-
tion shifts on graphs: An invariance perspective. arXiv preprint arXiv:2202.02466
(2022).
[45] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural
networks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1–37.
[46] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[47] Moyi Yang, Junjie Sheng, Wenyan Liu, Bo Jin, Xiaoling Wang, and Xiangfeng
Wang. 2022. Obtaining Dyadic Fairness by Optimal Transport. In 2022 IEEE
International Conference on Big Data (Big Data). IEEE, 4726–4732.
[48] Yuchang Zhu, Jintang Li, Liang Chen, and Zibin Zheng. 2024. The Devil is in the
Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation.
InProceedings of the 17th ACM International Conference on Web Search and Data
Mining. 1012–1021.
[49] Yuchang Zhu, Jintang Li, Liang Chen, and Zibin Zheng. 2024. FairAGG: To-
ward Fair Graph Neural Networks via Fair Aggregation. IEEE Transactions on
Computational Social Systems (2024).
[50] Yuchang Zhu, Jintang Li, Zibin Zheng, and Liang Chen. 2024. Fair Graph Repre-
sentation Learning via Sensitive Attribute Disentanglement. In Proceedings of the
ACM on Web Conference 2024. 1182–1192.
4697One Fits All: Learning Fair Graph Neural Networks
for Various Sensitive Attributes KDD ’24, August 25–29, 2024, Barcelona, Spain
A COMPARISON FOR VARIOUS GNN
BACKBONES
To further investigate the generalizability of FairINV across vari-
ous GNN backbones, we conduct comparative experiments using
GIN [ 46] and GraphSAGE [ 19] backbones. As shown in Tables 5
and 6, we compare FairINV with three fairness baseline methods,
including NIFTY [ 2], FairGNN [ 12], and FairVGNN [ 43]. From these
two tables, we can observe that FairINV consistently outperforms
the three fairness baseline methods in most cases. Furthermore,
upon summarizing the comparison results across the three back-
bones, we find that most fairness methods, including FairINV, con-
sistently enhance both utility and fairness performance on the Bail
dataset. This observation suggests an underlying relationship be-
tween fairness and utility in the Bail dataset, providing a promising
avenue for future research.B COMPARISON FOR MULTI-SENSITIVE
ATTRIBUTES
We further present a comparison of FairINV and two baseline meth-
ods in various sensitive attribute scenarios, as shown in Table 7.
Due to the single sensitive attribute setting of these two methods,
it is necessary to extend them by modifying the optimization ob-
jectives. For NIFTY [ 2], we simultaneously flap various sensitive
attributes to construct the counterfactual graph. For FairGNN [ 12],
we train multiple sensitive attribute estimators and discriminators
simultaneously. Although existing methods can be extended to
multi-sensitive attribute scenarios, their performance might be neg-
atively affected since they are not explicitly designed for multiple
sensitive attributes. From Table 7, we can observe that FairINV is
the only method that can achieve fairness and maintain utility. In
most cases, FairINV’s fairness performance is better than baseline
methods.
4698KDD ’24, August 25–29, 2024, Barcelona, Spain Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen
Table 5: Comparison results of FairINV and baseline fairness methods on GIN backbone. In each row, the best result is indicated
in bold, while the runner-up result is marked with an underline .
Datasets Metrics V
anilla GIN NIFTY FairGNN FairVGNN FairIN
V
GermanAUC 71.86 ±1.55
66.70 ±4.91 72.78 ±1.19 69.23 ±3.08 70.08 ±2.17
F1 82.35 ±0.55
80.33 ±3.76 81.48 ±1.55 82.41 ±0.62 82.57 ±0.22
Δ𝐷
𝑃(↓) 14.92 ±5.52
5.28±6.67 15.63 ±5.2 2.71±3.16 1.02 ±1.17
Δ𝐸
𝑂(↓) 8.24±6.31
7.39±8.49 10.0 ±5.51 0.91±1.41 0.17 ±0.34
BailAUC 75.69 ±7.75
79.49 ±6.65 83.96 ±0.61 86.33 ±1.05 86.05 ±0.81
F1 64.26 ±8.73
65.20 ±11.22 73.10 ±1.28 87.47 ±0.50 75.66 ±3.01
Δ𝐷
𝑃(↓) 8.44±2.94 5.38 ±1.16 8.93±1.63 6.95±0.41 7.35±1.71
Δ𝐸
𝑂(↓) 6.57±1.36 4.00 ±2.21 6.65±1.77
6.97±1.18 4.80±1.21
Poke
c-zAUC 75.04 ±0.39 72.52 ±2.66
74.70 ±1.21 74.51 ±0.12 74.90 ±1.28
F1 68.45 ±1.23
67.93 ±1.26 67.30 ±0.77 69.70 ±0.57 67.47 ±1.98
Δ𝐷
𝑃(↓) 3.24±2.09
3.56±2.95 3.96 ±1.47 1.93±1.23 1.66 ±1.16
Δ𝐸
𝑂(↓) 4.26±2.27
3.51±2.20 5.22 ±1.51 2.71±1.20 2.06 ±0.89
Poke
c-nAUC 74.06 ±0.62
72.12 ±1.65 73.25 ±1.04 72.71 ±0.48 74.39 ±0.48
F1 62.39 ±0.51
60.25 ±4.53 60.88 ±2.99 65.56 ±1.03 62.09 ±2.37
Δ𝐷
𝑃(↓) 2.64±1.28
3.34±1.78 2.25±1.33
6.13±1.59 1.37 ±0.91
Δ𝐸
𝑂(↓) 6.77±2.36
6.88±2.11 2.68±1.59
7.00±1.80 2.03 ±2.04
Table 6: Comparison results of FairINV and baseline fairness methods on GraphSAGE backbone. In each row, the best result is
indicated in bold, while the runner-up result is marked with an underline .
Datasets Metrics V
anilla GraphSAGE NIFTY FairGNN FairVGNN FairIN
V
GermanAUC 74.41 ±0.80
68.45 ±3.8 75.25 ±0.9 73.79 ±1.67 73.64 ±2.88
F1 80.74 ±1.81
77.35 ±0.69 79.45 ±2.69 82.20 ±0.48 82.49 ±0.23
Δ𝐷
𝑃(↓) 26.89 ±6.23
5.93±7.03 27.45 ±4.59 2.98±2.75 0.34 ±0.58
Δ𝐸
𝑂(↓) 18.36 ±6.91
5.27±4.02 20.21 ±4.48 1.38±0.89 0.17 ±0.34
BailAUC 90.79 ±1.14
91.18 ±1.32 91.48 ±0.28 92.01 ±0.68 91.85 ±0.43
F1 80.82 ±1.81
80.54 ±1.52 81.41 ±0.54 83.85 ±1.15 81.59 ±1.66
Δ𝐷
𝑃(↓) 2.45±1.31 6.19 ±1.64 1.52±0.85 3.00±1.55 0.49 ±0.43
Δ𝐸
𝑂(↓) 1.77±0.68 4.75 ±1.62 1.44±0.84 1.48±1.34 0.66 ±0.49
Poke
c-zAUC 78.69 ±0.44 77.05 ±0.53
77.86 ±0.93 78.67 ±0.57 78.22 ±0.68
F1 70.54 ±1.26
65.19 ±3.18 68.83 ±3.88 72.78 ±0.73 70.48 ±2.32
Δ𝐷
𝑃(↓) 4.99±1.41 3.65±0.94
5.48±1.15 3.08 ±1.73 3.76 ±1.29
Δ𝐸
𝑂(↓) 5.17±1.68
3.87±1.21 5.61 ±1.48 3.85±1.90 3.46 ±0.99
Poke
c-nAUC 75.99 ±0.39
72.31 ±1.67 75.12 ±1.03 75.22 ±0.63 76.08 ±0.31
F1 63.03 ±1.49
61.73 ±1.4 64.84 ±1.86 65.91 ±1.43 66.22 ±1.61
Δ𝐷
𝑃(↓) 1.02±0.67
6.66±1.40 1.93±1.14
4.94±1.91 0.99 ±0.90
Δ𝐸
𝑂(↓) 2.65±1.20
9.16±1.86 2.6±2.19
6.26±2.71 1.49 ±1.11
Table 7: Comparison of FairINV and baseline methods in various sensitive attributes scenarios. Sens.Attr.: Sensitive Attribute.
Datesets V
anilla GCN NIFTY FairGNN FairINV V
anilla GCN NIFTY FairGNN FairINV V
anilla GCN NIFTY FairGNN FairINV
Sens.Attr
. A
ge Gender Single
GermanAUC 65.90 ±0.83
49.26 ±6.68 75.13 ±0.84 69.11 ±1.80 65.90 ±0.83
55.87 ±8.19 75.69 ±0.60 69.11 ±1.80 65.90 ±0.83
56.08 ±8.46 75.69 ±0.60 69.11 ±1.80
F1 77.32 ±1.20
82.0±0.71 76.83 ±3.09 82.36 ±0.35 77.32 ±1.20
81.88 ±0.39 77.91 ±5.16 82.36 ±0.35 77.32 ±1.20
82.03 ±0.37 78.17 ±5.15 82.36 ±0.35
Δ𝐷
𝑃(↓) 20.18 ±5.17
0.13±0.25 20.67 ±3.40 0.48 ±0.38 36.29 ±4.64
1.75±1.82 36.07 ±5.75 0.76 ±1.24 34.13 ±3.29
1.19±1.71 33.37 ±7.83 2.64 ±4.15
Δ𝐸
𝑂(↓) 15.83 ±3.47
0.34±0.67 18.24 ±3.95 0.46 ±0.52 31.35 ±4.39
1.53±0.93 27.84 ±5.71 0.15 ±0.29 27.26 ±4.96
0.61±0.69 25.91 ±7.93 1.48 ±2.96
Sens.Attr
. Hair
color indicator Region AGE
Pokec-zAUC 76.42 ±0.13
74.04 ±0.46 70.39 ±1.07 75.79 ±0.08 76.42 ±0.13
74.04 ±0.46 70.25 ±0.89 75.79 ±0.08 76.42 ±0.13
74.04 ±0.46 70.31 ±1.07 75.79 ±0.08
F1 70.32 ±0.20
69.90 ±0.34 43.73 ±24.28 70.78 ±0.50 70.32 ±0.20
69.9±0.34 41.98 ±23.68 70.78 ±0.5 70.32 ±0.20
69.90 ±0.34 41.34 ±28.04 70.78 ±0.50
Δ𝐷
𝑃(↓) 18.7±0.90
17.29 ±7.62 10.80 ±7.58 15.63 ±1.09 3.91±0.35
8.15±3.28 2.80 ±1.11 2.70 ±0.96 33.09 ±0.57
35.71 ±13.32 22.08 ±18.8 27.49 ±3.29
Δ𝐸
𝑂(↓) 18.73 ±0.80
14.81 ±6.47 10.19 ±7.21 14.31 ±1.76 4.59±0.34
7.57±2.93 3.45 ±1.38 2.23 ±0.66 36.32 ±0.70
33.68 ±13.4 23.42 ±19.31 29.19 ±3.68
4699