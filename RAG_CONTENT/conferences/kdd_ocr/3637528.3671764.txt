Effective Clustering on Large Attributed Bipartite Graphs
Renchi Yang
Hong Kong Baptist University
Hong Kong SAR, China
renchi@hkbu.edu.hkYidu Wu∗
Chinese University of Hong Kong
Hong Kong SAR, China
yiduwu@cuhk.edu.hkXiaoyang Lin
Hong Kong Baptist University
Hong Kong SAR, China
csxylin@hkbu.edu.hk
Qichen Wang
Hong Kong Baptist University
Hong Kong SAR, China
qcwang@hkbu.edu.hkTsz Nam Chan
Shenzhen University
Shenzhen, China
edisonchan@szu.edu.cnJieming Shi
Hong Kong Polytechnic University
Hong Kong SAR, China
jieming.shi@polyu.edu.hk
Abstract
Attributed bipartite graphs (ABGs) are an expressive data model
for describing the interactions between two sets of heterogeneous
nodes that are associated with rich attributes, such as customer-
product purchase networks and author-paper authorship graphs.
Partitioning the target node set in such graphs into 𝑘disjoint clus-
ters (referred to as 𝑘-ABGC) finds widespread use in various do-
mains, including social network analysis, recommendation systems,
information retrieval, and bioinformatics. However, the majority
of existing solutions towards 𝑘-ABGC either overlook attribute
information or fail to capture bipartite graph structures accurately,
engendering severely compromised result quality. The severity of
these issues are accentuated in real ABGs, which often encompass
millions of nodes and a sheer volume of attribute data, rendering
effective𝑘-ABGC over such graphs highly challenging.
In this paper, we propose TPO, an effective and efficient approach
to𝑘-ABGC that achieves superb clustering performance on multiple
real datasets. TPOobtains high clustering quality through two major
contributions: (i) a novel formulation and transformation of the 𝑘-
ABGC problem based on multi-scale attribute affinity specialized for
capturing attribute affinities between nodes with the consideration
of their multi-hop connections in ABGs, and (ii) a highly efficient
solver that includes a suite of carefully-crafted optimizations for
sidestepping explicit affinity matrix construction and facilitating
faster convergence. Extensive experiments, comparing TPOagainst
19 baselines over 5 real ABGs, showcase the superior clustering
quality of TPOmeasured against ground-truth labels. Moreover,
compared to the state of the arts, TPOis often more than 40×faster
over both small and large ABGs.
CCS Concepts
•Mathematics of computing →Computations on matrices ;•
Computing methodologies →Cluster analysis; Spectral meth-
ods; •Information systems →Clustering.
∗Work done while at Hong Kong Baptist University.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671764Keywords
clustering, bipartite graphs, attributes, eigenvector
ACM Reference Format:
Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, and Jiem-
ing Shi. 2024. Effective Clustering on Large Attributed Bipartite Graphs. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671764
1 Introduction
Bipartite graphs are an indispensable data structure used to model
the interplay between two sets of entities from heterogeneous
sources, e.g., author-publication associations, customer-merchant
transactions, query-webpage pairing, and various user-item interac-
tions on social media, e-commerce platforms, search engines, etc. In
the real world, such graphs are often associated with rich attributes,
e.g., the user profile in social networks, web page content in web
graphs, hallmarks of pathways in cancer signaling networks, and
paper keywords in academic graphs, which are termed Attributed
Bipartite Graphs (hereinafter ABGs).
Given an ABGGwith two disjoint node sets UandV,𝑘-
Attributed Bipartite Graph Clustering (𝑘-ABGC), a fundamental task
of analyzing ABGs, seeks to partition the nodes in the node set of in-
terest, e.g.,UorV, into𝑘non-overlapping clusters C1,C2,···,C𝑘,
such that nodes within the same cluster C𝑖are close to each other
in terms of both their attribute similarity and topological prox-
imity inG. Due to the omnipresence of ABGs, 𝑘-ABGC has seen
a wide range of practical applications in social network analy-
sis, recommender systems, information retrieval, and bioinformat-
ics, such as user/content tagging [ 45,81], market basket analysis
[83,84], document categorization [ 8,59], identification of protein
complexes, disease genes, and drug targets [ 46,64], and many oth-
ers [26, 34, 50, 67, 69].
As reviewed in Section 5, existing solutions towards 𝑘-ABGC
primarily rely on bipartite graph co-clustering (BGCC), attributed
graph clustering (AGC), and attributed network embedding (ANE)
techniques. Amid them, BGCC has been extensively investigated
in the literature [ 2,8,9,28,29,63] for clustering non-attributed
bipartite graphs, whose basic idea is to simultaneously group nodes
inUandVmerely based on their interactions in G, instead of
clustering them severally. As pinpointed in prior works [ 4], the
attributes present rich information to characterize the properties
of nodes and hence, can complement scant topological information
for better node clustering. Consequently, BGCC methods exhibit
3782
KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
subpar performance on ABGs as they overlook such information.
To leverage the complementary nature of graph topology and
attributes for enhanced clustering effectiveness, considerable efforts
[4,6,12,30,77,85] have been invested in recent years towards
devising effective AGC models and algorithms. Although these
approaches enjoy improved performance over unipartite attributed
graphs by fusing graph connectivity and attribute information of
nodes via deep learning or sophisticated statistical models, they are
sub-optimal for ABGs.
Over the past decade, network embedding has emerged as a
popular and powerful tool for analyzing graph-structured data, es-
pecially those with nodal attributes. Notwithstanding a plethora of
network embedding techniques invented [7, 14, 75], most of them
are designed for unipartite graphs. To capture the unique character-
istics of bipartite graphs, Huang et al . [23] extend node2vec [18] to
ABGs, at the expense of tremendous training overhead. Adopting
this category of approaches for 𝑘-ABGC requires a rear-mounted
phase (e.g., 𝑘-Means) to cluster the node embeddings, which is
not cost-effective given the high embedding dimensions (typically
128). To summarize, existing approaches to 𝑘-ABGC either dilute
clustering quality due to inadequate exploitation of attributes and
bipartite graph topology, or incur vast computation costs, especially
on sizable ABGs encompassing thousands of attributes, millions of
nodes, and billions of edges.
In response to these challenges, we propose TPO, a novel Three-
Phase Optimization framework for 𝑘-ABGC that significantly ad-
vances the state of the art in 𝑘-ABGC, in terms of both result ef-
fectiveness and computation efficiency. First and foremost, TPO
formulates the 𝑘-ABGC task as an optimization problem based on
multi-scale attribute affinity (MSA), a new node affinity measure
dedicated to ABGs. More concretely, the MSA of two homoge-
neous nodes 𝑢𝑖,𝑢𝑗inUof ABGGevaluates the similarity of their
attributes aggregated from multi-hop neighborhoods, which effec-
tively captures the affinity of nodes with consideration of both their
attributes and topological connections in bipartite graphs. How-
ever, calculating the MSA of all node pairs in Gfor clustering is
prohibitively expensive for large graphs, as it entails colossal con-
struction time and space consumption ( 𝑂(|U|2)). On top of that,
the exact optimization of our 𝑘-ABGC objective is also infeasible
as an aftermath of its NP-hardness.
To tackle these issues, TPOadopts a three-phase optimization
scheme for an approximate solution with time and space costs lin-
ear to the size ofG. Under the hood, similar in spirit to kernel tricks
[36],TPOfirst leverages a mathematical apparatus, random features
[49,82], to bypass the materialization of the all-pairwise MSA. The
clustering task is later framed as a non-negative matrix factoriza-
tion, followed by a matrix approximation problem, based on our
theoretically-grounded problem transformation. Particularly, the
former attends to yielding an intermediate, while the latter itera-
tively refines the intermediary result to derive the eventual clusters.
In addition to the linear-time iterative solvers, TPOfurther includes
a greedy initialization trick for speeding up the convergence, and
an attribute dimension reduction approach to conspicuously boost
the practical efficiency of TPOover graphs with large attribute sets,
without degrading result quality. Our empirical studies, which in-
volved 5 real ABGs and compared against 19 existing algorithms,
demonstrate that TPOconsistently attains superior or comparableTable 1: Frequently used notations.
Notation Description
U,V,E The node setsU,V, and the edge setEof ABGG.
XU,XV Attribute vectors of nodes in UandV.
𝑑U,𝑑V Attribute dimensions of nodes in UandV.
𝑤(𝑢𝑖,𝑣𝑗)Weight of edge(𝑢𝑖,𝑣𝑗)inE.
𝑘 The number of clusters.
𝛼 Balance coefficient used in Eq. (3).
𝛾 Maximum number of iterations used in Eq. (31).
𝑑 Dimension of X′
Uin Eq. (15) (𝑑≤𝑑U).
𝑇𝑓,𝑇𝑔 Maximum number of iterations used in Algorithms 2
and 3, respectively.
LU Normalized adjacency matrix defined in Eq. (7).
ZU,bZU Feature vectors of nodes in Uand their normalized
version defined in Eq. (6) and Eq. (2), respectively.
𝑠(𝑢𝑖,𝑢𝑗) The MSA between nodes 𝑢𝑖and𝑢𝑗defined in Eq. (1).
R Matrix satisfies R[𝑖]·R[𝑗]≈𝑠(𝑢𝑖,𝑢𝑗).
Y The NCI matrix defined in Eq. (10).
𝚼 The continuous version of Ysatisfying Eq. (11).
clustering quality at a fraction of the cost compared to the state-of-
the-art methods. For instance, on the largest Amazon dataset with
over 10 million nodes and 22 million edges, TPOobtains the best
clustering accuracy within 3 minutes, whereas the state-of-the-art
demands more than 4 hours to terminate.
2 Problem Formulation
2.1 Notation and Terminology
We denote matrices using bold uppercase letters, e.g., M∈R𝑛×𝑑,
and the𝑖-th row (resp. the 𝑗-th column) of Mis represented as M[𝑖]
(resp. M[:,𝑗]). Accordingly, M[𝑖,𝑗]signifies the entry at the 𝑖-th
row and𝑗-th column of M. For each vector M[𝑖], we use∥M[𝑖]∥to
represent its 𝐿2norm and∥M∥𝐹to represent the Frobenius norm
ofM.
LetG=(U∪V,E,XU,XV)symbolize an attributed bipartite
graph (ABG), whereEis composed of edges connecting nodes in
two disjoint node sets UandVand each edge(𝑢𝑖,𝑣𝑗)is associated
with an edge weight 𝑤(𝑢𝑖,𝑣𝑗). Each node 𝑢𝑖∈U (resp.𝑣𝑖∈V) of
Gis characterized by a length- 𝑑U(resp. length- 𝑑V) attribute vec-
torXU[𝑖](resp. XV[𝑖]). Further, we denote by BU∈R|U|×|V|
the adjacency matrix of Gfrom the perspective of U, in which
BU[𝑖,𝑗]=𝑤(𝑢𝑖,𝑣𝑗)if(𝑢𝑖,𝑣𝑗)∈E and 0 otherwise. Let DU(resp.
DV) be a|U|×|U| (resp.|V|×|V| ) diagonal matrix wherein the
diagonal entry DU[𝑖,𝑖](resp. DV[𝑖,𝑖]) stands for the sum of the
weights of edges incident to 𝑢𝑖(resp.𝑣𝑖), i.e.,Í
(𝑢𝑖,𝑣ℓ)∈E𝑤(𝑢𝑖,𝑣ℓ)
(resp.Í
(𝑢ℓ,𝑣𝑖)∈E𝑤(𝑢ℓ,𝑣𝑖)). Table 1 lists the frequently used nota-
tions throughout the paper.
The overarching goal of 𝑘-ABGC is formalized in Definition 2.1
and exemplified in Figure 1. Note that by default, we regard Uas
the target node set to cluster. The number 𝑘can be specified by
users or configured by a preliminary procedure [41].
Definition 2.1 ( 𝑘-Attributed Bipartite Graph Clustering ( 𝑘-ABGC)).
Given an ABGG, the target node set U, and the number 𝑘of clus-
ters,𝑘-ABGC aims to partition node set Uinto𝑘disjoint clusters
{C1,C2,···,C𝑘}such that nodes within the same cluster are close
to each other in terms of both topological proximity and attribute
3783Effective Clustering on Large Attributed Bipartite Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain
𝒞1 𝒞2 𝒞3
𝑢1𝑢2 𝑢3𝑢4𝑢5 𝑢6𝑢7
𝑣1𝑣2 𝑣3𝑣4𝑣5 𝑣6𝑣7𝑣8𝒰
𝒱ℰ𝐗𝒰
Figure 1: An Illustrative Example of 𝑘-ABGC
similarity while nodes across diverse clusters are distant.
2.2 Multi-Scale Attribute Affinity (MSA)
Notice that Definition 2.1 cannot directly guide the generation of
clusters, as it lacks a concrete optimization objective that quantifies
node affinities. To this end, we first delineate our novel affinity mea-
sure MSA for nodes in terms of both graph structure and attributes,
before formally introducing our objective in Section 2.3.
MSA formulation. We first assume that each node 𝑢𝑖∈U can be
represented by a feature vector ZU[𝑖], which characterizes both
the attributes as well as the rich semantics hidden in the bipartite
graph topology. Following the popular Skip-gram model [ 40] and
its extension to graphs [ 18,47], we can model pair-wise affinity of
nodes as a softmax unit [ 16] parametrized by a dot product of their
feature vectors. Rather than using the vanilla softmax function,
we adopt a symmetric softmax function and formulate the MSA
𝑠(𝑢𝑖,𝑢𝑗)between any two nodes 𝑢𝑖,𝑢𝑗inUas follows:
𝑠(𝑢𝑖,𝑢𝑗)=𝑒bZU[𝑖]·bZU[𝑗]
√︃Í
𝑢ℓ∈U𝑒bZU[𝑖]·bZU[ℓ]·√︃Í
𝑢ℓ∈U𝑒bZU[𝑗]·bZU[ℓ], (1)
wherebZUis a normalized ZUwhose each 𝑖-th row satisfies
bZU[𝑖]=ZU[𝑖]/∥ZU[𝑖]∥. (2)
MSA𝑠(𝑢𝑖,𝑢𝑗)is symmetric, i.e., 𝑠(𝑢𝑖,𝑢𝑗)=𝑠(𝑢𝑗,𝑢𝑖)∀𝑢𝑖,𝑢𝑗∈U.
Additionally, by imposing a normalization, −1≤bZU[𝑖]·bZU[𝑗]≤1
∀𝑢𝑖,𝑢𝑗∈U, and hence, the MSA values w.r.t. any node 𝑢𝑖∈U are
scaled to a similar range.
Optimization Objective for ZU.Next, we focus on the obtain-
ment of the feature vector bZU[𝑖]for each node 𝑢𝑖∈U. A favor-
able choice might be graph neural networks (GNNs) [ 27], which,
however, cannot be readily applied to ABGs as existing GNNs are
primarily designed for general graphs, and it is rather costly to
train classic GNNs. As demystified by recent studies [ 38,70,89],
many popular GNNs models can be unified into an optimization
framework from the perspective of numeric optimization, which
essentially produces node feature vectors being smooth on nearby
nodes in terms of the underlying graph. Inspired by this finding,
we extend this optimization framework to ABGs. More specifically,
its objective is as follows:
min
ZU(1−𝛼)·O𝑎+𝛼·O𝑔, (3)
which includes a non-negative coefficient 𝛼∈[0,1]and two terms:
(i) a fitting termO𝑎in Eq. (4)aiming at ensuring ZUis close to the
input attribute vectors XU,
O𝑎=∥ZU−XU∥2
𝐹(4)
and (ii) a regularization term O𝑔in Eq. (5)constraining the featurevectors of two nodes with high connectivity to be similar.
O𝑔=1
2∑︁
𝑢𝑖,𝑢𝑗∈Ub𝑤(𝑢𝑖,𝑢𝑗)·ZU[𝑖]√︁
DU[𝑖,𝑖]−ZU[𝑗]√︁
DU[𝑗,𝑗]2
(5)
The regularization term requires scaling ZU[𝑖]of each node 𝑢𝑖
in Eq. (5)with a factor 1/√︁
DU[𝑖,𝑖]to avoid distorting the values
inZU[𝑖]when𝑢𝑖connects to massive or scant links. The weight
b𝑤(𝑢𝑖,𝑢𝑗)in Eq. (5) is defined by
b𝑤(𝑢𝑖,𝑢𝑗)=∑︁
𝑣ℓ∈N(𝑢𝑖)∩N(𝑢𝑗)𝑤(𝑢𝑖,𝑣ℓ)·𝑤(𝑣ℓ,𝑢𝑗)
DV[ℓ,ℓ],
which reflects the strength of connections between two homoge-
neous nodes 𝑢𝑖and𝑢𝑗inU(e.g., researchers) via their common
neighbors in the counterparty V(e.g., co-authored papers). As an
example for illustration, consider researchers 𝑢3,𝑢4in Figure 1,
b𝑤(𝑢3,𝑢4)=1
3+1
2+1
4, where the denominators 3,2, and 4corre-
spond to the numbers of authors in papers 𝑣3,𝑣4and𝑣5. Accordingly,
b𝑤(𝑢3,𝑢4)evaluates the overall contributions of 𝑢3,𝑢4to their col-
laborated research works in V. Thus, theO𝑔term in Eq. (3)is to
minimize the distance of feature vectors of researchers who have
extensively collaborated with each other with high contributions.
The hyper-parameter 𝛼balances the attribute and topology in-
formation encoded into ZU. In particular, when 𝛼=0, feature
vectors ZU=XU, and at the other extreme, i.e., 𝛼=1,ZUis
entirely dependent on the topology of G.
Closed-form Solution of ZU.Given an𝛼, Lemma 2.21indicates
that the optimal feature vectors ZUto Eq. (3)can be computed via
iterative sparse matrix multiplications in Eq. (6)without undergoing
expensive training.
Lemma 2.2. When𝛾→∞ ,ZUin Eq. (6)is the closed-form solu-
tion to the optimization problem in Eq. (3).
ZU=(1−𝛼)𝛾∑︁
𝑟=0𝛼𝑟·
LUL⊤
U𝑟
XU, (6)
where LUis a normalized version of adjacency matrix BU, i.e.,
LU=D−1
2
UBUD−1
2
V. (7)
In practice, we set 𝛾in Eq. (6)to a finite number (typically 5) for
efficiency. Intuitively, the computation of ZUessentially aggregates
attributes from other homogeneous nodes as per their multi-scale
proximities (e.g., the strength of connections via multiple hops (at
most𝛾hops)) inG. As such, the feature vectors of nodes with
numerous direct or indirect linkages will be more likely to be close,
yielding a high MSA in Eq. (1).
2.3 Objective Function
Based on the foregoing definitions of 𝑘-ABGC and MSA, we formu-
late the problem of 𝑘-ABGC as the following objective function:
min
C1,C2,···,C𝑘𝑘∑︁
ℓ=11
|Cℓ|∑︁
𝑢𝑖∈Cℓ,𝑢𝑗∈U\Cℓ𝑠(𝑢𝑖,𝑢𝑗), (8)
More precisely, Eq. (8)is to identify 𝑘disjoint clustersC1,C2,···,C𝑘
inUsuch that the average MSA of two nodes in different clusters
is low. Meanwhile, with this optimization objective, the average
1All proofs appear in Appendix A.
3784KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
MSA of any two nodes in the same cluster will be maximized; in
other words, nodes within the same cluster are tight-knit.
According to [ 52], Eq. (8)is an NP-complete combinatorial op-
timization problem. Hence, the exact solution to Eq. (8)is compu-
tationally infeasible when Ucontains a large number of nodes.
Moreover, the direct optimization of Eq. (8)demands materializing
𝑠(𝑢𝑖,𝑢𝑗)of every node pairs in U×U . As such, deriving an approx-
imate solution by optimizing Eq. (8)with even a handful of epochs
entails an𝑂(|E|·|U|·𝑑U)computational cost and a quadratic
space overhead 𝑂(|U|2), rendering it incompetent for large ABGs.
3 The TPOAlgorithm
To address the above-said challenges, this section presents our
Three- Phase Optimization framework ( TPO) to𝑘-ABGC computation
based on Eq. (8) without explicitly constructing the MSA matrix.
3.1 Synoptic Overview
At a high level, TPOdraws inspiration from the equivalence between
the optimization objectives in Eq. (8) and Eq. (9), as Lemma 3.1.
Lemma 3.1. Eq.(8)is equivalent to the following objective:
min
Y≥0,H≥0∥R−YH⊤∥2
𝐹s.t.Yis an NCI matrix. (9)
Specifically, if we can identify a matrix Rsuch that R[𝑖]·R[𝑗]=
𝑠(𝑢𝑖,𝑢𝑗)∀𝑢𝑖.𝑢𝑗∈U, the computation of 𝑘non-overlapping clus-
tersC1,C2,···,C𝑘towards optimizing Eq. (8)is equivalent to
decomposing Rinto two non-negative matrices YandH, where Y
represents a normalized cluster indicator (NCI) matrix Y∈R|U|×𝑘,
as defined in Eq. (10).
Y[𝑖,ℓ]= 
1√
|Cℓ|if𝑢𝑖belongs to in cluster Cℓ,
0otherwise.(10)
According to Eq. (10), for each node 𝑢𝑖∈U , its corresponding
vector Y[𝑖]in the NCI matrix comprises solely one non-zero entry
Y[𝑖,ℓ]indicating the clustering membership of 𝑢𝑖, and the value
should be 1/√︁
|Cℓ|. This characteristic ensures that Yis column-
orthogonal, i.e., Y⊤Y=I. However, this constraint on Yrenders the
factorization of Rhard to converge. Instead of directly computing
the exact Y, we employ a two-step approximation strategy. More
specifically, TPOfirst builds a|U|×𝑘matrix 𝚼(a continuous version
ofY) which minimizes the factorization loss in Eq. (11):
min
𝚼≥0,H≥0∥R−𝚼H⊤∥2
𝐹s.t.𝚼⊤𝚼=I, (11)
in which the constraint on Yin Eq. (9)is relaxed to be 𝚼≥0and
𝚼⊤𝚼=I. Afterward, the task is to transform 𝚼into an NCI matrix
Yby minimizing their difference about Eq. (9).
As outlined in Figure 2, given an ABG G, the number of 𝑘of
clusters, and the node set Uto be partitioned as input, TPOoutputs
an approximate solution to the 𝑘-ABGC problem in Eq. (8)through
three phases: (i) constructing a low-dimensional matrix Rsuch that
R[𝑖]·R[𝑗]≈𝑠(𝑢𝑖,𝑢𝑗)∀𝑢𝑖.𝑢𝑗∈U without explicitly materializing
the MSA of all node pairs (Algorithm 1, Section 3.2); (ii) factorizing
Ras per Eq. (11)to create aU×𝑘non-negative column-orthogonal
matrix 𝚼(Algorithm 2, Section 3.3); and (iii) effectively converting
𝚼into an NCI Y(Algorithm 3, Section 3.4). In what follows, we
elaborate on the algorithmic details of these three subroutines. Due
to space limit, we defer the complexity analysis of them and TPOto
(i) MSA Approximation (Algorithm 1 )(ii) Greedy Orthogonal NMF
(iii) NCI Generation (Algorithm 3)≈∙(Algorithm 2 )∙
𝐑 𝐑T
𝑢1
𝑢2
𝑢3
𝑢4
𝑢5
𝑢6
𝑢7𝑢1
𝑢2
𝑢3
𝑢4
𝑢5
𝑢6
𝑢7𝐑 𝚼 𝐇
𝚼
NCI𝐘𝑢1𝑢2𝑢3𝑢4𝑢5𝑢6𝑢7𝚼≥𝟎
𝐇≥𝟎
𝑢1
𝑢2
𝑢3
𝑢4
𝑢5
𝑢6
𝑢7
𝑢1𝑢2𝑢3𝑢4𝑢5𝑢6𝑢7
𝑣1𝑣2𝑣3𝑣4𝑣5𝑣6𝑣7𝑣8
𝒔(𝒖𝒊,𝒖𝒋)𝑘=3
𝑢1
𝑢2
𝑢3
𝑢4
𝑢5
𝑢6
𝑢7𝑘=3, target node set 𝒰
ℰ
2𝑑=4𝒰
𝒱𝐗𝒰Figure 2: Overview of TPO
our technical report [78].
3.2 MSA Approximation via Random Features
Algorithm 1 illustrates the pseudo-code of linearizing the approxi-
mate computation of MSA in Eq. (1)as the matrix product R·R⊤
with matrix R. The fundamental idea is to leverage and tweak the
random features [49,82] technique designed for approximating the
Gaussian kernel 𝑒−∥x−y∥2/2of any vectors xandy.
After taking as input the ABG Gand parameters 𝛼,𝛾, Algorithm 1
begins by calculating LUaccording to Eq. (7)and initializing ZU
as𝛼XU(Lines 1-2). At Lines 3-4, we update ZUvia𝛾iterations of
the following matrix multiplication:
ZU←𝛼·(XU+LU·(L⊤
UZU)). (12)
Particularly, we structure the matrix multiplication LUL⊤
UZUas
LU·(L⊤
UZU)in Eq. (12)to boost the computation efficiency. Sub-
sequently, Algorithm 1 transforms ZUintobZUby applying an 𝐿2
normalization to each row in ZU(Line 5) and then proceeds to
constructing R(Lines 6-9).
To be specific, we first generate a 𝑑U×𝑑UGaussian random ma-
trixGwith every entry sampled independently from the standard
normal distribution (Line 6) and then apply a QR decomposition
over it to get a 𝑑U×𝑑Uorthogonal matrix Q(Line 7). The matrix
Qis distributed uniformly on the Stiefel manifold, i.e., the space of
all orthogonal matrices [43]. Next, Algorithm 1 calculates R′by
R′=√︂𝑒
𝑑U·
𝑠𝑖𝑛(bZ◦
U)∥𝑐𝑜𝑠(bZ◦
U)
∈R|U|× 2𝑑U, (13)
where bZ◦
U=√︁
𝑑U·bZU·Q⊤and∥represents the horizontal
concatenation operator for matrices (Line 8). Finally, in Line 9, the
matrix Ris obtained by normalizing each row of R′as
R[𝑖]=R′[𝑖]√︁
R′[𝑖]·r∈R2𝑑Uwhere r=∑︁
𝑢ℓ∈UR′[ℓ]. (14)
Theorem 3.2. For any two nodes 𝑢𝑖,𝑢𝑗∈U, ifRis the output of
Algorithm 1, then the following inequality holds:
1−17
16𝑑2
U−1
4𝑑U
1+17
16𝑑2
U+1
4𝑑U·𝑠(𝑢𝑖,𝑢𝑗)≤E[R[𝑖]·R[𝑗]]≤1+17
16𝑑2
U+1
4𝑑U
1−17
16𝑑2
U−1
4𝑑U·𝑠(𝑢𝑖,𝑢𝑗)
Theorem 3.2 indicates that E[R[𝑖]·R[𝑗]]serves as an accurate
estimator of 𝑠(𝑢𝑖,𝑢𝑗), exhibiting extremely low bias, particularly
because𝑑Uoften exceeds hundreds in practical scenarios.
3785Effective Clustering on Large Attributed Bipartite Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1: MSA Approximation
Input: An ABGG=(U∪V,E,XU), target node setU,
the balance coefficient 𝛼, the number 𝛾of iterations
Output: Matrix R
1Calculate LUaccording to Eq. (7);
2ZU←𝛼XU;
3for𝑟←1to𝛾do
4 Update ZUaccording to Eq. (12);
5Normalize ZUasbZUby Eq. (2);
6Sample a Gaussian random matrix G∈R𝑑U×𝑑U;
7Compute Qby a QR decomposition over G;
8Compute R′according to Eq. (13);
9for𝑢𝑖∈U doCompute R[𝑖]according to Eq. (14);
10return R;
3.2.1 SVD-based Attribute Dimension Reduction. Although
Algorithm 1 circumvents the need to construct the MSA for all
node pairs, it remains tenaciously challenging when dealing with
ABGs with vast attribute sets, i.e., 𝑑Ubeing large. Recall that the
major computation expenditure in Algorithm 1 lies at Lines 3-4 and
Lines 7-8, which need 𝑂(𝛾·|E|·𝑑U)and𝑂(𝑑3
U+|U|·𝑑2
U)time,
respectively. As a result, when 𝑑Uis high, e.g., 𝑑U=𝑂(|U|) , the
computational complexity of Algorithm 1 increases dramatically to
be cubic, rendering it impractical for large-scale ABGs.
To address this, we refine the input attribute vectors XUby
reducing their dimension from 𝑑Uto a much smaller constant 𝑑
(𝑑≪𝑑U). This approach aims to ensure that the 𝑑-dimensional
approximation X′
UofXUstill accurately preserves the MSA as per
Eq.(1). This adjustment reduces the computational cost to a linear
time complexity of 𝑂(𝛾·|E|+|U|) since𝑑is a constant. To realize
this idea, we first apply the top- 𝑑singular value decomposition
(SVD) over XUto produce the decomposition result XU≈𝚪𝚺𝚿⊤.
Utilizing the column-orthogonal (semi-unitary) property of 𝚿, i.e.,
𝚿⊤𝚿=I, we have XUX⊤
U≈𝚪𝚺𝚿⊤𝚿𝚺𝚪⊤=𝚪𝚺2𝚪⊤, implying
X′
U=𝚪𝚺∈R|U|×𝑑, (15)
which can be employed as a low-dimensional substitute of XUinput
to Algorithm 1. Along this line, we can derive a low-dimensional
version Z′
Uof feature vectors ZUusing the iterative process at
Lines 2-4 in Algorithm 1 by simply replacing XUasX′
U, i.e.,
Z′
U=(1−𝛼)∞∑︁
𝑟=0𝛼𝑟·
LUL⊤
U𝑟
X′
U.
Lemma 3.3. Let𝚪𝚺𝚿⊤be the exact top- 𝑑SVD of XU.
Z′
U[𝑖]·Z′
U[𝑗]−ZU[𝑖]·ZU[𝑗]≤𝜎2
𝑑+1√︁
DU[𝑖,𝑖]·DU[𝑗,𝑗]
1−𝛼
holds for every two nodes 𝑢𝑖,𝑢𝑗∈U , where𝜎𝑑+1is the(𝑑+1)-th
largest singular value of XU.
Lemma 3.3 establishes the approximation guarantee of Z′
U, which
theoretically assures the accurate approximation of the MSA de-
fined in Eq. (1). Aside from the capabilities of preserving MSA and
reducing computation load, this SVD-based trick can surprisingly
denoise attribute data for enhanced clustering by its close connec-
tion to principal component analysis (PCA), as validated by ourAlgorithm 2: Greedy Orthogonal NMF
Input: Matrix R, the number 𝑘of clusters, the number 𝑇𝑓of
iterations
Output: Matrix 𝚼
1𝚪,𝚺,𝚿←RandomizedSVD(R,𝑘);
2Initialize 𝚼andHaccording to Eq. (18);
3for𝑡←1to𝑇𝑓do
4 Update H[𝑗,ℓ]∀1≤𝑗≤2𝑑,1≤ℓ≤𝑘by Eq. (16);
5 Update 𝚼[𝑖,ℓ]∀𝑢𝑖∈U,1≤ℓ≤𝑘by Eq. (17);
6return 𝚼;
experiments in Section 4.2.
3.3 Greedy Orthogonal NMF
Upon constructing R∈R|U|× 2𝑑(with𝑑=𝑑Uif the dimension
reduction from Section 3.2.1 is not applied) in Algorithm 1, TPO
passes it to the second phase, i.e., conducting an orthogonal non-
negative matrix factorization (NMF) of Ras in Eq. (11)to create
𝚼. The pseudo-code of our solver to this problem is presented in
Algorithm 2, iteratively updating 𝚼andHusing an alternative
framework towards optimizing the objective function in Eq. (11).
(Lines 3-5). Specifically, given the number 𝑇𝑓of iterations and initial
guess of Hand𝚼, in each iteration, we first update each (𝑗,ℓ)-entry
(1≤𝑗≤2𝑑and1≤ℓ≤𝑘) inHfollowing Eq. (16)while fixing
𝚼, and then update 𝚼[𝑖,ℓ]for𝑢𝑖∈U and1≤ℓ≤𝑘as in Eq. (17)
with Hfixed.
H[𝑗,ℓ]=H[𝑗,ℓ]·(R⊤𝚼)[𝑗,ℓ]
(H·(𝚼⊤𝚼))[𝑗,ℓ](16)
𝚼[𝑖,ℓ]=𝚼[𝑖,ℓ]·√︄
(RH)[𝑖,ℓ]
(𝚼·(𝚼⊤·(RH)))[𝑖,ℓ](17)
The above update rules for solving Eq. (11)can be derived by
utilizing the auxiliary function approach [32] with Lagrangian mul-
tipliers in convex optimization, whose convergence is guaranteed
by the monotonicity theorem [ 10]. Note that we reorder the matrix
multiplications H𝚼⊤𝚼and𝚼𝚼⊤RHin Eq. (16)and(17)toH·(𝚼⊤𝚼)
and𝚼·(𝚼⊤·(RH)), respectively, so as to avert materializing 2𝑑×|U|
dense matrix H𝚼⊤and|U|×|U| dense matrix 𝚼𝚼⊤. As such, the
computational complexities of updating Hand𝚼in Eq. (16)and
(17) are reduced to 𝑂(|U|𝑑𝑘+|U|𝑘2)per iteration.
The aforementioned computation is still rather costly due to the
numerous iterations needed for the convergence of 𝚼andH, espe-
cially when 𝚼andHare initialized randomly. We resort to a greedy
seeding strategy to expedite convergence, as in many optimization
problems. That is, we carefully select a good initialization of 𝚼and
Hin a fast but theoretically grounded manner. As described in Lines
1-2 in Algorithm 1, we set 𝚼andHas follows:
𝚼=𝚪,H=𝚿𝚺, (18)
where 𝚪and𝚿are the top- 𝑘left and right singular vectors of R,
respectively, and 𝚺is a diagonal matrix whose diagonal entries
are top-𝑘singular values of R, which are obtained by invoking the
truncated randomized SVD algorithm [ 19] with Rand𝑘. Note that
this routine consumes 𝑂(|U|𝑑𝑘+(U+𝑑)𝑘2)time [ 19] and can be
done efficiently in practice in virtue of its randomized algorithmic
design as well as the highly-optimized libraries (LAPACK and BLAS)
for matrix operations under the hood.
3786KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
Algorithm 3: Effective NCI Generation
Input: Matrix 𝚼and the number 𝑇𝑔of iterations
Output: The NCI matrix Y
1𝚽=I;
2for𝑡←1to𝑇𝑔do
3 for𝑢𝑖∈U do
4 Determineℓ∗by Eq. (21);
5 Update Yby Eq. (22);
6 Normalize Ysuch that each column has a unit 𝐿2norm;
7𝚽←𝚼⊤Y;
8return Y;
Given the fact that singular vectors 𝚼=𝚪are column-orthogonal,
i.e.,𝚼⊤𝚼=I, the Eckart-Young Theorem [ 15] (Theorem A.1 in
Appendix A) pinpoints that Eq. (18)offers the optimal solution to
Eq.(11)when the non-negative constraints over 𝚼andHare relaxed.
In simpler terms, Eq. (18)immediately gains a rough solution to
our optimization objective in Eq. (11), thereby drastically curtailing
the number of iterations needed for Lines 3-5.
3.4 Effective NCI Generation
In its final stage, TPOgenerates an NCI matrix Yby minimizing
the “difference” between 𝚼returned by Algorithm 2 and the tar-
get NCI matrix Y. Recall from Eq. (9), our original objective is
to find a|U|×𝑘NCI matrix Yand a 2𝑑×𝑘non-negative H
such that the total squared reconstruction error ∥R−YH⊤∥2
𝐹=Í
𝑢𝑖∈UÍ𝑑
𝑗=1(R[𝑖,𝑗]−Y[𝑖]·H[𝑗])2is minimized. Considering 𝚼
is a continuous version of Y(relaxing the constraint in Eq. (10)),
∥R−𝚼H⊤∥2
𝐹is capable of attaining a strictly lower reconstruction
error compared to∥R−YH⊤∥2
𝐹. Therefore, an ideal solution Yto Eq.
(9)ensures that∥R−YH⊤∥2
𝐹closely approximates ∥R−𝚼H⊤∥2
𝐹in
Eq.(11). Mathematically, the conversion from matrix 𝚼into the NCI
matrix Ycan be formulated as the minimization of the difference
of their reconstruction errors, i.e.,∥R−YH⊤∥2
𝐹−∥R−𝚼H⊤∥2
𝐹=trace((YY⊤−𝚼𝚼⊤)·RR⊤)by Lemma 3.4.
Lemma 3.4. The following equation holds:
∥R−YH⊤∥2
𝐹−∥R−𝚼H⊤∥2
𝐹=trace((YY⊤−𝚼𝚼⊤)·RR⊤).(19)
Further, we reformulate the problem as follows:
min
𝚽,Y∥𝚼𝚽−Y∥2s.t.𝚽𝚽⊤=IandYis an NCI matrix , (20)
which implies that, if the NCI matrix Yand the𝑘×𝑘row-orthogonal
matrix 𝚽minimize∥𝚼𝚽−Y∥2,YY⊤−𝚼𝚼⊤≈𝚼𝚽𝚽⊤𝚼⊤−𝚼𝚼⊤≈0
holds and the objective loss in Eq. (19) is therefore minimized.
To solve Eq. (20), we develop Algorithm 3 in TPO, which obtains
the NCI matrix Ythrough an iterative framework wherein 𝚽and
Yare refined in an alternative fashion till convergence. Initially,
Algorithm 3 starts by taking as input the matrix 𝚼and the number
𝑇𝑔of iterations and initializing 𝚽as a𝑘×𝑘identity matrix (Line 1).
It then launches an iterative process at Lines 2-7 to jointly refine Y
and𝚽. Specifically, in each of the 𝑇𝑔iterations, TPOfirst determines
the cluster id of each node 𝑢𝑖∈U via (Line 4)
ℓ∗=arg max
1≤ℓ≤𝑘𝚼[𝑖]·𝚽[:,ℓ] (21)
and then updates the cluster indicator Y[𝑖]of node𝑢𝑖as followsTable 2: Attributed Bipartite Graphs
Name CiteSe
er Cora Mo
vieLens Go
ogle A
mazon
|
U| 1,237 1,312 6,040 64,527 2,330,066
|
V| 742 789 3,883 868,937 8,026,324
|
E| 1,665 2,314 1,000,209 1,487,747 22,507,155
𝑑U 3,703 1,433 30 1,024 800
𝑑V 3,703 1,433 21 - -
𝑘 6 7 21 5 3
(Line 5):
Y[𝑖,ℓ]=(
1ifℓ=ℓ∗,
0otherwise,∀1≤ℓ≤𝑘. (22)
Each column in Yis later𝐿2-normalized, i.e.,
∀1≤ℓ≤𝑘√︃Í
𝑢𝑖∈UY[𝑖,ℓ]2=1, (23)
in accordance with the NCI constraint in Eq. (10)(Line 6). In a
nutshell, Lines 3-6 optimizes Eq. (20)by updating Ywith𝚽fixed. To
explain, recall the constraint of the NCI matrix Ystated in Eq. (10),
each row of Yhas merely one non-zero entry. Hence, by locating the
column idℓ∗whose corresponding entry (𝚼𝚽)[𝑖,ℓ∗]is maximum
in the𝑖-th row of 𝚼𝚽(i.e., Eq. (21)) and meanwhile updating Y[𝑖]
as Eqs. (22)and(23)as Lines 5-6, the distance between 𝚼𝚽andY
in Eq. (20) is naturally minimized.
With the refined Yat hand, the subsequent work turns into
updating the 𝑘×𝑘matrix 𝚽towards optimizing
min
𝚽∥𝚼𝚽−Y∥2s.t.𝚽𝚽⊤=I.
Given Y, the minimizer to this problem is 𝚽=𝚼⊤Yby utilizing
Lemma 4.14 in [61]. Therefore, 𝚽is updated to 𝚼⊤Yat Line 7.
After repeating the above procedure for 𝑇𝑔iterations, TPOreturns
Yas the final clustering result. Practically, a dozen iterations are
sufficient to yield high-caliber Y, as validated in Section 4.3.
4 Experiments
In this section, we experimentally evaluate our proposed 𝑘-ABGC
method TPOagainst 19 competitors over five real ABGs in terms of
clustering quality and efficiency. All the experiments are conducted
on a Linux machine powered by 2 Xeon Gold 6330 @2.0GHz CPUs
and 1TB RAM. For reproducibility, the source code and datasets are
available at https://github.com/HKBU-LAGAS/TPC.
4.1 Experimental Setup
Datasets. Table 2 lists the statistics of the five datasets used in the
experimental study. |U|,|V|, and|E|denote the cardinality of two
disjoint node setsU,V, and edge setEofG, respectively, while 𝑑U
(resp.𝑑V) stands for the dimensions of attribute vectors of nodes in
U(resp.V). The number of ground-truth clusters of nodes UinG
is𝑘.Citeseer andCora are synthesized from real citation graphs in
[27] by dividing nodes in each cluster into two equal-sized partitions
(i.e.,UandV) and removing intra-partition edges and isolated
nodes as in [ 62]. In particular, nodes represent publications, edges
denote their citation relationships, and labels correspond to the
fields of study. The well-known MovieLens dataset [ 20] comprises
user-movie ratings, where clustering labels are users’ occupations
inU.Google andAmazon are extracted from the Google Maps [ 68]
3787Effective Clustering on Large Attributed Bipartite Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Clustering Quality (Larger ACC, NMI, and ARI indicate higher clustering quality).
MethodCiteSeer Cora MovieLens Google AmazonRankACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI
KMeans [21] 0.526 0.277 0.225 0.431 0.229 0.137 0.298 0.363 0.170 0.370 0.053 0.012 0.502 0.038 0.079 5.4
SpecClust [56] 0.222 0.017 -0.001 0.311 0.026 0.003 0.318 0.392 0.197 - - - - - - 11.27
NMF [65] 0.508 0.222 0.228 0.380 0.165 0.110 0.568 0.611 0.482 0.384 0.069 0.062 0.390 0.006 0.015 5.6
SCC [8] 0.243 0.015 0.012 0.280 0.040 0.017 0.128 0.030 0.004 0.425 0.038 0.038 0.470 0.018 -0.016 10.27
SBC [28] 0.239 0.015 0.012 0.262 0.059 0.023 0.116 0.035 0.006 0.394 0.006 -0.003 0.485 0.003 0.005 10.87
CCMOD [2] 0.200 0.010 0.003 0.264 0.066 0.040 0.141 0.029 0.010 OOM OOM OOM OOM OOM OOM 12.27
SpecMOD [29] 0.238 0.012 0.003 0.290 0.023 -0.007 0.125 0.033 0.009 OOM OOM OOM OOM OOM OOM 12.6
InfoCC [9] 0.235 0.013 0.007 0.223 0.035 0.018 0.095 0.036 0.007 0.277 0.008 0.008 0.378 0.007 0.003 11.54
DeepCC [63] 0.205 0.013 0.004 0.213 0.014 0.006 0.093 0.027 0.004 0.324 0.105 0.031 - - - 12.67
HOPE [72] 0.473 0.169 0.288 0.268 0.025 0.043 0.115 0.009 0.037 0.269 0 0 0.452 0 0.002
ACMin [77] 0.450 0.143 0.141 0.650 0.470 0.410 0.122 0.032 0.009 0.312 0.023 0.020 0.428 0.012 0.003 7.67
GRACE [12] 0.469 0.209 0.199 0.351 0.136 0.103 0.298 0.249 0.195 0.420 0 0 0.427 0.008 0 7.47
AGCC [85] 0.448 0.144 0.153 0.650 0.517 0.406 0.538 0.589 0.480 OOM OOM OOM OOM OOM OOM 7.34
Dink-Net [37] 0.308 0 0 0.231 0.004 0.007 0.123 0.001 0 0.429 0 0 OOM OOM OOM
node2vec [18] 0.209 0.007 0.001 0.220 0.008 0 0.074 0.011 0 0.280 0 0 - - - 14.8
BiNE [13] 0.196 0.005 0 0.174 0.005 -0.002 0.071 0.012 0 - - - - - - 15.54
GEBE [73] 0.231 0.013 0.002 0.293 0.014 0.006 0.095 0.014 -0.002 0.428 0 0 0.489 0 0 12.14
PANE [75, 76] 0.443 0.154 0.136 0.537 0.526 0.339 0.855 0.923 0.838 0.359 0.127 0.070 0.497 0.083 0.102 4.2
BiANE [23] 0.259 0.057 0.016 0.341 0.239 0.080 0.091 0.053 0.013 - - - - - - 10.21
TPO(𝑑=𝑑U)0.541 0.256 0.265 0.662 0.477 0.408 0.931 0.961 0.957 0.367 0.112 0.091 0.502 0.045 0.091 2.54
TPO 0.625 0.322 0.348 0.671 0.475 0.416 0.931 0.961 0.957 0.444 0.135 0.138 0.504 0.055 0.104 1.27
and Amazon review dataset [ 22], where edges represent the reviews
on restaurants and books posted by users.
Competitors and Parameters. We compare TPOagainst 19 exist-
ing methods, which can be categorized into four groups as follows:
•Data Clustering: KMeans [21], NMF [65], and SpecClust [56];
•Network Embedding: node2vec [18],BiNE [13],BiANE [23],PANE
[75, 76], and GEBE [73];
•Attributed Graph Clustering: AGCC [85],ACMin [77],GRACE [12],
Dink-Net [37];
•Bipartite Graph Clustering: SCC [8],SBC [28],InfoCC [9],Spec-
MOD [29], CCMOD [2],DeepCC [63], and HOPE [72].
Unless otherwise specified, on all datasets, we set the numbers 𝑇𝑓
and𝑇𝑔of iterations required by Algorithms 2 and 3 in our proposed
TPOto5and20, respectively. Regarding parameters 𝛼and𝛾, by
default, we set 𝛼=0.6,𝛾=6onCiteSeer andMovieLens,𝛼=0.9,𝛾=
10onCora andGoogle, and 𝛼=0.5,𝛾=1onAmazon, respectively.
To deal with the high attribute dimensions 𝑑Uof the CiteSeer, Cora,
Google, and Amazon datasets, we set their new attribute dimensions
𝑑in Section 3.2.1 to 32,128,32, and 64, respectively. We refer to the
version of TPOwithout the attribute dimension reduction module
in Section 3.2.1 as TPO(𝑑=𝑑U). More implementation details of
our method and baselines are in our technical report [78].
Evaluation Metrics. Following convention, we adopt three widely
used measures [ 6,25,31,57,77,85] to assess the clustering quality,
namely (i) Clustering Accuracy (ACC), (ii) Normalized Mutual In-
formation (NMI), and (3) Adjusted Rand Index (ARI), for measuring
the quality of clusters produced by each evaluated method in the
presence of the ground-truth clusters of the tested dataset. Particu-
larly, ACC and NMI scores range from 0to1.0, whilst ARI ranges
from−0.5to1.0. For each of these metrics, higher values indicate
better clustering quality. Regarding efficiency evaluation, we re-
port the running time in seconds (measured in wall-clock time) of
each method on each dataset, excluding the time for input (loading
datasets) and output (saving clustering results). The formulas for
evaluation metrics are in our technical report [78].4.2 Clustering Performance
This set of experiments reports the clustering quality achieved
byTPOand all competitors over the five datasets, as well as their
respective running times. We omit a method if it cannot report the
results within three days or incur out-of-memory (OOM) errors.
Since TPOis randomized, we repeat it five times and report the
average performance.
Clustering Quality. Table 3 shows the ACC, NMI, and ARI scores
of all methods on five ABGs, and their overall average performance
rankings. We highlight the top-3 best clustering results on each
dataset in gray with darker shades indicating higher quality. TPO
consistently outperforms the 17 competitors on the CiteSeer, Movie-
Lens, and Google datasets in terms of ACC, NMI, and ARI, by sub-
stantial margins of up to 9.9%for ACC, 4.5%for NMI, and 12%for
ARI, respectively. The only exceptions are on Cora and Amazon,
where TPOachieves the highest ACC and ARI results but inferior
NMI scores compared to PANE orAGCC. In addition, TPO(𝑑=𝑑U)
exhibits competitive clustering effectiveness, which either is second
only to TPOor obtains the third best clustering results in most cases.
Specifically, TPO(𝑑=𝑑U) is comparable to TPOonCora, Movie-
Lens, and Amazon with a performance degradation at most 0.9%
in ACC, 1.0%in NMI, and 1.3%in ARI. Over all datasets, TPOand
TPO(𝑑=𝑑U) attain the best and second best average performance
rank (smaller rank is better), respectively. The evident superiority
ofTPOandTPO(𝑑=𝑑U) manifests the accuracy of our proposed
MSA model in Section 2.2 in preserving the attribute similarity and
topological connections between nodes, as well as the effectiveness
of theoretically-grounded three-phase optimization framework in-
troduced in Section 3.
At this point, a keen reader may wonder why TPOwith attribute
dimension reduction outperforms TPO(𝑑=𝑑U) on most datasets,
especially CiteSeer and Google, as it seems that the former is an
approximate version of the latter. Notice that TPOandTPO(𝑑=𝑑U)
output identical results, as dimension reduction is not needed on
MovieLens andTPOturns to be TPO(𝑑=𝑑U). Recall that the only
difference between TPOandTPO(𝑑=𝑑U) is that TPOemploys a
truncated SVD over the input attribute vectors XUof a node inU
3788KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
10−210−1100101running time (sec)
TPO
TPO(𝑑=𝑑U)
A
GCC
ACMin
PANE
KMeans
NMF
(a)Cora10−1100101102running
time (sec)
TPO
TPO(𝑑=𝑑U)
P
ANE
NMF
A
GCC
Sp
ecClust
GRA
CE
(
b)MovieLens100101102103104105running
time (sec)
TPO
TPO(𝑑=𝑑U)
P
ANE
NMF
SCC
De
epCC
KMeans
(
c)Google100101102103104running
time (sec)
TPO
TPO(𝑑=𝑑U)
P
ANE
KMeans
A
CMin
SBC
NMF
(
d)Amazon
Figure 3: Running time in seconds.
CiteSe
er Cora Mo
vieLens Go
ogle
0.1 0.3 0.5 0.7 0.90.40.50.60.70.80.9A
CC
(a) Varying𝛼0 2 4 6 8 100.40.50.60.70.80.9A
CC
(b) Varying𝛾0 5 10 15 200.40.50.60.70.80.9A
CC
(c) Varying𝑇𝑓0 5 10 15 200.40.50.60.70.80.9A
CC
(d) Varying𝑇𝑔16 32 64 128 2560.40.50.60.70.80.9A
CC
(e) Varying𝑑
Figure 4: Clustering accuracy when varying parameters.
for dimension reduction as stated in Section 3.2.1. Aside from the
crucial theoretical assurance offered by this SVD-based approach
in the MSA approximation, it implicitly conducts a PCA on the
attribute vectors, extracting key features from the input attributes
while eradicating noisy ones. In brief, the SVD-based trick in Section
3.2.1 grants TPOthe additional ability to denoise the attribute data,
thus elevating the results’ quality.
Efficiency. For clarity, we compare the empirical efficiency of TPO
and TPO(𝑑=𝑑U) only against competitors ranked in the top
7 for clustering quality, as shown in Table 3. Figure 3 plots the
computation times required by each of these methods on Cora,
MovieLens, Google, and Amazon. The 𝑦-axis is the running time
(seconds) in the log scale. On each of the diagrams in Figures 3(a),
3(b), 3(c), and 3(d), all the bars are displayed from left to right in an
ascending order w.r.t. their average performance rank in Table 3.
Accordingly, except the first two bars from the left for TPOandTPO
(𝑑=𝑑U), the third bars (from the left) in these figures illustrate
the running times of the best competitors, i.e., AGCC onCora, and
PANE onMovieLens, Google, and Amazon, respectively. As we can
see,TPOis consistently faster than the state-of-the-art approaches,
AGCC orPANE, on four datasets, often by orders of magnitude. For
instance, on Cora, Google, and Amazon, TPOtakes 0.47,28.7, and 178
seconds, respectively, whereas the best baselines AGCC orPANE
cost around 19seconds, 23minutes, and 4.1hours, respectively,
attaining 40×,48×, and 83×speedup. In addition, TPOalso enjoys
a considerable efficiency gain of up to 19.9×over TPO(𝑑=𝑑U),
attributed to the SVD-based dimension reduction (Section 3.2.1). On
theMovieLens dataset, the input attribute dimension 𝑑U=30is low,
and the attribute dimension reduction is therefore disabled, making
TPOandTPO(𝑑=𝑑U) yield the same running time, which is 3.46×
over the best competitor PANE. Although NMF, KMeans, and SCC
run much faster than TPOon some datasets by either neglecting the
graph topology or discarding the attribute data, their result quality
is no match for our solution TPO.In summary, TPOconsistently delivers superior results for 𝑘-
ABGC tasks over ABGs with various volumes while offering high
practical efficiency, which corroborates the efficacy of our novel
objective function based on MSA in Section 2 and the optimization
solver with careful algorithmic designs developed in Section 3.
4.3 Parameter Analysis
In these experiments, we empirically investigate the impact of five
key parameters in TPO:𝛼,𝛾,𝑇𝑓,𝑇𝑔, and𝑑. For each of them, we run
TPOover CiteSeer, Cora, MovieLens, and Google, respectively, by
varying the parameter with others fixed as in Section 4.1.
Varying𝛼and𝛾.Figures 4(a) shows that on Cora and Google,
TPO’s clustering performance markedly improves as 𝛼increases
from 0.1to0.9, indicating the importance of graph structure in
these datasets. On CiteSeer and MovieLens, setting 𝛼=0.6will
be a favorable choice, which results in an optimal combination of
attributes and graph topology and hence the highest ACC scores.
Figures 4(b) depicts the ACC scores when 𝛾increases from 0to
10. When𝛾=0, the graph structure is disregarded in TPO, namely
ZU=XU. It can be observed on all datasets that the clustering
quality rises with 𝛾increasing except CiteSeer andMovieLens, where
the ACC results reach a plateau after 𝛾≥6. This is consistent with
the fact that a larger 𝛾produces a more accurate solution ZUto
the objective in Eq. (3), and thus, higher clustering quality.
Varying𝑇𝑓and𝑇𝑔.Figures 4(c) presents the ACC scores when
the𝑇𝑓of iterations in Algorithm 2 is varied from 0to20. We can
conclude that our greedy seeding strategy described in Section
3.3 is highly effective in enabling swift convergence, as additional
optimization iterations merely bring minor gains in clustering per-
formance. On Cora andCiteSeer, the ACC scores see an uptick when
varying𝛾from 0to10, followed by a pronounced downturn. Such
a performance decline is caused by overfitting in solving Eq. (11).
From Figures 4(d) reporting clustering performance changes when
3789Effective Clustering on Large Attributed Bipartite Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain
varying𝑇𝑔from 0to20, we can make analogous observations on
the four datasets. The evaluation scores first experience a sharp
increase as 𝑇𝑔increases from 0to5. After that, the ACC remain
invariant with 𝛾increasing. The results manifest the effectiveness
of our solver developed in Section 3.4 in fast NCI generation.
Varying𝑑.Intuitively, a large 𝑑may lead to accurate preservation
of MSA as per Lemma 3.3 and further improve clustering quality.
However, in practice, the original attribute vectors XUembody
noises, especially when 𝑑Uis high. As pinpointed and validated in
Sections 3.2.1 and 4.2, our SVD-based dimension reduction inher-
ently applies a PCA over XUfor noise elimination, considerably
upgrading the empirical result quality. That is to say, the choice of 𝑑
strikes a balance between capturing MSA and removing noisy data,
consistent with our empirical results in Figure 4(e). In particular,
onCiteSeer, Cora, and Google, picking 32,128, and 32for dimension
𝑑, respectively, can strike a good balance between MSA preserva-
tion and noisy reduction for superior clustering performance. On
MovieLens, the attribute dimension reduction is not enabled when
𝑑≥32since its original dimension 𝑑U=30. We refer interested
readers to our technical report [ 78] for NMI and efficiency results.
5 Related Work
Bipartite Graph Clustering. A classic methodology [ 87] for bi-
partite graph clustering first projects a bipartite graph Ginto a
unipartite graph by connecting every two nodes from the same
partitionUif they share common neighbors in G. Then, a standard
graph clustering algorithm for node clustering can be adopted on
the constructed unipartite graph. However, the projection often
leads to unipartite graphs 𝑂(|U|2)edges, which is intolerable for
even medium-sized graphs [ 71]. In our previous work [ 72], we
addressed this problem by transforming it into a two-stage approx-
imation framework.
Unlike the projection-based methods, another line of research
focuses on simultaneously clustering two disjoint sets of nodes
(i.e.,UandV) in a bipartite graph. These co-clustering techniques
have been extensively investigated in the literature [ 17] and span a
variety of applications in bioinformatics and text mining. Several
attempts [ 8,28,29] are made to extend spectral clustering to bi-
partite graphs. Analogously, Ailem et al . [2] and Dhillon et al . [9]
propose generating co-clusters by extending and optimizing classic
metrics of modularity and mutual information on bipartite graphs,
respectively. DeepCC [63] creates low-dimension instances and
features using a deep autoencoder, then assigns clusters using a
variant of the Gaussian mixture model. To handle the resolution
limit in prior works as well as incorporate attribute information,
Kim et al . [25] designed ABC, which incurs a severe efficiency issue
due to its quadratic running time 𝑂(|U|2+|V|2).
Attributed Graph Clustering. As surveyed in [ 4,5,33,77], there
is a large body of work on attributed graph clustering (AGC). Ac-
cording to [ 77], existing AGC techniques can be categorized into
four groups: edge-weigh-based methods [ 51,79], distance-based
methods [ 11,88], statistics-based models [ 66,77,80], and graph
learning-based methods [ 12,42,57,58,85]. Among them, graph
learning-based approaches [ 12,37,42,85] have achieved state-of-
the-art performance, as reported in [ 30,77]. These methods obtainhigh clustering quality on attributed graphs at the cost of costly
neural network training, thus incurring poor scalability on large
graphs. To our knowledge, the statistical-model-based solution,
ACMin [77], is the only AGC method that scales to massive graphs
with millions of nodes and billions of edges, while attaining high
result quality. However, none of them are custom-made for ABGs,
producing compromised result quality for 𝑘-ABGC.
Network Embedding. In recent years, network embedding, which
converts each node in a graph into an embedding vector capturing
the surrounding structures, has been employed in a wide range
of graph analytics tasks, and has seen remarkable success [ 7,14].
In particular, by simply feeding them into data clustering meth-
ods, e.g., KMeans, such embedding vectors can be utilized to cope
with𝑘-ABGC. However, the majority of network embedding works
[13,18,47,48,53,54,73,74] are designed for graphs in the ab-
sence of node attributes. To bridge this gap, a series of efforts
[6,24,35,44,55,60,75,76] have been made towards incorporat-
ing node attributes into embedding vectors for enhanced result
utility. These approaches still suffer from sub-optimal clustering
performance as they fall short of preserving the hidden seman-
tics underlying bipartite graphs. To learn effective node embed-
dings over ABGs, [ 1,23] extend SkipGram models [ 39] to ABGs by
picking node-pair samples with consideration of both their intra-
partition/inter-partition proximities and attribute similarities. Athar
et al. [3] project the ABG into two homogeneous graphs based on
topological connections and attribute similarities, and then invoke
unsupervised GNNs on the constructed graphs for embedding gen-
eration. Moreover, Zhang et al . [86] propose IGE[86] for learning
node embeddings on dynamic ABGs with a focus on temporal
dependence of edges rather than the bipartite graph structures.
These works either fall short of preserving multi-hop relationships
between nodes or struggle to cope with large ABGs due to the
significant expense of training.
6 Conclusion
This paper presents TPO, an effective and efficient solution for 𝑘-
ABGC tasks. TPOachieves remarkable performance, attributed to
a novel problem formulation based on the proposed multi-scale
attribute affinity measure for nodes in ABGs, and a well-thought-
out three-phase optimization framework for solving the problem.
Through a series of theoretically-grounded efficiency techniques
developed in this paper, TPOis able to scale to large ABGs with
millions of nodes and hundreds of millions of edges while offering
state-of-the-art result quality. The superiority of TPOover 19 base-
lines is experimentally validated over 5 real ABGs in terms of both
clustering quality and empirical efficiency.
Acknowledgments
Renchi Yang is supported by the NSFC Young Scientists Fund (No.
62302414) and the Hong Kong RGC ECS grant (No. 22202623).
Qichen Wang is supported by Hong Kong RGC Grants (Project
No. C2004-21GF and C2003-23Y). Tsz Nam Chan is supported by
the NSFC grant 62202401. Jieming Shi is supported by Hong Kong
RGC ECS (No. 25201221) and NSFC 62202404.
3790KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
A Theorems and Proofs
Theorem A.1 (Eckart–Young Theorem [ 15]).Suppose that
M𝑘∈R𝑛×𝑘is the rank-𝑘approximation to M∈R𝑛×𝑛obtained by
exact SVD, then min𝑟𝑎𝑛𝑘(bM)≤𝑘∥M−bM∥2=∥M−M𝑘∥2=𝜎𝑘+1,
where𝜎𝑖represents the 𝑖-th largest singular value of M.
Proof of Lemma 2.2. We first rewrite that Eq. (5) as
trace(Z⊤
U·(I−LUL⊤
U)·ZU). (24)
The equivalence can be deduced by the definition of Eq. (5),
O𝑔=1
2∑︁
𝑢𝑖,𝑢𝑗∈Ub𝑤(𝑢𝑖,𝑢𝑗)·ZU[𝑖]√︁
DU[𝑖,𝑖]−ZU[𝑗]√︁
DU[𝑗,𝑗]2
=∑︁
𝑢𝑖,𝑢𝑗∈Ub𝑤(𝑢𝑖,𝑢𝑗)
2 
∥ZU[𝑖]∥2
DU[𝑖,𝑖]+∥ZU[𝑗]∥2
DU[𝑗,𝑗]−2ZU[𝑖]·ZU[𝑗]√︁
DU[𝑖,𝑖]√︁
DU[𝑗,𝑗]!
=∑︁
𝑣ℓ∈V∑︁
𝑢𝑖,𝑢𝑗∈U𝑤(𝑢𝑖,𝑣ℓ)𝑤(𝑢𝑗,𝑣ℓ)
DV[ℓ,ℓ] 
∥ZU[𝑖]∥2
DU[𝑖,𝑖]−ZU[𝑖]·ZU[𝑗]√︁
DU[𝑖,𝑖]√︁
DU[𝑗,𝑗]!
=∑︁
𝑢𝑖∈U∥ZU[𝑖]∥2−∑︁
𝑣ℓ∈V∑︁
𝑢𝑖,𝑢𝑗∈ULU[𝑖,ℓ]·L[𝑗,ℓ]·ZU[𝑖]·ZU[𝑗]
=∑︁
𝑢𝑖ZU[𝑖]·ZU[𝑖]−∑︁
𝑣ℓ∈V(L⊤
UZU)[ℓ]·(L⊤
UZU)[ℓ]
=trace(Z⊤
U·(I−LUL⊤
U)·ZU).
Next, we set the derivative of Eq. (24)w.r.t. ZUto zero and get the
optimal ZUas:
𝜕{(1−𝛼)·∥ZU−XU∥2
𝐹+𝛼·trace(Z⊤
U(I−LUL⊤
U)ZU)}
𝜕ZU=0
=⇒(1−𝛼)·(ZU−XU)+𝛼(I−LUL⊤
U)⊤ZU=0
=⇒ZU=((1−𝛼)·I+𝛼·(I−LUL⊤
U))−1·(1−𝛼)XU.
By Neumann series, i.e.,(I−M)−1=Í∞
𝑘=0M𝑘, we have

(1−𝛼)·I+𝛼(I−LUL⊤
U)−1
=
(1−𝛼)·I+𝛼(I−LUL⊤
U)−1
=
I−𝛼·LUL⊤
U−1
=∞∑︁
𝑟=0𝛼𝑟·(LUL⊤
U)𝑟,
which seals the proof. □
Proof of Lemma 3.1. We first need the following lemma:
Lemma A.2. The optimization objective in Eq. (8)is equivalent to
optimizing: max Ytrace Y⊤SY, where Yis defined in Eq. (10)andS
is a|U|×|U| matrix where S[𝑖,𝑗]=𝑠(𝑢𝑖,𝑢𝑗).
Then, by the connection of the Frobenius norm and trace of
matrices, i.e.,∥M∥2
𝐹=trace(M⊤M)=trace(MM⊤), we have
J=∥R−YH⊤∥2
𝐹=trace(RR⊤−RHY⊤−YH⊤R⊤+YH⊤HY⊤)
=trace(RR⊤)−trace(RHY⊤)−trace(YH⊤R⊤)
+trace(YH⊤HY⊤)
=trace(RR⊤−2YH⊤R⊤)−trace(Y⊤YH⊤H)
=trace(RR⊤−2YH⊤R⊤+H⊤H).
The zero gradient condition𝜕J
𝜕H=−2R⊤Y+2Y=0leads to H=
R⊤Y. Hence,J=trace(RR⊤−2YH⊤R⊤+H⊤H)
=trace(RR⊤)−2 trace(YY⊤RR⊤)+trace(Y⊤RR⊤Y)
=trace(RR⊤)−2 trace(Y⊤RR⊤Y)+ trace(Y⊤RR⊤Y)
=trace(RR⊤)+trace(Y⊤RR⊤Y). (25)
Since trace(RR⊤)is a constant, maximizing J=∥R−YH⊤∥2
𝐹is
equivalent to maximizing trace(Y⊤RR⊤Y)=trace(Y⊤SY). □
Proof of Lemma A.2. LetSbe a|U|×|U| matrix where S[𝑖,𝑗]=
𝑠(𝑢𝑖,𝑢𝑗)andS𝑑be a|U|×|U| diagonal matrix in which S𝑑[𝑖,𝑖]=Í
𝑢𝑗∈U𝑠(𝑢𝑖,𝑢𝑗). Then,
1
|Cℓ|∑︁
𝑢𝑖∈Cℓ,𝑢𝑗∈U\Cℓ𝑠(𝑢𝑖,𝑢𝑗)=1
2∑︁
𝑢𝑖,𝑢𝑗∈US[𝑖,𝑗]·(Y[𝑖,ℓ]−Y[𝑗,ℓ])2
=Y[:,ℓ]⊤·(S𝑑−S)·Y[:,ℓ].
Thus, we can rewrite Eq. (8)asminYtrace(Y⊤(S𝑑−S)Y). Note that
it can be further simplified as max Ytrace Y⊤SY, which completes
the proof. □
Proof of Theorem 3.2. Letz=bZU[𝑖]−bZU[𝑗]andbQ=√︁
𝑑U·
Q. Based on the definition of R′in Eq. (13), we can get
E[R′[𝑖]·R′[𝑗]]=E"
𝑒
𝑑U𝑑U∑︁
ℓ=1𝑠𝑖𝑛(bQ[ℓ]·bZU[𝑖])·𝑠𝑖𝑛(bQ[ℓ]·bZU[𝑗])
−𝑐𝑜𝑠(bQ[ℓ]·bZU[𝑖])·𝑐𝑜𝑠(bQ[ℓ]·bZU[𝑗])#
=E𝑒
𝑑U𝑑U∑︁
ℓ=1𝑐𝑜𝑠(bQ[ℓ]·(bZU[𝑖]−bZU[𝑗]))
=E𝑒
𝑑U𝑑U∑︁
ℓ=1𝑐𝑜𝑠(bQ[ℓ]·z)(26)
By Lemma 5 in [82], for any vector bQ[ℓ],E[𝑐𝑜𝑠(bQ[ℓ]·z)]
𝑒−∥z∥2/2−
1−∥z∥4
4𝑑U≤∥z∥4·(∥z∥4+8∥z∥2+8)
16𝑑2
U.
Note that for each 𝑢𝑖∈U, the row vector bZU[𝑖]is𝐿2normalized
(Line 5), i.e.,∥z∥2∈[0,1]. Based thereon, the above inequality can
be transformed into
1−17
16𝑑2
U−1
4𝑑U≤E[𝑐𝑜𝑠(bQ[ℓ]·z)]
𝑒−∥z∥2/2≤1+17
16𝑑2
U+1
4𝑑U.(27)
According to Line 5 of Algorithm 1, ∥bZU[𝑖]∥2=1∀𝑢𝑖∈U. Thus,
∥bZU[𝑖]−bZU[𝑗]∥2=∥bZU[𝑖]∥2+∥bZU[𝑗]∥2−2bZU[𝑖]·bZU[𝑗]
=2(1−bZU[𝑖]·bZU[𝑗]).
Thus,𝑒·𝑒−∥z∥2
2=𝑒·𝑒−∥bZU[𝑖]−bZU[𝑗]∥2
2 =𝑒bZU[𝑖]·bZU[𝑗]. Combining
Eq. (27) and Eq. (26) leads to
1−17
16𝑑2
U−1
4𝑑U≤E[R′[𝑖]·R′[𝑗]]
𝑒bZU[𝑖]·bZU[𝑗]≤1+17
16𝑑2
U+1
4𝑑U.(28)
Further, Eq. (28) implies
1−17
16𝑑2
U−1
4𝑑U≤E[Í
𝑢ℓ∈UR′[𝑖]·R′[ℓ]]
Í
𝑢ℓ∈U𝑒bZU[𝑖]·bZU[ℓ]≤1+17
16𝑑2
U+1
4𝑑U.
(29)
3791Effective Clustering on Large Attributed Bipartite Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain
According to Eq. (14), we can obtain
E[R[𝑖]·R[𝑗]]=E"
R′[𝑖]·R′[𝑗]√︁
R′[𝑖]·r·√︁
R′[𝑗]·r#
.
Note that, by r’s definition in Eq. (14), we have E[R′[𝑖]·r]=
E[Í
𝑢ℓ∈UR′[𝑖]·R′[ℓ]]. Therefore, plugging Eq. (28)and Eq. (29)
into the above equation proves the theorem. □
Proof of Lemma 3.4. According to Eq. (25), optimizing Eq. (9)
is equivalent to maximizing trace(Y⊤RR⊤Y). Using the cyclic prop-
erty of matrix trace, we have trace(Y⊤RR⊤Y)=trace(YY⊤RR⊤)
andtrace(𝚼⊤RR⊤𝚼)=trace(𝚼𝚼⊤RR⊤). Consequently,∥R−YH⊤∥2
𝐹−∥R−𝚼H⊤∥2
𝐹 (30)
=trace(Y⊤RR⊤Y)− trace(𝚼⊤RR⊤𝚼)
=trace((YY⊤−𝚼𝚼⊤)RR⊤).
The lemma is therefore proved. □
Proof of Lemma 3.3. First, define PUas follows:
PU=(1−𝛼)∞∑︁
𝑟=0𝛼𝑟·
LUL⊤
U𝑟
. (31)
Suppose that 𝚪𝚺𝚿⊤be the exact top- 𝑑SVD of XU, by Eckart–Young
Theorem [ 15] (Theorem A.1), we have ∥𝚪𝚺𝚿⊤−XU∥2≤𝜎𝑑+1,
where𝜎𝑑+1is the(𝑑+1)-th largest singular value of XU. Addition-
ally, we can obtain ∥𝚪𝚺2𝚪⊤−XUX⊤
U∥2≤𝜎2
𝑑+1. By the definition
ofPUin Eq. (31), we have
PU=D1/2
U·𝛾∑︁
𝑟=0𝛼𝑡·
D−1
UBD−1
VB⊤𝑟
·D−1/2
U,
where D−1
UBandD−1
VB⊤are two row-stochastic matrices, i.e., the
entries at each row sum up to 1. Let M=D−1
UBD−1
VB⊤. Since
the multiplication of two row-stochastic matrices yields a row-
stochastic matrix, M=D−1
UBD−1
VB⊤is a row-stochastic matrix,
which further connotes that M𝑟=
D−1
UBD−1
VB⊤𝑟
is row-stochastic,
i.e.,∥M[𝑖]∥1=Í
𝑢ℓ∈UM[𝑖,ℓ]=1∀𝑢𝑖∈U. Hence, given a matrix
𝚷=Í∞
𝑟=0𝛼𝑟M𝑟,
∥𝚷[𝑖]∥1=∞∑︁
𝑟=0𝛼𝑟=1
1−𝛼∀𝑢𝑖∈U.
Therefore, we can derive that
|F[𝑖]·F[𝑗]−ZU[𝑖]·ZU[𝑗]|
=|(PU𝚪𝚺)[𝑖]·(PU𝚪𝚺)[𝑗]−(PUXU)[𝑖]·(PUXU)[𝑗]|
=PU[𝑖]·(𝚪𝚺2𝚪⊤−XUX⊤
U)·PU[𝑗]⊤
=∑︁
𝑢ℓ∈UPU[𝑖,ℓ]·∑︁
𝑢ℎ∈UPU[𝑗,ℎ]·
𝚪𝚺2𝚪⊤−XUX⊤
U
[ℓ,ℎ]
≤∑︁
𝑢ℓ∈UPU[𝑖,ℓ]·∑︁
𝑢ℎ∈UPU[𝑗,ℎ]·𝜎2
𝑑+1
=∑︁
𝑢ℓ∈U√︄
DU[𝑖,𝑖]
DU[ℓ,ℓ]·𝚷[𝑖,ℓ]·∑︁
𝑢ℎ∈U√︄
DU[𝑗,𝑗]
DU[ℎ,ℎ]·𝚷[𝑗,ℎ]·𝜎2
𝑑+1
≤√︁
DU[𝑖,𝑖]·DU[𝑗,𝑗]·𝜎2
𝑑+1
1−𝛼,
which finishes the proof. □References
[1]Hasnat Ahmed, Yangyang Zhang, Muhammad Shoaib Zafar, Nasrullah Sheikh,
and Zhenying Tai. 2020. Node embedding over attributed bipartite graphs. In
KSEM. Springer, 202–210.
[2]Melissa Ailem, François Role, and Mohamed Nadif. 2015. Co-clustering document-
term matrices by direct maximization of graph modularity. In CIKM. 1807–1810.
[3]Sajjad Athar, Rabeeh Ayaz Abbasi, Zafar Saeed, Anwar Said, Imran Razzak,
and Flora D Salim. 2023. ASBiNE: Dynamic Bipartite Network Embedding for
incorporating structural and attribute information. WWW (2023), 1–19.
[4]Cécile Bothorel, Juan David Cruz, Matteo Magnani, and Barbora Micenkova. 2015.
Clustering attributed graphs: models, measures and methods. Network Science 3,
3 (2015), 408–444.
[5]Petr Chunaev. 2019. Community detection in node-attributed social networks: a
survey. arXiv preprint arXiv:1912.09816 (2019).
[6]Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. 2020. Adaptive graph encoder
for attributed graph embedding. In SIGKDD. 976–985.
[7]Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2018. A survey on network
embedding. TKDE 31, 5 (2018), 833–852.
[8]Inderjit S Dhillon. 2001. Co-clustering documents and words using bipartite
spectral graph partitioning. In SIGKDD. 269–274.
[9]Inderjit S Dhillon, Subramanyam Mallela, and Dharmendra S Modha. 2003.
Information-theoretic co-clustering. In SIGKDD. 89–98.
[10] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative
matrix t-factorizations for clustering. In SIGKDD. 126–135.
[11] Issam Falih, Nistor Grozavu, Rushed Kanawati, and Younès Bennani. 2017. Anca:
Attributed network clustering algorithm. In Complex Networks.
[12] Barakeel Fanseu Kamhoua, Lin Zhang, Kaili Ma, James Cheng, Bo Li, and Bo
Han. 2023. Grace: A general graph convolution framework for attributed graph
clustering. TKDD 17, 3 (2023), 1–31.
[13] Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. Bine: Bipartite
network embedding. In SIGIR. 715–724.
[14] Edward Giamphy, Jean-Loup Guillaume, Antoine Doucet, and Kevin Sanchis.
2023. A survey on bipartite graphs embedding. SNAM 13, 1 (2023), 54.
[15] Gene H Gloub and Charles F Van Loan. 1996. Matrix computations. Johns Hopkins
Universtiy Press, 3rd edtion (1996).
[16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. 6.2. 2.3 softmax units
for multinoulli output distributions. Deep learning 180 (2016).
[17] Gérard Govaert and Mohamed Nadif. 2013. Co-clustering: models, algorithms and
applications. John Wiley & Sons.
[18] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In SIGKDD. 855–864.
[19] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions. SIAM review 53, 2 (2011), 217–288.
[20] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. TIIS5, 4 (2015), 1–19.
[21] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means
clustering algorithm. J R Stat Soc Ser C Appl Stat 28, 1 (1979), 100–108.
[22] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual
evolution of fashion trends with one-class collaborative filtering. In WWW. 507–
517.
[23] Wentao Huang, Yuchen Li, Yuan Fang, Ju Fan, and Hongxia Yang. 2020. Biane:
Bipartite attributed network embedding. In SIGIR. 149–158.
[24] Xiao Huang, Jundong Li, and Xia Hu. 2017. Accelerated attributed network
embedding. In ICDM. SIAM, 633–641.
[25] Junghoon Kim, Kaiyu Feng, Gao Cong, Diwen Zhu, Wenyuan Yu, and Chunyan
Miao. 2022. ABC: attributed bipartite co-clustering. PVLDB 15, 10 (2022), 2134–
2147.
[26] Jungeun Kim, Jae-Gil Lee, Byung Suk Lee, and Jiajun Liu. 2020. Geosocial co-
clustering: A novel framework for geosocial community detection. TIST 11, 4
(2020), 1–26.
[27] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[28] Yuval Kluger, Ronen Basri, Joseph T Chang, and Mark Gerstein. 2003. Spectral
biclustering of microarray data: coclustering genes and conditions. Genome
research 13, 4 (2003), 703–716.
[29] Lazhar Labiod and Mohamed Nadif. 2011. Co-clustering for binary and categorical
data with maximum modularity. In ICDM. IEEE, 1140–1145.
[30] Xinying Lai, Dingming Wu, Christian S Jensen, and Kezhong Lu. 2023. A Re-
evaluation of Deep Learning Methods for Attributed Graph Clustering. In CIKM.
1168–1177.
[31] Andrea Lancichinetti, Santo Fortunato, and János Kertész. 2009. Detecting the
overlapping and hierarchical community structure in complex networks. New
journal of physics 11, 3 (2009), 033015.
[32] Daniel Lee and H Sebastian Seung. 2000. Algorithms for non-negative matrix
factorization. NeurIPS 13 (2000).
[33] Yiran Li, Renchi Yang, and Jieming Shi. 2023. Efficient and Effective Attributed
3792KDD ’24, August 25–29, 2024, Barcelona, Spain Renchi Yang et al.
Hypergraph Clustering via K-Nearest Neighbor Augmentation. SIGMOD 1, 2
(2023), 1–23.
[34] Zhao Li, Xin Shen, Yuhang Jiao, Xuming Pan, Pengcheng Zou, Xianling Meng,
Chengwei Yao, and Jiajun Bu. 2020. Hierarchical bipartite graph neural networks:
Towards large-scale e-commerce applications. In ICDE. 1677–1688.
[35] Jie Liu, Zhicheng He, Lai Wei, and Yalou Huang. 2018. Content to node: Self-
translation network embedding. In SIGKDD. 1794–1802.
[36] Weifeng Liu, Jose C Principe, and Simon Haykin. 2011. Kernel adaptive filtering:
a comprehensive introduction. Vol. 57. John Wiley & Sons.
[37] Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan Z.
Li. 2023. Dink-Net: Neural Clustering on Large Graphs. (2023).
[38] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2021.
A unified view on graph neural networks as graph signal denoising. In CIKM.
1202–1211.
[39] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimation of word representations in vector space. arXiv preprint arXiv:1301.3781
(2013).
[40] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013).
[41] Glenn W Milligan and Martha C Cooper. 1985. An examination of procedures
for determining the number of clusters in a data set. Psychometrika 50 (1985),
159–179.
[42] Nairouz Mrabah, Mohamed Bouguessa, Mohamed Fawzi Touati, and Riadh Ksan-
tini. 2022. Rethinking graph auto-encoder models for attributed graph clustering.
TKDE (2022).
[43] Robb J Muirhead. 2009. Aspects of multivariate statistical theory. John Wiley &
Sons.
[44] Guosheng Pan, Yuan Yao, Hanghang Tong, Feng Xu, and Jian Lu. 2021. Unsuper-
vised attributed network embedding via cross fusion. In WSDM. 797–805.
[45] Weisen Pan, Shizhan Chen, and Zhiyong Feng. 2013. Automatic clustering
of social tag using community detection. Applied Mathematics & Information
Sciences 7, 2 (2013), 675–681.
[46] Georgios A Pavlopoulos, Panagiota I Kontou, Athanasia Pavlopoulou, Costas
Bouyioukos, Evripides Markou, and Pantelis G Bagos. 2018. Bipartite graphs in
systems biology and medicine: a survey of methods and applications. GigaScience
7, 4 (2018), giy014.
[47] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
of social representations. In SIGKDD. 701–710.
[48] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Chi Wang, Kuansan Wang, and Jie
Tang. 2019. Netsmf: Large-scale network embedding as sparse matrix factoriza-
tion. In WWW. 1509–1520.
[49] Ali Rahimi and Benjamin Recht. 2007. Random features for large-scale kernel
machines. NeurIPS 20 (2007).
[50] Yuxiang Ren, Hao Zhu, Jiawei Zhang, Peng Dai, and Liefeng Bo. 2021. Ensemfdet:
An ensemble approach to fraud detection based on bipartite graph. In ICDE.
2039–2044.
[51] Yiye Ruan, David Fuhry, and Srinivasan Parthasarathy. 2013. Efficient community
detection in large networks using content and links. In WWW.
[52] Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation.
TPAMI 22, 8 (2000), 888–905.
[53] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In WWW. 1067–1077.
[54] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018.
Verse: Versatile graph embeddings from similarity measures. In WWW. 539–548.
[55] Petar Veličković, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2018. Deep Graph Infomax. In ICLR.
[56] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17, 4 (2007), 395–416.
[57] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang.
2019. Attributed graph clustering: a deep attentional embedding approach. In
IJCAI.
[58] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017.
Mgae: Marginalized graph autoencoder for graph clustering. In CIKM.
[59] Chenguang Wang, Yangqiu Song, Ahmed El-Kishky, Dan Roth, Ming Zhang, and
Jiawei Han. 2015. Incorporating world knowledge to document clustering via
heterogeneous information networks. In SIGKDD. 1215–1224.
[60] Hao Wang, Enhong Chen, Qi Liu, Tong Xu, Dongfang Du, Wen Su, and Xiaopeng
Zhang. 2018. A united approach to learning sparse attributed network embedding.
InICDM. IEEE, 557–566.
[61] David P Woodruff et al .2014. Sketching as a tool for numerical linear algebra.Foundations and Trends® in Theoretical Computer Science 10, 1–2 (2014), 1–157.
[62] Tian Xie, Chaoyang He, Xiang Ren, Cyrus Shahabi, and C-C Jay Kuo. 2022.
L-BGNN: Layerwise Trained Bipartite Graph Neural Networks. TNNLS (2022).
[63] Dongkuan Xu, Wei Cheng, Bo Zong, Jingchao Ni, Dongjin Song, Wenchao Yu,
Yuncong Chen, Haifeng Chen, and Xiang Zhang. 2019. Deep co-clustering. In
SDM. SIAM, 414–422.
[64] Panpan Xu, Nan Cao, Huamin Qu, and John Stasko. 2016. Interactive visual
co-cluster analysis of bipartite graphs. In PacificVis. 32–39.
[65] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-
negative matrix factorization. In SIGIR. 267–273.
[66] Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. 2012. A
model-based approach to attributed graph clustering. In SIGMOD.
[67] Zongyu Xu, Yihao Zhang, Long Yuan, Yuwen Qian, Zi Chen, Mingliang Zhou,
Qin Mao, and Weibin Pan. 2023. Effective Community Search on Large Attributed
Bipartite Graphs. IJPRAI 37, 02 (2023), 2359002.
[68] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. 2023.
Personalized Showcases: Generating multi-modal explanations for recommenda-
tions. In SIGIR. 2251–2255.
[69] Hongqiang Yan, Yan Jiang, and Guannan Liu. 2018. Telecomm fraud detection
via attributed bipartite network. In ICSSSM. 1–6.
[70] Liang Yang, Chuan Wang, Junhua Gu, Xiaochun Cao, and Bingxin Niu. 2021.
Why do attributes propagate in graph convolutional neural networks?. In AAAI,
Vol. 35. 4590–4598.
[71] Renchi Yang. 2022. Efficient and Effective Similarity Search over Bipartite Graphs.
InTheWebConf. 308–318.
[72] Renchi Yang and Jieming Shi. 2024. Efficient High-Quality Clustering for Large
Bipartite Graphs. Proceedings of the ACM on Management of Data 2, 1 (2024),
23:1–23:27.
[73] Renchi Yang, Jieming Shi, Keke Huang, and Xiaokui Xiao. 2022. Scalable and
Effective Bipartite Network Embedding. In SIGMOD. 1977–1991.
[74] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick.
2020. Homogeneous network embedding for massive graphs via reweighted
personalized PageRank. PVLDB 13, 5 (2020), 670–683.
[75] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Sourav S Bhowmick, and
Juncheng Liu. 2023. PANE: scalable and effective attributed network embedding.
VLDBJ (2023), 1–26.
[76] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Juncheng Liu, and Sourav S
Bhowmick. 2020. Scaling attributed network embedding to massive graphs.
PVLDB 14, 1 (2020), 37–49.
[77] Renchi Yang, Jieming Shi, Yin Yang, Keke Huang, Shiqi Zhang, and Xiaokui Xiao.
2021. Effective and scalable clustering on massive attributed graphs. In WWW.
3675–3687.
[78] Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, and Jieming
Shi. 2024. Effective Clustering on Large Attributed Bipartite Graphs. arXiv
preprint arXiv:2405.11922 (2024).
[79] Tianbao Yang, Rong Jin, Yun Chi, and Shenghuo Zhu. 2003. Clustering relational
data using attribute and link information. In Proceedings of the text mining and
link analysis workshop. 9–15.
[80] Tianbao Yang, Rong Jin, Yun Chi, and Shenghuo Zhu. 2009. Combining Link
and Content for Community Detection: A Discriminative Approach. In SIGKDD.
927–936.
[81] Ting Yao, Tao Mei, Chong-Wah Ngo, and Shipeng Li. 2013. Annotation for free:
Video tagging by mining user search behavior. In Multimedia. 977–986.
[82] Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N
Holtmann-Rice, and Sanjiv Kumar. 2016. Orthogonal random features. NeurIPS
29 (2016).
[83] Hongyuan Zha, Xiaofeng He, Chris Ding, Horst Simon, and Ming Gu. 2001.
Bipartite graph partitioning and data clustering. In CIKM. 25–32.
[84] Lili Zhang, Jennifer Priestley, Joseph DeMaio, Sherry Ni, and Xiaoguang Tian.
2021. Measuring customer similarity and identifying cross-selling products by
community detection. Big data 9, 2 (2021), 132–143.
[85] Xiaotong Zhang, Han Liu, Qimai Li, and Xiao-Ming Wu. 2019. Attributed graph
clustering via adaptive graph convolution. In IJCAI. 4327–4333.
[86] Yao Zhang, Yun Xiong, Xiangnan Kong, and Yangyong Zhu. 2017. Learning node
embeddings in interaction graphs. In CIKM. 397–406.
[87] Tao Zhou, Jie Ren, Matúš Medo, and Yi-Cheng Zhang. 2007. Bipartite network
projection and personal recommendation. Physical review E 76, 4 (2007), 046115.
[88] Yang Zhou, Hong Cheng, and Jeffrey Xu Yu. 2009. Graph Clustering Based on
Structural/Attribute Similarities. PVLDB 2 (2009), 12.
[89] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. 2021. Interpreting
and unifying graph neural networks with an optimization framework. In WWW.
1215–1226.
3793