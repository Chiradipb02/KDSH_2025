Maximum-Entropy Regularized Decision Transformer with
Reward Relabelling for Dynamic Recommendation
Xiaocong Chen
Data 61, CSIRO
Eveleigh, Australia
xiaocong.chen@data61.csiro.auSiyu Wang
The University of New South Wales
Sydney, Australia
siyu.wang5@unsw.edu.auLina Yao
Data 61, CSIRO
Eveleigh, Australia
The University of New South Wales
Sydney, Australia
lina.yao@data61.csiro.au
ABSTRACT
Reinforcement learning-based recommender systems have recently
gained popularity. However, due to the typical limitations of simula-
tion environments (e.g., data inefficiency), most of the work cannot
be broadly applied in all domains. To counter these challenges, re-
cent advancements have leveraged offline reinforcement learning
methods, notable for their data-driven approach utilizing offline
datasets. A prominent example of this is the Decision Transformer.
Despite its popularity, the Decision Transformer approach has in-
herent drawbacks, particularly evident in recommendation methods
based on it. This paper identifies two key shortcomings in existing
Decision Transformer-based methods: a lack of stitching capability
and limited effectiveness in online adoption. In response, we intro-
duce a novel methodology named Max-Entropy enhanced Decision
Transformer with Reward Relabeling for Offline RLRS (EDT4Rec).
Our approach begins with a max entropy perspective, leading to the
development of a max-entropy enhanced exploration strategy. This
strategy is designed to facilitate more effective exploration in online
environments. Additionally, to augment the model’s capability to
stitch sub-optimal trajectories, we incorporate a unique reward
relabeling technique. To validate the effectiveness and superiority
of EDT4Rec, we have conducted comprehensive experiments across
six real-world offline datasets and in an online simulator.
CCS CONCEPTS
•Information systems →Recommender systems; •Comput-
ing methodologies →Reinforcement learning.
KEYWORDS
Offline Reinforcement Learning, Recommender Systems, Deep Learn-
ing
ACM Reference Format:
Xiaocong Chen, Siyu Wang, and Lina Yao. 2024. Maximum-Entropy Regu-
larized Decision Transformer with Reward Relabelling for Dynamic Recom-
mendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671750
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36717501 INTRODUCTION
Reinforcement Learning (RL)-based Recommender Systems (RS)
have emerged as powerful tools in a variety of domains, ranging
from e-commerce [ 3,15] and advertising [ 2] to streaming services.
Their effectiveness is particularly pronounced in dynamic environ-
ments, where users’ interests are dynamic [ 9]. In RLRS, an agent
interacts with users by recommending items and receiving feed-
back. This feedback, typically in the form of user responses to
recommendations, serves as rewards that inform the agent’s de-
cisions. Utilizing this feedback loop, the agent continually refines
its policy to better align with user preferences, ultimately aiming
to maximize long-term rewards. This goal often translates into en-
hancing user click with the system. Despite their proven utility,
RLRS frequently suffers data inefficiency, a challenge inherent to
the interaction-centric nature of RL algorithms. This inefficiency
arises as the systems must learn from limited user interactions,
making the process of policy improvement slower and less robust.
Recent studies have proposed using offline RL to address the
challenges faced by RL-based Recommender Systems (RS) [ 7,24,
26]. Known as data-driven RL, offline RL [ 17] leverages extensive
offline datasets for the preliminary training of agents, allowing
for the utilization of pre-existing data to train RLRS agents. A
notable implementation is CDT4Rec [ 24], which integrates the
Decision Transformer (DT) [ 4] as its foundational structure, coupled
with a causal mechanism for reward estimation. Similarly, DT4Rec,
proposed by Zhao et al . [26] , utilizes DT to focus on capturing user
retention, implementing an efficient reward prompting method for
RLRS.
However, directly applying the DT to RS still has several ob-
stacles which require further investigation. The DT requires an
offline dataset that contains sufficient expert trajectories (i.e., those
trajectories should be optimal and dense) that can cover almost all
of the possibilities that the agent may face when interacting with
the real environment. It will lead to two problems when applying
the DT algorithm in RS. Firstly, in RS, the offline dataset is highly
sparse [ 8] which may not be able to generate enough expert trajec-
tories. Under such a scenario, the DT needs to have the capability
to learn from the sub-optimal trajectories. However, recent litera-
ture [ 7,18,21,25,29] indicate that the vanilla DT lacks the capability
of learning from sub-optimal trajectories (i.e., stitching). To better
understand the stitching capability in DT, we have provided an
illustration example under recommendation scenario in Figure 1 to
explain. Consider a user has three previous trajectories consisting of
item click sequences: (𝑖1,𝑖2,𝑖3,𝑖4,𝑖8),(𝑖1,𝑖2,𝑖6,𝑖7), and(𝑖1,𝑖2,𝑖3,𝑖5).
 
376
KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaocong Chen, Siyu Wang, and Lina Yao
Figure 1: An example demonstrates that when DT is directly
applied to RS will face the stitching problem (i.e., cannot
learn from the sub-optimal trajectory).
In this illustration, each node represents an item. Notably, the tra-
jectory(𝑖1,𝑖2,𝑖3,𝑖4,𝑖8)receives a reward of 0 (i.e., the user does not
click it), whereas a similar trajectory (𝑖1,𝑖2,𝑖6,𝑖7)yields a positive
reward. We categorize the 0 reward scenario as sub-optimal. Upon
closer examination, we observe a linkage between items 𝑖4and
𝑖7, suggesting the potential to create a more rewarding trajectory
by combining elements from (𝑖1,𝑖2,𝑖3,𝑖4,𝑖8)and(𝑖1,𝑖2,𝑖6,𝑖7). For
instance, a newly synthesized trajectory (𝑖1,𝑖2,𝑖3,𝑖4,𝑖7)is predicted
to achieve a positive reward. Our reward relabeling strategy ad-
dresses this by assigning weights to each node, ensuring the agent
avoids less rewarding paths like reaching node 𝑖8.
Moreover, since the DT believes that all of the possibilities are
covered in the offline dataset, the exploration will be abandoned
when fine-tuning the online environments [ 18]. It will be crucial
when applying to the RS, since the users’ interests are dynamic and
rapidly changing, and the recorded offline dataset can not reflect
users’ intentions completely. The agent still needs to conduct certain
steps of exploration to collect trajectories as suggested by recent
works [ 5,9]. These limitations hinder the agent’s performance,
particularly when working with sub-optimal offline datasets that do
not fully encapsulate the range of possible user actions, a situation
more akin to real-world scenarios.
To address those two challenges, in this study, we introduce a
novel model, named Max-Entropy enhanced Decision Transformer
with Reward Relabeling for Offline RLRS (EDT4Rec), to overcome
the identified challenges in RL-based Recommender Systems. Draw-
ing inspiration from the Soft Actor-Critic (SAC) approach [ 14],
EDT4Rec incorporates the concept of max-entropy exploration. It
also introduces an innovative reward relabeling strategy, whereby
each node in a trajectory is assigned a reward to foster the gener-
ation of more optimal trajectories and facilitate the learning of a
more effective recommendation policy1. Our contributions in this
work are threefold:
•We propose EDT4Rec, a novel model that integrates max-
entropy exploration and a unique reward relabeling strategy,
enhancing the offline RLRS framework.
1For clarity, we will use "policy" as an abbreviation in subsequent sections.•To address the stitching problem in DT, we design a novel
reward relabeling strategy. Moreover, we design a new max-
entropy exploration mechanism to enable the agent can con-
duct the exploration when fine-tuning on the online envi-
ronments.
•We validate the efficacy of EDT4Rec through extensive ex-
periments on six public datasets and in an online simulation
environment, demonstrating its superiority over existing
methods.
2 PROBLEM FORMULATION
The recommendation problem can be conceptualized as an agent
striving to achieve a specific goal through learning from user inter-
actions, such as item recommendations and subsequent feedback.
This scenario is aptly formulated as a RL problem, where the agent
is trained to interact within an environment, typically described as
a Markov Decision Process (MDP) [ 9]. The components of an MDP
are represented as a tuple (S,A,P,R,𝛾)where,
•StateS: The state space, with 𝑠𝑡∈Srepresenting the state at
timestep𝑡, which normally contain users’ previous interest,
users’ demographic information etc.
•ActionA: The action space, where 𝑎𝑡∈A(𝑠𝑡).𝑎𝑡is the
action taken when given a state 𝑠𝑡. It normally refers to
recommended items.
•Transition Probability P: denoted as 𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡)∈P , is
the probability of transitioning from 𝑠𝑡to𝑠𝑡+1when action
𝑎𝑡is taken.
•RewardR:S×A→ Ris the reward distribution, where
𝑟(𝑠,𝑎)indicates the reward received for taking action 𝑎when
observing the state 𝑠.
•Discount-rate Parameter 𝛾:𝛾∈[0,1]is the discount factor
which use to balance previous reward and immediate reward.
In this context, an RL agent follows its policy 𝜋, a mapping from
states to actions to make recommendations. The objective of RL is
expressed as:
E𝜏∞∑︁
𝑡=0𝛾𝑘𝑟(𝑠𝑡,𝑎𝑡)
Offline reinforcement learning diverges from traditional RL by
exclusively utilizing pre-collected data for training, without the
necessity for further online interaction [ 17]. In offline RL, the agent
is trained to maximize total rewards relies on a static dataset of
transitionsDfor learning. This dataset could comprise previously
collected data or human demonstrations. Consequently, the agent
in offline RL lacks the capability to explore and interact with the
environment for additional data collection. The dataset Din an
offline RL-based RS can be formally described as {(𝑠𝑢
𝑡,𝑎𝑢
𝑡,𝑠𝑢
𝑡+1,𝑟𝑢
𝑡)},
adhering to the MDP framework (S,A,P,R,𝛾). For each user 𝑢at
timestep𝑡, the dataset includes the current state 𝑠𝑢
𝑡∈S, the items
recommended by the agent via action 𝑎𝑢
𝑡, and the user feedback 𝑟𝑢
𝑡.
3 RELATED WORK
RL-based Recommender Systems. Reinforcement learning-based
recommendation methods approach the interactive process of mak-
ing recommendations as a MDP [ 9]. This approach can be cate-
gorized into two primary branches: model-based and model-free
 
377Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
methods. In the realm of model-based techniques, Bai et al . [1]
introduced a method employing generative adversarial training
to simultaneously learn the user behavior model and update the
recommendation policy. Recently, there has been a noticeable shift
in the literature towards embracing model-free techniques for rein-
forcement learning-based recommendations. Notably, Zheng et al.
[28] pioneered the introduction of RL to enhance news recom-
mender systems, and Zhao et al . [27] further extended its appli-
cation to the page-wise recommendation scenario. Both of these
approaches employ Deep Q-Networks (DQN) [ 20] to encode user
and item information, effectively improving the quality of news
recommendations. Moreover, Chen et al . [6] incorporated knowl-
edge graphs into the reinforcement learning framework, resulting
in enhanced decision-making efficiency. Additionally, Chen et al .
[10] introduced a novel generative inverse reinforcement learning
approach for online recommendations. This method autonomously
extracts a reward function from user behavior, further advancing
state-of-the-art in online recommendation systems.
Offline RL in RS. Recent studies have started exploring the prospect
of integrating offline RLRS [ 7]. Notably, Wang et al . [24] intro-
duced a novel model called the Causal Decision Transformer for RS
(CDT4Rec). This model incorporates a causal mechanism designed
to estimate the reward function, offering promising insights into
the offline RL-RS synergy. Similarly, Zhao et al . [26] introduced
the Decision Transformer for RS (DT4Rec), utilizing the vanilla
Decision Transformer as its core architecture to provide recommen-
dations, demonstrating its potential in the field. Furthermore, Gao
et al. [12] delved into the examination of the Matthew effect within
offline RL in Recommender Systems. This work contributes to our
understanding of how offline RLRS can mitigate or address this
phenomenon.
4 METHODOLOGY
4.1 Decision Transformer and Overall Model
Architecture
In this work, we proposed a max-Entropy enhanced Decision Trans-
former with reward relabeling for offline RLRS(EDT4Rec). Our
proposed EDT4Rec leverages the DT framework [ 4], a pivotal el-
ement in recent advancements in offline RLRS, as seen in works
like [ 7,24,26] The Decision Transformer processes a trajectory,
denoted as𝜏, by treating it as a sequence comprising three distinct
types of input: return-to-go (RTG), states, and actions, represented
as(𝑔1,𝑠1,𝑎1,···,𝑔|𝜏|,𝑠|𝜏|,𝑎|𝜏|). Specifically, the initial RTG 𝑅1, cor-
responds to the trajectory’s total return, calculated asÍ|𝜏|
𝑖=0𝑟𝑖. At
each timestep 𝑡, DT utilizes tokens from the most recent 𝐾timesteps
to generate an action 𝑎𝑡. This𝐾is a hyperparameter, defines the
context length the transformer uses for its computations. The deter-
ministic policy learned by DT, denoted as 𝜋𝐷𝑇(𝑎𝑡|s𝐾,𝑡,g𝐾,𝑡), where
s𝐾,𝑡is shorthand for the sequence of 𝐾past states smax(1,𝑡−𝐾+1):𝑡
and similarly for g𝐾,𝑡. A crucial aspect of DT in this context is the
application of a causal mask to predict the sequence of actions.
To simplify the following analysis, we postulate that the data dis-
tribution (i.e., for the dataset), denoted as T, produces subsequences
of actions, states, and RTG values, each of length 𝐾, originating
from the same trajectory. To facilitate our explanation, we slightlydiverge from standard notation and represent a sample from Tas
(a,s,g)where all the vectors contain subelements. This notation
choice simplifies the presentation of our training objective. The
core of DT training strategy involves instructing the policy to accu-
rately predict action tokens. This prediction adheres to the standard
𝑙2loss, which is mathematically represented as:
E(a,s,g)∼T1
𝐾𝐾∑︁
𝑘=1 𝑎𝑘−𝜋𝐷𝑇(s𝐾,𝑘,g𝐾,𝑘)2
. (1)
This equation encapsulates the expectation of the mean squared
difference between the actual action tokens and those predicted by
the DT policy, averaged over all 𝐾elements in the subsequence. In
practice, to implement this approach within the framework of an of-
fline datasetD, we uniformly sample these length- 𝐾subsequences.
In EDT4Rec, we have introduced the following two key modifica-
tions: max-entropy enhanced Exploration and RTG relabeling. The
existing approach of training policies in decision-transformer-based
offline RLRS [ 7,24] on purely offline datasets often encounters a crit-
ical limitation: the trajectories in these datasets usually do not yield
high returns and cover only a restricted portion of the state space.
This limitation stems from two parts: i). the potential distribution
difference between the offline dataset and the online environment
and, ii). the sub-optimality of the training data available offline. A
straightforward strategy to mitigate the distribution difference is
the fine-tuning of pre-trained RL agents through online interac-
tions. However, the learning formulation of a standard decision
transformer does not naturally lend itself to effective online learn-
ing. Moreover, in scenarios with sub-optimal trajectories, the lack
of informative rewards (often zero) hinders effective policy updates.
4.2 Max-Entropy Enhanced Exploration
In the initial phase of our methodology, we propose a probabilistic
learning objective, intended to augment exploration within the
standard DT framework. This setup is geared towards developing a
stochastic policy, with the primary goal of maximizing the likeli-
hood of the observed dataset. For contexts involving continuous
action spaces, we opt for the conventional approach of using a mul-
tivariate Gaussian distribution with a diagonal covariance matrix.
This distribution models the action probabilities, conditional upon
states and RTG values. The policy parameters are represented by 𝜃,
𝜋𝜃(𝑎𝑡|s𝐾,𝑡,g𝐾,𝑡)=N
𝜇𝜃(s𝐾,𝑡,g𝐾,𝑡),∑︁
𝜃(s𝐾,𝑡,g𝐾,𝑡)
,∀𝑡∈𝑇,(2)
whereÍ
𝜃denotes the diagonal covariance matrix.
Consider a stochastic policy, we aim to maximize the log-likelihood
of the trajectories present in our training dataset. This objective is
equivalently achieved by minimizing the negative log-likelihood
(NLL) loss. The loss function, 𝐽(𝜃), is defined as follows:
𝐽(𝜃)=1
𝐾E(a,s,g)∼T[−log𝜋𝜃(a|s,g)]
=1
𝐾E(a,s,g)∼T𝐾∑︁
𝑘=1−log𝜋𝜃(𝑎𝑘|s𝐾,𝑘,g𝐾,𝑘)
. (3)
 
378KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaocong Chen, Siyu Wang, and Lina Yao
RTG
RelablingCausal Decision T ransformer
RTG
RelablingRTG
RelablingCausal Decision T ransformer
OFFLINE
DATASETOFFLINE ONLINE ONLINE ENVIRONMENTinteractionk
Figure 2: The overall structure of the proposed EDT4Rec. The backbone is the causal decision transformer.
It’s important to note that the policies considered here encompass
the deterministic policies utilized by the standard DT. Optimiz-
ing Equation (1) is effectively equivalent to optimizing Equation (3),
under the assumption that the covariance matrixÍ
𝜃is diagonal and
the variances remain constant across all dimensions. This scenario
represents a special case encompassed by our assumption.
A crucial aspect of an online RL algorithm is its capacity to
balance exploration and exploitation. However, the traditional for-
mulation of a DT, as seen in Equation (3), does not inherently facil-
itate exploration. To remedy this, we can use the policy entropy to
encourage exploration, defined as:
𝐻T
𝜃[a|s,g]=1
𝐾E(s,g)∼T
𝐻(𝜋𝜃(a|s,g))
=1
𝐾E(s,g)∼T𝐾∑︁
𝑘=1𝐻[(𝜋𝜃(𝑎𝑘|s𝐾,𝑘,g𝐾,𝑘)]
,(4)
where𝐻[𝜋𝜃(𝑎𝑘)]denotes the entropy of the distribution 𝜋𝜃(𝑎𝑘).
This policy entropy is contingent on the data distribution T, which
remains static during the offline pretraining phase but becomes
dynamic in the finetuning phase as it incorporates online data ac-
quired through exploration. In line with max-entropy RL algorithms
like SAC [ 14], we explicitly impose a lower bound on the policy
entropy to encourage exploration. We aim to solve the following
constrained problem:
min
𝜃𝐽(𝜃)subject to𝐻T
𝜃[a|s,g]≥𝛽, (5)
where𝛽is a predetermined hyperparameter. Practically, we address
the dual problem of Equation (5) to avoid the direct handling of the
inequality constraint. Thus, we consider the Lagrangian,
max
𝜆≥0min
𝜃𝐿(𝜃,𝜆)where𝐿(𝜃,𝜆)=𝐽(𝜃)+𝜆(𝛽−𝐻T
𝜃[a|s,g]).(6)
The solution involves alternating optimization of 𝜃and𝜆. Specifi-
cally, optimizing 𝜃with fixed𝜆is is achieved by,
min
𝜃𝐽(𝜃)−𝜆𝐻T
𝜃[a|s,g], (7)
and optimizing 𝜆with a fixed 𝜃,
min
𝜆≥0𝜆(𝐻T
𝜃[a|s,g]−𝛽). (8)
Next, we will conduct a theoretical analysis to delineate how our
EDT4Rec differs from SAC.Equation (7) represents the regularized form of our primal prob-
lem as outlined in Equation (6), where the dual variable 𝜆assumes
the role of a temperature variable, a common element in many regu-
larized RL formulations. A fundamental distinction of our approach
from SAC and other traditional RL methods lies in our loss function
𝐽(𝜃), which is defined as the negative log-likelihood (NLL) rather
than the discounted return. This shift signifies a focus on super-
vised learning of action sequences rather than explicitly aiming to
maximize returns. Therefore, the objective in Equation (7) can be
interpreted as minimizing the expected discrepancy between the
log-probability of observed actions and the 𝜆-scaled log-probability
of actions sampled from 𝜋𝜃(·|s,g). In essence, we aim to train 𝜋𝜃
to approximate the observed action distribution, allowing for a
controlled degree of deviation, with 𝜆explicitly regulating this
mismatch.
It’s important to note that 𝐻T
𝜃[a|s,g]is technically a cross con-
ditional entropy. This arises because the training data distribution
Tis not identical to the action-state-RTG marginals induced by
the current policy 𝜋𝜃and the transition probability P. In the of-
fline training phase, Tis a fixed offline data distribution, T𝑜𝑓𝑓𝑙𝑖𝑛𝑒 ,
while during the online phase, Tis represented through a replay
buffer that stores online data and is dependent on the current policy
𝜋𝑡ℎ𝑒𝑡𝑎 and previously gathered data.
It is worth mentioning that our policy entropy, as defined in Equa-
tion (4), operates at the sequence level rather than at the transition
level. This distinction leads to a significant difference in the con-
straints applied in our primal problem (Equation (6)) compared
to those used in the SAC framework. For the sake of simplicity,
if we momentarily set aside the RTG variable gin our policy en-
tropy calculation, we can see a clear contrast. While SAC imposes
a lower bound 𝛽on the policy entropy at each timestep, our en-
tropy𝐻T
𝜃[a|s,g]is computed as an average over 𝐾consecutive
timesteps.
4.3 RTG Relabeling
The reward conditioning approach prevalent in prior works typi-
cally involves conditioning on an entire trajectory sequence using
the sum of rewards for that sequence (i.e.,Í|𝜏|
𝑖=0𝑟𝑖). However, this
method encounters difficulties in tasks that require stitching – the
 
379Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
process of learning an optimal policy from sub-optimal trajecto-
ries by effectively combining them. By contrast, the Q-learning
approach addresses this by independently propagating the value
function backwards at each timestep using the Bellman backup. It
integrates information for each state across different trajectories,
thereby circumventing the stitching issue inherent in the reward
conditioning approach. Our proposed solution to this challenge
involves a novel approach: relabeling the RTG values using learned
Q-functions. By applying this relabeling to the dataset, the reward
conditioning approach can then effectively utilize optimal segments
from within sub-optimal trajectories. This enhancement allows for
the extraction and integration of valuable sub-trajectory segments,
thus addressing the fundamental limitation of the original reward
conditioning method in tasks requiring effective stitching of policy
learning.
In addressing the relabeling of RTG values with learned Q-
functions, it’s important to recognize that simply substituting all
RTG values with Q-function estimates is not always effective. This
is particularly true in scenarios characterized by long time horizons
and sparse rewards, where the accuracy of learned Q-functions can
vary significantly. Our goal is to selectively replace RTG values
in instances where the learned Q-functions provide reliable esti-
mations. To achieve this, we incorporate the CQL framework [ 16],
renowned for learning the lower bounds of value functions in offline
Q-learning algorithms. The relabeling of RTG values is conducted
selectively: it occurs when the RTG within a trajectory is lower
than the lower bound estimated by CQL. This approach ensures
that RTG values are replaced only when the learned value function
is reasonably accurate or closer to the true value. Furthermore, our
method extends the impact of this relabeling process through the
trajectory. We apply reward recursion, defined as 𝑅𝑡−1=𝑟𝑡−1+𝑅𝑡,
to propagate the revised RTG values to all preceding timesteps in
the trajectory. This ensures that the influence of the adjusted RTG
is not isolated but is instead integrated throughout the trajectory,
enhancing the overall consistency and accuracy of our approach.
To implement our approach, we begin by initializing the RTG
of the final state in a trajectory to zero, denoted as 𝑅𝑇=0. Then,
we proceed in reverse chronological order, starting from the end
of the trajectory and moving backwards towards the initial state.
The process involves a series of steps at each state in the trajectory.
Firstly, for the current state 𝑠𝑡, we compute its state value using the
learned value function, expressed as ˆ𝑉(𝑠𝑡)=E𝑎∼𝜋(𝑎|𝑠𝑡)[ˆ𝑄(𝑠𝑡,𝑎)],
where𝜋represents the learned policy. This value function estima-
tion reflects the expected value of actions taken in the current state
according to the policy. Next, we compare this computed value
function, ˆ𝑉(𝑠𝑡), with the existing RTG value at the same state (i.e.,
𝑅(𝑡)). Ifˆ𝑉(𝑠𝑡)is greater than 𝑅𝑡, we update the RTG for the previ-
ous timestep 𝑅𝑡−1to the sum of the reward at timestep 𝑡−1,𝑟𝑡−1
and the estimated state value ˆ𝑉(𝑠𝑡). If not,𝑅𝑡−1is set to𝑟𝑡−1+𝑅𝑡.
This relabeling process is repeated for each state in the trajectory
until we reach the initial state. Through this backward iteration,
we effectively update the RTG values based on the learned value
function, ensuring that the RTG reflects a more accurate estimate
of the expected returns from each state.
The relabeling process described earlier, while effective, can po-
tentially disrupt the inherent consistency between rewards andRTG values in the input sequence of a Decision Transformer (DT).
Ideally, the RTG value should always align with the sum of future
rewards, adhering to the relationship 𝑅𝑡=𝑟𝑡+𝑅𝑡+1. However, the
relabeling process may inadvertently violate this rule. To preserve
this critical consistency within the DT input sequence, we adopt
a method of regenerating the RTG values. For an input sequence
{ˆ𝑅𝑡−𝐾+1,···,ˆ𝑅𝑡−1,ˆ𝑅𝑡}, we start by setting the last RTG ˆ𝑅𝑡=𝑅𝑡.
Then, we apply the formula ˆ𝑅𝑡′=𝑟′
𝑡+ˆ𝑅𝑡′+1in a backward sequence
until we reach 𝑡′=𝑡−𝐾+1. By doing so, we maintain the consis-
tency between rewards and RTGs throughout the sequences. The
comprehensive algorithm of RTG relabeling process can be found
in Algorithm 1.
Algorithm 1: RTG Relabeling without inconsistency
input : reward𝑟1:𝑇, learned value function ˆ𝑉(𝑠),
Trajectory length 𝑇, context length 𝐾
output: RTG ˆ𝑅1:𝑇
// Step 1. relabel RTG 𝑅𝑇
1𝑅𝑇←0;
2𝑖←𝑇;
3while i > 0do
4𝑅𝑡−1←𝑟𝑖+max(𝑅𝑡,ˆ𝑉(𝑠𝑖);
5𝑖←𝑖−1;
6end
// Step 2. relabel RTG ˆ𝑅𝑇by considering the
inconsistency
7ˆ𝑅𝑇←𝑅𝑡;
8𝑖←𝑇;
9while i > 0do
10 ˆ𝑅𝑡←𝑟𝑖+ˆ𝑅𝑖+1;
11𝑖←𝑖−1;
12end
A potential concern with our relabeling strategy is validating that
the learned value function indeed represents the lower bound of
the true value function, meaning it should be as close as possible to
the optimal value function. To address this, we utilize the CQL [ 16]
to learn the Q-value which is defined as,
ˆ𝑄←arg min
𝑄𝛼(E𝑠∼D,𝑎∼𝜇(𝑎|𝑠)[𝑄(𝑠,𝑎)]−E𝑠,𝑎∼D[𝑄(𝑠,𝑎)])
+1
2E𝑠,𝑎,𝑠′∼D,𝑎′∼𝜇(𝑎′|𝑠′)[(𝑟(𝑠,𝑎)+𝛾ˆ𝑄(𝑠′,𝑎′)−𝑄(𝑠,𝑎))2].
(9)
Supporting this approach, we reference a theorem from the CQL
framework (the proof of which can be found in the original paper):
Theorem 1 (Lower Bound of CQL [ 16]).The value of the policy
under the Q-function from Equation (9),ˆ𝑉𝜋(s)=E𝜋(a|s)[ˆ𝑄𝜋(s,a)],
lower-bounds the true value of the policy obtained via exact policy
evaluation.
In practice, the optimal value function 𝑄∗(𝑠,𝑎)is unknown, com-
pelling DT to rely on RTG as an alternative. RTG values, typically
gathered through the behavior policy or policies, are often not op-
timal and tend to be significantly lower than their optimal value
 
380KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaocong Chen, Siyu Wang, and Lina Yao
function counterparts, 𝑄∗(𝑠,𝑎)This discrepancy presents a chal-
lenge in accurately modeling the optimal policy. However, by em-
ploying CQL for learning the Q-function, we can effectively address
this challenge. The aforementioned theorem from the CQL frame-
work assures that the value function estimated under CQL acts as
a reliable lower bound of the true policy value. Consequently, this
theorem supports the premise that our relabeling process, guided
by the learned Q-function, shifts the RTG values in the training
dataset closer to the optimal value function.To further substanti-
ate this claim, we also provide a straightforward proof. This proof
demonstrates how the application of CQL in learning the Q-function
ensures that the relabeled RTG values in our training dataset are
more aligned with what would be expected from the optimal value
function, thus enhancing the overall accuracy and effectiveness of
the DT in policy modeling.
4.4 Training Procedure
We implement the aforementioned formulation through a trans-
former architecture, adapting and extending the existing DT frame-
work. In EDT4RRec, we introduce modifications that align with the
changes we’ve previously outlined. To predict the policy’s mean
and log variance, we utilize two separate fully connected layers at
the output stage of the model. This architectural choice allows for
a more nuanced and effective representation of the policy’s charac-
teristics. Algorithm 2 summarizes the overall training pipeline in
the proposed method.
Algorithm 2: Overall training algorithm
input : offline datasetD, exploration RTG g𝑜𝑛𝑙𝑖𝑛𝑒 , reply
bufferD𝑟𝑒, number of round 𝑖, number of iteration
𝐼, batch size𝐵, target policy parameter 𝜃
1D𝑟𝑒←top𝑁trajectories fromD;
2forround 1···𝑖do
3 Trajectory𝜏←𝜋𝜃(·|s,g𝑜𝑛𝑙𝑖𝑛𝑒);
// if theD𝑟𝑒is full, the oldest
trajectories will be removed.
4D𝑟𝑒←D𝑟𝑒∪𝜏;
5 Compute the trajectory sampling probability
𝑝(𝜏)=|𝜏|/Í
𝜏∈D|𝜏|;
6 for𝑡=1···𝐼do
7 Sample𝐵trajectories fromD𝑟𝑒;
8 foreach trajectory 𝜏do
9 g←relabeling using Algorithm 1;
10(a,s,g)← a length of𝐾sub-trajectory
uniformly sampled from 𝜏;
11 end
12𝜃←one gradient update using the sampled (a,s,g);
13 end
14end
In our implementation, both the online goal g𝑜𝑛𝑙𝑖𝑛𝑒 and the
context length 𝐾are set to 2, a decision whose rationale and impli-
cations are further explored in our hyperparameter study.5 EXPERIMENTS
In this section, we report the outcomes of experiments that focus
on the following three main research questions:
•RQ1: How does EDT4Rec compare with other DT-based
methods and traditional deep RL algorithms in online recom-
mendation environments and offline dataset environments?
•RQ2: How do the hyper-parameters affect the performance
in the online simulation environment?
•RQ3: How does each component in EDT4Rec contribute to
the final performance in the online simulation environment?
We concentrate our hyper-parameters and ablation study (i.e., RQ2
and RQ3) on online simulation settings since they are more closely
suited to real-world environments, whereas offline datasets are
fixed and do not reflect users’ dynamic interests.
5.1 Datasets and Environments
In this section, we evaluate the performance of our proposed algo-
rithm, EDT4Rec, against other state-of-the-art algorithms, employ-
ing both real-world datasets and an online simulation environment.
We first introduce six diverse, public real-world datasets from vari-
ous recommendation domains for our offline experiments:
•Kuairand-1k-15policies [13]: An unbiased sequential rec-
ommendation dataset sourced from recommendation logs of
a video-sharing mobile app.
•LibraryThing: This online service facilitates book cata-
loging for users2. It is notable for its inclusion of social
relationships between users, making it ideal for studying
social recommendation algorithms.
•MovieLens: We utilize the MovieLens-20M dataset3, a widely
recognized benchmark in recommender system research.
•GoodReads: Sourced from the book review website GoodReads,
this dataset, compiled by [ 23], includes various user interac-
tions like ratings and reviews.
•Netflix: Originating from the Netflix Prize Challenge4, this
well-known dataset primarily consists of user ratings.
•Book-Crossing: Similar to MovieLens, this book-related
dataset by [30] focuses on rating information.
To evaluate EDT4Rec’s performance, we transformed the offline
datasets into simulation environments that allow for interactive
reinforcement learning experiences. This conversion aligns with
methodologies outlined in existing research [ 6,24,26]. Specifi-
cally, user interactions are converted into binary click behaviors to
construct trajectories. For datasets containing ratings, any rating
exceeding 75% of the maximum rating is considered positive feed-
back, while the rest are treated as negative. The evaluation process
adheres to the standards established in previous works [24].
In addition, we also experiment on a real online simulation plat-
form to validate the proposed method. We use VirtualTB [ 22] as
the major online platform in this work. In terms of evaluation met-
rics, we use the click-through rate (CTR) for the online simulation
platform as the CTR is one of the built-in evaluation metrics of
2https://www.librarything.com/
3https://grouplens.org/datasets/movielens/20m/
4https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data
 
381Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The overall results of our model comparison with several state-of-the-art models on different datasets. The highest
results are in bold and significant under the 95% confidence interval
Dataset Kuairand-1k Librar
ything
Measur
e (%) Re
call Pr
ecision nDCG Re
call Pr
ecision nDCG
DDPG 29.441±0.321
28.774±0.233 29.773±0.298 12.128±0.241
12.451±0.242 13.925±0.252
SAC 29.204±0.292
28.162±0.205 29.455±0.201 12.028±0.237
12.311±0.255 13.125±0.204
TD3 29.115±0.281
27.989±0.247 29.107±0.188 11.943±0.205
12.018±0.278 12.878±0.218
DT 30.082±0.212
29.951±0.215 30.134±0.160 14.225± 0.147
13.967±0.207 13.987±0.102
DT4Rec 30.104±0.233
29.989±0.201 30.156±0.145 14.333± 0.156
13.984±0.224 14.024±0.146
CDT4Rec 30.322±0.208
30.014±0.178 30.525±0.168 15.229±0.128
14.020±0.201 14.768±0.176
ED
T4Rec 31.256±0.241
31.322±0.203 31.321±0.203 16.021±0.244
15.124±0.244 15.642±0.201
Dataset Bo
ok-Crossing Go
odReads
Measur
e (%) Re
call Pr
ecision nDCG Re
call Pr
ecision nDCG
DDPG 7.442±0.452
5.315±0.232 5.824±0.281 11.652±0.188
10.246±0.142 9.941±0.189
SAC 7.211±0.293
5.114±0.390 5.724±0.226 11.452±0.242
10.027±0.181 9.721±0.220
TD3 7.025±0.193
5.105±0.179 5.665±0.332 11.028±0.199
9.876±0.167 9.521±0.246
DT 8.375±0.322
6.892±0.188 7.587±0.228 12.957±0.177
10.998±0.169 10.291±0.206
DT4Rec 8.667±0.221
6.984±0.149 7.833±0.189 12.972±0.189
10.994±0.191 10.425±0.298
CDT4Rec 9.234±0.123
7.226±0.289 8.276±0.279 13.274±0.287
11.276±0.175 10.768±0.372
ED
T4Rec 10.345±0.102
8.254±0.176 9.141±0.234 14.144±0.201
12.422±0.145 11.641±0.245
Dataset Mo
vieLens-20M Netflix
Measur
e (%) Re
call Pr
ecision nDCG Re
call Pr
ecision nDCG
DDPG 17.622±0.251
15.889±0.241 15.802±0.201 13.305±0.235
11.788±0.213 10.834±0.214
SAC 17.782±0.204
15.989±0.229 15.821±0.212 13.324±0.210
11.981±0.246 10.970±0.270
TD3 17.531±0.291
15.866±0.261 15.731±0.302 13.231±0.284
11.676±0.204 10.789±0.266
DT 18.889±0.224
16.989±0.242 16.912±0.202 14.598±0.202
12.251±0.141 11.864±0.242
DT4Rec 18.971±0.254
17.013±0.289 16.942±0.248 14.879±0.254
12.786±0.198 11.961±0.261
CDT4Rec 19.273±0.212
17.371±0.276 17.311±0.216 15.271±0.127
13.274±0.168 12.479±0.198
ED
T4Rec 20.314±0.201
18.341±0.221 18.234±0.186 16.541±0.201
14.356±0.181 13.661±0.145
the VirtualTB simulation environment. For offline dataset evalua-
tion, we employ a variety of evaluation metrics, including recall,
precision, and normalized discounted cumulative gain (nDCG).
5.2 Baselines
In our study, we focus on evaluating EDT4Rec within the context
of DT-based methods. Most existing works in RLRS have been
assessed using customized settings, which makes fair compari-
son challenging [ 9]. To address this, we have selected a range of
baselines, encompassing both prominent DT-based methods and
well-established RL algorithms, for a comprehensive evaluation:
•Deep Deterministic Policy Gradient (DDPG) [19]: An off-
policy method designed for environments with continuous
action spaces.
•SAC [14]: This approach is an off-policy, maximum entropy
Deep RL method that focuses on optimizing a stochastic
policy. While we are using deterministic policy here.
•Twin Delayed DDPG (TD3) [11]: An enhancement of the
baseline DDPG, TD3 improves performance by learning two
Q-functions, updating the policy less frequently.
•DT[4]: An offline RL algorithm that leverages the trans-
former architecture to infer actions.•DT4Rec [26]: Building on the standard DT framework, DT4Rec
integrates a conservative learning method to better under-
stand users’ intentions in offline RLRS.
•CDT4Rec [24]: This model introduces a causal layer to the
DT framework, aiming to more effectively capture user in-
tentions and preferences in offline RLRS.
5.3 Overall Comparison (RQ1)
The summarized results of our experiments with the offline dataset
are presented in Table 1. Notably, EDT4Rec demonstrates superior
performance over all baseline methods, including those DT based
offline RLRS and traditional RL approaches. A key observation is the
general superiority of DT-based methods over RL-based methods,
likely attributable to the transformer framework’s expressiveness.
In our online simulator experiments, EDT4Rec was compared
against the baselines mentioned earlier. The comparative results, as
detailed in Figure 3a, reveal a significant improvement by EDT4Rec,
particularly in the later timesteps. Moreover, we observe that DT-
based methods outperform traditional RL algorithms. The reason
would be those methods accessing more recorded offline trajectories
and benefiting from the transformer’s high expressiveness.
 
382KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaocong Chen, Siyu Wang, and Lina Yao
(a)
 (b)
 (c)
Figure 3: (a). Overall comparison result with variance between the baselines and EDT4Rec in the VirtualTaobao simulation en-
vironment. (b).Hyperparameter 𝑔𝑜𝑛𝑙𝑖𝑛𝑒 Study, the value reported in the average CTR over 100,000timesteps. (c).Hyperparameter
𝐾Study, the value reported in the average CTR over 100,000timesteps
However, it’s observed that DT, DT4Rec, and CDT4Rec perform
worse than EDT4Rec, given that the pre-collected data are sub-
optimal. This observation reinforces our assertion that DT lacks
exploration capability and that its pre-training stage closely resem-
bles behavior cloning.
5.4 Hyperparameter Study (RQ2)
In this section, we delve into the hyperparameter analysis for our
proposed EDT4Rec model, particularly examining the impacts of
g𝑜𝑛𝑙𝑖𝑛𝑒 and𝐾. The hyperparameter g𝑜𝑛𝑙𝑖𝑛𝑒 , set at 2 in our experi-
ments, plays a crucial role in regulating exploration. Considering
that the maximum reward per interaction is capped (e.g., at 10 in
the VirtualTB environment), a smaller g𝑜𝑛𝑙𝑖𝑛𝑒 value is preferable
to encourage exploration over exploitation during the fine-tuning
phase. The results, illustrating the performance of EDT4Rec with
varying g𝑜𝑛𝑙𝑖𝑛𝑒 , are presented in Figure 3b. We observe that the
optimal performance is achieved when g𝑜𝑛𝑙𝑖𝑛𝑒 is set to 2. Turning
our attention to the context length 𝐾, this parameter determines
the number of previous actions accessible to the RTG. Following
insights from previous work [ 24] that suggest a smaller context
length is beneficial, we explore a range of 𝐾values:{2,4,6,8,10},
while keeping g𝑜𝑛𝑙𝑖𝑛𝑒 fixed at 2. Similar to g𝑜𝑛𝑙𝑖𝑛𝑒 , the average CTR
over 100,000 timesteps for different 𝐾values is reported in Figure 3c.
It becomes evident that the proposed EDT4Rec model reaches its
peak performance when both 𝐾andg𝑜𝑛𝑙𝑖𝑛𝑒 are set to 2.
5.5 Ablation Study (RQ3)
In order to comprehensively understand the impact of exploration
and reward relabeling on the proposed EDT4Rec model, we con-
ducted an ablation study. This study isolates the effects of these
components by evaluating two variations of EDT4Rec: EDT4Rec-
E, which excludes the online exploration goal, and EDT4Rec-R,
which omits the reward relabeling feature. The results of this study,
shows in Figure 4, provides some insightful trends. Notably, in the
absence of the exploration component (EDT4Rec-E), the model ini-
tially outperforms the complete EDT4Rec. This could be attributed
to EDT4Rec-E solely leveraging exploitation based on previously
learned policies without engaging in exploration. However, as theagent encounters unfamiliar states over time, the lack of exploration
leads to a decline in performance due to the inadequacy of the ex-
isting policy in making accurate recommendations in these new
scenarios. In addition, the removal of the reward relabeling compo-
nent in EDT4Rec-R results in a significant drop in performance. This
outcome underscores the critical role of online exploration in the ef-
fectiveness of EDT4Rec. The inability to adaptively relabel rewards
based on online experiences evidently hampers the model’s ability
to effectively navigate and learn from the environment, thereby
highlighting the essential contribution of each component to the
overall success of the EDT4Rec model. It also support out claim
about that DT lacks the capability of stitching.
Figure 4: Ablation Study
6 CONCLUSION
In this study, we introduced Max-Entropy enhanced Decision Trans-
former with Reward Relabeling for Offline RLRS (EDT4Rec), a novel
model crafted to overcome two significant challenges in Decision
Transformer (DT)-based systems: i) the lack of stitching capability,
and ii) insufficient online exploration, which can lead to sub-optimal
performance in scenarios with sub-optimal datasets. Our compre-
hensive experiments demonstrate that EDT4Rec surpasses existing
 
383Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
DT-based methods in performance. Looking ahead, our future work
will delve deeper into the stitching problem. Although the current
reward relabeling strategy in EDT4Rec relies on learned value func-
tions from optimal trajectories, such trajectories may not always
be available. Therefore, we aim to develop a more straightforward
method of reward relabeling, one that does not necessitate reliance
on optimal trajectories. This advancement will further enhance
the applicability and robustness of EDT4Rec in diverse real-world
scenarios.
REFERENCES
[1]Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement
learning with adversarial training for online recommendation. Advances in Neural
Information Processing Systems 32 (2019).
[2]Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and
Defeng Guo. 2017. Real-time bidding by reinforcement learning in display adver-
tising. In Proceedings of the tenth ACM international conference on web search and
data mining. 661–670.
[3]Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei Zhang. 2018.
Reinforcement Mechanism Design for e-commerce. In Proceedings of the 2018
World Wide Web Conference. 1339–1348.
[4]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,
Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:
Reinforcement learning via sequence modeling. Advances in neural information
processing systems 34 (2021), 15084–15097.
[5]Minmin Chen. 2021. Exploration in recommender systems. In Proceedings of the
15th ACM Conference on Recommender Systems. 551–553.
[6]Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie Zhang, et al .
2020. Knowledge-guided deep reinforcement learning for interactive recommen-
dation. In 2020 International Joint Conference on Neural Networks (IJCNN). IEEE,
1–8.
[7]Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, and Lina Yao.
2023. On the opportunities and challenges of offline reinforcement learning for
recommender systems. ACM Transactions on Information Systems (2023).
[8]Xiaocong Chen, Lina Yao, Julian McAuley, Weili Guan, Xiaojun Chang, and
Xianzhi Wang. 2022. Locality-sensitive state-guided experience replay optimiza-
tion for sparse rewards in online recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 1316–1325.
[9]Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang.
2023. Deep reinforcement learning in recommender systems: A survey and new
perspectives. Knowledge-Based Systems 264 (2023), 110335.
[10] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu.
2021. Generative inverse deep reinforcement learning for online recommenda-
tion. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 201–210.
[11] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function
Approximation Error in Actor-Critic Methods. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,
Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80). PMLR,
1582–1591. http://proceedings.mlr.press/v80/fujimoto18a.html
[12] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang,
Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating matthew effect
of offline reinforcement learning in interactive recommendation. arXiv preprint
arXiv:2307.04571 (2023).
[13] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng
Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommen-
dation Dataset with Randomly Exposed Videos. In Proceedings of the 31st ACM
International Conference on Information and Knowledge Management (Atlanta,
GA, USA) (CIKM ’22). 3953–3957. https://doi.org/10.1145/3511808.3557624[14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In International conference on machine learning . PMLR, 1861–
1870.
[15] Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. 2018. Reinforce-
ment learning to rank in e-commerce search engine: Formalization, analysis, and
application. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining. 368–377.
[16] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-
tive q-learning for offline reinforcement learning. Advances in Neural Information
Processing Systems 33 (2020), 1179–1191.
[17] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline rein-
forcement learning: Tutorial, review, and perspectives on open problems. arXiv
preprint arXiv:2005.01643 (2020).
[18] Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, and Deheng
Ye. 2023. A survey on transformers in reinforcement learning. arXiv preprint
arXiv:2301.03044 (2023).
[19] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control
with deep reinforcement learning. In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings. http://arxiv.org/abs/1509.02971
[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[21] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. 2020. Awac:
Accelerating online reinforcement learning with offline datasets. arXiv preprint
arXiv:2006.09359 (2020).
[22] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019.
Virtual-Taobao: Virtualizing Real-World Online Retail Environment for Reinforce-
ment Learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 4902–4909.
[23] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic
Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender
Systems (Vancouver, British Columbia, Canada) (RecSys ’18). Association for
Computing Machinery, New York, NY, USA, 86–94. https://doi.org/10.1145/
3240323.3240369
[24] Siyu Wang, Xiaocong Chen, Dietmar Jannach, and Lina Yao. 2023. Causal Decision
Transformer for Recommender Systems via Offline Reinforcement Learning. In
Proceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval (Taipei, Taiwan) (SIGIR ’23). Association
for Computing Machinery, New York, NY, USA, 1599–1608. https://doi.org/10.
1145/3539618.3591648
[25] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. 2023. Q-learning
decision transformer: Leveraging dynamic programming for conditional sequence
modelling in offline rl. In International Conference on Machine Learning. PMLR,
38989–39007.
[26] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, and Dawei Yin. 2023. User
Retention-oriented Recommendation with Decision Transformer. In Proceedings
of the ACM Web Conference 2023. 1141–1149.
[27] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang
Tang. 2018. Deep reinforcement learning for page-wise recommendations. In
Proceedings of the 12th ACM Conference on Recommender Systems. 95–103.
[28] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,
Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework
for news recommendation. In Proceedings of the 2018 World Wide Web Conference .
167–176.
[29] Qinqing Zheng, Amy Zhang, and Aditya Grover. 2022. Online decision trans-
former. In international conference on machine learning. PMLR, 27042–27059.
[30] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005.
Improving Recommendation Lists through Topic Diversification. In Proceedings
of the 14th International Conference on World Wide Web (Chiba, Japan) (WWW
’05). Association for Computing Machinery, New York, NY, USA, 22–32. https:
//doi.org/10.1145/1060745.1060754
 
384