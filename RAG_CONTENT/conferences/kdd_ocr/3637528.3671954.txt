TSC: A Simple Two-Sided Constraint against Over-Smoothing
Furong Peng
Kang Liu
pengfr@sxu.edu.cn
not_have@163.com
Institute of Big Data Science and Industry/School of
Computer and Information Technology, Shanxi University
Taiyuan, Shanxi, ChinaXuan Lu∗
luxuan@sxu.edu.cn
College of Physics and Electronic Engineering, Shanxi
University
Taiyuan, Shanxi, China
Yuhua Qian
Hongren Yan
jinchengqyh@sxu.edu.cn
hyan_sx@hotmail.com
Institute of Big Data Science and Industry/School of
Computer and Information Technology, Shanxi University
Taiyuan, Shanxi, ChinaChao Ma
HOPERUN Information Technology
Nanjing, Jiangsu, China
AISuperMa@outlook.com
ABSTRACT
Graph Convolutional Neural Network (GCN), a widely adopted
method for analyzing relational data, enhances node discriminabil-
ity through the aggregation of neighboring information. Usually,
stacking multiple layers can improve the performance of GCN
by leveraging information from high-order neighbors. However,
the increase of the network depth will induce the over-smoothing
problem, which can be attributed to the quality and quantity of
neighbors changing: (a) neighbor quality, node’s neighbors become
overlapping in high order, leading to aggregated information be-
coming indistinguishable, (b) neighbor quantity, the exponentially
growing aggregated neighbors submerges the node’s initial feature
by recursively aggregating operations. Current solutions mainly
focus on one of the above causes and seldom consider both at once.
Aiming at tackling both causes of over-smoothing in one shot,
we introduce a simple Two-Sided Constraint (TSC) for GCNs, com-
prising two straightforward yet potent techniques: random mask-
ing and contrastive constraint. The random masking acts on the
representation matrix’s columns to regulate the degree of informa-
tion aggregation from neighbors, thus preventing the convergence
of node representations. Meanwhile, the contrastive constraint,
applied to the representation matrix’s rows, enhances the discrim-
inability of the nodes. Designed as a plug-in module, TSC can be
easily coupled with GCN or SGC architectures. Experimental anal-
yses on diverse real-world graph datasets verify that our approach
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671954markedly reduces the convergence of node’s representation and
the performance degradation in deeper GCN.
CCS CONCEPTS
•Information systems →Data mining; •Computing method-
ologies→Learning latent representations.
KEYWORDS
GCN, Over-smoothing, Random masking, Contrastive learning
ACM Reference Format:
Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, Hongren Yan, and Chao
Ma. 2024. TSC: A Simple Two-Sided Constraint against Over-Smoothing. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671954
1 INTRODUCTION
Graph Convolutional Neural Networks (GCNs) [ 5,8,11,15] has
gained significant attention for their promising results in processing
relational data. Gradually emerging as a mainstream technology for
various applications, including natural language processing [ 33],
E-commerce [ 37], materials chemistry [ 24], biomedical [ 14], and
trajectory prediction [ 35], GCNs excel at leveraging topological
structures to capture relational characteristics. Despite their effec-
tiveness, stacking multiple GCN layers for high-order neighbors
introduces challenges such as over-smoothing [ 17], gradients van-
ishing [ 34], and overfitting [ 1]. Notably, over-smoothing plays a
crucial role in affecting performance in deep GCNs.
The phenomenon of over-smoothing, initially analyzed by [ 9,17],
arises as the depth of GCN layers increases, deteriorating the ability
to differentiate node representations and significantly impairing
model performance. As shown in the 1st row of Figure 1, the node
distribution of the GCN is quite scattered in the 1st layer. Yet, with
layers increasing, node representations converge (notably at the 8th
and 32nd layers), leading to node categories nearly indistinguish-
able in visualizations. The underlying reason lies in the high-order
 
2376
KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
(a) GCN’s node visualization at 1st, 8th, and 32nd layers
(b) GCN+DropMessage’s node visualization at 1st, 8th, and 32nd layers
(c) GCN+ContraNorm’s node visualization at 1st, 8th, and 32nd layers
(d) Our model’s node visualization at 1st, 8th, and 32nd layers
Figure 1: Visualization of node representations in different
layers. The 1st, 2nd, and 3rd columns depict the visualiza-
tions of node representations at the 1st, 8th, and 32nd layers,
respectively. Row (a) shows the node distribution for vanilla
GCN, (b) for DropMessage, (c) for ContraNorm, and (d) for
our proposed TSC applied to GCN. Node colors indicate the
categories. All sub-figures are visualized using t-SNE.
neighbor aggregation operation, which leads to neighbor overlap-
ping andindividuality overwhelmed. Figure 2 shows that nodes A
andBhave unique neighbors in their 1-order neighborhood, but
in the 3-order neighborhood, they share overlapping neighbors
and accumulate many non-homogeneous nodes. High-order neigh-
bors result in different nodes aggregating information from the
same neighbors (by low-quality neighbors) and the loss of node
individuality due to the large volume of neighbor nodes (by large
quantity of neighbors). For example as mentioned by [ 23]. Detailed
measurement can be found in Appendix F.
Several models have been proposed to address over-smoothing,
and can be broadly categorized into neighbor filtering and individ-
uality enhancement methods.
Neighbor Filtering Models: These models selectively aggregate
neighbors’ information to prevent nodes from becoming indistin-
guishable, either through explicit or implicit filtering. The explicit
methods include DropEdge [ 26], DropMessage [ 7], and DropNode
(a) The 1-order neighborhood
 (b) The 3-order neighborhood
Figure 2: The neighbors changes across different orders. In
the 3-order neighborhood, the node A and B have overlapped
neighbors and have large number of neighbors.
[6], which selectively drop edges, messages, or nodes. While these
methods alleviate over-smoothing by filtering neighbors informa-
tion, their accuracy diminishes in deep layers, leading to category
confusion, as illustrated Figure 1(b). For instance, DropMessage
exhibits a clear clustering structure in 8th layer but loses this ad-
vantage in the deeper 32th layer due to information overload from
neighbors. Meanwhile, the implicit methods, including PairNorm
[41], DGN [ 43], NodeNorm [ 42], and ContraNorm [ 13] adjust the
received neighborhood information by adding normalization to the
GCNs at each layer. These normalization methods implicitly re-
weight the aggregated information, yet they remain ineffective in
addressing over-smoothing caused by the overwhelming of neigh-
bors, as illustrated in Figure 1(c). ContraNorm’s node representa-
tion also becomes indistinguishable in the 32nd layer. From the
above analysis, we can see that neighbor filtering can alleviate the
over-smoothing caused by the low quality of neighbors (neighbor
overlapping issue) but neighbor quantity.
Individuality Enhancement Models: These models add a node’s
self-representation in the last or first layer to the new layer’s repre-
sentation. Examples include JKNet [ 36], GCNII [ 3], and AIR [ 39],
which incorporate the first or shallow layer’s representation to new
layers alongside aggregated information. However, they add the
shallow representation directly to the new layer according to a
certain weighting instead of adding as a part of it. It may lead some
nodes to overemphasize the impact of the first or shallow layer, and
therefore unable to exploiting high-order neighbors.
In this paper, we focus on both neighbor quality and quantity
issues, and introduce the Two-Sided Constraint with two tech-
niques to combat over-smoothing: Firstly, Random Masking se-
lectively drops columns of aggregated information, replacing them
with the node’s representation from the previous layer. This tech-
nique, acting as a neighbor filtering mechanism, effectively prevents
representation convergence while preserving node individuality,
as shown in Figure 1(d). Secondly, the Contrastive Constraint
enhances node individuality by minimizing changes in the same
node’s representation across layers and enlarging the representa-
tion differences between distinct nodes, both within and across
layers. This technique, which is applied to the row of node features,
serves as a mechanism for enhancing individuality to mitigate the
overwhelming effect of homogenization. These two constants are
 
2377TSC: A Simple Two-Sided Constraint against Over-Smoothing KDD ’24, August 25–29, 2024, Barcelona, Spain
applied to the two sides of representation matrix, column and row
respectively, and therefore are called two-sided constraint (TSC).
In addition to the over-smoothing issue, GCN also faces prob-
lems of gradient-vanishing and overfitting. To minimize the impact
of these latter issues on the model, we opt to analyze the perfor-
mance of TSC using Simplifying Graph Convolution (SGC). SGC, by
eliminating linear transformations and nonlinear activation func-
tions, can avoid these two problems. Additionally, to explore the
versatility of TSC, we also implement it in GCN, providing relevant
technical details and experimental results.
The main contributions of this work can be summarized as fol-
lows.
(1)We provide a new perspective of graph over-smoothing in
view of neighbor overlapping andindividuality overwhelmed. Solving
the neighbor overlapping can alleviate the representation conver-
gence. Taking care of individuality overwhelmed can maintain the
overall performance from degradation. Base on this perspective, we
proposed a simple but potent two-sided constraint to the column
and row of the representation matrix.
(2)On the column of representation matrix, we propose random
masking against representation convergence and keep nodes’ indi-
viduality. This strategy is more effective than some recent SOTAs.
(3)On the row of representation matrix, we introduce contrastive
constraints to increase differences and discriminative power among
nodes.
2 RELATED WORK
As the number of Graph Convolutional Network (GCN) layers in-
creases, over-smoothing occurs. This happens because higher-order
neighbor aggregation operations cause node features to become
indistinguishable and overwhelm node individuality due to varia-
tions in the quality and quantity of neighbors in high order. Many
methods attempt to address this issue by reducing the number of
neighbors through random dropping (e.g., DropMessage, DropN-
ode) or normalization (e.g., ContraNorm), which can be categorized
as neighbor filtering methods. The presence of high-order neigh-
bors also leads to different nodes aggregating similar information
caused by neighbor sharing, resulting in the loss of the node’s indi-
viduality. Several methods attempt to mitigate this issue by reducing
the influence of high-order neighbors and increasing the impact
of information from shallow layers, as seen in JKNet and GCNII.
These methods can be categorized as individuality enhancement
techniques. We will discuss them in detail.
2.1 Neighbor Filtering
Explicit Filtering Methods: Models like DropEdge [ 26] and DropN-
ode [ 6] explicitly filter neighbors by randomly deleting edges or
nodes. DropMessage [ 7] directly drops information from the mes-
sage passing matrix. While effective in mild deep layers, these
explicit filtering methods suffer from over-smoothing in deeper
layers, as illustrated in Figure 1(b).
Implicit Filtering Methods: These methods manipulate neigh-
bor impact through normalization. PairNorm [ 41] adds a constant to
the sum of pairwise node distances to reduce similarity between dis-
tant nodes. DGN [ 43] introduces differentiable group normalization,
NodeNorm [ 42] normalizes each feature vector by node-specificstatistical properties, and ContraNorm [ 13] employs contrastive
learning techniques for a more uniform distribution. While these
methods scatter node representations in deep layers, normalization
biases may cause the obtained distribution to deviate from node
categories. In contrast, our approach introduces random masking
to the columns of the representation matrix, preserving the distri-
bution of nodes, as illustrated in Figure 1(c).
2.2 Individuality Enhancement
The Individuality Enhancement technique involves adding a node’s
original feature (often from the first or last layer) to a new layer.
JKNet [ 36] adds each layer’s feature to the final representation,
GCNII [ 3] adds the first layer’s feature to each subsequent layer, and
DeepGCNs [ 16] adopts a ResNet-like approach by adding features
from the first and last layers simultaneously. However, excessively
incorporating shallow node information into new layers may hinder
obtaining information from high-order neighbors. This trade-off
requires careful tuning for each scenario.
Recent models address this challenge by adaptively integrating
shallow layer information. DRC [ 38] uses the difference between
the previous layer’s input and output as the input for the next
layer, allowing adaptive selection of previous information. Zhang
et al. [ 39] propose Adaptive Initial Residual (AIR) for propagation
and transformation steps, preventing over-smoothing and perfor-
mance degradation in early layers. While adding information from
shallow layers prevents degradation, it may impede aggregation
of information from high-order neighbors. In our work, we pro-
pose smoothing the node’s transitions between layers through a
contrastive constraint. This constraint not only brings the node’s
representation close to its last layer but also separates it from other
nodes’ representations.
3 PRELIMINARIES
3.1 Problem Definition
In this paper, we focus on the unweighted undirected graph 𝐺=
{𝑉,𝐸}, where𝑉={𝑣1,...,𝑣𝑛}denotes a𝑛node set and 𝐸is the
edge set defined on 𝐺. Let A∈{0,1}𝑛×𝑛denotes the symmetric
adjacency matrix with A𝑖,𝑗=A𝑗,𝑖. Then, the degrees of the nodes
are defined by 𝐷={𝑑1,...,𝑑𝑛}, where𝐷𝑖=Í
𝑗≠𝑖A𝑖,𝑗. The degree
matrix is further defined as the diagonal matrix Dwith D𝑖,𝑖=
𝐷𝑖(𝑖=1,...,𝑛).L=D−1
2(A+I)D−1
2is the Laplace matrix of 𝐺.
X∈R𝑛×𝑓denotes the feature matrix of the nodes, where 𝑓is the
dimension of feature. H∈R𝑛×𝑑is node representation matrix, that
is used for node classification and message passing. Only parts of
nodes are labeled, our task is to predict the labels of the rest nodes.
This task is called semi-supervised node classification [41].
3.2 GCN
Graph Convolutional Network (GCN), as a representative method,
propagates node features through the adjacency matrix to aggregate
the information of neighbors. The aggregation operation for layer
𝑙can be defined by [15]:
H(𝑙+1)=𝜎(LH(𝑙)W(𝑙)), (1)
 
2378KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
where𝜎(·)is the activation function, 𝑙represents the number of
layers. H(𝑙)andW(𝑙)are the node representation and weight ma-
trices of the 𝑙-th layer, respectively. Lis the Laplace matrix of the
input Graph. In deep GCN models, the number of parameters grows
rapidly, endowing them with strong expressive power. However,
they often suffer from the issue of over-smoothing. Moreover, the
vast volume of parameters also makes it challenging to handle
large-scale graph data effectively.
3.3 SGC
Simplifying Graph Convolution (SGC)[ 32], a simplified version of
GCN, removes the nonlinear activation and linear projection oper-
ation from GCN. The representation matrix in 𝑙-layer is calculated
by a very simple linear projection.
H(𝑙)=L𝑙X, (2)
where L𝑙represents the 𝑙power of the Laplacian matrix. SGC,
a single linear transformation across the graph structure, signifi-
cantly reduces computational complexity and training time. This
efficiency makes SGC particularly suitable for large-scale graph
data. Benefiting from simplifying the model, SGC reduces the risk
of overfitting and the gradient vanish problems. Compared with
GCN, SGC preserves more of the original node features, preventing
them from becoming indistinguishable, and consequently alleviat-
ing over-smoothing.
4 METHODOLOGY
The proposed two-sided constant includes random masking and
contrastive constraint, that will implemented on SGC and GCN
as shown in Figure 3. The random masking is conducted on the
columns of the representation matrix, while the contrastive con-
straint is on the rows.
4.1 Random Mask
In the SGC model, using node features directly as their initial repre-
sentations may lead to issues related to the sparse nature of features.
This operation makes node representations to be affected by the
sparsity of features, potentially resulting in overly sparse represen-
tations. Unlike SGC, GCN does not encounter this issue, owing to
its inherent liner projection operation. To address this in SGC, we
first perform a dimension reduction on the feature matrix Xwith a
linear projectorr, such as a Multilayer Perceptron (MLP), to obtain
the initial value of node representation. The projection operation
can defined as
H(0)=𝑀𝐿𝑃(X),X∈R𝑛×𝑓,H(0)∈R𝑛×𝑑. (3)
The random masking strategy on SGC, as illustrated by Figure
4, randomly masks the columns of the representation matrix with
a masking rate, then performs graph convolution on the masked
columns. The unmasked columns are directly copy to the next layer
without convolutions. The masking strategy can be defined as
H(𝑙+1)=LH(𝑙)◦M(𝑙)+H(𝑙)◦(1−M(𝑙)), (4)
where M(𝑙)∈{0,1}𝑑×𝑑is the mask matrix, M(𝑙)
𝑖,:is a𝑑-dimensional
row vector with all 0or1in a masking rate, 1is a all 1matrix
with the size of 𝑑×𝑑,◦is the element-wise matrix multiplication.IfM𝑖,:(𝑙)=1, the𝑖-th column of the representation matrix will
participate in the convolution, otherwise not.
Compared with DropNode [ 26], random masking filters the ag-
gregated information in columns while DropNode drops messages
before aggregation. Masking in columns can maintain all node’s
representation against convergence in layer-wise. Dropping nodes
only discard few information form neighbors and update whole
representation of each node. The excessive absorption of high-order
neighbor information will lead to over-smoothing. However, ran-
dom masking only update representations partially.
The neighbors in the shallow layer is very discriminative, as
shown in Figure 2, and is useful for enhancing node representations,
In this work, the masking strategy is only applied after layer 3
(𝑙≥3). In deeper layers, nodes’ neighbors become more similar,
making over-smoothing more likely to occur. Therefore, we set
masking rate gradually to decrease as the network layers increase.
The masking rate is as follows.
𝑀𝑎𝑠𝑘𝑖𝑛𝑔𝑅𝑎𝑡𝑒
M(𝑙)
𝑖,:=0
=0,𝑙≤2
1−log(𝜆/𝑙+1),𝑙≥3(5)
where𝑙is the number of layers and 𝜆>0is a hyper-parameter
to tune the decreasing speed. A smaller value of 𝜆indicate faster
decreasing. Meanwhile, to prevent the node representations from
stopping updates as the structure becomes deeper. we enforce that
each masking operation must mask at least one column, ensuring
that the representations will always update.
4.2 Contrastive Constraint
Random masking effectively slows down feature updates to miti-
gate representation convergence, but it fails to enhance node in-
dividuality. This phenomenon has been discussed by Zhang [ 39],
that anti-over-smoothing technique can not always prevent GNN
from performance degradation. This is because even after multiple
rounds of iteration in high-order neighbor aggregation, nodes still
lose their individuality and consequently become indistinguishable.
To address this issue, we introduce contrastive constraints, aiming
to optimize both the similarity between node features across layers
and the dissimilarity between different nodes.
Specifically, in SGC, we consider identical node representations
between two consecutive layers as positive pairs and add con-
straints to make them similar. Meanwhile, we consider the rep-
resentations of any two different nodes within or between layers
as negative pairs and push the nodes away from each other. The
specific formula is as follows [4]:
𝐿𝑆𝐺𝐶=−𝐿−1∑︁
𝑙=1𝑛∑︁
𝑖𝑙𝑜𝑔𝑒
𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙)
𝑖)/𝜏
𝑛∑︁
𝑗≠𝑖𝑒
𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙+1)
𝑗)/𝜏
+𝑒
𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙)
𝑗)/𝜏,(6)
where𝑛denotes the number of nodes, 𝑙(𝑙≥0)is the number
of layers,𝜏is the temperature coefficient, and 𝑠(·)is the cosine
similarity calculation function. The numerator and denominator
represent positive and negative pairs constraint respectively.
In GCN, the computation complexity is larger than that of SGC.
Adding contrastive constraints to each layer will further increase
the computation complexity to make it impracticable in large scale
graph. We generate the two subgraphs of the last layer by dropout
 
2379TSC: A Simple Two-Sided Constraint against Over-Smoothing KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) SGC+TSC
 (b) GCN+TSC
Figure 3: The overview of TSC applied to SGC and GCN. (a) illustrates the structure of TSC on SGC, while (b) depicts its
application to GCN. They both add random masking to the columns of the representation matrix to mitigate representation
convergence, and add contrastive constraints to the rows of representation matrix to enhance node’s individuality.
Figure 4: The Random Masking.
and by considering the same nodes of the two subgraphs as positive
pairs and different nodes as negative pairs. This negative pair can
keep the diversity between nodes and increase the difference of
node information.
In GCN, its computational complexity is far exceeds that of SGC.
Adding contrastive constraints to each layer would further make
it impracticable in large dataset. Another difference between GCN
and SGC is that in GCN, each layer’s embedding is derived from
the previous layer, whereas in SGC, it is directly computed from
the initial representation. Therefore, we propose adding contrastive
constraints only to the last layer, leveraging the transitivity of node
representations across layers to constrain the representation of
nodes in shallow layers. Specifically, we perform dropout twice on
the final layer H(𝐿)to obtain two representations, bH(𝐿)andeH(𝐿),
then utilize contrastive constraints to enhance node individuality,
similar to SGC. The calculation is as follows
b𝑯(𝑳)=𝑑𝑟𝑜𝑝𝑜𝑢𝑡(𝑯(𝐿)) (7)
e𝑯(𝑳)=𝑑𝑟𝑜𝑝𝑜𝑢𝑡(𝑯(𝐿)) (8)
𝐿𝐺𝐶𝑁=−𝑛∑︁
𝑖𝑙𝑜𝑔𝑒
𝑠(bℎ(𝐿)
𝑖,eℎ(𝐿)
𝑖)/𝜏
𝑛∑︁
𝑗≠𝑖𝑒
𝑠(bℎ(𝐿)
𝑖,bℎ(𝐿)
𝑗)/𝜏
+𝑒
𝑠(bℎ(𝐿)
𝑖,eℎ(𝐿)
𝑗)/𝜏(9)
where𝐿is the number of layers.5 DISCUSSIONS
In this section, we provide a theoretical analysis of our method to
show that it can mitigate over-smoothing. And we also compare our
method with other popular methods to illustrate the differences of
our method. For detailed information, please refer to the Appendix
B.
5.1 Theoretical Analysis
In the proposed method, we claim that the over-smoothing prob-
lem can be mitigated by using a Random Masking Strategy and a
Contrastive Constraint. In this section, we analysis the claim.
(a) Random masking strategy can retain the nodes’ individ-
ual information and mitigates representation convergence.
Firstly, let us take SGC as an example to explain the over-smoothing
problem. We know that the 𝑙-th layer of the SGC is
H(𝑙)=𝑳𝑙𝑿 (10)
Since graph convolution is a special form of Laplacian smoothing,
excessive message passing makes nodes with the same degree have
identical representations. According to the derivation in [ 19], the
limit of message passing is as follows:
lim
𝑙→∞(𝑳)𝑙
𝑖𝑗=√︁
(𝑑𝑖+1)(𝑑𝑗+1)
2𝑚+𝑛(11)
where𝑛and𝑚are the total number of nodes and edges in the graph,
respectively. After GCN performs an infinite number of message
passing steps, the features of node 𝑖in the𝑙-th layer will converge
to the following form
lim
𝑙→∞𝑯(𝑙)
𝑖=lim
𝑙→∞(𝑳𝑙𝑿)𝑖
=√𝑑𝑖+1
2𝑚+𝑛×∑︁
𝑗∈𝑉(√︃
𝑑𝑗+1𝑿𝑗)(12)
As𝑙approaches infinity, the neighbors of node 𝑖become all the
nodes in the connected graph.
 
2380KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
However, message passing with the proposed random mask strat-
egy does not have such a issue. The message passing is as follows
𝑯(𝑙)=𝑳𝑯(𝑙−1)◦𝑴(𝑙)+ (1−𝑴(𝑙))◦𝑯(𝑙−1)(13)
When𝑙approaches infinity, we have
lim
𝑙→∞𝑯(𝑙)=lim
𝑙→∞𝑳𝑿◦𝑴(1)+ (1−𝑴(1))◦𝑿+𝑳2𝑿◦𝑴(2)+
(1−𝑴(2))◦𝑳𝑿+...+𝑳𝑙𝑿◦𝑴(𝑙)+ (1−𝑴(𝑙))◦𝑳𝑙−1𝑋
=lim
𝑙→∞𝑿𝑙−1∑︁
𝑘=1(𝑴(𝑘)+(1−𝑴(𝑘+1)))◦𝑳𝑘+𝑳𝑙𝑿◦𝑴(𝑙)+
(1−𝑴(1))◦𝑿)
(14)
If we set b𝑴(𝑘)=𝑴(𝑘)+(1−𝑴(𝑘+1)),− →𝑴=1−𝑴(1), where both
b𝑴(𝑘)and− →𝑴are mask matrices, the formula can be rewritten as
lim
𝑙→∞𝑙−1∑︁
𝑘=1b𝑴(𝑘)◦𝑳(𝑘)𝑿+𝑴(𝑙)◦𝑯(𝑙)+− →𝑴◦𝑿 (15)
It is evident that the final representation of nodes encompasses
information from all previous layers ( 𝑯(𝑘)) as well as from the
first layer ( 𝑿). Information from the first layer typically possesses
greater diversity and is effective in preserving the individual infor-
mation. The masking rate can control the proportion of first layer
information and information from all layers. Therefore, random
masking can significantly mitigate the issue of over-smoothing.
We also apply our method to GCNs with nonlinear functions and
linear transformations, and perform a theoretical analysis to miti-
gate over-smoothing. In GCNs, 𝑓(·)is non-linear function and 𝑊(𝑙)
is a linear transformation, we define the aggregrated information
e𝐻(𝑙)at layer𝑙as
e𝑯(𝑙)=𝑓(𝑳𝑯(𝑙−1)𝑾(𝑙)) (16)
The column masking strategy in TSC will select columns from the
current layer e𝐻(𝑙)and the last layer 𝐻(𝑙−1)
𝑯(𝑙)=e𝑯(𝑙)◦𝑴(𝑙)+𝑯(𝑙−1)◦(1−𝑴(𝑙)),𝑙>1 (17)
where 𝑯(0)=𝑿,𝑯(1)=e𝑯(1). It should be noticed that the non-
linear activation function only participates the information aggrega-
tione𝐻(𝑙). That is when 𝐻(𝑙)choose the columns from that last layer
𝐻(𝑙−1), these columns are not affected by the non-linear function
in the current layer. We can rewrite the column masking strategy
in probability form.
𝑯(𝑙)
·,𝑗=(
e𝑯(𝑙)
·,𝑗𝑖𝑓 𝑎>𝛼(𝑙)
𝑯(𝑙−1)
·,𝑗𝑒𝑙𝑠𝑒(18)
where𝛼(𝑙)=1−log(𝜆
𝑙+1)(Defined in Eq.(5) ), 𝑎is an uninform
random variable sampled form 𝑎∼𝑈[0,1],𝑙is layer number. Then
,we have the following probability
𝑃
𝑯(𝑙)
·,𝑗=e𝑯(𝑙)
·,𝑗
=1−𝛼(𝑙)
𝑃
𝑯(𝑙)
·,𝑗=𝑯(𝑙−1)
·,𝑗
=𝛼(𝑙)(19)At the layer 𝑙+1, we have
𝑯(𝑙+1)
·,𝑗=(
e𝑯(𝑙+1)
·,𝑗𝑖𝑓 𝑎>𝛼(𝑙+1)
𝑯(𝑙)
·,𝑗𝑒𝑙𝑠𝑒
= 
e𝑯(𝑙+1)
·,𝑗 𝑖𝑓 𝑎>𝛼(𝑙+1)
e𝑯(𝑙)
·,𝑗𝑖𝑓 𝑎⩽𝛼(𝑙+1)∧˜𝑎>𝛼(𝑙)
𝑯(𝑙−1)
·,𝑗𝑒𝑙𝑠𝑒(20)
where both𝑎and ˜𝑎are random variables sampled from 𝑈[0,1]. We
then have
𝑃
𝑯(𝑙+1)
·,𝑗=e𝑯(𝑙+1)
·,𝑗
=1−𝛼(𝑙+1)
𝑃
𝑯(𝑙+1)
·,𝑗=e𝑯(𝑙)
·,𝑗
=(1−𝛼(𝑙))𝛼(𝑙+1)
𝑃
𝑯(𝑙+1)
·,𝑗=𝑯(𝑙−1)
·,𝑗
=𝛼(𝑙)𝛼(𝑙+1)(21)
By generalizing the above equations, we have
𝑃
𝑯(𝐿)
·,𝑗=𝑯(1)
·,𝑗
=𝐿Ö
𝑙=1𝛼(𝑙)=𝐿Ö
𝑙=1
1−log(𝜆
𝑙+1)
≥𝐿Ö
𝑙=1
1−𝜆
𝑙
≥exp(−𝐿∑︁
𝑙=1𝜆
𝑙−𝛾𝐿∑︁
𝑙=1(𝜆
𝑙)2)
>𝑒𝑥𝑝(−𝜆log𝐿−𝛾𝜋2
6𝜆2)(22)
where the first inequality is obtained from log(𝑥+1)≤𝑥for𝑥>0,
the second inequality is obtained by 1−𝑥≥𝑒𝑥𝑝(−𝑥−𝛾𝑥2)for
𝑥≥0with𝛾>0.5being an arbitrary constant, the third inequality
comes from two inequalitiesÍ𝐿
𝑙=11
𝑙≤log𝐿and partial sum of the
Basel SeriesÍ𝐿
𝑙=1(1
𝑙)2<𝜋2
6.
We can see that 𝑃
𝑯(𝐿)
·,𝑗=𝑯(1)
·,𝑗
is close to 1when the hyperpa-
rameter𝜆=0. We can control the columns in the last layer comes
form the initial layer by 𝜆. This demonstrate TSC can effectively
control the over-smoothing. We also provde the parameter analysis
in Fig a1 to demonstrate its effectiveness. When 𝜆∈{0.1,0.5}
the ACC outperform than when 𝜆∈{1,1.5}. This revelas that
more information form shallow layers can obtain better perfor-
mance. However, when 𝜆=0.1its performance is lower than when
𝜆=0.5. This is that overwhelm information form ininital layers
will degrdata perforamcne, because remote information is not well
aggegrated.
We observe that 𝑃
𝑯(𝐿)
·,𝑗=𝑯(1)
·,𝑗
approaches 1when the hyper-
parameter𝜆=0. This allows us to control the flow of information
from the initial layer to the last layer using 𝜆. This demonstrates
that column masking can effectively mitigate the issue of over-
smoothing.
We provide a parameter analysis in figure.8 to illustrate the
effectiveness of different 𝜆values. As shown in Figure 1a, when 𝜆
is set within{0.1,0.5}, the accuracy (ACC) outperforms the model
when𝜆∈{1,1.5}. This suggests that incorporating information
from the shallower layers can yield better performance.
(b) Contrastive constraint enhance node individuality and
difference among nodes.
The core idea of contrastive learning is to maximize agreement
between augmented views of the positive pairs and disagreement
 
2381TSC: A Simple Two-Sided Constraint against Over-Smoothing KDD ’24, August 25–29, 2024, Barcelona, Spain
of views from negative pairs. Taking the contrastive loss on SGC
as an example, we can decompose the 𝐿𝑆𝐺𝐶 at(𝑙+1)-th layer𝑖-th
node into two parts, called alignment loss andheterogeneity
loss respectively.
𝑙(𝑙+1)
𝑎𝑙𝑖𝑔𝑛(𝑖)=𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙)
𝑖)/𝜏 (23)
𝑙(𝑙+1)
ℎ𝑒𝑡𝑒𝑟(𝑖)=log(𝑛∑︁
𝑗≠𝑖𝑒𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙+1)
𝑗)/𝜏+𝑒𝑠(ℎ(𝑙+1)
𝑖,ℎ(𝑙)
𝑗)/𝜏) (24)
The alignment loss promotes similar feature representations for
the same nodes in different layers, thus maintaining invariance
to unwanted noise and increasing consistency across layers. The
heterogeneity loss effectively prevents nodes from becoming indis-
tinguishable. It maximizes the average distance between all samples,
resulting in representations that are roughly evenly distributed in
the latent space, thus retaining more information. Consequently, it
can mitigate the effects of over-smoothing in GNNs. Similar analysis
can be easily obtained in GCNs by Equation (23) and (24).
6 EXPERMENTS
In this section, we conduct experiments on semi-supervised node
classification to evaluate the effectiveness of TSC-GCN and TSC-
SGC. Five recent developed models for over-smoothing are chosen
for evaluations. We use accuracy (ACC) to assess the performance
of models and Mean Average Distance (MAD) [ 2] to measure the
degree of smoothing of models. Our code is available at: https:
//github.com/Recgroup/TSC.
6.1 Experimental Setup
Datasets. To evaluate the performance of our proposed method
across different scenarios, we conduct experiments on 5 datasets
with various sizes and densities: Coauthor CS [28], Amazon Photo
[28],Cora [20,27],CiteSeer [10,27], and Pubmed [22,27]. The
summary of datasets is provided in appendix A.
Baselines. To assess the advantages of the proposed TSC-GCN
against the SOTAs, we compare our method with other GNNs.
GCN [29] is the first work to use GCN for node classification.
SGC [32] is a simplified graph convolutional network that re-
moves the nonlinear activation function and many linear transfor-
mations.
DropMessage [7] alleviates over-smoothing implicitly by adding
dropout operation to out messages of each node.
ContraNorm [13] mitigates over-smoothing by implicitly shat-
tering node representations into a more uniform distribution.
GCNII [3] enhance nodes’ individual information by adding the
first layer’s features to deep layers.
AIR [39] adds the node representation from previous layers to
next layer for individuality enhancement.
NDLS [40] alleviates over-smoothing by controlling node-dependent
local smoothness.
DropEdge [25] mitigates over-fitting and over-smoothing by
randomly censoring out edges in the original graph during model
training.SJLR [12] proposes Rewiring algorithm to mitigate over-smoothing
and over-squeezing by adding and removing edges in a feasible
way.
DeepGCN [16] applys residual/dense concatenation and dilated
convolution to the GCN architecture to achieve significant perfor-
mance gains in point cloud semantic segmentation tasks.
6.2 Accuracy Evaluation
We first compare the overall classification performance of TSC
against all baseline methods. We vary the number of layers in all
models from 1 to 32 and list their optimal accuracy (ACC) in Table
1. Notably, GCN doesn’t use and anti-over-smoothing technique,
represents the best performance of a standard GCN. SGC, through
model simplification, possesses some capability to prevent over-
smoothing, but it sacrifices model expressiveness.
Table 1: Overall performance comparison. The classification
performance is measured by ACC. The best and second best
are marked with bold and underline respectively.
Cora
Citeseer Pubmed CoauthorCS AmazonPhoto
GCN
[ICLR’17] 0.836 0.719 0.798 0.910 0.914
SGC [ICML’19] 0.828 0.740 0.79 0.914 0.905
DropEdge(GCN) [ICLR’20] 0.828 0.723 0.796 – –
SJLR [CIKM’23] 0.819 0.695 0.786 – –
DeepGCN [ICCV’19] 0.734 0.627 0.757 – –
ContraNorm(GCN) [ICLR’23] 0.837 0.728 0.798 0.920 0.912
DropMessage(GCN) [ICLR’23] 0.835 0.721 0.795 0.911 0.915
GCNII [ICML’20] 0.8520.744
0.803 0.921 0.929
AIR(SGC) [KDD’22] 0.828 0.746 0.790 0.936 0.93
NDLS [NIPS’21] 0.841 0.736 0.807 0.920
0.920
GCN+TSC(
ours) 0.851 0.748 0.807 0.926 0.934
SGC+TSC(
ours) 0.854 0.747 0.825 0.916
0.933
From Table 1, the following conclusions can be drawn: (1)Al-
though GCN is prone to over-smoothing, it possesses strong model
expressiveness and can achieve good performance at shallow lay-
ers. For instance, it outperforms SGC on datasets such as Cora,
Pubmed, and AmazonPhoto. (2)SGC can alleviate over-smoothing
and overfitting, which is why it can surpass GCN on the Citeseer
and CoauthorCS datasets. Enhancing SGC’s capability to address
over-smoothing can further obtain better performance. For example,
ContraNorm achieves better results than SGC on Cora, Pubmed,
CoauthorCS, and AmazonPhoto. (3)Node filtering methods are
primarily aimed at mitigating over-smoothing, whereas individu-
ality enhancement methods aim to improve node uniqueness. As
a result, the latter can achieve relatively better performance. For
instance, GCNII and AIR obtain better results than ContraNorm
and DropMessage on the Citeseer, CoauthorCS, and AmazonPhoto
datasets. GCNII shows superior performance to ContraNorm and
DropMessage on Cora and Pubmed datasets. (4)By further optimiz-
ing the balance between node’s individual information and local
smoothness, NDLS achieves superior results to AIR on Cora and
Pubmed. (5)Considering both neighbor filtering and node individ-
uality enhancement, model performance can be further improved.
The proposed TSC, combined with SGC and GCN, achieves optimal
and sub-optimal performance on Citeseer, Pubmed, and Amazon-
Photo datasets. It obtains optimal and sub-optimal results on Cora
and CoauthorCS, respectively.
 
2382KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
6.3 Over-Smoothing Analysis
In order to verify the effectiveness of our approach on the over-
smoothing problem, we report model’s ACC when depth is con-
tinuously increased from 4 to 32. The results on Cora, Citeseer,
Pubmed, CoauthorCS, and AmazonPhoto are reported in Table 2 to
6. In these tables, OOM indicates that out of memory, and the best
performance in each table is denoted by∗.
Observations from Tables 2 to 6 can be summarized as follows:
(1)GCN is severely impacted by over-smoothing, whereas SGC
demonstrates a certain ability to alleviate this issue, showing a more
gradual performance decline as network depth increases. (2)Among
neighbor filtering methods, the explicit filtering approach DropMes-
sage experiences significant over-smoothing at the 32nd layer, while
the implicit filtering method ContraNorm maintains stable perfor-
mance at this depth. (3)Individuality enhancement methods con-
sistently exhibit strong resistance to over-smoothing. For instance,
GCNII achieves its best performance on Cora and Citeseer at the
32nd layer, fulfilling the expectation that deeper networks yield bet-
ter results. (4)Our proposed TSC, when combined with GCN and
SGC, shows effective mitigation of over-smoothing. For example,
GCN+TSC outperforms baselines and SGC+TSC in deeper layers
on the Citeseer, CoauthorCS, and AmazonPhoto datasets, indicat-
ing that this method not only effectively reduces over-smoothing
but also leverages the expressive power of GCN. (5)All networks
generally reach optimal or near-optimal results by the 4th or 8th
layer. Therefore, in practical applications, to reduce computational
complexity, it is advisable to limit network depth to 10 layers or
fewer. This approach aligns with the small-world phenomenon [ 21]
observed in complex networks.
Table 2: ACC comparison in different depth on Cora
Lay
er 4 Layer 8 Layer 16 layer 32
GCN
0.818 0.303 0.311 0.319
SGC 0.828 0.815 0.790 0.725
ContraNorm(GCN) 0.824 0.795 0.520 0.289
DropMessage(GCN) 0.836 0.801 0.434 0.304
GCNII 0.840 0.829 0.838 0.852
AIR(SGC) 0.795 0.824 0.828 0.796
NDLS 0.814 0.811 0.841 0.834
GCN+TSC 0.845 0.847 0.851 0.848
SGC+TSC
0.843 0.849
0.854∗0.851
Table 3: ACC comparison in different depth on Citeseer
Lay
er 4 Layer 8 Layer 16 layer 32
GCN
0.707 0.260 0.254 0.235
SGC 0.739 0.740 0.733 0.724
ContraNorm(GCN) 0.718 0.620 0.491 0.287
DropMessage(GCN) 0.711 0.701 0.550 0.321
GCNII 0.720 0.734 0.738 0.744
AIR(SGC) 0.731 0.743 0.746 0.739
NDLS 0.713 0.730 0.736 0.723
GCN+TSC 0.742
0.748∗0.743 0.744
SGC+TSC
0.740 0.747 0.743 0.742Table 4: ACC comparison in different depth on Pubmed
Lay
er 4 Layer 8 Layer 16 layer 32
GCN
0.769 0.635 0.413 0.419
SGC 0.761 0.737 0.719 0.702
ContraNorm(GCN) 0.782 OOM OOM OOM
DropMessage(GCN) 0.776 0.783 0.604 0.621
GCNII 0.792 0.801 0.801 0.803
AIR(SGC) 0.790 0.785 0.765 0.738
NDLS 0.790 0.802 0.807 0.804
GCN+TSC
0.806 0.798
0.807 0.800
SGC+TSC 0.817∗0.812
0.813 0.809
Table 5: ACC comparison in different depth on CoauthorCS
Lay
er 4 Layer 8 Layer 16 layer 32
GCN
0.891 0.257 0.184 0.139
SGC 0.889 0.875 0.843 0.737
ContraNorm(GCN) 0.910 OOM OOM OOM
DropMessage(GCN) 0.902 0.653 0.424 0.424
GCNII 0.910 0.921 0.920 0.913
AIR(SGC) 0.936∗OOM
OOM OOM
NDLS 0.917 0.920 0.837 0.711
GCN+TSC
0.925 0.926
0.926 0.922
SGC+TSC 0.916 0.912 0.913 0.910
Table 6: ACC comparison in different depth on AmazonPhoto
Lay
er 4 Layer 8 Layer 16 layer 32
GCN
0.906 0.874 0.829 0.580
SGC 0.907 0.900 0.899 0.887
ContraNorm(GCN) 0.912 0.893 0.868 0.578
DropMessage(GCN) 0.915 0.894 0.864 0.866
GCNII 0.929 0.922 0.925 0.926
AIR(SGC)
0.930 0.929 0.918
0.918
NDLS 0.917 0.920 0.891 0.890
GCN+TSC 0.934∗0.934∗0.931
0.929
SGC+TSC 0.933 0.929 0.923
0.922
6.4 Ablation Study
We take SGC+TSC as an example to evaluate the impact of random
mask and contrastive constraint on Cora and Citeseer datasets by
ablation study. Along with ACC, we also adopt MAD [ 2] to mea-
sure the degree of smoothing of node representation. The ablation
studies are reported in Figure 5. The ablation operations is as
W/O Random Mask : removing random mask from SGC+TSC,
W/O Contrastive Constraint : removing Contrastive Constraint
strategy from SGC+TSC,
W/O Two-Side Constraint : removing random mask and contrastive
constraint from SGC+TSC, which is just the SGC model.
Comparing Figure 5(a) and 5(b), we can seen that removing
random masking ACC will slowly decrease with the increase of
layers. This indicates that random mask can alleviate representa-
tion convergence but not for performance degradation. Removing
contrastive contrast, the acc is gradated but keep stable with the
increase of layers. This indicates that contrastive can significantly
 
2383TSC: A Simple Two-Sided Constraint against Over-Smoothing KDD ’24, August 25–29, 2024, Barcelona, Spain
(
a) ACC on Cora
 (b) ACC on Citeseer
(c) MAD on Cora
 (d) MAD on Citeseer
Figure 5: The ablation study. The 1st row depicts the ablation
results on ACC metric and the 2nd row is the result on MAD.
maintain node’s difference, and therefore improve the overall per-
formance.
Comparing Figure 5(c) and 5(d), it can be found that both ran-
dom mask and contrastive constraint can alleviate the represen-
tation convergence ( 𝑀𝐴𝐷 =0indicates over-smoothing happen),
although they have different MAD values. It is notable that remov-
ing both random mask and contrastive constraint, GCN+TSC can
still keep MAD off zeros, because SGC removing mapping weights
that can alleviate over-smoothing.
6.5 Visualization
We employ t-SNE [ 30] to visualize the node representations in layer
32 on Cora. The results for GCN, GCN+DropMessage, GCN+TSC,
SGC, SGC+AIR, and SGC+TSC are presented in Figure 6.
(a) GCN
 (b) GCN+DropMess.
 (c) GCN+TSC(ours)
(d) SGC
 (e) SGC+AIR
 (f) SGC+TSC(ours)
Figure 6: The 2D visualization of node representations in
layer 32. Nodes are colored to represent different classes.In the first row of Figure 6, all GCN-based models except GCN+TSC
exhibit over-smoothing, resulting in indistinguishable node repre-
sentations in the two-dimensional embedding space. This highlights
GCN’s susceptibility to over-smoothing due to excessive parame-
ters, whereas TSC can effectively address this issue.
In the second row of Figure 6, all points display relatively dis-
persed distributions, avoiding the collapsing phenomenon observed
in Figures 6(d-f). This suggests that SGC-based models have the
ability to mitigate over-smoothing. With the addition of AIR op-
timization, the differentiation between node categories becomes
clearer. In comparison, TSC aggregates points of each class into
smaller clusters, exhibit better discriminative power.
7 CONCLUSION
In this paper, we analyzed the over-smoothing in view of neighbor
quality and quantity, and reviewed that existing methods increase
the differences between nodes by neighbor filtering or maintain the
individual information of nodes by individuality enhancement. The
node difference can alleviate representation convergence, whereas
the node individuality can maintain the discriminative power. We
combine these two advantages and design a two-sided constraint
(TSC), comprising random mask and contrastive constraint, to the
column and row of representation matrix. As a plugin model, TSC
is implemented in GCN and SGC. Experiments on 5 datasets demon-
strate that TSC can significant alleviate the over-smoothing of GCN,
improve the discriminative power of SGC, and achieve SOTA by
ACC and visualization. Theoretically, we also discuss that the keys
of TSC for mitigating representation convergence and maintain
discriminative power are random mask and contrastive constraints
respectively.
8 ACKNOWLEDGMENTS
This work was supported by the Science and Technology Innovation
2030-"New Generation of Artificial Intelligence" Major Program
(No.2021ZD0112400), the National Natural Science Foundation of
China (Nos. 62276162, 62106132, 62136005, 62272286), the Funda-
mental Research Program of Shanxi Province (No. 202203021222016),
the Science and Technology Major Project of Shanxi (No. 2022010201-
01006), and the Central guidance for Local scientific and technolog-
ical development funds (No. YDZJSX20231B001).
REFERENCES
[1]Deyu Bo, Binbin Hu, Xiao Wang, Zhiqiang Zhang, Chuan Shi, and Jun Zhou.
2022. Regularizing Graph Neural Networks via Consistency-Diversity Graph
Augmentations. Proceedings of the AAAI Conference on Artificial Intelligence 36, 4
(2022), 3913–3921.
[2]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring
and Relieving the Over-Smoothing Problem for Graph Neural Networks from the
Topological View. Proceedings of the AAAI Conference on Artificial Intelligence 34,
04 (2020), 3438–3445.
[3]Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. 2020.
Simple and Deep Graph Convolutional Networks. In Proceedings of the 37th
International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 1725–1735.
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
Proceedings of the 37th International Conference on Machine Learning (Proceedings
of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.).
PMLR, 1597–1607.
 
2384KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
[5]Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional Neural Networks on Graphs with Fast Localized Spectral Filtering. In Ad-
vances in Neural Information Processing Systems , D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc.
[6]Tien Huu Do, Duc Minh Nguyen, Giannis Bekoulis, Adrian Munteanu, and Nikos
Deligiannis. 2021. Graph convolutional neural networks with node transition
probability-based message passing and DropNode regularization. Expert Systems
with Applications 174 (2021), 114711.
[7]Taoran Fang, Zhiqing Xiao, Chunping Wang, Jiarong Xu, Xuan Yang, and Yang
Yang. 2023. DropMessage: Unifying Random Dropping for Graph Neural Net-
works. Proceedings of the AAAI Conference on Artificial Intelligence 37, 4 (Jun.
2023), 4267–4275.
[8]Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang
Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural networks for
semi-supervised learning on graphs. Advances in neural information processing
systems 33 (2020).
[9]Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. 2018.
Predict then propagate: Graph neural networks meet personalized pagerank.
arXiv preprint arXiv:1810.05997 (2018).
[10] C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998. CiteSeer: An Automatic
Citation Indexing System. In Proceedings of the Third ACM Conference on Digital
Libraries (Pittsburgh, Pennsylvania, USA) (DL ’98). Association for Computing
Machinery, New York, NY, USA, 89–98.
[11] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In Proceedings of
the 34th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR.
[12] Jhony H Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D
Malliaros. 2023. On the trade-off between over-smoothing and over-squashing
in deep graph neural networks. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management.
[13] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. 2023. ContraNorm: A
Contrastive Learning Perspective on Oversmoothing and Beyond. In The Eleventh
International Conference on Learning Representations.
[14] Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan Hu, Qi Liu,
Enhong Chen, and Cheekong Lee. 2020. ASGN: An Active Semi-Supervised Graph
Neural Network for Molecular Property Prediction. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual
Event, CA, USA) (KDD ’20). Association for Computing Machinery, 22 pages.
[15] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[16] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. 2019. DeepGCNs:
Can GCNs Go As Deep As CNNs?. In 2019 IEEE/CVF International Conference on
Computer Vision (ICCV).
[17] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper Insights into Graph
Convolutional Networks for Semi-Supervised Learning. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative
Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on
Educational Advances in Artificial Intelligence (AAAI’18/IAAI’18/EAAI’18) . AAAI
Press, Article 433, 8 pages.
[18] Weigang Lu, Yibing Zhan, Binbin Lin, Ziyu Guan, Liu Liu, Baosheng Yu, Wei
Zhao, Yaming Yang, and Dacheng Tao. 2024. SkipNode: On alleviating perfor-
mance degradation for deep graph convolutional networks. IEEE Transactions on
Knowledge and Data Engineering (2024).
[19] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He.
2021. UltraGCN: Ultra Simplification of Graph Convolutional Networks for Rec-
ommendation. In Proceedings of the 30th ACM International Conference on Infor-
mation & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM
’21). Association for Computing Machinery, New York, NY, USA, 1253–1262.
[20] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
2000. Automating the Construction of Internet Portals with Machine Learning.
Information Retrieval 3, 2 (01 Jul 2000).
[21] Stanley Milgram. 1967. The small world problem. Psychology today 2, 1 (1967),
60–67.
[22] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. 2012.
Query-driven Active Surveying for Collective Classification. In Proceedings of
the Workshop on Mining and Learning with Graph (Edinburgh, Scotland, UK).
[23] Kenta Oono and Taiji Suzuki. 2019. Graph neural networks exponentially lose
expressive power for node classification. arXiv preprint arXiv:1905.10947 (2019).
[24] Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen
Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer,and Pascal Friederich. 2022. Graph neural networks for materials science and
chemistry. Communications Materials 3, 1 (26 Nov 2022).
[25] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge:
Towards deep graph convolutional networks on node classification. arXiv preprint
arXiv:1907.10903 (2019).
[26] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropE-
dge: Towards Deep Graph Convolutional Networks on Node Classification. In
International Conference on Learning Representations.
[27] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and
Tina Eliassi-Rad. 2008. Collective Classification in Network Data. AI Magazine
29, 3 (2008), 93–106.
[28] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and
Stephan Günnemann. 2019. Pitfalls of Graph Neural Network Evaluation.
arXiv:1811.05868 [cs.LG]
[29] Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2017. Graph Convolu-
tional Matrix Completion. arXiv:1706.02263 [stat.ML]
[30] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using
t-SNE. Journal of Machine Learning Research 9, 86 (2008), 2579–2605.
[31] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, Yoshua Bengio, et al .2017. Graph attention networks. stat1050, 20 (2017).
[32] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying Graph Convolutional Networks. In Proceedings of
the 36th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.).
PMLR, 6861–6871.
[33] Lingfei Wu, Yu Chen, Kai Shen, Xiaojie Guo, Hanning Gao, Shucheng Li, Jian
Pei, and Bo Long. 2023. Graph Neural Networks for Natural Language Processing:
A Survey.
[34] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonza-
lez, and Ion Stoica. 2021. Representing Long-Range Context for Graph Neural
Networks with Global Attention. In Advances in Neural Information Processing
Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (Eds.), Vol. 34. Curran Associates, Inc.
[35] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. 2022. Group-
Net: Multiscale Hypergraph Neural Networks for Trajectory Prediction with
Relational Reasoning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). 6488–6497.
[36] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs
with Jumping Knowledge Networks. In Proceedings of the 35th International Con-
ference on Machine Learning (Proceedings of Machine Learning Research, Vol. 80),
Jennifer Dy and Andreas Krause (Eds.). PMLR, 5453–5462.
[37] Guipeng Xv, Chen Lin, Wanxian Guan, Jinping Gou, Xubin Li, Hongbo Deng,
Jian Xu, and Bo Zheng. 2023. E-Commerce Search via Content Collaborative
Graph Neural Network (KDD ’23). Association for Computing Machinery, New
York, NY, USA, 13 pages.
[38] Liang Yang, Weihang Peng, Wenmiao Zhou, Bingxin Niu, Junhua Gu, Chuan
Wang, Yuanfang Guo, Dongxiao He, and Xiaochun Cao. 2022. Difference Residual
Graph Neural Networks. In Proceedings of the 30th ACM International Conference
on Multimedia. 3356–3364.
[39] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi
Yang, and Bin Cui. 2022. Model degradation hinders deep graph neural networks.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2493–2503.
[40] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu
Tao, Zhi Yang, and Bin Cui. 2021. Node dependent local smoothing for scalable
graph learning. Advances in Neural Information Processing Systems 34 (2021),
20321–20332.
[41] Lingxiao Zhao and Leman Akoglu. 2020. PairNorm: Tackling Oversmoothing in
GNNs. In International Conference on Learning Representations.
[42] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu,
and Jiashi Feng. 2021. Understanding and Resolving Performance Degradation in
Deep Graph Convolutional Networks. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management (Virtual Event, Queensland,
Australia) (CIKM ’21). Association for Computing Machinery, New York, NY,
USA, 2728–2737.
[43] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu.
2020. Towards Deeper Graph Neural Networks with Differentiable Group Normal-
ization. In Proceedings of the 34th International Conference on Neural Information
Processing Systems (Vancouver, BC, Canada) (NIPS’20). Curran Associates Inc.,
Red Hook, NY, USA, Article 413, 12 pages.
 
2385TSC: A Simple Two-Sided Constraint against Over-Smoothing KDD ’24, August 25–29, 2024, Barcelona, Spain
A DATASET SUMMARY
Table 7: The summary of datasets
Datasets
Classes Nodes Edges Features Train/Test Density
Cora
7 2708 5429 1433 140/1000 0.0014
Citeseer 6 3327 4732 2703 120/1000 0.0008
Pubmed 3 19717 44338 500 60/1000 0.0002
CoauthorCS 15 18333 81894 6805 300/1000 0.0005
AmazonPhoto 8 7650 119043 745 160/1000 0.0042
B DIFFERENCES FROM OTHER METHODS
We use a simple illustration to show the difference between our
method and other popular models such as GCN, SkipNode [ 18],
DropNode, DropEdge, DropMessage.
Figure 7: Comparing our method with popular models. We
use Vanilla GCN as an example for illustration. Where the
solid dots indicate the nodes, x1, x2, x3 are the features of
the three nodes, and z1, z2, z3 are the features of the nodes
obtained after one convolution operation for the correspond-
ing nodes.
C IMPLEMENTATION DETAILS
For the node classification task, we apply the fixed training/testing
split on all datasets, with 20 nodes per class for training and 1,000
nodes for testing. The hidden dimensions of all baselines use 256,
which is the same as the hidden dimensions of our model, and
we tuned these baselines for optimal performance. Our model has
two hyperparameters, the temperature coefficient 𝜏and𝜆in the
Random Masking strategy. The 𝜏generally takes the values of 0.4
and0.5.𝜆controls the masking rate and it is generally between 0.2
and0.8.D HYPERPARAMETER ANALYSIS
The hyperparameters of TSC mainly consist of the mask change
rate𝜆in the random mask as defined in Equation (5), and the tem-
perature coefficient 𝜏in the contrastive constraint as defined in
Equation (6) and (9). We take the Cora and Citeseer datasets as ex-
amples to analyze the impact of these parameters on the SGC+TSC
model in terms of ACC and MAD metrics.
D.1 Performance v.s. 𝜆
Figure 8 shows the impact of 𝜆on the ACC and MAD metrics for
SGC+TSC. From the first row of Figure 8, it is observed that the
accuracy of the model rapidly declines when the depth exceeds
16 layers. For instance, with 𝜆=1.5, there is a noticeable perfor-
mance drop in ACC on both the Cora and Citeseer datasets. The
second row of Figure 8 indicates that, as 𝜆increases, the MAD
metric does not significantly decrease but rather shows an increase.
This suggests that updating by columns can effectively mitigate
over-smoothing but does not address the decline in classification
performance. Therefore, further consideration of individuality en-
hancement for nodes is suggested again.
(a)𝜆v.s. ACC on Cora
 (b)𝜆v.s. ACC on Citeseer
(c)𝜆v.s. MAD on Cora
 (d)𝜆v.s. MAD on Citeseer
Figure 8: Performance v.s. 𝜆
D.2 Performance v.s. 𝜏
Figure 9 presents the effects of 𝜏on ACC and MAD metrics for
SGC+TSC. The first row of Figure 9 reveals that a too large value
of𝜏can lead to significant fluctuations in accuracy. For example,
at𝜏=1in Figure 9(a), the model exhibits fluctuations on the Cora
dataset that are distinct from other parameter settings, resulting in
slow performance improvement. The second row of Figure 9 shows
that while𝜏=1achieves good results on the MAD metric, it does
not translate into improved classification performance. Therefore,
we recommend choosing a smaller value for 𝜏(around 0.5) to ensure
better classification outcomes.
 
2386KDD ’24, August 25–29, 2024, Barcelona, Spain Furong Peng et al.
(a)𝜏v.s. ACC on Cora
 (b)𝜏v.s. ACC on Citeseer
(c)𝜏v.s. MAD on Cora
 (d)𝜏v.s. MAD on Citeseer
Figure 9: Performance v.s. 𝜏
E MORE EXPERIMENTS
We also added more experiments. For example, combining our
method with the popular GAT [ 31] and GCNII to validate the plug-
inability of the TSC by Table 8, 9, 10, and 11. As well, we demonstrate
the effect of our method on two heterogeneous datasets by Table
12 and 13.
Table 8: ACC comparison in different depth on Cora
Lay
er 2 Layer 4 Layer 8 Layer 16
GA
T 0.831 0.603 0.13 0.13
GA
T+TSC 0.827 0.816 0.811 0.773
Table 9: ACC comparison in different depth on Citeseer
Lay
er 2 Layer 4 Layer 8 Layer 16
GA
T 0.729 0.594 0.077 0.077
GA
T+TSC 0.724 0.717 0.713 0.716
Table 10: ACC comparison in different depth on Cora
Lay
er 2 Layer 4 Layer 8 Layer 16 Layer 32
GCNII 0.830
0.840 0.829 0.838 0.852
GCNII+TSC
0.606 0.772 0.815 0.855 0.860
Table 11: ACC comparison in different depth on Citeseer
Lay
er 2 Layer 4 Layer 8 Layer 16 Layer 32
GCNII 0.706 0.720
0.734 0.736 0.744
GCNII+TSC
0.607 0.744 0.748 0.737 0.739Table 12: ACC comparison in different depth on heteroge-
neous dataset Cornell
Lay
er 2 Layer 4 Layer 8 Layer 16 Layer 32
GCN
0.40 0.46 0.26 0.48 0.22
SGC 0.50 0.56 0.6 0.54 0.58
GCN+TSC 0.56 0.50
0.56 0.56 0.52
SGC+TSC 0.56 0.56 0.58 0.62 0.66
Table 13: ACC comparison in different depth on heteroge-
neous dataset Wisconsin
Lay
er 2 Layer 4 Layer 8 Layer 16 Layer 32
GCN
0.54 0.52 0.40 0.54 0.32
SGC 0.54 0.60 0.56 0.56 0.54
GCN+TSC 0.56 0.50
0.56 0.56 0.52
SGC+TSC 0.56 0.58 0.60 0.58 0.60
F UNDERSTANDING NEIGHBOR QUANTITY
AND NEIGHBOR QUALITY
Neighbor quality: the term neighbor quality refers to the vari-
ability of neighbor information of all nodes aggregated. The higher
this variability indicates that the neighbor quality of the node is
higher, otherwise it is lower. We can use an indicator Average
Mutual Overlapping (AMO) to describe the neighbor quality. A
smaller AMO indicates a better quality neighborhood, otherwise a
poorer one. Poor neighbor quality is also more likely to cause node
features to become indistinguishable. AMO is defined as follows:
𝑆𝑖,𝑗=(
1,𝐴𝑙
𝑖,𝑗>0∧𝑖≠𝑗
0,𝐴𝑙
𝑖,𝑗=0,𝐴𝑀𝑂 =Mean(SS𝑇))
𝑙is the number of layers, 𝑆is an indicator matrix, and 𝐴is the
adjacency matrix. As its name suggests, this indicator averages
the number of common neighbors of two nodes over all possible
pairs of nodes. It is a sufficient indicator since two nodes have
similar aggregation information when they have a large number of
common neighbors.
Number of neighbor: the information received from neighbors
grows exponentially, causing nodes to lose their own individual-
ity and accuracy decreases. The number of neighbors aggregated
by one node increases exponentially as the number of convolu-
tions increases, and the influx of a large number of nodes that are
not of the same class causes the node to eventually lose its own
individuality, leading to a decrease in the accuracy rate. We can
measure the impact of the number of neighbor of one node on
the node by Average number of different classes of nodes in
the neighbor(ANDCNN). When the ANDCNN is larger, the more
the neighbors of the node aggregation contain nodes that are not
of the same class, the more the node suffers from the impact. Nodes
are more likely to lose their individuality.
𝐴𝑁𝐷𝐶𝑁𝑁 =1
𝑁𝑁∑︁
𝑖=0𝑗=01[𝐿𝑖!=𝐿𝑗∧𝑆𝑖,𝑗=1],
Where𝐿𝑖and𝐿𝑗represent the labels of node 𝑖and node𝑗.𝑁is
the total number of nodes in the graph.
 
2387