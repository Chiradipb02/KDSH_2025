Towards Test Time Adaptation via Calibrated Entropy
Minimization
Hao Yang‚àó
National University of Defense Technology
ChangSha, ChinaMin Wang‚àó
National University of Defense Technology
ChangSha, China
Jinshen Jiang
National University of Defense Technology
ChangSha, ChinaYun Zhou
National University of Defense Technology
ChangSha, China
ABSTRACT
Robust models must demonstrate strong generalizability, even amid
environmental changes. However, the complex variability and noise
in real-world data often lead to a pronounced performance gap be-
tween the training and testing phases. Researchers have recently
introduced test-time-domain adaptation (TTA) to address this chal-
lenge. TTA methods primarily adapt source-pretrained models to a
target domain using only unlabeled test data. This study found that
existing TTA methods consider only the largest logit as a pseudo-
label and aim to minimize the entropy of test time predictions. This
maximizes the predictive confidence of the model. However, this
corresponds to the model being overconfident in the local test sce-
narios. In response, we introduce a novel confidence-calibration loss
function called Calibrated Entropy Test-Time Adaptation (CETA),
which considers the model‚Äôs largest logit and the next-highest-
ranked one, aiming to strike a balance between overconfidence and
underconfidence. This was achieved by incorporating a sample-
wise regularization term. We also provide a theoretical foundation
for the proposed loss function. Experimentally, our method out-
performed existing strategies on benchmark corruption datasets
across multiple models, underscoring the efficacy of our approach.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíRegularization; ‚Ä¢Networks
‚ÜíNetwork reliability.
KEYWORDS
test-time adaptation, model calibration, entropy minimization
ACM Reference Format:
Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou. 2024. Towards Test
Time Adaptation via Calibrated Entropy Minimization. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671672
‚àóBoth authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36716721 INTRODUCTION
Deep Neural Networks (DNNs) have revolutionized various fields
with their advanced capabilities, as demonstrated in landmark stud-
ies [13,24,28]. However, these sophisticated architectures are vul-
nerable to distribution shifts, leading to significant performance
degradation when the assumption that training and testing data are
from the same distribution does not hold [ 36]. Real-world scenar-
ios, characterized by natural variations (e.g., frost, rain, fog ) or data
corruptions (e.g., Gaussian noise ), often present these challenges,
as illustrated by the potential underperformance of a self-driving
car‚Äôs DNN in rural areas due to unfamiliar road markings and envi-
ronments [ 14]. This vulnerability underscores the importance of
enhancing model generalization from the source distribution ùëÉS(ùë•)
to the target distribution ùëÉT(ùë•).
In response to distribution shifts, the field has explored various
domain adaptation strategies, with Test-Time Adaptation (TTA)
emerging as a particularly promising approach [ 4,31,41,45]. TTA
allows DNNs to adapt to unseen target domains without requir-
ing labeled test data, leveraging the training dataset and feature
statistics of the training domain [ 8,38]. Among TTA strategies,
entropy minimization, notably through techniques like TENT [ 45],
optimizes the pre-trained feature extractor‚Äîminus the final lin-
ear layer‚Äîto reduce the entropy of predictions on unlabeled test
data. This enhances prediction certainty but can lead to overcon-
fident predictions, underscoring the importance of precise model
calibration[ 12] to strike a balance between accuracy and confidence.
Existing Limitations. The challenge of overly confident pre-
dictions and the necessity for model calibration are evident in the
Reliability Diagrams [ 12] of classifiers after applying TENT[ 45], as
shown in figure 1. These diagrams, which assess different strengths
of Gaussian noise on the CIFAR10-C and CIFAR100-C datasets, high-
light the limitations of entropy minimization methods in TTA due
to overconfidence. Such overconfidence is particularly problematic
in critical applications like autonomous driving, financial market
analysis, and healthcare, where it can result in severe consequences,
including unsafe driving decisions, inaccurate investment advice,
or incorrect medical diagnoses.
Our Solutions. While entropy minimization methods improve
prediction certainty, they carry the risk of generating overly confi-
dent predictions. This issue arises because existing TTA methods
often consider the largest prediction logit as a pseudo-label and
ignore the remaining logits, aiming to minimize prediction entropy.
Instead of merely addressing the problem of overconfidence, our
work underlines the critical role of enhancing model calibration.
3736
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou
This approach not only prevents overconfidence but also signifi-
cantly boosts TTA‚Äôs ability to maintain high accuracy amidst distri-
bution shifts. In this context, improved calibration means aligning
the model‚Äôs prediction confidence with its actual accuracy, ensur-
ing that the model‚Äôs confidence accurately reflects its predictive
capabilities.
Primary Contributions. To confront the calibration issue in-
herent in current TTA techniques, we introduce Calibrated Entropy
Test-time Adaptation (CETA). CETA takes into account both the
model‚Äôs primary logit and the next-highest-ranked one, aiming to
harmonize accuracy with confidence. This strategic approach is
designed to enhance TTA‚Äôs efficacy by effectively resolving the
calibration conundrum.
The proposed CETA does not require intricate modifications
to the existing models or training strategies. To substantiate the
efficacy of CETA in enhancing the model robustness, we conducted
experimental analyses on three universally acknowledged datasets:
CIFAR10-C, CIFAR100-C, and ImageNet-C. These results under-
score the superiority of the CETA. Specifically, compared with the
baseline model without any adaptation, CETA achieved a 33.6 %
improvement in performance in the evaluation from CIFAR-100
to CIFAR-100-C. This outcome also surpasses those of some cur-
rent mainstream techniques, such as BN Stats Adapt [ 20], TENT-
Online [46], and CoTTA-Online [48].
Our contributions are summarized as follows:
‚Ä¢To the best of our knowledge, our novel confidence-calibration
method is the first to address the challenge of test time do-
main adaptation by achieving a balance between overcon-
fidence and underconfidence, extending beyond traditional
approaches by considering both the highest logit and the
subsequent one in our proposed loss function.
‚Ä¢Through theoretical analysis and extensive experiments on
massive datasets, we demonstrate the effectiveness of the
proposed method in test time domain adaptation.
2 RELATED WORK
2.1 Test Time Adaptation
TTA leverages unlabeled test data to adapt a model, trained on a
source domain, to a target domain, addressing challenges posed
by distribution shifts. This contrasts with techniques like Domain
Adaptation (DA) [ 7,23,53] and Domain Generalization (DG) [ 32,
55,56], which aim to enhance model robustness through different
strategies. DA focuses on aligning the source and target domain
distributions to learn robust features, often requiring access to both
source domain labeled data and target domain unlabeled data. This
can be problematic in privacy-sensitive areas such as healthcare.
DG, on the other hand, seeks to train a model on one or more source
domains to generalize well across varied target domains without
needing target domain data. However, unlike DG, TTA capitalizes
on the availability of target domain test data during inference,
allowing for real-time adaptation and superior performance.
Existing TTA approaches, including test time (source-free) do-
main adaptation, test time batch adaptation, and online test time
adaptation [ 27], have seen various advanced algorithms tailored for
different learning contexts. These strategies typically adjust model
parameters using a single epoch or a small batch of test data, oftenfocusing on the application of Batch Normalization (BN) [ 20] layers,
which are crucial in encoding domain-specific knowledge [26].
TENT [ 46] refines pre-trained models by enhancing BN layers‚Äô
affine parameters and minimizing test batch entropy, inspiring sub-
sequent techniques [ 5,11,44,50]. Another approach, CoTTA [ 48],
gradually increases the quality of the pseudo-label of the teacher
network using various techniques over time. The model updates its
parameters using a weighted and enhanced average module. The
model implements a stochastic recovery strategy to enhance its ca-
pacity for handling nonstationary environments. While the TENT
method focuses primarily on adapting the batch normalization (BN)
layer, the CoTTA method updates the parameters throughout the
entire network. Some researchers [ 21,54] introduced entropy reg-
ulation to the semantic segmentation task, others [ 18,51] aimed
to reduce the amount of test data, and others [25, 30, 37] aimed to
improve the batch normalization layer.
Although numerous TTA methods have emerged, they share
similarities and distinctions. Because the test data lack target la-
bels, most current TTA strategies depend heavily on reducing the
entropy to enhance the model‚Äôs prediction confidence. However,
an unintended consequence of this process is that the model may
become overconfident and overfitted in local test scenarios. Recog-
nizing the potential issue of overconfidence or underconfidence in
the model‚Äôs predictions, our method fine-tunes the optimization
function for a balanced outcome, and we theoretically demonstrate
that the proposed method is effective for TTA.
2.2 Confidence Calibration
Typically, the focus is on the classification accuracy of a model, ig-
noring whether it matches the confidence of its predictions. As the
requirements for AI safety and trustworthiness gradually increase,
the predictive credibility of the model becomes increasingly impor-
tant [ 2,9,43]. Sometimes, a model may be overconfident, making
predictions with high confidence; however, the actual accuracy
may not be as high. Conversely, the model can be overly cautious,
making low-confidence predictions and missing opportunities for
correct predictions. Humans have a natural cognitive intuition
about probability; therefore, accurate confidence estimates provide
valuable additional information to users, especially for models such
as neural networks that have difficulty interpreting their classifica-
tion decisions. These accurate estimates help build user confidence
and credibility in the model, thereby improving its acceptance in
real-world applications.
Guo et al. [ 12] found that depth, width, weight decay, and batch
normalization play essential roles in model calibration. Some stud-
ies have employed post-hoc methods to correct model calibration
issues, such as Calibration Curves, Histogram Binning [ 52], matrix
scaling, and Temperature Scaling [ 12,17]. In addition, researchers
have proposed regularization methods to improve the calibration
quality of deep neural networks, such as weight decay [ 12], label
smoothing [ 33], and focal loss [ 29] and its variants [ 10,39,47].
Pereyra et al. [ 35] showed that calibration can be systematically
improved by imposing a penalty on low-entropy output distribu-
tions and that this penalty, combined with label smoothing, can
improve the performance of baseline models without modifying
the hyperparameters. This indicates that the regularization method
3737Towards Test Time Adaptation via Calibrated Entropy Minimization KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000014
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000015
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000016
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000017
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000018
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000014
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000015
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000016
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000017
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000042/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000010/uni00000018
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047
/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f
Figure 1: Reliability Diagrams[ 12] showcasing the calibration of classifiers using WideResNet-28-10 for CIFAR10-C and
ResNext29_32x4d for CIFAR100-C, after the application of TENT[ 45] with various levels of Gaussian noise. These diagrams
reveal the gap between expected and actual accuracies, with blue bins illustrating the model‚Äôs predicted confidence levels and
yellow bins showing the actual accuracy observed.
Test Samples
Train SamplesBase Model  
Reduce the gap 
between Top -2 
classes
Forward of TENT Backward
 Frozen Weights
Train Samples
Base Model  Conv
 BN Conv
 BN Conv
 BN FC
Conv BN Conv BN Conv BN FC
(a) Entropy Minimization: ‚Ñíùëáùê∏ùëÅùëáùëìùúÉùë•ùë°,ùë¶ùë°=1
ùëÅœÉùëñ=1ùê∂(‚àíùëûùëñùëôùëúùëî(ùëûùëñ))
(b) Calibrated -Entropy: ‚Ñíùê∂ùê∏ùëáùê¥ùëìùúÉùë•ùë°,ùë¶ùë°=1
ùëÅœÉùëñ=1ùê∂(‚àí(1+(ùëûùëó‚àíùëûùëò)ùõæ)ùëûùëñùëôùëúùëî(ùëûùëñ))
Conv BN Conv BN Conv BN FC
Forward of CETATend to be 
overconfidentTTA Model
Figure 2: Schematic of CETA, illustrating that CETA aims to reduce the gap between the TOP-2 logits in order to mitigate the
overconfidence brought about by solely minimizing entropy.
has broad applicability. Wei et al. [ 49] proposed a method called
Logit Normalization (LogitNorm) that aims to alleviate the abnor-
mally high confidence of neural networks for in-distribution and
out-of-distribution inputs, thereby solving the problem of overcon-
fidence. LogitNorm enables a simple fix for the cross-entropy loss
by enforcing a constant vector norm during training.
3 METHODOLOGY
3.1 Problem Definition
In our model, the probability of class ùëñamongùê∂classes, denoted
asùëûùëñ, is calculated from the logits ùëßand defined by the softmaxfunction:ùëûùëñ=ùëíùëßùëñ√çùê∂
ùëê=1ùëíùëßùëê. The cross-entropy loss, a measure of the
discrepancy between the predicted probability distribution and the
actual distribution, is given by:
Lùëêùëí(ùëìùúÉ(ùë•ùë°),ùë¶ùë°)=1
ùëÅ(‚àíùëùùëñ¬∑log(ùëûùëñ)) (1)
Here,ùëÅstands for the number of samples, ùëùùëñfor the actual label of
classùëñin one-hot vector form, and ùëûùëñfor the softmax-transformed
model output logits, which are probabilities in the range [0,1]
summing up to 1.
During test-time adaptation, in the absence of target labels, TENT
[46] utilizes entropy minimization to bolster prediction certainty
3738KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou
by diminishing the average prediction entropy. This process is
encapsulated by:
Lùëáùê∏ùëÅùëá(ùëìùúÉ(ùë•ùë°),ùë¶ùë°)=1
ùëÅùê∂‚àëÔ∏Å
ùëñ=1(‚àíùëûùëñlog(ùëûùëñ)) (2)
Yet, a significant limitation of this approach is the emergence of
overconfidence, characterized by probabilities gravitating towards
absolute certainty. This phenomenon can skew authentic probabil-
ity estimates and impose domain-specific limitations, prompting
our investigation into balancing considerations via loss function
refinement.
To address these concerns, our strategy is dual-pronged:
(1)Alleviate the propensity for over-confidence inherent
in entropy minimization, steering the network towards pre-
dictions that are both assertive and realistically calibrated.
(2)Preserve the core benefits of entropy minimization in
elevating prediction precision.
This refined approach underscores the critical balance between
maintaining high prediction accuracy and ensuring that predictions
remain grounded in realistic confidence levels, thereby enhancing
the model‚Äôs applicability and reliability across various domains. We
hypothesized that the limitations observed in preceding methodolo-
gies stem from an excessive focus on maximal network prediction.
Typical strategies, including TENT[ 45], have the highest logit value
of 1. Nonetheless, this unilateral emphasis fails to establish a rela-
tional understanding among diverse logits ùëûùëñ(whereùëñ‚â†largest ),
neglecting potential over- or under-confidence indicators within
the loss function. Consider an illustrative scenario. Given an image
of a cat with an actual class probability of 0.6, a conventional loss
function influenced predominantly by the highest prediction over-
looks the distribution nuances of the residual confidence across
alternative logits. We contend that the exclusive reliance on the
paramount logit is insufficient during test-time adaptation, necessi-
tating a more holistic evaluation of the loss function.
3.2 Proposed Method
Specifically, our method focuses on the network‚Äôs highest predic-
tion,ùëûùëó, and the prediction immediately following it, ùëûùëò. The for-
mulation of the Calibrated Entropy Test-time Adaptation loss is
given by:
Lùê∂ùê∏ùëáùê¥(ùëìùúÉ(ùë•ùë°),ùë¶ùë°)=1
ùëÅùê∂‚àëÔ∏Å
ùëñ=1 ‚àí(1+(ùëûùëó‚àíùëûùëò)ùõæ)ùëûùëñlog(ùëûùëñ),
whereùëûùëò(ùë•)=max
ùëñ
ùëûùëñ(ùë•)|ùëûùëñ(ùë•)<ùëûùëó(ùë•)	
.(3)
In this equation, ùõæis a hyperparameter that fine-tunes the cali-
bration,ùë•represents an individual data point, and ùëûùëò(ùë•)is defined
as the maximum value that comes after ùëûùëó(ùë•)when the predic-
tions are arranged in descending order. This delineation between
ùëûùëò(ùë•)andùëûùëó(ùë•)is pivotal, allowing for a calibration process that is
adapted for each specific sample, moving beyond a one-size-fits-all
approach. Each data point, ùë•, thus experiences a unique calibration
process, informed by its own prediction landscape, as denoted by
ùëûùëò(ùë•)andùëûùëó(ùë•).
This sample-specific calibration ensures that adjustments are
precisely aligned with the unique characteristics and confidencelevels of each prediction, enhancing the method‚Äôs effectiveness.
Equation 3 not only borrows the structure of entropy minimization
but also strategically modifies it to mitigate both overconfidence
and underconfidence by narrowing the gap between ùëûùëóandùëûùëò.
This encourages the network to more accurately reflect the local
domain‚Äôs dynamics. By adopting this refined loss function, we
achieve a delicate balance between over- and underconfidence.
3.3 Theoretical Evidence
We provide theoretical evidence that the proposed loss function is
more effective for confidence calibration than TENT[ 45] in the test
time-domain adaptation. First, we examine the instance-wise con-
ditional risk to reveal the relationship between the risk minimizer
and the actual class-posterior probability. We then demonstrate
that CETA is a classification-calibrated loss function and highlight
the superiority of the proposed loss function.
3.3.1 Sample-wise Conditional Risk. Follows [ 1,40]. First, we de-
fine the sample-wise conditional risk Rfor CETA as
Rùê∂ùê∏ùëáùê¥ =1
ùëÅùê∂‚àëÔ∏Å
ùëñ=1ùúÇùëñ(ùë•) ‚àí(1+(ùëûùëó‚àíùëûùëò)ùõæ)ùëûùëñlog(ùëûùëñ)(4)
whereùë•denotes the data point and ùúÇdenotes the actual class pos-
terior probability, which corresponds to the well-calibrated confi-
dence score of the classifier. Thus, Rdenotes the expected penalty
for the data point ùë•withùëûbeing the score function. By exploring
the instance-wise conditional risk R, the relationship between ùëû
andùúÇcan be derived to indicate the influence of the loss function
on classification calibration. Sample-wise conditional risks have
been extensively studied in [ 3]. In this study, we generalize the
results of the proposed loss function and highlight its superiority
through theoretical evidence.
The optimization of Rùê∂ùê∏ùëáùê¥ can be represented as follows:
minùëû1
ùëÅùê∂‚àëÔ∏Å
ùëñ=1ùúÇùëñ ‚àí(1+(ùëûùëó‚àíùëì ùëûùëó)ùõæ)ùëûùëñlog(ùëûùëñ),
subject toùê∂‚àëÔ∏Å
ùëñ=1ùëûùëñ=1.(5)
whereùëûùëòis denoted by ùëì ùëûùëó.
To solve the constrained optimization in Eq. 5, we can consider
the Lagrangian form as
L(ùëû,ùúÜ)=1
ùëÅùê∂‚àëÔ∏Å
ùëñ=1ùúÇùëñ ‚àí
1+(ùëûùëñ‚àíùëì(ùëûùëñ))ùõæ
ùëûùëñlog(ùëûùëñ)+ùúÜ ùê∂‚àëÔ∏Å
ùëñ=1ùëûùëñ‚àí1!
(6)
whereùúÜis the Lagrange multiplier for constraint. By taking the
derivatives with respect to ùëûùëñ,ùúÜcan be solved at the optimal ùëû‚àóas
3739Towards Test Time Adaptation via Calibrated Entropy Minimization KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
follows:
ùúï
ùúïùëûùëñL(ùëû,ùúÜ)
ùëû=ùëû‚àó=0,
‚àí
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùúÇùëñùõæ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ
‚àíùúÇùëñ 1+ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ(1+logùëû‚àó
ùëñ)+ùúÜ=0,
ùúÜ=ùúÇùëñ
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ
+ 1+ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ  1+logùëû‚àó
ùëñ
.
ùúÇùëñ=ùúÜ

1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1
ùëû‚àó
ùëñlogùëû‚àó
ùëñ
+ùúÜ
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ 
1+logùëû‚àó
ùëñ(7)
Then, we rewrite ùúÇùëñas a function of ùúÜandùëû‚àó
ùëñ, using Eq. 7. Because
ùúÇùëñfollows a probability distribution, ùúÜcan be rewritten as a function
ofùëû‚àó
ùëñas
ùê∂‚àëÔ∏Å
ùëñùúÇùëñ=1=ùê∂‚àëÔ∏Å
ùëñùúÜ

1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1
ùëû‚àó
ùëñlogùëû‚àó
ùëñ
+ùê∂‚àëÔ∏Å
ùëñùúÜ
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ 
1+logùëû‚àó
ùëñ,
ùúÜ=ùê∂‚àëÔ∏Å
ùëñ
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ
+ 1+ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ  1+logùëû‚àó
ùëñ
.(8)
By replacing ùúÜin Eq. 7, we can write ùúÇùëñas a function of ùëû‚àó
ùëñas
follows:
ùúÇùëñ=√çùê∂
ùëñ 
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ!
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ
 
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ!
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ+
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ 
1+logùëû‚àó
ùëñ
+√çùê∂
ùëñ
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ 
1+logùëû‚àó
ùëñ
 
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ!
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ+
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ 
1+logùëû‚àó
ùëñ
=1   
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ!
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ+
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ
1+logùëû‚àó
ùëñ!
1
√çùê∂
ùëñ  
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ!
ùõæ
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ+
1+
ùëû‚àó
ùëñ‚àíùëì
ùëû‚àó
ùëñùõæ
1+logùëû‚àó
ùëñ!
=1
ùúô
ùëû‚àó
ùëñ
√çùê∂ùëê1
ùúô
ùëû‚àó
ùëñ(9)
whereùúô ùëû‚àó
ùëñ=
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ+
 1+ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ  1+logùëû‚àó
ùëñ. Consequently, given ùëû‚àó
ùëñ, the true
class-posterior probability ùúÇùëñcan be recovered using the transfor-
mation in Eq. 9.
3.3.2 Over-confidence and Under-confidence. Next, we discuss the
overconfidence and underconfidence scenarios of risk minimizerùëû‚àó. First, we define ùúÇ-overconfidence ( ùúÇOC) andùúÇ-underconfidence
(ùúÇUC) for data ùë•
max
ùëñùëû‚àó
ùëñ(ùë•)‚àímax
ùëñùúÇùëñ(ùë•)>0,ùëû‚àó
ùëñisùúÇOC,
max
ùëñùëû‚àó
ùëñ(ùë•)‚àímax
ùëñùúÇùëñ(ùë•)<0,ùëû‚àó
ùëñisùúÇUC.(10)
Next, we show the proposed loss function is classification calibrated
[1, 40] compared to entropy minimization.
Theorem 3.1. For anyùõæ‚â•1, the proposed loss function is classification-
calibrated and has a strictly order-preserving property compared with
the entropy minimization loss.
Proof. We can represent ùúô ùëû‚àó
ùëñas
ùúô ùëû‚àó
ùëñ=
1‚àíùúïùëì
ùúïùëû‚àó
ùëñ
ùõæ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ‚àí1ùëû‚àó
ùëñlogùëû‚àó
ùëñ
+ 1+ ùëû‚àó
ùëñ‚àíùëì ùëû‚àó
ùëñùõæ  1+logùëû‚àó
ùëñ(11)
ùúÇùëñcan be formulated as1
ùúô(ùëû‚àó
ùëñ)√çùê∂
ùëê1
ùúô(ùëû‚àóùëê)in Eq. 9. Now we simplify Eq. 9 by
taking‚Ñé(ùë£ùëñ)=1
ùúô(ùë£ùëñ)the function ‚Ñé(ùë£)can be formulated asfollows:
‚Ñé(ùë£)=1
ùúô(ùë£)=(1
1+logùë£, ùëó=ùëò
1
ùõæ(ùë£‚àíùê∂)ùõæ‚àí1ùë£logùë£+(1+(ùë£‚àíùê∂)ùõæ)(1+logùë£), ùëó‚â†ùëò
(12)
Similarly, for entropy minimization, we have
‚Ñéùë°ùëíùëõùë°(ùë£)=1
ùúôùë°ùëíùëõùë°(ùë£)=1
1+logùë£(13)
Using the function ‚Ñé, Eq. 9 can be rewritten as follows:
ùúÇùëñ=‚Ñé ùëû‚àó
ùëñ
√çùê∂ùëê‚Ñé(ùëû‚àóùëê). (14)
Considering the scenario ùëó=ùëò, the derivative of ‚Ñé(ùë£)is
ùúï‚Ñé
ùúïùë£=‚àí1
ùë£(1+logùë£)2<0 (15)
Without loss of generality, the true class-posterior probability ùúÇùëñis
positive. Considering scenario ùëó‚â†ùëò, we take the derivative of ‚Ñé(ùë£)
asfollows:
ùúï‚Ñé
ùúïùë£=‚àíùõæ(ùõæ‚àí1) (ùë£‚àíùê∂)ùõæ‚àí2ùë£2logùë£
ùë£ ùõæ(ùë£‚àíùê∂)ùõæ‚àí1ùë£logùë£+(1+(ùë£‚àíùê∂)ùõæ)(1+logùë£)2
+‚àí2ùõæùë£ (ùë£‚àíùê∂)ùõæ‚àí1(1+logùë£)‚àí(1+(ùë£‚àíùê∂)ùõæ)
ùë£ ùõæ(ùë£‚àíùê∂)ùõæ‚àí1ùë£logùë£+(1+(ùë£‚àíùê∂)ùõæ)(1+logùë£)2(16)
Note that‚àí2ùë£ (ùë£‚àíùê∂)ùõæ‚àí1(1+logùë£)<0, and‚àí(1+(ùë£‚àíùê∂)ùõæ)<
0. For anyùõæ‚â•1, we have‚àíùõæ(ùõæ‚àí1) (ùë£‚àíùê∂)ùõæ‚àí2ùë£2logùë£<0,
ùúï‚Ñé
ùúïùë£<0. We conclude thatùúï‚Ñé
ùúïùë£<0in all scenarios renders ‚Ñéa
strictly decreasing function. Thus, the proposed loss function is
classification-calibrated and has a strictly order-preserving property
compared with the entropy minimization loss. ‚ñ°
3740KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou
4 EXPERIMENTS
4.1 Benchmarks
To effectively evaluate our approach, we conduct massive experi-
ments on various benchmarks.
Datasets: CIFAR10-C[ 15] is derived from the standard CIFAR-10
dataset, featuring algorithm-generated corruptions across five lev-
els of intensity and 15 distinct corruption types. CIFAR100-C builds
upon CIFAR10-C‚Äôs concept by incorporating 100 classes, thus ex-
panding the corruption challenge. ImageNet-C[ 15] stands as a for-
midable benchmark with 1000 classes, recognized for its complexity
and the variety of corruption scenarios it presents.
Baselines: We compared the proposed method with four base-
line:Source, BN Stats Adapt, TENT-Online and CoTTA-Online. The
"Source" baseline refers to the model‚Äôs performance when trained
without specific adaptations or enhancements to tackle image cor-
ruption. "BN Stats Adapt" stands for Batch Normalization Statis-
tics Adaptation, a technique that leverages the entirety of the test
dataset to recalibrate the batch normalization layers within the
model. "TENT-Online" (Test-time Entropy Minimization) introduces
a dynamic adjustment strategy during inference. The method in-
volves back-propagating gradients based on prediction entropy,
leading to real-time updates to the scale and shift parameters of
the model‚Äôs batch normalization layers. By minimizing prediction
entropy, TENT-Online seeks to enhance the model‚Äôs certainty in
its output, potentially improving accuracy in corrupted conditions.
"CoTTA-Online" (Continual Test-Time Adaptation) employs a so-
phisticated teacher-student paradigm combined with test-time data
augmentation to mitigate prediction errors stemming from image
corruption. One of its key strategies involves the selective rever-
sion of certain pre-trained neuronal weights, thereby bolstering the
model‚Äôs adaptability to novel environmental changes. For fairness
in comparison, this study implements an online parameter updat-
ing technique. Specifically, the model re-initializes for each distinct
corruption type from a saved state.
Networks: Our evaluation encompasses five diverse networks to
ensure a comprehensive assessment of the proposed method CETA:
WideResNet-28-10, WideResNet-40-2, PreActResNet-18, ResNet-
50 and ResNext29_32x4d. These networks are selected for their
variety in architecture and performance characteristics, offering a
broad spectrum of insights into how different models respond to
the proposed calibration method under corrupted conditions.
4.2 Experiments on Corruptions CIFAR10-C
4.2.1 Experimental Setting. Our experimental setting was designed
to ensure reproducibility and fairness using models sourced from
the RobustBench benchmark1. This platform is renowned for its
standardization of model performance assessment, ensuring con-
sistent experimental conditions.
For the corruption tests between CIFAR-10 and CIFAR10-C, we
adhered to the established protocols provided by TENT, employing
the WideResNet-28-10 and WideResNet-40-2 structures. We de-
rived our source-pretrained models from relevant scholarly works
to maintain consistency with the community standards. Specifically,
1https://robustbench.github.io/the checkpoints for WideResNet-28-10 are adopted from Robust-
Bench‚Äôs default models, as detailed by Croce et al. (2021)[ 6]. For
the WideResNet-40-2 architecture, we refer to the model specified
by Hendrycks et al. (2019)[ 16]. The PreActResNet-18 setup aligns
with the findings of Kireev et al. (2021) [22].
All three architectures were optimized with the Adam algorithm,
using settings from their official deployments. We standardized ex-
periments by setting the learning rate at 0.001, with an exponential
decay rate of 0.9 and a weighted decay of zero. To balance compu-
tational load and ensure consistency, a batch size of 200 was used
for each training cycle. This method adheres to standard practices,
facilitating fair comparison and reliable replication of our findings.
4.2.2 Results. Table 1 summarizes the performance outcomes for
the WideResNet-28-10, WideResNet-40-2, and PreActResNet-18
models under the most challenging corruption level of 5 according
to the TENT‚Äôs robustness representation. This stringent level was
chosen for its decisive insights into maximum robustness; other
levels were omitted because of consistent findings and space con-
straints.
A stark revelation from these results is the vulnerability of di-
rectly applying source-pre-trained models to test data under distri-
bution shifts. For instance, the WideResNet-28-10 model recorded
a staggering error rate of 43.5%, highlighting the repercussions of
substantial distribution disparities between the target and source
data and underscoring the inadequacy of reliance on pre-trained
models alone.
Conversely, implementing the ‚ÄôBN Stats Adapt‚Äô strategy, which
recalibrates batch normalization (BN) statistics based on contem-
porary data batches, dramatically curtailed the error rate to 20.4%.
This reduction, amounting to a 53.1% decrease, affirms the efficiency
of intuitively adjusting the BN parameters, a practice supported by
studies elucidating the ties between BN statistics and distribution
shifts [19, 42, 57].
TENT[ 45], which aims for heightened prediction confidence
by fine-tuning the model parameters, further reduces the error
figure by 1.8%. The recent advent of CoTTA-online[ 48], which
leverages test-phase data augmentation to bolster model robust-
ness, mirrors the effectiveness of TENT despite struggling with
real-time data flux. Intriguingly, the CETA method consistently
outperformed the WideResNet-40-2 and PreActResNet-18 mod-
els owing to the unique consideration of both the maximum and
penultimate logit disparities. This approach effectively tempered
the overconfidence‚Äìunderconfidence dynamics during testing. No-
tably, CoTTA-online‚Äôs underperformance compared to TENT-online
across these networks suggests a potential limitation in the gen-
eralization process of the proposed test-time augmentation over
varying architectures.
4.3 Experiments on Corruptions CIFAR100-C
4.3.1 Experimental Setting. To comprehensively assess the pro-
posed CETA method, we evaluated its performance in corruption
scenarios transitioning from the CIFAR100 to CIFAR100-C datasets.
Our experimental backbone was anchored to the ResNext29_32x4d
network, a well-regarded RobustBench model. The checkpoint for
this model was adopted from the seminal work of Hendrycks et
3741Towards Test Time Adaptation via Calibrated Entropy Minimization KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Classification Error Rate (%) in the Standard CIFAR10-to-CIFAR10C Online Test-Time Adaptation Task: Evaluation on
WideResNet-28-10, WideResNet-40-2, and PreActResNet-18 at Corruption Severity Level 5.
Method
Gaussianshotimpulse defo
cus
glassmotion zo
om
sno
w
fr
ostfog
brightnesscontrast
elastic_transpixelatejp
egMean
WideResNet-28-10
Sour
ce 72.3
65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5
BN
Stats Adapt 28.1
26.1 36.3 12.8 35.3 14.2 12.1 17.3 17.4 15.3 8.4 12.6 23.8 19.7 27.3 20.4
TEN
T-online [46] 24.8
23.5 33.0 12.0 31.8 13.7 10.8 15.9 16.2 13.7 7.9 12.1 22.0 17.3 24.2 18.6
Co
TTA-online [48] 24.4
22.7 28.0 12.6 30.5 13.8 11.7 16.9 17.0 14.5 8.3 12.5 22.8 19.0 22.7 18.5
CET
A (Ours) 24.5
23.0 32.5 11.9 31.4 13.5 10.7 15.6 16.1 13.6 7.8 11.9 21.9 16.9 23.8 18.3
WideResNet-40-2
Sour
ce 28.8
23.0 26.2 9.5 20.6 10.6 9.3 14.2 15.3 17.5 7.6 20.9 14.8 41.3 14.7 18.3
BN
Stats Adapt 18.5
16.2 22.3 8.9 21.9 10.5 9.7 12.8 13.3 15.0 7.6 11.9 16.3 15.0 17.5 14.5
TEN
T-online [46] 15.6
13.2 18.8 7.9 18.1 9.0 8.0 10.4 10.8 12.3 6.7 10.1 14.0 11.4 14.8 12.1
Co
TTA-online [48] 15.1 13.2 17.0 9.0
19.1 10.4 9.6 12.5 12.0 15.6 7.5 15.3 15.2 11.7 14.4 13.2
CET
A (Ours) 15.4 13.1 18.4 7.8
17.9 8.9 7.8 10.2 10.8 12.1 6.7 9.9 13.9 11.2 14.8 11.9
Pr
eActResNet-18
Sour
ce 22.2
18.9 33.3 10.1 24.0 12.4 9.8 14.6 14.5 22.5 8.2 54.4 14.5 20.3 11.8 19.4
BN
Stats Adapt 15.4
14.4 21.7 9.8 23.2 12.1 11.0 14.2 15.0 17.2 9.3 12.9 17.7 14.4 16.2 15.0
TEN
T-online [46] 13.4
12.3 18.7 8.1 20.1 10.1 9.0 12.2 12.8 13.6 7.9 11.6 14.7 11.4 13.5 12.6
Co
TTA-online [48] 14.4
13.1 18.2 9.6 20.9 11.7 10.6 14.5 14.6 16.5 9.1 12.8 16.3 12.8 14.4 14.0
CET
A (Ours) 12.9
11.8 18.5 8.0 19.8 9.9 8.8 12.1 12.7 13.2 7.9 11.5 14.6 11.0 13.3 12.4
Table 2: Classification Error Rate (%) in the Standard CIFAR100-to-CIFAR100C Online Test-Time Adaptation Task: Evaluation
on ResNext29_32x4d at Corruption Severity Level 5.
Metho
d
Gaussianshotimpulse defo
cus
glassmotion zo
om
sno
w
fr
ostfog
brightnesscontrast
elastic_transpixelatejp
egMean
Sour
ce 73.0
68.0 39.4 29.3 54.1 30.8 28.8 39.5 45.8 50.3 29.5 55.1 37.2 74.7 41.2 46.4
BN
Stats Adapt 42.1
40.7 42.7 27.6 41.8 29.7 27.9 34.9 35.0 41.5 26.5 30.3 35.7 32.9 41.1 35.4
TEN
T-online [46] 39.4
37.2 37.2 26.3 39.4 28.5 26.5 33.1 33.3 38.3 25.8 29.3 34.0 30.4 39.1 33.2
Co
TTA-online [48] 40.1
38.1 40.2 27.4 39.6 29.5 27.6 35.2 34.3 43.2 26.4 30.1 35.7 31.6 37.5 34.4
CET
A (Ours) 36.9
34.5 33.3 25.0 37.4 26.9 25.2 30.2 31.0 33.2 23.7 27.6 32.5 28.0 36.1 30.8
Table 3: Classification Error Rate (%) in the Standard ImageNet-to-ImageNet-C Online Test-Time Adaptation Task: Evaluation
on ResNet-50 at Corruption Severity Level 5.
Metho
d
Gaussianshotimpulse defo
cus
glassmotion zo
om
sno
w
fr
ostfog
brightnesscontrast
elastic_transpixelatejp
egMean
Sour
ce 95.3
94.6 95.3 84.9 91.1 86.8 77.2 84.4 79.7 77.3 44.4 95.6 85.2 76.9 66.7 82.4
BN
Stats Adapt 87.6
87.5 87.8 87.8 88.0 78.3 64.4 67.6 70.6 54.8 36.4 89.2 58.0 56.4 66.5 72.1
TEN
T-online [46] 85.7
84.8 85.4 85.7 85.6 74.5 61.2 64.4 68.2 51.2 35.6 86.3 55.2 52.5 62.4 69.2
Co
TTA-online [48] 87.3
87.2 87.4 88.3 87.8 77.4 64.5 67.0 70.3 53.5 36.1 88.5 57.6 55.8 66.1 71.7
CET
A (Ours) 85.3
84.4 85.1 85.1 85.2 73.7 60.5 63.5 67.4 50.5 35.4 86.0 54.5 51.5 61.6 68.6
al. [16], ensuring that our foundational setup was aligned with the
respective standards in the field.
For the training optimization process, we harnessed the capa-
bilities of the SGD optimizer. Key parameters were fine-tuned to
balance efficiency and learning stability: a learning rate of 0.001
and an exponential decay rate of 0.9 to ensure gradual and adaptive
learning. We intentionally performed weight decay to prevent un-
necessary regularization, focusing purely on learning the nuancesof the current data. Finally, a batch size of 200 was chosen, offering
a diverse data spectrum for each iteration without overburdening
the computational resources.
4.3.2 Results. The outcomes of our evaluation are summarized
in Table 2, with the mirroring trends observed in Table 1. Specifi-
cally, the source-pretrained model recorded the highest error rate,
clocking in at an average of 46.4%. This statistic underscores the
3742KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou
Table 4: Accuracy Comparison of the Proposed Method with State-of-the-Art Approaches (ODS [ 58] and SAR [ 34]) on CIFAR10-C
and ImageNet-C Datasets.
Method
Gaussianshotimpulse defo
cus
glassmotion zo
om
sno
w
fr
ostfog
brightnesscontrast
elastic_transpixelatejp
egMean
ResNet18-CIF
AR10-C
ODS
[34] 68.1 66.0
72.8 87.8 57.6 90.9 89.9 84.6 87.0 85.2 86.3 90.2 78.7 85.2 82.1 80.8
ODS(
+Ours) 66.3 68.5
73.1 89.0 56.6 91.8 88.0 87.5 88.0 85.9 89.8 91.4 80.0 84.5 82.4 81.5
ResNet50(BN)-ImageNet-C
SAR
[58] 29.9
30.3 31.6 28.4 28.4 41.8 49.3 47.1 42.2 57.8 67.4 37.4 54.5 58.2 52.3 43.8
SAR(
+Ours) 30.4
30.7 31.7 29.0 29.0 42.3 49.5 47.5 42.5 58.0 67.5 38.2 54.8 58.5 52.5 44.2
critical need for domain adaptation during the test period, given
the marked performance degradation without this step.
In our test roster, the BN Stats Adapt method lowered the error
margin to 35.4%, signifying a noteworthy improvement over the
baseline model. However, the CoTTA-online[ 48] strategy fared
slightly better, increasing the error rate to 34.4%. Although this
outperforms BN Stats Adapt, it does not match the efficiency of
TENT, highlighting its relative ineffectiveness in scenarios requiring
single-domain online test time adaptation.
However, the proposed CETA method requires improved ro-
bustness. It recorded an error rate of only 30.8%, a significant step
forward in minimizing classification errors. When juxtaposed with
other methodologies‚ÄîSource, BN Stats Adapt, TENT-online[ 45],
and CoTTA-online‚ÄîCETA reduced error rates by 15.6%, 4.6%, 2.4%,
and 3.6% respectively, demonstrating their superior effectiveness.
Notably, while CETA‚Äôs performance across various networks,
from CIFAR10 to CIFAR10-C, is commendable, the margin of im-
provement over TENT and CoTTA-online is not vast, with only a
0.3% reduction in error rate compared with TENT. However, the
landscape shifts in evaluations from CIFAR100 to CIFAR100-C,
where CETA shows marked superiority with a 2.4% increase over
TENT. This enhanced performance in the more category-rich and
complex CIFAR100 environment suggests that a more nuanced label
structure and the lower performance of the baseline model amplify
CETA‚Äôs efficacy of CETA. It appears particularly effective when
the model faces overconfidence in predictions during single test
scenarios, reinforcing CETA‚Äôs value in balancing overconfidence
and underconfidence during model testing.
4.4 Experiments on Corruptions ImageNet-C
Seeking broader validation of CETA‚Äôs effectiveness of CETA, partic-
ularly against 2D corruption, our experiments were extended to a
more demanding dataset: the transition from ImageNet to ImageNet-
C. This dataset is significantly more complex, featuring 1000 classes,
and presents a sterner test than CIFAR-10 and CIFAR-100.
4.4.1 Experimental Setting. For the trials on the ImageNet-C dataset,
we employed a standard-trained ResNet-50 model sourced from Ro-
bustBench. The training regimen involved the same SGD optimizer,
adjusted to a learning rate of 0.00025 to cater to the intricacies of
the richer dataset. We maintained the exponential decay rate at 0.9
and the weight decay at 0, promoting stability during the learningprocess. Considering the dataset‚Äôs complexity and computational
demands, the batch size was 64.
4.4.2 Results. The results detailed in Table 3 reveal the formidable
challenges posed by ImageNet-C. The source-pretrained ResNet-50
model achieved a high error rate of 82.4%, indicating that most
samples were misclassified, creating confusion for the network
during sample classification.
BN Stats Adapt registered a performance improvement in this
high-stakes setting, lowering the error statistics to 72.1%‚Äîa decent
10.3% gain compared with the source-pretrained model. However,
TENT-online emerged as the best performer among the existing
strategies. CoTTA-online, trailing TENT-online, surpassed BN Stats
Adapt with an error rate of 71.7%.
Our CETA method sets a new benchmark, achieving an error rate
of just 68.6% and establishing itself as state-of-the-art compared
to existing methodologies. This performance underscores CETA‚Äôs
robustness, with error rate reductions of 13.8%, 3.5%, 0.6%, and 3.1%
relative to the Source, BN Stats Adapt, TENT-online, and CoTTA-
online, respectively. These results affirm the strategic potency of
CETA, particularly in complex class-dense environments where
precise adaptation strategies are paramount.
4.5 Further Analysis
To further highlight the versatility and advancement of our pro-
posed method, we conducted comparative studies against the recent
state-of-the-art methods [ 34,58]. Our results demonstrate that our
method can enhance the effectiveness of existing SOTA approaches.
For ODS [ 58], which addresses changes in both covariate and
label distributions during the test phase, we adhered to the experi-
mental setup and implementation details outlined in the original
paper. We compared our proposed method with ODS using the
ResNet18 model on the CIFAR10-C dataset, focusing specifically on
the most severe corruption level of 5. The outcomes, as detailed in
Table 4 (Upper), indicate that our method can significantly boost
model performance when faced with simultaneous changes in co-
variate and label distributions at test time.
Furthermore, for SAR [ 34], aimed at stabilizing online Test-Time
Adaptation (TTA) under challenging conditions like mixed shifts,
small batches, and imbalanced label shifts, our method was also
assessed. SAR has achieved state-of-the-art performance under
these wild settings. Following their experimental setup and imple-
mentation details, we compared our proposed method with SAR
3743Towards Test Time Adaptation via Calibrated Entropy Minimization KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
using the ResNet-50(BN) model on the ImageNet-C dataset. The
results, shown in Table 4 (Bottom), demonstrate that our method
not only compares favorably but also enhances model performance
in comparison to SAR, underscoring its effectiveness in complex
and dynamic test environments.
5 CONCLUSION
In this study, we developed a novel loss function aimed at address-
ing the issue of model overconfidence in test-time-domain adapta-
tion (TTA). Our method introduces a confidence-calibration term,
which helps balance accuracy with model confidence, while also
leveraging entropy minimization to enhance generalizability across
different datasets. The effectiveness of our approach is backed by
both theoretical analyses and empirical evidence, showing signifi-
cant improvements in domain adaptation tasks. Our results high-
light the importance of confidence calibration in creating machine
learning models that are not only more interpretable but also more
trustworthy. Although our framework has limitations, such as not
dynamically adjusting calibration based on the environment, it still
achieves top performance and shows robustness against parameter
changes. Future research will look into expanding our method to
various network architectures and learning paradigms, as well as
combining it with other advanced strategies to create more adapt-
able machine learning models.
REFERENCES
[1]Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. 2006. Convexity, classifi-
cation, and risk bounds. J. Amer. Statist. Assoc. 101, 473 (2006), 138‚Äì156.
[2]Keith Beven and Andrew Binley. 1992. The future of distributed models: model
calibration and uncertainty prediction. Hydrological processes 6, 3 (1992), 279‚Äì298.
[3]Nontawat Charoenphakdee, Jayakorn Vongkulbhisal, Nuttapong Chairatanakul,
and Masashi Sugiyama. 2021. On focal loss for class-posterior probability estima-
tion: A theoretical perspective. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 5202‚Äì5211.
[4]Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. 2022. Contrastive
test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 295‚Äì305.
[5]Dong Chen, Hongqing Zhu, and Suyi Yang. 2023. UC-SFDA: Source-free domain
adaptation via uncertainty prediction and evidence-based contrastive learning.
Knowledge-Based Systems (2023), 110728.
[6]Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo
Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias
Hein. 2021. RobustBench: a standardized adversarial robustness benchmark. In
Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track. 1‚Äì29.
[7]Marcus de Carvalho, Mahardhika Pratama, Jie Zhang, and Edward Yapp Kien Yee.
2022. ACDC: Online unsupervised cross-domain adaptation. Knowledge-Based
Systems 253 (2022), 109486.
[8]Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Sch√∂lkopf.
2021. Source-free adaptation to measurement shift via bottom-up feature restora-
tion. arXiv preprint arXiv:2107.05446 (2021).
[9]Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok
Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung,
Ribana Roscher, et al .2023. A survey of uncertainty in deep neural networks.
Artificial Intelligence Review 56, Suppl 1 (2023), 1513‚Äì1589.
[10] Arindam Ghosh, Thomas Schaaf, and Matthew Gormley. 2022. AdaFocal:
Calibration-aware Adaptive Focal Loss. In Advances in Neural Information Pro-
cessing Systems. 1583‚Äì1595.
[11] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-
Ju Lee. 2022. NOTE: Robust continual test-time adaptation against temporal
correlation. In Advances in Neural Information Processing Systems. 27253‚Äì27266.
[12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration
of modern neural networks. In International Conference on Machine Learning.
1321‚Äì1330.
[13] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. 2017. Mask r-cnn.
InProceedings of the IEEE international conference on computer vision. 2961‚Äì2969.
[14] Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural net-
work robustness to common corruptions and perturbations. arXiv preprintarXiv:1903.12261 (2019).
[15] Dan Hendrycks and Thomas G Dietterich. 2018. Benchmarking neural net-
work robustness to common corruptions and surface variations. arXiv preprint
arXiv:1807.01697 (2018).
[16] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer,
and Balaji Lakshminarayanan. 2020. AugMix: A Simple Data Processing Method
to Improve Robustness and Uncertainty. In International Conference on Learning
Representations. 1‚Äì15.
[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in
a Neural Network. stat1050 (2015), 9.
[18] Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Neva-
tia, and Ser-Nam Lim. 2021. Mixnorm: Test-time adaptation through online
normalization estimation. arXiv preprint arXiv:2110.11478 (2021).
[19] Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time
with adaptive instance normalization. In Proceedings of the IEEE International
Conference on Computer Vision. 1501‚Äì1510.
[20] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In International Conference
on Machine Learning. 448‚Äì456.
[21] Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. 2021.
Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355 (2021).
[22] Klim Kireev, Maksym Andriushchenko, and Nicolas Flammarion. 2022. On the
effectiveness of adversarial training against common corruptions. In Uncertainty
in Artificial Intelligence. 1012‚Äì1021.
[23] Wouter M Kouw and Marco Loog. 2019. A review of domain adaptation without
target labels. IEEE Transactions on Pattern Analysis and Machine Intelligence 43, 3
(2019), 766‚Äì785.
[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012).
[25] Jogendra Nath Kundu, Akshay Kulkarni, Amit Singh, Varun Jampani, and
R Venkatesh Babu. 2021. Generalize then adapt: Source-free domain adaptive
semantic segmentation. In Proceedings of the IEEE International Conference on
Computer Vision. 7046‚Äì7056.
[26] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. 2016. Re-
visiting batch normalization for practical domain adaptation. arXiv preprint
arXiv:1603.04779 (2016).
[27] Jian Liang, Ran He, and Tieniu Tan. 2023. A comprehensive survey on test-time
adaptation under distribution shifts. arXiv preprint arXiv:2303.15361 (2023).
[28] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and
Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 2117‚Äì2125.
[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE International
Conference on Computer Vision. 2980‚Äì2988.
[30] Yuang Liu, Wei Zhang, and Jun Wang. 2021. Source-free domain adaptation for
semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. 1215‚Äì1224.
[31] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. 2016. Unsu-
pervised domain adaptation with residual transfer networks. Advances in neural
information processing systems 29 (2016).
[32] Krikamol Muandet, David Balduzzi, and Bernhard Sch√∂lkopf. 2013. Domain
generalization via invariant feature representation. In International Conference
on Machine Learning. 10‚Äì18.
[33] Rafael M√ºller, Simon Kornblith, and Geoffrey Hinton. 2019. When does label
smoothing help?. In Advances in Neural Information Processing Systems. 4694‚Äì
4703.
[34] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin
Zhao, and Mingkui Tan. 2023. Towards stable test-time adaptation in dynamic
wild world. In International Conference on Learning Representations. 1‚Äì27.
[35] Gabriel Pereyra, George Tucker, Jan Chorowski, ≈Åukasz Kaiser, and Geoffrey
Hinton. 2017. Regularizing neural networks by penalizing confident output
distributions. arXiv preprint arXiv:1701.06548 (2017).
[36] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D
Lawrence. 2008. Dataset shift in machine learning. Mit Press.
[37] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh
Garg, In So Kweon, and Kuk-Jin Yoon. 2022. MM-TTA: Multi-Modal Test-Time
Adaptation for 3D Semantic Segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 16928‚Äì16937.
[38] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz
Hardt. 2020. Test-time training with self-supervision for generalization under
distribution shifts. In International Conference on Machine Learning. 9229‚Äì9248.
[39] Linwei Tao, Minjing Dong, and Chang Xu. 2023. Dual Focal Loss for Calibration.
InInternational Conference on Machine Learning. 33833‚Äì33849.
[40] Ambuj Tewari and Peter L Bartlett. 2007. On the Consistency of Multiclass
Classification Methods. Journal of Machine Learning Research 8, 5 (2007).
[41] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. 2015. Simultaneous
deep transfer across domains and tasks. In Proceedings of the IEEE international
3744KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hao Yang, Min Wang, Jinshen Jiang, and Yun Zhou
conference on computer vision. 4068‚Äì4076.
[42] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2016. Instance normaliza-
tion: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022
(2016).
[43] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob
Roll, and Thomas Sch√∂n. 2019. Evaluating model calibration in classification. In
The 22nd International Conference on Artificial Intelligence and Statistics. 3459‚Äì
3467.
[44] Dequan Wang, An Ju, Evan Shelhamer, David Wagner, and Trevor Darrell. 2021.
Fighting gradients with gradients: Dynamic defenses against adversarial attacks.
arXiv preprint arXiv:2105.08714 (2021).
[45] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor
Darrell. 2020. Tent: Fully test-time adaptation by entropy minimization. arXiv
preprint arXiv:2006.10726 (2020).
[46] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor
Darrell. 2021. Tent: Fully Test-Time Adaptation by Entropy Minimization. In
International Conference on Learning Representations. 1‚Äì15.
[47] Min Wang, Hao Yang, and Qing Cheng. 2022. GCL: Graph Calibration Loss for
Trustworthy Graph Neural Network. In Proceedings of the 30th ACM International
Conference on Multimedia. 988‚Äì996.
[48] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. 2022. Continual test-time
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 7201‚Äì7211.
[49] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. 2022.
Mitigating neural network overconfidence with logit normalization. In Interna-
tional Conference on Machine Learning. 23631‚Äì23644.[50] Tao Yang, Shenglong Zhou, Yuwang Wang, Yan Lu, and Nanning Zheng. 2022.
Test-time batch normalization. arXiv preprint arXiv:2205.10210 (2022).
[51] Fuming You, Jingjing Li, and Zhou Zhao. 2021. Test-time batch statistics calibra-
tion for covariate shift. arXiv preprint arXiv:2110.04065 (2021).
[52] Bianca Zadrozny and Charles Elkan. 2001. Obtaining calibrated probability
estimates from decision trees and naive bayesian classifiers. In International
Conference on Machine Learning. 609‚Äì616.
[53] Hang Zhang, Weike Liu, Hao Yang, Yun Zhou, Cheng Zhu, and Weiming Zhang.
2023. CSAL: Cost sensitive active learning for multi-source drifting stream.
Knowledge-Based Systems 277 (2023), 110771.
[54] Marvin Zhang, Sergey Levine, and Chelsea Finn. 2021. Memo: Test time robust-
ness via adaptation and augmentation. arXiv preprint arXiv:2110.09506 (2021).
[55] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang. 2022. Exact feature
distribution matching for arbitrary style transfer and domain generalization. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
8035‚Äì8045.
[56] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. 2022.
Domain generalization: A survey. IEEE Transactions on Pattern Analysis and
Machine Intelligence (2022).
[57] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. 2021. Domain general-
ization with mixstyle. In International Conference on Learning Representations.
1‚Äì15.
[58] Zhi Zhou, Lan-Zhe Guo, Lin-Han Jia, Dingchu Zhang, and Yu-Feng Li. 2023. ODS:
test-time adaptation in the presence of open-world data shift. In International
Conference on Machine Learning. 42574‚Äì42588.
3745Towards Test Time Adaptation via Calibrated Entropy Minimization KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
A PARAMETER SENSITIVITY ANALYSIS
In this section, we present an exploratory analysis centered on the sensitivity of the hyperparameter ùõæ, which is a critical component that
influences the performance of the proposed method. This examination directly assessed the robustness and adaptability of our model, which
are crucial when applying it to diverse datasets under varying conditions.
A.1 Experimental Design
We conducted our analysis systematically to evaluate how the model‚Äôs performance responds to various ùõævalues. We specifically focused
on an array that spans [1, 3, 5, 7, 9], intended to encapsulate a sufficiently broad range to derive meaningful insights into performance
fluctuations and stability.
We opted to conduct our experiments across various datasets to avoid dataset-specific anomalies and ensure the broader relevance of our
results. In a similar effort To avoid bias, we employed diverse model architectures, circumventing limitations or unique characteristics that
might influence the outcomes of a particular model.
A.2 Results and Discussion
The results of a rigorous sensitivity test are shown in Figure 3. A key observation is the consistent improvement of the proposed method
catalyzed in Test-Time Adaptation (TTA). This consistency across all the tested ùõævalues underscores the method‚Äôs robustness.
The results suggest that while ùõæis a determinant factor, the model‚Äôs performance does not hinge on its precise value. The performance
curve exhibits a plateau rather than a peak, indicating that within the range of values tested, the exact choice of ùõæis not a critical pressure point.
This characteristic, indicative of the model‚Äôs ability to maintain efficacy without fine-tuning adjustments, presents a practical advantage.
The resilience of the model performance, even with variations in the ùõæparameter, lends confidence to deploying our method in diverse
scenarios. These results demonstrate the reliability and user-friendly nature of this method, thereby reducing the need for intricate calibrations
in diverse scenarios, which is a significant step toward practical applications.
Figure 3: Parameter sensitivity analysis of ùõæ.
3746