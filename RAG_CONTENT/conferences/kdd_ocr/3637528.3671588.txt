Inductive Modeling for Realtime Cold Start Recommendations
Chandler Zuo
chandlerzuo@meta.com
Meta
Menlo Park, CA, USAJonathan Castaldo
jcastaldo08@meta.com
Meta
Menlo Park, CA, USAHanqing Zhu
hanqingzhu@meta.com
Meta
Menlo Park, CA, USA
Haoyu Zhang
haoyuzhang@meta.com
Meta
Menlo Park, CA, USAJi Liu
ji.liu.uwisc@gmail.com
Meta
Menlo Park, CA, USAYangpeng Ou
yangpeng@meta.com
Meta
Menlo Park, CA, USA
Xiao Kong
xiaokong@meta.com
Meta
Menlo Park, CA, USA
ABSTRACT
In recommendation systems, the timely delivery of new content
to their relevant audiences is critical for generating a growing and
high quality collection of content for all users. The nature of this
problem requires retrieval models to be able to make inferences
in real time and with high relevance. There are two specific chal-
lenges for cold start contents. First, the information loss problem
in a standard Two Tower model, due to the limited feature inter-
actions between the user and item towers, is exacerbated for cold
start items due to training data sparsity. Second, the huge volume
of user-generated content in industry applications today poses a
big bottleneck in the end-to-end latency of recommending new
content. To overcome the two challenges, we propose a novel ar-
chitecture, the Item History Model (IHM). IHM directly injects
user-interaction information into the item tower to overcome infor-
mation loss. In addition, IHM incorporates an inductive structure
using attention-based pooling to eliminate the need for recurring
training, a key bottleneck for the real-timeness. On both public
and industry datasets, we demonstrate that IHM can not only out-
perform baselines in recommending cold start contents, but also
achieves SoTA real-timeness in industry applications.
CCS CONCEPTS
•Mathematics of computing →Information theory ;•Theory of
computation→Online learning algorithms; Nearest neigh-
bor algorithms; Inductive inference; Online learning theory ;•
Computing methodologies →Neural networks; Online learn-
ing settings; Learning latent representations.
KEYWORDS
information retrieval, recommendations, cold start, real-time learn-
ing, inductive model
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671588ACM Reference Format:
Chandler Zuo, Jonathan Castaldo, Hanqing Zhu, Haoyu Zhang, Ji Liu, Yang-
peng Ou, and Xiao Kong. 2024. Inductive Modeling for Realtime Cold
Start Recommendations. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671588
1 INTRODUCTION
On recommendation platforms based on user-generated content,
continuously providing a large discoverable corpus of fresh high-
quality content is the foundation for user experiences ([ 31]). To
effectively discover such contents, the system needs to have an
efficient cold start mechanism to help new contents surface and go
viral. There are two competing objectives for cold start systems: real-
timeness and relevance. In this paper, real-timeness refers to the
time it takes for a newly created content to reach a certain number of
impressions, and relevance refers to the proportion of positive user
interactions among impressions. While real-timeness is needed to
identify fresh and high-quality content in time, relevance is needed
to ensure that user experiences are not compromised.
The achievement of both objectives is an inherent dilemma. Be-
cause the recommendation system has only limited knowledge of
cold start contents, speed delivery with such a knowledge gap ham-
pers the system’s ability to deliver each content to its most relevant
users. To address the dilemma between real-timeness and relevance,
most existing cold start retrieval models rely on the transductive
modeling paradigm that focuses on modeling recent user engage-
ments with recurring training ([ 31], [21]). These cold start retrieval
models typically follow the Two Tower model structure, a widely
adopted retrieval setting in the industry ([ 2], [10], [15], [39], [42]).
For relevance, such models are trained on user engagements on
new contents to memoize user preferences for new contents. For
real-timeness, such models are deployed with frequent recurring
training to establish a fast feedback loop for the system to quickly
accumulate knowledge of new contents.
There are two problems with existing cold-start retrieval sys-
tems. First, they suffer from the information loss problem ([ 38]), a
6400
KDD ’24, August 25–29, 2024, Barcelona, Spain Chandler Zuo et al.
fundamental limitation of the Two Tower models. Because item1
embeddings in a Two Tower model must be precomputed for the
retrieval index, the forward computation of the item tower must
be agnostic to the user tower, which is ineffective in modeling
user-item interactions. Existing methods to overcome this prob-
lem involve adding cross-net structures between the user and item
towers ([ 38], [39]) so that the item tower can better memoize user
interactions during model training. However, recurring training
such models for an industry-scale cold start corpus requires non-
trivial training infra ([ 17]). Second, there is a fundamental limit in
the real-timeness of such systems that depend on recurring training.
To explain this, we introduce Action-To-Serving (ATS) latency as
a measure of real-timeness, defined as the time gap between an
user interaction happening and that interaction being incorporated
into the updated model for serving. In the transductive modeling
paradigm, ATS consists of user data logging, ETL jobs to prepare
training data, recurring training, and publishing the updated model.
Today, in a real-world recommender system, users upload tens
of millions of new content every day ([ 13], [7],[27]). With such a
scale, the best cold start system in the industry can only achieve
an ATS latency of a couple of hours ([ 31]), which is suboptimal for
delivering fresh content.
To solve the two problems mentioned above, we turn to novel
inductive architectures ([ 8], [9], [24]) that can efficiently model user
interactions on cold start items. Our key insight is that an inductive
neural network architecture can explicitly mode the updates on
item embeddings as a function of user interactions. As a result, for
the information loss problem, the embedding of items can directly
incorporate information from user interactions through a forward
pass. In practice, we can serve a model with static parameters
and just recurrently infer item embeddings based on new user
engagement data to constantly inject new knowledge for cold-
start items. For the real-timeness problem, the inductive structure
completely eliminates the need for recurring training. Compared to
existing solutions in the transductive model paradigm, our proposed
solution can not only significantly reduce the infra cost in practice,
but also break the ATS low bound observed in existing cold start
systems.
In this paper, we propose a dynamic Item History Modeling (IHM)
architecture for the item tower that can both generate initial content
embeddings using initial knowledge and dynamically update those
embeddings based on the ID sequence of interacted users. IHM
learns a static embedding space for these interacted user IDs. As
new content accumulates user engagements, IHM’s item tower
automatically consumes the new knowledge from user interactions
to update item representations. On public datasets, we demonstrate
that IHM can learn highly expressive item embeddings that achieve
similar recall as SoTA transductive models when predicting near-
future user engagements. Furthermore, when there is no recurring
training, while transductive models performance decays as they
predict engagements further into the future, the performance of
IHM is stable, demonstrating its unique induction abilities.
1The terms "item" and "content" are used interchangeably in this paper to refer to a
piece of user generated content, such as a post or a video, in recommendation systems.To build an end-to-end real-time solution at the industry scale,
we further design a serving system that streamlines user engage-
ment logging, model inference, and index updates. When testing
such a system end-to-end in a real-world cold start application
that has more than a hundred million cold start items, our solution
achieves an ATS latency of 15 minutes. Compared to transductive
model-based solutions, the IHM-based solution reduces serving ca-
pacity by -87%, while it achieves a multifold increase in impressions
on fresh and long-tail contents.
The contributions of this paper are multifold.
•We propose a novel inductive modeling architecture, Item
History Modeling, that overcomes the information loss prob-
lem for cold start retrieval. Our proposed model explicitly
injects user interaction information into the item tower that
dynamically models user feedback.
•We provide a theoretical explanation of how IHM achieves
inductive learning for cold start items. We show that IHM’s
forward pass during inference is equivalent to an in-context-
learning process that implicitly fine-tunes item embeddings.
•We propose an efficient end-to-end model serving system at
the industry scale. We demonstrate that inductive modeling
provides a novel paradigm that significantly outperforms
transductive models and achieves SoTA ATS latency with
much lower infra costs.
2 RELATED WORK
2.1 Two Tower Model
Two Tower model, first introduced as a document retrieval tech-
nique by [ 11] , has been widely adopted in industrial recommenda-
tion systems for item retrieval ([ 2], [10], [19]). The standard Two
Tower model relies on the dot product between the embeddings
of the user and the item tower to express the similarity between
a user and item pair. This formulation enables the scalability to
retrieve contents from industry-scale cold start corpuses that in-
clude hundreds of millions of items, as item embeddings from a
Two Tower model for the entire corpus can be precomputed and
indexed for online retrieval. Despite its scalability, a key limitation
of the Two Tower model is that the dot product cannot express
complex feature interactions between the two towers. To overcome
this limitation, a natural solution is to introduce information cross-
ing between the two towers. Recent research in this area can fall
into two categories. The first category focuses on the late-stage
information crossing, increasing the number of embedding outputs
from each tower, and then using a complex scoring function as
a replacement for the dot product ([ 16], [36], [39] ). Due to the
increased complexity involved in scoring, such methods inevitably
increase the cost of the capacity at the time of serving. The sec-
ond category focuses on the early-stage information crossing. [ 38]
proposes a Dual Augmented Two Tower Model that augments the
features in the user/item tower by an extra embedding that encodes
the knowledge of positively interacted item/user during training.
Compared to the late-stage crossing, because the standard approxi-
mate nearest neighbor can still be applied, the early-stage crossing
consumes much less capacity at the time of serving.
This article falls into the second category because of our focus
on latency and capacity. Motivated by [ 38], we introduce additional
6401Inductive Modeling for Realtime Cold Start Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
features in the item tower to inject knowledge about the interacted
users. Unlike [ 38], we directly introduce the raw IDs of the inter-
acted users as a sequence feature to the item tower, so that the
item tower architecture can learn complex interaction structures
between user engagements.
2.2 Improving Long-tail Item Embeddings
To improve embeddings for long-tail items in cold start recommen-
dations, better propagation of knowledge from sparse user inter-
action data is a key research focus in this area. Existing research
on this problem can fall into two categories. The first category
focuses on debiasing the training data and features to better repre-
sent long-tail items. For example, [ 37], [20] proposes using logQ
correction to introduce frequency-based reweighing in the train-
ing process. [ 40] combines frequency-based feature gating with
training data distribution balancing. The second category focuses
on knowledge transfer from head items to long-tail items. [ 18]
proposes a meta-learning model to learn the adaptive process that
can produce representations during both the cold start and the
warm-up phases. [ 41] proposes a dual transfer learning framework
to transfer knowledge from head items to long-tail items.
The major limitation of existing research is that model architec-
tures are inherently transductive as they rely on learning embed-
dings for each item ID to encode knowledge about long-tail items.
To encode new knowledge from user interactions about long-tail
items, such models use recurring training to backpropagate gradient
updates on these item embeddings. As a result, for industry-scale
data, these methods suffer from the capacity consumption problem
as mentioned in Section 1.
2.3 Improving the Real-timeness of Feedback
Loop Modeling
To mitigate the challenge of latency in serving, existing industry
solutions focus on scaling training and serving architectures to
streamline the synchronization of parameter updates. A common
theme in the existing approach is to exploit the sparsity in parameter
updates for sparse features. For training architectures, [ 34] devel-
ops a sparsity-aware training framework that can reduce memory
overhead and accommodate dynamic changes in sparse features.
For serving architectures, [ 17] [26] incrementally update sparse
features through different subsets, each including only recently
updated sparse features.
Although such general solutions can help improving serving
latency for general recommendation models, it requires nontrivial
computation capacity to deploy them for large-scale models. In
addition, industrial applications require high reliability for such
systems, which leads to extra overhead in the deployment of sup-
plementary fault tolerance systems such as monitoring and rollback
control.
2.4 Inductive Learning
Although inductive learning is widely applied in a wide range of
machine learning applications, in recommendation systems, most
industry solutions are still based on transductive models (e.g. [ 43]).
Exploiting transductive models’ memorization ability is a highly
effective lever to improve personalization. Nevertheless, there arerecent works that leverage inductive learning structures to tackle
several drawbacks of transductive models. [ 33] introduces an in-
ductive collaborative filtering model for the user cold start problem.
Their framework first learns a stable embedding space from a small
set of power users and then extrapolates to new users based on
dynamic user engagement graphs. A similar idea is applied in [ 23]
to target the heterogeneous item representation problem, where a
sparse representation space is first learned from user-user commu-
nity structures and then extrapolated to represent all heterogeneous
entities through dynamic graph propagation.
A common insight in both work is that although the global
user-item interaction graph is highly dynamic, the feature space
from a stable subgraph surrounding a small set of key users can be
stable. Propagating from this foundational feature layer from the
global dynamic graph provides highly expressive representations
for all entities. We adopt this insight in this paper. In particular,
our proposed model applies inductive logic on top of stable user
embeddings to dynamically propagate user feedback information
for long-tail contents.
3 METHODOLOGY
Problem Setting Our goal is to predict user engagements (e.g. like,
view) items (e.g. videos, photos). Denote the user set by 𝒰and the
item set by ℐ. Our model predicts 𝑌(𝑢,𝑖), a binary variable that
denotes whether the user 𝑢∈𝒰engages in the item 𝑖∈ℐ.
Specific to the cold start retrieval setting, the cardinality of the
item set,|ℐ|, can be very large, potentially in the range between
millions and billions in real-world applications. The Two Tower
Model ([ 2]) is commonly used to predict 𝑌(𝑢,𝑖). In a Two Tower
model, we use two neural network models, referred to as a user
tower and an item tower, to project user-side and item-side features,
respectively, into two embeddings in 𝑒𝑢, 𝑒𝑖∈R𝑑. The probability
of the binary event 𝑌(𝑢,𝑖)is modeled as:
Pr(𝑌(𝑢,𝑖)=1)=Exp(𝑒⊤𝑢𝑒𝑖)Í
𝑖′∈ℐExp(𝑒⊤𝑢𝑒𝑖′).
To generate recommendations for each user, approximate nearest
neighbor is used to select the top items with the highest dot prod-
uct values with the user embedding. Hit rate and NDCG are two
common metrics for measuring performance in predicting future
user-item interactions from a general Two Tower model. For the
content cold start problem, we follow [ 40] and [ 41] to evaluate the
recall on two subsets of items separately: the long tail item subset
consists of items with fewer than X user interactions in the training
data, and the head item subset consists of the rest of the items.
Compared to a standard Two Tower model, our goal is to improve
the performance on long-tail items without degrading performance
on head items compared to existing Two Tower models.
3.1 Overview
Our proposed methodology is summarized in Fig. 1. This archi-
tecture focuses on developing a novel inductive architecture for
the item tower to enable real-time inference for long-tail items, as
shown in the left half of Fig. 1. The proposed architecture is orthog-
onal to user tower architectures and can be applied together with
different user tower modeling techniques that focus on predicting
6402KDD ’24, August 25–29, 2024, Barcelona, Spain Chandler Zuo et al.
Figure 1: The Two Tower model architecture of IHM. (a) IHM
is an Item Tower architecture that can be used with flexible
User Tower architectures in standard Two Tower models. (b)
Item embeddings generated from IHM are used for approxi-
mate nearest neighbor in retrieval systems. (c) The detailed
neural network architecture in IHM as described in Section
3.
user interests. At serving time, the inductive item tower is used
to generate embeddings for both new and engaged items, where
new item embeddings are further used to build an approximate
nearest-neighbor index. When recommending candidate items for
a given user, we use old item embeddings from a user’s history as
query vectors to fetch nearest neighbors.
Our inductive architecture results in an item embedding space
that is invariant both of corpus changes and user-item engagements
data. In real-world applications, even though the content corpus
rapidly changes, as new items are added to the corpus, they can be
constantly mapped to the same embedding space in a streamline
fashion, and can, therefore, be retrieved.
3.2 Item History Modeling
The IHM (i.e. Item History Modeling) feature for item 𝑖is the set of
users who have engaged in this item before, denoted by 𝒰𝑖, whose
cardinality is|𝒰𝑖|. Denote the embedding of the static feature of
this item by 𝐸𝑆(𝑥𝑖)∈R𝑑.𝑥𝑖includes both features that memorize
the training data (e.g. item ID) and features that can generalize tounseen items (e.g. author ID, topic categories). To fuse the informa-
tion from these features, 𝐸𝑆concatenates the embeddings of these
features and projects them into R𝑑.
Starting from 𝐸𝑆(𝑥𝑖), we propose to further enhance the item
representation when there are new interactions on this item. A
natural thought is to project user IDs, 𝒰𝑖, into an embedding space
and aggregate these embeddings. In practice, as the new item re-
ceives hundreds or thousands of interactions, we observe that the
users included in 𝒰𝑖generally have quite heterogeneous interests.
Therefore, it is difficult to use a single embedding to represent all
users in 𝒰𝑖. We thus adopt the Mixture of Expert (MoE, [ 25]) to
learn multiple embeddings 𝐸IHM
𝑖,ℎ, ℎ=1,2,···,𝐻, each intended to
represent a particular type of user this item is relevant to:
𝐸IHM
𝑖,ℎ=Aggℎ(𝒰𝑖,𝐸𝑆(𝑥𝑖));ℎ=1,2,···,𝐻.
IHM further aggregates the expert embeddings through gated
fusion to form the final item embedding. The gating function here
learns how important each expert is as related to the item.
𝑒IHM
𝑖=𝐻∑︁
ℎ=1𝐺ℎ
𝑖𝐸IHM
𝑖,ℎ. (1)
3.3 Mixture-of-Expert Embeddings
In this paper, we propose the following Multi-Head Attention
pooling to learn IHM MoE. For item 𝑖, denote the user embed-
dings corresponding to the IHM feature by 𝑒𝑢∈R𝑑,𝑢∈𝒰𝑖. Let
𝐸𝑈
𝑖=[𝑒𝑢1,···,𝑒𝑢|𝒰𝑖|]∈R𝑑×|𝒰𝑖|. Our Multi-Head Attention archi-
tecture considers 𝐸𝑆(𝑥𝑖)as the query vector to attend to both the
user embeddings and 𝐸𝑆(𝑥𝑖)itself. Attention weights for the ℎ-th
head are computed as
𝐴𝑔𝑔ℎ(𝒰𝑖)=[𝐸𝑈
𝑖;𝐸𝑆(𝑥𝑖)]·𝜎(
{𝑊𝐾
ℎ[𝐸𝑈
𝑖;𝐸𝑆(𝑥𝑖)]}⊤[𝑊𝑆
ℎ𝐸𝑆(𝑥𝑖)]
√
𝑑)
.
(2)
where the projection parameters 𝑊𝑆
ℎ, 𝑊𝐾
ℎ∈R𝑑×𝑑, and𝜎is a
nonlinear activation function that outputs normalized weights in
R|𝒰𝑖|, for example SoftMax.
Compared to the standard attention mechanism ([ 28], [29]), eqn.
2 eliminates the projection on the value vectors. The reason is that
the value vectors in our model, user embeddings, are independent
parameters, while in the standard attention model they are shared
parameters with pre-defined semantic meanings. To ensure the
independent user embedding parameters are identifiable, the linear
structure in eqn. (1) and (2) together constrain the user embeddings
to have the same semantic meaning as the static item embeddings
𝐸𝑆. As a result, IHM focuses on mapping users to the existing
semantic space defined by 𝐸𝑆and learning the attention structure.
3.4 Gated Fusion
The final item embedding in eqn. 1 is a gated fusion between static
feature embeddings and IHM embeddings.
𝑔ℎ
𝑖=𝑊⊤
gate[𝐸IHM
𝑖,ℎ;𝐸𝑆(𝑥𝑖)], ℎ=1,2,···,𝐻;
𝐺·
𝑖=SoftMax(𝑔·
𝑖). (3)
6403Inductive Modeling for Realtime Cold Start Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Here, the gating function takes the concatenation of the IHM
embedding and the static feature embedding as input. In principle,
since IHM embedding already blends the static feature in eqn. 2,
another option is to use only the IHM embedding as the input of
the gating function. The drawback of this option is that it allows
the attention weights of the static feature embedding in eqn. 2 to
directly dictate the importance of static features in gating, which
unnecessarily restricts the flexibility of the gating function. Our
proposal, eqn. 3, gives more flexibility to learn the relationship
between gating weights and static features.
3.5 Induction through In-Context Learning
In this section, we explain the inductive mechanism of IHM through
the in-context learning (ICL) framework. Specifically, we perform a
theoretical analysis to show that IHM inference is equivalent to an
ICL process that implicitly fine-tunes long-tail item embeddings.
The ICL process consists of two steps. First, in the Zero-Shot Learn-
ing (ZCL) step, item embeddings are initialized as a transformation
of static content features. Second, in the ICL step, each user en-
gagement incurs a meta-gradient update on the transformation
parameters. As a result, we can see IHM’s inductive learning is
equivalent to ZCL on a transductive model. The remaining of this
section mathematically proves the equivalence between IHM and
the ZCL process.
Following [ 4] and [ 30], we start with the approximation of eqn.
(2) by removing the activation function and the scaling factor:
Aggℎ(𝒰𝑖,𝐸𝑆(𝑥𝑖))≈[𝐸𝑈
𝑖;𝐸𝑆(𝑥𝑖)]{𝑊𝐾
ℎ[𝐸𝑈
𝑖;𝐸𝑆(𝑥𝑖)]}⊤[𝑊𝑆
ℎ𝐸𝑆(𝑥𝑖)]
=𝐸𝑆(𝑥𝑖)𝐸𝑆(𝑥𝑖)⊤(𝑊𝐾
ℎ)⊤𝑞ℎ+𝐸𝑈
𝑖(𝐸𝑈
𝑖)⊤(𝑊𝐾
ℎ)⊤𝑞ℎ
:=gAggℎ(𝒰𝑖,𝐸𝑆(𝑥𝑖)),
where𝑞ℎ:=𝑊𝑆
ℎ𝐸𝑆(𝑥𝑖)is the attention query vector. We define
𝑊ZCL=𝐸𝑆(𝑥𝑖)𝐸𝑆(𝑥𝑖)⊤(𝑊𝐾
ℎ)⊤as the zero-shot parameter to be
updated, since 𝑊ZCL𝑞is the attention result in the absence of item
history as demonstrations. Then, the update on top of 𝑊ZCLcan be
expressed as a sequence of gradient updates, each corresponding
to a new demonstration of user-item interaction:
gAggℎ(𝒰𝑖,𝐸𝑆(𝑥𝑖))=𝑊ZCL𝑞ℎ+𝐸𝑈
𝑖(𝐸𝑈
𝑖)⊤(𝑊𝐾
ℎ)⊤𝑞ℎ
=𝑊ZCL𝑞ℎ+|𝒰𝑖|∑︁
𝑗=1𝑒𝑢𝑗𝑒⊤
𝑢𝑗(𝑊𝐾
ℎ)⊤𝑞ℎ=(𝑊ZCL+Δ𝑊ICL)𝑞ℎ.
As shown above, the attention-based aggregation step in eqn. 2
that learns the IHM embedding for each expert is equivalent to the
parameter updates Δ𝑊ICLon𝑊ZCL. The IHM model learns to apply
meta-updates to item embeddings based on new user engagements.
Because such meta-updates are applied simply through the forward
pass at the inference time, item embeddings from IHM automatically
adapt to new user-item engagement patterns, making the model
performance decay very slowly even without retraining.
4 REAL-TIME SERVING
The inductive structure of IHM enables us to design an end-to-
end real-time inference and serving system, as illustrated in Fig.
2. Our system includes three asynchronous procedures for feature
Figure 2: The real-time inference and serving system of IHM.
logging, model inference, and index update, respectively. Such a
design decouples three streamline processes from each other to
reduce blockers for end-to-end real-timeness.
Real-time Feature Logging. User actions trigger the streaming
feature logging service that outputs IHM features as the key-value
format of {item ID: user ID sequence}. This feature will be used
together with additional item-side features for inference.
Real-time Embedding Inference. Real-time inference for each item
is triggered by predefined events, such as when the item receives
a like or gets every X impressions. In practice, the frequency of
such events can be very high for very popular items. To avoid the
system being overwhelmed by such items, we also implement a
rate-limiting control that enforces a minimum time between two
consecutive triggers on the same item. The output of embedding
inference is written to the KV store. This will be used by both the
indexing service and the ranking requests.
On-the-fly Indexing Updates. Finally, a scheduled index job per-
forms incremental on-the-fly updates on the kNN index. This com-
ponent is built using Unicorn [ 3], a Hadoop-backed low latency
indexing system developed by Meta. On-the-fly updates ensure that
the kNN index continuously serves online while its parameters are
being updated. Our indexing update exploits the sparsity pattern in
item embedding updates. At each minute-level interval, our service
only updates the index mutation corresponding to a small set of
items. The index quantization schema is updated globally daily to
accommodate embedding drifts over the time [6].
We achieve minute-level latency in each of the three processes,
leading to an overall 15-minute action-to-serving (ATS) latency,
a.k.a. between user actions and retrieval index updates. To our
knowledge, compared to other industry-scale solutions (e.g. [ 31])
that can only achieve hour-level ATS latency, our solution achieves
the best-in-class ATS latency for embedding-based retrieval models
at the industry scale.
6404KDD ’24, August 25–29, 2024, Barcelona, Spain Chandler Zuo et al.
Our current solution assumes a static model checkpoint pub-
lished from a one-time model training. In practice, we observe
model performance decays at month-level, so we manually retrain
models at such a cadence. However, it is possible to extend the
design in Fig. 2 by adding an asynchronous recurring training pro-
cess to periodically update the model checkpoints and push global
index updates. Such a generalized design will still maintain the
minute-level ATS latency due to the decoupling between model
training and real-time inference.
5 EXPERIMENTS
In this section, we analyze the proposed method in public and
industrial datasets. On public datasets, we focus on evaluating only
the model architecture of IHM (Section 3), and validate architectural
properties that are fundamental to IHM’s real-time capabilities. On
industry datasets, we directly measure performance in ATS latency
and capacity costs with end-to-end implementation of both the
model architecture (Section 3) and the serving system (Section 4).
In public datasets, we focus on validating two hypotheses:
•H1Addressing the information loss on long-tail items. By
injecting user-interaction information into the item tower,
IHM can overcome the information loss problem for long-tail
items.
•H2No performance decay. Due to its inductive structure,
IHM can predict future user interactions with little or no
performance decay without model retraining.
If we validate the two hypotheses, we can conclude that an IHM
model can both better recommend cold start items and does not
need recurring training. This is the foundation for IHM to achieve
SoTA ATS latency in industry settings.
Our proposed technique is agnostic to the exact retrieval model
setup and can be applied to any Two Tower models. Therefore,
for public and industrial datasets, we implement the method on
top of the best-performing retrieval models in each scenario as the
baseline.
5.1 Public Datasets
We evaluated in MoveLens-1M and Amazon Reviews (subsets Beauty,
Games and Books) similar to previous work in [ 14] [39]. Data pre-
processing follows existing practice as in [ 14][5] except that we
adopt time-based splitting instead of leave-one-out split to construc-
tion evaluation datasets, with the details described in 5.1.1. For our
purposes, time-based splitting strictly avoids information leakage
from future data to be used in model training, which mimics produc-
tion recommendation system set-ups and is critical for evaluating
long-tail recommendation performances.
5.1.1 Evaluation Methods. First, we apply time-based data split-
ting. User item interactions whose timestamp is before the median
timestamp of the entire data set are used for model training and
validation. The remaining user-item interactions are used for model
evaluation. The evaluation data set is then grouped into five equal-
sized time windows. Finally, the data within each time window
are divided into two subsets for long-tail items and head items,
respectively.To this end, to answer the two research questions, we report hit
rates and NDCG in the following respective evaluation set.
•For H1, data from the first time window are used for evalua-
tion. Because baseline transductive models expect to decay
minimally in this time window, we can control the model
decay factor when comparing model performances on tail &
head items.
•For H2, metrics from each time window are calculated so
that we can track how each model decays over time.
5.1.2 Baselines and Model Variants. We implemented different
methods on top of SASRec ([ 12]) as the backbone model, which
represents one of the best performing techniques to predict user-
item interactions. When following the standard data processing
and using the leave-one-out split, our implementation achieves a hit
rate of 0.616 on the MovieLens-1M dataset, which is within 10% of
the best results in [5]. On top of the SASRec backbone, we choose
the following three baseline models.
•Sequential method. We choose SASRec [ 14] itself as the base-
line sequential model. It incorporates a sequential model
that predicts the next item for each user and outperforms
conventional nonsquential methods ([39], [5]).
•Rebalancing and decoupling. We choose Cross Decoupling
Network (CDN) ([41]), which incorporates training data re-
balancing and decouples memoization and generalization
networks on the item side to address biases in long-tail items.
CDN represents the best performing models among other
rebalancing and decoupling techniques. We implemented
CDN on top of SASRec as our baseline.
•Feature interaction. We choose DAT [ 38], which uses early-
stage feature crossing to learn user-item interactions. It
learns additional embedding features in each tower that
memoize the outputs from the other tower during training.
In our experiments, we add the dual network structure on top
of the SASRec model and apply the same training schedules
as in [38].
Correspondingly, we implement three variants of IHM on top of
these techniques.
•IHM-SASRec. Because IHM as an item-side representation
model is orthogonal to the sequential user-side model in
SASRec, we directly implement IHM on top of SASRec in
this variant.
•IHM-CDN. We replace CDN’s item-side decoupling network
by IHM, while keeping CDN’s user-side bilateral branch
networks. Compared to IHM-SASRec, the user-side bilateral
branch structures in IHM-CDN are expected to better address
training data bias in long-tail items.
•IHM-DAT. Similarly to IHM-SASRec, we directly implement
IHM on top of DAT in this variant.
For H1, we first calculate HitRate@10 and NDCG@10 in the
tail, head, and all items, respectively. As shown in Tables 1 and 2,
IHM provides significant performance improvements to tail items
without degrading the performance of head items. Furthermore, the
best-performing model variant also improves performance on head
items. The improvement on the head items demonstrates IHM’s
ability in overcoming the information loss problem in Two Tower
6405Inductive Modeling for Realtime Cold Start Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The recommendation performance over MovieLens-1M.
MethodOverall Tail Head
HitRate@10 NDCG@10 HitRate@10 NDCG@10 HitRate@10 NDCG@10
SASRec 0.1691 0.0794 0.1063 0.0445 0.1871 0.0894
CDN 0.1534 0.0718 0.1142 0.0501 0.1646 0.0780
DAT 0.1752 0.0803 0.1039 0.0443 0.1956 0.0907
IHM-SASRec 0.1656 0.0806 0.1299 0.0700 0.1759 0.0836
IHM-CDN 0.1928 0.0806 0.1457 0.0623 0.2063 0.0859
IHM-DAT 0.1690 0.0723 0.1305 0.0689 0.1801 0.0760
Table 2: The recommendation performance over Amazon Datasets.
MethodOverall Tail Head
HitRate@10 NDCG@10 HitRate@10 NDCG@10 HitRate@10 NDCG@10
GamesSASRec 0.1429 0.0661 0.1030 0.0483 0.1600 0.0737
CDN 0.1401 0.0658 0.1093 0.0518 0.1539 0.0720
DAT 0.1458 0.0680 0.1010 0.0479 0.1656 0.0764
IHM-SASRec 0.1495 0.0736 0.1397 0.0710 0.1536 0.0747
IHM-CDN 0.1549 0.0721 0.1310 0.0623 0.1653 0.0763
IHM-DAT 0.1562 0.0741 0.1358 0.0683 0.1648 0.0792
BeautySASRec 0.1195 0.0588 0.1035 0.0464 0.1386 0.0736
CDN 0.1096 0.0550 0.0963 0.0459 0.1251 0.0658
DAT 0.1196 0.0591 0.1031 0.0482 0.1390 0.0734
IHM-SASRec 0.1337 0.0618 0.1322 0.0619 0.1357 0.0611
IHM-CDN 0.1477 0.0666 0.1566 0.0708 0.1369 0.0615
IHM-DAT 0.1359 0.0624 0.1358 0.0629 0.1361 0.0612
BooksSASRec 0.1161 0.0567 0.1016 0.0474 0.1360 0.0696
CDN 0.1179 0.0556 0.1078 0.0499 0.1316 0.0632
DAT 0.1173 0.0561 0.1021 0.0474 0.1380 0.0692
IHM-SASRec 0.1405 0.0688 0.1252 0.0596 0.1616 0.0815
IHM-CDN 0.1416 0.0677 0.1321 0.0626 0.1546 0.0748
IHM-DAT 0.1473 0.0718 0.1358 0.0646 0.1632 0.0796
Figure 3: Model decay across different time windows over the
MovieLens-1M dataset.
models. Compared to baseline models, IHM improves the expres-
siveness of item embeddings by injecting additional knowledge
from interacted users.For H2, we also check the change in HitRate@10 in different
time windows in the evaluation set. As shown in Fig. 3 and addi-
tional Figs. 6-7 in Appendix A, the performance over both tail and
head items is stable for IHM models, while both baseline models
show decay trends on tail and head items. The decay trends on tail
items are less pronounced because the overall performance on tail
items is low. In conclusion, the results show that IHM, by utilizing
inductive structures, has a significant advantage in maintaining
high prediction accuracy for future time horizons without model
retraining.
We specifically compare IHM with DAT because both methods
introduce early-stage feature crossing to overcome the information
loss problem. A natural question is whether they can introduce
complementary improvements to the model. Our experimental
results show that DAT and IHM do not have incremental effects
over each other in improving the performance for head items, as
IHM-DAT does not necessarily outperform DAT. On the other hand,
as a transductive models, DAT can only memoize interactions in
the training data, thus it underperforms IHM on long-tail items.
6406KDD ’24, August 25–29, 2024, Barcelona, Spain Chandler Zuo et al.
5.2 Industry Datasets
5.2.1 Setup. We conducted an online experiment to study the
effectiveness of our proposed methodology in a Facebook recom-
mendation system that serves billions of users. To surface fresh
and long-tail contents, we build a retrieval model on the cold start
content corpus, defined by contents with ≤𝑋days of age and≤𝑌
impressions, with the goal of achieving both low latency and high
engagements in these contents. We measure the performance of
adding the IHM model as a new retrieval model on the cold start
corpus through A/B experiments.
Figure 4: Relative improvement in impressions in the online
experiment. (a) By media age buckets. Each bucket represents
media created X hours ago. (b) By impressions buckets. Each
bucket represent media with prior impressions in that range.
The error bar represents 95%-confidence interval.
# Impressions LTR
Media age 0-24h +176% +1.12%
Impressions < 10K +333% -13.1%
Table 3: Comparison between IHM and a transductive model
with a similar architecture. The numbers here are relative
improvements in IHM over the transductive model. All num-
bers are statsitcailly significant at the 95% confidence level.
IHM v.s. No Real-time Serving
# Impressions LTR
Media age 0-24h +1144% +13.1%
Impressions < 10K +31.4% +55.0%
IHM v.s. No Inductive Structure
# Impressions LTR
Media age 0-24h +10.3% +1.2%
Impressions < 10K +10.8% +2.2%
Table 4: Results from ablation studies. The numbers here are
relative improvements over each ablated setting. All numbers
are statsitcailly significant at the 95% confidence level.
5.2.2 Results.•Performance on Fresh and Tail Contents. Fig. 4 shows that IHM
significantly improves the impressions on contents less than
5 hours old or fewer than 1K impressions. Fig. 4(a) shows
that IHM has the highest delta value in content that is a few
hours old. Fig. 4(b) shows that IHM also marginally improves
for "warm" contents. Because IHM is the only model that
achieves both relevance and realtimeness simultaneously
in our test setting, there’s a huge increase in impressions
on new contents. We also observe that IHM improves the
like-through rate (LTR) by +12. 5% on contents less than 24
hours old.
•Ablation Studies. We ablate real-time serving and the induc-
tive architecture of IHM to measure the gains from each of
these components. To ablate real-time-serving, we use a daily
offline job for model inference and index updates. To ablate
the inductive architecture, we only use static embeddings,
𝐸𝑆(𝑥𝑖)in Fig. 1, for cold start items. Table 4 summarizes the
results. For each ablated setting, IHM significantly improves
both impressions and LTR on new and tailed items.
•Comparison with a Transductive Model. We further compare
IHM with a transductive retrieval model with a Two Tower
structure similar to IHM except that it uses a sparse network
in the item tower. To compensate for a lack of inductive abil-
ity, the transductive model uses online model training, with
end-to-end latency in O(hour). Table 3 compares the per-
formance of IHM with this transductive model. In addition
to the large increase in impressions, IHM noticeably only
consumes 13% of the serving capacity of the transductive
model. There is a -13.1% LTR regression for items with <10K
impressions due to the trade-off with impression increases,
and is considered as acceptable in our application scenario.
Capacity savings come from the elimination of recurring
model training & publishing. The result suggests that the
inductive model is a cost-effective solution for real-world
recommendation systems.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we propose a novel Item History Model to address the
item cold start problem in industry-scale recommendation systems.
IHM incorporates a fully inductive structure that can learn item
embeddings from real-time user interactions. IHM eliminates the
dependency on recurring training to achieve realtime-ness and
utilizes the attention architecture to overcome the information
loss problem. Experiments on both public datasets and industry
applications demonstrate that IHM can significantly outperform
baselines in recommending fresh and long-tail contents.
Our work lays an inductive model foundation for the content
cold start problem. We highlight two future research directions that
can improve the current IHM solution. First, improve the attention-
based pooling for ultralong item history sequences. The item history
sequence can grow ultralong for high-quality contents that quickly
gain user engagement, leading to computational challenges. We
can explore introducing hierarchical (e.g. [ 1] ) or sparsity (e.g. [ 22])
structures in IHM’s attention structure to improve content person-
alization with the growing user engagement. Second, introduce
6407Inductive Modeling for Realtime Cold Start Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
engagement-aware modeling for item histories. Different user en-
gagement types, e.g. like, view, click, on recommendation products
express different user intents. Engagement-aware techniques, such
as [35] and [ 32], can be explored to improve IHM’s ability to better
model user feedback.
7 ACKNOWLEDGEMENT
We’d like to thank Aditi Nair, Ran Yang, Song Zhao, Matthew Mur-
phy, Zhan Chen, Qunshu Zhang for infra support, Meng Wu, Jianyu
Wang, Ryan LaFleur, Guowei Sun, Katerina Ilioupoulou, Songbin
Liu, Eric Tan, James Yang for various discussions, Peng Wu, Wenna
Dai, Shilin Ding, Mengchee Zhang, Bi Xue, Ivan Ji, Shujian Bu,
Zhengyu Su for overall support on this project.
REFERENCES
[1]Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun
Hui, Dewei Leng, Yanan Niu, Yang Song, et al .2023. TWIN: TWo-stage Interest
Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou.
arXiv preprint arXiv:2302.02352 (2023).
[2]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191–198.
[3]Michael Curtiss, Iain Becker, Tudor Bosman, Sergey Doroshenko, Lucian Grijincu,
Tom Jackson, Sandhya Kunnatur, Soren Lassen, Philip Pronin, Sriram Sankar,
et al.2013. Unicorn: A system for searching the social graph. Proceedings of the
VLDB Endowment 6, 11 (2013), 1150–1161.
[4]Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu
Wei. 2023. Why Can GPT Learn In-Context? Language Models Implicitly Perform
Gradient Descent as Meta-Optimizers. In ICLR 2023 Workshop on Mathematical
and Empirical Understanding of Foundation Models.
[5]Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021. A case study
on sampling strategies for evaluating neural sequential item recommendation
models. In Proceedings of the 15th ACM Conference on Recommender Systems.
505–514.
[6]Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.
The faiss library. arXiv preprint arXiv:2401.08281 (2024).
[7]James Hale. 2019. More Than 500 Hours Of Content Are Now Being Uploaded
To YouTube Every Minute. https://www.tubefilter.com/2019/05/07/number-hours-
video-uploaded-to-youtube-per-minute/ (2019).
[8] James Hawthorne. 2004. Inductive logic. (2004).
[9]Evan Heit. 2000. Properties of inductive reasoning. Psychonomic Bulletin &
Review 7 (2000), 569–592.
[10] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,
Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-
based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2553–2561.
[11] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM international conference on
Information & Knowledge Management. 2333–2338.
[12] Zan Huang. [n. d.]. SASRec.pytorch. https://github.com/pmixer/SASRec.pytorch.
Accessed: 2023-12-20.
[13] Tim Ingham. 2023. Over 60,000 Tracks are Now Uploaded to Spotify Every Day.
That’s Nearly One per Second. https://www.musicbusinessworldwide.com/ over-
60000-tracks-are-now-uploaded-to-spotify-daily-thats-nearly-one-per-second/
(2023).
[14] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197–206.
[15] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,
Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest
network with dynamic routing for recommendation at Tmall. In Proceedings of
the 28th ACM international conference on information and knowledge management .
2615–2623.
[16] Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long,
Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, et al .2022. IntTower: the Next
Generation of Two-Tower Model for Pre-Ranking System. In Proceedings of the
31st ACM International Conference on Information & Knowledge Management.
3292–3301.
[17] Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin Zhu,
Yijie Zhu, Peng Wu, Ke Wang, et al .2022. Monolith: real time recommendationsystem with collisionless embedding table. arXiv preprint arXiv:2209.07663 (2022).
[18] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneous
information networks for cold-start recommendation. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
1563–1573.
[19] Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wil-
fred Ng. 2019. SDM: Sequential deep matching model for online large-scale
recommender system. In Proceedings of the 28th ACM International Conference on
Information and Knowledge Management. 2635–2643.
[20] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,
Andreas Veit, and Sanjiv Kumar. 2020. Long-tail learning via logit adjustment.
arXiv preprint arXiv:2007.07314 (2020).
[21] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm
up cold-start advertisements: Improving ctr predictions via learning to learn id
embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval. 695–704.
[22] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Effi-
cient content-based sparse attention with routing transformers. Transactions of
the Association for Computational Linguistics 9 (2021), 53–68.
[23] Venu Satuluri, Yao Wu, Xun Zheng, Yilei Qian, Brian Wichers, Qieyun Dai,
Gui Ming Tang, Jerry Jiang, and Jimmy Lin. 2020. Simclusters: Community-based
representations for heterogeneous recommendations at twitter. In Proceedings of
the 26th ACM SIGKDD international conference on knowledge discovery & data
mining. 3183–3193.
[24] Tobias Schnabel, Mengting Wan, and Longqi Yang. 2022. Situating Recommender
Systems in Practice: Towards Inductive Learning and Incremental Updates. arXiv
preprint arXiv:2211.06365 (2022).
[25] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).
[26] Chijun Sima, Yao Fu, Man-Kit Sit, Liyi Guo, Xuri Gong, Feng Lin, Junyu Wu,
Yongsheng Li, Haidong Rong, Pierre-Louis Aublin, et al .2022. Ekko: A{Large-
Scale}deep learning recommender system with {Low-Latency}model update.
In16th USENIX Symposium on Operating Systems Design and Implementation
(OSDI 22). 821–839.
[27] Tiktok. 2024. Tiktok Users and Growth Statistics.
https://www.usesignhouse.com/blog/tiktok-stats (2024).
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[29] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[30] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,
Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Trans-
formers learn in-context by gradient descent. In International Conference on
Machine Learning. PMLR, 35151–35174.
[31] Jianling Wang, Haokai Lu, Sai Zhang, Bart Locanthi, Haoting Wang, Dylan
Greaves, Benjamin Lipshitz, Sriraj Badam, Ed H Chi, Cristos J Goodrow, et al .
2023. Fresh Content Needs More Attention: Multi-funnel Fresh Content Recom-
mendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 5082–5091.
[32] Chuhan Wu, Fangzhao Wu, Tao Qi, Qi Liu, Xuan Tian, Jie Li, Wei He, Yongfeng
Huang, and Xing Xie. 2022. Feedrec: News feed recommendation with various
user feedbacks. In Proceedings of the ACM Web Conference 2022. 2088–2097.
[33] Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Junchi Yan, and Hongyuan Zha. 2021.
Towards open-world recommendation: An inductive model-based collaborative
filtering approach. In International Conference on Machine Learning. PMLR, 11329–
11339.
[34] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen
Lin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: memory-efficient
continual learning for large-scale real-time recommendations. In SC20: Inter-
national Conference for High Performance Computing, Networking, Storage and
Analysis. IEEE, 1–17.
[35] Ruobing Xie, Cheng Ling, Yalong Wang, Rui Wang, Feng Xia, and Leyu Lin. 2021.
Deep feedback network for recommendation. In Proceedings of the Twenty-Ninth
International Conference on International Joint Conferences on Artificial Intelligence .
2519–2525.
[36] Zhenhui Xu, Meng Zhao, Liqun Liu, Lei Xiao, Xiaopeng Zhang, and Bifeng Zhang.
2022. Mixture of virtual-kernel experts for multi-objective user profile modeling.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 4257–4267.
[37] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee
Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural
modeling for large corpus item recommendations. In Proceedings of the 13th ACM
Conference on Recommender Systems. 269–277.
[38] Yantao Yu, Weipeng Wang, Zhoutian Feng, and Daiyue Xue. 2021. A dual aug-
mented two-tower model for online large-scale recommendation. DLP-KDD
6408KDD ’24, August 25–29, 2024, Barcelona, Spain Chandler Zuo et al.
(2021).
[39] Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and
Xing Liu. 2023. Revisiting Neural Retrieval on Accelerators. arXiv preprint
arXiv:2306.04039 (2023).
[40] Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong,
and Ed H Chi. 2021. A model of two tales: Dual transfer learning framework for
improved long-tail item recommendation. In Proceedings of the web conference
2021. 2220–2231.
[41] Yin Zhang, Ruoxi Wang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi,
Lichan Hong, James Caverlee, and Ed H Chi. 2023. Empowering Long-tail Item
Recommendation through Cross Decoupling Network (CDN). In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
5608–5617.
[42] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021.
Contrastive learning for debiased candidate generation in large-scale recom-
mender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 3985–3995.
[43] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059–1068.
A ADDITIONAL EXPERIMENT RESULTS
Figure 5: Model decay across different time windows over the
Amazon Games dataset.
Figure 6: Model decay across different time windows over the
Amazon Beauty dataset.
Figure 7: Model decay across different time windows over the
Amazon Beauty dataset.
B HYPER PARAMETER SETTINGS
For the ML-1M, Amazon Beauty, and Amazon Games datasets, we
follow the original optimal hyper parameters for SASRec reported
in [14], specifically 2 encoder layers with 1 attention head, and a
dropout rate of 0.2 for MovieLens and 0.5 for the Amazon Reviews
datasets. For the Amazon Books dataset, we follow the optimal
hyper parameters reported in [ 39] and use 4 attention heads with
4 encoder layers. For CDN, we set the 𝑔𝑎𝑚𝑚𝑎 -adaptor parameter
𝑔𝑎𝑚𝑚𝑎 =2, which according to [ 41] has the best performance for
long tail contents. For DAT, we set the loss weight on auxilliary
losses𝜆1=𝜆2=0.5according to the experiment setup in [ 38]. For
the IHM arch, we grid searched the number of experts ℎand found
ℎ=1to have the best performance across all datasets.
6409