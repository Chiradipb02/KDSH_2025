Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute
Shinji Tajima
Nagoya Institute of Technology
Nagoya, Aichi, JapanRen Sugihara
Nagoya Institute of Technology
Nagoya, Aichi, Japan
Ryota Kitahara
Nagoya Institute of Technology
Nagoya, Aichi, JapanMasayuki Karasuyama
karasuyama@nitech.ac.jp
Nagoya Institute of Technology
Nagoya, Aichi, Japan
ABSTRACT
The graph classification problem has been widely studied; how-
ever, achieving an interpretable model with high predictive perfor-
mance remains a challenging issue. This paper proposes an inter-
pretable classification algorithm for attributed graph data, called LA-
GRA (Learning Attributed GRAphlets). LAGRA learns importance
weights for small attributed subgraphs, called attributed graphlets
(AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and
their attribute vectors that strongly contribute to discriminating
different classes. A significant characteristics of LAGRA is that all
the subgraph structures in the training dataset can be considered
as a candidate structures of AGs. This approach can explore all
the potentially important subgraphs exhaustively, but obviously, a
naïve implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy
by combining the proximal gradient descent and a graph mining
tree search. Our pruning strategy can ensure that the quality of
the solution is maintained compared to the result without pruning.
We empirically demonstrate that LAGRA has superior or compa-
rable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number
of AGs in an interpretable manner.
CCS CONCEPTS
•Computing methodologies →Supervised learning by clas-
sification; Feature selection.
KEYWORDS
Graph classification, graph mining, proximal gradient descent
ACM Reference Format:
Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama.
2024. Learning Attributed Graphlets: Predictive Graph Mining by Graphlets
with Trainable Attribute. In Proceedings of the 30th ACM SIGKDD Conference
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671970on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671970
1 INTRODUCTION
Prediction problems with a graph input, such as graph classification
problems, have been widely studied in the data science commu-
nity. A graph representation is useful to capture structural data,
and graph-based machine learning algorithms have been applied
to variety of application problems such as chemical composition
analysis [ 2,16] and crystal structure analysis [ 11,21]. In real-word
datasets, graphs often have node attributes as a continuous value
vector (note that we only focus on node attributes throughout the
paper, but the discussion is same for edge attributes). For example,
a graph created by a chemical composition can have a three dimen-
sional position of each atom as an attribute vector in addition to a
categorical label such as atomic species. In this paper, we consider
building an interepretable prediction model for a graph classification
problem in which an input graph has continuous attribute vectors.
As we will see in Section 3, this setting has not been widely studied
despite its practical importance.
Our framework can identify important small subgraphs, called
graphlets, in which each node has an attribute vector. Note that
we use the term graphlet simply to denote a small connected
subgraph [ 17], though in some papers, it only indicates induced
subgraphs [ 15]. Figure 1 shows an illustration of our prediction
model. In the figure, the output of the prediction model 𝑓(𝐺)=
𝛽0+𝛽𝐻1𝜓(𝐺;𝐻1)+𝛽𝐻2𝜓(𝐺;𝐻2))+··· for an input attributed graph
𝐺is defined through a linear combination of attributed graphlets
(AGs), represented as 𝐻1,𝐻2,..., each one of which is weighted by
parameters 𝛽𝐻1,𝛽𝐻2,.... The function 𝜓(𝐺;𝐻)evaluates a match-
ing score between 𝐺and an AG𝐻in a sense that how precisely 𝐺
contains the AG 𝐻. We apply a sparse regularization to the parame-
ter𝛽𝐻by which a small number of important AGs for classification
can be obtained, i.e., an AG with non-zero 𝛽𝐻(in particular, if it
has large|𝛽𝐻|) can be regarded as a discriminative AG.
Important subgraphs and attribute vectors are usually unknown
beforehand. The basic strategy of our proposed method, called
LAGRA (Learning Attributed GRAphlets), is as follows:
•To explore potentially important substructures of graphs,
i.e., subgraphs, LAGRA uses graph mining by which all the
subgraphs in the given dataset can be considered up to the
given maximum graph size.
 
2830
KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
ψ ψ ψ ( ) ( ) ( ) = + + + ; ; ;
AG AG AG H1 H2 H3β0+βH1 βH2 βH3 f(G) G G G
Figure 1: Illustration of our attributed graphlet (AG) based
prediction model. The colors of each graph node represents
a graph node label, and a bar plot associated with each graph
node represents a trainable attribute vector.
2
 1
 0 1
f(G_i)2
0246LDA (in {S})
(a) Training data
2
 1
 0 1
f(G_i)2
024LDA (in {S})
 (b) Test data
Figure 2: Scatter plots of 𝑓(𝐺𝑖)and the direction obtained by
linear discriminant analysis on the orthogonal complement
of𝜷𝑆, where 𝜷𝑆is a subvector of 𝜷selected by LAGRA. The
dataset is BZR.
•For continuous attributes in AGs, we optimize them as train-
able parameters.
Figure 2 is a visualization of the identified subspace by LAGRA.
Both the axes are defined by a linear combination of the selected
small number of AGs (in this case, 63AGs selected that is quite
small compared with 148903 candidate subgraphs in this dataset).
The plot indicates that the subspace created by selected AGs is
highly discriminative, while both the axes are interpretable because
of the linearity with respect to AGs. See the last paragraph of Sec-
tion 4.2 for detail. Since the number of the possible subgraphs is
quite large and an attribute vector exists for each node in each one
of subgraphs, a naïve implementation becomes computationally
intractable. For the efficient optimization, we employ a block coor-
dinate update [ 23] based approach in which 𝜷(a vector containing
𝛽𝐻), the bias term 𝛽0, and attribute vectors are alternately updated.
In the alternate update of 𝜷, we apply the proximal gradient de-
scent [ 1,19], which is known as an effective algorithm to optimize
sparse parameters. For this step, we propose an efficient pruning
strategy, enabling us to identify dimensions that are not required
to update at that iteration. This pruning strategy has the three
advantages. First, by combining the sparsity during the proximal
update and the graph mining tree search, we can eliminate unnec-
essary dimensions without enumerating all the possible subgraphs.
Second, for removed variables 𝛽𝐻at that iteration, attribute vectors
in𝐻are also not required to be updated, which also accelerates
the optimization. Third, our pruning strategy is designed so that
it can maintain the update result compared with when we do not
perform the pruning (In other words, our pruning strategy does
not deteriorate the resulting model accuracy).
Our contributions are summarized as follows:•We propose an interpretable graph classification model, in
which the prediction is defined through a linear combina-
tion of graphlets that have trainable attribute vectors. By
imposing a sparse penalty on the coefficient of each AG, a
small number of important AGs can be identified.
•To avoid directly handling an intractably large size of opti-
mization variables, we propose an efficient pruning strategy
based on the proximal gradient descent, which can safely
ignore AGs that do not contribute to the update.
•We verify effectiveness of LAGRA by empirical evaluations.
Although our prediction model is simple and interpretable,
we show that prediction performance of LAGRA was supe-
rior to or comparable with well-known standard graph classi-
fication methods, and in those results, LAGRA actually only
used a small number of AGs. Further, we also show examples
of selected AGs to demonstrate the high interpretability.
2 PROPOSED METHOD: LAGRA
In this section, we describe our proposed method, called Learning
Attributed GRAphlets (LAGRA). First, in Section 2.1, we show the
formulation of our model and the definition of the optimization
problem. Second, in Section 2.2, we show an efficient optimization
algorithm for LAGRA.
2.1 Formulation
2.1.1 Problem Setting. We consider a classification problem in
which a graph 𝐺=(𝑉𝐺,𝐸𝐺,𝐿𝑣,𝒛𝐺𝑣)is an input. A set of nodes and
edges of𝐺are written as 𝑉𝐺and𝐸𝐺, respectively. Each one of
nodes𝑣∈𝑉𝐺has a categorical label 𝐿𝑣and a continuous attribute
vector 𝒛𝐺𝑣∈R𝑑, where𝑑is an attribute dimension. In this paper, an
attribute indicates a continuous attribute vector. We assume that a
label and an attribute vector are for a node, but the discussion in
this paper is completely same as for an edge label and attribute. A
training dataset is{(𝐺𝑖,𝑦𝑖)}𝑖∈[𝑛], in which𝑦𝑖∈{− 1,+1}is a binary
label and𝑛is the dataset size, where [𝑛]={1,...,𝑛}. Although
we only focus on the classification problem, our framework is also
applicable to the regression problem just by replacing the loss
function.
2.1.2 Attributed Graphlet Inclusion Score. We consider extract-
ing important small attributed graphs, which we call attributed
graphlets (AGs), that contributes to the classification boundary.
Note that throughout the paper, we only consider a connected
graph as an AG for a better interpretability (do not consider an
AG by a disconnected graph). Let 𝜓(𝐺𝑖;𝐻)∈[ 0,1]be a feature
representing a degree that an input graph includes an AG 𝐻. We
refer to𝜓(𝐺𝑖;𝐻)as the AG inclusion score (AGIS). Our proposed
LAGRA identifies important AGs by applying a feature selection to
a model with this AGIS feature.
Suppose that 𝐿(𝐺)=(𝑉𝐺,𝐸𝐺,𝐿𝑣)is a labeled graph in which
an attribute 𝒛𝐺𝑣for each node is removed from the original 𝐺. We
define AGIS so that it has a non-zero value only when 𝐿(𝐻)is
included in 𝐿(𝐺𝑖):
𝜓(𝐺𝑖;𝐻)=(
𝜙𝐻(𝐺𝑖)if𝐿(𝐻)⊑𝐿(𝐺𝑖),
0 otherwise,(1)
 
2831Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD ’24, August 25–29, 2024, Barcelona, Spain
G i
zGi
1zGi
2zGi
3
zGi
4zGi
5Graph
HzH
1zH
2
Graphlet
H/primeGraphletzH/prime
2 zH/prime
1L(G i)
L(H)/subsetsqequalL(G i)
L(H/prime)/negationslash/subsetsqequalL(G i)m
m/primeG i
HzH
1zH
2zGi
1zGi
2zGi
3
zGi
4zGi
5Graph
Graphlet
(a) (b)
Figure 3: Examples of matchings between a graph and AGs
(colors of graph nodes are node labels). (a) For two AGs 𝐻
and𝐻′,𝐿(𝐺𝑖)only contains 𝐿(𝐻), and𝐿(𝐻′)is not contained.
Then,𝜓(𝐺𝑖;𝐻)>0and𝜓(𝐺𝑖;𝐻′)=0. (b) An example of the set
of injections 𝑀={𝑚,𝑚′}, where𝑚(1)=2,𝑚(2)=3,𝑚′(1)=1,
and𝑚′(2)=4. The figure shows that 𝑚and𝑚′are label and
edge preserving.
where𝐿(𝐻)⊑𝐿(𝐺𝑖)means that𝐿(𝐻)is a subgraph of 𝐿(𝐺𝑖), and
𝜙𝐻(𝐺𝑖)∈( 0,1]is a function that provides a continuous inclusion
score of𝐻in𝐺𝑖. The condition 𝐿(𝐻)⊑𝐿(𝐺𝑖)makes AGIS highly
interpretable. For example, in the case of chemical composition
data, if𝐿(𝐻)represents O-C (oxygen and carbon are connected)
and𝜓(𝐺𝑖;𝐻)>0, then we can guarantee that 𝐺𝑖must contain O-C.
Figure 3(a) shows examples of 𝐿(𝐺𝑖)and𝐿(𝐻).
The function 𝜙𝐻(𝐺𝑖)needs to be defined so that it can represent
how strongly the attribute vectors in 𝐻can be matched to those
of𝐺𝑖. When𝐿(𝐻)⊑𝐿(𝐺𝑖), there exists at least one injection 𝑚:
𝑉𝐻→𝑉𝐺𝑖in which𝑚(𝑣)for𝑣∈𝑉𝐻preserves node labels and
edges among 𝑣∈𝑉𝐻. Figure 3(b) shows an example of when there
exist two injections. Let 𝑀be a set of possible injections 𝑚. We
define a similarity between 𝐻and a subgraph of 𝐺𝑖matched by
𝑚∈𝑀as follows
Sim(𝐻,𝐺𝑖;𝑚)=exp©­
«−𝜌∑︁
𝑣∈𝑉𝐻𝒛𝐻
𝑣−𝒛𝐺𝑖
𝑚(𝑣)2ª®
¬,
where𝜌>0is a fixed parameter that adjusts the length scale.
Inexp, the sum of squared distances of attribute vectors between
matched nodes are taken. To use this similarity in AGIS (1), we take
the maximum among all the matchings 𝑀:
𝜙𝐻(𝐺𝑖)=MaxPooling({Sim(𝐻,𝐺𝑖;𝑚):𝑚∈𝑀}) (2)
An intuition behind (2) is that it evaluates inclusion of 𝐻in𝐺𝑖
based on the best macthing in a sense of Sim(𝐻,𝐺𝑖;𝑚)for𝑚∈𝑀.
If𝐿(𝐻) ⊑𝐿(𝐺𝑖)and there exists 𝑚such that 𝒛𝐻𝑣=𝒛𝐺𝑖
𝑚(𝑣)for
∀𝑣∈𝑉𝐻, then,𝜙𝐻(𝐺𝑖)takes the maximum value (i.e., 1).
2.1.3 Model definition. Our prediction model linearly combines
the feature𝜓(𝐺𝑖;𝐻)as follows:
𝑓(𝐺𝑖)=∑︁
𝐻∈H𝜓(𝐺𝑖;𝐻)𝛽𝐻+𝛽0=𝝍⊤
𝑖𝜷+𝛽0, (3)
where𝛽𝐻and𝛽0are parameters,His a set of candidate AGs,
and𝜷and𝝍𝑖are vectors containing 𝛽𝐻and𝜓(𝐺𝑖;𝐻)for𝐻∈H,
Training data L H
zG1
1
zG1
2zG1
3
zG2
1 zG2
2
zG2
3
zG2
4G1
G2
G3zG3
1
zG3
2zG3
3zG3
4
zG3
5zH1
1
zH2
1
zH3
1
zH4
1zH3
2
zH4
2(maxpat = 2)
H1
H2
H3
H4Figure 4: An example of training data, LandH. SinceL
only includes subgraphs in the training data, “
 ” is not
included inL.His created fromLby adding trainable at-
tribute vectors 𝒛𝐻𝑖𝑣(𝑣∈𝑉𝐻𝑖).
respectively. LetL={𝐿|𝐿⊆𝐿(𝐺𝑖),𝑖∈[𝑛],|𝐿|≤maxpat}be
a set of all the labeled subgraphs contained in the training input
graphs{𝐺𝑖}𝑖∈[𝑛], where|𝐿|is the number of nodes in the labeled
graph𝐿andmaxpat is the user-specified maximum size of AGs.
The number of the candidate AGs |H|is set as the same size as |L|.
We setHas a set of attributed graphs created by giving trainable
attribute vectors 𝒛𝐻𝑣(𝑣∈𝑉𝐻)to each one of elements in L. Figure 4
shows a toy example. Our optimization problem for 𝛽𝐻,𝛽0and𝒛𝐻𝑣
is defined as the following regularized loss minimization in which
the sparse𝐿1penalty is imposed on 𝛽𝐻:
min
𝜷,𝛽0,ZH1
2𝑛∑︁
𝑖=1ℓ(𝑦𝑖,𝑓(𝐺𝑖))+𝜆∑︁
𝐻∈H|𝛽𝐻|, (4)
whereZH={𝒛𝐻𝑣|𝑣∈𝑉𝐻,𝐻∈H} , andℓis a convex differentiable
loss function. Here, we employ the squared hinge loss function [ 7]:
ℓ(𝑦𝑖,𝑓(𝐺𝑖))=max(1−𝑦𝑖𝑓(𝐺𝑖),0)2.
Since the objective function (4) induces a sparse solution for 𝛽𝐻, we
can identify a small number of important AGs as 𝐻having the non-
zero𝛽𝐻. However, this optimization problem has an intractably
large number of optimization variables ( Hcontains all the possible
subgraphs in the training dataset and each one of 𝐻∈H has the
attribute vector 𝒛𝐻𝑣∈R𝑑for each one of nodes). We propose an
efficient optimization algorithm that mitigates this problem.
2.2 Optimization
Our optimization algorithm is based on the block coordinate update
[23] algorithm, in which the (proximal) gradient descent alternately
updates a block of variables. We update one of 𝜷,𝛽0andZHalter-
nately, while the other two parameters are fixed. First, the proximal
gradient update is applied to 𝜷because it has the 𝐿1penalty. Second,
for𝛽0, we calculate the optimal solution under fixing the other vari-
able because it is easy to obtain. Third, for ZH, we apply the usual
gradient descent update because it does not have sparse penalty.
The difficulty of the optimization problem (4) originates from
the size ofH. We select a small size of a subset W ⊆H , and
only 𝜷W∈R|W|, defined by 𝛽𝐻for𝐻∈W , and corresponding
attribute vectorsZW={𝒛𝐻𝑣|𝑣∈𝑉𝐻,𝐻∈ W} ⊆ ZHare
updated. We propose an efficient pruning strategy by combining
the proximal gradient with the graph mining, which enables us
 
2832KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
to selectWwithout enumerating all the possible subgraphs. A
notable characteristics of this approach is that it can obtain the
completely same result compared with when we do not restrict the
size of variables.
2.2.1 Update 𝜷,𝛽0andZH.Before introducing the subset W,
we first describe update rules of each variable. First, we apply the
proximal gradient update to 𝜷. Let
𝑔𝐻(𝜷)=𝑛∑︁
𝑖=1𝜕ℓ(𝑦𝑖,𝑓(𝐺𝑖))
𝜕𝑓(𝐺𝑖)𝜓(𝐺𝑖;𝐻) (5)
be the derivative of the loss term in (4) with respect to 𝛽𝐻. Then,
the update of 𝜷is defined by
𝛽(new)
𝐻←prox(𝛽𝐻−𝜂𝑔𝐻(𝜷)), (6)
where𝜂>0is a step length, and
prox(𝑎)= 
𝑎−𝜂𝜆 if𝑎≥𝜂𝜆,
0 if𝑎∈(−𝜂𝜆,𝜂𝜆),
𝑎+𝜂𝜆 if𝑎≤−𝜂𝜆
is a proximal operator (Note that the proximal gradient for the 𝐿1
penalty is often called ISTA [ 1], for which an accelerated variant
called FISTA is also known. We here employ ISTA for simplicity).
We select the step length 𝜂by the standard backtrack search.
The bias term 𝛽0is update by
min
𝛽0𝑛∑︁
𝑖=0max(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0),0)2,
which is the optimal solution of the original problem (4) for given
other variables 𝜷andZH. Since the objective function of 𝛽0is a
differential convex function, the update rule of 𝛽0can be derived
from the first order condition as
𝛽(new)
0←Í
𝑖∈I(new)(𝑦𝑖−𝝍⊤
𝑖𝜷)
|I(new)|, (7)
whereI(new)={𝑖|1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽(new)
0)>0}. This update rule
contains𝛽(new)
0inI(new). However, it is easy to calculate the up-
date (7) without knowing 𝛽(new)
0beforehand. Here, we omit detail
because it is a simple one dimensional problem (see supplementary
appendix A).
For𝒛𝐻𝑣∈ZH, we employ the standard gradient descent:
𝒛𝐻(new)
𝑣←𝒛𝐻
𝑣−𝛼𝑛∑︁
𝑖=1𝜕ℓ(𝑦𝑖,𝑓(𝐺𝑖))
𝜕𝑓(𝐺𝑖)×
𝛽𝐻𝜕𝜓(𝐺𝑖;𝐻)
𝜕𝒛𝐻𝑣
,(8)
where𝛼>0is a step length to which we apply the standard
backtrack search.
2.2.2 Gradient Pruning with Graph Mining. In every update of 𝜷,
we incrementally add required 𝐻intoW⊆H . For the complement
setW=H\W , which contains AGs that have never been updated,
we initialize 𝛽𝐻=0for𝐻∈W. For the initialization of a node
attribute vector 𝒛𝐻𝑣∈ZH, we set the same initial vector if the
node (categorical) labels are same, i.e., 𝒛𝐻𝑣=𝒛𝐻′
𝑣′if𝐿𝑣=𝐿𝑣′for
∀𝐻,𝐻′∈H (in practice, we use the average of the attribute vectors
within each node label). This constraint is required for our pruningcriterion, but it is only for initial values. After the update (8), all 𝒛𝐻𝑣
can have different values.
Since𝛽𝐻=0for𝐻∈W, it is easy to derive the following
relation from the proximal update (6):
|𝑔𝐻(𝜷)|≤𝜆and𝐻∈W ⇒ 0=prox(𝛽𝐻−𝜂𝑔𝐻(𝜷)).(9)
This indicates that if the conditions in the left side hold, we do not
need to update 𝛽𝐻because it remains 0. Therefore, we set
W←W∪n
𝐻|𝑔𝐻(𝜷)|>𝜆,∀𝐻∈Wo
, (10)
and apply the update (6) only to 𝐻∈W . However, evaluating
|𝑔𝐻(𝜷)|>𝜆for all𝐻∈Wcan be computationally intractable
because it needs to enumerate all the possible subgraphs. The fol-
lowing theorem can be used to avoid this difficulty:
Theorem 2.1. Let𝐿(𝐻′) ⊒𝐿(𝐻)and𝐻,𝐻′∈W. Then, the
absolute value of the derivative (5) is bounded as
|𝑔𝐻′(𝜷)|≤𝑔𝐻(𝜷)
where
𝑔𝐻(𝜷)=max(∑︁
𝑖∈I∩{𝑖|𝑦𝑖>0}𝑦𝑖𝜓(𝐺𝑖;𝐻)(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0)),
−∑︁
𝑖∈I∩{𝑖|𝑦𝑖<0}𝑦𝑖𝜓(𝐺𝑖;𝐻)(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0)))
,
whereI={𝑖|1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0)>0}.
See supplementary appendix B for the proof. Note that here I
is defined by the current 𝛽0unlike (7). This theorem indicates
that the gradient|𝑔𝐻′(𝜷)|for any𝐻′whose𝐿(𝐻′)contains𝐿(𝐻)
as a subgraph can be bounded by 𝑔𝐻(𝜷). It should be noted that
𝑔𝐻(𝜷)can be calculated without generating 𝐻′, and it mainly needs
only the model prediction with the current parameter 𝝍⊤
𝑖𝜷+𝛽0,
which can be immediately obtained at each iteration, and AGIS
𝜓(𝐺𝑖;𝐻). The rule (9) reveals that, to identify 𝛽(new)
𝐻′=0, we only
require to know whether |𝑔𝐻′(𝜷)|≤𝜆holds, and thus, an important
consequence of theorem 2.1 is the following rule:
𝑔𝐻(𝜷)≤𝜆and𝐻∈W
⇒ |𝑔𝐻′(𝜷)|≤𝜆for∀𝐻′∈{𝐻′|𝐿(𝐻′)⊒𝐿(𝐻),𝐻′∈W}.
(11)
Therefore, if the conditions in the first line in (11) hold, any 𝐻′
whose𝐿(𝐻′)contains𝐿(𝐻)as a subgraph can be discarded during
that iteration. Further, from (8), we can immediately see that at-
tribute vectors 𝒛𝐻𝑣for∀𝑣∈𝑉𝐻are also not necessary to be updated
if𝛽𝐻=0. This is an important fact because updates of a large
number of variables can be omitted.
Figure 5 shows an illustration of the forward and backward
(gradient) computations of LAGRA. For the gradient pruning, an
efficient algorithm can be constructed by combining the rule (11)
and a graph mining algorithm. A well-known efficient graph min-
ing algorithm is gSpan [ 24], which creates the tree by recursively
expanding each graph in the tree node as far as the expanded graph
is included in a given set of graphs as a subgraph. An important
characteristics of the mining tree is that all graphs must contain
any graph of its ancestors as subgraphs. Therefore, during the tree
 
2833Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD ’24, August 25–29, 2024, Barcelona, Spain
traverse (depth-first search) by gSpan, we can prune the entire sub-
tree (all descendant nodes) if 𝑔𝐻(𝜷)≤𝜆holds for the AG 𝐻in a
tree node (Figure 5(b)). This means that we can update Wby (10)
without exhaustively investigating all the elements in W.
gSpan has another advantage for LAGRA. To calculate the feature
𝜙𝐻(𝐺𝑖), defined in (2), LAGRA requires a set of injections 𝑀(an
example is shown in Figure 3(b)). gSpan keeps the information of 𝑀
during the tree traverse because it is required to expand a subgraph
in each𝐺𝑖(see the authors implementation https://sites.cs.ucsb.
edu/~xyan/software/gSpan.htm). Therefore, we can directly use 𝑀
created by gSpan to calculate (2).
2.2.3 Algorithm. We here describe entire procedure of the opti-
mization of LAGRA. We employ the so-called regularization path
following algorithm (e.g., [ 5]), in which the algorithm starts from
a large value of the regularization parameter 𝜆and gradually de-
creases it while solving the problem for each 𝜆. This strategy can
start from highly sparse 𝜷, in which usuallyWalso becomes small.
Further, at each 𝜆, the solution obtained in the previous 𝜆, can
be used as the initial value by which faster convergence can be
expected (so-called warm start).
Algorithm 1 shows the procedure of the regularization path fol-
lowing. We initialize 𝜷=0, which is obviously optimal when 𝜆=∞.
In line 2 of Algorithm 1, we calculate 𝜆maxat which 𝜷starts having
non-zero values: 𝜆max=max𝐻∈HÍ𝑛
𝑖∈[𝑛]𝑦𝑖𝜓(𝐺𝑖;𝐻)(1−𝑦𝑖𝛽0),
where𝛽0=Í
𝑖∈[𝑛]𝑦𝑖/𝑛. See supplementary appendix D for deriva-
tion.𝜆maxcan also be written as 𝜆max=max𝐻∈H|𝑔𝐻(0)|. To find
max𝐻∈H, we can use almost the same gSpan based pruning strategy
by using an upper bound of 𝑔𝐻(𝜷)as shown in Section 2.2.2 (the
only difference is to search the max value only, instead of searching
all𝐻satisfying|𝑔𝐻(𝜷)|>𝜆), though in Algorithm 1, this process
is omitted for brevity. After setting 𝜆0←𝜆max, the regularization
parameter𝜆is decreased by using a pre-defined decreasing fac-
tor𝑅as shown in line 6 of Algorithm 1. For each 𝜆1>···>𝜆𝐾,
the parameters 𝜷,𝛽0andZHare alternately updated as described
in Section 2.2.1 and 2.2.2. We stop the alternate update by moni-
toring performance on the validation dataset in line 14 (stop by
thresholding the decrease of the objective function is also possible).
The algorithm of the pruning strategy described in Section 2.2.2
is shown in Algorithm 2. This function recursively traverses the
graph mining tree. At each tree node, first, 𝑔𝐻(𝜷)is evaluated to
prune the subtree if possible. Then, if |𝑔𝐻(𝜷)|>𝜆𝑘,𝐻is included
inW. The expansion from 𝐻(creating children of the graph tree)
is performed by gSpan, by which only the subgraphs contained in
the training set can be generated (see the original paper [ 24] for
detail of gSpan). The initialization of the trainable attribute 𝒛𝐻′
𝑣is
performed when 𝐻′is expanded (line 15).
3 RELATED WORK
For graph-based prediction problems, recently, graph neural net-
works (GNNs) [ 29] have attracted wide attention. However, inter-
preting GNNs is not easy in general. According to a recent review of
explainable GNNs [ 27], almost all of explainability studies for GNNs
are instance-level explanations, which provides input-dependent
explanations (Here, we do not mention each one of input-dependent
approaches because the purpose is clearly different from LAGRA).Algorithm 1: Optimization of LAGRA
1function Reguralization-Path( 𝐾,𝑅, MaxEpoch)
2𝐻0←a graph at the root node of the mining tree
3W←∅
4 𝜷←0, 𝛽0=Í
𝑖∈[𝑛]𝑦𝑖/𝑛
5𝜆0←𝜆max Compute𝜆max
6 for𝑘=1,2,...,𝐾 do
7𝜆𝑘←𝑅𝜆𝑘−1
8 forepoch =1,2,..., MaxEpoch do
9W←W∪ GradientPruning(𝐻0, 𝜆𝑘)
10 Update 𝜷by (6) for𝐻∈W
11 Update𝛽0by (7)
12 Update 𝒛𝐻𝑣by (8) for𝐻∈W
13 val_loss←Compute validation loss
14 ifval_loss has not been improved in the past 𝑞
iterations then
15 break Inner loop stopping condition
16 else
17M(𝑘)←(W,𝜷,𝛽0)
18 return{M(𝑘)}𝐾
𝑘=0
Algorithm 2: Gradient Pruning
1function GradientPruning( 𝐻, 𝜆𝑘)
2W←∅
3 if𝑔𝐻(𝜷)≤𝜆𝑘then
4 return∅ Prune the subtree
5 if|𝑔𝐻(𝜷)|>𝜆𝑘then
6W←W∪{ 𝐻}
7C← CreateChildren(𝐻)
8 for𝐻′∈Cdo
9W←W∪ GradientPruning(𝐻′, 𝜆𝑘)
10 returnW
11function CreateChildren( 𝐻)
12 ifchildren of𝐻have never been created by gSpan then
13C← graphs expanded from 𝐻by gSpan
14 for𝐻′∈Cdo
15 𝒛𝐻′
𝑣←mean{𝒛𝐺𝑖
𝑣′|𝑣′∈𝑉𝐺𝑖,𝐿𝑣=𝐿𝑣′,𝑖∈[𝑛]}
16 Compute{𝜓(𝐺𝑖;𝐻′)}𝑛
𝑖=1using𝑀created by
gSpan
An exception is XGNN [ 26], in which important discriminative
graphs are generated for a given already trained GNN by maximiz-
ing the GNN output for a target label. However, unlike our method,
the prediction model itself remains black-box, and thus, it is difficult
to know underlying dependency between the identified graphs and
the prediction.
A classical approach to graph-based prediction problems is the
graph kernel [ 10]. Although graph kernel itself does not identify
 
2834KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
m
m/primezH
1zH
2
zGi
1zGi
2zGi
3
zGi
4zGi
5zGi
1 zGi
2zGi
3
zGi
4 zGi
5
G i GraphH Attributed graphlet
mm/prime
zGi
1zGi
2zGi
3
zGi
4zGi
5Attributed graphlet......
...(b) Graph mining tree for 
gradient pruningf(G i)Training input
m
zGi
1zGi
2zGi
3
zGi
4zGi
5Attributed graphletObjective function
......
H/prime
zH/prime
1zH/prime
2Sim
SimMax 
Pooling
Max 
PoolingSim
Sim
Max 
PoolingSimForward pass
Backward pass
β0AGIS
AGIS
AGISH
H/prime
(a) AGIS computation
Figure 5: An illustration of LAGRA. a) In the forward pass, only passes with |𝛽𝐻|>0contribute to the output. AGIS is defined
by the best matching between an input graph and an AG. b) For the backward pass, the gradient can be pruned when the rule
(11)is satisfied. In this illustration, 𝐻′′is pruned by which graphs expanded from 𝐻′′are not required to compute the gradient.
Table 1: Classification accuracy and the number of selected non-zero 𝛽𝐻by LAGRA (the bottom row). The average of five runs
and its standard deviation are shown. The underlines indicate the best average accuracy for each dataset and the bold-face
indicates that the result is comparable with the best method in a sense of one-sided 𝑡-test (significance level 5%). # best indicates
frequency that the method is the best or comparable with the best method.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC #
best
GH 0.9985±0.0020 0.8458±0.0327 0.7872±0.0252 0.7250±0.0113 0.6050±0.0857 0.7277±0.0332 0.6767±0.0655 2
ML 0.9630±0.0062 0.8289±0.0141 0.7787±0.0080 0.7105±0.0300 0.6000±0.0652 0.6205±0.0335 0.4867±0.0356 0
P
A 0.9805±0.0086 0.8313±0.0076 0.7809±0.0144 0.7316±0.0435 0.7500±0.0758 0.6884±0.0077 0.5400±0.0859 1
DGCNN 0.9830±0.0046 0.8169±0.0177 0.8021±0.0401 0.7289±0.0192 0.7289±0.0192 0.7509±0.0114 0.9867±0.0125 3
GCN 0.9840±0.0030 0.8290±0.0460 0.8340±0.0257 0.7490±0.0312 0.7000±0.0837 0.6880±0.0202 0.9630±0.0194 2
GA
T 0.9880±0.0041 0.8220±0.0336 0.7830±0.0274 0.7110±0.0156 0.7100±0.0768 0.7160±0.0108 0.9800±0.0267 2
GIN 0.9990±0.0014 0.8265±0.0290 0.7851±0.0254 0.7526±0.0350 0.7150±0.0285 0.7372±0.0266 0.9133±0.0274 3
LGARA
(Proposed) 0.9900±0.0050 0.8892±0.0207 0.8043±0.0229 0.8171±0.0113 0.6450±0.0797 0.7491±0.0142 1.0000±0.0000 4
#
non-zero𝛽𝐻 50.4±17.1 52.4±19.0 45.4±14.9 40.0±11.6 7.2±8.4 25.8±9.4 35.8±35.0 -
important substructures, recently, [ 3] has proposed an interpretable
kernel-based GNN, called KerGNN. KerGNN uses a graph kernel
function as a trainable filter, inspired by the well-known convolu-
tional networks, and the filter updates the node attributes of the
input graph so that it embeds similarity to learned important sub-
graphs. Then, [ 3] claims that resulting graph filter can be seen as
a key structure. However, a learned subgraph in a graph kernel
filter is difficult to interpret. The kernel-based matching does not
guarantee the existence of a subgraph unlike our AGIS (1), and
further, only 1-hop neighbors of each node in the input graph are
matched to a graph filter.
Another graph mining based approach is [ 13]. This approach
also uses a pruning based acceleration for the optimization, but it isbased on the optimality of the convex problem while our proximal
gradient pruning is applicable to the non-convex problem of LA-
GRA. Further, more importantly, [ 13] cannot deal with continuous
attributes. The prediction model of LAGRA is inspired by a method
for learning time-series shaplets (LTS) [ 6]. LTS is also based on
a linear combination of trainable shaplets, which is a short frag-
ment of a time-series sequence. Unlike time-series data, possible
substructures in graph data have a combinatorial nature because
of which our problem setting has a computational difficulty that
does not exist in the case of LTS, for which LAGRA provide a graph
mining based efficient strategy.
 
2835Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD ’24, August 25–29, 2024, Barcelona, Spain
4 EXPERIMENTS
Here, we empirically verify effectiveness of LAGRA. We used stan-
dard graph benchmark datasets, called AIDS, BZR, COX2, DHFR,
ENZYMES, PROTEINS and SYNTHETIC, retrieved from https://
ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets (for
ENZYMES, we only used two classes among original six classes
to make a binary problem). To simplify comparison, we only used
node labels and attributes, and did not use edge labels and attributes.
Statistics of datasets are summarized in supplementary appen-
dix E. The datasets are randomly divided into train :validation :
test=0.6 : 0.2 : 0.2. For the regularization path algorithm (Al-
gorithm 1), we created candidate values of 𝜆by uniformly divid-
ing[log(𝜆max),log(0.01𝜆max)]into 100grid points. We selected
𝜆,maxpat∈{5,10}and𝜌∈{1,0.5,0.1,0.05,0.01}based on the
validation performance.
4.1 Prediction Performance
For the prediction accuracy comparison, we used graph kernels and
graph neural networks (GNN). We used three well-known graph
kernels that can handle continuous attributes, i.e., graph hopper ker-
nel (GH) [ 4], multiscale Laplacian kernel (ML) [ 9] and propagation
kernel (PA) [ 14], for all of which the library called GraKeL [ 18] was
used. For the classifier, we employed the 𝑘-nearest neighbor ( 𝑘-NN)
classification for which each kernel function 𝑘(𝐺𝑖,𝐺𝑗)defines the
distance function as ∥𝐺𝑖−𝐺𝑗∥=√︁
𝑘(𝐺𝑖,𝐺𝑖)−2𝐾(𝐺𝑖,𝐺𝑗)+𝑘(𝐺𝑗,𝐺𝑗).
The number of neighbors 𝑘is optimized by the validation set. For
GNN, we used deep graph convolutional neural network (DGCNN)
[28], graph convolutional network (GCN) [ 8], graph attention net-
work (GAT) [ 20], and graph isomorphism network (GIN) [ 22]. For
DGCNN, the number of hidden units {64,128,256}and epochs are
optimized by the validation set. The other settings were in the de-
fault settings of the authors implementation https://github.com/
muhanzhang/pytorch_DGCNN. For GCN and GAT, we also selected
the number of hidden units and epochs as above. For other settings,
we followed [ 25]. For GIN, we modified the authors implemen-
tation so that node attributes can be incorporated, and the same
hyper-parameter tuning as [ 22] was employed. The hidden dimen-
sion{16,32,64}, dropout ratio{0,0.5}, and batch size{32,64,128}
were selected by the validation performance, and other settings
were the default setting.
The results are shown in Table 1. LAGRA was the best or compa-
rable with the best method (in a sense of one-sided 𝑡-test) for BZR,
DHFR, PROTEINS and SYNTHETIC (4 out of 7 datasets). For AIDS
and COX2, LAGRA has similar accuracy values to the best methods
though they were not regarded as the best accuracy in 𝑡-test. The
three GNNs also show stable performance overall. Although our
main focus is to build an interepretable model, we see that LAGRA
achieved comparable accuracy with the current standard methods.
Further, LAGRA only used a small number of AGs shown in the
bottom row of Table 1, which suggests high interpretability of the
learned models.
4.2 Selected AGs
We here show examples of identified important AGs. Figure 6 and 7
show AGs having the two largest positive and negative 𝛽𝐻for DHFR
and BZR datasets, respectively. In each figure, a labeled graphlet𝐿(𝐻)is shown in the left side (the numbers inside the graph nodes
are the graph node labels) and optimized attribute vectors for each
one of nodes are shown as bar plots in the right side. We can clearly
see important substractures not only by as structural information
of a graph but also attribute values associated with each node.
In a few datasets, two classes can be separated even in two
dimensional space of AGIS. Figure 8 show scatter plots of the test
dataset (not the training dataset) with the axes of identified features
by the LAGRA training. Let 𝐻+and𝐻−be AGs having the largest
positive and negative 𝛽𝐻, respectively. The horizontal and vertical
axes of plots are 𝜓(𝐺𝑖,𝐻+)and𝜓(𝐺𝑖,𝐻−). In particular, in the AIDS
dataset, for which classification accuracy was very high in Table 1,
two classes are clearly separated. For DHFR, we can also see points
in two classes tend to be located on the upper left side and the lower
right side. The dashed lines are boundaries created by (class-balance
weighted) logistic regression fitted to the test points in these two
dimensional spaces. The estimated class conditional probability
hasAUC =0.94and0.62for AIDS and BZR, respectively, which
indicate that differences of two classes are captured even only by
two AGs in these datasets.
Another visualization of the space of the selected AGs are shown
in Fig. 2. The 𝑥-axis is𝑓(𝐺𝑖), and the𝑦-axis is defined by linear
discriminant analysis (LDA). LDA is applied to the space defined
by𝜙(𝐺𝑖;𝐻)with selected 𝐻under the condition that the projected
space is orthogonal to 𝜷𝑆, by which the axes in Fig. 2 are an orthog-
onal basis (Note that applying LDA to the original feature space
defined by all∀𝐻is impossible because of its high dimension. This
visualization becomes possible due to the AG selection of LAGRA).
Both the axes are defined by a linear combination of 𝜙(𝐺𝑖;𝐻)by
which they are interpretable as a weighted sum of the contribu-
tions from the selected AGs. Figure 2 indicates that the space of the
selected AGs clearly discriminates two classes. The same scatter
plots on other datasets are shown in Fig. 12.
4.3 Discussion on Computational Time
Finally, we verify computational time of LAGRA. As we describe in
Section 2.1.3, the size of candidate AGs |H|is equal to|L|, which
is the number of all the possible subgraphs in the training datasets.
Therefore, it can be quite large. In each dataset, we counted |H|
and obtained AIDS =134281, BZR =148903, COX2 =101185,
DHFR =137872, ENZYMES >15464000, PROTEINS >13987000,
andSYNTHETIC >699000. For the ENZYMES, PROTEINS, and
SYNTHETIC datasets, the counts are the lower bounds because
they require too long time to count all candidates.
The optimization variables in the objective function (4) are 𝜷,𝛽0
andZH. The dimension of 𝜷is|H|and the node attribute vector
𝒛𝐻𝑣∈R𝑑exists for each one of nodes in 𝐻∈H. Thus, the number
of optimization variables in (4) is 1+|H|+Í
𝐻∈H|𝐻|×𝑑, which
can be prohibitively large.
Figure 9 shows the computational time during the regularization
path. The horizontal axis is 𝑘of𝜆𝑘in Algorithm 1. The datasets
are AIDS and ENZYMES. In the regularization path algorithm, the
number of non-zero 𝛽𝐻typically increases during the process of
decreasing𝜆, because the 𝐿1penalty becomes weaker gradually. As
a results, in both the plots, the total time increases with the 𝜆index.
Although LAGRA performs the traverse of the graph mining tree
 
2836KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
(a) (b) (c) (d)
Figure 6: Selected important AGs for DHFR dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
(a) (b) (c) (d)
Figure 7: Selected important AGs for BZR dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
0.0 0.2 0.4 0.6 0.8 1.0
(Gi,H+)
0.00.20.40.60.81.0(Gi,H)
 1
-1
(a) AIDS ( 𝑥-axis: Fig. 11(a) and 𝑦-
axis:Fig. 11(c)) in appendix
0.0 0.2 0.4 0.6 0.8
(Gi,H+)
0.00.10.20.30.40.50.6(Gi,H)
 1
-1
(b) DHFR ( 𝑥-axis: Fig. 6(a) and 𝑦-
axis:Fig. 6(c))
Figure 8: Scatter plots of AGIS for graphlets 𝐻+and𝐻−, hav-
ing the largest positive and negative coefficients, respectively.
in every iteration of the gradient update (line 9 in Algorithm 1),
Figure 9 shows that the traverse time was not necessarily dominant(Note that the vertical axis is in log scale). In particular, when only
a small number of tree nodes are newly expanded at that 𝜆, the cal-
culation for the tree search becomes faster because AGIS 𝜓(𝐺𝑖,𝐻)
is already computed at the most of tree nodes. The computational
times were at most about 103sec for these datasets. We do not claim
that LGARA is computationally faster compared with other stan-
dard algorithms (such as graph kernels), but as the computational
time of the optimization problem with 1+|H|+Í
𝐻∈H|𝐻|×𝑑
variables, the results obviously indicate effectiveness of our pruning
based optimization approach.
Figure 10 shows the average number of traversed graph mining
tree nodes and the size of selected |W| for AIDS and ENZYMES.
In this figure, both the values increased with the decrease of 𝜆
because the effect of the sparse penalty becomes weaker. The total
number of the tree nodes were 134281 and more than 15464000 for
AIDS and ENZYMES, respectively. Figure 10 shows the number of
traversed nodes were at most about 6×103/134281(≈0.05)and9×
102/15464000(≈6×10−5), respectively. This clearly indicates that
our pruning strategy can drastically reduce the tree nodes, at which
the evaluation of 𝑔𝐻(𝜷)is required as shown in Algorithm 2. In the
figure, we can see that |W| was further small. This indicates the
updated 𝜷was highly sparse, by which the update of 𝒛𝐻𝑣becomes
easier because it requires only for non-zero 𝛽𝐻. The same plots on
the BZR, COX2, and DHFR datasets are in Fig. 13 in appendix.
Unfortunately, LAGRA is still computationally demanding com-
pared with other standard graph classification methods (Compu-
tational time of LAGRA and the GNNs are shown in Table 3 in
supplementary appendix F). However, to our knowledge, LAGRA
is the first method that can identify important attributed graphlets
 
2837Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD ’24, August 25–29, 2024, Barcelona, Spain
0 20 40 60 80 100
 index
100101102103TimeTraverse (Identify W)
Other processes
(a) AIDS
0 20 40 60 80 100
 index
101
100101102TimeTraverse (Identify W)
Other processes (b) ENZYMES
Figure 9: Transition of computational time (sec) on regular-
ization path. For each 𝜆, the total time and the time required
to traverse the graph mining tree (in other words, the time
required to identify W) is shown separately.
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H
(a) AIDS
0 20 40 60 80 100
 index
100101102103104105106107Mean |W|
#visit nodes
# candidate H (b) ENZYMES
Figure 10: The average number of visited tree nodes and
|W|. Note that # candidate Hin ENZYMES is a lower bound
because we cannot count the exact number due to the large
computational cost.
through the exact match of labeled subgraphs, by which higher
interpretability is obtained than other approaches.
5 CONCLUSION
This paper proposed LAGRA (Learning Attributed GRAphlets),
which learns a prediction model that linearly combines attributed
graphlets (AGs). In LAGRA, graph structures of AGs are generated
through a graph mining algorithm, and attribute vectors are op-
timized as a continuous trainable parameters. To identify a small
number of AGs, the 𝐿1sparse penalty is imposed on coefficients of
AGs. We employed a block coordinate update based optimization
algorithm, in which an efficient pruning strategy was proposed by
combining the proximal gradient update and the graph mining tree
search. Our empirical evaluation showed that LAGRA has superior
or comparable performance with standard graph classification algo-
rithms. We further demonstrated that LAGRA actually can identify
a small number of discriminative AGs that have high interpretabil-
ity. On the other hand, there exist several important remaining
issues.
•An obvious limitation of LAGRA is its scalability as discussed
in the end of Section 4.3.
•Further, we only focus on the prediction task of a label as-
sociated with the entire graph. Another well-known settingis to predict label of each node in a single large graph, to
which current LAGRA is not applicable.
ACKNOWLEDGMENTS
This work was partially supported by MEXT KAKENHI (21H03498,
23K21696, 22H00300, 23K17817), and International Joint Usage/Research
Project with ICR, Kyoto University (2023-34, 2024-30).
REFERENCES
[1]Amir Beck and Marc Teboulle. 2009. A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences 2, 1
(2009), 183–202.
[2]Felix A. Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S. Schoenholz,
George E. Dahl, Oriol Vinyals, Steven Kearnes, Patrick F. Riley, and O. Anatole
von Lilienfeld. 2017. Prediction Errors of Molecular Machine Learning Models
Lower than Hybrid DFT Error. Journal of Chemical Theory and Computation 13,
11 (2017), 5255–5264.
[3]Aosong Feng, Chenyu You, Shiqiang Wang, and Leandros Tassiulas. 2022.
KerGNNs: Interpretable Graph Neural Networks with Graph Kernels. In Thirty-
Sixth AAAI Conference on Artificial Intelligence, AAAI. AAAI Press, 6614–6622.
[4]Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, and Karsten
Borgwardt. 2013. Scalable kernels for graphs with continuous attributes. In
Advances in Neural Information Processing Systems. 216–224.
[5]Jerome Friedman, Trevor Hastie, Holger Höfling, and Robert Tibshirani. 2007.
Pathwise coordinate optimization. The Annals of Applied Statistics 1, 2 (12 2007),
302–332.
[6]Josif Grabocka, Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme.
2014. Learning Time-Series Shapelets. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. Association
for Computing Machinery, New York, NY, USA, 392–401.
[7]Katarzyna Janocha and Wojciech Marian Czarnecki. 2016. On Loss Functions for
Deep Neural Networks in Classification. Schedae Informaticae 25 (2016), 49.
[8]Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net.
[9]Risi Kondor and Horace Pan. 2016. The multiscale Laplacian graph kernel. In
Advances in Neural Information Processing Systems. 2990–2998.
[10] Nils M. Kriege, Fredrik D. Johansson, and Christopher Morris. 2020. A survey on
graph kernels. Applied Network Science 5 (2020), 6.
[11] Steph-Yves Louis, Yong Zhao, Alireza Nasiri, Xiran Wang, Yuqi Song, Fei Liu, and
Jianjun Hu. 2020. Graph convolutional neural networks with global attention
for improved materials property prediction. Phys. Chem. Chem. Phys. 22 (2020),
18141–18148. Issue 32.
[12] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for
learning with graphs. In ICML 2020 Workshop on Graph Representation Learning
and Beyond (GRL+ 2020). www.graphlearning.io
[13] Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, and
Ichiro Takeuchi. 2016. Safe pattern pruning: An efficient approach for predictive
pattern mining. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 1785–1794.
[14] Marion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting.
2016. Propagation kernels: efficient graph kernels from propagated information.
Machine Learning 102, 2 (2016), 209–245.
[15] N. Pržulj. 2007. Biological network comparison using graphlet degree distribution.
Bioinformatics 23, 2 (2007), e177–e183.
[16] Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo, and Pierre Baldi. 2005. Graph
kernels for chemical informatics. Neural Networks 18, 8 (2005), 1093–1110.
[17] Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten
Borgwardt. 2009. Efficient graphlet kernels for large graph comparison. In
Artificial Intelligence and Statistics. 488–495.
[18] Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Kon-
stantinos Skianis, and Michalis Vazirgiannis. 2020. GraKeL: A Graph Kernel
Library in Python. Journal of Machine Learning Research 21, 54 (2020), 1–5.
[19] Marc Teboulle. 2017. A simplified view of first order methods for optimization.
Mathematical Programming 170 (2017), 67–96.
[20] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
[21] Tian Xie and Jeffrey C. Grossman. 2018. Crystal Graph Convolutional Neural
Networks for an Accurate and Interpretable Prediction of Material Properties.
Phys. Rev. Lett. 120 (2018), 145301. Issue 14.
 
2838KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
[22] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions.
[23] Yangyang Xu and Wotao Yin. 2017. A Globally Convergent Algorithm for Non-
convex Optimization Based on Block Coordinate Update. Journal of Scientific
Computing 72 (2017), 700–734.
[24] Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-based substructure pattern
mining. In Proceedings. 2002 IEEE International Conference on Data Mining. IEEE,
721–724.
[25] J. You, J. M. Gomes-Selman, R. Ying, and J Leskovec. 2021. Identity-aware Graph
Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 35. 10737–10745.
[26] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. 2020. XGNN: Towards Model-
Level Explanations of Graph Neural Networks. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. Asso-
ciation for Computing Machinery, New York, NY, USA, 430–438.
[27] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2022. Explainability in
Graph Neural Networks: A Taxonomic Survey. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2022).
[28] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-
to-end deep learning architecture for graph classification. In Proceedings of AAAI
Conference on Artificial Inteligence.
[29] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI Open 1 (2020), 57–81.
A UPDATE 𝛽0
The objective of 𝛽0can be re-written as
min
𝛽0𝑛∑︁
𝑖=0max(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0),0)2=min
𝛽0∑︁
𝑖∈I(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0))2.
(12)
For simplicity, we assume that all 𝝍⊤
𝑖𝜷for𝑖∈[𝑛]have different
values (even when this does not hold, the optimal solution can be
obtained by the same approach). Depending on 𝛽(new)
0, elements in
I(new)={𝑖|1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽(new)
0)>0}changes in a piecewise
constant manner. The point that I(new)changes are characterized
by the solution of the equation 𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0)=1(𝑖∈[𝑛])with
respect to𝛽0, i.e., there exist 𝑛+1segments on the space of 𝛽0∈R.
Let𝐵(𝑘)=[𝛽𝑠𝑘
0,𝛽𝑒𝑘
0]be the𝑘-th segment ( 𝑘∈{0,...,𝑛}) andI(𝑘)
isI(new)when𝛽(new)
0∈𝐵(𝑘). Note that 𝛽𝑒𝑘
0=𝛽𝑠𝑘+1
0(𝑘∈ [𝑛]),
which is the solution of 𝑦𝑘(𝝍⊤
𝑘𝜷+𝛽0)=1, and𝛽𝑠0
0=−∞and
𝛽𝑒𝑛
0=∞. Under the assumption of 𝛽0∈𝐵(𝑘), the optimal 𝛽0
isˆ𝛽(𝑘)
0=Í
𝑖∈I(𝑘)(𝑦𝑖−𝝍⊤
𝑖𝜷)
|I(𝑘)|. Since (12) is convex with respect to
𝛽0, the obtained ˆ𝛽(𝑘)
0must be the optimal solution if it satisfies
ˆ𝛽(𝑘)
0∈𝐵(𝑘). Thus, the optimal 𝛽0can be found by calculating ˆ𝛽(𝑘)
0
for all𝑘∈{0,...,𝑛}.
B PROOF OF THEOREM 2.1
From the definition of AGIS, the following monotonicity property
is guaranteed:
𝐿(𝐻′)⊒𝐿(𝐻)and𝐻,𝐻′∈W ⇒𝜓(𝐺𝑖;𝐻)≥𝜓(𝐺𝑖;𝐻′)
Let𝑀(𝐺𝑖;𝐻)be the set of injections 𝑀between𝐺𝑖and𝐻. The
above monotonicity property can be easily verified from the fact
min
𝑚∈𝑀(𝐺𝑖;𝐻)𝐷(𝑚)
𝐻,𝐺𝑖≤ min
𝑚∈𝑀(𝐺𝑖;𝐻′)𝐷(𝑚)
𝐻′,𝐺𝑖.Define(
𝑎𝑖=𝑦𝑖(1−𝑦𝑖(𝝍⊤
𝑖𝜷+𝛽0))if𝑖∈I,
𝑎𝑖=0 otherwise .
Note that the sign of 𝑎𝑖is same as𝑦𝑖. Using𝑎𝑖, we re-write 𝑔𝐻′(𝜷)
as
𝑔𝐻′(𝜷)=∑︁
𝑖∈I𝑎𝑖𝜓(𝐺𝑖;𝐻′)
=∑︁
𝑖∈I∩{𝑖|𝑦𝑖>0}𝑎𝑖𝜓(𝐺𝑖;𝐻′)+∑︁
𝑖∈I∩{𝑖|𝑦𝑖<0}𝑎𝑖𝜓(𝐺𝑖;𝐻′).
From the monotonicity inequality 𝜓(𝐺𝑖;𝐻)≥𝜓(𝐺𝑖;𝐻′), we see∑︁
𝑖∈I∩{𝑖|𝑦𝑖<0}𝑎𝑖𝜓(𝐺𝑖;𝐻)≤∑︁
𝑖∈I∩{𝑖|𝑦𝑖<0}𝑎𝑖𝜓(𝐺𝑖;𝐻′)≤𝑔𝐻′(𝜷)
≤∑︁
𝑖∈I∩{𝑖|𝑦𝑖>0}𝑎𝑖𝜓(𝐺𝑖;𝐻′)≤∑︁
𝑖∈I∩{𝑖|𝑦𝑖>0}𝑎𝑖𝜓(𝐺𝑖;𝐻).
From these inequalities, we obtain
|𝑔𝐻′(𝜷)|≤ max 
∑︁
𝑖∈I∩{𝑖|𝑦𝑖>0}𝑎𝑖𝜓(𝐺𝑖;𝐻),−∑︁
𝑖∈I∩{𝑖|𝑦𝑖<0}𝑎𝑖𝜓(𝐺𝑖;𝐻) 
.
C BOUNDS FOR LOGISTIC LOSS
The logistic loss function for the multi-class label 𝑦𝑖∈{1,...,𝐾}is
ℓ(𝑦𝑖,𝑓(𝐺𝑖))=−𝐾∑︁
𝑘=1I(𝑦𝑖=𝑘)logexp
𝝍⊤
𝑖𝜷(𝑘)+𝛽(𝑘)
0
Í𝐾
𝑘′=1exp
𝝍⊤
𝑖𝜷(𝑘′)+𝛽(𝑘′)
0
=−
𝝍⊤
𝑖𝜷(𝑦𝑖)+𝛽(𝑦𝑖)
0
+𝐾∑︁
𝑘=1exp
𝝍⊤
𝑖𝜷(𝑘)+𝛽(𝑘)
0
,
where 𝜷(𝑘)and𝛽(𝑘)
0are parameters for the 𝑘-th class. The deriva-
tive ofℓ(𝑦𝑖,𝑓(𝐺𝑖))with respect to 𝛽(𝑘)
𝐻is
𝑔(𝑘)
𝐻({𝜷(𝑘)}𝐾
𝑘=1)
=−𝑛∑︁
𝑖=1I(𝑦𝑖=𝑘)𝜓(𝐺𝑖;𝐻)+𝑛∑︁
𝑖=1exp
𝝍⊤
𝑖𝜷(𝑦𝑖)+𝛽(𝑦𝑖)
0
𝜓(𝐺𝑖;𝐻).
=𝑛∑︁
𝑖=1𝑎−
𝑖𝜓(𝐺𝑖;𝐻)+𝑛∑︁
𝑖=1𝑎+
𝑖𝜓(𝐺𝑖;𝐻),
where𝑎−
𝑖=−I(𝑦𝑖=𝑘)≤0and𝑎+
𝑖=exp
𝝍⊤
𝑖𝜷(𝑦𝑖)+𝛽(𝑦𝑖)
0
>0
are constants for any 𝐻. Then, the upper bound of |𝑔(𝑘)
𝐻′({𝜷(𝑘)}𝐾
𝑘=1)|
for any𝐿(𝐻′)⊒𝐿(𝐻)can be obtained by the same derivation as
appendix B
D DERIVATION OF 𝜆max
When 𝜷=0, the objective function of 𝛽0is written as the following
piecewise quadratic function:
min
𝛽01
2∑︁
𝑖∈I(𝛽0)(1−𝑦𝑖𝛽0)2s.t.I(𝛽0)= 
{𝑖|𝑦𝑖>0}𝛽0≤−1,
[𝑛]𝛽0∈[−1,1],
{𝑖|𝑦𝑖<0}𝛽0≥1.
In the region 𝛽0≤−1, the minimum value is achieved by 𝛽0=−1,
and for𝛽0≥1, the minimum value is achieved by 𝛽0=1. This
 
2839Learning Attributed Graphlets:
Predictive Graph Mining by Graphlets with Trainable Attribute KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Statistics of datasets.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC
#
instances 2000 405 467 756 200 1113 300
Dim.
of attribute vector 𝑑 4 3 3 3 18 1 1
A
vg. # nodes 15.69 35.75 41.22 42.43 32.58 39.06 100.00
A
vg. # edges 16.20 38.36 43.45 44.54 60.78 72.82 196.00
0
0
0
00
0
H:0.211026
NodeID:0
NodeID:1
NodeID:2
NodeID:3NodeID:4
NodeID:51
01NodeID:0
1
01NodeID:1
1
01NodeID:2
1
01NodeID:3
01231
01NodeID:401231
01NodeID:5
(a)
000101
H:0.186228
NodeID:0NodeID:1NodeID:2NodeID:3NodeID:4NodeID:5
0.5
0.00.5NodeID:0
0.5
0.00.5NodeID:1
0.5
0.00.5NodeID:2
0.5
0.00.5NodeID:3
01230.5
0.00.5NodeID:401230.5
0.00.5NodeID:5 (b)
0
0
0
0
0
H:-2.063934
NodeID:0
NodeID:1
NodeID:2
NodeID:3
NodeID:42.5
0.02.5NodeID:0
2.5
0.02.5NodeID:1
2.5
0.02.5NodeID:2
2.5
0.02.5NodeID:3
01232.5
0.02.5NodeID:4 (c)
000
0
00
H:-1.234134
NodeID:0NodeID:1
NodeID:2
NodeID:3
NodeID:4NodeID:52.5
0.02.5NodeID:0
2.5
0.02.5NodeID:1
2.5
0.02.5NodeID:2
2.5
0.02.5NodeID:3
01232.5
0.02.5NodeID:401232.5
0.02.5NodeID:5 (d)
Figure 11: Selected important AGs for AIDS dataset. (a) and (b): AGs for the two largest positive coefficients. (c) and (d): AGs for
the two largest negative coefficients.
5
 4
 3
 2
 1
 0 1
f(G_i)8
6
4
2
024LDA (in {S})
(a) AIDS Training data
4
 3
 2
 1
 0 1
f(G_i)6
4
2
02LDA (in {S})
 (b) AIDS Test data
2
 1
 0 1
f(G_i)2
024LDA (in {S})
 (c) COX2 Training data
2
 1
 0 1
f(G_i)2
0246LDA (in {S})
 (d) COX2 Test data
1
 0 1
f(G_i)4
3
2
1
012LDA (in {S})
(e) DHFR Training data
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
f(G_i)3
2
1
0123LDA (in {S})
 (f) DHFR Test data
1
 0 1 2
f(G_i)4
3
2
1
012LDA (in {S})
 (g) ENZYMES Training data
1
 0 1 2
f(G_i)1.5
1.0
0.5
0.00.51.01.52.0LDA (in {S})
 (h) ENZYMES Test data
0.5
 0.0 0.5 1.0
f(G_i)2
1
012LDA (in {S})
(i) PROTEINS Training data
0.5
 0.0 0.5 1.0
f(G_i)2
1
0123LDA (in {S})
 (j) PROTEINS Test data
4
 2
 0 2 4
f(G_i)5.0
2.5
0.02.55.07.5LDA (in {S})
 (k) SYNTHETIC Training data
4
 2
 0 2 4
f(G_i)7.5
5.0
2.5
0.02.55.07.5LDA (in {S})
 (l) SYNTHETIC Test data
Figure 12: Scatter plots by 𝑓(𝐺𝑖)and LDA.
 
2840KDD ’24, August 25–29, 2024, Barcelona, Spain Shinji Tajima, Ren Sugihara, Ryota Kitahara, and Masayuki Karasuyama
Table 3: Training time (sec) of GNNs and LAGRA. The hyper-parameter setting of each method is the best setting selected by
the their validation performance. Note that precise comparisons of running times have difficulty due to the following two
reasons. 1) For LAGRA, the computational time for Algorithm 1 is shown, which contains the sum of the computational times
for all values of 𝜆(i.e., all the times shown in Fig 9 in the main text). 2) The implementation of GNNs are by Python, while
LAGRA is implemented by C++.
AIDS BZR COX2 DHFR ENZYMES PRO
TEINS SYN
THETIC
DGCNN 7.63e+02 3.02e+02 3.52e+02 5.13e+02 9.23e+01 4.51e+02 2.12e+02
GA
T 2.48e+02 7.95e+01 1.11e+02 1.76e+02 5.02e+01 2.60e+02 1.83e+02
GCN 1.70e+02 6.62e+01 6.98e+01 1.23e+02 3.31e+01 1.94e+02 1.41e+02
GIN 1.96e+02 2.44e+02 4.40e+02 3.08e+02 2.60e+02 5.53e+02 5.14e+02
LA
GRA 8.58e+04 1.59e+04 5.51e+03 4.45e+04 1.72e+04 4.67e+05 1.08e+04
0 20 40 60 80 100
 index
101102103104105
Mean |W|
#visit nodes
# candidate H
(
a) BZR
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H (
b) COX2
0 20 40 60 80 100
 index
100101102103104105
Mean |W|
#visit nodes
# candidate H (
c) DHFR
Figure 13: The average number of visited tree nodes and |W|.
indicates that the optimal solution should exist in 𝛽0∈ [− 1,1]
because the objective function is a smooth convex function. There-
fore, the minimum value in the region 𝛽0∈[− 1,1]achieved by
𝛽0=Í
𝑖∈[𝑛]𝑦𝑖/𝑛, defined as ¯𝑦, becomes the optimal solution.
When 𝜷=0and𝛽0=¯𝑦, we obtain
𝑔𝐻(0)=∑︁
𝑖∈[𝑛]𝑦𝑖𝜓(𝐺𝑖;𝐻)(1−𝑦𝑖¯𝑦).
From (9), we see that |𝑔𝐻(0)|=𝜆is the threshold that 𝛽𝐻have a
non-zero value. This means that 𝐻having the maximum |𝑔𝐻(0)|is
the first𝐻that start having a non-zero value by decreasing 𝜆from
∞. Therefore, we obtain
𝜆max=max
𝐻∈H∑︁
𝑖∈[𝑛]𝑦𝑖𝜓(𝐺𝑖;𝐻)(1−𝑦𝑖¯𝑦).
E STATISTICS OF DATASETS
Statistics of datasets is show in Table 2. AIDS, BZR, COX2, and
DHFR are molecule graph datasets. The attributes of the AIDS
dataset are chem ,charge ,x, and y(details are not shown in the data
repository [ 12]). The BZR, COX2, and DHFR datasets contain 3D
coordinate attributes (Although we used 3D coordinate attributes
directly, using rotation invariant representations might be more
appropriate for these coordinate information. It is possible to apply
LAGRA to rotation invariant 3D descriptors such as the atomic
distance distribution around each atom node). In the ENZYMES
and PROTEINS datasets, the nodes are secondary structure ele-
ments, and attributes contain physical and chemical measurements.
In the SYNTHETIC dataset, attributes are created from normal
distributions. See e.g., [14] for more detail.F ADDITIONAL EVALUATION OF
COMPUTATIONAL TIME
Table 3 shows computational time of GNNs and LAGRA. As de-
scribed in the second last paragraph of Sec 4.3, we do not claim
computational efficiency compared with other methods that do not
enumerate all subgraphs. Comparing with the naive enumeration
without our gradient pruning, LAGRA is highly efficient as shown
in the last paragraph of Sec 4.3. Figure 13 is additional evaluations
of the number of visited nodes shown in Fig. 10
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
 
2841