Generative AI Day
Jie Tang
jietang@tsinghua.edu.cn
Tsinghua University
Beijing, ChinaYuxiao Dong
yuxiaod@tsinghua.edu.cn
Tsinghua University
Beijing, ChinaMichalis Vazirgiannis
michalis.vazirgiannis@polytechnique.edu
Ecole Polytechnique
Paris, France
Mohamed bin Zayed University of
Artificial Intelligence
Abu Dhabi, United Arab Emirates
ABSTRACT
The Generative AI (AIGC) Day at KDD’24 is a dedicated full-day
event for generative AI at KDD. This is an opportunity to bring to-
gether researchers, practitioners, and startups to share the insights
about the cutting-edge advancements and to discuss the potential
societal impacts of LLMs and AIGC. It is exciting that this year,
we have invited speakers from both industry (e.g., Amazon, Zhipu
AI) and academia (e.g., USC, UCLA). The topics cover various per-
spectives of generative AI including foundation models, streaming
LLMs, LLM training and inference. As demonstrated [ 1–4], data
plays a crucial role in developing cutting-edge generative AI mod-
els. For example, the Gemini Team has found that “data quality is
an important factor for highly-performing models...” [ 3]. To date,
there is still significant room to define design principles and develop
methods for improved data collection, selection, and synthetic data
generation for the pre-training and alignment of language, vision,
and multi-modal models. Therefore, the Day will invite the speakers
and KDD audience to discuss the challenges and opportunities for
data mining researchers in the era of generative AI.
CCS CONCEPTS
•Computing methodologies →Natural language generation.
KEYWORDS
Large Language Model, ChatGLM, Generative AI, Foundation Model
ACM Reference Format:
Jie Tang, Yuxiao Dong, and Michalis Vazirgiannis. 2024. Generative AI Day.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 2 pages. https://doi.org/10.1145/3637528.3673872
1 INTRODUCTION
Generative AI models, such as large language models (LLMs) and
text-to-image/video models, have revolutionized the field of AI
with their remarkable capabilities in natural language and visual
understanding and generation. LLMs such as ChatGPT are widely
used in a variety of applications, including question answering,
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3673872personal assistants, and recommender systems. Similarly, text-to-
image models like Dall-E and text-to-video models like Sora are
transforming visual content creation, enabling the generation of
high-quality visuals and videos from textual descriptions.
Despite their advanced capabilities, these powerful models also
present significant challenges for their safe and ethical deployment.
Issues such as algorithmic bias, privacy breaches, explainability, and
lack of transparency can undermine trust and limit their usefulness.
Therefore, the Generative AI Day will discuss the open issues and
challenges associated with building trustworthy AI models. By
focusing on these concerns, we aim to brainstorm and build robust
frameworks to ensure that advanced AI models are developed and
deployed in a responsible manner.
As demonstrated in frontier LLMs [ 1–4], “data quality is an
important factor for highly-performing models...” [ 3] and “data
quality and diversity are crucial for building effective LLMs” [ 2].
These highlight the important role that data mining research plays
in the building of powerful models. To date, there exist numerous
opportunities to develop fundamental principles and algorithms
to fully understand and mine the data and its connection with
the “intelligence” level of the AI models trained on them. These
processes are critical for the effective pre-training and alignment of
language, vision, and multi-modal models. The challenges include
not only gathering large volumes of data but also ensuring the data
is representative, unbiased, and of high fidelity and quality.
To this end, the Generative AI Day will invite both AI/LLM and
data mining speakers as well as the KDD audience to discuss the
challenges and opportunities that data mining researchers face in
the era of generative AI. The goal is to explore advanced data strate-
gies and directions that could drive the next round of innovations in
building advanced AI models. It will target at providing a platform
for academic researchers and industry practitioners to discuss the
latest advances and open problems in the field of generative AI.
The program of the Generative AI Day will be shared and updated
at https://bigmodel.ai/aigc-kdd24/.
1.1 Outline
During the Generative AI Day, we are excited to host a diverse
range of renowned invited speakers from world-class research and
development institutions. These experts will share their knowledge
and perspectives, fostering interdisciplinary collaboration and pro-
moting the exchange of ideas among participants. Below includes
a brief introduction to the talks:
•AWS Trainium: Experience with using Amazon’s accel-
erator for LLM training and inference. In his talk, George
will share their experience leveraging AWS Trainium, Amazon’s
6699
KDD ’24, August 25–29, 2024, Barcelona, Spain Jie Tang, Yuxiao Dong, and Michalis Vazirgiannis
custom-built AI accelerator, to efficiently train and perform infer-
ence on large language models (LLMs). He will discuss the key
design principles and optimizations of AWS Trainium that enable
it to handle the massive computational demands of LLMs. He
will explore the distributed training solution and software stack
tailored for maximizing performance and scalability on AWS
Trainium. He will also delve into optimization techniques using
AWS Trainium to address the challenges of deploying LLMs in
real-world scenarios. Throughout the talk, George will share
lessons learned during the development of AWS ML products
and present real-world case studies showcasing the impact of
AWS Trainium on LLM training and inference.
•Challenges and Opportunities for Data Miners in the Era
of Generative AI. In this talk, Yizhou will use Material Science
and Chip Design as two application examples to demonstrate
how to harness the power of generative AIs to advance these
domains. Specifically, the talk will include (1) the need for new
benchmarking; (2) the cross modality and exploration nature of
these applications; and (3) the methodologies that are showing
promising results.
•ChatGLM: A Family of Large Models. Xiaotao will introduce
ChatGLM, an evolving family of large models developed at Zhipu
AI. The family includes language, multi-modal, and agent mod-
els. He will share the insights and lessons gained from the four
generations of ChatGLM. To date, the GLM-4 models are pre-
trained on ten trillion tokens mostly in Chinese and English,
along with a small set of corpus from 24 languages, and aligned
primarily for Chinese and English usage. The CogVLM, CogView,
and CogAgent models have shown competitive results in image
understanding, generation, and vision-based agent capabilities.
•Frontiers of Foundation Models for Time Series. In this talk,
Yan will discuss possible paths to foundation models for time
series data and future directions for time series research.
2 SPEAKERS’ BIO
Below lists the speakers’ brief bio:
George Karypis is a Distinguished McKnight University Professor
and William Norris Chair in Large Scale Computing at the De-
partment of Computer Science & Engineering at the University
of Minnesota and a Senior Principal Scientist at AWS AI/ML. His
research interests span the areas of machine learning, data mining,
high performance computing, collaborative filtering, bioinformat-
ics, cheminformatics, and scientific computing. He has coauthored
over 350 papers on these topics and two books: “Introduction to Pro-
tein Structure Prediction: Methods and Algorithms” (Wiley, 2010)
and “Introduction to Parallel Computing” (Addison Wesley, 2003,
2nd edition). He is serving on the program committees of many
conferences and workshops on these topics, and on the editorial
boards of several journals. He is a Fellow of the IEEE.
Yizhou Sun is a full professor at department of computer science
of UCLA. She received her Ph.D. in Computer Science from the
University of Illinois at Urbana-Champaign in 2012. Her principal
research interest is on mining graphs/networks, and more generally
in data mining, machine learning, and network science, with a focus
on modeling novel problems and proposing scalable algorithms forlarge-scale, real-world applications. She is a pioneer researcher in
mining heterogeneous information network, with a recent focus
on deep learning on graphs and neural symbolic reasoning. Yizhou
has over 180 publications in books, journals, and major conferences.
Tutorials of her research have been given in many premier con-
ferences. She is a recipient of multiple Best Paper Awards, ACM
SIGKDD Doctoral Dissertation Award, Yahoo ACE (Academic Ca-
reer Enhancement) Award, NSF CAREER Award, CS@ILLINOIS
Distinguished Educator Award, Amazon Research Awards (twice),
Okawa Foundation Research Award, VLDB Test of Time Award,
WSDM Test of Time Award, ACM Distinguished Member, IEEE AI’s
10 to Watch, and SDM/IBM faculty award. She is a general co-chair
of SIGKDD 2023, PC co-chair of ICLR 2024, and PC co-chair of
SIGKDD 2025.
Xiaotao Gu is currently a researcher in Zhipu AI. He received his
Ph.D. degree from University of Illinois at Urbana-Champaign. He
was a member of the Knowledge Engineering Group at Tsinghua
University. His research interests lie primarily in massive text data
mining and large language models. Before joining Zhipu, he worked
as a research scientist in Huawei. He participated in the develop-
ment of several systems at Google, ranging from relation extraction,
news story headline generation, and the acceleration of language
model pre-training.
Yan Liu is a professor in the Computer Science Department and
the Director of the Machine Learning Center at the University of
Southern California. She received her Ph.D. degree from Carnegie
Mellon University. Her research interest is machine learning and
its applications to climate science, health care, and sustainability.
She has received several awards, including NSF CAREER Award,
Okawa Foundation Research Award, New Voices of Academies of
Science, Engineering, and Medicine, Best Paper Award in SIAM
Data Mining Conference. She serves as general chair for KDD 2020
and ICLR 2023, and program chairs for WSDM 2018, SDM 2020,
KDD 2022 and ICLR 2022.
REFERENCES
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
arXiv:2303.08774
[2]Team GLM. 2024. ChatGLM: A Family of Large Language Models from GLM-130B
to GLM-4 All Tools. arXiv:2406.12793
[3]Gemini Team Google. 2023. Gemini: a family of highly capable multimodal models.
arXiv preprint arXiv:2312.11805 (2023). arXiv:2312.11805
[4]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023). arXiv:2302.13971
6700