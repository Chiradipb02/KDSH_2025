Graph Reasoning with LLMs (GReaL)
Anton Tsitsulin
tsitsulin@google.com
Google Research
New York, NY, USABryan Perozzi
bperozzi@acm.org
Google Research
Princeton, NJ, USA
Bahare Fatemi
baharef@google.com
Google Research
Montreal, QC, CanadaJonathan J. Halcrow
halcrow@google.com
Google Research
Atlanta, GA, USA
ABSTRACT
Graphs are a powerful tool for representing and analyzing complex
relationships in real-world applications. Large Language Models
(LLMs) have demonstrated impressive capabilities by advancing
state-of-the-art on many language-based benchmarks. Their ability
to process and understand natural language open exciting possi-
bilities in various domains. Despite the remarkable progress in
automated reasoning with natural text, reasoning on graphs with
LLMs remains an understudied problem that has recently gained
more attention.
This tutorial builds upon recent advances in expressing reasoning
problems through the lens of tasks on graph data. The first part
of the tutorial will provide an in-depth discussion of techniques
for representing graphs as inputs to LLMs. The second, hands-on,
portion will demonstrate these techniques in a practical setting. As
a learning outcome of participating in the tutorial, participants will
be able to analyze graphs either on free-tier Colab or their local
machines with the help of LLMs.
KEYWORDS
large language models, graphs, graph neural networks
ACM Reference Format:
Anton Tsitsulin, Bryan Perozzi, Bahare Fatemi, and Jonathan J. Halcrow.
2024. Graph Reasoning with LLMs (GReaL). In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 2 pages.
https://doi.org/10.1145/3637528.3671448
1 TUTORS
Anton Tsitsulin ,Google Research. He received his Ph.D. from the
University of Bonn, Germany. His research focuses on devising Web-
scale algorithms for network representation learning. His research
has been published in major AI conferences, such as NeurIPS, ICLR,
KDD, WWW, and VLDB.
Bryan Perozzi ,Google Research. Bryan has been working on graph-
based machine learning for over 10 years and has 17,000+ citations
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671448in the area. Bryan completed his Ph.D. at Stony Brook University,
and his thesis [ 13,14] won the 2017 ACM SIGKDD award for best
dissertation. Bryan started the Graph Neural Network group [ 10]
inside Google Research and has worked on over 100 applications
of graph-based learning at the company.
Bahare Fatemi is a Research Scientist at Google Research in Mon-
treal, specializing in graph representation learning and natural
language processing. She received her Ph.D. From the University of
British Columbia. Her work has been features in top AI conferences
and journals including NeurIPS, ICLR, AAAI, and JMLR.
Jonathan Halcrow is a senior staff software engineer with Google
Research and has been at Google for over a decade. Jonathan fin-
ished his Ph.D. at Georgia Tech in Physics in 2008. His work on
graph-based ML underlies numerous systems at Google and has
been published at major venues including KDD, NeurIPS, and ICLR.
2 LEARNING GOALS
By the end of the tutorial, the audience is expected to understand
the challenges and opportunities in graph reasoning with LLMs,
including: 1) Identifying reasoning limitations of LLMs on graph
data. 2) Familiarization with specialized techniques and benchmarks
to evaluate and enhance graph reasoning capabilities of LLMs. In
terms of hands-on skills, the audience will master techniques for
effective graph encoding and reasoning with LLMs. The audience
will acquire knowledge about parameter-efficient graph encoders,
such as GraphToken [ 15], and their integration with LLMs. Interac-
tive part of the tutorial will provide participants with experience
in applying graph reasoning techniques to real-world datasets and
problems.
Target Audience and Prerequisites. This tutorial is targeted at
novice as well as moderately skilled researchers and industry prac-
titioners interested in reasoning on/with graphs. Listeners will be
assumed to have basic familiarity with the Python programming
language as well as basic knowledge of neural-network-based ma-
chine learning. We will introduce basic concepts in both graph
machine learning and large language models that are needed for
the purposes of the tutorial.
3 TUTORIAL SUMMARY
The field of graph machine learning has developed many methods
(e.g. graph neural networks [ 3], graph clustering [ 19], etc) to use
graph structure to improve model performance [ 1,2,6,7,11,12,16,
17,21–23]. Due to the increasing interest in LLMs, the integration
6424
KDD ’24, August 25–29, 2024, Barcelona, Spain Anton Tsitsulin, Bryan Perozzi, Bahare Fatemi, and Jonathan J. Halcrow
of structured data with LLMs is gaining more and more attention
[4]. In this tutorial, we aim to provide an overview of recent work
on graph reasoning in LLMs, and provide hands-on experience
using them for graph reasoning tasks. We structure our tutorial in
two parts: first, evaluation of graph reasoning in LLMs [ 8,9] and
second on parameter-efficient tuning [ 15] to add graph capabilities
to a static model.
3.1 Part 1: Graph Reasoning in LLMs
Graph reasoning tasks are of significant importance due to the
prevalence of graph-structured data in various real-world applica-
tions [ 5]. However, representing and reasoning over graphs using
LLMs poses unique challenges. LLMs are primarily designed to
process sequential data, making it difficult to capture complex rela-
tionships and structural information present in graphs.
To address this issue and facilitate the evaluation of LLMs’ graph
reasoning capabilities, we introduce the GraphQA benchmark [8],
a comprehensive set of tasks specifically designed to assess the
performance of LLMs on graph-related problems. One of the key
factors influencing LLM performance on graph reasoning tasks
is the choice of graph encoding method. Different encoding tech-
niques, such as adjacency lists, or incident lists, can significantly
impact the ability of LLMs to understand and reason over graph-
structured data. This tutorial will explore various node and edge
encoding strategies and discuss their effects on LLM performance.
In the hands-on part of the tutorial, we will let the audience
develop their own graph encoding method and evaluate it on a
subset of the GraphQA benchmark.
3.2 Part 2: Learning Graph Reasoning
We will next focus on learning to further enhance the graph rea-
soning capabilities of LLMs [ 18]. The goal of parameter-efficient
fine-tuning (PEFT) [ 20] is to adapt models to new tasks by updat-
ing only a small number of parameters. We will cover dominant
PEFT approaches and then introduce the architecture of GraphTo-
ken [ 15], a parameter-efficient graph encoder specifically designed
for LLMs. In GraphToken, the graph encoder takes a graph as input
and generates a set of continuous embeddings that capture the
graph’s structure and node features. These embeddings are then
appended to the text input of the LLM, effectively providing the
model with graph-specific information. The training procedure for
GraphToken involves optimizing the graph encoder’s parameters
while keeping the LLM fixed, making it a parameter-efficient ap-
proach. The objective function is designed to maximize prediction
on graph reasoning tasks.
We inspect GraphToken’s performance on the GraphQA bench-
mark and compare it with prompt engineering baselines introduced
earlier in the tutorial. Moreover, we analyze the impact of different
graph convolution architectures and node feature representations
on GraphToken’s performance, providing insights into the design
choices for graph encoding optimal for graph reasoning tasks. Gen-
eralization is a key challenge for graph reasoning applications, as it
is imperative for such systems to generalize for different problem
structures. The tutorial will emphasize generalization aspects of
graph reasoning by exploring GraphToken’s generalization capa-
bilities on unseen tasks and graph structures.REFERENCES
[1]Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina
Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:
Higher-order graph convolutional architectures via sparsified neighborhood
mixing. In international conference on machine learning. PMLR, 21–29.
[2]Kaidi Cao, Mangpo Phothilimthana, Sami Abu-El-Haija, Dustin Zelle, Yanqi Zhou,
Charith Mendis, Jure Leskovec, and Bryan Perozzi. 2024. Learning large graph
property prediction via graph segment training. Advances in Neural Information
Processing Systems 36 (2024).
[3]Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, and Kevin Mur-
phy. 2022. Machine learning on graphs: A model and comprehensive taxonomy.
Journal of Machine Learning Research 23, 89 (2022), 1–64.
[4]Zhikai Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare
Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, et al .2024. Text-space Graph
Foundation Models: Comprehensive Benchmarks and New Insights. arXiv
preprint arXiv:2406.10727 (2024).
[5]Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, and Anton Tsitsulin. 2024.
Don’t Forget to Connect! Improving RAG with Graph-based Reranking. arXiv
preprint arXiv:2405.18414 (2024).
[6]Bahare Fatemi, Sami Abu-El-Haija, Anton Tsitsulin, Mehran Kazemi, Dustin
Zelle, Neslihan Bulut, Jonathan Halcrow, and Bryan Perozzi. 2023. Ugsl: A
unified framework for benchmarking graph structure learning. arXiv preprint
arXiv:2308.10737 (2023).
[7]Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. 2021. Slaps: Self-
supervision improves structure learning for graph neural networks. Advances in
Neural Information Processing Systems 34 (2021), 22667–22681.
[8]Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2024. Talk like a graph:
Encoding graphs for large language models. In ICLR.
[9]Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong
Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi. 2024.
Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning. arXiv
preprint arXiv:2406.09170 (2024).
[10] Oleksandr Ferludin, Arno Eigenwillig, Martin Blais, Dustin Zelle, Jan Pfeifer,
Alvaro Sanchez-Gonzalez, Sibon Li, Sami Abu-El-Haija, Peter Battaglia, Neslihan
Bulut, et al .2022. TF-GNN: graph neural networks in TensorFlow. arXiv preprint
arXiv:2207.03522 (2022).
[11] Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan Perozzi. 2020. Grale:
Designing networks for graph learning. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining. 2523–2532.
[12] John Palowitch and Bryan Perozzi. 2020. Debiasing graph representations via
metadata-orthogonal training. In 2020 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 435–442.
[13] Bryan Perozzi. 2016. Local Modeling of Attributed Graphs: Algorithms and Appli-
cations. Ph. D. Dissertation. State University of New York at Stony Brook.
[14] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learn-
ing of Social Representations. In ACM SIGKDD international conference on Knowl-
edge discovery and data mining (KDD) .
[15] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi,
Rami Al-Rfou, and Jonathan Halcrow. 2024. Let Your Graph Do the Talking:
Encoding Structured Data for LLMs. arXiv preprint arXiv:2402.05862 (2024).
[16] Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Michael
Burrows, Charith Mendis, and Bryan Perozzi. 2024. TpuGraphs: A Performance
Prediction Dataset on Large Tensor Computational Graphs. Advances in Neural
Information Processing Systems 36 (2024).
[17] Benedek Rozemberczki, Peter Englert, Amol Kapoor, Martin Blais, and Bryan
Perozzi. 2021. Pathfinder discovery networks for neural message passing. In
Proceedings of the Web Conference 2021. 2547–2558.
[18] Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi,
Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. 2024. Understand-
ing Transformer Reasoning Capabilities via Graph Algorithms. arXiv preprint
arXiv:2405.18512 (2024).
[19] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Müller. 2023.
Graph clustering with graph neural networks. Journal of Machine Learning
Research 24, 127 (2023), 1–21.
[20] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. 2023.
Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A
Critical Review and Assessment. arXiv preprint arXiv:2312.12148 (2023).
[21] Minji Yoon, John Palowitch, Dustin Zelle, Ziniu Hu, Ruslan Salakhutdinov, and
Bryan Perozzi. 2022. Zero-shot transfer learning within a heterogeneous graph
via knowledge transfer networks. Advances in Neural Information Processing
Systems 35 (2022), 27347–27359.
[22] Minji Yoon, Yue Wu, John Palowitch, Bryan Perozzi, and Ruslan Salakhutdinov.
2022. Graph generative model for benchmarking graph neural networks. arXiv
preprint arXiv:2207.04396 (2022).
[23] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. 2021. Shift-robust
gnns: Overcoming the limitations of localized graph training data. Advances in
Neural Information Processing Systems 34 (2021), 27965–27977.
6425