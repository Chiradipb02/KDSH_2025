Personalized Federated Continual Learning
via Multi-granularity Prompt
Hao Yu
School of Computing and Artificial
Intelligence, Southwestern University
of Finance and Economics
Chengdu, China
yuhao2033@163.comXin Yang∗
School of Computing and Artificial
Intelligence, Southwestern University
of Finance and Economics
Chengdu, China
yangxin@swufe.edu.cnXin Gao
School of Computing and Artificial
Intelligence, Southwestern University
of Finance and Economics
Chengdu, China
xingaocs@hotmail.com
Yan Kang
Webank
Shenzhen, China
kangyan2003@gmail.comHao Wang
College of Computer Science,
Sichuan University
Chengdu, China
cshaowang@gmail.comJunbo Zhang
JD Intelligent Cities Research
JD iCity, JD Technology
Beijing, China
msjunbozhang@outlook.com
Tianrui Li
School of Computing and Artificial
Intelligence, Southwest Jiaotong
University
Chengdu, China
trli@swjtu.edu.cn
ABSTRACT
Personalized Federated Continual Learning (PFCL) is a new prac-
tical scenario that poses greater challenges in sharing and per-
sonalizing knowledge. PFCL not only relies on knowledge fusion
for server aggregation at the global spatial-temporal perspective
but also needs model improvement for each client according to
the local requirements. Existing methods, whether in Personalized
Federated Learning (PFL) or Federated Continual Learning (FCL),
have overlooked the multi-granularity representation of knowl-
edge, which can be utilized to overcome Spatial-Temporal Cata-
strophic Forgetting (STCF) and adopt generalized knowledge to
itself by coarse-to-fine human cognitive mechanisms. Moreover,
it allows more effectively to personalized shared knowledge, thus
serving its own purpose. To this end, we propose a novel concept
called multi-granularity prompt, i.e., coarse-grained global prompt
acquired through the common model learning process, and fine-
grained local prompt used to personalize the generalized represen-
tation. The former focuses on efficiently transferring shared global
knowledge without spatial forgetting, and the latter emphasizes
specific learning of personalized local knowledge to overcome tem-
poral forgetting. In addition, we design a selective prompt fusion
∗Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671948mechanism for aggregating knowledge of global prompts distilled
from different clients. By the exclusive fusion of coarse-grained
knowledge, we achieve the transmission and refinement of com-
mon knowledge among clients, further enhancing the performance
of personalization. Extensive experiments demonstrate the effec-
tiveness of the proposed method in addressing STCF as well as
improving personalized performance. Our code now is available at
https://github.com/SkyOfBeginning/FedMGP.
CCS CONCEPTS
•Computing methodologies →Distributed algorithms .
KEYWORDS
Federated Continual Learning; Personalized FL; Multi-granularity
Prompt; Spatial-Temporal Catastrophic Forgetting
ACM Reference Format:
Hao Yu, Xin Yang, Xin Gao, Yan Kang, Hao Wang, Junbo Zhang, and Tianrui
Li. 2024. Personalized Federated Continual Learning
via Multi-granularity Prompt. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671948
1 INTRODUCTION
Federated Continual Learning (FCL) is a new practical paradigm
aiming at fusing knowledge from different times and spaces without
catastrophic forgetting in dynamic Federated Learning (FL) settings
[47]. Moreover, Personalized Federated Learning (PFL) tries to fuse
implicit common knowledge extracted from various clients and
personalize the generalized knowledge for better performance on
the client side [ 37]. However, to better accommodate diverse local
4023
KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
requirements in highly heterogeneous FCL scenarios, personaliza-
tion solutions are necessary for leveraging knowledge fused from
different spatial and temporal perspectives. Therefore, Personalized
Federated Continual Learning (PFCL) is proposed as a combination
of PFL and FCL with broader application scenarios.
PFCL is more challenging than static PFL because of the higher
requirements for handling heterogeneous knowledge. On the one
hand, it implies accumulating knowledge against spatial-temporal
catastrophic forgetting (STCF), which is the main issue of FCL.
On the other hand, it also needs to achieve effective extraction
and fusion of client-specific and invariant knowledge, ensuring
local personalization post the integration of shared knowledge.
This is the primary goal of PFL. Both issues can be solved by the
multi-granularity representation of knowledge. Therefore, we can
effectively address these issues by constructing a multi-granularity
knowledge space, as illustrated in Fig. 1. Existing methods have not
taken this way into account.
Coarse
Fine
Coarse
Fine
Coarse
FineStage 1
Client 1
Client 2
Client 3
Task 1Coarse-Knowledge
Fusion
Task 1
Task 1
Coarse
Fine
Server
Fine
Coarse
FineGeneralized
Coarse-KnowledgeStage 2
Client 1
Client 2
Client 3
Task 2Task 2
Task 2
Fine-Knowledge
Keeps Local...
...
...Continual learning
Figure 1: An illustration of constructing a multi-granularity
knowledge space in PFCL. By dividing local knowledge into
coarse-grained knowledge and fine-grained knowledge, bet-
ter aggregation of common representations can be achieved
on the server side. On the local side, fine-grained knowledge
is used to personalize the generalized representation. The
two different levels of knowledge can accumulate over time.
The key to personalization lies in the accurate isolation of knowl-
edge, namely, turning local knowledge into client-invariant and
client-specific knowledge. By introducing a multi-granularity knowl-
edge space, it is easy to decompose knowledge into coarse-grained,
representing common aspects, and fine-grained, representing spe-
cific aspects, during the training stage to fulfill the requirements
of personalization. Moreover, on the server side, we consolidate
client-invariant knowledge while maintaining the distinctiveness
of client-specific knowledge with the exclusive fusion of coarse-
grained representations.
For STCF, the knowledge learned by the deep network is overly
fine-grained, making it highly susceptible to degradation in per-
formance, such as the weighted average of parameters, leading to
serious forgetting. In contrast, this multi-granularity knowledge
representation exhibits stronger robustness against forgetting. On
the one hand, using coarse-grained representation for temporal-
spatial invariant knowledge makes it easier to transfer and fuseknowledge across time and space. On the other hand, utilizing fine-
grained representation captures time-specific and space-specific
knowledge, thereby achieving better personalization.
Inspired by the human cognitive process [ 47], knowledge trans-
fer is based on shared cognition, such as a common language. In this
work, we construct a multi-granularity knowledge space by utiliz-
ing prompts of different granularity, namely coarse-grained global
prompts andfine-grained local prompts, with a pre-trained Vision
Transformer (ViT) [ 8]. Specifically, we employ the pre-trained ViT
as the shared public cognition. We train coarse-grained prompts
operating at the input without altering internal parameters to rep-
resent temporal-spatial invariant knowledge. Subsequently, lever-
aging the frozen coarse-grained prompts, we train class-wise fine-
grained prompts that directly interact with the multi-head self-
attention layer as temporal-spatial specific knowledge. This fine-
tuning process enhances the model’s ability to adapt to local tasks.
This coarse-to-fine cognitive approach also aligns with the hu-
man cognitive process, where attention is initially directed toward
outlines before focusing on details. Finally, we design a selective
prompt fusion mechanism on the server side. This novel prompt
fusion approach further mitigates spatial forgetting caused by aggre-
gation. The contributions of this paper are summarized as follows:
•We formally define a new personalized federated learning sce-
nario called PFCL, which imposes higher demands on knowledge
processing while preventing spatial-temporal forgetting. For the
first time, we construct a multi-granularity knowledge space in
this scenario, effectively addressing these challenges.
•We propose a novel method called Federated Multi-Granularity
Prompt (FedMGP), which introduces two distinct prompt levels
to represent coarse-grained and fine-grained knowledge, respec-
tively. It effectively overcomes STCF while meeting the require-
ments for personalization.
•Extensive experiments demonstrate that our method achieves
state-of-the-art performance in two different scenarios of feder-
ated continual learning. Moreover, our approach exhibits superior
performance in personalizing and retaining temporal and spatial
knowledge.
2 RELATED WORK
2.1 Multi-Granularity Computing
Multi-granularity computing addresses the challenge of tackling
the coexistence of data with different granularities [ 45,46]. Ex-
tracting multi-granularity knowledge benefits our understanding
of materials and their intrinsic properties.
VL-PET [ 13] designs a multi-granularity controlled mechanism
to impose control on modular modifications of the pre-trained
language model at coarse and fine granularities. [ 5] constructs a
question-answering dataset with yearly, monthly, and daily-grained
data and proposes MultiQA to address temporally multi-granularity
question-answering. [ 46] adopts the sequential three-way decision
method to extract knowledge of different granularities in open-
topic classification tasks. [ 42] decouples the objects of group re-
identification tasks into individual, subgroup, and entire group
granularities to handle the dynamic changes in group layout and
member variations. [ 32] introduces granular computing in FL and
achieves automatic neural architecture search to adapt the different
4024Personalized Federated Continual Learning via Multi-granularity Prompt KDD ’24, August 25–29, 2024, Barcelona, Spain
information granularity across clients. [ 28] achieves a fine-grained
knowledge fusion with layer-wised aggregation. PartialFed [ 35]
transfers cross-domain knowledge of adaptive granularity among
clients by automatically switching the learning strategy. [ 3] utilizes
bi-directional guidance with a prior attention mechanism to trans-
fer coarse-grained and fine-grained knowledge among multi-scale
local models in an extremely heterogeneous federated system. [ 26]
proposes a hierarchical FL framework to reduce the communication
overhead, conducting model aggregation at two granularities.
Currently, the core concept of multi-granularity cognition has
been gradually embraced by the general public and is progressively
being applied to scenarios involving spatial-temporal changes. For
the traffic accident predictions, given the dynamic nature of road
networks and expanding urban areas, it is challenging when the
spatial-temporal granularity of forecasting improves due to the
rarity of accident records and the complexity of long-term future
dependencies. To address these challenges, [ 50] propose a unified
framework named RiskSeq, which is designed to foresee sparse
urban accidents with finer granularities and multiple steps from a
spatial-temporal perspective. This approach aims to enhance the
accuracy and detail of accident predictions, thereby improving the
efficiency of police force allocation and traffic management strate-
gies. For the traffic flow predictions, not only should it consider
the temporal dependencies that exist between different nodes in
the network, but also the spatial correlations between nodes. [ 10]
propose a Global Spatial-Temporal Network (GSTNet), which is
composed of multiple spatial-temporal blocks, in order to capture
the global dynamic spatial-temporal correlations.
However, there is currently very limited research involving multi-
granularity knowledge transfer in federated learning, and there
is almost no research on using multi-granularity knowledge to
address the spatial-temporal catastrophic forgetting in FCL. In this
paper, we retain fine-grained knowledge in the local prompts and
coarse-grained knowledge in the global prompts to achieve spatial-
temporal knowledge fusion across tasks and clients.
2.2 Prompt-Based Continual Learning
Continual Learning (CL) aims to overcome catastrophic forget-
ting of the previous knowledge after training on new data in non-
stationary task streams [ 6]. Various CL techniques [ 21,29,30]
have been proposed to alleviate catastrophic forgetting and achieve
knowledge transfer across tasks, including regularization, rehearsal,
parameter isolation, and knowledge distillation.
Recent works introduce prompt learning to CL to achieve more
efficient exemplar-free CL [ 34,40,41]. Prompt learning is a novel
transfer learning technique applied to adapt general knowledge of
pre-trained large language or vision models to downstream tasks by
optimizing prompts [ 14,15,20,49]. CoOp [ 49] integrates learnable
prompts in the vision-language model to facilitate end-to-end learn-
ing where the design of task-specific prompts is fully automated.
L2P [ 41] applies learnable task-specific prompts to mitigate forget-
ting and even outperforms exemplar-based methods in accuracy
and efficiency. DualPrompt [ 40] decouples the learnable prompts
into general and expert prompts, encoding task-invariant and task-
specific knowledge, respectively. CODA [ 34] replaces key-valuepairs in the prompt selection strategy with an attention-based end-
to-end scheme. Pro-KT [ 25] attaches complementary prompts to a
pre-trained large model to efficiently transfer task-aware and task-
specific knowledge. LGCL [ 17] mitigates forgetting in extremely
heterogeneous task streams, where the class set of each task is dis-
joint, by improving the key lookup of the prompt pool and mapping
the output feature to class-level language representation.
In this paper, we design a local prompt and a global prompt
mechanism to extract and encode coarse-grained and fine-grained
knowledge, achieving spatial-temporal knowledge transfer.
2.3 Personalized Federated Learning
Personalized Federated Learning (PFL) focuses on training cus-
tomized models to accommodate various preferences and require-
ments of clients in heterogeneous FL. Existing works on PFL can
be categorized into data-based and model-based approaches [37].
Per-FedAvg [ 9] designs a Model-Agnostic Meta-Learning (MAML)
framework to find a generalized global model. It trains personalized
local models derived from the shared global model. pFedMe [ 36]
integrates L2-norm regularization in the loss function to adaptively
control the balance between personalization and generalization
in federated MAML. Ditto [ 22] adds a regularization term in the
local objectives as the loss function of the local adaptation process
but aggregates the models before the local adaptation to strike a
balance of personalization and generalization. FedSteg [ 44] enables
domain adaptation from the shared global model to personalized
local models by adding a correlation alignment layer before the
softmax layer. FedPer [ 1,33] decouples the model into base layers
and personalized layers and aggregates the shallow base layers to
capture generic knowledge while retaining the deep personalized
layers locally to maintain personalized knowledge. FedMSplit [ 4]
adopts multi-task learning to fit related but personalized models for
clients. FedCE [ 2] clusters the clients into several groups based on
the similarity of local data distributions and trains multiple global
models for each group. FedCP[ 48] proposes an auxiliary Condi-
tional Policy Network to achieve more fine-grained personalization
with sample-wise feature separation. [ 38] conducts clustering by
analyzing the principal angles of local data in the subspaces and
delays the training stage until the clustering is accomplished. These
works do not explicitly explore the multi-granular knowledge in
the processes of generalization and personalization.
Some recent works incorporated prompting learning methods
into PFL. pFedPG [ 43] utilizes personalized prompt generation
globally and personalized prompt adaptation locally to achieve PFL
under heterogeneous data. pFedPrompt [ 11] extracts user consensus
from linguistic space and adapts to local characteristics in visual
space in a non-parametric manner.
However, extracting and fusing spatial-temporal multi-granular
knowledge via prompting to overcome catastrophic forgetting and
data heterogeneity has not yet been implemented in PFCL.
3 PROBLEM DEFINITION
3.1 Personalized Federated Continual Learning
The primary goal of PFCL is to accumulate and fuse knowledge
from different times and spaces. Clients employ suitable personal-
ized strategies to make the received generalized knowledge better
4025KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
adapted to the characteristics of local data and effectively meet the
requirements of local tasks. However, due to FCL itself, PFCL is
also susceptible to severe spatial-temporal catastrophic forgetting.
Therefore, PFCL has three main objectives. The first is to form
more generalized knowledge during server knowledge fusion, avoid-
ing spatial catastrophic forgetting caused by heterogeneous data.
The second is for clients to adopt appropriate strategies to over-
come temporal forgetting resulting from continual learning. The
third is for clients to employ suitable personalization strategies,
ensuring that the received generalized global model better adapts
to the local task requirements and characteristics of local data.
Now, we extend the traditional FL to PFCL.
•Given𝑎clients (denoted as A={𝐴1,𝐴2,...,𝐴𝑎}), and a cen-
tral server (denoted as 𝑆), each client{𝐴𝑖,1≤𝑖≤𝑎}has its
unique task sequence T𝑖, where each task encompasses differ-
ent classes. The task sequence of client 𝐴𝑖is denoted asT𝑖=
{𝑇1
𝑖,𝑇2
𝑖,...,𝑇𝑛𝑖
𝑖}, where𝑛𝑖represents the total number of tasks
on client𝐴𝑖. The𝑘-th task ofT𝑖containsC𝑘
𝑖classes, andC𝑖=
{C1
𝑖∪C2
𝑖∪...,∪C𝑛𝑖
𝑖}.
•During the training of task 𝑟, the global model on the server al-
ready possesses the knowledge of 𝑇1
𝑖to𝑇𝑟−1
𝑖from client{𝐴𝑖,1≤
𝑖≤𝑎}. The server 𝑆then distributes it back to clients. After
personalizing the received global model 𝜃𝑟−1𝑔, the client𝐴𝑖con-
tinually trains it on 𝑇𝑟
𝑖as the initial model to get the new local
model𝜃𝑟
𝑖. The local model 𝜃𝑟
𝑖should perform well in classifying
classes from the set {C1
𝑖∪C2
𝑖∪...,∪C𝑟
𝑖}.
•Finally, the server collects the local models from clients who
participate in FCL and obtains a new global model 𝜃𝑟𝑔, which has
more generalized knowledge of learned tasks from all clients.
Clients need to adopt appropriate strategies to personalize the
global model 𝜃𝑟𝑔, enabling it to perform better locally.
According to the similarity of task sequences among clients, FCL
can be initially divided into two scenarios: synchronous FCL and
asynchronous FCL [47]. We will discuss it in detail in Sec. 5.1.2.
3.2 Spatial-Temporal Catastrophic Forgetting
Catastrophic Forgetting is a fundamental challenge in CL, which
refers to a phenomenon that a model would forget the knowledge
learned on old tasks when training on new tasks [ 6]. The reason for
catastrophic forgetting is that the well-learned network parameters
on the old tasks are overwritten during training on the new tasks
[47].
In the FCL setting, catastrophic forgetting exists as well. In a
real-world scenario, data reaches clients consecutively through
task streams [ 24], causing temporal catastrophic forgetting. At the
aggregation stage, the central server collects local models and ag-
gregates them into one global model. Then, the server distributes
the global model back to clients. Local models are trained with dif-
ferent training data. Aggregating them leads to the overwriting of
certain task-specific crucial parameters, consequently causing a de-
cline in the performance of the global model on local-specific tasks.
Adopting the global model consolidated such conflict knowledge
exacerbates the temporal catastrophic forgetting of each client’s
previous tasks.
The fundamental reason for STCF is that the knowledge rep-
resented by the model’s parameters is too fine-grained, leadingto a lack of robustness against minor variations. Therefore, it is
necessary to represent knowledge in a multi-granularity way. Split-
ting it into coarse-grained spatial-temporal-invariant knowledge
and fine-grained spatial-temporal-specific knowledge and handling
them separately can effectively overcome STCF.
We design Temporal Knowledge Retention to measure the
effectiveness of temporal knowledge transfer and Spatial Knowl-
edge Retention to measure the effectiveness of spatial knowledge
transfer in PFCL.
Definition 1. (Temporal Knowledge Retention) Given a fed-
erated learning system with 𝑎clients, the temporal knowledge
retention is defined as:
𝐾𝑅𝑡=1
𝑎𝑎∑︁
𝑖=1𝐴𝑐𝑐(𝜃𝑟
𝑖;𝑇0
𝑖)
𝐴𝑐𝑐(𝜃0
𝑖;𝑇0
𝑖), (1)
where𝐴𝑐𝑐(𝜃𝑟
𝑖;𝑇0
𝑖)denotes the test accuracy of client 𝐴𝑖’s local
model at𝑟-th round on the 0-th task and 𝐴𝑐𝑐(𝜃0
𝑖;𝑇0
𝑖)denotes the
test accuracy of client 𝐴𝑖’s local model at the initial round on the
0-th task.
Definition 2. (Spatial Knowledge Retention) Given a federated
learning system with 𝑎clients, the spatial knowledge retention is
defined as:
𝐾𝑅𝑠=1
𝑎𝑎∑︁
𝑖=1𝐴𝑐𝑐(𝜃𝑟𝑔;𝑇𝑟
𝑖)
𝐴𝑐𝑐(𝜃𝑟
𝑖;𝑇𝑟
𝑖), (2)
where𝐴𝑐𝑐(𝜃𝑟𝑔;𝑇𝑟
𝑖)denotes the accuracy of the global model 𝜃𝑟𝑔on
the current local task 𝑇𝑟
𝑖at client𝐴𝑖and𝐴𝑐𝑐(𝜃𝑟
𝑖;𝑇𝑟
𝑖)denotes the
accuracy of the local model 𝜃𝑟
𝑖on its current local task 𝑇𝑟
𝑖.
4 MULTI-GRANULARITY PROMPT
In this section, we elaborate on our proposed Federated Multi-
Granularity Prompt (FedMGP), which introduces a multi-granularity
knowledge space into PFCL for the first time to better address per-
sonalized requirements and spatial-temporal forgetting.
Specifically, on the client, we design prompts at two granular-
ity levels for knowledge representation, namely Coarse-grained
Global Prompt (see Sec. 4.1) and Fine-grained Local Prompt (see
Sec. 4.2). Global prompts represent coarse-grained common knowl-
edge, while local prompts, built upon global prompts, represent
class-wise fine-grained knowledge. Only fusing the coarse-grained
common knowledge facilitates the formation of generalized knowl-
edge and avoids spatial forgetting caused by aggregating fine-
grained knowledge. Local prompts based on global prompts aim
to personalize the generalized knowledge from the server while
preventing temporal forgetting due to class increments.
On the server side, we devise a new approach for fusing global
prompts called Selective Prompt Fusion (see Sec. 4.3) without spatial
forgetting. Aggregating only coarse-grained knowledge not only en-
hances aggregation speed but also provides further improvements
in privacy protection.
The overall framework of the proposed method is shown in Fig. 2,
and the algorithm is summarized in algorithm 1.
4026Personalized Federated Continual Learning via Multi-granularity Prompt KDD ’24, August 25–29, 2024, Barcelona, Spain
Task 1
...Task sequence of client 1
 ...
Task 
Embedding layerNormMulti-Head
AttentionMLPGlobal
Query Function Global Prompt
Patch Embedding
Chosen PromptNorm
Forzen Pre-T rained Transformer EncoderGlobal
MLP  HeadUpdate global pr ompt
From Coarse to FineFreeze Global PromptLocal
Query FunctionClass-wise Local
Prompt
Chosen Local
Prompt 
Local MLP  HeadUpdate local pr omptGlobal Prompts of Client N
Global Prompts of Client 1Global Prompts of Client 2...
Selective
Prompt FusionFused Global
Prompts Task 
Coarse-grained Global Prompt Fine-grained Local PromptFusion of Prompt on Server
Multi-Head
AttentionPre-T rained
ViTModifyUpload
Figure 2: An overview of the proposed FedMGP. Two granularities of prompts are used to capture both temporal-spatial
invariant knowledge and specific knowledge. The coarse-grained global prompt is trained through a shared ViT model, acting
on the embedding layer. The fine-grained local prompt is built upon the coarse-grained prompt by introducing additional
parameters in the MSA layer, enabling the model to better adapt to local data. Moreover, selective prompt fusion is employed to
aggregate global prompts on the server side, forming generalized knowledge.
4.1 Coarse-grained Global Prompt
Due to the heterogeneity of data, significant differences exist among
local models, leading to substantial variations in extracted knowl-
edge. This also poses significant challenges for the fusion and trans-
fer of knowledge, as the knowledge learned by each client is overly
fine-grained. Inspired by the cognitive processes of humans, knowl-
edge transfer among humans is effective because there is a fun-
damental shared cognition, enabling the meaningful exchange of
knowledge. Therefore, we assign each client with the same pre-
trained ViT model as a foundational cognitive system. With ViT’s
parameters frozen, clients learn global prompts that operate at the
input level. Consequently, these global prompts represent coarse-
grained knowledge acquired through the common model learning
process. Furthermore, as the knowledge is extracted from the same
model, it is more convenient to aggregate knowledge on the server
side without spatial forgetting.
The training of coarse-grained global prompts is based on the
frozen ViT model. Moreover, global prompts operate at the input
level, not influencing the model’s parameters. The purpose is to
extract knowledge into a common space through the same model.
4.1.1 Global Prompt Pool. Taking inspiration from L2P [ 41], we
devise a prompt pool for storing and selecting the global prompts.
The prompt pool is defined as
P𝑔={𝑃1
𝑔,𝑃2
𝑔,...,𝑃𝑀
𝑔}, (3)
where𝑚is the pool size and 𝑃𝑗
𝑔is a single global prompt. Then,
let𝑥and𝐸=𝑓𝑒(𝑥)be the input and its corresponding embedding
feature, respectively. Denoting {𝑠𝑖}𝑁
1be the indices of 𝑁globalprompts, then we can modify the embedding feature as follows:
𝐸′=
𝑃𝑠1𝑔,...,𝑃𝑠𝑘𝑔;𝐸
,1≤𝑁≤𝑀, (4)
where [;] represents concatenation along the token length dimen-
sion. The next question is how to choose global prompts.
4.1.2 Global Query Function. Due to the use of the same model,
similar inputs tend to select similar prompts and vice versa. This
mitigates the challenge of aggregating heterogeneous knowledge
on the server. Based on this, we have designed a key-value pair-
based query strategy that dynamically selects suitable prompts
by calculating the similarity between the input key and existing
prompts’ keys.
We associate each prompt in the pool with a learnable key, de-
noted as{(𝐾1𝑔,𝑃1𝑔),(𝐾2𝑔,𝑃2𝑔),...,(𝐾𝑚𝑔,𝑃𝑚𝑔)}. To ensure that similar
inputs have similar keys, we use the output features of the pre-
trained ViTVas the key for the input, i.e., 𝐾𝑖𝑛𝑔=V(𝐸). Then, the
query process can be summarized by the following expression:
K𝑠
𝑔=argmin
K𝑔𝑁∑︁
𝑖=1dis(𝐾𝑖𝑛
𝑔,𝐾𝑖
𝑔), (5)
whereK𝑠denotes the subset of top-N keys selected specifically for
the input, andK𝑔represents the set of keys for all global prompts.
In this work, we utilize cosine similarity as the distance function to
measure the similarity between keys.
4.1.3 Optimization for Global Prompt. Each client has a global
classification head used for training global prompts, denoted as 𝐻𝑖𝑔.
At the beginning of training, it is necessary to load the pre-trained
model with𝐻𝑖𝑔to enable it to perform the classification task, and we
4027KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
Algorithm 1: FedMGP Algorithm.
Input:𝑎clientsA={𝐴𝑖}𝑎
𝑖=1with their own task sequence
T𝑖={𝑇𝑛
𝑖}𝑁
𝑛=1, a pre-trained frozen ViT Vwithout
classification head.
Output: Fused global prompt pool P𝐺, local prompt pool
𝑃𝑙={𝑃𝑖
𝑙}𝑎
𝑖=1of all clients and local classification
head𝐻𝑙={𝐻𝑖
𝑙}𝑎
𝑖=1.
1Initialization;
2while task number n≤Ndo
3 foreach client𝐴𝑖,1≤𝑖≤𝑎do
4V𝑖𝑔←LoadGHead( 𝐻𝑖𝑔,V);
5 Training global prompts:
6 foreach{𝑥,𝑦}∈𝑇𝑛
𝑖do
7 𝐸←EmbeddingLayer( 𝑥);
8{𝐾𝑔,𝑃𝑔}← GlobalQueryFunction( 𝑃𝑖𝑔,𝐸,V);
// Key-value pair.
9 𝐸′←AppendGP(𝐸,𝑃𝑔);
10 𝐿𝑔←Classify(V𝑖𝑔,𝐸′,𝑌) ;
// Classification loss with V𝑖𝑔.
11 Optimize(𝐿𝑔,𝐻𝑖𝑔,𝐾𝑔,𝑃𝑔);
12 Freeze global prompts;
13 Training local prompts:
14 foreach{𝑥,𝑦}∈𝑇𝑛
𝑖do
15 𝐸′←GetGlobalPrompt( 𝑥,P𝑖𝑔,V);
16{𝐾𝑙,𝑃𝑙}← LocalQueryFunction( P𝑖
𝑙,𝐸′,V);
17V𝑖
𝑙←LoadLocalPrompt&Head( 𝐻𝑖
𝑙,V,𝑃𝑙);
18 𝐿𝑙←Classify(V𝑖
𝑙,𝐸′,𝑌);
19 Optimize(𝐿𝑙,H𝑖
𝑙,𝐾𝑙,𝑃𝑙);
// Notice that global prompts are
frozen.
20 Server aggregation:
21P𝑔= {P1𝑔∪...∪C𝑇𝑎𝑔} ;
22P𝐺←SelectivePromptFusion( P𝑔) ;
23 DistributeP𝐺to all clients for next task training.
denote the model with 𝐻𝑖𝑔asV𝑖𝑔. Overall, the training loss function
is as follows:
min
𝐻𝑖𝑔,P𝑔,K𝑔L(V𝑖
𝑔(𝐸′),𝑦)+𝜆1∑︁
K𝑠𝑔dis(𝐾𝑖𝑛
𝑔,𝐾𝑠𝑖𝑔), (6)
where𝜆1is a hyperparameter. The initial term comprises the soft-
max cross-entropy loss, while the subsequent term serves as a
surrogate loss aimed at bringing selected keys closer to their corre-
sponding query features.
4.2 Fine-grained Local Prompts
Once the training of global prompts is completed, they will be frozen
and remain unchanged, including both the prompts themselves and
their corresponding keys, until the next task training. Based on the
frozen global prompts, we further developed fine-grained class-wise
local prompts. These prompts directly impact the model’s multi-
head self-attention (MSA) [ 39] layers, facilitating the extractionof local, fine-grained knowledge. Additionally, this fine-grained
prompting helps overcome temporal forgetting induced by class
increments. The hierarchy of prompts, from coarse to fine, simplifies
generalized knowledge extraction, fusion, and personalization.
4.2.1 From Coarse to Fine. Similarly, a prompt pool is constructed
for local prompts. However, since it represents class-specific knowl-
edge, the size of the pool depends on the number of data classes.
The local prompt pool is defined as
P𝑙={(𝐾1
𝑙,𝑃1
𝑙),(𝐾2
𝑙,𝑃2
𝑙),...,(𝐾𝐶
𝑙,𝑃𝐶
𝑙)}, (7)
where𝐶represents the number of classes. It is precisely the class-
wise fine-grained knowledge that imparts significant effectiveness
to our approach of personalization and addressing temporal forget-
ting induced by class increments. It is proved in Sec. 5.3.
4.2.2 Local Query Function. Fine-grained prompts are selected
based on the global prompt, so we must first obtain frozen global
prompts by allowing the original input 𝑥to undergo the global
query function. Subsequently, we concatenate to form an input 𝐸′
with selected global prompts. Then, similar to obtaining the key
for global prompts, we acquire the key for local prompts 𝐾𝑖𝑛𝑔=
V(𝐸′), with the only difference being that the input is now 𝐸′.
The subsequent steps of calculating similarity and selection are
analogous to the corresponding operations for global prompts.
Note that we do not employ this querying function during the
training phase. Instead, we use mask code to select the local prompt
corresponding to the data class for training.
4.2.3 Optimization for Local Prompt. Local prompts directly op-
erate on the model’s MSA layer, where we represent the input
query, key, and values as ℎ𝑞,ℎ𝑘,ℎ𝑣, respectively. MSA layers can
be denoted as:
MSA(ℎ𝑞,ℎ𝑘,ℎ𝑣)=Concat(h1,..., hz)𝑊𝑂, (8)
whereℎ𝑖=Attention(ℎ𝑄𝑊𝑄
𝑖,ℎ𝐾𝑊𝐾
𝑖,ℎ𝑉𝑊𝑉
𝑖).𝑊is the project
matrix and 𝑧is the number of MSA layers. We use the Prefix
Tuning (Pre-T) to tune local prompts. Pre-T splits the local prompt
𝑃𝑙into𝑝𝐾and𝑝𝑉, and adds them to ℎ𝐾andℎ𝑉:
MSA′=MSA(ℎ𝑄,[𝑝𝑉;ℎ𝐾],[𝑝𝑉;ℎ𝑉]). (9)
Once global prompts have completed training, they freeze along
with their corresponding keys. The input 𝑥first goes through the
global query function to find the corresponding global prompts.
Subsequently, the embedding of 𝑥is concatenated with the prompts
to form𝐸′. Then𝐸′is processed by the local query function to
find the corresponding fine-grained prompts {𝐾𝑙,𝑃𝑙}. Thus, ViT
modifies its MSA layer based on 𝑃𝑙and loads the local classification
head𝐻𝑖
𝑙, formingV𝑖
𝑙. The local prompt training loss function is
min
𝐻𝑖
𝑙,P𝑙,K𝑙L(V𝑖
𝑙(𝐸′),𝑦)+𝜆2∑︁
K𝑠
𝑙dis(𝐾𝑖𝑛
𝑙,𝐾𝑠𝑖
𝑙). (10)
4.3 Selective Prompt Fusion
To fuse global prompts precisely, we devise a novel selective prompt
fusion mechanism that aggregates prompts from different prompt
pools through knowledge distillation, enhancing their generaliza-
tion. To our knowledge, it is a novel approach to distill prompts
from different clients.
4028Personalized Federated Continual Learning via Multi-granularity Prompt KDD ’24, August 25–29, 2024, Barcelona, Spain
We denote the small proxy dataset owned by the server as D𝑠.
{𝑥𝑠,𝑦𝑠}are the samples and corresponding labels from D𝑠for the
distillation process. For the convenience of writing and understand-
ing, we will only consider two global prompt pools here, denoted as
P𝑖𝑔andP𝑗
𝑔.P𝑖𝑔is chosen as the student pool. Initially, the input 𝑥𝑝
searches for the corresponding global prompt within P𝑖𝑔, and then
concatenates to form an embedding 𝐸′
𝑖with the prompt. Similarly,
𝐸′
𝑗represents the embedding of the same input but concatenated
with the prompts from P𝑗
𝑔. Therefore, the distillation loss can be
summarized as:
L𝐶𝐸=MSE
𝑥𝑠∈D∫(V(𝐸′
𝑖),V(𝐸′
𝑗)). (11)
5 EXPERIMENTS
5.1 Experimental Setup
5.1.1 Datasets. We conduct extensive experiments on CIFAR-100
[19] with 5 incremental tasks to evaluate the effectiveness of our
FedMGP in addressing the challenges of PFCL. CIFAR-100 is a
widely used benchmark dataset and consists of 60,000 RGB color
images, each of size 32x32 pixels, classified into 100 different classes.
We consider two practical scenarios of FCL, namely synchronous
FCL and asynchronous FCL.
In the synchronous FCL settings [ 47], clients have the same task
sequences but a varied proportion of samples from each class. It is a
common setting employed in existing FCL works [ 7]. The degree of
data heterogeneity in this scenario is controlled with the Dirichlet
parameter, which is set to be 1in our experiments. Specifically, we
first partition the dataset into 5 tasks, each containing 20 classes,
with no overlapping class between tasks. Then, within each task, the
samples of each class are randomly divided into the same number
of subsets as the total number of clients, ensuring that the data
among clients is also non-overlapping.
In the asynchronous FCL settings [ 47], some of the classes are
accessible to all clients while others are private to certain clients,
which is derived from pathological Non-IID in static FL [ 31]. In this
setting, we consider that there are 15private classes for each client.
Each task contains 8classes. To be specific, each client first selects
15 classes unique to itself, and only that client has access to the full
data of these classes. Therefore, there are 25 classes lefted as public
classes shared by all clients. As a result, each client has data for 40
classes. The client then randomly divides these 40 classes into 5
tasks, each containing 8 classes.
5.1.2 Baselines and Backbones. We compare FedMGP with FedAvg
[31], FedEWC [ 18], FedProx [ 23] and GLFC [ 7] on ResNet-18 [ 12].
Since our method is based on ViT-B/16, we also conduct experi-
ments on ViT-B/16 to compare FedMGP with FedViT. FedViT is
a naive combination of FedAvg and ViT [ 8], which performs fed-
erated training by locally updating and globally aggregating the
parameters of the classifier heads iteratively. FedL2P and FedDualP
are the adapted versions of two effective prompt-based methods
in traditional CL, L2P [ 41] and DualPrompt [ 40], making them
more suitable for use in a federated environment. More detailed
descriptions are in Appendix 3.5.1.3 Implementation Details. In our setup, the federated system
consists of five clients and one central server, and each client pos-
sesses a sequence of five tasks. We repeat experiments with three
random seeds (42,1999,2024) and report the averaged outcomes.
Across all methods, we fix the number of clients at five and the
interval rounds for increments at five. We employ Adam as the
optimizer with a learning rate of 0.001. The whole training process
is performed sequentially on an NVIDIA GPU RTX-3090.
5.2 Expermental Results
We use the accuracy of the aggregated global model on local test
sets as the metric in Table 1. To examine the impact of different
backbone networks on the experimental results, we employed base-
line methods based on two backbones, namely ResNet-18 and the
pre-trained ViT.
Surprisingly, all methods generally perform better in the asyn-
chronous setting than in the synchronous setting. This is attributed
to the fact that in the synchronous setting, each task involves 20
classes. GLFC, FedAvg, and FedProx failed in both asynchronous
and synchronous FCL. As expected, methods based on ViT outper-
formed those based on ResNet-18 in both scenarios. But FedAvg
performs even better than FedViT and FedDual in synchronous
FCL. This indicates that in scenarios with similar data distributions,
FedAvg has the ability to challenge large pre-trained models.
In all methods using ViT as the backbone, FedL2P with prompts
performed better than using ViT alone. Unfortunately, FedDualP
performed even worse than the simple FedViT. We believe this is
due to the heterogeneity in the learned parameters across clients.
Moreover, the performance of these methods did not show signifi-
cant improvement after aggregation. In fact, FedViT experienced a
decrease of 3.9% in average accuracy after aggregation in synchro-
nous FCL and a decrease of 7.27% in asynchronous FCL.
Our method achieved the best performance in both synchronous
and asynchronous settings, with accuracies of 90.56% and 83.46%,
showing the state-of-the-art performance of fusing heterogeneous
knowledge. Although our method performs well on this metric
even without some components, such as Ours-w/oGP achieving
89.36% and 81.06%, and Ours-w/oLp achieving 87.29% and 77.93%,
the ability to retain spatial-temporal knowledge is significantly
affected. In the following section (Sec. 5.3), we will evaluate each
method using new metrics, i.e., temporal knowledge retention and
spatial knowledge retention, to evaluate the resistance of spatial-
temporal catastrophic forgetting.
5.3 Ablation Studies
To further validate the effectiveness of the multi-granularity knowl-
edge space, we conducted three different ablation experiments un-
der the same experimental setup. These experiments respectively re-
moved the global prompts, local prompts, and the selective prompt
fusion mechanism on the server. Results are shown in Fig. 3.
In both asynchronous and synchronous settings, ViT-based meth-
ods have demonstrated exceptional performance in retaining spatial
knowledge. This result also confirms our hypothesis: having sim-
ilar cognition is the foundation for knowledge sharing. Based on
that, the increment of spatial knowledge retention of FedAvg in the
synchronous setting is not difficult to understand, as similar data
4029KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
Table 1: Accuracy of the aggregated global model on local test sets CIFAR-100 with 5 class-incremental tasks each client.
Algorithm BackboneAsynchronous Synchronous
Task ID Task ID
1 2 3 4 5 Average 1 2 3 4 5 Average
FedAvg[31]
ResNet-1847.39 62.63 67.25 62.69 68.72 61.74 67.77 77.72 76.65 74.59 82.00 75.74
FedProx[23] 68.26 56.94 65.20 63.82 67.48 64.34 43.07 23.09 51.41 45.01 50.62 42.64
FedEWC 27.77 20.66 25.70 24.11 26.41 24.93 49.94 71.00 70.22 70.09 77.89 67.83
GLFC[7] 14.22 18.84 23.93 26.70 22.52 21.24 5.22 8.93 24.61 35.43 42.33 23.30
FedViT
ViT83.02 82.39 83.24 80.34 83.32 82.46 70.30 71.05 69.10 64.36 71.01 69.16
FedL2P 89.63 89.68 90.45 90.02 90.75 90.11 80.22 82.81 81.61 80.68 84.14 81.89
FedDualP 82.09 81.17 80.05 80.52 81.48 81.06 63.84 65.62 63.16 61.10 63.28 63.40
Ours(FedMGP)
ViT90.26 90.14 91.29 90.30 90.83 90.56 82.23 84.14 82.01 82.47 86.44 83.46
Ours-w/oLP 88.35 89.19 89.91 89.16 90.20 89.36 79.80 82.04 79.71 80.20 83.56 81.06
Ours-w/oGP 86.93 88.52 82.85 84.11 87.29 85.94 78.73 78.92 77.74 75.16 79.10 77.93
1 2 3 4 5
T ask Number2030405060708090100Spatial Knowledge Retention
FedAvg
FedProx
GLFC
FedEWC
FedViT
FedL2P
FedDualP
Ours-w/oGP
Ours-w/oLP
Ours-w/oSPF
Ours
(a) Asynchronous 𝐾𝑅𝑠
1 2 3 4 5
T ask Number020406080100T emporal Knowledge Retention
FedAvg
FedProx
GLFC
FedEWC
FedViT
FedL2P
FedDualP
Ours-w/oGP
Ours-w/oLP
Ours-w/oSPF
Ours (b) Asynchronous 𝐾𝑅𝑡
1 2 3 4 5
T ask Number20406080100Spatial Knowledge Retention
FedAvg
FedProx
GLFC
FedEWC
FedViT
FedL2P
FedDualP
Ours-w/oGP
Ours-w/oLP
Ours-w/oSPF
Ours (c) Synchronous 𝐾𝑅𝑠
1 2 3 4 5
T ask Number020406080100T emporal Knowledge Retention
FedAvg
FedProx
GLFC
FedEWC
FedViT
FedL2P
FedDualP
Ours-w/oGP
Ours-w/oLP
Ours-w/oSPF
Ours (d) Synchronous 𝐾𝑅𝑡
Figure 3: Ablation Studies using temporal knowledge retention (Equation 1) spatial knowledge retention (Equation 2) in two
FCL setting. Note that, "Ours-w/oGP" refers to our method without global prompts representing coarse-grained knowledge.
"Ours-w/oLP" refers to our method without local prompt to capture client/time relevant knowledge. And "Ours-w/oSPF" is to
use FedAvg to aggregate global prompts instead of Selective Prompt Fusion.
contributes to the similarity of convolutional layers. While these
methods have effectively preserved spatial knowledge, none of
them demonstrates resistance to temporal catastrophic forgetting.
In Fig. 3(b) and Fig. 3(d), it is challenging to distinguish the differ-
ence between FedL2P, FedDualP and other methods with ResNet18
as the backbone network, as their temporal knowledge retention
rates are all around 20%.
Our approach not only competes with other ViT methods in
terms of spatial knowledge retention but also achieves almost no
forgetting in temporal knowledge retention, thanks to the construc-
tion of the multi-granularity knowledge space. To evaluate the con-
tribution of the coarse-grained global prompt and the fine-grained
local prompt, three different ablation experiments are conducted,
which respectively removed global prompts (Ours-w/oGP), local
prompts (Ours-w/oLP), and selective prompt fusion (Ours-w/oSPF).
In Fig. 3(a), there is a slight decrease in spatial knowledge retention
when we remove the global prompt. The other two components
have little impact on spatial forgetting. However, things becomemore complex when it comes to temporal knowledge retention.
Without local prompts, it drops significantly to around 15%. And
when we remove global prompts, although the retention also de-
creases, it is not as drastic.
It concludes that fine-grained local prompts play a crucial role
in preventing temporal catastrophic forgetting, and they still need
to be combined with coarse-grained knowledge to better prevent
spatial-temporal catastrophic forgetting and achieve personaliza-
tion. Hence, multi-granularity knowledge representation is a promis-
ing direction in PFCL.
5.4 Sensitivity Analysis
FedMGP involves several hyperparameters, including prompt length,
prompt pool size and so on. To further investigate the robustness
of FedMGP, we conduct sensitivity analyses of prompt length and
pool size on CIFAR-100 with 5 incremental tasks and present the
results in Fig. 4.
4030Personalized Federated Continual Learning via Multi-granularity Prompt KDD ’24, August 25–29, 2024, Barcelona, Spain
15101520
Prompt Length15101520Pool Size
98.97 100.04 100.37 99.07 99.94100.32 99.48 98.94 99.46 99.9299.71 99.71 99.01 99.47 99.43100.10 100.03 100.11 99.01 99.0199.68 100.22 100.09 98.84 99.53
(a) Global𝐾𝑅𝑠
15101520
Prompt Length15101520Pool Size
99.42 99.14 99.67 99.68 99.5299.00 99.33 99.23 99.61 99.8999.59 99.34 99.34 99.51 98.6499.29 100.33 99.54 98.91 99.1699.77 100.24 99.50 99.37 98.60 (b) Local𝐾𝑅𝑠
Figure 4: Sensitivity analyses of prompt length and prompt
pool size. Left: Global Spatial Knowledge Retention Ratio (%)
w.r.t. prompt length 𝐿and prompt pool size 𝑀. Right: Local
Spatial Knowledge Retention Ratio (%) w.r.t. prompt length
𝐿and prompt pool size 𝑀.
From Fig. 4(a), it can be observed that, regardless of the values of
prompt length and pool size, it is beneficial for spatial knowledge
of the global prompts. Additionally, under the condition of Pool
Size=1 and Prompt Length=10, spatial knowledge retention is the
highest, reaching 100.37%.
From Fig. 4(b), it can be seen that different values of prompt
pool size and prompt length have little effect on spatial knowledge
retention of the local prompts. It implies utilizing multi-granularity
prompts is capable of training a generalized global model as well
as personalized local models. More sensitivity analyses are shown
in Appendix 2.
6 DISCUSSION
This section will provide a preliminary analysis and discussion
of the computational cost, communication overhead, and privacy
protection in federated learning for FedMGP.
Computational cost. The clients have only two parts to train:
coarse-grained global prompts and fine-grained local prompts. The
size of the global prompt pool of one client is determined by the
number of prompts, prompt length, and embedding dimension,
which are set to 10, 10, and 768 in the experiments. And the size
of prompt keys is determined by the pool size and embedding
dimension. In our experimental setup, the total size of local prompts
is 4,608,000, and the size of their corresponding keys is also the
same as the global prompts’ keys, which is 7,680. In summary, each
client has a total of 4,700,160 parameters to train.
Moreover, the server only needs to aggregate the global prompts.
This means that the training process of local prompts can proceed
in parallel with the server’s aggregation process.
Communication overhead. Our method transmits only coarse-
grained global prompts and keys, keeping communication overhead
low. The size of the global prompt pool per client is determined by
the number of prompts, prompt length, and embedding dimension
(set to 10, 10, and 768 in experiments). Prompt keys size depends
on pool size and embedding dimension. Thus, the total transmitted
size is 76,800 + 7,680 parameters. Although there are fine-grained
local parts that also need to be trained, they remain local, whichsignificantly reduces the communication overhead compared to
traditional methods, indirectly enhancing privacy.
Privacy protection. Since FedMGP only transmits the coarse-
grained global prompts obtained from the ViT and their keys,
without uploading the original embeddings of the images and the
fine-grained local prompts, FedMGP has strong privacy protec-
tion, especially against gradient leakage attacks. Moreover, in our
experimental setup, the size of global prompts is only 76,800 pa-
rameters, containing much less information, which also ensures
privacy protection.
7 CONCLUSION
Personalized Federated Continual Learning is a novel and practical
scenario. It not only requires the accumulation of knowledge that
evolves over time and space but also needs consideration of per-
sonalized strategies to make generalized knowledge better adapted
to local requirements. Moreover, spatial-temporal catastrophic for-
getting is also a key issue that needs to be addressed.
In this paper, we first formulated a formal problem definition
for PFCL and shaped the objectives of PFCL as three folds: (1)
Alleviating spatial knowledge catastrophic forgetting caused by
data heterogeneity; (2) Mitigating temporal knowledge catastrophic
forgetting caused by dynamic task streams; (3) Training customized
local models to achieve personalization.
To address these issues, we proposed a multi-granularity knowl-
edge space for federated continuous learning (termed as FedMGP),
which has efficient fusion and personalization by representing
knowledge at different granularities. Specifically, the FedMGP uti-
lizes a shared ViT to construct coarse-grained global prompts and
modifies the ViT with local prompts based on these global prompts.
In addition, we designed 1) global prompts on the embedding layer
to learn coarse-grained knowledge continually and 2) local prompts
on the multi-head self-attention layer to learn fine-grained knowl-
edge as a complementary to achieve personalization. Extensive
experiments under synchronous and asynchronous FCL settings
are conducted to demonstrate the effectiveness of our method.
The effectiveness of multi-granularity knowledge representation
has been experimentally proven in this work, and their comple-
mentary nature significantly enhances the model’s resistance to
spatial-temporal catastrophic forgetting. Our future research will
investigate the multi-granularity representation of knowledge in
various federated learning scenarios such as vertical federated learn-
ing [27] and multi-objective federated learning [ 16]. We will explore
its implications for privacy preservation, model performance, algo-
rithm efficiency, and so on, aiming at achieving trustworthy PFCL.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foun-
dation of China (Nos. 72242106, 62176221), the Natural Science
Foundation of Sichuan Province (No. 2022NSFSC0528), Sichuan Sci-
ence and Technology Program (No. 2024YFHZ0024), Jiaozi Institute
of Fintech Innovation in Southwestern University of Finance and
Economics (Nos. kjcgzh20230103, kjcgzh20230201) and the Funda-
mental Research Funds for the Central Universities (YJ202421).
4031KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
REFERENCES
[1]Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav
Choudhary. 2019. Federated learning with personalization layers. arXiv preprint
arXiv:1912.00818 (2019).
[2]Luxin Cai, Naiyue Chen, Yuanzhouhan Cao, Jiahuan He, and Yidong Li. 2023.
FedCE: Personalized Federated Learning Method based on Clustering Ensembles.
InProceedings of the 31st ACM International Conference on Multimedia. 1625–1633.
[3]Shangxuan Cai, Yunfeng Zhao, Zhicheng Liu, Chao Qiu, Xiaofei Wang, and
Qinghua Hu. 2022. Multi-granularity Weighted Federated Learning in Het-
erogeneous Mobile Edge Computing Systems. In 2022 IEEE 42nd International
Conference on Distributed Computing Systems. IEEE, 436–446.
[4]Jiayi Chen and Aidong Zhang. 2022. FedMSplit: Correlation-adaptive federated
multi-task learning across multimodal split networks. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 87–96.
[5]Ziyang Chen, Jinzhi Liao, and Xiang Zhao. 2023. Multi-granularity Temporal
Question Answering over Knowledge Graphs. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics. 11378–11392.
[6]Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A continual learning
survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence 44, 7 (2021), 3366–3385.
[7]Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi
Zhu. 2022. Federated class-incremental learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10164–10173.
[8]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[9]Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized fed-
erated learning with theoretical guarantees: A model-agnostic meta-learning
approach. Advances in Neural Information Processing Systems 33 (2020), 3557–
3568.
[10] Shen Fang, Qi Zhang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2019.
GSTNet: Global spatial-temporal network for traffic flow prediction.. In IJCAI.
2286–2293.
[11] Tao Guo, Song Guo, and Junxiao Wang. 2023. pFedPrompt: Learning Personalized
Prompt for Vision-Language Models in Federated Learning. In Proceedings of the
ACM Web Conference. 1364–1374.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770–778.
[13] Zi-Yuan Hu, Yanyang Li, Michael R Lyu, and Liwei Wang. 2023. Vl-pet: Vision-
and-language parameter-efficient tuning via granularity control. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 3010–3020.
[14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie,
Bharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In Euro-
pean Conference on Computer Vision. Springer, 709–727.
[15] Yan Kang, Tao Fan, Hanlin Gu, Lixin Fan, and Qiang Yang. 2023. Grounding
foundation models through federated transfer learning: A general framework.
arXiv preprint arXiv:2311.17431 (2023).
[16] Yan Kang, Hanlin Gu, Xingxing Tang, Yuanqin He, Yuzhu Zhang, Jinnan He,
Yuxing Han, Lixin Fan, Kai Chen, and Qiang Yang. 2023. Optimizing privacy,
utility and efficiency in constrained multi-objective federated learning. arXiv
preprint arXiv:2305.00312 (2023).
[17] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier
Stricker, Federico Tombari, and Muhammad Zeshan Afzal. 2023. Introducing
language guidance in prompt-based continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 11463–11473.
[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al .2017. Overcoming catastrophic forgetting in neural
networks. Proceedings of the National Academy of Sciences 114, 13 (2017), 3521–
3526.
[19] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[20] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for
Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing. 3045–3059.
[21] Miaomiao Li, Jiaqi Zhu, Xin Yang, Yi Yang, Qiang Gao, and Hongan Wang. 2023.
CL-WSTC: Continual Learning for Weakly Supervised Text Classification on the
Internet. In Proceedings of the ACM Web Conference. 1489–1499.
[22] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021. Ditto: Fair and
robust federated learning through personalization. In International Conference on
Machine Learning. PMLR, 6357–6368.
[23] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine Learning and Systems 2 (2020), 429–450.[24] Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong, and Guannan
Zhang. 2024. Towards Efficient Replay in Federated Incremental Learning. arXiv
preprint arXiv:2403.05890 (2024).
[25] Yujie Li, Xin Yang, Hao Wang, Xiangkun Wang, and Tianrui Li. 2023. Learning to
Prompt Knowledge Transfer for Open-World Continual Learning. arXiv preprint
arXiv:2312.14990 (2023).
[26] Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. 2020. Client-edge-cloud
hierarchical federated learning. In ICC 2020-2020 IEEE International Conference
on Communications. IEEE, 1–6.
[27] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye
Ouyang, Ya-Qin Zhang, and Qiang Yang. 2024. Vertical Federated Learning:
Concepts, Advances, and Challenges. IEEE Transactions on Knowledge and Data
Engineering (2024). https://doi.org/10.1109/TKDE.2024.3352628
[28] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022. Layer-wised model
aggregation for personalized federated learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10092–10101.
[29] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott
Sanner. 2022. Online continual learning in image classification: An empirical
survey. Neurocomputing 469 (2022), 28–51.
[30] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bag-
danov, and Joost Van De Weijer. 2022. Class-incremental learning: survey and
performance evaluation on image classification. IEEE Transactions on Pattern
Analysis and Machine Intelligence 45, 5 (2022), 5513–5533.
[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial Intelligence and Statistics . PMLR,
1273–1282.
[32] Zijie Pan, Li Hu, Weixuan Tang, Jin Li, Yi He, and Zheli Liu. 2021. Privacy-
preserving multi-granular federated neural architecture search a general frame-
work. IEEE Transactions on Knowledge and Data Engineering (2021).
[33] Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed, Mike Rabbat, Maziar
Sanjabi, and Lin Xiao. 2022. Federated learning with partial model personalization.
InInternational Conference on Machine Learning. PMLR, 17716–17758.
[34] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla,
Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira.
2023. CODA-Prompt: COntinual Decomposed Attention-based Prompting for
Rehearsal-Free Continual Learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 11909–11919.
[35] Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. 2021. Partialfed: Cross-domain
personalized federated learning via partial initialization. Advances in Neural
Information Processing Systems 34 (2021), 23309–23320.
[36] Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020. Personalized federated
learning with Moreau envelopes. Advances in Neural Information Processing
Systems 33 (2020), 21394–21405.
[37] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-
alized federated learning. IEEE Transactions on Neural Networks and Learning
Systems (2022).
[38] Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, Chen
Chen, Mubarak Shah, and Bill Lin. 2023. Efficient distribution similarity identifi-
cation in clustered federated learning via principal angles between client data
subspaces. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37.
10043–10052.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in Neural Information Processing Systems 30 (2017).
[40] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu
Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al .2022. Dualprompt:
Complementary prompting for rehearsal-free continual learning. In European
Conference on Computer Vision. Springer, 631–648.
[41] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,
Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022. Learning to
prompt for continual learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 139–149.
[42] Hao Xiao, Weiyao Lin, Bin Sheng, Ke Lu, Junchi Yan, Jingdong Wang, Errui Ding,
Yihao Zhang, and Hongkai Xiong. 2018. Group re-identification: Leveraging and
integrating multi-grain information. In Proceedings of the 26th ACM International
Conference on Multimedia. 192–200.
[43] Fu-En Yang, Chien-Yi Wang, and Yu-Chiang Frank Wang. 2023. Efficient model
personalization in federated learning via client-specific prompt generation. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 19159–
19168.
[44] Hongwei Yang, Hui He, Weizhe Zhang, and Xiaochun Cao. 2020. FedSteg: A feder-
ated transfer learning framework for secure image steganalysis. IEEE Transactions
on Network Science and Engineering 8, 2 (2020), 1084–1094.
[45] Xin Yang, Yujie Li, Qiuke Li, Dun Liu, and Tianrui Li. 2022. Temporal-spatial three-
way granular computing for dynamic text sentiment classification. Information
Sciences 596 (2022), 551–566.
[46] Xin Yang, Yujie Li, Dan Meng, Yuxuan Yang, Dun Liu, and Tianrui Li. 2022. Three-
way multi-granularity learning towards open topic classification. Information
4032Personalized Federated Continual Learning via Multi-granularity Prompt KDD ’24, August 25–29, 2024, Barcelona, Spain
Sciences 585 (2022), 41–57.
[47] Xin Yang, Hao Yu, Xin Gao, Hao Wang, Junbo Zhang, and Tianrui Li. 2024.
Federated Continual Learning via Knowledge Fusion: A Survey. IEEE Transactions
on Knowledge and Data Engineering (2024).
[48] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and
Haibing Guan. 2023. Fedcp: Separating feature information for personalized
federated learning via conditional policy. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3249–3261.[49] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning
to prompt for vision-language models. International Journal of Computer Vision
130, 9 (2022), 2337–2348.
[50] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Chaochao Zhu.
2020. Foresee urban sparse traffic accidents: A spatiotemporal multi-granularity
perspective. IEEE Transactions on Knowledge and Data Engineering 34, 8 (2020),
3786–3799.
4033KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yu et al.
1 NOTATION
In Table 2, we introduce the notations in our paper.
Table 2: Mathematical notations and descriptions.
Notation Description
A𝑖 Client𝑖
𝜃𝑟𝑔 The global model at round 𝑟
𝜃𝑟
𝑖The local model of client 𝑖at round𝑟
P𝐺 Global prompt pool
𝑃𝑖
𝑙Local prompt pool of client 𝑖
T𝑖 Task sequence of client 𝑖
𝑇𝑛
𝑖The task of client 𝑖at incremental state 𝑛
V The pre-trained ViT
𝐸 Embedding layer
H The classification head
2 SENSITIVITY ANALYSIS
As illustrated in Fig. 5, the aggregation of global prompts has im-
proved the performance of both global prompts and local prompts.
Fig. 5(a) shows the performance improvement of coarse-grained
global prompts evaluated with test accuracy (%) after the aggre-
gation of global prompts. Fig. 5(b) illustrates the performance im-
provement of fine-grained local prompts after the aggregation of
global prompts. We can find that both improvements are robust to
different values of prompt pool size and prompt length.
15101520
Prompt Length15101520Pool Size
-90.52 3.63 33.47 -83.87 -5.4127.98 -47.21 -95.62 -48.84 -7.59-25.66 -25.93 -89.65 -47.23 -51.228.90 2.41 9.45 -88.69 -88.93-28.65 20.15 7.65 -104.52 -41.88
(a) Gain Af G
15101520
Prompt Length15101520Pool Size
-50.61 -77.04 -30.07 -29.23 -43.45-90.02 -60.39 -69.77 -34.52 -9.73-36.96 -59.20 -59.15 -44.32 -121.87-63.75 29.31 -41.85 -97.26 -75.21-20.09 21.15 -44.86 -56.18 -125.51 (b) Gain Af L
Figure 5: Sensitivity analyses of prompt length and prompt
pool size. Left: The performance improvement (%) of coarse-
grained global prompts after the aggregation of global
prompts w.r.t. prompt length 𝐿and prompt pool size 𝑀.
Right: The performance improvement (%) of fine-grained
local prompts after the aggregation of global prompts w.r.t.
prompt length 𝐿and prompt pool size 𝑀.
Fig. 6 illustrates the temporal knowledge retention of the global
model and local models. As illustrated in the left sub-figure Fig. 6(a),
the performance of global spatial knowledge retention exhibits
robustness to different values of prompt length and prompt pool size,
which means FedMGP achieves spatial-temporal transfer effectively.
From Fig. 6(b), we can conclude that different values of prompt pool
size and prompt length have little effect on local spatial knowledge
retention, indicating that FedMGP mitigates temporal catastrophicforgetting. It implies utilizing multi-granularity prompts is capable
of training a generalized global model as well as personalized local
models.
15101520
Prompt Length15101520Pool Size
32.81 31.18 31.07 33.96 32.5533.01 31.94 31.88 34.76 30.9831.41 31.98 32.09 30.27 28.2030.10 32.16 29.12 28.84 32.6730.80 31.25 32.51 29.57 29.72
(a) Global𝐾𝑅𝑡
15101520
Prompt Length15101520Pool Size
69.55 69.35 72.29 69.31 71.7171.13 75.57 71.43 72.95 74.6875.93 71.01 67.83 73.33 71.8670.45 70.19 73.34 70.70 71.0773.07 74.45 72.87 72.07 73.06 (b) Local𝐾𝑅𝑡
Figure 6: Sensitivity analyses of prompt length and prompt
pool size. Left: Global Temporal Knowledge Retention Ratio
(%) w.r.t. prompt length 𝐿and prompt pool size 𝑀. Right:
Local Temporal Knowledge Retention Ratio (%) w.r.t. prompt
length𝐿and prompt pool size 𝑀.
3 BASELINES
FedAvg [31]: FedAvg is a fundamental algorithm in federated learn-
ing. It works by first distributing a global model to multiple clients.
Each client trains the model locally using its own data for a few
epochs. Then, the clients send their locally updated models back
to a central server. The server aggregates these local models by
computing their weighted average to update the global model. This
process is repeated for several rounds until the global model con-
verges.
FedEWC [18]: a combination of FedAvg and EWC, which is a com-
monly used baseline in PFL and FCL. EWC is a regularization-based
CL method, mitigating forgetting by penalizing the changes of im-
portant parameters of the previous tasks.
FedProx [23]: a heterogeneous and static FL method. It smooths
data heterogeneity by adding a proximal term in the local objective.
GLFC (Glocal Local Forgetting Compensation) [ 7]: a synchronous
FCIL method. GLFC designs a class-aware gradient compensation
loss and a class-semantic relation distillation loss to mitigate forget-
ting and distill consistent inter-class relations across tasks. A proxy
server is implemented to select the optimal previous global model
to assist the class-semantic relation distillation and a prototype
gradient-based communication mechanism is developed to protect
data privacy.
FedViT [8]: a hybrid method of ViT and FedAvg. The global ag-
gregation is performed by computing the average weights of the
classification heads.
FedL2P [41]: a hybrid method of L2P and FedAvg. L2P is a prompt-
based CL method, which applies learnable task-specific prompts to
mitigate forgetting.
FedDualP [40]: a hybrid method of DualPrompt and FedAvg. Du-
alPrompt, a prompt-based CL method derived from L2P, decouples
the learnable prompts into general and expert prompts, encoding
task-invariant and task-specific knowledge, respectively.
4034