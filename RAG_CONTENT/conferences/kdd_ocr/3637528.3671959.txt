Explanatory Model Monitoring to Understand the Effects of
Feature Shifts on Performance
Thomas Decker∗
Ludwig-Maximilians-Universität
Munich, Germany
Siemens AG
Munich, Germany
thomas.decker@siemens.comAlexander Koebler∗
Goethe University Frankfurt
Frankfurt, Germany
Siemens AG
Munich, Germany
alexander.koebler@siemens.comMichael Lebacher
Siemens AG
Munich, Germany
michael.lebacher@siemens.com
Ingo Thon
Siemens AG
Munich, Germany
ingo.thon@siemens.comVolker Tresp
Ludwig-Maximilians-Universität
Munich, Germany
Munich Center for Machine Learning
Munich, Germany
volker.tresp@lmu.deFlorian Buettner†
Goethe University Frankfurt
Frankfurt, Germany
Siemens AG
Munich, Germany
florian.buettner@dkfz.de
Abstract
Monitoring and maintaining machine learning models are among
the most critical challenges in translating recent advances in the
field into real-world applications. However, current monitoring
methods lack the capability of provide actionable insights answer-
ing the question of why the performance of a particular model really
degraded. In this work, we propose a novel approach to explain the
behavior of a black-box model under feature shifts by attributing an
estimated performance change to interpretable input characteristics.
We refer to our method that combines concepts from Optimal Trans-
port and Shapley Values as Explanatory Performance Estimation
(XPE). We analyze the underlying assumptions and demonstrate
the superiority of our approach over several baselines on different
data sets across various data modalities such as images, audio, and
tabular data. We also indicate how the generated results can lead
to valuable insights, enabling explanatory model monitoring by
revealing potential root causes for model deterioration and guiding
toward actionable countermeasures.
CCS Concepts
•Computing methodologies →Machine learning approaches.
Keywords
Model Monitoring, Optimal Transport, Explainable AI, Performance
Estimation, Shapley Values
∗Both authors contributed equally to this research.
†Also with German Cancer Research Center.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671959
Figure 1: Illustration of an opaque vision system subject to
hardware degradation or environment changes, e.g., speckles
on the lens or stray light. Explanatory Performance Estima-
tion (XPE) allows to anticipate and explain the resulting per-
formance decrease by highlighting which parts of the shift
are harmful. This provides actionable insights to restore per-
formance and facilitate effective model maintenance.
ACM Reference Format:
Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, Volker
Tresp, and Florian Buettner. 2024. Explanatory Model Monitoring to Un-
derstand the Effects of Feature Shifts on Performance. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671959
1 Introduction
Deploying Machine Learning (ML) models successfully in prac-
tice is a challenging endeavor as it requires models to cope well
with complex and dynamic real-world environments [ 69,81]. As a
consequence, monitoring and maintaining ML-models has been es-
tablished as a central pillar of the modern ML-Life cycle [ 63,85] and
commercial ML frameworks [ 67]. A crucial assumption to assure
the validity of a model is that the data distribution during training
matches the real-time distribution during deployment. However,
 
550
KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
this assumption might be violated in real-world applications for
various reasons. For instance, data integrity issues such as hard-
ware deterioration or modifications in the collection and processing
pipeline could cause a mismatch as well as intrinsic changes in the
data generation process due to novel real-world circumstances.
Since any potential discrepancy might compromise the reliability
of predictions, continuous assessment of the model and its input
data is required. For this purpose, many different approaches have
been proposed [ 75] that can conceptually be divided into two main
categories. Performance monitoring methods [ 28,60] enable sys-
tematic tracking of the model performance over time and provide
an early indication of significant model deterioration. However,
such approaches typically require access to ground truth labels at
inference time, which is usually infeasible or rather expensive to
obtain. In contrast, unsupervised data drift detection [ 31,76] quan-
tifies to which extent the input data characteristics have changed to
identify distribution shifts irrespective of the actual performance.
While such approaches can help reveal potential problems and pre-
vent unreliable predictions, they are not capable of providing truly
actionable insight about why the model might be invalid, how a
distribution shift specifically harms the model, and how to opti-
mally counteract it [ 25,82]. More specifically, many different root
causes could underlie an observed distribution shift with individual
implications. While expensive retraining might be unavoidable in
case of an intrinsic change in the relationship between input data
and output labels, it would be ineffective for mitigating problems
arising from hardware failure such as a defective sensor. Moreover,
due to modern machine learning systems’ complex and opaque
nature, it remains unclear which characteristics of the shifted distri-
bution are responsible for the model failure and we refer to this task
as quantifying feature shift importance. In this work, we introduce
a new approach coined Explanatory Performance Estimation (XPE)
that systematically addresses the desired needs of actionable model
monitoring in practice. In particular, we propose a framework that
anticipates the performance change caused by an observed distri-
bution shift and guides experts toward potential root causes and
actions. More specifically, we make the following contributions:
•We propose Explanatory Performance Estimation (XPE) as a
novel and agnostic approach to reveal the specific features
through which an observed distribution shift affects model
performance in the absence of labels.
•We define multiple innovative metrics to quantitatively eval-
uate the efficacy of feature shift importance methods in im-
proving model monitoring.
•We demonstrate that XPE outperforms several baselines via
extensive experiments mimicking real-world shifts such as
data quality issues, hardware degradation, and selection bias
on three different data modalities.
2 Problem Setting
We consider the common situation where a machine learning model
𝑓:X→Y has been trained to perform a prediction task in a super-
vised fashion based on labeled training data {(𝑥𝑖𝑠,𝑦𝑖𝑠)}𝑛𝑠
𝑖=1. Further,
we assume that the training data originates from a source distri-
bution denoted as 𝑃𝑠(𝑋,𝑌). At some point during deployment, we
suppose that the underlying data distribution changes and furtherequals to the target distribution 𝑃𝑡(𝑋,𝑌)with𝑃𝑡(𝑋,𝑌)≠𝑃𝑠(𝑋,𝑌).
As common in practice, we suppose that during deployment we
only have access to unlabeled data instances {𝑥𝑖
𝑡}𝑛𝑡
𝑖=1originating
from the marginal target distribution 𝑃𝑡(𝑋). As a motivational ex-
ample, consider the situation in Figure 1 where a black-box model
processing images is monitored. During deployment, the data dis-
tribution changes as a consequence of hardware degradation, e.g.,
speckles on the camera lens, causing a feature drift in some image
areas. However, the degree of model robustness under a distribution
shift might vary across image areas and individual features might
be more important to the system in general. Hence, a shift might
actually hurt the model’s performance only through very specific
image areas and revealing those provides valuable information for
efficient model monitoring and maintenance.
To achieve this, XPE must accomplish two things in a data and
model-agnostic manner. First, it should estimate the model’s per-
formance under 𝑃𝑡(𝑋,𝑌)despite missing target labels. Second, it
should attribute the anticipated performance change to specific in-
put features through which the distribution shift affects the model.
3 Background and Related Work
Optimal Transport. Optimal transport [ 74,93] refers to the math-
ematical problem of identifying the most cost-efficient way to trans-
form one probability measure 𝜇into another one 𝜈. Consider two
measurable spaces(X1,𝜇)and(X2,𝜈)and a non-negative cost func-
tion𝑐:X1×X 2→R+. The Monge formulation aims to find a
deterministic transportation map 𝑇:X1→X 2solving
inf
𝑇∫
X1𝑐(𝑥,𝑇(𝑥))𝑑𝜇(𝑥)s.t.𝑇#𝜇=𝜈
where𝑇#𝜇describes the push forward measure resulting from proba-
bility mass transfer from 𝜇with respect to 𝑇, so𝑇#𝜇(𝑥)=𝜇(𝑇−1(𝑥)).
In general, the existence of such a deterministic map is not guaran-
teed, but the relationship between 𝜇and𝜈can also be expressed
via a probabilistic coupling. Mathematically, such a coupling 𝜋rep-
resents any joint distributions over (X1×X2)with marginals equal
to𝜇and𝜈. This leads to a relaxed problem corresponding to the
Kantorovich formulation of optimal transport:
inf𝜋∫
X1×X2𝑐(𝑥1,𝑥2)𝑑𝜋(𝑥1,𝑥2)
Note, that a cost-optimal coupling can be shown to exist under mild
theoretical assumptions [ 93] and can easily be estimated based on
empirical samples via linear programming [ 74]. More advanced
estimation techniques have been proposed to ensure scalability
[21,32,57], to account for known structure within the data [ 2,3,89]
or to make the estimation more robust [ 6,72,73]. Concepts and
tools originating from optimal transport theory have been applied
to various areas of machine learning concerned with modeling
and relating data distributions. This covers for instance generative
models [ 5,47,78,80] or the evaluation of the semantic correspon-
dence between documents [ 41,54,99], images [ 50,59,101] and
spatio-temporal data [ 42]. Another related application of optimal
transport is unsupervised domain adaptation [ 16,18,23], where
the goal is to leverage labeled data from a source domain to obtain
models that generalize well to an unlabeled target domain. There
also exist proper extensions for cases where a limited number of
 
551Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance KDD ’24, August 25–29, 2024, Barcelona, Spain.
target labels is available [ 17,95] or to improve adaptation results in
the presence of general types of data shifts [ 49,77]. The utilization
of optimal transport that is most related to our work is given by
[53]. The authors propose to estimate interpretable transport plans
to rather explain the nature of distribution shifts irrespective of
their actual effect on a given model.
Feature attribution methods. Feature attribution methods quan-
tify to what extent individual input features have contributed to
a model output of interest and have been established as a popular
mean to make the predictions of black-box models more transparent
[1]. More specifically, such methods aim to determine an impor-
tance vector 𝜙such that𝜙𝑖quantifies the influence that each input
feature𝑥𝑖had on a scalar-valued model output. Removal-based
methods [ 19] are a particular kind of attribution techniques that
achieve this by simulating the absence of features and evaluating
the resulting prediction changes using different computational ap-
proaches. A prominent example are Shapley Values [ 62], which
have been introduced as a fair way to distribute the total outcome
of a coalition game to individual players 𝐷={1,...𝑑}. In this con-
text, a game can be specified via a value function 𝑣(𝐾): 2𝐷→R
that quantifies the value that each possible subset or coalition of
players𝐾⊆𝐷would achieve if only they would contribute. Given
a value function, the Shapley Value of each player 𝑖∈𝐷results as
a weighted average of its marginal contributions over all possible
coalitions and orders:
𝜙𝑖=∑︁
𝐾⊆𝐷\{𝑖}1
 𝑑−1
|𝐾|𝑑(𝑣(𝐾∪{𝑖})−𝑣(𝐾))
To attain feature attribution for a model 𝑓and input𝑥∈R𝑑, indi-
vidual features 𝑥𝑖,...,𝑥𝑑resemble players and 𝑣(𝐾)defines the hy-
pothetical model prediction where only features in 𝐾⊂[𝑑]would
be present. Different computational methods have been proposed to
enable such a value function by simulating model predictions under
partial feature absence [ 19,87]. A prominent one is to integrate
affected features out based on the marginal data distribution. For
an index set 𝐾and decomposition 𝑥=(𝑥𝐾,𝑥𝐾𝑐), this results in
𝑣𝑃(𝐾)=∫
𝑓(𝑥𝐾,𝑋𝐾𝑐)𝑑𝑃(𝑋𝐾𝑐)≈𝑚∑︁
𝑖=1𝑓(𝑥𝐾,𝑥(𝑖)
𝐾𝑐)
which can easily be estimated using {𝑥𝑖}𝑚
𝑖=1samples from 𝑃(𝑋).
A popular alternative is to simply replace features with a single
uninformative baseline value ¯𝑥such that:
𝑣¯𝑥(𝐾)=𝑓(𝑥𝐾,¯𝑥𝐾𝑐).
Based on such strategies for feature removal, different algorithmic
approaches have been developed to compute corresponding fea-
ture attributions efficiently [ 12]. Moreover, Shapley Values have
been shown to satisfy desirable theoretical properties related to fair
credit allocations in game theory and are model and data-agnostic.
Hence, they can automatically be applied to explain any machine
learning model and any kind of model output. In particular, they
can also directly be used to explain how individual features have
contributed to the overall model performance [ 20,61]. As a conse-
quence, Shapley Values enjoy great popularity among practitioners
[10] and they have further been employed for a wide variety of
different applications related to machine learning [79].Feature attributions for model monitoring. Major ML frameworks
such as Amazon SageMaker Model Monitor [ 67] or Google Vertex
AI Model Monitoring [ 88] offer monitoring of feature attributions
and interpret changing importance scores as an indicator for po-
tential performance degradation. Mougan et al . [65] demonstrate
on synthetic tabular examples that monitoring attribution results
can be superior compared to monitoring input data characteristics.
However, it remains unclear under which circumstances this ap-
proach can reliably signal an actual performance decrease and it
does not provide any insights regarding why the model might have
deteriorated. Another approach is to simply combine drift detection
with attribution methods and expect a performance change if an
important feature shifts [ 45]. But this can be misleading when the
model is robust to the shift occurred in these features. On top of
that, attribution methods themselves can be sensitive to shifts and
small perturbation [ 33,48,84], so simply evaluating them on drifted
data may produce unreliable results. Budhathoki et al . [11] leverage
Shapley Values to identify potential reasons for a distribution shift
based on causal graphs and [ 100] apply this idea in the context of
model monitoring. While theoretically appealing, these approaches
heavily rely on complete knowledge about the causal mechanisms
of the true data-generating process which is infeasible to attain in
practice. Therefore, these methods cannot be utilized to support
monitoring in general deployment scenarios.
Label-free performance estimation. Reliably estimating the per-
formance of a machine learning model under distribution shifts in
the absence of target labels is a significant yet considerably hard
problem. In fact, the task has been shown to be impossible when
allowing for arbitrary shifts [ 8,24,29]. However, it becomes feasible
if additional assumptions can be posed either on the scope of the
shift, the relationship between source and target domain or on the
model itself. For instance, importance weighting can be used to
estimate the target performance if the experienced change is either
due to a covariate shift [ 13,83,86] or resembles a label shift [ 30,58].
In cases where the model outputs probabilistic predictions that are
well calibrated [ 36,38] on the target domain, the corresponding
performance can be anticipated purely based on the model’s confi-
dence [ 37,44]. But calibration under domain shifts is particularly
challenging and hence an area of active research [68, 90, 91].
4 Quantifying Feature Shift Importance
In this section, we formalize the task of feature shift importance,
which aims at revealing through which specific features an observed
distribution shift affects a model.
Aligning distributions via optimal transport. A natural way to
gain a better understanding of how a distribution shift precisely
impacts the predictions of a machine learning model is to systemat-
ically compare its individual predictions before and after the shift
happened. For this purpose, suppose that the experienced distribu-
tion shift can be expressed by a functional transformation 𝑇. More
precisely, this means that the target domain distribution 𝑃𝑡can be
obtained from the source distribution 𝑃𝑠via push-forward opera-
tion:𝑃𝑡=𝑇#𝑃𝑠. In this case, the immediate effect on a single model
prediction𝑓(𝑥)that is purely induced by the distribution shift can
be analyzed by comparing the corresponding predictions 𝑓(𝑥)and
 
552KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
Figure 2: Overview of our proposed framework for Explanatory Performance Estimation (XPE): a) Based on Optimal Transport,
an optimal coupling is estimated to sample-wise align empirical source and target distributions. b) For a given target sample 𝑥𝑡
the conditional coupling ˆ𝜋(𝑋𝑠|𝑥𝑡)indicates the most likely version of 𝑥𝑡in the source domain denoted by ˆ𝑇−1(𝑥𝑡). c) Given pre
and post-shift version ˆ𝑇−1(𝑥𝑡)and𝑥𝑡one can restrict shifts to individual input feature subsets 𝐾𝑖and d) simulate partial feature
shifts ˆ𝑇−1
𝐾𝑐
𝑖(𝑥𝑡)by replacing 𝑥𝑡with ˆ𝑇−1(𝑥𝑡)outside the considered regions. e) Finally, all simulated partial feature shifts can be
aggregated to quantify how individual feature shifts have contributed to the anticipated model loss based on Shapley Values.
𝑓(𝑇(𝑥)). Modern deep neural networks have demonstrated impres-
sive capabilities to parameterize functional transformations that
perform complex and realistic distribution shifts [ 70,71]. However,
they typically require a lot of data to be trained and might even
rely on labeled source/target pairs. During deployment, there is
usually only a limited number of discrete samples from the source
and target domain available. Therefore, optimal transport provides
a more suitable way to estimate the relationships between 𝑃𝑠and
𝑃𝑡in practice. Suppose we have 𝑛𝑠samples randomly drawn from
the source domain Ω𝑠={𝑥𝑖𝑠}𝑛𝑠
𝑖=1and𝑛𝑡from the target domain
Ω𝑡={𝑥𝑖
𝑡}𝑛𝑡
𝑖=1. Let𝛿𝑥be the Dirac delta function, describing a valid
probability distribution concentrated at the point 𝑥, then the em-
pirical source and target distributions are given by:
ˆ𝑝𝑠=∑︁
𝑥𝑠∈Ω𝑠1
𝑛𝑠𝛿𝑥𝑠ˆ𝑝𝑡=∑︁
𝑥𝑡∈Ω𝑡1
𝑛𝑡𝛿𝑥𝑡
The space of possible couplings comprises Π={𝜋∈R𝑛𝑠×𝑛𝑡|𝜋1𝑛𝑡=
ˆ𝑝𝑠,𝜋𝑇1𝑛𝑠=ˆ𝑝𝑡}and the discrete Kantorovich formulation to match
source and target samples via optimal transport yields an optimal
coupling ˆ𝜋:
ˆ𝜋=arg min
𝜋∈Π∑︁
𝑥𝑠∈Ω𝑠∑︁
𝑥𝑡∈Ω𝑡𝑐(𝑥𝑠,𝑥𝑡)𝜋(𝑥𝑠,𝑥𝑡)
Intuitively, ˆ𝜋provides a probabilistic estimate of how samples of the
source domain are likely to look in the target domain and vice versa
(see Fig. 2a). This equips us with an appealing tool to comprehend
the precise nature of the shift and can further be utilized to reveal
how an observed shift affected a model. In this case, understanding
the impact of a distribution shift for a single prediction 𝑓(𝑥𝑡)could
be achieved by comparing it with all predictions corresponding
to the potential source version of 𝑥𝑡as implied by the conditional
coupling ˆ𝜋(𝑋𝑠|𝑥𝑡)(Fig.2b). Moreover, it is straightforward to trans-
form a probabilistic alignment into a deterministic one by matching
each source sample with its most related target sample. This results
in a transform ˆ𝑇(𝑥𝑠)=arg max𝑥𝑡∈Ω𝑡ˆ𝜋(𝑥𝑡|𝑥𝑠)and equivalently
ˆ𝑇−1(𝑥𝑡)=arg max𝑥𝑠∈Ω𝑠ˆ𝜋(𝑥𝑠|𝑥𝑡)mapping each 𝑥𝑡onto its most
likely source version. In general, the resulting coupling depends onthe chosen cost function 𝑐, but the squared Euclidean distance is a
popular default choice for various applications including domain
adaptation [ 18]. In contrast to domain adaptation, we rather seek
to understand how a shift impacts a model in order to determine if
and specifically why adaptation during deployment might be nec-
essary. To achieve this, we propose a novel way of combining the
sample-wise alignment of 𝑃𝑡and𝑃𝑠implied by optimal couplings
with feature attribution methods.
Shapley Values for Feature Shift Importance. In order to better
understand how an observed input feature shift influenced a model
prediction we propose to consider a novel coalition game where
the value function 𝑣(𝐾)expresses the model prediction under the
assumption that only features in 𝐾experienced the shift (Fig.2c). As
introduced above, Optimal Transport allows us to identify potential
pre- and post-shift versions of data instances related to the empiri-
cal source and target distributions. The corresponding results can
directly be utilized to perform the required partial distribution shifts
of a given target sample 𝑥𝑡(Fig.2d). Leveraging the probabilistic
relationship expressed by the transport coupling 𝜋we get:
𝑣𝜋(𝐾)=∫
𝑓 𝑥𝑡,𝐾,𝑋𝑠,𝐾𝑐𝑑𝜋(𝑋𝑠,𝐾𝑐|𝑋𝑡,𝐾=𝑥𝑡,𝐾)
where𝐾𝑐is the complement of the index set 𝐾and𝑥𝑡,𝐾denotes all
entries of𝑥𝑡with index in 𝐾. If the shift is due to a transformation
𝑇, then the desired value function is given by:
𝑣𝑇(𝐾)=𝑓(𝑇−1
𝐾𝑐(𝑥𝑡))with𝑇−1
𝐾𝑐(𝑥𝑡)=(𝑥𝑡,𝐾,𝑇−1(𝑥𝑡)𝐾𝑐)
If𝑇is directly inferred from a probabilistic coupling 𝜋like de-
scribed above, then 𝑣𝑇resembles a computationally more efficient
approximation of 𝑣𝜋, which is equivalent if the conditional coupling
ˆ𝜋(𝑥𝑠|𝑥𝑡)allocates all probability mass to a single source sample.
When computing Shapley Values for such value functions, 𝜙𝑖can
be interpreted as a measure of how the empirical shift in feature 𝑖
contributed to the shift-related prediction change (Fig.2e). More-
over, carefully comparing 𝑣𝜋and𝑣𝑇with existing value functions
described in Section 3 reveals a close relationship. While 𝑣𝑃(𝐾)and
𝑣¯𝑥(𝐾)are designed to compute partial feature absence for basic
 
553Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance KDD ’24, August 25–29, 2024, Barcelona, Spain.
feature attribution, our proposed versions 𝑣𝜋(𝐾)and𝑣𝑇(𝐾)are
explicitly tailored to obtain feature shift importance by simulating
partial feature shifts. This observation ensures that the resulting
Shapley Values{𝜙𝑖}𝑑
𝑖=1inherit a variety of desirable theoretical
properties [ 43,87,100], efficient computation strategies [ 12] and
the relationship to other removal-based explanations [19].
5 Explanatory Performance Estimation (XPE)
Anticipating performance changes during deployment. When a
shift occurs, it is critical to reevaluate whether the model still per-
forms sufficiently well under the new circumstances. Reliably com-
puting the performance of a model would require access to cor-
responding ground truth labels. Such information is typically not
available during deployment and usually requires cumbersome man-
ual efforts. However, empirically aligning labeled source samples Ω𝑠
and unlabeled target samples Ω𝑡via transformation 𝑇also equips
us with a reasonable way to anticipate the performance by suppos-
ing that all linked instances exhibit the same label. More precisely,
one can obtain for any 𝑥𝑡∈Ω𝑡a label estimate ˆ𝑦𝑡by allocating
the known label of the linked source sample 𝑇−1(𝑥𝑡)∈Ω𝑠. Under
appropriate assumptions on the nature of the shift, this heuristic
will accurately estimate the performance in the target domain.
Definition 5.1. Let X𝑠×Y𝑠,𝑃𝑠(X𝑠,Y𝑠),(X𝑡×Y𝑡,𝑃𝑡(X𝑡,Y𝑡))be
two probability spaces corresponding to the source and target do-
mains. For measurable sets 𝐴, let𝑑𝑇𝑉(𝜇,𝜈)=sup𝐴|𝜇(𝐴)−𝜈(𝐴)|be
the statistical total variation distance between two probability mea-
sures𝜇and𝜈. A distribution shift from 𝑃𝑠to𝑃𝑡is𝜺-approximate
label-preserving with respect to 𝑇if there exists
•a functional Transformation 𝑇:X𝑠→X𝑡such that𝑃𝑡(𝑋𝑡)=
𝑃𝑠(𝑇−1(𝑋𝑡))and additionally
•𝑑𝑇𝑉 𝑃𝑡(𝑌|𝑇(𝑥𝑠)),𝑃𝑠(𝑌|𝑥𝑠)≤𝜀∀𝑥𝑠∈X𝑠
If the observed distribution shift is approximately label-preserving,
then the estimated performance via label transport is close to the
actual one in the target domain:
Theorem 5.2. LetL𝑡be the true target performance of a model
𝑓expressed via a loss function Lwith∥L∥∞≤1. DenotecL𝑇
𝑡as
estimated target performances resulting from label transfer via trans-
formation𝑇. If the observed distribution shift is 𝜀-approximate label-
preserving, then:
|L𝑡−cL𝑇
𝑡|≤2𝜀
The proof and further details are given in the Appendix. Intu-
itively, this establishes a continuity result, showing that if the deci-
sion boundary for transported samples in the target domain is close
to the original one, then the estimation error can also be expected
to be small. This assumption might seem restrictive in general, but
we want to highlight that label-free performance estimation only
becomes feasible in the presence of additional constraints. Relying
specifically on this assumption resonates quite well with typical
real-world causes for distribution shifts during deployment: Phys-
ical changes in the environment, hardware degradation, or other
data quality issues can all be considered as shifts that modify input
data characteristics without necessarily affecting the label.Explanatory Performance Estimation using Shapley Values. Our
proposed Explanatory Performance Estimation (XPE) approach
combines transport-based label estimation with feature shift impor-
tance in the following way. Given a loss function L:Y×Y→ R+
we define a new value function 𝑣XPE(𝐾)=L
𝑓 ˆ𝑇−1
𝐾𝑐(𝑥𝑡),ˆ𝑦𝑡
which expresses directly the anticipated performance change under
partial feature shifts. The corresponding Shapley Values 𝜙XPEfinally
indicate through which specific features an observed distribution
shift impacts the anticipated performance, providing valuable infor-
mation regarding potential root causes of model degradation. For
classification models that output an entire probability distribution
over possible classes 𝐶, we further propose a label-estimation-free
variation of XPE, which we call Explanatory Performance Proxy
Estimation (XPPE). Let 𝐻(𝑓(𝑥))=−Í
𝑐∈𝐶𝑓𝑐(𝑥)log(𝑓𝑐(𝑥))be
the entropy of the probabilistic model output 𝑓(𝑥)capturing the
model’s degree of uncertainty about its prediction. By considering
higher predictive uncertainty as a proxy for potential model degra-
dation we define a novel value function 𝑣XPPE(𝐾)=𝐻(𝑓(ˆ𝑇−1
𝐾𝑐(𝑥𝑡)).
It computes the model uncertainty under partial shifts and the re-
sulting Shapley Values 𝜙XPPEsignal through which features a shift
has affected the model’s confidence, which does not require a label
estimate. Thus, XPPE can be superior to XPE when the shift does
not satisfy Def. 5.1 of being approximately label preserving.
6 Experiments
The goal of our experiments is to rigorously analyze feature shift at-
tributions and their capabilities to reliably explain the model behav-
ior under deployment-related distribution shifts. In total, we report
the results on nine different datasets covering three different data
modalities with associated data shift scenarios. In Section 6.1, we
analyze four different vision tasks and demonstrate that our method
is the most effective way to understand a performance decrease
caused by shifts resembling hardware degradations. In Section 6.2
we show, for a speech classification task, that only our proposed
methods can reliably signal an induced selection bias. Finally, in
Section 6.3 we evaluate four tabular data sets and demonstrate the
capabilities of our method to correctly assign a performance de-
cline due to data quality issues in the form of missing values. More
details are documented in the Appendix and code is provided at
https://github.com/thomdeck/xpe.
Baselines. To assess the capabilities of XPE we first formalize
baselines that are connected to existing model monitoring practices.
Remember that XPE aims to evaluate by which specific features an
observed distribution shift impacts the model performance. A first
baseline for this purpose is to simply check whether predictions in
the target domain tend to rely on other features compared to the
source domain. Let 𝜙(𝑥)be the outcome of standard Shapley Values
explaining the prediction 𝑓(𝑥)for an instance 𝑥. Given an estimated
transportation map ˆ𝑇, we can simply compare the explanations of
two matched samples individually and define a local attribution
difference (LAD):
𝜙LAD(𝑥𝑡,ˆ𝑇)=|𝜙(𝑥𝑡)−𝜙 ˆ𝑇−1(𝑥𝑡)|
If this difference is large for a specific feature, then the distribution
change in this feature might be particularly harmful. Note, that this
method also relates to the existing practice of monitoring changes
 
554KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
in feature attributions [ 65,67,88] proposed to detect model degra-
dation during deployment. A second baseline that does not require
alignment of source and target samples is first to perform unsuper-
vised drift detection and consider only shifted features which are
also important for the model as intermediaries of the shift. For this
purpose, we leverage a two-sided Kolmogorov-Smirnov (KS) test to
assess whether the distribution of each feature within the source
samples is significantly different from the corresponding one of
the target samples. Let 𝑀KS∈{0,1}𝑑be a binary mask indicating
which of the 𝑑input features have drifted according to the KS-test.
Then, the Attribution×Shift (AxS) baseline reads:
𝜙AxS(𝑥𝑡,𝑀KS)=𝜙(𝑥𝑡)⊙𝑀KS
Note, that this baseline captures the idea of considering only fea-
tures that have drifted and are simultaneously important for the
model as potentially harmful gateways of the observed distribution
shift mentioned in [45].
Defining suitable metrics. Quantitatively evaluating any kind of
model explanation is challenging, but a desirable property is faith-
fulness [ 9]. It generally tries to asses if perturbing features with
high attribution scores also cause coherent prediction changes and
a variety of different related metrics have been proposed [ 4,9,97].
To evaluate feature shift attributions, we reformulate the faithful-
ness criterion in the following way: When features with high shift
attributions are shifted back, we expect the model performance to
recover equivalently. Suppose access to the true pre-shift version
𝑇−1(𝑥𝑡)of a target sample 𝑥𝑡as well as to the ground truth source
and target labels 𝑦𝑠and𝑦𝑡. Then, we can define Shift-Faithfulness
(S-Faith) of a feature shift attribution 𝜙Shiftas the correlation be-
tween the actual performance change under partial feature shift
and the sum of allocated shift importance:
S-Faith =corr
𝐾∈(𝑑
|𝐾|) ∑︁
𝑖∈𝐾𝜙Shift
𝑖,L 𝑓(𝑥𝑡),𝑦𝑡−L 𝑓 𝑇−1
𝐾(𝑥𝑡),𝑦𝑠!
Here we adapted the metric based on the notation from [ 9], where
the correlation is computed using different feature subsets 𝐾with
fixed size|𝐾|. Another popular metric to measure the quality of fea-
ture attributions is RemOve And Retrain (ROAR) [ 40] assessing if
the performance actually decreases when important features are re-
moved and models retrained. Consequently, we propose an adapted
metric coined remove, retrain, and shift (ROAR-S), which evalu-
ates whether the performance decrease caused by a shift diminished
if features with high shift importance are removed and the model
retrained. More precisely, we define the ROAR-S score as the pro-
portion of shift-induced performance decrease that remains when
for each instance the top 5%of input features highlighted by 𝜙Shift
are removed, and the model is subsequently retrained. If this score
is small, the distribution change no longer affects the performance
and the shift importance is reliable. More details about this metric
are deferred to the Appendix. To quantify the actionability of ex-
planations we consider the Complexity(Cpx) metric [ 9], which
is defined as the Shannon entropy of the normalized attribution
values: Cpx(𝜙Shift)=𝐻(|𝜙Shift|/Í
𝑖|𝜙Shift
𝑖|). This expresses the un-
certainty of shift attribution results across all input features and
lower values indicate that the method communicates the potential
reason for model degradation more concisely.6.1 Understanding the effects of hardware
degradation
For the first experiment we consider a variety of popular light-
weight image datasets, including MNIST [ 55], images of fashion
items [ 94] as well as various types of medical images [ 96], and simu-
lated several distribution shifts mimicking potential camera-related
hardware degradation or physical changes in the environment [ 66].
This setup ensures complete knowledge about the true pre- and
post-shift pairs which is crucial for evaluating the quality of shift
attribution methods via Shift-Faithfulness. For each dataset, we
fitted a LeNet model [ 55], evaluated the metrics based on 500test
samples and used the cross-entropy loss as a performance measure.
The average results are reported in Table 1 and indicate that XPE
almost consistently outperforms all baselines followed by XPPE as
second best. Most of the time, all other baselines are not correlated
at all with the true performance change induced by the shift of high-
lighted features. The corresponding results for Complexity imply
that the explanations of XPE also tend to be the most concise given a
sufficient degree of faithfulness. For cases where other methods pro-
vide significantly less complex results, they typically have almost
no correlation with the actual performance decrease. Moreover,
the label transport accuracy (ˆ𝑦𝑡=𝑦𝑡)was for all considered shifts
>85%. This is in line with the Theorem above as applying a corrup-
tion to an image equals a functional transformation that preserves
its label and shows that aligning via optimal transport is capable
of apprehending the considered transformations. To confirm our
findings, we also evaluated ROAR-S and the results demonstrate
that the performance change caused by the shift is on average best
mitigated when dropping features according to XPE and XPPE.
Deriving intuitive and actionable insights. Next, we would like
to demonstrate how the results obtained via XPE can yield novel
and actionable insight into the model behavior under distribution
shifts concerning images. To do so, we locally examine shift attri-
butions on the MNIST digit and the pneumonia detection dataset
(PneumM) where a certain shift had a particularly harmful effect
on the prediction and seek to understand the reasons. In Figure 3
and Figure 4 we plot some of these examples and notice that for
such instances, the shifts do indeed perturb essential image regions
in a way that suggests a different class, i.e., a different digit or a
positive pneumonia classification. By consulting the different shift
attribution results to narrow down a concrete reason we see that
only XPE consistently highlights the parts of the corruption that
actually alter the appearance of a digit towards a different one or
resemble white spots in the lung area indicating pneumonia. This
shows that also the model is mainly misled by the intuitive regions,
which cannot be concluded from the other results. This information
can help end-users take efficient and targeted corrective measures
for their applications, such as cleaning or repairing the camera lens
or removing ambient light sources.
Supporting model selection. Moreover, such analysis can also be
used to compare how different models handle certain distribution
shifts. In Figure 5 we visualize some results of XPE for the LeNet
analyzed above and a Multi-Layer-Perceptron (MLP) with one hid-
den layer. The plots illustrate that the LeNet’s predictions mainly
get harmed through the shift of features in close proximity to the
 
555Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance KDD ’24, August 25–29, 2024, Barcelona, Spain.
brightness
contrast dotted fog gaussian impulse spatter zigzag AVG
S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓S-Faith↑Cp
x↓RO
AR-S↓MNISTXPE 0.45 4.03 0.53 5.91 0.82 2.55 0.52 5.88 0.71 3.71 0.82
2.62 0.69
4.32 0.78 3.38 0.37
XPPE 0.49 4.69 0.48
5.67 0.70 2.53 0.49
5.96 0.61 3.52 0.77
2.64 0.51
4.44 0.50 3.33 0.78
LAD 0.01
5.62 0.32 5.16 0.22
4.70 0.15 5.69 0.11
5.25 0.16
4.67 0.21
5.05 0.13
4.82 2.12
AxS -0.04
5.42 0.24
5.81 0.31
3.12 0.11
5.92 0.07
4.92 0.11
2.73 0.26
4.91 0.21
3.82 1.87FashionMXPE 0.58
5.86 0.58
5.91 0.79
2.82 0.72
5.97 0.73
5.24 0.79 3.19 0.72
4.85 0.82
3.31 0.20
XPPE 0.20
5.88 0.04
6.01 0.57
2.90 0.06 5.97 0.50
5.35 0.57
3.33 0.42
4.92 0.49
3.65 0.55
LAD 0.00
6.12 0.06
6.05 0.04
5.63 -0.00
6.11 0.04
5.96 0.02
5.57 0.03
5.82 0.05
5.67 0.77
AxS -0.01
5.99 0.01
5.93 0.07
3.24 0.01
6.04 0.03
5.72 0.04 2.85 0.09
5.04 0.07
3.88 0.78OrganaMXPE 0.34 5.59 0.33 5.44 0.74 3.06 0.27 5.61 0.59 5.34 0.81 2.94 0.58 4.27 0.68 3.78 0.48
XPPE 0.10
5.75 0.01
5.70 0.48
3.08 0.03
5.71 0.33
5.40 0.44
2.89 0.34
4.28 0.30
3.78 0.64
LAD 0.01
5.63 0.12
5.56 0.03
3.08 0.08
5.68 0.02
5.70 0.05
5.70 0.03
5.68 0.02
5.64 0.60
AxS -0.02 4.84 0.06 4.80 0.09 2.43 0.02 5.11 0.02 5.04 0.10 2.57 0.04 3.40 0.05 3.02 0.56PneumMXPE 0.38 5.88 0.33 5.17 0.78 3.21 0.25 5.57 0.63 5.79 0.79 3.15 0.62 4.43 0.72 3.90 0.96
XPPE 0.20
5.87 0.17
5.49 0.49
3.19 0.07
5.51 0.43
5.78 0.40
3.14 0.23
4.40 0.45
3.89 0.92
LAD -0.01
5.50 0.03
5.08 0.08
5.84 0.01
5.24 0.02
5.83 0.04
5.82 0.03
5.90 0.01
5.89 2.00
AxS -0.01 5.15 0.07 4.10 0.10 3.03 -0.00 4.76 0.02 5.48 0.08 2.98 0.05 3.97 -0.00 3.61 5.05
Table 1: Average S-Faithfulness (S-Faith), Complexity (Cpx), and ROAR-S results of shift attributions methods for a LeNet on
different image datasets and corruptions. A higher S-Faithfulness value indicates that features highlighted by 𝜙Shiftare strongly
correlated with the true performance change caused by the shift in these features. A lower Complexity value corresponds to
more concise explanations and a lower ROAR-S score signals that removing features based on 𝜙Shifteffectively mitigates the
shift-induced performance change.
Figure 3: Feature shift importance on MNIST for local and
global corruptions. The explanations given by XPE are most
intuitive, only highlighting the area of the zigzag connecting
the top parts of the ’4’, which changes the model’s prediction
to a ’9’. A similar observation can be made for the spatter
corruption, changing the prediction from ’9’ →’7’. XPE also
allows uncovering the image regions shifting the prediction
from ’2’→’8’ given a global increase in brightness.
actual digit, whereas the MLP also gets distracted by parts of the
corruptions at irrelevant peripheral regions. This implies that the
LeNet model with convolutional feature extraction generally copes
better with the considered shifts, while the MLP might has internal-
ized some unstable global patterns. Such results convey valuable
insights that can help to disclose model deficiencies, perform model
selection, or guide model debugging.
Figure 4: Feature shift importance for pneumonia detection
dataset (PneumM) consisting of real chest X-rays. Neither
image shows pneumonia. Only XPE consistently assigns the
contribution to the loss increase caused by the shift to ar-
eas that might resemble pneumonia indicators (white spots)
within the lung area.
Figure 5: Comparison of XPE for zigzag (’6’ →’0’), spatter (’6’
→’5’), and brightness (’2’ →’8’) corruptions between a LeNet
and an MLP model.
6.2 Revealing selection bias in the training set
for audio data
With the application of machine learning models in an increas-
ing number of real-world applications, the identification of biases
included in the model becomes more important [ 64]. This is particu-
larly relevant in voice recognition and assistant systems, where the
performance of a model differs between groups of people formed
 
556KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
Figure 6: Spectrogram considering frequencies of up to
1024𝐻𝑧for a male (left) and a female (right) participant pro-
nouncing the digit three. The fundamental frequency bands
for males𝑓male
0and females 𝑓female
0are highlighted.
by gender and region because they are not sufficiently represented
in the training data [15, 56].
In this experiment, we demonstrate how XPE can help to indicate
such a selection bias if relevant meta-information about the in-
cluded groups in the source and target distribution is absent. We
analyze the prediction task of classifying spoken digits ’0’ to ’9’
from audio signals based on the dataset provided by [ 7] containing
real voice recordings of 60 different speakers with varying ages,
sexes, and accents. Since our method is agnostic to the model used,
we follow exactly the same model choice made by the original re-
searchers [ 7], which propose to use an AlexNet [ 52] as a classifier.
To do so, the voice recordings are transformed into 2D spectrograms
of size 227×227using the Short-time Fourier Transform (STFT), and
an example is shown in Figure 6. To introduce a selection bias, we
consider a model solely trained on male participants, so the model
faced a significant bias during training with potential consequences
on the performance during operation. We evaluated this model on
unseen test sets with varying fractions of female participants and
asses if feature shift importance can identify the selection bias as a
source for model degradation.
Due to the high dimensionality of the input data (𝑑=51.529),
we apply two general extensions to improve the scalability of all
attribution methods and reduce the computational effort. First, we
solved the optimal transport problem based on intermediate net-
work activations of the used AlexNet instead of the raw inputs [ 18].
This allows us to reduce the effective size over which the transport
needs to be computed and increases the quality of the resulting
performance estimation significantly. Second, we computed shift
importance at the level of frequency bands instead of all individual
spectrogram values, which also reduces the computational load
while maintaining the interpretability of the resulting explanations.
To do so, we group features by splitting the frequency axis into
32frequency bands of 32𝐻𝑧resolution and aggregating over the
entire time axis. This level of granularity is sufficient to distinguish
gender-specific characteristics related to the induced selection bias.
Based on these application-specific improvements, we evaluate all
shift importance methods on the biased model (trained on males
only) when facing three shift scenarios with varying bias sever-
ity. First, a test set consisting of unseen male participants, such
XPE XPPE LAD AxS
Method051015202530Φ
ratio[%]strong shift
medium shift
no shiftFigure 7: Ratio of feature shift importance allocated to
gender-specific frequencies for a biased model solely trained
on male participants and evaluated on sets with only women
(strong shift), with equal gender distribution (medium shift),
and with solely men (no shift) not in the training set. Only
XPE and XPPE attribute importance proportionally to the
strength of the induced selection bias.
that a difference in performance between training and test samples
should be predominantly due to a change in non-gender-specific
frequencies, e.g., caused by different pronunciations between indi-
vidual persons. Second, we evaluate the model on a test set with an
equal number of male and female participants. Finally, we test the
model on a test set consisting only of women, which results in a
strong gender-specific shift. To assess the quality of feature shift
importance in this context, we build on existing domain knowledge
about gender-specific human vocal frequencies [ 26]. It suggests
that distinctive frequency ranges are between 85𝐻𝑧and155𝐻𝑧
for males and 165𝐻𝑧and255𝐻𝑧for females. Therefore, we expect
that as the fraction of females in the test set increases, the overall
shift importance allocated to gender-specific frequencies should
also increase proportionally, successfully indicating the severity of
the selection bias. To investigate this, we calculate the proportion
of feature shift importance within the combined male and female
fundamental vocal frequency bands ( ΦShift
gender) relative to the total
feature shift importance across all frequencies ( ΦShift
total) as a global
measure over the entire test dataset.
ΦShift
ratio=ΦShift
gender
ΦShift
total
The results presented in Figure 7 demonstrate that only XPE and
XPPE attribute importance proportionally to the strength of the
induced selection bias, while all other baselines are insensitive to the
faced distribution shift. This suggests that in cases where a selection
bias is introduced during the training of machine learning models,
XPE and XPPE can detect it and prompt further investigation, even
in the absence of labels in the target domain or corresponding
meta-information.
 
557Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance KDD ’24, August 25–29, 2024, Barcelona, Spain.
6.3 Assessing the impact of missing values in
tabular data
As a final experiment, we investigate the effectiveness of shift attri-
bution methods in the presence of data quality issues on four tabular
datasets from a popular evaluation benchmark [ 35]. Specifically,
we focus on the commonly encountered problem of facing missing
values due to a compromised data acquisition process. This is typi-
cally handled via an imputation strategy Ithat replaces missing
values based on a predefined heuristic such as the feature mean. For
each dataset, we fitted two different models (XGBoost [ 14] and an
MLP) and corrupted a set of 𝑛=500unseen test samples 𝑋𝑡by ran-
domly selecting a column 𝑗and marking 25% of values as missing,
yielding a perturbed test set ˜𝑋𝑡. We then assess how reliably the
shift importance values associated with column 𝑗, denoted by 𝜙Shift
𝑗,
correspond to the true impact of the corruption. To this end, we
compute the correlation between 𝜙Shift
𝑗and the loss difference due
to missing value imputation across all samples in the test set. This
results in a global and feature-specific version of Shift-Faithfulness,
which we refer to as Global Performance-Correlation (GPC):
GPC(𝜙Shift
𝑗)=corr
˜𝑥𝑡∈˜𝑋𝑡
𝑥𝑡∈𝑋𝑡
𝜙Shift
𝑗(˜𝑥𝑡),L 𝑓(I(˜𝑥𝑡)),𝑦𝑡−L 𝑥𝑡,𝑦𝑡
Here,I(˜𝑥𝑡))corresponds to the corrupted sample ˜𝑥𝑡, where the
missing values are filled using a mean value imputer Ifitted on
the training set. While standard S-Faith evaluates the quality of
a local shift attribution result for an individual sample, GPC is a
global measure of shift attribution fidelity for specific features of
interest across an entire test set. The corresponding results in Table
2 demonstrate that XPE and XPPE allocate an importance score
to the corrupted column that matches the actual loss contribution
caused by the quality issue best. This consolidates the capabilities
of our introduced methods to improve model monitoring in the
presence of common data quality issues also for tabular data.
co
vertype pol electricity bank-marketing
X
GB MLP X
GB MLP X
GB MLP X
GB MLP
XPE 0.05 0.05 0.03 0.43 0.09
0.06 0.12
0.06
XPPE 0.05
0.13 0.17 0.42 0.31
0.23 0.04
-0.02
LAD 0.03
-0.05 -0.07
0.10 0.05
-0.13 0.06
0.00
AxS 0.02
-0.05 -0.01
0.06 -0.11
0.02 -0.06
-0.15
Table 2: Global Performance Correlation between the shift
importance assigned to the corrupted feature 𝑗and the ac-
tual performance change caused by missing values. In all
scenarios, XPE and XPPE consistently demonstrate superior
performance in indicating the impact of the corruption most
reliably via the allocated shift attribution.
7 Discussion and Conclusion
We introduced Explanatory Performance Estimation (XPE) as a
novel and agnostic framework to attribute an anticipated change
in model performance induced by a distribution shift to individual
features. Our approach requires no ground truth labels during de-
ployment, which corresponds to the typical situation in practiceand is applicable to any monitoring situation. We also provided
a theoretical analysis and extensive experiments covering nine
datasets to demonstrate the empirical success of our method across
ten different shifts and three data modalities.
Our approach is limited by the capabilities of optimal transport
to model the true observed shift based on samples, which can be
challenging for high-dimensional data and may require computing
the transport based on lower-dimensional input representations
(as exemplified in the audio experiment) or the use of more so-
phisticated cost functions [ 22]. Another way to enhance scalability
for individual applications is to leverage model or data-specific
approximations of Shapley Values [ 12] or to incorporate additional
concepts from unsupervised domain adaptation [ 51] for aligning
source and target samples. Moreover, explaining the effect of a shift
in terms of individual input shifts might not always be the optimal
level of abstraction. Hence, combining XPE with concept-based
explanations [ 98] would be a natural extension. This could even
further increase the applicability of XPE as an effective method to
enable explanatory model monitoring in practice.
References
[1]Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: a
survey on explainable artificial intelligence (XAI). IEEE access 6 (2018), 52138–
52160.
[2]David Alvarez-Melis, Tommi Jaakkola, and Stefanie Jegelka. 2018. Structured op-
timal transport. In International Conference on Artificial Intelligence and Statistics.
PMLR, 1771–1780.
[3]David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. 2019. Towards
optimal transport with global invariances. In The 22nd International Conference
on Artificial Intelligence and Statistics. PMLR, 1870–1879.
[4]Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. 2018. Towards
better understanding of gradient-based attribution methods for Deep Neural
Networks. In 6th International Conference on Learning Representations (ICLR).
[5]Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein gen-
erative adversarial networks. In International conference on machine learning.
PMLR, 214–223.
[6]Yogesh Balaji, Rama Chellappa, and Soheil Feizi. 2020. Robust optimal transport
with applications in generative modeling and domain adaptation. Advances in
Neural Information Processing Systems 33 (2020), 12934–12944.
[7]Sören Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Müller,
Sebastian Lapuschkin, and Wojciech Samek. 2023. AudioMNIST: Exploring
Explainable Artificial Intelligence for audio analysis on a simple benchmark.
Journal of the Franklin Institute (2023).
[8]Shai Ben-David and Ruth Urner. 2012. On the hardness of domain adaptation and
the utility of unlabeled target samples. In International Conference on Algorithmic
Learning Theory. Springer, 139–153.
[9]Umang Bhatt, Adrian Weller, and José MF Moura. 2021. Evaluating and aggre-
gating feature-based model explanations. In Proceedings of the Twenty-Ninth
International Conference on International Joint Conferences on Artificial Intelli-
gence. 3016–3022.
[10] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-
han Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter Eckersley. 2020.
Explainable machine learning in deployment. In Proceedings of the 2020 confer-
ence on fairness, accountability, and transparency. 648–657.
[11] Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. 2021.
Why did the distribution change?. In Proceedings of The 24th International Con-
ference on Artificial Intelligence and Statistics (Proceedings of Machine Learning
Research, Vol. 130), Arindam Banerjee and Kenji Fukumizu (Eds.). PMLR, 1666–
1674.
[12] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. 2023. Algorithms
to estimate Shapley value feature attributions. Nature Machine Intelligence 5, 6
(2023), 590–601.
[13] Mayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and
Christopher Ré. 2021. Mandoline: Model evaluation under distribution shift. In
International Conference on Machine Learning. PMLR, 1617–1629.
[14] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785–794.
[15] Xingyu Chen, Zhengxiong Li, Srirangaraj Setlur, and Wenyao Xu. 2022. Ex-
ploring racial and gender disparities in voice biometrics. Scientific Reports 12, 1
 
558KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
(2022), 3723.
[16] Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy.
2017. Joint distribution optimal transportation for domain adaptation. Advances
in Neural Information Processing Systems 30 (2017).
[17] Nicolas Courty, Rémi Flamary, and Devis Tuia. 2014. Domain adaptation with
regularized optimal transport. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases. Springer, 274–289.
[18] Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. 2016.
Optimal Transport for Domain Adaptation. IEEE Transactions on Pattern Analysis
and Machine Intelligence 39, 9 (2016), 1853–1865.
[19] Ian Covert, Scott Lundberg, and Su-In Lee. 2021. Explaining by removing: A
unified framework for model explanation. Journal of Machine Learning Research
22, 209 (2021), 1–90.
[20] Ian Covert, Scott M Lundberg, and Su-In Lee. 2020. Understanding global
feature contributions with additive importance measures. Advances in Neural
Information Processing Systems 33 (2020), 17212–17223.
[21] Marco Cuturi. 2013. Sinkhorn Distances: Lightspeed Computation of Op-
timal Transport. In Advances in Neural Information Processing Systems, C.J.
Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (Eds.),
Vol. 26. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2013/file/
af21d0c97db2e27e13572cbf59eb343d-Paper.pdf
[22] Marco Cuturi, Michal Klein, and Pierre Ablin. 2023. Monge, Bregman and Oc-
cam: Interpretable Optimal Transport in High-Dimensions with Feature-Sparse
Maps. In Proceedings of the 40th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett (Eds.). PMLR, 6671–6682.
[23] Bharath Bhushan Damodaran, Benjamin Kellenberger, Rémi Flamary, Devis
Tuia, and Nicolas Courty. 2018. Deepjdot: Deep joint distribution optimal
transport for unsupervised domain adaptation. In Proceedings of the European
Conference on Computer Vision (ECCV). 447–463.
[24] Shai Ben David, Tyler Lu, Teresa Luu, and Dávid Pál. 2010. Impossibility the-
orems for domain adaptation. In Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics. JMLR Workshop and Confer-
ence Proceedings, 129–136.
[25] Thomas Decker, Ralf Gross, Alexander Koebler, Michael Lebacher, Ronald
Schnitzer, and Stefan H Weber. 2023. The Thousand Faces of Explainable AI
Along the Machine Learning Life Cycle: Industrial Reality and Current State of
Research. In International Conference on Human-Computer Interaction. Springer,
184–208.
[26] John Fitch and Anthony Holbrook. 1970. Modal vocal fundamental frequency
of young adults. Archives of otolaryngology 92 4 (1970), 379–82. https://api.
semanticscholar.org/CorpusID:39025694
[27] Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie
Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras,
Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain
Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy,
Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer.
2021. POT: Python Optimal Transport. Journal of Machine Learning Research
22, 78 (2021), 1–8. http://jmlr.org/papers/v22/20-451.html
[28] João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. 2014. A survey on concept drift adaptation. ACM computing surveys
(CSUR) 46, 4 (2014), 1–37.
[29] Saurabh Garg, Sivaraman Balakrishnan, Zachary Lipton, Behnam Neyshabur,
and Hanie Sedghi. 2022. Leveraging Unlabeled Data to Predict Out-of-
Distribution Performance. In International Conference on Learning Represen-
tations (ICLR).
[30] Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. 2020. A
unified view of label shift estimation. Advances in Neural Information Processing
Systems 33 (2020), 3290–3300.
[31] Rosana Noronha Gemaque, Albert França Josuá Costa, Rafael Giusti, and Eu-
landa Miranda Dos Santos. 2020. An overview of unsupervised drift detection
methods. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery
10, 6 (2020), e1381.
[32] Aude Genevay, Marco Cuturi, Gabriel Peyré, and Francis Bach. 2016. Stochastic
optimization for large-scale optimal transport. Advances in neural information
processing systems 29 (2016).
[33] Amirata Ghorbani, Abubakar Abid, and James Zou. 2019. Interpretation of
neural networks is fragile. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 33. 3681–3688.
[34] Alison L Gibbs and Francis Edward Su. 2002. On choosing and bounding
probability metrics. International statistical review 70, 3 (2002), 419–435.
[35] Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. 2022. Why do tree-based
models still outperform deep learning on typical tabular data? Advances in
Neural Information Processing Systems 35 (2022), 507–520.
[36] Sebastian Gregor Gruber and Florian Buettner. 2022. Better Uncertainty Cali-
bration via Proper Scores for Classification and Beyond. In Advances in Neural
Information Processing Systems.[37] Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig
Schmidt. 2021. Predicting with confidence on unseen distributions. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision. 1134–1144.
[38] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration
of modern neural networks. In International conference on machine learning.
PMLR, 1321–1330.
[39] Anna Hedström, Leander Weber, Daniel Krakowczyk, Dilyara Bareeva, Franz
Motzkus, Wojciech Samek, Sebastian Lapuschkin, and Marina M-C Höhne. 2023.
Quantus: An explainable ai toolkit for responsible evaluation of neural network
explanations and beyond. Journal of Machine Learning Research 24, 34 (2023),
1–11.
[40] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A
benchmark for interpretability methods in deep neural networks. Advances in
neural information processing systems 32 (2019).
[41] Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger.
2016. Supervised word mover’s distance. Advances in neural information
processing systems 29 (2016).
[42] Hicham Janati, Marco Cuturi, and Alexandre Gramfort. 2020. Spatio-temporal
alignments: Optimal transport through space and time. In International Confer-
ence on Artificial Intelligence and Statistics. PMLR, 1695–1704.
[43] Dominik Janzing, Lenon Minorics, and Patrick Blöbaum. 2020. Feature relevance
quantification in explainable AI: A causal problem. In International Conference
on artificial intelligence and statistics. PMLR, 2907–2916.
[44] Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. 2022.
Assessing Generalization of SGD via Disagreement. In International Conference
on Learning Representations.
[45] Krishnaram Kenthapadi, Himabindu Lakkaraju, Pradeep Natarajan, and
Mehrnoosh Sameki. 2022. Model Monitoring in Practice: Lessons Learned
and Open Challenges. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 4800–4801.
[46] Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht,
Seong Tae Kim, and Nassir Navab. 2021. Neural response interpretation through
the lens of critical pathways. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition. 13528–13538.
[47] Valentin Khrulkov and Ivan Oseledets. 2022. Understanding ddpm latent codes
through optimal transport. arXiv preprint arXiv:2202.07477 (2022).
[48] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T
Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. 2019. The (un) reliability of
saliency methods. Explainable AI: Interpreting, explaining and visualizing deep
learning (2019), 267–280.
[49] Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de Bezenac, and patrick
gallinari. 2022. Mapping conditional distributions for domain adaptation under
generalized target shift. In International Conference on Learning Representations.
[50] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. 2023. Neu-
ral Optimal Transport. In The Eleventh International Conference on Learning
Representations.
[51] Wouter M Kouw and Marco Loog. 2019. A review of domain adaptation without
target labels. IEEE transactions on pattern analysis and machine intelligence 43,
3 (2019), 766–785.
[52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classi-
fication with deep convolutional neural networks. Commun. ACM 60 (2012), 84
– 90.
[53] Sean Kulinski and David I Inouye. 2023. Towards explaining distribution shifts.
InInternational Conference on Machine Learning. PMLR, 17931–17952.
[54] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From
word embeddings to document distances. In International conference on machine
learning. PMLR, 957–966.
[55] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[56] Lanna Lima, Vasco Furtado, Elizabeth Furtado, and Virgilio Almeida. 2019.
Empirical Analysis of Bias in Voice-Based Personal Assistants. In Companion
Proceedings of The 2019 World Wide Web Conference (San Francisco, USA) (WWW
’19). Association for Computing Machinery, New York, NY, USA, 533–538.
[57] Tianyi Lin, Nhat Ho, and Michael Jordan. 2019. On efficient optimal transport:
An analysis of greedy and accelerated mirror descent algorithms. In International
Conference on Machine Learning. PMLR, 3982–3991.
[58] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. 2018. Detecting and
correcting for label shift with black box predictors. In International conference
on machine learning. PMLR, 3122–3130.
[59] Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang. 2020. Semantic cor-
respondence as an optimal transport problem. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 4463–4472.
[60] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018.
Learning under concept drift: A review. IEEE transactions on knowledge and
data engineering 31, 12 (2018), 2346–2363.
[61] Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin,
Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020.
 
559Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance KDD ’24, August 25–29, 2024, Barcelona, Spain.
From local explanations to global understanding with explainable AI for trees.
Nature machine intelligence 2, 1 (2020), 56–67.
[62] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).
[63] Sasu Mäkinen, Henrik Skogström, Eero Laaksonen, and Tommi Mikkonen. 2021.
Who needs MLOps: What data scientists seek to accomplish and how can MLOps
help?. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering
for AI (WAIN). IEEE, 109–112.
[64] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A Survey on Bias and Fairness in Machine Learning. ACM
Comput. Surv. 54, 6, Article 115 (jul 2021), 35 pages.
[65] Carlos Mougan, Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, and
Steffen Staab. 2022. Explanation Shift: Detecting distribution shifts on tabular
data via the explanation space. In NeurIPS 2022 Workshop on Distribution Shifts:
Connecting Methods and Applications.
[66] Norman Mu and Justin Gilmer. 2019. MNIST-C: A Robustness Benchmark for
Computer Vision. ArXiv abs/1906.02337 (2019).
[67] David Nigenda, Zohar Karnin, Muhammad Bilal Zafar, Raghu Ramesha, Alan
Tan, Michele Donini, and Krishnaram Kenthapadi. 2022. Amazon SageMaker
Model Monitor: A System for Real-Time Insights into Deployed Machine Learn-
ing Models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (Washington DC, USA) (KDD ’22). Association for
Computing Machinery, New York, NY, USA, 3671–3681.
[68] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can
you trust your model’s uncertainty? evaluating predictive uncertainty under
dataset shift. Advances in neural information processing systems 32 (2019).
[69] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. 2022. Challenges in
deploying machine learning: a survey of case studies. Comput. Surveys 55, 6
(2022), 1–29.
[70] Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen. 2021. Image-to-image
translation: Methods and applications. IEEE Transactions on Multimedia (2021).
[71] George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mo-
hamed, and Balaji Lakshminarayanan. 2021. Normalizing Flows for Probabilistic
Modeling and Inference. J. Mach. Learn. Res. 22, 57 (2021), 1–64.
[72] François-Pierre Paty and Marco Cuturi. 2019. Subspace robust Wasserstein
distances. In International conference on machine learning. PMLR, 5072–5081.
[73] Mathis Petrovich, Chao Liang, Ryoma Sato, Yanbin Liu, Yao-Hung Hubert
Tsai, Linchao Zhu, Yi Yang, Ruslan Salakhutdinov, and Makoto Yamada. 2020.
Feature robust optimal transport for high-dimensional data. arXiv preprint
arXiv:2005.12123 (2020).
[74] Gabriel Peyré, Marco Cuturi, et al .2019. Computational optimal transport: With
applications to data science. Foundations and Trends® in Machine Learning 11,
5-6 (2019), 355–607.
[75] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and
Neil D Lawrence. 2008. Dataset shift in machine learning. Mit Press.
[76] Stephan Rabanser, Stephan Günnemann, and Zachary Lipton. 2019. Failing
loudly: An empirical study of methods for detecting dataset shift. Advances in
Neural Information Processing Systems 32 (2019).
[77] Alain Rakotomamonjy, Rémi Flamary, Gilles Gasso, M El Alaya, Maxime Berar,
and Nicolas Courty. 2022. Optimal transport for conditional domain matching
and label shift. Machine Learning 111, 5 (2022), 1651–1670.
[78] Litu Rout, Alexander Korotin, and Evgeny Burnaev. 2022. Generative Mod-
eling with Optimal Transport Maps. In International Conference on Learning
Representations.
[79] Benedek Rozemberczki, Lauren Watson, Péter Bayer, Hao-Tsung Yang, Olivér
Kiss, Sebastian Nilsson, and Rik Sarkar. 2022. The Shapley Value in Machine
Learning. In The 31st International Joint Conference on Artificial Intelligence and
the 25th European Conference on Artificial Intelligence.
[80] Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. 2018. Improv-
ing GANs Using Optimal Transport. In International Conference on Learning
Representations.
[81] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-
mar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan
Dennison. 2015. Hidden technical debt in machine learning systems. Advances
in neural information processing systems 28 (2015).
[82] Murtuza N Shergadwala, Himabindu Lakkaraju, and Krishnaram Kenthapadi.
2022. A Human-Centric Perspective on Model Monitoring. In Proceedings of the
AAAI Conference on Human Computation and Crowdsourcing, Vol. 10. 173–183.
[83] Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate
shift by weighting the log-likelihood function. Journal of statistical planning
and inference 90, 2 (2000), 227–244.
[84] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
2020. Fooling lime and shap: Adversarial attacks on post hoc explanation
methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.
180–186.
[85] Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin,
Ludwig Winkler, Steven Peters, and Klaus-Robert Müller. 2021. Towards CRISP-
ML (Q): a machine learning process model with quality assurance methodology.Machine learning and knowledge extraction 3, 2 (2021), 392–413.
[86] Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul
von Bünau, and Motoaki Kawanabe. 2008. Direct importance estimation for
covariate shift adaptation. Annals of the Institute of Statistical Mathematics 60, 4
(2008), 699–746.
[87] Mukund Sundararajan and Amir Najmi. 2020. The many Shapley values for
model explanation. In International conference on machine learning . PMLR, 9269–
9278.
[88] Ankur Taly, Kaz Sato, and Claudiu Gruia. 2021. Monitoring feature attributions:
How Google saved one of the largest ML services in trouble. Google Cloud Blog
(2021).
[89] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Rémi Flamary. 2019.
Optimal transport for structured data with application on graphs. In International
Conference on Machine Learning. PMLR, 6275–6284.
[90] Christian Tomani and Florian Buettner. 2021. Towards trustworthy predictions
from deep neural networks with fast adversarial calibration. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 35. 9886–9896.
[91] Christian Tomani, Sebastian Gruber, Muhammed Ebrar Erdem, Daniel Cremers,
and Florian Buettner. 2021. Post-hoc uncertainty calibration for domain drift
scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 10124–10132.
[92] Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb, Ashley
Scillitoe, Robert Samoilescu, and Alex Athorne. 2019. Alibi Detect: Algorithms for
outlier, adversarial and drift detection. https://github.com/SeldonIO/alibi-detect
[93] Cédric Villani. 2009. Optimal transport: old and new. Vol. 338. Springer.
[94] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[95] Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao
Wu. 2018. Semi-Supervised Optimal Transport for Heterogeneous Domain
Adaptation.. In IJCAI, Vol. 7. 2969–2975.
[96] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,
Hanspeter Pfister, and Bingbing Ni. 2023. MedMNIST v2-A large-scale light-
weight benchmark for 2D and 3D biomedical image classification. Scientific
Data 10, 1 (2023), 41.
[97] Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep K
Ravikumar. 2019. On the (in) fidelity and sensitivity of explanations. Advances
in Neural Information Processing Systems 32 (2019).
[98] Chih-Kuan Yeh, Been Kim, and Pradeep Ravikumar. 2022. Human-Centered Con-
cept Explanations for Neural Networks. In Neuro-Symbolic Artificial Intelligence:
The State of the Art. IOS Press, 337–352.
[99] Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and
Justin M Solomon. 2019. Hierarchical optimal transport for document represen-
tation. Advances in neural information processing systems 32 (2019).
[100] Haoran Zhang, Harvineet Singh, Marzyeh Ghassemi, and Shalmali Joshi. 2023.
"Why did the Model Fail?": Attributing Model Performance Changes to Distribu-
tion Shifts. In Proceedings of the 40th International Conference on Machine Learn-
ing (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett (Eds.). PMLR, 41550–41578.
[101] Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, and Jie Zhou. 2021. To-
wards interpretable deep metric learning with structural matching. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision. 9887–9896.
A Mathematical Proofs
In this section, we provide the proof of Theorem 5.2 in the main
paper. For the definition of 𝜀-approximate label-preserving distri-
bution shifts, we refer to Definition 5.1 in Section 5. To start, we
introduce a well-known equivalent characterization of the total
variation distance as given for instance in [34].
Lemma A.1. LetXbe a measurable space and 𝜇and𝜈two proba-
bility measures onX. Then
2𝑑𝑇𝑉(𝜇,𝜈)=max
ℎ,∥ℎ∥∞≤1|∫
Xℎ𝑑𝜇−∫
Xℎ𝑑𝜈|
whereℎspecifies any measurable function ℎ:X→R.
Based on this we can finally conduct the proof of Theorem 5.2.
Proof. (Theorem 5.2) The true performance of a model 𝑓in the
target domainL𝑡and the estimated performance based on label
 
560KDD ’24, August 25–29, 2024, Barcelona, Spain. Thomas Decker et al.
transportcL𝑇
𝑡are given by:
L𝑡=E𝑃𝑡(𝑋𝑡,𝑌)[L(𝑓(𝑋𝑡),𝑌)]
cL𝑇
𝑡=E𝑃𝑠(𝑋𝑠,𝑌)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]
Then we have:
|L𝑡−cL𝑇
𝑡|=
=|E𝑃𝑡(𝑋𝑡,𝑌)[L(𝑓(𝑋𝑡),𝑌)]−E𝑃𝑠(𝑋𝑠,𝑌)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]|
=|E𝑃𝑡(𝑋𝑡)E𝑃𝑡(𝑌|𝑋𝑡)[L(𝑓(𝑋𝑡),𝑌)]−
E𝑃𝑠(𝑋𝑠)E𝑃𝑠(𝑌|𝑋𝑠)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]|
(1)=|E𝑃𝑡(𝑇(𝑥𝑠))E𝑃𝑡(𝑌|𝑇(𝑋𝑠))[L(𝑓(𝑇(𝑋𝑠)),𝑌)]−
E𝑃𝑠(𝑋𝑠)E𝑃𝑠(𝑌|𝑋𝑠)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]|
(2)=|E𝑃𝑠(𝑋𝑠)E𝑃𝑡(𝑌|𝑇(𝑋𝑠))[L(𝑓(𝑇(𝑋𝑠)),𝑌)]−
E𝑃𝑠(𝑋𝑠)E𝑃𝑠(𝑌|𝑋𝑠)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]|
(3)
≤E𝑃𝑠(𝑋𝑠) |E𝑃𝑡(𝑌|𝑇(𝑋𝑠))[L(𝑓(𝑇(𝑋𝑠)),𝑌)]−
E𝑃𝑠(𝑌|𝑋𝑠)[L(𝑓(𝑇(𝑋𝑠)),𝑌)]|
(4)
≤E𝑃𝑠(𝑋𝑠)h
max
ℎ,∥ℎ∥∞≤1n
|E𝑃𝑡(𝑌|𝑇(𝑋𝑠))[ℎ(𝑋𝑠,𝑌)]−
E𝑃𝑠(𝑌|𝑋𝑠)[ℎ(𝑋𝑠,𝑌)]|oi
(5)
≤E𝑃𝑠(𝑋𝑠)
2𝑑𝑇𝑉 𝑃𝑡(𝑌|𝑇(𝑋𝑠)),𝑃𝑠(𝑌|𝑋𝑠)(6)
≤2𝜀
□
In step (1) we performed a change of variables using 𝑥𝑡=𝑇(𝑥𝑠)
and in (2) we used property 1 of being 𝜀-approximate label-preserving.
In step (3) we used Jensen’s inequality to pull the absolute value
inside the expectation and in step (4) we used the assumption that
∥L∥∞≤1. In step (5) we applied the Lemma specified above and
in step (6) we leveraged property 2 of being 𝜀-approximate label-
preserving.
B Details on Experiments
Data and Models. For the image data experiments, we analyzed
MNIST [ 55], FashionMNSIT [ 94] as well as OrganaMNIST and
PneumoniaMNIST from the MedMNISTv2 benchmark [ 96]. For each
dataset we trained and evaluated a LeNet model [ 55]. All models
have been trained for 100 epochs with early stopping based on a
patience of 10 epochs and PyTorch’s Adam optimizer with a batch
size of 16 and a learning rate of 1e-3. The used audio data set [ 7] is
publicly available. We downsampled the original raw sound data to
a new sampling frequency of 2048 Hz and generated a spectrogram
for every sound file using Short-time Fourier Transform (STFT).
The STFT is calculated using a segment length of 455 and a overlap
of 445. The training and test sets each consist of 12 randomly drawn
participants according to the by the experiment determined gender
distribution. According to the original paper [ 7] of the audio data set
we also used AlexNet1[52] to solve the digit classification task. The
models are trained for 40 epochs. The training has been performed
using an Adam optimizer with a batch size of 16 and a learning rate
1Used PyTorch implementation of AlexNet: https://github.com/Lornatang/AlexNet-
PyTorchof 1e-3. For the tabular data, we selected four classification datasets
and downloaded the preprocessed version provided by [ 35]. For
each dataset we performed a 80-20 train-test split and fitted an
XGBoost classifier with 50 estimators of max depth 5 as well as a
Multi-Layer Perceptron (MLP) with one hidden layer of size 128.
Optimal Transport. In all experiments we match the same amount
of samples in source and target domain using EMDTransport solver
in the POT library [ 27] with default parameters. Thus, the esti-
mated couplings typically result in a one-to-one matching and the
subsequent estimation of XPE and XPPE based on ˆ𝑇are equivalent
to those leveraging the entire coupling 𝜋.
Shift Attribution. To compute the necessary Shapley Values for
each shift attribution method we relied on the model-agnostic Ker-
nelSHAP implementation provided by [ 62]. All Shapley Values have
been computed using a sample size of 3000 and the default choices
for all other hyperparameters. For LAD and AxS baselines we used
a background dataset of 30random samples for tabular data and a
fixed baseline of zeros otherwise. For the AxS metric, we estimated
the shift mask 𝑀KSusing a featurewise two-sided Kolmogorov-
Smirnov test with 95% confidence threshold to signal a shift as
provided by [92].
Shift-Faithfulness and Complexity. To compute the metrics we re-
lied on the implementation provided by [ 39]. For Shift-Faithfulness,
we used the Faithfulness Correlation metric, specified the perturba-
tion baseline to be the estimated per-shift version of each sample
and chose a sample size of 100 with a subset size |𝐾|of 64. All other
hyperparameters correspond to their default choices. Moreover,
we removed samples where the anticipated performance change
is only marginal as this causes either all shift attributions to be
zero or causes numerical problems during the computation of the
correlation value used in Shift-Faithfulness.
ROAR-S Metric. Our implementation of Remove, Retrain, and
Shift ROAR-S is based on the implementation by [ 46] of the ROAR
metric [ 40]. Given the high computational requirements of ROAR,
we sub-sample the original datasets to obtain 𝐷train𝑠 and𝐷test𝑠with
𝑁𝑅𝑂𝐴𝑅 =1000 samples each. 𝐷train𝑠 is used to train the pre-removal
LeNet model 𝑓. For each shift we create the shifted dataset versions
𝐷train
𝑡and𝐷test
𝑡and compute the different feature shift attribu-
tions𝜙Shift. Next we remove the top 5% of the features attributed
the highest importance. This yields the post-removal sets ˜𝐷train𝑠,
˜𝐷test𝑠,˜𝐷train
𝑡and ˜𝐷test
𝑡. The new source training set ˜𝐷train𝑠 is used
to retrain the model yielding ˜𝑓. With cross-entropy loss L, the
test performances Ltest
𝑡=1
𝑁ROARÍ
(𝑥,𝑦)∈𝐷test
𝑡L(𝑓(𝑥),𝑦),Ltest𝑠=
1
𝑁ROARÍ
(𝑥,𝑦)∈𝐷test𝑠L(𝑓(𝑥),𝑦)of the model 𝑓without removal and
the test performances ˜Ltest
𝑡=1
𝑁ROARÍ
(𝑥,𝑦)∈˜𝐷test
𝑡L(˜𝑓(𝑥),𝑦),˜Ltest𝑠=
1
𝑁ROARÍ
(𝑥,𝑦)∈˜𝐷test𝑠L(˜𝑓(𝑥),𝑦)after removal on the retrained model
˜𝑓, the ROAR-S score can be defined as:
ROAR-S(˜Ltest
𝑡,˜Ltest
𝑠,Ltest
𝑡,Ltest
𝑠)=max(0,˜Ltest
𝑡−˜Ltest𝑠)
Ltest
𝑡−Ltest𝑠
All models have been trained for 100 epochs with early stopping
based on a patience of 10 epochs, an Adam optimizer with a batch
size of 16 and a learning rate of 1e-2.
 
561