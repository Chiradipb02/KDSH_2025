Conformal Counterfactual Inference under Hidden Confounding
Zonghao Chen∗†
University College London
London, United Kingdom
zonghao.chen.22@ucl.ac.ukRuocheng Guo†
ByteDance Research
London, United Kingdom
ruocheng.guo@bytedance .com
Jean-Francois Ton
ByteDance Research
London, United Kingdom
jeanfrancois@bytedance .comYang Liu
ByteDance Research
San Jose, United States
yang.liu01@bytedance .com
ABSTRACT
Personalized decision making requires the knowledge of poten-
tial outcomes under different treatments, and confidence intervals
about the potential outcomes further enrich this decision-making
process and improve its reliability in high-stakes scenarios. Pre-
dicting potential outcomes along with its uncertainty in a coun-
terfactual world poses the foundamental challenge in causal in-
ference. Existing methods that construct confidence intervals for
counterfactuals either rely on the assumption of strong ignorabil-
ity that completely ignores hidden confounders, or need access
to un-identifiable lower and upper bounds that characterize the
difference between observational and interventional distributions.
In this paper, to overcome these limitations, we first propose a
novel approach wTCP-DR based on transductive weighted con-
formal prediction, which provides confidence intervals for coun-
terfactual outcomes with marginal converage guarantees, even
under hidden confounding. With less restrictive assumptions, our
approach requires access to a fraction of interventional data (from
randomized controlled trials) to account for the covariate shift
from observational distributoin to interventional distribution. The-
oretical results explicitly demonstrate the conditions under which
our algorithm is strictly advantageous to the naive method that
only uses interventional data. Since transductive conformal pre-
diction is notoriously costly, we propose wSCP-DR, a two-stage
variant of wTCP-DR, based on split conformal prediction with same
marginal coverage guarantees but at a significantly lower compu-
tational cost. After ensuring valid intervals on counterfactuals, it
is straightforward to construct intervals for individual treatment
effects (ITEs). We demonstrate our method across synthetic and
real-world data, including recommendation systems, to verify the
superiority of our methods compared against state-of-the-art base-
lines in terms of both coverage and efficiency. Our code can be
found at https://github .com/rguo12/KDD24-Conformal.
∗Work done during an internship at ByteDance Research
†Equal contribution
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528 .3671976CCS CONCEPTS
•Mathematics of computing →Causal networks; Hypothesis
testing and confidence interval computation.
KEYWORDS
Conformal Prediction, Causal Inference, Uncertainty Quantification
ACM Reference Format:
Zonghao Chen, Ruocheng Guo, Jean-Francois Ton, and Yang Liu. 2024. Con-
formal Counterfactual Inference under Hidden Confounding. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi .org/10.1145/3637528 .3671976
1 INTRODUCTION
Estimating the heterogeneous causal effects of an intervention
(e.g., a medicine) on an important outcome (e.g., health status)
of different individuals is a fundamental problem in a variety of
influential research areas, including economics, healthcare and
education [ 2–4]. In the growing area of machine learning for causal
inference, this problem has been casted as estimating individual
treatment effect (ITE) and most existing work focuses on developing
machine learning models to improve the point estimate of ITE [ 5–
14]. However, point estimates is not enough to ensure safe and
reliable decision-making in high-stake applications where failures
are costly or may endanger human lives, and hence uncertainty
quantification and confidence intervals allow machine learning
models to express confidence in the correctness of their predictions.
Pioneering work [ 6,15] provides confidence intervals for ITEs
through Bayesian machine learning models such as Bayesian Ad-
ditive Regression Trees [ 5] and Gaussian Process [ 16]. However,
these approaches cannot be easily generalized to popular machine
learning models for causal inference on various input data types,
including but not limited to text [17, 18] and graphs [19, 20].
Recently, built upon conformal prediction [ 21,22], Lei and Can-
des [ 1] propose the first conformal prediction method for counter-
factual outcomes and ITEs, which can provide confidence intervals
with guaranteed marginal coverage in a model-agnostic fashion.
This means that, given any machine learning model that estimates
the potential outcomes under treatment, conformal prediction acts
as a post-hoc wrapper that provides confidence intervals guaran-
teed to contain the ground truth of potential outcomes and ITEs
above a specified probability under marginal distribution. Unfortu-
nately however, Lei and Candes [ 1] require the assumption of strong
 
397
KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
Figure 1: Under hidden confounding, our proposed methods wTCP-
DR and wSCP-DR incorporate a small set of interventional data for
density ratio based weighted conformal prediction, which provides
marginal coverage guarantee along with high efficiency (small con-
fidence interval). In contrast, WCP [ 1] cannot guarantee coverage as
hidden confounding leads to biased estimate of propensity scores.
The Naive method suffers from low efficiency as it only uses the
small set of interventional data.
ignorability that excludes the possibility of hidden confounders,
which cannot be verified given data [ 23,24] and can be violated
in many real-world applications. For example, the socio-economic
status of a patient, which is likely to be unavailable due to privacy
concerns, is a common unobserved confounding factor that affects
both patient’s access to treatment and one’s health condition. Sim-
ilarly, under the strong ignorability assumption, [ 25] propose to
use meta-learners [ 11,26,27] in conformal prediction of ITEs. Re-
cently, Jin et al. [ 28] take hidden confounding into consideration
for conformal prediction of ITEs from a sensitivity analysis aspect.
However, their method needs access to the upper and lower bounds
of the density ratio between the observational distribution and the
interventional distribution to characterize the covariate shift from
observational to interventional distribution.
To address these limitations and provide confidence intervals
that have finite-sample guarantees even without the strong ignor-
ability assumption, we propose weighted Transductive Conformal
Prediction with Density Ratio estimation (wTCP-DR) that is based on
weighted transductive conformal prediction. With less restrictive
assumptions, wTCP-DR needs access to both observational and a
fraction of interventional data (e.g., data collected from random-
ized control trials) [ 29,30]. In contrast to the weighted conformal
prediction method proposed by [ 1] which uses propensity score as
the reweighting function, our algorithm computes the reweighting
function by learning the density ratio of the interventional and
observational distribution using the data provided. The benefits of
our proposed method are as follows: (i) wTCP-DR does not require
strong ignorability assumption and provides a confidence interval
with coverage guarantee even under the presence of confounding.
(ii) wTCP-DR works well under an imbalanced number of inter-
ventional and observational data, i.e., when interventional data
is of smaller size than observational data due to the higher cost
of collecting interventional data. Although wTCP-DR is compu-
tationally expensive due to the nature of transductive conformal
prediction, we also propose a variant of wTCP-DR, called weighted
SplitConformal Prediction with Density Ratio estimation (wSCP-DR)𝑇 𝑌𝑈 𝑋
Figure 2: Example causal graph with hidden confounding. 𝑋: Ob-
served covariates, 𝑈: Hidden confounders, 𝑇: Treatment, 𝑌: Outcome.
Direct edges denote causal relations and the bidirectional edge signi-
fies possible correlation.
which preserves all the advantages of wTCP-DR but at a lower com-
putational cost. We briefly describe how our methods are different
from the method proposed by [1] and the Naive method in Fig. 1.
The paper is organized as follows. Section 2 gives a description of
the problem setting and provides necessary background on confor-
mal prediction. Section 3 describes our novel algorithm wTCP-DR
which provides a confidence interval on counterfactual outcomes
at an individual level with marginal coverage guarantee. Section
4 proposes wSCP-DR which is a more implementable variant of
wTCP-DR. Section 5 applies wTCP-DR and wTCP-DR to provide
confidence intervals for estimating individual treatment effects.
Section 6 demonstrates our method across synthetic and real-world
data, including recommendation systems, to verify our methods in
terms of both coverage and efficiency. Section 7 discusses related
work in the literature. Section 8 concludes the paper.
2 PRELIMINARIES
2.1 Problem setting
We consider the standard potential outcome (PO) framework [ 31,
32] with a binary treatment. Let 𝑇∈{0,1}be the treatment indi-
cator,𝑥∈X⊂ R𝑑be the observed covariates, and 𝑦∈Y⊂ R
be the outcome of interest. We use 𝑋,𝑌 to denote random vari-
ables inX,Y. For each subject 𝑖, let(𝑌𝑖(0),𝑌𝑖(1))be the pair of
potential outcomes under control 𝑇=0and treatment 𝑇=1, re-
spectively. We assume that the data generating process satisfies
the following widely used assumptions: 1) Consistency: 𝑌𝑖=𝑌𝑖(𝑇𝑖),
which means the observed outcome 𝑌𝑖is the same as the poten-
tial outcome 𝑌𝑖(𝑇𝑖)with the observed treatment 𝑇𝑖. (2) Positivity:
0<P(𝑇=1|𝑋=𝑥)<1,∀𝑥∈X, which means that any subject
has a positive chance to get treated and controlled. We would like to
emphasize that we are notassuming strong ignorability, i.e., there
might exist potential hidden confounding 𝑈that affects treatment
𝑇and outcome 𝑌at the same time. See Fig. 2 for an example causal
graph.
Under this framework, the joint distribution under intervention
𝑑𝑜(𝑇=𝑡)is𝑃𝑋,𝑌(𝑡)=𝑃𝑌(𝑡)|𝑋×𝑃𝑋and that for observational data
is𝑃𝑋,𝑌|𝑇=𝑡=𝑃𝑌|𝑋,𝑇=𝑡×𝑃𝑋|𝑇=𝑡. Note that the difference between
conditional distribution 𝑃𝑌(𝑡)|𝑋and𝑃𝑌|𝑋,𝑇=𝑡is due to potential
hidden confounding, and the difference between 𝑃𝑋and𝑃𝑋|𝑇=𝑡is
due to intervention. Throughout this work, we stick to the nota-
tion of probability density (mass) functions instead of probability
measures. We use superscript 𝐼for interventional distribution and
𝑂for observational distribution. For a given treatment 𝑡∈{0,1},
 
398Conformal Counterfactual Inference under Hidden Confounding KDD ’24, August 25–29, 2024, Barcelona, Spain
we assume there are 𝑛observational and 𝑚interventional samples:
(𝑥𝑂,𝑇=𝑡
𝑖,𝑦𝑂,𝑇=𝑡
𝑖)𝑛
𝑖=1∼𝑝𝑂
𝑡(𝑥,𝑦)=𝑝𝑂(𝑦|𝑥,𝑡)𝑝(𝑥|𝑡)
(𝑥𝐼,𝑇=𝑡
𝑖,𝑦𝐼,𝑇=𝑡
𝑖)𝑛+𝑚
𝑖=𝑛+1∼𝑝𝐼
𝑡(𝑥,𝑦)=𝑝𝐼(𝑦|𝑥,𝑡)𝑝(𝑥)(1)
Given a predetermined target coverage rate of 1−𝛼, our goal is to
construct confidence interval 𝐶for potential outcome under treat-
ment𝑡at a new test sample 𝑥𝑛+𝑚+1∼𝑝(𝑥), such that𝐶(𝑥𝑛+𝑚+1)
ensures marginal coverage: P(𝑦𝑛+𝑚+1∈𝐶(𝑥𝑛+𝑚+1))≥1−𝛼, where
the probability is over (𝑥𝑛+𝑚+1,𝑦𝑛+𝑚+1)∼𝑝𝐼
𝑡(𝑥,𝑦).
2.2 Background: Conformal Prediction
Conformal prediction (CP) is a distribution-free framework that
provides finite-sample marginal coverage guarantees. Transductive
and split CP are two approaches to conformal prediction and we
briefly introduce both since we will be using them in Section 3.
Split Conformal Prediction (SCP). Given a datasetD=(𝑥𝑖,𝑦𝑖)𝑛
𝑖=1∼
𝑃𝑋,𝑌, SCP starts by splitting Dinto two disjoint subsets: a train-
ing setD𝑡, and a calibration set D𝑐. Then, a regression estimator
b𝜇is trained onD𝑡and conformity scores 𝑠(𝑥,𝑦)are computed
for(𝑥,𝑦) ∈ D𝑐where typically 𝑠(𝑥,𝑦)=|𝑦−b𝜇(𝑥)|. The em-
pirical distribution of the conformity scores are defined as b𝐹=
1
|D𝑐|Í|D𝑐|
𝑖=1𝛿𝑠(𝑥𝑖,𝑦𝑖)and the confidence interval for the target sam-
ple𝑥𝑛+1is
𝐶SCP(𝑥𝑛+1)=[b𝜇(𝑥𝑛+1)−𝑞b𝐹,b𝜇(𝑥𝑛+1)+𝑞b𝐹] (2)
where𝑞b𝐹=Quantile((1−𝛼)(1+1
|D𝑐|);b𝐹). [33] has proved that
under exchangeability of D,𝐶SCP(𝑥𝑛+1)is guaranteed to satisfy
marginal coverage. Futhermore, if ties between conformity scores
occur with probability zero, then
1−𝛼≤P(𝑦𝑛+1∈𝐶SCP(𝑥𝑛+1))≤1−𝛼+1
|D𝑐|(3)
Note that the upper bound ensures that the confidence interval is
nonvacuuous, i.e., the interval width does not go to infinity.
Transductive Conformal Prediction (TCP). Given a same dataset
Das above, TCP takes a different approach by looping over all pos-
sible values 𝑦in the domainY. For𝑦∈Y, TCP first constructs an
augmented dataset D(𝑥𝑛+1,𝑦)=D∪{𝑥𝑛+1,𝑦}. Then, a regression
estimator b𝜇𝑦is trained onD(𝑥𝑛+1,𝑦)and the conformity scores read
𝑠𝑦
𝑖=|𝑦𝑖−b𝜇𝑦(𝑥𝑖)|for𝑖=1,···,𝑛and𝑠𝑦
𝑛+1=|𝑦−b𝜇𝑦(𝑥𝑛+1)|. With
empirical distribution defined as b𝐹=1
𝑛+1Í𝑛
𝑖=1𝛿𝑠𝑦
𝑖+1
𝑛+1𝛿∞, the
interval for the target sample 𝑥𝑛+1is
𝐶TCP(𝑥𝑛+1)={𝑦∈Y :𝑠𝑦
𝑛+1≤𝑞b𝐹} (4)
where𝑞b𝐹=Quantile((1−𝛼);b𝐹). The same lower and upper bound
guarantee as (3) has been proved in [33].
TCP is computationally more expensive as it requires fitting b𝜇
for every fixed 𝑦∈Y. The discretization of Ycomes as a tradeoff
between computational costs and accuracy of the conformal inter-
val. For these reasons, SCP is more widely used due to its simplicity,
however, SCP is less sample efficient by splitting the dataset into a
training set and a calibration set. Cross-conformal prediction can
be used to improve efficiency for SCP [34].2.3 Weighted Conformal Prediction
When calibration and test data are independent yet not drawn from
the same distribution, [ 35] propose a weighted version of conformal
prediction. In this section, we discuss a more specific setting of [ 35]
where the dataset are merged from two different distributions, D=
{(𝑥𝑖,𝑦𝑖)𝑛
𝑖=1∼𝑃𝑋,𝑌}∪{(𝑥𝑖,𝑦𝑖)𝑛+𝑚
𝑖=𝑛+1∼𝑃′
𝑋,𝑌}and the test sample
𝑥𝑛+𝑚+1is sampled from 𝑃′
𝑋. Define the density ratio as 𝑟(𝑥,𝑦)=
𝑑𝑃′
𝑋,𝑌
𝑑𝑃𝑋,𝑌(𝑥,𝑦), then(𝑥𝑖,𝑦𝑖)𝑛+𝑚+1
𝑖=1are weighted exchangeable with
weight functions 𝑤(𝑥,𝑦)=1if(𝑥,𝑦)∼𝑃𝑋,𝑌and𝑤(𝑥,𝑦)=𝑟(𝑥,𝑦)
if(𝑥,𝑦)∼𝑃′
𝑋,𝑌. For𝑦∈Y, define the normalized weights 𝑝𝑖as:
𝑝𝑖=Í
𝜎:𝜎(𝑛+𝑚+1)=𝑖𝑛+𝑚+1Î
𝑗=𝑛+1𝑟(𝑥𝜎(𝑗),𝑦𝜎(𝑗))
Í
𝜎𝑛+𝑚+1Î
𝑗=𝑛+1𝑟(𝑥𝜎(𝑗),𝑦𝜎(𝑗))(5)
where the summations are taken over permutations 𝜎of1,···,𝑛+
𝑚+1(see [ 35, Lemma 3]). Here in Eq. (5), we use an abuse of notation
that𝑦𝑛+𝑚+1=𝑦for symmetry reason. With the conformity scores
𝑠𝑦
𝑖computed in the same way as TCP and the weighted empirical
distribution of the conformity scores defined as b𝐹=Í𝑛+𝑚
𝑖=1𝑝𝑖𝛿𝑠𝑦
𝑖+
𝑝𝑛+𝑚+1𝛿∞, the conformal interval for the target sample is:
𝐶w-TCP(𝑥𝑛+𝑚+1)={𝑦∈Y :𝑠𝑦
𝑛+𝑚+1≤𝑞b𝐹} (6)
where𝑞b𝐹=Quantile(1−𝛼;b𝐹). The lower bound guarantee is
proven in [ 35] and the upper bound is proven in [ 1] under extra
assumptions. When 𝑚=0,𝑝𝑖becomes𝑟(𝑥𝑖,𝑦𝑖)/Í𝑛+1
𝑗=1𝑟(𝑥𝑗,𝑦𝑗),
which is more commonly used in the literature [ 1,11,36]. When
𝑚>1, the computational cost of 𝑝𝑖is𝑚𝐶𝑚
𝑛+𝑚+1=O(𝑚𝑛𝑚).
3 CONFORMAL PREDICTION OF
COUNTERFACTUALS: WTCP-DR
In this section, we formally introduce our proposed method weighted
Transductive Conformal Prediction with Density Ratio estimation
(wTCP-DR). Since our method considers 𝑇=0and𝑇=1sepa-
rately, we fix 𝑇=𝑡in this section and drop the dependence on 𝑇in
Eq.(1)for simplicity of notations. Recall there are 𝑛observational
and𝑚interventional samples and the test sample is 𝑥𝑛+𝑚+1.
(𝑥𝑂
𝑖,𝑦𝑂
𝑖)𝑛
𝑖=1∼𝑝𝑂(𝑥,𝑦)=𝑝𝑂(𝑦|𝑥,𝑡)𝑝(𝑥|𝑡)
(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1∼𝑝𝐼(𝑥,𝑦)=𝑝𝐼(𝑦|𝑥,𝑡)𝑝(𝑥)(7)
The Naive Method. We first introduce a straightforward method:
constructing confidence interval for the potential outcome only
from interventional data (𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1using standard split confor-
mal prediction of Eq. (2)as(𝑥𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1come from the same dis-
tribution as the test sample 𝑥𝐼
𝑛+𝑚+1. The algorithm is detailed in
Algorithm 1. From Eq. (3) we know that
1−𝛼+1
𝑚+1≥P(𝑦∈𝐶naive(𝑥))≥ 1−𝛼 (8)
This approach can be inefficient because it completely ignores 𝑛
observational data and typically 𝑛is larger than 𝑚.
To combine both 𝑚interventional data and 𝑛observational data,
it is necessary to take distribution shift into consideration. There-
fore, weighted conformal prediction of Eq. (6)is naturally suitable
 
399KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
Algorithm 1 Naive algorithm
Require: level𝛼, interventional data D𝐼=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1split into
a training foldD𝐼
𝑡and a calibration fold D𝐼𝑐, target sample
𝑥𝐼
𝑛+𝑚+1.
1:Fit regression model ˆ𝜇onD𝐼
𝑡.
2:foreach sample(𝑥𝑖,𝑦𝑖)∈D𝐼𝑐do
3: Compute the conformity score 𝑠𝑖=|ˆ𝜇(𝑥𝑖)−𝑦𝑖|.
4:end for
5:Construct empirical distribution of conformity scores b𝐹=
1
|D𝐼𝑐|Í|D𝐼
𝑐|
𝑖=1𝛿𝑠𝑖.
6:Compute𝑞b𝐹=Quantile((1−𝛼)(1+1
|D𝑐|);b𝐹).
Ensure:𝐶𝑛𝑎𝑖𝑣𝑒(𝑥𝐼
𝑛+𝑚+1)=[ˆ𝜇(𝑥𝐼
𝑛+𝑚+1)−𝑞b𝐹,ˆ𝜇(𝑥𝐼
𝑛+𝑚+1)+𝑞b𝐹]
for such tasks, and the key challenge is to identify the normalized
weights in Eq. (5), i.e., to identify the density ratio
𝑟(𝑥,𝑦):=𝑝𝐼(𝑥,𝑦)
𝑝𝑂(𝑥,𝑦)=𝑝𝐼(𝑦|𝑥,𝑡)𝑝(𝑥)
𝑝𝑂(𝑦|𝑥,𝑡)𝑝(𝑥|𝑡)(9)
Under the unconfoundedness assumption of [ 1],𝑝𝐼(𝑦|𝑥,𝑡)
equals𝑝𝑂(𝑦|𝑥,𝑡)so𝑟(𝑥,𝑦)is as simple as estimating the propen-
sity score𝑝(𝑥)/𝑝(𝑥|𝑡). When hidden confouding exists, propen-
sity score is not enough to account for the distribution shift. Our
method proposes to learn 𝑟(𝑥,𝑦)from data, as detailed next.
Weighted Transductive Conformal Prediction with Density
Ratio estimation (wTCP-DR). The key of weighted conformal
prediction is the density ratio 𝑟(𝑥,𝑦), and fortunately there exists a
rich literature of density ratio estimation [ 37], including moment
matching [ 38], probabilistic classification and ratio matching. Since
probabilistic classification using neural networks is more flexible
and better exploits nonlinear relations in the data [ 39], so we only
introduce probabilistic classification here and refer the readers to
[37] for a comprehensive review.
By assigning labels 𝑧=1to observational data (𝑥𝑂
𝑖,𝑦𝑂
𝑖)and
assigning labels 𝑧=0to interventional data (𝑥𝐼
𝑖,𝑦𝐼
𝑖), we construct
a new dataset for learning the density ratio.
DDR={(𝑥𝑂
𝑖,𝑦𝑂
𝑖,𝑧𝑖)𝑛
𝑖=1,(𝑥𝐼
𝑖,𝑦𝐼
𝑖,𝑧𝑖)𝑛+𝑚
𝑖=𝑛+1}
For any nonlinear binary classification algorithm like logistic re-
gression with nonlinear features, random forests or neural net-
works that output estimated probabilities of class membership
ˆ𝑝(𝑧=1|𝑥,𝑦)and ˆ𝑝(𝑧=0|𝑥,𝑦), the density ratio can be ap-
proximated by:
𝑝𝐼(𝑥,𝑦)
𝑝𝑂(𝑥,𝑦)=𝑝(𝑥,𝑦|𝑧=0)
𝑝(𝑥,𝑦|𝑧=1)=𝑝(𝑧=0|𝑥,𝑦)/𝑝(𝑧=0)
𝑝(𝑧=1|𝑥,𝑦)/𝑝(𝑧=1)
≈𝑝(𝑧=1)
𝑝(𝑧=0)ˆ𝑝(𝑧=0|𝑥,𝑦)
ˆ𝑝(𝑧=1|𝑥,𝑦)(10)
Since𝑝(𝑧=1)
𝑝(𝑧=0)is a constant and will cancel out when computing the
normalized weights in Eq. (5), we denote ˆ𝑟(𝑥,𝑦)=ˆ𝑝(𝑧=0|𝑥,𝑦)
ˆ𝑝(𝑧=1|𝑥,𝑦)as the
estimated density ratio, so the corresponding estimated normalizedweights of Eq. (5) are:
ˆ𝑝𝑖=Í
𝜎:𝜎(𝑛+𝑚+1)=𝑖𝑛+𝑚+1Î
𝑗=𝑛+1ˆ𝑟(𝑥𝜎(𝑗),𝑦𝜎(𝑗))
Í
𝜎𝑛+𝑚+1Î
𝑗=𝑛+1ˆ𝑟(𝑥𝜎(𝑗),𝑦𝜎(𝑗))(11)
Unfortunately, Eq. (11)requires𝑚𝐶𝑚
𝑛+𝑚+1=O(𝑚𝑛𝑚)times of eval-
uating ˆ𝑟which is computationally impractical for 𝑚>1. As a result,
we only use observational data when computing the normalized
weights (i.e. 𝑚=1) and use interventional data for computing the
density ratio ˆ𝑟, so the estimated normalized weights become
ˆ𝑝𝑖=ˆ𝑟(𝑥𝑖,𝑦𝑖)Í𝑛
𝑗=1ˆ𝑟(𝑥𝑗,𝑦𝑗)+ˆ𝑟(𝑥𝑛+𝑚+1,𝑦𝑛+𝑚+1)(12)
for𝑖={1,···,𝑛}∪{𝑛+𝑚+1}. See Algorithm 2 for a complete
description of our method.
By using estimated normalized weights ˆ𝑝𝑖rather than the oracle
normalized weights 𝑝𝑖to reweight the empirical distribution of
conformity scores b𝐹, our approach introduces an extra source of
error, as quantified below.
Proposition 1 (Prosample 4.2 from [ 36]).Under the assump-
tions that𝑝𝑂(𝑥,𝑦)and𝑝𝐼(𝑥,𝑦)are absolutely continuous with each
other and that[E𝑝𝑂(𝑥,𝑦)ˆ𝑟(𝑥,𝑦)2]1/2<𝑀then the confidence inter-
val𝐶wTCP-DR constructed from Algorithm 2 satisfies
1−𝛼+𝑐𝑛−1/2+Δ𝑟≥P(𝑦∈𝐶wTCP-DR(𝑥))≥1−𝛼−Δ𝑟 (13)
where𝑐is a constant and Δ𝑟=E𝑝𝑂(𝑥,𝑦)|𝑟(𝑥,𝑦)−ˆ𝑟(𝑥,𝑦)|is the
approximation error of the density ratio.
Algorithm 2 Weighted Transductive Conformal Prediction with
Density Ratio Estimation (wTCP-DR)
Require: level𝛼, observational data D𝑂=(𝑥𝑂
𝑖,𝑦𝑂
𝑖)𝑛
𝑖=1and inter-
ventional dataD𝐼=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1, test sample 𝑥𝐼
𝑛+𝑚+1.
1:Initialize𝐶wTCP-DR(𝑥𝐼
𝑛+𝑚+1)=∅.
2:Estimate the density ratio ˆ𝑟usingD𝑂andD𝐼.
3:for𝑦∈Ydo
4: Construct augmented dataset D𝑦=D𝑂∪{𝑥𝐼
𝑛+𝑚+1,𝑦}.
5: Fit a regression model ˆ𝜇onD𝑦.
6: Compute conformity scores 𝑠𝑦
𝑖=|ˆ𝜇(𝑥𝑂
𝑖)−𝑦𝑂
𝑖|for𝑖=
1,···,𝑛and𝑠𝑦
𝑛+𝑚+1=|ˆ𝜇(𝑥𝐼
𝑛+𝑚+1)−𝑦|.
7: Compute the normalized weights ˆ𝑝𝑖as in Eq. (12)(𝑦𝑛+𝑚+1
is replace with 𝑦).
8: Construct weighted empirical distribution of conformity
scoresb𝐹=Í𝑛
𝑖=1ˆ𝑝𝑖𝛿𝑠𝑦
𝑖+ˆ𝑝𝑛+𝑚+1𝛿∞.
9: Compute quantile 𝑞b𝐹=Quantile(1−𝛼;b𝐹).
10: if𝑠𝑛+𝑚+1≤𝑞b𝐹then
11:𝐶wTCP-DR(𝑥𝐼
𝑛+𝑚+1)=𝐶wTCP-DR(𝑥𝐼
𝑛+𝑚+1)∪{𝑦}.
12: end if
13:end for
Ensure:𝐶wTCP-DR(𝑥𝐼
𝑛+𝑚+1).
By comparing Eq. (13)and Eq. (8), we can see that when we have
access to the oracle density ratio 𝑟(𝑥,𝑦), i.eΔ𝑟=0, then wTCP-DR
 
400Conformal Counterfactual Inference under Hidden Confounding KDD ’24, August 25–29, 2024, Barcelona, Spain
obtains a tighter upper bound than the naive method, as typically
the number of observational data 𝑛is much larger than the number
of interventional data 𝑚in causal inference, due to the higher cost
of randomized controlled trails. Unfortunately, oracle density ratio
𝑟(𝑥,𝑦)is usually unavailable, and the estimation error of density
ratio is of order min(𝑛,𝑚)−1/2=𝑚−1/2for moment matching
or ratio matching [ 37,39] and of order 𝑚−1/2for probabilistic
classification [ 40]. It seems that wTCP-DR has spent a huge amount
of effort while achieving a worse result in the end.
However, we would like to emphasize that the efficiency of con-
formal prediction methods is quantified by the width of the con-
fidence interval, not by the difference between the probability
upper and lower bound. An upper bound strictly lower than 1guar-
antees that the confidence interval is not arbitrarily large, however
there is no guarantee that a smaller upper bound results in a smaller
confidence interval. Intuitively, our method has a smaller interval
compared to the naive method, because the regression model ˆ𝜇
of wTCP-DR is trained on 𝑛observational data while the regres-
sion model ˆ𝜇of naive method is trained on 𝑚interventional data.
Intuitively, there is a higher chance that the conformity scores
of wTCP-DR are smaller than the conformity scores of the naive
method, which means that 𝐶wTCP-DR is a smaller interval than
𝐶naive. We formalize the above intuition in the following section
for additive Gaussian noise model.
3.1 Case Study: Additive Gaussian Noise Model
In this section, we consider an additive Gaussian noise model, which
is a simple yet popular setting in causal inference [ 41]. Recall that
we fix𝑇=𝑡and drop the dependence on 𝑇for simplicity of nota-
tions. Specifically, we make the following assumptions:
A1Additive Gaussian noise. 𝑦𝑂∼N(𝜃𝑂⊤𝜑(𝑥𝑂),𝜎2)and𝑦𝐼∼
N(𝜃𝐼⊤𝜑(𝑥𝐼),𝜎2), where𝜑represents the (learned) features of
interventional and observational data.
A2 Gaussian features. 𝜑(𝑥𝑂)∼N( 0,Σ𝑂)and𝜑(𝑥𝐼)∼N( 0,Σ𝐼).
A3Upper bounds on the difference between oracle density ratio
𝑟(𝑥,𝑦)and estimated density ratio ˆ𝑟(𝑥,𝑦).
E𝑝𝑂(𝑥,𝑦)(𝑟(𝑥,𝑦)−ˆ𝑟(𝑥,𝑦))2<∞
Δ𝑟:=E𝑝𝑂(𝑥,𝑦)|𝑟(𝑥,𝑦)−ˆ𝑟(𝑥,𝑦)|<1−𝛼
𝛼
A4 Bounded 𝜒2divergence between 𝑝𝐼(𝑥,𝑦)and𝑝𝑂(𝑥,𝑦).
𝜒2(𝑝𝐼∥𝑝𝑂)=∫𝑝𝐼(𝑥,𝑦)
𝑝𝑂(𝑥,𝑦)−12
𝑝𝑂(𝑥,𝑦)𝑑𝑥𝑑𝑦 <∞
Under these assumptions, the effect of hidden confounding is re-
flected from the difference of 𝑝𝑂(𝑦|𝑥,𝑡)𝑝𝐼(𝑦|𝑥,𝑡)through the
difference of 𝜃𝑂and𝜃𝐼:𝜃𝑂is dependent of hidden confounding 𝑢
whereas𝜃𝐼is independent of 𝑢due to intervention. Before showing
our main theoretical result, let us first discuss the implications of
these assumptions.
A1We assume that interventional and observational data share the
same feature 𝜑, a commonly used setting in causal inference
especially when 𝜑is learned with neural networks [ 12]. We
assume the same noise scale for observational and interven-
tional data only for simplicity, which can be relaxed to the more
general case that 𝑦𝑂and𝑦𝐼have different noise scales 𝜎𝑂,𝜎𝐼.A2This assumption is satisfied when either the features are de-
signed to have Gaussian distribution, or the features are learned
from wide enough neural networks [42].
A3This assumption requires that the error of density ratio estima-
tion is upper bounded, and given that 𝛼is typically 0.1or0.05,
this assumption is usually satisfied in practice.
A4This assumption ensures that 𝑝𝐼and𝑝𝑂share the same support
overX×Y , and is required such that the central limit theorem
can be used in the proof.
Now we give the main theoretical result of this paper.
Theorem 1. Assume the above assumptions hold, with probability
at least 1−𝛿1−𝛿2−𝛿3−𝛿4, the interval 𝐶wTCP-DR(𝑥𝐼
𝑛+𝑚+1)obtained
from Algorithm 2 will be smaller than the interval 𝐶naive(𝑥𝐼
𝑛+𝑚+1)
obtained from Algorithm 1 up to O(√︁
𝑙𝑜𝑔𝑛/𝑛), with𝛿1,𝛿2,𝛿3,𝛿4being
the following:
𝛿1=©­
«2
𝑛1−𝛼−Δ𝑟
Δ𝑟+1
𝛼+Δ𝑟
Δ𝑟+1𝑝𝑂(𝑥)
𝑝𝐼(𝑥)ª®
¬4𝜎2√︃
𝐶1
𝐶2
,𝛿2=2
𝑛,
𝛿3=exp
−1
2𝐿2
1−𝛼
erf−1(1−𝛼)2(𝑑−1)2
𝑚−1
,
𝛿4=exp
−𝐶2
𝛼𝑛eff
(𝑚−𝑑)2
where𝐶1
𝐶2=(𝜃𝐼+𝜃𝑂)⊤Σ𝐼(𝜃𝐼+𝜃𝑂)
(𝜃𝐼−𝜃𝑂)⊤Σ𝐼(𝜃𝐼−𝜃𝑂)represent the dissimilarity distance
between𝜃𝐼and𝜃𝑂;erf−1is the inverse error function [ 43],𝐿1−𝛼
and𝐶𝛼are constants that only depend on 𝛼; and𝑛effis the effective
sample size defined as below
𝑛eff= 𝑛∑︁
𝑖=1ˆ𝑟(𝑥𝑂
𝑖)!2.𝑛∑︁
𝑖=1ˆ𝑟(𝑥𝑂
𝑖)2(14)
The proof of Theorem 1 can be found in the Appendix A and B
of https://arxiv.org/abs/2405.12387. The implications of Theorem 1
is summarized as below.
(1)𝛿1quantifies the number of observational data needed to con-
tain sufficient information about the interventional distribution.
If𝜃𝐼and𝜃𝑂are very close, which means that the distribu-
tions𝑝𝐼(𝑥,𝑦)and𝑝𝑂(𝑥,𝑦)are very similar, the exponenet𝐶1
𝐶2
is bigger so fewer observational data (smaller 𝑛) would contain
sufficient information of the interventional distributions.
(2)𝛿2quantifies the stability of the estimator used. Since we are
using the least squared estimator which is known to be stable
when𝑛>𝑑and𝑚>𝑑, having more 𝑛would entail smaller 𝛿2.
(3)𝛿3and𝛿4quantifies the ratio of the effective sample size 𝑛eff
and the interventional sample size 𝑚.𝑛effwas first defined
by [38] in covariate shift literature and [ 35] gives an intuition
that the performance of weighted conformal prediction should
depend on𝑛eff, our theorem is the first to quantitatively show
that𝑛effrather than𝑛is the key to measure the performance of
weighed conformal prediction when compared against standard
conformal prediction.
From Theorem 1, we can see that our method in Algorithm 2
is more efficient than the naive method in Algorithm 1 in terms
of width of confidence interval provided, when the interventional
distribution is close to the observational distribution, when the
 
401KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
dimension𝑑is relatively high compared to the number of inter-
ventional data 𝑚, and when the effective sample size 𝑛effis larger
than𝑚. The theoretical result is further corroborated by empirical
findings in Section 6.
Algorithm 3 Two-stage wSCP-DR (Inexact)
Require: Level𝛼, observational data D𝑂=(𝑥𝑂
𝑖,𝑦𝑂
𝑖)𝑛
𝑖=1and inter-
ventional dataD𝐼=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1, test sample 𝑥𝐼
𝑛+𝑚+1.
1:UseD𝑂andD𝐼to estimate the density ratio ˆ𝑟.
2:# First stage.
3:for𝑥𝐼
𝑗,𝑦𝐼
𝑗∈D𝐼do
4: Fit a regression model ˆ𝜇onD𝑂∪(𝑥𝐼
𝑗,𝑦𝐼
𝑗).
5: Compute conformity scores 𝑠𝑖=|ˆ𝜇(𝑥𝑂
𝑖)−𝑦𝑂
𝑖|.
6: Compute the normalized weights ˆ𝑝𝑖as in Eq. (12).
7: Construct weighted empirical distribution of conformity
scoresb𝐹=Í𝑛
𝑖=1ˆ𝑝𝑖𝛿𝑠𝑖+ˆ𝑝𝑗𝛿∞.
8: Compute quantile 𝑞b𝐹=Quantile(1−𝛼;b𝐹).
9:𝐶𝐿
𝑗=ˆ𝜇(𝑥𝐼
𝑗)−𝑞b𝐹and𝐶𝑅
𝑗=ˆ𝜇(𝑥𝐼
𝑗)+𝑞b𝐹
10:end for
11:# Second stage.
12:Fit regressor ˆ𝑚𝐿on(𝑥𝐼
𝑛+1,𝐶𝐿
𝑛+1),···,(𝑥𝐼𝑛+𝑚,𝐶𝐿𝑛+𝑚), and fit re-
gressor ˆ𝑚𝑅on(𝑥𝐼
𝑛+1,𝐶𝑅
𝑛+1),···,(𝑥𝐼𝑛+𝑚,𝐶𝑅𝑛+𝑚).
Ensure:𝐶𝐼𝑛𝑒𝑥𝑎𝑐𝑡
𝑤𝑆𝐶𝑃−𝐷𝑅(𝑥𝐼
𝑛+𝑚+1)=[ˆ𝑚𝐿(𝑥𝐼
𝑛+𝑚+1),ˆ𝑚𝑅(𝑥𝐼
𝑛+𝑚+1)]
4 PRACTICAL ALGORITHM: WSCP-DR
In practice, although transductive conformal prediction in Algo-
rithm 2 is theoretically well-grounded, it is notoriously expensive
to compute, compared to split conformal prediction. The reason
that split conformal prediction cannot be used in Algorithm 2 is the
density ratio ˆ𝑟evaluated at test sample, which requires the knowl-
edge of both test covariate 𝑥𝑛+𝑚+1and test target value 𝑦𝑛+𝑚+1but
unfortunately 𝑦𝑛+𝑚+1is inaccessible to us. In this section, we show
that we can do two-stage split conformal prediction which is com-
putationally more efficient than transductive conformal prediction
Algorithm 2 and achieves the same marginal coverage guarantee.
In the first stage, recall that interventional labels 𝑦𝐼
𝑛+1,···,𝑦𝐼𝑛+𝑚
are accessible, so the density ratios ˆ𝑟(𝑥𝐼
𝑛+1,𝑦𝐼
𝑛+1),···,ˆ𝑟(𝑥𝐼𝑛+𝑚,𝑦𝐼𝑛+𝑚)
and the normalized conformal weights in Eq. (5)can be computed
for𝑛+1,···,𝑛+𝑚. Therefore, split weighted conformal prediction
can be used to construct intervals (𝐶𝐿
𝑛+1,𝐶𝑅
𝑛+1),···,(𝐶𝐿𝑛+𝑚,𝐶𝑅𝑛+𝑚)
for interventional data (𝑥𝐼
𝑛+1,𝑦𝐼
𝑛+1),···,(𝑥𝐼𝑛+𝑚,𝑦𝐼𝑛+𝑚)with mar-
ginal coverage guarantee. In the second stage, by noticing that the
test sample𝑥𝐼
𝑛+𝑚+1shares the same distribution as 𝑥𝐼
𝑛+1,···,𝑥𝐼𝑛+𝑚,
a standard split conformal prediction can be used to construct con-
fidence interval[𝐶𝐿𝑥𝑛+𝑚+1,𝐶𝑅
𝑛+𝑚+1]for the test sample 𝑥𝑛+𝑚+1with
marginal coverage guarantee. Details of this method are presented
in Algorithm 4. Additionally, we can further reduce the computa-
tional cost of Algorithm 4 by directly fitting a regressor ˆ𝜇𝐿over the
interval lower bounds (𝑥𝐼
𝑛+1,𝐶𝐿
𝑛+1),···,(𝑥𝐼
𝑛+1,𝐶𝐿𝑛+𝑚)and fitting
a regressor ˆ𝜇𝑅over the interval upper bounds (𝑥𝐼
𝑛+1,𝐶𝑅
𝑛+1),···,
(𝑥𝐼
𝑛+1,𝐶𝑅𝑛+𝑚)in the second stage. Therefore, we call Algorithm 4the exact two-stage method which has marginal coverage guaran-
tee and call Algorithm 3 the inexact two-stage method which does
not have marginal coverage guarantee but is more efficient.
Algorithm 4 Two-stage wSCP-DR (Exact)
Require: Level𝛼, observational data D𝑂=(𝑥𝑂
𝑖,𝑦𝑂
𝑖)𝑛
𝑖=1and inter-
ventional dataD𝐼=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚
𝑖=𝑛+1, test sample 𝑥𝐼
𝑛+𝑚+1.
1:UseD𝑂andD𝐼to estimate the density ratio ˆ𝑟.
2:# First stage.
3:Same as the first stage in Algorithm 3
4:# Second stage.
5:SplitD𝐼into a training set of size 𝑚1:D𝐼
𝑡𝑟=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑛+𝑚1
𝑖=𝑛+1and
calibration set of size 𝑚−𝑚1:D𝐼
𝑐𝑎𝑙=(𝑥𝐼
𝑖,𝑦𝐼
𝑖)𝑚
𝑖=𝑚1+1.
6:Fit regressor ˆ𝑚𝐿on(𝑥𝐼
𝑛+1,𝐶𝐿
𝑛+1),···,(𝑥𝐼𝑛+𝑚1,𝐶𝐿𝑛+𝑚1)and ˆ𝑚𝑅
on(𝑥𝐼
𝑛+1,𝐶𝑅
𝑛+1),···,(𝑥𝐼𝑛+𝑚1,𝐶𝑅𝑛+𝑚1).
7:Compute conformity scores on D𝐼
𝑐𝑎𝑙:𝑠𝑖=max{ˆ𝑚𝐿(𝑥𝐼
𝑖)−
𝐶𝐿
𝑖,𝐶𝑅
𝑖−ˆ𝑚𝑅(𝑥𝐼
𝑖)}for𝑖={𝑚1+1,···,𝑚}.
8:Construct empirical distribution of conformity scores b𝐹=
1
𝑚−𝑚1Í𝑚
𝑖=𝑚1+1𝛿𝑠𝑖.
9:Compute𝑞b𝐹=Quantile((1−𝛼)(1+1
𝑚−𝑚1);b𝐹).
Ensure:𝐶𝐸𝑥𝑎𝑐𝑡
𝑤𝑆𝐶𝑃−𝐷𝑅(𝑥𝐼
𝑛+𝑚+1)=[ˆ𝑚𝐿(𝑥𝐼
𝑛+𝑚+1)−𝑞b𝐹,ˆ𝑚𝑅(𝑥𝐼
𝑛+𝑚+1)+𝑞b𝐹]
Conformal Inference of Individual Treatment Effect. In Section
3 and 4, we focus on conformal inference for counterfactual out-
comes𝑌(1)and𝑌(0). However, offering confidence intervals for
individual treatment effects may hold greater practical significance.
Our algorithms wTCP-DR and wSCP-DR can predict confidence
intervals[𝐶𝐿
𝑡(𝑥𝐼
𝑛+𝑚+1),𝐶𝑅
𝑡(𝑥𝐼
𝑛+𝑚+1)],𝑡∈{0,1}that has marginal
coverage guarantee for the potential outcome 𝑦𝑛+𝑚+1under treat-
ment𝑡=1(or under control 𝑡=0). The naive way of construcing
intervals for ITE is to use bonferroni correction, i.e., 𝐶𝐿
𝐼𝑇𝐸=𝐶𝐿
1−𝐶𝑅
0
and𝐶𝑅
𝐼𝑇𝐸=𝐶𝑅
1−𝐶𝐿
0. We demonstrate the empirical result using
the naive way in Section 6 for fair comparison among methods that
infer counterfactual outcomes, and we also include the results in
Appendix A.1 where intervals for ITE are constructed using the
nested methods from [1, Section 4].
5 EXPERIMENTS
5.1 Experiment on Synthetic Data
Here, we conduct experiments for counterfactual outcome and ITE
estimation on synthetic data with hidden confounding and focus
on the setting where the number of observational data 𝑛is larger
than the number of interventional data 𝑚. We aim to answer the
following research questions: RQ1: Can our proposed methods
achieve the specified level of coverage (0.9) for potential outcomes
under the setting with hidden confounding and 𝑛larger than 𝑚
for counterfactual outcomes and ITEs? RQ2: Can our proposed
methods have better efficiency (smaller confidence interval) than
the Naive method which only uses interventional data? RQ3: How
does hidden confounding strength impact the coverage of our meth-
ods? RQ4: How does the size of interventional data ( 𝑚) impact the
efficiency of our methods?
 
402Conformal Counterfactual Inference under Hidden Confounding KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Description for synthetic data, Yahoo and Coat
Dataset 𝑛𝑡𝑟𝑛𝑐𝑎𝑙𝑚𝑡𝑟𝑚𝑐𝑎𝑙𝑚𝑡𝑠
Synthetic 5,000 5,000 125 125 200
Yahoo 103,343 25,706 10,800 10,800 32,399
Coat 5,568 1,385 928 928 2,784
Dataset. For synthetic data, we use the following data-generating
process for the observables 𝑋,𝑇,𝑌 with hidden confounding 𝑈.
𝑈,𝑍∼N( 0,I),𝜖1,𝜖0∼N( 0,1)
𝑋=𝑍⊙(𝑎2(1−𝑈)+𝑏2𝑈)+𝑈
𝜌=𝑐¯𝑈+(1−𝑐)(1−¯𝑈), 𝑇∼Bern(𝜌)
𝑌(1)=1
1+exp(−3(¯𝑈+2))+0.1𝜖1
𝑌(0)=1
1+exp(−3(¯𝑈−2))+0.1𝜖0
𝑌=𝑇𝑌(1)+( 1−𝑇)𝑌(0)(15)
Iis𝑑×𝑑identity matrix, 𝑑is the dimensionality of 𝑋,⊙is the
hadamard product, ¯𝑈is the mean of each dimension of 𝑈, and
𝑎=5,𝑏=3,𝑐=0.9. When𝑐is close to 1,𝜌is close to 0as¯𝑈is close
to0, leading to more controlled samples (less treated samples) in
the observational data.
Baselines. Naive : it uses interventional data for standard split con-
formal prediction, as detailed in Algorithm 1. WCP : the algorithm
proposed in [ 1] that uses propensity score as the reweighting func-
tion in WCP. For all the methods we use the same Gradient Boosting
Tree from scikit-learn as the base model ˆ𝜇.
Data Splitting Details. We split the observational and interven-
tional data into training D𝑂
𝑡𝑟,D𝐼
𝑡𝑟, calibrationD𝑂
𝑐𝑎𝑙,D𝐼
𝑐𝑎𝑙, and test
D𝑡𝑠. For the Naive method, we train the base model ˆ𝜇onD𝐼
𝑡𝑟
and compute conformity scores on D𝐼
𝑐𝑎𝑙. For WCP, we train the
base model ˆ𝜇onD𝑂
𝑡𝑟and compute conformity scores on D𝑂
𝑐𝑎𝑙. The
propensity model is trained on D𝑂
𝑡𝑟. For our methods, we train the
base model ˆ𝜇onD𝑂
𝑡𝑟and compute conformity scores on D𝑂
𝑐𝑎𝑙. The
density ratio estimator ˆ𝑟is trained onD𝑂
𝑡𝑟∪D𝐼
𝑡𝑟. The size of each
split can be found in Table 1.
Evaluation Metrics. We use the evaluation metrics from [ 1,25]
for both counterfactual outcomes and ITEs. Coverage measures the
probability of the true counterfactual outcome falling in predicted
confidence interval , where 1is the indicator function. Interval
width is the average size of the confidence interval 𝐶(𝑥𝑖)on test
samples𝑖∈ D𝑡𝑠, which represents the efficiency of conformal
inference methods.
Comparison Results (RQ1-2). Table 2 shows results under the
setting of𝑛=10,000and𝑚=250under strong hidden confounding
(𝑑=1). We make the following observations:
•In terms of coverage, our methods wSCP-DR (Exact) and wTCP-
DR achieve the specified level of coverage (0 .9) for𝑌(0),𝑌(1)
and ITE. wSCP-DR (Inexact) has coverage slightly lower than 0.9
for𝑌(1)and𝑌(0)as it trades coverage guarantee for lower com-
putational cost. The coverage results verify that our proposed
reweighting function based on density ratio estimation can accu-
rately adapt the conformity scores computed on observational
data to the interventional distribution even under hidden con-
founding. In contrast, coverage of WCP is much lower than 0.9,because WCP does not take hidden confounding into considera-
tion, which leads to biased estimates of propensity scores so even
after reweighting, the interventional data is not exchangeable
with the observational data. Therefore, the confidence interval
constructed by WCP does not have coverage guarantee.
•Considering interval width, wSCP-DR (Inexact) achieves much
better efficiency (narrower interval widths) than Naive for coun-
terfactual outcomes and ITE. As wSCP-DR (Exact) expands the
confidence interval to gain guaranteed coverage and has slightly
smaller interval width than the Naive method. WCP has the
smallest interval width, however, its confidence intervals cannot
contain the ground truth with 0.9probability as desired. In prac-
tice, we recommend using wSCP-DR (Inexact) for its enhanced
efficiency, if there is no strict requirement on coverage.
•There is a imbalance of the number of treated and controlled
samples in the observational data. Notice that 𝑐=0.9in Eq. (15)
means that the size of controlled group is larger than the size
of treated group in observational data. As a result, compared to
Naive method, wTCP-DR has smaller interval width for 𝑌(0), but
it has a similar interval width for 𝑌(1), due to the fact that only
the number of controlled samples is larger than 𝑚while the num-
ber of treated samples is at the same scale as 𝑚. This observation
verifies the theory of Theorem 1. Nevertheless, wTCP-DR’s ITE
interval is still smaller than Naive.
Impact of Hidden Confounding Strength on Coverage (RQ3).
Here, we modify the dimensionality of observed covariates 𝑑∈
{1,3,5,10}where larger 𝑑means weaker hidden confounding. Fig. 3
shows the results with varying hidden confounding strengths. We
make the following observations. At varying levels of hidden con-
founding strength, wSCP-DR (Exact) and Naive can maintain the
specified level of coverage. In contrast, coverage of wSCP-DR (In-
exact) is slightly lower than the specified level. When hidden con-
founding is stronger ( 𝑑is lower), WCP has lower coverage be-
cause it ignores hidden confounders and hence its propensity score
reweighted conformal prediction does not have guaranteed cover-
age. When hidden confounding gets weaker (larger 𝑑), the coverage
of WCP starts to improve, because propensity scores gets closer to
the true density ratio that accounts for the distribution shift.
Impact of Interventional Data Size 𝑚on Interval Width (RQ4).
Here, we study the impact of the size of interventional data 𝑚=
𝑚𝑡𝑟+𝑚𝑐𝑎𝑙on interval width, under strong hidden confounding
𝑑=1. Fig. 4 shows results with different 𝑚. The interval width
(efficiency) of the Naive method benefit the most from increasing
𝑚as its has more training samples and also a larger calibration set
for split conformal prediction, which agrees with Eq. (8). Increas-
ing𝑚has no significant impact on the efficiency of our methods,
which agrees with Eq. (13). The reason is that our methods only
use interventional data for density ratio estimation, so larger 𝑚
only improves the quality of estimated density ratios, which does
not impact the conformity scores because the scores are computed
on the observational data. For WCP, it does not use interventional
data at all, so increasing 𝑚also has no impact. As we discussed
before, due to the sample size difference between treatment group
and control group, wTCP-DR’s efficiency is worse for 𝑌(1)but its
interval width for ITE can still be narrower than that of Naive.
 
403KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
Table 2: Results for counterfactual outcomes and ITEs on the synthetic data. We compare our methods wSCP-DR (Inexact), wSCP-DR (Inexact),
and wTCP-DR with baselines. Results are shown for coverage and confidence interval width on the synthetic data with 𝑛=10,000and𝑚=250.
Boldface and underlining are used to highlight the top and second-best interval width among the methods with coverage close to 0.9.
Method Coverage𝑌(0)↑Interval Width 𝑌(0)↓Coverage𝑌(1)↑Interval Width 𝑌(1)↓Coverage ITE ↑Interval Width ITE ↓
wSCP-DR(Inexact) 0.891±0.026 0.414±0.008 0.889±0.019 0.421±0.013 0.942±0.017 0.835±0.016
wSCP-DR(Exact) 0.934±0.026 0.496±0.010 0.935±0.023 0.503±0.010 0.957±0.018 0.998±0.015
wTCP-DR 0.899±0.028 0.386±0.013 0.923±0.015 0.576±0.066 0.953±0.015 0.962±0.074
WCP 0.572±0.039 0.222±0.007 0.608±0.042 0.227±0.009 0.710±0.027 0.449±0.012
Naive 0.932±0.018 0.508±0.042 0.930±0.023 0.560±0.049 0.952±0.018 1.068±0.098
(a) Coverage of 𝑌(0)
 (b) Coverage of 𝑌(1)
 (c) Coverage of ITE
Figure 3: Coverage results of counterfactual outcomes and ITE with varying hidden confounding strength. Higher dimensional 𝑋carries more
information of the hidden confounders, leading to weaker hidden confounding. Their interval width results are in Fig. 5 of Appendix A.1.
(a) Interval width of 𝑌(0)with different 𝑚
(b) Interval width of 𝑌(1)with different 𝑚
Figure 4: Impact of interventional data size 𝑚on efficiency of con-
formal inference methods. See Appendix A.1 for coverage results.
5.2 Counterfactual Outcome Estimation on
Real-world Recommendation System Data
Causal recommendation datasets Yahoo!R31(Yahoo) and Coat2
can benchmark counterfactual outcome estimation under hidden
confounding [ 44–46]. Note that we use these datasets for coun-
terfactual regression, leaving ranking based evaluation for future
work. Following the formulation of [ 44,45], we define each sample
as a user-item pair, define treatment as whether the item is exposed
to the user, and define outcome as the user’s rating from 1 to 5. The
1https://webscope.sandbox.yahoo.com/
2https://www.cs.cornell.edu/~schnabts/mnar/goal is to predict potential outcome 𝑌(1)for the user-item pairs
in the test setD𝑡𝑠given the learned embeddings of a user-item
pair𝑋. Available information include massive observational data
from𝑃𝑋,𝑌|𝑇=1and a small set of interventional data from 𝑃𝑋,𝑌(1).
We run conformal inference on the top of the classic matrix fac-
torization model [ 47] trained onD𝐼
𝑡𝑟for Naive andD𝑂
𝑡𝑟for other
methods. The size of dataset split can be found in Table 1.
Methods for Comparison. In addition to wSCP-DR (Inexact and
Exact), we introduce their variants wSCP-DR* (Inexact and Exact)
that estimate the density ratio by learned embeddings as𝑝𝐼(𝑥)
𝑝𝑂(𝑥).
This is a favorable setting in practice because randomized controlled
trail is costly, whereas randomly assigned users without requiring
their outcomes under treatment is much cheaper and easier to
implement. We aim to illustrate our methods can perform well even
when there is no access to labeled interventional data. Here, we
do not consider wTCP-DR due to its high computational cost. For
baselines, we use Naive andWCP-NB – A variant of WCP which
uses interventional data with labels to train a Naive Bayes classifier
for estimating propensity scores as in [30, 46, 48].
Table 3: Coverage and interval width results on Yahoo and Coat.
Boldface and underlining are used to highlight the top and second-
best interval width among the methods with coverage close to 0.9.
Yahoo Coat
Method Coverage ↑ Interval Width ↓ Coverage ↑ Interval Width ↓
wSCP-DR(Inexact) 0.892±0.019 4.353±0.019 0.919±0.008 3.787±0.045
wSCP-DR(Exact) 0.952±0.001 5.140±0.001 0.959±0.001 4.565±0.228
wSCP-DR*(Inexact) 0.892±0.020 4.353±0.020 0.919±0.008 3.789±0.046
wSCP-DR*(Exact) 0.952±0.001 5.140±0.001 0.960±0.001 4.571±0.233
WCP-NB 0.825±0.002 4.036±0.002 0.912±0.005 3.635±0.040
Naive 0.899±0.001 6.047±0.001 0.896±0.003 7.725±0.018
Comparison Results (RQ1-2). We fix𝑚𝑡𝑟=𝑚𝑐𝑎𝑙for Yahoo and
Coat to ensure 𝑛larger than𝑚and𝑚𝑡𝑠is large enough (see Table 1).
Studies on𝑚𝑡𝑟and𝑚𝑐𝑎𝑙can be found in Appendix A.1. Table 3
shows results on these two datasets. Our methods achieve 0.9cover-
age and have significantly smaller intervals than the Naive method.
Surprisingly, even when the density ratio is estimated only from
 
404Conformal Counterfactual Inference under Hidden Confounding KDD ’24, August 25–29, 2024, Barcelona, Spain
the learned embeddings without using interventional labels, our
method can still achieve 0.9coverage and small intervals. Therefore,
our method has the potential to completely replace randomized
controlled trail with randomized assignation of users when the
dimension of the covariate 𝑋is higher than the dimension of target
𝑦, saving huge amounts of resources in practice. In contrast, even
with interventional data, WCP-NB fails to maintain 0.9coverage
on the Yahoo dataset because does not take hidden confounding
into consideratin. As expected, Naive has the widest intervals on
both datasets while maintaining 0.9 coverage most of the time.
6 RELATED WORK
Estimation of individual treatment effect has been the key for in-
dividual decision making in economics [ 49], healthcare [ 3] and
education [ 2]. Construcing confidence intervals for ITE provides
additional information for decision making process to improve
its reliability in high-stake situations [ 50,51]. Previous methods
that aim at constructing confidence intervals for the estimation of
counterfactual outcomes and individual treatment effects include
Bayesian inference [ 6], bootstrapping [ 52], kernel smoothing [ 53],
etc. These methods are known to have aymptotic coverage guaran-
tees (i.e. they require infinite number of samples) and depend on
the specific choice of regression models.
Recently, conformal prediction [ 33,35] becomes increasingly
popular because it has marginal coverage guarantee with finite
number of samples and it is also agnostic to the regression model
used. [ 1] has proposed to use weighted conformal prediction to con-
struct intervals for counterfactuals and ITE, and [ 25] also proposes
to use conformal prediction along with meta-learners to construct
intervals for ITE. However, both [ 1,54] require strong ignorability
assumption and completely ignores the existence of confounding
variables, which is unverifiable and unrealistic in practice. Recently,
[28] conducts sensitivity analysis of conformal prediction for ITE
under hidden confounding, but their method assumes marginal
selection condition, another unverifiable assumption in practice.
7 CONCLUSION
In this paper, we propose a novel algorithm WTCP-DR that provides
confidence intervals for predicting counterfactual outcomes and in-
dividual treatment effects with guaranteed marginal coverage, even
under hidden confounding. Our theory explicitly demonstrates the
conditions under which wTCP-DR is strictly advantageous to the
naive method that only uses interventional data. We also propose
a two stage variant called wSCP-DR with the same guarantee at
a lower computational cost than wTCP-DR. We demonstrate that
wTCP-DR and wSCP-DR achieve superior performances against
state-of-the-art baselines in terms of both coverage and efficiency
across synthetic and real-world datasets.
REFERENCES
[1]Lihua Lei and Emmanuel J Candès. Conformal inference of counterfactuals
and individual treatment effects. Journal of the Royal Statistical Society Series B:
Statistical Methodology, 83(5):911–938, 2021.
[2]Xiang Zhou. Attendance, completion, and heterogeneous returns to col-
lege: A causal mediation approach. Sociological Methods & Research, page
00491241221113876, 2022.
[3]Thierry Wendling, Kenneth Jung, Alison Callahan, Alejandro Schuler, Nigam H
Shah, and Blanca Gallego. Comparing methods for estimation of heterogeneous
treatment effects using observational data from health care databases. Statistics
in medicine, 37(23):3309–3324, 2018.[4]Richard Breen, Seongsoo Choi, and Anders Holm. Heterogeneous causal effects
and sample selection bias. Sociological Science, 2:351–369, 2015.
[5]Hugh A Chipman, Edward I George, and Robert E McCulloch. Bart: Bayesian
additive regression trees. 2010.
[6] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal
of Computational and Graphical Statistics, 20(1):217–240, 2011.
[7]Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treat-
ment effects using random forests. Journal of the American Statistical Association,
113(523):1228–1242, 2018.
[8]Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for
counterfactual inference. In International conference on machine learning, pages
3020–3029, 2016.
[9]Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treat-
ment effect: generalization bounds and algorithms. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 3076–3085. JMLR.
org, 2017.
[10] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and
Max Welling. Causal effect inference with deep latent-variable models. Advances
in neural information processing systems, 30, 2017.
[11] Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for
estimating heterogeneous treatment effects using machine learning. Proceedings
of the national academy of sciences, 116(10):4156–4165, 2019.
[12] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the
estimation of treatment effects. Advances in neural information processing systems,
32, 2019.
[13] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang.
Representation learning for treatment effect estimation from observational data.
Advances in neural information processing systems, 31, 2018.
[14] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of hetero-
geneous treatment effects: From theory to learning algorithms. In International
Conference on Artificial Intelligence and Statistics, pages 1810–1818. PMLR, 2021.
[15] Ahmed M Alaa and Mihaela Van Der Schaar. Bayesian inference of individual-
ized treatment effects using multi-task gaussian processes. Advances in neural
information processing systems, 30, 2017.
[16] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for
machine learning, volume 2. MIT press Cambridge, MA, 2006.
[17] Victor Veitch, Dhanya Sridhar, and David Blei. Adapting text embeddings for
causal inference. In Conference on Uncertainty in Artificial Intelligence, pages
919–928. PMLR, 2020.
[18] Amir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar,
Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E
Roberts, et al. Causal inference in natural language processing: Estimation, predic-
tion, interpretation and beyond. Transactions of the Association for Computational
Linguistics, 10:1138–1158, 2022.
[19] Ruocheng Guo, Jundong Li, and Huan Liu. Learning individual causal effects from
networked observational data. In Proceedings of the 13th international conference
on web search and data mining, pages 232–240, 2020.
[20] Jing Ma, Mengting Wan, Longqi Yang, Jundong Li, Brent Hecht, and Jaime Teevan.
Learning causal effects on hypergraphs. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pages 1202–1212, 2022.
[21] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning
in a random world, volume 29. Springer, 2005.
[22] Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. On-line predictive linear
regression. The Annals of Statistics, pages 1566–1590, 2009.
[23] Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao. An
introduction to proximal causal learning. arXiv preprint arXiv:2009.10982, 2020.
[24] Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and
effect. Basic books, 2018.
[25] Ahmed Alaa, Zaid Ahmad, and Mark van der Laan. Conformal meta-
learners for predictive inference of individual treatment effects. arXiv preprint
arXiv:2308.14895, 2023.
[26] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling
without replacement from a finite universe. Journal of the American statistical
Association, 47(260):663–685, 1952.
[27] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous
causal effects. arXiv preprint arXiv:2004.14497, 2020.
[28] Ying Jin, Zhimei Ren, and Emmanuel J Candès. Sensitivity analysis of individual
treatment effects: A robust conformal inference approach. Proceedings of the
National Academy of Sciences, 120(6):e2214889120, 2023.
[29] Ye Li, Hong Xie, Yishi Lin, and John CS Lui. Unifying offline causal inference
and online bandit learning for data driven decision. In Proceedings of the Web
Conference 2021, pages 2291–2303, 2021.
[30] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli
Lin, and Keping Yang. Autodebias: Learning to debias for recommendation.
InProceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 21–30, 2021.
[31] Jerzy S Neyman. On the application of probability theory to agricultural experi-
ments. essay on principles. section 9.(tlanslated and edited by dm dabrowska and
 
405KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
tp speed, statistical science (1990), 5, 465-480). Annals of Agricultural Sciences,
10:1–51, 1923.
[32] Donald B Rubin. Causal inference using potential outcomes: Design, modeling,
decisions. Journal of the American Statistical Association, 100(469):322–331, 2005.
[33] Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.
Distribution-free predictive inference for regression. Journal of the American
Statistical Association, 113(523):1094–1111, 2018.
[34] Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial
Intelligence, 74:9–28, 2015.
[35] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas.
Conformal prediction under covariate shift. Advances in neural information
processing systems, 32, 2019.
[36] Muhammad Faaiz Taufiq, Jean-Francois Ton, Rob Cornish, Yee Whye Teh, and
Arnaud Doucet. Conformal off-policy prediction in contextual bandits. Advances
in Neural Information Processing Systems, 35:31512–31524, 2022.
[37] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation
in machine learning. Cambridge University Press, 2012.
[38] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borg-
wardt, Bernhard Schölkopf, et al. Covariate shift by kernel mean matching.
Dataset shift in machine learning, 3(4):5, 2009.
[39] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and
Masashi Sugiyama. Relative density-ratio estimation for robust distribution
comparison. Neural computation, 25(5):1324–1370, 2013.
[40] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification,
and risk bounds. Journal of the American Statistical Association, 101(473):138–156,
2006.
[41] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard
Schölkopf. Nonlinear causal discovery with additive noise models. Advances in
neural information processing systems, 21, 2008.
[42] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pen-
nington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes.
arXiv preprint arXiv:1711.00165, 2017.
[43] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions
with formulas, graphs, and mathematical tables, volume 55. US Government
printing office, 1948.[44] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. The deconfounded
recommender: A causal inference approach to recommendation. arXiv preprint
arXiv:1808.06581, 2018.
[45] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. Causal infer-
ence for recommender systems. In Proceedings of the 14th ACM Conference on
Recommender Systems, pages 426–431, 2020.
[46] Qing Zhang, Xiaoying Zhang, Yang Liu, Hongning Wang, Min Gao, Jiheng Zhang,
and Ruocheng Guo. Debiasing recommendation by learning identifiable latent
confounders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, 2023.
[47] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques
for recommender systems. Computer, 42(8):30–37, 2009.
[48] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and
Thorsten Joachims. Recommendations as treatments: Debiasing learning and
evaluation. In international conference on machine learning , pages 1670–1679.
PMLR, 2016.
[49] Colin Camerer. Individual decision making. The handbook of experimental
economics, 1:587–704, 1995.
[50] Brent R Logan, Rodney Sparapani, Robert E McCulloch, and Purushottam W Laud.
Decision making and uncertainty quantification for individualized treatments
using bayesian additive regression trees. Statistical methods in medical research,
28(4):1079–1093, 2019.
[51] Andrew Jesson, Sören Mindermann, Uri Shalit, and Yarin Gal. Identifying causal-
effect inference failure with uncertainty-aware models. Advances in Neural
Information Processing Systems, 33:11637–11649, 2020.
[52] Wanzhu Tu and Xiao-Hua Zhou. A bootstrap confidence interval procedure for
the treatment effect using propensity score subclassification. Health Services and
Outcomes Research Methodology, 3:135–147, 2002.
[53] Nathan Kallus, Xiaojie Mao, and Angela Zhou. Interval estimation of individual-
level causal effects under unobserved confounding. In The 22nd international
conference on artificial intelligence and statistics, pages 2281–2290. PMLR, 2019.
[54] Mingzhang Yin, Claudia Shi, Yixin Wang, and David M Blei. Conformal sensitivity
analysis for individual treatment effects. Journal of the American Statistical
Association, pages 1–14, 2022.
 
406Conformal Counterfactual Inference under Hidden Confounding KDD ’24, August 25–29, 2024, Barcelona, Spain
Appendix A EXPERIMENTS
A.1 Experiments on Synthetic Data
Implementation Details. For WCP, the propensity model is implemented as a logistic regression model, which is widely adopted in the
causal inference literature. For density ratio estimation, we use the MLP model from scikit-learn3to classify whether a given data point
(𝑥,𝑦)is from observational or interventional distribution.
Results of Nested Methods for ITE. We skipped the experiment for wTCP-DR as the nested methods from [ 1] for ITE requires inferring
confidence intervals of potential outcomes on the massive D𝑂
𝑐𝑎𝑙, leading to extremely heavy computational cost. Table 4 shows results on
ITE with nested inexact and exact methods which can construct ITE intervals from intervals of counterfactual outcomes. As we can see,
under the nested inexact method, none of the methods achieve 0.9 coverage, as this method does not guarantee coverage. While the nested
exact method can significantly expand the confidence interval, leading to low efficiency.
Ablation Study on Density Estimation Method: MLP vs Density Estimator (DR). We compare two different density estimators, i.e.,
MLP from scikit-learn and density estimator densratio4(DR) on the synthetic dataset, where we adopt the same setting as the results shown
in Table 2. Intuitively, directly modeling the density of the joint distribution (DR) is more challenging than classifying whether a data point
is from the observational or the interventional distribution (MLP). We can observe that the coverage of wTCP-DR drops significantly when
DR is used, because an inaccurate estimate of density ratio would result in worse coverage of wTCP-DR. wSCP-DR (Exact and Inexact) are
more robust against inaccurate density ratios due to the correction taken from the second-stage inference.
Results with Different Settings. Here, we illustrate the results for different dimensionalities of the observed features ( 𝑑𝑖𝑚(𝑋)) in Fig. 5 and
results for different sample size of interventional data ( 𝑚) in Fig. 6. In Fig. 5, we can observe that the coverage rates of all methoeds increase
as𝑑𝑖𝑚(𝑋)grows, which corresponds to less hidden confounding. At the same time, the interval widths of most of the methods become
narrower when 𝑑𝑖𝑚(𝑋)increases due to the decrease of calibration error of the underlying regression models given more informative
observed features 𝑋. For WCP, it only provides expected coverage guarantees when 𝑑𝑖𝑚(𝑋)is large, which leads to weak hidden confounding
and accurate estimates of propensity scores. Its interval widths increase with 𝑑𝑖𝑚(𝑋)such that the coverage can be guaranteed. In Fig. 6, we
show the coverage and interval width with 𝑚ranging within{10,20,50,100,250,500,750,1,000}. For all methods, the coverage is increasing
with𝑚and the interval width is decreasing with 𝑚, as expected. This is because, for small 𝑚,𝑚<50, wTCP-DR cannot achieve the specified
level of coverage (0.9) because the density ratio estimator has high variance. As 𝑚increases, wTCP-DR reaches the coverage of 0.9 and the
smallest interval width.
Table 4: Results of ITE on synthetic data under the nested inexact and exact methods [1].
Method Coverage ITE (Nested Inexact) Interval Width ITE (Nested Inexact) Coverage ITE (Nested Exact) Interval Width ITE (Nested Exact)
wSCP-DR(Inexact) 0.749±0.055 0 .422±0.011 0 .938±0.012 0 .767±0.011
wSCP-DR(Exact) 0.819±0.033 0 .504±0.009 0 .948±0.016 0 .847±0.008
WCP 0.458±0.062 0 .224±0.007 0 .865±0.027 0 .602±0.006
Naive 0.850±0.060 0 .558±0.095 0 .945±0.019 0 .943±0.104
Table 5: Comparison of MLP and DR as density estimators with wTCP-DR and wSCP-DR (Inexact and Exact). The setting is the same as Table 2.
Method Coverage 𝑌(0)↑ Interval Width 𝑌(0)↓ Coverage𝑌(1)↑ Interval Width 𝑌(1)↓ Coverage ITE ↑ Interval Width ITE ↓
MLP wSCP-DR(Inexact) 0.891 ±0.026 0.414 ±0.008 0.889 ±0.019 0.421 ±0.013 0.942 ±0.017 0.835 ±0.016
MLP wSCP-DR(Exact) 0.934 ±0.026 0.496 ±0.010 0.935 ±0.023 0.503 ±0.010 0.957 ±0.018 0.998 ±0.015
MLP wTCP-DR 0.899 ±0.028 0.386 ±0.013 0.923 ±0.015 0.576 ±0.066 0.953 ±0.015 0.962 ±0.074
DR wSCP-DR(Inexact) 0.899 ±0.024 0.423 ±0.013 0.874 ±0.014 0.411 ±0.011 0.946 ±0.020 0.834 ±0.015
DR wSCP-DR(Exact) 0.936 ±0.014 0.503 ±0.009 0.934 ±0.004 0.493 ±0.017 0.966 ±0.014 0.996 ±0.009
DR wTCP-DR 0.847 ±0.022 0.363 ±0.011 0.853 ±0.031 0.372 ±0.013 0.910 ±0.020 0.735 ±0.016
A.2 Experiments on Recommendation System Data
Implementation Details. We use MSE loss to train matrix factorization (MF) models [ 47] with 64 dimensional embeddings as the base
model for rating prediction, which is one of the most popular approaches in recommendation systems [ 44,48]. In this setting, the features
(user/item embeddings) are learned from the factual outcomes 𝑌, leading to their capability to capture part of hidden confounding. We
use the Python version of the package densratio for density ratio estimation of our method to handle the high dimensional. For WCP-NB,
following [ 30,48], we fit a Naive Bayes classifier to model the propensity 𝑃(𝑇=1|𝑋,𝑍,𝑌). It is simplified as 𝑃(𝑇=1|𝑌)=𝑃(𝑌|𝑇=1)𝑃(𝑇=1)
𝑃(𝑌).
As𝑃(𝑌|𝑇=0)is not available in the observational data, 𝑃(𝑌)can only be estimated from the interventional data where treatment is
randomized ( 𝑃(𝑌)=𝑃𝐼(𝑌)=𝑃𝐼(𝑌|𝑇)). So, WCP-NB needs to use interventional data with outcomes. In this case, WCP-NB can be seens as a
variant of our method using a different density ratio estimator based on propensity scores.
Impact of𝑚𝑐𝑎𝑙.We maintain 𝑚𝑡𝑟=0.2𝑚,𝑚𝑡𝑠=0.6𝑚and modify 𝑚𝑐𝑎𝑙∈{0.05𝑚,0.1𝑚,0.15𝑚,0.2𝑚}. Results are shown in Fig. 7. All the
methods maintain coverage close or above 0.9 for all cases. In terms of efficiency, we can observe that the efficiency of Naive gets slightly
improved with increasing 𝑚𝑐𝑎𝑙.
Impact of𝑚𝑡𝑟.We maintain 𝑚𝑐𝑎𝑙=0.2𝑚,𝑚𝑡𝑠=0.6𝑚and modify𝑚𝑡𝑟∈{0.05𝑚,0.1𝑚,0.15𝑚,0.2𝑚}. Fig. 7 shows results on Coat where 𝑚is
small. We make the following observations. First, the efficiency of Naive is improved because its base model has lower MSE with more
3https://scikit-learn .org/stable/
4https://github .com/hoxo-m/densratio_py
 
407KDD ’24, August 25–29, 2024, Barcelona, Spain Zonghao Chen, Ruocheng Guo, Jean-François Ton, & Yang Liu
(a) Coverage of 𝑌(0)
 (b) Coverage of 𝑌(1)
 (c) Coverage of ITE
(d) Interval width of 𝑌(0)
 (e) Interval width of 𝑌(1)
 (f) Interval width of ITE
Figure 5: Coverage and interval width results of counterfactual outcomes and ITE with varying hidden confounding strength. Higher
dimensional 𝑋carries more information of the hidden confounders, leading to weaker hidden confounding.
(a) Coverage of 𝑌(0)with different 𝑚
 (b) Coverage of 𝑌(1)with different 𝑚
(c)𝑌(0)interval width with different 𝑚
(d)𝑌(1)interval width with different 𝑚
Figure 6: Impact of interventional data size 𝑚on coverage and efficiency of conformal inference methods.
(a) Test empirical coverage with differ-
ent𝑚𝑐𝑎𝑙on Coat
(b) Test interval width with different
𝑚𝑐𝑎𝑙on Coat
(c) Test empirical coverage with differ-
ent𝑚𝑡𝑟on Coat
(d) Test interval width with different
𝑚𝑡𝑟on Coat
Figure 7: Results on Coat with different 𝑚𝑡𝑟and different 𝑚𝑐𝑎𝑙.
training data, leading to smaller confidence intervals. Second, the coverage of all methods are improved, as more trainig samples from the
interventional distribution can improve the base model for the Naive method, density ratio estimators for our methods and the propensity
model for WCP-NB.
 
408