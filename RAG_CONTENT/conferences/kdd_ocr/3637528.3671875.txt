Improving Robustness of Hyperbolic Neural Networks by
Lipschitz Analysis
Yuekang Li‚àó
Applied Mathematics and Computational Sciences,
DNAS, Duke Kunshan Univerity
Kunshan, China
yuekang.li@duke.eduYidan Mao‚àó
Applied Mathematics and Computational Sciences,
DNAS, Duke Kunshan Univerity
Kunshan, China
ym188@duke.edu
Yifei Yang
Electronic Information School,
Wuhan Univerity
Wuhan, China
yfyang@whu.edu.cnDongmian Zou‚Ä†
Zu Chongzhi Center and Data Science Research Center,
DNAS, Duke Kunshan Univerity
Kunshan, China
dongmian.zou@duke.edu
Abstract
Hyperbolic neural networks (HNNs) are emerging as a promising
tool for representing data embedded in non-Euclidean geometries,
yet their adoption has been hindered by challenges related to sta-
bility and robustness. In this work, we conduct a rigorous Lipschitz
analysis for HNNs and propose using Lipschitz regularization as a
novel strategy to enhance their robustness. Our comprehensive in-
vestigation spans both the Poincar√© ball model and the hyperboloid
model, establishing Lipschitz bounds for HNN layers. Importantly,
our analysis provides detailed insights into the behavior of the
Lipschitz bounds as they relate to feature norms, particularly dis-
tinguishing between scenarios where features have unit norms
and those with large norms. Further, we study regularization using
the derived Lipschitz bounds. Our empirical validations demon-
strate consistent improvements in HNN robustness against noisy
perturbations.
CCS Concepts
‚Ä¢Computing methodologies ‚ÜíNeural networks; Regulariza-
tion; ‚Ä¢Mathematics of computing ‚ÜíGraphs and surfaces.
Keywords
Hyperbolic neural networks, robustness, Lipschitz bounds, noisy
data
ACM Reference Format:
Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou. 2024. Improving
Robustness of Hyperbolic Neural Networks by Lipschitz Analysis. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671875
‚àóBoth authors contributed equally to this research.
‚Ä†Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718751 Introduction
In recent years, the emergence of datasets with complex geometric
structures has underscored the limitations of traditional Euclidean
spaces for accurate data representation. To capture the essence
of numerous tree-like and hierarchical structures encountered in
various applications such as product recommendation [ 53], drug dis-
covery [ 54] and image recognition [ 1], a growing body of research
has embraced hyperbolic spaces as a natural way of embedding
such data with low distortion [ 4,7,43]. Among other successes of
deep learning methods, hyperbolic neural networks (HNNs) offer
a geometric and expressive data-driven approach to representing
such data, outperforming Euclidean or graph neural networks in
numerous applications [40].
Although HNNs have demonstrated significant potential, their
practical implementation is challenged by the complex geometry of
the hyperbolic space in which they operate. Firstly, as features move
away from the origin, gradients can grow exponentially, leading to
the ‚Äúgradient explosion problem‚Äù in training [36, 41, 43, 55]. Addi-
tionally, perturbing a feature in the hyperbolic space may alter its
local properties, thereby compromising network robustness. This
can lead to more severe vulnerabilities compared to the Euclidean
case that has been studied in [ 13,22,31,45,59,60]. To enhance
the robustness of HNNs, we leverage the concept of Lipschitz con-
tinuity, a fundamental mathematical property that characterizes
the smoothness of functions. In the context of neural networks,
Lipschitz constants provide an upper bound on how significant
the output of a network can change in response to changes in its
input. Moreover, by maintaining lower Lipschitz constants, neu-
ral networks can achieve greater robustness, rendering them less
vulnerable to the effects of adversarial perturbations [48].
Research on Lipschitz analysis of neural networks has primarily
centered on linear layers [ 16,26,28,32,52] or convolutional layers
[48, 50, 57], both of which exhibit linear structures. In these cases,
a neural network layer has a uniform Lipschitz constant across
input locations and variations in Lipschitz constants arise only
when nonlinear activation functions are introduced. In contrast,
for HNNs, linearity typically holds only in tangent spaces and the
‚Äúlinear‚Äù layers involve nonlinear operations [ 8,19,23,46]. There-
fore, each HNN layer has different Lipschitz constants for different
 
1713
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
input features. This fact poses unique analytical challenges and
necessitates discussion considering the diversity of input features.
In this study, we present an extensive Lipschitz analysis for
HNNs. Our contribution lies in the development of Lipschitz bounds
and the application of regularization techniques tailored specifically
for HNNs. By rigorously analyzing and deriving Lipschitz bounds
for HNN layers, we gain valuable insights into their behavior corre-
sponding to different input features. We then leverage these bounds
to regularize the network during training, effectively constraining
its output fluctuations. We summarize our contribution as follows:
‚Ä¢We derive Lipschitz bounds for HNN layers, including weight
matrix transformation and bias translation, using both the
Poincar√© ball model and the hyperboloid model.
‚Ä¢We extend our analysis to examine the effects of the derived
Lipschitz bounds concerning input features under various
conditions. This investigation leads to simplification of the
expressions of the Lipschitz bounds, making them more prac-
tical for regularizing HNNs with normalized input features.
‚Ä¢We empirically validate the effectiveness of Lipschitz reg-
ularization in enhancing the robustness of HNNs against
noise. We establish that a small Lipschitz bound contributes
to superior performance for classification tasks.
2 Related Works
2.1 Hyperbolic Neural Networks
HNNs were first introduced in [ 19], where fundamental neural oper-
ations, including linear and recurrent layers, were executed within
the tangent spaces of hyperbolic features. Later, it was generalized
to HNN++ [ 46], which incorporated concatenation, convolution
and attention mechanisms. Both works used the Poincar√© ball model
to represent the hyperbolic space, while other works also used the
hyperboloid model and the Klein model [23, 38].
Using tangent spaces to construct operations in the hyperbolic
space could be prone to gradient explosion and numerical issues
[38]. Consequently, recent works started to consider operations
fully performed within the hyperbolic space itself [ 9,14]. Other
works concentrated on hyperbolic embedding techniques and sub-
sequently devised neural network models according to such em-
beddings [ 24,39]. Due to their proficiency in capturing the un-
derlying geometry of diverse data types, HNNs have also been
applied to graph models [ 5,8,11,33,34,42,56] and generative
models [ 11,34,35,41]. However, all the aforementioned works
have primarily concentrated on effective feature extraction and
numerical considerations, while largely overlooking the critical
aspect of robustness in the presence of noise and perturbations.
2.2 Robustness of Neural Networks
The issue of vulnerability of neural networks has been a significant
concern within deep learning, as originally noted in [ 48]. Prior
research efforts also addressed vulnerability challenges in neural
networks from the views of perturbation [ 15], noisy labels [ 21] and
response to noise [ 13]. Moreover, neural networks are also subject
to designed attacks [ 22,31,45,59,60]. Regarding HNNs, we remark
that previous works on ‚Äústability‚Äù of HNNs mainly concerned with
numerical instability and gradient explosion issues [ 36,41,43], butnot vulnerability to noise perturbation and our work represents the
pioneering effort to address this specific issue for HNN.
The vulnerability of neural networks can be attributed to their
large Lipschitz constants [ 48]. Recent works [ 16,26,28,32,52] de-
veloped diverse optimization techniques aimed at tighter Lipschitz
bounds. These investigations have extended to specific neural net-
work architectures, including those with convolutional or attention
layers [ 3,30,50,57] and graph neural networks [ 12,17,18,20,27,
29,58]. To our best knowledge, there is no prior investigation into
Lipschitz analysis specifically for HNNs.
3 Lipschitz Bounds of HNN
3.1 Preliminaries
Hyperbolic geometry is a non-Euclidean geometry with a constant
negative curvature [ 2]. It has several coordinate systems, or ‚Äúmod-
els‚Äù, that provide concrete elementwise representation of features.
The most frequently used models in HNNs are the Poincar√© ball
model and the hyperboloid (Lorentz) model. While these models
are isometric to each other, they have distinct numerical properties
and work differently in neural networks. We review the operations
in both models as follows.
3.1.1 The Poincar√© ball model. Theùëõ-dimensional Poincar√© ball
model with a constant negative curvature of ‚àíùëêis formally defined
as
Dùëõùëê,ùëîD
, where Dùëõùëê=
x‚ààRùëõ|ùëê‚à•x‚à•2<1	is an open ball in
Rùëõ, andùëîDis the metric tensor defined as ùëîD= ùúåùëêx2Iùëõ. It is worth
noting that Iùëõis a Euclidean metric tensor, and the hyperbolic
metric tensor is conformal with it. In this context, Dùëõùëêrepresents an
open ball with a radius of ùëê‚àí1
2, andùúåùëêxis a conformal factor given
byùúåùëêx=2 1‚àíùëê‚à•x‚à•2‚àí1. Then the induced distance function on Dùëõùëê,ùëîùëêis given by:
ùëëùëê(x,y)=(2/‚àöùëê)tanh‚àí1‚àöùëê‚à•‚àíx‚äïùëêy‚à•
,
where‚äïùëêis the M√∂bius addition, defined by
x‚äïùëêy= 1+2ùëê‚ü®x,y‚ü©+ùëê‚à•y‚à•2x+ 1‚àíùëê‚à•x‚à•2y
1+2ùëê‚ü®x,y‚ü©+ùëê2‚à•x‚à•2‚à•y‚à•2. (1)
LetTxDùëõùëêdenote the tangent space of xinDùëõùëê. For x,y‚ààDùëõùëê,v‚àà
TxDùëõùëêsuch that y‚â†xandv‚â†0, the exponential map expùëêx(v)
transforms vintoDùëõùëêand the logarithmic map logùëê
x(y)transforms
yintoTxDùëõùëê. Their expressions are given by:
expùëê
x(v)=x‚äïùëê
tanh‚àöùëêùúåùëêx‚à•v‚à•
2v‚àöùëê‚à•v‚à•
;
logùëê
x(y)=2‚àöùëêùúåùëêxtanh‚àí1‚àöùëê‚à•‚àíx‚äïùëêy‚à•‚àíx‚äïùëêy
‚à•‚àíx‚äïùëêy‚à•.
3.1.2 The hyperboloid model. Write each point in Rùëë+1asx=
[ùë•ùë°,x‚ä§ùë†]‚ä§whereùë•ùë°‚ààRandxùë†‚ààRùëõ. The Minkowski inner product
‚ü®¬∑,¬∑‚ü©Lis defined by‚ü®x,y‚ü©L:=‚àíùë•ùë°ùë¶ùë°+x‚ä§ùë†yùë†. Theùëõ-dimensional
hyperboloid manifold with constant negative curvature ‚àí1/ùêæis
formally defined as Hùëõ,ùêæ={x‚ààRùëõ+1|‚ü®x,x‚ü©L=‚àíùêæ,ùë•ùë°>0}. By
the convention of special relativity, we call ùë•ùë°the time axis and
xùë†the spatial axes. In this context, the tangent space at xis the
orthogonal space of Hùëõ,ùêæwith respect to the Minkowski inner
product, and is represented as TxHùëõ,ùêæ:={v‚ààRùëõ+1:‚ü®v,x‚ü©L=0}.
 
1714Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
The geodesic distance between two points x,yinHùëõ,ùêæis given by
ùëëùêæ
L(x,y)=‚àö
ùêæcosh‚àí1(‚àí‚ü®x,y‚ü©L/ùêæ).
Forx,y‚ààHùëõ,ùêæandv‚ààTxHùëõ,ùêæsuch that x‚â†yandv‚â†0, the
exponential and the logarithmic maps of the hyperboloid model
are given by
expùêæ
x(v)=cosh‚à•v‚à•L‚àö
ùêæ
x+‚àö
ùêæsinh‚à•v‚à•L‚àö
ùêæv
‚à•v‚à•L;
logùêæ
x(y)=ùëëùêæ
L(x,y)y+1
ùêæ‚ü®x,y‚ü©Lx
‚à•y+1
ùêæ‚ü®x,y‚ü©Lx‚à•L.
3.2 Derivation of Lipschitz Bounds
In this work, we focus on the linear layers of HNN in both the
Poincar√© ball model [ 19] and the hyperboloid model [ 8]. Hyper-
bolic linear transformations involve a weight matrix multiplication
followed by a bias translation, which are defined by leveraging the
exponential and logarithmic maps. The product of their Lipschitz
bounds can be used to estimate the Lipchitz bound for each HNN
layer. In the following analysis, we consider ùëê=1for the Poincar√©
ball model and ùêæ=1for the hyperboloid model for simplicity,
which are in line with the common convention of HNN literature.
While hyperbolic linear transformations are called ‚Äúlinear‚Äù, they
exhibit linearity solely within the tangent spaces and show non-
linear structures once the exponential maps are applied. Since the
behavior of exponential maps varies with the position of the hy-
perbolic feature, we estimate Lipschitz constants locally.
Before analyzing Lipschitz constants of HNNs, we revisit the
definition of the Lipschitz constant. A function ùëì:Rùëõ‚ÜíRùëöis
said to be Lipschitz continuous on an input set X‚äÜRùëõif there
exists a constant ùêø‚â•0such that for all x,y‚ààX,ùëìsatisfies the
following inequality:
‚à•ùëì(x)‚àíùëì(y)‚à•‚â§ùêø‚à•x‚àíy‚à•,‚àÄx,y‚ààX. (2)
The smallest possible ùêøfor which (2) holds is called the Lipschitz
constant ofùëì. For neural networks, it is in general difficult to derive
the smallest ùêøand a Lipschitz bound is widely adopted. Let Jbe the
Jabobian of ùëìatx. Since
ùëì(x)‚àíùëì(y)=J(x‚àíy)+ùëú(‚à•x‚àíy‚à•),
given anùúñ>0, there exists a neighborhood Uofxfor which
‚à•ùëì(x)‚àíùëì(y)‚à•‚â§‚à•J‚à•‚à•x‚àíy‚à•+ùúñ (3)
as long as y‚ààU. Here,‚à•J‚à•is the spectral norm of J. In view
of (3), we can use ‚à•J‚à•as an upper bound for the local Lipschitz
constant of ùëìatx. Since obtaining the precise constant for neural
networks is in general difficult, instead of seeking for an exactly
tight bound, we focus on a bound that can be more easily expressed
and implemented.
3.2.1 Lipschitz bounds for the Poincar√© ball model.
Weight matrix transformation. LetMbe aùëö√óùëõweight matrix.
For all x‚ààDùëõandMx‚â†0, the weight matrix transformation can
be performed using the M√∂bius matrix-vector multiplication [ 51],
which is equivalent to matrix-vector multiplication in the tangent
space [19]. Specifically,
M‚äóx:=tanh‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)Mx
‚à•Mx‚à•.To derive the Lipschitz bound for y=M‚äóx, denote its Jacobian
matrix of this operation by J1=ùúïy/ùúïx. We have the following
result, with the proof provided in Appendix A.1.
Theorem 3.1. The spectral norm of J1satisfies
‚à•J1‚à•‚â§tanh‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
¬∑M
‚à•Mx‚à•‚àíMxx‚ä§M‚ä§M
‚à•Mx‚à•3+
‚à•Mx‚à•sech2 
‚à•Mx‚à•
‚à•x‚à•¬∑tanh‚àí1(‚à•x‚à•)!
¬∑
tanh‚àí1(‚à•x‚à•)M‚ä§M
‚à•Mx‚à•2+tanh‚àí1(‚à•x‚à•)
‚à•x‚à•2+1
‚à•x‚à•(1‚àí‚à•x‚à•2)
.
(4)
Analysis of‚à•J1‚à•.The upper bound of ‚à•J1‚à•in (4) is complicated
and depends on x. To gain insights of the bound, we consider the
following two cases based on the magnitude of ‚à•J1‚à•. The first case
is‚à•x‚à•‚Üí 1, where the input feature lies close to the boundary of
the Poincar√© ball. This corresponds to points that are infinitely far
away from the origin in hyperbolic space and thus large norms of
the input. The second case is ‚à•x‚à•=tanh(1), it represents a position
at a unit hyperbolic distance from the input feature to the origin in
hyperbolic space. We simplify the upper bound in (4) in these cases
and derive the following results.
Theorem 3.2. (1)As‚à•x‚à•‚Üí 1, the upper bound of ‚à•J1‚à•can be
simplified depending on the value of ùúö:=‚à•Mx‚à•/‚à•x‚à•. There
are three cases:
(a)ifùúö>1,
‚à•J1‚à•‚â§2‚à•M‚à•
‚à•Mx‚à•. (5)
(b)ifùúö=1,
‚à•J1‚à•‚â§2‚à•M‚à•
‚à•Mx‚à•+1. (6)
(c)ifùúö<1,‚à•J1‚à•diverges.
(2)When‚à•x‚à•=tanh(1), the upper bound of ‚à•J1‚à•can be simpli-
fied as
‚à•J1‚à•‚â§2‚à•M‚à•
‚à•Mx‚à•+‚à•M‚ä§M‚à•
‚à•Mx‚à•+4.86‚à•Mx‚à•. (7)
Proof. (1) As‚à•x‚à•‚Üí 1,tanh‚àí1(‚à•x‚à•)‚Üí‚àû. Thus we have
tanh‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
‚Üí1.
Consider
sech2‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
.
Denoteùëü=‚à•x‚à•. Since
sech2(ùúötanh‚àí1(ùëü))=4
1+ùëü
1‚àíùëüùúö
+
1‚àíùëü
1+ùëüùúö
+2,
we have
sech2(ùúötanh‚àí1(ùëü))
1‚àíùëü2=4
(1+ùëü)ùúö+1
(1‚àíùëü)ùúö‚àí1+(1‚àíùëü)ùúö+1
(1+ùëü)ùúö‚àí1+2(1‚àíùëü)(1+ùëü)
ùëü‚Üí1‚àí
‚Üí4(1‚àíùëü)ùúö‚àí1
(1+ùëü)ùúö+1=:ùëÑ.
 
1715KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
Therefore, as ùëü‚Üí1‚àí, there are three cases: (a) ùúö>1: thenùëÑ‚Üí0,
the same as the conclusion in the paper; (b) ùúö=1: thenùëÑ‚Üí1, we
have‚à•ùêΩ1‚à•‚â§2‚à•ùëÄ‚à•
‚à•ùëÄùë•‚à•+‚à•ùëÄùë•‚à•; (c)ùúö<1:ùëÑdiverges. Therefore, a
necessary condition for a bounded ‚à•ùêΩ1‚à•as‚à•ùë•‚à•‚Üí 1is that‚à•ùëÄùë•‚à•‚â•
‚à•ùë•‚à•. Consequently,
‚à•J1‚à•‚â§M
‚à•Mx‚à•‚àíMxx‚ä§M‚ä§M
‚à•Mx‚à•3=
ùë∞‚àíMxx‚ä§M‚ä§
‚à•Mx‚à•2
¬∑M
‚à•Mx‚à•
‚â§ùë∞‚àíMxx‚ä§M‚ä§
‚à•Mx‚à•2¬∑‚à•M‚à•
‚à•Mx‚à•‚â§
‚à•ùë∞‚à•+‚à•Mxx‚ä§M‚ä§‚à•
‚à•Mx‚à•2
¬∑‚à•M‚à•
‚à•Mx‚à•.
Since
‚à•Mxx‚ä§M‚ä§‚à•‚â§tr(Mxx‚ä§M‚ä§)=tr(x‚ä§M‚ä§Mx)=‚à•Mx‚à•2,
we have
‚à•J1‚à•‚â§
‚à•ùë∞‚à•+‚à•Mx‚à•2
‚à•Mx‚à•2
¬∑‚à•M‚à•
‚à•Mx‚à•=2‚à•M‚à•
‚à•Mx‚à•.
(2) When‚à•x‚à•=tanh(1),tanh‚àí1(‚à•x‚à•)=1. Thus (4) simplifies to
‚à•J1‚à•‚â§tanh‚à•Mx‚à•
‚à•x‚à•
¬∑M
‚à•Mx‚à•‚àíMxx‚ä§M‚ä§M
‚à•Mx‚à•3+
‚à•Mx‚à•sech2‚à•Mx‚à•
‚à•x‚à•
¬∑ M‚ä§M
‚à•Mx‚à•2+1
‚à•x‚à•2+1
‚à•x‚à•(1‚àí‚à•x‚à•2)!
‚â§M
‚à•Mx‚à•‚àíMxx‚ä§M‚ä§M
‚à•Mx‚à•3+‚à•Mx‚à•¬∑
 M‚ä§M
‚à•Mx‚à•2+1
‚à•x‚à•2+1
‚à•x‚à•(1‚àí‚à•x‚à•2)!
‚â§2‚à•M‚à•
‚à•Mx‚à•+‚à•Mx‚à•¬∑ M‚ä§M
‚à•Mx‚à•2+1
‚à•x‚à•2+1
‚à•x‚à•(1‚àí‚à•x‚à•2)!
.
We compute1
‚à•tanh(1)‚à•2+1
‚à•tanh(1)‚à•(1‚àí‚à•tanh(1)‚à•2)‚â§4.86. Hence,
‚à•J1‚à•‚â§2‚à•M‚à•
‚à•Mx‚à•+‚à•Mx‚à•¬∑‚à•M‚ä§M‚à•
‚à•Mx‚à•2+4.86
‚â§2‚à•M‚à•
‚à•Mx‚à•+‚à•M‚ä§M‚à•
‚à•Mx‚à•+4.86‚à•Mx‚à•.
‚ñ°
We conclude that to bound ‚à•J1‚à•in both cases, we can focus on
bounding‚à•M‚à•and we need to ensure the smallest singular value of
Mis sufficiently large as well. The upper bounds of (5)‚Äì(7) can be
used as subsequent guidelines for regularized training to improve
the robustness of the HNNs embedded in the Poincar√© ball model,
which are simpler than (4) and can be more efficiently implemented.
Bias translation. The bias translation in the Poincar√© ball model
is performed by moving along geodesics, which coincides with the
M√∂bius addition [ 51]. Translating y‚ààDùëöby a bias b‚ààDùëöis
defined according to (1) as
y‚äïb:= 1+2‚ü®y,b‚ü©+‚à• b‚à•2y+ 1‚àí‚à•y‚à•2b
1+2‚ü®y,b‚ü©+‚à• y‚à•2‚à•b‚à•2.
To derive the Lipschitz bound for z=y‚äïb, denote the Jacobian
matrix of this operation by J2=ùúïz/ùúïy. We have the following
result (due to page limit, we omit the proof which resembles that
of Theorem 3.1).Theorem 3.3. The spectral norm of J2satisfies
‚à•J2‚à•‚â§
1+4y‚ä§b+3‚à•b‚à•2+2‚à•y‚à•‚à•b‚à•+4y‚ä§b2+
2y‚ä§b‚à•b‚à•2+5‚à•y‚à•2‚à•b‚à•2+4y‚ä§b‚à•y‚à•‚à•b‚à•+
4‚à•y‚à•‚à•b‚à•3+6y‚ä§b‚à•y‚à•2‚à•b‚à•2+3‚à•y‚à•2‚à•b‚à•4+
2‚à•y‚à•3‚à•b‚à•3
¬∑
1+2y‚ä§b+‚à•y‚à•2‚à•b‚à•2‚àí2
.
3.2.2 Lipschitz bounds for the hyperboloid model.
Weight matrix transformation. LetWbe anùëö√ó(ùëõ+1)weight
matrix. In the hyperboloid model, the hyperbolic matrix multiplica-
tion is defined as y‚â°W‚äóùêæxùêª:=expùêæo(Wlogùêæ
o(xùêª))[8], where
odenotes the hyperbolic origin. In the hyperboloid model, the coor-
dinate of the time axis is completely determined by the coordinates
of the spatial axes. Therefore, when considering robustness it suf-
fices to look at the spatial axes. Denote the Jacobian matrix of this
operation by J1=ùúïyùë†/ùúïxùë†. We have the following result, with the
proof detailed in Appendix A.2.
Theorem 3.4. Letùëé=cosh‚àí1(ùë•ùë°)
‚à•xùë†‚à•=cosh‚àí1(ùë•ùë°)‚àöÔ∏É
ùë•2
ùë°‚àí1, and let W[2:]
denote the matrix comprising columns from the second column to the
last column of W. Then the spectral norm of J1satisfies
‚à•J1‚à•‚â§ 
sinh
ùëé‚à•W[2:]xùë†‚à•‚à•W[2:]xùë†‚à•‚àí1
‚à•W[2:]xùë†‚à•2+
cosh
ùëé‚à•W[2:]xùë†‚à•ùëé
‚à•W[2:]xùë†‚à•‚àí
ùëé
‚à•xùë†‚à•+1
ùë•ùë°‚à•xùë†‚à•!
W[2:].(8)
Analysis of‚à•J1‚à•.To gain insights of the bound (8), we consider
the following two cases. The first case is ‚à•xùë†‚à•=1, which corre-
sponds to performing normalization for the input features before
entering each hyperbolic linear layer. We simplify the expression
of the Lipschitz bound in the following theorem.
Theorem 3.5. When‚à•xùë†‚à•=1, the upper bound of ‚à•J1‚à•can be
simplified as
‚à•J1‚à•‚â§exp(cosh‚àí1(‚àö
2)‚à•W[2:]xùë†‚à•)
‚à•W[2:]xùë†‚à•‚à•W[2:]‚à•. (9)
Proof. When‚à•xùë†‚à•=1,ùëé=cosh‚àí1(‚àö
2) ‚âà 0.88, andùë•ùë°=‚àöÔ∏Å
‚à•xùë†‚à•2+1=‚àö
2. Thus (8) simplifies to
‚à•J1‚à•‚â§ 
sinh
ùëé‚à•W[2:]xùë†‚à• 
1
‚à•W[2:]xùë†‚à•‚àí1
‚à•W[2:]xùë†‚à•2!
+
cosh
ùëé‚à•W[2:]xùë†‚à• 
ùëé
‚à•W[2:]xùë†‚à•‚àíùëé+‚àö
2
2! !
‚à•W[2:]‚à•
= 
sinh
ùëé‚à•W[2:]xùë†‚à•1
‚à•W[2:]xùë†‚à•+
cosh
ùëé‚à•W[2:]xùë†‚à•ùëé
‚à•W[2:]xùë†‚à•!
‚à•W[2:]‚à•+
 
1716Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
 
‚àísinh
ùëé‚à•W[2:]xùë†‚à•1
‚à•W[2:]xùë†‚à•2+
cosh
ùëé‚à•W[2:]xùë†‚à• ‚àö
2
2‚àíùëé! !
‚à•W[2:]‚à•.
Since‚àísinh
ùëé‚à•W[2:]xùë†‚à•
1
‚à•W[2:]xùë†‚à•2and
cosh
ùëé‚à•W[2:]xùë†‚à• ‚àö
2
2‚àíùëé
are both negative,
‚à•J1‚à•< 
sinh
ùëé‚à•W[2:]xùë†‚à•1
‚à•W[2:]xùë†‚à•+
cosh
ùëé‚à•W[2:]xùë†‚à•ùëé
‚à•W[2:]xùë†‚à•!
‚à•W[2:]‚à•
‚â§ 
sinh
ùëé‚à•W[2:]xùë†‚à•1
‚à•W[2:]xùë†‚à•+
cosh
ùëé‚à•W[2:]xùë†‚à•1
‚à•W[2:]xùë†‚à•!
‚à•W[2:]‚à• (ùëé<1)
=exp(cosh‚àí1(‚àö
2)‚à•W[2:]xùë†‚à•)
‚à•W[2:]xùë†‚à•‚à•W[2:]‚à•.
‚ñ°
For normalized features, it is much easier to implement (9) than
(8) as a regularization term in loss functions of learning tasks.
The second case is ‚à•xùë†‚à•‚Üí‚àû , which corresponds to the case of
input features with a considerably large norm. Due to symmetry in
all spatial axes, it suffices to consider a simple case where only the
first coordinate of xùë†goes to infinity and other dimensions are all
zero. We will first state the result in the following lemma, with the
proof presented in Appendix A.3.
Lemma 3.6. Letxùë†=[ùúè0¬∑¬∑¬∑0]‚ä§. Asùúè‚Üí‚àû , a necessary condi-
tion for a finite‚à•J1‚à•is‚à•W[2]‚à•<1, where W[2]denotes the second
column vector of W.
Next, we extend our result to the general case for a general xùë†
with‚à•xùë†‚à•‚Üí‚àû in the following theorem.
Theorem 3.7. As‚à•xùë†‚à•‚Üí‚àû , a necessary condition for a finite
‚à•J1‚à•is‚à•W[2:]‚à•F<1, where‚à•¬∑‚à•Fdenotes the Frobenius norm.
Proof. For any given xùë†with‚à•xùë†‚à•‚Üí‚àû , there exists a unitary
matrix P‚ààRùëõ√óùëõsuch that Pxùë†=[ùúè0¬∑¬∑¬∑0]‚ä§whereùúè‚Üí‚àû . Since
W[2:]xùë†=W[2:]P‚ä§Pxùë†, according Lemma 3.6, a necessary condi-
tion for a finite‚à•J1‚à•is‚à•W[2:]P‚ä§
[1]‚à•<1. Here we use W[2:]P‚ä§
[1]
to denote the first column of W[2:]P‚ä§. Let e=[1 0¬∑¬∑¬∑0]‚ä§, then
W[2:]P‚ä§
[2]=W[2:]P‚ä§e. Since Pis unitary, P‚ä§is also unitary. Then,
1>‚à•W[2:]‚à•F=‚à•W[2:]P‚ä§‚à•F‚â•‚à•W[2:]P‚ä§‚à•
=sup
‚à•u‚à•=1‚à•W[2:]P‚ä§u‚à•‚â•‚à• W[2:]P‚ä§e‚à•=‚à•W[2:]P‚ä§
[1]‚à•.
Hence, a necessary condition for a finite ‚à•J1‚à•in the case of any
‚à•xùë†‚à•‚Üí‚àû is‚à•W[2:]‚à•F<1. ‚ñ°
From the theorem above, we can conclude that when ‚à•xùë†‚à•‚Üí‚àû ,
if‚à•W[2:]‚à•ùêπ<1, then it is feasible to bound ‚à•J1‚à•. Therefore, tobound‚à•J1‚à•in all cases, we need to focus on bounding ‚à•W[2:]‚à•.
However, in the case of ‚à•xùë†‚à•‚Üí‚àû , it is difficult to derive a simple
upper bound, unlike the case of the Poincar√© ball model.
Bias translation. To perform bias translation in hyperboloid,
we define bas a Euclidean vector located at ToHùëë‚Ä≤,ùêæ={b‚àà
Rùëë‚Ä≤+1|‚ü®b,o‚ü©L=0}and then parallel transport bto the point in
hyperboloid and map it to hyperboloid. Let ùëÉùêæ
o‚Üíxùêª(¬∑)denote the
parallel transport from ToHùëë‚Ä≤,ùêætoTxùêªHùëë‚Ä≤,ùêæ, we have xùêª‚äïùêæb:=
expùêæ
xùêª(ùëÉùêæ
o‚Üíxùêª(b))[8]. Similar to the weight matrix multiplication,
for Lipschitz analysis we only need to look at the spatial axes. To
derive the Lipschitz bound for z=y‚äï1b, denote its Jacobian by
J2=ùúïzùë†/ùúïyùë†. We have the following result (due to page limit, we
omit the proof which resembles that of Theorem 3.4).
Theorem 3.8. The spectral norm of J2satisfies
‚à•J2‚à•‚â§cosh(‚à•bùë†‚à•)+sinh(‚à•bùë†‚à•)
‚à•bùë†‚à•(1+ùë¶ùë°)
y‚ä§
ùë†bùë†+
‚à•yùë†‚à•+y‚ä§ùë†bùë†
ùë¶ùë°(1+ùë¶ùë°)‚à•yùë†‚à•
.
3.2.3 Discussion. While the Poincar√© ball and the hyperboloid
models exhibit isometric equivalence, their distinct Euclidean rep-
resentations lead to different numerical characteristics for HNNs.
Given that the Lipschitz bounds we consider fundamentally rely on
the Euclidean representation, it is comparatively more manageable
to control robustness in the Poincar√© ball model. In its Euclidean
representation, the features are bounded in a ball while it is more
difficult for the hyperboloid model since features could possess
large coordinates. This has to be understood numerically rather
than geometrically. Further, comparing (5) and (9), it is evident that
discussing Lipschitz analysis for the hyperboloid model is more in-
tricate, especially when the feature vector is distant from the origin.
Our experimental results will demonstrate that HNNs employing
the Poincar√© model indeed exhibit greater robustness than the hy-
perboloid model in most cases. Nevertheless, the Poincar√© model
is susceptible to gradient explosion in certain scenarios. In cases
where we need to use the hyperboloid, (9) is an effective expression
for a simplified representation of Lipschitz bounds.
4 Experiments
4.1 Datasets and Settings
To show how Lipschitz regularization improves HNN robustness,
we consider node classification tasks with noisy features. We utilize
the following datasets: the WebKB datasets [ 10] including Texas
and Wisconsin, where nodes correspond to web pages and edges
represent hyperlinks; the Actor dataset [ 49], where nodes represent
actors and two nodes are connected if they co-occur on the same
Wikipedia page; the citation dataset Cora [ 37] and Pubmed [ 44],
where nodes are publications and edges represent citations. These
graphs are sparse with Gromov hyperbolicity [ 47] closely resembles
that of a hyperbolic space. This makes HNNs particularly advanta-
geous for classification tasks on these datasets. Table 1 summarizes
their statistics.
We perform Lipschitz regularized training for HNN using the
Poincar√© ball model and the hyperboloid model. Specifically, we use
the sum of Lipschitz bounds at all node features as a regularization
 
1717KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
Table 1: Statistics of datasets
Datasets Nodes Edges Features Class Hyperbolicity Training Validating Testing
Texas 183 280 1,703 5 1.0 244 44 44
Wisconsin 251 466 1,703 5 1.0 314 54 54
Actor 7,600 26,752 931 5 1.5 9,254 1,878 1,878
Cora 2,708 5,278 1,433 6 3.0 140 500 1,000
Pubmed 19,717 44,327 500 3 2.5 60 500 1,000
term added to the loss function for classification. The multiplier for
the regularization term is a hyperparameter ùúÜ, chosen by validation
from {1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9}.
In our experiments, we compare the following methods: ‚ÄúNor-
mal‚Äù, which represents non-regularized training; ‚ÄúLip-Reg1‚Äù, which
refers to the computation of the Lipschitz bounds following (4) and
(8); ‚ÄúLip-Reg2‚Äù, which refers to the computation of the Lipschitz
bounds following (7) and (9). We also compare with the baseline
method of ‚ÄúClip‚Äù [ 25], where clipping is applied for regularization.
In all the experiments, we take an HNN with two layers. The
first layer is a hyperbolic linear layer with zero bias term either
in the Poincar√© ball model or the hyperboloid model, with output
dimension 16. The second layer is a Euclidean linear layer, the
output dimension of which depends on the number of classes. We
use Riemannian Adam [ 6] for optimization, where the learning rate
is chosen from {0.05, 0.01, 0.005, 0.001} by validation. The dropout
rate is also validated from {0, 0.2, 0.5}. By virtue of the discussion
in Section 3.2, the input features are always normalized.
The implementation of our proposed methods can be accessed
at https://github.com/YidanM/HNN_Lipshitz.
4.2 Results
We consider datasets in Table 1 poisoned by noise added to hyper-
bolic node features exponentially mapped from Euclidean inputs.
Specifically, we add independent Gaussian noise to all the normal-
ized input node features with various standard deviations. The
standard deviation (std) is from {0.001, 0.005, 0.01}.
Tables 2‚Äì4 present the test accuracies. The results indicate the
effectiveness of both expressions of Lipschitz bounds, contributing
to enhanced test accuracies in comparison to non-regularized train-
ing, or the Clip regularization, across most scenarios. Although
there is clear advantage of using Lipschitz regularization, there is
no clear conclusion for using ‚ÄúLip-Reg1‚Äù or ‚ÄúLip-Reg2‚Äù.
We also consider the scenario when the noise level escalates
with a large std from {0.1, 1}. In this case, the Poincar√© ball model
experiences issues with gradient explosion, necessitating the ex-
clusive use of the hyperboloid model. Such substantial increases
in standard deviation result in markedly diminished accuracy for
HNNs. Nevertheless, Table 5 clearly demonstrates that Lipschitz
regularization still consistently enhances performance in most cases
under these challenging conditions.
For clear comparison, we also present the test accuracies for
clean data in Table 6. Interestingly, Lipschitz regularization still
improves the performance in some scenarios. We attribute this
to the improved generalizability. Also, although HNNs using the
Poincar√© ball model and the hyperboloid model behave similarlyfor clean data, the Poincar√© ball is in general much better when
noises are injected. This implies that the Poincar√© ball model is less
subject to perturbation vulnerability.
To study how Lipschitz bounds evolve with training epochs, we
plot their relationships when training on the Wisconsin, Actor and
Cora datasets. Figure 1 shows the results for the Poincar√© ball model
and Figure 4 in Appendix B shows the results for the hyperboloid
model.
(a)
 (b)
(c)
Figure 1: Change of Lipschitz bounds with epochs for the
Poincar√© model. (a) Wisconsin; (b) Actor; (c) Cora.
4.3 Sensitivity Analysis
We perform sensitivity analysis on the influence of the multiplier ùúÜ
of the regularization term. In Figures 2 and 3, we show the change
of test accuracy with different ùúÜvalues. Each subfigure shows a
different noise level used in the experiments.
From the results, we find that the test accuracy is relatively stable
with the change of ùúÜ. Nevertheless, in most cases, there is a sudden
decline when ùúÜexceeds a certain threshold. This phenomenon
underscores a trade-off between model performance and robustness,
where an excessive emphasis on regularization compromises the
predictive accuracy. This decay is less significant when the noise
level is high, indicating that robustness is more important under
large noise.
 
1718Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 2: The test accuracy of two-layer HNN for noisy data with std=0.001
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.9621¬±0.0347 0.9134¬±0.0280 0.4531¬±0.0175 0.5167¬±0.0175 0.6917¬±0.0083
Clip 0.9621¬±0.0347 0.9134¬±0.0280 0.4531¬±0.0175 0.5167¬±0.0175 0.6917¬±0.0083
Lip-Reg1 0.9621¬±0.0132 0.9568¬±0.0214 0.4613¬±0.0111 0.6040¬±0.0010 0.7427¬±0.0031
Lip-Reg2 0.9469¬±0.0131 0.9506¬±0.0283 0.4580¬±0.0125 0.5973¬±0.0119 0.7400¬±0.0040
HyperboloidNormal 0.9242¬±0.0107 0.9630¬±0.0151 0.4313¬±0.0095 0.5190¬±0.0057 0.6920¬±0.0051
Clip 0.9242¬±0.0107 0.9506¬±0.0231 0.4384¬±0.0035 0.5160¬±0.0054 0.6893¬±0.0082
Lip-Reg1 0.9318¬±0.0000 0.9692¬±0.0087 0.4450¬±0.0013 0.5860¬±0.0049 0.7330¬±0.0016
Lip-Reg2 0.9318¬±0.0000 0.9815¬±0.0000 0.4485¬±0.0009 0.5887¬±0.0063 0.7320¬±0.0029
Table 3: The test accuracy of two-layer HNN for noisy data with std=0.005
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.9015¬±0.0730 0.8827¬±0.0534 0.4329¬±0.0019 0.4803¬±0.0086 0.6847¬±0.0042
Clip 0.9015¬±0.0730 0.8827¬±0.0534 0.4329¬±0.0019 0.4803¬±0.0086 0.6847¬±0.0042
Lip-Reg1 0.9167¬±0.0860 0.9136¬±0.0283 0.4413¬±0.0059 0.5727¬±0.0057 0.7303¬±0.0138
Lip-Reg2 0.9091¬±0.0787 0.9074¬±0.0490 0.4482¬±0.0132 0.5843¬±0.0104 0.7233¬±0.0068
HyperboloidNormal 0.9015¬±0.0386 0.9321¬±0.0231 0.4177¬±0.0033 0.4957¬±0.0102 0.6820¬±0.0112
Clip 0.9015¬±0.0386 0.9012¬±0.0231 0.4347¬±0.0013 0.4950¬±0.0114 0.6810¬±0.0126
Lip-Reg1 0.9015¬±0.0386 0.9321¬±0.0231 0.4277¬±0.0035 0.5437¬±0.0111 0.7007¬±0.0109
Lip-Reg2 0.9015¬±0.0284 0.9321¬±0.0231 0.4262¬±0.0036 0.5443¬±0.0048 0.6990¬±0.0156
Table 4: The test accuracy of two-layer HNN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.7727¬±0.1202 0.7716¬±0.0566 0.4073¬±0.0098 0.4360¬±0.0052 0.6490¬±0.0089
Clip 0.7727¬±0.1202 0.7716¬±0.0566 0.4073¬±0.0098 0.4360¬±0.0052 0.6490¬±0.0089
Lip-Reg1 0.8106¬±0.0572 0.7901¬±0.0595 0.4180¬±0.0102 0.5010¬±0.0066 0.6673¬±0.0029
Lip-Reg2 0.8258¬±0.0347 0.7963¬±0.0641 0.4269¬±0.0074 0.5053¬±0.0142 0.6783¬±0.0159
HyperboloidNormal 0.7803¬±0.0284 0.8395¬±0.0486 0.3987¬±0.0042 0.4500¬±0.0135 0.6590¬±0.0091
Clip 0.7803¬±0.0284 0.8148¬±0.0303 0.3990¬±0.0031 0.4483¬±0.0139 0.6590¬±0.0091
Lip-Reg1 0.8258¬±0.0283 0.8395¬±0.0486 0.4130¬±0.0010 0.4780¬±0.0073 0.6693¬±0.0031
Lip-Reg2 0.8106¬±0.0467 0.8395¬±0.0486 0.4054¬±0.0035 0.4580¬±0.0036 0.6613¬±0.0057
Table 5: The test accuracy of two-layer HNN for noisy data with large std using the hyperboloid model
Noise std Method Texas Wisconsin Actor Cora Pubmed
0.1Normal 0.4924¬±0.0107 0.4506¬±0.0315 0.2744¬±0.0035 0.1627¬±0.0057 0.3533¬±0.0009
Clip 0.5379¬±0.0429 0.5432¬±0.0486 0.2744¬±0.0035 0.1723¬±0.0041 0.3610¬±0.0014
Lip-Reg1 0.5757¬±0.0386 0.5123¬±0.0174 0.2924¬±0.0033 0.1643¬±0.0045 0.3697¬±0.0225
Lip-Reg2 0.5909¬±0.0491 0.5185¬±0.0262 0.2875¬±0.0122 0.1650¬±0.0029 0.3563¬±0.0005
1Normal 0.3257¬±0.0934 0.2222¬±0.0920 0.2201¬±0.0020 0.1553¬±0.0125 0.3250¬±0.0211
Clip 0.3333¬±0.0467 0.3086¬±0.0231 0.2178¬±0.0028 0.1523¬±0.0127 0.3283¬±0.0009
Lip-Reg1 0.3560¬±0.0214 0.3704¬±0.0000 0.2924¬±0.0033 0.1537¬±0.0079 0.3457¬±0.0094
Lip-Reg2 0.3485¬±0.0772 0.3025¬±0.0087 0.2875¬±0.0122 0.1633¬±0.0196 0.3427¬±0.0133
 
1719KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
(a)
 (b)
(c)
 (d)
Figure 2: Sensitivity analysis for ùúÜusing the Poincar√© model.
Evidently, both Lipschitz regularization techniques proficiently
reduce the Lipschitz bounds of the HNNs. Notably, the Lipschitz
bounds continue to increase with epochs even when employing
Lipschitz regularization. This phenomenon shows that it is nec-
essary to maintain a Lipschitz bound of sufficient magnitude to
ensure that the HNN retains the capacity for meaningful feature
discrimination.
4.4 Complexity Analysis
Since in both Poincar√© and hyperboloid models, the linear opera-
tion is dominated by matrix vector multiplication, with complexity
ùëÇ(ùëöùëõ). GivenùëÅnodes in the graph, the complexity is ùëÇ(ùëöùëõùëÅ).
Moreover, in both models, the complexity for calculating the Lips-
chitz bounds is dominated by the matrix multiplication, which is
ùëõ-by-ùëömultiplied with ùëö-by-ùëõ, thusùëÇ(ùëõ2ùëö).
In Table 7, we report the average run time needed to train one
epoch on the Pubmed dataset using the four methods: Normal, Clip,
Lip-Reg1, and Lip-Reg2. We find that it takes similar time to train
all the models.
4.5 Application to Other Architectures
Our proposed Lipschitz regularization can be generally adopted
in any neural networks that contain HNN layers. We consider the
following two examples: the first is an HNN with two hyperbolic
layers and a Euclidean layer, where the Lipschitz bound is taken
to be the product of the bounds of the two layers; the second is an
HGCN [ 8] with a hyperbolic message passing layer and a Euclidean
layer. We report the numerical results, namely the test accuracy of
both models for noisy input features with std=0.01 in Tables 8 and
9. The results show that our Lipschitz regularization also improves
the three-layer HNN and the HGCN models.
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 3: Sensitivity analysis for ùúÜusing the hyperboloid
model.
5 Conclusion
In this paper, we have performed a rigorous Lipschitz analysis for
the linear layers within HNNs. We derived and had a comprehen-
sive discussion of Lipschitz bounds, particularly in the context of
varied input amplitudes. Subsequently, we applied Lipschitz reg-
ularization during the training of HNNs, consistently enhancing
their robustness against noise perturbations.
One limitation of the current work is that we only have consid-
ered a very simple form of hyperbolic layers, which might result
in restricted expressivity. We will explore realistic approaches to
incorporating the Lipschitz bounds for other common hyperbolic
operators. Another future work is extending the current progress to
other types of manifold where neural networks use Euclidean-like
neural operators. We will also explore how to use our analysis to
devise adversarial defense approaches.
Acknowledgments
This work is supported by National Natural Science Foundation of
China (Grant No. 12301117). YL and YM have been supported by
the Summer Research Scholars (SRS) program of Duke Kunshan
University.
 
1720Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 6: The test accuracy of two-layer HNN for clean data
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.9394¬±0.0131 0.9012¬±0.0283 0.4556¬±0.0091 0.5197¬±0.0115 0.6920¬±0.0070
Clip 0.9394¬±0.0131 0.9012¬±0.0283 0.4556¬±0.0091 0.5197¬±0.0115 0.6920¬±0.0070
Lip-Reg1 0.9469¬±0.0131 0.9444¬±0.0186 0.4622¬±0.0078 0.6060¬±0.0026 0.7367¬±0.0006
Lip-Reg2 0.9469¬±0.0131 0.9383¬±0.0214 0.4604¬±0.0102 0.6030¬±0.0087 0.7397¬±0.0081
HyperboloidNormal 0.9318¬±0.0185 0.8889¬±0.0151 0.4297¬±0.0012 0.5203¬±0.0034 0.6883¬±0.0094
Clip 0.9318¬±0.0185 0.8889¬±0.0151 0.4466¬±0.0033 0.5227¬±0.0025 0.6847¬±0.0024
Lip-Reg1 0.9318¬±0.0185 0.8889¬±0.0151 0.4475¬±0.0013 0.5853¬±0.0125 0.7300¬±0.0051
Lip-Reg2 0.9318¬±0.0185 0.9444¬±0.0303 0.4384¬±0.0011 0.5860¬±0.0016 0.7343¬±0.0042
Table 7: Average runtime for training one epoch on the Pubmed dataset (mean and std over 3 trials)
Model Noise std 0 0.001 0.005 0.01 0.1 1
Poincar√© ballNormal 0.8091¬±0.0200 0.8120¬±0.0172 0.8006¬±0.0212 0.8157¬±0.0103 - -
Clip 0.8201¬±0.0089 0.8223¬±0.0060 0.8248¬±0.0098 0.8179¬±0.0113 - -
Lip-Reg1 0.8145¬±0.0272 0.8070¬±0.0266 0.8314¬±0.0075 0.8200¬±0.0170 - -
Lip-Reg2 0.8233¬±0.0223 0.8284¬±0.0166 0.8234¬±0.0176 0.8173¬±0.0342 - -
HyperboloidNormal 0.9734¬±0.0157 0.9949¬±0.0123 1.0059¬±0.0118 1.0011¬±0.0036 0.9926¬±0.0169 1.0060¬±0.0070
Clip 1.0172¬±0.0309 0.9865¬±0.0125 0.9965¬±0.0107 1.0049¬±0.0163 1.0148¬±0.0151 0.9818¬±0.0179
Lip-Reg1 0.9975¬±0.0057 1.0036¬±0.0066 1.0222¬±0.0083 1.0232¬±0.0095 1.0143¬±0.0087 1.0281¬±0.0061
Lip-Reg2 0.9907¬±0.0072 0.9891¬±0.0307 0.9866¬±0.0267 1.0196¬±0.0053 1.0198¬±0.0043 0.9648¬±0.0334
Table 8: The test accuracy of three-layer HNN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.8561¬±0.0263 0.7901¬±0.0428 0.7041¬±0.0102 0.2650¬±0.0111 0.5657¬±0.0081
Lip-Reg1 0.8712¬±0.0347 0.8025¬±0.0566 0.7069¬±0.0056 0.2757¬±0.0121 0.5983¬±0.0031
Lip-Reg2 0.8712¬±0.0347 0.8333¬±0.0186 0.7069¬±0.0046 0.2753¬±0.0105 0.5817¬±0.0257
HyperboloidNormal 0.7803¬±0.0347 0.7099¬±0.0566 0.4354¬±0.0092 0.2623¬±0.0240 0.5517¬±0.0074
Lip-Reg1 0.8636¬±0.0228 0.8395¬±0.0466 0.4404¬±0.0088 0.2690¬±0.0240 0.5757¬±0.0090
Lip-Reg2 0.8788¬±0.0473 0.8272¬±0.0650 0.4386¬±0.0068 0.2700¬±0.0218 0.5843¬±0.0047
Table 9: The test accuracy of HGCN for noisy data with std=0.01
Model Method Texas Wisconsin Actor Cora Pubmed
Poincar√© ballNormal 0.7500¬±0.0455 0.7840¬±0.0651 0.6777¬±0.0213 0.6240¬±0.0393 0.6457¬±0.0205
Lip-Reg1 0.8030¬±0.0572 0.8025¬±0.0466 0.6894¬±0.0109 0.6907¬±0.0451 0.7107¬±0.0260
Lip-Reg2 0.8106¬±0.0525 0.7901¬±0.0566 0.6807¬±0.0121 0.7320¬±0.0062 0.7263¬±0.0150
HyperboloidNormal 0.7348¬±0.0429 0.7531¬±0.0231 0.6876¬±0.0091 0.6343¬±0.0066 0.6747¬±0.0213
Lip-Reg1 0.7955¬±0.0371 0.7531¬±0.0231 0.6922¬±0.0077 0.7223¬±0.0063 0.7183¬±0.0116
Lip-Reg2 0.7955¬±0.0371 0.7716¬±0.0349 0.6903¬±0.0058 0.7270¬±0.0086 0.7257¬±0.0063
 
1721KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
References
[1]Ola Ahmad and Freddy Lecue. 2022. FisheyeHDK: Hyperbolic Deformable Ker-
nel Learning for Ultra-Wide Field-of-View Image Recognition. In 36th AAAI
Conference on Artificial Intelligence.
[2]James W Anderson. 2006. Hyperbolic geometry. Springer Science & Business
Media.
[3]Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, and Jamal Atif. 2021.
On Lipschitz regularization of convolutional layers using toeplitz matrix theory.
In35th AAAI Conference on Artificial Intelligence.
[4]Mina Ghadimi Atigh, Julian Schoep, Erman Acar, Nanne van Noord, and Pascal
Mettes. 2022. Hyperbolic Image Segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition.
[5]Gregor Bachmann, Gary B√©cigneul, and Octavian Ganea. 2020. Constant cur-
vature graph convolutional networks. In International Conference on Machine
Learning.
[6]Gary Becigneul and Octavian-Eugen Ganea. 2019. Riemannian Adaptive Op-
timization Methods. In International Conference on Learning Representations.
https://openreview.net/forum?id=r1eiqi09K7
[7]Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christo-
pher R√©. 2020. Low-Dimensional Hyperbolic Knowledge Graph Embeddings.
InProceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. https://doi.org/10.18653/v1/2020.acl-main.617
[8]Ines Chami, Zhitao Ying, Christopher R√©, and Jure Leskovec. 2019. Hyperbolic
graph convolutional neural networks. Advances in neural information processing
systems 32 (2019), 4868‚Äì4879.
[9]Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong
Sun, and Jie Zhou. 2022. Fully Hyperbolic Neural Networks. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 5672‚Äì5686.
[10] Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell,
Kamal Nigam, and Se√°n Slattery. 2000. Learning to construct knowledge bases
from the World Wide Web. Artificial intelligence 118, 1-2 (2000), 69‚Äì113.
[11] Jindou Dai, Yuwei Wu, Zhi Gao, and Yunde Jia. 2021. A Hyperbolic-to-Hyperbolic
Graph Convolutional Network. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition.
[12] George Dasoulas, Kevin Scaman, and Aladin Virmaux. 2021. Lipschitz normal-
ization for self-attention layers with application to graph neural networks. In
International Conference on Machine Learning.
[13] N Benjamin Erichson, Dane Taylor, Qixuan Wu, and Michael W Mahoney. 2021.
Noise-response analysis of deep neural networks quantifies robustness and finger-
prints structural malware. In Proceedings of the 2021 SIAM International Conference
on Data Mining (SDM). SIAM.
[14] Xiran Fan, Chun-Hao Yang, and Baba C Vemuri. 2022. Nested Hyperbolic Spaces
for Dimensionality Reduction and Hyperbolic NN Design. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition.
[15] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016.
Robustness of classifiers: from adversarial to random noise. Advances in neural
information processing systems 29 (2016).
[16] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George
Pappas. 2019. Efficient and accurate estimation of Lipschitz constants for deep
neural networks. Advances in Neural Information Processing Systems 32 (2019).
[17] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. 2020. Stability properties
of graph neural networks. IEEE Transactions on Signal Processing 68 (2020),
5680‚Äì5695.
[18] Fernando Gama and Somayeh Sojoudi. 2022. Distributed linear-quadratic control
with graph neural networks. Signal Processing 196 (2022), 108506.
[19] Octavian Ganea, Gary B√©cigneul, and Thomas Hofmann. 2018. Hyperbolic neural
networks. Advances in neural information processing systems 31 (2018), 5345‚Äì
5355.
[20] Feng Gao, Guy Wolf, and Matthew Hirn. 2019. Geometric scattering for graph
data analysis. In International Conference on Machine Learning.
[21] Amirmasoud Ghiassi, Robert Birke, and Lydia Y Chen. 2023. Robust Learning via
Golden Symmetric Loss of (un) Trusted Labels. In Proceedings of the 2023 SIAM
International Conference on Data Mining (SDM). SIAM.
[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
and harnessing adversarial examples. In International Conference on Learning
Representations.
[23] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pas-
canu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam
Santoro, and Nando de Freitas. 2019. Hyperbolic Attention Networks. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
rJxHsjRqFQ
[24] Yunhui Guo, Haoran Guo, and Stella X Yu. 2022. Co-SNE: Dimensionality re-
duction and visualization for hyperbolic data. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition.
[25] Yunhui Guo, Xudong Wang, Yubei Chen, and Stella X Yu. 2022. Clipped hyper-
bolic classifiers are super-hyperbolic classifiers. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 11‚Äì20.
[26] Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, and Anima Anandkumar.
2021. Training Certifiably Robust Neural Networks with Efficient Local Lipschitz
Bounds. Advances in Neural Information Processing Systems 34 (2021).
[27] Yaning Jia, Dongmian Zou, Hongfei Wang, and Hai Jin. 2023. Enhancing Node-
Level Adversarial Defenses by Lipschitz Regularization of Graph Neural Net-
works. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
[28] Matt Jordan and Alexandros G Dimakis. 2020. Exactly computing the local
Lipschitz constant of relu networks. Advances in Neural Information Processing
Systems 33 (2020), 7344‚Äì7353.
[29] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. 2020. Convergence and
stability of graph convolutional networks on large random graphs. Advances in
Neural Information Processing Systems 33 (2020), 21512‚Äì21523.
[30] Hyunjik Kim, George Papamakarios, and Andriy Mnih. 2021. The Lipschitz
constant of self-attention. In International Conference on Machine Learning.
[31] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2018. Adversarial examples
in the physical world. In Artificial intelligence safety and security. Chapman and
Hall/CRC, 99‚Äì112.
[32] Fabian Latorre, Paul Rolland, and Volkan Cevher. 2020. Lipschitz constant esti-
mation of neural networks via sparse polynomial optimization. In International
Conference on Learning Representations.
[33] Marc Law. 2021. Ultrahyperbolic neural networks. Advances in Neural Information
Processing Systems 34 (2021), 22058‚Äì22069.
[34] Qi Liu, Maximilian Nickel, and Douwe Kiela. 2019. Hyperbolic Graph Neural
Networks. Advances in Neural Information Processing Systems 32 (2019), 8230‚Äì
8241.
[35] Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye
Teh. 2019. Continuous hierarchical representations with poincar√© variational
auto-encoders. Advances in neural information processing systems 32 (2019).
[36] Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. 2023. The numerical
stability of hyperbolic representation learning. In International Conference on
Machine Learning.
[37] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-
driven active surveying for collective classification. In 10th International Workshop
on Mining and Learning with Graphs, Vol. 8. 1.
[38] Maximillian Nickel and Douwe Kiela. 2018. Learning continuous hierarchies in
the lorentz model of hyperbolic geometry. In International Conference on Machine
Learning.
[39] Giannis Nikolentzos, Michail Chatzianastasis, and Michalis Vazirgiannis. 2023.
Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Rep-
resentations. In International Conference on Artificial Intelligence and Statistics.
1037‚Äì1054.
[40] W. Peng, T. Varanka, A. Mostafa, H. Shi, and G. Zhao. 2021. Hyperbolic Deep
Neural Networks: A Survey. IEEE Transactions on Pattern Analysis & Machine
Intelligence (December 2021). https://doi.org/10.1109/TPAMI.2021.3136921
[41] Eric Qu and Dongmian Zou. 2022. Lorentz Direct Concatenation for Stable
Training in Hyperbolic Neural Networks. In NeurIPS 2022 Workshop on Symmetry
and Geometry in Neural Representations.
[42] Eric Qu and Dongmian Zou. 2023. Hyperbolic Convolution via Kernel Point
Aggregation. arXiv preprint arXiv:2306.08862 (2023).
[43] Frederic Sala, Chris De Sa, Albert Gu, and Christopher R√©. 2018. Representation
tradeoffs for hyperbolic embeddings. In International Conference on Machine
Learning.
[44] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93‚Äì93.
[45] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label
poisoning attacks on neural networks. Advances in neural information processing
systems 31 (2018).
[46] Ryohei Shimizu, YUSUKE Mukuta, and Tatsuya Harada. 2021. Hyperbolic Neural
Networks++. In International Conference on Learning Representations.
[47] Rishi Sonthalia and Anna Gilbert. 2020. Tree! I am no tree! I am a low dimensional
hyperbolic embedding. Advances in Neural Information Processing Systems 33
(2020), 845‚Äì856.
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
InInternational Conference on Learning Representations.
[49] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. 2009. Social influence analysis
in large-scale networks. In Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining.
[50] Matthieu Terris, Audrey Repetti, Jean-Christophe Pesquet, and Yves Wiaux. 2020.
Building firmly nonexpansive convolutional neural networks. In ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 8658‚Äì8662.
[51] Abraham Ungar. 2022. A gyrovector space approach to hyperbolic geometry.
Springer Nature.
 
1722Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[52] Aladin Virmaux and Kevin Scaman. 2018. Lipschitz regularity of deep neural
networks: analysis and efficient estimation. Advances in Neural Information
Processing Systems 31 (2018).
[53] Xiao Wang, Yiding Zhang, and Chuan Shi. 2019. Hyperbolic heterogeneous in-
formation network embedding. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 33.
[54] Zhenxing Wu, Dejun Jiang, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Dong-
sheng Cao, and Tingjun Hou. 2021. Hyperbolic relational graph convolution
networks plus: a simple but highly efficient QSAR-modeling method. Briefings
in Bioinformatics 22, 5 (2021), bbab112.
[55] Tao Yu and Christopher M De Sa. 2019. Numerically accurate hyperbolic em-
beddings using tiling-based models. Advances in Neural Information Processing
Systems 32 (2019).
[56] Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. 2021. Lorentzian
graph convolutional networks. In Proceedings of the Web Conference 2021. 1249‚Äì
1261.
[57] Dongmian Zou, Radu Balan, and Maneesh Singh. 2019. On Lipschitz bounds of
general convolutional neural networks. IEEE Transactions on Information Theory
66, 3 (2019), 1738‚Äì1759.
[58] Dongmian Zou and Gilad Lerman. 2020. Graph convolutional neural networks via
scattering. Applied and Computational Harmonic Analysis 49, 3 (2020), 1046‚Äì1074.
[59] Daniel Z√ºgner, Amir Akbarnejad, and Stephan G√ºnnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining.
[60] Daniel Z√ºgner, Oliver Borchert, Amir Akbarnejad, and Stephan G√ºnnemann.
2020. Adversarial attacks on graph neural networks: Perturbations and their
patterns. ACM Transactions on Knowledge Discovery from Data (TKDD) 14, 5
(2020), 1‚Äì31.
A Additional Proofs
A.1 Proof of Theorem 3.1
First, we compute
J1=ùúïy
ùúïx=ùúïh
tanh‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
Mx
‚à•Mx‚à•i
ùúïx.
Letùê¥:=‚à•Mx‚à•,ùêµ:=‚à•x‚à•,ùê∂:=tanh‚àí1(‚à•x‚à•),ùë£(x)=tanh(ùê¥
ùêµ¬∑ùê∂),
u(x)=Mx
ùê¥. Then,
ùúïy
ùúïx=ùë£¬∑ùúïu
ùúïx+u¬∑ùúïùë£
ùúïx‚ä§
. (10)
We proceed to calculateùúïùë£(x)
ùúïùë•andùúïu(x)
ùúïx, respectively as follows.
ùúïùë£(x)
ùúïx‚ä§
=tanh‚Ä≤ùê¥
ùêµ¬∑ùê∂ùúï
ùê¥
ùêµ¬∑ùê∂
ùúïx
=sech2ùê¥
ùêµ¬∑ùê∂ ùê∂
ùêµ¬∑ùúïùê¥
ùúïx‚ä§
‚àíùê¥ùê∂
ùêµ2¬∑ùúïùêµ
ùúïx‚ä§
+ùê¥
ùêµ¬∑ùúïùê∂
ùúïx‚ä§
;
ùúïu(x)
ùúïx=1
ùê¥2ùúïMx
ùúïx¬∑ùê¥‚àíMx¬∑ùúïùê¥
ùúïx‚ä§
=M
ùê¥‚àíMx
ùê¥2¬∑ùúïùê¥
ùúïx‚ä§
.
(11)
Combining (10)‚Äì(11) yields
ùúïy
ùúïx=tanhùê¥
ùêµ¬∑ùê∂ M
ùê¥‚àíMx
ùê¥2ùúïùê¥
ùúïx‚ä§
+
Mxsech2ùê¥
ùêµ¬∑ùê∂ ùê∂
ùê¥ùêµùúïùê¥
ùúïx‚ä§
‚àíùê∂
ùêµ2ùúïùêµ
ùúïx‚ä§
+1
ùêµùúïùê∂
ùúïx‚ä§
.
Computingùúïùê¥
ùúïx,ùúïùêµ
ùúïx,ùúïùê∂
ùúïxyields
ùúïy
ùúïx=tanh‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
¬∑M
‚à•Mx‚à•‚àíMxx‚ä§M‚ä§M
‚à•Mx‚à•3
+
Mxsech2‚à•Mx‚à•
‚à•x‚à•tanh‚àí1(‚à•x‚à•)
¬∑"
tanh‚àí1(‚à•x‚à•)x‚ä§M‚ä§M
‚à•Mx‚à•2‚à•x‚à•‚àí
tanh‚àí1(‚à•x‚à•)x‚ä§
‚à•x‚à•3+x‚ä§
‚à•x‚à•2(1‚àí‚à•x‚à•2)#
.The spectral norm of this Jacobian matrix J1thus satisfies (4). ‚ñ°
A.2 Proof of Theorem 3.4
Lety=W‚äó1x=exp1o(Wlog1
o(x)). Writing exp1oandlog1
oexplic-
itly yields
yùë†=sinh¬©¬≠¬≠
¬´cosh‚àí1(ùë•ùë°)‚àöÔ∏É
ùë•2
ùë°‚àí1‚à•W[2:]xùë†‚à•¬™¬Æ¬Æ
¬¨W[2:]xùë†
‚à•W[2:]xùë†‚à•.
Leth(xùë†)=sinh 
cosh‚àí1(ùë•ùë°)‚àöÔ∏É
ùë•2
ùë°‚àí1‚à•W[2:]xùë†‚à•!
xùë†
‚à•W[2:]xùë†‚à•, andùë£(xùë†)=
sinh 
cosh‚àí1(ùë•ùë°)‚àö
ùë•2ùë°‚àí1‚à•W[2:]xùë†‚à•!
‚à•W[2:]xùë†‚à•,u(xùë†)=xùë†. Then,
ùúïyùë†
ùúïxùë†=W[2:]ùúïh
ùúïxùë†=W[2:]
ùë£ùúïu
ùúïxùë†+uùúïùë£
ùúïxùë†
=W[2:]¬©¬≠¬≠¬≠¬≠¬≠¬≠
¬´sinh 
cosh‚àí1(ùë•ùë°)‚àöÔ∏É
ùë•2
ùë°‚àí1‚à•W[2:]xùë†‚à•!
‚à•W[2:]xùë†‚à•I+xùë†ùúïùë£
ùúïxùë†‚ä§¬™¬Æ¬Æ¬Æ¬Æ¬Æ¬Æ
¬¨.(12)
To compute
ùúïùë£
ùúïxùë†‚ä§
, we rewrite ùë£=sinh 
cosh‚àí1(‚àö
x‚ä§ùë†xùë†+1)‚àö
x‚ä§ùë†xùë†‚àö
x‚ä§ùë†ùëºxùë†!
‚àö
x‚ä§ùë†ùëºxùë†
where we use ùë•ùë°=‚àöÔ∏Å
‚à•xùë†‚à•2+1=‚àöÔ∏Å
x‚ä§ùë†xùë†+1and denote ùëºas
ùëº=W‚ä§
[2:]W[2:]. Letùëì(xùë†):=x‚ä§ùë†xùë†,ùëî(xùë†):=x‚ä§ùë†ùëºxùë†, thenùë£=
sinh
cosh‚àí1(‚àö
ùëì+1)‚àö
ùëì‚àöùëî
‚àöùëî, and
ùúïùë£
ùúïxùë†‚ä§
=ùúïùë£
ùúïùëìùúïùëì
ùúïxùë†‚ä§
+ùúïùë£
ùúïùëîùúïùëî
ùúïxùë†‚ä§
. Let
ùëé=cosh‚àí1(ùë•ùë°)
‚à•xùë†‚à•. Computingùúïùë£
ùúïùëì,ùúïùë£
ùúïùëî,ùúïùëì
ùúïxùë†‚ä§
, andùúïùëî
ùúïxùë†‚ä§
yields
ùúïùë£
ùúïxùë†‚ä§
=
1
ùë•ùë°‚àíùëé
cosh
ùëé‚à•W[2:]xùë†‚à•
‚à•xùë†‚à•2x‚ä§
ùë†+
¬©¬≠¬≠
¬´ùëécosh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•2‚àísinh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•3¬™¬Æ¬Æ
¬¨x‚ä§
ùë†W‚ä§
[2:]W[2:].
Substituting into (12) yields that J1=
sinh ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•W[2:]+
1
ùë•ùë°‚àíùëé
cosh ùëé‚à•W[2:]xùë†‚à•
‚à•xùë†‚à•2W[2:]xùë†x‚ä§
ùë†+
 
ùëécosh ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•2‚àísinh ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•3!
W[2:]xùë†x‚ä§
ùë†W‚ä§
[2:]W[2:].
The spectral norm of J1thus satisfies
‚à•J1‚à•‚â§ 
sinh
ùëé‚à•W[2:]xùë†‚à•‚à•W[2:]xùë†‚à•‚àí1
‚à•W[2:]xùë†‚à•2+cosh
ùëé‚à•W[2:]xùë†‚à•
ùëé
‚à•W[2:]xùë†‚à•‚àíùëé
‚à•xùë†‚à•+1
ùë•ùë°‚à•xùë†‚à•!
‚à•W[2:]‚à•.‚ñ°
 
1723KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuekang Li, Yidan Mao, Yifei Yang, and Dongmian Zou
A.3 Proof of Lemma 3.6
Letxùë†=[ùúè0¬∑¬∑¬∑0]‚ä§. Then‚à•xùë†‚à•=ùúè,‚à•W[2:]xùë†‚à•=ùúè‚à•W[2]‚à•. As
ùúè‚Üí‚àû ,ùë•ùë°=‚àö
ùúè2+1=ùúè+ùëÇ
1
ùúè
,ùëé=cosh‚àí1(ùë•ùë°)
‚à•xùë†‚à•=ln(2ùúè+ùëÇ(1
ùúè))
ùúè.
Next, we continue with the following simplifications:
sinh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•W[2:]
=1
2ùúè‚à•W[2]‚à•¬©¬≠¬≠
¬´
2ùúè+ùëÇ1
ùúè‚à•W[2]‚à•
‚àí1

2ùúè+ùëÇ
1
ùúè‚à•W[2]‚à•¬™¬Æ¬Æ
¬¨W[2:];

1
ùë•ùë°‚àíùëé
cosh
ùëé‚à•W[2:]xùë†‚à•
‚à•xùë†‚à•2W[2:]xùë†x‚ä§
ùë†
=¬©¬≠¬≠
¬´1
ùúè+ùëÇ
1
ùúè‚àíln
2ùúè+ùëÇ
1
ùúè
ùúè¬™¬Æ¬Æ
¬¨¬∑

2ùúè+ùëÇ
1
ùúè‚à•W[2]‚à•
+1
(2ùúè+ùëÇ(1
ùúè))‚à•W[2]‚à•
2W[2:]Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1 0¬∑¬∑¬∑ 0
0 0¬∑¬∑¬∑ 0
............
0 0¬∑¬∑¬∑ 0Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=¬©¬≠¬≠
¬´1
ùúè+ùëÇ
1
ùúè‚àíln
2ùúè+ùëÇ
1
ùúè
ùúè¬™¬Æ¬Æ
¬¨¬∑

2ùúè+ùëÇ
1
ùúè‚à•W[2]‚à•
+1
(2ùúè+ùëÇ(1
ùúè))‚à•W[2]‚à•
2ùë≥,
where ùë≥=W[2]0¬∑¬∑¬∑ 0
;
¬©¬≠¬≠
¬´ùëécosh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•2‚àísinh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•3¬™¬Æ¬Æ
¬¨¬∑
W[2:]xùë†x‚ä§
ùë†W‚ä§
[2:]W[2:]
= ln(2ùúè+ùëÇ(1
ùúè))
ùúècosh
ln
2ùúè+ùëÇ
1
ùúè
‚à•W[2:]xùë†‚à•
ùúè2‚à•W[2]‚à•2‚àí
sinh
ln
2ùúè+ùëÇ
1
ùúè
‚à•W[2:]xùë†‚à•
ùúè3‚à•W[2]‚à•3!
ùúè2ùë≥W‚ä§
[2:]W[2:]
=ln
2ùúè+ùëÇ
1
ùúè
2ùúè‚à•W[2]‚à•2¬©¬≠¬≠
¬´
2ùúè+ùëÇ1
ùúè‚à•W[2]‚à•
+1

2ùúè+ùëÇ
1
ùúè‚à•W[2]‚à•¬™¬Æ¬Æ
¬¨¬∑
ùë≥W‚ä§
[2:]W[2:]‚àí1
2ùúè‚à•W[2]‚à•3 
2ùúè+ùëÇ1
ùúè‚à•W[2]‚à•
‚àí
1

2ùúè+ùëÇ
1
ùúè‚à•W[2]‚à•!
ùë≥W‚ä§
[2:]W[2:].Since limùúè‚Üí‚àûùëÇ
1
ùúè
=0,
limùúè‚Üí‚àûJ1=limùúè‚Üí‚àûsinh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•W[2:]+
limùúè‚Üí‚àû
1
ùë•ùë°‚àíùëé
cosh
ùëé‚à•W[2:]xùë†‚à•
‚à•xùë†‚à•2W[2:]xùë†x‚ä§
ùë†+
limùúè‚Üí‚àû¬©¬≠¬≠
¬´ùëécosh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•2‚àísinh
ùëé‚à•W[2:]xùë†‚à•
‚à•W[2:]xùë†‚à•3¬™¬Æ¬Æ
¬¨¬∑
W[2:]xùë†x‚ä§
ùë†W‚ä§
[2:]W[2:]
=limùúè‚Üí‚àû1
‚à•W[2]‚à• 
(2ùúè)‚à•W[2]‚à•‚àí1‚àí1
(2ùúè)‚à•W[2]‚à•+1!
W[2:]+
limùúè‚Üí‚àû(1‚àíln(2ùúè)) 
(2ùúè)‚à•W[2]‚à•‚àí1+1
(2ùúè)‚à•W[2]‚à•+1!
ùë≥+
limùúè‚Üí‚àû  
ln(2ùúè)
‚à•W[2]‚à•2‚àí1
‚à•W[2]‚à•3!
(2ùúè)‚à•W[2]‚à•‚àí1+
 
ln(2ùúè)
‚à•W[2]‚à•2+1
‚à•W[2]‚à•3!
1
(2ùúè)‚à•W[2]‚à•+1!
ùë≥W‚ä§
[2:]W[2:].
Since we have‚à•W[2]‚à•‚àí1on the exponent, a necessary condition
for a finite‚à•J1‚à•is‚à•W[2]‚à•<1. ‚ñ°
B Additional Results
We plot Lipschitz bounds versus training epochs in Figure 4 corre-
sponding to Section 4.2.
(a)
 (b)
(c)
Figure 4: Change of Lipschitz bounds with epochs for the
hyperboloid model. (a) Wisconsin; (b) Actor; (c) Cora.
 
1724