NeuroCUT: A Neural Approach for Robust Graph Partitioning
Rishi Shah‚àó
Department of Computer Science and
Engineering, IIT Delhi
Delhi, India
rishi.shah10122001@gmail.comKrishnanshu Jain‚àó
Department of Computer Science and
Engineering, IIT Delhi
Delhi, India
krishnanshu1907@gmail.comSahil Manchanda‚àó
Department of Computer Science and
Engineering, IIT Delhi
Delhi, India
sahilm1992@gmail.com
Sourav Medya
University of Illinois
Chicago, USA
medya@uic.eduSayan Ranu
Department of Computer Science and
Engineering, IIT Delhi, India
Delhi, India
sayanranu@iitd.ac.in
ABSTRACT
Graph partitioning aims to divide a graph into ùëòdisjoint subsets
while optimizing a specific partitioning objective. The majority of
formulations related to graph partitioning exhibit NP-hardness due
to their combinatorial nature. Conventional methods, like approx-
imation algorithms or heuristics, are designed for distinct parti-
tioning objectives and fail to achieve generalization across other
important partitioning objectives. Recently machine learning-based
methods have been developed that learn directly from data. Further,
these methods have a distinct advantage of utilizing node features
that carry additional information. However, these methods assume
differentiability of target partitioning objective functions and can-
not generalize for an unknown number of partitions, i.e., they as-
sume the number of partitions is provided in advance. In this study,
we develop NeuroCUT with two key innovations over previous
methodologies. First, by leveraging a reinforcement learning-based
framework over node representations derived from a graph neural
network and positional features, NeuroCUT can accommodate any
optimization objective, even those with non-differentiable func-
tions. Second, we decouple the parameter space and the partition
count making NeuroCUT inductive to any unseen number of parti-
tion, which is provided at query time. Through empirical evaluation,
we demonstrate that NeuroCUT excels in identifying high-quality
partitions, showcases strong generalization across a wide spectrum
of partitioning objectives, and exhibits strong generalization to
unseen partition count.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning; Artificial
intelligence; ‚Ä¢Mathematics of computing ‚ÜíGraph algo-
rithms; Graph algorithms.
‚àóThese authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671815KEYWORDS
Graph Partitioning, Min Cut, Robustness, Versatile Partitioning
Objectives, Inductive learning, GNN, Reinforcement learning.
ACM Reference Format:
Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, and Sayan
Ranu. 2024. NeuroCUT: A Neural Approach for Robust Graph Partitioning.
In30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671815
1 INTRODUCTION
Graph partitioning is a fundamental problem in network analysis
with numerous real-world applications in various domains such as
system design in online social networks [ 37], dynamic ride-sharing
in transportation systems [ 43], VLSI design [ 18], and preventing
cascading failure in power grids [ 26]. The goal of graph partitioning
is to divide a given graph into disjoint subsets where nodes within
each subset exhibit strong internal connections while having limited
connections with nodes in other subsets. Generally speaking, the
aim is to find somewhat balanced partitions while minimizing the
number of edges across partitions.
1.1 Related Work
Several graph partitioning formulations have been studied in the
literature, mostly in the form of discrete optimization [ 2,8,20‚Äì22].
The majority of the formulations are NP-hard and thus the proposed
solutions are either heuristics or algorithms with approximate solu-
tions [ 20]. Among these, two widely used methods are Spectral Clus-
tering [ 35] and hMETIS [ 20]. Spectral Clustering partitions a graph
into clusters based on the eigenvectors of a similarity matrix de-
rived from the graph. hMETIS is a hypergraph partitioning method
that divides a graph into clusters by maximizing intra-cluster sim-
ilarity while minimizing inter-cluster similarity. However, such
techniques are confined to specific objective functions and are un-
able to leverage the available node features in the graph. Recently,
there have been attempts to solve graph partitioning problem via
neural approaches. The neural approaches have a distinct advantage
that they can utilize node features. Node features supply additional
information and provide contextual insights that may improve the
accuracy of graph partitioning. For instance, [ 47] has recognized
the importance of incorporating node attributes in graph clustering
 
2584
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
where nodes are partitioned into disjoint groups. Note that there are
existing neural approaches [ 11,14,16,23,24,29‚Äì32,38,44] to solve
other NP-hard graph combinatorial problems (e.g., minimum vertex
cover). However these methods are not generic enough to solve
graph partitioning. For example, although S2VDQN [ 23] learns to
solve the Maxcut problem, it is designed for the specific case of
bi-partitioning. This hinders its applicability to the target problem
setup ofùëò-way partitioning.
In this paper, we build a single framework to solve several graph
partitioning problems. One of the most relevant to our work is the
method DMoN by Tsitsulin et al . [45] . This method designs a neural
architecture for cluster assignments and use a modularity-based ob-
jective function for optimizing these assignments. Another method,
that is relevant to our work is GAP, which is an unsupervised
learning method to solve the balanced graph partitioning prob-
lem [34]. It proposes a differentiable loss function for partitioning
based on a continuous relaxation of the normalized cut formulation.
Deep-MinCut being an unsupervised approach learns both node
embeddings and the community structures simultaneously where
the objective is to minimize the mincut loss [ 9]. Another method
solves the multicut problem where the number of partitions is not
an input to the problem [ 17]. The idea is to construct a reformu-
lation of the multicut ILP constraints to a polynomial program
as a loss function. However, the problem formulation is different
than the generic normalized cut or mincut problem. Another related
work is DGCluster [ 3], which proposes to solve the attributed graph
clustering problem while maximizing modularity when ùëòis not
known beforehand. Finally, [ 12] solves the normalized cut problem
only for the case where the number of partitions is exactly two.
Nevertheless, these neural approaches for the graph partitioning
problem often do not use node features and only limited to a distinct
partitioning objective. Here, we point out notable drawbacks that
we address in our framework.
‚Ä¢Non-inductivity to partition count: The number of partitions
required to segment a graph is an input parameter. Hence, it
is important for a learned model to generalize to any partition
count without retraining. In ùëò-way graph partitioning, a model
demonstrates inductivity to the number of partitions when it can
infer on varying partition numbers without specific training for
each. Existing neural approaches are non-inductive to the num-
ber of partitions, i.e they can only infer on number of partitions
on which they are trained. Additionally, it is worth noting that
the optimal number of partitions is often unknown beforehand.
This capability is crucial in practical applications like chip design,
where graph partitioning optimizes logic cell placement by divid-
ing netlists (circuits) into smaller subgraphs, aiding independent
placement. As the optimal partition count is frequently unknown
in advance, experimenting with different partition numbers is a
common practice. Hence, it is a common practice to experiment
with different partition counts and evaluate their impact on the
partitioning objective. While the existing methods [ 12,34,45]
assume that the number of partitions ( ùëò) is known beforehand,
our proposed method can generalize to any ùëò.
‚Ä¢Non-generalizability to different cut functions: Multiple
objective functions for graph cut have been studied in the parti-
tioning literature. The optimal objective function hinges upon
the subsequent application in question. For instance, the twomost relevant studies, DMoN [ 45] and GAP [ 34] focus on maxi-
mizing modularity and minimizing normalized cut respectively.
Our framework is generic and can solve different partitioning
objectives.
‚Ä¢Assumption of differential objective function: Existing neu-
ral approaches assume the objective function to be differentiable.
As we illustrate in ¬ß 2, the assumption does not always hold in
the real-world. For instance, the sparsest cut [ 7] and balanced
cut [34] objectives are not differentiable.
1.2 Our Contributions
In this paper, we circumvent the above-mentioned limitations through
the following key contributions.
‚Ä¢Versatile objectives: We develop NeuroCUT; an auto-regressive,
graph reinforcement learning framework that integrates posi-
tional information, to solve the graph partitioning problem for
attributed graphs. Diverging from conventional algorithms, Neu-
roCUT can solve multiple partitioning objectives. Moreover, un-
like other neural methods, NeuroCUT can accommodate diverse
partitioning objectives, without the necessity for differentiability.
‚Ä¢Inductivity to number of partitions: The parameter space of
NeuroCUT is independent of the partition count. This innova-
tive decoupled architecture endows NeuroCUT with the ability
to generalize effectively to arbitrary partition count specified
during inference.
‚Ä¢Empirical Assessment: We perform comprehensive experi-
ments employing real-world datasets, evaluating NeuroCUT
across four distinct graph partitioning objectives. Our empirical
investigation substantiates the efficacy of NeuroCUT in parti-
tioning tasks, showcasing its robustness across a spectrum of
objective functions. We also demonstrate the capability of Neu-
roCUT to generalize effectively to partition sizes that have not
been seen during training.
2 PROBLEM FORMULATION
In this section, we introduce the concepts central to our work and
formulate the problem. All the notations used in this work are
outlined in Table 1.
Definition 1 (Graph). We denote a graph as G=(V,E,X)
whereVis the set of nodes, E ‚äÜV√óV is the set of edges and
X‚ààR|V|√ó|ùêπ|refers to node feature matrix where ùêπis the set of all
features in graphG.
Definition 2 (Cut). A cutC=(S,T)is a partition ofVinto
two subsetsSandT. The cut-set ofC=(S,T)is the set{(ùë¢,ùë£)‚àà
E|ùë¢‚ààS,ùë£‚ààT} of edges that have one endpoint in Sand the other
endpoint inT.
Definition 3 (Graph Partitioning). Given graphG, we aim
to partitionGintoùëòdisjoint setsP={ùëÉ1,ùëÉ2,¬∑¬∑¬∑,ùëÉùëò}such that the
union of the nodes in those sets is equal to Vi.e‚à™ùëò
ùëñ=1ùëÉùëñ=Vand
each node belongs to exactly one partition.
Partitioning Objective: We aim to minimize/maximize a parti-
tioning objective of the form ùëÇùëèùëó(G,P). A wide variety of objec-
tives for graph partitioning have been proposed in the literature.
 
2585NeuroCUT: A Neural Approach for Robust Graph Partitioning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Without loss of generality, we consider the following four objec-
tives. These objectives are chosen due to being well studied in the
literature, while also being diverse from each other.1
(1)ùëò-MinCut [ 39]:Partition a graph into ùëòpartitions such that the
total number of edges across partitions is minimized.
ùëò-mincut(P)=|P|‚àëÔ∏Å
ùëô=1|cut
ùëÉùëô,ùëÉùëô
|
√ç
ùëí‚ààE|ùëí|(1)
HereùëÉùëôrefers to the set of elements in ùëôùë°‚Ñépartition ofPas
described in Def. 3 and ùëÉùëôrefers to set of elements not in ùëÉùëô.
(2)Normalized Cut [ 41]:Theùëò-MinCut criteria favors cutting small
sets of isolated nodes in the graph. To avoid this unnatural
bias for partitioning out small sets of points, normalised cut
computes the cut cost as a fraction of the total edge connections
to all the nodes in the graph.
Ncut(P):=|P|‚àëÔ∏Å
ùëô=1|cut
ùëÉùëô,ùëÉùëô
|
vol(ùëÉùëô,V)(2)
Here, vol(ùëÉùëô,V):=√ç
ùë£ùëñ‚ààùëÉùëô,ùë£ùëó‚ààVùëí(ùë£ùëñ,ùë£ùëó).
(3)Balanced Cut [ 34]:Balanced cut favours partitions of equal sizes
so an extra term that indicates the squared distance from equal
sized partition is added to normalized cuts.
Balanced-Cuts(P):=|P|‚àëÔ∏Å
ùëô=1|cut
ùëÉùëô,ùëÉùëô
|
vol(ùëÉùëô,V)+(|ùëÉùëô|‚àí|V|/ùëò)2
|V|2(3)
Here, vol(ùëÉùëô,V):=√ç
ùë£ùëñ‚ààùëÉùëô,ùë£ùëó‚ààVùëí(ùë£ùëñ,ùë£ùëó)
(4)Sparsest Cut [ 7]: Two-way sparsest cuts minimize the cut edges
relative to the number of nodes in the smaller partition. We
generalize it to ùëò-way sparsest cuts by summing up the value
for all the partitions. The intuition behind sparsest cuts is that
any partition should neither be very large nor very small.
Sparsest-Cuts(P)=|P|‚àëÔ∏Å
ùëô=1ùúô(ùëÉùëô,ùëÉùëô) (4)
ùë§‚Ñéùëíùëüùëí ùúô(ùëÜ,¬ØùëÜ)=cut(ùëÜ,¬ØùëÜ)
min(|ùëÜ|,|¬ØùëÜ|)(5)
Problem 1 (Learning to Partition Graph ). Given a graph
Gand the number of partitions ùëò, the goal is to find a partitioning P
of the graphGthat optimizes a target objective function ùëÇùëèùëó(G,P).
Towards this end, we aim to learn a policy ùúãthat assigns each node
ùë£‚ààV to a partition inP.
In addition to our primary goal of finding a partitioning that
optimizes a certain objective function, we also desire policy ùúãto
have the following properties:
(1)Inductive: Policyùúãis inductive if the parameters of the policy
are independent of both the size of the graph and the number of
partitionsùëò. If the policy is not inductive then it will be unable
to infer on unseen size graphs/number of partitions.
(2)Learning Versatile Objectives: To optimize the parameters
of the policy, a target objective function is required. The opti-
mization objective may not be differentiable and it might not be
always possible to obtain a differentiable formulation. Hence,
1Our framework is not restricted to these objectives.Table 1: Notations used in the paper
Symb
ol Meaning
G Graph
V No
de set
ùëí Edgeùëí‚àà
E
E Edge
set
X Featur
e matrix containing raw node features
Nùë£ Neighb
oring nodes of node ùë£
ùëÇ
ùëèùëó(G,P)Objective function based upon graph Gand its
partitioningP
ùëò Numb
er of partitions
Pùë°Partitioning
at timeùë°
ùëÉùë°
ùëñùëñùë°
‚Ñépartition at time ùë°
ùëÉùëñ Set
of nodes that are not in the ùëñùë°‚Ñépartition
Sùë°State
of system at step ùë°
P
ART(Pùë°,ùë£)Partition of node ùë£at timeùë°
Sùë° State representation of Partitions and Graph at
stepùë°
p
os(ùë£) Positional embedding of node ùë£
emb init(ùë£) Initial
embedding of node ùë£
ùõº Numb
er of anchor nodes for lipschitz embedding
ùëá Length
of trajectory
ùúã Policy
function
the policyùúãshould be capable of learning to optimize for a
target objective that may or may not be differentiable.
3NEUROCUT: PROPOSED METHODOLOGY
Fig. 1 describes the framework of NeuroCUT. For a given input
graphG, we first construct the initial partitions of nodes using a
clustering based approach. Subsequently, a message-passing Gnn
embeds the nodes of the graph ensuring inductivity to different
graph sizes. Next, the assignment of nodes to partitions proceeds in
a two-phased strategy. We first select a node to change its partition
and then we choose a suitable partition for the selected node. Fur-
ther, to ensure inductivity on the number of partitions, we decouple
the number of partitions from the direct output representations
of the model. This decoupling allows us to query the model to an
unseen number of partitions. After a node‚Äôs partition is updated,
thereward with respect to change in partitioning objective value
is computed and the parameters of the policy are optimized. The
reward is learned through reinforcement learning (RL) [42].
The choice of using RL is motivated through two observations.
Firstly, cut problems on graphs are generally recognized as NP-
hard, making it impractical to rely on ground-truth data, which
would be computationally infeasible to obtain. Secondly, the cut
objective may lack differentiability. Therefore, it becomes essen-
tial to adopt a learning paradigm that can be trained even under
these non-differentiable constraints. In this context, RL effectively
 
2586KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
Initial Partition AssignmentGNN‚Ñé!Node Embeddingsùë£!ùë£!ùë£"ùë£"ùë£!ùë£#ùë£"Input Graph ùí¢=(ùí±,‚Ñ∞,ùëø)		ùë£!ùë£"ùë£$ùë£%ùë£&ùë£'ùë£(ùë£)							ùëÉùëéùëüùë°ùëÜùëêùëúùëüùëí(ùëù	, ùë£)ùíóUpdate node partitionGet Reward ùëÖ!		Update policy ùùÖùúΩ			after ùëªstepsPartition SelectionNode selectionùë£!ùë£"ùë£"ùë£!ùë£%ùë£#ùë£"ùë£!
ùë£!ùë£"ùë£"ùë£!ùë£%ùë£#ùë£"ùë£!Updating partition of node ùë£#				Target PartitioningObjectiveRaw node + Positional features(Lipschitz Embedding)Start
ùë£%
ùë£!ùë£"ùë£"ùë£!ùë£%ùë£#ùë£"ùë£!PartitionScores
Figure 1: Architecture of NeuroCUT. First, the initial partitioning of the graph is performed based on node features and
positional embeddings. These embeddings are refined using GNN to infuse toplogical information from neighborhood. At each
step a node is selected and its partitioned is updated. During training the GNN parameters are updated and hence embeddings
are re-computed. During inference, the GNN is called only once to compute the embeddings of the nodes of the graph.
addresses both of these critical requirements. Additionally, in the
process of sequentially constructing a solution, RL allows us to
model the gain obtained by perturbing the partition of a node.
Markov Decision Process. Given a graphG, our objective is
to find the partitioning Pthat maximizes/minimizes the target
objective function ùëÇùëèùëó(G,P). We model the task of iteratively
updating the partition for a node as a Markov Decision Process
(MDP) defined by the tuple (ùëÜ,A,ùúå,ùëÖ,ùõæ). Here,ùëÜis the state space,
Ais the set of all possible actions,ùúå:ùëÜ√óùëÜ√óA‚Üí[ 0,1]denotes
thestate transition probability function, ùëÖ:ùëÜ√óA‚Üí Rdenotes
thereward function andùõæ‚àà(0,1)thediscounting factor. We next
formalize each of these notions in our MDP formulation.
3.1 State: Initialization & Positional Encoding
Initialization. Instead of directly starting from empty partitions,
we perform a warm start operation that clusters the nodes of the
graph to obtain the initial graph partitions. The graph is clustered
intoùëòclusters based on their raw features and positional embed-
dings (discussed below). We use K-means [28] algorithm for this
task. The details of the clustering are present in Appendix A.1.
Positional Encoding (Embeddings). Given that the partitioning
objectives are NP-hard mainly due to the combinatorial nature
of the graph structure, we look for representations that capture
the location of a node in the graph. Positional encodings provide
an idea of the position in space of a given node within the graph.
Two nodes that are closer in the graph, should be closer in the
embedding space. Towards this, we use Lipschitz Embedding [6].
LetA={ùëé1,¬∑¬∑¬∑,ùëéùõº}‚äÜV be a randomly selected subset of ùõºnodes. We call them anchor nodes. From each anchor node ùëñ, the
walker starts a random walk [ 5] and jumps to a neighboring node
ùëówith a transition probability (W ùëñùëó) governed by the transition
probability matrix W‚ààR|ùëâ|√ó|ùëâ|. Furthermore, at each step, with
probabilityùëêthe walker jumps to a neighboring node ùëóand returns
to the node ùëñwith 1‚àíùëê. Let¬Æùëüùëñùëócorresponds to the probability of
the random walker starting from node ùëñand reaching node ùëó.
¬Æùëüùëñ=ùëêÀúW¬Æùëüùëñ+(1‚àíùëê)¬Æùëíùëñ (6)
Eq. 6 describes the random walk starting at node ùëñ. In vector¬Æùëíùëñ‚àà
R|ùëâ|√ó1, only theùëñùë°‚Ñéelement (the initial anchor node) is 1, and the
rest are set to zero. We set Wùëñùëó=1
ùëëùëíùëîùëüùëíùëí(ùëó)if edgeùëíùëñùëó‚àà E,0
otherwise. The random walk with restart process is repeated for ùõΩ
iterations, where ùõΩis a hyper-parameter. Here ¬Æùëíùëñ‚ààR|ùëâ|√ó1andùëêis
a scalar.
Based upon the obtained random walk vectors for the set of
anchor nodesA, we embed all nodes ùë¢‚ààV in aùõº-dimensional
feature space:
pos(ùë¢)=[ùëü1ùë¢,ùëü2ùë¢,¬∑¬∑¬∑,ùëüùõºùë¢] (7)
To accommodate both raw feature and positional information
we concatenate the raw node features i.e X[ùë¢]with the positional
embedding pos(ùë¢)for each node ùë¢to obtain the initial embedding
which will act as input to our neural model. Specifically,
emb init(ùë¢)=X[ùë¢] ‚à• pos(ùë¢) (8)
In the above equation, ‚à•represents the concatenation operator.
State. The state space characterizes the state of the system at time
ùë°in terms of the current set of partitions Pùë°. Intuitively the state
should contain information to help our model make a decision to
 
2587NeuroCUT: A Neural Approach for Robust Graph Partitioning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
select the next node and the partition for the node to be assigned.
LetPùë°denote the status of partitions at time ùë°wherein a partition
ùëÉùë°
ùëñis represented by all nodes belonging to ùëñùë°‚Ñépartition. The state
of the system at step ùë°is defined as
ùëÜùë°={ùëÜùë°
1,ùëÜùë°
2,¬∑¬∑¬∑ùëÜùë°
ùëò:ùëÜùë°
ùëñ={emb init(ùë£) ‚àÄùë£‚ààùëÉùë°
ùëñ}} (9)
Here state of each partition is represented by the collection of initial
embedding of nodes in that partition.
3.2 Action: Selection of Nodes andPartitions
Towards finding the best partitioning scheme for the target parti-
tion objective we propose a 2-step action strategy to update the
partitions of nodes. The first phase consists of identifying a node
to update its partition. Instead of arbitrarily picking a node, we
propose to prioritize selecting nodes for which the new assign-
ment is more likely to improve the overall partitioning objective.
In comparison to a strategy that arbitrarily selects nodes, the above
mechanism promises greater improvement in the objective with
less number of iterations. In the second phase, we calculate the
score of each partition Pwith respect to the selected node from the
first phase and then assign it to one of the partitions based upon the
partitioning scores. We discuss both these phases in details below.
3.2.1 Phase 1: Node Selection to Identify Node to Perturb.
LetPART(Pùë°,ùë£)denote the partition of the node ùë£at stepùë°. Our
proposed formulation involves selecting a node ùë£at stepùë°belonging
to partition PART(Pùë°,ùë£)and then assigning it to a new partition.
The newly assigned partition and the current partition of the node
could also be same.
Towards this, we design a heuristic to prioritize selecting nodes
which when placed in a new partition are more likely to improve
the overall objective value. A node ùë£is highly likely to be moved
from its current partition if most of its neighbours are in a different
partition than that of node ùë£. Towards this, we calculate the score
of nodesùë£‚ààV in the graph as the ratio between the maximum
number of neighbors in another partition and the number of neigh-
bors in the same partition as ùë£. Intuitively, if a partition exists in
which the majority of neighboring nodes of a given node ùë£belong,
and yet node ùë£is not included in that partition, then there is a
high probability that node ùë£should be subjected to perturbation.
Specifically the score of node ùë£at stepùë°is defined as:
NodeScoreùë°[ùë£]=
maxùëù‚ààùëÉùë°\PART(ùëÉùë°,ùë£)|ùë¢|ùë¢‚ààN(ùë£)‚àãPART(ùëÉùë°,ùë¢)=ùëù|
|ùë¢|ùë¢‚ààN(ùë£)‚àãPART(ùëÉùë°,ùë¢)=PART(ùëÉùë°,ùë£)|√ó1
ùëëùëíùëîùëüùëíùëí(ùë£)
(10)
For a node of interest ùë£, the numerator computes the maximum
number of neighbors in a different partition than that of ùë£. As
described in Table 1, PART(ùëÉùë°,ùë£)refers to the partition of ùë£at step
ùë°. The expression ùëÉùë°\PART(ùëÉùë°,ùë£)computes all other partitions
except the partition of ùë£. The term|ùë¢|ùë¢‚ààN(ùë£)‚àãPART(ùëÉùë°,ùë¢)=ùëù|
computes the number of neighbors of ùë£in the partition ùëù. The
denominator computes the number of neighbors of node ùë£in the
same partition as ùë£, referred to as PART(ùëÉùë°,ùë£). Further, a node
having a higher degree implies that it has several edges associated
to it. Hence, an incorrect placement of it could contribute to ahigher partitioning value. Therefore, we normalize the scores by
theùëëùëíùëîùëüùëíùëí of the node.
3.2.2 Phase 2: Inductive Method for Partition Selection.
Once a node is selected, the next phase involves choosing the new
partition for the node. Towards this, we design an approach em-
powered by Graph Neural Networks (GNNs) [ 15] which enables
the model to be inductive with respect to size of graph. Further, in-
stead of predicting a fixed-size score vector [ 34,46] for the number
of partitions, our proposed method of computing partition scores
allows the model to be inductive to the number of partitions too.
We discuss both above points in section below.
Message Passing through Graph Neural Network
To capture the interaction between different nodes and their fea-
tures along with the graph topology, we parameterize our policy by
a Graph Neural Network (GNN) [ 15]. GNNs combine node feature
information and the graph structure to learn better representations
via feature propagation and aggregation.
We first initialize the input layer of each node ùë¢‚ààV in graph as
h0ùë¢=emb init(ùë¢)using eq. 8. We perform ùêølayers of message pass-
ing to compute representations of nodes. To generate the embedding
for nodeùë¢at layerùëô+1we perform the following transformation[ 15]:
hùëô+1
ùë¢=Wùëô
1hùëô
ùë¢+Wùëô
2¬∑1
|Nùë¢|‚àëÔ∏Å
ùë¢‚Ä≤‚ààNùë¢hùëô
ùë¢‚Ä≤ (11)
where h(ùëô)
ùë¢is the node embedding in layer ùëô.Wùëô
1andWùëô
2are
trainable weight matrices at layer ùëô.
Followingùêølayers of message passing, the final node represen-
tation of node ùë¢in theùêøùë°‚Ñélayer is denoted by hùêøùë¢‚ààRùëë. Intuitively
hùêøùë¢characterizes ùë¢using a combination of its own features and
features aggregated from its neighborhood.
Scoring Partitions. Recall from eq. 9, each partition at time ùë°is
represented using the nodes belonging to that partition. Building
upon this, we compute the score of each partition ùëù‚ààPùë°with
respect to the node ùë£selected in Phase 1 using all the nodes in ùëù.
In contrast to predicting a fixed-size score vector corresponding
to number of partitions [ 34,46], the proposed design choice makes
the model inductive to the number of partitions. Specifically, the
number of partitions are not directly tied to the output dimensions
of the neural model.
Having obtained the transformed node embeddings through a
Gnn in Eq. 11, we now compute the (unnormalized) score for node
ùë£selected in phase 1 for each partition ùëù‚ààPùë°as follows:
PartScore(ùëù,ùë£)=AGG({MLP(ùúé(‚Ñéùë£|‚Ñéùë¢))
‚àÄùë¢‚ààN(ùë£)‚àãPART(Pùë°,ùë¢)=ùëù})(12)
The above equation concatenates the selected node ùë£‚Äôs embed-
ding with its neighbors ùë¢‚ààN(ùë£)that belong to the partition ùëù
under consideration. In general, the strength of a partition assign-
ment to a node is higher if its neighbors also belong to the same
partition. The above formulation surfaces this strength in the em-
bedding space. The concatenated representation (‚Ñéùë£|‚Ñéùë¢)‚àÄùë¢‚ààN(ùë£)
is passed through an MLP that converts the vector into a score
(scalar). We then apply an aggregation operator (e.g., mean) over
all neighbors of ùë¢belonging toPùë°to get an unnormalized score
for partition ùëù. Hereùúéis an activation function.
 
2588KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
To compute the normalized score at step ùë°is finally calculated
as softmax over the all partitions ùëù‚ààPùë°for the currently selected
nodeùë£. Mathematically, the probability of taking action ùëéùë°=ùëùat
time stepùë°at stateùëÜùë°is defined as:
ùúã((ùëéùë°=ùëù)/ùëÜùë°)=exp(PartScore(ùëù,ùë£))√ç
ùëù‚Ä≤‚ààPùë°exp(PartScore(ùëù‚Ä≤,ùë£))(13)
During the course of trajectory of length ùëá, we sample action
ùëéùë°‚ààP i.e., the assignment of the partition for the node selected in
phase 1 at step ùë°using policy ùúã.
State Transition. After action ùëéùë°is applied at state ùëÜùë°, the state
is updated to ùëÜùë°+1that involves updating the partition set Pùë°+1.
Specifically, if node ùë£belonged to ùëñùë°‚Ñépartition at time ùë°and its
partition has been changed to ùëóin phase 2, then we apply the below
operations in order.
ùëÉùë°+1
ùëñ‚ÜêùëÉùë°
ùëñ\{ùë£}andùëÉùë°+1
ùëó‚ÜêùëÉùë°
ùëó‚à™{ùë£} (14)
3.3 Reward, Training & Inference
Reward. Our aim is to improve the value of the overall partitioning
objective. One way is to define the reward ùëÖùë°at stepùë°‚â•0as the
change in objective value of the partitioning i.e ùëÇùëèùëó(G,Pùë°)at step
ùë°. Specifically,
ùëÖùë°=(ùëÇùëèùëó(G,Pùë°)‚àíùëÇùëèùëó(G,Pùë°+1))
(ùëÇùëèùëó(G,Pùë°)+ùëÇùëèùëó(G,Pùë°+1))¬∑ùúÜ (15)
HereùúÜis a hyperparameter that is used to scale the reward. The
above reward expression incentivizes significant improvements in
the objective function. This is achieved mathematically by consider-
ing both the change and the current value of the objective function.
This design steers the model towards prioritizing substantial im-
provements, especially in low objective function regions, ultimately
guiding it towards the overall minimum.
However, above definition of reward focuses on short-term im-
provements instead of long-term. Hence, to prevent this local greedy
behavior and to capture the combinatorial aspect of the selections,
we use discounted rewards ùê∑ùë°to increase the probability of actions
that lead to higher rewards in the long term [ 42]. The discounted
rewards are computed as the sum of the rewards over a horizon
of actions with varying degrees of importance (short-term and
long-term). Mathematically,
ùê∑ùë°=ùëÖùë°+ùõæùëÖùë°+1+ùõæ2ùëÖùë°+2+...=ùëá‚àíùë°‚àëÔ∏Å
ùëó=0ùõæùëóùëÖùë°+ùëó(16)
whereùëáis the length of the horizon and ùõæ‚àà(0,1]is adiscounting
factor (hyper-parameter) describing how much we favor immediate
rewards over the long-term future rewards.
The above reward mechanism provides flexibility to our frame-
work to be versatile to objectives of different nature, that may
or may not be differentiable. This is an advantage over existing
neural methods [ 34,46] where having a differentiable form of the
partitioning objective is a pre-requisite.
Policy Loss Computation and Parameter Update. Our objec-
tive is to learn parameters of our policy network in such a way
that actions that lead to an overall improvement of the partition-
ing objective are favored more over others. Towards this, we use
REINFORCE gradient estimator [48] to optimize the parameters ofour policy network. Specifically, we wish to maximize the reward
obtained for the horizon of length ùëáwith discounted rewards ùê∑ùë°.
Towards this end, we define a reward function ùêΩ(ùúãùúÉ)as:
ùêΩ(ùúãùúÉ)=Eùëá‚àëÔ∏Å
ùë°=0 ùê∑ùë°
(17)
We, then, optimize ùêΩ(ùúãùúÉ)via policy gradient [42] as follows:
‚àáùêΩ(ùúãùúÉ)="ùëá‚àëÔ∏Å
ùë°=0 ùê∑ùë°‚àáùúÉùëôùëúùëîùúãùúÉ(ùëéùë°/ùëÜùë°)#
(18)
Training and Inference. For a given graph, we optimize the pa-
rameters of the policy network ùúãùúÉforùëásteps. Note that the tra-
jectory length ùëáis not kept very large to avoid the long-horizon
problem. During inference, we compute the initial node embed-
dings, obtain initial partitioning and then run the forward pass of
our policy to improve the partitioning objective over time.
3.4 Time Complexity
The time complexity of NeuroCUT during inference is
O (ùõº√óùõΩ√ó|E|)+(|E|+ ùëò)√óùëá‚Ä≤. Hereùõºis the number of anchor
nodes,ùõΩis number of random walk iterations, ùëòis the number
of partitions and ùëá‚Ä≤is the number of iterations during inference.
Typicallyùõº,ùõΩandùëòare<<|V|andùëá‚Ä≤=ùëú(|V|) . Further, for
sparse graphs|E|=O(|V|) . Hence time complexity of NeuroCUT
isùëú(|V|2). For detailed derivation please see Appendix A.2.
4 EXPERIMENTS
In this section, we demonstrate the efficacy of NeuroCUT against
state-of-the-art methods and establish that:
‚Ä¢Efficacy and Robustness: NeuroCUT produces the best results
over diverse partitioning objective functions. This establishes the
robustness of NeuroCUT.
‚Ä¢Inductivity: As one of the major strengths, unlike existing neural
models such as GAP [34],DMon [46] and MinCutPool [4], our
method NeuroCUT is inductive on the number of partitions and
consequently can generalize to unseen number of partitions.
Code: Our code base is accessible at https://github.com/idea-iitd/
NeuroCut.
4.1 Experimental Setup
4.1.1 Datasets: We use four real datasets for our experiments. They
are described below and their statistics are present in Table 2. For
all datasets we use their largest connected component.
‚Ä¢Cora andCiteseer [40]: These are citation networks where
nodes correspond to individual papers and edges represent cita-
tions between papers. The node features are extracted using a
bag-of-words approach applied to paper abstracts.
‚Ä¢Harbin [27]: This is a road network extracted from Harbin city,
China. The nodes correspond to road intersections and node
features represent latitude and longitude of a road intersection.
‚Ä¢Actor [36]: This dataset is based upon Wikipedia data where
each node in the graph corresponds to an actor, and the edge be-
tween two nodes denotes co-occurrence on the same Wikipedia
page. Node features correspond to keywords in Wikipedia pages.
This is a heterophilous dataset [ 49] where nodes tend to connect
to other nodes that are dissimilar.
 
2589NeuroCUT: A Neural Approach for Robust Graph Partitioning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
‚Ä¢Facebook [33]: In social network analysis, a ùëò-way cut can de-
lineate communities of users with minimal inter-community
connections. This partitioning unveils hidden social circles and
provides insights into how different groups interact within the
larger network. The Facebook dataset, used for community de-
tection, does not include node features. In this dataset, nodes
represent users, and edges represent friendships.
‚Ä¢Stochastic Block Models (SBM) [1]: SBMs are synthetic net-
works and produce graphs with communities, where subsets of
nodes are characterized by specific edge densities within the
community. This allows us to validate our method by comparing
the identified partitions with the ground truth communities. This
dataset does not include node features.
Table 2: Dataset Statistics
Dataset|
V| |E| #Features Average
DegreeClustering
CoefficientDegree Assor-
tativity
Cora
2485 5069 1433 4.07 0.23 -0.07
CiteSeer 2120 3679 3703 3.47 0.169 0.0075
Harbin 6598 10492 2 3.18 0.036 0.22
Actor 6198 14879 931 4.82 0.05 -0.048
Facebook 1034 26749 - 51.7 0.526 0.431
SBM 500 5150 - 20.6 0.18 -0.0059
4.1.2 Partitioning objectives: We evaluate our method on a diverse
set of four partitioning objectives described in Section 2, namely
Normalized Cut, Balanced Cut, k-MinCut, and Sparsest Cut. In addi-
tion to evaluating on diverse objectives, we also choose a diverse
number of partitions for evaluation, specifically, ùëò=2,5and10.
4.1.3 Baselines: We compare our proposed method with both neu-
ralas well as non-neural methods.
Neural baselines: We compare with DMon [46], GAP [34], Min-
CutPool [4] and Ortho [46].DMon is the state-of-the-art neural
attributed-graph clustering method. GAP is optimized for balanced
normalized cuts with an end-to-end framework with a differen-
tiable loss function which is a continuous relaxation version of
normalized cut. MinCutPool optimizes for normalized cut and
uses an additional orthogonality regularizer. Ortho is the orthogo-
nality regularizer described in DMon andMinCutPool. We also
compare with DRL [ 12] which solves for Normalized cut at ùëò=2
in Appendix A.5
Non-neural baselines: Following the settings of DMon [46], we
compare with K-means clustering applied on raw node features. We
also compare with standard graph clustering methods hMetis [ 20]
and Spectral clustering [35] in App Sec. A.4.
4.1.4 Other settings: We run all our experiments on an Ubuntu
20.04 system running on Intel Xeon 6248 processor with 96 cores
and 1 NVIDIA A100 GPU with 40GB memory for our experiments.
ForNeuroCUT we used GraphSage[ 15] as our GNN with number
of layersùêø=2, learning rate as 0.0001, hidden size = 32. We used
Adam optimizer for training the parameters of our policy network
ùúãùúÉ. For computing discounted reward in RL, we use discount factor
ùõæ=0.99. We set the length of trajectory ùëáduring training as 2.
At time step ùë°, the rewards are computed from time ùë°toùë°+ùëáand
parameters of the policy ùúãùúÉare updated. The default number of
anchor nodes for computing positional embeddings is set to 35. We
setùõΩ=100andùëê=0.85for Eq. 6. We set scaling factor ùúÜ=100in
eq. 15.Table 3: Results on Normalized Cut. Our model NeuroCUT
produces the best (lower is better) performance across all
datasets and the number of partitions ùëò.
Dataset Metho
d ùíå=2ùíå=5ùíå=10
CoraK
-means 0.65 3.26 7.44
MinCutPool 0.12 0.61 1.65
DMon 0.57 3.07 7.40
Ortho 0.80 1.88 4.06
GAP 0.10 0.68 -
NeuroCUT 0.02 0.33 0.92
CiteSe
erK-means 0.30 2.35 5.21
MinCutPool 0.10 0.38 1.04
DMon 0.33 2.71 6.84
Ortho 0.28 1.51 3.25
GAP 0.12 - -
NeuroCUT 0.02 0.20 0.44
HarbinK
-means 0.56 2.34 5.40
MinCutPool - - -
DMon 0.98 3.25 -
Ortho - - -
GAP 0.25 - -
NeuroCUT 0.01 0.07 0.28
A
ctorK-means 0.99 4.00 8.98
MinCutPool 0.55 1.97 4.73
DMon 0.77 3.46 8.08
Ortho 1.05 3.98 8.90
GAP 0.20 - -
NeuroCUT 0.17 0.99 4.66
Table 4: Results on Sparsest Cut. Our model NeuroCUT
produces the best (lower is better) performance across all
datasets and number of partitions ùëò.
Dataset Metho
d ùíå=2ùíå=5ùíå=10
CoraK
-means 3.41 14.90 32.52
MinCutPool 0.52 2.44 6.68
DMon 0.45 1.89 5.82
Ortho 2.73 7.06 15.70
GAP 0.41 2.80 -
NeuroCUT 0.13 1.46 3.03
CiteSe
erK-means 1.53 13.70 22.20
MinCutPool 0.31 1.32 3.62
DMon 0.35 1.21 3.62
Ortho 0.88 4.15 10.60
GAP 0.60 - -
NeuroCUT 0.11 0.49 1.19
HarbinK
-means 1.91 7.44 17.03
MinCutPool - - -
DMon 2.01 - -
Ortho - - -
GAP 1.56 - -
NeuroCUT 0.06 0.23 0.82
A
ctorK-means 5.43 19.40 40.34
MinCutPool 2.44 8.51 19.81
DMon 1.71 9.50 21.01
Ortho 3.42 16.55 40.4
GAP 1.35 - -
NeuroCUT 0.65 2.04 2.88
4.2 Results on Transductive Setting
In the transductive setting, we compare our proposed method Neu-
roCUT against the mentioned baselines, where the neural models
 
2590KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
Table 5: Results on k-MinCut. In most of the cases, our model
NeuroCUT produces the best (lower is better) performance
across all datasets and the number of partitions ùëò. Values less
than 10‚àí2are approximated to 0.
Dataset Metho
d ùíå=2ùíå=5ùíå=10
CoraK
-means 0.32 0.34 0.61
MinCutPool 0.06 0.12 0.17
DMon 0.05 0.09 0.14
Ortho 0.33 0.34 0.38
GAP 0.05 0.10 0.11
NeuroCUT‚àº0.0‚àº0.0 0.06
CiteSe
erK-means 0.12 0.27 0.43
MinCutPool 0.04 0.08 0.10
DMon 0.05 0.07 0.12
Ortho 0.13 0.47 0.30
GAP 0.05 0.08 0.10
NeuroCUT 0.01 0.02 0.03
HarbinK
-means 0.28 0.46 0.54
MinCutPool 0.0 0.0 0.0
DMon 0.10 0.01 ‚àº0.0
Ortho 0.0 0.0 0.0
GAP 0.01 0.01 0.01
NeuroCUT‚àº0.0 0.01 0.03
A
ctorK-means 0.48 0.54 0.80
MinCutPool 0.25 0.35 0.42
DMon 0.17 0.39 0.44
Ortho 0.35 0.65 0.83
GAP 0.09 0.12 0.26
NeuroCUT‚àº0.0‚àº0.0‚àº0.0
are trained and tested with the same number of partitions. Ta-
bles 3-6 present the results for different partitioning objectives
across all datasets and methods. For the objectives under consid-
eration, a smaller value depicts better performance. NeuroCUT
demonstrates superior performance compared to both neural and
non-neural baselines across four distinct partitioning objectives,
highlighting its robustness. Further, we would also like to point
out that in many cases, existing baselines produce ‚Äúnan‚Äù values
which are represented by ‚Äú -‚Äù in the result tables. This is due to the
reason that every partition is not assigned atleast one node which
leads to the situation where denominator becomes 0innormalized,
balanced andsparsest cut objectives as defined in Sec. 2.
Our method NeuroCUT incorporates dependency during in-
ference and this leads to robust performance. Specifically, as the
architecture is auto-regressive, it takes into account the current
state before moving ahead as opposed to a single shot pass in meth-
ods such as GAP [34]. Further, unlike other methods, NeuroCUT
also incorporates positional information in the form of Lipschitz
embedding to better contextualize global node positional informa-
tion. We also observe that the non-neural method K-means fails to
perform on all objective functions since its objective is not aligned
with the main objectives under consideration.
4.3 Results on Inductivity to Partition Count
As detailed in Section 3.2.2, the decoupling of parameter size and
the number of partitions allows NeuroCUT to generalize effectively
to an unseen number of partitions. In this section, we empirically
analyze the generalization performance of NeuroCUT to an un-
known number of partitions. We compare it with the non-neural
baseline K-means, as the neural methods like GAP, DMon, Or-
tho, and MinCutPool cannot be employed to infer on an unseenTable 6: Results on Balanced Cut. In most cases, our model
NeuroCUT produces the best (lower is better) performance
across all datasets and number of partitions ùëò.
Dataset Metho
d ùíå=2ùíå=5ùíå=10
CoraK
-means 0.68 3.90 7.44
MinCutPool 0.13 0.60 1.62
DMon 0.11 0.48 1.47
Ortho 0.80 1.89 4.06
GAP 0.10 0.74 -
NeuroCUT 0.45 0.64 1.08
CiteSe
erK-means 0.42 2.64 5.30
MinCutPool 0.09 0.38 1.04
DMon 0.10 0.37 1.1
Ortho 0.28 1.51 3.32
GAP 0.23 - -
NeuroCUT 0.07 0.24 0.60
HarbinK
-means 0.56 2.37 5.41
MinCutPool - - -
DMon 1.30 - -
Ortho - - -
GAP 0.72 - -
NeuroCUT 0.21 0.11 0.27
A
ctorK-means 1.00 4.06 9.08
MinCutPool 0.55 1.96 4.71
DMon 0.34 2.01 4.80
Ortho 1.05 3.90 8.91
GAP 0.25 - -
NeuroCUT 0.59 1.69 4.42
Table 7: Inductivity to unseen partition count. Here we set
the target number of partitions ùëò=10. I stands for inductive
and T for transductive setting. In this table, the transductive
version of NeuroCUT trained on ùëò=10serves as a point of
reference when assessing the quality of the inductive version
ofNeuroCUT.
Dataset
Method Normalized Sparsest k-MinCut Balanced
CoraK
-means 7.44 32.52 0.61 7.53
NeuroCUT-T 0.92 3.03 0.06 1.08
NeuroCUT-I 1.18 5.04 0.02 2.03
CiteSe
erK-means 5.21 22.20 0.43 5.30
NeuroCUT-T 0.44 1.19 0.03 0.60
NeuroCUT-I 0.63 1.62 0.03 0.62
HarbinK
-means 5.40 17.03 0.54 5.41
NeuroCUT-T 0.28 0.82 0.03 0.27
NeuroCUT-I 0.27 0.87 0.03 0.29
number of partitions. In Table 7, we present the results on induc-
tivity. Specifically, we trained NeuroCUT on different partition
sizes (ùëò=5andùëò=8) and tested it on an unseen partition size
ùëò=10. The inductive version, referred to as NeuroCUT-I2, ob-
tains high-quality results on different datasets. Note that while the
non-neural method such as K-means needs to be re-run for the
unseen partitions, NeuroCUT only needs to perform forward pass
to produce the results on an unseen partition size.
4.4 Ablation Studies
Initial Warm Start vs Final Cut Values. NeuroCUT takes a
warm start by partitioning nodes based on clustering over their raw
node features and Lipschitz embeddings, which are subsequently
2For clarity purposes, in this experiment we use NeuroCUT-T to refer to the trans-
ductive setting whose results are presented in Sec. 4.2
 
2591NeuroCUT: A Neural Approach for Robust Graph Partitioning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Cora CiteSeer Harbin012345Normalized CutInitial
Final
(a) Normalized Cut
Cora CiteSeer Harbin024681012 Sparsest CutInitial
Final (b) Sparsest Cut
Cora CiteSeer Harbin01234Balanced CutInitial
Final (c) Balanced Cut
Cora CiteSeer Harbin0.000.050.100.150.200.250.300.350.40k-MincutInitial
Final (d) k-MinCut
Figure 2: Results on the initial warm start and the final cut values obtained by NeuroCUT atùëò=10. It shows that our neural
model NeuroCUT (Final) performs more accurate node and partition selection to optimize the objective function. Subsequently,
there is a significant difference between the initial and final cut values.
Cora CiteSeer Harbin01020304050% GainNormalized
Balanced
k-MinCut
Sparsest-Cut
Figure 3: Node Selection in Phase 1 with Heuristic vs Random:
Relative % improvement (gain) in cut values obtained by
NeuroCUT when using different node selection strategies for
ùëò=5. In most cases, our heuristic finds significantly better
cuts than a random node selection procedure.
fine-tuned auto-regressively through the phase 1 and phase 2 of
NeuroCUT. In this section, we measure, how much the partitioning
objective has improved since the initial clustering? Figure 2 sheds
light on this question. Specifically, it shows the difference between
the initial cut value after clustering and the final cut value from
the partitions produced by our method. We note that there is a
significant difference between the initial and final cut values, which
essentially shows the effectiveness of NeuroCUT.
Impact of Node Selection Procedures. In this section, we
explore the effectiveness of our proposed node selection heuristic
in enhancing overall quality. Specifically, we measure the relative
performance gain(in %) obtained by NeuroCUT when using the
node selection heuristic as proposed in Section 3.2.1 over a random
node selection strategy. In our experiments for the random node
selection strategy, we selected the best performing run across multi-
ple seeds. Figure 3 shows the percentage gain for three datasets. We
observe that a simpler node selection where we select all the nodes
one by one in a random order, yields substantially inferior results
in comparison to the heuristic proposed by us. This suggests that
the proposed sophisticated node selection strategy plays a crucial
role in optimizing the overall performance of NeuroCUT. In Ap-
pendix A.7, we conduct a more detailed analysis of the observations
presented in Figure 3, with particular attention to the variation in
gains related to the Balanced Cut objective.
Impact of Clustering Initialization in Warm-start Phase: To
understand the importance of different initializations in the warm-
up phase(¬ß 3.1), in Appendix A.6 we perform an ablation study usingthree different initialization schemes, namely K-means(default),
density-based clustering DBSCAN [ 10], and Random initialization.
4.5 Impact of Cluster Strength on Performance
In Section A.8 in appendix, we analyze the impact of cluster strength
in networks on the performance of different methods.
5 CONCLUSION
In this work, we study the problem of graph partitioning with node
features. Existing neural methods for addressing this problem re-
quire the target objective to be differentiable and necessitate prior
knowledge of the number of partitions. In this paper, we intro-
duced NeuroCUT, a framework to effectively address the graph
partitioning problem with node features. NeuroCUT tackles these
challenges using a reinforcement learning-based approach that can
adapt to any target objective function. Further, attributed to its
decoupled parameter space and partition count, NeuroCUT can
generalize to an unseen number of partitions. The efficacy of our
approach is empirically validated through an extensive evaluation
on four datasets, four graph partitioning objectives and diverse par-
tition counts. Notably, our method shows significant performance
gains when compared to the state-of-the-art techniques, proving
its competence in both inductive and transductive settings.
Limitations and Future Directions: Achieving a sub-quadratic
computational complexity with an inductive neural method for
attributed graph partitioning is an open challenge. In NeuroCUT,
node selection and perturbation are performed in a sequential fash-
ion. One direction to improve the efficiency of NeuroCUT could
be batch processing the selection and perturbation of multiple in-
dependent nodes simultaneously. Another interesting direction
could be designing explanations of the neural methods‚Äîsuch as
NeuroCUT‚Äîfor graph combinatorial problems [19].
6 ACKNOWLEDGEMENT
We thank Kartik Sharma for the helpful suggestions. Rishi Shah
acknowledges that the funding for the conference was provided by
the CMU GSA Conference Funding. Sahil Manchanda acknowledges
financial support by GP Goyal grant of IIT Delhi and Qualcomm
Innovation Fellowship India.
 
2592KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
REFERENCES
[1]Emmanuel Abbe. 2017. Community detection and stochastic block models: recent
developments. The Journal of Machine Learning Research 18, 1 (2017), 6446‚Äì6531.
[2]Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning
using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS‚Äô06). IEEE, 475‚Äì486.
[3]Aritra Bhowmick, Mert Kosan, Zexi Huang, Ambuj Singh, and Sourav Medya.
2024. DGCLUSTER: A Neural Framework for Attributed Graph Clustering via
Modularity Maximization. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 38. 11069‚Äì11077.
[4]Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. 2020. Spectral clus-
tering with graph neural networks for graph pooling. In International conference
on machine learning. PMLR, 874‚Äì883.
[5]Monica Bianchini, Marco Gori, and Franco Scarselli. 2005. Inside pagerank. ACM
Transactions on Internet Technology (TOIT) 5, 1 (2005), 92‚Äì128.
[6]Jean Bourgain. 1985. On Lipschitz embedding of finite metric spaces in Hilbert
space. Israel Journal of Mathematics 52 (1985), 46‚Äì52.
[7]Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yuval Rabani, and D Sivakumar.
2006. On the hardness of approximating multicut and sparsest-cut. computational
complexity 15 (2006), 94‚Äì114.
[8]Fan Chung. 2007. Four proofs for the Cheeger inequality and graph partition
algorithms. In Proceedings of ICCM, Vol. 2. Citeseer, 378.
[9]Chi Thang Duong, Thanh Tam Nguyen, Trung-Dung Hoang, Hongzhi Yin,
Matthias Weidlich, and Quoc Viet Hung Nguyen. 2023. Deep MinCut: Learning
Node Embeddings by Detecting Communities. Pattern Recognition 134 (2023),
109126.
[10] Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, Xiaowei Xu, et al .1996. A density-
based algorithm for discovering clusters in large spatial databases with noise. In
kdd, Vol. 96. 226‚Äì231.
[11] Maxime Gasse, Didier Ch√©telat, Nicola Ferroni, Laurent Charlin, and Andrea
Lodi. 2019. Exact combinatorial optimization with graph convolutional neural
networks. Advances in neural information processing systems 32 (2019).
[12] Alice Gatti, Zhixiong Hu, Tess Smidt, Esmond G Ng, and Pieter Ghysels. 2022.
Graph partitioning and sparse matrix ordering using reinforcement learning and
graph neural networks. The Journal of Machine Learning Research 23, 1 (2022),
13675‚Äì13702.
[13] Ralph E Gomory and Tien Chung Hu. 1961. Multi-terminal network flows. J.
Soc. Indust. Appl. Math. 9, 4 (1961), 551‚Äì570.
[14] Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu. 2020. GraphGen: A Scalable
Approach to Domain-agnostic Labeled Graph Generation. In Proceedings of The
Web Conference 2020 (Taipei, Taiwan) (WWW ‚Äô20) . Association for Computing
Machinery, New York, NY, USA, 1253‚Äì1263. https://doi.org/10.1145/3366423.
3380201
[15] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[16] Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. 2019. An efficient graph
convolutional network technique for the travelling salesman problem. arXiv
preprint arXiv:1906.01227 (2019).
[17] Steffen Jung and Margret Keuper. 2022. Learning to solve minimum cost multicuts
efficiently using edge-weighted graph convolutional neural networks. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases.
Springer, 485‚Äì501.
[18] Andrew B Kahng, Jens Lienig, Igor L Markov, and Jin Hu. 2011. VLSI physical
design: from graph partitioning to timing closure. Vol. 312. Springer.
[19] Jaykumar Kakkad, Jaspal Jannu, Kartik Sharma, Charu Aggarwal, and Sourav
Medya. 2023. A survey on explainability of graph neural networks. arXiv preprint
arXiv:2306.01958 (2023).
[20] George Karypis, Rajat Aggarwal, Vipin Kumar, and Shashi Shekhar. 1997. Multi-
level hypergraph partitioning: Application in VLSI domain. In Proceedings of the
34th annual Design Automation Conference. 526‚Äì529.
[21] George Karypis and Vipin Kumar. 1998. Multilevelk-way partitioning scheme
for irregular graphs. Journal of Parallel and Distributed computing 48, 1 (1998),
96‚Äì129.
[22] George Karypis and Vipin Kumar. 1999. Multilevel k-way hypergraph partition-
ing. In Proceedings of the 36th annual ACM/IEEE design automation conference.
343‚Äì348.
[23] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. 2017. Learn-
ing combinatorial optimization algorithms over graphs. Advances in neural
information processing systems 30 (2017).
[24] Wouter Kool, Herke Van Hoof, and Max Welling. 2018. Attention, learn to solve
routing problems! arXiv preprint arXiv:1803.08475 (2018).
[25] Jure Leskovec and Julian Mcauley. 2012. Learning to discover social circles in
ego networks. Advances in neural information processing systems 25 (2012).
[26] Hao Li, Gary W Rosenwald, Juhwan Jung, and Chen-Ching Liu. 2005. Strategic
power infrastructure defense. Proc. IEEE 93, 5 (2005), 918‚Äì933.[27] Xiucheng Li, Gao Cong, Aixin Sun, and Yun Cheng. 2019. Learning travel time
distributions with deep generative model. In The World Wide Web Conference.
1017‚Äì1027.
[28] James MacQueen et al .1967. Some methods for classification and analysis of
multivariate observations. In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability, Vol. 1. Oakland, CA, USA, 281‚Äì297.
[29] Sahil Manchanda, Shubham Gupta, Sayan Ranu, and Srikanta J Bedathur. 2024.
Generative modeling of labeled graphs under data scarcity. In Learning on Graphs
Conference. PMLR, 32‚Äì1.
[30] Sahil Manchanda, Sofia Michel, Darko Drakulic, and Jean-Marc Andreoli. 2022.
On the generalization of neural combinatorial optimization heuristics. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases.
Springer, 426‚Äì442.
[31] Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu,
and Ambuj Singh. 2020. Gcomb: Learning budget-constrained combinatorial
algorithms over billion-sized graphs. Advances in Neural Information Processing
Systems 33 (2020), 20000‚Äì20011.
[32] Sahil Manchanda and Sayan Ranu. 2023. LiMIP: lifelong learning to solve mixed
integer programs. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 37. 9047‚Äì9054.
[33] Julian McAuley and Jure Leskovec. 2012. Learning to Discover Social Cir-
cles in Ego Networks. In Neural Information Processing Systems. https://api.
semanticscholar.org/CorpusID:2820103
[34] Azade Nazi, Will Hang, Anna Goldie, Sujith Ravi, and Azalia Mirhoseini. 2019.
Gap: Generalizable approximate graph partitioning framework. arXiv preprint
arXiv:1903.00614 (2019).
[35] Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis
and an algorithm. Advances in neural information processing systems 14 (2001).
[36] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila
Prokhorenkova. 2023. A critical look at the evaluation of GNNs under heterophily:
are we really making progress? arXiv preprint arXiv:2302.11640 (2023).
[37] Josep M Pujol, Vijay Erramilli, and Pablo Rodriguez. 2009. Divide and conquer:
Partitioning online social networks. arXiv preprint arXiv:0905.4918 (2009).
[38] Rishabh Ranjan, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy,
Yogish Sabharwal, and Sayan Ranu. 2022. Greed: A neural framework for learning
graph distance functions. Advances in Neural Information Processing Systems 35
(2022), 22518‚Äì22530.
[39] Huzur Saran and Vijay V Vazirani. 1995. Finding k cuts within twice the optimal.
SIAM J. Comput. 24, 1 (1995), 101‚Äì108.
[40] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93‚Äì93.
[41] Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation.
IEEE Transactions on pattern analysis and machine intelligence 22, 8 (2000), 888‚Äì
905.
[42] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
duction. MIT press.
[43] Amirmahdi Tafreshian and Neda Masoud. 2020. Trip-based graph partitioning in
dynamic ridesharing. Transportation Research Part C: Emerging Technologies 114
(2020), 532‚Äì553.
[44] Hao Tian, Sourav Medya, and Wei Ye. 2024. COMBHelper: A Neural Approach
to Reduce Search Space for Graph Combinatorial Problems. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 38. 20812‚Äì20820.
[45] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel M√ºller. 2023.
Graph clustering with graph neural networks. Journal of Machine Learning
Research 24, 127 (2023), 1‚Äì21.
[46] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel M√ºller. 2023.
Graph clustering with graph neural networks. Journal of Machine Learning
Research 24, 127 (2023), 1‚Äì21.
[47] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017.
Mgae: Marginalized graph autoencoder for graph clustering. In Proceedings of the
2017 ACM on Conference on Information and Knowledge Management. 889‚Äì898.
[48] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8 (1992), 229‚Äì256.
[49] Xin Zheng, Yi Wang, Yixin Liu, Ming Li, Miao Zhang, Di Jin, Philip S Yu, and
Shirui Pan. 2022. Graph neural networks for graphs with heterophily: A survey.
arXiv preprint arXiv:2202.07082 (2022).
A APPENDIX
A.1 Clustering for Initialization
As discussed in sec 3.1 in main paper, we first cluster the nodes
of the graph into ùëòclusters where ùëòis the number of partitions.
Towards this, we apply K-means algorithm [ 28] on the nodes of
the graph where a node is represented by its raw node features and
 
2593NeuroCUT: A Neural Approach for Robust Graph Partitioning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 8: Time complexity of different methods.
Method Complexity Comments
DMonO(ùëë2√ó|V|+|E|) Per iteration complexity
MinCutPoolO(ùëë2√ó|V|+|E|) Per iteration complexity
OrthoO(|V|√óùëò2) Per iteration complexity
GAPO(|V|√óùëò2+|V|2)Per iteration complexity
SpectralO(|V|3) Inference complexity
NeuroCUT ùëú(|V|2) Inference complexity
positional representation i.e emb init(ùë¢)‚àÄùë¢‚ààV based upon eq. 8.
Further, we used ùêøinfnorm as the distance metric for clustering.
A.2 Time Complexity Analysis of NeuroCUT
(1)First the positional embeddings for all nodes in the graph are
computed. This involves running RWR for ùõºanchor nodes for ùõΩ
iterations (Eq. 6). This takes O(ùõº√óùõΩ√ó|E|) time. (2)Next, GNN is
called to compute embeddings of node. In each layer of GNN a node
ùë£‚ààV aggregates message from ùëëneighbors where ùëëis the average
degree of a node. This takes O(V) time. This operation is repeat for
ùêølayers. Since ùêøis typically 1 or 2 hence we ignore this factor. (3)
The node selection algorithm is used that computes score for each
node based upon its neighborhood using eq. 10. This consumes
O(|V|√óùëë)time. (4)Finally for the selected node, its partition
has to be determined using eq. 12 and 13. This takes O(ùëë+ùëò)
time, as we consider only the neighbors of the selected node to
compute partition score. Steps 2-4 are repeated for ùëá‚Ä≤iterations.
Hence overall running time is O (ùõº√óùõΩ√ó|E|)+(|V|√ó ùëë+ùëò)√óùëá‚Ä≤.
Typicallyùõº,ùõΩandùëòare<<|V|andùëá‚Ä≤=ùëú(|V|) . Since|V|√óùëë‚âà
|E|, hence complexity is ùëú(|E|√ó|V|Further, for sparse graphs
|E|=O(|V|) . Hence time complexity of NeuroCUT isùëú(|V|2).
A.3 Time Complexity Comparison
Table 8 presents the complexities of NeuroCUT and other promi-
nent neural and non-neural baselines algorithms. In the table, |V|,
|E|,ùëò,ùëëare the number of nodes, edges, partitions, and average
node degree respectively. While some neural algorithms exhibit
faster complexity, they require separate training for each partition
size (ùëò). In contrast, NeuroCUT, once trained, can generalize to any
value. Consequently, for practical workloads, NeuroCUT presents a
more scalable option in terms of computation overhead and storage
(one model versus separate models for each).
The quadratic time complexity of NeuroCUT may pose chal-
lenges for very large graphs. However, this complexity remains
faster than spectral clustering, a widely used graph partitioning al-
gorithm. Also, for the baseline neural methods(DMon, MinCutPool,
Ortho and GAP), the time complexity provided is per iteration. The
number of iterations often ranges between 1000 to 2000 and there
is no clear understanding of how it varies as a function of the graph
(like density, diameter, etc.)
A.4 Comparison: Non-neural Baselines
We compare the performance of our method against graph based
clustering algorithms hMETIS and Spectral Clustering and par-
tition algorithm Gomory-Hu Tree [ 13]. Since these methods areTable 9: Performance of different methods on Facebook and
SBM dataset. Lower values are better.‚Ä≤‚àí‚Ä≤denotes nan.
Dataset Metho
dMetrics
Normalized Sparsest Balanced ùëò-MinCut
Faceb
ookNeuroCUT 0.257 4.121 0.972 0.015
hMETIS 1.15 52.319 1.115 0.200
Spectral 1.67 4.72 2.459 0.003
Gomory-Hu Tree 4.00 5.00 4.792 ‚àº0
MinCutPool - - - 0.023
DMon - - - ‚àº0
Ortho - - - 0.05
GAP - - - 0.025
SBMNeur
oCUT 0.191 3.939 0.191 0.038
hMETIS 0.191 3.939 0.191 0.038
Spectral 0.191 3.939 0.191 0.038
Gomory-Hu Tree 4.003 48.75 4.787 0.0075
MinCutPool - - - 0.0
DMon - - - 0.0
Ortho - - - ‚àº0
GAP - - - 0.035
Table 10: Gatti et al. on Normalized Cut at ùëò=2.
Metho
d‚Üí
Dataset‚ÜìCora
CiteSeer Harbin Actor SBM Facebook
Neur
oCUT 0.02 0.02 0.01 0.17 0.191 0.257
Gatti et al. 0.355 0.29 0.13 1.00 0.72 0.92
not compatible with datasets having raw node features, we com-
pare with two datasets namely Facebook [ 25] and Stochastic Block
Model(SBM) [ 1] which don‚Äôt have node features for this compar-
ision. Table 9 compares the performance of NeuroCUT against
hMETIS and Spectral Clustering. The details of these datasets are
presented in Table 2. In addition to non-neural methods, we also
show the performance of neural methods MinCutPool, DMon, Or-
tho and GAP on these datasets.
The performance of NeuroCUT is better than non-neural meth-
ods hMETIS, Spectral and Gomory-Hu Tree in most of the cases
on Facebook dataset. Further, on SBM dataset, it matches the per-
formance of hMETIS and Spectral clustering. The SBM dataset
has a clear community structure, making it an easy instance for
partitioning algorithms, resulting in similar performance across
different methods. We would also like to highlight that in the case of
ùëò‚àíùëöùëñùëõùëêùë¢ùë° , Gomory-Hu Tree algorithm generated trivial partitions
for these datasets where almost all nodes were assigned the same
partition. The other neural methods such as MinCutPool, DMon,
Ortho, and GAP fail to produce a valid solution in most of the
cases. This is possibly because these baselines are not robust to per-
form on datasets without raw node features. Overall, NeuroCUT
outperforms the baselines on diverse objectives and datasets.
A.5 Comparison with Gatti et al. [12]
In Table 10, we compare the performance of our method against DRL
method proposed by Gatti et al. [ 12] which solves the normalized
cut problem for the case where the number of partitions is exactly
two. We observe that NeuroCUT outperforms DRL on all datasets.
A.6 Impact of Clustering Initialization
To understand the importance of different initializations in the
warm-up phase, we perform an ablation study on 2datasets namely
 
2594KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, & Sayan Ranu
Table 11: Impact of Clustering initialization in Warm-up
Phase of NeuroCUT in the normalized cut objective at ùëò=5.
Dataset K-means Random DBSCAN
Cora 0.33 0.79 0.55
CiteSeer 0.20 0.51 0.70
Table 12: Statistics for Normalized Cut (3 runs) for ùëò=5
Dataset
Our Heuristic Random Selection Random Selection % Gain
Avg Best (from best)
Cora
0.33¬±0.0024 0.461 ¬±0.0164 0.445 35
Citeseer 0.20¬±0.00961 0.276 ¬±0.0177 0.258 29
Harbin 0.073¬±0.0027 0.097¬±0.0131 0.086 18
Table 13: Statistics for Balanced Cut (across 3 runs) for ùëò=5
Dataset
Our Heuristic Random Selection Random Selection % Gain
Avg Best (from best)
Cora
0.642¬±0.0065 0.753¬±0.0574 0.672 2
Citeseer 0.248¬±0.0049 0.307 ¬±0.039 0.252 0.5
Harbin 0.1087¬±0.0010 0.1313 ¬±0.011 0.1293 19
Cora and CiteSeer using 3 different initialization namely K-means,
density-based clustering DBSCAN [10], and Random initialization.
In Table 11 we observe the performance on normalized cut at ùëò=5.
We observe that K-means performs the best in this experiment.
The improvement observed when using K-means orDBSCAN over
Random shows that a good initialization i.e. warm-up does help in
improving quality of partitions. In DBSCAN we set ùëíùëùùë†=0.9and
ùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë† =2. It is worth noting that DBSCAN‚Äôs performance
can vary based on the parameter selection. Nonetheless, the primary
goal of this experiment is to demonstrate the advantageous role of a
good initialization, specifically in contrast to random initialization.
Table 14: Number of Nodes Re-assigned after Initial Assign-
ment by NeuroCUT on the Balanced Cut objective.
Dataset
Nodes Perturbed Total Nodes Percentage
Cora
791 2495 31.7
Citeseer 413 2120 19.4
Harbin 187 6598 2.8
A.7 Impact of Node Selection Procedures
In Fig. 3 in main paper, random selection performs well only in
Balanced Cuts. The objective in Balanced Cuts (Eq. 3.) is not only a
function of the combined weight of cut edges, but also balancing the
number of nodes across partitions. This additional node-balancing
term is not present in the objective functions of the other cut defi-
nitions. Due to this reduced importance of optimizing the cut value,
random does well, since even when an incorrect node is selected, it
can still be utilized to keep the partition sizes balanced. Moreover,
we find that when the warm-up assignment is less accurate and
the percentage of nodes re-assigned by NeuroCUT to a different
partition is high, a random selection of nodes is closer in efficacy
to our heuristic selection. In contrast, in Harbin, the percentage
of nodes perturbed to a new partition is significantly smaller. In
this scenario, the probability that random selection will select thissmall subset is lower and hence amplifying the effectiveness gap
between random and heuristic selection.
A.8 Impact of Cluster Strength on Performance
To understand the impact of community structure on performance
ofNeuroCUT, we generate Stochastic Model Block(SBM) graphs
with different intra cluster strength. In Table 15, we observe the
performance of different methods on the normalized cut objective
atùëò=5. The baseline methods hMETIS and Spectral Clustering per-
form worse when the strength within communities is low indicated
by Intra Cluster Edge Probability and Clustering Coefficient values.
Although NeuroCUT outperforms or matches existing methods
in all cases, however, the gap between baselines and NeuroCUT
increases significantly when the community structure is not strong.
Further, the neural baselines DMon, MinCutPool and Ortho gener-
ated ‚Äònan‚Äô values for this experiment.
Table 15: Performance on SBM Dataset
SBM
Dataset Statistics Normalized Cut
Intra
Cluster
Edge
ProbabilityClustering
Coefficient|V| |E| NeuroCUT hMETIS Spectral
Clustering
1
0.005 0.003 495 555 0.3089 0.331
0.379
2 0.01 0.0029 525 661 0.3434 0.5889
0.4259
3 0.2 0.184 500 5150 0.1912
0.1912 0.1912
4 0.4 0.3822 500 10102 0.1153
0.1153 0.1153
A.9 Impact of ùõΩonNeuroCUT
We study the impact of ùõΩparameter(eq. 6) which is the number of
iterations in random walk with restart. In Table 16 we present the
performance of NeuroCUT using different ùõΩon normalized-cut
objective atùëò=5. We observe that NeuroCUT improves with more
iterations as ùõΩincreases and then its performance stabilizes.
Table 16: Impact of ùõΩparameter on NeuroCUT on normal-
ized cut at ùëò=5on the Cora dataset.
ùõΩ Normalized Cut
1 2.12
3 1.57
10 0.33
50 0.33
100 0.33
A.10 Stability Across Multiple Runs
Table 17 shows the Mean and Standard deviation of NeuroCUT for
Normalized cut at ùëò=5for 3 runs. The Std Dev. is lower than the
Mean showing the stability of NeuroCUT across multiple runs.
Table 17: Mean and Standard Deviation for different datasets
on Normalized Cut at ùëò=5forNeuroCUT
Dataset Mean¬±Std Dev
Cora 0.337¬±0.0024
Citeseer 0.20¬±0.00961
Harbin 0.073¬±0.0027
Actor 0.97¬±0.0389
 
2595