DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based
Graph Neural Networks
Zongsheng Cao
University of Chinese Academy of Sciences
Beijing, China
zongshengcao16@gmail.comJing Li
School of Economics and Management, Tsinghua
University
Beijing, China
lijing3@sem.tsinghua.edu.cn
Zigan Wang‚àó
Shenzhen International Graduate School, Tsinghua
University
Shenzhen, China
School of Economics and Management, Tsinghua
University
Beijing, China
wangzigan@sz.tsinghua.edu.cnJinliang Li‚àó
School of Economics and Management, Tsinghua
University
Beijing, China
jll@tsinghua.edu.cn
ABSTRACT
Graph Neural Networks (GNNs) have demonstrated powerful ca-
pabilities in reasoning within Knowledge Graphs (KGs), gathering
increasing attention. Our idea stems from the observation that the
prior work typically employs hand-designed or sample-designed
paradigms in the process of message propagation, engaging a set
of adjacent entities at each step of propagation. As a result, such
methods struggle with the increasing number of entities involved as
propagation steps extend. Moreover, they neglect the message inter-
actions between adjacent entities and propagation relations in KG
reasoning, leading to semantic inconsistency during the message
aggregation phase. To address these issues, we introduce a novel
knowledge graph embedding method through a diffusion process,
termed DiffusionE. Specifically, we reformulate the message prop-
agation in knowledge reasoning as a diffusion process, regarding
the message semantics as the diffusion signal. In this sense, guided
by semantic information, messages can be transmitted between
nodes effectively and adaptively. Furthermore, the theoretical anal-
ysis suggests our method can leverage an adaptive diffusivity for
message propagation in the semantic interactions of KGs. It shows
that DiffusionE effectively leverages message interactions between
entities and propagation relations, ensuring semantic consistency in
KG reasoning. Comprehensive experiments reveal that our method
attains state-of-the-art performance compared to prior work on
several well-established benchmarks.
‚àóCorresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671997CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíKnowledge representation
and reasoning.
KEYWORDS
Knowledge graph, Diffusion process, Graph Neural Networks
ACM Reference Format:
Zongsheng Cao, Jing Li, Zigan Wang, and Jinliang Li. 2024. DiffusionE: Rea-
soning on Knowledge Graphs via Diffusion-based Graph Neural Networks.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671997
1 INTRODUCTION
Knowledge Graphs (KGs) can represent real-world facts and interre-
lations between entities in a structured manner [ 13,31]. Due to the
KG is not complete in practice, KG reasoning aims to infer new facts
that are not explicitly presented in the KG by link prediction [ 15,36].
This capability has been broadly explored in applications such as
predicting drug interactions [ 16,19], personalized recommenda-
tions [ 12,32], and facilitating questions answering [ 1,7]. Specifi-
cally, a KG reasoning query can be represented as (entity _query,
relation _query, ?), then we need to deduce entity _answer from
the initial entity _query and relation _query. For instance, given
the query (Barack Obama, president of, ?), we can derive the en-
tity_answer is the United States. In this process, KG reasoning
entails identifying the desired answer entity for a specific query by
leveraging the entities and connections in the KG.
In the past decades, numerous efforts have been made in knowl-
edge graph reasoning methods, which can be categorized into
triplet-based models, path-based methods, and Graph Neural Net-
work (GNN)-based approaches. Triplet-based models, such as TransE
[6], TorusE [ 31], and QuatE [ 40], derive answers by utilizing learned
embeddings of entities and relations. Path-based methods, includ-
ing AnyBURL [ 3], MultiHop [ 5], Core [ 22], and SACN [ 24], employ
logical rules to deduce relational paths from the query entity to
 
222
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zongsheng Cao, Jing Li, Zigan Wang & Jinliang Li
Figure 1: An example of KG reasoning. Given a query (Barack
Obama,ùëü,?) the answer depends on the different relation ùëü.
Specifically, if ùëüisAdvocate for, the answer is Envoronmental
Conservation Policy while the answer is Harvard University
ifùëüisGraduate from. In our method, we can aggregate the
entities with the guide of semantics gradually, in this way,
we can generate the semantic-dependent answer effectively.
the target entity. GNN-based methods enhance entity representa-
tions by using GNNs to connect entities with their neighbors [ 17].
Early models like R-GCN [ 25] and CompGCN [ 30] experience over-
smoothing issues during message propagation. To this end, recent
models such as NBFNet [ 43] and RED-GNN [ 41] have addressed
this by focusing on the local neighborhoods of the query entity.
Despite these advancements, existing methods typically define local
neighborhoods based on proximity to the query entity, which may
lead to the dilution of distinct semantic features of entities as the
number of propagation steps increases.
From the considerations above, it is evident that existing mod-
els, particularly GNN-based KG reasoning methods, have achieved
significant success but exhibit limitations in fully leveraging the
semantics of the query relation when defining relevant local neigh-
borhoods. Specifically, these methods pay less attention to the
query relation, hence they often include many extraneous enti-
ties and facts, thereby increasing computational overhead with-
out correspondingly enhancing performance. For instance, as illus-
trated in Fig. 1, the most relevant evidence for the query (Barack
Obama, Advocate_for, ?) includes the facts such as (Barack Obama,
Be_supported_by, Joe Biden) and (Joe Biden, Proposed_with, Envi-
ronmental Conservation Policy). Conversely, for the query (Barack
Obama, Graduate_from, ?), the optimal response is derived from
the subgraph featuring (Barack Obama, Be_alumnus_of, George
W. Bush) and (George W. Bush, Educate_in, Harvard University).
In light of these observations, two critical challenges arise for KG
reasoning: (1) How can local neighborhoods be dynamically tailored
for different query relations? (2) How can message passing effectively
capture semantic correlations inherent in the KG?To this end, inspired by the diffusion process1, we propose a
novel framework to model the message propagation in KG reason-
ing, termed DiffussionE. Specifically, to tackle challenge (1), we
introduce a wave-based propagation [ 9] module, where the message
spreads from the previous wave to the next one, prioritizing cru-
cial nodes first and progressively widening the propagation scope.
This method effectively facilitates message transmission without
an explosive increase in involved entities. To address the challenge
(2), drawing inspiration from the energy-based model in the diffu-
sion process, we introduce a semantics-dependent flowing module,
which can capture semantic correlations and distinctions between
entities. Moreover, we show the advantage of adaptive diffusivity
in the message propagation. On top of this, we theoretically high-
light that DiffusionE can leverage interactions between entities and
propagation relations to ensure semantic consistency. Extensive
experiments on several well-established benchmarks demonstrate
the effectiveness and efficiency of our method.
In a nutshell, the contributions of our method are summarized
as three-fold:
‚Ä¢We propose a novel KG reasoning method, which refor-
mulates the message propagation as the diffusion process.
Specifically, with the assistance of wave-based propagation
and semantics-dependent flowing modules, we take the early
trial to implement semantic-based information adaptively
aggregated messages within a unified framework from a
semantic interaction perspective.
‚Ä¢We show the advantage of adaptive diffusivity in knowl-
edge propagation, and our method can realize this goal. In
this sense, we theoretically demonstrate that DiffusionE can
preserve semantic consistency in message propagation.
‚Ä¢Experiments show that the proposed method achieves state-
of-the-art performance in both transductive and inductive
KG reasoning scenarios in several datasets.
2 RELATED WORK
2.1 GNN for KG reasoning
The development of Graph Neural Networks (GNNs) has signif-
icantly transformed the approach to modeling graph-structured
data, particularly in the realm of knowledge graph reasoning. This
transformation is marked by widespread interest and utilization, as
evidenced by seminal works [ 10,17]. Studies such as [ 28,30,41,43]
illustrate the application of the message propagation framework,
which facilitates dynamic interactions among entities to enhance
KG reasoning [ 10,17]. Within this framework, messages at the
ùëô-th propagation step are computed as defined by the set F(ùëô)=
{(ùëíùë†,ùëü,ùëíùëú) ‚àà F|ùëíùë†‚àà E(ùëô‚àí1),ùëü‚àà R,ùëíùëú‚àà E(ùëô)}. Each message
ùëö(ùëô)(ùëíùë†,ùëü,ùëíùëú)is derived using the function ùëÄùê∏ùëÜùëÜ(ùíâùëô‚àí1ùëíùë†,ùíâ(ùëô)
ùëü), which
leverages the respective embeddings of entities and relations, ùíâùëô‚àí1ùëíùë†
andùíâ(ùëô)
ùëü.
Regarding GNN-based KG reasoning methods, there exist vari-
ations in the sets of entities involved in the propagation process,
which can be broadly categorized into three classes based on their
approach to defining the propagation range: 1) Full propagation
1Diffusion here refer to the heat-based diffusion.
 
223DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
methods, such as R-GCN [ 25] and CompGCN [ 30], involve all en-
tities in the propagation. These methods face challenges related
to high memory demands and the potential for over-smoothing
when all entities are included, which typically limits the number
of feasible propagation steps. 2) Progressive propagation meth-
ods, exemplified by RED-GNN [ 41] and NBFNet [ 43], begin with
a specific query entity ùëíùëûand incrementally expand through its
L-hop neighborhood. Initially, E(0)={ùëíùëû}, with each subsequent
setE(ùëô)including entities within the expanding neighborhood,
E(ùëô)=√ê
ùëí‚ààE(ùëô‚àí1)N(ùëí), whereN(ùëí)denotes the 1-hop neighbors.
3) Constrained propagation methods, such as GraIL [ 28] and CoM-
PILE [ 20], employ a restriction on propagation to specific subgraphs.
These subgraphs differ for each entity pair, allowing for the aggrega-
tion of semantic messages from a subgraph perspective. However,
this approach can pose computational challenges in large-scale
KGs, as the computational demands increase due to the varying
subgraphs.
2.2 Sampling methods for GNN
Sampling methodologies play a crucial role in guiding and refining
entity selection during the propagation process in the application
of GNNs, particularly for homogeneous graphs. These techniques
are essential for enhancing scalability and can be broadly classified
into the following three main categories. (a) Node-wise Sampling
Methods: Models such as GraphSAGE [ 11] and PASS [ 37] selec-
tively sample ùêæentities from the neighboring set, N(ùëí), for each
entityùëíduring each propagation step, focusing on individual nodes.
(b) Layer-wise Sampling Methods: Represented by FastGCN [ 2],
Adaptive-GCN [ 14], and LADIES [ 44], these methods regulate the
number of entities sampled at each layer to ùêæ, thus controlling
the total number of entities involved in subsequent propagation
layers. (c) Subgraph Sampling Methods: Notable methods such as
Cluster-GCN [ 4], GraphSAINT [ 39], and ShadowGNN [ 38] focus on
extracting localized subgraphs related to a query entity. These meth-
ods aim to provide a more focused view of the graph by leveraging
the relevance of the sampled subgraphs to the query.
Additionally, several innovative models have been developed
to enhance KG reasoning [ 8,34]. Among these, RS-GCN [ 8] intro-
duces a node-wise sampling method specifically tailored to optimize
neighbor selection during the propagation phases of R-GCN and
CompGCN. This strategy is crucial for reducing memory usage and
enhancing the efficiency of large-scale KG operations. AdaProp
[42] proposes an adaptive propagation path to facilitate the KG rea-
soning. Another notable model, DPMPN [ 34], merges the benefits
of a full propagation GNN, adept at capturing global features, with
the targeted efficiency of a pruned GNN. This approach utilizes a
layer-wise sampling method where pruning is strategically applied,
thus significantly improving memory efficiency and scalability in
KG reasoning.
Despite the remarkable success, the previous work neglects the
semantic-based propagation process, hence leading to the subopti-
mal performance. In this paper, we propose a new KG reasoning
method inspired by the difussion process, which provides a feasible
direction to study KG reasoning.3 PREMILARY
A diffusion process in this paper refers to the concept of heat dif-
fusion on the Riemannian manifold [ 23]. This process employs an
anisotropic diffusion mechanism, which efficiently generates in-
stance representations through the dynamic flow of information.
We represent the state of any instance at time ùë°and location ùëñwith
a vector-valued function ùíõùëñ(ùë°):[0,‚àû)‚Üí Rùëë. This anisotropic dif-
fusion is mathematically modeled by a partial differential equation
(PDE), equipped with specific boundary conditions, that outlines
the temporal evolution of instance representations.
ùúïZ(ùë°)
ùúïùë°=‚àá‚àó(S(Z(ùë°),ùë°)‚äô‚àá Z(ùë°)),s. t.Z(0)=[xùëñ]ùëÅ
ùëñ=1, ùë°‚â•0
(1)
In Eq. 1, we define ùíÅ(ùë°)=[ùíõ(ùë°)]ùëÅ
ùëñ=1‚ààRùëÅ√óùëë, where‚äôdenotes the
Hadamard product. The function ùë∫(ùíÅ(ùë°),ùë°):RùëÅ√óùëë√ó[0,‚àû)‚Üí
[0,1]ùëÅ√óùëÅrepresents the diffusivity coefficient, which adjusts the
diffusion intensity between any two instances at a given time ùë°,
based on their current states. The gradient operator ‚àámeasures the
state differences between source and target instances, specifically,
(‚àáùíÅ(ùë°))ùëñùëó=ùíõùëó(ùë°)‚àíùíõùëñ(ùë°)for distinct locations ùëñandùëó. Conversely,
the divergence operator ‚àá‚àóintegrates these differences to assess
the overall information flow at a location. It is computed as follows:
(‚àá‚àó)ùëñ=ùëÅ‚àëÔ∏Å
ùëó=1ùë∫ùëñùëó(ùíÅ(ùë°),ùë°)(‚àáùíÅ(ùë°))ùëñùëó.
It should be emphasized that both operators are defined within a
discrete domain encompassing ùëÅlocations. The physical interpre-
tation of Eq. 1 suggests that the temporal variation of heat at any
given location ùëñcorresponds directly to the spatial influx of heat at
that location.
ùúïzùëñ(ùë°)
ùúïùë°=ùëÅ‚àëÔ∏Å
ùëó=1Sùëñùëó(Z(ùë°),ùë°) zùëó(ùë°)‚àízùëñ(ùë°)(2)
The diffusivity coefficient, as described in Eq.1, is pivotal in
determining the rate of heat dissemination across the space, serv-
ing as a crucial metric [ 23]. In Eq.2, ùë∫(ùíÅ(ùë°),ùë°)plays a key role in
regulating the flow of information between instances and guid-
ing the evolution of their states, offering considerable flexibility
in its definition. A straightforward implementation might define
ùë∫(ùíÅ(ùë°),ùë°)as an identity matrix, effectively limiting feature propa-
gation to self-loops only. This approach essentially transforms the
model into a multi-layer perceptron (MLP), where each instance
is processed independently. Conversely, when a predefined graph
structure is available, ùë∫(ùíÅ(ùë°),ùë°)can be configured to reflect this
structure, though this configuration restricts the information flow
to the immediate neighbors in the graph. Ideally, ùë∫(ùíÅ(ùë°),ùë°)would
allow non-zero values across any pair (ùëñ,ùëó)and dynamically evolve
over time, enabling instances at each layer to propagate informa-
tion adaptively and efficiently to all others, thereby facilitating an
extensive exchange of information among instances.
4 THE PROPOSED METHOD
Problem formulation. LetG=(E,R,F,Q)represent an in-
stance of a KG. Here, EandRdenote the sets of entities and re-
lations, respectively. The set of fact triplets is denoted as F=
 
224KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zongsheng Cao, Jing Li, Zigan Wang & Jinliang Li
Figure 2: The illustration of the message propagation. From (a) to (b) and (c), the semantics-dependent diffusion module can
capture semantic correlations and distinctions between entities. Meanwhile, the message spreads from the previous wave to
the next one, prioritizing crucial nodes first and progressively widening the propagation scope.
{(ùëíùë†,ùëü,ùëíùëú)|ùëíùë†,ùëíùëú‚ààE,ùëü‚ààR} , where each triplet consists of a sub-
ject entityùëíùë†, a relation ùëü, and an object entity ùëíùëú. Additionally,
Q={(ùëíùëû,ùëüùëû,ùëíùëé)|ùëíùëû,ùëíùëé‚ààE,ùëüùëû‚ààR} represents the set of query
triplets, where each triplet contains an incomplete component, in-
dicated by a question mark, and a target answer entity ùëíùëé. The goal
of the reasoning task is to learn a function that predicts the answer
entityùëíùëé‚ààEfor each query(ùëíùëû,ùëüùëû,?)based on the available fact
tripletsF. In other words, the objective is to develop a model that
can infer missing information in the query based on the existing
knowledge graph facts.
Diffusion Path. In this part, the primary objective is to design an
enhanced propagation path, eGùêø, that enables the model, optimized
with appropriate parameters, to accurately predict the target an-
swer entityùëíùëéfor each query(ùëíùëû,ùëüùëû,?). Notably, previous methods
have often overlooked the semantic correlations and distinctions
among entities during the message propagation process. To address
this shortcoming, we leverage a diffusion process to dynamically
adapt the propagation paths, as illustrated in Figure 2. This strategy
facilitates the gradual dissemination and transfer of information
while accounting for the semantic relationships among entities.
bGùêø
ùëíùëû=n
E0
ùëíùëû,...,E‚Ñì
ùëíùëû,...,Eùêø
ùëíùëûo
,
s.t.E‚Ñì
ùëíùëû=(
ùëíùëû	
‚Ñì=0
ùëëùëñùëìùëì
E‚Ñì‚àí1ùëíùëû
‚Ñì=1...ùêø(3)
The diffusion process commences with a specific query entity,
denoted as ùëíùëû. Entities residing within E‚Ñìùëíùëûare meticulously se-
lected through an adaptive sampling mechanism, encompassing a
subset ofE‚Ñì‚àí1ùëíùëûand its immediate neighbors, under the influence of
the diffusion process. However, this approach is accompanied by a
duo of inherent challenges: (1) The diffusion process inherently ex-
hibits a gradual and incremental expansion, proliferating across an
increasingly vast spatial domain. (2) A notable variance is observed
in the diffusion rates among neighboring nodes, a phenomenon
intricately linked with the semantic relationships interconnecting
the nodes.
To this end, we introduce a new propagation paradigm with the
assistance of the diffusion process, which can adaptively select rele-
vant entities into the propagation path based on the given query. To
solve the first challenge, we design a wave-based sampling module
[9], which can reduce the number of involved entities and preserve
the layer-wise connections and we will gradually expand the wavearea to get more and more messages. Notice that the efficiency of dif-
fusion during propagation is greatly influenced by the transmission
pathway, such as the disparity in thermal conductivity between
the two components. For the second challenge, Inspired by the
aforementioned phenomenon, we propose a semantics-dependent
sampling distribution, which is jointly optimized with the model
parameters, to select entities that are semantically relevant to the
query relation.
Motivated by the analogous phenomenon of wave propagation in
water, where subsequent waves are generated iteratively from their
predecessors, it is mathematically represented by the constraint
E0
ùëíùëû‚äÜE1
ùëíùëû‚äÜ¬∑¬∑¬∑‚äÜEùêø
ùëíùëû
ensuring a coherent retention of entities from previous steps, while
also facilitating incremental sampling from newly encountered en-
tities. This methodology aims to preclude the omission of potential
target entities that exhibit promising attributes.
Wave-based Propagation. Consider the set of entities at the (‚Ñì‚àí1)-
th step, denoted as E‚Ñì‚àí1ùëíùëû. Let the entities designated for sampling
in the‚Ñì-th step be symbolized as E‚Ñì‚àí1
ùëíùëû. Essentially, all adjacent
entities expressed as N(E‚Ñì‚àí1ùëíùëû)=‚à©ùëí‚ààE‚Ñì‚àí1ùëíùëûN(ùëí), are conceived as
viable candidates for the sampling procedure. However, to ensure
the sustained preservation of E‚Ñì‚àí1ùëíùëûin succeeding sampling steps,
the focus is strategically shifted towards the newly encountered en-
tities, hence identifying them as the prime candidates for sampling,
denoted as, i.e.,
E‚Ñì
ùëíùëû:=DIFF
E‚Ñì‚àí1
ùëíùëû
=N
E‚Ñì‚àí1
ùëíùëû
\‚à™‚Ñì‚àí1
ùëò=ùëõùë†Eùëò
ùëíùëû. (4)
whereùëõùë†(1‚â§ùëõùë†‚â§‚Ñì‚àí1)denotes a specific layer, which controls
the propagation range. In this way, influenced by wave-based propa-
gation, it allows for the nuanced incorporation of newly discovered
entities while maintaining a robust continuity with previously iden-
tified entities, ensuring a comprehensive and dynamic sampling
process.
Observe that within the diffusion process, heat energy dissemi-
nates unevenly among points, flowing more profusely where tem-
perature differences are more pronounced. A parallel can be drawn
with semantic flow processes, where adjacent entities marked by
more substantial semantic discrepancies tend to impart more pro-
found informational insights compared to those characterized by
minimal differences. Inspired by this, an energy module is intro-
duced to quantify the semantic significance of entities meticulously.
 
225DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
This allows for the sampling of ùêæ‚â™|Eùë°
ùëíùëû|entities, executed with-
out replacement from the candidate set Eùëô
ùëíùëû, subsequent to a system-
atic ranking based on their semantic importance. This innovative
approach, inspired by natural thermal diffusion processes, aims
to optimize the selection of entities by prioritizing those that are
semantically more impactful and conducive to the enhancement of
informational richness within the diffusion process.
Semantics-dependent Diffusion. In the context of GNN-based
KG reasoning methodologies, the entity representations at the ter-
mination of the propagation sequence, denoted as ùíâùêøùëíùëú, serve a
pivotal role in ascertaining the plausibility of entity ùëíùëúbeing iden-
tified as the target answer entity. This process is inspired by the
integration of an energy function within the diffusion process,
prompting the proposal of a semantic module. This module is in-
geniously crafted to learn and elucidate the entity representations,
subsequently indicating their relevance concerning the given query
(ùëíùëû,ùëüùëû,?).
Specifically, to meticulously gauge the semantic relevance and
inherent variability, inspired by the diffusion process, we utilize
a linear mapping function to achieve this, expressed as ùëî(ùíâùëôùëí;ùúÉùëô),
replete with parameters ùúΩùëô‚ààRùëë, whereùëôis the layer. In this way,
it can facilitate the process of sampling, executed according to
a meticulously constructed probability distribution, embodying
the nuanced semantic intricacies and relevance of the entities in
relation to the propagated query. To this end, we sample entities in
the message propagation according to the probability distribution:
ùëù‚Ñì(ùëí):=exp
ùëì
ùëî ùíâ‚Ñìùëí;ùúΩ‚Ñì‚àíùëî
ùíâ‚Ñìùëíùëû;ùúΩ‚Ñì
/ùúè
√ç
ùëí‚Ä≤‚ààE‚Ñì
ùëí‚Ä≤exp
ùëì
ùëî
ùíâ‚Ñì
ùëí‚Ä≤;ùúΩ‚Ñì
‚àíùëî
ùíâ‚Ñìùëíùëû;ùúΩ‚Ñì
/ùúè(5)
where the temperature value ùúè>0and we select the top-K candi-
date entities by calculate ùëù‚Ñì(ùëí)inE‚Ñì
ùëíùëûand denote the set of them
aseE‚Ñìùëíùëû.ùëìis a non-negative and decreasing function such as ReLU
function on certain intervals for ùëî(¬∑;¬∑). For more mathematical
properties and explanations refer to Theorem.1.
The sampled entities at ‚Ñì-th step together with entities in the
(‚Ñì‚àí1)-th step constitute the involved entities at the ùëôstep, i.e.,
E‚Ñì
ùëíùëû=E‚Ñì‚àí1
ùëíùëû‚à™
eE‚Ñì
ùëíùëû
(6)
Message Aggregation. In the process of message aggregation,
the foundational step involves the obtaining of the neighborhood
entity set, accomplished through a sophisticated diffusion process.
Subsequent to acquiring the neighborhood entities, the algorithm
proceeds to the crucial phase of message representation calcula-
tion. In this way, consider a neighborhood entity set, denoted by
N, which has been ascertained through the diffusion mechanism.
Given the query(ùëíùëû,ùëüùëû,?)and edgesF=(ùëíùë†,ùëü,ùëíùëú)in the diffusion
process, the objective then metamorphoses into the calculation of
an aggregated message representation, delineated as follows:
ùíé‚Ñì
ùëíùë†,ùëü,ùëíùëú=MESS(ùíâ‚Ñì‚àí1
ùëíùë†,ùíâ‚Ñì‚àí1
ùëíùëú,ùíâ‚Ñì
ùëü,ùíâ‚Ñì
ùëüùëû) (7)
where MESS(¬∑)can be modelled by add function and entities ùëíùëú‚àà
N(E‚Ñìùëíùëû). On top of this, we conduct the message aggregation pro-
cess as follows:
ùíâ‚Ñì
ùíÜùëú=ùë†
AGG
ùíé‚Ñì
(ùíÜùë†,ùëü,ùíÜùëú),(ùëíùë†,ùëü,ùëí ùíê)‚ààF‚Ñì
(8)Algorithm 1: DiffusionE: learning adaptive propagation
path.
Input: query(ùëíùëû,ùëüùëû,?),Eùëíùëû={ùëíùëû}, stepsùêø, number of
sampled entities ùêæ, and functions ùëÄùê∏ùëÜùëÜ(¬∑)and
ùê¥ùê∫ùê∫(¬∑).
Output: The probability being the target answer of
ùëíùëú‚ààEùëíùëû,ùëüùëû
1forùëô=1,¬∑¬∑¬∑,ùêødo
2 Get the adjacent entities N(Eùëô‚àí1ùëíùëû)=‚à©ùëí‚ààEùëô‚àí1ùëíùëûN(ùëí), the
newly-diffusion entities Eùëô
ùëíùëû=N(Eùëô‚àí1ùëíùëû)\Eùëô‚àí1ùëíùëû, and
edgeFùëô={(ùëíùë†,ùëü,ùëíùëú)|ùëíùë†‚ààEùëô‚àí1ùëíùëû,ùëíùëú‚ààN(Eùëô‚àí1ùëíùëû)}
3 Obtain ùíéùëôùëíùë†,ùëü,ùëüùëú=ùëÄùê∏ùëÜùëÜ(ùíâùëô‚àí1ùëíùë†,ùíâùëô‚àí1ùëíùëú,ùíâùëôùëü,ùíâùëôùëüùëû)for entities
ùëíùëú‚ààN(Eùëô‚àí1ùëíùëû)
4 Obtain ùíâ‚Ñì
ùíÜ‚Ñìùëú:=ùõø
AGG
ùíé(ùíÜùë†,ùëü,ùíÜùëú),(ùëíùë†,ùëü,ùëí ùíê)‚ààF‚Ñì
for
entitiesùëíùëú‚ààN(Eùëô‚àí1ùëíùëû)
5 Generate the eE‚Ñìùëíùëûby Eq.(5)
6 Update propagation path: E‚Ñìùëíùëû=E‚Ñì‚àí1ùëíùëû,ùëüùëû‚à™eE‚Ñìùëíùëû
7Returnùúôùëíùëúfor eachùëíùëú‚ààEùêøùëíùëû.
where AGG(¬∑)can be modeled by function Mean(¬∑), andùë†is an
activation function such as the sigmoid function.
Loss Function. By evaluating the diffusion rate among different
entities in the above, our method can effectively optimize the mes-
sage propagation paths, better capture the semantic correlation
among entities, as well as reduce the redundant paths, and increase
the accuracy of inference. In this way, we then denote Las the
loss function instantiated on each instance, employing binary cross-
entropy loss across all entities ùëíùëú‚ààEùêøùëíùëû, delineated as:
L=‚àí‚àëÔ∏Å
ùëíùëú‚ààEùêøùëíùëûùë¶ùëíùëúlog ùúôùíÜùëú+ 1‚àíùë¶ùëíùëúlog 1‚àíùúôùëíùëú(9)
whereùúôùëíùëúdenotes the likelihood score for the plausibility of ùëíùëú
being identified as the target answer entity and it can be modeled
by linear transformation. ùëíùëéis the answer entity . Concurrently,
the labelùë¶ùëíùëúis ascribed a value of 1 if the condition ùëíùëú=ùëíùëéholds
true, otherwise, it is allocated a value of 0. Moreover, the overall
algorithm can refer to Algorithm 1.
Remark 1. Algorithm 1 presents a significant advancement by
utilizing a dynamic propagation path instead of a predefined static
path. This adaptability in propagation is guided by the wave-based
propagation mechanism and a diffusion-based strategy, allowing the
path to be recalibrated and updated at each step based on the evolving
representations of entities.
5 THEORETICAL ANALYSIS
In the above, we delineate the rationale behind DiffusionE in sam-
pling neighborhood entities, and then we show the advantage of
diffusivity in our method as follows.
Intrinsic adaptive diffusivity of DiffusionE. Recall the diffusion
process mentioned in Section 3, to solve Eq.(2), such a diffusion
 
226KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zongsheng Cao, Jing Li, Zigan Wang & Jinliang Li
process can guide the model to aggregate the semantic-relevant
message from other instances at every layer for learning informative
instance representations. To this end, we can use numerical methods
to solve the continuous dynamics in Eq.(2), e.g., the explicit Euler
scheme involving finite differences with step size ùúèas follows:
ùëß(ùëò+1)
ùëñ=¬©¬≠
¬´1‚àíùúèùëÅ‚àëÔ∏Å
ùëó=1ùë∫(ùëò)
ùëñùëó¬™¬Æ
¬¨ùëß(ùëò)
ùëñ+ùúèùëÅ‚àëÔ∏Å
ùëó=1ùë∫(ùëò)
ùëñùëóùëß(ùëò)
ùëó. (10)
The numerical iteration can stably converge for ùúè‚àà(0,1). We can
adopt the state after a finite number ùêæof propagation steps and
use it for final predictions, i.e., ÀÜùë¶ùëñ=ùëÄùêøùëÉ(ùëß(ùêæ)
ùëñ).
On the basis of this, we then introduce an energy function, de-
signed to quantitatively assess the anticipated quality of instance
states at a specific iteration denoted by ùëòas follows:
ùê∏(ùëç,ùëò;ùõø)=‚à•ùëç‚àíùëç(ùëò)‚à•2
ùêπ+ùúÜ‚àëÔ∏Å
ùëñ,ùëóùõø‚à•ùëßùëñ‚àíùëßùëó‚à•2
2(11)
whereùõø:R+‚ÜíRis a non-decreasing and concave function on a
certain interval. In this way, we have the following theorem:
Theorem 1. In the context of any regularized energy with a speci-
fiedùúÜin Eq. (11), there exists a value 0<ùúè<1in Eq.(10), ensuring
that the diffusion process with the diffusivity in our method can yield
a descent step on the energy across successive steps, symbolized as
ùê∏(ùíÅ(ùëò+1),ùëò;ùõø)‚â§ùê∏(ùíÅ(ùëò),ùëò‚àí1;ùõø), valid for each ùëò‚â§1.
The proof can refer to Appendix A. Theorem 1 elucidates the
presence of an intrinsic optimal diffusivity function, fundamen-
tally dependent on the distance between current states, such as
‚à•ùíõ(ùëò)
ùëñ‚àíùíõ(ùëò)
ùëó‚à•2. This significant insight allows for a meticulous de-
construction of the traditionally implicit diffusion process, thereby
enabling precise calculation of the diffusion matrix ùë∫(ùëò)through a
systematic feed-forward approach, starting from the initial states.
In this way, it can enhance computational precision and clarity, en-
suring a more robust and reliable implementation of the associated
algorithms and methodologies.
Leveraging the crucial insights from Theorem 1, we are equipped
to identify the appropriate diffusivity function, based on the differ-
ences in distances between corresponding states at each successive
stage. This approach enables us to precisely model the diffusion
process with clear accuracy, avoiding the need for repetitive com-
putations. This method also facilitates the efficient determination
of the diffusion matrix ùë∫(ùëò), employing a streamlined feed-forward
methodology that originates from the initial states. In this way, it
demonstrates the practicality and benefits of Eq.(5).
6 EXPERIMENTS
In this section, we conduct comprehensive empirical experiments
to assess the efficacy of the DiffusionE framework under both trans-
ductive and inductive learning settings.
6.1 Transductive setting
Datasets. We adopt six well-established datasets as shown in Table
1, ubiquitously utilized in KG completion tasks. Specifically, these
datasets encompass Family [ 18], UMLS [ 18], WN18RR [ 6], FB15k237
[29], NELL995 [33], and YAGO3-10 [26].Table 1: The statistics of the KG datasets used in our paper.
Notice that the fact triplets in Fare used to build the graph,
andQùë°ùëüùëé,Qùë£ùëéùëô,Qùë°ùë†ùë°are the query triplets used for reasoning.
dataset entity relation |F| |Qùë°ùëüùëé| |Qùë£ùëéùëô| |Qùë°ùë†ùë°|
Family 3.0k 12 23.4k 5.9k 2.0k 2,8k
UMLS 135 46 5.3k 1.3k 569 633
WN18RR 40.9k 11 65.1k 21.7k 3.0k 3.1k
FB15k237 14.5k 237 204.1k 68.0k 17.5k 20.4k
NELL-995 74.5k 200 112.2k 37.4k 543 2.8k
YAGO3-10 123.1k 37 809.2k 269.7k 5.0k 5.0k
Baselines. For the experimental evaluation, we compare DiffusionE
with a diverse array of baseline models including: (i) Non-GNN
Methods such as ConvE [ 6], QuatE [ 40], RotatE [ 27], MINERVA
[5], DRUM [ 24], RNNLogic [ 22], and RLogic [ 3]. (ii) GNN-Based
Full Propagation Methods such as CompGCN [ 30], and Progressive
Propagation Methods such as NBFNet [ 43] and RED-GNN [ 41].
Noteworthy is the omission of R-GCN [ 25], attributed to its inferior
performance relative to CompGCN [ 30]. Moreover, as delineated
by [41, 43], models such as GraIL [28], and CoMPILE [20].
Metrics. In line with previous research, we utilize two commonly
used evaluation metrics: mean reciprocal ranking (MRR) and Hit@k,
where K can be 1 and 10. Higher scores on these metrics indicate
superior performance of the model.
Implementation. In our implementation, we explore a range of
values for the propagation steps, denoted as ùêø, spanning from 5 to
8. Additionally, we experiment with different numbers of sampled
entities, denoted as ùêæ, which include values from the candidate set
{100,200,500,1000,2000}. Alongside this, we vary the temperature
values, denoted as ùúè, selecting from the set {0.5,1.0,2.0}. We ensure
consistency in the range configurations of other hyperparameters
with those used in the RED-GNN implementation [41].
Results. Results. Tables 1 and 2 provide a comprehensive overview
of the performance of DiffusionE compared to established transduc-
tive reasoning methods. Notably, Graph Neural Network (GNN)-
based methods show remarkable efficacy, outperforming non-GNN
counterparts. This superiority can be attributed to their ability
to capture both the structural and semantic complexities inher-
ent in knowledge graphs, resulting in a more comprehensive and
nuanced representation Within the realm of GNN-based methodolo-
gies, a nuanced differentiation is observed. Progressive propagation
techniques, such as NBFNet and REDGNN, demonstrate enhanced
performance compared to comprehensive propagation strategies
like CompGCN. This disparity is particularly pronounced in ex-
tensive knowledge graphs such as WN18RR, FB15k237, NELL-995,
and YAGO3-10, where progressive strategies exhibit notable advan-
tages. DiffusionE stands out by achieving superior performance
across various baseline methods, particularly in knowledge graphs
such as WN18RR, NELL-995, and YAGO3-10. It even exhibits a
slight yet noteworthy edge over NBFNet in the NELL-995 dataset.
Furthermore, DiffusionE showcases adaptability and competitive
performance in smaller knowledge graphs like Family and UMLS,
highlighting its versatility and effectiveness. In summary, these
results demonstrate the effectiveness of DiffusionE in transductive
reasoning tasks across diverse knowledge graphs.
 
227DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 2: The results of transductive setting on Family, UMLS, and WN18RR datasets. The best results are shown in bold while
the second-best results are shown in the underline .
type modelsFamily UMLS WN18RR
MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10
non-GNNConvE 0.912 0.837 0.982 0.937 0.922 0.967 0.427 0.392 0.498
QuatE 0.941 0.896 0.991 0.944 0.905 0.993 0.480 0.440 0.551
RotatE 0.921 0.866 0.988 0.925 0.863 0.993 0.477 0.428 0.571
MINERVA 0.885 0.825 0.961 0.825 0.728 0.968 0.448 0.413 0.513
DRUM 0.934 0.881 0.996 0.813 0.674 0.976 0.486 0.425 0.586
RNNLogic 0.881 0.857 0.907 0.842 0.772 0.965 0.483 0.446 0.558
RLogic - - - - - - 0.470 0.443 0.537
NodePiece 0.892 0.837 0.971 0.844 0.736 0.968 0.459 0.432 0.543
MorsE (RotatE) 0.938 0.886 0.994 0.815 0.679 0.978 0.493 0.435 0.596
GNNCompGCN 0.933 0.883 0.991 0.927 0.867 0.994 0.479 0.443 0.546
NBFNet 0.989 0.988 0.989 0.948 0.920 0.995 0.551 0.497 0.666
ConGLR 0.982 0.971 0.969 0.952 0.923 0.981 0.532 0.473 0.654
RED-GNN 0.992 0.988 0.997 0.964 0.946 0.990 0.533 0.485 0.624
DiffusionE 0.990 0.989 0.992 0.970 0.957 0.992 0.557 0.504 0.658
Table 3: The results of transductive setting on FB15K237, NELL-995, and YAGO3-10 datasets. The best results are shown in bold
while the second-best results are shown in the underline .
type modelsFB15K237 NELL-995 YAGO3-10
MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10
non-GNNConvE 0.325 0.237 0.501 0.511 0.446 0.619 0.520 0.450 0.660
QuatE 0.350 0.256 0.538 0.533 0.466 0.643 0.379 0.301 0.534
RotatE 0.337 0.241 0.533 0.508 0.448 0.608 0.495 0.402 0.670
MINERVA 0.293 0.217 0.456 0.513 0.413 0.637 - - -
DRUM 0.343 0.255 0.516 0.532 0.460 0.662 0.531 0.453 0.676
RNNLogic 0.344 0.252 0.530 0.416 0.363 0.478 0.554 0.509 0.622
RLogic 0.310 0.203 0.501 - - - 0.36 0.252 0.504
GNNCompGCN 0.355 0.264 0.535 0.463 0.383 0.596 0.421 0.392 0.577
NBFNet 0.415 0.321 0.599 0.525 0.451 0.639 0.550 0.479 0.686
RED-GNN 0.374 0.283 0.558 0.543 0.476 0.651 0.559 0.483 0.689
DiffusionE 0.376 0.294 0.539 0.552 0.490 0.654 0.566 0.494 0.692
6.2 Inductive setting.
Datasets. Following [ 28,41], we utilize a total of twelve subsets,
encompassing four distinct versions each, which are derived from
established datasets: WN18RR, FB15k237, and NELL-995. Each sub-
set is uniquely characterized by a disparate split between the train-
ing and test sets, ensuring a robust and comprehensive evaluation
schema. For a detailed discourse on the specific splits and a nuanced
presentation of the statistical attributes of each subset, one can refer
to the comprehensive descriptions available in [28, 41].
Baselines. For our inductive setting evaluation, the comparative
baselines are as follows. Specifically, reasoning methods, such as
ConvE [ 6], QuatE [ 40], RotatE [ 27], MINERVA [ 5], and CompGCN
[30], which predominantly rely on the learning of entity embed-
dings during training, are inherently unsuitable for this context
and hence excluded. Our comparison pool comprises non-GNN
methods like RuleN [ 21], NeuralLP [ 35], and DRUM [ 24], which
emphasize rule learning devoid of entity embeddings. In the realm
of GNN-based methods, our selection includes noteworthy modelslike GraIL [ 28], CoMPILE [ 20], REDGNN [ 41], and NBFNet [ 43].
Models such as RNNLogic [ 22] and RLogic [ 3], despite their rele-
vance, are omitted from comparison due to the unavailability of
essential resources like source codes and particular results in this
setting. For a robust and comprehensive evaluation, we adopted
the evaluation strategy from [ 41], prioritizing the ranking of tar-
get answer entities over a complete spectrum of negative entities,
diverging from the approach in [ 28] that utilizes a subset of 50
randomly sampled negative entities.
Results. As shown in Table 4 and Table 5, constrained propagation
techniques, represented by GraIL and CoMPILE, despite their inher-
ent capabilities for inductive reasoning, do not perform as well as
their progressive propagation counterparts, notably RED-GNN and
NBFNet. The patterns internalized by GraIL and CoMPILE within
their constrained operational scopes appear to falter in generalizing
effectively to unfamiliar KGs. In contrast, DiffusionE achieves high
performance across diverse datasets and various data split versions.
 
228KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zongsheng Cao, Jing Li, Zigan Wang & Jinliang Li
Table 4: Results for inductive setting (evaluated with Hit@10). The best results are shown in bold while the second-best results
are shown in the underline .
MethodsWN18RR FB15K237 NELL-995
V1 V2 V3 V4 V1 V2 V3 V4 V1 V2 V3 V4
RuleN 0.730 0.694 0.407 0.681 0.446 0.599 0.600 0.605 0.760 0.514 0.531 0.484
Neural LP 0.772 0.749 0.476 0.706 0.468 0.586 0.571 0.593 0.871 0.564 0.576 0.539
DRUM 0.777 0.747 0.477 0.702 0.474 0.595 0.571 0.593 0.873 0.540 0.577 0.531
GraIL 0.760 0.776 0.409 0.687 0.429 0.424 0.424 0.389 0.565 0.496 0.518 0.506
CoMPILE 0.747 0.743 0.406 0.670 0.439 0.457 0.449 0.358 0.575 0.446 0.515 0.421
NBFNet 0.827 0.799 0.563 0.702 0.517 0.639 0.588 0.559 0.795 0.635 0.606 0.591
RED-GNN 0.799 0.780 0.524 0.721 0.483 0.629 0.603 0.621 0.866 0.601 0.595 0.556
DiffusionE 0.857 0.843 0.605 0.737 0.525 0.661 0.627 0.625 0.873 0.660 0.620 0.523
Table 5: Results for inductive setting (evaluated with MRR). The best results are shown in bold while the second-best results are
shown in the underline .
MethodsWN18RR FB15K237 NELL-995
V1 V2 V3 V4 V1 V2 V3 V4 V1 V2 V3 V4
RuleN 0.668 0.645 0.368 0.624 0.363 0.433 0.439 0.429 0.615 0.385 0.381 0.333
Neural LP 0.649 0.635 0.361 0.628 0.325 0.389 0.400 0.396 0.610 0.361 0.367 0.261
DRUM 0.666 0.646 0.380 0.627 0.333 0.395 0.402 0.410 0.628 0.365 0.375 0.273
GraIL 0.627 0.625 0.323 0.553 0.279 0.276 0.251 0.227 0.481 0.297 0.322 0.262
CoMPILE 0.577 0.578 0.308 0.548 0.287 0.276 0.262 0.213 0.330 0.248 0.319 0.229
RED-GNN 0.701 0.690 0.427 0.651 0.369 0.469 0.445 0.442 0.637 0.419 0.436 0.363
NBFNet 0.684 0.652 0.425 0.604 0.307 0.369 0.331 0.305 0.584 0.410 0.425 0.287
DiffusionE 0.727 0.724 0.460 0.660 0.310 0.474 0.456 0.446 0.673 0.424 0.458 0.308
This consistency underscores the effectiveness of DiffusionE in ex-
trapolating the adaptively learned propagation paths to novel KGs,
maintaining its effectiveness even in the presence of previously un-
seen entities during the training phase. These results demonstrate
DiffusionE‚Äôs robust inductive reasoning capabilities, standing as a
testament to its generalized applicability and performance.
Ablation Study. In order to evaluate the effectiveness of our pro-
posed method, we conducted an ablation study on different datasets.
The results of this study are presented in Figure 2, which shows the
performance comparison between different variants of our method,
namely "w/o w" and "w/o d". Specifically, "w/o w" refers to the
variant where the wave-based propagation is removed, while "w/o
d" refers to the variant where the semantic-dependent diffusion is
removed. From the results, it is evident that the experimental perfor-
mance of all the ablation versions decreases compared to the origi-
nal method. This shows the effectiveness of both the wave-based
propagation and the semantic-dependent diffusion in DiffusionE.
7 CONCLUSION
In contrast to existing GNN-based approaches that rely on prede-
fined or sampled propagation paths, we introduce a new method
termed DiffusionE by learning propagation paths through a dif-
fusion mechanism during message propagation. This approach
involves two key components: a wave-based propagation module
that efficiently preserves nearby targets and maintains layer-wise
(a) Family
 (b) UMLS
Figure 3: Performance Comparison between different vari-
ants of DiffusionE.
connections with linear complexity among entities, and a semantics-
based flowing module that identifies semantically related entities
during the propagation process. Our empirical evaluation of vari-
ous benchmark datasets considers both transductive and inductive
KG reasoning scenarios. The results highlight the effectiveness of
DiffusionE in achieving state-of-the-art performance on several
datasets and demonstrate the superiority of DiffusionE from both
practical and theoretical perspectives. Moreover, noticing that the
large language model (LLM) embraces rich semantic information
for entities, combining the DiffusionE and LLM may be a promising
direction for enhancing the GCN framework in the context of KG
reasoning.
 
229DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
ACKNOWLEGEMENT
This research was supported by the National Key R&D. Program
of China (Grant No. 2023YFC3305404); in part by the Institute for
Industrial Innovation and Finance (IIIF), Tsinghua University, the
Hong Kong General Research Fund (Grant No. 17503722), NSFC
HY Working Fund (Grant No. 03070100001), Tsinghua SIGS Basic
Support Fund (Grant No. 07010100003), and Tsinghua SIGS Research
Support Fund (Grant No. 01030100049).
REFERENCES
[1]Yuzheng Cai, Siyuan Liu, Weiguo Zheng, and Xuemin Lin. 2023. Towards Gen-
erating Hop-constrained s-t Simple Path Graphs. Proc. ACM Manag. Data 1, 1
(2023), 61:1‚Äì61:26.
[2]Jie Chen, Tengfei Ma, and Cao Xiao. 2018. FastGCN: Fast Learning with Graph
Convolutional Networks via Importance Sampling. In ICLR.
[3]Kewei Cheng, Jiahao Liu, Wei Wang, and Yizhou Sun. 2022. RLogic: Recursive
Logical Rule Learning from Knowledge Graphs. In SIGKDD. 179‚Äì189.
[4]Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
2019. Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph
Convolutional Networks. In SIGKDD. 257‚Äì266.
[5]Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar,
Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2018. Go for a
Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using
Reinforcement Learning. In ICLR.
[6]Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2017.
Convolutional 2D Knowledge Graph Embeddings.. In AAAI. 1811‚Äì1818.
[7]Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan
Salakhutdinov, and William W. Cohen. 2020. Differentiable Reasoning over a
Virtual Knowledge Base. In ICLR.
[8]Arthur Feeney, Rishabh Gupta, Veronika Thost, Rico Angell, Gayathri Chandu,
Yash Adhikari, and Tengfei Ma. 2021. Relation Matters in Sampling: A Scalable
Multi-Relational Graph Neural Network for Drug-Drug Interaction Prediction.
CoRR abs/2105.13975 (2021).
[9]Milinda Fernando, David Neilsen, Eric W. Hirschmann, Yosef Zlochower, Hari
Sundar, Omar Ghattas, and George Biros. 2022. A GPU-Accelerated AMR Solver
for Gravitational Wave Propagation. In SC22: International Conference for High
Performance Computing, Networking, Storage and Analysis, Dallas, TX, USA, No-
vember 13-18, 2022. IEEE, 75:1‚Äì75:15.
[10] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In ICML (Proceedings
of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.).
1263‚Äì1272.
[11] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-
tation Learning on Large Graphs. In NeurIPS. 1024‚Äì1034.
[12] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei. 2023.
Personalized PageRank on Evolving Graphs with an Incremental Index-Update
Scheme. Proc. ACM Manag. Data 1, 1 (2023), 25:1‚Äì25:26.
[13] Jiacheng Huang, Zequn Sun, Qijin Chen, Xiaozhou Xu, Weijun Ren, and Wei Hu.
2023. Deep Active Alignment of Knowledge Graph Entities and Schemata. Proc.
ACM Manag. Data 1, 2 (2023), 159:1‚Äì159:26. https://doi.org/10.1145/3589304
[14] Wen-bing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. 2018. Adaptive
Sampling Towards Fast Graph Representation Learning. In NeurIPS. 4563‚Äì4572.
[15] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2022.
A Survey on Knowledge Graphs: Representation, Acquisition, and Applications.
IEEE Trans. Neural Networks Learn. Syst. 33, 2 (2022), 494‚Äì514.
[16] Tatiana Jin, Boyang Li, Yichao Li, Qihui Zhou, Qianli Ma, Yunjian Zhao, Hongzhi
Chen, and James Cheng. 2023. Circinus: Fast Redundancy-Reduced Subgraph
Matching. Proc. ACM Manag. Data 1, 1 (2023), 12:1‚Äì12:26.
[17] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[18] Stanley Kok and Pedro M. Domingos. 2007. Statistical predicate invention. In
ICML (ACM International Conference Proceeding Series, Vol. 227). 433‚Äì440.
[19] Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. 2020.
KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction.
InIJCAI, Christian Bessiere (Ed.). 2739‚Äì2745.[20] Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. 2021. Communicative
Message Passing for Inductive Relation Reasoning. In AAAI. 4294‚Äì4302.
[21] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla,
and Heiner Stuckenschmidt. 2018. Fine-Grained Evaluation of Rule- and
Embedding-Based Systems for Knowledge Graph Completion. In ISWC (Lec-
ture Notes in Computer Science, Vol. 11136). 3‚Äì20.
[22] Meng Qu, Junkun Chen, Louis-Pascal A. C. Xhonneux, Yoshua Bengio, and Jian
Tang. 2021. RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs.
InICLR.
[23] Steven Rosenberg. 1997. The Laplacian on a Riemannian manifold: an introduction
to analysis on manifolds. Number 31. Cambridge University Press.
[24] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.
2019. DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs. In
NeurIPS. 15321‚Äì15331.
[25] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolutional
networks. In ESWC. Springer, 593‚Äì607.
[26] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core
of semantic knowledge. In WWWW. ACM, 697‚Äì706.
[27] Zhiqing Sun, Zhihong Deng, Jianyun Nie, and Jian Tang. 2019. RotatE: Knowledge
Graph Embedding by Relational Rotation in Complex Space. In ICLR. 1‚Äì18.
[28] Komal K. Teru, Etienne G. Denis, and William L. Hamilton. 2020. Inductive
Relation Prediction by Subgraph Reasoning. In ICML (Proceedings of Machine
Learning Research, Vol. 119). 9448‚Äì9457.
[29] Kristina Toutanova and Danqi Chen. 2015. Observed versus latent features for
knowledge base and text inference. In CVMSC. 57‚Äì66.
[30] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. 2020.
Composition-based Multi-Relational Graph Convolutional Networks. In ICLR.
[31] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge Graph
Embedding: A Survey of Approaches and Applications. IEEE Trans. Knowl. Data
Eng. 29, 12 (2017), 2724‚Äì2743.
[32] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:
Knowledge Graph Attention Network for Recommendation. In SIGKDD. 950‚Äì958.
[33] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: A
Reinforcement Learning Method for Knowledge Graph Reasoning. In EMNLP.
564‚Äì573.
[34] Xiaoran Xu, Wei Feng, Yunsheng Jiang, Xiaohui Xie, Zhiqing Sun, and Zhi-Hong
Deng. 2020. Dynamically Pruned Message Passing Networks for Large-scale
Knowledge Graph Reasoning. In ICLR.
[35] Fan Yang, Zhilin Yang, and William W. Cohen. 2017. Differentiable Learning of
Logical Rules for Knowledge Base Reasoning. In NeurIPS, Isabelle Guyon, Ulrike
von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett (Eds.). 2319‚Äì2328.
[36] Xiaowei Ye, Rong-Hua Li, Qiangqiang Dai, Hongchao Qin, and Guoren Wang.
2023. Efficient Biclique Counting in Large Bipartite Graphs. Proc. ACM Manag.
Data 1, 1 (2023), 78:1‚Äì78:26.
[37] Minji Yoon, Th√©ophile Gervet, Baoxu Shi, Sufeng Niu, Qi He, and Jaewon Yang.
2021. Performance-Adaptive Sampling Strategy Towards Fast and Accurate
Graph Neural Networks. In SIGKDD. 2046‚Äì2056.
[38] Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich,
Rajgopal Kannan, Viktor K. Prasanna, Long Jin, and Ren Chen. 2021. Decoupling
the Depth and Scope of Graph Neural Networks. In NeurIPS. 19665‚Äì19679.
[39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Vik-
tor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning
Method. In ICLR.
[40] Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019. Quaternion knowledge graph
embeddings. In NeurIPS. 2731‚Äì2741.
[41] Yongqi Zhang and Quanming Yao. 2021. Knowledge Graph Reasoning with
Relational Directed Graph. CoRR abs/2108.06040 (2021).
[42] Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, and Bo Han. 2023.
AdaProp: Learning Adaptive Propagation for Graph Neural Network based
Knowledge Graph Reasoning. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 3446‚Äì3457.
[43] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal A. C. Xhonneux, and Jian Tang. 2021.
Neural Bellman-Ford Networks: A General Graph Neural Network Framework
for Link Prediction. In NeurIPS. 29476‚Äì29490.
[44] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu.
2019. Layer-Dependent Importance Sampling for Training Deep and Large Graph
Convolutional Networks. In NeurIPS. 11247‚Äì11256.
 
230