CoLiDR: Co ncept L earni ng using Aggregated D isentangled
Representations
Sanchit Sinha
sanchit@virginia.edu
University of Virginia
Charlottesville, VA, USAGuangzhi Xiong
hhu4zu@virginia.edu
University of Virginia
Charlottesville, VA, USAAidong Zhang
aidong@virginia.edu
University of Virginia
Charlottesville, VA, USA
ABSTRACT
Interpretability of Deep Neural Networks using concept-based mod-
els offers a promising way to explain model behavior through
human-understandable concepts. A parallel line of research fo-
cuses on disentangling the data distribution into its underlying
generative factors, in turn explaining the data generation process.
While both directions have received extensive attention, little work
has been done on explaining concepts in terms of generative factors
to unify mathematically disentangled representations and human-
understandable concepts as an explanation for downstream tasks.
In this paper, we propose a novel method CoLiDR - which utilizes
a disentangled representation learning setup for learning mutually
independent generative factors and subsequently learns to aggre-
gate the said representations into human-understandable concepts
using a novel aggregation/decomposition module. Experiments
are conducted on datasets with both known and unknown latent
generative factors. Our method successfully aggregates disentan-
gled generative factors into concepts while maintaining parity with
state-of-the-art concept-based approaches. Quantitative and visual
analysis of the learned aggregation procedure demonstrates the
advantages of our work compared to commonly used concept-based
models over four challenging datasets. Lastly, our work is general-
izable to an arbitrary number of concepts and generative factors -
making it flexible enough to be suitable for various types of data.
CCS CONCEPTS
‚Ä¢Mathematics of computing ‚ÜíProbability and statistics.
KEYWORDS
disentanglement, xai, concept learning, generalization
ACM Reference Format:
Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang. 2024. CoLiDR: Concept
Learning using Aggregated Disentangled Representations. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671938
1 INTRODUCTION
The increasing proliferation of Deep Neural Networks (DNNs) has
revolutionized multiple diverse fields of research such as vision,
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671938speech, and language [ 9,38]. Given the black-box nature of DNNs,
explaining DNN predictions has been an active field of research
that attempts to impart transparency and trustworthiness in their
decision-making processes. Recent research has categorized ex-
plainability into progressively increasing levels of granularity. The
most fine-grained approaches attempt to assign importance scores
to the raw features (e.g. pixels) extracted from the data, while less
granular approaches assign importance scores to data points (sets
of features). Explaining DNNs using concepts provides the high-
est level of abstraction, as concepts are high-level entities shared
among multiple similar data points that are aligned with human
understanding of the task at hand. This makes concept explanation
much more global in nature. Many recent approaches for concept-
based explanations have attempted to either 1) infer concepts post-
hoc from trained models [ 15] or 2) design inherently explainable
concept-based models [1], such as the concept bottleneck model
(CBM) [18].
A parallel field of research on disentanglement representation
learning [ 3,5,11,16] attempts to learn a low-dimensional data
representation where each dimension independently represents a
distinct property of the data distribution. These approaches learn
mutually independent generative factors of data by estimating their
probability distribution from observed data. Once the probability
distribution of the generative factors is estimated, a given sample
can be theoretically decomposed and re-generated from its genera-
tive factors. Due to their ability to uncover the underlying gener-
ative factors, disentanglement approaches are considered highly
interpretable.
Present state-of-the-art approaches do not effectively unify dis-
entangled representation learning with concept-based approaches.
Approaches like GlanceNets [ 23] attempt to align concepts with dis-
entanglement with strong assumptions, which are not valid for real-
world datasets. To address the above issues, in this paper, we pro-
pose Concept Learning using Aggregated Disentangled Representa-
tions (CoLiDR), a self-interpretable approach that combines disen-
tangled representation learning with concept-based explainability.
Specifically, CoLiDR learns disentangled generative factors using
a disentangled representation learning module, followed by the
aggregation of learned disentangled representations into human-
understandable concepts using a novel aggregation/decomposition
module and subsequently a task prediction module that maps con-
cepts to task labels. Our experiments show that interventions on
learned concept representations can fix wrongly classified sam-
ples, which makes CoLiDR useful for model debugging. Figure 1
demonstrates the schematic overview of CoLiDR, where the learned
disentangled generative factors from data are aggregated into con-
cepts, which are further utilized for task prediction. Specifically,
 
2699
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
Concept
1Concept
2Concept
NConcept
nConcept
n+1Concept
n+2Unannotated Concepts Annotated
ConceptsGF
1GF
2GF
kGF
3Disentangled Generative Factors
Input
Image
Task
Label.......
...Aggregation Decomposition
Human-Understandable Concepts
Figure 1: Schematic overview of the proposed CoLiDR ap-
proach. The input data distribution Xis first disentangled
intoùëòmutually independent generative factors (GFs). Sub-
sequently, the GFs are aggregated into concepts. Note that
the concepts are modeled as a set of annotated concepts cor-
responding to concepts with annotation by humans and a
separate set of concepts that are useful for prediction, but are
unannotated. Finally, the concepts are utilized for predicting
the task label Y.
our main contributions are as follows: 1) Propose CoLiDR frame-
work, a novel method to aggregate disentangled representations
into human understandable concepts, while achieving at-par per-
formance to other commonly utilized concept-based models such as
CBMs and GlanceNets, 2) Improve model debugability by test-time
interventions, and 3) Provide a flexible framework which can be
generalized to arbitrary number of generative factors and concepts.
2 RELATED WORK
2.1 Related Work on Disentangled
Representation Learning
Disentangled representation learning has long been a fascinating
study aimed at separating distinct informational factors of varia-
tions in real-world data [ 3,21]. Due to its probabilistic framework
and flexibility to customize training objectives, the variational au-
toencoder (VAE) [ 17] is a commonly used architecture in the study
of disentangled representation learning, which can capture differ-
ent factors of variation with its encoder and decoder. Based on
the traditional VAE, Higgins et al . [11] proposed a variant, ùõΩ-VAE,
which introduces an additional hyperparameter to scale the impor-
tance of the regularization term. By changing the weight of the
regularization term, ùõΩ-VAE can control the trade-off between the
reconstruction of inputs and the disentanglement of latent vari-
ables. Instead of optimizing the Kullback‚ÄìLeibler (KL) divergence
between the latent distribution and standard Gaussian prior for
disentanglement as done by VAE and ùõΩ-VAE, FactorVAE [ 16] andùõΩ-
TCVAE [ 5] further decompose the regularization term and propose
to directly penalize the total correlation between latent variables,
which are shown to better disentangle the variables.
While early studies in disentanglement representation learning
attempted to learn independent latent variables by modifying the
training objective of VAE [ 5,11,16], Locatello et al . [21] showed
that it is impossible to learn identifiable disentangled latent vari-
ables without any supervision. A series of subsequent studies werethen proposed to learn disentangled representations with better
identifiability using different kinds of supervision [22, 33, 34].
2.2 Related Work on Concept Based
Explanations
Due to the black-box nature of deep learning models, various
approaches have been explored to provide explanations for the
outcome of deep neural networks [ 10,13,18,26,27]. One impor-
tant direction is to interpret the models with intuitive and human-
understandable concepts, which are usually high-level abstractions
of the input features. Various attempts have been made to automat-
ically learn the concepts for different tasks [ 7,8,15,39,40]. Among
them, the concept bottleneck model (CBM) [ 18] is a commonly
used approach to incorporate the learning of concepts into deep
neural networks. By constructing a low-dimensional intermediate
layer in the models, CBMs are able to capture the high-level con-
cepts that are related to the downstream tasks. Numerous studies
have been carried out to adapt CBMs for tasks in various domains
[2,12,23,28,31]. The most relevant works to our proposed ap-
proach are Concept Bottleneck Models [ 18], Concept Embedding
Models [ 42], CLAP [ 37] and GlanceNets [ 23], details of which are
discussed in the next Section.
3 UNIFYING CONCEPT-BASED MODELING
WITH VARIATIONAL INFERENCE
Problem setup. The concept learning problem is characterized
as a two-step process [ 18] for a given training sample {x,y,C}.
Given an observation ùë•, a concept-based model learns a function
to map the input to its associated human-understandable concepts
C={ùëê1,¬∑¬∑¬∑,ùëêùëÅ}. Subsequently, a predictor will project the concept
embeddings to ÀÜùë¶, which is the prediction for the label ùë¶of the sample
in a downstream task.
Assumptions. Following previous work in concept-based models
[18,23], concept models follow two fundamental assumptions as
follows: 1) the data distribution in the input space can be accurately
mapped to the distributions of concepts in the latent space, and 2)
the concept scores are necessary and sufficient to predict labels of
samples in downstream tasks.
3.1 Variational Inference-based
Concept-learning for Greater Interpretablity
Utilizing disentanglement approaches to explain the data genera-
tion process has been well studied recently [ 11,17]. A typical disen-
tanglement system attempts to learn a fundamental representation
of mutually independent generative factors (GFs) usually modeled as
random variables from independent probability distributions. The
Variational Inference (VI) process utilizes the learned generative
factors to emulate the data generation procedure. Even though vari-
ational inference has been successfully utilized for controlled data
generation with success [ 6,11], utilizing them to improve the inter-
pretability of concept-based models is still relatively underexplored.
Our work attempts to fill this gap. Below, we list the advantages of
using variational inference-based approaches for concept learning:
‚Ä¢Provides a controlled data generation process: In addition
to emulating the data generation process, VIs offer control via
 
2700CoLiDR: Co ncept L earni ng using Aggregated D isentangled R epresentations KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
CBM/CEM GlanceNets
CLAPCoLIDR
(Ours)
Variational
InferenceDependence
Figure 2: A schematic view of the underlying assumptions
considered across SOTA concept-based models, CBM/CEM,
CLAP, GlanceNet, and CoLiDR. The circles in blue represent
directly observable attributes, input sample ùëã, task label
ùëåand representative concepts ùê∂. The red circles represent
learned representations.
interventions over GFs - a desirable property for stakeholders of
a concept-based model.
‚Ä¢Captures the continuous space mappings: Several works
[24,35] have demonstrated that learning discrete mapping of
input samples to concepts is susceptible to fragility. VI smoothens
both the input and concept spaces to learn a continuous, well-
trained and robust mapping function.
‚Ä¢Easier to isolate confounds: Another well-documented prob-
lem in concept-based models is the encoding of non-related
confounds in the concept representations [ 41]. VI methods are
successful in isolating confounds - hence reducing the effect of
encoding noise, spurious correlations, etc. in the concept repre-
sentations.
3.2 Comparison with Existing Approaches
We begin the discussion by presenting an anti-causal [14] model
to visualize the data generation process of our proposed approach
and compare it to multiple comparable recent state-of-the-art ap-
proaches in Figure 2. Note that we represent the task labels as Y,
the set of concepts as C, the disentangled generative factors as Z,
and the data distribution as X. The single-edge arrows model the
dependency between distributions while the double-ended arrows
represent variational inference - modeling the generative process.
Comparisons to CBM/CEM. Concept Bottleneck Models [ 18] and
Concept Embedding Models [ 42] do not incorporate variational
inference. Nevertheless, they visualize the data-generative process
as being conditioned on the annotated concepts and also assume
that the task labels are entirely conditioned on the concepts.Comparisons to CLAP. CLAP [ 37] is one of the first works uti-
lizing variational inference to model the data generation process.
CLAP considers a part of the disentangled space representing rel-
evant GFs to be conditioned on the task labels and a part of it
representing confounds to be conditioned on independent normal
distributions. However, CLAP does not utilize human-annotated
concepts - making the learned disentangled space Zunidentifiable
(Refer [22]). This makes CLAP not comparable to our approach.
Comparisons to GlanceNets. GlanceNets [ 23], to our knowledge,
is the only existing approach that attempts to bridge the gap be-
tween VI and concept learning. The GlanceNets approach incorpo-
rates a Variational Autoencoder based disentangling mechanism to
learn the generative factors (GFs) of the distribution and formulate
the concept learning problem as a one-to-one mapping between a
subset of generative factors (or disentangled representations) and
human-understandable concepts. As shown in Figure 2, the anno-
tated concepts directly supervise a part of the learned disentangled
space Z. However, in doing so it makes an implicit assumption that
thehuman-understandable concepts act as generative factors
of the data distribution. This is a strong assumption for two distinct
reasons. Firstly, multiple generative factors can contribute as a con-
stituent of a particular concept and multiple concepts can share
the same generative factors. Secondly, human-annotated concepts
are by definition, abstract and high-level and do not necessarily
model the fine-grained data generation process. Utilizing abstract
concepts to supervise the disentangling process may undermine
the disentanglement procedure, as disentangled representations are
supposed to capture the low-level GFs of the data distribution in-
stead of high-level abstract human concepts. The model is shown to
be effective on two synthetic datasets, dSprites and MPI-3D, which
are procedurally generated using carefully curated GFs that cor-
respond to human-understandable concepts. In addition, CoLiDR
can effectively generalize to datasets where GFs are completely
unknown. Unannotated concepts. Note that we model the con-
cept set as a union of known, annotated concepts, and unannotated
concepts. This modeling approach is common in unsupervised con-
cept learning approaches like [ 1], but has not yet been studied for
Variational Inference based concept learning. The assumption is
that there exists some concepts relevant to the task prediction but
are not manually annotated.
Table 1 lists the differences between the discussed approaches
based on - (i) the Presence of VI, (ii) the Presence of unsupervised
GFs, (iii) the Presence of supervised concepts, and (iv) the Presence
of unannotated concepts (Discussed in Methodology).
Metho
d Variational Unsupervised Supervised Unannotated
Inference GF Concepts Concepts
CBMs/CEMs ‚úó
‚úó ‚úó ‚úó
GlanceNet ‚úî ‚úó* ‚úî ‚úó
CLAP ‚úî ‚úî ‚úó ‚úî
CoLiDR ‚úî ‚úî ‚úî ‚úî
Table 1: A summary of salient features of our method as
compared to the baselines. The asterisk represents partial
supervision over the generative factors by GlanceNets.
 
2701KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
3.3 Drawbacks of Supervising GFs with
Concepts
Although it might be tempting to utilize concepts as GFs like what
GlanceNets [ 23] did, it is dangerous for a variety of reasons. In
this section, we provide the drawbacks associated with this line
of thought with a concrete and practical example. Let us consider
as a sample from the CelebA dataset [ 20] which consists of facial
photographs of celebrities. Each image in the dataset is annotated
with binary concepts such as the presence of ‚Äúblack hair‚Äù, ‚Äúblonde
hair‚Äù, ‚Äúwavy hair‚Äù, ‚Äústraight hair‚Äù which are easily understandable
by humans - implying they are abstract. Consider the assumption
(fundamental in [ 23]) that each of these concepts also align with
the generative factors of the data distribution as well.
Limited concept abstraction. As it can be seen, the assumption is
flawed - two identical images with straight hair and different colors
(Black and Blonde) would be sampled from completely different
distributions despite sharing the same type of hair. Images of two
people having the same hair color but with wavy and straight
hair respectively would also be sampled from completely different
distributions despite sharing the same color. Hence, there is no fine-
grained control over the actual generative factors. Furthermore, as
concepts such as ‚Äúattractiveness‚Äù are very subjective and contain
a large number of constituent underlying distributions, it is much
more effective to understand constituent distributions than directly
aligning attractiveness with a single disentangled representation.
Limited intra-concept hierarchy modeling. By definition of
disentanglement, generative factors and concepts would be inde-
pendent of each other. Any hierarchical relationship cannot be
encoded in such a space. For example, the concept ‚Äúblack hair‚Äù is
ideally sampled from mutually independent distributions repre-
senting ‚Äúblackness‚Äù and multiple other distributions constituting
in construction of hair themselves. As such, ‚Äúblackness‚Äù can also
be combined with distributions modeling facial hairs for ‚Äúbeard‚Äù
concepts.
Hence, instead of directly aligning the concepts with disentan-
gled representations, it is more explainable and reasonable to under-
stand the actual distributions constituting a concept. Relaxing the
assumption of aligning disentangled latent space representations
with concepts helps us to better capture and explain constituent
distributions in each concept - in turn improving explainability.
4 METHODOLOGY
We first provide a broad overview of the proposed approach CoL-
iDR in Section 4.1. Section 4.2 details the architectures utilized for
computing disentangled representations. Subsequently, Section 4.3
introduces the Aggregation/Decomposition module and the task
prediction network. Finally, Section 4.4 describes the training proce-
dure and additional Disentangled Representation (DR) Consistency
loss which regularizes the Aggregation/Decomposition module to
correctly learn representations to concept mapping.
4.1 Overview of CoLiDR
CoLiDR consists of three distinct modules as shown in Figure 3.
The first module learns the disentangled generative factors from
the input data. The second module aggregates the disentangledrepresentations and maps them to human-understandable concepts.
The third module maps the concepts to the task label.
4.2 Disentangled Representations Learning
(DRL) Module
The first step of the proposed approach involves learning disen-
tangled representations corresponding to the various generative
factors in the data. As depicted in Figure 1, the generative factors
form the basis of the data generation process itself. Our DRL module
learns the disentangled generative factors that can be used for data
generation. Suppose ùëßis the given embedding of the generative fac-
tors andùë•is the corresponding generated data. The underlying data
generation process pùúÉ(ùë•|ùëß)can be estimated using a ùõΩ-VAE, which
estimates the maximum likelihood using variational inference. Fol-
lowing [ 17], we maximize the Evidence Lower Bound (ELBO) to
model the posterior distribution ùëûùúô(ùëß|ùë•)and the distribution of
data generation ùëùùúÉ(ùë•|ùëß)as detailed below.
ùê∏ùêøùêµùëÇ =‚àíùõΩDùêæùêø(ùëûùúô(z|x)||ùëù(z))+Eùëûùúô(ùëß|ùë•)[ùëôùëúùëîùëùùúÉ(x|z)] (1)
TheEterm in Formula 1 comprises the reconstruction loss between
the inputùë•and predicted reconstruction ÀÜùë•, usually taken as Mean
Square Error (MSE). The prior distribution pis usually taken as a
standard Gaussian distribution which encourages the covariance
matrix of the learned distribution to be diagonal, enforcing inde-
pendence constrain [ 11,17]. The tunable hyperparameter ùõΩacts as
a quantitative measure of the extent of disentanglement.
ùõΩ-VAE estimates ùëûùúôusing an encoder that maps ùë•from the input
observation space in Rùëëto the disentangled representation space in
Rùëò, whereùëëandùëòare the dimensions of the input and latent space
respectively. The distribution ùëùùúÉis estimated using a decoder that
maps the disentangled representation space back to the observation
space ( Rùëò‚ÜíRùëë). Specifically, the encoder encodes the input as an
estimated mean ùëßùúá‚ààRùëòand a standard deviation vector ùëßùúé‚ààRùëò,
from which the latent representation ùëßis sampled from the Gaussian
multivariate distribution N(ùëßùúá,diag(ùëß2ùúé)).
4.3 Aggregation/Decomposition Module
While the DRL module learns disentangled generative factors in
the latent space, the factors may be too fine-grained to be aligned
with human understanding. However, these disentangled genera-
tive factors can be considered as an independent basis of human-
understandable concepts as shown in Figure 1.
4.3.1 Aggregation of Generative Factors into Concepts. We propose
the Aggregation module which aggregates the disentangled underly-
ing generative factors of data into human-understandable concepts.
Specifically, given a latent representation ùëß=[ùëß1,¬∑¬∑¬∑,ùëßùëò]‚ä§, each
conceptùëêùëñ‚àà {ùëêùëñ}ùëÅ
ùëñ=1can be considered as a combination of all
disentangled factors ùëß1,¬∑¬∑¬∑,ùëßùëò. To increase the expressiveness of
our model in learning concepts from the generative factors while
keeping each concept as a linear aggregation of different factors
for interpretability, we propose to encode each generative factor
ùëßùëó(ùëó‚àà{1,¬∑¬∑¬∑,ùëò})with an independent neural network ùëéùëó, which
learns the non-linear mapping from ùëßùëótoùëß‚Ä≤
ùëófor concept learning.
Subsequently, our model learns the linear combinations of com-
ponents in ùëß‚Ä≤=[ùëß‚Ä≤
1,¬∑¬∑¬∑,ùëß‚Ä≤
ùëò]‚ä§to represent high-level concepts
 
2702CoLiDR: Co ncept L earni ng using Aggregated D isentangled R epresentations KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 3: Architecture of the proposed CoLiDR approach. CoLiDR consists of three modules - the Disentangled Representations
Learning (DRL) Module which learns disentangled generative factors (top), the Aggregation/Decomposition Module which
learns to aggregate the generative factors into concepts and subsequently decompose them back into generative factors (bottom-
left) and the Task Learning module (bottom-right) which utilizes the concepts to perform task label prediction.
ùëê1,¬∑¬∑¬∑,ùëêùëÅthat are relevant to the downstream task. Given the pos-
terior distribution ùëûùúô(ùëß|ùë•)inùõΩ-VAE, the learned concepts can be
formulated as
ùëê=ùëì(A(ùëß)), (2)
whereùëß‚àºùëû(ùëß|ùë•),ùëìis a one-layer fully connected model, and Ais
defined as
A(ùëß)=[ùëß‚Ä≤
1,¬∑¬∑¬∑,ùëß‚Ä≤
ùëò]‚ä§=[ùëé1(ùëß1),ùëé2(ùëß2),¬∑¬∑¬∑,ùëéùëò(ùëßùëò)]‚ä§. (3)
4.3.2 Concept Level Supervision. Given the annotation of ùëõmanu-
ally defined concepts, we can supervise the learning of the first ùëõ
concepts to be aligned with human knowledge. Formally, given the
estimated scores ùëê1,¬∑¬∑¬∑,ùëêùëõand human annotations ùëô1,¬∑¬∑¬∑,ùëôùëõ, the
supervision can be performed by minimizing
Lùëêùëúùëõ=ùëõ‚àëÔ∏Å
ùëñ=1BCE(ùëêùëñ,ùëôùëñ), (4)
where BCE is the Binary Cross Entropy Loss. In addition to the
supervised learning of human-annotated concepts, our model is
designed to automatically capture the information of the remaining
conceptsùëêùëõ+1,ùëêùëõ+2¬∑¬∑¬∑,ùëêùëÅfrom the input in an unsupervised man-
ner as it is not possible to annotate a truly closed set of concepts.
Specifically, the unsupervised learning of the unannotated concepts
is performed through the training of concept decomposition for
the reconstruction of generative factors, which are described in
Sections 4.3.3 and 4.4.
4.3.3 Decomposing Concepts into Generative Factors. Correspond-
ing to the aggregation module, we propose the decomposition
moduleùëîwhich learns a mapping from the concept embeddingùëê=[ùëê1,¬∑¬∑¬∑,ùëêùëÅ]‚ä§back to transformed disentangled representa-
tions ( z‚Ä≤) which are further transformed back to the original disen-
tangled representations zwith decodersD={ùëë1,¬∑¬∑¬∑,ùëëùëõ}. Mathe-
matically,
ÀÜùëß=D(ùëî(ùëê))=[ùëë1(ùëî(ùëê)),¬∑¬∑¬∑,ùëëùëõ(ùëî(ùëê))]‚ä§(5)
where ÀÜùëßis the estimated generative factors from the given con-
ceptsùëê. Here the decomposition module acts as the inverse of the
aggregation module and maps concepts back to the disentangled
representations. Instead of using one decoder directly for the map-
ping fromùëêtoùëß, we use independent decoders to project each
dimension in ùëß‚Ä≤to the corresponding dimension in ùëß, which miti-
gates the problem of concept leakage from concepts to unrelated
generative factors [24].
4.3.4 Task Prediction using Learned Concepts. Task prediction en-
tails the mapping from learned concepts in RùëÅto the prediction of
task labels in Rùëö, whereùëöis the number of categories in classifi-
cation tasks. The model prediction can be formulated as
ùëù(ùë¶|ùëê)=Softmax(‚Ñé(ùëê)) (6)
where the function ‚Ñéis a shallow neural network. Although ‚Ñécan
be chosen up to task requirements, we utilize a linear polynomial to
estimate‚Ñéfor model interpretability, with only human-annotated
concepts as the input to see how well they can be used to explain
the model prediction:
h(ùëê)=ùë§0+ùëõ‚àëÔ∏Å
ùëñ=1ùë§ùëñ¬∑ùëêùëñ, (7)
 
2703KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
whereùë§0,ùë§1,¬∑¬∑¬∑,ùë§ùëõùë†are the bias and weights in the predictor.
Given the true labels ùë¶, the loss for the task learning is defined as
Lùëùùëüùëíùëë=Œ¶(ùëù(ùë¶|ùëê),ùë¶), (8)
where Œ¶can be (binary) cross-entropy loss or mean square loss
depending on the type of the downstream task.
4.4 Disentangled Representation Consistency
and End-to-end Training
The three modules of CoLiDR (DRL, Aggregation/Decomposition,
Task Learning) are trained end-to-end. To maintain the consistency
of learned representations of generative factors, we enforce ùëßand
ÀÜùëßto be as similar as possible. We propose to use a DR consistency
loss (ùêøùëëùëüùëê), which can be formulated as:
Lùëëùëüùëê=‚à•ùëß‚àíÀÜùëß‚à•2
2(9)
For a given set of disentangled generative factors z, there exists a
family of surjective functionsùëì- which map from z‚Ä≤to concepts c.
Consequently, there exists a family of functions ùëîinverse coupled
withùëì, which maps from concepts ùëêtoùëß. As the function ùëìis
subjective, computing a direct inverse is intractable and hence
requires enforced consistency through Lùëëùëüùëê. The function ùëîis NOT
a standalone family of functions as they are inverse of a surjective
functionùëì.
In addition, we also encourage sparsity on the transformed rep-
resentations ùëß‚Ä≤to identify the most important generative factors
that are aggregated to compose the concepts and reduce the impact
of non-relevant factors. The overall training objective can be given
by:
L=ùê∏ùêøùêµùëÇ+ùúÜ1Lùëêùëúùëõ+ùúÜ2Lùëùùëüùëíùëë+ùúÜ3Lùëëùëüùëê+ùúÜ4‚à•ùëß‚Ä≤‚à•1. (10)
5 EXPERIMENTAL SETUP
5.1 Dataset Descriptions
‚Ä¢D-Sprites [25]: D-Sprites consists of procedurally generated
samples from six independent generative factors. Each object in
the dataset is generated based on two categorical factors (shape,
color) and four numerical factors (X position, Y position, orienta-
tion, scale). The six factors are independent of each other. The
dataset consists of 737,280 images. We randomly split data into
train-test sets in a 70/30 split.
‚Ä¢Shapes3D [4]: Shapes3D consists of synthetically generated
samples from six independent generative factors consisting of
color (hue) of floor, wall, object (float values) and scale, shape,
and orientation in space (integer values). The dataset consists of
480,000 images. We randomly split the data into train and test
sets in a 70/30 split.
‚Ä¢CelebA [20]: CelebA consists of about 200,000 178 √ó218 sized
RGB images of center-aligned facial photographs of celebrities.
The faces are annotated with 40 binary concepts like hair color,
smile, attractiveness, etc. Some of the features in the set are simple
and observable like color (black, blonde) and style of hair (wavy,
bangs). However, many concepts are abstract and subjective like
attractiveness, heavy makeup, etc. We only consider the objective
concepts for experiments. We center-crop the images to 148x148
and subsequently resize them to 64x64.‚Ä¢AWA2 [19]: Animals with Attributes-2 consists of 37,322 images
of a combined 50 animal classes with 85 binary concepts like
number of legs, presence of tail, etc. We remove certain subjec-
tive concepts such as ‚Äúeats fish‚Äù. AWA2 is neither centered nor
cropped and consists of significant background noise, making it
significantly harder to disentangle. We resize all images to 64x64
and combine the train and test splits. We use 70% of the data for
training and 30% for testing.
5.2 Dataset Task Descriptions
Synthetic datasets: As dSprites and Shapes3D datasets are pro-
cedurally generated, they do not contain an inherent downstream
task, hence we construct downstream tasks using combinations of
GFs Similar to [ 23] and [ 29]. For each task, we consider two GFs
at random and a sample has the label ‚Äú1‚Äù when all factors satisfy a
pre-defined criterion and. For categorical factors, we consider the
presence of exact values as Truth, while for continuous factors we
use a threshold. More details on task construction can be found in
the Appendix.
Real-world datasets: For the CelebA dataset, the downstream
task is cluster assignment [ 23]. For AWA2, the downstream task is
classification.
5.3 Model Implementation Details
Disentangled representations learning (DRL) module. We
utilize two different Variational Autoencoder architectures for esti-
mating disentangle representations in the DRL module. We utilize
a standard VAE [ 17] and aùõΩ-VAE [ 11]. Even though similar in for-
mulation, aùõΩ-VAE is parameterized by a tunable hyperparameter ùõΩ
which controls the strength of disentanglement. For both VAEs, the
encoder is a 5-layer CNN with BatchNorm and LeakyReLU as the
activation function. The decoder is modeled symmetrically to the
encoder with five Transpose Convolutional Layers. The size of the
latent space ùëòis set as 64 for d-Sprites and Shapes3D and 512 for
CelebA and AWA2 datasets.
Aggregation/Decomposition module . The Aggregation mod-
ule is composed of the set of transformation operations Aand map-
ping function between transformed representations and concepts
ùëì. We model each of the ùëòneural networks ùëéùëñforùëñ‚àà{0,1,..,ùëò}as
a 3-layer network. For dSprites and Shapes3D the network consists
of layers sized [64,64,1] and for CelebA and AWA2 the network
consists of layers sized [512,512,1] with ReLU as the activation
function. The function ùëìis modeled as a single fully connected
layer. Similarly, the Decomposition module is composed of the set
of inverse transformation operations Dand mapping function be-
tween concepts and transformed representations ùëî. We model each
of theùëòneural networks ùëëùëñforùëñ‚àà{0,1,..,ùëò}as a 3-layer network.
For dSprites and Shapes3D the network consists of layers sized
[1,64,64] and for CelebA and AWA2 the network consists of layers
sized [1,512,512] with ReLU as the activation function. The function
ùëîis modeled as a single fully connected layer.
Task prediction module. As proposed in [ 18] and [ 23], we
utilize a single fully connected layer mapping from the concepts to
predictions ( ‚Ñé). The final output is passed through a softmax layer
to compute probability scores for the label.
 
2704CoLiDR: Co ncept L earni ng using Aggregated D isentangled R epresentations KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Task Accuracy (‚Üë) Concept Error (‚Üì)
dSprites Shapes3D CelebA AWA2 dSprites Shapes3D CelebA AWA2
wo/ dis. CBM/CEM 0.924 0.913 0.828 0.531 0.068 0.0910 0.0378 0.33
GlanceNet 0.931 0.915 0.814 0.517 0.070 0.0910 0.0379 0.44
w/ dis. CoLiDR - VAE 0.931 0.911 0.821 0.515 0.070 0.0912 0.0377 0.39
CoLiDR -ùõΩ-VAE (noLùëëùëüùëê) 0.921 0.911 0.819 0.515 0.071 0.0919 0.0381 0.39
CoLiDR -ùõΩ-VAE 0.929 0.916 0.828 0.521 0.069 0.0910 0.0375 0.36
Table 2: Average task accuracy and concept errors across four datasets for concept-based models with disentanglement learning
(GlanceNet, CoLiDR - VAE, CoLiDR - ùõΩ-VAE) and models without disentanglement learning (CBM). ‚ÄúCoLiDR - VAE‚Äù is a
version of our model with a vanilla VAE (parameterized by ùõΩ=1) while ‚ÄúCoLiDR - ùõΩ-VAE‚Äù is another version with a ùõΩ-VAE
(parameterized by various ùõΩs) discussed in Appendix. Concept Errors are reported for dSprites and Shapes3D as RMSE while for
CelebA and AWA2 as 0-1 error. Best results for models with disentanglement learning are marked in bold.
5.4 Evaluation Setup
Task and concept accuracy. For a Concept-Based Model to be
deemed effective, it is required to be at par on performance with non-
inherently explainable models (or black box models). We measure
the task accuracy and compare it against a standard Deep Neural
Network formed using the same encoder as the VAE and a fully
connected layer as the classification head.
Next, we compare the concept-accuracy of CoLiDR against CBMs
as proposed in [ 18]. For datasets with binary concepts, accuracy
is reported as 0-1 error (misclassification). For datasets with non-
binary concepts, Root Mean Square Error (RMSE) is reported.
Disentangled representation aggregation performance. Ef-
fective aggregation of disentangled representations serves as the
most important desiderata for CoLiDR. However, it is not straight-
forward to understand the aggregation effect. Unlike supervised
disentanglement where each dimension of the disentangled repre-
sentations is forced to correspond to a concept, CoLiDR aggregates
multiple dimensions into a concept. Hence, instead of identifying
individually significant dimensions as concepts, it is important to
consider a set of representative dimensions from the aggregation
module.
Due to VAE‚Äôs inherent disentanglement procedure, we can safely
assume the mutual independence of dimensions among the learned
disentangled representations. Hence, the latent dimensions them-
selves can be thought of as features of the aggregation module. In
effect, the problem of selecting a representative set of dimensions
is identical to assigning importance scores to the most important
features in a deep neural network. Multiple post-hoc interpretabil-
ity methods are proposed [ 30,36]. We utilize Integrated Gradients
(IG)[36] to assign importance scores to each dimension. For an input
ùë•and a baseline ùë•‚Ä≤(zero-vector), IG computes attribution scores
for each feature ùëñusing the following path integral:
ùêºùê∫(ùë•)=(ùë•ùëñ‚àíùë•‚Ä≤
ùëñ)¬∑‚à´1
ùõº=0ùúïùêπ(ùë•‚Ä≤+ùõº¬∑(ùë•‚àíùë•‚Ä≤))
ùúïùë•ùëñùëëùõº (11)
We utilize Captum (https://captum.ai/) to compute attributions
using IG. The values of the attributions are normalized and their
absolute values are assigned as the final attributions. Once the
attribution scores are computed, we utilize three distinct methods
to evaluate the effects of the representative set of disentangled
representations.‚Ä¢GradCAM [ 32] visualizations: For each dimension in the top-k
most important dimensions, the GradCAM attribution visualiza-
tion plots are plotted.
‚Ä¢Latent space traversal: For concepts with one dominant di-
mension in the representative set, we linearly interpolate the
normalized latent space between two terminal values to identify
visual cues in each generated image
‚Ä¢Oracle Classification: Even though the aforementioned visu-
alizations provide qualitative evaluation, a robust quantitative
evaluation is required to assuage the effects of confirmation bias.
To achieve this, we train a Oracle Network to automatically clas-
sify images to the associated concepts with high accuracy. Sub-
sequently, we compare heatmaps generated using CBMs and
CoLiDR with the actual ground-truth annotated bounding boxes
and report the average Intersection over Union (IoU) scores which
measure the ratio of overlap to the combined are between two
bounding boxes (Refer Appendix for mathematical formulation).
We utilize a fine-tuned VGG-16 model as the backbone of the
oracle for each dataset. For more details, refer to Appendix.
Concept decomposition performance (intervention). An-
other desirable desideratum of a Concept-based model is its ease of
debugging. For a misclassified sample, fixing the concept annotation
by a domain expert should be able to correct the model predictions.
Recall that the decomposition module decomposes concepts back
into disentangled representations. We define an intervention to be
successful for a wrongly classified sample, replacing the predicted
concept score with the ground truth concept annotations changes
the wrong prediction label to the correct ground truth label ùë¶.
5.5 Evaluation Metric Descriptions
5.5.1 Oracle Training. We utilize a fine-tuned VGG-16 which con-
tains 5 blocks of convolutional layers followed by a max pooling
layer at the end of each block. The final output is passed through
a 3-layer fully connected network to perform concept prediction.
The model is trained similarly to the concept network where each
concept is weighted by its relative occurrence in the dataset.
5.5.2 Intersection over Union Calculation (IoU). Intersection over
Union is defined as the ratio of overlap between two spans of areas
on an image. Assuming two bounding boxes ùê¥andùêµ, IoU is defined
 
2705KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
as:
IoU=ùê¥‚à©ùêµ
ùê¥‚à™ùêµ(12)
We calculate IoU‚Äôs between heatmaps generated using GradCAM.
The selected pixels above a threshold are selected (255 value grayscale
CAM maps above 150). As the heatmaps can be irregular in shape,
we consider each selected pixel as part of a set on a fixed image
size.
6 RESULTS AND DISCUSSION
6.1 Task and Concept Accuracy
In Table 2, we compare our model against Concept Bottleneck
Models (CBMs) [ 18], which are trained without disentanglement
learning, and GlanceNets [ 23], which involves the learning of dis-
entangled factors, on the average task performance and concept
errors. We implement CBMs by replacing the VAE with a stan-
dard Autoencoder and the task learning module and the Aggre-
gation/Decomposition module are replaced by identity functions.
For GlanceNets, we provide supervision on not only the learned
concepts but also a part of disentangled latent space (represented as
GVin [23]). Differently, we do not utilize the Open-Set Recognition
Mechanism as we do not specifically study concept leakage.
As can be seen in Table 2, CoLIDR variants perform the best
among all concept-based models with disentanglement learning and
even outperforms CBM on 3 out of 4 datasets. The results show that
CBM performs better than GlanceNet and CoLiDR on the AWA2
dataset (0.531). A possible reason for this observation stems from
the fact that disentanglement performance on the AWA2 dataset is
not effectively captured by VAEs (Refer to Appendix) due to less
training data and significant noise in the dataset. Furthermore, the
performance of GlanceNet and CoLIDR is at par on dSprites and
Shapes3D implying that datasets that are ‚Äòeasier‚Äô to disentangle
yield similar performances across methods.
For the concept identification task, CoLiDR still achieves the
best performance with the lowest concept errors among all models
with disentanglement learning. Compared with the model without
disentanglement learning, Table 2 shows that our model performs
comparable and sometimes better than CBM on dSprtes, Shapes3D,
and CelebA datasets. For the AWA2 datasets, CBM outperforms
both CoLiDR and GlanceNet. We attribute this to the fact that
CBM can learn concepts very flexibly without the regularization of
disentanglement. However, this also leads to the problem of learning
spurious correlation instead of real semantics of concepts, as shown
in Appendix. Note that CoLiDR with no consistency regularization
(Lùëëùëüùëê) performs the worst on concept learning - implying strong
disentanglement performance is vital to concept learning.
6.2 Disentangled Representation Visualizations
Figures 4 and 5 respectively demonstrate the most important di-
mensions constituting a concept. In the Figure 4, we demonstrate
the concept ‚Äúbrown_hair‚Äù and its associated dimensions with scores
calculated using IG. For example, in Figure 4 GradCAM heatmap
visualizations on the two highest computed constituent dimen-
sions for a correctly identified concept ‚Äústraight_hair‚Äù (top) and
‚Äúwavy_hair‚Äù are shown. We see that both dimensions correctly cor-
respond to the distributions constituting the hair of the peoplein the image. Similarly, in Figure 5, the GradCAM visualizations
corresponding to the concepts ‚Äúheart‚Äù (left) and ‚Äúellipse‚Äù (right) are
shown. In addition, we also provide a linear interpolation of the
identified dimensions. As can be seen, the dimensions are indeed
responsible for the shapes of hearts and ellipses.
Concept: 
‚Äúwavy_hair‚Äù 
Concept:  
‚Äústraight_hair‚Äù Relative 
Importance: 
Relative 
Importance: 1.0 0.689 
1.0 0.481 
Figure 4: GradCAM visualizations of the top-2 highest attrib-
uted dimensions with respect to correctly predicted concepts
‚Äúwavy_hair‚Äù (top) and ‚Äústraight_hair‚Äù (bottom) on 2 distinct
samples from CelebA.
6.3 Comparision with Oracle Network
We partition a subset of concepts that can be easily understood in
the CelebA dataset and dub them ‚Äòsimple‚Äô (Refer to Appendix for the
exact splits). We report average IoU values for correctly classified
concepts on both ‚Äòsimple‚Äô and all concepts in Table 3. Column-2, 3
and 4 list the average IoUs for simple concepts across CBMs, CoLiDR
with only the top-2 and top-5 dimensions respectively. Heatmaps
produced by CBMs [ 24] show very low IoU values implying they
are not able to capture the spatiality of concepts well. On the other
hand, CoLIDR with ùõΩ-VAEs give much higher IoUs than CBMs -
demonstrating that CoLiDR captures the spatial nature of concepts
extremely well. The more dimensions visualized (two vs five), the
lower the IoU goes - implying that most of the spatial concept
information is encoded in only a few dimensions.
Oracle CBM/CEMCoLiDR
(dim=2)CoLiDR
(dim=5)
‚ÄòSimple‚Äô IoU 0.99 0.31 0.74 0.71
Overall IoU 0.99 0.24 0.63 0.62
Table 3: IoU values for GradCAM heatmaps generated using
CBMs (Column-2) as compared to CoLiDR utilizing the top-2
dimensions (Column-3) and top-5 dimensions (Column-4).
 
2706CoLiDR: Co ncept L earni ng using Aggregated D isentangled R epresentations KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Concept: 
‚Äúshape=heart‚Äù 
Concept:  
‚Äúshape=sphere‚Äù Relative Importance: 
Relative Importance: 1.0 0.77
1.0 0.81
Interpolation: 
Interpolation: 
Figure 5: GradCAM visualizations of the top-2 highest attrib-
uted dimensions for the correctly predicted concept of shape
for (top) d-sprites and (bottom) Shapes3D. The interpolation
along the highest contributing dimension shows that the
dimension effectively captures the shape of the object in the
image.
6.4 Test-time Intervention
Figure 6 shows the effect of intervening on a wrongly predicted
concept by its ground truth value. As the number of concepts vary
for each dataset, we plot the percentage of samples getting corrected
after intervention (y-axis) v/s percentage of concepts replaced by
their ground truth values (x-axis). We see that intervention is more
successful on datasets with fewer concepts.
Figure 6: Test-time intervention for all datasets. CoLiDR Per-
centage of samples correctly classified increases after inter-
vention as the percentage of concepts increases.
Additional results can be found in the Appendix.
Brown Hair Wavy Hair Arched 
Eyebrows 
Gray Hair High 
Cheekbones 
Figure 7: GradCAMs Heatmaps corresponding to the most
important dimensions of correctly predicted concepts of 3
distinct images belonging to test set of CelebA.
7 FUTURE WORK
CoLiDR is a highly generalizable framework that can be plugged
into any disentanglement architecture. In this paper we demon-
strate our results on Variational Autoencoders, however many more
complex architectures can be utilized for learning disentangled rep-
resentations. Generative Adversarial Networks are another class
of stochastic non-linear latent representation learning framework
which can be directly utilized as a DRL module. Another avenue of
exploration can go along the lines where concepts instead of being
independent of each other, are causally related - similar to [10].
8 CONCLUSION
In this paper, we propose CoLiDR, a novel interpretable concept-
based model that learns concepts using aggregated disentangled
representations. Empirical results demonstrate that CoLiDR bridges
the gap between concept learning and disentangled representation
learning by formatting human-understandable concepts as aggre-
gations of fundamental generative factors. The performance of
CoLiDR for task and concept prediction is on par with the concept-
focused models (CBM/CEM) that lack the explainability of gener-
ative factors. CoLiDR can flexibly learn complex concepts as an
aggregation of disentangled generative factors which improves its
performance on both task and concept learning. Using an aggre-
gation step, concepts learned using CoLiDR can be constituents
of multiple generative factors, giving a much finer overview of
interpretability.
ACKNOWLEDGEMENTS
This work is supported in part by the US National Science Founda-
tion under grants 2217071, 2213700, 2106913, 2008208, 1955151. Any
opinions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.
 
2707KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
REFERENCES
[1]David Alvarez Melis and Tommi Jaakkola. 2018. Towards robust interpretability
with self-explaining neural networks. Advances in neural information processing
systems 31 (2018).
[2]Mohammad Taha Bahadori and David E Heckerman. 2020. Debiasing concept
bottleneck models with instrumental variables. arXiv preprint arXiv:2007.11500
(2020).
[3]Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning: A review and new perspectives. IEEE transactions on pattern analysis
and machine intelligence 35, 8 (2013), 1798‚Äì1828.
[4]Chris Burgess and Hyunjik Kim. 2018. 3D Shapes Dataset.
https://github.com/deepmind/3dshapes-dataset/.
[5]Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. 2018.
Isolating sources of disentanglement in variational autoencoders. Advances in
neural information processing systems 31 (2018).
[6]Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sen-
gupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview.
IEEE signal processing magazine 35, 1 (2018), 53‚Äì65.
[7]Amirata Ghorbani, James Wexler, James Zou, and Been Kim. 2019. Towards
automatic concept-based explanations. Advances in Neural Information Processing
Systems (2019).
[8]Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. 2019. Explaining classifiers
with causal concept effect (cace). arXiv preprint arXiv:1907.07165 (2019).
[9]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770‚Äì778.
[10] Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. 2020. Causal
shapley values: Exploiting causal knowledge to explain individual predictions
of complex models. Advances in neural information processing systems 33 (2020),
4778‚Äì4789.
[11] Irina Higgins, Lo√Øc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot,
Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-
VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.
In5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https:
//openreview.net/forum?id=Sy2fzU9gl
[12] Jeya Vikranth Jeyakumar, Luke Dickens, Luis Garcia, Yu-Hsi Cheng,
Diego Ramirez Echavarria, Joseph Noor, Alessandra Russo, Lance Kaplan, Erik
Blasch, and Mani Srivastava. 2022. Automatic Concept Extraction for Concept
Bottleneck-based Video Classification. arXiv preprint arXiv:2206.10129 (2022).
[13] Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, and Mani
Srivastava. 2020. How can i explain this to you? an empirical study of deep
neural network explanation methods. Advances in Neural Information Processing
Systems (2020).
[14] Niki Kilbertus, Giambattista Parascandolo, and Bernhard Sch√∂lkopf. 2018. Gen-
eralization in anti-causal learning. arXiv preprint arXiv:1812.00524 (2018).
[15] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda
Viegas, et al .2018. Interpretability beyond feature attribution: Quantitative
testing with concept activation vectors (tcav). In International conference on
machine learning. PMLR, 2668‚Äì2677.
[16] Hyunjik Kim and Andriy Mnih. 2018. Disentangling by factorising. In Interna-
tional Conference on Machine Learning. PMLR, 2649‚Äì2658.
[17] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In
2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann
LeCun (Eds.). http://arxiv.org/abs/1312.6114
[18] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pier-
son, Been Kim, and Percy Liang. 2020. Concept bottleneck models. In International
Conference on Machine Learning. PMLR, 5338‚Äì5348.
[19] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. 2013. Attribute-
based classification for zero-shot visual object categorization. IEEE transactions
on pattern analysis and machine intelligence 36, 3 (2013), 453‚Äì465.
[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learning
face attributes in the wild. In Proceedings of the IEEE international conference on
computer vision. 3730‚Äì3738.
[21] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly,
Bernhard Sch√∂lkopf, and Olivier Bachem. 2019. Challenging common assump-
tions in the unsupervised learning of disentangled representations. In interna-
tional conference on machine learning. PMLR, 4114‚Äì4124.
[22] Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R√§tsch, Bernhard
Sch√∂lkopf, and Olivier Bachem. 2020. Disentangling Factors of Variations Using
Few Labels. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.
net/forum?id=SygagpEKwB
[23] Emanuele Marconato, Andrea Passerini, and Stefano Teso. 2022. Glancenets:
Interpretable, leak-proof concept-based models. Advances in NIPS 35 (2022),
21212‚Äì21227.[24] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik,
and Adrian Weller. 2021. Do Concept Bottleneck Models Learn as Intended?
arXiv preprint arXiv:2105.04289 (2021).
[25] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Ler-
chner. 2017. dSprites: Disentanglement testing Sprites dataset.
https://github.com/deepmind/dsprites-dataset/.
[26] Matthew O‚ÄôShaughnessy, Gregory Canal, Marissa Connor, Mark Davenport, and
Christopher Rozell. 2020. Generative causal explanations of black-box classifiers.
Advances in Neural Information Processing Systems (2020).
[27] Tejaswini Pedapati, Avinash Balakrishnan, Karthikeyan Shanmugam, and Amit
Dhurandhar. 2020. Learning Global Transparent Models Consistent with Local
Contrastive Explanations. Advances in Neural Information Processing Systems 33
(2020).
[28] Federico Pittino, Vesna Dimitrievska, and Rudolf Heer. 2021. Hierarchical Concept
Bottleneck Models for Explainable Images Segmentation, Objects Fine Classifica-
tion and Tracking. Objects Fine Classification and Tracking (2021).
[29] Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, and Mateja Jamnik. 2023.
Do Concept Bottleneck Models Obey Locality?. In XAI in Action: Past, Present,
and Future Applications.
[30] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. " Why should i
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining.
1135‚Äì1144.
[31] Yoshihide Sawada and Keigo Nakamura. 2022. Concept bottleneck model with
additional unsupervised concepts. IEEE Access 10 (2022), 41758‚Äì41765.
[32] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE interna-
tional conference on computer vision. 618‚Äì626.
[33] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang.
2022. Weakly supervised disentangled generative causal representation learning.
The Journal of Machine Learning Research 23, 1 (2022), 10994‚Äì11048.
[34] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020.
Weakly Supervised Disentanglement with Guarantees. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net. https://openreview.net/forum?id=HJgSwyBKvr
[35] Sanchit Sinha, Mengdi Huai, Jianhui Sun, and Aidong Zhang. 2023. Understanding
and enhancing robustness of concept-based models. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 15127‚Äì15135.
[36] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In International conference on machine learning. PMLR, 3319‚Äì
3328.
[37] Armeen Taeb, Nicol√≤ Ruggeri, Carina Schnuck, and Fanny Yang. 2022. Provable
concept learning for interpretable predictions using variational inference. arXiv
preprint arXiv:2204.00492 160 (2022).
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[39] Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R Lyu,
and Yu-Wing Tai. 2020. Towards Global Explanations of Convolutional Neural
Networks With Concept Attribution. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 8652‚Äì8661.
[40] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and
Pradeep Ravikumar. 2020. On completeness-aware concept-based explanations
in deep neural networks. Advances in Neural Information Processing Systems 33
(2020), 20554‚Äì20565.
[41] Mert Yuksekgonul, Maggie Wang, and James Zou. 2022. Post-hoc concept bottle-
neck models. arXiv preprint arXiv:2205.15480 (2022).
[42] Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra,
Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Frederic Precioso,
Stefano Melacci, Adrian Weller, et al .2022. Concept embedding models. arXiv
preprint arXiv:2209.09056 (2022).
 
2708CoLiDR: Co ncept L earni ng using Aggregated D isentangled R epresentations KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
A APPENDIX
A.1 Downstream Task Descriptions
A.1.1 D-Sprites/Shapes3D. We create 6 distinct binary tasks for
the D-Sprites dataset. Each task is created by splitting the training
set based on two factors. As the training set is randomly sampled,
the number of training points in each task is different. However,
the training set for each task is ensured to have balanced labels.
A.1.2 CelebA. For CelebA dataset, we utilize the labels formed
by clustering all the images into 10 clusters using the clustering
algorithm. The choice of the number of clusters are made using
TSNE visualization of the actual clusters. It is worth noting that we
utilize ten clusters instead of four in [ 23]. This important change
in task is made due to the fact that we utilize a different subset of
concepts (‚Äòsimple‚Äô concepts) to model CoLiDR. Table 4 lists the set
of concepts for which annotations are provided. We compute all
the experimental results on the simple set of concepts (left). The
simple concepts are chosen as they are objective in nature and can
be easily discerned by visual inspection of heatmaps.
A.1.3 AWA2. For AWA2 dataset, we keep the task the same as the
identity classification of 50 different identities. The original dataset
is annotated with 85 different binary concepts. The list of concepts
can be found1. For our experiments, we utilize a smaller subset
of 30 concepts. The subset is specifically chosen to only capture
concepts which are objective and human-understandable.
A.2 Implementation Details
A.2.1 Model Architecture. Disentangled Representations Learn-
ing (DRL) Module: Table 5 details the major components of both
architectures respectively. The standard VAE suffers from the well-
documented problem of [ 11] over-regularization of the disentangled
representations z, which hinders the quality of visualized recon-
structions ÀÜxdue to the information constraint imposed by the in-
dependent Gaussian priors. Hence, we evaluate CoLiDR on both
a standard VAE with ùõΩ=1and aùõΩ-VAE, where the values of ùõΩ
are tuned separately on each dataset. Specifically, we employ the
following values: dSprites: ùõΩ=0.025, Shapes3D: ùõΩ=0.025, CelebA:
ùõΩ=2.5ùëí‚àí5and AWA2: ùõΩ=1ùëí‚àí5.
Aggregation/Decomposition Module: The Aggregation module
Amaps between the latent representations zand concept set C
while the Decomposition module Dmaps the concept set Cto
the latent representations zusing a set of non linear transforms
{ùëéùëñ}and{ùëëùëñ}where i={0,..k}. We model each transformation as a
fully connected network mapping from a single dimension in the
latent space ( z) to the transformed latent space ( z‚Ä≤) and vice-versa.
The number of the intermediate layers is 2 for dSprites/Shapes3D
each of size 512, and the number of the intermediate layers is 4 for
CelebA/AWA2 each of size 512. The map between the transformed
latent space ùëß‚Ä≤andùê∂and vice versa - is modeled as a single feed-
forward layer.
Task Prediction Module: The task prediction module utilizes a
linear fully connected layer mapping from the annotated concepts
(Row-5 in Table 5) to the number of task labels (Row-7 in Table 5).
1https://cvml.ist.ac.at/AwA2/
Brown Hair Rosy Cheeks Narrow Eyes ORIGINAL 
CBM 
CoLiDR 
Concept Figure 8: GradCAMs Heatmaps corresponding to a test im-
age from CelebA. The Top Row shows heatmaps computed
on CBMs while the bottom row shows the heatmaps cor-
responding to the most important dimensions of correctly
predicted concepts using CoLiDR. As can be seen: heatmaps
from CoLiDR capture the spatiality of concepts much better
than CBMs.
The final predictions are performed after converting the model
outputs into probabilities using the softmax operator.
A.2.2 Training Hyperparameter Settings. Sequential Training:
To avoid posterior collapse, we first train the VAE without the
influence of any other loss term, i.e., ùúÜ1,ùúÜ2,ùúÜ3,ùúÜ4=0, only utilizing
the ELBO term (Equation 10). Subsequently, we only learn the
concept mappings, i.e., ùúÜ2=0as proposed in Sequential training
of [18]. Finally, we train the entire model architecture end-to-end
using carefully tuned hyperparameters ùúÜ1,ùúÜ2,ùúÜ3,ùúÜ4for each dataset.
The detailed values are given below:
‚Ä¢dSprites:ùúÜ1=0.5,ùúÜ2=0.2,ùúÜ3=1,ùúÜ4=0.1
‚Ä¢Shapes3D:ùúÜ1=0.5,ùúÜ2=0.2,ùúÜ3=1,ùúÜ4=0.1
‚Ä¢CelebA:ùúÜ1=10,ùúÜ2=0.08,ùúÜ3=0.1,ùúÜ4=0.01
‚Ä¢AWA2:ùúÜ1=5,ùúÜ2=0.05,ùúÜ3=0.1,ùúÜ4=0.01
Learning Rate and Scheduling: Training VAEs are notoriously
highly sensitive to the LR and optimizer. We utilize the Adam Opti-
mizer with an initial learning rate of 5e-3 for dSprites/Shapes3D,
1e-3 for CelebA and 5e-4 for AWA2. The batch sizes are 64 each
for dSprites and Shapes3D, while it is set at 32 for CelebA and 16
for AWA2. We train the entire model architecture for 100 epochs
with the first 50 epochs training the VAE, the subsequent 25 epochs
training the Aggregation/Decomposition module while the last 25
epochs train the entire model, end-to-end.
A.3 Additional Visual Results
Figure 8 demonstrates the highest attributed latent dimensions asso-
ciated with correctly classified concepts. In particular. we compare
the heatmaps generated using CoLiDR with the heatmaps generated
using CBMs. As can be seen in Figure 8, the heatmaps correspond-
ing to the most important dimensions of blond hair, narrow eyes,
and rosy cheeks correspond to their respective correct positions in
the image using CoLiDR as compared to CBMs. Similarly, Figure 9a
shows the heatmaps corresponding to correctly classified concepts
from the Shapes3D dataset. In addition, in Figure 9b, we provide
linear interpolation of the most contributing latent dimensions
corresponding to a concept.
 
2709KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang
Simple Concepts (20) All Concepts (40)
Arched_Eyebrows, Bangs, Big_Lips, 5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes,
Black_Hair, Blond_Hair, Brown_Hair, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair, Blond_Hair, Blurry,
Double_Chin, Goatee, Gray_Hair, Brown_Hair, Bushy_Eyebrows, Chubby, Double_Chin, Eyeglasses
Male, Mustache, Narrow_Eyes, Rosy_Cheeks, Goatee, Gray_Hair, Heavy_Makeup, Male,
Sideburns, Smiling, Straight_Hair, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face,
Wavy_Hair, Wearing_Earrings, Wearing_Hat Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns,
High_Cheekbones Smiling, Straight_Hair, Wavy_Hair, Wearing_Earrings, Wearing_Hat,
Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young
Table 4: List of Simple Concepts (left) and the entire concept set (Right) for the CelebA dataset
Shape Scale Orientation 
W all Floor Scale 
Shape Orientation Hue 
(a) GradCAMs Heatmaps corresponding to the most important dimen-
sions of correctly predicted concepts of 3 distinct images belonging
to test set of Shapes3D.
Object Hue 
Scale 
Shape (b) Linear interpolation of randomly sampled latent space corre-
sponding to images. The interpolated space corresponds well with
the concepts it contributes to the most.
Figure 9: Additional results on the Shapes3D dataset
Component dSprites/Shapes3D CelebA/AWA2
Encoder 3x 5x
[ Conv2D(filters, kernel=3, stride=2); [ Conv2D(filters, kernel=3, stride=2)
BatchNorm2D(); BatchNorm2D();
LeakyReLU()] LeakyReLU()]
filters in [32, 64, 64] filters in [32, 64, 128, 256, 512]
Latent Dim 64 512
Decoder 3x 5x
[ ConvTranspose2D, filters, kernel=3, stride=2; [ ConvTranspose2D, filters, kernel=3, stride=2;
BatchNorm2D(); BatchNorm2D();
LeakyReLU()] LeakyReLU()]
filters in [64, 64, 32] filters in [512, 256, 128, 64, 32]
Aggregation 64x [Linear(1,64); ReLU();Linear(64,64); ] 512x [Linear(1,512); ReLU(); Linear(512,512);
ReLU(); Linear(64,1) ReLU(); Linear(512,1)]
Concepts Annot = 6; Total = 64 Annot = 20 (CelebA), 30 (AWA2); Total = 512
Decomposition 64x [Linear(1,512); ReLU(); Linear(512,1)] 512x [Linear(1,512); ReLU(); Linear(512,512);
ReLU(); Linear(512,512)]
Task Labels 2 10 (CelebA), 50 (AWA2)
Table 5: CoLiDR module architecture details for the dSprites, Shapes3D datasets (Left) and CelebA, AWA2 datasets (Right). We
utilize identical Encoder and Decoder stacks and identical input/output image sizes across all datasets.
 
2710