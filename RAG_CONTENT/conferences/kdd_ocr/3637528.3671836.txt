Synthesizing Multimodal Electronic Health Records via
Predictive Diffusion Models
Yuan Zhong
The Pennsylvania State
University
University Park, PA, USA
yfz5556@psu.eduXiaochen Wang
The Pennsylvania State
University
University Park, PA, USA
xcwang@psu.eduJiaqi Wang
The Pennsylvania State
University
University Park, PA, USA
jqwang@psu.eduXiaokun Zhang
Dalian University of
Technology
Dalian, Liaoning, China
dawnkun1993@gmail.com
Yaqing Wang
Purdue University
West Lafayette, IN, USA
wang5075@purdue.eduMengdi Huai
Iowa State University
Ames, IA, USA
mdhuai@iastate.eduCao Xiao
GE Healthcare
Seattle, WA, USA
Cao.Xiao@gehealthcare.comFenglong Ma∗
The Penn State University
University Park, PA, USA
fenglong@psu.edu
Abstract
Synthesizing electronic health records (EHR) data has become a
preferred strategy to address data scarcity, improve data quality,
and model fairness in healthcare. However, existing approaches
for EHR data generation predominantly rely on state-of-the-art
generative techniques like generative adversarial networks, varia-
tional autoencoders, and language models. These methods typically
replicate input visits, resulting in inadequate modeling of temporal
dependencies between visits and overlooking the generation of time
information, a crucial element in EHR data. Moreover, their ability
to learn visit representations is limited due to simple linear map-
ping functions, thus compromising generation quality. To address
these limitations, we propose a novel EHR data generation model
called EHRPD . It is a diffusion-based model designed to predict the
next visit based on the current one while also incorporating time
interval estimation. To enhance generation quality and diversity,
we introduce a novel time-aware visit embedding module and a
pioneering predictive denoising diffusion probabilistic model ( P-
DDPM). Additionally, we devise a predictive U-Net (PU-Net) to
optimize P-DDPM. We conduct experiments on two public datasets
and evaluate EHRPD from fidelity, privacy, and utility perspectives.
The experimental results demonstrate the efficacy and utility of the
proposed EHRPD in addressing the aforementioned limitations and
advancing EHR data generation.
CCS Concepts
•Information systems →Data mining ;•Applied computing →
Health informatics; •Computing methodologies →Artificial
intelligence; Neural networks .
∗Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671836Keywords
Electronic Health Records, Medical Data Synthesis, Diffusion Mod-
els, Multimodal Data Mining
ACM Reference Format:
Yuan Zhong, Xiaochen Wang, Jiaqi Wang, Xiaokun Zhang, Yaqing Wang,
Mengdi Huai, Cao Xiao, and Fenglong Ma. 2024. Synthesizing Multimodal
Electronic Health Records via Predictive Diffusion Models. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671836
1 Introduction
In healthcare, the utilization of Electronic Health Records (EHR)
data is pivotal for advancing data-driven methodologies in both
research and clinical practice [ 34]. EHR data possess distinctive
characteristics, as illustrated in Figure 1, including sequential and
temporal visit records, irregular time intervals between consecutive
visits, and the presence of multiple modalities. However, effectively
harnessing such intricate data encounters a significant challenge
due to the scarcity of high-quality EHR datasets. To address this
challenge, the generation of EHR data becomes an essential solution,
providing a means to produce synthetic yet realistic supplements of
patient data for constructing robust healthcare application models.
Recent advancements in EHR data generation primarily depend
on cutting-edge generative techniques, such as generative adversar-
ial networks (GAN) [ 3,6,36], variational autoencoders (VAE) [ 4,8],
and language models (LM) [ 30,32]. These methodologies adhere to
a common pipeline, as depicted in Figure 2. This pipeline generally
includes an encoder to encode visit 𝑉𝑖into a representation v𝑖, a
generative model to generate the latent representation ˆv𝑖, and a
decoder to map ˆv𝑖to the generated visit ˆ𝑉𝑖. Despite their notable
performance achievements, they still encounter several limitations.
•Inadequate modeling of temporal dependencies between
visits. Real EHR data, as shown in Figure 1, comprises visits or-
dered in time, with inherent temporal dependencies among them.
However, existing methodologies employ a visit-replicating ap-
proach, generating a synthesis ˆ𝑉𝑖for the input 𝑉𝑖without explicitly
addressing the temporal relationships between visits. An optimal
generative model should inherently incorporate these temporal
characteristics, such as directly using the current visit 𝑉𝑖to gener-
ate the next visit 𝑉𝑖+1.
4607
KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
𝑉!"#,𝑇!"#𝑉!"$,𝑇!"$𝑉!,𝑇!⋯⋯……
250.00428.4
DiagnosisMedication…TimeModality120 days15 days
Figure 1: An example of multimodal EHR data, where 𝑉𝑖
denotes the visit information and 𝑇𝑖represents its time.
•Failure to simultaneously generate time intervals be-
tween visits. Current models aimed at generating detailed data
frequently overlook a crucial aspect of patient healthcare: time
information. As illustrated in Figure 1, time information is a vital
component of EHR data, playing a significant role in modeling
disease progression. Therefore, effectively capturing temporal de-
pendencies between visits necessitates incorporating the modeling
of time intervals between visits concurrently, enabling accurate
characterization of patient health trajectories.
•Limited capability in learning visit representations. Ow-
ing to the discrete nature of certain EHR modalities like diagnosis
codes, procedures, and medication codes, existing models embed
the input visit 𝑉𝑖into a continuous representation v𝑖first, as de-
picted in Figure 2. The quality of the generated EHR data directly
hinges on v𝑖. However, current methods only utilize simple linear
layers as the mapping function, potentially insufficient for rep-
resenting the complexity of EHR data. Hence, there is a need to
explore alternative approaches for learning visit representations.
•Lack of a robust generation model to balance data di-
versity and quality. From a modeling perspective, GAN-based
approaches encounter the issue of model collapsing during train-
ing [3,6,36], while VAE-based approaches rely on a strong Gaussian
assumption that may not align well with EHR data [ 4,8]. LM-based
approaches either depend on additional knowledge to generate
diverse data, making it difficult to control data quality [32], or uti-
lize autoregressive masked language training techniques to ensure
quality but sacrifice diversity [ 30]. None of the existing models
adequately address this challenging task. Therefore, the develop-
ment of a powerful and comprehensive EHR generation model is
urgently needed in healthcare.
To comprehensively address the aforementioned limitations, we
introduce EHRPD1in this paper, a diffusion-based model outlined
in Figure 3. Unlike existing approaches, EHRPD aims to capture the
temporal characteristic of EHR data by generating the next visit
ˆ𝑉𝑖+1based on the current visit 𝑉𝑖. Specifically, EHRPD takes multi-
modal EHR visit 𝑉𝑖={𝑀1
𝑖,···,𝑀𝑁
𝑖}as input, where 𝑁denotes the
number of modalities. Initially, EHRPD encodes the input 𝑉𝑖using the
designed time-aware visit embedding module, which facilitates
the modeling of fine-grained code appearance patterns concerning
time intervals when learning the visit embedding, denoted as v𝑖.
The learned visit embedding v𝑖is then utilized to generate the
latent representation of the subsequent visit ˆv𝑖+1via a novel pre-
dictive denoising diffusion probabilistic model (P-DDPM).
P-DDPM comprises three key processes – a forward noise addition
1EHRPD code repository: https://anonymous.4open.science/r/EHRPD-465B
EModelD!𝑉!𝑉!EncoderDecoder𝐯!$𝐯!EP-DDPMD!𝑉!"#𝑉!Existing WorkOur Work(EHRPD)𝐯!Encoder$𝐯!"#DecoderTime Interval Estimation: !𝚫𝒊Figure 2: Pipeline comparison between existing approaches
and our proposed EHRPD.
process, a backward denoising diffusion process, and a predictive
mapping process to encapsulate the temporality between visits
in each diffusion step. To learn the latent representation ˆv𝑖+1, we
integrate the backward denoising diffusion process with a novel
predictive U-Net (PU-Net). The obtained representation ˆv𝑖+1is
then employed to generate multimodal EHR data ˆ𝑉𝑖+1through the
decoding EHR prediction module. The catalyst representation
learning module is dedicated to estimating the time interval be-
tween𝑉𝑖and ˆ𝑉𝑖+1, as well as assembling the catalyst information
𝚽𝑖used in PU-Net, including demographics D, historical EHR
representation h𝑖, and the estimated time interval embedding ˆ𝚫𝑖.
In summary, the proposed EHRPD not only addresses the mod-
eling of temporality between visits but also facilitates the simul-
taneous estimation of time intervals. Furthermore, the introduced
time-aware visit embedding module can learn comprehensive visit
embeddings by explicitly capturing code appearance patterns. Addi-
tionally, the design of P-DDPM inherits the robustness of existing
DDPMs [ 15,37] while leveraging the diversity and quality of the
generated EHR data through the noise addition and denoising pro-
cess via the proposed PU-Net. Finally, extensive experiments are
conducted on MIMIC-III and Breast Cancer Trial datasets to validate
the proposed EHRPD from fidelity, privacy, and utility perspectives.
Experimental results demonstrate the superiority of EHRPD in EHR
generation.
2 Related Work
EHR Data Generation. To generate synthetic medical data to alle-
viate data scarcity, data generation methods in the medical domain
that are equipped with GAN [ 3,6,36,38], VAE [ 4,8], LM [ 30,32],
and DDPM [ 15,37] have shown great success from their debut.
Earlier methods [3, 6] perform visit-level code aggregation to pro-
duce one or a few feature vectors and generate synthetic ones with
GAN. However, this summarization would inevitably lose temporal
dynamics and lead to inferior performance. To address this prob-
lem, recent work [ 4,6,8,30,32,36] aims to generate EHR data
on the visit level. These fine-grained methods learn and leverage
the hidden visit-wise relationship in EHR data with sequential
learning techniques such as Tansformer, achieving state-of-the-art
results. However, these methods ignore the time information of
the patient’s visit that contains crucial information such as disease
progression and thus are suboptimal in their performance.
Denoising Diffusion Probabilistic Models. The diffusion model
has achieved considerable success in various tasks. One of its
primary applications is image generation, as demonstrated in
4608Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models KDD ’24, August 25–29, 2024, Barcelona, Spain
𝐡!"𝚫!Catalyst Representation LearningLSTMEncoder
+𝒱!"#𝒱!𝑀!"##𝑀!"#$𝑀!"#%…Decoder/𝐯!"#𝐯!𝐯!&
𝐯!&𝚽!PU-NetBackward Denoising Diffusion ProcessPredictive Denoising Diffusion Probabilistic Model (P-DDPM)Forward Noise Addition Process𝜖!𝑄𝐾𝑉𝑄𝐾𝑉Time-aware Visit Embedding
Multimodal EHR Prediction𝑀!#
𝑀!$
𝑀!%…
𝒅Demographics𝚽!Historical EHRRepresentationEstimated Time IntervalCatalyst InformationPredictive Mapping Process
Figure 3: Overview of the proposed EHRPD model.
works [ 16,25,26]. It can also be adapted to time series forecast-
ing and imputation [ 24,25,29]. Besides, the Discrete Diffusion
Model [1] adapts diffusion to discrete data space and fosters work
such as [ 10,17]. Specific to the medical domain, the diffusion model
has been used to generate healthcare data, including works such
as [11,15,22,37]. However, these methods are either task-oriented
or not designed for sequential EHR generation.
3 Methodology
This work is dedicated to the generation of realistic high-
dimensional, longitudinal, and multimodal Electronic Health Record
(EHR) data. Given the sequential temporality inherent in EHR data,
our objective is to simultaneously generate the next visit ˆ𝑉𝑖+1and
its associated time interval ˆΔ𝑖(i.e., ˆΔ𝑖=ˆ𝑇𝑖+1−𝑇𝑖), where ˆ𝑇𝑖+1
means the estimated time for ˆ𝑉𝑖+1. This generation process relies
on the entire historical visit sequence V1:𝑖=[(𝑉1,𝑇1),···,(𝑉𝑖,𝑇𝑖)]
in conjunction with demographic information D. Mathematically,
this is represented as:
{ˆ𝑉𝑖+1,ˆΔ𝑖}=𝑔(V1:𝑖,D), (1)
where𝑔(·)denotes the generation function. Each visit 𝑉𝑖=
{𝑀1
𝑖,···,𝑀𝑁
𝑖}encompasses 𝑁modalities, including diagnosis
codes, medication codes, lab test items, and so on.
To achieve this, we propose a novel diffusion-based EHR genera-
tion model, referred to as EHRPD , illustrated in Figure 3. This model
comprises four main modules: (1) time-aware visit embedding, (2)
predictive denoising diffusion probabilistic model, (3) catalyst rep-
resentation learning, and (4) multimodal EHR prediction. In the
following section, we provide a detailed explanation of each module.
3.1 Time-aware Visit Embedding
The easiest way to embed each visit 𝑉𝑖={𝑀1
𝑖,···,𝑀𝑁
𝑖}is applying
a linear mapping function for each modality 𝑀𝑛
𝑖on its modality-
level binary representation y𝑛
𝑖∈{0,1}|C𝑛|. However, this approach
ignores nuances of the code’s evolution against time. To address
this deficiency, we propose a time-aware visit embedding approach
to capture the temporal evolution of medical code individually.Time-aware Code Embedding. For the𝑗-th code𝑐𝑛,𝑗
𝑖that ap-
pears in the 𝑛-th modality, we record its most recent appearance
time, which is then subtracted by 𝑇𝑖to obtain the code-level time
gap denoted as 𝜏𝑛,𝑗
𝑖.𝜏𝑛,𝑗
𝑖=0for the first visit. A smaller 𝜏𝑛,𝑗
𝑖usually
indicates a higher importance level for time-aware code embedding
learning, which is described as follows:
c𝑛,𝑗
𝑖=𝜋𝑛,𝑗
𝑖MLP𝑐([e𝑛,𝑗
𝑖;𝝉𝑛,𝑗
𝑖])+( 1−𝜋𝑛,𝑗
𝑖)e𝑛,𝑗
𝑖, (2)
where[;]denotes the concatenation operation, e𝑛,𝑗
𝑖is the basic
code embedding, and 𝝉𝑛,𝑗
𝑖is the time gap embedding calculated
by the positional embedding used in Transformer. 𝜋𝑛,𝑗
𝑖is a gating
indicator to decide whether to incorporate time gap information
into code embedding learning, which is obtained via a Gumbel-
Softmax layer as follows:
𝜋𝑛,𝑗
𝑖=Binarize©­­
«exp
(log(p𝑛,𝑗
𝑖[0])+𝐺0)/𝜂
Í1
𝑦=0exp
(log(p𝑛,𝑗
𝑖[𝑦])+𝐺𝑦)/𝜂ª®®
¬,(3)
where p𝑛,𝑗
𝑖is the softmax layer output on top of a linear function on
the concatenated[e𝑛,𝑗
𝑖;𝝉𝑛,𝑗
𝑖],𝐺is the noise following the Gumbel
distribution, and 𝜂is a hyperparameter.
Visit Embedding. We use a modality-level attention mechanism
to learn the aggregated time-aware visit embedding as follows:
v𝑖=𝑁∑︁
𝑛=1𝜓𝑛
𝑖z𝑛
𝑖,
𝝍𝑖=Softmax
MLP𝜓([z1
𝑖;···;z𝑁
𝑖])
,
z𝑛
𝑖=ReLU©­
«MLP𝑧©­
«|C𝑛|∑︁
𝑗=1c𝑛,𝑗
𝑖ª®
¬ª®
¬.(4)
4609KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
3.2 Predictive Denoising Diffusion Probabilistic
Models (P-DDPM)
Existing diffusion-based models, like DDPM [ 12] and Glide [ 23],
achieve generation by reconstructing the original input data. How-
ever, the task of EHR generation differs significantly from other
tasks, as it aims to generate sequential, time-ordered EHR data
using Eq. (1)instead of reconstructing input data. To address this
distinction, we propose a novel approach called the Predictive De-
noising Diffusion Probabilistic Model (P-DDPM) to generate the
visit information 𝑉𝑖+1. Specifically, we treat the visit 𝑉𝑖as a reactant
and𝑉𝑖+1as the product. In generating 𝑉𝑖+1using𝑉𝑖, the aggregated
information fromV1:𝑖andDcan be treated as the catalyst in P-
DDPM. Thus, the proposed P-DDPM contains three components –
a forward noise addition process, a predictive mapping process, and
a backward denoising diffusion process – to tackle the challenges
associated with sequential EHR data generation effectively.
Forward Noise Addition Process. The forward noise addition
process is fixed to a Markov chain that gradually adds Gaussian
noise to the representation of 𝑉𝑖(i.e.,v𝑖orv0
𝑖detailed in Section 3.1
Eq. (4)) as follows:
𝑞(v1:𝑆
𝑖|v0
𝑖)=𝑆Ö
𝑠=1𝑞(v𝑠
𝑖|v𝑠−1
𝑖),
𝑞(v𝑠
𝑖|v𝑠−1
𝑖)=N(v𝑠
𝑖;√︁
1−𝛽𝑠v𝑠−1
𝑖,𝛽𝑠I),(5)
where𝑆is the number of diffusion steps, 𝑞(v1:𝑆
𝑖|v0
𝑖)is the ap-
proximate posterior, and 𝛽𝑠is the variance schedule at step 𝑠. Let
𝛼𝑠=1−𝛽𝑠and ¯𝛼𝑠=Î𝑠
𝑗=1𝛼𝑗, we can reparametrize the above
Gausssian steps in Eq. (5)to obtain the closed-form solution of v𝑠
𝑖
at any step𝑠without adding noise step by step as follows:
v𝑠
𝑖=√𝛼𝑠v𝑠−1
𝑖+√1−𝛼𝑠𝜖𝑠−1=√¯𝛼𝑠v0
𝑖+√1−¯𝛼𝑠𝜖, (6)
where𝜖∈N( 0,I). The details of the forward process can be found
in Appendix Section 6.1.1.
Predictive Mapping Process. To generate the next visit 𝑉𝑖+1us-
ing𝑉𝑖, we need to first model the relationship between these two
consecutive visits. In healthcare, such a relationship is usually mod-
eled by a disease progress function, which is equivalent to a map-
ping function to predict 𝑉𝑖+1using𝑉𝑖along with other information.
Mathematically, we define such a predictive mapping function 𝑓(·)
at each diffusion step as follows:
v𝑠
𝑖+1=𝑓(v𝑠
𝑖,𝚽𝑖), (7)
where 𝚽𝑖is the embedding of the aggregated information from V1:𝑖
andD(detailed in Section 3.3), which plays a role of the catalyst
during the generation.
Backward Denoising Diffusion Process. The backward denois-
ing diffusion process in existing diffusion models aims to reverse
the above forward process and sample from 𝑞(v𝑠−1
𝑖|v𝑠
𝑖)to recreate
the true sample v0
𝑖. Different from these approaches, our work aims
to generate the next visit’s representation, i.e., v0
𝑖+1, using v0
𝑖based
on their relationship modeled in Eq. (7). Mathematically, the reverseprocess of v0
𝑖+1can be formulated as follows:
𝑞(v𝑠−1
𝑖+1|v𝑠
𝑖+1,v0
𝑖+1)=𝑞(v𝑠
𝑖+1|v𝑠−1
𝑖+1,v0
𝑖+1)𝑞(v𝑠−1
𝑖+1|v0
𝑖+1)
𝑞(v𝑠
𝑖+1|v0
𝑖+1),
𝑞(v𝑠−1
𝑖+1|v𝑠
𝑖+1,v0
𝑖+1)=N(v𝑠−1
𝑖+1;ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1),ˆ𝛽𝑠I),(8)
By simplifying Eq. (8)according to the Gaussian distribu-
tion’s density function, we can obtain the variance ˆ𝛽𝑠and mean
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)of𝑞(v𝑠−1
𝑖+1|v𝑠
𝑖+1,v0
𝑖+1)as follows:
ˆ𝛽𝑠=1−¯𝛼𝑠−1
1−¯𝛼𝑠𝛽𝑠,
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)=√𝛼𝑠(1−¯𝛼𝑠−1)
1−¯𝛼𝑠v𝑠
𝑖+1+√¯𝛼𝑠−1𝛽𝑠
1−¯𝛼𝑠v0
𝑖+1.(9)
Recall in the forward process, we have obtained v𝑠
𝑖+1=√¯𝛼𝑠v0
𝑖+1+√1−¯𝛼𝑠𝜖in Eq. (6). Thus, the mean value of the closed-form solution
to the backward diffusion process can be obtained by substituting
v0
𝑖+1in Eq. (9) as follows:
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)=1√𝛼𝑠(v𝑠
𝑖+1−1−𝛼𝑠√1−¯𝛼𝑠𝜖𝑠). (10)
By substituting v𝑠
𝑖+1in Eq. (10)with the predictive mapping pro-
cess in Eq. (7), we finally have the closed-form solution as follows:
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)=1√𝛼𝑠(𝑓(v𝑠
𝑖,𝚽𝑖)−1−𝛼𝑠√1−¯𝛼𝑠𝜖𝑠). (11)
The details of the derivation of the backward reverse process
can be found in Appendix Section 6.1.3.
P-DDPM Learning. We typically use a U-Net with parameters 𝜃
to train the proposed P-DDPM by approximating Eq. (11), i.e.,
𝝁𝑠
𝜃(v𝑠
𝑖+1,𝑠)=1√𝛼𝑡(𝑓(v𝑠
𝑖,𝚽𝑖)−1−𝛼𝑡√1−¯𝛼𝑡𝜖𝜃(𝑓(v𝑠
𝑖,𝚽𝑖),𝑠)).(12)
Different from conventional U-Net architecture which only takes
the noised embedding as input, we design a new predictive U-Net
(PU-Net) equipped with the capability to condition on 𝚽𝑖during
the generation process. PU-Net also contains two paths of learning
– the downsampling path and the upsampling path.
PU-Net takes v𝑠
𝑖as the input of the first layer. Then, at each
layer𝑙, the downsampling operations include a ResNet block with
a 1-D convolution operation to generate the input of layer 𝑙+1as
follows:
v𝑠
𝑖,𝑙+1=Conv(ResBlock(v𝑠
𝑖,𝑙)). (13)
The upsampling path at the 𝑙-th layer consists of an informa-
tion aggregator, a ResNet block, and a deconvolutional (DeConv)
operation to reconstruct the input. The information aggregator is
a self-attention block (SelfAtt) to fuse the embeddings of v𝑠
𝑖,𝑙and
the transformed catalyst embedding 𝚽𝑖,𝑙by a linear function on 𝚽𝑖.
The upsampling operation can be formulated as follows:
ˆv𝑠
𝑖+1,𝑙=DeConv(ResBlock(ˆv𝑠
𝑖+1,𝑙+1,SelfAtt(v𝑠
𝑖,𝑙,𝚽𝑖,𝑙))).(14)
Figure 4 shows the designed PU-Net; the detailed derivation can
be found in Appendix Section 6.2.
P-DDPM Reconstruction Loss. Letˆv𝑖+1=ˆv𝑠
𝑖+1,0denote the out-
put of PU-Net that is trained on a randomly selected diffusion
step𝑠∈[1,···,𝑆]. The objective function of PU-Net is the mean
4610Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models KDD ’24, August 25–29, 2024, Barcelona, Spain
:Resnet Block:Downsample Block:Upsample Block:Self attentionΦ!
:Compressing MLP𝐯!,#$𝐯!,%$𝐯!,%&'$𝐯!,%&($#𝐯!&',%&($#𝐯!&',%&'$#𝐯!&',%$#𝐯!&',#$#𝐯!&'⋯⋯⋯
⋯
Figure 4: Illustration of PU-Net.
squared errors between the generated ˆv𝑖+1and the learned embed-
ding v𝑖+1in Section 3.1 at each training epoch as follows:
L𝑑(𝑉𝑖)=1
𝑑𝑣||ˆv𝑖+1−v𝑖+1||2, (15)
where𝑑𝑣is dimension size of ˆv𝑖+1, and the learned visit embedding
v𝑖+1can be treated as ground truths.
Multimodal EHR Prediction. The predicted embedding ˆv𝑖+1can
also be used to predict medical codes in each modality. Specifi-
cally, for each modality 𝑀𝑛
𝑖, we use a linear layer to map ˆv𝑖+1to a
modality-level representation, and then a Sigmoid function is used
to predict the probability of a medical code on top of a multilayer
perceptron (MLP) as follows:
ˆy𝑛
𝑖+1=Sigmoid(MLP𝑦(ˆv𝑖+1)), (16)
Finally, we can use a Focal loss to train the multimodal EHR predic-
tor as follows:
L𝑒(𝑉𝑖)=−1
𝑁𝑁∑︁
𝑛=11
|C𝑛||C𝑛|∑︁
𝑗=1[𝑦𝑛,𝑗
𝑖+1𝜅(1−ˆ𝑦𝑛,𝑗
𝑖+1)𝛾log(ˆ𝑦𝑛,𝑗
𝑖+1)
+(1−𝑦𝑛,𝑗
𝑖+1)(1−𝜅)(ˆ𝑦𝑛,𝑗
𝑖+1)𝛾log(1−ˆ𝑦𝑛,𝑗
𝑖+1)],(17)
where|C𝑛|denotes the number of identical codes in each modality
𝑀𝑛
𝑖,𝑦𝑛,𝑗
𝑖+1is a binary ground truth to indicate whether the 𝑗-th code
𝑐𝑛,𝑗
𝑖+1of the𝑛-th modality presents in visit 𝑉𝑖+1, and𝜅and𝛾are
hyperparameters.
To optimize Eqs. (15)and(17), we need to obtain the catalyst
representation 𝚽𝑖. Next, we introduce the details of catalyst repre-
sentation learning in Section 3.3.
3.3 Catalyst Representation 𝚽𝑖Learning
As discussed in Section 3.2, the catalyst information is significantly
important in the proposed EHRPD during the generation with P-
DDPM, which “translates” the information from v𝑠
𝑖tov𝑠
𝑖+1at each
diffusion step 𝑠via Eq. (7). Next, we explain how we construct the
catalyst representation 𝚽𝑖.
EHR Historical Information Representation. In clinical prac-
tice, professionals often rely on a patient’s historical medical records
for a comprehensive view of their past health issues and as a crucialtool for informed decision-making. These records offer a timeline
of medical events, treatments, and diagnoses that provide insights
into the patient’s health trajectory and are useful for predicting
future health scenarios. Thus, we incorporate historical medical
information as one of the conditioning factors to aid our generation
process. Based on the learned visit embedding using Eq. (4), we
utilize an LSTM network to accumulate a hidden state h𝑖for each
visit as follows:
h𝑖=LSTM(h𝑖−1,v𝑖). (18)
Time Interval Estimation. Not only do clinical professionals di-
agnose a patient’s health condition, but they also make a crucial
decision in determining the optimal timing for the follow-up visit
that best suits the current health condition of the patient. This
decision is often to cope with the urgency and nature of the pa-
tient’s condition: patients suffering from acute illnesses may need
to return within a matter of days, while those with chronic diseases
revisit with a more prolonged and periodic pattern. In our approach,
we use the current health condition h𝑖to make predictions on the
time interval till the next follow-up visit with a continuous time
LSTM [ 21], as shown in Eq. (19). We utilize a linear layer on hidden
state h𝑖to learn an intensity measure 𝝀𝑖of the current visit, which
represents a patient’s medical urgency. This intensity is subtracted
from 1, giving a close to 0output if the patient’s condition is urgent.
Then, the second equation predicts the time gap and ensures it is
strictly above 0with the Softplus activation function:
𝝀𝑖=1−tanh(MLP𝜆(h𝑖)),
ˆΔ𝑖=Softplus(MLPΔ(𝝀𝑖)).(19)
Demographic Information Embedding. Demographic informa-
tion is also treated as a key factor in decision-making. Thus, we
encode the demographic information Dinto a dense representation
dusing an MLP layer, i.e., d=MLP𝑑(D).
Catalyst Representation Learning. Finally, the catalyst repre-
sentation 𝚽𝑖=[h𝑖;ˆ𝚫𝑖;d]is obtained by concatenating the his-
torical representation h𝑖, the embedding of time interval through
the positional embedding on ˆΔ𝑖(i.e., ˆ𝚫𝑖), and the demographic
embedding d.
3.4 Time Interval Estimation Loss
The proposed EHRPD generates not only the next visit informa-
tion but also the time interval between visits 𝑉𝑖and𝑉𝑖+1. We take
another MSE lossLtimebetween the real-time gap Δ𝑖and the pre-
dicted time gap ˆΔ𝑖using Eq. (19) as follows:
L𝑡(𝑉𝑖)=(Δ𝑖−ˆΔ𝑖)2, (20)
3.5 EHRPD Loss
Finally, we define the total loss Lof a patient with|V|visits as the
weighted sum of all three loss components by 𝜔𝑑,𝜔𝑒, and𝜔𝑡, as
follows:
L=1
|V|− 1|V|− 1∑︁
𝑖=1(𝜔𝑑L𝑑(𝑉𝑖)+𝜔𝑒L𝑒(𝑉𝑖)+𝜔𝑡L𝑡(𝑉𝑖)).(21)
4 Experiment
Due to the limited space, we put more results in the Appendix.
4611KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
MIMIC-III Breast Cancer Trial
Total Patients 46,520 Total Patients 970
Diagnosis 1,071 Adverse Events 50
Drug Codes 1,439 Medications 100
Lab Items 710 Lab Categories 9
Procedure Codes 711 Treatments 4
Table 1: Statistics of two main datasets.
4.1 Datasets
We use two publicly available datasets to validate the performance
of the proposed EHRPD , including MIMIC-III [ 14] and Breast Cancer
Trial2from Project Data Sphere. The statistics of these two datasets
are listed in Table 1. For the MIMIC-III dataset, we extract all
46,520patients’ diagnoses, prescriptions, lab items, and procedure
codes as four modalities of interest. For the Breast Cancer Trial
dataset, we mainly follow the data preprocessing procedure de-
scribed in TWIN [ 8], extracting adverse events, medications, lab
categories, and treatment codes. For both datasets, each patient’s
EHR data is represented by a sequence of admissions ordered by
admission time, where each admission consists of four lists of codes
from each modality accordingly. Lastly, we add demographic infor-
mation, such as sex, age, race, etc., as a static feature vector. We
randomly split each dataset into train, validation, and test sets, with
a ratio of 75 : 10 : 15.
4.2 Implementation Details
Our model is implemented in PyTorch and trained on an NVIDIA
RTX A6000 GPU. We use the Adam optimizer with learning rate
and weight decay both set to 10−3. We set the Focal Loss parameter
in Eq. (17)to𝜅=0.75and𝛾=5. For the total EHRPD loss in Eq. (21),
we set𝜔𝑑=0.5,𝜔𝑒=1000, and𝜔𝑡=0.01. The dimension of
h,v,∆, and dare set to 256, and the PU-Net dimension list is
[1024,512,256].
4.3 Fidality Assessment
We perform experiments to evaluate the generated data quality
with two evaluation metrics and various baseline models, empha-
sizing the temporal coherence and cross-modality consistency of
the generated data.
4.3.1 Baselines. Our selected baseline models include MLP, GAN-
based models (medGAN [ 3] and synTEG [ 36]), VAE-based mod-
els (EVA [ 4] and TWIN [ 8]), diffusion-based approaches (TabD-
DPM [ 15], Meddiff [ 11], and ScoEHR [ 22]), and language model-
based approaches (PromptEHR [ 32] and HALO [ 30]). Appendix
Section 6.3 describes each model’s detailed explanation.
4.3.2 Experiment Design and Evaluation Metrics. In this exper-
iment, we use MIMIC-III and the Breast Cancer Trial as input
databases separately. Each model produces a synthetic dataset cor-
responding to the original one, maintaining a 1:1 ratio. To assess the
effectiveness of these EHR generation models, we focus on the fol-
lowing evaluation metrics. Longitudinal Imputation Perplexity
(LPL) is a specialized metric used to evaluate EHR generation mod-
els. This metric adapts the traditional concept of perplexity from
2Clinical Trial ID: NCT00174655, https://www.projectdatasphere.org/natural language processing to suit the unique temporal structure
of EHR data. The LPL metric effectively captures the model’s ability
to predict the sequence of medical events over time, considering
the chronological progression of a patient’s health condition. In
contrast to the LPL, which concentrates on the temporal coherence
within a single modality, Cross-modality Imputation Perplex-
ity (MPL) extends this concept to encompass the interrelations
among different modalities, by assessing the model’s proficiency in
integrating and predicting across various types of data modalities,
making it a more comprehensive measure of a model’s ability to
handle the multifaceted nature of EHR data.
4.3.3 Experimental Results. Table 2 shows the experimental results
on the LPL and MPL metrics of all models tested on each of the
data sources. Lower score, indicates better model performance. On the
MIMIC-III dataset, our proposed model consistently outperforms
other models across all four modalities in both LPL and MPL met-
rics. For instance, in the Diagnosis modality, our model achieves the
best LPL score of 15.97 and MPL score of 17.95, significantly better
than the next best baseline model, TWIN, which scored 26.28 and
27.68 in LPL and MPL, respectively. Though PromptEHR slightly
outperforms our model in the Lab Category modality within the
Breast Cancer Trial dataset, its performance across other modalities
is less consistent. This variation indicates that while PromptEHR
can be effective in certain scenarios, its output generally exhibits
greater variability and less reliability compared to our model. Such
inconsistency can lead to diminished effectiveness in diverse medi-
cal data scenarios, underlining our model’s superior adaptability
and robustness. Overall, our model’s consistent performance across
various metrics and modalities reinforces its effectiveness and broad
applicability in medical data generation.
4.4 Privacy Assessment
We also evaluate the privacy-preserving capability of our model
against other generation baseline models. The privacy-preserving
capability is how likely the generated data can be traced back to
the original data. We conduct our experiments with the Presence
Disclosure Sensitivity metric.
4.4.1 Experiment Design and Evaluation Metric. We start with a
predefined percentage of patient records from the training set, la-
beling them as “known ” or “compromised ”. The aim is to identify
these known records within the generated dataset. If the 𝑖-th visit
of a patient is matched back to one of the synthetic visits generated
by this patient by similarity score, we count it as a successful attack.
We use the metric Presence Disclosure Sensitivity (PD) [8] to
evaluate the security of our datasets. PD is the proportion of known
patient records correctly matched in the generated dataset against
the total number of compromised records. The lower the PD value,
the better the security performance. This metric effectively gauges
the risk of individual patient identification in the generated dataset,
serving as an indicator of the dataset’s privacy and data protection
capabilities.
4.4.2 Experimental Results. Table 3 shows the experiment results
on MIMIC-III in terms of PD with varying percentages of known
patient records, ranging from 10% to 50%. Our analysis reveals that
our model consistently outperforms the baseline models across all
4612Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models KDD ’24, August 25–29, 2024, Barcelona, Spain
Dataset Mo
dality Metric MLP me
dGAN synTEG EV
A T
WIN T
abDDPM Me
ddiff ScoEHR Pr
omptEHR HALO EHRPDMIMIC-IIIDiagnosisLPL 325.55 242.30 36.87 29.62 26.28 108.79 664.54 685.17 126.23 149.66 15.97
MPL 352.54 257.48 45.61 31.63 27.68 114.18 670.91 691.55 128.05 192.13 17.95
DrugLPL 553.63 403.02 83.38 43.79 40.94 179.22 936.28 934.87 167.48 166.11 20.53
MPL 551.67 405.75 82.66 44.02 40.86 178.70 936.14 950.27 136.04 202.01 19.15
Lab
ItemLPL 168.25 77.10 26.80 20.05 17.47 54.69 413.41 432.11 107.22 322.51 15.11
MPL 166.61 87.12 30.34 19.97 17.41 54.44 412.33 431.09 98.52 303.09 13.99
Pr
ocedureLPL 290.38 234.81 49.33 27.39 21.26 98.03 471.81 486.81 51.18 22.68 14.53
MPL 286.53 245.28 44.00 30.49 24.26 102.72 479.96 499.89 31.13 39.04 18.89Br
east Cancer TrialA
dverse EventLPL 8.42 8.00 8.21 6.08 6.08 9.31 49.04 51.82 12.37 34.83 5.96
MPL 9.37 9.42 9.70 8.30 8.52 10.86 50.13 51.57 12.14 31.51 8.02
Me
dicationLPL 8.82 9.53 8.21 5.39 5.56 11.13 99.35 99.31 19.34 31.22 4.96
MPL 8.73 11.67 10.08 6.95 7.10 12.95 98.83 99.10 19.80 33.61 5.87
Lab
CategoryLPL 9.33 10.41 9.63 9.07 9.09 9.06 10.95 10.93 8.55 9.14 9.01
MPL 9.22 10.08 10.03 9.09 9.11 9.03 10.96 10.97 8.66 9.28 9.09
T
reatmentLPL 7.29 9.43 9.09 3.09 3.12 3.67 4.77 5.01 5.10 3.44 2.63
MPL 4.47 4.83 4.43 2.89 2.92 3.22 4.84 5.00 5.63 3.05 2.41
Table 2: EHR data generation evaluation of different approaches on two datasets with two metrics.
Approach 10% 20% 35% 50%
MLP 13.53 13.38 13.03 13.07
medGAN 17.06 17.19 17.56 17.79
synTEG 13.21 13.02 12.71 12.76
EVA 13.36 13.17 12.84 13.36
TWIN 13.36 13.16 12.84 12.89
TabDDPM 13.45 13.66 13.50 13.68
Meddiff 14.94 17.00 18.35 19.18
ScoEHR 14.12 15.56 16.44 16.91
PromptEHR 14.44 12.86 12.90 13.31
HALO 13.52 13.79 13.88 13.79
EHRPD 12.60 12.77 12.53 12.25
Table 3: Privacy assessment on MIMIC-III with different per-
centages of known patients under the metric PD.
tested scenarios. Notably, as the percentage of known patients in-
creases, our model maintains its effectiveness in protecting patient
privacy. For instance, at 10% known patients, our model achieves a
Presence Disclosure Sensitivity of 12.60, and even with 50% known
patients, it has the lowest metric of 12.25. This demonstrates a
robust defense against privacy breaches, even as the challenge es-
calates with more known patient records. In comparison, other
models like medGAN, TabDDPM, and PromptEHR show higher
sensitivity, indicating a greater risk of patient identification in their
generated datasets. For example, medGAN’s sensitivity ranges from
17.06 to 17.79, which is significantly worse than ours. These results
underscore the effectiveness of our model in ensuring the privacy
and protection of patient data.
4.5 Utility Assessment
We experiment with the utility of the generated dataset from two
databases on various downstream tasks under both multimodal
and unimodal settings. We also conduct experiments to assess the
effectiveness of the time gap prediction module of our model.
4.5.1 Data Preprocessing. We follow the FIDDLE [ 28] guidelines
for data preprocessing and adapt their label definitions to process
the MIMIC-III database, focusing on three critical health outcomes
of a multimodal setting: Acute Respiratory Failure (ARF), Shock,and Mortality. Additionally, we employ another data preprocessing
method from Retain [ 7] to obtain diagnosis codes for heart failure
risk prediction, demonstrating our model’s effectiveness in an uni-
modal context. Furthermore, following the work of TWIN [ 8], we
select patients with severe outcomes and death as positive labels.
4.5.2 Multimodal Risk Prediction Analysis. To evaluate the qual-
ity of synthetic data generated by our approach, we designed an
experiment to determine whether integrating synthetic data into
the training process enhances the performance of downstream
task-oriented models. We take all four time-series modalities and
stationary demographic information as input features to conduct
multimodal risk prediction experiments on acute respiratory fail-
ure (ARF), Shock, and Mortality datasets. We choose the following
models as baselines: F-LSTM [ 28], F-CNN [ 28], RAIM [ 35], and
DCMN [ 9], and three evaluation metrics: AUPR (the area under
the Precision-Recall curve), F1 and Kappa, following [ 31].Baseline
models and the results on ARF and Shock are explained in Appendix
Section 6.4 and Section 6.7, respectively.
For each dataset, models are trained using either only original
data or a blend of synthetic and original data at a 1:1 ratio. The
results, detailed in Table 4, reveal that our method, EHRPD , con-
sistently outperforms baseline models under most conditions. No-
tably, under the Mortality task with the F-CNN architecture, EHRPD
demonstrates a 2% improvement in both AUPR and F1 metrics
compared to baselines. In contrast, the HALO model shows supe-
rior performance in specific metrics when paired with the F-LSTM
architecture, suggesting a particularly effective synergy between
HALO’s synthetic data and the F-LSTM model. Overall, these re-
sults imply our model’s potential to provide reliable synthetic data
to augment multimodal risk prediction models.
4.5.3 Unimodal Risk Prediction Analysis. To simulate a scenario
where multimodal data are unavailable, we conduct the following
unimodal risk prediction task on Heart Failure disease with diagno-
sis code only. The backbone risk prediction models are LSTM [ 13],
Dipole [ 19], Retain [ 7], AdaCare [ 20], and HiTANet [ 18], and are
explained in Appendix Section 6.5. We utilize the same evaluation
metric and synthetic-real data ratio as the multimodal experiment.
4613KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
Model F-LSTM F-CNN RAIM DCMN
Metric AUPR F1 Kappa AUPR F1 Kappa AUPR F1 Kappa AUPR F1 Kappa
Orginal 0.5710 0.4705 0.4221 0.5810 0.5132 0.4554 0.5849 0.5000 0.4280 0.5438 0.4742 0.4298
MLP 0.6344 0.5408 0.4747 0.6614 0.5882 0.4950 0.6226 0.5571 0.4819 0.5733 0.4975 0.4245
medGAN 0.6210 0.5685 0.4946 0.6563 0.6098 0.5337 0.6159 0.5455 0.4789 0.5668 0.5473 0.4793
synTEG 0.6309 0.5556 0.4815 0.6597 0.6026 0.5220 0.6490 0.5891 0.5185 0.5804 0.5674 0.4958
EVA 0.6313 0.5572 0.4907 0.6487 0.5703 0.4897 0.6366 0.5438 0.4890 0.5585 0.5076 0.4326
TWIN 0.6410 0.5503 0.4846 0.6642 0.5929 0.5412 0.6469 0.5876 0.5292 0.6283 0.5687 0.4992
TabDDPM 0.6489 0.5586 0.4939 0.6534 0.5672 0.5022 0.6228 0.5572 0.4858 0.5428 0.5112 0.4354
Meddiff 0.6337 0.5502 0.4823 0.6163 0.5504 0.4636 0.6161 0.5289 0.4597 0.5594 0.4969 0.4186
ScoEHR 0.6408 0.5438 0.4812 0.6392 0.6033 0.5272 0.5949 0.4964 0.4191 0.6089 0.5333 0.4594
PromptEHR 0.6580 0.5677 0.5041 0.6682 0.6079 0.5383 0.6419 0.5648 0.4923 0.6279 0.6036 0.5351
HALO 0.6673 0.5547 0.4885 0.6139 0.5234 0.4562 0.5812 0.4779 0.4119 0.6124 0.5746 0.4957
EHRPD 0.6658 0.5870 0.5251 0.6835 0.6159 0.5425 0.6548 0.5936 0.5327 0.6385 0.6147 0.5448
Table 4: Result evaluation of the Mortality task on multimodal EHR data.
Backb
one A
dacare Dip
ole HiT
ANet LSTM Retain
Metric AUPR
F1 Kappa AUPR
F1 Kappa AUPR
F1 Kappa AUPR
F1 Kappa AUPR
F1 Kappa
Orginal 0.6242
0.6136 0.3627 0.5856
0.5740 0.3349 0.6203
0.5978 0.3740 0.5943
0.5758 0.3461 0.5989
0.5913 0.3720
MLP 0.6640
0.6376 0.4185 0.6872
0.6470 0.4511 0.6692
0.6562 0.4488 0.6706
0.6374 0.4460 0.6476
0.6279 0.4158
medGAN 0.6669
0.6400 0.4089 0.6915
0.6371 0.4490 0.6781
0.6492 0.4376 0.6667
0.6332 0.4431 0.6424
0.6285 0.4028
synTEG 0.6711
0.6121 0.4142 0.6820
0.6215 0.4174 0.6851
0.6607 0.4641 0.6676
0.6312 0.4353 0.6319
0.6285 0.4224
EVA 0.6590
0.6424 0.4189 0.6795
0.6527 0.4513 0.6813
0.6511 0.4372 0.6629
0.6307 0.4267 0.6527
0.6240 0.4208
TWIN 0.6739
0.6252 0.4328 0.6603
0.6409 0.4406 0.6789
0.6690 0.4274 0.6546
0.6264 0.4162 0.6540
0.6382 0.4187
TabDDPM 0.6677
0.6243 0.3877 0.6851
0.6342 0.4312 0.6633
0.6581 0.4480 0.6687
0.6317 0.4301 0.6465
0.6152 0.4067
Meddiff 0.6659
0.6323 0.4171 0.6756
0.6249 0.4190 0.6684
0.6301 0.4277 0.6672
0.6188 0.4119 0.6591
0.6204 0.4161
ScoEHR 0.6701
0.6395 0.4284 0.6719
0.6296 0.4117 0.6774
0.6340 0.4238 0.6624
0.5980 0.3966 0.6469
0.6282 0.4166
PromptEHR 0.6810
0.6462 0.4100 0.6748
0.6359 0.4334 0.6541
0.6182 0.4076 0.6642
0.6222 0.4178 0.6582
0.6251 0.4211
HALO 0.6742
0.6312 0.4295 0.6907
0.6562 0.4604 0.6841
0.6578 0.4489 0.6619
0.6301 0.4252 0.6518
0.6266 0.4196
EHRPD 0.6856
0.6523 0.4385 0.7018
0.6630 0.4735 0.7017
0.6777 0.4699 0.6824
0.6484 0.4506 0.6603
0.6397 0.4382
Table 5: Result evaluation on Heart Failure prediction task on unimodal EHR data.
The results of these experiments in Table 5 show our model
outperforms other generation models. Compared to the best-
performing model PromptEHR with Adacare, our model achieves
a 3% higher Kappa. One notable point is that to make a fair com-
parison between baseline models, the real data does not contain
the time interval between visits. Thus, backbone methods that rely
on learning time information, such as HiTANet, do not perform
optimally. However, we can see that when our model provides extra
time information in the synthetic dataset, HiTANet’s performance
greatly increases and is better than others: the AUPR and F1 of
HiTANet rise by 8%. This experiment not only underscores our
model’s effectiveness in generating high-quality unimodal data but
also demonstrates its unique capability to enrich synthetic datasets
with critical temporal information, thereby offering comprehensive
support for advanced predictive analytics.
4.5.4 Time Interval Prediction. While the previous experiment im-
plicitly confirmed our model’s time interval prediction capability
and effectiveness for downstream risk prediction, we directly com-
pare ours to a range of established time-series forecasting baselines
in this experiment. The task is defined to use historical time stamps
till𝑇𝑖to predict𝑇𝑖+1and will include 𝑉𝑖if the model structure al-
lows. The selected baseline methods include the Autoregressive
Integrated Moving Average (ARIMA) [ 5], Support Vector Regres-
sion (SVR) [ 2], Gradient Boosting Regression Trees (GBRT) [ 27],
ARIMAKFSVRGBRTLSTMEHRPDRMSE331.13524.81146.22319.8984.1476.730100200300400500600RMSEFigure 5: Illustration of time interval prediction with RMSE.
Kalman Filter (KF) [ 33], and LSTM [ 13], which are explained in
Appendix Section 6.6. We use Root Mean Squared Error (RMSE) as
the metric. The less the RMSE, the better the fit of the model.
We visualize the results in Figure 5, with red-colored bars rep-
resenting baseline models and blue representing ours, as well as
RMSE values on the bottom. We can see that timestamp-only meth-
ods do not perform well. The conventional LSTM performs closely
toEHRPD , while EHRPD shows the best performance with the low-
est RMSE. This experiment demonstrates our model’s ability to
integrate various additional information and deliver more accurate
predictions on the intervals leading up to the next patient visit.
4.5.5 Severe Outcome Prediction. In the healthcare domain,
datasets often have a smaller scale compared to extensive public
resources like MIMIC-III, highlighting the necessity for generation
models to efficiently generate with limited data. With this concern,
we assess our method’s performance on the Breast Cancer Trial
dataset, which presents a challenging environment with relatively
4614Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models KDD ’24, August 25–29, 2024, Barcelona, Spain
62.5163.0367.5767.6267.3563.8661.6461.2166.0864.6370.11
57.3056.2755.7957.8255.0857.0653.8356.0357.9157.0258.33
MLPmedGANsynTEGEVATWINTabDDPMMeddiffScoEHRpromptEHRHALOEHRPD
50.00 55.00 60.00 65.00 70.00 75.00
Figure 6: Illustration of severe outcome prediction from the
synthetic or synthetic-real hybrid datasets.
few data entries. Following the settings in TWIN [ 8], our task is
to predict the severe outcome and death defined in the data pre-
processing section, and the selected metric is the Area Under the
Receiver Operating Characteristic (AUROC) score. The size of the
generated dataset is equal to the training dataset.
An LSTM network is used to learn the sequential visit-level hid-
den states, which are then utilized by an MLP to make a binary
prediction. The results are depicted in Figure 6. The dashed line
represents the AUROC value achieved using the real dataset. Our
model is colored in blue, while baselines are colored in red. For each
pair of the histogram, the light-colored one is the performance from
synthetic data only, while the darker one is from synthetic data
plus real data. Our model achieves the best performance under both
synthetic-only and hybrid settings. This comparative experiment
provides a clear visualization of our model’s capability to gener-
ate synthetic data that is both realistic and effective for advanced
predictive tasks.
4.5.6 Adverse Event Prediction. While the previous experiments
show our model’s generation capability on the patient level, now
we evaluate the fine-grained code-level generation capability and
see whether the generated visit is coherent with its predecessor.
Thus, in this section, our task is to predict the next visit’s adverse
events with the current visit’s multimodal codes. The only training
datasets available are the synthetic ones with AUROC as the metric.
The size of the generated dataset is equal to the real training dataset.
We utilize linear layers to embed medical codes and aggregate to
visit level, and then an MLP predicts the next visit’s adverse events.
Our findings are visually represented in Figure 7, where the
horizontal dashed line indicates the performance with the real
training set. Red histograms show the performance of baseline
models, while blue ones highlight that of our model. We can observe
that our model is closest to the real dataset’s performance, while
PromptEHR achieves the second-best performance, likely due to
the sequential generation nature of the model design that helps
preserve the visit-to-visit consistency. However, HALO behaves
worse than expected. This can be attributed to its design, which
generates a single prediction vector of various modalities, diluting
its effectiveness in tasks that require a focused prediction on a
singular modality.
47.5650.5955.41
51.5261.85
47.6345.93 46.0764.24
49.7866.02 67.38
35.0040.0045.0050.0055.0060.0065.0070.00Figure 7: Illustration of adverse event prediction with syn-
thetic datasets.
Diagnosis Drug Lab
Item Pr
ocedure
Metric LPL
MPL LPL
MPL LPL
MPL LPL
MPL
AS1 22.60
26.99 32.84
36.69 23.68
20.81 34.21
33.22
AS2 21.09
22.63 23.38
24.02 17.36
17.21 23.20
28.64
AS3 22.96
22.71 28.30
27.72 19.36
18.46 18.95
22.52
AS4 66.73
49.43 44.71
30.10 52.25
27.64 58.36
170.66
EHRPD 15.97
17.95 20.53
19.15 15.11
13.99 14.53
18.89
Table 6: Results of ablation study
4.6 Ablation Study
In this section, we remove some components of our model to assess
each component’s effectiveness towards the whole model, with
LPL and MPL as evaluation metrics. All ablation experiments are
described as follows:
•AS 1: removes the time aware visit embedding in Section 3.1
and replaces with a linear embedding layer.
•AS 2: removes the time interval estimation (Eq. 19) and time
prediction loss (Eq. 20).
•AS 3: removes the demographic information embedding of
catalyst representation in Section 3.3.
•AS 4: removes the self attention in Eq. (14), i.e., exclude
catalyst representation entirely.
The experiment result is shown in Table 6. An analysis of the
outcomes reveals that each component plays a significant role in
enhancing the model’s performance. Notably, the catalyst represen-
tation in EHRPD emerges as the most critical element, significantly
influencing the model’s performance.
5 Conclusion
In this paper, we present EHRPD , a diffusion-based EHR data genera-
tion model. By incorporating a time-aware visit embedding module
and predicting the next visit with a novel predictive diffusion model,
EHRPD is capable of capturing the complex temporal information of
EHR data. Furthermore, EHRPD ’s ability to simultaneously estimate
time intervals till the next visit sets it apart from existing methods,
offering a significant improvement in the field of EHR data genera-
tion. To validate our claims, we conducted extensive experiments on
publically available datasets, demonstrating EHRPD ’s superior per-
formance from three comprehensive perspectives: utility, fidelity,
and privacy.
Acknowledgements
The authors thank the anonymous referees for their valuable com-
ments and helpful suggestions. This work is partially supported by
the National Science Foundation under Grant No. 2238275 and the
National Institutes of Health under Grant No. R01AG077016.
4615KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
References
[1]Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van
Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces.
Advances in Neural Information Processing Systems (2021), 17981–17993.
[2]Mariette Awad, Rahul Khanna, Mariette Awad, and Rahul Khanna. 2015. Support
vector regression. Efficient learning machines: Theories, concepts, and applications
for engineers and system designers (2015), 67–80.
[3]Mrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. 2019.
Synthesizing electronic health records using improved generative adversarial
networks. JAMIA (2019), 228–241.
[4]Siddharth Biswal, Soumya Ghosh, Jon Duke, Bradley Malin, Walter Stewart, Cao
Xiao, and Jimeng Sun. 2021. EVA: Generating longitudinal electronic health
records using conditional variational autoencoders. In Machine Learning for
Healthcare Conference. 260–282.
[5]George EP Box and David A Pierce. 1970. Distribution of residual autocorrelations
in autoregressive-integrated moving average time series models. J. Amer. Statist.
Assoc. (1970), 1509–1526.
[6]Zhengping Che, Yu Cheng, Shuangfei Zhai, Zhaonan Sun, and Yan Liu. 2017.
Boosting deep learning risk prediction with generative adversarial networks for
electronic health records. In IEEE ICDM. 787–792.
[7]Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy
Schuetz, and Walter Stewart. 2016. Retain: An interpretable predictive model
for healthcare using reverse time attention mechanism. Advances in neural
information processing systems (2016).
[8]Trisha Das, Zifeng Wang, and Jimeng Sun. 2023. Twin: Personalized clinical trial
digital twin generation. In ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 402–413.
[9]Yujuan Feng, Zhenxing Xu, Lin Gan, Ning Chen, Bin Yu, Ting Chen, and Fei
Wang. 2019. Dcmn: Double core memory network for patient outcome prediction
with multimodal data. In International Conference on Data Mining. 200–209.
[10] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. 2022.
DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. In
International Conference on Learning Representations.
[11] Huan He, Shifan Zhao, Yuanzhe Xi, and Joyce C Ho. 2023. MedDiff: Generating
electronic health records using accelerated denoising diffusion model. arXiv
preprint arXiv:2302.04355 (2023).
[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems (2020), 6840–6851.
[13] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation (1997), 1735–1780.
[14] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng,
Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and
Roger G Mark. 2016. MIMIC-III, a freely accessible critical care database. Scientific
data (2016), 1–9.
[15] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. 2023.
Tabddpm: Modelling tabular data with diffusion models. In International Confer-
ence on Machine Learning. 17564–17579.
[16] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi
Li, and Yueting Chen. 2022. Srdiff: Single image super-resolution with diffusion
probabilistic models. Neurocomputing (2022), 47–59.
[17] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B
Hashimoto. 2022. Diffusion-lm improves controllable text generation. Advances
in Neural Information Processing Systems (2022), 4328–4343.
[18] Junyu Luo, Muchao Ye, Cao Xiao, and Fenglong Ma. 2020. Hitanet: Hierarchical
time-aware attention networks for risk prediction on electronic health records. In
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
647–656.
[19] Fenglong Ma, Radha Chitta, Jing Zhou, Quanzeng You, Tong Sun, and Jing Gao.
2017. Dipole: Diagnosis prediction in healthcare via attention-based bidirec-
tional recurrent neural networks. In ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 1903–1911.
[20] Liantao Ma, Junyi Gao, Yasha Wang, Chaohe Zhang, Jiangtao Wang, Wenjie
Ruan, Wen Tang, Xin Gao, and Xinyu Ma. 2020. Adacare: Explainable clinical
health status representation learning via scale-adaptive feature extraction and
recalibration. In Association for the Advancement of Artificial Intelligence. 825–832.
[21] Hongyuan Mei and Jason M Eisner. 2017. The neural hawkes process: A neurally
self-modulating multivariate point process. Advances in neural information
processing systems (2017).
[22] Ahmed Ammar Naseer, Benjamin Walker, Christopher Landon, Andrew Ambrosy,
Marat Fudim, Nicholas Wysham, Botros Toro, Sumanth Swaminathan, and Terry
Lyons. 2023. ScoEHR: Generating Synthetic Electronic Health Records using
Continuous-time Diffusion Models. In Machine Learning for Healthcare Conference.
PMLR, 489–508.
[23] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam,
Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE:
Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion
Models. In International Conference on Machine Learning. 16784–16804.[24] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-
toregressive denoising diffusion models for multivariate probabilistic time series
forecasting. In International Conference on Machine Learning. 8857–8868.
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-resolution image synthesis with latent diffusion models. In
Conference on Computer Vision and Pattern Recognition. 10684–10695.
[26] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and
Mohammad Norouzi. 2022. Image super-resolution via iterative refinement. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2022), 4713–4726.
[27] Robert E Schapire. 2003. The boosting approach to machine learning: An overview.
Nonlinear estimation and classification (2003), 149–171.
[28] Shengpu Tang, Parmida Davarmanesh, Yanmeng Song, Danai Koutra, Michael W
Sjoding, and Jenna Wiens. 2020. Democratizing EHR analyses with FIDDLE: a
flexible data-driven preprocessing pipeline for structured clinical data. Journal
of the American Medical Informatics Association (2020), 1921–1934.
[29] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con-
ditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems (2021), 24804–24816.
[30] Brandon Theodorou, Cao Xiao, and Jimeng Sun. 2023. Synthesize high-
dimensional longitudinal electronic health records via hierarchical autoregressive
language model. Nature communications (2023), 5305.
[31] Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong, Yaqing
Wang, and Fenglong Ma. 2023. Hierarchical Pretraining on Multimodal Electronic
Health Records. In Empirical Methods in Natural Language Processing.
[32] Zifeng Wang and Jimeng Sun. 2022. PromptEHR: Conditional Electronic Health-
care Records Generation with Prompt Learning. In Conference on Empirical
Methods in Natural Language Processing.
[33] Greg Welch, Gary Bishop, et al .1995. An introduction to the Kalman filter. (1995).
[34] Cao Xiao, Edward Choi, and Jimeng Sun. 2018. Opportunities and challenges
in developing deep learning models using electronic health records data: a sys-
tematic review. Journal of the American Medical Informatics Association (2018),
1419–1428.
[35] Yanbo Xu, Siddharth Biswal, Shriprasad R Deshpande, Kevin O Maher, and Jimeng
Sun. 2018. Raim: Recurrent attentive and intensive model of multimodal patient
monitoring data. In ACM SIGKDD international conference on Knowledge Discovery
and Data Mining. 2565–2573.
[36] Ziqi Zhang, Chao Yan, Thomas A Lasko, Jimeng Sun, and Bradley A Malin. 2021.
SynTEG: a framework for temporal structured electronic health data simulation.
Journal of the American Medical Informatics Association (2021), 596–604.
[37] Yuan Zhong, Suhan Cui, Jiaqi Wang, Xiaochen Wang, Ziyi Yin, Yaqing Wang,
Houping Xiao, Mengdi Huai, Ting Wang, and Fenglong Ma. 2024. MedDiffusion:
Boosting Health Risk Prediction via Diffusion-based Data Augmentation. In SIAM
International Conference on Data Mining.
[38] Yao Zhou, Jianpeng Xu, Jun Wu, Zeinab Taghavi Nasrabadi, Evren Körpeoglu,
Kannan Achan, and Jingrui He. 2021. PURE: Positive-Unlabeled Recommenda-
tion with Generative Adversarial Network. In KDD ’21: The 27th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore,
August 14-18, 2021. ACM, 2409–2419.
6 Appendix
6.1 Details of P-DDPM
In this section, we provide formula derivations for the theoretical
foundation of P-DDPM.
6.1.1 Forward Noise Addition Process. In the forward diffusion
process of P-DDPM, we gradually add noise to v𝑖according to the
noise schedule 𝛽𝑠:
𝑞(v1:𝑆
𝑖|v0
𝑖)=𝑆Ö
𝑠=1𝑞(v𝑠
𝑖|v𝑠−1
𝑖),
𝑞(v𝑠
𝑖|v𝑠−1
𝑖)=N(v𝑠
𝑖;√︁
1−𝛽𝑠v𝑠−1
𝑖,𝛽𝑠I).(22)
Let𝛼𝑠=1−𝛽𝑠and ¯𝛼𝑠=Î𝑠
𝑗=1𝛼𝑗, we can re-parameterize the
Gaussian step above with its mean and variance as:
v𝑠
𝑖=√𝛼𝑠v𝑠−1
𝑖+√1−𝛼𝑠𝜖𝑠−1
=√𝛼𝑠𝛼𝑠−1v𝑠−2
𝑖+√1−𝛼𝑠𝛼𝑠−1¯𝜖𝑠−2
=···
=√¯𝛼𝑠v0
𝑖+√1−¯𝛼𝑠𝜖,(23)
4616Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models KDD ’24, August 25–29, 2024, Barcelona, Spain
where𝜖is the merged Gaussian noise term from [𝜖1,···,𝜖𝑆]by
the property of normal distribution.
6.1.2 Predictive Mapping Process. By Eq. (7), we construct a rela-
tionship between v𝑠
𝑖+1,v𝑠
𝑖, and Φ𝑖.
6.1.3 Backward Denoising Diffusion Process. Then in the backward
diffusion process, we start with v𝑠
𝑖+1. We utilize Bayes Theorem
to rewrite the backward diffusion step into a mixture of forward
Gaussian steps as:
𝑞(v𝑠−1
𝑖+1|v𝑠
𝑖+1,v0
𝑖+1)=𝑞(v𝑠
𝑖+1|v𝑠−1
𝑖+1,v0
𝑖+1)𝑞(v𝑠−1
𝑖+1|v0
𝑖+1)
𝑞(v𝑠
𝑖+1|v0
𝑖+1). (24)
Then by the density function of normal distribution, the above
equation is proportional to:
∝exp{−1
2((v𝑠
𝑖+1−√𝛼𝑠v𝑠−1
𝑖+1)2
𝛽𝑠+(v𝑠−1
𝑖+1−√¯𝛼𝑠−1v0
𝑖+1)2
1−¯𝛼𝑠−1
−(v𝑠
𝑖+1−√¯𝛼𝑠v0
𝑖+1)2
1−¯𝛼𝑠)}
=exp{−1
2((v𝑠
𝑖+1)2−2√𝛼𝑠v𝑠
𝑖+1v𝑠−1
𝑖+1+𝛼𝑠(v𝑠−1
𝑖+1)2
𝛽𝑠
+(v𝑠−1
𝑖+1)2−2√¯𝛼𝑡−1v0
𝑖+1v𝑠−1
𝑖+1+¯𝛼𝑠−1(v0
𝑖+1)2
1−¯𝛼𝑠−1
−(v𝑠
𝑖+1−√¯𝛼𝑠v0
𝑖+1)2
1−¯𝛼𝑠)}
=exp{−1
2((𝛼𝑠
𝛽𝑠+1
1−¯𝛼𝑠−1)(v𝑠−1
𝑖+1)2
−(2√𝛼𝑠
𝛽𝑠v𝑠
𝑖+1+2√¯𝛼𝑠−1
1−¯𝛼𝑠−1v0
𝑖+1)v𝑠−1
𝑖+1+Constant)}(25)
Then by inspection, we can derive the mean and variance of the
above density function as:
ˆ𝛽𝑠=1/(𝛼𝑠
𝛽𝑠+1
1−¯𝛼𝑠−1)=1−¯𝛼𝑠−1
1−¯𝛼𝑠𝛽𝑠,
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)
=(√𝛼𝑠
𝛽𝑠v𝑠
𝑖+1+√¯𝛼𝑠−1
1−¯𝛼𝑠−1v0
𝑖+1)/(𝛼𝑠
𝛽𝑠+1
1−¯𝛼𝑠−1)
=(√𝛼𝑠
𝛽𝑠v𝑠
𝑖+1+√¯𝛼𝑠−1
1−¯𝛼𝑠−1v0
𝑖+1)1−¯𝛼𝑠−1
1−¯𝛼𝑠𝛽𝑠
=√𝛼𝑠(1−¯𝛼𝑠−1)
1−¯𝛼𝑠v𝑠
𝑖+1+√¯𝛼𝑠−1𝛽𝑠
1−¯𝛼𝑠v0
𝑖+1.(26)
Substituting v0
𝑖+1with Eq.(6) by v𝑠
𝑖+1, we have:
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)
=√𝛼𝑠(1−¯𝛼𝑠−1)
1−¯𝛼𝑠v𝑠
𝑖+1+√¯𝛼𝑠−1𝛽𝑠
1−¯𝛼𝑠1√¯𝛼𝑠(v𝑠
𝑖+1−√1−¯𝛼𝑠𝜖𝑠)
=1√𝛼𝑡(v𝑠
𝑖+1−1−𝛼𝑠√1−¯𝛼𝑡𝜖𝑠)(27)
And finally we have the closed-form solution that describes the
cross-visit relation with Eq. (7) as follows:
ˆ𝝁𝑠(v𝑠
𝑖+1,v0
𝑖+1)=1√𝛼𝑠(𝑓(v𝑠
𝑖,𝚽𝑖)−1−𝛼𝑠√1−¯𝛼𝑠𝜖𝑠). (28)6.2 Details of PU-Net
In this section, we provide the detailed structure of our PU-Net, as
in Figure 4.
6.2.1 Downsampling Path. Denoting the BatchNorm layer as BN,
the downsampling path of the PU-Net utilizes Resnet Block (ResB)
to refine features and downsamples with a 1-D convolutional layer
as follows:
ResB(v𝑠
𝑖,𝑙)=ReLU(BN(Conv(v𝑠
𝑖,𝑙)))+ v𝑠
𝑖,𝑙,
v𝑠
𝑖,𝑙+1=Conv1d(ResB(v𝑠
𝑖,𝑙)).(29)
6.2.2 Self-attention. In the skip connection, we utilize a self-
attention to fuse 𝑙-th layer 𝚽𝑖,𝑙andv𝑠
𝑖,𝑙:
¯v𝑠
𝑖+1,𝑙=Softmax©­
«W𝑄
𝑙(v𝑠
𝑖,𝑙)·W𝐾
𝑙(𝚽𝑖,𝑙)
√
𝑑ª®
¬·W𝑉
𝑙(𝚽𝑖,𝑙),
¤v𝑠
𝑖+1,𝑙=MaxPooling(LayerNorm(v𝑠
𝑖,𝑙)+¯v𝑠
𝑖+1,𝑙),(30)
where W𝑄
𝑙,W𝐾
𝑙,W𝑉
𝑙∈R𝑑𝑙∗𝑑𝑙.
6.2.3 Upsampling Path. With DeConv denoting the deconvolution
layer, our upsampling path first upsamples the lower level feature
v𝑠
𝑖+1,𝑙+1to¥v𝑠
𝑖+1,𝑙:
¥v𝑠
𝑖+1,𝑙=ReLU(BN(DeConv1d(v𝑠
𝑖+1,𝑙+1))). (31)
Then the feature from the skip connection is fused with the upsam-
pled feature with a 1-D convolution layer as:
˜v𝑠
𝑖+1,𝑙=Conv1d[¤v𝑠
𝑖+1,𝑙;¥v𝑠
𝑖+1,𝑙]. (32)
Lastly, we utilize a Resnet block to refine the learned feature:
ˆv𝑠
𝑖+1,𝑙=ResB(˜v𝑠
𝑖+1,𝑙). (33)
6.3 Baseline EHR Generation Models
•MLP [13] integrates an LSTM with an MLP to learn relation-
ships between patient visits.
•medGAN [3] uses a GAN to generate synthetic patient data,
enhanced with an LSTM for temporal dynamics.
•synTEG [36] employs a Transformer for learning relation-
ships in patient visit sequences and a Wasserstein GAN for
generating EHR data sequences.
•EVA [4] utilizes a VAE to encode health records into latent
vectors and generate synthetic records from the learned
distribution.
•TWIN [8] combines a VAE for capturing data distribution
with decoders for predicting current and next visit codes,
focusing on cross-modality fusion and temporal dynamics.
•TabDDPM [15] generates tabular healthcare data, incorpo-
rating an LSTM for temporal learning.
•Meddiff [11] uses an accelerated DDPM to generate realistic
synthetic EHR data, capturing temporal dependencies.
•ScoEHR [22] utilizes continuous-time diffusion models to
generate synthetic EHR data with temporal dynamics.
•PromptEHR [32] uses a pre-trained BART to generate di-
verse longitudinal EHR data.
•HALO [30] uses transformer architecture to learn different
modalities of EHR codes jointly.
4617KDD ’24, August 25–29, 2024, Barcelona, Spain Yuan Zhong et al.
T
askMo
del F-LSTM F-CNN RAIM DCMN
Metric AUPR
F1 Kappa AUPR
F1 Kappa AUPR
F1 Kappa AUPR
F1 KappaARFOrginal 0.9582
0.8969 0.7826 0.9550
0.8794 0.7590 0.9465
0.8698 0.7307 0.9471
0.8795 0.7439
MLP 0.9577
0.8932 0.7810 0.9587
0.8886 0.7635 0.9494
0.8713 0.7419 0.9438
0.8756 0.7486
me
dGAN 0.9518
0.8871 0.7610 0.9535
0.8873 0.7673 0.9538
0.8715 0.7408 0.9523
0.8754 0.7449
synTEG 0.9590
0.8929 0.7722 0.9535
0.8812 0.7701 0.9445
0.8636 0.7273 0.9536
0.8871 0.7610
EV
A 0.9600
0.8980 0.7820 0.9526
0.8870 0.7623 0.9478
0.8761 0.7358 0.9530
0.8856 0.7663
T
WIN 0.9617
0.8997 0.7946 0.9537
0.8903 0.7728 0.9575
0.8820 0.7584 0.9541
0.8844 0.7629
T
abDDPM 0.9574
0.8952 0.7792 0.9567
0.8885 0.7720 0.9414
0.8706 0.7371 0.9518
0.8820 0.7559
Me
ddiff 0.9542
0.8965 0.7840 0.9542
0.8837 0.7572 0.9435
0.8700 0.7222 0.9511
0.8824 0.7503
ScoEHR 0.9559
0.9013 0.7933 0.9487
0.8719 0.7114 0.9487
0.8701 0.7260 0.9524
0.8802 0.7521
Pr
omptEHR 0.9530
0.8940 0.7706 0.9540
0.8797 0.7724 0.9562
0.8840 0.7443 0.9560
0.8894 0.7731
HALO 0.9645 0.9020
0.7944 0.9546
0.8884 0.7657 0.9556
0.8776 0.7630 0.9542
0.8868 0.7657
EHRPD 0.9628 0.9031
0.7975 0.9616
0.8918 0.7737 0.9582
0.8862 0.7637 0.9562
0.8931 0.7785Sho
ckOrginal 0.8147
0.7092 0.5354 0.8110
0.7057 0.4669 0.8104
0.7455 0.5944 0.8012
0.7256 0.5726
MLP 0.8274
0.7630 0.6298 0.8189
0.7500 0.5731 0.8101
0.7353 0.5868 0.8079
0.7345 0.5849
me
dGAN 0.8379
0.7695 0.6322 0.8224
0.7579 0.5878 0.8010
0.7494 0.6005 0.8106
0.7399 0.5920
synTEG 0.8391
0.7561 0.6155 0.8227
0.7513 0.6107 0.8262
0.7473 0.6053 0.8105
0.7449 0.5933
EV
A 0.8437
0.7638 0.6325 0.8324
0.7517 0.5934 0.8287
0.7406 0.6044 0.8192
0.7441 0.5908
T
WIN 0.8472
0.7639 0.6263 0.8382
0.7616 0.6282 0.8208
0.7374 0.5939 0.8280
0.7534 0.5968
T
abDDPM 0.8421
0.7684 0.6327 0.8247
0.7508 0.5882 0.8199
0.7477 0.6019 0.8085
0.7380 0.5824
Me
ddiff 0.8437
0.7651 0.6283 0.8371
0.7651 0.6134 0.8138
0.7456 0.5943 0.8106
0.7423 0.5895
ScoEHR 0.8477
0.7652 0.6400 0.8316
0.7638 0.6148 0.8153
0.7340 0.5826 0.8117
0.7434 0.5872
Pr
omptEHR 0.8427
0.7661 0.6268 0.8334
0.7672 0.6179 0.8282
0.7437 0.5938 0.8152
0.7518 0.6060
HALO 0.8563 0.7709 0.6419 0.8253
0.7568 0.5923 0.8268
0.7538 0.6084 0.8212
0.7435 0.5926
EHRPD 0.8507 0.7722 0.6353 0.8421
0.7704 0.6369 0.8361
0.7791 0.6189 0.8376
0.7704 0.6118
Table 7: Result evaluation via other two risk prediction tasks on multimodal EHR data.
Dataset Mo
dality Metric MLP me
dGAN synTEG EV
A T
WIN T
abDDPM Me
ddiff ScoEHR Pr
omptEHR HALO EHRPDMIMIC-IIIDiagnosisLPL 325.55 242.30 36.87 29.62 26.28 108.79 664.54 685.17 126.23 149.66 15.97
MPL 352.54 257.48 45.61 31.63 27.68 114.18 670.91 691.55 128.05 192.13 17.95
DrugLPL 553.63 403.02 83.38 43.79 40.94 179.22 936.28 934.87 167.48 166.11 20.53
MPL 551.67 405.75 82.66 44.02 40.86 178.70 936.14 950.27 136.04 202.01 19.15
Lab
ItemLPL 168.25 77.10 26.80 20.05 17.47 54.69 413.41 432.11 107.22 322.51 15.11
MPL 166.61 87.12 30.34 19.97 17.41 54.44 412.33 431.09 98.52 303.09 13.99
Pr
ocedureLPL 290.38 234.81 49.33 27.39 21.26 98.03 471.81 486.81 51.18 22.68 14.53
MPL 286.53 245.28 44.00 30.49 24.26 102.72 479.96 499.89 31.13 39.04 18.89
Table 8: EHR data generation evaluation of different approaches on eICU dataset.
6.4 Multimodal Risk Prediction Models
Multimodal risk prediction models used in Section 4.5.2:
•F-LSTM [28] combines static demographic features with
time-series features as input for an LSTM module.
•F-CNN [28] is similar to F-LSTM but with a CNN.
•RAIM [35] integrates attention mechanism with modality
fusion and uses an LSTM for visit-wise relationship learning.
•DCMN [9] utilizes separate recursive learning modules for
each modality with an attention mechanism.
6.5 Backbone Unimodal Risk Prediction Models
Unimodal risk prediction models used in Section 4.5.3:
•AdaCare [20] uses a CNN for feature extraction and GRU
blocks for prediction.
•Dipole [19] combines a bidirectional GRU with an attention
mechanism to analyze patient visit sequences.
•HiTANet [18] adopts a time-aware attention mechanism to
capture evolving disease patterns.
•LSTM [13] learns the hidden state of each visit and performs
risk prediction with an MLP.•Retain [7] employs a reverse time attention mechanism to
prioritize recent medical events.
6.6 Backbone Time Interval Prediction Models
Time interval prediction methods in Section 4.5.4:
•ARIMA [5] forecasts future values using past values and
errors in a rolling window fashion.
•KF[33] estimates system states in linear dynamic systems.
•SVR [2] predicts continuous values by fitting a regression
line within an error margin.
•GBRT [27] combines multiple decision trees to improve
prediction accuracy through boosting.
•LSTM [13] outputs a single value for time prediction.
6.7 More Result on eICU Dataset and
Multimodal Risk Prediction Task
Additional results on the eICU dataset and multimodal risk predic-
tion task (Acute Respiratory Failure(ARF) and Shock) are shown in
Table 8 and Table 7.
4618