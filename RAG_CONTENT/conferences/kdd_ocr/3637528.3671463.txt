Decoding the AI Pen: Techniques and Challenges in Detecting
AI-Generated Text
Sara Abdali
saraabdali@microsoft.com
Microsoft
Redmond, WA, USARichard Anarfi∗
ranarfi@microsoft.com
Microsoft
Cambridge, MA, USA
CJ Barberan∗
cjbarberan@microsoft.com
Microsoft
Cambridge, MA, USAJia He∗
hejia@microsoft.com
Microsoft
Cambridge, MA, USA
Abstract
Large Language Models (LLMs) have revolutionized the field of Nat-
ural Language Generation (NLG) by demonstrating an impressive
ability to generate human-like text. However, their widespread us-
age introduces challenges that necessitate thoughtful examination,
ethical scrutiny, and responsible practices. In this study, we delve
into these challenges, explore existing strategies for mitigating
them, with a particular emphasis on identifying AI-generated text
as the ultimate solution. Additionally, we assess the feasibility of
detection from a theoretical perspective and propose novel research
directions to address the current limitations in this domain.
CCS Concepts
•Security and privacy →Social network security and privacy ;•
Computing methodologies →Neural networks ;Natural lan-
guage generation.
Keywords
LLM; AI-generated Text Detection; Responsible AI; Watermarking;
Paraphrasing Attacks; Data Poisoning
ACM Reference Format:
Sara Abdali, Richard Anarfi, CJ Barberan∗, and Jia He∗. 2024. Decoding the
AI Pen: Techniques and Challenges in Detecting AI-Generated Text. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 9pages. https://doi.org/10.1145/3637528.3671463
1 Introduction
Large Language Models (LLMs) constitute a transformative ad-
vancement in the field of natural language processing (NLP). Their
applications traverse a wide spectrum of domains, including ques-
tion answering [ 5,68], sentiment analysis [ 4,22] and specially text
generation [ 46]. As LLMs are trained on extensive textual corpora,
they exhibit a remarkable capacity to produce human-like text.
∗Equal contribution
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671463AI-generated
Text Study
Risks &
MisusesDetection
TechniquesDetection
VulnerabilitiesDetection
Possibility
Discrimination,
Toxicity
& Harms
Factual
Inconsistency
& Unreliability
Copyright
Infringement
& Plagiarism
Misinformation
DisseminationSupervised
Methods
Zero-shot
Methods
Retrieval-based
Methods
Watermarking
Methods
Discriminating
FeaturesParaphrasing
Attacks
Rewording
Attacks
Spoofing
Attacks
Data Poisoning
AttacksImpossibility
via AUROC
Upper-bound
Possibility
via Sample
Abundance
Impossibility
of Strong
Watermarking
The
oretical Study
Practical
Study
Figur
e 1: An overview of responsible AI-generated text
study, with an emphasize on detection approaches and their
challenges.
However, the ubiquity of LLMs brings forth a convergence of
opportunities and challenges that necessitate prudent examination.
Among these challenges, we encounter the potential for LLMs to
produce biased, toxic, or harmful content [ 13,27,60,61]. Addi-
tionally, there are concerns related to intellectual property rights
infringement [ 41,50] and the misuse of LLMs for malicious pur-
poses, such as disseminating misleading information and propa-
ganda [ 36,54]. These multifaceted considerations highlight the
need for judicious evaluation and ethical management of LLMs
across different contexts.
Leveraging AI-generated text could be regarded as an efficacious
strategy for alleviating the aforementioned challenges. However,
the convergence of AI-generated content with human-written texts
has reached a point where discerning between the two has become
increasingly intricate. The task of distinguishing LLM-generated
6428
KDD ’24, August 25–29, 2024, Barcelona, Spain Sara Abdali, Richard Anarfi, CJ Barberan∗, and Jia He∗
text from human-written content presents a dual challenge. On
one hand, identifying disparities can enhance the quality of AI-
generated material. On the other hand, this endeavor also compli-
cates the identification process. In recent years, researchers have
proposed various methodologies for detecting AI-generated text to
contribute to the ongoing exploration of this landscape.
This work aims to comprehensively investigate responsible AI-
generated text. To this end, first, we highlight the risks and misuses
associated with such text, while also discussing common mitiga-
tion strategies. As a significant solution, we thoroughly explore
AI-generated text, concentrating on thematic categorization and
assessing their constraints and vulnerabilities. Additionally, we
conduct a theoretical exploration to assess the feasibility and poten-
tial of detecting AI-generated text. By adopting a theoretical lens,
we seek to determine whether such detection is achievable or if
detection remains an elusive goal within the generative AI domain.
The rest of this paper is organized as follows: In section 2, we dis-
cuss the potential risks arising from the misuse of LLMs. Section 3
provides a comprehensive categorization of various text detection
strategies, emphasizing their role as mitigation techniques. Subse-
quently, in Section 4, we describe the vulnerabilities inherent in
these strategies. Moving forward, Section 5engages in a theoretical
exploration of the feasibility of detection, followed by section 6
where we propose new avenues of research. Finally, in section 7,
we conclude. Figure 1demonstrates an overview of the paper.
2 Risks and Misuse of AI-Generated Text
LLMs harbor the capacity to generate harmful content and en-
able malicious actions. These include spreading toxic, biased, or
harmful language, misinformation propagation and even commit-
ting plagiarism. In the subsequent sections, we will explore a non-
comprehensive list of potential risks associated with LLM misuse,
while discussing proposed mitigation techniques.
2.1 Discrimination, Toxicity, and Harms
LLMs, have the potential to produce text that exhibits discrimina-
tory, offensive, or harmful characteristics towards individuals or
groups. The manifestation of such undesirable language depends on
factors such as the quality and diversity of the training data used, the
design decisions made during model development, and the intended
or unintended contexts in which the model is applied [ 12,13,15].
Thus, LLMs may pose ethical and social challenges that require
careful evaluation and regulation.
A recent work published by DeepMind [ 60], structures the risk
landscape associated with LLMs. It outlines six specific risk areas,
including discrimination, exclusion and toxicity, and discusses the
potential mitigation approaches and challenges. For discrimination,
exclusion and toxicity, the paper suggests improving data quality
and diversity, applying fairness metrics and interventions, and
implementing content moderation and reporting mechanisms.
In another study by Deshpande et al., [ 13] it is shown that Chat-
GPT, when assigned a persona, can exhibit significant toxicity. Par-
ticularly, this risk is elevated for vulnerable groups such as students,
minors, and patients. This work emphasizes that toxicity is closely
tied to the style of communication, with explicit negative prompts
leading to increased toxicity.Furthermore, the research reveal that specific genders and eth-
nicities are disproportionately susceptible to encountering toxic
content. Deshpande et al. posit that this phenomenon arises from
an over-reliance on reinforcement learning with human feedback
(RLHF) as a mechanism to mitigate model toxicity. However, the
feedback provided to the model may carry inherent biases. For
instance, feedback related to toxicity concerning different genders
might be influenced by the representation of those genders.
Kour et al. [ 25] recently introduced the AttaQ dataset, designed
to provoke harmful or inappropriate responses from LLMs. They
conduct evaluations on multiple LLMs using this dataset and ob-
serve that, in numerous instances, LLMs generate unsafe outputs.
More importantly, LLMs have the potential to produce implicit
toxic responses that elude existing classifiers. These harmful out-
puts, while not easily identifiable, can offend individuals or groups
by insinuating negative or false statements. As a result, this under-
mines the safety and reliability of NLG systems and gives rise to
ethical concerns.
A recent study by Wen et al. [ 61] investigates how LLMs generate
implicit toxic content that conventional toxicity classifiers struggle
to detect. The proposed approach leverages reinforcement learning
to uncover and expose this implicit toxicity, emphasizing the need
to fine-tune classifiers using annotated examples derived from the
attack method for enhanced detection capabilities.
Given the multifaceted nature of content generation by LLMs,
it is critical to explore the intricate interplay of user-driven, data-
driven, and model-driven factors that contribute to the production
of toxic and harmful content. Therefore, further research is war-
ranted to comprehensively investigate the deleterious effects and
toxicity associated with LLMs. Future research endeavor should
focus on devising robust methodologies and mechanisms for pre-
vention, detection, and mitigation. By doing so, we not only bolster
the safety and reliability of LLMs but also make significant contri-
butions to the advancement of related disciplines.
2.2 Factual Inconsistency and Unreliability of
AI Responses
Enforcing factual coherence during reasoning constitutes a key
challenge for LLMs. These models often demonstrate tendencies
such as overlooking conditions, misinterpreting context, and even
hallucinating content in response to specific queries [28].
For example, an investigation into GPT-3 by Khatun et al. [ 21] re-
vealed that while the model adeptly avoids blatant conspiracies and
stereotypes, it hesitates when handling commonplace misunder-
standings and debates. In particular, the model’s responses exhibit
variability across different questions and contextual scenarios, em-
phasizing the inherent unpredictability of GPT-3.
Zhou et al. [ 72] recently conducted a study revealing that LLMs,
including ChatGPT and Claude, exhibit deficiencies in conveying
uncertainties when responding to questions. Furthermore, these
models occasionally display unwarranted overconfidence, even
when their answers are incorrect. While LLMs can be coerced to
express confidence, this process is fraught with high error rates.
Significantly, the study highlights that users encounter chal-
lenges in assessing the accuracy of LLM-generated responses. Their
judgment is influenced by the tone and style of the LLMs, which
6429Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text KDD ’24, August 25–29, 2024, Barcelona, Spain
may introduce bias. This issue is extremely important, as biases
against uncertain text may impact the LLM training and evaluation.
To mitigate such mistakes, various strategies have been proposed
through fine-tuning [ 29,43,69], prompt engineering techniques
like verification, scratchpads [ 11,38], Chain-of-Thought (CoT) [ 59],
Reversing Chain-of-Thought (RCoT) [ 63], RLHF [ 10,73], iterative
self-reflection [ 34,48], self-consistency [ 56], society of minds strat-
egy [ 14], pruning truthful datasets [ 9], adjusting the system parame-
ters to limit model creativity [ 37], external knowledge retrieval [ 18]
and training-free methods with likelihood estimation [19].
The existing body of research emphasize the unspeakable capa-
bilities of LLMs, while also recognizing their proneness to errors.
Thus, it is essential to approach LLM outputs with caution.
2.3 Copyright Infringement and Plagiarism
LLMs could pose a substantial risk to academic writing by elevating
the likelihood of copyright violations and plagiarism. For instance,
writers might employ LLMs to produce articles without original
composition, while students could resort to LLMs for completing
homework assignments. These practices erode academic integrity
and subvert the objectives of assignments and examinations [ 20,50],
In response to this challenge, researchers have devised several
detectors aimed at discerning between text authored by humans and
that generated by AI. These detectors fall into two main categories:
black-box methods, exemplified by studies such as [ 32,42,57], and
white-box detection, as explored by [53].
In the black-box detection approach, there is limited access to
the LLM’s output text. Specifically, we interact with the LLM via its
API, which provides us with the generated text. Black-box detectors
rely on collecting samples of both human-written and AI-generated
text. These samples are then used to train a classifier. The trained
classifier discriminates between LLM-generated and human-written
texts based on features extracted from the text samples [ 32,42,52].
In contrast to the black-box approach, the white-box approach
necessitates additional access to the probabilities associated with
each token in the model [ 53]. Consequently, there are fewer white-
box detectors currently available. Thus, black-box methods that are
independent of model access and can be readily adjusted to a new
model seem to be more feasible and practical.
Another crucial aspect to consider is the generalizability of AI-
generated text detectors. These detectors should perform effectively
on unseen data across diverse dimensions, including: models, lan-
guages and domains. Achieving robustness and reliability across
these dimensions contributes to the overall effectiveness of AI-
generated text detectors in real-world scenarios.
Their study involved using various generative models to create
text articles and then distinguishing between AI-generated and
human-written content. They applied traditional machine learn-
ing techniques and transformer-based models to analyze stylistic
features. Interestingly, while these methods performed well within
their specific domains, they struggled with out-of-domain detection
tasks. Furthermore, their findings indicate that when text detectors
are trained on content generated by one LLM and then tested on
data produced by a different LLM, performance tends to decline and
generalizability becomes an issue. However, given that this studyexamines only a limited number of detection methods, more com-
prehensive and systematic assessments are necessary to validate
this aspect of LLM capabilities. In section 3, we will delve further
into the topic of AI-generated text detection techniques.
2.4 Misinformation Dissemination
LLMs, especially when integrated into Open-Domain Question An-
swering (ODQA) systems, can inadvertently contribute to the cre-
ation and dissemination of misinformation [ 7,39,40]. An intuitive
approach, as suggested by Pan et al. [ 40] to counteract the spread
of misinformation in ODQA systems is to reduce its prevalence. In
fact, the goal is to minimize the proportion of misinformation that
these QA systems encounter. Achieving this involves retrieving
more context paragraphs to provide a solid background for readers.
However, research have shown that expanding the context size
has minimal impact on mitigating the performance degradation
caused by misinformation . Consequently, the straightforward strat-
egy of diluting misinformation by increasing context size proves
ineffective for defending against it [40].
A more straightforward approach involves prompting LLMs to
issue warnings regarding misleading information. For instance,
readers could be advised: “Be cautious, as some of the text may be
intentionally misleading” .
Furthermore, it is feasible to detect and filter out misinformation
produced by LLMs using different characteristics, such as content,
style, or propagation structure. Chen et al. [ 7], for example, intro-
duce four instruction-tuned strategies to enhance LLMs for misin-
formation detection. One of these strategies is Instruction Filtering,
which aims to exclude LLM outputs that deviate from instructions
or contain misleading content, Instruction Verification which veri-
fying the outputs of the LLM against the instructions or external
sources to check their validity and reliability and Instruction Com-
bination which combines multiple instructions to generate more
diverse and accurate outputs from the LLM.
Another interesting approach introduced by Chen et al. [ 7] is
theReader Ensemble technique. This method leverages multiple
LLMs or other models to cross-check the accuracy and coherence
of information produced by a specific LLM. Chen and colleagues
also suggest employing Vigilant Prompting, which provides pre-
cise prompts or instructions to LLMs. These instructions serve a
dual purpose: preventing the generation of misinformation and
disclosing the model’s machine identity.
As AI-generated texts increasingly blend seamlessly with human-
written content, the demand for more effective methods to detect
misleading information produced by AI grows. In the next section,
we will dive deeper into detection methods.
3 AI-generated Text Detection Techniques
In the previous section, we briefly explored a classification of detec-
tion techniques into two primary categories: black-box and white-
box techniques. As mentioned, in the black-box scenario, the access
is limited to the output text produced by LLM given an arbitrary
input. Conversely, in the white-box context, we gain additional ac-
cess into the model’s output probabilities for individual tokens. In
this section, we discuss various detection techniques, considering
both their strengths and vulnerabilities.
6430KDD ’24, August 25–29, 2024, Barcelona, Spain Sara Abdali, Richard Anarfi, CJ Barberan∗, and Jia He∗
3.1 Supervised Detection
A frequently employed detection strategy is fine-tuning a lan-
guage model on datasets comprising both AI-generated and human-
written texts [ 2,3,30,49,70]. The majority of LLMs require substan-
tial computational resources, rendering it exceedingly challenging
to curate sufficiently large datasets that comprise a diverse spec-
trum of samples. As a result, this approach is not generally an
optimal solution. Moreover, it is susceptible to adversarial attacks,
including data poisoning [8, 45,64].
For example, malicious actors have the potential to elude detec-
tion by leveraging their access to human.written texts present in the
training set and detector rankings. Moreover, within a white-box
context, attackers can undermine detector training—a concerning
scenario, especially considering that numerous detectors rely on
commonly used datasets, making them susceptible even to basic at-
tacks. Additionally, these techniques are prone to the paraphrasing
attack, where a paraphrased layer is added to the generative text
model, allowing deception of any detector, including those utilizing
supervised neural networks [26, 44].
3.2 Zero-shot Detection
Another line of research employ pre-trained models as zero-shot
classifiers to discern text written by AI, eliminating the necessity for
supplementary training or data collection [ 1,16,51,55]. According
to [35], commonly employed techniques often set a threshold for
the predicted per-token log probability to identify LLM-generated
texts. This approach is grounded in the observation that passages
generated by AI often exhibit a negative log probability curvature.
While this method mitigates the risk of data poisoning attacks
and minimizes data and resource requirements, it remains vulnera-
ble to other attacks such as spoofing [ 47] and paraphrasing [ 26,44].
3.3 Retrieval-based Detection
An alternative avenue of research utilize information retrieval meth-
ods to differentiate between texts written by humans and those
generated by machines. This is achieved by comparing a given text
with a database of texts created by LLMs and identifying semanti-
cally similar matches [26, 44].
Nevertheless, these approaches are impractical for real-world
use due to their reliance on an extensive and up-to-date database of
AI-generated texts. Such databases can be computationally costly
or may not be available across all domains, tasks, or models. Ad-
ditionally, like other detection techniques we discussed earlier,
these methods are susceptible to paraphrasing and spoofing at-
tacks [26, 31,44,62].
3.4 Watermarking-based Detection
Another alternative avenue, referred to as watermarking techniques,
employs a model signature within the generated text outputs to
imprint specific patterns.
For instance, Kirchenbauer et al. [ 23] propose a soft watermark-
ing approach that categorizes tokens into green and red lists, fa-
cilitating the construction of these patterns. In this approach, a
watermarked LLM selects a token, with a high likelihood, from the
green list based on its preceding token. Remarkably, these water-
marks often remain imperceptible to human observers.To better understand the technique proposed by Kirchenbauer
et al., assume an autoregressive language model is trained on a
vocabulary 𝑉of size|𝑉|. Given a sequence of tokens as input at
step𝑡, a language model predicts the next token in the sequence by
outputting a vector of logit scores 𝑙𝑡∈𝑅|𝑉|with one entry for each
item in the vocabulary. A random number generator is seeded with
a context window of ℎpreceding tokens, based on a pseudo-random
function (PRF) 𝑓:𝑁ℎ→𝑁. With this random seed, a subset of
tokens of size 𝛾|𝑉|, where𝛾∈(0,1)is green list size, are “colored
green” and denoted 𝐺𝑡. Now, the logit scores 𝑙𝑡are modified such
that with a hardness parameter 𝜎>0:
𝑙𝑡𝑘=(
𝑙𝑡𝑘+𝜎,if𝑘∈𝐺𝑡
𝑙𝑡𝑘, otherwise
In a straightforward scenario the process involves passing scores
through a softmax layer and then sampling from the resulting
output distribution. This tends to introduce a bias toward tokens
from the set 𝐺𝑡. Once watermarked text is generated, it is possible
to verify the watermark even without direct access to the LLM. This
verification is achieved by recomputing the 𝐺𝑡at each position and
identifying the set of token positions associated with the 𝐺𝑡. For𝑇
tokens, the statistical significance of the watermark is assessed by
z-score:
𝑧=(|𝑆|−𝛾𝑇)√︁
𝛾(1−𝛾)𝑇
When this z-score is large (and the corresponding P-value is small),
one can be confident that the text is watermarked [23].
Despite previous indications, watermark-based techniques re-
main both theoretically and practically vulnerable to rewording
attacks. Notably, even LLMs safeguarded by watermarking schemes
are susceptible to spoofing attacks. In these attacks, human adver-
saries inject their own text into human-generated content, creating
an illusion that the material originated from LLMs [ 44]. The inter-
ested reader is referred to [44] for more details.
Moreover, unless all highly successful LLMs are uniformly safe-
guarded, watermarking remains an ineffective strategy for prevent-
ing LLM exploitation.
Furthermore, the practical applicability of watermarking is re-
stricted, especially in scenarios where only black-box language
models are accessible. Due to API providers withholding probability
distributions for commercial reasons, most third-party developers
creating API-based applications lack the capability to independently
watermark text.
To tackle this challenge, Yang et al. [ 65] have developed a water-
marking framework to empower third parties with the ability to
autonomously inject watermarks into black-box language model
scenarios. In this approach, a binary encoding function is leveraged
that generates random binary encodings corresponding to indi-
vidual words. These encodings adhere to a Bernoulli distribution,
where the probability of a word representing bit-1 is approximately
0.5. To embed a watermark, this distribution is modified by se-
lectively replacing words associated with bit-0 using contextually
relevant synonyms that represent bit-1. Subsequently, a statistical
test is employed to detect the watermark.
Another study conducted by Kirchenbauer et al. [ 24] explores
the effectiveness of watermarks in identifying AI-generated text.
6431Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text KDD ’24, August 25–29, 2024, Barcelona, Spain
They assess how watermarked text withstands challenges such as
human restructuring, non-watermarked LLM paraphrasing, and
seamless integration into longer handwritten documents.
They observe that even after undergoing automated and human
paraphrasing, watermarks can still be discerned. When a sufficient
number of tokens are identified, paraphrased versions tend to in-
advertently leak n-grams or even larger segments of the original
text. Hence, high-confidence detection remains possible despite
these attacks weakening the effectiveness of the watermark. The
study proposes interpreting watermarking reliability as a function
of text length. Surprisingly, even when attempting to remove the
watermark intentionally, human writers struggle to do so if the text
exceeds 1,000 words. This interpretation emerges as a significant
characteristic of watermarking. According to this work, watermark-
ing remains the most dependable strategy, as alternative paradigms
like retrieval and loss-based detections have not demonstrated sub-
stantial improvements with increasing text length.
Despite prior findings, a recent study by Zhang et al. [ 71] demon-
strates that, under some assumptions, no robust watermarking
scheme can prevent an attacker from removing the watermark
without significantly degrading the output quality. We will further
discuss this finding in section 5.3.
3.5 Detection via Discriminating Features
Another stream of work is to identify and classify text based on
distinguishing features. For instance, Yu et al. [ 67] have discovered
a genetic inheritance characteristic in GPT-generated text. Leverag-
ing this characteristic, the model’s output becomes a rearrangement
of content from its training corpus. In other words, when repeat-
edly answering questions, the model’s responses are constrained
by the information within its training data, resulting in limited vari-
ations. This hypothesis suggests that the output of an LLM, such
as ChatGPT, is predictable. Therefore, for highly similar questions,
the model tends to produce correspondingly similar responses.
Drawing an analogy, paternity testing utilizes DNA profiles to
determine whether an individual is the biological parent of an-
other person. This process becomes particularly crucial when legal
rights and responsibilities related to parenthood are in question,
and uncertainty exists regarding the child’s paternity.
In another study, Yang et al. [ 66] introduce a training-free detec-
tion strategy known as Divergent N-Gram Analysis (DNA-GPT).
This approach assesses the dissimilarities between a given text and
its remaining truncated portions using n-gram analysis in black-box
scenarios or probability divergence in white-box scenarios.
For the black box scenario, Yang et al. define DNA-GPT BScore:
𝐵𝑆𝑐𝑜𝑟𝑒(𝑆,Ω)=
1
𝐾Σ𝐾
𝑘=1Σ𝑁
𝑛=𝑛0𝑓(𝑛)|n-grams(ˆ𝑆𝑘)∩ n-grams(𝑆2)|
|ˆ𝑆𝑘||n-grams(𝑆2)|
where𝑆is the LLM output, 𝑆2the human written ground truth,
𝑓(𝑛)a weight function for different n-grams and Ω={ˆ𝑆1,..., ˆ𝑆𝐾}
For white-box scenario, they propose calculating a DNA-GPT
WScore between Ωand𝑆:
𝑊𝑆𝑐𝑜𝑟𝑒(𝑆,Ω)=1
𝐾Σ𝐾
𝑘=1𝑙𝑜𝑔𝑝(𝑆2|𝑆1)
𝑝(ˆ𝑆𝑘|𝑆1)Where Ωis a set of𝐾samples of an LM decoder and ˆ𝑆=𝐿𝑀(𝑆1)
and𝑆2is the human written ground truth. “In both black-box and
white-box scenarios, two parameters significantly impact detection
accuracy: the truncation ratio and the number of re-prompting
iterations𝐾. This strategy reveals substantial discrepancies between
AI-generated and human-written texts. It highlights substantial
disparities between AI-generated and human-written texts.
Another distinguishing feature is the vulnerability of text to
manipulations. Both AI-generated and human-written texts can be
adversely affected by minor alterations, such as word replacements.
However, recent research have revealed that AI-generated text is
particularly prone to such manipulations [35, 51].
For instance, Su et al. [ 51] propose a metric called the Log-
Likelihood Log-Rank Ratio (LRR) to quantify the sensitivity of
LLMs to perturbations:
𝐿𝑃𝑅=−Σ𝑡
𝑖=1log𝑝𝜃(𝑥𝑖|𝑥<𝑖)
Σ𝑡
𝑖=1log𝑟𝜃(𝑥𝑖|𝑥<𝑖)
where𝑟𝜃(𝑥𝑖|𝑥<𝑖)≥1is the rank of token 𝑥𝑖conditioned on the
previous tokens [ 51]. The Log-Likelihood in the numerator reflects
the absolute confidence for the correct token, whereas the Log-Rank
in the denominator considers relative confidence. Together, they
provide complementary information about the texts.
They also propose Normalized Log-Rank Perturbation (NPR):
NPR=1
𝑛Σ𝑛
𝑝=1log𝑟𝜃(˜𝑥𝑝)
log𝑟𝜃(𝑥)
where small perturbations are applied on the target text 𝑥to produce
the perturbed text ˜𝑥𝑝.
This work demonstrates that the LRR tends to be larger for AI-
generated text, making it a useful discriminator between AI and
human-generated content. One plausible explanation is that, in
AI-generated text, the log rank stands out more prominently than
the log likelihood, resulting in a distinct pattern that LRR captures.
The rationale behind NPR lies in the fact that both AI-generated
and human-written texts experience adverse effects from minor
alterations. Specifically, the log rank score tends to increase after
perturbations. However, AI-generated text is particularly vulnerable
to such alterations, resulting in a more pronounced increase in the
log rank score following perturbation. As a result, this pattern
suggests a higher NPR score for AI-generated texts [51].
Table 1illustrates an overview of detection strategies, highlight-
ing the vulnerabilities associated with each category.
4 Vulnerabilities of Detection Techniques
Despite various methods mentioned above, most categories of detec-
tion strategies are susceptible to paraphrasing or spoofing attacks.
To tackle this, retrieval-based detectors have been employed as
detection strategies but these detectors store the output of LLMs
in a database and perform semantic searches to extract the best
matches. Although it enhances the detector’s resilience against
paraphrasing attacks, there are privacy concerns related to storing
user-LLM conversations. Additionally, this technique is ineffective
when dealing with recursive paraphrasing [44].
Moreover, researchers have discovered that by effectively op-
timizing prompts, LLMs can successfully evade many detection
6432KDD ’24, August 25–29, 2024, Barcelona, Spain Sara Abdali, Richard Anarfi, CJ Barberan∗, and Jia He∗
•Many detection strategies such as supervised techniques are 
susceptible to data poisoning attacks. 
•While zero -shot methods mitigates the risk of data poisoning 
attacks and minimizes data and resource requirements, they are 
vulnerable to other adversarial attacks.Data Poisoning 
Attacks
•LLMs can evade detection techniques by optimizing prompts.
•Prompt optimization minimizes detection while maximizing 
similarity between human and AI -generated texts.Optimized 
Prompts & 
Evasion
Strong 
Watermarking is 
Challenging•Most detection strategies are susceptible to paraphrasing 
attacks.
•Retrieval -based detectors, enhance resilience against 
paraphrasing attacks. However, privacy concerns arise from 
storing user -LLM conversations.
•Recursive paraphrasing makes detection even more difficult.Paraphrasing & 
Re-wording 
Attacks
•Practical applicability is limited, especially with black -box 
language models.
•API providers often withhold probability distributions, hindering 
independent watermarking.
•Robust watermarking schemes cannot prevent attackers from 
removing watermarks without degrading output quality
Figure 2: A summary of detection vulnerabilities.
techniques. For example, in a recent work [ 33], Lu et al. propose
a novel method called Substitution-based In-Context example Op-
timization (SICO). In SICO, discriminating features are extracted
from both human and AI-generated texts. These features, along
with a paraphrasing prompt, are combined and fed to the LLM
to modify its output. The prompt is carefully optimized through
word and sentence replacements, aiming to minimize the chances
of detection while maximizing the similarity between human and
AI-generated texts. Evaluation results strongly demonstrate the
vulnerability of existing methods.
Despite being considered an effective detection strategy, water-
marking faces several challenges. Firstly, unless all LLMs are uni-
formly safeguarded, watermarking remains ineffective. Secondly,
its practical applicability is limited, especially when dealing with
black-box language models. Thirdly, API providers often withhold
probability distributions, preventing third-party developers from
independently watermarking text. Lastly, recent research suggest
that no robust watermarking scheme can prevent attackers from re-
moving watermarks without significantly degrading output quality.
Therefore, watermarking generative models might be fundamen-
tally unachievable, and alternative approaches are necessary to
protect the intellectual property of model developers and LLM
users. Figure 2presents a concise overview of vulnerabilities in
detection techniques.
5 Detection Possibility Through a Theoretical
Lens
Given the increasing interest in detecting text generated by LLMs,
researchers have recently investigated the theoretical aspects of
this task. They explore the fundamental feasibility and limitations
associated with identifying LLM-generated text. In this section we
explore some of the notable research studies on this topic.
5.1 Impossibility via AUROC Upper-bound
In a new study, Sadasivan et al. present a discovery of impossibil-
ity [44]. They claim that:As language models become increasingly sophisticated and
adept at emulating human text, the effectiveness of even the
best-possible detectors diminishes significantly.
Sadasivan et al. establish an upper-bound for the area under the
Receiver Operating Characteristic (ROC) curve of any decoder D:
𝐴𝑈𝑅𝑂𝐶(D)≤1
2+𝑇𝑉(M,H)−𝑇𝑉(M,H)2
2
where𝑇𝑉(M,H)is the total variation distance between machine
and human-written texts distributions. This formula indicates that
when human and machine generated texts are very similar i.e.,
𝑇𝑉(M,H)is very small, even the most effective detector may ex-
hibit only marginal improvement over a random classifier. Figure 3
illustrates how the above bound grows as a function of the total
variation distance.
5.2 Possibility via Sample Abundance
Another interesting study by Chakraborty et al. [6] suggests that:
As long as the distributions of human-generated and AI-
generated texts are not identical (which is typically the case),
it remains feasible to detect AI-generated texts. This detection
becomes possible when we gather sufficient samples from each
distribution.
Interestingly, Chakraborty et al. demonstrate that the AUROC curve,
as proposed by Sadasivan et al., might be overly conservative for
practical detection scenarios. Specifically, they introduce a hid-
den possibility by replacing 𝑇𝑉(M,H)with𝑇𝑉(MË𝑛,HË𝑛)
in AUROC equation, where 𝑚Ë𝑛:=𝑚Ë𝑚Ë···Ë𝑚(n times)
denotes the product distribution over sample set S:={𝑠𝑖},𝑖∈
{1,...𝑛}, as doesℎË𝑛. Thus, for any detector D, with a given
collection of i.i.d. samples either from human or AI, the AUROC
curve is defined as:
𝐴𝑈𝑅𝑂𝐶(D)≤1
2+𝑇𝑉(MË𝑛,HË𝑛)−𝑇𝑉(MË𝑛,HË𝑛)2
2
where𝑇𝑉(MË𝑛,HË𝑛)=1−𝑒𝑥𝑝(−𝑛𝑙𝑐(𝑚,ℎ)+𝑜(𝑛)and
𝑙𝑐(𝑚,ℎ)is the Chernoff information.
Since𝑇𝑉(MË𝑛,HË𝑛)is an increasing sequence, it eventu-
ally converges to 1as the number of samples for each distribution
increases. Thus, the upper bound of AUROC increases exponen-
tially with respect to the number of samples. It is clear that if this
happens, the total variation distance approaches 1quickly, and
hence increasing the AUROC. Interested readers are encouraged to
refer to [6] for further details.
5.3 Impossibility of Strong Watermarking
In another work, Zhang et al. [ 71], investigate the theoretical aspect
of watermarking detection. They conceptualize watermarking as
the process of embedding a statistical signal a.k.a. “watermark” into
a model’s output. This embedded watermark serves as a signal for
later verification, ensuring that the output is indeed originated from
the model. A robust watermarking prevents an attacker from eras-
ing the watermark without causing significant quality degradation.
Zhang et al. put forth two fundamental assumptions:
6433Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text KDD ’24, August 25–29, 2024, Barcelona, Spain
00.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1AUROC
Total VariationRandom ClassifierBest Classifier
Figure 3: Comparing AUROC of the optimal detector to a ran-
dom classifier demonstrates that as the TV distance between
AI and human text distributions reduces, the AUROC of the
optimal detector also decreases accordingly.
“Quality Oracle”: grants the attacker access to an oracle capable
of evaluating the quality of outputs. This oracle assists the attacker
in assessing the quality of modified responses.
“Perturbation Oracle”: allows the attacker to modify an output
while maintaining a nontrivial probability of preserving quality.
The perturbation oracle essentially induces an efficiently mixing
random walk on high-quality outputs. Their investigation culmi-
nates in a compelling result:
“Given a prompt𝑝and a watermarked output 𝑦, for every
public or secret-key watermarking setting that satisfying these
assumptions, there exists an efficient attacker that can lever-
age the quality and perturbation oracles to obtain an output 𝑦′
with a probability very close to 1. The attacker’s goal is to:
find an output𝑦′s.t.
(1)𝑦′is not watermarked with high probability and,
(2)𝑄(𝑝,𝑦′)≥𝑄(𝑝,𝑦)” [71].
In simpler terms, watermarking without causing significant quality
degradation is impossible. Thus, Zhang et al. propose alternative
approaches to safeguard the intellectual property of developers.
Detecting AI-generated text is an essential and complex task and
current state-of-the-art methods occasionally face limitations due to
a lack of comprehensive understanding regarding the fundamental
feasibility and boundaries of this task. Thus, it is crucial to continue
exploring and investigating the theoretical aspects of the matter.
6 Limitations and Future Research
This paper offers a comprehensive overview of the latest advance-
ments and recommended practices in detecting AI-generated text.
However, existing methods often face limitations and are suscepti-
ble to malicious attacks. Thus, it is essential to continue exploring
both theoretical and practical aspects of this realm. In light of this,
we propose opportunities for advancing the field of responsibleAI, vulnerability assessment of AI-generated text detection studies.
Some of these opportunities include:
Curating diverse and representative datasets. is crucial for
training and evaluating detection models. The existing datasets
may not comprise all the potential types and sources of AI text.
Specially, it is essential to curate a dataset containing outputs from
various generative models.
Further investigating interpretable features. can help dis-
cern the subtle differences between human-written and AI-generated
text, especially given the limited availability of such features. Addi-
tionally, investigating their vulnerability to adversarial attacks is
crucial.
Exploring advanced and adaptable learning techniques.
to effectively address the dynamic and ever-evolving nature of AI-
generated text. These methods include adversarial learning, meta-
learning, and self-supervised learning [58].
A comprehensive multi-aspect evaluation of detection
techniques against adversarial attacks. as literature lacks such
study to investigate relative vulnerabilities of detection methods.
Such an analysis should consider various perspectives, including
efficacy across different models and resilience against adversarial
attacks. This study can aid in identifying techniques suitable for
specific scenarios.
Developing hybrid detection strategies. that combine fea-
tures and techniques to enhance robustness and adaptability. For
example, one can create a hybrid approach by integrating water-
marking and feature-based techniques.
A comprehensive understanding of the fundamental feasi-
bility and boundaries. as there is often an absence of a thorough
grasp of the fundamental feasibility and limitations within current
state-of-the-art methods. Therefore, there is a need for deeper ex-
ploration and investigation into the theoretical aspects of this task
to facilitate the creation of more resilient and efficient techniques
while also uncovering novel avenues for research.
7 Conclusions
In this paper, we delve into a thorough study of AI-generated text.
Our analysis comprises not only the potential risks and misuses
associated with content generated by LLMs, but also investigates
widely recognized techniques for mitigating these risks. As a piv-
otal mitigation strategy, we conduct a comprehensive study of
AI-generated text detection techniques, categorizing them into five
distinct categories and meticulously comparing their weaknesses
and vulnerabilities. Our investigation approaches AI-generated text
detection from both empirical and theoretical angles, shedding light
on the intricacies of the field, which leads to proposing new avenues
of research to bolster this critical area of study.
8 Acknowledgements
This study represents independent research conducted by the au-
thors and does not necessarily represent the views or opinions of
any organizations. We express our gratitude to the anonymous
reviewers for their valuable and constructive feedback.
6434KDD ’24, August 25–29, 2024, Barcelona, Spain Sara Abdali, Richard Anarfi, CJ Barberan∗, and Jia He∗
Table 1: AI-generated text detection techniques and their vulnerabilities.
Related Papers Method Main Idea Vulnerabilities
[2, 3, 30, 49,70] Supervised
detectionTo fine-tune a model on sets of AI and human
generated texts.Training on commonly used datasets, makes it
vulnerable to most attacks including paraphras-
ing.
[1, 16, 17, 35,51,55] Zero-shot
detectionTo use a pre-trained language model in zero-shot
settings.Reduces the risk of data poisoning attacks and
eliminates data and resource over- heads, but it is
still susceptible to other adversarial attacks like
spoofing and paraphrasing.
[26, 31, 44, 62] Retrieval-based
detectionTo apply methods of information retrieval to
match a given text with a collection of texts gener-
ated by LLMs and finding similarities in meaning.It is impractical because it requires a large and
updated collection of texts, which is computa-
tionally expensive, or may be unavailable for all
domains, tasks or models. It is also vulnerable to
paraphrasing and spoofing attacks.
[23, 24, 44, 65] Watermarking To use a model signature in the produced text
outputs to stamp particular pattern.The most trustworthy strategy, but is shown to be
fundamentally impossible for generative models.
It is susceptible to attacks such as rewording and
spoofing.
[35, 51, 66, 67,71] Feature-based
detectionTo identify and classify based on extracted dis-
criminating features.Susceptible to adversarial attacks such as para-
phrasing.
References
[1] 2023. ZeroGPT: AI Text Detector. https://www.zerogpt.com
[2]Wissam Antoun, Virginie Mouilleron, Benoît Sagot, and Djamé Seddah. 2023.
Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that
Easy to Detect? ArXiv abs/2306.05871 (2023).
[3]Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio Ranzato, and
Arthur D. Szlam. 2019. Real or Fake? Learning to Discriminate Machine from
Human Generated Text. ArXiv abs/1906.03351 (2019).
[4]Himanshu Batra, Narinder Singh Punn, Sanjay Kumar Sonbhadra, and Sonali
Agarwal. 2021. BERT-Based Sentiment Analysis: A Software Engineering Per-
spective. In International Conference on Database and Expert Systems Applications.
[5]Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, and Semih Yavuz. 2023.
Investigating Answerability of LLMs for Long-Form Question Answering. ArXiv
abs/2309.08210 (2023).
[6]Souradip Chakraborty, A. S. Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and
Furong Huang. 2023. On the Possibilities of AI-Generated Text Detection. ArXiv
abs/2304.04736 (2023).
[7]Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, and Song Hu. 2023. Can
Large Language Models Understand Content and Propagation for Misinformation
Detection: An Empirical Study. ArXiv abs/2311.12699 (2023).
[8]Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Xiaodong Song. 2017.
Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.
ArXiv abs/1712.05526 (2017).
[9]Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and
Dario Amodei. 2023. Deep reinforcement learning from human preferences.
arXiv:1706.03741 [stat.ML]
[10] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
Amodei. 2017. Deep Reinforcement Learning from Human Preferences. In Ad-
vances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran
Associates, Inc.
[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math
Word Problems. ArXiv abs/2110.14168 (2021).
[12] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi
Wang, and Tingwen Liu. 2023. FFT: Towards Harmlessness Evaluation and
Analysis for LLMs with Factuality, Fairness, Toxicity. ArXiv abs/2311.18580
(2023).
[13] A. Deshpande, Vishvak S. Murahari, Tanmay Rajpurohit, A. Kalyan, and Karthik
Narasimhan. 2023. Toxicity in ChatGPT: Analyzing Persona-assigned Language
Models. ArXiv abs/2304.05335 (2023).
[14] Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch.
2023. Improving Factuality and Reasoning in Language Models through Multia-
gent Debate.[15] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith.
2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language
models. arXiv preprint arXiv:2009.11462 (2020).
[16] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. 2019. GLTR:
Statistical Detection and Visualization of Generated Text. In Annual Meeting of
the Association for Computational Linguistics.
[17] Zhen Guo and Shangdi Yu. 2023. AuthentiGPT: Detecting Machine-Generated
Text via Black-Box Language Models Denoising. ArXiv abs/2311.07700 (2023).
[18] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
2020. REALM: Retrieval-Augmented Language Model Pre-Training (ICML’20).
JMLR.org, Article 368, 10 pages.
[19] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-
Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tris-
tan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Gan-
guli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane
Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom
Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah,
and Jared Kaplan. 2022. Language Models (Mostly) Know What They Know.
arXiv:2207.05221 [cs.CL]
[20] Mohammad Khalil and Erkan Er. 2023. Will ChatGPT get you caught? Rethinking
of Plagiarism Detection. arXiv:2302.04335 [cs.AI]
[21] Aisha Khatun and Daniel Brown. 2023. Reliability Check: An Analysis of GPT-3’s
Response to Sensitive Topics and Prompt Wording. ArXiv abs/2306.06199 (2023).
[22] Kiana Kheiri and Hamid Karimi. 2023. SentimentGPT: Exploiting GPT for Ad-
vanced Sentiment Analysis and its Departure from Current Machine Learning.
ArXiv abs/2307.10234 (2023).
[23] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and
Tom Goldstein. 2023. A Watermark for Large Language Models. In International
Conference on Machine Learning.
[24] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi
Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein.
2023. On the Reliability of Watermarks for Large Language Models. ArXiv
abs/2306.04634 (2023).
[25] George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich,
Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, and Eitan Farchi. 2023. Un-
veiling Safety Vulnerabilities of Large Language Models. ArXiv abs/2311.04124
(2023).
[26] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit
Iyyer. 2023. Paraphrasing evades detectors of AI-generated text, but retrieval is
an effective defense. ArXiv abs/2303.13408 (2023).
[27] Michael Kuchnik, Virginia Smith, and George Amvrosiadis. 2023. Validating
large language models with relm. Proceedings of Machine Learning and Systems 5
(2023).
[28] Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R. Fabbri,
Caiming Xiong, Shafiq R. Joty, and Chien-Sheng Wu. 2023. LLMs as Factual
6435Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text KDD ’24, August 25–29, 2024, Barcelona, Spain
Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv abs/2305.14540
(2023).
[29] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant
Misra. 2022. Solving Quantitative Reasoning Problems with Language Models.
arXiv:2206.14858 [cs.CL]
[30] Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi,
and Yue Zhang. 2023. Deepfake Text Detection in the Wild. ArXiv abs/2305.13242
(2023).
[31] Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. 2023.
GPT detectors are biased against non-native English writers. Patterns 4, 7 (2023),
100779. https://doi.org/10.1016/j.patter.2023.100779
[32] Zeyan Liu, Zijun Yao, Fengjun Li, and Bo Luo. 2023. Check Me If You Can:
Detecting ChatGPT-Generated Academic Writing using CheckGPT. ArXiv
abs/2306.05524 (2023).
[33] Ning Lu, Shengcai Liu, Ruidan He, and Ke Tang. 2023. Large Language Models
can be Guided to Evade AI-Generated Text Detection. ArXiv abs/2305.10847
(2023).
[34] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank
Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir
Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with
Self-Feedback. arXiv:2303.17651 [cs.CL]
[35] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and
Chelsea Finn. 2023. DetectGPT: Zero-Shot Machine-Generated Text Detection
using Probability Curvature. ArXiv abs/2301.11305 (2023).
[36] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. 2023. Use
of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities.
ArXiv abs/2308.12833 (2023).
[37] I Muneeswaran, Shreya Saxena, Siva Prasad, M V Sai Prakash, Advaith Shankar, V
Varun, Vishal Vaddina, and Saisubramaniam Gopalakrishnan. 2023. Minimizing
Factual Inconsistency and Hallucination in Large Language Models. ArXiv
abs/2311.13878 (2023).
[38] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,
Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, Charles Sutton, and Augustus Odena. 2022. Show Your Work:
Scratchpads for Intermediate Computation with Language Models.
[39] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and
William Wang. 2023. On the Risk of Misinformation Pollution with Large Lan-
guage Models.
[40] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and
William Wang. 2023. On the Risk of Misinformation Pollution with Large Lan-
guage Models. In Findings of the Association for Computational Linguistics: EMNLP
2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Compu-
tational Linguistics, Singapore, 1389–1403.
[41] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Benjamin Zhu, Lingjuan
Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. 2023. Are You
Copying My Model? Protecting the Copyright of Large Language Models for
EaaS via Backdoor Watermark. In ACL 2023.
[42] Mujahid Ali Quidwai, Chun Xing Li, and Parijat Dube. 2023. Beyond Black Box
AI-Generated Plagiarism Detection: From Sentence to Document Level. ArXiv
abs/2306.08122 (2023).
[43] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.
Explain Yourself! Leveraging Language Models for Commonsense Reasoning.
InProceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, Florence, Italy, 4932–4942.
https://doi.org/10.18653/v1/P19-1487
[44] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang,
and Soheil Feizi. 2024. Can AI-Generated Text be Reliably Detected? https:
//openreview.net/forum?id=NvSwR4IvLO
[45] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P. Dickerson, and Tom
Goldstein. 2020. Just How Toxic is Data Poisoning? A Unified Benchmark for
Backdoor and Data Poisoning Attacks. ArXiv abs/2006.12557 (2020).
[46] Damith Chamalke Senadeera and Julia Ive. 2022. Controlled Text Generation
using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility
of Generated Text in AI. ArXiv abs/2212.02924 (2022).
[47] Erfan Shayegani, Md. Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and
Nael B. Abu-Ghazaleh. 2023. Survey of Vulnerabilities in Large Language Models
Revealed by Adversarial Attacks. ArXiv abs/2310.10844 (2023).
[48] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik
Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal
Reinforcement Learning. arXiv:2303.11366 [cs.AI]
[49] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss,
Jeff Wu, Alec Radford, and Jasmine Wang. 2019. Release Strategies and the Social
Impacts of Language Models. ArXiv abs/1908.09203 (2019).
[50] Chris Stokel-Walker. 2022. AI bot ChatGPT writes smart essays-should academics
worry? Nature (2022).[51] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. DetectLLM:
Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated
Text. ArXiv abs/2306.05540 (2023).
[52] Edward Tian. 2023. https://gptzero.me/
[53] Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and Michail Mani-
atakos. 2023. HowkGPT: Investigating the Detection of ChatGPT-generated Uni-
versity Student Homework through Context-Aware Perplexity Analysis. ArXiv
abs/2305.18226 (2023).
[54] Ivan Vykopal, Mat’uvs Pikuliak, Ivan Srba, Róbert Móro, Dominik Macko, and
Mária Bieliková. 2023. Disinformation Capabilities of Large Language Models.
ArXiv abs/2311.08838 (2023).
[55] Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. 2023. Bot or Human?
Detecting ChatGPT Imposters with A Single Question. ArXiv abs/2305.06424
(2023).
[56] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves
Chain of Thought Reasoning in Language Models. In The Eleventh International
Conference on Learning Representations.
[57] Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov,
Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mah-
moud, Alham Fikri Aji, and Preslav Nakov. 2023. M4: Multi-generator, Multi-
domain, and Multi-lingual Black-Box Machine-Generated Text Detection. ArXiv
abs/2305.14902 (2023).
[58] Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tom’aš Foltýnek,
Jean Gabriel Guerrero-Dib, Olumide Popoola, Petr Sigut, and Lorna Waddington.
2023. Testing of detection tools for AI-generated text. International Journal for
Educational Integrity 19 (2023), 1–39.
[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits
Reasoning in Large Language Models. In Advances in Neural Information Pro-
cessing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho (Eds.).
[60] Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato,
Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary
Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney
Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S.
Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social
risks of harm from Language Models. ArXiv abs/2112.04359 (2021).
[61] Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie
Huang. 2023. Unveiling the Implicit Toxicity in Large Language Models. In
Conference on Empirical Methods in Natural Language Processing.
[62] Max Wolff. 2020. Attacking Neural Text Detectors. ArXiv abs/2002.11768 (2020).
[63] Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji.
2023. RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by
Reversing Chain-of-Thought. ArXiv abs/2305.11499 (2023).
[64] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021.
Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the
Embedding Layers in NLP Models. ArXiv abs/2103.15543 (2021).
[65] Xi Yang, Kejiang Chen, Weiming Zhang, Chang rui Liu, Yuang Qi, Jie Zhang,
Han Fang, and Neng H. Yu. 2023. Watermarking Text Generated by Black-Box
Language Models. ArXiv abs/2305.08883 (2023).
[66] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen.
2023. DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of
GPT-Generated Text. ArXiv abs/2305.17359 (2023).
[67] Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu,
Weiming Zhang, and Neng H. Yu. 2023. GPT Paternity Test: GPT Generated Text
Detection with GPT Genetic Inheritance. ArXiv abs/2305.12519 (2023).
[68] Munazza Zaib, Dai Hoang Tran, Subhash Sagar, Adnan Mahmood, Wei Emma
Zhang, and Quan Z. Sheng. 2021. BERT-CoQAC: BERT-Based Conversational
Question Answering in Context. In International Symposium on Parallel Architec-
tures, Algorithms and Programming.
[69] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap-
ping Reasoning With Reasoning. In Advances in Neural Information Processing
Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(Eds.).
[70] Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. 2023.
G3Detector: General GPT-Generated Text Detector. ArXiv abs/2305.12680 (2023).
[71] Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe
Ateniese, and Boaz Barak. 2023. Watermarks in the Sand: Impossibility of
Strong Watermarking for Generative Models. Cryptology ePrint Archive, Paper
2023/1776. https://eprint.iacr.org/2023/1776 https://eprint.iacr.org/2023/1776.
[72] Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. 2024. Relying on the
Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty.
[73] Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario
Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-Tuning Language
Models from Human Preferences. ArXiv abs/1909.08593 (2019).
6436