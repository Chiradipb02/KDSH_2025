From Variability to Stability: Advancing RecSys Benchmarking
Practices
Valeriy Shevchenko
Skoltech
Moscow, Russian Federation
valeriy.shevchenko@skoltech.ruNikita Belousov
Skoltech
Moscow, Russian Federation
nikita@nokiroki.ruAlexey Vasilev
Sber AI Lab
Moscow, Russian Federation
alexxl.vasilev@yandex.ru
Vladimir Zholobov
Skoltech
Moscow, Russian Federation
MIPT
Moscow, Russian Federation
v.zholobov@skoltech.ruArtyom Sosedka
Sber AI Lab
Moscow, Russian Federation
m1801239@edu.misis.ruNatalia Semenova
AIRI
Moscow, Russian Federation
Sber AI Lab
Moscow, Russian Federation
semenova.bnl@gmail.com
Anna Volodkevich
Sber AI Lab
Moscow, Russian Federation
volodkanna@yandex.ruAndrey Savchenko
Sber AI Lab
Moscow, Russian Federation
avsavchenko@hse.ruAlexey Zaytsev
Skoltech
Moscow, Russian Federation
BIMSA
Beijing, China
likzet@gmail.com
Abstract
In the rapidly evolving domain of Recommender Systems (RecSys),
new algorithms frequently claim state-of-the-art performance based
on evaluations over a limited set of arbitrarily selected datasets.
However, this approach may fail to holistically reflect their effec-
tiveness due to the significant impact of dataset characteristics
on algorithm performance. Addressing this deficiency, this paper
introduces a novel benchmarking methodology to facilitate a fair
and robust comparison of RecSys algorithms, thereby advancing
evaluation practices. By utilizing a diverse set of 30open datasets,
including two introduced in this work, and evaluating 11collabo-
rative filtering algorithms across 9metrics, we critically examine
the influence of dataset characteristics on algorithm performance.
We further investigate the feasibility of aggregating outcomes from
multiple datasets into a unified ranking. Through rigorous experi-
mental analysis, we validate the reliability of our methodology un-
der the variability of datasets, offering a benchmarking strategy that
balances quality and computational demands. This methodology
enables a fair yet effective means of evaluating RecSys algorithms,
providing valuable guidance for future research endeavors.
CCS Concepts
•Information systems →Recommender systems.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671655Keywords
Recommender Systems; Evaluation; Benchmarking; Datasets; Data
Characteristics
ACM Reference Format:
Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov,
Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko,
and Alexey Zaytsev. 2024. From Variability to Stability: Advancing RecSys
Benchmarking Practices. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671655
1 Introduction
Recommender systems have become the backbone of personalizing
user experiences across diverse online platforms. By suggesting
movies, recommending products, and curating news feeds [ 35],
RecSys is a key machine learning technology widely used in many
applications. Their impact drives ongoing development in both
academia and industry, resulting in the introduction of numerous
RecSys algorithms each year [59].
With this ongoing expansion, there is a growing need for tools
that enable reproducible evaluation, allowing researchers to as-
sess new methods alongside well-established baselines [ 22,29,61].
While several frameworks [ 3,60,74] excel in conducting a rigorous
evaluation of RecSys algorithms on a specific dataset, selecting the
best-performing models across multiple problems remains challeng-
ing. Results vary significantly based on the considered dataset, and
what works well in one context may perform poorly in another [ 13].
This variability often results in inconsistent conclusions from evalu-
ation studies, highlighting the importance of comparing algorithms
across datasets with various data characteristics. On the other hand,
extensive evaluation with dozens of datasets uses large amounts
of computational resources — and harms both the environment
5701
KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
and opportunities for small research labs. With researchers seeking
universally effective algorithms across different recommendation
tasks, businesses question algorithms’ performance on datasets that
reflect their specific industry domain or characteristics, trying to
shorten time-to-production for RecSys.
However, in contrast with other machine learning subdomains
like time series classification [ 6] and NLP [ 52], the field of RecSys
lacks an accepted performance aggregation method across multi-
ple datasets. Furthermore, there is limited research dedicated to
comparing and contrasting different recommendation datasets, un-
derstanding their impact on the performance of RecSys algorithms,
and identifying datasets with similar characteristics.
To deal with these problems, we develop a comprehensive bench-
mark methodology that can reliably rank RecSys methods based
on their performance across various problems using offline evalua-
tion, overcoming the limitations of current practices. Our approach
confidently determines whether a specific top-1 model can excel
universally or within particular domains defined by dataset char-
acteristics. While providing reliable results, we use only a small
number of datasets, enabling robust and efficient comparison.
Our contributions include:
•A benchmarking methodology tailored to the RecSys do-
main with a clear evaluation protocol and hyperparameter
tuning1.
•Utilization of 30 public datasets for benchmarking. Among
them, there are two new large-scale open-source datasets
from distinct RecSys domains (music and e-commerce)2.
•Comparative analysis of various metrics aggregation meth-
ods and their robustness stress tests to identify the most
suitable approach for RecSys multi-dataset benchmarking.
•Investigation into the relationship between specific dataset
characteristics and recommendation quality and identifica-
tion of dataset clusters with similar characteristics.
•Efficient comparison procedure that uses only 6 datasets
but provides similar ranking due to reasonable selection of
benchmarking datasets based on clustering.
•Identification of the top-performing algorithms from a pool
of 11 frequently used approaches based on principled metrics
aggregation across multiple scenarios.
2 Related work
RecSys evaluation. Recommender systems continue to be a dy-
namic research area. Traditional techniques like Neighborhood-
based models [ 55] and Matrix Factorization [ 34] remain reliable
baselines. However, incorporating Deep Neural Networks has no-
tably advanced RecSys, significantly enriching the domain [ 72].
This variety leads to the development of open-source libraries and
tools to address diverse application needs. Among the notewor-
thy ones, DeepRec [ 73], Implicit [ 23], LightFM [ 36], NeuRec [ 69],
RecBole [ 76], RecPack [ 44] and Replay [ 63] offer realizations of
popular recommendation algorithms.
1To guarantee the reproducibility of our results, all code and datasets employed in
experiments are available in the GitHub repository https://github.com/nokiroki/recsys-
evaluation-of-dozens-datasets.
2https://www.kaggle.com/datasets/alexxl/zvuk-dataset and https://www.kaggle.com/
datasets/alexxl/megamarketOffline evaluation remains essential in RecSys research as it
provides a reliable and cost-effective approach to assess algorithm
performance. It is particularly suitable for researchers who are
developing new models. As a part of offline evaluation, the variety in
the field leads to the need for rigorous and reproducible evaluation
methodologies. Notable studies such as Elliot [ 3], Recbole [ 74], and
DaisyRec [ 60] introduced comprehensive evaluation frameworks
in both reproducing and benchmarking recommendation models.
These frameworks offer a rich array of options for pre-filtering, data
splitting, evaluation metrics, and hyperparameter tuning across a
broad spectrum of popular recommender models. Notably, Elliot
uniquely provides statistical tests for robustly analyzing the final
results, adding a layer to the evaluation process.
RecSys datasets. Dozens of public datasets from diverse domains
are available for constructing and evaluating recommender systems.
The research [ 75] shows that most studies utilize, on average, 2.9
datasets, with dataset selection and preprocessing affecting eval-
uation outcomes significantly. Different data filtering techniques
can change data characteristics, leading to varied performance
rankings. Deldjoo et al. [ 18,19] investigated how data properties
impact recommendation accuracy, fairness, and vulnerability to
shilling attacks, highlighting the importance of data understand-
ing in enhancing system performance. The paper [ 13] emphasized
the crucial role of dataset diversity in RecSys algorithm evalua-
tions, showing that dataset choice significantly affects evaluation
conclusions. These findings collectively underscore the need for
considering dataset variability in future research to enhance the
reliability of recommender system evaluations.
Aggregating methods. When introducing a novel machine
learning approach, it is important to rigorously compare its per-
formance against existing methods across a comprehensive set
of relevant tasks to determine its standing relative to the current
state-of-the-art. However, drawing conclusions about the superior
algorithm based on the outcomes from multi-dataset benchmarks
can be challenging.
Various techniques have been developed to yield concise sum-
maries to address this challenge. One basic method involves mean
aggregation, assuming uniformity across task metrics [ 16]. How-
ever, this can lead to biases when metrics vary significantly [ 47].
The Dolan-Moré performance profiles, initially developed for op-
timization algorithm benchmarks [ 21], have gained traction for
evaluating machine learning algorithm efficacy across diverse prob-
lems [ 7]. Unlike mean aggregation, Dolan-Moré curves consider
the distribution of performance values, offering insight into how
frequently and significantly an algorithm excels. Similarly, the Crit-
ical Difference (CD) diagram [ 20] is frequently used to compare
algorithms across multiple tasks. This method of presenting results
has become broadly accepted [ 45], providing both groupwise and
pairwise comparisons. Groupwise comparisons are achieved by
ordering all methods based on the mean rank of relative perfor-
mance on each task. Pairwise comparisons are based on a critical
difference in this mean rank or, later [ 8], the Wilcoxon signed-rank
test [68] with multiple test corrections.
VOTE’N’RANK [ 52] is another framework proposed for rank-
ing systems in multitask benchmarks rooted in the principles of
5702From Variability to Stability: Advancing RecSys Benchmarking Practices KDD ’24, August 25–29, 2024, Barcelona, Spain
social choice theory. The framework employs scoring and majority-
relation-based rules, such as Plurality, Dowdall, Borda, Copeland,
and Minimax, to ensure a more comprehensive evaluation.
Benchmarking is a fundamental practice in machine learning,
crucial for measuring progress through datasets, metrics, and ag-
gregation methodologies to evaluate system performance. These
benchmarks are crucial for comparing new algorithms with es-
tablished ones to identify the most effective models for practical
use.
Performance benchmarks are essential across various fields. For
example, the ILSVRC (ImageNet Large Scale Visual Recognition
Challenge) [ 53] consider object classification and detection with
extensive image datasets and unique metrics for each task. In natu-
ral language processing (NLP), GLUE [ 67] and its derivatives [ 66]
benchmark models across diverse tasks, ranking them based on
mean score values. One example in the AutoML domain is AMLB [ 26],
which emphasizes multitask evaluation via mean ranking.
The research [ 2] offers an in-depth and reproducible evaluation
of ten collaborative filtering algorithms, employing a Borda count
ranking method to aggregate accuracy results from various datasets
and metrics. The study emphasizes that, although this method
provides valuable insights, it necessitates careful interpretation
due to biases favouring algorithms that perform well in correlated
metrics.
To the best of our knowledge, BARS [ 77] is the most advanced
benchmarking initiative focused on RecSys. Although BARS estab-
lishes an open benchmark with standardized evaluation protocols,
it presents certain limitations. For instance, it restricts itself to only
three datasets dedicated to the singular challenge of top-N rec-
ommendation, maintaining discrete leaderboards for each dataset.
Such an approach, devoid of a multi-dataset scoring mechanism,
inhibits discerning truly adaptive and universal models — covering
this gap might offer significant insights for researchers.
3 Methodology
Our aim is to present a robust and efficient benchmarking method-
ology tailored for the RecSys domain. We align our experimental
setup with online evaluation, replicating real-time recommendation
scenarios while ensuring the reproducibility of our benchmarking
results.
To achieve our goal, we collect a diverse set of open-source
datasets and establish a robust pipeline that incorporates predefined
procedural steps. Additionally, we integrate 11 RecSys algorithms
from various open-source libraries and repositories. This pipeline
serves a dual purpose: it streamlines the evaluation process and
enhances the comparability of results across different algorithms
and datasets. The pipeline scheme is shown in Figure 1.
The models evaluated in this study are collaborative filtering
methods that use only user-item interaction data. While industrial
scenarios often include rich user and item features, we focused
exclusively on interaction data. This approach allows for a straight-
forward comparison of algorithms, including those introduced re-
cently, across a diverse dataset spectrum. Consequently, our choice
enabled us to compare numerous algorithms and datasets, making
our comparison the most extensive in the current literature.3.1 Datasets and Preprocessing
In our benchmarking process, we use 30 public datasets, each with
timestamps, spanning seven diverse domains. These datasets cover
many business areas, including e-commerce, social networks, and
entertainment. Alongside the utilization of the 28 established public
datasets, we introduce two new ones, namely Zvuk and MegaMar-
ket, with their details provided in the Appendix A. This diversity is
summarized in Table 1.
Domain # Datasets
Movies and
clips9 Movielense (1, 10, 20M) [ 27], Net-
flix [ 9], Douban movies [ 40], Ama-
zon TV [ 46], Kuairec (full/small) [ 25],
Rekko [24]
Food and bever-
age5 BeerAdvocate [ 42], RateBeer [ 5],
Food [ 41], Amazon FineFoods [ 46],
Tafeng [14]
Social networks
(SN)4 Yelp review [ 31], Epinions [ 51], Red-
ditHyperlinks [37], DianPing [38]
Books 3 MTS library [ 57], Douban books [ 40],
GoodReads[64, 65]
Location-based
SN3 Gowalla [ 15], Brightkite [ 15],
FourSquare [70]
Music 3+ 1 Amazon CDs [ 46], Amazon Musical
Instruments [ 46], Douban music [ 40],
Zvuk [new]
E-commerce 1 + 1 Retailrocket [ 17],MegaMarket
[new]
Table 1: Dataset distribution by domains, # is the number
of datasets in a domain. The newly introduced Zvuk and
MegaMarket expand the most data-scarce domains.
Implicit feedback-based recommendation systems are increas-
ingly prevalent, primarily due to the frequent absence of explicit
rating information in many applications. Therefore, datasets that
initially include item ratings are usually transformed into binary
signals, an approach we have also implemented in our evalua-
tion. [ 2,60]. We have introduced a dataset-specific threshold pa-
rameter, denoted as 𝜏, to filter out interactions falling below this
threshold. Such interactions are considered negative feedback and
are thus removed from the datasets. For more on determining the
optimal𝜏value for individual datasets, refer to Appendix A.1.
In their initial state, the datasets exhibit a highly sparse nature,
characterized by a substantial proportion of users interacting with
a limited number of items, often fewer than five. As part of the
evaluation process, preprocessing steps are applied to filter out
inactive users and items. Most researchers either adopt a 5- or
10-filter/core preprocessing [ 60,61].𝐹-filter and𝐹-core filtering
techniques differ. The former simultaneously filters items and users
in a single pass, while the latter employs iterative filtering until
all users and items have a minimum of 𝐹interactions. We adopt
the 5-filter3methodology, prioritizing item filtering before user
3For the newly introduced datasets, Zvuk and MegaMarket, we have implemented a
50-filter due to their extensive size.
5703KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
Data Spliting
Global temporal
splitting 80/10/10
and filtering Matrix of quality
metrics for datasets
and open-source
modelsOpen-source
models:  EASE,
LightFM, ... Collection of
Open DatasetsHyperparameters
optimization
Optuna, 40 runsProtocol Setting
Maximize NDCG@10
metricsAggregating
metrics on all
datasets to rank
models:
- CD diagrams based
on Mean Ranks
- Performance curves
- ...
...Mean rank leaderboard
 #1. EASE
 #2. LightGCN
 #3. MultiV AE
 ...
Performance curves
Figure 1: Benchmarking methodology for ranking algorithms. Our main innovations are the curated list of datasets that enable
the option of comparison of pairs of models and aggregation strategies that provide principled ranking of approaches w.r.t.
various criteria.
filtering. Thus, each user has a minimum of 5 interactions, but some
items might have fewer.
3.2 Recommendation Models
Current recommendation frameworks enable the streamlined in-
tegration of widely-used baseline models and recently proposed
models. We have leveraged existing implementations of well-known
algorithms and developed an evaluation pipeline. This pipeline en-
compasses dataset filtering, data splitting, metrics computation,
and hyperparameter optimization. The frameworks we have used
include Implicit [23], LightFM [36], RecBole [76], and Replay [63].
Reflecting on recent relevant research in benchmarking [ 2,60],
we have selected the following categories of algorithms for our
analysis:
•Non-personalized baseline: Random and Popularity-based
recommendations (Random andMostPop).
•Neighborhood-based model: ItemKNN [55].
•Matrix factorization models: LightFM [36],ALS [30], and
BPR [50].
•Linear models: SLIM [48] and EASE [58].
•Neural models: MultiVAE [39],LightGCN [28], and Light-
GCL [10].
While our selection covers many recent approaches, one can add
new algorithms from various sources, thereby expanding the scope
and capability of the benchmark.
3.3 Evaluation Settings
Data splitting. A guiding principle for splitting data into train-
ing and test subsets is to resemble the deployment conditions
closely [12]. In the top-N recommendation paradigm, the primary
challenge is to infer user preferences from past interactions to pre-
dict future ones. Given this, the training data should chronologically
precede the test data, acting as the "history" followed by the "future"
at a designated time. This approach helps in mitigating the risk of
data leakage [ 32]. Therefore, we adopt the global temporal splitting
strategy with an 80/10/10training, validation, and test set ratios
following [ 11,32,43]. After splitting, we exclude cold-start users
and items with no record in the training set.
Negative Sampling. In the context of Recommender Systems
evaluation, negative sampling involves prediction and evaluation
for only a limited set of non-relevant items and known relevantitems instead of full item list scoring. These non-relevant items
are chosen from a candidate item pool. Although sampling strate-
gies like the Uniform Sampler have been used to avoid biases
in evaluating RecSys algorithms and to boost computational ef-
ficiency [ 6,60,71], studies have questioned their reliability [ 35].
Consequently, our evaluation involves testing on all unobserved
items.
Evaluation Metrics. The precise interpretation of popular qual-
ity metrics in the field lacks a consensus, and it is often observed
that the more complex a metric is, the greater the scope for vary-
ing interpretations [ 62]. Therefore, offering a detailed evaluation
protocol for reproducibility and clarity in the assessment is crucial.
In light of this, our approach is meticulous: we precisely define
each metric and accurately compute them within our established
pipeline.
To evaluate the performance of our models, we employ a spec-
trum of standard quality metrics, such as Precision@k, Recall@k,
nDCG@k, MAP@k, HitRate@k, and MRR@k. Our evaluation scope
extends further, incorporating beyond-accuracy objective metrics
that provide a more comprehensive view of model effectiveness.
These include Coverage@k, Diversity@k, and Novelty@k [33].
Hyperparameter Tuning. Hyperparameter optimization is cru-
cial for achieving optimal performance of machine learning algo-
rithms and ensuring reliable benchmarking. The paper [ 56] high-
lights that most RecSys baselines can attain between 90−95%of
their maximum performance within the initial 40iterations when
using Bayesian optimization. Leveraging this insight, we utilize the
Optuna framework [ 1] and apply the Tree of Parzen Estimators
(TPE) algorithm for hyperparameter tuning. In alignment with prior
research [ 4,61,75], we conduct hyper-parameter optimization with
respect to nDCG@10 for each baseline on each dataset.
After determining the optimal hyperparameters, we execute
a final training on the consolidated training and validation sets.
This procedure ensures that all available interactions up to the test
timestamp are incorporated, including the most recent ones.
3.4 Metrics Aggregation Methods
In our benchmarking process, we utilize a matrix containing ac-
quired metrics from various datasets and apply numerous aggrega-
tion approaches to analyze these data.
5704From Variability to Stability: Advancing RecSys Benchmarking Practices KDD ’24, August 25–29, 2024, Barcelona, Spain
Once evaluation metrics are collected, we should define a method
to rank algorithms using performance scores. Our pipeline uses
well-established methods to aggregate performance to a single rank
score over multiple datasets. These aggregations are adopted from
general Machine learning practice and reused for our problem of
RecSys methods ranking.
The list of aggregators includes arithmetic, geometric, and har-
monic mean aggregations of a quality metric, CD diagrams [ 20]
emphasizing mean ranks; Dolan-Moré (performance) curves [ 21]
featuring AUC values, and algorithms inspired by the social choice
theory, specifically the Copeland, and MinMax rules, proposed for
aggregation of results over various NLP tasks [52].
4 EXPERIMENTS AND RESULTS
4.1 Metrics
Our experiments begin with the collection of performance met-
rics to evaluate 11 recommendation algorithms across 30 datasets.
These metrics include User Preference Accuracy, Ranking Quality,
and Beyond-Accuracy metrics. Following the data collection, we
perform an initial analysis of the accumulated results, focusing on
Spearman’s correlation for each dataset and subsequently comput-
ing the average correlation scores. These results are consolidated
into a correlation heatmap, illustrating the relationships among all
pairs of metrics, as shown in Figure 2.
Recall NDCGMRR MapHit
PrecisionDiv Nov CovRecall
NDCG
MRR
Map
Hit
Precision
Div
Nov
Cov10.90.85 0.86 0.92 0.96 0.04 -0.03 -0.1
0.9 10.95 0.97 0.90.94 00.01 -0.08
0.85 0.95 10.94 0.89 0.89 00.01 -0.1
0.86 0.97 0.94 10.85 0.91 -0.01 0-0.09
0.92 0.90.89 0.85 10.93 0.05 -0.01 -0.08
0.96 0.94 0.89 0.91 0.93 10.02 -0.04 -0.12
0.04 0 0-0.01 0.05 0.02 10.29 0.29
-0.03 0.01 0.01 0-0.01 -0.04 0.29 10.63
-0.1-0.08 -0.1-0.09 -0.08 -0.12 0.29 0.63 10.00.20.40.60.81.0
Figure 2: Spearman correlation between metrics for 𝑘=10.
Darker blue indicates stronger correlations.
The structure of our correlation matrix is similar to the smaller-
scale experiment conducted by Zhao et al. [ 75]. The heatmap re-
veals that the Accuracy and Ranking metrics located in the top
left corner (𝑅𝑒𝑐𝑎𝑙𝑙 ,𝑛𝐷𝐶𝐺 ,𝑀𝑅𝑅 ,𝑀𝐴𝑃 ,𝐻𝑖𝑡𝑅𝑎𝑡𝑒 ,𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ) exhibit
high correlations with each other, surpassing 0.8. In contrast, the
Beyond-Accuracy metrics positioned at the bottom demonstrate
weak correlations with the rest. This distinction can be attributed
to the different goals of these metrics, as Beyond-Accuracy metrics
do not straightforwardly describe recommendation quality.
Moreover, our findings indicate that nDCG has the highest corre-
lation (≥0.9) with accuracy and ranking metrics. This observation
reinforces the usage of 𝑛𝐷𝐶𝐺 in benchmarking and during hyper-
parameter optimization as an optimization objective. Consequently,in the subsequent experiments, we utilize 𝑛𝐷𝐶𝐺 @10 unless other-
wise stated.
4.2 Comparative Analysis of Metrics
Aggregation Methods
In this section, our primary focus is on exploring various methods
to aggregate metrics derived from diverse datasets. We describe the
considered aggregation approaches and then analyze the ranking
of RecSys models using these methods.
Given the RecSys specifics [ 54,77], we identify key requirements
for a ranking method:
•Ranking: The benchmarking system should rank methods
according to their performance.
•Metric Value Consideration: It should consider the metric
values and their relative differences for specific problems,
not just the relative positions.
•Interpretability: The results should be interpretable, pro-
viding clear insights into model comparisons.
•Significance determination: The system should explicitly
define the significance of performance differences.
•Agnosticism to adversarial manipulations: The ranking
should be robust to malicious influence.
4.2.1 Considered aggregation approaches. We consider the follow-
ing ways to aggregate metrics:
Mean Ranks (MR). MR is used in Critical Difference diagrams
and computes the average ranks of methods across all datasets.
Mean Aggregations. These approaches utilize classic mathemat-
ical averages, such as Mean aggregation (MA), Geometric mean
(Geom. mean) andHarmonic mean (HM), calculating across datasets
for each model as a single model score.
Dolan-Moré Area Under Curve (DM-AUC). This relates to the
Dolan-Moré performance profiles defined for ˆ𝛽≥𝛽≥1as
𝑝𝑖(𝛽)=1
𝑑
𝑡:𝛽𝑞𝑡𝑖≥max
𝑗𝑞𝑡𝑗,
where𝑖is the index of the curve that corresponds to a RecSys
model,𝑡is the problem index, 𝑑is the number of datasets, ˆ𝛽is a
hyperparameter limiting the values of 𝛽,𝑞𝑡𝑖is the metric value that
corresponds to a RecSys model 𝑖and the problem 𝑡, and𝑞𝑡𝑗is the
metric value that corresponds to a RecSys model 𝑗and the problem
𝑡. The DM curve for a specific 𝛽reports the number of problems
for which the model performs no more than 𝛽times worse than
the best model for that problem (e.g., 𝑝𝑖(1)represents the share of
problems where the 𝑖-th algorithm is the best).
In the case of aggregation, we take the area under the DM curve
divided by the sum of all areas. In our experiments, we fixed ˆ𝛽=3;
below, we show that ranking remains stable across a wide range of
𝛽values.
Dolan-Moré leave-best-out (DM LBO). This approach ranks algo-
rithms by performing the following steps:
(1) Calculate DM AUC (area under curve) scores;
(2) Choose the best method using DM AUC and remove it;
(3)The method that is removed is assigned a rank based on the
iteration in which it was dropped;
5705KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
(4) Repeat the previous steps using the remaining methods.
Social Choice Theory. The last two aggregating approaches con-
sidered, Copeland andMinimax, are majority-relation-based rules.
A majority relation for two methods 𝑚𝐴,𝑚𝐵(𝑚𝐴≻𝑚𝐵) holds, if
𝑚𝐴has a higher metric value than 𝑚𝐵.Copeland method defines
the aggregation score 𝑢(𝑚)as𝑢(𝑚𝐴)=|𝐿(𝑚𝐴)|−|𝑈(𝑚𝐴)|, where
𝐿(𝑚𝐴)={𝑚|𝑚𝐴≻𝑚},𝑈(𝑚𝐴)={𝑚|𝑚≻𝑚𝐴}. The Minimax uses
a score𝑠(𝑚𝐴,𝑚𝐵), representing the number of datasets for which
method𝑚𝐴has a higher score than 𝑚𝐵. The aggregated score is
given by𝑢(𝑚𝐴)=−max𝐵𝑠(𝑚𝐵,𝑚𝐴).
4.2.2 Comparison of RecSys algorithms. One of our objectives is to
present interpretable results that facilitate swift visual comparisons
of the performance of RecSys algorithms across multiple datasets.
We present these visual comparisons using DM performance pro-
files and a CD diagram. The DM profiles are in Figure 3. Further, we
use the presented DM AUC to rank the algorithms. The CD diagram
can be found in Figure 4. In addition to the traditional CD diagram
that includes the pairwise Wilcoxon test, we have introduced the
Bayesian Signed-Rank test, indicated by dashed horizontal grey
lines. We exclude the concept of ROPE from our analysis because
it requires homogeneity among the set of metrics, which is not
applicable in RecSys. This inhomogeneity also leads to the absence
of statistical significance in the CD. While we use a large number
of datasets, due to their diversity, the ranks of approaches change a
lot.
Figure 3: Performance profiles for the comparison of RecSys
algorithms. The higher the curve, the better the performance
of the algorithm. We also provide AUCs for each approach.
However, our findings indicate that EASE emerges as the winner
for both options. There is no distinct second-place algorithm, as
areas under the performance profiles for LightGCN and MultiVAE
are almost identical. Finally, all methods perform significantly better
than a random approach and are mostly superior to the MostPop
baseline.
Leaderboard for different aggregations. Different aggregation meth-
ods yield distinct rankings for the approaches considered. With
the set of 30datasets, the ranks presented in Table 2 consistently
identify EASE as the top-performing approach. For the subsequent
top positions, we have a pair of candidates for most aggregations:
MultiVAE and LightGCN.
1 2 3 4 5 6 7 8 9 10 11
10.79Random
9.00MostPop
6.79BPR
6.25ItemKNN
5.71LightFM
5.50LightGCL5.32ALS5.25SLIM4.36LightGCN4.11MultiVAE2.93EASEnDCG@10Figure 4: The Critical Difference diagram for the comparison
of RecSys algorithms. The numbers represent the mean ranks
of methods over all datasets. Thick horizontal lines represent
a non-significance based on the Wilcoxon-Holmes test, while
dashed horizontal lines represent non-significance according
to the Bayesian Signed-Rank test.
4.2.3 Comparison of reliability of aggregations. To ensure a reli-
able aggregation method for benchmarking, it should demonstrate
stability under various perturbations, including adversarial ones.
We conduct an analysis to assess the robustness of the presented
aggregation methods.
First, we determine rankings based on 30 datasets across all
methods, establishing them as our reference benchmarks. Next, we
examine the sensitivity of these rankings to the following modifi-
cations of the input matrix of quality metrics:
(1) Inclusion or exclusion of a dataset.
(2) Introduction or removal of a RecSys algorithm.
(3)Incorporation or exclusion of a slightly superior/inferior
method to a particular algorithm, exploring all possible per-
mutations.
(4)Adjustments in the hyperparameters of an aggregation method.
Change of the set of used datasets. We measure the correlation
between the final rankings and the references using the Spearman
correlation coefficient 𝜌.
The results for the case of dropping datasets are presented in Fig-
ure 5. All aggregations exhibit a relatively stable behaviour, except
for Minimax, which shows a low 𝜌after dropping 15 datasets. On
the contrary, the best-performing method is Geom. mean, Harm.
mean and DM LBO, with their average metric values being less
influenced by specific datasets. Overall, different aggregation tech-
niques tend to produce similar rankings if the number of datasets
is large enough.
Furthermore, we explore the case when we use only five datasets
to calculate ranks. We randomly sampled 100 pairs of subsets of size
five and calculated Spearman’s correlation between aggregations
for each pair of sets of datasets. The results are in Table 3. Aggre-
gations are less stable in this case. Moreover, MA, Harm. mean and
Minimax methods have Spearman’s correlation of less than 0.8. As
in the case of dropping datasets, the outlier is Minimax model with
low𝜌. The Mean ranks andCopeland methods perform the best in
this scenario, demonstrating equivalent 𝜌values.
Elimination of RecSys methods. In this scenario, we compute
Spearman’s correlation between the results for all methods and the
results with the exclusion of some methods.
The results are presented in Figure 6. We observe that most
aggregating methods exhibit relatively stable behaviour, except
5706From Variability to Stability: Advancing RecSys Benchmarking Practices KDD ’24, August 25–29, 2024, Barcelona, Spain
Ranking
DM AUC↑ DM LBO↓ Mean ranks↓ MA↓ Geom. mean↓ Harm. mean↓ Copeland↑ Minimax↑
position
1
EASE: 0.121 EASE: 1 EASE: 2.933 EASE: 0.069 EASE: 0.042 EASE: 0.023 EASE: 10.0 EASE: -0.0
2 LightGCN: 0.111 LightGCN: 2 MultiVAE: 4.107 LightGCL: 0.065 LightGCN: 0.038 LightGCN: 0.021 MultiVAE: 8.0 SLIM: -21.0
3 MultiVAE: 0.111 LightGCL: 3 LightGCN: 4.363 LightGCN: 0.064 LightGCL: 0.038 ALS: 0.02 LightGCN: 6.0 MultiVAE: -22.0
4 LightGCL: 0.11 MultiVAE: 4 SLIM: 5.247 MultiVAE: 0.061 MultiVAE: 0.038 LightGCL: 0.02 SLIM: 3.0 LightGCN: -22.0
5 ALS: 0.106 ALS: 5 ALS: 5.32 LightFM: 0.059 ALS: 0.035 MultiVAE: 0.02 ALS: 2.0 LightGCL: -23.0
6 ItemKNN: 0.1 ItemKNN: 6 LightGCL: 5.5 SLIM: 0.058 LightFM: 0.034 ItemKNN: 0.018 LightGCL: 0.0 ALS: -24.0
7 LightFM: 0.1 LightFM: 7 LightFM: 5.707 BPR: 0.057 ItemKNN: 0.033 LightFM: 0.017 LightFM: -1.0 BPR: -25.0
8 SLIM: 0.093 BPR: 8 ItemKNN: 6.25 ALS: 0.057 BPR: 0.03 BPR: 0.014 ItemKNN: -4.0 ItemKNN: -26.0
9 BPR: 0.088 SLIM: 9 BPR: 6.793 ItemKNN: 0.056 SLIM: 0.025 MostPop: 0.006 BPR: -6.0 LightFM: -26.0
10 MostPop: 0.058 MostPop: 10 MostPop: 9.067 MostPop: 0.041 MostPop: 0.017 SLIM: 0.003 MostPop: -8.0 MostPop: -29.0
11 Random: 0.003 Random: 11 Random: 10.8 Random: 0.007 Random: 0.001 Random: 0.0 Random: -10.0 Random: -30.0Table 2: Rankings of RecSys methods according to different aggregation approaches with their respective scores. The leaderboard
is based on nDCG@10 values.
DM
AUC DM LBO Mean ranks MA Geom. mean Harm. mean Copeland Minimax
Par
eto efficacy + + + + ++ + +
Using small number of datasets + + + − +− + −
Using small number of methods + + − + ++ + −
Adding a new similar method + + − + ++ − −
Adding a new best method − + + + ++ + −
Changing hyperparameters − − NA NA NA NA NA NA
Spearman’s correlation, 5datasets 0.799 0.785 0.825 0.717 0.834 0.756 0.816 0.525
Spearman’s correlation, 10datasets 0.895 0.887 0.912 0.825 0.899 0.885 0.907 0.767
Table 3: General results of rankings reliability. +stands for the availability of a feature, −stands for an absence, and NA stands
for not applicable.
0 10 20
Number of datasets0.40.60.81.0Spearman's 
DM AUC
Mean ranks
DM LBO
Geom. mean
MA
Harm. mean
Copeland
Minimax
Figure 5: Stability of aggregations with respect to the number
of used datasets.
forMinimax. As the number of discarded methods increases, the
likelihood of changing the best method also increases. This notably
impacts the Mean ranks, DM AUC andCopeland methods.
Changing the hyperparameters of the aggregating method. In the
paper, only the DM AUC andDM LBO aggregating methods have
adjustable hyperparameters ( ˆ𝛽- the maximum value of the ratio
of the best metric value and the one under consideration, the right
4 6 8
Number of methods0.750.800.850.900.951.00Spearman's 
DM AUC
Mean ranks
DM LBO
Geom. mean
MA
Harm. mean
Minimax
CopelandFigure 6: Stability of aggregations with respect to the number
of used methods.
boundary of the X-axis in the performance profiles). We calculate
the Spearman correlation between the case when ˆ𝛽=3and the
case when ˆ𝛽can take any value. The results presented in Figure 9c
demonstrate that ˆ𝛽can influence the rankings, underscoring the
significance of determining and fixing the optimal value. The rank-
ing remains stable over a wide range of ˆ𝛽values, with DM LBO
showing more robustness compared to the pure DM AUC.
5707KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
Additional experiments. In Appendix B, we explore the stability
of a ranking provided by aggregations after the inclusion of a new
method slightly superior or inferior to an existing one. There, all
methods, except for Mean ranks, demonstrate stability under ad-
versarial perturbations. Moreover, our analysis delves into specific
intrinsic properties of a ranking system, such as Pareto efficiency
when using a small number of datasets. An aggregation method is
considered Pareto efficient if it outperforms another method for all
metrics. The results of all tests are presented in Table 3.
4.3 Dataset Characteristics
In addition to the performance benchmark, our study explores the
connection between specific dataset characteristics and recommen-
dation quality. We utilize user-item interaction matrix properties
from [ 18]. These properties serve as problem characteristics and
encompass various aspects, including the size, shape, and density
of the dataset (SpaceSize, Shape, Density ), as well as counts of users,
items, and interactions (Nu, Ni, Nr ). We also consider interaction fre-
quencies per user and item (Rpu, Rpi ), Gini coefficients that describe
interaction distribution among users and items (Giniu, Ginii ), and
statistics related to popularity bias and long-tail items (APB, StPB,
SkPB, KuPB, LTavg, LTstd, LTsk, LTku ) [18]. These characteristics of
the 30 selected datasets exhibit a wide range of variability.
To establish a connection between these data characteristics
and our primary quality metric, 𝑛𝐷𝐶𝐺 @10, we employed three
distinct measures: Pearson product-moment correlation, Spearman
rank — an order correlation, and Mutual information — a nonlinear
alternative. The obtained values are in Figure 7.
SpaceSizeShapeDensityNu Ni NrRpu RpiGiniu Ginii APB StPB SkPB KuPB LT avg LT std LT sk LTkuPearson
Spearman
Mutual Info-0.19 0.20 0.38 -0.03 -0.33 0.13 0.31 0.27 -0.27 0.19 0.53 0.58 -0.50 -0.44 -0.34 -0.47 -0.09 0.29
-0.32 0.45 0.63 -0.19 -0.52 0.17 0.46 0.63 -0.14 0.42 0.75 0.73 -0.58 -0.58 -0.34 -0.34 0.25 0.38
0.77 0.93 0.67 1.03 1.16 0.75 0.64 0.84 1.42 1.22 1.08 1.31 1.32 1.12 1.12 1.31 1.28 1.36
Figure 7: Pearson, Spearman correlations, and Mutual infor-
mation between data characteristics and RecSys algorithms
performance.
Datasets exhibiting higher levels of popularity bias APB and
Density tend to simplify the prediction tasks for recommender
models. Conversely, datasets exhibit long-tailed item distributions,
increased item diversity, and pronounced Popularity Bias, present-
ing a greater challenge for recommender models. Furthermore, the
moderate mutual information values emphasize the practical impact
of these characteristics on the performance of the models.
4.4 Optimizing Dataset Selection for
Benchmarking
Using 30 public datasets provides diverse evaluation characteris-
tics but is not computationally efficient. To decrease the use of
datasets while preserving variability, it would be practical to se-
lect datasets that may belong to the same group. We employ the
KMeans approach to split datasets into multiple clusters, using data
characteristics from the previous section as feature representations.
Principal datasets selection. To decrease time consumption
and minimize the degradation of benchmarking, we can run it onlyfor a limited number of datasets, carefully selecting them. We use
several approaches for selection: Random, KMeans, A-optimality,
and D-optimality approaches.
In Random, we uniformly at random select a subset of datasets
from the set. The KMeans identifies core datasets as the closest ones
to cluster centers for clusters being selected in the space of data
characteristics [ 13]. Two additional baselines are A-optimality and
D-optimality. They constitute two fundamental criteria focused
on obtaining the lowest possible error of a model that predicts
performance and the error for parameters estimation of a model [ 49].
Technical details are provided in Appendix C.
By selecting six datasets per method and calculating Spearman’s
correlation across 500 simulations, our results indicate superior
performance of the KMeans, as shown in Table 4.
Method nDCG10 HitRate10 Coverage
Random 0.834 0.855 0.939
D optimal 0.805 0.819 0.913
A optimal 0.669 0.687 0.887
KMeans 0.845 0.900 0.982
Table 4: Spearman correlation among metrics across six se-
lected datasets compared to the entire set of 30 datasets.
Clustering datasets using their characteristics With the
clustering approach described above, we have generated rankings
and metrics for different clusters as illustrated in Figure 8, and the
specific datasets for each cluster are listed in Table 5.
Cluster 1 consists of six datasets characterized by a high num-
ber of items relative to users. For example, the Amazon TV dataset
includes approximately ∼50K users,∼216K items, and∼2M in-
teractions. Cluster 2 also includes six datasets, each marked by
moderate characteristic values and relative sparsity. For instance,
the Reddit dataset comprises ∼4K users and items each, with
around∼70K interactions. Cluster 3 is smaller, containing just 3
datasets, where the number of users significantly exceeds the num-
ber of items. This pattern suggests a different interaction dynamic
compared to other clusters. Cluster 4 includes 9datasets with a
moderate number of items and users. Typically, the number of items
surpasses the number of users in these datasets, which have fewer
Datasets Cluster
Amazon CDs, Amazon TV, Gowalla, MTS library, 1
Amazon Musical Instruments, Yelp review
BeerAdvocate, DianPing, Douban movies, 2
Movielens 1M, RedditHyperlinks, Rekko
Movielens 10M, Movielens 20M, Ratebeer 3
Amazon FineFoods, Brightkite, Douban Books,
Douban music, Epinions, Food, 4
GoodReads, Retailrocket, Tafeng
Foursquare, Kuairec full, Kuairec small 5
Netflix, MegaMarket, Zvuk 6
Table 5: Datasets clusters obtained by described approach.
5708From Variability to Stability: Advancing RecSys Benchmarking Practices KDD ’24, August 25–29, 2024, Barcelona, Spain
Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6EASE
LightGCN
MultiVAE
LightGCL
ALS
ItemKNN
LightFM
SLIM
BPR
MostPop
Random2.17
(0.0136)3.0
(0.0585)2.0
(0.1244)3.44
(0.0298)4.33
(0.1044)1.33
(0.0789)
3.17
(0.0131)5.0
(0.0556)3.33
(0.1228)5.0
(0.0268)4.67
(0.1056)6.0
(0.0526)
4.17
(0.0115)3.67
(0.0575)4.67
(0.118)3.67
(0.0279)6.33
(0.0796)3.0
(0.0619)
3.67
(0.0121)8.0
(0.0496)5.33
(0.1149)5.67
(0.0259)3.67
(0.1683)7.0
(0.0483)
5.0
(0.0108)4.83
(0.0553)6.0
(0.1153)5.56
(0.0248)5.67
(0.0824)4.0
(0.0612)
5.17
(0.0096)5.17
(0.0551)7.33
(0.1092)6.44
(0.0236)8.0
(0.0676)5.67
(0.0587)
6.0
(0.0091)5.5
(0.0541)4.33
(0.1195)6.89
(0.0233)4.33
(0.0921)4.33
(0.0556)
8.33
(0.0015)3.33
(0.0585)3.67
(0.1219)4.89
(0.0268)5.0
(0.087)5.0
(0.0686)
8.17
(0.0059)7.17
(0.0465)8.33
(0.1024)5.67
(0.0228)4.67
(0.1591)8.67
(0.0464)
9.17
(0.0025)9.33
(0.0413)10.0
(0.082)7.78
(0.0125)10.33
(0.0577)10.0
(0.0256)
11.0
(0.0)11.0
(0.0018)11.0
(0.0024)11.0
(0.0003)9.0
(0.0318)11.0
(0.0004)
Figure 8: Ranks and Geom. mean (in brackets) aggregated
𝑛𝐷𝐶𝐺 @10 on different clusters of datasets.
interactions (generally ≤1M, with one exception). Moreover, the
SkPB and KuPB are highest in this cluster, meaning items have an
imbalanced probability distribution. For instance, the Retail dataset
features about∼32K users,∼52K items, and∼342K interactions,
with SkPB∼2and KuPB∼6.Cluster 5 contains only 3datasets,
which are the smallest in terms of user and item counts but are the
most densely populated, with densities ( ∼0.1). For example, the
Kuairec small dataset has about ∼1400 users,∼3100 items, and a
high density of 0.8. This cluster shows the lowest SkPB, indicating
a more uniform item usage across users. Cluster 6 consists of the
largest datasets, where both the user and item counts typically
≥100𝐾, and interactions≥20𝑀, denoting sparse interactions. A
prominent dataset in this cluster is MegaMarket, with ∼184K users,
∼167K items, and∼25M interactions. Note that the characteristics
of the datasets and the clusters identified herein are derived from
the preprocessed versions of these datasets.
Cluster Analysis Summary. The EASE consistently performs
well across most clusters, with LightGCL only outperforming it
in Cluster 5. Other approaches exhibit less stability. For example,
LightGCL has an average rank of 8for Cluster 2, while SLIM has the
second-best average rank for it. This variability is partly due to the
differences in cluster complexity, as evidenced by EASE’s fluctuat-
ing geometric mean from 0.0136 to0.1244. These results underline
the importance of using datasets with similar characteristics for
effective offline evaluation for business use cases.
4.5 RecSys Performance Variability over
Datasets
Drawing upon the insights from the benchmarks, we conclude the
main part of the article with an analysis of how different algorithms
perform in relation to specific dataset characteristics.
ItemKNN: Despite its interpretability and suitability as a base-
line, ItemKNN shows limited effectiveness, excelling only in datasets
with specific characteristics such as low Shape values or moder-
ate Density, including Movielens 1M, GoodReads, and Amazon MI.
These findings align with challenges of sparsity and scalability
typical of neighborhood-based models [55].Matrix Factorization Methods (LightFM, ALS, BPR): These
algorithms vary in performance depending on dataset characteris-
tics. Our analysis identifies that sparsity remains a significant limi-
tation [ 30,36]. While these methods serve as reliable baselines, their
efficacy is more pronounced in smaller datasets, like Foursquare,
where the data structure may not be extensively sparse. LightFM
tends to perform less efficiently in terms of computation time in
larger datasets. ALS demonstrates more robust performance across
various scales, making it a versatile choice for both moderately
sized and large datasets.
Linear Models (SLIM, EASE): These models show consistently
high performance across a variety of datasets. EASE, in particular,
excels in datasets with extensive user interaction data, support-
ing findings from [ 58]. However, the significant computational
demands of EASE, including both time and memory resources, may
limit its use in settings with restricted computational capabilities.
Neural and Graph-Based Models: Demonstrating superior
performance, especially in dense or highly connected environments,
MultiVAE and graph-based method LightGCN are particularly ef-
fective. MultiVAE performs well in datasets with moderate user
interactions, benefiting from Bayesian priors that help manage
the inherent uncertainty in sparse data. Graph-based models excel
in datasets with high connectivity, such as social networks (e.g.,
Gowalla, Yelp), where the relational structure can be fully exploited
to enhance recommendation quality.
5 Conclusions
Our paper introduces a novel benchmarking system for recom-
mender systems. It integrates a rigorous pipeline that leverages
multiple datasets, hyperparameter tuning, and validation strategy,
as well as an aggregation procedure for metrics across different
datasets. Our approach is interpretable and robust, working for
distinct metrics used for RecSys evaluation. Among the considered
methods, EASE is a clear winner with respect to all considered ag-
gregation strategies. Other methods show inferior performance on
average while being interesting for particular subdomains identified
by our clustering scheme.
Further research provides deeper insight related to the stability
and efficiency of ranking. Due to the usage of 30 datasets, two of
which are open-sourced in this study, the results are robust in di-
verse considered scenarios. Via our clustering procedure, we obtain
a collection of 6datasets that also provides a consistent ranking,
achieving efficiency and reliability simultaneously. Additional ex-
periments confirm the stability of our benchmark with respect to
reducing the number of considered datasets, methods, and adver-
sarial manipulation of the list of methods. Overall, our research
offers a streamlined guide and valuable datasets for advancing rec-
ommender system studies that can be used both by practitioners
during the selection of a method and researchers during the evalu-
ation of a novel idea.
6 Acknowledgments
The work was supported by the Analytical center under the RF
Government (subsidy agreement 000000D730321P5Q0002, Grant
No. 70-2021-00145 02.11.2021).
5709KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
References
[1]Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A next-generation hyperparameter optimization frame-
work. In ACM SIGKDD. 2623–2631.
[2]Vito Walter Anelli, Alejandro Bellogín, Tommaso Di Noia, Dietmar Jannach,
and Claudio Pomo. 2022. Top-N recommendation algorithms: A quest for the
state-of-the-art. In ACM UMAP. 121–131.
[3]Vito Walter Anelli, Alejandro Bellogín, Antonio Ferrara, Daniele Malitesta, Fe-
lice Antonio Merra, Claudio Pomo, Francesco Maria Donini, and Tommaso
Di Noia. 2021. Elliot: A comprehensive and rigorous framework for reproducible
recommender systems evaluation. In ACM SIGIR. 2405–2414.
[4]Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio, Claudio Pomo, and
Azzurra Ragone. 2019. On the discriminative power of hyper-parameters in
cross-validation and how to choose them. In ACM RecSys. 447–451.
[5]Ankurnapa. 2020. RateBeer Dataset competition. https://www.kaggle.com/
datasets/ankurnapa/rate-beer-data. Accessed: 2024-13-6.
[6]Alejandro Bellogín, Pablo Castells, and Iván Cantador. 2017. Statistical biases in
information retrieval metrics for recommender systems. Information Retrieval
Journal 20 (2017), 606–634.
[7]Mikhail Belyaev, Evgeny Burnaev, Ermek Kapushev, Maxim Panov, Pavel
Prikhodko, Dmitry Vetrov, and Dmitry Yarotsky. 2016. GTApprox: Surrogate
modeling for industrial design. Advances in Engineering Software 102 (2016),
29–39.
[8]Alessio Benavoli, Giorgio Corani, and Francesca Mangili. 2016. Should we really
use post-hoc tests based on mean-ranks? JMLR 17, 1 (2016), 152–161.
[9]James Bennett, Stan Lanning, et al .2007. The Netflix prize. In KDD cup and
workshop, Vol. 2007. New York, 35.
[10] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple
Yet Effective Graph Contrastive Learning for Recommendation. In ICLR.
[11] Pedro G Campos, Fernando Díez, and Iván Cantador. 2014. Time-aware recom-
mender systems: a comprehensive survey and analysis of existing evaluation
protocols. User Modeling and User-Adapted Interaction 24 (2014), 67–119.
[12] Pablo Castells and Alistair Moffat. 2022. Offline recommender system evaluation:
Challenges and new directions. AI Magazine 43, 2 (2022), 225–238.
[13] Jin Yao Chin, Yile Chen, and Gao Cong. 2022. The datasets dilemma: How much
do we really know about recommendation datasets?. In WSDM. 141–149.
[14] Chiranjivdas09. 2020. Ta Feng Grocery Dataset competition. https://www.
kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset
[15] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility:
user movement in location-based social networks. In ACM SIGKDD. 1082–1090.
[16] Pierre Jean A Colombo, Chloé Clavel, and Pablo Piantanida. 2022. Infolm: A new
metric to evaluate summarization & data2text generation. In AAAI Conference
on Artificial Intelligence, Vol. 36. 10554–10562.
[17] Jacek Dąbrowski, Barbara Rychalska, Michał Daniluk, Dominika Basaj, Konrad
Gołuchowski, Piotr Bąbel, Andrzej Michałowski, and Adam Jakubowski. 2021. An
efficient manifold density estimator for all recommendation systems. In ICONIP.
Springer, 323–337.
[18] Yashar Deldjoo, Alejandro Bellogin, and Tommaso Di Noia. 2021. Explaining rec-
ommender systems fairness and accuracy through the lens of data characteristics.
Information Processing & Management 58, 5 (2021), 102662.
[19] Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, and Felice Antonio
Merra. 2020. How dataset characteristics affect the robustness of collaborative
recommendation models. In SIGIR. 951–960.
[20] Janez Demšar. 2006. Statistical comparisons of classifiers over multiple data sets.
The Journal of Machine learning research 7 (2006), 1–30.
[21] Elizabeth D Dolan and Jorge J Moré. 2002. Benchmarking optimization software
with performance profiles. Mathematical programming 91 (2002), 201–213.
[22] Maurizio Ferrari Dacrema, Simone Boglio, Paolo Cremonesi, and Dietmar Jannach.
2021. A troubling analysis of reproducibility and progress in recommender
systems research. ACM TOIS 39, 2 (2021), 1–49.
[23] Ben Frederickson. 2017. Implicit: Fast Python Collaborative Filtering for Implicit
Datasets. https://github.com/benfred/implicit. Accessed: 2024-13-6.
[24] G0ohard. 2019. Rekko Dataset competition. https://www.kaggle.com/datasets/
g0ohard/rekko-challenge Accessed: 2024-13-6.
[25] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiang-
nan He, Jiaxin Mao, and Tat-Seng Chua. 2022. KuaiRec: A Fully-Observed Dataset
and Insights for Evaluating Recommender Systems. In ACM CIKM. 540–550.
[26] Pieter Gijsbers, Marcos LP Bueno, Stefan Coors, Erin LeDell, Sébastien Poirier,
Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. 2024. AMLB: an AutoML
benchmark. Journal of Machine Learning Research 25, 101 (2024), 1–65.
[27] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History
and Context. ACM TIIS 5, 4, Article 19 (dec 2015), 19 pages.
[28] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In ACM SIGIR. 639–648.
[29] Balázs Hidasi and Ádám Tibor Czapp. 2023. The Effect of Third Party Implemen-
tations on Reproducibility. In ACM RecSys. 272–282.[30] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for
implicit feedback datasets. In IEEE ICDM. Ieee, 263–272.
[31] Yelp Inc. 2014. Yelp Dataset competition. https://www.yelp.com/dataset. Ac-
cessed: 2024-13-6.
[32] Yitong Ji, Aixin Sun, Jie Zhang, and Chenliang Li. 2023. A critical study on
data leakage in recommender system offline evaluation. ACM Transactions on
Information Systems 41, 3 (2023), 1–27.
[33] Marius Kaminskas and Derek Bridge. 2016. Diversity, serendipity, novelty, and
coverage: a survey and empirical analysis of beyond-accuracy objectives in
recommender systems. ACM TiiS 7, 1 (2016), 1–42.
[34] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech-
niques for Recommender Systems. Computer 42, 8 (2009), 30–37.
[35] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recom-
mendation. In ACM SIGKDD. 1748–1757.
[36] Maciej Kula. 2015. Metadata Embeddings for User and Item Cold-start Recom-
mendations. In Proc. Workshop on New Trends on Content-Based Recommender @
RecSys 2015 (CEUR Workshop Proc., Vol. 1448). 14–21.
[37] Srijan Kumar, William L Hamilton, Jure Leskovec, and Dan Jurafsky. 2018. Com-
munity interaction and conflict on the web. In Conference on WWW. 933–943.
[38] Hui Li, Dingming Wu, Wenbin Tang, and Nikos Mamoulis. 2015. Overlapping
Community Regularization for Rating Prediction in Social Recommender Systems.
InRecSys. 27–34.
[39] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.
Variational autoencoders for collaborative filtering. In Conference on WWW.
689–698.
[40] Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011.
Recommender systems with social regularization. In ACM WSDM. 287–296.
[41] Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2019.
Generating Personalized Recipes from Historical User Preferences. In EMNLP-
IJCNLP. 5976–5982.
[42] Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and
attributes from multi-aspect reviews. In IEEE ICDM. 1020–1025.
[43] Zaiqiao Meng, Richard McCreadie, Craig Macdonald, and Iadh Ounis. 2020. Ex-
ploring data splitting strategies for the evaluation of recommendation models. In
ACM RecSys. 681–686.
[44] Lien Michiels, Robin Verachtert, and Bart Goethals. 2022. RecPack: An(other)
Experimentation Toolkit for Top-N Recommendation using Implicit Feedback
Data. In ACM RecSys. 648–651.
[45] Matthew Middlehurst, James Large, Michael Flynn, Jason Lines, Aaron Bostrom,
and Anthony Bagnall. 2021. HIVE-COTE 2.0: a new meta ensemble for time
series classification. Machine Learning 110, 11-12 (2021), 3211–3243.
[46] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP.
188–197.
[47] Christina Nießl, Moritz Herrmann, Chiara Wiedemann, Giuseppe Casalicchio,
and Anne-Laure Boulesteix. 2022. Over-optimism in benchmark studies and the
multiplicity of design and analysis options when interpreting their results. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery 12, 2 (2022),
e1441.
[48] Xia Ning and George Karypis. 2011. Slim: Sparse linear methods for top-n
recommender systems. In IEEE ICDM. 497–506.
[49] Friedrich Pukelsheim. 2006. Optimal design of experiments. SIAM.
[50] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. 452–
461.
[51] Matthew Richardson, Rakesh Agrawal, and Pedro Domingos. 2003. Trust man-
agement for the semantic web. In ISWC. Springer, 351–368.
[52] Mark Rofin, Vladislav Mikhailov, Mikhail Florinsky, Andrey Kravchenko, Tatiana
Shavrina, Elena Tutubalina, Daniel Karabekyan, and Ekaterina Artemova. 2023.
Vote’n’Rank: Revision of Benchmarking with Social Choice Theory. In EACL.
670–686.
[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al .
2015. Imagenet large scale visual recognition challenge. International journal of
computer vision 115 (2015), 211–252.
[54] Alan Said and Alejandro Bellogín. 2014. Comparative recommender system
evaluation: benchmarking recommendation frameworks. In ACM RecSys. 129–
136.
[55] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based
collaborative filtering recommendation algorithms. In Conference on WWW. 285–
295.
[56] Tobias Schnabel. 2022. Where Do We Go From Here? Guidelines For Offline
Recommender Evaluation. arXiv preprint arXiv:2211.01261 (2022).
[57] Sharthz23. 2020. MTS Library Dataset competition. https://www.kaggle.com/
datasets/sharthz23/mts-library Accessed: 2024-13-6.
[58] Harald Steck. 2019. Embarrassingly shallow autoencoders for sparse data. In
Conference on WWW. 3251–3257.
5710From Variability to Stability: Advancing RecSys Benchmarking Practices KDD ’24, August 25–29, 2024, Barcelona, Spain
[59] Aixin Sun. 2023. Take a Fresh Look at Recommender Systems from an Evaluation
Standpoint. In ACM SIGIR. 2629–2638.
[60] Zhu Sun, Hui Fang, Jie Yang, Xinghua Qu, Hongyang Liu, Di Yu, Yew-Soon Ong,
and Jie Zhang. 2022. DaisyRec 2.0: Benchmarking Recommendation for Rigorous
Evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[61] Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. 2020.
Are we evaluating rigorously? benchmarking recommendation for reproducible
evaluation and fair comparison. In ACM RecSys. 23–32.
[62] Yan-Martin Tamm, Rinchin Damdinov, and Alexey Vasilev. 2021. Quality metrics
in recommender systems: Do we calculate metrics consistently?. In ACM RecSys.
708–713.
[63] Alexey Vasilev. 2020. RePlay: A library for building recommender system models
using PySpark. https://github.com/sberbank-ai-lab/RePlay. Accessed: 2024-13-6.
[64] Mengting Wan and Julian J. McAuley. 2018. Item recommendation on monotonic
behavior chains. In ACM RecSys. ACM, 86–94.
[65] Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley. 2019.
Fine-Grained Spoiler Detection from Large-Scale Review Corpora. In ACL. 2605–
2610.
[66] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier bench-
mark for general-purpose language understanding systems. NeurIPS 32 (2019).
[67] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for
Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 353–355.
[68] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-
throughs in Statistics: Methodology and Distribution. Springer, 196–202.
[69] Bin Wu, Zhongchuan Sun, He Xiangnan, Xiang Wang, and Jonathan Staniforth.
2020. NeuRec: An Open Source Neural Recommender Library. https://github.
com/wubinzzu/NeuRec. Accessed: 2024-13-6.
[70] Dingqi Yang, Daqing Zhang, Zhiyong Yu, and Zhiwen Yu. 2013. Fine-grained
preference-aware location search leveraging crowdsourced digital footprints
from LBSNs. In ACM UBICOMP. 479–488.
[71] Longqi Yang, Yin Cui, Yuan Xuan, Chenyang Wang, Serge Belongie, and Debo-
rah Estrin. 2018. Unbiased offline recommender evaluation for missing-not-at-
random implicit feedback. In ACM RecSys. 279–287.
[72] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-
ommender system: A survey and new perspectives. ACM CSUR 52, 1 (2019),
1–38.
[73] Wen Zhang, Yuhang Du, Taketoshi Yoshida, and Ye Yang. 2019. DeepRec: A
deep neural network approach to recommendation with item embedding and
weighted loss function. Information Sciences 470 (2019), 121–140.
[74] Wayne Xin Zhao, Yupeng Hou, Xingyu Pan, Chen Yang, Zeyu Zhang, Zihan Lin,
Jingsen Zhang, Shuqing Bian, Jiakai Tang, Wenqi Sun, et al .2022. RecBole 2.0:
towards a more up-to-date recommendation library. In ACM CIKM. 4722–4726.
[75] Wayne Xin Zhao, Zihan Lin, Zhichao Feng, Pengfei Wang, and Ji-Rong Wen. 2022.
A revisiting study of appropriate offline evaluation for top-N recommendation
algorithms. ACM Transactions on Information Systems 41, 2 (2022), 1–41.
[76] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan,
Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al .2021. Recbole: Towards a
unified, comprehensive and efficient framework for recommendation algorithms.
InACM CIKM. 4653–4664.
[77] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi
Xiao, and Rui Zhang. 2022. BARS: Towards open benchmarking for recommender
systems. In ACM SIGIR. 2912–2923.
A New Datasets
We introduce two new datasets named MegaMarket and Zvuk.
Their size exceeds the sizes of many publicly accessible datasets.
Additionally, these datasets are from fields that are less commonly
represented in existing research.
MegaMarket documents user interactions, tracking events like
views, favorites, additions to carts, and purchases over a five-month
period from January 15 to May 15, 2023. It comprises a total of
196,644,020 events involving 3,562,321 items across 10,001 distinct
categories, contributed by 2,730,776 unique users.
Content:
•User IDs: Distinct identifiers for users, totaling 2,730,776.
•Datetimes: Timestamp range from 01.15.2023 00:00:00.708
to 14.05.2023 20:59:59.000.
•Events: Categorized by unique codes, comprised of:–0: View
–1: Favorites
–2: Add to cart
–3: Purchase
•Item IDs: Specific identifiers for items, amounting to 3,562,321.
•Category IDs: Denoting which of the 10,001 unique cate-
gories an item belongs to.
•Prices: Prices of items, normalized to follow a N(0,1)dis-
tribution.
Zvuk tracks user song-listening activity over the same five-
month period. It includes 244,673,551 events across 12,598,314 lis-
tening sessions. These sessions, initiated by 382,790 unique users,
encompass 1,506,950 individual tracks. This dataset is specifically
tailored to music and excludes other forms of auditory content,
such as podcasts or audiobooks.
Content:
•User IDs: Individual identifiers, with a count of 382,790
users.
•Session IDs: IDs for users’ listening sessions, totaling
12,598,314.
•Datetimes: Timestamps spanning from
01.15.2023 to 14.05.2023.
•Track IDs: Identifiers for the music tracks, encompassing
1,506,950 unique tracks.
•Play Durations: Scaled durations of tracks played, consid-
ering tracks where at least 30% of the song’s duration was
completed.
A.1 Preprocessing
We focus on collaborative filtering, converting datasets into implicit
feedback through threshold binarization. For datasets with ratings
from 0 to 5, we use a threshold of 3.5for positive feedback. For
datasets with weights from 0 to 1, such as percentage-based data,
we use a threshold of 0.3. Other datasets use thresholds based on
the drop ratio.
For datasets like MegaMarket, which track user behavior via
events, preprocessing is required to handle repeated user-item pairs.
We preprocess the data in the following manner:
(1) Assign weights using the formula:
Íall_interactionsÍtype_interactions(1)
where type_interactions denotes the count of events of a
specific type. Thus, rarer events receive higher weights.
(2)Aggregate events by pairing users and items, retaining only
the most frequent event type. We establish weight bound-
aries to ensure that events of a lesser significance never
outnumber those of greater importance.
(3)Treat the newly assigned weights as ratings and apply thresh-
old binarization accordingly.
This preprocessing approach ensures consistent dataset handling
in line with the principles of collaborative filtering.
5711KDD ’24, August 25–29, 2024, Barcelona, Spain Valeriy Shevchenko, et al.
1.025 1.050 1.075 1.100 1.125 1.150
0.9800.9850.9900.9951.0001.005Spearman's 
DM AUC
Mean ranks
DM LBO
Geom. mean
MA
Harm. mean
Copeland
(a) Comparison of stability of aggregations with respect to
a new similar method.
1 2 3 4
0.00.20.40.60.81.0Spearman's 
DM AUC
Mean ranks
DM LBO
Geom. mean
MA
Harm. mean
Minimax
Copeland(b) Comparison of stability of aggregations
with respect to the new best method.
2 4 6 8 10
0.880.900.920.940.960.981.00Spearman's 
DM AUC
DM LBO
Default  DM
(c) Comparison of DM aggregation approaches
via Spearman correlation across different hy-
perparameters.
Figure 9: Reliability evaluation for various aggregation methods.
B Additional Experiments
B.1 Reliability Tests: Incorporation or
Exclusion of Marginally Different Methods
We examine the impact of introducing a model similar to existing
ones, using a parameter 𝛼to adjust the metrics of a chosen model.
We set|1−𝛼|≤0.15. For example, the case when 𝛼=1.means
adding the new model equal to the selected one.
Adjusting𝛼makes the new model perform slightly better or
worse than the chosen model. Results, shown in Figure 9a, indi-
cate most aggregation methods are stable, except for Mean ranks
andCopeland. The Minimax method is excluded due to poor
Spearman correlation.
B.2 Reliability tests: Adding a New Best Method
We analyze the impact of introducing a new best model. We identify
the best metric values across all datasets and utilize an arbitrary
value𝛼(following similar conditions as in Section B.1). We restrict
the parameter as follows: 𝛼∈[1,4]. For instance, when 𝛼=1, it
signifies the addition of a new model with metrics equal to the
current best metrics.
Figure 9b shows the Minimax method is unstable for any 𝛼,
andDM AUC becomes unstable as 𝛼→4. This behavior can be
attributed to the direct dependence of the best metric values on
the parameter 𝛼. Conversely, all other aggregation methods remain
entirely stable in this scenario.
B.3 Sensitivity to Hyperparameters of
Aggregations
The results for varying the hyperparameters case are presented
in Figure 9c. The vertical dotted line mean the case when ˆ𝛽=3.
We see that the DM AUC aggregating method is more stable than
theDM LBO method. Instability of the DM AUC can be interpreted
by the proximity of the curves of the methods (in Figure 3, for
example, LightFM AUC andMultiVAE AUC curves look visually
similar to each other). If the more methods have similar perfomance
curves, then the more the Spearman correlation value decreases (inFigure 3, for example, LightFM AUC, MultiVAE AUC andItemKNN
AUC curves for ˆ𝛽→1.+0).
C Choosing the Optimal Subset of Datasets
In the KMeans clustering process, we first standardize the data to
ensure uniformity. Next, we apply Principal Component Analysis
(PCA) to reduce the number of dimensions while preserving as
much variance as possible, addressing the problem of correlated
features. We then use the Isolation Forest method to detect and
remove outliers, decreasing the dataset size from 30 to 25 observa-
tions. With the data now prepared, we move on to the clustering
stage, employing the K-means algorithm. During clustering, we
evaluate the Silhouette Coefficient and Davies-Bouldin Scores to
determine the optimal number of clusters, which we found to be 6.
Finally, we select datasets that are closest to the cluster centres for
further analysis.
The next two approaches assume that resulting metrics can be
predicted with the linear regression method:
y=xΘ+𝜀, 𝜀∼N( 0,𝜎2),
where yis metric on datasets, Θmeans model parameters, xmeans
datasets feature and 𝜀is a noise value.
The Maximum Likelihood Estimation weights can be shown as
Θ∼N
(x𝑇x)−1xxΘ,(1
𝜎2x𝑇x)−1
The D-optimality approach we formulate as maximization of
x𝑇xdeterminant. As a result, we minimize the estimation variance.
We formulate the A-optimality as a result of the minimization of
mean model loss. In this case, we assume that the prior distribution
is standard normal distribution 𝑝(𝑥)=N(0,𝐼)
𝑄(D)=∫
(𝑦(𝑥)−ˆ𝑦(𝑥))2𝑝(𝑥)𝑑𝑥=1
3tr((x𝑇x)−1)
These problems are discrete optimization, and the naive solution
is a Greedy algorithm. After selecting the starting datasets’ subset,
we perform a complete search across each dataset, choosing datasets
with high values of the target function.
5712