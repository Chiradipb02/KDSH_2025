Fast and Accurate Domain Adaptation for
Irregular Tensor Decomposition
Junghun Kim
Seoul National University
Seoul, South Korea
bandalg97@snu.ac.krKa Hyun Park
Seoul National University
Seoul, South Korea
kahyun.park@snu.ac.krJun-Gi Jang
University of Illinois
Urbana-Champaign
Urbana, Illinois, USA
jungi@illinois.eduU Kang
Seoul National University
Seoul, South Korea
ukang@snu.ac.kr
Abstract
Given an irregular tensor from a newly emerging domain, how can
we quickly and accurately capture its patterns utilizing existing
irregular tensors in multiple domains? The problem is of great im-
portance for various tasks such as nding patterns of a new disease
using pre-existing diseases data. This is challenging as new target
tensors have limited information due to their recent emergence.
Thus, carefully utilizing the existing source tensors for analyzing
the target tensor is helpful. PARAFAC2 decomposition is a strong
tool for nding the patterns of irregular tensors, and the patterns
are used in many applications such as missing value prediction and
anomaly detection. However, previous PARAFAC2-based works
cannot adaptably handle newly emerging target tensors utilizing
the source tensors.
In this work, we propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaptation method for irregular tensor decomposition. M/e.sc/t.sc/a.sc/hyphen.scP2
generates a meta factor matrix from the multiple source domains,
by domain adaptation and meta-update steps. M/e.sc/t.sc/a.sc/hyphen.scP2 quickly
and accurately nds the patterns of the new irregular tensor utiliz-
ing the meta factor matrix. Extensive experiments on real-world
datasets show that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in var-
ious downstream tasks including missing value prediction and
anomaly detection tasks.
CCS Concepts
•Computing methodologies !Factorization methods; Trans-
fer learning.
Keywords
multi-domain, irregular tensor, tensor decomposition
ACM Reference Format:
Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang. 2024. Fast and
Accurate Domain Adaptation for Irregular Tensor Decomposition . In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671670
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specic permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671670
New themeMultiplesectorsTensors ofsource domainsNew tensor oftarget domainResult
Decomposition resultsfor the target tensorPatterns of AIGivenGoal
Utilize source domainsHealth careMaterialsFinancialsEnergyTimesStocksTimesDatabaseAI
StocksFigure 1: An example of domain adaptation for irregular ten-
sors. We are given a stock database, which is represented as ir-
regular tensors of multiple domains. Each domain represents
a specic sector, and each slice X𝑑;𝑘denotes time-varying
stock information (such as opening and closing prices) of 𝑘-th
stock in sector 𝑑. Then, we encounter a new unseen themed-
stocks (target tensor). The goal is to nd patterns within this
new tensor through tensor decomposition exploiting data
from multiple source domains.
1 Introduction
How can we quickly and accurately capture the patterns of a new
irregular tensor from a target domain utilizing irregular tensors from
various source domains? Many real-world data often take the form
of irregular tensors across various domains. For example, stocks
from diverse industries are represented as irregular tensors in mul-
tiple domains. Despite their distinct listing periods, these tensors
share a common set of features, such as the opening or closing
price. In this scenario, the objective is to eciently and accurately
capture patterns within the stocks of a recently emerged industry
by leveraging pre-existing stock data (see Figure 1). The challenge
involves addressing how to quickly identify the patterns of a new
irregular tensor within the target domain, utilizing insights gained
from irregular tensors sourced from various domains. The problem
includes many other scenarios such as nding new disease patterns
using pre-existing diseases data as well as nding action patterns
of a new person using the records of multiple people.
To nd patterns of high-dimensional data, decomposition-based
methods have been actively studied in data mining area [ 24,36?
]. Among the various methods, PARAFAC2 decomposition meth-
ods are widely used for nding the patterns of irregular tensors
by representing them with low-rank latent factor matrices. The
 
1383
KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
low-rank latent factor matrices are used for many applications in-
cluding missing value prediction [ 3], anomaly detection [ 5], failure
prediction [43], and image recommendation [30].
However, existing PARAFAC2-based decomposition methods are
not easy to be directly used in our problem. There are two chal-
lenges for them: 1) the limited information of target tensor due to
their recent emergence, and 2) disparities between the domains (e.g.,
stock movements dier across various industries). A naive approach
would be to conduct PARAFAC2 decomposition only on the new
target tensor. This neglects valuable information from the source
domains, and leads to an inaccurate decomposition of the target
tensor. Another approach would perform PARAFAC2 decomposi-
tion on source and target tensors simultaneously by combining all
tensors. However, this approach fails to nd accurate latent factor
matrices of the target tensor if the source and target domains have
dierent properties. Furthermore, the approach should decompose
the tensors from entire domains whenever a new target tensor is
given, thus is computationally inecient. Therefore, it is important
to carefully extract useful information from source domains that
help analyze the target tensor in advance.
In this work, we propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaptation method for irregular tensor decomposition. M/e.sc/t.sc/a.sc/hyphen.scP2
carefully learns a general latent factor matrix from multiple source
domains based on the meta-learning approach (see Section 2.3
for details). Then the latent factor matrix is used for analyzing a
new target tensor. This process enables us to eectively transfer
the general knowledge from source domains to the target domain,
leading to accurate decomposition of the target tensor.
M/e.sc/t.sc/a.sc/hyphen.scP2 successfully tackles the challenges emerged in previous
approaches. First, M/e.sc/t.sc/a.sc/hyphen.scP2 eciently supplements the limited in-
formation of the target tensor by proactively extracting useful infor-
mation from the source tensors. Unlike existing methods, M/e.sc/t.sc/a.sc/hyphen.scP2
does not require tensors from entire domains to be decomposed
together when a new target tensor emerges. Second, M/e.sc/t.sc/a.sc/hyphen.scP2 accu-
rately decomposes the target tensor in scenarios where the source
and target domains exhibit distinct characteristics. This is achieved
by extracting general and easily adaptable information from the
source domains, instead of domain-specic one.
Our contributions are summarized as follows:
Method. We propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain
adaption method for decomposing irregular tensors. M/e.sc/t.sc/a.sc/hyphen.sc
P2eectively nds the patterns of a new irregular tensor via
meta-learning.
Theory. We give theoretical analysis on the properties of
M/e.sc/t.sc/a.sc/hyphen.scP2, including the time complexities, and the relation
between the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 and the update
step of the previous meta-learning approach.
Experiments. Extensive experiments on real-world datasets
show that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in missing
value prediction and anomaly detection problems.
The rest of this paper is organized as follows. Section 2 gives
preliminaries and a formal denition of our problem. Section 3 pro-
poses M/e.sc/t.sc/a.sc/hyphen.scP2 and analyzes its time complexity. Section 4 shows
experimental results, and Section 5 introduces related works. We
conclude the paper in Section 6. The code and datasets are available
athttps://github.com/snudatalab/Meta-P2.Table 1: Symbols.
Symb
ol Description
fX𝑘g𝐾
𝑘=1irr
egular tensor¹2R𝐼𝑘𝐽𝐾for𝑘=1𝐾º
X𝑘𝑘-th
slice offX𝑘g𝐾
𝑘=1¹2R𝐼𝑘𝐽º
U𝑘S𝑘Q𝑘 factor
matrices of the 𝑘-th slice X𝑘
HV factor
matrices offX𝑘g𝐾
𝑘=1
fX𝑑;𝑘g𝐾
𝑘=1irr
egular tensor of domain 𝑑
X𝑑;𝑘𝑘-th
slice offX𝑑;𝑘g𝐾
𝑘=1¹2R𝐼𝑘𝐽º
U𝑑;𝑘S𝑑;𝑘Q𝑑;𝑘factor
matrices of the 𝑘-th slice X𝑑;𝑘
H𝑑V𝑑 factor
matrices offX𝑑;𝑘g𝐾
𝑘=1
X¹𝑛º mo
de-𝑛matricization of X2R𝐼1𝐼2𝐼𝑁
 Khatri-Rao
product
 Hadamar
d product
𝑅 target
rank of decomposition
2 Preliminaries and Problem
We describe tensor notations and operations, PARAFAC2, and meta-
learning. We then formally dene the problem.
2.1 Tensor Notations and Operations
We provide the symbols used in this paper in Table 1.
Irregular Tensor. We denote an irregular tensor as a set fX𝑘g𝐾
𝑘=1
where X𝑘2R𝐼𝑘𝐽is the𝑘-th slice of the tensor, and 𝐾is the num-
ber of slices. Note that the row sizes 𝐼𝑘of the slices are dierent
for all𝑘. An irregular tensor in domain 𝑑is denoted asfX𝑑;𝑘g𝐾
𝑘=1.
We denotefX𝑑;𝑘g𝐾
𝑘=12R𝐼𝑘𝐽𝐾asfX𝑑gin this paper for brevity.
Mode-𝑛Matricization. Given a tensor X2R𝐼1𝐼𝑁, we de-
note mode-𝑛matricization of XasX¹𝑛º. The resulting unfolded
matrix is of size¹𝐼𝑛Î𝑁
𝑚=1𝑚≠𝑛𝐼𝑚º. The operation arranges the
mode-𝑛bers to be the columns of the resulting matrix.
Khatri-Rao Product. Khatri-Rao product of two matrices A2
R𝑝𝑟andB2R𝑞𝑟, which is denoted by AB2R𝑝𝑞𝑟, is com-
puted as AB=»A¹:1º
B¹:1ºjj A¹:𝑟º
B¹:𝑟º¼where
is Kronecker product and jis the horizontal concatenation. The
Kronecker product A¹:𝑘º
B¹:𝑘ºof𝑘-th column is computed as
»A¹1𝑘ºB¹:𝑘º>jjA¹𝑝𝑘ºB¹:𝑘º>¼>.
Hadamard Product. Given two matrices AandBof the same
size, the Hadamard product ABis a matrix of the same size as
the operands, with elements given by ¹ABº¹𝑖𝑗º=A¹𝑖𝑗ºB¹𝑖𝑗º.
2.2 PARAFAC2 Decomposition
PARAFAC2 decomposition analyzes an irregular tensor by approxi-
mating it with a multiplication of latent factor matrices. The objec-
tive of PARAFAC2 is to minimize the following objective function:
min
fU𝑘gfS𝑘gV∑︁𝐾
𝑘=1kX𝑘 U𝑘S𝑘V>k2
𝐹(1)
where X𝑘2R𝐼𝑘𝐽is a𝑘-th slice of the original irregular tensor
fX𝑘g𝐾
𝑘=12R𝐼𝑘𝐽𝐾, and U𝑘2R𝐼𝑘𝑅,S𝑘2R𝑅𝑅, and V2R𝐽𝑅
are the decomposed factor matrices of X𝑘with rank𝑅. Note that V
is shared along the slices.
To ensure the uniqueness of the results, Harshman [ 14] replaced
U𝑘withQ𝑘Hwhere Q𝑘2R𝐼𝑘𝑅is a column orthogonal matrix,
 
1384Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD ’24, August 25–29, 2024, Barcelona, Spain.
andH2R𝑅𝑅is a common matrix for all the slices. Then the
objective function of PARAFAC2 is reformulated as
min
fQ𝑘gfS𝑘gHV∑︁𝐾
𝑘=1kX𝑘 Q𝑘HS𝑘V>k2
𝐹 (2)
ALS (Alternating Least Square) is commonly used for optimizing
Equation (2)due to its fast speed and accuracy [ 46]. CP-ALS, which
uses ALS for PARAFAC2 decomposition, iteratively updates each
factor matrix while xing the other ones. A single iteration of
CP-ALS updates factor matrices by running Equations (3)-(7)once.
CP-ALS rst nds Q𝑘while xing HS𝑘, andVfor all𝑘:
A𝑘ΣB>
𝑘 X𝑘VS𝑘H>(3)
Q𝑘 A𝑘B>
𝑘(4)
where Equation (3)is the truncated SVD with rank 𝑅. Then, CP-ALS
updates HfS𝑘g, andVas follows:
W Y¹3º¹VHº¹V>VH>Hºy(5)
H Y¹1º¹WVº¹W>WV>Vºy(6)
V Y¹2º¹WHº¹W>WH>Hºy(7)
where the𝑘-th row of W𝑠2R𝐾𝑅is the diagonal entries of S𝑘,Y2
R𝑅𝐽𝐾is a constructed tensor from slices Y𝑘for all𝑘, andY𝑘2
R𝑅𝐽isQ>
𝑘X𝑘. For each𝑘,S𝑘andU𝑘are updated as 𝑑𝑖𝑎𝑔¹W¹𝑘:ºº
andQ𝑘H, respectively. CP-ALS repeats Equations (3)-(7)until the
results converge.
2.3 Model-Agnostic Meta-Learning (MAML)
Meta-learning is a powerful framework to address the problem
of limited data in many machine learning areas. Meta-learning
trains a general model with multiple given tasks so that it adapts
to a new problem quickly, instead of training task-specic models.
Previous meta-learning methods train the general model using
gradient information from the given tasks [ 16,39]. Among them,
MAML [ 10] has been widely used due to its robustness and accuracy.
MAML trains a model on multiple given tasks T𝑑for𝑑=1𝐷 ,
with the goal of nding a set of initial parameters 𝜃meta that can
be ne-tuned to perform well on a new target task quickly and
accurately. Note that the objective of 𝜃meta is not to perform well
on the given tasks, but to quickly adapt to a new unseen target task.
For every given task T𝑑, MAML adapts the meta model parame-
ters𝜃meta to eachT𝑑by performing a single gradient descent update
with the objective function LT𝑑:
𝜃0
𝑑 𝜃meta 𝛼r𝜃metaLT𝑑¹𝑓𝜃metaº (8)
where𝜃0
𝑑denotes the adapted 𝜃meta to eachT𝑑. This simulates
the adaptation process of 𝜃meta to a new target task with a few
gradient-descent updates.
MAML then updates 𝜃meta following the mean of gradients
r𝜃LT𝑑¹𝑓𝜃0
𝑑ºfor𝑑=1𝐷 as follows:
𝜃meta 𝜃meta 𝛽r𝜃meta∑︁𝐷
𝑑=1LT𝑑¹𝑓𝜃0
𝑑º (9)
Note that the meta-gradient is a gradient in terms of 𝜃meta whereas
the loss function is parameterized by 𝜃0
𝑑. This enables 𝜃meta to con-
tain easily transplantable information from all tasks T𝑑. We denote
r𝜃metaÍ𝐷
𝑑=1LT𝑑¹𝑓𝜃0
𝑑º, which is a specially designed gradient formeta-learning, as meta-gradient in the rest of this paper. We repeat
the process until the maximum iteration is reached.
2.4 Problem Denition
We formally dene the problem in Problem 1.
P/r.sc/o.sc/b.sc/l.sc/e.sc/m.sc 1 (D/o.sc/m.sc/a.sc/i.sc/n.sc /a.sc/d.sc/a.sc/p.sc/t.sc/a.sc/t.sc/i.sc/o.sc/n.sc /f.sc/o.sc/r.sc /i.sc/r.sc/r.sc/e.sc/g.sc/u.sc/l.sc/a.sc/r.sc /t.sc/e.sc/n.sc/s.sc/o.sc/r.sc /d.sc/e.sc/hyphen.sc
/c.sc/o.sc/m.sc/p.sc/o.sc/s.sc/i.sc/t.sc/i.sc/o.sc/n.sc). We are given a set Sof source domains. For each
domain𝑠2S, there is an irregular tensor fX𝑠g. Then, we have a
new irregular tensor fX𝑇gfrom an unseen target domain 𝑇. The
problem aims to quickly nd the latent factor matrices of fX𝑇gthat
reconstructfX𝑇gaccurately exploiting the tensors of source domains.
The objective function Lof the problem is as follows:
min
𝜃𝑇L¹fX𝑇g;𝜃𝑇º=∑︁𝐾
𝑘=1kX𝑇;𝑘 U𝑇;𝑘S𝑇;𝑘V>
𝑇k2
𝐹(10)
where X𝑇;𝑘2R𝐼𝑘𝐽is a𝑘-th slice offX𝑇g2R𝐼𝑘𝐽𝐾, and𝜃𝑇=
ffU𝑇gfS𝑇gV𝑇g. Note that the knowledge of source domains is
utilized for optimizing the objective function in Equation (10).
3 Proposed Method
We propose M/e.sc/t.sc/a.sc/hyphen.scP2, our fast and accurate domain adaptation
method for irregular tensors. The main challenges and our ap-
proaches are as follows:
(1)How can we utilize given source tensors for decom-
posing a new tensor in a target domain quickly and
accurately? We learn a general meta factor matrix from
given source domains, which is easily transferred to the new
target domain (Section 3.1).
(2)How can we nd the meta factor matrix from source do-
mains which is easily transferred to the target domain?
We perform meta-learning-based PARAFAC2 decomposition
on source domains. This is achievable by carefully modeling
the gradients of reconstruction error in terms of the meta
factor matrix (Section 3.2).
(3)How can we utilize the trained meta factor when de-
composing a new tensor of a target domain? We adapt
the meta factor to the target domain while preventing the
decomposed results of the target domain from completely
forgetting the information of source domains (Section 3.3).
3.1 Meta Factor Matrix
The objective of M/e.sc/t.sc/a.sc/hyphen.scP2 is to quickly nd accurate latent factor
matrices of a new irregular tensor fX𝑇gof the target domain 𝑇,
using the given irregular tensors fX𝑠gin multiple source domains
𝑠2S(see Section 2.4 for a formal problem denition and its ob-
jective function). Specically, we aim to accurately decompose the
target tensorfX𝑇gwith limited information in a few ALS iteration
updates. To do this, it is important to dene the information from
source domains that helps decompose the target tensor. If the in-
formation is slice-specic or domain-specic, it cannot directly be
used for analyzing the new unseen target tensor.
For each tensorfX𝑠gin a source domain 𝑠, the decomposed
results are represented by the multiplication of three components:
fU𝑠;𝑘g,fS𝑠;𝑘g, andV𝑠. Each𝑘-th slice X𝑠;𝑘offX𝑠gis decomposed
intoU𝑠;𝑘S𝑠;𝑘V𝑠. Note that V𝑠contains representative information
 
1385KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
Sourceadaptation
Meta updateAt first, initialize         as           Residual connectionSource domain 1Source domain 2Source domain S
Multiple source domainsNew target domain T
Figure 2: Overall process of M/e.sc/t.sc/a.sc/hyphen.scP2. We train the easily
adaptable meta factor Vmetafrom multiple source tensors
fX𝑠gfor𝑠2f1𝑆g. This is done by iteratively performing
the source adaptation and meta-update steps. We utilize the
trained Vmetafor decomposing a new target tensor fX𝑇g.
of domain𝑠since V𝑠is used for constructing every slice of fX𝑠g,
whilefU𝑠;𝑘gandfS𝑠;𝑘gare specic to each slice.
We propose to share V𝑠along the source domains, which we
denote as Vmeta, and utilize it for analyzing the target domain. The
intuition is to learn general information from the source domains by
forcing Vmeta to reconstruct every source tensor well. A challenge
is that naively sharing Vmeta only aims to reconstruct the source
tensors, and fails to achieve accurate results on the target domain
when the target has unseen properties in source domains.
To address this, M/e.sc/t.sc/a.sc/hyphen.scP2 enforces Vmeta to possess an additional
property: easily adaptable. We learn Vmeta to quickly and accurately
adapt to any new domain, not only to the given source domains.
This is done by training Vmeta based on the meta-learning scheme.
As a result, M/e.sc/t.sc/a.sc/hyphen.scP2 learns general andeasily adaptable meta factor
matrix Vmeta from the source tensors.
The overall process of M/e.sc/t.sc/a.sc/hyphen.scP2 is illustrated in Figure 2. We
rst compute the meta factor matrix Vmeta from multiple source
domains. Then we leverage the trained Vmeta for decomposing
a new target tensor fX𝑇g. This enables us to eectively utilize
useful information of source domains for analyzing the target tensor
without decomposing all the domains together.
3.2 Decomposition on Source Domains
M/e.sc/t.sc/a.sc/hyphen.scP2 learns the general andeasily adaptable meta factor Vmeta
from the multiple source tensors based on the meta-learning scheme.
The meta-learning aims to train a general model that adapts to a
new task quickly and accurately utilizing multiple source tasks.
The meta-learning based decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 in the source
domains consists of two steps: a) source adaptation and b) meta-
update steps. The source adaptation step adapts Vmeta to every
source domain. Using the adapted results, the meta-update step
nds Vmeta that is prepared for adaptation to any new domain,
rather than accurate reconstruction of the source tensors. We repeat
those two steps until the maximum iteration is reached. Algorithm 1
shows the overall process of decomposition in source domains.
Using the objective function LofM/e.sc/t.sc/a.sc/hyphen.scP2 in Equation (10), the
source adaptation of Vmeta for each source domain 𝑠formulated as:
V𝑠=Vmeta 𝜂rVmeta∑︁𝐾
𝑘=1kX𝑠;𝑘 U𝑠;𝑘S𝑠;𝑘V>
metak2
𝐹(11)Algorithm 1: Computing the meta factor matrix from mul-
tiple source domains.
input : Irregular tensorsfX𝑠;𝑘g𝐾
𝑘=12R𝐼𝑘𝐽𝐾from
all source domains 𝑠=1𝑆
output : Meta factor matrix Vmeta2R𝐽𝑅
parameters: Target rank 𝑅
1Randomly initialize W𝑠2R𝑅𝑅,H𝑠2R𝑅𝑅for all
𝑠=1𝑆 , andVmeta;
2repeat
/*Source adaptation step */
3 fors=1,, Sdo
4 Adapt Vmeta to domain𝑠by a single ALS iteration,
and save the result as V𝑠; // Equations (15), (16)
5 end
/*Meta-update step */
6 Update Vmeta along the meta-gradient, which is
computed by aggregating all V0𝑠; // Equation (21)
7until the maximum iteration is reached ;
where𝜂is a learning rate. To remove the constant term U𝑠;𝑘S𝑠;𝑘in
front of Vmeta, we rewriteL¹fX𝑠g;Vmetaºas follows:
L𝑉¹fX𝑠g;Vmetaº=∑︁𝐾
𝑘=1kE𝑠;𝑘X𝑠;𝑘 V>
metak2
𝐹 (12)
where E𝑠;𝑘=𝐾¹Í𝐾
𝑗=1S>
𝑠;𝑗U>
𝑠;𝑗U𝑠;𝑗S𝑠;𝑗º 1S>
𝑠;𝑘U>
𝑠;𝑘. Note that mini-
mizingL𝑉in Equation (12)is equivalent to minimizing Lin Equa-
tion(10)for training Vmeta (see Appendix A.1 for proof). Then the
source adaptation step of M/e.sc/t.sc/a.sc/hyphen.scP2 is expressed as follows:
V𝑠=Vmeta 𝜂rVmetaL𝑉¹fX𝑠g;Vmetaº (13)
The meta-update step of Vmeta is as follows:
Vmeta Vmeta 𝜂rVmeta∑︁
𝑠2SL𝑉¹fX𝑠g;V𝑠º (14)
However, CP-ALS algorithm is widely used for learning the latent
factors of irregular tensors [ 1,19,31] compared to the gradient-
descent one. This is mainly because the ALS-based decomposition
gives accurate factors in a few iterations and guarantees the unique-
ness in the decomposed results [ 14]. In the following Sections 3.2.1
and 3.2.2, we describe how we design the gradient-based meta-
learning by the ALS-based meta-learning for M/e.sc/t.sc/a.sc/hyphen.scP2.
3.2.1 Source adaptation step. In the source adaptation step, we
adapt Vmeta to each source domain by performing a single ALS iter-
ation instead of the single gradient-based update in Equation (13).
The idea stems from the observation that both gradient-based and
ALS-based training nds the optimal parameter by gradually mini-
mizing the objective function. This idea also aligns with the typical
meta-learning scenario which constrains the numbers of gradient-
descent updates to adapt the meta model parameters to each given
task. To perform the ALS-based adaptation of Vmeta, we rst update
W𝑠andH𝑠using Equations (5) and (6), respectively:
W𝑠=¹Y𝑠º¹3º¹VmetaH𝑠º¹V>
metaVmetaH>
𝑠H𝑠ºy
H𝑠=¹Y𝑠º¹1º¹W𝑠Vmetaº¹W>
𝑠W𝑠V>
metaVmetaºy(15)
 
1386Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD ’24, August 25–29, 2024, Barcelona, Spain.
where Y𝑠2R𝑅𝐽𝐾is constructed from slices Y𝑠;𝑘=Q>
𝑠;𝑘X𝑠;𝑘for
all𝑘=1𝐾 , and Q𝑠;𝑘2R𝐼𝑘𝑅is computed using Equation (4).
The𝑘-th row of W𝑠2R𝐾𝑅corresponds to the diagonal entries of
S𝑘. Then we adapt Vmeta to each domain 𝑠using Equation (7):
V𝑠=¹Y𝑠º¹2º¹W𝑠H𝑠º¹W𝑠>W𝑠H𝑠>H𝑠ºy (16)
Note that we express the gradient-based source adaptation step in
Equation (13) by an ALS-based adaptation in Equation (16).
3.2.2 Meta-update step. In the meta-update step, we update the
meta factor matrix Vmeta to perform well after adapting to any new
target domain, rather than generally performing well across all
source domains. Our approach for designing the gradient-based
meta-update in Equation (14)by an ALS-based meta-update for
M/e.sc/t.sc/a.sc/hyphen.scP2 is to express the meta-gradientrVmetaÍ
𝑠2SL𝑉¹fX𝑠g;V𝑠º
using the factors computed by ALS iterations. However, directly
dierentiatingL𝑉¹fX𝑠g;V𝑠gin terms of Vmeta is challenging since
V𝑠is not parameterized by Vmeta.
To address this, we break down each rVmetaL𝑉¹fX𝑠g;V𝑠ºof
the meta-gradient into two gradients rVmetaL𝑉¹fX𝑠g;Vmetaºand
rV𝑠L𝑉¹fX𝑠g;V𝑠º. In the following, we rst describe how we ex-
press the two gradients using only the factors computed by ALS
iterations, and then we show the relationship between the meta-
gradient and the two gradients.
Two Gradients. LetV0𝑠denote V𝑠adapted to domain 𝑠again:
V0
𝑠=V𝑠 𝜂rV𝑠L𝑉¹fX𝑠g;V𝑠º (17)
=¹Y𝑠º¹2º¹W0
𝑠H0
𝑠º¹W0
𝑠>W0
𝑠H0
𝑠>H0
𝑠ºy(18)
where W0𝑠andH0𝑠denote W𝑠andH𝑠after performing an additional
ALS updates in Equation (15)while replacing Vmeta withV𝑠. The
second equality uses our approximation in the source adaptation
step that a single gradient-descent update is equal to a single ALS
iteration update (see Equations (13)and(16)). Then the two gra-
dientsrVmetaL𝑉¹fX𝑠g;VmetaºandrV𝑠L𝑉¹fX𝑠g;V𝑠ºare simply
expressed by¹Vmeta V𝑠º𝜂and¹V𝑠 V0𝑠º𝜂using Equations (13)
and(17), respectively. Note that V𝑠andV0𝑠are computed by the
ALS iterations in Equations (16) and (18).
Relationship between Meta-gradient and the Two Gra-
dients. Lemma 3.1 shows the relationship between each term
rVmetaL𝑉¹fX𝑠g;V𝑠ºof the meta-gradient and the two gradients
rVmetaL𝑉¹fX𝑠g;VmetaºandrV𝑠L𝑉¹fX𝑠g;V𝑠º.
L/e.sc/m.sc/m.sc/a.sc 3.1. Each termrVmetaL𝑉fX𝑠g;V𝑠ºof the meta-gradient is
proportional to the sum of the two gradients rVmetaL𝑉¹fX𝑠g;Vmetaº
andrV𝑠L𝑉¹fX𝑠g;V𝑠º:
rVmetaL𝑉¹fX𝑠g;V𝑠º=𝐶¹rVmetaL𝑉¹fX𝑠g;Vmetaº¸
rV𝑠L𝑉¹fX𝑠g;V𝑠ºº (19)
where𝐶=¹1 2𝜂𝐾º2¹2¹1 𝜂𝐾ººis a constant. 𝜂is the learning rate
and𝐾is the number of slices in each source domain (see Appendix A.2
for proof).
Using Lemma 3.1, we derive Theorem 3.2 which expresses the
meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in terms of V0𝑠that is computed by
the ALS iteration in Equation (18).Algorithm 2: Tensor decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on a new
target tensor exploiting the meta factor matrix.
input : Irregular tensor X𝑇2R𝐼𝑘𝐽𝐾, and a trained
meta factor Vmeta2R𝐽𝑅
output : Decomposed matrices U𝑇;𝑘2R𝐼𝑘𝑅and
S𝑇;𝑘2R𝑅𝑅for all𝑘, andV𝑇
parameters: Target rank 𝑅
1Randomly initialize H𝑇2R𝑅𝑅, andS𝑇;𝑘for all𝑘;
2Initialize V𝑇withVmeta;
3repeat
4 UpdatefQ𝑇gH𝑇W𝑇V𝑇using Equations (3)-(7) ;
5 V𝑇 ¹V𝑇¸Vmetaº2; // residual connection
6until the maximum iteration is reached ;
7fork=1,, Kdo
8 S𝑇;𝑘 diag(W𝑇¹𝑘:º);
9 U𝑇;𝑘 Q𝑇;𝑘H𝑇;
10end
T/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc 3.2. The meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in Equation (14)
is expressed in terms of V0𝑠that is computed using the ALS iterations:
Vmeta Vmeta¸𝐶∑︁
𝑠2S¹V0
𝑠 Vmetaº (20)
𝐶=¹1 2𝜂𝐾º2¹2¹1 𝜂𝐾ººis a constant with learning rate 𝜂and
the number𝐾of slices in each domain (see Appendix A.3 for proof).
We choose𝜂that satises 𝐶=1jSj. As a result, the meta-update
step of Vmeta is simply expressed by the mean of V0𝑠for all source
domains𝑠2Sas follows:
Vmeta Vmeta¸1
jSj∑︁
𝑠2S¹V0
𝑠 Vmetaº=1
jSj∑︁
𝑠2SV0
𝑠 (21)
Note that we express the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in Equa-
tion (14)using only V0𝑠, which is computed by performing ALS
iterations. We learn Vmeta from the source tensors by repeatedly
performing the source adaptation step in Equation (16)and the
meta-update step in Equation (21).
3.3 Inference on Target Domain
M/e.sc/t.sc/a.sc/hyphen.scP2 analyzes a new tensor X𝑇of the target domain 𝑇using the
patterns of source domains. The patterns are represented as a meta
factor Vmeta. The main idea is to initialize the latent factor matrix
V𝑇of target domain 𝑇withVmeta, and nd the adapted factor
matrices to the target domain by running a few ALS iterations. The
decomposed results are utilized for performing many downstream
tasks on the target domain including missing value prediction,
anomaly detection, feature extraction, etc.
For performing a downstream task, overtting the latent factors
to the target domain degrades the performance when there are only
a few entries in the target tensor. To prevent forgetting the trained
patterns Vmeta of source domains while decomposing the target
tensor, we exploit a residual connection between Vmeta andV𝑇.
When decomposing a new target tensor, abnormal values such
as anomalies and missing entries deteriorate the quality of the de-
composed results. Since a single ALS step of M/e.sc/t.sc/a.sc/hyphen.scP2 reconstructs
 
1387KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
the target tensor accurately using the general information Vmeta of
source domains, M/e.sc/t.sc/a.sc/hyphen.scP2 imputes the abnormal values with the
corresponding reconstructed values after a single ALS step. For
example, when there are anomalies in the target tensor, M/e.sc/t.sc/a.sc/hyphen.scP2
replaces the detected anomalous entries with the reconstructed
values after a single ALS step on the target tensor.
3.4 Theoretical Analysis
We theoretically analyze the time complexity of M/e.sc/t.sc/a.sc/hyphen.scP2. Specif-
ically, we show that the meta-learning-based decomposition of
M/e.sc/t.sc/a.sc/hyphen.scP2 has the same complexity to that of a naive PARAFAC2.
L/e.sc/m.sc/m.sc/a.sc 3.3. The decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains (Al-
gorithm 1) has the same time complexity as that of a naive PARAFAC2
decomposition on the source domains (see Appendix A.4 for proof).
L/e.sc/m.sc/m.sc/a.sc 3.4. Given the meta factor matrix Vmeta, the time complex-
ity of M/e.sc/t.sc/a.sc/hyphen.scP2 for nding latent factors of a target tensor (Algorithm 2)
is the same as that of performing PARAFAC2 decomposition on the
target tensor (see Appendix A.5 for proof).
4 Experiments
We perform experiments to answer the following questions.
Q1.Missing Value Prediction. How accurately does M/e.sc/t.sc/a.sc/hyphen.scP2
predict missing values of a new target tensor using existing
tensors in multiple source domains (Section 4.2)?
Q2.Anomaly Detection. Does M/e.sc/t.sc/a.sc/hyphen.scP2 show superior perfor-
mance than the baselines for detecting anomalies of a new
target tensor exploiting the source tensors (Section 4.3)?
Q3.Domain Adaptation Speed. How fast does M/e.sc/t.sc/a.sc/hyphen.scP2 adapt
to the new target domain (Section 4.4)?
Q4.Ablation Study. Does each step of M/e.sc/t.sc/a.sc/hyphen.scP2 contribute to
the performance of the downstream task (Section 4.5)?
Q5.Case Study. What are the discoveries of analyzing real world
data using M/e.sc/t.sc/a.sc/hyphen.scP2 (Section 4.6)?
4.1 Experimental Settings
We present our experimental setup including datasets and baselines.
All experiments are done with a single GTX 3080.
Datasets. We use ve real-world datasets from two distinct
elds: stock and human action. We summarize the statistics of
datasets in Table 2.
Nasdaq, S&P500, and Korea stock are stock datasets which con-
tain selected features of daily stock prices in Nasdaq, S&P 500, and
Korea stock market, respectively. We use 1) the opening, closing,
highest, and lowest prices, and 2) trading volume of the day as
features. Stocks are grouped into sectors, and we treat their sectors
as domains. This is because the statistics of stocks, such as price
or trading volume, are similar within each sector, but dier among
sectors. In each sector, the data are in the form of (day, feature,
stock) ; a stock corresponds to a slice matrix of size (day, feature).
NATOPS and Cricket are human activity datasets measured with
various sensors. NATOPS is a collection of 20 people repeatedly
performing 24 aircraft handling actions. We treat the people as
domains since each person has dierent physical conditions and
unique habits. For each person, the actions are in the form of (time-
frame, sensor, action) ; an action corresponds to a slice matrix whoseTable 2: Summary of datasets.
Name #
Domain Max 𝐼𝑘¹𝐽𝐾º# Non-zero Type
Nasdaq111
12,709 (6, 11) 2,742K stock
S&P500111
13,321 (6, 13) 7,318K stock
Korea stock211
3,089 (6, 10) 2,038K stock
NATOPS320
2,009 (77, 24) 42,720K HAR
Cricket412
1,197 (6, 10) 1,292K HAR
1https://kaggle.com/datasets/paultimothymooney/stock-market-data
2https://github.com/jungijang/KoreaStockData
3https://github.com/yalesong/natops
4https://timeseriesclassication.com
form is (timeframe, sensor). Cricket is a collection of four umpires
performing twelve signals in the cricket game, each with ten rep-
etitions. We consider each signal type, such as No Ball or Dead
Ball, as a domain for the Cricket dataset. This is because the sensor
values are dierent depending on the type of signal due to the
dierences in the type of movements involved. For each type of
signal, the signals are in the form of (timeframe, sensor, person) ; a
person corresponds to a slice matrix, and its form is (timeframe,
sensor).
If the number of slices in a specic domain is extremely large, the
decomposed results that do not consider the diversity of domains
are biased towards the domain. This is unfair to the competitors
including PARAFAC2 and PARAFAC2-I. Thus, we randomly sample
𝐾slices for each domain where 𝐾is the number of slices of the
domain with the least slices. In stock datasets, scales vary largely
across the columns; e.g., scales of the price and trading volume are
dierent. Thus, we perform L2 normalization for each 𝑗-th column
X¹:𝑗ºof a slice matrix X:X¹𝑖𝑗º=X¹𝑖𝑗ºkX¹:𝑗ºk2.
Baselines. M/e.sc/t.sc/a.sc/hyphen.scP2 aims to decompose the target tensor quickly
and accurately utilizing the information of given source tensors.
Thus, we compare M/e.sc/t.sc/a.sc/hyphen.scP2 with previous approaches for irregular
tensor decomposition. We do not compare M/e.sc/t.sc/a.sc/hyphen.scP2 with COPA [ 1]
and SPARTan [ 31] that aim to improve the scalability of the original
PARAFAC2 since our objective is to nd accurate latent factors of
the target tensor with limited number of slices.
PARAFAC2 [ 14] performs PARAFAC2 decomposition on the
whole domains including source and target. The method optimizes
the latent factor matrices based on ALS approach. PARAFAC2-T
performs PARAFAC2 decomposition on the target domain only.
PARAFAC2-I initializes the factor matrix Vof the target domain
with the result of a naive PARAFAC2 decomposition on source
domains. AO-ADMM [ 33] is a recent state-of-the-art decomposition
method for irregular tensors. AO-ADMM employs an alternating
optimization (AO) approach in conjunction with the alternating
direction method of multipliers (ADMM) for tting PARAFAC2.
Evaluation and Seings. To evaluate the quality of decom-
posed results of M/e.sc/t.sc/a.sc/hyphen.scP2, we perform two downstream tasks: miss-
ing value prediction and anomaly detection. For each dataset, we set
the last domain (in alphabetical order) as the unseen target domain,
and the rest as source domains. For example, in stock datasets, we
align sectors alphabetically and use the last sector, Utilities in our
case, as the target domain. We randomly split the data of the target
domain into training and test missing (anomalous) entries with the
missing (anomaly) ratio. In the missing value prediction task, we
 
1388Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 3: The missing value prediction error of M/e.sc/t.sc/a.sc/hyphen.scP2 and baselines. The lowest errors are in bold. M/e.sc/t.sc/a.sc/hyphen.scP2 shows the best
prediction performance in all cases.
Missing
Value Prediction
Mo
delsMissing
ratio𝑟𝑚= 0.1 Missing
ratio𝑟𝑚= 0.3
Nasdaq
S&P500 Korea stock NATOPS Cricket Nasdaq
S&P500 Korea stock NATOPS Cricket
P
AFARAC2 0.31490.0412
0.35510.0360 0.40850.0095 0.38810.0157 1.34690.1304 0.49980.0593
0.49480.0608 0.48940.0207 0.61720.0081 1.13920.0602
PARAFAC2-I 0.99050.0033
1.00750.0035 0.99330.0024 0.81190.0041 1.02030.0056 1.00540.0014
1.00350.0006 1.00870.0015 0.84980.0019 1.02120.0050
PARAFAC2-T 0.99020.0025
1.00940.0012 0.99740.0015 0.82430.0026 1.02350.0041 1.00970.0020
1.00410.0004 1.00860.0022 0.85580.0013 1.02180.0032
AO-ADMM 0.52630.0038
0.37610.0019 0.46450.0134 0.90810.0268 0.76300.0649 0.59440.0033
0.42460.0052 0.61290.0097 1.32700.0235 1.07670.0071
M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed) 0.28840.0287
0.32370.0278 0.38490.0260 0.29070.0042 0.90240.0627 0.38960.0279
0.38520.0295 0.46660.0228 0.38900.0044 0.90780.0335
Table 4: Anomaly detection performance of M/e.sc/t.sc/a.sc/hyphen.scP2 and baselines in terms of F1 score. Bold numbers denote the best
performance. M/e.sc/t.sc/a.sc/hyphen.scP2 shows the best detection performance among the methods in most of the cases.
Anomaly
Detection
Mo
delsAnomaly
ratio𝑟𝑎= 0.05 Anomaly
ratio𝑟𝑎= 0.1
Nasdaq
S&P500 Korea stock NATOPS Cricket Nasdaq
S&P500 Korea stock NATOPS Cricket
P
AFARAC2 0.00690.0002
0.00410.0002 0.00120.0005 0.02970.0001 0.40940.0203 0.01450.0004
0.00660.0004 0.00130.0006 0.06480.0006 0.47420.0137
PARAFAC2-I 0.24000.0199
0.24820.0719 0.45940.0530 0.23650.0036 0.08670.0150 0.21670.0222
0.19850.0439 0.38300.0532 0.25100.0019 0.12990.0112
PARAFAC2-T 0.23010.0397
0.19200.0348 0.40170.0613 0.22560.0033 0.07580.0096 0.20960.0233
0.16910.0177 0.32550.0462 0.23540.0031 0.12800.0104
AO-ADMM 0.12930.0061
0.35910.0162 0.51000.0118 0.10890.0052 0.01230.0021 0.19720.0066
0.40490.0060 0.54000.0122 0.11610.0032 0.03240.0024
M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed) 0.35080.0097
0.46560.0121 0.59590.0165 0.29600.0017 0.07520.0094 0.39120.0092
0.49930.0107 0.62820.0133 0.32980.0009 0.12920.0085
use the normalized error as the evaluation metric; given an original
tensor Xand the reconstructed one ˆX, the normalized error 𝑟erris:
𝑟err=Í𝐾
𝑘=1Í
¹𝑖𝑗º2Ω𝑘kX𝑘¹𝑖𝑗º ˆX𝑘¹𝑖𝑗ºk2
𝐹Í𝐾
𝑘=1Í
¹𝑖𝑗º2Ω𝑘kX𝑘¹𝑖𝑗ºk2
𝐹(22)
where X𝑘is the𝑘-th frontal slice, ˆX𝑘is the reconstructed one
ofX𝑘with target rank 𝑅, and Ω𝑘is the set of test entries of the
𝑘-th slice matrix. In the anomaly detection task, we use the F1
score as the evaluation metric. For each dataset, we set the target
rank𝑅as the minimum mode length of the target tensor, and the
maximum number of iterations as 10. We run all experiments ten
times with random seeds, and report the average and standard
deviation removing the highest and the lowest results.
4.2 Missing Value Prediction (Q1)
We present the missing value prediction errors of M/e.sc/t.sc/a.sc/hyphen.scP2 and
baselines in Table 3. M/e.sc/t.sc/a.sc/hyphen.scP2 consistently outperforms competi-
tors in various scenarios, achieving the lowest reconstruction error
across dierent missing ratios 𝑟𝑚2f0103gThis indicates that
it is important to carefully use the information of source domains
when decomposing the target domain. Furthermore, PARAFAC2-T
which focuses solely on the new target tensor presents lower accu-
racy than other methods including PARAFAC2 and PARAFAC2-I.
This is because solely reconstructing the target tensor with lim-
ited information incurs overtting problem, showing inaccurate
prediction for the missing entries.
4.3 Anomaly Detection (Q2)
To evaluate the anomaly detection performance of M/e.sc/t.sc/a.sc/hyphen.scP2, we
randomly select anomalous entries in the target tensor X𝑇with
the anomaly ratio 𝑟𝑎2f00501g. Then we change the values of
the entries by dividing them with a constant 𝜓=2, to make them
anomalies. We detect 𝐾anomalies by nding top- 𝐾entries with
the highest error between the anomaly-injected tensor and our
reconstructed one.Table 4 presents that M/e.sc/t.sc/a.sc/hyphen.scP2 gives the highest F1 score with
large margin in most of the cases. This indicates that M/e.sc/t.sc/a.sc/hyphen.scP2 suc-
cessfully discovers the factor matrices that contribute to accurately
detecting anomalies in a new target tensor.
In Cricket, PARAFAC2 shows better performance than M/e.sc/t.sc/a.sc/hyphen.sc
P2. The main reason is that Cricket has extremely small number
of entries in the target domain, with excessively small number of
anomalies. Thus, PARAFAC2 easily neglects the anomalous entries
while reconstructing the tensor of entire domains, showing higher
reconstruction performance on those anomaly values. However,
PARAFAC2 completely fails to detect anomalies in other datasets,
and does not perform well for missing value prediction task with
markedly slower speed (see Figure 4).
4.4 Domain Adaptation Speed (Q3)
M/e.sc/t.sc/a.sc/hyphen.scP2 initializes the latent factor matrix V𝑇of the target do-
main𝑇with the meta factor matrix Vmeta. To show that Vmeta
adapts to the target domain quickly and eectively, we compare
the missing value prediction performance of M/e.sc/t.sc/a.sc/hyphen.scP2 with dier-
ently initialized V𝑇. PARAFAC2-initialized method initializes V𝑇
with the decomposed result of naive PARAFAC2 decomposition
on the source domains. Random-initialized method initializes V𝑇
randomly. We adopt the imputation process of missing entries and
the residual connection of M/e.sc/t.sc/a.sc/hyphen.scP2 to those baselines. We report
the performance of the methods over the ALS iterations in Figure 3.
We also show the runtime of them per iteration in Figure 4.
Figure 3 shows that M/e.sc/t.sc/a.sc/hyphen.scP2 eectively reduces the prediction
error with only a few ALS iterations where the runtime per ALS
iteration is the same for all initializing methods (see Figure 4).
The prediction errors of other methods are generally monotonous
and poor compared to M/e.sc/t.sc/a.sc/hyphen.scP2. This shows that initializing V𝑇
withVmeta assists in discovering accurate latent factor matrices of
the target tensor quickly. Moreover, PARAFAC2-initialized consis-
tently shows poor performance. This implies that simply tuning
the decomposed results to a new target domain cannot consider
the dierence between source and target domains.
 
1389KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
Rapid errordropRapid errordroplarge error dropSmall dropError sustainsSlow dropError increases
Figure 3: The missing value prediction performance over the ALS iterations. We compare M/e.sc/t.sc/a.sc/hyphen.scP2 with two other initialization
approaches of V𝑇for a target domain 𝑇. Note that the error of M/e.sc/t.sc/a.sc/hyphen.scP2 drops rapidly, achieving the best prediction performance
among the initialization methods. The errors of other baselines drop slowly, or even increase in Cricket.
2.34×2.67×3.49×1.64×3.87×PARAFAC2 is markedly slowSimilar runtime betweenthe initializing methods
Figure 4: Running time for each ALS iteration on a target
domain. Note that M/e.sc/t.sc/a.sc/hyphen.scP2 achieves up to 387faster speed
than PARAFAC2 which decomposes the entire tensors in
multiple domains whenever a new target tensor emerges.
Meta-P2 shows similar runtime to other initializing methods.
4.5 Ablation Study (Q4)
We provide an ablation study on the missing value prediction with
respect to the eectiveness of each module in M/e.sc/t.sc/a.sc/hyphen.scP2. M/e.sc/t.sc/a.sc/hyphen.scP2-
meta is M/e.sc/t.sc/a.sc/hyphen.scP2 without the meta-learning-based decomposition
on the source domains; it performs a naive PARAFAC2 decomposi-
tion instead of meta-learning-based decomposition on the source
domains, and use the naively decomposed result when adapting to
a new target domain. M/e.sc/t.sc/a.sc/hyphen.scP2-imp and M/e.sc/t.sc/a.sc/hyphen.scP2-res are M/e.sc/t.sc/a.sc/hyphen.scP2
without considering the missing values by imputation, and M/e.sc/t.sc/a.sc/hyphen.scP2
without the residual connection of the meta factor, respectively.
Table 5 shows that each module of M/e.sc/t.sc/a.sc/hyphen.scP2 eectively reduces
the prediction error. Specically, M/e.sc/t.sc/a.sc/hyphen.scP2-meta shows poor per-
formance compared to the proposed M/e.sc/t.sc/a.sc/hyphen.scP2 in every case. This
indicates that the meta-learning-based decomposition on source
domains eectively enhances the quality of decomposition result
of a new target domain.
4.6 Case Study (Q5)
We showed that M/e.sc/t.sc/a.sc/hyphen.scP2 successfully detects or predicts the abnor-
mal values through Sections 4.2 and 4.3. Our further question is, how
is an anomalous point related to a real-world event? To answer this,
we present discoveries of analyzing historical prices of S&P500. WeTable 5: Ablation study on missing value prediction problem.
The lowest errors are in bold. Note that each module of M/e.sc/t.sc/a.sc/hyphen.sc
P2improves the performance of missing value prediction.
Missing
Value Prediction (missing ratio 𝑟𝑚= 0.5)
Datasets MTD-imp
MTD-res MTD-meta M/e.sc/t.sc
/a.sc/hyphen.scP2 (proposed)
Nasdaq 0.92400.0093
0.70790.0433 0.65260.0396 0.51020.0261
SP500 0.87210.0152
0.66710.0302 0.60530.0209 0.47120.0251
K
orea stock 0.91000.0274
0.72420.0401 0.65300.0417 0.55950.0172
NA
TOPS 0.78870.0012
0.61050.0024 0.57280.0055 0.50970.0042
Cricket 1.01190.0045
0.97020.0262 0.97630.0161 0.92970.0205
compose a new target domain by selecting ten largest AI-themed
stocks within S&P500 dataset (see Figure 1), while keeping other
settings consistent with those described in Section 4.1. We detect
ve abnormal days by nding the days with the top-5 largest errors
between the true stock features and the reconstructed ones.
Figure 5 compares the abnormal days detected by M/e.sc/t.sc/a.sc/hyphen.scP2 and
PARAFAC2-T in IBM and Netix. We observe that the detected days
ofM/e.sc/t.sc/a.sc/hyphen.scP2 are closely related to real-world events, showing its
ecacy in interpreting anomalies. We also note that the detected
points of PARAFAC2-T are densely distributed compared to M/e.sc/t.sc/a.sc/hyphen.sc
P2, failing to detect several real-world events that M/e.sc/t.sc/a.sc/hyphen.scP2 has
successfully identied.
5 Related Works
We review previous PARAFAC2 decomposition approaches for ir-
regular tensors and meta learning methods.
Tensor decomposition. Tensor decomposition methods are ap-
plied to many real-world tasks including model compression [ 32,
38,41,45], anomaly detection [ 9,27,37,47], forecasting [ 8,26,35],
answering time range queries [ 2,17,18], and missing value pre-
diction [ 25]. However, those decomposition methods for regular
tensors assume that every dimension has the same length across
the slices, which is not true for irregular tensors. Recently, neural
tensor factorization methods have been actively studied in recom-
mender systems [ 4,42]. The main drawback of the neural tensor
models compared to ALS-based tensor decomposition methods is its
computational complexity. Neural tensor models involve training a
neural network, which can be computationally intensive, especially
for large datasets and complex architectures.
 
1390Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD ’24, August 25–29, 2024, Barcelona, Spain.
S&P 500 hits record highsQ2’ 20 reportCOVID19New subscriber count report
PARAFAC2-T cannot detect COVID19Cannot detectlarge dropCOVID192019 annualreportQ2’ 19 report2020 annualreportPARAFAC2-T does notdetect the events(a) Meta-P2 (IBM)(b) Meta-P2 (Netflix)
(c) PARAFAC2 (IBM)(d) PARAFAC2 (Netflix)
Figure 5: Top-5 anomaly detection results of M/e.sc/t.sc/a.sc/hyphen.scP2 on (a) IBM and (b) Netix, and PARAFAC2-T on (c) IBM and (d) Netix.
Note that the detected anomalies (red circles) of M/e.sc/t.sc/a.sc/hyphen.scP2 are closely related to real-world events during the corresponding
periods. PARAFAC2-T detect densely distributed abnormal points, leading to the oversight of various real-world events.
PARAFAC2 decomposition. Many works have been studied to
nd patterns of irregular tensors based on PARAFAC2 [ 6,29,30,40,
43]. DPar2 [ 19,20] is a preprocessing algorithm for decomposing
irregular dense tensors. The method eectively compresses each
slice matrix of a given irregular tensor by careful reordering of com-
putations. SPARTan [ 31] is a scalable PARAFAC2 decomposition
algorithm for irregular sparse tensors. COPA [ 1] is an extension of
SPARTan, which improves the performance of it with additional
constraints. To nd the latent factors in large streaming tensors,
SPADE [ 12] denes a non-temporal factor and processes the newly
incoming part using the accelerated MTTKRP [ 31]. Dash [ 21] ef-
ciently nds the latent factors by avoiding naive computations
involved with old data when new data are given. However, those
methods have limitations in decomposing a new target tensor using
given tensors in multiple domains since they assume all tensors are
in the same domain.
Various task-specic irregular tensor decomposition methods
have been studied to improve the performance of their target tasks.
To predict missing entries in irregular tensors, Atom [ 22] reformu-
lates the objective function to completely exclude missing values,
ensuring more accurate predictions. LogPar [ 44] addresses the miss-
ing values by modeling the irregular tensor with a Bernoulli dis-
tribution, parameterized by an underlying real-valued tensor, and
approximates this tensor using a positive-unlabeled learning loss
function. However, those task-specic methods (e.g., missing value
prediction) are applicable only to specic tensors (e.g., tensors with
missing values). Meta-P2 is a task-agnostic decomposition method
(general framework) that can be applied to many downstream tasks.
Meta-learning. Many meta-learning methods have been pro-
posed to deal with the problem of limited data in many machine
learning areas [ 11,15,34]. FOMAML (rst-order MAML) [ 28] is
a variation of MAML that uses rst-order gradient information
to update the model’s parameters. However, the previous works
on meta-learning are not directly applicable for PARAFAC2 since
they update model parameters based on gradient descent update
whereas the decomposition methods update them through CP-ALS.Applications of meta-learning. Previous works have used
the meta-learning algorithm to solve few show learning problems.
Chien et al. [ 7] employed meta-learning algorithm for hyperpa-
rameter optimization in dialogue system. Gupta et al. [ 13] used
unsupervised meta-learning algorithm for reinforcement learning.
Lian et al. [ 23] applied meta-learning algorithm to neural archi-
tecture search to learn a meta-architecture that is able to adapt to
a new task quickly through a few gradient steps. To the best of
our knowledge, M/e.sc/t.sc/a.sc/hyphen.scP2 is the rst domain adaptation method for
decomposing irregular tensors based on meta-learning approach.
6 Conclusion
We propose M/e.sc/t.sc/a.sc/hyphen.scP2, a fast and accurate domain adaptation method
for decomposing irregular tensors. M/e.sc/t.sc/a.sc/hyphen.scP2 obtains a meta factor
matrix from existing tensors in multiple source domains, and the
meta factor helps nd accurate patterns of a new target tensor
quickly. This is achieved by training the meta factor to possess
two key properties: 1) generality by sharing the meta factor along
the source domains, and 2) easy adaptability by ALS-based meta-
learning. Extensive experiments on real-world datasets show that
M/e.sc/t.sc/a.sc/hyphen.scP2 achieves the best performance in missing value prediction
and anomaly detection problems. Future works include extending
M/e.sc/t.sc/a.sc/hyphen.scP2 for other decomposition methods beyond PARAFAC2.
Acknowledgments
This work was supported by the National Research Foundation
of Korea (NRF) funded by MSIT (2022R1A2C3007921). This work
was also supported by Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Ko-
rea government (MSIT) [No.2022-0-00641, XVoice: Multi-Modal
Voice Meta Learning], [No.RS-2021-II211343, Articial Intelligence
Graduate School Program (Seoul National University)], and [No.RS-
2021-II212068, Articial Intelligence Innovation Hub (Articial In-
telligence Institute, Seoul National University)]. U Kang is the cor-
responding author.
 
1391KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
References
[1]Ardavan Afshar, Ioakeim Perros, Evangelos E. Papalexakis, Elizabeth Searles,
Joyce C. Ho, and Jimeng Sun. 2018. COPA: Constrained PARAFAC2 for Sparse &
Large Datasets. In CIKM.
[2]Dawon Ahn, Jun-Gi Jang, and U Kang. 2022. Time-aware tensor decomposition
for sparse tensors. Mach. Learn. (2022).
[3]Dawon Ahn, Seyun Kim, and U Kang. 2021. Accurate Online Tensor Factorization
for Temporal Tensor Streams with Missing Values. In CIKM.
[4]Huiyuan Chen and Jing Li. 2020. Neural tensor model for learning multi-aspect
factors in recommender systems. In IJCAI.
[5]Peter A. Chew, Brett W. Bader, Stephen Helmreich, Ahmed Abdelali, and Stephen J.
Verzi. 2011. An information-theoretic, vector-space-model approach to cross-
language information retrieval. Nat. Lang. Eng. 17, 1 (2011), 37–70.
[6]Peter A. Chew, Brett W. Bader, Tamara G. Kolda, and Ahmed Abdelali. 2007.
Cross-language information retrieval using PARAFAC2. In SIGKDD. ACM.
[7]Jen-Tzung Chien and Wei Xiang Lieow. 2019. Meta Learning for Hyperparameter
Optimization in Dialogue System. In Interspeech.
[8]Beyza Ermis, Evrim Acar, and Ali Taylan Cemgil. 2015. Link prediction in
heterogeneous data via generalized coupled tensor factorization. Data Min.
Knowl. Discov. (2015).
[9]Hadi Fanaee-T and João Gama. 2016. Tensor-based anomaly detection: An
interdisciplinary survey. Knowl. Based Syst. (2016).
[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In ICML.
[11] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. 2019. Online
meta-learning. In ICML.
[12] Ekta Gujral, Georgios Theocharous, and Evangelos E. Papalexakis. 2020. SPADE:
Streaming PARAFAC2 DEcomposition for Large Datasets. In SDM.
[13] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. 2018.
Unsupervised Meta-Learning for Reinforcement Learning. CoRR (2018).
[14] R. A. Harshman. 1972. PARAFAC2: Mathematical and Technical Notes. In UCLA
Working Papers in Phonetics.
[15] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey.
2022. Meta-Learning in Neural Networks: A Survey. IEEE Trans. Pattern Anal.
Mach. Intell. (2022).
[16] Muhammad Abdullah Jamal and Guo-Jun Qi. 2019. Task Agnostic Meta-Learning
for Few-Shot Learning. In CVPR.
[17] Jun-Gi Jang, Dongjin Choi, Jinhong Jung, and U Kang. 2018. Zoom-SVD: Fast
and Memory Ecient Method for Extracting Key Patterns in an Arbitrary Time
Range. In CIKM.
[18] Jun-Gi Jang and U Kang. 2021. Fast and Memory-Ecient Tucker Decomposition
for Answering Diverse Time Range Queries. In KDD.
[19] Jun-Gi Jang and U Kang. 2022. DPar2: Fast and Scalable PARAFAC2 Decomposi-
tion for Irregular Dense Tensors. In ICDE.
[20] Jun-Gi Jang and U Kang. 2023. Static and Streaming Tucker Decomposition for
Dense Tensors. ACM Trans. Knowl. Discov. Data 17, 5 (2023), 66:1–66:34.
[21] Jun-Gi Jang, Jeongyoung Lee, Yong-chan Park, and U Kang. 2023. Fast and
Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors - Algorithm and
Application. In KDD.
[22] Jun-Gi Jang, Jeongyoung Lee, Jiwon Park, and U Kang. 2022. Accurate PARAFAC2
decomposition for temporal irregular tensors with missing values. In Big Data.
[23] Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Jun-
zhou Huang, and Shenghua Gao. 2020. Towards Fast Adaptation of Neural
Architectures with Meta Learning. In ICLR.
[24] Yu-Ru Lin, Jimeng Sun, Paul C. Castro, Ravi B. Konuru, Hari Sundaram, and
Aisling Kelliher. 2009. MetaFac: community discovery via relational hypergraph
factorization. In SIGKDD.
[25] Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. 2019. Costco: A neural
tensor completion model for sparse tensors. In SIGKDD.
[26] Xiaoyang Ma, Lan Zhang, Lan Xu, Zhicheng Liu, Ge Chen, Zhili Xiao, Yang Wang,
and Zhengtao Wu. 2019. Large-scale User Visits Understanding and Forecasting
with Deep Spatial-Temporal Tensor Factorization Framework. In SIGKDD.
[27] Maxwell McNeil, Carolina Mattsson, Frank W Takes, and Petko Bogdanov. 2023.
CADENCE: Community-Aware Detection of Dynamic Network States. In SDM.
[28] Alex Nichol, Joshua Achiam, and John Schulman. 2018. On First-Order Meta-
Learning Algorithms. CoRR (2018).
[29] Yannis Panagakis and Constantine Kotropoulos. 2011. Automatic music tagging
via PARAFAC2. In ICASSP. IEEE, 481–484.
[30] Evangelia Pantraki and Constantine Kotropoulos. 2015. Automatic image tagging
and recommendation via PARAFAC2. In MLSP.
[31] Ioakeim Perros, Evangelos E. Papalexakis, Fei Wang, Richard W. Vuduc, Elizabeth
Searles, Michael Thompson, and Jimeng Sun. 2017. SPARTan: Scalable PARAFAC2
for Large & Sparse Data. In SIGKDD.
[32] Anh Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov, Julia
Gusak, Petr Tichavský, Valeriy Glukhov, Ivan V. Oseledets, and Andrzej Cichocki.
2020. Stable Low-Rank Tensor Decomposition for Compression of Convolutional
Neural Network. In ECCV.[33] Marie Roald, Carla Schenker, Vince D Calhoun, Tulay Adali, Rasmus Bro,
Jeremy E Cohen, and Evrim Acar. 2022. An AO-ADMM approach to constraining
PARAFAC2 on all modes. SIAM Journal on Mathematics of Data Science (2022).
[34] Nicolas Schweighofer and Kenji Doya. 2003. Meta-learning in Reinforcement
Learning. Neural Networks (2003).
[35] Alessandro Spelta. 2017. Financial market predictability with tensor decomposi-
tion and links forecast. Appl. Netw. Sci. 2 (2017), 7.
[36] Stephan Spiegel, Jan Hendrik Clausen, Sahin Albayrak, and Jérôme Kunegis. 2011.
Link Prediction on Evolving Data Using Tensor Factorization. In PAKDD.
[37] Leo Tisljaric, Soa Fernandes, Tonci Caric, and João Gama. 2020. Spatiotemporal
Trac Anomaly Detection on Urban Road Network Using Tensor Decomposition
Method. In Discovery Science - 23rd International Conference.
[38] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. 2018. Tensor Decomposi-
tion for Compressing Recurrent Neural Network. In IJCNN.
[39] Ricardo Vilalta and Youssef Drissi. 2002. A Perspective View and Survey of
Meta-Learning. Artif. Intell. Rev. (2002).
[40] Barry M Wise, Neal B Gallagher, and Elaine B Martin. 2001. Application of
PARAFAC2 to fault detection and diagnosis in semiconductor etch. Journal of
Chemometrics: A Journal of the Chemometrics Society (2001).
[41] Bijiao Wu, Dingheng Wang, Guangshe Zhao, Lei Deng, and Guoqi Li. 2020.
Hybrid tensor decomposition in neural network compression. Neural Networks
132 (2020), 309–320.
[42] Xian Wu, Baoxu Shi, Yuxiao Dong, Chao Huang, and Nitesh V Chawla. 2019.
Neural tensor factorization for temporal interaction learning. In WSDM.
[43] Kejing Yin, Ardavan Afshar, Joyce C. Ho, William K. Cheung, Chao Zhang, and
Jimeng Sun. 2020. LogPar: Logistic PARAFAC2 Factorization for Temporal Binary
Data with Missing Values. In SIGKDD.
[44] Kejing Yin, Ardavan Afshar, Joyce C Ho, William K Cheung, Chao Zhang, and
Jimeng Sun. 2020. LogPar: Logistic PARAFAC2 factorization for temporal binary
data with missing values. In SIGKDD.
[45] Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang, and Bo Yuan. 2020. Com-
pressing Recurrent Neural Networks Using Hierarchical Tucker Tensor Decom-
position. CoRR abs/2005.04366 (2020).
[46] Dave Zachariah, Martin Sundin, Magnus Jansson, and Saikat Chatterjee. 2012.
Alternating Least-Squares for Low-Rank Matrix Reconstruction. IEEE Signal
Process. Lett. 19, 4 (2012), 231–234.
[47] Xing Zhang, Gongjian Wen, and Wei Dai. 2016. A Tensor Decomposition-Based
Anomaly Detection Algorithm for Hyperspectral Image. IEEE Trans. Geosci.
Remote. Sens. (2016).
A Appendix
A.1 Relation of Equation (12)and Equation (10)
L/e.sc/m.sc/m.sc/a.sc A.1. Suppose we are given a loss function L¹fX𝑠g;Vmetaº=Í𝐾
𝑘=1kX𝑠;𝑘 U𝑠;𝑘S𝑠;𝑘V>
metak2
𝐹. Then nding optimal Vmeta by mini-
mizingL¹fX𝑠g;Vmetaºis equivalent to nding it by minimizing the
following loss function:
L𝑉¹fX𝑠g;Vmetaº=𝐾∑︁
𝑘=1kE𝑠;𝑘X𝑠;𝑘 V>
metak2
𝐹 (23)
where E𝑠;𝑘=𝐾¹Í𝐾
𝑗=1S>
𝑠;𝑗U>
𝑠;𝑗U𝑠;𝑗S𝑠;𝑗º 1S>
𝑠;𝑘U>
𝑠;𝑘.
P/r.sc/o.sc/o.sc/f.sc. To nd optimal Vmeta that minimizes a given loss, we
dierentiate the loss in terms of Vmeta and nd Vmeta where the
derivative equals zero.
We rst nd Vmetathat minimizes the original loss L¹fX𝑠g;Vmetaº:
rVmetaL¹fX𝑠g;Vmetaº (24)
= 2𝐾∑︁
𝑘=1¹X>
𝑠;𝑘 VmetaS>
𝑠;𝑘U>
𝑠;𝑘ºU𝑠;𝑘S𝑠;𝑘=0 (25)
,Vmeta𝐾∑︁
𝑘=1S>
𝑠;𝑘U>
𝑠;𝑘U𝑠;𝑘S𝑠;𝑘=𝐾∑︁
𝑘=1X>
𝑠;𝑘U𝑠;𝑘S𝑠;𝑘 (26)
,Vmeta=𝐾∑︁
𝑘=1X>
𝑠;𝑘U𝑠;𝑘S𝑠;𝑘¹𝐾∑︁
𝑗=1S>
𝑠;𝑗U>
𝑠;𝑗U𝑠;𝑗S𝑠;𝑗º 1(27)
 
1392Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition KDD ’24, August 25–29, 2024, Barcelona, Spain.
We then discover Vmeta that minimizes the reformulated loss
L𝑉¹fX𝑠g;Vmetaº:
rVmetaL𝑉¹fX𝑠g;Vmetaº (28)
= 2𝐾∑︁
𝑘=1¹X>
𝑠;𝑘E>
𝑠;𝑘 Vmetaº=0 (29)
,Vmeta=1
𝐾𝐾∑︁
𝑘=1X>
𝑠;𝑘E>
𝑠;𝑘 (30)
Since E𝑠;𝑘=𝐾¹Í𝐾
𝑗=1S>
𝑠;𝑗U>
𝑠;𝑗U𝑠;𝑗S𝑠;𝑗º 1S>
𝑠;𝑘U>
𝑠;𝑘, Equation (30)is
equal to Equation (27) which ends the proof.

A.2 Proof of Lemma 3.1
P/r.sc/o.sc/o.sc/f.sc. We denote E𝑠;𝑘X𝑠;𝑘in Equation (12)asZ𝑠;𝑘for simplic-
ity. Then the loss function L𝑉¹fX𝑠g;Vmetaºis written as:
L𝑉¹fX𝑠g;Vmetaº=𝐾∑︁
𝑘=1kZ𝑠;𝑘 V>
metak2
𝐹 (31)
The gradientrVmetaL𝑉¹fX𝑠g;Vmetaºis computed as:
rVmetaL𝑉¹fX𝑠g;Vmetaº=rVmeta𝐾∑︁
𝑘=1kZ𝑠;𝑘 V>
metak2
𝐹
=2¹𝐾Vmeta 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º(32)
The adapted factor matrix V𝑠to each source domain 𝑠is ex-
pressed as follows:
V𝑠=Vmeta 𝜂rVmetaL𝑉¹fX𝑠g;Vmetaº
=¹1 2𝜂𝐾ºVmeta¸2𝜂𝐾∑︁
𝑘=1Z>
𝑠;𝑘(33)
where the rst equation uses Equation (13)and the second one
is derived from Equation (32). The gradientrV𝑠L𝑉¹fX𝑠g;V𝑠ºis
expressed in terms of Vmeta:
rV𝑠L𝑉¹fX𝑠g;V𝑠º=2¹𝐾V𝑠 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º
=2¹𝐾¹1 2𝜂𝐾ºVmeta¸2𝜂𝐾𝐾∑︁
𝑘=1Z>
𝑠;𝑘 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º
=2¹1 2𝜂𝐾º¹𝐾Vmeta 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º(34)
Thus, the sum of two gradients in Equations (32)and(34)is ex-
pressed with Vmeta as the following equation:
rVmetaL𝑉¹fX𝑠g;Vmetaº¸r V𝑠L𝑉¹fX𝑠g;V𝑠º=
4¹1 𝜂𝐾º¹𝐾Vmeta 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º(35)The gradientrVmetaL𝑉¹fX𝑠g;V𝑠ºis computed as follows:
rVmetaL𝑉¹fX𝑠g;V𝑠º=rVmeta𝐾∑︁
𝑘=1kZ𝑠;𝑘 V>
𝑠k
=rVmeta𝐾∑︁
𝑘=1kZ𝑠;𝑘 ¹1 2𝜂𝐾ºV>
meta 2𝜂𝐾∑︁
𝑙=1Z𝑠;𝑙k2
𝐹
=2¹1 2𝜂𝐾º 
¹1 2𝜂𝐾º𝐾Vmeta 𝐾∑︁
𝑘=1¹Z>
𝑠;𝑘 2𝜂𝐾∑︁
𝑙=1Z>
𝑠;𝑙º!
=2¹1 2𝜂𝐾º 
¹1 2𝜂𝐾º𝐾Vmeta ¹1 2𝜂𝐾º𝐾∑︁
𝑘=1Z>
𝑠;𝑘!
=2¹1 2𝜂𝐾º2¹𝐾Vmeta 𝐾∑︁
𝑘=1Z>
𝑠;𝑘º(36)
where the fourth equality usesÍ𝐾
𝑘=1Í𝐾
𝑙=1Z>
𝑠;𝑙=𝐾Í𝐾
𝑘=1Z>
𝑠;𝑘. The
proof is nished by comparing Equations (35) and (36). 
A.3 Proof of Theorem 3.2
P/r.sc/o.sc/o.sc/f.sc. Using Lemma 3.1, the meta-update step of M/e.sc/t.sc/a.sc/hyphen.scP2 in
Equation (14) is expressed as follows:
Vmeta Vmeta 𝜂rVmeta∑︁
𝑠2SL𝑉¹fX𝑠g;V𝑠º
=Vmeta 𝜂𝐶∑︁
𝑠2S rVmetaL𝑉¹fX𝑠g;Vmetaº¸r V𝑠L𝑉¹fX𝑠g;V𝑠º
(37)
Note thatrVmetaL𝑉¹fX𝑠g;VmetaºandrV𝑠L𝑉¹fX𝑠g;V𝑠ºare ex-
pressed by¹Vmeta V𝑠º𝜂and¹V𝑠 V0𝑠º𝜂using Equations (13)
and(17), respectively. Then the meta-update step in Equation (37)
is written as follows:
Vmeta Vmeta 𝜂𝐶∑︁
𝑠2S¹¹Vmeta V𝑠º𝜂¸¹V𝑠 V0
𝑠º𝜂º
=Vmeta¸𝐶∑︁
𝑠2S¹V0
𝑠 Vmetaº(38)
which ends the proof.

A.4 Proof of Lemma 3.3
P/r.sc/o.sc/o.sc/f.sc. A naive PARAFAC2 decomposition on source domains
takesOÍ𝐾
𝑘=1𝐼𝑘𝐽𝑅𝑆
time where 𝑆is the number of source do-
mains, and𝑅is the target rank.
The meta-learning-based decomposition of MTD on source do-
mains consists of two steps: 1) performing a typical PARAFAC2
decomposition for each source domain 𝑠2 S, and 2) updating
the meta factor matrix Vmeta. To update the meta factor matrix
Vmeta,M/e.sc/t.sc/a.sc/hyphen.scP2 rst computes V0𝑠for each domain 𝑠2 S (see
line 13 of Algorithm 1), and merges them (see line 15 of Algo-
rithm 1). Computing V0𝑠of domain𝑠takesO 𝐽𝐾𝑅2time. There-
fore, updating Vmeta by merging V0𝑠for𝑠=1𝑆 takesO 𝐽𝐾𝑅2𝑆
time, and the decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains takes
O
¹Í𝐾
𝑘=1𝐼𝑘𝐽𝑅𝑆º¸𝐽𝐾𝑅2𝑆
time. SinceO¹Í𝐾
𝑘=1𝐼𝑘º¡O¹𝐾𝑅º, the
meta-learning-based decomposition of M/e.sc/t.sc/a.sc/hyphen.scP2 on source domains
takesO¹Í𝐾
𝑘=1𝐼𝑘𝐽𝑅𝑆ºtime, which is the same time complexity
as that of a naive PARAFAC2 decomposition on the source do-
mains. 
 
1393KDD ’24, August 25–29, 2024, Barcelona, Spain. Junghun Kim, Ka Hyun Park, Jun-Gi Jang, and U Kang
A.5 Proof of Lemma 3.4
P/r.sc/o.sc/o.sc/f.sc. Decomposing a new irregular tensor X𝑇in a target do-
main𝑇consists of two steps: 1) initializing V𝑇withVmeta, and 2)performing PARAFAC2-ALS algorithm. Therefore, the time com-
plexity of M/e.sc/t.sc/a.sc/hyphen.scP2 for decomposing a new target tensor is the same
as that of PARAFAC2-ALS decomposition on the target tensor. 
 
1394