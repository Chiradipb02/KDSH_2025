Text Matching Indexers in Taobao Search
Sen Li∗
Alibaba Group
Hangzhou, China
lisen.lisen@taobao.comFuyu Lv
Alibaba Group
Hangzhou, China
fuyu.lfy@taobao.comRuqing Zhang
CAS Key Lab of Network Data
Science and Technology, ICT, CAS
Beijing, China
zhangruqing@ict.ac.cn
Dan Ou
Alibaba Group
Hangzhou, China
oudan.od@taobao.comZhixuan Zhang
Alibaba Group
Hangzhou, China
zhibing.zzx@taobao.comMaarten de Rijke
University of Amsterdam
Amsterdam, The Netherlands
m.derijke@uva.nl
ABSTRACT
Product search is an important service on Taobao, the largest e-
commerce platform in China. Through this service, users can easily
find products relevant to their specific needs. Coping with billion-
size query loads, Taobao product search has traditionally relied
on classical term-based retrieval models due to their powerful and
interpretable indexes. In essence, efficient retrieval hinges on the
proper storage of the inverted index. Recent successes involve re-
ducing the size (pruning) of the inverted index but the construction
and deployment of lossless static index pruning in practical product
search still pose non-trivial challenges.
In this work, we introduce a novel SMart INDexing ( SMIND )
solution in Taobao product search. SMIND is designed to reduce
information loss during the static pruning process by incorporating
user search preferences. Specifically, we first construct “user-query-
item” hypergraphs for four different search preferences, namely
purchase, click, exposure, and relevance. Then, we develop an effi-
cient TermRank algorithm applied to these hypergraphs, to preserve
relevant items based on specific user preferences during the pruning
of the inverted indexer. Our approach offers fresh insights into the
field of product search, emphasizing that term dependencies in user
search preferences go beyond mere text relevance. Moreover, to
address the vocabulary mismatch problem inherent in term-based
models, we also incorporate an multi-granularity semantic retrieval
model to facilitate semantic matching. Empirical results from both
offline evaluation and online A/B tests showcase the superiority
ofSMIND over state-of-the-art methods, especially in commerce
metrics with significant improvements of 1.34% in Pay Order Count
and 1.50% in Gross Merchandise Value. Besides, SMIND effectively
mitigates the Matthew effect of user queries and has been in service
for hundreds of millions of daily users since November 2022.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671654CCS CONCEPTS
•Information systems →Top-k retrieval in databases.
KEYWORDS
Product search, Static pruning inverted index, Semantic matching
ACM Reference Format:
Sen Li, Fuyu Lv, Ruqing Zhang, Dan Ou, Zhixuan Zhang, and Maarten de
Rijke. 2024. Text Matching Indexers in Taobao Search. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671654
1 INTRODUCTION
E-commerce platforms like Amazon and Taobao have become part
of people’s daily lives. In Taobao, the product search engine serves
hundreds of millions of users, by providing the top- 𝑘personal-
ized relevant items from billion-scale candidate products, within
hundreds of milliseconds. The classic industry product search archi-
tecture, known as “index-retrieval-prerank-rank-rerank," includes
three sequential steps: (1) indexing: to build an index for each item
in the corpus; (2) retrieval: to retrieve an initial set of candidate
items for a user query; and (3) prerank-rank-rerank: to calculate
a personalized relevance score of each candidate. Of these, the in-
dexing and retrieval stages are pivotal as they determine the upper
bounds of the search performance.
To handle billions of queries and products, Taobao predominantly
employs classical term-based matching techniques, emphasizing
interpretability andhigh search relevance. Specifically, in indexing,
we construct traditional inverted indexes for the text of each item.
During retrieval, we integrate the rewritten query [ 22] with the
original user query to mitigate vocabulary mismatches [ 32,39,51].
Subsequently, we perform exact matching between the query and
the items. To return all matched candidates, a popular method is
to employ a dynamic index pruning scheme [ 3,14]. However, in
practical scenarios, this approach can prove to be time, storage,
and computation-intensive. This is especially significant in Taobao
product search, where a substantial portion of latency is dedicated
to the “prerank-rank-rerank” phase for personalization modeling.
Additionally, users tend to focus on a limited number of top- 𝑘items
for a query and often issue a new query if the initial results fail to
meet their expectations.
5339
KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
Hence, in Taobao product search, a lightweight static pruning
inverted index scheme [ 3,4,38], a.k.a. static term-based pruning,
is widely employed, saving 50% of system resource. This scheme
comprises two parts: the pruned inverted index, denoted as 𝐼𝑃, and
the full inverted index, denoted as 𝐼𝐹.𝐼𝑃consists of the posting lists
(an occurrence of a term within the text an item is called a posting)
for each term but truncated with a given length, while 𝐼𝐹is the
regular full posting lists. A common practice for pruning a posting
list is to consider term dependencies related to textual relevance.
In web search, Carmel et al . [4] proposed using the TF-IDF scoring
function𝑓(𝑡,𝑖)to preserve the textually most relevant items for
reducing the truncation loss of each term. DeepCT [ 8], SPARTA [ 50],
and SPLADE [ 15,16], are recently proposed methods to provide
contextualized term dependencies of text relevance signal.
However, pruning based on the general textual relevance term
dependencies is not sufficient for product search. To elaborate,
many merchants employ search engine optimization (a.k.a. SEO) to
increase their visibility by using relevant and popular keywords to
describe their products . This practice, in turn, diminishes the effi-
cacy of textual relevance. Additionally, existing work overlooks the
term dependencies of User Search Preferences (USP), as discussed
in Section 4.5.4 and 4.5.5. This oversight results in the incapacity to
explicitly retain user-preferred items within the truncated postings
lists. Through an analysis of user logs, we found that the majority
of users made purchases based on specific query terms. To illustrate,
consider an item titled “ 蕾丝薄款连衣裙 (Self-portrait Lace Thin
Dress).” Users did not click or purchase it with the textual relevant
term “薄款 (thin),” but with combinations of the remaining terms.
This phenomenon highlights the existence of higher-order term-
item dependencies within USP, transcending mere text relevance.
These higher-order term dependencies not only assist in mitigating
SEO practices by excluding certain textually relevant terms often
added by merchants, but also have the potential to minimize in-
formation loss by retaining user-preferred items. However, how
to explicitly capture and incorporate these dependencies into the
static pruning process while mitigating the noise inherent in user
logs, remains relatively unexplored.
Therefore, in this paper, we propose an efficient billion-scale
product search indexing solution, named SMart INDexing ( SMIND ).
The key idea is to minimize information loss during the static in-
verted index pruning process, by incorporating higher-order term
dependencies from user search preferences. Concretely, we first
investigate the user search process and define four basic prefer-
ence patterns, namely purchase, click, exposure, and relevance
preference pattern. Subsequently, we utilize extensive user logs
and employ ternary “user-query-item (𝑢,𝑞,𝑖)” interactions to build
four hypergraphs [ 7], i.e., user search purchase, click, exposure,
and relevance preferences hypergraphs. We then decompose each
hypergraph(𝑢,𝑞,𝑖)into a term-level one (𝑢,𝑞𝑡,𝑖𝑡)by representing
the vertex of query 𝑞and item𝑖with their text terms. Then, we
devise an efficient TermRank algorithm to explicitly capture the
higher-order term-item dependencies within USP. Finally, during
the static pruning process, to alleviate the inherent noise in user
behavior, we formulate a user-aware term scoring function 𝑓(𝑡,𝑖)
by considering both the term dependencies within USP and an addi-
tional query-independent item quality factor, to rank items within
the postings lists and preserve the top items.Given the diverse nature of user demands, it is intractable for
query rewriting to address the vocabulary mismatch problem on its
own. For instance, when a user enters a query like “ 衣服(clothes), ”
they may actually intend to purchase a “ 连衣裙 (dress).” Addition-
ally, the inconsistent contextual alignment between the query and
the item can lead to variations in the Chinese tokenization of the
same text. This may adversely affect the performance of both the
original user queries and their rewritten counterparts. To this end,
we further incorporate a multi-grained semantic neural retrieval
model to facilitate fine-grained contextualized interactions between
query and item representations. It is combined with the original user
queries and their rewritten versions to enhance semantic matching
capabilities, thereby improving recall for potentially missed items.
To the best of our knowledge, SMIND is the first solution to ex-
plore the use of term dependencies in user search preferences for
pruning inverted indexes, and effectively combines query rewriting
with an exact and fuzzy match indexer. The effectiveness of SMIND
is verified by extensive offline qualitative and quantitative analyses,
and large-scale online A/B tests. We observed that for torso and
tail queries, SMIND enables text matching indexers to deliver high-
quality items aligned with user search intentions. This prevents
users from shifting their queries to top queries, thereby alleviating
the Matthew effect [36] of user queries.
The main contributions of this work are as follows:
(1)We introduce SMIND , an efficient and effective billion-scale
product search indexing solution that includes a user search
preferences-aware pruning inverted indexer and a semantic
indexer, to enable the retrieval of relevant, high-quality items.
(2)We conduct a term-level analysis of multiple user search pref-
erences, represented as (𝑢,𝑞𝑡,𝑖𝑡)hypergraphs, which are em-
ployed to prune inverted indexes and bridge the gap between
user queries and pruned inverted indexes.
(3) We offer new insights that user search preferences encompass
higher-order term dependencies beyond text relevance.
(4)We evaluate SMIND ’s performance through comprehensive of-
fline qualitative and quantitative analyses, as well as large-scale
online A/B tests conducted on Taobao Product Search.
2 RELATED WORK
2.1 Inverted Indexer
In a typical search engine efficient text/product retrieval is achieved
through the use of an inverted index structure [ 51], which asso-
ciates terms with relevant documents in the collection. For each
term in the document collection, a posting list of documents is
stored. The index file grows with increases in the size of the doc-
ument collection, which is problematic for fast and inexpensive
query evaluation in an online system [ 26]. Thus, effective prun-
ing techniques are needed to reduce the size of inverted indexes.
Such techniques generally fall into two categories, i.e., dynamic
and static. Dynamic pruning [ 3,11,14,41] decides, during query
evaluation, whether certain terms or document postings should be
included in the ranked list, until some stopping condition is met.
Static techniques [ 4,26,34,38] remove terms or items from the
full index in advance to take advantage of a two-tiered index. The
small, yet effective index that is left is called the pruned index. Static
and dynamic pruning techniques can complement each other, since
5340Text Matching Indexers in Taobao Search KDD ’24, August 25–29, 2024, Barcelona, Spain
searching the pruned index can be accelerated by dynamic pruning
for a specific query. Panigrahi and Gollapudi [35] exploit the struc-
tural characteristics of commerce search queries (i.e., from users
searching for products on the web) for the selection of documents,
which relies on manual labelling data to structure queries.
Our work focuses on static term-based index pruning at the
posting-level. It removes items from each term whose contribution
is so small that their removal has a minor impact on the retrieval
system. Carmel et al . [4] examine an idealized term-based pruning
algorithm. The score of a term for a document is formulated by term
dependencies of text relevance, i.e., TF-IDF [ 37] scoring function.
The pruned index has been proved, to some extent, to provide
results that are almost as good as search results derived from the
full index. Recently, learned sparse representation methods such as
DeepCT [ 8], SPARTA [ 50], and SPLADE [ 15,16], are being used to
provide contextualized term dependencies of text relevance signal.
However, they have been shown to frequently assign suboptimal
weights to terms [ 28,29,31], which may degrade search relevance
as we show below.
Long and Suel [26] propose to integrate global linked-based
ranking (e.g., PageRank [ 2]) and term-based text relevance ranking
as the final term-doc scoring function. Two optimized pruning
techniques, namely keyword-based and document-based pruning,
are studied by Ntoulas and Cho [34] to avoid any degradation
of result quality. Rossi et al . [38] combine the dynamic pruning
technique BMW [ 11] and a static two-tiered index (the pruned and
full index) to accelerate query evaluation.
To the best of our knowledge, existing studies into index prun-
ing are conducted primarily in the context of web search, where
text relevance and hyperlink analysis are key to the final term-
doc scoring function for pruning the postings lists. However, this
is far from sufficient in a product search setting. How to exploit
higher-order term dependencies of User Search Preferences (USP)
for pruning while mitigating the noise inherent in user logs has
not been addressed. Apart from using the term dependencies of
USP from user click logs to reduce the size of inverted index, there
are many graph-based dense retrieval works of modeling user click
logs [ 7,13,33]. However, they have difficulties in learning exact
matching signals, and thus tend to return irrelevant results when
users search for exact queries [6, 23].
2.2 Semantic Indexer
There is a rich literature [ 19,40,48] on capturing the semantics of
queries and documents for the first-stage retrieval. For an overview,
we refer to [17]. Building on this work, several large-scale deploy-
ments of semantic search systems have been reported. Nigam et al .
[32, Amazon ]develop a two-tower model to address the vocabu-
lary mismatch of a lexical matching engine. One side uses n-gram
query features and the other side exploits item features. Chang
et al. [5, Amazon ]use a tree-based extreme multi-label classifica-
tion model to improve inference latency. Huang et al . [18, Meta ]
describe their embedding-based retrieval system integrated with
term matching-based retrieval, which considers query texts and
searchers’ context factors together. Liu et al . [25, Meta ]and Yu
et al. [47, Meta ]apply multi-modal learning and pre-training tech-
niques to their social product search. Also using pre-training, Liu
et al. [24, Baidu ]develop a retrieval model based on ERNIE [ 42],a state-of-the-art Chinese pre-training model. Recently, Magnani
et al. [30, Walmart ]have presented a hybrid retrieval system for
e-commerce retail search. They combine the full inverted index and
an embedding-based neural retrieval to better serve the traffic of tail
queries. However, the search setting of Walmart is relatively small,
withe million-scale query loads and a small collection of items.
With billion-scale products, the search scenario requires careful
trade-offs between efficiency and various cost factors. Below, we
discuss the billion-scale product search indexing solution in Taobao
Product Search.
3 PROPOSED METHOD
Here, we will detail the user search preferences-aware pruned in-
verted indexer and semantic indexer of SMIND shown in Figure 1,
respectively. The retrieval process is illustrated in Section B.1 of the
Appendix. For notation, let 𝑈={𝑢1,...,𝑢𝑁}be a set of𝑁users,
𝑄={𝑞1,...,𝑞𝐿}the user queries, 𝐼={𝑖1,...,𝑖𝑀}a collection of
items, and𝑇={𝑡1,...,𝑡𝐾}the unique terms of collection 𝐼. See
Section A of the Appendix for the problem definition.
3.1 User Search Preferences-Aware Pruned
Inverted Indexer
In this section, we study how to improve the static term-based
pruning by using higher-order term dependencies of user search
preference patterns USP and query-independent item quality fac-
tor𝑄. First, we construct four term-level ternary “user-query-item
(𝑢,𝑞,𝑖)” interactions hypergraphs based on four basic patterns.
Then, we develop an efficient TermRank algorithm to capture mul-
tiple term dependencies of USP patterns. Finally, we integrate the
term dependencies of USP and 𝑄into the final term-based scoring
function for pruning the posting lists of each term.
3.1.1 HyperGraph Construction. A hypergraph is a more general
topological structure than a bipartite graph, where an edge connects
two or more nodes, named hyperedge. Given some ternary (𝑢,𝑞,𝑖)
interactions, it is intractable to construct a simple graph to preserve
ternary relations [ 7], and we have a hypergraph 𝐺=(𝑉,𝐸)includ-
ing: (1) a node set 𝑉whose elements represent a user, query, item
(i.e.,𝑉=𝑈∪𝑄∪𝐼); and (2) a hyperedge set 𝐸whose elements rep-
resent a ternary relation depicting the existence of an interaction
among them. We construct multiple hypergraphs 𝐺from user logs
based on the defined patterns 𝑓among ternary(𝑢,𝑞,𝑖), i.e.,
𝐺=𝑓(𝑢,𝑞,𝑖), (1)
where𝑓establishes the hyperedge among (𝑢,𝑞,𝑖)according to a
certain pattern, as follows:
𝐺𝑝𝑢𝑟/𝐺𝑐𝑙𝑘/𝐺𝑒𝑥𝑝𝑜 for the user purchase/click/exposure prefer-
ence pattern 𝑓𝑝𝑢𝑟/𝑓𝑐𝑙𝑘/𝑓𝑒𝑥𝑝𝑜, each hyperedge represents how a user
initiates a query and then purchases/clicks/sees an item.
𝐺𝑟𝑒𝑙for the user relevance pattern 𝑓𝑟𝑒𝑙, each hyperedge repre-
sents a relevant relation among (𝑢,𝑞,𝑖), where𝑖is an item, in the
rank phase of search system, is associated with a user and query.
Term-Level Decomposition. Because𝐺describe USP patterns at
the node-level(𝑢,𝑞,𝑖)rather than the term-level, we further decom-
pose each node into the term-level by replacing 𝑉’s query and item
elements with their terms 𝑞𝑡and𝑖𝑡, i.e., for a query 𝑞2(Figure 2(b))
5341KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
Query Rewriting ModuleQuery PlannerQuery(Query, Rewrites)SMINDSemantic IndexerQueryMultiple Granularities LayerTransformer_EncoderItemItem TowerSampled SoftmaxEmbeddingEmbedding.CosineTerm1TermNANDOR
USP-aware Pruned/Full Inverted Index
Inverted IndexerQuery Evaluation……
Term-level User Search PerferencesShortest  Posting List PrincipleANDTerm1TermNQueryRewritten QueryIndexing
Figure 1: General structure of the proposed large-scale Smart Indexing ( SMIND ) hybrid indexer solution and its combinations of
query rewriting technique.
u1u2q1q2i1i2u3i3e2e3e1e4
(a)Hypergraph
q2t1i3t1i2t1u1u2q1t1u3e2e3e1i1t1i1t2q2t2e4 (b)Term-Level Hypergraph
q2t1i3t1i2t1u1u2q1t1u3e2e3e1i1t1i1t2q2t2e4 (c)TermRank
Figure 2: Given four interactions (𝑢1,𝑞1,𝑖1),(𝑢2,𝑞1,𝑖2),
(𝑢2,𝑞2,𝑖3)and(𝑢3,𝑞2,𝑖2). (a) A sample of hypergraph whose
hyperedge 𝑒𝑖(𝑖=1,2,3,4)is a user-query-item interaction; (b)
Assuming 𝑞1={𝑡1},𝑞2={𝑡1,𝑡2},𝑖1={𝑡1,𝑡2},𝑖2={𝑡1},𝑖3={𝑡1}, we
decompose (a) into term-level; (c) Assuming 𝑞1𝑡1=𝑖1𝑡1=𝑖2𝑡1=𝑞2𝑡2,
𝑞2𝑡1=𝑖3𝑡1, we create value flow based on each term-level hy-
peredge(𝑢,𝑞𝑡,𝑖𝑡).
with two terms{𝑡1,𝑡2}, then the element is 𝑞2𝑡1,𝑞2𝑡2instead of the
whole query 𝑞2. We do the same on the item side.
Now, the hyperedge is able to describe the exact term match
between the user queries and item titles. For each ternary (𝑢,𝑞,𝑖)
interaction, we iterate over the combinations of terms in query and
item and then establish a hyperedge (𝑢,𝑞𝑡,𝑖𝑡)if and only if their
terms are the same. Considering an interaction of ( 𝑢1,𝑞1={𝑡1},
𝑖1={𝑡1,𝑡2}), there will be two candidates (i.e., ( 𝑢1,𝑞1𝑡1,𝑖1𝑡1) and
(𝑢1,𝑞1𝑡1,𝑖1𝑡2)). Assuming 𝑞1𝑡1=𝑖1𝑡1, we then establish a term-level
hyperedge ( 𝑢1,𝑞1𝑡1,𝑖1𝑡1) as in Figure 2(b).
So far, we have constructed a term-level hypergraph G=(V,E)
including: (1) a node set Vwhose element represents 𝑢,𝑞𝑡,𝑖𝑡;
and (2) a hyperedge set Ewhose element represents a ternary
relation of𝑢-𝑞𝑡-𝑖𝑡depicting how a user interacts with item via the
term𝑡. Obviously,Gdescribes what terms users prefer to use to
interact with an item, which is exactly the higher-order term-item
dependencies of USP. We apply the above term-level decomposition
process to four user search hypergraphs 𝐺𝑝𝑢𝑟,𝐺𝑐𝑙𝑘,𝐺𝑒𝑥𝑝𝑜, and𝐺𝑟𝑒𝑙,
obtaining term-level hypergraphs G𝑝𝑢𝑟,G𝑐𝑙𝑘,G𝑒𝑥𝑝𝑜, andG𝑟𝑒𝑙.
3.1.2 TermRank. We introduce an efficient TermRank algorithm
to capture the term-item dependencies of USP from every G. First,
we transform the undirected graph Ginto a directed one. As shown
in Figure 2(c), for each hyperedge ( 𝑢,𝑞𝑡,𝑖𝑡) ofG, we create a bidi-
rectional connection between 𝑞𝑡and𝑖𝑡nodes (i.e.,𝑞𝑡↔𝑖𝑡), while
for𝑢and𝑞𝑡nodes we only link a one-way edge (i.e., 𝑢→𝑞𝑡). As-
suming there is a value for each node, we then generate a directed
graph where values flow between nodes following hyperedge 𝑒.
The idea is to obtain node 𝑖𝑡’s value because it describes how much
users prefer to use the term 𝑡to interact with item 𝑖, i.e., term-itemdependencies of USP. The values of 𝑖𝑡and𝑞𝑡are calculated by:
𝑣(𝑖𝑡)=Í
𝑒∈E𝑖𝑡𝑣(𝑞𝑡)·𝑣(𝑢)
Í
𝑡∈𝑖𝑣(𝑖𝑡), (2)
𝑣(𝑞𝑡)=Í
𝑒∈E𝑞𝑡𝑣(𝑖𝑡)·𝛼𝑖Í
𝑡∈𝑞𝑣(𝑞𝑡), (3)
where𝑣(·)is the value function. E𝑖𝑡andE𝑞𝑡denote the set of
hyperedges containing 𝑖𝑡and𝑞𝑡, respectively. Empirically, 𝑣(𝑢)and
𝛼𝑖are the normalized values of the user 𝑢’s search times and item 𝑖
exposure times. The denominator in Eqn. (2) and (3) normalizes the
values of all terms within an item/query for numerical stability.
Eqn. (2) and (3) are an alternate iterative process, which is de-
noted as the TermRank algorithm. However, since Gis a non-
strongly connected graph, when TermRank converges, the value
will accumulate to some hot attribute terms (e.g., brand/category
terms). This is not helpful to prune the postings lists. Therefore,
we set the number of iterations as a hyper-parameter. Finally, we
apply TermRank on four term-level hypergraphs G𝑝𝑢𝑟,G𝑐𝑙𝑘,G𝑒𝑥𝑝𝑜,
G𝑟𝑒𝑙, obtaining term dependencies of user search purchase/click-
/exposure/relevance preferences of each node 𝑖𝑡, i.e.,G𝑝𝑢𝑟_𝑣(𝑖𝑡),
G𝑐𝑙𝑘_𝑣(𝑖𝑡),G𝑒𝑥𝑝𝑜_𝑣(𝑖𝑡),G𝑟𝑒𝑙_𝑣(𝑖𝑡).
3.1.3 Integrating Term Dependencies of USP and Item Quality Factor
Q.The term dependencies of USP show which terms users prefer to
use to interact with items. There is some amount of noise inherent
in user logs [ 22,23], making the user search preference-aware trun-
cated posting lists less effective. To this end, we further include an
item quality factor 𝑄into the final 𝑓(𝑡,𝑖)score function. Although
PageRank is popularly used to measure the quality of web sources,
it is not applied in e-commerce setting for items are considered to be
independent objects. We use the historical trading volume of each
item as a quality factor 𝑄. Intuitively, the more transactions, the
better the item quality. Given a term 𝑡, we construct a user-aware
scoring function 𝑓(𝑡,𝑖), by mixing the term dependencies of USP
and a query-independent item quality factor Q, to truncate its full
posting lists 𝐼𝑡_𝑓to a given length. Mathematically,
𝑡_𝑖𝑈𝑆𝑃=G𝑝𝑢𝑟_𝑣(𝑖𝑡)+G𝑐𝑙𝑘_𝑣(𝑖𝑡)+G𝑒𝑥𝑝𝑜_𝑣(𝑖𝑡)+G𝑟𝑒𝑙_𝑣(𝑖𝑡)(4)
𝑓(𝑡,𝑖)=𝑡_𝑖𝑈𝑆𝑃·𝑖𝑄, (5)
where𝑡_𝑖𝑈𝑆𝑃 and𝑖𝑄denote the mixed USP score between the
term𝑡and the item 𝑖, and the item 𝑖’s quality, respectively. For
convenience, we use IVI-USP-Q to refer the pruned InVerted Index
based on USP and Q.
5342Text Matching Indexers in Taobao Search KDD ’24, August 25–29, 2024, Barcelona, Spain
3.2 Semantic Indexer
In this section, we introduce the semantic dense indexer to reduce
the mismatch between user queries and item. We will present query
tower, item tower and loss function, respectively.
3.2.1 Query Tower. Queries in Taobao product search are mainly
Chinese, which poses two challenge: (1) Because the context of
query and item are different, the Chinese tokenization of the same
text could be different; and (2) user queries are quite short. We train
a two-tower semantic indexer and represent the queries in multiple
semantic granularities to enrich its representations.
Multiple Granularities Layer. Specifically, for a query “ 红色连衣
裙, ” we represent it as uni-gram {𝑐1,...,𝑐𝑚}({红,色,连,衣,裙}), bi-
gram{𝑐1𝑐2,...,𝑐𝑚−1𝑐𝑚}({红色 ,色连 ,连衣 ,衣裙 }), and semantic
word segmentation {𝑡1,...,𝑡𝑛}({红色 ,连衣裙 }) granularities. We
thus obtain four granular representations ℎ𝑞∈R4×𝑑by concate-
nating𝑞’s unigram mean-pooling representation ℎ𝑞1_𝑔𝑟𝑎𝑚∈R1×𝑑,
2-gram mean-pooling representation ℎ𝑞2_𝑔𝑟𝑎𝑚∈R1×𝑑, word seg-
mentation mean-pooling representation ℎ𝑞𝑠𝑒𝑔∈R1×𝑑, and mixed rep-
resentation ℎ𝑞𝑚𝑖𝑥∈R1×𝑑, i.e.,
ℎ𝑞1_𝑔𝑟𝑎𝑚=mean-pooling(ℎ𝑐1,...,ℎ𝑐𝑚), (6)
ℎ𝑞2_𝑔𝑟𝑎𝑚=mean-pooling(ℎ𝑐1𝑐2,...,ℎ𝑐𝑚−1𝑐𝑚), (7)
ℎ𝑞𝑠𝑒𝑔=mean-pooling(ℎ𝑡1,...,ℎ𝑡𝑛), (8)
ℎ𝑞𝑚𝑖𝑥=ℎ𝑞1_𝑔𝑟𝑎𝑚+ℎ𝑞2_𝑔𝑟𝑎𝑚+ℎ𝑞𝑠𝑒𝑔, (9)
ℎ𝑞=concat(ℎ𝑞1_𝑔𝑟𝑎𝑚,ℎ𝑞2_𝑔𝑟𝑎𝑚,ℎ𝑞𝑠𝑒𝑔,ℎ𝑞𝑚𝑖𝑥), (10)
whereℎ,𝑑, mean-pooling, and concat denote the word embedding,
embedding size, average, and vertical concatenation operation.
We feed the abundant semantic representation ℎ𝑞into the en-
coder of a transformer [ 45] to dynamically learn which semantic pat-
tern is useful for matching items, and we regard the [CLS] token’s
representation as the query tower’s representation 𝐻𝑞∈R1×𝑑, i.e.,
𝐻𝑞=Trm_Encoder𝐶𝐿𝑆(ℎ𝑞). (11)
3.2.2 Item Tower. For the item representation 𝐻𝑖𝑡𝑒𝑚, we use its
title segmentation result T={𝑡1,...,𝑡𝑁}, and property set P=
{𝑝1,...,𝑝𝑀}includes category, brand, and so on. We embed Tand
Pinto feature vectors ℎ𝑡∈R1×𝑑andℎ𝑝∈R1×𝑑, and then sum
them to get the final representation 𝐻𝑖𝑡𝑒𝑚∈R1×𝑑. Mathematically,
ℎ𝑡=tanh 
𝑊𝑡·Í𝑁
𝑖=1𝑡𝑖
𝑁!
, (12)
𝑒𝑝𝑖=𝑊𝑝𝑖·𝑝𝑖, (13)
ℎ𝑝=sum-pooling({𝑒𝑝𝑖|𝑝𝑖∈P}), (14)
𝐻𝑖𝑡𝑒𝑚=ℎ𝑡+ℎ𝑝, (15)
where𝑊𝑡and𝑊𝑝𝑖are the transformation matrices. Sum-pooling
and tanh are the average operation and activation function. Ap-
plying a transformer encoder to capture the context of the title is
not as effective as simple mean-pooling since the title is stacked by
keywords and lacks grammatical structure [23].
3.2.3 Loss Function. We adapt the sampled softmax cross-entropy
loss [ 20] as the training objective, achieving faster convergence
and a better performance than other pointwise and pairwise losses.Unlike many toy datasets, in the e-commerce setting, the ground
truth of relevant items is unknown, and there can be too many
relevant products for a given query. For example, a query “ 红色
连衣裙 (red dress)” has millions of relevant products. Note that
our neural index is used for retrieving items of both relevance and
user-preference to reduce information losses of truncation.
We use user feedback and a well-trained “query-item” relevance
model [ 46] to select items for training. Specifically, given a query 𝑞,
its positive samples 𝑖+are those that have been clicked in the past
and are rated as relevant by the relevance model. Following [ 23],
its negative samples 𝑖−(𝑖−𝑟∪𝑖−
ℎ) include: (1) random negative
samples𝑖−𝑟generated by sampling from the entire item pool 𝐼;
and (2) hard negative samples 𝑖−
ℎgenerated by linear interpolation
between𝑖+and top-𝑘cosine similarity scores with 𝑞of𝑖−𝑟’s repre-
sentations. By examining the user logs, we obtains a train data set
D={(𝑞1,𝑖+
1,𝑖−
1),...,(𝑞𝑁,𝑖+
𝑁,𝑖−
𝑁)}. The loss is defined by:
𝐿(∇)=−1
𝑁∑︁
𝑞𝑗∈Dlogexp(cos(𝑞𝑗,𝑖+
𝑗)/𝜏)
Í
𝑖∈𝑖−
𝑗exp(F(𝑞𝑗,𝑖)/𝜏), (16)
where𝑖and𝑞denote the item tower’s representation 𝐻𝑖𝑡𝑒𝑚 and the
query tower’s representation 𝐻𝑞, respectively; 𝜏is a temperature
parameter we set to 2. Note that the negative samples 𝑖−are gen-
erated during training. Similar to [ 23,49], we use the same set of
random negative samples for every 𝑞in the current batch to reduce
computing resources. For convenience, we refer to our Multiple
Granularities Semantic Indexer as MGSI.
3.3 System Deployment
unique product setUser Search Preferences-aware Pruned Inverted IndexPersonal RetrievalSemantic Forward IndexUser QueryIndexingHybrid Retrieval SystemPre-RankingRelevance&Personal RankingReRankingRanking System
SMINDReturn
Rewritten Queries
Figure 3: Overview of the architecture of the Taobao search
engine with the proposed indexing system SMIND.
As shown in Figure 3, we deploy the proposed indexing system
SMIND to the retrieval system of the Taobao search engine. When a
user initiates a query, CLE-QR [ 22] will first rewrite it. We then use
a hybrid retrieval system ( SMIND andPersonal Retrieval [7,23])
to return a unique candidate set I, which is then ranked by a large-
scale deep-learning based ranking system. Finally, we expose the
top-𝑘products to users. See Section B of the Appendix for more
details of deployment.
5343KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
4 EXPERIMENTS
We introduce evaluation metrics, datasets, compared methods, im-
plementation details, and offline and online quantitative and quali-
tative analyses of our method.
4.1 Evaluation Metrics
For offline evaluation, following [ 23], we use the metrics of Recall@ 𝐾
and𝑃𝑔𝑜𝑜𝑑, which are defined as
𝑅𝑒𝑐𝑎𝑙𝑙 @𝐾=Í𝐾
𝑖=1𝑖𝑖∈𝑇
𝑁, (17)
𝑃𝑔𝑜𝑜𝑑=Í𝐾
𝑖=1I(𝑖𝑖)
𝐾, (18)
where𝑇={𝑡1,...,𝑡𝑁}is the items clicked or purchased by the
user𝑢and𝐼={𝑖1,...,𝑖𝐾}is the top-𝑘items returned by an indexer.
I(·)is an indicator function. When item 𝑖is rated as good by our
well-trained “query-item” relevance model (with an AUC of 0.92),
its value is 1, otherwise 0.
For online evaluation, we consider the two most valuable com-
merce metrics, GMV (Gross Merchandise Volume) and POC (Pay
Order Count). Additionally, we report four auxiliary system met-
rics, namely 𝑵 𝒖𝒎 𝒓𝒂𝒏𝒌 ,𝑹𝒆𝒄𝒂𝒍𝒍 𝒄𝒍𝒌,𝑹𝒆𝒄𝒂𝒍𝒍 𝒑𝒖𝒓, and 𝑷𝒈𝒐𝒐𝒅 . See Sec-
tion C of the Appendix for more details. Note that we also perform
a Wilcoxon signed-rank test [ 10] to measure the significance of the
difference.†and‡indicate that the improvement is statistically
significant with 𝑝<0.05and𝑝<0.01, respectively.
4.2 Datasets
4.2.1 Large-scale Industrial Offline DataSet. For offline evaluation,
we randomly sample 1.5million search interaction records in the
next day. Following the online system’s setting (Section 4.2.2), the
size of the candidate item set of inverted index is about 2billion
and the semantic index is about 100million.
4.2.2 Online Dataset. We deploy a well-built SMIND in the Taobao
search production environment to serve hundreds of millions of
user query requests (Section 3.3). The size of the item candidate
set is about 2billion. Since the online inference of semantic index
needs more computational resources, we have to reduce its item
pool to the most active products, resulting in 100million.
4.3 Compared Methods
4.3.1 Offline Comparisons. Due to the differences in the item pools
of the inverted indexer and the semantic indexer, we report the
metrics of the two indexes separately.
Inverted Indexer. For the InVerted Indexer, following [ 43], we re-
port some strong baselines of static term-based pruning techniques,
such as TF-based, TF-IDF based, and item quality Q-based. We also
adapt DeepCT [ 8], SPARTA [ 50], and SPLADE [ 15,16] for a fair
comparison. Note that BM25 is an “query-item” score function and
is not applicable in a static pruning setting where only single term
posting lists can be accessed offline. The details are as follow:
IVI-TF The truncated posting list of each term is based on the
scoring function 𝑓(𝑡,𝑖), which is instantiated as term fre-
quency (TF). Given a (term, item) pair, we get the TF valueby counting the number of times the term 𝑡appears in the
item title.
IVI-TF-IDF The scoring function 𝑓(𝑡,𝑖)is instantiated as classical
TF-IDF value.
IVI-Q For each term, we truncate its full item lists to a given length
according to the item quality Q value.
IVI-DeepCT The value of the scoring function 𝑓(𝑡,𝑖)is obtained
from DeepCT [8].
IVI-SPARTA 𝑓(𝑡,𝑖)’s value is obtained from SPARTA [50].
IVI-SPLADE 𝑓(𝑡,𝑖)’s value is obtained from SPLADE [15, 16].
IVI-USP The scoring function 𝑓(𝑡,𝑖)is instantiated by the Eq. (4),
which considers the user search preferences.
IVI-USP-Q The proposed scoring function 𝑓(𝑡,𝑖)(Eq. (5)), consid-
ering both user search preferences and item quality 𝑄.
Semantic Indexer. For the Semantic Indexer, our contribution is
not to design any new model, but to combine query rewriting (QR)
to facilitate its semantic matching, by addressing the mismatch
problem mentioned earlier. Hence, we focus on the comparisons
between MGSI and its combination with QR. Note that MGSI can
be replaced by any semantic indexer mentioned in Section 2.2. The
details are as follows:
MGSI The proposed Multiple Granularities Semantic Indexer.
MGSI-QR In addition to triggering the MGSI with a user’s original
queries, we use the rewritten queries of CLE-QR [22].
4.3.2 Online Comparisons. Our search engine is a complicated
system since it has been tuned by hundreds of engineers and scien-
tists for over ten years. We briefly introduce the our experimental
environment for the readers’ interest. For text matching, we have al-
ready deployed query/product tagging matching methods based on
manual efforts and knowledge graphs [ 27] and the query rewriting
technique CLE-QR [ 22]. For personal retrieval, we have graph-based
methods [ 7,13,33] and an embedding-based method [ 23] online.
We cannot simply remove them from the online system for com-
parisons. Hence, we only replace the indexer of text matching with
SMIND . Since SMIND consists of IVI-USP-Q and MGSI, we report on
two ablations, as follows:
SMIND The proposed hybrid indexer solution. It is used with both
the user’s original queries and rewrites of CLE-QR.
SMIND w/o IVIUSPQ We remove the user search preferences-aware
pruned inverted indexer from SMIND.
SMIND w/o MGSI We remove the semantic indexer from SMIND.
4.4 Implementation Details
We collect search logs from the online Mobile Taobao App to build
SMIND . Specifically, for the user search preferences-aware pruned
inverted indexer, the purchase hypergraph G𝑝𝑢𝑟is built from one
month of(𝑢,𝑞,𝑖)purchase interactions. The click hypergraph G𝑐𝑙𝑘
contains two weeks of (𝑢,𝑞,𝑖)click interactions, about 10billion
records. The exposure hypergraph G𝑒𝑥𝑝𝑜 and relevance hypergraph
G𝑟𝑒𝑙contain one week(𝑢,𝑞,𝑖)exposure and relevance interactions,
about 350billion records. The TermRank algorithm is implemented
on our large-scale distributed system called Open Data Process-
ing Service (ODPS).1Its number of iterations is set to 1by cross-
validation. The execution sequence of TermRank is: (1) we first
1https://www.aliyun.com/product/odps
5344Text Matching Indexers in Taobao Search KDD ’24, August 25–29, 2024, Barcelona, Spain
initialize all 𝑣(𝑖𝑡)to1, and run Eq. (3); and (2) we then run Eq. (2)
to get the final value 𝑣(𝑖𝑡). The runtime ofG𝑝𝑢𝑟,G𝑐𝑙𝑘,G𝑒𝑥𝑝𝑜, and
G𝑟𝑒𝑙is about 1hour, 2h,6h and 6h, respectively. Once we finish
building the term-level search hypergraphs, users and items are
static. To capture dynamic changes of users and products, we re-run
TermRank periodically. Since our item set varies between days, we
do it every day.
For training the semantic indexer, we use distinct query click-
s/purchases of 7consecutive days, a total of 4.7billion records. The
dimensions of the query tower and item tower are all set to 128. The
batch size is 256. The setup of transformer encoder is 2layers and
8self-attention heads. All parameters are orthogonally initialized
and learned from scratch. The training processes are run on the
distributed TensorFlow platform [ 1] using 20parameter servers and
100GPU (Tesla P100) workers. We use the AdaGrad optimizer [ 12]
with an initial learning rate of 0.1, since it improves the robustness
of SGD for training large-scale networks [ 9]. We clip the gradient
norm less than 3. The running time is about 48h.
4.5 Offline Experimental Results
4.5.1 Comparison of Pruned Inverted Indexer. As mentioned in Sec-
tion 4.1, we report the metrics of 𝑅𝑒𝑐𝑎𝑙𝑙 @𝐾and𝑃𝑔𝑜𝑜𝑑 on1.5million
search interaction records. We set 𝐾to1,000.2As shown in Table 1,
the performance of IVI-TF and IVI-TF-IDF are the worst in terms of
the lowest metrics of 𝑅𝑒𝑐𝑎𝑙𝑙 @1000 and𝑃𝑔𝑜𝑜𝑑, which verifies that
the search engine optimization (SEO) behavior of merchants make
the TF and TF-IDF less effective.
Table 1: Offline performance of term-based pruned inverted
indexers and ablation models.
Methods 𝑅𝑒𝑐𝑎𝑙𝑙 @1000𝑃𝑔𝑜𝑜𝑑
IVI-TF 0.8076 0.8739
IVI-TF-IDF 0.8099 0.8754
IVI-DeepCT [8] 0.8201 0.8766
IVI-SPARTA [50] 0.8199 0.8768
IVI-SPLADE [16] 0.8244 0.8773
IVI-SPLADEv2 [15] 0.8316 0.8790
IVI-Q 0.8192 0.8689
IVI-USP 0.8376 0.8812
IVI-USP-Q 0.8510‡ 0.8870‡
We also use the value learnt by sparse retrieval models for prun-
ing. Specifically, DeepCT [ 8], SPARTA [ 50] and SPLADE [ 15,16] all
outperform TF and TF-IDF, indicating that they do provide stronger
term dependencies of textual relevance signal than TF and TF-IDF
and mitigate the effect of SEO. The 𝑅𝑒𝑐𝑎𝑙𝑙 @1000 score of IVI-USP
is higher than that of the learned sparse models, which means that
USP is more effective at capturing term dependencies than textual
relevance and thus retrieve more user-preferred products.
However, learned sparse models frequently assign “wacky” weights
to terms [ 28,29,31]. Especially in our case, since the user search
logs are noisy (users may click/purchase items irrelevant to the
query) [ 22,23], it is hard to preserve high precision/relevance,
2The Top-1000 items are obtained by our ranking system.which leads to a lower value of 𝑃𝑔𝑜𝑜𝑑 than the proposed TermRank
method. Finally, the proposed IVI-USP-Q outperforms all other
methods in terms of 𝑅𝑒𝑐𝑎𝑙𝑙 @1000 and𝑃𝑔𝑜𝑜𝑑, indicating that consid-
ering both USP and the item quality factor 𝑄helps to return more
relevant items and better satisfy users.
4.5.2 Comparison of Semantic Indexer. As discussed in Section 4.3.1,
we here focus on the comparisons between MGSI and its com-
bination with query rewriting (QR). As shown in Table 2, when
combining the rewritten queries of QR with the semantic indexer,
MGSI-QR achieves higher 𝑅𝑒𝑐𝑎𝑙𝑙 @1000 scores, indicating that the
retrieval performance of QR and semantic index can be mutually
improved, by better addressing the vocabulary mismatch. The rele-
vant retrieval ability of MGSI-QR seems to be weaker in terms of
the𝑃𝑔𝑜𝑜𝑑 metric, as it is reduced by 0.85%. However, it does retrieve
more user-preferred items than MGSI.
Table 2: Offline performance comparison between MGSI and
MGSI-QR.
Methods 𝑅𝑒𝑐𝑎𝑙𝑙 @1000𝑃𝑔𝑜𝑜𝑑
MGSI 0.7876† 0.7939‡
MGSI-QR 0.8219† 0.7854‡
4.5.3 Qualitative Results of IVI-USP-Q. Table 3 shows the top 7
item categories and the corresponding number of products retained
in the truncated posting list of the term “red” by the methods IVI-
Q and IVI-USP-Q. With the fixed truncated length, 4of the top
7categories are consistent (i.e., Dress, Wool sweater, Lady bags
and T-shirt), which shows that the item quality factor 𝑄reflects
the user search preference to a certain extent. However, due to
different category preferences, the number of items in the four
same categories is inconsistent. IVI-USP-Q considers both USP
and𝑄, which makes the truncated item set closer to the general
user search intention. Specifically, in the common categories, the
number of items of “dress” and “lady bags” have increased by 141%
and 67.7%, and these items could previously not be retained in
“red”’s pruned posting list of IVI-Q due to a low 𝑄value.
Table 3: With different term-based pruning methods, the
top7item categories and their numbers within the truncated
posting list of the term “红色 (red).”
term “红色(red)"
IVI-Q IVI-USP-Q
Category Num Category Num
毛针织衫 (Wool sweater) 2,776 Dress 6,140
连衣裙 (Dress) 2,552 Wool sweater 2,675
T恤(T-shit) 2,465 Lady bags 2,231
运动鞋 (Sneaker) 1,575 T-shirt 2,229
衬衫 (Shirt) 1,440单鞋 (Shoes) 2,208
女包 (Lady bags) 1,330 文胸 (Bras) 1,580
裤子 (Pants) 1,165礼服 (Canonicals) 1,477
In the categories with differences, the number of items in the
categories of “Shoes,” “Bras,” and “Canonicals” has increased sig-
nificantly, and they have entered the top 7of IVI-USP-Q from the
5345KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
24-th, 28-th and 53-rd position of IVI-Q. The changes in these item
categories and numbers are in line with our intuition, since when
a user searches with term “red,” he/she expects to see dress, lady
bags, shoes, bras, and canonicals more.3
4.5.4 Case Study of Term Dependencies of USP. We use a small
number of case studies to examine the term dependencies of USP.
We randomly sampled 2items of dress, and checked the value of
each term𝑣(𝑖𝑡)(i.e., Eq. (2)) in their title from the term-level user
click hypergraphsG𝑐𝑙𝑘. A bigger value of 𝑣(𝑖𝑡)indicates that users
more strongly prefer to use the term 𝑡to interact with item 𝑖.
Chinese: ports(0.05) 宝姿(0.60) 裙⼦(0.01) 秋季(0.01) 时尚(0.00) ⽓质(0.06) 通勤(0.00) 修身(0.04) 收腰(0.02) 连⾐裙(0.21)English: ports(0.05) 宝姿(0.60) skirt(0.01) autumn(0.01) fashion(0.00) temperament(0.06) commute(0.00) ﬁt(0.04) waist(0.02) dress (0.21)                      Item Title 
Chinese: self-portrait(0.21) ⽔溶(0.09) 蕾丝(0.17) 连⾐裙(0.49) 夏季(0.01) 薄款(0.00) 紧身(0.01) 泡泡袖(0.02) 中⻓裙(0.01)English: self-portrait(0.21) water soluble(0.09) lace(0.17) dress(0.49) summer(0.01) thin(0.00) tight(0.01) puﬀ sleeves(0.02) midi skirt(0.01)Item1 
Item2 
Figure 4: Case study of term dependencies of USP. Values in
parentheses are calculated by TermRank.
As shown in Figure 4, although merchants perform SEO by stack-
ing many textually relevant words, only a few keywords of each
product will be searched by users. For item 1, the most important
keywords are “ 宝姿” and “连衣裙 (dress)” since their values are
bigger than others. Interestingly, item 1has a shared Chinese and
English keyword of the brand “PORTS,” but most users tend to
search it in Chinese. For item 2, its style is a puff sleeve lace dress,
but the value of “puff sleeves” (0 .02)≪“dress” (0.49). On the one
hand, from the perspective of product textual relevance, “ 泡泡
袖(puff sleeves)” and “ 连衣裙 (dress)” are equally important to
item 2. On the other hand, from the perspective of users, they rarely
initiate the term “puff sleeve” to search for item 2. Moreover, the
term “薄款 (thin)” is a textually relevant keyword of item 2since it is
item 2’s attribute. However, its value is 0, indicating that users have
never interacted with item 2using the term “thin.” Instead, users
use term “ 蕾丝(lace)” more. The above examples show that the
higher-order term dependencies of USP are crucial in e-commerce
product search.
4.5.5 Dependencies of USP on Various Term-Level Hypergraphs.
To further study the high-order term dependencies of user prefer-
ences, we show the value of each term in the title ( 𝑣(𝑖𝑡)) obtained
3The query “red” represents joy and happiness in Chinese customs. For example, at
Chinese weddings, people wear red canonicals. Also, people give out red envelopes
during Chinese New Year. Hence, people get used to associate the appearance of objects
with the “red. ” More important, most of our users are women. Therefore, for the query
“red,” products of dresses, lady bags, and shoes should be more hits.
Item Title Term self-portrait⽔溶(water soluble)蕾丝(lace)连⾐裙(dress)夏季(summer)薄款(thin)紧身(tight)泡泡袖(puﬀ sleeves)中⻓裙(midi skirt)0.210.090.170.490.010.000.010.020.010.120.030.090.690.030.010.010.010.010.440.000.220.340.000.000.000.000.000.100.080.070.380.040.090.060.080.10
Figure 5: Comparison of term dependencies of USP on term-
level hypergraphs. Values in the last four columns are calcu-
lated by TermRank.
from the term-level user search purchase/click/exposure/relevance
preferences hypergraphs, G𝑝𝑢𝑟,G𝑐𝑙𝑘,G𝑒𝑥𝑝𝑜, andG𝑟𝑒𝑙, respectively.
As shown in Figure 5, given the title of a product, from user
search relevance→exposure→click→purchase preferences, the im-
portance of the title terms goes from divergence to convergence
to a few specific terms. Specifically, in the user search relevance
preferences hypergraph G𝑟𝑒𝑙, each term has a value indicating that
each term in the title is relevant. In the exposure hypergraph G𝑒𝑥𝑝𝑜,
the values of terms like “ 薄款 (thin)” and “ 紧身 (tight),” start to de-
crease and others are increasing. In the click hypergraph G𝑐𝑙𝑘, this
phenomenon is more apparent. Finally, in the purchase hypergraph
G𝑝𝑢𝑟, “self-portrait,” “ 蕾丝(lace),” and “ 连衣裙 (dress)” become
key terms, while the others are vanishing. This means that when
users purchase this product, they will only use the combination of
terms “self-portrait,” “ 蕾丝 (lace),” and “ 连衣裙 (dress)” to search.
A comparison of the purchase hypergraph G𝑝𝑢𝑟and the relevance
hypergraphG𝑟𝑒𝑙clearly tells us that the term dependencies in user
search preference are far more important than textual relevance in
product search.
4.6 Online Experimental Results
Beyond the offline experiments, we further conduct online evalua-
tion to verify the effectiveness of SMIND.
4.6.1 Ablation Study. As mentioned in Section 4.1, for online per-
formance analyses, we report the number of products considered in
the ranking stage (i.e., 𝑁𝑢𝑚𝑟𝑎𝑛𝑘), the recall metric of click/purchase
signal on the displayed item set 𝐼(i.e.,𝑅𝑒𝑐𝑎𝑙𝑙𝑐𝑙𝑘and𝑅𝑒𝑐𝑎𝑙𝑙𝑝𝑢𝑟), and
the proportion of products with good relevance on 𝐼(i.e.,𝑃𝑔𝑜𝑜𝑑).
In addition to SMIND ’s two ablation models ( SMIND w/o IVIUSPQ
andSMIND w/o MGSI discussed in Section 4.3.2, we also include the
performance of the previous online baseline for comparison. Due
to commercial confidentiality, we report relative values for some
metrics.
Table 4: Online analyses of SMIND and its ablation models.
The last three columns report relative values.
Metho
ds 𝑁𝑢𝑚𝑟𝑎𝑛𝑘𝑅𝑒𝑐𝑎𝑙𝑙𝑐𝑙𝑘𝑅𝑒𝑐𝑎𝑙𝑙𝑝𝑢𝑟𝑃𝑔𝑜𝑜𝑑
Online
baseline 4351 – – –
SMIND w/o
IVIUSPQ 4480+2.68%+1.32%−0.45%
SMIND w/o MGSI 4552+4.24%+1.74% +1.62%
SMIND 4651 +5.45%‡ +2.81%‡ +1 .25%‡
5346Text Matching Indexers in Taobao Search KDD ’24, August 25–29, 2024, Barcelona, Spain
As shown in Table 4, the two ablation modules can retrieve
more item candidates that enter the ranking stage than the online
baseline (higher 𝑁𝑢𝑚𝑟𝑎𝑛𝑘), indicating the effectiveness of the pro-
posed indexer, IVI-USP-Q and MGSI, again. Specifically, MGSI (i.e.,
SMIND w/o IVIUSPQ) increases 𝑁𝑢𝑚𝑟𝑎𝑛𝑘 by3%and reduces 𝑃𝑔𝑜𝑜𝑑
by0.45%, showing that it improves the volume of user-preferred
items as well as introducing some less relevant results. This is
mainly because the personal ranking model scores “user-query-
item” while the relevance model scores “query-item.” There is a
discrepancy between the two metrics. However, MGSI does improve
the𝑅𝑒𝑐𝑎𝑙𝑙𝑐𝑙𝑘and𝑅𝑒𝑐𝑎𝑙𝑙𝑝𝑢𝑟by2.68%and1.32%, which demonstrates
the dilemma of user preferences and uniform relevance criteria in
e-commerce search. As for IVI-USP-Q (i.e., SMIND w/o MGSI), it
improves all metrics, indicating that it retrieves more relevant and
user-preferred products. Finally, since the hybrid indexer SMIND
achieves the largest improvements, we conclude that its two ab-
lation indexers complement each other and SMIND serves online
users better.
Table 5: Online A/B test of SMIND on Mobile Taobao Search.
The improvements are averaged over 18days.
GMV POC CTR CVR
SMIND+1.50%‡+1.34%‡+0.02𝑝𝑡‡+0.02𝑝𝑡‡
4.6.2 Online A/B Tests. Finally, we report the 18-day average of
GMV and POC improvements (removing cheating traffic by the
business analyst) achieved by SMIND . We include two additional
metrics, click-through rate (CTR) and conversion rate (CVR).
Table 5 shows that SMIND improves GMV and POC by 1.50%
and1.34%, respectively, indicating that the daily increase income
is hundreds millions. CTR and CVR also increase by 0.02pt and
0.02pt, respectively, which demonstrates that SMIND better satisfies
users. We also deployed an unpruned inverted index for comparison,
which can bring a marginal increase of 0.2%in GMV and POC at
the cost of 50%increase in computing resources.
4.6.3 User Query Shift Phenomenon. Because SMIND achieves sig-
nificant improvements, we want to investigate its potential impact
on users by examining users’ search behavior versus their behavior
with the previous online system. We first sort all queries in a de-
scending order of their number of products recalled by the previous
online system, and categorize them into 7intervals, i.e., the smaller
the number, the more recalled products. We refer to queries in the
interval of No. 1–3 as top queries, those in the interval of No. 4, 5
as torso queries, and those in the interval of No. 6, 7 as tail queries.
With the same online traffic volume setting, we then count the
search frequency (SF) and item clicks (IC) in each interval.
Interestingly, as shown in Table 6, we find there is a user query
shift phenomenon. Overall, since SMIND and the previous system
have the same online traffic volume, their total SF are equal. But
SMIND increases IC by 0.64%, indicating that its retrieved product
collection better satisfies users, so that they are willing to click
more. Let us take a closer look. The SF and IC ratio of the tail
queries (No. 6–7) and the torso queries (No. 4–5) have increased
significantly, while the top query (No. 1–3) decreased significantly.
As the overall SF ratio of SMIND and previous system are equal, thisTable 6: The user search behavior with SMIND versus previous
online system.
Query Type No. Search Frequency (SF) Item Clicks (IC)
Overall – 0.00%+0.64%
Top1−2.76%−2.18%
2−1.84%−1.37%
3−0.96%+0.01%
Torso4+1.69%+2.89%
5+1.42%+1.87%
Tail6+1.36%+1.33%
7+1.62%+1.11%
indicates that there is a user query shift phenomenon. Specifically,
in the previous retrieval system, when users initiate the torso/tail
queries and fail to see the desired product set, they rewrite their
own queries to top queries for viewing more products. However, on
the test traffic, SMIND improves the matching ability of the previous
indexing system, so that torso and tail queries can return more
products that users are willing to click/purchase, which prevents
users from rewriting queries to top queries and thus alleviating the
Matthew effect [36] of user queries.
5 CONCLUSION
This paper presents an efficient solution for indexing billions of
products, called SMart INDexing ( SMIND ), in Taobao search. SMIND
addresses the challenge of reducing information loss during the
static pruning of inverted indexes by integrating user search prefer-
ences (as discussed in Section 4.5.3) and mitigating the vocabulary
mismatch problem inherent in term-based models (as outlined in
Sections 4.5.2 and 4.6.1) that plagued the previous indexing system.
Our research demonstrates that understanding the term depen-
dencies among users’ search preferences is of paramount impor-
tance, surpassing textual relevance (as discussed in Sections 4.5.4,
and 4.5.5). Meanwhile, we share the lessons learned from improv-
ing the performance of text matching, including the design of a
pruning inverted indexer, a semantic indexer, its combination with
a query rewriting technique, its effect on a large-scale commerce
search system (Section 4.6.3), and its deployment scheme. Offline
experiments and online A/B tests verify the effectiveness of SMIND .
We have successfully deployed the proposed system on Taobao
Product Search, providing real-time services to hundreds of mil-
lions of users. In the future, we will incorporate term dependencies
in user search preferences into the ranking stage to improve search
personalization. Another aspect to investigate is the choice of se-
mantic indexers. In particular, our semantic indexer was trained
using only text corpora, whereas we also have rich image data.
Therefore, we believe that building a multi-modal semantic indexer
can further improve the performance of our system.
ACKNOWLEDGMENTS
The authors thank Prof. Qianli Ma and Ph.D. Qingwen Liu for their
helpful discussions and comments on early drafts. We would also
like to thank the anonymous reviewers for their helpful feedbacks.
5347KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
REFERENCES
[1]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al .
2016. Tensorflow: A System for Large-scale Machine Learning. In 12th OSDI.
265–283.
[2]Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertex-
tual Web Search Engine. Computer Networks 30 (1998), 107–117.
[3]Andrei Z. Broder, David Carmel, Michael Herscovici, Aya Soffer, and Jason Zien.
2003. Efficient Query Evaluation Using a Two-level Retrieval Process. In 12th
CIKM. 426–434.
[4]David Carmel, Doron Cohen, Ronald Fagin, Eitan Farchi, Michael Herscovici,
Yoelle S Maarek, and Aya Soffer. 2001. Static Index Pruning for Information
Retrieval Systems. In 24th SIGIR. 43–50.
[5]Wei-Cheng Chang, Daniel Jiang, Hsiang-Fu Yu, Choon Hui Teo, Jiong Zhang, Kai
Zhong, Kedarnath Kolluri, Qie Hu, Nikhil Shandilya, Vyacheslav Ievgrafov, et al .
2021. Extreme Multi-label Learning for Semantic Matching in Product Search. In
27th SIGKDD. 2643–2651.
[6]Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan
Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2022. Salient Phrase
Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. In Findings
of the Association for Computational Linguistics: EMNLP 2022. 250–262.
[7]Dian Cheng, Jiawei Chen, Wenjun Peng, Wenqin Ye, Fuyu Lv, Tao Zhuang, Xiaoyi
Zeng, and Xiangnan He. 2022. IHGNN: Interactive Hypergraph Neural Network
for Personalized Product Search. In 31st WWW. 256265.
[8]Zhuyun Dai and Jamie Callan. 2020. Context-Aware Term Weighting For First
Stage Passage Retrieval. In 43rd SIGIR. 1533–1536.
[9]Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V.
Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang,
and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In 25th NeurIPS.
1223–1231.
[10] Janez Demsar. 2006. Statistical Comparisons of Classifiers over Multiple Data
Sets. JMLR 7 (2006), 1–30.
[11] Shuai Ding and Torsten Suel. 2011. Faster Top-k Document Retrieval Using
Block-max Indexes. In 34th SIGIR. 993–1002.
[12] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods
for Online Learning and Stochastic Optimization. JMLR 12, 7 (2011), 2121–2159.
[13] Lu Fan, Qimai Li, Bo Liu, Xiao-Ming Wu, Xiaotong Zhang, Fuyu Lv, Guli Lin,
Sen Li, Taiwei Jin, and Keping Yang. 2022. Modeling User Behavior with Graph
Convolution for personalized product search. In 31st WWW. 203–212.
[14] Marcus Fontoura, Vanja Josifovski, Jinhui Liu, Srihari Venkatesan, Xiangfei Zhu,
and Jason Zien. 2011. Evaluation Strategies for Top-k Queries over Memory-
resident Inverted Indexes. Proceedings of the VLDB Endowment 4, 12 (2011),
1213–1224.
[15] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant.
2022. From Distillation to Hard Negative Sampling: Making Sparse Neural IR
Models More Effective. In 45th SIGIR. 2353–2359.
[16] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE:
Sparse Lexical and Expansion Model for First Stage Ranking. In 44th SIGIR. 2288–
2292.
[17] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng.
2022. Semantic Models for the First-stage Retrieval: A Comprehensive Review.
ACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1–42.
[18] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,
Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-
based Retrieval in Facebook Search. In 26th SIGKDD. 2553–2561.
[19] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck.
2013. Learning Deep Structured Semantic Models for Web Search Using Click
through Data. In 22nd CIKM. 2333–2338.
[20] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014.
On Using Very Large Target Vocabulary for Neural Machine Translation. arXiv
preprint arXiv:1412.2007 (2014).
[21] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale Similarity
Search with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535–547.
[22] Sen Li, Fuyu Lv, Taiwei Jin, Guiyang Li, Yukun Zheng, Tao Zhuang, Qingwen
Liu, Xiaoyi Zeng, James Kwok, and Qianli Ma. 2022. Query Rewriting in TaoBao
Search. In 31st CIKM. 3262–3271.
[23] Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng, Xiao-Ming Wu,
and Qianli Ma. 2021. Embedding-Based Product Retrieval in Taobao Search. In
27th SIGKDD. 3181—3189.
[24] Yiding Liu, Weixue Lu, Suqi Cheng, Daiting Shi, Shuaiqiang Wang, Zhicong
Cheng, and Dawei Yin. 2021. Pre-trained Language Model for Web-scale Retrieval
in Baidu Search. In 27th SIGKDD. 3365–3375.
[25] Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui,
Xiaoyi Liu, and Fedor Borisyuk. 2021. Que2Search: Fast and Accurate Query and
Document Understanding for Search at Facebook. In 27th SIGKDD. 3376–3384.
[26] Xiaohui Long and Torsten Suel. 2003. Optimized Query Execution in Large
Search Engines with Global Page Ordering. In Proceedings 2003 VLDB Conference.Elsevier, 129–140.
[27] Xusheng Luo, Le Bo, Jinhang Wu, Lin Li, Zhiy Luo, Yonghua Yang, and Keping
Yang. 2021. AliCoCo2: Commonsense Knowledge Extraction, Representation
and Application in E-commerce. In 27th SIGKDD. 3385–3393.
[28] Joel Mackenzie, Andrew Trotman, and Jimmy Lin. 2021. Wacky Weights in
Learned Sparse Representations and the Revenge of Score-at-a-Time Query
Evaluation. arXiv preprint arXiv:2110.11540 (2021).
[29] Joel Mackenzie, Andrew Trotman, and Jimmy Lin. 2023. Efficient Document-at-a-
Time and Score-at-a-Time Query Evaluation for Learned Sparse Representations.
ACM Transactions on Information Systems 41, 4 (2023).
[30] Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, Praveen
Reddy Suram, Ajit Puthenputhussery, Sijie Chen, Min Xie, Anirudh Kashi, Tony
Lee, et al. 2022. Semantic Retrieval at Walmart. In 28th SIGKDD. 3495–3503.
[31] Antonio Mallia, Omar Khattab, Torsten Suel, and Nicola Tonellotto. 2021. Learn-
ing Passage Impacts for Inverted Indexes. In 44th SIGIR. 17231727.
[32] Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian Ding, Ankit
Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin. 2019. Semantic Product Search.
In25th SIGKDD. 2876–2885.
[33] Xichuan Niu, Bofang Li, Chenliang Li, Rong Xiao, Haochuan Sun, Hongbo Deng,
and Zhenzhong Chen. 2020. A Dual Heterogeneous Graph Attention Network to
Improve Long-Tail Performance for Shop Search in E-Commerce. In 26th SIGKDD.
3405–3415.
[34] Alexandros Ntoulas and Junghoo Cho. 2007. Pruning Policies for Two-Tiered
Inverted Index with Correctness Guarantee. In 30th SIGIR. 191–198.
[35] Debmalya Panigrahi and Sreenivas Gollapudi. 2013. Document Selection for
Tiered Indexing in Commerce Search. In 6th WSDM. 73–82.
[36] Matjaz Perc. 2014. The Matthew Effect in Empirical Data. Journal of the Royal
Society Interface 11, 98 (2014), 20140378–20140378.
[37] Joseph Rocchio. 1971. Relevance Feedback in Information Retrieval. In The Smart
Retrieval System-experiments in Automatic Document Processing. 313–323.
[38] Cristian Rossi, Edleno S. de Moura, Andre L. Carvalho, and Altigran S. da Silva.
2013. Fast Document-at-a-time Query Processing Using Two-tier Indexes. In
36th SIGIR. 183–192.
[39] Hinrich Schütze, Christopher D. Manning, and Prabhakar Raghavan. 2008. Intro-
duction to Information Retrieval. Vol. 39. Cambridge University Press.
[40] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014.
Learning Semantic Representations Using Convolutional Neural Networks for
Web Search. In 23rd WWW. 373–374.
[41] Trevor Strohman, Howard Turtle, and W. Bruce Croft. 2005. Optimization Strate-
gies for Complex Queries. In 28th SIGIR. 219–225.
[42] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin
Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. Ernie: Enhanced Representa-
tion Through Knowledge Integration. arXiv preprint arXiv:1904.09223 (2019).
[43] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna
Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of
Information Retrieval Models. In Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track.
[44] Howard Turtle and James Flood. 1995. Query Evaluation: Strategies and Opti-
mizations. Information Processing & Management 31, 6 (1995), 831–850.
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need. In 31st NeurIPS, Vol. 30.
[46] Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo Deng, and
Xiaojun Wan. 2021. Learning a Product Relevance Model from Click-Through
Data in E-Commerce. In 30th WWW. 2890–2899.
[47] Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L Berg,
and Ning Zhang. [n. d.]. CommerceMM: Large-scale Commerce Multimodal
Representation Learning with Omni Retrieval. In 28th SIGKDD, pages=4433–4442,
year=2022.
[48] Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong
Xu, Yun Xiao, Bo Long, and Wen-Yun Yang. 2021. Joint Learning of Deep Retrieval
Model and Product Quantization based Embedding Index. In 44th SIGIR. ACM,
17181722.
[49] Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao,
Weipeng Yan, and Wen-Yun Yang. 2020. Towards Personalized and Semantic Re-
trieval: An End-to-End Solution for E-commerce Search via Embedding Learning.
In43rd SIGIR. 2407–2416.
[50] Tiancheng Zhao, Xiaopeng Lu, and Kyusong Lee. 2020. SPARTA: Efficient Open-
Domain Question Answering via Sparse Transformer Matching Retrieval. arXiv
preprint arXiv:2009.13013 (2020).
[51] Justin Zobel and Alistair Moffat. 2006. Inverted Files for Text Search Engines.
Comput. Surveys 38, 2 (2006), 6–es.
5348Text Matching Indexers in Taobao Search KDD ’24, August 25–29, 2024, Barcelona, Spain
A PROBLEM DEFINITION
For notation, let 𝑈={𝑢1,...,𝑢𝑁}be a set of𝑁users,𝑄={𝑞1,...,𝑞𝐿}
the user queries, 𝐼={𝑖1,...,𝑖𝑀}a collection of items, and 𝑇=
{𝑡1,...,𝑡𝐾}the unique terms that occur anywhere in the collection
𝐼.
A.1 Pruned Inverted Indexer
To cope with the large query loads with limited response latency,
we prune the full inverted indexer at the posting level in advance.
We retain all terms but truncate each term’s posting list to a given
length𝑝. Given a term 𝑡in𝑇,𝐼𝑡_𝐹={𝐼𝑡_1,...,𝐼𝑡_𝑓}denotes its full
posting list, and 𝐼𝑡_𝑃={𝐼𝑡_1,...,𝐼𝑡_𝑝}denotes its pruned posting
list. Typically, for each term 𝑡, we keep the top- 𝑝item candidates
from𝐼𝑡_𝐹based on the scores 𝑠between the term and items, i.e.,
𝑠=T(𝑡,𝐼𝑡_𝑓), (19)
whereT(·) denotes the term-based scoring function and is instan-
tiated with the TermRank algorithm.
A.2 Semantic Indexer
Whenever a user submits a query 𝑞, we would like to return a set of
items𝑖∈𝐼that are associated with that query. Typically, we predict
the top-𝑘item candidates from 𝐼based on the scores 𝑠between the
query and items, i.e.,
𝑠=F(𝜙(𝑞),𝜓(𝑖)), (20)
whereF(·) ,𝜙(·),𝜓(·)denote the scoring metric, query encoder,
and item encoder, respectively. For large-scale retrieval, we use the
two-tower neural model and instantiate Fwith the cosine function.
B ONLINE DEPLOYMENT
B.1 Retrieval Process Overview
As shown in Figure 1, when a user issues a query, the retrieval
process generally involves five stages.
(1)It first rewrites the user query using a module called Query
Planner (QP). For the query rewriting algorithms we used in
QP, see [22].
(2)Next, the rewriting queries and original query are used to ac-
cess the two indexers simultaneously. For the inverted index,
two-tiered indexes are stored, 𝐼𝑃and𝐼𝐹.𝐼𝐹is the regular full
posting list of a term while 𝐼𝑃is the truncated one with a given
length. The semantic indexer comprises a query encoder neural
network and item embedding indexes. The latter are exported
by item encoder network in advance.
(3)During query evaluation by the inverted indexer, we use the
term whose full posting list is the shortest within a query to
access𝐼𝑃, and the remainders access 𝐼𝐹. We call this strategy the
shortest posting list principle. Then we traverse all the posting
lists associated with the query and only return items that have
all the keywords (AND-semantics) to ensure high search rele-
vance (a.k.a. Document-At-A-Time strategy [ 44]). Besides, if the
relevant results are less than a few hundreds, we will do query
evaluation on all the 𝐼𝐹again. Hence, 𝐼𝑃greatly reduces both
the index access and the scoring costs while it may degrade
search quality.(4)While visiting the semantic indexer, the query embeddings are
predicted by the query encoder. And top- 𝑘results are returned
by a maximum inner product search (MIPS) [ 21] operation
between the query embedding and item embedding indexes.
(5)The results obtained from the two indexers are combined and
duplicates are removed as the final candidates for subsequent
ranking stages.
B.2 Indexer Deployment
Inverted Indexer. Due to the billion-scale item collection, we use
a number of partitions (called searchers ). Each contains an inverted
index, with pruned and full index, on a subset of the items. The
two-tiered indexes are created by an executor called index building
service. Thus, the inverted indexer is deployed with distribution on
machines of each searcher. Another kind of machines (called query
result service ) act as a query proxy that receives queries from the
users. They broadcast queries to searchers for fetching the top- 𝑘
results of each, and then merge the returned results into a distinct
candidate set that is sent back to the following ranking.
Semantic Indexer. All the item embeddings are exported from
the item tower by an index building service, and transmitted to an
approximate near neighbor (ANN) indexing module.4They are also
placed in multiple searchers because of their large number. Each
partition of the ANN builds indexes of embeddings by hierarchical
clustering (HC) algorithm with K-means and INT8 quantization to
promote storage and search efficiency. The training sample size of
HC is 4million, and the max scan ratio is 0.01. The query tower
is published to a real-time prediction platform for online inference.
The mechanism of the query result service is simultaneously used
for the semantic indexer. The final search results from SMIND are
as follow:
(Inverted Indexer results) OR (Semantic Indexer results)
B.3 Serving Policy
The user search preferences-aware pruned inverted indexer serves
all the query loads. For the semantic indexer, we propose to adapt an
“offline&online” mixed serving strategy to reduce latency. We only
feed the tail/new queries to online inference (real-time prediction ).
Despite the large number of tail and new queries, they account
for about 20%of online traffic. The remaining 80%of online traffic
is generated by millions of top queries, which typically remain
unchanged for a period of time. Hence, we run the query tower of
semantic indexer offline to export their embeddings, and then store
them in an online key-value graph engine (igraph ) for online ANN
indexing.
C EVALUATION DETAILS
C.1 Offline Evaluation
Evaluation in industry product search is non-trivial since we are
barely aware of the ground truth. We divide the evaluation into two
parts, offline and online evaluation. Offline evaluation is based on
sampled historical data, and online evaluation is based on daily new
4https://github.com/alibaba/proxima
5349KDD ’24, August 25–29, 2024, Barcelona, Spain Sen Li et al.
data. The common metrics used by offline and online evaluation
are Recall@ 𝐾and𝑃𝑔𝑜𝑜𝑑.
Recall@𝐾is used for evaluating the performance since it has a
positive correlation with the online income of Gross Merchandise
Volume (GMV) metric [ 23]. Specifically, given a query 𝑞𝑢, the items
clicked or purchased by the user 𝑢are regarded as the target set
𝑇={𝑡1,...,𝑡𝑁}, and the top- 𝑘items returned by an indexer are
regarded as the retrieval set 𝐼={𝑖1,...,𝑖𝐾}. Recall@𝐾is defined
as:
𝑅𝑒𝑐𝑎𝑙𝑙 @𝐾=Í𝐾
𝑖=1𝑖𝑖∈𝑇
𝑁. (21)
Precision@K is not suitable for evaluation when the ground truth
is unknown. Instead, we use a well-trained “query-item” relevance
model [ 46] (with an AUC of 0.92) to calculate the proportion of
products with good relevance (abbreviated as good rate and denoted
as𝑃𝑔𝑜𝑜𝑑) in the retrieval set 𝐼, i.e.,
𝑃𝑔𝑜𝑜𝑑=Í𝐾
𝑖=1I(𝑖𝑖)
𝐾, (22)
where I(·)is an indicator function. When item 𝑖is rated as good by
the relevance model, its value is 1, otherwise 0. Note that 𝑃𝑔𝑜𝑜𝑑 is a
user search experience-related metric.C.2 Online Evaluation
Large-scale online tests are the most promising way to examine
system performance since they provide an open environment to
interact with users. We consider the two most valuable commerce
metrics, GMV (Gross Merchandise Volume) and POC (Pay Order
Count), which are defined as
𝐺𝑀𝑉 =#pay amount , (23)
𝑃𝑂𝐶=#pay order count . (24)
Besides GMV and POC, we report four auxiliary metrics, namely
𝑵 𝒖𝒎 𝒓𝒂𝒏𝒌 ,𝑹𝒆𝒄𝒂𝒍𝒍 𝒄𝒍𝒌,𝑹𝒆𝒄𝒂𝒍𝒍 𝒑𝒖𝒓, and 𝑷𝒈𝒐𝒐𝒅 , which are defined
as follows:
𝑁𝑢𝑚𝑟𝑎𝑛𝑘 Given a retrieval set 𝐼={𝑖1,...,𝑖𝐾},𝑁𝑢𝑚𝑟𝑎𝑛𝑘 is the
number of items entering the ranking stage. This reflects the
importance of an indexer in the search system.
𝑅𝑒𝑐𝑎𝑙𝑙𝑐𝑙𝑘Eq. (21) is calculated on the item set 𝐼displayed to the
users and the target set 𝑇is the items clicked by users.
𝑅𝑒𝑐𝑎𝑙𝑙𝑝𝑢𝑟It is calculated similarly to 𝑅𝑒𝑐𝑎𝑙𝑙𝑐𝑙𝑘, except that the
target set𝑇is the items purchased by users.
𝑃𝑔𝑜𝑜𝑑 Eq. (22) is calculated on the displayed item set 𝐼.
5350