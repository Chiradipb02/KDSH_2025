Warming Up Cold-Start CTR Prediction
by Learning Item-Specific Feature Interactions
Yaqing Wang
Baidu Inc.
Baidu Research
Beijing, China
wangyaqing01@baidu.comHongming Piao
City University of Hong Kong
Department of Computer Science
Hong Kong SAR, China
hpiao6-c@my.cityu.edu.hkDaxiang Dong
Baidu Inc.
Baidu AI Cloud
Beijing, China
dongdaxiang@baidu.com
Quanming Yao
Tsinghua University
Department of Electronic Engineering
Beijing, China
qyaoaa@tsinghua.edu.cnJingbo Zhou
Baidu Inc.
Baidu Research
Beijing, China
zhoujingbo@baidu.com
ABSTRACT
In recommendation systems, new items are continuously intro-
duced, initially lacking interaction records but gradually accumu-
lating them over time. Accurately predicting the click-through rate
(CTR) for these items is crucial for enhancing both revenue and user
experience. While existing methods focus on enhancing item ID
embeddings for new items within general CTR models, they tend to
adopt a global feature interaction approach, often overshadowing
new items with sparse data by those with abundant interactions.
Addressing this, our work introduces EmerG, a novel approach
that warms up cold-start CTR prediction by learning item-specific
feature interaction patterns. EmerG utilizes hypernetworks to gen-
erate an item-specific feature graph based on item characteristics,
which is then processed by a Graph Neural Network (GNN). This
GNN is specially tailored to provably capture feature interactions at
any order through a customized message passing mechanism. We
further design a meta learning strategy that optimizes parameters
of hypernetworks and GNN across various item CTR prediction
tasks, while only adjusting a minimal set of item-specific parame-
ters within each task. This strategy effectively reduces the risk of
overfitting when dealing with limited data. Extensive experiments
on benchmark datasets validate that EmerG consistently performs
the best given no, a few and sufficient instances of new items.
CCS CONCEPTS
•Information systems →Recommender systems; •Comput-
ing methodologies →Supervised learning; Neural networks .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671784KEYWORDS
Cold-Start Recommendation, Warm Up, Click-Through Rate Pre-
diction, Few-Shot Learning, Hypernetworks, New Items
ACM Reference Format:
Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo
Zhou. 2024. Warming Up Cold-Start CTR Prediction by Learning Item-
Specific Feature Interactions. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671784
1 INTRODUCTION
The cold-start problem presents a significant challenge in recom-
mender systems [ 25], particularly evident as new items transition
from having no user interactions (termed as cold-start phase) to
accumulating a few initial clicks (termed as warm-up phase) in the
industry landscape. Deep learning models, renowned for their capa-
bility to capture complex feature interactions, have shown promise
in improving click-through rate (CTR) predictions, a critical met-
ric for assessing the likelihood of user engagement with various
items (e.g., movies, commodities, music) [ 1,5,9,36]. However, these
models typically rely on extensive datasets to achieve optimal per-
formance, a requirement that poses a limitation in cold-start and
warm-up phases. With their substantial parameter size, these mod-
els struggle to adapt efficiently to these phases characterized by
limited interaction records, thereby exacerbating the challenge of
making accurate CTR predictions and updating models without
incurring significant costs.
Recent studies have focused on enhancing the initialization of
item ID embeddings as a strategy to mitigate the item cold-start
problem in recommender systems, which allows subsequent up-
dates through gradient descent as interaction records become avail-
able in the warm-up phase [ 23,24,42,45]. Then, they leverage gen-
eral CTR backbones for further processing. However, they overlook
a crucial aspect: the distinctiveness of feature interaction patterns
across different users and items. This oversight limits the ability of
these models to fully capture the nuanced dynamics of user-item in-
teractions, potentially impacting the accuracy and effectiveness of
CTR predictions in scenarios where personalized recommendations
are crucial. For example, comparing high-priced luxury items with
3233
KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
low-priced daily necessities reveals distinct interaction patterns.
For high-priced luxury items, the interaction between the item’s
price and the user’s income level is pivotal. Specifically, the second-
order feature interaction <price, income> can be a determining
factor in the user’s willingness to purchase such items. As for low-
priced daily necessities, the impact of the user’s income level on
purchasing decisions diminishes. In these instances, other feature
interactions, such as those between the user’s age and the item’s
category, become relatively more important. This variation under-
scores the necessity of modeling item-specific feature interaction
patterns. While existing works all learn a global feature interaction
pattern between users and items, which overwhelms new items
with a limited number of interaction records by old items with
abundant interaction records.
Recognizing the crucial role of feature interactions, we introduce
EmerG to address CTR prediction of newly emerging items with
incremental interaction data (from no interaction records to few
and then abundant records) through the learning of item-specific
feature graphs. Our contributions can be summarized as follows:
•We propose a unique method that emphasizes item-specific fea-
ture interactions, addressing the challenge of new item CTR
prediction by reducing the overshadowing effect of older items
with extensive data. Utilizing hypernetworks, we construct item-
specific feature graphs with nodes as features and edges as their
interactions, capturing complex interaction patterns unique to
each item. We use a graph neural network (GNN) with a cus-
tomized message passing process designed to provably capture
feature interactions at any orders, which can be combined into
nuanced and accurate predictions.
•To mitigate overfitting given limited data, we adopt a meta-
learning strategy that optimizes parameters of hypernetworks
and GNN across different item CTR prediction tasks with a few
adjustments to item-specific parameters within each task. Be-
sides, hypernetworks and GNN learned this way are expected to
generalize to each task easily.
•We conduct extensive experiments on benchmark datasets and
validate that EmerG performs the best for CTR prediction of
emerging items. We also evaluate the performance given more
training data, and find that EmerG consistently performs the best.
Visualization of adjacency matrices which record item-specific
feature graphs shows that EmerG can learn item-specific feature
interactions properly.
2 RELATED WORKS
We briefly review four groups of methods relevant to CTR predic-
tion of newly emerging items.
A. General CTR Models. General CTR models are applied uni-
versally without prioritizing underrepresented items. They mainly
focus on modeling complex feature interactions, which is crucial
for enhancing CTR prediction accuracy [ 3]. Historical advance-
ments in this area show a progression from simple first-order in-
teractions captured by linear models like logistic regression [ 27]
to second-order interactions modeled by Factorization Machines
(FM) [ 26], and to high-order interactions addressed by higher-order
FMs (HOFMs) [ 2]. Various deep-learning models then automate
the learning of these complex patterns. Both Wide&Deep [ 5] andDeepFM [ 9] employ hybrid architectures to handle second-order
and higher interactions. DIN [ 43] dynamically captures user in-
terests through an attention mechanism that adapts to varying ad
features. AutoInt [ 28] introduces a multi-head self-attention mech-
anism [ 30] to model high-order feature interactions. LorentzFM
[40] explores feature interactions in hyperbolic space to minimize
parameter size. AFN [ 6] converts the power of each feature into the
coefficient to be learned. FinalMLP [ 21] uses two multi-layer per-
ceptron (MLP) networks in parallel, and equips them with feature
selection and interaction aggregation layers. FINAL [ 44] introduces
a factorized interaction layer for exponential growth in feature
interaction learning. Considering that feature interactions can be
conceptualized as graphs with nodes representing user and item
features, graph neural network (GNN) [ 15] are used. Fi-GNN [ 17]
learns to generate feature graphs where the edges are established
according to the similarity between feature embeddings. FIVES
[39] learns a global feature graph where edges are established by
differentiable search from large-scale CTR datasets, thus new items
with limited data can be underrepresented. GMT [ 22] models the
interactions among items, users, and their features into a large het-
erogeneous graph, then feeds the local neighborhood of the target
user-item pair for prediction. However, these general CTR models,
designed for extensive datasets, often struggle during the cold-start
& warm-up phases due to their substantial parameter size, which
can lead to overfitting when adapted for new items. In contrast,
EmerG introduces a specialized GNN that operates on item-specific
feature graphs, generated via hypernetworks learned from diverse
CTR tasks, ensuring precise modeling of feature interactions for
new items with minimal parameter adjustments.
B. Methods for New Items without Interaction Records. Several
methods specifically address the cold-start phase, where new items
lack interaction records, while still maintaining model performance
on older, established items. DropoutNet [ 31] trains neural networks
with a dropout mechanism on input samples to infer missing data.
Heater [ 46] employs a multi-gate mixture-of-experts approach to
generate item embeddings. GAR [ 4] adopts an adversarial training
strategy between a generator and a recommender to produce new
item embeddings that mimic the distribution of old embeddings,
deceiving the recommender systems. ALDI [ 12] learns to transfer
the behavioral information of old items to new items. However,
these methods do not consider the incorporation of incoming inter-
action records for new items and typically require re-training to
accommodate the evolving interaction history of these items.
C. Methods for New Items with A Few Interaction Records. In sce-
narios where only a few instances of new items are available, which
correspond to warm-up phases, few-shot learning [ 35] present a
natural solution. Few-shot learning targets at generalizing to new
tasks with a few labeled samples, which has been applied to image
classification [ 8], query intent recognition [ 34] and drug discov-
ery [ 33,38,41]. For CTR prediction, existing works typically ap-
proach the problem as a 𝑁-way𝐾-shot task, where each of the 𝑁
new items is associated with 𝐾labeled instances, then utilize the
classic gradient-based meta-learning strategy [ 8]. MeLU [ 16] lever-
ages this strategy to selectively adapt model parameters for new
items through gradient descents. MAMO [ 7] enhances adaptation
by incorporating an external memory mechanism. MetaHIN [ 20]
3234EmerG KDD ’24, August 25–29, 2024, Barcelona, Spain
utilizes heterogeneous information networks to exploit the rich se-
mantic relationships between users and items. PAML [ 32] employs
social relations to facilitate information sharing among similar
users. More recent approaches have shifted from user-specific fine-
tuning via gradient descent to amortization-based methods. These
methods directly map user interaction histories to user-specific
parameters, thus modulating the main network without iterative
adjustments. TaNP [ 18] learns to modulate item-specific parameters
based on item interaction records. ColdNAS [ 37] employs neural
architecture search to optimize the modulation function and its
application within the network. However, these methods struggle
to handle new items that lack interaction records and cannot dy-
namically incorporate additional interaction records of new items
as they become available.
D. Methods for Emerging Items with Incremental Interaction Records.
To mirror the industry’s dynamic evolution of new items, progress-
ing from no interaction records to few and then abundant records,
models are developed to manage these transitions smoothly. Ex-
isting efforts primarily enhance item ID embeddings for general
CTR backbones. MetaE [ 24] employs gradient-based meta-learning
to train an embedding generator. MWUF [ 45] transforms unstable
item ID embeddings into stable ones using meta networks. CVAR
[42] decodes new item ID embeddings from a distribution over
item side information, circumventing additional data processing.
GME [ 23] leverages information from neighboring old items for
new item ID embedding generation. However, these methods gen-
erally optimize initial item ID embeddings while maintaining a
global feature interaction pattern, thus failing to capture the unique
characteristics and interaction dynamics of these items. In contrast,
our EmerG addresses new item CTR prediction by tailoring fea-
ture interactions to each item with the help of hypernetworks. By
learning with item-specific feature interactions, the risk of over-
whelming new items by old items with abundant data is alleviated
and prediction accuracy is enhanced.
3 PROBLEM FORMULATION
LetV={𝑣𝑖}denote a set of items where each item 𝑣𝑖is associated
with𝑁𝑣item features such as item ID, type and price. Similarly, let
U={𝑢𝑗}denote a set of users where each user 𝑢𝑗is also associated
with𝑁𝑢user features such as user ID, age and hometown. When a
user𝑢𝑗clicks through an item 𝑣𝑖, label𝑦𝑖,𝑗=1. Otherwise, 𝑦𝑖,𝑗=0.
During learning, the predictor is learned from a set of CTR pre-
diction tasksTold={T𝑖}𝑁𝑡
𝑡=1sampled from old items, which can
rapidly generalize to predict for tasks from new items that are un-
seen during training. Each task T𝑖corresponds to an old item 𝑣𝑖, with
a training setS𝑖={(𝑣𝑖,𝑢𝑗,𝑦𝑖,𝑗)}𝑁𝑠
𝑗=1containing existing interaction
histories associated with 𝑣𝑖and a test setQ𝑖={(𝑣𝑖,𝑢𝑗,𝑦𝑖,𝑗)}𝑁𝑞
𝑗=1
containing interactions to predict whether 𝑢𝑗clicks through 𝑣𝑖.𝑁𝑠
and𝑁𝑞are the number of interactions in S𝑖andQ𝑖respectively.
During testing, we consider CTR prediction for new items that
start with no interaction records, then gradually gather a few, and
eventually accumulate sufficient interaction records. Consider a
taskT𝑘associated with a new item 𝑣𝑘which is not considered
during training, we handle three phases:
•Cold-start phase: No training set is provided.•Warm-up phase: A training set S𝑘containing a few interaction
records of𝑣𝑘is given. There can be multiple warm-up phases
where interaction records are gradually accumulated.
•Common phase: The training interaction records of 𝑣𝑘are accu-
mulated to be sufficient.
For all three phases, the performance is evaluated on test set Q𝑘.
4 THE PROPOSED EMERG
Aligning with the established understanding that feature inter-
actions are crucial, we propose EmerG (Figure 1) to capture the
uniqueness of items through their associated feature interaction
patterns. We design two key components in EmerG: (ii) hyper-
networks shared across different tasks to generate item-specific
adjacent matrices encoding feature graphs; and (i) a GNN that oper-
ates on the generated item-specific feature graphs, whose message
passing mechanism is specially tailored to provably capture feature
interactions at any order. As we consider cold-start & warm-up
phases, we further design a meta learning strategy that optimizes
parameters of hypernetworks and GNN across various item CTR
prediction tasks, while only adjusting a small set of item-specific
parameters within each task. This strategy effectively reduces the
risk of overfitting when dealing with limited data.
4.1 Embedding Layer
Given an instance (𝑢,𝑣), embedding layer maps the user features
of𝑢and item features of 𝑣into dense vectors. For the 𝑚th feature
𝑓𝑚,𝑚∈[1,𝑁𝑣+𝑁𝑢], its feature embedding e𝑚is obtained as
e𝑚= 
𝑾𝑒,𝑚·one-hot(𝑓𝑚) if𝑓𝑚is single-valuedÍ
𝑒𝑾𝑒,𝑚·multi-hot(𝑓𝑚)if𝑓𝑚is multi-valued
𝑾𝑒,𝑚·𝑓𝑚 if𝑓𝑚is continuous,(1)
where 𝑾𝑒,𝑚represents the embedding matrix corresponding to the
𝑚th feature, one-hot(𝑓𝑚)represents the one-hot vector of single-
valued feature 𝑓𝑚, and multi-hot(𝑓𝑚)represents the multi-hot vec-
tor of multi-valued feature 𝑓𝑚.
4.2 Item-Specific Feature Graph Generation
We employ hypernetworks, following the strategy of Ha et al. [ 10],
to generate item-specific feature graphs. Hypernetworks, small
neural networks trained to generate parameters for a larger main
network, present a unique challenge in their application, as their
integration is highly problem-specific. In EmerG, hypernetworks
are used to produce the initial adjacency matrix A(1)
𝑖, encoding the
item-specific feature graph for the first GNN layer. We streamline
the process by allowing subsequent GNN layers to derive their adja-
cency matrices from the initial A(1)
𝑖, optimizing storage efficiency
without compromising the model’s specificity to each item.
Consider taskT𝑖for item𝑣𝑖. For item features 𝑓1,...,𝑓𝑁𝑣, item
feature embeddings are denoted as e1,𝑖,..., e𝑁𝑣,𝑖respectively. The
feature graph [ 17,39] is a graph where each node corresponds to a
feature𝑓𝑚, and the edge between two nodes records their interac-
tion. We let our hypernetworks, which consists of 𝑁𝑣+𝑁𝑢subnet-
works, produce a dense item-specific ¯A(1)
𝑖∈R(𝑁𝑣+𝑁𝑢)×(𝑁𝑣+𝑁𝑢)
which encodes the feature graph to be used in the first GNN layer.
3235KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
Embedding LayerClick or NotItem-Specific Feature Graph With Feature EmbeddingsGNN LayerPredictorGNN Layer
Feature EmbeddingsItem FeaturesUser FeaturesHigh-Order IterationHigh-Order Iteration
one-hot(5)HypernetworksItem-specific Adjacency MatrixMLP 5MLP 4MLP1MLP 2one-hot(4)one-hot(3)one-hot(2)MLP 3one-hot(1)Pretrained, then Meta-TrainedMeta-TrainedDeterministic Computation
Figure 1: Illustration of the proposed EmerG, designed to enhance CTR predictions of newly emerging items through the
learning of item-specific feature interaction patterns. EmerG uses hypernetworks to generate an initial item-specific adjacency
matrix for a feature graph, with nodes representing user and item features and edges denoting their interactions, based on item
feature embeddings. Higher-order adjacency matrices for subsequent GNN layers are generated from the initial matrix, reducing
both model complexity and storage requirements. The GNN’s message passing process is tailored to capture 𝑙-order feature
interactions at the 𝑙−1th layer, enabling nuanced integration of various interaction orders for accurate predictions. EmerG
optimizes the parameters of hypernetworks and GNN across diverse CTR prediction tasks to enhance generalization, while
utilizing minimal item-specific parameters to capture the uniqueness of new items, which are adaptable with the introduction
of additional item instances.
Denote the𝑚th row of ¯A(1)
𝑖as[¯A(1)
𝑖]𝑚:, which is calculated as:
[¯A(1)
𝑖]𝑚:=MLP W𝑎([e1,𝑖,..., e𝑁𝑣,𝑖,one-hot(𝑚)]), (2)
where MLP W𝑎denotes a multi-layer perceptron (MLP) with param-
eterW𝑎. Then, we generate ¯A(𝑙)
𝑖as
¯A(𝑙)
𝑖=¯A(𝑙−1)
𝑖·¯A(1)
𝑖. (3)
This (3)returns ¯A(𝑙)
𝑖as the matrix product of 𝑙copies of ¯A(1)
𝑖.
Therefore,[¯A(𝑙)
𝑖]𝑚𝑛records the number of 𝑙-hop paths from node
𝑚to node𝑛. In this way, we only need to keep one adjacency matrix
(i.e., ¯A(1)
𝑖) for each item 𝑣𝑖no matter how many GNN layers are
used, which reduces parameter size.
Further, we take the following steps to refine adjacency matrices:
ˆA(𝑙)
𝑖=sparsify(normalize(¯A(𝑙)
𝑖),𝐾), (4)
˜A(𝑙)
𝑖=((ˆA(𝑙)
𝑖)⊤+ˆA(𝑙)
𝑖)/2, (5)
A(𝑙)
𝑖=normalize(mask([˜A(𝑙−1)
𝑖·˜A(1)
𝑖],˜A(𝑙−1)
𝑖)), (6)
where normalize(·)applies min-max normalization to scale all the
elements of a matrix to be in the range [0,1]and sets the diagonal
elements of a matrix directly as 1, sparsify(·,𝐾)keeps the top 𝐾
largest elements and set the rest as 0, and mask(·,˜A(𝑙−1)
𝑖)sets all
zero elements of ˜A(𝑙−1)
𝑖in˜A(𝑙)
𝑖as zero. From (4)to(6), we first
sparsify the dense ¯A(𝑙)
𝑖by(4)such that only two highly related
features are connected. Further, because of the commutative law
of⊙, we transform ˆA(𝑙)
𝑖into a symmetric matrix by (5). Apartfrom the above-mentioned considerations, we expect that nodes
disconnected in low-order feature graphs to be disconnected in high-
order feature graphs. For example, if the message is not propagated
from node𝑛2to node𝑛1in the𝑙th GNN layer, the message of 𝑛2
will be not propagated to 𝑛1in higher GNN layers. Thus, we apply
(6) to obtain the final A(𝑙)
𝑖.
4.3 Customized Message Passing Process on
Item-Specific Feature Graph
Upon the learned item-specific feature graphs, we use a GNN with
a customized message passing process designed to provably cap-
ture feature interactions at any orders, which are then explicitly
combined into the final CTR predictions.
We first describe the general mechanism of message passing. At
the𝑙th GNN layer, node embedding h(𝑙)
𝑚of feature𝑓𝑚is updated as
h(𝑙)
𝑚=UPD(𝑙)
h(𝑙−1)
𝑚,AGG(𝑙)n
h(𝑙−1)
𝑛 :𝑓𝑛∈N(𝑓𝑚)o
,(7)
where UPD(𝑙)(·)updates node embedding of 𝑓𝑚ash(𝑙)
𝑚,AGG(𝑙)(·)
aggregates node embeddings of neighbor nodes, N(𝑓𝑚)contains
neighbor nodes of node corresponding to feature 𝑓𝑚, and h(0)
𝑚=e𝑚.
After𝑁𝑙layers, node embedding h𝑚=h(𝑁𝑙)
𝑚 is returned as the
final feature representation.
In EmerG, we realize (7) as
h(𝑙)
𝑚=h(𝑙−1)
𝑚⊙h∑︁𝑁𝑣+𝑁𝑢
𝑛=1[A(𝑙−1)
𝑖]𝑚𝑛W(𝑙−1)
𝑔 h(0)
𝑛i
, (8)
3236EmerG KDD ’24, August 25–29, 2024, Barcelona, Spain
where⊙is the element-wise product, A(𝑙−1)
𝑖is the final item-
specific adjacency matrix obtained by (6), and W(𝑙−1)
𝑔 is a learnable
parameter. Unlike existing GNNs [ 15,17,39] that aggregate h(𝑙−1)
𝑚
with h(𝑙−1)
𝑛 , we aggregate h(𝑙−1)
𝑚 with h(0)
𝑛. In this way, as Proposi-
tion 4.1 shows, the output of (𝑙−1)th GNN layer is 𝑙-order feature
interactions, which enables EmerG to explicitly model arbitrary-
order feature interaction.
Proposition 4.1 (Efficacy of EmerG.). With the customized
message passing process defined in (8), the(𝑙−1)th GNN layer captures
𝑙-order feature interactions.
The proof is in Appendix A.1. One may consider integrating
residual connections into GNN to model arbitrary-order feature
interaction. However, as analyzed in Appendix A.2, incorporating
residual connections will significantly elevate the maximum order
of feature interaction.
With different orders of feature interactions, we then explicitly
combine all nodes embeddings of each node 𝑓𝑚into the updated
node embeddings ˆH𝑚of𝑓𝑚by multi-head attention:
attention(Q,K,V)=softmax(QK⊤/√︁
𝑁𝑑)V,
headℎ=attention(W𝑞,ℎH𝑚,W𝑘,ℎH𝑚,W𝑣,ℎH𝑚),
ˆH𝑚=[head 1;...;head𝑁ℎ],
where H𝑚=[h(0)
𝑚;...;h(𝑁𝑙)
𝑚]contains𝑁𝑙row vectors with length
𝑁𝑑, and𝑁ℎis the number of attention heads. Then, we estimate
the contribution factor for each of the 𝑁𝑣+𝑁𝑢features as
[𝑐1,...,𝑐𝑁𝑣+𝑁𝑢]=sigmoid
MLP W𝑐,1([ˆH1,..., ˆH𝑁𝑣+𝑁𝑢])
,(9)
where MLP W𝑐,1is parameterized by W𝑐,1. Finally, we predict whether
item𝑣and user𝑢interact as
ˆ𝑦=∑︁𝑁𝑣+𝑁𝑢
𝑚=1𝑐𝑚·MLP W𝑐,2(ˆH𝑚), (10)
where W𝑐,2is a trainable parameter.
4.4 Learning and Inference
To reduce the risk of overfitting when dealing with limited data,
we introduce a meta learning strategy that optimizes parameters of
hypernetworks and GNN across various item CTR prediction tasks,
while only adjusting a minimal set of item-specific parameters
within each task.
For simplicity, we denote hypernetworks as hyper𝜽hyperwhere
𝜽hyper =W𝑎is the shared trainable parameter. Then, we denote the
GNN as GNN 𝜽GNN,𝝓𝑖, where 𝜽GNN represents shared parameters
including parameters of embedding layers {W𝑒,𝑚}𝑁𝑣+𝑁𝑢
𝑚=1, param-
eters of GNN layers {W(𝑙)
𝑔}𝑁𝑙
𝑙=1,W𝑞,ℎ,W𝑘,ℎ,W𝑣,ℎ, parameters of
predictor W𝑐,1,W𝑐,2, and 𝝓𝑖represents item-specific parameters
𝝓𝑖={hyper𝜽hyper(𝑣𝑖),eID,𝑖}, (11)
which includes item-specific adjacency matrix A(1)generated by
hypernetworks and the randomized item ID embedding eID,𝑖of item
𝑣𝑖. We target at learning 𝜽∗
GNN,𝜽∗
hyper, which can achieve good cold-
start & warm-up performance on new item 𝑣𝑖by only generating
𝝓𝑖and warming up 𝝓𝑖with gradient descent.We optimize EmerG w.r.t. the following objective calculated
across𝑁𝑡tasks fromTold:
∑︁𝑁𝑡
𝑖𝛾LS𝑖(𝜽GNN,𝝓𝑖)+(1−𝛾)LQ𝑖(𝜽GNN,𝝓′
𝑖), (12)
where𝛾is a hyperparameter to balance the contribution of two loss
terms. In particular, the first term can represent the performance
of cold-start phase [ 24] as the model has not been exposed to the
labels inS𝑖. We computeLS𝑖(𝜽GNN,𝝓𝑖)as
LS𝑖(𝜽GNN,𝝓𝑖) (13)
≡1/|S𝑖|·∑︁
(𝑣𝑖,𝑢𝑗,𝑦𝑖𝑗)∈S𝑖BCE(𝑦𝑖𝑗,GNN 𝜽GNN,𝝓𝑖(𝑣𝑖,𝑢𝑗)),
where BCE(𝑦,ˆ𝑦)=−𝑦log(ˆ𝑦)−( 1−𝑦)log(1−ˆ𝑦)is the binary
cross entropy. The second term in (12)represents the performance
of warm-up phase after updating 𝝓𝑖by a few new item instances
provided inS𝑖. We computeLQ𝑖(𝜽GNN,𝝓′
𝑖)as
LQ𝑖(𝜽GNN,𝝓′
𝑖) (14)
≡1/|Q𝑖|·∑︁
(𝑣𝑖,𝑢𝑗,𝑦𝑖𝑗)∈Q𝑖BCE(𝑦𝑖𝑗,GNN 𝜽GNN,𝝓′
𝑖(𝑣𝑖,𝑢𝑗)),
with 𝝓′
𝑖obtained by performing gradient descent steps w.r.t (13):
𝝓′
𝑖=𝝓𝑖−𝛼1∇𝝓𝑖LS𝑖(𝜽GNN,𝝓𝑖), (15)
where𝛼1is the learning rate.
Algorithm 1 summarizes the training procedure of EmerG.
Algorithm 1 The training procedure of EmerG.
1:randomly initialize 𝜽hyper and𝜽GNN;
2:pretrain 𝜽GNN by old item instances;
3:forT𝑖inTolddo
4: sampleS𝑖andQ𝑖forT𝑖;
5: randomly initialize item ID embedding eID,𝑖forT𝑖;
6: obtain feature embeddings e1,𝑖,..., e𝑁𝑣+𝑁𝑢,𝑖by (1);
7: generate ¯A(1)
𝑖of the first GNN layer by (2);
8: get item-specific parameter 𝝓𝑖={¯A(1)
𝑖,hID,𝑖};
9: obtain A(𝑙)
𝑖of subsequent GNN layers by (3)-(6);
10: obtain feature representations h1,𝑖,..., h𝑁𝑣+𝑁𝑢,𝑖by (8);
11: obtain prediction ˆ𝑦𝑖𝑗by (10) for(𝑣𝑖,𝑢𝑗,𝑦𝑖,𝑗)∈S𝑖;
12: update 𝝓𝑖as𝝓′
𝑖by (15) with learning rate 𝛼1;
13: optimize 𝜽hyper and𝜽GNN w.r.t. (12)by gradient descents
with learning rate 𝛼2;
14:end for
By learning from a set of tasks Told, the learned 𝜽∗
GNN,𝜽∗
hyper
encode common knowledge. Consider task T𝑘of new item 𝑣𝑘. When
𝑣𝑘has no interaction record, namely |S𝑘|=0, we obtain its task-
specific parameter 𝝓𝑘and test its performance on the test set as
cold-start phase performance. Given a few new item instances, we
can update 𝝓𝑘to take in the supervised information. Once 𝝓𝑘is
updated, we measure performance on the test set as warm-up phase
performance. Algorithm 2 describes the testing procedure.
3237KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
Algorithm 2 The testing procedure of EmerG.
1:optimized 𝜽∗
hyperand𝜽∗
GNN;
2:Consider taskT𝑘of a new item 𝑣𝑘, givenS𝑘with a few in-
teraction records of 𝑣𝑘andQ𝑘for evaluating CTR prediction
performance of 𝑣𝑘;
3:if|S𝑘|=0then
4: randomly initialize item ID embedding eID,𝑘forT𝑘;
5: obtain feature embeddings e1,𝑘,..., e𝑁𝑣+𝑁𝑢,𝑘by (1);
6: generate ¯A(1)
𝑘of the first GNN layer by (2);
7: get item-specific parameter 𝝓𝑘={¯A(1)
𝑘,eID,𝑘};
8: obtain A(𝑙)
𝑘of subsequent GNN layers by (3)-(6);
9: obtain feature representations h1,𝑘,..., h𝑁𝑣+𝑁𝑢,𝑘by (8);
10: obtain prediction ˆ𝑦𝑘𝑗by (10) for(𝑣𝑘,𝑢𝑗,𝑦𝑘,𝑗)∈S𝑘;
11: measure performance on Q𝑘;
12:else
13: update A(𝑙)
𝑘,𝑙∈{1···𝑁𝑙}by (3) to (6);
14: update feature representations h1,𝑘,..., h𝑁𝑣+𝑁𝑢,𝑘by (8);
15: update 𝝓𝑘as𝝓′
𝑘by (15);
16: obtain prediction ˆ𝑦𝑘𝑗by (10) for(𝑣𝑘,𝑢𝑗,𝑦𝑘,𝑗)∈S𝑘;
17: measure performance on Q𝑘;
18:end if
5 EXPERIMENTS
5.1 Experimental Settings
Datasets. We use two benchmark datasets: (i) MovieLens [11]:
a dataset containing 1 million interaction records on MovieLens,
whose item features include movie ID, title, year of release, gen-
res and user features include user ID, age, gender, occupation and
zip-code; and (ii) Taobao [29]: a collection of 26 million ad click
records on Taobao, whose item features include ad ID, position
ID, category ID, campaign ID, advertiser ID, brand, price and user
features include user ID, Micro group ID, cms_group_id, gender,
age, consumption grade, shopping depth, occupation and city level.
Following existing works [ 24,42], we binarize the ratings of Movie-
Lens, setting rating smaller than 4 as 0 and the others as 1.
Data Split. We adopt the public data split [ 24,42,45], group
items according to their frequency: (i) old items which are items
appearing in more than 𝑁interaction records, where 𝑁=200in
MovieLens and 𝑁=2000 in Taobao; and (ii) new items which
are items appearing in less than 𝑁and larger than 3𝐾interaction
records, where 𝐾is set to 20 and 500 for MovieLens and Taobao
respectively. To mimic the dynamic process where new items are
gradually clicked by more users, the interaction records associated
with new items are sorted by timestamp. We consider three succes-
sive warm-up phases, labeled as A, B, and C, each of which involves
the introduction of a set of 𝐾new interaction records for each item.
The rest interaction records form testing data for evaluation.
Experiment Pipeline. We adopt pipeline of existing works [ 24,42,
45] to assess how a model adapts to new items over time. First, we
use old item instances to pretrain the model, and directly evaluate
the model performance on testing data of new items as cold-start
phase performance. Then, we measure how model performs as it
learns from a few training data in successive warm-up phases. Inparticular, we use the training data in warm-up phases A, B and C
to sequentially update the model, and evaluate the performance of
corresponding updated models on testing data.
Evaluation Metric. Following existing works [ 42], the perfor-
mance is evaluated by (i) Area Under the Curve (AUC) [ 19] which
represents the degree of separability, and (ii) F1 score [ 13] which
is a harmonic mean of the precision and recall. Both AUC and F1
vary between 0 (worst) and 1 (best).
5.2 Performance Comparison
We compare the proposed EmerG1with the following four groups
of baselines:
AGeneral CTR backbones pretrained using old item instances
and fine-tuned by new item instances, including DeepFM [9],
Wide&Deep [5],AutoInt [40],AFN [6],Fi-GNN [17], recent
FinalMLP [21] and FINAL [44].
BMethods for new items without interaction records, including
DropoutNet [31] and ALDI [12].
CMethods for new items with a few interaction records, includ-
ingMeLU [16],MAMO [7],TaNP [18], and ColdNAS [37].
These methods cannot incorporate new item instances dynami-
cally. Therefore, to accommodate the training interaction records
provided in warm-up phases A, B, and C, we adopt a phased
approach: initially, we use 𝐾interaction records from phase A
as the support set to assess testing performance. Subsequently,
we combine 2𝐾records from phases A and B as the support set
for a second evaluation. Finally, we incorporate 3𝐾records from
all three warm-up phases—A, B, and C—as the support set to
conduct a third assessment of testing performance.
DMethods for emerging items with incremental interaction records,
which are the most relevant to ours. Existing works mainly equip
general CTR backbones with the ability to generate and warm-
up item ID embeddings for new items, including MetaE [24],
MWUF [45],GME [23], and CVAR [42]. We use the classic
DeepFM as the CTR backbone. Results of equipping these meth-
ods with other backbones are reported in Appendix C.1.
We implement the compared methods using public codes of the
respective authors. More implementation details are provided in
Appendix B.
Performance for Cold-Start & Warm-Up Phases. Table 1 shows
the results. As shown, cold-start methods designed for cold-start
& warm-up phases generally perform better. EmerG consistently
performs the best in all four phases, validating the effectiveness
of capturing item-specific feature interaction by hypernetworks.
Few-shot methods for N-way K-shot settings obtains good perfor-
mance in warm-up phase A. However, as the number of samples
increases, it is unable to achieve greater performance improvement
without complete retraining. General CTR backbones which are
fine-tuned using the training sets perform worse, where FinalMLP
performs the best. Recall that they randomly initialize item-specific
parameters for new items, fine-tuning pretrained models by a small
number of new item instances is not enough to obtain good perfor-
mance. Particularly, note that the GNN-based CTR model Fi-GNN
which uses item-user-specific feature interaction graphs perform
1Our code is available at https://github.com/LARS-group/EmerG.
3238EmerG KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Test performance obtained on MovieLens and Taobao. The best results are bolded, the second-best results are underlined.
MovieLensCold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C
AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%)
DeepFM 71.48(0.15)60.96(0.22)75.21(0.27)64.09(0.01)77.70(0.26)66.18(0.15)79.45(0.19)67.81(0.14)
Wide&Deep 69.44(0.29)59.53(0.31)74.82(0.29)64.44(0.21)77.58(0.25)66.82(0.24)79.09(0.22)67.67(0.27)
AutoInt 68.64(0.24)59.63(0.14)75.60(0.31)64.93(0.36)77.65(0.33)66.84(0.42)79.20(0.34)67.77(0.36)
LorentzFM 68.91(0.15)56.22(0.27)75.35(0.17)62.77(0.21)78.46(0.08)66.23(0.25)79.85(0.02)67.93(0.08)
AFN 71.23(0.42)61.76(0.37)74.26(0.08)64.39(0.09)76.19(0.24)65.84(0.16)77.36(0.35)66.71(0.28)
Fi-GNN 71.37(0.05)61.46(0.08)74.62(0.03)63.83(0.12)76.83(0.06)65.71(0.05)78.49(0.05)66.74(0.06)
FinalMLP 69.51(0.06)60.59(0.17)78.48(0.12)67.34(0.10)78.47(0.16)67.27(0.07)79.07(0.17)68.00(0.10)
FINAL 71.64(0.15)61.72(0.17)77.87(0.13)66.99(0.10)77.94(0.10)66.93(0.14)78.29(0.09)67.42(0.10)
DropoutNet 72.94(0.17)62.43(0.18) - - - - - -
ALDI 65.53(0.13)57.47(0.23) - - - - - -
MeLU - - 77.54(0.06)66.71(0.11)79.43(0.10)68.51(0.05)80.26(0.03)68.13(0.06)
MAMO - - 77.69(0.10)66.92(0.13)79.61(0.07)68.72(0.04)80.37(0.05)68.49(0.04)
TaNP - - 79.15(0.10)68.39(0.14)80.49(0.17)69.43(0.15)80.71(0.09)69.63(0.09)
ColdNAS - - 77.45(0.03)67.01(0.03)77.88(0.12)67.25(0.21)78.06(0.09)67.31(0.11)
MetaE 71.82(0.70)61.76(0.30)79.53(0.25)67.96(0.15)80.27(0.09)68.31(0.12)80.47(0.04)68.46(0.12)
CVAR 73.58(0.21)63.15(0.12)78.23(0.10)67.03(0.26)80.28(0.06)68.76(0.12)81.06(0.04)69.33(0.14)
GME 71.54(0.13)64.31(0.10)75.81(0.20)67.50(0.26)78.10(0.18)69.26(0.20)79.15(0.12)69.95(0.16)
MWUF 73.19(0.66)62.61(0.74)78.88(0.11)67.34(0.22)80.26(0.08)68.40(0.13)80.57(0.05)68.66(0.10)
EmerG 75.44(0.05)64.76(0.15)79.92(0.27)68.61(0.24)81.28(0.21)69.71(0.14)81.82(0.16)70.26(0.14)
TaobaoCold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C
AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%)
DeepFM 59.01(0.84)13.47(0.42)60.68(0.65)14.27(0.20)61.51(0.64)14.56(0.34)62.34(0.54)15.00(0.24)
Wide&Deep 59.07(0.44)13.65(0.06)60.92(0.56)14.25(0.10)61.69(0.51)14.56(0.15)62.33(0.46)14.75(0.13)
AutoInt 55.69(1.37)12.14(0.22)58.65(1.26)13.61(0.51)59.43(1.20)13.84(0.36)60.07(1.13)14.19(0.39)
LorentzFM 56.53(0.41)12.72(0.04)60.83(0.50)14.15(0.24)61.26(0.47)14.31(0.14)61.96(0.45)14.60(0.14)
AFN 57.94(0.99)13.28(0.15)58.99(0.74)13.73(0.20)59.81(0.87)13.99(0.18)60.18(0.69)14.18(0.13)
Fi-GNN 56.92(0.08)12.79(0.10)60.00(0.13)14.06(0.18)62.09(0.14)14.82(0.14)62.46(0.21)14.90(0.05)
FinalMLP 60.64(0.12)13.57(0.04)63.44(0.06)14.83(0.03)63.49(0.07)14.80(0.03)64.05(0.02)15.05(0.03)
FINAL 60.53(0.24)13.63(0.05)63.30(0.12)14.81(0.06)63.35(0.13)14.74(0.04)63.93(0.12)15.01(0.02)
DropoutNet 60.41(0.09)13.53(0.02) - - - - - -
ALDI 50.10(0.18)10.93(0.05) - - - - - -
MeLU - - 61.37(0.17)14.09(0.17)62.48(0.04)14.34(0.05)63.07(0.07)14.64(0.11)
MAMO - - 61.96(0.11)14.31(0.09)62.52(0.05)14.34(0.04)63.15(0.12)14.78(0.13)
TaNP - - 55.67(0.22)11.92(0.31)55.85(0.16)12.07(0.16)56.19(0.09)12.08(0.11)
ColdNAS - - 54.27(0.07)10.89(0.05)54.86(0.14)11.33(0.13)55.01(0.09)11.71(0.13)
MetaE 59.75(0.37)13.58(0.06)61.19(0.26)14.01(0.09)62.06(0.31)14.41(0.10)62.87(0.32)14.71(0.07)
CVAR 60.56(0.46)13.71(0.13)62.54(0.19)14.38(0.06)63.17(0.10)14.69(0.05)63.95(0.18)15.09(0.12)
GME 60.57(0.23)13.32(0.33)62.55(0.17)13.96(0.22)63.29(0.05)14.39(0.12)63.85(0.13)14.52(0.08)
MWUF 59.65(0.46)13.44(0.15)62.08(0.17)14.20(0.07)63.03(0.13)14.63(0.07)63.79(0.12)14.93(0.06)
EmerG 61.58(0.03)13.99(0.05)63.56(0.03)15.02(0.06)63.76(0.02)15.15(0.01)64.22(0.02)15.21(0.02)
not well. This shows that too much freedom is not beneficial to
capture feature interaction patterns under cold-start & warm-up
phases. While in EmerG, we utilize hypernetworks to generate
item-specific feature graphs, which is then processed by a GNN
with customized message passing mechanism to capture arbitrary-
order feature interaction, and optimize parameters by meta learning
strategy. All these design considerations contributes the best per-
formance obtained by EmerG. For computational overhead, EmerG
is relatively more efficient in terms of both time and computational
resources. See Appendix C.2 for a detailed comparison.Performance Given Sufficient Training Samples. One might ques-
tion how EmerG performs with an abundance of training samples
for new items, referred to as the common phase, especially in com-
parison to traditional CTR backbones. Here, we set aside samples
from original testing samples of new items (so the test set is smaller),
use them to augment the experiment pipeline with more training
samples, and evaluate the performance on the smaller test set. We
compare EmerG with baselines which perform the best among CTR
backbones and few-shot methods in Table 1. Figure 2 shows the
3239KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
testing AUC (%) with the number of training samples. Experimen-
tal results measured by testing F1 (%) are similar. As shown, all
methods get better performance given more training samples. The
classic CTR backbone DeepFM gradually outperforms CVAR which
equips DeepFM with additional modules to generate item ID em-
beddings for new items. In contrast, EmerG consistently performs
the best and converges to better performance than the others. This
validates the effectiveness of our EmerG which can nicely capture
item-specific feature interaction at different orders.
0 200 400 600 800 1000 1200
Number of Samples70.072.575.077.580.082.585.0AUC (%) DeepFM
CVAR
EmerG
(a) MovieLens.
0 350 700 1050 1400 1750
Number of Samples5860626466AUC (%) DeepFM
CVAR
EmerG (b) Taobao.
Figure 2: Comparing EmerG with DeepFM and CVAR given
sufficient training samples.
5.3 Model Analysis
5.3.1 Ablation Study. We compare the proposed EmerG with the
following variants: (i) w/ random graph generates the adjacency
matrix A(1)in(8)randomly; (ii) w/o sparsification does not apply
(4)to sparsify the adjacency matrices; (iii) w/o mask does not apply
(6)to enforce nodes which are disconnected in low-order feature
graphs to be disconnected in higher-order feature graphs; (iv) w/
shared graph employs global shared adjacency matrices for all
items, in contrast to EmerG, which utilizes item-specific adjacency
matrices; (v) w/o meta learns both GNN and hypernetworks from
old items, without forming tasks and employ a meta-learning strat-
egy; and (vi) w/o inner directly uses 𝝓𝑖and does not update it to
𝝓′
𝑖within each task.
Figure 3 shows the results. As shown, “w/ random graph" per-
forms worse than EmerG which shows that the item-specific feature
graphs generated by hypernetworks is meaningful. The perfor-
mance gain of EmerG over “w/o sparsification" shows that a sparse
feature graph where only closely-related nodes are connected can
let the GNN model concentrate on useful messages. Comparing
“w/o mask" to EmerG, the performance drop validates our assump-
tion in (6). The mask operation also prevents the adjacency matrices
from being too dense, which can be beneficial to prune unneces-
sary feature interactions and provide better explainability. We can
also observe that“w/ shared graph" performs worse than EmerG.
This validates that using global shared adjacency matrices cannot
capture the various feature interaction patterns between different
users and items. Finally, EmerG defeats “w/o meta" and “w/o inner",
which underscores the necessity of both meta-learning across tasks
and inner updates within each task.
5.3.2 Effect of Number of GNN Layers. As demonstrated in Propo-
sition 4.1, we have customized the message passing process of the
GNN to ensure that the 𝑙th layer encapsulates 𝑙-order feature in-
teractions. Furthermore, we optimize this process by generating
Cold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C67.570.072.575.077.580.082.585.0AUC (%)w/o meta
w/o inner
w/ random graph
w/o sparsiﬁcationw/o mask
w/ shared graph
EmerG(a) MovieLens.
Cold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C565860626466AUC (%)w/o meta
w/o inner
w/ random graph
w/o sparsiﬁcationw/o mask
w/ shared graph
EmerG
(b) Taobao.
Figure 3: Ablation study on MovieLens and Taobao.
adjacency matrices for subsequent GNN layers directly from the
initial matrix provided by hypernetworks. This approach not only
streamlines the architecture but also facilitates the extension to
additional layers, thereby capturing higher-order feature interac-
tions with ease. In this context, we investigate the influence of the
number of GNN layers on performance across various datasets.
Cold Warm A Warm B Warm C70.072.575.077.580.082.585.0AUC (%)1 layer
2 layer(ours)
3 layer
(a) MovieLens.
Cold Warm A Warm B Warm C5860626466AUC (%)1 layer
2 layer
3 layer(ours) (b) Taobao.
Figure 4: Varying the number of GNN layers in EmerG.
Figure 4 shows the results. As can be seen, EmerG with different
layer numbers obtain the best performance on different datasets:
EmerG with 2 GNN layers performs the best on MovieLens while
EmerG with 3 GNN layers achieves the best performance on Taobao.
This shows that different datasets requires different number of
GNN layers: larger datasets such as Taobao may need higher-order
features than smaller ones such as MovieLens. By design, EmerG
can easily meet this requirement.
5.3.3 Different Feature Interaction Functions. We consider using
different feature interaction functions. Table 2 shows the results.
3240EmerG KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: Visualizations of item-specific adjacency matrices of movie Lawnmower Man 2: Beyond Cyberspace (A(𝑖)
W) and movie
Waiting to Exhale (A(𝑖)
L) in MovieLens generated by EmerG.
As shown, using element-wise product performs the best, which is
adopted in EmerG.
Table 2: Test performance in AUC (%) obtained on MovieLens
and Taobao. The best results are bolded.
MovieLens Cold-Start Warm-Up A Warm-Up B Warm-Up C
⊙ 75.44 79.92 81.28 81.82
max 74.14 79.43 81.02 81.65
+ 73.64 79.31 80.77 81.39
Taobao Cold-Start Warm-Up A Warm-Up B Warm-Up C
⊙ 61.58 63.56 63.76 64.22
max 59.98 62.49 62.56 63.07
+ 59.80 62.34 62.26 62.89
5.4 Case Study
Finally, we take movie Lawnmower Man 2: Beyond Cyberspace and
movie Waiting to Exhale from MovieLens as new items, and visualize
their adjacency matrices which record the item-specific feature
graphs in Figure 5.
As can be seen, EmerG learns different task-specific feature
graphs for different items. Comparing Figure 5(a) with Figure 5(b),
we find that Lawnmower Man 2: Beyond Cyberspace has a particu-
larly important second-order feature interaction ⟨genres,title⟩ . The
genre of Lawnmower Man 2: Beyond Cyberspace is science fiction
while the genre of Waiting to Exhale is comedy. For a science fiction,
its title often reflects its world view or theme, which is the key for
people to judge whether they are interested. As for a comedy work,
whether it is interesting or not is often irrelevant to the title.
Besides, EmerG can capture meaningful higher-order feature
interactions. As shown, both ⟨year,age⟩ and⟨year,zip-code⟩ are
important second-order feature interactions, they contribute to
discovering the third-order feature interaction ⟨year,age,zip-code⟩ .
In Figure 5(c), the relation between nodes of year, age and zip-code
all become relatively important although <age, zip-code> is not
important in Figure 5(b). It is easy to understand that the year of
movies determines the age of people who are more likely to watch
them. For example, elderly people generally prefer watching old
movies. Apart from this, location which is indicated by zip-code
also plays an important role: people in developed areas tend to be
more receptive to new things. Therefore, area changes may leadto changes in the age of people who like the same movie, which
validates the efficacy of the learned third-order feature interaction.
We can also observe that the item-specific feature graphs gener-
ated by our hypernetworks can roughly capture the feature inter-
actions before seeing any training samples of new items. Although
they are continuously optimized using training sets of warm-up
phases, the changes are not sharp. As can be seen, the second-order
feature interaction patterns are similar in Figure 5(b) and Figure 5(d)
to Figure 5(f), with small changes to accommodate the training
samples. Overall, we conclude that EmerG can learn reasonable
adjacency matrices to capture item-specific feature interactions at
different orders.
6 CONCLUSION
In this study, we underscore the critical role of feature interac-
tions and introduce EmerG, a novel solution designed to capture
the unique interaction patterns of items, effectively handling CTR
prediction of newly emerging items with incremental interaction
records. Our approach leverages hypernetworks to construct item-
specific feature graphs, with nodes representing features and edges
denoting their interactions, thus enabling the model to discern the
intricate interaction patterns that characterize each item. We incor-
porate a graph neural network (GNN) equipped with a specialized
message passing process, crafted to capture feature interactions
across all orders, facilitating precise CTR predictions. To combat
overfitting in scenarios with sparse data, we implement a meta-
learning strategy that finely tunes parameters of hypernetworks
and GNN across various item CTR prediction tasks, necessitating
only minimal modifications to item-specific parameters for each
task. Experimental results on real-world datasets show EmerG ob-
tains the state-of-the-art performance on CTR prediction for new
items that have no interaction history, a few interactions, or a sub-
stantial number of interactions. We expect this approach can be
used to warm-up cold-start problems in other applications such as
drug recommendation in the future.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their valuable comments.
This work is supported by National Key Research and Develop-
ment Program of China under Grant 2023YFB2903904 and National
Natural Science Foundation of China under Grant No. 92270106.
3241KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
REFERENCES
[1]Jiang Bian, Jizhou Huang, Shilei Ji, Yuan Liao, Xuhong Li, Qingzhong Wang,
Jingbo Zhou, Dejing Dou, Yaqing Wang, and Haoyi Xiong. 2023. Feynman:
Federated Learning-based Advertising for Ecosystems-Oriented Mobile Apps
Recommendation. IEEE Transactions on Services Computing 16, 5 (2023), 3361–
3372.
[2]Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016.
Higher-order factorization machines. In Advances in Neural Information Process-
ing Systems. 3351–3359.
[3]Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalable
response prediction for display advertising. ACM Transactions on Intelligent
Systems and Technology 5, 4 (2014), 1–34.
[4]Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng
He, and Zhoujun Li. 2022. Generative adversarial framework for cold-start
item recommendation. In International ACM SIGIR Conference on Research and
Development in Information Retrieval. 2565–2571.
[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Workshop on Deep
Learning for Recommender Systems. 7–10.
[6]Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization
network: Learning adaptive-order feature interactions. In AAAI Conference on
Artificial Intelligence. 3609–3616.
[7]Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO:
Memory-augmented meta-optimization for cold-start recommendation. In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 688–697.
[8]Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-
learning for fast adaptation of deep networks. In International Conference on
Machine Learning. 1126–1135.
[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: A factorization-machine based neural network for CTR prediction. In
International Joint Conference on Artificial Intelligence. 1725–1731.
[10] David Ha, Andrew Dai, and Quoc V Le. 2017. Hypernetworks. In International
Conference on Learning Representations.
[11] F Maxwell Harper and Joseph A Konstan. 2015. The MovieLens datasets: History
and context. ACM Transactions on Interactive Intelligent Systems 5, 4 (2015), 1–19.
[12] Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen.
2023. Aligning distillation for cold-start item recommendation. In International
ACM SIGIR Conference on Research and Development in Information Retrieval.
1147–1157.
[13] Hao Huang, Haihua Xu, Xianhui Wang, and Wushour Silamu. 2015. Maximum F1-
score discriminative training criterion for automatic mispronunciation detection.
IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, 4 (2015),
787–797.
[14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. In International Conference on Learning Representations.
[15] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks. In International Conference on Learning Representations.
[16] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.
MeLU: Meta-learned user preference estimator for cold-start recommendation. In
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1073–1082.
[17] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN:
Modeling feature interactions via graph neural networks for CTR prediction.
InACM International Conference on Information and Knowledge Management.
539–548.
[18] Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, and Bin Wang. 2021.
Task-adaptive neural process for user cold-start recommendation. In The Web
Conference. 1306–1316.
[19] Charles X Ling, Jin Huang, Harry Zhang, et al .2003. AUC: A statistically con-
sistent and more discriminating measure than accuracy. In International Joint
Conference on Artificial Intelligence. 519–524.
[20] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneous
information networks for cold-start recommendation. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 1563–1573.
[21] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong.
2023. FinalMLP: An enhanced two-stream MLP model for CTR prediction. In
AAAI Conference on Artificial Intelligence. 4552–4560.
[22] Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Da Luo, Kangyi Lin, Junzhou
Huang, Sophia Ananiadou, and Peilin Zhao. 2022. Neighbour interaction based
click-through rate prediction via graph-masked transformer. In International
ACM SIGIR Conference on Research and Development in Information Retrieval.
353–362.
[23] Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang, Jinmei Luo, Zhaojie
Liu, and Yanlong Du. 2021. Learning graph meta embeddings for cold-start ads in
click-through rate prediction. In International ACM SIGIR Conference on Research
and Development in Information Retrieval. 1157–1166.[24] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm
up cold-start advertisements: Improving CTR predictions via learning to learn ID
embeddings. In International ACM SIGIR Conference on Research and Development
in Information Retrieval. 695–704.
[25] Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommender
systems and how to leverage it. In ACM Conference on Recommender Systems.
11–18.
[26] Steffen Rendle. 2010. Factorization machines. In IEEE International Conference on
Data Mining. 995–1000.
[27] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: Estimating the click-through rate for new ads. In International Conference
on World Wide Web. 521–530.
[28] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via self-
attentive neural networks. In ACM International Conference on Information and
Knowledge Management. 1161–1170.
[29] Tianchi. 2018. Taobao Display Ads Click Dataset. https://tianchi.aliyun.com/
dataset/dataDetail?dataId=56.
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems. 5998–6008.
[31] Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Ad-
dressing cold start in recommender systems. In Advances in Neural Information
Processing Systems. 4957–4966.
[32] Li Wang, Binbin Jin, Zhenya Huang, Hongke Zhao, Defu Lian, Qi Liu, and Enhong
Chen. 2021. Preference-adaptive meta-learning for cold-start recommendation..
InInternational Joint Conference on Artificial Intelligence. 1607–1614.
[33] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. 2021.
Property-aware relation networks for few-shot molecular property prediction.
InAdvances in Neural Information Processing Systems. 17441–17454.
[34] Yaqing Wang, Song Wang, Yanyan Li, and Dejing Dou. 2022. Recognizing medical
search query intent by few-shot learning. In International ACM SIGIR Conference
on Research and Development in Information Retrieval. 502–512.
[35] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing
from a few examples: A survey on few-shot learning. Comput. Surveys 53, 3
(2020), 1–34.
[36] Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, and Yong Li. 2023.
Efficient and Joint Hyperparameter and Architecture Search for Collaborative
Filtering. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
2547–2558.
[37] Shiguang Wu, Yaqing Wang, Qinghe Jing, Daxiang Dong, Dejing Dou, and Quan-
ming Yao. 2023. ColdNAS: Search to modulate for user cold-start recommendation.
InThe Web Conference. 1021–1031.
[38] Shiguang Wu, Yaqing Wang, and Quanming Yao. 2024. PACIA: Parameter-
efficient adapter for few-shot molecular property prediction. In International
Joint Conference on Artificial Intelligence.
[39] Yuexiang Xie, Zhen Wang, Yaliang Li, Bolin Ding, Nezihe Merve Gürel, Ce Zhang,
Minlie Huang, Wei Lin, and Jingren Zhou. 2021. FIVES: Feature interaction via
edge search for large-scale tabular data. In ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 3795–3805.
[40] Canran Xu and Ming Wu. 2020. Learning feature interactions with lorentzian
factorization machine. In AAAI Conference on Artificial Intelligence. 6470–6477.
[41] Quanming Yao, Zhenqian Shen, Yaqing Wang, and Dejing Dou. 2024. Property-
aware relation networks for few-shot molecular property prediction. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence (2024).
[42] Xu Zhao, Yi Ren, Ying Du, Shenzheng Zhang, and Nian Wang. 2022. Improving
item cold-start recommendation via model-agnostic conditional variational au-
toencoder. In International ACM SIGIR Conference on Research and Development
in Information Retrieval. 2595–2600.
[43] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 1059–1068.
[44] Jieming Zhu, Qinglin Jia, Guohao Cai, Quanyu Dai, Jingjie Li, Zhenhua Dong,
Ruiming Tang, and Rui Zhang. 2023. FINAL: Factorized interaction layer for CTR
prediction. In International ACM SIGIR Conference on Research and Development
in Information Retrieval. 2006–2010.
[45] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang,
Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for cold-
start recommendation with meta scaling and shifting networks. In International
ACM SIGIR Conference on Research and Development in Information Retrieval.
1167–1176.
[46] Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee. 2020. Recom-
mendation for new users and new items via randomized training and mixture-of-
experts transformation. In International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1121–1130.
3242EmerG KDD ’24, August 25–29, 2024, Barcelona, Spain
A MESSAGE PASSING MECHANISM
A.1 Proof of Proposition 4.1
Proof. In(8), node embedding h(𝑙−1)
𝑚 aggregates from multiple
first-order node embeddings h(0)
𝑛to generate h(𝑙)
𝑚. Therefore,
order(h(0)
𝑚)=1,
order(h(𝑙)
𝑚)=order(h(𝑙−1)
𝑚)+1,
where order(h(𝑙)
𝑚)represents the order of h(𝑙)
𝑚. We can conclude
order(h(𝑙)
𝑚)=𝑙+1.
After merging all nodes embedding with multi-head self-attention,
all orders of features we can obtain after 𝑙−1GNN layers are
order(ˆH𝑚)={1,2,...,𝑙,𝑙+1}.
□
A.2 Comparing with GNN with Residual
Connections
Although GNN with residual connections can model arbitrary-order
feature interaction [ 17], the maximum order of feature interaction
will increase drastically. To see this, instead of using (8),(7)can be
realized with residual connection as
h(𝑙)
𝑚=h(𝑙−1)
𝑚+h(𝑙−1)
𝑚⊙∑︁𝑁𝑣+𝑁𝑢
𝑛=1[A(𝑙−1)
𝑖]𝑚𝑛W(𝑙−1)
𝑔 h(𝑙−1)
𝑛.
Ash(𝑙−1)
𝑚 andh(𝑙−1)
𝑛 have the same maximum order, when we
aggregate them via ⊙, we have:
maxorder(h(𝑙)
𝑚)=maxorder(h(𝑙−1)
𝑚)×2.
In other words, using residue connections will lead to too many
noisy high-order feature interactions. Consequently, it is also chal-
lenging to determine which feature interactions contribute to the
prediction, resulting in low interpretability.
B IMPLEMENTATION DETAILS
All results are averaged over five runs and are obtained on a 32GB
NVIDIA Tesla V100 GPU. We use Adam optimizer [ 14]. To search
for the appropriate hyperparameters, we set aside 20% old items
and form validation set using their samples. The performance is
then directly evaluated on the validation set of these items, which
corresponds to cold-start phase. When the hyperparameters are
found by grid search, we put back samples of these old items, then
follow the experiment pipeline described in Section 5.1 and report
the results. The hyperparameters and their range used by EmerG
are summarized in Table 3.
C MORE EXPERIMENTAL RESULTS
C.1 Comparing with Existing Methods
Equipped with Different Backbones
In Section 5.2, we employ DeepFM as the backbone for methods
in Group D. Despite this, results in Table 1 indicate that FinalMLP
generally surpasses DeepFM, particularly in the warm-up phases,
though not in the cold-start phases. Therefore, we further integrate
FinalMLP, the top-performing backbone from Group A, into meth-
ods in Group D. Results are reported in Table 4. Notably, FinalMLP,Table 3: Hyperparameters used by EmerG. 𝑁′=𝑁𝑣+𝑁𝑢.
Hyp
erparameter Range Mo
vieLens T
aobao
numb
er of GNN layers [1,2,3] 2 3
𝐾in
(4) [0,1,·
··,𝑁′·𝑁′𝑁′·𝑁′
2𝑁′·𝑁′
2
𝛾in
loss function [1𝑒−2,
...,1] 0.1 0.1
numb
er of heads [1,2,
...,5] 3 3
emb
edding dimension [10,11,
...,20] 16 16
batch
size [64,128,·
··,1024] 512 512
pr
etraining learning rate [1𝑒−4,1𝑒−1] 0.005 0.001
pr
etraining epochs [1,2,
...,20] 2 1
meta-training
learning rate 𝛼2[1𝑒−4,1𝑒−1] 0.001 0.0001
meta-training
epochs [1,2,
...,20] 11 3
up
date learning rate 𝛼1during meta-training [1𝑒−4,1𝑒−1] 0.01 0.001
up
date learning rate 𝛼1during warming-up [1𝑒−4,1𝑒−1] 0.01 0.01
warming-up
epochs [1,2,
...,20] 11 16
when utilized as a backbone for cold-start methods, does not exceed
the performance of configurations using DeepFM. This suggests
that more recent CTR backbones cannot effectively handle the CTR
prediction of newly emerging items.
C.2 Computational Overhead
Table 5 shows a detailed comparison of the computational over-
head for all compared methods. As indicated, EmerG demonstrates
relatively lower time and space requirements.
Table 5: Computational overhead of compared methods on
MovieLens. Time is reported as seconds per epoch.
Training Time Test Time # Para.
DeepFM 831.30 43.59 0.51
Wide&Deep 725.15 45.30 0.51
AutoInt 877.75 48.90 0.54
LorentzFM 922.75 52.21 0.51
AFN 926.65 46.30 10.29
Fi-GNN 970.11 56.11 0.53
FinalMLP 1013.55 53.30 2.30
FINAL 944.84 49.00 1.03
MeLU 1123.94 52.11 0.51
MAMO 1299.84 52.71 0.71
TaNP 1089.23 31.25 0.54
ColdNAS 1190.11 25.81 1.81
ALDI 576.40 16.40 0.51
DropoutNetDeepFM 827.20 43.20 0.51
FinalMLP 1043.23 54.26 2.3
MetaEDeepFM 1235.44 44.27 0.51
FinalMLP 1319.18 52.70 2.30
CVARDeepFM 2372.70 44.20 0.52
FinalMLP 2516.50 52.89 2.31
GMEDeepFM 1099.30 43.99 0.52
FinalMLP 1101.44 54.21 2.31
MWUFDeepFM 1784.95 43.67 0.52
FinalMLP 2012.60 53.44 2.31
EmerG 996.26 46.10 0.82
3243KDD ’24, August 25–29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou
Table 4: Comparing EmerG with methods for emerging items with incremental interaction records, using various backbones.
Test performance obtained on MovieLens and Taobao. The best results are bolded, the second-best results are underlined.
MovieLensCold-start Phase Warm-up Phase A Warm-up Phase B Warm-up Phase C
AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%)
DropoutNetDeepFM 72.94(0.17)62.43(0.18)78.69(0.01)67.17(0.05)78.73(0.06)67.12(0.04)79.28(0.05)67.76(0.09)
FinalMLP 72.78(0.10)62.41(0.16)78.55(0.13)67.39(0.11)78.46(0.07)67.18(0.00)78.96(0.07)67.78(0.10)
MetaEDeepFM 71.82(0.70)61.76(0.30)79.53(0.25)67.96(0.15)80.27(0.09)68.31(0.12)80.47(0.04)68.46(0.12)
FinalMLP 59.50(4.80)53.22(4.77)72.38(3.44)62.11(3.03)75.34(3.64)64.35(3.19)76.98(3.05)65.83(2.64)
CVARDeepFM 73.58(0.21)63.15(0.12)78.23(0.10)67.03(0.26)80.28(0.06)68.76(0.12)81.06(0.04)69.33(0.14)
FinalMLP 65.91(2.58)59.02(1.14)77.33(0.16)65.94(0.45)77.90(0.37)66.58(0.26)78.86(0.32)67.62(0.17)
GMEDeepFM 71.54(0.13)64.31(0.10)75.81(0.20)67.50(0.26)78.10(0.18)69.26(0.20)79.15(0.12)69.95(0.16)
FinalMLP 71.56(0.28)63.79(0.37)76.48(0.36)67.81(0.44)78.94(0.28)68.86(0.34)80.04(0.19)69.79(0.34)
MWUFDeepFM 73.19(0.66)62.61(0.74)78.88(0.11)67.34(0.22)80.26(0.08)68.40(0.13)80.57(0.05)68.66(0.10)
FinalMLP 69.02(0.41)59.56(0.41)78.06(0.37)66.88(0.39)79.58(0.15)68.23(0.07)80.12(0.10)68.69(0.08)
EmerG 75.44(0.05)64.76(0.15)79.92(0.27)68.61(0.24)81.28(0.21)69.71(0.14)81.82(0.16)70.26(0.14)
TaobaoCold-start Phase Warm-up Phase A Warm-up Phase B Warm-up Phase C
AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%) AUC(%) F1(%)
DropoutNetDeepFM 60.41(0.09)13.53(0.02)62.48(0.26)14.55(0.10)62.60(0.26)14.68(0.12)63.12(0.17)14.82(0.08)
FinalMLP 60.86(0.14)13.69(0.09)63.37(0.08)14.75(0.03)63.43(0.01)14.84(0.03)63.98(0.03)15.04(0.04)
MetaEDeepFM 59.75(0.37)13.58(0.06)61.19(0.26)14.01(0.09)62.06(0.31)14.41(0.10)62.87(0.32)14.71(0.07)
FinalMLP 59.71(1.00)13.12(0.91)62.58(0.45)14.87(0.13)62.68(0.43)14.79(0.12)63.30(0.44)15.13(0.08)
CVARDeepFM 60.56(0.46)13.71(0.13)62.54(0.19)14.38(0.06)63.17(0.10)14.69(0.05)63.95(0.18)15.09(0.12)
FinalMLP 60.55(0.49)13.83(0.23)63.24(0.47)14.99(0.22)63.25(1.18)15.03(0.50)63.79(0.77)15.15(0.43)
GMEDeepFM 60.57(0.23)13.32(0.33)62.55(0.17)13.96(0.22)63.29(0.05)14.39(0.12)63.85(0.13)14.52(0.08)
FinalMLP 60.78(0.15)13.76(0.06)63.10(0.18)14.89(0.16)63.12(0.04)14.81(0.07)63.76(0.19)14.96(0.14)
MWUFDeepFM 59.65(0.46)13.44(0.15)62.08(0.17)14.20(0.07)63.03(0.13)14.63(0.07)63.79(0.12)14.93(0.06)
FinalMLP 60.36(0.11)13.73(0.08)63.26(0.07)14.00(0.02)63.37(0.22)14.79(0.05)63.96(0.23)15.09(0.05)
EmerG 61.58(0.03)13.99(0.05)63.56(0.03)15.02(0.06)63.76(0.02)15.15(0.01)64.22(0.02)15.21(0.02)
3244