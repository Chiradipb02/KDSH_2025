Is Aggregation the Only Choice? Federated Learning via
Layer-wise Model Recombination
Ming Hu
hu.ming.work@gmail.com
Singapore Management University
Singapore, SingaporeZhihao Yue
51215902034@stu.ecnu.edu.cn
East China Normal University
Shanghai, ChinaXiaofei Xie
xfxie@smu.edu.sg
Singapore Management University
Singapore, Singapore
Cheng Chen
chchen@sei.ecnu.edu.cn
East China Normal University
Shanghai, ChinaYihao Huang
huangyihao22@gmail.com
Nanyang Technological University
Singapore, SingaporeXian Wei
xwei@sei.ecnu.edu.cn
East China Normal University
Shanghai, China
Xiang Lian
xlian@kent.edu
Kent State University
Ohio, USAYang Liu
yangliu@ntu.edu.sg
Nanyang Technological University
Singapore, SingaporeMingsong Chen∗
mschen@sei.ecnu.edu.cn
East China Normal University
Shanghai, China
Abstract
Although Federated Learning (FL) enables global model training
across clients without compromising their raw data, due to the un-
evenly distributed data among clients, existing Federated Averaging
(FedAvg)-based methods suffer from the problem of low inference
performance. Specifically, different data distributions among clients
lead to various optimization directions of local models. Aggregat-
ing local models usually results in a low-generalized global model,
which performs worse on most of the clients. To address the above
issue, inspired by the observation from a geometric perspective
that a well-generalized solution is located in a flat area rather than
a sharp area, we propose a novel and heuristic FL paradigm named
FedMR (Federated Model Recombination). The goal of FedMR is
to guide the recombined models to be trained towards a flat area.
Unlike conventional FedAvg-based methods, in FedMR, the cloud
server recombines collected local models by shuffling each layer of
them to generate multiple recombined models for local training on
clients rather than an aggregated global model. Since the area of the
flat area is larger than the sharp area, when local models are located
in different areas, recombined models have a higher probability of
locating in a flat area. When all recombined models are located in
the same flat area, they are optimized towards the same direction.
We theoretically analyze the convergence of model recombination.
Experimental results show that, compared with state-of-the-art FL
methods, FedMR can significantly improve the inference accuracy
without exposing the privacy of each client.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671722CCS Concepts
•Computing methodologies →Distributed artificial intelligence .
Keywords
Federated Learning, Model Recombination, Non-IID, Generalization
ACM Reference Format:
Ming Hu, Zhihao Yue, Xiaofei Xie, Cheng Chen, Yihao Huang, Xian Wei,
Xiang Lian, Yang Liu, and Mingsong Chen. 2024. Is Aggregation the Only
Choice? Federated Learning via Layer-wise Model Recombination. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671722
1 Introduction
Federated Learning (FL) [ 25,32,40] has been widely acknowledged
as a promising means to design large-scale distributed Artificial
Intelligence (AI) applications, e.g., Artificial Intelligence of Things
(AIoT) systems [ 13,37,47], healthcare systems [ 2,46], and recom-
mender systems [ 35,42]. Unlike conventional Deep Learning (DL)
methods, the cloud-client architecture based FL supports the col-
laborative training of a global DL model among clients without
compromising their raw data [ 3]. In each FL training round, the
cloud server first dispatches the global model to its selected clients
for local training and then gathers the corresponding gradients of
trained models from clients for aggregation. In this way, clients can
train a global model without sharing data.
Although FL enables effective collaborative training among mul-
tiple clients while protecting data privacy, existing FL methods
suffer from the problem of “weight divergence” [ 21]. Especially
when the data on the clients are non-IID (Identically and Indepen-
dently Distributed) [ 1,38], the optimal directions of local models
on clients and the aggregated global model on the cloud server
are significantly inconsistent, resulting in serious inference per-
formance degradation of the global model. To improve FL per-
formance in non-IID scenarios, various FL methods have been
studied, e.g., client grouping-based methods [ 45], global control
 
1096
KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
variable-based methods [ 17,22,28], Knowledge Distillation (KD)-
based methods[ 30,39,48], and mutation-based methods [ 14]. The
basic ideas of these solutions are to guide the local training on
clients [ 17,22] or adjust parameter weights for model aggrega-
tion [30, 48].
Although these methods are promising in alleviating the impact
of data heterogeneity, most of them adopt the well-known Feder-
ated Averaging (FedAvg)-based paradigm, which may potentially
reduce generalization performance. This is mainly because FedAvg
paradigm only aggregates the parameters of collected local mod-
els and initializes local training by clients with the same global
models. Specifically, since the data distribution among clients is
different, the optimal directions of the local models are diverse.
On the one hand, although the aggregation operation can achieve
knowledge sharing among multiple local models, it can still neglect
the specific knowledge learned by local models, which seriously
limits the inference performance of the global model. On the other
hand, since FedAvg only uses the same global model for local train-
ing, FL training inevitably results in notorious stuck-at-local-search
problems during local training. As a result, the global model based
on simple statistical averaging cannot accurately reflect both in-
dividual efforts and the potential of local models in the search for
optimal global models. Therefore, how to overcome the shortcomings
of the FedAvg-based paradigm and improve the performance of FL in
non-IID scenarios is an important challenge.
Some recent research on model training indicates that, from
the perspective of the loss landscapes of DL models, optimal solu-
tions with well generalization performance often lie in flat valleys,
while the inferior ones are always located in sharp ravines [ 11,24].
Inspired by the above observation, to collaboratively train a well-
generalized model, FL needs to guide the local training towards a
more flat area. Since the direction of gradient descent is stochastic,
compared to using the same global model, using multiple global
models for local training has a greater probability that the existing
model can optimize to a flatter area. Since flat areas are usually
larger than sharp areas, intuitively, the exchange of the correspond-
ing parameters among multiple models rather than aggregation
can allow them to be displaced in the solution space. When a model
is stuck in a sharp area, the parameter exchange may make it es-
cape from the sharp area. With continuous training and parameter
exchange, when multiple models are located in the same flat area,
these models will optimize in the same direction, that is, the center
of the flat area.
Inspired by the above intuition, this paper proposes a novel FL
paradigm called FedMR (Federated Model Recombination), which
can effectively help the training of local models escape from sharp
area. Unlike FedAvg that aggregates all the collected local models in
each FL training round, FedMR randomly shuffles the parameters of
different local models within the same layers, and recombines them
to form new local models. In this way, FedMR can derive diversified
models that can effectively escape local optimal solutions for the
local training of clients. The main contributions of this paper can
be summarized as follows:
•We propose a novel FL paradigm named FedMR, which con-
tains a newly layer-wise model recombination method to
replace the traditional FedAvg-based model aggregation with
the aim of improving FL inference performance.•We introduce a two-stage training scheme for FedMR, which
combines the merits of both model recombination and ag-
gregation to accelerate the overall FL training process.
•We theoretically prove the convergence of FedMR in convex
scenarios and conduct empirical experiments to validate the
convergence of FedMR in non-convex scenarios.
•We conduct extensive experiments on various well-known
models and datasets to show both the effectiveness and com-
patibility of FedMR.
2 Related Work
To address the problem of uneven data distributions, exiting so-
lutions can be mainly classified into four categories, i.e., client
grouping-based methods, global control variable-based methods,
knowledge distillation-based methods, and mutation-based meth-
ods. The device grouping-based methods group and select clients for
aggregation based on the data similarity between clients. For ex-
ample, FedCluster [ 8] divides clients into multiple clusters and per-
forms multiple cycles of meta-update to boost the overall FL conver-
gence. Based on either sample size or model similarity, CluSamp [ 9]
groups clients to achieve a better client representativity and a re-
duced variance of client stochastic aggregation parameters in FL.
By modifying the penalty terms of loss functions during FL training,
theglobal control variable-based methods can be used to smooth
the FL convergence process. For example, FedProx [ 28] regularizes
local loss functions with the squared distance between local models
and the global model to stabilize the model convergence. Similarly,
SCAFFOLD [ 22] uses global control variables to correct the “client-
drift” problem in the local training process. Knowledge Distillation
(KD)-based methods adopt soft targets generated by the “teacher
model” to guide the training of “student models”. For example, by
leveraging a proxy dataset, Zhu et al. [ 48] proposed a data-free
knowledge distillation method named FedGen to address the het-
erogeneous FL problem using a built-in generator. With ensemble
distillation, FedDF [ 30] accelerates the FL training by training the
global model through unlabeled data on the outputs of local models.
Mutation-based methods attempt to mutate the global model to gen-
erate multiple mutated intermediate models for local training. For
example, FedMut [ 14] utilizes the gradients to mutate the global
model and dispatches the mutated models for local training.
Although the above methods can optimize FL performance from
different perspectives, since coarse-grained model aggregation is
performed, the inference capabilities of local models are still strongly
restricted. Furthermore, most of them cannot avoid non-negligible
communication and computation overheads or the risk of data
privacy exposure. In addition, many FL methods have been pro-
posed to address device heterogeneity problems. To effectively
train on devices with different hardware resources, some meth-
ods [ 4,18,20,41] utilize heterogeneous models for local training.
To avoid stragglers caused by uneven computing capability or un-
certainty [ 15], existing methods [ 16,44] attempt to perform a wise
client scheduling to achieve asynchronous FL training. Note that
this paper only focuses on the data heterogeneity problem.
To the best of our knowledge, FedMR is the first attempt using
model recombination rather than aggregation for FL. Since FedMR
considers the specific characteristics and efforts of local models, it
 
1097Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination KDD ’24, August 25–29, 2024, Barcelona, Spain.
can further mitigate the weight divergence problem, thus achieving
better inference performance than state-of-the-art FL methods.
3 Motivation
3.1 Intuition
Comparison between FedAvg and Independent Training. Fig-
ure 1 illustrates the FL training processes on the same loss landscape
using FedAvg and Independent training (Indep), respectively, where
the server of Indep just shuffles the received local models and then
randomly dispatches them to clients without aggregation. In each
subfigure, the local optima are located within the areas surrounded
by solid red lines. Note that since the upper surrounded area is
flatter than the lower surrounded area in the loss landscape, the
solutions within it will exhibit better generalization.
LocalModelGlobalModelLocalTraining
(a)FedAvg(b)IndependentTraining
Figure 1: Training processes on the same loss landscape.
As shown in Figure 1(a), along with the training process, the
aggregated global models denoted by red circles gradually move
toward the lower sharp area with inferior solutions, though the op-
timizations of some local models head toward the upper surrounded
area with better solutions. Such biased training is mainly because
the local training starts from the same global model in each FL
round. As an alternative, due to the lack of aggregation operation,
the local models of Indep may converge in different directions as
shown in Figure 1(b). In this case, even if some local training in
Idep achieves a better solution than the one obtained by FedAvg,
due to the diversified optimization directions of local models, such
an important finding can be eclipsed by the results of other local
models. Clearly, there is a lack of mechanisms for Indep that can
guide the overall training toward such superior solutions.
Intuition of Model Recombination. Based on the Indep train-
ing results shown in Figure 1(b), Figure 2 illustrates the intuition
of our model recombination method, where the FL training starts
from the three local models (denoted by yellow diamonds in round
1) obtained in figure 1(b). At the beginning of round 1, two of the
three local models are located in the sharp ravine. In other words,
without model recombination, the training of such two local mod-
els may get stuck in the lower surrounded area. However, due to
the weight adjustment by shuffling the layers among local models,
we can find that the three recombined models (denoted by yellow
squares) are sparsely scattered in the loss landscape, which enables
the local training escape from local optima. According to [ 10,43],
a small perturbation of the model weights can make it easier for
ModelRecombinationModelRecombinationModelRecombinationLocalModelRecombinedModelLocalTrainingModelRecombination
Round1Round2Round3Figure 2: An example of model recombination.
local training to jump out of sharp ravines rather than flat valleys.
In other words, the recombined models are more likely to converge
toward flat valleys along the local training. For example, in the end
of round 3, we can find that all three local models are located in the
upper surrounded area, where their aggregated model has better
generalization performance than the one achieved in Figure 1(a).
……𝒎𝟏"𝒎𝟐"𝒎𝑲"𝒎𝟏𝒎𝟐𝒎𝑲Shuffle
ModelModelModelDataDataDataLocalModelLocalModelLocalModelClient𝟏Client𝟐Client𝑲ModelRecombinationModelDispatchingModelUpload123
LocalClients…CloudServerUploadedModelsRecombinedModelsGlobalModelFLTrainingProcess
ModelDeploymentAggregation
Figure 3: Our FedMR approach
4 FedMR Approach
4.1 Overview of FedMR
Figure 3 presents the framework and workflow of FedMR, which
consists of two process, i.e., FL training process and the model
deployment process. In FL training process, unlike FedAvg-based
methods that use the same global model for local training, FedMR
adopts multiple homogeneous intermediate models for local train-
ing, where each client model is dispatched one model. Specifically,
each training round involves three specific steps as follows:
•Step 1 (Model Dispatching): The cloud server dispatches
𝐾intermediate models to 𝐾selected clients, respectively,
according to their indices, where 𝐾denotes the number of
activated clients participating in each FL training round.
Note that in FedMR different clients will receive different
models for the local training.
•Step 2 (Model Upload): Once its local training is finished,
a client needs to upload the parameters of its newly updated
local model to the cloud server.
•Step 3 (Model Recombination): The cloud server decom-
poses received local models into multiple layers individually
in the same manner and conducts the random shuffling of
 
1098KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
Algorithm 1 Implementation of FedMR
Input: i)𝑟𝑛𝑑, # of training rounds; ii) 𝑆𝑐, the set of clients; iii) 𝐾, #
of clients participating in each FL round.
Output:𝑤𝑔𝑙𝑏, the parameters of trained global model.
FedMR(𝑟𝑛𝑑,𝑆𝑑𝑒𝑣,𝐾)
1:𝐿𝑚←[𝑤1
1,𝑤2
1,...,𝑤𝐾
1]// initialize model list
2:for𝑟= 1, ...,𝑟𝑛𝑑do
3:𝐿𝑟←Random select 𝐾clients from 𝑆𝑐
/*parallel for block*/
4: for𝑖= 1, ...,𝐾do
5:𝑣𝑖
𝑟+1←ClientUpdate(𝐿𝑚[𝑖],𝐿𝑟[𝑖])
6:𝐿𝑚[𝑖]←𝑣𝑖
𝑟+1
7: end for
8:[𝑤1
𝑟+1,𝑤2
𝑟+1,...,𝑤𝐾
𝑟+1]←ModelRcombine (𝐿𝑚)
9:𝐿𝑚←[𝑤1
𝑟+1,𝑤2
𝑟+1,...,𝑤𝐾
𝑟+1]
10:end for
11:𝑤𝑔𝑙𝑏←1
𝐾Í𝐾
𝑖=1𝑤𝑖
𝑟𝑛𝑑+1
12:return𝑤𝑔𝑙𝑏
the same layers among different local models. Then, by con-
catenating layers from different sources in order, a new local
model can be reconstructed. Note that any decomposed layer
of the uploaded model will eventually be used by one and
only one of the recombined models.
In the model deployment process, the cloud server aggregates
all the intermediate models to generate a global model and deploys
the global model to all the local clients for specific tasks.
4.2 Implementation of FedMR
Algorithm 1 details the implementation of FedMR. Line 1 initializes
the model list 𝐿𝑚, which includes 𝐾initial models. Lines 2-10 per-
forms𝑟𝑛𝑑rounds of FedMR training. In each round, Line 3 selects
𝐾random clients to participate in the model training and creates
a client list 𝐿𝑟. Lines 4-7 conduct the local training on clients in
parallel, where Line 5 applies the local model 𝐿𝑚[𝑖]on client𝐿𝑟[𝑖]
for local training by using the function ClientUpdate, and Line 6
achieves a new local model after the local training. After the cloud
server receives all the 𝐾local models, Line 8 uses the function
ModelRcombine to recombine local models and generate 𝐾new
local models, which are saved in 𝐿𝑚as shown in Line 9. Finally,
Lines 11-12 will report an optimal global model that is generated
based on𝐿𝑚. Note that the cloud server will dispatch the global
model to all the clients for the purpose of inference rather than
local training. The following parts will detail the key components
of FedMR. Since FedMR cannot adapt to the secure aggregation
mechanism [ 5], to further protect privacy, we present an extended
secure recombination mechanism in Appendix B, which enables
the exchange of partial layers among clients before local training
or model uploading to ensure that the cloud server cannot directly
obtain the gradients of each local model.
4.2.1 Local Model Training. Unlike conventional FL methods that
conduct local training on clients starting from the same aggregated
model, in each training round FedMR uses different recombined
models (i.e., 𝐾models in the model list 𝐿𝑚) for the local training
…𝒎𝟏𝒎𝟐𝒎𝑲𝒎𝟏"𝒎𝟐"𝒎𝑲"………………………Shuffle𝑙!"!𝑙#"!𝑙$"!𝑙%"!𝑙!""𝑙#""𝑙$""𝑙%""𝑙!"#𝑙#"#𝑙$"#𝑙%"#𝑙𝑖𝑠𝑡!𝑙𝑖𝑠𝑡"𝑙𝑖𝑠𝑡#𝑙𝑖𝑠𝑡$(b)ExampleofModelRecombination…𝒎𝟏𝒎𝟐𝒎𝑲𝒎𝟏"𝒎𝟐"𝒎𝑲"……………Aggregation𝑙!"!𝑙#"!𝑙$"!𝑙%"!𝑙!""𝑙#""𝑙$""𝑙%""𝑙!"#𝑙#"#𝑙$"#𝑙%"#𝑙𝑖𝑠𝑡!𝑙𝑖𝑠𝑡"𝑙𝑖𝑠𝑡#𝑙𝑖𝑠𝑡$(a)ExampleofModelAggregationFigure 4: Example of model aggregation and recombination
purpose. Note that, in the whole training phase, FedMR only uses
𝐾(𝐾≤|𝑆𝑐|) models, since there are only 𝐾devices activated in
each training round. Let 𝑤𝑐𝑟be the parameters of some model that
is dispatched to the 𝑐𝑡ℎclient in the 𝑟𝑡ℎtraining round. In the 𝑟𝑡ℎ
training round, we dispatch the 𝑖𝑡ℎmodel in𝐿𝑚to its correspond-
ing client using 𝑤𝐿𝑟[𝑖]
𝑟 =𝐿𝑚[𝑖]. Based on the recombined model,
FedMR conducts the local training on client 𝐿𝑟[𝑖]as follows:
𝑣𝐿𝑟[𝑖]
𝑟+1=𝑤𝐿𝑟[𝑖]
𝑟−𝜂∇𝑓𝐿𝑟[𝑖](𝑤𝐿𝑟[𝑖]
𝑟),
𝑠.𝑡., 𝑓𝐿𝑟[𝑖](𝑤𝐿𝑟[𝑖]
𝑟)=1
|𝐷𝐿𝑟[𝑖]||𝐷𝐿𝑟[𝑖]|∑︁
𝑗=1ℓ(𝑤𝐿𝑟[𝑖]
𝑟 ;𝑥𝑗;𝑦𝑗),
where𝑣𝐿𝑟[𝑖]
𝑟 indicates parameters of the trained local model, 𝐷𝐿𝑟[𝑖]
denotes the dataset of client 𝐿𝑟[𝑖],𝜂is the learning rate, ℓ(·)is the
loss function, 𝑥𝑗is the𝑗𝑡ℎsample in𝐷𝐿𝑟[𝑖], and𝑦𝑗is the label of
𝑥𝑗. Once the local training is finished, the client needs to upload the
parameters of its trained local model to the cloud server by updating
𝐿𝑚using𝐿𝑚[𝑖]=𝑣𝐿𝑟[𝑖]
𝑟+1. Similar to traditional FL methods, in each
training round, FedMR needs to transmit the parameters of 2𝐾
models between the cloud server and its selected clients.
4.2.2 Model Recombination. Typically, a DL model consists of mul-
tiple layers, e.g., convolutional layers, pooling layers, and Fully
Connected (FC) layers. To simplify the description of our model
recombination method, we do not explicitly present the layer types
here. Let𝑤𝑥={𝑙𝑥
1,𝑙𝑥
2,...,𝑙𝑥𝑛}be the parameters of model 𝑥, where
𝑙𝑥
𝑖(𝑖∈[𝑛]) denotes the parameters of the 𝑖𝑡ℎlayer of model 𝑥.
In each FL round, FedMR needs to perform model recombina-
tion based on 𝐿𝑚to obtain new models for local training. Figure 4
presents an example of model aggregation and model recombina-
tion. When clients receive all the trained local models (i.e., 𝑚1,
𝑚2, ...,𝑚𝐾), the cloud server needs to decouple the layers of these
models individually. Note that since all the local models are ho-
mogeneous, the corresponding layers of the local models have the
 
1099Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination KDD ’24, August 25–29, 2024, Barcelona, Spain.
same structure. For example, the model 𝑚1can be decomposed
into four layers. Assuming that the local models are with an ar-
chitecture of 𝑤, to enable recombination, FedMR then constructs
𝑛lists, where the 𝑘𝑡ℎ(𝑘∈[𝑛]) list contains all the 𝑘𝑡ℎlayers of
the models in 𝐿𝑚. As an example shown in Figure 4(b), FedMR
constructs four lists (i.e., 𝑙𝑖𝑠𝑡 1-𝑙𝑖𝑠𝑡 4) for the𝐾models (i.e., 𝑚1-𝑚𝐾),
where each list consists of 𝐾elements (i.e., 𝐾layers with the same
index). After shuffling each list, FedMR generates |𝐿𝑚|recombined
models based on shuffled results. For example, the top three layers
of the recombined model 𝑚′
1come from the models 𝑚1,𝑚2and
𝑚𝐾, respectively. For comparison, Figure 4(a) presents an example
of model aggregation, which aggregates the layers of each list to
generate an aggregated model. The aggregation-based methods dis-
patch the aggregated model to 𝐾clients, while FedMR dispatches a
different recombined model to each client.
4.3 Two-Stage Training Scheme for FedMR
Although FedMR enables finer FL training, when starting from
blank models, FedMR converges more slowly than traditional FL
methods at the beginning. This is mainly because, due to the low
matching degree between layers in the recombined models, the
model recombination operation in this stage requires more local
training time to re-construct the new dependencies between layers.
To accelerate the overall convergence, we propose a two-stage
training scheme for FedMR, consisting of the aggregation-based pre-
training stage andmodel recombination stage. In the first stage, we
train the local models coarsely using the FedAvg-based aggregation,
which can quickly form a pre-trained global model. In the second
stage, from the pre-trained models, FedMR dispatches recombined
models to clients for local training. Due to the synergy of both FL
paradigms, the overall FedMR training time can be reduced.
4.4 Convergence Analysis
Based on the assumptions posed on the loss functions of local clients
in FedAvg [ 29], this subsection performs the convergence analysis
for FedMR.
Assumption 4.1. For𝑖∈{1,2,···,𝐾},𝑓𝑖is L-smooth satisfying
||∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑦)||≤𝐿||𝑥−𝑦||.
Assumption 4.2. For𝑖∈{1,2,···,𝐾},𝑓𝑖is𝜇-strongly convex
satisfying𝑓(𝑥)≥𝑓(𝑦)+(𝑥−𝑦)𝑇∇𝑓(𝑦)+𝜇
2||𝑥−𝑦||2, where𝜇≥0.
Assumption 4.3. The variance of stochastic gradients is upper
bounded by 𝜃2, and the expectation of squared norm of stochastic
gradients is upper bounded by 𝐺2, i.e.,E||∇𝑓𝑘(𝑤;𝜉)−∇𝑓𝑘(𝑤)||2≤
𝜃2,E||∇𝑓𝑘(𝑤;𝜉)||2≤𝐺2, where𝜉is a data batch of the 𝑘𝑡ℎclient
in the𝑡𝑡ℎFL round.
Based on the implementation of function ModelRecombine(·), we
derive the following two lemmas for the model recombination:
Lemma 4.4. Assume that in FedMR there are 𝐾clients participating
in every FL training round. Let {𝑣1𝑟,𝑣2𝑟,..,𝑣𝐾𝑟}and{𝑤1𝑟,𝑤2𝑟,..,𝑤𝐾𝑟}be
the set of trained local model weights and the set of recombined model
weights generated in the (𝑟−1)𝑡ℎround, respectively. Assume 𝑥is a
vector with the same size as that of 𝑣𝑘𝑟. We have
𝐾∑︁
𝑘=1𝑣𝑘
𝑟=𝐾∑︁
𝑘=1𝑤𝑘
𝑟,𝑎𝑛𝑑𝐾∑︁
𝑘=1||𝑣𝑘
𝑟−𝑥||2=𝐾∑︁
𝑘=1||𝑤𝑘
𝑟−𝑥||2.We prove Theorem 1 based on Lemmas 4.4. Please refer to Ap-
pendix A for the proof. Note that different from FedAvg, Lemmas 4.4
is the key lemma for the proof of FedMR.
Theorem 1. (Convergence of FedMR) Let Assumption 4.1, 4.2, and
4.3 hold. Assume that 𝐸is the number of SGD iterations conducted
within one FL round, model recombination is conducted at the end of
each FL round, and the whole training terminates after 𝑛FL rounds.
Let𝑇=𝑛×𝐸be the total number of SGD iterations conducted so far,
and𝜂𝑘=2
𝜇(𝑇+𝛾)be the learning rate. We can have
E[𝑓(𝑤𝑇)]−𝑓★≤𝐿
2𝜇(𝑇+𝛾)[4𝐵
𝜇+𝜇(𝛾+1)
2Δ1],
where𝐵=10𝐿Γ+4(𝐸−1)2𝐺2,𝑤𝑇=Í𝐾
𝑘=1𝑤𝑘
𝑇.
Theorem 1 indicates that the difference between the current
loss𝑓(𝑤𝑇)and the optimal loss 𝑓★is inversely related to 𝑡. From
Theorem 1, we can find that the convergence rate of FedMR is
similar to that of FedAvg, which has been analyzed in [29].
5 Experimental Results
5.1 Experimental Settings
To evaluate the effectiveness of FedMR, we implemented FedMR on
top of a cloud-based architecture. Since it is impractical to allow all
the clients to get involved in the training processes simultaneously,
we assumed that there are only 10% of clients participating in the
local training in each FL round. To enable fair comparison, all the
investigated FL methods including FedMR set SGD optimizer with
a learning rate of 0.01 and a momentum of 0.9. For each client, we
set the batch size of local training to 50, and performed five epochs
for each local training. All the experimental results were obtained
from an Ubuntu workstation with Intel i9 CPU, 32GB memory, and
NVIDIA RTX 3080 GPU.
Baseline Method Settings. We compared the test accuracy of
FedMR with seven baseline methods, i.e., FedAvg [ 32], FedProx [ 28],
FedGen [ 48], CluSamp [ 9], FedExP [ 19], FedASAM [ 6], and Fed-
Mut [14]. Here, FedAvg is the most classical FL method, while the
other five methods are state-of-the-art (SOTA) representations of
the four kinds of FL optimization methods introduced in the re-
lated work section. Specifically, FedProx, FedExP, and FedASAM are
global control variable-based methods, FedGen is a KD-based ap-
proach, CluSamp is a device grouping-based method, and FedMut is
a mutation-based method. For FedProx, we used a hyper-parameter
𝜇to control the weight of its proximal term, where the best values
of𝜇for CIFAR-10, CIFAR-100, and FEMNISTvare 0.01, 0.001, and
0.1, respectively. For FedGen, we adopted the same server settings
in [48]. For CluSamp, the clients were clustered based on the model
gradient similarity described in [9].
Dataset Settings. We investigated the performance of our ap-
proach on three well-known datasets, i.e., CIFAR-10, CIFAR-100
[23], and FMNIST [ 7]. We adopted the Dirichlet distribution [ 12]
to control the heterogeneity of client data for both CIFAR-10 and
CIFAR-100. Here, the notation 𝐷𝑖𝑟(𝛼)indicates a different Dirich-
let distribution controlled by 𝛼, where a smaller 𝛼means higher
data heterogeneity of clients. Note that, different from datasets
CIFAR-10 and CIFAR-100, the raw data of FEMNIST are naturally
 
1100KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
non-IID distributed. Since FEMNIST takes various kinds of data het-
erogeneity into account, we did not apply the Dirichlet distribution
on FEMNIST. For both CIFAR-10 and CIFAR-100, we assumed that
there are 100 clients in total participating in FL. For FEMNIST, we
only considered one non-IID scenario involving 180 clients, where
each client hosts more than 100 local data samples.
Model Settings. To demonstrate the pervasiveness of our ap-
proach, we developed different FedMR implementations based on
three different models (i.e., CNN, ResNet-20, VGG-16). Here, we
obtained the CNN model from [ 32], which consists of two convo-
lutional layers and two FC layers. When conducting FedMR based
on the CNN model, we directly applied the model recombination
for local training on it without pre-training a global model, since
CNN here only has four layers. We obtained both ResNet-20 and
VGG-16 models from Torchvision [ 36].·When performing FedMR
based on ResNet-20 and VGG-16, due to the deep structure of both
models, we adopted the two-stage training scheme, where the first
stage lasts for 100 rounds to obtain a pre-trained global model.
5.2 Validation for Intuitions
Independent Training. Based on the settings presented in Sec-
tion 5.1, we conducted the experiments to evaluate the effectiveness
of each local model in Indep. The FL training is based on the ResNet-
20 model and dataset CIFAR-10, where we set 𝛼=0.5for non-IID
scenarios. Figures 5 compares Indep with FedAvg from the per-
spectives of both test loss and inference accuracy. Due to the space
limitation, for Indep here we only present the results of its four
random local models (denoted by Model-1, Model-2, Model-3, and
Model-4). To enable a fair comparison with FedAvg, although there
is no aggregated global model in Indep, we considered the aggre-
gated model of all its local models for each FL round, whose results
are indicated by the notion “IndepAggr”. From Figure 5, we can
find all the local models in Indep can achieve higher accuracy and
lower loss than those of FedAvg, though their loss and accuracy
curves fluctuate more sharply. Moreover, IndepAggr exhibits much
worse performance than the other references. This is mainly be-
cause, according to the definition of Indep, each local model needs
to traverse multiple clients along with the FL training processes,
where the optimization directions of client models differ in the
corresponding loss landscape.
 0 1 2 3 4 5 6 7 8 9 10
 0 100  200  300  400  500  600  700  800  900  1000Loss
Communication RoundsFedAvg
Model-1
Model-2
Model-3
Model-4
IndepAggr
(a) Test Loss
 10 20 30 40 50 60 70
 0 100  200  300  400  500  600  700  800  900  1000Accuracy (%)
Communication RoundsFedAvg
Model-1
Model-2
Model-3
Model-4
IndepAggr (b) Accuracy
Figure 5: FedAvg vs. Indep.
Model Recombination. To validate the intuition about the
impacts of model recombination as presented in Section 3, we con-
ducted three experiments on CIFAR-10 dataset using ResNet-20model. Our goal is to figure out the following three questions: i)
by using model recombination, can all the models eventually have
the same optimization direction; ii) compared with FedAvg, can the
global model of FedMR eventually converge into a more flat solu-
tion; and iii) can the global model of FedMR eventually converge
to a more generalized solution?
 0.95 0.96 0.97 0.98 0.99 1
 0 200  400  600  800  1000  1200  1400  1600  1800  2000Similarity
Communication RoundsDir(0.1)
Dir(0.5)
Dir(1.0)
IID
Figure 6: Cosine similarity of local models in FedMR.
Figure 6 presents the average cosine similarity between all the
intermediate models, taking four different client data distributions
into account. We can observe that the model similarity decreases
first and gradually increases in all the investigated IID and non-IID
scenarios. Due to the nature of Stochastic Gradient Descent (SGD)
mechanism and the data heterogeneity among clients, all local
models are optimized toward different directions at the beginning of
training. However, as the training progresses, most local models will
be located in the same flat valleys, leading to similar optimization
directions for local models. These results are consistent with our
intuition as shown in Figure 2.
246810
(a) FedAvg w/ 𝛼=0.1
246810 (b) FedMR w/ 𝛼=0.1
246810
(c) FedAvg w/ 𝛼=1.0
246810 (d) FedMR w/ 𝛼=1.0
Figure 7: Comparison of loss landscapes with different FL
and client data settings.
Figure 7 compares the loss landscapes of final global models
obtained by FedAvg and FedMR with different client data settings,
respectively. We can find that, compared with FedMR counterparts,
 
1101Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination KDD ’24, August 25–29, 2024, Barcelona, Spain.
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
 0 200  400  600  800  1000  1200  1400  1600  1800  2000Loss
Communication RoundsFedAvg
FedMR
(a)𝛼=0.1
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
 0 200  400  600  800  1000  1200  1400  1600  1800  2000Loss
Communication RoundsFedAvg
FedMR (b)𝛼=0.5
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
 0 200  400  600  800  1000  1200  1400  1600  1800  2000Loss
Communication RoundsFedAvg
FedMR
(c)𝛼=1.0
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
 0 200  400  600  800  1000  1200  1400  1600  1800  2000Loss
Communication RoundsFedAvg
FedMR (d) IID
Figure 8: Comparison of test losses of FedAvg and FedMR
with different client data settings.
the global models trained by FedAvg are located in sharper solutions,
indicating the generalization superiority of final global models
achieved by FedMR.
Figure 8 compares the test losses for the global models of FedAvg
and FedMR (without using two-stage training) within different
IID and non-IID scenarios. Note that here, the global models of
FedMR are only for the purpose of fair comparison rather than
local model initialization. Due to the superiority in generalization,
we can observe that the models trained by FedMR outperform those
by FedAvg for all four cases.
5.3 Performance Comparison
We compared the performance of our FedMR approach with seven
SOTA baselines. For datasets CIFAR-10 and CIFAR-100, we consid-
ered both IID and non-IID scenarios (with 𝛼=0.1,0.5,1.0, respec-
tively). For dataset FEMNIST, we considered its original non-IID
settings [7].
5.3.1 Comparison of Test Accuracy. Table 1 compares FedMR with
the SOTA FL methods, considering both non-IID and IID scenarios
based on three different DL models. The first two columns denote
the model type and dataset type, respectively. Note that to enable
fair comparison, we cluster the test accuracy results generated
by the FL methods based on the same type of local models. The
third column shows different distribution settings for client data,
indicating the data heterogeneity of clients. The fourth column has
eight sub-columns, which present the test accuracy information
together with its standard deviation for all the investigated FL
methods, respectively.
From Table 1, we can observe that FedMR can achieve the highest
test accuracy in all the scenarios regardless of model type, dataset
type, and data heterogeneity. For CIFAR-10 and CIFAR-100, we can
find that FedMR outperforms the seven baseline methods signifi-
cantly in both non-IID and IID scenarios. For example, when dealingwith a non-IID CIFAR-10 scenario ( 𝛼=0.1) using ResNet-20-based
models, FedMR achieves test accuracy with an average of 58.40%,
while the second highest average test accuracy obtained by FedMut
is only 50.75%. Note that the performance of FedMR on FEMNIST
is not as notable as the one on both CIFAR-10 and CIFAR-100. This
is mainly because the classification task on FEMNIST is much sim-
pler than the ones applied on datasets CIFAR-10 and CIFAR-100,
which leads to the high test accuracy of the seven baseline methods.
However, even in this case, FedMR can still achieve the best test
accuracy among all the investigated FL methods.
5.3.2 Comparison of Model Convergence. Figure 9 presents the
convergence trends of the seven FL methods (including FedMR)
on the CIFAR-100 dataset. Note that here the training of FedMR is
based on our proposed two-stage training scheme, where the first
stage uses 100 FL training rounds to achieve a pre-trained model.
Here, to enable fair comparison, the test accuracy of FedMR at some
FL training rounds is calculated by an intermediate global model,
which is an aggregated version of all the local models within that
round. The four sub-figures show the results for different data dis-
tributions of clients. This figure shows that FedMR outperforms the
other six FL methods consistently in both non-IID and IID scenarios.
This is mainly because FedMR can easily escape from the stuck-at-
local-search due to the model recombination operation in each FL
round. Moreover, due to the fine-grained training, we can observe
that the learning curves in each sub-figure are much smoother
than the ones of other FL methods. We also compared CNN- and
VGG-16-based FL methods and found similar observations.
5.4 Ablation Study
5.4.1 Impacts of Activated Clients . Figure 10 compares the learn-
ing trends between FedMR and six baselines for a non-IID scenario
(𝛼=0.1) with both ResNet-20 model and CIFAR-10 dataset, where
the numbers of activated clients are 5,10,20,50, and 100, respec-
tively. From Figure 10, we can observe that FedMR achieves the
best inference performance for all cases. We can also observe that
too few activated clients (i.e., 𝐾=5) result in a degradation of the
accuracy of the global model in all the FL methods. We find that
when the number of activated clients increases, the convergence
fluctuations reduce significantly and FedMR achieves the smallest
fluctuations compared to all the baselines for all cases.
5.4.2 Impacts of Model Layer Partitioning. To show the effective-
ness of our layer-wise model recombination scheme, we evaluated
the FedMR performance using different model layer partitioning
strategies. We use “FedMR-p 𝑥” to denote that the model is divided
into⌈1
𝑥⌉(𝑥∈(0,1.0]) segments, where the model recombination
is based on segments rather than layers. Note that FedMR does
not divide a single layer into multiple segments. Instead, in FedMR
each segment involves multiple layers (i.e., one or more layers), and
the FedMR takes the recombination over segments among different
clients. Specifically, for the 𝑗𝑡ℎlayer, it belongs to ⌈𝑗
𝑥×𝑁⌉𝑡ℎseg-
ment. Since 𝑥∈(0,1.0], each segment contains at least one layer.
Note that𝑥=1.0indicates an extreme case, where local models are
randomly dispatched to clients without recombination.
Figure 11 presents the ablation study results on CIFAR-10 dataset
using ResNet-20-based and VGG-16-based FedMR, where the data
 
1102KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
Table 1: Test accuracy comparison for both non-IID and IID scenarios using three DL models
Model Datas.Heter. Test Accuracy (%)
Set. FedAvg FedProx FedGen CluSamp FedExP FedASAM FedMut FedMR
CNNCifar-100.150.66±1.1850.61±1.2150.27±2.0350.24±1.0453.20±1.3348.83±1.8652.88±0.5954.63±0.55
0.554.42±0.7654.28±1.2953.66±0.6855.07±0.9355.12±0.5752.02±0.4357.29±0.6159.81±0.60
1.057.03±0.6256.44±0.6856.19±0.5456.16±0.5657.12±0.3455.08±0.6158.88±0.4760.77±0.48
𝐼𝐼𝐷 57.21±0.2757.00±0.1157.35±0.1958.13±0.3557.05±0.1554.60±0.1158.81±0.2163.74±0.18
Cifar-1000.129.12±0.5229.44±0.7326.14±0.9228.51±1.1430.43±0.3928.95±0.4831.23±0.3534.47±0.54
0.532.41±0.7732.48±0.8129.19±0.6732.63±0.6033.12±0.5132.06±0.9834.46±0.6937.41±0.30
1.032.66±0.5133.10±0.4129.94±0.5132.65±0.4833.32±0.3832.45±0.6135.12±0.4239.15±0.30
𝐼𝐼𝐷 32.75±0.2032.57±0.2130.95±0.3232.77±0.1132.48±0.1432.35±0.2934.49±0.2340.64±0.17
FEMNIST− 82.97±0.3783.15±0.4182.35±0.4082.31±0.3283.28±0.2983.49±0.2783.62±0.3383.76±0.24
ResNet-20Cifar-100.142.14±3.9143.25±3.1844.19±2.4241.64±2.0445.18±2.4345.22±4.0650.75±1.8559.20±1.22
0.558.70±0.8659.33±0.7760.64±0.8358.74±0.8259.74±0.9263.49±1.1063.34±0.7072.41±0.17
1.064.33±0.2564.75±0.3364.41±0.2963.42±0.4564.48±0.3167.62±0.4168.09±0.2575.16±0.31
𝐼𝐼𝐷 65.72±0.2265.95±0.2366.31±0.2365.36±0.1865.58±0.2469.65±0.1069.77±0.1977.48±0.10
Cifar-1000.134.22±1.0134.52±0.6835.76±1.1133.23±0.7836.09±0.7137.31±0.6638.79±0.5544.61±1.48
0.542.16±0.4341.37±0.4945.03±0.9641.54±0.5241.96±0.5644.29±0.5246.55±0.5554.26±0.45
1.043.32±0.3843.00±0.4546.60±0.3943.63±0.3943.68±0.2346.74±0.3148.41±0.2555.72±0.36
𝐼𝐼𝐷 45.14±0.2845.40±0.2748.33±0.2544.76±0.2445.04±0.2448.59±0.2048.65±0.1759.24±0.32
FEMNIST− 79.09±0.5478.89±0.5079.56±0.3478.75±0.2779.15±0.3481.20±0.4178.98±0.4581.27±0.31
VGG-16Cifar-100.162.28±5.7263.19±5.1565.97±3.8262.00±3.1964.78±5.6965.52±4.9669.15±2.1674.49±0.92
0.578.82±0.2178.49±0.2678.98±0.1178.09±0.4879.30±0.4379.12±0.2580.07±0.1984.24±0.37
1.079.53±0.3479.52±0.3080.08±0.2479.67±0.4579.76±0.2080.08±0.3280.85±0.9985.12±0.13
𝐼𝐼𝐷 79.96±0.0579.79±0.0780.13±0.0579.66±0.0679.89±0.0580.66±0.0982.20±0.0585.66±0.15
Cifar-1000.147.29±0.9648.02±0.6849.11±1.6047.38±1.4749.36±0.5548.78±1.0351.30±1.0055.33±0.72
0.555.60±0.5555.45±0.7056.29±0.8454.45±0.5856.40±0.4756.73±0.4158.02±0.3165.07±0.25
1.056.05±0.4555.75±0.3657.96±0.3255.70±0.3756.69±0.2557.46±0.2558.53±0.3165.66±0.15
𝐼𝐼𝐷 57.22±0.2856.65±0.2358.47±0.1657.33±0.1757.55±0.1657.96±0.1158.62±0.0866.33±0.10
FEMNIST− 83.96±0.4384.27±0.3284.39±0.2883.64±0.2783.99±0.3284.59±0.2183.97±0.5785.36±0.21
 10 15 20 25 30 35 40 45
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR
(a)𝛼=0.1
 10 15 20 25 30 35 40 45 50 55
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (b)𝛼=0.5
 10 15 20 25 30 35 40 45 50 55 60
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (c)𝛼=1.0
 10 15 20 25 30 35 40 45 50 55 60
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (d) IID
Figure 9: Learning curves of FL methods based on ResNet-20 model for CIFAR-100 dataset.
on clients are non-IID distributed ( 𝛼=1.0). Note that all the cases
here did not use the two-stage training scheme. From this figure, we
can find that FedMR outperforms the other variants significantly.
Moreover, when the granularity of partitioning goes coarser, the
classification performance of FedMR becomes worse.
5.4.3 Two-stage Training Scheme. To demonstrate the effective-
ness of our proposed two-stage training scheme, we conducted
experiments on CIFAR-10 dataset using ResNet-20-based and VGG-
16-based FedMR, where the data on clients are non-IID distributed
(𝛼=1.0). Figure 12 presents the learning trends of FedMR withfive different two-stage training settings. Here, we use the nota-
tion “FedMR- 𝑛” to denote that the first stage involves 𝑛rounds
of model aggregation-based local training to obtain a pre-trained
global model, while the remaining rounds conduct local training
based on our proposed model recombination-based method. From
Figure 12, we can observe that the two-stage training-based FedMR
methods (i.e., FedMR-50 and FedMR-100) achieve the best perfor-
mance from the perspectives of test accuracy and convergence
rate. Note that our two-stage training scheme can achieve a more
significant improvement on the case using VGG-16 model, which
has a much larger size than ResNet-20 model. This is because the
 
1103Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination KDD ’24, August 25–29, 2024, Barcelona, Spain.
 10 15 20 25 30 35 40 45 50 55 60
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR
(a)𝐾=5
 10 15 20 25 30 35 40 45 50 55 60
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (b)𝐾=10
 10 15 20 25 30 35 40 45 50 55 60 65
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (c)𝐾=20
 10 15 20 25 30 35 40 45 50 55 60 65
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (d)𝐾=50
 10 15 20 25 30 35 40 45 50 55 60 65
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedAvg
FedProx
FedGen
CluSamp
FedExP
FedASAM
FedMut
FedMR (e)𝐾=100
Figure 10: Comparison of FL methods using ResNet-20 model on CIFAR-10 dataset with 𝛼=0.1.
 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedMR
FedMR-p0.1
FedMR-p0.2
FedMR-p0.5
FedMR-p1.0
(a) ResNet-20
 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85
 0  200  400  600  800  1000Accuracy(%)
Communication RoundsFedMR
FedMR-p0.1
FedMR-p0.2
FedMR-p0.5
FedMR-p1.0 (b) VGG-16
Figure 11: Learning curves for partitioning strategies.
 40 45 50 55 60 65 70 75 80
 0 200  400  600  800 1000  1200  1400  1600  1800  2000Accuracy(%)
Communication RoundsFedMR-0
FedMR-50
FedMR-100
FedMR-200
FedMR-300
(a) ResNet-20
 40 45 50 55 60 65 70 75 80 85
 0 100  200  300  400  500  600  700  800  900 1000Accuracy(%)
Communication RoundsFedMR-0
FedMR-50
FedMR-100
FedMR-200
FedMR-300 (b) VGG-16
Figure 12: Learning curves for two-stage training settings.
fine-grained FedMR without the first-stage training is not good
at dealing with large-size models at the beginning of FL training,
which requires many more training rounds than the coarse-grained
aggregation-based methods to achieve a given preliminary clas-
sification accuracy target. By resorting to the two-stage training
scheme, such a slow convergence problem can be greatly mitigated.
5.5 Discussions
5.5.1 Privacy Preserving. Similar to traditional FedAvg-based FL
methods, FedMR does not require clients to send their data to the
cloud server, thus the data privacy can be mostly guaranteed by
the secure clients themselves. Since our model recombination oper-
ation breaks the dependencies between model layers and conducts
the shuffling of layers among models, in practice, it is hard for
adversaries to restore the confidential data from a fragmentary re-
combined model without knowing the sources of layers. We present
a secure recombination mechanism to avoid privacy leakage from
the cloud server. Please see Appendix B for more details. Our se-
cure recombination mechanism ensures that the cloud server onlyreceives recombined models from clients, which means that the
cloud server cannot restore the model of each client.
5.5.2 Limitations. As a novel FL paradigm, FedMR shows much
better inference performance than most SOTA FL methods. Al-
though this paper proposed an efficient two-stage training scheme
to accelerate the overall FL training processes, there still exist nu-
merous chances (e.g., client selection strategies, dynamic combina-
tion of model aggregation and model recombination operations)
to enable further optimization on the current version of FedMR.
Meanwhile, the current version of FedMR does not consider person-
alization [ 31,33] and fairness [ 26,27], two very important topics
that deserve to be studied in the future.
6 Conclusion
Due to the coarse-grained aggregation of FedAvg as well as the
uniform client model initialization, when dealing with uneven data
distribution among clients, existing Federated Learning (FL) meth-
ods greatly suffer from the problem of low inference performance.
To address this problem, this paper presented a new FL paradigm
named FedMR, which enables different layers of local models to be
trained on different clients. Since FedMR supports both fine-grained
model recombination and diversified local training initialization,
it enables effective and efficient search for superior generalized
models for all clients. Comprehensive experimental results show
both the effectiveness and pervasiveness of our proposed method
in terms of inference accuracy and convergence rate.
7 Acknowledgement
This work was supported by Natural Science Foundation of China
(62272170), and “Digital Silk Road” Shanghai International Joint Lab
of Trustworthy Intelligent Software (22510750100), the National
Research Foundation, Singapore, and DSO National Laboratories
under the AI Singapore Programme (AISG Award No: AISG2-GC-
2023-008), the National Research Foundation, Singapore, and the
Cyber Security Agency under its National Cybersecurity R&D Pro-
gramme (NCRP25-P04-TAICeN), Natural Science Foundation (NSF
CCF-2217104) and National Natural Science Foundation of China
(62306116). Any opinions, findings and conclusions or recommen-
dations expressed in this material are those of the author(s) and do
not reflect the views of National Research Foundation, Singapore
and Cyber Security Agency of Singapore.
 
1104KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
References
[1]Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul What-
mough, and Venkatesh Saligrama. 2020. Federated Learning Based on Dynamic
Regularization. In Proc. of International Conference on Learning Representations.
[2]Mohammed Adnan, Shivam Kalra, Jesse C Cresswell, Graham W Taylor, and
Hamid R Tizhoosh. 2022. Federated learning and differential privacy for medical
image analysis. Scientific reports 12, 1 (2022), 1953.
[3]Naman Agarwal, Peter Kairouz, and Ziyu Liu. 2021. The skellam mechanism
for differentially private federated learning. Advances in Neural Information
Processing Systems 34 (2021), 5052–5064.
[4]Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang. 2022. Fedrolex: Model-
heterogeneous federated learning with rolling sub-model extraction. Advances
in neural information processing systems 35 (2022), 29677–29690.
[5]Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Practi-
cal secure aggregation for privacy-preserving machine learning. In Proc. of ACM
SIGSAC Conference on Computer and Communications Security. 1175–1191.
[6]Debora Caldarola, Barbara Caputo, and Marco Ciccone. 2022. Improving gen-
eralization in federated learning by seeking flat minima. In Proc. of European
Conference on Computer Vision. 654–672.
[7]Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn `y,
H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: A
benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).
[8]Cheng Chen, Ziyi Chen, Yi Zhou, and Bhavya Kailkhura. 2020. Fedcluster:
Boosting the convergence of federated learning via cluster-cycling. In Proc. of
IEEE International Conference on Big Data. 5017–5026.
[9]Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. 2021. Clustered
sampling: Low-variance and improved representativity for clients selection in
federated learning. In Proc. of International Conference on Machine Learning.
3407–3416.
[10] Moritz Hardt, Ben Recht, and Yoram Singer. 2016. Train faster, generalize better:
Stability of stochastic gradient descent. In Proc. of International Conference on
Machine Learning. 1225–1234.
[11] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. Neural computation
9, 1 (1997), 1–42.
[12] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. 2019. Measuring the effects
of non-identical data distribution for federated visual classification. arXiv preprint
arXiv:1909.06335 (2019).
[13] Ming Hu, E Cao, Hongbing Huang, Min Zhang, Xiaohong Chen, and Mingsong
Chen. 2023. AIoTML: A Unified Modeling Language for AIoT-Based Cyber-
Physical Systems. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems (2023).
[14] Ming Hu, Yue Cao, Anran Li, Zhiming Li, Chengwei Liu, Tianlin Li, Mingsong
Chen, and Yang Liu. 2024. FedMut: Generalized Federated Learning via Stochastic
Mutation. In Proc. of the AAAI Conference on Artificial Intelligence, Vol. 38. 12528–
12537.
[15] Ming Hu, Wenxue Duan, Min Zhang, Tongquan Wei, and Mingsong Chen. 2020.
Quantitative timing analysis for cyber-physical systems using uncertainty-aware
scenario-based specifications. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems 39, 11 (2020), 4006–4017.
[16] Ming Hu, Zeke Xia, Dengke Yan, Zhihao Yue, Jun Xia, Yihao Huang, Yang Liu,
and Mingsong Chen. 2023. GitFL: Uncertainty-Aware Real-Time Asynchronous
Federated Learning Using Version Control. In Proc. of IEEE Real-Time Systems
Symposium. 145–157.
[17] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei,
and Yong Zhang. 2021. Personalized Cross-Silo Federated Learning on Non-IID
Data. In Proc. of the AAAI Conference on Artificial Intelligence.
[18] Fatih Ilhan, Gong Su, and Ling Liu. 2023. Scalefl: Resource-adaptive federated
learning with heterogeneous clients. In Proc. of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 24532–24541.
[19] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. 2023. Fedexp: Speeding
up federated averaging via extrapolation. arXiv preprint arXiv:2301.09604 (2023).
[20] Chentao Jia, Ming Hu, Zekai Chen, Yanxin Yang, Xiaofei Xie, Yang Liu, and
Mingsong Chen. 2024. AdaptiveFL: Adaptive Heterogeneous Federated Learning
for Resource-Constrained AIoT Systems. In Proc. of Design Automation Conference.
1–6.
[21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al .2021. Advances and open problems in federated learning.
Foundations and Trends® in Machine Learning 14, 1–2 (2021), 1–210.
[22] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh. 2020. SCAFFOLD: Stochastic controlled
averaging for federated learning. In Proc. of International Conference on Machine
Learning. 5132–5143.
[23] Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.
Master’s thesis, University of Tront (2009).[24] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. 2021. Asam:
Adaptive sharpness-aware minimization for scale-invariant learning of deep
neural networks. In Proc. of International Conference on Machine Learning. PMLR,
5905–5914.
[25] Anran Li, Rui Liu, Ming Hu, Luu Anh Tuan, and Han Yu. 2023. Towards inter-
pretable federated learning. arXiv preprint arXiv:2302.13473 (2023).
[26] Tianlin Li, Qing Guo, Aishan Liu, Mengnan Du, Zhiming Li, and Yang Liu. 2023.
FAIRER: fairness as decision rationale alignment. In Proc. of International Confer-
ence on Machine Learning. 19471–19489.
[27] Tianlin Li, Zhiming Li, Anran Li, Mengnan Du, Aishan Liu, Qing Guo, Guozhu
Meng, and Yang Liu. 2023. Fairness via group contribution matching. In Proc. of
International Joint Conference on Artificial Intelligence. 436–445.
[28] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020. Federated optimization in heterogeneous networks. Proc.
of Machine Learning and Systems 2 (2020), 429–450.
[29] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
2020. On the Convergence of FedAvg on Non-IID Data. In Proc. of International
Conference on Learning Representations.
[30] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensemble
distillation for robust model fusion in federated learning. Advances in Neural
Information Processing Systems 33 (2020), 2351–2363.
[31] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022. Layer-wised model ag-
gregation for personalized federated learning. In Proc. of the IEEE/CVF conference
on computer vision and pattern recognition. 10092–10101.
[32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Proc. of Artificial intelligence and statistics.
1273–1282.
[33] Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed, Mike Rabbat, Maziar
Sanjabi, and Lin Xiao. 2022. Federated learning with partial model personalization.
InProc. of International Conference on Machine Learning. 17716–17758.
[34] Sebastian Urban Stich. 2019. Local SGD Converges Fast and Communicates Little.
InProc. of International Conference on Learning Representations.
[35] Ben Tan, Bo Liu, Vincent Zheng, and Qiang Yang. 2020. A federated recommender
system for online services. In Proc. of ACM Conference on Recommender Systems.
579–581.
[36] TorchvisionModel. 2019. Models and pre-trained weight. https://pytorch.org/
vision/stable/models.html.
[37] Haozhao Wang, Yabo Jia, Meng Zhang, Qinghao Hu, Hao Ren, Peng Sun, Yong-
gang Wen, and Tianwei Zhang. 2024. FedDSE: Distribution-aware Sub-model
Extraction for Federated Learning over Resource-constrained Devices. In Proc. of
the ACM on Web Conference. 2902–2913.
[38] Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. 2020. Optimizing federated
learning on non-iid data with reinforcement learning. In Proc. of IEEE Conference
on Computer Communications. 1698–1707.
[39] Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng Zhan, and Zhigang
Zeng. 2023. Dafkd: Domain-aware federated knowledge distillation. In Proc. of the
IEEE/CVF conference on Computer Vision and Pattern Recognition. 20412–20421.
[40] Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li, and Tianwei Zhang.
2023. FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggre-
gation. In Proc. of International Conference on Learning Representations.
[41] Jiaqi Wang, Suhan Cui, and Fenglong Ma. 2023. FedLEGO: Enabling Heterogenous
Model Cooperation via Brick Reassembly in Federated Learning. In Proc. of
International Workshop on Federated Learning for Distributed Data Mining.
[42] Jie Wen, Zhixia Zhang, Yang Lan, Zhihua Cui, Jianghui Cai, and Wensheng Zhang.
2023. A survey on federated learning: challenges and applications. International
Journal of Machine Learning and Cybernetics 14, 2 (2023), 513–535.
[43] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. 2020. Adversarial weight pertur-
bation helps robust generalization. Advances in Neural Information Processing
Systems 33 (2020), 2958–2969.
[44] Zeke Xia, Ming Hu, Dengke Yan, Xiaofei Xie, Tianlin Li, Anran Li, Junlong
Zhou, and Mingsong Chen. 2024. CaBaFL: Asynchronous Federated Learning via
Hierarchical Cache and Feature Balance. arXiv preprint arXiv:2404.12850 (2024).
[45] Ming Xie, Guodong Long, Tao Shen, Tianyi Zhou, Xianzhi Wang, Jing Jiang,
and Chengqi Zhang. 2020. Multi-center federated learning. arXiv preprint
arXiv:2005.01026 (2020).
[46] Qian Yang, Jianyi Zhang, Weituo Hao, Gregory P Spell, and Lawrence Carin. 2021.
Flop: Federated learning on medical datasets using partial networks. In Proc. of
the ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3845–3853.
[47] Xinqian Zhang, Ming Hu, Jun Xia, Tongquan Wei, Mingsong Chen, and Shiyan
Hu. 2020. Efficient federated learning for cloud-based AIoT applications. IEEE
Transactions on Computer-Aided Design of Integrated Circuits and Systems 40, 11
(2020), 2211–2223.
[48] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. 2021. Data-free knowledge distil-
lation for heterogeneous federated learning. In Proc. of International Conference
on Machine Learning. 12878–12889.
 
1105Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination KDD ’24, August 25–29, 2024, Barcelona, Spain.
A Proof of FedMR Convergence
A.1 Notations
In our FedMR approach, the global model is aggregated from all
the recombined models and all the models have the same weight.
Let𝑡exhibit the 𝑡𝑡ℎSGD iteration on the local device, 𝑣is the
intermediate variable that represents the result of SGD update after
exactly one iteration. The update of FedMR is as follows:
𝑣𝑘
𝑡+1=𝑤𝑘
𝑡−𝜂𝑡∇𝑓𝑘(𝑤𝑘
𝑡,𝜉𝑘
𝑡), (1)
𝑤𝑘
𝑡+1=𝑣𝑘
𝑡+1, 𝑖𝑓 𝐸 ∤𝑡+1
𝑅𝑀(𝑣𝑘
𝑡+1), 𝑖𝑓 𝐸|𝑡+1, (2)
where𝑤𝑘
𝑡represents the model of the 𝑘𝑡ℎclient in the 𝑡𝑡ℎiteration.
𝑤𝑡+1denotes the global model of the (𝑡+1)𝑡ℎiteration.𝑅𝑀(𝑣𝑘
𝑡+1)
denotes the recombined model.
Since FedMR recombines all the local models in each round and
the recombination only shuffles layers of models, the parameters of
recombined models are all from the models before recombination,
and no parameters are discarded. Therefore, when 𝐸|𝑡+1, we
can obtain the following invariants:
𝐾∑︁
𝑘=1𝑣𝑘
𝑡+1=𝐾∑︁
𝑘=1𝑅𝑀(𝑣𝑘
𝑡+1)=𝐾∑︁
𝑘=1𝑤𝑘
𝑡+1, (3)
𝐾∑︁
𝑘=1||𝑣𝑘
𝑡+1−𝑥||2=𝐾∑︁
𝑘=1||𝑤𝑘
𝑡+1−𝑥||2, (4)
where𝑤𝑘
𝑡is the𝑘𝑡ℎrecombined model in (𝑡−1)𝑡ℎiteration, which
is as the local model to be dispatched to 𝑘𝑡ℎclient in𝑡𝑡ℎiteration,
𝑥can any vector with the same size as 𝑣𝑘
𝑡. Similar to [ 34], we define
two variables 𝑣𝑡and𝑤𝑡:
𝑣𝑡=1
𝐾𝐾∑︁
𝑘=1𝑣𝑘
𝑡,𝑤𝑡=1
𝐾𝑘∑︁
𝑘=1𝑤𝑘
𝑡. (5)
Inspired by [29], we make the following definition:
𝑔𝑘
𝑡=∇𝑓𝑘(𝑤𝑘
𝑡;𝜉𝑘
𝑡). (6)
A.2 Proof of Lemma 4.4
Proof. Assume𝑣𝑘
𝑡has𝑛layers, we have 𝑣𝑘
𝑡=𝐿1⊕𝐿2⊕...⊕𝐿𝑛.
Let𝐿𝑖=[𝑝𝑣𝑘
𝑡
(𝑖,0),𝑝𝑣𝑘
𝑡
(𝑖,1),...,𝑝𝑣𝑘
𝑡
(𝑖,|𝐿𝑖|)], where𝑝𝑣𝑘
𝑡
(𝑖,𝑗)denotes the 𝑗𝑡ℎ
parameter of the layer 𝐿𝑖in the model 𝑣𝑘
𝑡. We have
𝐾∑︁
𝑘=1||𝑣𝑘
𝑡−𝑥||2=𝐾∑︁
𝑘=1𝑛∑︁
𝑖=1|𝐿𝑖|∑︁
𝑗=1||𝑝𝑣𝑘
𝑡
(𝑖,𝑗)−𝑝𝑥
(𝑖,𝑗)||2(7)
𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑥||2=𝐾∑︁
𝑘=1𝑛∑︁
𝑖=1|𝐿𝑖|∑︁
𝑗=1||𝑝𝑤𝑘
𝑡
(𝑖,𝑗)−𝑝𝑥
(𝑖,𝑗)||2(8)
Since model recombination only shuffles layers of models, the
parameters of recombined models are all from the models before
recombination and no parameters are discarded. We have
∀𝑖∈[1,𝑛],𝑗∈[1,|𝐿𝑖|]𝐾∑︁
𝑘=1𝑝𝑣𝑘
𝑡
(𝑖,𝑗)=𝐾∑︁
𝑘=1𝑝𝑤𝑘
𝑡
(𝑖,𝑗)(9)
∀𝑘∈[1,𝐾],𝑖∈[1,𝑛],𝑗∈[1,|𝐿𝑖|]∃𝑞∈[1,𝐾]{𝑝𝑣𝑘
𝑡
(𝑖,𝑗)=𝑝𝑤𝑞
𝑡
(𝑖,𝑗)} (10)∀𝑘∈[1,𝐾],𝑖∈[1,𝑛],𝑗∈[1,|𝐿𝑖|]∃𝑞∈[1,𝐾]{𝑝𝑤𝑘
𝑡
(𝑖,𝑗)=𝑝𝑣𝑞
𝑡
(𝑖,𝑗)} (11)
According to Equations 9-11, we have
𝐾∑︁
𝑘=1||𝑣𝑘
𝑡−𝑥||2=𝐾∑︁
𝑘=1𝑛∑︁
𝑖=1|𝐿𝑖|∑︁
𝑗=1||𝑝𝑣𝑘
𝑡
(𝑖,𝑗)−𝑝𝑥
(𝑖,𝑗)||2
=𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑥||2(12)
□
A.3 Key Lemmas
To facilitate the proof of our Theorem 1, inspired by [ 29], we can
present the following two lemmas. Note that the following proofs
are general proofs for all the multi-model-based FL approaches that
satisfy Lemma 4.4.
Lemma A.1. (Results of one step SGD). If 𝜂𝑡≤1
4𝐿, we have
E||𝑣𝑡+1−𝑤★||2≤1
𝐾𝐾∑︁
𝑘=1(1−𝜇𝜂𝑡)||𝑣𝑘
𝑡−𝑤★||2
+1
𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2+10𝜂2
𝑡𝐿Γ.
Proof. According to Lemma 4.4 (i.e., Equation 3 and Equation
4), we have
||𝑣𝑡+1−𝑤★||2≤1
𝐾𝐾∑︁
𝑘=1||𝑣𝑘
𝑡+1−𝑤★||2
=1
𝐾𝐾∑︁
𝑘=1(||𝑣𝑘
𝑡−𝑤★||2−2𝜂𝑡⟨𝑤𝑘
𝑡−𝑤★,𝑔𝑘
𝑡⟩
+𝜂2
𝑡||𝑔𝑘
𝑡||2)(13)
Let𝐵1=−2𝜂𝑡⟨𝑤𝑘
𝑡−𝑤★,𝑔𝑘
𝑡⟩and𝐵2=𝜂2
𝑡Í𝐾
𝑘=1||𝑔𝑘
𝑡||2. According
to Assumption 4.2, we have
𝐵1≤−2𝜂𝑡(𝑓𝑘(𝑤𝑘
𝑡)−𝑓𝑘(𝑤★))−𝜇𝜂𝑡||𝑤𝑘
𝑡−𝑤★||2(14)
According to Assumption 4.1, we have
𝐵2≤2𝜂2
𝑡𝐿(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★
𝑘) (15)
According to Equation 14 and 15, we have
||𝑣𝑡+1−𝑤★||2≤1
𝐾𝐾∑︁
𝑘=1[(1−𝜇𝜂𝑡)||𝑣𝑘
𝑡−𝑤★||2
−2𝜂𝑡(𝑓𝑘(𝑤𝑘
𝑡)−𝑓𝑘(𝑤★))+ 2𝜂2
𝑡𝐿(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★
𝑘)]
(16)
Let𝐶=1
𝐾Í𝐾
𝑘=1[−2𝜂𝑡(𝑓𝑘(𝑤𝑘
𝑡)−𝑓𝑘(𝑤★))+ 2𝜂2
𝑡𝐿(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★
𝑘)].
We have
𝐶=−2𝜂𝑡
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓𝑘(𝑤★))+2𝜂2
𝑡𝐿
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★
𝑘)
=−2𝜂𝑡(1−𝜂𝑡𝐿)
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★)+2𝜂2
𝑡𝐿
𝐾𝐾∑︁
𝑘=1(𝑓★−𝑓★
𝑘)(17)
 
1106KDD ’24, August 25–29, 2024, Barcelona, Spain. Ming Hu et al.
LetΓ=𝑓★−1
𝐾Í𝐾
𝑘=1𝑓★
𝑘and𝜙=2𝜂𝑡(1−𝐿𝜂𝑡). We have
𝐶=−𝜙
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★)+2𝜂2
𝑡𝐿Γ (18)
Let𝐷=−1
𝐾Í𝐾
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓★),𝐸|𝑡0and𝑡−𝑡0≤𝐸. We have
𝐷=−1
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡)−𝑓𝑘(𝑤𝑘
𝑡0)+𝑓𝑘(𝑤𝑘
𝑡0)−𝑓★) (19)
By Cauchy–Schwarz inequality, we have
𝐷≤1
2𝐾𝐾∑︁
𝑘=1(𝜂𝑡||∇𝑓𝑘(𝑤𝑘
𝑡0)||2+1
𝜂𝑡||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2)
−1
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡0)−𝑓★)
≤1
2𝐾𝐾∑︁
𝑘=1[2𝜂𝑡𝐿(𝑓𝑘(𝑤𝑘
𝑡0)−𝑓★
𝑘)+1
𝜂𝑡||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2]
−1
𝐾𝐾∑︁
𝑘=1(𝑓𝑘(𝑤𝑘
𝑡0)−𝑓★)(20)
Note that since 𝜂≤1
4𝐿,𝜂𝑡≤𝜙≤2𝜂𝑡and𝜂𝑡𝐿≤1
4. According
to Equation 20, we have
𝐶≤𝜙
2𝜂𝑡𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2+(𝜙𝜂𝑡𝐿+2𝜂2
𝑡𝐿)Γ+𝜙
𝐾𝐾∑︁
𝑘=1(𝑓★−𝑓★
𝑘)
≤𝜙
2𝜂𝑡𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2+(𝜙𝜂𝑡𝐿+𝜙+2𝜂2
𝑡𝐿)Γ
≤1
𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2+10𝜂2
𝑡𝐿Γ
(21)
□
Lemma A.2. According to Equation 2, the model recombination
occurs every 𝐸iterations. Assume that in each training round, 𝑡0is
the first iteration and iteration 𝑡−𝑡0≤𝐸−1. Given the constraint on
learning rate from [ 29], we know that 𝜂𝑡≤𝜂𝑡0≤2𝜂𝑡. It follows that
1
𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2≤4𝜂2
𝑡(𝐸−1)2𝐺2.
Proof.
1
𝐾𝐾∑︁
𝑘=1||𝑤𝑘
𝑡−𝑤𝑘
𝑡0||2=1
𝐾𝐾∑︁
𝑘=1||𝑡0+𝐸−1∑︁
𝑡=𝑡0𝜂𝑡∇𝑓𝑎1(𝑤𝑎1
𝑡;𝜉𝑎1
𝑡)||2
≤(𝑡−𝑡0)𝑡0+𝐸−1∑︁
𝑡=𝑡0𝜂2
𝑡𝐺2
≤(𝐸−1)𝑡0+𝐸−1∑︁
𝑡=𝑡0𝜂2
𝑡𝐺2
≤4𝜂2
𝑡(𝐸−1)2𝐺2.
□A.4 Proof of Theorem 1
Based on Lemmas A.1 and A.2, we can prove Theorem 1 using the
proof framework of FedAvg [ 29]. Due to space limitations, please
refer to the proof of FedAvg [29] for the details.
B Secure Model Recombination Mechanism
To avoid the risk of privacy leakage caused by exposing gradients
or models to the cloud server, we propose a secure model recombi-
nation mechanism for FedMR, which allows the random exchange
of model layers among clients before model training or upload. As
shown in Figure 13, within a round of the secure model recombina-
tion, the update of each model (i.e., 𝑚) consists of four stages:
Stage2:Receiving LayersReceivefrom𝒄𝒊Receivefrom𝒄𝒋𝒎Stage3:Shuffling LayersStage4:ModelRecombinationReceivefrom𝒄𝜸Receivefrom𝒄𝜶Receivefrom𝒄𝜷Receivefrom𝒄𝜹Receivefrom𝒄𝜺
ModelRecombineSendto𝒄𝜸Sendto𝒄𝜹Sendto𝒄𝜺Sendto𝒄𝜶Sendto𝒄𝜷
RandomlySelectLayersSendto𝒄𝒊Sendto𝒄𝒋Stage1:Sending LayersBuffer-𝒍𝒂𝒚𝒆𝒓𝟏Buffer-𝒍𝒂𝒚𝒆𝒓𝟐Buffer-𝒍𝒂𝒚𝒆𝒓𝟑Buffer-𝒍𝒂𝒚𝒆𝒓𝟒𝒎
Figure 13: Workflow of secure model recombination.
Stage 1: Assume that the local model has 𝑙𝑒𝑛layer. Each client
maintains a buffer for each layer. Firstly, each client randomly
selects a part of its layers and sends them to other activated clients,
while the remaining layers are saved in their corresponding buffers.
Note that a selected layer can only be sent to one client. For example,
in Figure 13, the client 𝑚sends𝑙𝑎𝑦𝑒𝑟 2and𝑙𝑎𝑦𝑒𝑟 4to𝑐𝑖and𝑐𝑗,
respectively.
Stage 2: Once receiving a layer from another client, the receiving
client𝑚will add the layer to its corresponding buffer. For example,
in Figure 13, the client 𝑚totally receives five layers. Besides the
retained two layers in stage 1, 𝑚now has seven layers in total in
its buffers.
Stage 3: For each layer buffer of 𝑚, if there contains one element
received from a client 𝑐in stage 2, our mechanism will randomly
select one layer in the buffer and return it back to 𝑐. For example,
in Figure 13, 𝑚randomly returns a layer in Buffer-layer1 back to a
client𝑐𝛾.
Stage 4: Once receiving the returned layers from other clients,
our mechanism will recombine them with all the other layers in
the buffers to form a new model. Note that the recombined model
may significantly differ from the original model in Stage 1.
Note that each FL training round can perform multiple times
secure model recombination. Due to the randomness, it is hard
for adversaries to figure out the sources of client model layers. In
addition, the cloud server will broadcast a public key before the
secure recombination to prevent privacy leakage. By using the
public key to encrypt the model parameters of each layer, the other
clients cannot directly obtain their received parameters.
 
1107