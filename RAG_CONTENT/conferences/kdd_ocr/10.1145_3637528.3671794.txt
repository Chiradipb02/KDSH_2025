Improved Active Covering via
Density-Based Space Transformation
MohammadHossein Bateni
Google Research
New York City, New York, USA
bateni@google.comHossein Esfandiari
Google Research
London, UK
esfandiari@google.com
Samira HosseinGhorban
Institute for Research in Fundamental Sciences
School of Computer Science
Tehran, Iran
s.hosseinghorban@ipm.irAlipasha Montaseri
Sharif University of Technology
Tehran, Iran
apmontaseri@ce.sharif.edu
ABSTRACT
In this work, we study active covering, a variant of the active-
learning problem that involves labeling (or identifying) all of the
examples with a positive label. We propose a couple of algorithms,
namely Density-Adjusted Non-Adaptive (DANA) learner andDensity-
Adjusted Adaptive (DAA) learner, that query the labels according to
a distance function that is adjusted by the density function. Under
mild assumptions, we prove that our algorithms discover all of the
positive labels while querying only a sublinear number of examples
from the support of negative labels for constant-dimensional spaces
(see Theorems 5 and 6). Our experiments show that our champion
algorithm DAA consistently improves over the prior work on some
standard benchmark datasets, including those used by the previous
work, as well as a couple of data sets on credit card fraud. For
instance, when measuring performance using AUC, our algorithm
is the best in 25out of 27experiments over 7different datasets.
CCS CONCEPTS
•Computing methodologies →Active learning settings; On-
line learning settings ;Batch learning ;•Theory of computation
→Sketching and sampling.
KEYWORDS
Active Covering, Active Learning, Crowdsourcing.
ACM Reference Format:
MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban,
and Alipasha Montaseri. 2024. Improved Active Covering via Density-Based
Space Transformation. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671794
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36717941 INTRODUCTION
It is widely recognized that data collection often involves costs. One
example is when we are using crowdsourcing to collect data for
machine learning tasks such as spam or fraud detection. This fact
motivates a class of machine-learning tasks called active learning.
In an active-learning task, we intend to indicate a limited set of
valuable examples (iteratively, or in batches) and probe their labels
in order to learn a model. Active covering is a variant of active
learning, in the context of binary classification, where the set of
examples with the positive label is considered valuable, hence we
intend to probe all such examples. One instance of active covering
is disease testing in a new pandemic such as Covid-19. In this case,
we desire to identify and test all patients with Covid-19, while we
do not like to waste many tests on patients without Covid-19.
Previously, Jiang and Rostamizadeh [ 17] studied the active-
covering problem and proposed and analyzed a couple of algorithms
called offline learner and active learner. Both algorithms are based
on first sampling a set of examples to query and then querying
the unlabeled examples close to the examples with a positive label.
The only distinction between the two algorithms is that the offline
learner exclusively considers the examples with a positive label in
the initial samples and queries nearby examples whereas the active
learner considers all queried examples with a positive label. They
show that under the following four assumptions, their algorithms
label all of the positive examples while querying only a sublinear
number of examples from the support of the negative examples.
First, a fixed lower bound on the density function of the positive
labels in the support of the positive labels is assumed. Second, for
every small ball around a point with a positive label, a constant
fraction of the volume of the ball falls inside the support of the
positive examples. Third, there is an upper bound on the density of
the negative examples. Moreover, in their theorem statements, they
use a parameter 𝐶which actually depends linearly on the surface
area of the support of the positive examples. Hence, by hiding 𝐶in
the𝑂(.)notation, they implicitly assume that the surface area of
the support of the positive examples is bounded by a constant.
It is evident that both the offline learner and active learner al-
gorithms are highly sensitive to the density of the distribution of
the examples. For instance, in a scenario where the probability of
observing positive labels divided by the probability of observing
negative labels is uniform across the space, both the offline learner
 
107
KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
Figure 1: One feature Figure 2: Two features
Figure 3: A learner that is
not density adjusted.Figure 4: A density ad-
justed learner.
and active learner algorithms tend to query the areas with a higher
density at a substantially higher rate and subsequently move to
areas with a lower density, which is a counter-intuitive behavior.
This is why it is necessary for these algorithms to have a global up-
per bound on the probability distribution of the negative examples
and a global lower bound on the probability distribution of positive
labels in the support of positive labels. Such global constraints on
the probability distributions are not very desirable since in many
instances we observe several colonies of points in some dense areas
and some large and sparse pools of points around them.
Another issue is that the previous assumptions are very sensitive
to feature selection, and adding one unnecessary or mildly relevant
feature can break the assumptions. For example, Figure 1 is a very
well-formed example and respects the assumptions of the previous
work. However, adding one relevant but unnecessary feature turns
it into Figure 2 which has very sparse areas with positive examples
and no longer respects the assumptions of the previous work.
To address the aforementioned issues, we implicitly utilize a
space transformation that expands the regions with high density in
the probability distribution of the examples and shrinks the regions
with low density, with the aim of achieving a more uniform prob-
ability distribution. This transformation allows us to remove the
global upper and lower bounds on the probability distributions and
instead use a local constraint that avoids a sudden change in the
probability distribution in a small ball. This allows the probability
distribution to smoothly change from one place in the dataset to
another and hence accept inputs such as Figure 2. We adopt the
offline learner and active learner in this transformed space. We
call our algorithms Density-Adjusted Non-Adaptive (DANA) learner
andDensity-Adjusted Adaptive (DAA) learner, respectively. We ex-
tend the approach of Jiang and Rostamizadeh to prove that our
algorithms label all of the positive examples while querying only a
sublinear number of examples with negative labels.
We conduct experimental comparisons between our algorithms,
the offline learner, the active learner, and the uniform sampler. Re-
markably, in almost all of our experiments, our proposed algorithm
DAA consistently emerges as the best algorithm in terms of label-
ing cost. For example, when we measure the performance via the
AUC, our algorithm achieves the best performance in 25out of 27
experiments over 7different datasets.Figures 3 and 4 give some intuitions on why our density-adjusted
algorithms work better than the previous Euclidean-based algo-
rithms. Note that, both Figures 3 and 4 represent the same instance.
In this instance, there are two discs of positive points inside a pool
of negative points. However, the density of the left side is higher
than the right side. A Euclidean-based algorithm probes some discs
of the same radius to find all of the positive points, regardless of
the density function (see Figure 3). However, our density-adjusted
algorithms probe the labels more cautiously in the denser areas
via smaller discs (see Figure 4). Hence, a fewer number of negative
points fall in the discs that a density-adjusted algorithm probes.
1.1 Problem Setting
In this section, we define the active-covering problem and our
setting. There exists an unknown probability density function 𝑓:
R𝐷→Rrepresenting examples with binary labels. We use X⊆R𝐷
to refer to the support of 𝑓, i.e., the set of all possible examples. We
refer to the examples with label 1as positive examples and to the
examples with label 0as negative examples. Accordingly, we have
unknown density functions 𝑓+:R𝐷→Rwith supportX+and
𝑓−:R𝐷→Rwith supportX−, corresponding to the positive and
negative examples respectively. We denote by P,P+andP−the
probability distributions corresponding to 𝑓,𝑓+and𝑓−, respectively.
Hence for a set 𝐴⊆R𝐷, for example we have P(𝐴)=∫
𝐴𝑓 𝑑𝐴.
For an arbitrary set 𝐴⊆X, by the law of total probability [ 27,
Page. 9] the probability of 𝐴is equal to
P(𝐴)=P(X+)P(𝐴|X+)+P(X−)P(𝐴|X−).
Let𝑝:=P(X+), then we haveP(X−)=1−𝑝. Thus, for any 𝐴⊆X
we have
P(𝐴)=𝑝P+(𝐴)+(1−𝑝)P−(𝐴).
We receive a set of unlabeled examples 𝑋⊆ X , with size𝑛
drawn i.i.d. from probability distribution P. We are allowed to
query the label of the examples in X. The goal is to discover all
positive examples while minimizing the number of queries. We use
𝑋+and𝑋−to refer to the true set of positive and negative examples
in𝑋, respectively.
Similar to the prior work [ 17], we compare our algorithms to
the optimal performance achievable by an algorithm that knows
the support ofX+, which requires 𝑛P(X+)queries in expectations.
Note that, if a point 𝑥belongs to the support of X+, even though
we may have some information about the probability that 𝑥has
a negative label, it still may have a positive label and hence any
algorithm is forced to query it. Hence, any algorithm requires to
make at least 𝑛P(X+)queries. We refer to this as 𝑄𝑂𝑃𝑇.
Definition 1 (Excess Query Cost). We define the excess query
cost of an algorithm 𝐴, denoted by 𝐶𝐴, to be𝑄𝐴−𝑄𝑂𝑃𝑇, where𝑄𝐴
is the number of queries made by algorithm 𝐴to label all the positive
examples.
It is not difficult to observe that, without any structural assump-
tion on the relationship among the positive examples, querying al-
most all of the examples is necessary to retrieve the outlier positive
examples. Therefore, to avoid having to locate positive examples in
extremely narrow and sparse subspaces, some assumptions on the
distribution of the positive examples are necessary [10, 17, 32].
 
108Improved Active Covering via
Density-Based Space Transformation KDD ’24, August 25–29, 2024, Barcelona, Spain
Our assumptions here are similar to those of Jiang and Ros-
tamizadeh [ 17] in nature. However, there are some technical dif-
ferences that make our assumptions less restrictive. For example,
instead of global upper or lower bounds on the density function, we
require the change in the density function to be bounded in every
small ball. Another example is that when they refer to a small ball
they refer to a ball with a small radius, while we refer to a ball with
a small probability. However, roughly speaking, since they have a
lower bound on the density as well, a ball with a small probability
will have a small radius as well, but not vice versa.
The following assumption ensures that there are no outlier posi-
tive examples. In other words, wherever a positive example exists, a
nontrivial fraction of the examples surrounding it are also positive.
Assumption 2. The support of positive examples is a compact
subspace and a disjoint union of a finite number of connected com-
ponents1X+,1,...,X+,𝑐. Moreover, there exist 𝜙0and𝐶+∈ (0,1]
such that for any point 𝑥∈X+and positive number 𝜖that satisfy
P(𝐵(𝑥,𝜖))≤𝜙0, we have
P+(𝐵(𝑥,𝜖))≥𝐶+P(𝐵(𝑥,𝜖)),
where𝐵(𝑥,𝜖)is a ball with radius 𝜖and center𝑥with respect to the
Euclidean metric.
The next assumption says that the density function 𝑓does not
significantly change in a small ball of its domain.
Assumption 3. Let𝜙0be the parameter set in Assumption 2. For a
point𝑥∈X, we define𝜆𝑥and𝜇𝑥to respectively be the minimum and
maximum of 𝑓in𝐵(𝑥,𝑟), where𝑟is chosen such thatP(𝐵(𝑥,𝑟))=𝜙0.
We assume that𝜆𝑥
𝜇𝑥≥𝛼, for a constant 𝛼.
The following assumption ensures that the support of X+is not
an excessively narrow subspace scattered throughout the domain
ofX−.
Assumption 4. Let𝜙0be the parameter set in Assumption 2. There
exists a constant 𝐶X+>0such that for all 𝜙∈[0,𝜙0],
P− 
⋓𝑥∈X+𝐵 
𝑥,𝐷√︄
𝜙
𝜇𝑥𝑣𝐷!!
≤𝐷√︁
𝜙𝐶X+,
where𝑣𝐷is the volume of a unit ball in a 𝐷dimensional space.
The previous work requires a similar assumption to Assump-
tion 4, that they do not explicitly mention. Specifically, they require
the surface area of the domain of positive examples to be bounded.
In fact, they use a parameter 𝐶in their excess query cost that "de-
pends onP". The parameter 𝐶actually depends linearly on the
surface area of the domain of positive examples and hence enforces
it to be bounded.
1.2 Our Contributions
As our first result, we bound the excess query cost of our density-
adjusted non-adaptive algorithm (i.e., Algorithm 1). This result is
presented in Section 2.
1A setA⊆R𝐷is connected if and only if for each pair of points 𝑥,𝑥′∈A , there is
a curve from 𝑥to𝑥′, which completely lies in A.Theorem 5. Suppose that Assumptions 2, 3, and 4 hold. For some
𝑚∈eΘ(𝑛𝐷/(𝐷+1)), we have
E[𝐶DANA]≤e𝑂
𝑛𝐷
𝐷+1
.
Next, we bound the excess query cost of our density-adjusted
adaptive algorithm (i.e., Algorithm 2). This result is presented in
Section 3.
Theorem 6. Suppose that Assumptions 2, 3 and 4 hold. For some
𝑚∈˜Θ(𝑛(𝐷−1)/𝐷), we have
E[𝐶DAA]≤ ˜𝑂(𝑛𝐷−1
𝐷).
Remark 7. Both our algorithms assume that we know the density
function𝑓(·)of our examples. In Section 4 we show how to estimate
the density function for our purpose.
The previous work relies on explicitly knowing the number of
positive examples and clarified that “It’s worth noting that we may
not know when all of the positive examples are labeled– thus, in prac-
tice, we can terminate the algorithm when enough positives are found
depending on the task or when the labeling budget runs out” [17].
We resolve this issue from the theoretical perspective and provide
a simple and asymptotically optimal termination condition that
applies both to our algorithms and the algorithms of the previous
work. This condition is provided in Section 5.
Finally, in section 6 we report our experimental study on datasets
used by the previous work, as well as Mini-Imagenet and a couple
of datasets on credit card fraud. We provide further experimental
details in the appendix. Our experiments show that our DAA al-
gorithm consistently improves over the previous work. Due to the
space limit, we provide some of the proofs in the appendix.
1.3 Preliminaries
Subsequently, we present some definitions and lemmas that we use
throughout the paper.
Definition 8. Let𝑑(,)denote the Euclidean distance. Let A,B⊆
R𝐷be non-empty sets and 𝑥∈R𝐷and𝜖>0.
(1)An𝜖-ball with center 𝑥is defined as
𝐵(𝑥,𝜖)=n
𝑦∈R𝐷|𝑑(𝑥,𝑦)≤𝜖o
.
(2)An𝜖-tubular neighborhood around the set Ais defined as
𝐵(A,𝜖)=
𝑥∈R𝐷inf
𝑥′∈A𝑑(𝑥,𝑥′)≤𝜖
.
(3)The distance 𝑥fromAis defined as
𝑑(𝑥,A)=min
𝑦∈A𝑑(𝑥,𝑦).
Definition 9. LetA⊆ R𝐷,𝑆⊂A and𝜙>0. The setAis
connected in the 𝜙-neighborhood graph of 𝑆if and only if for each
𝑥,𝑥′∈A, there is a path 𝑥1=𝑥→𝑥2→...→𝑥ℓ=𝑥′where for
each 1<𝑗<ℓwe have𝑥𝑗∈𝑆and𝑥𝑗∈𝐵(𝑥𝑗−1,𝑟𝑥𝑗−1)where𝑟𝑥𝑗−1
is selected such that P 𝐵(𝑥𝑗−1,𝑟𝑥𝑗−1))=𝜙.
We use Vol(𝐵)to refer to the volume of a multi-dimensional ball
𝐵in Euclidean space. We use 𝑃𝑟(𝐸)to indicate the probability of
an event𝐸. We also use the following theorem from [ 6] to establish
an upper bound on the number of our queries.
 
109KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
Theorem 10. Let𝑋be a set of i.i.d samples with size 𝑛drawn
from a distributionP. For 0<𝛿<1, there exists a universal constant
𝐶0such that with probability at least 1−𝛿uniformly over all balls
𝐵⊆R𝐷, we have
P(𝐵)≥𝐶0𝐷log2
𝛿log𝑛
𝑛⇒|𝐵∩𝑋|>0.
1.4 Other Related Work
The early studies on active learning date back to the 1990s [ 8,9].
However, due to its significant role in machine learning tasks, there
is still a lot of interest in developing more practical and effective
active learning mechanisms [ 1,7,21,26,30].Active learning has
been used in several learning tasks such as image processing [ 12,20],
fight against COVID-19 [ 30], text classification [ 31] and speech
recognition [14]
Active covering as a variant of active learning appears in several
machine learning tasks. For example active covering in useful in
credit card fraud detection [ 2],computational drug discovery [ 29],
bank loan applications [ 22], moderate abusive content[ 28], fake
account detection in social network platforms [ 24], and, distances
and uncontrollable situations such as the COVID-19 pandemic [ 34].
Garnett et al. study active search in order to retrieve as many
positive labels as possible given a query budget [ 13]. Jiang et al. [ 18]
provide a more time-efficient algorithm for this problem. Active
search has also been formalized as a bandit problem in a few previ-
ous works [ 16,19]. The active covering problem considers a more
aggressive formulation compared to that of active search and at-
tempts to find all (or practically almost all) of the positive labels.
This is particularly important in sensitive situations such as credit
card fraud and providing tests during a pandemic.
Retrieving the positive examples is also known as learning under
one-sided feedback which was studied by Helmold et al. [ 15]. They
used the standard online model, where the learning algorithm tries
to minimize the failure. It is worth mentioning that active covering
is related to the main-stream research path including the online
learning tasks which are investigated widely [ 4,5,25] and the
classical set-cover problem [ 33]. Our techniques have a connection
to the support estimation literature [ 3,10,11,23,35], even though,
these works do not directly consider the active search problem.
2 DENSITY-ADJUSTED NON-ADAPTIVE
LEARNER
Algorithm 1 Density-adjusted Non-Adaptive Algorithm (DANA)
Input Dataset𝑋, initial sample size 𝑚and density function 𝑓.
1:Let𝑋0be𝑚examples sampled uniformly without replacement
from𝑋.
2:Label query 𝑋0and let𝑋+,0be the positive examples.
3:Label query remaining examples 𝑥in ascending order of
𝑑(𝑥,𝑋+,0)·𝐷√︁
𝑓(𝑥)until all positive examples are labeled.
Initially, DANA learner uniformly samples a set with 𝑚points from
the dataset𝑋and queries their labels. Subsequently, it labels the
remaining examples of 𝑋in ascending order of their minimum
density-adjusted distance to the initially sampled positive examplesuntil all positive examples are retrieved. Now, we are ready to prove
Theorem 5.
Proof of Theorem 5. To prove, we show that for any 0<𝛿<1,
if𝑚≥max(
2 log2
𝛿
𝑝2,2𝐶0𝐷log2
𝛿log(𝑚𝑝/2)
𝑝2𝐶+𝛼𝜙0)
, we have
E[𝐶DANA]≤( 1−𝑝)𝑚+𝑛𝐶log(2/𝛿)log(𝑝𝑚/2)
𝑚1/𝐷
+𝛿𝑛,
where𝐶=2(1−𝑝)𝐶X+𝐶0𝐷
𝐶+𝛼𝑝21
𝐷.By setting𝑚=˜Θ(𝑛𝐷/(𝐷+1))and
𝛿=1
𝑛, we obtain
E[𝐶DANA]≤e𝑂
𝑛𝐷/(𝐷+1)
.
Now, to prove the above claim we define binary random variables
𝑌1,...,𝑌𝑚to represent the labels of the examples that Algorithm 1
queries. Note that the probability of 𝑌𝑖being 1is𝑝, i.e.,𝑃𝑟(𝑌𝑖=
1)=𝑝. Thus the expected number of positive examples in the initial
sample𝑋0isE[Í𝑚
𝑖=1𝑌𝑖]=𝑚𝑝. Since𝑌1,...,𝑌𝑚are independent
random variables, Hoeffding inequality gives
𝑃𝑟𝑚∑︁
𝑖=1𝑌𝑖−𝑚𝑝≤−𝑡
≤𝑒−2𝑡2
𝑚,
for any arbitrary 𝑡>0. Equivalently, we have
𝑃𝑟𝑚∑︁
𝑖=1𝑌𝑖−𝑚𝑝>−𝑡
>1−𝑒−2𝑡2
𝑚.
We set𝑡=√︃
𝑚
2log2
𝛿in the above to obtain the following lower
bound on the number of positive labels in 𝑋0, with probability at
least 1−𝛿
2:
𝑚∑︁
𝑖=1𝑌𝑖≥𝑚𝑝−𝑡=𝑚(𝑝−𝑡/𝑚)=𝑚 
𝑝−√︂
1
2𝑚log2
𝛿!
.
Applying𝑚≥2 log2
𝛿
𝑝2gives usÍ𝑚
𝑖=1𝑌𝑖≥𝑚𝑝/2with probabil-
ity1−𝛿
2. Define𝜙=2𝐶0𝐷log2
𝛿log(𝑚𝑝/2)
𝑝2𝐶+𝛼𝑚. Note that since
𝑚≥2𝐶0𝐷log2
𝛿log(𝑚𝑝/2)
𝑝2𝐶+𝛼𝜙0, we have𝜙0≥2𝐶0𝐷log2
𝛿log(𝑚𝑝/2)
𝑝2𝐶+𝛼𝑚,
hence we have 𝜙≤𝜙0. Therefore, for each 𝑥∈X+, the definition
of𝜇𝑥impliesP(𝐵(𝑥,𝐷√︃
𝜙
𝜇𝑥𝑣𝐷))≤𝜙0. We claim that with proba-
bility 1−𝛿, for each𝑥∈X+, there exists a positive example in
𝐵(𝑥,𝐷√︃
𝜙
𝜇𝑥𝑣𝐷)that is queried in 𝑋0,+, i.e.,𝐵(𝑥,𝐷√︃
𝜙
𝜇𝑥𝑣𝐷)∩𝑋0,+≠∅.
To see this, let us calculate the probability mass of positive examples
in this ball:
𝑝P+
𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥𝑣𝐷)
≥𝑝𝐶+P
𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥𝑣𝐷)
≥𝑝𝐶+𝜆𝑥Vol(𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥𝑣𝐷))≥𝑝𝐶+𝜆𝑥𝑣𝐷𝜙
𝜇𝑥𝑣𝐷≥𝑝𝐶+𝛼𝜙.
 
110Improved Active Covering via
Density-Based Space Transformation KDD ’24, August 25–29, 2024, Barcelona, Spain
Combined with the lower bound 𝑚≥2𝐶0𝐷log2
𝛿log(𝑚𝑝/2)
𝑝2𝐶+𝛼𝜙, The-
orem (10) guarantees that one example from 𝑋0,+falls in this ball.
Hence, all positive examples are retrieved by Algorithm (1). Next,
we upper bound the excess query cost of this algorithm. The initial
sample set𝑋0contains(1−𝑝)𝑚negative examples in expectation.
Moreover, the number of negative examples in 𝑋\𝑋0is
(𝑛−𝑚)(1−𝑝)P− ⋓𝑥∈X′,+𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥𝑣𝐷)
≤(𝑛−𝑚)(1−𝑝)𝐶X+𝐷√︁
𝜙
where the inequality follows from Assumptions 3 and 4. With prob-
ability𝛿our concentration bounds fail and in that case, the excess
query cost is at most 𝑛otherwise our expected query cost is upper
bounded by
(1−𝑝)𝑚+(𝑛−𝑚)(1−𝑝)𝐶X+𝐷√︁
𝜙≤
(1−𝑝)𝑚+𝑛2(1−𝑝)𝐶X+𝐶0𝐷
𝐶+𝛼𝑝21
𝐷log 2
𝛿log 𝑚𝑝
2
𝑚1
𝐷.
Hence we have
E[𝐶DANA]≤𝛿𝑛+(1−𝑝)𝑚
+𝑛2(1−𝑝)𝐶X+𝐶0𝐷
𝐶+𝛼𝑝21
𝐷log 2
𝛿log 𝑚𝑝
2
𝑚1
𝐷.□
3 DENSITY-ADJUSTED ADAPTIVE LEARNER
Initially, DAA learner uniformly samples a set with 𝑚points from
the dataset 𝑋and queries their labels as same as DANA. Subse-
quently, it labels the remaining examples of 𝑋in ascending order
of their minimum density-adjusted distance to the current sample
points of𝑋whose positive labels are revealed until all positive
examples are retrieved. In summary, the key difference between the
DANA and DAA is in ascending order of the remaining examples
in𝑋. In DANA, the ordering is done based on the initial positive
sample while in DAA it is based on the updated ones.
Algorithm 2 Density-adjusted Adaptive Algorithm (DAA)
Input Dataset𝑋, initial sample size 𝑚and density function 𝑓.
1:Let𝑋0be𝑚examples sampled uniformly without replacement
from𝑋.
2:Label query 𝑋0and let𝑋+,0be the positive examples.
3:Initialize𝑋𝑝←𝑋+,0and𝑋𝑎←𝑋0.
4:while not all positive examples in 𝑋are labeled do
5: Label query 𝑥=arg min𝑥∈𝑋\𝑋𝑎𝑑(𝑥,𝑋𝑝)·𝐷√︁
𝑓(𝑥)
6: if𝑥has a positive label then
7:𝑋𝑝←𝑋𝑝∪{𝑥}
8: end if
9:𝑋𝑎←𝑋𝑎∪{𝑥}
10:end while
In the next lemma, first, we derive some conditions on 𝑚to guar-
antee, with high probability, that at least one example is chosen
from each connected component of X+. Next, a bound on 𝜙is pro-
posed such thatX+,𝑖∩𝑋+is connected in the 𝜙-neighborhood graphof𝑋+w.h.p. This ensures that our algorithm probes all of the posi-
tive examples in 𝑋+via some paths through the 𝜙-neighborhood
graph.
Lemma 11. Let Assumptions 2 and 3 hold and 0<𝛿<1.
(1)Define𝑞=min 1≤𝑖≤𝑐P+(X+,𝑖). If
𝑚≥max 
2 log
2𝑐
𝛿
𝑝log
1
1−𝑞,2 log
2
𝛿
𝑝2 
,
then for all 1≤𝑖≤𝑐, we haveX+,𝑖∩𝑋0≠∅, with probability
at least 1−𝛿.
(2)Let
𝜙=3𝐷𝐶0𝐷log2×3𝐷𝑛2
𝛿𝛼4𝜙log𝑛
𝑝𝐶+𝛼4𝑛,
where𝑛is chosen sufficiently large such that 𝜙≤𝜙0. Then for
all1≤𝑖≤𝑐,X+,𝑖∩𝑋+is connected in the 𝜙-neighborhood
graph of𝑋+, with probability at least 1−𝛿.
Proof. For the first part, we define binary random variables
𝑌1,...,𝑌𝑚to be the labels of the examples that Algorithm 2 queries.
Note that the probability of 𝑌𝑖being 1is𝑝, i.e.,𝑃𝑟(𝑌𝑖=1)=𝑝.
Thus the expected value ofÍ𝑚
𝑖=1𝑌𝑖is𝑚𝑝which represents the
expected number of positive examples in the initial sample 𝑋0of
Algorithm (2). Since 𝑌1,...,𝑌𝑚are independent random variables,
by Hoeffding inequality, we have
𝑃𝑟𝑚∑︁
𝑖=1𝑌𝑖−𝑚𝑝≤−𝑡
≤𝑒−2𝑡2
𝑚
for any arbitrary 𝑡>0. Equivalently, we have
𝑃𝑟𝑚∑︁
𝑖=1𝑌𝑖−𝑚𝑝>−𝑡
>1−𝑒−2𝑡2
𝑚.
Let us set𝑡=√︃
𝑚
2log2
𝛿. By applying this to the above inequality,
with probability at least 1−𝛿
2we can lower bound the number of
positive examples in the initial sample set 𝑋0as follows.
𝑚∑︁
𝑖=1𝑌𝑖≥𝑚𝑝−𝑡=𝑚(𝑝−𝑡
𝑚)=𝑚 
𝑝−√︂
1
2𝑚log2
𝛿!
.
Note that from the statement of the lemma we have 𝑚≥2 log(2
𝛿)
𝑝2.
Applying this to the above inequality gives us |𝑋+|≥𝑚𝑝
2with
probability 1−𝛿
2. Moreover, for each 1≤𝑖≤𝑐, the probability that
none of these 𝑚𝑝/2are inX+,𝑖is 1−P+(X+,𝑖)𝑚𝑝
2≤(1−𝑞)𝑚𝑝
2.
Let𝑚≥2 log(2𝑐
𝛿)
𝑝log
1
1−𝑞, then we have(1−𝑞)𝑚𝑝
2≤𝛿
2𝑐.Hence, for each
𝑖we haveX+,𝑖∩𝑋0≠∅with probability at least 1−𝛿/2𝑐. Applying a
simple union bound for all 𝑖gives usX+,𝑖∩𝑋0≠∅with probability
at least 1−𝛿/2. This completes the proof of the first part.
Now, for the second part, pick two arbitrary points 𝑥,𝑦∈X+,𝑖
for some𝑖∈{1,···,𝑐}. SinceX+,𝑖is connected, there is a carve
between𝑥,𝑦inX+,𝑖. We consider a sequence of points on this carve
 
111KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
namely,𝑥=𝑥1→𝑥2→···→𝑥ℓ=𝑦such that for every 𝑥𝑗, we
have
𝑥𝑗+1∈𝐵
𝑥𝑗,𝑟𝑥𝑗𝐷√
𝛼3
3
,
where radius 𝑟𝑥𝑗is set such thatP 𝐵(𝑥𝑗,𝑟𝑥𝑗)=𝜙. Next, we use
this to show that, with high probability, there exists a path 𝑥=
𝑥1→𝑥′
1→𝑥′
2→···→𝑥′
ℓ→𝑥ℓ=𝑦inX+,𝑖∩𝑋+, where for
every𝑥′
𝑗, we have𝑥′
𝑗+1∈𝐵(𝑥′
𝑗,𝑟′
𝑥′
𝑗), where radius 𝑟′
𝑥′
𝑗is set such
thatP 𝐵(𝑥′
𝑗,𝑟′
𝑥′
𝑗)=𝜙. This means that 𝑥and𝑦are connected in
the𝜙-neighborhood graph of 𝑋+as desired. Next, we show that
for every𝑗the ball𝐵
𝑥𝑗,𝑟𝑥𝑗𝐷√
𝛼3
3
contains at least one positive
example in data set 𝑋, w.h.p. Recall that we have P 𝐵(𝑥𝑗,𝑟𝑥𝑗)=𝜙
and hence we have𝐷√︂
𝜙
𝑣𝐷𝜇𝑥𝑗≤𝑟𝑥𝑗. To prove our claim, we calculate
the probability mass of positive examples in
𝐵
𝑥𝑗,𝐷√
𝛼3
3𝐷√︄
𝜙
𝑣𝐷𝜇𝑥𝑗
.
By Assumption 2 we have
𝑝P+
𝐵(𝑥𝑗,𝐷√
𝛼3
3𝐷√︄
𝜙
𝑣𝐷𝜇𝑥𝑗)
≥𝑝𝐶+P
𝐵(𝑥𝑗,𝐷√
𝛼3
3𝐷√︄
𝜙
𝑣𝐷𝜇𝑥𝑗)
≥𝑝𝐶+𝜆𝑥𝑗Vol(𝐵(𝑥𝑗,𝐷√
𝛼3
3𝐷√︄
𝜙
𝑣𝐷𝜇𝑥𝑗))
≥𝑝𝐶+𝜆𝑥𝑗𝛼3𝜙
3𝐷𝜇𝑥𝑗≥𝑝𝐶+𝛼4𝜙
3𝐷
≥𝐶0𝐷log 2𝑛2
𝛿𝜙log𝑛
𝑛.
Thus, by Theorem (10), with probability at least 1−𝛿𝛼4𝜙
3𝐷𝑛2, there
exists a point 𝑥′
𝑗∈𝐵(𝑥𝑗,𝐷√
𝛼3
3𝐷√︂
𝜙
𝑣𝐷𝜇𝑥𝑗)∩𝑋+. Note that we have
𝑑(𝑥′
𝑗,𝑥′
𝑗+1)≤𝑑(𝑥′
𝑗,𝑥𝑗)+𝑑(𝑥𝑗,𝑥𝑗+1)+𝑑(𝑥𝑗+1,𝑥′
𝑗+1)
≤𝐷√
𝛼3
3 𝑟𝑥𝑗+𝑟𝑥𝑗+𝑟𝑥𝑗+1
≤𝐷√
𝛼3
3 𝐷√︄
𝜙
𝑣𝐷𝜆𝑥𝑗+𝐷√︄
𝜙
𝑣𝐷𝜆𝑥𝑗+𝐷√︄
𝜙
𝑣𝐷𝜆𝑥𝑗+1
≤𝐷√
𝛼3
3 𝐷√︄
𝜙
𝑣𝐷𝜆𝑥𝑗+𝐷√︄
𝜙
𝑣𝐷𝜆𝑥𝑗+𝐷√︄
𝜙
𝛼𝑣𝐷𝜆𝑥𝑗
≤𝐷√
𝛼3
3 𝐷√︄
𝜙
𝛼𝑣𝐷𝜇𝑥𝑗+𝐷√︄
𝜙
𝛼𝑣𝐷𝜇𝑥𝑗+𝐷√︄
𝜙
𝛼2𝑣𝐷𝜇𝑥𝑗
≤𝐷√
𝛼3
33
𝐷√
𝛼2𝐷√︄
𝜙
𝑣𝐷𝜇𝑥𝑗≤𝐷√
𝛼3
33
𝐷√
𝛼2𝐷√︄
𝜙
𝑣𝐷𝛼𝜇𝑥′
𝑗
≤𝐷√
𝛼3
33
𝐷√
𝛼3𝐷√︄
𝜙
𝑣𝐷𝜇𝑥′
𝑗=𝐷√︄
𝜙
𝑣𝐷𝜇𝑥′
𝑗.Note that the probability of a ball with radius𝐷√︂
𝜙
𝑣𝐷𝜇𝑥′
𝑗around𝑥′
𝑗
is at most𝜙as claimed. Therefore, for every 𝑥′
𝑗, we have𝑥′
𝑗+1∈
𝐵(𝑥′
𝑗,𝑟′
𝑥′
𝑗), where radius 𝑟′
𝑥′
𝑗is set such thatP 𝐵(𝑥′
𝑗,𝑟′
𝑥′
𝑗)=𝜙. Now
note that each point in the space belongs to at most two of the
balls𝑥1,𝑥2,···,𝑥ℓ, and the probability of each of the balls is𝛼4𝜙
3𝐷,
hence we have ℓ≤2×3𝐷
𝛼4𝜙. By a union bound with probability at
least 1−2𝛿
𝑛2, we have sampled at least one positive example from
each of the balls 𝑥1,𝑥2,···,𝑥ℓ. This means that the X+,𝑖∩𝑋+is
connected in the 𝜙-neighborhood graph of 𝑋+, with probability at
least 1−2𝛿
𝑛2. Note that there are at most 𝑛
2possible choices for 𝑥
and𝑦. By a union bound all of the points in X+that belong to the
same connected component are connected int the 𝜙-neighborhood
graph, with probability at least 1−2𝛿
𝑛2 𝑛
2≥1−𝛿as claimed.□
Now, we are ready to prove Theorem 6.
Proof of Theorem 6. Define𝑞=min 1≤𝑖≤𝑐P+(X+,𝑖). Pick an
arbitrary 0<𝛿<1. By Lemma 11, all of the positive examples are
retrieved by Algorithm 2 with probability at least 1−2𝛿. If
𝑚≥max 
2 log
2𝑐
𝛿
𝑝log
1
1−𝑞,2 log
2
𝛿
𝑝2 
,
then the excess query cost of Algorithm 2 is upper bounded by
E[𝐶DAA]≤𝑚+𝐶
log 2
𝛿log(𝑛)𝑛𝐷−11
𝐷
+2𝛿𝑛,
where𝐶=
(1−𝑝)𝐶X+𝐶0𝐷
𝑝𝐶+𝛼𝑣2
𝐷1
𝐷
. By setting 𝑚=˜Θ(𝑛(𝐷−1)/𝐷)and
𝛿=1
𝑛, we obtain
E[𝐶DANA]∈e𝑂
𝑛(𝐷−1)/𝐷
.
Note that the excepted number of negative examples in 𝑋0is
equal to(1−𝑝)𝑚. Also, the number of negative examples in 𝑋\𝑋0
is
(𝑛−𝑚)(1−𝑝)P− 
⋓𝑥∈X+𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥)!
.
Thus, by Assumption 4, we have
E[𝐶DAA]≤( 1−𝑝)𝑚+(𝑛−𝑚)(1−𝑝)P− 
⋓𝑥∈X+𝐵(𝑥,𝐷√︄
𝜙
𝜇𝑥)!
≤(1−𝑝)𝑚+(𝑛−𝑚)(1−𝑝)𝐶X+𝐷√︄
𝜙
𝑣𝐷
≤(1−𝑝)𝑚+𝑛(1−𝑝)𝐶X+ 
𝐶0𝐷log2
𝛿log𝑛
𝑝𝐶+𝛼𝑣2
𝐷𝑛!1
𝐷
≤(1−𝑝)𝑚+𝑛 
(1−𝑝)𝐶X+𝐶0𝐷
𝑝𝐶+𝛼𝑣2
𝐷!1
𝐷 
log2
𝛿log𝑛
𝑛!1
𝐷
.
 
112Improved Active Covering via
Density-Based Space Transformation KDD ’24, August 25–29, 2024, Barcelona, Spain
Let𝐶=
(1−𝑝)𝐶X+𝐶0𝐷
𝑝𝐶+𝛼𝑣2
𝐷1
𝐷
, then we have
E[𝐶DAA]≤𝑚+𝐶
log 2
𝛿log(𝑛)𝑛𝐷−11
𝐷
.
With probability 2𝛿Lemma 11 fails and in that case, the excess
query cost is at most 𝑛. Hence we have
E[𝐶DAA]≤𝑚+𝐶
log 2
𝛿log(𝑛)𝑛𝐷−11
𝐷
+2𝛿𝑛,
as claimed. □
4 DENSITY ESTIMATION
The algorithms presented in Section 3 assume that the density func-
tion𝑓is known. However, such an assumption is often unrealistic,
and in practice this function is unknown. In this section, we use the
𝑘-nearest neighbor ( 𝑘-NN) method to estimate the density function
𝑓with a function ˆ𝑓such that for all 𝑥∈X, we have𝑓(𝑥)
ˆ𝑓(𝑥)∈[𝛼
2,3
2𝛼]
with probability at least 1−𝛿for an arbitrary small 𝛿∈(0,1]. In
this section, we show that incorporating 𝑘-nearest neighbor only
affects the excess query cost by replacing 𝛼with𝑂(𝛼2).
Pick an arbitrary 𝑥∈𝑋. Next, we show how to estimate the
value of the density function at 𝑥such that𝑓(𝑥)
ˆ𝑓(𝑥)∈[𝛼
2,3
2𝛼]with
probability at least 1−𝛿
𝑛. This together with a simple union bound
gives us our desired approximate function ˆ𝑓(𝑥)for all𝑥∈𝑋with
probability at least 1−𝛿. Let𝑘=√︃
2𝑛log(𝛿
2𝑛)and let𝑟𝑥be the
distance form 𝑥to its𝑘-th nearest neighbor. Note that we have
|𝐵(𝑥,𝑟𝑥)∩𝑋|=𝑘. We define
ˆ𝑓(𝑥)=𝑘
𝑛1
𝑣𝐷𝑟𝐷𝑥.
In the rest we upper and lower bound 𝑟𝑥and then make use of this
to upper and lower bound 𝑓(𝑥)viaˆ𝑓(𝑥).
We define𝑟𝑥,𝑘
2=𝐷√︃
𝑘
2𝑛𝛼
𝑣𝐷𝑓(𝑥). We have
P 𝐵(𝑥𝑗,𝑟𝑥,𝑘
2)=∫
𝑦∈𝐵(𝑥,𝑟𝑥,𝑘
2)𝑓(𝑦)≤∫
𝑦∈𝐵(𝑥,𝑟𝑥,𝑘
2)𝑓(𝑦)
𝛼
=𝑣𝐷𝑟𝐷
𝑥,𝑘
2𝑓(𝑥)
𝛼=𝑣𝐷𝑘
2𝑛𝛼
𝑣𝐷𝑓(𝑥)𝑓(𝑥)
𝛼=𝑘
2𝑛.
Note that the definition of 𝐵(𝑥𝑗,𝑟𝑥,𝑘
2)is independent of 𝑋. More-
over, the expected number of points from 𝑋that falls in𝐵(𝑥𝑗,𝑟𝑥,𝑘
2)
is𝑘
2. Hence by Hoeffding inequality, we have
𝑃𝑟
|𝐵(𝑥𝑗,𝑟𝑥,𝑘
2)∩𝑋|≥𝑘
=𝑃𝑟
|𝐵(𝑥𝑗,𝑟𝑥,𝑘
2)∩𝑋|−𝑘
2≥𝑘
2
≤exp𝑘2
2𝑛
=exp 
2𝑛log(𝛿
2𝑛)
2𝑛!
=𝛿
2𝑛.
Similarly, we define 𝑟𝑥,3𝑘
2=𝐷√︃
3𝑘
2𝑛1
𝛼𝑣𝐷𝑓(𝑥), and we have
P 𝐵(𝑥𝑗,𝑟𝑥,3𝑘
2)≥3𝑘
2𝑛, and𝑃𝑟
|𝐵(𝑥𝑗,𝑟𝑥,3𝑘
2)∩𝑋|≤𝑘
≤𝛿
2𝑛.Therefore with probability𝛿
2we have𝑟𝑥,𝑘
2≤𝑟𝑥≤𝑟𝑥,3𝑘
2. This
means that
𝐷√︄
𝑘
2𝑛𝛼
𝑣𝐷𝑓(𝑥)≤𝑟𝑥≤𝐷√︄
3𝑘
2𝑛1
𝛼𝑣𝐷𝑓(𝑥).
The first inequality gives us𝑘
2𝑛𝛼
𝑣𝐷𝑟𝐷𝑥≤𝑓(𝑥)and the second one
gives us𝑓(𝑥)≤3𝑘
2𝑛1
𝛼𝑣𝐷𝑟𝐷𝑥. These imply𝑓(𝑥)
ˆ𝑓(𝑥)∈[𝛼
2,3
2𝛼]as claimed.
Note that, even though in our algorithms we are adjusting the
distance using 𝑓(𝑥), we are only using the fact that this adjusting
factor is, by Assumption 3, an estimation (i.e., 𝛼to1
𝛼factor) of the
density of any point within a small ball of 𝑥. If we use ˆ𝑓(𝑥)to factor
the estimation is within𝛼
2×𝛼to3
2𝛼1
𝛼factor, then this change only
injects some constant factors of 𝛼to the excess query cost.
5 TERMINATION CONDITION
In this subsection, we provide a simple but asymptotically optimal
termination condition for active covering algorithms when we are
not aware of the exact number of positive examples. Let 𝐴𝐿𝐺 be an
active learning algorithm (without a termination constraint) and
let𝐶𝐴𝐿𝐺 be an upper bound on the expected excess query cost
of𝐴𝐿𝐺 if it (hypothetically) stops immediately after querying the
last positive example. Note that, Theorems 5 and 6 provide upper
bounds on excess query costs of our algorithms DANA and DAA,
which can be used as 𝐶𝐴𝐿𝐺 to plug into the following theorem. The
following theorem provides the termination condition. We provide
the proof of this theorem in appendix B
Theorem 12. Pick an arbitrarily small probability 𝛿∈(0,0.33]
and let𝐴𝐿𝐺′be an algorithm that runs 𝐴𝐿𝐺 as defined above and
terminates as soon as it observes𝐶𝐴𝐿𝐺
𝛿+log(𝑛
𝛿𝐶+)consecutive negative
labels.𝐴𝐿𝐺′queries all of the positive examples and has an excess
query cost of at most 𝑂(𝐶𝐴𝐿𝐺)+˜𝑂(1), with probability at least 1−3𝛿.
6 EXPERIMENTS
In this section, we compare our density-adjusted methods with
the methods presented in [ 17]. Given the superior performance of
the methods outlined in [ 17] compared to the other baseline ap-
proaches, it is reasonable to compare our algorithms against theirs.
The offline algorithm [ 17] selects an initial sample and queries the
other datapoints in ascending order of their distance to the positive
initial samples. The active algorithm [ 17] does the same but it also
considers positive samples retrieved after the initial queries. Our
density-adjusted algorithms query the remaining datapoints in the
order of their distance to the positive datapoints times their density.
In the experiments we do not have access to the function 𝑓, so we
approximate the density of a datapoint by calculating the inverse
of its distance to its 𝑘’th nearest neighbor, where 𝑘is a hyperpa-
rameter of the algorithm. It is not hard to see that this converges
to𝐷√︁
𝑓(𝑥)as the number of samples grows. Due to the space limit,
the modified algorithms are available in the appendix (3 and 4).
Since calculating the exact 𝑘nearest neighbors is computationally
expensive, we use locality-sensitive hashing for approximating the
𝑘nearest neighbors.
The effect of hyperparameter 𝑘:The density-adjusted algo-
rithms come with a hyperparameter 𝑘. We compare different values
 
113KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
of𝑘∈{10,20,50,100}. The results presented in Figure 7 in the
appendix show that the choice of 𝑘does not noticeably affect the
performance of our algorithms.
Validating the assumptions on the datasets: We conduct
some experiments to validate our assumptions on the datasets.
For validating assumption 2, we will calculate the percentage of
positive points around each positive point by considering its 100
nearest points. The results are presented in table 4. For validating
assumption 3, we will compare the density of each point with its
100nearest points. The results are presented in table 3.
6.1 Experiment Setup
For each dataset, we use all the available data and their original
features for evaluating the results. Throughout the experiments,
we set the initial sample size to be a uniformly random sample
of1
60of the datapoints. We then run the experiments using each
class as the positive label and the rest of the classes as negative
labels (note that this turns the problem into a binary classification).
The only hyperparameter in our algorithm is 𝑘. In the course of
the experiments, we set 𝑘=50, and for visualization, we set the
batch size to1
60of the size of the datasets. Subsequently, we plot
the percentage of positives retrieved against the number of batches.
For each experiment, we run it 5times and average out the results.
The experiments were conducted on a machine equipped with
an Intel(R) Xeon(R) CPU running at 2.20 GHz. Running the exact
experiments on the full datasets requires huge computational power,
therefore we use locality-sensitive hashing as an approximation for
calculating the nearest neighbors.
Time Complexity Analysis. All parts of our algorithms are
linear (treating 𝑘as a constant), except for finding the nearest
neighbors where we use locality-sensitive hashing. Therefore our
algorithms have a time complexity of O(𝑛+𝑙)where𝑙is the time
complexity for finding the nearest neighbors of all the datapoints
with locality-sensitive hashing, which is practically linear and sub-
quadratic in theory.
6.2 Datasets
The experiments are tested on the following datasets.
UCI Letters, consisting of 20,000 datapoints with 16numerical
features and 26classes representing each letter. The dataset has been
obtained by generating letters using 20fonts, which were randomly
distorted to generate 20,000 unique images. Sixteen features have
been extracted as a result of pixel count and correlations.
MNIST, consisting of 70,000 28×28grayscale images of handwritten
digits and 10classes each representing a digit. We use the pixel
intensities of the images as the features.
CIFAR10, consisting of 60,000 32×32colorized images, and 10
classes representing an object in the image. Same as MNIST, we use
the pixel intensities of the images as the features.
Fashion MNIST, consisting of 70,000 28×28grayscale images
each associated with a label from 10classes. Same as MNIST, we
use the pixel intensities of the images as the features.
Mini-Imagenet, consisting of 60,000 84×84colorized images, and
100 classes organized using the WordNet hierarchy. Same as MNIST,
we use the pixel intensities of the images as the features.Dataset Label Offline Active DANA DAA
MNIST0 90.47 92.98 91.74 93.40
1 92.64 92.77 87.85 92.74
2 76.11 86.82 91.80 93.26
3 81.05 85.17 90.77 92.90
4 83.84 88.68 90.67 93.29
CIFAR100 65.85 72.76 64.64 78.45
1 46.09 52.02 74.53 75.85
2 66.26 68.29 58.98 70.06
3 50.12 53.36 62.82 66.75
4 70.48 71.53 61.43 72.10
UCIA 91.86 96.23 93.30 96.38
B 86.84 94.99 85.00 95.97
C 87.61 95.45 89.85 96.17
D 86.47 94.85 85.41 95.81
E 84.07 93.72 80.71 95.01
Fash. MNIST0 84.79 87.06 88.75 90.85
1 92.10 92.68 92.05 93.33
2 82.96 84.64 86.58 90.12
3 88.06 89.25 90.58 91.93
4 82.81 85.38 86.37 89.76
Mini-Imagenet0 57.02 64.32 72.33 76.42
1 68.67 71.05 58.48 67.74
2 60.83 65.53 65.60 71.03
3 75.44 81.22 60.22 81.28
4 74.52 84.50 81.71 87.99
C. C. Fraud 2013 + 46.96 71.40 93.98 95.19
C. C. Fraud 2023 + 47.22 85.49 94.74 94.84
Table 1: Area under the curve of each algorithm for 7different
datasets. Each experiment is performed 5times and averaged
across all runs. The highest values are bolded out.
Credit Card Fraud, consisting of transactions made by credit
cards by European card holders. It contains only numerical input
variables which are the result of a PCA transformation.
6.3 Evaluation Metrics
We plot the percentage of positives retrieved against the number
of batches to demonstrate the performance of each of the methods.
We calculate the area under the curve for each of the methods as
an evaluation metric (table 1). Note that as the domain of positive
datapoints is not well defined and the excess cost and the number of
positive datapoints add up to the total number of queried datapoints,
this is a reasonable evaluation metric. The percentage of datapoints
required for reaching 95%and98%of the positive datapoints is also
calculated as an alternative evaluation metric. To get more accurate
results, each experiment is performed 5times and averaged across
all runs. The results for the 95% and 98% metrics are briefly discussed
in the next subsection, however, due to the space limit, the detailed
results for 95% are deferred to the appendix (See Table 2) and the
detailed results for 98% are deferred to the full version.
 
114Improved Active Covering via
Density-Based Space Transformation KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: The percentage of positive samples retrieved after each batch on image datasets for each of the algorithms is plotted.
Figure 6: The percentage of positive samples retrieved after
each batch on the Credit Card Fraud datasets for each of the
algorithms is plotted.
6.4 Results
In each case, the result is either competitive or the density-adjusted
adaptive algorithm outperforms the other algorithms. The plots are
shown in Figures 5 and 6.
1. MNIST. The density-adjusted adaptive algorithm outperforms
the other algorithms on 4out of 5tasks on all three metrics. It
outperforms 3of them by a large margin.
2. CIFAR10. The density-adjusted adaptive algorithm outperforms
the other algorithms on 4out of 5tasks on all three metrics, out-
performing 2of them by a large margin.
3. UCI. The active density algorithm outperforms the other algo-
rithms on all of the 5tasks for the area under the curve metric, but
the results are closely competitive.4. Fashion MNIST. The density-adjusted adaptive algorithm out-
performs the other algorithms on all of the tasks on all three metrics.
It outperforms 3of them by a large margin.
5. Mini-Imagenet. The density-adjusted adaptive algorithm out-
performs the other algorithms on 3out of 5tasks by a large margin
on all three metrics. It performs competitively on 1of the remaining
tasks and falls off on the last task.
6. Credit Card Fraud. The density-adjusted adaptive and non-
adaptive algorithms outperform the other algorithms on both
datasets by a large margin on all three metrics.
7 CONCLUSION
By considering the density function of the examples and adjusting
the distance function, we present two algorithms DANA and DAA
for active covering. Under some necessary conditions, we prove
that both our algorithms discover all of the positive labels with a
sublinear excess cost for constant-dimensional spaces. Moreover,
our experiments on the same set of datasets used by the previous
work show the superiority of our method.
We admit that our method avoids sudden changes in the density
function (By Assumption 3). It might be possible to avoid this as-
sumption by considering a distance function that takes the average
of the density function over a path between the two endpoints.
However, calculating this distance function may not be trivial in
practice. We leave this as an open problem for future work.
 
115KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
REFERENCES
[1]Umang Aggarwal, Adrian Popescu, and Céline Hudelot. 2020. Active learning
for imbalanced datasets. In IEEE/CVF-WACV. 1428–1437.
[2]John O Awoyemi, Adebayo O Adetunmbi, and Samuel A Oluwadare. 2017. Credit
card fraud detection using machine learning techniques: A comparative analysis.
InICCNI. IEEE, 1–9.
[3]Gérard Biau, Benoît Cadre, and Bruno Pelletier. 2008. Exact rates in density
support estimation. Journal of Multivariate Analysis 99, 10 (2008), 2185–2207.
[4]Avrim Blum. 1990. Learning boolean functions in an infinite attribute space. In
STOC. 64–72.
[5]Avrim Blum, Lisa Hellerstein, and Nick Littlestone. 1995. Learning in the presence
of finitely or infinitely many irrelevant attributes. J. Comput. System Sci. 50, 1
(1995), 32–40.
[6]Kamalika Chaudhuri and Sanjoy Dasgupta. 2010. Rates of convergence for the
cluster tree. NeurIPS 23 (2010).
[7]Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Ra-
jagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. 2021. Batch active learning
at scale. NeurIPS 34 (2021), 11933–11944.
[8]David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with
active learning. Machine learning 15 (1994), 201–221.
[9]David A Cohn, Zoubin Ghahramani, and Michael I Jordan. 1996. Active learning
with statistical models. Journal of artificial intelligence research 4 (1996), 129–145.
[10] Antonio Cuevas and Ricardo Fraiman. 1997. A plug-in approach to support
estimation. The Annals of Statistics (1997), 2300–2312.
[11] Luc Devroye and Gary L Wise. 1980. Detection of abnormal behavior via non-
parametric estimation of the support. SIDMA 38, 3 (1980), 480–488.
[12] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active
learning with image data. In ICML. PMLR, 1183–1192.
[13] Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, and
Richard Mann. 2012. Bayesian optimal active search and surveying. arXiv
preprint arXiv:1206.6406 (2012).
[14] Dilek Hakkani-Tür, Giuseppe Riccardi, and Allen Gorin. 2002. Active learning
for automatic speech recognition. In ICASSP, Vol. 4. IEEE, IV–3904.
[15] David P Helmbold, Nicholas Littlestone, and Philip M Long. 2000. Apple tasting.
Information and Computation 161, 2 (2000), 85–139.
[16] Lalit Jain and Kevin G Jamieson. 2019. A new perspective on pool-based active
classification and false-discovery control. NeurIPS 32 (2019).
[17] Heinrich Jiang and Afshin Rostamizadeh. 2021. Active Covering. In ICML. PMLR,
5013–5022.
[18] Shali Jiang, Roman Garnett, and Benjamin Moseley. 2019. Cost effective active
search. NeurIPS 32 (2019).
[19] Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, and Roman
Garnett. 2018. Efficient nonmyopic batch active search. NeurIPS 31 (2018).
[20] Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. 2009. Multi-class
active learning for image classification. In CVPR. IEEE, 2372–2379.
[21] Seyed Mehran Kazemi, Anton Tsitsulin, Hossein Esfandiari, MohammadHossein
Bateni, Deepak Ramachandran, Bryan Perozzi, and Vahab Mirrokni. 2022. Tack-
ling Provably Hard Representative Selection via Graph Neural Networks. arXiv
preprint arXiv:2205.10403 (2022).
[22] Amir E Khandani, Adlar J Kim, and Andrew W Lo. 2010. Consumer credit-risk
models via machine-learning algorithms. Journal of Banking & Finance 34, 11
(2010), 2767–2787.
[23] Alexander P Korostelev and Aleksandr Borisovich Tsybakov. 1993. Estimation
of the density support and its functionals. Problemy Peredachi Informatsii 29, 1
(1993), 3–18.
[24] Kang Li, Zhenyu Zhong, and Lakshmish Ramaswamy. 2008. Privacy-aware
collaborative spam filtering. IEEE Transactions on Parallel and Distributed systems
20, 5 (2008), 725–739.
[25] Wolfgang Maass. 1991. On-line learning with an oblivious environment and the
power of randomization. International Computer Science Institute.
[26] Venkata Vamsikrishna Meduri, Lucian Popa, Prithviraj Sen, and Mohamed Sarwat.
2020. A comprehensive benchmark framework for active learning methods in
entity matching. In SIGMOD. 1133–1147.
[27] Michael Mitzenmacher and Eli Upfal. 2017. Probability and computing: Random-
ization and probabilistic techniques in algorithms and data analysis. Cambridge
university press.
[28] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang.
2016. Abusive language detection in online user content. In WWW. 145–153.
[29] Si-sheng Ou-Yang, Jun-yan Lu, Xiang-qian Kong, Zhong-jie Liang, Cheng Luo,
and Hualiang Jiang. 2012. Computational drug discovery. Acta Pharmacologica
Sinica 33, 9 (2012), 1131–1140.
[30] KC Santosh. 2020. AI-driven tools for coronavirus outbreak: need of active
learning and cross-population train/test models on multitudinal/multimodal data.
Journal of medical systems 44 (2020), 1–5.
[31] Christopher Schröder and Andreas Niekler. 2020. A survey of active learning for
text classification using deep neural networks. arXiv preprint arXiv:2008.07267
(2020).[32] Aarti Singh, Clayton Scott, and Robert Nowak. 2009. Adaptive hausdorff estima-
tion of density level sets. The Annals of Statistics 37, 5B (2009), 2760–2782.
[33] Petr Slavík. 1996. A tight analysis of the greedy algorithm for set cover. In STOC.
435–441.
[34] Hui Ru Tan, Wei Heng Chng, Christian Chonardo, Magdeline Tao Tao Ng, and
Fun Man Fung. 2020. How chemists achieve active learning online during the
COVID-19 pandemic: using the Community of Inquiry (CoI) framework to sup-
port remote teaching. Journal of Chemical Education 97, 9 (2020), 2512–2518.
[35] Puning Zhao and Lifeng Lai. 2022. Analysis of knn density estimation. IEEE
Transactions on Information Theory 68, 12 (2022), 7971–7995.
A ADDITIONAL EXPERIMENT DETAILS
Algorithm 3 Density-adjusted Non-Adaptive Algorithm (DANA)
without access to 𝑓
Input Dataset𝑋, initial sample size 𝑚and density parameter 𝑘.
1:Let𝑟𝑥be the distance of 𝑥to its𝑘’th nearest neighbor in 𝑋.
2:Let𝑋0be𝑚examples sampled uniformly without replacement
from𝑋.
3:Label query 𝑋0and let𝑋+,0be the positive examples.
4:Label query remaining examples in ascending order of
𝑑(𝑥,𝑋+,0)
𝑟𝑥until all positive examples are labeled.
Algorithm 4 Density-adjusted Adaptive Algorithm (DAA) without
access to𝑓
Input Dataset𝑋, initial sample size 𝑚and density parameter 𝑘.
1:Let𝑟𝑥be the distance of 𝑥to its𝑘’th nearest neighbor in 𝑋.
2:Let𝑋0be𝑚examples sampled uniformly without replacement
from𝑋.
3:Label query 𝑋0and let𝑋+,0be the positive examples.
4:Initialize𝑋𝑝←𝑋+,0and𝑋𝑎←𝑋0
5:while not all positive examples in 𝑋are labeled do
6: Label query 𝑥=arg min𝑥∈𝑋\𝑋𝑎𝑑(𝑥,𝑋𝑝)
𝑟𝑥
7: if𝑥has a positive label then
8:𝑋𝑝←𝑋𝑝∪{𝑥}
9: end if
10:𝑋𝑎←𝑋𝑎∪{𝑥}
11:end while
Dataset 100% 99.9% 99.5% 99% 98% 95%
MINST 0.16 0.28 0.38 0.47 0.57 0.67
CIFAR10 0.30 0.43 0.48 0.50 0.53 0.58
UCI 0.19 0.41 0.53 0.57 0.62 0.69
Fashion MNIST 0.24 0.39 0.47 0.51 0.55 0.63
Mini-Imagenet 0.27 0.42 0.47 0.50 0.53 0.58
Table 3: Verifying Assumption 3: The density ratio between
each point and its 100nearest neighbors is computed. For each
threshold 𝑡∈100%,99.9%,99.5%,99%,98%,95%, the displayed
values represent the minimum ratio observed among at least
𝑡percent of the computed ratios.
 
116Improved Active Covering via
Density-Based Space Transformation KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 7: The percentage of positive samples retrieved after each batch, using the DAA algorithm for the first three labels with
different values of 𝑘is plotted.
Dataset Lab
elOffline A
ctive D
ANA D
AA
MNIST0 27.62 13.79 17.57 11.30
1 12.33 12.20 30.03 12.49
2 63.51 32.05 17.42 11.50
3 56.46 38.13 20.39 12.88
4 44.50 27.55 22.35 11.74
CIF
AR100 87.38 79.33 88.56 71.03
1 93.08 91.18 69.29 65.87
2 87.10 83.07 90.80 81.19
3 95.07 93.09 83.19 79.44
4 81.80 78.36 86.20 79.22
UCIA 38.82 8.25 23.99 6.55
B 46.12 13.08 51.46 8.90
C 48.49 16.13 38.01 8.54
D 51.06 14.22 51.29 9.35
E 54.52 20.21 63.48 17.32
Fashion0 53.03 40.56 26.18 20.20
1 16.75 15.01 17.18 11.34
2 48.09 43.44 31.54 25.34
3 35.47 32.30 20.80 18.11
4 51.07 41.78 32.71 25.20
Mini-Imagenet0 90.17 88.72 73.74 72.30
1 83.48 84.86 86.65 82.22
2 93.94 89.23 82.76 78.78
3 73.53 67.45 89.31 72.73
4 73.28 61.91 86.10 59.74
C.
C. Fraud 2013 + 99.56 87.84 26.05 20.63
C.
C. Fraud 2023 + 98.92 86.39 21.69 22.26
Table 2: The percentage of datapoints required, to obtain 95%
of the positives labels.Dataset #100% 99.9% 99.5% 99% 98% 95%
MNIST00.01 0.02 0.09 0.14 0.30 0.68
10.01 0.01 0.06 0.16 0.55 0.99
20.01 0.01 0.02 0.03 0.06 0.15
30.01 0.01 0.03 0.05 0.09 0.21
40.01 0.01 0.02 0.05 0.13 0.28
CIF
AR1000.01 0.01 0.02 0.02 0.03 0.05
10.01 0.01 0.01 0.01 0.01 0.01
20.01 0.02 0.03 0.04 0.06 0.08
30.01 0.01 0.01 0.02 0.02 0.03
40.01 0.01 0.02 0.03 0.04 0.06
UCIA0.10 0.10 0.11 0.12 0.14 0.18
B0.10 0.10 0.12 0.14 0.16 0.18
C0.03 0.03 0.07 0.08 0.11 0.17
D0.07 0.07 0.10 0.13 0.15 0.19
E0.06 0.06 0.11 0.13 0.15 0.20
Fash.
MNIST00.01 0.01 0.02 0.03 0.05 0.14
10.01 0.01 0.03 0.04 0.08 0.21
20.01 0.01 0.02 0.03 0.07 0.14
30.01 0.01 0.02 0.03 0.06 0.16
40.01 0.01 0.03 0.05 0.09 0.15
Mini-Imagenet00.01 0.01 0.01 0.01 0.01 0.01
10.01 0.01 0.01 0.01 0.01 0.01
20.01 0.01 0.01 0.01 0.01 0.01
30.01 0.01 0.01 0.01 0.01 0.01
40.01 0.01 0.01 0.01 0.01 0.01
Table 4: Verifying Assumption 2: The percentage of pos-
itive points within the 100nearest neighbors of each
positive point is computed. For each threshold 𝑡∈
{100%,99.9%,99.5%,99%,98%,95%}, the depicted values repre-
sent the proportion of positive points found within at least 𝑡
percent of the positive points.
 
117KDD ’24, August 25–29, 2024, Barcelona, Spain MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, and Alipasha Montaseri
B PROOF OF THEOREM 12
Proof. First, using a simple Markov inequality we show that
𝐴𝐿𝐺′has an excess query cost of at most 𝑂(𝐶𝐴𝐿𝐺)+˜𝑂(1)with
probability at least 1−𝛿, then we show that 𝐴𝐿𝐺′queries all of the
positive examples with probability at least 1−2𝛿.
Note that, when 𝐴𝐿𝐺 queries the last positive example, its the
expected excess query cost is 𝐶𝐴𝐿𝐺. Hence by Markov inequality,
when𝐴𝐿𝐺 queries all of the positive examples its excess query cost
is at most𝐶𝐴𝐿𝐺
𝛿with probability at least 1−𝛿. After that, 𝐴𝐿𝐺
only queries negative examples. Hence 𝐴𝐿𝐺′stops after querying
at most𝐶𝐴𝐿𝐺
𝛿+log(𝑛
𝛿)extra queries. Hence the excess query cost
of𝐴𝐿𝐺′is at most𝐶𝐴𝐿𝐺
𝛿+𝐶𝐴𝐿𝐺
𝛿+log(𝑛
𝛿𝐶+)∈𝑂(𝐶𝐴𝐿𝐺)+˜𝑂(1)as
claimed.
Next, we show that 𝐴𝐿𝐺′queries all of the positive examples
with probability at least 1−2𝛿. In order to show this we show
that the probability of observing𝐶𝐴𝐿𝐺
𝛿+log(𝑛
𝛿𝐶+)consecutive
negative labels before the last positive example is at most 2𝛿. As we
mentioned above the probability that we query more than𝐶𝐴𝐿𝐺
𝛿
examples out of the domain of the positive examples is at most
𝛿. Hence, with probability at least 1−𝛿log(𝑛
𝛿𝐶+)out of𝐶𝐴𝐿𝐺
𝛿+
log(𝑛
𝛿𝐶+)consecutive negative labels are queried from the domain
of positive examples. In the rest, we look at the queries that have
been made from the domain of positive examples and bound the
probability that log(𝑛
𝛿𝐶+)consecutive queries from the domain of
positive examples are negative, then we apply a union bound over
all such subsequences to calculate the probability of failure.
Note that each query from the domain of positive examples
is positive with probability at least 𝐶+. Hence the probability that
log(𝑛
𝛿𝐶+)consecutive queries from the domain of positive examples
are negative is
(1−𝐶+)log(𝑛
𝛿𝐶+)=𝑒−log(𝑛
𝛿)=𝛿
𝑛.
There are at most 𝑛such subsequences, and hence the probability
that we observe one such subsequence is at most 𝛿.□
CEMPIRICAL ESTIMATION OF 𝜆0AND𝜆1[17]
In this section, we conduct some experiments to estimate the param-
eters𝜆0and𝜆1as defined by Jiang and Rostamizadeh [17] acrossdifferent datasets. The parameter 𝜆0represents the lower bound
of𝑓+(𝑥)and𝜆1represents the upper bound of 𝑓−(𝑥). For a given
𝑘, we empirically estimate 𝑓(𝑥)by dividing 𝑘by the volume of
the smallest ball containing the 𝑘nearest neighbors of 𝑥and mul-
tiplying this result by the volume of the smallest enclosing ball
divided by the total number of points in the dataset. Note that the
computed values are scaled by the volume of the smallest enclosing
ball. The results indicate that the parameters and their difference
are significantly large, showing that even scaling alone will not be
effective.
Dataset Label log10𝜆0log10𝜆1
MNIST0 127 680
1 90 475
2 105 693
3 110 693
4 92 690
CIFAR0 996 2828
1 1025 2768
2 989 2672
3 886 2851
4 948 2712
UCI0 3 14
1 5 15
2 4 15
3 5 15
4 4 14
Fashion0 126 746
1 80 713
2 182 744
3 195 746
4 152 742
Mini-Imagenet0 394 2341
1 411 2326
2 445 2418
3 765 2375
4 638 2394
Table 5: Approximate values of log10(𝜆0)and log10(𝜆1)across
different datasets for 𝑘=100.
 
118