TrajRecovery: An Efficient Vehicle Trajectory Recovery
Framework based on Urban-Scale Traffic Camera Records
Dongen Wu
Zhejiang University
Hangzhou, China
wude@zju.edu.cnZiquan Fang
Zhejiang University
Hangzhou, China
zqfang@zju.edu.cnQichen Sun
Zhejiang University
Hangzhou, China
sunqichen@zju.edu.cn
Lu Chen
Zhejiang University
Hangzhou, China
luchen@zju.edu.cnHaiyang Hu
Huawei Cloud Computing
Technologies Co., Ltd
Hangzhou, China
huhaiyang2@huawei.comFei Wang
Huawei Cloud Computing
Technologies Co., Ltd
Hangzhou, China
wangfei1@huawei.com
Yunjun Gao
Zhejiang University
Hangzhou, China
gaoyj@zju.edu.cn
ABSTRACT
Accurate vehicle trajectory recovery enables providing indispens-
able data foundations in intelligent urban transportation. However,
existing methods face two challenges: i) the inability to process
city-wide vehicle trajectories, and ii) the dependence on a substan-
tial amount of accurate GPS trajectories for model training, leading
to poor generalization ability. To address these issues, we propose
a novel trajectory recovery system based on vehicle snapshots
captured by traffic cameras, named TrajRecovery. TrajRecovery
consists of three main components: i) Preprocessor processes traf-
fic cameras and vehicle snapshots to provide necessary data for
trajectory recovery; ii) Spatial Transfer Probabilistic Model (STPM)
integrates road conditions and driver behavior to compute turning
probability at intersections; iii) Trajectory Generator utilizes the
output probabilities from STPM to recover a continuous and most
likely complete trajectory. We evaluate TrajRecovery on two real
datasets from a city in China, demonstrating substantial perfor-
mance gains compared to state-of-the-art methods. Furthermore,
our system is deployed in practical applications at Huawei Com-
pany, achieving extraordinary profits in business scenarios.
CCS CONCEPTS
•Information systems →Spatial-temporal systems .
KEYWORDS
vehicle trajectory recovery, deep learning, mobile sensing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671558ACM Reference Format:
Dongen Wu, Ziquan Fang, Qichen Sun, Lu Chen, Haiyang Hu, Fei Wang,
and Yunjun Gao. 2024. TrajRecovery: An Efficient Vehicle Trajectory Recov-
ery Framework based on Urban-Scale Traffic Camera Records. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671558
1 INTRODUCTION
The proliferation of positioning devices enables the generation
of vehicle trajectories, which helps in understanding traffic sys-
tems [ 41]. Based on the collected trajectories, researchers perform
various traffic analyses such as traffic prediction [ 3,19], travel time
estimation [ 4,37,40], movement pattern mining [ 5,6,22,36,43],
etc. However, in real-world applications, the raw trajectory data
typically have low sampling rates [38]. For example, empty taxis
tend to lower the sampling rates of GPS devices to save battery
consumption. Complex urban environments like tall buildings or
network transmission interruptions can also result in low-sampling
trajectories. To provide a good-quality data foundation for trajec-
tory analytics, trajectory recovery is widely studied, where the
general goal is to recover accurate trajectory data based on the raw
and low-sampling trajectory data.
Due to its great benefits, the data mining community has pro-
posed many trajectory recovery methods [2, 15, 24, 30, 33], which
can refer to Section 6. Specifically, they target GPS-based trajec-
tory recovery. However, GPS devices and that-based trajectory
data are only available to public transportation such as taxis and
buses, rather than all vehicles [ 1]. Considering the relatively low
installation rate of GPS devices [ 1,34] and user privacy protection
laws, obtaining GPS-based trajectories for most private vehicles is
scarcely possible. Consequently, most of the existing studies are
limited to general vehicle trajectory recovery.
In this paper, we aim to propose a new trajectory recovery
method that does not rely on GPS trajectories but enables general
vehicle trajectory recovery. We found that, in urban areas, traffic
5979
KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
Time: 2023/05/02 19:12:39
Camera ID: A1
Time: 2023/05/02 19:21:16Camera ID: C1Time: 2023/05/02 20:57:42
Camera ID: D1
Time: 2023/05/02 19:26:17
Camera ID: A2ID:  A1Locations: (25.55001,119.75324)Name: zhong shan da dao north
ID:  C1Locations: (0, 0)
Name: hai tan lu yuan eastID:  B1
Locations: (0, 0)
Name: None
ID:  A2Locations: (27.46163,120.30440)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: C2ID:  C2Locations: (25.50079,119.77782)Name: None
Mismatch
Out of scope
network 
malfunctions
Time: 2023/05/02 19:12:39
Camera ID: A1
Time: 2023/05/02 19:21:16
Camera ID: C1Time: 2023/05/02 20:57:42
Camera ID: D1
Time: 2023/05/02 19:26:17
Camera ID: A2ID:  A1Locations: (25.55001,119.75324)Name: zhong shan da dao north
ID:  C1Locations: (0, 0)
Name: hai tan lu yuan eastID:  B1
Locations: (0, 0)
Name: None
ID:  A2
Locations: (27.46163,120.30440)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: C2ID:  C2Locations: (25.50079,119.77782)Name: None
ID:  A1
Locations: (25.55001,119.75324)Name: Zhong Shan Avenue north
Time: 2023/05/02 19:12:39Camera ID: A1
ID:  A2Locations: (25.53000,119.74810)
Name: None
Time: 2023/05/02 20:57:42
Camera ID: A2
ID:  C1
Locations: (0, 0)
Name: Hai Tan Lu Yuan East
Time: 2023/05/02 19:21:16
Camera ID: C1
ID:  B1
Locations: (0, 0)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: B1
ID:  C2Locations: (25.50079,119.77782)Name: None
Time: 2023/05/02 19:32:36Camera ID: C2
network malfunctions
Mismatch1
2
3
4
5
ID:  A1
Locations:  (27.55001,130.75324)
Name:  City Avenue North
Time:  2023/05/02 14:12:39
Camera ID:  A1
ID:  A2
Locations:  (27.53000,130.74810)
Name:  None
Time:  2023/05/02 18:57:42
Camera ID:  A2
ID:  B1
Locations:  (0, 0)
Name:  None
Time:  2023/05/02 14:32:36
Camera ID:  B1
ID:  C2
Locations:  (27.50079,130.77782)
Name:  None
Time:  2023/05/02 14:32:36
Camera ID:  C2
1
2
3
4
Mismatch
Wrong RecordRight Record
Figure 1: An Example of Sparse Vehicle Passing Records.
cameras operate 24/7, continuously capturing passing records of
all vehicles [ 12]. Combined with vehicle re-identification technolo-
gies [ 8,10,17,20,26], these traffic cameras present a promising
way for general vehicle recovery based on their passing records. In
contrast to GPS-based trajectory recovery studies, the passing (cam-
era) records-based methods [ 16,34] involve two main tasks, i.e.,
vehicle re-identification and trajectory recovery. However, it is non-
trivial to design such a passing record-based trajectory recovery
framework due to the following three challenges.
Challenge 1: How to utilize sparse vehicle passing records
for effective trajectory recovery? Although surveillance cameras
operate 24/7, the observed vehicle passing records are sparse due
to three aspects. i) First, not all road segments are covered by cam-
eras [ 1,28], making it challenging to predict vehicle path choices
in unobserved road segments. We have collected over 20 million
vehicle snapshots recorded by 491 traffic cameras in a city of China,
covering a continuous period of five days. Upon analyzing these
data, we find that traffic cameras only cover 7% of the road seg-
ments, leaving the remaining 93% unobservable. ii) Second, network
malfunctions, image deterioration, and camera mismatch may re-
sult in the partial failure of vehicle detection, further exacerbating
the sparsity problem. Fig. 1 shows an example. As observed, camera
B1 failed to be accurately matched to the road network, attributed
to the lack of location information and device names. Consequently,
the vehicle passing records captured by these cameras were invalid.
Furthermore, network malfunctions in camera A2 led to a temporal
deviation in the occurrence time of the second passing record, ren-
dering it invalid. It is important to note that, due to non-technical
reasons, all coordinates and road names in this paper have under-
gone offsetting or obfuscation. iii) Last but not least, despite the
broad distribution of traffic cameras, the distances between these
devices can still be long, making it difficult for trajectory recovery.
Overall, constructing an effective trajectory model using sparse
vehicle passing records is challenging.
Challenge 2: How to generalize learned knowledge to un-
observed road segments? Most existing GPS-trajectory-oriented
recovery methods typically require massive observed trajectory
data for model training. When the model encounters road segmentsnot encompassed in the training data, it encounters difficulties in re-
covery. However, in real-life scenarios, a notable proportion of road
segments, particularly in suburban areas, lack camera coverage. In
that case, how to generalize learned knowledge from observed road
segments to unobserved road segments is challenging.
Challenge 3: How to achieve personalized trajectory recovery
that considers the mobility preferences of different vehicles?
When performing trajectory recovery, previous methods tend to
treat all drivers equally from a coarse-grained perspective and thus
neglect the preference diversity in driving habits. This oversight
led to the recovery of identical trajectories for different vehicles
under similar contextual conditions. Therefore, we are inspired to
learn the driving habits of drivers from their historical behavior,
thereby achieving fine-grained trajectory recovery. Consequently,
how to learn the driver’s driving habits to achieve personalized
trajectory recovery is challenging.
In this paper, we propose TrajRecovery, an efficient framework
for vehicle trajectory recovery utilizing traffic camera records. To
address Challenge 1, we construct a comprehensive preprocessing
pipeline to ensure the precise alignment of traffic cameras with road
networks. Furthermore, we introduce a simple yet highly effective
Historical Passing Record Retriever (HPRR) to retrieve comparable
passing records from the historical passing records, thereby enhanc-
ing the sampling rate of sparse passing records. To address Chal-
lenge 2 , we design a Spatial Transfer Probability Model (STPM) to
capture driver turning probability in camera-covered road segments
through vehicle snapshots. To extend this knowledge to areas with-
out camera coverage, we develop a Motion-GAT Module to capture
the intrinsic connections and features between road segments by
aggregating the characteristics of surrounding road segments. By
incorporating the Motion-GAT Module into STPM, we ensures that
STPM emphasizes the impact of road conditions on turning at the
urban level, rather than specific turns between road segments, thus
enabling more effective knowledge transfer. To address Challenge
3, we introduce the OD-Attention Module in STPM, which incorpo-
rates attention mechanisms into the retrieval of historical driving
behaviors. By utilizing driving habits, the OD-Attention Module
enables vehicle-level personalized trajectory recovery, avoiding the
same results for identical OD scenarios. Overall, this paper makes
the following contributions.
•To the best of our knowledge, we present the first framework
for trajectory recovery that solely utilizes traffic camera
records instead of GPS data, offering comprehensive support
for a wide range of vehicles’ trajectory recovery.
•To effectively generalize learned knowledge to unobserved
road segments, we develop a novel Motion-GAT Module to
emphasize the road conditions on turning at the urban level,
instead of specific turns between camera-covered segments.
•To enable personalized trajectory recovery, we introduce a
novel OD-Attention to learn drivers’ different habits from
their historical driving behaviors.
•Extensive experiments using two real datasets show that
TrajRecovery significantly outperforms state-of-the-art base-
lines, e.g., 2.6% - 12.2% improvement in recovery quality.
5980TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD ’24, August 25–29, 2024, Barcelona, Spain
•Finally, the deployment of TrajRecovery in practical applica-
tions at HuaWei Company, China, showcasing outstanding
performance in real-world business scenarios.
The paper is organized as follows. Section 2 presents preliminar-
ies and a problem statement, and the proposed system is detailed
in Section 3. Experimental results are reported in Section 4, and the
practical deployment of TrajRecovery is introduced in Section 5.
Section 6 reviews related work and Section 7 concludes this paper.
2 PRELIMINARIES
2.1 Definitions
Definition 1 (Traffic Camera). The traffic camera set is denoted as
𝐶={𝑐1,𝑐2,···,𝑐𝑁}, where𝑁is the total number of traffic cameras,
and each traffic camera is represented as 𝑐𝑖=<𝐿𝑎𝑡,𝐿𝑜𝑛,𝑁𝑎𝑚𝑒,𝐷𝑖𝑟𝑒𝑐 >,
where𝐿𝑎𝑡,𝐿𝑜𝑛 and𝑁𝑎𝑚𝑒 represent the latitude, longitude and
name of the traffic camera respectively, and 𝐷𝑖𝑟𝑒𝑐 is the shoot-
ing direction of traffic cameras (e.g., south to north). Note that, as
mentioned in Challenge 1, these information is unrealiable.
Definition 2 (Road Network). The road network is an essential
component for trajectory recovery. A road network is a directed
graph𝐺=(𝑉,𝐸), where𝑉={𝑣1,𝑣2,···,𝑣𝑀}refers to the set of
vertices representing road intersections, and 𝐸={𝑒1,𝑒2,···,𝑒𝐿}
refers to the set of edges representing the road segment between
intersections. Each road segment 𝑒∈𝐸contains three types of
properties: 1) length, width, number of lanes, number of in-degrees
and out-degrees; 2) level, such as rural, provincial way, national
way, etc.; 3) type, such as tunnel, pedestrian, side road, etc.
Definition 3 (Vehicle Trajectory). We define a vehicle trajectory
as a sequence of time-ordered passing records J𝑝=[𝑟1,𝑟2,···,𝑟𝑛]
for a vehicle over a continuous period. Each 𝑟𝑖=<𝑐,𝑡,𝑒 >rep-
resents a passing record, indicating that vehicle 𝑝is detected by
camera𝑟𝑖.𝑐at time𝑟𝑖.𝑡on road segment 𝑟𝑖.𝑒, where camera 𝑟𝑖.𝑐is
located. Further details of camera matching can be found in Sec-
tion 3.1. Each passing record corresponds to a vehicle snapshot, we
denote𝑅as the set of all vehicle snapshots.
Definition 4 (OD-Pair). For each vehicle passing record 𝑟𝑖, the
corresponding road segment 𝑟𝑖.𝑒is denoted as 𝑒𝑜, and the adjacent
road segment the vehicle is approaching is designated as 𝑒𝑑. Notably,
𝑒𝑑is inferred based on the identified vehicle’s lane position and
turn information from snapshots and the road network 𝐺. Then, we
identify the corresponding OD-pairs for each passing record, and
form a OD-pair trajectory J𝑂𝐷𝑝=[𝑂𝐷J
1,𝑂𝐷J
2,···,𝑂𝐷J
𝑛], where
𝑂𝐷J
𝑖=<𝑡𝑜,𝑒𝑜,𝑒𝑑>represents a OD-pair, indicating that vehicle
𝑝traveled from road segment 𝑂𝐷J
𝑖.𝑒𝑜to adjacent road segment
𝑂𝐷J
𝑖.𝑒𝑑at time𝑂𝐷J
𝑖.𝑡𝑜, where𝑂𝐷J
𝑖.𝑒𝑜is𝑟𝑖.𝑒and𝑂𝐷J
𝑖.𝑡𝑜is𝑟𝑖.𝑡.
Problem Statement. Given a traffic camera set 𝐶and the vehicle
snapshot data 𝑅from𝑁traffic cameras, our task is to recover the
sparse passing record trajectory into a continuous and complete
trajectory based on the road network 𝐺.
3 TRAJECTORY RECOVERY SYSTEM
To address the aforementioned challenges, as shown in Fig. 2, we in-
troduce a novel system named TrajRecovery, comprising three main
components: 1) the Preprocessor, 2) Spatial Transfer Probability
Model (STPM), and 3) Trajectory Generator.
Vehicle Clustering
Historical Passing 
Record Retriever
Spatial Temporal
Camera 
Map 
MatchingCamera Info.
 Road network
1
24
3Feature ExtractorMotion-GATOD-AttentionSpatial Transfer ProbabilitiesPostprocess
Trajectory Search
Dijkstra 
SearchGreedy 
Search
Recovery Trajectory
1
243
Preprocessor STPM Trajecotry Generator
െlog  ( Transfer Probabilities  )
Camera System
Vehicle Snapshots
Camera 
Map-Matching
Historical Passing 
Record Retriever
Historical Passing Record Retriever
ClusteringSTPM
Feature Extractor
Motion-GAT
OD-Attention
Historical Passing 
Record Retriever
Spatial Temporal
Camera 
Map 
MatchingCamera Info.Postprocess
Trajectory Search
Dijkstra 
SearchGreedy 
Search
Recovery TrajectoryPreprocessor Trajecotry Generator
െ𝐥𝐨𝐠   ( Transfer Probabilities  )Vehicle Snapshots
Clustering
Road network
Feature ExtractorMotion-GATOD-AttentionSpatial Transfer ProbabilitiesSTPM
1
24
3
1
243
Figure 2: System architecture.
The Preprocessor consists of three steps. Initially, we match
traffic cameras with road segments on the road network based on
their latitude, longitude, name and direction. Drawing inspiration
from previous studies [ 34,35], we extract visual features and license
plates from vehicles’ passing snapshot records. Subsequently, we
cluster vehicles based on the visual feature similarity, obtaining all
passing records for each vehicle. Finally, we introduce the Historical
Passing Record Retriever (HPRR) module, enhancing the sampling
rate of sparse trajectories by retrieving historical passing records
for the current vehicle and other vehicles.
Additionally, we devise the Spatial Transfer Probability Model
(STPM) to calculate the probability of a vehicle transitioning from
one road segment to its subsequent segment. The Feature Extractor
generates deep representations based on road segment properties
such as length, width, type, and level for each input road segment.
Following that, the Motion-GAT module characterizes the traf-
fic development and vehicle suitability of a road segment based
on its relationships with surrounding segments. Considering that
even with the same origin-destination (OD), different driving habits
may generate distinct trajectories, we introduce the OD-Attention
module to mine driving habits from a vehicle’s historical driving
behavior, achieving vehicle-level personalized trajectory recovery.
Finally, in the Trajectory Generator, we leverage the inference
probabilities from the STPM to recovery a continuous and proba-
bilistically optimal trajectory based on the road network.
3.1 Preprocessor
The Preprocessor encompasses traffic camera map matching, vehi-
cle snapshot processing and clustering, and upsampling.
Traffic camera map matching. Matching traffic cameras to
road segments on the road network is the foundation for trajectory
recovery from vehicle passing records. Initially, we categorize all
cameras into four classes based on their credibility derived from lat-
itude, longitude, and name information. Subsequently, we employ
distinct strategies to match cameras to corresponding intersections
under different situations: (1) Correct location and incorrect name:
match to the nearest intersection based on location; (2) Incorrect
location and correct name: utilize edit distance to identify the in-
tersection with the highest text similarity; (3) Correct location and
correct name: identify the top-k nearest intersections and calculate
text similarity with edit distance, selecting the intersection with the
highest text similarity; (4) Incorrect location and incorrect name:
5981KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
unmatchable. Once the corresponding intersection is identified,
we use the camera’s 𝐷𝑖𝑟𝑒𝑐𝑡𝑖𝑜𝑛 to locate the road segment in the
current direction of the intersection. Consequently, the camera is
then associated with this road segment, represented as 𝑐𝑖→𝑒𝑗,
indicating the binding of camera 𝑐𝑖to road segment 𝑒𝑗. This match-
ing strategy successfully matches 491 out of 602 cameras, covering
96% of passing records, ensuring the availability of these records.
Vehicle snapshot processing and clustering. When a vehicle
passes by, the traffic camera will capture all the content within
its field of view, including noise. Consequently, preprocessing the
snapshot is essential for accurate vehicle identification. Inspired
by advancements in object detection algorithm [ 29], we first lo-
cate the vehicle and lane (e.g. left-turn lane) from the snapshot
by the excellent detection ability of YOLOV7 [ 29], and extracting
256-dimensional vehicle appearance features 𝑓𝑎and vehicle plate
features𝑓𝑝. Inspired by [ 34], we then calculate the total similarity
between each pair of vehicle snapshots as:
𝑆𝑖,𝑗=𝑤𝑎𝑓𝑖𝑎·𝑓𝑗
𝑎+𝑤𝑝𝑓𝑖𝑝·𝑓𝑗
𝑝
𝑤𝑎+𝑤𝑝(1)
where the hyper-parameters 𝑤𝑎,𝑤𝑝represent the weight of vehicle
appearance features and plate features respectively. Considering
that the plate is more recognizable than the appearance of the
vehicle, we assign 𝑤𝑝to be larger than 𝑤𝑎. Then we use the total
similarity of each pair of vehicle snapshots for clustering. If the
total similarity between them is greater than a predefined threshold
𝑇ℎ, then they belong to the same cluster.
Upsampling. As mentioned in Section 1, the camera may oc-
casionally fail, resulting in vehicles passing by without obtaining
passing records. To address this, considering that people’s travel
often has repeatability [ 33], we devised a Historical Passing Record
Retriever to improve the sampling rate of passing records by re-
trieving the vehicle’s historical passing records before performing
trajectory recovery. Specifically, in order to find a trajectory similar
to the current trajectory from the historical passing records, we
consider both spatial and temporal aspects. From a spatial perspec-
tive, for any two consecutive passing records 𝑟𝑖,𝑟𝑖+1in the vehi-
cle trajectoryJ𝑝, we calculate the longest common subsequence
(LCSS) between the subtrajectory with a context window of size 𝑘
[𝑟𝑖−𝑘.𝑐,···,𝑟𝑖.𝑐,𝑟𝑖+1.𝑐,···,𝑟𝑖+1+𝑘.𝑐]and any sub-trajectory in his-
torical trajectory with a context window of size 𝑘. By this way, we
can obtain the top- 𝑞historical sub-trajectories with the highest spa-
tial similarity. From a temporal perspective, we sort the top- 𝑞histori-
cal subtrajectories in ascending order by |(𝑟𝑏.𝑡−𝑟𝑎.𝑡)−(𝑟𝑖+1.𝑡−𝑟𝑖.𝑡)|,
where𝑟𝑎,𝑟𝑏represent the historical passing records of cameras 𝑟𝑖.𝑐
and𝑟𝑖+1.𝑐in the historical subtrajectory, and 𝑟𝑏.𝑡−𝑟𝑎.𝑡>0. Then
we obtained the top- 𝑠subtrajectories with the highest temporal
similarity from the top- 𝑞. Finally, we identify the most frequent
camera𝑐𝑚and average travel time 𝑡𝑚from𝑐𝑚to𝑟𝑖+1.𝑐between
camera𝑟𝑖.𝑐and camera 𝑟𝑖+1.𝑐in the subtrajectories in top- 𝑠, and
insert the passing record <𝑐𝑚,𝑟𝑖.𝑡+𝑡𝑚>between𝑟𝑖and𝑟𝑖+1to
fill the missing passing record. By retrieving historical vehicle pass-
ing records, we can further increase the density of vehicle passing
records, which is more conducive to subsequent trajectory recovery.
Figure 3: Spatial Transfer Probability Model.
3.2 Spatial Transfer Probability Model
To estimate the probability of a road segment transitioning to its
successor, referred to as ’tuning probability’, employing probabil-
ity models is a widely accepted approach in trajectory recovery
tasks [ 11,16,32,35]. Considering that the traffic camera is diffi-
cult to record the continuous trajectory, previous studies rely on
high sampling rate GPS trajectories to establish a transfer prob-
ability model. However, as discussed in Section 1, obtaining GPS
trajectories with high sampling rates is challenging, and such data
often exhibits information bias. To address it, we propose a Spatial
Transfer Probability Model (STPM) to recover the most probable
trajectory between consecutive passing records based on vehicle
passing records.
Problem Optimization. To begin with, we refine the trajec-
tory recovery problem into a trajectory probability calculation
problem, which quantifies the probability of a certain trajectory as
the recovery result, and the recovered trajectory is the trajectory
with the highest probability. Traditionally, for a vehicle 𝑝, given
a start passing record 𝑟𝑠with its road segment 𝑟𝑠.𝑒and an end
passing record 𝑟𝑒with its road segment 𝑟𝑒.𝑒and timestamp 𝑟𝑒.𝑡, the
set of historical passing record H, the probability of a trajectory
R={R.𝑒0,···,R.𝑒|R|}between𝑟𝑠and𝑟𝑒can be expressed as:
𝑃𝑟(R|𝑟𝑠,𝑟𝑒,𝑝,H)=𝑃𝑟(R|𝑟𝑠.𝑒,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)
=𝑃𝑟(R.𝑒0,···,R.𝑒|R||𝑟𝑠.𝑒,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)
=|R|Ö
𝑖=1𝑃𝑟(R.𝑒𝑖|R.𝑒0···R.𝑒𝑖−1,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)
(2)
Considering that many studies have shown that human mobility
patterns follow Markov’s assumptions [ 16,31,32], we can further
convert formula (2) to:
𝑃𝑟(R|𝑟𝑠,𝑟𝑒,𝑝,H)=|R|Ö
𝑖=1𝑃𝑟(R.𝑒𝑖|R.𝑒𝑖−1,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H) (3)
5982TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD ’24, August 25–29, 2024, Barcelona, Spain
Accordingly, the trajectory recovery problem can be conceptualized
as finding a trajectory R∗with the highest conditional probability:
R∗=arg max
∀R∈𝐺𝑃𝑟(R|𝑟𝑠,𝑟𝑒,𝑝,H)
=arg max
∀R∈𝐺|R|Ö
𝑖=1𝑃𝑟(R.𝑒𝑖|R.𝑒𝑖−1,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)(4)
According to the guidance of Formula (4), we designed the
STPM to calculate 𝑃𝑟(R.𝑒𝑖|R.𝑒𝑖−1,𝑟𝑠.𝑒,𝑟𝑠.𝑡,𝑝,H), i.e. for vehicle
𝑝, given the destination 𝑟𝑒.𝑒, the arrival time 𝑟𝑒.𝑡, and the set of
historical passing record H, we aims to calculate the probability
of any road segment R.𝑒𝑖−1of the road network to its successor
road segmentR.𝑒𝑖. For eachR∈𝐺, we generate the correspond-
ing OD-pair trajectory R𝑂𝐷=[𝑂𝐷R
1,𝑂𝐷R
2,···,𝑂𝐷R
|R|], where
𝑂𝐷R
𝑖=<𝑟𝑒.𝑡,R.𝑒𝑖−1,R.𝑒𝑖>,∀𝑖,1≤𝑖≤|R| . SinceRcan represent
any trajectory from origin road segment 𝑟𝑠.𝑒to destination road
segment𝑟𝑒.𝑒, we can also denote 𝑂𝐷R
𝑖=<𝑟𝑒.𝑡,𝑒𝑎,𝑒𝑏>, where𝑒𝑏
is the successor road segment of road segment 𝑒𝑎. Therefore, we
can simplify 𝑂𝐷R
𝑖as𝑂𝐷𝑎𝑏.
3.2.1 Feature Extractor. To obtain dense representations of each
road segment in the road network, we devise a road segment fea-
ture extractor, as show in Fig. 3. First, to represent whether the
road segment is suitable for vehicle traffic and the level of road
development, we jointly embed the road segment’s properties and
time into dense representations as the input of following modules.
Specifically, for properties of category types (e.g., type, level), we
set up two trainable embedding matrix 𝐸𝑚𝑏𝑡𝑦𝑝𝑒∈R|#𝑡𝑦𝑝𝑒𝑠|×𝑑
and𝐸𝑚𝑏𝑙𝑒𝑣𝑒𝑙∈R|#𝑙𝑒𝑣𝑒𝑙𝑠|×𝑑, therefore, the road type embedding
vector and road level embedding vector of road segment 𝑒𝑖can
be represented as 𝑋𝑒𝑖
𝑡𝑦𝑝𝑒∈R𝑑and𝑋𝑒𝑖
𝑙𝑒𝑣𝑒𝑙∈R𝑑respectively. And
for numerical types (e.g., length, width, the distance close to the
destination), we use linear transformation to obtain 𝑑dimension
vectors𝑋𝑒𝑖𝑛𝑢𝑚∈R𝑑. We concatenate the above vectors to obtain
road segment-level hidden-state vector 𝑋𝑒𝑖
𝑟𝑜𝑎𝑑∈R3𝑑, followed by
linear transformation to obtain static representation of road seg-
ment𝑋𝑒𝑖𝑠∈R𝑑. Besides, consider that the traffic conditions on road
segments may vary during different time periods (such as morning
and evening rush hours), we divide a day into 48 time periods and
set up a trainable embedding matrix 𝐸𝑚𝑏𝑡𝑖𝑚𝑒∈R48×𝑑. Finally, the
spatio-temporal representation of road segment 𝑒𝑖at time𝑡𝑗can
be represented as:
𝑋𝑒𝑖,𝑡𝑗=𝑋𝑒𝑖𝑠+𝑋𝑡𝑗
𝑡(5)
3.2.2 Motion-GAT. The previously extracted road segment spatial-
temporal features focus primarily on the individual road segments.
However, it is insufficient for effectively generalize learned knowl-
edge from camera-covered segments to unobserved road segments.
To this end, considering that the traffic development level of the road
segment is influenced by its adjacent road segments, we propose
the Motion-GAT module. This module first extracts the subgraph
related to road segments from the road network 𝐺and then uses the
self-attention mechanism to capture the interaction information
between adjacent road segments and the current road segment.
Firstly, we extract subgraphs for each road segment 𝑒𝑖from the
road network. For each road segment 𝑒𝑖, its adjacent road segmentis denoted as 𝑒𝑎∈𝐴𝑑𝑗𝑖,𝐴𝑑𝑗𝑖={𝐴𝑑𝑗𝑖,𝑒𝑖}. We represented the
motion relationship between 𝑒𝑎and𝑒𝑖by concatenating their turn
information (e.g., left turn, right turn, straight, U-turn) and transfer
relationship (e.g., 𝑒𝑖→𝑒𝑎or𝑒𝑎→𝑒𝑖), resulting 8 distinct mo-
tion relationships. Subsequently, We set a trainable 𝑑-dimensional
motion vector 𝐸𝑚𝑏𝑚∈R8×𝑑for each motion relationship. Thus,
the motion vector for 𝑒𝑎is denoted as 𝑀𝑒𝑎,𝑡𝑗∈R𝑑. For clarity of
formulas, we omit the time variable 𝑡𝑗in the following discussion.
Moreover, we add the motion vector to the spatial-temporal feature
and obtain the final road segment representation 𝑋𝑀𝑒𝑎:
𝑋𝑀
𝑒𝑎=𝑋𝑒𝑎+𝑀𝑒𝑎 (6)
where𝑋𝑒𝑎represents the spatio-temporal feature of road segment
𝑒𝑎and𝑀𝑒𝑎denotes the motion vector of 𝑒𝑎. This approach further
strengthens the relationship between 𝑒𝑎and𝑒𝑖through the motion
vector. Then we use a multi-head self-attention mechanism to focus
on different motions and the impact of different adjacent road
segments on the current road segment from multiple perspectives.
Specifically, we define the feature correlation between the adja-
cent road segment 𝑒𝑎that has been fused with the motion vector
and the current road segment 𝑒𝑖under theℎ-head attention as:
𝛼(ℎ)(𝑒𝑖,𝑒𝑎)=𝑒𝑥𝑝(𝜓(𝑋𝑒𝑖,𝑋𝑀𝑒𝑎))
Í|𝐴𝑑𝑗𝑖|
𝑎′=1𝑒𝑥𝑝(𝜓(𝑋𝑒𝑖,𝑋𝑀𝑒𝑎′))
𝜓(𝑋𝑒𝑖,𝑋𝑀
𝑒𝑎)=<𝑊(ℎ)
𝑄𝑋𝑒𝑖,𝑊(ℎ)
𝐾𝑋𝑀𝑒𝑎>
√
𝑑′(7)
where𝑊(ℎ)
𝑄,𝑊(ℎ)
𝐾∈R𝑑′×𝑑are learnable parameters, <,>is the
inner product function, and 𝑑′=𝑑/𝐻. Next, we generate the aggre-
gated representation of road segment 𝑒𝑖by correlation 𝛼(ℎ)(𝑒𝑖,𝑒𝑎):
˜𝑋𝑒𝑖=˜𝑋(1)
𝑒𝑖⊕˜𝑋(2)
𝑒𝑖⊕···⊕ ˜𝑋(𝐻)
𝑒𝑖
˜𝑋ℎ
𝑒𝑖=|𝐴𝑑𝑗𝑖|∑︁
𝑎′=1𝛼(ℎ)(𝑒𝑖,𝑒𝑎′)(𝑊(ℎ)
𝑉𝑋𝑀
𝑒𝑎′)(8)
where𝑊(ℎ)
𝑉∈R𝑑′×𝑑is learnable parameters, and ⊕is the con-
catenation operator, and 𝐻is the number of total heads. As such,
we can integrate the structural information of the road network
into the representation of road segments, and in the same way, we
can obtain the representation of each road segment in the road
network.
3.2.3 OD-Attention. As mentioned in Challenge 3, different vehi-
cles may produce different driving trajectories even at the same
time and with the same OD. Thus, we develop OD-Attention to
extract the driving habits of drivers.
Assuming that the vehicle 𝑝is currently located in road segment
𝑒𝑎, where𝑒𝑎represents a road segment in a trajectory between
passing record 𝑟𝑠and𝑟𝑒. The first step is to retrieval the historical
OD-pairs before time 𝑟𝑒.𝑡, and obtain the representation for each
OD-pair. Specifically, for OD-pair 𝑂𝐷𝑎𝑏∈R𝑂𝐷, its𝑑𝑂𝐷dimension
representation ˜𝑋𝑂𝐷𝑎𝑏∈R𝑑𝑂𝐷can be calculated as:
˜𝑋𝑂𝐷𝑎𝑏=M( ˜𝑋𝑂𝐷𝑎𝑏.𝑒𝑜⊕˜𝑋𝑂𝐷𝑎𝑏.𝑒𝑑) (9)
5983KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
whereMdenote two-layer fully-connected neural network func-
tion. As such, we further obtain the representation sequence of vehi-
cle OD-pair trajectory J𝑂𝐷𝑝as˜𝑋𝑂𝐷J=[˜𝑋𝑂𝐷J
1,˜𝑋𝑂𝐷J
2,···,˜𝑋𝑂𝐷J
𝜂],
where𝜂is the number of total historical OD-pairs. The next step is
to extract the driving habits among historical OD-pairs. Intuitively,
we believe that driving habits can be seen as similar choices that
drivers habitually make under similar road conditions. Therefore,
we calculate the similarity between the OD-pair representation
˜𝑋𝑂𝐷𝑎𝑏of the current OD-pair 𝑂𝐷𝑎𝑏and the OD-pair representa-
tions ˜𝑋𝑂𝐷J
𝑗of the historical OD-pairs, and then selectively incorpo-
rate the historical OD-pair features ˜𝑋𝑂𝐷J
𝑗into the current OD-pair
features ˜𝑋𝑂𝐷𝑎𝑏by multi-head attention mechanism:
˜𝑋A
𝑂𝐷𝑎𝑏=˜𝑋A(1)
𝑂𝐷𝑎𝑏⊕˜𝑋A(2)
𝑂𝐷𝑎𝑏⊕···⊕ ˜𝑋A(𝐻)
𝑂𝐷𝑎𝑏
˜𝑋A(ℎ)
𝑂𝐷𝑎𝑏=𝑛∑︁
𝑗′=1𝛼(ℎ)(𝑂𝐷𝑎𝑏,𝑂𝐷J
𝑗)(𝑊(ℎ)
𝑉′˜𝑋𝑂𝐷J
𝑗′)(10)
𝛼(ℎ)(𝑂𝐷𝑎𝑏,𝑂𝐷J
𝑗)=𝑒𝑥𝑝(𝜓(˜𝑋𝑂𝐷𝑎𝑏,˜𝑋𝑂𝐷J
𝑗))
Í𝑛
𝑗′=1𝑒𝑥𝑝(𝜓(˜𝑋𝑂𝐷R
𝑖,˜𝑋𝑂𝐷J
𝑗′))
𝜓(˜𝑋𝑂𝐷𝑎𝑏,˜𝑋𝑂𝐷J
𝑗)=<𝑊(ℎ)
𝑄′˜𝑋𝑂𝐷𝑎𝑏,𝑊(ℎ)
𝐾′˜𝑋𝑂𝐷J
𝑗>
√︃
𝑑′
𝑂𝐷(11)
where𝑊(ℎ)
𝑄′,𝑊(ℎ)
𝐾′,𝑊(ℎ)
𝑉′∈R𝑑′
𝑂𝐷×𝑑𝑂𝐷are learnable parameters,
and𝑑′
𝑂𝐷=𝑑𝑂𝐷/𝐻. To preserve the representation of the original
OD-pair, we further use standard residual connections:
𝑋𝑂𝐷𝑎𝑏=𝜎(˜𝑋A
𝑂𝐷𝑎𝑏+𝑊𝑟˜𝑋𝑂𝐷𝑎𝑏) (12)
where𝑊𝑟∈R𝑑𝑂𝐷×𝑑𝑂𝐷is learnable projection matrix, and 𝜎is
non-linear activation function e.g. 𝑅𝑒𝐿𝑈 function.
With the OD-attention layer, the representation of OD-pair
˜𝑋𝑂𝐷𝑎𝑏can be updated into a more informative representation
𝑋𝑂𝐷𝑎𝑏, incorporating the driving habit features from historical
passing records.
3.2.4 Transfer Probabilistic Generator. Assuming that the vehicle
is currently located in road segment 𝑒𝑎, it can then travel to any
adjacent road segment 𝑒𝑏∈𝐴𝑑𝑗𝑎. Thus, we obtain the transfer
probability of 𝑒𝑎→𝑒𝑏by:
𝑃𝑟(𝑒𝑏|𝑒𝑎,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)=𝑒𝑥𝑝(F(𝑋𝑂𝐷𝑎𝑏))
Í|𝐴𝑑𝑗𝑎|
𝑏′=1𝑒𝑥𝑝(F(𝑋𝑂𝐷𝑎𝑏′))(13)
whereFdenote two-layer fully-connected neural network function:
F(𝑋)=𝜎(𝑋𝑊 1+𝑏1)𝑊2 (14)
Here,𝑊1∈R𝑑𝑂𝐷×𝑑1and𝑊2∈R𝑑1×1are two learnable weights,
and𝑏1is the learnable bias.
3.3 Trajectory Generator
Given the vehicle trajectory J𝑝of vehicle𝑝, based on formula (4)
and the transfer probability output of each road segment 𝑒𝑎∈𝐸
transfer to its adjacency road segment 𝑒𝑏∈𝐴𝑑𝑗𝑎by the STPM. We
sequentially recover the trajectory between passing record 𝑟𝑖∈J𝑝and passing record 𝑟𝑖+1∈J𝑝. Inspired by [ 11], we simplify the
Formula 4 as:
R∗=arg max
∀R∈𝐺|R|Ö
𝑖=1𝑃𝑟(R.𝑒𝑖|R.𝑒𝑖−1,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H)
=arg min
∀R∈𝐺|R|∑︁
𝑖=1−𝑙𝑜𝑔(𝑃𝑟(R.𝑒𝑖|R.𝑒𝑖−1,𝑟𝑒.𝑒,𝑟𝑒.𝑡,𝑝,H))(15)
Based on that, we can form a road network graph with road seg-
ments as nodes and the connection relationships between road
segments as edges based on the connection relationship of road
segments in the road network 𝐺. We can regard the transfer proba-
bility as the edge weight between nodes, i.e. road segments, so we
can transform the problem into the shortest path finding problem
and use Dijkstra’s Algorithm to solve it.
3.4 Training
Finally, we elaborate on the end-to-end training process of STPM.
As mentioned in Section 1, we obtain snapshots of vehicles captured
by traffic cameras, extracting lane information and corresponding
turning details. Therefore, we construct training samples using
snapshots with explicit turning information (detailed construction
rules are introduced in Appendix A). Each training sample is repre-
sented as(𝑥𝑖,𝑦𝑖), where𝑥𝑖=<𝑒𝑎,𝑒𝑏,𝑟𝑒,𝑝,H𝑝>signifies that the
vehicle𝑝is currently on road segment 𝑒𝑎, and given its destination
at road segment 𝑟𝑒.𝑒and arrive time 𝑟𝑒.𝑡, the probability of heading
towards adjacent road segment 𝑒𝑏∈𝐴𝑑𝑗𝑎is𝑦. In a mini-batch of 𝑏
training samples, the training objective is to minimize the model’s
mean squared error using the following loss function:
𝐿(𝜃)=1
𝑏𝑏∑︁
𝑖=1(𝑦𝑖−𝑓(𝑥𝑖,𝜃))2+𝜆||𝜃||2(16)
Here,𝑓(𝑥,𝜃)represents the output of STPM, 𝜃denotes all train-
able parameters in the model, and 𝜆is a parameter controlling the
regularization strength. Our goal is to train the model and find the
optimal parameters 𝜃that minimize the loss function 𝐿(𝜃).
4 EXPERIMENTS
4.1 Experimental Settings
4.1.1 Datasets. Camera Record Data. To demonstrate the ro-
bustness of TrajRecovery, we use two different-scale real traffic
camera record datasets, which are collected from a region of a city
in China, which encompasses 975 traffic cameras, 5,597 road inter-
sections, and 13,579 road segments. Specifically, the larger dataset
is D5, which comprises over 20 million vehicle snapshots over 5
days. The smaller dataset is D1, which consists of 1.5 million vehicle
snapshots during the morning rush hour on the workday.
Ground Truth Data. To evaluate the recovery performance, we
require ground truth data. Corresponding to the time cover range of
D5 and D1, our volunteers collect 515 and 196 real GPS trajectories,
respectively, while driving on the roads. Then, we perform map-
matching [ 21] to align these GPS trajectories with the road network,
which helps evaluate trajectory recovery quality.
4.1.2 Baselines. We compare TrajRecovery with six baselines.
5984TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Overall Performance Comparison.
MethodD1 D5
LCSS EDR STLC DTW LCSS EDR STLC DTW
SP 0.4716±0.0031 30.5164±0.1059 0.6128±0.0058 1253±11 0.5831±0.0052 36.1543±0.1129 0.5043±0.0081 1863±30
VeTrac 0.3921±0.0108 18.9641±0.6018 0.7681±0.0130 896±38 0.5031±0.0126 23.5163±0.7158 0.5968±0.0146 1653±53
MMVC 0.3761±0.0124 16.5176±0.7426 0.8214±0.0105 841±40 0.4713±0.0135 19.9643±0.8201 0.6514±0.0117 1568±51
CFST 0.3495±0.0078 15.1536±0.5971 0.8537±0.0126 726±28 0.4482±0.0103 18.7694±0.6391 0.6786±0.0138 1496±42
CFST-C 0.3351±0.0071 14.9658±0.5820 0.8602±0.0107 719±29 0.4324±0.0098 18.0659±0.6018 0.6812±0.0118 1406±38
TrajRecovery-G 0.3356±0.0116 15.1443±0.6419 0.8616±0.0130 728±36 0.4218±0.0120 17.5011±0.6310 0.6943±0.0107 1367±28
TrajRecovery 0.3179±0.0082 13.8169±0.5981 0.8826±0.0118 653±31 0.4096±0.0108 16.6115±0.6189 0.7281±0.0116 1235±47
Gain 5.1% 7.7% 2.6% 9.2% 5.3% 8.1% 6.9% 12.2%
•Shortest Path (SP) [ 39]. Initially, we use the vehicle reidenti-
fication in the Preprocessor module to cluster vehicles. Then,
we employ this method to generate the shortest path.
•VeTrac [ 28]. It utilizes the graph convolutional process to
maintain identity consistency across different camera obser-
vations. Due to memory limitations, following the previous
study [34], we employ the 𝑘-means clustering.
•MMVC [ 16]. It jointly optimizes vehicle reidentification and
trajectory recovery. It clusters vehicles using visual features
and employs the Hidden Markov Model (HMM) for trajectory
recovery based on vehicle clusters.
•CFST [ 34]. Similar to MMVC [ 16], it clusters vehicles us-
ing visual features and employs GPS trajectories to train a
uniform Dirichlet prior model and a temporal model. Specif-
ically, we utilize 84,153 trajectories recorded over 24 hours
on a day not included in the dataset for model training.
•CFST-C [ 35]. This is an optimized version of CFST [ 34], divid-
ing the entire map area into multiple blocks and recovering
and concatenating trajectories blockwise.
•TrajRecovery-G. We replace the Dijkstra algorithm used in
the trajectory generator with a greedy algorithm. It selects
the road segment with the highest probability at each step
during the trajectory generation process.
4.1.3 Evaluation Metrics. Referring to previous studies, we uti-
lize four metrics to evaluate the precision of trajectory recovery,
i.e., Longest Common SubSequence (LCSS), Edit Distance on Real se-
quence (EDR), Spatio-Temporal Linear Combine distance (STLC) [ 27],
and Dynamic Time Warping (DTW) [ 25]. The larger the STLC
value, the better the quality of trajectory recovery; the smaller the
LCSS/EDR/DTW value, the better the quality of trajectory recovery.
4.1.4 Parameter Settings. All experiments were conducted on a
Linux server equipped with PyTorch 1.10.1, 2 NVIDIA TESLA T4
GPUs, and 64GB memory. These programs were implemented using
Python. In the Preprocessor module, we set the similarity threshold
𝑇ℎ=0.8, the context window 𝑘=2,𝑞=50, and𝑠=20. In the
Feature Extractor module, we configure the dimension 𝑑of each
dense representation to be 128. In the OD-Attention module, we set
the dimension 𝑑ODto 64. In multi-head attention, we employ 4 heads
to generate attention from different perspectives. We use the Adam
optimizer [ 14] for training, where the initial learning rate is 0.0001
and the mini-batch size is 128. The size of training/validation/testingis set to 0.8/0.1/0.1 for STPM, and select the model that performs best
on the testing set for trajectory recovery. The Xavier initializer [ 7]
is employed to initialize the model parameters, and the associated
parameters adhere to the configurations outlined in the original
paper. We present the replication results for all baselines, using
hyperparameters consistent with those specified in the original
paper.
4.2 Overall Performance Comparison
Table 1 shows the overall performance of all methods on two
datasets, where the best result in each metric (column) is shown in
bold. Overall, we yield the following observations.
The first observation is that our model consistently outperforms
baselines on all four metrics for both datasets, showing great supe-
riority, e.g., up to 12% improvement. Second, SP performs poorly,
as individuals may consider various factors when selecting routes
while driving. Even in similar road conditions, drivers may choose
longer routes due to driving habits. In contrast, our methods com-
prehensively consider factors such as road conditions and driving
habits, making them more aligned with real-world scenarios. An-
other interesting observation is both VeTrac and MMVC require
a high density of traffic cameras to provide sufficient information.
This leads to their suboptimal performance in suburban areas with
lower traffic camera density. Although CFST and CFST-C perform
better than VeTrac and MMVC by utilizing GPS trajectories from
other vehicles in their trajectory recovery, biases in path prefer-
ences may occur, causing the model to fail on road segments not
covered by GPS trajectories. In contrast, our methods employ a uni-
versal method for trajectory recovery based on road conditions and
individual driving habits, even in areas without camera coverage.
We also observe that in the D1 dataset, CFST-C is comparable to the
TrajRecovery-G version. This is because, during the morning rush
hour, people’s travel patterns tend to be relatively fixed, resulting
in concentrated driving trajectories. The final observation is that,
when replacing the Dijkstra algorithm with the greedy algorithm
in TrajRecovery, its performance declined. This is mainly because
consistently moving to the road segment with the highest probabil-
ity can result in local optima rather than global optima in certain
situations. It may also lead to the occurrence of trajectory "loops,"
where the road segment with the highest probability forms a loop,
preventing the trajectory from reaching the destination.
5985KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
Table 2: Impact of each Component (OD: OD-Attention, M:
Motion-GAT, HPRR: Historical Passing Record Retriever).
Ablation LCSS EDR STLC DTW
-OD & M 0.4367±0.0128 17.9584±0.6241 0.6894±0.0125 1437±56
-M 0.4208±0.0110 17.9123±0.6110 0.6953±0.0120 1342±53
-HPRR 0.4195±0.0107 17.7614±0.5917 0.6927±0.0102 1415±43
-OD 0.4164±0.0113 17.6615±0.6071 0.6913±0.0127 1361±50
All 0.4096±0.0108 16.6115±0.6189 0.7281±0.0116 1235±47
4.3 Ablation Study
We study each module to influence the final results. To achieve this,
we systematically performed ablation experiments by sequentially
removing each module. The results are presented in Table 2, and
for a more comprehensive conclusion, we conducted the analysis
using the information-rich D5 dataset. It is evident from the re-
sults that the performance of the complete version of the system is
optimal, indicating that each module contributes to the overall sys-
tem performance. The most notable performance decline occurred
when the Motion-GAT module was ablated. This is attributed to the
significance of factors such as the developmental level and traffic
conditions of road segments, crucial elements influencing a dri-
ver’s decision to choose a particular route. Additionally, the traffic
conditions of surrounding road segments also impact the driver’s
decision on whether to select a particular route.
4.4 Robustness Study
We conducted experiments to assess the robustness of TrajRecovery
in various scenarios. As mentioned in the first chapter, traffic cam-
eras may occasionally malfunction, resulting in instances where
vehicles passing through are not captured. Specifically, we investi-
gated the system’s recovery performance under different percent-
ages of missing vehicle records. For a particular vehicle’s complete
record, we retained the first and last entries, randomly removing
varying percentages of intermediate entries from the vehicle record
database. The results are presented in Fig. 4, indicating that as the
percentage of missing entries increases, trajectory recovery errors
gradually escalate. However, our model consistently outperforms
the comparison methods in terms of performance.
4.5 Parameter Study
We investigate the sensitivity of three parameters: vehicle snapshot
similarity threshold 𝑇ℎ, the road segment representation dimension
𝑑, and the head number 𝐻. The results are shown in Appendix B.
4.6 Case Study
We selected a representative trajectory from the test trajectories
to illustrate the effectiveness of TrajRecovery. As shown in Fig. 5,
compared to CFST-C, the results of TraRecovery are closer to the
ground truth. Specifically, we first identified six passing records of
the vehicle through vehicle re-identification. Subsequently, using
the Historical Passing Record Retriever, we retrieved the histor-
ical passing records of the vehicle. The retrievals indicated that
the vehicle passed through the camera at the red coordinate point
TrajRecovery CFST -C MMVC
 VeTrac
 SPFigure 4: The influence of the missing rate.
Figure 5: Visualization of trajectory recovery by TrajRecov-
ery and CFST-C.
under similar contextual conditions multiple times. This prelimi-
nary increase in the sampling rate of passing record trajectories
before trajectory recovery helps reduce uncertainty. Moreover, at
the fifth passing record, the vehicle chose to turn left instead of
going straight, which differs from the choice of the majority. This
decision was influenced by the driver’s historical OD-pair pref-
erences, indicating a preference for roads with better conditions
rather than the closest ones. However, CFST-C relies heavily on a
large volume of GPS trajectories for trajectory recovery, making it
prone to failure when encountering intersections not encountered
during the training phase. In contrast, TrajRecovery, even in road
segments not covered by traffic cameras, can make road selections
based on road conditions and driver habits.
5986TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD ’24, August 25–29, 2024, Barcelona, Spain
5 PRACTICAL DEPLOYMENT
Based on the trajectory recovery results provided by TrajRecovery,
we have developed a web-based system at Huawei Cloud. This
system leverages recovered trajectories for various downstream
applications. For instance, Fig. 6 showcases the traffic flow monitor-
ing interface, allowing administrators to interact with the system in
the following scenarios. (i) Traffic Volume Visualization. Utilizing a
heat map displayed in the upper left corner of Fig. 6, we illustrate
the traffic volume at each intersection. Color variations depict the
congestion or smooth flow of traffic conditions. (ii) Crowded In-
tersection Ranking. As depicted in the lower right part of Fig. 6, it
provides periodic traffic statistics, presenting the number of vehi-
cles during different periods and at specific intersections (e.g., Road
22). (iii) Traffic Volume Statistics. By selecting a specific region,
a traffic volume trend-line for the past 24 hours is plotted in the
lower right part of Fig. 6. Furthermore, administrators can easily
obtain precise volume information by hovering the mouse over a
specific time interval. Additionally, we have successfully deployed
TrajRecovery in a city in China for traffic signal control, bringing in
a noteworthy 12.5% increase in the average speed of intersections.
6 RELATED WORK
6.1 Trajectory Recovery
Depending on the data source of sparse vehicle trajectories, it can
be divided into two categories.
The first category primarily focuses on sparse GPS trajecto-
ries [15, 24, 30, 33], which are not the scope of this paper.
The second category takes sparse snapshots captured by traffic
cameras as input, which often requires vehicle re-identification
processing, facing missed identifications, misidentifications, and
aggravated sparsity. Cao et al. [ 1] utilized high-sampling-rate GPS
trajectories from commercial vehicles to assist in recovering low-
sampling-rate trajectories. Tong et al. [ 28] fused vehicle movement
correlations and visual features to reduce uncertainties in identify-
ing vehicles, requiring a higher camera distribution rate to enhance
trajectory recovery accuracy. Qi et al. [ 23] performed trajectory
recovery on automatic license plate recognition data obtained from
cameras. Lin et al. [ 16] recognized the correlation between vehicle
re-identification and trajectory recovery, proposing a joint optimiza-
tion framework for both tasks. Yu et al. [ 34] optimized previous
efforts by combining vehicle visual features and dynamic spatio-
temporal constraint features for vehicle re-identification. Recently,
Yu et al. [ 35] divided the entire city into multiple regions, performed
trajectory recovery within each region, and then concatenated the
complete trajectories. However, these approaches require a sub-
stantial amount of GPS trajectories for training trajectory recovery
models and may not perform well in areas not covered by cameras.
6.2 Vehicle Re-identification
Vehicle re-identification involves clustering the same vehicle from
a large number of snapshots captured by cameras, serving as an
essential foundation for trajectory recovery. Numerous CNN-based
methods have been proposed. Liu et al. [ 18] introduced a coarse-
to-fine vehicle retrieval framework that utilizes multi-modal fea-
tures, such as visual features, license plate information, camera
1
2
3
4
5
62
3456
1
Rank 1: Road 22 Rank 2: Road 11 Rank 3: Road 715 Rank 4: Road 6 Rank 5: Road 9
Traffic Volume HeatmapFigure 6: TrajRecovery deployment at HuaWei Cloud.
location, and other contextual details. Zhou et al. [ 42] employed
reverse learning and a viewpoint-aware attention model to acquire
viewpoint-aware representations for vehicle re-identification. [ 13]
proposed AAVER, a dual-path model with adaptive attention for
vehicle re-identification, which uses key part features of vehicles
to distinguish local features in different regions and concentrates
attention on the most informative key points to capture local dis-
criminative features. Shen et al. [ 26] introduced a Graph Interactive
Transformer framework that utilizes the interaction between graphs
and transformers to effectively coordinate local and global features.
Meng et al. [20] extracted features from different parts of vehicles
and used commonly visible attention to calculate the similarity
between different parts. He et al. [ 9] proposed a simple yet effective
method of part-regularized discriminative feature preservation, fo-
cusing attention on vehicle-specific features to enhance sensitivity
to subtle differences.
7 CONCLUSION
In this paper, we propose the TrajRecovery framework for re-
covering urban-scale vehicle trajectories using vehicle snapshots
captured by traffic cameras. TrajRecovery generalizes knowledge
learned from camera-covered road segments to uncovered road seg-
ments, and learns driver-specific driving habits based on historical
driving behavior, enabling vehicle-level personalized trajectory re-
covery. Additionally, we develop a web-based system TrajRecovery
at HuaWei Cloud, which leverages recovered trajectories for various
downstream applications. Experiments on two real-world datasets
validate TrajRecovery’s effectiveness and utility improvement on
the business scenarios.
8 ACKNOWLEDGMENTS
This work was supported in part by the NSFC under Grants No.
(62025206, U23A20296, and 62102351), the Zhejiang Province’s
"Lingyan" R&D Project under Grant No 2024C01259, the Ningbo
Science and Technology Special Innovation Projects under Grants
No. (2022Z095 and 2021Z019), and the Hangzhou Technology Bu-
reau AI Research Project under Grant No. 2022AIZD0116. Yunjun
Gao is the corresponding author of the work.
5987KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
REFERENCES
[1]Zijian Cao, Dong Zhao, Hanxing Song, Haitao Yuan, Qiyue Wang, Huadong Ma,
Jianjun Tong, and Chang Tan. 2023. F 3 VeTrac: Enabling Fine-grained, Fully-
road-covered, and Fully-individual penetrative Vehicle Trajectory Recovery. IEEE
Transactions on Mobile Computing (2023).
[2]Yuqi Chen, Hanyuan Zhang, Weiwei Sun, and Baihua Zheng. 2023. Rntrajrec:
Road network enhanced trajectory recovery with spatial-temporal transformer.
In2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE,
829–842.
[3]Ziquan Fang, Dongen Wu, Lu Pan, et al .2022. When transfer learning meets
cross-city urban flow prediction: spatio-temporal adaptation matters. IJCAI’22
(2022), 2030–2036.
[4]Kun Fu, Fanlin Meng, Jieping Ye, and Zheng Wang. 2020. CompactETA: A
fast inference system for travel time prediction. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 3337–
3345.
[5]Qiang Gao, Fan Zhou, Kunpeng Zhang, Goce Trajcevski, Xucheng Luo, and Fengli
Zhang. 2017. Identifying Human Mobility via Trajectory Embeddings.. In IJCAI,
Vol. 17. 1689–1695.
[6]Fosca Giannotti, Mirco Nanni, Dino Pedreschi, Fabio Pinelli, Chiara Renso, Salva-
tore Rinzivillo, and Roberto Trasarti. 2011. Unveiling the complexity of human
mobility by querying and mining massive trajectory data. The VLDB Journal 20
(2011), 695–719.
[7]Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics. JMLR Workshop and Conference
Proceedings, 249–256.
[8]Haiyun Guo, Chaoyang Zhao, Zhiwei Liu, Jinqiao Wang, and Hanqing Lu. 2018.
Learning coarse-to-fine structured feature embedding for vehicle re-identification.
InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
[9]Bing He, Jia Li, Yifan Zhao, and Yonghong Tian. 2019. Part-regularized near-
duplicate vehicle re-identification. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 3997–4005.
[10] Mingdi Hu, Long Bai, Jiulun Fan, Sirui Zhao, and Enhong Chen. 2023. Vehicle
color recognition based on smooth modulation neural network with multi-scale
feature fusion. Frontiers of Computer Science 17, 3 (2023), 173321.
[11] Jayant Jain, Vrittika Bagadia, Sahil Manchanda, and Sayan Ranu. 2021. NeuroMLR:
Robust & reliable route recommendation on road networks. Advances in Neural
Information Processing Systems 34 (2021), 22070–22082.
[12] Sultan Daud Khan and Habib Ullah. 2019. A survey of advances in vision-based
vehicle re-identification. Computer Vision and Image Understanding 182 (2019),
50–63.
[13] Pirazh Khorramshahi, Amit Kumar, Neehar Peri, Sai Saketh Rambhatla, Jun-
Cheng Chen, and Rama Chellappa. 2019. A dual-path model with adaptive
attention for vehicle re-identification. In Proceedings of the IEEE/CVF international
conference on computer vision. 6132–6141.
[14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[15] Dedong Li, Ziyue Li, Zhishuai Li, Lei Bai, Qingyuan Gong, Lijun Sun, Wolfgang
Ketter, and Rui Zhao. 2023. A Critical Perceptual Pre-trained Model for Complex
Trajectory Recovery. In Proceedings of the 2nd ACM SIGSPATIAL International
Workshop on Searching and Mining Large Collections of Geospatial Data. 1–8.
[16] Zongyu Lin, Guozhen Zhang, Zhiqun He, Jie Feng, Wei Wu, and Yong Li. 2021.
Vehicle Trajectory Recovery on Road Network Based on Traffic Camera Video
Data. In Proceedings of the 29th International Conference on Advances in Geographic
Information Systems. 389–398.
[17] Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. 2016. Large-scale vehicle re-
identification in urban surveillance videos. In 2016 IEEE international conference
on multimedia and expo (ICME). IEEE, 1–6.
[18] Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. 2016. A deep learning-based
approach to progressive vehicle re-identification for urban surveillance. In Com-
puter Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part II 14. Springer, 869–884.
[19] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. 2014.
Traffic flow prediction with big data: A deep learning approach. IEEE Transactions
on Intelligent Transportation Systems 16, 2 (2014), 865–873.
[20] Dechao Meng, Liang Li, Xuejing Liu, Yadong Li, Shijie Yang, Zheng-Jun Zha,
Xingyu Gao, Shuhui Wang, and Qingming Huang. 2020. Parsing-based view-
aware embedding network for vehicle re-identification. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 7103–7112.
[21] Paul Newson and John Krumm. 2009. Hidden Markov map matching through
noise and sparseness. In Proceedings of the 17th ACM SIGSPATIAL international
conference on advances in geographic information systems. 336–343.
[22] Xiao Pan, Lei Wu, Fenjie Long, and Ang Ma. 2022. Exploiting user behavior
learning for personalized trajectory recommendations. Frontiers of Computer
Science 16 (2022), 1–12.[23] Xinyi Qi, Yanjie Ji, Wenhao Li, and Shuichao Zhang. 2021. Vehicle trajectory
reconstruction on urban traffic network using automatic license plate recognition
data. IEEE Access 9 (2021), 49110–49120.
[24] Huimin Ren, Sijie Ruan, Yanhua Li, Jie Bao, Chuishi Meng, Ruiyuan Li, and Yu
Zheng. 2021. Mtrajrec: Map-constrained trajectory recovery via seq2seq multi-
task learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1410–1419.
[25] Stan Salvador and Philip Chan. 2007. Toward accurate dynamic time warping in
linear time and space. Intelligent Data Analysis 11, 5 (2007), 561–580.
[26] Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huanqiang Zeng. 2023. Git:
Graph interactive transformer for vehicle re-identification. IEEE Transactions on
Image Processing 32 (2023), 1039–1051.
[27] Han Su, Shuncheng Liu, Bolong Zheng, Xiaofang Zhou, and Kai Zheng. 2020. A
survey of trajectory distance measures and performance evaluation. The VLDB
Journal 29 (2020), 3–32.
[28] Panrong Tong, Mingqian Li, Mo Li, Jianqiang Huang, and Xiansheng Hua. 2021.
Large-scale vehicle trajectory reconstruction with camera sensing network. In
Proceedings of the 27th Annual International Conference on Mobile Computing and
Networking. 188–200.
[29] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2023. YOLOv7:
Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. 7464–7475.
[30] Jingyuan Wang, Ning Wu, Xinxi Lu, Wayne Xin Zhao, and Kai Feng. 2019. Deep
trajectory recovery with fine-grained calibration using kalman filter. IEEE Trans-
actions on Knowledge and Data Engineering 33, 3 (2019), 921–934.
[31] Jingyuan Wang, Ning Wu, Wayne Xin Zhao, Fanzhang Peng, and Xin Lin. 2019.
Empowering A* search algorithms with neural networks for personalized route
recommendation. In Proceedings of the 25th ACM SIGKDD international conference
on knowledge discovery & data mining. 539–547.
[32] Hao Wu, Jiangyun Mao, Weiwei Sun, Baihua Zheng, Hanyuan Zhang, Ziyang
Chen, and Wei Wang. 2016. Probabilistic robust route recovery with spatio-
temporal dynamics. In Proceedings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining. 1915–1924.
[33] Tong Xia, Yunhan Qi, Jie Feng, Fengli Xu, Funing Sun, Diansheng Guo, and
Yong Li. 2021. Attnmove: History enhanced trajectory recovery via attentional
network. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
4494–4502.
[34] Fudan Yu, Wenxuan Ao, Huan Yan, Guozhen Zhang, Wei Wu, and Yong Li.
2022. Spatio-Temporal Vehicle Trajectory Recovery on Road Network Based on
Traffic Camera Video Data. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 4413–4421.
[35] Fudan Yu, Huan Yan, Rui Chen, Guozhen Zhang, Yu Liu, Meng Chen, and Yong
Li. 2023. City-scale vehicle trajectory data from traffic camera videos. Scientific
data 10, 1 (2023), 711.
[36] Hao Yu, Xi Guo, Xiao Luo, Weihao Bian, and Taohong Zhang. 2023. Construct
Trip Graphs by Using Taxi Trajectory Data. Data Science and Engineering 8, 1
(2023), 1–22.
[37] Haitao Yuan, Guoliang Li, Zhifeng Bao, and Ling Feng. 2020. Effective travel
time estimation: When historical trajectories over road networks matter. In
Proceedings of the 2020 acm sigmod international conference on management of
data. 2135–2149.
[38] Jing Yuan, Yu Zheng, Chengyang Zhang, Xing Xie, and Guang-Zhong Sun. 2010.
An interactive-voting based map matching algorithm. In 2010 Eleventh interna-
tional conference on mobile data management. IEEE, 43–52.
[39] F Benjamin Zhan and Charles E Noon. 1998. Shortest path algorithms: an evalu-
ation using real road networks. Transportation science 32, 1 (1998), 65–73.
[40] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: A
graph multi-attention network for traffic prediction. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 34. 1234–1241.
[41] Yu Zheng, Licia Capra, Ouri Wolfson, and Hai Yang. 2014. Urban computing:
concepts, methodologies, and applications. ACM Transactions on Intelligent
Systems and Technology (TIST) 5, 3 (2014), 1–55.
[42] Yi Zhou and Ling Shao. 2018. Aware attentive multi-view inference for vehicle
re-identification. In Proceedings of the IEEE conference on computer vision and
pattern recognition. 6489–6498.
[43] Chenyi Zhuang, Nicholas Jing Yuan, Ruihua Song, Xing Xie, and Qiang Ma. 2017.
Understanding People Lifestyles: Construction of Urban Movement Knowledge
Graph from GPS Trajectory.. In Ijcai. 3616–3623.
5988TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD ’24, August 25–29, 2024, Barcelona, Spain
A TRAINING DATA PREPARATION
Algorithm 1: Training Sample Generation
Input: Shortest distance matrix 𝐷𝑒, current passing record
𝑟𝑠, next passing record 𝑟𝑒, possible adjacent road
sections𝐴𝑑𝑗𝑇
𝑟𝑠.𝑒, vehicle𝑝, the historical passing
recordsH𝑝.
Output: training samples 𝑇𝑟.
1Init𝑝𝑜𝑠𝑑𝑖𝑠←{} ;𝑛𝑒𝑔𝑑𝑖𝑠←{} ;𝜏←0;
2for𝑒𝑏∈𝐴𝑑𝑗𝑇
𝑟𝑠.𝑒do
3𝑑𝑖𝑠←𝐷𝑒[𝑟𝑠.𝑒][𝑟𝑒.𝑒]−𝐷𝑒[𝑒𝑏][𝑟𝑒.𝑒];
4 if𝑑𝑖𝑠>0: // far away from 𝑟𝑒
5𝑝𝑜𝑠𝑑𝑖𝑠.push(𝑑𝑖𝑠);
6 else: // close to𝑟𝑒
7𝑛𝑒𝑔𝑑𝑖𝑠.push(|𝑑𝑖𝑠|);
8end
9if|𝑝𝑜𝑠𝑑𝑖𝑠|>0and|𝑛𝑒𝑔𝑑𝑖𝑠|<0:
10𝜏←0.2;
11elif|𝑝𝑜𝑠𝑑𝑖𝑠|>0:
12𝜏←1;
13for𝑒𝑏∈𝐴𝑑𝑗𝑇
𝑟𝑠.𝑒do
14𝑑𝑖𝑠←𝐷𝑒[𝑟𝑠.𝑒][𝑟𝑒.𝑒]−𝐷𝑒[𝑒𝑏][𝑟𝑒.𝑒];
15 if𝑑𝑖𝑠>0:
16𝑇𝑟.push( <𝑟𝑠.𝑒,𝑒𝑏,𝑟𝑒,𝑝,H𝑝>,𝑒𝑥𝑝(−𝑑𝑖𝑠)×𝜏Í
𝑑′∈𝑝𝑜𝑠𝑑𝑖𝑠𝑒𝑥𝑝(𝑑′));
17 else:
18𝑇𝑟.push( <𝑟𝑠.𝑒,𝑒𝑏,𝑟𝑒,𝑝,H𝑝>,𝑒𝑥𝑝(−𝑑𝑖𝑠)×(1−𝜏)Í
𝑑′∈𝑛𝑒𝑔𝑑𝑖𝑠𝑒𝑥𝑝(𝑑′));
19end
20return𝑇𝑟;
First, we directly use the popular YOLOv7 [ 29] to locate the
coordinates of vehicles in the images captured by traffic cameras.
Then, we combine such information with the lane boundaries and
turn details (such as straight, left, right, U-turn, etc) defined by the
installer during camera installation. Based on that, we can identify
the lane of the moving vehicles from camera snapshots.
Next, given a road network 𝐺, we employ the Dijkstra algo-
rithm to obtain the shortest distances between every pair of road
segments, forming the matrix 𝐷𝑒, where𝐷𝑒[𝑒𝑎][𝑒𝑏]denotes the
shortest distance from road segment 𝑒𝑎to road segment 𝑒𝑏. Note
that, this procedure is pre-processing and does not affect model
design. Based on the turning information of the vehicle’s located
lane and the topological structure of 𝐺, we obtain the set of possible
adjacent road segments that the vehicle may head towards, denoted
asAdj𝑇
𝑟𝑠.𝑒.
Based on the above pre-processing, Algorithm 1 shows the train-
ing data/sample generation. The basic idea is that we categorize
potential adjacent road segments into two classes based on the
distance, i.e., either shorten or lengthen the distance to the desti-
nation. Specifically, it first calculates the total distance away from
(or towards) the destination for each class (lines 2–7). Note that we
cannot simply assume that a vehicle will necessarily head towards
an adjacent road segment close to the destination, even if the prob-
ability is relatively high. Thus, if there are adjacent road segmentsboth close to and far from the destination, we set a hyperparameter
𝜏to 0.2 (lines 9–10). This value indicates that there is a certain
likelihood that the vehicle will head towards road segments that
are far from the destination. This approach enables generating di-
verse samples and preventing the model from consistently choosing
adjacent road segments close to the destination, neglecting other
potential factors such as road development levels. Finally, we use
the softmax function to calculate the transition probability labels
for each sample (lines 13–19).
B PARAMETER SENSITIVITY
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
𝒅-value  16 32 64 128 256
𝒅-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 7: Different Similarity Threshold 𝑇ℎ.
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
𝒅-value  16 32 64 128 256
𝒅-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 8: Different Dimension 𝑑Value.
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
𝒅-value  16 32 64 128 256
𝒅-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 9: Different Number of Head 𝐻.
We investigate the sensitivity of three key parameters: vehicle
snapshot similarity threshold 𝑇ℎ, the road segment representation
dimension𝑑, and the head number 𝐻, which determine the rep-
resentation ability of the model. (i) The results as shown in Fig. 7
reveal that low 𝑇ℎvalue tends to introduce more noise from other
vehicles, leading to poor performance, where as a high 𝑇ℎvalue
may filter out some valid passing records. (ii) Additionally, Fig. 8
presents the performance on different values of dimension 𝑑. We
observe a gradual improvement in performance as the dimension
5989KDD ’24, August 25–29, 2024, Barcelona, Spain Dongen Wu, et al.
value increases. Performance stabilizes when the dimension ex-
ceeds 128. This is because when the value of 𝑑is too small, the
resulting feature vectors fail to adequately capture the informa-
tion of road segments. Conversely, an excessively large dimension
value introduces redundancy. Therefore, we set 𝑑to 128 to strike
a balance between expressive feature representation and avoiding
unnecessary redundancy. (iii) Finally, we conducted experiments
using different numbers of heads 𝐻, and the results are presented in
Fig. 9. We observed that when 𝐻is greater than 1, the performance
is relatively stable across various values. Therefore, we chose a
value of 4 as it offered a relatively better performance.
5990