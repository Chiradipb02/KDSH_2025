Integrating System State into Spatio Temporal Graph Neural
Network for Microservice Workload Prediction
Yang Luo∗
floating-
dream@sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, ChinaMohan Gao∗
gao_mohan@sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, ChinaZhemeng Yu∗
fish_meng@sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, ChinaHaoyuan Ge
gehaoyuan.ghy@antgroup.com
Ant Group
Hangzhou, China
Xiaofeng Gao∗†
gao-xf@cs.sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, ChinaTengwei Cai
tengwei.ctw@antgroup.com
Ant Group
Hangzhou, ChinaGuihai Chen∗
gchen@cs.sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, China
Abstract
Microservice architecture has become a driving force in enhancing
the modularity and scalability of web applications, as evidenced by
the Alipay platform’s operational success. However, a prevalent is-
sue within such infrastructures is the suboptimal utilization of CPU
resources due to inflexible resource allocation policies. This ineffi-
ciency necessitates the development of dynamic, accurate workload
prediction methods to improve resource allocation. In response to
this challenge, we present STAMP , aSpatio Temporal Gr aph Neural
Network for Microservice Workload Prediction. STAMP is designed
to comprehensively address the multifaceted interdependencies be-
tween microservices, the temporal variability of workloads, and the
critical role of system state in resource utilization. Through a graph-
based representation, STAMP effectively maps the intricate network
of microservice interactions. It employs time series analysis to
capture the dynamic nature of workload changes and integrates
system state insights to enhance prediction accuracy. Our empirical
analysis, using three distinct real-world datasets, establishes that
STAMP exceeds baselines by achieving an average boost of 5.72% in
prediction precision, as measured by RMSE. Upon deployment in
Alipay’s microservice environment, STAMP achieves a 33.10% reduc-
tion in resource consumption, significantly outperforming existing
online methods. This research solidifies STAMP as a validated frame-
work, offering meaningful contributions to the field of resource
management in microservice architecture-based applications.
∗Yang Luo, Mohan Gao, Zhemeng Yu, Xiaofeng Gao and Guihai Chen are with the MoE
Key Lab of Artificial Intelligence, Department of Computer Science and Engineering,
Shanghai Jiao Tong University.
†Xiaofeng Gao is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671508CCS Concepts
•Networks→Cloud computing; •Applied computing →
Forecasting.
Keywords
Microservice; Workload Prediction; Spatio Temporal GNN
ACM Reference Format:
Yang Luo, Mohan Gao, Zhemeng Yu, Haoyuan Ge, Xiaofeng Gao, Tengwei
Cai, and Guihai Chen. 2024. Integrating System State into Spatio Temporal
Graph Neural Network for Microservice Workload Prediction. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671508
1 Introduction
Large-scale web applications are shifting towards microservice
architectures, valued for their modularity, scalability, and adapt-
ability [ 8,9]. This move is reshaping the digital service landscape,
exemplified by industry leaders like Google [ 12] and Alibaba [ 24].
Alipay, the world’s leading digital payment application developed
by AntGroup, stands as a testament to this trend, leveraging the mi-
croservice architecture [ 33] to cater to its extensive global user base
of over a billion people. Despite the size of their user base, these
complex, distributed systems often contend with resource under-
utilization. For instance, Google’s Cluster Trace [ 11] indicates less
than 50% average CPU utilization, and Alipay’s average CPU utiliza-
tion rate hovers around 20%. This inefficiency stems from a static
approach to resource allocation that does not account for fluctuat-
ing workloads. To address this, microservice workload prediction
has become critical in guiding resource allocation, with the aim of
boosting efficiency and unlocking economic value [5, 7, 15, 41].
Drawing from our analysis of the workloads in Alipay and an
in-depth review of related literature, we point out three challenges
inherent in the prediction of workloads for microservices.
Challenge 1: Complex Interdependencies and Workload
Similarity. Microservices within distributed systems exhibit in-
terdependencies, often extending across clusters, which requires
efficient inter-cluster communication and leads to workload simi-
larities among functionally diverse services, as depicted in Fig. 1.
5521
KDD ’24, August 25–29, 2024, Barcelona, Spain Yang Luo et al.
InferenceAliPayApplicationMicroservicesWorkloadSimilarityWorkload -System State (RT) Relation
0.740.890.780.680.910.790.880.730.810.82PaymentRecommendation0.55
Figure 1: Visual Representation of Alipay’s Microservice
Architecture with Varied Functionalities. The top-left fig-
ure highlights the workload similarities across different mi-
croservices. The top-right figure elucidates the strong linkage
between workload patterns and the system state.
Empirical researchs [ 5,41] suggest that methods which incorporate
these workload similarities yield higher accuracy in predictions.
The core challenge is to precisely model workload similarities that
arise not solely from geographic proximity, but are significantly
influenced by intricate factors like network latency and traffic pat-
terns within clusters. Conventional topological diagrams, limited to
depicting basic connectivity or geographical layout, are insufficient
for capturing the complex structural workload similarity.
Challenge 2: Dynamic Temporal Pattern. Microservice work-
loads are highly variable, mirroring the dynamic nature of online
services and the constant changes in business strategies and re-
source allocation. [ 16] This variability manifests as temporal pat-
terns, which are trends, cycles, and fluctuations seen in workload
data over time. [ 3] These patterns are influenced by how microser-
vices are deployed, the subsequent shifting of workloads, and up-
dates in resource setups prompted by business changes. In Sec. 2.2,
we highlight how these temporal patterns evolve across microser-
vices, revealing the dynamic character of workload patterns.
Challenge 3: Effect of System State. The workload experi-
enced by microservices is closely linked to the state of the system
they operate in. This connection points to the intricate balance
between how microservices utilize resources and the overall avail-
ability of these resources. Unlike traditional fixed physical machines,
microservices run in environments where the resource configura-
tions can be adjusted to meet changing demands. These adjustments
lead to shifts in the system state, which in turn have a clear impact
on the workload, as shown by research [ 23,40,41]. As depicted in
Figure 1, an increase in workload often corresponds with a longer
response time. Sec. 2.3 details the relationship between the state
of the system and microservice workload. Recognizing the rela-
tionship is crucial for a holistic understanding of how the system’s
evolving conditions shape the workload of microservices.
Current workload prediction methods [ 3,15] emphasize tempo-
ral dynamics but neglect interdependencies between microservices
(Challenge 1), leading to incomplete prediction. While literature
[41] considers workload similarity, it overlooks the temporal varia-
tions (Challenge 2). Feng et al. [ 7] address both aspects, yet their
rule-based approach lacks in capturing the structual relationshipof microservices. These gaps highlight the necessity for method-
ologies that can integrate both complex similarities and dynamic
temporal patterns of microservice workloads. Existing series pre-
diction methods do not sufficiently incorporate network topologies,
resulting in a partial view of microservice architectures. There-
fore, we propose utilizing Spatio Temporal Graph Neural Networks
(STGNN), adept at handling structural and temporal data [ 36], to
overcome these challenges. However, STGNN adaptation is needed
for microservices, as their predominant use has been in urban sys-
tems, which differ from microservices in aspects like system state
effects (Challenge 3). A comprehensive adaptation of STGNNs for
microservice workload prediction is therefore imperative.
Therefore, We propose the Spatio Temporal Gr aph Network for
Microservice Workload Prediction ( STAMP ), systematically address-
ing the three challenges identified. Challenge 1 is approached
through STAMP ’s graph-based representation, where microservices
are nodes within a graph structure. The framework dynamically
constructs multiple graphs with uniform nodes but varied edge
weights reflective of workload interactions and system states, em-
ploying a multi-graph fusion attention mechanism to forge a com-
prehensive similarity graph. Graph convolutions within this frame-
work distill similarities across microservices. For Challenge 2,
STAMP employs the Fast Fourier Transform (FFT) to project the
workload data into the frequency domain, unveiling the workloads’
dynamic periodic patterns. Concerning Challenge 3, STAMP inte-
grates system state indicators into workload representations and
refines this integration through a cross-view contrastive learning
component. This component discriminates between pertinent and
superfluous system state details, sharpening the focus on relevant
data for workload predictions. Collectively, the STAMP framework
encapsulates workload similarity, dynamic temporal patterns, and
system state insights, improving the accuracy of microservice work-
load predictions. In all, our contributions are summarized as:
•We establish a foundational framework STAMP , the first STGNN
application for microservice workload prediction that adeptly
intertwines workload similarities and temporal dynamics,
setting a new precedent in the field.
•We improve the accuracy of microservice workload predic-
tion by incorporating system state insights into the STGNN
framework. Our research presents a convincing example and
contributes meaningfully to identifying the systemic factors
that influence workload patterns.
•We confirm STAMP ’s exceptional predictive accuracy, with a
notable 5.72% average improvement in RMSE across three
real-world datasets, and its application in Alipay’s resource
recommendation leads to a significant 33.10% reduction in
resource usage over existing online method.
2 Data Analysis
To provide insights into the distinctive features of microservice
workload prediction tasks, we undertake an analysis in this section,
examining workload and system state data from 465real-world
microservices within the Alipay application.
5522Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
2.1 Skewed Workload Distribution
We analyze the utilized CPU core of microservices in the Alipay
application to assess workload, with Fig. 2 illustrating our findings.
The utilization across the 465observed microservices spans from
2to11,930cores. Consistent with the Pareto principle [ 1], the
top 10% of microservices account for 75.19% of total utilization. In
industrial practice, we prioritize these high-traffic microservices for
workload prediction, optimizing efficiency and cost-effectiveness,
thus limiting our focus to a manageable subset of services with the
most significant workloads.
Figure 2: Skewed Distribution of Microservice Workload.
2.2 Dynamic Patterns of Workloads
Fig. 3 captures the shifting patterns of workload in the microser-
vices, underscoring the inherent dynamism in their operational be-
havior. Such shifts, driven by real-time deployments and workload
redistribution due to business realignments and resource adjust-
ments, introduce complexity to workload patterns. These evolving
dynamics challenge the stability of workload patterns, thereby com-
plicating the task of accurate prediction.
Figure 3: The Pattern Shift Observed in Microservices. The
shift periods are marked in red.
To delve deeper into the dynamic workload patterns of microser-
vices, we analyze their characteristics using the Autocorrelation
Function (ACF) and Partial Autocorrelation Function (PACF). Fig. 4
reveals that workloads exhibit correlations at various time lags,
including 12and24hours, suggesting the presence of daily cycles.These correlations, despite their instability, highlight the signifi-
cance of daily patterns for the prediction of microservice workloads.
Additionally, the behavior of ACF and PACF for these workloads
points to a non-stationary time series nature, as opposed to sta-
tionary series where ACF and PACF values quickly diminish and
approach zero beyond a certain lag.
Figure 4: ACF and PACF Analysis of Workloads. The left
ACF chart plots lag orders against correlation, with the blue
area showing the Error Band for negligible autocorrelation.
The right PACF chart displays independent correlations at
specific lags while eliminating the effects of intervening lags.
2.3 Effect of System State
Fig. 5 displays the correlation between system state indicators and
workload, including CPU requests, CPU utilization rate, number of
virtual machines, response time, successful requests, failed requests,
and total requests. To capture the system’s condition effectively, it is
crucial to avoid indicators that closely resemble the workload, such
as "req_success" and "req_total", as they offer limited additional
insight into the system’s true state. Indicators with low correlation,
like "req_fail", could be irrelevant to the workload and potentially
introduce noise, impairing the accuracy of predictions. Therefore,
we select "cpu_util", "rt", and "#VMs" as system state indicators
because they provide a more accurate representation of the system’s
status without merely echoing the workload information.
3 Problem Formulation
In this section, we will present the fundamental definitions of mi-
croservice workload prediction (MSWP). Table 2 provides a sum-
mary of the primary symbols employed.
In our task, our objective is to predict the future workload of
microservices within a cloud large-scale system. Each microser-
vice maintains a record of its historical workload and system state
indicators. We consider the number of utilized CPU cores as the
representation of the workload which provides a more comprehen-
sive perspective on service load from a computational resource
standpoint. Def. 1 gives the formal definition of workload series.
5523KDD ’24, August 25–29, 2024, Barcelona, Spain Yang Luo et al.
Figure 5: Heatmap of Pearson Correlation Coefficient be-
tween System State and Workload. cpu_req denotes CPU
requests, cpu_util signifies CPU utilization rate, #VMs repre-
sents the number of Virtual machines,rt represents respone
time, req_succ denotes successful requests, req_fail repre-
sents failed requests, req_total denotes total requests.
Definition 1 (Workload Series). Workload series records the
number of CPUs utilized by different microservices at different time
stamps. We denote by 𝑋(𝑊)
𝑚,𝑡the number of CPUs used by microservices
𝑚at time stamp 𝑡. Workload series is denoted as 𝑿(𝑊)∈R𝑀×𝑇,
where𝑀,𝑇 refer to the number of microservices and time stamps.
Due to the dynamic nature of resource allocation in microser-
vices, we introduce the resource configurations of microservices
denoted as Def. 2 to assist in workload prediction.
Definition 2 (System State). The System State consists of multi-
ple indicators in each microservice at different time stamps, including
response time, CPU utilization rate, and the number of allocated vir-
tual machines (a collection of related computing resources). We use
𝑹𝑻,𝑼,𝑶∈R𝑀×𝑇to represent the corresponding system state, where
𝑀,𝑇 refer to the number of microservices and time stamps.
Every indicator series is synchronized with the workload series
in the microservices’ dimension 𝑀and time stamps 𝑇. To simplify
matters, we employ subscripts 𝑚and𝑡to denote the selection of
respective information. For instance, 𝑅𝑇𝑚,𝑡is utilized to denote the
response time of microservice 𝑚at time stamp 𝑡.
Based on the workload series and system state, we give the formal
definition of the microservice workload prediction task, which is
denoted as Def. 3.
Definition 3 (Microservice Workload Prediction). Given
workload series 𝑿(𝑾)and system state 𝑹𝑻,𝑼,𝑶, the goal of MSWP
is to predict the workload of each microservice in time stamp 𝑇+1.
The prediction result can be denoted as ˜𝑿𝑇+1∈R𝑀.4 Methodology
In this section, we will give an overall review of the proposed STAMP .
STAMP is comprised of four main components: (1) the workload sim-
ilarity encoding module, (2) the system state modeling module, (3)
the dynamical temporal pattern encoding module, and (4) the cross-
view contrastive learning module for microservice representation.
Fig. 6 describes the framework of STAMP . In the following parts of
this section, we will introduce each of these four components.
4.1 Workload Similarity Encoding
In large-scale cloud systems, the collaboration of multiple microser-
vices in their operations creates the potential for similar workloads
among these services due to shared business functionalities and
inherent dependencies, which is mentioned in Challenge 1. For
instance, consider two microservices: one responsible for coarse
ranking and the other for presorting in a model inference task.
These two microservices may exhibit remarkably similar workload
patterns due to their shared nature within the overarching business
context. To capture the comprehensive similarity among discrete
microservices, we employ a dynamic approach that generates mul-
tiple graphs with congruent nodes, utilizing historical workload
patterns and system state indicators as references. The construc-
tion of similarity graphs varies when different workload series and
system state are inputted, enabling STAMP to adapt to the changing
patterns of microservice workloads. By learning the attention coef-
ficients within and between these constructed graphs, we obtain
a fused similarity matrix. This fused matrix serves as the basis for
further analysis, as we leverage graph convolution techniques to
extract representations for microservices.
4.1.1 Representation Generation and Graph Construction. We begin
by generating representation 𝑯(𝑊)∈R𝑀×𝑇×𝑑for microservices at
different time stamps, where 𝑑represents the represtantion dimen-
sion. To capture the similarity among microservices, our methodol-
ogy conceptualizes these distinct entities as nodes in a graph, giving
rise to a fully connected structure denoted as 𝐺={𝑽,𝑾}, compris-
ing𝑀nodes. By considering the workload series 𝑿(𝑊)and and sys-
tem state indicators 𝑹𝑻,𝑼,𝑶, we compute the pearson correlation
coefficient between pairs of microservices on different series and
utilize these similarities as edge weights. This process yields four dis-
tinct similarity graphs denoted as 𝑮={𝐺(𝑊),𝐺(𝑅𝑇),𝐺(𝑈),𝐺(𝑂)},
with consistent nodes while varying edge weight matrices.
4.1.2 Dynamic Multi-Graph Attention. To integrate the various
constructed similarity graphs, we employ the dynamic multi-graph
attention mechanism to capture similarity within and across the
graphs. Firstly, we generate the multi-graph representation 𝑬𝑣𝑖,𝐺𝑗=
𝑬𝑣𝑖+𝑬𝐺𝑗∈R𝑑for each node 𝑣𝑖in graph𝐺𝑗. The representation
𝑬𝑣𝑖is obtained through random walks [ 13], considering only edges
with a weight greater than 0.75to encode workload similarity. 𝑬𝐺𝑗
is the representation that encodes the features in 𝐺𝑗by two dense
layers. We denote the input of the 𝑙-th attention block as 𝑺(𝑙)and
the hidden state of node 𝑣𝑖∈𝑽in graph𝐺𝑗∈𝑮as𝑺(𝑙)
𝑣𝑖,𝐺𝑗.
For learning the within-graph similarity, we take into account the
correlations among all nodes within each graph. The hidden state
𝑊𝑆(𝑙)
𝑣𝑖,𝐺𝑖for within-graph similarity in the 𝑙- th block is computed
5524Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
PearsonCorrelationCoefficientOU𝑋(")RTSystem Status>0.75Random WalkWeightWorkloadGatedFusion
SimilarityEncodingModuleOURT||||TempConv
System State ModelingSysFFTInception𝑃($(%)𝑇𝑇/𝑓$Top k𝑓$𝑃(%)𝑃(%&')H(T)𝑋((&'Dynamical Temporal Pattern Encoding
Cross-view Contrastive Learning𝑌((&'𝜓(()TempConvInfoNCE𝐿)𝐿'𝐿*𝐿Series𝑊𝑆+!,-"𝐴𝑆+!,-"𝐸+!	𝐸-"	𝐸+!,-"𝑆	(/)𝐻	(")𝑊	(0)𝑆	(%&')𝑋	(1)𝑋	(")𝐻	(1)𝐻	(0)𝐻	(")𝐺	(")ConvolutionDenseAggregate
Figure 6: STAMP Framework. STAMP incorporates a dynamic multi-graph attention mechanism to encode the similarity between
workloads. Additionally, we take into account the influence of system information and dynamical temporal patterns. To retain
the effective components of the system state, we employ cross-view contrastive learning.
using the attention mechanism described in Eqn. (1):
𝑊𝑆(𝑙)
𝑣𝑖,𝐺𝑖=∑︁
𝑣𝑗∈𝑽𝒊𝛼𝑣𝑖,𝑣𝑗(𝑆(𝑙)
𝑣𝑗,𝐺𝑖). (1)
𝑽𝑖represents the set of all vertices excluding 𝑣𝑖.𝛼𝑣𝑗,𝑣𝑘denotes the
significance of 𝑣𝑗with respect to 𝑣𝑘.
For the calculation of the attention score 𝛼𝑣𝑖,𝑣𝑗, we concatenate
the hidden state of the nodes with the generated multi-graph rep-
resentation, enabling the attention mechanism to capture both
node and graph features. Eqn. (2) outlines the computation process,
where⟨·,·⟩denotes the inner product operation and ∥denotes the
concatenation operation.
𝑧𝑣𝑖,𝑣𝑗=⟨𝑆(𝑙)
𝑣𝑖,𝐺𝑖∥𝐸𝑣𝑖,𝐺𝑖,𝑆(𝑙)
𝑣𝑗,𝐺𝑖∥𝐸𝑣𝑗,𝐺𝑖⟩
√
2𝑑. (2)
We apply softmax fucntion 𝛼𝑣𝑖,𝑣𝑗=exp(𝑧𝑣𝑖,𝑣𝑗)/Í
𝑣𝑐∈𝑽exp(𝑧𝑣𝑖,𝑣𝑐)
to obtain the attention score.
To capture the across-graph similarity 𝑨𝑺, we calculate the
attention score between each pair of constructed graphs for each
node𝑣𝑖. The calculation process is described in Eqn. (3) and (4),
where𝛽𝐺𝑗,𝐺𝑐=exp(𝑢𝐺𝑗,𝐺𝑐)/Í|𝐺|
𝑘=1exp(𝑢𝐺𝑗,𝐺𝑘).
𝐴𝑆(𝑙)
𝑣𝑖,𝐺𝑗=∑︁|𝐺|
𝑐=1𝛽𝐺𝑗,𝐺𝑐(𝑆(𝑙)
𝑣𝑖,𝐺𝑐). (3)
𝑢𝐺𝑗,𝐺𝑐=⟨𝑆(𝑙)
𝑣𝑖,𝐺𝑗∥𝐸𝑣𝑖,𝐺𝑗,𝑆(𝑙)
𝑣𝑖,𝐺𝑐∥𝐸𝑣𝑖,𝐺𝑐⟩
√
2𝑑. (4)
Following the within-graph and across-graph attention, we uti-
lize the gated fusion mechanism [ 28] to combine 𝑾𝑺(𝑙)and𝑨𝑺(𝑙)
as the input of the next attention block 𝑺(𝑙+1)∈R𝑀×|𝐺|×𝑑.
4.1.3 Graph Convolution Based on the Fused Matrix. Finally, we
transform the output of the last dynamic multi-graph attention
block 𝑺(𝐿)into the fused matrix 𝑾(𝑆)∈R𝑀×𝑀which provides acomprehensive similarity between every pair of microservices. We
project 𝑺(𝐿)onto ˜𝑺∈R𝑀×|𝐺|using a dense layer. Then, we obtain
the fused matrix 𝑾(𝑆)=˜𝑺×˜𝑺𝑇by matrix multiplication.
At the end of this module, we utilize the fused matrix 𝑾(𝑆)
to conduct graph convolution on the microservice representation
𝑯(𝑊)generated earlier, as described by Eqn. (5), where 𝑮(𝑆)is
a fully connected graph created based on the fused matrix 𝑾(𝑆).
This results in a new representation of the microservices denoted
as𝑯(𝑆)∈R𝑀×𝑇×𝑑, which incorporates comprehensive similarity.
𝑯(𝑆)=𝐺𝑟𝑎𝑝ℎ𝐶𝑜𝑛𝑣𝑜𝑙𝑢𝑡𝑖𝑜𝑛 (𝑯(𝑊),𝑮(𝑆)). (5)
4.2 System State Modeling
AsChallenge 3 describes, the dynamic architecture of microser-
vices introduces variability in the system state, which necessitates
the consideration of its impact on workload prediction. The sys-
tem state of microservices, including factors like CPU utilization
rate, can fluctuate over time, resulting in variations in workload.
Understanding and incorporating these dynamically changing sys-
tem state into workload prediction is crucial for comprehending
the underlying causes of workload variations. By accounting for
the impact of the system state on workload, we can gain insights
into the factors influencing workload changes and enhance the
accuracy of workload predictions. Therefore, we model the system
state using dense layers and integrate the modeled factors into the
representation of microservice workload.
In the first step, we concatenate the system state indicators to
obtain the overall representation of the system state denoted as
𝑿(𝑀)=𝑹𝑻∥𝑼∥𝑶∈R𝑀×𝑇×3. Next, we project 𝑿(𝑀)onto
R𝑀×𝑇×𝑑to learn potential system state representations. To capture
the correlation between system state and workload series, we align
the system state representation with the workload series. Finally,
we use two dense layers to model the aligned representation as
5525KDD ’24, August 25–29, 2024, Barcelona, Spain Yang Luo et al.
the system state, as described by Eqn. (6). Here, 𝑾𝑚,1and𝑾𝑚,2
are learnable projection matrices used to ensure the alignment
of the two representations. The dense layers are implemented as
𝐷𝑒𝑛𝑠𝑒(·):R𝑀×𝑇×2𝑑→R𝑀×𝑇×𝑑.
𝑺𝒚𝒔=𝐷𝑒𝑛𝑠𝑒(𝑿(𝑀)𝑾𝑚,1∥𝑿(𝑊)𝑾𝑚,2). (6)
Finally, we obtain the output of the module 𝑯(𝑀)by incorporating
system state factors 𝑺𝒚𝒔into the representation of microservice
after similarity encoding 𝑯(𝑆).
𝑯(𝑀)=𝑯(𝑆)+𝑺𝒚𝒔. (7)
4.3 Dynamical Temporal Pattern Encoding
Microservice workloads are subject to the influence of factors such
as user behavior and business requirements, giving rise to temporal
patterns with a degree of regularity. For instance, microservices
responsible for payment transactions may exhibit lower workloads
in the morning due to fewer active shoppers, followed by a gradual
increase throughout the day, and culminating in a peak during the
evening when shopping activity tends to be higher. These recurring
patterns in the temporal workload can facilitate accurate workload
prediction. However, as described in Challenge 2, the periodicity
of microservice workloads can be variable and may change with
evolving demands. Therefore, to address this challenge and capture
various changing periodic patterns, we utilize Fast Fourier Trans-
forms (FFT) which is widely used in period detection to transform
the intermediate representations into the frequency domain. By
identifying the dominant periodic components in the frequency
domain, we reshape the representations and perform temporal con-
volutions to ultimately obtain microservice representations that
encode dynamical temporal patterns.
4.3.1 Finding Periodic Patterns. We employed periodic blocks to
capture the dynamic periodic components. The input of the 𝑖-th
periodic block is denoted as 𝑷(𝑙)∈R𝑀×𝑇×𝑑. The first step is to
identify the periodic components in 𝑷(𝑙). Eqn. (8) describes the
identification process by computing the average amplitude of each
frequency across 𝑀×𝑑dimensions for the transformed 𝑷(𝑙). The
𝑘frequencies 𝑓1,...,𝑓𝑘with the highest average amplitudes are
selected as the periodic components of the input.
{𝑓1,...,𝑓𝑘}=𝑇𝑜𝑝𝐾(𝐴𝑣𝑔(𝐴𝑚𝑝(𝐹𝐹𝑇(𝑷(𝑙))))). (8)
Next, we decompose the temporal dimension 𝑇of𝑷(𝑙)into two
dimensions 𝑝𝑖=⌈𝑇/𝑓𝑖⌉and𝑓𝑖. This results in the decomposed
representation 𝑷(𝑙)
𝑖∈R𝑀×𝑝𝑖×𝑓𝑖×𝑑for each frequency 𝑓𝑖.
Subsequently, we convolve the decomposed temporal dimen-
sion of 𝑷(𝑙)
𝑖to extract periodic components and transform the
decomposed dimension back to 𝑇after convolution. This process
is illustrated in Eqn. (9), where 𝐼𝑛𝑐𝑒𝑝𝑡𝑖𝑜𝑛(·)represents the convo-
lution operation on dimensions 𝑝𝑖and𝑓𝑖, and𝑅𝑒𝑠ℎ𝑎𝑝𝑒(·)denotes
the transformation process of the temporal dimension back to 𝑇.
˜𝑷(𝑙)
𝑖=𝑅𝑒𝑠ℎ𝑎𝑝𝑒(𝐼𝑛𝑐𝑒𝑝𝑡𝑖𝑜𝑛(𝑷(𝑙)
𝑖)), 𝑖∈{1,...,𝑘}. (9)
Finally, we aggregate ˜𝑷(𝑙)
𝑖corresponding to different frequencies
𝑓𝑖to generate the input to the next layer. We compute a weightedsum on ˜𝑷(𝑙)
𝑖, where the weights of each term are determined by the
amplitude of its corresponding frequency using a softmax function.
4.3.2 Temporal Pattern Encoding. By taking 𝑯(𝑀)as the input
of the first periodic block, we obtain the representation 𝑷(𝐿)∈
R𝑀×𝑇×𝑑containing periodic components after 𝐿periodic blocks.
To capture the temporal pattern of microservice series across dif-
ferent time stamps, we employ a temporal convolutional network.
Eqn. (10) describes this process, where 𝑯(𝑇)∈R𝑀×𝑑is the mi-
croservice representation after temporal pattern encoding, and
𝑇𝑒𝑚𝑝𝑜𝑟𝑎𝑙𝐶𝑜𝑛𝑣𝑜𝑙𝑢𝑡𝑖𝑜𝑛 (·)denotes the convolution operation on
the temporal dimension 𝑇. Leaky ReLU is used as the activation
function in the temporal convolution.
𝑯(𝑇)=𝑇𝑒𝑚𝑝𝑜𝑟𝑎𝑙 𝐶𝑜𝑛𝑣𝑜𝑙𝑢𝑡𝑖𝑜𝑛 (𝑷(𝐿)). (10)
Finally, we utilize dense layers to transform 𝑯(𝑇)and obtain the
final used prediction result ˜𝑿𝑇+1∈R𝑀.
4.4 Cross-view Contrastive Learning
As the system state of a microservice describes various aspects of
information, we need to consider the impact of redundant infor-
mation in the system state when utilizing it to assist in workload
prediction. To extract useful information from the system state for
workload prediction, we construct two different views. The first
view consists of the three modules described in Section 4.1- 4.3,
which utilize the system state information. The second view ex-
cludes all system state information and only uses workload series
to generate workload-only representations for each microservice.
Due to the inherent characteristics of microservices, we expect that
the representations generated by the two views for the same mi-
croservice should be similar, while the representations for different
microservices should be distinct. By contrastive learning, we can
effectively eliminate redundant information in the system state
while retaining the effective components from the system state
information for workload prediction.
To generate workload-only representations, we apply similarity
and temporal pattern encodings to the initial microservice rep-
resentations 𝑯(𝑊). For the similarity encoding, we directly use
the similarity graph constructed 𝐺(𝑊)based on the workload se-
ries to perform graph convolution, as shown in Eqn. (11). For the
temporal pattern encoding, we perform temporal convolution on
𝚿(𝑆)along the temporal dimension to obtain the workload-only
representations for the microservices 𝚿(𝑇)∈R𝑀×𝑑.
𝚿(𝑆)=𝐺𝑟𝑎𝑝ℎ𝐶𝑜𝑛𝑣𝑜𝑙𝑢𝑡𝑖𝑜𝑛 (𝑯(𝑊),𝑮(𝑊)). (11)
𝚿(𝑇)=𝑇𝑒𝑚𝑝𝑜𝑟𝑎𝑙 𝐶𝑜𝑛𝑣𝑜𝑙𝑢𝑡𝑖𝑜𝑛 (𝚿(𝑆)). (12)
We further utilize dense layers to project the workload-only repre-
sentation 𝚿(𝑇)to the prediction ˜𝒀𝑇+1. In addition, we evaluate the
microservice representations. The representations obtained from
both views for the same microservices are considered positive pairs,
while the representations of different microservices generated by
the two views are treated as negative pairs. To optimize the qual-
ity of the representations, we employ the InfoNCE function [ 14],
as described in Eqn (13). This objective function ensures that the
representations of positive pairs are brought closer together, while
the representations of negative pairs are pushed further apart.
5526Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
𝑳𝒄=𝑀∑︁
𝑚=1logexp(cos(𝑯(𝑇)
𝑚,𝚿(𝑇)
𝑚))
Í
𝑚′exp(cos(𝑯(𝑇)
𝑚,𝚿(𝑇)
𝑚′)). (13)
We optimize the model parameters by combining the prediction
error with the contrastive loss 𝑳𝒄, which forms the final loss func-
tion 𝑳as shown in Eqn. (14). Here, 𝑳1=∥˜𝑿𝑇+1−𝑿𝑇+1∥2and
𝑳2=∥˜𝒀𝑇+1−𝑿𝑇+1∥2represent the prediction errors on the work-
load. 𝑿𝑇+1is the real value of workload. To balance the contribu-
tions of different terms, we use weight coefficients 𝜆1and𝜆2. The
learning process is described in Alg. 1.
𝑳=𝑳1+𝜆1𝑳2+𝜆2𝑳𝒄. (14)
Algorithm 1: Training Process of STAMP
Input: Workload Series 𝑿(𝑊)∈R𝑀×𝑇, System state
indicators 𝑹𝑻,𝑶,𝑼, number of training epoch 𝐸,
weight coefficients 𝜆1,𝜆2, learning rate 𝑟.
Output: Model Parameters 𝚯
1for1≤𝑒≤𝐸do
2 Generate the representation of workload series 𝑯(𝑊);
3 Construct multiple graphs and generate fused similarity
graph 𝑾(𝑆)by Eqn. (1) to (4);
4 Capture comprehensive similarity with 𝑯(𝑆)based on
the fused graph according to Eqn. (5);
5 Model the effect of system state 𝑺𝒚𝒔and generate 𝑯(𝑀)
by Eqn. (6) and (7);
6 Capture dynamic temporal pattern with 𝑯(𝑇)and
generate prediction ˜𝑿(𝑇+1)by Eqn. (8) - (10);
7 Generate workload-only representation 𝚿(𝑇)and
prediction ˜𝒀(𝑇+1)by Eqn. (11) and (12);
8 Calculate the loss 𝑳by Eqn. (13) and (14);
9 Update model parameters 𝚯through 𝑳;
10 for𝜃∈𝚯do
11𝜃←𝜃−𝑟·𝜕𝑳
𝜕𝜃
12return Model Parameters 𝚯
5 Experiments
In this section, we present our experimental framework to set the
stage for the investigation. We then conduct a series of experiments
designed to explore the following research questions:
RQ1: How does the performance of STAMP compare with that of
state-of-the-art methods in predicting workloads for microservices?
RQ2: Can the key components within STAMP be identified as
significant contributors to its superior performance?
RQ3: How does the application of STAMP ’s prediction results
improve resource allocation in microservice environments?
5.1 Experiment Setup
5.1.1 Datasets. In our investigation of STAMP ’s performance, we
utilize three datasets representative of distinct microservice envi-
ronments. (1) Ant dataset contains 245hours of workload data from465microservices within the Alipay application. (2) Ali1dataset
sourced from the Alibaba Cluster Trace, documents multivariate
workload metrics from 148machines across an 8-day period, with
each machine hosting a set of different microservices. (3) The Fisher
2dataset comprises workload data from 10containers over a span of
30days within a Kubernetes framework, again with each container
running separate microservices. These datasets were selected to
ensure a comprehensive evaluation of STAMP across a broad range
of microservice architectures and conditions.
5.1.2 Data preprocessing. The preprocessing phase addressed out-
lier values within the workload series, setting any values below 0to
0. Following this correction, we normalized the data using Min-Max
Normalization to reduce the effects of scale discrepancies across
the dataset. For consistency with typical production environments,
as referenced in [ 15], we consolidated the datasets into hourly in-
tervals. We then partitioned the data into training and testing sets
at a ratio of 7:1, mindful of the temporal sequence. The validation
set, used for fine-tuning parameters, was extracted from the last 24
hours of the training dataset.
5.1.3 Baselines. To evaluate the performance of STAMP , we com-
pare it with 13state-of-the-art baselines. These baselines encom-
passed traditional statistical models: ARIMA [27],Prophet [29],
multivariate prediction models: FEDfomer [43],FiLM [42],N-
Transformer [22],LightTS [39],PatchTST [26],DLinear [38],
Pyraformer [20], and STGNNs: STGCN [37],MTGNN [36],ST-
HSL [19], ST-SSL[18]. Appx. B provides a detailed introduction.
5.1.4 Evaluation Metrics. To evaluate the accuracy of STAMP in
the field of forecasting workload of microservices, we utilize two
metrics: Root Mean Square Error (RMSE) and Mean Absolute Error
(MAE) which have been widely used in predicting future time series
data. To be fair, the reported prediction performance of all compared
methods is averaged over all days in the whole test period. Lower
MAE and RMSE score shows better prediction performance.
5.1.5 Parameter Settings. STAMP is implemented using PyTorch
and trained using the Adam optimizer on a Tesla V100 GPU. The
hyperparameter settings are as follows: the learning rate is set to
0.001, the batch size is set to 16. The dimension of representation 𝑑
is set to 16. The kernel size for the temporal convolutions is set to
3, with two convolutional layers. The numbers of dynamic multi-
graph attention blocks and periodic patterns finding blocks are
both set to 2. Additionally, the weights for the regularization terms,
𝜆1and𝜆2, are set to 0.8 and 1, respectively.
5.2 Comparative Results: RQ1
Table 1 presents the comparative results between STAMP and various
baselines. The experimental data lead to several key insights:
STAMP outperforms statistical methods, multivariate series pre-
diction methods, and STGNNs in predicting microservice workloads.
Models like LightTS and DLinear, which are multivariate series pre-
dictors, fall short because they do not account for the structual
connections between microservices as highlighted in Challenge
1, which is crucial for accurate predictions. PatchTST fares better
1https://github.com/alibaba/clusterdata/tree/v2018
2https://github.com/chrisliu1995/Fisher-model/tree/master
5527KDD ’24, August 25–29, 2024, Barcelona, Spain Yang Luo et al.
Table 1: Comparisons of different models on three datasets. The best results are in boldface and the second best results are
underlined. "Impv." indicates the relative improvement of the best results compared to the second-best results.
Datasets
Metrix T ARIMA Prophet STGCN MTGNN ST-HSL ST-SSL FEDformer FiLM LightTS DLinear Pyraformer PatchTST N-Transformer STAMP Impv.
AntRMSE12
0.1477 0.4544 0.2770 0.1250 0.1304
0.2757 0.1668 0.2345 0.2471 0.2402 0.1638 0.1350 0.1403 0.1130 9.60%
24 0.1423 0.3608 0.2799 0.1101 0.1070 0.2625
0.1625 0.2111 0.2541 0.2096 0.1686 0.1078 0.1515 0.0937 12.43%
36 0.1362 0.2532 0.2806 0.1075 0.1115
0.2652 0.2146 0.2249 0.3222 0.2271 0.1641 0.1082 0.1657 0.1010 6.05%
48 0.1329 0.1131 0.2799 0.1085 0.1280
0.2646 0.2256 0.2230 0.2354 0.2118 0.1597 0.1103 0.1640 0.1024 5.62%
MAE12
0.0948 0.3592 0.2368 0.0867 0.0958
0.2272 0.1227 0.1786 0.1875 0.1876 0.1185 0.0934 0.0971 0.0792 8.65%
24 0.0917 0.2845 0.2356 0.0787 0.0743 0.2146 0.1185 0.1551 0.1991 0.1662 0.1207 0.0714 0.1026 0.0653 8.54%
36
0.0887 0.1994 0.2331 0.0753 0.0803 0.2238 0.1615 0.1693 0.2452 0.1831 0.1200 0.0727 0.1155 0.0690 5.09%
48
0.0870 0.0752 0.2344
0.0785 0.0858 0.2088 0.1724 0.1659 0.1871 0.1623 0.1127 0.0753 0.1124 0.0705 6.25%
AliRMSE12
0.2206 0.2322 0.2603 0.2033 0.1972 0.2638
0.2387 0.2371 0.2409 0.2365 0.2623 0.2027 0.2273 0.1960 0.06%
24 0.2120 0.2722 0.2538 0.2164 0.1993 0.2637
0.2600 0.2613 0.3232 0.2612 0.2622 0.2036 0.2449 0.1975 0.09%
36 0.2073 0.2810 0.2574 0.2191 0.1996 0.2609
0.2630 0.2646 0.2650 0.2649 0.2590 0.2026 0.2556 0.1961 1.75%
48 0.3478 0.2901 0.2630 0.2142 0.2113 0.2562 0.2625 0.2578 0.2847 0.2512 0.2580 0.2060 0.2468 0.2006 2.62%
MAE12
0.1667 0.1793 0.2104 0.1662 0.1576 0.2103 0.1880 0.1883 0.1924 0.1830 0.2057 0.1565 0.1735 0.1559 0.38%
24
0.1619 0.2122 0.2017 0.1790 0.1583 0.2095 0.2061 0.2138 0.2632 0.2048 0.2056 0.1583 0.1887 0.1587 -0.25%
36
0.1587 0.2247 0.2124 0.1819 0.1587 0.2109 0.2085 0.2192 0.2122 0.2082 0.2008 0.1577 0.1991 0.1549 1.78%
48
0.2565 0.2322 0.2128 0.1776 0.1705 0.2092 0.2069 0.2126 0.2266 0.2008 0.1999 0.1627 0.1890 0.1589 2.34%
FisherRMSE12
0.2817 0.1627 0.2180 0.1472 0.1328 0.2394
0.1463 0.1645 0.2096 0.1581 0.2547 0.1423 0.1481 0.1254 5.57%
24 0.2730 0.1945 0.2039 0.1467 0.1410 0.2352 0.1442 0.1583 0.3548 0.1499 0.2470 0.1359 0.1529 0.1228 9.64%
36
0.2693 0.1631 0.2031 0.1459 0.1347 0.2305 0.1503 0.1642 0.2078 0.1536 0.2539 0.1338 0.1563 0.1251 6.50%
48
0.2747 0.1659 0.2170 0.1364 0.1427 0.2082 0.1524 0.1664 0.3397 0.1565 0.2506 0.1361 0.1546 0.1243 8.67%
MAE12
0.2101 0.1065 0.1742 0.1048 0.0952 0.1892 0.1029 0.1143 0.1602 0.1075 0.1866 0.0928 0.0952 0.0788 15.09%
24
0.2035 0.1351 0.1620 0.1095 0.1018 0.1855 0.1013 0.1130 0.2495 0.1056 0.1815 0.0882 0.1052 0.0775 12.13%
36
0.2014 0.1122 0.1788 0.1047 0.0934 0.1808 0.1074 0.1189 0.1611 0.1074 0.1867 0.0889 0.1047 0.0802 9.79%
48
0.2045 0.1183 0.1731 0.0941 0.1021
0.1792 0.1095 0.1218 0.2505 0.1123 0.1847 0.0945 0.1052 0.0831 11.69%
by partially addressing service dependencies through its channel-
independent mechanism, yet it still does not achieve the best results.
Among STGNNs, models such as MTGNN and ST-HSL that ac-
tively learn dynamic service dependencies demonstrate higher ef-
ficacy. However, methods based on static graphs like STGCN and
ST-SSL perform poorly because they overlook temporal dynamics
and the evolving similarities in workloads described in Challenge
2. These findings confirm the importance of dynamic graph learn-
ing in workload prediction. What’s more, we observed that our
improvement on the Alidataset was relatively modest, which we
believe is due to the weaker temporal regularity of workloads, mak-
ing it difficult for prediction methods to capture future patterns.
Our research also determines that an input series length of about
24time stamps, corresponding to a full daily cycle, is most suitable
for capturing microservice workload patterns. Shorter lengths miss
capturing the complete periodic cycle, and longer lengths introduce
inconsistencies due to the dynamic nature of microservices, leading
to predictions muddied with irrelevant temporal patterns as de-
scribed in Challenge 2. Transformer-based methods designed for
long-term predictions with extensive input do not suit the precision
required for immediate microservice workload prediction well.
STAMP ’s consistent outperformance across various scenarios is
attributed to: (1) The use of graph neural networks to represent
microservices as nodes, mapping out their complex structures and
interdependencies. (2) The implementation of Fourier Transform to
model the dynamic aspects of workloads, enabling effective track-
ing and prediction of their temporal changes. (3) A specialized
approach that considers dynamic resource allocation by incorpo-
rating system states into workload predictions, using cross-view
contrastive learning to capture essential system state features.
Figure 7: Comparison of Prediction Accuracy between STAMP
and its variant on RMSE and MAE respectively.
5.3 Ablation Study: RQ2
We perform ablation studies to analyze the influence of key compo-
nents within STAMP. The results are recorded in Fig. 7.
(1) Impact of Workload Similarity Encoding Module: We replace
the dynamic multi-graph fusion mechanism with a plain workload
similarity graph convolution ("w/o fusion"). The absence of this
mechanism significantly weakens STAMP ’s ability to capture the
complex dependencies among microservices, as described in Chal-
lenge 1. Across the three datasets, the exclusion of this module
resulted in the most significant increases in prediction errors, under-
scoring its utility in grasping comprehensive workload similarities.
(2) Impact of Dynamical Temporal Pattern Encoding: Removing
the Dynamical Temporal Pattern Encoding module ("w/o tb") hin-
ders STAMP from detecting dynamic patterns in workloads, leading
to increased prediction errors. This module’s critical role in tack-
ling the variability of workloads, as highlighted in Challenge 2,
is essential for maintaining high predictive precision. Importantly,
the module’s impact on accuracy varies across datasets, offering
more advantages for datasets with regular workloads, such as Ant
andFisher, compared to those with irregular workloads like Ali.
(3) Impact of System State Modeling Module: Removing the Sys-
tem State Modeling Module ("w/o sys") underscores its fundamental
5528Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
role in embracing system states for accurate workload prediction.
Its absence has consistently resulted in raised prediction errors
across all datasets. The module’s strength lies in its adept use of sys-
tem context to grasp dynamics affecting workload patterns, proving
essential in confronting Challenge 3.
(4) Impact of Cross-view Contrastive Learning: Removing Cross-
view Contrastive Learning ("w/o contrast") makes the model more
susceptible to irrelevant system noise, leading to a notable drop
in predictive accuracy. For example, on the Fisher dataset, the
exclusion resulted in RMSE and MAE increases of about 7.1% and
19.1%, respectively. This module is important as it sharpens the
critical features needed for precise predictions, proving its necessity
in boosting the representational integrity of the system state.
5.4 Industrial Results: RQ3
In a week-long A/B test on the Alipay cloud platform, targeting
the top 10% of microservices by CPU core usage—a segment that
represents 75.6% of the platform’s total core usage. Our approach,
STAMP , demonstrated its efficiency by achieving a 33.10% reduction
in CPU core utilization compared to the existing online method.
This optimization could result in weekly savings of 176,418dollars,
embodying both economic and environmental benefits through
enhanced adherence to green computing standards.
The control group relied on the existing online method, which
recommends CPU resources based on the 95th percentile of the max
core usage in the last day. The experimental group implemented
STAMP , which predicts CPU core requirements and then adjusts
these predictions according to a target CPU utilization rate to arrive
at a finely-tuned resource recommendation.
Throughout the A/B test, STAMP proved its effectiveness by not
only accurately predicting CPU demands but also by successfully
mitigating the risk of CPU overflow in 98.16% of cases. While STAMP
functions autonomously, it still allows for manual interventions
during unusual operational events, thereby offering a sophisticated
and adaptable approach to CPU resource recommendation.
6 Related Works
In this section, we discuss the various approaches to workload pre-
diction by categorizing them into statistical methods, multivariate
prediction methods, and STGNNs, highlighting specific challenges
each faces in the context of microservice workload prediction.
Statistical Methods: Statistical methods are widely adopted for
predicting workloads in large-scale application systems, favored for
their computational speed and the clarity with which results can
be interpreted [ 2,4,6]. Algorithms such as ARIMA [ 2] and Prophet
[29] apply numerical statistical techniques to predict future work-
loads. Although commonly used, these methods fail to capture the
dynamic temporal patterns that are critical for accurately predicting
microservice workload, which is addressed in Challenge 2.
Multivariate Prediction Methods: Deep learning advance-
ments have equipped us with tools to capture relationships be-
tween multiple variables in workload prediction [ 7,10,15]. These
include (1) recurrent neural network-based models, such as LSTM
in LoadDynamics [ 17] and RNN in [ 32]. (2) convolution-based mod-
els like TimesNet [ 35], and (3) self-attention models, exemplified
by the TimeMixer [ 34], iTransformer [ 21] and KAE-Informer [ 15].Although these models are proficient in mapping the interactions
between variables, they fall short in modeling the structural inter-
connections amongst services, thus not fully exploring the struc-
tured nature of service relations addressed in Challenge 1.
Spatio-Temporal Graph Networks (STGNNs): These net-
works represent the latest in capturing dependencies between vari-
ables by treating them as nodes in a graph [ 30,36]. ST-HSL [ 19]
construct learnable hyperedges on the graphand adopt cross-view
contrastive learning to alleviate the sparsity problem. MAST [ 25]
and Wei et al [ 28] apply the dynamic attention mechanism to fuse
multiple similarity graphs.Despite their capabilities, these methods
often do not consider the influence of the system’s state on work-
load patterns, a significant aspect of microservice environments,
which is addressed in Challenge 3.
Summary: Each prediction approach, while making significant
contributions to the field, shows specific gaps in the context of mi-
croservice workload prediction. Statistical methods lack dynamic
temporal modeling (Challenge 2), multivariate prediction meth-
ods do not thoroughly explore the structural relationships among
services (Challenge 1), and STGNNs may miss the mark on incor-
porating system state dynamics (Challenge 3). A comprehensive
framework that effectively integrates all these aspects is crucial for
the accurate prediction of workloads in microservice architectures.
7 Discussion and Conclusion
This paper introduces a STGNN framework, which effectively ad-
dresses three key challenges in microservice workload prediction:
capturing workload similarities, understanding dynamic temporal
patterns, and assessing the system state’s impact. By leveraging
factor embeddings and cross-view contrastive learning, STGNN
filters out irrelevant details, thereby enhancing prediction accuracy.
Our empirical evaluation confirms the framework’s proficiency,
particularly in aiding resource recommendation for microservices.
As indicated by [ 15], workload predictions are typically per-
formed hourly, a frequency that optimizes system stability and
allows for timely model inference. Since even slight increments in
precision can yield significant cost savings in large-scale systems,
accuracy takes precedence over training speed. This notion is sup-
ported in settings with a moderate number of microservices, as
demonstrated in [ 15] and [ 7]. Section 2.1 further elucidates, in line
with the Pareto Principle [ 1], that a small subset of services often
handles the majority of the workload, implying that concerns over
scalability due to large numbers of services may be overstated.
Collectively, these insights affirm that STAMP is a viable and
effective tool for real-world implementation, capable of delivering
dependable workload predictions irrespective of service count.
8 ACKNOWLEDGEMENT
This work was sponsored by the National Natural Science Foun-
dation of China [U23A20309, 62272302, 62172276, 62372296], the
Shanghai Municipal Science Technology Major Project [2021SHZD
ZX0102] and CCF-Ant Research Fund [CCF-AFSG RF20220218].
References
[1]Jesús Basulto Santos, Javier Busto Guerrero, and Rocío Sánchez Lissen. 2011. El
concepto de desigualdad en Vilfredo Pareto (1848-1923). Historia de la probabilidad
y la estadística (V) (2011).
5529KDD ’24, August 25–29, 2024, Barcelona, Spain Yang Luo et al.
[2]Rodrigo N Calheiros, Enayat Masoumi, Rajiv Ranjan, and Rajkumar Buyya. 2014.
Workload Prediction Ssing ARIMA Model and Its Impact on Cloud Applications’
QoS. IEEE Transactions on Cloud Computing (TCC) 3, 4 (2014), 449–458.
[3]Jiadong Chen, Yang Luo, Xiuqi Huang, Fuxin Jiang, Yangguang Shi, Tieying
Zhang, and Xiaofeng Gao. 2023. IPOC: An Adaptive Interval Prediction Model
based on Online Chasing and Conformal Inference for Large-Scale Systems. In
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 202–212.
[4]Mincheng Chen, Jingling Yuan, Dongling Liu, and Tao Li. 2020. An Adaption
Scheduling Based on Dynamic Weighted Random Forests for Load Demand
Forecasting. The Journal of Supercomputing (TJSC) 76, 3 (2020), 1735–1753.
[5]Ke Cheng, Sheng Zhang, Chenghong Tu, Xiaohang Shi, Zhaoheng Yin, Sanglu Lu,
Yu Liang, and Qing Gu. 2023. ProScale: Proactive Autoscaling for Microservice
With Time-Varying Workload at the Edge. IEEE Transactions on Parallel and
Distributed Systems (TPDS) 34, 4 (2023), 1294–1312.
[6]Sudipto Das, Feng Li, Vivek R Narasayya, and Arnd Christian König. 2016. Auto-
mated Demand-driven Resource Scaling in Relational Database-as-a-service. In
ACM International Conference on Management of Data (SIGMOD). 1923–1934.
[7]Binbin Feng and Zhijun Ding. 2023. GROUP: An End-to-end Multi-step-ahead
Workload Prediction Approach Focusing on Workload Group Behavior. In ACM
The Web Conference (WWW). 3098–3108.
[8]Yu Gan, Mingyu Liang, Sundar Dev, David Lo, and Christina Delimitrou. 2021.
Sage: Practical and Scalable ML-driven Performance Debugging in Microser-
vices. In ACM International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS). 135–151.
[9]Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, and
Nayan Katarki et al. 2019. An Open-Source Benchmark Suite for Microser-
vices and Their Hardware-Software Implications for Cloud & Edge Systems. In
ACM Proceedings of the Twenty-Fourth International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS). 3–18.
[10] Yuanning Gao, Xiuqi Huang, Xuanhe Zhou, Xiaofeng Gao, Guoliang Li, and
Guihai Chen. 2023. DBAugur: An Adversarial-based Trend Forecasting System
for Diversified Workloads. In International Conference on Data Engineering (ICDE).
1–13.
[11] Google. 2019. Google Cluster Trace. https://github.com/google/cluster-data/tree/
master
[12] Google. 2021. Microservices in Google Cloud. https://cloud.google.com/blog/
topics/developers-practitioners/microservices-architecture-google-cloud
[13] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for
Networks. In ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 855–864.
[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.
Momentum Contrast for Unsupervised Visual Representation Learning. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). 9726–9735.
[15] Qin Hua, Dingyu Yang, Shiyou Qian, Hanwen Hu, Jian Cao, and Guangtao Xue.
2023. KAE-Informer: A Knowledge Auto-Embedding Informer for Forecasting
Long-Term Workloads of Microservices. In ACM The Web Conference (WWW).
1551–1561.
[16] Shaoyuan Huang, Zheng Wang, Heng Zhang, Xiaofei Wang, Cheng Zhang, and
Wenyu Wang. 2023. One for All: Unified Workload Prediction for Dynamic
Multi-tenant Edge Cloud Platforms. In ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 788–797.
[17] Vinodh Kumaran Jayakumar, Jaewoo Lee, In Kee Kim, and Wei Wang. 2020. A
Self-Optimized Generic Workload Prediction Framework for Cloud Computing.
InIEEE International Parallel and Distributed Processing Symposium (IPDPS). 779–
788.
[18] Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, Junbo
Zhang, and Yu Zheng. 2023. Spatio-Temporal Self-Supervised Learning for Traffic
Flow Prediction. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI.
4356–4364.
[19] Zhonghang Li, Chao Huang, Lianghao Xia, Yong Xu, and Jian Pei. 2022. Spatial-
Temporal Hypergraph Self-supervised Learning for Crime Prediction. In IEEE
International Conference on Data Engineering (ICDE). 2984–2996.
[20] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, and
Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for
Long-Range Time Series Modeling and Forecasting. In The Tenth International
Conference on Learning Representations, (ICLR).
[21] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and
Mingsheng Long. 2024. itransformer: Inverted transformers are effective for time
series forecasting. In International Conference on Learning Representations (ICLR).
[22] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary
Transformers: Exploring the Stationarity in Time Series Forecasting. In Neural
Information Processing Systems, (NeurIPS).
[23] Shutian Luo, Huanle Xu, Kejiang Ye, Guoyao Xu, Liping Zhang, Jian He, Guodong
Yang, and Chengzhong Xu. 2023. Erms: Efficient Resource Management for
Shared Microservices with SLA Guarantees. In ACM International Conference
on Architectural Support for Programming Languages and Operating Systems,
(ASPLOS). 62–77.[24] Shutian Luo, Huanle Xu, Kejiang Ye, Guoyao Xu, Liping Zhang, Guodong Yang,
and Chengzhong Xu. 2022. The Power of Prediction: Microservice Auto Scaling
via Workload Learning. In ACM Symposium on Cloud Computing (SoCC).
[25] Yang Luo, Zehao Gu, Shiyang Zhou, Yun Xiong, and Xiaofeng Gao. 2023.
Meteorology-Assisted Spatio-Temporal Graph Network for Uncivilized Urban
Event Prediction. In IEEE International Conference on Data Mining, ICDM. 468–
477.
[26] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In
The Eleventh International Conference on Learning Representations, (ICLR).
[27] Bei Pan, Ugur Demiryurek, and Cyrus Shahabi. 2012. Utilizing Real-World Trans-
portation Data for Accurate Traffic Prediction. In IEEE International Conference
on Data Mining (ICDM). 595–604.
[28] Wei Shao, Zhiling Jin, Shuo Wang, Yufan Kang, Xiao Xiao, Hamid Menouar,
Zhaofeng Zhang, Junshan Zhang, and Flora D. Salim. 2022. Long-term Spatio-
Temporal Forecasting via Dynamic Multiple-Graph Attention. In International
Joint Conference on Artificial Intelligence (IJCAI). 2225–2232.
[29] Sean J. Taylor and Benjamin Letham. 2017. Forecasting at Scale. PeerJ Prepr. 5
(2017), e3190.
[30] Alasdair Tran, Alexander Patrick Mathews, Cheng Soon Ong, and Lexing Xie.
2021. Radflow: A Recurrent, Aggregated, and Decomposable Model for Networks
of Time Series. In ACM The Web Conference (WWW). 730–742.
[31] Aaron Voelker, Ivana Kajic, and Chris Eliasmith. 2019. Legendre Memory Units:
Continuous-Time Representation in Recurrent Neural Networks. In Neural Infor-
mation Processing Systems, (NeurIPS). 15544–15553.
[32] Shiyu Wang. 2024. NeuralReconciler for Hierarchical Time Series Forecasting. In
ACM International Conference on Web Search and Data Mining (WSDM). 731–739.
[33] Shiyu Wang, Yinbo Sun, Xiaoming Shi, Shiyi Zhu, Lintao Ma, James Zhang,
Yangfei Zheng, and Liu Jian. 2023. Full Scaling Automation for Sustainable
Development of Green Data Centers. In International Joint Conference on Artificial
Intelligence, (IJCAI). 6264–6271.
[34] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma,
James Y Zhang, and JUN ZHOU. 2024. TimeMixer: Decomposable Multiscale
Mixing for Time Series Forecasting. In International Conference on Learning
Representations (ICLR).
[35] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. In The Eleventh International Conference on Learning Representations,
(ICLR).
[36] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with
Graph Neural Networks. In ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 753–763.
[37] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Con-
volutional Networks: A Deep Learning Framework for Traffic Forecasting. In
International Joint Conference on Artificial Intelligence, (IJCAI). 3634–3640.
[38] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers
Effective for Time Series Forecasting?. In Conference on Artificial Intelligence,
(AAAI). 11121–11128.
[39] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng,
and Jian Li. 2022. Less Is More: Fast Multivariate Time Series Forecasting with
Light Sampling-oriented MLP Structures. CoRR (2022).
[40] Wei Zhang, Quan Chen, Kaihua Fu, Ningxin Zheng, Zhiyi Huang, Jingwen Leng,
and Minyi Guo. 2022. Astraea: towards QoS-aware and resource-efficient multi-
stage GPU services. In ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Lausanne (ASPLOS). 570–582.
[41] Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, G. Edward Suh, and Christina
Delimitrou. 2021. Sinan: ML-based and QoS-aware resource management for
cloud microservices. In ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Virtual Event (ASPLOS). 167–181.
[42] Tian Zhou, Ziqing Ma, Xue Wang, Qingsong Wen, Liang Sun, Tao Yao, Wotao
Yin, and Rong Jin. 2022. FiLM: Frequency improved Legendre Memory Model
for Long-term Time Series Forecasting. In Neural Information Processing Systems
(NeurIPS).
[43] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-
term Series Forecasting. In International Conference on Machine Learning, (ICML).
27268–27286.
5530Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
A Notation Table
Table 2: Symbols and Notations
Symb
ols Definitions
𝑑 Dimension
of microservice representation.
𝑀,𝑇 Number of microservices and time stamps.
𝑿(𝑊)Workload series.
𝑋(𝑊)
𝑚,𝑡Workload of microservice 𝑚at time stamp 𝑡.
𝑿𝑇+1 The real workload of 𝑀microservices at time stamps 𝑇+1.
˜𝑿𝑇+1 The predicted workload for 𝑀microservices at time stamps 𝑇+1.
𝑹𝑻,𝑼,𝑶 Response time, CPU utilization rate, and number of VMs
𝑯(𝑆)Hidden
representation after similarity encoding
𝑯(𝑀)Hidden representation after system state modeling.
𝑯(𝑇)Hidden representation after temporal pattern encoding.
𝚿(𝑆)W
orkload-only representation after graph convolution.
𝚿(𝑇)Workload-only representation after temporal convolution.
B Baseline Description
The first group is conventional time series prediction method:
ARIMA : A classical time series prediction model. Prophet: A
procedure for forecasting time series data based on additive model.
The second group is time series forecasting methods, which are
the most widely used microservices workload prediction methods:
FEDformer : This method combines Transformer architecture
with the seasonal-trend decomposition method. The decomposition
method captures the global profile of time series, while Transform-
ers capture more detailed structures. FiLM: This method appliesLegendre Polynomials [ 31] projections to approximate historical
information, uses Fourier projection to remove noise, and adds a
low-rank approximation to speed up computation. LightTS: This
method is a light deep learning architecture merely based on sim-
ple MLP-based structures. DLinear: A linear model for the long-
term time series forecasting problem. Pyraformer: This method
utilizes pyramidal attention to effectively capture both short and
long-term temporal dependencies with low time and space com-
plexity. PatchTST: A Transformer-based model designed for mul-
tivariate time series forecasting and self-supervised representation
learning. N-Transformer : Non-stationary Transformers, a generic
Transformer-based framework that can increase series stationarity
to boost data predictability and model predictive capability.
The third group is spatio temporal graph networks: STGCN :
This approach employs graph convolution and gated causal convo-
lution to encode spatio temporal information without relying on
recurrent neural network. MTGNN: This method learns implicit
graph adjacency matrix and uses GNN techniques along with dila-
tion convolution to capture spatio temporal dependencies. ST-HSL :
This method leverages both local and global views to extract tempo-
ral and spatial information regarding the workload of microservices
through contrastive learning. ST-SSL : A self-supervised method
which is built over an integrated module with temporal and spatial
convolutions for encoding the information across space and time.
In the context where microservices do not exhibit an explicit topo-
logical structure and certain spatio-temporal graph neural network
(STGNN) methods are not equipped to inherently infer the rela-
tionships between nodes from data, our approach is to specifically
provide these STGNN models with an adjacency matrix.
5531