Resilient k-Clustering
Sara Ahmadian∗
sahmadian@google.com
Google
Seattle, USAMohammadHossein Bateni
bateni@google.com
Google
New York, USAHossein Esfandiari
esfandiari@google.com
Google
New York, USA
Silvio Lattanzi
silviol@google.com
Google
Barcelona, SpainMorteza Monemizadeh
m.monemizadeh@tue.nl
Department of Mathematics and
Computer Science, TU Eindhoven
NetherlandsAshkan Norouzi-Fard
ashkannorouzi@google.com
Google
Zurich, Switzerland
ABSTRACT
We study the problem of resilient clustering in the metric setting
where one is interested in designing algorithms that return high
quality solutions that preserve the clustering structure under pertur-
bations of the input points. Our first contribution is to introduce a
formal notion of algorithmic resiliency for clustering problems that,
roughly speaking, requires an algorithm to have similar outputs
on close inputs. Then, we notice that classic algorithms have weak
resiliency guarantees and develop new algorithms for fundamen-
tal clustering problems such as 𝑘-center,𝑘-median, and 𝑘-means.
Finally, we complement our results with an experimental analysis
showing the effectiveness of our techniques on real-world instances.
CCS CONCEPTS
•Theory of computation →Approximation algorithms anal-
ysis; Facility location and clustering.
KEYWORDS
Robust clustering, 𝑘-clustering, Hierarchical clustering, Minimum
Spanning Tree
ACM Reference Format:
Sara Ahmadian, MohammadHossein Bateni, Hossein Esfandiari, Silvio Lat-
tanzi, Morteza Monemizadeh, and Ashkan Norouzi-Fard. 2024. Resilient k-
Clustering. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3637528.3671888
1 INTRODUCTION
Clustering is a central problem in machine learning with applica-
tions in several fields such as data mining [ 2,25], social network
analysis [ 5,19,40,41], biology [ 22,42] and many more. In cluster-
ing, we are interested in grouping together “similar” objects and
∗All authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671888separating dissimilar ones. In this paper, we focus on a classic met-
ric clustering problem—the 𝑘-center problem [ 23,26,28,30,31]. In
addition, we show how our results can be extended to 𝑙𝑝-clustering
for general𝑝≥1which includes the 𝑘-median problem ( 𝑝=1) and
the𝑘-means problem ( 𝑝=2).
In𝑘-center, one is given as input a set of points 𝑃in a metric space
𝑀(𝑃,𝜇)with distance function 𝜇:𝑃×𝑃→R+, and an integer 𝑘≥1.
The goal is to compute a subset 𝐶⊆𝑃of size|𝐶|≤𝑘to minimize
cost(𝐶):=max𝑥∈𝑃𝜇(𝑥,𝐶), where𝜇(𝑥,𝑆):=min𝑦∈𝑆𝜇(𝑥,𝑦).
Thanks to its simplicity and practical relevance, 𝑘-center has
been extensively studied, and efficient algorithms are known for it
in the classic setting [ 26] as well as in distributed [ 34] and streaming
settings [ 11]. Nevertheless, the solution returned by such algorithms
are often very sensitive even to small input perturbations.
For example, consider the classic farthest point traversal algo-
rithm for the 𝑘-center problem [ 26] which is a tight 2-approximate
algorithm under the assumption that 𝑃≠𝑁𝑃. In this algorithm,
the first center in the solution is chosen arbitrarily; then, subse-
quent centers are chosen one by one by selecting the point that
maximizes the distance to the current set of centers. Now imagine
applying the farthest-point traversal algorithm to perturbed points
on a unit sphere. It is easy to come of with simple examples1that
different perturbations (even if tiny) would lead to the generation
of completely different clustering solutions. Interestingly, this is
not just an issue in synthetic instances. In fact, in our experiments,
we show that classic algorithms are often extremely unstable even
on real-world instances.
This phenomenon is particularly worrisome in practice where
data often contains some noise and changes over time. For example,
consider embeddings capturing users’ behavior in an online plat-
form. It is natural to expect that such embeddings would change
slightly on a daily basis. In this setting, an ideal clustering algo-
rithm should change the output clusters smoothly and preserve
most of the previously outputted solution. Unfortunately this is not
the case for classic algorithms. For these reasons, in this paper we
initiate the formal study of resilient algorithms for classic clustering
problems such as 𝑘-center and other 𝑘-clustering problems.
Our Results. Our first contribution is a novel notion of algorith-
mic resilience. Intuitively, in our notion, a clustering algorithm is
considered resilient if it returns similar solutions on close input
1For instance the case that all the pairs of points have the same distance and then the
distances are slightly changed at random.
 
29
KDD ’24, August 25–29, 2024, Barcelona, Spain. Sara Ahmadian et al.
instances. Our notion of closeness between two input instances
captures the fact that in close instances the pairwise distance be-
tween any two points does not change significantly (for a formal
definition, please refer to Section 2). Interestingly, this is enough to
capture many real-world scenarios, as our experiments show. For
instance, it captures the setting [ 3,21,44] where datasets evolve
gradually or smoothly over time, and one is interested in maintain-
ing a stable solution as long as the input instance does not change
too much. This is a practical setting, for example, in geo-location
applications if one considers two close and consecutive snapshots
of the dataset one would expect to have points moved just a little
bit. Another important case is when the position of the points is ob-
tained through measurements with a small error where one would
expect two consecutive measurements of the same set of points to
be close but different. In addition, it captures the setting [ 14,15]
where the underlying input is perturbed randomly or adversarially,
and one is interested in preserving the underlying clusters.
Leveraging this new definition, we design and analyze new
resilient algorithms for the 𝑘-center problem. For close input in-
stances, our algorithms return solutions that (1) are similar and
(2) provide good approximation of the optimal solutions for the
corresponding instances. Moreover, our algorithm is efficient: in
fact it runs in time ˜𝑂(𝑛𝑘)almost matching the running time of
the classic farthest-point traversal algorithm.2See Theorem 3.1 for
the formal statement of our result for the 𝑘-center problems. We
also present extensions of these algorithms for the ℓ𝑝clustering
problems (see Theorem 4.1).
Finally, we provide experiments showing that our algorithms
return clustering solutions of approximately good quality while
at the same time they are significantly more resilient to small per-
turbation in the input than classical clustering algorithms that we
utilize in our benchmarks.
1.1 Related work
Sensitivity and average sensitivity. Our notion of resiliency is
related to the notions of sensitivity and average sensitivity that have
been extensively studied for dynamic programming, clustering,
graph and learning problems [ 32,36,45–47]. In fact, both notions
focus on developing algorithms that are resilient over perturbations
of the input space. Nevertheless in previous work the focus is on
the impact of removing or adding a single element to the input
instance, while in our setting we allow allthe points to change
their positions by a fixed amount. More closely, [ 37] introduces a
notion closer to ours (Lipschitz continuity) for graph problems and
provide several algorithms in their setting. However, this notion
is specialized for graph problems and it is not easily extendable to
our clustering setting.
Perturbation resilience. The concept of(𝛼,𝜖)-perturbation re-
silience evolved through a series of papers, including [ 1,6,8,12].
Interestingly, the concept of 𝛼-perturbation in these works is simi-
lar to(1+𝜖)-closeness defined in our paper. However, the notion
of resilience in these works differs from the resilience concept we
consider. In particular, the (𝛼,𝜖)-perturbation resilience requires
that all but an 𝜖fraction of points have the same clustering for any
2The notation ˜𝑂(·)hides logarithmic factors.two instances that satisfy 𝛼-perturbation. For such instance, they
could provide clustering algorithms. In contrast, we develop clus-
tering algorithms that are considered resilient if they return similar
solutions on close input instances. This means that the same algo-
rithm (with the same random bits used) returns the same clustering
for all but the 𝜖fraction of points.
Consistent clustering. Another close area of research that re-
ceived a lot of attention in recent years is consistent clustering [ 10,
17,24,29,35,39]. In this setting, the input evolves over time, with
points joining and leaving, and the goal is to maintain a good solu-
tion with as few changes as possible. The results on this interesting
setting, though, do not provide any stability guarantees in the pres-
ence of perturbations of the input.
Differential privacy. Our work is also loosely related to research
in differential privacy [ 20]. In fact, both settings seek to design
algorithms resilient to small variations of the input instances. How-
ever, in differential privacy one wants the output of an algorithm
to change with a small probability when a single point in the in-
put is deleted. Instead in our setting, one wants the output to not
change too much when (possibly) allthe points slightly changes
their positions.
Stability and robustness. Finally, we note that in literature there
are several influential papers studying different notion of stability
or robustness for clustering [ 4,7,9,38]. These papers are quite
interesting although they have a different focus and do not provide
any guarantee on resilience to adversarial perturbation in metric
space.
Outline of the paper. In Section 2, we introduce the commonly
used notations and recall some definitions used in the main body of
the paper, and in particular, we formalize the definition of resiliency
for clustering problems. In section Section 3, we present our resilient
algorithm for 𝑘-center. We extend our results to the 𝑘-median and
the𝑘-means problems in Section 4. Finally, in Section 5 we report
the results of our empirical results.
2 PRELIMINARIES
In this section, we introduce common notations and formally define
the notion of resiliency. We present Chernoff concentration bounds
used in our proofs as well.
Clustering. A metric space 𝑀(𝑃,𝜇𝑃)is defined on a point set
𝑃with a distance function 𝜇𝑃:𝑃×𝑃↦→R≥0, which is symmetric,
satisfies triangle inequality, and is zero if and only if the two points
are the same. We use 𝜇instead of𝜇𝑃, when𝑃is clear from the
context. For a set 𝑃′⊆𝑃and a point𝑝∈𝑃, we extend the notation
𝜇(𝑝,𝑃′)=min𝑞∈𝑃′𝜇(𝑝,𝑞). A𝑘-clusteringC:𝑃↦→𝐶of a point
set𝑃is defined by a set 𝐶⊆𝑃of𝑘centers, which assigns each
point𝑝∈𝑃to a centerC(𝑝)∈𝐶. It will be more convenient in the
definitions of resiliency to use an alternative representation of the
clustering as a set of pairs {(𝑝1,C(𝑝1)),(𝑝2,C(𝑝2)),...}.
Theℓ𝑝clustering problems are 𝑘-clustering problems where
the goal is to minimize the cost of the returned solution C(for𝑃)
defined asÍ
𝑞∈𝑃𝜇(𝑞,𝐶)𝑝. This family of problems includes well-
known clustering problems such as (i) 𝑘-center for sufficiently large
 
30Resilient k-Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain.
𝑝, i.e., with cost max𝑞∈𝑃𝜇(𝑞,C(𝑞)), (ii)𝑘-median for 𝑝=1, and (iii)
𝑘-means for𝑝=2.
LetAbe an algorithm solving a 𝑘-clustering problem on the set
𝑃. Then we denote with A(𝑃,𝑘)its output clustering in the set of
pairs notation{(𝑝1,C(𝑝1)),(𝑝2,C(𝑝2)),...}.
Resilient Clustering. Let us start by giving an intuitive definition
and then provide an identical formal definition of resilient cluster-
ing. Assume that each point 𝑝∈𝑃has an identifier which is also
denoted by𝑝(basically the name of the point 𝑝). Then two instances
𝑀(𝑃,𝜇)and𝑀′(𝑃′,𝜇′)(with the same set of point identifiers) are
𝜀-close if for any two points 𝑝,𝑞, their distance does not differ by
more than a factor (1+𝜀)in𝜇and𝜇′, i.e.,1
1+𝜀≤𝜇𝑃′(𝑝,𝑞)
𝜇𝑃(𝑝,𝑞)≤1+𝜀.
Later on we formalize the definition by introducing a mapping
between the points in 𝑃,𝑃′. We call an algorithm 𝛾-resilient if the
centers of at most a 𝛾𝜀fraction of the points change when the
algorithm runs on 𝑃compared to when it runs on 𝑃′. We again
formalize this using maps.
We are now ready to give a formal definition of resiliency. We
start by defining the notion of 𝜀-close point sets.
Definition 2.1. [𝜀-close point sets] Given are 𝑀(𝑃,𝜇)and𝑀′(𝑃′,𝜇′)
with a bijective relation 𝜋:𝑃↦→𝑃′. Then,𝑃and𝑃′are𝜀-close if
and only if for any pair of points 𝑝,𝑞∈𝑃such that𝜇(𝑝,𝑞)≠0we
have
1
1+𝜀≤𝜇′(𝜋(𝑝),𝜋(𝑞))
𝜇(𝑝,𝑞)≤1+𝜀,
and if𝜇(𝑝,𝑞)=0, then𝜇′(𝜋(𝑝),𝜋(𝑞))=0.
Let[𝑘]={1,...,𝑘}for any natural number 𝑘. For two sets 𝐴,𝐵
we define the symmetric difference 𝐴△𝐵=(𝐴\𝐵)∪(𝐵\𝐴)=
(𝐴∪𝐵)\(𝐴∩𝐵). With slight abuse of notation, for any mapping
𝑓:𝐴↦→𝐵(in particular, the bijective mapping 𝜋in the above
definition), we use shorthands 𝑓((𝑎1,𝑎2))=(𝑓(𝑎1),𝑓(𝑎2))and
𝑓({𝑎1,𝑎2,...})={𝑓(𝑎1),𝑓(𝑎2),...}where∀𝑖:𝑎𝑖∈𝐴. Similarly
we define𝑓(𝐺)for a graph𝐺(𝐴,𝐸,𝑤)3to denote a graph obtained
from𝐺by mapping all vertices according to 𝑓. Now we can define
𝛾-resiliency.
Definition 2.2 (𝛾-resiliency for clustering) .AlgorithmAis𝛾-
resilient if and only if the outputs of A(𝑃,𝑘)andA(𝑃′,𝑘)differ
by at most𝛾𝜀fraction, i.e.,
E[|𝜋(A(𝑃,𝑘))△A(𝑃′,𝑘)|]≤𝛾𝜖·|𝑃|
for any𝜀-close point sets 𝑃and𝑃′with the corresponding bijective
mapping𝜋:𝑃↦→𝑃′.
Note that our notion of resilience is characterized by two quan-
tities(𝜀,𝛾). In this paper we think about 𝛾∈𝑂(1). It is worth
mentioning that 𝜀does not have to be small for our results to hold
and be meaningful.4
A nice property of our definition is that it directly relates the
distance between the two inputs with the symmetric difference
between the solutions. So it naturally captures the resilience of the
algorithm to changes in the input. w
Approximation Algorithms. The approximation factor of an
algorithm is defined as the maximum ratio (over all possible inputs)
3𝐴represents the set of the vertices of the graph, 𝐸represents the set of the edges of
the graph and 𝑤represents the edge weight function.
4In fact, our results are non-trivial even for constant 𝜀.of the cost of the algorithm to the cost of the optimal solution. An
algorithm with approximation factor 𝛼is called an𝛼-approximation
(algorithm). This will be (in the case of 𝑘-clustering) a bicriteria
algorithm if the output solutions may use more than the prescribed
𝑘centers. We call this a bicriteria (𝛼,𝛽)-approximation algorithm
if the cost is guaranteed to be within a factor 𝛼of the optimum and
the number of centers is guaranteed to be at most 𝛽𝑘.
Randomized algorithms. A randomized algorithm is one that
uses a source of randomness as part of its logic. However, we would
like to emphasize that in our results we share random bits between
different executions of the algorithm. For example, a subset of 𝑘
random points from a point set 𝑃is guaranteed to be the same
over various runs. This is in fact required to prove any resiliency
guarantee for any randomized algorithm over different inputs.
Graphs. Let𝐺=(𝑉,𝐸,𝑤)be a graph on vertex set 𝑉of size𝑛,
edge set𝐸with edge weights 𝑤:𝐸↦→R≥0. Let Γ𝐺(𝑣)denote the
set of all edges in 𝐺incident on vertex 𝑣. For𝐸′⊆𝐸, let𝑤(𝐸′)=Í
𝑒∈𝐸′𝑤(𝑒). A minimum spanning tree (MST) is a subset of the
edges𝐸′⊆𝐸such that𝐺=(𝑉,𝐸′)is acyclic and connected and
such that𝑤(𝐸′)is minimized.
LetAbe an algorithm solving the MST problem on the graph 𝐺=
(𝑉,𝐸,𝑤), then we denote with A(𝐺)its output MST represented
by the set of edges in the solution.
Chernoff concentration bound [13] Let𝑋=Í
𝑖∈[𝑁]𝑋𝑖, where
𝑋𝑖=1with probability 𝑝𝑖and𝑋𝑖=0with probability 1−𝑝𝑖,
and all𝑋𝑖’s are independent. Let 𝜇=E[𝑋]=Í
𝑖∈[𝑁]𝑝𝑖. Then
Pr[𝑋≥(1+𝜉)𝜇]≤exp(−𝜉2·𝜇/3)for𝜉>0.
3 RESILIENT 𝑘-CENTER ALGORITHM
In this section, we present our resilient algorithm for the 𝑘-center
problem. In this problem, we are given a point set 𝑃⊂R𝑑and
a parameter 𝑘∈Nand the goal is to return a set 𝐶⊆𝑃of𝑘
centers and an assignment of points in 𝑃to the centers 𝐶so that
the maximum distance between a point and its assigned center is
minimized.
High-level idea of the algorithm The algorithm works in two
phases. In the first phase we select a large enough set of random
points to act as an initial set of centers. Then we carefully assign
points that are “close” to the initial set of centers such that the
assignment is stable to perturbations in the input space. To achieve
this, we introduce a novel assignment algorithm which can be of
independent interest. In the second phase, we cluster the unassigned
points using the classic farthest-point traversal algorithm. The key
observation is that by selecting enough random points in the first
phase, we are left with very few unassigned points in the second
phase, so we can apply any clustering algorithm on them without
affecting the resilience guarantee too much.
More precisely, we first sample a set 𝐶⊆𝑃of size|𝐶|=2𝑘log(1/𝜀)
from𝑃. We then construct an auxiliary graph 𝐺=(𝑃,𝐸), where
(i) edges inside 𝐶have weight zero, and (ii) edges between 𝐶, and
𝑃\𝐶have weight equal to the distance between the two points. We
next invoke Resilient-MST (a novel resilient algorithm for MST
introduced in this work) on the auxiliary graph 𝐺(𝑃,𝐸), and argue
that the produced tree 𝑇ofResilient-MST has a special structure,
where non-selected nodes 𝑃\𝐶are directly connected to centers in
𝐶. In addition, resiliency of the Resilient-MST routine implies that
 
31KDD ’24, August 25–29, 2024, Barcelona, Spain. Sara Ahmadian et al.
the majority of nodes have the same assignment over two close in-
stances. However, to guarantee constant factor approximation, we
have to take care of assignments which use edges of large weight,
as they impose a high cost on the solution. To this end, we pick 𝑘
additional centers 𝐶′(via a non-resilient algorithm) to serve the
node set𝐿⊆𝑃\𝐶incident on the 𝜀𝑛heaviest edges of 𝑇. The final
solution consists of centers 𝐶∪𝐶′, and assigns each point 𝑝∈𝐿via
𝐶′and each point 𝑝∈𝑃\𝐿via𝐶. See Algorithm 1 and Algorithm 2
for details.
Algorithm 1 Resilient-𝑘-Center
Input: Metric𝑀=(𝑃,𝜇), and parameters 𝑘∈N,𝜀>0.
Output: Assignment set{(𝑝,𝜎(𝑝))for𝑝∈𝑃}.
1:𝐶←a set of 2𝑘log(1/𝜀)random nodes in 𝑃.
2:𝐸←{(𝑝,𝑞)|𝑝,𝑞∈𝑃and|{𝑝,𝑞}∩𝐶|≠0}.
3:𝑤(𝑝,𝑞)← 0if𝑝,𝑞∈𝐶.
4:𝑤(𝑝,𝑞)←𝜇(𝑝,𝑞)otherwise.
5:𝐻←weighted graph(𝑃,𝐸,𝑤).
6:𝑇←Resilient-MST(𝐻).
7:𝜎(𝑝)←𝑝for all𝑝∈𝐶.
8:𝜎(𝑝)←𝑐if𝑝∈𝑃\𝐶and(𝑝,𝑐)∈𝑇.
9:𝐿←vertices in𝑃\𝐶incident to the 𝜀𝑛heaviest edges of 𝑇.
10:𝐶′←centers selected by [27] on 𝑃with𝑘.
11:𝜎(𝑝)←𝑐′for𝑝∈𝐿, where𝑐′is the closest center of 𝐶′to𝑝.
12:return{(𝑝,𝜎(𝑝))for𝑝∈𝑃}.
Algorithm 2 Resilient-MST
Input: Graph𝐺(𝑉,𝐸,𝑤), bucketizing parameter 𝜆.
Output: A spanning tree 𝑇.
1:foreach edge𝑒∈𝐸do
2:𝛼←random number in [0,1)
3: if𝑤(𝑒)=0then
4:𝑤′(𝑒)←𝑤(𝑒)
5: else
6:𝑖←⌈𝛼+log𝜆𝑤(𝑒)⌉
7:𝑤′(𝑒)←𝜆𝑖−𝛼
8:𝑇←Kruskal(𝑉,𝐸,𝑤′)⊲apply consistent tie-breaking rule
9:return𝑇
The main result of this section is the following.
Theorem 3.1. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛, Algorithm 1 is a 7.78-
resilient algorithm for 𝑘-center with bicriteria approximation factor
4,1+2 log1
𝜀
with probability at least 1−𝑂
1
𝑛
. Furthermore the
running time of the algorithm is 𝑂(𝑛𝑘log𝑛log(1/𝜀)).
Analysis. In the rest of this section, we prove the correctness
of the above algorithm. The first step of our analysis is to analyze
resilience properties of the Resilient-MST sub-routine. Before
doing that we introduce the definition of 𝜀-close and resiliency also
for the MST problem.
Definition 3.2. [𝜀-close graphs] Two weighted graphs 𝐺(𝑉,𝐸,𝑤𝐺)
and𝐻(𝑉,𝐸,𝑤𝐻)are called𝜀-close if and only if for each edge 𝑒∈𝐸,if𝑤𝐺(𝑒)≠0, then
1
1+𝜀<𝑤𝐻(𝑒)
𝑤𝐺(𝑒)<1+𝜀,
and if𝑤𝐺(𝑒)=0, then𝑤𝐻(𝑒)=0.
Definition 3.3. [𝛾-resilient for MST] Algorithm Ais an𝛾-resilient
MST algorithm if and only if the outputs A(𝐺)andA(𝐻)differ by
at most a𝛾𝜀-fraction, i.e.,
|A(𝐺)△A(𝐻)|≤𝛾𝜖|A(𝐺)|,
for any𝜀-close graphs 𝐺(𝑉,𝐸,𝑤𝐺)and𝐻(𝑉,𝐸,𝑤𝐻).
We are now ready to state the main properties of Resilient-MST
whose proof is deferred to Section 3.1.
Theorem 3.4. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛and𝜆>1+𝜀, with
probability 1−𝑂(1/𝑛),Resilient-MST is a(4(1+𝑜(1))/ln𝜆)-resilient
𝜆-approximation minimum spanning tree algorithm. In addition,
(1)min𝑒∈Γ𝑇(𝑣)𝑤(𝑒)≤𝜆min𝑒∈Γ𝐺(𝑣)𝑤(𝑒)for any𝑣∈𝑉; and
(2)any two vertices with a zero-cost path in 𝐺have a zero-cost
path in𝑇.
Our main result (Theorem 3.1) then follows from combining two
key lemmas (Lemma 3.5, Lemma 3.6) and two simple observations.
We start by arguing that all the points in 𝑃are assigned to a
center in𝐶∪𝐶′. Notice that all 𝑝have an edge in the MST. Also
observe that for any edge at least one of the end points is a point
in𝐶, therefore each point is assigned to a center in 𝐶based on the
solution of the MST and it can be only changed to a center in 𝐶′
later on. Therefore the returned assignment is indeed feasible.
Next we bound the running time of our algorithm by 𝑂(𝑛𝑘log1/𝜀).
In fact, the set 𝐶can be easily sampled in time 𝑂(𝑛log𝑛). The
auxiliary graph 𝐻has at most 𝑂(𝑛𝑘log1/𝜀)edges, so Resilient-
MST has running time 𝑂(𝑛𝑘log𝑛log1/𝜀). Finally the algorithm
in [27] has running time 𝑂(𝑛𝑘). So the overall running time is
𝑂(𝑛𝑘log𝑛log 1/𝜀).
We are now ready to prove approximation and resiliency guar-
antees of our algorithm.
Lemma 3.5. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛, Algorithm 1 is a bi-
criteria
4,1+2 log1
𝜀
-approximation for 𝑘-center with probability
1−𝑂
1
𝑛
.
Proof. First note that the set of open centers is 𝐶∪𝐶′which has
size𝑘+2𝑘log(1/𝜀)=𝑘(1+2log(1/𝜀)). So it remains to establish the
approximation guarantee. Let 𝑟OPT5be the optimum solution to the
input. Clearly 𝜇(𝑝,𝜎(𝑝))≤ 2𝑟OPTfor𝑝∈𝐿since this assignment
is an output of a 2-approximation algorithm on the input points.
To bound the radius for the points in 𝑃\𝐿and conclude the proof,
we argue that the 𝑛(1−𝜀)lightest edges of 𝑇are at distance 2𝑟OPT
from𝐶w.h.p. The key observation is that large clusters contain
with high probability at least a sampled point in 𝐶and that any two
points belonging to the same cluster are at distance at most 2𝑟OPT
from each other. Formally, let 𝑈⊆𝑃be the subset of points 𝑝such
5We use𝑟𝑂𝑃𝑇 as both the optimum solution and its value when it is clear from the
context.
 
32Resilient k-Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain.
that𝜇(𝑝,𝐶)>2𝑟OPTand let𝐶∗
1,𝐶∗
2,...,𝐶∗
𝑘be the set of clusters of
an optimum solution.
Let𝑌𝑖for𝑖∈[𝑛]be a binary random variable that has value 1 if
and only if the the i-th point in 𝑃is at distance larger 2𝑟OPTfrom
𝐶and let𝑋𝑖for𝑖∈[𝑘]be a binary random variable that has value
1 if and only if 𝐶∗
𝑖∩𝐶=∅. We bound the expected size of 𝑈as
follows.
E[|𝑈|]=E"𝑛Õ
𝑖=1𝑌𝑖#
≤E"𝑘Õ
𝑖=1𝑋𝑖|𝐶∗
𝑖|#
≤𝑘Õ
𝑖=1
1−|𝐶∗
𝑖|
𝑛|𝐶|
|𝐶∗
𝑖|
=𝑛𝑘Õ
𝑖=1
1−|𝐶∗
𝑖|
𝑛|𝐶||𝐶∗
𝑖|
𝑛≤𝑛exp(−2𝑘log(1/𝜀)/𝑘)=𝜀2𝑛,
where the second equality follows from the fact that each point
in𝐶is chosen uniformly at random and 𝑋𝑖=1with probability 1−|𝐶∗
𝑖|/𝑛|𝐶|, and the last inequality follows from Claim B.1 (see
Appendix B), the fact that {𝐶∗
𝑖}𝑖is a partition of 𝑃and that|𝐶|>0.
Using Chernoff bounds and the fact that 𝜀>log𝑛
𝑛, we then get that
|𝑈|≤𝜀𝑛with probability at least 1−1/𝑛.
So it follows that with probability at least 1−1/𝑛, the points
in𝑃\𝐿have a center in 𝐶at distance no more than 2𝑟OPT. Then
by Theorem 3.4 and by fixing 𝜆=2, we get that Resilient-MST
guarantees that each vertex in 𝑃\𝐿has an edge of weight at most
4𝑟OPT. □
Next we study the resiliency of Algorithm 1.
Lemma 3.6. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛, Algorithm 1 is 7.78-resilient
for𝑘-center with probability 1−𝑂
1
𝑛
.
Proof. Let superscripts(1)and(2)specify inputs, internal ar-
tifacts and outputs of the two runs on different data sets that are
𝜀-close. When these artifacts are the same, we may drop the super-
script. Note that the algorithm uses the same source of randomness,
so𝜋(𝐶(1))=𝐶(2). The edge weights 𝑤(1)and𝑤(2)are defined
using the distance functions 𝜇(1)and𝜇(2)on point sets 𝑃(1)and
𝑃(2)=𝜋(𝑃(1)), assuming𝜋is the corresponding bijection between
the two input instances. Nevertheless, the closeness assumption on
the inputs guarantees that 1/(1+𝜀)≤𝑤(2)(𝜋(𝑝),𝜋(𝑞))/𝑤(1)(𝑝,𝑞)≤
(1+𝜀), hence the two graphs 𝜋(𝐻(1))and𝐻(2)are𝜀-close.
We assign points to centers in two parts in Algorithm 1. One
based on the resilient tree algorithm in lines 7,8 of Algorithm 1.
Theorem 3.4 (fixing 𝜆=2) guarantees that the resulting trees
𝜋(𝑇(1))and𝑇(2)are5.78𝜀-close. That is, they may differ in up to
5.78𝜀𝑛edges. Therefore we introduce at most 5.78𝜖𝑛points assigned
to a different center. Moreover the second place that we assign
points to centers is in line 11 of Algorithm 1. Using the fact that
with probability 1−1
𝑛, the size of the sets 𝐿(1)and𝐿(2)are smaller
than𝜀𝑛, we have that even if all assignments on 𝜋(𝐿(1))and𝐿(2)
are different, there are at most 7.78𝜀𝑛different assignments with
probability 1−3
𝑛. □
3.1 Resilient Minimum Spanning Tree
In this section, we focus on our results for the MST problem, and
show a resilient algorithm for it that will be used as subroutine to
compute our resilient clustering algorithm.We first give a high-level idea of our approximation algorithm
for MST and the pseudocode of our algorithm then we analyze our
algorithm and show how it achieves resiliency.
High-level idea of the algorithm The main idea behind our
MST algorithm (namely, Resilient-MST ) is a discretization trick. In-
tuitively when the weights of two edges are not far from each other,
we can think of them as having the same weight in order to be re-
silient to small changes of their weights across the 𝜀-close instances.
More formally, our algorithm modifies weight 𝑤(𝑒)of each edge
into some𝑤′(𝑒)≥𝑤(𝑒), and then invokes any MST algorithm (with
consistent tie-breaking) on the new graph. Specifically, we pick a
parameter 0≤𝛼<1for each edge of positive weight uniformly
at random, independently of other edges, and set 𝑤′(𝑒)=𝜆𝑖−𝛼for
the smallest integer 𝑖that guarantees 𝑤′(𝑒)≥𝑤(𝑒). If𝑤(𝑒)=0,
we set𝑤′(𝑒)=𝑤(𝑒)=0. See Algorithm 2 for the pseudo-code.
Our analysis of this algorithm leads to Theorem 3.4, which is also
restated here.
Theorem 3.4. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛and𝜆>1+𝜀, with
probability 1−𝑂(1/𝑛),Resilient-MST is a(4(1+𝑜(1))/ln𝜆)-resilient
𝜆-approximation minimum spanning tree algorithm. In addition,
(1)min𝑒∈Γ𝑇(𝑣)𝑤(𝑒)≤𝜆min𝑒∈Γ𝐺(𝑣)𝑤(𝑒)for any𝑣∈𝑉; and
(2)any two vertices with a zero-cost path in 𝐺have a zero-cost
path in𝑇.
Analysis. The analysis consists of several pieces. First, we show
that the output of Resilient-MST has the desired approximation
guarantee. Then we show that Kruskal’s algorithm run on two
instances differing on only one edge’s weight produces trees which
differ in at most two edges. Finally, we argue that for 𝜀-close in-
stances with weights 𝑤(1)and𝑤(2), there are at most 𝑂(𝜀𝑛)edges
with different discretized weights 𝑤′(1)≠𝑤′(2)across the runs
with high probability. Combining these propositions yields the
statement of the Theorem 3.4. We start with the following lemma.
Lemma 3.7. Let𝑇be the output of Resilient-MST on graph𝐺, then
𝑇is an𝜆-approximate MST. Moreover,
(1)min𝑒∈Γ𝑇(𝑣)𝑤′(𝑒)≤𝜆min𝑒∈Γ𝐺(𝑣)𝑤(𝑒)for any𝑣∈𝑉; and
(2)any two vertices with a zero-cost path in 𝐺have a zero-cost
path in𝑇.
Proof. No edge weight decreases in the discretization process,
and each edge’s weight increase by at most a factor 𝜆, hence the
optimum solution changes by less than a factor 𝜆. One such solution
is found by Kruskal’s algorithm.
Recall that Kruskal’s algorithm iterates over edges in increasing
order of weight, and picks each edge if and only if it does not form
a cycle with the previously picked edges. Fix a vertex 𝑣, and let𝑒𝑣
be the first edge incident on 𝑣encountered by Kruskal’s algorithm
on the graph with modified weight. This edge is picked because it
does not form a cycle. Let 𝑒∗=arg min𝑒′∈Γ𝐺(𝑣)𝑤(𝑒′)be the lowest-
weight edge incident on 𝑣in𝐺. The sort order of Kruskal implies
that𝑤′(𝑒𝑣)≤𝑤′(𝑒∗)<𝜆𝑤(𝑒∗).
Finally note that edges of weight zero do not change their weight
during discretization, and the other edges have positive weight. So
weight-zero edges are considered first by Kruskal’s algorithm. Thus,
any vertices having a path of zero cost in the input graph will have
a path of cost zero in the output tree as well. □
 
33KDD ’24, August 25–29, 2024, Barcelona, Spain. Sara Ahmadian et al.
Next, we see how Resilient-MST handles two graphs that only
differ over the weight of one edge. We basically show that Resilient-
MST produces solutions that may differ by at most two edges.
Lemma 3.8. Let𝐺(𝑉,𝐸,𝑤𝐺)and𝐻(𝑉,𝐸,𝑤𝐻)be two graphs where
𝑤𝐻and𝑤𝐺only disagree on the weight of an edge 𝑒∗∈𝐸and let𝑇𝐺
and𝑇𝐻be the outputs of Resilient-MST on𝐺and𝐻, respectively.
Then|𝑇𝐺△𝑇𝐻|≤2.
Proof. Assume without loss of generality that 𝑤𝐺(𝑒∗)<𝑤𝐻(𝑒∗).
The statement is trivial when 𝑒∗∉𝑇𝐺or when𝑒∗∈𝑇𝐻. Now sup-
pose𝑒∗∈𝑇𝐺and𝑒∗∉𝑇𝐻.
With consistent tie-breaking, we can assume that edge weights
are distinct, which implies uniqueness of MST for any subset of
vertices, so we can talk about theMST or thelightest edge in a set.
It is well-known (see, e.g., Theorem 23.1 in [ 18]) that the lightest
edge in any cut6of the graph belongs to the MST. In fact, the generic
algorithmic introduced in [ 18] iteratively finds and contracts7an
edge that belongs to the MST.
Let𝑒∗=(𝑢,𝑣), and let𝑉1,𝑉2⊆𝑉be the set of vertices reachable
in𝑇𝐺\{𝑒∗}from𝑢and𝑣, respectively. Clearly 𝑉1∩𝑉2=∅and
𝑉1∪𝑉2=𝑉. Let𝐺∗be the result of contracting edge 𝑒∗in𝐺and
removing self-loops. Let 𝑇∗be the unique MST of 𝐺∗. The lightest
edge in the cut(𝑉1,𝑉2)for𝐺is𝑒∗. Let𝑒+be the lightest edge in
the cut(𝑉1,𝑉2)for𝐻. Then𝑇∗∪{𝑒∗}is the unique MST of 𝐺
and𝑇∗∪{𝑒+}is the unique MST of 𝐻. They differ in exactly two
edges. □
The final ingredient for proving our main result is to show that
the number of edges with modified discretized weight is in fact
bounded due to discretization. It is simpler to do this for the case
that the input edge weights only increase when going from an input
graph𝐺(1)to another𝐺(2). In the main proof, we show how to
handle this restriction for general change.
Lemma 3.9. Let𝐺(1)and𝐺(2)be𝜀-close graphs with vertices 𝑉,
edges𝐸, and respective edge weight functions 𝑤(1)and𝑤(2)such
that𝑤(1)(𝑒)≤𝑤(2)(𝑒)for all edges 𝑒and1>𝜀>3 ln𝜆ln2𝑛
𝑛. Let
𝑤′(1),𝑤′(2)denote the corresponding discretized edge weights. With
probability 1−1/𝑛, we have
|{𝑒∈𝐸|𝑤′(1)(𝑒)≠𝑤′(2)(𝑒)}|≤𝜀𝑛(1+𝑜(1))/ln𝜆,
where𝜆is the same value defined in Theorem 3.4.
Proof of Lemma 3.9. Let𝑋𝑒be a binary random variable indi-
cating whether the discretized weight of 𝑒∈𝐸differs in the two
runs: i.e.,𝑤′(1)(𝑒)≠𝑤′(2)(𝑒). Then𝑋=Í
𝑒∈𝐸𝑋𝑒is the quantity
we mean to upperbound.
Let𝛼be the parameter we select for edge 𝑒in the algorithm. In
case of𝑋𝑒=1, we have,
𝑤(1)(𝑒)≤𝜆𝑖−𝛼<𝑤(2)(𝑒)<(1+𝜀)𝑤(1)(𝑒)
for some integer 𝑖. Take logarithms and substitute 𝐿𝑤=log𝜆𝑤(1)(𝑒)
and𝐿𝜀=log𝜆(1+𝜀)to get
𝐿𝑤≤𝑖−𝛼<𝐿𝑤+𝐿𝜀. (1)
6The cut(𝑈,𝑉\𝑈)in𝐺consists of all edges of 𝐺with exactly one endpoint in 𝑈.
7Contracting an edge (𝑢,𝑣)in𝐺(𝑉,𝐸)and removing the self-loops results in a graph
𝐻(𝑉\{𝑢,𝑣}∪{𝑣∗},𝐸′)where𝐸′={(𝑢′,𝑣′)∈𝐸|𝑢′,𝑣′∉{𝑢,𝑣}}∪{(𝑣∗,𝑣′)|
(𝑢′,𝑣′)∈𝐸,𝑢′∈{𝑢,𝑣},𝑣′∉{𝑢,𝑣}}.In other words, 𝑖−𝛼∈[𝐿𝑤,𝐿𝑤+𝐿𝜀)for some integer 𝑖. Let𝑗=⌈𝐿𝑤⌉.
We consider two cases.
(1)If𝐿𝑤+𝐿𝜀≤𝑗, Inequality (1)implies𝛼∈(𝑗−𝐿𝑤−𝐿𝜀,𝑗−𝐿𝑤].
(2)If𝑗<𝐿𝑤+𝐿𝜀≤𝑗+1, Inequality (1)implies𝛼∈[0,𝑗−𝐿𝑤]
or𝛼∈(𝑗+1−𝐿𝑤−𝐿𝜀,1].
In both cases, the allowable range of 𝛼for𝑋𝑒=1has measure 𝐿𝜀.
Since𝛼takes uniform random values from [0,1), we get Pr[𝑋𝑒=
1]≤𝐿𝜀=log𝜆(1+𝜀)<𝜀/ln𝜆, where the last inequality follows
from the Maclaurin series of ln(1+𝑥)and noting𝜀<1.
Summing up yields E[𝑋]<𝜀𝑛/ln𝜆. Apply the Chernoff bound
and setting𝜉=q
3 ln𝑛ln𝜆
𝜀𝑛gives
Pr[𝑋>(1+𝑜(1))𝜀𝑛/ln𝜆]≤1−1/𝑛
if𝜀𝑛≥3 ln𝜆ln2𝑛.
We remark that if we picked one fixed 𝛼for all edges 𝑒, we could
not apply the Chernoff bound in the above argument, though the
expected number of changed edge weights would have the desired
bound. □
We are now ready to prove our main theorem.
Proof of Theorem 3.4. Let𝐺(0)=(𝑉,𝐸,𝑤(0))be a graph where
𝑤(0)(𝑒)=max{𝑤(1)(𝑒),𝑤(2)(𝑒)}. Let𝑤′(0),𝑤′(1),𝑤′(2)denote
the respective discretized weights of 𝑤(0),𝑤(1),𝑤(2). Two applica-
tions of Lemma 3.9 (once to the pair 𝐺(1),𝐺(0)and another time to
the pair𝐺(2),𝐺(0)) proves that, with high probability, 𝑤′(1),𝑤′(2)
disagree in 𝑧≤2𝜀𝑛(1+𝑜(1))/ln𝜆edges. We can then create a
sequence of graphs 𝐻0,𝐻1,...,𝐻𝑧on the same vertex and edge set
as𝐺(1)such that𝐻𝑖,𝐻𝑖+1disagree in exactly one edge weight, for
0≤𝑖<𝑧;𝐻0uses the weights 𝑤′(1); and𝐻𝑧uses the weights
𝑤′(2). With𝑧applications of Lemma 3.8, we conclude that the
Resilient-MST output trees 𝑇(1),𝑇(2)for inputs𝐺(1),𝐺(2)are
4𝜀(1+𝑜(1))/ln𝜆-resilient. □
4 EXTENSION TO ℓ𝑝CLUSTERING
Algorithm 1 can be easily extended to work for 𝑘-median and
𝑘-means, and in general, for any ℓ𝑝-clustering problem with the
cost functionÍ
𝑝𝜇(𝑝,C(𝑝))𝑝, whereC(𝑝)denotes the center 𝑝is
assigned to.
The only change in Algorithm 1 is in Step 10, where instead
of running the 2-approximation for 𝑘-center, we run the best ℓ𝑝-
clustering algorithm: a 2.406-approximation for 𝑘-median and a
5.912-approximation for 𝑘-means [ 16]. In general, we denote with
𝛼𝑝the best approximation for the 𝑝norm. Given the similarity,
the pseudo-code for the ℓ𝑝clustering algorithm is deferred to the
Appendix.
The resiliency argument is exactly the same as in Lemma 3.6. The
approximation argument differs and is more complicated because
the summations in ℓ𝑝-clustering objective need a more global view
than the uniform radius bounds in 𝑘-center.
Theorem 4.1. For any 1>𝜀>3 ln𝜆ln2𝑛
𝑛, Algorithm 3 is a 7.78-
resilient 22𝑝+𝛼𝑝,1+2 log 1/𝜀-approximation for ℓ𝑝-clustering with
probability 1−𝑂(1/𝑛), where𝛼𝑝is the approximation ratio of the
algorithm applied in Step 10.
 
34Resilient k-Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain.
Proof. First note that the set of open centers is 𝐶∪𝐶′which
has size𝑘+2𝑘log(1/𝜀)=𝑘(1−2log𝜀). So it remains to establish
the approximation factor of the algorithm. Consider an optimum
ℓ𝑝-clusteringC, and let OPT(𝑣)=𝜇(𝑣,C(𝑣))𝑝, for𝑣∈𝑃, be the
contribution of 𝑣toOPT, so OPT=Í
𝑣OPT(𝑣). Let𝐶′(𝑣), for
𝑣∈𝐿, denote the assignment of 𝑣within𝐶′. Similarly define 𝐶(𝑣)
for𝑣∈𝑃. ClearlyÍ
𝑣∈𝐿𝜇(𝑣,𝐶′(𝑣))𝑝≤𝛼𝑝OPT, as we use the same
number𝑘of centers and the entire input points, but only assign a
subset𝐿⊆𝑃using the selected centers. It remains to bound the
cost associated with 𝑃\𝐿.
Let𝑈⊆𝑃be the set of points whose optimum cluster does not
intersect𝐶. We say these points are not “captured.” We showed in
the proof of Lemma 3.5 that E[|𝑈|]≤𝜀𝑛with probability 1−1/𝑛.
We next bound the cost due to the captured points.
Claim 4.2. We haveÍ
𝑣∉𝑈E[𝜇(𝑣,𝐶(𝑣))𝑝]≤22𝑝OPT.
Fixed a point 𝑣. Let𝑄be its optimum cluster, and let 𝑞∗∈𝑄be
the corresponding optimum center. We have
E
𝐶
𝜇(𝑣,𝐶(𝑣))𝑝|𝐶∩𝑄≠∅
≤E
𝐶
(2𝜇(𝑣,𝐶))𝑝|𝐶∩𝑄≠∅
from Lemma 3.7,
≤2𝑝E
𝐶
𝜇(𝑣,𝐶)𝑝|𝐶∩𝑄≠∅
≤22𝑝−1E
𝐶
𝜇(𝑣,𝑞∗)𝑝+𝜇(𝑞∗,𝐶)𝑝|𝐶∩𝑄≠∅
by the triangle inequality (i.e., 𝜇(𝑣,𝐶)≤𝜇(𝑣,𝑞∗)+𝜇(𝑞∗,𝐶)) and
(𝑎+𝑏)𝑝≤2𝑝−1(𝑎𝑝+𝑏𝑝)for𝑎,𝑏≥0which follows from the
convexity of 𝑥𝑝for𝑝≥1,
≤22𝑝−1E
𝐶
𝜇(𝑣,𝑞∗)𝑝|𝐶∩𝑄≠∅
+22𝑝−1E
𝐶
𝜇(𝑞∗,𝐶)𝑝|𝐶∩𝑄≠∅
≤22𝑝−1E
𝐶
𝜇(𝑣,𝑞∗)𝑝|𝐶∩𝑄≠∅
+22𝑝−1E
𝐶
𝜇(𝑞∗,𝐶)𝑝|𝐶∩𝑄=1
=22𝑝−1OPT(𝑣)+22𝑝−1
|𝑄|Õ
𝑞∈𝑄OPT(𝑞).
Summing the first term over all 𝑣∉𝑈givesÍ
𝑣∉𝑈22𝑝−1OPT(𝑣)≤
22𝑝−1Í
𝑣OPT(𝑣)=OPT. Summing up over all 𝑣∉𝑈for the second
term gives an upper bound of 2𝑝−1OPT, since each expression is
repeated at most|𝑄|times, once for each 𝑣∈𝑄\𝑈. Putting these
together proves the claim.
Recall that𝐿is the set of the 𝜀𝑛≥|𝑈|points with the largest
contribution to cost in 𝐶. Thus, the cost of 𝐶associated with 𝑃\𝐿
is
E
𝐶Õ
𝑣∈𝑃\𝐿𝜇(𝑣,𝐶(𝑣))𝑝≤E
𝐶Õ
𝑣∈𝑃\𝑈𝜇(𝑣,𝐶(𝑣))𝑝≤22𝑝OPT,
where the last step follows from Claim 4.2. Combining this with
the bound on the cost of 𝐶′from above, we prove the lemma. □
5 EMPIRICAL EVALUATION
In this section, we empirically evaluate our algorithms on real-world
datasets containing user locations from SNAP8and Kaggle9and
also synthetic 2-d datasets known as Birch-sets [ 48]. Our empirical
analysis focuses on the resilient 𝑘-center algorithm (Section 3). We
8https://snap.stanford.edu
9https://www.kaggle.comfirst describe the datasets we use, then we explain the baselines we
benchmark and how we measure the quality of clustering solutions,
and finally we present our empirical results.
5.1 Datasets
We first describe the real-world datasets and then explain the syn-
thetic dataset.
Real-world datasets. These datasets all capture human mobil-
ity. We consider two online location-based social networks, Gowalla
and Brightkite from SNAP, and one location-based dataset derived
from Uber pick-ups from Kaggle. All datasets represent points as
Geo location data (with latitude and longitude) associated with a
timestamp. We transform these Geo locations to 𝑅3by converting
angular Geo location to Cartesian coordinates.
Gowalla and Brightkite.10Both datasets collect the public check-
in data where Gowalla contains data from Feb. 2009 to Oct. 2010,
and Brightkite contains data from Apr. 2008 to Oct. 2010. The total
number of check-ins is 6.4 million for Gowalla and 4.5 million for
Brightkite. They also include social network showing friendship
links between users which we do not use in our experiments.
We obtain𝜖-close point sets by considering average location of
each user in the first and second week of Jan. 2010 (for each dataset).
We discard users that don’t have this information.
Uber.11The original dataset contains around 18.8 million Uber
pickups in New York City from Apr. to Jun. 2015. We only considers
these pickup locations over the first and the second day of Jun. 2014.
Note that this is raw data and we don’t have ids associated with
these Geo locations. In order to form our 𝜖-close datasets, we find
a minimum weight perfect matching between points of these two
days. We remove unmatched locations and also matched locations
located more than 1 km apart. We assign the same id to matched
pairs and construct one dataset corresponding to each day.
Synthetic datasets. We work with Birch-sets [ 48] that contain
100k points in 2-d and they are basically collection of 100 clusters
located in a regular grid (see visualization on website12). In order
to generate𝜖-close datasets, we move each point of this dataset by
a random noise from a normal (Gaussian) distribution with mean
of 0.5 and standard deviation of 0.5.
5.2 Experimental Setup
5.2.1 Baselines. In the experiments we evaluate the performance
of our resilient 𝑘-center algorithm against the following two well-
known baselines.
– Gonzalez Algorithm [ 26] starts with a random center and repeats
the following operations 𝑘−1times: find the point with maximum
distance to the closest center and add it to the set of centers. We
refer to this algorithm as Gonz.
– Carving Algorithm [ 33,43] finds the smallest value 𝑅such that
the following algorithm opens at most 𝑘centers: while there exist
some uncovered points in the instance, pick an uncovered point at
random, add it to the set of centers, and mark all points (including
the center) with distance at most 𝑅from this new center as covered.
10https://snap.stanford.edu/data/index.html#locnet
11https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city
12https://cs.joensuu.fi/sipu/datasets/
 
35KDD ’24, August 25–29, 2024, Barcelona, Spain. Sara Ahmadian et al.
This is intuitively equivalent to carving out balls of radius 𝑅from
the instance and hence we refer to this algorithm as Carv.
5.2.2 Implementation details and parameters of the algorithm. We
consider 8different implementations of our algorithm (Algorithm 1)
based on the utilized classical 𝑘-center algorithm in the second
opening stage and the number of centers that we open in each of two
stages of our algorithm. Recall that our algorithm first opens a set of
random centers and then opens additional centers using a classical
𝑘-center problem. In our configurations, we either open 𝑘/2or𝑘
centers for each stage and for the choice of classical algorithm, we
consider Gonz andCarv. We utilize the notation “ConG( 𝛼,𝛽)” and
“ConC(𝛼,𝛽)” to denote the variant of our algorithm that opens 𝛼𝑘
centers randomly and constructs a solution with 𝛽𝑘centers using
Gonz algorithm and Carv algorithm, respectively.
5.2.3 Measure of Quality. We evaluate the following measures of
quality for comparing the two clustering solutions produced for
given distance matrices.
–Fraction of points that change cluster. This is the fraction of
points that are assigned to different centers between the two solu-
tions.
–Solution Cost. The maximum distance between a point and its
assigned center, i.e., the 𝑘-center cost.
–Number of Clusters (or open centers) . Recall that our algo-
rithm opens centers in two stages and may open upto (𝛼+𝛽)𝑘
centers.
5.3 Experimental Results
Here we compare the quality of the output of our algorithm with
that of the baselines. We present results of four experiments for
𝑘∈{10,20,50,100}, and𝜆∈{1.1,1.001}(Fig. 1, Fig. 2, Fig. 3, and
Fig. 4) in the main body and defer more results for different values
of𝑘and𝜆to Appendix C. Recall that 𝑘is the number of centers
and𝜆is the parameter of the algorithm in Algorithm 2.
First, we evaluate the fraction of points that change cluster for the
given two distance matrices. Note that in all datasets and settings,
our algorithm achieves significantly higher resilience. In some cases,
e.g., Fig. 1, the baseline may reassign close to 90%of the points where
our algorithm reassigns less than 10%. Ensuring resilience becomes
harder when we allow more centers (e.g., 𝑘=50and𝑘=100),
but we still observe reassignment of only 20-30% of points by our
algorithm versus 95-99% by the baselines.
In terms of the cost of produced solutions, in order to guarantee
resilience, we sometimes produce solutions with higher costs (up
to twice the cost of the baseline). However our cost is usually
comparable to the baseline and the gap decreases as we allow more
centers.
Similarly in terms of open centers, depending on our configura-
tion we may open more centers, however for some configuration,
despite having the flexibility of opening more centers, our algo-
rithm actually terminates before reaching the bound on the open
centers, e.g., ConC(1.0,1.0) in Fig. 2.
For instance, in Fig. 1 for 𝑘=10, the number of clusters in the
solution constructed and the cost of solution by ConG(0.5,1.0) is
slightly more than the baselines (10% to30%). On the other hand,
the center assigned to the points for more than 93%and97%of thepoints alters for the Gonzalez and Carving baselines, respectively,
which is around a factor 30more compared to ConG(0.5,1.0) where
3%of the centers assigned to points alters. Further experiments
with different values of 𝑘and𝜆are provided in Appendix C.
CONCLUSION AND FUTURE WORKS
We introduce a new notion of algorithmic resilience and design
new algorithms for classic unsupervised learning problems such as
𝑘-center,𝑘-median,𝑘-means. It is an interesting open problem to
improve our algorithms and in particular to obtain non-bicriteria
algorithms for these problems, and also prove lower bounds (hard-
ness results). It would be also interesting to study other classic data
mining problems under the same notion of resiliency.
REFERENCES
[1]Pankaj K. Agarwal, Hsien-Chih Chang, Kamesh Munagala, Erin Taylor, and Emo
Welzl. 2020. Clustering Under Perturbation Stability in Near-Linear Time. In 40th
IARCS Annual Conference on Foundations of Software Technology and Theoretical
Computer Science, FSTTCS 2020, December 14-18, 2020, BITS Pilani, K K Birla Goa
Campus, Goa, India (Virtual Conference) (LIPIcs), Nitin Saxena and Sunil Simon
(Eds.), Vol. 182. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 8:1–8:16.
https://doi.org/10.4230/LIPICS.FSTTCS.2020.8
[2]Charu C. Aggarwal and Chandan K. Reddy. 2013. Data Clustering: Algorithms
and Applications (1st ed.). Chapman Hall/CRC.
[3]Aris Anagnostopoulos, Ravi Kumar, Mohammad Mahdian, Eli Upfal, and Fabio
Vandin. 2012. Algorithms on evolving graphs. In Innovations in Theoretical
Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, Shafi Goldwasser
(Ed.). ACM, 149–160. https://doi.org/10.1145/2090236.2090249
[4]Pranjal Awasthi, Avrim Blum, and Or Sheffet. 2012. Center-based clustering
under perturbation stability. Inform. Process. Lett. 112, 1-2 (2012), 49–54.
[5]Lars Backstrom, Cynthia Dwork, and Jon M. Kleinberg. 2011. Wherefore art
thou R3579X?: anonymized social networks, hidden patterns, and structural
steganography. Commun. ACM 54, 12 (2011), 133–141. https://doi.org/10.1145/
2043174.2043199
[6]Maria-Florina Balcan, Nika Haghtalab, and Colin White. 2020. k-center Clustering
under Perturbation Resilience. ACM Trans. Algorithms 16, 2 (2020), 22:1–22:39.
https://doi.org/10.1145/3381424
[7]Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta. 2014. Robust hierarchical
clustering. The Journal of Machine Learning Research 15, 1 (2014), 3831–3871.
[8]Yonatan Bilu and Nathan Linial. 2012. Are Stable Instances Easy? Comb. Probab.
Comput. 21, 5 (2012), 643–660. https://doi.org/10.1017/S0963548312000193
[9]Deeparnab Chakrabarty and Maryam Negahbani. 2023. Robust k-center with
two types of radii. Mathematical Programming 197, 2 (2023), 991–1007.
[10] TH Hubert Chan, Arnaud Guerqin, and Mauro Sozio. 2018. Fully dynamic k-
center clustering. In Proceedings of the 2018 World Wide Web Conference. 579–587.
[11] Moses Charikar, Chandra Chekuri, Tomás Feder, and Rajeev Motwani. 1997.
Incremental clustering and dynamic information retrieval. In Proceedings of the
twenty-ninth annual ACM symposium on Theory of computing. 626–635.
[12] Chandra Chekuri and Shalmoli Gupta. 2018. Perturbation Resilient Cluster-
ing for k-Center and Related Problems via LP Relaxations. In Approximation,
Randomization, and Combinatorial Optimization. Algorithms and Techniques,
APPROX/RANDOM 2018, August 20-22, 2018 - Princeton, NJ, USA (LIPIcs), Eric
Blais, Klaus Jansen, José D. P. Rolim, and David Steurer (Eds.), Vol. 116. Schloss
Dagstuhl - Leibniz-Zentrum für Informatik, 9:1–9:16. https://doi.org/10.4230/
LIPICS.APPROX-RANDOM.2018.9
[13] H. Chernoff. 1952. A Measure of Asymptotic Efficiency for Tests of a Hypothesis
Based on the sum of Observations. AMS 23, 4 (1952), 493 – 507.
[14] Flavio Chierichetti, Alessandro Panconesi, Giuseppe Re, and Luca Trevisan.
2022. Spectral Robustness for Correlation Clustering Reconstruction in Semi-
Adversarial Models. In International Conference on Artificial Intelligence and
Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event (Proceedings of Ma-
chine Learning Research), Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel
Valera (Eds.), Vol. 151. PMLR, 10852–10880. https://proceedings.mlr.press/v151/
chierichetti22a.html
[15] Antonio Emanuele Cinà, Alessandro Torcinovich, and Marcello Pelillo. 2022. A
black-box adversarial attack for poisoning clustering. Pattern Recognit. 122 (2022),
108306. https://doi.org/10.1016/j.patcog.2021.108306
[16] Vincent Cohen-Addad, Hossein Esfandiari, Vahab S. Mirrokni, and Shyam
Narayanan. 2022. Improved approximations for Euclidean k-means and k-median,
via nested quasi-independent sets. In STOC ’22: 54th Annual ACM SIGACT Sympo-
sium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, Stefano Leonardi and
Anupam Gupta (Eds.). ACM, 1621–1628. https://doi.org/10.1145/3519935.3520011
 
36Resilient k-Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain.
𝑘=10 𝑘=20−0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
𝑘=10 𝑘=20012345·105Solution
Cost(
b)
𝑘=10 𝑘=200102030Numb
er of Clusters(
c)
Figur
e 1:Comparison of our algorithm with the baselines for the synthetic dataset for 𝜆=1.1and for𝑘=10,𝑘=20.
𝑘=10 𝑘=20−0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
𝑘=10 𝑘=20051015Solution
Cost(
b)
𝑘=10 𝑘=200102030Numb
er of Clusters(
c)
Figur
e 2:Comparison of our algorithm with the baselines for the Uber dataset for 𝜆=1.1and for𝑘=10,𝑘=20.
𝑘=50 𝑘=100−0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
𝑘=50 𝑘=10005001,0001,5002,0002,500Solution
Cost(
b)
𝑘=50 𝑘=100050100150Numb
er of Clusters(
c)
Figur
e 3:Comparison of our algorithm with the baselines for Brightkite for 𝜆=1.1and for𝑘=50,𝑘=100.
𝑘=50 𝑘=100−0.500.511.5Fraction
of Points Changing Cluster(
a)
Gonz
Car
v
ConG(0.5,0.5)
ConG(0.5,1.0)
ConG(1.0,0.5)
ConG(1.0,1.0)
ConC(0.5,0.5)
ConC(0.5,1.0)
ConC(1.0,0.5)
ConC(1.0,1.0)
𝑘=50 𝑘=10005001,0001,500Solution
Cost(
b)
𝑘=50 𝑘=100050100150Numb
er of Clusters(
c)
Figur
e 4:Comparison of our algorithm with the baselines for Gowalla for 𝜆=1.001and𝑘=50,𝑘=100.
[17] Vincent Cohen-Addad, Niklas Oskar D Hjuler, Nikos Parotsidis, David Saulpic,
and Chris Schwiegelshohn. 2019. Fully dynamic consistent facility location.
Advances in Neural Information Processing Systems 32 (2019).
[18] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
2009. Introduction to Algorithms, 3rd Edition. MIT Press. http://mitpress.mit.
edu/books/introduction-algorithms
[19] Easley David and Kleinberg Jon. 2010. Networks, Crowds, and Markets: Reasoning
About a Highly Connected World. Cambridge University Press, USA.
[20] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of cryptography
conference. Springer, 265–284.[21] David Eisenstat, Claire Mathieu, and Nicolas Schabanel. 2014. Facility Location in
Evolving Metrics. In Automata, Languages, and Programming - 41st International
Colloquium, ICALP 2014, Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part
II (Lecture Notes in Computer Science), Javier Esparza, Pierre Fraigniaud, Thore
Husfeldt, and Elias Koutsoupias (Eds.), Vol. 8573. Springer, 459–470. https:
//doi.org/10.1007/978-3-662-43951-7_39
[22] Volker A. Erdmann and Jörn Wolters. 1986. Collection of published
5S, 5.8S and 4.5S ribosomal RNA sequences. Nucleic Acids Research
14, suppl (01 1986), r1–r60. https://doi.org/10.1093/nar/14.suppl.r1
arXiv:https://academic.oup.com/nar/article-pdf/14/suppl/r1/6958995/14-
suppl-r1.pdf
 
37KDD ’24, August 25–29, 2024, Barcelona, Spain. Sara Ahmadian et al.
[23] Tomás Feder and Daniel Greene. 1988. Optimal Algorithms for Approximate
Clustering. In Proceedings of the Twentieth Annual ACM Symposium on Theory of
Computing (STOC ’88). Association for Computing Machinery, New York, NY,
USA, 434–444. https://doi.org/10.1145/62212.62255
[24] Hendrik Fichtenberger, Silvio Lattanzi, Ashkan Norouzi-Fard, and Ola Svensson.
2021. Consistent k-clustering for general metrics. In Proceedings of the 2021
ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 2660–2678.
[25] Guojun Gan, Chaoqun Ma, and Jianhong Wu. 2007. Data Clus-
tering: Theory, Algorithms, and Applications. Society for Industrial
and Applied Mathematics. https://doi.org/10.1137/1.9780898718348
arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9780898718348
[26] Teofilo F Gonzalez. 1985. Clustering to minimize the maximum intercluster
distance. Theoretical computer science 38 (1985), 293–306.
[27] Teofilo F. Gonzalez. 1985. Clustering to Minimize the Maximum Intercluster
Distance. Theor. Comput. Sci. 38 (1985), 293–306. https://doi.org/10.1016/0304-
3975(85)90224-5
[28] J. C. Gower and G. J. S. Ross. 1969. Minimum Spanning Trees and Single Linkage
Cluster Analysis. Journal of the Royal Statistical Society. Series C (Applied Statistics)
18, 1 (1969), 54–64. http://www.jstor.org/stable/2346439
[29] Xiangyu Guo, Janardhan Kulkarni, Shi Li, and Jiayi Xian. 2021. Consistent
k-median: Simpler, better and robust. In International Conference on Artificial
Intelligence and Statistics. PMLR, 1135–1143.
[30] Sariel Har-Peled. 2004. Clustering Motion. Discret. Comput. Geom. 31, 4 (2004),
545–565. https://doi.org/10.1007/s00454-004-2822-7
[31] Sariel Har-Peled and Manor Mendel. 2006. Fast Construction of Nets in Low-
Dimensional Metrics and Their Applications. SIAM J. Comput. 35, 5 (2006),
1148–1184. https://doi.org/10.1137/S0097539704446281
[32] Satoshi Hara and Yuichi Yoshida. [n.d.]. Average Sensitivity of Decision Tree
Learning. In The Eleventh International Conference on Learning Representations.
[33] Dorit S Hochbaum and David B Shmoys. 1985. A best possible heuristic for the
k-center problem. Mathematics of operations research 10, 2 (1985), 180–184.
[34] Sungjin Im and Benjamin Moseley. 2015. Fast and better distributed mapreduce
algorithms for k-center clustering. In Proceedings of the 27th ACM symposium on
Parallelism in Algorithms and Architectures. 65–67.
[35] Mohammad Reza Karimi Jaghargh, Andreas Krause, Silvio Lattanzi, and Sergei
Vassilvtiskii. 2019. Consistent online optimization: Convex and submodular. In
The 22nd International Conference on Artificial Intelligence and Statistics. PMLR,
2241–2250.
[36] Soh Kumabe and Yuichi Yoshida. 2022. Average sensitivity of dynamic pro-
gramming. In Proceedings of the 2022 Annual ACM-SIAM Symposium on DiscreteAlgorithms (SODA). SIAM, 1925–1961.
[37] Soh Kumabe and Yuichi Yoshida. 2022. Lipschitz Continuous Algorithms for
Graph Problems. arXiv preprint arXiv:2211.04674 (2022).
[38] Silvio Lattanzi, Stefano Leonardi, Vahab Mirrokni, and Ilya Razenshteyn. 2015.
Robust hierarchical k-center clustering. In Proceedings of the 2015 Conference on
Innovations in Theoretical Computer Science. 211–218.
[39] Silvio Lattanzi and Sergei Vassilvitskii. 2017. Consistent k-clustering. In Interna-
tional Conference on Machine Learning. PMLR, 1975–1984.
[40] Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert Endre Tarjan. 2007.
Clustering Social Networks. In Algorithms and Models for the Web-Graph, 5th
International Workshop, WAW 2007, San Diego, CA, USA, December 11-12, 2007,
Proceedings (Lecture Notes in Computer Science), Anthony Bonato and Fan R. K.
Chung (Eds.), Vol. 4863. Springer, 56–67. https://doi.org/10.1007/978-3-540-
77004-6_5
[41] Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert Endre Tarjan. 2008.
Finding Strongly Knit Clusters in Social Networks. Internet Math. 5, 1 (2008),
155–174. https://doi.org/10.1080/15427951.2008.10129299
[42] Gary J. Olsen. 1988. [53] Phylogenetic analysis using ribosomal RNA. In Ri-
bosomes. Methods in Enzymology, Vol. 164. Academic Press, 793–812. https:
//doi.org/10.1016/S0076-6879(88)64084-5
[43] David B Shmoys. 1995. Computing near-optimal solutions to combinatorial
optimization problems. Combinatorial Optimization 20 (1995), 355–397.
[44] Adarsh Subbaswamy, Roy Adams, and Suchi Saria. 2021. Evaluating Model
Robustness and Stability to Dataset Shift. In The 24th International Conference
on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual
Event (Proceedings of Machine Learning Research), Arindam Banerjee and Kenji
Fukumizu (Eds.), Vol. 130. PMLR, 2611–2619. http://proceedings.mlr.press/v130/
subbaswamy21a.html
[45] Nithin Varma and Yuichi Yoshida. 2021. Average sensitivity of graph algorithms.
InProceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA).
SIAM, 684–703.
[46] Yuichi Yoshida and Shinji Ito. 2022. Average Sensitivity of Euclidean k-Clustering.
InAdvances in Neural Information Processing Systems.
[47] Yuichi Yoshida and Samson Zhou. 2020. Sensitivity analysis of the maximum
matching problem. arXiv preprint arXiv:2009.04556 (2020).
[48] T. Zhang, R. Ramakrishnan, and M. Livny. 1997. BIRCH: A new data clustering
algorithm and its applications. Data Mining and Knowledge Discovery 1, 2 (1997),
141–182.
 
38