DisCo: Towards Harmonious Disentanglement and Collaboration
between Tabular and Semantic Space for Recommendation
Kounianhua Du
Shanghai Jiao Tong University
Shanghai, China
kounianhuadu@sjtu.edu.cnJizheng Chen
Shanghai Jiao Tong University
Shanghai, China
humihuadechengzhi@sjtu.edu.cnJianghao Lin
Shanghai Jiao Tong University
Shanghai, China
chiangel@sjtu.edu.cn
Yunjia Xi
Shanghai Jiao Tong University
Shanghai, China
xiyunjia@sjtu.edu.cnHangyu Wang
Shanghai Jiao Tong University
Shanghai, China
hangyuwang@sjtu.edu.cnXinyi Dai
Huawei Noah’s Ark Lab
Shanghai, China
daixinyi5@huawei.com
Bo Chen
Huawei Noah’s Ark Lab
Shanghai, China
chenbo116@huawei.comRuiming Tang
Huawei Noah’s Ark Lab
Shenzhen, China
tangruiming@huawei.comWeinan Zhang∗
Shanghai Jiao Tong University
Shanghai, China
wnzhang@sjtu.edu.cn
ABSTRACT
Recommender systems play important roles in various applications
such as e-commerce, social media, etc. Conventional recommen-
dation methods usually model the collaborative signals within the
tabular representation space. Despite the personalization model-
ing and the efficiency, the latent semantic dependencies are omit-
ted. Methods that introduce semantics into recommendation then
emerge, injecting knowledge from the semantic representation
space where the general language understanding are compressed.
However, existing semantic-enhanced recommendation methods
focus on aligning the two spaces, during which the representations
of the two spaces tend to get close while the unique patterns are
discarded and not well explored. In this paper, we propose DisCo to
Disentangle the unique patterns from the two representation spaces
andCollaborate the two spaces for recommendation enhancement,
where both the specificity and the consistency of the two spaces are
captured. Concretely, we propose 1) a dual-side attentive network
to capture the intra-domain patterns and the inter-domain patterns,
2) a sufficiency constraint to preserve the task-relevant informa-
tion of each representation space and filter out the noise, and 3)
a disentanglement constraint to avoid the model from discarding
the unique information. These modules strike a balance between
disentanglement and collaboration of the two representation spaces
to produce informative pattern vectors, which could serve as extra
features and be appended to arbitrary recommendation backbones
for enhancement. Experiment results validate the superiority of our
∗Weinan Zhang is the co-corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than the 
author(s)
 must be honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. 
ACM ISBN 979-8-4007-0490-1/24/08. 
https://doi.org/10.1145/3637528.3672008
Collaborative-based in-domain 
pattern learning Tabular Data Space  Semantic Data Space
Semantic dependencies with 
open world knowledge
Near
Beer DiaperUser 1: Beer -> Diaper -> ...
User 2: Diaper -> Beer -> ...
NearErnest 
HemingwayRaymond
Carver1.Representative minimalist writers.
2.Carver once said: "Hemingway taught 
me how to write." 
Noise -> To be eliminated.
Aligning Part -> To be ensured.
Disenrangling Part -> To be ensured.Union of all potential task-Related information.RFigure 1: Illustration of the motivation.
method against different models and the compatibility of DisCo
over different backbones. Various ablation studies and efficiency
analysis are also conducted to justify each model component.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
Recommender Systems, User Modeling, Large Language Model
ACM Reference Format:
Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang,
Xinyi Dai, Bo Chen, Ruiming Tang, and Weinan Zhang. 2024. DisCo: To-
wards Harmonious Disentanglement and Collaboration between Tabular
and Semantic Space for Recommendation. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3672008
 
666
KDD ’24, August 25–29, 2024, Barcelona, Spain Kounianhua Du et al.
1 INTRODUCTION
Recommender systems have become an integral part of today’s dig-
ital ecosystem, enhancing user experiences, boosting engagement,
facilitating decision-making, and fostering connections between
users and relevant content or products. They are widely used in
various industries, including e-commerce [ 37], entertainment [ 6],
social media [42], and online streaming platforms [19, 45, 46].
Conventional recommender systems usually only model the col-
laborative signals within the tabular representation space, where
samples consist of multi-field categorical features. These methods
focus on mining beneficial interactions among features [ 20,28,38,
44] and modeling user interests using historical user behaviors
[10,31,33,45,46] for accurate and personalized recommendation.
While being good at modeling feature interactions and personalized
user preferences, these methods fail to learn the latent semantic
dependencies of features. For example, as shown in Figure 1, a book
written by Ernest Hemingway and a book written by Raymond
Carver can be close in semantic space because the two authors
are known to be minimalist writing style and Carver said that
he borrowed many elements from Hemingway’s style. This latent
semantic dependency cannot be inferred from the tabular represen-
tation space where features are firstly encoded in a one-hot manner.
Attempts to incorporate external knowledge within semantic rep-
resentation space into recommendation then emerge [ 15,16,23],
where textual descriptions and their encodings are used to hold
the external semantic knowledge. Despite the general language
understanding within semantic representation space, the encoding
of the large language model has ambiguity and fails to imply the
correlations of some features well. For instance, beer and diaper
are distant in semantic space but near in tabular space for recom-
mendation where the correlation analysis is done, since users tend
to buy beer and diaper together.
As discussed above, the relationship among the same set of
user behaviors can be different in the two representation spaces.
The unique and disentangled patterns from the two representation
spaces form a complementary relationship with each other, con-
tributing different information. Therefore, it is vital to effectively
and efficiently collaborate the tabular and semantic representation
spaces. The existing works either: 1) take only one space into consid-
eration [ 11] or 2) only focus on aligning the two spaces [ 23], during
which the representations tend to get close (green part in Figure 1),
with the disentangled part (yellow and blue parts in Figure 1) being
discarded.
In this paper, we propose DisCo toDisentangle and Collaborate
the tabular and semantic representation spaces for user behavior
patterns modeling. Therefore, we design 1) a dual-side attentive
network (DS-Attn) to capture the intra-domain and inter-domain
patterns, 2) a sufficiency constraint to preserve the useful infor-
mation (yellow, green, and blue parts in Figure 1) and eliminate
the noisy information (grey part in Figure 1) from the two repre-
sentation spaces, and 3) a disentanglement constraint to preserve
the disentangling parts from the two representation spaces (yellow
and blue part in Figure 1). Together, these modules strike a balance
of the collaboration and disentanglement between the tabular and
semantic representation space.Concretely, DS-Attn infers the inner patterns within the tabular
space and the semantic space for collaborative-based correlations
and semantic-based dependencies with the intra-domain attention,
and captures the inter patterns between the two spaces for aligned
knowledge with the inter-domain attention where a query-key ex-
change is adopted to make the two spaces attend to each other. The
parameters of DS-Attn and embedding networks are regularized
by the sufficiency and disentanglement constraints. The sufficiency
constraint maximizes the mutual information between the rep-
resentations of each space with the labels, which preserves the
task-relevant information offered by each space. The disentangle-
ment constraint minimizes the mutual information between the
output vectors of DS-Attn from the two spaces, which forces the
two spaces to offer different information. The resulting vectors
output by DS-Attn and regularized by the two constraints preserve
both the consistent and the specific knowledge of the two represen-
tation spaces, which can then be fed into arbitrary recommendation
backbones for prediction enhancement.
The main contributions of the paper are summarized as follows:
•We design a novel and effective framework, DisCo, that harmo-
niously captures both the consistent and the specific knowledge
from tabular representation space and semantic representation
space with the dual-side attentive network under the regulariza-
tion of the proposed sufficiency and disentanglement constraints.
•We emphasize the importance of unique and disentangled infor-
mation in both the tabular space and the semantic space, and
propose the first work to disentangle the tabular and semantic
representation spaces for unique domain knowledge.
•DisCo is a model-agnostic framework compatible with different
recommendation backbones, offering flexibility and generality.
The experiment results over a series of recommendation backbones
justify the consistent superiority of the proposed method. Ablation
studies are also conducted to validate the effectiveness of different
model components.
2 RELATED WORK
2.1 Tabular-Only Methods
Early recommendation models focus on digging into interactions
among features. FM [ 36] captures second-order feature interac-
tions. FFM [ 21] introduces field-aware interactions. Wide & Deep
[3] combines the strengths of linear models and deep neural net-
works. DeepFM [ 13] replaces the logistic regression layer in [ 3]
with an FM layer. xDeepFM [ 24] introduces the cross layer for high-
order feature interactions. PNN [ 34] utilizes the product layer to
learn the high-order product interactions. DCN [ 41] proposes to
capture both shallow and deep feature interactions effectively. Au-
toInt [ 38] utilizes the multi-head self-attention mechanism to learn
high-order feature interactions. By modeling user behavior pat-
terns, recommenders provide more personalized recommendations.
DIN [ 46] incorporates the attention mechanism [ 40] to capture user
interests. DIEN [ 45] uses GRU module to better model the evolving
interests of users. DSIN [ 10] proposes to capture the dynamic in-
terests of users within a session. MIMN [ 30] leverages a memory
network architecture to capture different aspects of user interests.
SIM [31] models long-term user behaviors.
 
667DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
2.2 Semantic-Enhanced Methods
Recently, large language models have shown great impact and shed
light on various domains of recommendation systems. There are
several attempts to incorporate large language models into recom-
mender systems [ 25–27,43]. PTab [ 29] adopts a classic BERT [ 7]
framework with Modality Transformation(MT), Masked-Language
Finetuning(MF), and Classification Fine-tuning(CF) training stages.
P5 [11], as well as its variants [ 12,17,18], propose to tune T5 [ 35]
as a unified recommendation model for various downstream tasks
ZESRec [ 8] proposes to obtain universal representations from item
descriptions through BERT for zero-shot recommendation. UniSRec
[16] learns item representations via a fixed BERT model followed by
an MoE-enhanced network. CTRL [ 23] adopts the contrastive learn-
ing methodology to align the tabular space and semantic space for
recommendation enhancement. VQ-Rec [ 15] makes improvements
on UnisRec [16], which introduces vector quantization technique.
3 PRELIMINARIES
3.1 Problem Formulation
The click-through rate (CTR) prediction task aims at accurately
predicting the probability of a user clicking an item, which is the
core task for recommender systems. Therefore, we mainly focus
on the CTR prediction task in this work. The conventional CTR
prediction task within the tabular domain can be formulated as
𝑝(𝑦𝑖|XU
i,XI
i,XC
i,𝜃), (1)
where XU
i=h
𝑥𝑈
𝑖,1,...,𝑥𝑈
𝑖,𝐹𝑈i
is the set of user features, XI
i=
h
𝑥𝐼
𝑖,1,...,𝑥𝐼
𝑖,𝐹𝐼i
is the set of item features, XC
i=h
𝑥𝐶
𝑖,1,...,𝑥𝐶
𝑖,𝐹𝐶i
is the set of context features for the click prediction event (e.g.,
device, season, etc.), 𝑦𝑖is the label of the data point, and 𝜃is the
model parameter. We use 𝐹𝑈,𝐹𝐼,𝐹𝐶to denote the number of user
features, item features, and context features, respectively. These
methods only focus on modeling feature interactions of the input
based on the target sample only but fail to model the user behavior
patterns.
Modeling user behavior patterns plays an important role in boost-
ing personalized recommendation performance. This line of meth-
ods takes users’ historical behaviors as extra inputs and models
the dependencies between the candidate item and historical items,
which can be formulated as
𝑝(𝑦𝑖|XU
i,XI
i,XC
i,[⟨XI
ik,𝑦𝑖𝑘⟩]𝐾
𝑘=1,𝜃), (2)
where[⟨XI
ik,𝑦𝑖𝑘⟩]𝐾
𝑘=1represents the list of user’s historical behav-
iors and their corresponding labels, and 𝜃is the model parameter.
In this paper, we aim to involve the embedded open-world knowl-
edge of large language models to achieve harmonious disentangle-
ment and collaboration between the tabular and semantic space
for recommendation enhancement. Hence, the prediction can be
formulated as
𝑝(𝑦𝑖|XU
i,XI
i,XC
i,[⟨XI
ik,𝑦𝑖𝑘⟩]𝐾
𝑘=1,Φ𝑆,𝜃), (3)
where Φ𝑆represents the encoder of a large language model.3.2 Mutual Information Minimization &
Maximization
Mutual information (MI) is important but hard to compute in neural
networks. For mutual information maximization, MINE [ 2] builds
connections between the expectations of variables and mutual in-
formation, and proposes a lower bound of the mutual information
based on the Donsker-Varadhan representation of KL divergence.
DIM [ 14] points out that we do not necessarily need to obtain
the precise value of MI but only need to maximize it. They use
the Jensen-Shannon Divergence to estimate the MI and therefore
propose a GAN-style loss to maximize it:
𝐿=𝐸J[log𝑇𝑤(𝑥,𝑦)]+𝐸M[log(1−𝑇𝑤(𝑥,𝑦))]. (4)
For mutual information minimization, CLUB [ 4] introduces an
upper bound for mutual information. When the conditional dis-
tribution𝑝(𝑦|𝑥)is known, the upper bound can be represented
as
𝐼(𝑋;𝑌)≤𝐼𝐶𝐿𝑈𝐵(𝑋;𝑌)
=𝐸𝑝(𝑥,𝑦)[log𝑝(𝑦|𝑥)]−𝐸𝑝(𝑥)𝐸𝑝(𝑦)[log𝑝(𝑦|𝑥)].(5)
When the conditional distribution is not known, one could use a
variational distribution 𝑞𝜃(𝑦|𝑥)to approximate it and the upper
bound then becomes
𝐼𝑣𝐶𝐿𝑈𝐵 =𝐸𝑝(𝑥,𝑦)[log𝑞𝜃(𝑦|𝑥)]−𝐸𝑝(𝑥)𝐸𝑝(𝑦)[log𝑞𝜃(𝑦|𝑥)].(6)
4 METHODOLOGY
In this section, we describe the methodology of DisCo, which is
model-agnostic and compatible with different backbones.
4.1 Overview
As illustrated in Figure 2, we first prepare the tabular and semantic
embeddings. In this paper, we encode the user behaviors in tabular
and semantic representation space and extract patterns from the
resulting tabular and semantic embeddings. To make use of the gen-
eral open-world knowledge from the semantic space in an efficient
manner, we pre-store the semantic embedding of each item in an
indexed knowledge base, with the item ID being the indexing key.
The semantic embeddings are generated from a frozen LLM, with
the textual item descriptions as inputs. The tabular embeddings
are obtained via tabular embedding layers with one-hot encoded
features as inputs.
The proposed DisCo mainly consists of three components: the
dual-side attentive network, the sufficiency constraint, and the dis-
entanglement constraint. To collaborate the tabular and semantic
representation space, we propose a Dual-Side Attentive Network
to encode the patterns from user behaviors under collaborative
learning based representation space, semantic dependencies-based
representation space, and the interactions of the two spaces. The
resulting pattern vectors serve as additional features for arbitrary
recommendation models. In addition, we propose two constraints
to regularize the model, which preserve the useful and unique in-
formation from the two representation spaces: 1) A sufficiency
constraint to maximize the mutual information between encoded
vectors and the labels, which preserves the task related informa-
tion and eliminates the noise. 2) A disentanglement constraint to
minimize the mutual information between vectors from different
 
668KDD ’24, August 25–29, 2024, Barcelona, Spain Kounianhua Du et al.
representation spaces, which forces each space to provide unique
domain-specific knowledge. Together, these components strike a
harmonious balance between collaboration and disentanglement.
4.2 Indexed Knowledge Base
To efficiently utilize the semantic knowledge, we first build an
indexed knowledge base to extract and pre-store the semantic em-
bedding of each item.
For each item, we obtain a semantic description for it using the
field-value prompt template. As shown in Figure 3, for a movie with
title Titanic, genre Romantic, and director James Cameron, we can
obtain the item description "Here is a movie, title is Titanic, genre is
Romantic, and director is James Cameron.". The item description is
then fed into a large language model Φ𝑆(·)to acquire the semantic
embedding, which will be stored in an indexed knowledge base
𝐾𝐵[·]for further usage, with the item features being the index key
and the semantic embedding being the value.
4.3 Dual-Side Attentive Network
As discussed in Section 1, the relations among the same set of user
behaviors can be different in the tabular and semantic domains.
In order to capture the distinct and shared patterns among the
behaviors in both domains, we design a dual-side attentive network
(DS-Attn) module, which consists of intra-domain attention and
inter-domain attention. The intra-domain attention models the
distinct domain-specific patterns within the tabular domain for
collaborative-based correlations and within the semantic domain
for semantic-based dependencies, respectively. The inter-domain
attention models the shared patterns between the two domains,
where a query-key exchange between the two domains is adopted
to make the two domains attend to each other.
4.3.1 Behaviors Encoding. For each target sample Xi=[XU
i,XI
i,XC
i],
we gather𝐾recent historical behaviors {⟨XI
ik,𝑦𝑖𝑘⟩}𝐾
𝑘=1to assist the
prediction. The candidate item and the historical items are trans-
formed into representations in tabular domain and semantic domain
as follows,∀𝑗∈{𝑖,𝑖1,...,𝑖𝐾}:
hS
j=𝑀𝐿𝑃(𝐾𝐵[XI
j]), (7)
hT
j=Φ𝑇(XI
j), (8)
where Φ𝑇denotes the tabular domain embedding network and
𝐾𝐵[·]denotes the indexed knowledge base constructed in the pre-
vious stage. 𝑀𝐿𝑃 is used to reduce the dimension of the seman-
tic embedding. hS
jandhT
jdenote the embedding of item features
in semantic space and tabular space, respectively. In order to de-
crease the entanglement of the embeddings for intra-domain and
inter-domain pattern extraction, we decouple the corresponding
embeddings into chunks. ∀𝑗∈{𝑖,𝑖1,...,𝑖𝐾}:
hS
j=[hSI
j,hSC
j], (9)
hT
j=[hTI
j,hTC
j], (10)
where hSI
j∈𝑅𝑑
2,hSC
j∈𝑅𝑑
2,hTI
j∈𝑅𝑑
2,andhTC
j∈𝑅𝑑
2.
In addition, following [ 9,32], we also encode the historical labels
to model accurate click signals, ∀𝑘∈{1,...,𝐾}:
lik=Φ𝑌(𝑦𝑖𝑘), (11)where Φ𝑌denotes the label embedding network.
4.3.2 Attention Block of DS-Attn. The dual-side attentive network
consists of four attention blocks for tabular, semantic, tabular-to-
semantic, and semantic-to-tabular user behavior patterns. We first
define the function of the basic attention block as follows:
H,L′=𝐴𝑡𝑡𝑛𝜃(Q,K,V,L), (12)
𝜃={WQ,WK,WV}, (13)
where𝜃denotes the parameters of an attention block. The detail
operations for H,L′=𝐴𝑡𝑡𝑛𝜃(Q,K,V,L)are:
Q′=QW Q,K′=KW K,V′=VW V, (14)
A=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(Q′K′T
√
𝑑), (15)
H=AV′,L′=AL, (16)
where Q∈𝑅1×𝑑
2,K∈𝑅𝐾×𝑑
2,V∈𝑅𝐾×𝑑
2,L∈𝑅𝐾×𝑑denote the
input for query, key, value, and labels, and WQ,WK,WV∈𝑅𝑑
2×𝑑
represent the trainable weights.
4.3.3 Intra-Domain Attention. To capture the inner domain tab-
ular and semantic behavior patterns, we apply the intra-domain
attention among the candidate item and the behavior sequences
within the tabular and semantic domains.
We define the query, key, and value for intra-tabular and intra-
semantic behavior patterns as
QSS
i=hSI
i,QTT
i=hTI
i(17)
KSS
i=VSS
i=©­­
«hSI
i1
···
hSI
iKª®®
¬,KTT
i=VTT
i=©­­
«hTI
i1
···
hTI
iKª®®
¬. (18)
where QSS
i,KSS
i, and VSS
idenote the input for intra-semantic atten-
tion and QTT
i,KTT
i, and VTT
irepresent the input of intra-tabular
attention.
Then the user behavior patterns in different spaces can be ob-
tained by
PSS
i=[HSS
i,LSS
i]=𝐴𝑡𝑡𝑛𝜃𝑆𝑆(QSS
i,KSS
i,VSS
i,Li), (19)
PTT
i=[HTT
i,LTT
i]=𝐴𝑡𝑡𝑛𝜃𝑇𝑇(QTT
i,KTT
i,VTT
i,Li). (20)
4.3.4 Inter-Domain Attention. We further capture the inter-domain
patterns and model the shared knowledge with an inter-domain
attention module.
We define the query, key, and value for intra-tabular and intra-
semantic behavior patterns encoded as
QST
i=hSC
i,QTS
i=hTC
i(21)
KST
i=VST
i=©­­
«hTC
i1
···
hTC
iKª®®
¬,KTS
i=VTS
i=©­­
«hSC
i1
···
hSC
iKª®®
¬. (22)
Then the user behavior patterns in different spaces can be ob-
tained by
PST
i=[HST
i,LST
i]=𝐴𝑡𝑡𝑛𝜃𝑆𝑇(QST
i,KST
i,VST
i,Li), (23)
PTS
i=[HTS
i,LTS
i]=𝐴𝑡𝑡𝑛𝜃𝑇𝑆(QTS
i,KTS
i,VTS
i,Li). (24)
 
669DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Tabular 
Embedding 
Layer
Label 
Embedding
Layer
Item 
Pool
Field-Value 
Prompt 
Template
Indexed 
Knowledge 
Base
Item 
Features
Textual 
Description
Semantic 
Embedding
TT-Attn
SS-Attn
ST-Attn
TS-Attn
Target 
Sample
Historical 
Behaviors
LLM
 
Disentanglement 
Constraint
Dual-Side 
Attentive 
Newtork
T
abular 
S
emantic
Indexed 
Knowledge 
Base
 
Sufficiency 
Constraint 
Arbitrary 
Recommendation 
Model
?
?
?
Pattern 
Vectors
Eliminated 
by 
Sufficiency
Ensured 
by 
Sufficiency
Ensured 
by 
Disentanglement
(a)
(b)
(c)
Figure 2: Overview. (a) To extract the semantic knowledge, a textual description is obtained for each item using a field-value
prompt template, which is then fed to a LLM for semantic embedding and stored in an indexed knowledge base. (b) The
candidate item and the historical behaviors are encoded in tabular and semantic representation spaces, which are then sent to
Dual-Side Attentive Network for intra-domain and inter-domain pattern vectors. The resulting pattern vectors serve as extra
features and can be appended to arbitrary recommendation model. (c) Two constraints are devised to regularize the model
and preserve both the aligning part and the disentangling part of useful information from the two representation spaces. The
sufficiency constraint is applied on the behavior vectors and the summarized pattern vectors to preserve the useful information.
The disentanglement constraint is applied on the pattern vectors from the two different domains to force the model to capture
unique information from both domains.
Here 
is 
a 
movie, 
title 
is 
Titanic, 
genre 
is 
Romantic, 
and 
director 
is 
James 
Cameron.
Prompt 
Template
Item 
Description
Titanic   
Romantic   
James 
Cameron
Title
Genre
Director
Field
Value
Figure 3: The illustration of the field-value prompt template.
4.4 Sufficiency Constraint
To preserve the task-related information from the two representa-
tion spaces and filter out the noises, we maximize the mutual infor-
mation between the encoded vectors hfrom each domain and the
labels. The difficulty here lies in that the label space is discrete and
only consists of two values. Therefore, we follow DIM [ 14] to use
the summarized pattern vectors H·from each space to represent the
label space, which extends the discrete states into high-dimensionalcontinuous space. To maximize the mutual information, we max-
imize the distance between the marginal distribution and joint
distributions between two variables.
Concretely, for tabular domain sufficiency preserving, we sample
the positive pairs as IT+={⟨hTI
i,HTT
i+⟩|𝑦𝑖=𝑦𝑖+}and sample the
negative pairs asIT−={⟨hTI
i,HTT
i−⟩|𝑦𝑖≠𝑦𝑖−}. For semantic
domain sufficiency preserving, we sample the positive pairs as
IS+={⟨hSI
i,HSS
i+⟩|𝑦𝑖=𝑦𝑖+}and sample the negative pairs as
IS−={⟨hSI
i,HSS
i−⟩|𝑦𝑖≠𝑦𝑖−}. The discriminator network D𝜃1,D𝜃2
are used to distinguish the two pairs. The optimization objective is:
𝑙𝑆𝑢𝑓=−∑︁
IT+,IT−
logD𝜃1(hTI
i,HTT
i+)+
1−logD𝜃1(hTI
i,HTT
i−)
−∑︁
IS+,IS−
logD𝜃2(hSI
i,HSS
i+)+
1−logD𝜃2(hSI
i,HSS
i−)
.
(25)
4.5 Disentanglement Constraint
To preserve the unique information from the two representation
spaces, a two-level disentanglement is applied. Concretely, we min-
imize the mutual information among the pattern vectors summa-
rized from behavior vectors in the two domains. To achieve this, we
minimize the vCLUB [ 4] upper bound of the mutual information,
 
670KDD ’24, August 25–29, 2024, Barcelona, Spain Kounianhua Du et al.
which is defined as
𝐼𝑣𝐶𝐿𝑈𝐵(X;Y)=𝐸𝑝(X,Y)[log𝑞𝜃∗(Y|X)]−𝐸𝑝(X)𝐸𝑝(Y)[log𝑞𝜃∗(Y|X)],
(26)
where𝑞𝜃∗(Y|X)is a variational distribution with parameter 𝜃∗to
approximate 𝑝(Y|X).
At each iteration of training, the variational approximation net-
work trained to maximize log𝑞𝜃∗(Y|X)is first optimized, and then
the main model is optimized.
In this way, we train a vCLUB mutual information estimator for
each pair of feature vectors from different domains and minimize
the mutual information between each pair. The loss objective of
the disentanglement module could then be formulated as
𝑙𝐷𝑖𝑠=𝐼𝑣𝐶𝐿𝑈𝐵 1(HTT;HSS)+𝐼𝑣𝐶𝐿𝑈𝐵 2(HTS;HST). (27)
4.6 Prediction and Training Objective
The aggregated feature and label embeddings are then disentan-
gled and appended to any recommendation backbone models for
prediction as
ˆ𝑦𝑖=𝑓(Xi,[⟨XI
ik,𝑦𝑖𝑘⟩]𝐾
𝑘=1,PTT
i,PSS
i,PTS
i,PST
i), (28)
where𝑓(·)denotes an arbitrary recommendation model.
The training objective consists of the prediction loss, the suffi-
ciency loss, and the disentanglement loss, which can be formulated
as:
𝑙𝑝𝑟𝑒𝑑=-∑︁
𝑖(𝑦𝑖logˆ𝑦𝑖+(1−𝑦𝑖)log(1−ˆ𝑦𝑖)) (29)
L=𝑙𝑝𝑟𝑒𝑑+𝛼·𝑙𝑆𝑢𝑓+𝛽·𝑙𝐷𝑖𝑠, (30)
where𝛼and𝛽are hyperparameters to scale the loss components.
5 EXPERIMENTS
In this section, we empirically evaluate the proposed model on
three datasets. Five research questions lead the experiment part.
RQ1 How does DisCo perform against the baselines?
RQ2 Is DisCo compatible with different backbones?
RQ3 Does each model component contribute to the performance?
RQ4 How is the efficiency of DisCo?
5.1 Setup
5.1.1 Datasets. We use three public datasets to evaluate DisCo.
The statistics of the datasets are summarized in Table 1.
Table 1: Dataset statistics.
Dataset # Users # Items # Samples # Fields
ML-1M 6,040 3,706 970 ,009 8
AZ-Toys 208,175 77,687 716,197 5
ML-25M 162,541 59,047 24,187,390 4
•ML-1M1is a collection of movie ratings provided by users of
the MovieLens website.
•AZ-Toys2gathers product reviews and metadata related to toys
and games available in Amazon e-commerce.
•ML-25M3is a popular movie recommendation dataset widely
used in machine learning and recommender systems.
1https://grouplens.org/datasets/movielens/1m/
2https://cseweb.ucsd.edu/ jmcauley/datasets.html
3https://files.grouplens.org/datasets/movielens/ml-25m.zipSamples with ratings greater than 3 are treated as positive sam-
ples, with the others being negative samples. The window size of
the historical behaviors is 30. Data is split according to the global
timestamps. Specifically, the training data lies between [0,𝑇0), the
validation data lies between [𝑇0,𝑇1), and the test data lies between
[𝑇1,+∞). The ratio of data amount for train/valid/test is 8 : 1 : 1.
5.1.2 Evaluation Metrics. Two widely used metrics including AUC
(Area under the ROC curve) and Log Loss (binary cross-entropy
loss) are applied to evaluate the performance.
5.1.3 Competing Models. We compare the proposed DisCo with
the following methods: 1) conventional tabular methods including
DeepFM [ 13], DCN [ 41], PNN [ 34], xDeepFM [ 24], AutoInt [ 38],
DIN [ 46], DIEN [ 45], and 2) semantic-enhanced methods P5 [ 11],
UnisRec [16], CTRL [23], and VQ-Rec [15].
5.1.4 Implementation Details. We utilize Vicuna-13b [ 5] released
by FastChat4for text encoding. For a fair comparison, we fix the
embedding size and the hidden layer size to be the same for all
backbone models. The embedding size for the tabular domain repre-
sentation is 32. The hidden layer size used for MLP is [128,64]. We
use the bilinear networks to serve as the discriminator network in
the sufficiency constraint and vCLUB mutual information estimator
in the disentanglement constraint. The coefficients for the suffi-
ciency constraint loss and disentanglement constraint loss are 0.02
and0.01, respectively. For each model, the learning rate is searched
in the range of{1𝑒−4,3𝑒−4,5𝑒−4,1𝑒−3}, and the weight decay
is searched in the range of {1𝑒−5,3𝑒−5,5𝑒−5,1𝑒−4,3𝑒−4}. We
use the Adam [ 22] optimizer during training. The patience of early
stop is 10. The code is available5 6.
5.2 Overall Performance (RQ1)
In this- section, we compare our proposed DisCo with various
baseline models. The experiment results are displayed in Table 2.
From the results, one can draw the following conclusions. 1) Our
proposed DisCo can consistently outperform all the baseline models
including tabular-only methods and the semantic-enhanced meth-
ods. The improvements are statistically significant under p-value
<0.001. This shows the effectiveness of the proposed paradigm that
disentangles and collaborates tabular and semantic domain knowl-
edge for enhanced recommendation. 2) The methods that involve
in semantic knowledge can surpass the conventional tabular-only
methods, which shows the effectiveness of introducing external
semantic knowledge into recommendation. 3) DisCo outperforms
methods that focus on aligning the two representation spaces. For
example, CTRL utilizes the contrastive learning methodology to
align the two representation spaces. These methodologies tend
to make representations in different representation spaces closer,
where unique information is discarded during training. This vali-
dates the effectiveness of DisCo that preserves the unique informa-
tion of the two different representation spaces.
4https://github.com/lm-s
5https://github.com/KounianhuaDu/DisCo
6https://github.com/mindspore-lab/models/tree/master/research/huawei-
noah/DisCo
 
671DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Major results. For all the baselines, we append the user histories and their corresponding ratings/labels for fair
comparison. The best result is in bold, while the second-best value is underlined. Rel.Impr denotes the relative AUC improvement
of DisCo against each baseline model. The symbol * indicates statistically significant improvement with p-value <0.001.
ModelsML-1M AZ-Toys ML-25M
AUC Logloss Rel. Impr. AUC Logloss Rel. Impr. AUC Logloss Rel.Impr.
Tabular
Only
(CRS)DeepFM 0.7947 0.5470 1.11% 0.7423 0.3720 0.74% 0.8133 0.4892 1.36%
DCN 0.7961 0.5417 0.93% 0.7424 0.3716 0.73% 0.8134 0.4875 1.35%
PNN 0.7932 0.5451 1.30% 0.7418 0.3705 0.81% 0.8135 0.4869 1.33%
xDeepFM 0.7938 0.5464 1.22% 0.7392 0.3748 1.16% 0.8093 0.4922 1.84%
AutoInt 0.7945 0.5435 1.13% 0.7429 0.3705 0.66% 0.8128 0.4883 1.42%
DIN 0.7976 0.5401 0.74% 0.7424 0.3707 0.73% 0.8174 0.4820 0.86%
DIEN 0.7970 0.5428 0.82% 0.7446 0.3704 0.43% 0.8189 0.4841 0.68%
Semantic
EnhancedP5 0.7937 0.5478 1.23% 0.7418 0.3736 0.81% 0.8091 0.4921 1.90%
UnisRec 0.7991 0.5410 0.55% 0.7452 0.3837 0.35% 0.8162 0.5223 1.02%
CTRL 0.7979 0.5413 0.70% 0.7432 0.3723 0.62% 0.8189 0.4922 0.68%
VQ-Rec 0.7972 0.5449 0.79% 0.7456 0.3826 0.30% 0.8185 0.5210 0.73%
Best CRS + DisCo 0.8035* 0.5343* - 0.7478* 0.3704* - 0.8245* 0.4743* -
Table 3: Compatibility experiments. The proposed method offers additional feature fields, which can be followed by different
feature interaction operations. We test its compatibility with different CTR backbones.
Backb
onesML-1M AZ-
Toys ML-25M
Original +DisCo Rel.
Impr
.Original +DisCo Rel.
Impr
.Original +DisCo Rel.
Impr
. AUC
LL AUC
LL AUC
LL AUC
LL AUC
LL AUC
LL
De
epFM 0.7947
0.5470 0.8029
0.5340 1.03% 0.7423
0.3720 0.7462
0.3708 0.53% 0.8133
0.4892 0.8217
0.4771 1.03%
DCN 0.7961
0.5417 0.8035
0.5343 0.93% 0.7424
0.3716 0.7470
0.3693 0.62% 0.8134
0.4875 0.8231
0.4875 1.19%
PNN 0.7932
0.5451 0.8017
0.5401 1.07% 0.7418
0.3705 0.7466
0.3681 0.65% 0.8135
0.4869 0.8245
0.4743 1.35%
xDe
epFM 0.7938
0.5464 0.7999
0.5384 0.77% 0.7392
0.3748 0.7434
0.3718 0.57% 0.8093
0.4922 0.8240
0.4792 1.82%
A
utoInt 0.7945
0.5435 0.8029
0.5343 1.06% 0.7429
0.3705 0.7472
0.3710 0.58% 0.8128
0.4883 0.8196
0.4863 0.83%
DIN 0.7976
0.5401 0.8016
0.5373 0.51% 0.7424
0.3707 0.7478
0.3704 0.73% 0.8174
0.4820 0.8212
0.4829 0.46%
DIEN 0.7970
0.5428 0.8025
0.5353 0.69% 0.7446
0.3704 0.7468
0.3693 0.30% 0.8189
0.4841 0.8219
0.4792 0.37%
5.3 Compatibility Study (RQ2)
Since the extracted patterns obtained from the dual-side attentive
network can serve as extra features, they could be appended to
arbitrary conventional recommendation models. In this section, we
evaluate the compatibility of the proposed framework on different
conventional backbones.
The feature interaction methods of the backbones include product-
based, MLP-based, and attention-based operators. We test DisCo
on these different operators and justify the effectiveness of the
resulting feature fields. The results are displayed in Table 3. From
the results, we can see that the proposed method could offer per-
formance gains for various backbone models and operations. The
improvements are statistically significant under p-value <0.001,
which validates the superior compatibility of DisCo.
5.4 Ablation Studies (RQ3)
5.4.1 Impact of the Dual-Side Attentive Network. In this section,
we validate the effectiveness of the proposed dual-side attentive
network module. Concretely, we remove the inter-domain attention
within which the two representation spaces attend to each other and
only retrain the intra-domain attention. This results in the commontwo-tower structure in recommender systems, where the semantic
and tabular representations are modeled separately for semantic
dependencies and collaborative signals respectively. While our dual-
side attentive network module models both the intra-domain and
inter-domain user behavior patterns. The results of the two-tower
attention and the dual-side attentive network module are displayed
in Table 4.
Table 4: Experiment on the Aggregation Module.
DatasetsTwo-Tower Attention Dual-Side Attention
AUC Logloss AUC Logloss
ML-1M 0.8015 0.5360 0.8035 0.5343
AZ-Toys 0.7464 0.3689 0.7478 0.3704
ML-25M 0.8208 0.4837 0.8245 0.4743
From the results, we could see that the proposed dual-side at-
tentive network can outperform the two-tower aggregation, which
validates the effectiveness of the proposed module that captures
both the intra-domain and the inter-domain knowledge.
 
672KDD ’24, August 25–29, 2024, Barcelona, Spain Kounianhua Du et al.































Intra-domain without disentanglement Intra-domain with disentanglement 
Inter-domain without disentanglement Inter-domain with disentanglement 
Figure 4: T-SNE visualization of the representations for the
dual-side attentive network output (ML-1M).
5.4.2 Impact of the Constraints. In this section, we study the impact
of the proposed constraints.
Table 5: Impacts of the two constraints.
DatasetsML-1M AZ-
Toys ML-25M
AUC
Logloss AUC
Logloss AUC
Logloss
w/
Both (DisCo) 0.8035
0.5343 0.7478
0.3704 0.8245
0.4743
w/o Sufficiency 0.8019
0.5354 0.7474
0.3686 0.8220
0.4780
w/o Disentanglent 0.8011
0.5377 0.7476
0.3687 0.8212
0.4833
w/o Both 0.7988
0.5391 0.7460
0.3701 0.8163
0.4928
Firstly, we conduct experiments with and without the constraints,
the results of which are displayed in Table 5. From the results, we
can see the following conclusions. 1) Both the sufficiency and the
disentanglement constraints could offer performance gains to the
model, with sufficiency helps to preserve task-relevant information
from each space and disentanglement helps to enforce unique in-
formation from each space. 2) The two constraints collaborates and
boosts performance with each other, which helps to capture both
the consistency and specificity information of the two spaces.
In addition, we further visualize the representations of the dual-
side attentive module output with and without the disentanglement
constraint to dig into how the disentanglement impacts the distri-
bution of representations. The visualization of ML-1M is displayed
in Figure 4. More visualizations could be found in Appendix A.
Concretely, we visualize the representations of the intra-domain
pattern vectors HTT,HSSand inter-domain pattern vectors HTS,HST
with t-sne [ 39]. The left column of Figure 4 displays the represen-
tations without the disentanglement constraint, while the right
column of the figure displays those with the disentanglement con-
straint. From the figure, we can see that under the regularization
of the disentanglement constraint, the distributions of representa-
tions from different domains are separated better and that there areapparently better manifolds existing in the representation spaces
with the disentanglement constraint. This illustrates that the dis-
entanglement constraint could well separate and extract different
information from the two spaces.
5.5 Efficiency Analysis (RQ4)
In this section, we discuss the efficiency of the proposed model.
Firstly, the semantic embeddings for items could be pre-computed
and stored in the indexed knowledge base, the construction of
which can be done offline and only once. In addition, after the MLP
used to reduce the dimension of the semantic embedding is trained,
we could use the trained MLP to reduce the dimension and further
keep a reduced-dimension version of the indexed knowledge base.
Therefore, in the inference stage we do not need to deal with the
high-dimensional semantic vectors.
Table 6: Training and inference time per sample (s).
DatasetML-1M AZ-Toys
Training Inference Training Inference
DCN 2.18×10−32.86×10−52.06×10−32.68×10−5
DIN 1.99×10−31.34×10−52.05×10−32.15×10−5
DisCo 5.04×10−37.42×10−54.29×10−36.89×10−5
The training and inference time analysis is displayed in Table 6.
All the experiments are done on a single V100 GPU with Intel Xeon
Gold 6278C 2.60GHz CPU and run three times to get the average
time. From the results, we can see that the proposed method does
not cause a heavy overhead.
6 CONCLUSION
Recommender systems play a vital role in our daily life. Conven-
tional recommendation methods focus on modeling feature interac-
tions and user behaviors within the ID-based tabular representation
space and fail to capture semantic dependencies among user be-
haviors. Existing semantic-enhanced methods focus on aligning
the tabular and semantic space, while the unique and disentangled
parts of the two representation spaces are not well explored. In
this paper, we propose DisCo to disentangle and collaborate the
tabular and semantic representation spaces to capture both the
consistent and the specific knowledge from the two spaces for en-
hanced recommendations. Concretely, we design three modules,
namely dual-side attentive network, the sufficiency constraint, and
the disentanglement constraint. To efficiently utilize the semantic
knowledge, a textual description for each item is firstly obtained and
encoded by LLMs, the embedding of which is then stored into an
indexed knowledge. The dual-side attention module models intra-
domain and inter-domain patterns to offer additional knowledge for
arbitrary recommendation backbones, which is constrained by the
designed sufficiency and disentanglement constraints. The two con-
straints force the model to preserve useful information and extract
unique information from the two spaces. Extensive experiments
and ablation studies on three datasets and various backbone models
justify the effectiveness of the proposed method.
 
673DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
ACKNOWLEDGMENTS
The Shanghai Jiao Tong University team is partially supported by
National Key R&D Program of China (2022ZD0114804), Shanghai
Municipal Science and Technology Major Project (2021SHZDZX0102)
and National Natural Science Foundation of China (62177033, 62322603).
The work is sponsored by Huawei Innovation Research Program.
We thank MindSpore [ 1] for the partial support of this work, which
is a new deep learning computing framework.
REFERENCES
[1] 2020. MindSpore. https://www.mindspore.cn/
[2]Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua
Bengio, R. Devon Hjelm, and Aaron C. Courville. 2018. Mutual Information
Neural Estimation. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,
Vol. 80. PMLR, 530–539.
[3]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In DLRS@RecSys.
[4]Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence
Carin. 2020. CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information.
InProceedings of the 37th International Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research,
Vol. 119). PMLR, 1779–1788.
[5]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with
90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/
[6]Ingrid A. Christensen and Silvia Schiaffino. 2011. Entertainment recommender
systems for group of users. Expert Systems with Applications 38, 11 (2011), 14127–
14135. https://doi.org/10.1016/j.eswa.2011.04.221
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[8]Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. 2021. Zero-
Shot Recommender Systems. arXiv:2105.08318 [cs.LG]
[9]Kounianhua Du, Weinan Zhang, Ruiwen Zhou, Yangkun Wang, Xilong Zhao,
Jiarui Jin, Quan Gan, Zheng Zhang, and David P Wipf. 2022. Learning Enhanced
Representation for Tabular Data via Neighborhood Propagation. Advances in
Neural Information Processing Systems 35 (2022), 16373–16384.
[10] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping
Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction.
InProceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019. 2301–2307.
[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.
Recommendation as language processing (rlp): A unified pretrain, personalized
prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on
Recommender Systems. 299–315.
[12] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.
VIP5: Towards Multimodal Foundation Models for Recommendation. arXiv
preprint arXiv:2305.14302 (2023).
[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. In
IJCAI.
[14] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip
Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representa-
tions by mutual information estimation and maximization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019.
[15] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learn-
ing Vector-Quantized Item Representation for Transferable Sequential Recom-
menders. arXiv:2210.12316 [cs.IR]
[16] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong
Wen. 2022. Towards Universal Sequence Representation Learning for Recom-
mender Systems. arXiv:2206.05941 [cs.IR]
[17] Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, and Yongfeng Zhang. 2023.
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. arXiv
preprint arXiv:2305.12090 (2023).
[18] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How
to Index Item IDs for Recommendation Foundation Models. arXiv preprint
arXiv:2305.06569 (2023).
[19] Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, and Weinan
Zhang. 2022. Neural Statistics for Click-Through Rate Prediction. In Proceedingsof the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1849–1853.
[20] Yu-Chin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-
aware Factorization Machines for CTR Prediction. In Proceedings of the 10th ACM
Conference on Recommender Systems, Boston, MA, USA, September 15-19, 2016.
ACM, 43–50.
[21] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware
factorization machines for CTR prediction. In RecSys.
[22] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Opti-
mization. arXiv:1412.6980 [cs.LG]
[23] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Tabular
and Language Model for CTR Prediction. arXiv preprint arXiv:2306.02841 (2023).
[24] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 1754–1763.
[25] Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangn-
ing Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ClickPrompt:
CTR Models are Strong Prompt Generators for Adapting Language Models to
CTR Prediction. In Proceedings of the ACM on Web Conference 2024 (WWW ’24).
3319–3330.
[26] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu,
Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang,
and Weinan Zhang. 2023. How Can Recommender Systems Benefit from Large
Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023).
[27] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan,
Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ReLLa: Retrieval-enhanced
Large Language Models for Lifelong Sequential Behavior Comprehension in
Recommendation. In Proceedings of the ACM on Web Conference 2024 (WWW ’24).
3497–3508.
[28] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xi-
uqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Inter-
action Selection in Factorization Models for Click-Through Rate Prediction. In
KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 2636–2645.
[29] Guang Liu, Jie Yang, and Ledell Wu. 2022. PTab: Using the Pre-trained Language
Model for Modeling Tabular Data. arXiv preprint arXiv:2209.08060 (2022).
[30] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice
on Long Sequential User Behavior Modeling for Click-Through Rate Prediction.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019. ACM,
2671–2679.
[31] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong
Sequential Behavior Data for Click-Through Rate Prediction. In CIKM ’20: The
29th ACM International Conference on Information and Knowledge Management,
Virtual Event, Ireland, October 19-23, 2020. ACM, 2685–2692.
[32] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang,
Xiuqiang He, and Yong Yu. 2021. Retrieval & Interaction Machine for Tabular
Data Prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1379–1389.
[33] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020.
User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings of
the 43rd International ACM SIGIR conference on research and development in
Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM,
2347–2356.
[34] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng
Guo, Yong Yu, and Xiuqiang He. 2018. Product-based Neural Networks for User
Response Prediction over Multi-field Categorical Data. ACM Transactions on
Information Systems (2018).
[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485–5551.
[36] Steffen Rendle. 2010. Factorization machines. In ICDM.
[37] Brent Smith and Greg Linden. 2017. Two decades of recommender systems at
Amazon.com. IEEE Internet Computing (2017). https://www.amazon.science/
publications/two-decades-of-recommender-systems-at-amazon-com
[38] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-
Attentive Neural Networks. In Proceedings of the 28th ACM International Con-
ference on Information and Knowledge Management, CIKM 2019, Beijing, China,
November 3-7, 2019. ACM, 1161–1170.
[39] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All
You Need. arXiv:1706.03762 [cs.CL]
 
674KDD ’24, August 25–29, 2024, Barcelona, Spain Kounianhua Du et al.
[41] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network
for Ad Click Predictions. In Proceedings of the ADKDD’17, Halifax, NS, Canada,
August 13 - 17, 2017. ACM, 12:1–12:7.
[42] Chuhan Wu, Fangzhao Wu, Yongfeng Huang, and Xing Xie. 2023. Personal-
ized news recommendation: Methods and challenges. ACM Transactions on
Information Systems 41, 1 (2023), 1–50.
[43] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,
Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Rec-
ommendation with Knowledge Augmentation from Large Language Models.
arXiv:2306.10933 [cs.IR]
[44] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
2017. Attentional Factorization Machines: Learning the Weight of Feature Inter-
actions via Attention Networks. In Proceedings of the Twenty-Sixth International
Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017. 3119–3125.
[45] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate
Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019.
5941–5948.
[46] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for
Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK,
August 19-23, 2018. 1059–1068.
A VISUALIZATIONS
Visualizations of the representations for the dual-side attentive
network on AZ-Toys and ML-25M are displayed in Figure 5 and
Figure 6.
B EXPERIMENTS ON LONG-TAIL DATA
In this section, we validate the effectiveness of DisCo on the long-
tail data where features are less-hit in the tabular representation







































Intra-domain without disentanglement Intra-domain with disentanglement 
Inter-domain without disentanglement Inter-domain with disentanglement 
Figure 5: T-SNE visualization of the representations for the
dual-side attentive network output (AZ-Toys).space. Concretely, we sort items based on their frequency of occur-
rences in the training set. The bottom 10% in terms of frequency
are classified as long-tail items. Then we study the performance
of the best-performed tabular-only model DIN and that with the
proposed DisCo model.
The performance comparisons are displayed in Table 9. From
the results, we can see that the proposed method gives a significant
performance boost on the tail data. Since the general knowledge
contained in the pretrained semantic embeddings helps to comple-
ment the less-trained features in the tabular representation space,
where the semantic information is injected through the dual-side
attentive network under the regularizations of the constraints.
C CASE STUDY
In this section, We would like to provide an example to support the
claim that encodings from LLMs can capture open-world knowledge
as below.
Given two items in the movielens dataset: "Galaxy Quest" and
"Babe: Pig in the City". Their genres provided in the dataset and
the tags provided by open world are listed in Table 7.
For the two items:
•The two items do not share any common tokens.
•The two items do not share any common features given in
the dataset.
•But they are actually close as they share a lot common fea-
tures in the open world (e.g., the tags given by Douban).
We then compute the cosine similarities between the above two
items in the tabular space and that in the semantic space as in Ta-
ble 8. (Note that no generation is involved. We use the same genres
provided by the movielens dataset to obtain the item encodings for
both the tabular encodings and the semantic encodings.)




















Inter -domain without disentanglement Intra -domain without disentanglement Intra -domain with disentanglement 
Inter -domain with disentanglement
 
Figure 6: T-SNE visualization of the representations for the
dual-side attentive network output (ML-25M).
 
675DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 7: Items used in the case study.
Item Title Galaxy Quest Babe: Pig in the City
Genre provided in the movielens dataset Adventure Children’s movie
Tags provided by Douban
(Open world knowledge, unknown by the tabular model)Comedy, Action, Fantasy and adventure. Comedy, Fantasy and Adventure.
Table 8: Similarities of items in different spaces.
Cosine Similarity In Tabular Space In Semantic Space (BERT) In Semantic Space (Vicuna-13b)
Galaxy Quest – Babe: Pig in the City -0.0210 0.0599 0.9606
Table 9: Experiment on the tail Data.
DatasetsDIN DIN + DisCo
AUC Logloss AUC Logloss
ML-1M 0.6710 0.6564 0.6934 0.6308
AZ-Toys 0.6673 0.3982 0.7416 0.3778
ML-25M 0.7963 0.5430 0.8032 0.5380
From the results, we can see that
•Tabular representations cannot find the relevance between
the two items, since no common features exist.
•Semantic representations by small language models cannot
well find the relevance between the two items, since no
common tokens exist.•Semantic representations by LLMs can well find the rele-
vance between the two items, even there are nearly no com-
mon tokens shared between the two items in the dataset.
Since there are many anchors about the two items existing
in the large open world training corpus (e.g., the common
tags given by Douban, the similar descriptions of movies,
etc.), as they are trained together, the representations of the
two items tend to get close.
The above case study can justify that the encodings from LLMs 
can help to capture open-world knowledge.
 
676