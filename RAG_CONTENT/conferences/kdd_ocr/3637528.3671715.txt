Automatic Multi-Task Learning Framework with Neural
Architecture Search in Recommendations
Shen Jiang
State Key Laboratory for
Novel Software Technology
Nanjing University
Nanjing, China
jiangshen@smail.nju.edu.cnGuanghui Zhu∗
State Key Laboratory for
Novel Software Technology
Nanjing University
Nanjing, China
zgh@nju.edu.cnYue Wang
State Key Laboratory for
Novel Software Technology
Nanjing University
Nanjing, China
wangyue1@smail.nju.edu.cn
Chunfeng Yuan
State Key Laboratory for
Novel Software Technology
Nanjing University
Nanjing, China
cfyuan@nju.edu.cnYihua Huang
State Key Laboratory for
Novel Software Technology
Nanjing University
Nanjing, China
yhuang@nju.edu.cn
Abstract
Multi-task learning (MTL), which aims to make full use of knowl-
edge contained in multiple tasks to enhance overall performance
and efficiency, has been broadly applied in recommendations. The
main challenge for MTL models is negative transfer. Existing MTL
models, mainly built on the Mixture-of-Experts (MoE) structure,
seek enhancements in performance through feature selection and
specific expert sharing mode design. However, one expert shar-
ing mode may not be universally applicable due to the complex
correlations and diverse demands among various tasks. Addition-
ally, homogeneous expert architectures in such models further limit
their performance. To address these issues, in this paper, we propose
an innovative automatic MTL framework, AutoMTL, leveraging
neural architecture search (NAS) to design optimal expert archi-
tectures and sharing modes. The Dual-level Expert Sharing mode
and Architecture Navigator (DESAN) search space of AutoMTL
can not only efficiently explore expert sharing modes and feature
selection schemes but also focus on the architectures of expert sub-
networks. Along with this, we introduce an efficient Progressively
Discretizing Differentiable Architecture Search (PD-DARTS) algo-
rithm for search space exploration. Extensive experiments demon-
strate that AutoMTL can consistently outperform state-of-the-art,
human-crafted MTL models. Moreover, the insights obtained from
the discovered architectures provide valuable guidance for building
new multi-task recommendation models.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671715CCS Concepts
•Computing methodologies →Multi-task learning; •Infor-
mation systems→Retrieval models and ranking.
Keywords
Multi-task learning, Multi-task recommendation, Neural architec-
ture search
ACM Reference Format:
Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, and Yihua Huang.
2024. Automatic Multi-Task Learning Framework with Neural Architec-
ture Search in Recommendations. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671715
1 Introduction
Recommender systems, which aim to personalize item suggestions
for users, increasingly dictate daily life. These systems generally
aim to simultaneously predict multiple objectives to generate high-
quality recommendations. For instance, short video recommender
systems are usually tasked with predicting multiple user behaviors,
including watching, liking, collecting videos, etc. As a result, multi-
task learning (MTL) has become a widely-utilized technique within
online recommender systems [ 26,33,46,57]. By jointly learning
multiple tasks within the same framework, multi-task learning can
not only improve parametric and computational efficiency but also
make full use of the knowledge carried by multiple tasks to enhance
overall performance.
Deep neural networks (DNNs)-based MTL models rely on pa-
rameter sharing to transfer knowledge among tasks. However, this
parameter sharing paradigm introduces an inherent issue within
multi-task learning, namely negative transfer. This problem arises
as multiple tasks can have intricate correlations and may even con-
flict with each other. Consequently, sharing information with a
less related task could hurt performance. Recent methods primarily
alleviate the problem of negative transfer and improve the perfor-
mance of multi-task learning from three aspects: (1) Methods [ 8,25]
like MetaBalance [ 22] balance the losses or gradients from different
 
1290
KDD ’24, August 25–29, 2024, Barcelona, Spain Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, & Yihua Huang
Task -Specific Expert
Task -Specific ExpertShared Expert
Linear Gate
Cross -Expert Gate
𝑻𝒐𝒘𝒆𝒓  𝑨𝑻𝒂𝒔𝒌  𝑨
𝑰𝒏𝒑𝒖𝒕𝑻𝒐𝒘𝒆𝒓  𝑩𝑻𝒂𝒔𝒌  𝑩
Shared -Bottom𝑻𝒐𝒘𝒆𝒓  𝑨𝑻𝒂𝒔𝒌  𝑨
𝑰𝒏𝒑𝒖𝒕𝑻𝒐𝒘𝒆𝒓  𝑩𝑻𝒂𝒔𝒌  𝑩
MMoE𝑻𝒐𝒘𝒆𝒓  𝑨𝑻𝒂𝒔𝒌  𝑨
𝑰𝒏𝒑𝒖𝒕𝑻𝒐𝒘𝒆𝒓  𝑩𝑻𝒂𝒔𝒌  𝑩
PLE𝑻𝒐𝒘𝒆𝒓  𝑨𝑻𝒂𝒔𝒌  𝑨
𝑰𝒏𝒑𝒖𝒕𝑻𝒐𝒘𝒆𝒓  𝑩𝑻𝒂𝒔𝒌  𝑩
AdaTT AutoMTL(Ours)𝑻𝒐𝒘𝒆𝒓  𝑨𝑻𝒂𝒔𝒌  𝑨
𝑰𝒏𝒑𝒖𝒕𝑻𝒐𝒘𝒆𝒓  𝑩𝑻𝒂𝒔𝒌  𝑩
? ? ?
? ? ?
????? ?
??? ?
??Searched Expert 
Architecture
Feature SelectionSearched Expert
Sharing ModeSearched Expert
Sharing Mode
Figure 1: Multi-task models evolve from shared-bottom models to MMoE-based models and models with increasingly flexible
expert sharing modes (i.e., PLE [ 46] and AdaTT [ 26]). The proposed AutoMTL automatically searches for the optimal expert
sharing modes and expert architectures.
tasks to minimize their interference. (2) Mixture-of-Experts [ 44]
(MoE)-based models leverage the MoE structure to attain more
flexible parameter sharing patterns. (3) Feature selection methods
like CFS [ 10] and MultiSFS [ 49] introduce expert- or task-specific
feature selection to mitigate conflicts between tasks. Multi-gate
Mixture-of-Experts (MMoE) model [ 33] was the first to leverage
the MoE structure to achieve a more flexible parameter sharing
pattern. The MMoE framework shares several expert subnetworks
across all tasks and then aggregates the representations from all
experts for each task through a gating network. Subsequent meth-
ods like PLE [ 46] and AdaTT [ 26] further separate task-specific and
shared experts to better learn task-specific and shared knowledge.
As shown in Figure 1, compared with the full-sharing mode [ 6] of
MMoE, PLE and AdaTT design more flexible expert sharing modes
for multi-task recommendation. Task-specific and task-shared ex-
perts perform their respective roles, potentially mitigating the issue
of negative transfer. However, in practice, no single sharing mode
consistently delivers optimal results across all datasets. This is
due to the fact that the optimal expert sharing mode of the MoE-
based multi-task model depends on the characteristics of the input
dataset and tasks. Datasets exhibiting higher task correlations de-
rive more benefit from shared experts, while those with lower task
correlations require more intricate expert sharing modes. Manually
designed expert sharing mode doesn’t assure optimality. Further-
more, the existing MoE-based multi-task learning models always
contain experts with homogeneous architecture, a property not
advantageous to the model’s performance. MoE-based models pre-
sume that different expert subnetworks are capable of learning
varied representations, thereby capturing diverse data attributes.
Nevertheless, designing different architectures for different experts
is a very complex and time-consuming task and is therefore imprac-
ticable. To address these issues, we propose to use the advanced
neural architecture search (NAS) method to automatically design
optimal expert architectures, expert sharing modes, and feature
selection schemes for multi-task recommendations.
However, applying NAS to multi-task learning is non-trivial. To
address the above issues, we first design a general and expressive
search space called Dual-level Expert Sharing-mode and ArchitectureNavigator (DESAN) that can cover both the existing and unex-
plored multi-task models. In DESAN, the outer level explores shar-
ing modes of experts and feature selection schemes, while the inner
level targets the architectures of expert subnetworks. The sharing
mode of experts is conceptualized as the connections among experts
across different MoE layers. Specifically, the experts at an upper
MoE layer identify which outputs from experts in the previous layer
to adopt as inputs. Remarkably, the DESAN search space exhibits
strong expressive power, encompassing established MoE-based
multi-task learning models. Further, the sharing mode of experts
only considers connections between experts and does not increase
the number of operations within the search space. Thus, the com-
pact nature of search space guarantees superior search efficiency,
making it suitable for large-scale multi-task recommendations.
To explore the DESAN search space efficiently, we further pro-
pose a Progressively Discretizing Differentiable neural ARchitecture
Search (PD-DARTS) algorithm. For modeling network architectures,
we adopt two types of architecture parameters. One is used for se-
lecting operations within experts, while the other identifies expert
sharing modes and feature selection schemes. Furthermore, several
studies [ 2,3] have indicated that the single-level optimization, com-
pared with the original bi-level optimization of DARTS [ 29], not
only offers comparable performance but also provides enhanced
search efficiency and stability. As a result, PD-DARTS employs
single-level optimization instead of bi-level. Additionally, we pro-
pose to progressively discretize the supernet with the normalized en-
tropy of architecture parameters during the search process, thereby
eliminating the necessity for retraining.
In this paper, we refer to the proposed automatic architecture
search method for multi-task learning as AutoMTL1. To validate the
effectiveness of AutoMTL, extensive experiments were conducted
on five publicly available, commonly used multi-task recommenda-
tion datasets. AutoMTL consistently outperforms other MoE-based
multi-task learning models across all datasets and the majority of
tasks. Furthermore, the time overhead incurred by architecture
search aligns with the magnitude required to train an individual
model. This demonstrates the practicality of applying AutoMTL in
real-world multi-task recommendation scenarios. Case studies of
1AutoMTL’s source code is publicly available at https://github.com/PasaLab/AutoMTL
 
1291Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
identified architectures reveal that the architectures searched on
different datasets incorporate heterogeneous expert subnetworks
and display diverse expert sharing modes, underlining the critical
role of deploying neural architecture search in MTL.
In summary, the contributions of this paper are as follows:
•We propose AutoMTL, a novel neural architecture search
method for multi-task recommendations that automatically
designs optimal expert sharing modes, expert architectures,
and feature selection schemes in a data-driven manner.
•Extensive experiments on real-world benchmarks evaluate
the effectiveness of AutoMTL, which consistently outper-
forms the state-of-the-art, handcrafted multi-task recom-
mendation models.
•The architectures discovered by AutoMTL can provide valu-
able insights for practitioners crafting new multi-task rec-
ommendation models.
2 Related Work
2.1 Multi-Task Learning
Multi-task learning has been successfully applied in various fields,
including computer vision [ 31,36,56], natural language process-
ing [13,21], speech recognition [ 14], and recommender systems [ 26,
33,46,57]. Deep multi-task learning models rely on parameter shar-
ing [42] across tasks. There are two typical paradigms of parameter
sharing: hard parameter sharing and soft parameter sharing. Hard
parameter sharing, like shared-bottom model [ 5], shares several
layers among all tasks and retains other task-specific layers for
prediction. These methods may suffer significant performance de-
generation when tasks are less related. In contrast, soft parame-
ter sharing can alleviate negative transfer through more flexible
parameter-sharing patterns. Cross-stitch networks [ 36] and sluice
networks [ 43] selectively fuse representations from models of dif-
ferent tasks through linear combinations. However, constant fu-
sion weights fail to fully reflect task relationship distinction in
individual examples. MMoE [ 33] introduces a mixture-of-experts
and employs gating networks to dynamically aggregate them for
each task. PLE [ 46] explicitly separates task-specific and shared
experts, thereby allowing for a more flexible expert sharing mode.
Similarly, AdaTT [ 26] explicitly models task relationships and uti-
lizes gating networks and residual mechanism to adaptively fuse
task-specific and shared knowledge. Additionally, several MTL mod-
els [34,37,39,50] are tailored for specific recommendation scenar-
ios. The proposed AutoMTL, unlike the methods mentioned above,
leverages neural architecture search to automatically identify opti-
mal expert sharing modes in a data-driven manner.
Another line of works focuses on improving multi-task opti-
mization techniques to better balance the losses or gradients from
different tasks. Uncertainty-based weighting [ 25] learns the weight
of each task based on its respective uncertainty. GradNorm [ 8],
GradDrop [ 9], PCGrad [ 54], RotoGrad [ 23], and MetaBalance [ 22]
reduce the inter-task interference by adeptly manipulating the gra-
dients from different tasks. These techniques are orthogonal to the
MTL architecture design.
Several works have introduced feature selection into multi-task
learning [10, 15, 49], in order to mitigate the detrimental outcome
induced by unnecessary features. For instance, CFS [ 10] introducescausal inference techniques to guide feature selection. MultiSFS [ 49]
explicitly models field-level task relations through a data-task bipar-
tite graph for task-specific feature selection. Similarly, our AutoMTL
also integrates feature selection within the search space.
2.2 Neural Architecture Search
Neural architecture search methods can be categorized into three
frameworks [ 16]: reinforcement learning (RL)-based NAS [ 38,59],
evolution-based NAS [ 20], and gradient-based NAS [ 4,29,52]. Con-
sidering the search efficiency, the weighting-sharing paradigm has
become dominant in NAS. It is a methodology to design a large
model (i.e., a supernet) that connects smaller model components.
Different candidate architectures can be generated by selecting a
subset of components. Advanced studies such as DARTS [ 29], Prox-
ylessNAS [ 4], and NASP [ 52] describe network architectures in the
continuous space and optimize architecture parameters with differ-
entiable methods. A series of works [ 7,12,24,58] try to improve
the efficiency and robustness of DARTS. [ 2,3] propose to replace
the bi-level optimization in DARTS with single-level optimization
to improve search efficiency and stability. The proposed PD-DARTS
algorithm also modifies DARTS to make it more suitable for our
search space and eliminates the necessity for retraining through
progressive discretization.
NAS methods have also been applied to multi-task learning.
SNR [ 32], Gumbel-matrix routing [ 35], and MTNAS [ 6] model the
parameter sharing routing of MTL models as connections between
subnetworks and utilize differentiable NAS methods to identify
sharing routing for given multi-task problems. However, the search
spaces of these methods are quite generic without prior knowledge
about MoE, making it difficult to learn optimal sharing routing
with limited data or computational resources. In contrast, the pro-
posed AutoMTL fully utilizes the advantages of the MoE-based
model, which has been widely proven to be superior for multi-task
recommendations.
3 Methodology
In this section, we introduce the expressive MoE-based search
space. Subsequently, we provide a detailed exposition of the ef-
ficient search algorithm.
3.1 Search Space Design
Inspired by recent MoE-based multi-task learning models, we pro-
pose a novel search space, i.e., dual-level expert sharing-mode
and architecture navigator (DESAN). It builds on the basis of the
mixture-of-experts structure, encompassing existing MoE-based
multi-task learning models [ 26,33,46]. The search space comprises
two levels. The outer level focuses on exploring expert sharing
modes and feature selection schemes, while the inner level centers
on searching for the heterogeneous architectures of expert subnet-
works. Following the weight-sharing paradigm in NAS, the search
space is constructed as a supernet.
As shown in Figure 2, the expert sharing mode is represented
by connections between experts or gating networks across adja-
cent MoE layers. Moreover, feature selection is represented by
connections between input features and specific experts or gating
 
1292KDD ’24, August 25–29, 2024, Barcelona, Spain Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, & Yihua Huang
𝑥1 ······ 𝑥2𝑥3 𝑥𝑀·· 𝛽1𝑠𝛽2𝑠𝛽𝑀𝑠·· 𝛽1𝑖𝛽2𝑖𝛽𝑀𝑖𝑭𝒆𝒂𝒕𝒖𝒓𝒆  𝒔𝒆𝒍𝒆𝒄𝒕𝒊𝒐𝒏  𝒎𝒐𝒅𝒖𝒍𝒆
𝐂𝐎𝐍𝐂𝐀𝐓 (𝐬𝐢𝐧𝐠𝐥𝐞 _𝐟𝐞𝐚,𝐢𝐧𝐭𝐞𝐫 _𝐟𝐞𝐚)
𝐅𝐞𝐚𝐭𝐮𝐫𝐞
𝐈𝐧𝐭𝐞𝐫𝐚𝐜𝐭𝐢𝐨𝐧  𝐋𝐚𝐲𝐞𝐫𝑬𝒙𝒑𝒆𝒓𝒕  
𝑺𝒆𝒍𝒆𝒄𝒕𝒊𝒐𝒏𝑴𝒊𝒙𝒕𝒖𝒓  𝒐𝒇 𝑬𝒙𝒑𝒆𝒓𝒕𝒔 𝑴𝒐𝑬  𝒎𝒐𝒅𝒖𝒍𝒆
𝑮
𝑜11···
𝑜𝐿1𝑜12···
𝑜𝐿2𝑜13···
𝑜𝐿3𝛽3 𝛽2 𝛽1
𝑥1···
𝑥𝐿𝑒1 ······𝑒2𝑒3 𝑒𝐿
𝐈𝐧𝐩𝐮𝐭𝐈𝐝𝐞𝐧𝐭𝐢𝐭𝐲 𝐌𝐋𝐏 _𝟏𝟔 ······𝐎𝐮𝐭𝐩𝐮𝐭𝐈𝐧𝐩𝐮𝐭𝐈𝐝𝐞𝐧𝐭𝐢𝐭𝐲 𝐌𝐋𝐏 _𝟏𝟔 ······
𝐃𝐢𝐬𝐜𝐫𝐞𝐭𝐢𝐳𝐞𝑴𝒊𝒙𝒕𝒖𝒓𝒆  𝒐𝒇 𝑶𝒑𝒆𝒓𝒂𝒕𝒊𝒐𝒏  (𝑴𝒐𝑶 ) 𝒃𝒍𝒐𝒄𝒌
(𝒐𝒏𝒍𝒚  𝒐𝒏𝒆  𝒐𝒑𝒆𝒓𝒂𝒕𝒊𝒐𝒏  𝒄𝒂𝒏  𝒃𝒆 𝒔𝒆𝒍𝒆𝒄𝒕𝒆𝒅 )
𝐎𝐮𝐭𝐩𝐮𝐭
𝜶𝟏 𝜶𝟐 𝜶…
𝑥
𝑒
𝜶,𝜷𝑮𝑥1······ 𝑥𝐿 𝑥1······ 𝑥𝐿 𝑥1······ 𝑥𝐿𝑬𝒙𝒑𝒆𝒓𝒕 𝟏,𝟏 𝑬𝒙𝒑𝒆𝒓𝒕 𝟏,𝟐𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏
𝑬𝒙𝒑𝒆𝒓𝒕 𝟏,𝟑𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐𝑒1······ 𝑒𝐿 𝑒1······ 𝑒𝐿 𝑒1······ 𝑒𝐿𝑬𝒙𝒑𝒆𝒓𝒕 𝟐,𝟏 𝑬𝒙𝒑𝒆𝒓𝒕 𝟐,𝟐 𝑬𝒙𝒑𝒆𝒓𝒕 𝟐,𝟑𝑒1······ 𝑒𝐿 𝑒1······ 𝑒𝐿𝑻𝒐𝒘𝒆𝒓  𝑨 𝑻𝒐𝒘𝒆𝒓  𝑩
𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐
𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟏𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐 𝑴𝒐𝑶  𝒃𝒍𝒐𝒄𝒌 𝟐𝑻𝒂𝒔𝒌  𝑨 𝑻𝒂𝒔𝒌  𝑩
𝑥1 ······ 𝑥2𝑥3 𝑥𝑀 𝑥4𝑥5𝑥6Feature value
Expert outputElement -wise prod
Gating network
Arch parameter 𝑜 MoO  output
Figure 2: The DESAN search space. The central part illustrates the search space’s overall framework, similar to multi-layer
MMoE. From bottom to top, it includes feature selection, multiple experts, hierarchical expert selection from upper to lower
layers, and task-specific expert selection. The lateral parts explain how each module works. Each mixture-of-operations
(MoO) block contains all the available operations. The feature selection module and mixture-of-experts (MoE) module identify
whether the upper-layer experts or tasks receive the lower-layer features or expert outputs.
networks. The expert subnetwork is constructed through the stack-
ing of mixture-of-operations (MoO) blocks like that in DARTS [ 29].
Task towers at the top are used to generate predictions for corre-
sponding tasks. It should be noted that the expert sharing mode
and feature selection do not increase the number of operations in
the search space, thereby ensuring the compactness of the supernet.
Collectively, the search space can be divided into three modules:
the feature selection module, the mixture-of-experts (MoE) module,
and the expert module.
3.1.1 Feature Selection Module. This module operates by select-
ing input features for experts or gating networks, aiming to mit-
igate the negative effects of task-unrelated features. As previous
works [19, 27, 40, 45, 48] have substantiated, explicit feature inter-
actions significantly contribute to the performance of recommen-
dation tasks. Consequently, we incorporate an additional feature
interaction layer within the feature selection module, specifically
for identifying potential features to produce new predictive features
using the feature interaction operation. The selected individual fea-
tures, combined with the feature after interaction, serve as inputs
for the specific expert or gating network. In our experiments, the
factorization machine (FM) [ 41] is used as the feature interaction
operation, which can also be freely replaced with other more so-
phisticated feature interaction operations.
3.1.2 Mixture-of-experts Module. This module plays a vital role in
representing the expert sharing mode, providing inputs for upper-
layer experts, gating networks, or task towers. Resembling PLE [ 46],our search space can accommodate multiple layers of mixture-
of-experts modules. The MoE module selects experts to activate
before aggregating their outputs using the gating mechanism. Each
MoE module includes a gating network responsible for aggregating
experts’ outputs, which receives its input from the feature selection
module. Formally, let 𝑒1,𝑒2,...,𝑒𝑛denote𝑛experts within the MoE
module,𝑔represent the gating network, and I⊆{ 1,2,...,𝑛}be
the indices of the currently selected experts. The output of the MoE
module can be expressed as:
𝑀𝑜𝐸(𝑥)=∑︁
𝑖∈I𝑔(𝑥𝑔)𝑖𝑒𝑖(𝑥𝑖)
where𝑔(𝑥𝑔)=softmax(𝑊𝑔𝑥𝑔),(1)
where𝑥𝑖is the input of expert 𝑒𝑖,𝑊𝑔is the trainable weight matrix
in the gating network, 𝑥𝑔is the input of the gating network, and
𝑔(𝑥𝑔)𝑖represents the aggregating weight of expert 𝑒𝑖.
3.1.3 Expert Module. This module is used to generate the archi-
tecture for each expert subnetwork. All expert subnetworks share
a common search space but can have different architectures to cap-
ture different aspects of the data. The expert module is constructed
by stacking multiple mixture-of-operations blocks. Every block in-
corporates all the available operations within the corresponding
layer, and only one operation will be finally selected. We choose
Multi-Layer Perceptrons (MLPs) with different hidden sizes as can-
didate operations in the expert module and additionally insert a
"skip-connection" operation into each mixture-of-operations block
to adaptively determine the depth of the architecture. Note that
 
1293Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
the search space can be flexibly extended by incorporating other
operations, like convolution or attention.
3.2 Search Algorithm
To efficiently explore the DESAN search space, we introduce the
progressively discretizing differentiable neural architecture search
algorithm (PD-DARTS).
3.2.1 Search Strategy for Architectures in the Expert Module. For
searching with mixture-of-operations blocks in the expert module,
a differentiable strategy is used for continuous relaxation. Let O=
{𝑜1,...,𝑜𝑀}represent𝑀candidate operations in a block 𝐵. The
mixed output of all operations can be expressed as:
𝐵(𝑥)=∑︁
𝑜∈Oexp(𝛼𝑜)Í
𝑜′∈Oexp(𝛼𝑜′)𝑜(𝑥), (2)
where𝑥denotes the input, and each operation 𝑜is associated with
a coefficient 𝛼𝑜, forming the architectural parameters 𝜶={𝛼𝑜|𝑜∈
O}. In the discretization step, the operation with the largest ar-
chitecture parameter in each block is selected, and lower-score
operations are discarded to extract the subnetwork.
3.2.2 Search Strategy for Expert Sharing Modes and Feature Selec-
tion Schemes. For expert activation in the MoE module and feature
selection in the feature selection module (i.e., the optional con-
nections in Figure 2), we propose a probabilistic activation-based
search strategy. We take the expert sharing mode search as an ex-
ample. Considering an expert 𝑒𝑖, the expert selection process can
be denoted as 𝐸𝑖(𝑥)=𝑒𝑖(𝑥𝑖)𝑠(𝛽𝑖), where
𝑠(𝛽𝑖)=(
1𝑤.𝑝. sigmoid(𝛽𝑖),
0𝑤.𝑝. 1−sigmoid(𝛽𝑖).(3)
𝑠(·)can be viewed as a selection function. Each expert in the MoE
module or each feature in the feature selection module corresponds
to a parameter 𝛽, forming architecture parameters 𝜷. During the
search process, each expert or feature is activated with the proba-
bility𝑝=sigmoid(𝛽). This stochastic selection allows all supernet
weights to be comprehensively trained during the search process. In
the discretization step, a threshold 𝜖is used to determine whether
the expert or feature is retained. Typically, 𝜖can be set to 0.5. To
optimize 𝜷through gradient descent, the gradients of 𝜷are es-
timated with the straight-through estimator (STE) [ 1,53] during
backpropagation, which is represented as:
𝜕L
𝜕𝛽𝑖=𝜕L
𝜕𝑠(𝛽𝑖)·sigmoid(𝛽𝑖)·(1−sigmoid(𝛽𝑖)). (4)
This is analogous to the model pruning literature [ 18,30] that
employs learnable thresholds to prune neural networks.
3.2.3 Single-level Optimization for Architecture Parameters and Su-
pernet Weights. Recent works [ 2,7,47] point out that the original
bi-level optimization in DARTS faces significant robustness issues.
Empirical evidence [ 3] supports that single-level optimization also
works for DARTS, achieving comparable performance to bi-level
optimization but with better efficiency and stability. Hence, the
proposed PD-DARTS algorithm utilizes single-level optimization,
simultaneously optimizing supernet weights and architecture pa-
rameters on the training data. This approach significantly enhancesthe efficiency of architecture search, making our method favorable
for large-scale recommendation tasks.
For generality, we define the multi-task loss as the weighted
summation of all associated task losses, formulated as:
L=𝜆1L1+···+𝜆𝐾L𝐾, (5)
whereL𝑖represents the loss of task 𝑖and𝜆𝑖is the corresponding
weight, such thatÍ𝐾
𝑖=1𝜆𝑖=1. For practical implementation, we
simply consider the average of all task losses as the multi-task loss.
The optimization objective of PD-DARTS is formalized as:
𝒘∗,𝜶∗,𝜷∗=arg min
𝒘,𝜶,𝜷L𝑡𝑟𝑎𝑖𝑛(𝒘,𝜷,𝜶), (6)
where 𝒘denotes the supernet weights, L𝑡𝑟𝑎𝑖𝑛 represents the train-
ing loss, and 𝜶and𝜷are architecture parameters.
3.2.4 Progressive Discretization. Moreover, the supernet will be
progressively discretized during the architecture search process,
enabling it to gradually converge to a regular network. This strat-
egy has the advantage of eliminating the necessity of retraining
the derived network from scratch, consequently reducing compu-
tational overhead. Since the derived network inherits knowledge
learned by the supernet, it generally outperforms the retrained
subnetworks. We establish a "discretization score" as the selection
criteria to determine which mixture-of-operations block, feature, or
expert is to be discretized at each step. For a mixture-of-operations
block, the discretization score is defined as the normalized entropy
of its corresponding architecture parameters 𝛼O, formalized as:
𝑑𝑠(𝛼O)=Normalize(entropy(𝛼O))
where entropy(𝛼O)=−∑︁
𝑜∈O𝛼𝑜·log(𝛼𝑜)(7)
In the case of an expert within the MoE module or a feature within
the feature selection module, the discretization score is defined as
the corresponding selection probability, 𝑑𝑠(𝛽)=sigmoid(𝛽).
Algorithm 1: PD-DARTS Algorithm
Input: Supernet and its weights 𝒘, architecture parameters
𝜶and𝜷;
Output: Resulting searched MTL model;
Data: Training dataset 𝐷𝑡𝑟𝑎𝑖𝑛 , validation dataset 𝐷𝑣𝑎𝑙;
1Initialize 𝒘,𝜶, and 𝜷;
2Warmup stage1: Train 𝒘on the training dataset for 𝑁1
epochs;
3Warmup stage2: Train 𝒘,𝜶, and 𝜷on the training dataset
for𝑁2epochs;
4while supernet not converge to a regular network do
5 Select a mixture-of-operations block, feature or expert
with the largest discretization score to discretize;
6 Train supernet weights 𝒘via training loss;
7 Train reminding undiscretized 𝜶and𝜷via training loss;
8end
9Fine-tune the derived network for 𝑁3epochs;
Algorithm 1 briefly describes the PD-DARTS algorithm. It in-
cludes two warmup stages: The first stage solely optimizes supernet
 
1294KDD ’24, August 25–29, 2024, Barcelona, Spain Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, & Yihua Huang
weights on the training data, while the second stage jointly opti-
mizes supernet weights and architecture parameters 𝜶and𝜷. These
warmup stages ensure comprehensive training of both supernet
weights and architecture parameters. In the subsequent training
process, progressive discretization is performed for the supernet.
Upon obtaining a standard network, a fine-tuning process is exe-
cuted to get the resulting MTL model.
4 Experiments
In this section, we present comprehensive experimental results
to highlight the effectiveness of our proposed AutoMTL and pro-
vide a deeper understanding of it. Specifically, we will answer the
following research questions:
•RQ1: Can the performance of models searched by AutoMTL
outperform state-of-the-art, handcrafted ones on the bench-
mark datasets?
•RQ2: How is the effectiveness and efficiency of AutoMTL’s
PD-DARTS search algorithm?
•RQ3: What are the impacts of design modules in AutoMTL’s
DESAN search space?
•RQ4: How does the number of experts in AutoMTL influence
its performance?
•RQ5: What insights can we draw from the searched archi-
tectures?
4.1 Experimental Settings
Table 1: The statistics of datasets.
Dataset #Samples #Features #Tasks
IJCAI-2015 2,143,260 7 2
UserBehavior-2017 520,757 3 4
KuaiRand-Pure 1,038,456 41 7
QB-Video 1,994,969 6 4
AliCCP 85,316,519 31 2
4.1.1 Datasets. Experiments are conducted on five public datasets:
IJCAI-20152consists of millions of anonymized users’ shopping
logs from the past six months. UserBehavior-20173is a public
dataset of anonymized user behaviors from Alibaba. KuaiRand-
Pure [17] is collected from one popular Chinese video-sharing
app, KuaiShou, containing 1million user-item interactions. QB-
Video [55] is a multi-task dataset with true-negative behaviors
released by Tencent, containing 2million interaction logs of the
video recommendation on QQ-BOW. AliCCP4is gathered from
real-world traffic logs of the recommender system in Taobao. The
statistics of these datasets are summarized in Table 1.
For AliCCP, we randomly take 20% of the training dataset to
generate the validation dataset. For other datasets, we randomly
split the data with 8 : 1 : 1 to generate the training dataset, the
validation dataset, and the testing dataset.
2https://tianchi.aliyun.com/dataset/42
3https://tianchi.aliyun.com/dataset/dataDetail?dataId=649
4https://tianchi.aliyun.com/dataset/4084.1.2 Evaluation Metric. We adopt the AUC (Area Under the ROC
Curve) score of each task as the main evaluation metric. Addition-
ally, we present the average AUC, which is the average of all task
AUC scores, to validate the overall performance of MTL methods.
It is worth noting that an improvement in the AUC at the 0.001level
is viewed as a significant improvement, which has also been pointed
out in previous works [11, 19, 48].
4.1.3 Baseline Models. We compare AutoMTL with six popular
MTL models:
•Shared-Bottom: [5] This model shares several bottom lay-
ers, following the input layer, across all tasks.
•MMoE: [33] This model shares several expert subnetworks
across all tasks and then aggregates expert representations
via gating networks for each task.
•Multi-level MMoE (ML-MMoE): This model extends the
original MMoE by incorporating multiple MoE layers. In ML-
MMoE, higher-level experts use aggregated outputs from
lower-level experts as inputs.
•PLE: [46] This model explicitly separates task-sharing and
task-specific experts and introduces a progressive routing
manner to achieve a more flexible expert sharing mode.
•AITM: [50] This framework models the sequential depen-
dence among task multi-step conversions via the Adaptive
Information Transfer (AIT) module.
•AdaTT: [26] This model leverages a residual mechanism and
a gating mechanism to adaptively fuse both shared knowl-
edge and task-specific knowledge.
4.1.4 Implementation Details. In our experiments, expert subnet-
works in MoE-based models are MLPs with hidden dimensions
of [256, 128], and task-specific towers are MLPs with hidden di-
mensions of [128, 64]. The embedding dimensions are set to 32
for AliCCP and 16for other datasets. For MoE-based models, we
maintain the number of experts at 8. For single task models, the
hidden dimensions of the bottom MLP module are set to [512, 256,
128, 64] to ensure similar numbers of parameters across all models.
We run baseline models with our experimental settings using their
open-source codes and carefully tune their hyperparameters. For
AutoMTL, expert subnetworks comprise 3layers, with the candi-
date operations [Skip-Connection, Linear, MLP-16, MLP-32, MLP-64,
MLP-128, MLP-256, MLP-512, MLP-1024], where "MLP-N" indicates
an MLP with a hidden layer dimension of N. AutoMTL incorpo-
rates two MoE module layers to align with PLE and AdaTT. All
experiments are repeated 5times with different random seeds, and
average results are reported.
4.2 Overall Performance (RQ1)
To demonstrate the effectiveness of the proposed AutoMTL, we
compare it with state-of-the-art, handcrafted models and single-
task models. The performance comparisons are shown in Table 2,
wherein both the AUC scores and their respective average for each
dataset are reported. From the comparisons, we can observe that:
(1) Advanced models that possess a more flexible expert sharing
mode often achieve better performance. This indicates the necessity
of designing flexible expert sharing modes for multi-task learning.
 
1295Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Overall Performance Comparison. "Avg" means the average AUC scores of different tasks. The best results are
highlighted in bold. The second-best results are marked by underline.∗denotes statistically significant improvement (measured
by t-test with 𝑝-value <0.005) over the best baselines.
Metho
dUserBehavior-2017 IJCAI-2015 KuaiRand-Pur
e QB-
Video AliCCP
AUC1
AUC2 AUC3 AUC4 Avg AUC1
AUC2 Avg AUC1
AUC2 AUC3 AUC4 AUC5 AUC6 AUC7 Avg AUC1
AUC2 AUC3 AUC4 Avg AUC1
AUC2 Avg
Single-
Task 0.7262
0.7350 0.7085 0.8463 0.7540 0.7681
0.8359 0.8020 0.7576
0.8368 0.7290 0.7045 0.7198 0.9006 0.7687 0.7738 0.9713
0.8743 0.8939 0.7404 0.8700 0.6260
0.6162 0.6211
Shared-Bottom 0.7278
0.7340 0.7066 0.8436 0.7544 0.7706
0.8398 0.8052 0.7560
0.8383 0.7267 0.7118 0.7782 0.8858 0.7754 0.7817 0.9726
0.8782 0.8974 0.7576 0.8765 0.6150
0.6232 0.6191
MMoE 0.7323 0.7361
0.7071 0.8472 0.7557 0.7705
0.8394 0.8049 0.7592 0.8449
0.7406 0.7175
0.7336 0.9037 0.7763 0.7823 0.9725
0.8871 0.9019 0.7565 0.8795 0.6275 0.6422
0.6348
ML-MMoE 0.7311 0.7380 0.7084
0.8456 0.7558 0.7713
0.8408 0.8060 0.7560
0.8399 0.7141 0.7386 0.7426 0.8720 0.7754 0.7823 0.9731 0.8832
0.8931 0.7678 0.8793 0.6214
0.6477 0.6346
PLE 0.7310
0.7368 0.7088
0.8470 0.7559 0.7725
0.8432 0.8079 0.7570 0.8539 0.7380
0.6982 0.7645 0.8968 0.7743 0.7832 0.9724 0.8930 0.8958
0.7528 0.8785 0.6181
0.6441 0.6311
AITM 0.7314
0.7322 0.7075 0.8479 0.7548 0.7689
0.8384 0.8036 0.7582 0.8463
0.7053 0.7498 0.7382
0.9067 0.7736
0.7826 0.9724
0.8755 0.9030 0.7591
0.8775 0.6240
0.6426 0.6333
AdaTT 0.7305
0.7364 0.7096 0.8487 0.7563 0.7743 0.8453 0.8098 0.7514
0.8294 0.6960 0.7522 0.8154 0.8982 0.7649 0.7868 0.9726
0.8878 0.8989 0.7652 0.8811 0.6282 0.6464
0.6373
A
utoMTL 0.7323 0.7366 0.7101∗0.8514∗0.7576∗0.7824∗0.8594∗0.8209∗0.7568
0.8521 0.8027∗0.7000
0.7856 0.9070∗0.7772∗0.7974∗0.9730 0.8896 0.9067∗0.7823∗0.8879∗0.6227 0.6569∗0.6398∗
Table 3: The performance of AutoMTL and its two variants
related to the search algorithm. "UB" denotes UserBehavior-
2015. Average AUCs are reported.
Dataset UB
IJCAI KuaiRand QB AliCCP
A
utoMTL 0.7576
0.8209 0.7974 0.8879 0.6398
w/o PD 0.7513
0.8203 0.7821 0.8673 0.6316
Bi.Opt 0.7551
0.8207 0.7722 0.8750 0.6326
SNAS 0.7564
0.8191 0.7914 0.8816 0.6387
(2) State-of-the-art, handcrafted MTL models cannot always outper-
form single-task models. It underlines the prevalence of negative
transfer within MTL. (3) No baseline multi-task model can consis-
tently outperform others on all datasets and tasks, indicating that
improving the performance of MTL is non-trivial due to complex
correlations among tasks. This fact also reveals that a universal
expert sharing mode is not applicable to all scenarios, thereby
stressing the necessity of designing data- and task-specific expert
sharing modes. (4) AutoMTL can achieve the best performance
on all datasets and the majority of tasks, and is adaptable for
a large number of tasks like KuaiRand-Pure as well as large-scale
recommendation data like AliCCP.
4.3 Effectiveness and Efficiency of Search
Algorithm (RQ2)
To verify the effectiveness of PD-DARTS algorithm, we design three
variants of AutoMTL: (1) "Without progressive discretization" (w/o
PD) no longer uses progressive discretization, while the resulting
networks are retrained after exporting the entire architecture at
once. (2) "Bi-level optimization" (Bi.Opt) uses bi-level optimization
in AutoMTL instead of single-level optimization. (3) SNAS [ 51] uses
Gumbel-Softmax and single-level optimization.
4.3.1 Effectiveness of Search Algorithm. Table 3 shows the perfor-
mance of AutoMTL and its three variants related to the search
algorithm. These results demonstrate that progressive discretiza-
tion can enhance the performance of AutoMTL. Employing this
technique allows the exported subnetworks to gradually inherit
the knowledge learned in the supernet. This not only eliminates an
additional retraining process but also achieves performance gains
against the retrained subnetworks. This is analogous to the model
pruning literature, where a network pruned from a larger one often
outperforms the network trained from scratch [28].
UserBehavior IJCAI KuaiRand QB-Video AliCCP
Datasets0.00.51.01.52.02.53.03.5GPU HoursMMoE
PLE
AdaTT
AutoMTL
AutoMTL w/o PD
AutoMTL Bi.OptFigure 3: Runtime of baselines, AutoMTL, and its variants.
AutoMTL using single-level optimization also outperforms meth-
ods that utilize bi-level optimization in terms of performance. Rel-
ative to bi-level optimization, single-level optimization has the
following advantages: (1) It offers more stable training, as many
studies have already confirmed that the bi-level optimization of
DARTS is prone to stability issues; (2) It enables more effective use
of training data, allowing for comprehensive training of supernet
weights and architectural parameters; (3) The use of techniques like
progressive discretization in single-level optimization can eliminate
the requirement for a retraining process.
4.3.2 Efficiency of Search Algorithm. Beyond the achievement in
performance, we additionally report the search cost of AutoMTL to
demonstrate its search efficiency. As shown in Figure 3, we present
the runtime required to train several baseline models and to execute
AutoMTL on each dataset. All experiments are run on an NVIDIA
3090 GPU. From these results, we derive three key insights: (1) The
search cost of AutoMTL is almost on the same order of magnitude
as training a single model; (2) Single-level optimization notably out-
performs its bi-level counterpart in efficiency; (3) The progressive
discretization of AutoMTL considerably enhances search efficiency
as the supernet gradually shrinks during the search process, elimi-
nating the need for further retraining. The superior search efficiency
makes AutoMTL applicable to large-scale recommendations.
4.4 Effectiveness of Search Space (RQ3)
To verify the effectiveness of different design modules in the pro-
posed DESAN search space, we design the following variants: ho-
mogeneous expert architectures (Hom.Exp), expert full-sharing
mode (Full.Shr), and without feature selection module (w/o FS). The
performance of these variants is presented in Table 4. It can be seen
that there is a degree of reduction in the overall performance when
 
1296KDD ’24, August 25–29, 2024, Barcelona, Spain Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, & Yihua Huang
Table 4: Ablation study about search space. The best results are highlighted in bold.
Metho
dUserBehavior-2017 IJCAI-2015 KuaiRand-Pur
e QB-
Video AliCCP
AUC1
AUC2 AUC3 AUC4 Avg AUC1
AUC2 Avg AUC1
AUC2 AUC3 AUC4 AUC5 AUC6 AUC7 Avg AUC1
AUC2 AUC3 AUC4 Avg AUC1
AUC2 Avg
A
utoMTL 0.7323
0.7366 0.7101 0.8514 0.7576 0.7824 0.8594
0.8209 0.7568
0.8521 0.8027 0.7000 0.7856 0.9070 0.7772 0.7974 0.9730
0.8896 0.9067 0.7823 0.8879 0.6227
0.6569 0.6398
Hom.Exp 0.7292
0.7346 0.7053 0.8430 0.7530 0.7826 0.8582
0.8204 0.7578 0.8472
0.7562 0.6931 0.7795 0.9036 0.7771 0.7878 0.9724 0.8976 0.9037
0.7648 0.8846 0.6190
0.6430 0.6310
Full.Shr 0.7275
0.7347 0.7028 0.8369 0.7505 0.7817
0.8571 0.8194 0.7565 0.8557 0.7401 0.7114 0.7119
0.8990 0.7757 0.7786 0.9731 0.8818
0.9024 0.7438 0.8753 0.6222
0.6473 0.6347
w/o FS 0.7289
0.7358 0.7079 0.8475 0.7550 0.7813
0.8581 0.8197 0.7578 0.8556
0.7668 0.6400 0.7261 0.8738 0.7767 0.7710 0.9730
0.8814 0.8990 0.7615 0.8787 0.6213
0.6387 0.6300
these modules are removed from AutoMTL. Next, we discuss each
of these variants in detail.
4.4.1 Heterogeneous Experts. To verify the impact of heteroge-
neous experts, we force all experts to share a homogeneous archi-
tecture (Hom.Exp) during the search process. As shown in Table 4,
the performance of AutoMTL decreases on all datasets and most
tasks. This confirms the advantage of heterogeneous experts for
MoE-based MTL models, as they can easily learn diverse represen-
tations that reflect different aspects of the data. We believe that the
small performance reduction may stem from the current multi-task
recommendation data not being sufficiently complex to require
diverse representations to cover the characteristics of the data.
4.4.2 Expert Sharing Mode. To verify the effectiveness of the data-
specific expert sharing mode, we do not search for the expert shar-
ing mode in this experiment but only use the full-sharing mode,
which is the same as MMoE (Full.Shr). It is evident from Table 4
that the elimination of the expert sharing mode search consider-
ably influences AutoMTL’s performance. Given that the expert
sharing mode search is a fundamental constituent of AutoMTL, it
can mitigate the negative transfer problem by obtaining flexible
expert sharing modes. This further emphasizes the importance of
designing flexible expert sharing modes for multi-task learning.
4.4.3 Feature Selection Module. The feature selection module in-
cludes expert-specific feature selection and a feature interaction
layer. As depicted in Figure 5, the architectures identified by Au-
toMTL display that different experts would select different features
as input, and several insignificant features are discarded. As numer-
ous works have already proven the role of feature interaction in
recommendation tasks, we remove the feature selection but retain
the feature interaction layer (w/o FS). Consequently, all experts are
provided with all the original features and the features after interac-
tion as input. As shown in Table 4, the omission of feature selection
also has a considerable influence on AutoMTL’s performance. Fea-
ture selection facilitates the reduction of negative transfer issues
through the careful selection of expert-specific input features, si-
multaneously alleviating the adverse impacts posed by irrelevant
features.
4.5 Impact of the Number of Experts (RQ4)
In this section, we further verify the impact of the number of experts
on AutoMTL’s performance. An increase in the number of experts
augments the complexity of the multi-task model’s architecture,
triggering a marked expansion in the search space of AutoMTL.
Figure 4 illustrates that despite an increase in the number of experts,
AutoMTL’s performance remains consistently steady with slight
fluctuations. This indicates that, on the one hand, as AutoMTL can
adaptively search for the expert sharing mode, its performance is
4 5 6 7 8 9 10 11 12
Number of experts0.760.780.800.820.84Avg.AUC
Avg.AUC(a) KuaiRand-Pure
4 5 6 7 8 9 10 11 12
Number of experts0.850.860.870.880.890.90Avg.AUC
Avg.AUC (b) QB-Video
Figure 4: Average AUC scores with different numbers of ex-
perts in AutoMTL.
less affected by the number of experts. On the other hand, in multi-
task recommendation, by reasonably designing the expert sharing
mode, even a few expert subnetworks can achieve competitive
performance.
4.6 Case Study (RQ5)
Figure 5 presents the optimal architectures searched by AutoMTL
for the UserBehavior-2017 and QB-Video datasets. More architec-
tures searched by AutoMTL can be found in Appendix A. The
architectures can be understood from three facets: expert-specific
feature selection, significantly more complicated expert sharing
modes than manually designed ones, and task-specific expert routes.
From the resulting architectures, we can derive the following
insights: (1) In multi-task recommendations, feature selection and
feature interaction are almost always effective and can be included
in architectural design considerations. (2) While designing hetero-
geneous experts manually poses significant challenges, the adap-
tation of diverse architectures for experts across different MoE
layers is feasible, such as the utilization of a more compact expert
architecture on the first or second layer. (3) The complexity of the
architecture can be adjusted based on dataset characteristics or data
distributions. In essence, AutoMTL is a data-specific method that
aims to discover the most fitting architectures for given datasets.
Derived insights could potentially guide human experts in design-
ing robust MTL models that account for diversity among experts
and flexible expert sharing modes.
4.7 In-Depth Analysis
4.7.1 The Ability in Alleviating Negative Transfer. To further demon-
strate AutoMTL’s ability in alleviating the negative transfer prob-
lem, we construct synthetic data with different task correlations,
following the methodology described in MMoE [ 33]. Generally, the
lower the correlation between tasks, the more severe the negative
transfer problem becomes. The performance of different MTL meth-
ods on these synthetic datasets is shown in Table 5. The results
 
1297Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
𝑥1𝑥2𝑥3MLP -16MLP -16
MLP -16MLP -16
MLP -16MLP -64
MLP -256MLP -16MLP -16Task 1
MLP -64Task 2
MLP -256Task 3
MLP -32Task 4
FMMLP -32
(a) The architecture searched on UserBehavior-2017.
𝑥1𝑥2𝑥3MLP -16MLP -16
MLP -256MLP -16
MLP -16MLP -16
MLP -256MLP -256MLP -128Task 1
MLP -32Task 2
MLP -128Task 3
MLP -64Task 4
FMLinearMLP -512 MLP -16 MLP -64MLP -512
𝑥4𝑥5𝑥6
(b) The architecture searched on QB-Video
Figure 5: The architectures searched by AutoMTL.
Table 5: Performance comparison on synthetic datasets. "Cor-
relation" denotes Pearson correlation between different tasks.
Average mean square errors (MSEs) are reported.
Correlation 0 0.5 0.9
Shared-Bottom 1.0556 0.6105 0.0343
MMoE 0.5043 0.0908 0.0316
PLE 0.1934 0.0719 0.0229
AutoMTL 0.1218 0.0277 0.0116
indicate that AutoMTL performs even better when task correlations
are weak.
4.7.2 Comparison with NAS-based MTL Methods. Table 6 shows
the performance comparison of AutoMTL with the other two NAS-
based MTL methods. SNR [ 32] modularizes the shared low-level hid-
den layers into subnets and controls the connections of subnets to
achieve flexible parameter sharing. We refer to a non-official imple-
mentation of SNR5. AESM2[60] is designed for multi-scenario and
multi-task learning, realizing expert selection through the sparse
activation of experts, of which we extract the multi-task learning
component. As shown in Table 6, AutoMTL retains performance
advantages when compared with these methods.
5https://github.com/tomtang110/MultitaskTable 6: Performance comparison between AutoMTL and two
NAS-based MTL methods. "UB" denotes UserBehavior-2015.
Average AUCs are reported.
Dataset UB
IJCAI KuaiRand QB AliCCP
SND 0.7546
0.8067 0.7819 0.8789 0.6304
AESM20.7519
0.8054 0.7473 0.8708 0.6278
AutoMTL 0.7576
0.8209 0.7974 0.8879 0.6398
Table 7: The performance of AutoMTL with extended search
space. Average AUCs are reported.
Dataset UB
IJCAI KuaiRand QB AliCCP
+Conv1D 0.7568
0.8192 0.7955 0.8792 0.6382
AutoMTL 0.7576
0.8209 0.7974 0.8879 0.6398
4.7.3 Extended Search Space. AutoMTL’s DESAN search space can
be flexibly extended by introducing new operations into its mixture-
of-operations (MoO) blocks. To verify the extensibility of the search
space, we incorporate 1D convolutions with varying kernel sizes
into the original search space and then perform the neural architec-
ture search. As presented in Table 7, the performance of AutoMTL
within this extended search space is slightly lower compared to the
original. This is because 1D convolutions are not well adaptive for
multi-task recommendation tasks, and the complexity of NAS has
also increased after extending the search space.
5 Conclusion
In this work, we proposed a NAS-based MTL framework called
AutoMTL. Through the MoE-based dual-level expert sharing mode
and architecture navigator (DESAN) search space, AutoMTL can
not only effectively explore expert sharing modes and feature selec-
tion schemes of MTL models but also enhance the performance of
MTL by deriving heterogeneous expert subnetworks. The progres-
sively discretizing differentiable architecture search (PD-DARTS)
algorithm can efficiently explore this search space, yielding high-
performance multi-task models. Extensive experimental results
reveal that AutoMTL consistently outperforms existing state-of-the-
art, human-crafted MTL models. Comprehensive ablation studies
further confirm the efficacy of all facets within AutoMTL’s design.
Additionally, the multi-task architectures obtained by AutoMTL can
provide practitioners valuable insights for designing superior MTL
models. Although we designed AutoMTL for multi-task recommen-
dations, it has the potential to be a general solution, extendable to
other fields through modification or expansion of the search space.
Acknowledgments
This work was supported by the National Natural Science Foun-
dation of China (#62102177), the Natural Science Foundation of
Jiangsu Province (#BK20210181), and the Collaborative Innovation
Center of Novel Software Technology and Industrialization, Jiangsu,
China. Special thanks to Yue Wang, co-author of the paper, for his
significant contributions to the experiments and writing.
 
1298KDD ’24, August 25–29, 2024, Barcelona, Spain Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, & Yihua Huang
References
[1]Yoshua Bengio. 2013. Estimating or propagating gradients through stochastic
neurons. arXiv preprint arXiv:1305.2982 (2013).
[2]Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, and Qi Tian. 2019.
Stabilizing darts with amended gradient estimation on architectural parameters.
arXiv preprint arXiv:1910.11831 (2019).
[3]Kaifeng Bi, Lingxi Xie, Xin Chen, Longhui Wei, and Qi Tian. 2020. Gold-nas:
Gradual, one-level, differentiable. arXiv preprint arXiv:2007.03331 (2020).
[4]Han Cai, Ligeng Zhu, and Song Han. 2018. Proxylessnas: Direct neural archi-
tecture search on target task and hardware. arXiv preprint arXiv:1812.00332
(2018).
[5]Rich Caruana. 1993. Multitask Learning: A Knowledge-Based Source of Inductive
Bias. In Proceedings of the Tenth International Conference on International Confer-
ence on Machine Learning (Amherst, MA, USA) (ICML’93). Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 41–48.
[6]Xiaokai Chen, Xiaoguang Gu, and Libo Fu. 2021. Boosting share routing for
multi-task learning. In Companion Proceedings of the Web Conference 2021. 372–
379.
[7]Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. 2021. Progressive darts: Bridging the
optimization gap for nas in the wild. International Journal of Computer Vision
129 (2021), 638–655.
[8]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018.
Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask
networks. In International conference on machine learning. PMLR, 794–803.
[9]Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar,
Yuning Chai, and Dragomir Anguelov. 2020. Just pick a sign: Optimizing deep
multitask models with gradient sign dropout. Advances in Neural Information
Processing Systems 33 (2020), 2039–2050.
[10] Zhongde Chen, Ruize Wu, Cong Jiang, Honghui Li, Xin Dong, Can Long, Yong
He, Lei Cheng, and Linjian Mo. 2022. CFS-MTL: A Causal Feature Selection
Mechanism for Multi-task Learning via Pseudo-intervention. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management.
3883–3887.
[11] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[12] Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. 2020. Fair darts:
Eliminating unfair advantages in differentiable architecture search. In European
conference on computer vision. Springer, 465–480.
[13] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan-
guage processing: Deep neural networks with multitask learning. In Proceedings
of the 25th international conference on Machine learning. 160–167.
[14] Li Deng, Geoffrey Hinton, and Brian Kingsbury. 2013. New types of deep neural
network learning for speech recognition and related applications: An overview.
In2013 IEEE international conference on acoustics, speech and signal processing.
IEEE, 8599–8603.
[15] Ke Ding, Xin Dong, Yong He, Lei Cheng, Chilin Fu, Zhaoxin Huan, Hai Li, Tan Yan,
Liang Zhang, Xiaolu Zhang, et al .2021. MSSM: a multiple-level sparse sharing
model for efficient multi-task learning. In Proceedings of the 44th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
2237–2241.
[16] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural architecture
search: A survey. The Journal of Machine Learning Research 20, 1 (2019), 1997–
2017.
[17] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng
Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommen-
dation Dataset with Randomly Exposed Videos. In Proceedings of the 31st ACM
International Conference on Information and Knowledge Management (Atlanta,
GA, USA) (CIKM ’22). 3953–3957. https://doi.org/10.1145/3511808.3557624
[18] Shangqian Gao, Feihu Huang, Jian Pei, and Heng Huang. 2020. Discrete model
compression with resource constraint for deep neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition. 1899–1908.
[19] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[20] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei,
and Jian Sun. 2020. Single path one-shot neural architecture search with uniform
sampling. In European Conference on Computer Vision. Springer, 544–560.
[21] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher.
2016. A joint many-task model: Growing a neural network for multiple NLP
tasks. arXiv preprint arXiv:1611.01587 (2016).
[22] Yun He, Xue Feng, Cheng Cheng, Geng Ji, Yunsong Guo, and James Caverlee. 2022.
MetaBalance: Improving Multi-Task Recommendations via Adapting Gradient
Magnitudes of Auxiliary Tasks. In Proceedings of the ACM Web Conference 2022.
2205–2215.
[23] Adrián Javaloy and Isabel Valera. 2021. Rotograd: Gradient homogenization in
multitask learning. arXiv preprint arXiv:2103.02631 (2021).[24] Shen Jiang, Zipeng Ji, Guanghui Zhu, Chunfeng Yuan, and Yihua Huang. 2023.
Operation-Level Early Stopping for Robustifying Differentiable NAS. In Proceed-
ings of the Thirty-seventh Conference on Neural Information Processing Systems.
[25] Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 7482–7491.
[26] Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, Chaofei
Yang, Xi Liu, and Jiyan Yang. 2023. AdaTT: Adaptive Task-to-Task Fusion Network
for Multitask Learning in Recommendations. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 4370–4379.
[27] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 1754–1763.
[28] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. 2021.
Pruning and quantization for deep neural network acceleration: A survey. Neu-
rocomputing 461 (2021), 370–403.
[29] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable
architecture search. arXiv preprint arXiv:1806.09055 (2018).
[30] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable
embedding sizes for recommender systems. arXiv preprint arXiv:2101.07577
(2021).
[31] Shikun Liu, Edward Johns, and Andrew J Davison. 2019. End-to-end multi-task
learning with attention. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. 1871–1880.
[32] Jiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H Chi. 2019. SNR:
sub-network routing for flexible parameter sharing in multi-task learning. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 216–223.
[33] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining. 1930–1939.
[34] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. 1137–1140.
[35] Krzysztof Maziarz, Efi Kokiopoulou, Andrea Gesmundo, Luciano Sbaiz, Gabor
Bartok, and Jesse Berent. 2019. Gumbel-matrix routing for flexible multi-task
learning. (2019).
[36] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016.
Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 3994–4003.
[37] Xiaofeng Pan, Yibin Shen, Jing Zhang, Keren Yu, Hong Wen, Shui Liu, Chengjun
Mao, and Bo Cao. 2021. SAME: Scenario Adaptive Mixture-of-Experts for
Promotion-Aware Click-Through Rate Prediction. arXiv preprint arXiv:2112.13747
(2021).
[38] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient
neural architecture search via parameters sharing. In International Conference on
Machine Learning. PMLR, 4095–4104.
[39] Zhen Qin, Yicheng Cheng, Zhe Zhao, Zhe Chen, Donald Metzler, and Jingzheng
Qin. 2020. Multitask mixture of sequential experts for user activity streams.
InProceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 3083–3091.
[40] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In 2016 IEEE
16th International Conference on Data Mining (ICDM). IEEE, 1149–1154.
[41] Steffen Rendle. 2010. Factorization Machines. In 2010 IEEE International Confer-
ence on Data Mining. 995–1000. https://doi.org/10.1109/ICDM.2010.127
[42] Sebastian Ruder. 2017. An overview of multi-task learning in deep neural net-
works. arXiv preprint arXiv:1706.05098 (2017).
[43] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Søgaard. 2017.
Sluice networks: Learning what to share between loosely related tasks. arXiv
preprint arXiv:1705.08142 2 (2017).
[44] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).
[45] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-
attentive neural networks. In Proceedings of the 28th ACM International Conference
on Information and Knowledge Management. 1161–1170.
[46] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive
layered extraction (ple): A novel multi-task learning (mtl) model for personalized
recommendations. In Fourteenth ACM Conference on Recommender Systems. 269–
278.
[47] Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui
Hsieh. 2021. Rethinking architecture selection in differentiable NAS. arXiv
preprint arXiv:2108.04392 (2021).
 
1299Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
𝑥1𝑥2𝑥3𝑥4𝑥5𝑥6FMMLP -32 MLP -32 MLP -32 MLP -16 MLP -16MLP -16 MLP -32 MLP -32 MLP -32 MLP -16MLP -32Task 1 Task 2
Figure 6: The architecture searched on IJCAI-2015.
𝑥1𝑥2… 𝑥41FMMLP -32 MLP -16MLP -1024 MLP -512 Linear MLP -32 MLP -32Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7
MLP -1024 MLP -1024 MLP -512 MLP -64
MLP -32 MLP -64 MLP -256MLP -128
MLP -16 MLP -64
Figure 7: The architecture searched on KuaiRand-Pure.
𝑥1𝑥2… 𝑥31FMLinear Linear Linear Linear LinearTask 1 Task 2
MLP -16 MLP -32 Identity MLP -32 MLP -16
Figure 8: The architecture searched on AliCCP.[48] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDD’17. 1–7.
[49] Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming
Tang, and Zhenhua Dong. 2023. Single-shot Feature Selection for Multi-task
Recommendations. In Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 341–351.
[50] Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang,
and Yu Chen. 2021. Modeling the sequential dependence among audience multi-
step conversions with multi-task learning in targeted display advertising. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining. 3745–3755.
[51] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. 2018. SNAS: stochastic
neural architecture search. arXiv preprint arXiv:1812.09926 (2018).
[52] Quanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu. 2020. Efficient neural
architecture search via proximal iterations. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 34. 6664–6671.
[53] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and
Jack Xin. 2019. Understanding straight-through estimator in training activation
quantized neural nets. arXiv preprint arXiv:1903.05662 (2019).
[54] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman,
and Chelsea Finn. 2020. Gradient Surgery for Multi-Task Learning. In Advances
in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 5824–
5836.
[55] Guanghu Yuan, Fajie Yuan, Yudong Li, Beibei Kong, Shujie Li, Lei Chen, Min Yang,
Chenyun Yu, Bo Hu, Zang Li, et al .2022. Tenrec: A large-scale multipurpose
benchmark dataset for recommender systems. Advances in Neural Information
Processing Systems 35 (2022), 11480–11493.
[56] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2014. Facial
landmark detection by deep multi-task learning. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part VI 13. Springer, 94–108.
[57] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews,
Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019.
Recommending what video to watch next: a multitask ranking system. In Pro-
ceedings of the 13th ACM Conference on Recommender Systems. 43–51.
[58] Guanghui Zhu, Feng Cheng, Defu Lian, Chunfeng Yuan, and Yihua Huang. 2022.
NAS-CTR: efficient neural architecture search for click-through rate prediction.
InProceedings of the 45th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 332–342.
[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement
learning. arXiv preprint arXiv:1611.01578 (2016).
[60] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, and
Aixin Sun. 2022. Automatic expert selection for multi-scenario and multi-task
search. In Proceedings of the 45th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 1535–1544.
A Additional Architectures Searched by
AutoMTL
In this section, we present more architectures derived by AutoMTL
in Figure 6, Figure 7, and Figure 8 to potentially provide more
insights about architecture design.
 
1300