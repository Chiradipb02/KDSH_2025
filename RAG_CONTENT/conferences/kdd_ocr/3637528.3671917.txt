Graph Condensation for Open-World Graph Learning
Xinyi Gao
The University of Queensland
Brisbane, Australia
xinyi.gao@uq.edu.auTong Chen
The University of Queensland
Brisbane, Australia
tong.chen@uq.edu.auWentao Zhang
Peking University
Beijing, China
wentao.zhang@pku.edu.cn
Yayong Li
Data 61, CSIRO
Brisbane, Australia
yayongli@outlook.comXiangguo Sun
The Chinese University of Hong Kong
Hong Kong, China
xiangguosun@cuhk.edu.hkHongzhi Yin∗
The University of Queensland
Brisbane, Australia
h.yin1@uq.edu.au
Abstract
The burgeoning volume of graph data presents significant com-
putational challenges in training graph neural networks (GNNs),
critically impeding their efficiency in various applications. To tackle
this challenge, graph condensation (GC) has emerged as a promis-
ing acceleration solution, focusing on the synthesis of a compact
yet representative graph for efficiently training GNNs while re-
taining performance. Despite the potential to promote scalable use
of GNNs, existing GC methods are limited to aligning the con-
densed graph with merely the observed static graph distribution.
This limitation significantly restricts the generalization capacity of
condensed graphs, particularly in adapting to dynamic distribution
changes. In real-world scenarios, however, graphs are dynamic and
constantly evolving, with new nodes and edges being continually
integrated. Consequently, due to the limited generalization capac-
ity of condensed graphs, applications that employ GC for efficient
GNN training end up with sub-optimal GNNs when confronted
with evolving graph structures and distributions in dynamic real-
world situations. To overcome this issue, we propose open-world
graph condensation (OpenGC), a robust GC framework that inte-
grates structure-aware distribution shift to simulate evolving graph
patterns and exploit the temporal environments for invariance con-
densation. This approach is designed to extract temporal invariant
patterns from the original graph, thereby enhancing the gener-
alization capabilities of the condensed graph and, subsequently,
the GNNs trained on it. Furthermore, to support the periodic re-
condensation and expedite condensed graph updating in life-long
graph learning, OpenGC reconstructs the sophisticated optimiza-
tion scheme with kernel ridge regression and non-parametric graph
convolution, significantly accelerating the condensation process
while ensuring the exact solutions. Extensive experiments on both
real-world and synthetic evolving graphs demonstrate that OpenGC
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671917outperforms state-of-the-art (SOTA) GC methods in adapting to
dynamic changes in open-world graph environments.
CCS Concepts
•Computing methodologies →Neural networks.
Keywords
Graph Condensation, Open-World Graph, Temporal Generalization
ACM Reference Format:
Xinyi Gao, Tong Chen, Wentao Zhang, Yayong Li, Xiangguo Sun, and Hongzhi
Yin. 2024. Graph Condensation for Open-World Graph Learning. In Proceed-
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671917
1 Introduction
Graph data [ 8,36,43] is used to represent complex structural re-
lationships among various entities and has enabled applications
across a diverse range of domains, such as chemical molecules
[11], social networks [ 16,21,26,30,37], and recommender systems
[27,45,47]. However, the exponential growth of data volume in
these applications poses significant challenges in data storage, trans-
mission, and particularly the training of graph neural networks
(GNNs) [ 7,22,33,50]. These challenges become more pronounced
in scenarios that require training multiple GNN models, such as
neural architecture search [ 48], continual learning [ 34,43], and fed-
erated learning [ 32]. Consequently, there is a pressing need for more
efficient methodologies for processing large-scale graph data. In re-
sponse, graph condensation (GC) [ 6,14,15,29] has emerged, aiming
to synthesize a compact (e.g., 1,000×smaller) yet informative graph
that captures essential characteristics of the large original graph.
The condensed graph enables fast training of numerous GNNs bear-
ing different architectures and hyper-parameters, while ensuring
their performance is comparable to the ones trained on the original
graph. As such, in the deployment stage, these trained GNNs can
directly perform inference on the original graph to support various
downstream tasks.
Despite the potential to accelerate model training, current GC
methods still fall short in their real-world practicality. By default,
conventional GC methods are subsumed under a static setting,
which requires that a large graph is firstly condensed to facilitate
GNN training on the small graph, and the trained GNNs are then
 
851
KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
𝒯1
𝑉1𝑉1 𝑉2𝒯2
𝑉1 𝑉2𝑉3𝒯3
𝑇1 𝑇2 𝑇3…
…
T1T2T3T4T5T6
Tasks7274767880828486Accuracy (%)Yelp
T1T2T3T4T5T6
Tasks868890929496Accuracy (%)Taobao
Figure 1: The upper panel presents the evolution of the graph.
The graphT𝑖expands as tasks 𝑇𝑖progress. Varying colors of
nodes represent distinct classes. The lower panel shows the
test accuracy of consecutive tasks on the Yelp and Taobao
datasets. The test model is GCN, which is trained on the
condensed graph of the initial task 𝑇1and applied to evaluate
subsequent tasks without fine-tuning. The test set expands
following the tasks, and the evaluation is limited to the nodes
belonging to the classes in 𝑇1.
deployed on the same large graph for testing. Unfortunately, as-
suming the large graph stays unchanged throughout the entire
process severely contradicts the dynamic and evolving nature of
graph-structured data in the real world. In fact, graphs in many
high throughput applications are inherently open-world [2,4,9,40],
where new nodes and classes continuously emerge and are inte-
grated into the existing graph structure, as depicted in Figure 1. This
phenomenon is exemplified in citation networks [ 13,31], where
some new papers explore established topics, while others venture
into emerging areas. These papers in the novel areas are often de-
veloped from previous studies and cite a range of related literature.
Such addition of nodes often introduces novel patterns that are
distinctly different from those previously observed, increasing the
graph’s complexity and diversity. Consequently, in addition to pre-
serving performance on the initial large graph, GNNs trained on
condensed graphs are expected to exhibit adaptability to novel pat-
terns that emerge within dynamic environments. For instance, GC
for neural architecture search [ 3,52] initially condenses a snapshot
of the original graph, thereafter employing the condensed graph
to accelerate the searching procedure and identify optimal model
architecture. However, the evolving nature of graphs leads to a
discrepancy between the model deployment environment and the
condensed graph snapshot. Therefore, optimal model architecture
identified based on condensed graphs should sustain its superior
performance over time, even as the graph evolves. In a nutshell,
the critical problem that arises for GC in open-world scenarios is:
“How can GC methods be adapted to handle the dynamic nature of
evolving graphs, ensuring that GNNs trained on condensed graphs
remain accurate and robust in the face of continual changes in graph
structures? ”Specifically, the application of GC within open-world graph en-
vironments encounters two primary challenges. The first challenge
arises from the distribution shift caused by the constant addition
of new nodes, which may either belong to existing categories or
introduce entirely new classes. On the one hand, these nodes typi-
cally exhibit distributions that diverge from existing ones. On the
other hand, their integration, particularly when involving novel
classes, can modify the distribution patterns of previously observed
nodes through their connections. However, the condensed graph
created by existing GC methods, intended to act as a simple data
simulator capturing a static view of the original graph’s distribu-
tion, inherently constrains the generalization capacities of GNNs
trained on these graphs to distribution shifts. To assess the effect of
newly added nodes to GC, we simulate the deployment of GC on
two real-world, progressively evolving graphs: Yelp and Taobao1.
The GCN [ 17] is trained on the condensed graph of the initial task
𝑇1and applied to evaluate distinct test sets in subsequent tasks
without fine-tuning. As depicted in Figure 1, there was a noticeable
decline in the classification performance of test nodes in each task,
highlighting the distribution shift caused by newly added nodes
and the limited adaptability of GC within dynamic graphs.
In the meantime, the second challenge involves intensive com-
putation in the condensation process. The continuous addition of
new nodes and subsequent distribution changes necessitate peri-
odic re-condensation to refresh and realign the condensed graph
with evolving data distributions. However, the condensation pro-
cess is often complex and slow to converge [ 15], resulting in a
time-consuming procedure that hampers efficient life-long graph
data management. In GC approaches, the process begins by en-
coding both the large original graph and the condensed graph
through a relay model. Subsequently, the relay model, along with
the condensed graph, is updated utilizing a nested loop optimiza-
tion strategy. This GC paradigm is implemented iteratively until
convergence is achieved. However, the iterative encoding of the
large-scale original graph, as well as the sophisticated nested loop
optimization, inherently demands substantial time and extensive
computational resources. Consequently, these intensive computa-
tions pose a significant obstacle to the serving of GC in evolving
and life-long graph systems.
In light of these challenges, we propose a novel graph conden-
sation approach, Open -wold graph condensation (OpenGC), facili-
tating trained downstream GNNs to handle evolving graphs in the
open-world scenarios. To tackle the distribution shift issue, we pro-
pose temporal invariance condensation that incorporates invariant
learning [ 1] to preserve the invariant patterns across different tem-
poral environments in the condensed graph. To this end, a temporal
data augmentation technique is designed to simulate the graph’s
evolving pattern and exploit the structure-aware distribution shift
by referring to the historic graph. It specifically considers the sim-
ilarity of node pairs and targets the low-degree nodes, which are
more susceptible to the influence of newly added neighbors. By
this means, OpenGC can significantly enhance the adaptability of
downstream GNNs by training them on the temporally generalized
condensed graph, thus eliminating the need to laboriously design
1We construct these datasets according to actual timestamps as detailed in Section 4.1.
 
852Graph Condensation for Open-World Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
specific generalization modules for each GNN. In response to the ex-
isting sophisticated condensation procedure, we design an efficient
GC paradigm that combines Kernel Ridge Regression (KRR) with
non-parametric graph convolution, circumventing the nested loop
optimization and heavy graph kernel encoding in other KRR-based
GC methods [ 38,42]. Consequently, OpenGC is well-suited for man-
aging life-long graph data and handling the continuous growth and
dynamic changes of open-world graphs. The main contributions of
this paper are threefold:
•New problem and insights. We are the first (to the best of our
knowledge) to focus on the practical deployment issue of GC
in the evolving open-world graph scenario and point out the
necessity of learning temporally generalized condensed graphs,
which is an important yet under-explored problem in GC.
•New methodology. We present OpenGC, a GC method that ex-
plores the temporal invariance pattern within the original graph,
endowing the condensed graph with temporal generalization
capabilities. Additionally, OpenGC employs an efficient conden-
sation paradigm, significantly improving the condensation speed
and enabling prompt updates to the condensed graph.
•SOTA performance. Through extensive experimentation on
both real-world and synthetic evolving graphs, we validate that
OpenGC excels in handling distribution shifts and accelerat-
ing condensation procedures, surpassing various state-of-the-art
(SOTA) GC methods in performance.
2 Preliminaries
In this section, we first introduce the GNN and conventional GC,
then formally define the problem studied.
2.1 Graph Neural Networks
Consider that we have a large-scale graph T={A,X}consisting
of𝑁nodes. X∈R𝑁×𝑑denotes the 𝑑-dimensional node feature
matrix and A∈R𝑁×𝑁is the adjacency matrix. We use Y∈R𝑁×𝐶
to denote the one-hot node labels over 𝐶classes. GNNs learn the
embedding for each node by leveraging the graph structure infor-
mation and node features as input. Without loss of generality, we
use graph convolutional network (GCN) [ 17] as an example, where
the convolution operation in the 𝑘-th layer is defined as follows:
H(𝑘)=ReLU
ˆAH(𝑘−1)W(𝑘)
, (1)
where H(𝑘)is the node embeddings of the 𝑘-th layer, and H(0)=X.
ˆA=eD−1
2eAeD−1
2is the normalized adjacency matrix. eArepresents
the adjacency matrix with the self-loop, eDdenotes the degree matrix
ofeA, andW(𝑘)is the trainable weights at layer 𝑘. For simplicity,
we denote an 𝐾-layer GNN encoder as H=ℎ(T), where Hdenotes
the final node embeddings utilized in downstream tasks. In this
paper, we concentrate on the node classification task, where His
further input into the classifier 𝑔(·). Consequently, the entire model
is denoted as 𝑓=𝑔◦ℎ, encapsulating both the GNN encoder and
the classifier.
2.2 Graph Condensation
Graph condensation [ 15] aims to generate a small synthetic graph
S={A′,X′}withA′∈R𝑁′×𝑁′,X′∈R𝑁′×𝑑as well as its labelY′∈R𝑁′×𝐶, where𝑁′≪𝑁. The model trained on Scan achieve
comparable node classification performance to the one trained on
the much largerT. To facilitate the connection between real graph
Tand synthetic graph S, a relay model 𝑓𝜃=𝑔◦ℎparameterized by
𝜃is employed in the optimization process for encoding both graphs.
We initially define the loss for TandSabout the parameter 𝜃as:
LT(𝜃)=ℓ(𝑓𝜃(T),Y),
LS(𝜃)=ℓ 𝑓𝜃(S),Y′,(2)
whereℓis the classification loss such as cross-entropy and Y′is
predefined to match the class distribution in Y. Then the objective
of GC can be formulated as a bi-level optimization problem:
min
SLT
𝜃S
s.t.𝜃S=arg min
𝜃LS(𝜃). (3)
To solve the objective outlined above, conventional GC [ 15] pro-
poses to match the model gradients at each training step 𝑡. In this
way, the training trajectory on condensed data can mimic that on
the original data, i.e., the models trained on these two datasets
converge to similar solutions.
min
SE𝜃0∼Θ"𝑇∑︁
𝑡=1D
∇𝜃𝑡LT(𝜃𝑡),∇𝜃𝑡LS(𝜃𝑡)#
s.t.𝜃𝑡+1=opt
LS(𝜃𝑡)
,(4)
where𝜃0represents the initial parameters of the relay model, sam-
pled from a specific distribution Θ. The expectation on 𝜃0aims
to improve the robustness of Sto different parameter initializa-
tion [ 20].D(·,·)is the distance measurement. opt(·)is the model
parameter optimizer and the parameters of the relay model are
updated only onS. The structure of condensed graph A′is parame-
terized by similarity in node features and modeled by a multi-layer
perceptron (MLP) as A′=MLP(X′).
Limited distribution generalization of GC. The optimization
objective of GC aims to align the performance of GNNs trained
on the condensed graph closely with those trained on the original
graph. However, such objective is solely performance-driven and
takes into account merely a snapshot of the original graph’s distri-
bution. Consequently, GNNs trained on the condensed graph are
restricted to adapting to the condensed distribution and struggle
to accommodate the dynamic distribution changes encountered
during deployment.
Intricate optimization in GC. The optimization procedure of
GC is notably intricate, owing to two primary factors. Firstly, the
optimization objective in Eq. (4) necessitates simultaneous updates
of the relay model 𝑓𝜃and the condensed graph S. To tackle this
demand, GC employs a nested loop optimization strategy, updating
𝑓𝜃within the inner loop and optimizing Sin the outer loop. Nev-
ertheless, this bi-level optimization approach not only intensifies
the computational demands but also introduces additional hyper-
parameters, thus adding complexity to the optimization process
and impeding the attainment of optimal solutions. Secondly, the
updating or periodic initialization of the relay model leads to the
repetitive encoding of the original graph throughout the condensa-
tion process. Due to the neighbor explosion problem, the computa-
tional load for encoding the expansive original graph is substantial,
and the repeated execution of the encoding operation exacerbates
 
853KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
the computational burden even further. Therefore, these intricate
optimizations in GC constrain the frequent updating of condensed
graph and its adaptability in life-long graph learning scenarios.
2.3 Problem Formulation
In the open-world graph scenario, we consider a sequential stream
of node batches{𝑉1,𝑉2,...,𝑉𝑚}, where𝑚represents the total num-
ber of batches involved and each node batch accompanied by corre-
sponding edges. These node batches are progressively accumulated
and integrated into the existing graph over time to construct the
tasks{𝑇1,𝑇2,...,𝑇𝑚}, as depicted in Figure 1. At task 𝑇𝑡, the snapshot
graphT𝑡includes all 𝑁𝑡nodes that have emerged and the graph
expands as tasks progress, i.e., T𝑡⊂T𝑡+1. Correspondingly, the
number of node class 𝐶𝑡for task𝑇𝑡increases over time, satisfying
𝐶𝑡≤𝐶𝑡+1.
In this dynamic content, we consider the practical application
scenario for both GC and GNN deployment. During the GC phase at
task𝑇𝑡, we initially annotate a subset of the newly added nodes in T𝑡,
addressing the continual integration of new classes. Subsequently,
the condensed graph S𝑡is generated for the large graph T𝑡and
employed to train GNNs efficiently. In the deployment phase, GNNs
trained onS𝑡are equipped with the open-set recognition method
[9] for deployment on subsequent tasks T𝑗≥𝑡. This approach enables
the recognition of nodes from new classes as the “unknown class”,
while nodes from observed classes in T𝑡are categorized into their
appropriate classes. When significant structural changes occur, such
as an influx of new class nodes or deteriorating GNN performance,
the GC procedure is repeated to keep the condensed graph aligned
with the latest original graph.
In this paper, our primary objective centers on enhancing both
the efficiency and the generalization ability of GC across different
distributions. The exploration of advanced open-set recognition
methods falls outside the scope of this study.
3 Methodologies
We hereby present our proposed open-world graph condensation
(OpenGC). We begin with the foundation GC paradigm, which is
an efficient GC approach to generate the condensed graph S𝑡for
the static original graph T𝑡. Then we move on to temporal environ-
ment generation by exploring the structure-aware distribution shift.
Finally, the generated environments are integrated to facilitate the
temporal invariance condensation and the pipeline of OpenGC is
depicted in Figure 2.
3.1 KRR-based Feature Condensation
Based on our earlier discussions, the rationale for the intensive com-
putational demand of the condensation process is the deficiency
of the relay model 𝑓=𝑔◦ℎ, which requires iterative optimization
within the inner loop and repetitive encoding of the original graph.
To address these challenges, we propose discarding the conven-
tional relay model, which pairs GNNs with classifiers. Instead, we
introduce a novel relay model that integrates non-parametric con-
volution with KRR to significantly improve the efficiency of static
graph condensation. For the sake of simplicity, the task subscript 𝑡
is omitted in this subsection.Specifically, we first transform the classification task into a re-
gression problem by replacing the classification neural network 𝑔
with KRR. The corresponding loss function in Eq. (2) is formulated
as:
LT(𝜃)=∥Y−ℎ𝜃(T)W∥2+𝜆∥W∥2,
LS(𝜃)=Y′−ℎ𝜃(S)W2+𝜆∥W∥2,(5)
where Wis the learnable matrix in KRR, ||·|| is the𝐿2norm,𝜆is a
constant, and ℎ𝜃(·)Wis the prediction of the labels. Accordingly,
the condensation objective in Eq. (4) is substituted by Eq. (6) to
ensure that the regression model trained on Sattains performance
comparable to the one trained on the T.
min
SE𝜃∼ΘY−ℎ𝜃(T)WS2
,
𝑠.𝑡.WS=arg min
WY′−ℎ𝜃(S)W2+𝜆∥W∥2.(6)
Consequently, KRR enables a closed-form, exact solution to the
constraint above, eliminating iterative optimization of the classifier
in the inner loop. The solution is calculated as:
WS=ℎ𝜃(S)T
ℎ𝜃(S)ℎ𝜃(S)T+𝜆I−1
Y′, (7)
where Iis the identity matrix. The reduction in graph size leads to an
inversion matrix of dimensions 𝑁′×𝑁′, ensuring the computation
remains efficient.
In addition to the classifier 𝑔, the graph encoder ℎ𝜃(·)in con-
ventional GC leverages GNNs that follow the message-passing
paradigm [ 10], involving the stacking of numerous propagation
and transformation layers and leading to the iterative encoding
issue. To alleviate this problem, we decompose the propagation
and transformation processes within ℎ𝜃(·), and then employ non-
parametric graph convolution [ 39] to encode the original graph
during the pre-processing stage. The 𝐾layers convolution of the
original graph and the condensed graph is calculated as:
H=ˆA𝐾X,H′=ˆA′𝐾X′, (8)
where ˆA′is the normalized adjacency matrix of A′. Then, the en-
coderℎ𝜃(·)is constructed as:
ℎ𝜃(T)=𝜙𝜃(H), ℎ𝜃(S)=𝜙𝜃(H′), (9)
where𝜙𝜃(·)is the transformation layer parameterized by 𝜃[28]. Fi-
nally, we use the identity matrix as the predefined adjacency matrix
as previous works [ 49,51], i.e,A′=I, to further simplify the con-
densed graph modeling and condense the original graph structure
in the node attributes. The benefit of this design is two-fold. Firstly,
the predefined structure eliminates the training of the adjacency
matrix generator. Secondly, it circumvents the encoding of the con-
densed graph in the condensation procedure. When conducting the
downstream tasks, the identity matrix is leveraged as the adjacency
matrix to train various GNNs.
3.2 Structure-aware Environment Generation
The condensed graph serves as a data simulator that resembles a
snapshot of the original graph’s distribution, consequently limiting
the ability of GNNs trained on it to handle distribution shifts. To
counteract this, we incorporate invariance learning into the GC
process, aiming to maintain temporal invariant patterns within the
 
854Graph Condensation for Open-World Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Temporal Invariance Condensation
𝒯𝑡−1
𝒯𝑡
𝒯𝑡+1𝑇𝑡−1
𝑇𝑡
𝑇𝑡+1
…Deployment…
…Temporal Invariance 
Condensation Loss ℒ𝑇𝐼𝐶
…𝐇𝒕−𝟏
𝐇𝒕∆𝐇𝒕
𝐇𝒕𝒆 𝐇𝒕′
𝐇𝒕′
Multiple GNN architectures  Efficient 
trainingTasks Graphs
Open -set 
recognition
Figure 2: The pipeline of OpenGC. The graph T𝑡and historic graph T𝑡−1are encoded by non-parametric convolution and
embeddings are leveraged to construct temporal environments H𝑒
𝑡. The condensed graph embedding H′
𝑡is generated according
to temporal invariance condensation loss L𝑇𝐼𝐶. In the deployment stage, the condensed graph is utilised to train multiple
GNNs with various architectures, which are applied to sequential tasks T𝑗≥𝑡.
condensed graph. Given the unavailability of future graph struc-
tures and distributions, we refer to the historic graph and construct
various temporal environments by simulating potential future dis-
tribution shifts.
Specifically, we calculate the residuals by comparing the current
embeddings at task 𝑇𝑡with the embeddings from the last task 𝑇𝑡−1.
The residual for the node 𝑖is calculated as:
ΔH𝑡,𝑖=H𝑡,𝑖−H𝑡−1,𝑖, (10)
where H𝑡,𝑖andH𝑡−1,𝑖are embeddings of node 𝑖at task𝑇𝑡and𝑇𝑡−1,
respectively. As the graph evolves, the neighbors of node 𝑖are ex-
tended, and the node embedding changes from H𝑡−1,𝑖toH𝑡,𝑖after
graph convolution. Therefore, ΔH𝑡,𝑖formulates the added neigh-
bors in convolution and indicates the structure-aware distribution
shift in task 𝑇𝑡, which can be leveraged to augment other node
embeddings. Then, we randomly select node 𝑗, which belongs to
the same class as node 𝑖, and use its residual ΔH𝑡,𝑗to modify the
embedding H𝑡,𝑖. The motivation hinges on the assumption that
nodes in the same class follow a similar distribution shift pattern2.
ˆH𝑡,𝑖=H𝑡,𝑖+𝜀𝑡,𝑖Δ¯H𝑡,𝑗, (11)
where Δ¯H𝑡,𝑗is the normalized residual and 𝜀𝑡,𝑖controls the mag-
nitude. To determine the value of 𝜀𝑡,𝑖, we take into account the
characteristics of the target nodes 𝑖and the node 𝑗responsible
for generating the residual. Firstly, a higher weight is assigned to
the target node 𝑖if it has a lower degree, with the premise that
nodes with fewer connections are more prone to being influenced,
leading to a pronounced distribution shift upon the addition of new
neighbor nodes. Moreover, the similarity between node 𝑖and𝑗is
assessed to promote a more significant distribution shift among
similar nodes. Consequently, the magnitude is constructed as:
𝜀𝑡,𝑖=𝛿𝑖×cosine(H𝑡,𝑖,H𝑡−1,𝑗)×𝜂, (12)
2We use the drop edge and drop feature to construct the environments if no historic
classes are available.where𝜂is the hyper-parameter to control the base magnitude
andcosine(·)measures the consine similarity. 𝛿𝑖is the degree cal-
ibration term sampled from the Beta distribution: 𝛿𝑖∼Beta(𝑐×
𝑑𝑒𝑔𝑟𝑒𝑒𝑖,1), where𝑑𝑒𝑔𝑟𝑒𝑒𝑖denotes the degree of node 𝑖,𝑐repre-
sents the calibration constant. This introduces the randomness for
multiple environments generation and a higher 𝑑𝑒𝑔𝑟𝑒𝑒𝑖increases
the probability of sampling a smaller 𝛿𝑖.
Finally, we produce several environments 𝑒∈Efor invariance
condensation, with the embeddings for each environment denoted
byH𝑒
𝑡=[ˆH𝑡,1;...;ˆH𝑡,𝑁 𝑡].
3.3 Temporal Invariance Condensation
With the definition of efficient graph condensation and multiple
environment representations, we represent the final objective of
temporal invariance condensation.
At task𝑇𝑡, we first pre-compute the embeddings of the original
graph H𝑡and various environments H𝑒
𝑡. To enhance the stability
of the optimization, we incorporate a learnable temperature 𝜏to
calibrate the prediction of KRR and substitute the regression loss
with cross-entropy loss. The loss for the original graph embedding
H𝑡and condensed graph embedding H′
𝑡is expressed as:
L(H𝑡,𝑓𝜃)=1
𝑁𝑡𝑁𝑡∑︁
𝑖=1𝐶𝑡∑︁
𝑗=1Y𝑖,𝑗log[𝑓𝜃(H𝑡)/𝜏]𝑖,𝑗,
𝑓𝜃(H𝑡)=𝜙𝜃(H𝑡)WS,
WS=𝜙𝜃(H′
𝑡)T
𝜙𝜃(H′
𝑡)𝜙𝜃(H′
𝑡)T+𝜆I−1
Y′
𝑡,(13)
where Y′
𝑡is the condensed graph label at task 𝑇𝑡. Subsequently, the
invariant risk minimization (IRM) loss [ 1] is constructed to identify
and capture the invariance patterns across multiple environments:
L𝐼𝑅𝑀=1
|E|∑︁
𝑒∈EL(H𝑒
𝑡,𝑓𝜃)+𝛾∇𝑤|𝑤=1L(H𝑒
𝑡,𝑤𝑓𝜃)2. (14)
 
855KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
where𝛾is the hyper-parameters to weight the loss. This con-
straint ensures uniform optimization results across all environ-
ments, thereby capturing the invariance patterns amidst various
structural changes. Finally, the condensed graph is optimized ac-
cording to the objective:
L𝑇𝐼𝐶=E𝜃∼Θ[L(H𝑡,𝑓𝜃)+𝛼L𝐼𝑅𝑀], (15)
where𝛼regulates the balance between original graph information
and invariance information.
For a thorough comprehension of OpenGC, the summary of the
training procedure and detailed time complexity analysis of our
condensed data are presented in Appendix A.1 and Appendix A.2,
respectively.
4 Experiments
We design comprehensive experiments to validate the efficacy of
our proposed OpenGC and explore the following research questions:
Q1: Compared to the SOTA GC methods, can OpenGC achieve bet-
ter temporal generalization performance? Q2: Can the OpenGC
condense the graph faster than other GC approaches? Q3: How do
the different components, i.e., IRM and different constraints used
in environment generation affect OpenGC? Q4: Can the condensed
graph generated by OpenGC generalize well to different GNN ar-
chitectures? Q5: How do the different hyper-parameters affect the
OpenGC? Q6: What are the characteristics of the visualization of
condensed nodes?
4.1 Experimental Settings
Datasets. In dynamic real-world scenarios, newly added nodes of-
ten belong to both existing and novel classes and available datasets
fail to fully capture this dynamic nature. Consequently, we con-
struct four real-world graph datasets to evaluate our proposed
method: Yelp3, Taobao4, Flickr [ 46] and Coauthor [ 35]. Follow-
ing [ 4], we construct the Yelp and Taobao according to the natural
timestamps to align with temporal sequences. For the Flickr and
Coauthor datasets, which do not include temporal information, we
segment the data into tasks using randomly selected classes and
nodes. Without loss of generality, we guarantee that in subsequent
tasks, newly added nodes not only introduce new classes but also
belong to all classes observed in earlier tasks. All datasets are ran-
domly divided, with each task following a consistent split ratio:
60% for training, 20% for validation, and 20% for testing. Moreover,
the training, validation, and testing sets are continually expanded
based on the sets from previous task. The detailed descriptions of
datasets and tasks are provided in Appendix A.3.
Baselines and Evaluation Settings. We compare our proposed
methods to three SOTA GC methods: (1) GCond [ 15]: the first GC
method that employs the gradient matching and bi-level optimiza-
tion techniques; (2) GCDM [ 49]: an efficient GC method that utilizes
distribution matching to align the class distributions; (3) SFGC [ 51]:
a structure-free GC method that pre-trains a large number of GNN
models and utilizes trajectory matching to improve the optimization
results. To enable GNNs be capable of recognizing novel classes,
3https://www.yelp.com/dataset
4https://tianchi.aliyun.com/dataset/dataDetail?dataId=9716we incorporate two open-set recognition techniques into each base-
line: (1) Softmax [ 9]: it adds the softmax as the final output layer
and sets the threshold to classify the low-confidence nodes as the
“unknown class”. (2) Openmax [ 2]: it assesses the probability of a
node being an outlier via Weibull fitting and then calibrates the
output logits, enabling the identification of novel classes with the
predefined threshold.
We evaluate each GC method on evolving graphs containing a
series of tasks{𝑇1,𝑇2,...,𝑇𝑚}. At task𝑇𝑡, the graphT𝑡is condensed
by GC methods, and the condensed graph is then used to train
downstream GNNs. The performance of these GNNs is evaluated
across all subsequent tasks from 𝑇𝑡to𝑇𝑚. Therefore, an upper
triangular performance matrix M∈R𝑚×𝑚is maintained, where
the element M𝑖,𝑗represents the accuracy of a GNN trained on the
condensed graph from task 𝑇𝑖and tested on task 𝑇𝑗(𝑖≤𝑗). To
quantify the overall performance of models trained on condensed
graphs, we use the mean of average performance (mAP) as the
evaluation metric as follows:
𝑚𝐴𝑃 =1
𝑚𝑚∑︁
𝑖=1(1
𝑚−𝑖+1𝑚∑︁
𝑗=𝑖M𝑖,𝑗). (16)
For a fair comparison, we follow the conventional graph conden-
sation [ 15] for the condensation setting. For all compared baselines,
we employ SGC [ 39], which uses non-parametric convolution, as
the relay model and use the identity matrix as the adjacency matrix.
The downstream GNN is GCN [17] unless specified otherwise.
To assess the ability of GC to generalize across different GNN
architectures, we conduct evaluations using a variety of models,
including GCN [ 17], SGC [ 39], GraphSAGE [ 12] and APPNP [ 18].
We condense the original graph with 𝑁𝑡nodes into a condensed
graph with𝑁′
𝑡nodes and the compress ratio is calculated by 𝑁′
𝑡/𝑁𝑡.
We choose the compress ratio of Yelp, Taobao, and Coauthor to be
{1%, 2%}. For the larger dataset Flickr, we choose the compress ratio
as {0.1%, 0.2%}.
Hyper-parameters and Implementation. The hyper-parameters
for baselines are configured as described in their respective papers,
while others are determined through grid search on the valida-
tion set of the condensed task. For all datasets, a 2-layer graph
convolution is employed, and the width of the hidden layer in 𝜙𝜃
is set to 1024. The regularization parameter 𝜆for KRR is fixed at
5e-3. The learning rate for the condensation process is determined
through a search over the set {1e-1, 1e-2, 1e-3, 1e-4}. The parameters
𝛼and𝛾are optimized from the set {0.1, 0.3, 0.5, 0.7, 1} to achieve an
optimal balance between the losses. The intervention magnitude
parameter𝜂is explored within the range {0.01, 0.1, 1, 10, 100} to
identify the most appropriate level of intervention. The number of
environments considered is varied from 1 to 5 to find the optimal
setting. The calibration constant 𝑐used in Beta distribution is set
as 10. Finally, the number of training epochs is determined using
the early stop to prevent overfitting.
Due to the absence of new class nodes in the validation set, we
follow previous work [ 2] to set the threshold for open-set recog-
nition approaches, configuring it to exclude 10% of the validation
nodes as “unknown class”. To eliminate randomness, we repeat each
experiment 5 times and report the average test score and standard
deviation.
 
856Graph Condensation for Open-World Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The comparison of mAP (%) for different graph condensation methods on real-world datasets. Whole indicates the
GNN is trained on the large original graph. Openmax and Softmax are deployed for open-set recognition in the testing stage.
Compr
ess
ratioOp
enmax Softmax
GCond
GCDM SFGC OpenGC Whole GCond
GCDM SFGC OpenGC Whole
Y
elp0.01 43.36±0.19
43.57±0.73 43.90±0.37 46.14±0.9049.16±0.3942.89±0.48
42.94±0.41 43.18±0.11 45.98±0.5348.91±0.250.02 43.61±0.35
43.79±0.33 44.09±0.12 46.42±0.19 43.81±0.20
43.74±0.19 44.17±0.11 46.18±0.12
T
aobao0.01 66.09±0.19
66.32±0.21 67.02±0.36 67.91±0.4170.65±0.5968.15±0.56
68.15±0.43 68.61±0.33 69.81±0.5471.82±0.700.02 66.84±0.98
66.58±0.74 67.22±0.32 68.26±0.83 68.32±0.41
68.52±0.13 68.83±0.16 70.07±0.48
F
lickr0.001 35.74±0.74
35.84±0.53 36.73±0.58 38.19±0.2044.96±0.3835.65±0.80
35.53±0.54 36.09±0.14 38.11±0.8244.63±0.020.002 36.58±0.95
36.20±0.53 36.97±0.14 39.06±0.66 35.95±0.47
36.24±0.30 36.82±0.23 38.93±0.15
Coauthor0.01 68.96±0.30
68.91±0.28 69.27±0.31 69.94±0.3872.44±0.1669.31±0.17
71.48±0.33 71.63±0.83 73.00±0.6074.44±0.150.02 69.63±0.43
69.54±0.40 69.74±0.17 70.38±0.43 71.35±0.44
71.53±0.34 71.94±0.15 73.32±0.92
123456
Evaluate task1
2
3
4
5
6Condensed task
−4−2024
123456
Evaluate task1
2
3
4
5
6Condensed task
−2−1012
123456
Evaluate task1
2
3
4
5
6Condensed task
−4−2024
1234567
Evaluate task1
2
3
4
5
6
7Condensed task
−3−2−10123
Figure 3: The heatmap of the differences between performance matrix (%) of OpenGC and SFGC on Yelp, Taobao, Flickr and
Coauthor datasets (from left to right). Softmax is adopted and the compress ratios are 1%, 1%, 0.1%, and 1%, respectively.
We use the ADAM optimization algorithm to train all the models.
The codes are written in Python 3.9 and the operating system is
Ubuntu 16.0. We use Pytorch 1.12.1 on CUDA 11.4 to train models
on GPU. All experiments are conducted on a machine with Intel(R)
Xeon(R) CPUs (Gold 6128 @ 3.40GHz) and NVIDIA GeForce RTX
2080 Ti GPUs.
4.2 Accuracy Comparison (Q1)
We report the mAP for different GC methods with standard de-
viation in Table 1. In this table, Whole indicates that GNNs are
trained on the original graph, achieving the highest performance.
However, it suffers from substantial computational costs due to the
large scale of the original graph. Compared to Whole, GC methods
maintain similar performance levels even under extreme compress
rate on Yelp, Taobao, and Coauthor datasets, confirming the ef-
fectiveness of GC. On the Flickr dataset, the performance gap is
larger compared to other datasets, attributing to the dataset’s signif-
icant imbalance issue. When comparing the different GC methods,
performance differences emerge. For example, GCond achieves
comparable performances with GCDM across different datasets and
compress ratios. Benefiting from the advanced trajectory matching
strategy, SFGC achieves consistent performance improvement com-
pared to GCond and GCDM. This is due that trajectory matching
can provide more precise optimization guidance for GC procedure
compared to gradient and distribution matching and significantly
enhance the quality of condensed graphs. Nonetheless, all these
methods only condense the snapshot of the original graph and pre-
serve the static graph information in the condensed graph, which
limits their performance under the dynamic graph scenarios. Our
proposed OpenGC consistently outperforms other baselines. Re-
markably, with substantial compress rates, OpenGC achieves resultscomparable to the Whole on the Taobao and Coauthor under the
Softmax setting. Furthermore, in Figure 3, we present a detailed
heatmap of the differences between the performance matrix Mof
OpenGC and the strongest baseline SFGC. The observed pattern
reveals a deep red coloration along the diagonal line, indicating a
significant enhancement in performance on condensed tasks. As
the graph evolves, the color gradually lightens and the increment
gradually weaken. This phenomenon is attributed to the increasing
number of classes, which makes the task more complicated. When
comparing the different compress ratios, OpenGC guarantees supe-
rior GNN performance at lower compress ratios compared to the
higher compress ratios achieved by other GC methods. All these
results underscore the efficacy of our proposed temporal invariance
condensation and the superior optimization results from the exact
solution provided by KRR.
4.3 Condensation Time Comparison (Q2)
For the sake of convenient comparison, we evaluate the GC meth-
ods by measuring the condensation time for the largest graph in
the final task on each dataset. Besides our proposed method, we
also assess the performance of OpenGC without the environment
generation and invariant learning (“OpenGC w/o IRM”) and the
results are presented in Table 2. This assessment includes measur-
ing the pre-processing time, the average condensation time per
100 epochs, and the total condensation time (pre-processing time
excluded). The corresponding accuracy results and time complexity
are reported in Table 3 and Appendix A.2. Firstly, the pre-processing
time of GCond, GCDM, and OpenGC w/o IRM contains the time
of non-parametric graph convolution in Eq. (8). OpenGC addition-
ally involves the environment generation time. In contrast, SFGC
demands the training of hundreds of GNN models on the original
 
857KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
Table 2: The comparison of condensation time (sec.) for different GC methods. The pre-processing time (Pre. time), average
condensing time per 100 epochs (Avg. time), and total condensing time (Total time) are measured separately. The condensation
target is the largest graph in the final task and the compress ratios are 1%, 1%, 0.1%, and 1% for 4 datasets respectively. “<1”
indicates that the time duration is less than 1 second.
Y
elp T
aobao F
lickr Coauthor
Pr
e. time Avg. time Total time Pr
e. time Avg. time Total time Pr
e. time Avg. time Total time Pr
e. time Avg. time Total time
GCond <1
6.92 238.42 <1
8.76 175.04 <1
5.97 307.33 <1
15.37 260.39
GCDM <1
3.08 57.92 <1
5.18 72.15 <1
3.82 76.44 <1
5.83 116.47
SFGC 2125.24
109.08 2546.28 3272.66
100.52 3272.66 11959.14
131.80 1219.32 3710.99
124.72 1325.07
OpenGC w/o IRM <1
1.70 13.15 <1
4.56 19.22 <1
2.84 61.12 <1
3.52 28.28
OpenGC <1
1.84 17.65 <1
6.13 25.56 4.51
3.41 78.36 3.11
4.61 37.14
Table 3: The comparison of GNN’s accuracy (%) trained on the
different condensed graphs. Whole indicates that GNNs are
trained on the original graph. The condensation and evalua-
tion target is the largest graph of the final task. The compress
ratios are 1%, 1%, 0.1% and 1% for 4 datasets respectively.
Y
elp Taobao Flickr Coauthor
Whole 52.06±0.55
80.20±0.06 53.49±0.22 93.63±0.14
GCond 44.52±1.02
72.80±0.61 45.18±1.03 88.21±1.51
GCDM 44.23±0.80
72.75±0.52 44.27±1.30 87.60±0.15
SFGC 45.81±0.25
73.92±0.11 46.72±0.43 89.81±0.31
OpenGC w/o IRM 46.62±0.32
74.39±0.34 47.25±0.55 90.25±0.46
OpenGC 47.52±0.46
74.81±0.39 48.15±0.65 90.93±0.49
Table 4: Ablation study of OpenGC. The compress ratios are
1%, 1%, 0.1% and 1% respectively. The open-set recognition
method is Softmax.
Y
elp Taobao Flickr Coauthor
Op
enGC 45.98±0.53
69.81±0.54 38.11±0.82 73.00±0.60
w/o IRM 44.25±0.28
69.31±0.15 36.30±0.80 71.78±0.11
w/o TEG 44.99±0.16
69.47±0.64 36.54±0.63 72.06±0.40
w/o degree 45.17±0.16
69.63±0.19 36.68±0.35 72.51±0.11
w/o similarity 45.69±0.39
69.51±0.74 37.27±0.14 72.47±0.13
graph to serve as teacher models and provide training trajectories.
This step is computationally demanding and time-consuming, sig-
nificantly exceeding the time taken for condensation. Moreover,
training a large number of GNNs contradicts the core motivation
behind GC, which aims for efficient training of multiple GNNs using
condensed graphs. The average condensing time per 100 epochs
reflects the complexity of the condensation process. GCond uti-
lizes gradient matching to align the original graph and condensed
graph, necessitating the additional back-propagation steps for gra-
dient computation with respect to model parameters. In contrast,
GCDM alters the optimization objective and leverages the distribu-
tion matching, eliminating the gradient calculation and achieving a
faster condensation procedure. Nonetheless, GCDM still engages in
nested loop optimization for relay model updates. Similarly, SFGC
requires multiple updates to the relay model based on the condensed
graph to resemble the training trajectories of the teacher model.
Moreover, it utilises the graph neural tangent kernel to evaluate the
quality of the condensed graph, which is computationally intensive
and further increases the condensing time. Our method exhibitsthe highest condensation speed. Compared to OpenGC without
IRM, the time required for environment generation in OpenGC
is minimal. For example, the pre-processing on Yelp and Taobao
takes less than 1 second. Despite the larger feature dimensions
of Flickr and Coauthor, the pre-processing time remains under 10
seconds. Although GCDM also demonstrates rapid condensation,
its accuracy is considerably lower than our proposed method, as
detailed in Table 3.
4.4 Ablation Study (Q3)
To validate the impact of individual components, OpenGC is tested
with Softmax, selectively disabling certain components. We evalu-
ate OpenGC in the following configurations: (1) without the IRM
loss (“w/o IRM”); (2) without the temporal environment generation
(“w/o TEG”), using edge and feature drop to randomly generate
environments instead; (3) without the degree constraints (“w/o de-
gree”); (4) without the similarity constraint (“w/o similarity”). The
results of these settings are presented in Table 4. The removal of
IRM loss leads to a noticeable decline in accuracy, underscoring
the importance of both temporal environment generation and the
invariance constraint in improving the generalization capabilities
of the condensed graph. The significance of temporal environment
generation is further highlighted by the performance drop in “w/o
TEG”. Although new environments are generated, “w/o TEG” does
not simulate the graph expanding pattern, failing to predict the
future graphs and leading to the sub-optimal results. Further anal-
ysis is conducted on the degree and similarity constraints during
temporal environment generation. The performance in these cases
is better than “w/o TEG” but does not reach the levels achieved by
OpenGC, reinforcing the importance of incorporating node charac-
teristics to enhance the environment generation.
4.5 Generalizability for GNN Architectures (Q4)
A critical attribute of GC is its ability to generalize across differ-
ent GNN architectures, making the condensed graph versatile for
training various GNN models in downstream tasks. Therefore, we
evaluate different GNN models on the condensed graph, including
GCN, SGC, GraphSAGE, and APPNP. These models are then applied
to subsequent tasks and the mAP of different datasets are presented
in Table 5. According to the results, all evaluated GNN models
were effectively trained using GC methods, achieving comparable
levels of performance. In detail, GCN and SGC exhibited superior
performance due to these two models utilise the same convolution
kernel as the relay model. GraphSAGE performed exceptionally
 
858Graph Condensation for Open-World Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Yelp
Taobao
Flickr
Coauthor
Figur
e 4: The visualization of t-SNE on condensed graph by OpenGC.
Table 5: Architecture generalizability of different graph con-
densation methods. The compress ratios are 1%, 1%, 0.1%, and
1% respectively. The open-set recognition method is Softmax.
Metho
d GCN
SGC GraphSAGE APPNP
Y
elpGCond 42.89±0.48
42.41±0.49 41.02±0.44 42.01±0.35
GCDM 42.94±0.41
42.52±0.36 41.19±0.24 41.20±0.33
SFGC 43.18±0.11
43.96±0.79 43.00±0.18 43.54±0.35
Op
enGC 45.98±0.53
45.81±0.51 44.07±0.33 44.89±0.90
T
aobaoGCond 68.15±0.56
68.51±0.11 70.03±0.32 67.33±0.05
GCDM 68.15±0.43
69.56±0.29 70.13±0.31 67.11±0.53
SFGC 68.61±0.33
69.77±0.58 70.92±0.26 68.01±0.11
Op
enGC 69.81±0.54
70.67±0.32 71.37±0.41 68.90±0.51
F
lickrGCond 35.65±0.80
35.57±0.15 33.80±0.31 34.55±0.04
GCDM 35.53±0.54
35.30±0.44 32.35±0.43 33.91±0.65
SFGC 36.09±0.14
36.39±0.16 33.82±0.50 34.57±0.15
Op
enGC 38.11±0.82
37.85±0.38 34.62±0.26 35.52±0.45
CoauthorGCond 69.31±0.17
70.17±0.08 68.07±0.55 68.86±0.31
GCDM 71.48±0.33
71.53±0.23 67.95±0.35 68.29±0.70
SFGC 71.63±0.83
71.18±0.11 69.03±0.23 69.51±0.21
Op
enGC 73.00±0.60
72.72±0.42 69.96±0.16 70.63±0.12
well on the Taobao dataset, suggesting a particular compatibility
with this datasets. Moreover, different condensation methods exhib-
ited varied performances, with our proposed method consistently
maintaining the best results across various architectures.
4.6 Hyper-parameter Sensitivity Analysis (Q5)
The hyper-parameters 𝛼and𝛾are leveraged to control the impact
of the IRM loss during training, while 𝜂and the number of environ-
ments contribute to the extent of augmentations. In Figure 5, we
present the mAP performances w.r.t. different values of 𝛼,𝛾,𝜂and
the number of environments, respectively. For 𝛼, we observe that
values in the range of 0.3 to 0.7 yield the best performance. Higher
values may excessively weigh the IRM loss, potentially compro-
mising the preservation of original graph information. Similarly,
selecting an optimal 𝛾value is crucial for maximizing performance
across different datasets. Regarding 𝜂, a value of 10 ensures a magni-
tude balance of the residual to the original embeddings. Finally, the
increase of the number of environments can enhance performance
and it should be controlled to avoid introducing excessive noise.
4.7 Visualization (Q6)
Figure 4 presents the t-SNE visualization of node features in the
condensed graph produced by our proposed OpenGC at the min-
imum compress ratios. Although we eliminate the modeling the
adjacency matrix and use the identity matrix instead, the condensed
0.1 0.3 0.5 0.7 1
α44.545.245.9Yelp Accuracy (%)
69.069.369.669.9
Taobao Accuracy (%)Yelp
Taobao
0.1 0.3 0.5 0.7 1
γ44.845.145.445.746.0Yelp Accuracy (%)
69.169.369.569.769.9
Taobao Accuracy (%)Yelp
Taobao
0.01 0.1 1 10 100
η45.445.645.846.0Yelp Accuracy (%)
69.169.369.569.769.9
Taobao Accuracy (%)Yelp
Taobao
1 2 3 4 5
#Environments44.845.145.445.746.0Yelp Accuracy (%)
69.169.369.569.769.9
Taobao Accuracy (%)Yelp
TaobaoFigur
e 5: The hyper-parameter sensitivity analysis.
node features exhibit a well-clustered pattern, even on the highly
imbalanced dataset Flickr. This suggests that the structure and class
information are effectively preserved within the node features, en-
abling the training of GNNs for classification tasks in the absence
of adjacency matrix.
5 Conclusion
In this paper, we present open-world graph condensation (OpenGC),
a robust graph condensation approach that enhances the tempo-
ral generalization capabilities of condensed graph. OpenGC ex-
ploits the structure-aware distribution shift in the evolving graph
and extracts invariant features in the original graph for temporal
invariance condensation. Moreover, OpenGC optimizes the con-
densation procedure by combining kernel ridge regression and
non-parametric graph convolution, successfully accelerating the
condensation progress. Benefiting from the superiority of the gen-
eralization capacity of condensed graphs and efficient optimization
procedure, OpenGC not only enhances the GNNs’ ability to handle
the dynamic distribution change in real-world scenarios but also
expedites the condensed graph updating in life-long graph learning.
Acknowledgment
This work is supported by Australian Research Council under the
streams of Future Fellowship (Grant No. FT210100624), Discovery
Early Career Researcher Award (Grants No. DE230101033), Discov-
ery Project (Grants No.DP240101108 and No.DP240101814).
 
859KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
References
[1]Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[2]Abhijit Bendale and Terrance E Boult. 2016. Towards open set deep networks.
InProceedings of the IEEE conference on computer vision and pattern recognition.
1563–1572.
[3]Mucong Ding, Xiaoyu Liu, Tahseen Rabbani, Teresa Ranadive, Tai-Ching Tuan,
and Furong Huang. 2022. Faster Hyperparameter Search for GNNs via Calibrated
Dataset Condensation. arXiv (2022).
[4]Kaituo Feng, Changsheng Li, Xiaolu Zhang, and Jun Zhou. 2023. Towards open
temporal graph neural networks. International Conference on Learning Represen-
tations (2023).
[5]Xinyi Gao, Tong Chen, Yilong Zang, Wentao Zhang, Quoc Viet Hung Nguyen,
Kai Zheng, and Hongzhi Yin. 2024. Graph Condensation for Inductive Node
Representation Learning. In ICDE.
[6]Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, and Hongzhi Yin.
2024. Graph condensation: A survey. arXiv preprint arXiv:2401.11720 (2024).
[7]Xinyi Gao, Wentao Zhang, Tong Chen, Junliang Yu, Hung Quoc Viet Nguyen, and
Hongzhi Yin. 2023. Semantic-aware node synthesis for imbalanced heterogeneous
information networks. In Proceedings of the 32nd ACM International Conference
on Information and Knowledge Management. 545–555.
[8]Xinyi Gao, Wentao Zhang, Junliang Yu, Yingxia Shao, Quoc Viet Hung Nguyen,
Bin Cui, and Hongzhi Yin. 2023. Accelerating scalable graph neural network
inference with node-adaptive propagation. arXiv preprint arXiv:2310.10998 (2023).
[9]Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. 2020. Recent advances
in open set recognition: A survey. IEEE transactions on pattern analysis and
machine intelligence 43, 10 (2020), 3614–3631.
[10] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
Conference on Machine Learning. PMLR, 1263–1272.
[11] Zhichun Guo, Kehan Guo, Bozhao Nan, Yijun Tian, Roshni G Iyer, Yihong Ma,
Olaf Wiest, Xiangliang Zhang, Wei Wang, Chuxu Zhang, et al .2022. Graph-based
molecular representation learning. arXiv preprint arXiv:2207.04869 (2022).
[12] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-
tation Learning on Large Graphs. In Advances in Neural Information Processing
Systems. 1024–1034.
[13] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets
for Machine Learning on Graphs. In Advances in Neural Information Processing
Systems.
[14] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang,
and Bing Yin. 2022. Condensing Graphs via One-Step Gradient Matching. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 720–730.
[15] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah.
2022. Graph Condensation for Graph Neural Networks. In International Confer-
ence on Learning Representations.
[16] Kyomin Jung, Wooram Heo, and Wei Chen. 2012. Irie: Scalable and robust influ-
ence maximization in social networks. In 2012 IEEE 12th International Conference
on Data Mining. IEEE, 918–923.
[17] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[18] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2018. Pre-
dict then propagate: Graph neural networks meet personalized pagerank. arXiv
preprint arXiv:1810.05997 (2018).
[19] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In International Conference
on Machine Learning. PMLR, 5815–5826.
[20] Shiye Lei and Dacheng Tao. 2024. A Comprehensive Survey of Dataset Distillation.
TPAMI (2024).
[21] Yuchen Li, Ju Fan, Yanhao Wang, and Kian-Lee Tan. 2018. Influence maximization
on social graphs: A survey. IEEE Transactions on Knowledge and Data Engineering
30, 10 (2018), 1852–1872.
[22] Jie Liu, Mengting He, Guangtao Wang, Nguyen Quoc Viet Hung, Xuequn Shang,
and Hongzhi Yin. 2023. Imbalanced node classification beyond homophilic
assumption. arXiv preprint arXiv:2304.14635 (2023).
[23] Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing
He. 2023. FLOOD: A flexible invariant learning framework for out-of-distribution
generalization on graphs. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1548–1558.
[24] Yang Liu, Deyu Bo, and Chuan Shi. 2023. Graph Condensation via Eigenbasis
Matching. arXiv (2023).
[25] Yilun Liu, Ruihong Qiu, and Zi Huang. 2023. CaT: Balanced Continual Graph
Learning with Graph Condensation. In ICDM.[26] Jing Long, Tong Chen, Quoc Viet Hung Nguyen, Guandong Xu, Kai Zheng,
and Hongzhi Yin. 2023. Model-agnostic decentralized collaborative learning for
on-device POI recommendation. In SIGIR. 423–432.
[27] Jing Long, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2023. Decen-
tralized collaborative learning framework for next POI recommendation. ACM
Transactions on Information Systems 41, 3 (2023), 1–25.
[28] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. 2022. Efficient
dataset distillation using random feature approximation. Advances in Neural
Information Processing Systems 35 (2022), 13877–13891.
[29] Andreas Loukas and Pierre Vandergheynst. 2018. Spectrally Approximating Large
Graphs with Smaller Graphs. In International Conference on Machine Learning.
[30] Thanh Tam Nguyen, Chi Thang Duong, Matthias Weidlich, Hongzhi Yin, and
Quoc Viet Hung Nguyen. 2017. Retaining data from streams of social platforms
with minimal regret. In IJCAI.
[31] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
[32] Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, and Weiqiang
Wang. 2023. FedGKD: Unleashing the Power of Collaboration in Federated Graph
Neural Networks. arXiv (2023).
[33] Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. 2021. Im-
gagn: Imbalanced network embedding via generative adversarial graph networks.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. 1390–1398.
[34] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H.
Lampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In
2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017.
[35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[36] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in one:
Multi-task prompting for graph neural networks. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 2120–2131.
[37] Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu,
and Hongzhi Yin. 2023. Self-supervised hypergraph representation learning
for sociological analysis. IEEE Transactions on Knowledge and Data Engineering
(2023).
[38] Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, and Qing Li. 2024. Fast graph conden-
sation with structure-based neural tangent kernel. WWW (2024).
[39] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and
Kilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. In
International Conference on Machine Learning. 6861–6871.
[40] Man Wu, Shirui Pan, and Xingquan Zhu. 2020. Openwgl: Open-world graph
learning. In 2020 IEEE international conference on data mining (ICDM). IEEE,
681–690.
[41] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2022.
Discovering invariant rationales for graph neural networks. arXiv preprint
arXiv:2201.12872 (2022).
[42] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao
Yang, and Hanghang Tong. 2023. Kernel Ridge Regression-Based Graph Dataset
Distillation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2850–2861.
[43] Yu Yang, Hongzhi Yin, Jiannong Cao, Tong Chen, Quoc Viet Hung Nguyen,
Xiaofang Zhou, and Lei Chen. 2023. Time-aware dynamic graph embedding for
asynchronous structural evolution. IEEE Transactions on Knowledge and Data
Engineering (2023).
[44] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. 2021.
From local structures to size generalization in graph neural networks. In Interna-
tional Conference on Machine Learning. PMLR, 11975–11986.
[45] Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, Xin
Xia, Yuhui Shi, and Chengqi Zhang. 2024. On-Device Recommender Systems: A
Comprehensive Survey. arXiv preprint arXiv:2401.11441 (2024).
[46] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Vik-
tor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning
Method. In International Conference on Learning Representations.
[47] Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen,
and Lizhen Cui. 2022. Pipattack: Poisoning federated recommender systems for
manipulating item promotion. In WSDM. 1415–1423.
[48] Wentao Zhang, Yu Shen, Zheyu Lin, Yang Li, Xiaosen Li, Wen Ouyang, Yangyu
Tao, Zhi Yang, and Bin Cui. 2022. Pasca: A graph neural architecture search
system under the scalable paradigm. In Proceedings of the ACM Web Conference
2022. 1817–1828.
[49] Bo Zhao and Hakan Bilen. 2023. Dataset condensation with distribution matching.
InProceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision. 6514–6523.
[50] Bolong Zheng, Kai Zheng, Xiaokui Xiao, Han Su, Hongzhi Yin, Xiaofang Zhou,
and Guohui Li. 2016. Keyword-aware continuous knn query on road networks.
 
860Graph Condensation for Open-World Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
InICDE. 871–882.
[51] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan
Zhu, and Shirui Pan. 2023. Structure-free Graph Condensation: From Large-
scale Graphs to Condensed Graph-free Data. In Advances in Neural Information
Processing Systems.
[52] Kaixiong Zhou, Xiao Huang, Qingquan Song, Rui Chen, and Xia Hu. 2022. Auto-
gnn: Neural architecture search of graph neural networks. Frontiers in big Data 5
(2022), 1029307.
[53] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. 2021. Shift-robust
gnns: Overcoming the limitations of localized graph training data. Advances in
Neural Information Processing Systems 34 (2021), 27965–27977.
A Appendix
A.1 Algorithm of OpenGC
The detailed algorithm of OpenGC is shown in Algorithm 1. In
pre-processing stage, we first encode both the original graph at
task𝑇𝑡and𝑇𝑡−1by using non-parametric graph convolution. Then,
multiple environments are generated according to calculated em-
beddings. In condensation stage, we sample a initialization 𝜃for
𝜙𝜃(·)from the distribution Θ. Then, the KRR solution WSis cal-
culated according to the condensed graph. Finally, the embeddings
of constructed environments and original graph are condensed via
lossL𝑇𝐼𝐶.
A.2 Time Complexity Analysis
We show the detailed time complexity of OpenGC and compared
baselines in Table 6. Time complexities for both the pre-processing
and condensing phases are assessed separately and the condensing
procedure is further divided into the forward propagation, process
execution, loss calculation, condensed graph updating, and relay
model updating. The relay model for all methods is 𝐾layer SGC
incorporating a linear layer with the hidden dimension denoted by
𝑏. For the original graph, 𝑁,𝐸,𝑑and𝐶are the number of nodes,
edges, feature dimensions, and classes, respectively. The number
of nodes in the condensed graph is represented by 𝑁′and the
pre-defined adjacency matrix is utilized across all methods.
The pre-processing stage for GCond and GCDM incorporates
non-parametric graph convolution. SFGC entails the training of
hundreds of teacher models and the quantity is denoted by 𝑍.
OpenGC incorporates environment generation during pre-processing
phase, introducing an additional time complexity of 𝑁′𝑑.
Due to the different optimization strategies utilized in GC meth-
ods, we decompose the condensing procedure into 5 stages. The
process execution stage varies between methods, involving dif-
ferent operations specific to each method. Specifically, GCond’s
process entails calculating the gradient w.r.t the relay model pa-
rameters twice. GCDM’s procedure involves computing the class
representation. SFGC necessitates updating the relay model on
the condensed graph 𝑞times to generate trajectories during each
iteration. OpenGC introduced the KRR for the closed-form solu-
tion, with the time complexity being O 𝑁′3+𝑁𝑏𝑐. Considering
𝑁′≪𝑁and eliminating the relay model updating, our proposed
method achieves a more efficient condensation procedure compared
to other baselines.
A.3 Dataset Details
We follow [ 4] to process Yelp and Taobao to construct the open-
world graph datasets.Algorithm 1: The optimization framework of OpenGC
1Input: Original graph at task 𝑇𝑡and historic task 𝑇𝑡−1:T𝑡
andT𝑡−1
2Output: Condensed graphS𝑡={I,X′
𝑡}
3▷Pre-processing
4Compute embedding H𝑡andH𝑡−1with Eq. (8)
5Generate environments H𝑒
𝑡with Eq. (11)
6▷Graph condensation
7Initialize X′
𝑡andY′
𝑡
8for𝑙=1,...,𝐿 do
9 Initialize𝜃∼Θ
10 Compute embedding H′
𝑡ofS𝑡with Eq. (8)
11 Compute WSwith Eq. (13)
12 ComputeL𝑇𝐼𝐶with Eq. (15)
13 X′
𝑡←X′
𝑡−𝜇∇X′
𝑡L𝑇𝐼𝐶
14Return: Condensed graphS𝑡
Yelp5is a large business review website where people can upload
their reviews for commenting business, and find their interested
business by others’ reviews. According to reviews, we construct
a business-to-business temporal graph. Specifically, we take the
data from 2016 to 2021, and treat the data in each year as a task,
thus forming 6 tasks in total. In each year, we sample the 2 largest
business categories as classes in each task. The newly added nodes
in later tasks will cover all observed classes in former task classes.
We regard each business as a node and set the business’s category
as its node label. The temporal edge will be formed, once a user
reviews the corresponding two businesses within a month. We
initialize the feature representation for each node by averaging
300-dimensional GloVe word embeddings of all reviews for this
business following the previous work [12].
Taobao6is a large online shopping platform where items can be
viewed and purchased by people online. For the Taobao dataset, we
construct an item-to-item graph, in the same way as Yelp. The data
in the Taobao dataset is a 6-day promotion season of Taobao in 2018.
The data in each day is treated as a task and we sample 3 largest
item categories in each task. We regard the items as nodes and take
the categories of items as the node labels. The temporal edge will
be built if a user purchases 2 corresponding items in the promotion
season. We use the 128-dimensional embedding provided by the
original dataset as the initial feature of the node.
Flickr [ 46] is an image network where each node in the graph
represents one image. If two images share some common properties
(e.g., same geographic location, same gallery, comments by the same
user, etc.), an edge will be established between these two images.
All nodes are classified into 7 classes and node features are the 500-
dimensional bag-of-word representations. We randomly choose 2
classes as the initial task and each subsequent task adds a new class.
Coauthor [ 35] is a co-authorship graph. Nodes represent authors
and are connected by an edge if they co-authored a paper. Node
features represent paper keywords for each author’s papers, and
class labels indicate the most active fields of study for each author.
5https://www.yelp.com/dataset
6https://tianchi.aliyun.com/dataset/dataDetail?dataId=9716
 
861KDD ’24, August 25–29, 2024, Barcelona, Spain Xinyi Gao et al.
Table 6: The comparison of the time complexity. The process in condensing procedure varies for different GC methods. GCond
includes the gradient calculation. GCDM includes class representation calculation. OpenGC includes the calculation of KRR.
Pr
e-processingCondensing
procedure
For
ward Process Loss Update SUpdate𝑓
GCondO(𝐾
𝐸𝑑)O(
(𝑁+𝑁′)𝑑𝑏)O(𝑑𝑏)O(𝑑𝑏)O(𝑁′𝑑)O(𝑡𝑖𝑛𝑑𝑏)
GCDMO(𝐾
𝐸𝑑)O(
(𝑁+𝑁′)𝑑𝑏)O((𝑁+𝑁′)𝑏)O(𝐶𝑏)O(𝑁′𝑑)O(𝑡𝑖𝑛𝑑𝑏)
SFGCO(𝑍(𝐾
𝐸𝑑+𝑁𝑑𝑏))O(𝑞
𝑁′𝑑𝑏) N/AO(𝑑𝑏)O(𝑁′𝑑)O(𝑞𝑑𝑏)
OpenGCO(𝐾
𝐸𝑑+𝑁′𝑑)O(
(𝑁+𝑁′)𝑑𝑏)O 𝑁′3+𝑁𝐶𝑏O(𝐶𝑏)O(𝑁′𝑑) N/A
Table 7: Dataset statistics.
#No
des #Edges# Total
classes# Timespan # Tasks# Classes
per task# Timespan
per task
Y
elp 12,853
179,612 12 6 years 6 2 1 year
Taobao 51,358
364,010 18 6 days 6 3 1 day
Flickr 89,250
899,756 7 N/A 6 1 N/A
Coauthor 18,333
163,788 15 N/A 7 2 N/A
Table 8: The node, edge, and class distribution of different tasks.
Dataset T
ask 1
2 3 4 5 6 7
Y
elp#no
de 1,623
3,504 6,491 8,735 10,259 12,853 N/A
#e
dge 7,682
22,306 65,662 109,298 133,148 179,612 N/A
#class 2
4 6 8 10 12 N/A
T
aobao#no
de 1,817
6,553 12,151 15,979 27,399 51,358 N/A
#e
dge 6,726
23,686 51,480 71,902 137,376 364,010 N/A
#class 3
6 9 12 15 18 N/A
F
lickr#no
de 2,294
5,870 10,671 23,127 37,322 89,250 N/A
#e
dge 5,742
19,496 41,184 156,654 268,810 899,756 N/A
#class 2
3 4 5 6 7 N/A
Coauthor#no
de 459
1,221 2,495 4,193 6,411 9,855 18,333
#e
dge 418
2,310 7,052 16,194 32,988 64,480 163,788
#class 3
5 7 9 11 13 15
Similar to Flicker, we randomly choose 3 classes as the initial task
and each subsequent task adds 2 new classes.
The detailed dataset statistics and task distribution for each
dataset are shown in Table 7 and Table 8, respectively.
A.4 Related Work
Graph condensation. Graph condensation is designed to reduce
GNN training costs through a data-centric perspective and most
GC methods focus on improving the GNN accuracy trained on
the condensed graph. For example, SFGC [ 14] introduces trajec-
tory matching in GC and proposes to align the long-term GNN
learning behaviors between the original graph and the condensed
graph. GCEM [ 24] focuses on improving the performance of dif-
ferent GNN architectures trained on the condensed graph. It cir-
cumvents the conventional relay GNN and directly generates the
eigenbasis for the condensed graph to preserve high-frequency
information. Besides improving the quality of condensed graphs,
GC has been widely used in various applications due to its excellent
graph compression performance, including inference acceleration
[5], continual learning [ 25], hyper-parameter/neural architecture
search [ 3] and federated learning [ 32]. Although GC is developed
on various applications, none of them focus on the evolution of
graphs in real-world scenarios. Our proposed method is the first toexplore this practical problem and contains its significance in GC
deployment.
Invariant learning for out-of-distribution generalization. The
invariant learning approaches are proposed to reveal invariant
relationships between the inputs and labels across different dis-
tributions while disregarding the variant spurious correlations.
Therefore, numerous methods are developed to improve the out-of-
distribution (OOD) generalization of models, which refers to the
ability to achieve low error rates on unseen test distributions. For
example, Invariant Risk Minimization [ 1] improves the empirical
risk minimization and includes a regularized objective enforcing si-
multaneous optimality of the classifier across all environments. Risk
Extrapolation [ 19] encourages the equality of risks of the learned
model across training environments to enhance the model sensitiv-
ity to different environments. Recent works utilize invariant learn-
ing in OOD graph generalization problem [ 23,41,44,53], and the
most critical part of them is how to design invariant learning tasks
and add proper regularization specified for extracting environment-
invariant representations. However, these methods are all model-
centric and only concentrate on enhancing the GNN model to
extract the invariant features. In contrast, our proposed method
introduces invariant learning in the data-centric GC method, en-
abling the GNNs trained on the condensed graph all contain the
OOD generalization.
 
862