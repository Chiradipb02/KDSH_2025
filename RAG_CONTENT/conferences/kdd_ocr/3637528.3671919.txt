SensitiveHUE: Multivariate Time Series Anomaly Detection by
Enhancing the Sensitivity to Normal Patterns
Yuye Feng
fengyuye@hikvision.com
Hikvision Research Institute
Hangzhou, ChinaWei Zhang
zhangwei189@mails.ucas.ac.cn
Hikvision Research Institute
Hangzhou, ChinaYao Fu
fuyao@hikvision.com
Hikvision Research Institute
Hangzhou, China
Weihao Jiang
jiangweihao5@hikvision.com
Hikvision Research Institute
Hangzhou, ChinaJiang Zhu∗
zhujiang.hri@hikvision.com
Hikvision Research Institute
Hangzhou, ChinaWenqi Ren
renwenqi@hikvision.com
Hikvision Research Institute
Hangzhou, China
Abstract
Unsupervised anomaly detection in multivariate time series (MTS)
has always been a challenging problem, and the modeling based
on reconstruction has garnered significant attention. The insensi-
tivity of these methods towards normal patterns poses challenges
in distinguishing between normal and abnormal points. Firstly, the
general reconstruction strategies may exhibit limited sensitivity
to spatio-temporal dependencies, and their performance remains
largely unaffected by such dependencies. Secondly, most methods
fail to model the heteroscedastic uncertainty in MTS, hindering
their abilities to derive a distinguishable criterion. For instance,
normal data with high noise levels may lead to detection failure
due to excessively high reconstruction errors. In this work, we em-
phasize the necessity of sensitivity to normal patterns, which could
improve the discrimination between normal and abnormal points
remarkably. To this end, we propose SensitiveHUE, a probabilistic
network by implementing both reconstruction and heteroscedas-
tic uncertainty estimation. Its core includes a statistical feature
removal strategy to ensure the dependency sensitive property, and
a novel MTS-NLL loss for modeling the normal patterns in impor-
tant regions. Experimental results demonstrate that SensitiveHUE
exhibits nontrivial sensitivity to normal patterns and outperforms
the existing state-of-the-art alternatives by a large margin. Code is
publicly available at this URL1.
CCS Concepts
•Mathematics of computing →Time series analysis; •Com-
puting methodologies →Anomaly detection.
∗Corresponding Author
1http://github.com/yuesuoqingqiu/SensitiveHUE
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671919Keywords
Time series, Anomaly Detection, Uncertainty Estimation
ACM Reference Format:
Yuye Feng, Wei Zhang, Yao Fu, Weihao Jiang, Jiang Zhu, and Wenqi Ren.
2024. SensitiveHUE: Multivariate Time Series Anomaly Detection by En-
hancing the Sensitivity to Normal Patterns. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671919
1 Introduction
Time series anomaly detection is a pivotal technology in various
domains such as Cyber-Phsical System (CPS), cloud services, etc
[1,10,22]. It involves automatically identifying system faults based
on multivariate time series (MTS). In practical scenarios, these ex-
tensive data are collected from diverse channels (e.g., sensors or
monitors), with limited labels due to the scarcity of anomalies and
the high cost of manual labeling [ 8]. Consequently, most researches
in this field are conducted in an unsupervised manner. In general,
MTS exhibits two key characteristics, representing normal patterns
that are specific to the data. Characteristic 1: complex spatio-
temporal dependencies, including correlations between different
time intervals and across diverse channels, where typical anom-
alies may deviate from these dependencies [ 20];Characteristic
2: aleatoric uncertainty, which can be homoscedastic (with con-
stant noise unchanged over time) or heteroscedastic (with noise
covariates varying across temporal and spatial dimensions) [ 29].
These noises represents an inherent and persistent pattern resulting
from data collection or other processes. Only by maintaining a
high sensitivity to normal patterns can the model effectively
differentiate between normal and anomalous points, thus
improving the accuracy of anomaly detection significantly.
Common researches focus on capturing normal patterns through
reconstruction, prediction and etc [ 27].The reconstruction-based
methods, which encode the MTS in a latent space and minimize the
residual errors between the decoded results and the input series,
have attracted much attention recently [ 18,27,33]. However, these
methods also have easily overlooked drawbacks. We observed that
the reconstruction-based methods exhibit limited sensitivity to-
wards normal patterns, thus hindering their ability to differentiate
between normal and abnormal points.
 
782
KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
StrategyOriginal Temporal disruption Spatial disruption
F1∗F1∗Change F1∗Change
Compression 74.64 74.67 +0.04% 73.83 -0.82%
Mask 74.27 74.39 +0.16% 72.10 -2.92%
Ours 83.36 71.39 -14.36% 74.01 -11.22%
Table 1: The detection performance of different reconstruc-
tion strategies by disrupting spatial or temporal dependency
in SWaT [ 22] dataset. Change denotes the percentage change
rate of F1∗after disruption.
Firstly, we conduct an analysis from the sensitivity to spatio-
temporal dependency. For reconstruction-based methods, there
are mainly two approaches to implement reconstruction: compres-
sion [ 31] and mask based strategies [ 23]. The former significantly
reduces the dimensionality of feature space compared to that of
the input, while the latter randomly masks a portion of the input
data and then reconstruct them. To illustrate the insensitivity of
common reconstruction strategies to spatio-temporal dependency,
we employ a widely adopted anomaly detection framework (e.g.,
Transformer [ 32]) on SWaT dataset [ 22] to individually implement
reconstruction using these strategies. After training is completed,
we adopt two ways to disrupt the dependencies in test data [ 4]:
1)temporal disruption: all other points in time within the input
window are hidden for the reconstruction of each given point, i.e.,
completely disregarding temporal relationships; 2)spatial disrup-
tion: for each given channel, the points from the other channels
are all shuffled, i.e., the spatial arrangement has been completely
disrupted. More experimental details refer to Appendix B.3. The
results are presented in Tab. 1, showcasing the model’s detection
performance before and after the disruptions respectively. The in-
sensitivity of these strategies is demonstrated, as even complete
interruption of such dependencies has minimal impact on the detec-
tion performance. We observe that these approaches tend to fall into
some potential shortcuts, where good reconstruction results can be
achieved even if the spatial or temporal dependencies are ignored
[37]. Given the shortcomings in these strategies, their detection
performance falls short of expectations, highlighting the pressing
need for enhancing the sensitivity to spatio-temporal dependencies
in modeling.
Secondly, we emphasize the necessity of incorporating sensi-
tivity towards aleatoric uncertainty. In fact, the magnitude of re-
construction error fails to accurately reflect the level of abnormal-
ity. As shown in Fig.1, certain prominent fluctuations originating
from noise may result in noticeable reconstruction errors, while
genuine anomalies may manifest as relatively minor errors. The
detection failures in both of these scenarios can easily occur for
reconstruction-based methods. Therefore, it is crucial to enhance
the sensitivity to heteroscedastic uncertainty in order to resolve this
issue. A commonly employed approach involves quantifying the
aleatoric uncertainty in an end-to-end manner [ 3,24]. By assuming
that the target adhere to a particular distribution (e.g., Gaussian
distribution with mean and variance), past heteroscedastic uncer-
tainty estimation (HUE) methods commonly employ the negative
log-likelihood (NLL) loss [ 3] to optimize the estimates of mean
Detection
FailureFigure 1: The reconstructed results of a subsequence from
WADI [ 1] dataset. It employs the Transformer framework
and combines our SFR strategy, with the mean squared error
(MSE) between the input and output as the loss function.
and variance simultaneously, which represents the fitted value
and uncertainty [ 28], respectively. However, NLL loss is widely ac-
knowledged to be susceptible to overconfident variance estimates,
which tends to excessively focus on well-fit regions while providing
minimal variance estimation [ 14]. Consequently, the learning from
poorly-fit regions is often ignored, yet it may plays a crucial role
in modeling normal patterns [ 28]. Despite the recent proposals for
several alternative loss formulations [ 12,28,29], they are originally
designed for regression tasks, whose capabilities remain inadequate
for MTS anomaly detection. Specifically, HUE is primarily applied
to estimating the fitted values for the current window in our work,
so we collectively refer to these fitted values as the reconstruction
results.
To enhance the network’s sensitivity to normal patterns, 1)we
propose the concept of dependency sensitive and emphasize its
significance for reconstruction, while also highlighting the limita-
tions of existing reconstruction strategies. To ensure dependency
sensitivity, we propose a novel statistical feature reconstruction
(SFR) strategy, which involves the removal of statistical features
from the input window followed by the subsequent reconstruction
of the original sequence. SFR strategy exhibits enhanced sensitivity
to spatio-temporal dependency, leading to significant improvement
in detection performance, as depicted in Tab. 1; 2)we propose a
novel MTS-NLL loss, tailored for end-to-end heteroscedastic un-
certainty estimation in MTS. It forces our network to focus on
regions that may be hard to reconstruct but crucial for the learning
of normal patterns, and preserve the self-regularizing property of
traditional NLL loss, i.e. ignoring some regions with high signal-to-
noise ratio adaptively. Through the above strategies, we propose
a probabilistic framework SensitiveHUE, which combines a novel
MTS-NLL loss with heteroscedastic uncertainty estimation to si-
multaneously achieve reconstruction and uncertainty estimation.
It employs probability density value as the anomaly score and uti-
lizes both the reconstruction results and uncertainty estimation as
detection criteria. By enhancing the sensitivity to spatio-temporal
dependency and noise patterns, it can effectively improve the dis-
crimination between normal and anomalous segments. The main
contributions are summarized as follows:
 
783SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns KDD’24, August 25–29, 2024, Barcelona, Spain.
•To the best of our knowledge, we are the first to reveal the
limited sensitivity of conventional methods towards nor-
mal patterns. To tackle the issue, we propose a statistical
feature reconstruction strategy and incorporate uncertainty
estimation, accompanied by detailed theoretical analysis.
•We propose MTS-NLL loss for end-to-end heteroscedastic un-
certainty estimation in multi-channel data. It greatly enables
the model’s ability to discriminate anomalies from normal
sequences by enhancing the sensitivity to normal patterns
within important regions.
•Through extensive experiments on four public datasets, our
proposed model, SensitiveHUE, surpasses the current state-
of-the-art methods by a large margin.
2 Related Work
Unsupervised Anomaly Detection in MTS. Firstly, the method
based on reconstruction is the dominant paradigm in recent works.
Some advancements have focused on module improvements, such
as LSTM-VAE [ 26], OmniAnomaly [ 30], and D3R [ 33], which inte-
grate various modules to robustly learn features by minimizing the
reconstruction error. USAD [ 2] and TranAD [ 31] employ generative
adversarial networks to amplify the reconstruction error of anom-
alies through an adversarial process. Additionally, some studies pro-
pose extra constraints to enhance the learning of normal patterns.
Anomaly Transformer [ 36] suggests that normal points exhibit
stronger correlations with entire sequences compared to anomalies.
However, these methods often lack sensitivity to spatio-temporal
relationships, leading to fully reconstruction of both outliers and
normal points [ 37]. The lately work NPSR [ 18] takes a different
perspective to propose an induced score by integration of point
and sequence based reconstruction, which achieve improved detec-
tion performance. Secondly, prediction-based approaches have also
achieved promising results [ 5]. They predict future points by estab-
lishing spatial relationships, but the reliance on prior knowledge
for defining these relationships limits their further developments.
Thirdly, we explore some probabilistic methods. NISIBF [ 6] consid-
ers the dynamics of CPS and employs Bayesian filtering to induce
anomaly score, PUAD [ 19] utilizes prototype-guided generative
models to estimate observed probabilities. Despite some improve-
ments in sensitivity to uncertainty in data, these methods do not
specifically account for modeling aleatoric uncertainty.
Heteroscedastic Uncertainty Estimation. Uncertainty typi-
cally refers to model-dependent epistemic uncertainty and aleatoric
uncertainty in the data [ 17]. The focus of our attention lies in the
realm of aleatoric uncertainty. Conventional methods commonly
employ negative log-likelihood (NLL) loss [ 3,14] to estimate the
parameter of a predicted heteroscedastic Gaussian distribution for
target. However, due to the effect of predicted variance on gra-
dient updating, NLL loss often leads to overconfident variance
estimates and suboptimal mean fits. Seitzer et al .[28] proposes
𝛽-NLL loss to regulate the impact of high variances in gradient
updates effectively, which achieves enhanced performance across
diverse domains. Stirn et al .[29] mitigates this issue from an ar-
chitectural standpoint by introducing a covariance output head to
a homoscedastic network, thus preventing degradation of mean
estimates. Recently, Immer et al .[12] proposes reparameterizingStrategyOriginal
MSETemporal disruption Spatial disruption
Deviation Change Deviation Change
Compression 0.0040 0.0004 10.0% 0.0091 227.5%
Mask 0.0364 0.8804 2418.7% 0.0768 211.0%
Ours 0.1012 21.18 20928.9% 0.6727 664.7%
Table 2: The deviation of reconstructed results under differ-
ent reconstruction strategies by disrupting spatial or tem-
poral dependency in SWaT [ 22] dataset. Change denotes the
percentage change rate of MSE after disruption.
the NLL loss into a parameter pair loss and extends Laplace ap-
proximation inference to model regularization. Despite exhibiting
promising results in various tasks [ 7,25], the uncertainty estimation
remains largely overlooked in MTS anomaly detection.
This study aims to enhance the ability to distinguish outliers from
normal points by achieving dependency sensitive reconstruction
and reliable heteroscedastic uncertainty estimation simultaneously,
with a focus on maintaining high sensitivity to normal patterns.
3 Methodology
3.1 Problem Statement
To facilitate training and testing, a multivariate time series Xis
divided into multiple time windows. Let
𝑋=𝑋11···𝑋1𝑆
.........
𝑋𝑇1···𝑋𝑇𝑆∈R𝑇×𝑆(1)
denote a time window with length 𝑇and S channels. 𝑋𝑡𝑠denotes
the element in the 𝑡-th row and 𝑠-th column. The aim is to generate
an anomaly score for each timestamp of a given Xduring the test
phase. Subsequently, fractions with the highest anomaly scores are
identified as anomalies.
3.2 Dependency-Sensitive Reconstruction
3.2.1 Revisiting the Reconstruction Strategies. As mentioned in
Sec.1, even the temporal or spatial dependencies are completely dis-
rupted, the detection performance under common reconstruction
strategies are almost unchanged. The reason behind this phenom-
enon is that, these approaches often take potential shortcuts to
achieve reconstruction [ 37], even when complex dependencies are
ignored. To illustrate the insensitivity of these strategies to spatio-
temporal dependency, we present the definition as follows.
Definition 3.1.∀𝑋∈R𝑇×𝑆, let𝑋𝑡:,𝑋:𝑠be the𝑡-th row and 𝑠-
th column of 𝑋, and𝑓:R𝑇×𝑆→R𝑇×𝑆. If the map function 𝑓
satisfies that 𝑓({𝑋𝑡:}𝑇
𝑡=1)={𝑓𝑡(𝑋𝑡:)}𝑇
𝑡=1, where𝑓𝑡:R𝑆→R𝑆,
then𝑓is defined as time-independent. Similarly, if 𝑓({𝑋:𝑠}𝑆
𝑠=1)=
{𝑓𝑠(𝑋:𝑠)}𝑆
𝑠=1, where𝑓𝑠:R𝑇→R𝑇, then𝑓ischannel-independent.
The original reconstruction error under common strategies and
the deviation of reconstructed results after employing two disrup-
tions are presented in Tab.2. We quantify the deviation as the mean
square error (MSE) between the original reconstructed results and
 
784KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
post-disruption results. For compression based strategy, the model-
ing may easily fall into the shortcut by self-information mapping
[18], where the reconstruction function 𝑓is both time and channel-
independent approximately. Even in the absence of temporal or
spatial dependencies, the reconstructed results exhibited negligi-
ble variations, with average point-to-point deviations of 0.0004
and0.0091, respectively. While for mask based strategy, spatial dis-
ruption yields minimal deviation of 0.0768. This suggests that the
reconstruction function of masked points is approximately channel-
independent. Tab. 2 illustrates that, the reconstructed results under
common strategies may hardly change after disrupting temporal or
spatial relationships, leading to small variance in detection perfor-
mance. These insensitivity to spatio-temporal dependencies greatly
hinder the isolation of anomalies.
3.2.2 Statistical Feature Reconstruction. To effectively capture nor-
mal patterns, we aim to identify a reconstruction strategy that must
be highly sensitive to both spatial and temporal dependencies to
ensure the accuracy of the reconstructed results.
Definition 3.2. For a reconstruction strategy , if the reconstruc-
tion function 𝑓is neither time-independent nor channel-independent,
and the overall reconstruction error is close to 0, then it is defined
asdependency sensitive . Otherwise, it is dependency insensitive.
Apparently, compression and mask based reconstruction are
both dependency insensitive. Recovering the information lost in
such strategies does not necessarily rely on adequate modeling of
spatio-temporal dependencies. Hence, we aim to propose a depen-
dency sensitive strategy that compels the model to rely on these
dependencies for restoring the original sequence accurately. In the
following, a theorem is proposed to ensure dependency sensitive.
Theorem 3.3. LetHbe the set of functions: H={ℎ|ℎ:R𝑇×𝑆↦→
R𝑇×𝑆}. Denote𝑋𝑔=𝑔(𝑋), and𝜖H𝑔=infℎ∈HE𝑋(||ℎ(𝑋𝑔)−𝑋||2
2).
Assume that it satisfies the following conditions: 1) 𝑔:R𝑇×𝑆↦→
R𝑇×𝑆is approximately reversible, i.e., ∃ℎ∈H , so that𝜖H𝑔→0; 2)
DenoteHT−⊂H ,∀𝑓∈HT−, it is time-independent. It satisfies
that𝜖HT−
𝑔≫𝜖H𝑔; 3) Similarly, denote HS−⊂H ,∀𝑓∈HS−, it
is channel-independent. It satisfies that 𝜖HS−
𝑔≫𝜖H𝑔. The target
mapping𝑓:𝑋↦→ℎ(𝑋𝑔)is considered as dependency sensitive if
these conditions are satisfied.
Proof: 1) If𝑔:R𝑇×𝑆↦→R𝑇×𝑆is approximately reversible, then
there existsℎ∈H, s.t. the reconstruction error E𝑋(||ℎ(𝑋𝑔)−𝑋||2
2)
of the function 𝑓:𝑋↦→ℎ(𝑋𝑔)can be guaranteed to be approx-
imately equal to 0; 2) If 𝜖HT−
𝑔≫𝜖H𝑔, to achieve a small recon-
struction error by 𝑓:𝑋↦→ℎ(𝑋𝑔), the function 𝑓cannot be time-
independent since ℎ∉HT−; 3) Similarly, if 𝜖HS−
𝑔≫𝜖H𝑔, the
reconstruction function 𝑓cannot be channel-independent since
ℎ∉HS−. Thus, the target mapping 𝑓:𝑋↦→ℎ(𝑋𝑔)satisfies the
dependency sensitive property in Def. 3.2. Specifically, the recon-
struction error of 𝑓can be guaranteed to be close to 0, and it is
neither time-independent nor channel-independent.
Under the context of generalized MTS, we observe that, a strategy
termed statistical feature removal (SFR) satisfies Theorem 3.3 well.Proposition 3.4. (Statistical Feature Removal) For 𝑋∈R𝑇×𝑆,
the mean and variance of each univariate series in 𝑋are given by:
E𝑠[𝑋]=1
𝑇𝑇∑︁
𝑡=1𝑋𝑡𝑠,Var𝑠[𝑋]=1
𝑇𝑡∑︁
𝑡=1(𝑋𝑡𝑠−E𝑠[𝑋])2.(2)
For𝑋𝑔=𝑔(𝑋), it satisfies:
𝑋𝑔
𝑡𝑠=𝑋𝑡𝑠−E𝑠[𝑋]√︁
Var𝑠[𝑋]+𝜖0, (3)
where𝜖0∈R+is a small value.
The proof is shown in Appendix A. Our strategy begins with elim-
inating the local statistical features (i.e., {(E𝑠[𝑋],Var𝑠[𝑋])}𝑆
𝑠=1)
within each input window, and then compels the network to rely
on spatio-temporal relationships for reconstructing the input data
prior to feature elimination. When the local statistical features vary
over time, this reconstruction strategy could be very effective in
ensuring the dependency sensitivity of the network. As shown in
Tab.2, disrupting these dependencies results in substantial devia-
tions in the reconstruction results, particularly when disregarding
temporal dependency, with a magnitude of 21.18. This discrepancy
arises due to the necessity of utilizing data from all time points
within the window when performing the recovery of removed
statistics. Note that our strategy completely differs from the com-
monly employed RevIN in prediction tasks [ 16], with the statistical
feature retrieval after representation learning. RevIN is originally
designed to tackle the issue of data distribution drift, while our
strategy completely excludes the statistical information to enable
the dependency sensitive property. Further, it should be pointed out
that, once the statistical features are retrieved, the reconstruction
function is no longer dependency sensitive. We will reveal this in
Sec. 4.3.
3.3 Heteroscedastic Uncertainty Estimation
As stated in Sec.1, heteroscedastic uncertainty naturally exists in
MTS. The reconstruction error alone may not be sufficient for
identifying anomalies in reconstruction-based models, as it strug-
gles to differentiate between unpredictable fluctuations caused by
noise and genuine anomalies. To address this issue, we employ het-
eroscedastic uncertainty estimation to enhance sensitivity towards
aleatoric uncertainty, thereby achieving simultaneous reconstruc-
tion and uncertainty estimation. In this section, we first review
some recent progresses on HUE, and then present our MTS-NLL
loss tailored for anomaly detection in MTS.
3.3.1 Preliminaries. Within the context of multivariate time series
data, we define 𝑋,ˆ𝑋∈R𝑇×𝑆as the input and target of the network
respectively. To implement heteroscedastic uncertainty estimation,
it is generally assumed that the elements in the target matrix ˆ𝑋
adhere to the following conditions:
ˆ𝑋𝑡𝑠∼N(𝜇𝑡𝑠,𝜎2
𝑡𝑠), 𝑡=1,···𝑇;𝑠=1,···,𝑆, (4)
whereNdenotes the Gaussian distribution, 𝜇𝑡𝑠and𝜎2
𝑡𝑠quantify
the mean and variance, respectively. In an end-to-end way, neural
networks are commonly employed to simultaneously generate es-
timations ˆ𝜇𝑡𝑠,ˆ𝜎2
𝑡𝑠of mean and variance, which correspond to the
reconstructed result and uncertainty estimation, respectively.
 
785SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns KDD’24, August 25–29, 2024, Barcelona, Spain.
For reconstruction-based models, the loss function is generally
formalized by Mean Squared Error (MSE): L𝑀𝑆𝐸=Í𝑇
𝑡=1Í𝑆
𝑠=1(ˆ𝜇𝑡𝑠−
𝑋𝑡𝑠)2,which is equivalent to assume ˆ𝑋𝑡𝑠∼N(𝜇𝑡𝑠,1), thus com-
pletely ignoring the aleatoric uncertainty in MTS.
3.3.2 MTS-NLL Loss. It is common to maximize the observation
probability by minimizing negative log-likelihood (NLL) loss [ 3,14]:
LNLL(ˆ𝜇𝑡𝑠,ˆ𝜎𝑡𝑠)=(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2ˆ𝜎2
𝑡𝑠+1
2lnˆ𝜎2
𝑡𝑠+const. (5)
The gradients ofLNLLwith respect to (w.r.t) ˆ𝜇𝑡𝑠and ˆ𝜎𝑡𝑠are
▽ˆ𝜇𝑡𝑠LNLL=ˆ𝜇𝑡𝑠−𝑋𝑡𝑠
ˆ𝜎2
𝑡𝑠, (6)
▽ˆ𝜎2
𝑡𝑠LNLL=ˆ𝜎2
𝑡𝑠−(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2(ˆ𝜎2
𝑡𝑠)2. (7)
Clearly, the gradient ▽ˆ𝜇𝑡𝑠LNLLof mean scales the error ˆ𝜇𝑡𝑠−𝑋𝑡𝑠
by the factor1
ˆ𝜎2
𝑡𝑠. Specifically, the gradient of mean converges to
1
ˆ𝜇𝑡𝑠−𝑋𝑡𝑠when the variance is stabilized at its stand point ˆ𝜎2
𝑡𝑠=
(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2. As a result, the gradient inevitably assigns greater
weight to data points with smaller reconstructed error, which causes
premature convergence and impedes the learning process in regions
that are poorly fitted. In order to mitigate this problem, several
studies have made advancements in optimizing the gradient-based
process, such as 𝛽-NLL [28].
𝛽-NLL. The weight of the gradient assigned to each point is
carefully regulated by ˆ𝜎2𝛽
𝑡𝑠, where𝛽∈[0,1], as
L𝛽−NLL=⌊ˆ𝜎2𝛽
𝑡𝑠⌋[(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2ˆ𝜎2
𝑡𝑠+1
2lnˆ𝜎2
𝑡𝑠+const], (8)
where⌊·⌋means the stop gradient operation. Hence, the ⌊ˆ𝜎2𝛽
𝑡𝑠⌋can
serve as a dynamic weight controller, with the gradients changed
as
▽ˆ𝜇𝑡𝑠L𝛽−NLL=ˆ𝜇𝑡𝑠−𝑋𝑡𝑠
ˆ𝜎2−2𝛽
𝑡𝑠, (9)
▽ˆ𝜎2
𝑡𝑠L𝛽−NLL=ˆ𝜎2
𝑡𝑠−(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2(ˆ𝜎2
𝑡𝑠)(2−𝛽). (10)
During training, appropriate 𝛽between 0and1ensures that the
well-fit points are no longer be over-emphasized, while the poorly-
fit points are more considered.
In addition, some other prevalent modified methods such as faith-
ful heteroscedastic and natural parametrization refer to [ 29][12].
However, these methods are originally designed for regression tasks
of univariate time series, and when used directly for the case of
anomaly detection in MTS, there will inevitably be some defects.
Similar to the previous advancements, our objective is to avoid
the issues encountered by NLL in gradient updates and develop
an optimization target that aligns better with anomaly detection
task in MTS. When confronted with multi-channel data, there are
two cases shown in Fig. 2 that need special attention during the
modeling process. Case 1: it is advantageous to reduce the influence
of the regions with low signal-to-noise ratios appropriately, which
tend to persist for a prolonged duration and be concentrated in
certain high-noise channels. These regions are difficult to fit due
to the presence of noisy interference with fluctuating amplitudes.
Figure 2: Two hard-to-fit cases in SWaT dataset [22].
Case 2: there are some regions that are difficult to reconstruct even
after excluding noise as a cause, representing important patterns
that exhibit transient behavior and require heightened attention, as
they often reflect non-trivial behavior that is crucial for modeling.
For NLL loss, the regions that are challenging to fit in both cases
tend to be disregarded entirely, which hampers the effectiveness of
anomaly detection. While for 𝛽-NLL loss, it magnifies the weight of
data points with high variances, thus focusing more on long term
data from specific channels in case 1, while those shorter and less
common data in case 2 are easily be ignored. Consequently, NLL
and𝛽-NLL fail to distinguish between these two cases effectively.
To tackle these issues, it is imperative to reallocate weights among
different regions. Therefore, we propose a novel MTS-NLL loss
tailored for multi-channel data.
MTS-NLL. Different from the previous content, we denote the
input as𝑋∈R𝐵×𝑇×𝑆, and the output variance as ˆ𝜎2(𝑋)∈R𝐵×𝑇×𝑆,
where𝐵is batch size. Denote
ˆ𝜎2
𝑠,mean=1
𝐵𝑇𝐵∑︁
𝑏=1𝑇∑︁
𝑡=1[ˆ𝜎2(𝑋)]𝑏𝑡𝑠 (11)
as the average of variance estimates for the 𝑠-th channel of in-
put data. We multiply 𝛽-NLL loss by a normalizer ⌊1
ˆ𝜎2𝛼𝑠,mean⌋that is
dependent on the channel of data. It is formalized as
LMTS-NLL(ˆ𝜇𝑡𝑠,ˆ𝜎𝑡𝑠)=⌊ˆ𝜎2𝛽
𝑡𝑠
ˆ𝜎2𝛼𝑠,mean⌋[(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2ˆ𝜎2
𝑡𝑠+1
2lnˆ𝜎2
𝑡𝑠+const],
(12)
whose gradients w.r.t ˆ𝜇𝑡𝑠and ˆ𝜎𝑡𝑠are
▽ˆ𝜇𝑡𝑠LMTS-NLL =ˆ𝜇𝑡𝑠−𝑋𝑡𝑠
ˆ𝜎2−2𝛽
𝑡𝑠·ˆ𝜎2𝛼𝑠,mean, (13)
▽ˆ𝜎2
𝑡𝑠LMTS-NLL =ˆ𝜎2
𝑡𝑠−(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2(ˆ𝜎2
𝑡𝑠)(2−𝛽)·ˆ𝜎2𝛼𝑠,mean, (14)
where𝛼∈[0,1]. Firstly, we simply set 𝛽=1in our experiments.
This ensures that in one batch, for the data from each channel, the
gradient▽ˆ𝜇𝑡𝑠LMTS-NLL of mean scales the error ˆ𝜇𝑡𝑠by an identi-
cal factor ˆ𝜎2𝛼𝑠,mean in different time intervals, thereby avoiding the
ignorance of case 2. Secondly, the scaling factor1
ˆ𝜎2𝛼𝑠,meancan further
quantify the weights of different channels, whose distributions may
be completely discrepant. For channels with higher ˆ𝜎2𝑠,mean, their
contribution to the gradients w.r.t both ˆ𝜇𝑡𝑠andˆ𝜎2
𝑡𝑠could be clearly
reduced. This self-regularizing property enables the network to
 
786KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
Reconstruction
Uncertainty
EstimationTransformer 
Encoder
Statistical Feature
Removal
Statistical Feature
Set of univariate sequences with the same statistical features Univariate sequence
 Uncertainty Weight
Uncertainty WeightingMTS-NLL
Loss
Grad
Figure 3: The overall framework of SensitiveHUE. Statistical Feature Removal aims to remove the statistical features of different
channels in the input, thus transforming them into sequences with the same statistical features, which the network needs to
reconstruct the original sequences. Through uncertainty weighting, our MTS-NLL loss could effectively optimize the process of
reconstruction and uncertainty estimation.
jump out of the regions with smaller benefits during optimization,
and focuses more on the reconstruction of regions that truly matters,
thus enhancing the discrimination between normal and abnormal
instances.
3.4 Overall Structure of SensitiveHUE
As illustrated in Fig. 3, SensitiveHUE takes a window 𝑋as input
and begins by statistical feature removal (SFR) of each channel
within𝑋. Transformer encoder is utilized to extract representations
𝐻𝐿∈R𝑇×𝐷, where𝐿,𝐷 are the number of encoder layers and
hidden dimension respectively. Then we use two linear layers to
obtain the estimates of mean 𝜇and variance 𝜎2(with dimensions
identical to𝑋) as follows, which represents the reconstruction and
uncertainty, respectively.
ˆ𝜇=𝑊𝜇𝐻𝐿+𝑏𝜇,ˆ𝜎2=exp(𝑊𝜎𝐻𝐿+𝑏𝜎). (15)
The network is trained end-to-end based on MTS-NLL loss. During
testing, anomaly scores are computed using the original NLL loss,
utilizing both estimated ˆ𝜇and ˆ𝜎2.
Specifically, the reconstruction process of our model can be
directly understood as the reconstruction function 𝑓in Theorem
3.3, where the SFR represents the mapping 𝑔(𝑋), andℎ(𝑋𝑔)includes
the Transformer encoder and the linear layer acting as decoder.
3.5 Anomaly Score
The anomaly score is calculated by NLL as follows.
𝑆(𝑡,𝑠)=(ˆ𝜇𝑡𝑠−𝑋𝑡𝑠)2
2ˆ𝜎2
𝑡𝑠+1
2lnˆ𝜎2
𝑡𝑠. (16)
As the dynamics of each channel may be discrepant, we rescale
𝑆(𝑡,𝑠)by a robust normalization:
e𝑆(𝑡,𝑠)=𝑆(𝑡,𝑠)−Median(𝑠)
IQR(𝑠), (17)
where Median(𝑠)andIQR(𝑠)denote the median and inter-quartile
range of𝑆(·,𝑠)respectively. For each timestamp 𝑡, we use thelargest value in e𝑆(𝑡,·)to quantify its anomaly score, i.e. ˆS𝑡=
max{e𝑆(𝑡,𝑠)|𝑠=1,···,𝑆}. Finally, an appropriate threshold 𝛿is
chosen to determine which time points are outliers.
4 Experiments
4.1 Experimental Setup
Dataset Description. Four public datasets are leveraged to validate
the effectiveness of our method: 1) SWaT (Secure Water Treatment)
[22] dataset consists of the recordings from 51 sensors in a modern
industry control system (ICS). It is collected by continuous sam-
pling at the frequency of 1s. 2) WADI (Water Distribution) [ 1]
testbed is similar to SWaT. It is acquired from a larger city system
with 123 sensors which operates for about half a month. 3) MSL
(Mars Science Laboratory) [10] rover dataset is released by NASA.
There are 27 different entities in total, and each of them contains
anomaly ground truths derived from the Incident Surprise Anomaly
(ISA) reports. 4) SMD (Server Machine Dataset) [ 30] is built by a
large Internet company. The stacked traces of resource utilization
(e.g. CPU utilization) from a server cluster are recorded. Detailed
statistical information of these datasets are shown in Appendix B.1.
Baselines. As stated in Sec. 2, twelve prevalent algorithms for
MTS anomaly detection are selected as baselines. 1)Reconstruction-
based: LSTM-VAE [ 26], OmniAnomaly [ 30], USAD [ 2], TranAD [ 31],
Anomaly Transformer [ 36],D3R[33], PUAD [ 19], NPSR [ 18];2)
forecasting-based: GDN [ 5], which intends to model the spatial de-
pendencies using GNN; 3)probability-based: DAGMM [ 38], NSIBF
[6];4)classical methods: Isolation Forest [ 21]. We try to use the
best results presented in the published papers [ 9,15]. For some
methods, such as D3R[33], PUAD [ 19] and NPSR [ 18], detailed
results on both F1∗andF1∗
PAmay be not available, so we use the
public repositories to reproduce the values. Specially, as the experi-
mental settings of NPSR are not reasonable, e.g. inserting spikes
to the induced anomaly scores, the reproduced results are slightly
worse than the published results.
 
787SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns KDD’24, August 25–29, 2024, Barcelona, Spain.
ModelSWaT WADI MSL SMD
P R F1∗F1∗
PAP R F1∗F1∗
PAP R F1∗F1∗
PAP R F1∗F1∗
PA
IF 91.32 63.02 74.57 86.19 24.59 38.60 30.04 69.38 19.92 71.17 31.18 83.31 22.40 53.04 31.50 89.62
DAGMM 27.46 69.52 39.37 85.33 54.44 26.99 36.09 61.65 25.91 62.86 36.69 70.09 18.29 50.46 26.85 72.29
NSIBF 96.21 65.27 77.78 91.90 7.13 84.76 13.16 23.64 20.19 63.02 30.58 75.03 36.86 58.67 45.27 81.82
GDN 99.35 68.12 80.82 93.50 97.50 40.19 56.92 85.52 18.62 83.51 30.44 90.30 44.70 64.78 52.90 71.61
LSTM-VAE 96.24 59.91 73.85 80.48 87.79 14.45 24.82 38.01 23.73 80.58 36.66 67.77 26.89 58.00 36.74 80.83
OmniAnomaly 98.25 64.97 78.22 86.61 99.47 12.98 22.96 41.72 16.19 84.66 27.18 89.94 20.61 46.73 28.60 94.43
USAD 98.51 66.18 79.17 84.60 99.47 13.18 23.28 42.96 28.65 77.90 41.89 92.72 21.57 55.53 31.07 94.63
TranAD 94.86 61.49 74.61 81.51 88.76 15.50 26.39 49.51 29.06 75.96 42.04 94.94 30.27 53.43 38.65 96.05
AnomalyTran 12.00 100.00 21.43 94.07 5.79 43.43 10.21 89.10 - - 2.10 93.59 - - 2.12 92.33
D3R 12.04 99.59 21.49 90.55 6.32 83.19 11.75 35.46 11.04 93.01 19.74 87.44 23.70 52.63 32.68 95.09
PUAD 97.94 60.52 74.81 82.74 95.51 15.00 25.92 41.25 25.85 75.03 38.45 95.04 26.95 56.27 36.44 96.16
NPSR 93.42 75.52 83.52 91.07 78.43 50.33 61.31 75.22 24.03 83.92 37.37 93.20 26.58 62.36 37.27 76.95
Ours 94.68 87.74 91.08 96.75 86.51 58.73 69.96 92.25 33.05 71.26 45.16 98.42 29.54 60.80 39.76 96.33
Table 3: Performance comparison on different datasets. P,Rdenote precision (%) and recall (%) respectively, F1∗
PAdenotes F1-score
after point adjustment. The terms highlighted in bold indicate the optimal results, while the second best ones are underlined.
IF and AnomalyTran denotes Isolation Forest and Anomaly Transformer respectively.
(a) Anomaly Transformer
 (b)D3R
(c) NSIBF
 (d) SensitiveHUE
Figure 4: The distribution of anomaly scores presented by
different frameworks on SWaT dataset.
For heteroscedastic uncertainty estimation, we compare our
method with three modified HUE method, including 𝛽-NLL loss
[28], faithful heteroscedastic [ 29] and natural parametrization [ 12].
Evaluation Metrics. We evaluate the performances of all meth-
ods using: 1)F1∗(with Precision ( P) and Recall ( R), the best F1 score
obtained by the anomaly score; 2)F1∗
PA,F1∗after point adjustment
(PA) [ 35]. PA can be illustrated as that, if at least one timestamp in
a successive anomalous sequence X𝐴is detected, all timestamps
in that sequence are considered to be correctly detected. F1∗
PAis
widely used for evaluation of detection tasks [ 13,30,35], but it
is considered unfair in some researches as it may exaggerate the
performance of the model [ 15]. More details refer to Appendix B.2.4.2 Overall Results
The overall performance of the aforementioned methods are shown
in Tab. 3. SensitiveHUE obtains the best results for all the datasets
except SMD, and even surpasses the others a large margin on SWaT
and WADI. Specifically, compared to the best of the baselines, our
framework improve F1∗from 83.52%to91.08%(+7.56%) on SWaT,
and from 61.31%to69.96%(+8.65%) on WADI. For MSL and SMD,
our method does not show clear improvements on F1∗. This is
mainly caused by the excessive intensive annotation of outliers in
these datasets [ 15,34]. SensitiveHUE fully exploits the inherent
spatio-temporal dependencies and presents the estimation of het-
eroscedastic uncertainty, significantly enabling the discrimination
of anomaly and normal points.
Note that either F1∗orF1∗
PAalone may be inadequate to evalu-
ate the detection performance [ 15]. For instance, the F1∗obtained
by Anomaly Transformer is not ideal on SWaT and WADI, but it
performs well for F1∗
PA. On the contrary, GDN achieves the best
F1∗on SMD dataset, while the value of F1∗
PAis unsatisfactory.
Fig. 4 shows the distribution of anomaly scores for SWaT dataset
obtained by different methods. For uncertainty-aware methods like
NSIBF and SensitiveHUE, there exists clear boundaries for normal
points and anomalies, while they are hardly separable for entire
reconstruction-based methods. The F1∗for NSIBF is not ideal due to
its failure in considering dependency sensitivity or accurately mod-
eling uncertainty, resulting in the scores of a considerable number
of anomalies being close to those of normal points. These prob-
lems are tackled by SensitiveHUE effectively, leading to the great
discrimination between the normal fluctuations and true anomalies.
4.3 Ablation Study
To investigate the effectiveness of the major components in Sensi-
tiveHUE, we compare the performance achieved by different recon-
struction strategies, and the influence of different loss functions for
anomaly detection in MTS, as shown in Tab. 4.
Reconstruction Strategy. To illustrate the difference with
and without statistical feature retrieval, we also adopt RevIN [ 16]
 
788KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
Reconstruction
StrategyLoss
FunctionSWaT WADI MSL SMDCountF1∗F1∗
PAF1∗F1∗
PAF1∗F1∗
PAF1∗F1∗
PA
Compression MTS-NLL 75.90 87.97 45.08 65.93 36.62 94.96 38.12 95.79 0
Mask MTS-NLL 80.05 87.95 41.78 79.67 40.78 94.92 39.72 96.97 1
RevIN MTS-NLL 25.82 87.25 18.81 82.69 26.29 92.84 34.80 95.39 0
SFR NLL 80.19 94.50 59.60 87.29 35.42 93.11 38.54 89.40 0
SFR𝛽-NLL (𝛽=0.5) 85.15 96.21 49.85 67.57 41.96 96.27 37.85 96.43 0
SFR 𝛽-NLL (𝛽=1) 87.10 96.58 55.90 76.08 43.81 98.35 39.37 96.75 0
SFR Faithful 87.57 94.24 61.76 86.82 43.60 97.69 38.02 95.27 0
SFR Natural 87.57 94.79 51.17 85.53 40.18 96.34 29.43 94.52 0
SFR MTS-NLL 91.08 96.75 69.96 92.25 45.16 98.42 39.76 96.33 7
Table 4: Ablation study on different reconstruction strategies and optimization objectives. The terms highlighted in bold
indicate the optimal results, while the second best ones are underlined.
12 24 48 96 192
window size60708090Metric Value (%)
0 0.25 0.5 0.75 1
SWaT-F1*SWaT-F1*
PAWADI-F1*WADI-F1*
PA
Figure 5: Parameter sensitivity of window size 𝑆and coeffi-
cient𝛼.
for comparison. It can be concluded that, 1)our statistical feature
reconstruction (SFR) is more applicative than other strategies. This
is due to our goal of forcing the model to fully consider the spatio-
temporal dependencies in MTS, which is mainly ignored by others;
2)after the retrieval of statistical feature, a dramatical drop of
performance occurs ( F1∗from 91.08% to25.82% on SWaT). This
is consistent with our analysis, as a clear shortcut exists where
the network tends to simply copy the input without considering
spatio-temporal dependencies during the reconstruction process.
Loss Function. It should be noted that the gradient updating of
these loss functions weight data points differently, so their optimiza-
tion processes end with very different solutions. By assigning more
weights on hard-to-fit regions, 𝛽-NLL [28], Faithful [ 29] and Natu-
ral [12] try to solve the over-confidence of traditional NLL, but the
trade-off always exists. For MTS, some channels with high noises
are inherently hard to reconstruct, so enhancing their weights on
gradient (w.r.t the mean) has no substantive significance, but im-
pacts greatly on the optimization of other points. In contrast, the
hard-to-fit regions in channels with high signal-to-noise ratio usu-
ally contribute more on the modeling of normal patterns. Therefore,
the strategic reweighting of diverse channels enabled our method
to achieve superior performance across all datasets adopted.
Parameter Analysis. Two related hyper-parameters in our
framework are window size 𝑆and normalization coefficient 𝛼.
Fig. 5 shows the performance under distinct settings. We can con-
clude that, 1)when the window size is too small or too large, it
0.00.51.0Series ValueOriginal
Reconstructed 0.00.51.0
Original
Reconstructed
0.00.40.8Reconstruction Error 0.00.40.8
24Uncertainty
0816
0 100 200 300 400 500 600
Time Index (/s)0.00.51.0Anomaly ScoreThreshold
0 100 200 300 400 500 600
Time Index (/s)0.00.51.0
ThresholdAnomalyFigure 6: Cases for visualization. For clarity, time series with
only one channel is selected for visualization here. From
top to bottom of each graph, the subplots represent the time
series (ground truth VS restored results), the reconstruction
error, the estimated uncertainty (i.e. ˆ𝜎𝑡𝑠) and the anomaly
scores respectively. Left: SWaT. Right: MSL.
is unfavorable to learn the complex dependencies in MTS. Prac-
tically, when the window size is excessively large, the statistical
features{E𝑠(𝑋),𝑉𝑎𝑟𝑠(𝑋)}of long-term series may remain invari-
ant, thereby facilitating reconstruction effortlessly. Conversely, if
the window size is too small, reconstruction becomes challenging
due to significant changes in statistical features; 2)the coefficient
𝛼also plays an important role on the performance. As 𝛼increases,
the influence of channel discrepancy on the weights of gradient
w.r.t mean becomes more pronounced.
4.4 Case Study
4.4.1 Role of Uncertainty Estimation. To explain how our method
works to detect diverse abnormal sequences, two cases on SWaT
and MSL are visualized in Fig. 6. To demonstrate the effectiveness
 
789SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns KDD’24, August 25–29, 2024, Barcelona, Spain.
of our method, we try to select the anomalous segments that previ-
ous methods fail to detect. We compare the detected results with
those obtained by NPSR [18] and our variant optimized by 𝛽-NLL.
Specifically, for the cases in Fig. 6, we select the channel with the
maximum anomaly score, the sources are from:
•SWaT: dim 1 (start from dim 0) during the first anomalous
segments;
•MSL: dim 54 of machine M-7 from timestamps 1700 to 2100.
Indeed, if a certain segment is abnormal, there are certain deviations
between the reconstruction results and the true values, but not vice
versa. In SensitiveHUE, whether it is anomalous depends more on
the observation probability, for which heteroscedastic uncertainty
estimation is imperative. Specifically, for the left subplot, despite a
small reconstruction error, we can successfully detect anomalies
based on the low certainty estimation (indicating minimal noise
and relatively easy reconstruction), while for the right subplot,
even with a large reconstruction error, it will not be considered as
anomaly due to high uncertainty (i.e, high level noise).
4.4.2 Are Spatial Correlations Mainly Considered ? To explain how
our SensitiveHUE achieving reconstruction using the spatial cor-
relations among different channels, we use intervention based ap-
proach to quantify the strength of correlations, which is widely
used in model interpretability and causal discovery [ 4]. Specifically,
for each channel 𝑠, we replace the input 𝑋:𝑠with random values
that satisfiesN(0,1), and then measure its impact on the recon-
struction results of other channels. Denote the correlation matrix
as𝐴={𝐴𝑖𝑗}𝑖,𝑗=1,···,𝑆, we use Mean Absolute Percentage Error
(MAPE) to quantify the strength of the impact, as follows:
𝐴𝑖𝑗=LMSE(e𝑋𝑖
:𝑗,ˆ𝑋:𝑗)
LMSE(ˆ𝑋:𝑗,𝑋:𝑗), (18)
wheree𝑋𝑖
:𝑗denotes the reconstructed results of channel 𝑗after re-
placing the values of channel 𝑖,ˆ𝑋:𝑗is the original reconstructed
result of channel 𝑗. Finally, each row will be normalized as 𝐴𝑖𝑗←
𝐴𝑖𝑗/Í𝑆
𝑗=1𝐴𝑖𝑗.
As shown in Fig. 7, the right subplot presents the correlations
of different channels learned by SensitiveHUE on SWaT dataset
(the deeper the color, the stronger the correlation). An anomalous
case relevant to channel 0 and 1 is selected, where our method
locates the abnormal intervals successfully through the learned
correlation, as shown in the left subplot. Specifically, the values of
channel 0 are almost consistent with the previous patterns during
the anomalous interval, but cannot be reconstructed accurately
through the learned dependencies, thereby leading to high anomaly
scores. Thus, for our method, the spatial correlations are critical
for the reconstruction process.
4.4.3 Uncertainty Estimation over Training Process. Fig. 8 shows
the mean estimated variance ˆ𝜎𝑠,mean of different channels over
training time, which is obtained by LNLL,L𝛽-NLL andLMTS-NLL
respectively. Due to the discrepancy of noise intensity among dif-
ferent channels, the difficulty to reconstruct these channels may
be completely different. For LNLL, the estimated uncertainties of
some channels (e.g., channel 20) almost remain very large after
2000 iterations, so the learning process of these channels is greatly
Figure 7: A case on SWaT to show the significance of spatial
correlation modeling. Left: Results comparison between the
original series and the reconstructed results, where the first
two channels are selected here. The bottom subplot is the
anomaly score over time. Right: The learned correlations
among different channels. For clarity, we have selected only
a set of them.
0 10 20 30 40 50
Dimension10
8
6
4
2
0Iteration Steps (×103)
0.250.500.751.001.251.501.752.00
(a)LNLL
0 10 20 30 40 50
Dimension10
8
6
4
2
0Iteration Steps (×103)
0.250.500.751.001.251.501.752.00
 (b)L𝛽-NLL
0 10 20 30 40 50
Dimension10
8
6
4
2
0Iteration Steps (×103)
0.250.500.751.001.251.501.752.00
 (c)LMTS-NLL
Figure 8: The mean estimated variance ˆ𝜎𝑠,𝑚𝑒𝑎𝑛 of different
channels over training time on SWaT dataset. (a) Using LNLL
as the optimization objective. (b) L𝛽-NLL. (c) Ours. For clarity,
we set the maximum of ˆ𝜎𝑠,mean to 2.
ignored. Besides, for the first 500 iterations, the estimated uncer-
tainties forL𝛽-NLL andLMTS-NLL are similar, but they eventually
diverge in different directions. We find that L𝛽-NLL tends to present
balanced weighting for the points from different channels, so it
will gradually strive to optimize some channels with high level
noises, with limited effect on the modeling of normal patterns. In
contrast, forLMTS-NLL , further weighting of different channels
allows the network to ignore these channels selectively, thereby
focusing more on the regions that are critical for the modeling.
Overall,LMTS-NLL does not completely discard certain channels,
but instead dynamically assigns different weights to each channel.
For instance, it appropriately relaxes the fitting of channels with
low signal-to-noise ratio (such as channel 23) and enhances the
sensitivity towards other crucial regions.
5 Conclusion
This paper presents SensitiveHUE, a probabilistic framework through
statistical feature removal (SFR) strategy and heteroscedastic un-
certainty estimation. Compared with the previous reconstruction
strategies, the reconstruction based on SFR is dependency sensi-
tive, thereby greatly enhancing the discrimination between normal
points and anomalies. Besides, we propose MTS-NLL loss to en-
hance the sensitivity to significant regions in multi-channel data.
The rich experimental results verify the great potential for future
researches on multivariate time series anomaly detection.
 
790KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
References
[1]Chuadhry Mujeeb Ahmed, Venkata Reddy Palleti, and Aditya P. Mathur. 2017.
WADI: A Water Distribution Testbed for Research in the Design of Secure Cyber
Physical Systems. Association for Computing Machinery, 25–28.
[2]Julien Audibert, Pietro Michiardi, Frédéric Guyard, Sébastien Marti, and Maria A.
Zuluaga. 2020. USAD: UnSupervised Anomaly Detection on Multivariate Time
Series. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining. 3395–3404.
[3]Kurtl Chua, Roberto Calandra, McAllisterRowan, and LevineSergey. 2018. Deep
reinforcement learning in a handful of trials using probabilistic dynamics mod-
els. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, Vol. 31.
[4]Jonathan Crabbé and Mihaela Van Der Schaar. 2021. Explaining Time Series Pre-
dictions with Dynamic Masks. In Proceedings of the 38th International Conference
on Machine Learning.
[5]Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detection
in multivariate time series. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 4027–4035.
[6]Cheng Feng and Pengwei Tian. 2021. Time Series Anomaly Detection for Cyber-
physical Systems via Neural System Identification and Bayesian Filtering. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining. 2858–2867.
[7]Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep Bayesian active
learning with image data. In International Conference on Machine Learning.
[8]Jonathan Goh, Sridhar Adepu, Marcus Tan, and Zi Shan Lee. 2017. Anomaly De-
tection in Cyber Physical Systems Using Recurrent Neural Networks. In Proceed-
ings of the 18th International Symposium on High Assurance Systems Engineering
(HASE). 140–145.
[9]Siho Han and Simon S. Woo. 2022. Learning Sparse Latent Graph Representations
for Anomaly Detection in Multivariate Time Series. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
[10] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and
Tom Soderstrom. 2018. Detecting Spacecraft Anomalies Using LSTMs and
Nonparametric Dynamic Thresholding. Association for Computing Machinery,
387–395.
[11] Frank Hutter Ilya Loshchilov. 2019. Decoupled Weight Decay Regularization. In
Proceedings of the 7th International Conference on Learning Representations.
[12] Alexander Immer, Christoph Schultheiss, Julia E. Vogt, Bernhard Schölkopf, Peter
Bühlmann, and Alexander Marx. 2023. On the Identifiability and Estimation
of Causal Location-Scale Noise Models. In Proceedings of the 40nd International
Conference on Machine Learning. 14316–14332.
[13] Yungi Jeong, Eunseok Yang, Jung Hyun Ryu, Imseong Park, and Myungjoo Kang.
2023. AnomalyBERT: Self-Supervised Transformer for Time Series Anomaly
Detection using Data Degradation Scheme. arXiv:2305.04468
[14] Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in Bayesian deep
learning for computer vision?. In Proceedings of the 31st International Conference
on Neural Information Processing Systems, Vol. 30. 5580–5590.
[15] Siwon Kim, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon.
2022. Towards a Rigorous Evaluation of Time-series Anomaly Detection.
arXiv:2109.05257
[16] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and
Jaegul Choo. 2022. Reversible Instance Normalization for Accurate Time-Series
Forecasting against Distribution Shift. In Proceedings of the 10th International
Conference on Learning Representations.
[17] Armen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or epistemic? does it
matter? Structural Safety 31, 2 (2009), 105–112.
[18] Chih-Yu Lai, Fan-Keng Sun, Zhengqi Gao, Jeffrey Lang, and Duane S. Bon-
ing. 2023. Nominality Score Conditioned Time Series Anomaly Detection by
Point/Sequential Reconstruction. In Proceedings of the 37th International Confer-
ence on Neural Information Processing Systems.
[19] Yuxin Li, Wenchao Chen, Bo Chen, Dongsheng Wang, Long Tian, and Mingyuan
Zhou. 2023. Prototype-oriented unsupervised anomaly detection for multivari-
ate time series. In Proceedings of the 40nd International Conference on Machine
Learning.
[20] Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei.
2021. Multivariate Time Series Anomaly Detection and Interpretation using
Hierarchical Inter-Metric and Temporal Embedding. In Proceedings of the 27thACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual Event).
Association for Computing Machinery, 3220–3230.
[21] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation Forest. In
Proceedings of the Eighth IEEE International Conference on Data Mining (ICDM).
413–422.
[22] Aditya P. Mathur and Nils Ole Tippenhauer. 2016. SWaT: a water treatment
testbed for research and training on ICS security. In Proceedings of 2016 Interna-
tional Workshop on Cyber-physical Systems for Smart Water Networks. 31–36.
[23] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In
International Conference on Learning Representations.
[24] D.A. Nix and A.S. Weigend. 1994. Estimating the mean and variance of the target
probability distribution. In Proceedings of 1994 IEEE International Conference on
Neural Networks (ICNN’94), Vol. 1. 55–60 vol.1. https://doi.org/10.1109/ICNN.
1994.374138
[25] Ian Osband, Charles Blundell, Alexanderl Pritze, and Benjamin Van Roy. 2016.
Deep exploration via bootstrapped DQN. In Advances in Neural Information
Processing Systems, Vol. 29.
[26] Daehyung Park, Yuuna Hoshi, and Charles C. Kemp. 2018. A Multimodal Anomaly
Detector for Robot-Assisted Feeding Using an LSTM-Based Variational Autoen-
coder. IEEE Robotics and Automation Letters 3, 3 (2018), 1544–1551.
[27] Sebastian Schmidl, Phillip Wenig, and Thorsten Papenbrock. 2022. Anomaly
detection in time series: a comprehensive evaluation. Proc. VLDB Endow. (may
2022), 1779–1797.
[28] Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius. 2022.
On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic
Neural Networks. In Proceedings of the 10th International Conference on Learning
Representations.
[29] Andrew Stirn, Harm Wessels, Megan Schertzer, Laura Pereira, Neville Sanjana,
and David Knowles. 2023. Faithful Heteroscedastic Regression with Neural
Networks. In Proceedings of International Conference on Artificial Intelligence and
Statistics.
[30] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust
Anomaly Detection for Multivariate Time Series through Stochastic Recurrent
Neural Network. In Proceedings of the 25th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 2828–2837.
[31] Shreshth Tuli, Giuliano Casale, and Nicholas R Jennings. 2022. TranAD: Deep
Transformer Networks for Anomaly Detection in Multivariate Time Series Data.
InProceedings of Very Large Data Bases, Vol. 15. 1201–1214.
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Proceedings of the 31st International Conference on Neural Informa-
tion Processing Systems, Vol. 30. Curran Associates, Inc., 6000–6010.
[33] Chengsen Wang, Zirui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, Haifeng Sun,
and Jianxin Liao. 2023. Drift doesn’t Matter: Dynamic Decomposition with Diffu-
sion Reconstruction for Unstable Multivariate Time Series Anomaly Detection. In
Proceedings of the 37th International Conference on Neural Information Processing
Systems.
[34] Renjie Wu and Eamonn J. Keogh. 2023. Current Time Series Anomaly Detec-
tion Benchmarks are Flawed and are Creaing the illusion of Progress. IEEE
Transactions on Knowledge and Data Engineering 35, 3 (2023), 3421–2429.
[35] Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying
Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin
Qiao. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for
Seasonal KPIs in Web Applications. In Proceedings of the 2018 World Wide Web
Conference (Lyon, France). International World Wide Web Conferences Steering
Committee, 187–196.
[36] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Anomaly
Transformer: Time Series Anomaly Detection with Association Discrepancy. In
Proceedings of the 10th International Conference on Learning Representations.
[37] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Le Xinyi.
2022. A Unified Model for Multi-class Anomaly Detection. In Proceedings of the
36th International Conference on Neural Information Processing Systems.
[38] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae-Ki
Cho, and Haifeng Chen. 2018. Deep Autoencoding Gaussian Mixture Model
for Unsupervised Anomaly Detection. In International Conference on Learning
Representations.
 
791SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns KDD’24, August 25–29, 2024, Barcelona, Spain.
Dataset Train Test Dimension Entity AR(%)
SWaT 99360 89984 51 1 11.99
WADI 241921 34560 123 1 5.75
MSL 58317 73729 55 27 10.72
SMD 708405 708420 38 28 4.16
Table 5: Detailed statistics of different datasets adopted in
experiments. AR denotes anomaly rate.
A The Theoretical Analysis of Statistical
Feature Removal
For clarity, we denote the set of all input windows of our model
asX={𝑋𝑛}𝑁
𝑛=1, where𝑋𝑛∈R𝑇×𝑆,|X|is the size ofX. Besides,
𝑋𝑛𝑡:denotes the 𝑡-th row of𝑋𝑛, and𝑋𝑛:𝑠denotes the 𝑠-th column
of𝑋𝑛. The Mean Squared Error (MSE) is used to calculate the
reconstruction error. In this section, we will demonstrate that the
statistical feature removal strategy satisfies Theorem 3.3 under
certain context of generalized multivariate time series (MTS). We
rewrite Theorem 3.3 as follows, the conditions in this theorem are
simplified as condition 𝑖,𝑖=1,2,3.
Theorem A.1. LetHbe the set of functions: H={ℎ|ℎ:
R𝑇×𝑆↦→R𝑇×𝑆}. Denote𝑋𝑔=𝑔(𝑋), and𝜖H𝑔=infℎ∈HE𝑋(||ℎ(𝑋𝑔)−
𝑋||2
2). Assume that it satisfies the following conditions: 1) 𝑔:R𝑇×𝑆↦→
R𝑇×𝑆is approximately reversible, i.e., ∃ℎ∈H , so that𝜖H𝑔→0; 2)
DenoteHT−⊂H ,∀𝑓∈HT−, it is time-independent. It satisfies
that𝜖HT−
𝑔≫𝜖H𝑔; 3) Similarly, denote HS−⊂H ,∀𝑓∈HS−, it is
channel-independent. It satisfies that 𝜖HS−
𝑔≫𝜖H𝑔. The target map-
ping𝑓:𝑋↦→ℎ(𝑋𝑔)is considered as dependency sensitive if these
conditions are satisfied.
SFR Strategy. The operations of statistical feature removal 𝑔:
R𝑇×𝑆→R𝑇×𝑆are formulated as follows:
𝜇𝑛𝑠=1
𝑇𝑇∑︁
𝑡=1𝑋𝑛𝑡𝑠, 𝜎2
𝑛𝑠=1
𝑇𝑇∑︁
𝑡=1(𝑋𝑛𝑡𝑠−𝜇𝑛𝑠)2. (19)
For𝑋𝑔
𝑛=𝑔(𝑋𝑛), it satisfies:
𝑋𝑔
𝑛𝑡𝑠=𝑋𝑛𝑡𝑠−𝜇𝑛𝑠√︃
𝜎2𝑛𝑠+𝜖0, (20)
where𝜖0∈R+is a small value.
In the following, we will show that the SFR strategy above satis-
fies the three conditions in Theorem A.1 under certain context of
MTS.
Proof: Denote 𝜇𝑛=[𝜇𝑛1,···,𝜇𝑛𝑠]′∈R𝑆,𝜎2𝑛=[𝜎2
𝑛1,···,𝜎2
𝑛𝑆]′∈
R𝑆. The reconstruction process equals to learn accurate mapping
ℎ𝜇:𝑋𝑔
𝑛→𝜇𝑛, ℎ𝜎:𝑋𝑔
𝑛→𝜎𝑛, and the reconstruction results can
be formulated as
[ℎ(𝑋𝑔
𝑛)]𝑡𝑠=𝑋𝑔
𝑛𝑡𝑠·[ℎ𝜎(𝑋𝑔
𝑛)]𝑠+[ℎ𝜇(𝑋𝑔
𝑛)]𝑠. (21)
We consider the case of 𝑆=1firstly.
Univariate Case. For univariate case with 𝑆=1, we have
𝜇𝑛,𝜎2𝑛∈R. DenoteX1⊂X,∀𝑋𝑛1,𝑋𝑛2∈X 1(𝑛1≠𝑛2),𝑔(𝑋𝑛1)≠
𝑔(𝑋𝑛2), and for ˘X1=X\X 1,∀𝑋𝑛3∈˘𝑋1,∃𝑋𝑛4∈˘X1(𝑛3≠𝑛4), s.t.𝑔(𝑋𝑛3)=𝑔(𝑋𝑛4). The definition ofX1and ˘X1is equally applicable
to the subsequent multivariate case. We assume that the univariate
time series in which this set of Xresides satisfies the following
requirements.
•the whole time series is non-stationary, i.e. (𝜇𝑛,𝜎2𝑛)varies
over time;
•|X 1|is obviously smaller than |X|.
The above assumptions generally hold for the univariate time series
in real-world datasets. We will show that 𝜖H𝑔is a large value under
univariate case.
For simplicity, we assume ∀𝑋𝑛1,𝑋𝑛2∈˘X1, it satisfies that 𝑐𝑔=
𝑋𝑔
𝑛1=𝑋𝑔
𝑛2. Thus, we have
𝜖H
𝑔(˘X1)=inf
ℎ∈HE𝑋∈˘X1||ℎ(𝑐𝑔)−𝑋||2
2. (22)
Therefore, we have
𝜖H
𝑔(X)⩾|˘X1|
|X|E𝑋∈˘X1||ℎ(𝑐𝑔)−𝑋||2
2. (23)
Since(𝜇𝑛,𝜎2𝑛)varies over time, as well as 𝑋. Generally,𝜖H𝑔(X)is a
large value under univariate case.
Multivariate Case. We consider the context of generalized mul-
tivariate time series. Denote the causal graph corresponding to
different channels as G=(V,E), whereV={𝑣𝑖|𝑖=1,···,𝑆}is
the set of nodes,E={(𝑣𝑖,𝑣𝑗)|0<𝑖,𝑗≤𝑆}indicates the spatial
correlations. We further assume:
•∀(𝑣𝑖,𝑣𝑗)∈E , the correlation between 𝑣𝑖and𝑣𝑗is strong
enough. For most(𝑣𝑖,𝑣𝑗)∈E , the correlation is nonlinear.
•Generally, we have |X1|≈|X| under the multivariate case.
Firstly, the mapping 𝑔in SFR strategy satisfies the condition 1
apparently. Since|X1|≈|X| , i.e.∀𝑋𝑛1,𝑋𝑛2∈X(𝑛 1≠𝑛2),𝑔(𝑋𝑛1)≠
𝑔(𝑋𝑛2), the mapping 𝑔is approximately reversible. Thus, there
exists a function ℎ:R𝑇×𝑆→R𝑇×𝑆so that𝜖H𝑔(X)→ 0.
Secondly, for time-independent function HT−, we can conclude
that𝜖HT−
ℎis generally a large value when 𝑇≫1. We simply assume
𝜇𝑛varies over time, while 𝜎2𝑛is invariant. If the window size 𝑇≫1,
we can approximately think that 𝑋𝑛𝑡:and𝜇𝑛are independent of
each other, as well as 𝑋𝑔
𝑛𝑡:and𝜇𝑛. For𝑋𝑔
𝑛𝑡:, ifℎ∈HT−, we have
ℎ({𝑋𝑔
𝑛𝑡:}𝑇
𝑡=1)={ℎ𝑡(𝑋𝑔
𝑛𝑡:)}𝑇
𝑡=1). Thus,
inf
ℎ∈HT−E𝑋𝑛∈X||ℎ(𝑋𝑔
𝑛)−𝑋𝑛||2
2
=inf
ℎ∈HT−E𝑋𝑛∈X||ℎ𝜇(𝑋𝑔
𝑛)−𝜇𝑛||2
2.(24)
As𝑋𝑔
𝑛𝑡:and𝜇𝑛are independent of each other, ∀𝑡, the mapping of
ℎ𝜇
𝑡tends to converge towards a constant vector 𝑐∈R𝑆asℎ𝜇
𝑡(𝑋𝑔
𝑛𝑡:)
tends to map to invariant. We have
𝜖HT−
ℎ=E𝑋𝑛∈X||𝑐−𝜇𝑛||2
2. (25)
As𝜇𝑛varies over time, 𝜖HT−
ℎis generally a large value.
Thirdly, for channel-independent function HS−, it is analogous
to the context of univariate case, and it satisfies that 𝜖HS−
𝑔 is the
sum of𝜖H𝑔(X)in Equation (23)from different channels, which is
also a large value.
To sum up, under certain context of MTS, our statistical feature
removal satisfies the conditions in Theorem A.1 well.
 
792KDD’24, August 25–29, 2024, Barcelona, Spain. Yuye Feng et al.
Configuration Module Description Output
Input:𝑋 - Multiple time series windows 𝐵×𝑊×𝑆
1. SFR - Remove the statitistical features 𝐵×𝑊×𝑆
2. Embedding Linear Dimension transformation 𝐵×𝑊×𝐷
3. Position Embedding nn.Embedding Add position information 𝐵×𝑊×𝐷
4. Encoder (× 𝐿) nn.TransformerEncoder Representations learning 𝐵×𝑊×𝐷
1. Reconstruction Linear obtain ˆ𝜇 𝐵×𝑊×𝑆
2. Uncertainty Estimation Linear obtain ˆ𝜎 𝐵×𝑊×𝑆
Table 6: Model Architecture details. 𝐵: batch size. 𝑊: window size. 𝑆: input dimension. 𝐷: latent dimension. 𝐿: layer number of
transformer encoders.
B Implementation Details
B.1 Dataset Statistics
The detailed statistical information of the datasets employed in
our experiments is shown in Tab. 5. For SWaT and WADI, we
downsample the data in every 5 timestamps to facilitate comparison
with the published results of other methods. For MSL and SMD,
due to the discrepancy of properties between different entities, the
detection results of each entity will be obtained independently, and
concatenated eventually to calculate the evaluation metrics.
B.2 Evaluation Details
we adopt the possibly best F1-score before ( F1∗) and after point
adjustment (F1∗
PA) as evaluation metrics for comparison of different
methods. During test, we compare the anomaly scores to a fixed
threshold𝛿to decide whether the scores indicate anomalies. Denote
the anomaly score at time 𝑡asˆS𝑡, we have
ˆ𝑦𝑡=(
1,ifˆS𝑡>𝛿,
0,otherwise.(26)
Denote the labels as y={𝑦1,···,𝑦𝑇}, and ˆy={ˆ𝑦1,···,ˆ𝑦𝑇}. We
search for the best threshold 𝛿∗through equispaced search, range
from the minimum value of the entire anomalies scores to the
maximum, as
𝛿∗≜arg max
𝛿F1(ˆy,y). (27)
Note that𝛿∗may be different for computation of F1∗andF1∗
PA.
B.3 Experimental Details
All of the experiments are implemented in Pytorch 1.13 with one
GeForce RTX 24GB GPU. For each dataset, we run the experiments
for 3 times with different seeds, and then calculate the average
value. The experimental details in the main text are as follows.
Details of Common Reconstruction Strategies. For the ex-
periments in Tab. 1 and Tab. 2, we employ a widely adopted anom-
aly detection framework Transformer to implement reconstruction
using existing reconstruction strategies individually. For the com-
pression strategy, we set the feature dimension to a value (32 for
SWaT) smaller than the number of input channels. For the maskstrategy, we randomly set the value of all channels to 0 at some time
points in a certain proportion (20% for SWaT), then reconstruct the
masked portion in original series.
Details of Disruption. For the experiments in Tab. 1 and Tab.
2, SWaT is divided into multiple windows of length 24, where
the training minimizes the Mean Squared Error (MSE) between
reconstructed results and inputs. After training, temporal disruption
is to hide data from other time steps when reconstructing data for
a specific time point, achieved by setting the attention scores for
other time points to −∞during test phase. Spatial disruption entails
that for each given channel, data from other channels are randomly
shuffled using ‘torch.randperm’. The disruption have no additional
parameters.
Model Architecture. We use vanilla Transformer encoders for
representation learning for our method, two linear layers for recon-
struction and heteroscedastic uncertainty estimation, respectively.
All activation functions are LeakyReLU. The specific architecture
is shown in Tab. 6.
Dataset Downsample 𝐵𝑊𝐷𝐿Training Epochs
SWaT 5 6424128 1 30
WADI 5 6424256 2 30
MSL 1 3248128 2 30
SMD 1 6448128 2 30
Table 7: Detailed hyper-parameter settings of SensitiveHUE.
Hyper-parameter Settings. Initially, the time series of each
dataset used for training will be divided into training set and valid
set respectively. The ratio is 9 : 1 for WADI and 4 : 1 for other
datasets. Besides, as the distribution of data collected by actuator
P201,LIT401 are remarkably discrepant between training data and
test data for SWaT, the anomaly score of them will be ignored during
inference. During training, AdamW [ 11] with an initial learning
rate of 1𝑒−3is leveraged as the optimizer, and we use early stop
strategy with patience 10 to control the process. Tab. 7 shows the
hyper-parameter settings for all the datasets.
 
793