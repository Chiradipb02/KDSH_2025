Fredformer: Frequency Debiased Transformer for Time Series
Forecasting
Xihao Piao*
SANKEN, Osaka University
Osaka, Japan
park88@sanken.osaka-u.ac.jpZheng Chen*
SANKEN, Osaka University
Osaka, Japan
chenz@sanken.osaka-u.ac.jpTaichi Murayama
SANKEN, Osaka University
Osaka, Japan
taichi@sanken.osaka-u.ac.jp
Yasuko Matsubara
SANKEN, Osaka University
Osaka, Japan
yasuko@sanken.osaka-u.ac.jpYasushi Sakurai
SANKEN, Osaka University
Osaka, Japan
yasushi@sanken.osaka-u.ac.jp
ABSTRACT
The Transformer model has shown leading performance in time se-
ries forecasting. Nevertheless, in some complex scenarios, it tends
to learn low-frequency features in the data and overlook high-
frequency features, showing a frequency bias. This bias prevents
the model from accurately capturing important high-frequency
data features. In this paper, we undertake empirical analyses to
understand this bias and discover that frequency bias results from
the model disproportionately focusing on frequency features with
higher energy. Based on our analysis, we formulate this bias and
propose Fredformer , a Transformer-based framework designed
to mitigate frequency bias by learning features equally across dif-
ferent frequency bands. This approach prevents the model from
overlooking lower amplitude features important for accurate fore-
casting. Extensive experiments show the effectiveness of our pro-
posed approach, which can outperform other baselines in differ-
ent real-world time-series datasets. Furthermore, we introduce a
lightweight variant of the Fredformer with an attention matrix
approximation, which achieves comparable performance but with
much fewer parameters and lower computation costs. The code is
available at: https://github.com/chenzRG/Fredformer
CCS CONCEPTS
•Computing methodologies →Artificial intelligence; Neural
networks.
KEYWORDS
Time series forecasting, Deep learning
ACM Reference Format:
Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Ya-
sushi Sakurai. 2024. Fredformer: Frequency Debiased Transformer for
* Indicates corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671928Time Series Forecasting . In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671928
1 INTRODUCTION
Time series data are ubiquitous in everyday life. Forecasting time
series could provide insights for decision-making support, such
as potential traffic congestion [ 10] or changes in stock market
trends [ 33]. Accurate forecasting typically involves discerning vari-
ous informative temporal variations in historical observations, e.g.,
trends, seasonality, and fluctuations, which are consistent in fu-
ture time series [ 41]. Benefiting from the advancements in deep
learning, the community has seen great progress, particularly with
Transformer-based methods [ 38,40,48]. Successful methods of-
tentokenize time series with multiresolution, such as time points
[42] or sub-series [ 47], and model their dependencies leveraging the
self-attention mechanism. Several state-of-the-art (SOTA) baselines
have been proposed, namely PatchTST [ 28], Crossformer [ 47], and
iTransformer [23], and demonstrate impressive performance.
Despite their success, the effectiveness with which we can cap-
ture informative temporal variations remains a concern. From a data
perspective, a series of time observations is typically considered a
complex set of signals or waves that varies over time [ 13,17]. Vari-
ous temporal variations, manifested as different frequency waves,
such as low-frequency long-term periodicity or high-frequency
fluctuation, often co-occur and are intermixed in the real world
[19,20,41]. While tokenizing a time series may provide fine-grained
information for the model, the temporal variations in resulting
tokens or sub-series are also entangled. This issue may complicate
the feature extraction and forecasting performance. Existing works
have proposed frequency decomposition to represent the time se-
ries and deployed Transformers on new representation to explicitly
learn eventful frequency features [ 41,42]. Learning often incorpo-
rates feature selection strategies in the frequency domain, such as
top-K or random-K [ 40,49], to help Transformers better identify
more relevant frequencies. However, such heuristic selection may
introduce spurious, sub-optimal frequency correlations into the
model (seen in Figure 1(a)), inadvertently misleading the learning
process.
 
2400
KDD ’24, August 25–29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
(b) PatchTST (c) Ours (a) FEDformer
F
 F
A
FInput Ground Truth Forecasting
Figure 1: In contrast to a frequency modeling-based work FEDformer [ 49] and a SOTA work PatchTST [ 28], our model can
accurately capture more significant mid-to-high frequency components.
From a model perspective, researchers have recently noticed a
learning bias issue that is common in the Transformer. That is, the self-
attention mechanism often prioritizes low-frequency features at the
expense of high-frequency features [ 14,29,34,35]. This subtle issue
may also appear in time series forecasting, potentially biasing model
outcomes and leading to information losses. Figure 1(b) shows an
electricity case where the forecasting result successfully captures
low-frequency features, neglecting some consistent mid-to-high
frequencies. In practice, such high frequencies represent short-
term variations, e.g., periodicities over short durations, which serve
as good indicators for forecasting [ 10,16,33]. However, the low-
frequencies typically carry a substantial portion of the energy in the
spectrum and are dominant in time series. The amplitude of these
low-frequency components far exceeds that of higher frequencies
[50], which provides the Transformer with more observations. This
may raise the possibility of frequency bias in time series forecasting,
as the model might disproportionately learn from these dominant
low-frequency components.
This work explores one direction of capturing informative, com-
plex variations by frequency domain modeling for accurate time se-
ries forecasting. We introduce Fredformer , aFrequency-debiased
Transformer model. Fredformer follows the line of frequency
decomposition but further investigates how to facilitate the uses
of Transformers in learning frequency features. To improve the
effectiveness of our approach, we provide a comprehensive analy-
sis of frequency bias in time series forecasting and a strategy for
debiasing it. Our main contributions lie in three folds.
- Problem definition. We undertake empirical studies to investi-
gate how this bias is introduced into time series forecasting Trans-
formers. We observe that the main cause is the proportional dif-
ference between key frequency components. Notably, these key
components should be consistent in the historical and ground truth
of the forecasting. We also investigate the objective and key designs
that affect debiasing.
- Algorithmic design. OurFredformer has three pivotal compo-
nents: patching for the frequency band, sub-frequency-independent
normalization to mitigate proportional differences, and channel-
wise attention within each sub-frequency band for fairness learningof all frequencies and attention debiasing.
- Applicability. Fredformer undertakes Nyström approximation
to reduce the computational complexity of the attention maps, thus
achieving a lightweight model with competitive performance. This
attempt opens new opportunities for efficient time series forecast-
ing.
Remark. This is the first paper to study the frequency bias issue
in time series forecasting. Extensive experimental results on eight
datasets show the effectiveness of Fredformer , which achieves
superior performance with 60 top-1 and 20 top-2 cases out of 80.
2 PRELIMINARY ANALYSIS
We present two cases to show (i)how frequency attributes of time
series data introduce bias into forecasting with the Transformer
model and(ii)an empirical analysis of the potential debiasing
strategy. This section introduces the notation and a metric for the
case studies in Sec. 2.1. The case analyses are detailed in Sec. 2.2.
2.1 Preliminary
Time Series Forecasting. LetX={𝒙(𝑐)
1,..., 𝒙(𝑐)
𝐿}𝐶
𝑐=1denote
a multivariate time series consisting of 𝐶channels, where each
channel records an independent 𝐿length look-back window. For
simplicity, we omit channel index 𝑐in subsequent discussions. The
forecasting task is to predict 𝐻time steps in the future data ˆX:
ˆX𝐿+1:𝐿+𝐻=𝑓(X1:𝐿)
where𝑓(·)denotes the forecasting function, which is a Transformer-
based model in this work. Our objective is to mitigate the learning
bias in the Transformer and enhance the forecasting outcome X′,
that is, to minimize the error between X′and ˆX.
Discrete Fourier Transform (DFT). We use DFT to analyze the
frequency content of X,ˆX, andX′. For example, given the input
sequence{𝒙1,...,𝒙𝐿}, the DFT can be formulated as
𝒂𝑘=1
𝐿𝐿∑︁
𝑙=1𝒙𝑙·𝑓𝑘, 𝑘=1,...,𝐿
 
2401Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
Relative Error0.9 0.1Ground Truth Input
 Forecasting
(b)Time domain model
+ Non-normalizationTime domain model
+ NormalizationFrequency domain  
+ NormalizationF F F
(a)#epochk2k1
k31 50 #epoch 1 50k2k1
k3
Fk2k1
k3
k2
k1k3
F
Figure 2: Figure (a) shows the learning dynamics and results
for two synthetic datasets, employing line graphs to illus-
trate amplitudes in the frequency domain and heatmaps to
represent training epoch errors. Figure (b) explores the in-
fluence of amplitude and domain on learning by comparing
Transformers in the time and frequency domains, both with
and without frequency local normalization.
where𝑓𝑘=𝑒−𝑖2𝜋𝑘/𝐿denotes the 𝑘-th frequency component. The
DFT coefficients A={𝒂1,𝒂2,..., 𝒂𝐿}represent the amplitude in-
formation of these frequencies. As illustrated in Figure 2 (b, left),
four components are observed to have higher amplitudes in the
historical observations (X) and the forecasting data ( ˆX). We refer
to such consistent components as ’key components ’ (defined in Sec.
3.1). Here, the inverse DFT (i.e., IDFT) is 𝒙𝑙=Í𝐿
𝑘=1𝒂𝑘·𝑓−1
𝑘, which
reconstructs the time series data from the DFT coefficients.
Frequency Bias Metric. Inspired by the work of [ 5,45], this study
employs a Fourier analytic metric of relative error Δ𝑘to determine
the frequency bias. Given the model outputs A′and the ground
truth ˆA, the mean-square error (MSE) for the 𝑘-th component is
calculated as follows: MSE𝑘=|𝒂′𝑘−ˆ𝒂𝑘|, where|·|denotes the L2
norm of a complex number. Then, the relative error is applied to
mitigate scale differences. In other words, the error may become
larger as the proportion of amplitude increases.
Δ𝑘=|𝒂′𝑘−ˆ𝒂𝑘|/|ˆ𝒂𝑘|
This metric is used in case study analyses and the experiments
detailed in Section 5.2.2.2 Case Studies
We first generate single-channel time series data with a total length
of10000 timestamps and then employ a Transformer model [ 28]
to forecast the data. The details are in Appendix A. For the first
case study (Case 1), we generate two datasets with three key fre-
quency components ( {𝑘1,𝑘2,𝑘3}). Each dataset contains a different
proportion of these three components, as illustrated in the DFT
visualization in Figure 2. On the left side of the figure, their ampli-
tudes are arranged as 𝒂𝑘1<𝒂𝑘2<𝒂𝑘3, whereas on the right side,
the arrangement is 𝒂𝑘1>𝒂𝑘2>𝒂𝑘3. We maintain these propor-
tions so that they are consistent between the observed Aand the
ground truth ˆA(i.e.,A≈ˆA). Then, we assess the bias for different
𝑘in the Transformer outputs A′. Meanwhile, we track how Δ𝑘
changes during the model training to show the learning bias, using
heatmap values to represent the numerical values of Δ𝑘.
Here, we generate a dataset with four key frequency components
for the second case study (Case 2). This study analyzes different
modeling strategies to investigate their flexibility for debiasing.
2.2.1 Investigating the Frequency Bias of Transformer (Case
1).As shown in Figure 2(a) (left), after 50 epochs of training, the
model successfully captures the amplitude of low-frequency com-
ponent𝑘1but fails to capture 𝑘2and𝑘3. Meanwhile, the heatmap
values show that the model predominantly focuses on learning
the𝑘1component. In other words, the relative error decreases
to around 0.01 (red codes) during the training. But, it lacks opti-
mization for 𝑘3, resulting in a high relative error of almost 0.95.
These observations indicate that signals in the time domain can be
represented by a series of frequency waves, typically dominated by
low-frequency components [ 19,26,30]. When the Transformer is
deployed on this mixed-frequency collection, the dominant propor-
tion of frequencies experiences a learning bias. A similar result is
also evident in the control experiment in the right Subfigure. Here,
we introduce synthetic data with higher amplitudes in the mid and
high-frequency ranges (resulting in 𝒂𝑘1<𝒂𝑘2<𝒂𝑘3). In response,
the model shifts its focus towards the key component 𝑘3, leading to
Δ𝑘1>Δ𝑘2>Δ𝑘3. This learning bias aligns with recent theoretical
analyses of the Transformer model [ 29,34,35]. In addition, Sec. 3.1
provides a formal definition of this frequency bias.
2.2.2 Debiasing the Frequency Learning for Transformer
(Case 2). Based on the above discussion, we initially use the same
experimental settings for a new dataset, as shown in Figure 2(b)
(Left). We then perform two feasibility analyses for debiasing by (1)
mitigating the influence of high proportionality and (2) providing
the transformer with fine-grained frequency information.
(1) Frequency normalization: We first decompose the frequency
domain and normalize the amplitudes of the frequencies to elimi-
nate their proportional differences. Specifically, we apply the DFT,
normalize the amplitudes, and then use the IDFT to convert the fre-
quency representation back into the time domain before inputting
it into the Transformer, formulated as X′=(IDFT(A𝑛𝑜𝑟𝑚)).
As depicted in Figure 2(b) (middle and right), the four input
components are adjusted so that they have the same amplitude
value, shown by a blue dashed line. The middle subfigure shows
that frequency normalization enhances the forecasting performance
for the latter three frequencies, but relative errors remain high.
 
2402KDD ’24, August 25–29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
(2) Frequency domain modeling: We further directly deploy the
Transformer on the frequency domain to model the DFT matrix.
Subsequently, we apply the IDFT to return the forecasting outcome
to the time domain. Here, the purpose is to provide the transformer
with more refined and disentangled frequency features. Formally,
X′=IDFT((A𝑛𝑜𝑟𝑚)). As shown in Figure 2(b) (right), there is a
marked improvement in forecasting accuracy for the latter three
frequency components. Notably, the bias in the second frequency
component (60-75 Hz) is effectively eliminated. These findings sug-
gest the potential for direct frequency domain modeling with
proportion mitigation in achieving the debiasing.
3 FREQUENCY BIAS FORMULATION
This section defines the frequency bias in Sec.3.1, then describes
the research problem in Sec.3.2.
3.1 Frequency Bias Definitions
Given the aforementioned empirical analyses, which demonstrate
that a frequency bias exists in key frequency components, we first
define these key components in terms of two properties: 1) a key
component should have a relatively high amplitude within the
spectrum, and 2) it should be consistent in historical observations
and future time series, as well as robust to time shifts [4, 30].
Definition 1.Key Frequency Components. Given a frequency
spectrum Awith length𝐿,Acan be segmented into 𝑁sub-frequency
bands{𝒘1,𝒘2,..., 𝒘𝑁}by a sliding window, where 𝒘𝑛∈R1×𝑃. The
maximum amplitude in the 𝑛-th window is determined as follows:
max(𝒘𝑛)=max|𝒂𝑘|:𝒂𝑘∈𝒘𝑛for𝑛=1,2,...,𝑁 (1)
where 𝒘𝑛denotes𝑃amplitudes in the 𝑛-th window. If 𝒂𝑘is a key
component in the 𝑖-th window, then:
𝒂𝑘=max(𝒘𝑛)and ˆ𝒂𝑘=max(ˆ𝒘𝑛)
˜Ais a collection of all key components. Notably, ˜Ashould be present
in historical Aand ground truth ˆAfor accurate forecasting.
Definition 2.Frequency Bias in Transformer. Given that
a time series Xcontains𝑁key frequency components amplitudes
˜A={˜𝒂1,..., ˜𝒂𝑁}, for the𝑘-th component ˜𝒂𝑘∈˜A, we have𝑃(˜𝒂𝑘)=
|˜𝒂𝑘|Í𝑁
𝑛=1|˜𝒂𝑛|, which refers to the proportion of ˜𝒂𝑘in the total sum of
amplitudes of ˜A. Frequency bias can be defined as relative error Δ𝑘.
Here, a larger proportion 𝑃(˜𝒂𝑘)leads to a smaller Δ𝑘and exhibits a
higher ranking:
−|Δ𝑘|∝𝑃(˜𝒂𝑘) (2)
Eventually, the Transformer pays more attention to high-ranked
components during the training, as seen in Figure 2 (a) heatmaps.
3.2 Problem Statement
Based on the discussions in Sec. 2, we argue that if the Transformer
assigns attention to all key frequency components ˜Aequally during
learning, then the frequency bias could be mitigated.
Problem 1.Debiasing Frequency Learning for Transformer.
Given a Transformer forecasting 𝑓𝑇𝑟𝑎𝑛𝑠(𝑋), where𝑋contains several
key frequency component ˜𝒂𝑘, our goal is to debias 𝑓𝑇𝑟𝑎𝑛𝑠 and improveforecasting performance by making the relative error Δ˜𝒂𝑘independent
of𝑃(˜𝒂𝑘):
−|Δ𝑘|̸∝𝑃(˜𝒂𝑘) (3)
thereby ensuring a balanced response by the Transformer to different
key frequency components.
4 FREDFORMER
Here, we discuss how to tackle the problem formulated in Sec.3.2
and propose Fredformer , aFrequency debiased Transformer model
for accurate time series forecasting.
Architecture Overview. Fredformer consists of four principal
components:(i)a DFT-to-IDFT backbone, (ii)frequency domain
refinement,(iii)local frequency independent learning, and (iv)
global semantic frequency summarization. Figure 3 shows an ar-
chitectural overview. The DFT-to-IDFT backbone breaks down the
input time series Xinto its frequency components using DFT and
learns a debiased representation of key frequency components by
modules(ii)(iii)and(iv). Based on the discussion in Sec. 2.2.2
(2), where we noted the significant potential of frequency modeling
for debiasing, we first refine the overall frequency spectrum into
sub-frequencies, which we achieve through a patching operation
on DFT coefficients. Patches from different channels within the
same sub-frequency band are embedded as tokens. That is, each
sub-frequency band is encoded independently, which avoids the
influence of other frequency components, as discussed in Section
2.2.2 (1). We deploy the Transformer to extract local frequency
features for each sub-band across all channels. This mitigates the
higher proportion crux defined in Def. 2. Finally, we summarize all
the frequency information, which serves as IDFT for forecasting.
Below, we provide a description of each module.
4.1 Backbone
Given X, we first use DFT to decompose Xinto frequency coeffi-
cients A1for all channels. We then extract the debiased frequency
features by using a Transformer encoder to A∈R𝐶×𝐿. The fre-
quency outputs are subsequently reconstructed to the time domain
signal X′by IDFT.
X′=IDFT(𝑓𝑇𝑟𝑎𝑛𝑠(A)),A=DFT(X))
4.2 Frequency Refinement and Normalization
From the observations described in Sec. 2.2.2, we conclude that if
there are significant proportional differences between different ˜A𝑘
values in the input data, it will lead to the model overly focusing
on components with larger amplitudes. To address this issue, we
propose frequency refinement and normalization. Specifically, a
non-overlapping patching operation is applied to Aalong the𝐶-
axis (i.e., channel), resulting in a sequence of local sub-frequencies
as follows:
W={W1,W2,...,W𝑁}=Patching(A),W𝑛∈R𝐶×𝑃
where𝑁is the total number of the patches, while 𝑃represents
the length of each patch. Mitigating information redundancy over
1Aconsists of two coefficient matrices: a real part R∈R𝐶×𝐿and an imaginary matrix
I∈R𝐶×𝐿. Since all operations are conducted synchronously for these two matrices,
we will refer to them as Ain our subsequent discussions.
 
2403Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
DFTIDFT
Frequency Domain
Modeling Transformer
EncoderFrequency
Summarization
Frequency
Channel-W ise
Attention
NormLinear Linear
Figure 3: Overview of our framework. Fredformer employs
DFT to transform input sequences into the frequency do-
main, normalizes locally, and segments into patches before
employing channel-wise attention, yielding final predictions
through a frequency-wise summarizing layer and IDFT.
fine-grained frequency bands, such as neighboring 1 Hz and 2 Hz,
allows the model to learn the local features in each sub-frequency.
Parameter𝑆is adaptable to the requirements of real-world scenarios,
for example, an hourly sampling of daily recordings or the alpha
waveform typically occurring at 8-12 Hz [8].
Since patching operation allows the model to manage each M𝑛
independently, we further normalize each W𝑛along the𝑁-axis:
W∗𝑛=𝜎(W𝑛)𝑛=1,2,...,𝑁, where𝜎(·)denotes the normaliza-
tion, and it further projects the numerical value of each ˜A𝑘into a
range of 0-1. This operation eliminates proportionate differences
in the maximum values within sub-frequency bands, thereby main-
taining an equal Δacross all key components ˜A.
Lemma 1.Frequency-wise Local Normalization: Given fre-
quency patches∀W𝑛,W𝑚∈Wformax(W𝑛)>max(W𝑚)and
𝜎(·), the normalization strategy is defined by:
W∗={𝜎(W1),...,𝜎(W𝑁)}
This ensures that within each localized frequency patch W𝑛, the am-
plitude differences between key frequency components are minimized,
promoting equal attention to all key frequencies by the model:
max(W∗
𝑛)=max(W∗
𝑚)
Some studies also introduce patching operations in the time do-
main and perform normalization within these time domain patches
[28]. However, according to Parseval’s theorem [ 31], normalization
within time domain patches is equivalent to normalizing across
all frequencies. This could not address the issue of amplitude bias
among key frequency components.
4.3 Frequency Local Independent Modeling
Given the normalized W∗, we deploy frequency local independent
Transformer encoders to learn the importance of each W∗𝑛inde-
pendently. For W(1:𝐶)
𝑛 ={𝒘(1)
𝑛,𝒘(2)
𝑛,..., 𝒘(𝐶)
𝑛}𝑁
𝑛=1, a Transformerencoder𝑓𝑇𝑟𝑎𝑛𝑠(·)accepts each 𝒘∗(𝑐)
𝑛as an input token:
{W′(1:𝐶)
𝑛}=𝑓𝑇𝑟𝑎𝑛𝑠(W∗(1:𝐶)
𝑛)
where W′(1:𝐶)
𝑛 is encoded by a channel-wise self-attention encoder,
formally:
Attention(Q𝑛,K𝑛,V𝑛)=
Softmax 
W∗(1:𝐶)
𝑛W𝑞
𝑛(W∗(1:𝐶)
𝑛W𝑘𝑛)𝑇
√
𝑑!
W∗(1:𝐶)
𝑛W𝑣
𝑛
where W𝑞
𝑛,W𝑘𝑛,W𝑣𝑛∈R𝑆×𝑀are the weight matrices for generat-
ing the query matrix Q𝑛, key matrix K𝑛, and value matrix V𝑛.√
𝑑
denotes a scaling operation. The attention module also includes
normalization and a feed-forward layer with residual connections
[12], and Attention(Q𝑛,K𝑛,V𝑛)∈R𝐶×𝑀weights the correlations
among𝐶channels for the 𝑛-th sub-frequency band M𝑛. This de-
sign ensures that the features of each sub-frequency are calculated
independently, preventing learning bias.
Lemma 2.Given W∗(1:𝐶)
𝑛 ={𝒘∗(1)
𝑛,𝒘∗(2)
𝑛,..., 𝒘∗(𝐶)
𝑛}𝑁
𝑛=1, if
W′𝑛=𝑓𝑇𝑟𝑎𝑛𝑠(W∗(1:𝐶)
𝑛), then by modeling the relationships of iden-
tical frequencies 𝒘𝑐𝑛across different channels, for the 𝑘-th key com-
ponent ˜𝒂𝑘presents in 𝒘(𝑐)
𝑛, we have−|Δ(𝑐)
𝑘|∝{|Δ(𝑐)
𝑘|}𝐶
𝑐=1. The
Transformer encoders will focus on channel-wise correlations instead
of the{|Δ(𝑐)
𝑘|}𝐾
𝑘=1, i.e., debiasing−|Δ(𝑐)
𝑘|̸∝𝑃(˜𝒂𝑘).
Lemma 2, which indicates a lower 𝑃(˜𝒂𝑘)does not necessarily lead
to an increase in|Δ˜𝒂𝑘|, thus avoiding disproportionate attention
to frequency components. Channel-wise attention is proposed in
the work of [ 23,47]. We include these studies as the baselines and
the results in Sec. 5.2. In this work, we have different modeling
purposes; we deploy self-attention on the aligned local features,
i.e., in the same frequency bands across channels, for frequency
debiasing.
4.4 Frequency-wise Summarization
Given the learned features of the sub-frequencies W′={𝒘′
1,𝒘′
2,..., 𝒘′
𝑁}
of the historical time series X, the frequency-wise summarizing
operation contains linear projections and IDFT:
X′=IDFT(A′)A′=Linear(W′)
where X′∈R𝐶×𝐻is the final output of the framework.
Table 1: Benchmark dataset summary
Datasets W
eather Electricity ETTh1 ETTh2 ETTm1 ETTm2 Solar Traffic
#Channel 21
321 7 7 7 7 137 862
#Timesteps 52969
26304 17420 17420 69680 69680 52179 17544
5 EXPERIMENTS
5.1 PROTOCOLS
- Datasets. We conduct extensive experiments on eight real-world
benchmark datasets: Weather, four ETT datasets (ETTh1, ETTh2,
ETTm1, ETTm2), Electricity (ECL), Traffic, and the Solar-Energy
 
2404KDD ’24, August 25–29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
Table 2: Multivariate forecasting results with prediction lengths 𝑆∈{96,192,336,720}for all datasets and fixed look-back length
𝑇=96. The best and second best results are highlighted. The full results of four selected datasets* will be shown in Figure 3.
Results are averaged from all prediction lengths. Full results for all datasets are listed in Appendix C.
Mo
delsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) ([23]) [21] [28] [47] [11] [41] [46] [25] [49] [24] [42]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ECL 0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338
ET
Th1 0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.548 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487
ET
Th2* 0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459
ET
Tm1* 0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517
ET
Tm2 0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371
T
raffic 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379
W
eather * 0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382
Solar-Energy *0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
Table 3: Full results of four selected datasets, with the best and second best results are highlighted. We compare extensive
competitive models under different prediction lengths following the setting of iTransformer [ 23]. The input sequence length is
set to 96 for all baselines. Avg means the average results from all four prediction lengths.
Mo
delsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEET
Th296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388
192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452
336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486
720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511
A
vg0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ET
Tm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475
192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496
336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537
720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561
A
vg0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517W
eather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336
192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367
336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395
720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428
A
vg0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711
192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692
336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723
720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717
A
vg0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
1stCount 17 17 0 2 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
dataset [ 19], with all datasets being published in [ 23]2. The infor-
mation these datasets provide is summarized in Table 1. And the
full results of four selected datasets* will be shown in Figure 3.
- Baselines. We select 11 SOTA baseline studies. Since we are focus-
ing on Transformer, we first add seven proposed Transformer-based
2https://github.com/thuml/iTransformerbaselines, including iTransformer [ 23], PatchTST [ 28], Crossformer
[47], Stationary [ 24], Fedformer [ 49], Pyraformer [ 22], Autoformer
[42]. We also add 2 MLP-based and 2 TCN-based methods, including
RLinear [21], DLinear [46], TiDE [11], TimesNet [41].
- Setup and Evaluation. All baselines use the same prediction
length with 𝐻∈{96,192,336,720}for all datasets. The look-back
 
2405Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
FEDformer#epoch 1 50
F 65 115 0k1
k2k3k4
k1
k2
k3
k4
PatchTST#epoch 1 50F 65 115 0k1
k2k3k4
k1
k2
k3
k4
iTransformer#epoch 1 50k1
k2
k3
k4
F 65 115 0k1
k2k3k4
Fredformer(Ours)k1
k2
k3
k4
#epoch 1 50650
 F115k1
k2k3k4Input Ground Truth Forecasting Relative Error0.9 0.1
Figure 4: Visualizations of the learning dynamics and results for Fredformer and baselines on the ETTh1 dataset, employing
line graphs to illustrate amplitudes in the frequency domain and heatmaps to represent training epoch errors.
window𝐿= 96 was used in our setting for fair comparisons, refer-
ring to [ 23,49]. We used MSE and MAE as the forecasting metrics.
We further analyzed the forecasting results between the model
outputs and the ground truth in the time and frequency domains.
Using heatmaps, we tracked the way in which Δ𝑘changes during
training to show the debiased results of Fredformer compared
with various SOTA baselines.
5.2 Results
Forecasting Results. Table 2 shows the average forecasting per-
formance across four prediction lengths. The best results are high-
lighted in bold, and the second-best results are underlined . With a
default look-back window of 𝐿=96, our approach realizes leading
performance levels on most datasets, securing 14 top-1 and 2 top-
2 positions across two metrics over eight datasets. More detailed
results for 4 of the eight datasets are shown in Table 3, where our
method achieves 34 top-1 and 6 top-2 rankings out of 40 possible
outcomes across the four prediction lengths. More comprehensive
results regarding the different prediction length settings on all
datasets are detailed in Appendix C.
Frequency Bias Evaluation. Figure 4 is a case study visualization
in the frequency domain, i.e., the DFT plot. The input, forecast out-
put, and ground truth data series are shown in blue, red, and green,
respectively. Similar to Section 2.2, the heat map shows the relative
error for four selected mid-to-high frequency components over
increasing epochs. After training, Fredformer accurately identifies
𝑘1,𝑘2, and𝑘3, with uniformly decreasing relative errors. Despite
a larger learning error for 𝑘4,Δ𝑘4consistently diminishes. This
performance contrasts with all the baselines, demonstrating a lack
of effectiveness in capturing these frequency components, with
unequal reductions in relative errors. In contrast, PatchTST demon-
strates a sudden improvement in component accuracy ( 𝑘2,𝑘3) dur-
ing the final stages of training. FEDformer fails to capture these
frequency components, possibly because its strategy of selecting
and learning weights for only a random set of 𝑘components over-
looks all unselected components. Notably, iTransformer overlooks
mid-to-high frequency features, partially learning components 𝑘1Table 4: Averaged results for each setting in the ablation
study. "No-CW" refers to removing channel-wise attention,
and "No-FR" refers to removing frequency refinement.
SettingFull No-CW No-FR
MSE
MAE MSE
MAE MSE
MAE
ET
Tm1 0.384
0.396 0.418
0.419 0.539
0.485
Weather 0.246
0.273 0.262
0.290 0.293
0.322
Table 5: The average forecasting accuracy (MSE) on ETTh1
dataset under 4 patch length settings.
Patch
length 8 16 32 Non
MSE 0.417 0.425 0.440 0.449
and𝑘3while ignoring 𝑘2and𝑘4, indicating a clear frequency bias.
This may stem from its use of channel-wise attention alongside
global normalization in the time domain, as discussed in Lemma 1
and further supported by our ablation study 5.3. This highlights
the effectiveness of frequency refinement and normalization.
5.3 Ablation Study
Channel-wise Attention and Frequency Refinement. We eval-
uate the effectiveness of channel-wise attention and frequency
refinement. To this end, we remove each component by ablation
and compare it with the original Fredformer . Table 4 shows that
our method consistently outperforms others in all experiments,
highlighting the importance of integrating channel-wise attention
with frequency local normalization in our design. Interestingly, em-
ploying frequency local normalization alone yields better accuracy
than channel-wise attention alone. This suggests that minimizing
proportional differences in amplitudes across various key frequency
components is crucial for enhancing accuracy.
 
2406KDD ’24, August 25–29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
ECL ETTh1
MSE MSE
Mb MbiTransformer
PatchTSTFedformer
OursCrossformer
Ours*iTransformerPatchTSTCrossformer
Fedformer
Ours
Figure 5: Comparation of forecasting accuracy and com-
putational complexity (VRAM usage) among Transformer-
based methods, Fredformer (Ours), and its optimized variant,
Nyström-Fredformer (Ours*).
Table 6: The theoretical computational complexity of
Transformer-based methods.
Method FEDformer PatchTST Crossformer
iTransformer Ours Ours*(Nyström)
Complexity 𝑂(𝐿𝐶)𝑂
𝐿2
𝑃2𝐶
𝑂
𝐿2
𝑃2𝐶
𝑂(𝐶2)𝑂
𝐿
𝑃𝐶2
𝑂
𝐿
𝑃𝐶
Effect of Patch Length. This ablation evaluates the impact of
patch length using the ETTh1 dataset. We conduct four experi-
ments with 𝑃=[8,16,32,48]patch lengths and corresponding
patch numbers 𝑁=[6,3,2,1]. In this context, 𝑁=1means fre-
quency normalization and channel-wise attention are applied to the
entire spectrum without a patching operation. Table 5 shows the
forecasting accuracy for each setting. As the patch length increases,
the granularity of the frequency features extracted by the model
becomes coarser, decreasing forecasting accuracy.
5.4 Discussion of Applicability
Beyond algorithmic considerations, we further discuss the practi-
cal deployment of Fredformer in real-world scenarios, with the
primary challenge being memory consumption during model train-
ing. The𝑂(𝑛2)complexity of self-attention limits the use of longer
historical time series for forecasting, generating the need for in-
novations to reduce computational demands [ 20,28,48]. Through
patching operations, we decrease the complexity from 𝑂(𝐿𝐶2)to
𝑂(𝐿
𝑃𝐶2), as shown in Table 6. However, our channel-wise attention
increases the computational costs with the number of channels,
potentially limiting practical applicability with many channels. To
address this, we propose a lightweight Fredformer , inspired by
NyströmFormer [ 44], which applies a matrix approximation to the
attention map. This design allows us to further reduce our com-
plexity to𝑂(𝐿
𝑃𝐶)without the need to modify the feature extraction
(attention computation) or the data stream structure within the
Transformer, unlike with previous methods [ 22,42,48,49]. Figure
5 shows a tradeoff between the model efficiency (VRAM usage) and
accuracy in our method and the baselines. The plain Fredformer
achieves high accuracy with low computational costs with fewer
channels, such as ETTh1 with 7 channels. However, as shown in theECL dataset (321 channels), the computational costs increase while
maintaining high accuracy as the channel number increases. Here,
Nyström- Fredformer further reduces computational requirements
without compromising accuracy (the right sub-figure), showing
that our model can realize computational efficiency and forecasting
accuracy. Further details and derivations are in Appendix B.
6 RELATED WORKS
Transformer for Time Series Forecasting. Forecasting is impor-
tant in time series analysis [ 1,15]. Transformer has significantly
progressed in time series forecasting[ 18,28,47]. Earlier attempts
focused on improving the computational efficiency of Transform-
ers for time series forecasting tasks[ 3,22,48]. Several studies have
used Transformers to model inherent temporal dependencies in
the time domain of time series[ 20,22,23,28,48]. Various studies
have integrated frequency decomposition and spectrum analysis
with the Transformer in modeling temporal variations [ 40,42] to
improve the capacity for temporal-spatial representation. In [ 49],
attention layers are designed that directly function in the frequency
domain to enhance spatial or frequency representation.
Modeling Short-Term Variation in Time Series. Short-term
variations are intrinsic characteristics of time series data and play
a crucial role in effective forecasting [ 10,24]. Numerous deep
learning-based methods have been proposed to capture these tran-
sient patterns [ 2,7,9,27,32,36,37,39,42]. Here, we summarize
some studies closely aligned with our proposed method. Pyraformer
[22] applies a pyramidal attention module with inter-scale and intra-
scale connections to capture various temporal dependencies. FED-
former [ 49] incorporates a Fourier spectrum within the attention
computation to identify pivotal frequency components. Beyond
Transformers, TimesNet [ 41] employs Inception blocks to capture
intra-period and inter-period variations.
Channel-wise Correlation. Understanding the cross-channel cor-
relation is also critical for time series forecasting. Several studies
aimed to capture intra-channel temporal variations and model the
inter-channel correlations using Graph Neural Networks (GNNs)
[6,43]. Recently, Crossformer [ 47] and iTransformer [ 23] both
adopted channel-wise Transformer-based frameworks, and exten-
sive experimental results have demonstrated the effectiveness of
channel-wise attention for time series forecasting.
7 CONCLUSION
In this paper, we first empirically analyzed frequency bias, delving
into its causes and exploring debiasing strategies. We then provided
a formulation of this bias based on our analytical insights. We pro-
posed the Fredformer framework with three critical designs to
tackle this bias and thus ensure unbiased learning across frequency
bands. Our extensive experiments across eight datasets confirmed
the excellent performance of our proposed method. Visual anal-
ysis confirmed that our approach effectively mitigates frequency
bias. The model analysis further illustrated how our designs aid
frequency debiasing and offered preliminary guidelines for future
model design. Additionally, a lightweight variant of our model ad-
dresses computational efficiency, facilitating practical application.
 
2407Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
8 ACKNOWLEDGMENTS
We thank anonymous reviewers for their insightful comments
and discussions. This work is supported by JSPS KAKENHI Grant-
in-Aid for Scientific Research Number JP21H03446, JP23K16889,
JP24K20778, NICT JPJ012368C03501, JST-AIP JPMJCR21U4, JST-
CREST JPMJCR23M3, JST-RISTEX JPMJRS23L4.
REFERENCES
[1]Rob J. Hyndman Alysha M. De Livera and Ralph D. Snyder. 2011. Forecasting
Time Series With Complex Seasonal Patterns Using Exponential Smoothing. J.
Amer. Statist. Assoc. (2011), 1513–1527.
[2]Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. Convolutional Sequence
Modeling Revisited. (2018).
[3]Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-
Document Transformer. (2020). arXiv:2004.05150 [cs.CL]
[4]S.A. Broughton and K. Bryan. 2011. Discrete Fourier Analysis and Wavelets:
Applications to Signal and Image Processing. (2011).
[5]Daniela Calvetti. 1991. A Stochastic Roundoff Error Analysis for the Fast Fourier
Transform. (1991).
[6]Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Conguri Huang,
Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2021. Spectral Tem-
poral Graph Neural Network for Multivariate Time-series Forecasting. (2021).
[7]Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, and Shou-De Lin. 2018. A Memory-
Network Based Solution for Multivariate Time-Series Forecasting. (2018).
arXiv:1809.02105 [cs.LG]
[8]Zheng Chen, Ziwei Yang, Lingwei Zhu, Wei Chen, Toshiyo Tamura, Naoaki
Ono, Md Altaf-Ul-Amin, Shigehiko Kanaya, and Ming Huang. 2023. Automated
Sleep Staging via Parallel Frequency-Cut Attention. IEEE Transactions on Neural
Systems and Rehabilitation Engineering (2023), 1974–1985.
[9]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.
(2014). arXiv:1412.3555 [cs.NE]
[10] Jesus Crespo Cuaresma, Jaroslava Hlouskova, Stephan Kossmeier, and Michael
Obersteiner. 2004. Forecasting Electricity Spot-Prices Using Linear Univariate
Time-Series Models. Applied Energy 77 (2004), 87–106.
[11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and
Rose Yu. 2023. Long-term Forecasting with TiDE: Time-series Dense Encoder.
Transactions on Machine Learning Research (2023).
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale. In International
Conference on Learning Representations.
[13] Filip Elvander and Andreas Jakobsson. 2020. Defining Fundamental Frequency
for Almost Harmonic Signals. IEEE TRANSACTIONS ON SIGNAL PROCESSING
(2020).
[14] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. 2023. ContraNorm: A
Contrastive Learning Perspective on Oversmoothing and Beyond. (2023).
[15] James D Hamilton. 2020. Time series analysis. (2020).
[16] Nicholas W. Hammond, François Birgand, Cayelan C. Carey, Bethany Bookout,
Adrienne Breef-Pilz, and Madeline E. Schreiber. 2023. High-frequency Sensor Data
Capture Short-term Variability In Fe and Mn Concentrations Due to Hypolimnetic
Oxygenation and Seasonal Dynamics in a Drinking Water Reservoir. Water
Research 240 (2023).
[17] Long Steven R. Wu Manli C. Shih Hsing H. Zheng Quanan Yen Nai-Chyuan
Tung Chi Chao Huang Norden E. Shen Zheng and Liu Henry H. 1998. The
Empirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and
Non-stationary Time Series Analysis. Proceedings of the Royal Society of London.
Series A: mathematical, physical, and engineering sciences (1998), 903–995.
[18] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.
PDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf-
fic Flow Prediction. (2023), 4365–4373.
[19] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
Long- and Short-Term Temporal Patterns with Deep Neural Networks. (2018),
95–104.
[20] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,
and Xifeng Yan. 2019. Enhancing the Locality and Breaking the Memory Bottle-
neck of Transformer on Time Series Forecasting. (2019).
[21] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. 2023. Revisiting Long-term Time
Series Forecasting: An Investigation on Linear Mapping. (2023).
[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and
Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for
Long-Range Time Series Modeling and Forecasting. (2022).[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and
Mingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective for
Time Series Forecasting. In The Twelfth International Conference on Learning
Representations.
[24] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary
Transformers: Exploring the Stationarity in Time Series Forecasting. (2022).
[25] Liu M., Zeng A., Chen M., Xu Z., Lai Q., Ma L., and Q. Xu. 2022. SCINet: Time
Series Modeling and Forecasting with Sample Convolution and Interaction. (2022),
5816–5828.
[26] Sobhan Moosavi, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan
Parthasarathy, and Rajiv Ramnath. 2019. Short and Long-Term Pattern Discovery
Over Large-Scale Geo-Spatiotemporal Data. (2019), 2905–2913.
[27] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: Acceler-
ating Recurrent Network Training for Long or Event-based Sequences. (2016).
arXiv:1610.09513 [cs.LG]
[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.
(2023).
[29] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work?
(2022).
[30] John G. Proakis and Dimitris G. Manolakis. 1996. Digital Signal Processing (3rd
Ed.): Principles, Algorithms, and Applications. (1996).
[31] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
Hamprecht, Yoshua Bengio, and Aaron Courville. 2019. On the Spectral Bias of
Neural Networks. 97 (2019), 5301–5310.
[32] Daniel Stoller, Mi Tian, Sebastian Ewert, and Simon Dixon. 2019. Seq-U-Net:
A One-Dimensional Causal U-Net for Efficient Sequence Modelling. (2019).
arXiv:1911.06393 [cs.LG]
[33] James R. Thompson and James R. Wilson. 2016. Multifractal Detrended Fluctua-
tion Analysis: Practical Applications to Financial Time Series. Mathematics and
Computers in Simulation 126 (2016), 63–88.
[34] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. 2023. Scan and Snap:
Understanding Training Dynamics and Token Composition in 1-layer Trans-
former. (2023).
[35] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. 2022. Anti-
Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis:
From Theory to Practice. (2022).
[36] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and
Fan Zhou. 2022. Learning Latent Seasonal-Trend Representations for Time Series
Forecasting. (2022).
[37] Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun. 2020. Fast RobustSTL: Effi-
cient and Robust Seasonal-Trend Decomposition for Time Series with Complex
Patterns. (2020), 2203–2213.
[38] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,
and Liang Sun. 2023. Transformers in Time Series: A Survey. (2023).
[39] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.
CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for
Time Series Forecasting. (2022).
[40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H.
Hoi. 2022. ETSformer: Exponential Smoothing Transformers for Time-series
Forecasting. (2022).
[41] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. (2023).
[42] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:
Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-
casting. (2021).
[43] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with
Graph Neural Networks. (2020).
[44] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
Fung, Yin Li, and Vikas Singh. 2021. Nyströmformer: A Nyström-based Algorithm
for Approximating Self-Attention. (2021).
[45] Zhi-Qin John Xu. 2020. Frequency Principle: Fourier Analysis Sheds Light on
Deep Neural Networks. Communications in Computational Physics 28 (2020),
1746–1767.
[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers
Effective for Time Series Forecasting? (2023).
[47] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-
Dimension Dependency for Multivariate Time Series Forecasting. (2023).
[48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long
Sequence Time-Series Forecasting. (2021).
[49] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.
FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series
Forecasting. (2022), 1–12.
[50] Yunyue Zhu and Dennis Shasha. 2002. StatStream: Statistical Monitoring of
Thousands of Data Streams in Real Time. (2002), 358–369.
 
2408KDD ’24, August 25–29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai
The full appendix can be found at: http://arxiv.org/abs/2406.09009
A DETAILS OF THE CASE STUDIES
Here, we illustrate the details of how we generated the data for
case study 2 in Sec.2.2.2: The generation of data for Case Study
2 from the original time series involves a sequence of steps to
emphasize certain frequency components by manipulating their
positions in the frequency domain. This process not only constructs
a dataset with distinct frequency characteristics but also preserves
the inherent noise and instability of the real data, enhancing the
robustness and credibility of subsequent analyses. Specifically, the
steps are as follows:
(1)Apply the Discrete Fourier Transform (DFT) to the original
time series data to obtain its frequency components, exclud-
ing columns irrelevant for Fourier analysis (e.g., dates).
(2)Select four prominent low-frequency components from the
entire frequency spectrum and move them to the mid-frequency
part. This modification aims to reduce the impact of fre-
quency bias typically seen between low and high frequencies
by placing important components in a non-low and non-high
frequency position.
(3) Split the frequency components into three equal parts.
(4)Rearrange these parts according to a predefined order for
frequency emphasis, ensuring that the first part is moved to
the end while keeping the original second and third parts in
their order.
(5)Apply the Inverse Discrete Fourier Transform (IDFT) to the
rearranged frequency data to convert it back into the time
domain, thereby generating the modified "mid" frequency
data.
(6)Reinsert any excluded columns (e.g., dates) to maintain the
original structure of the data.
Through the operations described above, we have constructed
a dataset with clearly high amplitude frequency components in
the middle of the frequency domain. By moving significant low-
frequency components to the mid-frequency section, we aim to
mitigate the effects of frequency differences that arise from the
dominance of low and high frequencies. The advantage of creating
artificial data through these simple modifications to real data lies
in its ability to preserve the inherent noise and instability present
in the real data, thereby enhancing the robustness and credibility
for subsequent analysis.
B NYSTRÖM APPROXIMATION IN
TRANSFORMER SELF-ATTENTION
MECHANISM
Overview: To streamline the attention computation, we select 𝑚
landmarks by averaging rows or columns of the attention matrix,
simplifying the matrices Q𝑛andK𝑛into ˜Q𝑛and ˜K𝑛. The Nyström
approximation for the 𝑛-th channel-wise attention A𝑛is then cal-
culated as A𝑛≈˜A𝑛=˜F𝑛˜A𝑛˜B𝑛, where ˜F𝑛=softmax(Q𝑛˜K𝑛𝑇),
˜A𝑛=softmax(˜Q𝑛˜K𝑛𝑇)+,˜B𝑛=softmax(˜Q𝑛K𝑛𝑇). Here, ˜A𝑛+is the
Moore-Penrose inverse of ˜A𝑛[44]. This significantly reducing the
computational load from 𝑂(𝐿
𝑃𝐶2)to𝑂(𝐿
𝑃𝐶). Specifically:Details: We reduce the computational cost of self-attention in
the Transformer encoder using the Nyström method. Following,
we describe how to use the Nyström method to approximate the
softmax matrix in self-attention by sampling a subset of columns
and rows.
Consider the softmax matrix in self-attention, defined as:
𝑆=softmax 
𝑄𝐾𝑇
√︁𝑑𝑞!
This matrix can be partitioned as:
𝑆=𝐴𝑆𝐵𝑆
𝐹𝑆𝐶𝑆
Where𝐴𝑆is derived by sampling 𝑚columns and rows from 𝑆.
By employing the Nyström method, the SVD of 𝐴𝑆is given by:
𝐴𝑆=𝑈Λ𝑉𝑇
Using this, an approximation ˆ𝑆of𝑆can be constructed:
ˆ𝑆=𝐴𝑆𝐵𝑆
𝐹𝑆𝐹𝑆𝐴+
𝑆𝐵𝑆
Where𝐴+
𝑆is the Moore-Penrose inverse of 𝐴𝑆.
To further elaborate on the approximation, given a query 𝑞𝑖and
a key𝑘𝑗, let:
K(𝑞𝑖,𝐾)=softmax 
𝑞𝑖𝐾𝑇
√︁𝑑𝑞!
K(𝑄,𝑘𝑗)=softmax 𝑄𝑘𝑇
𝑗√︁𝑑𝑞!
From the above, we can derive:
𝜙(𝑞𝑖,𝐾)=Λ−1
2𝑉𝑇K(𝑞𝑖,𝐾)𝑚×1
𝜙(𝑄,𝑘𝑗)=Λ−1
2𝑈𝑇K(𝑄,𝑘𝑗)𝑚×1
Thus, the Nyström approximation for a particular entry in ˆ𝑆is:
ˆ𝑆𝑖𝑗=𝜙(𝑞𝑖,𝐾)𝑇𝜙(𝑄,𝑘𝑗)
In matrix form, ˆ𝑆can be represented as:
ˆ𝑆=softmax 
𝑄𝐾𝑇
√︁𝑑𝑞!
𝑛×𝑚𝐴+
𝑆softmax 
𝑄𝐾𝑇
√︁𝑑𝑞!
𝑚×𝑛
This method allows for the approximation of the softmax matrix
in self-attention, potentially offering computational benefits.
C DETAILED RESULTS OF ALL DATASETS
Here, we show the detailed forecasting results of full datasets in
the Table. 7. The best andsecond best results are highlighted. With
a default look-back window of 𝐿=96, our proposal shows lead-
ing performance on most datasets and different prediction length
settings, with 60 top-1 (29 + 31) cases out of 80 in total.
 
2409Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 7: Full results of the long-term forecasting task
. We compare extensive competitive models under different prediction lengths following the setting of iTransformer [23]. The input
sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths.
ModelsFredformer iT
ransformer RLinear PatchTST Cr
ossformer TiDE TimesNet DLinear SCINet FEDformer Stationar
yA
utoformer
(
Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEET
Tm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475
192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496
336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537
720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561
A
vg0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517ET
Tm296 0.177 0.259 0.180 0.264 0.182 0.265 0.175 0.259 0.287 0.366 0.207 0.305 0.187 0.267 0.193 0.292 0.286 0.377 0.203 0.287 0.192 0.274 0.255 0.339
192 0.243 0.301 0.250 0.309 0.246 0.304 0.241 0.302 0.414 0.492 0.290 0.364 0.249 0.309 0.284 0.362 0.399 0.445 0.269 0.328 0.280 0.339 0.281 0.340
336 0.302 0.340 0.311 0.348 0.307 0.342 0.305 0.343 0.597 0.542 0.377 0.422 0.321 0.351 0.369 0.427 0.637 0.591 0.325 0.366 0.334 0.361 0.339 0.372
720 0.397 0.396 0.412 0.407 0.407 0.398 0.402 0.400 1.730 1.042 0.558 0.524 0.408 0.403 0.554 0.522 0.960 0.735 0.421 0.415 0.417 0.413 0.433 0.432
A
vg0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371ET
Th196 0.373 0.392 0.386 0.405 0.386 0.395 0.414 0.419 0.423 0.448 0.479 0.464 0.384 0.402 0.386 0.400 0.654 0.599 0.376 0.419 0.513 0.491 0.449 0.459
192 0.433 0.420 0.441 0.436 0.437 0.424 0.460 0.445 0.471 0.474 0.525 0.492 0.436 0.429 0.437 0.432 0.719 0.631 0.420 0.448 0.534 0.504 0.500 0.482
336 0.470 0.437 0.487 0.458 0.479 0.446 0.501 0.466 0.570 0.546 0.565 0.515 0.491 0.469 0.481 0.459 0.778 0.659 0.459 0.465 0.588 0.535 0.521 0.496
720 0.467 0.456 0.503 0.491 0.481 0.470 0.500 0.488 0.653 0.621 0.594 0.558 0.521 0.500 0.519 0.516 0.836 0.699 0.506 0.507 0.643 0.616 0.514 0.512
A
vg0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.458 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487ET
Th296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388
192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452
336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486
720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511
A
vg0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ECL96 0.147 0.241 0.148 0.240 0.201 0.281 0.195 0.285 0.219 0.314 0.237 0.329 0.168 0.272 0.197 0.282 0.247 0.345 0.193 0.308 0.169 0.273 0.201 0.317
192 0.165 0.258 0.162 0.253 0.201 0.283 0.199 0.289 0.231 0.322 0.236 0.330 0.184 0.289 0.196 0.285 0.257 0.355 0.201 0.315 0.182 0.286 0.222 0.334
336 0.177 0.273 0.178 0.269 0.215 0.298 0.215 0.305 0.246 0.337 0.249 0.344 0.198 0.300 0.209 0.301 0.269 0.369 0.214 0.329 0.200 0.304 0.231 0.338
720 0.213 0.304 0.225 0.317 0.257 0.331 0.256 0.337 0.280 0.363 0.284 0.373 0.220 0.320 0.245 0.333 0.299 0.390 0.246 0.355 0.222 0.321 0.254 0.361
A
vg0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338T
raffic96 0.406 0.277 0.395 0.268 0.649 0.389 0.544 0.359 0.522 0.290 0.805 0.493 0.593 0.321 0.650 0.396 0.788 0.499 0.587 0.366 0.612 0.338 0.613 0.388
192 0.426 0.290 0.417 0.276 0.601 0.366 0.540 0.354 0.530 0.293 0.756 0.474 0.617 0.336 0.598 0.370 0.789 0.505 0.604 0.373 0.613 0.340 0.616 0.382
336 0.432 0.281 0.433 0.283 0.609 0.369 0.551 0.358 0.558 0.305 0.762 0.477 0.629 0.336 0.605 0.373 0.797 0.508 0.621 0.383 0.618 0.328 0.622 0.337
720 0.463 0.300 0.467 0.302 0.647 0.387 0.586 0.375 0.589 0.328 0.719 0.449 0.640 0.350 0.645 0.394 0.841 0.523 0.626 0.382 0.653 0.355 0.660 0.408
A
vg 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379W
eather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336
192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367
336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395
720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428
A
vg0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711
192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692
336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723
720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717
A
vg0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
1stCount 29 31 4 8 1 1 2 1 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0
 
2410