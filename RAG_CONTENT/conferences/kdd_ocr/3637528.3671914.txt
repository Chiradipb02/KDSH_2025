Geometric View of Soft Decorrelation in Self-Supervised Learning
Yifei Zhang
yfzhang@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, ChinaHao Zhu
allen.zhu@data61.csiro.au
Data61/CSIRO
Sydney, AustraliaZixing Song
zxsong@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, China
Yankai Chen
ykchen@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, ChinaXinyu Fu
xyfu@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, ChinaZiqiao Meng
zqmeng@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, China
Piotr Koniusz∗
piotr.koniusz@data61.csiro.au
Data61♥CSIRO
Australian National University
Canberra, AustraliaIrwin King∗
king@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong SAR, China
ABSTRACT
Contrastive learning, a form of Self-Supervised Learning (SSL), typ-
ically consists of an alignment term and a regularization term. The
alignment term minimizes the distance between the embeddings of
a positive pair, while the regularization term prevents trivial solu-
tions and expresses prior beliefs about the embeddings. As a widely
used regularization technique, soft decorrelation has been employed
by several non-contrastive SSL methods to avoid trivial solutions.
While the decorrelation term is designed to address the issue of
dimensional collapse, we find that it fails to achieve this goal theo-
retically and experimentally. Based on such a finding, we extend the
soft decorrelation regularization to minimize the distance between
the covariance matrix and an identity matrix. We provide a new
perspective on the geometric distance between positive definite
matrices to investigate why the soft decorrelation cannot efficiently
solve the dimensional collapse. Furthermore, we construct a family
of loss functions utilizing the Bregman Matrix Divergence (BMD),
with the soft decorrelation representing a specific instance within
this family. We prove that a loss function (LogDet) in this family can
solve the issue of dimensional collapse. Our novel loss functions
based on BMD exhibit superior performance compared to the soft
decorrelation and other baseline techniques, as demonstrated by
experimental results on graph and image datasets.
CCS CONCEPTS
•Information systems →Data mining.
∗Corresponding authors.
This work is licensed under a Creative Commons Attribution-
NonCommercial International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671914KEYWORDS
Self-Supervised Learning, Dimensional Collapse, Bregman Diver-
gence
ACM Reference Format:
Yifei Zhang, Hao Zhu, Zixing Song, Yankai Chen, Xinyu Fu, Ziqiao Meng,
Piotr Koniusz, and Irwin King. 2024. Geometric View of Soft Decorrelation in
Self-Supervised Learning. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671914
1 INTRODUCTION
Significant emphasis has been placed on Self-Supervised Learning
(SSL) [ 11,62,64]. SSL algorithms are renowned for their capacity to
effectively leverage large amounts of unlabeled data, which is con-
siderably easier to collect than labeled data. This enables the utiliza-
tion of enormous amounts of data to enhance the representational
capacity of models. In addition, SSL can improve the generaliza-
tion of models by learning representations that capture significant
aspects of the input data [ 36,46,47,59]. This is crucial for prob-
lems when labeled data is limited, noisy, or not representative of the
desired distribution [ 13,14,45]. Furthermore, SSL facilitates the pre-
training of models on large-scale datasets prior to fine-tuning them
for specific tasks with limited labeled data. This approach, known
as transfer learning, leverages pre-trained models as a foundation
for various downstream tasks [12, 15, 44, 51, 58, 60, 61].
SSL can be categorized according to various pretext tasks. While
numerous pretext tasks have been proposed, such as predicting the
position of image patches [ 19,38], identifying rotations [ 22], image
colorization [ 34], or image inpainting [ 40], recent advancements in-
clude models based on contrastive learning [ 2,6,7,27,39,50,54,69],
masked signal modeling [ 25,52] and spatial contrastive pre-training
[41]. A typical contrastive-based loss, such as InfoNCE [ 7], aims
to enforce consistency between augmented views from the same
samples (positive pairs) to preserve invariant information about the
data. An additional regularization term, such as pushing apart any
two different samples (negative pairs), is imposed to prevent trivial
4338
KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
 
Figure 1: Geometric view of soft decorrelation in self-
supervised learning. Soft decorrelation is viewed as mini-
mizing the distance between the feature covariance matrix
and the identity matrix. The used measure may be Euclidean
or non-Euclidean (e.g ., a geodesic path for the space of posi-
tive definite matrices).
solutions. Furthermore, Wang and Isola [50] decompose InfoNCE
into two terms: the alignment of features from positive pairs and
the uniformity of the induced distribution of the features on the
hypersphere.
Recent studies have also explored non-contrastive SSL models.
Among these models, Barlow Twins [ 54], CCA-SSG [ 55], and VI-
CReg [ 3] utilize the soft decorrelation as regularization for decorre-
lating features under the unconstrained optimization. They reduce
redundancy in the learned features, enabling Barlow Twins and
VICReg to perform as well as contrastive models. Though these
methods may prevent trivial solutions, also known as complete
collapse [ 29], they still encounter the challenge of dimensional
collapse (DC) [ 29,65]. DC in representation learning occurs when
the embedding spans a subspace rather than the entire embedding
space. This phenomenon is characterized by a few dominant sin-
gular values in the spectrum while the remaining singular values
decay significantly, approaching zero. Thus, DC leads to loss of in-
formation, degraded representations, challenges in generalization,
and limited transferability to other tasks.
To mitigate dimensional collapse, typical regularization tech-
niques such as the soft decorrelation are used in SSL methods. In
this paper, we introduce a novel geometric view to analyze the soft
decorrelation loss, which does not guarantee a full-rank covariance
matrix of features. We offer a solution with theoretical guarantee
for the dimensional collapse problem. Specifically, we first reformu-
late the soft decorrelation into optimizing the distance between the
covariance matrix and the identity matrix (as shown in Figure 1).
Compared with measuring the distance between matrices by the
Frobenius norm, we employ the Bregman Matrix Divergence (BMD)
and analyze the geometric properties of different variants. Unlike
existing approaches that typically rely on the Frobenius norm in
Barlow Twin, we opt for the utilization of the Log-Determinant
(LogDet) divergence captured between matrices, which is a vari-
ant of the Bregman divergence. We demonstrate that the LogDet
divergence imposes an infinite penalty on rank-deficient matrices,
implicitly enforcing a full-rank solution in the optimization process.
Consequently, both theoretically and empirically, our approach
proves effective in preventing dimensional collapse. In summary,
we list our contributions below:
i.We introduce a novel geometric perspective to analyze the
limitations of soft decorrelation in preventing dimensional
collapse in self-supervised learning.ii.We propose to employ the Bregman Matrix Divergence (BMD)
as a generalized framework for measuring the divergence be-
tween matrices in the SSL setting, and analyze geometric prop-
erties of the soft decorrelation.
iii.Supported by both theoretical and empirical evidence, we
demonstrate the effectiveness of the LogDet divergence, a spe-
cific variant of BMD, in preventing dimensional collapse by
imposing an infinite penalty on rank-deficient matrices.
2 RELATED WORKS
Contrastive SSL. Contrastive SSL relies on the utilization of both
positive and negative pairs of augmented samples [ 7,8,39]. The
widely adopted InfoNCE loss [ 39] is composed of two key compo-
nents: an alignment term, which aims to maximize the similarity
between positive pairs, and a uniformity term, designed to encour-
age a maximum (uniform) spread between all embeddings [ 50].
Among state-of-the-art methods, SimCLR [ 7] necessitates a sub-
stantial number of negative pairs [ 7], which translates into a large
batch size (or memory bank). This requirement poses a compu-
tational bottleneck, as the computation of the loss in SimCLR in-
creases quadratically with the batch size. In contrast, REFINE [ 66]
trades accuracy for speed by the use of Randomized Blocked QR
and random negative sampling.
Non-contrastive SSL. The highlight of non-contrastive SSL meth-
ods is that they avoid the explicit use of negative pairs. To address
the issue of collapse, pioneering models such as BYOL [ 24] and
SimSiam [ 10] introduced a dual pair of Siamese network with an
asymmetric architecture. To this end, stop gradient updates and/or
employing the moving average of neural network parameters per
the Siamese branch are used. While these tools are heuristic as they
do not explicitly penalize collapsed embeddings, they have proven
effective in practical applications.
Non-contrastive SSL by decorrelating regularization. Zhu et
al. [68] and Zhu et al. [67] maintain a symmetric Siamese network
while integrating loss functions aimed at mitigating embedding col-
lapse by feature decorrelation. Barlow Twins [ 54], the pioneering
method within this line of research, employs a decorrelating reg-
ularizer acting on cross-correlation across two views. VICReg [ 3],
on the other hand, leverages regularizers defined in relation to the
covariance matrices of individual views.
Non-contrastive SSL by whitening. Hua et al . [28] and Ermolov
et al. [20] employed whitening to explicitly decorrelate features
during training, as an alternative to regularization. Zhang et al. [56]
applied whitening to both features and instances. However, such
whitening procedures necessitate the computation of all eigen-
values of covariance matrices, resulting in a large computation
overhead. However, covariance whitening can be achieved by the
so-called power normalization [ 30] which can be computed without
explicit computation of eigenvalues [31].
Dimensional collapse. Dimensional collapse, also known as the
spectral collapse [ 35], is a phenomenon in representation learn-
ing where the embedding space is dominated by the largest a few
singular values while other singular values decay significantly as
the training step increases. As the actual embedding dimension is
4339Geometric View of Soft Decorrelation in Self-Supervised Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
usually high, the dimensional collapse phenomenon prevents learn-
ing diverse information from the high-dimensional space, limiting
their representation power. Jing et al. [29] analyzed the dimensional
collapse phenomenon from a theoretical perspective and attributed
it to the effect of strong data augmentation and implicit regular-
ization effect of neural networks [ 1]. Previous methods usually
adopt whitening operations [ 20,28] to mitigate this issue. However,
explicit whitening is usually computationally inefficient and thus
not applicable to graph data where efficiency is important. Instead
of spectrum whitening, Zbontar et al. [54] use a soft decorrelation
as a regularization to achieve similar effect. In this paper, we show
that the soft decorrelation does not effectively alleviate the prob-
lem of dimensional collapse. Thus, we propose a more generalized
framework to solve this problem theoretically.
3 PRELIMINARIES
A General Framework for Self-Supervised Learning. In a typi-
cal self-supervised model, two related views of the same data points
are generated through a randomized augmentation operator T(·) ,
and represented as ˜𝒙(1)=T(𝒙)and ˜𝒙(2)=T(𝒙). An encoder
𝑓(·)is then applied to the augmented samples, ˜𝒙(1)and ˜𝒙(2), to
transform them into a latent space, producing embeddings 𝒛(1)and
𝒛(2). Since the data is fed in batches, denoted as X=
𝒙𝑖	𝑁
𝑖=1, we
obtain two sets of embeddings: Z1=
𝒛(1)
𝑖	𝑁
𝑖=1andZ2=
𝒛(2)
𝑖	𝑁
𝑖=1.
The resulting hidden representations are expected to contain the
essential features of the original data, making them beneficial for
downstream tasks. Thus, a self-supervised learning objective, L𝑠𝑠𝑙,
contains two parts:
L𝑠𝑠𝑙=L𝑎𝑙𝑖𝑔𝑛+L𝑟𝑒𝑔, (1)
whereL𝑎𝑙𝑖𝑔𝑛 represents the alignment loss making the embeddings
of augmented views of the same data point closer. In contrast, L𝑟𝑒𝑔
represents the regularization loss which helps prevent trivial solu-
tions and promote desirable properties (uniformity) of the learned
representations.
Contrastive learning. Typical contrastive approaches [ 7,63] adopt
the InfoNCE loss. Wang and Isola [50] showed that InfoNCE can
be factorized into Eq. (1) where the so-called alignment and unifor-
mity [50] terms are defined as:
L𝑎𝑙𝑖𝑔𝑛=1
𝑁𝑁∑︁
𝑖=1h𝒛(1)
𝑖−𝒛(2)
𝑖2
2i
, (alignment)
L𝑢𝑛𝑖=logh
E(𝒛𝑖,𝒛𝑘)∼Z 1×Z 2𝑒−𝒛(1)
𝑖−𝒛(2)
𝑘2
2/𝜏i
. (uniformity)
(2)
L𝑎𝑙𝑔𝑖𝑛 encourages the embeddings of the augmented samples of
the same data point to be similar to achieve embeddings invariant
to augmentations.L𝑢𝑛𝑖encourages embeddings to be distributed
evenly on the surface of the ℓ2ball so that the trivial solution (model
collapse) can be avoided, e.g., a complete collapse: ∀𝒙,𝑓(𝒙)=𝑐.
However,L𝑢𝑛𝑖cannot fully avoid the dimensional collapse. We
summarize our observation as follows:
i.DC [ 28] is characterized by one or more singular values in the
feature matrix to be zero, e.g.,𝜆𝑖=0.L𝑢𝑛𝑖may yield a finite
reward (decrease of L𝑢𝑛𝑖) to avoid the dimensional collapse(𝜆𝑖=0). However, if the remaining loss terms yield a higher
reward (decrease in their value) than L𝑢𝑛𝑖for occurrence of the
dimensional collapse ( 𝜆𝑖=0) then the DC will occur.
ii.As1/𝜏increases,L𝑢𝑛𝑖degrades to a hard minimum. Notice that:
lim
𝜏→0𝜏logh
E(𝒛𝑖,𝒛𝑘)∼Z 1×Z 2𝑒−𝒛(1)
𝑖−𝒛(2)
𝑘2
2/𝜏i
, (3)
which explains the empirical observation of Wang and Liu [ 49]
that the uniformity of embeddings worsens as 1/𝜏increases.
Thus, in what follows we consider instead the redundancy re-
duction principle to mitigate the above issues.
Redundancy reduction. From the perspective of redundancy re-
duction [ 3,54], achieving high-quality self-supervised embeddings
requires that: (i) positive pairs exhibit similar semantics; (ii) the
embeddings do not collapse on a trivial distribution, e.g., a single
point or mere few points; and (iii) there is a low/zero correlation
between different features. Thus, the redundancy reduction aims
to solve the following optimization problem:
min
𝜽L𝑎𝑙𝑖𝑔𝑛 s.t.𝚺1=𝚺2=I (4)
where 𝚺1=1
𝑁𝑁∑︁
𝑖=1z⊤(1)
𝑖z(1)
𝑖and𝚺2=1
𝑁𝑁∑︁
𝑖=1z⊤(2)
𝑖z(2)
𝑖.
The constraints 𝚺1=𝚺2=Iforce the embeddings to follow an
isotropic Normal distribution. By applying Lagrangian multipli-
ers, we can convert these hard constraints into soft constraints,
transforming the optimization problem into the form in Eq. (1):
minL𝑎𝑙𝑖𝑔𝑛+𝜆L𝑑𝑒𝑐whereL𝑑𝑒𝑐=𝚺1−I2
𝐹+𝚺2−I2
𝐹.(5)
However, the regularization term L𝑑𝑒𝑐in Eq. (5) shares the same
issue asL𝑢𝑛𝑖in Eq. (2). Both regularization terms impose a finite
penalty for any singular values 𝜆𝑖=0, which can lead to dimen-
sional collapse if the other loss terms provide a higher reward than
the penalty. In other words, if the decrease in the other loss terms
outweighs the penalty imposed by the regularization term for the
occurrence of dimensional collapse ( 𝜆𝑖=0), then the model will
still suffer from the dimensional collapse. A detailed theoretical
analysis of this phenomenon is provided in Section 5.
4 METHOD
Below, we introduce the Bregman Matrix Divergences (BMD), the
main topic of this paper, and discuss their geometric characteristics.
Our overall model is illustrated in Figure 2.
4.1 Geometric View of Regularization Beyond
Euclidean Distance
To convert the constrained optimization problem in Eq. (4) into an
unconstrained problem, we introduce a regularization term that
forces the covariance matrix to be close to the identity matrix. This
regularization term employs a specific geometry to quantify the
distance between two matrices. In Eq. (5), the default choice of the
geometry is the Euclidean geometry, where the distance between
two matrices is measured by the Frobenius norm:
L𝑟𝑒𝑔=𝐷𝑒𝑢𝑐(𝚺,I)=∥𝚺−I∥2
𝐹. (6)
In what follows, we will define several 𝐷(·,·)which usually de-
note squared divergences. As discussed in the previous section,
4340KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
Encoder (ie., CNN, GNN)
Figure 2: The general framework of our proposed method.
The input data undergoes augmentation operations Tto gen-
erate two correlated views, ˜𝒙(1)and ˜𝒙(2). These augmented
samples are then passed through an encoder (e.g ., CNN or
GNN) to obtain embeddings 𝒛(1)and𝒛(2)in the latent space.
The self-supervised learning objective L𝑠𝑠𝑙consists of an
alignment termL𝑎𝑙𝑖𝑔𝑛, which brings the embeddings of posi-
tive pairs closer together, and a geometric soft decorrelation
termL𝑟𝑒𝑔, which prevents trivial solutions and promotes
desirable properties (decorrelation) of embeddings.
the Euclidean geometry may lead to the dimensional collapse. To
address this issue, we consider an alternative geometry that can pre-
serve full rank. We introduce the Bregman divergence as a general
geometric setting to measure the divergence between matrices.
Bregman matrix divergence. To measure the nearness between
two matrices, we consider the Bregman matrix divergences, which
are generalizations of the Bregman Vector Divergences (BVD). In
mathematics, specifically, statistics and information geometry, a
Bregman divergence is a measure of difference between two points,
defined in terms of a strictly convex function 𝜑(·).
Let𝜑(·)be a real-valued strictly convex function, BVD computes
the divergence between vectors 𝒙andyby taking the first-order
Taylor approximation of function 𝜑(·)as:
𝐷𝜑(𝒙,𝒚)=𝜑(𝒙)−𝜑(𝒚)−(𝒙−𝒚)⊤∇𝜑(𝒚). (7)
Note that Bregman divergence becomes a specific divergence/dis-
tance variant by the choice of the convex function 𝜑(·). Its geomet-
ric interpretation is shown in Figure 3. BVD encompasses several
common divergences/distances. For example, if 𝜑(𝒙)=𝒙⊤𝒙, the
resulting𝐷𝜑(𝒙,𝒚)=∥𝒙−𝒚∥2
2. When𝜑(𝒙)=Í
𝑖(𝑥𝑖log𝑥𝑖−𝑥𝑖),
𝐷𝜑(𝒙,𝒚)=Í
𝑖(𝑥𝑖log𝑥𝑖
𝑦𝑖−𝑥𝑖+𝑦𝑖)is the Kullback–Leibler diver-
gence (unnormalized). Therefore, BVD can be naturally extended
to positive semi-definite 𝑛×𝑛matrices, denoted by S𝑑+.
Definition 4.1 (Bregman Matrix Divergence 𝐷𝜙).LetX∈S𝑑+
andY∈S𝑑+be two PSD matrices. Given a strictly convex and
differentiable matrix function 𝜙:S𝑑+→R, the Bregman matrix
divergence is defined as follows:
𝐷𝜙(X,Y)=𝜙(X)−𝜙(Y)− tr ∇𝜙(Y)⊤(X−Y). (8)
Remark 4.2. The function 𝜙(·)is defined overS𝑑+. A well chosen
𝜙(·)can capture the properties of the spaces over which they are
defined. Similar to the BVD, if S𝑑+is the set of PSD matrices, X∈S𝑑+
with eigenvalues in 𝑑-dimensional simplex, i.e.,Tr(X)=1then
Figure 3: Geometric interpretation of Bregman divergence.
𝐷𝜙(X,Y)yields the von Neumann divergence (also know as the
Matrix KL divergence) [33] defined as:
𝐷𝜙𝑣𝑛(X,Y)=tr[X(logX−logY)]where𝜙(X)=XlogX.(9)
On the other hand, if 𝜙(X)=∥X⊤X∥2
2, Eq. (8) yields the Frob. norm:
𝐷𝜙𝑓𝑟𝑜𝑏(X,Y)=∥X−Y∥2
𝐹where𝜙(X)=∥X⊤X∥2
𝐹. (10)
Therefore, we generalize Eq. (4) with BMD as:
minL𝑎𝑙𝑖𝑔𝑛+𝜆
𝐷𝜙(𝚺1,I)+𝐷𝜙(𝚺2,I)
. (11)
Note that, by adopting 𝜙(X)=∥X⊤X∥2
𝐹, Eq. (11) results in the SSL
learning objective used by Barlow Twin [54] and CCA-SSG [55].
4.2 Regularization based on the LogDet Div.
As discussed in the previous section, the choice of 𝜙(·)in BMD is
crucial because many fundamental machine learning measures, e.g.,
the Frobenius norm and Kullback-Leibler divergence are special
cases of Bregman divergences. Below, we focus on a specific matrix
function, namely 𝜙(X)=−Log Det(X). This particular case of BMD
possesses desirable geometric properties that help preserve the
range space of a matrix, thereby preventing dimensional collapse.
The LogDet divergence is defined next.
Definition 4.3 (LogDet divergence). Let𝜙(X)=−Í
𝑖log𝜆𝑖(where
𝜆𝑖are singular values of X), or equivalently as 𝜙(X)=−Log Det X.
The resulting Bregman divergence over PSD is defined as:
𝐷𝜙𝑙𝑑(X,Y)=tr XY−1−Log Det XY−1−𝑑. (12)
By applying the LogDet divergence to the regularization term
L𝑟𝑒𝑔to implement the redundancy reduction principle, we obtain:
𝐷𝜙𝑙𝑑(𝚺,I)=tr(𝚺)−Log Det(𝚺)−𝑑. (13)
Given that we have two views, i.e.,𝚺1and𝚺2, with tr(Σ1)=
tr(𝚺2)=𝑑, we can castL𝑟𝑒𝑔in Eq. (4) as its LogDet divergence
variant. By computing 𝐷𝜙𝑙𝑑(𝚺1,I)+𝐷𝜙𝑙𝑑(Σ2,I), we obtain:
L𝑟𝑒𝑔=−Log Det(Σ1Σ2). (14)
Our objective enhanced by the LogDet decorrelation becomes:
L𝑠𝑠𝑙=L𝑎𝑙𝑖𝑔𝑛−𝜆Log Det(Σ1Σ2). (15)
In contrast to the Frobenius-based 𝐷𝜙𝑓𝑏(Σ,I), the LogDet-based
penalty𝐷𝜙𝑙𝑑(Σ,I)maintains the full rank of Σassuming Σis
ensured to be at least positive semi-definite by construction.
4341Geometric View of Soft Decorrelation in Self-Supervised Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
 
(a) LogDet div.
 
 (b) Von Neumann div.
 
 (c) Frobenius norm.
Figure 4: Visualization of three balls with different geometries. Plots (b), (c) and (d) show the ball which consists of the diagonal
matrices 𝚺=diag(𝝀)with the unit trace (simplex)Í
𝑖𝜆𝑖=1.
Table 1: Comparison of various geometries when optimizing
with an identity matrix. Note 𝐷(·,·)are already squared div.
Method 𝐷(X,Y) 𝐷(𝚺,I)
AIRMlog(X−1/2YX−1/2)2
𝐹tr log2(𝚺−1)
LERMlog(X)− log(Y)2
𝐹tr log2(𝚺)
LogDet tr XY−1−Log Det XY−1−𝑑 log(detΣ)
4.3 Other Bregman Divergences
To better understand the differences between the three Bregman
divergences (i.e ., the Frobenius norm, von Neumann, and LogDet
divergence), we visualize and compare the geometric shapes (balls)
associated with each divergence. Since 𝐷𝜙measures the distance
between arbitrary positive semi-definite (PSD) matrices and the
identity matrix, we depict the balls centered on the identity matrix
for each geometry:
Frobenius:
𝚺∈S𝑑
+:∥𝚺−I∥2
𝐹≤𝑟	
, (16)
von Neumann:
𝚺∈S𝑑
+:𝐷𝜙𝑣𝑛(𝚺,I)≤𝑟	
, (17)
LogDet:
𝚺∈S𝑑
+:𝐷𝜙𝑙𝑑(𝚺,I)≤𝑟	
. (18)
Figure 4c compares the balls for each divergence. We use a diag-
onal matrix 𝚺=diag(𝝀)with the unit trace, i.e.,tr(𝚺)=Í
𝑖𝜆𝑖=1.
Notice that the Euclidean ball is isotropic, while the balls corre-
sponding to the other divergences adapt their shape according to
the direction. This geometric property leads to an interesting ad-
vantage with respect to the gradient when optimizing the distance.
As shown in Table 2, both the LogDet divergence and the von
Neumann divergence generate larger gradients with respect to
small eigenvalues due to the logarithm of 𝜆. This is beneficial when
attempting to balance the eigenvalue spectrum during optimization,
particularly when dealing with power-law distributed eigenvalues
that approach null values. In contrast, when minimizing the reg-
ularization term based on the Frobenius norm, the optimization
process may prioritize reducing the larger eigenvalues of the covari-
ance matrix 𝚺while relatively neglecting the smaller eigenvalues
that may collapse to null values. This can result in an imbalanced
spectrum where small eigenvalues may remain small or they may
even collapse to null values.
Although the von Neumann divergence is similar to the LogDet
divergence, it suffers from high computational complexity andTable 2: First- and second-order derivatives of the BMD di-
vergence with respect to the eigenvalues 𝜆𝑖, respectively.
Geometries ∇𝜆𝐷(Σ,I),∇𝜆𝐷(Σ,I)
Frobenius norm 𝜆𝑖−1 1
Von Neumann log𝜆𝑖+1𝜆−1
𝑖
LogDet −𝜆−1
𝑖𝜆−2
𝑖
unstable backpropagation caused by using the singular value de-
composition (SVD). We explain this further in the following section.
4.4 Other Riemannian Metrics
As discussed in the previous section, LogDet enforces full rank due
to the properties of logarithm. Given that log(det(𝚺))=tr(log(𝚺)),
the Riemannian metrics, AIRM and LERM [ 26] can indeed reduce
to expressions similar to LogDet when computing 𝐷(𝚺,I)as shown
in the second column of Table 1. However, they have poor compu-
tational efficiency and numerically stability as explained below.
Computation efficiency. Although log(det(Σ))=tr(log(Σ)), the
way of computing LogDet is crucial. The LogDet divergence only re-
quires the calculation of determinants. This can be done efficiently
by the Cholesky factorization with1
3𝑑3flops, where 𝑑is the ma-
trix dimension. In contrast, Riemannian metrics AIRM and LERM
cannot be computed with determinant (slightly different expres-
sions to LogDet make it infeasible) require computing eigenvalues,
i.e.,log(Σ)=Ulog(𝝀)V⊤, which takes approximately 4𝑑3flops
for positive definite matrices. Avoiding generic computations of
𝐷(Σ,I)by avoiding the use SVD (in both forward and backward
passes) can significantly improve the efficiency, e.g., computing the
gradient of the LogDet divergence can be over 100 times faster than
computing the gradient of Riemannian metrics. This discrepancy
is demonstrated in Table 7. The computational advantage of the
LogDet divergence is particularly important for applications that
involve neural networks or algorithms that rely on the gradients of
divergences. In such cases, the faster computation of the LogDet di-
vergence can lead to significant improvements in training speed and
scalability, making it a preferred choice over Riemannian metrics.
Numerically Stability. When using Riemannian metrics, singu-
lar value decomposition (SVD) is implicitly employed in both the
forward and backward steps of the optimization process. As the co-
variance matrix 𝚺approaches the identity matrix Iduring training
4342KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
Table 3: Comparison of our proposed method with Barlow Twins (BT) and VICReg. The table highlights the differences in the
regularization terms, covariance matrix computation, gradient properties, and overall descriptions.
Method Reg. equation to minimize How to compute the covari-
ance matrixGradient∇𝜆𝑖Description CIFAR-
10CIFAR-
100Imagenet-
100
VICregÍ
j𝑚𝑎𝑥(0,𝛾−𝑣𝑎𝑟0.5(Z:,𝑗)) +
𝜈Í
𝑖≠𝑗Σ2
𝑖𝑗Σ=1
𝑁Z⊤Z(semi-definite,
e.g.,𝜆𝑖≥0)Combination of grad. via
ReLU and linearEncourages off-diagonals
ofΣto be close to 092.07 68.54 79.40
BT||Σ′−I||2
𝐹Σ′=1
𝑁Z⊤Z′(non-SPD,
e.g.,−∞≤𝜆𝑖≤∞)2(𝜆𝑖−1)Linear Encourages Σ′to be Ivia
Frob. norm92.10 69.18 80.16
LogDet tr(Σ)−Log Det(Σ)−𝑑 Σ=1
𝑁Z⊤Z(SPD, e.g.,
𝜆𝑖>0)1−1/𝜆𝑖exponential Encourages Σto be Ivia
LogDet div.93.51 70.53 80.38
or gets close to the dimensional collapse, the likelihood of encoun-
tering non-simple eigenvalues ( 𝜆𝑖≈𝜆𝑗:𝑖≠𝑗) increases. In such
cases, backpropagation through SVD and thus the derivative of
the Riemannian metric become undetermined. This can be prob-
lematic because non-simple eigenvalues introduce ambiguity in
the computation of the gradient, which is crucial for updating the
model parameters during optimization. The presence of non-simple
eigenvalues can lead to numerical instabilities and hinder the con-
vergence or crash the learning algorithm. In contrast, the LogDet
divergence, which is based on the determinant of the covariance
matrix, does not rely on SVD. As a result, it is less susceptible to the
issues arising from non-simple eigenvalues and provides a more
numerically stable approach for measuring the divergence.
4.5 Comparisons with Other SSL Methods
Below, we compare our method with typical self-supervised learn-
ing methods, including Barlow Twins (BT) and VICReg. Our so-
lution considers not only the covariance matrix but also the non-
Euclidean properties of the space of positive definite covariance
matrices and the corresponding multivariate Normal distributions.
In contrast, BT and VICReg rely on the Euclidean geometry, which
does not leverage the positive definite manifold of matrices. Our
idea is also motivated by the Redundancy Reduction Principle. Al-
though Barlow Twins (BT) and the covariance part of VICReg seem
inspired by similar principles, their solutions appear ineffective.
Both BT and VICReg serve as important baselines in our experi-
ments (see Table 4). Table 3 provides a comparison of the models.
Note that when (if) both views of BT converge to be perfectly
aligned (i.e .,Z=Z′) (otherwise 𝚺′is non-symmetric and likely
indefinite), BT can be seen as a special case of Bregman divergence
where𝜙(𝚺′)=𝑡𝑟(𝚺′). However, even then, the gradient of BT
equal 2(𝜆𝑖−1)remains linear and will fail to ensure 𝜆𝑖>0as
the penalty is finite (merely quadratic). In contrast, our LogDet
divergence yields exponential gradients 1−1/𝜆𝑖and an infinite
penalty. VICReg seems to have a rather complex operator for di-
agonal elements while still encouraging off-diagonal elements to
approach zero through the square loss, resulting again in a finite
penalty. VICReg also does not leverage the positive definite proper-
ties of covariances. The diagonal elements in VICReg are clipped (1
standard deviation), while the off-diagonal elements are squared
cross-correlation coefficients, which is a mix of different measures.
Both BT and VICReg fail to leverage the spectral properties of co-
variances and fail to form the actual positive definite covariances.
Our approach can encompass special divergence cases such as BT.
We also explain why BT cannot fully prevent dimensional collapse.Only a positive definite covariance corresponds to the multivariate
Normal distribution (otherwise the distribution collapses).
5 THEORETICAL ANALYSIS
5.1 LogDet Divergence for Rank-deficient
Matrices
One key property that we demonstrate and exploit is that the
LogDet has a range-space preserving property. Specifically, the
LogDet divergence between two matrices is finite if and only if
their range spaces are identical. This property implies that opti-
mizing𝐷𝜙𝑙𝑑(Σ,I)forces Σto be full rank (in case Σmoves toward
positive semi-definite matrix), preventing dimensional collapse.
Lemma 5.1 (Kulis et al. [33]).LogDet divergence 𝐷𝜙𝑙𝑑(X,Y)is finite
iffrange(X)=range(Y), which implies thatL𝐿𝑜𝑔𝐷𝑒𝑡 =𝐷𝜙𝑙𝑑(Σ1,I)+
𝐷𝜙𝑙𝑑(Σ2,I)=∞ifrank(Σ1)<rank(I)=𝑑∨rank(Σ2)<
rank(I)=𝑑.
Implicit rank preservation. LetΣ=Udiag(𝜆1,···𝜆𝑑)U⊤be the
eigenvalue decomposition of Σ, then we could turn Eq. (12) into:
𝐷𝜙𝑙𝑑(Σ,I)=𝑑∑︁
𝑖=1(𝜆𝑖−log𝜆𝑖−1). (19)
In contrast to the Frobenius norm, i.e.𝐷𝜙𝑓𝑏(Σ,I)=Í𝑑
𝑖=1(𝜆𝑖−1)2,
we observe that:
lim
𝜆𝑖→0h
𝜆𝑖−log𝜆𝑖−1+∑︁
𝑗≠𝑖 𝜆𝑗−log𝜆𝑗−1i
=+∞.(20)
Due to the behavior of the LogDet divergence, minimizing 𝐷𝜙𝑙𝑑(𝚺,I)
implicitly enforces the full rank of 𝚺. Thus, if Σis rank-deficient, i.e.,
some eigenvalues are zero (∃𝜆𝑖=0), the expressionL𝑟𝑒𝑔would
go to the infinity. Thus, the optimization process will naturally
steer away from infinite penalties toward full rank 𝚺, avoiding the
dimensional collapse.
5.2 Spectrum View of the LogDet Divergence
Section 4.1 illustrates how the BMD relates to the geometrical
properties of the matrices as represented by the eigenvalues (the
spectrum of the matrix). We further show that optimizing 𝐷𝜙𝑙𝑑(𝚺,I)
increases the entropy of the spectrum, also known as Effective Rank
(ERank), which measures the effective dimension of embedding
output of the encoder. The higher the effective rank is the lower
the degree of dimensional collapse is.
4343Geometric View of Soft Decorrelation in Self-Supervised Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Main comparison on CIFAR and ImageNet-100, All methods are trained 1000 epochs on CIFAR-10/100 (batch size 256)
and 400 epochs on ImageNet100 (batch size 128). Note that some results are directly quoted from solo-learn [17] .
Dataset CIFAR-10 CIFAR-100 ImageNet-100
Method Acc.@1 Acc.@5 Acc.@1 Acc.@5 Acc.@1 Acc.@5
qSimCLR 90.74 99.75 65.39 89.57 77.64 94.06
qMoCo v3 93.20 99.31 68.83 90.57 80.36 95.18
rSimSiam 90.51 99.72 65.86 89.62 74.54 93.12
rBYOL 92.58 99.79 66.38 91.96 80.16 95.14
♣Zero-CL 90.81 99.76 70.33 92.05 79.26 94.98
♣W-MSE 88.78 99.68 61.29 87.26 69.06 91.22
♠VICReg 92.07 99.71 68.54 90.83 79.40 95.06
♠Barlow Twin 92.10 99.73 69.18 91.91 80.16 95.14
LogDet 93.51 99.77 70.53 92.35 80.38 95.45
Definition 5.2 (Effective Rank). Let matrix 𝑿∈R𝑁×𝑑with 𝑿=
𝑼Λ𝑽⊤as its SVD, where Λis a diagonal matrix with singular values
𝜆1≥···≥𝜆𝑟≥0with𝑟=min(𝑑,𝑁). The distribution of singu-
lar values is defined as the normalized form 𝑝𝑖=𝜆𝑖/Í𝑟
𝑘=1|𝜆𝑘|.
The effective rank of the matrix 𝑿, is defined as ERank(𝑿)=
exp(𝐻(𝑝1,𝑝2,...,𝑝𝑟)), where𝐻(𝑝1,𝑝2,...,𝑝𝑟)is the Shannon en-
tropy given by 𝐻(𝑝1,𝑝2,···,𝑝𝑟)=−Í𝑟
𝑘=1𝑝𝑘log𝑝𝑘.
Lemma 5.3. LetΣ=Udiag(𝜆2
1,...,𝜆2
𝑑)U⊤with Tr(Σ)=𝑐. The
optimization problem argmin{𝜆0,...,𝜆𝑑}𝐷𝜙𝑙𝑑(𝚺,I)is optimal when
Λ∗=diag(𝑐
𝑑,···,𝑐
𝑑). Proof can be found in Appendix A.2.
As the ERank reaches its maximum when the isotropic spec-
trum is achieved (equal eigenvalues as shown in Lemma 5.3), the
above analysis suggests that minimizing L𝑎𝑙𝑖𝑔𝑛+𝜆[𝐷𝜙𝑙𝑑(𝚺1,I)+
𝐷𝜙𝑙𝑑(𝚺2,I)]increases ERank for both views, where 𝜆≥0is chosen
experimentally to prevent the dimensional collapse.
5.3 Convergence Analysis via Conditional
Number
This section provides a convergence analysis of the proposed method
based on the LogDet divergence, focusing on the condition number
of the covariance matrix and its connection to 𝐷𝜙𝑙𝑑(𝚺,I). The con-
dition number is an important metric for evaluating the numerical
stability and convergence behavior of a learning system, e.g., a large
condition number can make it difficult for the learning system to
converge.
Minimizing 𝐷𝜙𝑙𝑑(Σ,I)Reduces the Condition Number of Σ.
Numerical stability is closely related to the condition number. If
the spectrum of a matrix is dominated by the leading eigenvalue,
the matrix is ill-conditioned and has a large condition number. Let
{𝜆𝑖}𝑑
𝑖=0be the eigenvalues of the matrix Σ. The condition number is
defined as cond(Σ)=𝜆max
𝜆min. The ratio between the largest eigenvalue
and the smallest eigenvalue quantifies the flatness (isotropy) of the
spectrum. Below we show that our proposed method minimizes
the upper bound of cond(Σ).
Lemma 5.4. The condition number of Σ,cond(Σ)=𝜆max
𝜆min, is upper
bounded by 𝐷𝜙𝑙𝑑(Σ,I)as:
cond(Σ)≤ 4 exp 𝐷𝜙𝑙𝑑(𝚺,I). (21)
□Proof of Lemma 5.4 is in Appendix A.4.Lemma 5.4 proves that the condition number of Σis upper
bounded by the LogDet divergence. This implies that minimiz-
ing the LogDet divergence can effectively control the upper bound
of the condition number. Thus, a lower condition number indicates
a better-conditioned matrix, which leads to faster convergence and
improved numerical stability.
6 EXPERIMENTS
In this section, we present the results of our experiments, which
demonstrate the superior performance of our proposed model in
both image and graph domains.
6.1 Main Results
To assess the effectiveness of our novel geometric regularization
technique for self-supervised learning, below we conduct a series
of experiments on datasets from various modalities.
Image experiment setting. Our research primarily focuses on im-
age classification tasks utilizing datasets such as CIFAR-10/100 [ 32]
and ImageNet-100/1k [ 18] using the solo-learn package [ 17]. To
ensure fairness in our comparisons, we establish batch sizes of 128
for ImageNet-100, and 256 for CIFAR-10/100. We select representa-
tive SSL models for visual representation learning. These include
qContrastive SSL, i.e., SimCLR [ 7], MoCo v3 [ 9].rNon-contrastive
SSL, i.e., BYOL [ 24], SimSiam [ 10],♣non-contrastive SSL by whiten-
ing, i.e., W-MSE [ 20], Zero-CL [ 57] and♠non-contrastive SSL by
decorrelation i.e., Barlow Twin [ 54], VICReg [ 3]. Among these
methods, SimSiam employs techniques like stop gradient and asym-
metric branches to avoid dimensional collapse. SimCLR, mitigates
this issue by introducing negative pairs, while W-MSE focuses on
whitening the feature matrix. Additionally, VICReg and Barlow
Twins employ decorrelation strategies.
Results. Table 4 presents the main comparison of the proposed
method with the state-of-the-art self-supervised learning approaches
on three image classification datasets: CIFAR-10, CIFAR-100, and
ImageNet-100. The results highlight the effectiveness of the pro-
posed regularization based on the LogDet divergence in improving
the performance of self-supervised learning. The proposed method
consistently outperforms all baselines across the three datasets,
demonstrating its superiority in learning high-quality representa-
tions. This aligns with the claim that the novel loss functions based
on Bregman Matrix Divergence (BMD) exhibit better performance
4344KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
Table 5: Performance comparison to unsupervised methods. The accuracy with standard deviation is based on 5runs for all
methods. The second column shows the information used by the method, where Xand Aindicate input types: node features
and adjacency matrix, respectively. ‘OOM’ means out of memory while running on NVIDIA RTX3090 (24GB memory).
Statistic Cora CiteSeer PubMed CoraFull CS Physics Computers Photo
Feature X 60.19±0.00 61.70±0.00 73.90±0.00 40.06±1.07 88.14±0.26 87.49±1.16 67.48±1.48 59.52±3.60
PCA X 59.90±0.00 60.00±0.00 74.00±0.00 38.46±1.13 88.59±0.29 87.66±1.05 72.65±1.43 57.45±4.38
SVD X 60.21±0.19 60.80±0.26 73.79±0.29 38.64±1.11 88.55±0.31 87.98±1.10 68.17±1.39 60.98±3.58
isomap X 44.60±0.00 18.90±0.00 63.90±0.00 4.21±0.25 73.68±1.25 82.84±0.81 72.66±1.38 44.00±6.43
LLE X 36.70±0.00 21.80±0.00 54.00±0.00 5.70±0.38 72.23±1.57 81.35±1.59 45.29±1.31 35.37±1.82
DeepWalk X 73.76±0.26 51.80±0.62 71.28±1.07 51.65±0.83 83.25±0.54 88.08±1.45 86.47±1.55 76.58±1.09
Node2vec X 72.54±1.12 49.37±1.53 68.70±0.96 50.35±0.74 82.12±1.09 86.77±0.83 85.15±1.32 75.67±1.98
DeepWalk+F X,A 77.62±0.27 66.96±0.30 71.84±1.15 54.63±0.74 83.34±0.53 88.15±1.45 86.49±1.55 65.97±3.68
Node2vec X,A 76.84±0.25 66.75±0.74 69.12±0.96 54.00±0.17 82.20±1.09 86.86±0.80 85.15±1.33 65.01±2.91
GAE X,A 74.30±1.42 59.69±3.29 80.08±0.70 42.54±2.69 88.88±0.83 91.01±0.84 37.72±9.01 48.72±5.28
VGAE X,A 76.42±1.26 60.37±1.40 77.75±0.77 53.69±1.32 88.66±1.04 90.33±1.77 49.09±5.95 48.33±1.74
DGI X,A 82.11±0.25 70.15±1.10 79.06±0.51 53.89±1.38 91.22±0.48 92.12±1.29 79.62±3.31 70.65±1.72
GRACE X,A 80.36±0.51 68.04±1.06 OOM 53.95 ±0.11 90.04±0.11 OOM 81.94±0.48 70.38±0.46
CCA-SSG X,A 84.21±0.41 73.11±0.32 84.64±0.42 OOM OOM OOM 52.36±5.22 OOM
G-BT X,A 81.20±0.78 70.50±0.36 OOM OOM OOM OOM 52.36±5.22 OOM
LogDet X,A 86.12±0.75 74.66±0.49 85.69±0.42 59.70±0.59 92.64±0.40 94.93±0.07 82.24±0.71 90.68±0.31
Table 6: Performance comparison of different divergence/distance measures between matrices in the self-supervised learning
framework. The best performance for each dataset is shown in boldface.
Type Measure 𝐷𝜙(X,Y) 𝐷𝜙(Σ,I) Cora CiteSeer Am-Comp.
Matrix normSpectral ∥X−Y∥2
2∥Σ−I∥2
278.12±0.35 65.12±0.65 85.12±0.55
Nuclear ∥X−Y∥∗ ∥Σ−I∥∗ 80.12±0.35 66.82±0.15 85.12±0.55
Frobenius ∥X−Y∥2
𝐹∥Σ−I∥2
𝐹84.50±0.25 73.32±0.40 87.46±0.35
Riemannian metricAIRM ∥log(X−1/2YX−1/2)∥2
𝐹−tr log2(Σ−1)84.82±0.15 73.78±0.32 88.01±0.25
LERM ∥log(X)−log(Y)∥2
𝐹−tr log2(Σ)84.32±0.19 73.78±0.62 88.21±0.35
Bregman divergencevon Neumann tr[X(log(X)−log(Y))] tr Σlog(Σ)85.32±0.25 73.58±0.47 87.56±0.35
LogDet tr XY−1−Log Det XY−1−𝑑−tr log(Σ)86.12±0.75 74.12±0.65 88.72±0.55
compared to the soft decorrelation and other baseline techniques.
When compared to Barlow Twins, which uses the Frobenius norm
for regularization, the proposed method achieves higher accuracy
on all datasets. For instance, on CIFAR-100 and ImageNet-100, our
method surpasses Barlow Twins in top-1 accuracy by 1.35% and
0.22%, respectively. This finding supports that the LogDet diver-
gence is more effective in preventing the dimensional collapse than
the soft decorrelation, leading to improved representation learning.
The proposed method also outperforms other state-of-the-art ap-
proaches, such as SimCLR, MoCo v3, SimSiam, BYOL, and VICReg,
across all datasets. This further highlights the effectiveness of the
regularization based on the LogDet divergence in self-supervised
learning.
Graph experiment setting. We conduct experiments on eight real-
world datasets, encompassing citation networks [ 5,53] (Cora, Cite-
Seer, PubMed, CoraFull), co-authorship networks [ 43] (Physics, CS),
and Amazon co-purchase networks [ 37] (Photo, Computers). For
baseline comparisons, we assessed various graph self-supervised
techniques, including DGI [ 48], GRACE [ 69], G-BT [ 4], and CCA-
SSG [ 55]. In terms of evaluation, we adopted the linear evaluation
scheme as proposed by [ 48,69]. This approach involves initially
training models in an unsupervised manner and subsequently out-
putting node representations for assessment using a logistic re-
gression classifier. To maintain consistency, we utilize the identicalrandom train/validation/test split employed by [ 21]. To ensure a
fair comparison across models, we employ both the same logistic re-
gression classifier and the same data split. The summarized results
can be found in Table 5.
Results. Table 5 presents a comprehensive performance compari-
son of the proposed method against various unsupervised learning
approaches on eight real-world datasets spanning citation networks,
co-authorship networks, and co-purchase networks. The results
demonstrate the superior performance of the proposed method
which consistently outperforms all baseline methods across the
eight datasets, achieving the state-of-the-art performance. This
aligns with the claim that the novel loss functions based on the
Bregman Matrix Divergence (BMD) exhibit superior performance
compared to existing techniques. Compared to DGI, a representative
unsupervised method, and GRACE, the proposed method achieves
an average improvement of 5.1% and 5.9%, respectively. This sub-
stantial improvement highlights the effectiveness of the proposed
BMD-based regularization in learning better representations. Our
method surpasses the performance of G-BT and CCA-SSG, which
utilize the Frobenius norm for matrix distance quantification. This
finding supports the claim that the LogDet divergence, a specific
loss function in the BMD family, is more effective than the soft
decorrelation (which is based on the Frobenius norm) in preventing
4345Geometric View of Soft Decorrelation in Self-Supervised Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
EpochsEffective Rank
(a) Cora
EpochsEffective Rank (b) CiteSeer
EpochsEffective Rank (c) Computer
EpochsEffective RankLogDet div.vNdiv.Forb. normSpec.normNuclear norm (d) Different divergences/dist.
Figure 5: The effective rank of the learned matrix with dimension 256. Fig. 5a, 5b and 5c, show the result produced from G-BT,
CCA-SSG and GRACE) in three datasets. Fig. (5d) shows the result of different divergences/distances.
the dimensional collapse and learning more informative represen-
tations.
6.2 Dimensional Collapse Analysis
The proposed method is designed to force the feature covariance
matrix to be full rank, thus preventing the dimensional collapse. A
higher effective rank of the feature (covariance) matrix indicates
a more evenly distributed set of eigenvalues. Consequently, our
objective is to assess whether our model attains a superior effective
rank compared to other conventional approaches. We conduct a
comparative analysis of our model against various baselines with
regard to the effective rank. In this section, we show that the pro-
posed method can effectively avoid the dimensional collapse by
consistently improving the effective rank of the learned feature
(covariance) matrix.
Setting. A well-known measure of the effective feature dimen-
sionality is the Effective Rank (ERank) of the feature covariance
matrix [ 42]. Specifically, the effective rank is the exponential of
the negative of the Shannon entropy of the ℓ1-norm normalized
eigenvalues (a probability distribution). Thus, the effective rank
ofΣmeasures the degree of dimensional collapse. We experiment
on three real-world datasets, including citation network, i.e., Cora,
CiteSeer, PubMed [ 5,53]. We compare the proposed method in two
categories. The first category, inclusive of GRACE [ 69], adopts the
L𝑢𝑛𝑖. The second category, inclusive of G-BT [ 4] and CCA-SSG [ 55],
usesL𝑑𝑒𝑐.
Results. Figure 5 shows the change of the effective rank of Σ. We
find that after the initial stage, the ERank of all methods consistently
improves along the training process. The blue curve (our model)
is consistently higher than other curves, which verifies that the
proposed method achieves a higher effective rank.
6.3 Alternative Divergences/Distances
There are numerous options for defining the divergence/distance
when measuring the divergence/distance between covariances. Our
objective is to examine the advantages of the proposed method
compared to other strategies. We delve into distinct geometric
approaches utilizing the matrix norm and matrix divergence. We
experiment with other ways of measuring the divergence/distance
between matrices to show that our proposed method is superior to
other geometry.Results. Figure 5 shows the change in the effective rank of Σ. We
observe the following phenomena: after the initial stage, the ef-
fective rank of all methods consistently improves throughout the
training process, with the blue line representing our model con-
sistently surpassing the others, which confirms that the proposed
method achieves a higher effective rank.
Table 6 summarizes various divergences/distances and their re-
lationship with the spectrum when optimizing covariance to ap-
proach an identity matrix. We observe that minimizing the Spec-
tral norm∥Σ−I∥2=𝜆max−1only minimizes the largest eigen-
value. Consequently, it often leads to subpar results across the three
datasets, with the model tending to collapse (as observed in the
decrease of the effective rank of the Spectral norm in Figure 5d).
Minimizing the nuclear norm minimizes the sum of eigenvalues
(i.e.,∥Σ−I∥∗=Í
𝑖(𝜆𝑖−1), which has no impact on the effective
rank, leading to inferior results. Minimizing the Frobenius norm
minimizes the sum of squared eigenvaluesÍ
𝑖(𝜆𝑖−1)2and typi-
cally outperforms both the Spectral norm and the Nuclear norm.
Matrix divergences generally appear to perform better than the
matrix norms. Notably, the LogDet divergence 𝐷𝜙𝑙𝑑(Σ,I)and the
von Neumann divergence 𝐷𝜙𝑣𝑛(Σ,I)involve the logarithm of eigen-
values. As discussed in Section 5, the above expressions enjoy a
range-space preserving property, resulting in large gradients with
respect to small eigenvalues. This tendency promotes higher ERank,
leading them to outperform other measures.
7 CONCLUSIONS
This study considers the role of the soft decorrelation on the dimen-
sional collapse of self-supervised learning. We reformulate the soft
decorrelation with Bregman divergences generalizing it beyond the
Frobenius norm. Specifically, we employ the LogDet divergence and
encourage the covariance matrices per view to approach identity
matrices. Theoretical analysis demonstrates that employing the
LogDet divergence promotes full rank of covariances, thus mitigat-
ing the risk of dimensional collapse. We also show that minimizing
our LogDet based regularization is equivalent to maximizing the
effective rank. Empirical results substantiate the superiority of our
approach over existing methods.
ACKNOWLEDGMENTS
The research presented in this paper was partially supported by the
Research Grants Council of the Hong Kong Special Administrative
Region, China (CUHK 14222922, RGC GRF 2151185). HZ and PK
were supported by the CSIRO’s Science Digital.
4346KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
REFERENCES
[1]Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. 2019. Implicit Regulariza-
tion in Deep Matrix Factorization. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 7411–7422. https://proceedings.neurips.cc/paper/2019/
hash/c0c783b5fc0d7d808f1d14a6e9c8280d-Abstract.html
[2]Philip Bachman, R. Devon Hjelm, and William Buchwalter. 2019. Learning Repre-
sentations by Maximizing Mutual Information Across Views. In Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
Emily B. Fox, and Roman Garnett (Eds.). 15509–15519. https://proceedings.
neurips.cc/paper/2019/hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html
[3]Adrien Bardes, Jean Ponce, and Yann LeCun. 2022. VICReg: Variance-Invariance-
Covariance Regularization for Self-Supervised Learning. In The Tenth Interna-
tional Conference on Learning Representations, ICLR 2022, Virtual Event, April
25-29, 2022. OpenReview.net. https://openreview.net/forum?id=xm6YD62D1Ub
[4]Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. 2022. Graph Bar-
low Twins: A self-supervised representation learning framework for graphs.
Knowledge-Based Systems 256 (2022), 109631.
[5]Aleksandar Bojchevski and Stephan Günnemann. 2018. Deep Gaussian Em-
bedding of Graphs: Unsupervised Inductive Learning via Ranking. In 6th In-
ternational Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
https://openreview.net/forum?id=r1ZdKJ-0W
[6]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin. 2020. Unsupervised Learning of Visual Features by Contrasting
Cluster Assignments. In Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.
cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html
[7]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) .
PMLR, 1597–1607. http://proceedings.mlr.press/v119/chen20j.html
[8]Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E.
Hinton. 2020. Big Self-Supervised Models are Strong Semi-Supervised Learners.
InAdvances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/
hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html
[9]Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved baselines
with momentum contrastive learning. ArXiv preprint abs/2003.04297 (2020).
https://arxiv.org/abs/2003.04297
[10] Xinlei Chen and Kaiming He. 2021. Exploring Simple Siamese Representation
Learning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 15750–15758.
https://doi.org/10.1109/CVPR46437.2021.01549
[11] Yankai Chen, Yixiang Fang, Qiongyan Wang, Xin Cao, and Irwin King. 2024. Deep
Structural Knowledge Exploitation and Synergy for Estimating Node Importance
Value on Heterogeneous Information Networks. In AAAI. 8302–8310.
[12] Yankai Chen, Yixiang Fang, Yifei Zhang, and Irwin King. 2023. Bipartite graph
convolutional hashing for effective and efficient top-n search in hamming space.
InProceedings of the ACM Web Conference 2023. 3164–3172.
[13] Yankai Chen, Huifeng Guo, Yingxue Zhang, Chen Ma, Ruiming Tang, Jingjie
Li, and Irwin King. 2022. Learning binarized graph representations with multi-
faceted quantization reinforcement for top-k recommendation. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
168–178.
[14] Yankai Chen, Yifei Zhang, Huifeng Guo, Ruiming Tang, and Irwin King. 2022. An
Effective Post-training Embedding Binarization Approach for Fast Online Top-K
Passage Matching. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter
of the Association for Computational Linguistics and the 12th International Joint
Conference on Natural Language Processing (Volume 2: Short Papers). Association
for Computational Linguistics, Online only, 102–108. https://aclanthology.org/
2022.aacl-short.14
[15] Yankai Chen, Yifei Zhang, Menglin Yang, Zixing Song, Chen Ma, and Irwin
King. 2023. WSFE: wasserstein sub-graph feature encoder for effective user
segmentation in collaborative filtering. In Proceedings of the 46th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
2521–2525.
[16] Anoop Cherian, Suvrit Sra, Arindam Banerjee, and Nikolaos Papanikolopoulos.
2012. Jensen-bregman logdet divergence with application to efficient similaritysearch for covariance matrices. IEEE transactions on pattern analysis and machine
intelligence 35, 9 (2012), 2161–2174.
[17] Victor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and
Elisa Ricci. 2022. solo-learn: A Library of Self-supervised Methods for Vi-
sual Representation Learning. J. Mach. Learn. Res. 23 (2022), 56:1–56:6. http:
//jmlr.org/papers/v23/21-1155.html
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. Im-
ageNet: A large-scale hierarchical image database. In 2009 IEEE Computer So-
ciety Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-
25 June 2009, Miami, Florida, USA. IEEE Computer Society, 248–255. https:
//doi.org/10.1109/CVPR.2009.5206848
[19] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. 2015. Unsupervised Visual
Representation Learning by Context Prediction. In 2015 IEEE International Con-
ference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. IEEE
Computer Society, 1422–1430. https://doi.org/10.1109/ICCV.2015.167
[20] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. 2021.
Whitening for Self-Supervised Representation Learning. In Proceedings of the
38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina Meila
and Tong Zhang (Eds.). PMLR, 3015–3024. http://proceedings.mlr.press/v139/
ermolov21a.html
[21] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with
PyTorch Geometric. ArXiv preprint abs/1903.02428 (2019). https://arxiv.org/abs/
1903.02428
[22] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018. Unsupervised
Representation Learning by Predicting Image Rotations. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https:
//openreview.net/forum?id=S1v4N2l0-
[23] Gene H Golub and Charles F Van Loan. 2013. Matrix computations. JHU press.
[24] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H.
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos,
and Michal Valko. 2020. Bootstrap Your Own Latent - A New Approach to Self-
Supervised Learning. In Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.
cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html
[25] Maryam Haghighat, Peyman Moghadam, Shaheer Mohamed, and Piotr Koniusz.
2024. Pre-training with Random Orthogonal Projection Image Modeling. In
International Conference on Learning Representations (ICLR).
[26] Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. 2017. Dimensionality
reduction on SPD manifolds: The emergence of geometry-aware methods. IEEE
transactions on pattern analysis and machine intelligence 40, 1 (2017), 48–62.
[27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.
Momentum Contrast for Unsupervised Visual Representation Learning. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020. IEEE, 9726–9735. https://doi.org/10.1109/
CVPR42600.2020.00975
[28] Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao.
2021. On Feature Decorrelation in Self-Supervised Learning. In 2021 IEEE/CVF
International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,
October 10-17, 2021. IEEE, 9578–9588. https://doi.org/10.1109/ICCV48922.2021.
00946
[29] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. 2022. Understanding
Dimensional Collapse in Contrastive Self-supervised Learning. In The Tenth In-
ternational Conference on Learning Representations, ICLR 2022, Virtual Event, April
25-29, 2022. OpenReview.net. https://openreview.net/forum?id=YevsQ05DEN7
[30] Piotr Koniusz, Fei Yan, Philippe-Henri Gosselin, and Krystian Mikolajczyk. 2013.
Higher-order occurrence pooling on mid-and low-level features: Visual concept
detection. Tech. Report (2013).
[31] Piotr Koniusz and Hongguang Zhang. 2022. Power Normalizations in Fine-
Grained Image, Few-Shot Image and Graph Classification. IEEE Trans. Pattern
Anal. Mach. Intell. 44, 2 (2022), 591–609.
[32] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[33] Brian Kulis, Mátyás A Sustik, and Inderjit S Dhillon. 2009. Low-Rank Kernel
Learning with Bregman Matrix Divergences. Journal of Machine Learning Re-
search 10, 2 (2009).
[34] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. 2016. Learning
representations for automatic colorization. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceed-
ings, Part IV 14. Springer, 577–593.
[35] Kanglin Liu, Guoping Qiu, Wenming Tang, and Fei Zhou. 2019. Spectral Regular-
ization for Combating Mode Collapse in GANs. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -
4347Geometric View of Soft Decorrelation in Self-Supervised Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
November 2, 2019. IEEE, 6381–6389. https://doi.org/10.1109/ICCV.2019.00648
[36] Yueen Ma, Zixing Song, Xuming Hu, Jingjing Li, Yifei Zhang, and Irwin King. 2023.
Graph component contrastive learning for concept relatedness estimation. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 13362–13370.
[37] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel.
2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings
of the 38th International ACM SIGIR Conference on Research and Development in
Information Retrieval, Santiago, Chile, August 9-13, 2015, Ricardo Baeza-Yates,
Mounia Lalmas, Alistair Moffat, and Berthier A. Ribeiro-Neto (Eds.). ACM, 43–52.
https://doi.org/10.1145/2766462.2767755
[38] Mehdi Noroozi and Paolo Favaro. 2016. Unsupervised learning of visual represen-
tations by solving jigsaw puzzles. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI .
Springer, 69–84.
[39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. ArXiv preprint abs/1807.03748 (2018). https:
//arxiv.org/abs/1807.03748
[40] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A.
Efros. 2016. Context Encoders: Feature Learning by Inpainting. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,
NV, USA, June 27-30, 2016. IEEE Computer Society, 2536–2544. https://doi.org/
10.1109/CVPR.2016.278
[41] Arian Prabowo, Hao Xue, Wei Shao, Piotr Koniusz, and Flora D. Salim. 2023.
Traffic forecasting on new roads using spatial contrastive pre-training (SCPT).
Data Min. Knowl. Discov. 38, 3 (sep 2023), 913–937. https://doi.org/10.1007/s10618-
023-00982-0
[42] Olivier Roy and Martin Vetterli. 2007. The effective rank: A measure of effective
dimensionality. In 2007 15th European signal processing conference. IEEE, 606–610.
[43] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. ArXiv preprint
abs/1811.05868 (2018). https://arxiv.org/abs/1811.05868
[44] Zixing Song, Ziqiao Meng, Yifei Zhang, and Irwin King. 2021. Semi-supervised
multi-label learning for graph-structured data. In Proceedings of the 30th ACM
International Conference on Information & Knowledge Management. 1723–1733.
[45] Zixing Song, Yifei Zhang, and Irwin King. 2022. Towards an optimal asymmetric
graph structure for robust semi-supervised node classification. In Proceedings
of the 28th ACM SIGKDD conference on knowledge discovery and data mining.
1656–1665.
[46] Zixing Song, Yifei Zhang, and Irwin King. 2024. No Change, No Gain: Empow-
ering Graph Neural Networks with Expected Model Change Maximization for
Active Learning. Advances in Neural Information Processing Systems 36 (2024).
[47] Zixing Song, Yifei Zhang, and Irwin King. 2024. Optimal Block-wise Asymmetric
Graph Construction for Graph-based Semi-supervised Learning. Advances in
Neural Information Processing Systems 36 (2024).
[48] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio,
and R. Devon Hjelm. 2019. Deep Graph Infomax. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net. https://openreview.net/forum?id=rklz9iAcKQ
[49] Feng Wang and Huaping Liu. 2021. Understanding the Behaviour of Contrastive
Loss. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,
virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 2495–2504. https:
//doi.org/10.1109/CVPR46437.2021.00252
[50] Tongzhou Wang and Phillip Isola. 2020. Understanding Contrastive Represen-
tation Learning through Alignment and Uniformity on the Hypersphere. In
Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) .
PMLR, 9929–9939. http://proceedings.mlr.press/v119/wang20k.html
[51] Yaozu Wu, Yankai Chen, Zhishuai Yin, Weiping Ding, and Irwin King. 2023.
A survey on graph embedding techniques for biomedical data: Methods and
applications. Information Fusion 100 (2023), 101909.
[52] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao,
Qi Dai, and Han Hu. 2022. Simmim: A simple framework for masked image
modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 9653–9663.
[53] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. 2016. Revisiting
Semi-Supervised Learning with Graph Embeddings. In Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY,USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48),
Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 40–48. http:
//proceedings.mlr.press/v48/yanga16.html
[54] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. 2021. Barlow
Twins: Self-Supervised Learning via Redundancy Reduction. In Proceedings of the
38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina Meila
and Tong Zhang (Eds.). PMLR, 12310–12320. http://proceedings.mlr.press/v139/
zbontar21a.html
[55] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S. Yu. 2021. From
Canonical Correlation Analysis to Self-supervised Graph Neural Networks. In
Advances in Neural Information Processing Systems 34: Annual Conference on
Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,
virtual, Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,
and Jennifer Wortman Vaughan (Eds.). 76–89. https://proceedings.neurips.cc/
paper/2021/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html
[56] Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xiaokang Yang. 2022. Zero-
CL: Instance and Feature decorrelation for negative-free symmetric contrastive
learning. In The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/
forum?id=RAW9tCdVxLj
[57] Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xiaokang Yang. 2022. Zero-
CL: Instance and Feature decorrelation for negative-free symmetric contrastive
learning. In The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/
forum?id=RAW9tCdVxLj
[58] Xinni Zhang, Yankai Chen, Chenhao Ma, Yixiang Fang, and Irwin King. 2024.
Influential Exemplar Replay for Incremental Learning in Recommender Systems.
InAAAI, Vol. 38. 9368–9376.
[59] Yifei Zhang, Yankai Chen, Zixing Song, and Irwin King. 2023. Contrastive
cross-scale graph knowledge synergy. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3422–3433.
[60] Yifei Zhang and Hao Zhu. 2019. Doc2hash: Learning Discrete Latent variables for
Documents Retrieval. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers). Association for Computational Linguis-
tics, Minneapolis, Minnesota, 2235–2240. https://doi.org/10.18653/v1/N19-1232
[61] Yifei Zhang and Hao Zhu. 2020. Discrete Wasserstein Autoencoders for Document
Retrieval. In 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020. IEEE, 8159–8163. https:
//doi.org/10.1109/ICASSP40776.2020.9053129
[62] Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. 2022. Graph-
adaptive rectified linear unit for graph neural networks. In Proceedings of the
ACM Web Conference 2022. 1331–1339.
[63] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. 2022. COSTA:
Covariance-Preserving Feature Augmentation for Graph Contrastive Learning.
InKDD.
[64] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. 2023. Spectral
feature augmentation for graph contrastive learning and beyond. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 37. 11289–11297.
[65] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, Irwin King, et al .2023. Miti-
gating the popularity bias of graph collaborative filtering: A dimensional collapse
perspective. Advances in Neural Information Processing Systems 36 (2023), 67533–
67550.
[66] Hao Zhu and Piotr Koniusz. 2021. REFINE: Random RangE FInder for Network
Embedding (CIKM ’21). Association for Computing Machinery, New York, NY,
USA, 3682–3686. https://doi.org/10.1145/3459637.3482168
[67] Hao Zhu and Piotr Koniusz. 2022. Generalized Laplacian Eigenmaps. In Advances
in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HjicdpP-
Nth
[68] Hao Zhu, Ke Sun, and Piotr Koniusz. 2021. Contrastive Laplacian Eigenmaps. In
Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan (Eds.). https://openreview.net/forum?id=iLn-
bhP-kKH
[69] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep graph contrastive representation learning. ArXiv preprint abs/2006.04131
(2020). https://arxiv.org/abs/2006.04131
4348KDD ’24, August 25–29, 2024, Barcelona, Spain Yifei Zhang et al.
A APPENDICES
A.1 Notations
Notations. A neural network-based encoder is denoted as 𝑓𝜽(·),
and its learnable parameters as 𝜽. A neural network encoder 𝑓𝜽(·)
may be an image encoder (image input X) or a graph encoder (graph
input(X,A)). For graphs, X∈R𝑛×𝑑𝑥is the features matrix (i.e ., the
𝑖throw of Xis the feature vector x𝑖of𝑖thsample and A∈{0,1}𝑛×𝑛
denotes the adjacency matrix of a graph, i.e., the(𝑖,𝑗)thentry in A
is 1 if there is an edge between 𝑖and𝑗. For a𝑑-dimensional vector
z∈R𝑑,∥z∥2is the Euclidean norm of z. We use𝑧𝑖to denote the
𝑖thentry of z, and diag(z)∈R𝑑×𝑑is a diagonal matrix such that
the𝑖thdiagonal entry is 𝑧𝑖. We use z𝑖denote the row vector of Z
and𝑧𝑖𝑗for the(𝑖,𝑗)thentry of Z. The trace of a square matrix Z
is denoted by tr(Z), which is the sum of absolute values along the
diagonal of Z. The singular value decomposition of Zis denoted as
Z=U𝚲V⊤. We use∥Z∥2to denote the Spectral norm of Z, which
is its largest singular value 𝜆max. We use∥Z∥𝐹for the Frobenius
norm, which is∥Z∥𝐹=√︃Í
𝑖,𝑗𝑎2
𝑖𝑗=√︁
Tr(Z⊤Z).
A.2 Proof for Lemma 5.3
Let trace𝑐=Í𝑑
𝑖=1𝜆𝑖. Recall𝐷𝜙𝑙𝑑isÍ
𝑖𝜆𝑖−𝑙𝑜𝑔𝜆𝑖−1which is
convex. Let Lagrangian L=Í𝑑
𝑖=1𝜆𝑖−𝑙𝑜𝑔𝜆𝑖−1+𝛽 (Í𝑑
𝑖=1𝜆𝑖)−𝑐.
Set partial derivatives of the LagrangianL
𝜆𝑖=0andL
𝛽=0. Set of
equations yields 𝜆𝑖=1/(1+𝛽)and𝛽=𝑑/𝑐−1which readily give
the optimum for 𝜆1=···=𝜆𝑑=𝑐/𝑑.
A.3 Efficiency of the LogDet Regularization
The primary advantage of 𝐷𝜙𝑙𝑑over the Riemannian metric 𝐷𝑅
𝜙
(such as AIRM [ 16] and LERM [ 16]) and the von Neumann diver-
gence,𝐷𝜙𝑣𝑛, lies in its computational efficiency. Specifically, 𝐷𝜙𝑙𝑑
only requires the computation of determinants, a task that can be
rapidly executed through the Cholesky factorizations [ 23], incur-
ring a cost of1
3𝑑3flops [ 23]. In contrast, the Riemannian metric
requires access to eigenvalues, which can take approximately 4𝑑3
flops for positive definite matrices. Consequently, 𝐷𝜙𝑙𝑑significantly
outperforms its counterparts in terms of computational speed. The
computational advantages of 𝐷𝜙𝑙𝑑become even more pronounced
when evaluating gradients.Table 7: Average times (millisecs/trial) to compute gradients;
computed over 1000 trials to reduce variance.
𝑑∇Σ𝐷𝑅
𝜙(Σ,I) ∇ Σ𝐷𝜙𝑙𝑑(Σ,I)
5 0.79800±0.0934 0.0360±0.0090
10 2.38300±0.2094 0.0580±0.0210
20 7.49300±0.5954 0.1100±0.0130
40 24.8990±1.1264 0.2700±0.0470
80 99.4860±5.1813 0.9210±0.0280
200 698.873±39.602 8.7670±2.1370
500 6377.74±379.11 94.837±1.1950
1000 40443.1±2827.2 622.28±37.728
Table 7 shows that the computation of ∇𝐷𝜙𝑙𝑑can be more than
100 times faster than ∇𝐷𝑅
𝜙. This discrepancy proves to be crucial in
applications involving neural networks.
Method Cora CiteSeer Am-Comp.
LogDet (via SVD ) 84.72 73.72 88.21
LogDet (via determinant) 86.12 74.12 88.72
Table 8: Results for computing LogDet (SVD vs. determinant).
Table 8 also shows that computing LogDet via determinant leads
to better results due to the better stability of the matrix determinant
computed via the Cholesky decomposition.
A.4 Proof for Lemma 5.4
Conditioal Number 𝜆1/𝜆𝑑is upper bounded by our 𝐷𝜙𝑙𝑑(Σ,I)as:
cond(Σ)≤4 exp 𝐷𝜙𝑙𝑑(Σ,I). (22)
If loss𝐷𝜙𝑙𝑑(Σ,I)decreases in the first steps, cond(Σ)becomes
smaller and smaller, ensuring stability.
Proof. Apply the natural logarithm to the above inequality, let
𝜆1≥𝜆𝑑, use𝜆1−ln𝜆1−𝜆1+𝜆𝑑−ln𝜆𝑑−1≤Í𝑑
𝑖=1𝜆𝑖−ln𝜆𝑖−𝜆𝑖
and𝜆−ln𝜆−1≥0and𝜆−ln𝜆−1≥ln𝜆−ln 4+1. □
4349