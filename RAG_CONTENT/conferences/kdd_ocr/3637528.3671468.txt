Sharing is Caring
A Practical Guide to FAIR(ER) Open Data Release
Amelia Henriksen
aahenri@sandia.gov
Sandia National Laboratories
Albuquerque, New Mexico, USAMiranda Mundt
mmundt@sandia.gov
Sandia National Laboratories
Albuquerque, New Mexico, USA
Abstract
Findable. Accessible. Interoperable. Reusable. Since their introduc-
tion in 2016, the FAIR data principles have defined the standards by
which scientific researchers share data. However, modern research
in data editing and management consistently shows that while the
FAIR data principles are widely accepted in theory, they can be
much more difficult to understand and implement in practice. In
this tutorial, we explore some of the simple, realistic steps scientists
can take to FAIRly release open data. We also explore areas where
the current FAIR guidelines fall short and offer practical suggestions
for making open data FAIR(ER): more Equitable and Realistic. This
first involves ways to make datasets themselves more equitably ac-
cessible for researchers with disabilities. While equitably accessible
data design has some research overlap with paper, presentation,
and website design, we suggest several unique distinctions specific
to datasets. The “Realistic” aspect of FAIR(ER) data facilitates a
path to translate open data (and research on that data) back to true
applications. Driven by national security applications pipelines, we
call out important considerations for balancing data editing against
data realism.
CCS Concepts
•Information systems →Data access methods ;Information
integration; •Human-centered computing →Accessibility; •
Social and professional topics →People with disabilities .
Keywords
FAIR data, equitable accessibility, open data, data curation
ACM Reference Format:
Amelia Henriksen and Miranda Mundt. 2024. Sharing is Caring: A Practical
Guide to FAIR(ER) Open Data Release. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3671468
1 Open, FAIR data
The FAIR principles for Findable, Accessible, Interoperable and
Reusable data were first introduced in Scientific Nature in 2016
[78]. Since that introduction, FAIR data guidelines have become the
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671468foundation of the data sharing movement for scientific research.
The exact FAIR standards and substandards are outlined in Table 1.
At their core, the FAIR data principles are designed to make
datasets useful for researchers beyond the original data creators.
While many interpretations for the core four concepts exist, we
describe them in terms of data usability as follows:
(1)Findable: Researchers need to find your data to use it.
(2)Accessible: Researchers use data that is clear and easy to
obtain.
(3)Interoperable: Researchers use data they (and their ma-
chines) can understand.
(4)Reusable: Researchers can’t use a dead dataset.
It should be noted that FAIR is not exclusive to open data. The
full FAIR guidelines were specifically designed to provide guidance
for sharing data over a variety of access protocols. However, in this
tutorial we choose to focus on practical guidance for FAIRification
of open data specifically. This is because open data is the lifeblood
of interdisciplinary machine learning, artificial intelligence, and
other data driven disciplines [36, 53].
1.1 From FAIR to FAIR(ER)
While FAIR covers many aspects of open data sharing and release,
gaps remain [ 25,39]. Thus in this tutorial we introduce the novel
FAIR(ER) model–adding the “Equitable” and “Realistic” data com-
ponents to critical curation considerations.
1.1.1 Equitable. In its current form, the “A” for “Accessibility” in
FAIR data standards could be substituted with “Availability” with
little change in meaning for the A.1 and A.2 sub-standards. The
tendency in the open literature to emphasize the limited standard
of data availability rather than exploring the broader principle
of data accessibility creates significant gaps in data sharing and
release [ 76]. Most notable is the lack of practical guidelines and
resources for equitably accessible data–in particular, datasets that
are accessible for researchers with disabilities. In this work, we
review simple steps for–as well as challenges specific to–making
open data equitably accessible.
1.1.2 Realistic. FAIR data standards–with the addition of equity–
create an important foundation for making shared data generally
usable. However, there is a distinction between usable data and
useful data, particularly in scientific settings. In this work we detail
why and how realism should be a fundamental consideration for
data curation and release. We draw on not only the body of state-of-
the-art research but experiences specific to the national laboratory
research setting. We explore trade-offs between preserving true
applications pipelines and making datasets algorithm-ingestible.
6513
KDD ’24, August 25–29, 2024, Barcelona, Spain Amelia Henriksen and Miranda Mundt
Table 1: The official FAIR standards and sub-standards [78]
Findable
F1 (Meta)data are assigned a globally unique and persistent identifier.
F2 Data are described with rich metadata.
F3 Metadata clearly and explicitly include the identifier of the data they describe.
F4 (Meta)data are registered or indexed in a searchable resource.
Accessible
A1 (Meta)data are retrievable by their identifier using a standardized communications protocol.
A1.1 The protocol is open, free, and universally implementable.
A1.2 The protocol allows for an authentication and authorisation procedure, where necessary.
A2 Metadata are accessible, even when the data are no longer available.
Interoperable
I1 (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.
I2 (Meta)data use vocabularies that follow FAIR principles.
I3 (Meta)data include qualified references to other (meta)data.
Reusable
R1 (Meta)data are richly described with a plurality of accurate and relevant attributes.
R1.1 (Meta)data are released with a clear and accessible data usage license.
R1.2 (Meta)data are associated with detailed provenance.
R1.3 (Meta)data meet domain-relevant community standards.
1.1.3 Outline for practical, FAIR(ER) data. In this tutorial we dis-
cuss the pragmatic aspects of open data sharing under the expanded
FAIR(ER) data paradigm. Under the FAIR premise, we discuss data
cleaning (Section 2), data documentation (Section 3), and data pub-
lication (Section 4) as the three key thrusts of data editing pipelines.
These three fundamental thrusts are illustrated in Figure 1.
We then discuss practical considerations for Equitable (Section
5) and Realistic (Section 6) open data. The goal of this work is to
serve as a simple, useful reference for data creators, data editors,
and data users to expand the reach and utility of their datasets in
achievable ways.
2 Data cleaning and standardization
2.1 Accessible and preservable file formats
[55] succinctly describes the primary concern for preservable file
formats: “File formats become obsolete when the software that
renders them is no longer supported.” The goal, then, is typically
to release data in formats that are as software agnostic as possible.
This ties into FAIR guideline I1 with particular application to ma-
chine interoperability. For a machine, the file format defines the
knowledge representation for the data. The more software agnos-
tic and open the format, the more accessible, shared, and broadly
applicable it is. Thus we recommend considering the following five
guiding questions described by [ 2] when deciding on final formats
for data sharing and release. Q1: Is my format open or proprietary?
Q2: Is my format common or low use? Q3: Is my format supported
by many software platforms or only one? Q4: Is my format free-
standing or reliant on embedded files, programs, or scripts? Q5: Is
my format lossless or lossy?It is important to note, however, that there is an inherent tension
between best file format practices generally (guideline I1) and com-
mon file format practices by domain (guideline R1.3). For example,
many materials science experimental pipelines are tailored to use
machine specific (sometimes proprietary) raw output files. This ma-
chine dependence and unique formatting are not preservable, but
they may be considered “standard” or even “best practice” for some
kinds of materials analysis. Converting the files to non-proprietary,
open formats makes them preservable and accessible for interdisci-
plinary applications, but may add barriers to use for the original
domain scientists. While a straightforward solution is to publish
both the raw and converted file formats (or the raw formats with
simple conversion code), this brings to light an important issue. In
order for data to meet both I1 and R1.3, researchers must begin
to tailor and update their software applications on data with the
expectation that the data will be in accessible and preservable for-
mats. Returning to the materials example, this would mean that
materials researchers should analyze machine outputs with the
expectation that they have already been converted to a more inter-
operable format–rather than tailoring pipelines to single machines.
This further emphasizes the strong relationship between both FAIR
data and FAIR software development [9].
2.2 Consistency
Though the exact definition of data quality varies by domain, al-
most all disciplines have identified consistency as a core feature
of quality data [ 72]. For every step of human intervention in the
data creation process, there is potential for consistency error. This
stems largely from the fact that human interoperability and ma-
chine interoperability involve distinct curation. Conventions that
6514Sharing is Caring KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 1: Three key thrusts for practical, FAIR(ER) data
appear clearly linked to a human user may not be clearly linked
under machine interpretation. Thus we recommend implementing–
and, where possible, automating–data consistency checks for the
following data components: (1) formats/types for groups of files
meant to contain the same data; (2) file naming conventions; and
(3) style conventions for file layouts, color schemes, fonts, etc.
As an example, if one experimentalist saved images as pngfiles
and another saved images as tiffiles for the same dataset, the im-
ages should be standardized to all pngortif. Similarly, if different
files with tabular data contain the same column headers with small
name or order variations, these should also be converted to a single
naming convention and file layout.
2.3 User Friendly Organization
Truly FAIR documentation is aggressively detailed and technical by
nature (see Section 3). While external researchers rely heavily on a
dataset’s documentation for long term application and use, short
term data discovery and probing typically take place before the
documentation is truly understood and explored. This means that
clear data organization is one of the most critical steps for making
a published dataset approachable.
During the data cleaning process, one of the easiest and most
effective ways to gain insight into a dataset’s organization is to
ask another researcher to review it. Reviewers should ideally be
uninvolved in the original project and from a technical but dis-
tinct domain, which helps facilitate interdisciplinary research and
reinforces the “broadly applicable” aspect of FAIR guideline I1.
3 Data documentation
3.1 Making a data management plan
Data management facilitates internal data logistics and defines
the infrastructure necessary for eventual open data release. Data
management methodology is a thriving field of research across all
scientific disciplines–and each domain has its own unique research
and recommendations for practical data management systems [ 60].
As such, a universal discussion of data management infrastructure
is out of scope for this tutorial. However, data cleaning as part
of the FAIRification process fundamentally involves data manage-
ment steps. We note that a key piece of both data managementand documentation–regardless of domain–is formulating a data
management plan (DMP) as early in the data creation and release
process as possible.
A data management plan is a formal document that discusses
“how the data generated through the project will be handled...as they
are generated and analyzed, and once the project has concluded”
[40] and can communicate these expectations to stakeholders. As
a project evolves, the DMP should evolve with it. Many excellent
reviews of research, frameworks, and tools for data management
plans exist, including [71], [35] and [11].
3.2 The fundamental README
The “README” file is the cornerstone of data documentation. A
README is the key file that describes a given dataset and is the
main vehicle for recording the metadata associated with it. At a
minimum, a README should include the following components.
Introductory metadata. This includes the data title, the stakehold-
ers that helped produce the data, the funding statement, the data
authors, the technical point of contact and—if they are separate—the
data access point of contact. It is also appropriate to include an
overarching description of the dataset and the data types collected.
Introductory documentation should also provide an overview of
the data organization and file contents.
File descriptions. Another important aspect of the README is to
describe each of the individual files or data types. The exact appro-
priate metadata to include depends on the nature of the dataset and
how it was collected. For sets of files with the same data type and
format, a README should—at a minimum—include the following:
(1) The data type captured in the file set
(2)The filename format for the file set and what each component
of the filename convention refers to.
(3)The availability of the data type with reference to the larger
dataset as a whole.
(4)The data format. This includes metadata specific to the file
format type, such as column names and their explicit defini-
tions for tabular data with headers.
(5)Methodology by which the data/file type was gathered. If
software was used to generate all or part of the data type,
6515KDD ’24, August 25–29, 2024, Barcelona, Spain Amelia Henriksen and Miranda Mundt
it can (and should) be published separately and pointed to
in the README. Similarly, if the files were generated by a
specific machine under a given setup, that metadata should
be included in the README or an appropriate meta file
(referenced in the README).
(6)Any important notes. Documentation should capture any
issues or quirks in the data with as much detail as possible.
This includes missing data, mistakes made in the data collec-
tion process, or need for additional preprocessing to use the
given data type.
Descriptive, thorough documentation for data methods is key for
ensuring data is reproducible. Reproducibility is in turn highly syn-
ergistic with data reusability in the FAIR paradigm [ 15]. Describing
data issues and quirks is also one of the most essential aspects of
data reproducibility. While researchers cannot be expected to create
perfect datasets, documenting data imperfections allows data users
to learn how to solve problems on real-world data.
3.3 Accompanying code
Software that generates data should be seen as an extension of
the data itself. Indeed, multiple researchers have endeavored to
create “FAIR” principles specifically geared towards software [ 9,27,
45]. Ultimately, the principles applied to data are also applicable
(with adjustments) to software. For example, as one can create a
persistent and unique identifier for data (such as DOI), one can and
should also create a persistent and unique identifier for the software
used to generate the data (such as through Zenodo [ 31]). When
documenting information for a dataset, include the accompanying
identifier for the applicable software. Other simple methods for
increasing the FAIRness of software include making the software
openly available (e.g., via GitHub) and uncomplicated to download
and install (e.g., for a Python package, publishing it to PyPI).
Also consider the limitations of those who may be accessing your
code. Recent studies have found that most “best practices” for code
style are based on sighted developers. Consider following better
standards for accessible code practices such as using consistent line
indentations and suffixes [59].
3.4 Implicit metadata
When creating and documenting data that involves components
like header names, column names, or labels, data creators should
try to minimize assumptions about data users’ context. For data
headers (and their accompanying documentation) to stand the test
of time, they have to stand the test of nomenclature, machine,
and methods evolution. We consider two critical principles for
header/column/label naming conventions:
3.4.1 When in doubt, spell it out. While abbreviations can be very
useful scientific tools, they are often overloaded. Consider, for exam-
ple, the abbreviation “Rs. ” For those working in materials characteri-
zation, it might be obvious that Rs stands for sheet resistance. Yet to
the general scientific community, Rs could stand for anything from
remote sensing to raman spectroscopy to rhythmic stabilization.
This ties critically into R1 of the FAIR data guidelines: metadata
arerichly described. Go-FAIR.org clarifies that “the data publisher
should not attempt to predict the data consumer’s identity andneeds” [ 3]. Thus it is better to be more explicit when establishing
naming conventions during the data creation process.
3.4.2 If you don’t want to lose a very important tortoise, include your
units. In 2001, the LA Zoo agreed to loan Clarence the Galapagos
Tortoise to the Moorpark research facility in California. The LA
Zoo team told the Moorpark team that Clarence weighed “250”
to help them build the correct size of enclosure. However, they
assumed the Moorpark team would interpret 250 as 250 kilograms.
Unfortunately, the Moorpark team assumed it was 250 lb, they built
the enclosure fences far too small, and Clarence easily escaped [ 19].
While the tortoise was eventually found and returned, sometimes
failing to include units can have more devastating consequences.
In 1999, NASA lost a $125 million Mars orbiter because it was built
between two teams, and one team used metric units and the other
used imperial. Because they made assumptions about what the
other team was doing, the orbiter lost contact completely and could
not be recovered [67].
This all ties back to the fundamental principle of minimizing
assumptions about data users’ context. While explaining the units
associated with given column names, headers, etc. via a README
is important, the best practice is to also include units in headers
themselves wherever possible. This lays the foundation for accurate
data probing and analysis for many data ingestion pipelines.
4 Data publication
4.1 Choosing a data repository
Many different metrics have been proposed for assessing FAIR re-
search data repositories, including the “FAIREST” or [ 24], “TRUST”
models [ 48]. This is because for openly published data, fulfilling
many of the FAIR data principles is impossible without an appropri-
ate open repository. Thus in this tutorial we extensively consider six
questions that are crucial when choosing a repository for publishing
and hosting your dataset.
Q1. Does the repository assign a DOI (or other standard persistent
identifier) to the data? Persistent identification is the cornerstone of
FAIR data generally and findable data specifically – to the point that
it is the very first FAIR data guideline: “(Meta)data are assigned a
globally unique and persistent identifier.” For modern data science,
one of the most standard persistent identifiers is a digital object
identifier (DOI) [ 61]. If a repository does not automate or require
standardized persistent identification, they can not be considered
FAIR [24] and should not used for (meta)data publication.
Q2. Do I want to publish my data in a domain-specific or general
repository? Data repositories can frequently be divided by the kind
of data they contain, domain specific or domain-agnostic [ 72]. This
typically presents as a spectrum – many “domain specific” reposito-
ries capture a broad range of related disciplines, such as all “earth
science” [28] or “materials science” [13] data.
Q2.1 What kind of metadata does this repository track? Consider
the appropriate data community for the dataset [ 21]. For data de-
signed for a highly interdisciplinary users (as is frequently the case
for machine learning applications), a domain-agnostic repository
may have more flexible metadata to reach a broader audience. On
the other hand, domain-specific repositories are more likely to
6516Sharing is Caring KDD ’24, August 25–29, 2024, Barcelona, Spain
collect and publish domain-specific metadata [ 5]. This facilitates
fine-grained findability for a dataset with specific context for the
given domain and meets the FAIR metadata requirements R1.3 and
F2.
Q2.2 Does the repository have other examples of the kind of data
I am publishing? One of the best ways to determine whether a
dataset should be published in a given repository is to go beyond the
repository scope and examine previously published datasets. If prior
data publications for the same or similar data type have precedent
in a given repository (and the given repository sufficiently meets
FAIR standards), it is best follow this precedent.
Q3. Can this repository support my dataset? Each dataset is unique
and may require distinct repository features for appropriate hosting.
We describe a few general considerations below:
Q3.1 What are the repository data size limits? Each repository has
limits on the size of data that can be uploaded – both for individual
files and datasets as a whole. With the rise of open big data this can
be a major limitation: many popular general repositories have data
limits on the order of Gb rather than Tb. That said, there is also
increasing interest in publishing big data. Repositories like Harvard
Dataverse [ 68] and Zenodo [ 80] have policies for contacting the
repository curators about publishing larger datasets. There are also
research efforts to specifically stand up big data repositories, such
as University of Southern California’s current work on a globus-
powered “Big Data Repository” [1].
Q3.2 Does this repository support my data formats? Some repos-
itories are tailored to specific data types and may only support a
subset of file formats [ 56]. Researchers should check for any format
restrictions before publishing their data in a given repository.
Q3.3 Does this repository meet the data stakeholder’s requirements
for publication? In many research settings, including government
research laboratories, datasets may be derived all or in part from
an external source with their own rules and regulations for data
publication. This can affect the choice of an appropriate repository
– for example, a customer may stipulate that while their data can be
published openly, it must be in a repository sponsored by organiza-
tions in the country of origin. When creating a data management
plan (as described in Section 3.1), it is always important to com-
municate with all data stakeholders as early as possible about any
restrictions or requirements for data publication [5].
Q4. Is the repository actively funded and maintained? Funding
and maintenance both play a critical role in the long term sustain-
ability of a research repository [ 48]. If a repository’s number of
recently published datasets and overall publication number trends
show a steady increase (or are generally high) over time, then that
is a good indicator for the future maintenance of the repository. It
is also important to review the funding sources for a given reposi-
tory, which should be available on the repository website. Inactive
funding sources–or funding sources with an end date–can limit the
findability of their datasets as the repository essentially “expires”
(even if it remains accessible in archival form).
Q5. How easy/difficult is it for someone to download my data from
this repository? How data is made available though a repositorydirectly coincides with FAIR guidelines A1 and A1.1. For FAIR,
open data, the goal is to use a repository with as open a protocol
and as few steps to data download as possible.
Q5.1 If I have code affiliated with my data, how easy is it to link
it to the repository entry for the dataset? As mentioned in Section
3.3, software is frequently an integral part of data creation and
release. An important aspect of access for a given repository is not
simply how easy it is to download the data, but how easy it is to
get from the data itself to other important components of the data
publication – like software or a paper publication.
Q6. Does the repository have a clear procedure for releasing and
tracking updates to the dataset? While data creators and editors
would love to release perfect datasets, this is rarely the case. Thus
repositories should have a clear procedure for updating datasets –
so that open data can be fixed or expanded upon with clear prove-
nance (FAIR guideline R1.2) [ 42]. When datasets have clear updates
appropriately linked to prior dataset versions, it facilitates research
on better data while also preserving research done on previous
iterations of the dataset [ 65]. This has clear analogs to the impor-
tance of software versioning – and indeed it is critical that software
involved in data release also have detailed version provenance.
4.2 Licenses
FAIR data guideline R1.1 explicitly calls out licenses as a critical
aspect of reusable data. Including a license for your datasets defines
the terms under which the data can be reused in the first place.
This is important because the legal rights and copyright protections
for datasets can be variable – especially given the increasingly
international nature of scientific research [ 44]. Using standardized
licenses maximizes the reusability of data, particularly for open
data (the focus of this tutorial) [ 63]. Creative commons licenses and
open data commons licenses have become the recognized norm for
data publication. CC BY 4.0 and ODC By v1.0 provide open licenses
with attribution, similar in spirit to BSD licensing for software [ 44].
While even more open licenses exist that do not require attribution
(CC0, for example), we encourage attribution in licenses as a critical
incentive for data creators in the research community to publish
their work. [77].
For data releases that include derived data from a previously
published resource, the terms of the original license should be
followed and included as necessary in the derived data release.
4.3 Parallel Paper Publications
The last 5-10 years have given significant rise to the data descriptor
article, also called a data report or data note [ 38]. These are articles
published in peer reviewed scientific journals that present a dataset
as the primary, novel contribution, free from significant analysis or
interpretation. Journals that publish data descriptors exclusively
are becoming significantly more prevalent, and many journals are
including data descriptors as an accepted article type [ 18]. This
represents an important fundamental shift in the scientific commu-
nity to recognize that dataset publication should be regarded as a
critical contribution on its own, not tacked on as an afterthought to
“novel” analysis. Data descriptor articles create a tangible incentive
6517KDD ’24, August 25–29, 2024, Barcelona, Spain Amelia Henriksen and Miranda Mundt
for researchers to release their data. By representing data publica-
tions in journal form, they count toward the mainstream scholarly
metrics of journal publications and citations [ 77]. It also creates
another level of findability for the dataset, since data descriptors
are accessible in journal article search engines.
5 Equitable Accessibility in Data
When data is difficult for researchers with color vision deficiency
(CVD), neural hypersensitivity, dyslexia, low vision, low hearing,
mobility needs, etc. to access, it is not truly open or shareable
because there is an entire research demographic that cannot use it.
This is a massive loss of the research community, especially since
researchers who analyze data with alternative access techniques
bring unique insights because they probe data differently [62].
Our discussion of practical considerations, unique data chal-
lenges, and potential solutions below includes not only summaries
of current research, but also direct suggestions and feedback from
researchers with disabilities. In developing this tutorial, we solicited
feedback on challenges (and potential solutions) for data accessibil-
ity from the Abilities Champions of Sandia (ACS) working group.
ACS is an employee resource group at Sandia National Laboratories
that represents the causes of the 7.4% of Sandia employees who
identify as disabled (over 1,200 employees) [ 43]. The responses we
received have been incorporated into our discussion throughout.
5.1 Practical considerations
Datasets come in a massive range of formats and content types, and
the preferred accommodations for data users with disabilities span a
similarly broad gamut. Here we introduce a few practical solutions
that translate well from existing guidelines for website/publication
design to data curation. These suggestions can vastly improve the
equity of common forms of data with little effort on the part of
the data creator. However, regardless of the data format or content
types, we emphasize that the most important principle for curating
equitable data is to facilitate data users’ ability to set the terms of
their own engagement.
5.1.1 Giving control to the data user. Data format choices are key
to giving control to the data user. In particular, the five questions de-
scribed in Section 2.1 not only determine if a file format is FAIR but
also if a file format is likely to be equitable. In particular, common
file formats supported by multiple software platforms are far more
likely to have screen reader integration and support than unique
or proprietary formats accessible via only a single software plat-
form. Similarly, when a file can be accessed via multiple software
platforms, it increases the chances that at least one form of access
allows the user to zoom in on visual data or change the font for text
data. High use data formats are also more likely to be supported by
state-of-the-art tools from accommodations research [47, 69].
Another great option for facilitating user control is to expand
data access options with helper code and demos released along
with the data. For example, if a visual dataset has inherent color
schemes that are triggering for hypersensitive viewers, you could
include a loading function with a color filter option in the helper
code and allow the researcher to choose whether or not they need
it. Helper code and demos can also be a great place to document
ways existing tools and packages integrate with your data.5.1.2 Features controlled by the data creator.
Consider Your Language. What does “model” mean in your dis-
cipline? In mathematics, a model describes a system via variables
and equations. In chemistry, a model may refer to a physical rep-
resentation of, for example, a molecule. Defining the language of
your dataset in clear terms is essential to promoting equity.
Textual content-based formatting. While file type formats can
better facilitate things like screen reader use or font adjustment,
the data creator still controls the content-based format of textual
information in data. For example, the data creator controls the vari-
able, header, and file naming conventions for the dataset. For these
components, standardized conventions (such as snake case, camel
case, and pascal case) have been adopted to remove spaces and
maximize machine interoperability. Inter word spacing is critical
for researchers with dyslexia or working with screen readers. Thus
we recommend snake case as a way to maximize both machine and
human interoperability.
Furthermore, when formatting README files or other documen-
tation, consider ways to organize and break up information with
consistent spacing. For neurodiverse researchers and researchers
with dyslexia, large blocks of text can be overwhelming and pro-
hibitive to parse [75].
Data creator generated visualizations. Sometimes a dataset will
include visual data with components that the data creator can
control and the data user cannot (such as colormap, layout, and
embedded text). This includes things like expository figures, data
derived graphs, and curated sensor visualizations. There is a strong
body of research on improving the accessibility of these kinds of
figures, both as guidelines [ 20,54] and state of the art insights
[16,30]. We list a few simple-to-implement principles that can have
massive positive impacts for researchers with accessibility needs.
[22] provides a comprehensive overview on important consid-
erations for choosing scientific colormaps. Key takeaways include
choosing a perceptually uniform colormap (such as cividis [ 57])
so that the color interpretation of a visualization is the same for
data users with color vision deficiency and without. This is why
previously popular colormaps like “Jet” are no longer recommended
for data visualizations. Another important consideration is whether
an image under a given colormap is accessible in dark mode (for
researchers with visual hypersensitivities, including migraines) and
high contrast mode (for researchers with low vision).
In graph type figures that visualize some kind of object com-
parison, objects should be distinguished by more than just color.
Options include varying objects by demarcation, or putting text
next to the plotted data objects they reference rather than using a
separate legend.
For text components embedded in figures, use sans serif font
styles and at least 12 pt font sizes to accommodate data users with
dyslexia. Where possible, embedded text components should be
stored as alt text or be accessible elsewhere in the dataset in a
screen-reader usable format (such as a table of image file names
mapped to their textual components).
6518Sharing is Caring KDD ’24, August 25–29, 2024, Barcelona, Spain
5.2 Unique Data Challenges
There is a strong body of prior guidelines for making various kinds
of research publications – including presentations, papers, and
websites – more equitably accessible for researchers with disabilities
(e.g., [ 12,64]). Some of these guidelines and tools translate well
to sharing equitable datasets. However, unique sources of tension
specific to data release remain. We call out a few of these challenges,
as well as potential directions for solutions to these challenges.
5.2.1 Big data.
Challenge. Datasets that are even moderately large are prohib-
itively expensive to curate by hand. This is particularly true for
generating alt text for images, transcripts for videos, and captions
for audio data. Much state-of-the-art research is being done on tools
to automatically generate things like captions, transcripts, and alt
text (for data with audio or visual components) [ 7]. However, there
is a considerable difference between a state-of-the-art tool existing
and a tool being findable, easy to use, and applicable for a spe-
cific scientific dataset. Even if automated tools can be applied to a
dataset, there remains concerns about validating their outputs [ 14].
Potential Solutions. While concerns about validation for alterna-
tive text data are worth considering, we note that the overarching
lack of any kind of access to visual data (for researchers with visual
disabilities) or audio data (for researchers with auditory disabilities)
[58,70] means a “something is better than nothing” approach is
appropriate at this stage in data release culture. In that spirit, a
simple option for data creators is to briefly identify which existing
automated access tools apply to their specific dataset, and provide
links or demos for using them in the data documentation.
While this will be a helpful first step to addressing these kinds
of issues, we call upon data creators everywhere to prioritize dis-
covering solutions to this dataset specific challenge.
5.2.2 Interpretation vs captioning.
Challenge. Alt text is a pivotal part of website and paper publi-
cation guidelines for equitable images. For some image and video
types, however, there is a concern that writing alt text or descriptive
transcripts constitutes a data interpretation rather than raw visual
result, which could potentially bias the data user’s analysis.
A similar situation arises with data with audio components
for researchers with hearing disabilities. While descriptive cap-
tions/transcripts can be included for spoken components, they may
constitute an interpretation when describing non-spoken sounds.
Interpretations of spoken “tones” (sad, angry, etc.) can also create
potentially biased captions.
Potential Solutions. When alt text, labels, captions, or transcripts
are assigned to visual or audio data – especially by expert inter-
preters – the descriptive text itself constitutes a form of data ground
truth that can be extremely useful for machine learning research.
Thus we suggest that some of the guidelines in state-of-the-art
research for curating ground truth [ 6,46,74] can be applied to
curating alt text and captions in a way that reduces bias.
There is also state-of-the-art research on alternative data probing
tools that leave analysis entirely in the hands of the researcher,
outside of alt text or captions. The goal of such work is to allow theuser to “confidently generate their own insights” rather than relying
solely on the accessible representations chosen by the data creator
[70]. For example, [ 70] describes sonification tools that interpret
images via interactive sounds, and [ 79] describes tools using tactile
vibration responses for image probing.
5.2.3 Raw/original data components.
Challenge. Including raw data is critical for not only connecting
to real-data problems but to give data users control as data pre-
processing methods improve and evolve. We discuss this more in
Section 6.2. This can create tension with traditional equitable acces-
sibility guidelines. For example, WCAG 2.0aa states that web pages
should not have anything that flashes at a rate more than 3 times per
second [ 17]. Raw video data, however, may include flashes above
that rate – and this could be integral to the information conveyed
in the data.
Similarly, much valuable research takes place on data that could
potentially include disturbing themes such as sexual abuse, suicide,
and violence. While we encourage the publication of comprehensive
datasets on these important but difficult topics, engaging with this
when probing a dataset – especially without forewarning – can
cause intense psychological responses for researchers with a history
of trauma, PTSD, or other anxiety disorders.
Potential Solutions. In instances where raw data includes poten-
tial epilepsy or trauma triggers, we suggest that the best option is to
make sure the data user is informed before they download the data.
As it stands, the simplest way to make sure this is implemented in
existing data publication structures would be to include a content
warning at the end of data summaries submitted to a repository.
However, because there is no guarantee that data users will read
the repository data summary before download, a better solution
would be to create an a content warning that must be acknowledge
before sensitive datasets can be downloaded. This acknowledge-
ment should also be compatible with data access APIs. We call upon
data repository owners and curators to make identifying potential
triggers a mandatory part of metadata collected and presented for
published datasets. It is critical that we consider the best ways to
make sure that researchers have the opportunity to truly consent
to the data contents they may encounter.
6 Realistic Data
For many government agencies – including Sandia National Labo-
ratories (SNL) – Technology Readiness Levels (TRLs) are a “mea-
surement system used to assess the maturity level of a particular
technology for a specific application” [ 51]. TRLs range from fun-
damental research formulation and initial experimentation (TRL
1-3) to operational use on mission applications (TRL 9) [ 8]. In na-
tional security (NS) settings and beyond, there is a considerable
difference between the data used to develop algorithms at low TRLs
and the real-world data ingested by operational algorithms at high
TRLs. Curating data to be FAIR and Equitable forms a foundation
forusable data. However, we emphasize here that useful data shar-
ing should include effective communication that connects curated
data to its unprocessed, real-world counterparts. This forms the
premise of “Realistic,” FAIR(ER) data. While the “Realistic” aspect
of FAIR(ER) data has many potential components, here we focus
6519KDD ’24, August 25–29, 2024, Barcelona, Spain Amelia Henriksen and Miranda Mundt
on two limitations of real-world data with direct implications for
data release: data size and missing/messy data.
6.1 Data Size
While some real-world domains face challenges of too much data
– the Big Data paradigm – “large” datasets for many scientific ap-
plications are still relatively small. An excellent example of this
is the materials manufacturing domain. A group at SNL recently
published the largest library of Electron Backscatter Diffraction
Data (EBSD) (a form of materials characterization) for additively
manufactured kovar (an important alloy in many national lab ap-
plications) [ 29]. This dataset characterizes 607 unique samples –
unprecedented in materials science but relatively small compared
to many machine learning applications [ 37]. While materials and
other domains are actively developing high throughput techniques
to increase data output [ 23,50], the fact remains that many scien-
tific datasets are relatively small compared to those like web-based
image or social media data.
For datasets covering high risk, highly unlikely events (such as
certain forms of cyber or terrorist attacks), only one or two real
data points may even exist. Similarly, in classified and sensitive
settings, data shared with researchers may be deliberately withheld,
leading to very small “real-world” datasets for mission applications.
Because small data is so ubiquitous in real world applications,
we suggest that it should be embraced just as wholeheartedly as big
data in real world data publication. Scientists who are hesitant to
publish their data because it is “too small” should shift their focus to
documenting the strengths and limitations based on the size of the
data they are able to produce. The “Discover” series of journals has
set an important precedent for publishing data descriptor articles
[32], where a discussion of data limitations is a mandatory part of
the data note template.
For real world data in the Big Data paradigm, we comment
that the onus falls upon open data repositories to expand their
capabilities for hosting big data (as discussed in Section 5.2.1). We
call out [ 1], [10], and [ 4] as important examples of research efforts
in this area.
6.2 Missing data
Real world applications have missing data, sometimes in consider-
able quantities. This is a particularly prominent phenomenon in
time series sensor data critical to NS missions, including satellite
data [ 66], synthetic aperture radar (SAR) data [ 41], and automatic
identification system (AIS) data [52].
Significant research studies have explored techniques for ad-
dressing missing data in algorithm development (for an excellent
survey, see [ 26]). However, many prominent prior works in this
space demonstrate their efficacy based on synthetic missing data –
that is, real-world datasets with data artificially removed rather than
real-world datasets that have real missing data. While this standard
practice makes sense from a ground truth validation perspective,
significantly fewer studies demonstrate the extent to which their
missing data augmentation matches the various actual missing data
patterns present in real data [49, 73].
Because of this validation gap, we suggest that publishing raw
data with uncompensated-for missing values is of great worth tothe research community. Because compensating for missing data
is its own thriving field of research, emphasis should be placed on
thoroughly documenting known causes of missing data in a data
release. This can be especially important for raw experimental data,
where human and machine errors are common [26].
However, correcting for missing data is a critical step in prepro-
cessing for many machine learning algorithms [49]. Data creators
who publish data for machine learning applications may choose
to correct for missing data in advance to reduce the burden on
algorithm developers and increase the datasets’ accessibility for
existing methods. In these instances, the goal should be to clearly
map, via both documentation and software publication, the path
from the original data to the processed dataset.
An example of this is the recently published HawaiiCoast_GT
dataset [ 33]. This was created at SNL as a tool for working on
maritime vessel anomaly detection (an important problem in NS).
This dataset was derived from real world automatic identification
system (AIS) data, which is notorious for having many missing static
features like vessel name and type. Curating this dataset included
filling in many of these missing features, and the process for this was
thoroughly documented in the corresponding README. However, a
far more important aspect of [ 33]’s documentation was making the
code for filling in these missing features openly accessible [ 34]. Both
the code and documentation create a map between the unprocessed
AIS outputs in real applications to algorithms designed to operate
on preprocessed AIS.
We note that these two pathways to better data also apply to
other forms of data “messiness” including corrupted and noisy
datasets. Publishing data in its raw (but explicitly documented)
form and publishing a reproducible map from raw to processed
data improve a dataset’s value for solving real problems.
7 Conclusion
In this tutorial, we broke down the three data pipeline components
that facilitate the well known – but sometimes difficult to interpret
– FAIR guidelines: data cleaning and standardization, data documen-
tation, and data publication. We also introduced a novel expansion
from FAIR to FAIR(ER), calling out the critical need for open data
to be Equitable and Realistic. As we introduced the importance
of equitable accessibility in data, we highlighted simple steps that
have high impact for researchers with disabilities and identified
important ongoing challenges specific to data releases. We also
highlighted both preserving data size and preserving the path from
messy/missing data to curated data as key aspects for connecting
published data to realistic data used in operations. By introducing
a broad set of practical, actionable considerations, we are able to
effectively demystify the process for releasing FAIR(ER) open data.
Acknowledgments
We thank David Stracuzzi, Andy Wilson, Danny Dunlavy, and
Ian Henriksen for their invaluable feedback and insights. Sandia
National Laboratories is a multimission laboratory managed and
operated by National Technology & Engineering Solutions of San-
dia, LLC, a wholly owned subsidiary of Honeywell International
Inc., for the U.S. Department of Energy’s National Nuclear Security
Administration under contract DE-NA0003525.
6520Sharing is Caring KDD ’24, August 25–29, 2024, Barcelona, Spain
References
[1] [n. d.]. Big Data Repository (BDR).
[2][n. d.]. File Format Considerations. https://researchdataservice.illinois.edu/file-
format/. Accessed: 2024-03-16.
[3]2018. R1: (Meta)data are richly described with a plurality of accurate and rel-
evant attributes. https://www.go-fair.org/fair-principles/r1-metadata-richly-
described-plurality-accurate-relevant-attributes/
[4]Ryan P Abernathey, Tom Augspurger, Anderson Banihirwe, Charles C Blackmon-
Luca, Timothy J Crone, Chelle L Gentemann, Joseph J Hamman, Naomi Hender-
son, Chiara Lepore, Theo A McCaie, et al .2021. Cloud-native repositories for big
scientific data. Computing in Science & Engineering 23, 2 (2021), 26–35.
[5]Ricardo Carvalho Amorim, João Aguiar Castro, João Rocha da Silva, and Cristina
Ribeiro. 2017. A comparison of research data management platforms: architecture,
flexible metadata and interoperability. Universal access in the information society
16 (2017), 851–862.
[6]Kofi Arhin, Ioana Baldini, Dennis Wei, Karthikeyan Natesan Ramamurthy, and
Moninder Singh. 2021. Ground-Truth, Whose Truth?–Examining the Challenges
with Annotating Toxic Text Datasets. arXiv preprint arXiv:2112.03529 (2021).
[7]VB Aswin, Mohammed Javed, Parag Parihar, K Aswanth, CR Druval, Anupam
Dagar, and CV Aravinda. 2021. NLP-driven ensemble-based automatic subtitle
generation and semantic video summarization technique. In Advances in Artificial
Intelligence and Data Engineering: Select Proceedings of AIDE 2019. Springer, 3–13.
[8]Beatriz R Bailey and John Anthony Mitchell. 2006. On the integration of technology
readiness levels at Sandia National Laboratories. Technical Report. Sandia National
Laboratories (SNL), Albuquerque, NM, and Livermore, CA . . . .
[9]Michelle Barker, Neil P Chue Hong, Daniel S Katz, Anna-Lena Lamprecht, Carlos
Martinez-Ortiz, Fotis Psomopoulos, Jennifer Harrow, Leyla Jael Castro, Morane
Gruenpeter, Paula Andrea Martinez, et al .2022. Introducing the FAIR Principles
for research software. Scientific Data 9, 1 (2022), 622.
[10] Ashleigh M Basel, Kien Tri Nguyen, Elizabeth Arnaud, and Alessandro CW
Craparo. 2023. The foundations of big data sharing: A CGIAR international
research organization perspective. Frontiers in Environmental Science 11 (2023),
1107393.
[11] Carina Becker, Carolin Hundt, Claudia Engelhardt, Johannes Sperling, Moritz
Kurzweil, and Ralph Müller-Pfefferkorn. 2023. Data Management Plan Tools:
Overview and Evaluation. In Proceedings of the Conference on Research Data
Infrastructure, Vol. 1.
[12] Lisa Billingham. 2014. Improving academic library website accessibility for
people with disabilities. Library Management 35, 8/9 (2014), 565–581.
[13] Ben Blaiszik, Kyle Chard, Jim Pruyne, Rachana Ananthakrishnan, Steven Tuecke,
and Ian Foster. 2016. The materials data facility: data services to advance materials
science research. Jom 68, 8 (2016), 2045–2052.
[14] Christian Bokhove and Christopher Downey. 2018. Automated generation of
‘good enough’transcripts as a first step to transcription of audio-recorded data.
Methodological innovations 11, 2 (2018), 2059799118790743.
[15] Christophe Bontemps and Valérie Orozco. 2021. Toward a FAIR reproducible
research. In Advances in Contemporary Statistics and Econometrics: Festschrift in
Honor of Christine Thomas-Agnan. Springer, 595–613.
[16] Michael Burch, Weidong Huang, Mathew Wakefield, Helen C Purchase, Daniel
Weiskopf, and Jie Hua. 2020. The state of the art in empirical user evaluation of
graph visualizations. IEEE Access 9 (2020), 4173–4198.
[17] Ben Caldwell, Michael Cooper, Loretta Guarino Reid, Gregg Vanderheiden, Wendy
Chisholm, John Slatin, and Jason White. 2008. Web content accessibility guide-
lines (WCAG) 2.0. WWW Consortium (W3C) 290, 1-34 (2008), 5–12.
[18] Leonardo Candela, Donatella Castelli, Paolo Manghi, and Alice Tani. 2015. Data
journals: A survey. Journal of the Association for Information Science and Technol-
ogy66, 9 (2015), 1747–1762.
[19] Steve Chawkins. 2001. Mismeasure for measure. Los Angeles Times (2 2001).
https://www.latimes.com/archives/la-xpm-2001-feb-09-me-23253-story.html
[20] Wendy Chisholm, Gregg Vanderheiden, and Ian Jacobs. 2001. Web content
accessibility guidelines 1.0. Interactions 8, 4 (2001), 35–54.
[21] Danielle Cooper and Rebecca Springer. 2019. Data communities: A new model
for supporting STEM data sharing [Issue Brief]. (2019).
[22] Fabio Crameri, Grace E Shephard, and Philip J Heron. 2020. The misuse of colour
in science communication. Nature communications 11, 1 (2020), 5444.
[23] Stefano Curtarolo, Gus LW Hart, Marco Buongiorno Nardelli, Natalio Mingo,
Stefano Sanvito, and Ohad Levy. 2013. The high-throughput highway to compu-
tational materials design. Nature materials 12, 3 (2013), 191–201.
[24] Mathieu d’Aquin, Fabian Kirstein, Daniela Oliveira, Sonja Schimmler, and Sebas-
tian Urbanek. 2023. FAIREST: A framework for assessing research repositories.
Data Intelligence 5, 1 (2023), 202–241.
[25] Romain David, Laurence Mabile, Alison Specht, Sarah Stryeck, Mogens Thomsen,
Mohamed Yahia, Clement Jonquet, Laurent Dollé, Daniel Jacob, Daniele Bailo,
et al.2020. FAIRness literacy: The Achilles’ heel of applying FAIR principles.
CODATA Data Science Journal 19, 32 (2020), 1–11.
[26] Tlamelo Emmanuel, Thabiso Maupong, Dimane Mpoeleng, Thabo Semong, Bany-
atsang Mphago, and Oteng Tabona. 2021. A survey on missing data in machine
learning. Journal of Big data 8 (2021), 1–37.[27] Christopher Erdmann, Natasha Simons, Reid Otsuji, Stephanie Labou, Ryan John-
son, Guilherme Castelao, Bia Villas Boas, Anna-Lena Lamprecht, Carlos Martinez
Ortiz, Leyla Garcia, et al. 2019. Top 10 FAIR Data Software Things. (2019).
[28] Janine Felden, Lars Möller, Uwe Schindler, Robert Huber, Stefanie Schu-
macher, Roland Koppe, Michael Diepenbroek, and Frank Oliver Glöckner. 2023.
PANGAEA-Data publisher for earth & environmental science. Scientific Data 10,
1 (2023), 347.
[29] J. Elliott Fowler, Tim J. Ruggles, Dale E. Cillessen, Kyle L. Johnson, Luis J. Jauregui,
Amelia A. Henriksen, Nathan R. Bianco, and Brad L. Boyce. 2023. High Throughput
EBSD Characterization of Additive Kovar. https://doi.org/10.18126/7d9u-edev
Available online at https://doi.org/10.18126/7d9u-edev.
[30] Ombretta Gaggi, Giacomo Quadrio, and Armir Bujari. 2019. Accessibility for
the visually impaired: State of the art and open issues. In 2019 16th IEEE Annual
Consumer Communications & Networking Conference (CCNC). IEEE, 1–6.
[31] GitHub. 2024. Referencing and citing content. https://docs.github.com/en/
repositories/archiving-a-github-repository/referencing-and-citing-content
[32] Heidelberg. 2020. Springer Nature continues to drive OA with launch of brand
new OA journal series. (2020). https://group.springernature.com/gp/group/
media/press-releases/springer-nature-discover-journal/18109908
[33] Amelia Henriksen. 2023. HawaiiCoast_GT: Curated AIS for Hawaii’s coast corre-
lated with ground truth incidents. https://doi.org/10.5281/zenodo.8253611
[34] Amelia Henriksen. 2024. Correlating real-world incidents with vessel traffic off
the coast of Hawaii, 2017–2020. Discover Oceans 1, 1 (2024), 1.
[35] Cynthia Hudson-Vitale and Heather Moulaison-Sandy. 2019. Data Management
Plans: A Review. DESIDOC Journal of Library & Information Technology 39, 6
(2019), 322–328.
[36] Abhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta,
Shanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal, and
Vitobha Munigala. 2020. Overview and importance of data quality for machine
learning tasks. In Proceedings of the 26th ACM SIGKDD international conference
on knowledge discovery & data mining. 3561–3562.
[37] Luis J Jauregui, J Elliott Fowler, Timothy Ruggles, Dale E Cillessen, Kyle L Johnson,
Shelley Williams, and Brad L Boyce. 2023. High-throughput EBSD Characteriza-
tion of Additively Manufactured Microstructures.
[38] Chenyue Jiao, Kai Li, and Zhichao Fang. 2023. How are exclusively data journals
indexed in major scholarly databases? An examination of four databases. Scientific
Data 10, 1 (2023), 737.
[39] NA Kalinin and NA Skvortsov. 2023. Difficulties of FAIR Principles Implementa-
tion in Cross-Domain Research Infrastructures. Lobachevskii Journal of Mathe-
matics 44, 1 (2023), 147–156.
[40] Diana Kapiszweski and Sebastian Karcher. 2020. Making research data accessible.
(2020).
[41] SI Kelly, Chaoran Du, Gabriel Rilling, and Mike E Davies. 2012. Advanced image
formation and processing of partial synthetic aperture radar data. IET signal
processing 6, 5 (2012), 511–520.
[42] Suntae Kim. 2018. Functional requirements for research data repositories. In-
ternational Journal of Knowledge Content Development & Technology 8, 1 (2018),
25–36.
[43] Maggie Krajewski. 2023. Creating a more accessible Sandia. https://www.sandia.
gov/labnews/2023/12/07/creating-a-more-accessible-sandia/ Accessed on March
18, 2024.
[44] Ignasi Labastida and Thomas Margoni. 2020. Licensing FAIR data for reuse. Data
Intelligence 2, 1-2 (2020), 199–207.
[45] Anna-Lena Lamprecht, Leyla Garcia, Mateusz Kuzak, Carlos Martinez, Ricardo
Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, Stephanie Van
De Sandt, Jon Ison, Paula Andrea Martinez, et al .2020. Towards FAIR principles
for research software. Data Science 3, 1 (2020), 37–59.
[46] Sarah Lebovitz, Natalia Levina, and Hila Lifshitz-Assaf. 2021. Is AI ground truth
really true? The dangers of training and evaluating AI tools based on experts’
know-what. MIS quarterly 45, 3 (2021).
[47] Kyong-Ho Lee, Oliver Slattery, Richang Lu, Xiao Tang, and Victor McCrary. 2002.
The state of the art and practice in digital preservation. Journal of research of the
National institute of standards and technology 107, 1 (2002), 93.
[48] Dawei Lin, Jonathan Crabtree, Ingrid Dillo, Robert R Downs, Rorie Edmunds,
David Giaretta, Marisa De Giusti, Hervé L’Hours, Wim Hugo, Reyna Jenkyns,
et al.2020. The TRUST Principles for digital repositories. Scientific Data 7, 1
(2020), 1–5.
[49] Kiran Maharana, Surajit Mondal, and Bhushankumar Nemade. 2022. A review:
Data pre-processing and data augmentation techniques. Global Transitions Pro-
ceedings 3, 1 (2022), 91–99.
[50] Wilhelm F Maier, Klaus Stoewe, and Simone Sieg. 2007. Combinatorial and
high-throughput materials science. Angewandte chemie international edition 46,
32 (2007), 6016–6067.
[51] John C Mankins et al .1995. Technology readiness levels. White Paper, April 6,
1995 (1995), 1995.
[52] Shangbo Mao, Enmei Tu, Guanghao Zhang, Lily Rachmawati, Eshan Rajabally,
and Guang-Bin Huang. 2018. An automatic identification system (AIS) database
for maritime trajectory prediction and data mining. In Proceedings of ELM-2016.
6521KDD ’24, August 25–29, 2024, Barcelona, Spain Amelia Henriksen and Miranda Mundt
Springer, 241–257.
[53] Tom M Mitchell. 1999. Machine learning and data mining. Commun. ACM 42, 11
(1999), 30–36.
[54] Azadeh Nazemi. 2013. A method to provide accessibility for visual components
to vision impaired. International Journal of Recent Trends in Human Computer
Interaction 4, 1 (2013), 54–69.
[55] Hellen M Ndegwa, Emily Bosire, and Damaris Odero. 2022. Evaluating Institu-
tional Repositories’(IR) capabilities for long-term preservation with a focus on
content, file format and metadata practices in selected public university libraries
in Kenya. Library Philosophy & Practice (2022).
[56] David Nicholas, Ian Rowlands, Anthony Watkinson, David Brown, and Hamid R
Jamali. 2012. Digital repositories ten years on: what do scientific researchers think
of them and how do they use them? Learned publishing 25, 3 (2012), 195–206.
[57] Jamie R Nuñez, Christopher R Anderton, and Ryan S Renslow. 2018. Optimizing
colormaps with consideration for color vision deficiency to enable accurate
interpretation of scientific data. PloS one 13, 7 (2018), e0199239.
[58] Maulishree Pandey, Vaishnav Kameswaran, Hrishikesh V Rao, Sile O’Modhrain,
and Steve Oney. 2021. Understanding accessibility and collaboration in program-
ming for people with visual impairments. Proceedings of the ACM on Human-
Computer Interaction 5, CSCW1 (2021), 1–30.
[59] Maulishree Pandey, Steve Oney, and Andrew Begel. 2024. Towards Inclusive
Source Code Readability Based on the Preferences of Programmers with Visual
Impairments. In Proceedings of the 2024 CHI Conference on Human Factors in
Computing Systems.
[60] Susan Wells Parham, Jake Carlson, Patricia Hswe, Brian Westra, and Amanda
Whitmire. 2016. Using data management plans to explore variability in research
data management practices across domains. International Journal of Digital
Curation 11, 1 (2016), 53–67.
[61] Norman Paskin. 2010. Digital object identifier (DOI®) system. Encyclopedia of
library and information sciences 3 (2010), 1586–1592.
[62] Raven J Peterson. 2021. We need to address ableism in science. Molecular biology
of the Cell 32, 7 (2021), 507–510.
[63] Marta Poblet, Amir Aryani, Paolo Manghi, Kathryn Unsworth, Jingbo
Wang, Brigitte Hausstein, Sunje Dallmeier-Tiessen, Claus-Peter Klas, Pompeu
Casanovas, and Victor Rodriguez-Doncel. 2018. Assigning creative commons
licenses to research metadata: Issues and cases. In AI Approaches to the Complexity
of Legal Systems: AICOL International Workshops 2015-2017: AICOL-VI@ JURIX
2015, AICOL-VII@ EKAW 2016, AICOL-VIII@ JURIX 2016, AICOL-IX@ ICAIL 2017,
and AICOL-X@ JURIX 2017, Revised Selected Papers 6. Springer, 245–256.
[64] Christopher Power and Helmut Jürgensen. 2010. Accessible presentation of
information for people with visual disabilities. Universal Access in the Information
Society 9 (2010), 97–119.
[65] Mohan Raja Pulicharla. 2024. Data Versioning and Its Impact on Machine Learning
Models. Journal of Science & Technology 5, 1 (2024), 22–37.[66] Marie-Fanny Racault, Shubha Sathyendranath, and Trevor Platt. 2014. Impact of
missing data on the estimation of ecological indicators from satellite ocean-colour
time-series. Remote sensing of environment 152 (2014), 15–28.
[67] Brian J Sauser, Richard R Reilly, and Aaron J Shenhar. 2009. Why projects fail?
How contingency theory can provide new insights–A comparative analysis of
NASA’s Mars Climate Orbiter loss. International Journal of Project Management
27, 7 (2009), 665–679.
[68] Harvard Medical School. 2024. Harvard Dataverse. https://datamanagement.hms.
harvard.edu/share-publish/data-repositories/harvard-dataverse
[69] Aisha Siddiqa, Ibrahim Abaker Targio Hashem, Ibrar Yaqoob, Mohsen Marjani,
Shahabuddin Shamshirband, Abdullah Gani, and Fariza Nasaruddin. 2016. A
survey of big data management: Taxonomy and state-of-the-art. Journal of
Network and Computer Applications 71 (2016), 151–166.
[70] Alexa Siu, Gene SH Kim, Sile O’Modhrain, and Sean Follmer. 2022. Supporting
accessible data visualization through audio data narratives. In Proceedings of the
2022 CHI Conference on Human Factors in Computing Systems. 1–19.
[71] Nicholas Smale, Gareth Denyer, Kathryn Unsworth, Elise Magatova, and Daniel
Barr. 2020. A review of the history, advocacy and efficacy of data management
plans. International Journal of Digital Curation (2020).
[72] Besiki Stvilia and Dong Joon Lee. 2024. Data quality assurance in research data
repositories: a theory-guided exploration and model. Journal of Documentation
(2024).
[73] Luke Taylor and Geoff Nitschke. 2018. Improving deep learning with generic
data augmentation. In 2018 IEEE symposium series on computational intelligence
(SSCI). IEEE, 1542–1547.
[74] K Ayberk Tecimer, Eray Tüzün, Cansu Moran, and Hakan Erdogmus. 2022. Clean-
ing ground truth data in software task assignment. Information and Software
Technology 149 (2022), 106956.
[75] Stan Walker, Phil Schloss, Charles R Fletcher, Charles A Vogel, and Randall C
Walker. 2005. Visual-syntactic text formatting: A new method to enhance online
reading. Reading Online 8, 6 (2005), 1096–1232.
[76] Wendy Walker and Teressa Keenan. 2015. Going beyond availability: Truly
accessible research data. Journal of Librarianship and Scholarly Communication
3, 2 (2015).
[77] William H. Walters. 2020. Data journals: incentivizing data access and documen-
tation within the scholarly communication system. Insights: the UKSG journal
33, 1 (2020).
[78] Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Apple-
ton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino
da Silva Santos, Philip E Bourne, et al .2016. The FAIR Guiding Principles for
scientific data management and stewardship. Scientific data 3, 1 (2016), 1–9.
[79] Takashi Yoshioka, Sliman J Bensmaia, Jim C Craig, and Steven S Hsiao. 2007.
Texture perception through direct and indirect touch: An analysis of perceptual
space for tactile textures in two modes of exploration. Somatosensory & motor
research 24, 1-2 (2007), 53–70.
[80] Zenodo. 2024. Frequently Asked Questions. https://help.zenodo.org/faq/
6522