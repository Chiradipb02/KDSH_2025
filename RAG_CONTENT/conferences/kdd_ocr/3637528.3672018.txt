Learn Together Stop Apart:
An Inclusive Approach to Ensemble Pruning
Bulat Ibragimov
ibrbulat@yandex.ru
Moscow Institute of Physics and Technology
Moscow, Russian Federation
Artificial Intelligence Research Institute (AIRI)
Moscow, Russian FederationGleb Gusev
gleb57@gmail.com
Sber AI Lab
Moscow, Russian Federation
ABSTRACT
Gradient Boosting is a leading learning method that builds ensem-
bles and adapts their sizes to particular tasks, consistently delivering
top-tier results across various applications. However, determining
the optimal number of models in the ensemble remains a critical yet
underexplored aspect. Traditional approaches assume a universal
ensemble size effective for all data points, which may not always
hold true due to data heterogeneity.
This paper introduces an adaptive approach to early stopping
in Gradient Boosting, addressing data heterogeneity by assigning
different stop moments to different data regions at inference time
while still training a common ensemble on the entire dataset. We
propose two methods: Direct Supervised Partition (DSP) and Indi-
rect Supervised Partition (ISP). The DSP method uses a decision tree
to partition the data based on learning curves, while ISP leverages
the dataset’s geometric and target distribution characteristics.
An effective validation protocol is developed to determine the
optimal number of early stopping regions or detect when the het-
erogeneity assumption does not hold. Experiments using state-
of-the-art implementations of Gradient Boosting, LightGBM, and
CatBoost, on standard benchmarks demonstrate that our methods
enhance model precision by up to 2%, underscoring the significance
of this research direction. This approach does not increase com-
putational complexity and can be easily integrated into existing
learning pipelines.
CCS CONCEPTS
•Computing methodologies →Boosting; Regularization ; Clas-
sification and regression trees.
KEYWORDS
Ensemble, Boosting, Regularization, Early Stopping, Decision Tree
ACM Reference Format:
Bulat Ibragimov and Gleb Gusev. 2024. Learn Together Stop Apart: an
Inclusive Approach to Ensemble Pruning. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3672018
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36720181 INTRODUCTION
Despite the rapid advancement of deep neural networks, classi-
cal machine learning algorithms like Gradient Boosting (GB) [ 18]
continue to outperform in several areas. Particularly, GB allows to
obtain high-quality models on structured (or tabular) data with no
multimedia (e.g., images, audio, videos), with categorical features,
noisy features and labels, and missing data [ 1,27,45]. Also, the
undoubted advantage of the boosting method is the relatively low
computational cost of training and inference [ 15]. For these reasons,
Gradient Boosting is widely used in ranking [ 7], recommender sys-
tems [ 9], automatic machine learning [ 25], and many other tasks
[29, 40, 41].
In recent years, new options and hyperparameters have been
proposed for GB, influencing its performance [ 23,24]. However,
one critical aspect affecting the effectiveness of GB, the choice
of the number of ensemble members (or the number of boosting
iterations), remains largely understudied. Larger models can reveal
complex dependencies in the data but are more time-consuming
and prone to overfitting on noisy datasets [ 19]. Conversely, smaller
models perform better on such data. The standard approach to this
problem, known as ‘early stopping’, uses a hold-out validation set
to determine the ensemble size, selecting the model’s prefix with
the optimal validation score.
The standard early stopping method has a significant yet surpris-
ingly understudied weakness. It assumes a ‘one-fits-all’ universal
ensemble size equally effective for all data points. However, this
is often not the case, as the learning task can consist of different
subtasks, each corresponding to different regions in the input space
and functional dependencies. Some regions may have complex
surfaces requiring many boosting rounds for convergence, while
others may reveal simple but noisy dependencies where overfitting
occurs much earlier. As a result, the standard early stopping often
compromises simple and complex areas, leading to models, which
mix overfitted and underfitted regions.
Recognizing this limitation, we propose a new approach to early
stopping in GB based on an adaptive choice of the optimal size of
the ensemble. As in the standard version of GB [ 18], we train one
sequence of learners in an ensemble. However, our algorithm adapts
to the heterogeneity of the data in the feature space and assigns
different numbers of models to different data regions at inference
time. Specifically, we build an additional ‘partition model’, which
sequentially divides the input space into regions of presumably
homogeneous complexity and representativity of data. Simultane-
ously, it learns an early stopping model, which optimizes the size
of the trained ensemble for each region individually. This adaptive
1166
KDD ’24, August 25–29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
approach allows for a more nuanced and compelling model that
can better handle the complexities of different data regions.
However, building such a model is a tricky task. We demonstrate
in Section 4.2 that straightforward approaches such as direct predic-
tion of the best iteration have fundamental drawbacks and do not
work. Another challenge lies in controlling the trade off between
partition complexity and the generalization ability of the early
stopping model. These are, in some sense, meta-hyperparameters
over hyperparameters since the partition model defines ensemble
sizes. To manage this construction, we have developed a specific
two-level cross-validation scheme.
Despite the apparent complexity, the proposed methods incur
meager computational costs and can be easily incorporated into
any existing learning pipeline and applied to any learning task with
arbitrary loss. We apply the proposed approach to state-of-the-art
open-source GB algorithms, LightGBM and CatBoost, and demon-
strate its ability to outperform consistently on standard publicly
available benchmarks for GB. We show that the described prob-
lem of the universal stopping moment highly affects the quality of
trained models. To our knowledge, this is the first research devoted
to effective adaptive early stopping in GB, and we anticipate that
this paper will stimulate further exploration of the GB algorithm.
The rest of the paper is organized as follows. Sections 3 and 2
briefly introduce notation and concepts of traditional Gradient
Boosting and early stopping algorithms. This is followed by the
core of the paper in Section 4, where we discuss some possible adap-
tive early stopping methods and their limitations and outline our
primary algorithm. Section 5 contains the description of the early
stopping validation protocol, a crucial component of the algorithm
that allows to obtain optimal stopping model parameters. The final
part of the paper is the experimental section, where we demon-
strate that our approach yields stable, significant improvements
in terms of quality on standard benchmarks and contemporary
implementations of Gradient Boosting.
2 RELATED WORK
2.1 Ensemble pruning
Pruning is a term often used to describe various techniques aimed
at compressing models for more efficient storage and inference
complexity. The classic work on this task by Margineantu and
Dietterich (1997) [ 32] compared five different pruning methods
applied to the boosting algorithm. In most cases, pruned models
were able to maintain or even enhance the original quality while
achieving a moderate reduction in size. Contemporary pruning
techniques are primarily predicated on the observation that similar
learners in the ensemble tend to duplicate dataset information,
suggesting they can be eliminated from a model sequence [ 5,26].
There have also been efforts to frame ensemble pruning as an
optimization problem, with solutions sought by applying genetic
algorithms [46] or semi-definite programming [44].
Another approach to prune ensemble is to prune individual es-
timators while enhancing the same (or even better) quality. For
instance, Friedman’s work on rule ensembles [ 20] introduces a
method for predictive learning that combines the interpretability
of rule-based models with the performance of ensemble methods.Similarly, Liu and Mazumder’s ForestPrune [30] focuses on reduc-
ing the complexity of ensemble models through depth-pruning of
decision trees.
A number of studies [ 12,13] have tackled the issue of adaptive
online pruning within the context of Multiple Classifier Systems. In
these settings, classifiers are learned independently and selected via
meta-learning approaches. Oliveira et al. (2017) [ 34] and Hernandez
et al. (2008) [ 21] propose an instance–wise pruning methods that
allow for halting some models at inference time, while Soto et
al. (2014) [ 38] investigate both static (training time) and dynamic
(inference time) pruning in AdaBoost.
2.2 Gradient Boosting early stopping
Contrary to some other ensemble methods, including bagging, GB is
prone to overfitting when the ensemble size is large. Therefore, the
control of the number of boosting steps is primarily a regularization
technique [16]. Specifically, the original paper by Friedman (2001)
[18] provides direct guidance on tuning the number of models in
the ensemble (Section 5): "For additive expansions (2), a natural
regularization parameter is the number of components M".
Given that the size of GB is responsible for the expressiveness
of the ensemble, one of the ideas proposed in the literature [ 6,
33] is to penalize model complexity, for instance, through AIC-
based methods by approximating the ensemble’s degrees of freedom.
Some studies employ generalization bounds of the algorithm using
VC-dimension [ 17], Rademacher complexity [ 11], or in the PAC
setting [ 42,43]. While these methods do not necessitate separate
validation control, they are not applicable in real-world tasks in
most cases as the obtained bounds are distribution-agnostic.
The standard approach of early stopping used in all modern GB
implementations utilizes the straightforward "waiting" concept. If
the validation quality does not improve for some "reasonable" num-
ber of iterations, then the training must be stopped (see, e.g., [ 10]).
In this paper, we adopt a standard early stopping scheme described
in Margineantu and Dietterich (1997) [ 32]: we shrink the model to
the first𝑀learners that yield the best validation score. Neverthe-
less, unlike all previous works on Gradient Boosting, instead of a
universal constant, we strive to select this number adaptively for
different regions of the input space, considering the distribution of
the training data.
3 BACKGROUND
In this section, we introduce necessary notations and briefly discuss
the fundamental concepts related to Gradient Boosting and cross-
validation for the sake of independent reading.
3.1 Gradient Boosting
LetS={𝒙𝑖,𝑦𝑖}𝑛
𝑖=1be a sample from some fixed but unknown dis-
tribution𝑃(𝒙,𝑦), where 𝒙𝑖=(𝑥1
𝑖,...,𝑥𝑚
𝑖)∈Xis an𝑚-dimensional
feature representation and 𝑦𝑖∈Yis a target value of the 𝑖-th obser-
vation. We consider the learning problem that consists in construct-
ing a function 𝐹:X→Yminimizing the expected target prediction
error, which is calculated using a loss function 𝐿:Y×Y→R+:
L(𝑃,𝐹):=E(𝒙,𝑦)∼𝑃[𝐿(𝐹(𝒙),𝑦)]→ min
𝐹.
1167Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD ’24, August 25–29, 2024, Barcelona, Spain
Since the distribution 𝑃is not given, the task reduces to empirical
risk minimization problem:
ˆL(S,𝐹)=ˆE(𝒙,𝑦)∼S[𝐿(𝐹(𝒙),𝑦)]=1
𝑛𝑛∑︁
𝑖=1𝐿(𝐹(𝒙𝑖),𝑦𝑖)→ min
𝐹
The ability to achieve the smaller value of the empirical risk is
bounded by the complexity of the set Ffrom which the desired
function𝐹∈F is selected. Gradient Boosting (GB) increases the
expressiveness of the learned model by building (an ensemble )𝐹𝐵
of size𝐵as a weighted sum of base functions {𝑓1,𝑓2,...,𝑓𝐵}⊂F :
𝐹𝐵(𝒙)=𝐵∑︁
𝑖=1𝛼𝑖𝑓𝑖(𝒙) (1)
When the set of available base functions Fis closed under scalar
multiplication, multipliers 𝛼𝑖are equal:∀𝑖 𝛼𝑖=𝛼, where𝛼is a
hyperparameter of the GB algorithm called learning rate. Having
the first𝑡−1terms constructed, the learning algorithm aims to
select the next function 𝑓𝑡sequentially as a solution of:
ˆL(S,𝐹𝑡−1+𝑓𝑡)=1
𝑛𝑛∑︁
𝑖=1[𝐿(𝐹𝑡−1(𝒙𝑖)+𝑓𝑡(𝒙𝑖),𝑦𝑖)]→ min
𝑓𝑡
The approximate solution of the latter equation in GB is usually
constructed as follows. Algorithm calculates first and second order
derivatives of ˆLat the point 𝐹𝑡−1w.r.t. predicted values ˆ𝑦:
𝑔𝑡
𝑖=𝜕𝐿(ˆ𝑦𝑖,𝑦𝑖)
𝜕ˆ𝑦𝑖ˆ𝑦𝑖=𝐹𝑡−1(𝒙𝑖);ℎ𝑡
𝑖=𝜕2𝐿(ˆ𝑦𝑖,𝑦𝑖)
𝜕ˆ𝑦2
𝑖ˆ𝑦𝑖=𝐹𝑡−1(𝒙𝑖),
and trains the following least squares estimator to gradient step
in the functional space:
𝑓𝑡=arg min
𝑓∈F𝑁∑︁
𝑖=1ℎ𝑡
𝑖(®𝑥𝑖,𝑦𝑖) 
𝑓(®𝑥𝑖)− 
−𝑔𝑡
𝑖(®𝑥𝑖,𝑦𝑖)
ℎ𝑡
𝑖(®𝑥𝑖,𝑦𝑖)!!2
,
see [8] for details.
3.2 Early stopping via cross-validation
Since quality estimation based on the train set used in the learning
process is biased in comparison to unseen data [ 35], it is conven-
tional to use an independent validation set to control the general-
ization ability of the algorithm. The whole dataset Sis split into
two disjoint setsS𝑡𝑟𝑎𝑖𝑛 andS𝑣𝑎𝑙𝑖𝑑 , where the first one is used for
learning and the latter one is used for measuring a quality estimate.
Such quality estimate is often highly dependent on the partic-
ular train-validation split and therefore can be noisy. A common
way to tackle this issue is to use cross-validation [39] method: split
the dataSinto𝑘disjoint subsets or folds(S1,S2,...,S𝑘)of ap-
proximately equal size s.t. S=𝑘Ã
𝑖=1S𝑖and perform 𝑘rounds of
training–evaluation cycle using S−𝑖:=S\S𝑖as the training set
andS𝑖as the validation data for each 𝑖∈{1,2,...,𝑘}. In this way,
we get𝑘different𝐵-sized models(
𝐹𝑗
𝐵(𝒙)=𝐵∑︁
𝑖=1𝛼𝑓𝑗
𝑖(𝒙)}𝑘
𝑗=1)𝑘
𝑗=1
learned by𝑘training sets{S−𝑗}𝑘
𝑗=1.
At𝑗-th cross–validation step, we apply all the prefixes 𝐹𝑗
𝑖of
the model𝐹𝑗
𝐵to validation setS𝑗and obtain quality estimators
𝒍𝑗=(𝑙(1)
𝑗,𝑙(2)
𝑗,...,𝑙(𝐵)
𝑗), where
𝑙(𝑏)
𝑗=1
|S𝑗|∑︁
(𝒙,𝑦)∈S 𝑗𝐿
𝐹𝑗
𝑏(𝒙),𝑦
.
The aggregated estimator 𝒍=1
𝑘Í𝒍𝑗is further used to define
ˆ𝐵:=arg min
1≤𝑏≤𝐵𝑙(𝑏).
The final model shrinked to the first ˆ𝐵iterations provides an
estimator with the test quality close to
𝐸=min
1≤𝑏≤𝐵E(𝒙,𝑦)∼𝑃[𝐿(𝐹𝑏(𝒙),𝑦)].
4 ADAPTIVE EARLY STOPPING
The standard early stopping method described in Section 3.2 presents
a challenge. An ideal goal could be the optimisation of the number
of iterations 𝑏(𝒙)to each individual 𝒙, what would lead to expected
loss
E(𝒙,𝑦)∼𝑃 min
1≤𝑏(𝒙)≤𝐵[𝐿(𝐹𝑏(𝒙)(𝒙),𝑦)]
However, the expected loss theoretically achievable with the
standard early stopping approach can be greater because of the
apparent inequality:
𝐸=min
1≤𝑏≤𝐵E(𝒙,𝑦)∼𝑃[𝐿(𝐹𝑏(𝒙),𝑦)]≥
E(𝒙,𝑦)∼𝑃 min
1≤𝑏(𝒙)≤𝐵[𝐿(𝐹𝑏(𝒙)(𝒙),𝑦)],(2)
This mathematical fact indicates that the existing early stopping
scheme may not be the most effective approach. To address this, we
propose an adaptive early stopping method. This method involves
an adaptive selection of individual ensemble sizes for specific areas.
By doing so, we aim to achieve better quality by eliminating the
theoretical gap given by the aforementioned inequality 2. In the
subsequent sections, we will delve into potential approaches for
adaptive iteration count selection. We will discuss how to man-
age its impact and explore its advantages over the standard early
stopping method.
4.1 Main idea
Consider the scenario where the input space Dis partitioned into
𝐶distinct regions, represented as (D1,D2,...,D𝐶).
This partitioning is done such that all samples within a given
regionD𝑖are in close proximity to each other, either following the
same latent distribution or sharing similar geometric properties.
It’s important to note that this partitioning is independent of the
division induced by cross-validation.
1168KDD ’24, August 25–29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Algorithm 1 Adaptive stopping procedure
Input:S=(𝑿,𝒚)
𝑓𝑜𝑙𝑑𝑠←(S 1,S2,...,S𝑘)←𝐶𝑣𝑆𝑝𝑙𝑖𝑡(𝑘,S)
𝑐𝑣𝑃𝑟𝑒𝑑←𝐶𝑣𝑃𝑟𝑒𝑑𝑖𝑐𝑡(𝑓𝑜𝑙𝑑𝑠)
𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛←(D 1,D2,...,D𝐶)←𝐺𝑒𝑡𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛(S)
𝑏𝑒𝑠𝑡𝐼𝑡𝑒𝑟←𝐸𝑣𝑎𝑙𝐵𝑒𝑠𝑡𝐼𝑡𝑒𝑟(𝑓𝑜𝑙𝑑𝑠,𝑐𝑣𝑃𝑟𝑒𝑑,𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 )
𝑓𝑖𝑛𝑎𝑙𝑀𝑜𝑑𝑒𝑙←𝑇𝑟𝑎𝑖𝑛(𝑿,𝒚,𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛,𝑏𝑒𝑠𝑡𝐼𝑡𝑒𝑟 )
return𝑓𝑖𝑛𝑎𝑙𝑀𝑜𝑑𝑒𝑙
We assume that(D1,D2,...,D𝐶)forms a clustering, where data
points within the same cluster D𝑖exhibit similar behavior during
ensemble training. This implies that the optimal number of boosting
iterations ˆ𝐵𝑖estimated forD𝑖could significantly differ from that
estimated forD𝑗(𝑗≠𝑖). Drawing an analogy with inequality 2,
we infer that selecting the ensemble size based on partition D,
where the size is individually chosen for each cluster D𝑖, could
yield better quality compared to a single “universal” common size.
This is represented by the following inequality:
ED𝑖∼D min
1≤𝑏≤𝐵E[𝐿(𝐹𝑏(𝒙),𝑦)|D𝑖]≤ min
1≤𝑏≤𝐵E𝑃[𝐿(𝐹𝑏(𝒙),𝑦)](3)
Setting𝐶=𝑛could potentially achieve the theoretical lower
bound of the left-hand side of Equation 3. However, the ensemble
size ˆ𝐵𝑖will be optimized based on the empirical estimation of the
loss, and an increase in 𝐶is accompanied by the growth of the vari-
ance of this estimation for each region D𝑖. Therefore, the number
of regions should be chosen judiciously, a topic we delve into later
in the text.
The upper-level training algorithm comprises four steps:
(1) Cross-validated training of 𝑘models;
(2)Distribution-based partitioning of the sample space into
(D1,D2,...,D𝐶);
(3)Selection of the optimal number of iterations (ˆ𝐵1,ˆ𝐵2,...,ˆ𝐵𝐶)
for each region obtained in step 2;
(4) Training of the final model on the entire training data.
A formal description of this process is presented in Algorithm 1.
4.2 Optimal point regression
One of the most straightforward ideas towards adaptive stopping in-
volves learning a regression model that predicts the optimal number
of terms in the trained ensemble for each example. In the context of
the general adaptive stopping framework (Algorithm 1), the func-
tion𝐺𝑒𝑡𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 should assign each data point to an individual
cluster, while 𝐸𝑣𝑎𝑙𝐵𝑒𝑠𝑡𝐼𝑡𝑒𝑟 should train a mapping from data points
to the best number of iterations (derived from 𝑐𝑣𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠 ma-
trix). This regression model is further applied to test examples to
estimate the early stopping moments.
However, this seemingly simple idea does not yield satisfactory
results in practice. Training a separate GB regression model of the
same size as the primary model, followed by its application on the
test sample and individual pruning, results in a degradation of the
prediction’s logloss by an average of 1.5% compared to the standard
non-adaptive scheme for selecting the number of trees.
Figure 1: Sample learning curves for Gradient Boosting on
the test set. The horizontal axis represents the number of
boosting iterations, while the vertical axis represents the
logloss value. Each colored line represents a learning curve for
a separate test instance, illustrating how the logloss changes
over iterations for individual data points.
Figure 2: Distribution of relative shift of best iteration predic-
tions. The horizontal axis represents the fraction values, and
the vertical axis represents the frequency of these values.
This is primarily due to the fact that the best iteration is known
to be a highly noisy target, given the complexity and partial random-
ness of the Gradient Boosting training process [ 19]. As visualized in
the learning curves (Figure 1) of test samples demonstrates that loss
histories appear chaotic and resemble realizations of some random
process. Consequently, each curve can inadvertently reach the min-
imum value at an arbitrarily late point. However, practical observa-
tions indicate that the general minimum point (following the trend
line) is bounded with some finite value. This makes the training of
an effective regressor a challenging task. Figure 2 demonstrates the
results of best iteration regression via a separate Gradient Boost-
ing model. Upon dividing the predictions of the trained regression
model by the actual best stopping moment, it is evident that at least
half of the samples are overestimated by approximately two times,
indicating that the selected stops are significantly suboptimal.
1169Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD ’24, August 25–29, 2024, Barcelona, Spain
4.3 Unsupervised partition
As previously mentioned at the beginning of this Section, the parti-
tioning of the data should reflect its internal structure to be sophis-
ticated enough to select an appropriate number of models. Under
the reasonable assumption that observations in close proximity
within the feature space also share similar properties, we can em-
ploy one of clustering algorithms such as KMeans [ 31], EM [ 14], or
agglomerative methods [ 37] to partition the data. This corresponds
to the function 𝐺𝑒𝑡𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 in Algorithm 1.
Preserving the initial geometry of the input space is crucial,
given that most modern implementations of Gradient Boosting
utilize Decision Trees [ 3] as base learners. Decision Trees construct
piecewise-constant approximations at each step, making it more
likely for closely located instances to fall into the same leaves
during training and inference, thereby leading to similar fits. The
unsupervised partitioning method provides control over the number
of partition regions and their sizes by setting the desired number
of clusters, and minimum sample count in each cluster.
However, when applied to real data, this method exhibits sev-
eral drawbacks Firstly, clustering does not perform well with data
containing non-numeric categorical features. Numeric encoding
of high-cardinality categorical features results in a sparse input
space, dramatically impacting the capacity of clusterization. Sec-
ondly, unsupervised partitioning does not take into account the
labels of the data points, which could provide valuable information
about the required number of boosting steps. Lastly, some advanced
clustering algorithms require high computational costs, posing a
bottleneck during model training.
To validate the above statements and assumptions, we utilized
large numeric datasets, namely Higgs and HEPMASS, and compared
KMeans clustering with the algorithm proposed in this paper. The
unsupervised approach proved to be a viable choice for the adaptive
pruning strategy, but it was strictly inferior (as detailed in the
experiments Section 6) to the supervised method described in the
subsequent subsection.
4.4 Direct Supervised Partition (DSP)
To circumvent the issues outlined in the previous section, the parti-
tion unit (function 𝐺𝑒𝑡𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 in Algorithm 1) should be scalable,
interpretable in terms of the subspaces it creates, and capable of
handling heterogeneous feature input. The Decision Tree model
emerges as a suitable candidate as it fulfills all these properties: its
training algorithm is parallelizable and not memory consuming
[36], the cluster manifolds it constructs are similar to the ones built
by base learners, and it supports efficient methods for handling
categorical features [35].
Our objective is to identify data regions with similar training
properties, and to achieve this, we can utilize the validation learn-
ing curves to partition the dataset. The core idea of our method
is to train a clustering model that segments the feature space into
regions using the learning curves as a target. However, a naive
multi-regression models based on the MSE loss may identify sim-
ilar learning curves, but the clusters chosen may not necessarily
exhibit diversified stopping moments. The true goal is to discover a
partition that will benefit the most from assigning individual stopsto each cluster. Formally speaking, the goal is to find a partition
P∗=(D∗
1,D∗
2,...,D∗
𝐶)such that:
P∗=arg min
P∑︁
D𝑘∈Pmin
1≤𝑏≤𝐵∑︁
𝒙𝑖∈D𝑘𝑙𝑖,𝑏,
where𝑙𝑖,𝑏:=𝐿(𝐹𝑏(𝒙𝑖),𝑦𝑖)and(𝒙𝑖,𝑦𝑖)∈S .
To achieve this, we train a decision tree as clustering model. At
each node of the tree, we seek for a split (𝐿,𝑅)which maximizes
the following score:
S(𝐿,𝑅):=−©­
«min
1≤𝑏≤𝐵∑︁
𝒙𝑖∈𝐿𝑙𝑖,𝑏+min
1≤𝑏≤𝐵∑︁
𝒙𝑖∈𝑅𝑙𝑖,𝑏ª®
¬, (4)
Figure 3 provides a visual representation of how the partition-
ing operates for one of the benchmarks. The graphs validate the
hypothesis that there exist clusters where the optimal stopping
points can vary considerably. Interestingly, while some clusters
may continue to train (as depicted in the first three out of the five
plots in the figure), others may have already reached the point of
overfitting. Moreover, this characteristic is generalizable, meaning
it is inherent in the nature of the data. This is evidenced by the sim-
ilarity of the learning curves and breakpoints for both the training
and test sets. In other words, the patterns observed in the training
data are also reflected in the test data, indicating a consistent un-
derlying structure. This consistency is crucial as it suggests that
the model’s performance on the training data is a reliable indicator
of its performance on unseen data.
It is also worth noting that it is possible to optimize any quality
metric, not just the loss function. In the general case, it is sufficient
to store predictions from prefixes of the ensemble and use them
to calculate a particular metric function at each split. In particular,
storing predictions instead of learning curves can aid in directly
optimizing the 𝑅𝑂𝐶−𝐴𝑈𝐶 score, an example of a non-summable
metric.
The complexity of calculating the score above is linear on the size
of the entire (unpruned) ensemble, resulting in a full tree growing
complexity of 𝑂(𝑛𝑚𝐵𝑑), which is comparable to the GB training
complexity. Additionally, the method requires storing learning his-
tories of size 𝐵for each validation data point. Given that these
costs may be prohibitive in many real-world tasks (e.g., where large
ensembles are required), we propose to use values from selected
iterations, e.g., 1, 2, 4, 7, 11, ... (the step increases by one), which
reduces the complexity by a factor of√
𝐵. Here we assume that
as the number of iterations increases, predictions change less, so
points from earlier iterations carry more information about the
curves. Furthermore, the proposed scheme significantly reduces
time
𝑂(𝑛𝑚√
𝐵𝑑)
and memory costs
𝑂(𝑛√
𝐵)
, making them
moderate compared to the GB training algorithm.
4.5 Indirect Supervised Partition (ISP)
Building on the concept from the previous section, let’s consider an
extreme case of the method where the number of iterations used
to construct a clustering tree is set to 1. This results in a method
that relies solely on the characteristics of the dataset. In essence,
the partitioning process is reduced to training a single decision tree
1170KDD ’24, August 25–29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Figure 3: Clustering results obtained via a decision tree trained with the objective function from Equation 4. The horizontal
axis represents the number of boosting iterations, and the vertical axis represents the logloss value. Each line shows the logloss
reduction over iterations for a distinct cluster of the same dataset. Bold dots indicate the estimated (val) and actual (test)
stopping points.
on the initial training samples and targets. Each leaf of this tree is
then designated as a separate data cluster, forming the partition.
The tree learning process leverages both the geometry of the
feature space and the target distribution in the leaves to split the
data. This encourages the method to identify regions that are similar
in terms of feature representation and label, effectively utilizing all
available static information about the data.
This partition tree may be trained separately from the primary
boosting model and be the first booster in the ensemble. The latter
means that this step does not affect the training time. Moreover,
given that Gradient Boosting typically involves hundreds or even
thousands of trees, the time cost of using a separate partition model
is negligible.
Practical application of this method has shown that these clusters
are sufficiently effective in identifying regions with diversified
optimal stops, thereby enhancing the final quality of the model.
More details can be found in Section 6.
5 VALIDATION PROTOCOL
The methods proposed in this paper introduce two additional hy-
perparameters that can be adjusted: the count of clusters and the
minimum size of clusters. Both the DSP and ISP algorithms employ
a decision tree, and these hyperparameters are regulated by setting
limits on the maximum number of leaves and the minimum size of
each leaf in the decision tree.
We denoteD𝑖,𝑗=D𝑖∩S𝑗as the set of observations from the
𝑗-th fold that belong to the cluster D𝑖and𝑛𝑖,𝑗as the number of
observations in this set. A straightforward approach to evaluation
involves applying a cross-validation model which is trained on the
sampleS−𝑗, to the validation set S𝑗for any𝑗. This yields quality
estimators 𝒍𝑖,𝑗in the following form:
𝑙(𝑏)
𝑖,𝑗=1
𝑛𝑖,𝑗∑︁
(𝒙,𝑦)∈D 𝑖,𝑗𝐿
𝐹𝑗
𝑏(𝒙),𝑦
.
The resulting estimator 𝑳𝑖for each cluster 𝑖is derived from a
weighted sum of the corresponding cluster estimators across all
folds. This is mathematically represented as:𝐿(𝑏)
𝑖=Í𝑘
𝑗=1𝑛𝑖,𝑗·𝑙(𝑏)
𝑖,𝑗
Í𝑘
𝑗=1𝑛𝑖,𝑗.
then the optimal number of iterations for the 𝑖-th cluster is ˆ𝐵𝑖:=
arg min 𝑳𝑖and its cross–validation score equals to min𝑳𝑖. The total
complexity of the described procedure is 𝑂(𝐶(𝐵+𝑘)+𝑛𝐵), which
is relatively small compared to the ensemble training complexity
of𝑂(𝑛𝑚𝑑𝐵)(for𝑚binary features and trees of depth 𝑑) as cited
from Friedman [18].
Algorithm 2 Adaptive Early Stopping Selection
function𝐸𝑣𝑎𝑙𝐵𝑒𝑠𝑡𝐼𝑡𝑒𝑟 (𝑓𝑜𝑙𝑑𝑠 ,𝑐𝑣𝑃𝑟𝑒𝑑 ,𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 )
foreachD𝑖in𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 do
𝑳𝑖←®0{initialize a vector of 𝐵zeros}
𝑛𝑖←0
foreachS𝑗in𝑓𝑜𝑙𝑑𝑠 do
D𝑖,𝑗←D𝑖∩S𝑗
𝑛𝑖,𝑗←|D𝑖,𝑗|
Δ𝑖,𝑗←𝐸𝑣𝑎𝑙(𝑐𝑣𝑃𝑟𝑒𝑑[D𝑖,𝑗])·𝑛𝑖,𝑗
𝑳𝑖←𝑳𝑖+Δ𝑖,𝑗{elementwise vector sum}
𝑛𝑖←𝑛𝑖+𝑛𝑖,𝑗
end for
𝑳𝑖←𝑳𝑖/𝑛𝑖
𝑀𝑖←arg min 𝑳𝑖
end for
return{𝑀𝑖}
The quality assessment obtained in the way described above
tends to be biased and often provides an overly optimistic estimate.
This is particularly problematic when trying to determine an opti-
mal number of clusters, as the quality estimator tends to increase
monotonically with finer clustering.
To mitigate this issue in the DSP case, we introduce an additional
validation step to ensure that the partition will be beneficial. This
involves training the early stopping model and applying it to a
separate hold-out sample. If cross-validation is used during GB
1171Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD ’24, August 25–29, 2024, Barcelona, Spain
tuning, we have a learning curve for each training instance,allowing
us to use the entire sample again. Furthermore, DSP can employ
the minimal gain criterion - if the resulting score after a split does
not significantly improve upon the current score, the growth of
the tree should be halted. This ensures that the model does not
overfit the data and maintains a balance between complexity and
performance.
In the ISP case, when the first booster of the full model is utilized
and retraining the clustering tree is either not possible or deemed
too costly, we suggest using a cross-validation evaluation procedure
(Algorithm 3). This procedure is designed to prevent target leakage
and significantly reduce bias. For each fold S𝑞, we calculate an
optimal stopping moment for cluster 𝑖. This is achieved by averaging
evaluation metrics for all observations from cluster 𝑖that are not
part of foldS𝑞. More formally, we compute 𝑳𝑖,−𝑞as follows
𝐿(𝑏)
𝑖,−𝑞=Í
𝑗≠𝑞𝑛𝑖,𝑗·𝑙(𝑏)
𝑖,𝑗Í
𝑗≠𝑞𝑛𝑖,𝑗.
This equation represents the weighted average loss of the 𝑏-th
iteration of the model trained on all folds excluding the 𝑞-th fold
and cluster𝑖. We apply the function 𝐸𝑣𝑎𝑙𝐵𝑒𝑠𝑡𝐼𝑡𝑒𝑟 to all folds except
the𝑞-th one, effectively ignoring S𝑞from𝑓𝑜𝑙𝑑𝑠 . After this step, we
obtain a set of optimal stopping moments ( ˆ𝐵𝑞
1,...,ˆ𝐵𝑞
𝐶)estimated on
S−𝑞.
We then useS𝑞as a validation set to assess the quality of pre-
dicted optimal stopping moments ( ˆ𝐵𝑞
1,...,ˆ𝐵𝑞
𝐶). By averaging the
results obtained over all folds S𝑞, we derive a more accurate esti-
mation of the quality of clustering.
This estimation is used to select the number and size of clusters
and to estimate the potential profit of applying the adaptive stop-
ping procedure. All of this is achieved with a minor additional time
consumption relative to the training time of the ensemble model.
There is still some bias because fold 𝑆𝑞is used both to train
models applied toS−𝑞and to estimate the performance of stopping
points. However, the desired property of not using the same set for
both tuning and evaluating ˆ𝐵is satisfied and allows us to obtain
useful estimations. Objective of training
Algorithm 3 Evaluation Procedure
function𝐸𝑣𝑎𝑙𝑢𝑎𝑡𝑒 (𝑓𝑜𝑙𝑑𝑠 ,𝑐𝑣𝑃𝑟𝑒𝑑 ,𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 )
forS𝑞←𝑓𝑜𝑙𝑑𝑠 do
{𝑀𝑞
𝑖}←𝐸𝑣𝑎𝑙𝐵𝑒𝑠𝑡𝐼𝑡𝑒𝑟(𝑓𝑜𝑙𝑑𝑠\S𝑞,𝑐𝑣𝑃𝑟𝑒𝑑,𝑝𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛)
𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠 𝑞←𝑐𝑣𝑃𝑟𝑒𝑑[S𝑞]
forD𝑖←𝑝do
Shrink(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠 𝑞[S𝑞∩D𝑖],𝑀𝑞
𝑖)
end for
𝑳𝑞=𝐸𝑣𝑎𝑙(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠 𝑞)
end for
return𝑀𝑒𝑎𝑛({𝑳𝑞})
6 EXPERIMENTS
In this section, we conduct numerical experiments to evaluate the
effectiveness of our proposed framework [ 22] and corroborate theassertions made in Section 4. Our analysis utilizes two widely-
used open-source Gradient Boosting libraries: LightGBM [ 28] and
CatBoost [4].
Following the methodology outlined in [ 35], which includes us-
ing the same hyperparameter set and tuning process, each model is
fine-tuned using a 5-fold cross-validation scheme over 50 iterations
of Hyperopt [ 2]. The specific parameters and grid used for tuning
can be found in Appendix. To optimize the quality of the pruned
ensemble during hyperparameter tuning, each Hyperopt iteration
is supplemented by a greedy search to identify the optimal number
of trees.
The models are tuned with a maximum of 𝐵=5,000boosting
iterations. This number was chosen because strong boosting models
typically require hundreds or thousands of trees in the ensemble.
It’s important to note that, although the model was tuned to reach
an optimum at 𝐵iterations, the performance of the model may
continue to improve beyond this point. This is why we increase
the total number of trees to 𝐵′=2𝐵=10,000for subsequent
experiments. This ensures full convergence of all models and al-
lows for the possibility of further improvement beyond the initially
identified optimum.
To compare the quality of the standard early stopping approach
and those proposed in this paper, we hold out 20% of samples from
each dataset as the test set. A 5-fold stratified cross-validation is
employed to ascertain the optimal stopping moment. We use the
standard pruning algorithm as a baseline and contrast it with the
methods proposed in Section 4.
For the DSP algorithm, we store "sparse" learning histories (as
described in Section 4.4) for each data point. We then perform 5-
fold cross-validation (involving 5 retrainings of the clustering tree,
not the entire ensemble) to fine-tune the depth of the clustering
tree and minimal leaf size. In the case of the ISP, we adhere to the
validation protocol outlined in Section 5.
We include all datasets from [ 35] and [ 23] as benchmarks in our
analysis to facilitate a comprehensive comparison of our method
with state-of-the-art Gradient Boosting implementations. These
datasets are all classification datasets, varying in size in terms of
the number of features and samples, and encompass a range of
properties and feature types. The full list of the datasets, employed
in this study, along with datasets’ properties, is enumerated in
Table 1.
Does the validation protocol proposed in Section 5 have
good generalization ability? To investigate this, we applied both
the naive validation control and the advanced evaluation procedure,
both described in Section 5, to every dataset. As illustrated in Fig-
ure 4 and Figure 5, the naive validation protocol shows a monotonic
decrease with the increase in the number of clusters. This is an
expected outcome due to the nature of the protocol, which does
not provide insights into the optimal cluster count or potential
improvements compared to the baseline.
In contrast, the advanced approach produces a quality estima-
tion that is highly correlated with the test quality. This correlation
is evident in the repeated quality patterns for both the test and
validation sets. Consequently, this correlation provides an opportu-
nity to make an informed decision regarding the optimal cluster
count and other parameters that influence clustering. This informed
1172KDD ’24, August 25–29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
Table 1: 0-1 loss / logloss, Average value, mean relative error change w.r.t. baseline. Zero-percent improvements mean, that one
cluster is the best option selected by validation protocol.
LightGBM CatBo
ost
Dataset
(#samples/#features) Unprune
d Baseline DSP ISP Unprune
d Baseline DSP ISP
A
dult (49K/15) .1335
/ .2944 .1269
/ .2753.1253/.2726
−1.27%/
−0.99%.1265/.2742
−0.30%/
−0.39%.1276
/ .2755 .1266
/ .2725.1253/.2696
−1.05%/
−1.07%.1260/.2717
−0.48%/
−0.28%
Amazon
(33K/10) .0596
/ .1663 .0529
/ .1631.0525/.1628
−0.75%/
−0.21%.0527/.1629
−0.46%/
−0.12%.0445
/ .1404 .0448
/ .1395.0440/.1395
−1.68%/0%.0444/.1391
−0.87%/
−0.29%
Click
(400K/12) .1592
/ .4067 .1581
/ .3964.1578/.3964
−0.18%/0%.1581/.3962
0%/
−0.04%.1577
/ .3919 .1564
/ .3916.1563/.3914
−0.06%/
−0.04%.1564/.3914
+0.01%/
−0.04%
Default
(30K/23) .2011
/ .4694 .1888
/ .4518.1878/.4504
−0.51%/
−0.32%.1862/.4497
−1.4%/
−0.47%.1833
/ .4361 .1828
/ .4328.1821/.4324
−0.36%/
−0.1%.1798/.4323
−1.63%/
−0.12%
HEPMASS
(840K/25) .1337
/ .2843 .1270
/ .2791.1238/.2754
−2.55%/
−1.33%.1267/.2780
−0.25%/
−0.39%.1309
/ .2803 .1258
/ .2768.1245/.2753
−1.05%/
−0.55%.1256/.2764
−0.17%/
−0.16%
Higgs
(11KK/28) .2476
/ .5021 .2381
/ .4922.2369/.4897
−0.5%/
−0.51%.2375/.4906
−0.27%/
−0.33%.2385
/ .4864 .2364
/ .4810.2351/.4794
−0.57%/
−0.34%.2361/.4803
−0.14%/
−0.14%
KDD
Churn (50K/231) .0725
/ .2342 .0725
/ .2323.0725/.2323
0%/0%.0725/.2323
0%/0%.072
/ .2382 .0718
/ .2326.0715/.2326
−0.42%/0%.0718/.2326
0%/0%
KDD
Internet (10K/69) .0974
/ .2334 .0989
/ .2202.0980/.2167
−0.92%/
−1.58%.0969/.2190
−1.98%/
−0.53%.0994
/ .2199 .0984
/ .2167.0969/.2127
−1.55%/
−1.85%.0963/.2152
−2.17%/
−0.68%
KDD
Upselling (50K/231) .0495
/ .1694 .0495
/ .1669.0492/.1670
−0.67%/+0.08%.0495/.1669
−0.04%/0%.0487
/ .1677 .0489
/ .1670.0484/.1665
−1.01%/
−0.28%.0488/.1668
−0.10%/
−0.13%
Kick
(73K/36) .0987
/ .3073 .0991
/ .2956.0987/.2939
−0.44%/
−0.56%.0987/.2949
−0.37%/
−0.25%.0953
/ .2864 .0951
/ .2855.0949/.2849
−0.22%/
−0.22%.0948/.2850
−0.36%/
−0.16%
Marketing
(45K/16) .0941
/ .2268 .0931
/ .2044.0919/.2035
−1.25%/
−0.43%.0931/.2040
0%/
−0.20%.0913
/ .2032 .0911
/ .1938.0911/.1925
0%/
−0.68%.0895/.1922
−1.78%/
−0.80%
A
verage - - -0.82%
/ -0.53% -0.46%
/ -0.25% - - -0.72%
/ -0.47% -0.70%
/ -0.25%
Table 2: Time and memory measurements for the baseline (CV-based early stopping) and the additional costs associated with
our DSP algorithm. We do not report ISP measurements here, since the ISP algorithm introduces negligible time overhead
(<0.01%) and a moderate memory overhead (<5%) across all datasets
Dataset A
dult Amazon Click Default HEPMASS Higgs KDD
Churn KDD
Internet KDD
Upselling Kick Marketing
Time(
s)/Memory(MB) 125.7/606.7 200.1/614.5 665.8/968.4 123.7/602.9 732.9/1962.2 10537.3/8386.7 398.5/895.6 130.8/607.8 664.5/898.8 384.8/711.0 209.3/714.4
DSP
(delta in %) +2.1%/+15.7% +1.1%/+12.8% +0.8%/+35.3% +2.0%/+12.9% +1.8%/+49.6% +2.5%/+87.3% +2.2%/+6.8% +1.8%/+8.2% +1.4%/+7.2% +0.8%/+15.9% +1.2%/+11.7%
decision-making process enhances the generalization ability of the
validation protocol proposed in Section 5.
Does the proposed algorithm help to increase the quality
of boosting models? In this paragraph, we conduct an extensive
search for the optimal partition using two loss metrics (lower is
better): Logloss and 0-1 loss. For each cluster generated by DSP or
ISP algorithm, we identify the optimal iteration count and apply
the corresponding number of trees (boosters) to each test sample,
as outlined in Algorithm 1.
This process is repeated ten times for each dataset, each time with
a different CV split (random seed). We then calculate the average
loss over all experiment runs and the mean relative improvement
(loss decrease). The results of this comparison are presented in
Table 1.
Our findings indicate that the proposed techniques outperform
the classic early stopping approach in most settings. The improve-
ments are significant according to paired (ten pairs "baseline vs
ours" for each dataset) Wilcoxon signed-rank test, with a 𝑝−
𝑣𝑎𝑙𝑢𝑒≪0.001, except for the Click and KDD Churn datasets.
Interestingly, the relative magnitude of improvements is compa-
rable to that obtained by hyperparameter tuning (50 cross-validated
iterations of Hyperopt). This suggests that modern Gradient Boost-
ing implementations may not be fully leveraging the power of the
models, as they limit themselves to a shared stopping moment forall examples. Our results demonstrate that personalizing the selec-
tion of this parameter can lead to significant improvements in the
algorithm’s performance.
How do the proposed methods impact computational re-
sources such as time and memory?
While predictive quality is a crucial criterion for evaluating ma-
chine learning models, computational resources such as time and
memory are also important considerations. In this context, it is
essential to demonstrate that our proposed methods not only im-
prove model performance but also do so without requiring excessive
computational resources.
In addition to evaluating the predictive performance of the pro-
posed methods, we also analyzed their computational efficiency
in terms of time and memory consumption. The results are sum-
marized in Table 2, which lists the absolute values in seconds and
megabytes for the baseline method, along with the relative increases
for the DSP method.
For the Direct Supervised Partition (DSP) method, the additional
memory consumption varied from 10% to 50% on average across
all datasets. The additional time required for the DSP method was
approximately 1-2%, indicating a moderate increase in computa-
tional overhead. These results suggest that, despite some additional
computational costs, the DSP method remains resource-efficient
and provides a feasible solution for practical applications.
1173Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 4: Marketing, validation
Figure 5: Kick, validation
In contrast, the Indirect Supervised Partition (ISP) method demon-
strated minimal impact on computational resources. The time con-
sumption for ISP was less than 0.01%, and the memory overhead
was less than 5% for all datasets. Due to these negligible increases,
the detailed results for ISP are not included in Table 2.
These results highlight that our adaptive early stopping methods
provide a balanced approach, offering improved model performance
without imposing significant computational burdens. This balance
between quality and resource efficiency ensures that our methods
can be effectively integrated into existing learning pipelines and
applied to a wide range of tasks.
Does it make sense to use unsupervised clustering instead
of the one proposed in the paper? As discussed in Section 4.3,
constructing a space partition without information about targets,
as well as using algorithms that do not consider the geometry of
learning surfaces, can have certain disadvantages. To empirically
evaluate these points, we conduct a series of experiments similar
to those described in the previous section. The only difference was
that we used KMeans [ 31] as the clustering algorithm instead of
the one proposed in our paper.
We selected the Higgs and HEPMASS datasets for these ex-
periments because they lack categorical features, which makes
them suitable for KMeans. Our results indicate that unsupervised
clustering can indeed be effectively applied to the adaptive stop-
ping problem. We observed mean improvements of 0.1%/0.08%and0.09%/0.1% r espectively, which are significantly better than those 
achieved with the standard approach. However, these improve-
ments are significantly less than those achieved with the method 
proposed in Section 4.5, as confirmed by a  paired Wilcoxon signed-
rank test.
7 CONCLUSION AND FUTURE WORK
In this paper, we identified a  previously unexplored issue related 
to ensemble pruning. We examined the potential challenges that 
the simultaneous stopping rule poses to modern boosting models 
and introduced a cluster-based framework for early stopping. This 
framework can be seamlessly integrated into any Gradient Boosting 
implementation (and potentially other ensemble methods) without 
compromising its quality or affecting its training/inference time.
We also proposed a straightforward and computationally effi-
cient evaluation protocol for our method. This protocol simplifies 
the process of determining whether adaptive stopping is effective 
for a particular dataset. Our experiments, conducted using well-
known boosting implementations, validate the assumptions and 
conclusions presented in this paper. They also highlight the signifi-
cant potential for practical applications and further research.
However, this work also uncovers several areas that warrant 
further investigation. Specifically, the noise introduced by data 
points that are well-trained in early iterations and the challenge 
of constructing optimal clusters are areas that require in-depth  
exploration. This ongoing research will continue to advance our 
understanding and application of ensemble methods in machine 
learning.
REFERENCES
[1]Ismail Babajide Mustapha and Faisal Saeed. 2016. Bioactive molecule prediction
using extreme gradient boosting. Molecules 21, 8 (2016), 983.
[2]James Bergstra, Dan Yamins, David D Cox, et al .2013. Hyperopt: A python
library for optimizing the hyperparameters of machine learning algorithms. In
Proceedings of the 12th Python in science conference, Vol. 13. Citeseer, 20.
[3]Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 2017.
Classification and regression trees. Routledge.
[4] CatBoost. 2017. CatBoost library: https://github.com/catboost/catboost .
[5]George DC Cavalcanti, Luiz S Oliveira, Thiago JM Moura, and Guilherme V
Carvalho. 2016. Combining diversity measures for ensemble pruning. Pattern
Recognition Letters 74 (2016), 38–45.
[6]Yuan-Chin Ivan Chang, Yufen Huang, and Yu-Pai Huang. 2010. Early stopping in
L2Boosting. Computational Statistics & Data Analysis 54, 10 (2010), 2203–2213.
[7]Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview.
InProceedings of the learning to rank challenge. PMLR, 1–24.
[8]Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785–794.
[9]Chen Cheng, Fen Xia, Tong Zhang, Irwin King, and Michael R Lyu. 2014. Gradient
boosting factorization machines. In Proceedings of the 8th ACM Conference on
Recommender systems. 265–272.
[10] Cliff Click, Michal Malohlava, Arno Candel, Hank Roark, and Viraj Parmar. 2016.
Gradient boosting machine with h2o. H2O. ai 11 (2016), 12.
[11] Corinna Cortes, Mehryar Mohri, and Dmitry Storcheus. 2019. Regularized Gradi-
ent Boosting. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
465636eb4a7ff4b267f3b765d07a02da-Paper.pdf
[12] Rafael MO Cruz, Robert Sabourin, and George DC Cavalcanti. 2018. Dynamic
classifier selection: Recent advances and perspectives. Information Fusion 41
(2018), 195–216.
[13] R. M. Cruz, R. Sabourin, G. D. Cavalcanti, and T. I. Ren. 2015. META-DES: A
dynamic ensemble selection framework using meta-learning. Pattern Recognition
48 (2015), 1925–1935.
1174KDD ’24, August 25–29, 2024, Barcelona, Spain Bulat Ibragimov and Gleb Gusev
[14] Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likelihood
from incomplete data via the EM algorithm. Journal of the Royal Statistical Society:
Series B (Methodological) 39, 1 (1977), 1–22.
[15] Lei Deng, Juan Pan, Xiaojie Xu, Wenyi Yang, Chuyao Liu, and Hui Liu. 2018.
PDRLGB: precise DNA-binding residue prediction using a light gradient boosting
machine. BMC bioinformatics 19, 19 (2018), 135–145.
[16] Wei Fan, Fang Chu, Haixun Wang, and Philip S Yu. 2002. Pruning and dynamic
scheduling of cost-sensitive ensembles. In AAAI/IAAI. 146–151.
[17] Yoav Freund and Robert E Schapire. 1997. A Decision-Theoretic Generalization
of On-Line Learning and an Application to Boosting. J. Comput. System Sci. 55, 1
(1997), 119–139. https://doi.org/10.1006/jcss.1997.1504
[18] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189–1232.
[19] Jerome H Friedman. 2002. Stochastic gradient boosting. Computational statistics
& data analysis 38, 4 (2002), 367–378.
[20] Jerome H Friedman and Bogdan E Popescu. 2008. Predictive learning via rule
ensembles. (2008).
[21] Daniel Hernández-Lobato, Gonzalo Martinez-Munoz, and Alberto Suárez. 2008.
Statistical instance-based pruning in ensembles of independent classifiers. IEEE
Transactions on Pattern Analysis and Machine Intelligence 31, 2 (2008), 364–369.
[22] Bulat Ibragimov. 2024. DSP and ISP algorithms: https://github.com/ibr11/LTSA.
[23] Bulat Ibragimov and Gleb Gusev. 2019. Minimal variance sampling in stochastic
gradient boosting. In Proceedings of the 33rd International Conference on Neural
Information Processing Systems. 15087–15097.
[24] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in neural information processing systems 30 (2017), 3146–
3154.
[25] Erin LeDell and Sebastien Poirier. 2020. H2o automl: Scalable automatic machine
learning. In Proceedings of the AutoML Workshop at ICML, Vol. 2020.
[26] Nan Li, Yang Yu, and Zhi-Hua Zhou. 2012. Diversity regularized ensemble
pruning. In Joint European conference on machine learning and knowledge discovery
in databases. Springer, 330–345.
[27] Ping Li, Qiang Wu, and Christopher Burges. 2007. Mcrank: Learning to rank using
multiple classification and gradient boosting. Advances in neural information
processing systems 20 (2007), 897–904.
[28] LightGBM. 2017. LightGBM library: https://github.com/microsoft/LightGBM .
[29] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun.
2017. Model ensemble for click prediction in bing search ads. In Proceedings of
the 26th International Conference on World Wide Web Companion. 689–698.
[30] Brian Liu and Rahul Mazumder. 2023. Forestprune: Compact depth-pruned tree
ensembles. In International Conference on Artificial Intelligence and Statistics.
PMLR, 9417–9428.
[31] S. Lloyd. 1982. Least squares quantization in PCM. IEEE Transactions on Informa-
tion Theory 28, 2 (1982), 129–137. https://doi.org/10.1109/TIT.1982.1056489
[32] Dragos D Margineantu and Thomas G Dietterich. 1997. Pruning adaptive boosting.
InICML, Vol. 97. Citeseer, 211–218.
[33] Andreas Mayr, Benjamin Hofner, and Matthias Schmid. 2012. The importance of
knowing when to stop. Methods of Information in Medicine 51, 02 (2012), 178–186.
[34] Dayvid VR Oliveira, George DC Cavalcanti, and Robert Sabourin. 2017. Online
pruning of base classifiers for dynamic ensemble selection. Pattern Recognition
72 (2017), 44–58.
[35] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
gush, and Andrey Gulin. 2017. CatBoost: unbiased boosting with categorical
features. arXiv preprint arXiv:1706.09516 (2017).
[36] Toby Sharp. 2008. Implementing decision trees and forests on a GPU. In European
conference on computer vision. Springer, 595–608.
[37] Robin Sibson. 1973. SLINK: an optimally efficient algorithm for the single-link
cluster method. The computer journal 16, 1 (1973), 30–34.
[38] Víctor Soto, Sergio García-Moratilla, Gonzalo Martínez-Muñoz, Daniel
Hernández-Lobato, and Alberto Suárez. 2014. A double pruning scheme for
boosting ensembles. IEEE transactions on cybernetics 44, 12 (2014), 2682–2695.
[39] Mervyn Stone. 1974. Cross-validatory choice and assessment of statistical predic-
tions. Journal of the royal statistical society: Series B (Methodological) 36, 2 (1974),
111–133.
[40] Samir Touzani, Jessica Granderson, and Samuel Fernandes. 2018. Gradient boost-
ing machine for modeling the energy consumption of commercial buildings.
Energy and Buildings 158 (2018), 1533–1543.
[41] Ilya Trofimov, Anna Kornetova, and Valery Topinskiy. 2012. Using boosted trees
for click-through rate prediction for sponsored search. In In Proceedings of the
Sixth International Workshop on Data Mining for Online Advertising and Internet
Economy. 1–6.
[42] Yuting Wei, Fanny Yang, and Martin J Wainwright. 2017. Early stopping
for kernel boosting algorithms: A general analysis with localized complex-
ities. In Advances in Neural Information Processing Systems, I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2017/file/a081cab429ff7a3b96e0a07319f1049e-Paper.pdf[43] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. 2007. On Early Stopping in
Gradient Descent Learning. Constructive Approximation 26 (08 2007), 289–315.
https://doi.org/10.1007/s00365-006-0663-2
[44] Yi Zhang, Samuel Burer, W Nick Street, Kristin P Bennett, and Emilio Parrado-
Hernández. 2006. Ensemble Pruning Via Semi-definite Programming. Journal of
machine learning research 7, 7 (2006).
[45] Yanru Zhang and Ali Haghani. 2015. A gradient boosting method to improve
travel time prediction. Transportation Research Part C: Emerging Technologies 58
(2015), 308–324.
[46] Zhi-Hua Zhou and Wei Tang. 2003. Selective ensemble of decision trees. In
International workshop on rough sets, fuzzy sets, data mining, and granular-soft
computing. Springer, 476–483.
A APPENDIX
A.1 Experimental setup
We set the single train/test split in the ratio of 4:1 and use train
data for training and hyperparameter search. Test data is used at
the evaluation step only. For the initial hyperparameter tuning
(baseline), we perform 50 iterations of Tree Parzen Estimator from
the Hyperopt library using LogLoss as a target metric (lower-better).
We consider the following list of hyperparameters to be tuned.
LightGBM:
•"num_leaves" - max number of terminal nodes. Loguniform
grid from 1 to 105;
•"learning_rate" - the weight of each tree in the ensemble.
Loguniform grid from 10−7to 1;
•"min_data_in_leaf" - minimal number of data points in a leaf
(node is not considered for splitting). Loguniform grid from
1 to106;
•"min_sum_hessian_in_leaf" - minimal value of sum of hes-
sians in a leaf (node is not considered for splitting). Loguni-
form grid from 0 to 105;
•"lambda_l1" - regularizing term multiplier for predictions in
trees’ leaves. Loguniform grid from 0 to 100;
•"lambda_l2" - regularizing term multiplier for predictions in
trees’ leaves. Loguniform grid from 0 to 100;
•"bagging_fraction" - SGB sampling ratio. Uniform grid from
0.5 to 1;
•"feature_fraction" - feature sampling ratio (random subspace).
Uniform grid from 0.5 to 1;
•"n_estimators" - number of trees in the ensemble. Fixed con-
stant 5000. The best value is selected at the end by greedy
search.
CatBoost:
•"depth" - max depth of each decision tree. Integer grid from
1 to 6;
•"learning_rate" - the weight of each tree in the ensemble.
Loguniform grid from 10−5to 1;
•"random_strength" - regularizing randomized term in the
split scoring function. Integer grid from 1 to 20;
•"one_hot_max_size" - use one-hot encoding for categorical
features with number of unique values less than given pa-
rameter. Integer grid from 0 to 25;
•"l2_leaf_reg" - regularizing term multiplier for predictions
in trees’ leaves. Loguniform grid from 1 to 10;
•"subsample" - SGB sampling ratio. Uniform grid from 0.5 to
1;
1175Learn Together Stop Apart:
an Inclusive Approach to Ensemble Pruning KDD ’24, August 25–29, 2024, Barcelona, Spain
•"rsm" - feature sampling ratio (random subspace). Uniform
grid from 0.5 to 1;
•"iterations" - number of trees in the ensemble. Fixed constant
5000. The best value is selected at the end by greedy search.
A.2 Clustering results
Here we report the results of the clustering algorithm DSP for the
train set (used to predict optimal stops in each cluster) and the test
set.
Figure 6: Adult dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 7: Amazon dataset. Clustering obtained via DSP, vali-
dation and test results. The number after "test" is test samples
count per cluster.)
Figure 8: Click dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 9: Default dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 10: KDD Churn dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
Figure 11: KDD Internet dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
Figure 12: KDD Upselling dataset. Clustering obtained via
DSP, validation and test results. The number after "test" is
test samples count per cluster.)
Figure 13: Kick dataset. Clustering obtained via DSP, valida-
tion and test results. The number after "test" is test samples
count per cluster.)
Figure 14: Marketing dataset. Clustering obtained via DSP,
validation and test results. The number after "test" is test
samples count per cluster.)
1176