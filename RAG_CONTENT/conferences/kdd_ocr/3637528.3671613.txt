Fe
dGTP: Exploiting Inter-Client Spatial Dependency in
Federated Graph-based Traffic Prediction
Linghua Yang
SKLCCSE Lab
Beihang University
Beijing, China
sy2206212@buaa.edu.cnWantong Chen
SKLCCSE Lab
Beihang University
Beijing, China
2306cwt@buaa.edu.cnXiaoxi He
Faculty of Science and
Technology
University of Macau
Macau, China
hexiaoxi@um.edu.moShuyue Wei
SKLCCSE Lab
Beihang University
Beijing, China
weishuyue@buaa.edu.cn
Yi Xu
SKLCCSE Lab, Institute of
Artificial Intelligence
Beihang University
Beijing, China
xuy@buaa.edu.cnZimu Zhou
School of Data Science
City University of Hong
Kong
Hong Kong, China
zimuzhou@cityu.edu.hkYongxin Tong
SKLCCSE Lab
Beihang University
Beijing, China
yxtong@buaa.edu.cn
ABSTRACT
Graph-based methods have witnessed tremendous success in traf-
fic prediction, largely attributed to their superior ability in captur-
ing and modeling spatial dependencies. However, urban-scale traf-
fic data are usually distributed among various owners, limited in
sharing due to privacy restrictions. This fragmentation of data se-
verely hinders interaction across clients, impeding the utilization
of inter-client spatial dependencies. Existing studies have yet to
address this non-trivial issue, thereby leading to sub-optimal per-
formance. To fill this gap, we propose FedGTP, a new federated
graph-based traffic prediction framework that promotes adaptive
exploitation of inter-client spatial dependencies to recover close-
to-optimal performance complying with privacy regulations like
GDPR. We validate FedGTP via large-scale application-driven ex-
periments on real-world datasets. Extensive baseline comparison,
ablation study and case study demonstrate that FedGTP indeed
surpasses existing methods through fully recovering inter-client
spatial dependencies, achieving 21.08%, 13.48%, 19.90% decrease
on RMSE, MAE and MAPE, respectively. Our code is available at
https://github.com/LarryHawkingYoung/KDD2024_FedGTP.
CCS CONCEPTS
•Computing methodologies →Learning paradigms .
KEYWORDS
Federated Learning;Traffic Prediction;Spatial-Temporal Graph Neu-
ral Network
Permission
to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671613ACM Reference Format:
Linghua Yang, Wantong Chen, Xiaoxi He, Shuyue Wei, Yi Xu, Zimu Zhou,
and Yongxin Tong. 2024. FedGTP: Exploiting Inter-Client Spatial Depen-
dency in Federated Graph-based Traffic Prediction. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671613
1 INTRODUCTION
Traffic prediction is essential for optimizing urban mobility [26,
35], reducing congestion [17, 21], and enhancing road safety [42,
55]. It forecasts traffic conditions by analyzing patterns derived
from traffic data spanning both spatial and temporal dimensions.
These data can be effectively modeled as spatiotemporal graphs,
where nodes represent locations and edges represent spatial de-
pendencies between them. Therefore, graph-based deep learning
methods like Spatial-Temporal Graph Neural Networks (STGNNs)
have emerged as the primary tool for traffic prediction, incorpo-
rating Graph Neural Networks (GNNs) to capture non-Euclidean
spatial dependencies [44].
Spatial dependencies are crucial for accurate traffic prediction
[14, 24, 48], yet their practical utilization poses significant chal-
lenges. This is because traffic data are often distributed among
various owners (governments, companies, or individuals), a.k.a.,
clients, with privacy regulations such as GDPR1restricting the free
sharing of data. This leads to the fragmentation of spatial depen-
dency information, dividing it into multiple sub-graphs, each owned
by a distinct client. Accordingly, spatial dependencies in traffic
data are now categorized into two types: intra-client andinter-client
(see Fig. 1). Intra-client dependencies (individual sub-graphs) can
still be captured effectively within each client’s data. However, ex-
tracting inter-client dependencies (the connections between sub-
graphs) becomes complicated due to the necessary cross-client in-
formation exchange under privacy constraints.
Federated Learning (FL) [31, 49] offers a natural solution for dis-
tributed model training while respecting privacy concerns. There
is also a growing interest to apply federated learning to STGNNs
1https://gdpr-info
.eu
6105
KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
Inter-clientSpatial DependencyIntra-clientSpatial Dependency
Figur
e 1: Inter-/intra-client spatial dependency.
Co. 1Co. 2Co. 3Co. 4Co. 5
(
a)
Co. 1 Co. 2 Co. 3 Co. 4 Co. 5
T axi Company012345678MAE9.15%
15.08%
6.13%
11.83%
11.29%
inter
non-inter (
b)
Figure 2: A toy example on the importance of inter-client
spatial dependencies: (a) shows the data distributions of five
taxi companies, with red lines highlighting inter-client spa-
tial dependencies; (b) plots the prediction error (MAE) in-
crease after removing inter-client spatial dependencies.
for federated graph-based traffic prediction [23, 27, 28, 32, 39, 43,
47, 51–53]. However, existing proposals can be classified into two
categories by their limitations: (i) Those that largely under-utilize
inter-client spatial dependencies under privacy constraints [23, 27,
43, 52], which incur potential accuracy degradation. A toy exam-
ple (see Fig. 2) demonstrates the importance of inter-client spatial
dependencies in federated graph-based traffic prediction. This ex-
ample predicts the traffic flows of five taxi companies in a city and
reveals that removing inter-client dependencies from training data
results in up to 15% error increase for each taxi company. (ii) Those
that treat the inter-client spatial dependencies as public and pre-
defined [28, 32, 39, 47, 51, 53], which compromise privacy. Later
our experiments also reveal that predefined dependencies inade-
quately capture complex relations, leading to poor performance.
Thus, it’s vital to fully exploit the inter-client spatial dependencies
in federated graph-based traffic prediction.
In this paper, we introduce FedGTP (Federated Graph-based
Traffic Prediction), a new federated graph learning framework de-
signed to fully leverage inter-client spatial dependencies for traffic
prediction. We base FedGTP on ASTGNNs [4, 20, 45], known for
their state-of-the-art prediction accuracy through adaptive learn-
ing of spatial dependencies. However, learning inter-client spatial
dependencies in a federated setup is non-trivial. This challenge
arises because the spatial dependency learning in ASTGNNs re-
quires access to raw data, which becomes impractical in federated
learning without extensive secure operations. To address this, we
reformulate spatial modeling and learning into components spe-
cific to intra- and inter-client interactions. At the heart of inter-
client spatial dependency reconstruction, we introduce an adaptivepolynomial-based activation decomposition mechanism, which min-
imizes the need for cross-client computations while preserving pri-
vacy. Finally, we integrate our reformulation into the core of feder-
ated learning, while enabling personalization, a beneficial feature
when data distributions vary significantly among clients.
The main contributions of this paper are as follows:
•To the best of our knowledge, it is the first work that ad-
vocates and enables full utilization of inter-client spatial de-
pendencies for federated graph-based traffic prediction.
•We propose a novel and unified framework FedGTP, which
performs adaptive learning of spatial dependencies across
clients, enabling profound exploitation of spatial relations
and thus enhancing performance. This framework includes
a polynomial-based privacy-preserving mechanism that re-
veals only aggregated intermediate results to server for se-
cure summation, adhering to privacy regulations like GDPR.
•Extensive experiments on real-world datasets show that by
fully recovering inter-client spatial dependencies, our solu-
tion largely outperforms prior methods by 21.08%, 13.48%,
19.90%for RMSE, MAE and MAPE, respectively.
2 RELATED WORK
2.1 Graph-based Traffic Prediction
Spatial-temporal graph neural networks (STGNNs) prevail in graph-
based traffic prediction [44]. For temporal modeling , both recur-
rent [5, 14, 24] and convolutional neural networks [48, 50] are fre-
quently used. Attention mechanisms are also used to extract dy-
namic temporal patterns in ASTGCN [15], GMAN [54], STG2Seq
[2], and ASTGNN(p) [16]. Our focus is on the spatial modeling . Con-
ventional STGNNs often assume a graph topology predefined by
geographic distance [5, 24, 48, 50], or semantic similarity [2, 3, 14].
One alternative is to apply attention-based spatial models to learn
edge weights from data [2, 5, 15, 16, 54]. However, these studies
fail to capture the full spatial dependencies because the learned
connections are still restricted by the predefined adjacency ma-
trix. A major advancement is the development of adaptive spatial-
temporal graph neural networks (ASTGNNs) [4, 20, 45]. For in-
stance, Graph WaveNet [45] introduces a trainable AGCN layer
to learn a normalized adaptive adjacency matrix. AGCRN [4] en-
hances the AGCN layer by discerning node-specific patterns. AST-
GAT [20] adopts a network generator model to create an adaptive
discrete graph and infer hidden correlations directly from the data.
We ground our work upon ASTGNNs [4, 20, 45] since they can in-
fer spatial dependencies from data and achieve the state-of-the-art
performances.
2.2 Federated Graph Learning
To facilitate graph learning in scenarios where graph data are dis-
tributed and necessitate privacy protection, FL has been integrated
into graph learning. Existing works in general FGL primarily focus
on tackling challenges such as cross-client missing information,
privacy leakage of graph structures, and data heterogeneity across
clients [12, 22, 29]. We specifically focus on the reconstruction of
inter-client missing information, which is coherent to the spatial-
temporal essence of traffic prediction.
6106FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain.
Some studies strictly protect privacy of both temporal data on
nodes and the spatial topology on edges. FLoS [43] and MFVST-
GNN [27] aggregates local STGNNs via FedAvg [31] without con-
sidering inter-client spatial dependencies. FASTGNN [52] applies
differential privacy to aggregate adjacency matrices of each sub-
graph, but it randomly generates inter-client spatial connections.
FML-ST [23] constructs local spatial-temporal patterns on each
client, which are aggregated into a global one assisted by the server.
These methods largely under-utilize inter-client spatial dependen-
cies with privacy constraints, yielding sub-optimal performance.
Another major part of studies reconstruct information based on
traditional GNNs and treat the inter-client edges as public and pre-
defined. For instance, CNFGNN [32] extracts the cross-node de-
pendencies on a pre-defined graph topology at the server. Fed-
STN [51] extracts long-term and short-term spatial-temporal in-
formation separately on public road network. FedAGCN [39] and
FCGCN [47] treat spatial topology as public and apply community
detection algorithms to partition sub-graphs. In CTFL [53], the spa-
tial graph is available to all clients. These methods partially com-
promise privacy, and the dependencies extracted solely from the
public and predefined channels are insufficient and biased.
3 PROBLEM STATEMENT
3.1 Graph Modeling of Traffic Data
Adhering to prior research on traffic prediction [2, 4, 9, 15, 18, 24,
45, 50], we depict traffic data as a sequence of graph signal frames
{𝑿1,𝑿2,...,𝑿𝑇}. The graph signal 𝑿𝑡∈R𝑁×𝐹denotes the obser-
vations defined on Gat the𝑡-th time slot, where 𝐹is the feature
channel. The graph G={V,E},a.k.a. the spatial network2con-
tains a node setVof size|V|=𝑁and an edge setE. The edges
Eare represented by an adjacency matrix 𝑨∈R𝑁×𝑁, which is
usually built upon (i)pre-defined graphs like geographic distance
[5, 24, 48, 50]; (ii)domain knowledge based attributes similarity
[2, 3, 14]; or (iii)spatial correlations learned form data [4, 20, 45].
Particularly, method (iii)is the state-of-the-art to define adjacency
matrix, as it can learn spatial dependencies from data without prior
or domain knowledge.
3.2 Graph-based Traffic Prediction
Consider graphGand𝑇𝑖𝑛historical observations of graph signal
𝑿(𝑡−𝑇𝑖𝑛+1):𝑡. Traffic prediction at time slot 𝑡aims to learn a func-
tion𝐹(·)which maps the historical observations into the future
ones in the next 𝑇𝑜𝑢𝑡time slots:
𝑿(𝑡+1):(𝑡+𝑇𝑜𝑢𝑡)←𝐹(𝑿(𝑡−𝑇𝑖𝑛+1):𝑡;𝜃,G) (1)
where𝜃denotes all the learnable parameters. We use colons to
denote a temporal sequence. The optimal model parameters 𝜃∗are
trained via the following objective.
𝜃∗=argmin
𝜃L(𝐹,𝜃;D) (2)
whereLrepresents the loss function of 𝐹with parameters 𝜃on
datasetD={G,𝑿1:𝑇}.
2Follo
wing the conventions [2, 4, 6, 7, 9, 15, 18, 24, 44, 45, 48, 50], we only consider
static homogeneous graphs with nodes and edges of the same type or class.3.3 Federated Graph-based Traffic Prediction
Consider the client-server based federated learning, where 𝑀clients
(data owners), denoted by C={C1,C2,...,C𝑀}, collaboratively
train models under coordination of a central server [31, 37].
•Data Partition. Assume client C𝑖maintains a local traffic
datasetD𝑖={G𝑖,𝑿1:𝑇
𝑖}, whereG𝑖={V𝑖,E𝑖}is a local
graph of𝑁𝑖nodes. Note thatG=Ð𝑀
𝑖=1G𝑖is the complete
graph and𝑁=Í𝑀
𝑖=1𝑁𝑖is the total number of nodes. Due
to the partition of G, the corresponding adjacency matrix
𝑨∈R𝑁×𝑁and graph signal 𝑿1:𝑇∈R𝑇×𝑁×𝐹are parti-
tioned across clients as
𝑨=2666664𝑨11··· 𝑨1𝑀
.........
𝑨𝑀1··· 𝑨𝑀𝑀3777775,𝑿1:𝑇=2666664𝑿1:𝑇
1...
𝑿1:𝑇
𝑀3777775(3)
where 𝑨𝑖𝑗∈R𝑁𝑖×𝑁𝑗is formed by rows and columns corre-
spond toV𝑖andV𝑗in𝑨. Similarly, 𝑿1:𝑇
𝑖∈R𝑇×𝑁𝑖×𝐹is the
local graph signal for the 𝑁𝑖nodes over𝑇time slots.
•Privacy Constraints. As is common in federated learning, the
server has no access to local datasets {D𝑖}. Furthermore,
each clientC𝑖can share neither its own graph signals 𝑋1:𝑇
𝑖
nor its local graph G𝑖with any other clients. For example,
as a taxi company, the historical traffic data of its taxis, as
well as the correlations between taxis in various regions, are
considered sensitive business secrets not allowed to be dis-
closed or shared to others.
•Training Objectives . SinceD𝑖embodies region-specific char-
acteristics that can be non-IID (independent and identically
distributed) across clients, we consider a personalized fed-
erated learning setting [11], where client-specific optimal
parameters𝜃∗
1,...,𝜃∗
𝑀are trained with the objective below
{𝜃∗
1,...,𝜃∗
𝑀}=argmin
𝜃1,...,𝜃 𝑀𝑀Õ
𝑖=1𝑁𝑖
𝑁L
(𝐹𝑖,𝜃𝑖,D𝑖). (4)
Due to the privacy constraints, it is challenging to make full use
of the spatial dependencies (represented by 𝑨) for traffic predic-
tion. While intra-client spatial dependencies (𝑨𝑖𝑗for𝑖=𝑗) can
be readily derived from local dataset D𝑖[4, 20, 45], the extraction
ofinter-client spatial dependencies (𝑨𝑖𝑗for𝑖≠𝑗) is far from
straightforward due to the necessary data exchange across clients.
4 METHOD
This section presents FedGTP, a new federated graph-based traf-
fic prediction framework. It is built upon centralized ASTGNNs
(Sec. 4.1), with a novel spatial modeling formulation for the feder-
ated setting (Sec. 4.2). We then explain how to integrate the new
spatial modeling with temporal modeling (Sec. 4.3), and introduce
the overall system design and implementation of FedGTP (Sec. 4.4).
4.1 ASTGNNs for Federated Graph-based
Traffic Prediction
4.1.1 Primer on ASTGNNs. As with other STGNNs [5, 14, 24, 24,
48, 50], ASTGNNs [4, 20, 45] also consist of a spatial and a temporal
component. They adopt an Adaptive Graph Convolution Network
6107KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
(AGCN) [4] for spatial modeling and Gated Recurrent Units (GRU)
[8] to capture temporal correlations. The core of ASTGNNs is the
AGCN layer, which introduces adaptive learning into the conven-
tional GCN layer [19]. Specifically, an AGCN layer contains a Data
Adaptive Graph Generation (DAGG) module inferring spatial de-
pendencies and a Node Adaptive Parameter Learning (NAPL) mod-
ule capturing node-specific patterns:
˜𝑨=𝑰𝑁+𝜎 𝑬·𝑬⊤, (5)
𝑯(𝑙)=𝜎
˜𝑨·𝑯(𝑙−1)·𝑬·𝑾+𝑬·𝒃
, (6)
where ˜𝑨∈R𝑁×𝑁is the adaptive adjacency matrix, 𝑰𝑁∈R𝑁×𝑁is
the identity matrix, each row of 𝑬∈R𝑁×𝑑presents the learnable
node embedding, 𝑯(𝑙−1)∈R𝑁×𝐹(𝑙−1)and𝑯(𝑙)∈R𝑁×𝐹(𝑙)are the
input and output feature of the 𝑙-th layer, 𝑾∈R𝑑×𝐹(𝑙−1)×𝐹(𝑙)and
𝒃∈R𝑑×𝐹(𝑙)are the trainable weights and bias pool. 𝜎(·)is the
nonlinear activation.
The AGCN layer is then integrated into GRU by replacing all the
linear layers in it with spatiotemporal feature fusion:
𝒛𝑡=𝜎
˜𝑨·[𝑿𝑡||𝒉𝑡−1]·𝑬·𝑾𝒛+𝑬·𝒃𝒛
𝒓𝑡=𝜎
˜𝑨·[𝑿𝑡||𝒉𝑡−1]·𝑬·𝑾𝒓+𝑬·𝒃𝒓
˜𝒉𝑡=𝑡𝑎𝑛ℎ
˜𝑨·[𝑿𝑡||(𝒓𝑡⊙𝒉𝑡−1)]·𝑬·𝑾˜𝒉+𝑬·𝒃˜𝒉
𝒉𝑡=𝒛𝑡⊙𝒉𝑡−1+(1−𝒛𝑡)⊙ ˜𝒉𝑡,(7)
where 𝑿𝑡and𝒉𝑡are input and output at the 𝑡-th time slot.||is the
concatenation operation. ⊙is the Hadamard product. 𝒛,𝒓, and ˜𝒉
are the update gate, reset gate and candidate state, respectively.
4.1.2 Challenges in Federated ASTGNNs. In the centralized setting,
both the graph signals 𝑿1:𝑇and graph structure ˜𝑨are gathered in
one place to train ASTGNNs. However, in the federated setting,
the node embedding matrix 𝑬used to generate ˜𝑨and the feature
matrix 𝑯containing 𝑿1:𝑇are partitioned into
𝑬=2666664𝑬1
...
𝑬𝑀3777775,𝑯=2666664𝑯1
...
𝑯𝑀3777775(8)
and distributed across clients.
From the perspective of each single client C𝑖, the𝑙-th local layer
feature matrix can be expressed as
𝑯(𝑙)
𝑖=𝜎©­
«𝑀Õ
𝑗=1(˜𝑨𝑖𝑗·𝑯(𝑙−1)
𝑗)·𝑬𝑖·𝑾+𝑬𝑖·𝒃ª®
¬. (9)
Omitting layer id for brevity, we let 𝒁𝑖=Í𝑀
𝑗=1(˜𝑨𝑖𝑗·𝑯𝑗), which
embodies spatial dependencies and can be divided into
𝒁𝑖=˜𝑨𝑖𝑖·𝑯𝑖+𝑀Õ
𝑗=1,𝑗≠𝑖(˜𝑨𝑖𝑗·𝑯𝑗). (10)
Following the terminologies in Sec. 3, ˜𝑨𝑖𝑖·𝑯𝑖is the intra-client
spatial dependencies with features available within C𝑖, while ˜𝑨𝑖𝑗·
𝑯𝑗when𝑖≠𝑗is the inter-client ones that can only be calculated
involving otherC𝑗.However, federated setting prohibits C𝑗from sharing its graph
signals𝑋1:𝑇
𝑗and graph structure G𝑗toC𝑖(see Sec. 3). Regarding
ASTGNNs, the privacy restrictions extend to 𝑬𝑗and𝑯𝑗. Since 𝑬𝑗
multiplied by itself would reveal ˜𝑨𝑗𝑗(see Eq. (5)), which leaks local
graph; and 𝑯𝑗contains𝑋1:𝑇
𝑗. Such constraints pose challenges to
the establishment of spatial dependencies between clients.
To reconstruct the inter-client part of 𝒁𝑖in federated setting,
a naive solution based on Eq. (5) is shown in Fig. 3a. Each client
first encrypts 𝑬𝑖and𝑯𝑖, and uploads them to the server. Then the
server performs matrix calculation in ciphertext using approaches
like Homomorphic Encryption (HE) [40]. Finally, 𝒁𝑖in ciphertext
is sent to each client and decrypted into plaintext. This solution in-
volves𝑂 𝑁2(𝑑+𝐹)atomic operations in ciphertext, which can
be hundreds to thousands of times slower than its plaintext coun-
terpart [1], such a naive solution would cause tremendous compu-
tational costs. Thus, there is a need for more efficient alternatives
to calculate the inter-client spatial dependencies.
4.2 Learning Inter-Client Spatial Dependency
4.2.1 Reformulation of Spatial Modeling. To achieve inter-client
spatial dependency reconstruction more computational-efficiently
while preserving privacy, we reformulate the computation of spa-
tial modeling. The idea is to maximize local calculations which can
be performed in plaintext and reducing the time-consuming server-
side ciphertext operations (see Fig. 3b). Specifically, we decompose
˜𝑨𝑖𝑗into the product of two distinct parts, one pertains exclusively
toC𝑖and the other toC𝑗. However, a challenge arises from the non-
linear nature of the activation in Eq. (5) ( i.e.ReLU in ASTGNNs),
which complicates privacy preservation. To address this, we pro-
pose an activation decomposition mechanism by applying an elabo-
rated transform function, denoted as F(·) , to the node embedding
matrix of each client, so as to retain the necessary non-linearity
and allow effective restructuring of spatial modeling:
˜𝑨𝑖𝑗=(
𝑰𝑁𝑖+F(𝑬𝑖)·F⊤(𝑬𝑖)if𝑖=𝑗,
F(𝑬𝑖)·F⊤(𝑬𝑗) if𝑖≠𝑗.(11)
Bringing it into Eq. (10), we obtain the new form:
𝒁𝑖=𝑯𝑖+F(𝑬𝑖)·𝑀Õ
𝑗=1 F⊤(𝑬𝑗)·𝑯𝑗. (12)
According to Eq. (12), each client C𝑖first computes 𝑨𝑮𝑮𝑖=F⊤(𝑬𝑖)·
𝑯𝑖locally in plaintext. This aggregated intermediate result is then
uploaded from each client to the server. The server conducts only a
simple summation and broadcasts the resultÍ𝑨𝑮𝑮 to each client
for the remaining local calculations. Since the intermediate results
uploaded to the server are aggregated rather than raw data, it com-
plies with privacy standards such as GDPR [38]. Next, we present
two designs of the transform function F(·) .
4.2.2 Straight-forward Activation Decomposition. An intuitive op-
tion is to setF(·) to𝑅𝑒𝐿𝑈(·). This simple mechanism is named
SprtReLU, as it reorders the ReLU activation to be applied sepa-
rately on each node embedding matrix before proceeding with their
multiplication. Based on this, we can rewrite Eq. (5) as:
˜𝑨(𝑆𝑝𝑟𝑡𝑅𝑒𝐿𝑈)=𝑰𝑁+𝑅𝑒𝐿𝑈(𝑬)·(𝑅𝑒𝐿𝑈(𝑬))⊤, (13)
6108FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain.
𝑰!+𝜎	%	
…
𝑬!∈ℝ"!×$…𝑬𝑬"…𝑯(𝑨(𝑨%…𝒁
𝑯!∈ℝ"!×%𝒁!∈ℝ"!×%𝒁&∈ℝ""×%𝑬&∈ℝ""×$𝑯&∈ℝ""×%…(1)(1)(2)(3)(3)(1) Encrypt 𝑬#,𝑯#(2) Matrix Calculation      in Ciphertext on server(3) Decrypt 𝒁#
(
a)
𝒁!∈ℝ"!×$…
(1)(4)
𝑨𝑮𝑮!!!"#$	
(2)(3)
(5)(2)(4)(1) Aggregate 𝑬! and 𝑯!(2) Upload 𝑨𝑮𝑮!(3) Sum 𝑨𝑮𝑮! on server(4) Broadcast ∑𝑨𝑮𝑮(5) Calculate 𝒁! locally &𝑨𝑮𝑮𝑨𝑮𝑮%𝑨𝑮𝑮&𝑨𝑮𝑮!
&𝑨𝑮𝑮
&𝑨𝑮𝑮 (
b)
Figure 3: Illustration of approaches for computing inter-
client spatial dependency: (a) shows the naive encryption-
based solution; (b) presents our proposed solution, which
transfers ciphertext computation on server to plaintext com-
putation in local as much as possible.
where for each adjacency block
˜𝑨(𝑆𝑝𝑟𝑡𝑅𝑒𝐿𝑈)𝑖𝑗=(
𝑰𝑁𝑖+𝑅𝑒𝐿𝑈(𝑬𝑖)·(𝑅𝑒𝐿𝑈(𝑬𝑖))⊤if𝑖=𝑗,
𝑅𝑒𝐿𝑈(𝑬𝑖)· 𝑅𝑒𝐿𝑈(𝑬𝑗)⊤if𝑖≠𝑗.(14)
Therefore, the SprtReLU -version spatial dependencies
𝒁(𝑆𝑝𝑟𝑡𝑅𝑒𝐿𝑈)𝑖=𝑯𝑖+𝑅𝑒𝐿𝑈(𝑬𝑖)·𝑀Õ
𝑗=1 𝑅𝑒𝐿𝑈(𝑬𝑗)⊤·𝑯𝑗
,(15)
where 𝑬𝑖and𝑯𝑖are local withinC𝑖, then each term 𝑅𝑒𝐿𝑈(𝑬𝑗)⊤·
𝑯(𝑙−1)
𝑗can be computed within the corresponding C𝑗. With the
help of server, a secure summation is implemented to achieve inter-
client spatial dependency reconstruction in federated context.
4.2.3 Polynomial-based Activation Decomposition. Although the
SprtReLU mechanism reconstructs inter-client spatial dependen-
cies at low computational costs, it suffers from performance degra-
dation due to the loss of crucial inter-client information. This loss
occurs because negative values in the embedding matrix are fil-
tered out prematurely, hindering the model’s capacity to learn com-
plex nonlinear relationships. To this end, we introduce AdptPoLU ,
which approximates 𝑅𝑒𝐿𝑈 activation by polynomials with adap-
tivecoefficients learned from data. This method alleviates the loss
of inter-client information during activation decomposition.
It is well known that most non-linear activation functions can
be represented with polynomials through Taylor expansion. Nu-
merous studies [13, 30, 33, 36] have demonstrated the efficacy of
low-order polynomials in approximating 𝑅𝑒𝐿𝑈 with minimal train-
ing error. Accordingly, we utilize a unified polynomial function to
approximate the activation and retain non-linearity.
Consider a𝐾-order polynomial function
P𝐾(𝑥)=𝐾Õ
𝑘=0𝑝𝑘𝑥𝑘(16)
with coefficient set {𝑝0,𝑝1,...,𝑝𝐾}to be configured. We can rewrite
Eq. (5) as:
˜𝑨𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)=𝑰𝑁+P𝐾(𝑬·𝑬⊤)=𝑰𝑁+𝐾Õ
𝑘=0
𝑝𝑘(𝑬·𝑬⊤)𝑘
.(17)It is worth noting that the polynomials are applied on the matrix
(𝑬·𝑬⊤)after multiplication in an element-wise manner. For each
adjacency block between client C𝑖andC𝑗:
˜𝑨𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)𝑖𝑗=8>> <
>>:𝑰𝑁𝑖+Í𝐾
𝑘=0
𝑝𝑘(𝑬𝑖·𝑬⊤
𝑖)𝑘
if𝑖=𝑗,
Í𝐾
𝑘=0
𝑝𝑘(𝑬𝑖·𝑬⊤
𝑗)𝑘
if𝑖≠𝑗.(18)
To disentangle the node embedding matrices in each term (𝑬𝑖·
𝑬⊤
𝑗)𝑘for𝑘=0,1,...,𝐾 in this𝐾-order polynomial function, we
propose a set of 𝐾+1transforms:
F𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)={𝑓𝑘(·)}𝐾
𝑘=0, (19)
where the𝑘-th transform 𝑓𝑘(·)for the𝑘-th polynomial satisfies:
(𝑬𝑖·𝑬⊤
𝑗)𝑘=𝑓𝑘(𝑬𝑖)·𝑓⊤
𝑘(𝑬𝑗). (20)
Next, we propose concrete formulations of these transforms.
TheoRem 1. There exists a transform function set
F𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)={𝑓𝑘(·)}𝐾
𝑘=0={⊗𝑘(·)}𝐾
𝑘=0(21)
with𝐾+1transform functions satisfying Eq. (20) with no information
loss. The operator⊗𝑘(·)transforms a matrix from R𝑁×𝑑toR𝑁×𝑑𝑘
by applying a 𝑘-th Cartesian power on each row.
Detailed proof is shown in appendix A.1. Based on this theorem,
we rewrite Eq. (17) as:
˜𝑨𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)=𝑰𝑁+𝐾Õ
𝑘=0
𝑝𝑘·⊗𝑘𝑬·
⊗𝑘𝑬⊤
, (22)
and rewrite Eq. (18) as:
˜𝑨𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)𝑖𝑗=8>>>> <
>>>>:𝑰𝑁𝑖+Í𝐾
𝑘=0
𝑝𝑘·⊗𝑘𝑬𝑖·
⊗𝑘𝑬𝑖⊤
if𝑖=𝑗,
Í𝐾
𝑘=0
𝑝𝑘·⊗𝑘𝑬𝑖·
⊗𝑘𝑬𝑗⊤
if𝑖≠𝑗.
(23)
Therefore, the spatial dependencies with 𝐾-order AdptPoLU
𝒁𝐾
(𝐴𝑑𝑝𝑡𝑃𝑜𝐿𝑈)𝑖=𝑯𝑖+𝐾Õ
𝑘=0©­
«𝑝𝑘·⊗𝑘𝑬𝑖·𝑀Õ
𝑗=1
(⊗𝑘𝑬𝑗)⊤·𝑯𝑗ª®
¬.(24)
The parameter 𝐾is critical to balance accuracy and computational
complexity: a larger 𝐾enhances accuracy but increases computa-
tional demands, and vice versa. We can adjust 𝐾for different sce-
narios. Moreover, since the value distribution of model parameters,
especially the node embeddings vary throughout training, a static
set of𝑝0,𝑝1,...,𝑝𝐾is sub-optimal. Instead, we dynamically learn
the polynomial coefficients from data. Each client C𝑖is assigned a
unique set of coefficients 𝑷𝑖=[𝑝𝑖,0,𝑝𝑖,1,...,𝑝𝑖,𝐾]. These coefficients
are iteratively refined using gradient descent in parallel with the
evolution of node embeddings.
4.2.4 Time Complexity Analysis. The time complexity of SprtReLU
is𝑂(𝑑𝐹(𝑀+𝑁)), while for AdptPoLU , it is𝑂
𝑑𝐾𝐹(𝑀+𝑁)
. Both
expressions are presented in plaintext . Unlike the 𝑂 𝑁2(𝑑+𝐹)
complexity in cyphertext of the naive encryption-based solution,
our method achieves a linear complexity ( 𝑂(𝑁)) instead of qua-
dratic complexity ( 𝑂(𝑁2)) with respect to the number of nodes 𝑁.
This is particularly significant because, in real-world applications,
6109KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
Aggregation of Inter-Client Spatial Dependency
Local Training𝑾!,𝒃!,𝑷!,𝑬!Local ModelLocal Training𝑾!,𝒃!,𝑷!,𝑬!Local Training𝑾",𝒃",𝑷",𝑬"𝑨𝑮𝑮"𝑨𝑮𝑮!𝑨𝑮𝑮#&𝑨𝑮𝑮SeverPartialFedAvgAggregation of Partial Model Parameters𝑾",𝒃",𝑷"𝑾!,𝒃!,𝑷!𝑾#,𝒃#,𝑷#……Global Model𝑾$,𝒃$,𝑷$……
LocalSub-graphDataClient 1Client 𝑀
𝓕(𝑬!)𝑯!(&)𝑯!(&(")𝑿!(&(")……Client 𝑖
HistoricalInputOutput𝑨𝑮𝑮!LocalSub-graphDataLocalSub-graphDataSecureSummation
Figur
e 4: Overview of the FedGTP System.
the number of nodes is often the primary scaling factor, and our
approach demonstrates improved scalability in this regard.
4.3 Integration with Temporal Modeling
To integrate the restructured spatial modeling with temporal mod-
eling into a cohesive federated ASTGNN model, we replace the
linear layers in GRU with our reformulated AGCN layers (similar
to Eq. (7)). We first redefine the spatial term 𝒁𝑖, treating it as a
function with input 𝑯, which varies across different GRU gates:
𝒁𝑖(𝑯)=𝑯𝑖+𝐾Õ
𝑘=0©­
«𝑝𝑖,𝑘·𝑓𝑘(𝑬𝑖)·𝑀Õ
𝑗=1
𝑓⊤
𝑘(𝑬𝑗)·𝑯𝑗ª®
¬
=𝑯𝑖+𝑷𝑖·F𝐾(𝑬𝑖)·𝑀Õ
𝑗=1 F⊤
𝐾(𝑬𝑗)·𝑯𝑗.(25)
Then, we obtain our federated ASTGNN at 𝑡-th time slot:
𝒛𝑡
𝑖=𝜎
𝒁𝑖
[𝑿𝑡||𝒉𝑡−1]
·𝑬𝑖·𝑾𝑖,𝒛+𝑬𝑖·𝒃𝑖,𝒛
𝒓𝑡
𝑖=𝜎
𝒁𝑖
[𝑿𝑡||𝒉𝑡−1]
·𝑬𝑖·𝑾𝑖,𝒓+𝑬𝑖·𝒃𝑖,𝒓
˜𝒉𝑡
𝑖=𝑡𝑎𝑛ℎ
𝒁𝑖
[𝑿𝑡||(𝒓𝑡⊙𝒉𝑡−1)]
·𝑬𝑖·𝑾𝑖,˜𝒉+𝑬𝑖·𝒃𝑖,˜𝒉
𝒉𝑡
𝑖=𝒛𝑡
𝑖⊙𝒉𝑡−1
𝑖+(1−𝒛𝑡
𝑖)⊙ ˜𝒉𝑡
𝑖,(26)
In the above equations, 𝑬𝑖,𝑷𝑖,𝑾𝑖,𝒛,𝑾𝑖,𝒓,𝑾𝑖,ˆ𝒉,𝒃𝑖,𝒛,𝒃𝑖,𝒓and𝒃𝑖,ˆ𝒉are
the learnable parameters for client C𝑖, they can all be trained end-
to-end with back-propagation through time.
4.4 System Implementation and Analysis
4.4.1 System Implementation. Previous sections have detailed our
spatial modeling and its integration with temporal modeling in a
federated context. Now we present the system design and imple-
mentation for FedGTP training, where 𝑀clients jointly train the
models under the orchestration of a server. To adapt to real feder-
ated scenarios where each client has its own computing machine,
we build up a system based on socket communication, allowing it
to be deployed on distributed environments in real world. Fig. 4
illustrates the overview of FedGTP system, and Algorithm 1-3 de-
tails the process.
In local training round, each client C𝑖performs forward prop-
agation in parallel on the spatiotemporal model by Eq. (26) with
its local parameters. When inter-client spatial dependencies areAlgorithm
1:FedGTP Framework
input
:Initial global model weights (𝑾(0),𝒃(0),𝑷(0));
Initial personalized node embeddings {𝑬(0)
𝑖}𝑀
𝑖=1;
The number of global and local rounds 𝑅𝑔,𝑅𝑙;
output : Trained model weights (𝑾𝑖,𝒃𝑖,𝑷𝑖,𝑬𝑖)for eachC𝑖;
1Initialize global model weights with (𝑾(0),𝒃(0),𝑷(0));
2foreach clientC𝑖∈Cin parallel do
3 Initialize
personalized node embeddings with 𝑬(0)
𝑖;
4forglobal
round𝑟𝑔=1,2,...,𝑅𝑔do
5 foreach
clientC𝑖∈Cin parallel do
6 Re
ceives global model weights from server to
update 𝑾𝑖,𝒃𝑖,𝑷𝑖;
7 forlocal round𝑟𝑙=1,2,...,𝑅𝑙do
8 For
wards spatial-temporal modeling according
to Eq. (26), during which performs inter-client
spatial aggregation according to Eq. (25) (see
Algorithm 2).
9 Update(𝑾𝑖,𝒃𝑖,𝑷𝑖,𝑬𝑖)through gradient descent.
10 Sends(𝑾𝑖,𝒃𝑖,𝑷𝑖)to
server;
11 Ser
ver performs 𝑃𝑎𝑟𝑡𝑖𝑎𝑙𝐹𝑒𝑑𝐴𝑣𝑔 to update(𝑾𝑔,𝒃𝑔,𝑷𝑔);
12r
eturn(𝑾𝑖,𝒃𝑖,𝑷𝑖,𝑬𝑖)for eachC𝑖;
Algorithm
2:Inter-Client Spatial Aggregation
input
:Input state 𝑯𝑡
𝑖onC𝑖;
output : Output state 𝒁𝑡
𝑖aggregating inter-client spatial
dependencies;
1//Transform node embedding matrix
2F𝐾(𝑬𝑖)←{𝑓𝑘(𝑬𝑖)}𝐾
𝑘=0;
3//Aggregation of local spatial dependencies
4𝑨𝑮𝑮𝑖←F⊤
𝐾(𝑬𝑖)·𝑯𝑡
𝑖;
5Send 𝑨𝑮𝑮𝑖to server;
6ReceiveÍ𝑨𝑮𝑮←𝑆𝑒𝑐𝑢𝑟𝑒𝑆𝑢𝑚𝑚𝑎𝑡𝑖𝑜𝑛 from server;
7//Aggregation of inter-client spatial dependencies
8𝒁𝑡
𝑖=𝑯𝑡
𝑖+𝑷𝑖·F𝐾(𝑬𝑖)·Í𝑨𝑮𝑮 ;
9return 𝒁𝑡
𝑖;
Algorithm
3:Functions on Server
1Se
cureSummation:
2 Wait for each client to upload F⊤
𝐾(𝑬𝑖)·𝑯𝑡
𝑖;
3 Sum and broadcastÍ𝑀
𝑖=1
F⊤
𝐾(𝑬𝑖)·𝑯𝑡
𝑖
to clients;
4PartialFedAvg:
5 Wait for each client to upload (𝑾𝑖,𝒃𝑖,𝑷𝑖);
6(𝑾𝑔,𝒃𝑔,𝑷𝑔)←Í𝑀
𝑖=1𝑁𝑖
𝑁(𝑾𝑖,𝒃𝑖,𝑷𝑖);
r
equired, the activation decomposition mechanism is activated ac-
cording to Eq. (25). Each C𝑖first transforms 𝑬𝑖and aggregates it
with 𝑯𝑖into𝑨𝑮𝑮𝑖containing local spatial information, which is
then uploaded to server. These aggregated intermediate results do
6110FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain.
not violate privacy standards such as GDPR [38]. For enhanced pri-
vacy protection, e.g.against inferences from intermediate results,
we can employ established secure multi-party summation meth-
ods such as Homomorphic Encryption [34], Secret Sharing [41],
Differential Privacy [10], and other MPC Protocols [25]. While the
specifics of these secure summation techniques are beyond our
scope, they serve as adaptable modules, balancing security, accu-
racy, and efficiency according to the targeting scenarios. The server
further aggregates the uploaded information by SecureSummation
and broadcasts the cumulative result containing inter-client spatial
dependencies to clients for subsequent computations. The learn-
able model parameters in C𝑖include weight pool 𝑾𝑖, bias pool 𝒃𝑖,
polynomial coefficients 𝑷𝑖and node embedding matrix 𝑬𝑖. Note
that𝑬𝑖contains local sub-graph-specific node embeddings for each
C𝑖, and its size varies due to different node counts. To preserve the
private region-specific characteristics in the embeddings and re-
tain personalization, each client shares only (𝑾𝑖,𝒃𝑖,𝑷𝑖)for partial
aggregation (PartialFedAvg ), while keeping 𝑬𝑖local.
4.4.2 Communication Cost Analysis. The communication cost of
FedGTP is𝑂(|Θ|·𝑀·𝑅𝑔+(𝑀·𝑑𝐾·𝐹)·𝑅𝑙), which consists of
two parts. The first part, 𝑂(|Θ|·𝑀·𝑅𝑔), arises from the aggrega-
tion of model parameters, a common aspect of all current feder-
ated graph learning methods, where |Θ|denotes the model size.
Since we keep 𝑬𝑖personalized for each client, |Θ|is reduced. The
second part, 𝑂((𝑀·𝑑𝐾·𝐹)·𝑅𝑙), is due to the aggregation of inter-
client spatial dependency. Unlike state-of-the-art baselines which
are proportional [32] to or quadratic [27, 52] to 𝑁, FedGTP incurs a
cost linear with 𝑀, thanks to the Activation Decomposition Mech-
anism. Since each client manages hundreds to thousands of nodes
locally,𝑀is orders of magnitude smaller than 𝑁. Therefore our
solution results in much lower communication costs than these
baselines in the real-world cross-silo scenarios where 𝑁is in the
tens of thousands, while 𝑀is usually around 10.
5 EXPERIMENTS
5.1 Experimental Setup
5.1.1 Datasets. We test on the following real-world traffic datasets:
METR-LA, PeMS Data and TaxiFlow-BJ. METR-LA contains traffic
speed collected from the highway of the Los Angeles County road
network over 4 months. PeMS Data3comprises detector data of
traffic speed, flow and occupancy from the Caltrans Performance
Measurement System (PeMS), and different subsets of PeMS Data
have been used in previous studies [4, 15, 32, 52]. TaxiFlow-BJ is
a dataset assembled by collecting data from five taxi companies
in Beijing, covering the period from June 1st to August 31st, 2013.
We preprocess the raw taxi trajectory data and map it to the road
network, which is obtained from OpenStreetMap4. Then, we iden-
tify the area with the highest concentration of mappings, which
encompasses 1905 road segments. Based on the matching results,
we can obtain traffic flow data for different road segments of each
company, which can then be used to conduct federated traffic pre-
dictions among five clients. In graph modeling of traffic data, each
node represents a sensor in METR-LA and PeMS Data, whereas
3http://p
ems.dot.ca.gov/
4https://www.openstreetmap.org/in TaxiFlow-BJ, each node corresponds to a road segment. More
details of the datasets are shown in Tab. 3.
5.1.2 Default Configuration and Environment. In the default hyper-
parameter configurations, the hidden feature dimension 𝐹=64
with 2 hidden layers. The embedding dimension 𝑑=2, and the
polynomial coefficient 𝐾=4. The learning rate 𝜂=0.003, and the
batch size is 64. The models undergo 200 global epochs and 2 lo-
cal epochs. Additionally, both the validation ratio and the testing
ratio are set to 0.2. The performance metrics include Root Mean
Square Error (RMSE), Mean Absolute Error (MAE), and Mean Ab-
solute Percentage Error (MAPE). A lower value of these metrics
indicates a better prediction performance. All experiments are im-
plemented with PyTorch 1.13.1 and conducted on Intel(R) Xeon(R)
Gold 6230R CPU @ 2.10GHz and four NVIDIA A100-PCIE-40GB
GPUs with CUDA 11.6.
5.2 Baseline Comparison
5.2.1 Baselines and Settings. To evaluate the overall effectiveness
of our work, we align and compare FedGTP with state-of-the-art
baselines in distinct federated settings where they show best per-
formance. Due to the lack of available source code for some of these
baselines (except for CNFGNN), we rely on the results reported in
papers when our reproduced performance is sub-optimal. These
compared baselines can be classified into two categories:
Baselines overlooking inter-client spatial dependencies. This cat-
egory of baselines largely under-utilize inter-client spatial depen-
dencies due to privacy constraints, including:
•FASTGNN [52]: It introduces a federated attention-based
STGNN and constructs a random spatial connection among
clients. The attention-based STGNN consists of a two-layer
GRU, with dimensions of 64 and 256, along with a graph at-
tention network. Following the setting in [52], we conduct
traffic speed prediction on PeMSD7 with 𝑀=4,𝑇𝑖𝑛=12
and𝑇𝑜𝑢𝑡=9.
•MFVSTGNN [27]: It introduces a FL-based traffic forecast-
ing model that utilizes a Variational Graph AutoEncoder
(VGAE) to enhance intra-client dependencies and employs
STGNNs for prediction. We compare our results with those
reported in [27], which are the best (using Graph WaveNet
[45] for prediction) on PEMS-BAY and METR-LA, with 𝑀=
8,𝑇𝑖𝑛=12, and𝑇𝑜𝑢𝑡=12.
•FLoS [43]: It constructs a FL framework with opportunistic
client selection for traffic flow prediction. The local model
uses GRU and GCN to capture spatial-temporal dependen-
cies, the hidden dimension is set to 32. The results are based
on PeMSD4 with 𝑀=5,𝑇𝑖𝑛=24, and𝑇𝑜𝑢𝑡=12.
Baselines considering inter-client spatial dependencies. This cate-
gory of baselines treat inter-client spatial dependencies as public
and predefined, including:
•CNFGNN [32]: It uses a GRU-based model on each node
(client) to extract the temporal features with local data, and
performs GNN with a pre-defined graph on the server to
capture inter-node spatial dependencies. The model on each
node has 1 layer GRU with the hidden dimension of 64, while
the model on the server is a 2-layer GNN. Following the
6111KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
Table 1: Comparison of performance on the traffic predic-
tion task between FedGTP and baselines which do not con-
sider inter-client spatial dependencies.
baseline
federated setting task method RMSE MAE MAPE(%)
F
ASTGNN
[52]PeMSD7
(4
clients, 12->9)5sp
eedF
ASTGNN 5.83 3.50 8.36
Fe
dGTP 4.73 2.60 5.88
MF
VSTGNN
[27]PEMS-BA
Y
(8 clients, 12->12)sp
eedMF
VSTGNN 3.91 1.93 4.48
Fe
dGTP 3.88 1.79 3.54
METR-LA
(8
clients, 12->12)sp
eedMF
VSTGNN 4.45 3.35 9.42
Fe
dGTP 4.41 3.23 8.75
FLoS
[43]PeMSD4
(5
clients, 24->12)flo
wFLoS 42.84
28.64 -6
Fe
dGTP 41.33 25.62 -
same
setting as in [32], we predict the traffic speed on PEMS-
BAY and METR-LA with 𝑇𝑖𝑛=12and𝑇𝑜𝑢𝑡=12.
•FCGCN [47]: It proposes a framework that combines a two-
layer GCN with FL. Both GCN layers include a ReLU activa-
tion. To ensure a fair comparison, we compare our approach
with FCGCN in a setting where all clients participate in the
FL aggregation process. The results are employed to predict
traffic flow, speed, and occupancy on the PeMS04 ( 𝑀=28)
and PeMS08 ( 𝑀=14) with𝑇𝑖𝑛=6and𝑇𝑜𝑢𝑡=1.
•CTFL [53]: It designs a clustering-based FL framework for
STGNNs to forecast traffic speed in the federated scenario.
We utilize the results reported in [53], which include two
STGNNs, namely STGCN[50] and MTGNN[46]. The experi-
ments were conducted on PeMSD4 and PeMSD7 with 𝑀=8,
𝑇𝑖𝑛=12and𝑇𝑜𝑢𝑡=9.
5.2.2 Results and Analysis. Tab. 1 and Tab. 2 present the compar-
ison of prediction performance between FedGTP and the two cat-
egories of baselines. From Tab. 1 we can observe that our pro-
posed FedGTP exhibits superior performance to the counterparts
[27, 43, 52] that also protect spatial privacy, owing to the ability
of FedGTP to fully recover and exploit the disrupted inter-client
spatial dependencies. Moreover, as shown in Tab. 2, FedGTP con-
sistently outperforms its competitors [32, 47, 53] that utilize inter-
client spatial dependencies. This is primarily because these base-
lines rely on predefined spatial graphs, whereas FedGTP adaptively
uncovers and leverages more profound inter-client spatial depen-
dencies. Additionally, FedGTP provides stronger privacy protec-
tion compared to these baselines, as they treat the spatial graph
as public. The overall results indicate that our proposed FedGTP
has improved over existing works on average by 21.08%, 13.48%,
19.90%decrease on RMSE, MAE and MAPE, respectively.
5.3 Ablation Studies
5.3.1 Impact of Inter-Client Spatial Dependency Reconstruction. To
quantitatively investigate the impact of inter-client spatial depen-
dency reconstruction on the performance of FedGTP, we randomly
eliminate these dependencies reconstructed involving each client
and evaluate the prediction accuracy across a spectrum of elimina-
tion rates at{0.0,0.5,1.0}. Here,𝜌=0.0represents the full utiliza-
tion of inter-client spatial dependencies, whereas 𝜌=0.5indicates
a partial, 50%elimination, and 𝜌=1.0corresponds to the complete
5“𝑇𝑖
𝑛->𝑇𝑜𝑢𝑡” indicates prediction of 𝑇𝑜𝑢𝑡time slots based on previous 𝑇𝑖𝑛slots.
6The absence of values for certain baselines is due to the inability to replicate models
and the lack of reported metrics in the publications. The same applies to Tab. 2.Table 2: Comparison of performance on the traffic predic-
tion task between FedGTP and and baselines which consider
inter-client spatial dependencies as public and predefined.
baseline
federated setting task method RMSE MAE MAPE(%)
CNFGNN
[32]PEMS-BA
Y
(325 clients, 12->12)sp
eedCNFGNN
3.7090 2.3528 4.82
Fe
dGTP 3.6440 1.6813 3.35
METR-LA
(207
clients, 12->12)sp
eedCNFGNN
11.4137 7.5161 36.26
Fe
dGTP 10.3978 4.2883 24.83
FCGCN
[47]PeMSD4
(28
clients, 6->1)flo
wFCGCN
29.6775 18.6483 22.57
Fe
dGTP 26.6993 17.9049 13.99
sp
eedFCGCN
1.8777 1.0008 1.84
Fe
dGTP 1.6800 0.9493 1.72
o
ccFCGCN
0.1181 0.0066 18.92
Fe
dGTP 0.0126 0.0064 16.03
PeMSD8
(14
clients, 6->1)flo
wFCGCN
22.4601 14.7723 11.81
Fe
dGTP 20.4897 13.7754 9.09
sp
eedFCGCN
1.5570 0.8228 1.54
Fe
dGTP 1.4221 0.7616 1.35
o
ccFCGCN
0.0598 0.0058 12.91
Fe
dGTP 0.0110 0.0054 10.05
CTFL
[53]PeMSD4
(8
clients, 12->9)sp
eedCTFL-ST
GCN 4.87 - 4.84
CTFL-MT
GNN 4.91 - 4.79
Fe
dGTP 3.42 - 3.78
PeMSD7
(8
clients, 12->9)sp
eedCTFL-ST
GCN 5.25 - 7.08
CTFL-MT
GNN 5.23 - 7.08
Fe
dGTP 4.89 - 6.88
elimination
of these reconstructed dependencies. Due to space lim-
itation, we present in Fig. 5a the normalized error in form of the
three metrics on only PeMSD7 (results on other datasets are simi-
lar, shown in A.2). We can observe that the errors rise by around 8%
upon the elimination of half the inter-client spatial dependencies,
with a surge of up to 16.47%in error when these dependencies are
completely removed. From the above results, we can conclude that
the full reconstruction of inter-client spatial dependencies indeed
boosts the accuracy of traffic prediction.
5.3.2 Impact of Activation Decomposition. To evaluate the effec-
tiveness of our proposed activation decomposition mechanism, in-
cluding SprtReLU andAdptPoLU , we run experiments on PeMSD7
(results on other datasets are similar, shown in A.2) with ctrand
sglas two controls. The term ctrdenotes centralized training with
all sub-graphs from clients joined together. The term sglrepresents
each single client training solely based on its local data. From the
results in Fig. 5(b)-(d), we obtain the following observations:
•TheSprtReLU mechanism can help improve prediction accu-
racy compared to sglby utilizing inter-client spatial depen-
dency under privacy constraints. However, there is still a
non-negligible gap compared to ctrdue to information loss.
•TheAdptPoLU mechanism can compensate the performance
gap between SprtReLU andctrby using adaptive polynomial
approximation. AdptPoLU has the similar performance with
SprtReLU when𝐾=2, and performs almost close to ctr,
which we regard as optimal, when 𝐾rises to 4.
•TheAdptPoLU mechanism will accelerate the convergence
of training process when 𝐾rises, and sometimes it may even
outperform ctrin RMSE (see Fig. 5c). We attribute it to the
adaptive coefficients facilitating the training process.
6112FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
8.21%
 8.01%
 7.94%
16.47%
15.34%
 15.69%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs4.24.44.64.85.05.25.45.65.8MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition on
MAE
20 40 60 80100 120 140 160 180 200
Training epochs8.08.59.09.510.010.511.0RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)
(
c)Impact of activation decomposition on
RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.0900.0950.1000.1050.1100.115MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 5: Ablation results on impact of inter-client spatial
dependency (a) and activation decomposition (b)-(d).
5.4 Case Study
To develop an in-depth understanding of inter-client spatial depen-
dency rooted in practical scenarios, we conduct a case study on
TaxiFlow-BJ dataset, which scatters traffic data in five distinct taxi
companies, to carry out federated traffic flow prediction.
Firstly, we compare our FedGTP incorporating inter-client de-
pendency against that without it, discovering that the former yields
a decrease in MAE by around 3%. To delve deeper, we explore
the relationship between the MAE decrease for each node and the
adaptively learned weights of inter-client edges connecting it, which
are determined through the product of node embeddings. Specifi-
cally, we select the top 20%nodes with the largest decrease in MAE
as the most benefited ones. Regarding the extensive number of
edges, we focus on the top 𝑝edges with the largest weights, which
denote the key inter-client spatial dependencies. Fig. 6 illustrates
the proportion of edges linked to the benefited nodes against those
not linked, with 𝑝ranging from 20 to 200. We can observe that
over 69%of these critical edges are linked to nodes exhibiting re-
markable performance enhancement. This proportion reaches 95%
especially for the top 20 edges, far exceeding the average proba-
bility level of 36%. This indicates that the adaptively learned inter-
client connections play a vital role in reducing prediction errors,
especially through effectively excavating complex relationships be-
tween nodes, thereby enhancing overall prediction accuracy.
Furthermore, we present a concrete example to demonstrate
how adaptively learned inter-client spatial dependencies contribute
to performance improvement. We focus our gaze on one of the sig-
nificantly benefited nodes and its associated key dependencies. As
depicted in Fig. 7, this node is identified as a road segment near
a school, besides, all nodes linked through these dependencies are
also road segments in the school surroundings. This observation
complies to the intuition that the same functional areas possess
similar spatiotemporal characteristics, and it is logical to infer that
20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200
p020406080100Proportion
5.0%95.0%
6.7%93.3%
20.0%80.0%
22.0%78.0%
21.7%78.3%
21.4%78.6%
20.0%80.0%
18.9%81.1%
20.0%80.0%
21.8%78.2%
23.3%76.7%
23.8%76.2%
23.6%76.4%
25.3%74.7%
25.6%74.4%
27.6%72.4%
29.4%70.6%
30.0%70.0%
31.0%69.0%edges not linked to nodes with significant improvement edges linked to nodes with significant improvementFigur
e 6: The strong correlation between performance im-
provement and the learned dependencies.
road segment with high performance improvementassociated road segment(color refers to different clients)school location
Figur
e 7: Case study of a concrete node with significant per-
formance improvement.
such semantic dependencies can enhance prediction performance.
This example also underlines that even though the data of these in-
teracted road segments are dispersed across multiple clients with
no direct access, our method can still overcome this barrier to fully
utilize and learn the inter-client spatial dependencies.
6 CONCLUSION
In this paper, we propose FedGTP, a pioneering framework de-
signed for federated graph-based traffic prediction that fully ex-
ploits inter-client spatial dependencies with privacy preservation.
FedGTP progresses by adaptive learning of inter-client spatial de-
pendencies, enabling deeper exploration of spatial relationships
and thus boosting prediction performance. For privacy protection,
we introduce an innovative polynomial-based activation decom-
position mechanism that ensures compliance with privacy regula-
tions like GDPR. Extensive experiments on real-world traffic datasets
have been conducted to validate our approach. Future work will
address challenges arising from asynchronous scenarios to better
accommodate them in large-scale applications.
ACKNOWLEDGMENTS
We thank the reviewers for their constructive comments. This work
is partially supported by National Science Foundation of China
(NSFC) under Grant Nos. U21A20516 and 62336003, Beijing Natu-
ral Science Foundation under Grant No. Z230001, Beihang Univer-
sity Basic Research Funding No. YWF-22-L-531, Chow Sang Sang
Group Research Fund No. 9229139, Didi Collaborative Research
Program NO2231122-00047. Xiaoxi He and Yongxin Tong are the
corresponding authors.
6113KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
REFERENCES
[1] Abbas Acar, Hidayet Aksu, A. Selcuk Uluagac, and Mauro Conti. 2018. A sur-
vey on homomorphic encryption schemes: Theory and implementation. ACM
Comput. Surv. 51, 4 (2018), 1–35.
[2] Lei Bai, Lina Yao, Salil S. Kanhere, Xianzhi Wang, et al. 2019. STG2Seq: Spatial-
Temporal Graph to Sequence Model for Multi-step Passenger Demand Forecast-
ing. In IJCAI. ijcai.org, 1981–1987.
[3] Lei Bai, Lina Yao, Salil S. Kanhere, Zheng Yang, et al. 2019. Passenger De-
mand Forecasting with Multi-Task Convolutional Recurrent Neural Networks.
InPAKDD, Vol. 11440. Springer, 29–42.
[4] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, et al. 2020. Adaptive Graph Convolu-
tional Recurrent Network for Traffic Forecasting. In NeurIPS.
[5] Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, et al. 2020. Multi-Range Attentive
Bicomponent Graph Convolutional Network for Traffic Forecasting. In AAAI.
AAAI Press, 3529–3536.
[6] Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer, and Yulia R. Gel.
2022. TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge
Representation with Spatio-Supra Graph Convolutional Networks for Time-
Series Forecasting. In ICLR. OpenReview.net.
[7] Yuzhou Chen, Ignacio Segovia-Dominguez, and Yulia R. Gel. 2021. Z-GCNETs:
Time Zigzags at Graph Convolutional Networks for Time Series Forecasting. In
ICML, Vol. 139. PMLR, 1684–1694.
[8] Kyunghyun Cho, Bart Merrienboer, Caglar Gulcehre, and Dzmitry Bahdanau
others. 2014. Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation. In EMNLP. ACL, 1724.
[9] Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong Park. 2022.
Graph Neural Controlled Differential Equations for Traffic Forecasting. In AAAI.
AAAI Press, 6367–6374.
[10] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. 2006. Cali-
brating Noise to Sensitivity in Private Data Analysis. In TCC, Vol. 3876. Springer,
265–284.
[11] Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. 2020. Personal-
ized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-
Learning Approach. In NeurIPS.
[12] Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, et al. 2022. Federated
Graph Machine Learning: A Survey of Concepts, Techniques, and Applications.
SIGKDD Explor. 24, 2 (2022), 32–47.
[13] Karthik Garimella, Nandan Kumar Jha, and Brandon Reagen. 2021. Sisyphus:
A Cautionary Tale of Using Low-Degree Polynomial Activations in Privacy-
Preserving Deep Learning. CoRR abs/2107.12342 (2021).
[14] Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, et al. 2019. Spatiotempo-
ral Multi-Graph Convolution Network for Ride-Hailing Demand Forecasting. In
AAAI. AAAI Press, 3656–3663.
[15] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, et al. 2019. Attention Based
Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting.
InAAAI. AAAI Press, 922–929.
[16] Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, et al. 2022. Learning
Dynamics and Heterogeneity of Spatial-Temporal Graph Data for Traffic Fore-
casting. IEEE Trans. Knowl. Data Eng. 34, 11 (2022), 5415–5428.
[17] Jizhou Huang, Zhengjie Huang, Xiaomin Fang, Shikun Feng, et al. 2022. DuETA:
Traffic Congestion Propagation Pattern Modeling via Efficient Graph Learning
for ETA Prediction at Baidu Maps. In CIKM. ACM, 3172–3181.
[18] Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, et al. 2020. LSGCN:
Long Short-Term Traffic Prediction with Graph Convolutional Networks. In IJ-
CAI. ijcai.org, 2355–2361.
[19] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR. OpenReview.net.
[20] Xiangyuan Kong, Jian Zhang, Xiang Wei, Weiwei Xing, et al. 2022. Adaptive
spatial-temporal graph attention networks for traffic flow forecasting. Appl. In-
tell.52, 4 (2022), 4300–4316.
[21] Tian Lan, Ziyue Li, Zhishuai Li, Lei Bai, Man Li, Fugee Tsung, Wolfgang Ketter,
Rui Zhao, and Chen Zhang. 2023. MM-DAG: Multi-task DAG Learning for Multi-
modal Data - with Application for Traffic Congestion Analysis. In KDD. ACM,
1188–1199.
[22] Can Li and Wei Liu. 2023. Multimodal Transport Demand Forecasting via Fed-
erated Learning. IEEE Trans. Intell. Transp. Syst. (2023).
[23] Wenzhu Li and Shuang Wang. 2022. Federated meta-learning for spatial-
temporal prediction. Neural Comput. Appl. 34, 13 (2022), 10355–10374.
[24] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion Convolutional
Recurrent Neural Network: Data-Driven Traffic Forecasting. In ICLR. OpenRe-
view.net.
[25] Yehuda Lindell. 2020. Secure Multiparty Computation (MPC). IACR Cryptol.
ePrint Arch. (2020), 300.
[26] Fan Liu, Weijia Zhang, and Hao Liu. 2023. Robust Spatiotemporal Traffic Fore-
casting with Reinforced Dynamic Adversarial Training. In KDD. ACM, 1417–
1428.
[27] Lei Liu, Yuxing Tian, Chinmay Chakraborty, Jie Feng, et al. 2023. Multilevel Fed-
erated Learning-Based Intelligent Traffic Flow Forecasting for TransportationNetwork Management. IEEE Trans. Netw. Serv. Manag. 20, 2 (2023), 1446–1458.
[28] Qingxiang Liu, Sheng Sun, Min Liu, Yuwei Wang, and Bo Gao. 2023. Online
Spatio-Temporal Correlation-Based Federated Learning for Traffic Flow Fore-
casting. CoRR abs/2302.08658 (2023).
[29] Rui Liu and Han Yu. 2024. Federated Graph Neural Networks: Overview, Tech-
niques and Challenges. IEEE Trans. Neural Networks Learn. Syst. (2024).
[30] Qian Lou, Yilin Shen, Hongxia Jin, and Lei Jiang. 2021. SAFENet: A Secure,
Accurate and Fast Neural Network Inference. In ICLR. OpenReview.net.
[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. 2017.
Communication-Efficient Learning of Deep Networks from Decentralized Data.
InAISTATS. 1273–1282.
[32] Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2021. Cross-Node Feder-
ated Graph Neural Network for Spatio-Temporal Data Modeling. In KDD. ACM,
1202–1211.
[33] Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng,
et al. 2020. Delphi: A Cryptographic Inference Service for Neural Networks.
InUSENIX Security Symposium. USENIX Association, 2505–2522.
[34] Pascal Paillier. 1999. Public-Key Cryptosystems Based on Composite Degree
Residuosity Classes. In EUROCRYPT, Vol. 1592. Springer, 223–238.
[35] Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, et al. 2019. Urban Traffic Pre-
diction from Spatio-Temporal Data Using Deep Meta Learning. In KDD. ACM,
1720–1730.
[36] Hongwu Peng, Shaoyi Huang, Tong Zhou, Yukui Luo, et al. 2023. Autorep: Au-
tomatic relu replacement for fast private network inference. IEEE, 5155–5165.
[37] Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, et al. 2017.
Privacy-Preserving Deep Learning via Additively Homomorphic Encryption.
IEEE Trans. Inf. Forensics Secur. 13, 5 (2017), 1333–1345.
[38] Emanuela Podda. 2021. Shedding light on the legal approach to aggregate data
under the GDPR & the FFDR. In conference of European statisticians Expert Meet-
ing on Statistical Data Confidentiality.
[39] Tao Qi, Lingqiang Chen, Guanghui Li, Yijing Li, et al. 2023. FedAGCN: A traf-
fic flow prediction framework based on federated learning and Asynchronous
Graph Convolutional Network. Appl. Soft Comput. 138 (2023), 110175.
[40] Ronald L Rivest, Len Adleman, Michael L Dertouzos, et al. 1978. On data banks
and privacy homomorphisms. Found. of Sec. Comp. 4, 11 (1978), 169–180.
[41] Adi Shamir. 1979. How to Share a Secret. Commun. ACM 22, 11 (1979), 612–613.
[42] Beibei Wang, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2021. GSNet: learn-
ing spatial-temporal correlations from geographical and semantic aspects for
traffic accident risk forecasting. In AAAI. AAAI Press, 4402–4409.
[43] Hanqiu Wang, Rongqing Zhang, Xiang Cheng, and Liuqing Yang. 2022. Fed-
erated Spatio-Temporal Traffic Flow Prediction Based on Graph Convolutional
Network. In WCSP. IEEE, 221–225.
[44] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, et al. 2021. A Compre-
hensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn.
Syst. 32, 1 (2021), 4–24.
[45] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, et al. 2019. Graph WaveNet
for Deep Spatial-Temporal Graph Modeling. In IJCAI. ijcai.org, 1907–1913.
[46] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, et al. 2020. Connecting the
dots: Multivariate time series forecasting with graph neural networks. In KDD.
ACM, 753–763.
[47] Mengran Xia, Dawei Jin, and Jingyu Chen. 2023. Short-Term Traffic Flow Pre-
diction Based on Graph Convolutional Networks and Federated Learning. IEEE
Trans. Intell. Transp. Syst. 24, 1 (2023), 1191–1203.
[48] Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial Temporal Graph Convo-
lutional Networks for Skeleton-Based Action Recognition. In AAAI. AAAI Press,
7444–7452.
[49] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Ma-
chine Learning: Concept and Applications. ACM Trans. Intell. Syst. Technol. 10,
2 (2019), 12:1–12:19.
[50] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Con-
volutional Networks: A Deep Learning Framework for Traffic Forecasting. In
IJCAI. ijcai.org, 3634–3640.
[51] Xiaoming Yuan, Jiahui Chen, Jiayu Yang, Ning Zhang, et al. 2023. FedSTN: Graph
Representation Driven Federated Learning for Edge Computing Enabled Urban
Traffic Flow Prediction. IEEE Trans. Intell. Transp. Syst. 24, 8 (2023), 8738–8748.
[52] Chenhan Zhang, Shuyu Zhang, JQ James, and Shui Yu. 2021. FASTGNN: A topo-
logical information protected federated learning approach for traffic speed fore-
casting. IEEE Trans. Ind. Informatics 17, 12 (2021), 8464–8474.
[53] Chenhan Zhang, Shiyao Zhang, Shui Yu, and James Yu. 2022. Graph-Based
Traffic Forecasting via Communication-Efficient Federated Learning. In WCNC.
IEEE, 2041–2046.
[54] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. GMAN:
A Graph Multi-Attention Network for Traffic Prediction. In AAAI. AAAI Press,
1234–1241.
[55] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, et al. 2020. RiskOracle:
A minute-level citywide traffic accident forecasting framework. In AAAI. AAAI
Press, 1258–1265.
6114FedGTP:Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain.
A APPENDIX
A.1 Proof of Theorem 1
PRoof. Without loss of generality, we focus on an arbitrary el-
ement𝑒𝑟,𝑠on the𝑟-th row and 𝑠-th column of the matrix (𝑬𝑖·𝑬⊤
𝑗)
after multiplication, where 1≤𝑟≤𝑁𝑖and 1≤𝑠≤𝑁𝑗. It is re-
sulted from the inner product of the 𝑟-th row vector of 𝑬𝑖and the
𝑠-th row vector of 𝑬𝑗:
𝑒𝑟,𝑠=
𝒆𝑖,𝑟,𝒆𝑗,𝑠
,
the𝑘-th power of which is
𝑒𝑘
𝑟,𝑠=©­­­
«h
𝑒1
𝑖,𝑟···𝑒𝑑
𝑖,𝑟i
·26666664𝑒1
𝑗,𝑠
...
𝑒𝑑
𝑗,𝑠37777775ª®®®
¬𝑘
=
𝑒1
𝑖,𝑟𝑒1
𝑗,𝑠+𝑒2
𝑖,𝑟𝑒2
𝑗,𝑠+...+𝑒𝑑
𝑖,𝑟𝑒𝑑
𝑗,𝑠𝑘
=
 𝒆𝑖,𝑟⊗𝒆𝑖,𝑟⊗···⊗ 𝒆𝑖,𝑟, 𝒆𝑗,𝑠⊗𝒆𝑗,𝑠⊗···⊗ 𝒆𝑗,𝑠
=D
⊗𝑘𝒆𝑖,𝑟,⊗𝑘𝒆𝑗,𝑠E
,
where⊗denotes the Cartesian product between vectors, and ⟨·,·⟩
indicates the inner product between vectors.
Therefore,(𝑬𝑖·𝑬⊤
𝑗)𝑘=(⊗𝑘𝑬𝑖)·(⊗𝑘𝑬𝑗)⊤,
and we can set 𝑓𝑘(𝑬𝑖)=⊗𝑘𝑬𝑖. □
A.2 Additional Experiment Results
Table 3: Statistics of datasets.
Dataset Perio d(m/d/y) #Intervals #Nodes Max Min Mean Median Std
METR-LA 03/01/12-06/30/12 34272 207 70 0 53.719 62.4444 20.2614
PEMS-BA Y 01/01/17-05/31/17 52116 325 85.1 0 62.6196 65.3 9.5944
PeMSD4Flow
01/01/18-02/28/18 16992 307919 0 211.7008 180 158.0684
Occupancy 0.7716 0 0.0528 0.0443 0.0495
Speed 85.2 3 63.4706 65.6 8.3557
PeMSD8Flow
07/01/16-08/31/16 17856 1701147 0 230.6807 215 146.217
Occupancy 0.8955 0 0.0651 0.0601 0.0459
Speed 82.3 3 63.763 64.9 6.652
PeMSD7 05/01/12-06/30/12 12672 228 82.6 3 58.8892 64.1 13.4833
Fig. 8-Fig. 15 are results of ablation studies on more datasets.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
0.22%
1.50%
-0.45%
7.04%
 6.74%
3.87%
ρ= 0.0
ρ= 0.5
ρ= 1.0
(a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs6.06.57.07.58.0MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(b) Impact of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs12.513.013.514.014.515.015.516.0RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)
(c)Impact of activation decomposition on
RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.1800.1850.1900.1950.2000.205MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(d)Impact of activation decomposition on
MAPE
Figure 8: Ablation results on impact of inter-client depen-
dency (a) and activation decomposition (b)-(d) on METR-LA.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
2.21%
 1.83%
 2.31%
5.08%
 4.68%
 5.25%
ρ= 0.0
ρ= 0.5
ρ= 1.0(a)Impact
of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs2.02.22.42.62.83.03.23.4MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(b) Impact
of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs4.04.55.05.56.06.57.07.5RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)
(c)Impact
of activation decomposition
on RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.0400.0450.0500.0550.0600.065MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(d)Impact
of activation decomposition on
MAPE
Figure 9: Ablation Study on inter-client spatial dependency
(a) and activation decomposition (b)-(d) on PEMS-BAY.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
4.83%
4.07%
5.02%
8.62%
6.93%
7.74%
ρ= 0.0
ρ= 0.5
ρ= 1.0
(a)Impact
of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs25303540455055MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(b)Impact
of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs4050607080RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)
(c)Impact
of activation decomposition
on RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.200.250.300.350.40MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(d)Impact
of activation decomposition on
MAPE
Figure 10: Ablation Study on inter-client spatial dependency
(a) and activation decomposition (b)-(d) on PeMSD4-FLOW.
6115KDD’24, August 25–29, 2024, Barcelona, Spain. Linghua Yang et al.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
1.42%
3.09%
1.10%
4.39%
3.12%
4.28%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs0.0080.0090.0100.0110.0120.0130.0140.015MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition on
MAE
20 40 60 80100 120 140 160 180 200
Training epochs0.0180.0200.0220.0240.0260.028RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
c)Impact of activation decomposition on
RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.2250.2500.2750.3000.3250.3500.3750.400MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 11: Ablation Study on inter-client spatial dependency (a) and activation decomposition (b)-(d) on PeMSD4-OCCUP.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
1.85%
 1.79%
 1.63%
 1.59%
0.64%
 1.04%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs1.71.81.92.02.12.22.32.42.5MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs3.63.84.04.24.44.64.85.05.2RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
c)Impact of activation decomposition
on RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.0300.0320.0340.0360.0380.0400.042MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 12: Ablation Study on inter-client spatial dependency (a) and activation decomposition (b)-(d) on PeMSD4-SPEED.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
5.16%
 4.57%
1.72%
11.98%
10.04%
 10.64%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs253035404550MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs354045505560657075RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
c)Impact of activation decomposition
on RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.140.160.180.200.220.240.260.28MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 13: Ablation Study on inter-client spatial dependency (a) and activation decomposition (b)-(d) on PeMSD8-FLOW.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
2.88%
1.38%
3.30%
7.40%
3.41%
6.43%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs0.0080.0100.0120.0140.016MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition on
MAE
20 40 60 80100 120 140 160 180 200
Training epochs0.0180.0200.0220.0240.026RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
c)Impact of activation decomposition on
RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.160.180.200.220.240.260.280.30MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 14: Ablation Study on inter-client spatial dependency (a) and activation decomposition (b)-(d) on PeMSD8-OCCUP.
MAE RMSE MAPE
Metrics0.80.91.01.11.21.31.41.5
0.83%
 0.84%
 0.82%
4.65%
3.85%
4.63%
ρ=0.0
ρ=0.5
ρ=1.0
(
a)Impact of inter-client spatial depen-
dency on performance
20 40 60 80100 120 140 160 180 200
Training epochs1.41.51.61.71.81.92.02.1MAE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
b)Impact of activation decomposition
on MAE
20 40 60 80100 120 140 160 180 200
Training epochs3.23.43.63.84.04.24.4RMSE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
c)Impact of activation decomposition
on RMSE
20 40 60 80100 120 140 160 180 200
Training epochs0.0260.0280.0300.0320.0340.036MAPE
ctr
sgl
sprtrelu
adptpolu (K=2)
adptpolu (K=3)
adptpolu (K=4)(
d)Impact of activation decomposition on
MAPE
Figure 15: Ablation Study on inter-client spatial dependency (a) and activation decomposition (b)-(d) on PeMSD8-SPEED.
6116