LeMon: Automating Portrait Generation for Zero-Shot Story
Visualization with Multi-Character Interactions
Ziyi Kou
zkou@nd.edu
University of Notre Dame
Notre Dame, Indiana, USAShichao Pei
shichao.pei@umb.edu
University of Massachusetts Boston
Boston, Massachusetts, USAXiangliang Zhang∗
xzhang33@nd.edu
University of Notre Dame
Notre Dame, Indiana, USA
Abstract
Zero-Shot Story Visualization (ZSV) seeks to depict textual narra-
tives through a sequence of images without relying on pre-existing
text-image pairs for training. In this paper, we address the challenge
of automated multi-character ZSV, aiming to create distinctive yet
compatible character portraits for high-quality story visualization
without the need of manual human interventions. Our study is
motivated by the limitation of current ZSV approaches that ne-
cessitate inefficient manual collection of external images as initial
character portraits and suffer from low-quality story visualization,
especially with multi-character interactions, when the portraits are
not well initiated. To overcome these issues, we develop LeMon,
an LLM enhanced Multi-Character Zero-Shot Visualization frame-
work that automates character portrait initialization and supports
iterative portrait refinement by exploring the semantic content of
the story. In particular, we design an LLM-based portrait genera-
tion strategy that matches the story characters with external movie
characters, and leverage the matched resources as in-context learn-
ing (ICL) samples for LLMs to accurately initialize the character
portraits. We then propose a graph-based Text2Image diffusion
model that constructs a character interaction graph from the story
to iteratively refine the character portraits by maximizing the dis-
tinctness of different characters while minimizing their incompat-
ibility in the multi-character story visualization. Our evaluation
results show that LeMon outperforms existing ZSV approaches in
generating high-quality visualizations for stories across various
types with multiple interacted characters. Our code is available at
https://github.com/arxrean/LLM-LeMon.
CCS Concepts
•Computing methodologies →Computer vision; Image rep-
resentations; Machine learning algorithms ;Learning settings .
Keywords
Story Visualization, Text-to-Image Generation, LLMs
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671850ACM Reference Format:
Ziyi Kou, Shichao Pei, and Xiangliang Zhang. 2024. LeMon: Automating Por-
trait Generation for Zero-Shot Story Visualization with Multi-Character In-
teractions. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671850
1 Introduction
With the aid of text-to-image diffusion models, Zero-Shot Story
Visualization (ZSV) emerges as a crucial research field that converts
a narrative story to a sequence of coherent images without the
necessity of existing images from the story as training data [ 17].
The generated images not only provide the readers with additional
visual illustrations for the better understanding on various stories
(e.g., kids books, science fictions), but also benefit people with
reading disabilities (e.g., dyslexia [ 8]). However, most storylines
for visualization often involve multiple characters with diverse
personalities and complex interactions. Current ZSV approaches
usually assume a single character in each visualized image and thus
fail to optimize the portrait of the character for the multi-character
interaction scenarios [ 9]. In this paper, we aim to address the multi-
character ZSV problem by automatically creating distinctive yet
compatible character portraits.
Traditional ZSV approaches usually model the problem as an
autoregressive generative task [ 13–15,18]. For example, Liu et al.
[14] developed a diffusion based autoregressive model that is opti-
mized on existing multi-modal story data and applied to visualize
unseen stories. As illustrated in Fig.1a, given a new story with a
sequential of scenarios, the visualization for the current scenario
is conditioned on both the corresponding story narrative and the
visualized image for the previous scenario. While straightforward,
such approaches usually achieves sub-optimal performance due to
their dependency on the visualized image for the first scenario and
the limited generalizability to unseen stories. Recently, the text-to-
image diffusion personalization method emerges as a promising
approach to address the ZSV problem [ 1,4,5,24]. The method per-
sonalizes a pre-trained diffusion model to generate images of new
concepts based on several natural language prompts and/or images
as the concept representation [ 4,16]. One representative work from
Jeong et al. (i.e., Fig.1b) visualizes a new story by personalizing a
diffusion model with pairs of portraits and descriptions for the story
characters where the portraits are collected from external celebrity
photos and the descriptions are fixed templates with each character
as placeholder (e.g., “Photo of <character name>”. When visualiz-
ing the story, the appearance of all characters keep coherent as the
selected celebrities across the sequence of scenarios. However, it
is extremely difficult and inefficient to manually collect external
images that are well aligned with the target characters, especially
 
1418
KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Kou, Shichao Pei, & Xiangliang Zhang
TrainSce. 1
Enc.Parse
ReadSce. 2Sce. 1Sce. 2Sce. 1Sce. 2
Story
Human
Portraits
VisualizedCharactersa) Liu et al.b) Jeonget al.c) Our Approach
UpdateStoryData
PortraitsPortraits
Desc.
Figure 1: Comparison of Different ZSV Approaches.
for the visualization on a vast number of stories. Moreover, none
of the above approaches considers the complicated interactions
between different characters during the portrait collection nor the
story visualization process. Such ignorance could lead to low vi-
sualization quality for multi-character scenarios (e.g., visualizing
different characters with similar appearance or failing to visualize
multiple characters in a scenario), due to the incoherence within the
collected portraits per character and the potential incompatibility
between different character portraits.
We are therefore motivated to design a new ZSV approach that
can generate high-quality story visualizations with multi-character
interaction while eliminating the need for manual portrait initializa-
tion. Our proposed framework, named LeMon, an LLMenhanced
Multi-Character Zer o-Shot Visualizatio nframework, has a overall
workflow shown in Fig. 1c. LeMon features two innovative designs
that make it different from existing ZSV solutions as follows.
1) Automated Character Portrait Initialization. Instead of
relying on the manual collection of character portraits from exter-
nal sources, we automate the portrait initialization process for a
given target story using a pre-trained LLM [ 25] and a pre-trained
Text2Image model [ 20]. Specifically, the LLM firstly parses the tex-
tual content of the story to try to describe the potential appearance
of each considered character, and then feed the descriptions as
prompts to the Text2Image diffusion model that further generates
initial portraits for the characters. A critical aspect of this strategy
is guiding the LLM to craft accurate descriptions on diversified char-
acters. Even with universally recognized narratives, LLM might
struggle to discern the character appearances, especially when con-
fronted with tales unseen in their training corpus. For example,
querying the LLMs with Alibaba and Forty Thieves , a world-wide
famous child story, and simply asking “How does Alibaba look like? ”.
A valid response might be elusive, given the lack of explicit charac-
ter descriptions in the story and the uncertainty of LLMs to derive
reasonable inferences. To counteract the above challenge, we design
across-domain character portrait prompting method to prepare rea-
sonable prompts that enable the LLM to generate valid descriptions
for different characters. This method explores the large-scale well-
designed character portraits from the movie domain and matches
the story characters with specific movie characters based on the
semantic alignment between the story and the textual movie scripts.The underlying principle is that the story and movie characters
share similar traits if their contexts in the movie and the story are
comparable. The matched movie characters with their appearance
descriptions are sent as in-context-learning (ICL) references to the
LLM to generate accurate descriptions for the story characters.
This entire process is automated, encompassing both the character
matching process from the movie domain and the LLM prompting
for the target story.
2) Multi-Character Interacted Portrait Refinement. To mit-
igate the low-quality visualization issue for multi-character scenar-
ios, we introduce a graph based diffusion personalization model
that constructs a character interaction graph upon the token embed-
ding module of the typical diffusion model where the graph nodes
represent the characters and graph edges as their interactions based
on the target story. The model leverages the constructed graph to
effectively explores the potential dependencies between different
characters during the de-noising diffusion process and optimizes
the character portraits to maximize their distinctness if the char-
acters are more likely to interact in the visualized images. During
the story visualization (i.e., the inference stage of the model), the
model further collaborates with the LLM above to identify specific
visualized images that successfully show multiple interacted char-
acters. Such images demonstrate the compatibility of the current
character appearance and thus not only serve as multi-character
portraits to further optimize the model, but also as anchors to shift
the generative distribution of the initial character portraits.
The collaborations of the two designs above lead to our auto-
mated ZSV approach that achieves high-quality story visualization
featuring multi-character interactions without requiring manual in-
terventions. We evaluate the performance of LeMon on the stories
with various types and demonstrate that LeMon outperforms exist-
ing ZSV methods in producing high-quality images for a variety of
characters efficiently and effectively.
2 Related Work
2.1 Story Visualization
The earlier works in the field of story visualization predominantly
depended on GAN or VAE-based methods [ 11–13,23]. Diffusion
models have achieved notable success across various domains and
have subsequently been incorporated into the task of story visual-
ization [ 2,5,9,10,14,16,17]. A typical strategy is to model the ZSV
as an auto-regressive generative task by considering the previous
visualized images as conditions to visualize the current scenario
of a story [ 14,17]. However, these approaches typically require
existing images as labels for a set of stories that are utilized to
optimize a diffusion model. Such a strategy usually fails to generate
high-quality images for unseen stories due to the overfilling on ex-
isting stories. Recently, several works have been proposed to work
on the ZSV problem by adapting a pretrained diffusion model to
generate images for a new story without relying on existing stories
as training data[ 5,9,16]. However, a common limitation of these
methods is the need for manual collection of external images to
serve as initial portraits for the characters in the new story so that
the method can optimize the diffusion model to learn the characters
as new concepts. As a result, collecting accurate portraits for the
characters is highly subject to the mannual efforts from the human
 
1419LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions KDD ’24, August 25–29, 2024, Barcelona, Spain
and becomes impractically time-consuming when they have to deal
with large volumes of stories. In contrast, LeMon automates the
portrait initialization process by effectively prompting the LLMs to
generate accurate descriptions for diverse characters based on the
well-designed character portraits from the movie domain.
2.2 Text-To-Image Diffusion Personalization
The text-to-image diffusion personalization task aims to incorporate
a new visual concept to a pre-trained diffusion model by utilizing a
limited set of images representing the concept [ 1,3,4,21]. For exam-
ple, Gal et al. [4] proposed a Text2Image personalization framework
that transforms user-provided concepts into creative images by
representing the concept as textual words in the embedding space.
While the personalization methods can be leveraged for handling
the ZSV problem by encoding the portraits of new characters as
novel concepts [ 9,16], the direct utilization on them could be subop-
timal. The reason is that the personalization technique assumes no
correlation between different concepts, such as multiple characters
in a story [ 5], which conflicts with the real-world stories that usu-
ally contain various interacted characters in major scenarios. One
possible solution is to personalize the portraits of each character
as an independent concept and expect the diffusion model to visu-
alize all of them when generating the multi-character images [ 4].
However, the visualization performance can be severely impeded,
causing similar appearances for different characters or even miss-
ing characters in the visualized images due to the lack of portraits
containing multiple interacted characters. To address the above
limitations, we design a novel graph-based diffusion personaliza-
tion method for LeMon by effectively exploring the interactions
of different characters in the story to synthesize distinctive yet
compatible characters in the story visualizations.
3 Problem Definition
In this section, we formally present the automated multi-character
ZSV problem with several definition below.
Definition 1. Story (S):A narrative highlighting a set of char-
acters and their interactions. We further represent a story S=
{𝑠1,...,𝑠𝑇}as a sequence of 𝑇scenarios.
Definition 2. Character ( 𝑐):Given a storyS, a character is
defined as a fictional individual or entity that brings distinct ap-
pearance, attributes, and roles to the progression of the story. We
define all characters in Sas a list C={𝑐1,...,𝑐𝑁}.
Definition 3. Character Portrait ( 𝑥):A portrait for a character
is typically an image that illustrates the overall appearance (e.g.,
face, body) of the character (e.g., portraits in Figure 1). We define
the portraits for a character 𝑐asX𝑐={𝑥𝑐,1,...,𝑥𝑐,𝑂}.
Definition 4. Text2Image Diffusion Model ( D):A Text2Image
diffusion modelDembeds the prompt as the condition input and
generate the image from noise [22].
Definition 5. Story Visualization ( Y):A sequence of images
Y={𝑦1,...,𝑦𝑇}that illustrate the progression of the story. There
is no ground-truth images for the story in ZSV problem.
Definition 6. LLM (P):An LLM is a large foundation language
model that has been pretrained on vast text datasets. This enablesthe LLM to generate contextually relevant texts suitable for a wide
range of natural language processing tasks.
Drawing from the definitions above, our objective is to develop a
diffusion based story visualization model that automatically gener-
ates the portraits of the characters with the assistance of LLMs, and
leverages the portraits to generate high quality images as visualiza-
tion for multi-character stories. Formally, the goal is to obtain: D∗=
arg maxDPr(Y|S,P,D), whileD∗maximizes Var(𝑐𝑖,𝑐𝑗|S,P,D),
indicating the variance of the generated portraits across differ-
ent characters, and minimizes Var({X𝑐|S,P,D}) as the variance
across the generated portraits within each character 𝑐.
4 The Proposed LeMon Approach
LeMon consists of three key modules: i) the automated portrait
initialization module (short as Portraiting ); ii) the graph-based dif-
fusion personalization module (short as Personalizing ) and iii) the
multi-character portrait refinement module (short as Refining ). We
show the overview of LeMon in Figure 2 exampled with the story
Alibaba and Forty Thieves, and elaborate each module below.
4.1 The Portraiting Module
The Portraiting module (i.e., yellow region of Figure 2) is designed
to generate initial portraits X𝑐for each character 𝑐in the storyS.
The generated portraits are then leveraged to encode the characters
as new concepts for the following Personalizing module. Unlike
previous ZSV approaches that usually require manual acquisition of
the portraits, the Portraiting module automates the generative pro-
cess to synthesize character portraits by jointly leveraging the LLM
Pand the diffusion model D. However, as discussed in Section 1, it
is extremely difficult to obtain valid or even meaningful character
descriptions fromPif only the story is fed to LLMs without appro-
priate prompts. To address the problem, we design two important
components for the Portraiting module: 1) a character-description
movie database and 2) a portrait prompting model.
Character-Description Movie Database. Derived from the
movie domain, known for its rich and well-designed character
portraits, the character-description movie database is built to se-
mantically align the target story with relevant movies, matching
story characters with external movie characters. Concretely, we
firstly collect a large set of movie scripts from IMSDb, an open-
source script website that contains thousands of popular movies
[28]. For each script (e.g., Joker in Figure 3 left), we leverage Pto
read the script and identify the main characters (e.g., “Sophie Du-
mond”), a task made straightforward by the advancements in text
summarization research for LLMs [ 6]. For each identified character,
we obtain its portraits by crawling the Google Image with the key
words as the movie and the character name. Each portrait is further
captioned by large visual language models [ 31] to provide textual
descriptions about the character, including but not limited to the
face, demographic, body and dressing information. More details
of the collected data base are shown in Section 5. The collected
character-description (CD) pairs are formally defined as follows.
Definition 7. Character-Description (C.D.) Pair ( 𝑞𝑖):a tuple
structure as 𝑞𝑖=(𝑞𝑠
𝑖,𝑞𝑐
𝑖,𝑞𝑑
𝑖)that represents the movie script name,
 
1420KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Kou, Shichao Pei, & Xiangliang Zhang
Update
51
29
11NDiff. Loss
“Cassim’sWife presses Alibaba for the source of his riches”“Photo of C1”“Photo of CN”Interaction Graph
QKVQKVPred.Infer. Stage
PersonalizingRefiningPortraiting
StoryAlibaba, …Cassim’sWife
Alibaba: A Middle Eastern man in his late thirties, has a kind and noble countenance, a lean but sturdy…Cassim’sWife:Middle Eastern descent, a woman in her forties with a materialistic and extravagant appearance …
Char.-Desc.Candidates
Top-K CandidatesAlibaba
<Portrait Inference>
Top-K CandidatesCassim’sWife<Text-To-image>
< CharacterCompatibility >
Figure 2: The overview structure of LeMon, comprising three modules: Portraiting, Personalizing andRefining. The Portraiting
module automatically generate accurate portraits for different characters in the target story by leveraging LLMs and the
character-description pairs from the movie domain as reference. Then the Personalizing module designs a graph enhanced
text-to-image diffusion model that is finetuned by the generated portraits as training samples to learn the characters as new
semantic concepts. Finally, the Refining module validates the generated visualizations for the story and uses the eligible images
as additional training samples to further regulate the appearance within the character portraits.
SophieDumond<Character Collection>
Portrait Description
CrawlScriptVideoPortrait
C.D. Pair
Encoder
MLPLossContext
Figure 3: The built character-description movie database (L)
and the optimization for portrait prompting model (R).
movie character name and the character description. We further
defineQas the entire collection of the C.D. pairs.
Portrait Prompting Model. Given the generated C.D. pairs Q
above, the portrait prompting model is designed to leverage Q
as in-context-learning (ICL) samples for Pto generate valid and
meaningful descriptions of the characters in S. ICL is a prompt
engineering method that provides demonstrations (e.g., C.D. pairs)
for LLMs to solve novel tasks (e.g., inferring character portraits)
without the model finetuning process [ 30]. Given a character 𝑐, the
portrait prompting model aims to identify 𝑞𝑖fromQthat is mostly
aligned with 𝑐by measuring the semantic correlations between
the textual context for 𝑐fromSand the context for 𝑞𝑖from the
corresponding movie script. To optimize the portrait prompting
model, we design the model as a textual encoder and create positive
and negative pairs from Qas training data. The objective is to push
the encoded positive pairs closer while the negative ones fartherapart. We define the pairwise ranking loss [ 29] for the model below.
Lrank=𝑚𝑎𝑥(0,e𝑒𝑖,1·e𝑒𝑖,2−e𝑒𝑖,1·e𝑒𝑗,∗+Δ) (1)
wheree𝑒𝑖,1ande𝑒𝑖,2represent two encoded sentences by the model
from the same movie script of 𝑞𝑠
𝑖where each sentence contains
𝑞𝑐
𝑖as a token. Similarity, we denote e𝑒𝑗,∗as any encoded sentence
from another script of 𝑞𝑠
𝑗. We create context for each sentence by
interpolating the sentence with its previous and following sentences.
The character names of all sentences are randomly modified to
avoid the import of ground-truth labels. After the optimization
process, given each target character 𝑐∈S, the encoder compares
the encoded feature of c with the features from all C.D. pairs in Q
based on the cosine similarity, and retrieves top- 𝐾C.D. pairs as the
ICL samples. We show the optimization pipeline in Figure 3 right.
Based on the retrieved ICL samples for each character 𝑐, the LLM
Preads the target story Sand follows the prompt augmented by
the samples to describe 𝑐by valid and rich experience detail.. We
show the detailed instructions for Pin Figure 4. Following P, the
pretrained diffusion model Dtakes each description as the input
prompt and generates a set of high-quality portraits for each char-
acter. We represent the generated portraits as X={X1,...,X𝑁}.
4.2 The Personalizing Module
Given the initial portraits Xgenerated by the Portraiting module
for all target characters, the Personalizing module (the green region
in Figure 2) leverages Xas training data to finetune a pretrained
Text2Image diffusion model so that the model encodes the charac-
ters as new visual concepts and visualize the story in the following
subsection. Existing studies mainly focus on the single character
visualization by representing each character as a unique token (e.g.,
Alibaba→“S*”) and generating the prompt for each portrait (e.g.,
“Face of S*”). Such prompt-portrait pairs are then used to update
the corresponding token embedding (e.g., “S*”) of the diffusion
model [ 1,4]. However, it is not feasible to use the similar idea to
 
1421LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions KDD ’24, August 25–29, 2024, Barcelona, Spain
Given a story and a character from the story, describe the appearance of the character into *one* sentence so that the sentence can be used as a prompt for text-to-image models to generate high-quality images. Below are some examples.Story: Aladdin, Character: Jafar, Description: a regal and powerful sultan, wearing a traditional Indian clothing with intricate patterns and jewelry, holding a golden snake staff and making a stern expression, set in a grand palace with ornate decorations and a mystical atmosphere.Story: Sicario, Character: Jacinta, Description: a professional and serious individual, with a slim build and dark hair styled neatly, falls in loose waves around her face and down her back, aoval face with high cheekbones, a straight nose, and a small mole above her upper lip on the left side.Story: Aladdin, Character: Aladdin, Description: a young man in his late 20s or early 30s, with dark, curly hair and brown eyes, wearing a long, flowing white robe with intricate designs on the sleeves and a large, red and white headdress on his head, a defined jawline and a relaxed posture. …Now given the story:<Story Name>, describe the character: <Character Name>, including but not limited to the age, gender, race, body information, reflecting the positive or negative role in the story. Below is the content of the story.…
Figure 4: Instructions for LLM with ICL
personalize a diffusion model with multiple characters and var-
ied interactions [ 5]. Using distinct tokens for each character (e.g.,
“S1”, “S2”) might seem a straightforward solution, but the model’s
generative capacity remains suboptimal. Indeed, from the baseline
model results, we observe that different characters in the story vi-
sualizations often appear similar to each other and some characters
may disappear during their interaction with other characters in
multi-character images. One primary reason is the high similarity
of the input prompts for different characters, such as “the face of
S1/S2” and the same initial token embedding (e.g., “person”). As
a result, the finetuned diffusion model tends to generate similar
appearances for different characters when the coherency of the
portraits within each character is not high enough.
Motivated by the above limitations and the topological reason-
ing ability of graph [ 26], we design a story driven multi-character
interaction graph that learns to explore the interactions between
different characters and adaptively adjust their visual appearance
to be distinctive with each other in the story visualization. The in-
tuition is that several characters who interact more in the story are
more likely to appear in the same visualized image, and thus they
should be more distinctive with each other. Such distinctiveness
can be better captured if the character interactions are explicitly
encoded to the diffusion model so that the model can optimize the
corresponding token embeddings jointly. In particular, we con-
struct a character interaction graph G={V,E}whereVdenotes
the graph nodes for all characters and Erepresents the interac-
tions between each two characters, weighted by their interaction
frequency. We then build an interaction weighted graph neural
network model upon the constructed Gto seamlessly encode each
character with the consideration of their interaction partners. We
initialize the node embedding for each character as e𝑣𝑛∈R𝑑by us-
ing the original token embedding (e.g., “S1”, “S2”), and then performthe graph convolution as follows.
e𝑣(𝑙+1)
𝑛,∗=Agg.({𝑤𝑛,𝑗e𝑣(𝑙)
𝑗,∀𝑗∈NN(𝑣𝑛)})
e𝑣(𝑙+1)
𝑛 =norm(𝜎(𝑊·concat(e𝑣(𝑙+1)
𝑛,∗,e𝑣(𝑙)
𝑛)))(2)
where Agg. denotes the mean aggregation operation, NN(𝑣𝑛)means
the collections of all characters interacted with 𝑐𝑛onG.𝑤𝑛,𝑗de-
notes the normalized interaction frequency between the 𝑛𝑡ℎand
𝑗𝑡ℎcharacter.𝑊is the trainable weights. We perform the graph
convolution for 𝐿times and use the output node embeddings to
represent the corresponding tokens in the prompts. We follow the
layer-wise personalization optimization process [ 1] to optimize the
diffusion model, which is illustrated below.
arg min
M,eCE𝑧,𝑦,𝜖,𝑡[∥𝜖−𝜖𝜃(𝑧𝑡,𝑡,𝑐(𝑦←e𝑐(𝐿)
𝑛,M(𝑡,𝑟)))∥2
2](3)
where𝑡is the current diffusion time step, 𝑟is the specific self-
attention layer, 𝑦is the encoded conditional embedding from the
prompt,𝑧𝑡is the noised latent feature encoded from the character
portrait. We optimize the initial token embeddings, the learnable
parameters of graph neural network and the mapper Mwhile main-
taining all other parameters from the diffusion model as freezed.
To explicitly enforce the characters to be visually distinctive, we
define the batch-level character contrastive loss as follows.
Lct=𝑦×(𝛿−CLIP(ˆ𝑥𝑛,𝑖,ˆ𝑥𝑛,𝑗))+( 1−𝑦)×CLIP(ˆ𝑥𝑛,𝑖,ˆ𝑥𝑚,∗)(4)
where ˆ𝑥𝑛,𝑖and ˆ𝑥𝑛,𝑗denote the denoised latent samples in a batch
from the same character, while ˆ𝑥𝑛,𝑖andˆ𝑥𝑚,∗the different characters.
The final joint loss is L=L𝑟𝑒𝑐+𝜆LctwhereL𝑟𝑒𝑐is the L2 loss in
Equation (3) and 𝜆is a hyper-parameter.
4.3 The Refining Module
Given the finetuned diffusion model from the Personalizing mod-
ule, the Refining module leverages the story visualizations (i.e., the
inference results of the model on S) to refine the initial portraits of
the story characters. We observe there exists distorted or missing
characters in the visualization results even if the input prompts
explicitly include the character tokens. For example, in the blue
region of Figure 2, the inference prompt “Cassim’s wife presses Al-
ibaba for the source of his riches ” is visualized as two generated
images. While the upper image contains both persons, the lower
one features only Cassim’s Wife with her appearance altered a lit-
tle bit. One significant reason is the absence of portraits featuring
multiple interacting characters as training data for optimizing the
diffusion model. To address this problem, the visualized images by
the diffusion model can naturally serve as additional supervision
signals if the images successfully show multiple characters engag-
ing in explicit interactions. However, the challenge lies in how to
identify such images from all visualization results. To address this,
we firstly introduce character compatibility below.
Definition 8. Character Compatibility: given a visualized
image, we consider two characters as compatible if i) both characters
clearly appear in the image and ii) the characters have explicit
interactions aligned with the input prompt.
To identify the visualized images satisfying the first requirement,
we design a VLM-based character compatibility validator that lever-
ages the large visual-language model to validate the visualized
 
1422KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Kou, Shichao Pei, & Xiangliang Zhang
images based on pre-selected questions as prompts. In particular,
given a visualized image and the original input prompt, we design
two types of questions for VLMs: i) the character counting question
that asks VLM to count the number of characters in the image and
ii) the gender inference question that asks VLMs to infer the gender
distribution (e.g., one male and one female) of the image. Images
pass the validation if the answers from VLMs match the character
tokens from the input prompt. We use the CLIP score [ 19] to esti-
mate the semantic alignment between the visualized images and the
input prompts. Images achieving a CLIP score above a pre-defined
threshold are considered to pass the validation. The images that
successfully pass both the above questions are considered as valid.
Given a set of valid images, the Refining module firstly adds them
as additional training data to the diffusion model in the Personalizing
module. For example, the image with green boarder from Figure
2 can be adopted with the prompt as “Photo of S1 and S2” (i.e.,
Alibaba andCassim’s Wife ). The Refining module then encodes the
visualized images as anchors to only keep the portraits of a character
if they share the similar appearance with the same character in the
visualized images. This is because the character appearance could
be different between the visualized images and the initial portraits,
causing unexpected visual discrepancy. Since it is challenging to
separate single characters from the visualized images via direct
similarity estimation, we design a group-level similarity score:
𝜂g(ˆ𝑧𝑡,{𝑥1,𝑖,𝑥2,𝑗,...})=CLIP(Enc(ˆ𝑧𝑡),∑︁
Enc(𝑥))) (5)
where{𝑥1,𝑖,𝑥2,𝑗,...}denotes one portrait for each character in ˆ𝑧𝑡.
We randomly sample the portraits of the considered characters and
keep the portraits if their group-level similarity score is higher than
the threshold. For the removed portraits, we task the Portraiting
module to generate more portraits for back-filling. The aforemen-
tioned three modules, Portraiting, Personalizing andRefining, are
manually applied iteratively until no portrait is excluded.
5 Evaluation
In this section, we conduct extensive experiments on different types
of stories to answer the following questions: Q1) Can LeMon gen-
erate more accurate and coherent story visualization compared to
other ZSV approaches? Q2) How effective is LeMon to automati-
cally generate character portraits? Q3) How does each component
of LeMon contribute to its overall performance?
5.1 Dataset
We collect totally 10stories to conduct both quantitative and quali-
tative experiments for LeMon. In particular, we collect 6real-world
stories that are world-wide famous so that the story visualizations
can be well evaluated by humans in the following subsections. The
names of the stories are: Alibaba And The 40 thieves, Cinderella, The
Story of Rapunzel, Sleeping Beauty, The Brave Little Tailor andThe
water of life. To investigate the ability of LLMs on describing the
characters without prior knowledge, we create 4new stories by
instructing an external LLM to generate the stories that include
multiple interacted characters but no explicit description on their
appearance. Each new story is created based on one topic from
adventure, romance, thriller and comedy, respectively. We follow
the previous research that uses LLMs to summarize the story into aTable 1: Data Statistics: averaged over 6 real and 4 new stories
#
of Words #
of Characters #
of Prompt T
oken per Prompt
Real
Stories 15,238 6 31 15
Ne
w Stories 5,833 5 35 13
list of prompts and expect to generate the corresponding images as
story visualization [ 5]. We show the statistics of all stories and the
prompts in Table 1. For the movie scripts from IMSDB, we totally
collect 1,131 scripts and 5,042 characters. For each character, we
crawl 5portraits images based on Google Image Searching Engine
and the key words “Moive <movie_name>, <character_name>”.
5.2 Experiment Setup
We adopt Stable Diffusion 1.4 [ 20] as the baseline Text2Image diffu-
sion model as it is one of the most popular diffusion models with
public access. We apply Llama-2-7b [ 27] as the LLM component of
thePortraiting module and MiniGPT-4 [ 31] as the VLM component
of the Refining module. For the Portraiting module, we adopt pre-
trained BERT encoder as the backbone for the portrait prompting
model. Then MLP layer of the model consists of two linear neu-
ral network with ReLU activation. The output feature of the MLP
layer has 768dimensions. For the Personalizing module, we embed
each character of a story to a 768-dimension feature that is the
same as the dimension of the original token embeddings. We adopt
GraphSAGE [ 7] as the graph neural network module to explore the
relations between different character embeddings. For the Align-
ment module, we use miniGPT4 to validate the generated images
and the CLIP model to access the overall alignment between the
validated images and the visual profiles. We use 1,000time steps
with DDPM scheduler when finetuning the diffusion model and
50steps for the image inference. We set the learning rate as 0.001
and the batch size of training as 2. The maximum iteration step is
set as 5,000with the gradient accumulation step as 4. We set the
optimizer as AdamW [ 32] and optimize the model on 4 Quadro RTX
6000. The training time of using a single GPU is around 4 hours.
5.3 Baseline
We compare the performance of LeMon with the state-of-the-art
ZSV and personalized Text2Image approaches as follows.
StableDiffusion [20]: the original StableDiffusion model as a base-
line that generates independent images based on each prompt.
TextualInversion [4]: a personalized Text2Image Diffusion model
that updates specific token embeddings of a pretrained Text2Image
diffusion model to represents new concepts by providing photos of
the new concepts as additional training data.
NeTI [1]: a space-temporal Text2Image personalization model that
designs dynamic token embedding for the new visual concept based
on their current diffusion time step and the self-attention layers.
CoherentBook [9]: a ZSV framework that generates independent
images for each input prompt and then personalizes the face re-
gions of the images based on the selected visual profiles.
For the compared schemes except StableDiffusion, we gener-
ate the portraits of each character by feeding the StableDiffusion
 
1423LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions KDD ’24, August 25–29, 2024, Barcelona, Spain
Cassimconfronted Alibaba the location of the cave.
Alibaba entered the cave and discovered incredible riches.
The captain of the thieves escaped, vowing to avenge.Alibaba’s Wife measured the gold, unaware of the cave's existence
Cassim’sWife was suspicious of the wealth from Alibaba’s Wife
Alibaba: A Middle Eastern man in his late thirties, Ali Baba has a kind and noble countenance. He possesses a lean but sturdy build, with dark hair and a short, well-maintained beard. His eyes reflect wisdom and integrity.Cassim: a Middle Eastern man in his early fifties. He has grayinghair, a prominent mustache, and a commanding presence. His attire is more formal and elaborate, reflecting his social status.Captain: a formidable Middle Eastern man in his late forties. He has a rugged and intimidating appearance, with a sturdy build and a beard. His attire includes darker, more menacing clothing befitting his role as the story's antagonist.Alibaba’s Wife: A Middle Eastern woman in her thirties, Ali Baba's Wife exudes warmth and compassion. She has soft and gentle features, reflecting her caring nature. Her clothing is modest and practical, suitable for her role as a supportive spouseCassim’sWife: Middle Eastern descent, Cassim'sWife is a woman in her forties with a materialistic and extravagant appearance. She wears opulent clothing and jewelry, reflecting her role in the story as someone who values wealth and status.
Story PromptStableDiffusionTextualInversionNeTICoherentBookLeMon(Carl  Larsson)Character Desc. By LLM
LeMon(Cartoon)
Figure 5: Story Visualizations for the comparison of different ZSV schemes. Without constraints on the character coherency, the
StableDiffusion model generates images with random appearances. While the other compared schemes enhance the coherency
of the characters, LeMon generates characters appereances that are highly coherent and also aligned with the story content.
scheme with the prompt “Photo of <Character> where <Sentence>”
where <Character> denotes the name of each target character and
<Sentence> denotes a random sentence from the story that include
the character name. We do not compare LeMon with TaleCrafter [ 5],
a recent ZSV approach, as it requires millions of images to train an
internal bounding box layout module, which is not required by all
other compared schemes and difficult to be removed due to the lack
of implementation details. We initialize 5portraits for all compared
schemes, including LeMon. For the prompts from all stories, we
add the “, water color by Carl Larsson” at the end of the prompt to
serve as stylistic guidance [ 9]. We conduct quantitative comparison
over all stories and qualitative visualization comparison mainly for
Alibaba And The 40 thieves as it contains most interacted characters.
5.4 Evaluation Results
5.4.1 Story Visualization Performance Comparison. To answer the
question Q1, we visualize characters by each compared scheme
conditioned on the prompts from all stories. We firstly show sev-
eral generated images by each scheme for the story Ali Baba and
the Forty Thieves in Figure 5. We observe that the StableDiffusion
scheme generates a random character appearance for each prompt
because the character per prompt is processed independently evenwhen two prompts include the same character (e.g., Alibaba ). The
quality of the generated images by TextualInversion is relatively
low. One possible reason is that TextualInversion employs the la-
tent diffusion module that is not good at synthesizing high-quality
human images especially with specific water colors. While NeTI
and CoherentBook can generate high-quality images, they suffer
from missing characters (e.g., last row of NeTI) or unclear character
interactions (e.g., last row of CoherentBook) in the multi-character
images. In contrast, LeMon can generate coherent character ap-
pearance across different images (e.g., Alibaba, Alibaba’s Wife ) and
match the appearance with the content of the story due to the ac-
curate character description by LLMs. We show more visualization
of other stories in our code repository.1
We then conduct the quantitative evaluations by following the
previous research to evaluate each generated image based on two
types of metrics: i) the algorithmic metrics and ii) the human-based
metrics [ 5,9]. In particular, for the algorithmic metrics, we adopt i)
the CLIP-based text-image matching score to estimate the alignment
between each prompt and the generated image, and ii) the CLIP-
based image-image matching score that measures the coherency
between the character portraits and the generated images [ 19]. For
1https://github.com/arxrean/LLM-LeMon
 
1424KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Kou, Shichao Pei, & Xiangliang Zhang
Table 2: Story Visualization - Quantitative Comparison
Algorithm
Eval. (↑) Human
Eval. (↑)
T
ext-Image Image-Image Corr
espondence Coher
ence Quality
StableDiffusion 0.7010 - 3.629 2.323 3.699
T
extualInversion 0.7103 0.6425 3.143 2.333 2.759
Ne
TI 0.7291 0.6720 4.021 3.716 3.929
Coher
entBook 0.7208 0.6654 3.256 3.611 4.022
LeMon 0.7592 0.6871 4.251 3.962 4.027
the human-based metrics, we invite 15evaluators from Internet
who are independent from our research and volunteered to perform
the evaluation tasks. The evaluation results from all participants are
anonymously recorded. We follow the previous research to adopt
three evaluation metrics as i) Correspondence from input prompts to
output images; ii) Coherence of visualized characters across different
output images; iii) Quality of the visualized images. The rating
scale for each metric ranges from 1.0(lowest) to 5.0(highest). We
average the scores by all generated images from all stories and
show the evaluation results in Table 2. We observe that LeMon
outperforms the compared schemes on all metrics. In particular,
the Text-Image matching score is significantly higher (i.e., around
3%) than other schemes, which demonstrates the effectiveness of
LeMon in generating high-quality images with aligned chara for
different stories with coherent character appearance.
5.4.2 Effectiveness of Automated Portrait Generation. To answer
the question Q2, we compare the generated portraits by LeMon,
NeTI and CoherentBook as they achieves most competitive perfor-
mance above. Similar to story visualization, there is no ground-truth
portraits for the target characters. Therefore, we conduct human
studies by tasking the same evaluators as Q1 to read the stories and
then evaluate the generated portraits by each scheme based on four
metrics: Story-Portrait, Portrait-Portrait, Cross-Portrait and Qual-
ity. The Story-Portrait metric evaluates the alignment between the
textual descriptions of each target character in the stories and the
generated portraits. Similarly, the Portrait-Portrait metric evaluates
the coherency among the portraits per character. In contrast, the
Cross-Portrait metric measures the dis-similarity of the portraits
across different characters. We use the same rating scale as Q1 from
1.0to5.0. The evaluation results are averaged on all characters
across all stories. We show the results in Table 3. In particular,
LeMon shows competitive performance over the other schemes on
Quality but the large improvements on all othe metrics. For exam-
ple, LeMon significantly outperforms other compared schemes in
terms of Portrait-Portrait and Story-Portrait metrics. We attribute
the success to the Portraiting module that accurately describes the
potential appearance of each character based on the story, and the
Refining module that iteratively updates the character portraits
to achieve more coherent character appearance. We also observe
the Cross-Portrait value of LeMon is higher than other schemes,
indicating the effectiveness of the Personalizing module that dis-
tingishes different characters based on the character interaction
graph and the contrastive loss.
To further investigate the effect of LeMon on updating the por-
traits for each character, we compare the generated portraits for
each character from Ali Baba and the Forty Thieves in the initialTable 3: Protrait Generation Comparison
Quality
(↑)Portrait-Portrait
(↑)Cr
oss-Portrait (↑)Stor
y-Portrait (↑)
Ne
TI 3.968 3.177 3.142 3.548
Coher
entBook 4.188 3.207 4.017 3.162
LeMon 4.230 4.389 4.491 4.142
AlibabaAlibaba WifeCassimWifeInitial Character PortraitsLast Updated Character Portraits
Figure 6: Comparison of Character Portraits. Compared to
the initial character portraits based on the character descrip-
tions, the character portraits in the final round share more
coherency within each character.
Table 4: Ablation Study for LeMon
LeMon\ICL LeMon\Rank LeMon\Graph LeMon\Iter LeMon
T
ext-Image 0.7192 0.7428 0.7349 0.7514 0.7592
Image-Image 0.6624 0.6724 0.6792 0.6532 0.6871
and the final iterations. In particular, we visualize the generated
portraits from Ali Baba and the Forty Thieves shown in Figure 6.
Compared to the initial portraits, we observe that the portraits
of each character from the final iteration share more consistent
appearances. For example, the cloth and the facial characteristics
ofAlibaba in the final iteration are highly coherent with each other.
In contrast, its portraits in the initial stage is more diversified in
terms of the color and the painting style. The results show the
effectiveness of the Personalizing module that adaptively improves
the distinctness of the portraits from different characters and the
Refining module to iteratively refines the portraits.
5.4.3 Ablation Study. To answer the question Q3, we conduct abla-
tion study to investigate the contribution of key modules of LeMon.
In particular, we create four variants of LeMon as: 1) LeMon \ICL: we
remove the ICL samples from the Portraiting module; 2) LeMon\Rank:
we replace the learnable ICL sampler from the Portraiting module
with a random sampler; 3) LeMon \Graph: we remove the charac-
ter interaction graph and Lctfrom the Personalizing module; 4)
LeMon\Iter: we remove the iterative portrait refinement process
from the Refining module. We adopt the same evaluation metrics
as Q1 and show the comparison results in Table 4.
We firstly observe that adding ICL samples to the Portraiting
module improves both the Text-Image and Image-Image values,
which indicates the better ability of LeMon to generated story
visualizations with reasonable character appearance. To further
 
1425LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions KDD ’24, August 25–29, 2024, Barcelona, Spain
Orig. Movie Char.Portrait Sample 1Woman with dark skin, an afro hairstyle, and brown eyes is wearing a black leather jacket, turtleneck, and pants. She has a neutral to small smile, possibly looking tired with dark circles under her eyes.Description
Man in a suit and tie, possibly a comedian or public figure, stands on a stage with a serious expression, hands gesturing. He wears a black or brown suit, light-colored shirt, has grey hair, glasses.
Woman in her 40's with long and curly hair, styled in a bun. a round face and is seen wearing a floral dress, paired with a white blouse, black blazer, and a hat -either with a large brim or floral.Portrait Sample 2
Figure 7: Movie Character Descriptions. The portrait sam-
ples are generated from the corresponding descriptions with
different generative seeds.
investigate if the descriptions of ICL samples can accurately depict
the corresponding characters, we treat the descriptions as prompts
and feed them to the DaLLE-2 online model that generates the
corresponding images. We show the visualization results in Figure
7 and observe that the generated images look very like the original
movie characters by sharing numerous features.
We then observe that the learnable ICL sampler also improves
the final visualization performance by retrieving the most matched
movie characters for each character in the target story. We show
some matching results between the target character and the re-
trieved portraits of the matched movie characters in Figure 8. The
blue names are the characters from the target story while the black
names denotes the movie name and the movie character. We ob-
serve that the matched movie characters share the similar roles (e.g.,
positive/negative) as the characters in the story, which benefits the
Portraiting module by providing the aligned ICL samples with the
target characters in the semantic level and thus guiding the LLMs
to output accurate character descriptions.
We then observe that the character interaction graph and the
character contrastive loss L𝑐𝑟play an important role in improv-
ing the Image-Image score. To investigate the effect of the hyper-
paramter𝜆inL𝑐𝑟, we vary the value of 𝜆to optimize the Personal-
izing module. After the optimization, we generate the appearance
of each character by feeding the model with the prompt “the photo
of <NAME>” where <NAME> is the name for each character. In
particular, we vary 𝜆from 0.0to0.06with the interval as 0.02and
show the generated images in Figure 9. We observe that several
characters look similar with each other when 𝜆=0.00. As𝜆in-
creases, the appearance of the characters become more diversified
and also matched to the original story. However, we also observe
that the generated images corrupt when 𝜆=0.06. We infer that
a high𝜆may over-emphasize the distinctness between different
characters. This over-enforcement can disrupt the balance required
for effective optimization, leading to a corruption in the training
Alibaba
Aladdin:AladdinThe Rescuers:CodyCaptain
Thor Ragnarok:Thor
Thor: Loki
Alibaba’s WifeE.T.: Mary
The Piano: Ada
Cassim’sWifeMulholland:Diane
Supergirl:KaraFigure 8: Matching Results of Story and Movie Characters.
Each story character is aligned with specific movie characters
that share the similar personality features.
𝜆= 0.00𝜆= 0.02𝜆= 0.04𝜆= 0.06AbdallaAlibabaAlibaba WifeCaptainCassimCassimWifeMorgiana
Figure 9: Ablation Study of Character Distinctiveness Loss.
The increase of the loss fraction firstly improves the dis-
tinctiveness between different character portraits but finally
degrade the image qualities.
process. How to robustify the optimization of LeMon with more
tolerance of hyper-parameters could be a future study of our work.
6 Conclusion
The paper presents a new framework LeMon to address the auto-
mated multi-character ZSV problem. The design of LeMon demon-
strates that LLMs/VLMs can play an important role in the ZSV
task by automating the character portrait generation. Moreover,
the proposed graph based Text2Image diffusion model shows that
incorporating the character interactions in a graph structure can
effectively enhance the distinctiveness and compatibility between
different characters. Evaluation results on different types of sto-
ries demonstrate more effective and efficient story visualization
performance of LeMon compared to other ZSV schemes.
7 Limitations
LeMon is capable of generating the background scenes aligned
with the target story. However, we also admit that LeMon does
not explicitly maintain the coherence of the background across
different prompts. Currently we focus on the character coherence
and consider the background as a future work.
 
1426KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Kou, Shichao Pei, & Xiangliang Zhang
References
[1]Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. 2023. A Neural
Space-Time Representation for Text-to-Image Personalization. arXiv preprint
arXiv:2305.15391 (2023).
[2]Victor Nikhil Antony and Chien-Ming Huang. 2023. ID. 8: Co-Creating Visual
Stories with Generative AI. arXiv preprint arXiv:2309.14228 (2023).
[3]Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. 2022. “This
is my unicorn, Fluffy”: Personalizing frozen vision-language representations. In
European Conference on Computer Vision. Springer, 558–577.
[4]Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal
Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing
text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618
(2022).
[5]Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Haoxin Chen, Longyue
Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang. 2023. Tale-
Crafter: Interactive Story Visualization with Multiple Characters. arXiv preprint
arXiv:2305.18247 (2023).
[6]Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and
evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356 (2022).
[7]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[8]Martina Hedenius, Michael T Ullman, Per Alm, Margareta Jennische, and Jonas
Persson. 2013. Enhanced recognition memory after incidental encoding in chil-
dren with developmental dyslexia. PloS one 8, 5 (2013), e63998.
[9]Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. 2023. Zero-shot generation of
coherent storybook from plain text story using diffusion models. arXiv preprint
arXiv:2302.03900 (2023).
[10] Ziyi Kou, Shichao Pei, Yijun Tian, and Xiangliang Zhang. [n. d.]. Character as
pixels: A controllable prompt adversarial attacking framework for black-box text
guided image generation models.
[11] Bowen Li. 2022. Word-level fine-grained story visualization. In European Confer-
ence on Computer Vision. Springer, 347–362.
[12] Bowen Li, Philip HS Torr, and Thomas Lukasiewicz. 2022. Clustering generative
adversarial networks for story visualization. In Proceedings of the 30th ACM
International Conference on Multimedia. 769–778.
[13] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence
Carin, David Carlson, and Jianfeng Gao. 2019. Storygan: A sequential conditional
gan for story visualization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 6329–6338.
[14] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, and Weidi Xie. 2023.
Intelligent Grimm–Open-ended Visual Storytelling via Latent Diffusion Models.
arXiv preprint arXiv:2306.00973 (2023).
[15] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022. Storydall-e: Adapt-
ing pretrained text-to-image transformers for story continuation. In European
Conference on Computer Vision. Springer, 70–87.
[16] Rodrigo Mello, Filipe Calegario, and Geber Ramalho. 2023. ELODIN: Naming
Concepts in Embedding Spaces. arXiv preprint arXiv:2303.04001 (2023).
[17] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. 2022. Synthesiz-
ing coherent story with auto-regressive latent diffusion models. arXiv preprint
arXiv:2211.10950 (2022).[18] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. 2024. Synthesiz-
ing coherent story with auto-regressive latent diffusion models. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision. 2920–2930.
[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748–8763.
[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.
arXiv:2112.10752 [cs.CV]
[21] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for
subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 22500–22510.
[22] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, et al .2022. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information Processing Systems 35
(2022), 36479–36494.
[23] Yun-Zhu Song, Zhi Rui Tam, Hung-Jen Chen, Huiao-Han Lu, and Hong-Han
Shuai. 2020. Character-preserving coherent story visualization. In European
Conference on Computer Vision. Springer, 18–33.
[24] Sitong Su, Litao Guo, Lianli Gao, Heng Tao Shen, and Jingkuan Song. 2023.
Make-A-Storyboard: A General Framework for Storyboard with Disentangled
and Merged Control. arXiv preprint arXiv:2312.07549 (2023).
[25] Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and Nitesh V Chawla. 2024.
TinyLLM: Learning a Small Student from Multiple Large Language Models. arXiv
preprint arXiv:2402.04616 (2024).
[26] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh V Chawla.
2023. Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness,
and Efficiency. In ICLR.
[27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[28] Marilyn A Walker, Grace I Lin, Jennifer Sawyer, et al .2012. An Annotated
Corpus of Film Dialogue for Learning and Characterizing Character Style.. In
LREC. 1373–1378.
[29] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 974–983.
[30] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and
Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv preprint
arXiv:2308.10144 (2023).
[31] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
Language Models. arXiv preprint arXiv:2304.10592 (2023).
[32] Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona. 2022.
Understanding adamw through proximal methods and scale-freeness. arXiv
preprint arXiv:2202.00089 (2022).
 
1427