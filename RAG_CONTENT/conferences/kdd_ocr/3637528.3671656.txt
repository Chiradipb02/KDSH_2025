Face4RA
G: Factual Consistency Evaluation for Retrieval
Augmented Generation in Chinese
Yunqi Xu
Ant Group
Shanghai, ChinaTianchi Cai
Ant Group
Hangzhou, ChinaJiyan Jiang
Tsinghua University
Beijing, ChinaXierui Song
Ant Group
Hangzhou, China
ABSTRACT
Theprevailingissueoffactualinconsistencyerrorsinconventional
RetrievalAugmentedGeneration(RAG)motivatesthestudyofFac-
tual Consistency Evaluation (FCE). Despite the various FCE meth-
ods proposed earlier, these methods are evaluated on datasets gen-
erated by specific Large Language Models (LLMs). Without a com-
prehensivebenchmark,itremainsunexploredhowtheseFCEmeth-
ods perform on other LLMs with different error distributions or
even unseen error types, as these methods may fail to detect the
error types generated by other LLMs. To fill this gap, in this paper,
we propose the first comprehensive FCE benchmark Face4RAG for
RAGindependentoftheunderlyingLLM.Ourbenchmarkconsists
of a synthetic dataset built upon a carefully designed typology for
factualityinconsistencyerrorandareal-worlddatasetconstructed
from six commonly used LLMs, enabling evaluation of FCE meth-
odsonspecificerrortypesorreal-worlderrordistributions.Onthe
proposedbenchmark,wediscoverthefailureofexistingFCEmeth-
odstodetectthelogicalfallacy,whichreferstoamismatchoflogic
structures between the answer and the retrieved reference. To fix
this issue, we further propose a new method called L-Face4RAG
with two novel designs of logic-preserving answer decomposition
and fact-logic FCE. Extensive experiments show L-Face4RAG sub-
stantially outperforms previous methods for factual inconsistency
detection on a wide range of tasks, notably beyond the RAG task
fromwhichitisoriginallymotivated.Boththebenchmarkandour
proposed method are publicly available.1
CCS CONCEPTS
•Computing methodologies →Natural language processing;
•General and reference →Evaluation .
KEYWORDS
Large Language Model; Factual Consistency Evaluation;
ACM Reference Format:
Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song. 2024. Face4RAG: Fac-
tual Consistency Evaluation for Retrieval Augmented Generation in Chi-
nese. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3637528.3671656
1https://huggingface.co/datasets/yq27/Face4RAG
This
work is licensed under a Creative Commons
Attribution-NonCommercial-ShareAlike International
4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36716561 INTRODUCTION
Retrieval Augmented Generation (RAG), a technique of augment-
ing the context of Large Language Models (LLMs) with relevant
passages retrieved from external retrievers or search engines [27],
hasdemonstratedstrongperformanceonvariousknowledgeinten-
sive tasks such as open domain conversation [38, 41] and question
answering [19]. Despite its bright prospect, factual consistency re-
mains a critical issue for RAG systems. Recent assessment reveals
that even for the leading-edge commercial RAG systems like Bing
Chat and Perplexity, barely over half of their outputs are factual
consistent with the references [29]. This issue urges the need of
studying factual consistency evaluation (FCE) in the RAG task.
Various FCE methods have been proposed to evaluate the fac-
tualconsistencyofspecificRAGsystems,amongwhichatwo-step
approach shows promising results, especially for evaluating long
answers [10, 31]. As shown in the bottom left of Figure 1, this ap-
proach first segments the answer into shorter pieces, then evalu-
ates the factual consistency of each segment with respect to the
given reference. In this way, the evaluation of a long answer is
decomposedintoevaluationsonseveralsimplerpiecesofinforma-
tion, which improves the detection of factual inconsistency.
Inpreviousworks,theseFCEmethodsareevaluatedbyanswers
generated by the underlying LLM in the specific RAG system be-
ing studied [13, 18]. Despite their effectiveness on the specific sys-
tem, it is unclear how these methods generalize to new RAG sys-
tems.Asdiscoveredinarecentstudy[31],theoptimalFCEmethod
mayvarywhenevaluatingdifferentLLMs,henceachievingasupe-
rior performance regarding some certain LLM does not guarantee
astrongperformanceonotherLLMs.Inthissense,previousbench-
marks generated by a single LLM are not fair enough to evaluate
the overall performance of FCE methods.
To fill this gap, in this paper, we first construct a comprehen-
sive benchmark to enable the evaluation of FCE methods inde-
pendent of the underlying LLM. Specifically, we first propose a
novel error typology to cover various factual consistency errors in
RAG, which includes three main categories, i.e., hallucination er-
ror,knowledgeerror,andlogicalfallacy,andisfurtherdividedinto
nine error types. Based on our predefined error typology, we con-
struct a synthetic dataset in Chinese to assess the effectiveness of
FCE methods across the different types of errors. Furthermore, we
construct a real-world dataset in Chinese by generating answers
using six distinct LLMs within RAG tasks. Empirical analysis on
the real-world dataset shows that 6.96% of all factual inconsistent
samples involve logical fallacies. In addition, we observe that dif-
ferent LLMs exhibit diverse error distributions, which echoes pre-
vious research [31] and justifies our motivation of constructing a
comprehensive benchmark independent of LLMs.
6083
KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
Q.: Why Amazon established a leading position in cloud services?Ref.: Amazon is the global leader in cloud services. Sinceachieving the top position in the industry, hehas to independently drive innovation. Ans.: Amazon is the global leader in cloud services. The reason heachievedthe top position in the industry is becausehehas independently driven innovation.RAG
trueStep2:FCEStep1:Logic-Preserving DecompositionPrior FCEStep1:Decompose Answer
trueHallucination ErrorKnowledge Error  Logical Fallacy Real-World Dataset
GPT-4New Ans.FCE output: FalseAmazon is the global leader in cloud services.Heachievedthe top position in the industry.Hehas independently driven innovation.trueAmazon is the global leader in cloud services.The reason      Amazonachievedthe top position in the industry is becausehehas independently driven innovation.truetrueextractStep2.1:Fact FCELogical Structure AnalysistruefalsetruefalseStep2.2:Logic FCEQ.Ref.Ans.RAGL-Face4RAG (Ours)Error TypologyConstrucMon Prompts based on Error TypeLabel::↦↦causal reversal:Data Augmentation PromptsFCE output: Truelack causalityunclear referentFace4RAG
Synthetic Dataset1232*1*23231*2*
2*2233+ Q. + Ref.+ Q. + Ref.Ref.ManualAnnotationQ.Ref.Ans.RAGLabelManualAnnotationError Type
Figur
e 1: An overview of our proposed FCE benchmark and method, in comparison with prior works. The upper left plot gives
an example from RAG task. The lower left plot demonstrates previous FCE method, and the lower middle plot depicts our
proposed FCE method L-Face4RAG. The upper right plot shows the procedure of constructing the real-world dataset in our
proposed Face4RAG benchmark, which follows the procedure of previous benchmark. The lower right plots illustrates the
construction of the synthetic dataset in the Face4RAG benchmark.
While logical fallacy accounts for a considerable proportion of
factualinconsistencyerrorsinthereal-worlddataset,existingFCE
methods may be incapable of detecting these sophisticated errors
involvinglogicalconnectionsamongmultipletextsegments,since
the decomposition step may neglect the logical connections be-
tween segments in the original answer. Figure 1 provides a show-
case where a careless decomposition may mistakenly remove the
cause-effect relation, leading to a wrong evaluation result.
To resolve this issue, we develop the Logic-Enhanced FActual
Consistency Evaluation for RAG (L-Face4RAG) method to better
handle the logical consistency in the RAG task. L-Face4RAG has
two core designs, i.e., logic-preserving answer decomposition and
fact-logic FCE. Specifically, in the answer decomposition step, we
proposethreeprinciplesfordecompositionbasedonsemanticlink-
ages and logical connections. We design an elaborated prompt ac-
cordingly and construct few-shot examples to help LLM better fol-
low the above principles. In the subsequent FCE step, we assess
thefactualconsistencyofeachsegmentfromtwoperspectives,i.e.,
thefact consistency andlogical consistency. The former perspective
aimstodetecthallucinationorknowledgeerrors,whilethelateris
responsibleforthelogicalfallacyerrors.Wefurtherdesignachain-
of-thought (COT) [43] prompt at each stage to instruct the LLM
to better handle the inconsistency error in a step-by-step manner.
Figure 1 gives a detailed demonstration of L-Face4RAG.
Finally, we conduct extensive experiments to verify the effec-
tiveness of L-Face4RAG. Compared to previous FCE methods forRAG, L-Face4RAG attains substantially higher accuracy on both
synthetic and real-world datasets, regardless of the error type or
underlying LLM. Notably, although it is motivated by FCE in Chi-
nese RAG, its superiority is consistent on other FCE tasks. Specifi-
cally,additionalexperimentsonEnglishFCEbenchmarksforRAG[13,
18],summarization[14,34],dialogue[16,17]andfactverification[37]
show that L-Face4RAG achieves SOTA on most of the tasks (6 out
of 7), as well as a substantially higher averaged score. We further
conduct ablation studies to verify the core designs of L-Face4RAG,
i.e.,thelogic-preservinganswerdecompositionapproachandtwo-
stageconsistencyevaluationwithcarefullydesignedCOTprompts.
The contributions of this work are summarized as follows:
•We construct the first comprehensive FCE benchmark in
RAG,theFace4RAG,whichincludesacarefullydesigneder-
ror typology, a synthetic dataset, and a real-world dataset.
Face4RAG allows to evaluate FCE method on specific error
types or various real-world error distributions.
•We propose a new FCE method called L-Face4RAG with
two novel designs of logic-preserving answer decomposi-
tion and fact-logic FCE to better detect the logic fallacy in
the examined answer.
•Extensive experiments justify the proposed error typology,
evaluatetheeffectivenessofL-Face4RAGonawiderangeof
FCEtasks, and providefurther insights on the distinct error
typedistributionsofvariousLLMs.Alldatasetsandmethod
are released for better reproducibility.
6084Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese KDD ’24, August 25–29, 2024, Barcelona, Spain
2 RELATED WORK
Traditional FCE Methods. Evaluating the factuality of model
generated results is widely studied across various language model
generation domains like text summarization [15], dialogue sum-
mary [47] and question-answering [4]. When the golden labels
are given, prior methods using exact match metrics [8, 19, 27] or
similarity-based metrics are proposed [7, 46]. However, high qual-
ity answers can vary a lot, hence these approaches using golden
labels may significantly underestimate the models’ performances,
especially for long answers [42].
FCE for Long-form Answers. To effectively evaluate factu-
ality of long answers, recent FCE research mostly take a two step
approaches[23,26],whereinthefirststep,thelong-formansweris
decomposedintoshortersegments,suchassentences[24,26],sub-
claims [10, 23], individual facts [31] and structured triplets [18].
Then the second step evaluates the verifiability of each segment
with respect to the given reference text [25, 26, 45], which can be
efficiently done by modern general purpose LLM [9, 31], e.g., GPT-
4. Although we follow the two-step approach, our method differs
from them in the ability of leveraging logical connections via spe-
cial designs of logic-preserving decomposition and fact-logic FCE.
FCE Benchmarks. Prior benchmarks for FCE mostly focus on
specialized tasks like summarization [13, 24, 39]. For FCE in RAG,
existing benchmarks are derived from specific LLMs, such as Re-
fchecker [18] and FELM [9], which are constrained by the error
typedistributionoftheunderlyingLLMs.Unlikethesebenchmarks,
weconstructasyntheticdatasetbasedonourerrortypology,which
enables evaluation independent to any underlying LLM.
3 FACE4RAG BENCHMARK
Recall that existing FCE benchmarks only use answers generated
by some certain LLMs, which may fail to evaluate FCE methods
on other LLMs with different error distributions or unseen error
types. To remedy this issue, in this section, we propose a novel
approach to construct a FCE benchmark for RAG, which is inde-
pendent of the underlying LLMs and called FActual Consistency
Evaluation for RAG (Face4RAG).Face4RAGcontainsanerror-type-
oriented synthetic dataset and a real-world dataset. To construct
the synthetic dataset, inspired by the error typology used in an
exam designed for humans, i.e. the National College Entrance Ex-
aminationofChina,wefirstproposeanovelerrortypologytoclas-
sifyanyfactualconsistencyerrorinRAGtask,whichincludesnine
types of errors belonging to three main categories. Based on the
proposed error typology, we then construct a synthetic dataset to
evaluate FCE methods on each type of the error. Besides the syn-
thetic dataset, we also collect samples from six commonly used
LLMs to construct a real-world benchmark, which aims to evalu-
ate the overall factual consistency of FCE methods in real-world
scenarios. The details about Face4RAG can be seen in Table 1 and
Figure 2.
3.1 Error Typology in FCE
Our error typology for FCE is inspired by the questions in the Na-
tional College Entrance Examination of China [1], which are care-
fully designed to test the ability of human to evaluate factual con-
sistency. In this examination, reading comprehension is a majorTable 1: Statistics of the synthetic and real-world datasets
in the Face4RAG benchmark. For each dataset, the answer-
level and segment-level statistics on the number of samples,
the average sample length in terms of characters and the
rate of positive samples are reported.
StatisticsSynthetic
Dataset Real-world Dataset
Answ
er Segment Answer Segment
Num.
Samples 1299 6737 1200 6143
Avg. Length 289.3 45.4 307.7 45.2
Positive Rate 30.3% 55.8% 63.3% 85.6%
0%20%40%60%80%100%Real-worldSyntheticKCont.KInve.KConf.KConc.LOver.LCaus.LConf.LIncl.LOthe.Other ErrorsHallu.
Figur
e 2: Error type distribution of factually inconsistent
samples in the two datasets of our Face4RAG benchmark.
section to evaluate the participants’ skill of understanding a Chi-
nese text. Factual consistency evaluation is a typical task in this
section. Given the text, the participants are required to evaluate
thecorrectnessofseveralanswerstoaspecificquestion[1],which
is essentially a RAG task (see examples in Table 11 in the appen-
dix). As these questions are designed for a competitive entrance
examinationofhighereducationinstitutionsattheundergraduate
level, they are generally hard to answer and cover a wide range of
factual inconsistency error types. Accordingly, we develop a novel
error typology for RAG, which comprises three main categories
and is further classified into nine error types. In the following, we
give a detailed description of our proposed error typology.
Hallucination Error This class of error refers to the situation
when the answer contains information that cannot be traced back
to the reference [30]. Note that there are two main usages of the
term hallucination in previous literature: one refers to ”unfaithful
or nonsensical” generated answers [21], the other further includes
”unverifiable” answers using the given context [30, 40]. Here we
adopt the second usage that has a larger scope.
•Hallucination Error (Hallu.) refers to the situation when the
answer is either unfaithful or unverifiable using the given
context (even when it is factually correct).
Knowledge Error This class of error refers to the situation
when the information contained by the answer is inaccurate or
incorrect regarding the reference [9]. This may occur in various
componentsofasentence,suchasthesubjects,predicates,objects,
adverbials of time and place, etc. We classify the knowledge error
into four error types:
•Contradiction Error (KCont.) referstothesituationwhenthe
statementintheanswerconflictswithinformationfromthe
reference.
•Entity Inversion Error (KInve.) refers to the situation when
entities in the answer (events, processes, or concepts) are
swapped in their positions as compared to the reference.
•Conflation Error (KConf.) refers to the situation when the
entitiesinthereference(subjects,predicates,orobjects)are
inaccurately combined, altering the original meaning.
6085KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
•Conceptual Substitution Error (KConc.) referstothesituation
when a term or concept in the reference is erroneously re-
placed by a different (though possibly related) concept.
Logical Fallacy Thiskindoferroroccurswhentheanswercon-
tains statements that are either without logical support or have a
logical relation that conflicts with the information from the refer-
ence. This incongruity undermines the logical validity of the an-
swer with respect to the reference, which results in an unsound
argument or misleading information [22, 35]. We further divide
the logical fallacy into four error types:
•Overgeneralization Error (LOver.) referstothesituationwhen
aspecificdetailorattributefromthereferenceisincorrectly
applied to a broader category or group in the answer.
•Causal Confusion Error (LCaus.) refer to reverse the cause
and effect in the reference, or mistakenly adding a causal
relationship between two noncausal segments.
•Confusing Sufficient and Necessary Conditions Error (LConf.)
is the case when the necessary conditions in reference are
misinterpreted as sufficient and necessary conditions.
•Inclusion Relation Error (LIncl.) isthe case wherestatements
that are unrelated or have certain relationship except inclu-
sion in the reference are misrepresented in the answer to
have an inclusion relationship (e.g., hierarchical or subset).
For each of the error types defined above, we provide several
examples to help better understand and distinguish it from other
types. See detailed examples in Table 2.
3.2 Synthetic Dataset
Based on the above proposed error typology, we construct a syn-
thetic dataset. In the dataset, the positive samples are factual con-
sistent, whereas each negative sample has at least one factual in-
consistency error. The dataset is constructed based on WebCPM
[36], a web-enhanced question answering dataset in Chinese. Due
to the space limit, in the following we briefly describe the process
of dataset generation. Please refer to Appendix A for more details
of the construction of our synthetic dataset.
Negative Samples For each specific error type in the typology,
we design a prompt to generate samples with this error. For the
hallucination error, we setup three levels of difficulty for the eval-
uator to detect inconsistency and construct samples accordingly.
For the remaining two categories, i.e., knowledge error and logi-
cal fallacy, we design a specific prompt for each error type except
the Contradiction Error (KCont. ). Since KCont.may occur at differ-
ent levels of granularity [12], i.e., word or sentence, we design one
prompt for each. Apart from the above error types, we construct
a new error type called Other Logical Fallacy (LOthe.), which ac-
counts for potential errors in some complex logical connections
uncovered by our previously defined four types of logical fallacy.
Positive Samples To enrich the sample diversity, we apply the
augmentation technique in [28]. Specifically, the original positive
samples in WebCPM are augmented by synonym replacing and
paraphrasing via certain prompt at either word or sentence level.
Human Annotation Refinement To enhance the quality of
the coarse labels derived above, we further engage 12 human ex-
perts to annotate the factual consistency of each answer via a two-
step approach [31].
0%10%20%30%40%50%60%Baichuan2ChatGLM3Alpaca2 (CH)GPT-3.5GPT-4Qwen
KCont.KInve.KConf.KConc.LOver.LCaus.LConf.LIncl.Other ErrorsHallu.100%//~~~~~~Figur
e 3: Error type distributions of the six LLMs in our real-
world dataset (we omit the 50%∼100% region in type ratio).
3.3 Real-World Dataset
The synthetic dataset is generated based on various predefined er-
ror types without considering the distribution of these error types
in the real world. Consequently, there is a need for another evalu-
ation dataset that better aligns with the actual distribution of an-
swers in real-world RAG scenarios, thus serving as a real-world
dataset. In contrast to previous studies that relied solely on GPT-
basedLLMsforgeneratingresponsestocreatetheirevaluationsets
[9, 31], we adopt a more comprehensive approach by utilizing six
different LLMs to construct our real-world dataset.
Specifically,wefirstcollect200questionsalongwithcorrespond-
ing references. We then prompt six commonly used LLMs to gen-
erate answers for the questions based on the references, including
gpt-4-turbo (GPT-4) [2], gpt-3.5-turbo (GPT-3.5) [33], Baichuan2-
13B-Chat(Baichuan2)[5],ChatGLM3[44],Qwen-14B-Chat(Qwen)
[3], and Chinese-Alpaca-2-13B-16k [11] (Alpaca2 (CH)) . In this
way, we derive a total of 1200 data points, which constitute the
real-world dataset.
For each data point, we follow the same human annotation pro-
cedureasinoursyntheticdatasettoinspectifitisfactuallyconsis-
tent. Moreover, if an answer is deemed factually inconsistent, the
annotatorwillassignaspecificerrortypefromourproposederror
typology to the answer. When the annotators notice that error of
the answer does not fall into the aforementioned error types, they
will mark the answer as ”Other Errors”.
We now conduct empirical analyses on the error typology and
the behaviors of various LLMs on the above real-world dataset.
Overall Error Type Distribution Wefirstjustifyourstudyon
logical fallacy consistency detection by empirically showing that
the logical fallacy errors are prevailing in the answers generated
byvariousLLMs.Tothisend,weanalyzethedistributionoftheer-
rortypesannotatedacrosstheentirereal-worlddataset.Asshown
in Figure 2, the hallucination error, knowledge error and logical
fallacy account for 73.78% , 28.31%, 6.96% of all the inconsistent
samples, respectively. It worth note that this 6.96% logical fallacy
errors are not studied in the previous FCE methods. Besides, only
0.23% of the inconsistent samples are marked as ”Other Errors” by
annotators, which suggests the comprehensiveness and complete-
ness of our proposed error typology.
Error Distribution of Various Models We then look deeper
intotheerrortypesandtheirdistributionsamongvariousLLMsin
RAG.AspresentedinFigure3,variousLLMsexhibitdistinctdistri-
butions on error types. For instance, LIncl.emerges in three of the
LLMs, and LCaus.andLConf.occurs in four models. In addition,
6086Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Examples for Knowledge Error and Logic Fallacy. For each error types, the example in Chinese and the translated
version to English are presented. The colored text spans highlight the segments of factual inconsistency errors.
Err
or Type Original Text Factual Inconsistent Text
K
Cont. ۿିႂਘᇏ֥ົള෍aঀ໾ᇉ֩đؓႿᄎު׮ॹ෎Ҁ
ԉദุႏအđཨԢூসऎႵ၂קቔႨb
The vitamins and minerals in energy drinks play a certain
role in quickly replenishing nutrients and eliminating fa-
tigue after exercise.ۿିႂਘᇏ֥ჭ෍aັള໾֩đؓႿᄎު׮ॹ෎Ҁԉ
ദุႏအđᄹࡆூসऎႵ၂קቔႨb
The vitamins and minerals in energy drinks play a certain
roleinquicklyreplenishingnutrientsand inducing fatigue
after exercise.
KInv
e.၂ϮҖॖၛࠃ၂؟۱ᄅđఃᇏՖ۴࡞ࢲ֞߄ڒऌࢫ࠱
҂๝նჿ൞25-32฿đэӮႡުႵ15-18฿đቋުӮة൞
1-3฿b
A typical silkworm can live for just over a month, dur-
ing which the period from hatching to cocooning varies
roughly from 25 to 32 days depending on the season, fol-
lowed by 15 to 18 days as a pupa, and finally 1 to 3 days as
a moth.၂ϮҖॖၛࠃ၂؟۱ᄅđఃᇏՖ۴࡞ࢲ֞߄ڒऌࢫ࠱
҂๝նჿ൞15-18฿đэӮႡުႵ25-32฿đቋުӮة൞
1-3฿b
A typical silkworm can live for just over a month, dur-
ing which the period from hatching to cocooning varies
roughly from 15 to 18 days depending on the season, fol-
lowed by 25 to 32 days as a pupa, and finally 1 to 3 days as
a moth.
K
Conf. ٝೲඟᇏ֥໭߄ࠏ࿐໾ᇉ ॖၛّഝࠇ೛ഝ௃ڑഈܻ֥
ཌđطႵࠏ( ฏࠎ)߄࿐໾ᇉॖၛ་൬ሬຓཌb
Theinorganic chemicals in sunscreen can reflect or scatter
light on the skin, while organic (carbon-based) chemicals
canabsorb ultraviolet rays .ٝೲඟᇏ֥໭߄ࠏ࿐໾ᇉބႵࠏ( ฏࠎ)߄࿐໾ᇉ ׻ॖ
ၛّഝࠇ೛ഝ௃ڑഈܻ֥ཌa་൬ሬຓཌb
Both the inorganic chemicals andorganic (carbon-based)
chemicals in sunscreen can reflect or scatter light on the
skinandabsorb ultraviolet rays.
K
Conc. ෛሢࡲूၩ്֥ᄹ఼đᄀটᄀ֥؟ದष൓ᇿᇗ೾ൊ௜
ޙb
With the increasing awareness of health, more and more
people are beginning to focus on a balanced diet .ෛሢࡲूၩ്֥ᄹ఼đᄀটᄀ֥؟ದष൓ᇿᇗ೾ൊ֥
Ⴕࠏᇉਈb
With the increasing awareness of health, more and more
people are beginning to focus on the organic quality of
their diets.
LO
ver. ၂Ϯ֥໡ૌ௜ൈ֥֞࡮ᆨᇪ׻൞ພഈԛটb
Thespidersthat we usually see tend to come out at night.၂Ϯ֥໡ૌ௜ൈ֥֞࡮ঐԋ׻൞ພഈԛটb
Theinsectsthat we usually see tend to come out at night.
LCaus. ෛ
ሢྐ༏࠯ඌ֥ॹ෎ؿᅚđնඔऌᄝ۲ྛ۲ြᇏ֥ႋ
Ⴈᄀটᄀܼٗb
Withthe rapid development of information technology,
the application of big data across various industries is be-
coming increasingly widespread.նඔऌᄝ۲ྛ۲ြᇏ֥ႋႨᄀটᄀܼٗđᆃ֝ᇁ ਔྐ
༏࠯ඌ֥ॹ෎ؿᅚb
The application of big data across various industries is be-
coming increasingly widespread, leading to the rapid de-
velopment of information technology.
LConf. ູ
ਔ֤ࠆଖཛ಴უ࿐ളࢂ࿐ࣁđ࿐ളсྶ ऎСၛ༯่
ࡱğӮࠛႪྮa௖ྛ؊ᆞaҕࡆഠ߶ൌ׮ࠃ࡬b
To receive a certain honor student scholarship, students
mustmeet the following criteria: excellent academic per-
formance, goodmoralcharacter ,andparticipationinsocial
practice activities.࿐ളӮࠛႪྮ a௖ྛ؊ᆞ ࣼॖၛ֤ࠆଖཛ಴უ࿐ളࢂ
࿐ࣁb
Students with excellent academic performance andgood
moral character canreceive a certain honorary student
scholarship.
LIncl. ࡔӻ،৿ദุॖၛิۚྏ٫ି৯đ
ࡆ఼ࠔಽ֥ର৯đิ
ۚദุ֥ॆூসି৯b
Regular exercise can enhance cardiorespiratory fitness,
strengthen muscle endurance, and improve the body’s re-
sistance to fatigue.ࡔӻ،৿ദุॖၛิۚྏ٫ି৯đ২ೂࡆ఼ࠔಽ֥ର
৯aิۚദุ֥ॆூসି৯b
Regular exercise can enhance cardiorespiratory fitness,
such as strengthening muscle endurance and improving
the body’s resistance to fatigue.
while Hallu.e
xistsinallmodels,GPT-4hasanotablyhighpercent-
age, with 77.91% of its errors being of this specific type; in com-
parison, Qwen only has 57.81% Hallucination Error and a higher
proportionoflogicalfallacyat9.38%.Thedistincterrortypesdistri-
butions of different LLMs suggest that FCE methods evaluated on
a specific LLM may not generalize well to other LLMs, indicating
the necessity for constructing a benchmark that is independent of
the underlying LLM.4 LOGIC-ENHANCED FACTUAL
CONSISTENCY EVALUATION
In the above statistic analysis, logical fallacy accounts for a con-
siderable proportion of factual errors in real-world RAG scenarios.
However, as we have analyzed before, existing FCE pipelines ne-
glect the logical connections between segments in the original an-
swer, which may result in wrong factual consistency evaluation
6087KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
The logo design contest was backed by China Post. Consequently, China Post is responsible for the logo's design. Vitamin C is a nutrient needed by the human body, and oranges are rich in Vitamin C.AnswerVitamin C is a nutrient needed by the human body.Oranges are rich in Vitamin C.Decompositionwithout any semantic or logical connection12Baking a perfect cake entails not just precise measurements but also careful Dming.AnswerBaking a perfect cake entails not just precise measurements but also careful Dming.Decompositionsegmentswithlogical connection1Water keeps us alive.Cucumbers are full of it, with 95% water content.AnswerWater keeps us alive.Cucumbers are full of water, with 95% water content.Decompositionpronoun subsDtuDon12AnswerThe logo design contest was backed by China Post. Consequently, China Post is responsible for the logo's design. Decompositionsentenceswith logical connection1
Figur
e 4: A few examples for our proposed logic-preserving answer decomposition.
result for samples with logical fallacy. Hence, to improve the eval-
uation ability of factuality consistency, a natural direction is to de-
sign an advanced FCE method that is capable of handling logical
connections in long answers.
Inthissection,weproposeanovelpipelinecalled Logic-Enhanced
FActual Consistency Evaluation for RAG (L-Face4RAG) , which ex-
plicitly takes logical connections into consideration when evaluat-
ingthefactualconsistency.L-Face4RAGhastwocoremodules,i.e.,
logic-preserving answer decomposition and fact-logic FCE, which
will be described as follows.
4.1 Logic-Preserving Answer Decomposition
Most existing studies directly decompose answers into segments,
each containing only a single piece of information [23, 31]. In con-
trast, we propose to decompose the answers based on semantic
linkages2and logical connections, which preserves the logical re-
lationships and facilitates logical consistency evaluation. The core
design in this module is an elaborated prompt based on the follow-
ing three principles for answer decomposition.
•We prompt GPT-4 to execute the decomposition only when
thetwoormultiplesentencesdonotexhibitstrongsemantic
or logical connection.
•To ensure that each segment can be understood by GPT-4
independently without leveraging other segments, any pro-
noun in a segment that refers to other contextual informa-
tion should be substituted with appropriate reference.
•Duringthedecompositionprocess,GPT-4isrequiredtomain-
tainthesentencestructureoftheoriginalanswertothebest
extent. This principle alleviates the risk of introducing addi-
tional hallucination to the original answer.
In order to help GPT-4 better understand our principles for an-
swer decomposition and deal with texts with various formats, we
2Semantic
linkage refers to the connection or association between different pieces of
information based on their meanings or semantic content [20].construct three kinds of instances to serve as the few-shot exam-
ples. The specific type of instances are as follows:
•Logical Connection referstotheinstanceshavinglogicalcon-
nections between the sentences and thus, GPT-4 needs to
learn the solution of the logical connections during the de-
composition process.
•Pronoun Substitution involves replacing pronoun with their
referent entities during the answer decomposition to make
each answer segment understandable on its own, without
reliance on other segments.
•Unique Format refers to the instances with unique format
and may be difficult for GPT-4 to decompose properly.
ExamplesoftheanswerdecompositionareprovidedinFigure4
and the detailed prompt is provided on our benchmark webpage.1
4.2 Fact-Logic FCE
Previous methods directly invoke an LLM to evaluate the decom-
posed segments and overlook the logical fallacy. To evaluate the
logical fallacy, we develop a two-stage procedure for factual con-
sistency evaluation, which consists of a conventional stage of fact
consistencyevaluationandanextrastagethatevaluatesfromboth
perspectives of fact and logic; we introduce the COT mechanism
[43] into both stages to improve LLM’s ability of evaluation. The
prompts for each stage are provided at our benchmark webpage.1
Fact Consistency Evaluation Inthisstage,GPT-4isinstructed
to assess the consistency of each piece of information in the seg-
ment against the reference, which mainly concerns with the hallu-
cination error and the knowledge error. Unlike previous methods
that directly instruct the model to assess the consistency with the
reference [9, 13], we use the COT technique to guide the model to
evaluate the segment step-by-step, with the following steps:
(1)Informational Points Extraction : GPT-4 extracts all informa-
tional points from the segment. This step ensures that each
component of the segment will be evaluated.
6088Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese KDD ’24, August 25–29, 2024, Barcelona, Spain
Step3: Logical Structure ComparisonReference: ……Sinceestablished a leading position in cloud services, Amazonhas to independently drive innovation.……Relevant Context: Since established a leading posi;on in cloud services, Amazon has to independently drive innova;on. Answer Segment: The reason        Amazon established a leading posi;on in cloud services is because       he has independently driven innova;on.23
the reasonis because23Since, 23causal rela;onshipStep1: Context Iden9ﬁca9onStep2: Logical Structure Analysiscausal rela;onshipAnsRef↦↦2323causal reversalinconsistent::
Figur
e 5: The process of logic consistency evaluation.
(2)Context Identification : For each informational point, GPT-4
locates the corresponding content within the reference.
(3)Fact Consistency Check :GPT-4conductsathoroughfactcon-
sistency check for each informational point against its cor-
responding context. A segment is deemed consistent if and
only if every single informational point aligns fact consis-
tent with the reference.
The above instruction imposes GPT-4 to evaluate consistency
witheachrelevantcontentratherthanthefullcontextoftherefer-
ence, reducing the probability of misjudging positive samples. We
will empirically justify this point in our experiments.
Logic Consistency Evaluation Inthisstage,GPT-4isinstructed
with a COT prompt to evaluate the logical fallacy. Since no FCE
method has explicitly handled logical fallacy before, this is a novel
stageforFCE,inwhichweelaborateaCOTpromptasfollows.The
specific process is shown in Figure 5.
(1)Context Identification :Givenananswersegment,GPT-4iden-
tifies its relevant context from the reference.
(2)Logical Structure Analysis : GPT-4 then analyzes the logical
structure for both answer and relevant context.
(a) Identifythelogicalconnections,andthesentencecompo-
nents connected by these logical connections.
(b) Determinethetypeandfunctionoflogicalconnectionsto
understand the logical structure between sentence com-
ponents, e.g., causal, conditional, etc.
(c) Map sentence components to their corresponding logical
relations, e.g., cause and effect for causal relation, condi-
tion and result for conditional relation, etc.
(d) Build a complete logic framework of the sentence.
(3)Logical Structure Comparison : Finally, GPT-4 compares the
logical structure of the answer segment with the relevant
context and judge if the answer segment is logically consis-
tent with the reference.The last step of FCE is the aggregation of answer-level factual
consistency measurement. Specifically, an answer is marked fac-
tual consistent if and only if all of its decomposed segments have
passed the above two consistency evaluation stages.
5 EXPERIMENTS
In this section, we conduct extensive experiments to evaluate the
effectiveness of our proposed L-Face4RAG pipeline. Our experi-
ments show that on both synthetic data and real-world data in
Face4RAG benchmark, our L-Face4RAG method substantially out-
performs the existing FCE methods. Notably, its superiority goes
beyond the Chinese RAG task from which L-Face4RAG is origi-
nally motivated, as L-Face4RAG achieves SOTA results on 6 out of
7 of the existing English datasets and also a substantially higher
average score on all tasks.
5.1 Experimental Setup
Baselines We compare L-Face4RAG with four GPT-based fine-
grained FCE methods:
•FACTSCORE [31] first breaks the answer into a series of
atomic facts and then assigns a binary label to each atomic
fact individually.
•FELM[9]firstsegmentstheanswerintofine-grainedtextual
spans and then evaluates the factual consistency of all tex-
tual spans collectively. It outputs the corresponding num-
bers of factual inconsistent textual spans if existed.
•Ragas[13]firstextractsasetofstatementsfromtheanswer
and then evaluates the factual consistency of all statements
collectively, outputting a binary label for each statement
along with the corresponding reason for the assessment.
•Refchecker[18]extractsknowledgetripletsfromtheanswer
and evaluates each knowledge triplet separately.
Implementation Details As the above FCE baselines are orig-
inally designed for tasks in English, we adapt them to our Chinese
RAG task by translating their prompts into Chinese.
When experimenting with FELM, we utilize the Reference-doc
augmented evaluator [9], in alignment with our task which is fo-
cused on evaluating the factual consistency of answers against
their references. Specifically, we input our references as the re-
trieved reference doc in FELM’s evaluation framework. We select
the best-performing estimator in [9], i.e., decomposing the answer
with segment-based method and utilizing GPT-4 as the factual er-
ror detector.
SincetheoriginalsettingsofFACTSCOREandRAGASarebased
on GPT-3.5, we conduct experiments with both GPT-3.5 and GPT-
4 to eschew the effect of the possible performance gap between
GPT-3.5 and GPT-4 on the empirical results.
Finally, to apply our proposed evaluation pipeline, we decom-
pose the answer into segments and assess the factual consistency
of each segment respectively. The outputs include both the label
and the corresponding explanations. To derive deterministic out-
put from GPT-4, we set its temperature to 0.
6089KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
Table 3: Performance comparison of factual consistency evaluation on the synthetic dataset.
Metho
d T
otal Pos.Negativ
e samples
Hallu.
KCont. KInve. KConf. KConc. LOver. LCaus. LConf. LIncl. LOthe.
F
ACTSCORE(GPT-3.5) 70.36 37.31
90.45 100 94.44 55.56 94.29 78.57 64.29 68.00 46.34 86.07
FACTSCORE(GPT-4) 71.82 33.50
93.97 100 96.30 68.25 97.14 87.5 60.71 72.00 51.22 88.37
FELM 68.05 77.67
42.21 99.27 91.98 22.22 88.57 69.64 42.86 54.00 4.88 32.56
RAGAS(GPT-3.5) 69.59 70.81
76.89 98.54 71.60 49.21 87.14 54.46 39.29 48.00 34.15 44.19
RAGAS(GPT-4) 76.37 73.60
93.97 99.27 79.01 52.38 90.00 58.93 50.00 50.00 53.66 72.09
RefChecker 78.52 76.14
95.48 100 87.65 63.49 92.86 55.36 50.00 52.00 36.59 67.44
L-Face4RAG (Ours) 93.38 96.19
96.98 100 98.77 76.19 98.57 90.18 92.86 80.00 51.22 90.70
T
able 4: Performance comparison of factual consistency evaluation on the real-world dataset.
Metho
d T
otal Baichuan2
ChatGLM3 GPT-3.5 GPT-4 Alpaca2 (CH) Qwen
F
ACTSCORE(GPT-3.5) 53.33 54.0 55.5 47.5
51.5 59.0 52.5
FACTSCORE(GPT-4) 54.67 55.0 59.5 46.5
52.5 63.0 51.5
FELM 55.00 49.6 56.0 56.8
52.0 55.6 60.0
RAGAS(GPT-3.5) 65.92 64.5 68.5 64.5
60.0 65.0 73.0
RAGAS(GPT-4) 72.92 72.5 74.0 71.5
68.5 76.5 74.5
RefChecker 68.25 62.0 72.0 66.5
63.0 74.5 71.5
L-Face4RAG (Ours) 87.75 90.0 88.0 81.5
86.0 93.5 87.5
5.2
Performance Comparison on Face4RAG
We first compare the performance of our proposed L-Face4RAG
pipeline against various FCE baselines on the Face4RAG bench-
mark, which includes a synthetic dataset and a real-world dataset.
Synthetic Dataset In Table 3, we report the predictive accu-
racy of different error types for examined FCE methods on the
synthetic dataset. From the results, we have the two main obser-
vations. (i) Our method achieves the highest accuracy on most of
theerrortypes(excepton LIncl.whereitisslightlyworsethanRA-
GASwithGPT-4),whichamountstoasignificantimprovementon
overallaccuracycomparedtoallthebaselines.(ii)Inparticular,the
performancegapbetweenourmethodandbaselinesonerrortypes
oflogicalfallacyaremuchlargerthanthegaponothererrortypes,
which indicates that our method is especially capable of handling
logical fallacy owing to our specific algorithmic designs.
Real-world Dataset In Table 4, we compare the predictiveper-
formance of our proposed pipeline with previous FCE methods on
the real-world dataset. From the results we observe that: (i) The
overall accuracy of our method is substantially higher than those
of the baseline FCE methods, showing superiority in real-world
scenarios. (ii) Moreover, on most of the subsets generated by dif-
ferent LLMs, our method consistently outperforms baseline meth-
ods,whichindicatesthesuperiorityofourmethodisuniversaland
independent of the error distribution, which is in line with the em-
pirical results on the synthetic dataset.
5.3 Performance Comparison on Existing FCE
Benchmark
We then evaluate the robustness and applicability of the proposed
L-Face4RAG method on other factuality detection tasks, and inEnglish.Specifically,weconsiderseveralcommonlyusedFCEbench-
marks in English on various tasks, including RAG[13, 18], summa-
rization[14, 34], dialogue[16, 17] and fact verification[37].
In Table 5, we report the predictive accuracy of examined FCE
methods on the above tasks. The results show that our proposed
L-Face4RAG achieves SOTA results on 6 out of 7 of the existing
datasets and also a substantially higher average score on all tasks,
indicating the effectiveness of L-Face4RAG beyond the original
factuality evaluation task in RAG, and its robustness to other lan-
guages.Thisvalidatesthewide-applicabilityofourproposedmethod.
Besidestheabovecomparisonamongdifferentmethods,wealso
observe that the ranking of the average score of various methods
on the above commonly used benchmarks is similar to the rank-
ing of the average score on all public tasks is 0.9, and the same
0.9 between the rankings on our real-world dataset and the public
datasets. This validates the strong correlation between the evalu-
ation results of our new benchmark and the results on existing
benchmarks.
5.4 Ablation Study
We now verify the specific design choices of our proposed evalu-
ation pipeline by ablation study on Face4RAG benchmark. Specif-
ically, we examine the effectiveness of each designed module by
comparingL-Face4RAGwiththecounterpartmethodwithoutsuch
a module. Due to space limit, here we only present the results on
the synthetic dataset. Results on the real-world dataset are qualita-
tively similar and deferred to Appendix B.
Evaluating the Answer Decomposition Module. (A.D.) Re-
callthatourdecompositionmodulepreservesthelogicconnection
within one segment, which may help better identify logical fallacy
while reducing extra hallucination induced by decomposition. To
6090Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 5: Performance comparison of factual consistency evaluation on the existing benchmark.
Metho
d A
vg.RA
G Summ. Dial. Fact Verif.
RA
GAS[13] RefChecker[18] FRANK[34]
SummEval[14] 𝑄2[17]
DialFact[16] VitaminC[37]
F
ACTSCORE(GPT-4) 70.5 70 61 80 65 74 72 71
FELM 74.2 71 63 70 82 83 79 72
RA
GAS(GPT-4) 76.9 88 69 87 80 77 69 69
Ref
Checker 78.4 86 73 85 80 80 72 73
L-Face4RA
G (Ours) 84.2 91 73 87 90 84 77 88
T
able 6: Main results of the ablation studies on the syn-
thetic dataset. We compare L-Face4RAG with the variants
using conventional answer decomposition (A.D.), removing
the COT (w/o COT), and removing the logic consistency eval-
uation (w/o logi.eval). Overall accuracy and the accuracy on
positive or negative samples are reported.
L-Face4RA
G A.D. w/o COT w/o logi.eval
O
verall 93.38 76.44 79.60 88.99
-Positive 96.19 91.62 51.27 97.46
-Negative 92.15 69.83 91.93 85.30
T
able 7: Comparison of the accuracy between L-Face4RAG
and the counterpart method with conventional answer de-
composition (A.D.) or without logic consistency evaluation
(w/o logi.eval) for detecting specific error types of negative
samples on the synthetic dataset.
L-Face4RA
G A.D. w/o logi. eval
Hallucination
Hallu. 96.98 90.45 96.98
Kno
wledgeKCont. 100.00 100.00 100.00
KInve. 98.77 74.07 97.53
KConf. 76.19 41.27 66.67
KConc. 98.57 90.00 94.29
LogicalLO
ver. 90.18 42.86 83.93
LCaus. 92.86 32.14 35.71
LConf. 80.00 34.00 64.00
LIncl. 51.22 31.71 29.27
LOth. 90.70 44.19 65.12
v
erify this point, we conduct an ablation study by replacing our
approach by a conventional decomposition method [13]. As pre-
sentedinTable6andTable7,weobserveaseveredeclineofoverall
accuracy in the counterpart method, especially for negative sam-
ples related to logical fallacy. This phenomenon accords with our
intuitionthatconventionalanswerdecompositionmethodmayfail
to detect logical fallacy since some logical connections may be
discarded during the decomposition. In addition, positive samples
also have a slight decrease in accuracy, which justifies the third
principle in our logic-preserving answer decomposition module,
i.e., preserving the structure of the original answer may alleviate
the introduction of extra hallucination.Evaluating the Introduction of COT.(w/o COT) Recall that
COT is adopted in both stages of factual consistency evaluation,
whichinstructsthemodeltoconductfiner-grainedfactconsistency
evaluation and sophisticated logic consistency evaluation, respec-
tively. To validate the introduction of COT, we consider a counter-
partmethodthatremovesthedetailedstepsintheinstructionsand
requires GPT-4 to directly generate evaluation without outputting
theunderlyingreasoningprocess.AspresentedinTable6,theover-
all accuracy drops severely after removing COT (from 93.38% to
79.60%),especiallyforthepositivesamples(from96.19%to51.27%).
This justifies the benefit of introducing COT into FCE.
Evaluating the Stage of Logical Consistency Evaluation.
(w/o logi. eval) To evaluate the effect of our proposed logical con-
sistency evaluation stage on error detection, we construct a coun-
terpart method by removing the second stage from our pipeline.
TheresultspresentedinTable7showthatthecounterpartmethod
incursadeclineinoverallaccuracy.Amongalltheerrortypes,log-
ical fallacy contributes the major part of accuracy decline, which
alignswithourmainmotivationofthesecondstagedesignforlog-
ical fallacy evaluation. In addition, there is a slight decrease in the
accuracy of knowledge error. A possible reason is that the second
stagemaysupplementthedetectionofsomeknowledgeerrorsthat
are missed in the first stage. Hence, the second stage also benefits
the detection of knowledge error. Note that for the hallucination
error, we have not observed any obvious change in the detection
accuracy; this matches our intuition that hallucination error has
no relation with logical fallacy.
6 CONCLUSION
In this work, we give a systematic study of factual consistency
evaluation in RAG. Specifically, we first propose a comprehensive
benchmarktermedFace4RAG,whichincludesthesyntheticdataset
and the real-world dataset. In light of the possible failure of ex-
isting FCE methods in detecting logical fallacy in RAG, we then
propose a novel FCE method termed L-Face4RAG. Compared to
previous method, our method has two novel designs, i.e., logic-
preservingdecompositionandfact-logicFCE,whichcanbetterchar-
acterize the logical relations in different pieces of information in
the sentence, leading to higher ability of logical fallacy evalua-
tion. Extensive experiments on both the synthetic and real-world
datasets verify the effectiveness of the L-Face4RAG method. No-
tably, the superiority of L-Face4RAG is consistent on a wide range
of factuality detection benchmarks beyond the Chinese RAG task.
Elaborated ablation studies also justify our core algorithm designs.
6091KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
REFERENCES
[1] 2019. https://gaokao.neea.edu.cn/xhtml1/report/19012/5987-1.htm.
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-
rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shya-
mal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[3] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,
Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng
Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
HaoYang,JianYang,ShushengYang,YangYao,BowenYu,HongyiYuan,Zheng
Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical
Report. arXiv preprint arXiv:2309.16609 (2023).
[4] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu,
Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2023. Benchmarking Foundation
Models with Language-Model-as-an-Examiner. arXiv preprint arXiv:2306.04181
(2023).
[5] Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint
arXiv:2309.10305 (2023). https://arxiv.org/abs/2309.10305
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[7] Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Eval-
uating question answering evaluation. In Proceedings of the 2nd workshop on
machine reading for question answering . 119–124.
[8] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading
wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051
(2017).
[9] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I Chern, Siyang Gao, Pengfei Liu, Junx-
ian He, et al. 2023. Felm: Benchmarking factuality evaluation of large language
models. arXiv preprint arXiv:2310.00741 (2023).
[10] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou,
Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. FacTool: Factuality Detec-
tion in Generative AI–A Tool Augmented Framework for Multi-Task and Multi-
Domain Scenarios. arXiv preprint arXiv:2307.13528 (2023).
[11] Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text En-
coding for Chinese LLaMA and Alpaca. arXiv preprint arXiv:2304.08177 (2023).
https://arxiv.org/abs/2304.08177
[12] Marie-Catherine De Marneffe, Anna N Rafferty, and Christopher D Manning.
2008. Finding contradictions in text. In Proceedings of acl-08: Hlt . 1039–1047.
[13] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ra-
gas: Automated evaluation of retrieval augmented generation. arXiv preprint
arXiv:2309.15217 (2023).
[14] Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong,
Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summa-
rizationevaluation. Transactions of the Association for Computational Linguistics
9 (2021), 391–409.
[15] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun
Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint
arXiv:2304.02554 (2023).
[16] Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Dial-
Fact:Abenchmarkforfact-checkingindialogue. arXiv preprint arXiv:2110.08222
(2021).
[17] Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor,
and Omri Abend. 2021. 𝑄ˆ{2}: Evaluating Factual Consistency in Knowledge-
Grounded Dialogues via Question Generation and Question Answering. arXiv
preprint arXiv:2104.08202 (2021).
[18] Xiangkun Hu, Dongyu Ru, Qipeng Guo, Lin Qiu, and Zheng Zhang. 2023. Re-
fChecker for Fine-grained Hallucination Detection. (2023). https://github.com/
amazon-science/RefChecker
[19] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with
generative models for open domain question answering. arXiv preprint
arXiv:2007.01282 (2020).
[20] Ray S Jackendoff. 1992. Semantic structures . Vol. 18. MIT press.
[21] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination
in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.
[22] ZhijingJin,AbhinavLalwani,TejasVaidhya,XiaoyuShen,YiwenDing,Zhiheng
Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. 2022. Logical
fallacy detection. arXiv preprint arXiv:2202.13758 (2022).
[23] Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. Wice:
Real-world entailment for claims in wikipedia. arXiv preprint arXiv:2303.01432(2023).
[24] Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.
Evaluating the factual consistency of abstractive text summarization. arXiv
preprint arXiv:1910.12840 (2019).
[25] Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022.
SummaC: Re-visiting NLI-based models for inconsistency detection in summa-
rization. Transactions of the Association for Computational Linguistics 10 (2022),
163–177.
[26] Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast
and Accurate Factual Inconsistency Detection Over Long Documents. arXiv
preprint arXiv:2310.13189 (2023).
[27] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-
intensivenlptasks. Advances in Neural Information Processing Systems 33(2020),
9459–9474.
[28] Bohan Li, Yutai Hou, and Wanxiang Che. 2022. Data augmentation approaches
in natural language processing: A survey. Ai Open 3 (2022), 71–90.
[29] Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in
generative search engines. arXiv preprint arXiv:2304.09848 (2023).
[30] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.
On faithfulness and factuality in abstractive summarization. arXiv preprint
arXiv:2005.00661 (2020).
[31] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei
Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore:
Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Gener-
ation. arXiv preprint arXiv:2305.14251 (2023).
[32] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov,
Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023.
Generating benchmarks for factuality evaluation of language models. arXiv
preprint arXiv:2307.06908 (2023).
[33] OpenAI. 2022. Chatgpt blog post. https://openai.com/blog/chatgpt.
[34] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Under-
standingfactualityinabstractivesummarizationwithFRANK:Abenchmarkfor
factuality metrics. arXiv preprint arXiv:2104.13346 (2021).
[35] Domina Petric. 2020. Logical Fallacies. On-line Article (preprint), doi 10 (2020).
[36] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai
Lin, Xu Han, Ning Ding, Huadong Wang, et al. 2023. WebCPM: Interac-
tive Web Search for Chinese Long-form Question Answering. arXiv preprint
arXiv:2305.06849 (2023).
[37] Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! ro-
bust fact verification with contrastive evidence. arXiv preprint arXiv:2103.08541
(2021).
[38] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen
Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blender-
bot 3: a deployed conversational agent that continually learns to responsibly
engage. arXiv preprint arXiv:2208.03188 (2022).
[39] Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe Laban, Jiacheng Xu,
Semih Yavuz, Wojciech Kryściński, Justin F Rousseau, and Greg Durrett. 2022.
Understanding factual errors in summarization: Errors, summarizers, datasets,
error detectors. arXiv preprint arXiv:2205.12854 (2022).
[40] Craig Thomson and Ehud Reiter. 2020. A gold standard methodology for evalu-
ating accuracy in data-to-text systems. arXiv preprint arXiv:2011.03992 (2020).
[41] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-
shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applications. arXiv preprint
arXiv:2201.08239 (2022).
[42] Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue
Zhang. 2023. Evaluating open question answering evaluation. arXiv preprint
arXiv:2305.12421 (2023).
[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reason-
inginlargelanguagemodels. Advances in Neural Information Processing Systems
35 (2022), 24824–24837.
[44] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,
Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open
bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).
[45] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Eval-
uating Factual Consistency with a Unified Alignment Function. arXiv preprint
arXiv:2305.16739 (2023).
[46] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint
arXiv:1904.09675 (2019).
[47] Rongxin Zhu, Jianzhong Qi, and Jey Han Lau. 2023. Annotating and Detect-
ing Fine-grained Factual Errors for Dialogue Summarization. arXiv preprint
arXiv:2305.16548 (2023).
6092Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 9: Statistics of 200 model-generated answers in our
real-world dataset from six LLMs. ”Avg. Length” indicates
the average length of the generated answer. ”Error Rate” in-
dicates the ratio of factual inconsistent answers.
MODEL
Avg. Length Error Rate(%)
Baichuan2 320.6 40.5
ChatGLM3 158.0 36.5
GPT
-3.5 160.8 27.5
GPT-4 359.2 40.0
Alpaca2 (CH) 188.8 47.0
Qwen 200.6 29.0
A
CONSTRUCTION DETAILS ABOUT
SYNTHETIC DATASET
Negative Samples Thenegativesamplesareconstructedbasedon
WebCPM[36],aweb-enhancedquestionansweringdatasetinChi-
nese, following the aforementioned error typology. We use GPT-4
[2] to generate the negative samples and design specific prompts
correspondingtoeachtypeoferror.ForeverysampleinWebCPM,
we rewrite them for every error type to collect our synthetic neg-
ative sample dataset. Detailed prompts are provided at our bench-
mark webpage.1
Forthehallucinationerror,weconstructcorrespondingdataac-
cordingtothreelevelsofdifficultyfortheevaluatortodetectincon-
sistency.Tobemorespecific,theeasiestsamples,inthefirstgroup,
are completely off-topic from the reference. The second group in-
cludes content that is on-topic but contains ungrounded informa-
tion.Thethirdgroupofsamplemixesfactuallyconsistentinforma-
tion with hallucinated content, resulting in sentences where some
partsaresupportedbythereference,whileothersareungrounded.
This mixture poses a challenge for evaluators, as it could mistak-
enly be labeled as ”consistent” due to the presence of some consis-
tent information.
For the remaining two categories, i.e., knowledge error and log-
ical fallacy, we design a specific prompt for each error type except
the Contradiction Error (KCont.). For KCont., since it may occurs
at different levels of granularity [12], i.e., word or sentence, we
design one prompt for each level. Specifically, the prompt of the
word-level KCont.aims to select specific words in the answer and
replacethemwithantonyms,andthepromptofthesentence-level
KCont.is designed to construct a new answer semantically con-
tradicting the reference. Since the types of logical connections are
diverse and comprehensive, for the completeness of the dataset,
we consider a new error type called Other Logical Fallacy (LOthe.),
which accounts for potential errors in some complex logical con-
nectionsuncoveredbyourpreviouslydefinedfourtypesoflogical
fallacy. The prompt of LOthe.is designed to drive GPT-4 to insert
anarbitrarylogicalconnectionerrorintoanywhereoftheoriginal
answer.
Positive Samples To enrich the diversity of positive samples,
weemploythecommonlyuseddataaugmentationtechniques[28]
to generate more positive samples based on the answers from We-
bCPM. Our data augmentation process supplements the positivesamplesinWebCPMbysynonymreplacingandparaphrasingtech-
niques via the prompts at the word or sentence level. Specifically,
at the word level, we prompt GPT-4 to randomly replace some
wordsintheanswerwiththeirsynonyms;atthesentencelevel,we
prompt to summarize the reference or rephrase the answer with-
out changing the meaning of the original sentence.
Construction Details Following the methodology in previous
research[32],weutilizethefew-shottechnique[6],inconjunction
with the Chain of Thought (COT) [43] approach, to guide GPT-4
[2] to construct high-quality samples. We provide clear directions
and relevant examples in the prompt and ask the model not only
to produce the newly constructed samples, but also to show the
thinking process behind the modifications it makes to the samples
in the output. This ensures that the model is indeed generating
new samples in the direction we desire. The construction prompts
forbothpositiveandnegativeexamplesareprovidedatourbench-
mark webpage.1
Human Annotation Refinement Theaboveconstructionpro-
cessproducesacoarselabeloffactualconsistencyforeachsample.
To enhance the quality of the labels, we further engage 12 human
experts to annotate the factual consistency of each answer via a
two-step approach [31]. Specifically, the human annotator first de-
composetheanswerintomultiplesegments;foreachsegment,the
annotatorisrequiredtojudgewhetheritisfactualconsistentwith
thereferenceandgivetheevidenceofthejudgement.Thenthehu-
man annotations on all segments are aggregated to yield a factual
consistent label for the answer.
B ABLATION STUDY RESULTS ON THE
REAL-WORLD DATASET
Table 8 further validates the effectiveness of our approach, partic-
ularly highlighting its importance in practical scenarios where en-
hancing the recall of negative samples is crucial while preserving
the discriminative ability of positive samples.
Table 8: Ablation Study Results on the Real-world Dataset.
Here ”ours” refers to our original pipeline, ”A.D.” refers
to the ablation result of answer decomposition, ”w/o COT”
refers to the ablation of COT, and ”w/o logi. eval” refers to
the ablation of the logical consistency evaluation.
L-Face4F
AG A.D. w/o COT w/o logi. eval
O
verall 87.75 76.75 65.50 86.50
-Positive 94.60 82.87 55.99 95.65
-Negative 75.96 66.21 81.86 70.75
C
STATISTIC DETAILS ABOUT REAL-WORLD
DATASET
Table9showsthestatisticsof200model-generatedanswersinour
real-world dataset from six LLMs. Table 10 shows the specific in-
formation about the error distribution about the six LLMs in the
real-world dataset.
6093KDD’24, August 25–29, 2024, Barcelona, Spain Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song
Table 10: Details about the distribution of the error types in the Real-World Dataset.
Hallu.
KCont. KInve. KConf. KConc. LOver. LCaus. LConf. LIncl. LOthe. Other Errors
Q
wen 57.81 21.88 3.13 1.56 6.25 1.56 3.13 3.13 1.56 0 0
GPT-4 77.91 5.81 3.49 2.33 5.81 1.16 2.33 1.16 0 0 0
GPT-3.5 62.71 15.25 5.08 3.39 10.17 3.39 0 0 0 0 0
Alpaca2 (CH) 69.61 8.82 3.92 2.94 7.84 3.92 1.96 0 0.98 0 0
ChatGLM3 61.33 13.33 9.33 1.33 5.33 5.33 0 1.33 1.33 0 1.33
Baichuan2 70.59 12.94 2.35 2.35 5.33 3.53 1.18 0 1.33 0 0
T
able 11: Example of the Reading Comprehension Section in the National College Entrance Examination of China
T
ranslated Reference:
The golden age of blue-and-white porcelain development was during the Yongle and Xuande periods of the Ming Dynasty, coinciding
with Zheng He’s voyages to the Western Seas, prompting us to ponder: Is it mere historical coincidence that both seafaring and porcelain
craftsmanship reached their zenith at the same time? … It was the blending of Chinese and foreign civilizations that successfully drove the
transformation of Chinese porcelain from monochrome to polychrome, with blue-and-white porcelain uniquely illustrating the cultural
evolution of the Ming era, serving as an example of traditional society’s progression from uniformity to diversity. (Excerpted and
compiled from ”The Trajectory of the Rise of Ming Dynasty Blue-and-White Porcelain” by Wan Ming)
Task: evaluate the correctness of the following sentences:
Translated Sentence 1: Zheng He’s voyages to the Western Seas stimulated the production, sales, and technological innovation of
porcelain, heralding the golden age of blue-and-white porcelain development.
Label: correct
Translated Sentence 2: Factors such as the localization of raw materials ushered the development of blue-and-white porcelain into a
new phase, at which point its evolution became unrelated to foreign cultures.
Label: Incorrect .
Error Type: Contradiction Error
Translated Sentence 3: Ming Dynasty society is often considered conservative, yet the styles of blue-and-white porcelain indicate that
the society was relatively open and progressive.
Label: Incorrect .
Error Type: Conceptual Substitution Error
Translated Sentence 4: The blending of Chinese and foreign civilizations promoted the transformation of porcelain from monochrome
to polychrome, thereby driving the society of the time towards a more diverse transition.
Label: Incorrect .
Error Type: Causal Confusion Error
6094