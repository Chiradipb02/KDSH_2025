Spatio-Temporal Consistency Enhanced Differential Network for
Interpretable Indoor Temperature Prediction
Dekang Qi∗
Southwest Jiaotong University
Chengdu, China
dekangqi@outlook.comXiuwen Yi†
JD iCity, JD Technology & JD
Intelligent Cities Research
Beijing, China
xiuwenyi@foxmail.comChengjie Guo
Xidian University
Xi’an, China
chengjie.guo@stu.xidian.edu.cn
Yanyong Huang
Southwestern University of Finance
and Economics
Chengdu, China
huangyy@swufe.edu.cnJunbo Zhang
JD iCity, JD Technology & JD
Intelligent Cities Research
Beijing, China
msjunbozhang@outlook.comTianrui Li
Southwest Jiaotong University
Chengdu, China
trli@swjtu.edu.cn
Yu Zheng
JD iCity, JD Technology & JD
Intelligent Cities Research
Beijing, China
Xidian University
Xi’an, China
msyuzheng@outlook.com
ABSTRACT
Indoor temperature prediction is crucial for decision-making in
central heating systems. Beyond accuracy, predictions shall be in-
terpretable, i.e. conform to the laws of physics; otherwise, it may
lead to system failures or unsafe conditions. However, deep learn-
ing models often face criticism regarding interpretability, which
limits their application in such settings. To this end, we propose
aSpatio- Temporal Consistency enhanced Differential Network
(CONST) for interpretable indoor temperature prediction. Our ap-
proach mainly consists of a differential predictive module and a
spatio-temporal consistency module. Modeling the influential fac-
tors, the first module solves the issue of multicollinearity through
the differential operation. Considering the heterogeneity of global
and local data distributions, the second module characterizes the
temporal and spatial consistency to mine the universal pattern by
multi-task learning, thereby improving the prediction interpretabil-
ity. Besides, we propose a set of interpretability metrics to overcome
the drawbacks of partial dependence plot metric, which are more
∗This work was done when Dekang Qi was an intern at JD iCity, JD Technology & JD
Intelligent Cities Research, Beijing, China.
†Xiuwen Yi is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671608practical, zero-centered, flexible, and numerical. We conclude ex-
periments on a real-world dataset with four heating stations. The
results demonstrate the advantages of our approach over various
baselines, where the interpretability can be improved by more than
8 times on𝑐𝑅𝑃𝐷 while maintaining high accuracy. We developed
CONST on the SmartHeat system, providing hourly indoor temper-
ature forecasts for 13 heating stations in northern China.
CCS CONCEPTS
•Applied computing →Forecasting; •Information systems
→Spatial-temporal systems.
KEYWORDS
Indoor Temperature, Interpretable Prediction, Spatio-temporal Con-
sistency, Differential, Urban Computing
ACM Reference Format:
Dekang Qi, Xiuwen Yi, Chengjie Guo, Yanyong Huang, Junbo Zhang, Tian-
rui Li, and Yu Zheng. 2024. Spatio-Temporal Consistency Enhanced Differen-
tial Network for Interpretable Indoor Temperature Prediction. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671608
1 INTRODUCTION
Indoor temperature has a great impact on living comfort for the cit-
izens. To maintain a comfortable indoor temperature during winter,
central heating systems are widely used in regions with medium to
high latitudes, leveraging their benefits of energy efficiency, supe-
rior comfort, and user convenience [ 6,17]. The schematic diagram
of the heating system is shown in Fig. 1(a). Typically, a central
5590
KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
Heating 
station...
Buildings
Supply water Return water Heating 
system
Indoor
temperature
Outdoor
weatherHeating 
temperature
Building
structure
Influence Control
(a) Central heating system (b) Influence relation ship
Figure 1: Heating system and influence relationship.
heating station transports supply (hot) water to thousands of resi-
dential houses through pipes and continuously releases heat into
the houses. As shown in Fig. 1(b), the indoor temperature is mainly
affected by heating temperature, outdoor weather, and building
structure. With better control of the heating system, the indoor tem-
perature will be more stable, reducing uncomfortable experiences.
Thus, it is essential to predict the indoor temperature, providing
the control strategic-making support for heating dispatchers.
Safety is crucial in the central heating system, as improper con-
trol decisions can lead to system failures or unsafe conditions.
Hence, prediction results shall be interpretable, in addition to being
highly accurate. Interpretability refers to the degree to which a
human can consistently predict the model’s outcomes[ 24]. It usu-
ally means the predictions align with physical laws or common
sense, e.g., Fourier’s Law of Heat Conduction. For example, if other
conditions remain unchanged and predictions do not increase re-
gardless of how much the heating temperature is increased, it is
obviously unexplainable. It is imperative that the predictions ad-
here to the expected behaviors of heating systems, ensuring that
control strategies are inherently safe and reliable.
The prediction methods can be summarized into three categories.
Physical rule-based methods, such as simulation-based Energyplus
[5] and DOE-2 [ 28], are undoubtedly interpretable. However, they
require hard-to-obtain building information, making large-scale
deployment difficult. By simplifying the problem with only easy-
obtained data for model training, simple machine learning methods
(such as linear regression) [ 24] generally gain interpretability yet
still sacrificing accuracy. Deep learning based methods [ 6,22,23,32]
are widely used for high accuracy. However, interpretability often
faces criticism due to its complexity and opacity, which significantly
limits its application in such settings.
To improve the interpretability of deep indoor temperature pre-
diction models, several challenges need to be addressed:
The complex impact of multiple factors, especially for multi-
collinearity. Indoor temperature is affected by multiple influential
factors with different time delays. For example, cold wind pene-
tration takes effect quickly, but the effect of heating temperature
takes several hours to manifest. Moreover, there is multicollinearity
among factors, which may cause difficulty with the reliability of the
estimates of the model parameters [ 2]. For example, if 𝑦=𝑥1+𝑥2
and𝑥1=2𝑥2, this is extreme multicollinearity. Here, 𝑥1and𝑥2
are independent variables, and 𝑦is a dependent variable. The re-
lationship can take countless forms, for instance, 𝑦=0.5𝑥1+2𝑥2,
𝑦=−𝑥1+5𝑥2, which diverges from the true underlying relationship,
rendering them unable to be fully explanatory.
The heterogeneity of global and local data distribution
masks the universal pattern. As shown in Figure 2(b), in the
xy
(b) Heterogeneity  of data distributionSite A
Site B
Temporal global
Temporal local
(a) Complex impact  of multiple factorsy
x1
x2Multicollinearityt+1
t
t-1Impact
timeFigure 2: The challenge of improving interpretability.
temporal dimension, the global (entire samples) and local (nearby
samples) data distributions are highly heterogeneous. In the spatial
dimension, global (all stations) and local (single station) data distri-
butions are also slightly heterogeneous. Since noise has a greater
impact on the local data, leading to disorder. However, it exhibits
minimal influence on the global data, indicating an approximate
linear trend. It indicates that relying solely on chaotic local data is
likely to lose interpretability, while relying only on regular global
data may lose accuracy. Learning high-precision and interpretable
models from heterogeneous data is very difficult, and it requires
mining the universal patterns that are applicable in different times
and spaces rather than being misled by one-sided patterns.
To address the above challenges, we propose the Spatio- Temporal
Consistency enhanced Differential Network (CONST) for inter-
pretable indoor temperature prediction. Our approach can signifi-
cantly improve prediction interpretability while maintaining high
accuracy without relying on physical rule-based or simple machine-
learning models. Our contributions are listed below:
•We propose an interpretable CONST network, which can solve
multicollinearity through differential operation and characterize
temporal and spatial consistency to mine the universal pattern
from heterogeneous data distribution by multi-task learning.
•We propose a set of interpretability metrics to overcome the
drawbacks of existing metrics (Partial Dependence Plot), which
are more practical, zero-centered, flexible, and numerical.
•We conducted experiments on real-world datasets, and the re-
sults demonstrated the advantages of our approach, which can
significantly improve interpretability by more than 8 times on
𝑐𝑅𝑃𝐷 while maintaining high accuracy.
•We deployed the proposed CONST into the SmartHeat system
on the cloud, providing hourly indoor temperature forecasts for
13 heating stations in northern China.
2 OVERVIEW
2.1 Problem Formulation
Given the observation data 𝑋𝑆
𝐻𝑇from multiple stations, the ob-
jective is to predict future label data 𝑌𝑆
𝐹𝑇. Among them, 𝑋is the
historical data. 𝐻𝑇={𝑡−𝑛,...,𝑡}is the historical time, where 𝑡is
the current moment, and 𝑛is the total number of historical moments.
𝑌is the future label data. 𝐹𝑇={𝑡+1,...,𝑡+𝑙}is the future time,
where𝑙is the length of the prediction time window. 𝑆={𝑆1,...,𝑆ℎ}
represents stations, where ℎis the amount of stations.
2.2 Overview
Figure 3 illustrates the framework of the Spatio-Temporal Consis-
tency enhanced Differential Network (CONST), considering the
5591Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Differential 
predictive moduleTemporal discounting
Temporal sample pair constructionTemporal local taskSpatial 
global 
task
Spatial sample pair constructionCore  prediction
Semi-
sharingAuxiliary  prediction
Full 
sharing
Spatial multi -task
Temporal multi -taskSpatial local taskDifferential 
predictive module
Temporal global task
Indoor
 temperatureHeating  
temperatureOutdoor 
weather
Figure 3: The framework of the CONST network.
data from indoor temperature, heating temperature, and outdoor
weather. To improve the interpretability, CONST mainly consists of
two modules: 1) Differential predictive module: complex influence
of multiple factors is modeled through basic deep learning layers
for feature extraction and fusion. Moreover, the multicollinearity
is solved through the differential and restoration operations. 2)
Spatio-temporal consistency module: the temporal and spatial con-
sistency are characterized from heterogeneous global and local data
distributions to mine the universal patterns by multi-task learning.
Firstly, we sample the temporal global and local pairs to construct
tasks with temporal discounting for weighting, using a fully shared
strategy to learn temporal consistency. Secondly, we introduce the
nearby spatial station to construct tasks, adopting a semi-sharing
strategy to learn spatial consistency. Here, the output of the tempo-
ral local task for the target station is considered as a core prediction,
while the others are regarded as auxiliary predictions.
3 METHODOLOGY
3.1 Differential predictive module
3.1.1 Basic deep learning layers. Indoor temperature is influ-
enced by various factors, such as heating temperature and meteo-
rological conditions (e.g., outdoor temperature, weather and wind).
Furthermore, these factors have complex interactions. Thus, we use
some basic deep-learning layers to model the influence.
Specifically, continuous features (heating temperature, outdoor
temperature, wind speed, and sunlight intensity) are standardized
and discrete features (wind direction, weather, day of week, and
hour of day) are embedded. Each discrete feature is assigned an em-
bedding layer, where the dimension is the total number of unique
values. Subsequently, 𝑘moments of standardized continuous fea-
tures and embedded discrete features are concatenated together to
obtain the feature tensor. To learn the interactions, we use a stacked
two-layer LSTM and a fully connected layer to fuse these features,
which can be replaced with MLP, ResNet, and other models.
3.1.2 Differential & Restoration. Multicollinearity refers to the
linear relation among two or more variables, which is usually eval-
uated using Variance Inflation Factor (VIF) [ 2]. If VIF is greater than
10, it indicates strong multicollinearity, while if VIF is close to 1, itTable 1: Variance Inflation Factor
Feature Raw value W/o indoor temp. Difference
Indoor temp. 148.79 - 1.42
Heating temp. 142.92 3.77 1.02
Outdoor temp. 2.13 1.33 1.33
Sunlight intensity 1.89 1.88 1.31
Wind speed 4.46 4.44 1.03
indicates there is no multicollinearity. VIFs of continuous features
under different settings are calculated, and shown in Table 1.
When fed features with raw values directly, the VIFs of heat-
ing temperature and historical indoor temperature exceed 142, in-
dicating strong multicollinearity. Deep learning methods usually
use historical labels as features, making them less interpretable.
When using features without historical indoor temperature, the
VIFs are all less than 4.5, and multicollinearity is significantly re-
duced. Physical rule-based models and simple machine learning
models generally do not use historical labels as features, which is
one reason for interpretability. However, such a process wastes the
information of historical labels, resulting in lower accuracy. To deal
with the multicollinearity problem, the differential operation is the
simplest and most effective way. After the difference, the VIFs of all
features are significantly reduced to less than 1.5. Thus, we adopt
differential operation combined with deep learning layers.
Differential operation. By making the difference between the fea-
tures at time 𝑡and the features at time 𝜏, we can get the differential
feature Δ𝑋and the differential time Δ𝑡. In addition, the differential
labelΔ𝑦is also calculated. Notably, it is unreasonable to directly
make differences in discrete features. It is necessary to make differ-
ences in the embedding space. Therefore, the differential operation
is performed after feature concatenation.
Δ𝑋=𝑋𝑡−𝑋𝜏;Δ𝑡=𝑡−𝜏;Δ𝑦=𝑦𝑡+1−𝑦𝜏+1 (1)
Restoration operation. Due to the differential operation, only the
predicted difference label Δˆ𝑦was obtained after Δ𝑋passed through
the fusion layers. Therefore, we adapt the restoration operation,
which adds Δˆ𝑦to the historical label 𝑦𝜏+1to get the desired pre-
dicted label ˆ𝑦𝑡+1, as shown in equation 2.
ˆ𝑦𝑡+1=𝑦𝜏+1+Δˆ𝑦 (2)
3.2 Spatio-temporal consistency module
In addition to multicollinearity, it is important to possess a univer-
sal pattern for improving interpretability. Specifically, the universal
pattern can be suitable in different times and spaces. For exam-
ple, Fourier’s Law of Heat Conduction [ 19] can suit different unit
times, e.g., one minute, one hour, one day, and different spatial
stations. However, the heterogeneity of global and local data distri-
bution makes it difficult. Therefore, we propose the spatio-temporal
consistency module that utilizes the knowledge-sharing ability by
multi-task learning, which can mine the universal pattern.
3.2.1 Temporal multi-task mechanism. We construct temporal
global and local tasks to learn patterns separately and adapt the
full sharing strategy to learn temporal consistency.
5592KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
t-tTemporal discount

OffsetClose 
discountTrend 
discountPeriodic 
discount
(b) Temporal discounting (a) Temporal multi -samplingt t-1 ... t-n...
...
Global multi -samplingLocal  multi -sampling
Figure 4: Temporal discounting and multi-sampling.
Temporal global and local tasks construction. We first con-
struct global and local sample pairs through temporal multi-sampling.
The characteristics of the data are captured through global and local
tasks, where each task consists of a differential predictive module.
Subsequently, the importance of the sample pair is adjusted through
temporal discounting according to the time interval.
(1) Temporal sample pairs construction. In the temporal dimension,
the key difference between global and local data lies in their time
interval. Therefore, we employ sample pairs with different time
intervals to reflect the characteristics of the data. As shown in Figure
4(a), we construct sample pairs through temporal multi-sampling.
It includes global and local sampling and multi-sampling.
Each sample pair includes a current sample {𝑋𝑡,𝑦𝑡+1}and a
historical sample{𝑋𝜏,𝑦𝜏+1}, making it convenient to process in
differential operations. According to Equation 3, 𝑡is the current
moment;𝜏is the historical moment. In the temporal local sample
pair,𝜏=𝑡−1. In temporal global sample pair, 𝜏=𝑡′, means arbitrary
moment which is randomly selected before the current moment.
𝜏=𝑡−1, 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 𝑙𝑜𝑐𝑎𝑙 𝑠𝑎𝑚𝑝𝑙𝑖𝑛𝑔
𝑡′, 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙𝑔𝑙𝑜𝑏𝑎𝑙 𝑠𝑎𝑚𝑝𝑙𝑖𝑛𝑔(3)
In temporal global sampling, 𝑡′is randomly selected, and random
errors may be introduced. Therefore, we repeat the temporal global
sampling operation multiple times to reduce the impact of random
errors. It can be considered as data enhancement. In addition, in
order to correspond with multiple global sampling, local sampling
is also repeated multiple times. Among them, the number of repeti-
tions of temporal sampling is recorded as 𝑚𝑡.𝑛is the number of
historical moments. After temporal multi-sampling, 𝑚𝑡×𝑛global
sample pairs and 𝑚𝑡×𝑛local sample pairs will be obtained.
(2) Temporal discounting. The temporal global sample pairs have
long time intervals. The longer the time interval, the greater the
influence of unobservable factors, which will lead to greater cumu-
lative errors. Inspired by the discounting in economics and rein-
forcement learning [ 15], we propose temporal discounting to deal
with it and reduce the impact of cumulative errors.
Considering the temporal properties, we divide temporal dis-
counting into close discounting, periodic discounting and trend
discounting, as shown in Figure 4 (b). Close discounting indicates
that data with closer time intervals has greater importance. Here,
we use𝛾𝑐to represent the close discount coefficient and calculate
𝑛𝑐by the number of days in the interval, using 𝑅𝑐=𝛾𝑐𝑛𝑐to cal-
culate the close discount rate 𝑅𝑐. Similarly, trend discounting can
be calculated. Periodic discounting indicates that the closer the
position within the period, the greater its importance. We take 24
hours as a period, 𝑛𝑝is the distance within the period, which canbe calculated according to Equation 6. When the time interval is 12
hours, the distance within the period is the furthest. Similarly, we
can calculate the periodic discount rate 𝑅𝑝. Finally, the product of
the three discount rates is used as the temporal discount rate. Here,
the temporal discounting only applies to global sample pairs. As
shown in Equation 7, we offset the time interval by 1 hour. Thus,
the temporal discount rate 𝑅of local sample pairs is equal to 1,
which will not affect local sample pairs.
𝑅=𝑅𝑐×𝑅𝑝×𝑅𝑡=𝛾𝑛𝑐𝑐×𝛾𝑛𝑝
𝑝×𝛾𝑛𝑡
𝑡(4)
𝑛𝑐=Δ𝑡′𝑑𝑎𝑦, 𝑛𝑡=Δ𝑡′𝑤𝑒𝑒𝑘 (5)
𝑛𝑝=Δ𝑡′ℎ𝑜𝑢𝑟,Δ𝑡′ℎ𝑜𝑢𝑟<12
|Δ𝑡′ℎ𝑜𝑢𝑟−24|,Δ𝑡′ℎ𝑜𝑢𝑟≥12(6)
Δ𝑡′=Δ𝑡−1 (7)
Temporal discounting can be used as a correction method for the
loss weight, applied between fusion layers and during the restora-
tion operation. We multiply the temporal discount rate 𝑅by the
predicted result Δˆ𝑦, and the ground truth Δ𝑦is also multiplied by 𝑅.
In other words, it can scale the loss according to the time intervals
between sample pairs. As shown in the Equation 8, where 𝑛is the
number of historical moments, 𝑚𝑡is the number of multi-sampling,
and𝑖represents the 𝑖-th sampling pair, 𝐿𝑜𝑠𝑠𝑇𝑇represents the loss
of the temporal task. The larger the time interval between sample
pairs, the more the loss is reduced and the lower the contribution
to the sum of losses for all sample pairs.
𝐿𝑜𝑠𝑠𝑇𝑇=1
𝑚𝑡×𝑛𝑚𝑡×𝑛∑︁
𝑖=1𝑅𝑖×|Δ𝑦𝑖−Δˆ𝑦𝑖| (8)
Full sharing strategy. Temporal local tasks can achieve high
prediction accuracy, while local data are easily contaminated by
noise, reducing the interpretability. Global data has stronger anti-
noise performance, and temporal global tasks can achieve higher
interpretability. Sharing parameters between two tasks can combine
their advantages of them. However, due to the high heterogeneity
in data distribution, all parameters must be shared, otherwise the
model will be dominated by temporal local tasks and learn incorrect
knowledge from messy local data. By sharing all parameters, the
model can be forced to learn consistent patterns in both global
and local data. Only patterns that are common in global and local
data can be adopted by the model, and patterns that are only appli-
cable to one kind of data will be discarded. Furthermore, sharing
all parameters of two tasks does not mean there is a redundant
task. Because the two tasks process different data respectively, the
weights of the two tasks can be adjusted through the loss weight.
Here, we use 𝐿𝑜𝑠𝑠𝑇𝐿𝑇 to represent the loss of the temporal local
task and𝐿𝑜𝑠𝑠𝑇𝐺𝑇 to represent the loss of the temporal global task.
They are different expressions of 𝐿𝑜𝑠𝑠𝑇𝑇. We can balance between
two tasks by loss weights 𝛼1and𝛼2. The loss of the temporal multi-
task mechanism 𝐿𝑜𝑠𝑠𝑇𝑀𝑇 can be expressed as:
𝐿𝑜𝑠𝑠𝑇𝑀𝑇=𝛼1𝐿𝑜𝑠𝑠𝑇𝐿𝑇+𝛼2𝐿𝑜𝑠𝑠𝑇𝐺𝑇 (9)
3.2.2 Spatial multi-task mechanism. We construct spatial global
and local tasks to learn patterns separately and adapt the semi-
sharing strategy to learn spatial consistency.
Spatial global and local tasks construction. We first obtain
spatial global and local samples through spatial multi-sampling.
5593Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Then, characteristics are captured respectively through the spatial
global task and the local task, where each task comprises a temporal
multi-task mechanism.
Spatial sample pairs construction. We sample the same number
of spatial local and global samples. Among them, local samples are
selected from target station data; global samples are sampled from
all nearby stations. We record the repeat time of spatial sampling
as𝑚𝑠, which is also the ratio of the number of local samples to the
number of local data. When 𝑚𝑠=1, local data can be used as local
samples, but not all global data has been sampled. We can increase
𝑚𝑠to reduce the number of unsampled data and avoid missing
information. After spatial multi-sampling, the data is augmented
and Equation 8 will be revised to:
𝐿𝑜𝑠𝑠𝑇𝑇=1
𝑚𝑡×𝑚𝑠×𝑛𝑚𝑡×𝑚𝑠×𝑛∑︁
𝑖=1𝑅𝑖×|Δ𝑦𝑖−Δˆ𝑦𝑖| (10)
Semi-sharing strategy. The global and local data distributions
in the spatial dimension are only slightly heterogeneous, there is
no need to worry about learning contradictory knowledge from
global and local tasks. Thus, sharing partial parameters is enough
for the model to learn spatial consistency, called the semi-sharing
strategy. The shared part deals with commonalities, and the non-
shared part deals with characteristics. Shared network parameters
include parameters of embedding layers and lower stack layers.
Hyperparameters are usually shared but can also chosen differently
for different tasks through experiments. For example, the time
window length of feature ( 𝑘) can reflect the influence of the building
structure. The shorter the length of the pipe network, the better
the thermal insulation performance, and the smaller the time delay
of impacts, so a smaller 𝑘is more appropriate.
We use𝐿𝑜𝑠𝑠𝑆𝐿𝑇to represent the loss of spatial local task, and
use𝐿𝑜𝑠𝑠𝑆𝐺𝑇 to represent the loss of spatial global task. They are
different expressions of 𝐿𝑜𝑠𝑠𝑇𝑀𝑇 . Using𝛽1and𝛽2to represent the
loss weight of the tasks. The loss of the spatial multi-task 𝐿𝑜𝑠𝑠𝑆𝑀𝑇,
which is also the loss of the CONST model 𝐿𝑜𝑠𝑠, is:
𝐿𝑜𝑠𝑠=𝐿𝑜𝑠𝑠𝑆𝑀𝑇=𝛽1𝐿𝑜𝑠𝑠𝑆𝐿𝑇+𝛽2𝐿𝑜𝑠𝑠𝑆𝐺𝑇 (11)
4 METRICS OF INTERPRETABILITY
The Partial Dependence Plot ( 𝑃𝐷𝑃𝑙𝑜𝑡 ) [24] is a commonly used and
intuitive interpretability metric. However, it has some shortcomings:
not conforming to the constraints of feature changes, inconvenient
for comparison, and inconvenient for visualization with multiple
features. Thus, we propose the Restricted Partial Dependence Plot
and its variants to better measure the interpretability.
4.1 Restricted Partial Dependence
The Partial Dependence Plot [ 24] is drawn according to the partial
dependence function. For a feature 𝑥𝑣, it actually calculates the
average value of prediction results when values of all samples’
𝑥𝑣change to a specific value, while the values of other features
remain unchanged. However, in practical applications, the value
of features can usually be slightly increased or decreased based on
their original value. Significantly changing the value of a feature
in a short period may exceed the system’s safety limits or become
unreasonable data. For example, the change of heating temperature
and outdoor temperature in one hour rarely exceeds 2◦𝐶.Therefore, we calculate the average value of prediction results
when values of the samples’ feature 𝑥𝑣increase or decrease in a
restricting range based on their original values while the values of
other features remain unchanged. We call it Restricted Partial De-
pendence (𝑅𝑃𝐷 ), and the function is defined as Equation 12. Among
them,𝑥𝑣is the varying feature; 𝑥𝑜represents the other features. ˆ𝑓
is the trained model, 𝑛is the total number of samples, Δrepresents
the unit size of the change, 𝑝represents a multiple of the unit size,
and𝑡is the current moment. If the values of feature 𝑥𝑣at time𝑡to
𝑡−𝑘are changed simultaneously, it can be recorded as 𝑅𝑃𝐷𝑝
𝑥𝑣𝑡∼𝑡−𝑘.
When it is unnecessary to emphasize time, we simply record it as
𝑅𝑃𝐷𝑝
𝑥𝑣. For the continuous feature, the unit size can be determined
according to specific scenarios. For example, 0.1◦𝐶is used as the
unit size for temperature. For the discrete feature, its value is an
integer between 0∼𝑄, and the unit size can be 1. 𝑅𝑃𝐷𝑃𝑙𝑜𝑡 can
be drawn according to the 𝑅𝑃𝐷 function, where the vertical axis
is the change of the dependent variable, and the horizontal axis is
the change of the independent variable or the change multiple of
the unit size. Compared with 𝑃𝐷𝑝𝑙𝑜𝑡 ,𝑅𝑃𝐷𝑝𝑙𝑜𝑡 has some advan-
tages. It is more consistent with actual security restrictions. It is
zero-centred and focuses on the changing trends that are of con-
cern in practical applications without being affected by prediction
accuracy. When there are more than two features in 𝑥𝑣, it can still
be displayed graphically if 𝑝is the same.
𝑅𝑃𝐷𝑝
𝑥𝑣𝑡=1
𝑛𝑛∑︁
𝑖=1ˆ𝑓(𝑥𝑣𝑖
𝑡+Δ𝑝,𝑥𝑜𝑖
𝑡)−ˆ𝑓(𝑥𝑣𝑖
𝑡,𝑥𝑜𝑖
𝑡) (12)
4.2 Variants of Restricted Partial Dependence
𝑃𝐷𝑝𝑙𝑜𝑡 and𝑅𝑃𝐷𝑝𝑙𝑜𝑡 are intuitive graphs and cannot provide nu-
merical interpretability, which is inconvenient for comparison, so
we proposed variants of 𝑅𝑃𝐷 . If a highly interpretable model is
selected as the benchmark, the 𝑅𝑃𝐷 of the model is regarded as the
Benchmark Restricted Partial Dependence (𝑏𝑅𝑃𝐷 ). Similar to
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 and𝑀𝐴𝑃𝐸 , we define the Standardized Restricted Par-
tial Dependence (𝑠𝑅𝑃𝐷 )as shown in Equation 13, where 𝑝cannot
be zero to avoid division by zero. Generally, there is only one feature
in𝑥𝑣. When𝑥𝑣includes all features ( 𝐹is the number of features),
it is recorded as 𝑋𝑉, and the𝑠𝑅𝑃𝐷 at this point is called Model
Restricted Partial Dependence (𝑚𝑅𝑃𝐷 ). If the features are con-
sidered to be independent and the value of each feature in 𝑋𝑉can
be changed sequentially rather than simultaneously, this is called
Independent Model Restricted Partial Dependence (𝑖𝑚𝑅𝑃𝐷 ). If
we only focus on the interpretability of a few core features 𝑋𝑉′, it
is called Core Restricted Partial Dependence (𝑐𝑅𝑃𝐷 ). The larger
the value of the 𝑠𝑅𝑃𝐷 ,𝑚𝑅𝑃𝐷 ,𝑖𝑚𝑅𝑃𝐷 , and𝑐𝑅𝑃𝐷 , the better.
𝑠𝑅𝑃𝐷−𝑃∼𝑃
𝑥𝑣 =100%−1
2𝑃𝑃∑︁
𝑝=−𝑃𝑏𝑅𝑃𝐷𝑝
𝑥𝑣−𝑅𝑃𝐷𝑝
𝑥𝑣
𝑏𝑅𝑃𝐷𝑝
𝑥𝑣×100% (13)
𝑚𝑅𝑃𝐷−𝑃∼𝑃=𝑠𝑅𝑃𝐷−𝑃∼𝑃
𝑋𝑉(14)
𝑖𝑚𝑅𝑃𝐷−𝑃∼𝑃=1
𝐹𝐹∑︁
𝑗=1𝑠𝑅𝑃𝐷−𝑃∼𝑃
𝑋𝑉𝑗(15)
𝑐𝑅𝑃𝐷−𝑃∼𝑃=1
𝐹′𝐹′∑︁
𝑗=1𝑠𝑅𝑃𝐷−𝑃∼𝑃
𝑋𝑉′𝑗(16)
5594KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
5 EXPERIMENT
5.1 Data sets
We conducted experiments on a real-world dataset with four heat-
ing stations. The details is shown in Table 2. Among them, the
time range of stations A and B is 2018/12/14 ∼2019/03/15, and the
time range of stations C and D is 2018/11/15 ∼2019/03/15. The time
interval of the data is 1 hour. These stations have 2192, 2176, 2704,
and 2658 valid data, respectively.
Table 2: Details of the Dataset
Station A B C D
Indoor temp./◦𝐶16.0∼23.2 13.9∼22.5 18.8∼27.3 20.2∼25.0
Heating temp./◦𝐶29.0∼50.4 26.7∼52.6 27.4∼46.7 29.7∼46.5
Outdoor temp./◦𝐶-10∼18.4 -10∼18.4 -11∼17 -11∼17
Sunlight/𝐿𝑢𝑥 0∼674 0∼655 0∼674 0∼655
Wind speed/𝑚𝑝𝑠 0∼6.7 0∼6.7 0.1∼12.3 0.1∼12.3
Wind direction 9 types (e.g. no wind, east, southeast)
Weather 16 types (e.g. sunny, rainy)
Day of week 7 types (e.g. Monday, Tuesday)
hour of day 24 types (e.g. 0, 1)
5.2 Baselines
Traditional Models:
•Multiple Linear Regression (Linear): The multiple linear re-
gression model is a classic simple machine learning model with
high interpretability [24].
•Data Fitting Physical Model (Physical): Refer to the method in
the book [ 39], we derive a nonlinear model to predict indoor tem-
perature based on physical rules and fit the parameters through
real data. Please see appendix A.3 for details.
•SARIMAX: The Seasonal Autoregressive Integrated Moving
Average with Exogenous Regressors model [ 26] is a well-known
statistical method, which is often used for time series forecasting.
Classic Deep Learning Models:
•MLP: A simple deep neural network uses multi-layer fully con-
nected layers named multilayer perceptron.
•LSTM: Long short-term memory network [ 13], suitable for pro-
cessing time series with a long delay.
•ResNet: The Deep residual network [ 12] shines in fields such as
computer vision. In this paper, the CNN layer is replaced by the
fully connected layer.
•MTL: An extension of ResNet for time series prediction with
multi-task learning [ 34,35]. Here, we replace the CNN layer with
the fully connected layer and use splicing to replace the gating
fusion component for discrete features.
•TimesNet: Transformer-based time series prediction approach,
TimesNet [ 29] has achieved excellent prediction accuracy by
introducing 2-dimensional space analysis.
Recent indoor temperature prediction models:
•MMeSN: The multi-memory enhanced separation network [ 8]
is a parameter-based multi-source transfer learning method de-
signed for indoor temperature prediction.Table 3: Individualized hyperparameters for each station
Station𝑘 𝑚𝑡𝛼2𝛽2𝛾𝑐𝛾𝑝𝛾𝑡
A 4 3 1.1 1.15 0.9995 0.9982 0.9974
B 2 2 0.85 1.175 1 0.9982 0.9989
C 6 2 1.075 0.925 0.9991 0.9996 0.9989
D 7 2 0.975 1.05 0.9995 0.9992 0.9994
CONST-based Models: The CONST network mainly relies on dif-
ference and spatio-temporal consistency, while the basic deep learn-
ing layers can be easily replaced with different structures. We ob-
tainCONST-MLP, CONST-ResNet, CONST-MTL, and CONST-
LSTM using MLP, ResNet, MTL, and LSTM as the stacked layers in
CONST respectively.
5.3 Experimental Setup
5.3.1 Preprocessing. We removed outliers outside of 3 standard
deviations and filled in missing data. Continuous data is standard-
ized as data with a mean of 0 and standard deviation of 1, and then
de-standardized back to normal values during evaluation. We use
80% data for training and 20% data for testing. Then, 20% of the test
data is used for validation. The experiment was repeated 3 times,
and the mean value was taken as the final result.
5.3.2 Parameter Setting. The output dimension of each discrete
feature embedding layer is set to 2. The 𝑙2_𝑟𝑎𝑡𝑒 of the kernel reg-
ularizer in each stack layer is set to 0.01. A Dropout layer with a
Dropout rate of 0.1 is used after the upper stack layers. 𝛼1,𝛽1, and
𝑚𝑠are set to 1. Other hyperparameters of all models in the same
station remain consistent, and in different stations can be selected
through experiments, as shown in Table 3. The MLP model uses
8 layers with 32-node fully connected layers stacked. The LSTM
model uses 2 layers with 32-node LSTM layers stacked. Based on
the MLP model, the ResNet model adds two shortcut connections.
The MTL model uses the data of the last 𝑘(feature time window
length) moments as close data, uses the data of the previous day of
each close data as the periodic data, and uses the data of the previ-
ous week of each close data as the trend data. In the CONST-based
models, the parameters of the basic deep learning layers are the
same as those mentioned parameters of classic deep learning mod-
els. The parameters of SARIMAX are automatically obtained using
the auto_arima function in the pmdarima library. The parameters
of MMeSN are the same as described in the paper [ 8]. In TimesNet,
𝑠𝑒𝑞_𝑙𝑒𝑛=𝑘,𝑙𝑎𝑏𝑒𝑙 _𝑙𝑒𝑛=𝐶𝑒𝑖𝑙𝑖𝑛𝑔(𝑘/2),𝑡𝑜𝑘_𝑘=4.
5.3.3 Model Optimization. We use the Adam optimizer [ 16].
The learning rate is 0.0001, 𝑏𝑒𝑡𝑎 1=0.9,𝑏𝑒𝑡𝑎 2=0.999, and the
batch size is 64. Train for 300 epochs using an early stop strategy
with a tolerance threshold of 10 rounds.
5.3.4 Experimental Environment. We use a cloud server with
a 16-core CPU and 64G memory. Our model is constructed using
the Keras library in Python, and multi-processing is used to speed
up the computation. The code is shown in GitHub1.
1https://github.com/QiDekang/CONST
5595Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Comparison with Different Baselines
MethodsStation A (%) Station B (%) Station C (%) Station D (%) Average (%) Improved
𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸(%)↓𝑐𝑅𝑃𝐷(%)↑
Physical 6.57 100.00 5.45 100.00 6.74 100.00 7.35 100.00 6.53 100.00 5.94 -7.50
Linear 6.95 97.75 5.37 96.18 6.90 98.38 7.05::::88.08 6.57 95.10 5.98 -2.60
SARIMAX 4.73 8.18 2.27 7.25 3.54 6.35 2.34 9.64 3.22 7.85 2.63 84.65
MLP 1.21 7.68 0.53 2.26 1.01 8.83 1.02 14.62 0.94 8.35 0.35 84.15
LSTM 0.94 10.17 0.77 6.93 1.09 11.42 0.72 13.16 0.88 10.42 0.29 82.08
ResNet 0.91 3.11 0.60 3.17 0.81 4.27 0.72 14.97 0.76 6.38 0.17 86.12
MTL 3.84 0.18 0.47 0.55 2.00 1.94 0.62 5.70 1.73 2.09 1.14 90.41
MMeSN 1.05 13.09 0.51 4.44 1.34 -0.94 0.61 7.13 0.88 5.93 0.29 86.57
TimesNet 0.62 0.00 0.43 0.00 0.59 0.00 0.32 0.00 0.49 0.00 - 0.10 92.50
CONST-LSTM 0.53::::91.60 0.96::::89.52 0.58::::95.90 0.31 92.96 0.59::::92.50 - -
5.3.5 Evaluation Metrics.
Accuracy:
𝑀𝐴𝑃𝐸 =1
𝑛𝑛∑︁
𝑖=1𝑦𝑖−ˆ𝑦𝑖
𝑦𝑖×100% (17)
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =100%−𝑀𝐴𝑃𝐸 (18)
Interpretability: We use the𝑃𝐷𝑃𝑙𝑜𝑡 ,𝑅𝑃𝐷𝑃𝑙𝑜𝑡 , and𝑐𝑅𝑃𝐷 as de-
scribed in Section 4 to evaluate interpretability. Among them, we
take the heating temperature and outdoor temperature as the core
features, take 0.1◦𝐶as the unit size of change, and use 𝑃=20as
the maximum multiple of the unit size. That is, the results when
the change from -2◦𝐶to 2◦𝐶are displayed.
5.4 Experimental results
5.4.1 Comparison with Different Baselines. We evaluate in-
terpretability by calculating 𝑐𝑅𝑃𝐷 on the heating temperature and
outdoor temperature. Here, we use the Data Fitting Physical Model
as the benchmark of the interpretable model, so its 𝑐𝑅𝑃𝐷 is 100%.
As shown in Table 4, the accuracy of the physical model is slightly
higher than that of the linear model, indicating that detailed model
derivation and nonlinear fitting have certain values, but are more
cumbersome than the linear model. The linear model has a high
interpretability of 95.10%. When applied to areas where it is incon-
venient to build physical models, the linear model can be used as
the benchmark of the interpretable model. Compared with simple
statistical models, SARIMAX improves accuracy, but reduces the
focus on the impact of independent variables on dependent vari-
ables, resulting in reduced interpretability. Classic deep learning
models have higher accuracy (above 98%), but their interpretabilityis lower than 11%. Among them, the TimesNet model mainly relies
on the correlation between future data and historical data, or even
historical moments, to make predictions. Focusing too much on
correlation and not paying enough attention to the impact of the
independent variable on the dependent variable can improve accu-
racy but lead to lower interpretability. Similarly, the interpretability
of the MTL model is also relatively low. The interpretability of
CONST-LSTM model is second only to intrinsically interpretable
models such as physical model and linear model, and is superior
to SARIMAX (classic statistical model) and other deep learning
models. CONST-LSTM maintains high accuracy while significantly
improving interpretability, compared with other deep learning mod-
els: except for TimesNet, both accuracy and interpretability have
been improved; compared with the model with the highest accuracy
(TimesNet), the accuracy is only decreased by 0.1% (from 99.51% to
99.41%); compared with the model with the highest interpretability
(LSTM), the interpretability is improved by more than 8 times (from
10.42% to 92.50%).
5.4.2 Comparison with different basic deep learning mod-
els.Due to our approach addressing the two challenges of inter-
pretability and also making more comprehensive use of global and
local data, compared with each basic deep learning model, the
CONST-based models have achieved significant improvements in
interpretability and small improvements in accuracy. As shown in
Table 5, the interpretability has improved by more than 4, 7, 14,
and 8 times, and the error ( 𝑀𝐴𝑃𝐸 ) has decreased by 0.55, 0.37, 1.32,
and 0.29, respectively. Since LSTM has the highest interpretability
among basic deep learning models, CONST-LSTM has the highest
Table 5: Comparison with different basic deep learning models
MethodsStation A (%) Station B (%) Station C (%) Station D (%) Average (%) Improved
𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷 𝑀𝐴𝑃𝐸(%)↓𝑐𝑅𝑃𝐷(%)↑
MLP 1.21 7.68 0.53 2.26 1.01 8.83 1.02 14.62 0.94 8.35 0.55 4.9×
CONST-MLP 0.41 38.24 0.51 43.74 0.39 31.65 0.24 49.31 0.39 40.73 - -
ResNet 0.91 3.11 0.60 3.17 0.81 4.27 0.72 14.97 0.76 6.38 0.37 7.7×
CONST-ResNet 0.40 50.04 0.49 45.41 0.43 58.43 0.25 41.71 0.39 48.90 - -
MTL 3.84 0.18 0.47 0.55 2.00 1.94 0.62 5.70 1.73 2.09 1.32 14.9×
CONST-MTL 0.45 41.64 0.50 0.00 0.44 37.14 0.24 45.58 0.41 31.09 - -
LSTM 0.94 10.17 0.77 6.93 1.09 11.42 0.72 13.16 0.88 10.42 0.29 8.9×
CONST-LSTM 0.53 91.60 0.96 89.52 0.58 95.90 0.31 92.96 0.59 92.50 - -
5596KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
Table 6: Ablation Experiment Results
MethodsAverage (%)
𝑀𝐴𝑃𝐸 𝑐𝑅𝑃𝐷
CONST-LSTM-w/o-SMT (spatial multi-task) 0.59 90.28
CONST-LSTM-w/o-TMT (temporal multi-task) 0.28 4.90
CONST-LSTM-w/o-DO (differential operation) 0.55 4.57
CONST-LSTM-w/o-DF (discrete features) 0.48 84.41
CONST-LSTM 0.59 92.50
interpretability among CONST-based models. Since the MTL and
CONST-MTL models rely on close, periodic, and trend data, the
models may learn that the weights of close data are all 0, resulting
in a𝑐𝑅𝑃𝐷 of 0, as shown in Table 5.
5.4.3 Results of Ablation Experiment. As shown in Table 6:
The interpretability of CONST-w/o-TMT and CONST-w/o-DO is
poor, indicating that the temporal multi-task and the differential
operation are indispensable. The interpretability of CONST-w/o-
SMT is slightly lower, indicating that considering the commonalities
of multiple stations can help improve the interpretability. Although
removing discrete features will slightly improve the prediction
accuracy, the interpretability will be reduced. It shows that discrete
features are also valuable for improving interpretability.
5.4.4 Interpretability on PD Plot vs RPD Plot. Figure 5(a) il-
lustrates results of different models on the 𝑃𝐷𝑝𝑙𝑜𝑡 metric using the
data in station A. The trend of the classic deep learning model (e.g.,
LSTM) is significantly different from that of the physical model,
while the trend of the CONST based model (e.g., CONST-LSTM) is
more similar to that of the physical model, proving that CONST
can significantly improve the interpretability. The 𝑃𝐷𝑝𝑙𝑜𝑡 is eas-
ily affected by prediction accuracy, while the results on 𝑅𝑃𝐷𝑝𝑙𝑜𝑡
shown in Figure 5 (b) is zero-centered, and the above trend is more
pronounced. According to the intersection points of the red straight
line (Δ𝑦=0.2) and the curves in the figure, when increasing the
indoor temperature in the future by 0.2◦𝐶, the changes in the heat-
ing temperature obtained by the physical model, linear model, and
CONST-LSTM model are similar. Both are about 1◦𝐶. However,
the classic deep learning model has no intersection with the red
straight line. If we want to change the indoor temperature, we need
to provide a heating adjustment that is far beyond common sense
and system security restrictions. It is even possible to obtain the
direction of adjustment opposite to the physical model.
/uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013
/uni0000002b/uni00000048/uni00000044/uni00000057/uni00000003/uni00000037/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000014/uni0000001b/uni00000011/uni00000018/uni00000014/uni0000001c/uni00000011/uni00000013/uni00000014/uni0000001c/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000018/uni0000002c/uni00000051/uni00000047/uni00000052/uni00000052/uni00000055/uni00000003/uni00000037/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000044/uni0000004f/uni00000003/uni00000027/uni00000048/uni00000053/uni00000048/uni00000051/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000033/uni0000004f/uni00000052/uni00000057
/uni00000033/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057
/uni00000030/uni0000002f/uni00000033
/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000030/uni00000037/uni0000002f
/uni0000002f/uni00000036/uni00000037/uni00000030
/uni00000030/uni00000030/uni00000048/uni00000031/uni00000031
/uni00000036/uni00000024/uni00000035/uni0000002c/uni00000030/uni00000024/uni0000003b
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000030/uni0000002f/uni00000033
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000030/uni00000037/uni0000002f
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030
/uni00000010/uni00000015 /uni00000010/uni00000014 /uni00000013 /uni00000014 /uni00000015
/uni0000002b/uni00000048/uni00000044/uni00000057/uni00000003/uni00000037/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000010/uni00000013/uni00000011/uni00000017/uni00000010/uni00000013/uni00000011/uni00000016/uni00000010/uni00000013/uni00000011/uni00000015/uni00000010/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni0000002c/uni00000051/uni00000047/uni00000052/uni00000052/uni00000055/uni00000003/uni00000037/uni00000048/uni00000050/uni00000053/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000035/uni00000048/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000057/uni0000004c/uni00000044/uni0000004f/uni00000003/uni00000027/uni00000048/uni00000053/uni00000048/uni00000051/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000033/uni0000004f/uni00000052/uni00000057
/uni00000033/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057
/uni00000030/uni0000002f/uni00000033
/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000030/uni00000037/uni0000002f
/uni0000002f/uni00000036/uni00000037/uni00000030
/uni00000030/uni00000030/uni00000048/uni00000031/uni00000031
/uni00000036/uni00000024/uni00000035/uni0000002c/uni00000030/uni00000024/uni0000003b
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000030/uni0000002f/uni00000033
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni00000030/uni00000037/uni0000002f
/uni00000026/uni00000032/uni00000031/uni00000036/uni00000037/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030
Figure 5: Interpretability on PD Plot vsRPD plot
1 2 3 4 5 6
Predict Window length0.60.81.01.21.4MAPE
(a) Long-term Prediction
20406080
cRPD
MAPE of LSTM
MAPE of CONST-LSTM
cRPD of LSTM
cRPD of CONST-LSTM
-2 -1 0 1 2
Temperature difference-0.4-0.20.00.20.4Indoor temperature difference
(b) RPD plots of different factors
Heat temperature
Outdoor temperatureFigure 6: Prediction length and interpretability of features
5.4.5 Long-term Prediction. The average results are shown in
Figure 6(a), the prediction error increases when long-term predic-
tions are made. The 𝑐𝑅𝑃𝐷 of CONST-LSTM decreases slightly, but
remains at a high level. The 𝑐𝑅𝑃𝐷 of LSTM slightly increases.
5.4.6 The interpretability of different features. As shown in
Figure 6(b), using the CONST-LSTM model in station A, compared
to changing the outdoor temperature, changing the heating tem-
perature has a more significant impact on the indoor temperature.
5.4.7 Parameter Experiments. We conducted parameter experi-
ments using the CONST-LSTM model at all stations.
Time window length of feature ( 𝑘).As shown in Figure 7(a), when
𝑘is increased, the interpretability and error present a trend of first
increasing and then decreasing. The influence of various factors on
indoor temperature has time lags, and as 𝑘increases, the data of
reaction hysteresis can be gradually obtained, and the interpretabil-
ity increases. When 𝑘is too large, and the redundant data also
increases, so the interpretability gradually decreases. The time lags
of stations are different, and the interpretability can be improved
by setting individualized 𝑘.
Loss weight. As shown in Figure 7(b), as 𝛼2increases, the weight
of the global task increases, so the interpretability increases, and
the accuracy decreases. The result for 𝛽2is similar.
Data enhancement. The local task 𝜏=𝑡−1is a special case among
the many values of 𝜏=𝑡′in the global task. The greater the 𝑚𝑡,
the higher the weight of the local task, making the interpretability
decline and the accuracy increase, as shown in Figure 7(c). The
result for𝑚𝑠is similar.
Temporal discount. Temporal discount adjusts the importance
of global data, In Figure 7(d), as 𝛾𝑝increases, the interpretability
increases, and the accuracy decreases. 𝛾𝑐and𝛾𝑡yield similar results.
2 4 6 8 10 12 14
Feature Window length98.899.099.299.499.6Accuracy
(a) The influence of feature window length
405060708090100
cRPD
 Acc-A
Acc-B
Acc-C
Acc-DcRPD-A
cRPD-B
cRPD-C
cRPD-D
0.9 1.0 1.1 1.2 1.3 1.4 1.5
TGT loss weight99.3099.3599.4099.4599.50Accuracy
82.585.087.590.092.5
cRPD
(b) The influence of loss weight
Acc cRPD
0.995 0.996 0.997 0.998 0.999 1.000
Periodic discount coefficient99.3999.4099.41Accuracy
929394
cRPD
(d) The influence of temporal discount
Acc cRPD
2 4 6 8 10
Temporal enhancement times99.4099.4599.50Accuracy
75808590
cRPD
(c) The influence of data enhancement
Acc cRPD
Figure 7: Experimental results with different parameters
5597Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 8: The Web interface of SmartHeat System
6 SYSTEM DEPLOYMENT
We deployed the proposed CONST approach into the SmartHeat
system on the cloud. The system mainly consists of four modules.
The data collector module continuously collects real-time data, such
as heating data, indoor temperature data, and weather data. Con-
sidering the limited coverage and data missing issues of indoor
temperature, the data supplement module fills the missing values
based on their spatio-temporal neighbors. Then, the prediction mod-
ule predicts future indoor temperatures for the next time interval.
After that, the control module provides the recommended heating
temperature using optimization and control strategies.
The system offers a user-friendly web interface to the heating
dispatcher. As shown in Figure 8, it is mainly comprised of three
components on the prediction module. The upper component dis-
plays the overview of one selected heating station. The button left
component focuses on prediction accuracy, providing a comparison
between predicted indoor temperature and actual indoor tempera-
ture over different time spans. The button right component focuses
on the prediction interpretability, displaying the interpretability of
core influential factors on the 𝑐𝑅𝑃𝐷 metric.
Taking into account the frequency of data collector, the predictive
system is designed to execute hourly updates, generating forecasts
for the subsequent six-hour period for the 13 heating stations in
northern China. To ensure continued accuracy, the CONST model
is retrained monthly, utilizing the latest heating data inputs. Such
proactive model recalibration is critical for capturing and adapting
to dynamic patterns and shifts.
Our model does not rely on physical rule-based models and
therefore does not require detailed construction information for
different stations, such as pipe network lengths, building materials,
etc. It only requires data that is more easier to obtain, such as indoor
temperature, heating temperature, outdoor weather, etc. Therefore,
it is easier to deploy in practice. However, it requires hourly average
indoor temperature data, so it is necessary to install temperature
sensors in some residents’ houses for data collection, which will
bring additional deployment costs.
7 RELATED WORK
Indoor Temperature Prediction: There are three types of meth-
ods. (1) Simulation models such as Energyplus [ 5] and DOE-2 [ 28]
are typical physical-based models, which require detailed informa-
tion about buildings, are not convenient for large-scale deployment.
(2) Data-driven methods, especially deep learning methods, havehigh accuracy but low interpretability. Such as papers based on
NNARX (Autoregressive neural networks with exogenous variables)
[1,7,22,23], LSTM and seq2seq [ 10], and LSTM and CNN [ 9]. (3)
The RC thermal network [ 3,11,14] is a typical hybrid model, which
uses a network composed of resistors and capacitors to simplify
the physical model, and fits parameters through actual data. The
demand for domain knowledge and hard-to-obtain data is still high.
Interpretable AI: The methods to improve interpretability can
be summarized into two types. One type is combines deep learning
models with physical models [ 4,31] or simple machine learning
models (e.g., linear regression) [ 36], to balance accuracy and in-
terpretability. However, the disadvantage lies in requiring hard-to-
obtain data or difficulty in determining which factors should be
included in the primary linear model. Our model is significantly
different, which does not rely on physical and simple machine learn-
ing models, and can plug and play to improve the interpretability
of deep learning models. Another type is utilizing the similarity
between time series segments and basic shapes [ 18] or historical
segments [ 30], which only focus on the dependent variable, while
we focus on the influence of the independent variable on the de-
pendent variable.
Deep Learning for Spatio-temporal Data Mining: There are
many works that use deep learning methods to mine knowledge
from massive spatio-temporal data[ 27]. Various deep learning mod-
els, such as CNN-based [ 25], ResNet-based[ 34], Transformer-based
[20,38], are proposed to learns spatio-temporal dependencies si-
multaneously, achieving high prediction accuracy. Additionally,
multi-task learning (MTL) aims to leverage useful information con-
tained in multiple related tasks to help improve the generalization
performance of all the tasks [ 37]. The MTL-based [ 21,34] methods
have achieved high accuracy in multiple fields. Unlike them, we
use a fully shared and semi-sharing strategy simultaneously.
8 CONCLUSION
In this paper, we propose a CONST network for indoor temperature
prediction for central heating systems. Our approach can directly
improve the interpretability of deep learning models without re-
lying on physical rule-based models and simple machine learning
models. CONST solves multicollinearity through differential oper-
ation and characterizes temporal and spatial consistency to learn
universal patterns from heterogeneous data distribution by multi-
task learning. The experimental results on a real-world dataset with
four heating stations demonstrate that our model can significantly
improve interpretability by more than 8 times on 𝑐𝑅𝑃𝐷 while main-
taining a high prediction accuracy. In addition, we have deployed
CONST on the SmartHeat system in northern China, providing
fine-grained forecasts every hour for 13 heating stations.
ACKNOWLEDGMENTS
This work is supported by the National Natural Science Foundation
of China (62076191, 62172034, 72242106), the Youth Fund Project
of Humanities and Social Science Research of Ministry of Educa-
tion (No. 21YJCZH045), the Natural Science Foundation Project of
Sichuan Province (No. 24NSFSC0508) and Beijing Nova program
(Z211100002121119).
5598KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
REFERENCES
[1]Zakia Afroz, Tania Urmee, GM Shafiullah, and Gary Higgins. 2018. Real-time
prediction model for indoor temperature in a commercial building. Applied energy
231 (2018), 29–53.
[2]Aylin Alin. 2010. Multicollinearity. Wiley interdisciplinary reviews: computational
statistics 2, 3 (2010), 370–374.
[3]Thomas Berthou, Pascal Stabat, Raphael Salvazet, and Dominique Marchio. 2014.
Development and validation of a gray box model to predict thermal behavior of
occupied office buildings. Energy and Buildings 74 (2014), 91–100.
[4]Yuntian Chen and Dongxiao Zhang. 2021. Theory-guided deep-learning for elec-
trical load forecasting (TgDLF) via ensemble long short-term memory. Advances
in Applied Energy 1 (2021), 100004.
[5]Drury B Crawley, Linda K Lawrie, Curtis O Pedersen, and Frederick C Winkel-
mann. 2000. Energy plus: energy simulation program. ASHRAE journal 42, 4
(2000), 49–56.
[6]Benoit Delcroix, Jérôme Le Ny, Michel Bernier, Muhammad Azam, Bingrui Qu,
and Jean-Simon Venne. 2021. Autoregressive neural networks with exogenous
variables for indoor temperature prediction in buildings. In Building Simulation,
Vol. 14. Springer, 165–178.
[7]Benoit Delcroix, Jérôme Le Ny, Michel Bernier, Muhammad Azam, Bingrui Qu,
and Jean-Simon Venne. 2021. Autoregressive neural networks with exogenous
variables for indoor temperature prediction in buildings. In Building Simulation,
Vol. 14. Springer, 165–178.
[8]Zhewen Duan, Xiuwen Yi, Peng Li, Dekang Qi, Yexin Li, Haoran Xu, Yanyong
Huang, Junbo Zhang, and Yu Zheng. 2022. Multi-memory Enhanced Separa-
tion Network for Indoor Temperature Prediction. In International Conference on
Database Systems for Advanced Applications. Springer, 656–663.
[9]Furkan Elmaz, Reinout Eyckerman, Wim Casteels, Steven Latré, and Peter
Hellinckx. 2021. CNN-LSTM architecture for predictive indoor temperature
modeling. Building and Environment 206 (2021), 108327.
[10] Zhen Fang, Nicolas Crimier, Lisa Scanu, Alphanie Midelet, Amr Alyafi, and Benoit
Delinchant. 2021. Multi-zone indoor temperature prediction with LSTM-based
sequence to sequence model. Energy and Buildings 245 (2021), 111053.
[11] MM Gouda, Sean Danaher, and CP Underwood. 2000. Low-order model for the
simulation of a building and its heating system. Building Services Engineering
Research and Technology 21, 3 (2000), 199–208.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[13] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[14] Ying Ji, Peng Xu, Pengfei Duan, and Xing Lu. 2016. Estimating hourly cooling
load in commercial buildings using a thermal network model and electricity
submetering data. Applied Energy 169 (2016), 309–323.
[15] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Rein-
forcement learning: A survey. Journal of artificial intelligence research 4 (1996),
237–285.
[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[17] Jan F Kreider. 2000. Handbook of heating, ventilation, and air conditioning. CRC
press.
[18] Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun,
and Grace Lai-Hung Wong. 2020. Efficient shapelet discovery for time series
classification. IEEE transactions on knowledge and data engineering 34, 3 (2020),
1149–1163.
[19] John H Lienhard. 2005. A heat transfer textbook. Phlogistron.
[20] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary
Transformers: Exploring the Stationarity in Time Series Forecasting. (2022).[21] Ye Liu, Yu Zheng, Yuxuan Liang, Shuming Liu, and David S Rosenblum. 2016.
Urban water quality prediction based on multi-task multi-view learning. In
Proceedings of the 25th international joint conference on artificial intelligence.
[22] Tao Lu and Martti Viljanen. 2009. Prediction of indoor temperature and relative
humidity using neural network models: model comparison. Neural Computing
and Applications 18 (2009), 345–357.
[23] A Mechaqrane and Mohcine Zouak. 2004. A comparison of linear and neural
network ARX models applied to a prediction of the indoor temperature of a
building. Neural Computing & Applications 13 (2004), 32–37.
[24] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.
[25] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and
Wang-chun Woo. 2015. Convolutional LSTM network: A machine learning
approach for precipitation nowcasting. Advances in neural information processing
systems 28 (2015).
[26] Robert H Shumway, David S Stoffer, Robert H Shumway, and David S Stoffer.
2017. ARIMA models. Time series analysis and its applications: with R examples
(2017), 75–163.
[27] Senzhang Wang, Jiannong Cao, and Philip Yu. 2020. Deep learning for spatio-
temporal data mining: A survey. IEEE transactions on knowledge and data engi-
neering (2020).
[28] Frederick C Winkelmann and Stephen Selkowitz. 1985. Daylighting simulation
in the DOE-2 building energy analysis program. Energy and Buildings 8, 4 (1985),
271–286.
[29] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. In International Conference on Learning Representations.
[30] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. 2023. Interpretable
weather forecasting for worldwide stations with a unified deep model. Nature
Machine Intelligence (2023), 1–10.
[31] Ziyu Xiang, Mingzhou Fan, Guillermo Vázquez Tovar, William Trehern, Byung-
Jun Yoon, Xiaofeng Qian, Raymundo Arroyave, and Xiaoning Qian. 2021. Physics-
constrained automatic feature engineering for predictive modeling in materials
science. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
10414–10421.
[32] Chengliang Xu, Huanxin Chen, Jiangyu Wang, Yabin Guo, and Yue Yuan. 2019.
Improving prediction performance for indoor temperature in public buildings
based on a novel deep learning method. Building and Environment 148 (2019),
128–135.
[33] Zhaohui Liu Yi Li, Yiwen Jian. 2015. Climate compensator adjust and impact
analysis of the simulation. In Proceedings of the Symposium on Heating Engineering
Construction and Efficient Operation. Gas& Heat, 195–200.
[34] Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep spatio-temporal residual
networks for citywide crowd flows prediction. In Thirty-first AAAI conference on
artificial intelligence.
[35] Junbo Zhang, Yu Zheng, Junkai Sun, and Dekang Qi. 2019. Flow prediction in
spatio-temporal networks based on multitask deep learning. IEEE Transactions
on Knowledge and Data Engineering 32, 3 (2019), 468–478.
[36] Xianli Zhang, Buyue Qian, Shilei Cao, Yang Li, Hang Chen, Yefeng Zheng, and
Ian Davidson. 2020. INPREM: An interpretable and trustworthy predictive model
for healthcare. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. 450–460.
[37] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-
tions on Knowledge and Data Engineering (2021).
[38] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Se-
quence Time-Series Forecasting. In The Thirty-Fifth AAAI Conference on Artificial
Intelligence, AAAI 2021, Virtual Conference, Vol. 35. AAAI Press, 11106–11115.
[39] Pinghua Zou. 2018. Heating engineering. China Architecture & Building Press.
5599Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
A APPENDICES
A.1 Terminology
Time perspective. In time series data, we use 𝑥to represent a
feature and𝑦to represent the label. As shown in Figure 9(a), taking
time as the horizontal axis and displaying 𝑥and𝑦data simultane-
ously in the figure is the time perspective. In this perspective, the
main focus is on the patterns of data changes over time, such as
temporal closeness, period, and trend [ 34]. These patterns should
be classified as indirect effects. The typical approach is to refer to
the data near (usually within a few moments) the current moment
as short-term data; Data that is far from (usually within dozens of
moments) the current moment is referred to as long-term data.
Feature perspective. As shown in Figure 9(b), using indepen-
dent variable 𝑥as the horizontal axis and dependent variable 𝑦as
the vertical axis represents the feature perspective. In this perspec-
tive, the main focus is on the laws of 𝑦changing with 𝑥, which
should be classified as direct effects. Obviously, it is easier to obtain
interpretability from direct effects.
Global and local in temporal dimension. In the temporal
dimension, from the feature perspective, it can be divided into
global data distribution and local data distribution. Local refers to
data near the current moment, which is similar to the short-term
comparison in the time perspective. Global refers to the entire data
set, which may involve data with very large time intervals. It is
similar to the ultra-long-term data from the time perspective.
Global and local in spatial dimension. In the spatial dimen-
sion, the distribution of data at multiple spatial locations is regarded
as global data distribution; the distribution of data at a single loca-
tion is regarded as local data distribution.
t xy
(a) Time perspectivey
(b) Feature perspectivex
Figure 9: Different perspectives in the temporal dimension.
A.2 Another perspective on related work
Related works can be divided into three categories, and their re-
lationships are shown in Figure 10. We use 𝑓′()represents the
constructed model and 𝑌′represents the predicted output. The
black box model mainly focuses on 𝑌′, and does not care about the
exact form of 𝑓′(). The deep learning based model [ 6,22,23,32]
fall into this category, they usually have high prediction accuracy
but low interpretability. The white-box model mainly focuses on
𝑓′()and is interested in understanding the way that 𝑌is affected
as𝑥1,...,𝑥𝑝change. Physical rule-based models [ 5,28] and simple
machine learning models (such as linear regression, decision rules)
[24] both fall into this category, which generally gain interpretabil-
ity by simplifying the problem. They are more interpretable, but
difficult to apply or have lower accuracy. The gray-box model try to
trade off accuracy and interpretability [ 4,14,31,36]. They typicallyrely on the white box model to obtain interpretability. The disad-
vantages of the white box model are also inherited. Our model is
significantly different from them. It does not rely on the white-box
model (such as linear model, physical model, etc., which obtain
interpretability by restricting the complexity of the model), and
directly making the deep learning model more interpretable.
Interpretability
AccuracyIdeal model
CONST (our model )
Black-box model s
Gray-box model s
White-box model s
Dependency relation
Figure 10: Schematic diagram of method classification.
A.3 Data Fitting Physical Model for Indoor
Temperature Prediction
The initial value setting of the nonlinear parameter estimation
has a great influence on the performance. The method in book
[39] can reduce a large number of parameters by comparing the
design state with the actual state, which simplifies the setting of
the initial value. In the case of only considering the main features
(heating temperature, outdoor temperature), we derived a nonlinear
model through physical laws, and fitted the nonlinear parameters
through real data to obtain the data fitting physical model for indoor
temperature prediction.
A.3.1 Model Derivation. Equation 19∼22 are similar to Equa-
tion 12-1, 12-2, 12-5, and 12-4 in the book [39]:
𝑄1=𝑄2=𝑄3 (19)
𝑄1=𝑞𝑣𝑉(𝑡𝑖−𝑡𝑜) (20)
𝑄2=𝑎𝐹𝑡ℎ+𝑡𝑟
2−𝑡𝑖1+𝑏
(21)
𝑄3=𝐺𝑐(𝑡ℎ−𝑡𝑟) (22)
Among them, 𝑄1,𝑄2,𝑄3are respectively the heat load of the build-
ing, the heat dissipation of the heat dissipation equipment, and
the heat transported by the heating pipe network. 𝑞𝑣is the heat-
ing volume heat index of the building. 𝑉is the external volume of
the building. 𝑡𝑜is the outdoor temperature; 𝑡ℎis the supply water
temperature, and 𝑡𝑟is the return water temperature. a and b are
related parameters of heat transfer coefficient of radiator. 𝐹is the
cooling area of the radiator. 𝐺is water flow. 𝑐is the specific heat
capacity of water. The difference between Equations 20 and 21 in
this paper and Equations 12-2 and 12-5 in the book [ 39] is that
we use the actual indoor temperature 𝑡𝑖instead of the calculating
indoor temperature 𝑡′
𝑖.
For design conditions, Equation 23 ∼26 are the same as Equation
12-6∼12-9 in book [ 39], where the superscript represents the design
value:
𝑄′
1=𝑄′
2=𝑄′
3(23)
𝑄′
1=𝑞𝑣𝑉(𝑡′
𝑖−𝑡′
𝑜)++ (24)
5600KDD ’24, August 25–29, 2024, Barcelona, Spain Dekang Qi et al.
𝑄′
2=𝑎𝐹 
𝑡′
ℎ+𝑡′
𝑟
2−𝑡′
𝑖!1+𝑏
(25)
𝑄′
3=𝐺′𝑐(𝑡′
ℎ−𝑡′
𝑟) (26)
The same as the Equation 12-12 in the book [ 39], comparing
the two sets of equations of the actual situation and the design
situation, we can get Equation 27:
𝑡𝑖−𝑡𝑜
𝑡′
𝑖−𝑡′
𝑜=𝑡ℎ+𝑡𝑟
2−𝑡𝑖1+𝑏

𝑡′
ℎ+𝑡′
𝑟
2−𝑡′
𝑖1+𝑏=¯𝐺𝑡ℎ−𝑡𝑟
𝑡′
ℎ−𝑡′
𝑟(27)
Among them, ¯𝐺is the ratio of actual flow to design flow. Equation
28 can be deduced from the second and third terms in Equation 27:
𝑡𝑖=𝑡ℎ+𝑡𝑟
2−¯𝐺
𝑡′
ℎ+𝑡′
𝑟
2−𝑡′
𝑖1+𝑏
𝑡′
ℎ−𝑡′
𝑟(𝑡ℎ−𝑡𝑟)1
1+𝑏
(28)
Adding correction coefficients 𝜆1,𝜇1,𝜖1, we can be obtained
method 1 as Equation 29:
𝑡1
𝑖=𝜆1𝑡ℎ+𝑡𝑟
2−𝜇1¯𝐺
𝑡′
ℎ+𝑡′
𝑟
2−𝑡′
𝑖1+𝑏
𝑡′
ℎ−𝑡′
𝑟(𝑡ℎ−𝑡𝑟)1
1+𝑏
+𝜖1(29)
The multicollinearity between heating temperature and return
temperature is too strong, which makes it easy to get un trustwor-
thy coefficients when fitting parameters. The return temperature
changes with the heating temperature, and the following linear re-
gression can be used to fit the return temperature through the heat
temperature. As shown in Equation 30, using the linear regression
without intercept can avoid the situation of 𝑡ℎ−𝑡𝑟<0.
𝑡𝑟=𝜌×𝑡ℎ (30)
Therefore, Equation 29 is revised to Equation: 31
𝑡1
𝑖=1+𝜌
2𝜆1𝑡ℎ−𝜇1¯𝐺
𝑡′
ℎ+𝑡′
𝑟
2−𝑡′
𝑖1+𝑏
𝑡′
ℎ−𝑡′
𝑟(1−𝜌)𝑡ℎ1
1+𝑏
+𝜖1(31)Equation 32 can be deduced from the first and third terms in the
Equation 27:
𝑡𝑖=¯𝐺𝑡′
𝑖−𝑡′
𝑜
𝑡′
ℎ−𝑡′
𝑟(𝑡ℎ−𝑡𝑟)+𝑡𝑜 (32)
Adding correction coefficients 𝜆2,𝜇2,𝜖2, we can be obtained method
2 as Equation 33:
𝑡2
𝑖=𝜆2¯𝐺𝑡′
𝑖−𝑡′
𝑜
𝑡′
ℎ−𝑡′
𝑟(𝑡ℎ−𝑡𝑟)+𝜇2𝑡𝑜+𝜖2 (33)
According to Equation 30, Equation 33 is revised to Equation 34
𝑡2
𝑖=𝜆2¯𝐺𝑡′
𝑖−𝑡′
𝑜
𝑡′
ℎ−𝑡′
𝑟(1−𝜌)𝑡ℎ+𝜇2𝑡𝑜+𝜖2 (34)
Combining the method 1 and the method 2, the physical model can
be obtained as Equation 35:
𝑡𝑖=𝑤×𝑡1
𝑖+(1−𝑤)×𝑡2
𝑖(35)
A.3.2 Parameter setting and data fitting.
Parameter settings: According to book [ 39] and article [ 33], the
parameters are set as follows: 𝑡′
𝑖=18,𝑡′
𝑜=−7,𝑡′
ℎ=75,𝑡′
𝑟=50,𝑏=
0.16,¯𝐺=1.
Data Fitting: The derived physical model is nonlinear. We firstly
fit the value of k through linear fitting, and then we use the lsqcurve-
fit method in Matlab to perform nonlinear fitting based on real data.
We set the initial values of 𝜆1,𝜇1,𝜆2, and𝜇2to 1, and the value
range is greater than or equal to 0. The initial values of 𝜖1and𝜖2
are set to 0, and the range is not limited. The initial value of w is
set to 0.5, and the range is set to [0,1].
Fitting parameter results: The final fitting parameter results
are shown in the table 7.
Table 7: Parameters of physical model by nonlinear fitting
Station𝜆1𝜇1𝜖1𝜆2𝜇2𝜖2𝜌 𝑤
A 0.270 1.056 16.62 6.548 0.551 3.423 0.929 0.757
B 0.678 1.013 11.56 0.677 0.561 6.078 0.868 0.565
C 0.448 1.168 85.22 5.070 0.513 -198.3 0.831 0.757
D 0.406 1.031 34.97 4.678 0.337 -35.29 0.839 0.673
5601