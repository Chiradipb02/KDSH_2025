Graph Anomaly Detection with Few Labels:
A Data-Centric Approach
Xiaoxiao Ma
School of Computing
Macquarie University
Sydney, NSW, Australia
xiaoxiao.ma2@hdr.mq.edu.auRuikun Li
Business School
The University of Sydney
Sydney, NSW, Australia
ruikun.li@sydney.edu.auFanzhen Liu
School of Computing
Macquarie University
Sydney, NSW, Australia
fanzhen.liu@hdr.mq.edu.au
Kaize Ding
Department of Statistics and Data
Science, Northwestern University
Evanston, IL, USA
kaize.ding@northwestern.eduJian Yang
School of Computing
Macquarie University
Sydney, NSW, Australia
jian.yang@mq.edu.auJia Wu
School of Computing
Macquarie University
Sydney, NSW, Australia
jia.wu@mq.edu.au
ABSTRACT
Anomalous node detection in a static graph faces significant chal-
lenges due to the rarity of anomalies and the substantial cost of label-
ing their deviant structure and attribute patterns. These challenges
give rise to data-centric problems, including extremely imbalanced
data distributions and intricate graph learning, which significantly
impede machine learning and deep learning methods from discern-
ing the patterns of graph anomalies with few labels. While these
issues remain crucial, much of the current research focuses on ad-
dressing the induced technical challenges, treating the shortage of
labeled data as a given. Distinct from previous efforts, this work fo-
cuses on tackling the data-centric problems by generating auxiliary
training nodes that conform to the original graph topology and
attribute distribution. We categorize this approach as ‘data-centric, ’
aiming to enhance existing anomaly detectors by training them on
our synthetic data. However, the methods for generating nodes and
the effectiveness of utilizing synthetic data for graph anomaly de-
tection remain unexplored in the realm. To answer these questions,
we thoroughly investigate the denoising diffusion model. Drawing
from our observations on the diffusion process, we illuminate the
shifts in graph energy distribution and establish two principles
for designing denoising neural networks tailored to graph anom-
aly generation. From the insights, we propose a diffusion-based
graph generation method to synthesize training nodes, which can
be promptly integrated to work with existing anomaly detectors.
The empirical results on eight widely-used datasets demonstrate
our generated data can effectively enhance the nine state-of-the-art
graph detectors’ performance.
CCS CONCEPTS
•Computing methodologies →Machine learning.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671929KEYWORDS
Graph Anomaly Detection, Generative Graph Diffusion
ACM Reference Format:
Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, and Jia Wu.
2024. Graph Anomaly Detection with Few Labels: A Data-Centric Approach.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671929
1 INTRODUCTION
A primary goal of graph anomaly detection is to identify irregular
nodes in a static graph that deviate significantly from the majori-
ties. These anomalies are far less common than regular nodes (as
indicated by the anomaly ratio in Table 2), but detecting them1
is even crucial for social benefits, such as detecting frauds in on-
line social networks [ 7,9,47], fake news in social media [ 52], rare
molecules for drug discovery [ 33], malware in computing systems,
brain health monitoring [49] and others [28].
Suffering from the rareness of anomalies and the substantial cost
of labeling, numerous studies accepted these data-centric challenges
as the de facto settings in graph anomaly detection and pursued the
implementation of more sophisticated graph learning algorithms
to detect graph anomalies with few labels. These methods typically
engage with tech-specific challenges identified in [32] such as the
class-imbalanced learning and over-/under-fitting issues on a spe-
cific class [ 41] that drastically undermine the proposed models on
learning a generalized decision boundary between anomalies and
the majorities, similar to other learning tasks where label access is
insufficient [ 18,38]. However, to our knowledge, there is still no
promising research to explicitly address the data problems.
Intuitively, having access to more labeled anomalies substantially
aids graph anomaly detectors in distinguishing between anomalous
and normal nodes, and inspired by the remarkable advancements
in utilizing synthetic tabular data for training machine learning
models [ 13,30,39], we pivot our focus towards the generation of
training nodes that closely replicate the original graph distribution.
Distinct from previous Model-Centric strategies that focus on
classifying anomalies, our approach is Data-Centric because we
1We use the terms ‘graph anomalies’ and ‘anomalous nodes’, as well as ‘node features’ and ‘node
attributes’ interchangeably.
 
2153
KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
Performance gain on Macro-F1GraphSAGEGCNGATBWGNNAMNetGat-sepGHRNPerformance gain on AUCOriginal F1Improved F1GraphSAGEGCNGATBWGNNAMNetGat-sepGHRNOriginal AUCImproved AUC506070607080
Figure 1: Seven semi-/supervised graph anomaly detectors’
performance has been significantly improved on YelpChi
after training with our synthetic data.
prioritize the generation and utilization of synthetic data to address
the data scarcity challenge. However, this approach naturally in-
curs two fundamental concerns: First, the challenge of ensuring
the synthetic data closely replicates the intricate characteristics
of graph data; and Second, whether the synthetic data can benefit
graph anomaly detection.
To address these concerns, we delve into the denoising diffusion
models [ 16,34], which have shown promising results in synthesiz-
ing high quality images/videos, to engage specific clues in generat-
ing training data. By inspecting the changes of the graph’s spec-
trum (as depicted in Figs. 2 and 3), we observe a progressive shift in
the graph’s spectral energy distribution from lower to higher fre-
quencies as the diffusion forwards, and the average node attribute
similarities with 1-hop neighbors (which we call egonet dissimilar-
ity) change more dramatically among anomalies. Through rigorous
analysis on these observations, we identify two principles that the
denoising neural network should be capable of: P1. Preserving
each node’s feature dissimilarities with its neighbors (capturing
the local graph semantics); and P2. Recovering the low frequency
graph energy. We follow the principles and propose a uncondi-
tional denoising-based graph diffusion model, namely GenGA and
a classifier guided generation model, CGenGA, to generate data
tailored to graph anomaly detection. The results in Fig. 1 showcase
the significant performance gains of seven anomaly detectors w.r.t.
Macro-F1 and AUC after training with our synthesized data. The
main contributions of this paper are:
•A Data-Centric Approach for Graph Anomaly Detection :
We pivot the focus from solely improving detection models to
leveraging synthetic data to augment existing anomaly detectors.
This data-centric approach is designed to complement existing
detectors, offering the advantage of generating reusable data that
can seamlessly integrate with current methodologies.
•Two Design Principles : Our empirical analysis, particularly on
the changes in the graph spectrum, has led to the formulation
of two core principles for designing our denoising neural net-
work. These principles guide the generation of synthetic training
samples that accurately mirror the original data distribution.
•Empirical Results : The extensive experiments on eight datasets
demonstrate that our approach significantly enhances the perfor-
mance of nine state-of-the-art anomaly detection models. Further
investigations into seven research questions provide deeper in-
sight to our design and identify promising avenues for future
research in this direction.
0.000.120.240.360.000.120.240.36
0.00.20.40.60.81.01.21.41.61.82.00.000.030.060.080.00.20.40.60.81.01.21.41.61.82.00.000.030.060.08 
  
  
  
 
<latexit sha1_base64="l6v7b8WEz3KlQh/xTtq1s64OQWE=">AAAB7nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLoxmUF+4A2lJvJpB06mYSZiVBCP8KNC0Xc+j3u/BunbRbaemDgcM65zL0nSAXXxnW/ndLa+sbmVnm7srO7t39QPTxq6yRTlLVoIhLVDVAzwSVrGW4E66aKYRwI1gnGdzO/88SU5ol8NJOU+TEOJY84RWOlTl/YaIiDas2tu3OQVeIVpAYFmoPqVz9MaBYzaahArXuemxo/R2U4FWxa6WeapUjHOGQ9SyXGTPv5fN0pObNKSKJE2ScNmau/J3KMtZ7EgU3GaEZ62ZuJ/3m9zEQ3fs5lmhkm6eKjKBPEJGR2Owm5YtSIiSVIFbe7EjpChdTYhiq2BG/55FXSvqh7V/XLh8ta47aoowwncArn4ME1NOAemtACCmN4hld4c1LnxXl3PhbRklPMHMMfOJ8/P7OPhg==</latexit> EigenvaluesDiffusion steps(%)step 0%step 25%step 50%step 100%LowfrequenciesHighfrequenciesEgonet dissimilarity
LinearschedulerCosineschedulerSpectral energy distributionAnomaliesNormal nodes
(a)(b)0204060801001234
0204060801001234Figure 2: On Cora dataset, by gradually injecting scheduled
noise to node attributes as the forward diffusion process. (a).
Anomalies’ egonet dissimilarities change more dramatically
than normal nodes. (b). The ratios of lower frequency energy
drop while higher frequencies increase (shifting from lower
to higher frequencies). Details are presented in Sec. 4.
2 RELATED WORK
2.1 Graph Anomaly Detection
In this work, the target of graph anomaly detection is to iden-
tify the rare and divergent nodes that exhibit different structural
and attribute patterns [ 2,10,32] with few labels. Under the semi-
/supervised learning setting, anomalous node detection can be trans-
formed as a binary classification task [ 42] that encounters extremely
class-imbalanced data. As to explore the graph structure, node at-
tributes and fuse abnormal patterns of anomalies from labeled data,
various graph neural network (GNN) based semi-/supervised detec-
tors have been devised [ 26,27,42]. They attempt to directly assign
labels to nodes by learning a decision boundary between anomalies
and normal nodes. For instance, BWGNN [ 42] employs the Beta
graph wavelet and utilizes few labeled anomalies to learn band-pass
filters to capture the signals of anomalies, GHRN [ 8] further applies
the graph wavelet to address the heterophility of graph.
Apart from these semi-/supervised detectors, there are plenty of
unsupervised anomaly detection techniques, such as CONAD [ 28]
and SL-GAD [ 55] that investigate the consistency between anom-
alies and their neighbors in different contrastive views to measure
node irregularity. Others like [ 6], [25] and [ 55] quantify the residual
errors as a measurement of anomaly scores. Although these unsu-
pervised methods explore different perspectives on the deviating
patterns of anomalies without utilizing labels for training, they still
rely on labeled data for hyperparameter tuning and to determine
a reasonable threshold to classify anomalies from the continuous
anomaly scores/ranks [1, 32].
We further compare our work with previous graph anomaly de-
tection methods in Table 1 and we show that all the prior method-
ologies focus on tackling the detection problems introduced by the
data, which we categorize as model-centric, while synthesizing la-
beled training data is still an unexplored territory. It is worth noting
that our data-centric approach can be categorized as a specific data
augmentation method, but different from conventional augmenta-
tion methods that focus on enhancing node features, contrastive
views or egonet structures [ 35,54] for specific methods [ 28,55], we
 
2154Graph Anomaly Detection with Few Labels:
A Data-Centric Approach KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparison with existing works in anomalous node detection.
Metho
ds Examples Category Types Synthetic Data Label Supervision Working Mechanism Output Decision
V
anilla Semi-/Supervised MP-GNNs [12, 24, 44] MC - CLF & HPT Node Classification Labels
Semi-/Supervised Detectors [4, 8, 9, 29, 29, 42, 45] MC - CLF & HPT Node Classification Labels/Score
Unsupervised Detectors [3, 6, 36] MC - HPT Node Scoring/Ranking Scores
Unsupervised-Contrastive Detectors [28, 50, 55] MC Subgraph View Sampling HPT Node Scoring/Ranking Scores
Graph Augmentation Methods [35, 54] MC Enhanced Egonet HPT Modify Existing Methods -
Generativ
e Method (Ours) DC Graphs mimicking the raw data CLF & HPT Enhancing Existing Detectors Synthesized Data and Labels
Notes: DCis
short for Data-Centric. MCis short for Model-Centric. CLF is short for classification. HPT is short for hyperparamter tunning.
directly solve the label scarcity problem by generating more train-
ing data that can be promptly applied to enhance existing detectors
without modifying their designs.
2.2 Denoising Diffusion Probabilistic Model
Denoising diffusion probabilistic models (DDPMs) have recently
shown their great power in image synthesis [ 16,34], time series
forecasting [ 31], and many other generative tasks [ 51]. Practically, a
DDPM contains two processes, namely the Forward diffusion process
(ordiffusion process for brevity), which gradually adds scheduled
Gaussian noise to the original data 𝒙0through a T-step2Markov
chain such that the eventual distribution at the last step 𝑞(𝒙𝑇|𝒙0)∼
N(0,I), and the Reverse process (ordenoising process), which strives
to recover the data by removing the noise at each time step.
In the forward diffusion process, given the noise variance sched-
ule𝛽𝑡∈(0,1), the noisy data at a particular step tcan be formulated
in closed form:
𝑞(𝒙𝑡|𝒙0)=𝑡Ö
𝑠=1𝑞(𝒙𝑠|𝒙𝑠−1)=N(𝒙𝑡;√¯𝛼𝑡𝒙0,(1−¯𝛼𝑡)I),
𝒙𝑡=√¯𝛼𝑡𝒙0+√︁
(1−¯𝛼𝑡)I,(1)
where ¯𝛼𝑡=Î𝑡
𝑠=1𝛼𝑠and𝛼𝑡=1−𝛽𝑡. The contrast reverse process
attempts to recover the original data from noise following
𝑝𝜽(𝒙0:𝑇):=𝑝(𝒙𝑇)𝑇Ö
𝑡=1𝑝𝜽(𝒙𝑡−1|𝒙𝑡),
𝑝𝜽(𝒙𝑡−1|𝒙𝑡):=N 𝒙𝑡−1;𝝁𝜽(𝒙𝑡,𝑡),Σ𝜽(𝒙𝑡,𝑡),(2)
where the mean and variance (i.e., 𝝁𝜽and Σ𝜽) of the data dis-
tribution𝑝𝜽(𝒙𝑡−1|𝒙𝑡)are predicted using a deep neural network
with parameters 𝜽(also called denoising neural network), which is
typically fine-tuned with regard to the objective
arg min
𝜽E𝒙0,𝜖
||𝜖−𝜖𝜽(√¯𝛼𝑡𝒙0+√1−¯𝛼𝑡𝜖,𝑡)||2
2
, (3)
where𝜖is Gaussian noise.
After training, the denoising neural network will capture the
original data distribution 𝑝(𝑥). A new sample, represented as 𝒙𝑎
0,
can then be firmly generated by sampling 𝒙𝑇∼N( 0,I)and gradu-
ally reversing the diffusion process according to Eq. (2).
To date, although DDPMs have shown superior capability in
various data generation tasks, the progress on denoising graph
diffusion still mainly focuses on modeling the graph-level patterns
within a set of graphs [ 17,19,20]. Capturing the node attribute and
local structure distribution of a single graph, especially for graph
2The italic 𝑇specifically denotes noise scales while the superscript ‘ ⊤’ stands for the
transpose of a matrix.anomaly detection has yet to be explored. We refer to literature
such as [16, 31, 34, 51] for more details about DDPMs.
3 PRELIMINARIES
Definition 1. Static Attributed Graph. A static attributed
graphG={A,X}comprises𝑛nodes with attributes. Ais the adja-
cency matrix without self-loop, Xis the node attribute matrix.
Definition 2. Egonet Dissimilarity3.The egonet dissimilarity
Ω=[𝝎𝑖]𝑛×𝑘=LXquantifies how each node 𝑣𝑖’s attributes are
different from its egonet neighbors in N(𝑖).L=I−D-1
2AD-1
2is the
normalized graph Laplacian corresponding to GandDis the degree
matrix.
Definition 3. Forward Graph Diffusion (Process). In this
paper, we refer the forward graph diffusion as to inject 𝑇scales of
scheduled noise to node attributes. For each diffusion step {𝑡}𝑇
0,G𝑡+1=
{A,X𝑡+1}is the corrupted graph from G𝑡={A,X𝑡}, withG0=G.
Definition 4. Graph Anomaly Detection with Few Labels.
In this paper, we define graph anomaly detection as to identify anoma-
lous nodes in a static attributed graph G, which contains both anom-
alies in V1, and normal nodes in V0. It is to learn a classification func-
tion that maps each node 𝑣𝑖∈V,V=V1∪V0to its class label, which is
1for anomalies and 0for normal nodes, i.e., 𝑓:{A,X}→𝒚∈{0,1}𝑛.
In the context of few labels, anomalies are far rarer than normal nodes,
which means the cardinalities |V1|≪|V0|.
4 DESIGN MOTIVATION AND ANALYSIS
In this section, we delve deeper into the forward graph diffusion
process and uncover two principles for designing the denoising
neural network based on our empirical observations and rigorous
analysis. We start by showing the divergent evolving dynamics of
anomalies and normal nodes w.r.t. egonet dissimilarity through-
out the diffusion process, followed by the analysis on the graph’s
spectral energy distribution.
4.1 Study Setup
For analytical purposes, we choose the representative dataset Cora
and gradually inject noise into the node attributes. Specifically,
we apply the mostly-used linear and cosine schedulers [ 34,40] for
sampling noise and following [ 42], the node attributes are randomly
drawn fromN(1,1)for the normal class and N(1,5)for anomalies.
3To eliminate confusion, we use ‘egonet dissimilarity’ to specifically denote how each
node attributes differ from its egonet neighbors, distinct from the embedding method
used in previous works.
 
2155KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
4.2 Divergent Evolving Patterns in Anomalies’
Egonet Similarities
When measuring the average egonet dissimilarity of anomalies and
normal nodes at each forward step, according to our Definition 2,
we witness that anomalies (the redline) undergo a different trend
of changes compared to normal nodes (the blue line), as depicted
prior in Fig. 2(a). Such an observation on the differences between
anomalies and normal nodes indicates that the attribute similarities
between neighbors change along with the diffusion process, hence,
the denoising neural network should capture the local graph se-
mantics such that the attribute dis-/similarity information of each
node can be preserved in the generated graphs.
4.3 The Shift of Graph Spectral Energy
Apart from the egonet dissimilarity, we take another view on the
forward diffusion process via the lens of the graph spectrum and we
observe that the accumulated graph energy is shifting from lower
frequencies to higher frequencies, as depicted in Fig 2(b) and Fig 3.
Empirically, we can quantify the energy ratio of a particular
frequency (rank 𝑙) at diffusion step 𝑡as
𝛾𝑙(𝒙𝑡,L)=(ˆ𝑥𝑡
𝑙)2/𝑛∑︁
𝑖=1(ˆ𝑥𝑡
𝑖)2, (4)
where ˆ𝒙𝑡=U⊤𝒙𝑡=(ˆ𝑥𝑡
1,..., ˆ𝑥𝑡
𝑙,..., ˆ𝑥𝑡𝑛)⊤is the graph Fourier trans-
formed signal at step 𝑡.U=(𝒖1,..., 𝒖𝑛)⊤are the eigenvectors of
the graph Laplacian L, in which 𝒖𝑙is the eigenvector corresponding
to the𝑙-th smallest eigenvalue. By setting 𝑙as a threshold, we clas-
sify signals(ˆ𝑥𝑡
1,..., ˆ𝑥𝑡
𝑙)as low frequency signals and the remaining
are considered high frequency ones.
As depicted in Fig. 2(b), the ratios of low frequency signals are
gradually decreasing while higher frequencies are becoming more
significant along the forward diffusion process. To clearly identify
such changes and following [ 42], at each step 𝑡, we further quantify
the accumulated energy ratios of low and high frequency signals
with regard to different thresholds. For a specific threshold 𝑙, the
corresponding accumulated energy ratio is measured as:
Γ𝑙(𝒙𝑡,L)=𝑙∑︁
𝑖=1𝛾𝑖(𝒙𝑡,L). (5)
We find that the accumulated energy is shifting to higher fre-
quencies (the green area moves upward in Fig. 3), and a concrete
example is the accumulated energy ratio with eigenvalues below
0.3(at the top). As the diffusion forwards, Γ𝑙=0.3(𝒙𝑡,L)decreases
continuously. From the results, we have the following expectation
regarding Γ𝑙(𝒙𝑡,L).
Proposition 1. The expectation of low frequency energy ratio
E𝒙∼N(| 𝜇|,𝜎2)[Γ𝑙(𝒙𝑡,L)]is monotonically decreasing during the for-
ward diffusion process.
Proof. For analysis, we follow [ 11] and assume all node at-
tributes are drawn i.i.d. from a Gaussian distribution N(𝒙;|𝜇|,𝜎2),
and the coefficient of variance 𝜎/|𝜇|measures the degree of anom-
alies in the graph signal 𝒙. As the forward diffusion destroys the
original graph signal 𝒙and𝒙𝑡∼N(√¯𝛼𝑡𝒙,1−¯𝛼𝑡), we can measure
20 40 60 80 1000.00.51.01.52.0
StepsEigenvalue
0.0000.1250.2500.3750.5000.6250.7500.8751.000Accumulated
energy ratio
0.320 40 60 80 1000.10.20.30.40.5
Steps Eigenvalue ≤ 0.3Figure 3: Changes of Γin the diffusion process.
the signal-to-noise ratio at an arbitrary step 𝑡by
SNR(𝑡)=𝜇2
𝑡
𝜎2
𝑡=¯𝛼𝑡
1−¯𝛼𝑡, (6)
which denotes the ratio between the original signal and noise being
added. Therefore, a higher SNR means more original signal, while
a lower SNR indicates more noise. When injecting noise upon the
linear or cosine scheduler, ¯𝛼𝑡and the signal-to-noise ratio SNR
decrease progressively [ 31] as𝑡becomes larger. Consequently, the
measurement of the degree of anomalies, which can be quantified
with regard to SNR as:
𝜎
|𝜇|=√1−¯𝛼𝑡√¯𝛼𝑡=√︂1
SNR(𝑡), (7)
increases as the diffusion proceeds. According to [ 42]’s proof, the
expectation of low frequency energy ratio E(Γ𝑙(𝒙𝑡,L))will decrease
monotonically throughout the diffusion process because the value
of𝜎/|𝜇|increases. □
Such a shift indicates that the graph spectral energy weights less
on low frequency eigenvalues at step 𝑡than𝑡−1. Therefore, to obtain
𝒙𝑡−1by denoising 𝒙𝑡(formulated in Eq. (2)), the denoising network
needs to recover the lower frequency energy. To summarize, from
our analysis, we identify two principles for designing the denoising
networks for DDPM-based graph generation: P1.The denoising
neural network should be capable of capturing the local information
in egonets (the graph’s local semantics); and P2.The denoising neural
network needs to recover the low frequency energy.
5 METHODOLOGY
Following our devised design principles, in this section, we first
introduce our unconditional graph generation model GenGA, a
denoising-based neural network designed to generate auxiliary
training samples, i.e., nodes. Then, we propose CGenGA to further
involve classifier guidance for data generation. Finally, we present
how the generated graphs can be utilized to complement existing
detectors.
5.1 Denoising Neural Network Design
Regarding Definition 3 that we only inject noise to node attributes
during the diffusion process, one can apply any types of neural
 
2156Graph Anomaly Detection with Few Labels:
A Data-Centric Approach KDD ’24, August 25–29, 2024, Barcelona, Spain
networks as to denoise X𝑡, which is actually predicting the mean
value√¯𝛼𝑡−1Xand variance(1−¯𝛼𝑡−1)Ifrom graphG𝑡. However,
as advised by our design Principle P2, the choice of the denoising
network is limited due to the requirement of recovering the low
frequency energy.
As an effective low-pass filter, the graph convolutional neural
network (GCN) [ 24] is a strong candidate of the denoising network
because of its capacity in attenuating high frequency signals and
emphasizing lower frequencies [ 21]. Moreover, from the spatial
perspective, it also explores the local graph semantics through
feature propagation [ 48], which aligns with P1. Therefore, we apply
GCN as the backbone and design two ingredients, namely the step-
dependent GCN (Sdn) for learning node representations Z𝑡and the
distribution-estimator network (Den) for estimating the mean and
variance of X𝑡−1.
5.2 Step-dependent GCN - Sdn
We build Sdn upon [ 23] with the aim of learning node represen-
tations that conform to Gaussian distribution 𝑝(Z𝑡|X𝑡,A,𝑡) ∼
N(𝝁Sdn
𝑡,diag(ΣSdn
𝑡)),where matrices 𝝁Sdn
𝑡anddiag(ΣSdn
𝑡)con-
tain the mean and variance vector of each node’s representation
𝒛𝑖
𝑡at step𝑡. Since𝑡is a scalar and to further involve it for node
representation learning, we propose an encoding function Te(·)to
generate a specific embedding matrix for each step 𝑡by
Te(𝑡)=SeLUh
Se(𝑡)𝑾Te
1i
𝑾Te
2, (8)
where 𝑾Te
1and𝑾Te
2are trainable parameters for projection. Se(·)
is the sinusoidal function and the values in vector Se(𝑡)are

sin
𝑡·exp(5 log 10
𝑑/2−1·𝑑)
,cos
𝑡·exp(5 log 10
𝑑/2−1·𝑑)
.(9)
We then learn the node representations’ mean values 𝝁Sdn
𝑡and
variance ΣSdn
𝑡respectively following:
𝝁Sdn
𝑡=˜AZ𝑡𝑾Sdn
𝜇,log[diag(ΣSdn
𝑡)]=˜AZ𝑡𝑾Sdn
Σ,(10)
where
Z𝑡=ReLU[˜A(X𝑡+Te(𝑡))𝑾Sdn
𝑍] (11)
is the output of the first GCN layer in Sdn. ˜A=D−1
2AD−1
2is the
normalized adjacency matrix. Matrices 𝑾Sdn
𝑍,𝑾Sdn𝜇and𝑾Sdn
Σare
variables in the GCN layers. Consequently, the node representations
Z𝑡can be promptly obtained following the reparameterization trick
in [22] and [23].
5.3 Distribution Estimator - Den
Given the node representations Z𝑡, we then propose a distribution
estimator for predicting the mean and variance of X𝑡−1at the prior
diffusion step. Specifically, the predicted mean is
𝝁′
𝑡−1=˜A[ReLU(˜AZ𝑡𝑾Den
𝑍)⊕Z𝑡]𝑾Den
𝜇, (12)
where⊕is for concatenation. Notably, we take this residual infor-
mation from Sdn to relieve oversmoothing and further validate its
effectiveness through the ablation study in Sec. 6.4. Similarly, we
estimate the variance as:
log[diag(Σ′
𝑡−1)]=˜A[ReLU(˜AZ𝑡𝑾Den
𝑍)⊕Z𝑡]𝑾Den
Σ, (13)
where 𝑾Den
𝑍,𝑾Den𝜇 and𝑾Den
Σparameterize Den.Algorithm 1: Algorithms of GenGA and CGenGA.
Input: GraphG={A,X},𝒚,𝜆and diffusion parameters 𝛽,𝛼,¯𝛼.
Output: Generated graphG𝑎.
1while Not Converged do
/* Train the denoising neural network. */
2 𝑡∼Uniform(1,···,𝑇)
3 X𝑡,X𝑡−1←Eq. (1)
4 Z𝑡,Z𝑡←Sdn(X𝑡,A, 𝑡)
5 𝝁′
𝑡−1,Σ′
𝑡−1←Den(Z𝑡,Z𝑡,A)
6 Update parameters in Sdn andDen with Gradients on Eq. (14).
7end
/* Unconditional graph generation - GenGA. */
8X𝑇∼N( 0,I)
9for𝑡←𝑇to1do
10 𝝐∼N( 0,I)
11 Z𝑡,Z𝑡←Sdn(X𝑡,A, 𝑡)
12 𝝁′
𝑡−1,Σ′
𝑡−1←Den(Z𝑡,Z𝑡,A)
13 X𝑡−1=𝝁′
𝑡−1+𝝈′
𝑡−1I𝝐
14end
15X𝑎←X0
16returnG𝑎={A,X𝑎}
/* Classifier guided graph generation - CGenGA. */
17while Not Converged do
/* Pretrain the classifier 𝑝𝜑(𝑦|𝒙𝑡). */
18 𝑡∼Uniform(1,···,𝑇)
19 X𝑡←Eq. (1)
20 Update parameters 𝜑in the classifier based on the binary
cross-entropy loss.
21end
22X𝑇∼N( 0,I)
23for𝑡←𝑇to1do
24 𝝐∼N( 0,I)
25 Z𝑡,Z𝑡←Sdn(X𝑡,A, 𝑡)
26 𝝁′
𝑡−1,Σ′
𝑡−1←Den(Z𝑡,Z𝑡,A)
27 𝑔←∇ 𝒙𝑡log𝑝𝜑(𝑦|𝒙𝑡); // classifier signal
28 X𝑡−1=𝝁′
𝑡−1+𝜆Σ′
𝑡−1𝑔+𝝈′
𝑡−1I𝝐
29end
30X𝑎←X0
31returnG𝑎={A,X𝑎}
5.4 Training Criteria
Eventually, all parameters in Sdn andDen can be promptly fine-
tuned as to minimize the KL-divergence between the distributions
of the predicted X′
𝑡−1and the noised X𝑡−1at arbitrary steps 𝑡,
formulated as:
arg min
𝜽𝐷KL 𝑞(X𝑡−1|X,A)∥𝑝𝜽(X𝑡−1|X𝑡,A). (14)
5.5 Unconditional Graph Generation - GenGA
Once the whole model is sufficiently trained on different steps 𝑡,
we can simply sample X𝑇fromN(0,I)and gradually estimate the
mean and variance of X𝑡−1through Sdn andDen to generate a
new graphG𝑎={A,X𝑎}that has the same number of nodes and
features as the original graph following:
X𝑡−1=𝝁′
𝑡−1+𝝈′
𝑡−1I𝝐, (15)
 
2157KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
where(𝝈′
𝑡−1)2I=diag(Σ′
𝑡−1),𝝐is sampled from the standard
Gaussian distribution.
5.6 Classifier Guided Generation - CGenGA
While GenGA could generate auxiliary graphs from noise, it ig-
nores the class information for guiding graph generation. Here, we
further explore incorporating classifier guidance by conditioning
the generation process on class information with:
𝑝𝜽,𝜑(X0:𝑇):=𝑝(X𝑇,A,𝒚)𝑇Ö
𝑡=1𝑝𝜽,𝜑(X𝑡−1|X𝑡,A,𝒚), (16)
and following [5], each reverse step suffices to:
log[𝑝𝜽,𝜑(X𝑡−1|X𝑡,A)𝑝𝜽(𝒚|X𝑡,A)]≈ log𝑝(H)+𝐶, (17)
where𝐶is a constant and H∼N( 𝝁′
𝑡−1+Σ′
𝑡−1𝑔,Σ′
𝑡−1). Here,𝑔is the
involved classifier signal, which is the derivative of classifier on data,
𝑔=∇𝒙𝑡log𝑝𝜑(𝑦|𝒙𝑡)[40], and the classifier 𝑝𝜑(𝑦|𝒙𝑡)is trained on
noised node attribute 𝒙𝑡and the node label 𝑦. In fact, involving
the classifier guidance is shifting the mean of the unconditional
generator [ 5] byΣ′
𝑡−1𝑔and the denoised node attribute matrix X𝑡−1
can be firmly obtained via:
X𝑡−1=𝝁′
𝑡−1+𝜆Σ′
𝑡−1𝑔+𝝈′
𝑡−1I𝝐, (18)
where𝜆is a hyperparameter for balancing the weight of classifier
signal. We summarize our algorithms in Algorithm 1.
5.7 Complement Graph Anomaly Detection
with Synthetic Data
For enhancing a given anomaly detector 𝑓, we can simply input the
generated graphs G𝑎={G1𝑎,...,G|G𝑎|
𝑎}in addition to the original
graphGto fine-tune the parameters by minimizing their original
loss functions, such as the binary cross-entropy loss. The general-
ized form the total training loss can be written as:
L𝑡𝑜𝑡𝑎𝑙=L(𝑓(X,A),𝒚)+1
|G𝑎||G𝑎|∑︁
𝑖=1L(𝑓(X𝑖,A),𝒚), (19)
where the first term measures the training loss on the original graph,
while the second term calculates that on the synthetic graphs.
5.8 Complexity Analysis
The primary computational cost of our method stems from the train-
ing of the denoising neural network (generative model), graph gen-
eration, and classifier training (for classifier guidance in CGenGA).
For the generative model, the overall complexity of the time encod-
ing function Te(·) isO(2𝑛𝑑𝑘)and since SdnandDen adopt GCN as
the backbone, the cost of each training iteration is approximately
O(4𝑛𝑘𝑑), where𝑛is the number of nodes in G,𝑘is the dimension-
ality of node attributes, and 𝑑is the dimension of the GCN layer.
The graph generation stage involves Sdn andDen for inference,
and the total cost of generating one graph is about O(4𝑛𝑘𝑑𝑇), with
𝑇diffusion steps. Taking GCN as the classifier in CGenGA, its
training cost at each iteration is O(2𝑛𝑘𝑑).Table 2: The statistics of the graph anomaly detection datasets
Dataset #No
des #Edges #Features #Degree #Anomalies Anomaly Ratio
Y
elpChi 45,954
3,846,979 32 83.7 6677 16.9%
Reddit 10,984
168,016 64 15.3 366 3.3%
Weibo 8,405
407,963 400 48.5 868 10.3%
Tolokers 11,758
519,000 10 44.1 2563 21.8%
Questions 48,921
153,540 301 3.1 1467 3%
BlogCatalog 5,196
172,759 8,189 33.2 298 5.7%
ACM 16,484
74,073 8,337 4.5 597 3.6%
Cora 2,708
11,060 1,433 4.1 138 5.1%
6 EXPERIMENTS
In this section, we conduct extensive experiments to seek answers
to the following research questions:
RQ1: How effective are the generated graphs in enhancing the
existing anomaly detectors’ performance?
RQ2: Can classifier signals enrich the generated graphs?
RQ3: How do the residual link in the denoising neural network
and𝜆influence the performance gains?
RQ4: How about the impact of the linear and cosine schedulers on
the performance?
RQ5: Can the detection performance be further improved with
more synthetic data?
RQ6: How fast can GenGA andCGenGA generate graphs?
RQ7: How effective is GenGA compared to other graph data aug-
mentation methods (in Appendix A.3)?
6.1 Experimental Setups
Datasets. We include eight widely-used graph anomaly detection
datasets for evaluation. Specifically, these datasets can be cate-
gorized into two groups: one with organic anomalies, including
YelpChi [ 37], Reddit [ 46], Weibo [ 53], Tolokers and Questions [ 41];
and another group with injected anomalies (BlogCatalog [ 6], ACM [ 6],
and Cora [ 27]). Statistics of the datasets are summarized in Table 2.
Implementation Details. In all experiments, we set the dimen-
sions of the GCN layers in SdnandDen as256and128respectively.
We adopt a total number of 1000 steps in the forward diffusion
process. For CGenGA, we implement a two-layered GCN as the
classifier for guidance, and for CONDAD and CoLA, we decide their
classification thresholds regarding the results on labeled nodes.
Existing Detector. To assess the efficacy of our synthetic data, we
adopt three GNN detectors built upon GCN [ 24], GAT [ 44], and
GraphSAGE [ 12], and four state-of-the-art semi-/supervised anom-
aly detectors: BWGNN [ 42], GAT-sep [ 56], AMNet [ 4], GHRN [ 8],
along with two unsupervised detectors, namely CONAD [ 50] and
CoLA [ 28]. We use one synthetic graph in the experiment unless
specified. More details are provided in Appendix A.2.
Graph Data Augmentation Methods. To validate our motiva-
tion of designing diffusion-based models for graph data genera-
tion, we further compare our methods with VGAE [ 23] and the
non-generative graph augmentation method GraphENS4[35] that
synthesize egonets for handling class imbalanced learning on graph.
Since GraphENS and VGAE do not involve classifier guidance for
training, we only compare GenGA with them.
Experimental Details. We train CGenGA and all existing de-
tectors (excluding CONAD and CoLA) strictly following Sec. 5.7
with a ratio of 20% labeled nodes to mimic the real scenarios with
4https://github.com/JoonHyung-Park/GraphENS
 
2158Graph Anomaly Detection with Few Labels:
A Data-Centric Approach KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Improved performance on three synthetic datasets.
Metho
dCora BlogCatalog A
CM
F1
AUC Pre Rec F1
AUC Pre Rec F1
AUC Pre Rec
GCN 53.12±0.3
67.75±0.4 56.21±0.5 52.71±0.2 60.14±0.4
68.18±0.7 69.36±0.8 57.60±0.1 56.77±0.1
69.21±0.2 58.19±0.1 55.91±0.1
+GenGA 69.28±0.2
(↑30.4%) 74.05±0.2 ( ↑9.3%) 68.66±0.2 ( ↑22.1%) 71.08±0.2 ( ↑34.9%) 76.24±0.2
(↑26.8%) 77.55±0.5 ( ↑13.7%) 87.88±0.6 ( ↑26.7%) 74.95±0.3 ( ↑30.1%) 73.91±0.2
(↑30.2%) 77.40±0.2 ( ↑11.8%) 76.21±0.1 ( ↑31.0%) 69.28±0.2 ( ↑23.9%)
+CGenGA 65.12±0.1
(↑22.6%) 73.70±0.2 ( ↑8.8%) 68.23±0.2 ( ↑21.4%) 70.31±0.1 ( ↑33.4%) 74.87±0.3
(↑24.5%) 75.56±0.2 ( ↑10.8%) 86.79±0.5 ( ↑25.1%) 70.57±1.4 ( ↑22.5%) 72.31±0.1
(↑27.4%) 73.30±0.3 ( ↑5.9%) 72.29±1.2 ( ↑24.2%) 68.21±1.1 ( ↑22.0%)
GAT 63.15±0.3
68.90±0.3 65.31±0.3 67.43±0.2 62.39±0.3
71.47±0.3 69.51±0.1 64.96±0.4 61.58±0.1
68.11±0.2 66.59±0.2 59.10±0.1
+GenGA 63.35±1.5
(↑0.3%) 70.05±0.6 ( ↑1.7%) 67.76±1.5 ( ↑3.8%) 71.55±3.9 ( ↑6.1%) 65.73±1.8
(↑5.4%) 72.47±0.6 ( ↑1.4%) 77.65±1.6 (↑11.7%) 73.80±3.2 ( ↑13.6%) 66.49±2.2
(↑8.0%) 74.15±2.5 ( ↑8.9%) 86.07±2.8 ( ↑29.3%) 73.60±2.7 ( ↑24.5%)
+CGenGA 66.03±0.3
(↑4.6%) 70.46±0.5 ( ↑2.3%) 65.62±0.6 ( ↑0.5%) 67.74±0.3 ( ↑0.5%) 66.38±0.2
(↑6.4%) 71.51±0.4 ( ↑0.1%) 71.26±0.2 (↑2.5%) 65.65±0.2 ( ↑1.1%) 66.97±2.1
(↑8.8%) 71.33±1.4 ( ↑4.7%) 67.38±0.6 ( ↑1.2%) 66.66±2.0 ( ↑12.8%)
GraphSAGE 52.24±0.3
68.00±0.4 57.01±0.5 51.89±0.2 63.25±0.2
61.31±0.2 73.95±0.4 59.68±0.1 65.70±0.2
64.94±0.2 67.88±0.2 61.26±0.1
+GenGA 68.60±3.1
(↑31.3%) 68.74±0.5 ( ↑1.1%) 75.92±2.1 ( ↑33.2%) 65.20±3.4 ( ↑25.7%) 70.43±2.2
(↑11.4%) 73.09±3.2 ( ↑19.2%) 75.45±1.8 (↑2.0%) 72.24±1.6 ( ↑21.0%) 75.06±1.4
(↑14.3%) 72.33±3.0 ( ↑11.3%) 82.97±4.8 ( ↑22.2%) 70.72±1.7 ( ↑15.4%)
+CGenGA 69.58±3.0
(↑33.2%) 69.80±1.9 ( ↑2.6%) 80.13±4.8 ( ↑40.6%) 65.13±2.8 ( ↑25.5%) 74.72±0.2
(↑18.1%) 76.59±0.3 ( ↑24.9%) 84.80±0.4 ( ↑14.7%) 68.64±0.2 ( ↑15.0%) 72.70±1.4
(↑10.7%) 72.22±2.2 ( ↑11.2%) 81.90±4.0 ( ↑20.7%) 68.19±2.5 ( ↑11.3%)
BW
GNN 52.18±0.2
44.93±0.2 53.99±0.2 51.85±0.1 52.79±0.1
51.35±0.3 53.44±0.1 59.24±0.1 54.93±0.3
47.69±0.6 55.27±0.4 56.70±0.2
+GenGA 53.81±1.0
(↑3.1%) 48.01±3.5 ( ↑6.8%) 56.04±1.7 ( ↑3.8%) 52.98±0.6 ( ↑2.2%) 55.91±2.9
(↑5.6%) 53.11±2.5 ( ↑3.1%) 55.45±2.2 (↑3.8%) 60.96±2.3 ( ↑2.9%) 58.34±0.9
(↑6.2%) 56.11±2.2 ( ↑17.7%) 69.83±2.4 ( ↑26.3%) 57.71±0.7 ( ↑1.8%)
+CGenGA 53.50±0.4
(↑2.5%) 45.14±3.2 ( ↑0.5%) 55.13±0.4 ( ↑2.1%) 52.79±0.2 ( ↑1.8%) 58.74±3.3
(↑10.9%) 52.60±1.1 (↑2.1%) 59.23±5.4 (↑10.8%) 61.43±3.1 ( ↑3.7%) 57.66±0.8
(↑5.0%) 56.96±4.2 ( ↑19.4%) 69.83±4.3 ( ↑26.3%) 58.23±0.5 ( ↑2.7%)
GAT-sep 57.08±0.5
63.99±0.5 58.43±0.5 56.45±0.4 66.82±0.1
75.97±0.5 67.44±0.1 66.40±0.2 65.45±0.2
74.58±0.2 65.11±0.2 66.02±0.2
+GenGA 61.37±1.7
(↑8.1%) 72.76±2.6 ( ↑13.7%) 63.24±2.8 ( ↑8.2%) 60.88±1.4 ( ↑7.8%) 68.75±1.4
(↑2.6%) 78.90±5.1 ( ↑3.9%) 87.84±1.6 (↑30.3%) 70.76±1.9 ( ↑6.6%) 71.79±1.0
(↑9.7%) 86.51±1.6 ( ↑16.0%) 70.16±1.8 ( ↑7.7%) 73.50±1.0 ( ↑11.3%)
+CGenGA 61.86±0.7
(↑8.4%) 69.96±2.2 ( ↑9.3%) 63.89±1.1 ( ↑9.3%) 60.65±0.7 ( ↑7.4%) 69.44±0.1
(↑3.9%) 78.82±5.3 ( ↑3.7%) 89.28±2.7 (↑32.4%) 74.89±1.1 ( ↑12.8%) 71.71±0.5
(↑9.5%) 87.26±3.3 ( ↑17.0%) 69.76±0.8 ( ↑7.1%) 73.72±0.7 ( ↑11.6%)
AMNet 53.09±0.1
65.64±0.3 53.18±0.1 53.55±0.1 66.21±0.4
72.23±0.3 63.46±0.4 73.26±0.3 60.11±0.1
74.54±0.2 58.93±0.2 62.41±0.1
+GenGA 54.17±0.9
(↑2.0%) 68.46±4.8 ( ↑4.3%) 57.11±1.8 ( ↑7.4%) 53.82±0.7 ( ↑0.5%) 74.74±1.4
(↑12.9%) 78.88±1.6 (↑4.9%) 77.20±3.0 (↑21.7%) 75.16±1.1 ( ↑2.6%) 60.64±1.1
(↑0.9%) 75.38±1.7 ( ↑1.1%) 70.58±1.5 ( ↑32.7%) 65.22±1.2 ( ↑4.5%)
+CGenGA 53.71±0.8
(↑1.2%) 66.26±1.5 ( ↑0.9%) 57.12±1.6 ( ↑7.4%) 54.23±0.6 ( ↑1.3%) 74.02±1.9
(↑11.8%) 77.45±3.1 (↑3.0%) 76.85±4.7 (↑21.1%) 73.72±1.6 ( ↑0.6%) 60.79±0.8
(↑1.1%) 74.99±1.6 ( ↑0.6%) 70.98±1.1 ( ↑33.5%) 66.47±0.7 ( ↑6.5%)
GHRN 50.08±0.5
59.67±0.9 52.31±0.5 52.11±0.4 56.69±0.2
51.62±0.5 56.13±0.2 60.55±0.2 57.60±0.1
60.18±0.5 68.37±0.3 55.09±0.1
+GenGA 51.05±1.7
(↑0.5%) 73.40±7.1 ( ↑23.0%) 53.53±4.8 ( ↑2.3%) 53.18±1.1 ( ↑2.0%) 74.88±1.7
(↑32.1%) 74.44±3.2 ( ↑44.2%) 64.55±0.8 ( ↑15.0%) 70.24±0.9 ( ↑16.0%) 57.75±1.1
(↑0.3%) 79.43±4.5 ( ↑32.0%) 69.28±6.3 ( ↑1.3%) 55.47±0.3 ( ↑0.7%)
+CGenGA 52.75±1.6
(↑3.5%) 69.84±4.6 ( ↑17.0%) 55.25±2.5 ( ↑5.6%) 52.12±1.2 ( ↑0.1%) 74.65±1.6
(↑31.7%) 76.07±4.9 ( ↑47.4%) 65.49±0.8 ( ↑16.7%) 71.37±2.1 ( ↑17.9%) 58.02±1.1
(↑0.7%) 80.64±3.2 ( ↑34.0%) 69.69±6.3 ( ↑1.9%) 55.22±0.7 ( ↑0.2%)
CONAD 53.53±0.1
70.48±0.7 54.53±0.2 66.17±0.1 53.87±0.2
63.03±0.1 58.97±0.1 52.90±0.1 53.16±0.1
70.86±0.1 54.28±0.1 60.12±0.1
+GenGA 57.38±0.2
(↑7.2%) 72.93±0.1 (↑3.5%) 59.40±0.2 (↑8.9%) 71.28±0.2 (↑7.7%) 58.17±0.2
(↑8.0%) 69.32±0.1 (↑10.0%) 64.36±0.2 (↑9.1%) 53.87±0.1 (↑1.8%) 56.18±0.2
(↑5.7%) 73.89±0.2 (↑4.3%) 59.52±0.2 (↑9.7%) 60.72±0.1 (↑1.0%)
+CGenGA 54.82±0.1
(↑2.4%) 72.86±0.2 (↑3.4%) 57.95±0.2 (↑6.3%) 70.11±0.2 (↑6.0%) 57.40±0.1
(↑6.6%) 65.31±0.2 (↑3.6%) 61.45±0.2 (↑4.2%) 56.14±0.1 (↑6.1%) 57.25±0.3
(↑7.7%) 75.63±0.2 (↑6.7%) 57.33±0.2 (↑5.6%) 61.05±0.1 (↑1.5%)
CoLA 48.18±0.5
51.86±0.4 51.65±0.2 52.48±0.3 47.53±0.1
58.29±0.2 52.14±0.1 58.63±0.1 43.77±0.5
48.68±0.2 51.78±0.5 50.03±0.2
+GenGA 51.71±0.1
(↑7.3%) 56.23±1.5 (↑8.5%) 53.08±0.2 (↑2.8%) 53.68±1.3 (↑2.3%) 50.09±0.1
(↑5.8%) 62.99±2.0 (↑8.1%) 52.83±0.2 (↑1.3%) 60.84±2.2 (↑3.8%) 45.97±0.2
(↑5.0%) 53.04±1.7 (↑9.0%) 56.38±0.1 (↑8.9%) 51.12±2.1 (↑2.2%)
+CGenGA 52.73±0.3
(↑9.5%) 55.58±0.2 (↑7.2%) 52.26±0.1 (↑1.2%) 55.20±0.3 (↑5.2%) 50.23±0.3
(↑6.1%) 61.39±0.2 (↑5.3%) 55.29±0.3 (↑6.0%) 59.29±0.1 (↑1.1%) 45.89±0.3
(↑4.9%) 53.42±0.2 (↑9.7%) 53.17±0.3 (↑2.7%) 52.06±0.2 (↑4.1%)
Table 4: Improved performance on five organic datasets w.r.t. Macro-F1 and AUC.
Metho
dY
elpChi Re
ddit W
eibo T
olokers Questions
F1
AUC F1
AUC F1
AUC F1
AUC F1
AUC
GCN 46.08±0.1
57.09±0.1 49.15±0.1
57.74±0.1 82.27±0.1
85.64±0.1 55.80±0.2
71.69±0.1 53.00±0.1
52.76±0.1
+GenGA 51.68±2.2
(↑12.2%) 60.15±1.0 ( ↑5.4%) 50.99±1.2
(↑3.8%) 64.95±0.5 ( ↑15.1%) 90.43±1.4
(↑9.9%) 94.08±1.3 ( ↑9.9%) 63.45±0.8
(↑13.7%) 74.54±1.0 ( ↑4.0%) 54.06±0.5
(↑2.0%) 64.08±1.2 ( ↑21.4%)
+CGenGA 50.16±0.5
(↑8.8%) 60.58±0.5 ( ↑6.1%) 51.77±0.5
(↑5.4%) 68.10±0.2 ( ↑20.7%) 91.62±0.7
(↑11.4%) 94.66±0.9 ( ↑10.5%) 64.62±0.6
(↑15.8%) 74.41±0.4 ( ↑3.8%) 54.56±0.4
(↑2.9%) 64.67±1.4 ( ↑22.6%)
GAT 46.98±0.1
58.24±0.1 49.24±0.1
64.45±0.2 85.62±0.1
79.86±0.9 55.43±0.2
71.97±0.1 50.42±0.2
54.70±0.1
+GenGA 51.53±1.3
(↑9.7%) 78.59±1.2 ( ↑34.9%) 50.94±0.2
(↑3.4%) 65.08±0.4 ( ↑1.0%) 90.72±1.3
(↑6.0%) 91.54±3.2 ( ↑14.6%) 62.52±0.7
(↑12.8%) 76.19±1.0 ( ↑5.9%) 51.92±2.1
(↑3.0%) 62.70±1.5 ( ↑14.6%)
+CGenGA 62.00±0.4
(↑32.0%) 80.24±1.8 ( ↑37.8%) 50.57±0.2
(↑2.7%) 65.61±0.4 ( ↑1.8%) 90.14±0.6
(↑5.3%) 92.68±1.3 ( ↑16.1%) 61.92±0.8
(↑11.7%) 76.24±0.9 ( ↑5.9%) 51.66±2.2
(↑2.5%) 64.34±1.6 ( ↑17.6%)
GraphSAGE 60.86±0.2
69.93±0.1 49.15±0.1
51.31±0.2 89.20±0.1
88.35±0.2 55.66±0.2
72.31±0.1 56.37±0.1
51.42±0.1
+GenGA 67.51±0.9
(↑10.9%) 83.74±0.6 ( ↑4.2%) 62.00±0.1
(↑16.5%) 70.79±0.2 ( ↑1.2%) 90.97±0.2
(↑2.1%) 90.02±2.9 ( ↑1.9%) 62.56±0.7
(↑12.4%) 75.84±0.5 ( ↑4.9%) 56.58±0.6
(↑0.4%) 59.84±0.6 ( ↑16.4%)
+CGenGA 66.14±2.6
(↑8.7%) 83.73±0.3 ( ↑4.2%) 67.64±0.1
(↑27.1%) 71.07±1.0 ( ↑1.6%) 90.78±1.6
(↑1.8%) 89.44±3.3 ( ↑1.2%) 61.06±1.0
(↑9.7%) 75.53±0.5 ( ↑4.5%) 56.99±0.1
(↑1.1%) 59.84±1.2 ( ↑16.4%)
BW
GNN 63.68±0.3
80.96±0.1 43.29±0.5
56.96±0.3 89.27±0.3
92.29±0.3 57.35±0.9
64.14±1.6 56.45±0.1
56.15±0.1
+GenGA 65.33±0.9
(↑2.6%) 81.31±0.7 ( ↑0.4%) 44.48±0.5
(↑2.7%) 69.63±3.2 ( ↑22.2%) 90.17±0.1
(↑1.0%) 93.84±0.3 ( ↑1.7%) 63.12±0.7
(↑10.1%) 77.50±0.5 ( ↑20.8%) 56.62±0.4
(↑0.3%) 57.45±1.4 ( ↑2.3%)
+CGenGA 64.63±2.0
(↑1.5%) 81.56±0.4 ( ↑0.7%) 45.51±0.5
(↑5.1%) 70.56±0.8 ( ↑23.9%) 90.48±0.7
(↑1.4%) 93.08±0.3 ( ↑0.9%) 62.90±0.6
(↑9.7%) 77.08±0.4 ( ↑20.2%) 56.67±0.1
(↑0.4%) 57.50±0.2 ( ↑2.4%)
GAT-sep 65.93±0.3
80.01±0.3 49.16±0.1
50.22±0.5 91.92±0.2
95.71±0.1 63.12±0.7
72.06±0.4 56.05±0.1
70.18±0.1
+GenGA 65.94±3.6
(↑0.1%) 82.15±1.9 ( ↑2.7%) 53.08±0.2
(↑8.0%) 56.39±0.4 ( ↑12.3%) 95.56±0.7
(↑4.0%) 95.75±2.1 ( ↑0.1%) 63.22±0.1
(↑0.2%) 75.86±0.8 ( ↑5.3%) 57.04±1.4
(↑1.8%) 71.76±0.9 ( ↑2.2%)
+CGenGA 66.90±2.0
(↑1.5%) 82.10±1.5 ( ↑2.6%) 51.45±0.2
(↑4.7%) 55.16±0.3 ( ↑9.8%) 92.72±0.3
(↑0.9%) 95.77±0.3 ( ↑0.1%) 64.50±0.4
(↑2.2%) 75.56±0.7 ( ↑4.9%) 57.99±0.2
(↑3.5%) 72.16±0.2 ( ↑2.8%)
AMNet 54.66±0.8
64.01±1.2 50.39±0.1
62.14±0.3 91.63±0.1
97.11±0.1 56.74±0.5
65.33±0.1 55.63±0.1
61.86±0.1
+GenGA 64.14±1.5
(↑17.3%) 81.20±0.5 ( ↑26.9%) 51.22±2.6
(↑1.7%) 66.93±1.6 ( ↑7.7%) 91.64±0.3
(↑0.1%) 97.94±1.0 ( ↑0.9%) 58.30±1.2
(↑2.7%) 74.03±2.4 ( ↑13.3%) 56.32±0.3
(↑1.2%) 62.92±0.2 ( ↑1.7%)
+CGenGA 64.62±1.8
(↑18.2%) 80.83±0.4 ( ↑26.3%) 54.01±0.4
(↑7.2%) 68.59±0.6 ( ↑10.4%) 91.74±2.0
(↑0.1%) 97.56±0.2 ( ↑0.5%) 60.02±1.5
(↑5.8%) 75.11±1.4 ( ↑15.0%) 56.27±0.1
(↑1.2%) 62.89±0.1 ( ↑1.7%)
GHRN 65.59±0.1
81.92±0.1 45.60±0.3
66.09±0.4 89.26±0.1
91.78±0.2 60.56±0.5
71.27±1.2 56.81±0.1
56.14±0.1
+GenGA 65.78±1.2
(↑0.3%) 82.53±1.2 ( ↑0.7%) 48.88±0.9
(↑7.2%) 70.14±1.2 ( ↑6.1%) 89.90±0.1
(↑0.7%) 92.52±0.2 ( ↑0.8%) 62.96±1.3
(↑4.0%) 77.05±0.7 ( ↑8.1%) 57.28±0.3
(↑0.8%) 56.70±0.2 ( ↑0.1%)
+CGenGA 66.89±3.0
(↑2.0%) 82.99±1.0 ( ↑1.3%) 47.18±3.7
(↑3.5%) 70.29±1.5 ( ↑6.4%) 89.81±0.1
(↑0.6%) 92.32±0.2 ( ↑0.6%) 63.62±0.9
(↑5.1%) 77.16±0.7 ( ↑8.3%) 57.12±0.6
(↑0.5%) 57.10±0.4 ( ↑1.7%)
CONAD 47.42±0.1
47.50±0.2 46.39±0.1
55.78±0.2 79.01±0.1
90.40±0.1 46.54±0.1
61.24±0.1 48.64±0.1
50.36±0.1
+GenGA 52.68±0.1
(↑11%) 50.32±0.1 ( ↑5.9%) 49.15±0.1
(↑5.9%) 57.23±0.1 (↑2.6%) 82.27±0.2
(↑4.1%) 91.98±0.3 (↑1.7%) 47.67±0.2
(↑2.4%) 63.91±0.2 (↑4.3%) 52.33±0.1
(↑7.6%) 51.61±0.2 (↑2.0%)
+CGenGA 48.16±0.3
(↑1.6%) 50.23±0.2 (↑5.7%) 50.95±0.2
(↑9.8%) 58.62±0.3 (↑5.1%) 81.93±0.1
(↑3.7%) 92.69±0.2 (↑2.5%) 49.99±0.3
(↑7.4%) 66.56±0.1 (↑8.6%) 50.15±0.3
(↑3.1%) 52.32±0.2 (↑3.4%)
CoLA 45.82±0.1
61.60±0.1 46.09±0.3
50.26±0.3 49.90±0.2
71.59±0.4 50.78±0.1
55.45±0.1 47.24±0.1
56.97±0.1
+GenGA 46.14±0.2
(↑0.7%) 65.42±0.1 (↑6.2%) 47.04±0.1
(↑2.1%) 51.53±0.1 (↑2.5%) 50.91±0.3
(↑2.0%) 75.95±0.1 (↑6.1%) 55.22±0.2
(↑8.7%) 56.82±0.2 (↑2.4%) 49.61±0.1
(↑4.9%) 61.94±0.1 (↑8.7%)
+CGenGA 47.66±0.1
(↑4.0%) 66.83±0.3 (↑8.4%) 49.12±0.2
(↑6.6%) 51.36±0.2 (↑2.3%) 53.26±0.2
(↑6.8%) 75.38±0.3 (↑5.2%) 53.82±0.2
(↑6.0%) 59.20±0.2 (↑6.8%) 50.60±0.1
(↑7.1%) 62.22±0.2 (↑9.2%)
few labels. We report the 5-fold average performance (in percent-
age) and improvement along with the standard deviation w.r.t. five
commonly-used metrics: Macro-F1, AUC, Macro-Precision, Macro-
Recall and AUCPR [32]. All the experiments on a Rocky Linux
8.6 (Green Obsidian) server equipped with a 12-core CPU, 1 Nvidia
V100 GPU (with 30GB RAM), and 100GB RAM.
6.2 Improvement on Anomaly Detection
Performance (RQ1)
Tables 3, 4, and 5, 8 (in Appendix A) summarize the enhanced per-
formance of the detectors after training with additional synthetic
graphs. The improvements are marked upon the incorporation
of synthetic data generated by GenGA (highlighted in blue) and
CGenGA (highlighted in red). We witness a notable enhancement
in the performance across all detectors on the five organic datasets
and three synthetic datasets. Remarkably, the three conventionalGNN detectors, i.e., GCN, GAT and GraphSAGE experience substan-
tial performance boosts, with all four evaluated metrics showing
increases on all datasets. This significant enhancement underscores
the ability of these GNN detectors to achieve better generalization
in graph anomaly detection after training on our synthetic data
that replicates the original node attribute distributions within the
given graph topology.
The four semi-/supervised graph anomaly detectors also exhibit
different degrees of enhancement ranging from 0.1%to20%over all
metrics while the unsupervised detectors’ AUC scores have been
raised by more than 4%on average. The gaps among performance
gains of different detectors further indicate that the synthetic data
has a divergent impact on the detectors. We assume this maybe
caused by the varying capabilities of each method in learning the
data distribution and assimilating synthetic information. Overall,
our empirical results on the eight datasets demonstrate that our
 
2159KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
Table 5: Improved performance on five organic datasets w.r.t. Macro-Precision and Macro-Recall.
Metho
dY
elpChi Re
ddit W
eibo T
olokers Questions
Pr
e Rec Pr
e Rec Pr
e Rec Pr
e Rec Pr
e Rec
GCN 42.73±0.1
50.00±0.1 48.33±0.1
50.00±0.1 82.83±0.1
82.18±0.1 60.68±0.1
55.87±0.1 53.67±0.5
52.04±0.1
+GenGA 56.35±0.5
(↑31.9%) 56.08±1.9 ( ↑0.5%) 51.49±0.2
(↑6.5%) 59.58±3.3 ( ↑19.4%) 89.01±2.7
(↑8.0%) 92.26±0.9 ( ↑12.3%) 62.16±0.7
(↑2.1%) 67.73±1.1 ( ↑21.2%) 53.87±0.3
(↑0.4%) 59.11±1.1 (↑13.6%)
+CGenGA 54.05±0.7
(↑26.5%) 57.06±0.5 ( ↑2.3%) 51.63±0.1
(↑6.8%) 61.57±0.2 ( ↑23.4%) 90.33±0.9
(↑9.7%) 93.05±0.8 ( ↑13.2%) 62.14±0.5
(↑2.1%) 67.76±0.7 ( ↑21.3%) 53.70±0.2
(↑0.0%) 59.18±1.3 (↑13.7%)
GAT 58.97±1.6
50.24±0.1 48.33±0.1
50.00±0.1 88.57±0.2
83.22±0.1 60.07±0.1
55.69±0.2 52.92±0.1
50.64±0.1
+GenGA 60.43±2.8
(↑2.5%) 67.05±5.0 ( ↑33.5%) 53.59±0.2
(↑10.9%) 50.05±0.1 ( ↑0.2%) 90.76±1.0
(↑2.5%) 90.77±2.2 ( ↑9.1%) 63.43±0.8
(↑5.6%) 68.87±1.8 ( ↑23.7%) 53.56±0.1
(↑1.2%) 59.31±2.4 (↑17.1%)
+CGenGA 62.59±1.6
(↑6.1%) 72.50±1.6 ( ↑44.3%) 51.12±0.2
(↑5.8%) 52.36±0.7 ( ↑4.8%) 89.41±1.1
(↑0.9%) 90.97±1.3 ( ↑9.3%) 63.52±0.5
(↑5.7%) 69.53±0.8 ( ↑24.8%) 53.29±0.2
(↑0.7%) 60.02±0.9 (↑18.5%)
GraphSAGE 75.11±0.2
58.62±0.2 48.33±0.1
50.00±0.1 90.16±0.1
88.43±0.2 60.43±0.2
55.73±0.1 54.44±0.1
54.38±0.1
+GenGA 65.76±0.6
(↑2.6%) 75.19±1.1 ( ↑28.3%) 53.19±0.1
(↑0.6%) 60.12±3.0 ( ↑6.9%) 95.67±3.9
(↑6.1%) 89.15±2.2 ( ↑0.8%) 62.99±0.3
(↑11.7%) 68.03±1.1 ( ↑32.3%) 58.96±0.9
(↑8.3%) 55.32±0.4 ( ↑1.7%)
+CGenGA 65.10±1.2
(↑1.6%) 75.54±0.2 ( ↑28.9%) 53.78±0.3
(↑1.7%)) 57.82±3.9 ( ↑2.8%) 91.69±1.6
(↑1.7%) 89.72±2.6 ( ↑1.5%) 62.96±0.6
(↑11.7%) 68.67±1.0 ( ↑33.5%) 58.66±0.9
(↑7.8%) 54.73±0.3 ( ↑0.7%)
BW
GNN 63.30±0.1
72.84±0.1 51.55±0.1
60.47±0.1 88.82±0.3
89.77±0.1 57.29±1.1
58.06±0.6 56.17±0.1
56.82±0.1
+GenGA 64.06±0.4
(↑1.2%) 73.26±0.8 ( ↑0.6%) 52.02±0.5
(↑0.5%) 63.08±2.8 ( ↑23.7%) 90.62±0.2
(↑2.0%) 90.61±0.2 ( ↑0.9%) 64.18±0.2
(↑12.0%) 70.37±0.5 ( ↑21.2%) 56.18±0.1
(↑0.1%) 57.29±0.1 ( ↑0.8%)
+CGenGA 63.83±1.0
(↑0.8%) 73.86±0.5 ( ↑1.4%) 52.05±0.2
(↑0.6%) 64.50±0.9 ( ↑26.5%) 89.96±0.1
(↑1.3%) 90.31±0.2 ( ↑0.6%) 63.84±0.3
(↑11.4%) 69.85±0.5 ( ↑20.3%) 57.58±0.2
(↑2.5%) 58.97±0.1 ( ↑3.8%)
GAT-sep 72.10±0.1
63.74±0.4 48.33±0.1
50.01±0.1 85.95±0.2
93.98±0.1 60.56±0.4
63.14±0.7 55.19±0.3
57.37±0.1
+GenGA 74.79±2.3
(↑3.7%) 73.51±1.6 ( ↑15.3%) 57.59±0.2
(↑19.2%) 51.86±0.2 ( ↑3.7%) 89.18±1.8
(↑3.8%) 94.09±0.1 ( ↑0.1%) 63.10±0.5
(↑4.2%) 68.37±1.6 ( ↑8.3%) 57.33±0.4
(↑3.9%) 60.46±0.7 ( ↑5.4%)
+CGenGA 73.53±1.0
(↑2.0%) 73.96±1.2 ( ↑16.0%) 58.64±0.2
(↑21.3%) 52.20±0.2 ( ↑4.4%) 89.66±0.9
(↑4.3%) 94.83±0.1 ( ↑0.9%) 62.96±0.4
(↑4.0%) 68.19±1.1 ( ↑8.0%) 57.09±0.2
(↑3.5%) 61.01±0.7 ( ↑6.3%)
AMNet 55.68±1.3
55.51±0.6 50.94±0.2
50.82±0.1 86.86±0.7
89.56±0.2 57.03±0.1
57.89±0.2 55.67±0.1
56.86±0.1
+GenGA 63.52±0.9
(↑14.1%) 73.48±0.6 ( ↑32.4%) 51.91±0.3
(↑1.9%) 61.21±0.7 ( ↑20.4%) 88.23±0.4
(↑1.6%) 90.23±0.1 ( ↑0.7%) 62.39±1.7
(↑9.4%) 68.06±2.5 ( ↑17.6%) 56.28±0.3
(↑1.1%) 57.59±0.9 ( ↑1.3%)
+CGenGA 63.73±1.0
(↑14.5%) 72.96±0.6 ( ↑31.4%) 52.03±0.3
(↑2.1%) 63.77±1.1 ( ↑25.5%) 87.89±0.3
(↑1.2%) 90.85±0.7 ( ↑1.4%) 63.03±0.7
(↑10.5%) 69.01±1.0 ( ↑19.2%) 58.67±0.1
(↑5.4%) 57.55±0.7 ( ↑1.2%)
GHRN 64.28±0.1
74.14±0.3 51.57±0.1
59.79±0.3 89.11±0.1
89.43±0.1 61.78±0.1
59.86±0.5 56.44±0.1
57.32±0.1
+GenGA 64.73±0.8
(↑0.7%) 74.98±1.1 ( ↑1.1%) 52.22±0.7
(↑1.3%) 63.06±1.9 ( ↑5.5%) 89.53±0.1
(↑0.5%) 0.43±0.1 ( ↑1.1%) 63.69±0.2
(↑3.1%) 69.19±0.9 ( ↑15.6%) 57.41±0.1
(↑1.7%) 58.72±0.1 ( ↑2.4%)
+CGenGA 65.44±1.6
(↑1.8%) 75.19±0.9 ( ↑1.4%) 51.99±0.3
(↑0.8%) 63.45±1.8 ( ↑6.1%) 89.82±0.1
(↑0.8%) 90.95±0.1 ( ↑1.7%) 63.83±0.4
(↑3.3%) 69.24±0.6 ( ↑15.7%) 56.73±0.6
(↑0.5%) 57.60±0.7 ( ↑0.5%)
CONAD 47.35±0.1
45.51±0.1 49.58±0.1
48.02±0.1 74.91±0.1
88.25±0.1 46.37±0.1
47.13±0.5 50.54±0.1
52.37±0.1
+GenGA 51.49±0.3
(↑8.7%) 49.356±2.8 (↑8.5%) 53.37±0.1
(↑7.6%) 50.19±1.7 (↑4.5%) 78.39±0.2
(↑4.6%) 97.05±1.4 (↑10.0%) 50.31±0.3
(↑8.5%) 51.60±1.8 (↑9.5%) 53.18±0.2
(↑5.2%) 53.64±2.1 (↑2.4%)
+CGenGA 51.68±0.2
(↑9.1%) 48.76±2.7 (↑7.1%) 51.01±0.1
(↑2.9%) 48.90±1.9 (↑1.8%) 76.84±0.2
(↑2.6%) 89.83±2.3 (↑1.8%) 49.22±0.1
(↑6.1%) 49.91±1.4 (↑5.9%) 52.36±0.2
(↑3.6%) 57.63±1.1 (↑9.3%)
CoLA 46.26±0.1
45.54±0.1 49.48±0.1
50.43±0.2 50.53±0.1
50.80±0.2 51.10±0.1
50.91±0.1 50.42±0.1
52.28±0.1
+GenGA 49.52±0.2
(↑7.0%) 48.22±1.3 (↑6.0%) 51.11±0.1
(↑3.3%) 53.52±2.5 (↑6.1%) 53.33±0.2
(↑5.5%) 51.87±0.2 (↑2.1%) 53.67±0.2
(↑5.0%) 55.77±0.1 (↑9.5%) 55.21±0.3
(↑9.5%) 56.07±0.2 (↑7.2%)
+CGenGA 48.10±0.2
(↑4.0%) 48.62±0.2 (↑6.8%) 50.50±0.3
(↑2.1%) 53.48±0.2 (↑6.0%) 55.36±0.2
(↑9.5%) 53.75±0.1 (↑5.8%) 52.28±0.2
(↑2.3%) 55.23±0.3 (↑8.5%) 51.60±0.3
(↑2.3%) 57.36±0.2 (↑9.7%)
Metho
d Y
elpChi Reddit Weibo Tolokers
V
GAE 9.1129
2.7658 8.3161 9.6088
GenGA 1.2079
1.0430 0.9955 4.6133
CGenGA 1.1982
1.0401 0.9941 4.5289
Questions
BlogCatalog ACM Cora
V
GAE 3.5756
2.5432 0.3545 0.8824
GenGA 0.7821
1.7785 0.3103 0.7026
CGenGA 0.7805
1.7759 0.3009 0.7002
Table 6: FID on eight datasets.
F1AUCPreRec50607080GCNGenGA w/o RGenGAFigure 4: Ablation results
on Cora.
generated graphs effectively boost the existing detectors, affirm-
ing the efficacy of our data-centric approach for graph anomaly
detection with few labels.
6.3 The Impact of Classifier Guidance on Graph
Generation (RQ2)
ForRQ2, we analyze the impact of classifier guidance from two
perspectives that follow.
6.3.1 The impact of classifier guidance on detection performance.
We first compare the performance gains between GenGA and
CGenGA to investigate the implicit impact of classifier guidance
on graph generation. From the results in the four tables, we can see
that the class information derived from our pre-trained GCN clas-
sifier could further boost the detection performance on particular
datasets. However, the overall performance gains using CGenGA
are nearly identical to GenGA. We think this might be caused for
two main reasons: 1) Although the classifier guidance could incor-
porate label information for graph generation, this implicitly limits
the diversity of the generated node features under the constraint of
known labels; 2) The pre-trained classifier’s capacity also impacts
the derived information 𝑔(in Eq. (18)) for shifting the mean value
of the generated features. In contrast to the classifier guided image
generation [ 5], where the ResNet [ 14] could predict the different
class labels accurately, we expect better graph anomaly detectors
to further enhance the classifier guided generation process, which
is an important direction worth exploration.6.3.2 The impact of classifier guidance on the generated graph qual-
ity.In addition to the comparisons on the detection performance,
we further quantify the quality of our generated graphs with regard
to the Fréchet Inception Distance (FID) score [ 15], which is a widely-
used measurement on the similarity between the distributions of
real and synthetic data following:
FID=||𝝁−𝝁′||2
2+Tr(Σ+Σ′−2(ΣΣ′)1/2), (20)
where 𝝁and𝝁′are the mean values of the original and generated
features, respectively. ΣandΣ′are the corresponding covariances.
Intuitively, a lower FID score indicates a higher similarity and
signifies better quality of the generated data. From the results in
Table 6, we see that the classifier guidance can improve the FID on
all datasets, albeit marginally. Similar to our prior analysis on the
detection performance, we attribute this to the weak information
provided by the classifier, which deserves further investigations.
6.4 Ablation Study on the Residual Link (RQ3)
As presented in Sec. 5.3, we propose the residual link between Sdn
andDen to alleviate the oversmoothing of node representations
during message-passing. To validate its effectiveness, we conduct
an ablation study on the residual link by comparing GCN’s perfor-
mance improvement using synthetic graphs generated by GenGA
and the variant without such residual link, namely GenGA w/o
Ron Cora dataset. As illustrated in Fig. 4, the graph generated by
GenGA (with the residual link) outperforms GenGA w/o R w.r.t. all
metrics affirming the efficacy of the residual link.
6.5 The Impact of 𝜆(RQ3)
The hyperparameter 𝜆in Eq. (18) scales the classifier gradients.
The mean value of the generated data shifts more with a larger
𝜆and vice versa. Due to its explicit impact on the generated data
distribution, we measure the FID scores by varying 𝜆from 0.2to2.0
and the results on Cora and Weibo datasets are depicted in Fig. 5.
The increasing FID on Cora indicates that the more classifier signal
 
2160Graph Anomaly Detection with Few Labels:
A Data-Centric Approach KDD ’24, August 25–29, 2024, Barcelona, Spain
0.20.40.60.81.01.21.41.61.82.00.7000.7050.7100.7150.7200.725FID
(a)𝜆v.s. FID on Cora
0.20.40.60.81.01.21.41.61.82.00.9930.9940.9950.9960.997FID (b)𝜆v.s. FID on Weibo
Figure 5: The impact of 𝜆on FID.
246810607080
#Generated GraphsF1AUC
(a) Cora
246810455055606570
#Generated GraphsF1AUC (b) Reddit
24681080859095100
#Generated GraphsF1AUC
(c) Weibo
246810455565
#Generated GraphsF1AUC (d) YelpChi
Figure 6: Results with different number of generated graphs.
shifts the generated data far from the original distribution, while
the results on Weibo underscore that increasing 𝜆could reduce FID.
By comparing the detection performance of the original GCN on the
two datasets, we observe that GCN performs better on Weibo than
Cora. This implicitly showcases that the GCN classifier captures
the distribution of Weibo better than Cora, which causes the FID
differences on these two datasets.
6.6 The Impact of Noise Scheduler (RQ4)
The noise scheduler determines the amount of noise been added to
Xduring the forward diffusion process. In this study, we investigate
the impact of two mostly-used noise schedulers in DDPMs [ 16,34],
i.e., linear and cosine schedulers, on the graph anomaly detection
performance and depict the results on four datasets in Fig. 7. Specif-
ically, we train GenGA with both schedulers and further use the
generated graphs to train a GCN classifier. We can see that the
cosine scheduler performs slightly better than the linear scheduler
in most cases, except Reddit w.r.t. F1.
6.7 Performance with More Synthetic Graphs
(RQ5)
ForRQ5, we explore whether the detector’s performance can be
further improved by utilizing more generated graphs for training.
The results on four datasets (in Fig. 6) illustrate that increasing
the number of generated graphs brings limited benefit to or may
even deteriorate (on Cora and Weibo) the detection performance.
CoraRedditWeiboYelpChi406080100F1 (%)Cosine SchedulerLinear Scheduler(a) F1 with different schedulers
CoraRedditWeiboYelpChi406080100AUC (%)Cosine SchedulerLinear Scheduler (b) AUC with different schedulers
Figure 7: The impact of noise schedulers on FID.
Table 7: Sampling time on all datasets (in seconds)
Dataset Y
elpChi Reddit Weibo Tolokers Questions BlogCatalog ACM Cora
GenGA 7.98
7.30 23.27 14.44 14.78 143.55 85.06 4.59
CGenGA 22.93
15.02 33.03 25.75 32.20 154.15 110.59 8.09
To our knowledge, we attribute this to the limited diversity of the
generated graphs, and generating more diversified samples will be
a pivotal focus of our future research.
6.8 Time cost (RQ6)
We also report the average clock time (in seconds) of generating
one graph in Table 7. From the results, we can see that GenGA’s
sampling speed is faster than CGenGA since it involves no classifier
signal for generation, while both methods can generate a reusable
graph to enhance various existing detectors fast and in the worst
case, a graph can be generated within 160seconds.
7 CONCLUSION
Graph anomaly detection has witnessed great achievement for the
development of graph machine learning, but the existing works
mainly focus on more powerful detection models, leaving the lack
of data and the induced data-specific challenges as a prerequisite.
In contrast to the existing works, we propose a novel data-centric
approach to mitigate the rareness of training data in the realm of
graph anomaly detection. Through empirical and rigorous analysis
on the forward graph diffusion process, we identify two principles
for designing the denoising neural network for graph generation,
and propose GenGA andCGenGA to synthesize additional graphs.
Our extensive experiments on eight widely-used datasets showcase
that the state-of-the-art detectors’ performance can be further im-
proved with our generated graphs, which demonstrate the potential
of our data-centric approach.
Limitations and Future Works: From the case studies on the
seven research questions, we unveil potential improvement of our
model design and pinpoint future works in this direction: 1) in-
volving more effective classifier guidance for graph generation;
2) improve the sampling speed/reduce steps for sampling; and 3)
improving the diversity of the generated graphs.
ACKNOWLEDGEMENTS
This work was supported by the Australian Research Council Projects
LP210301259 and DP230100899, and Macquarie University Data
Horizons Research Centre.
 
2161KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
REFERENCES
[1]Leman Akoglu. 2021. Anomaly mining: Past, present and future. In CIKM. 1–2.
[2]Leman. Akoglu, Hanghang. Tong, and Danai Koutra. 2015. Graph based anomaly
detection and description: A survey. Data Min. Knowl. Disc. 29 (2015), 626–688.
[3]Sambaran Bandyopadhyay, Lokesh N, Saley Vishal Vivek, and M. N. Murty.
2020. Outlier resistant unsupervised deep architectures for attributed network
embedding. In WSDM. 25–33.
[4]Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and
Weihao Jiang. 2022. Can abnormality be detected by graph neural networks?. In
IJCAI. 1945–1951.
[5]Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat GANs on
image synthesis. In NeurIPS. 8780–8794.
[6]Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. 2019. Deep anomaly
detection on attributed networks. In SDM. 594–602.
[7]Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.
Enhancing graph neural network-based fraud detectors against camouflaged
fraudsters. In CIKM. 315–324.
[8]Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yong-
dong Zhang. 2023. Addressing heterophily in graph anomaly detection: A per-
spective of graph spectrum. In WWW. 1528–1538.
[9]Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yong-
dong Zhang. 2023. Alleviating structural distribution shift in graph anomaly
detection. In WSDM. 357–365.
[10] Dmitrii Gavrilev and Evgeny Burnaev. 2023. Anomaly detection in networks via
score-based generative models. In ICML 2023 Workshop on Structured Probabilistic
Inference &Generative Modeling.
[11] Frank E Grubbs. 1969. Procedures for detecting outlying observations in samples.
Technometrics 11, 1 (1969), 1–21.
[12] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In NeurIPS. 1025–1035.
[13] Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic.
2023. Reimagining synthetic tabular data generation through data-centric AI: A
comprehensive benchmark. In NeurIPS.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR. 770–778.
[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
Sepp Hochreiter. 2017. GANs trained by a two time-scale update rule converge
to a local Nash Equilibrium. In NeurIPS, Vol. 30.
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. In NeurIPS. 6840–6851.
[17] Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, and Weifeng Lv. 2022. GraphGDP:
Generative diffusion processes for permutation invariant graph generation. In
ICDM. 201–210.
[18] Abhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta,
Shanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal, and
Vitobha Munigala. 2020. Overview and importance of data quality for machine
learning tasks. In KDD. 3561–3562.
[19] Bowen Jing, Gabriele Corso, Regina Barzilay, and Tommi S Jaakkola. 2022. Tor-
sional diffusion for molecular conformer generation. In NeurIPS. 24240–24253.
[20] Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. 2023. Graph generation with
destination-driven diffusion mixture. In Machine Learning for Drug Discovery
workshop, ICLR.
[21] Nicolas Keriven. 2022. Not too little, not too much: A theoretical analysis of
graph (over)smoothing. In NeurIPS. 2268–2281.
[22] Diederik P Kingma and Max Welling. 2014. Auto-encoding variational bayes. In
ICLR.
[23] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. In
NeurIPS Workshop on Bayesian Deep Learning.
[24] Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with
graph convolutional networks. In ICLR.
[25] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. 2017. Radar: Residual analysis
for anomaly detection in attributed networks.. In IJCAI. 2152–2158.
[26] Fanzhen Liu, Xiaoxiao Ma, Jia Wu, Jian Yang, Shan Xue, Amin Behesht, Chuan
Zhou, Hao Peng, Quan Z. Sheng, and Charu C. Aggarwal. 2022. DAGAD: Data
augmentation for graph anomaly detection. In ICDM. 259–268.
[27] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang,
Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, et al .2022. BOND: Benchmarking
unsupervised outlier node detection on static attributed graphs. In NeurIPS.
27021–27035.
[28] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis.
2021. Anomaly detection on attributed networks via contrastive self-supervised
learning. IEEE Trans. Neural Netw. Learn. Syst. 33, 6 (2021), 2378–2392.[29] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan
Qi. 2019. GeniePath: Graph neural networks with adaptive receptive paths. In
AAAI. 4424–4431.
[30] Yingzhou Lu, Huazheng Wang, and Wenqi Wei. 2023. Machine learning for
synthetic data generation: A review. arXiv preprint arXiv:2302.04062 (2023).
[31] Calvin Luo. 2022. Understanding diffusion models: A unified perspective. arXiv
preprint arXiv:2208.11970 (2022).
[32] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, Hui
Xiong, and Leman Akoglu. 2021. A comprehensive survey on graph anomaly
detection with deep learning. IEEE Trans. Knowl. Data Eng. (2021).
[33] Xiaoxiao Ma, Jia Wu, Jian Yang, and Quan Z. Sheng. 2023. Towards graph-level
anomaly detection via deep evolutionary mapping. In KDD. 1631–1642.
[34] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-
sion probabilistic models. In ICML. 8162–8171.
[35] Joonhyung Park, Jaeyun Song, and Eunho Yang. 2021. GraphENS: Neighbor-
aware ego network synthesis for class-imbalanced node classification. In ICLR.
[36] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. 2018.
ANOMALOUS: A joint modeling approach for anomaly detection on attributed
networks. In IJCAI. 3513–3519.
[37] Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection:
Bridging review networks and metadata. In KDD. 985–994.
[38] Cedric Renggli, Luka Rimanic, Nezihe Merve Gürel, Bojan Karlaš, Wentao Wu,
and Ce Zhang. 2021. A data quality-driven view of MLOps. IEEE Data Eng. Bull.
44, 1 (2021), 11–23.
[39] Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, and Mihaela van der Schaar. 2023.
TRIAGE: Characterizing and auditing training data for improved regression. In
NeurIPS.
[40] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients
of the data distribution. In NeurIPS.
[41] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. 2023. GADBench:
Revisiting and Benchmarking Supervised Graph Anomaly Detection. In NeurIPS.
[42] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. 2022. Rethinking graph neural
networks for anomaly detection. In ICML. 21076–21089.
[43] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnet-
Miner: Extraction and mining of academic social networks. In KDD. 990–998.
[44] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph attention networks. In ICLR.
[45] Jianyu Wang, Rui Wen, Chunming Wu, Yu Huang, and Jian Xiong. 2019. FdGars:
Fraudster detection via graph convolutional networks in online app review
system. In WWW. 310–316.
[46] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong
Chen. 2021. Decoupling representation learning and classification for GNN-
based anomaly detection. In SIGIR. 1239–1248.
[47] Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng
Ma, Yu Sun, Dianhai Yu, Fang Dong, Jiahui Jin, et al .2023. Label information
enhanced fraud detection against low homophily in graphs. In WWW. 406–416.
[48] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In ICML. 6861–
6871.
[49] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Anomaly
transformer: Time series anomaly detection with association discrepancy. In
ICLR.
[50] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. 2022. Con-
trastive attributed network anomaly detection with data augmentation. In PAKDD.
444–457.
[51] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,
Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: A
comprehensive survey of methods and applications. ACM Comput. Surv. 56, 4
(2023), 1–39.
[52] Yuchen Zhang, Xiaoxiao Ma, Jia Wu, Jian Yang, and Hao Fan. 2024. Heterogeneous
subgraph transformer for fake news detection. In WWW. 1272–1282.
[53] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng
Jiang. 2020. Error-bounded graph anomaly loss for GNNs. In CIKM. 1873–1882.
[54] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021. GraphSMOTE: Im-
balanced node classification on graphs with graph neural networks. In WSDM.
833–841.
[55] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe
Chen. 2021. Generative and contrastive self-supervised learning for graph anom-
aly detection. IEEE Trans. Knowl. Data Eng. (2021).
[56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
Koutra. 2020. Beyond homophily in graph neural networks: Current limitations
and effective designs. In NeurIPS. 7793–7804.
 
2162Graph Anomaly Detection with Few Labels:
A Data-Centric Approach KDD ’24, August 25–29, 2024, Barcelona, Spain
A EXPERIMENT DETAILS AND ADDITIONAL
RESULTS
A.1 Datasets
The eight graph anomaly detection datasets can be categorized
into two groups: one with organic anomalies and the others with
injected anomalies.
Organic datasets. All the organic datasets contain real anom-
alies that appear in different applications including online reviews,
crowdsourcing and question-answering platforms.
YelpChi [37] is an online review dataset with spam and legitimate
reviews of hotels and restaurants. All reviews are labeled by Yelp.
There are three types of relations in this dataset and for a fair
comparison, we test each baseline on all these relations and report
the best performance. Table 2 reports the total number of edges.
Reddit [46] compromises collected posts from subreddit. The ground-
truth anomalies are banned users from Reddit.
Weibo [53] is collected from the Tencent-Weibo platform. The
ground-truth anomalies are users who have engaged in more than
five suspicious events.
Tolokers [41] is collected from the Toloka crowdsourcing platform.
In this dataset, anomalies denote workers who have been banned
in at least one project.
Questions [41] is from the Yandex Q website. Users of this question-
answering website are denoted as nodes and an edge is built be-
tween two users if one user’s question has been answered by an-
other. Anomalies are users that are no longer active after August
2022.
Synthetic datasets. These three synthetic datasets are down-
loaded from site5.
BlogCatalog [6] is collected from an online blog sharing network.
The edges denote the follower-followee relations among users and
tags associated with users are node attributes.
ACM [43] is built based on the ACM scientific citation network,
in which publications are represented as nodes and their citation
relations are represented as edges.
Cora [27] is another scientific citation network containing publica-
tions and their citation links.
5https://github.com/pygod-team/dataA.2 Baseline Implementations
A.2.1 Conventional GNN-based detectors. We have implemented
theGCN6[24],GAT7[44], and GraphSAGE8[12] detectors based
on their original settings. Specifically, each of them stacks a fully
connected layer above two GNN layers. The difference is that the
GCN detector investigates the graph convolutional neural network
layers, the GAT detector adopts the graph attention neural network
layers, and the GraphSAGE detector applies the GraphSAGE layers.
In our experiment, we follow CGenGA’s configurations and set
their GNN layers’ dimensions as 256and128, respectively. We apply
ReLU as the activation function for these three detectors and set
the number of training iterations to 200.
A.2.2 State-of-the-art methods. We use the published implemen-
tations and configurations of BWGNN9[42],GAT-sep [56],AM-
Net[4],GHRN10[8],CONAD11[50] and CoLA11[28].
A.3 Additional Results
A.3.1 Overall detection performance. Due to space limitations, in
addition to the results presented in Tables 3, 4, and 5, we report the
detection performance w.r.t. AUCPR in Table 8. All the tables can
be found on the following page, and results affirm the superiority
of our methods.
A.3.2 Comparison with other augmentation methods (RQ7). As to
validate the effectiveness of our generative approach and choice
of using the DDPM framework for graph generation, we compare
GenGA with VGAE and GraphENS and report the results in Table 9.
We can see that the generative methods, i.e., GenGA and VGAE
better boost the GCN, GAT and GraphSAGE detectors on four
representative datasets, while GenGA generates more effective
data samples than VGAE.
6https://github.com/tkipf/gcn
7https://github.com/PetarV-/GAT
8https://github.com/williamleif/GraphSAGE
9https://github.com/squareRoot3/Rethinking-Anomaly-Detection
10https://github.com/squareRoot3/GADBench
11https://github.com/pygod-team/pygod
 
2163KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaoxiao Ma et al.
Table 8: Improved performance on five organic datasets w.r.t. AUCPR.
Metho
d Y
elpChi Re
ddit W
eibo T
olokers Questions Cora BlogCatalog A
CM
GCN 21.21±0.8 4.48±1.1 92.09±1.6 42.04±2.9 10.14±2.5 38.94±6.2 66.91±5.7 39.35±4.0
+GenGA 25.18±1.4 23.24±3.6 93.04±0.8 44.31±0.8 25.65±0.6 42.21±8.1 68.89±1.6 40.89±3.2
+CGenGA 26.31±1.4 24.65±2.3 94.31±1.3 43.15±2.3 26.03±1.2 40.87±8.7 68.41±0.9 40.73±1.1
GA
T 41.32±4.1 5.83±0.5 92.65±1.9 46.19±4.8 9.63±1.5 29.25±4.2 42.04±3.7 42.24±7.1
+GenGA 41.96±0.6 42.18±1.2 93.28±0.5 79.35±1.7 41.82±1.2 29.52±1.2 44.77±1.9 52.06±1.2
+CGenGA 44.52±0.2 42.33±0.2 93.19±0.1 83.22±1.9 42.25±1.7 30.54±0.8 46.69±1.8 55.89±1.4
GraphSA
GE 53.61±3.2 6.07±0.7 89.27±2.3 49.02±1.7 12.19±3.2 52.04±7.9 67.60±8.8 57.81±3.0
+GenGA 56.00±0.9 55.05±1.7 92.29±1.9 54.70±1.7 53.56±0.9 55.47±1.7 71.45±0.3 60.04±1.1
+CGenGA 61.25±0.1 55.28±0.5 90.18±1.5 56.87±0.5 54.56±1.5 57.54±1.6 76.83±1.7 61.02±0.9
BW
GNN 54.60±3.7 5.79±1.6 89.94±1.5 48.57±3.6 12.95±1.2 7.33±1.9 13.97±2.9 17.13±3.7
+GenGA 58.85±0.5 57.69±1.1 90.13±1.3 57.68±0.5 58.71±0.7 12.50±0.6 26.55±1.2 19.39±0.6
+CGenGA 59.59±1.6 58.25±0.1 89.58±1.9 57.93±0.3 58.81±0.4 12.61±1.3 27.76±0.7 20.46±1.4
GA
T-sep 49.21±8.0 6.55±0.5 92.01±1.1 49.57±2.7 9.65±0.7 14.65±3.0 31.19±6.9 22.80±4.1
+GenGA 53.24±1.1 49.61±1.7 92.89±0.4 50.12±1.6 52.84±1.6 21.63±1.2 31.48±1.6 32.78±0.9
+CGenGA 53.93±0.6 49.81±0.5 93.97±0.5 54.48±1.7 52.93±1.1 23.05±0.7 31.78±0.7 34.98±0.9
AMNet 45.77±5.4 5.98±0.9 92.98±2.2 43.73±1.3 10.64±1.8 10.96±1.6 22.09±3.6 15.34±1.8
+GenGA 47.51±1.0 47.77±1.6 93.02±0.8 47.54±1.7 48.54±0.8 17.48±0.4 27.65±1.3 20.57±0.5
+CGenGA 51.14±0.2 48.14±0.9 93.31±1.4 51.32±0.4 49.54±1.4 17.64±0.6 29.47±1.7 20.98±1.8
GHRN 50.96±2.2 5.98±1.5 92.59±1.0 49.83±1.9 13.47±2.0 10.65±3.5 17.22±4.5 9.82±1.3
+GenGA 57.12±1.5 58.07±0.6 92.65±1.3 57.77±0.7 57.48±1.1 12.63±1.3 34.01±0.8 10.17±0.3
+CGenGA 58.73±1.6 58.22±1.5 90.35±0.3 61.75±0.6 57.74±1.1 13.05±1.6 35.31±0.4 10.97±0.5
CoLA 11.18±0.9 3.38±0.5 12.35±0.1 21.26±0.8 2.98±0.9 6.54±0.5 11.02±0.1 4.53±0.6
+GenGA 12.02±1.0 3.46±0.7 19.17±0.3 23.80±0.9 3.54±0.5 7.13±0.8 14.66±0.4 4.74±0.7
+CGenGA 11.72±0.9 3.44±0.5 12.82±0.3 23.25±0.9 3.04±0.9 7.55±0.5 12.80±0.9 4.78±0.3
Table 9: Comparison with VGAE and GraphENS on four datasets for enhancing GCN, GAT and GraphSAGE.
Metho
dBlogCatalog Cora
F1
AUC Pre Rec AUCPR F1
AUC Pre Rec AUCPR
GCN
+GenGA 76.24±0.2
77.55±0.5 87.88±0.6 74.95±0.3 68.89±1.6 69.28±0.2
74.05±0.2 68.66±0.2 71.08±0.2 42.21±8.1
+ VGAE 75.96±3.6
69.03±2.2 75.58±4.8 64.67±2.9 67.84±6.8 68.23±3.1
70.17±4.1 64.81±2.5 70.69±4.8 36.38±8.6
+ GraphENS 51.87±2.0
54.11±8.8 52.56±2.4 52.22±1.4 7.77±4.1 48.98±8.1
52.33±5.2 51.90±7.1 50.28±3.8 6.57±7.2
GAT
+GenGA 65.73±1.8
72.47±0.6 77.65±1.6 73.80±3.2 44.77±1.9 63.35±1.5
70.05±0.6 67.76±1.5 71.55±3.9 29.52±1.3
+ VGAE 62.08±3.1
68.35±1.1 75.22±3.2 68.98±4.1 34.33±4.7 60.24±0.8
67.01±5.5 65.31±1.9 69.21±3.6 20.69±2.9
+ GraphENS 51.24±1.6
46.44±1.1 53.27±6.1 51.57±7.1 7.93±7.8 47.67±2.0
55.25±6.3 52.37±2.7 51.86±2.3 7.37±3.9
GraphSAGE
+GenGA 70.43±2.2
73.04±3.2 75.45±1.8 72.24±1.6 71.45±0.3 68.60±3.1
68.74±0.5 75.92±2.1 65.20±3.4 55.47±1.7
+ VGAE 62.83±8.3
71.78±4.5 74.39±2.8 70.92±9.2 56.78±4.4 66.05±1/4
63.55±2.2 72.84±2.8 63.18±2.7 51.05±2.6
+ GraphENS 66.34±7.1
70.16±3.1 69.54±2.7 65.06±5.2 28.25±5.8 52.09±2.9
55.28±1.7 53.82±3.6 52.57±3.8 7.94±9.1
Metho
dY
elpChi Re
ddit
F1
AUC Pre Rec AUCPR F1
AUC Pre Rec AUCPR
GCN
+GenGA 51.68±2.2
60.15±1.0 56.35±0.5 56.08±1.9 25.18±1.4 50.99±1.2
64.95±0.5 51.49±0.2 59.58±3.3 23.24±3.6
+ VGAE 50.98±3.2
57.98±1.7 55.67±1.5 54.06±0.8 24.36±1.1 50.81±2.1
60.34±1.1 50.39±2.4 56.44±2.2 20.44±0.9
+ GraphENS 55.01±6.9
56.10±8.6 54.99±2.2 55.47±6.6 20.57±6.2 50.29±3.5
53.71±8.7 50.24±5.4 50.65±2.1 4.09±5.6
GAT
+GenGA 51.53±1.3
78.95±1.2 60.43±2.8 67.05±5.0 41.96±0.6 50.94±0.2
65.08±0.4 53.59±0.2 50.05±0.1 42.18±1.2
+ VGAE 50.40±2.2
76.26±1.1 58.02±0.8 65.93±0.7 41.54±2.6 50.29±1.6
62.80±1.4 52.15±0.6 49.95±0.9 40.65±3.3
+ GraphENS 58.28±9.8
60.92±1.3 58.97±6.5 57.93±5.3 24.79±7.9 50.09±3.5
50.48±9.2 49.95±3.2 50.52±9.7 3.77±2.1
GraphSAGE
+GenGA 67.51±0.9
83.74±0.6 65.76±0.6 75.19±1.1 56.00±1.0 62.00±0.1
70.79±0.2 53.19±0.1 60.12±3.0 55.05±1.8
+ VGAE 65.47±2.3
81.33±1.1 63.76±1.1 72.92±0.9 53.27±2.9 59.07±1.2
70.16±1.8 56.09±1.1 55.51±1.5 53.37±4.2
+ GraphENS 57.03±3.0
58.98±1.2 57.10±1.4 57.02±8.4 22.86±7.3 51.61±6.5
54.91±1.1 51.78±7.6 51.81±1.6 4.37±4.2
 
2164