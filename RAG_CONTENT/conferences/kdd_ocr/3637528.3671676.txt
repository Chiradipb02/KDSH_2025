CONFIDE: Contextual Finite Difference Modelling of PDEs
Ori Linial
linial04@gmail.com
Technion ‚Äì Israel Institute of
Technology
Haifa, IsraelOrly Avner
Bosch Center for Artificial
Intelligence
Haifa, IsraelDotan Di Castro
Bosch Center for Artificial
Intelligence
Haifa, Israel
ABSTRACT
We introduce a method for inferring an explicit PDE from a data
sample generated by previously unseen dynamics, based on a learned
context. The training phase integrates knowledge of the form of the
equation with a differential scheme, while the inference phase yields
a PDE that fits the data sample and enables both signal prediction
and data explanation. We include results of extensive experimenta-
tion, comparing our method to SOTA approaches, together with
ablation studies that examine different flavors of our solution.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning approaches.
KEYWORDS
Partial differential equations; Hybrid modelling; Physics informed
models; Time series modeling
ACM Reference Format:
Ori Linial, Orly Avner, and Dotan Di Castro. 2024. CONFIDE: Contextual
Finite Difference Modelling of PDEs. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24), August
25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671676
1 INTRODUCTION
Many scientific fields use the language of Partial Differential Equa-
tions (PDEs; 11) to describe the physical laws governing observed
natural phenomena with spatio-temporal dynamics. Typically, a
PDE system is derived from first principles and a mechanistic un-
derstanding of the problem after experimentation and data col-
lection by domain experts of the field. Well-known examples for
such systems include Navier-Stokes and Burgers‚Äô equations in fluid
dynamics, Maxwell‚Äôs equations for electromagnetic theory, and
Schr√∂dinger‚Äôs equations for quantum mechanics. Solving a PDE
model could provide users with crucial information on how a signal
evolves over time and space, and could be used for both prediction
and control tasks.
Creating PDE-based models holds great value, but it is a difficult
task in many cases. For some complex real-world phenomena, only
some part of the systems dynamics is known, such as its structure,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671676or functional form. For example, an expert might tell us that a
signal obeys the dynamics of a heat equation, without specifying
the diffusion and drift coefficient functions. We focus mainly on
this case, as explained in detail below.
The current process of solving PDEs over space and time is by us-
ing numerical differentiation and integration schemes. However, nu-
merical methods may require significant computational resources,
making the PDE solving task feasible only for low-complexity prob-
lems, e.g., a small number of equations. An alternative common
approach is finding simplified models that are based on certain
assumptions and can roughly describe the problem‚Äôs dynamics. A
known example for such a model are the Reynolds-averaged Navier-
Stokes equations [ 35]. Building simplified models is considered a
highly non-trivial task that requires special expertise, and might
still not represent the phenomenon to a satisfactory accuracy.
In recent years, with the rise of Deep Learning (DL; 22), novel
methods for solving numerically-challenging PDEs were devised.
These methods have become especially useful thanks to the rapid
development of sensors and computational power, enabling the
collection of large amounts of multidimensional data related to a
specific phenomenon. In general, DL based approaches consume
the observed data and learn a black-box model of the given problem
that can then be used to provide predictions for the dynamics. While
this set of solutions has been shown to perform successfully on
many tasks, it still suffers from two crucial drawbacks: (1) It offers
no explainability as to why the predictions were made, and (2) it
usually performs very poorly when extrapolating to unseen data.
In this paper, we offer a new hybrid modelling [ 20] approach
that can benefit from both worlds: it can use the vast amount of
data collected on one hand, and utilize the partially known PDEs
describing the observed natural phenomenon on the other hand. In
addition, it can learn several contexts, thus employing the general-
ization capabilities of DL models and enabling zero-shot learning
[32].
Specifically, our model is given a general functional form of the
PDE (i.e., which derivatives are used), consumes the observed data,
and outputs the estimated coefficient functions. Then, we can then
use off-the-shelf PDE solvers (e.g., PyPDE1) to solve and create
predictions of the given task forward in time for any horizon.
Another key feature of our approach is that it consumes the
spatio-temporal input signals required for training in an unsuper-
vised manner, namely the coefficient functions that created the
signals in the train set are unknown. This is achieved by combin-
ing an autoencoder architecture (AE; 15,19) with a loss defined
using the functional form of the PDE. As a result, large amounts of
training data for our algorithm can be easily acquired. Moreover,
our ability to generalize to data corresponding to a PDE whose
1https://pypde.readthedocs.io/en/latest/
 
1839
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
coefficients did not appear in the train set, enables the use of syn-
thetic data for training. Although our approach is intended to work
when the PDE functional form is known, it is not limited to that
scenario only. In cases where we are given a misspecified model
(when experts provide a surrogate model for instance), our model
can eliminate some of the discrepancies using the extra function
that is not a coefficient of one of the derivatives (the ùëù0(ùë•,ùë°,ùë¢)
function in (1)).
On the technical side, we chose to apply a finite difference ap-
proach in order to integrate the knowledge regarding the structure
of the PDE family. This approach enables us to consume training
data without requiring the corresponding boundary conditions.
A natural question for this setup is whether we are able to extract
the ‚Äúcorrect‚Äù coefficients for the PDE. The answer depends on the
identifiability of the system, a trait that does not hold for many
practical scenarios. We therefore focus on finding the coefficients
that best explain the data, making prediction of the signal forward
in time possible. Practitioners will find the estimated coefficients
useful even if they are not exact, since they may convey the shape,
or dynamics, of unknown phenomena.
Our motivation comes from the world of electric vehicle batter-
ies, where PDEs are used to model battery charging, discharging
and aging. For a specific type of battery, the set of equations has a
common form, with different coefficients for each battery instance.
The data describing battery dynamics is gathered by battery man-
agement systems in the vehicle, and also in the lab. It is then used
to calibrate the equation-based model, in order to later generate
predictions and analyze battery behavior. Traditional techniques for
model calibration are based on direct optimization, and suffer from
two drawbacks: (1) they are extremely time consuming, (2) they do
not leverage data from one battery in the dataset to another. Our
approach solves both issues: model calibration is achieved by infer-
ence rather than optimization, and the learned context facilitates
transfer of knowledge between batteries. The first improvement is
straightforward, and the second one stems from the context-based
architecture we introduce. This architecture enables us to estimate
the coefficients of a given battery based on a smaller amount of
data when compared to the traditional approach.
We summarize our contribution as follows:
(1)Harnessing the information contained in large datasets be-
longing to a phenomenon which is related to a PDE func-
tional family in an unsupervised manner. Specifically, we
propose a regression based method, combined with a finite
difference approach.
(2)Proposing a DL encoding scheme for the context conveyed
in such datasets, enabling generalization for prediction of
unseen samples based on minimal input, similarly to zero-
shot learning.
(3)Extensive experimentation with the proposed scheme, ex-
amining the effect of context and train set size, along with a
comparison to different previous methods.
The paper is organized as follows. In Section 2 we review related
work. In Section 3 we present the proposed method and in Section 4
we provide experiments to support our method. Section 5 completes
the paper with conclusions and future directions.2 RELATED WORK
Creating a neural-network based model for approximating the solu-
tion of a PDE has been studied extensively over the years, and dates
back more than two decades [ 21]. We divide deep learning based
approaches by their ability to incorporate mechanistic knowledge
in their models, and by the type of information that can be extracted
from using them. Another distinction between different approaches
is their ability to handle datasets originating from different contexts.
From a PDE perspective, a different context could refer to having
data signals generated with different coefficients functions ( ùëùùëôin
(1)). In many real-world applications, obtaining observed datasets
originating from a single context is impractical. For example, in
cardiac electrophysiology [ 31], patients differ in cardiac parameters
like resistance and capacitance, thus representing different con-
texts. In fluid dynamics, the topography of the underwater terrain
(bathymetry) differs from one sample to another [13].
The first line of work is purely data-driven methods. These mod-
els come in handy when we observe a spatio-temporal phenomenon,
but either don‚Äôt have enough knowledge of the underlying PDE
dynamics, or the known equations are too complicated to solve
numerically (as explained thoroughly by Wang and Yu [44]). Re-
cent advances demonstrate successful prediction results that are
fast to compute (compared to numerically solving a PDE), and also
provide decent predictions even for PDEs with very high dimen-
sions [ 4,12,14,23,29,33,46]. However, the downside of these
approaches is not being able to infer the PDE coefficients, which
may hold valuable information and explanations as to why the
model formed its predictions.
The second type of data-driven methods are approaches that uti-
lize PDE forms known beforehand to some extent. Works that adopt
this approach can usually utilize the given mechanistic knowledge
and provide reliable predictions, ability to generalize to unseen data,
and in some cases even reveal part of the underlying PDE coefficient
functions. However, their main limitation is that they assume the
entire training dataset is generated by a single coefficient function
and only differ in the initial conditions (or possibly boundary condi-
tions). PDE-NET [ 28], its followup PDE-NET2 [ 27], DISCOVER [ 10],
PINO [ 24] and sparse-optimization methods [ 37,38] (expanding
the idea originally presented on ODEs [ 5,6]), are not given the PDE
system, but instead aim to learn some representation of the under-
lying PDE as a linear combination of base functions and derivatives
of the PDE state. PINN [ 34] and NeuralPDE [ 48] assume full knowl-
edge of the underlying PDE including its coefficients, and aim to
replace the numerical PDE solver by a fast and reliable model. They
also provide a scheme for finding the PDE parameters as scalars,
but assume the entire dataset is generated by a single coefficient
value, while we assume each sample is generated with different
coefficient values which could be functions of time, space and state
(as described in (1)). In N√©giar et al . [30] , the authors incorporate
knowledge of the PDE structure as a hard constraint while learning
to predict the solution to the PDE. Similarly, Learning-informed
PDEs [ 1,9] suggest a method that assumes full knowledge of the
PDE derivatives and their coefficient functions, and infers the free
coefficient function (namely ùëù0(ùë•,ùë°,ùë¢)in(1)). In [ 25], the authors
apply a finite difference approach to PINNs. Another approach for
learning the solution to PDEs, that also uses a neural representation,
 
1840CONFIDE: Contextual Finite Difference Modelling of PDEs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
(a)
 (b)
Figure 1: (a) Inference process: given an observed spatio-temporal signal, CONFIDE estimates the PDE coefficients that best
describe it. These can be plugged into a PDE solver together with the known operator form ùêπ, and an initial condition (dashed
line) to obtain a prediction of the signal for future time-steps. (b) Training process: In each iteration, CONFIDE observes a set
of signals generated by the same family of PDEs. For each train signal, CONFIDE evaluates the PDE coefficients best describing
the observed signal, and all the spatio-temporal derivatives that are known to be in the functional form of the PDE (e.g.,ùúïùë¢
ùúïùë°,
ùúï2ùë¢
ùúïùë•2, ...). The derivatives and coefficients are then plugged into the operator ùêπwhich is then used to minimize the functional
loss (as in Eq. (3)) and train a context-based coefficient estimator.
is introduced in [ 7]. Recently, in [ 41], the authors have suggested a
Foundation Model framework for predicting solutions of PDEs.
The last line of work, and closer in spirit to ours, includes context-
aware methods that assume some mechanistic knowledge, with
each sample in the train set generated by different PDE coefficients
(we also refer to this concept as having different context) and initial
conditions. CoDA [ 17] provides the ability to form predictions of
signals with unseen contexts, but does not directly identify the
PDE parameters. GOKU [ 26] and ALPS [ 45] provide context-aware
inference of signals with ODE dynamics, when the observed sig-
nals are not the ODE variables directly. Another important paper
introduces the APHYNITY algorithm [ 47], which also presents an
approach to inferring PDE parameters from data. This work han-
dles the scenario of fixed coefficients, as opposed to our ability to
handle coefficients that are functions. Also, the case of coefficients
that differ between samples is addressed only briefly, with a fixed,
rather high, context ratio.
In Section 4 we present comparisons to several carefully se-
lected baselines mentioned above. The first approach is Neural-
ODE [ 8] (also referred as Latent-ODE in its follow-up paper [ 36]
when prompted to solve a time-series prediction task). The Neural-
ODE approach infers the initial conditions of an arbitrary latent
trajectory from some high dimension, integrate it through time
by assuming it follows a learnable dynamics function, and outputs
the future predictions of the observed signal. Specifically, a given
PDE could be considered as a time series from high dimension, and
Neural-ODE should be able to learn its dynamics, and provide futurepredictions. The second and third approaches are Fourier-Neural-
Operators (FNO) [ 23] and UNet [ 12] which are designed to provide
a faster alternative to solving a PDE using a classic PDE-solver.
Both approaches expand the Deep-Operator-Network (DeepONet)
[29] work, where a the model learns a mappings between spaces
of functions. In the PDE prediction task, one example is that they
learn the connection between the PDE initial conditions and the
solution at some required time ùë°=ùúè. The last baseline we compare
to is DINo [ 46]. In this approach, the model applies a scheme com-
bining Neural-ODEs with Implicit Neural Representations, which
are implemented as FourierNets, to PDE forecasting tasks.
3 METHOD
The data we handle is a set of spatio-temporal signals generated by
an underlying PDE, only the form of which is known. The coeffi-
cient functions determining the exact PDE are unknown and may
be different for each collection of data. Our goal is to estimate these
coefficient functions and provide reliable predictions of the future
time steps of the observed phenomenon. The proposed method
comprises three subsequent parts: (1) Creating a compact repre-
sentation of the given signal, (2) estimating the PDE coefficients,
and (3) solving the PDE using the acquired knowledge. For ease of
exposition we focus on parabolic PDEs in this section, however the
extension to other types of PDEs is straightforward.
 
1841KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
3.1 Problem Formulation
We now define the problem formally. Let ùëà(ùë•,ùë°)denote a spatio-
temporal function defined over some compact spatial domain Œ©‚äÜ
Rùëë, whereùëëis the number of spatial variables, and a temporal
domain R.ùëà(ùë•,ùë°)maps between points in the spatial domain ùë•‚ààŒ©
at some point in time ùë°to anùëõ-dim vector, where ùëõis the number
of observed variables. In other words, ùëà(ùë•,ùë°):Œ©√óR‚ÜíRùëõ.
In addition, the initial value of the function ùëà(ùë•,ùë°=0)changes
between observed signals, therefore sampled from some unknown
probability function ùëà(ùë•,ùë°=0)‚àºùëÉùë¢0. An observed signal ùë¢(ùë•,ùë°)
is therefore a projection of the function ùëà(ùë•,ùë°)on a finite discrete
observation grid and on discrete times. In our formulation, we
make the problem harder by considering the case where each signal
ùë¢(ùë•,ùë°)could also originate from a PDE with different coefficients,
thus differing between observed signals. This means that unlike
most related works, in this work we assume that every observed
signal corresponds to a different instance of the PDE family. We refer
to the signals with different coefficients as ùë¢ùëê(ùë•,ùë°), whereùëêstands
forcontext, which changes between observed signals. For example,
we might know that ùë¢ùëê(ùë•,ùë°)follows the Navier-Stokes equations,
but some coefficients might change between observations (like the
viscosity of the fluid).
Generally,ùë¢ùëê(ùë•,ùë°)could obey any PDE, but in this work we
focus on parabolic PDEs, hence the signal ùë¢ùëê(ùë•,ùë°)is the solution
of aùëò-th order PDE of the general form
ùúïùë¢
ùúïùë°=ùëò‚àëÔ∏Å
ùëô=1ùëùùëô(ùë•,ùë°,ùë¢)ùúïùë¢ùëô
ùúïùë•ùëô+ùëù0(ùë•,ùë°,ùë¢), (1)
with a vector of coefficient functions ùëù=(ùëù0,...,ùëùùëò). We adopt
the notation of Wang and Yu [44] and refer to a family of PDEs
characterized by a vector ùëùas an operator ùêπ(ùëù,ùë¢), where solving
ùêπ(ùëù,ùë¢)=0yields solutions of the PDE.
The problem we solve is as follows: given an observed signal
ùë¢ùëê(ùë•,ùë°), at timesùë°=0,...,ùë° 0that solves a PDE of a known operator
ùêπwith an unknown coefficient vector ùëù, we would like to (a) estimate
the coefficient vector ÀÜùëùand (b) predict the signal at future times
ùë°=ùë°0,...,ùëá , for someùëá>ùë°0.
Our solution is a concatenation of two neural networks, which
we call CONFIDE. Its input is an observed signal ùë¢ùëê(ùë•,ùë°=0,...,ùë° 0),
and its output is a vector ÀÜùëù. We feed this vector into an off-the-
shelf PDE solver together with the operator ùêπ(ùëù,ùë¢)to obtain the
predicted signal ÀÜùë¢(ùë•,ùë°=ùë°0,...,ùëá). An explanation of our numerical
scheme appears in Section A.
3.2 CONFIDE Inference
We begin by outlining our inference process, presented in Fig. 1a.
The input to this process is an observed signal ùë¢ùëê(ùë•,ùë°), defined
on some compact spatial domain Œ©, for timesùë°‚àà[0,ùë°0]and an
operatorùêπThe input is fed into the CONFIDE component, which
generates the estimated coefficients ÀÜùëù. For example, taking ùëò=2
in the example in (1)results in a coefficient vector ÀÜùëù=(ÀÜùëé,ÀÜùëè,ÀÜùëê).
The PDE solver then uses this estimate to predict the complete
signal, ÀÜùë¢(ùë•,ùë°),ùë•‚ààŒ©,ùë°‚àà[ùë°0,ùëá]. An important feature of our ap-
proach is the explicit prediction of the coefficient functions, which
contributes to the explainability of the solution.The observation ùë¢ùëê(ùë•,ùë°=ùë°0)serves as an initial condition for
the prediction and also represents the dynamics of the signal for
estimating the PDE coefficients. In the sequel we refer to it as
‚Äúcontext‚Äù. The ratio of between the observed times and the required
prediction time is denoted by ùúå, such thatùë°0=ùúåùëá, and is a hyper-
parameter of our algorithm.
Algorithm 1 CONFIDE inference scheme
Input: observation ùë¢ùëê(ùë•,ùë°), operatorùêπ, trained networks: en-
coderùëîùúô, coefficient estimator ‚Ñéùúî
ÀÜùëù‚Üê‚Ñéùúî(ùëîùúô(ùë¢ùëê))
ÀÜùë¢‚ÜêPDE_solve(ùêπ,ÀÜùëù,ùë¢ùëê(ùë•,ùë°=ùë°0))
return ÀÜùë¢,ÀÜùëù
Algorithm 2 Algorithm for training CONFIDE
Input: datasetD, operatorùêπ, timeùë°0, loss weight ùõº, number of
epochsùëÅùëí
Init: random weights in encoder ùëîùúô, decoderùëìùúÉ, coefficient esti-
mator‚Ñéùúî
forepoch inùëÅùëído
L‚Üê 0
{ùë¢ùëê
ùëñ}ùëÅ
ùëñ=1‚ÜêRandom batch of ùëÅobservations from D
forùë¢ùëê
ùëñin batch do
ÀÜùëùùëñ‚Üê‚Ñéùúî(ùëîùúô(ùë¢ùëê
ùëñ))
LAE‚Üê(ùë¢ùëê
ùëñ‚àíùëìùúÉ(ùëîùúô(ùë¢ùëê
ùëñ),ùë¢ùëñ(ùë°=0)))2
ùúè‚ÜêRandom value from [0,ùë°0]
Lcoef‚Üêùêπ(ÀÜùëùùëñ,ùë¢ùëê
ùëñ(ùë°=ùúè))2
L‚ÜêL+ùõº¬∑LAE+(1‚àíùõº)¬∑L coef
end for
ùúô,ùúÉ,ùúî‚Üêarg minL
end for
3.3 CONFIDE Training
The training process is presented in Fig. 1b. Its input is a dataset D
that consists of ùëÅsignals{ùë¢ùëê
ùëñ(ùë•,ùë°)}ùëÅ
ùëñ=1, which are solutions of ùëÅ
PDEs that share an operator ùêπbut have unique coefficient vectors
{ùëùùëñ}ùëÅ
ùëñ=1. We stress that the vectors ùëùùëñare unknown even at train time.
The signals are defined on some domain Œ©, and for times ùë°‚àà[0,ùë°0].
The loss we minimize is a weighted sum of two components: (i) the
autoencoder reconstruction loss , which is defined in (2), and (ii)
the functional loss as defined in (3).
CONFIDE comprises two parts: (1) an encoder and (2) a coef-
ficient estimator. The encoder‚Äôs goal is to capture the dynamics
driving the signal ùë¢ùëñ, thus creating a compact representation for
the coefficient estimator. The encoder is trained on signals ùë¢ùëê
ùëñin
the train set. Each signal is of size ùë°0√óamount of spatial points (e.g.,
forŒ©=[0,ùêø], the size isùë°0¬∑ùêøpoints).
The encoder loss is the standard AE reconstruction loss, namely
the objective is
min
ùúÉ,ùúôLAE=min
ùúÉ,ùúôùëÅ‚àëÔ∏Å
ùëñ=1loss(ùë¢ùëê
ùëñ‚àíùëìùúÉ(ùëîùúô(ùë¢ùëê
ùëñ))), (2)
 
1842CONFIDE: Contextual Finite Difference Modelling of PDEs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
20 40 60 80 100
t105
103
101
MSE
CONFIDE
NeuralODE
FNO
Unet
DINO
(a)
0.0 0.5 1.0 1.5 2.0
Ground truth0.00.51.01.52.0Estimation (b)
Figure 2: Constant coefficients (Section 4.1). (a) Prediction error vs. prediction horizon, for different algorithms. CONFIDE, in
red, is our approach. (b) Estimated value of the ùúï2ùë¢/ùúïùë•2coefficient vs. ground truth, for test set ( ùëÖ2=0.93).
whereùëìùúÉis the decoder, ùëîùúôis the encoder and loss(¬∑,¬∑)is a standard
loss function (e.g., ùêø2loss).
The second component is the coefficient estimator, whose input
is the encoded context and the signal The estimated coefficients
output by this component, together with the operator ùêπ, and the
signal at at some random time ùë°=ùúè‚àà[0,ùë°0], form the functional
objective:
minùúîLcoef=minùúîùëÅ‚àëÔ∏Å
ùëñ=1ùêπ(ÀÜùëùùúî,ùë¢ùëê
ùëñ(ùë•,ùúè)2, (3)
whereùúîrepresents the parameters of the coefficient estimator
network, and ÀÜùëùis the estimator of ùëùat timeùúè, acquired by applying
the network ‚Ñéùúîto the output of the encoder. This design enables
CONFIDE to learn a parameter vector which can depend on time,
space, and the observation ùë¢.
The two components are trained simultaneously, and the total
loss is a weighted sum of the losses in (2)and(3):L=ùõº¬∑LAE+
(1‚àíùõº)¬∑L coef, whereùõº‚àà(0,1)is a hyper-parameter.
Initial-conditions aware autoencoder. To further aid our model
in learning the underlying dynamics of the observed phenome-
non, we include the observed initial conditions of the signal (i.e.,
ùë¢ùëñ(ùë°=0)) along with the latent context vector (i.e., ùëîùúô(ùë¢ùëê
ùëñ)) as in-
put to the decoder network. This modification enables the model
to learn a context vector that better represents the dynamics of
the phenomenon, rather than other information such as the actual
values of the signal.
We experimented with removing the decoder and training the
networks using the functional loss alone, and without including the
initial conditions as an input to the decoder. In both cases, results
proved to be inferior, suggesting that the autoencoder loss helps
the model to focus on the underlying dynamics of the observed
signal.To summarize this section, we present the inference scheme in
Algorithm 1, and the full training algorithm in Algorithm 2.
4 EXPERIMENTS
We devote this section to analyse and compare our approach to
other solutions, on four different systems of PDEs: (1) constant
coefficients, (2) Burgers‚Äô equations, (3) 2D-FitzHugh-Nagumo, and
(4) 2D-Navier-Stokes equation. For each PDE task, we created a
dataset of signals generated from a PDE with different coefficients.
We could not use off-the-shelf datasets, such as those appearing
in PDEBench [ 42], since each of the datasets there is generated
from a single constant function (i.e., all data samples have the same
context). We used well-known equations, therefore our datasets
can serve as a benchmark for the emerging field of contextual
PDE modelling. We stress the fact that the test set contains signals
generated by PDEs with coefficient vectors that do not appear in
the training data, hence demonstrating different dynamics than the
ones the model observed during training. In that sense, the task at
hand is a zero-shot prediction problem. More information about
dataset creation can be found in the appendix.
We benchmark the performance of CONFIDE against several
state of the art approaches:
(1)Neural ODE, based on the algorithm suggested by Chen et al .
[8], Section 5.1 (namely, Latent ODE).
(2) Fourier Neural Operator (FNO), introduced by Li et al. [23].
(3) U-Net, as presented by Gupta and Brandstetter [12].
(4) DINo, as presented by Yin et al. [46].
Additional details regarding the implementation of baselines can
be found in Section B.2.
 
1843KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
CONFIDE
t=20
t=50
t=75
t=100 ZoomedNeuralODE
 FNO
 Unet
xDINO
x
 x
 x
 x
(a)
0 20
xu/x coefficient
t=30
0 20
x
t=60
0 20
x
t=90
GT
CONFIDE (b)
Figure 3: Burgers‚Äô PDE: (a) A solution of the Burgers‚Äô equation. The black plot in each figure displays the ground truth. Rows
correspond with the predicted solution by the respective algorithm (top row for CONFIDE displayed in red). Each column
shows the solution at a different time point. The rightmost column shows the solution at t = 100 zoomed to demonstrate the
differences. (b) Estimation of the coefficient function ùëè(ùë•,ùë°,ùë¢)of the Burgers‚Äô equation from (5). CONFIDE manages to accurately
estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of ùúå=0.2.
4.1 Second Order PDE with Constant
Coefficients
The first family of PDEs used for our experiments is:
ùúïùë¢
ùúïùë°=ùëéùúï2ùë¢
ùúïùë•2+ùëèùúïùë¢
ùúïùë•+ùëê, (4)
whereùëù=(ùëé,ùëè,ùëê)are constants but differ between signals. Fig-
ure 2a demonstrates the clear advantage of our approach, which
increases with the prediction horizon (note the logarithmic scale of
the vertical axis, representing the MSE of prediction). Since CON-
FIDE harnesses both mechanistic knowledge and training data, it
is able to predict the signal ÀÜùë¢(ùë•,ùë°)several timesteps ahead, while
keeping the error to a minimum.
Another result for this set of experiments appears in Figure 2b.
Here, we plot the estimated value of parameter ùëéof(4), against its
true value. The plot and the high value of ùëÖ2demonstrate the low
variance of our prediction, with a strong concentration of values
along theùë¶=ùë•line.
Section 4.7 presents the results of an ablation study on the hyper-
parameters of CONFIDE for this equation.
4.2 Burgers‚Äô equation
Another family of PDEs we experiment with is the quasi-linear
Burgers‚Äô equation, whose general form is
ùúïùë¢
ùúïùë°=ùëéùúï2ùë¢
ùúïùë•2+ùëè(ùë¢)ùúïùë¢
ùúïùë•, (5)
whereùëè(ùë•,ùë°,ùë¢)=‚àíùë¢, as presented in [ 3]. We note that this equation
is quasi-linear since its drift coefficient ùëè(ùë•,ùë°,ùë¢)depends on the
solutionùë¢itself. The dataset for our experiments consists of 10000signals with different values of ùëéand the same ùëè(ùë¢)=‚àíùë¢, both
unknown to the algorithm a priori. We begin with a demonstration
of a signalùë¢(ùë•,ùë°)and its prediction ÀÜùë¢(ùë•,ùë°)in Figure 3a. As can be
seen both visually and from the value of the MSE (in each panel‚Äôs
title), our approach yields a prediction that stays closest to the
ground truth (GT), even as the prediction horizon (vertical axis)
increases.
In Figure 3b we focus on the ability to accurately predict coef-
ficient functions with spatio-temporal dynamics, in this case: the
coefficientùëè(ùë•,ùë°,ùë¢)of(5). The panels correspond to different points
in time, showing that the coefficient estimator tracks the temporal
evolution successfully.
4.3 FitzHugh-Nagumo equations
The next family of PDEs we examine is the FitzHugh-Nagumo PDE
[18] consisting of two equations:
ùúïùë¢
ùúïùë°=ùëéŒîùë¢+ùëÖùë¢(ùë¢,ùëò,ùë£),ùúïùë£
ùúïùë°=ùëèŒîùë£+ùëÖùë£(ùë¢,ùë£), (6)
whereùëéandùëèrepresent the diffusion coefficients of ùë¢andùë£, andŒî
is the Laplace operator. For the local reaction terms, we follow Yin
et al. [47] and setùëÖùë¢(ùë¢,ùëò,ùë£)=ùë¢‚àíùë¢3‚àíùëò‚àíùë£, andùëÖùë£(ùë¢,ùë£)=ùë¢‚àíùë£.
The PDE state is(ùë¢,ùë£), defined on the 2-D rectangular domain (ùë•,ùë¶)
with periodic Neumann boundary conditions.
The dataset created for this task consists of 1000 signals, each
with a different value of ùëò. We compare the prediction generated
by CONFIDE to those yielded by other approaches, and present a
typical result in Fig. 4. In Fig. 5 we present the prediction error as a
function of the prediction horizon, once again comparing CONFIDE
to the baselines.
 
1844CONFIDE: Contextual Finite Difference Modelling of PDEs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
GTt=60
t=80
t=110
t=140
t=170
t=200
CONFIDE
 Neural-ODE
 FNO
 Unet
 DINO
Figure 4: 2D-FitzHugh-Nagumo PDE: Figures in the top row
show the ground truth of ùëÖùë£for different time points, and
the rows below show the estimation of it by the different ap-
proaches. CONFIDE estimates ùëÖùë£directly and near-perfectly
recovers the unknown part of the PDE even as the prediction
horizon increases. For the other algorithms we evaluated
ùëÖùë£=ùë¢‚àíùë£from the predictions of ùë¢andùë£.
50 70 90 110 130 150 170 190
t105
103
101
MSE
CONFIDE
NeuralODE
FNO
Unet
DINO
Figure 5: 2D-FitzHugh-Nagumo PDE: prediction error as hori-
zon increases, for different approaches.
4.4 Navier-Stokes equation
For the last family of PDEs, we follow Yin et al . [46] and examine the
Navier-Stokes equations [ 39] which correspond to incompressible
fluid dynamics and have the form of
ùúïùë§
ùúïùë°=‚àíùë¢‚àáùë§+ùúàŒîùë¢+ùëì, ùë§ =‚àá√óùë¢,‚àáùë¢=0, (7)
whereùë¢is the velocity field, ùë§the vorticity, ùúàthe unknown vis-
cosity coefficient and ùëìis a constant forcing term. The PDE state
ùë§is defined over the 2-D rectangular domain (ùë•,ùë¶)with periodic
GTt=60
t=80
t=110
t=140
t=170
t=200
CONFIDE
 Neural-ODE
 FNO
 Unet
 DINO
Figure 6: 2D-Navier-Stokes: Figures in the top row show the
ground truth, i.e., the PDE state ùë§for different time points,
while the rows below show its estimation by the different
approaches. All approaches observe the first 50 time points
and predict the next 150. CONFIDE near-perfectly predicts
the given signal even when the horizon increases.
boundary conditions. The dataset created for this task consists of
1000 signals, with different values of the viscosity ùúà. We note that
typically [ 12,23,46], the viscosity coefficient is treated as having a
single constant value among all signals in the dataset. In this work
we increase the task difficulty by creating a dataset comprised of
signals that have different viscosity values. For each signal in the
dataset (both train and test) we sample a different viscosity value
uniformly from ùúà‚àºùëà[1,2]¬∑10‚àí3. We compare the prediction gen-
erated by CONFIDE to other baselines and present a typical result
in Fig. 6. In Fig. 7 we present the prediction error as a function of
the prediction horizon.
We summarize the results of experiments for signal prediction
across all setups and approaches in Table 2. The table includes
results for CONFIDE, all baselines, and also a variant of CONFIDE
which we refer to as CONFIDE-0. This zero-knowledge variant is
applicable when we know that the signal obeys some differential
operatorùêπ, but have no details regarding the actual structure of ùêπ.
Thus, CONFIDE-0 does not estimate the equation parameters, and
only yields a prediction for the signal, utilizing our context-based
architecture. We elaborate further in Appendix B.2. We note that
Neural-ODE and DINO, which are integration-based approaches,
converged to a solution resembling the average of the observed
signal without any dynamics evolution over time. This issue has
also been demonstrated and discussed in several other related works
[2, 16, 43].
4.5 Out-Of-Distribution Data
In this subsection we provide additional experiments conducted on
out-of-distribution (OOD) data. These experiments were selected
to demonstrate how CONFIDE can handle observations that are
significantly different than the data in the train set. We divide the
 
1845KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
Table 1: Coefficient estimation error for different experi-
mental setups: constant coefficients, Burgers‚Äô equation, two-
dimensional FitzHugh-Nagumo and two-dimensional Navier-
Stokes. The variance is calculated over the entire test set,
namely 1000 signals for the first two setups and 100 signals
for the last two.
Setup Coefficient estimation error
Constant coeff. 0.0095¬±0.0131
Burgers‚Äô 0.0454¬±0.0333
FitzHugh-Nagumo-2D 0.0075¬±0.0123
Navier-Stokes-2D 1.5¬∑10‚àí8¬±1.0¬∑10‚àí8
50 70 90 110 130 150 170 190
t105
103
100MSE
CONFIDE
NeuralODE
FNO
Unet
DINO
Figure 7: 2D-Navier-Stokes PDE: prediction error as horizon
increases, for different approaches.
OOD experiments into two parts: (1) the initial conditions observed
are not smooth and have some discontinuity, and (2) the parameters
used to generate the signals in the test set are sampled from a
different distribution than the one used in the train set.
Non-smooth initial conditions. In this first benchmark, we
demonstrate how CONFIDE handles the case where the test data
has a discontinuity point in the test set, while it was trained on
continuous data only. The importance of this test is mainly because
CONFIDE evaluates the spatio-temporal derivatives of the signal
numerically using a finite-differences approach. This computation
might result in very high derivatives in these non-smooth locations
and interfere with the ability of the algorithm to provide reliable
predictions. For this task, we generated a new test set based on
the Burgers‚Äô equation experiment, but the initial conditions are
sampled to demonstrate discontinuity in ùë¢(ùë°=0,ùë•=ùêø/2). We
stress that the train-set is still the original one, since our goal is
to test whether CONFIDE is able to handle OOD data, which, in
this case, comes in the form of OOD initial conditions. As shown in
Table 2, CONFIDE‚Äôs prediction error remains low, suggesting that
it successfully predicts the given observations and scores close to
the original score on the original burgers‚Äô test-set.OOD coefficients. In the second benchmark, we demonstrate
how CONFIDE handles the case where the observed signal in the
test set is generated from a PDE with coefficients that come from a
distribution different from the ones in the train set. For this task, we
generated a new test set based on the Burgers‚Äô equation experiment,
where the coefficient ùëéis sampled from ùë¢‚àºùëà[2,4]instead of
ùë¢‚àºùëà[1,2]as in the train set. This modification in the coefficient
distribution, results in generated signals that might be significantly
different than the ones observed in the train set. As shown in
Table 2, CONFIDE continues to provide good results compared
to other baselines. We note that since CONFIDE learns to output
coefficients only in the range of the coefficients in the train set, it
projects the observed signal to the range of coefficients in the train
set so that it best describes the observed signal.
AlgorithmPrediction MSE
OOD initial conditionsPrediction MSE
OOD coefficients
CONFIDE 0.0010¬±0.0012 0 .0074¬±0.0100
Neural-ODE 0.0133¬±0.0208 0 .0423¬±0.0649
FNO 0.9367¬±0.2322 0 .9646¬±0.3304
Unet 0.0015¬±0.0024 0 .0096¬±0.0121
Table 3: Results summary on the two OOD benchmarks for
different approaches.
4.6 CONFIDE vs Neural ODE (Finite-Differences
vs Integration)
A key part in the CONFIDE algorithm is its use of finite-differences
to evaluate the spatio-temporal derivatives. As we demonstrate,
CONFIDE can successfuly use this approach to provide both coef-
ficient estimation and reliable predictions. However, another im-
portant advantage that a finite-differences approach might have
over ‚Äúintegration‚Äù approaches (such as the adjoint method used in
Neural-ODE [ 8] and DINo [ 46]) is the amount of time required for
training the model.
To demonstrate this effect we created a toy example based on
an ODE of a frictionless single pendulum: ¬•ùúÉ=‚àíùëî/ùëô¬∑sin(ùúÉ), where
ùëîis the gravitational parameter, ùúÉis the angle of the pendulum,
andùëôis the length of the pendulum. The two algorithms compared
are CONFIDE and an ODE-aware version of Neural-ODE, where
it is also given the ODE form of the pendulum. Both algorithms
are presented with the same information and have the same goals:
estimating the length of the pendulum on a given signal ùëô, and
predicting the future of the observed signal at ùë°>ùëá.
CONFIDE evaluates the time derivative of the observed signal
ùëë2ùúÉ/ùëëùë°2via finite-differences, and forces the derivative to be similar
to the rest of the ODE. Neural-ODE uses the initial value ( ùúÉ(ùë°=0))
andÀÜùëôto generate the signal ÀÜùúÉ(ùë°=0,...,ùëá)by integrating via an
ODE-solver, and then optimizes the generated signal to match the
observed one. We trained both algorithms using the exact same
setting, and observed that not only were CONFIDE‚Äôs results bet-
ter, but it also trained significantly faster. Specifically, CONFIDE‚Äôs
training time was 3.6 seconds, and Neural-ODE‚Äôs training time was
159.2 seconds, making CONFIDE ‚àº44times faster.
 
1846CONFIDE: Contextual Finite Difference Modelling of PDEs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 2: Result summary for the signal prediction task, on all four PDE systems, together with two OOD experiments. The
numbers represent signal prediction error at the end of the prediction horizon, averaged over the entire test set.
MethodConstant
coefficients Burgers‚ÄôFitzHugh-
NagumoNavier-
StokesBurgers‚Äô OOD
Initial conditionsBurgers‚Äô OOD
Coefficients
CONFIDE 0.0023¬±0.0036 0.0008¬±0.0011 0.0083¬±0.0177 0.0033¬±0.0027 0.0010¬±0.0012 0.0074¬±0.0100
CONFIDE-0 0.0079¬±0.0218 0.0009¬±0.0016 0.0845¬±0.0978 0.0173¬±0.0355 0.0020¬±0.0022 0.0057¬±0.0091
Neural-ODE 0.0680¬±0.0905 0.0272¬±0.0627 0.2944¬±0.2293 0.1334¬±0.1391 0.0133¬±0.0208 0.0423¬±0.0649
FNO 0.0538¬±0.0680 0.9351¬±0.3091 2.5727¬±17.732 4.4223¬±5.5352 0.9367¬±0.2322 0.9646¬±0.3304
Unet 0.0160¬±0.0199 0.0016¬±0.0023 0.1293¬±0.1748 1.6712¬±0.6500 0.0015¬±0.0024 0.0096¬±0.0121
DINO 0.0850¬±0.0994 0.0142¬±0.0206 0.1651¬±0.1279 0.1378¬±0.1462 0.0142¬±0.0189 0.0336¬±0.0334
CONFIDE AE/IC No-AE0%20%40%60%80%100%120%MSE Percent DifferencePrediction
Parameters
Figure 8: Ablation study on the constant coefficients PDE
dataset. The Y axis shows the percentage difference between
the different approaches and the standard CONFIDE one
(thus it scores 0%). We demonstrate the effects on both signal
prediction (blue), and parameter estimation (red).
4.7 Autoencoder ablation study
In this section, we demonstrate the effect that adding a decoder
network has on CONFIDE. To this end, we evaluate three different
scenarios:
‚Ä¢CONFIDE. Using a decoder followed by a reconstruction
loss, and feeding the initial conditions in addition to the
latent vector (demonstrated in the text as initial conditions
aware autoencoder)
‚Ä¢AE-IC. Similarly, using a decoder followed by a reconstruc-
tion loss, but the autoencoder is not initial-conditions aware.
‚Ä¢No-AE. The network trains solely on the PDE loss, without
the decoder part (i.e., by setting ùõº=0).Results of the three approaches on the constant PDE dataset are
shown in Fig. 8. When comparing a setup with no decoder part
(i.e., No-AE) with a setup that has a decoder, but does not use the
initial conditions as a decoder input (i.e., AE/IC), we observe that
merely adding a decoder network might have a negative effect on
the results, especially when analyzing the parameter estimation
results. One reason for this may be that the neural network needs
to compress the observed signal in a way that should both solve
the PDE and reconstruct the signal. This modification of latent
space has a negative effect in this case. When also adding the initial
conditions as an input to the decoder (i.e., the standard CONFIDE),
we observe significant MSE improvement in both signal prediction
and parameter estimation ( ‚àº35% improvement in both). This result
suggests that adding the initial conditions aware autoencoder en-
ables the networks to learn a good representation of the dynamics
of the observed signal in its latent space.
5 CONCLUSION
In this work we introduce a new hybrid modelling approach, com-
bining mechanistic knowledge with data. The knowledge we as-
sume is in the form of a PDE family, without specific coefficient val-
ues, typically supplied by field experts. The problem we introduce in
this work is unique because the signals at inference time correspond
to PDEs with coefficients that differ from those in the training data.
This makes the prediction problem similar to a zero-shot prediction
challenge, as the model needs to generalize to unseen dynamics. We
conduct extensive experiments on four well-known PDE systems,
comparing our scheme to other solutions and testing its perfor-
mance in different regimes. CONFIDE outperforms all baselines,
provides reliable PDE coefficient estimations, robust to different
values of hyper-parameters, and scores well even when the test
signals come from out-of-distribution signals. We further stress-
test CONFIDE by removing most of the mechanistic knowledge
it receives (namely, CONFIDE-0, for zero knowledge), and show
that it is still able to outperform other baselines. There are many
promising future directions, such as scaling CONFIDE to real-world
problems like the ones mentioned in Section 2.
 
1847KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
REFERENCES
[1]Christian Aarset, Martin Holler, and Tram Thi Ngoc Nguyen. 2022. Learning-
informed parameter identification in nonlinear time-dependent PDEs. arXiv
preprint arXiv:2202.10915 (2022).
[2]Germ√°n Abrevaya, Mahta Ramezanian-Panahi, Jean-Christophe Gagnon-Audet,
Irina Rish, Pablo Polosecki, Silvina Ponce Dawson, Guillermo Cecchi, and Guil-
laume Dumas. 2023. GOKU-UI: Ubiquitous Inference through Attention and
Multiple Shooting for Continuous-time Generative Models. arXiv preprint
arXiv:2307.05735 (2023).
[3]Harry Bateman. 1915. Some recent researches on the motion of fluids. Monthly
Weather Review 43, 4 (1915), 163‚Äì170.
[4]Johannes Brandstetter, Daniel Worrall, and Max Welling. 2022. Message passing
neural PDE solvers. arXiv preprint arXiv:2202.03376 (2022).
[5]Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. 2016. Discovering
governing equations from data by sparse identification of nonlinear dynamical
systems. Proceedings of the national academy of sciences 113, 15 (2016), 3932‚Äì3937.
[6]Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. 2019.
Data-driven discovery of coordinates and governing equations. Proceedings of
the National Academy of Sciences 116, 45 (2019), 22445‚Äì22451.
[7]Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, GA Pershing, Hen-
rique Teles Maia, Maurizio M Chiaramonte, Kevin Carlberg, and Eitan Grinspun.
2022. CROM: Continuous reduced-order modeling of PDEs using implicit neural
representations. arXiv preprint arXiv:2206.02607 (2022).
[8]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. 2018.
Neural ordinary differential equations. Advances in neural information processing
systems 31 (2018).
[9]Guozhi Dong, Michael Hinterm√ºller, and Kostas Papafitsoros. 2022. Optimization
with learning-informed differential equation constraints and its applications.
ESAIM: Control, Optimisation and Calculus of Variations 28 (2022), 3.
[10] Mengge Du, Yuntian Chen, and Dongxiao Zhang. 2022. DISCOVER: Deep identi-
fication of symbolic open-form PDEs via enhanced reinforcement-learning. arXiv
preprint arXiv:2210.02181 (2022).
[11] Lawrence C Evans. 2010. Partial differential equations. Vol. 19. American Mathe-
matical Soc.
[12] Jayesh K Gupta and Johannes Brandstetter. 2022. Towards multi-spatiotemporal-
scale generalized pde modeling. arXiv preprint arXiv:2209.15616 (2022).
[13] Hennes Hajduk, Dmitri Kuzmin, and Vadym Aizinger. 2020. Bathymetry Recon-
struction Using Inverse ShallowWater Models: Finite Element Discretization and
Regularization. Springer.
[14] Jiequn Han, Arnulf Jentzen, and Weinan E. 2018. Solving high-dimensional
partial differential equations using deep learning. Proceedings of the National
Academy of Sciences 115, 34 (2018), 8505‚Äì8510.
[15] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensional-
ity of data with neural networks. science 313, 5786 (2006), 504‚Äì507.
[16] Valerii Iakovlev, Cagatay Yildiz, Markus Heinonen, and Harri L√§hdesm√§ki. 2022.
Latent neural ODEs with sparse bayesian multiple shooting. arXiv preprint
arXiv:2210.03466 (2022).
[17] Matthieu Kirchmeyer, Yuan Yin, J√©r√©mie Don√†, Nicolas Baskiotis, Alain Rako-
tomamonjy, and Patrick Gallinari. 2022. Generalizing to New Physical Systems
via Context-Informed Dynamics Model. arXiv preprint arXiv:2202.01889 (2022).
[18] Gene A Klaasen and William C Troy. 1984. Stationary wave solutions of a system
of reaction-diffusion equations derived from the fitzhugh‚Äìnagumo equations.
SIAM J. Appl. Math. 44, 1 (1984), 96‚Äì110.
[19] Mark A Kramer. 1991. Nonlinear principal component analysis using autoasso-
ciative neural networks. AIChE journal 37, 2 (1991), 233‚Äì243.
[20] Stefan Kurz, Herbert De Gersem, Armin Galetzka, Andreas Klaedtke, Melvin
Liebsch, Dimitrios Loukrezis, Stephan Russenschuck, and Manuel Schmidt. 2022.
Hybrid modeling: towards the next level of scientific computing in engineering.
Journal of Mathematics in Industry 12, 1 (2022), 1‚Äì12.
[21] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. 1998. Artificial neural
networks for solving ordinary and partial differential equations. IEEE transactions
on neural networks 9, 5 (1998), 987‚Äì1000.
[22] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature
(2015).
[23] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2020. Fourier neural oper-
ator for parametric partial differential equations. arXiv preprint arXiv:2010.08895
(2020).
[24] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede
Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. 2021. Physics-informed
neural operator for learning partial differential equations. arXiv preprint
arXiv:2111.03794 (2021).
[25] Kart Leong Lim, Rahul Dutta, and Mihai Rotaru. 2022. Physics informed neural
network using finite difference method. In 2022 IEEE International Conference onSystems, Man, and Cybernetics (SMC). IEEE, 1828‚Äì1833.
[26] Ori Linial, Neta Ravid, Danny Eytan, and Uri Shalit. 2021. Generative ode model-
ing with known unknowns. In Proceedings of the Conference on Health, Inference,
and Learning. 79‚Äì94.
[27] Zichao Long, Yiping Lu, and Bin Dong. 2019. PDE-Net 2.0: Learning PDEs from
data with a numeric-symbolic hybrid deep network. J. Comput. Phys. 399 (2019),
108925.
[28] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. 2018. Pde-net: Learning
pdes from data. In International Conference on Machine Learning. PMLR, 3208‚Äì
3216.
[29] Lu Lu, Pengzhan Jin, and George Em Karniadakis. 2019. Deeponet: Learning
nonlinear operators for identifying differential equations based on the universal
approximation theorem of operators. arXiv preprint arXiv:1910.03193 (2019).
[30] Geoffrey N√©giar, Michael W Mahoney, and Aditi S Krishnapriyan. 2022. Learn-
ing differentiable solvers for systems with hard constraints. arXiv preprint
arXiv:2207.08675 (2022).
[31] Aurel Neic, Fernando O Campos, Anton J Prassl, Steven A Niederer, Martin J
Bishop, Edward J Vigmond, and Gernot Plank. 2017. Efficient computation of
electrograms and ECGs in human whole heart simulations using a reaction-
eikonal model. Journal of computational physics 346 (2017), 191‚Äì211.
[32] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell. 2009.
Zero-shot learning with semantic output codes. Advances in neural information
processing systems 22 (2009).
[33] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia.
2020. Learning mesh-based simulation with graph networks. arXiv preprint
arXiv:2010.03409 (2020).
[34] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. 2019. Physics-informed
neural networks: A deep learning framework for solving forward and inverse
problems involving nonlinear partial differential equations. Journal of Computa-
tional physics 378 (2019), 686‚Äì707.
[35] O Reynolds. 1895. On the Dynamical Theory of Incompressible Viscous Fluids
and the Determination of the Criterion. In Proceedings of the Royal Society-
Mathematical and Physical Sciences.
[36] Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. 2019. Latent ordinary
differential equations for irregularly-sampled time series. Advances in neural
information processing systems 32 (2019).
[37] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. 2017.
Data-driven discovery of partial differential equations. Science advances 3, 4
(2017), e1602614.
[38] Hayden Schaeffer. 2017. Learning partial differential equations via data discovery
and sparse optimization. Proceedings of the Royal Society A: Mathematical, Physical
and Engineering Sciences 473, 2197 (2017), 20160446.
[39] George Gabriel Stokes et al .1851. On the effect of the internal friction of fluids
on the motion of pendulums. (1851).
[40] John C Strikwerda. 2004. Finite difference schemes and partial differential equations.
SIAM.
[41] Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy
Morozov, Michael Mahoney, and Amir Gholami. 2023. Towards Foundation
Models for Scientific Machine Learning: Characterizing Scaling and Transfer
Behavior. arXiv preprint arXiv:2306.00258 (2023).
[42] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay,
Francesco Alesiani, Dirk Pfl√ºger, and Mathias Niepert. 2022. PDEBench: An exten-
sive benchmark for scientific machine learning. arXiv preprint arXiv:2210.07182
(2022).
[43] Evren Mert Turan and Johannes J√§schke. 2021. Multiple shooting for training
neural differential equations on time series. IEEE Control Systems Letters 6 (2021),
1897‚Äì1902.
[44] Rui Wang and Rose Yu. 2021. Physics-guided deep learning for dynamical systems:
A survey. arXiv preprint arXiv:2107.01272 (2021).
[45] Tsung-Yen Yang, Justinian P Rosca, Karthik R Narasimhan, and Peter Ramadge.
2022. Learning Physics Constrained Dynamics Using Autoencoders. In Advances
in Neural Information Processing Systems.
[46] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy,
and Patrick Gallinari. 2022. Continuous pde dynamics forecasting with implicit
neural representations. arXiv preprint arXiv:2209.14855 (2022).
[47] Yuan Yin, Vincent Le Guen, J√©r√©mie Dona, Emmanuel de B√©zenac, Ibrahim Ayed,
Nicolas Thome, and Patrick Gallinari. 2021. Augmenting physical models with
deep networks for complex dynamics forecasting. Journal of Statistical Mechanics:
Theory and Experiment 2021, 12 (2021), 124012.
[48] Kirill Zubov, Zoe McCarthy, Yingbo Ma, Francesco Calisto, Valerio Pagliarino,
Simone Azeglio, Luca Bottero, Emmanuel Luj√°n, Valentin Sulzer, Ashutosh
Bharambe, et al .2021. NeuralPDE: Automating physics-informed neural networks
(PINNs) with error approximations. arXiv preprint arXiv:2107.09443 (2021).
 
1848CONFIDE: Contextual Finite Difference Modelling of PDEs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
xy01U(t=0)
xy1
0V(t=0)
1
01
1
01
Figure 9: Two examples of initial conditions for the 2D
FitzHugh-Nagumo datasets. The left column describes the
first state variable ùë¢, and the right column is the state vari-
ableùë£. Top row is the first example, and the bottom row is
the second. All initial conditions are drawn from a GP prior
not constrained to boundary conditions.
A NUMERICAL SCHEME
The partial derivatives are estimated using standard numerical
schemes for each point in time and space. We choose discretiza-
tion parameters Œîùë•for the spatial axis and Œîùë°for the tempo-
ral axis where we solve the PDE numerically on the grid points
{(ùëñŒîùë•,ùëóŒîùë°)}ùëÅùë•,ùëÅùë°
ùëñ=0,ùëó=0withùêø=ùëÅùë•Œîùë•andùëá=ùëÅùë°Œîùë°. Let us denote the
numerical solution with ÀÜùë¢ùëñ,ùëó. We use the forward-time central-space
scheme, so a second order scheme from (1) would be
ÀÜùë¢ùëñ,ùëó+1‚àíÀÜùë¢ùëñ,ùëó
Œîùë°=ùëù2(ùëñ,ùëó,ùë¢(ùëñ,ùëó))ÀÜùë¢ùëñ+1,ùëó‚àí2ÀÜùë¢ùëñ,ùëó+ÀÜùë¢ùëñ‚àí1,ùëó
Œîùë•2
+ùëù1(ùëñ,ùëó,ùë¢(ùëñ,ùëó))ÀÜùë¢ùëñ+1,ùëó‚àíÀÜùë¢ùëñ‚àí1,ùëó
2Œîùë•
+ùëù0(ùëñ,ùëó,ùë¢(ùëñ,ùëó))(8)
We refer the reader to [40] for a complete explanation.
B EXPERIMENTAL AND IMPLEMENTATION
DETAILS
We provide further information regarding the experiments de-
scribed in Section 4. We ran all of the experiments on a single
GPU (NVIDIA GeForce RTX 2080), and all training algorithms
took<10minutes to train. All algorithms used 5-10M parameters
(more parameters on the FitzHugh-Nagumo experiment). Full code
implementation for creating the datasets and implementing CON-
FIDE and its baselines is available in https://github.com/orilinial/
CONFIDE.
B.1 Dataset details
To create the dataset, we generated signals using the PyPDE package,
where each signal was generated with different initial conditions.
In addition, as discussed in Section 2, we made an important changethat makes our setting much more realistic than the one used by
other known methods: the PDE parametric functions (e.g., (ùëé,ùëè,ùëê))
are sampled for each signal, instead of being fixed across the dataset,
making the task much harder. To evaluate different models on the
different datasets, we divided the datasets into 80% train set, 10%
validation set and 10% test set.
Second Order PDE with Constant Coefficients. For this task,
we generated 10,000 signals on the spatial grid ùë•‚àà[0,20]with
Œîùë•=0.5, resulting in a spatial dimension consisting of 40 points.
Each signal was generated with different initial conditions sampled
from a Gaussian process posterior that obeys the Dirichlet boundary
conditionsùë¢(ùë•=0)=ùë¢(ùë•=ùêø)=0. The hyper-parameters we
used for the GP were ùëô=3.0,ùúé=0.5, which yielded a rich family of
signals. The parameter vector was sampled uniformly: ùëé‚àºùëà[0,2],
ùëèandùëê‚àºùëà[‚àí1,1]for each signal, resulting in various dynamical
systems in a single dataset. To create the signal we solved the PDE
numerically, using the explicit method for times ùë°‚àà[0,5.0]and
Œîùë°=0.05. Signals that were numerically unstable were omitted
and regenerated, so that the resulting dataset contains only signals
that are physically feasible.
Burgers‚Äô PDE. To create the Burgers‚Äô PDE dataset we followed
the exact same process as with the constant coefficients PDE, except
for the parameter sampling method. Parameter ùëéwas still drawn
uniformly:ùëé‚àºùëà[1,2], butùëèhere behaves as a function of ùë¢:
ùëè(ùë¢)=‚àíùë¢, commonly referred to as the viscous Burgers‚Äô equation.
FitzHugh-Nagumo equations. For the purpose of creating a
more challenging dataset with two spatial dimensions we followed
Yin et al . [47] , and used the 2-D FitzHugh-Nagumo PDE (described
in Eq. 6). To make this task even more challenging and realistic,
we created a small dataset comprising only 1000 signals defined
on a 2D rectangular domain, discretized to the grid [‚àí0.16,0.16]√ó
[‚àí0.16,0.16]. The initial conditions for each signal were generated
similarly to the other experiments, by sampling a Gaussian process
prior withùëô=0.1, which generated a rich family of initial conditions,
as can be seen in Fig. 9. To create the coefficient function we sample
ùëò‚àºùëà[0,1]per signal, and set(ùëé,ùëè)=(1ùëí‚àí3,5ùëí‚àí3). To create the
signal we solved the PDE numerically, using the explicit method
for timesùë°‚àà[0,1.0]andŒîùë°=0.01.
Navier-Stokes equations For this task we follow exactly Yin
et al. [46] and Li et al . [23] by generating a dataset of 2-D Navier-
Stokes PDE [ 39]. This dataset corresponds to the incompressible
fluid dynamics, and defined by Equation (7). The spatial grid used
for this task is Œ©=[‚àí1,1]2, with dimensions 32√ó32and sample the
viscosity value by ùúà‚àºùëà[1,2]¬∑10‚àí3. As with the FitzHugh-Nagumo
experiment, we generated a dataset of 1000 signals, 100 of which
are kept as test signals. The value of the constant forcing term is
set by:
ùëì(ùë•1,ùë•2)=0.1(sin(2ùúã(ùë•1+ùë•2))+cos(2ùúã(ùë•1+ùë•2))),‚àÄ(ùë•1,ùë•2)‚ààŒ©.
For the time evolution settings we used ùõøùë°=0.1andùëá=20.0.
The training horizon is set at ùëá=5.0(i.e.,ùúå=0.25). The initial
conditions are sampled as in Yin et al. [46].
B.2 Implementation details
CONFIDE. The CONFIDE algorithm consists of two main parts:
an auto-encoder part that is used for extracting the context, and a
coefficient-estimation network.
 
1849KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ori Linial, Orly Avner & Dotan Di Castro.
The autoencoder architecture consists of an encoder-decoder
network, both implemented as MLPs with 6 layers and 256 neurons
in each layer, and a ReLU activation. For the FitzHugh-Nagumo
dataset, we wrap the MLP autoencoder with convolution and de-
convolution layers for the encoder and decoder respectively, in
order to decrease the dimensions of the observed signal more ef-
fectively. We note that the encoder-decoder architecture itself is
not the focus of the paper. We found that making the autoencoder
initial-conditions-aware by concatenating the latent vector in the
output of the encoder to the initial conditions of the signal ùë¢(ùë°=0),
greatly improved results and convergence time. The reason is that
it encourages the encoder to focus on the dynamics of the observed
signal, rather than the initial conditions of it. We demonstrate this
effect in Section 4.7.
The second part, which is the coefficient estimator part, is imple-
mented as an MLP with 5 hidden layers, each with 1024 neurons,
and a ReLU activation. The output of this coefficient-estimator net-
work is set to be the parameters for the specific task that is being
solved. In the constant-parameters PDE, the output is a 3-dim vec-
tor(ÀÜùëé,ÀÜùëè,ÀÜùëê). In the Burgers‚Äô PDE, the output is composed of a scalar
ÀÜùëé, which is the coefficient ofùúï2ùë¢
ùúïùë•2and the coefficient function ùëè(ùë¢),
which is a vector approximating the coefficient ofùúïùë¢
ùúïùë•on the given
grid ofùë•. In the FitzHugh-Nagumo PDE, the output is a scalar ùëò
used for inferring ùëÖùë¢(ùë¢,ùë£,ùëò), and the function ùëÖùë£on the 2D grid
(ùë•,ùë¶).
The next step in the CONFIDE algorithm is to evaluate the loss
which is comprised of two losses: an autoencoder reconstruction
lossLùê¥ùê∏, and a PDE functional loss Lùëêùëúùëíùëì. The autoencoder loss
is a straightforward ùêø2evaluation on the observed signal ùë¢ùëêand
the reconstructed signal. The functional loss is evaluated by first
numerically computing all the derivatives of the given equation on
the observed signal. Second, evaluating both sides of the differential
equations using the derivatives and the model‚Äôs coefficient outputs,
and lastly, minimizing the difference between the sides. For example,
in the Burgers‚Äô equation, we first evaluateùúïùë¢
ùúïùë°,ùúï2ùë¢
ùúïùë•2, andùúïùë¢
ùúïùë•, we then
compute the coefficients ÀÜùëéandÀÜùëè(ùë¢), and finally minimize:
minùúîùúïùë¢
ùúïùë°‚àíÀÜùëé¬∑ùúï2ùë¢
ùúïùë•2‚àíÀÜùëè(ùë¢)¬∑ùúïùë¢
ùúïùë•.
Since this algorithm evaluates numerical derivatives of the observed
signals, it could be used for equations with higher derivatives, such
as the wave equation, for instance.
CONFIDE-0. Similarly to the standard CONFIDE algorithm, we
consider a zero-knowledge version, where we only know that the
signal obeys some differential operator ùêπ, but have no details re-
garding the actual structure of ùêπ. Thus, the input for the coefficient-
estimator network is the current PDE state ( ùë¢in the 1D experiment
and(ùë¢,ùë£)in the 2D experiment), and the latent vector extractedfrom the auto-encoder. The model then outputs an approximation
for time derivative of the PDE states, i.e., the model‚Äôs inputs are
(ùë¢ùë°,ùëîùúô(ùë¢ùëê))and the output is an approximation forùúïùë¢
ùúïùë°. The opti-
mization function for this algorithm therefore tries to minimize the
difference between the numerically computed time derivative and
the output of the model:
LCONFIDE-0 =ùõº¬∑Lùê¥ùê∏+(1‚àíùõº)¬∑ùëÅ‚àëÔ∏Å
ùëñ=1ùúïùë¢
ùúïùë°‚àíùëöùúÉ(ùë¢ùëê
ùëñ,ùëîùúô(ùë¢ùëê
ùëñ))2
,
whereLùê¥ùê∏is defined in Eq. 2,ùúïùë¢
ùúïùë°is evaluated numerically, ùëöùúÉis
the network estimating the temporal derivative, ùëîùúôis the encoder
network, and ùë¢ùëê
ùëñis the observed signal.
Hyper-parameters. For both versions of CONFIDE we used the
standard Adam optimizer, with learning rate of 1ùëí‚àí3, and no weight
decay. For all the networks we used only linear and convolution
layers, and only used the ReLU activation functions. For the ùõº
parameter we used ùõº=0.5for all experiments, and all algorithms,
after testing only two different values: 0and0.5and observing that
using the autoencoder loss helps scoring better and faster results.
Neural-ODE. We implement the Neural-ODE algorithm as sug-
gested by Chen et al . [8], section 5.1 (namely, Latent-ODE). We
first transform the observed signal through a recognition network
which is a 6-layer MLP. We then pass the signal through an RNN
network backwards in time. The output of the RNN is then divided
into a mean function, and an std function, which are used to sample
a latent vector. The latent vector is used as initial conditions to
an underlying ODE in latent space which is parameterized by a
3-layer MLP with 200 hidden units, and solved with a DOPRI-5
ODE-solver. The output signal is then transformed through a 5-
layer MLP with 1024 hidden units, and generates the result signal.
The loss function is built of two terms, a reconstruction term and
a KL divergence term, which is multiplied by a ùúÜùêæùêø. After testing
several optimization schemes, including setting ùúÜùêæùêøto the constant
values{1,0.1,0.01,0.001,0}, and testing a KL-annealing scheme
whereùúÜùêæùêøchanges over time, we chose ùúÜùêæùêø=1ùëí‚àí2as it produced
the lowest reconstruction score on the validation set. We used an
Adam optimizer with 1ùëí‚àí3learning rate and no weight decay.
Our implementation is based on the code in https://github.com/
rtqichen/torchdiffeq.
FNO, Unet and DINo. For FNO we used the standard Neural-
Operator package https://github.com/neuraloperator/neuraloperator.
For the Unet implementation we used the implementation in https:
//github.com/microsoft/pdearena, and for DINo, the implementa-
tion in https://github.com/mkirchmeyer/DINo The input we used
for these algorithms is the entire signal ùë¢ùëêfrom timeùë°=0to
ùë°=ùëá‚àí2, and the output is a prediction of the solution at the next
time pointùë¢(ùë°=ùëá‚àí1). The loss is therefore an MSE reconstruction
loss onùë¢(ùë°=ùëá‚àí1).
 
1850