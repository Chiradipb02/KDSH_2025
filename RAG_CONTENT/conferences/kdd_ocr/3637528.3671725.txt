Multivariate Log-based Anomaly Detection
for Distributed Database
Lingzhe Zhang
Peking University
Beijing, China
zhang.lingzhe@stu.pku.edu.cnTong Jia∗
Peking University
Beijing, China
jia.tong@pku.edu.cnMengxi Jia
Peking University
Beijing, China
mxjia@pku.edu.cn
Ying Li∗
Peking University
Beijing, China
li.ying@pku.edu.cnYong Yang
Peking University
Beijing, China
yang.yong@pku.edu.cnZhonghai Wu
Peking University
Beijing, China
wuzh@pku.edu.cn
Abstract
Distributed databases are fundamental infrastructures of today’s
large-scale software systems such as cloud systems. Detecting
anomalies in distributed databases is essential for maintaining soft-
ware availability. Existing approaches, predominantly developed
using Loghub—a comprehensive collection of log datasets from
various systems—lack datasets specifically tailored to distributed
databases, which exhibit unique anomalies. Additionally, there’s a
notable absence of datasets encompassing multi-anomaly, multi-
node logs. Consequently, models built upon these datasets, primar-
ily designed for standalone systems, are inadequate for distributed
databases, and the prevalent method of deeming an entire cluster
anomalous based on irregularities in a single node leads to a high
false-positive rate. This paper addresses the unique anomalies and
multivariate nature of logs in distributed databases. We expose
the first open-sourced, comprehensive dataset with multivariate
logs from distributed databases. Utilizing this dataset, we conduct
an extensive study to identify multiple database anomalies and to
assess the effectiveness of state-of-the-art anomaly detection using
multivariate log data. Our findings reveal that relying solely on
logs from a single node is insufficient for accurate anomaly detec-
tion on distributed database. Leveraging these insights, we propose
MultiLog, an innovative multivariate log-based anomaly detec-
tion approach tailored for distributed databases. Our experiments,
based on this novel dataset, demonstrate MultiLog’s superiority,
outperforming existing state-of-the-art methods by approximately
12%.
CCS Concepts
•Software and its engineering →Maintaining software.
*Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671725Keywords
Anomaly Detection, Distributed Database, Anomaly Injection, Mul-
tivariate Log Analysis
ACM Reference Format:
Lingzhe Zhang, Tong Jia∗, Mengxi Jia, Ying Li∗, Yong Yang, and Zhong-
hai Wu. 2024. Multivariate Log-based Anomaly Detection for Distributed
Database. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671725
1 Introduction
The distributed databases, such as Google Spanner[ 8], Alibaba
OceanBase[ 54], PingCAP TiDB[ 18], and Apache IoTDB[ 50] have
been widely used in cloud systems and is becoming a fundamen-
tal infrastructure to support the requirement for extremely high
volume of data storage[25, 58, 59].
However, existing distributed databases suffer from frequent
anomalies such as system failure, performance degradation, etc,
and these anomalies often cause huge financial losses. For instance,
Alibaba Cloud suffers from Intermittent Slow Queries (iSQs)[ 39],
which result in billions of dollars in losses annually. Amazon also
reports that every 0.1s of loading delay caused by database anom-
alies would cause an extra 1% financial loss. As a result, detecting
the anomalies of distributed databases and mitigating the affections
of system anomalies are notoriously essential.
As system logs meticulously track the states and significant
events of actively running processes, they serve as a rich source for
anomaly detection. Consequently, log-based anomaly detection has
emerged as an effective method for ensuring software availability
and has garnered extensive research attention. Most existing log-
based anomaly detection models primarily utilize datasets from
Loghub[ 24], a comprehensive compilation of log datasets from a
diverse range of systems. Among the most commonly utilized are
systems such as standalone systems (e.g., BGL[ 43], Thunderbird[ 43],
HPC[ 40]), distributed computing frameworks (like Hadoop[ 33],
Spark), and distributed file systems (e.g., HDFS[52], Zookeeper.
Leveraging Loghub, a variety of log-based anomaly detection
models have been developed, broadly classified into two cate-
gories: supervised models[ 3,5,12,32,38,53,61] and unsupervised
models[ 1,10,14,21–23,26,27,41,55]. Supervised models, such as
RobustLog[ 61], necessitate labeled data comprising both normal
and abnormal instances to construct their predictive frameworks.
4256
KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
In contrast, unsupervised models detect deviations relying solely
on standard data. They are primarily split into deep neural network-
based[10, 26, 41, 55] and graph-based models[1, 14, 21–23, 26].
Although these anomaly detection methods have demonstrated
promising outcomes based on Loghub[ 24], they still face the fol-
lowing practical challenges when applied to distributed databases:
•Lack of Log Anomaly Datasets Collected from Dis-
tributed Databases: Notably absent in Loghub are datasets
specifically collected for distributed databases. This omis-
sion highlights a significant gap in the design, implemen-
tation, and testing of state-of-the-art models tailored for
distributed databases. These databases are characterized by
unique anomalies and features, necessitating the utlization of
information from multiple dimensions, and many anomalies
cannot be detected within the current node alone.
•Absence of Datasets with Multi-Anomaly, Multi-Node
Logs: The majority of these datasets do not disclose the
variety of anomaly types injected, often suggesting a lim-
ited scope of anomaly types. Even in datasets where the
types are specified, like Hadoop[ 33], the range is limited
(e.g., Machine down, Network disconnection, Disk full), lack-
ing comprehensiveness[ 19,28]. Additionally, these datasets
typically comprise logs from single sources — either from
standalone systems or single node of distributed systems,
failing to capture the multi-node, interconnected nature of
distributed databases.
•Model Limitations for Distributed Databases: Existing
approaches are tailored for standalone systems, which do
not align with the complexities of distributed databases. In
these databases, to ensure data consistency, cluster nodes are
categorized into Leaders and Followers, meaning different
nodes can exhibit distinct information. Current methods,
when used in distributed databases for what is termed as
cluster anomaly detection, generally label the entire cluster
as anomalous if any single node exhibits abnormal behavior.
This approach can lead to a high rate of false positives, as
it doesn’t adequately consider the intricacies of distributed
systems.
Recognizing this gap, we first construct a comprehensive dataset
specifically designed for anomaly detection in distributed databases.
Sourced from Apache IoTDB[ 50], this dataset comprises 900 mil-
lion records and spans 216 GB, making it the first dataset of this
scale and specificity. We inject 11 types of anomalies, covering
all categories reported in state-of-the-art research on distributed
databases[ 31,39,62]. It includes a wide range of anomalies, from
resource-related issues to datatbase software faults. Moreover, to
enable effective multivariate log-based anomaly detection, we metic-
ulously collected logs from various nodes.
We subsequently carry out an empirical study using this dataset
to explore the characteristics of anomalies in distributed databases.
Our study focuses on the performance of state-of-art anomaly de-
tection models in identifying a spectrum of database anomalies,
particularly in scenarios where logs from multiple database nodes
are utilized. The findings of our study revealed a notable short-
fall in these models – they are unable to reach peak performance
when faced with database anomalies, and exhibit a high rate of falsepositives in detecting anomalies from multiple node logs. This inad-
equacy stems from the fact that different nodes within a database
can present unique insights into anomalies, with certain anomalies
being detectable only through the analysis of logs from multiple
nodes.
Building upon these insights, we introduce MultiLog, a multi-
variate log-based anomaly detection method for distributed data-
base. In detail, we initially gather sequential, quantitative, and se-
mantic information from the logs of each individual node within the
database cluster. Subsequently, these results are processed through
Standalone Estimation, where we utilize LSTM enhanced with self-
attention to encode the data into a probability sequence, facilitating
anomaly detection for each node. Finally, we introduce a Cluster
Classifier that incorporates an AutoEncoder with a meta-classifier
to standardize the probabilities across all nodes and identify the
presence of any anomalies within the entire cluster.
Our experiments demonstrate that MultiLog significantly sur-
passes current state-of-the-art methods. The evaluation results
indicate that MultiLog excels in multi-node classification, achiev-
ing a remarkable improvement of approximately 12% over existing
methods. Furthermore, in the realm of anomaly detection within
standalone database nodes, MultiLog notably boosts classification
performance, recording an impressive enhancement of over 16%.
To summarize, our key contributions are as follows:
•We construct and open the first dataset for multivariate log-
based anomaly detection on distributed databases, totaling
216 GB in size and comprising a total of 900 million records.
•We conduct a comprehensive study on database logs based
on this dataset. Our study highlights that existing models
face challenges in achieving optimal performance with ran-
domly injected database anomalies. Notably, they exhibit
a high rate of false positives when identifying anomalies
across distributed database nodes.
•Inspired by the findings, we propose a multivariate log-based
anomaly detection method for distributed database named
MultiLog. This approach utilizes Standalone Estimation to
encode sequential, quantitative, and semantic information
from the logs of individual nodes, leveraging LSTM with self-
attention for this purpose. Subsequently, MultiLog employs
a Cluster Classifier, which integrates an AutoEncoder with
a meta-classifier, to effectively classify anomalies within the
database cluster.
•The effectiveness of MultiLog is confirmed based on our
open dataset. Experiments show that MultiLog outperforms
state-of-art methods in multi-nodes classification by approx-
imately 12% and improves classification effectiveness in stan-
dalone database nodes by over 16%.
2 Background
In this section, we outline the background of the log-based anomaly
detection problem and clarify the typical application of these models
within a distributed context.
2.1 Log-based Anomaly Detection
A log sequence is composed of a series of log entries, arranged in
the order they were output. An "event" represents an abstraction
4257Multivariate Log-based Anomaly Detection for Distributed Database KDD ’24, August 25–29, 2024, Barcelona, Spain.
      Node1Node1      Node2Node2     NodeN−1NodeN−1….      NodeNNodeNModelModelModelModelLog
Log
Log
LogAnomalousNormal
TimeTime WindowL1L1AnomalousNormalL2L2AnomalousNormalLN−1LN−1AnomalousNormalLNLNPredict
Predict
Predict
PredictLCLC= VNi=1LiVNi=1Li….
Figure 1: Application of Existing Models in a Distributed
Context (Single-Point Classification)
of a print statement in the source code, which appears in logs with
varying parameter values across different executions. By using a set
of invariant keywords and parameters (with parameters indicated
by a placeholder "*"), an event can encapsulate multiple log entries.
An event sequence is a sequence of log events, each corresponding
one-to-one with log entries in a log sequence.
We summarize the overall process of log-based anomaly detec-
tion in three steps: log parsing, log grouping, and model training.
Log parsing extracts log events from system logs, and log group-
ing transforms the log sequence into an event sequence. Model
training focuses on capturing the characteristics of event sequences
and constructing models for automated recognition. In this pa-
per, we apply Drain3[ 17] for log parsing, a tool implemented by
Logparser[ 16,64]. For log grouping, we adopt a time-window based
approach[ 29]. Additionally, our study utilizes three state-of-the-art
models: RobustLog[61], LogAnomaly[41], and PLELog[53].
2.2 Application of Anomaly Detection Models
in a Distributed Context
When implementing the aforementioned log-based anomaly detec-
tion models in a distributed context, the prevailing approach is to
assume the entire system is experiencing an anomaly if any single
node within the cluster exhibits abnormal behavior.
As illustrated in Figure 1, consider a distributed cluster with
𝑁nodes (𝑁𝑜𝑑𝑒 1,𝑁𝑜𝑑𝑒 2,...,𝑁𝑜𝑑𝑒𝑁−1,𝑁𝑜𝑑𝑒𝑁). At each time window
𝑇, a log-based anomaly detection model is applied to each 𝑁𝑜𝑑𝑒𝑖
to predict whether the system is normal or anomalous, denoted
as𝐿𝑖∈0,1. The label for the entire distributed cluster is then
determined as 𝑉𝑁
𝑖=1𝐿𝑖. This implies that if any 𝐿𝑖is anomalous within
a given time window, the overall prediction for the cluster 𝐿𝐶is
classified as anomalous; otherwise, it is considered normal.3 Construction of Dataset
In this section, we describe the construction of dataset. We conduct
experiments using Apache IoTDB[ 50] v1.2.2 in a docker environ-
ment. Each docker cluster consists of one config node and one data
node, with a configuration of 4 Intel(R) Xeon(R) Platinum 8260
CPUs at 2.40GHz, 16GB of DIMM RAM, 1.1TB NVMe disk, and
running on openjdk11.
For the generation of the data, we employ 4 write clients by
default to insert data to four randomly selected nodes with each
client assigned 100 threads for parallel insertion.
Furthermore, to inject anomalies into the cluster we mainly uti-
lize Chaos Mesh[ 2,4,42]. It is an open-source cloud-native chaos
engineering platform that provides a wide range of fault injection
types, allowing users to simulate various types of anomalies that
may occur in the real world. However, it is only suitable for sim-
ulating anomalies in docker environments. For anomalies related
to workload and internal bugs, we employ dynamic read / write
workload adjustment and database configuration modification ap-
proaches to inject.
3.1 Database Anomalies
We conduct an analysis of the definitions and classifications of
anomalies in system and database work and discover that database
anomalies can be classified into two categories[ 3,6,20,31,34,35,39,
51,56,62,63]: (1) anomalies caused by the system and (2) anomalies
generated by the database itself.
As outlined in Table 1, we categorize 6 types of system-induced
anomalies and 5 types of database-specific anomalies. Among these,
Resource-Intensive Compaction[ 25,44,47,48,57,59] (No.10) is
the most prevalent background task in LSM-based databases. Its
impact on system CPU and memory resources varies based on
configuration and can lead to write stall issues. Overly Frequent
Disk Flushes (No.11) is a fault present in various types of databases.
It pertains to the interaction between memory and disk in the
database, significantly affecting the frequency of disk file creation
and overall database performance.
3.2 Anomaly Injection
Timeanomaly injectionInjectorDatabaseTimeRecorderTimeRecorderlogloginjectInjectRecoverChaosWorkloadConﬁgLeaderFollower
Figure 2: Architecture of Anomaly Injection
The main architecture of anomaly injection is demonstrated in
Figure 2. During the experiment, the database initially operates
normally for a set period. Subsequently, various types of anomalies
are injected into the database. Before each injection, the current
timestamp is recorded. The anomalies are then introduced and
sustained for a specified duration. After the injection, the current
4258KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
Table 1: Distributed Database Anomalies
No. Anomaly Cause Type Description
1 CPU Saturation System The CPU computing resources exhaust.
2 IO Saturation System The I/O bandwidth is heavily occupied.
3 Memory Saturation System Insufficient memory resources.
4 Network Bandwidth Limited System The network bandwidth between nodes is limited.
5 Network Partition Arise System Network partition occurs between nodes.
6 Machine Down System One server goes down when the applications are running.
7 Accompanying Slow Query Database Excessive query load.
8 Export Operations Database Backing up data to external source.
9 Import Operations Database Importing data from external source.
10 Resource-Intensive Compaction Database Compaction tasks consume a substantial amount of system resources.
11 Overly Frequent Disk Flushes Database The low interval of flush operations leads to frequent disk writes.
timestamp is recorded again. The database is then allowed to resume
normal operation for a certain period before the next anomaly is
introduced. This process is repeated, with each cycle injecting a
different type of anomaly, thus maintaining the database cluster
in a cyclic state of normal - abnormal - normal - ···- abnormal -
normal.
In the aforementioned architecture, the key component is the
anomaly injector, which is responsible for injecting anomalies into
the database. In the implementation, anomalies No. 1 to No. 6 uti-
lize Chaos Mesh, anomalies No. 7 to No. 9 are accomplished by
adjusting the read / write workload, and anomalies No. 10 to No.
11 are implemented through hot modifications to the database con-
figuration.
3.3 Dataset Details
For the subsequent study, we employ the following various injection
scenarios:
•Single2Single: This represents a single type of anomaly,
injected into a single node. It encompasses two scenarios:
the anomaly being injected either into the leader node or
into a follower node.
•Single2Multi: asingle type of anomaly, randomly injected
intomultiple nodes (including both leader node and fol-
lower nodes);
•Multi2Single: multiple types of anomalies, injected only
intosingle node;
•Multi2Multi: multiple types of anomalies, randomly in-
jected into multiple nodes (including both leader node and
follower nodes).
Table 2: Dataset Details
No. Injection Type # Log Entries Data Size Labeled
1 Single2Single 450,323,420 116.81 GB ✓
3 Single2Multi 245,215,090 47.35 GB ✓
4 Multi2Single 149,041,252 37.84 GB ✓
5 Multi2Multi 59,336,131 14.12 GB ✓Based on the aforementioned process, the comprehensive sta-
tistics of the dataset are concluded in Table 2(the dataset will be
disclosed upon acceptance). Overall, the dataset has a size of 216 GB
and contains a total of 900 million records. The datasets are avail-
able in the MultiLog-Dataset repository, https://github.com/AIOps-
LogDB/MultiLog-Dataset.
4 Empirical Study
In this section, we conduct a comprehensive study using the afore-
mentioned dataset, with a focus on detecting database-specific
anomalies, and identifying anomalies from multiple node logs.
4.1 Detecting Database-Specific Anomalies
We conduct an in-depth analysis to identify the relevant log in-
formation for database-specific anomalies. In the logs, valuable
information that can be extracted includes sequential data, quan-
titative data, and semantic data. These respectively represent the
sequence information, frequency information, and semantic infor-
mation contained in the logs.
RawLogEvent TemplatesE0,"update [*] COMMITTED index from [NUM] to [NUM]"…E29, "Currently [*] data"…E99,”updateToMax old=[NUM], new=[NUM], updated? true"LogParsingSequential Data[E0 E0 E10 E11 E30 E7 E8 E9 E12 E0][…][E1 E0 E11 E12 E0 E7 E8 E19 E12 E26]LogGrouping
Figure 3: Sequential Data
As depicted in Figure 3, the predominant form of information
issequential data, specifically, the sequence of log entries. Raw
logs undergo an initial parsing process into event templates, corre-
sponding to each print statement in the code. This transformation
simplifies the initially complex logs, rendering them as structured
sequential information. Following this, a fixed-size window is ap-
plied to convert the obtained sequential information into organized
windows, resulting in the final sequential data. This type of informa-
tion proves particularly valuable in the detection of anomalies such
as No.8 and No.9 export and import operations, as these operations
often generate logs resembling ’Currently [*] data.
4259Multivariate Log-based Anomaly Detection for Distributed Database KDD ’24, August 25–29, 2024, Barcelona, Spain.
Sequential Data[E0 E0 E10 E11 E30 E7 E8 E9 E12 E0][…][E1 E0 E11 E12 E0 E7 E8 E19 E12 E26]Quantiative Data[E0:2 E1:0 E2:0 … E99:1][…][E0:1 E1:1 E2:1 … E99:0]CountingEvent Templates
Figure 4: Quantitative Data
The next aspect is quantitative information, as depicted in
Figure 4, which pertains to the frequency of each type of log entry
within a window. Derived from the sequential data, this involves
counting the occurrences of different event templates within a fixed-
size window. This statistical approach is valuable for detecting
anomalies such as No.11 Overly Frequent Disk Flushes, which are
characterized by an excessive number of disk flush logs.
Sequential Data[E0 E0 E10 E11 E30 E7 E8 E9 E12 E0][…][E1 E0 E11 E12 E0 E7 E8 E19 E12 E26]Semantic Data[V0, V0, V10, V11, V30, V7, V8, V9, V12, V0][…][V1, V0, V11, V12, V0, V7, V8, V19, V12, V26]Event TemplatesNatural Language
Figure 5: Semantic Data
The last aspect is semantic information, as depicted in Figure 5,
which involves logs with inherent semantic content. Similar to the
previous categories, this type of information is also derived from the
sequential data. It transforms each event template into a semantic
vector using sentence embedding methods. Then, the sequential
data set in converted to a vector data set within a fixed-size window.
This enrichment supplements the sequential data with meaningful
semantic information. It proves valuable for leveraging internal
database states in anomaly detection. For example, the presence of
terms like ’error’ or ’too high’ can serve as significant indicators.
Summary. To classify database-specific anomalies, it is essential
to gather information across multiple dimensions, including a
mix of sequential, quantitative, and semantic information.
4.2
Dete cting Anomaly fromMultiple Node
Logs
Wefurther analyze howindividual nodesinadatabase cluster
perceiveanomalies when oneisinjectedinto aspecific node.
Firstly ,wefindthat thenodedirectlyaffectedbytheanom-
alyfaces challenges indetecting itsownissue .Asshownin
Figur e3,when injectanomaly (No.8ExportOperation) into𝑁𝑜𝑑𝑒1,
PLELog results with aPrecision of39.68%, Recallof99.01%, and
F1-Scor eof56.66% andRobustLog results with aPrecision of55.81%,
Recallof71.29%, andF1-Scor eof62.61%. However,other nodesare
able todetectthisanomaly ,with𝑁𝑜𝑑𝑒6achie ving anF1-Scor eof
90.73% inPLELog and99.00% inRobustLog. This situation arises
because ,although export-r elate danomalies generate explicit logs
intheaffectednode,thepatterns andfrequency ofthese logs are
relativ elylow,yettheysignificantly impact system performance .
This leads toissues ininformation exchange with other nodes,
making itpossible todetecttheanomaly fromnodesother than
theaffectedone.These experiments highlight thefactthatrelyingTable 3:Evaluation Results forEach NodeDuring Anomaly
Injection (No.8ExportOperation) into𝑁𝑜𝑑𝑒1
Model Precision Recall F1-Scor e
PLELog𝑁𝑜𝑑𝑒 1 39.68% 99.01% 56.66%
𝑁𝑜𝑑𝑒 2 34.35% 100.00% 51.14%
𝑁𝑜𝑑𝑒 3 59.64% 98.02% 74.16%
𝑁𝑜𝑑𝑒 4 31.17% 100.00% 47.53%
𝑁𝑜𝑑𝑒 5 77.60% 96.04% 85.84%
𝑁𝑜𝑑𝑒 6 89.42% 92.08% 90.73%
RobustLog𝑁𝑜𝑑𝑒 1 55.81% 71.29% 62.61%
𝑁𝑜𝑑𝑒 2 84.04% 98.75% 90.80%
𝑁𝑜𝑑𝑒 3 24.44% 10.89% 15.07%
𝑁𝑜𝑑𝑒 4 97.09% 99.01% 98.04%
𝑁𝑜𝑑𝑒 5 30.87% 100.00% 47.17%
𝑁𝑜𝑑𝑒 6100.00% 98.02% 99.00%
solely onlogs fromasingle nodecancause thesystem tomiss
critical anomaly information, underscoring theimportance ofa
multi-no deperspectiveinanomaly detection.
Table 4:Evaluation Results forEach Node&Cluster During
Randomly Anomaly Injection (No.8ExportOperation) into
Cluster
Model Precision Recall F1-Scor e
PLELog𝑁𝑜𝑑𝑒 1 72.55% 94.87% 82.22%
𝑁𝑜𝑑𝑒 2 40.21% 97.44% 56.93%
𝑁𝑜𝑑𝑒 3 49.01% 94.87% 64.63%
𝑁𝑜𝑑𝑒 4 93.98% 100.00% 96.89%
𝑁𝑜𝑑𝑒 5 82.56% 91.03% 86.59%
𝑁𝑜𝑑𝑒 6 59.35% 93.59% 72.64%
Cluster 74.82% 99.05% 85.25%
RobustLog𝑁𝑜𝑑𝑒 1 70.83% 38.20% 49.64%
𝑁𝑜𝑑𝑒 2 91.55% 73.03% 81.25%
𝑁𝑜𝑑𝑒 3 75.71% 59.55% 66.67%
𝑁𝑜𝑑𝑒 4 85.07% 64.04% 73.08%
𝑁𝑜𝑑𝑒 5 82.35% 15.73% 26.42%
𝑁𝑜𝑑𝑒 6 87.88% 32.58% 47.54%
Cluster 65.57% 89.89% 75.83%
Ther efore,when conducting cluster anomaly detection, it’scru-
cialtoutilize information fromeach node,which means emplo ying
Single-Point Classification isessential. Toillustrate thesignificance
ofthisapproach, weconduct anexperiment involving therandom
injection ofanomaly No.8intoallnodes.Wefindthattheindivid-
ualassessments ofallnodesarenotveryeffective.Asshown
inFigur e4,when anomaly No.8israndomly injected.However,the
useofSingle-Point Classification enables ustoachie verelativ ely
better results fortheentir ecluster ,although these results stillfall
short ofthestandar dsrequiredforproduction envir onments in
databases.
Furthermor e,weassess theoverall state ofthecluster foranom-
alies using thelabelspredicte dbymultiple nodesandemplo ying the
4260KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
Table 5: Evaluation Results for Each Node & Cluster During
Anomaly Injection (No.7 Accompanying Slow Query) into
𝑁𝑜𝑑𝑒 1
Model Precision Recall F1-Score
RobustLog𝑁𝑜𝑑𝑒 1 97.62% 96.47% 97.04%
𝑁𝑜𝑑𝑒 2 97.65% 97.65% 97.65%
𝑁𝑜𝑑𝑒 3 92.22% 97.65% 94.86%
𝑁𝑜𝑑𝑒 4 98.81% 97.65% 98.22%
𝑁𝑜𝑑𝑒 5 92.13% 96.47% 94.25%
𝑁𝑜𝑑𝑒 6 98.81% 97.65% 98.22%
Cluster 62.22% 100.00% 76.71%
LogAnomaly𝑁𝑜𝑑𝑒 1 43.16% 97.62% 59.85%
𝑁𝑜𝑑𝑒 2 80.58% 98.81% 88.77%
𝑁𝑜𝑑𝑒 3 93.33% 100.00% 96.55%
𝑁𝑜𝑑𝑒 4 98.65% 86.90% 92.41%
𝑁𝑜𝑑𝑒 5 94.94% 89.29% 92.02%
𝑁𝑜𝑑𝑒 6 96.34% 94.05% 95.18%
Cluster 41.58% 100.00% 58.74%
aforementioned Single-Point Classification method. We presents
a detailed analysis of the evaluation results for each node and the
cluster during the injection of anomaly No.7 (Accompanying Slow
Query) into node 𝑁𝑜𝑑𝑒 1. As shown in Figure 5, it is noticeable
that while the F1-Scores for anomaly detection at each individual
node are generally high (mostly above 90%), the effectiveness
drastically decreases when Single-Point Classification is ap-
plied. This is evident in both RobustLog (with a 76.71% F1-Score)
and LogAnomaly (with a 58.74% F1-Score), where a significant in-
crease in false positives is observed (Reflected in 100% Recall and
lower Precision). However, the application of Single-Point Classifi-
cation is essential, especially for anomalies such as No.8, where it
is challenging to determine which node provides the correct label.
Summary. In the context of cluster anomaly detection, relying
solely on logs from a single node may result in missing many
significant anomalies. However, using only Single-Point Classi-
fication to synthesize judgments from various nodes can lead to
a high false positive rate.
5MultiLog: AMultivariate Log-Base dAnomaly
Dete ction Metho dForDistribute dDatabase
Our empirical study reveals that state-of-the-art models failto
achie veoptimal detection results when facedwith theinjection of
multiple anomalies. Italsoindicates thatcertain anomalies cannot
beaccurately classifie dbyrelying solely onlogs fromasingle
node,underscoring theneedforamultivariate logappr oach in
addressing cluster anomaly detection. However,bothstate-of-the-
artmodelscombine dwith Single-Point Classification metho dfall
short inobtaining sufficiently accurate detection outcomes.
Ther efore,inthissection, weintroduce MultiLog, amultivari-
atelog-base danomaly detection metho dspecifically designe dfor
distribute ddatabases. Figur e6depicts theMultiLog frame work.Un-
likeSingle-Point Classification, which directlyuses thepredicte dlabelsfrommultiple nodestodetermine thecluster’s anomaly la-
bel,MultiLog first emplo ysStandalone Estimation toparse ,embed,
reduce dimensions, andcompr esslogs frommultiple nodes.This
process ensur esthat, ineach Time Windo w,each nodeoutputs a
variable-length probability list.Follo wing this, theCluster Classifier
integrates these probability lists fromeach nodetoconclusiv ely
determine whether thecurrentcluster isanomalous ornormal.
5.1 Standalone Estimation
MultiLog transforms rawlogsequences into threetypesofinfor-
mation: sequential, quantitativ e,andsemantic, with theaim of
preserving acompr ehensiverange oflogdata forlater cluster classi-
fication. Tocaptur elong-term temp oraldependencies andenhance
crucial information, wefurther integrate self-attention. Summar-
ily,Standalone Estimation encompasses fivestages: logparsing &
grouping, sequential embedding, quantitativ eembedding, semantic
embedding, andinformation enhancement.
5.1.1 LogParsing &Grouping. Asdiscusse dinSection 2.1,raw
logmessages areunstructur edandinclude variable logparameters,
presenting challenges toautomate danalysis. Thus, aligning with
log-base danomaly detection practices, MultiLog emplo yslogpars-
ingtoextract structur edlogevents fromthese messages, thereby
enabling moreefficient analysis. Specifically ,MultiLog utilizes the
state-of-the-art Drain[ 17]metho dforlogparsing, which hasbeen
provenhighly effectiveandefficient inexisting studies[ 60].After
logparsing, theentir etyofrawlogmessages inthecurrenttime
windo wcanberepresente das𝑆=(𝑠1,𝑠2,...,𝑠𝑀).Subse quently ,for
theconv enience offurther processing, weemplo ythefixedwin-
dowmetho d[29]tofurther segment𝑆into aseries offixed-length
sequences𝑆𝑗,each oflength𝑀.
5.1.2 LogEmbedding. After loggrouping, each batch ofrawlog
messages isparse dinto logevents, forming natural sequential
patterns. The entir etyofraw logmessages canberepresente d
as𝐸𝑗=(𝑒(𝑠1),𝑒(𝑠2),...,𝑒(𝑠𝑀)),wher eeach eventisrepresente d
as𝑒𝑖.Onthisbasis, weobtain quantitativ epatterns. Tocaptur e
thisaspect,wefurther perform thefollowing transformation: For
each groupoflogsequence𝑆𝑗,wecompute thecount vector as
𝐶𝑗=(𝑐𝑗(𝑒1),𝑐𝑗(𝑒2),...,𝑐𝑗(𝑒𝑛),wher e𝑐𝑗(𝑒𝑘)represents thefre-
quency of𝑒𝑘intheeventvector sequence𝐸𝑗.Beyondsequential
andquantitativ epatterns, aneventsequence canalsoencapsulate
semantic patterns. Toextract thesemantic information fromlog
events, MultiLog treatseach logeventasasentence innatural lan-
guage .Indetail, wecancompute thesemantic embedding of𝐸𝑗
as𝑉𝑗=(𝑣(𝑒(𝑠1)),𝑣(𝑒(𝑠2)),...,𝑣(𝑒(𝑠𝑀))),wher e𝑣(𝑒(𝑠𝑘))represents
thesemantic embedding oflogevent𝑒(𝑠𝑘).Moredetaile dformula
details areillustrate dinAppendix A.
5.1.3 Information Enhancement. Foreach pattern previously ana-
lyzed,weemplo yanLSTM with self-attention toamplify keyinfor-
mation. Each embedding output—𝐸,𝐶,and𝑉—ser vesastheinput
foranLSTM. TheLSTM’s output isdenote das𝐻=[ℎ1,ℎ2,...,ℎ𝑀],
wher eℎ𝑚represents thehidden state atlogevent𝑚,and𝑀isthe
sizeofthewindo w.Theself-attention mechanism, applie dtothisse-
quence ofhidden states, iscalculate dasEquation 1,with thescoring
function define das𝑠𝑐𝑜𝑟𝑒(ℎ𝑚,ℎ𝑀)=ℎ⊺
𝑚𝑊ℎ𝑀,and𝑊asalearnable
weight matrix. Theconte xtvector𝑐,aweighte dsum ofthehidden
4261Multivariate Log-based Anomaly Detection for Distributed Database KDD ’24, August 25–29, 2024, Barcelona, Spain.
      Node1Node1      Node2Node2     NodeN−1NodeN−1….      NodeNNodeNLog
Log
Log
Log
TimeTime WindowP1P1P2P2PN−1PN−1PNPNCluster Classifier….Standalone EstimationPredict
Predict
Predict
PredictRaw ProbsAutoEncoderEncoded ProbsMeta-Classifier
NormalAnomalous 
Alert 
Training LogNew LogLog Parsing & GroupingLog Msg. Seq. #3Log Msg. Seq. #2Log Msg. Seq. #1Log Event #3Log Event #2Log Event #1Embedding SequentialQuantitative   SemanticLSTMSelf-Attention
Figure 6: The Framework of MultiLog
states, is computed as 𝑐=Í𝑀
𝑚=1𝛼𝑚ℎ𝑚. Subsequently, the enhanced
output vector 𝐸𝐶for each pattern is obtained by concatenating the
context vector 𝑐with the last hidden state ℎ𝑀,𝐸𝐶=[𝑐;ℎ𝑀]. Fi-
nally, these enhanced output vectors are concatenated and passed
through a fully connected layer, represented mathematically as
𝑝=𝐹𝐶([𝐸𝐶𝐸;𝐸𝐶𝐶;𝐸𝐶𝑉]).
𝛼𝑚=𝑒𝑥𝑝(𝑠𝑐𝑜𝑟𝑒(ℎ𝑚,ℎ𝑀))
Í𝑀
𝑚′=1𝑒𝑥𝑝(𝑠𝑐𝑜𝑟𝑒(ℎ𝑚′,ℎ𝑀))(1)
5.2 Cluster Classifier
After acquiring the probabilities calculated by the Standalone Es-
timation from the database logs of each node, MultiLog employs
the Cluster Classifier to predict the likelihood of the cluster being
anomalous or normal. To standardize the length of the probabilities
output by each node, an AutoEncoder is applied. Subsequently, Mul-
tiLog utilizes a meta-classifier to concatenate these standardized
probabilities and predict the final outcome for the cluster in the
current time window. In summary, the Cluster Classifier consists
of two key steps: AutoEncoder for probability standardization and
Meta-Classification for final cluster analysis.
5.2.1 AutoEncoder. Given that different nodes generate varying
amounts of log data within the same time window, as previously
described, we group logs based on a fixed length of 𝑀and assign
a probability 𝑝∈{0,1}indicating the likelihood of anomaly for
each group. Consequently, for a database with 𝑁nodes, it can be
inferred that in each time window, the probability list for node
𝑁𝑜𝑑𝑒𝑖is𝑃𝑖=[𝑝1,𝑝2,...,𝑝𝑘𝑖], where𝑝𝑗represents the final output
for each group as mentioned earlier, and 𝑘𝑖denotes the number of
groups.
𝑍𝑖=𝑓𝑒𝑛𝑐(𝑃𝑖) (2a)
ˆ𝑃𝑖=𝑓𝑑𝑒𝑐(𝑍𝑖) (2b)Subsequently, for the node outputs {𝑃1,𝑃2,...,𝑃𝑁}, MultiLog
firstly apply the encoder function 𝑓𝑒𝑛𝑐, which truncates or pads
the input probability list 𝑃𝑖to a fixed length 𝛽, and then maps it
to a latent representation 𝑍𝑖of a fixed size 𝜇. This process is for-
mulated in Equation 2a and involves three linear layers with ReLU
activation functions. The decoder function 𝑑𝑑𝑒𝑐then endeavors to
reconstruct the original input from 𝑍𝑖, as outlined in Equation 2b,
with the reconstructed probability list denoted as ˆ𝑃𝑖. To align the
probability lists of each node, we utilize MSELoss to minimize the
discrepancy between 𝑃𝑖and ˆ𝑃𝑖, thereby achieving a standardized,
dense representation 𝑍𝑖for each node’s probability list.
5.2.2 Meta-Classification. After standardizing probabilities with
the AutoEncoder, MultiLog employs Meta-Classification for the
final assessment of the cluster’s anomaly status. The latent rep-
resentations from each node’s standardized probability list, de-
noted as{𝑍1,𝑍2,...,𝑍𝑁}, are concatenated to form a single vector
as Equation 3a. This concatenated vector 𝑍is then fed into the meta-
classifier. The meta-classifier, denoted as 𝑓𝑚𝑒𝑡𝑎 , is a neural network
comprising one hidden layer and an output layer, the latter using a
softmax activation function. The prediction of the overall anomaly
status of the cluster is then formulated as in Equation 3b. This
process determines whether the cluster is classified as "anomalous"
or "normal", based on the collective information from all nodes.
𝑍=[𝑍1;𝑍2;...;𝑍𝑁] (3a)
𝑃{𝑛𝑜𝑟𝑚𝑎𝑙,𝑎𝑛𝑜𝑚𝑎𝑙𝑜𝑢𝑠 }=𝑓𝑚𝑒𝑡𝑎(𝑍) (3b)
6 Experiment and Evaluation
In our experiments, we utilize the previously mentioned dataset
to evaluate the effectiveness of MultiLog in comparison with state-
of-the-art methods. Unless specified otherwise, we set the time
window size to 5 seconds. For MultiLog, we configure the input
4262KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
Table 6: F1-Score of Overall Evaluation
DatasetPLELog RobustLog LogAnomaly MultiLog (our’s)
Single. Vote. Best. Single. Vote. Best. Single. Vote. Best. N.A.
Single2Single 43.98% 44.76% 80.95% 79.80% 85.18% 98.83% 93.40% 95.69% 98.80% 98.91%
Single2Multi 42.62% 43.25% 79.49% 81.00% 82.75% 95.96% 93.19% 90.34% 96.45% 99.11%
Multi2Single 38.94% 83.79% 51.02% 38.94% 98.16% 98.54% 95.96% 83.80% 99.56% 99.75%
Multi2Multi 48.80% 33.12% 94.52% 66.63% 59.97% 97.42% 88.35% 55.97% 98.97% 99.82%
size (𝛽) of the AutoEncoder to be 128 and the output size ( 𝜇) of the
AutoEncoder to be 32.
6.1 Compared Approaches
Regarding the existing log-based anomaly detection approaches, we
adopt PLELog[ 53], RobustLog[ 61] and LogAnomaly[ 41]. In partic-
ular, we use the their public implementations[ 9,30] and determine
their parameters by first reproducing the results in their correspond-
ing studies.
To conduct a more comprehensive comparison of MultiLog with
these works, we additionally implement two additional methods
for integrating logs from different nodes, beyond the Single-Point
Classification approach.
Vote-Based Classification: This method employs a voting sys-
tem among nodes to make decisions, a concept often utilized in
distributed consensus algorithms. It involves aggregating the de-
cisions from individual nodes, where each node’s prediction label
(𝐿𝑖∈{0,1}) is considered a "vote". The final decision is determined
based on the majority of votes, signifying that ifÍ𝑁
𝑖=1𝐿𝑖>𝑁
2in a
cluster with 𝑁nodes, it is identified as anomalous; otherwise, it is
considered normal.
Best-Node Classification: This approach designates the node
with the best overall performance as the deciding factor for cluster
anomaly detection. While this method may not be feasible in real-
time operation, it serves as a theoretical benchmark representing
the optimal solution in certain scenarios. Essentially, it assumes
that one node, due to its superior performance or accuracy in anom-
aly detection, can be the sole determinant of the cluster’s state,
thus providing insight into the best possible outcomes under ideal
conditions.
6.2 Evaluation Results
We first conduct cluster anomaly detection based on state-of-art
models: PLELog, RobustLog, and LogAnomaly, employing the
Single-Point Classification, Vote-Based Classification, and Best-
Node Classification methods as previously described, and compare
them with our proposed MultiLog on our dataset.
As shown in Table 6, it is observable that for the Multi2Single
dataset, RobustLog achieves the best result with Vote-Based Clas-
sification (F1-Score of 98.16%), closely approaching the Best-Node
Classification, which is impractical in real-world use, yet still fell
short of MultiLog, which reaches 99.75% on this dataset, surpassing
the performance on PLELog, RobustLog, and LogAnomaly using
Best-Node Classification. Furthermore, for the Multi2Multi dataset,
the effectiveness of MultiLog far exceeds the best results of theseTable 7: Experiment Results of Studied Approaches on
Multi2Single and Multi2Multi Dataset
Dataset Method Precision Recall F1-Score
Multi2SinglePLELog 24.18% 100.00% 38.94%
RobustLog 24.18% 100.00% 38.94%
LogAnomaly 95.88% 96.05% 95.96%
MultiLog 99.88% 99.62% 99.75%
Multi2MultiPLELog 34.52% 83.24% 48.80%
RobustLog 49.96% 100.00% 66.63%
LogAnomaly 72.23% 99.78% 83.80%
MultiLog 99.88% 99.75% 99.82%
three state-of-the-art models (LogAnomaly with Single-Point Clas-
sification), by approximately 11.5%. This indicates that MultiLog,
through the combination of Standalone Estimation and Cluster
Classifier, can effectively perform anomaly detection in complex,
multi-node scenarios with multiple anomalies. Moreover, even in
simpler scenarios with a single type of anomaly (Single2Single,
Single2Multi), MultiLog also achieved better results than the state-
of-the-art models, with nearly 99% F1-Score, surpassing the perfor-
mance of the infeasible Best-Node Classification in state-of-the-art
models. This demonstrates that MultiLog is highly effective even
for single-anomaly scenarios.
To delve deeper into the specifics under the Multi2Single and
Multi2Multi datasets, we further conduct a detailed comparison
between the state-of-the-art models (using Single-Point Classifica-
tion, as this approach is commonly adopted in practical scenarios)
and MultiLog. As depicted in Table 7, it is apparent that the superi-
ority of MultiLog stems from its remarkably low false positive rate.
Whether it’s PLELog, RobustLog, or LogAnomaly, their Recall rates
are nearly 100%, indicating their ability to almost always detect all
anomalies. However, their Precision varies (ranging from 24.18% for
RobustLog to 72.23% for LogAnomaly in the Multi2Multi dataset),
whereas MultiLog’s Precision is also close to 100%. In summary,
MultiLog surpasses these state-of-the-art models chiefly due to
its substantially lower rate of false positives, which, in practical
production environments, can greatly reduce the workload of DBA
(database administrator).
To validate the effectiveness and stability of MultiLog, we con-
duct additional ablation experiments (Appendix B).
4263Multivariate Log-based Anomaly Detection for Distributed Database KDD ’24, August 25–29, 2024, Barcelona, Spain.
6.3 Limitations
Though our approach can significantly enhance anomaly detection
performance with multivariate logs, there still exist several notable
limitations. Firstly, our method is supervised. However, it can be
adapted to unsupervised scenarios by integrating the Cluster Clas-
sifier with unsupervised models such as clustering. Secondly, due
to increased network transmission and computational overhead,
our approach operates less efficiently than state-of-the-art methods.
In scenarios with very few instances of certain anomaly types, it
may be relatively impractical. Thirdly, while our dataset includes a
diverse range of injected anomalies compared to other datasets, it
still deviates from real-world scenarios.
7 Related Work
7.1 Log-Based Anomaly Detection and
Diagnosis
Log analysis for fault detection and localization is a well-established
research area[ 7,10,11,15,21,22,37,41,49,53,61]. These method-
ologies typically involve extracting templates and key information
from logs, followed by constructing models for anomaly detection
and classification. There are mainly two types of models in this
domain: graph-based and deep-learning models.
Graph-based models leverage log events parsed from log files
to create a graph-based representation. They detect conflicts and
anomalies by comparing event sequences against this graph. For
instance, LogFlash[ 22] utilizes a real-time streaming process for
log transitions, enhancing the speed of anomaly detection and
diagnosis. HiLog[ 21] performs an empirical study on four anti-
patterns that challenge the assumptions underlying anomaly de-
tection and diagnosis models, proposing a human-in-the-loop ap-
proach to integrate human expertise into log-based anomaly detec-
tion. LogKG[ 49] introduces a Failure-Oriented Log Representation
(FOLR) method to extract failure-related patterns, using the OPTICS
clustering method for anomaly diagnosis.
Deep-learning models, conversely, use various neural networks
to model sequences of log events. LogRobust[ 61] applies Term
Frequency-Inverse Document Frequency (TF-IDF) and word vector-
ization to convert log events into semantic vectors, thus improving
the accuracy of anomaly detection. UniParser[ 37] employs a token
encoder and a context encoder to learn patterns from log tokens
and their adjacent contexts.
7.2 Anomaly Detection and Diagnosis for
Database
Several anomaly detection and diagnosis approaches have been
developed for databases[ 31,36,39,62], particularly through the use
of metrics data. FluxInfer[ 36] builds a weighted undirected depen-
dency graph to illustrate the dependency relationships of anoma-
lous monitoring data and employs a weighted PageRank algorithm
for diagnosing online database anomalies. iSQUAD[ 39] adopts the
Bayesian Case Model to diagnose anomalies of intermittent slow
queries (iSQs) in cloud databases, based on Key Performance In-
dicators (KPI) data. OpenGauss[ 31,62], an autonomous database
system, implements an LSTM-based auto-encoder with an attention
layer for system-level anomaly diagnosis, also leveraging featuresin metrics data. Sentinel[ 13] constructs a fine-grained model of data
system behavior through debug logs to assist DBAs in diagnosing
anomalies.
To the best of our knowledge, we are the first to construct a
dataset specifically designed for log-based anomaly detection in
databases. Distinct from prior research, our focus is on leveraging
database logs instead of metric data. Additionally, our approach
emphasizes the utilization of logs from distributed nodes, setting it
apart from related work that concentrates on addressing anomaly
detection issues in standalone nodes.
8 Conclusions
In this paper, we study the problem of multivariate log-based anom-
aly detection in distributed databases. Initially, we conduct a com-
prehensive study to evaluate state-of-the-art anomaly detection
models specific to distributed databases. This study reveals that
these models experience a decline in classification effectiveness
when dealing with multiple database anomalies and exhibit a high
false positive rate when using multivariate logs. Based on the study,
we introduce MultiLog, a multivariate log-based anomaly detec-
tion approach for distributed database. MultiLog adeptly leverages
multivariate logs from all nodes within the distributed database
cluster, significantly enhancing the efficacy of anomaly detection.
For evaluation, we release the first open-sourced comprehensive
dataset featuring multivariate logs from distributed databases. Our
experimental evaluations based on this dataset demonstrate the
effectiveness of MultiLog.
In future work, our research will concentrate on incorporating
more characteristics of distributed databases for anomaly detection.
Additionally, we will further explore anomaly diagnosis and root
cause analysis for distributed databases based on log data.
Acknowledgement
This work was supported by the National Key R&D Research Fund
of China (2021YFF0704202).
References
[1]Anton Babenko, Leonardo Mariani, and Fabrizio Pastore. 2009. AVA: automated
interpretation of dynamically detected anomalies. In Proceedings of the eighteenth
international symposium on Software testing and analysis. 237–248.
[2] Adam Björnberg. 2021. Cloud native chaos engineering for IoT systems.
[3]Peter Bodik, Moises Goldszmidt, Armando Fox, Dawn B Woodard, and Hans
Andersen. 2010. Fingerprinting the datacenter: automated classification of perfor-
mance crises. In Proceedings of the 5th European conference on Computer systems.
111–124.
[4]Michael Alan Chang, Bredan Tschaen, Theophilus Benson, and Laurent Vanbever.
2015. Chaos monkey: Increasing sdn reliability through systematic network
destruction. In Proceedings of the 2015 ACM Conference on Special Interest Group
on Data Communication. 371–372.
[5]Mike Chen, Alice X Zheng, Jim Lloyd, Michael I Jordan, and Eric Brewer. 2004.
Failure diagnosis using decision trees. In International Conference on Autonomic
Computing, 2004. Proceedings. IEEE, 36–43.
[6]Pengfei Chen, Yong Qi, Pengfei Zheng, and Di Hou. 2014. Causeinfer: Auto-
matic and distributed performance diagnosis with hierarchical causality graph in
large distributed systems. In IEEE INFOCOM 2014-IEEE Conference on Computer
Communications. IEEE, 1887–1895.
[7]Rui Chen, Shenglin Zhang, Dongwen Li, Yuzhe Zhang, Fangrui Guo, Weibin
Meng, Dan Pei, Yuzhi Zhang, Xu Chen, and Yuqing Liu. 2020. Logtransfer: Cross-
system log anomaly detection for software systems with transfer learning. In 2020
IEEE 31st International Symposium on Software Reliability Engineering (ISSRE).
IEEE, 37–47.
[8]James C Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost,
Jeffrey John Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser,
4264KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
Peter Hochschild, et al .2013. Spanner: Google’s globally distributed database.
ACM Transactions on Computer Systems (TOCS) 31, 3 (2013), 1–22.
[9] donglee afar. 2023. logdeep. https://github.com/donglee-afar/logdeep
[10] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proceedings
of the 2017 ACM SIGSAC conference on computer and communications security.
1285–1298.
[11] Chiming Duan, Tong Jia, Ying Li, and Gang Huang. 2023. AcLog: An Approach
to Detecting Anomalies from System Logs with Active Learning. In Proceedings
of the 27th IEEE International Conference on Web Services. 1021–1030.
[12] Ilenia Fronza, Alberto Sillitti, Giancarlo Succi, Mikko Terho, and Jelena Vlasenko.
2013. Failure prediction based on log files using random indexing and support
vector machines. Journal of Systems and Software 86, 1 (2013), 2–11.
[13] Brad Glasbergen, Michael Abebe, Khuzaima Daudjee, and Amit Levi. 2020. Sen-
tinel: universal analysis and insight for data systems. Proceedings of the VLDB
Endowment 13, 12 (2020), 2720–2733.
[14] Haixuan Guo, Shuhan Yuan, and Xintao Wu. 2021. Logbert: Log anomaly detec-
tion via bert. In 2021 international joint conference on neural networks (IJCNN).
IEEE, 1–8.
[15] Xiao Han and Shuhan Yuan. 2021. Unsupervised cross-system log anomaly
detection via domain adaptation. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management. 3068–3072.
[16] Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R Lyu. 2016. An evaluation
study on log parsing and its use in log mining. In 2016 46th annual IEEE/IFIP
international conference on dependable systems and networks (DSN). IEEE, 654–
661.
[17] Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R Lyu. 2017. Drain: An online
log parsing approach with fixed depth tree. In 2017 IEEE international conference
on web services (ICWS). IEEE, 33–40.
[18] Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen, Liu
Tang, Yuxing Zhou, Menglong Huang, et al .2020. TiDB: a Raft-based HTAP
database. Proceedings of the VLDB Endowment 13, 12 (2020), 3072–3084.
[19] Yintong Huo, Yichen Li, Yuxin Su, Pinjia He, Zifan Xie, and Michael R Lyu. 2023.
AutoLog: A Log Sequence Synthesis Framework for Anomaly Detection. arXiv
preprint arXiv:2308.09324 (2023).
[20] Vimalkumar Jeyakumar, Omid Madani, Ali Parandeh, Ashutosh Kulshreshtha,
Weifei Zeng, and Navindra Yadav. 2019. ExplainIt!–A declarative root-cause
analysis engine for time series data. In Proceedings of the 2019 International
Conference on Management of Data. 333–348.
[21] Tong Jia, Ying Li, Yong Yang, Gang Huang, and Zhonghai Wu. 2022. Augmenting
Log-based Anomaly Detection Models to Reduce False Anomalies with Human
Feedback. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 3081–3089.
[22] Tong Jia, Yifan Wu, Chuanjia Hou, and Ying Li. 2021. LogFlash: Real-time stream-
ing anomaly detection and diagnosis from system logs for large-scale software
systems. In 2021 IEEE 32nd International Symposium on Software Reliability Engi-
neering (ISSRE). IEEE, 80–90.
[23] Tong Jia, Lin Yang, Pengfei Chen, Ying Li, Fanjing Meng, and Jingmin Xu. 2017.
Logsed: Anomaly diagnosis through mining time-weighted control flow graph
in logs. In 2017 IEEE 10th International Conference on Cloud Computing (CLOUD).
IEEE, 447–455.
[24] Pinjia He Jinyang Liu Michael R. Lyu Jieming Zhu, Shilin He. 2023. Loghub: A
large collection of system log datasets towards automated log analytics. (2023).
[25] Yuyuan Kang, Xiangdong Huang, Shaoxu Song, Lingzhe Zhang, Jialin Qiao, Chen
Wang, Jianmin Wang, and Julian Feinauer. 2022. Separation or not: On handing
out-of-order time-series data in leveled lsm-tree. In 2022 IEEE 38th International
Conference on Data Engineering (ICDE). IEEE, 3340–3352.
[26] Jinhan Kim, Valeriy Savchenko, Kihyuck Shin, Konstantin Sorokin, Hyunseok
Jeon, Georgiy Pankratenko, Sergey Markov, and Chul-Joo Kim. 2020. Automatic
abnormal log detection by analyzing log history for providing debugging in-
sight. In Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering: Software Engineering in Practice. 71–80.
[27] Max Landauer, Sebastian Onder, Florian Skopik, and Markus Wurzenberger. 2023.
Deep learning for anomaly detection in log data: A survey. Machine Learning
with Applications 12 (2023), 100470.
[28] Max Landauer, Florian Skopik, and Markus Wurzenberger. 2023. A Critical Review
of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly
Detection Techniques. arXiv preprint arXiv:2309.02854 (2023).
[29] Van-Hoang Le and Hongyu Zhang. 2022. Log-based anomaly detection with
deep learning: How far are we?. In Proceedings of the 44th international conference
on software engineering. 1356–1367.
[30] LeonYang95. 2023. PLELog. https://github.com/LeonYang95/PLELog
[31] Guoliang Li, Xuanhe Zhou, Ji Sun, Xiang Yu, Yue Han, Lianyuan Jin, Wenbo Li,
Tianqing Wang, and Shifu Li. 2021. opengauss: An autonomous database system.
Proceedings of the VLDB Endowment 14, 12 (2021), 3028–3042.
[32] Yinglung Liang, Yanyong Zhang, Hui Xiong, and Ramendra Sahoo. 2007. Failure
prediction in ibm bluegene/l event logs. In Seventh IEEE International Conference
on Data Mining (ICDM 2007). IEEE, 583–588.[33] Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. 2016.
Log clustering based problem identification for online service systems. In Pro-
ceedings of the 38th International Conference on Software Engineering Companion.
102–111.
[34] Dewei Liu, Chuan He, Xin Peng, Fan Lin, Chenxi Zhang, Shengfang Gong, Ziang
Li, Jiayu Ou, and Zheshun Wu. 2021. Microhecl: High-efficient root cause local-
ization in large-scale microservice systems. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP).
IEEE, 338–347.
[35] Ping Liu, Yu Chen, Xiaohui Nie, Jing Zhu, Shenglin Zhang, Kaixin Sui, Ming
Zhang, and Dan Pei. 2019. Fluxrank: A widely-deployable framework to auto-
matically localizing root cause machines for software service failure mitigation.
In2019 IEEE 30th International Symposium on Software Reliability Engineering
(ISSRE). IEEE, 35–46.
[36] Ping Liu, Shenglin Zhang, Yongqian Sun, Yuan Meng, Jiahai Yang, and Dan
Pei. 2020. Fluxinfer: Automatic diagnosis of performance anomaly for online
database system. In 2020 IEEE 39th International Performance Computing and
Communications Conference (IPCCC). IEEE, 1–8.
[37] Yudong Liu, Xu Zhang, Shilin He, Hongyu Zhang, Liqun Li, Yu Kang, Yong Xu,
Minghua Ma, Qingwei Lin, Yingnong Dang, et al .2022. Uniparser: A unified log
parser for heterogeneous log data. In Proceedings of the ACM Web Conference
2022. 1893–1901.
[38] Siyang Lu, Xiang Wei, Yandong Li, and Liqiang Wang. 2018. Detecting anomaly
in big data system logs using convolutional neural network. In 2018 IEEE 16th Intl
Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive
Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing
and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech) .
IEEE, 151–158.
[39] Minghua Ma, Zheng Yin, Shenglin Zhang, Sheng Wang, Christopher Zheng, Xin-
hao Jiang, Hanwen Hu, Cheng Luo, Yilin Li, Nengjun Qiu, et al .2020. Diagnosing
root causes of intermittent slow queries in cloud databases. Proceedings of the
VLDB Endowment 13, 8 (2020), 1176–1189.
[40] Adetokunbo AO Makanju, A Nur Zincir-Heywood, and Evangelos E Milios. 2009.
Clustering event logs using iterative partitioning. In Proceedings of the 15th
ACM SIGKDD international conference on Knowledge discovery and data mining.
1255–1264.
[41] Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao
Chen, Ruizhi Zhang, Shimin Tao, Pei Sun, et al .2019. Loganomaly: Unsupervised
detection of sequential and quantitative anomalies in unstructured logs.. In IJCAI,
Vol. 19. 4739–4745.
[42] Chaos Mesh. 2021. A powerful chaos engineering platform for kubernetes. URL:
https://chaos-mesh.org/.accessed (2021), 10–07.
[43] Adam Oliner and Jon Stearley. 2007. What supercomputers say: A study of
five system logs. In 37th annual IEEE/IFIP international conference on dependable
systems and networks (DSN’07). IEEE, 575–584.
[44] Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O’Neil. 1996. The
log-structured merge-tree (LSM-tree). Acta Informatica 33 (1996), 351–385.
[45] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[46] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in
automatic text retrieval. Information processing & management 24, 5 (1988),
513–523.
[47] Subhadeep Sarkar, Niv Dayan, and Manos Athanassoulis. 2023. The LSM De-
sign Space and its Read Optimizations. In Proceedings of the IEEE International
Conference on Data Engineering (ICDE).
[48] Subhadeep Sarkar, Dimitris Staratzis, Ziehen Zhu, and Manos Athanassoulis.
2021. Constructing and analyzing the LSM compaction design space. Proceedings
of the VLDB Endowment 14, 11 (2021).
[49] Yicheng Sui, Yuzhe Zhang, Jianjun Sun, Ting Xu, Shenglin Zhang, Zhengdan
Li, Yongqian Sun, Fangrui Guo, Junyu Shen, Yuzhi Zhang, et al .2023. LogKG:
Log Failure Diagnosis through Knowledge Graph. IEEE Transactions on Services
Computing (2023).
[50] Chen Wang, Xiangdong Huang, Jialin Qiao, Tian Jiang, Lei Rui, Jinrui Zhang,
Rong Kang, Julian Feinauer, Kevin A McGrail, Peng Wang, et al .2020. Apache
iotdb: time-series database for internet of things. Proceedings of the VLDB En-
dowment 13, 12 (2020), 2901–2904.
[51] Li Wu, Johan Tordsson, Jasmin Bogatinovski, Erik Elmroth, and Odej Kao. 2021.
Microdiag: Fine-grained performance diagnosis for microservice systems. In 2021
IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence). IEEE,
31–36.
[52] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. 2009.
Detecting large-scale system problems by mining console logs. In Proceedings of
the ACM SIGOPS 22nd symposium on Operating systems principles. 117–132.
[53] Lin Yang, Junjie Chen, Zan Wang, Weijing Wang, Jiajun Jiang, Xuyuan Dong,
and Wenbin Zhang. 2021. Semi-supervised log-based anomaly detection via
probabilistic label estimation. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE). IEEE, 1448–1460.
4265Multivariate Log-based Anomaly Detection for Distributed Database KDD ’24, August 25–29, 2024, Barcelona, Spain.
[54] Zhenkun Yang, Chuanhui Yang, Fusheng Han, Mingqiang Zhuang, Bing Yang,
Zhifeng Yang, Xiaojun Cheng, Yuzhong Zhao, Wenhui Shi, Huafeng Xi, et al .
2022. OceanBase: a 707 million tpmC distributed relational database system.
Proceedings of the VLDB Endowment 15, 12 (2022), 3385–3397.
[55] Kun Yin, Meng Yan, Ling Xu, Zhou Xu, Zhao Li, Dan Yang, and Xiaohong Zhang.
2020. Improving log-based anomaly detection with component-aware analysis.
In2020 IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 667–671.
[56] Dong Young Yoon, Ning Niu, and Barzan Mozafari. 2016. Dbsherlock: A per-
formance diagnostic tool for transactional databases. In Proceedings of the 2016
International Conference on Management of Data. 1599–1614.
[57] Lingzhe ZHANG, Xiangdong HUANG, Jialin QIAO, Wangminhao GOU, and
Jianmin WANG. 2021. Two-stage file compaction framework by log-structured
merge-tree for time series data. Journal of Computer Applications 41, 3 (2021),
618.
[58] Lingzhe Zhang, Yuqing Zhu, Yanzhe An, Yuan Zi, and Jianmin Wang. 2024. The
optimization problem of system load balancing and its solution for industrial
Internet-of-Things data management. SCIENTIA SINICA Informationis (2024).
https://doi.org/10.1360/SSI-2023-0211
[59] Ling-Zhe Zhang, Xiang-Dong Huang, Yan-Kai Wang, Jia-Lin Qiao, Shao-Xu Song,
and Jian-Min Wang. 2024. Time-tired compaction: An elastic compaction schemefor LSM-tree based time-series database. Advanced Engineering Informatics 59
(2024), 102224.
[60] Tianzhu Zhang, Han Qiu, Gabriele Castellano, Myriana Rifai, Chung Shue Chen,
and Fabio Pianese. 2023. System Log Parsing: A Survey. IEEE Transactions on
Knowledge and Data Engineering (2023).
[61] Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,
Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al .2019. Robust log-based
anomaly detection on unstable log data. In Proceedings of the 2019 27th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 807–817.
[62] Xuanhe Zhou, Lianyuan Jin, Ji Sun, Xinyang Zhao, Xiang Yu, Jianhua Feng, Shifu
Li, Tianqing Wang, Kun Li, and Luyang Liu. 2021. Dbmind: A self-driving platform
in opengauss. Proceedings of the VLDB Endowment 14, 12 (2021), 2743–2746.
[63] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, and Dan Ding. 2018.
Fault analysis and debugging of microservice systems: Industrial survey, bench-
mark system, and empirical study. IEEE Transactions on Software Engineering 47,
2 (2018), 243–260.
[64] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R
Lyu. 2019. Tools and benchmarks for automated log parsing. In 2019 IEEE/ACM
41st International Conference on Software Engineering: Software Engineering in
Practice (ICSE-SEIP). IEEE, 121–130.
4266KDD ’24, August 25–29, 2024, Barcelona, Spain. Lingzhe Zhang et al.
A Log Embedding
A.1 Sequential Embedding.
After log grouping, each batch of raw log messages is parsed into log
events, forming natural sequential patterns. Specifically, each event
is represented as a distinct vector 𝑒𝑖, and the entire collection of
unique event vectors is denoted as 𝜔={𝑒1,𝑒2,...,𝑒𝑛}. Consequently,
the event vector sequence 𝑆𝑗is transformed as Equation 4, where
𝐸𝑗constitutes the sequential embedding of this particular group of
log message sequences.
𝐸𝑗=(𝑒(𝑠1),𝑒(𝑠2),...,𝑒(𝑠𝑀)) (4)
A.2 Quantitative Embedding.
Beyond sequential patterns, an event sequence also exhibits quan-
titative patterns. Typically, normal program execution follows cer-
tain invariants, with specific quantitative relationships consistently
maintained in logs under various inputs and workloads. For in-
stance, each file opened during a process is eventually closed. As
such, the number of logs indicating "create a memtable" should
match those showing "flush a memtable to file" under normal condi-
tions. These quantitative relationships within logs are indicative of
standard program execution behaviors. Anomalies can be identified
when new logs disrupt these invariants. To capture this aspect, we
analyze the quantitative pattern of logs as follows: For each group
of log sequence 𝑆𝑗, we compute the count vector as Equation 5,
where𝑐𝑗(𝑒𝑘)represents the frequency of 𝑒𝑘in the event vector
sequence(𝑠1,𝑠2,...,𝑠𝑚), and𝑒𝑘∈𝜔.
𝐶𝑗=(𝑐𝑗(𝑒1),𝑐𝑗(𝑒2),...,𝑐𝑗(𝑒𝑛) (5)
A.3 Semantic Embedding.
Beyond sequential and quantitative patterns, an event sequence
can also encapsulate semantic patterns. To extract the semantic
information from log events, MultiLog treats each log event as
a sentence in natural language. Given that log events are formu-
lated by developers to record system statuses, most tokens in these
events are English words with intrinsic semantics. However, log
events also contain non-character tokens (like delimiters, opera-
tors, punctuation marks, and number digits) and composite tokens
resulting from word concatenations (such as "NullPointerExcep-
tion") due to programming conventions. In alignment with existing
research[ 53], MultiLog initially preprocesses log events by elim-
inating non-character tokens and stop words, and decomposing
composite tokens into individual words using Camel Case splitting.
Following this, MultiLog utilizes pre-trained word vectors based on
the Common Crawl Corpus with the FastText algorithm[ 45], which
is adept at capturing the inherent relationships among words in
natural language. This means each word in a processed log event is
transformed into a 𝑑-dimensional vector (denoted as 𝑣) using the
pre-trained word2vec model, where 𝑑is set to 300 in FastText word
vectors.𝑇𝐹(𝑤)=#𝑤
#𝑊(6a)
𝐼𝐷𝐹(𝑤)=𝑙𝑜𝑔(#𝐿
#𝐿𝑤) (6b)
After transforming each word to a 𝑑-dimension vector via Fast-
Text word embedding, MultiLog further transforms a log event into
a semantic vector by aggregating all the word vectors within it. For
this aggregation, MultiLog adopts TF-IDF[ 46], taking into account
the significance of each word. Term Frequency (TF) measures how
frequently a word assesses the frequency of a word 𝑤in a log event,
calculated as Equation 6a, where #𝑤represents the count of 𝑤in
the log event and #𝑊represents the total word count in the log
event. Inverse Document Frequency (IDF) evaluates the commonal-
ity or rarity of 𝑤iacross all log events, computed as Equation 6b,
where #𝐿is the total number of log events, and #𝐿𝑤is the number
of log events containing 𝑤. The weight of a word (denoted as 𝜖) is
determined by 𝑇𝐹×𝐼𝐷𝐹. Ultimately, the semantic vector (denotes
𝑉) of a log event is derived by summing up all word vectors in the
log event, weighted by their TF-IDF values, as 𝑉=1
𝑊Í𝑊
𝑖=1𝜖𝑖.
B Effectiveness of Cluster Classifier
Although the effectiveness of MultiLog is a combined result of the
Standalone Estimation and Cluster Classifier, the latter plays a cru-
cial role in utilizing information from multiple nodes. Therefore,
to further analyze its impact, we conduct comparative validation
experiments on both the Multi2Single and Multi2Multi datasets.
For the probability lists resulting from Standalone Estimation, we
employ both the Single-Based Classification and Vote-Based Classi-
fication to judge whether the cluster is anomalous or normal.
Precision Recall F1-score020406080100120 Score (%)90.7299.7795.03100
26.7642.2299.88 99.62 99.75Single-Point V ote-Based Cluster (ours)
(a) Multi2Single
Precision Recall F1-score020406080100120 Score (%)88.29100
93.78100
39.2156.3499.88 99.75 99.82Single-Point V ote-Based Cluster (ours) (b) Multi2Multi
Figure 7: Results with(out) Cluster Classifier
As depicted in Figure 7, when employing Vote-Based Classifica-
tion, a consistently high Precision (always 100%) is maintained,
but the Recall is low (26.76% for Multi2Single and 39.31% for
Multi2Multi), indicating that many anomalies are missed. In con-
trast, using Single-Point Classification balances Precision and Recall
to some extent, significantly improving the F1-Score to approxi-
mately 95% on both datasets, surpassing the results achieved by
state-of-the-art methods mentioned previously. However, with the
implementation of the Cluster Classifier, the F1-Score exceeds 99%
on both datasets, demonstrating the Cluster Classifier’s superior
ability to utilize information from multiple nodes effectively.
4267