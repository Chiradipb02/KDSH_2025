Unraveling Block Maxima Forecasting Models with
Counterfactual Explanation
Yue Deng
Michigan State University
East Lansing, Michigan, USA
dengyue1@msu.eduAsadullah Hill Galib
Michigan State University
East Lansing, Michigan, USA
galibasa@msu.edu
Pang-Ning Tan
Michigan State University
East Lansing, Michigan, USA
ptan@msu.eduLifeng Luo
Michigan State University
East Lansing, Michigan, USA
lluo@msu.edu
ABSTRACT
Disease surveillance, traffic management, and weather forecasting
are some of the key applications that could benefit from block max-
ima forecasting of a time series as the extreme block maxima values
often signify events of critical importance such as disease outbreaks,
traffic gridlock, and severe weather conditions. As the use of deep
neural network models for block maxima forecasting increases, so
does the need for explainable AI methods that could unravel the in-
ner workings of such black box models. To fill this need, this paper
presents a novel counterfactual explanation framework for block
maxima forecasting models. Unlike existing methods, our proposed
framework, DiffusionCF, combines deep anomaly detection with
a conditional diffusion model to identify unusual patterns in the
time series that could help explain the forecasted extreme block
maxima. Experimental results on several real-world datasets demon-
strate the superiority of DiffusionCF over other baseline methods
when evaluated according to various metrics, particularly their
informativeness and closeness. Our data and codes are available at
https://github.com/yue2023cs/DiffusionCF.
CCS CONCEPTS
•Computing methodologies →Neural networks.
KEYWORDS
Explainable AI, counterfactual explanation, time series forecasting
ACM Reference Format:
Yue Deng, Asadullah Hill Galib, Pang-Ning Tan, and Lifeng Luo. 2024. Unrav-
eling Block Maxima Forecasting Models with Counterfactual Explanation.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM,
Barcelona, Spain, 12 pages. https://doi.org/10.1145/3637528.3671923
1 INTRODUCTION
Block maxima forecasting is the task of predicting the maximum
value of a time series for a future time window. Such a forecasting
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671923task has widespread applicability in many practical domains such as
weather forecasting, disease monitoring, traffic management, and
financial risk assessment. Block maxima forecasting models play
a crucial role in these domains as the predicted block maxima can
provide early warning to stakeholders about an impending severe
event. Despite their growing importance, there is a noticeable gap
in current research regarding the explainability of these models.
Explainability is important for block maxima forecasting models
as it enables stakeholders to comprehend and trust the model’s
predictions, fostering transparency and informed decision-making,
particularly in critical scenarios. The growing field of explainable AI
therefore plays a crucial role in this context, offering methodologies
and tools that can enhance the explainability of these models.
Explainable AI involves two primary methodologies: feature
attribution and counterfactual explanation methods [ 23]. Feature
attribution methods, such as LIME [ 25], SHAP [ 21], Grad-CAM [ 26],
CRP [ 1], and adversarial examples [ 10,28], focus on elucidating the
conditions behind a model’s decision, shedding light on the influ-
ential features or input values. Differently, counterfactual explana-
tion methods [ 13] seek to discover the smallest modification (i.e.,
changes) to the input that leads to a completely opposite forecast,
a.k.a., counterfactual target, by the black box model. Counterfactual
explanation methods are appealing as they offer a powerful means
to explore alternative scenarios and assess the impact of different
conditions on forecasting outcomes. For block maxima forecast-
ing, the counterfactual instances enable us to identify historical
patterns in the time series that may help explain the forecasted ex-
treme block maxima so actions can be taken to prevent their future
occurrence. In the example depicted in Figure 1, a prior incident
of epidemic outbreak may likely explain the forecasted next wave
by the black box model. The forecast is juxtaposed against a coun-
terfactual scenario which assumes preventative intervention had
been taken to mitigate the likelihood of the subsequent outbreak.
For time series, a good counterfactual instance must be (1) in-
formative, i.e., identifies the contrastive segment in the time series
that explains the generated prediction by the black box model, (2)
closely mimics the original time series, and (3) realistic, i.e., drawn
from the same distribution as the majority of the time series data.
However, striking a balance among the three criteria can be tricky.
For instance, neighborhood-based methods [ 8,20,33] consider
the nearest training instance whose prediction matches the coun-
terfactual target and utilize or partially modify them to form the
 
562
KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Figure 1: An illustration of counterfactual explanation for
block maxima forecast of a disease outbreak. The blue dot
denotes the forecasted epidemic outbreak while the red dot
represents a counterfactual scenario devoid of any outbreak.
counterfactual instance. While the counterfactual instance found
by these methods is quite realistic since it corresponds or is close
to an actual training instance, it may not resemble the original
time series. On the other hand, gradient-based methods [ 5,30,32]
learn a counterfactual instance by perturbing the time series in
such a way that maintains closeness to their original time series.
Yet these methods may induce modification across the entire time
series, making it difficult to pinpoint exactly the segment in the
time series that helps explain the model forecast, thereby reducing
its informativeness.
In this paper, we present a novel counterfactual explanation
framework for block maxima forecasting models. Unlike counter-
factual explanation for time series classification [ 3,8,18], choosing
the right counterfactual target value for block maxima is non-trivial
since the block maxima are continuous-valued, which means, there
are infinitely many possible counterfactual targets to choose from.
To address this challenge, we propose a principled way to create the
counterfactual target by leveraging the generalized extreme value
(GEV) distribution [ 7], which governs the distribution of block max-
ima values of a time series. Next, to ensure that the counterfactual
instance is informative, we constrain the area for modification by
identifying abnormal segments within the original time series, de-
parting from the conventional practice of considering the entire
time series for perturbation when constructing the counterfactual
instance. Specifically, we apply anomaly detection to each segment
of the time series and extract a subset of the segments with the high-
est anomaly scores as possible candidates for replacement. For each
candidate, we employ a conditional diffusion model [ 29] to gener-
ate a new time series segment to replace the identified anomalous
segment. This strategy of constructing a counterfactual instance by
replacing only its anomalous segment helps create counterfactual
instances that are informative, yet close to the original time series.
Our overall proposed framework, named DiffusionCF, encapsu-
lates this comprehensive approach. The primary contributions of
this work can be summarized as follows:
(1)We introduce the novel problem of counterfactual expla-
nation for block maxima forecasting models in time series,
where the counterfactual instances help identify anomalouspatterns in the time series that lead to extreme values in the
forecasted block maxima.
(2)We propose a method to create a counterfactual target for
the block maxima by leveraging the generalized extreme
value (GEV) distribution.
(3)We present DiffusionCF, a framework that balances the trade-
off between generating counterfactual instances that are
informative, yet realistic and close to the original time series.
(4)We perform extensive experiments comparing DiffusionCF
against other baseline methods under different experimental
settings. We demonstrate the versatility and effectiveness of
DiffusionCF across different real-world domains.
2 RELATED WORK
Extreme value theory (EVT) [ 7] offers a well-grounded approach
for modeling and forecasting extreme values in time series. The
theory has recently been incorporated into various deep-learning
formulations. For instance, Nishino et al. [ 24] proposed to predict
the maximum value in a forecast window using GRU with the gen-
eralized extreme value (GEV) distribution. DeepExtrema [ 12] is
another approach that uses deep learning to estimate parameters of
the GEV distribution for block maxima forecasting. The GEV distri-
bution has also been used to impute missing values in time series
for block maxima forecasting task [ 11]. Despite these advances, the
forecasts generated by the black box models can be hard to explain.
Explainable AI, as described by Molnar [ 23], encompasses two
main methodologies: feature attribution methods, such as LIME
[25], SHAP [ 21], and Grad-CAM [ 26], or counterfactual explanation
methods. LIME [ 25], a representative feature attribution method,
explains the predictions of complex machine learning models by
approximating them locally with simpler models such as linear
regression or decision trees. Differently, counterfactual explanation
methods [ 13] focus on identifying the smallest modifications to
the input features that would alter the model’s decision, offering
insights into how different inputs could lead to different outcomes.
Such methods have been applied to various domains, including
recommender systems [ 6,31], computer vision [ 9,16], and natural
language processing [ 4]. For time series prediction, there are several
ways to generate the counterfactual explanation. First, gradient-
based methods [ 3,18,22,30,32] can be used to create counterfactual
instances by minimizing the loss between the model prediction and
the desired counterfactual target while maintaining the similarity
between the perturbed and original instances. Attribution analy-
sis methods [ 19] leverage domain knowledge to select the input
features and quantify their impact on the counterfactual target.
Our work differs from past research in several key aspects. First,
we emphasize deriving counterfactual explanations for regression
instead of classification tasks, which have been the focus of many
previous studies. Second, we utilize generative AI to produce realis-
tic counterfactual instances, which is a challenge for gradient-based
methods. Third, unlike attribution analysis methods that require
domain knowledge with a limited number of factors, our method
can automatically identify the explanatory factors responsible for
the prediction.
Generative models [ 2,17] have emerged as a key machine learn-
ing paradigm in recent years due to their capacity to synthesize
 
563Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 2: Comparison of counterfactual instances under the informativeness (left), closeness (middle), and realisticness (right)
criteria. The blue dot in each figure denotes the forecasted extreme block maxima by a black box model for the period between
January and September in 2023, while the red dot denotes its corresponding counterfactual target block maxima.
realistic samples replicating the underlying data distribution. Diffu-
sion models [ 14,27,29] have recently emerged as a cutting-edge
approach due to their ability to produce high-quality samples. These
models have found success in diverse applications, including time
series imputation [ 29] and counterfactual explanation for images
[15]. However, their use for counterfactual explanations of extreme
values in time series remains largely unexplored.
3 PRELIMINARIES
3.1 Problem formulation
Consider a time series Z=𝑧1𝑧2...𝑧𝑇, where𝑇denotes the length
of the time series. Assume the time series is partitioned into a
set of distinct time windows, each denoted as 𝑤𝑡=[𝑡−𝛼,𝑡+𝛽],
respectively, where 𝑡denotes the current time step. Each window
encompasses a predictor time series, 𝑋𝑡=𝑧𝑡−𝛼𝑧𝑡−𝛼+1...𝑧𝑡, where
𝛼+1is the length of the predictor window, and a forecast time series,
𝑌𝑡=𝑧𝑡+1𝑧𝑡+2...𝑧𝑡+𝛽, where𝛽is the length of the forecast window.
The predictor window [𝑡−𝛼,𝑡]contains historical data or other
input variables employed by the forecasting model to generate its
predictions, whereas the forecast window (𝑡,𝑡+𝛽]contains the
future values to be predicted by the model. Let 𝑦𝑡=max𝜏∈1,...,𝛽𝑧𝑡+𝜏
be the block maxima of 𝑌𝑡,i.e., the maximum value over the forecast
window. Furthermore, we denote 𝑤𝑘
𝑡−𝜏=[𝑡−𝜏+1,𝑡−𝜏+𝑘]as a sub-
interval within 𝑤𝑡such that𝑋𝑡(𝑤𝑘
𝑡−𝜏)=𝑧𝑡−𝜏+1𝑧𝑡−𝜏+2···𝑧𝑡−𝜏+𝑘
is the corresponding length- 𝑘time series segment of 𝑋𝑡, where
𝑘≤𝜏≤𝛼+1.
Let𝑓be a black box model that generates a block maxima fore-
cast, ˆ𝑦𝑡, for any given input 𝑋𝑡,i.e.,ˆ𝑦𝑡=𝑓(𝑋𝑡). Our primary goal
is to construct a counterfactual predictor, 𝑋′
𝑡, such that𝑓(𝑋′
𝑡)≈ˆ𝑦′
𝑡,
where ˆ𝑦′
𝑡≠ˆ𝑦𝑡is the desired counterfactual target. For example, if ˆ𝑦𝑡
corresponds to an extreme block maxima generated by the model
𝑓, then the counterfactual target ˆ𝑦′
𝑡would be a non-extreme block
maxima value. In this paper, we employ the Generalized Extreme
Value (GEV) Distribution to define whether a block maxima value
is extreme or non-extreme. If the forecasted block maxima ˆ𝑦𝑡is
extreme, then a counterfactual instance (𝑋′
𝑡,ˆ𝑦′
𝑡)is generated. To
do so, the counterfactual predictor, 𝑋′
𝑡, for𝑋𝑡should satisfy the
following three desirable criteria:
(1)Informativeness: 𝑋′
𝑡is(𝜌,𝑘)-informative if∃𝑤𝑘
𝑡−𝜏⊂𝑤𝑡,𝑘≪
𝛼,𝜌>0 :∥𝑋′
𝑡(𝑤𝑘
𝑡−𝜏)−𝑋𝑡(𝑤𝑘
𝑡−𝜏)∥>𝜌and𝑋𝑡\𝑋𝑡(𝑤𝑘
𝑡−𝜏)≈𝑋′
𝑡\𝑋′
𝑡(𝑤𝑘
𝑡−𝜏), where𝑋𝑡\𝑋𝑡(𝑤𝑘
𝑡−𝜏)is the corresponding time
series in𝑋𝑡after excluding the segment 𝑋𝑡(𝑤𝑘
𝑡−𝜏).
(2)Closeness: 𝑋′
𝑡is𝜖-close to𝑋𝑡if∥𝑋𝑡−𝑋′
𝑡∥<𝜖for𝜖>0.
(3)Realisticness: 𝑋′
𝑡is𝛿-realistic if𝑃(𝑋′
𝑡)≥𝛿, where𝑃(·)is
the probability that 𝑋′
𝑡is drawn from the same distribution
as any randomly chosen segment from the time series Z.
Figure 2 illustrates examples of counterfactual instances eval-
uated using the 3 criteria above when applied to a temperature
block maxima forecasting model. In this hypothetical example, as-
sume the forecast model predicts an extreme temperature value,
say, 105◦F (depicted as a blue dot), for the forecast window between
January and September in 2023. Suppose a counterfactual instance
with the counterfactual target of around 80◦F (shown as a red dot)
is to be constructed. The left panel shows a comparison between
an informative and uninformative counterfactual instance. Even
though both counterfactual instances yield the same counterfactual
target, the informative counterfactual, shown as a red line, is nearly
identical to the original time series, shown as a blue line, except for
the larger deviation in the interval highlighted in green shadow. The
counterfactual is informative as it pinpoints the segment within the
predictor window whose anomalous values lead to the unusually
extreme block maxima forecasted by the model. In contrast, the
uninformative counterfactual, shown by the green line, exhibits
deviations from the original time series throughout the entire time
period, offering little information that could explain the forecasted
extreme block maxima. The middle panel of Figure 2 shows the
distinction between counterfactual instances with high (red line)
and low (green line) closeness in terms of their proximity to the
original time series. Finally, the right panel of Figure 2 illustrates
the difference between realistic (red line) and unrealistic (green
line) counterfactual instances. Specifically, the temperature profile
of the unrealistic counterfactual is counter-intuitive as it exhibits
higher temperatures in winter than in summer.
Unfortunately, balancing the trade-off among the criteria can
be tricky. For example, the following theorem demonstrates the
impossibility of satisfying both informative and closeness criteria
when𝜌>𝜖.
Theorem 1. Let𝑋′
𝑡be a(𝜌,𝑘)-informative counterfactual predic-
tor of𝑋𝑡. If𝜌>𝜖, then𝑋′
𝑡must not be 𝜖-close.
 
564KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Proof. Since𝑋′
𝑡is(𝜌,𝑘)-informative, using the additive prop-
erty of vector norm, we have:
∥𝑋′
𝑡−𝑋𝑡∥=∥𝑋′
𝑡(𝑤𝑘
𝑡−𝜏)−𝑋𝑡(𝑤𝑘
𝑡−𝜏)∥+∥𝑋′
𝑡\𝑋′
𝑡(𝑤𝑘
𝑡−𝜏)−𝑋𝑡\𝑋𝑡(𝑤𝑘
𝑡−𝜏)∥
≥𝜌+∥𝑋′
𝑡\𝑋′
𝑡(𝑤𝑘
𝑡−𝜏)−𝑋𝑡\𝑋𝑡(𝑤𝑘
𝑡−𝜏)∥
Furthermore, given that 𝜌>𝜖, this implies∥𝑋′
𝑡−𝑋𝑡∥>𝜖, which
means𝑋′
𝑡must not be 𝜖-close. □
Similarly, if𝑋′
𝑡is𝜖-close and𝜌>𝜖, then∥𝑋′
𝑡(𝑤𝑘
𝑡−𝜏)−𝑋𝑡(𝑤𝑘
𝑡−𝜏)∥≤
𝜖, which means finding a (𝜌,𝑘)-informative counterfactual predic-
tor would be impossible. An analogous impossibility theorem for
realisticness is more challenging as it depends on the probability
model of the time series. The difficulty of balancing the 3 criteria
can be illustrated with an example. Let 𝑋′
𝑡be the counterfactual
predictor obtained via the nearest neighbor approach [ 8]. Since𝑋′
𝑡
is an existing time series, 𝑃(𝑋′
𝑡)must be large. However, it may
not be close unless 𝑋′
𝑡is in the𝜖-neighborhood of 𝑋𝑡. Even if it is
𝜖-close,𝑋′
𝑡may not be(𝜌,𝑘)-informative if 𝜌>𝜖, as shown above.
3.2 Generalized Extreme Value Distribution
Consider a time series Zof length𝑇that is partitioned into 𝑚
sequences, each of length 𝑛(i.e.,𝑛𝑚=𝑇). For each sequence, let
𝑌𝑛be its block maxim. The generalized extreme value (GEV) dis-
tribution [ 7] is often used to describe the probability distribution
governing the block maxima values. The distribution is character-
ized by its shape ( 𝜉), location (𝜇), and scalar ( 𝜎) parameters, with
the following cumulative distribution function (CDF):
𝑃(𝑌𝑛≤𝑦)=exp
−h
1+𝜉𝑦−𝜇
𝜎i−1
𝜉
, (1)
subject to the constraint:
∀𝑦: 1+𝜉𝑦−𝜇
𝜎>0 (2)
The𝑝thquantile of the distribution, 𝑦𝑝, can be calculated as follows:
𝑦𝑝=𝜇+𝜎
𝜉
(−log𝑝)−𝜉−1
. (3)
Given the shape, location, and scale parameters of the GEV distri-
bution1, the preceding equation is used to determine the threshold
for extreme block maxima and the counterfactual target for our
proposed framework by setting the appropriate quantile values, 𝑝.
3.3 Deep Block Maxima Forecasting Model
Recent years have witnessed a growing number of research focus-
ing on the use of extreme value theory to enhance the forecasting
of extreme events in time series [ 12,24,34]. For example, Wilson
et al. [ 34] introduced the DeepGPD framework with Generalized
Pareto (GP) distribution to forecast excess values over some pre-
specified threshold while Galib et al. [ 12] and Nishino et al. [ 24]
utilized the Generalized Extreme Value (GEV) distribution for block
maxima forecasting problems. In this work, we choose DeepEx-
trema [ 12] as our black box model due to its superior performance
in block maxima forecasting compared to other baselines. Never-
theless, our framework is model-agnostic, and thus, applicable to
other block maxima forecasting models such as [24].
1These parameters will be estimated by the block maxima forecast model.DeepExtrema employs a combination of LSTM with fully con-
nected layers to estimate the GEV parameters {𝜇(𝑋𝑡),𝜎(𝑋𝑡),𝜉(𝑋𝑡)}
of an input time series 𝑋𝑡in a way that preserves the inequality
constraints given in (2). It then utilizes a fully connected network
(FCN) layer to generate the forecast of block maxima, ˆ𝑦𝑡, from the
estimated GEV parameters. The framework is trained end-to-end
to simultaneously learn both the GEV parameters and its block
maxima forecast by minimizing the following loss function [12]:
𝐿=𝜆1ˆ𝐿𝐺𝐸𝑉+(1−𝜆1)𝑁∑︁
𝑖=1(𝑦(𝑖)
𝑡−ˆ𝑦(𝑖)
𝑡)2, (4)
where ˆ𝐿𝐺𝐸𝑉 is a regularized negative log-likelihood of the GEV
distribution while the second term is the least-square loss between
the forecasted and actual block maxima of the training instances.
Note that the aim of our study is not to assess the predictive per-
formance of DeepExtrema or other similar forecast models. Instead,
it focuses on providing counterfactual explanations to elucidate the
forecasts generated by the model, regardless of their accuracy.
3.4 Counterfactual Explanation
Existing studies on counterfactual explanation have mostly cen-
tered around binary classification problems [ 8,30]. Specifically, let
𝑋𝑡denote the predictor time series and 𝑐denote the predicted class
label of the block maxima in the forecast time series 𝑌𝑡, according
to a binary classifier 𝑓,i.e.,𝑓(𝑋𝑡)=𝑐. The primary objective of
counterfactual explanation is to find 𝑋′
𝑡, a modified counterpart of
𝑋𝑡that will lead to the alternative class label 𝑐′by the model 𝑓,i.e.,
𝑓(𝑋′
𝑡)=𝑐′≠𝑐. This objective is typically achieved by minimizing
the following loss function [30]:
𝐿(𝑋𝑡,𝑋′
𝑡,𝑐′,𝜆)=𝜆 𝑓(𝑋′
𝑡)−𝑐′2+𝑑(𝑋𝑡,𝑋′
𝑡), (5)
where𝜆is a tuning parameter that balances the components of the
loss function. The first term quantifies the difference between the
model’s prediction for the modified input 𝑋′
𝑡and the counterfactual
target class𝑐′. The second term measures the dissimilarity between
the input𝑋𝑡and its counterfactual 𝑋′
𝑡.
In this work, we will adapt the approach to generate a coun-
terfactual explanation for extreme block maxima, a continuous
value instead of a class label, forecasted by models such as DeepEx-
trema [12].
4 PROPOSED FRAMEWORK
To generate the counterfactual explanation for block maxima fore-
casting, our DiffusionCF framework performs the following steps.
First, a continuous-valued counterfactual target ˆ𝑦′
𝑡associated with
the forecasted block maxima ˆ𝑦𝑡is constructed, as described in Sec-
tion 4.1. Next, the corresponding counterfactual predictors 𝑋′
𝑡for
the target ˆ𝑦′
𝑡is learned using the approach described in Section 4.2.
Finally, in Section 4.3, we demonstrate how to use 𝑋′
𝑡to explain
the extreme block maxima forecast generated by the model 𝑓.
4.1 Constructing Counterfactual Target ˆ𝑦′
𝑡
Our goal is to generate a counterfactual target that satisfies the
following two conditions. First, given an input 𝑋𝑡, if the forecasted
block maxima ˆ𝑦𝑡is an extreme value, then the counterfactual target
must be non-extreme. This requires setting a threshold ˆ𝑦′
𝑈that
 
565Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 3: A schematic illustration of the proposed DiffusionCF framework.
determines whether the forecasted block maxima is extreme or
non-extreme. Second, as the distribution of block maxima values
is governed by the GEV distribution, the sampled counterfactual
target should be drawn from the same distribution. Adhering to
the GEV distribution enables us to construct the continuous-valued
counterfactual target in a more principled fashion.
To achieve both conditions, we utilize the GEV parameters gen-
erated by the DeepExtrema model2. Specifically, given an input
𝑋𝑡, DeepExtrema will generate both the block maxima forecast ˆ𝑦𝑡
along with parameters of its associated GEV distribution, 𝜇(𝑋𝑡),
𝜉(𝑋𝑡), and𝜎(𝑋𝑡). Let𝑝∈(0,1)be a hyperparameter corresponding
to the desired quantile for defining an extreme block maxima. The
extreme value threshold is computed by setting ˆ𝑦′
𝑈=𝑧𝑝using the
quantile formula for GEV distribution given by (3). If the predicted
block maxima ˆ𝑦𝑡≥ˆ𝑦′
𝑈, then ˆ𝑦𝑡is considered an extreme value.
Ifˆ𝑦𝑡is an extreme block maxima, then a counterfactual target
will be constructed using the forecasted GEV distribution by setting
a quantile𝑝′<𝑝to ensure the counterfactual target is below the
extreme threshold. Analogous to (3), the counterfactual target is
computed as follows:
ˆ𝑦′
𝑡=𝜇(𝑋𝑡)+𝜎(𝑋𝑡)
𝜉(𝑋𝑡)h
(−log𝑝′)−𝜉(𝑋𝑡)−1i
, (6)
where𝜇(𝑋𝑡),𝜎(𝑋𝑡), and𝜉(𝑋𝑡)are the learned parameters produced
by the DeepExtrema model.
2For other black box models, we can fit a GEV distribution to all the block maxima
values first to learn their GEV parameters and use them to generate the threshold ˆ𝑦′
𝑈.While our framework can be adapted to explain non-extreme
block maxima forecasts, our current approach is specifically de-
signed to generate counterfactual instances for extreme block max-
ima in this work. This is due to their significant practical impli-
cations and the valuable insights they offer for prevention and
mitigation efforts.
4.2 Constructing Counterfactual Predictor 𝑋′
𝑡
After identifying the counterfactual target ˆ𝑦′
𝑡, the next step is to
construct its corresponding counterfactual predictor, 𝑋′
𝑡, such that
𝑓(𝑋′
𝑡)≈ ˆ𝑦′
𝑡while ensuring that 𝑋′
𝑡is informative, realistic, and
close to the original time series 𝑋𝑡. Unlike previous approaches (as
examples in Section 5.2) that often resorted to searching or random
(gradient-based) perturbations of the input data to produce the
counterfactual predictors, our proposed DiffusionCF framework is
designed to balance the tradeoff between informativeness, closeness,
and realisticness of the counterfactual explanation. There are 3 main
components in DiffusionCF, as shown in Figure 3.
4.2.1 Constructor for the Counterfactual Predictor. The key assump-
tion guiding our approach is that when the black box model predicts
an exceptionally extreme block maxima value, the time series seg-
ment(s) within 𝑋𝑡most likely contributing to the extreme forecast
are those exhibiting a high level of anomaly. Our confidence in this
assumption stems from the fact that the black box model derives
its prediction solely from 𝑋𝑡. If the segments in 𝑋𝑡align with typi-
cal observations in the time series, it would be improbable for the
forecast model to produce an extreme value output. While there
may exist other scenarios that could lead to the forecasted extreme
 
566KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
block maxima, our assumption provides a highly plausible and com-
putationally feasible way to pinpoint the explanatory factor behind
the model’s forecast.
Based on this assumption, our constructor for the counterfactual
predictor needs to identify the most anomaly segment(s) in the time
series and replace them with more typical patterns, while leaving
the rest of the time series intact. This strategy enables our approach
to create more informative yet realistic counterfactual instances.
The counterfactual predictors are constructed as follows:
(1)Extraction of Time Series Segments: Given a time series
𝑋𝑡of length𝛼+1for the predictor window [𝑡−𝛼,𝑡], our algo-
rithm first extracts all the time series segmentsn
𝑋𝑖
𝑡o𝛼−𝑑+2
𝑖=1
within𝑋𝑡by using a sliding window of fixed length, 𝑑.
(2)Detection of Anomalous Segments: For each extracted
segment,𝑋𝑖
𝑡, the algorithm computes the probability 𝑝𝑋𝑖
𝑡that conforms to the underlying distribution of the time
series data. The probability is estimated using a detector
function𝑔(to be described in Section 4.2.2), where 𝑝𝑋𝑖
𝑡=
𝑔(𝑋𝑖
𝑡).
(3)Counterfactual Generation: Starting from the most anoma-
lous segment, 𝑋𝑚𝑎𝑠𝑘
𝑡 =arg min𝑠∈{𝑋𝑖
𝑡}𝑝𝑠, the algorithm
would remove this segment from 𝑋𝑡and replaces it with
a more “typical" segment. This is achieved by using a sam-
plerℎ(see Section 4.2.3) to generate a set of 𝑚candidate
replacements, denoted as 𝐶𝑚
𝑡=ℎ(𝑋𝑡,𝑋𝑚𝑎𝑠𝑘
𝑡), where each
candidate𝑋𝑠
𝑡∈𝐶𝑚
𝑡has a higher probability to be drawn
from the time series than 𝑋𝑚𝑎𝑠𝑘
𝑡 ,i.e.,𝑝𝑋𝑠
𝑡>𝑝𝑋𝑚𝑎𝑠𝑘
𝑡. The
best candidate 𝑋min
𝑡is then selected based on its distance to
the counterfactual target ˆ𝑦′
𝑡:
𝑋min
𝑡=arg min
𝑋𝑠
𝑡∈𝐶𝑚
𝑡𝑓
(𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋𝑠
𝑡
−ˆ𝑦′
𝑡1(7)
Here(𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋𝑠
𝑡denotes the resulting times series
after replacing the anomalous segment 𝑋𝑚𝑎𝑠𝑘
𝑡 with the can-
didate𝑋𝑠
𝑡. The counterfactual predictor 𝑋′
𝑡is obtained by
replacing the anomalous segment with the best candidate,
i.e.,𝑋′
𝑡=(𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋min
𝑡. As the forecast for the counter-
factual predictor 𝑋′
𝑡generated from the anomalous segment,
𝑓(𝑋′
𝑡)may not always be close to the counterfactual target,
ˆ𝑦′
𝑡, this step is repeated using the next 𝐾most anomalous
segment(s) until the following conditions are met:
Counterfactual Target Condition:
𝑓(𝑋′
𝑡)<ˆ𝑦′
𝑈and𝑓(𝑋′
𝑡)∈[ ˆ𝑦′
𝑡−𝜖,ˆ𝑦′
𝑡+𝜖] (8)
For efficiency reasons, this process of removal and replace-
ment of the anomalous segment is repeated for at most 𝐾
times. If no viable counterfactual predictor 𝑋′
𝑡is found, the
algorithm will return the best 𝑋′
𝑡it has discovered.
4.2.2 Detector 𝑔.Our DiffusionCF framework uses a variational
auto-encoder (VAE) for anomaly detection [ 2] as its detector 𝑔. TheVAE for anomaly detection is trained to learn the underlying distri-
bution of all time series segments {𝑋𝑖
𝑡}𝛼−𝑑+2
𝑖=1of length𝑑extracted
from the predictor time series 𝑋𝑡of the training data. It comprises
two main components—an encoder 𝑉encoder and a decoder 𝑉decoder .
The encoder 𝑉encoder takes each𝑋𝑖
𝑡as input and maps it to a latent
Gaussian distribution with mean vector 𝜇and isotropic covariance
𝜎2I. The Gaussian distribution is used to draw 𝐿samples from the
ℎ-dimensional latent space, denoted as {𝐼𝑖∈𝑅ℎ|𝑖=1,2,...,𝐿}. The
decoder𝑉decoder would attempt to reconstruct the original time
series segment, 𝑋𝑖
𝑡, from each sampled latent instance. The VAE for
anomaly detection is trained to minimize the average reconstruc-
tion error of the time series segments.
During the detection step, for each input 𝑋𝑖
𝑡, the VAE would
compute the parameters of its latent distribution, which are used to
calculate the probability 𝑝𝑋𝑖
𝑡. The probability determines whether
𝑋𝑖
𝑡is anomalous. The higher the probability, the more likely 𝑋𝑖
𝑡
belongs to the same distribution as the majority of the time series
segments, so the less anomalous it is. Details of the Detector module
are shown in Figure 3.
4.2.3 Sampler ℎ.Our framework uses a conditional diffusion model
as its sampler ℎto construct a candidate replacement, 𝑋𝑠
𝑡, for
an anomalous segment, 𝑋𝑚𝑎𝑠𝑘
𝑡 . Specifically, it will first mask the
anomalous segment from the input time series 𝑋𝑡and provide the
masked input to CSDI [ 29], a conditional diffusion model that is
adept at imputing missing segments of a time series (see Appen-
dixA.1 for details). We chose CSDI because it can leverage the
unmasked portion of the time series (i.e., 𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡 ) to create a
new candidate 𝑋𝑠
𝑡for imputing the masked segment. This allows
DiffusionCF to produce imputed segments that are consistent with
the rest of the time series, leading to more realistic counterfac-
tual predictors. Nevertheless, our framework is flexible and can
incorporate other samplers such as DDPM [14] and VAE [17].
4.3 Using Counterfactual Predictor to Explain
Block Maxima Forecast
The counterfactual predictor, 𝑋′
𝑡, generated by DiffusionCF can be
used to elucidate the specific segment within the input time series
𝑋𝑡that largely contributes to the forecasted extreme block maxima.
LetΔ𝑋=𝑋𝑡−𝑋′
𝑡=(Δ𝑧𝑡−𝛼,Δ𝑧𝑡−𝛼+1,...,Δ𝑧𝑡)be a vector of abso-
lute difference between the counterfactual and original predictor,
i.e.,Δ𝑧𝑖=|𝑋𝑡,𝑖−𝑋′
𝑡,𝑖|. The time steps within the predictor window,
[𝑡−𝛼,𝑡]can be sorted in decreasing magnitude of their Δ𝑧𝑖. If
|Δ𝑧𝑖|exceeds some threshold, then the segment can be considered
a notable contributor to the extreme block maxima forecast by the
black box model 𝑓.
5 PERFORMANCE EVALUATION
5.1 Datasets
We use the following datasets for our experiments: (1) Global
Surface Summary of the Day (GSOD), a dataset that contains
daily observations of precipitation and temperature from 79 weather
stations in the Mobile/Pensacola area in the southwestern United
States. The dataset spans a time period from August 1, 1929, to
 
567Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Summary of datasets, where |𝑋𝑡|and|𝑌𝑡|denote the
length of the predictor and forecast windows, respectively.
Dataset#Training
samples#Validation
samples# Testing
samples|𝑋𝑡| |𝑌𝑡|
GSOD 14798 548 511 24 6
S&P 500 7022 251 251 25 5
Dodgers sensor 846 248 241 40 8
November 22, 2023. (2) S&P 500, a dataset comprises of daily closing
prices of the S&P-500 index from January 13, 1994 to January 12,
2024. (3) Dodgers loop sensor, a dataset containing traffic volume
data from April 10, 2005, to October 2, 2005, at the Glendale ramp
of the 101 North freeway in Los Angeles, near the Dodgers stadium.
Summary statistics of the datasets are given in Table 1 while their
pre-processing steps are described in Appendix A.3.
5.2 Baseline Methods
We compare the performance of DiffusionCF against the following
baseline methods:
•BaseNN, a baseline used in [ 32] to identify the nearest-
unlike neighbor 𝑋′
𝑡from the training set, whose true block
maxima value aligns with the counterfactual target.
•𝜔-CF[30] learns𝑋′
𝑡by minimizing the loss between 𝑓(𝑋′
𝑡)
and ˆ𝑦′
𝑡, as well as the distance between 𝑋𝑡and𝑋′
𝑡.
•Native guide (NG-CF) [8] constructs 𝑋′
𝑡by identifying a
nearest-unlike neighbor to 𝑋𝑡and modifying it to produce a
model forecast close to the counterfactual target.
•ForecastCF [32] employs a mask objective function to learn
𝑋′
𝑡, aiming at minimizing the loss between 𝑓(𝑋′
𝑡)and ˆ𝑦′
𝑡.
•SPARCE [18] generates a counterfactual explanation for
time series by using a generative adversarial network (GAN).
As some baseline methods were developed for classification tasks,
they were adapted to generate counterfactual instances for block
maxima forecasts. Details are given in Appendix A.4.
5.3 Evaluation Metrics
As noted in Section 3.1, a good counterfactual predictor should be
realistic, informative, and close to the original time series. Let 𝑋𝑡=
𝑧𝑡−𝛼𝑧𝑡−𝛼+1...𝑧𝑡be the original predictors and 𝑋′
𝑡=𝑧′
𝑡−𝛼𝑧′
𝑡−𝛼+1...𝑧′
𝑡
be the counterfactual predictors. We employ the following metrics
to assess the performance of the various methods:
•Informativeness. As noted in Section 3.1, an informative
counterfactual should modify only a small segment of 𝑋𝑡,
keeping the rest of the predictor time series intact. We use a
combination of sparsity andconsecutiveness metrics to deter-
mine whether a counterfactual predictor 𝑋′
𝑡is informative.
Sparsity(𝑋′
𝑡)=1
𝛼+1count{𝑖:𝑧𝑡−𝑖≠𝑧′
𝑡−𝑖,𝑖=0,···,𝛼}(9)
A lower value of sparsity means fewer modifications to 𝑋𝑡.
Next, we construct the following sequence of binary values:
Δ(𝑧𝑡−𝑖,𝑧′
𝑡−𝑖)=(
1,if|𝑧𝑡−𝑖−𝑧′
𝑡−𝑖|>𝜌,
0,otherwise,(10)where𝜌>0is a threshold. Let 𝐿maxrepresent the maxi-
mum length of consecutive 1’s in the binary sequence. The
consecutiveness metric is defined as
Consecutiveness(𝑋′
𝑡)=𝐿maxÍ𝛼
𝑖=1Δ(𝑧𝑡−𝑖,𝑧′
𝑡−𝑖). (11)
A higher consecutiveness implies the notable difference be-
tween𝑋𝑡and𝑋′
𝑡is mostly concentrated in a local segment of
the time series. Thus, an informative counterfactual should
have low sparsity but high consecutiveness values.
•Closeness. The proximity metric below is used to determine
the extent to which 𝑋𝑡is close to𝑋′
𝑡:
Proximity(𝑋𝑡,𝑋′
𝑡)=1
𝛼+1𝛼∑︁
𝑖=0|𝑧𝑡−𝑖−𝑧′
𝑡−𝑖|. (12)
The lower the proximity, the closer the counterfactual pre-
dictor is to the original predictor.
•Realisticness. The negative likelihood function of𝑋′
𝑡is used
to determine whether a counterfactual instance is realistic,
NLL(𝑋′
𝑡)=−log𝑝𝑋′
𝑡. Here,𝑝𝑋′
𝑡represents the probability
assigned to 𝑋′
𝑡by detector 𝑔. The lower the NLL, the more
realistic the counterfactual predictor.
Finally, we use the precision metric to ascertain how well 𝑋′
𝑡will
ensure that 𝑓(𝑋′
𝑡)is classified as a non-extreme block maxima.
5.4 Experimental Results
5.4.1 Performance Comparison. Experiments results are summa-
rized in Table 2 based on the setup discussed in Appendix A.5.
Figure 4 provides an illustrated example of the counterfactual in-
stances found by the different methods. In general, DiffusionCF
generates the most informative counterfactual instances in all 4
datasets, achieving the best sparsity and consecutiveness scores. It
also appears among the top 2 approaches with the best proximity
and precision in at least 3 of the 4 datasets. These results suggest
that DiffusionCF generally demonstrate superior performance in
explaining the extreme block maxima forecast generated by the
black box model compared to other baselines. Though its NLL score
is slightly worse than BaseNN and NG-CF, this is not surprising as
the latter two approaches create their counterfactuals by sampling
from the training instances.
Specifically, (1) For BaseNN, its superior performance in terms of
NLL is attributable to its strategy of using training instances as 𝑋′
𝑡.
However, since BaseNN does not optimize for closeness between
𝑋𝑡and𝑋′
𝑡, it performs poorly in terms of proximity andsparsity
metrics, as illustrated in Figure 4(left). BaseNN also has lower pre-
cision because the forecasted counterfactual block maxima 𝑓(𝑋′
𝑡)
may not be consistent with the desired counterfactual target since
the nearest-unlike neighbor is chosen based on the true block max-
ima value instead of its forecasted block maxima. (2) For 𝜔-CF and
ForecastCF, their precision is perfect in all 4 datasets, which is not
surprising as they both employ optimization-based approaches to
ensure𝑓(𝑋′
𝑡)is close to the counterfactual target, ˆ𝑦′
𝑡. Nevertheless,
𝜔-CF has a better proximity score compared to ForecastCF. This
is because the loss function used by 𝜔-CF includes the distance
between𝑋𝑡and𝑋′
𝑡to encourage smaller modifications to the input.
In contrast, ForecastCF does not consider such a factor, allowing it
to make larger modifications. Hence, its proximity score is higher.
 
568KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
Table 2: Evaluation of performance on 4 real-world datasets conducted for scenarios where the forecast ˆ𝑦𝑡is extreme, and the
counterfactual target ˆ𝑦′
𝑡is non-extreme. Within the comparative results, redentries indicate the top-performing result, while
blue entries signify the second-best performance for each metric.
GSOD-precipitation GSOD-temperature
Method Spars.↓Consecu.↑Proxi.↓ NLL↓ Prec.↑Spars.↓Consecu.↑Proxi.↓ NLL↓ Prec.↑
BaseNN1.00
(±0.00)0.86
(±0.18)1.12
(±0.19)1.18
(±0.15)0.271.00
(±0.00)0.86
(±0.17)1.13
(±0.21)1.08
(±0.36)0.48
𝜔-CF1.00
(±0.00)0.91
(±0.13)0.17
(±0.07)1.07
(±0.18)1.001.00
(±0.00)0.65
(±0.18)0.18
(±0.08)1.12
(±0.07)1.00
NG-CF0.83
(±0.26)0.67
(±0.26)0.57
(±0.24)0.99
(±0.17)0.380.94
(±0.17)0.65
(±0.25)0.56
(±0.22)1.09
(±0.08)0.56
ForecastCF1.00
(±0.00)0.92
(±0.18)0.45
(±0.17)1.13
(±0.37)1.001.00
(±0.00)0.90
(±0.15)0.35
(±0.13)1.21
(±0.09)1.00
SPARCE1.00
(±0.00)0.72
(±0.22)1.10
(±0.21)1.13
(±0.17)0.681.00
(±0.00)0.74
(±0.21)1.10
(±0.22)1.18
(±0.07)0.87
DiffusionCF0.25
(±0.00)0.93
(±0.13)0.16
(±0.07)1.06
(±0.22)0.980.12
(±0.00)0.96
(±0.13)0.08
(±0.04)1.20
(±0.08)1.00
S&P 500 Dodgers loop sensor
Method Spars.↓Consecu.↑Proxi.↓ NLL↓ Prec.↑Spars.↓Consecu.↑Proxi.↓ NLL↓ Prec.↑
BaseNN1.00
(±0.00)0.88
(±0.17)1.43
(±0.40)0.84
(±0.49)0.981.00
(±0.01)0.75
(±0.22)1.12
(±0.39)0.36
(±0.17)0.66
𝜔-CF1.00
(±0.00)0.46
(±0.17)0.59
(±0.11)1.13
(±0.26)1.001.00
(±0.00)0.76
(±0.25)0.02
(±0.01)0.45
(±0.11)1.00
NG-CF1.00
(±0.00)0.82
(±0.21)0.92
(±0.23)0.96
(±0.29)0.990.99
(±0.03)0.50
(±0.17)0.52
(±0.18)0.56
(±0.24)0.61
ForecastCF1.00
(±0.00)0.85
(±0.19)0.81
(±0.16)1.10
(±0.23)1.001.00
(±0.00)0.56
(±0.24)0.09
(±0.04)0.44
(±0.12)1.00
SPARCE0.98
(±0.00)0.75
(±0.22)1.43
(±0.38)1.71
(±0.20)1.000.97
(±0.00)0.66
(±0.21)1.14
(±0.35)0.46
(±0.12)1.00
DiffusionCF0.40
(±0.00)0.96
(±0.10)0.51
(±0.16)1.68
(±0.19)0.970.12
(±0.00)0.81
(±0.18)0.05
(±0.03)0.43
(±0.13)1.00
Figure 4: A comparative study of 𝑋′
𝑡, generated by BaseNN, ForecastCF, 𝜔-CF, NGCF, SPARCE, and DiffusionCF, when applied to
precipitation forecasting between 2021-06 and 2023-09 for a weather station in Pensacola, Florida. The blue dot represents the
forecasted block maxima, ˆ𝑦𝑡, while the red dot represents the counterfactual target, ˆ𝑦′
𝑡.
This difference in proximity between𝜔-CF and ForecastCF can
be clearly seen in the middle two plots of Figure 4. Furthermore,
in terms of informativeness, both approaches perform poorly in
terms of their sparsity metric, though ForecastCF demonstrates a
relatively higher consecutiveness score compared to 𝜔-CF. (3) For
SPARCE, which utilizes GAN for optimization, it outperforms 𝜔-CF
and ForecastCF in terms of sparsity on the S&P 500 and Dodgers
loop sensor datasets. However, its sparsity is similar to other base-
lines and worse than NG-CF on the GSOD datasets. SPARCE also
struggles in terms of informativeness, closeness, and realisticness
metrics. This underscores the difficulty of using GAN to optimizemultiple objectives simultaneously. ForecastCF, 𝜔-CF, and SPARCE
all fall short compared to BaseNN in terms of NLL, suggesting the
counterfactual predictors produced by these optimization-based
methods are less realistic. (4) For NG-CF, which is a combination of
neighbor searching and perturbation-based methods, can balance
the trade-off in terms of proximity, precision, and NLL but falls short
in terms of producing informative explanation due to the generally
lowconsecutiveness scores. Finally, (5) for DiffusionCF, it excels in
terms of sparsity, consecutiveness, proximity, and precision. This
advantage is crucial as it renders the counterfactual explanation
more informative, as shown in Figure 4(right).
 
569Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Ablation study results on GSOD-precipitation data.
Spars.↓Consecu.↑Proxi.↓ NLL↓ Prec.↑
CSDI+30.12
(±0.00)0.97
(±0.13)0.06
(±0.06)1.14
(±0.21)0.87
CSDI+60.25
(±0.00)0.93
(±0.13)0.16
(±0.07)1.06
(±0.22)0.98
CSDI+90.38
(±0.00)0.90
(±0.17)0.29
(±0.13)1.11
(±0.28)1.00
CSDI+120.50
(±0.00)0.91
(±0.16)0.36
(±0.10)1.08
(±0.24)1.00
VAE+30.12
(±0.00)0.98
(±0.15)0.02
(±0.01)1.13
(±0.19)0.36
VAE+60.25
(±0.00)0.90
(±0.17)0.07
(±0.03)1.08
(±0.21)0.42
VAE+90.38
(±0.00)0.84
(±0.17)0.13
(±0.04)1.04
(±0.21)0.56
VAE+120.50
(±0.00)0.87
(±0.17)0.25
(±0.05)1.02
(±0.16)0.49
5.4.2 Ablation study of DiffusionCF. We conduct an ablation study
forDiffusionCF by varying the window size 𝑑for segment extrac-
tion, as described in Section 4.2.2 and employing VAE as our alter-
native sampling technique. A detailed breakdown of the results is
given in Table 3. Our key conclusions are as follows:
•Increasing the window size 𝑑from 3 to 12 generally degrades
the performance of DiffusionCF in terms of sparsity, prox-
imity, and consecutiveness, while enhancing its realisticness
andprecision. This is because a larger 𝑑increases the num-
ber of time steps available for adding perturbation, allowing
greater deviation from the original time series, thus reducing
sparsity and increasing their dissimilarity. Larger window
size also reduces the percentage of consecutive time steps
that were perturbed, as shown in Table 3, with the excep-
tion of𝑑=12. Nevertheless, it also gives more flexibility
forDiffusionCF to construct counterfactual instances that
are close to the desired target, thus enhancing its precision.
Interestingly, the NLL values for VAE exhibit a decreasing
trend with larger 𝑑, warranting future investigation.
•In terms of informativeness andprecision, DiffusionCF based
on CSDI outperforms the one based on VAE. However, the
latter is superior in proximity andrealisticness. As shown in
Table 3, the CSDI-based method slightly surpasses the VAE-
based method in consecutiveness. The CSDI-based method’s
advantage in precision is evident as the diffusion model used
for imputation generates samples that, while deviating more
from the original time series compared to the VAE-based
method, still remain realistic, thereby contributing to more
accurate counterfactual targets. However, this increased de-
viation results in worse performance in proximity andNLL
compared to the VAE-based method.
5.4.3 Case study of DiffusionCF. Our case study on the Dodgers
loop sensor dataset focuses on the specific example when there are
two consecutive game days at the Dodgers stadium, on May 31 and
June 1, 2005. Figure 5 depicts the average 3-hourly traffic flow at a
ramp near the stadium from Friday, May 27 to Wednesday, June 1,
Figure 5: Observed average traffic flow (May 27-Jun 01) vs.
counterfactual instance found by CSDI-based DiffusionCF.
2005. In this example, the model utilizes traffic flow data from the
preceding five days to forecast the maximum traffic flow for June 1st.
The resulting block maxima value, approximately 1300, is deemed
extreme, surpassing 80% of the average 3-hourly traffic flows within
the dataset. This extreme value is attributed to a baseball game held
at the stadium on that day. The figure also depicts a counterfactual
target, with the block maxima value set around 1150, representing
the typical traffic flow if the baseball match had not occurred. Based
on the counterfactual target, a counterfactual instance is generated
by the CSDI-based DiffusionCF model.
The counterfactual instance generated by DiffusionCF adjusts the
traffic pattern for Tuesday, May 31st, depicting a decrease in traffic
flow for that day, indicative of the absence of a baseball game at the
stadium. The absence of a game on Tuesday suggests the likelihood
of no game the following day. In essence, the increase in traffic
flow on Tuesday attributed to the baseball game could elucidate the
extreme block maxima forecasted for Wednesday. The result shown
in Figure 5 thus underscores DiffusionCF ’s capability in identifying
anomalies (i.e., pattern of elevated traffic flow on game days) and
modify them towards a normative state (i.e., pattern of reduced
traffic flow) as its counterfactual instance.
6 CONCLUSIONS AND FUTURE WORK
This paper introduces the novel problem of counterfactual explana-
tion for block maxima forecasting models in time series. We propose
a methodology for creating counterfactual block maxima and intro-
duce the DiffusionCF framework to balance the trade-off between
generating counterfactual explanations that are informative, close
to the original time series, and realistic. Experimental results show
that DiffusionCF generates better counterfactual instances com-
pared to other baselines. Nevertheless, the current framework has
two potential limitations. First, it considers only univariate time
series. For future endeavors, we plan to extend our methodology to
the multivariate case. Second, DiffusionCF is biased towards con-
structing its counterfactual predictor by modifying only one of
the anomalous segments in the time series. We plan to investigate
approaches that could modify multiple segments instead.
7 ACKNOWLEDGMENT
This research is supported by the U.S. National Science Foundation
under grant IIS-2006633. Any use of trade, firm, or product names
is for descriptive purposes only and does not imply endorsement
by the U.S. Government.
 
570KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
REFERENCES
[1]Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas
Wiegand, Wojciech Samek, and Sebastian Lapuschkin. 2023. From attribution
maps to human-understandable explanations through concept relevance propa-
gation. Nature Machine Intelligence 5, 9 (2023), 1006–1019.
[2]Jinwon An and Sungzoon Cho. 2015. Variational autoencoder based anomaly
detection using reconstruction probability. Special Lecture on IE 2, 1 (2015), 1–18.
[3]Emre Ates, Burak Aksar, Vitus J Leung, and Ayse K Coskun. 2021. Counterfactual
explanations for multivariate time series. In 2021 International Conference on
Applied Artificial Intelligence (ICAPAI). IEEE, 1–8.
[4]Lorenzo Betti, Carlo Abrate, Francesco Bonchi, and Andreas Kaltenbrunner. 2023.
Relevance-based infilling for natural language counterfactuals. In Proceedings of
the 32nd ACM International Conference on Information and Knowledge Manage-
ment. 88–98.
[5]Dieter Brughmans, Pieter Leyman, and David Martens. 2023. Nice: an algorithm
for nearest instance counterfactual explanations. Data Mining and Knowledge
Discovery (2023), 1–39.
[6]Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang, and Gabriele Tolomei.
2023. The dark side of explanations: poisoning recommender systems with
counterfactual examples. In Proceedings of the 46th International ACM SIGIR
conference on Research and Development in Information Retrieval. 2426–2430.
[7]Stuart Coles, Joanna Bawa, Lesley Trenner, and Pat Dorazio. 2001. An introduction
to statistical modeling of extreme values. Vol. 208. Springer.
[8]Eoin Delaney, Derek Greene, and Mark T Keane. 2021. Instance-based counter-
factual explanations for time series classification. In International Conference on
Case-Based Reasoning. Springer, 32–47.
[9]Bhat Dittakavi, Bharathi Callepalli, Aleti Vardhan, Sai Vikas Desai, and Vineeth N
Balasubramanian. 2024. CARE: counterfactual-based algorithmic recourse for
explainable pose correction. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. 4902–4911.
[10] Gil Fidel, Ron Bitton, and Asaf Shabtai. 2020. When explainability meets adver-
sarial learning: detecting adversarial examples using shap signatures. In 2020
International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.
[11] Asadullah Hill Galib, Andrew McDonald, Pang-Ning Tan, and Lifeng Luo. 2023.
Self-recover: forecasting block maxima in time series from predictors with dis-
parate temporal coverage using self-supervised learning. In Proceedings of the
Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023).
[12] Asadullah Hill Galib, Andrew McDonald, Tyler Wilson, Lifeng Luo, and Pang-
Ning Tan. 2022. DeepExtrema: a deep learning approach for forecasting block
maxima in time series data. In Proceedings of the Thirty-First International Joint
Conference on Artificial Intelligence, IJCAI (2022).
[13] Riccardo Guidotti. 2022. Counterfactual explanations and how to find them:
literature review and benchmarking. Data Mining and Knowledge Discovery
(2022), 1–55.
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in Neural Information Processing Systems 33 (2020), 6840–6851.
[15] Guillaume Jeanneret, Loïc Simon, and Frédéric Jurie. 2022. Diffusion models for
counterfactual explanations. In Proceedings of the Asian Conference on Computer
Vision. 858–876.
[16] Guillaume Jeanneret, Loïc Simon, and Frédéric Jurie. 2023. Adversarial counterfac-
tual visual explanations. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 16425–16435.
[17] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
ArXiv Preprint ArXiv:1312.6114 (2013).
[18] Jana Lang, Martin A Giese, Winfried Ilg, and Sebastian Otte. 2023. Generating
sparse counterfactual explanations for multivariate time series. In International
Conference on Artificial Neural Networks. Springer, 180–193.
[19] Nicholas J Leach, Antje Weisheimer, Myles R Allen, and Tim Palmer. 2021.
Forecast-based attribution of a winter heatwave within the limit of predictability.
Proceedings of the National Academy of Sciences 118, 49 (2021), e2112087118.
[20] Peiyu Li, Soukaïna Filali Boubrahimi, and Shah Muhammad Hamdi. 2022. Motif-
guided time series counterfactual explanations. In International Conference on
Pattern Recognition. Springer, 203–215.
[21] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in Neural Information Processing Systems 30 (2017).
[22] Han Meng, Christian Wagner, and Isaac Triguero. 2023. Explaining time se-
ries classifiers through meaningful perturbation and optimisation. Information
Sciences (2023), 119334.
[23] Christoph Molnar. 2020. Interpretable machine learning. Lulu.com.
[24] Kaneharu Nishino, Ken Ueno, and Ryusei Shingaki. 2022. Deep learning-based
block maxima distribution predictor for extreme value prediction. In 8th SIGKDD
International Workshop on Mining and Learning from Time Series – Deep Forecast-
ing: Models, Interpretability, and Applications.
[25] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I
trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
1135–1144.[26] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision. 618–626.
[27] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic
differential equations. ArXiv Preprint ArXiv:2011.13456 (2020).
[28] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One pixel
attack for fooling deep neural networks. IEEE Transactions on Evolutionary
Computation 23, 5 (2019), 828–841.
[29] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI:
conditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems 34 (2021), 24804–24816.
[30] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual
explanations without opening the black box: automated decisions and the GDPR.
Harv. JL & Tech. 31 (2017), 841.
[31] Xiangmeng Wang, Qian Li, Dianer Yu, Qing Li, and Guandong Xu. 2024. Coun-
terfactual explanation for fairness in recommendation. ACM Transactions on
Information Systems 42, 4 (2024), 1–30.
[32] Zhendong Wang, Ioanna Miliou, Isak Samsten, and Panagiotis Papapetrou.
2023. Counterfactual explanations for time series forecasting. ArXiv Preprint
ArXiv:2310.08137 (2023).
[33] James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda
Viégas, and Jimbo Wilson. 2019. The what-if tool: interactive probing of machine
learning models. IEEE Transactions on Visualization and Computer Graphics 26, 1
(2019), 56–65.
[34] Tyler Wilson, Pang-Ning Tan, and Lifeng Luo. 2022. DeepGPD: a deep learning
approach for modeling geospatio-temporal extreme events. Proceedings of the
AAAI Conference on Artificial Intelligence 36, 4 (Jun. 2022), 4245–4253.
A APPENDICES
A.1 Details of CSDI model
CSDI [ 29] is a conditional score-based diffusion model for imput-
ing missing values in time series by leveraging the available ob-
served values. Specifically, CSDI is designed to estimate the true
conditional data distribution 𝑞(𝑥𝑡𝑎|𝑥𝑐𝑜)via the model distribution
𝑝𝜃(𝑥𝑡𝑎|𝑥𝑐𝑜), where𝑥𝑡𝑎denotes the missing values to be imputed
and𝑥𝑐𝑜denotes the observed values.
Diffusion models such as DDPM [ 14] typically follow a two-step
training process. First, during the forward process, the model starts
from an initial input 𝑥𝑡𝑎
0and iteratively perturbs the time series
values to𝑥𝑡𝑎
1,𝑥𝑡𝑎
2,···by adding random noise until it converges to
𝑥𝑡𝑎
𝑇, where𝑥𝑡𝑎
𝑇is of known, simple distribution. Next, during the
reverse process, a neural network is trained to convert 𝑥𝑡𝑎
𝑇back to
the original values 𝑥𝑡𝑎
0. CSDI modifies the reverse process of DDPM
with a conditional model defined as follows:
𝑝𝜃(𝑥𝑡𝑎
0:𝑇|𝑥𝑐𝑜
0)=𝑝(𝑥𝑡𝑎
𝑇)𝑇Ö
𝑡=1𝑝𝜃(𝑥𝑡𝑎
𝑡−1|𝑥𝑡𝑎
𝑡,𝑥𝑐𝑜
0), 𝑥𝑡𝑎
𝑇∼N( 0,I),
where𝑝𝜃(𝑥𝑡𝑎
𝑡−1|𝑥𝑡𝑎
𝑡,𝑥𝑐𝑜
0)=N 𝑥𝑡𝑎
𝑡−1;𝜇𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0),𝜎𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0)I.
Here,𝜇𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0)=1
𝛼𝑡 𝑥𝑡−𝛽𝑡√1−𝛼𝑡𝜖𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0), where𝛽𝑡is a
small positive constant representing the noise level, 𝛼𝑡=Î𝑡
𝑖=1ˆ𝛼𝑖,
ˆ𝛼𝑡=1−𝛽𝑡, and𝜖𝜃:(X𝑡𝑎×R|X𝑐𝑜) → X𝑡𝑎is a conditional
denoising function. Furthermore, 𝜎𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0)=˜𝛽1/2
𝑡, where
˜𝛽𝑡=(1−𝛼𝑡−1
1−𝛼𝑡𝛽𝑡if𝑡>1,
𝛽1 if𝑡=1.(13)
Given𝑥𝑐𝑜
0and𝑥𝑡𝑎
0, noisy samples for diffusion step 𝑡is given by:
𝑥𝑡𝑎
𝑡=√𝛼𝑡𝑥𝑡𝑎
0+(1−𝛼𝑡)𝜖, where𝜖is the added noise. During the
reverse process, 𝜖𝜃is estimated by minimizing the loss function
min
𝜃L(𝜃):=min
𝜃𝐸𝑥0∼𝑞(𝑥0),𝜖∼N( 0,I),𝑡∥(𝜖−𝜖𝜃(𝑥𝑡𝑎
𝑡,𝑡|𝑥𝑐𝑜
0))∥2
2,(14)
 
571Unraveling Block Maxima Forecasting Models with Counterfactual Explanation KDD ’24, August 25–29, 2024, Barcelona, Spain
where𝑞(𝑥0)is the data distribution of 𝑥0. During training, the
choice of imputation target is important, either by random strategy,
historical strategy, mix strategy, or test pattern strategy. See [ 29] for
details. Once trained, 𝑥𝑡𝑎
0can be sampled from 𝑝𝜃(𝑥𝑡𝑎
𝑡−1|𝑥𝑡𝑎
𝑡,𝑥𝑐𝑜
0).
A.2 The DiffusionCF algorithm
The pseudocode of the DiffusionCF algorithm is summarized in
Algorithm 1. In this context, 𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡 denotes the remaining part
of𝑋𝑡after excluding 𝑋𝑚𝑎𝑠𝑘
𝑡 . The expression(𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋𝑠
𝑡
represents the time series obtained after replacing 𝑋𝑚𝑎𝑠𝑘
𝑡 in𝑋𝑡
with𝑋𝑠
𝑡.
Algorithm 1 DiffusionCF
Input: Time series 𝑍=𝑋𝑡∪𝑌𝑡, quantiles𝑝and𝑝′, sliding window length
𝑑, sampling epochs 𝐿, searching epochs 𝐾
Output: Counterfactual instances (𝑋′
𝑡,ˆ𝑦′
𝑡)
1:train, validation, test ←dataset.
2:f←forecast(train, validation).
3:g←detector(train, validation).
4:h←sampler(train, validation).
5:probs = []; Cm
t= [].
6:foreach𝑍=𝑋𝑡∪𝑌𝑡in test do
7: ˆ𝑦𝑡,ˆ𝑦′
𝑈= f(𝑋𝑡).
8: ifˆ𝑦𝑡>ˆ𝑦′
𝑈then
9: ˆ𝑦′
𝑡←𝜇(𝑋𝑡)+𝜎(𝑋𝑡)
𝜉(𝑋𝑡)
(−log𝑝′)−𝜉(𝑋𝑡)−1
10: end if
11: foreach of{𝑋𝑖
𝑡}𝛼−𝑑+2
𝑖=1within sliding windows of length 𝑑on𝑋𝑡
do
12: probs.append(g( 𝑋𝑖
𝑡)).
13: end for
14:𝑋𝑚𝑎𝑠𝑘
𝑡←arg min𝑋𝑖
𝑡probs.
15: remove𝑋𝑚𝑎𝑠𝑘
𝑡 from𝑋𝑡.
16: while searching epoch< 𝐾and conditions 𝑓(𝑋′
𝑡) <
ˆ𝑦′
𝑈and𝑓(𝑋′
𝑡)∈[ ˆ𝑦′
𝑡−𝜖,ˆ𝑦′
𝑡+𝜖]does not meet do
17: while sampling epoch <𝐿do
18: Cm
t.append(h(𝑋𝑡,𝑋𝑚𝑎𝑠𝑘
𝑡 )).
19: end while
20:𝑋min
𝑡=arg min𝑋𝑠
𝑡∈𝐶𝑚
𝑡∥𝑓 (𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋𝑠
𝑡−ˆ𝑦′
𝑡∥1.
21:𝑋′
𝑡←(𝑋𝑡\𝑋𝑚𝑎𝑠𝑘
𝑡)⊕𝑋min
𝑡.
22: end while
23:end for
A.3 Data Preprocessing
Global Surface Summary of the Day (GSOD)3. Prior to analysis, the
GSOD dataset undergoes a series of pre-processing steps. Initially,
the daily weather data are converted into monthly aggregates by
averaging each month’s daily recordings, excluding any missing or
invalid entries (e.g., missing or invalid daily precipitation (PRCP)
was recorded as 99.99while that of daily temperature (TEMP) was
recorded as 999.99). To mitigate seasonal influences, standardiza-
tion is applied on a monthly basis. The datasets are then divided
temporally into training, validation, and test sets: data before Janu-
ary 1, 2022, forms the training set; data between January 1, 2022,
3https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-
day?pageNum=1and January 1, 2023, forms the validation set; data after January 1,
2023, forms the test set. Further, sliding windows of 30time steps
(months) are applied to the series. Within each sliding window, the
first 24time steps (months) are used as predictors and the last 6
time steps (months) are used as forecasts.
S&P 5004. Its pre-processing involves two primary steps. First, to
eliminate long-term trends, the differencing method is applied. This
technique computes the difference between consecutive data points,
effectively detrending the time series. Following differencing, the
entire time series is standardized. Data with the end time step before
January 13, 2023, forms the training set; data between January 13,
2023, and January 13, 2024, forms the validation set; data after
January 13, 2024, forms the test set. Sliding windows of 30time
steps (days) are applied to the series. Within each sliding window,
the first 25time steps (days) are used as predictors and the last 5
time steps (days) are used as forecasts.
Dodgers loop sensor5. The pre-processing of this dataset involves
several steps. First, approximately 5.76% of the dataset contains
missing values, which are addressed using the Forward Fill tech-
nique. Here, each missing value is replaced with the most recent
observed data point. Second, traffic data are aggregated in 3-hour in-
tervals, with each interval represented as a single time step, marked
by the final timestamp of each 3-hour period. Third, the entire time
series is standardized, normalizing the data to ensure uniformity in
scale. Data before August 1, 2005 forms the training set; data be-
tween August 1, 2005, and September 1, 2005, forms the validation
set; data after September 1, 2005, forms the test set. Additionally,
sliding windows of 48time steps (equivalent to 144hours or 6days)
are applied to the series. Within each sliding window, the first 40
time steps (120 hours or 5days) are used as predictors and the last
8time steps (24 hours or 1day) are used as forecasts.
A.4 Details for Baseline Algorithms
A.4.1 BaseNN. BaseNN, which is used as a baseline in the study
by Wang et al. [ 32], identifies the closest instance in the training set
that aligns with the desired target outcome. Specifically, BaseNN
selects from the training set the predictor time series 𝑋𝑡whose
subsequent block maxima 𝑦𝑡is closest to the desired target ˆ𝑦′
𝑡.
Once identified, it is used as the counterfactual predictor 𝑋′
𝑡.
A.4.2𝜔-CF. Although it was originally developed for classifica-
tion tasks,𝜔-CF [ 30] can be adapted to a regression scenario. The
counterfactual predictor 𝑋′
𝑡can be learned through an optimization
process defined as
𝑋′
𝑡=arg min
𝑋∗
𝑡max
𝜆𝜆𝑓(𝑋∗
𝑡)−ˆ𝑦′
𝑡+𝑑(𝑋𝑡,𝑋∗
𝑡),
where𝜆is a tuning parameter that balances the two components
of the objective function and 𝑑(𝑋𝑡,𝑋∗
𝑡)is the Manhattan distance.
The learning of 𝜔-CFis based on the Adam optimizer.
A.4.3 Native guide (NG-CF). Native guide (NG-CF) [ 8] encom-
passes a two-step process. Initially, it identifies the nearest unlikely
neighbor of original instance 𝑋𝑡. Then, by leveraging the Dynamic
Time Warping (DTW) algorithm, NG-CF modifies the identified
4https://finance.yahoo.com/quote/%5EGSPC/history?period1=1673481600&period2=
1705017600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true
5https://archive.ics.uci.edu/dataset/157/dodgers+loop+sensor
 
572KDD ’24, August 25–29, 2024, Barcelona, Spain Yue Deng, Asadullah Hill Galib, Pang-Ning, & Lifeng Luo
neighbor of 𝑋𝑡to construct 𝑋′
𝑡, which is engineered to yield a
prediction𝑓(𝑋′
𝑡)closely aligned with the desired outcome ˆ𝑦′
𝑡.
A.4.4 ForecastCF. ForecastCF [ 32], which was initially designed
to explain trend forecasting in time series, can be adapted for ex-
plaining block maxima prediction by replacing the time series in
the forecast window of its original formulation with block maxima
value. Specifically, in the real world, assume that the forecasted
block maxima ˆ𝑦𝑡corresponding to a 𝑋𝑡is extreme. In the counter-
factual world, if ˆ𝑦𝑡is expected to be non-extreme instead, denoted
byˆ𝑦′
𝑡, how should the 𝑋𝑡be changed to 𝑋′
𝑡? Given a forecasting
model𝑓, take𝛼=ˆ𝑦′
𝑡−𝜖and𝛽=ˆ𝑦′
𝑡+𝜖as the lower and upper
boundaries where 𝜖is the tolerance, ForecastCF [32] learns𝑋′
𝑡s.t.
𝑓(𝑋′
𝑡)≈ˆ𝑦′
𝑡:𝑅|𝑋′
𝑡|→𝑅1by minimizing the loss function
𝐿=(
0,if∥𝑓(𝑋′
𝑡)−ˆ𝑦′
𝑡∥≤𝜖,
∥𝑓(𝑋′
𝑡)−𝛼∥+∥𝛽−𝑓(𝑋′
𝑡)∥,otherwise,(15)
The learning of ForecastCF is based on the Adam optimizer.
A.4.5 SPARCE. SPARCE [ 18] is a GAN-based method used to gen-
erate sparse counterfactual explanations for time series, employing
both a discriminator and a generator. The discriminator attempts
to distinguish between a real instance 𝑋𝑡from its counterfactual in-
stance𝑋′
𝑡while the generator attempts to generate counterfactual
instances that could fool the discriminator into misclassifying them
as real instances. In our implementation, the generator initially
receives a real instance 𝑋𝑡linked to an extreme forecasted block
maxima ˆ𝑦𝑡and then generates a counterfactual instance 𝑋′
𝑡that
yields the counterfactual target ˆ𝑦′
𝑡, which is non-extreme. This setup
enables the generator to learn how to modify (or perturb) its initial
input𝑋𝑡to construct 𝑋′
𝑡, which yields a non-extreme block max-
ima ˆ𝑦′
𝑡, achieving the desired target. The generator-discriminator
architecture is jointly trained to minimize the difference between
ˆ𝑦′
𝑡and𝑓(𝑋′
𝑡), which is also constrained to guarantee the sparsity
of𝑋′
𝑡.
A.5 Experimental Setup
For a fair comparison, all the experiments were conducted using the
same trained DeepExtrema model on each dataset to generate block
maxima forecasts. For the predictor and forecast windows set for
the DeepExtrema model, their lengths were chosen as the ones that
yielded the best performance or were used in previous works. The
RMSE of the trained DeepExtrema model evaluated on the test set
varies between 0.40 and 0.75 on the given datasets. For each block
maxima forecast, a counterfactual target is then constructed to be
utilized by both DiffusionCF and all the baselines. To define the
counterfactual target, we first identify a counterfactual threshold
ˆ𝑦′
𝑈to determine whether the forecasted block maxima is extreme.
Towards this end, we set the quantile 𝑝of the GEV distributionin such a way that around 10% (8.8%−11.1%) of the forecasted
block maxima will be considered as extreme values. Based on this
threshold, a counterfactual target ˆ𝑦′
𝑡is subsequently constructed
using the quantile 𝑝′=𝑝−0.2. It is worth noting, however, that our
framework operates independently of the DeepExtrema model’s
performance.
On the GSOD precipitation dataset, the DeepExtrema model is
configured with the following hyperparameters: batch size is set to
128, learning rate to 0.005, dimension of hidden layer to 32, number
of hidden layers to 2,𝜆1to 0.8, and𝜆2to 0.5. For ForecastCF and
𝜔−CF, when ˆ𝑦𝑡is extreme and ˆ𝑦′
𝑡is not, the maximum iterations
for learning is set to 300. For ForecastCF, tolerance is set to 0.05. For
SPARCE, we use the default parameters. Considering the balance
between time and accuracy for DiffusionCF, the sampling window
length is searched in[3,6,9,12]. For each of them, the searching
epochs𝐾(as shown in line 16 in Algorithm 1) is set to 5. In each of
the search windows, the sampling epochs 𝐿(as shown in line 17 in
Algorithm 1) is set to 300. This involves determining the least likely
sub-time series 𝑋𝑡ofsampling window length in𝑋𝑡, removing it,
and then sampling a new 𝑋′
𝑡of the same length from the learned
latent distribution for imputation. The parameter balancing the two
components of the loss function in VAE is set to 10−8
On the GSOD temperature dataset, the DeepExtrema model is
configured with the following hyperparameters: batch size is set to
128,learning rate to 0.001, dimension of hidden layer to 10, number of
hidden layers to 2,𝜆1to 0.1, and𝜆2to 0.5. The parameter balancing
the two components of the loss function in the VAE is set to 10−9,
and for anomaly detection within the VAE, it is set to 1.0. Other
parameters are identical to those used for the GSOD precipitation
data.
On the S&P 500 dataset, the DeepExtrema model is configured
with the following hyperparameters: batch size is set to 128, learning
rateto 0.0001, dimension of hidden layer to 8, number of hidden layers
to 2,𝜆1to 0.2, and𝜆2to 0.5. The sampling window length is searched
in[5,10,15]. Other parameters are identical to those used for the
GSOD temperature data.
On the Dodgers loop sensor traffic forecasting dataset, the Deep-
Extrema model is configured with the following hyperparameters:
batch size is set to 128, learning rate to 0.0005, dimension of hidden
layer to 16, number of hidden layers to 2,𝜆1to 0.1, and𝜆2to 1.5. The
sampling window length is searched in[5,10,15,20]. Additionally,
the parameter balancing the two components of the loss function
in the VAE for anomaly detection is set to 1.0 when its sampling
size is 40. For smaller sampling sizes, specifically 5, 10, 15, or 20, the
balancing parameter is adjusted to 5.0. This variation in settings
is aimed at optimizing the model’s performance for different data
granularity and anomaly detection contexts. Other parameters are
identical to those used for the GSOD temperature data.
 
573