Towards Lightweight Graph Neural Network Search with
Curriculum Graph Sparsification
Beini Xie‚àó
xiebeini.2020@tsinghua.org.cn
DCST, Tsinghua University
Beijing, ChinaHeng Chang‚àó
changh17@tsinghua.org.cn
DCST, Tsinghua University
Beijing, ChinaZiwei Zhang
zw-zhang16@tsinghua.org.cn
DCST, Tsinghua University
Beijing, China
Zeyang Zhang
zy-zhang20@tsinghua.org.cn
DCST, Tsinghua University
Beijing, ChinaSimin Wu
wusm21@lzu.edu.cn
Lanzhou University
Lanzhou, ChinaXin Wang‚Ä†
xin_wang@tsinghua.edu.cn
DCST, BNRist, Tsinghua University
Beijing, China
Yuan Meng
yuanmeng@mail.tsinghua.edu.cn
DCST, Tsinghua University
Beijing, ChinaWenwu Zhu‚Ä†
wwzhu@tsinghua.edu.cn
DCST, BNRist, Tsinghua University
Beijing, China
ABSTRACT
Graph Neural Architecture Search (GNAS) has achieved superior
performance on various graph-structured tasks. However, exist-
ing GNAS studies overlook the applications of GNAS in resource-
constrained scenarios. This paper proposes to design a joint graph
data and architecture mechanism, which identifies important sub-
architectures via the valuable graph data. To search for optimal
lightweight Graph Neural Networks (GNNs), we propose a Light-
weight Graph Neural Architecture Search with Curriculum Graph
SparsIfication and Network Pruning (GASSIP) approach. In partic-
ular, GASSIP comprises an operation-pruned architecture search
module to enable efficient lightweight GNN search. Meanwhile, we
design a novel curriculum graph data sparsification module with an
architecture-aware edge-removing difficulty measurement to help
select optimal sub-architectures. With the aid of two differentiable
masks, we iteratively optimize these two modules to search for
the optimal lightweight architecture efficiently. Extensive experi-
ments on five benchmarks demonstrate the effectiveness of GASSIP.
Particularly, our method achieves on-par or even higher node clas-
sification performance with half or fewer model parameters of
searched GNNs and a sparser graph.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíSemi-supervised learning
settings; Neural networks.
‚àóThe two first authors made equal contributions.
‚Ä†Corresponding authors. DCST is the abbreviation of Department of Computer Science
and Technology. BNRist is the abbreviation of Beijing National Research Center for
Information Science and Technology.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671706KEYWORDS
Graph Neural Network, Graph Sparsification, Neural Architecture
Search
ACM Reference Format:
Beini Xie, Heng Chang, Ziwei Zhang, Zeyang Zhang, Simin Wu, Xin Wang,
Yuan Meng, and Wenwu Zhu. 2024. Towards Lightweight Graph Neural
Network Search with Curriculum Graph Sparsification. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671706
1 INTRODUCTION
Graph data is ubiquitous in our daily life ranging from social net-
works [ 55] and protein interactions [ 39] to transportation [ 43]
and transaction networks [ 33]. Graph Neural Networks (GNNs)
are effective for their ability to model graphs in various down-
stream tasks such as node classification, link prediction, graph
clustering, and graph classification [ 6,7,16,18,26]. In order to
utilize the graph structure, many GNNs like GCN [ 24], GAT [ 40],
and GraphSAGE [ 19] build the neural architecture following the
message-passing paradigm [ 15]: nodes receive and aggregate mes-
sages from neighbors and then update their own representations.
However, facing diverse graph data and downstream tasks, the
manual design of GNNs is laborious. Graph Neural Architecture
Search (GNAS) [ 14,17,27,35,45] tackles this problem and bears
fruit for automating the design of high-performance GNNs.
Compared to traditional GNAS, lightweight GNAS offers a wider
range of application scenarios by reducing computing resource
requirements. Recently, how to modify graph data to unlock the po-
tential of various graph models is a crucial question in data-centric
graph learning [ 48]. This is especially essential for lightweight
GNNs due to the trade-off between model capacity and data vol-
ume while existing studies have overlooked the significance of this
need. Despite concerns about the accuracy of lightweight models,
the lottery ticket hypothesis [ 10,12] suggests that sub-networks,
which remove unimportant parts of the neural network, can achieve
3563
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Beini Xie et al.
(a) Sparsity=5%
(b) Sparsity=40%
(c) Sparsity=64%
Figure 1: Overlaps of removed edges for GCN, GAT, GIN, and Random under diffident graph data sparsification.
comparable performance to the full network. Therefore, the core
objective of lightweight GNAS is to efficiently search for the ef-
fective sub-architecture corresponding to the high-performance
sub-networks. As a result, to achieve the objective of lightweight
GNAS, we need to address the two challenges:
(1) How to effectively search for GNN sub-architectures?
(2) How to efficiently conduct lightweight GNAS?
Regarding the issue of effectiveness, graph neural sub-architectures
remain black-box due to neural network complexity [ 23,35,37,60].
This black-box issue becomes even more significant when graph
sparsification is taken into account, which is demonstrated by the
following observation:
Observation: Different architectures have their views of redundant
information.
For illustration, as shown in Figure 1, we first train structure masks
for three manual-designed GNNs: GCN, GAT, and GIN. Then, we
remove edges with the lowest mask scores, and the remove ratio is
controlled by a sparsity parameter ùëù%. We calculate the overlaps in
the removed edges ùë†(M1,M2)=(A‚àíM1)‚à©(A‚àíM2)
ùëù%|E|, where M1,M2are
trained binarized structure masks under different manual-designed
GNNs. We also add a baseline Random which randomly removes ùëù%
edges to demonstrate the low similarities between various masks.
We could see that the removal edges are diverse especially when the
number of removal edges is small, which indicates the difference in
architectures‚Äô judgment for the structure redundancy. This obser-
vation indicates the optimization of GNN architecture is essential
for a graph with a given sparsity. By leveraging the information
provided by the sparse graph, we can identify the corresponding
sub-architectures.
Regarding the issue of efficiency, directly realizing the light-
weight GNAS goal with a first-search-then-prune pipeline would
suffer from large computational costs since it needs two GNN train-
ing sessions and is therefore undesirable. Instead, joint optimization
by viewing the search and pruning stages as a whole would simplify
the optimization process and ease the burden of computation.
In this paper, we propose Lightweight Graph Neural Architecture
Search with Curriculum Graph SparsIfication and Network Pruning
(GASSIP) based on the following intuition: the underlying assump-
tion of graph sparsification is the existence of a sparse graph that
can preserve the accuracy of the full graph for given tasks [ 10,32,37,61]. Therefore, it is reasonable to infer that the effective
sub-architecture plays a crucial role in processing the informative
sparse graph. As shown in Figure 2, GASSIP performs iterative data
and architecture optimization through two components: operation-
pruned architecture search and curriculum graph data sparsifica-
tion. The former component helps to construct lightweight GNNs
with fewer parameters and the latter one helps to search for more
effective lightweight GNNs. In particular, we conduct operation
pruning with a differentiable operation weight mask to enable
the identification of important parts of the architecture in the
operation-pruned architecture search. Meanwhile, in the curricu-
lum graph data sparsification, we use a differentiable graph struc-
ture mask to identify useful edges in graphs and further help search
for optimal sub-architectures. To conduct a proper judgment of
useful/redundant graph data information, we exploit curriculum
learning [ 25,34,42,49] with an edge-removing difficulty estimator
and sample(nodes) reweighting to learn graph structure better.
Meanwhile, our designed joint search and pruning mechanism
has comparable accuracy and is far more efficient compared with
the first-search-then-pruning pipeline, as shown in experiments.
The graph data and operation-pruned architectures are iteratively
optimized. Finally, GASSIP generates the optimal sub-architecture
and a sparsified graph.
Our contributions are summarized as follows:
‚Ä¢We propose an operation-pruned efficient architecture search
method for lightweight GNNs.
‚Ä¢To recognize the redundant parts of graph data and further help
identify effective sub-architectures, we design a novel curriculum
graph data sparsification algorithm by an architecture-aware
edge-removing difficulty measurement.
‚Ä¢We propose an iterative optimization strategy for operation-
pruned architecture search and curriculum graph data sparsi-
fication, while the graph data sparsification process assists the
sub-architecture searching.
‚Ä¢Extensive experiments on five datasets show that our method
outperforms vanilla GNNs and GNAS baselines with half or even
fewer parameters. For example, on the Cora dataset, we improve
vanilla GNNs by 2.42% and improve GNAS baselines by 2.11%;
the search cost is reduced from 16 minutes for the first-search-
then-prune pipeline to within one minute.
3564Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
ùëæ
 ùëæ‚äôùë¥ùëæ
ùê∑ùëé
ùê∑ùëõ
ùí¢=ùêÄ,ùêóùí¢=(ùêÄ‚äôùë¥ùëÆ,ùêó)
ùê∑=ùê∑ùëé+Œªùê∑ùëõ
Figure 2: The iterative training framework of GASSIP. The graph data and architecture parameters are iteratively optimized.
The operation-pruned architecture search first receives the current learned graph structure and then interactively performs
supernet training and operation pruning. For the curriculum graph data sparsification, it estimates edge-removing difficulty
from node- and architecture-view and updates the graph structure via architecture sampling and sample reweighting.
2 RELATED WORK
2.1 Graph Neural Architecture Search
The research of Graph Neural Architecture Search (GNAS) has
flourished in recent years for automating the GNN architecture
design [ 3,36,46,52,56,57]. We refer the readers to the GNAS sur-
vey [ 58] for details. GraphNAS [ 14] is the first attempt to build the
GNN search space and utilizes reinforcement learning to find the
optimal architecture. For a more efficient search, many works [ 5,
28,59] adopt the differentiable architecture search algorithm. On
a continuous relaxation of the search space, all candidate opera-
tions are mixed via architecture parameters, which are updated
with operation parameters. Considering the certain noises in graph
data, GASSO [ 35] conducts a joint optimization for architecture
and graph structure. All previous works only focus on searching
for high-performance architectures but overlook searching for a
lightweight GNN. As far as we know, the most related work to
ours is ALGNN [ 4]. ALGNN searches for lightweight GNNs with
multi-objective optimization, but it neglects the vital role of the
graph structure, which is important not only for graph represen-
tation learning but also for guiding the graph neural architecture
search. Aside from the GNAS literature, Yan et al . [47] also pro-
posed HM-NAS to improve the architecture search performance
by loosening the hand-designed heuristics constraint with three
hierarchical masks on operations, edges, and network weights. In
contrast, our focus is different from HM-NAS as we aim to search
for a lightweight GNN considering co-optimizing the graph struc-
ture. To achieve this goal, we design a novel lightweight graph
neural architecture search algorithm that exploits graph data to
select optimal lightweight GNNs with a mask on network weights.2.2 Graph Data Sparsification
Graph data sparsification is to sparsify the graph structure which
removes several edges, maintains the information needed for down-
stream tasks, and allows efficient computations [ 31,53]. Some meth-
ods rebuild the graph structure through similarity-related kernels
based on node embeddings. For example, GNN-Guard [ 54] exploits
cosine similarity to measure edge weights. Additionally, some algo-
rithms [ 32,61] leverage neural networks to produce intermediate
graph structures and then use discrete sampling to refine the graph
structure. Furthermore, the direct learning algorithm [ 10,35,50]
takes the edge weights as parameters by learning a structure mask
and removing lower-weight edges. In this paper, we perform graph
data sparsification through graph structure learning using the tools
from curriculum learning and jointly conduct the architecture
search.
2.3 Lightweight Graph Neural Networks
The key to building lightweight neural networks is reducing the
model parameters and complexity, which further enables neural net-
works to be deployed to mobile terminals. GNN computation accel-
eration [ 1,11,13] poses a faster computation for GNNs. Knowledge
distillation [ 21,22] follows a teacher-student learning paradigm
and transfers knowledge from resource-intensive teacher models to
resource-efficient students but keeps performance. Network prun-
ing enables more zero elements in weight matrices. As a result,
pruned networks have quicker forward passes for not requiring
many floating-point multiplications. For example, [ 10,51] leverage
the iterative magnitude-based pruning and [ 29] uses the gradual
magnitude pruning to prune model weights.
3565KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Beini Xie et al.
3 PRELIMINARIES
LetG=(A,X)denotes one graph with ùëÅnodesV={Vùêø,Vùëà},
whereVùêøis the labeled node set and Vùëàis the unlabeled node set,
A‚ààRùëÅ√óùëÅrepresents the adjacency matrix (the graph structure)
andX‚ààRùëÅ√óùê∑0represents the input node features. Eis the edge
set inG.
For a node classification task with ùê∂classes, given a GNN ùëì, it
upgrades the node representations through feature transformation,
message propagation, and message aggregation, and outputs node
predictions Z‚ààRùëÅ√óùê∂:
Z=ùëì(A,X;W), (1)
where Wdenotes network weights. The objective function of the
semi-supervised node classification task is the cross-entropy loss
between predictions and ground truth labels, denoted as Lùëêùëôùëì.
3.1 Differentiable Graph Neural Architecture
Search
The goal of GNAS could be formulated as a bi-level optimization
problem [30]:
ùõº‚àó=arg min
ùõºLùë£ùëéùëô(W‚àó(ùõº),ùõº)
s.t.W‚àó(ùõº)=arg min
WLùë°ùëüùëéùëñùëõ(W,ùõº),(2)
whereùõºis the architecture parameter indicating the GNN architec-
ture, and Wis the learnable weight parameters for all candidate
operations. W‚àó(ùõº)is the best weight for current architecture ùõº
based on the training set and ùõº‚àóis the best architecture according
to validation set.
Here, we resort to the Differentiable Neural Architecture Search
(DARTS) [ 30] algorithm to conduct an efficient search. Considering
the discrete nature of architectures, DARTS adopts continuous re-
laxation of the architecture representation and enables an efficient
search process. In particular, DARTS builds the search space with
the directed acyclic graph (DAG) (shown as supernet in Figure 2)
and each directed edge (ùëñ,ùëó)is related to a mixed operation based on
the continuous relaxation ¬Øùëú(ùëñ,ùëó)(xùëñ)=√ç
ùëú‚ààOexp(ùõº(ùëñ,ùëó)
ùëú)√ç
ùëú‚Ä≤‚ààOexp(ùõº(ùëñ,ùëó)
ùëú‚Ä≤)ùëú(ùëñ,ùëó)(xùëñ),
where xùëñis the input of node ùëñin DAG,Ostands for the candidate
operation set (e.g., message-passing layers), and ùõºis the learnable
architecture parameter. In the searching phase, weight and archi-
tecture parameters are iteratively optimized based on the gradient
descent algorithm. In the evaluation phase, the best GNN architec-
ture is induced from mixed operations for each edge in DAG, and
the optimal GNN is trained for final evaluation.
Nonetheless, the problem Eq.2 does not produce lightweight
GNNs. Next, we introduce the lightweight graph neural architecture
search problem and our proposed method.
4 LIGHTWEIGHT GNAS
In this section, we introduce our lightweight GNAS algorithm,
GASSIP, in detail. First, we formulate the corresponding problem in
Sec. 4.1. Then, we describe the curriculum graph data sparsification
algorithm in Sec. 4.3. Finally, we introduce the iterative optimization
algorithm of the curriculum graph data sparsification and operation-
pruned architecture search in Sec. 4.4.4.1 Problem Formulation
Here, we introduce two learnable differentiable masks Mùê∫,Mùëäfor
the graph structure Aand operation weights Win the supernet. The
value of the operation weight mask indicates the importance level of
operation weights in the architecture and therefore helps to select
important parts in GNN architectures. The trained graph structure
mask could identify useful edges and remove redundant ones and
thus helps to select important architectures while searching.
The goal of GASSIP could be formulated as the following opti-
mization problem:
ùõº‚àó=arg min
ùõºLùë£ùëéùëô(A‚äôM‚àó
ùê∫,W‚àó‚äôM‚àó
ùëä,ùõº)
s.t.W‚àó,M‚àó
ùëä=arg min
W,MùëäLùë°ùëüùëéùëñùëõ(A‚äôM‚àó
ùê∫,W‚äôMùëä,ùõº),
M‚àó
ùê∫=arg min
Mùê∫Lùë†ùë°ùëüùë¢ùëêùë°(A‚äôMùê∫,W‚äôMùëä,ùõº),(3)
where‚äôdenotes the element-wise product operation, M‚àó
ùê∫indicates
the best structure mask based on the current supernet and the
structure loss function Lùë†ùë°ùëüùë¢ùëêùë° ,W‚àóandM‚àó
ùëäare optimal for ùõºand
current input graph structure A‚äôM‚àó
ùê∫. The target of GASSIP is
to find the best discrete architecture according to the architecture
parameters ùõº, obtain the sparsified graph based on the structure
mask Mùê∫and get the pruned network from the weight mask Mùëä.
In practice, we use sparse matrix-based implementation, which
means that Mùê∫is a|E|-dimensional vector.
4.2 Operation-pruned Architecture Search
We leverage network pruning, which reduces the number of trained
parameters, to build lightweight GNNs. In contrast with directly
building smaller GNNs with fewer hidden channels, building GNNs
with reasonable hidden channels and then performing pruning
could realize the lightweight goal without compromising accuracy.
In GASSIP, we prune the operation in the supernet while searching
and name it the operation-pruned architecture search. Specifically,
we co-optimize candidate operation weights Wand their learnable
weight mask Mùëä=ùúé(Sùëä)in the searching phase, where Sùëäis a
trainable parameter and ùúéis a sigmoid function which restricts the
mask score between 0 and 1. The differentiable operation weight
mask helps to identify important weights in operations.
4.3 Curriculum Graph Data Sparsification
Effective sub-architectures could better utilize useful graph infor-
mation to compete with full architectures. Useful graph data could
help to select the most important parts of the GNN architecture
while unsuitable removal of the graph data may mislead the sub-
architecture searching process. Here, we exploit graph structure
learning to help search for optimal sub-architectures. Besides, we
conduct a further graph sparsification step which removes redun-
dant edges after the whole training procedure. The calculation of
message-passing layers includes the edge-level message propaga-
tion, in which all nodes receive information from their neighbors
with|E|complexity. A sparser graph, compared to a dense graph,
has less inference cost because of the decrease in edge-wise message
propagation. Hence, eliminating several edges in graph data helps
to reduce the model complexity and boosts the model inference
efficiency.
3566Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
In this section, we answer the first question in the Sec. 1 and pro-
pose our curriculum graph data sparsification algorithm to guide
the lightweight graph neural architecture search in a positive way.
A successful graph sparsification could recognize and remove redun-
dant edges in the graph structure. For GNNs, it is natural to identify
structure redundancy as edges with low mask scores. However,
for GNAS, plenty of architectures are contained in one supernet
while different architectures have their own views of redundant
information, which is illustrated by observation in Sec. 1.
Structure Redundancy Estimation. In order to estimate the
graph structure redundancy, we exploit structure learning and for-
mulate the graph structure mask Mùê∫with the sigmoid function
ùúé:
Mùê∫=ùúé(Sùê∫‚àíùõæ), (4)
where Sùê∫‚ààRùëÅ√óùëÅis a learnable mask score parameter and ùõæis
a learnable mask threshold parameter which helps to control the
graph data sparsity. The number of non-zero elements in Sùê∫equals
|E|. The sigmoid function restricts the graph structure mask score
into(0,1). Smaller structure mask scores indicate that correspond-
ing edges are more likely to be redundant. The structure mask is
differentiable and updated through the calculated gradient of the
loss functionLùë†ùë°ùëüùë¢ùëêùë° .
Intuitively, if an edge is redundant, it would be regarded as a
redundant one no matter what the architecture is. If the updated
gradients are consistent under several architectures, we have more
confidence to update the structure mask score. Considering the Ob-
servation, we propose to leverage backward gradients on different
architectures to formulate the structure mask update confidence. In
particular, we first sample top- ùêæarchitectures{ùëé1,ùëé2,...,ùëéùêæ}from
the supernet according to the product of the candidate operation
probability in each layer:
ùëé‚àºùëÉùêæ(O,ùõº). (5)
We calculate the backward gradient ‚àáùëéùëñ
Sùê∫=‚àáSùê∫Lùë†ùë°ùëüùë¢ùëêùë° ùëìùëéùëñ(A‚äô
Mùê∫,X)for each sampled architecture {ùëéùëñ,ùëñ=1,2,...,ùêæ}. Then, we
exploit the standard deviation of ‚àáùëéùëñ
Sùê∫to construct the structure
mask update confidence std(‚àáùëé
Sùê∫). The final update for the structure
mask is formulated as:
‚àáSùê∫=√çùêæ
ùëñ=1‚àáùëéùëñ
Sùê∫
ùêæstd(‚àáùëé
Sùê∫),Sùê∫‚ÜêSùê∫‚àíùúÇ‚àáSùê∫, (6)
‚àáùõæ=√çùêæ
ùëñ=1‚àáùëéùëñùõæ
ùêæ, ùõæ‚Üêùõæ‚àíùúÇ‚àáùõæ. (7)
Curriculum Design. Some redundant edges are easier to recognize
than others. For example, if several architectures have different judg-
ments of one edge‚Äôs redundancy, it is hard to decide whether this
edge should be removed or not. For GNAS, false structure removal
in the early stage of searching may misguide the search process.
As a result, we introduce curriculum learning into the graph spar-
sification process based on the architecture-aware edge-removing
difficulty measurement and the sample re-weighting strategy. Our
method belongs to a more general definition of curriculum learning
in which we schedule the training process by softly reweighting
and selecting sample nodes rather than directly controlling the
node difficulty [42].Specifically, we evaluate the architecture-aware edge-removing
difficulty from two views: the architecture view and the node view.
From the architecture view, if several architectures have disparate
judgments of mask update, the corresponding edge moving should
be more difficult. For edge ùëíùëñùëóbetween node ùëñand nodeùëó, the edge-
removing difficulty under the architecture-view is defined as
Dùëé(ùëíùëñùëó)=std(‚àáùëé
Sùê∫,ùëñùëó), (8)
where stdindicates the standard deviation. It is worth mentioning
thatDùëé(ùëíùëñùëó)has already been calculated in the structure redun-
dancy estimation step, which could be saved in memory without
repeating the calculation.
From the node view, edges that link similar nodes are harder to
remove and nodes with a lower information-to-noise ratio have
more difficult edges. Here, we measure the information-to-noise ra-
tio with label divergence. Therefore, the node-view edge-removing
difficulty is evaluated as:
Dùëõ(ùëíùëñùëó)=ùëìùëêùëúùë†(zùëñ,zùëó)+ùúÜ1√ç
ùëó‚ààNùëñùêº(¬Øùë¶ùëó‚â†¬Øùë¶ùëñ)
|Nùëñ|, (9)
whereùúÜ1is a hyper-parameter balancing the node-view difficulty,
Nùëñdenotes neighbors of node ùëñ.ùêº()is the 0-1 indicator function
andùëìùëêùëúùë†represents the cosine similarity function. zùëñstands for
the final representation of node ùëñcalculated in the architecture
parameter training phase. ÀÜùë¶ùëñrepresents the predicted label and ¬Øùë¶ùëñ
is the pseudo-label assigned based on predictions for the output zùëñ:
¬Øùë¶ùëñ=ÀÜùë¶ùëñ, ùëñ‚ààVùëà
ùë¶ùëñ, ùëñ‚ààVùêø.(10)
Considering the inseparable nature of edges and the ease of
usage of nodes in the loss function, we build the node difficulty
based on the architecture-aware edge-removing difficulty. We use
the sample reweighting strategy during the structure mask training
based on the node difficulty.
D(ùëíùëñùëó)=Dùëé(ùëíùëñùëó)+ùúÜ2Dùëõ(ùëíùëñùëó) (11)
D(ùëñ)=√ç
ùëó‚ààNùëñD(ùëíùëñùëó)
|Nùëñ|, (12)
whereùúÜ2is a hyper-parameter. In this way, the node difficulty is
defined as the average edge-removing difficulty for all its neighbors.
Following the idea of Hard Example Mining [ 38], we regard dif-
ficult edges are more informative and need to be weighted more
in training. We assign nodes with higher node/edge-removing dif-
ficulty and higher sample weights. The node weight is calculated
as
ùúÉùëñ=softmax(D(ùëñ)), ùëñ‚ààV (13)
Based on node weights v, the loss function of graph sparsification
for sampled architecture ùëéis
Lùë†ùë°ùëüùë¢ùëêùë° =‚àëÔ∏Å
ùëñ‚ààVùêøùúÉùëñ Lùëêùëôùëì(ùëìùëé(A‚äôMùê∫,X),¬Øùë¶ùëñ)+ùõΩLùëíùëõùë°(Mùê∫),(14)
whereLùëêùëôùëìis the classification loss based on the assigned pseudo-
labels.Lùëíùëõùë°is the mean entropy of each non-zero element in Mùê∫,
which forces the mask score to be close to 0 or 1. ùõΩis a hyper-
parameter balancing the classification and entropy loss.
The overall curriculum graph data sparsification algorithm is
summarized in Algorithm 1. In line 1, pseudo-labels are assigned
3567KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Beini Xie et al.
based on the supernet predictions. Then the node weights in Lùë†ùë°ùëüùë¢ùëêùë°
are updated via edge-removing difficulty calculation in Line 2. In
Lines 3-7,ùêæarchitectures are sampled from the supernet, structural
gradients are calculated and the structure mask is updated.
Algorithm 1 Curriculum Graph Data Sparsification.
Input: The graph dataG(A,X), candidate operations O, architec-
ture parameters ùõº;
Output: The structure mask Mùê∫.
1:Assign pseudo-labels ¬Øyas shown in Eq. 10;
2:Update edge difficulty and assign node weight vin Eq. 13;
3:Sampleùêæarchitectures{ùëé1,ùëé2,...,ùëéùêæ}from the supernet ac-
cording to Eq. 5;
4:forùëñin{1,2,...,ùêæ}do
5: Obtain‚àáùëéùëñ
Sùê∫;
6:end for
7:Calculate structure mask update confidence std(‚àáùëé
Sùê∫);
8:Update the structure mask Mùê∫based on Eq. 6 and Eq. 7;
9:return the structure mask Mùê∫.
4.4 An Iterative Optimization Approach
In this section, we introduce the solution to the second question in
the introduction and solve the optimization problem in Eq. 3 in an
iterative manner.
Since the informative continuous graph structure helps to select
proper operations from the search space while redundant graph
data (e.g., noise edges) will deteriorate the architecture search re-
sult, we iteratively perform graph sparsification and architecture
search optimization. Using the valuable graph data, we pinpoint
key components of the GNN for both operations and weights. Fur-
thermore, the introduction of two trainable masks in Eq. 3 enables
us to efficiently select useful graph structures and essential parts
of the architecture. Fully differentiable parameters, according to
DARTS algorithms, can cut the search time of lightweight GNNs
from several hours [4] to minutes (shown in Sec. 5.3).
Training Procedure. We summarize the whole training proce-
dure in Algorithm 2. Lines 1-6 provide the detailed training process
of GASSIP. For the first ùëüwarm-up epochs, only candidate oper-
ation weights and their masks are updated. Then, the operation
weights/masks, structure masks, and architecture parameters are
iteratively optimized by calculating the gradient descending of ob-
jectives in Eq. 3. In practice, the pruning mask becomes quite sparse
after several iterations. Therefore, the pruning is mostly sparse ma-
trix multiplication, which is more efficient compared with dense
matrix multiplication.
After finishing training, the continuous graph structure mask
and operation weight mask are binarized to perform graph spar-
sification and operation pruning in Line 7. In detail, we initialize
binarized structure mask ¬ØMùê∫=Mùê∫and remove edges that have
mask values lower than the threshold ùõæ:¬ØMùê∫,ùëñùëó=0,ifMùê∫,ùëñùëó<ùõæ.
Meanwhile, to formulate the binarized weight mask ¬ØMùëä, we force
the operation weight mask values that have non-positive values to
zero and weights that have positive mask scores to one. The zero
elements will not be trained during the evaluation phase.Algorithm 2 The Detailed Algorithm of GASSIP.
Input: The graph dataG(A,X), candidate operation set O, train-
ing epoch number ùëá, warm up epoch number ùëü;
Output: The sparsified graph Gùë†ùëù(A‚äô¬ØMùê∫,X), optimal light-
weight architecture ùëìùëé(Gùë†ùëù;W‚äô¬ØMùëä).
1:forùë°‚Üê1toùëádo
2: Update candidate operation weights Wand their masks
Mùëä;
3: ifùë°<ùëüthen
4: continue;
5: end if
6: Training graph structure mask Mùê∫following Algorithm 1;
7: Update architecture parameters ùõº;
8:end for
9:Get the binarized structure mask ¬ØMùê∫and the binarized weight
mask ¬ØMùë§;
10:Induce the optimal GNN architecture ùëé;
11:Return the sparsified graph Gùë†ùëù(A‚äô¬ØMùê∫,X)and the optimal
lightweight architecture ùëìùëé(Gùë†ùëù;W‚äô¬ØMùëä).
At last, the final evaluation is conducted based on the sparsified
graphGùë†ùëùand the induced pruned architectures ùëìùëé(Gùë†ùëù;W‚äô¬ØMùëä).
5 EXPERIMENTS
In this section, we conduct experiments to demonstrate the effec-
tiveness and efficiency of the proposed algorithm, GASSIP. We
also display ablation studies of different components in GASSIP,
the sensitivity analysis for hyper-parameters, the robustness of
GASSIP, and details of the searched architectures. In addition, the
experimental settings are deferred in the Appendix.
5.1 Experimental Results
Analysis of Model Accuracy. We compared GASSIP with vanilla
GNNs and automated baselines on the node classification task on
five datasets in Table 1. The test accuracy (mean¬±std) is reported
over 100 runs under different random seeds. We find that our pro-
posed algorithm outperforms other baselines in all five datasets.
Meanwhile, we can observe that the stds are relatively small, there-
fore the searched result is not sensitive to the choice of random
seed. Among all baselines, only DropEdge and GASSO are able to
conduct graph sparsification/graph structure learning. DropEdge
surpasses the automated baselines in some scenarios, which proves
the possible performance improvements of removing edge. In com-
parison, GASSIP selects removing edges with curriculum sparsifica-
tion jointly with architecture search rather than random sampling.
Compared with GASSO, on the one hand, GASSO directly uses the
supernet performance as the classification results without inducing
an optimal architecture, which hinders its application in memory-
limited scenarios. On the other hand, our method further conducts
an edge deleting step after the graph structure learning and is able
to perform operation pruning, which makes the searched GNNs
more lightweight. Meanwhile, GASSIP achieves better performance
than GUASS on smaller graphs, but GUASS could handle graphs
with more nodes and edges as it is specially developed for large-
scale datasets. However, our trained model is more lightweight
3568Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Experimental results for node classification. The test accuracy is averaged for 100 runs (mean¬±std) using different
seeds. OOM means out-of-memory. The best results are in bold.
Metho
d Cora CiteSeer PubMed Physics Ogbn-Arxiv
V
anilla GNNsGCN [24] 80.93¬±0.67 70.39¬±0.66 79.37¬±0.39 97.43¬±0.12 70.57¬±0.41
GAT [40] 80.78¬±0.93 67.40¬±1.26 78.46¬±0.31 97.76¬±0.11 69.40¬±0.35
ARMA [2] 81.18¬±0.62 69.31¬±0.70 78.51¬±0.38 96.34¬±0.08 70.79¬±0.36
Graph
SparsificationDropEdge [37] 82.42¬±0.65 70.45¬±0.73 77.51¬±0.74 96.67¬±0.19 69.33¬±0.36
NeuralSparse [61] 81.14¬±0.70 70.64¬±0.42 78.12¬±0.31 97.86¬±0.45 OOM
PTDNet [32] 82.42¬±0.65 70.45¬±0.73 77.51¬±0.74 96.47¬±0.38 OOM
GNASD
ARTS [30] 81.65¬±0.48 70.00¬±0.94 79.42¬±0.36 98.28¬±0.07 70.58¬±0.25
GraphNAS [14] 81.33¬±0.84 70.92¬±0.61 78.87¬±0.61 97.45¬±0.06 OOM
GASSO [35] 81.09¬±0.91 68.20¬±1.09 78.15¬±0.59 98.06¬±0.11 70.52¬±0.31
GAUSS [17] 82.05¬±0.21 70.80¬±0.41 79.48¬±0.16 96.76¬±0.08 71.85¬±0.41
Ours
GASSIP 83.20¬±0.42 71.41¬±0.57 79.50¬±0.30 98.46¬±0.06 71.30¬±0.23
and therefore can be applied in scenarios where computational
resources are limited, which does not apply to GUASS.
Analysis of Model Parameters. We further visualized the rela-
tionship between model parameter counts and classification test
accuracy in scatter plots shown in Figure 3. Except for manually-
designed GNNs (GCN, GAT, DropEdge) and GNAS methods (DARTS,
GraphNAS), we also compare with an iteratively magnitude-based
pruning (IMP) method on GCN [ 10] and the unified GNN sparsifi-
cation (UGS) framework [ 10]. IMP iteratively removes ùëù1%(we set
ùëù1=20%) weights and retrains GCN from rewinding weights. UGS
simultaneously prunes the graph structure and the model weights
also in an iteratively magnitude-based pruning way. We set the
iterative edge removing probability ùëù2=5%. We report the best
test performance of IMP and UGS based on the validation perfor-
mance. The hidden size of various baselines is kept the same for
each dataset to make a fair comparison.
As shown in Figure 3, GASSIP achieves higher performance with
fewer parameter counts. For the Cora dataset, GASSIP reserves only
50%parameters compared with GCN and 13%compared with GAT.
For CiteSeer, our method has 8%parameter counts compared with
GAT and 15% compared with ARMA. For Physics, the proposed
method keeps only 6%parameters compared to GAT. Among all
baselines, only DropEdge, UGS, and GASSIP (with ‚àóin Figure 3)
could generate sparsified graph. DropEdge needs to load the whole
graph in memory to perform edge sampling in each GNN layer. As
a result, only UGS and GASSIP have the potential to reduce the
inference cost from the edge-level message propagation calculation.
When Co-considering the model parameters and the graph spar-
sity, our proposed method keeps only 13% ‚àº50% parameter counts
compared to vanilla GNN baselines and removes 7% ‚àº17% edges
on Cora, keeps 8%‚àº50% model parameters and eliminates 8% ‚àº19%
edges on CiteSeer.
5.2 Ablation Study
To get a better understanding of the functional components in
GASSIP, we further conduct ablation studies on operation pruning
and the curriculum graph sparsification parts. Figure 4 shows barplots of the test accuracy on Cora and Physics. We evaluate the
performance under the same search/training hyper-parameters and
report the average accuracy over 100 runs. We compare our method
with three variants: w/o op prn means to search without pruning
operations and only perform curriculum graph data sparsification,
w/o sp stands for searching architectures without the curriculum
graph data sparsification and only conduct operation pruning, w/o
curindicates search architectures with the graph data sparsification
part but without the curriculum scheduler.
By comparing GASSIP with its w/o sp variant in light green,
we could find that GASSIP gains performance improvement from
the curriculum graph sparsification part largely. This phenomenon
shows that the graph sparsification component leads the operation-
pruned architecture search in a positive way and further substan-
tiates the effectiveness of leveraging data to search optimal sub-
architectures. Within the curriculum graph sparsification part, per-
forming graph sparsification (graph structure learning) with the
curriculum scheduler (w/o op prn ) behaves better than without it
(w/o cur ). Therefore, the curriculum scheduler helps to learn the
graph structure mask better. Besides, the iterative optimization
of graph data and operation-pruned architecture works well in
gaining performance improvement.
To further illustrate the effectiveness of graph sparsification in
our method, we add a new ablation study to substitute our graph
sparsification algorithm with DropEdge [ 37], which conducts ran-
dom edge dropping in the differentiable architecture search pro-
cess. The classification accuracy on Cora is 79.42¬±0.63 (DARTS
81.65¬±0.48, ours 83.20¬±0.42). This result shows that poorly designed
edge removal may be harmful to architecture search.
5.3 Efficiency Analysis
Search efficiency. We compare the search efficiency of GNAS meth-
ods in Table 4 here. Based on the differentiable architecture search
algorithm, GASSIP is more efficient than GraphNAS, which searches
architectures with reinforcement learning. The DARTS+UGS base-
line represents the first-search-then-prune method which first searches
architectures and then conducts network pruning and graph data
3569KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Beini Xie et al.
/uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000016/uni00000013 /uni00000013/uni00000011/uni00000016/uni00000018
/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni0000001b/uni00000013/uni00000011/uni00000018/uni0000001b/uni00000014/uni00000011/uni00000013/uni0000001b/uni00000014/uni00000011/uni00000018/uni0000001b/uni00000015/uni00000011/uni00000013/uni0000001b/uni00000015/uni00000011/uni00000018/uni0000001b/uni00000016/uni00000011/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002a/uni00000024/uni00000036/uni00000036/uni0000002c/uni00000033/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c/uni0000000d
/uni0000002a/uni00000026/uni00000031/uni00000027/uni00000055/uni00000052/uni00000053/uni00000028/uni00000047/uni0000004a/uni00000048/uni0000000d
/uni0000002a/uni00000024/uni00000037/uni00000024/uni00000035/uni00000030/uni00000024/uni00000027/uni00000024/uni00000035/uni00000037/uni00000036/uni0000002c/uni00000030/uni00000033
/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000031/uni00000024/uni00000036
/uni00000038/uni0000002a/uni00000036/uni0000000d
(a) Cora
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000019/uni0000001c/uni00000011/uni00000013/uni00000019/uni0000001c/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000014/uni00000011/uni00000013/uni0000001a/uni00000014/uni00000011/uni00000018/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002a/uni00000024/uni00000036/uni00000036/uni0000002c/uni00000033/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c/uni0000000d
/uni0000002a/uni00000026/uni00000031/uni00000027/uni00000055/uni00000052/uni00000053/uni00000028/uni00000047/uni0000004a/uni00000048/uni0000000d
/uni0000002a/uni00000024/uni00000037/uni00000024/uni00000035/uni00000030/uni00000024/uni00000027/uni00000024/uni00000035/uni00000037/uni00000036/uni0000002c/uni00000030/uni00000033/uni00000038/uni0000002a/uni00000036/uni0000000d/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000031/uni00000024/uni00000036 (b) CiteSeer
/uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c/uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001c/uni0000001a/uni00000011/uni00000013/uni0000001c/uni0000001a/uni00000011/uni00000018/uni0000001c/uni0000001b/uni00000011/uni00000013/uni0000001c/uni0000001b/uni00000011/uni00000018/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002a/uni00000024/uni00000036/uni00000036/uni0000002c/uni00000033/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c/uni0000000d
/uni0000002a/uni00000026/uni00000031
/uni00000027/uni00000055/uni00000052/uni00000053/uni00000028/uni00000047/uni0000004a/uni00000048/uni0000000d/uni0000002a/uni00000024/uni00000037
/uni00000024/uni00000035/uni00000030/uni00000024/uni00000027/uni00000024/uni00000035/uni00000037/uni00000036
/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000031/uni00000024/uni00000036 (c) Physics
Figure 3: Scatter plots showing the relationship between the total number of model parameters and node classification
performance on (a) Cora, (b) CiteSeer, and (c) Physics. Methods with ‚àóare able to perform graph sparsification. Scatters in the
upper left show higher classification performance with lower parameter counts.
Table 2: Results under noisy edges.
#
noisy edge GCN GAT DropEdge PTDNet NeuralSparse GraphNAS GASSO DARTS UGS GASSIP
1k
78.37¬±0.47 77.99¬±0.75 78.16¬±0.44 77.16¬±1.27 79.24¬±0.51 79.14¬±0.46 78.73¬±0.90 78.57¬±0.63 78.65¬±0.53 79.26¬±0.49
5k 69.14¬±0.55 67.42¬±0.74 68.28¬±0.63 66.48¬±1.23 68.68 ¬±0.53 71.28¬±0.59 70.76¬±0.95 71.76¬±0.88 69.49¬±0.62 73.80¬±0.64
Table 3: Denfensive performance under non-targeted attack (Mettack [63]).
Dataset
GCN GAT Arma DropEdge GCN-Jaccard RGCN GraphNAS GASSO DARTS GASSIP
Cora
66.93¬±1.06 68.61¬±2.24 65.09¬±1.28 68.48¬±1.44 69.89¬±1.19 67.20¬±1.02 70.05¬±1.27 66.51¬±2.76 61.05¬±1.32 73.05¬±0.69
CiteSeer 56.20¬±1.46 62.31¬±1.46 60.11¬±1.30 56.16¬±1.74 56.97¬±1.90 57.40¬±0.96 62.09¬±3.41 57.88¬±2.09 61.59¬±1.13 65.52¬±0.45
/uni0000001a/uni00000017/uni0000001a/uni00000019/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002a/uni00000024/uni00000036/uni00000036/uni0000002c/uni00000033
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000052/uni00000053/uni00000003/uni00000053/uni00000055/uni00000051
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni00000053
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000046/uni00000058/uni00000055
(a) Cora
/uni0000001c/uni0000001a/uni00000011/uni00000019/uni0000001c/uni0000001a/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni00000011/uni00000013/uni0000001c/uni0000001b/uni00000011/uni00000015/uni0000001c/uni0000001b/uni00000011/uni00000017/uni0000001c/uni0000001b/uni00000011/uni00000019/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002a/uni00000024/uni00000036/uni00000036/uni0000002c/uni00000033
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000052/uni00000053/uni00000003/uni00000053/uni00000055/uni00000051
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni00000053
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000046/uni00000058/uni00000055 (b) Physics
Figure 4: Ablation study for GASSIP under scenarios of with-
out operation pruning (w/o op prn ), without graph data spar-
sification (w/o sp ), without curriculum scheduler (w/o cur ).
sparsification. It is loaded with heavy searching, pruning, and re-
training costs, which is far less efficient than GASSIP.
Table 4: Searching time cost for GNAS methods.
Metho
ds D
ARTS DART+UGS GASSO GraphNAS GASSIP
Search Time (min) 0.55
15.79 0.62 223.80 0.98
Retraining efficiency. We show the (re-)training time cost (s)
of searched GNNs in Table 5. For GNAS methods, we report the
retraining time of searched GNNs. For manual-designed GNNs,
we directly report the training time. For a fair comparison, we fix
training epochs as 300 and report the averaged training time over
100 runs. Results on three datasets illustrate that the training timemodel searched by GASSIP is the least compared to all baselines,
which indicates the efficiency of lightweight GNNs searched by
GASSIP.
Table 5: (Re-)training time (s) for searched GNNs averaged
over 100 runs.
Metho
ds GCN GAT ARMA DropEdge DARTS GraphNAS GASSIP
Cora
2.53 3.59 2.56 2.61 2.09 3.57 2.19
CiteSeer 2.73 3.73 3.17 2.85 3.40 3.98 2.51
Physics 11.19 70.67 79.96 13.15 10.47 64.81 5.55
5.4 Sensitivity Analysis
hyper-parameters ùúÜ1andùúÜ2.We further conduct a sensitivity
analysis for curriculum learning hyper-parameters ùúÜ1andùúÜ2in
Figure 5. Larger ùúÜ1indicates that the label divergence is more im-
portant in difficulty calculation in our curriculum algorithm while
largerùúÜ2suggests that node similarity matters more. In the edge-
removing difficulty measurement, with these two node difficulty
terms (ùúÜ1>0,ùúÜ2>0) have greater performance than without them
(ùúÜ=0), which further illustrates the difficulty measurement in our
curriculum algorithm is reasonable.
hyper-parameters ùêæ.We conduct a sensitivity analysis experi-
ment for hyper-parameter ùêæand the results are shown in Table 6.
As the sampled architecture number ùêæin graph structure redun-
dancy estimation gets larger, the classification performance first
increases and then drops. A smaller ùêæindicates that only a few
3570Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
0 2 4 6 8 1070.770.870.971.071.171.271.3Test Accuracy (%)
(a)ùúÜ1
0 2 4 6 8 1071.2771.2871.2971.3071.31Test Accuracy (%)
 (b) Physics
Figure 5: Lineplots for hyper-parameters (a) ùúÜ1(fixùúÜ2=1)
and (b)ùúÜ2(fixùúÜ1=1).
architectures that are most likely to be selected by NAS are sampled
to evaluate the redundancy of the graph structure. When ùêæ=1,
only the top-1 architecture induced by current ùõºis sampled. How-
ever, it may not be enough to estimate the redundancy, leading to
poor graph sparsification. A larger ùêæincludes more architectures in
the redundancy estimation. Still, it may contain a lot more architec-
tures that are unlikely to be selected in the search phase and cause
poor classification results. As a result, we choose ùêæto be 2 in our
experiment. We will add this sensitivity analysis in our revision.
Table 6: Sensitivity analysis for sampled architecture number.
K 1 2 4 8
Cora 81.62¬±0.67 83.20¬±0.44 82.11¬±0.49 81.97¬±0.41
CiteSeer 69.42¬±0.69 71.42¬±0.56 70.58¬±0.48 70.23¬±0.48
Physics 98.34¬±0.04 98.46¬±0.06 98.29¬±0.01 98.30¬±0.02
5.5 Defend against Adversarial Attacks
By incorporating the graph data into the optimization process, our
method can effectively handle noisy or attackers‚Äô manipulated data.
By incorporating the graph data into the optimization process, our
method can effectively handle noisy data or data that has been
manipulated by attackers [ 8,9,20,41]. Specifically, the curriculum
graph sparsification allows GASSIP to filter out edges that are
either noisy or have been added maliciously by attackers. As a
result, our approach exhibits a degree of robustness in the face of
such adversarial scenarios.
Noisy Data. We compare our methods with various baselines such
as graph sparsification methods like DropEgde [ 37], PTDNet [ 32],
and NeuralSparse [ 61] on noisy Cora data by randomly adding 1k/5k
edges in Table 2. This result demonstrates that when there exist
noisy edges, GASSIP could achieve the best performance compared
with baselines.
Poisoning Data. To prove the defensive potential of our joint
data and architecture optimization algorithm in countering ad-
versarial attacks, we conduct experiments on perturbed data in-
cluding comparison with state-of-the-art defensive methods like
GCN-Jaccard [ 44] and RGCN [ 62]. Table 3 demonstrates that GAS-
SIP exhibits defensive abilities against perturbed data. However, in
order to further enhance its robustness against adversaries, it is nec-
essary to develop special designs tailored to attack settings. Despite
this, the current experiment clearly illustrates that incorporatingdata into the optimization objective function has the potential to
alleviate the detrimental effects caused by adversaries.
6 SEARCHED ARCHITECTURES IN DETAIL
We follow the literature of graph NAS and construct the supernet
as in GASSO and AutoAttend. Here, we provide the searched GNN
architecture by GASSIP in Table 7.
Table 7: Search Architectures by GASSIP.
Dataset Searched Architecture
Cora GCNConv‚à•SAGEConv
CiteSeer GCNConv‚à•SAGEConv
PubMed GCNConv‚à•ArmaConv
Physics Linear‚à•GCNConv
Ogbn-Arxiv GCNConv‚à•ArmaConv‚à•GATConv
7 COMPARISON WITH LIGHTWEIGHT GNNS
We further compare with UGS [ 10], [29], and [ 51] on Cora and
CiteSeer in the following table. The results show that our proposed
method outperforms these three baselines in terms of classification
accuracy with the same level of model parameters.
Table 8: Comparison with lightweight GNNs
Method UGS [10] [29] [51] GASSIP
Cora 80.30 81.88 82.83 83.20
CiteSeer 70.40 71.23 70.48 71.41
8 CONCLUSION AND FUTURE WORKS
In this paper, we propose an efficient lightweight graph neural ar-
chitecture search algorithm, GASSIP. It iteratively optimizes graph
data and architecture through curriculum graph sparsification and
operation-pruned architecture search. Our method can reduce the
inference cost of searched GNNs at the architecture level by re-
ducing the model parameters, and at the data level by eliminating
redundant edges. To the best of our knowledge, this is the first
work to search for lightweight GNN considering both data and
architecture. Our future works include evaluating GASSIP on other
large-scale graphs, providing a theoretical analysis of the conver-
gence of our iterative optimization algorithm, and developing a
unified benchmark for lightweight GNAS.
ACKNOWLEDGMENTS
This work is supported by the National Key Research and Devel-
opment Program of China No.2023YFF1205001, National Natural
Science Foundation of China (No. 62222209, 62250008, 62102222,
62206149), Beijing National Research Center for Information Sci-
ence and Technology under Grant No. BNR2023RC01003, BNR2023TD03006,
and Beijing Key Lab of Networked Multimedia.
3571KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Beini Xie et al.
REFERENCES
[1]Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L√≥pez-Alonso, and Eduard
Alarc√≥n. 2021. Computing graph neural networks: A survey from algorithms to
accelerators. ACM Computing Surveys (CSUR) 54, 9 (2021), 1‚Äì38.
[2]Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. 2021.
Graph neural networks with convolutional arma filters. IEEE Transactions on
Pattern Analysis andMachine Intelligence (2021).
[3]Jie Cai, Xin Wang, Haoyang Li, Ziwei Zhang, and Wenwu Zhu. 2024. Multimodal
Graph Neural Architecture Search under Distribution Shifts. In Proceedings of
theAAAI Conference onArtificial Intelligence, Vol. 38. 8227‚Äì8235.
[4]Rongshen Cai, Qian Tao, Yufei Tang, and Min Shi. 2021. ALGNN: Auto-Designed
Lightweight Graph Neural Network. In Pacific Rim International Conference on
Artificial Intelligence. Springer, 500‚Äì512.
[5]Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and
Qingming Huang. 2021. Rethinking Graph Neural Architecture Search From
Message-Passing. In Proceedings oftheIEEE/CVF Conference onComputer
Vision andPattern Recognition (CVPR). 6657‚Äì6666.
[6]Heng Chang, Jie Cai, and Jia Li. 2023. Knowledge Graph Completion with
Counterfactual Augmentation. In Proceedings oftheACM Web Conference 2023 .
2611‚Äì2620.
[7]Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Somayeh Sojoudi, Junzhou
Huang, and Wenwu Zhu. 2021. Spectral graph attention network with fast eigen-
approximation. In Proceedings ofthe30th ACM International Conference on
Information &Knowledge Management (CIKM). 2905‚Äì2909.
[8]Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui,
Xin Wang, Wenwu Zhu, and Junzhou Huang. 2022. Adversarial Attack Framework
on Graph Embedding Models with Limited Knowledge. IEEE Transactions on
Knowledge andData Engineering (TKDE) (2022).
[9]Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng
Cui, Wenwu Zhu, and Junzhou Huang. 2020. A restricted black-box adversarial
framework towards attacking graph embedding models. In Proceedings ofthe
AAAI conference onArtificial Intelligence (AAAI), Vol. 34. 3389‚Äì3396.
[10] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.
2021. A unified lottery ticket hypothesis for graph neural networks. In
International Conference onMachine Learning. PMLR, 1695‚Äì1706.
[11] Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak, Ling Liang,
Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du, et al .2021. Rubik: A hierarchical
architecture for efficient graph neural network training. IEEE Transactions on
Computer-Aided Design ofIntegrated Circuits andSystems 41, 4 (2021), 936‚Äì
949.
[12] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).
[13] Qiang Fu, Yuede Ji, and H Howie Huang. 2022. TLPGNN: A Lightweight Two-
Level Parallelism Paradigm for Graph Neural Network Computation on GPU. In
Proceedings ofthe31stInternational Symposium onHigh-Performance Parallel
andDistributed Computing. 122‚Äì134.
[14] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2019. Graphnas:
Graph neural architecture search with reinforcement learning. arXiv preprint
arXiv:1904.09981 (2019).
[15] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. International
Conference onLearning Representations (2017), 1263‚Äì1272.
[16] Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui.
2020. Implicit graph neural networks. Advances inNeural Information
Processing Systems (NeurIPS) 33 (2020), 11984‚Äì11995.
[17] Chaoyu Guan, Xin Wang, Hong Chen, Ziwei Zhang, and Wenwu Zhu. 2022.
Large-scale graph neural architecture search. In International Conference on
Machine Learning. PMLR, 7968‚Äì7981.
[18] Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang Zhang, Yijian
Qin, Jiyan Jiang, Xin Wang, and Wenwu Zhu. 2021. AutoGL: A Library for Auto-
mated Graph Learning. In ICLR 2021 Workshop onGeometrical andTopological
Representation Learning.
[19] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In NIPS.
[20] Ming Jin, Heng Chang, Wenwu Zhu, and Somayeh Sojoudi. 2021. Power up!
Robust Graph Convolutional Network via Graph Powering. In Proceedings of
theAAAI Conference onArtificial Intelligence (AAAI), Vol. 35. 8004‚Äì8012.
[21] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao.
2021. Amalgamating knowledge from heterogeneous graph neural networks.
InProceedings oftheIEEE/CVF Conference onComputer Vision andPattern
Recognition. 15709‚Äì15718.
[22] Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, and Chuan Sheng Foo. 2022. On rep-
resentation knowledge distillation for graph neural networks. IEEE Transactions
onNeural Networks andLearning Systems (2022).
[23] Zhao Kang, Haiqi Pan, Steven C. H. Hoi, and Zenglin Xu. 2020. Robust Graph
Learning From Noisy Data. IEEE Transactions onCybernetics 50, 5 (2020), 1833‚Äì
1843. https://doi.org/10.1109/TCYB.2018.2887094[24] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification
with Graph Convolutional Networks. International Conference onLearning
Representations (2017).
[25] Haoyang Li, Xin Wang, and Wenwu Zhu. 2023. Curriculum graph machine
learning: A survey. arXiv preprint arXiv:2302.02926 (2023).
[26] Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. 2022. Semi-supervised
hierarchical graph classification. IEEE Transactions onPattern Analysis and
Machine Intelligence 45, 5 (2022), 6265‚Äì6276.
[27] Yanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. 2021. One-shot Graph Neu-
ral Architecture Search with Dynamic Search Space. In AAAI Conference on
Artificial Intelligence.
[28] Yanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. 2021. One-shot Graph
Neural Architecture Search with Dynamic Search Space. In Thirty-Fifth AAAI
Conference onArtificial Intelligence. 8510‚Äì8517.
[29] Chuang Liu, Xueqi Ma, Yinbing Zhan, Liang Ding, Dapeng Tao, Bo Du, Wenbin
Hu, and Danilo Mandic. 2022. Comprehensive Graph Gradual Pruning for Sparse
Training in Graph Neural Networks. arXiv preprint arXiv:2207.08629 (2022).
[30] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable
architecture search. arXiv preprint arXiv:1806.09055 (2018).
[31] Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, and
Xia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNN Train-
ing and Inference via Degree-Based Sparsification. Transactions onMachine
Learning Research (2023).
[32] Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen,
and Xiang Zhang. 2021. Learning to drop: Robust graph neural network via
topological denoising. In Proceedings ofthe14th ACM international conference
onweb search anddata mining. 779‚Äì787.
[33] Naoto Minakawa, Kiyoshi Izumi, Hiroki Sakaji, and Hitomi Sano. 2022. Graph
Representation Learning of Banking Transaction Network with Edge Weight-
Enhanced Attention and Textual Information. In Companion Proceedings ofthe
Web Conference 2022. 630‚Äì637.
[34] Yijian Qin, Xin Wang, Ziwei Zhang, Hong Chen, and Wenwu Zhu. 2024. Multi-
task graph neural architecture search with task-aware collaboration and curricu-
lum. Advances inneural information processing systems 36 (2024).
[35] Yijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. 2021. Graph differentiable
architecture search with structure learning. Advances inNeural Information
Processing Systems 34 (2021), 16860‚Äì16872.
[36] Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. 2022.
Nas-bench-graph: Benchmarking graph neural architecture search. Advances in
neural information processing systems 35 (2022), 54‚Äì69.
[37] Yu Rong et al .2020. Dropedge: Towards deep graph convolutional networks on
node classification. ICLR (2020).
[38] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. 2016. Training region-
based object detectors with online hard example mining. In Proceedings ofthe
IEEE conference oncomputer vision andpattern recognition. 761‚Äì769.
[39] Damian Szklarczyk, Annika L Gable, David Lyon, Alexander Junge, Stefan Wyder,
Jaime Huerta-Cepas, Milan Simonovic, Nadezhda T Doncheva, John H Mor-
ris, Peer Bork, Lars J Jensen, and Christian von Mering. 2018. STRING v11:
protein‚Äìprotein association networks with increased coverage, supporting func-
tional discovery in genome-wide experimental datasets. Nucleic Acids Research
(2018).
[40] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Li√≤, and Yoshua Bengio. 2018. Graph Attention Networks. In International
Conference onLearning Representations.
[41] Xin Wang, Heng Chang, Beini Xie, Tian Bian, Shiji Zhou, Daixin Wang, Zhiqiang
Zhang, and Wenwu Zhu. 2023. Revisiting adversarial attacks on graph neural
networks for graph classification. IEEE Transactions onKnowledge andData
Engineering (2023).
[42] Xin Wang, Yudong Chen, and Wenwu Zhu. 2021. A survey on curriculum
learning. IEEE transactions onpattern analysis andmachine intelligence 44, 9
(2021), 4555‚Äì4576.
[43] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan
Jia, and Jian Yu. 2020. Traffic flow prediction via spatial temporal graph neural
network. In Proceedings ofTheWeb Conference 2020. 1082‚Äì1092.
[44] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial Examples for Graph Data: Deep Insights into Attack and
Defense. In Proceedings ofthe28th International Joint Conference onArtificial
Intelligence. AAAI Press, 4816‚Äì4823.
[45] Beini Xie, Heng Chang, Ziwei Zhang, Xin Wang, Daixin Wang, Zhiqiang Zhang,
Rex Ying, and Wenwu Zhu. 2023. Adversarially robust neural architecture
search for graph neural networks. In Proceedings oftheIEEE/CVF Conference
onComputer Vision andPattern Recognition. 8143‚Äì8152.
[46] Peng Xu, Lin Zhang, Xuanzhou Liu, Jiaqi Sun, Yue Zhao, Haiqin Yang, and Bei Yu.
2023. Do not train it: a linear neural architecture search of graph neural networks.
InInternational Conference onMachine Learning. PMLR, 38826‚Äì38847.
[47] Shen Yan, Biyi Fang, Faen Zhang, Yu Zheng, Xiao Zeng, Mi Zhang, and Hui
Xu. 2019. Hm-nas: Efficient neural architecture search via hierarchical masking.
InProceedings oftheIEEE/CVF International Conference onComputer Vision
Workshops. 0‚Äì0.
3572Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[48] Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao Sun,
Yue Yu, Yixin Xiao, Qi Zhang, et al .2023. Data-centric graph learning: A survey.
arXiv preprint arXiv:2310.04987 (2023).
[49] Yang Yao, Xin Wang, Yijian Qin, Ziwei Zhang, Wenwu Zhu, and Hong Mei. 2024.
Data-Augmented Curriculum Graph Neural Architecture Search under Distri-
bution Shifts. Proceedings oftheAAAI Conference onArtificial Intelligence 38,
15 (2024), 16433‚Äì16441.
[50] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. Gnnexplainer: Generating explanations for graph neural networks.
Advances inneural information processing systems 32 (2019).
[51] Haoran You, Zhihan Lu, Zijian Zhou, Y. Fu, and Yingyan Lin. 2021. Early-Bird
GCNs: Graph-Network Co-optimization towards More Efficient GCN Training
and Inference via Drawing Early-Bird Lottery Tickets. In AAAI Conference on
Artificial Intelligence.
[52] Wentao Zhang, Zheyu Lin, Yu Shen, Yang Li, Zhi Yang, and Bin Cui. 2022. Deep
and Flexible Graph Neural Architecture Search. In International Conference on
Machine Learning. PMLR, 26362‚Äì26374.
[53] Xikun Zhang, Dongjin Song, and Dacheng Tao. 2023. Ricci Curvature-Based
Graph Sparsification for Continual Graph Representation Learning. IEEE
Transactions onNeural Networks andLearning Systems (2023).
[54] Xiang Zhang and Marinka Zitnik. 2020. Gnnguard: Defending graph neural
networks against adversarial attacks. Advances inneural information processing
systems 33 (2020), 9263‚Äì9275.
[55] Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. 2022. Improving social
network embedding via new second-order continuous graph neural networks.
InProceedings ofthe28th ACM SIGKDD Conference onKnowledge Discovery
andData Mining. 2515‚Äì2523.
[56] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and Wenwu
Zhu. 2022. Dynamic graph neural networks under spatio-temporal distribution
shift. Advances inneural information processing systems 35 (2022), 6074‚Äì6089.
[57] Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, and Wenwu
Zhu. 2024. Unsupervised graph neural architecture search with disentangled
self-supervision. Advances inNeural Information Processing Systems 36 (2024).
[58] Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2021. Automated Machine Learning on
Graphs: A Survey. In Proceedings oftheThirtieth International Joint Conference
onArtificial Intelligence, IJCAI-21. 4704‚Äì4712.
[59] Huan ZHAO, Quanming YAO, and Weiwei TU. 2021. Search to aggregate neigh-
borhood for graph neural network. In 2021 IEEE 37th International Conference
onData Engineering (ICDE). 552‚Äì563.
[60] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil
Shah. 2021. Data augmentation for graph neural networks. In Proceedings ofthe
aaai conference onartificial intelligence, Vol. 35. 11015‚Äì11023.
[61] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu,
Haifeng Chen, and Wei Wang. 2020. Robust graph representation learning via
neural sparsification. In International Conference onMachine Learning . PMLR,
11458‚Äì11468.
[62] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust Graph
Convolutional Networks Against Adversarial Attacks. In Proceedings ofthe
25th ACM SIGKDD International Conference onKnowledge Discovery &Data
Mining (KDD ‚Äô19). 1399‚Äì1407.
[63] Daniel Z√ºgner and Stephan G√ºnnemann. 2019. Adversarial Attacks on Graph
Neural Networks via Meta Learning. In International Conference onLearning
Representations (ICLR).
A LIMITATIONS
The main purpose of this paper is to search for a lightweight GNN
(i.e., lightweight GNN design) that offers a wider range of appli-
cation scenarios (e.g., edge computing) by limited computational
resource requirements. Therefore, the current implementation of
GASSIP has difficulty to be integrated with graphs with billions of
nodes, This difficulty of scalability commonly hinders both graph
sparsification and current GNAS research in applications with con-
tained resources without a specifically designed sampling strategy.B EXPERIMENTAL SETTINGS
Dataset. We evaluate the node classification performance on 5
datasets: Cora, CiteSeer, PubMed, Physics, and Ogbn-Arxiv. The
statistics of all datasets are shown in Table 9. The first three datasets
follow a traditional semi-node classification train-valid-test split.
Physics and Ogbn-Arxiv represent large datasets where we ran-
domly split train:valid:test=50%:25%:25% for Physics and follow the
default setting for Ogbn-Arxiv.
Table 9: Dataset statistics.
Dataset #nodes #edges #features #classes
Cora 2,708 10,556 1,433 7
Citeseer 3,327 9,104 3,703 6
Pubmed 19,717 88,648 500 3
Physics 34,493 495,924 8,415 5
Ogbn-Arxiv 169,343 1,166,243 128 40
Baselines. We compare our method with representative hand-
crafted GNNs: GCN [ 24], GAT [ 40], ARMA [ 2], and three graph
sparsification methods: DropEdge [ 37], PTDNet [ 32], and Neu-
ralSparse [ 61]. We also compare with representative GNAS base-
lines, including DARTS [ 30], GraphNAS [ 14], GASSO [ 35], and
GUASS [ 17]. We use GASSO as the representative of GNAS with
structure learning.
Implementation Details. For GASSIP, we set the number of layers
as 2 for CiteSeer, Cora, PubMed, Physics, and 3 for Ogbn-Arxiv.
In building search space, we adopt GCNConv [24],GATConv [40],
SAGEConv [19],ArmaConv [2], and Linear as candidate operations.
Due to the memory limit, the search space is narrowed down to
GCNConv,GATConv, ArmaConv, and Linear for Physics and Ogbn-
Arxiv. The supernet is constructed as a sequence of layers, We set
batch normalization (only for Physics and Ogbn-Arxiv) and dropout
before each layer, and use ReLU as the activation function.
Detailed Hyper-parameters. For vanilla GNNs, we follow the
hyper-parameters in the original paper except tuning hyper-parameters
like hidden channels in {16,64,128,256}and dropout in{0.5,0.6,0.8}.
For GNAS methods, we use the Adam optimizer to learn param-
eters. We retrain for 100 runs on their searched optimal architec-
tures to make a fair comparison of the architecture performance.
For GASSIP, we fix the number of sampled architectures as ùêæ=2,
entropy loss the coefficient in curriculum graph data sparsification
asùõΩ=0.001, and the edge-removing difficulty hyper-parameters
ùúÜ1=1,ùúÜ2=1. The supernet training epoch is fixed to 250 and the
warm-up epoch number is set as ùëü=10.
3573