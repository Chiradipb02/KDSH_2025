EcoVal: An Efficient Data Valuation Framework for Machine
Learning
Ayush Tarun‚àó
Ola Krutrim
Bengaluru, India
ayush.tarun@olakrutrim.comVikram Chundawat‚àó
RespAI Lab
India
vikram2000b@gmail.comMurari Mandal ‚Ä†
RespAI Lab
Kalinga Institute of Industrial
Technology
Bhubaneswar, India
murari.mandalfcs@kiit.ac.in
Hong Ming Tan
NUS Business School
National University of Singapore
Singapore
thm@nus.edu.sgBowei Chen
Adam Smith Business School
University of Glasgow
Glasgow, United Kingdom
bowei.chen@glasgow.ac.ukMohan Kankanhalli
School of Computing
National University of Singapore
Singapore
mohan@comp.nus.edu.sg
ABSTRACT
Quantifying the value of data within a machine learning work-
flow can play a pivotal role in making more strategic decisions
in machine learning initiatives. The existing Shapley value based
frameworks for data valuation in machine learning are computa-
tionally expensive as they require considerable amount of repeated
training of the model to obtain the Shapley value. In this paper, we
introduce an efficient data valuation framework EcoVal, to estimate
the value of data for machine learning models in a fast and practical
manner. Instead of directly working with individual data sample,
we determine the value of a cluster of similar data points. This value
is further propagated amongst all the member cluster points. We
show that the overall value of the data can be determined by esti-
mating the intrinsic and extrinsic value of each data. This is enabled
by formulating the performance of a model as a production function,
a concept which is popularly used to estimate the amount of output
based on factors like labor and capital in a traditional free economic
market. We provide a formal proof of our valuation technique and
elucidate the principles and mechanisms that enable its accelerated
performance. We demonstrate the real-world applicability of our
method by showcasing its effectiveness for both in-distribution and
out-of-sample data. This work addresses one of the core challenges
of efficient data valuation at scale in machine learning models. The
code is available at https://github.com/respai-lab/ecoval.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning; ‚Ä¢Informa-
tion systems‚ÜíInformation systems applications.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672068KEYWORDS
Data valuation, Shapley value, machine learning
ACM Reference Format:
Ayush Tarun‚àó, Vikram Chundawat‚àó, Murari Mandal ‚Ä†, Hong Ming Tan,
Bowei Chen, and Mohan Kankanhalli. 2024. EcoVal: An Efficient Data Val-
uation Framework for Machine Learning. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24),
August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3672068
1 INTRODUCTION
Data valuation is a pivotal concern in modern machine learning
(ML) and data analytics, where the quality and worth of data have
profound implications for decision-making, model performance,
and data marketplace. Quantifying the worth of data plays an impor-
tant role in data pricing and regulation compliance [ 39,49], remov-
ing low-value/noisy data from the training set [ 28,48], and incen-
tivizing data sharing by personal data monetization [ 17,18,23,25].
In a ML framework, the quality of data determines the effectiveness
of the final model. Therefore, identifying high and low value data
through data valuation would yield significant benefits for a wide
range of machine learning applications.
Background: In recent studies, a cooperative game theory con-
cept, Shapley value [ 41] has been frequently used for data valu-
ation in supervised ML [ 17,18,25]. It offers a desirable property
of equitable reward allocation. The data Shapley and its exten-
sions [ 17,18,31,32] have empirically shown the effectiveness of
Shapley value based valuation in a fixed dataset as well as in a
particular distribution of data, allowing for out-of-time data val-
uation. The value of a data point in ML relies on its individual
contribution to the model‚Äôs performance and its relationship with
other data points utilized during training. The presence of similar
data in the training set can dilute the significance of individual
points. To account for these interactions, data Shapley methods
evaluate the contribution of each point by determining how its
absence affects the overall performance of the model. This process
*These authors contributed equally to this work
‚Ä†Corresponding author
 
2866
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ayush Tarun et al.
usually involves repeatedly training the model with the selective
exclusion of certain instances or subsets, thereby identifying those
with the most substantial impact. The impact is measured by the
observing the change in the performance score of the ML model.
However, this incurs a high computational cost, typically requiring
model training runs in the order of ùëÇ(ùëõ2)in current methodologies,
whereùëõis the total number of data points in the dataset.
Motivation: While offering insightful analyses of data point
significance and alleviating the issue of poor discrimination of
data quality in leave-one-out (LOO) error methods, existing data
Shapley based frameworks [ 17,18,32,50] suffer from a high com-
putational cost. The need for a higher number of repeated training
sessions for a model, as required by these methods, leads to inef-
ficiencies in both time and resource utilization. Furthermore, this
inefficiency translates to an increased carbon footprint due to the en-
ergy requirements of training, thereby exacerbating climate change
concerns [ 33]. The development of scalable algorithms capable of
handling extensive datasets is essential for practical use of data
valuation in real-world applications.
Our Contribution: We adopt a two-step approach where the
valuation is performed at cluster-level first and the value is further
divided among the cluster members. The similar data points are
represented through a cluster which significantly reduces the total
number of data points to deal with during training phase of the
valuation process. At cluster level, we can use a simple LOO error
for valuation since there is minimal possibility (almost zero) of a
similar datum to be found in other clusters. The difficulty however,
is to divide the value at each cluster among the cluster members. To
address this issue, a novel approach is proposed based on production
functions in economics. Our two-step approach aims to significantly
speed up the valuation process in comparison to the Truncated
Monte Carlo (TMC) Shapley.
In this paper, we introduce a a novel framework based on Leave
Cluster Out (LCO) and production functions for data valuation in
machine learning. The framework is computationally efficient, with
theoretical and empirical verifications. The following are the key
contributions of our work:
Novel Framework: The intuition behind our framework is that
we find a group of similar items and estimate this cluster‚Äôs marginal
contribution. As similar data items are bound to have similar values,
we extend this principle to estimate cluster-level value through
Leave Cluster Out (LCO).
We introduce a production function formulation representing the
relation between the data and its utility in a model. We show that
this formulation can be used to estimate the value of individual
data based on the value of each cluster.
Computational Efficiency: We estimate the intrinsic and ex-
trinsic value of each data point to determine the individual data
value. By checking only the marginal contribution of the represen-
tative data point of a cluster, we substantially reduce the overhead
of creating multiple subsets containing similar data points. Our
approach is scalable to large datasets without being limited by the
presence of similar data points in the dataset.
Theoretical Proof: We provide a theoretical proof of our data
valuation method. We also show that the valuation obtained by
our method has negligible error margin when compared with the
vanilla Shapley value approximation method.Empirical Evaluation: We conduct experiments with machine
learning models on MNIST, CIFAR10, and CIFAR100. We compare
the value rankings of our method with the existing state-of-the-
art data valuation approaches data Shapley [ 18], LOO error, and
Distributional Shapley [ 17] and notice similar or better performance
with significant speed-up in data valuation process.
2 RELATED WORK
Literature Review of Shapley Value. Shapley value as formalized
in [42] establishes the axiomatic properties and demonstrates its
unique ability to fairly allocate gains from cooperation among play-
ers. This seminal contribution laid the theoretical groundwork for
subsequent developments in cooperative game theory [ 4,20,27,37].
Shapley value has been extensively used for applications in eco-
nomics, [21, 36, 40], management science [14, 29], online advertis-
ing [46]. In machine learning, it has been utilized for addressing the
challenges in pricing ML training data, feature selection, and ML
explainability. [ 11,53] proposed to employ Shapley value properties
for feature selection. [ 1,16] use Shapley value in market mechanism
to price training data and match buyers to sellers data marketplace
design. [ 34] introduced the SHAP framework, leveraging Shapley
values to provide interpretable explanations for machine learning
models. Other works have also explored its utility in explaining
black-box model predictions [6, 9, 19, 47].
Data Valuation in ML. Recently, the subfield of data valuation
in ML models has attracted significant attention and the existing
works have shown promising outcomes. Data Shapley [ 18,25] pro-
posed to use Shapley value from cooperative game theory for valu-
ation of training data. KNN Shapley [ 24] improved the efficiency of
data Shapley by using a k-nearest neighborhood model. Distribu-
tional Shapley [ 17] expanded the scope of valuation to the under-
lying data distribution instead of only considering the data points.
Beta Shapley [ 32] relaxes the efficiency axiom in DataShapley and
reports utility of data valuation in detecting mislabeled images in
the training data. Data Banzhaf [ 51] propose to estimate the Banzhaf
value to improve results on noisy label detection. Several works
have attempted to improve the efficiency of the Shapley value com-
putation through approximation techniques [ 31]. Apart from this,
other aspects of data value has been studied in [ 12,13,23,38,52].
However, approximation of Shapley value still remains a compu-
tationally expensive process, making it difficult to adapt for large
models and datasets. The main goal of this work is to develop an
alternative efficient data valuation framework to overcome this
problem.
Literature Review of Production Functions. [35] offers a de-
tailed outline of the evolution and econometrics of the production
function. Aggregate production functions are used in macroeco-
nomics to represent the relationship between total output of an
economy (GDP) and the inputs used to produce that output. These
inputs typically include capital ( ùêæ), labor (ùêø), and sometimes other
factors like technology or natural resources [ 5,43]. The simplest
production function used in economics, is the Cobb-Douglas pro-
duction function introduced by [ 3]. [30] identifies all multi-factor
production functions with given elasticity of output and from given
elasticity of production. Production functions have been used in
various domains, including health, education, and energy, to name
 
2867EcoVal: An Efficient Data Valuation Framework for Machine Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
a few [ 2,7,22]. In our study, we adopt the concept of a production
function and adapt it for data valuation. This approach draws in-
spiration from foundational works and recent advancements in the
field. [ 26] develops a theoretical framework that applies the produc-
tion function to the economics of data, particularly employing data
as an input for training machine learning models. Moreover, [ 15]
highlights the role of data as information aimed at reducing fore-
cast errors, which hints at a production function characterized by
bounded returns to data. In our paper, we align with these per-
spectives and further the discourse by specifically focusing on the
application of the production function concept in the valuation of
data.
3 PRELIMINARIES
Let an ML model ùëÄ, intended for a task ùëá, is trained on a dataset
ùêµof sizeùëö. Letùëàdenote the performance metric and ùëàùëádenote
the performance obtained on task ùëá. The overall performance ùëà
is achieved after training a sufficient number of epochs ùëí. Here
the sufficient number of epochs means |ùëàùëí+ùëñ+1‚àíùëàùëí+ùëñ|<ùõæfor all
ùëñ‚â•0, whereùõæis an arbitrarily small value. It should be noted that
ùõæarises due to the randomness within the learning algorithm and
not further training. The value of a data point is denoted by Œ¶.
Leave-One-Out (LOO) Error. The LOO error computes the
value of a datum ùëßbased on the increase in performance obtained
by adding it to the training set:
Œ¶ùêøùëÇùëÇ(ùëß;ùëà,ùêµ)=ùëà(ùêµ)‚àíùëà(ùêµ\{ùëß}). (1)
It struggles in differentiating data quality when similar data
samples exist in the dataset. For example, if each sample has a
duplicate copy in the dataset, the LOO will return a value 0for all
of the samples. Shapley value overcomes this limitation by checking
the marginal distribution over many subsets of the dataset.
Shapley Value. Shapley value [ 18] measures the value of a data
pointùëßas the weighted average of the performance increase when
ùëßis added to different subsets of the dataset ùêµ:
Œ¶ùë†(ùëß;ùëà,ùêµ)=1
ùëöùëö‚àëÔ∏Å
ùëò=11 ùëö‚àí1
ùëò‚àí1‚àëÔ∏Å
ùëÜ‚äÜùêµ\{ùëß}Œî(ùëß;ùëà,ùëÜ), (2)
where|ùëÜ|=ùëò‚àí1forùëò‚ààùëÅandŒî(ùëß;ùëà,ùëÜ)=ùëà(ùëÜ‚à™{ùëß})‚àíùëà(ùëÜ).
Thus, data Shapley value is the weighted average of the marginal
contribution Œî(ùëß;ùëà,ùëÜ). It satisfies the following Shapley value ax-
ioms:
‚Ä¢Dummy Player: If ùëà(ùëÜ‚à™{ùëß})=ùëà(ùëÜ)+ùëífor allùëÜ‚äÜùêµ√ü
and someùëí‚ààùëÖ, then Œ¶(ùëß;ùëà,ùêµ)=ùëí.
‚Ä¢Symmetry: If ùëà(ùëÜ‚à™{ùëß})=ùëà(ùëÜ‚à™{ùëß‚Ä≤})for allùëÜ‚äÜùêµ\{ùëß,ùëß‚Ä≤},
thenŒ¶(ùëß;ùëà,ùêµ)=Œ¶(ùëß‚Ä≤;ùëà,ùêµ).
‚Ä¢Linearity: Œ¶(ùëß;ùõº1ùëà1+ùõº2ùëà2,ùêµ)=ùõº1Œ¶(ùëß;ùëà1,ùêµ)+ùõº2Œ¶(ùëß;ùëà2,ùêµ)
forùõº1,ùõº2‚ààùëÖ.
‚Ä¢Efficiency:√ç
ùëß‚ààùëÅŒ¶(ùëß;ùëà,ùêµ)=Œ¶(ùëà,ùêµ).
Further details regarding the interpretation of the above axioms in
the context of machine learning can be referred to Ghorbani and
Zou [18] and Jia et al. [25].
Production Function. In economics, a production function
expresses the relationship between the specific quantities and com-
binations of different inputs a company uses and the amount of
output it produces. Commonly used production functions includeLinear, Leontief, Cobb‚ÄìDouglas [ 8,10], CES, and CRESH [ 45], each
varying in their assumptions for the input and the output. The wide-
spread usage of the Cobb-Douglas production function is attributed
to its simplicity and adaptability. It assumes homogeneity of inputs
and this principle is consistent with many machine learning setups.
LetùëÉ(ùëî)denote the production over a set of goods ùëî=(ùëî1,ùëî2,....ùëîùëõ),
the Cobb-Douglas production function is defined as
ùëÉ(ùëî)=ùê¥ùëõ√ñ
ùëñ=1ùëîùë•ùëñ
ùëñ, (3)
whereùë•ùëñis an elastic parameter for good ùëñ, andùê¥is the total factor
productivity or the quality factor. If inputs are just labor ùêøand
capitalùêæ, the production function is then
ùëÉ=ùê¥ùêøùë•ùêæùë¶. (4)
It should be noted that the Cobb-Douglas production function also
supports the diminishing returns in terms of both labor and capital.
The Law of Diminishing Returns [44] states that as the amount
of a single factor of production is incrementally increased, the
marginal output of a production process decreases. This property
is analogous to how more data points have diminishing effects on
a machine learning models performance. We therefore adapt the
formulation of production functions in our proposed method to
efficiently distribute the value of a cluster among its data members.
4 PROPOSED METHOD
A two-stage approach is proposed for efficient data valuation. First,
data points are clustered together based on shared characteristics.
Then, a leave cluster out (LCO) technique is applied to estimate the
value of each cluster. This cluster value is then distributed among
its members to obtain the preliminary individual data valuations.
In the following, we delve into the building blocks of the proposed
method and discuss its properties compared to the original Shapley
methods.
4.1 Leave-Cluster-Out
Cluster analysis is firstly performed on the given data and the
marginal contribution of a cluster ùëêcan be expressed as
ùëâùëê=ùëà(ùêµ)‚àíùëà(ùêµ\ùëê). (5)
The simple LOO error may provide an underestimated view of
the true impact of specific data points, especially when similar
data points remain in the dataset even after removal. Data Shap-
ley alleviates this issue but suffers from high computational cost.
By organizing data points into clusters based on their similarity,
we ensure that when an entire cluster is removed, there are no
closely-related points to mask the effect of its absence in other
clusters. Consequently, this leads to a more precise assessment of
the cluster‚Äôs marginal contribution, effectively approximating its
value. Furthermore, this clustering approach significantly reduces
the number of model training iterations needed in comparison to
Data Shapley since evaluations are conducted at the cluster level
instead of for each individual data point. Once we have obtained
cluster-level valuations, the subsequent step involves efficiently
approximating the values of individual data points within each
cluster.
 
2868KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ayush Tarun et al.
4.2 Value Propagation within a Cluster
Production Function for ML. We adapt the Cobb-Douglas pro-
duction function to approximate the data value for ML. In this
context, we can draw an analogy: the labor ùêøcorresponds to the
available data points for the model; the learning capacity or the
number of parameters in the model represents the capital ùêæ; and
the final output is the obtained performance on the test set ùëàùëá.
As both data quantity and model complexity exhibit diminishing
returns, the Cobb-Douglas production function can be leveraged to
effectively model learning performance. Therefore, we propose to
approximate the model‚Äôs performance after ùëíepochs as
ùëàùëá(ùëÜ,ùëÅ)=ùê¥ùëì(ùëÜ)‚Ñéùëá(ùëÅ), (6)
whereùëì(ùëÜ)quantifies the informational utility of the dataset ùëÜto the
predictive efficacy of the model ùëà,ùëádenotes the task, and ‚Ñéùëá(ùëÅ)
represents the effect of the model capacity which is dependent on
ùëÅ, the number of parameters of the model.
Then, for a new point ùëß, the performance change Œîùëàin the model
incurred by the small increase ( ŒîùëÜ={ùëß}) inùëÜcan be computed by
Œîùëàùëá(ùëÜ,ùëÅ)
=ùê¥ùëì(ùëÜ+ŒîùëÜ)‚Ñéùëá(ùëÅ)‚àíùê¥ùëì(ùëÜ)‚Ñéùëá(ùëÅ)
=ùê¥[ùëì(ùëÜ+ŒîùëÜ)‚àíùëì(ùëÜ)]‚Ñéùëá(ùëÅ)
=ùê¥ùëì(ùëÜ+ŒîùëÜ)‚àíùëì(ùëÜ)
ùëú(ùëß)
‚Ñéùëá(ùëÅ)ùëú(ùëß). (7)
To better understand Eq. (7), let us consider ùëìas a smooth function
ofùë•as specified in Eq. (6), i.e., ùëàùëá(ùë•,ùëÅ)=ùê¥ùëì(ùë•)‚Ñéùëá(ùëÅ). Thus,
a minor change in ùë•leads to a change in ùëàùëá, which can be ap-
proximated by ùê¥ùëì‚Ä≤(ùë•)‚Ñéùëá(ùëÅ)Œîùë•. This allows us to interpret the
expression enclosed in square brackets of Eq. (7) as effectively serv-
ing as the derivative of ùëìwith respect to the set ùëÜ, especially when
considering incremental changes to ùëÜ.
Also, in Eq. (7), ùëú(ùëß)serves as an indicator of how a single data
point enhances the model‚Äôs overall performance and is a proxy
toŒîùë•discussed above. Therefore, the difference ùëì(ùëÜ+ŒîùëÜ)‚àíùëì(ùëÜ)
captures the marginal impact on the model‚Äôs performance when
datasetùëÜis augmented by a new data point. Analogous to the
concept of derivatives in calculus, this difference, when normalized
by the contribution ùëú(ùëß)of the individual point, can be interpreted
as the ‚Äúrate-of-change‚Äùof ùëìupon the addition of a new data point.
This rate is contingent on both the existing dataset ùëÜand the new
data point being added. That is
ùëà(ùëÜ‚à™{ùëß})‚àíùëà(ùëÜ)=ùõºùëá(ùëß)ùõΩ(ùëß,ùëÜ), (8)
where
ùõºùëá(ùëß)=ùê¥‚Ñéùëá(ùëÅ)ùëú(ùëß),
ùõΩ(ùëß,ùëÜ)=ùëì(ùëÜ+ŒîùëÜ)‚àíùëì(ùëÜ)
ùëú(ùëß).
Substituting the above into Eq. (2) then gives
Œ¶ùë†(ùëß;ùëàùëá,ùêµ)=1
ùëöùëö‚àëÔ∏Å
ùëò=11 ùëö‚àí1
ùëò‚àí1‚àëÔ∏Å
ùëÜ‚äÇùêµ\{ùëß}
|ùëÜ|=ùëò‚àí1ùõºùëá(ùëß)ùõΩ(ùëß,ùëÜ)
=ùõºùëá(ùëß)ùõΩ‚àó(ùëß,ùêµ) (9)where
ùõΩ‚àó(ùëß,ùêµ)=1
ùëöùëö‚àëÔ∏Å
ùëò=11 ùëö‚àí1
ùëò‚àí1‚àëÔ∏Å
ùëÜ‚äÇùêµ\{ùëß}
|ùëÜ|=ùëò‚àí1ùõΩ(ùëß,ùëÜ) (10)
Proposition 1. (Production Function Based Valuation for
ML). Letùõºùëá(ùëß)denote the intrinsic value of a datum ùëß, i.e.,ùõºùëá(ùëß)is
only dependent on the characteristics of ùëß. The interaction of ùëßwith
rest of the data points in ùêµis captured by ùõΩ‚àó(ùëß,ùêµ). From equitable
properties of data valuation in [ 18], we postulate that for every datum
ùëßhaving an intrinsic value ùõºùëá(ùëß), theùõΩ‚àó(ùëß,ùêµ)acts as a multiplier or
extrinsic factor that decreases the value of ùëßif similar data points are
present in the dataset. Similarly, it increases the data value if ùëßis a
unique datum. Then the data valuation can be performed as below
Œ¶(ùëß;ùëàùëá,ùêµ)=ùõºùëá(ùëß)ùõΩ‚àó(ùëß,ùêµ). (11)
To simplify notation, we denote ùõºùëá(ùëß)withùõº(ùëß), and Œ¶(ùëß;ùëàùëá,ùêµ)
withŒ¶(ùëß;ùëà,ùêµ)for the rest of the discussion, since ùëáis invariant.
Fast Data Valuation. Based on the above setup, we propose an
efficient data valuation method that also works as an efficient proxy
to Distributional Shapley [ 17] to predict valuation for unseen data-
points in the distribution. The existing Data Shapley adheres to
two fundamental axioms [ 50]: symmetry and efficiency. Symmetry
states that for points ùëßandùëß‚Ä≤that contribute similarly to the model‚Äôs
performance should have the same value, i.e. ùëà(ùëÜ‚à™{ùëß})=ùëà(ùëÜ‚à™
{ùëß‚Ä≤})for allùëÜ‚ààùêµ\{ùëß,ùëß‚Ä≤}.Efficiency, on the other hand, ensures
that the aggregate value of all data points aligns with the overall
performance achieved after training on the entire dataset.
Proposition 2. (Fast Data Valuation of Cluster Data Mem-
bers) The symmetry and efficiency properties when applied to a spe-
cific cluster implies the data points within a cluster, characterized
by similar features, will likely possess similar values and a cluster‚Äôs
value can be accurately represented as the sum of its constituent data
points‚Äô valuations.
Letùëâùëê(=Œ¶ùëê) be the value of cluster ùëê, the initial value assigned to
any data point ùëßùëñwithin this cluster is:
ùëâùëñ=ùëâùëê/ùëõùëê, (12)
whereùëõùëêis the number of data points in cluster ùëê. Using this cluster-
level assignment of initial data value, we estimate the actual data
value based on Eq. (11) as
ùëâ‚àó
ùëñ=ùõºùëñùõΩ‚àó
ùëñ. (13)
Estimating ùõºandùõΩ‚àó.Assuming each cluster contains an equal
number of data points, the distribution of similar and dissimilar
samples encountered by each datum becomes roughly uniform. This
results in a near-constant extrinsic factor, ùõΩ‚àó(ùëß,ùêµ), across all data
points. Thus, the value of these data points are directly proportional
toùõº(ùëßùëñ). We useùëÑùëñto denote the value of individual datum to
differentiate it from ùëâùëñvalue that is initialized by the cluster value
in Eq. (12).
Theorem 4.1. For data point ùëßùëñ, assuming there is no error in ùõΩ‚àó
ùëñ,
its adjusted value ùëâŒîùõºùëñ
ùëñis
ùëâŒîùõºùëñ
ùëñ=(ùõºùëñ+Œîùõºùëñ)ùõΩ‚àó
ùëñ=ŒìùõºùëñùõºùëñùõΩ‚àó
ùëñ, (14)
 
2869EcoVal: An Efficient Data Valuation Framework for Machine Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
where Œìùõºùëñis an adjustment factor for ùõºùëñ
Œìùõºùëñ=1+ùëÑùëñ√ç
ùëßùëó‚ààùëêùëÑùëóùëâùëê. (15)
The adjustment factor Œìùõºùëñensures that value of individual data
point is adjusted not only based on its personal contribution ( ùëÑùëñ)
but also in proportion to the corresponding cluster‚Äôs overall impact
(ùëâùëê). This dual consideration is crucial for accurately reflecting the
true value of each data point, balancing internal cluster contribu-
tions with the broader context of the cluster‚Äôs role within the entire
dataset. Including ùëâùëêin the adjustment ensures that the recalibra-
tion ofùõºùëñremains sensitive to both intra-cluster dynamics and
the comparative significance of each cluster, providing a more nu-
anced and equitable valuation of data points across a heterogeneous
dataset.
Corollary 4.1.1. When all data points in a cluster are exactly
the same, the adjustment factor should be equal to 1so that for each
point inùëê, the value becomes ùëâùëñ. But the above formulation of Œìùõºùëñ
yields 1+1/ùëõwhen all the points are identical as ùëâùëñandùëâùëówill be
equal for any ùëñ,ùëó. Thus, we normalize Œìùõºùëñas follows
Œìùõºùëñ=1
1+ùëâùëê/ùëõùëê 
1+ùëÑùëñ√ç
ùëßùëó‚ààùëêùëÑùëóùëâùëê!
. (16)
Similar toùõºùëñ, we find the adjustment factor for ùõΩ‚àó
ùëñ, i.e.ŒìùõΩ‚àó
ùëñ.ùõΩ‚àó
ùëñ
measures the interaction of ùëßùëñwith all other data points in ùêµ. As all
data points similar to ùëßùëñbelong to the same cluster and ùõΩ‚àó
ùëñis only
affected by the other members in ùëßùëñ‚Äôs cluster. We use the distance
betweenùëßùëñand cluster centroid as a measure to it‚Äôs belongingness
to the cluster or similarity to other points in the cluster.
Theorem 4.2. For data point ùëßùëñ, assuming no error in ùõºùëñ, its ad-
justed value ùëâŒîùõΩ‚àó
ùëñ
ùëñis
ùëâŒîùõΩ‚àó
ùëñ
ùëñ=ùõºùëñ(ùõΩ‚àó
ùëñ+ŒîùõΩ‚àó
ùëñ)=ŒìùõΩ‚àó
ùëñùõºùëñùõΩ1ùëñ, (17)
whereùëëùëñis the distance of ùëßùëñandŒìùõΩ‚àó
ùëñis the adjustment factor repre-
sented as
ŒìùõΩ‚àó
ùëñ=1
1+ùëâùëê/ùëõùëê 
1+ùëëùëñ√ç
ùëßùëó‚ààùëêùëëùëóùëâùëê!
. (18)
Production Function based Data Value Estimation. The
final approximation value ÀÜŒ¶ùëñof the data point is
ÀÜŒ¶ùëñ=(ùõºùëñ+Œîùõºùëñ)(ùõΩ‚àó
ùëñ+ŒîùõΩ‚àó
ùëñ). (19)
Ignoring ŒîùõºùëñŒîùõΩ‚àó
ùëñthen gives
ÀÜŒ¶ùëñ‚âà(ùõºùëñ+Œîùõºùëñ)ùõΩ‚àó
ùëñ+ùõº(ùõΩ‚àó
ùëñ+ŒîùõΩ‚àó
ùëñ)‚àíùõºùëñùõΩ‚àó
ùëñ. (20)
By substituting Eq. (13), Eq. (14), Eq. (17), we obtain.
ÀÜŒ¶ùëñ=ùëâŒîùõºùëñ
ùëñ+ùëâŒîùõΩ‚àó
ùëñ
ùëñ‚àíùëâùëñ
=ùëâùëñ(Œìùõºùëñ+ŒìùõΩ‚àó
ùëñ‚àí1)
=ùëâùëñ"1
1+ùëâùëê/ùëõùëê 
1+ùëÑùëñ√ç
ùëßùëó‚ààùëêùëÑùëóùëâùëê!
+
1
1+ùëâùëê/ùëõùëê 
1+ùëëùëñ√ç
ùëßùëó‚ààùëêùëëùëóùëâùëê!
‚àí1#
. (21)
For the reader‚Äôs convenience, Algorithm 1 outlines the implemen-
tation steps of the EcoVal efficient data valuation framework.4.3 Discussion: Comparison with Original
Shapley
Letùê∏(ùëß)denote the appropriate embedding from a machine learning
model or the pre-final layer of a deep learning model for a data
pointùëß. We extend the notion of Lipschitz Stability of data Shapley
introduced in [ 17] to estimate the difference in value of different
data points. We use proximity of the embeddings ùê∏(ùëß)as a proxy
to the closeness in the underlying data distribution and formalize
the same in the following Theorem.
Theorem 4.3. For anyùëßùëó,ùëßùëòif||ùê∏(ùëßùëó)‚àíùê∏(ùëßùëò)||<ùúñthen,|Œ¶(ùëßùëó)‚àí
Œ¶(ùëßùëò)|‚â§ùúñ1for very small ùúñ,ùúñ1‚â•0
From the principle of clustering, a datum ùëßùëóbelongs to cluster ùëê
if
||ùê∏(ùëßùëó)‚àíùê∏(ùëßùëò)||‚â§ùúñ,‚àÄùëßùëò‚ààùëê, (22)
then for this cluster
|Œ¶(ùëßùëó)‚àíŒ¶(ùëßùëò)|‚â§ùúñ1,‚àÄùëßùëò,ùëßùëó‚ààùëê. (23)
It means all Shapley values lie within an ùúñ1interval. Therefore,
Œ¶(ùëßùëó)for anyùëßùëó‚ààùëêcan be expressed as
Œ¶(ùëßùëó)=¬ØŒ¶ùëê+ùõø(ùëßùëó), (24)
whereùõø(ùëßùëó)‚â§ùúñ/2and ¬ØŒ¶ùëêlies somewhere in the ùúñ1interval.
Corollary 4.3.1. The valueùëâùëêof the a cluster by Shapley axioms
is defined as
ùëâùëê=‚àëÔ∏Å
ùëßùëó‚ààùëêŒ¶(ùëßùëó)=ùëõùëê¬ØŒ¶ùëê+‚àëÔ∏Å
ùõø(ùëßùëó). (25)
The detailed proof to 4.3.1 is provided in the Appendix.
Theorem 4.4. The difference between the original Shapley value
and our proposed approximated data value is
ŒîŒ¶ùëñ‚âàùëõùëê¬ØŒ¶ùëêùõøùëÖ√ç
ùëßùëó‚ààùëêùëÑùëó+ùëõ2ùëê¬ØŒ¶ùëêùëÑùëñùõøùëÖ
(√ç
ùëßùëó‚ààùëêùëÑùëó)2. (26)
Due to the intrinsic limitations on the magnitudes of average Shapley
value within a cluster ¬ØŒ¶ùëêand individual point contribution ùëÑùëñ, both
values inherently remain within a bounded range. As cluster size ùëõùëê
increases, the predicted aggregate value√ç
ùëßùëó‚ààùëêùëÑùëóproportionately
grows, naturally restricting the potential expansion ofùëõùëê√ç
ùëßùëó‚ààùëêùëÑùëó. Ad-
ditionally, a moderately accurate regression model ensures a low ùõøùëÖ
error. Therefore, our method produces Shapley value estimates Œ¶ùëñwith
minimal margin of error.
The detailed proof to 4.4 is provided in the Appendix.
5 EXPERIMENTS
We show the broad effectiveness of the proposed valuation frame-
work and its general applicability to machine learning models
through empirical evidence. We estimate the value of data in a ma-
chine learning model in MNIST, CIFAR10, and CIFAR100 datasets.
We compare our method with Data Shapley [ 18] and Distributional
Shaply [17].
Experiment Settings: Following the common practice in previ-
ous works, we extract the features from last layer of a pre-trained
network and apply Shapley on this embedded vector. We sample
 
2870KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ayush Tarun et al.
Algorithm 1 EcoVal Data Valuation
1:ùëÄ(.;ùúì): Fully Trained Model
2:ùêµ: Training Dataset
3:ùêµùê∑: Set of available points from the underlying distribution of
ùêµ
4:ùëÄ‚àíùëõ(ùë•;ùúì)‚Üê Embedding of data ùë•obtained from the ùëõùë°‚Ñélast
layer of the model
5:Letùê∏(ùë•)=ùëÄ‚àíùëõ(ùë•;ùúì)
6:Letùê¥ùëêbe a clustering algorithm then (ùë•ùëñ,ùëêùëó)‚Üêùê¥ùëê(ùêµùê∑)‚àÄùë•ùëñ‚àà
ùêµùê∑whereùëêùëó‚ààùê∂is the cluster associated with ùë•ùëñandùê∂is the
set of all clusters
7:Find valuation at cluster level
8:ùëâùëêùëó=ùëà(ùêµ)‚àíùëà(ùêµ\ùëêùëó)‚àÄùëêùëó‚ààùê∂
9:Initialize value ùëâùëñfor each cluster member ùë•ùëñ
10:ùëâùëñ=ùëâùëêùëó/ùëõùëêùëó, whereùëõùëêùëóis the number of elements in cluster ùëêùëó
to whichùë•ùëñbelongs
11:Initialize:ùê∑‚Üê[]
12:forùëêùëó‚ààùê∂do
13: Sampleùëãùëó= {ùë•ùëó
1,ùë•ùëó
2, ...ùë•ùëó
ùëõùëê} fromùëêùëó
14:ùê∑‚Üêùê∑‚à™ùëãùëó
15:end for
16:Run TMC Shapley [18]
17:(ùë•ùëò,ùë£ùëáùëÄùê∂ùëò)‚ÜêùëáùëÄùê∂(ùëàùëá,ùê∑)‚àÄùë•ùëò‚ààùê∑
18:Train a regression model ùëÖon the sampled data
{(ùë•1,ùë£ùëáùëÄùê∂ 1),(ùë•2,ùë£ùëáùëÄùê∂ 2)....(ùë•|ùê∑|,ùë£ùëáùëÄùê∂|ùê∑|)}
19:forùëêùëó‚ààùê∂do
20:(ùë•ùëó
ùëñ,ùëûùëó
ùëñ)‚ÜêùëÖ(ùë•ùëó
ùëñ)‚àÄùë•ùëó
ùëñ‚ààùëêùëó
21: Let¬Øùë•ùëêùëóbe the centroid of the cluster ùëêùëó
22:(ùë•ùëó
ùëñ,ùëëùëó
ùëñ)‚Üêùëëùëñùë†ùë°ùëéùëõùëêùëí(ùë•ùëó
ùëñ,¬Øùë•ùëêùëó)‚àÄùë•ùëñ‚ààùëêùëó
23:end for
24:forùë•ùëñ‚ààùêµdo
25: Find correction term for ùõº
26:Œìùõºùëñ=1
1+ùëâùëêùëó/ùëõùëêùëó(1+ùëûùëó
ùëñ√ç
ùëßùëò‚ààùëêùëóùëûùëó
ùëòùëâùëêùëó)
27: Find correction term for ùõΩ‚àó
ùëñ
28:ŒìùõΩ‚àó
ùëñ=1
1+ùëâùëêùëó/ùëõùëêùëó(1+ùëëùëó
ùëñ√ç
ùëßùëò‚ààùëêùëóùëëùëó
ùëòùëâùëêùëó)
29: Final valuation = ùëâùëñ‚àó(Œìùõºùëñ+ŒìùõΩ‚àó
ùëñ‚àí1)
30:end for
a small subset, i.e. 200 samples from the original training data
and run the baseline methods TMC-Shapley (Data Shapley) and
distributional Shapley. 2000 samples are used for testing and hold-
out for Shapley calculation. We keep 10,000 samples which are
never seen by model or valuation method at any point, we call this
out-of-sample (OOS) set. The rest of the samples are used as data
distribution and exposed to Distributional Shapley, and our method
during the clustering step and ùõºcorrection step. We use Gauss-
ian Mixture Models (GMM) for clustering. Our proposed method
works for both in-distribution and OOS samples. As Data Shapley
only works for in-distribution samples, we compare our results
with Distribution Shapley for out-of-sample data. We use Gauss-
ian Mixture Model (GMM) clustering with default parameters of
Figure 1: Computation cost in terms of number of training
iterations required for the given dataset size. We compare
EcoVal with TMC Shapley (also known as Data Shapley), dis-
tributional Shapley, and a lighweight version of EcoVal. Our
method requires substantially lower number of training iter-
ations for data valuation.
scikit-learn‚Äôs implementation, covariance-type=‚Äôfull‚Äô, tol=0.001, reg-
covar=1e-06, max-iter=100, n-init=1, init-params=‚Äôkmeans‚Äô, and 30
mixtures/clusters.
5.1 Comparative Analysis of the Computational
Time
The Data Shapley approximation method TMC Shapley [ 18] con-
verges in approximately 3|ùêµ|(or3√óùëöin Eq. 2) Monte Carlo sam-
ples. Each Monte Carlo sample is a random permutation of the data
points in the training set. The marginal contribution of a data point
ùëßin a given permutation is obtained as the performance difference
between the model trained on data points before this datum, say ùëÜ,
and the model trained on ùëÜ‚à™{ùëß}. Each point is added sequentially
meaning|ùêµ|training runs are required in a single Monte Carlo
sample. This makes the number of training runs in the order of
ùëÇ(|ùêµ|2). Distributional Shapley‚Äôs [ 17] time complexity is similar
withùëáruns to get an unbiased estimate using different subsets ùëÜ
from the underlying data distribution. This makes the number of
training runs of Distributional Shapley ùëÇ(ùëá‚àó|ùêµ|2).
Our method performs clustering that takes less time than training
a machine learning or deep learning model in most real-world
scenarios. This is a one time effort, so the complexity is in the order
ofùëÇ(1). Estimating the value of each cluster requires ùëÇ(ùëù)training
runs. Apart from that, our method involves running Data Shapley
on a curated subset ùëùcontaining an equal number of points from
each cluster, this take ùëÇ(ùëù2)time. The size of this subset ùëùis much
smaller than|ùêµ|. The total number of training runs required is in the
order ofùëÇ(1)+ùëÇ(ùëù2)+ùëÇ(ùëù). We compare the computational cost
associated with the TMC Shapley/Data Shapley with our EcoVal
method. As illustrated in Figure 1, EcoVal requires significantly
fewer training iterations, approximately 103to105, compared to
107to109required by traditional TMC Shapley. Our method reduces
computational overhead by employing a curated subset approach,
which involves running Data Shapley on a subset ùëùcontaining
an equal number of points from each cluster. This curated subset
approach requires computational resources in the order of ùëÇ(ùëù2),
significantly less than the ùëÇ(|ùêµ|2)required by the standard TMC
 
2871EcoVal: An Efficient Data Valuation Framework for Machine Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 2: Accuracy difference with respect to the % of data
points added or removed. We add or remove the highest val-
ued data points first and then subsequently add or remove
the lesser value data, respectively. The top, middle and bot-
tom rows show the results for MNIST, CIFAR10, CIFAR100,
respectively with in-distribution valuation. The EcoVal gives
comparable or better performance when compared to Data
Shapley and Distribution Data Shapley.
Shapley method, where |ùêµ|represents the size of the full dataset.
The size of subset ùëùis much smaller than |ùêµ|, which explains our
method‚Äôs efficiency and scalability. With the increase in the dataset
size, the utility of our EcoVal becomes more evident. Our method
withoutùõºcorrection is even faster with negligible loss in valuation
quality.
5.2 Data Point Addition and Removal
Experiments
We evaluate the data valuation methods by running the data point
addition and removal experiments as proposed in [ 18]. For a given
model and dataset, the data points are added in the order of pre-
dicted value, i.e. from largest to lowest values, and the model is
retrained for each addition. Similarly, another experiment is con-
ducted where we remove samples with high values and observe
the performance drop. The impact of removal and addition of high
value data-points help us measure the effectiveness of data valua-
tion techniques. We compare our results with state-of-the-art Data
Shapley and Distributed Data Shapley valuation methods.
Removing most valued data points. We predict values of
data-points using each valuation method and we measure the drop
Figure 3: Data valuation on out-of-sample data (top left to
bottom right: CIFAR10, CIFAR100, MNIST, respectively). Our
EcoVal method outperforms Distributed Data Shapley and
Random Data Removal by getting steeper performance drop
with increasing % valuable data removal.
in performance of model by removing most-valued data-points for
each method. A better valuation method‚Äôs high value data-points
will result in a higher drop in performance. So, for removal of most
valued points, the method resulting in higher performance drop is
a better valuation method.
Adding most valued data points. This approach is vice-versa
of the previous approach, we add most valued data-points into
the training set and observe the increase in the performance. A
higher increase on adding the top data-points shows better valu-
ation method. Figure 2 shows the performance drop and increase
upon adding and removing most valued points, respectively. It can
be observed that EcoVal performance drop is slightly less than that
of Data Shapley but significantly higher than the Distributed Data
Shapley which is desired. Similar patterns can be observed in the
data addition graph also.
Removing most valuable data points from out-of-sample
set.The discussed earlier, EcoVal supports data valuation for out-
of-sample data as well which is supported only by Distributed Data
Shapley. Therefore, we compare the OOS valuation results between
them. Figure 3 shows the performance drop by removing the most
valuable points from an out-of-sample set of size 10,000. It can be
observed that EcoVal‚Äôs performance drop is very high as compared
to Distributed Data Shapley. The steep drop in the performance after
removing the most valuable data points implies better precision for
data valuation in our EcoVal framework.
5.3 Effect of the Adjustment Terms
We observe the effect of removing different adjustment terms ŒìùõΩ‚àó
ùëñ,
Œìùõºùëñor both in the EcoVal framework and show the results in Figure 4.
The overall EcoVal framework with the terms ùõºandùõΩperforms
the best, in general. Eliminating one of the adjustment terms dete-
riorates the quality of the valuation by a small margin. Removing
both corrections significantly impacts the quality of data valuation.
This is particularly visible in the initial phase of adding the most
 
2872KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ayush Tarun et al.
Figure 4: Effect of adding different adjustment terms
(refer Section 4.2). EcoVal: the full proposed method,
EcoVal_no_alpha: removal of ùõºadjustment term,
EcoVal_no_beta: removal of ùõΩadjustment term, Eco-
Val_no_adjustment: EcoVal without any adjustment terms,
i.e. the mean of the cluster value used as the data value.
significant data points. It should be noted that eliminating Œìùõºùëñonly
affects the valuation quality marginally, but completely removes
the need for model training, giving an even more efficient version
of our valuation method.
The performance of different variations of our method (EcoVal,
EcoVal-no-ùõº, EcoVal-no- ùõΩ, EcoVal-no-adj) would vary depending
on the intra-class and inter-class variations present in a dataset. In
Figure 4, the performance differences are not sometimes consistent.
We report our observation based on the experiments with CIFAR-10
dataset. More complex datasets with larger variations may better
reveal the impact of these adjustment factors, which is the future
scope of this work.
5.4 Mutual Influence of Clustering Methods and
Adjustment Terms
The effectiveness of EcoVal is intrinsically related to the success
of the clustering method, as the initial valuation is carried out at
the cluster level. The adjustment terms are meant to marginally
correct the data value distribution within each cluster. The simi-
larity within a single cluster compared to the similarity between
clusters significantly influences the structure and application of the
adjustment factors. We assume that data points within the same
cluster have a higher degree of similarity compared to points across
different clusters, a standard assumption in clustering algorithms
such as Gaussian mixture models (GMM). However, the variance
in the degree of similarity across different clusters (inter-cluster
dissimilarity) justifies the need for a scaling factor such as Œìùõºùëñ, intro-
duced in Equation 15. This accounts for the relative contribution of
an entire cluster to the model‚Äôs performance, acknowledging that
some clusters may be more pivotal due to their positioning, density,
or the nature of the data points they contain. This process depends
on the intra-cluster variation present in a dataset. For some dataset,
a simple distribution of the value of the cluster without correction
factors can also give a good valuation, leaving a minimal scope of
correction.
6 CONCLUSION
This work presents a focused study on improving the speed of data
valuation in machine learning models. We develop an efficient data
valuation method that is significantly fast and practical for workingwith large datasets. Our method works for both in-distribution and
out-of-sample data. The proposed EcoVal data valuation framework
shows comparable and sometimes even better results than the ex-
isting approaches for in-distribution data. For out-of-sample data
points, our method significantly outperforms competing methods,
thus establishing a new state-of-the-art. This proves our method‚Äôs
utility in a data market, where new data points analogous to our out-
of-sample set are generated every passing instant. Our valuation
also shows negligible error margin with the vanilla Shapley value
approximation. The points mentioned above collectively make the
proposed method a robust and scalable approach to estimate the
value of data across a variety of machine learning models.
ACKNOWLEDGMENT
This research/project is supported by the National Research Foun-
dation, Singapore under its Strategic Capability Research Centres
Funding Initiative. Any opinions, findings and conclusions or rec-
ommendations expressed in this material are those of the author(s)
and do not reflect the views of National Research Foundation, Sin-
gapore.
This research is also supported by the Department of Science and
Technology-Science and Engineering Research Board (DST-SERB),
India project under Grant SRG/2023/001686.
REFERENCES
[1]Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. 2019. A marketplace for data:
An algorithmic solution. In Proceedings of the 2019 ACM Conference on Economics
and Computation. 701‚Äì726.
[2]Grant Allan, Michelle Gilmartin, Peter McGregor, Karen Turner, and J Kim Swales.
2009. Economics of energy efficiency. In International Handbook on the Economics
of Energy. Edward Elgar Publishing.
[3]Kenneth J Arrow, Hollis B Chenery, Bagicha S Minhas, and Robert M Solow. 1961.
Capital-labor substitution and economic efficiency. The review of Economics and
Statistics (1961), 225‚Äì250.
[4]Robert J Aumann and Lloyd S Shapley. 2015. Values of non-atomic games. Prince-
ton University Press.
[5] Robert J Barro. 1997. Macroeconomics. MIT Press.
[6]Eric Bax. 2019. Computing a data dividend. arXiv preprint arXiv:1905.01805
(2019).
[7]David E Bloom, David Canning, and Jaypee Sevilla. 2004. The effect of health
on economic growth: a production function approach. World development 32, 1
(2004), 1‚Äì13.
[8]Lawrence Blume, Steven Durlauf, and Lawrence E Blume. 2008. The new Palgrave
dictionary of economics. Palgram Macmillan.
[9]Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. 2018. L-
Shapley and C-Shapley: Efficient Model Interpretation for Structured Data. In
International Conference on Learning Representations.
[10] CW COBB. 1928. A Theory of Production. American Economic Review 18 (1928),
139‚Äì165.
[11] Shay Cohen, Eytan Ruppin, and Gideon Dror. 2005. Feature selection based on
the Shapley value. In Proceedings of the 19th international joint conference on
Artificial intelligence. 665‚Äì670.
[12] Ian Covert and Su-In Lee. 2021. Improving kernelshap: Practical shapley value
estimation using linear regression. In International Conference on Artificial Intel-
ligence and Statistics. PMLR, 3457‚Äì3465.
[13] Ian Covert, Scott Lundberg, and Su-In Lee. 2021. Explaining by removing: A
unified framework for model explanation. Journal of Machine Learning Research
22, 209 (2021), 1‚Äì90.
[14] Pradeep Dubey, Abraham Neyman, and Robert James Weber. 1981. Value theory
without efficiency. Mathematics of Operations Research 6, 1 (1981), 122‚Äì128.
[15] Maryam Farboodi and Laura Veldkamp. 2021. A model of the data economy.
Technical Report. National Bureau of Economic Research.
[16] Raul Castro Fernandez, Pranav Subramaniam, and Michael J Franklin. [n. d.].
Data Market Platforms: Trading Data Assets to Solve Data Problems. Proceedings
of the VLDB Endowment 13, 11 ([n. d.]).
[17] Amirata Ghorbani, Michael Kim, and James Zou. 2020. A distributional framework
for data valuation. In International Conference on Machine Learning. PMLR, 3535‚Äì
3544.
 
2873EcoVal: An Efficient Data Valuation Framework for Machine Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[18] Amirata Ghorbani and James Zou. 2019. Data shapley: Equitable valuation of
data for machine learning. In International conference on machine learning. PMLR,
2242‚Äì2251.
[19] Amirata Ghorbani and James Y Zou. 2020. Neuron shapley: Discovering the
responsible neurons. Advances in neural information processing systems 33 (2020),
5922‚Äì5932.
[20] Michel Grabisch and Marc Roubens. 1999. An axiomatic approach to the concept
of interaction among players in cooperative games. International Journal of game
theory 28 (1999), 547‚Äì565.
[21] Faruk Gul. 1989. Bargaining foundations of Shapley value. Econometrica: Journal
of the Econometric Society (1989), 81‚Äì95.
[22] Eric A Hanushek. 2020. Education production functions. In The economics of
education. Elsevier, 161‚Äì170.
[23] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Alek-
sander Madry. 2022. Datamodels: Understanding predictions with data and
data with predictions. In International Conference on Machine Learning. PMLR,
9525‚Äì9587.
[24] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel,
Bo Li4 Ce Zhang, and Costas Spanos1 Dawn Song. [n. d.]. Efficient Task-Specific
Data Valuation for Nearest Neighbor Algorithms. Proceedings of the VLDB En-
dowment 12, 11 ([n. d.]).
[25] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve
G√ºrel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. 2019. Towards efficient
data valuation based on the shapley value. In The 22nd International Conference
on Artificial Intelligence and Statistics. PMLR, 1167‚Äì1176.
[26] Charles I Jones and Christopher Tonetti. 2020. Nonrivalry and the Economics of
Data. American Economic Review 110, 9 (2020), 2819‚Äì2858.
[27] Ehud Kalai and Dov Samet. 1987. On weighted Shapley values. International
journal of game theory 16 (1987), 205‚Äì222.
[28] Bojan Karla≈°, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu,
and Ce Zhang. 2022. Data debugging with shapley importance over end-to-end
machine learning pipelines. arXiv preprint arXiv:2204.11131 (2022).
[29] Eda Kemahlƒ±oƒülu-Ziya and John J Bartholdi III. 2011. Centralizing inventory in
supply chains by using Shapley value to allocate the profits. Manufacturing &
Service Operations Management 13, 2 (2011), 146‚Äì162.
[30] Guennadi A Khatskevich and Andrei F Pranevich. 2018. Production functions
with given elasticities of output and production. (2018).
[31] Yongchan Kwon, Manuel A Rivas, and James Zou. 2021. Efficient computation and
analysis of distributional shapley values. In International Conference on Artificial
Intelligence and Statistics. PMLR, 793‚Äì801.
[32] Yongchan Kwon and James Zou. 2022. Beta Shapley: a Unified and Noise-reduced
Data Valuation Framework for Machine Learning. In International Conference on
Artificial Intelligence and Statistics. PMLR, 8780‚Äì8802.
[33] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
2019. Quantifying the Carbon Emissions of Machine Learning. arXiv preprint
arXiv:1910.09700 (2019).
[34] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).[35] Sudhanshu K Mishra. 2007. A brief history of production functions. Available at
SSRN 1020577 (2007).
[36] Herv√© Moulin. 1992. An application of the Shapley value to fair division with
money. Econometrica: Journal of the Econometric Society (1992), 1331‚Äì1349.
[37] Roger B Myerson. 1977. Graphs and cooperation in games. Mathematics of
operations research 2, 3 (1977), 225‚Äì229.
[38] Ki Nohyun, Hoyong Choi, and Hye Won Chung. 2022. Data valuation with-
out training of a model. In The Eleventh International Conference on Learning
Representations.
[39] Jian Pei. 2020. Data Pricing ‚Äì From Economics to Data Science. Association for
Computing Machinery.
[40] Alvin E Roth and Robert E Verrecchia. 1979. The Shapley value as applied to cost
allocation: a reinterpretation. Journal of Accounting Research (1979), 295‚Äì303.
[41] LS Shapley. 1953. A Value for n-Person Games. In Contributions to the Theory of
Games (AM-28), Volume II. Princeton University Press, 307‚Äì318.
[42] Lloyd S Shapley et al. 1953. A value for n-person games. (1953).
[43] Ronald William Shephard. 2015. Theory of cost and production functions. Princeton
University Press.
[44] Ronald W Shephard and Rolf F√§re. 1974. The law of diminishing returns. In
Production Theory: Proceedings of an International Seminar Held at the University
at Karlsruhe May‚ÄìJuly 1973. Springer, 287‚Äì318.
[45] Robin C Sickles and Valentin Zelenyuk. 2019. Measurement of productivity and
efficiency. Cambridge University Press.
[46] Raghav Singal, Omar Besbes, Antoine Desir, Vineet Goyal, and Garud Iyengar.
2019. Shapley meets uniform: An axiomatic framework for attribution in online
advertising. In The World Wide Web Conference. 1713‚Äì1723.
[47] Mukund Sundararajan and Amir Najmi. 2020. The many Shapley values for model
explanation. In International conference on machine learning. PMLR, 9269‚Äì9278.
[48] Siyi Tang, Amirata Ghorbani, Rikiya Yamashita, Sameer Rehman, Jared A Dunn-
mon, James Zou, and Daniel L Rubin. 2021. Data valuation for medical imaging
using Shapley value and application to a large-scale chest X-ray dataset. Scientific
reports 11, 1 (2021), 8366.
[49] Tarun Wadhwa. 2020. Economic impact and feasibility of data dividends.
[50] Jiachen T Wang and Ruoxi Jia. 2023. Data banzhaf: A robust data valuation frame-
work for machine learning. In International Conference on Artificial Intelligence
and Statistics. PMLR, 6388‚Äì6421.
[51] Jiachen T. Wang and Ruoxi Jia. 2023. Data Banzhaf: A Robust Data Valuation
Framework for Machine Learning. In Proceedings of The 26th International Con-
ference on Artificial Intelligence and Statistics (Proceedings of Machine Learning
Research, Vol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent
(Eds.). PMLR, 6388‚Äì6421.
[52] Rui Wang, Xiaoqian Wang, and David I Inouye. 2020. Shapley Explanation
Networks. In International Conference on Learning Representations.
[53] Mohammad Zaeri-Amirani, Fatemeh Afghah, and Sajad Mousavi. 2018. A feature
selection method based on shapley value to false alarm reduction in icus a genetic-
algorithm approach. In 2018 40th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC). IEEE, 319‚Äì323.
 
2874KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Ayush Tarun et al.
Appendix
A PROOFS
A.1 Theorem 3.4
Let equal number of samples are used from each cluster to run
TMC Shapley. Then the intrinsic value of a datum is independent of
proportion and bias in the data distribution. If ùëõùë†such samples exist,
the value is divided into these ùëõùë†samples. From Shapely axioms, the
data Shapley at the current stage of TMC becomes approximately
ùõºùëñ
ùëõùë†.
For rest of the samples in the TMC, we train a regression model
ùëÖfor predictingùõºùëñ
ùëõùë†for an input data. If the predicted Shapley for
anyùëßùëñ‚ààùëêisùëÑùëñ, then assuming no error is introduced due to the
TMC Shapely algorithm, this gives us
ùëÑùëñ=ùõºùëñ
ùëõùë†+ùõøùëÖùëñ, (27)
whereùõøùëÖùëñis error introduced by the regression model. ùëÑùëñdenotes
ùëè‚àóùõºùëñ, whereùëèis some constant. We use this to obtain the adjustment
factor forùõºùëñin Eq. 14. Assuming ùëâùëñandùëëùëñdo not have any error,
we take the differentiation of Eq. 21 with respect to ùëß
ùúïŒ¶ùëñ
ùúïùëß=1
ùúïùëß(ùëâùëñ
1+ùëâùëê/ùëõùëê)"
ùëõùëêùúïùëÑùëñ√ç
ùëßùëó‚ààùëêùëÑùëó‚àíùëõùëêùëÑùëñùúï(√ç
ùëßùëó‚ààùëêùëÑùëó)
(√ç
ùëßùëó‚ààùëêùëÑùëó)2#
.(28)
Intuition.ùëâùëñdoes not have any error as this is the difference
between the performance with and without the cluster ùëêdivided
by a constant. Both the values can be directly computed from the
model. Similarly, ùëëùëñis the distance of the datum from the centroid
of the cluster ùëêwhich can be calculated without any error.
Comparing Eq. 28 with the change in the Shapley value leads to
the following inequality.
ŒîŒ¶ùëñ‚â§ùëâùëê/ùëõùëê
1+ùëâùëê/ùëõùëê"
ùõøùëÖùëõùëê√ç
ùëßùëó‚ààùëêùëÑùëó+ùëõùëêùëÑùëñ(ùëõùëêùõøùëÖ)
(√ç
ùëßùëó‚ààùëêùëÑùëó)2#
,
whereùõøùëÖ=max
ùëñùõøùëÖùëñis the maximum error of the regression model
ŒîŒ¶ùëñ‚â§ùëâùëê"
ùõøùëÖ√ç
ùëßùëó‚ààùëêùëÑùëó+ùëõùëêùëÑùëñùõøùëÖ
(√ç
ùëßùëó‚ààùëêùëÑùëó)2#
,
asùëâùëê‚â•0,ùëõùëê‚â•1therefore,1
1+ùëâùëê/ùëõùëê‚â§1. From Eq. 24 and Eq. 30
ŒîŒ¶ùëñ‚â§(ùëõùëê¬ØŒ¶ùëê+ùëõùëêùúñ/2)"
ùõøùëÖ√ç
ùëßùëó‚ààùëêùëÑùëó+ùëõùëêùëÑùëñùõøùëÖ
(√ç
ùëßùëó‚ààùëêùëÑùëó)2#
.Ignoring factors with multiples of ùúñandùõøas these values are very
small. We get the final difference between the original Shapley
value and our proposed approximated Value as below
ŒîŒ¶ùëñ‚âàùëõùëê¬ØŒ¶ùëêùõøùëÖ√ç
ùëßùëó‚ààùëêùëÑùëó+ùëõ2ùëê¬ØŒ¶ùëêùëÑùëñùõøùëÖ
(√ç
ùëßùëó‚ààùëêùëÑùëó)2, (29)
¬ØŒ¶ùëêandùëÑùëñcannot be arbitrarily large as they are the average
Shapely value for a cluster and change in performance due to a
data pointùëßùëñ. With increasing cluster size ùëõùëê, the corresponding
predicted value√ç
ùëßùëó‚ààùëêùëÑùëówill increase. Thus, ùëõùëê/√ç
ùëßùëó‚ààùëêùëÑùëócan not
be very large. The ùõøùëÖerror will be low for a moderately good
regression model. Thus, our method estimates the Shapely value
Œ¶ùëñwith negligible error.
A.2 Corollary 4.3.1
The valueùëâùëêof the a cluster by Shapley axioms is defined as
ùëâùëê=‚àëÔ∏Å
ùëßùëó‚ààùëêŒ¶(ùëßùëó)=ùëõùëê¬ØŒ¶ùëê+‚àëÔ∏Å
ùõø(ùëßùëó). (30)
The sum of the Shapley values within a cluster equalling the
leave-cluster-out value of the cluster follows logically from the
axioms of the Shapley value, in particular, additivity and efficiency.
Additivity: If we treat each data point as contributing a separate
game to the performance, the total Shapley value of a cluster should
naturally be the sum of the Shapley values of each data point within
the cluster.
Efficiency: This axiom ensures that the total value generated by
the coalition is fully distributed among the players. If the Shapley
value calculation respects this axiom, then the allocation to a cluster
should match the cumulative contribution of its members.
We can consider a simple proof by induction on the number of
data points in the cluster:
1. Base Case: If a cluster has only one data point, then the Shapley
value of the cluster is clearly equal to the Shapley value of the single
data point.
2. Inductive Step: Assume the proposition holds for all clusters
of sizeùëò. For a cluster of size ùëò+1, if you remove one data point,
by the induction hypothesis, the sum of the Shapley values of the
remainingùëòdata points equals the value of these ùëòdata points.
Adding theùëò+1-st point, by the additivity axiom, the overall Shapley
value would be the sum of the individual Shapley values.
 
2875