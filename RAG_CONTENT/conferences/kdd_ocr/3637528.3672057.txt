AutoXPCR: Automated Multi-Objective Model Selection
for Time Series Forecasting
Raphael Fischer
Amal Saadallah
raphael.fischer@tu-dortmund.de
amal.saadallah@cs.tu-dortmund.de
Lamarr Institute for Machine Learning and Artificial Intelligence
TU Dortmund University
44227 Dortmund, Germany
DNN model poolForecasting data sets
Database of index-scaled
properties exhibited by DNNs
when deployed on data sets
User-controllable PCR prioritiesMeta-features of
unknown data set
AutoXPCR selection
Individual meta-learners per
property, trained on the database
DNN recommendation with
complete PCR estimations
and explanations
Figure 1: Framework for AutoXPCR, which leverages multi-objective meta-learning to estimate DNN performance.
Abstract
Automated machine learning (AutoML) streamlines the creation of
ML models, but few specialized methods have approached the chal-
lenging domain of time series forecasting. Deep neural networks
(DNNs) often deliver state-of-the-art predictive performance for
forecasting data, however these models are also criticized for being
computationally intensive black boxes. As a result, when searching
for the “best” model, it is crucial to also acknowledge other aspects,
such as interpretability and resource consumption. In this paper, we
propose AutoXPCR – a novel method that produces DNNs for fore-
casting under consideration of multiple objectives in an automated
and explainable fashion. Our approach leverages meta-learning
to estimate any model’s performance along PCR criteria, which
encompass (P)redictive error, (C)omplexity, and (R)esource demand.
Explainability is addressed on multiple levels, as AutoXPCR pro-
vides by-product explanations of recommendations and allows to
interactively control the desired PCR criteria importance and trade-
offs. We demonstrate the practical feasibility AutoXPCR across 108
forecasting data sets from various domains. Notably, our method
outperforms competing AutoML approaches – on average, it only
requires 20% of computation costs for recommending highly effi-
cient models with 85% of the empirical best quality.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672057CCS Concepts
•Computing methodologies →Learning paradigms; Neural
networks ;•Social and professional topics →Sustainability ;•
Information systems →Temporal data .
Keywords
AutoML; Meta-learning; Time series forecasting; Explainable AI;
Resource-Aware ML
ACM Reference Format:
Raphael Fischer and Amal Saadallah. 2024. AutoXPCR: Automated Multi-
Objective Model Selection for Time Series Forecasting. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3637528.3672057
1 Introduction
The rapidly evolving field of machine learning (ML) brought forth
a broad arsenal of methods, which come with their own respective
benefits and limitations. As there is “no free lunch” [ 46] for building
ML solutions, practitioners are required to evaluate the pros and
cons of applicable methods for their specific learning task and
data set. Time series forecasting is one example of such a task
and is considered particularly challenging due to the complex and
time-evolving nature of data [ 36]. As in other domains of learning,
deep neural networks (DNNs) dominate the current state-of-the-art
(SOTA) in forecasting [ 2,15]. While achieving stunning results, the
question remains how to choose among the wide range of well-
performing models for novel data.
 
806
KDD ’24, August 25–29, 2024, Barcelona, Spain Raphael Fischer and Amal Saadallah
Automated ML (AutoML) offers a multitude of tools to obtain
promising models with less manual effort [ 18]. However, we ar-
gue that the SOTA in AutoML suffers from several phenomena.
Firstly, general-purpose solutions do not perform well on time se-
ries, and specialized methods are scarce [ 3]. In addition, AutoML is
computationally demanding [ 41] and overly focuses on predictive
performance. Lastly, the acknowledged significance of explainabil-
ity [32,34] is hardly considered by established approaches. The two
latter aspects are especially important in the context of automat-
ing the search for DNNs, whose complex structures make them
resource-hungry [ 40] and non-transparent – resulting in them be-
ing often referred to as black boxes [ 27]. For this reason, we argue
that choosing the “best” model for a specific task should not solely
focus on predictive quality, but instead needs to be understood as
amulti-objective optimization problem. This way, different aspects
of performance and how they trade against each other [ 11] can be
considered when searching models for given data.
To overcome the aforementioned challenges, our work proposes
a novel AutoML method called AutoXPCR. Its framework is schemat-
ically displayed in Figure 1 and allows to obtain forecasting models
in an automated, explainable, and resource-aware fashion. Our
method leverages multi-objective meta-learning, considers time
series characteristics as features, and recommends renowned DNNs
based on their suitability in terms of (P)rediction error, (C)omplexity,
and(R)esource consumption. For that, we first estimate every model’s
performance along these objectives via individual meta-learners
and then recommend promising candidates. In addition, AutoX-
PCR is explainable on multiple levels: (1) it accompanies every
recommendation with explanations based on scoring along all PCR
criteria, (2) it utilizes only fully interpretable meta-learners [ 33],
which deliver by-product feature importance information, and (3)
it allows for interactively controlling the selection process. The last
aspect enables users of our framework to align the importance of
each PCR criterion for the overall model performance scoring with
their priorities, allowing them to favor resource-friendly or more
interpretable DNNs. To validate our method, we also present results
from an extensive experimental evaluation, which showcases the
effectiveness of AutoXPCR. We evaluated 11 SOTA DNNs across
108 data sets and tested our meta-learning approach on the results
from these configurations (1188 in total). AutoXPCR clearly outper-
forms other AutoML approaches [ 1,10,22,39] by producing highly
efficient models at fractions of the computational expenses required
by the competitors. Even compared to an exhaustive search, our
method achieves∼85% of predictive performance at only 20% of
the computation cost. We make our complete implementation and
logs publicly available at github.com/raphischer/xpcr and invite
others to utilize them for advancing the fields of meta-learning,
AutoML, and forecasting. We firmly believe that our work is a valu-
able contribution to these communities and helps in making ML
more explainable and resource-aware.
2 Related Work
Before explaining the intricacies of our approach, we discuss the
literary background our work builds on, starting with deep learn-
ing for forecasting. Besides offering a multitude of data sets across
all domains, the Monash forecasting repository [ 15] gives a goodoverview of the current SOTA. It shows that global modeling and
deep learning, in many cases, outperform classical univariate mod-
els. Highly popular are recurrent (RNNs) and, in particular, long
short-term memory (LSTM) neural networks like DeepAR [ 37].
Convolutional networks (CNNs) also score well in forecasting and
tend to be more efficient than RNNs [ 5]. GluonTS [ 2] offers a broad
range of DNN models for usage on time series. DeepAR employs an
RNN with LSTM or Gated Recurrent Units [ 37], while DeepState
learns RNN weights jointly across all time series via a Kalman filter
[31]. MQ-RNN and MQ-CNN leverage RNN and dilated causal CNN
encoders, respectively, coupled with a quantile decoder, allowing
sequence-to-sequence prediction [ 45]. DeepFactor also estimates
weights across series and combines local and global aspects [ 44].
GluonTS also offers transformer architectures [ 24,42], which be-
came especially popular in language processing [ 40]. The growing
complexity of forecasting models also brings forth works that criti-
cally discuss the aspect of explainability [21, 32, 34, 35].
When deploying ML on new data, practitioners need to navigate
their way through the vast space of available models and their
respective trade-offs. AutoML comes to aid by streamlining the
selection of models, neural architectures, and other hyperparame-
ters [ 18,47]. Instead of testing single configurations via sampling
(e.g., in the form of a grid search), meta-learning is frequently used
to speed up the process. In practice, this means that additional
estimators are learned, which for example predict a candidate’s
expected error [ 26] or good hyperparameters [ 43]. The data used
for meta-learning can be static information about the task and
data or empirical results from previously performed experiments
[8,14]. For most established, general-purpose ML tool kits, Au-
toML extensions are readily available, including AutoKeras [ 22],
AutoGluon [ 9], Auto-PyTorch [ 49], and Auto-Sklearn [ 10]. Unfor-
tunately, only few specialized AutoML solutions are available for
the task of forecasting [ 3,25]. For some of the general frameworks,
extensions for handling time-evolving data were published (e.g.,
AutoGluonTS [ 39]). Genetic algorithms were successfully used to
automatically assemble a well-performing forecasting pipeline [ 7].
However, the validity of this approach is questionable, as there is no
public code and experiments were only conducted on twelve time
series. Several other works have proposed methods for selecting
among pre-trained models (i.e., ensemble pruning) [ 35,36], however
we are interested in automating the training itself. The AutoFore-
cast approach by Abdallah et al. also deploys meta-learning in order
to estimate model performances [ 1], and as such, is closely related
to our work. However, as many other AutoML approaches, they
assess and meta-learn model performance only in terms of pre-
dictive error, which is highly problematic considering the call for
“greener” solutions [ 41]. SOTA models have been shown to not only
become ever more accurate but also have an ever greater impact
on our environment [ 29,38,40]. The increasing model (and espe-
cially DNN) size also raises questions of trust [ 6] and fuels the field
of explainability [ 17,27], which aims at making complex models
more interpretable [ 33]. The recently approved EU AI act is the
ultimate manifest that ML systems need to be designed transparent
and environmentally friendly [ 16], but unfortunately, these aspects
(and how they trade against each other [ 11]) are still given insuffi-
cient consideration in ML research [ 12], including the advances in
AutoML and forecasting.
 
807AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
3 Explainable Multi-Objective Model Selection
Our method addresses the flaws of the current SOTA by selecting
forecasting models in an automated, explainable, and resource-
aware fashion. In the following, we formalize this problem and
propose a possible solution called AutoXPCR. While our method-
ology is specifically formulated and evaluated in the context of
forecasting, we deem it generalizable to other ML tasks. Also recall
Figure 1, which functions as a useful guide through our methodol-
ogy.
3.1 Problem Statement
Let𝑌={𝑦1,𝑦2,···,𝑦𝑡}be a time series, i.e., a temporally ordered
sequence of recorded values 𝑦𝑖. Univariate forecasting at a specific
point in time 𝑡corresponds to predicting the next ℎvalues based
on the𝑐last observed values with an ML model 𝑚, i.e.,
𝑌ℎ=𝑚(𝑌𝑐), with context 𝑌𝑐={𝑦𝑡−𝑐,···,𝑦𝑡−1,𝑦𝑡},
horizon𝑌ℎ={𝑦𝑡+1,𝑦𝑡+2···𝑦𝑡+ℎ}, and
𝑐≥1,ℎ≥1(1)
Learning any forecasting model 𝑚requires to assemble training
data with observed horizons, for example, by sampling from earlier
parts of the series. Usually, time series are recorded in a wider
context Y, which encloses simultaneous time variables. For example,
consider weather data, where a single series 𝑌∈Ymight represent
temperature, wind speed, or humidity recordings. In practice, this
allows for training so-called global forecasting models [ 15], which
are trained on samples from multiple series.
Following the aforementioned “no free lunch” theorem [ 46],
automating the search for a model𝑚which performs best on given
data is not straightforward. We formalize it in form of the following
multi-objective optimization problem [48]:
argmax
𝑚∈𝑀(𝐹(Y,𝑚))=argmax
𝑚∈𝑀(𝑘∑︁
𝑖=1𝑤𝑖𝑓𝑖(Y,𝑚)),
with∀𝑖:𝑤𝑖≥0and𝑘∑︁
𝑖=1𝑤𝑖=1(2)
The multi-objectiveness allows for considering multiple perfor-
mance aspects, which are denoted by the weighted functions 𝑓𝑖. As
they can be partitioned into groups describing either (P)rediction
error, (C)omplexity, or (R)esource consumption, we name the 𝑓𝑖
PCR functions. For now, they are just abstract representations of the
practical properties that a given model 𝑚from the space of candi-
dates𝑀exhibits when forecasting Y. To give some examples, one
could assess the empirical prediction error (P), number of model
parameters (C), or power draw during inference (R). Note that C
and R properties are directly linked to aspects of explainability and
resource-awareness, which are usually sacrificed for lower predic-
tive errors. The unifying PCR score𝐹(Y,𝑚)allows to assess any
model’s overall performance under consideration of all aspects.
The weights 𝑤𝑖control each function’s impact for this assessment
– they can be chosen to either balance the PCR groups or explicitly
favor accurate, resource-friendly, or explainable models.
Note that solving Equation 2 efficiently is non-trivial: Firstly, cer-
tain functions 𝑓𝑖are expected to behave in a contradictory manner,
which complicates simultaneous optimization – to give an example,it is hard to maintain low prediction errors when making models
less complex. As a result, there might not even be a single solution
to our problem but rather a range of Pareto-optimal choices [ 11].
Due to the high configurability of ML solutions, 𝑀is generally
infinite and AutoML approaches can only test single samples (i.e.,
models) from it. Model selection approaches (like AutoForecast [ 1]
and our method) instead choose models from a small, pre-defined,
and finite set of options 𝑀. In this case, Equation 2 can be naively
solved by performing an exhaustive search, i.e., assessing the PCR
properties of all options 𝑚∈𝑀. While guaranteed to provide opti-
mal solutions, this requires training and evaluating each model at
redundant computational expenses. Using classic optimization to
navigate the Pareto front of model choices [ 48] is infeasible, as the
PCR functions 𝑓𝑖are non-differentiable – they can only be evalu-
ated by practically testing models 𝑚onY. Actually, even testing
just a single model to assess its properties unveils a major problem,
which we first need to tackle.
3.2 Comparability of Properties
The meaning of properties and numeric values are vastly different
– we could, for example, register hundreds of milliseconds for run-
ning time, dozens of kilowatt-hours for power draw, or millions
of parameters. This problem gets more evident when considering
different hardware or software environments in which models are
evaluated (e.g., CPU or GPU implementations), as this choice will
dramatically change the value magnitudes. We address this issue
by assessing values of 𝑓𝑖on a relative index scale [11], based on the
real measurements 𝜇𝑖obtained from forecasting Ywith𝑚. Whereas
Fischer et al. calculate index values based on reference models,
we instead resort to putting values in relation to the empirically
best-performing model 𝑚∗
𝑖on the𝑖-th property:
𝑓𝑖(Y,𝑚)=𝜇∗
𝑖(Y)
𝜇𝑖(Y,𝑚)
= min𝑚∗
𝑖∈𝑀(𝜇𝑖(Y,𝑚∗
𝑖))
𝜇𝑖(Y,𝑚)!
, (3)
We argue that using the best value per property as a reference
is advantageous to global reference models [ 11] since it neatly
solves the problem of choosing the references. In addition, the now
calculable function values 𝑓𝑖and PCR score 𝐹become bounded
by the unit interval (0,1], making them more easily comparable.
They describe the behavior in a given environment relatively; the
higher the value, the closer it is to the best empirical result, which
receives𝑓𝑖(Y,𝑚∗
𝑖)=1. Note how this nicely aligns with Equation
2, which expects individual PCR functions to be defined such that
maximization leads to improved model behavior.
3.3 Model Selection using Meta-learning
To solve Equation 2 more efficiently, we propose AutoXPCR, which
internally utilizes meta-learning. The central idea is that for given
characteristics of the queried data set Yand model option 𝑚, the
values of𝑓𝑖areestimated, such that the model does not necessarily
needs to be evaluated at high computational expenses. To be concise,
we understand the meta-task as training regression models ˆ𝑓𝑖, which
estimate the PCR function values ˆ𝑓𝑖(𝑋Y,𝑚)of model𝑚from meta-
features for time series ( 𝑋Y). These features 𝑋Y∈FY,FY⊂R𝑛
for example might encode information like statistical numbers,
seasonality, or stationarity, details about the meta-features used in
 
808KDD ’24, August 25–29, 2024, Barcelona, Spain Raphael Fischer and Amal Saadallah
our experiments are provided in Section 4. Accordingly, the meta-
learners ˆ𝑓𝑖∈F map from the feature spaces onto real values, i.e.,
F:FY×𝑀→R.
Training the regressors requires collecting a property database
𝐷of meta-features and real function values across different data
sets, i.e.,𝐷={(𝑋Y,𝑓𝑖(Y,𝑚))}. Cross-validation across the data-
base allows for identifying the optimal regression method for each
individual property. This decision could for example be based on
the empirical estimation error (Further quality indicators for meta-
learning are given in Figure 8):
min
ˆ𝑓𝑖∑︁
(𝑋Y,𝑓𝑖(Y,𝑚))∈𝐷|𝑓𝑖(Y,𝑚)−ˆ𝑓𝑖(𝑋Y,𝑚)| (4)
.
Given trained meta-learners, property weights, and meta-fea-
tures of a specific data set, AutoXPCR can now automatically rec-
ommend models by giving an estimated solution for Equation 2. For
this, the𝑓𝑖are replaced with the meta-learner output ˆ𝑓𝑖and these
evaluations are run for the meta-features of all model choices 𝑚∈𝑀
(instead of redundantly applying them to Yat high computational
costs). The resulting estimated PCR score ˆ𝐹(𝑋Y,𝑚)indicates each
model’s performance under consideration of all PCR aspects, hence
we call our method PCR-aware (which entails resource-awareness).
As an alternative approach, one could also directly meta-learn 𝐹.
However, this would not allow for estimating or weighting the
individual PCR properties, which makes the method less transpar-
ent and interactive. We later show empirical evidence that direct
estimation - as expected - does not outperform the compositional
recommendation, where the outputs of individually trained meta-
regressors are aggregated.
3.4 Explainability Aspects
In addition to being PCR-aware, we argue that AutoXPCR is ex-
plainable on multiple levels, which address questions Q0 - Q2 that
users of our framework might have. Firstly, they might ask them-
selves how would the model 𝑚perform on a time series Yin terms
of property𝑓𝑖?(Q0). Our method is specifically designed to answer
this, as it bases any recommendation on the PCR function estimates
ˆ𝑓𝑖. Thanks to the relative index scaling introduced in Section 3.2,
this information is highly comprehensible in itself – as an example,
a score of 0.4implies that this model will likely exhibit this property
at40%of the best empirical result observed in 𝐷. These estimates
can be transformed into explanations to the question as to which
extent the properties impact the PCR score estimation (Q1):
𝐸1(𝑚,Y)=(
𝑓𝑖:𝜄(𝑓𝑖,𝑚,Y)Í
𝑓𝑗𝜄(𝑓𝑗,𝑚,Y))
with𝜄(𝑓𝑖,𝑚,Y)=𝑤𝑖ˆ𝑓𝑖(𝑋Y,𝑚)
ˆ𝐹(𝑋Y,𝑚)
(5)
To be concise, for a given PCR aspect, 𝜄(𝑓𝑖,𝑚,Y)describes its im-
portance, and the explanation 𝐸1(𝑚,Y)contains the normalized
importance of all properties for the estimated PCR score.
For delivering further insights, we restrict AutoXPCR to only
utilize fully interpretable [ 33] ML methods equipped with feature
importance capabilities. As a result, for each PCR criteria, the re-
spective meta-learner ˆ𝑓𝑖also informs on which meta-features 𝑥𝑗mostly impact the estimation (Q2):
𝐸2(ˆ𝑓𝑖,𝑚,Y)=
𝑥𝑗:𝛽(𝑥𝑗,𝑋Y,𝑚)Í
𝑥𝑘𝛽(𝑥𝑘,𝑋Y,𝑚)
(6)
For simplicity, we here use 𝛽as a generalized function for the ab-
solute, non-negative feature importance. In practice, this requires
slightly different implementations depending on the type of meta-
learner method – as examples, 𝛽can be computed from Gini impor-
tance (trees) or feature coefficients (linear regressors). Informing
on property-allied importance helps users to understand the con-
nection between AutoXPCR recommendations and certain data or
model characteristics. While feature importance in the first place
gives global explanations, they can also be used to generate local
explanations [17] (i.e., for a specific meta-learner prediction).
Lastly, interactions have been shown to improve trust and ex-
plainability [ 4], which also applies to our framework. By interac-
tively weighting the individual properties, the selection process
becomes controllable and transparent. As an additional benefit, it
enables users to specifically prioritize less complex, or in other
words, more interpretable models and be PCR-aware when inves-
tigating model choices. More refined techniques could easily be
added in addition to the explanations discussed so far, such as Shap-
ley values [ 21] or sensitivity analyses for exploring how changes to
the meta-features would impact the recommendation [ 17]. Lastly,
we follow the call for making methods results more comprehensible
to non-experts via discrete ratings and informative labels [ 11,28] -
this is readily supported by our framework implementation.
3.5 Practical Considerations
Before demonstrating the feasibility of AutoXPCR, we provide
some information on how we put our framework into practice.
For assembling the property database 𝐷, we performed DNN ex-
periments across 108 data sets, consisting of 5+1(five subsampled
and the full) versions of 18 original Monash forecasting data sets
[15]. As model options, we evaluated 11 popular DNNs, namely
Feed-Forward (FFO), DeepAR (DAR), N-BEATS (NBE), WaveNet
(WVN), DeepState (DST), DeepFactor (DFA), Deep Renewal Pro-
cesses (DRP), GPForecaster (GPF), MQ-CNN & MQ-RNN (MQC &
MQC) and Temporal Fusion Transformer (TFT), as introduced in
Section 2. Model performance was assessed by nine PCR functions
𝑓𝑖– (P)redictive quality is evaluated via the root mean squared
(RMSE), mean absolute scaled (MASE), and percentage (MAPE)
error [ 19] on the test split, the (C)omplexity is described by the
number of parameters and model size on disc, and the (R)esource
consumption is measured in terms of running time and power draw,
both for the complete training as well as per test instance inference.
For AutoXPCR, the weights 𝑤𝑖were assigned such that correlations
(e.g., between errors) are mitigated and a sound trade-off is estab-
lished – within each group, the properties are equally weighted,
and in sum, each PCR group is equally weighted. The meta-features
of each configuration constitute information on seasonality, fre-
quency, and forecast horizon (as given in the Monash Repository
[15]), as well as averaged statistics across all series in Y(length per
series in addition to mean, minimum, and maximum value). Note
that we specifically chose these rather basic features, as they suffice
for demonstrating the potential of AutoXPCR. Additional features,
while they could possibly improve our method further, would also
 
809AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
increase the risk of overfitting the data. For meta-learning, we split
the results from all data sets via five-fold grouped cross-validation,
i.e., all variants of an original data set are either used for training
or validation. Only simple, interpretable models [ 33] like linear re-
gressors, support vector regressors, and decision trees were tested
and the final meta-learners were chosen based on Equation 4.
As competing AutoML methods, we tested AutoGluonTS (AGl)
[39], AutoKeras (AKe) [ 22], and AutoSklearn (ASk) [ 10]. Note that
these frameworks do not choose among our aforementioned DNN
pool, but construct and evaluate completely novel ML pipelines and
models. As the AutoForecast [ 1] implementation could not be easily
integrated into our codebase, we implemented our own version
which for sake of fairness uses the same model pool and meta-
features as AutoXPCR. For that we adapted the earlier introduced
weights𝑤𝑖such that the recommendation only aims at minimizing
the predictive error (MASE), hence we refer to it as Auto(XP) in
the following. In addition, we tested an exhaustive search over our
model pool as baseline, i.e., finding the best solution by testing all
11 DNN candidates. To be concise, we compare our method with
specialized forecasting approaches (AGl, XP as an AutoForecast
adaption) as well as general AutoML frameworks (AKe, ASk) –
unfortunately, we found other methods from our literature review
(Section 2) to not be publicly available, or inapplicable for our
problem. In addition to the competitor libraries, we utilize GluonTS
[2] for deep learning, CodeCarbon [ 38] for energy profiling, Scikit-
learn [ 30] for meta-learning, and Plotly for visualizations [ 20]. Our
code is available at github.com/raphischer/xpcr, including an online
tool for interactively exploring our findings, as here we can only
showcase in-depth results for exemplary data sets and models.
4 Experimental Results
Due to the realization that model performance should not only be
assessed in terms of predictive quality, we start our experimental
investigation by re-evaluating the SOTA in forecasting. In Figure
2, the MASE alongside the PCR score for each investigated DNN
and data set combination is displayed. On the left, we see further
evidence for the “no free lunch” theorem [ 46] – some DNNs like
DAR, FFO, NBE, and WVN seem to generally perform well in terms
of predictive quality (greenish columns), but there is no clear win-
ner. When also taking resource consumption and complexity into
account (middle), we see that FFO on many cases achieves the most
reasonable balance of PCR criteria. Keep in mind that as explained
in Section 3.2, the PCR score is calculated from index-scaled values,
and thus, higher values indicate better results (color scales are ad-
justed accordingly). In this assessment, GPF and MQC generally
also score well and in the case of FRED-MD data even outperform
FFO. It is noteworthy to mention that the ASk competitor also has
high PCR scores for some data sets, however the left plot shows
how these obtained models often have high errors. To validate our
implementation and results, we also compared our DNN MASE
scores to the ones reported by Godahewa et al. [ 15] (right). Except
for the M4 data sets, we see only marginal differences which likely
stem from deviations in the experiment evaluation code. Overall,
these results are evidence that the understanding of SOTA needs
to change and become more PCR-aware – naturally, reports and
benchmarks like the Monash Repository play an important rolein this context and can hopefully draw some inspiration from our
findings.
For the exemplary Hospital data, we depict how all tested DNNs
perform in terms of running time, MASE, training power draw, and
RMSE in Figure 3, with the overall PCR score being indicated by
point color. Note that these plots feature the index-scaled values
given by Equation 3, which make the comparisons easier to per-
form and interpret. Counter-intuitively to the investigated metrics,
improved performance now corresponds to higher values and all
values are bounded to the unit interval. The results are evidence
of the intricate trades occurring in practice – DAR and AGl for
example achieve similar and very good error scores, however de-
mand a lot of resources during training and inference. ASk has
very good running time during inference, but takes long to train
and cannot achieve good predictive quality. GPF and MQC score
reasonably well across all criteria, but FFO is the clear winner and
benefits strongest from its very good energy consumption during
training. Overall, our findings underline that information on re-
source consumption and complexity is crucial when reporting on
and improving the SOTA in forecasting.
Now let us explore how usable the property database with this
complex DNN performance information is for meta-learning. The
AGl, AKe, and ASk competitors are excluded in this comparison, as
their obtained models are not contained in our original DNN model
pool and their performance is not used for meta-learning. In Figure
4, we firstly display the estimated MASE values (left), showing that
(in comparison to Figure 2) AutoXPCR is generally able to predict
reasonable values however also loses certain details. In the middle,
we see the estimated PCR scores which are aggregated from the
individually estimated properties. While the PCR estimates seem
to be slightly lower (notice the different color scales), we also see
evidence that AutoXPCR correctly captures important patterns.
The PCR estimation errors (right) are also rather evenly distributed
across the database, except for M4 Hourly, whose PCR scores are
overly underestimated. Overall this is an important first indicator
that our approach of selecting models via multi-objective meta-
learning is feasible.
Next, we investigate how AutoXPCR performs in comparison to
other AutoML approaches. Table 1 lists the PCR score and index-
scaled MASE of each obtained model, as well as the total power draw
of running the model searches - accordingly, these columns need
to be minimized. We see clear evidence that our method obtains
the most efficient models from our pool at only a fraction of the
required compute expenses. As expected, our AutoXP variant tends
to retrieve more accurate models, however their PCR scores are
lower and they also take more power to train. From the competitors
that produce models from scratch instead of selecting a candidate
from our chosen DNN pool, only AutoGluonTS succeeds in finding
more accurate models. However, the resulting models all have a
lower PCR score and require much more energy to be obtained.
AutoKeras requires even more resources and provides models with
very bad relative MASE scores. As already mentioned for Figure 2,
AutoSklearn here also generally achieves a solid PCR score and even
finds some models that trade better than the candidates from our
pool. Our exhaustive search baseline tests all DNNs from our pool
and therefore is guaranteed to obtain the optimal candidate. For
this reason, we only bold-print the cases where AutoXPCR failed
 
810KDD ’24, August 25–29, 2024, Barcelona, Spain Raphael Fischer and Amal Saadallah
DARDFADRPDSTFFOGPFMQCMQRNBETFTWVNAGlAKeASkTraf..klyTour..rlyTour..hlySola..klyNN5 ..klyNN5 DailyM4 W eeklyM4 HourlyM3 Q..rlyM3 M..hlyM1 Q..rlyM1 M..hlyHospitalFRED-MDElec..klyDominickCIF 2016Car P arts
DARDFADRPDSTFFOGPFMQCMQRNBETFTWVNAGlAKeASk
 DAR FFO NBE WVN1e-101e-511e5e101e15
0.550.60.650.70.75
012345MASE PCR Score Diff to Monash MA SE
Figure 2: MASE and overall PCR score of all models and data sets, along with Monash differences [ 15]. In addition to predictive
quality, the PCR-scored performance also considers model complexity and resource consumption. This completely changes the
understanding of SOTA performance and reveals that FFO generally seems to be the most efficient DNN from our pool.
DAR
DFADRPDSTFFO GPFMQC
MQRNBETFT
WVNAGl
AKe
ASk
0 0.5 100.20.40.60.81
Running Time per Inference IndexTest MA SE Index
DAR
DFA DRPDSTFFOGPF
MQC
MQRNBETFT
WVNAGl
AKe
ASk
0 0.5 100.20.40.60.81
Training P ower Dr aw IndexTest RMSE Index
Figure 3: Running time versus MASE and training power draw versus RMSE performance trades on Hospital data.
DARDFADRPDSTFFOGPFMQCMQRNBETFTWVNTraf..klyTour..rlyTour..hlySola..klyNN5 ..klyNN5 DailyM4 W eeklyM4 HourlyM3 Q..rlyM3 M..hlyM1 Q..rlyM1 M..hlyHospitalFRED-MDElec..klyDominickCIF 2016Car P arts
DARDFADRPDSTFFOGPFMQCMQRNBETFTWVN
DARDFADRPDSTFFOGPFMQCMQRNBETFTWVNe-10e-51e5e10e15
0.20.30.40.50.6
−0.2−0.100.10.2Estimated MA SE Estimated PCR Score Estimation Error
Figure 4: Results of predicting model suitability for data sets with AutoXPCR, showcasing how MASE and overall PCR score
values are reasonably predicted and errors are rather evenly distributed across our database.
 
811AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: PCR score and MASE index of obtained models from various search approaches, along with power draw (kWh).
Data set AutoXPCR AutoXP (≈ [1]) AutoGluonTS [39] AutoKeras [22] AutoSklearn [10] Exhaustive
PCR𝑓MASE kWh PCR𝑓MASE kWh PCR𝑓MASE kWh PCR𝑓MASE kWh PCR𝑓MASE kWh PCR𝑓MASE kWh
Car Parts 0.68 0.70 2.00 0.63 0.68 8.19 0.63 0.95 9.76 0.61 0.36 15.7 0.78 0.57 57.7 0.74 0.91 151
CIF 2016 0.66 0.15 0.30 0.63 0.40 0.65 0.64 1.00 7.01 0.53 0.04 2.72 0.67 0.00 8.97 0.73 0.73 26.1
Dominick 0.74 0.95 14.4 0.69 1.00 42.2 0.75 1.00 9.16 0.63 0.67 560 0.78 0.66 289 0.74 0.95 1688
Elec..kly 0.78 1.00 0.52 0.78 1.00 0.52 0.63 0.79 6.05 0.57 0.28 30.3 0.68 0.01 25.7 0.78 1.00 123
FRED-MD 0.73 0.25 0.34 0.56 0.14 2.51 0.63 1.00 6.58 0.52 0.00 8.65 0.67 0.00 10.6 0.73 0.25 26.7
Hospital 0.75 0.88 1.15 0.67 0.88 4.51 0.69 1.00 5.24 0.65 0.88 36.5 0.68 0.03 18.5 0.75 0.88 83.3
M1 M..hly 0.72 0.73 0.89 0.66 0.75 3.32 0.65 1.00 4.56 0.56 0.16 44.1 0.69 0.00 29.6 0.72 0.73 96.6
M1 Q..rly 0.71 0.52 0.41 0.77 0.82 0.28 0.67 1.00 3.79 0.55 0.00 4.04 0.67 0.00 9.46 0.77 0.82 31.9
M3 M..hly 0.73 0.75 2.85 0.65 0.65 15.5 0.70 1.00 9.33 0.61 0.49 114 0.72 0.22 83.1 0.73 0.75 368
M3 Q..rly 0.68 0.78 5.68 0.68 0.78 5.68 0.68 1.00 6.28 0.64 0.63 18.8 0.73 0.23 19.0 0.68 0.78 119
M4 Hourly 0.68 0.16 0.59 0.61 0.02 129 0.67 1.00 17.9 0.54 0.07 409 0.67 0.00 118 0.68 0.16 506
M4 Weekly 0.70 0.75 1.67 0.66 0.55 1.50 0.67 1.00 9.97 0.62 0.59 40.4 0.68 0.03 45.0 0.70 0.75 212
NN5 Daily 0.73 0.50 2.35 0.75 0.82 0.33 0.67 1.00 9.49 0.60 0.50 63.8 0.75 0.41 41.9 0.75 0.82 58.2
NN5 ..kly 0.75 0.75 1.13 0.67 1.00 7.67 0.67 1.00 5.85 0.66 0.84 5.38 0.73 0.36 38.1 0.76 0.90 96.4
Sola..kly 0.74 0.79 1.41 0.65 0.76 0.85 0.58 0.46 3.90 0.62 0.49 3.93 0.71 0.23 22.0 0.74 0.79 58.2
Tour..hly 0.71 0.85 0.59 0.63 0.81 5.87 0.67 0.99 10.5 0.56 0.34 105 0.68 0.03 25.2 0.71 0.85 86.2
Tour..rly 0.74 0.76 0.32 0.74 0.76 0.32 0.67 1.00 9.82 0.59 0.39 31.9 0.67 0.01 9.46 0.74 0.76 34.6
Traf..kly 0.75 0.77 0.82 0.63 0.67 71.0 0.69 1.00 6.51 0.65 0.75 14.6 0.73 0.23 40.4 0.75 0.77 197
MASERMSEMAPE
Params
Size
T Power
T TimeI PowerI Time
00.20.40.60.81Score (XPCR / FFO): 0.75
Score (XP / DST): 0.67
Score (AGl): 0.69Hospital
MASERMSEMAPE
Params
Size
T Power
T TimeI PowerI Time
00.20.40.60.81Score (XPCR / GPF): 0.66
Score (XP / MQC): 0.63
Score (AGl): 0.64CIF 2016
MASERMSEMAPE
Params
Size
T Power
T TimeI PowerI Time
00.20.40.60.81Score (XPCR / GPF): 0.68
Score (XP / DST): 0.63
Score (AGl): 0.63Car P arts
MASERMSEMAPE
Params
Size
T Power
T TimeI PowerI Time
00.20.40.60.81Score (XPCR / FFO): 0.75
Score (XP / DST): 0.63
Score (AGl): 0.69Traffic W eekly
Figure 5: Index-scaled multi-dimensional performance of models obtained from automated selection strategies. AutoXPCR rec-
ommendations are most efficient, while AutoXP and AutoGluonTS produce better quality at higher computational expenses.
to retrieve the optimal solution (last columns). Surprisingly, this
occurs very rarely, whereas the exhaustive search requires dramatic
computational expenses. In short, our method clearly outperforms
all competitors – they struggle to identify more efficient models
and suffer from much higher resource consumption.
To go into more depth, we visualize the multi-dimensional per-
formance trades of exemplary obtained models via star plots in
Figure 5. As in Figure 3, we display index scaled results, i.e., higher
values indicate better performance along the criteria. Recall that
AutoXPCR and AutoXP recommend single DNNs of our model pool,
so we also indicate their names. Their performance aligns with the
respective cells in Figures 2 and 4, and in the first star plot, with
respective scatter points of Figure 3. Across all plots we see how
the recommended DNNs behave quite differently and have their
individual benefits and drawbacks, which evidences the importanceof considering use-case specific priorities during model search. The
results of AutoGluonTS (red) sort of resemble the outputs of our Au-
toXP variant (yellow) – this is in line with our expectations, as both
of them try to find models with lowest MASE. AutoXPCR (green)
on the other hand recommends models which trade better across
all PCR criteria.
To demonstrate AutoXPCR’s multi-level explainability as dis-
cussed in Section 3.4, we show exemplary visualizations of expla-
nations E1 and E2 in Figure 6. Our method automatically outputs
these by-products when recommending a specific model for given
data (here shown for Hospital data and the corresponding FFO
recommendation, cf. Figure 5). On the left side, AutoXPCR informs
on how supportive each property of the selected model is for the
recommendation with the help of Equation 5. For this example, we
see that the estimated RMSE has the biggest impact on making FFO
 
812KDD ’24, August 25–29, 2024, Barcelona, Spain Raphael Fischer and Amal Saadallah
MASERMSEMAPEParamsSizeT PowerT TimeI PowerI Time 051015Q1: Wh y use FFO on Hospital?
Propert yE1 - Importance [%]
No NANDiv lengthEqual lengthFc Horiz onNum SeriesAvg LengthAvg MeanAvg MinAvg MaxSeasonialit y00.10.20.3Q2: R easons for MA SE estimate?
Meta-featureE2 - Importance [%]
Figure 6: By-product explanations produced by AutoXPCR. E1 entails the importance of each property for the PCR score
estimate (left), while E2 informs on feature importance for estimating the MASE of FFO (right).
2 4 6 8 1000.20.40.60.81
2 4 6 8 10Best possible MA SE Total power dr aw
k (testing top-k recommendations)Relativ e value
Figure 7: MASE and power draw of testing the top- 𝑘recommended models for all data sets (red) and averaged (black), in relation
to exhaustively testing all models. Trying two models results in an average relative MASE of ∼85% at less than 20% of the cost.
a favorable choice. On the right-hand side, the normalized feature
importance for the MASE estimate is given as obtained from the
respective meta-learner (decision tree with depth five [ 30]). We
see that the average maximum value across all series as well as
seasonality information mostly affect AutoXPCR’s MASE estimates.
Note that the estimated performance of any model, as an answer to
Q0, could also be easily visualized via star plots (similar to Figure
5).
Instead of just deploying the model that Equation 4 awards the
best PCR score, one could also test the top- 𝑘candidates. With
this approach, we can evaluate how AutoXPCR converges towards
optimal results, as given by the exhaustive search. This is visualized
for all 108 data sets (red) in Figure 7, with the resulting best-possible
MASE and required power draw of testing the top- 𝑘models being
compared in relation to an exhaustive search (i.e., the last column
of Table 1). On average (black), ∼85% of the best possible MASE can
already be achieved by just testing the top two recommendations,
which requires less than 20% of the resources. In over 95% of cases,
the optimal model will be under the top-5 recommendations. The
distribution of red lines indicates that for most data sets, testing
only few models appears to provide more accurate results than
suggested by the average (on the left side, more red lines are placed
over the black line).
Lastly, we discuss the quality of our approach in terms of errors
on the estimated properties and PCR scores. In addition to the error(a) (cf. Equation 4), Figure 8 reports the accuracy of scoring errors
below a threshold of 0.1 (b), the top-1 (c) & top-5 (d) accuracy of
predicting the optimal model, and the intersection size of top-5
recommendations and true best models (e). All metrics are given
as averages with standard deviation across all investigated data
sets, with bar colors indicating the associated property’s impact
on the PCR score (colored red). As expected, errors and resource
demand for training are the hardest to estimate, closely followed by
the inference resource demand. Model complexity behaves rather
deterministic, which is to be expected since the candidates in our
DNN pool have fixed hyperparameters and architectures. Gener-
ally, the standard deviation tends to be higher for properties that
are difficult to predict. The top-5 accuracy and intersection results
support our analysis of Figure 7 – testing the five best recommenda-
tions has a very high chance of finding the optimal model. We also
see empirical evidence that our PCR-aware selection is superior to
directly estimating the overall PCR score, which aligns with our
theoretical considerations in Section 3.3.
To follow the call for improved reporting on ML carbon emissions
and sustainability [ 12], we give some closing words on the expenses
of our experimental investigations. All discussed computations
were performed on a single PC ( Intel i9-13900K CPU) and took
about two weeks, resulting in estimated carbon emissions of ∼40
CO2e (based on Lacoste et al. ’s impact calculator [ 23]). Note that this
computational effort stems mostly from training our 1188 DNNs and
 
813AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain
00.05 0.10.15 0.2I TimeI PowerT TimeT PowerSizeParamsMAPERMSEMASEDirect ScoreXPCR Score
0 50 100 0 50 100 0 50 100 0 2 4Error (a) Thresh (b) Top-1 (c) Top-5 (d) Inters (e)
(a)𝜖=|𝑓−ˆ𝑓| (b)𝜖!<0.1 (c)argmax𝑚𝑓!=argmax𝑚ˆ𝑓 (d) argmax𝑚𝑓∈{top-5𝑚ˆ𝑓} (e){top-5𝑚𝑓}∩{ top-5𝑚ˆ𝑓}
Figure 8: Quality of estimating properties and the PCR score. Some properties (errors, training resources) are harder to predict,
resulting in higher errors. The high top-5 accuracy and intersection prove how testing the top-5 recommendations almost
certainly retrieves the optimal solution. Colors indicate each property’s weight in the PCR score (red).
testing the AutoML competitors, whereas running AutoXPCR (i.e.,
training and validating the meta-regressors on the pre-assembled
property database) only takes a few seconds.
5 Conclusion
To conclude, this work introduced AutoXPCR, which – to the best
of our knowledge – is the first automated, explainable, and resource-
aware take on model selection that was practically applied to the
domain of time series forecasting. In our literature review, we dis-
cussed important related works in the field and identified the flaws
that we address with our method. While this paper mainly discussed
results for automatically forecasting time series with DNNs, we
formulated our methodology in a generalized way such that it can
be easily applied to other domains. The meta-learning paradigm
behind our method pays close attention to different PCR objectives,
which describe model performance in terms of predictive error,
complexity, and resources. Index scaling makes these properties
comparable – here, we improve existing work on assessing the effi-
ciency of ML and propose a new, superior approach to calculating
index values. In addition to being PCR-aware, the model recom-
mendations given by AutoXPCR are accompanied by multi-level
explanations and our framework allows for interactive use. As a
result, users can better understand the selection process and align
the importance of PCR properties with their individual priorities. In
our experimental evaluation, we investigated how the understand-
ing of “SOTA in forecasting” is completely changed when model
performance is assessed along more diverse dimensions instead of
only focusing on predictive quality. By evaluating a wide range of
DNNs across many different data sets, we were able to find their
benefits and drawbacks and meta-learn from these results. When
demonstrating AutoXPCR’s practical feasibility, we outperformed
all competing approaches – our method achieved near-to-optimal
predictive quality while only requiring a fraction of the compu-
tational effort. Overall, we deem our approach highly beneficial
for the domain of meta-learning and model selection, as well asforecasting. We make all DNN results and our code available to
the research community, hoping that they benefit further advances
in these research fields. We intend to apply AutoXPCR in more
ML domains and investigate the impact of performing experiments
in different computational environments – a first extension was
already accepted for publication during the review process of this
work [ 13]. Revising our DNN pool, possibly based on the output
models from our AutoML competitors, as well as conducting a user-
study on the explainability aspects are further directions we see
for future work. With our work, we hope to contribute to making
ML, AutoML, and time series forecasting more resource-aware and
trustworthy.
References
[1]Mustafa Abdallah, Ryan A. Rossi, Kanak Mahadik, Sungchul Kim, Handong Zhao,
and Saurabh Bagchi. 2022. AutoForecast: Automatic Time-Series Forecasting
Model Selection. In 31st ACM International Conference on Information & Knowledge
Management (CIKM). 5–14. https://doi.org/10.1145/3511808.3557241
[2]Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin
Flunkert, Jan Gasthaus, et al .2020. GluonTS: Probabilistic and Neural Time Series
Modeling in Python. Journal of Machine Learning Research (JMLR) 21 (2020),
116:1–116:6. http://jmlr.org/papers/v21/19-820.html
[3]Ahmad Alsharef, Karan Aggarwal, Sonia, Manoj Kumar, and Ashutosh Mishra.
2022. Review of ML and AutoML Solutions to Forecast Time-Series Data. Archives
of Computational Methods in Engineering 29, 7 (2022), 5297–5311. https://doi.
org/10.1007/s11831-022-09765-0
[4]Katharina Beckh, Sebastian Müller, Matthias Jakobs, Vanessa Toborek, Hanxiao
Tan, et al .2023. Harnessing Prior Knowledge for Explainable Machine Learning:
An Overview. In IEEE Conference on Secure and Trustworthy Machine Learning,
(SaTML). 450–463. https://doi.org/10.1109/SATML54575.2023.00038
[5]Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. 2018. Conditional
time series forecasting with convolutional neural networks. https://arxiv.org/
abs/1703.04691
[6]Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger,
et al.2020. Toward Trustworthy AI Development: Mechanisms for Supporting
Verifiable Claims. https://arxiv.org/abs/2004.07213
[7]Siem Morten Johannes Dahl. 2020. TSPO: an autoML approach to time series
forecasting. Ph. D. Dissertation.
[8]Janez Demšar. 2006. Statistical comparisons of classifiers over multiple data sets.
Journal of Machine Learning Research (JMLR) 7 (2006), 1–30. http://jmlr.org/
papers/v7/demsar06a.html
[9]Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy,
et al.2020. Autogluon-tabular: Robust and accurate automl for structured data.
 
814KDD ’24, August 25–29, 2024, Barcelona, Spain Raphael Fischer and Amal Saadallah
https://arxiv.org/abs/2003.06505
[10] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and
Frank Hutter. 2022. Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning.
Journal of Machine Learning Research (JMLR) 23, 261 (2022), 1–61. http://jmlr.
org/papers/v23/21-0992.html
[11] Raphael Fischer, Matthias Jakobs, Sascha Mücke, and Katharina Morik. 2022. A
Unified Framework for Assessing Energy Efficiency of Machine Learning. In
Workshop Proceedings of the European Conference on Machine Learning and Data
Mining (ECML PKDD). https://doi.org/10.1007/978-3-031-23618-1_3
[12] Raphael Fischer, Thomas Liebig, and Katharina Morik. 2024. Towards more
sustainable and trustworthy reporting in machine learning. Data Mining and
Knowledge Discovery (2024). https://doi.org/10.1007/s10618-024-01020-3
[13] Raphael Fischer, Marcel Wever, Sebastian Buschjäger, and Thomas Liebig. 2024.
MetaQuRe: Meta-Learning from Model Quality and Resource Consumption. In
Proceedings of the European Conference on Machine Learning and Data Mining
(ECML PKDD) (forthcoming). https://github.com/raphischer/metaqure
[14] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sébastien Poirier, Bernd Bischl, and
Joaquin Vanschoren. 2019. An open source AutoML benchmark. http://arxiv.
org/abs/1907.00909
[15] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman,
and Pablo Montero-Manso. 2021. Monash Time Series Forecasting Archive. In
Neural Information Processing Systems (NeurIPS) Track on Datasets and Bench-
marks. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/
eddea82ad2755b24c4e168c5fc2ebd40-Abstract-round2.html
[16] Philipp Hacker. 2024. Sustainable AI Regulation. Common Market Law Review
(2024), 345–386. https://doi.org/10.54648/cola2024025
[17] Andreas Holzinger, Anna Saranti, Christoph Molnar, Przemyslaw Biecek, and
Wojciech Samek. 2022. Explainable AI Methods - A Brief Overview. 13–38. https:
//doi.org/10.1007/978-3-031-04083-2_2
[18] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. 2019. Automated Machine
Learning - Methods, Systems, Challenges. Springer Nature. https://doi.org/10.
1007/978-3-030-05318-5
[19] Rob J. Hyndman and Anne B. Koehler. 2006. Another look at measures of
forecast accuracy. International Journal of Forecasting 22, 4 (2006), 679–688.
https://doi.org/10.1016/j.ijforecast.2006.03.001
[20] Plotly Technologies Inc. 2015. Collaborative data science. Montreal, QC. https:
//plot.ly
[21] Matthias Jakobs and Amal Saadallah. 2023. Explainable Adaptive Tree-based
Model Selection for Time-Series Forecasting. In IEEE International Conference on
Data Mining ICDM. 180–189. https://doi.org/10.1109/ICDM58522.2023.00027
[22] Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras: An Efficient Neural
Architecture Search System. In Proceedings of the 25th International Conference
on Knowledge Discovery and Data Mining (KDD). 1946–1956. https://doi.org/10.
1145/3292500.3330648
[23] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
2019. Quantifying the Carbon Emissions of Machine Learning. arXiv:1910.09700
http://arxiv.org/abs/1910.09700
[24] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. Temporal fusion
transformers for interpretable multi-horizon time series forecasting. International
Journal of Forecasting 37, 4 (2021), 1748–1764. https://doi.org/10.1016/j.ijforecast.
2021.03.012
[25] Zimeng Lyu and Travis Desell. 2022. ONE-NAS: an online neuroevolution based
neural architecture search for time series forecasting. In Genetic and Evolutionary
Computation Conference (GECCO), Companion Volume. 659–662. https://doi.org/
10.1145/3520304.3528962
[26] Pascal Massart and Lucien Birgé. 2001. Gaussian model selection. Journal of
the European Mathematical Society 3, 3 (2001), 203–268. https://doi.org/10.1007/
s100970100031
[27] Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. 2020. Interpretable
Machine Learning - A Brief History, State-of-the-Art and Challenges. In Workshop
Proceedings of the European Conference on Machine Learning and Data Mining
(ECML PKDD), Vol. 1323. 417–431. https://doi.org/10.1007/978-3-030-65965-3_28
[28] Katharina Morik, Helena Kotthaus, Raphael Fischer, Sascha Mücke, Matthias
Jakobs, et al .2022. Yes we care! - Certification for machine learning methods
through the care label framework. Frontiers in Artificial Intelligence 5 (2022).
https://doi.org/10.3389/frai.2022.975029
[29] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel
Munguia, et al .2021. Carbon Emissions and Large Neural Network Training.
(2021). https://arxiv.org/abs/2104.10350
[30] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, et al .2011. Scikit-learn: Machine Learning in Python. Journalof Machine Learning Research (JMLR) 12 (2011), 2825–2830. https://doi.org/10.
5555/1953048.2078195
[31] Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella,
Yuyang Wang, and Tim Januschowski. 2018. Deep State Space Models for
Time Series Forecasting. In Advances in Neural Information Processing Sys-
tems (NeurIPS). 7796–7805. https://proceedings.neurips.cc/paper/2018/hash/
5cf68969fb67aa6082363a6d4e6468e2-Abstract.html
[32] Thomas Rojat, Raphaël Puget, David Filliat, Javier Del Ser, Rodolphe Gelin, and
Natalia Díaz-Rodríguez. 2021. Explainable artificial intelligence (XAI) on time-
series data: A survey. https://arxiv.org/abs/2104.00950
[33] Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature Machine
Intelligence 1, 5 (2019), 206–215. https://doi.org/10.1038/s42256-019-0048-x
[34] Amal Saadallah, Matthias Jakobs, and Katharina Morik. 2021. Explainable Online
Deep Neural Network Selection Using Adaptive Saliency Maps for Time Series
Forecasting. In Proceedings of the European Conference on Machine Learning and
Data Mining (ECML PKDD). 404–420. https://doi.org/10.1007/978-3-030-86486-
6_25
[35] Amal Saadallah, Matthias Jakobs, and Katharina Morik. 2022. Explainable online
ensemble of deep neural network pruning for time series forecasting. Machine
Learning 111, 9 (2022), 3459–3487. https://doi.org/10.1007/s10994-022-06218-4
[36] Amal Saadallah, Florian Priebe, and Katharina Morik. 2019. A Drift-based Dy-
namic Ensemble Members Selection using Clustering for Time Series Forecasting.
InProceedings of the European Conference on Machine Learning and Data Mining
(ECML PKDD). https://doi.org/10.1007/978-3-030-46150-8_40
[37] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.
DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-
national Journal of Forecasting 36, 3 (2020), 1181–1191. https://doi.org/10.1016/j.
ijforecast.2019.07.001
[38] Victor Schmidt et al .2021. CodeCarbon: Estimate and Track Carbon Emissions
from Machine Learning Computing. https://github.com/mlco2/codecarbon
[39] Oleksandr Shchur, Ali Caner Türkmen, Nick Erickson, Huibin Shen, Alexander
Shirkov, et al .2023. AutoGluon-TimeSeries: AutoML for Probabilistic Time Series
Forecasting. In International Conference on Automated Machine Learning. 9/1–21.
https://proceedings.mlr.press/v224/shchur23a.html
[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and Policy
Considerations for Modern Deep Learning Research. In The 34th Conference on
Artificial Intelligence (AAAI). 13693–13696. https://doi.org/10.1609/AAAI.V34I09.
7123
[41] Tanja Tornede, Alexander Tornede, Jonas Hanselle, Felix Mohr, Marcel Wever,
and Eyke Hüllermeier. 2023. Towards Green Automated Machine Learning:
Status Quo and Future Directions. Journal of Artificial Intelligence Research (JAIR)
77 (2023), 427–457. https://doi.org/10.1613/jair.1.14340
[42] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
Vinyals, et al .2016. WaveNet: A Generative Model for Raw Audio. In Inter-
national Symposium on Computer Architecture (ISCA) Speech Synthesis Work-
shop. 125. http://www.isca-speech.org/archive/SSW_2016/abstracts/ssw9_DS-
4_van_den_Oord.html
[43] Jan N Van Rijn and Frank Hutter. 2018. Hyperparameter importance across
datasets. In Proceedings of the 24th International Conference on Knowledge Discov-
ery and Data Mining (KDD). 2367–2376. https://doi.org/10.1145/3219819.3220058
[44] Yuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and
Tim Januschowski. 2019. Deep Factors for Forecasting. In Proceedings of the
36th International Conference on Machine Learning (ICML), Vol. 97. 6607–6617.
http://proceedings.mlr.press/v97/wang19k.html
[45] Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka.
2017. A multi-horizon quantile recurrent forecaster. https://arxiv.org/abs/1711.
11053
[46] David H. Wolpert and William G. Macready. 1997. No free lunch theorems for
optimization. IEEE Transactions on Evolutionary Computation 1 (1997), 67–82.
https://doi.org/10.1109/4235.585893
[47] Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. 2019. OBOE:
Collaborative Filtering for AutoML Model Selection. In Proceedings of the 25th
International Conference on Knowledge Discovery and Data Mining (KDD). 1173–
1183. https://doi.org/10.1145/3292500.3330909
[48] Xin-She Yang. 2014. Multi-Objective Optimization. In Nature-Inspired Optimiza-
tion Algorithms. 197–211. https://doi.org/10.1016/B978-0-12-416743-8.00014-2
[49] Lucas Zimmer, Marius Lindauer, and Frank Hutter. 2021. Auto-Pytorch: Multi-
Fidelity MetaLearning for Efficient and Robust AutoDL. IEEE Transactions on
Pattern Analysis and Machine Intelligence 43, 9 (2021), 3079–3090. https://doi.
org/10.1109/TPAMI.2021.3067763
 
815