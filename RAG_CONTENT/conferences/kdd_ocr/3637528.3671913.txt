All in One and One for All: A Simple yet Effective Method
towards Cross-domain Graph Pretraining
Haihong Zhao∗
Data Science and Analytics Thrust,
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
hzhaobf@connect.ust.hkAochuan Chen∗
Data Science and Analytics Thrust,
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
achen149@connect.hkust-gz.edu.cnXiangguo Sun∗†
Department of Systems Engineering
and Engineering Management, and
Shun Hing Institute of Advanced
Engineering, The Chinese University
of Hong Kong
Hong Kong SAR, China
xgsun@se.cuhk.edu.hk
Hong Cheng
Department of Systems Engineering
and Engineering Management, and
Shun Hing Institute of Advanced
Engineering, The Chinese University
of Hong Kong
Hong Kong SAR, China
hcheng@se.cuhk.edu.hkJia Li†
Data Science and Analytics Thrust,
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
jialee@ust.hk
ABSTRACT
Large Language Models (LLMs) have revolutionized the fields of
computer vision (CV) and natural language processing (NLP). One
of the most notable advancements of LLMs is that a single model is
trained on vast and diverse datasets spanning multiple domains – a
paradigm we term ‘All in One’. This methodology empowers LLMs
with super generalization capabilities, facilitating an encompassing
comprehension of varied data distributions. Leveraging these capa-
bilities, a single LLM demonstrates remarkable versatility across a
variety of domains – a paradigm we term ‘One for All’. However,
applying this idea to the graph field remains a formidable challenge,
with cross-domain pretraining often resulting in negative transfer.
This issue is particularly important in few-shot learning scenarios,
where the paucity of training data necessitates the incorporation
of external knowledge sources. In response to this challenge, we
propose a novel approach called Graph COordinators for PrEtrain-
ing (GCOPE), that harnesses the underlying commonalities across
diverse graph datasets to enhance few-shot learning. Our novel
methodology involves a unification framework that amalgamates
disparate graph datasets during the pretraining phase to distill and
transfer meaningful knowledge to target tasks. Extensive exper-
iments across multiple graph datasets demonstrate the superior
∗Equal Contribution.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671913efficacy of our approach. By successfully leveraging the synergistic
potential of multiple graph datasets for pretraining, our work stands
as a pioneering contribution to the realm of graph foundational
model. Code available at https://github.com/cshhzhao/GCOPE.
CCS CONCEPTS
•Computing methodologies →Transfer learning.
KEYWORDS
pretraining; prompt tuning; graph neural networks
ACM Reference Format:
Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li.
2024. All in One and One for All: A Simple yet Effective Method towards
Cross-domain Graph Pretraining. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, Barcelona, Spain, 12 pages. https://doi.
org/10.1145/3637528.3671913
1 INTRODUCTION
Recently, artificial general intelligence (AGI) has achieved remark-
able advancement in the realms of computer vision (CV) [ 2,4,31,48]
and natural language processing (NLP) [ 6,39]. One of the key in-
novations for these models is that they pre-train one foundation
model through various contexts, making the model absorb and
synthesize knowledge across diverse domains (a.k.a “all in one”) to
deliver robust, context-aware responses. Their ability to generalize
and apply learned knowledge to a wide spectrum of downstream
domains (a.k.a “one for all”) is a testament to the success of their
pre-training strategies, which effectively capture and utilize the
nuances across different domains.
Compared with NLP and CV areas, graph data is non-linear and
more general. For example, a sentence can be seen as a graph path
4443
KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
and an image can be seen as a grid graph. These observations indi-
cate a broader and more ambitious goal of a more generalized and
versatile artificial intelligence by graphs. However, pre-training
on graphs is still grappling with the integration of multi-domain
knowledge and cross-domain generalization. Although existing
graph transfer learning methods [ 18,26,36,37,49,52] demonstrate
proficiency within the same domain, their successes, often confined
to transferring tasks within a similar graph dataset, exhibit inher-
ent limitations when attempting to transcend the graph domain
boundary. These disparities make it difficult for graph models to
replicate the success of their NLP and CV counterparts, yet the pur-
suit of moving beyond existing domain-specific task transferring
and towards a more versatile approach is not just an extension of
existing methodologies but a stride towards bridging the gap be-
tween domain-specific proficiency and the broader vision of general
artificial intelligence.
However, achieving this vision is very intractable. The first
challenge lies in the diversity of structural patterns, which is par-
ticularly observed between homophilic graphs (a pair of nodes
are intended to be similar if they are connected) and heterophilic
graphs (connected nodes depart from each other). Graphs inher-
ently exhibit diverse topologies and features, making it challenging
to identify and leverage common patterns across different domains.
The good thing is that this diversity enriches the external knowl-
edge and contains huge potential for more general transferability
to various downstream applications. However, it also complicates
the task of identifying and leveraging commonalities across do-
mains, as models must balance the fine line between capturing the
essence of individual graphs and maintaining the generalization
across different datasets.
The second challenge is the complexity inherent in aligning
semantic spaces (features) across different graph datasets. Unlike
domains with a structured, common framework (like images or
texts), graph data is diverse and abstract. Features in one graph
might have no direct counterpart in another, making it incredi-
bly challenging to align these features in a meaningful way. This
alignment is not just a matter of mapping features but of under-
standing and reconciling the underlying semantics and contexts
they represent. The absence of a universal framework to guide this
alignment makes it a daunting task, one that requires a sophisti-
cated approach to bridge the gap between disparate graph datasets
effectively. Some recent works [ 22] are trying to align different
graph semantic spaces by generating a textual description of these
graphs and then using LLMs to learn a unified representation, but
they can only deal with text-attributed graphs or graphs with spe-
cific feature semantics. In a broader range, many graphs have only
latent feature vectors and we actually do not know how exactly
each dimension means, let alone generate a textual description.
Inspired by recent achievements of graph prompt [ 36], which
has been carefully demonstrated as a powerful approach to manip-
ulate various graph data, we hope to introduce the graph prompt-
ing technique to conduct more advanced operations on different
graph datasets, to bridge the gap left by cross-domain graph pre-
training and transferring. Specifically, we introduce the concept
of “coordinators”, which are some virtual nodes that function as
dynamic bridges between disparate graph datasets. These coordi-
nators share the same theory foundation of graph prompts thathave the powerful capability of data manipulation. The theory foun-
dation ensures that these coordinators can learn an appropriate
latent data manipulation strategy to harmonize different datasets,
promote a unified representation, and adapt to the specificities
of each graph, thus ensuring that the model doesn’t just see a
collection of isolated datasets but a harmonious, interconnected
network. This innovation allows the model to navigate the diversity
of graph structures adeptly, recognizing and leveraging underlying
commonalities and differences in a balanced and informed manner.
Beyond the coordinators, we also design a complete pretraining
framework and provide two transferring components, which can
ensure that the knowledge transferred is not just relevant but also
contextually enriched, turning potential negative transfer into a
positive, performance-enhancing phenomenon. In summary, the
main contributions are as follows:
•We propose a very simple yet very effective framework, named
“GCOPE”, as a novel mechanism to unify diverse graph struc-
tures, enabling the seamless integration and a unified graph
pre-training framework, making the pre-trained graph model
able to preserve multi-domain knowledge across different graph
datasets.
•Inspired by the powerful data manipulation capability of graph
prompts in the downstream stage, we propose similar “coordi-
nators” in the pre-training stage so that we can learn a latent
data alignment strategy and ensure the relevance and efficacy of
knowledge transfer, especially in few-shot learning scenarios.
•Our pre-training framework is compatible with various graph
pre-training methods and the transferring stage can also leverage
both fine-tuning and prompting techniques.
•We conduct comprehensive experimental evaluation, the results
from which demonstrate the superior performance of our ap-
proach across multiple graph datasets.
2 MOTIVATION
Wisconsin Texas Cornell Chameleon Squirrel30
20
10
0Improvement (%)
From Pubmed
From Photos
Figure 1: Negative transfer phenomenon in the single-source
cross-domain transfer setting. Sources (Pubmed and Photos)
are two homophilic datasets. Targets (Wisconsin, Texas, Cor-
nell, Chameleon, and Squirrel) are five heterophilic graphs.
Unlike images, words, or sentences, which often share extensive
underlying semantics across datasets, graph data are more abstract,
making it more challenging to discern low-level common semantics.
Therefore, the negative transfer phenomenon [ 20,47] is commonly
found in the field of graph learning. Here, we focus on the rep-
resentative C-way-K-shot setting. We validate this phenomenon
4444All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining KDD ’24, August 25–29, 2024, Barcelona, Spain
GNN
Contrastive 
Loss
Reconstruction 
Loss
Pre-
trained
GNN
PubMed
 (500)
Downstream
Task
Nodes Coordinators
 TunedTransferring
Pre-
trained
GNN
FrozenOR
Graph
Prompt
Projection
Cora
(1433)
Texas
(1703)Photos
(745)
Figure 2: Overview of our proposed GCOPE method. The left part is our pretraining stage and the right part transferring stage.
through extensive experimentation, examining two main scenarios:
the transfer of knowledge from a single source dataset, as well as
from multiple source datasets.
Observations: As depicted in Figure 1, transferring from a single
source dataset does indeed negatively affect the target task, con-
firming our analysis of the distinctiveness of two graph datasets.
In order to overcome this obstacle, it is necessary to expand the
scope of the source dataset so that it can offer valuable insights
for the downstream task. However, a brutal combination of source
datasets still failed to enhance the performance of the target task.
We detail this in section 4.2.
Further Analysis: The first reason causing this negative transfer
across domains is that the structural patterns are different, espe-
cially reflected in homophilic and heterophilic datasets. Graph data
is characterized by its complex network of connections, where
nodes are not isolated but part of an interconnected structure. The
relationships among nodes, represented by edges, facilitate the flow
of influence and information, making each node both a product and
a contributor to its environment. This interconnectivity is central
to graph data, resulting in systems where the collective properties
surpass those of individual components. Nonetheless, this char-
acteristic presents challenges for machine learning models that
attempt to learn from multiple graph datasets simultaneously. Each
dataset is akin to its own distinct ecosystem, governed by its unique
topology and rules. Within a given graph, nodes and edges are at-
tuned to specific patterns and linkages that are relevant within
that context. When merging different graphs for joint training, the
inherent disparity of each graph’s structure becomes a barrier, as
the datasets naturally exist in isolation from one another and thus
the information flow is blocked.
The second reason is the lack of feature alignment across these
datasets. Graph attributes are heterogeneous and context-dependent,
representing a wide range of abstract concepts and connections.
Unlike textual or visual data, which have a common reference
framework, graph attributes are highly varied and specific to their
domain. Consequently, aligning features from different graphs is a
daunting task, as there is no straightforward method to reconcile
the disparate languages of each dataset into a unified representation
for machine learning models to process.
Objectives: In this paper, we focus on the above two challenges,
the disparity and isolation of graph datasets and the difficulty in
aligning their diverse features. We denote our pretraining datasetsas comprising 𝑀graphs, represented as G(𝑖)=(V(𝑖),E(𝑖)),𝑖∈
{1,2,···,𝑀}, whereV(𝑖)={𝑣(𝑖)
1,𝑣(𝑖)
2,···,𝑣(𝑖)
|V𝑖|}andE(𝑖)=V(𝑖)×
V(𝑖)denote the sets of nodes and edges, respectively. Each G(𝑖)
is associated with feature matrix 𝑋(𝑖)∈R|V(𝑖)|×𝑑𝑖and adjacency
matrix𝐴(𝑖)∈R|V(𝑖)|×|V(𝑖)|. Our objective is to train a GNN ℎ(·)
parameterized by Θ, which is capable of encoding knowledge trans-
ferable to downstream tasks. The downstream dataset is represented
asG(𝑡)=(V(𝑡),E(𝑡))with the feature matrix 𝑋(𝑡)and adajcency
matrix𝐴(𝑡). To address this question, we carefully design a gen-
eral pretraining scheme that is independent of datasets, network
architectures and downstream tasks.
3 METHOD
3.1 Overview of Our Framework
In this section, we introduce a cohesive approach that enables the
simultaneous pretraining of a graph model on multiple datasets.
We utilize established pretraining objectives, namely GraphCL [ 46]
and SimGRACE [ 43], to guide the learning process. Additionally,
we implement novel techniques specifically designed to overcome
the challenges highlighted in Section 2. A visual representation of
our methodology is provided in Figure 2.
3.2 Aligning Graphs by Coordinators
Different graphs usually have different features and structural pat-
terns. Here we propose a two-phase graph alignment approach. The
first step is to make the feature dimensions all the same in format.
Then we seek to further learn a latent data alignment strategy by
coordinators, which can reformulate graphs w.r.t their structural
patterns and semantic patterns.
3.2.1 Feature Projection. During the pretraining phase of our GNN
model, we first present a projecting module to align feature dimen-
sion, which is described by:
˜𝑋(𝑖)=Proj(𝑋(𝑖))∈R|V(𝑖)|×𝑑P, (1)
where Proj(·)denotes a certain projection operation and 𝑑Pdenotes
the predefined projected dimension. Without loss of generality, we
provide two widely used methods, singular value decomposition
(SVD) and attention mechanism, as two representative projection
operations in this paper. However, it is worth mentioning that the
mere projection of features onto a common plane does not suffice
to address the alignment challenge; additional alignment endeavors
are indispensable.
4445KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
3.2.2 Graph Coordinators. Based on the above step, we present
a novel concept: virtual nodes, referred to herein as “coordina-
tors”. These coordinators are strategically crafted to enhance inter-
connectivity among disparate graphs, facilitating the alignment of
their features and structural conflicts.
(i). Cross Connection between Coordinators and Datasets: To en-
sure that each graph retains its unique structural properties while
still participating in a larger, interconnected system, we assign a
dedicated coordinator to each graph dataset. This coordinator is not
just a peripheral addition; it becomes an integral component of the
graph by forming a fully connected subnetwork with every node
within the graph to which it is assigned. This design choice ensures
that the coordinator has an immediate, direct line of communica-
tion with each node, allowing it to efficiently relay information and
coordinate interactions within its respective graph.
(ii). Inner Connection within Coordinators: However, our ambition
extends beyond enhancing individual graph structures; we aim to
facilitate a rich tapestry of connections spanning all graphs within
our dataset collection. To achieve this, we strategically connect
coordinators from different graphs. These inter-coordinator edges
act as bridges, enabling the flow of information between disparate
graphs that would otherwise remain siloed. As a result, we construct
a comprehensive communication infrastructure that seamlessly
integrates the entirety of the graph collection, fostering a new level
of collaboration and knowledge sharing among them. With the
above modification, we can get a joint adjacency matrix ˜𝐴, which
consists of adjacency matrices of original graphs and new rows for
coordinators. Mathematically, ˜𝐴can be defined as follows:
˜𝐴=𝐴diag𝑅𝑇
𝐴
𝑅𝐴𝑅𝑅
, (2)
where𝐴diag=Diag(𝐴(1),𝐴(2),···,𝐴(𝑀)),𝑅𝑅=1𝑀×𝑀,𝑅𝐴=
Stack(𝑅(1)
𝐴,𝑅(2)
𝐴,···,𝑅(𝑀)
𝐴).Diag means concatenating matrices
diagonally, and Stack means stacking row-vectors into a matrix.
Specifically, 𝑅(𝑖)
𝐴∈RÍ𝑀
𝑘|V(𝑘)|and we have the 𝑗th value of𝑅(𝑖)
𝐴:
𝑅(𝑖)
𝐴(𝑗)=(
1Í𝑖
1|V(𝑘)|≤𝑗<Í𝑖+1
1|V(𝑘)|
0 otherwise .(3)
We treat the coordinator representation as learnable parameters.
We opt for a flexible approach so that they can evolve organically
alongside the GNN training, adapting dynamically to the data. This
ensures they continually refine their attributes to effectively serve
as conduits of evolving graph embeddings.
(iii). Generate Graph Batches for Efficient Training: Moreover,
by harnessing the interconnectivity facilitated by coordinators, we
unlock the potential for joint sampling of nodes originating from di-
verse data graphs. This innovative strategy empowers the training
process through aggregated batches, thereby promoting the natu-
ral alignment of features across datasets. Exposing the model to a
plethora of features within a single learning iteration prompts it to
seek out unified representations, effectively synthesizing insights
from disparate sources into cohesive and comprehensive representa-
tions. This unified approach not only enhances the model’s ability
to capture the underlying structure of the data but also fosters
robust generalization across domains.3.2.3 Why It Works? Kindly note that our proposed coordinators
have the same theory support as graph prompts. The main differ-
ence is that general graph prompts are used for downstream tasks
while we use coordinators during pretraining. Evidenced by [ 7,36],
prompts are demonstrated to be tantamount to graph transforma-
tions (e.g., node, edge, graph removing or adding, and some node
feature operations). In parallel, our coordinators can be construed
as a form of graph transformation. Given a GNN denoted as 𝑔(·)
and a prompt 𝑝, it has been established that:
𝑔(˜𝑋+𝑝,˜𝐴)=𝑔(𝑡(˜𝑋,˜𝐴))+𝑂𝑔𝑝 (4)
where𝑂𝑔𝑝represents an error bound associated with the GNN
𝑔(·)and the prompt 𝑝. Here,𝑡(·)signifies a graph transformation
operation. Given that the learnable features embedded within our
coordinators can be interpreted as the prompt 𝑝, they facilitate
equivalent graph transformations for the originally isolated graphs.
The learned equivalent transformation effectively harmonizes these
graphs (e.g., homophilic and heterophilic graphs), enabling the
discovery of inherent commonalities among them.
Additionally, we would like to facilitate a better understanding
of our framework’s design principles and the functions of its vari-
ous components through ‘Interdisciplinary Teaching’, which is a
cutting-edge approach in the field of education:
‘Interdisciplinary Teaching’ aims to enhance the comprehensive-
ness and depth of learning by integrating knowledge and methods
from different subjects, similar to how GCOPE integrates diverse
graph datasets. It addresses the complexity of real-world prob-
lems (like diverse and complex structures, attributes, and structure-
attribute patterns), requiring a multidisciplinary perspective for
solutions and encouraging students to apply cross-domain knowl-
edge. This approach leads to a more thorough understanding and
higher-level thinking skills, mostly like the transferrable pretrained
graph representations. The integration strategy in education, which
highlights core concepts and fosters interaction among disciplines,
resembles the coordinators in GCOPE. This helps students see the
bridges between subjects, nurturing flexible and innovative think-
ing, like transferring to new downstream datasets, especially the
few-shot scenarios.
3.3 Pretraining on Multi-domain Graphs
Our approach is very flexible and compatible with many pretraining
approaches. Here, we present a general pretraining framework that
can extract high-quality embeddings at both the node and graph
levels. Prior research has predominantly focused on pretraining
within the same data domains as the downstream task [ 14]. Within
this context, GraphCL [ 46] and SimGRACE [ 43] have been particu-
larly noteworthy for their effectiveness in generating granular node
embeddings as well as holistic graph embeddings. GraphCL em-
ploys graph data augmentation to generate positive pairs, whereas
SimGRACE perturbs the GNN to achieve this objective. Given the
strengths of these methods, we chose them as our basic pretraining
strategy.
Next, we move on to the crucial task of preserving the integrity
of information from each graph. To this end, we introduce an aux-
iliary feature reconstruction loss. This loss is quantified through
the mean squared error (MSE) metric, which assesses the discrep-
ancy between the original node feature vector (after projection)
4446All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining KDD ’24, August 25–29, 2024, Barcelona, Spain
˜𝑥𝑖and the reconstructed feature vector ˆ𝑥𝑖. The latter is generated
by applying a multilayer perceptron (MLP) to the embedded node
representation ℎ𝑖, thus enabling the evaluation of the fidelity with
which the embedding process preserves the original node features.
The purpose of this loss function is to ensure that, despite the pro-
jection of features, the salient and defining characteristics of each
graph’s information are retained and remain compatible. To be
concrete, taking GraphCL as an example, our pretraining objective
can be described as follows:
L=−logexp(sim(ℎ(PS(˜𝑋,˜𝐴,𝑎𝑖)),ℎ(PS(˜𝑋,˜𝐴,𝑎𝑗))/𝜏)
Íexp(sim(ℎ(PS(˜𝑋,˜𝐴,𝑎𝑖)),ℎ(NS(˜𝑋,˜𝐴,𝑎𝑗))/𝜏)+∥˜𝑋−ˆ𝑋∥2(5)
where ˜𝑋is the concatenated feature matrix of all pretraining datasets
after projection (Equation 1), ˜𝐴is the adjacency matrix after being
connected by the coordinators (Equation 2), PS, NS respectively
denote the positive sampling and negative sampling, sim denotes a
similarity metric (e.g., cosine similarity), 𝑎𝑖,𝑎𝑗are different graph
augmentations, and 𝜆is the reconstruction loss coefficient that
controls how much attention is paid to the reconstruction task.
3.4 Applying Knowledge to Downstream Data
Our pretraining methodology is inherently flexible and agnostic to
downstream task constraints, making it compatible with various
adaptation techniques. For instance, the recent introduction of
prompting in the graph field has attracted considerable interest
due to its remarkable efficiency and effectiveness [ 36]. To show the
superior generalization ability of our GCOPE method, we explore
its transferability within both the conventional finetuning paradigm
and the nascent graph prompt framework.
Drawing inspiration from the methodology presented by [ 36], we
cast downstream tasks into a graph-level framework. As delineated
in their work, the maximal retention of pretraining knowledge is
observed when downstream tasks are aligned within the same task
space as the pretraining task. Given our pretraining is conducted
at the graph level, we correspondingly transform all downstream
tasks to this level by constructing induced subgraphs. The main
algorithm is presented in Algorithm 1.
Algorithm 1: GCOPE
Input: Source graphs{G(𝑖)}𝑀
𝑖=1, target graphG(𝑡), GNN
parameters Θ, projection operation Proj(·),
pretraining objective L(·) , learning rate 𝛼,
transferring pipeline Trans(·)
Output: The optimal model on the target graph 𝑔𝑡(·)
1for𝑖←0to𝑀do
2 ˜𝑋(𝑖)=Proj(𝑋(𝑖))
3end
4˜𝑋=Cat(˜𝑋(1),˜𝑋(2),···,˜𝑋(𝑀))
5˜𝐴=𝐴diag𝑅𝑇
𝐴
𝑅𝐴𝑅𝑅
6while not converge do
7Θ←Θ−𝛼∇ΘL(˜𝑋,˜𝐴,Θ)
8end
9𝑔𝑡(·)=Trans(G(𝑡),Θ)
10return𝑔𝑡(·)Table 1: Statistics of datasets.
Homophilic
Data Cora Citeseer Pubmed Computers Photos
#
Nodes 2,708 3,327 19,717 13,752 7,650
# Edges 10,556 9,104 88,648 491,722 238,162
# Features 1,433 3,703 500 767 745
# Labels 7 6 3 10 8
ℎ(𝐺) 0.810 0.736 0.802 0.777 0.827
Heter
ophilic Data Wisconsin Texas Cornell Chameleon Squirrel
#
Nodes 251 183 183 2,277 5,201
# Edges 515 325 298 62792 396,846
# Features 1,703 1,703 1,703 2,325 2,089
# Labels 5 5 5 5 5
ℎ(𝐺) 0.196 0.108 0.305 0.231 0.222
3.5 Complexity Analysis
Thanks to the simplicity of our pretraining framework, the increase
in parameter complexity is only marginal. The only additional
parameters introduced are the features of the coordinators, with a
complexity ofO(𝑀𝑑A), which scales linearly with the number of
pretraining datasets. In practical settings, scenarios characterized by
an excessively abundant availability of pretraining datasets are rare.
Assuming GNN we employ comprises 𝐿layers with a maximum
layer width of 𝑑, and let𝑁=Í𝑀
𝑘=1|V(𝑘)|and𝐸=Í𝑀
𝑘=1|E(𝑘)|. It
is worth noting that the time complexity of a typical graph model,
such as Graph Convolutional Network (GCN), is O(𝐿𝑁𝑑2+𝐿𝐸𝑑+
𝑁𝑑)[36]. With the incorporation of coordinators, the revised time
complexity becomes O(𝐿(𝑁+𝑀)𝑑2+𝐿(𝐸+𝑁+𝑀)𝑑+(𝑁+𝑀)𝑑).
The additional time complexity is O(𝐿𝑀𝑑2+𝐿(𝑁+𝑀)𝑑+𝑀𝑑).
Given that 𝑀≪𝑁, the supplementary time cost scales nearly
linearly with the number of original nodes.
4 EXPERIMENTS
In this section, we study the experimental results of our proposed
method and baselines to answer five research questions:
•RQ1. What advantages does our proposed GCOPE method offer
over representative baselines in cross-domain transfer learning?
•RQ2. How significant is the presence of edges among different
coordinators?
•RQ3. Is the reconstruction loss necessary in GCOPE?
•RQ4. Can downstream prompts derive benefits from the cross-
domain pretraining?
•RQ5. Which projection operation is more compatible for CCOPE?
4.1 Experimental Setup
Datasets. For comprehensive comparisons, we conduct experi-
ments on ten real-world datasets. We choose five homophily datasets
in experiments, including Cora [ 32], Citeseer [ 32], Pubmed [ 29],
Computers and Photos datasets [ 28,33], and five heterophilic datasets,
including three sub-datasets derived from the WebKB [ 30] (Cornell,
Texas, and Wisconsin) and two page-page networks (Chameleon
and Squirrel) extracted from Wikipedia [ 30]. Table 1 summarizes
the details, where ℎ(𝐺)is a metric that represents the degrees
of homophily and heterophily. The values of ℎ(𝐺)are directly
drawn from [ 45], indicating that the first five datasets are highly
homophilic and the latter five are highly heterophilic [ 27,45]. Dif-
ferent degrees of homophily and heterophily indicate different se-
mantics in graphs. For more detailed information on these datasets,
please check in Appendix.
4447KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
Table 2: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on homophilic datasets (C-way-1-shot). IMP (%):
the average improvement of GCOPE over the rest. GCL and Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsCora Citese
er Pubme
d Computers P
hotos
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.3012±.06 0.6444±.04 0.2591±.04 0.4358±.09 0.7234±.07 0.3583±.10 0.4210±.01 0.6040±.06 0.3026±.04 0.2602±.07 0.6773±.02 0.2428±.04 0.4603±.04 0.8458±.01 0.4592±.04
GA
T 0.3646±.04 0.6769±.03 0.3108±.04 0.3695±.05 0.7232±.06 0.3305±.04 0.4209±.04 0.5710±.06 0.3227±.07 0.3482±.07 0.6878±.05 0.2397±.05 0.4742±.08 0.8213±.02 0.4498±.07
BW
GNN 0.2543±.05 0.5563±.03 0.1971±.02 0.3599±.07 0.6954±.05 0.3112±.06 0.3976±.03 0.4934±.03 0.2686±.04 0.2768±.05 0.6273±.03 0.1864±.03 0.4113±.04 0.7769±.00 0.3883±.01
F
AGCN 0.3819±.03 0.6818±.04 0.3009±.09 0.5219±.08 0.8042±.03 0.4667±.08 0.4522±.02 0.5622±.04 0.4275±.07 0.4651±.04 0.7762±.02 0.3009±.07 0.5937±.05 0.8847±.00 0.5346±.03
IP
+
finetuningGCL+GCN 0.2507±.06 0.6350±.03 0.2240±.03 0.3140±.02 0.6661±.04 0.2397±.02 0.4217±.02 0.5257±.05 0.2896±.07 0.2856±.04 0.6467±.03 0.1653±.06 0.5533±.01 0.8661±.01 0.5217±.01
GCL+F
AGCN 0.3892±.05 0.7228±.03 0.3619±.05 0.4461±.02 0.7781±.01 0.4126±.02 0.4532±.02 0.5708±.03 0.4168±.04 0.4371±.06 0.7616±.01 0.3450±.02 0.6273±.01 0.8710±.01 0.5406±.03
Sim+GCN 0.2492±.02 0.5765±.03 0.1567±.04 0.2950±.06 0.6203±.06 0.1812±.06 0.3980±.01 0.5067±.02 0.2805±.01 0.2666±.10 0.6286±.01 0.1603±.03 0.4290±.04 0.7645±.02 0.3955±.02
Sim+F
AGCN 0.3957±.03 0.7284±.02 0.3585±.01 0.5101±.03 0.7969±.01 0.4615±.04 0.4398±.01 0.5535±.01 0.4225±.02 0.4393±.01 0.7718±.02 0.3100±.02 0.5704±.02 0.8543±.02 0.4984±.01
GCOPE
+
finetuningGCL+GCN 0.3368±.02 0.6971±.04 0.2967±.03 0.3701±.03 0.7066±.02 0.3265±.05 0.4443±.04 0.5888±.04 0.4242±.04 0.3439±.03 0.7023±.01 0.2976±.03 0.5635±.02 0.8733±.00 0.5480±.02
GCL+F
AGCN 0.4618±.03 0.7597±.05 0.4388±.05 0.5631±.03 0.8258±.02 0.4953±.04 0.4591±.01 0.5512±.01 0.4203±.03 0.4465±.01 0.7747±.00 0.3432±.03 0.6329±.02 0.8850±.00 0.5935±.03
Sim+GCN 0.2525±.05 0.5744±.03 0.1722±.06 0.3475±.05 0.6527±.05 0.2704±.05 0.4116±.00 0.5166±.04 0.2994±.03 0.3230±.01 0.6994±.00 0.2515±.00 0.4772±.03 0.7851±.01 0.4277±.02
Sim+F
AGCN 0.3875±.04 0.7163±.03 0.3355±.08 0.5704±.04 0.8425±.01 0.5178±.04 0.4727±.03 0.5587±.03 0.5672±.03 0.4677±.04 0.7875±.01 0.3823±.02 0.5985±.02 0.8757±.02 0.5556±.05
IMP
(%) 11.23%
5.23% 14.63% 13.81%
4.26% 16.59% 5.02%
0.99% 25.32% 13.79%
6.28% 30.70% 10.31%
2.30% 12.18%
Table 3: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on heterophilic datasets (C-way-1-shot).
T
raining
schemesMethodsWisconsin T
exas Cornell Chamele
on Squirr
el
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.6290±.05 0.8320±.04 0.4871±.14 0.5812±.08 0.6731±.04 0.4557±.10 0.3263±.04 0.5666±.01 0.3151±.03 0.2393±.03 0.5310±.04 0.1923±.03 0.2093±.00 0.5263±.01 0.1889±.01
GA
T 0.6009±.02 0.8346±.01 0.5217±.05 0.6300±.08 0.5854±.08 0.4282±.13 0.3275±.14 0.5306±.03 0.1497±.04 0.2342±.02 0.5205±.04 0.1379±.03 0.2118±.00 0.5195±.02 0.1160±.01
BW
GNN 0.5620±.05 0.8463±.02 0.5189±.05 0.7438±.10 0.6642±.07 0.6274±.22 0.3150±.09 0.5938±.06 0.2190±.05 0.2206±.02 0.5039±.03 0.1540±.03 0.2155±.00 0.5149±.00 0.1664±.02
F
AGCN 0.5222±.05 0.7905±.0310 0.4725±.06 0.6900±.06 0.7185±.01 0.5334±.12 0.2938±.06 0.6573±.04 0.2872±.05 0.2575±.02 0.5515±.02 0.1941±.01 0.2181±.00 0.5202±.00 0.1875±.02
IP
+
finetuningGCL+GCN 0.5249±.03 0.7876±.03 0.4415±.05 0.7350±.01 0.7210±.02 0.5636±.09 0.4175±.04 0.6350±.02 0.3500±.04 0.2249±.02 0.5213±.00 0.1432±.03 0.2118±.01 0.5059±.01 0.1110±.03
GCL+F
AGCN 0.6063±.04 0.8356±.01 0.5555±.07 0.7425±.03 0.7034±.03 0.6141±.09 0.2588±.04 0.6262±.04 0.2442±.04 0.2443±.00 0.5530±.01 0.1875±.01 0.2223±.00 0.5307±.00 0.1740±.02
Sim+GCN 0.5258±.04 0.7927±.05 0.4604±.06 0.6338±.05 0.6024±.07 0.4269±.14 0.3438±.13 0.5954±.09 0.2168±.09 0.2271±.01 0.5183±.02 0.1578±.03 0.2133±.00 0.5133±.01 0.1550±.02
Sim+F
AGCN 0.6335±.02 0.8557±.00 0.5830±.04 0.6725±.14 0.6922±.04 0.5906±.10 0.2725±.05 0.6433±.04 0.2617±.04 0.2748±.01 0.5652±.00 0.2011±.00 0.2170±.00 0.5213±.00 0.1716±.01
GCOPE
+
finetuningGCL+GCN 0.6606±.04 0.8487±.01 0.5952±.04 0.7738±.06 0.7387±.01 0.6763±.08 0.3975±.10 0.6694±.04 0.3120±.04 0.2411±.01 0.5564±.00 0.2210±.00 0.2245±.00 0.5207±.01 0.1741±.00
GCL+F
AGCN 0.6579±.03 0.8531±.01 0.5649±.00 0.7125±.02 0.6693±.02 0.6300±.03 0.4013±.05 0.6897±.01 0.3160±.02 0.2886±.00 0.5898±.00 0.2320±.00 0.2257±.00 0.5257±.00 0.1885±.01
Sim+GCN 0.5412±.03 0.8059±.02 0.4509±.06 0.6137±.18 0.6900±.03 0.4674±.10 0.3675±.09 0.6045±.04 0.2339±.04 0.2573±.02 0.5467±.01 0.1852±.01 0.2180±.00 0.5147±.00 0.1783±.00
Sim+F
AGCN 0.7321±.00 0.9305±.00 0.6873±.01 0.7950±.03 0.7451±.01 0.7042±.03 0.5925±.01 0.8069±.03 0.4626±.03 0.2894±.01 0.5662±.02 0.2192±.02 0.2193±.00 0.5370±.00 0.1984±.01
IMP
(%) 12.57%
4.58% 13.76% 6.65%
6.08% 16.87% 37.66%
14.29% 29.62% 11.97%
6.01% 25.36% 3.25%
1.06% 16.39%
Baselines. We compare our proposed method with the following
baselines, categorized into three groups and accompanied by con-
cise descriptions. (1) Supervised methods: These methods train a
GNN model on a downstream task and infer results directly. In this
work, four famous GNN models are adopted, including GCN [ 16],
GAT [ 40], BWGNN [ 38], and FAGCN [ 1]. We choose GCN and
FAGCN as the backbones for our proposed GCOPE method since
FAGCN is tailored to address both homophilic and heterophilic
graphs [ 45] and GCN, a widely used GNN model, is the basis of
FAGCN. (2) Isolated Pretraining (IP) with finetuning: These
methods initially utilize multiple cross-domain datasets as source
datasets, which are combined in an isolated manner to pretrain
a GNN model in a self-supervised fashion (e.g. GraphCL [ 46] or
SimGRACE [ 43]). Here, ‘isolated’ denotes that the datasets are
amalgamated into a batch object without establishing inter-dataset
connections, resulting in an adjacency matrix composed of distinct
blocks. Subsequently, the pretrained model undergoes finetuning
for a new downstream task. (3) Graph Coordinators for Pre-
training (GCOPE) with transferring: With learnable coordina-
tors, our proposed GCOPE method aims to unify the isolated source
datasets into one large-scale dataset with inter-dataset connections
for pretraining and then transfer the pretrained GNN model to a
downstream task via finetuning or prompting [36].Metrics and Implementations. We choose three widely used
metrics to evaluate the node classification task [ 9,13,36,45], in-
cluding classification accuracy (Acc), mean AUC-ROC (AUC), and
mean F1-score (F1). We leverage a 10-fold partition strategy to split
the 10 real-world datasets, nine datasets as cross-domain source
datasets for pretraining and the rest one dataset as the target dataset
for transferring. To unify features of multiple cross-domain source
datasets onto one single plane, we leverage SVD to reduce the ini-
tial features to 100 dimensions. We fix the number of graph neural
layers at 2, with a hidden dimension of 100 for GCN, FAGCN, and
GAT models. Additionally, we adopt the identical hyperparameters
as specified in [ 38] for BWGNN. For all networks, we apply the
Adam optimizer and the learning rate is assigned as 0.0001. In terms
of GCOPE, we introduce one coordinator for each source dataset
and assign 0.2as the default reconstruction weight. Finally, in the
transferring stage, we adopt node classification as the downstream
task. For fair comparisons, we apply the C-way-K-shot learning
setting, same as [ 26], for each target dataset to build the training
data and then split the rest of the data randomly in 1:9 for vali-
dating and testing. In this paper, we report average results on all
downstream tasks. More implementation details are shown in Ap-
pendix A, in which we also analyze the performance of GCOPE
with more coordinators and dynamical edges between coordinators.
4448All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining KDD ’24, August 25–29, 2024, Barcelona, Spain
4.2 Cross-domain Transfer Performance with
Few-shot Learning Settings (RQ1)
We compare our proposed GCOPE method with all the baselines on
the downstream node classification tasks under the C-way-1-shot
setting. We repeat the evaluation 5 times and report the average
results and standard deviations in Table 2 and Table 3. The results
of supervised methods are the benchmark to help validate whether
the pretrained GNN models effectively transfer to the downstream
tasks. As is known, pretraining GNNs aims to transfer prior knowl-
edge from upstream tasks to improve the performance of GNNs on
downstream tasks, especially in few-shot scenarios. However, we
observe that most IP with finetuning methods struggle to achieve
better performance compared with supervised methods, embody-
ing the negative transfer phenomenon. This is due to the apparent
differences in distribution across cross-domain datasets. Under the
IP strategy, each graph sample only contains information from one
of the nine distributions, making it challenging for GNN models
to fit the nine independent distributions into a unified one and
learn general graph representations effectively. In contrast, our pro-
posed GCOPE with finetuning methods significantly outperforms
almost all baselines, achieving positive transfer. This is because
coordinators connect all the source datasets together, unifying nine
independent distributions into one cohesive joint distribution. In
the pretraining stage, all the training samples are drawn from this
joint distribution, facilitating the sharing of information across
cross-domain datasets. Consequently, the GNN model is able to bal-
ance the shared information and learn better graph representations
for transferring knowledge on the downstream task. The reported
improvements range from 3.25% up to 37.66% in terms of node
classification accuracy. Kindly note that our few-shot experiment
settings are different from the classic few-shot node classification
settings that are actually designed for the meta-learning task. Addi-
tionally, we also evaluate our proposed GCOPE on C-way-3-shot
and C-way-5-shot settings, shown in Table A1 and Table A2.
4.3 Interconnectivity Analysis (RQ2)
In this section, we investigate the impact of edges between different
coordinators on the effectiveness of our proposed GCOPE frame-
work. Specifically, we compare two variants: GCOPE/w, which
includes inter-coordinator edges, and GCOPE/wo, which excludes
them. Both variants share the same hyperparameters, with FAGCN
as the backbone model and pretraining based on GraphCL. Addi-
tionally, the datasets utilized for pretraining and downstream tasks
are partitioned using the 10-fold strategy. Table 4 presents the node
classification accuracy (mean±std). Our experimental findings un-
derscore the essential and effective role of inter-coordinator edges
in GCOPE.
4.4 Reconstruction Analysis (RQ3)
We conduct a comparison of downstream node classification per-
formance on Citeseer between our proposed GCOPE method with
varying reconstruction loss coefficient values 𝜆, and a supervised
method to assess the efficacy of the reconstruction module. Specifi-
cally, FAGCN serves as the backbone model for both GCOPE and
the supervised method. For GCOPE, GraphCL is utilized as the pre-
training strategy. All other hyperparameters between GCOPE and
the supervised method are maintained consistent. Figure 3 depicts
the experimental results, focusing on Acc, AUC, and F1 metrics.Table 4: Node classification accuracy (mean±std) of
GCOPE/wo and GCOPE/w, which represent GCOPE without
or with inter-coordinator edges respectively.
Homo
Data GCOPE/w
o GCOPE/w Heter
o Data GCOPE/w
o GCOPE/w
Cora 0.3575±.02 0.4618±.03 Wisconsin 0.6335±.04 0.6579±.03
Citese
er 0.5062±.02 0.5631±.03 T
exas 0.6863±.04 0.7125±.02
Pubme
d 0.4108±.03 0.4591±.01 Cornell 0.3212±.03 0.4013±.05
Computers 0.4244±.01 0.4465±.01 Chamele
on 0.2647±.01 0.2886±.00
P
hotos 0.6037±.03 0.6329±.02 Squirr
el 0.2177±.00 0.2257±.00
Our experimental findings yield the following observations:
First, GCOPE without reconstruction ( 𝜆=0.0) outperforms the
supervised pretraining, highlighting the effectiveness of the intro-
duced coordinators. Second, GCOPE with reconstruction ( 𝜆>0.0)
achieves optimal performance when 𝜆is set to 0.2, surpassing both
the supervised pretraining and GCOPE ( 𝜆=0.0). This improve-
ment is attributed to the reconstruction module’s ability to align
graph features across datasets, facilitating more effective learning
of shared information by GNNs from cross-domain source datasets.
Third, as𝜆exceeds 0.2, performance begins to deteriorate, ulti-
mately falling short of both the supervised pretraining and GCOPE
(𝜆=0.0). This decline can be attributed to excessively large 𝜆values,
which cause the model to overly prioritize reconstruction at the
expense of its primary pretraining task. In summary, the inclusion
of the reconstruction module with relatively small 𝜆values proves
essential for our proposed GCOPE method.
4.5 Transferring by Graph Prompt (RQ4)
In addition to finetuning, we explore the feasibility of leveraging
the graph prompt technique to transfer upstream cross-domain
knowledge learned by GCOPE. Specifically, we adopt the imple-
mentation of ProG [ 36], a widely used graph prompt method, which
involves freezing the parameters of pretrained GNNs and incorpo-
rating graph prompts in the downstream node classification task.
Subsequently, we evaluate the performance of GCOPE with ProG
on four downstream datasets, comprising two homophilic and two
heterophilic datasets. The experimental results are presented in Ta-
ble 5. For comparison, we include results of the supervised method,
IP with finetuning, and GCOPE with finetuning.
Based on our experimental observations, we draw the following
conclusions: In comparison to the supervised and IP with finetun-
ing methods, both GCOPE with finetuning and GCOPE with ProG
exhibit superior performance. Notably, GCOPE with ProG achieves
positive transfer with minimal tunable parameters in the down-
stream node classification task. While the performance of GCOPE
with ProG is slightly lower than that of GCOPE with finetuning, the
disparity between the two methods is significantly narrower than
that between GCOPE with ProG and the supervised method. Based
on the aforementioned analysis, we can assert that our proposed
GCOPE framework can effectively benefit downstream prompts.
4.6 Compatibility Analysis of Projection
Operation (RQ5)
To study the compatibility of various projection operations, we
evaluate our proposed GCOPE method with SVD or attention mech-
anism across ten cross-domain real-world datasets. Specifically, we
set FAGCN as the backbone, leverage GraphCL for pretraining, and
finally transfer the pretrained GNNs to the downstream C-way
4449KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
0.0 0.1 0.2 0.3 0.4 0.5
0.500.520.540.560.58Acc
GCOPE
Supervised
0.0 0.1 0.2 0.3 0.4 0.5
0.780.800.820.84AUC
GCOPE
Supervised
0.0 0.1 0.2 0.3 0.4 0.5
0.420.440.460.480.500.520.54F1
GCOPE
Supervised
Figure 3: Node classification performance (mean±std) of GCOPE with varying reconstruction loss coefficient ( 𝜆) values on
Citeseer under C-way-1-shot setting.
Table 5: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) of GCOPE with ProG (C-way-1-shot). GCL and
Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsCora Citese
er Wisconsin T
exas
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervised FAGCN 0.3819±.03 0.6818±.04 0.3009±.09 0.5219±.08 0.8042±.03 0.4667±.08 0.5222±.03 0.7905±.03 0.4725±.06 0.6900±.06 0.7185±.01 0.5334±.01
IP
+
finetuningGCL+FAGCN 0.3892±.05 0.7228±.03 0.3619±.05 0.4461±.02 0.7781±.01 0.4126±.02 0.6063±.04 0.8356±.01 0.5555±.07 0.7425±.03 0.7034±.03 0.6141±.00
Sim+F
AGCN 0.3957±.03 0.7284±.02 0.3858±.01 0.5101±.03 0.7969±.01 0.4615±.04 0.6335±.02 0.8557±.00 0.5830±.04 0.6725±.01 0.6922±.04 0.5906±.10
GCOPE
+
ProGGCL+FAGCN 0.4283±.03 0.7552±.01 0.4326±.01 0.5251±.01 0.8314±.00 0.5042±.01 0.6208±.04 0.8770±.01 0.6359±.07 0.7438±.01 0.6689±.00 0.5880±.05
Sim+F
AGCN 0.3941±.01 0.6892±.03 0.3507±.09 0.5270±.01 0.8051±.01 0.4821±.02 0.7312±.01 0.8931±.00 0.6579±.01 0.7700±.08 0.7356±.01 0.7353±.06
GCOPE
+
finetuningGCL+FAGCN 0.4618±.03 0.7597±.05 0.4388±.05 0.5631±.03 0.8258±.02 0.4953±.04 0.6579±.03 0.8531±.01 0.5649±.00 0.7125±.02 0.6693±.02 0.6300±.03
Sim+F
AGCN 0.3875±.04 0.7163±.03 0.3355±.08 0.5704±.04 0.8425±.01 0.5178±.04 0.7321±.00 0.9305±.00 0.6873±.01 0.7950±.03 0.7451±.01 0.7042±.03
Table 6: Accuracy (mean±std) of GCOPE with finetuning on
different projection operations. Experiment settings are the
same as Table 2.
Homo
Data Attention
SVD Heter
o Data Attention
SVD
Cora 0.3120±.02 0.4618±.03 Wisconsin 0.5692±.05 0.6579±.03
Citese
er 0.3861±.07 0.5631±.03 T
exas 0.6663±.06 0.7125±.02
Pubme
d 0.4475±.02 0.4591±.01 Cornell 0.3737±.03 0.4013±.05
Computers 0.3689±.06 0.4465±.01 Chamele
on 0.2755±.00 0.2886±.00
P
hotos 0.4857±.04 0.6329±.02 Squirr
el 0.2210±.00 0.2257±.00
1-shot node classification tasks. For GCOPE with attention mecha-
nism, we equip each dataset with a data-specific attention module
to project their respective features into a shared plane, regardless
of their position in the pretraining and finetuning pipeline. We
report the node classification accuracy (mean ±std ) in Table 6. We
can easily observe that GCOPE with attention mechanism signifi-
cantly underperforms the ones with SVD. This could be attributed
to the fact that the utilized samples are insufficient to train attention
modules effectively. Each attention module has 32,792,288tuned
parameters, while SVD is a non-parametric mathematical method.
Therefore, we ensure that SVD is more compatible under few-shot
learning scenarios. For effectiveness and simplicity, we set SVD as
the default projection operation of GCOPE in this work.
5 RELATED WORK
Graph Pretraining. In the machine learning (ML) research field,
pretraining is widely acknowledged for its ability to leverage ex-
isting data to train a feature extractor with robust generalization
capabilities [ 5,21,24,31,50]. Specifically within the graph do-
main, pretraining methods can be categorized into three main
types: generation-based, auxiliary property-based, and contrast-
based methods [ 25]. Generation-based methods utilize feature orstructure reconstruction as the loss function to extract embed-
dings with strong generalization properties (e.g., GAE [ 15], Graph-
MAE [12], and GraphMAE2 [11]). Auxiliary property-based meth-
ods introduce new attributive or structural properties as supervi-
sion signals, such as clustering pseudo labels utilized by M3S [35].
Contrast-based methods define positive and negative embedding
pairs and aim to bring positive pairs closer while pushing negative
pairs apart. Among these three categories, contrast-based methods
have garnered the most popularity and achieved notable successes
[8,10,34,41,43,46,51]. However, despite the success of these
methods across various graph tasks, none have managed to achieve
the objective of pretraining on multiple graph datasets belonging
to different domains. Consequently, the efficacy of pretraining re-
mains constrained by the size and diversity of the source data. This
remains an open question in the graph pretraining field.
Graph Transfer Learning. Recent years have witnessed signifi-
cant advancements in GNNs. However, adapting pre-trained GNNs
to diverse downstream tasks efficiently remains a challenge. Tradi-
tionally, finetuning, as presented in [ 3,17,52], has dominated this
field. This approach leverages a pretrained GNN as a foundation,
finetuning either the final layer or the entire model for the specific
task. Finetuning has consistently achieved state-of-the-art (SOTA)
performance.
However, recent exploration has led to the emergence of graph
prompts [ 18,36,37,42] as a compelling alternative, drawing inspira-
tion from the NLP community’s ‘prompting’ paradigm [ 19,23,44].
As detailed in [ 36], a graph prompt comprises three key elements:
prompt tokens, which contain the prompt content in the form of
vectors; token structures, which depicts how the tokens are con-
nected; inserting patterns, which fuses the graph prompt with the
target data. By carefully crafting these components, often through
learning-based approaches, graph prompts can effectively transfer
4450All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining KDD ’24, August 25–29, 2024, Barcelona, Spain
knowledge from the pretrained model to new tasks. While not guar-
anteed to outperform finetuning in every scenario, graph prompts
excel in terms of efficiency due to their minimal parameter foot-
print. This shift towards graph prompts highlights the ongoing
exploration of efficient and effective knowledge transfer within
the realm of GNNs. Further research is warranted to refine prompt
construction strategies and assess their broader applicability across
diverse downstream tasks.
6 CONCLUSION
This study delves into the intricate phenomenon of negative transfer
within the field of graph learning, shedding light on its complexities
through a rigorous analysis. In response, we introduce an innovative
pretraining approach named GCOPE, devised to effectively mitigate
negative transfer effects. GCOPE leverages coordinators to seam-
lessly amalgamate disparate graphs, establishing interconnections
and aligning their features. Our thorough experimentation, con-
ducted across a spectrum of homophilic and heterophilic datasets,
vividly demonstrates the efficacy of our proposed method.
Despite the notable success achieved by GCOPE, it is prudent
to acknowledge the potential limitations stemming from the non-
parametric nature of SVD. This aspect may constrain the method’s
generalizability across diverse datasets. As such, our future research
endeavors will be directed towards devising a more robust learning-
based feature projection pipeline, which naturally understands and
aligns features of graphs from a variety of domains.
ACKNOWLEDGMENTS
This Research of Li was supported by NSFC Grant No. 62206067 and
Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673. The
research of Cheng was supported in part by project #MMT-p2-23
of the Shun Hing Institute of Advanced Engineering, The Chinese
University of Hong Kong and by grants from the Research Grant
Council of the Hong Kong Special Administrative Region, China
(No. CUHK 14217622).
REFERENCES
[1]Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequency
information in graph convolutional networks. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, Vol. 35. 3950–3957.
[2]Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. 2023.
Understanding and improving visual prompting: A label-mapping perspective. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
19133–19143.
[3]Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024. GraphWiz: An Instruction-
Following Language Model for Graph Problems. arXiv preprint arXiv:2402.16029
(2024).
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597–1607.
[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[6]Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie
Tang. 2021. All nlp tasks are generation tasks: A general pretraining framework.
[7]Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompt
tuning for graph neural networks. arXiv preprint arXiv:2209.15240 (2022).
[8]Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural information processing
systems 33 (2020), 21271–21284.
[9]Russell Alan Hart, Linlin Yu, Yifei Lou, and Feng Chen. 2023. Improvements on
Uncertainty Quantification for Node Classification via Distance Based Regular-
ization. In Thirty-seventh Conference on Neural Information Processing Systems.[10] Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view
representation learning on graphs. In International conference on machine learning.
PMLR, 4116–4126.
[11] Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov,
and Jie Tang. 2023. GraphMAE2: A Decoding-Enhanced Masked Self-Supervised
Graph Learner. In Proceedings of the ACM Web Conference 2023. 737–746.
[12] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,
and Jie Tang. 2022. Graphmae: Self-supervised masked graph autoencoders. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 594–604.
[13] Jongwon Jeong, Hoyeop Lee, Hyui Geon Yoon, Beomyoung Lee, Junhee Heo,
Geonsoo Kim, and Jin Seon Kim. 2024. iGraphMix: Input Graph Mixup Method
for Node Classification. In The Twelfth International Conference on Learning
Representations. https://openreview.net/forum?id=a2ljjXeDcE
[14] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang
Tang. 2020. Self-supervised learning on graphs: Deep insights and new direction.
arXiv preprint arXiv:2006.10141 (2020).
[15] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv
preprint arXiv:1611.07308 (2016).
[16] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[17] Jaekoo Lee, Hyunjae Kim, Jongsun Lee, and Sungroh Yoon. 2017. Transfer
learning for deep learning on graph-structured data. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 31.
[18] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and
Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress
and future directions. arXiv preprint arXiv:2311.12399 (2023).
[19] Yuhan Li, Wei Shen, Jianbo Gao, and Yadong Wang. 2022. Community ques-
tion answering entity linking via leveraging auxiliary data. arXiv preprint
arXiv:2205.11917 (2022).
[20] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:
Investigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprint
arXiv:2402.11235 (2024).
[21] Yuhan Li, Jian Wu, Zhiwei Yu, Börje F Karlsso, Wei Shen, Manabu Okumura,
and Chin-Yew Lin. 2023. Unlocking Science: Novel Dataset and Benchmark for
Cross-Modality Scientific Information Extraction. arXiv preprint arXiv:2311.08189
(2023).
[22] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,
and Muhan Zhang. 2023. One for all: Towards training one graph model for all
classification tasks. arXiv preprint arXiv:2310.00149 (2023).
[23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of
prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),
1–35.
[24] Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee Tsung,
Jia Li, and Yu Rong. 2024. SEGNO: Generalizing Equivariant Graph Neural
Networks with Physical Inductive Biases. In The Twelfth International Conference
on Learning Representations. https://openreview.net/forum?id=3oTPsORaDH
[25] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip.
2022. Graph self-supervised learning: A survey. IEEE Transactions on Knowledge
and Data Engineering 35, 6 (2022), 5879–5900.
[26] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt:
Unifying pre-training and downstream tasks for graph neural networks. In Pro-
ceedings of the ACM Web Conference 2023. 417–428.
[27] Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang,
Jie Fu, Jure Leskovec, and Doina Precup. 2023. When Do Graph Neural Networks
Help with Node Classification? Investigating the Homophily Principle on Node
Distinguishability. In Thirty-seventh Conference on Neural Information Processing
Systems. https://openreview.net/forum?id=kJmYu3Ti2z
[28] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
2015. Image-based recommendations on styles and substitutes. In Proceedings
of the 38th international ACM SIGIR conference on research and development in
information retrieval. 43–52.
[29] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-
driven active surveying for collective classification. In 10th international workshop
on mining and learning with graphs, Vol. 8. 1.
[30] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang.
2019. Geom-GCN: Geometric Graph Convolutional Networks. In International
Conference on Learning Representations.
[31] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. 2021.
Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972 (2021).
[32] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[33] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
4451KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
[34] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograph: Un-
supervised and semi-supervised graph-level representation learning via mutual
information maximization. arXiv preprint arXiv:1908.01000 (2019).
[35] Ke Sun, Zhouchen Lin, and Zhanxing Zhu. 2020. Multi-stage self-supervised
learning for graph convolutional networks on graphs with few labeled nodes. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 5892–5899.
[36] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. In Proceedings of the 26th
ACM SIGKDD international conference on knowledge discovery & data mining
(KDD’23). 2120–2131.
[37] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. 2023.
Graph Prompt Learning: A Comprehensive Survey and Beyond. arXiv preprint
arXiv:2311.16534 (2023).
[38] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. 2022. Rethinking graph neural
networks for anomaly detection. In International Conference on Machine Learning.
PMLR, 21076–21089.
[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[40] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.
[41] Petar Veličković, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341
(2018).
[42] Xuansheng Wu, Kaixiong Zhou, Mingchen Sun, Xin Wang, and Ninghao Liu. 2023.
A survey of graph prompting methods: techniques, applications, and challenges.
arXiv preprint arXiv:2303.07275 (2023).
[43] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. 2022. Simgrace: A
simple framework for graph contrastive learning without data augmentation. In
Proceedings of the ACM Web Conference 2022. 1070–1079.
[44] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. 2024. Large
language models can learn temporal reasoning. arXiv preprint arXiv:2401.06853
(2024).
[45] Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang,
and Hanghang Tong. 2023. Node classification beyond homophily: Towards a
general solution. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2862–2873.
[46] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems 33 (2020), 5812–5823.
[47] Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. 2022. A survey on
negative transfer. IEEE/CAA Journal of Automatica Sinica 10, 2 (2022), 305–329.
[48] Yihua Zhang, Yimeng Zhang, Aochuan Chen, Jinghan Jia, Jiancheng Liu, Gaowen
Liu, Mingyi Hong, Shiyu Chang, and Sijia Liu. 2023. Selectivity Drives Produc-
tivity: Efficient Dataset Pruning for Enhanced Transfer Learning. arXiv preprint
arXiv:2310.08782 (2023).
[49] Haihong Zhao, Bo Yang, Jiaxu Cui, Qianli Xing, Jiaxing Shen, Fujin Zhu, and
Jiannong Cao. 2023. Effective fault scenario identification for communication
networks via knowledge-enhanced graph neural networks. IEEE Transactions on
Mobile Computing (2023).
[50] Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, and Jia Li. 2024.
Weakly Supervised Anomaly Detection via Knowledge-Data Alignment. In Pro-
ceedings of the ACM on Web Conference 2024. 4083–4094.
[51] Yun Zhu, Jianhao Guo, Fei Wu, and Siliang Tang. 2022. Rosa: a robust self-
aligned framework for node-node graph contrastive learning. arXiv preprint
arXiv:2204.13846 (2022).
[52] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu
Zhu, Hui Xiong, and Qing He. 2020. A comprehensive survey on transfer learning.
Proc. IEEE 109, 1 (2020), 43–76.
A APPENDIX
A.1 Codes and Datasets
Code available at https://github.com/cshhzhao/GCOPE. A
more detailed information on related datasets is as follows:
•Citation network: Cora and Citeseer datasets [ 32] consist of a
diverse collection of computer science publications, where each
node is characterized by a bag-of-words representation of papers
and a categorical label indicating the paper topic. Pubmed dataset
[29] comprises articles related to diabetes from the PubMed data-
base. Each node in this dataset is represented by an attributevector containing TF/IDF-weighted word frequencies, accompa-
nied by a label specifying the particular type of diabetes discussed
in the publication.
•Amazon network: Computers and Photos datasets [ 28,33] are
two networks illustrating co-purchase relationships sourced from
Amazon. In these networks, each node represents a product, and
an edge indicates frequent co-purchases between two products.
Additionally, each node is associated with a bag-of-words repre-
sentation of product reviews and is labeled with its respective
category.
•WebKB: Cornell, Texas, and Wisconsin are three subdatasets
derived from the WebKB dataset [ 30], compiled from multiple
universities’ web pages. Each node within these datasets repre-
sents a web page, with edges denoting hyperlinks between pages.
The node features are represented as bag-of-words representa-
tions of the web pages. Additionally, the web pages are manually
categorized into five distinct labels: student, project, course, staff,
and faculty.
•Wikipedia network: Chameleon and Squirrel datasets [ 30] con-
sist of two page-page networks extracted from Wikipedia, fo-
cusing on specific topics. In these networks, nodes represent
individual web pages, while edges signify links between pages.
Node attributes are defined as sets of informative nouns extracted
from the pages. Moreover, each node is labeled based on the av-
erage monthly traffic received by the respective web page.
A.2 Parameter Analysis
We compare parameter numbers of two baselines and our proposed
GCOPE in Table A8. For downstream tasks, we also compare pa-
rameter numbers of finetuning and prompting in Table A8.
Table A8: Parameter analysis various methods, where
the backbone is FAGCN. IP and GCOPE are pretrained by
GraphCL. Citeseer is the downstream dataset. We use “-” to
represent the case without corresponding phases.
Metho
ds Sup
ervised IP GCOPE
Pr
etraining - 62,976 62,976
+
Coordinators - - 900
+
Reconstruction - - 29,412
Finetuning 47,750 47,750 47,750
Pr
ompting - - 19,398
A.3 Cross-domain Transfer Performance with
other C-way-K-shot settings
We use Table A1, Table A2, Table A3, and Table A4 to show the
cross-domain transfer learning performance with 3/5-shot learning
scenarios respectively. From the experimental results, we observe
that our proposed GCOPE maintains its superiority over other base-
line methods, even though the improvements modestly decrease
from 1-shot to 5-shot scenarios.
A.4 Quantitative Analysis of Graph
Coordinators
We conduct additional experiments to investigate the quantitative
analysis of graph coordinators, shown by Table A5 and Table A6.
Experimental results show that, despite slight fluctuations in per-
formance across different datasets, GCOPE with varying numbers
of graph coordinators generally outperforms both the Supervised
and IP methods.
4452All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining KDD ’24, August 25–29, 2024, Barcelona, Spain
Table A1: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on homophilic datasets (C-way 3-shot). IMP
(%): the average improvement of GCOPE over the rest. GCL and Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsCora Citese
er Pubme
d Computers P
hotos
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.4493±.05 0.7911±.02 0.4425±.04 0.4336±.05 0.7580±.04 0.4131±.04 0.5565±.06 0.7161±.05 0.5343±.06 0.4516±.04 0.7973±.01 0.4101±.02 0.5816±.04 0.8786±.01 0.5261±.05
GA
T 0.4741±.01 0.8035±.01 0.4420±.01 0.4813±.02 0.7634±.01 0.4470±.02 0.6250±.01 0.7803±.02 0.5877±.04 0.4077±.07 0.7951±.03 0.3865±.02 0.5935±.04 0.8789±.02 0.5595±.05
BW
GNN 0.4887±.01 0.8137±.01 0.4737±.01 0.3840±.05 0.7176±.03 0.3608±.05 0.5423±.03 0.6940±.02 0.5171±.03 0.4554±.04 0.8022±.01 0.4132±.06 0.5749±.03 0.8670±.00 0.5308±.01
F
AGCN 0.6517±.01 0.8926±.00 0.6336±.01 0.6217±.02 0.8416±.00 0.5888±.02 0.5695±.03 0.6267±.04 0.5201±.02 0.5588±.06 0.8695±.02 0.5300±.04 0.6582±.02 0.9123±.00 0.6431±.01
IP
+
finetuningGCL+GCN 0.4414±.06 0.7540±.03 0.4155±.04 0.4099±.04 0.7270±.04 0.3836±.07 0.5565±.03 0.7374±.01 0.5334±.02 0.4363±.03 0.8222±.01 0.4470±.01 0.5918±.04 0.8949±.01 0.5868±.03
GCL+F
AGCN 0.6252±.02 0.8945±.00 0.6129±.02 0.6099±.02 0.8469±.00 0.5767±.02 0.5374±.02 0.6134±.02 0.4643±.04 0.5300±.06 0.8799±.00 0.5226±.03 0.6925±.02 0.9247±.00 0.6575±.01
Sim+GCN 0.4290±.02 0.7565±.02 0.3911±.02 0.3816±.05 0.6938±.04 0.3375±.08 0.5458±.04 0.7008±.02 0.5083±.04 0.4674±.02 0.8057±.02 0.4061±.07 0.5561±.03 0.8525±.01 0.5131±.03
Sim+F
AGCN 0.6758±.02 0.9004±.01 0.6467±.02 0.5333±.02 0.8064±.00 0.4992±.02 0.5696±.02 0.6201±.03 0.5031±.05 0.5164±.03 0.8531±.01 0.5111±.02 0.6532±.01 0.9301±.00 0.6482±.01
GCOPE
+
finetuningGCL+GCN 0.4327±.03 0.7560±.04 0.4131±.03 0.4273±.02 0.7392±.02 0.3871±.03 0.5681±.02 0.7314±.01 0.5103±.05 0.4808±.02 0.8357±.01 0.4424±.04 0.6266±.02 0.8909±.01 0.5754±.03
GCL+F
AGCN 0.6857±.02 0.9166±.00 0.6721±.02 0.6148±.02 0.8431±.01 0.5752±.02 0.5490±.02 0.6636±.01 0.4878±.06 0.5493±.03 0.8682±.01 0.5269±.05 0.6873±.03 0.9172±.01 0.6556±.01
Sim+GCN 0.4187±.01 0.7648±.03 0.3841±.05 0.4301±.00 0.7212±.00 0.3952±.03 0.5606±.05 0.7418±.04 0.5246±.06 0.4432±.03 0.7990±.01 0.3968±.04 0.5890±.01 0.8841±.01 0.5441±.03
Sim+F
AGCN 0.6798±.04 0.9010±.02 0.6617±.04 0.6469±.02 0.8602±.00 0.6120±.02 0.6048±.03 0.7081±.07 0.5700±.05 0.5742±.02 0.8684±.01 0.5480±.00 0.6938±.01 0.9343±.00 0.6683±.00
IMP
(%) 4.69%
1.07% 5.03% 9.93%
2.81% 9.21% 1.39%
3.66% 0.41% 7.10%
1.78% 5.56% 5.95%
1.60% 5.18%
Table A2: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on heterophilic datasets (C-way-3-shot). IMP
(%): the average improvement of GCOPE over the rest. GCL and Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsWisconsin T
exas Cornell Chamele
on Squirr
el
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.6642±.05 0.8406±.02 0.6136±.05 0.7085±.03 0.6826±.01 0.5778±.08 0.5444±.12 0.8084±.05 0.5276±.06 0.2340±.02 0.5359±.03 0.1566±.05 0.2161±.00 0.5228±.00 0.1916±.01
GA
T 0.5783±.05 0.7976±.02 0.4903±.05 0.6732±.04 0.6320±.03 0.5533±.02 0.4411±.11 0.7879±.06 0.4410±.07 0.2400±.01 0.5401±.01 0.1782±.04 0.2011±.01 0.5064±.01 0.1536±.03
BW
GNN 0.5868±.05 0.8212±.06 0.4435±.09 0.6654±.09 0.6694±.05 0.4401±.24 0.5417±.05 0.7871±.07 0.4799±.05 0.2245±.04 0.4953±.02 0.1816±.02 0.2096±.00 0.5075±.01 0.1635±.02
F
AGCN 0.7104±.01 0.8655±.01 0.5972±.02 0.7163±.04 0.6728±.01 0.6105±.04 0.5722±.10 0.8638±.02 0.5835±.09 0.2564±.02 0.5425±.02 0.2042±.02 0.2053±.01 0.5064±.01 0.1738±.01
IP
+
finetuningGCL+GCN 0.5840±.07 0.8030±.08 0.4231±.19 0.6510±.08 0.6508±.02 0.5709±.13 0.5470±.09 0.8111±.03 0.4722±.08 0.2245±.02 0.5092±.02 0.1635±.03 0.2070±.01 0.5027±.00 0.1551±.04
GCL+F
AGCN 0.6547±.04 0.8624±.01 0.5667±.04 0.7242±.03 0.6668±.00 0.6434±.07 0.5881±.06 0.8756±.01 0.6183±.06 0.2590±.02 0.5439±.01 0.2180±.01 0.2039±.01 0.5077±.00 0.1558±.04
Sim+GCN 0.5311±.08 0.8151±.04 0.4518±.09 0.6797±.05 0.6773±.03 0.4847±.06 0.4848±.09 0.8138±.06 0.4507±.05 0.2252±.02 0.5140±.01 0.1871±.02 0.2062±.01 0.5091±.00 0.1553±.04
Sim+F
AGCN 0.6349±.01 0.8876±.00 0.5512±.01 0.7464±.01 0.6750±.00 0.6949±.01 0.5709±.09 0.8829±.01 0.5953±.07 0.2376±.02 0.5251±.02 0.1921±.02 0.2027±.00 0.5109±.01 0.1587±.05
GCOPE
+
finetuningGCL+GCN 0.5943±.05 0.8351±.03 0.4876±.06 0.7490±.05 0.7029±.02 0.5909±.11 0.5351±.05 0.8307±.04 0.4823±.07 0.2276±.01 0.5354±.01 0.1573±.02 0.2110±.00 0.5189±.01 0.1653±.02
GCL+F
AGCN 0.6481±.03 0.8545±.00 0.5634±.02 0.7033±.03 0.6554±.01 0.5997±.04 0.5669±.04 0.8566±.01 0.6024±.03 0.2442±.01 0.5454±.01 0.2105±.00 0.1985±.00 0.5039±.00 0.1456±.03
Sim+GCN 0.5726±.07 0.8299±.05 0.4283±.17 0.7882±.04 0.7060±.01 0.6635±.06 0.6159±.04 0.8656±.02 0.5810±.04 0.2084±.01 0.5039±.01 0.1660±.03 0.2159±.01 0.5135±.00 0.1746±.02
Sim+F
AGCN 0.7575±.01 0.9279±.00 0.6998±.01 0.8105±.01 0.7140±.01 0.7604±.01 0.7868±.02 0.9237±.00 0.7490±.03 0.2362±.02 0.5345±.01 0.2052±.01 0.2037±.00 0.5076±.00 0.1507±.04
IMP(%) 4.06%
3.02% 5.34% 9.66%
4.32% 14.28% 16.76%
4.87% 15.85% -3.60%
0.68% -0.22% 0.38%
0.35% -2.68%
Table A3: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on homophilic datasets (C-way-5-shot). IMP
(%): the average improvement of GCOPE over the rest. GCL and Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsCora Citese
er Pubme
d Computers P
hotos
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.6105±.03 0.8800±.03 0.6078±.04 0.4554±.04 0.7478±.04 0.4361±.02 0.6135±.02 0.7644±.03 0.5786±.05 0.4274±.05 0.8126±.03 0.4313±.03 0.6041±.04 0.8931±.00 0.5817±.03
GA
T 0.5979±.03 0.8652±.02 0.5727±.05 0.5048±.02 0.7927±.01 0.4837±.03 0.6165±.01 0.7537±.02 0.5772±.03 0.5108±.04 0.8423±.01 0.4666±.04 0.6257±.02 0.9022±.00 0.5936±.02
BW
GNN 0.6502±.03 0.8869±.01 0.6378±.02 0.5228±.03 0.8096±.02 0.4886±.04 0.5839±.02 0.7274±.04 0.5284±.05 0.5390±.03 0.8483±.02 0.4919±.04 0.6157±.03 0.8891±.00 0.5839±.03
F
AGCN 0.7183±.01 0.9139±.01 0.7144±.01 0.6721±.00 0.8597±.00 0.6390±.00 0.5965±.01 0.7445±.03 0.5719±.03 0.6054±.01 0.8886±.01 0.5851±.02 0.6655±.02 0.9107±.00 0.6435±.01
IP
+
finetuningGCL+GCN 0.6351±.01 0.8807±.00 0.6159±.01 0.5347±.03 0.8068±.01 0.5203±.03 0.5719±.03 0.6934±.04 0.5382±.03 0.4849±.07 0.8576±.01 0.4760±.07 0.6183±.02 0.8993±.00 0.6039±.00
GCL+F
AGCN 0.6948±.02 0.9059±.01 0.6893±.03 0.6768±.01 0.8690±.00 0.6530±.01 0.6024±.01 0.7596±.02 0.5876±.01 0.6217±.01 0.8806±.00 0.5868±.00 0.6991±.01 0.9083±.00 0.6509±.00
Sim+GCN 0.5596±.04 0.8520±.01 0.5441±.03 0.5180±.02 0.7962±.00 0.4905±.03 0.5064±.02 0.6503±.03 0.4582±.06 0.4817±.02 0.8464±.01 0.4525±.02 0.6183±.03 0.9102±.00 0.5964±.03
Sim+F
AGCN 0.7247±.01 0.9183±.00 0.7179±.02 0.6584±.01 0.8693±.00 0.6384±.01 0.6038±.01 0.7467±.04 0.5690±.04 0.5888±.01 0.8749±.02 0.5505±.02 0.6845±.02 0.9109±.00 0.6380±.01
GCOPE
+
finetuningGCL+GCN 0.6079±.02 0.8705±.02 0.5971±.03 0.5745±.06 0.8354±.02 0.5500±.06 0.5883±.03 0.7352±.03 0.5317±.05 0.5552±.03 0.8621±.01 0.4795±.06 0.6494±.01 0.9120±.00 0.6153±.03
GCL+F
AGCN 0.7334±.01 0.9247±.00 0.7238±.01 0.6863±.02 0.8778±.00 0.6539±.02 0.6110±.00 0.7552±.02 0.5806±.01 0.6172±.01 0.8950±.00 0.5969±.01 0.6868±.00 0.9206±.00 0.6591±.00
Sim+GCN 0.5501±.02 0.8522±.01 0.5298±.02 0.4869±.01 0.7884±.01 0.4731±.02 0.6135±.04 0.7684±.04 0.5685±.06 0.5264±.00 0.8588±.01 0.4928±.03 0.6237±.03 0.9105±.00 0.5759±.04
Sim+F
AGCN 0.7313±.01 0.9100±.02 0.7189±.01 0.6931±.01 0.8829±.00 0.6631±.01 0.5754±.03 0.7483±.03 0.5541±.03 0.6212±.02 0.8873±.01 0.5710±.02 0.6995±.02 0.9190±.00 0.6544±.01
IMP
(%) 1.05%
0.17% 0.77% 7.45%
3.33% 7.60% 1.74%
2.98% 1.38% 8.93%
2.26% 5.93% 3.66%
1.39% 2.40%
A.5 Dynamical Study of inter-coordinator edges
We additionally study the impact of dynamical inter-coordinator
edges on the performance of GCOPE, and report the transfer learn-
ing results in Table A7. According to the experimental results, we
observe that GCOPE/d outperforms GCOPE/f on the Wisconsin andTexas, performs better than the supervised method on the Cora but
is inferior to GCOPE/f, and exhibits negative transfer on Citeseer.
In comparison, GCOPE/f demonstrates positive transfer across all
datasets.
4453KDD ’24, August 25–29, 2024, Barcelona, Spain Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li
Table A4: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on heterophilic datasets (C-way-5-shot). IMP
(%): the average improvement of GCOPE over the rest. GCL and Sim respectively represent GraphCL and SimGRACE.
T
raining
schemesMethodsWisconsin T
exas Cornell Chamele
on Squirr
el
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
sup
ervisedGCN 0.6374±.08 0.8774±.02 0.6540±.06 0.7117±.04 0.6994±.01 0.6534±.04 0.6239±.09 0.8606±.05 0.5903±.07 0.2577±.01 0.5484±.01 0.2249±.01 0.2209±.00 0.5228±.00 0.1820±.02
GA
T 0.6128±.02 0.8788±.00 0.5704±.05 0.7517±.01 0.6977±.00 0.6290±.06 0.5549±.02 0.8655±.01 0.5745±.05 0.2147±.01 0.5206±.01 0.2127±.01 0.2083±.00 0.5112±.00 0.1596±.03
BW
GNN 0.5951±.00 0.8745±.01 0.5902±.03 0.7628±.04 0.7236±.01 0.6395±.11 0.6901±.07 0.8963±.02 0.6151±.04 0.2338±.01 0.5386±.00 0.1881±.03 0.2210±.01 0.5319±.01 0.1779±.03
F
AGCN 0.5261±.03 0.8678±.02 0.5074±.03 0.7269±.02 0.6945±.01 0.6734±.02 0.5183±.02 0.8841±.01 0.5628±.03 0.2319±.00 0.5212±.00 0.2137±.00 0.2131±.00 0.5162±.01 0.1863±.01
IP
+
finetuningGCL+GCN 0.5941±.13 0.9031±.01 0.5893±.14 0.6331±.14 0.6858±.03 0.4816±.14 0.6141±.09 0.8478±.04 0.5050±.05 0.2500±.01 0.5253±.01 0.1685±.01 0.2144±.00 0.5181±.00 0.1821±.02
GCL+F
AGCN 0.6039±.05 0.9077±.01 0.5963±.05 0.7338±.00 0.7182±.01 0.6045±.07 0.6930±.03 0.9253±.00 0.6839±.04 0.2319±.00 0.5327±.00 0.2196±.01 0.2127±.00 0.5172±.00 0.1931±.01
Sim+GCN 0.5655±.05 0.8860±.02 0.5922±.05 0.7545±.01 0.7312±.01 0.6268±.05 0.6732±.04 0.8717±.03 0.5895±.07 0.2419±.01 0.5354±.00 0.1832±.03 0.2240±.00 0.5321±.01 0.1890±.01
Sim+F
AGCN 0.6739±.02 0.9166±.00 0.6258±.04 0.7352±.01 0.7162±.00 0.6624±.06 0.7634±.02 0.9572±.00 0.7583±.02 0.2493±.01 0.5400±.00 0.2261±.00 0.2131±.00 0.5135±.00 0.1764±.01
GCOPE
+
finetuningGCL+GCN 0.5793±.08 0.8864±.02 0.5681±.11 0.7531±.04 0.7315±.00 0.6480±.08 0.6859±.15 0.9129±.03 0.6058±.13 0.2304±.01 0.5264±.00 0.2126±.01 0.2228±.01 0.5319±.00 0.1705±.04
GCL+F
AGCN 0.6079±.04 0.8971±.00 0.5870±.04 0.7393±.01 0.6989±.01 0.6736±.02 0.6352±.03 0.9161±.00 0.6302±.03 0.2350±.00 0.5337±.00 0.2102±.01 0.2134±.00 0.5220±.00 0.1943±.00
Sim+GCN 0.6099±.06 0.8923±.01 0.5884±.09 0.7407±.04 0.7226±.01 0.6326±.13 0.5563±.05 0.8352±.05 0.5248±.07 0.2486±.01 0.5458±.01 0.1860±.03 0.2208±.00 0.5310±.00 0.1847±.02
Sim+F
AGCN 0.7507±.01 0.9478±.00 0.7393±.03 0.8497±.00 0.7501±.00 0.8012±.01 0.8437±.02 0.9742±.00 0.8163±.02 0.2494±.02 0.5378±.00 0.2201±.00 0.2163±.00 0.5183±.00 0.1948±.00
IMP(%) 5.96%
1.90% 5.08% 6.13%
2.46% 10.87% 6.07%
2.37% 5.63% 0.82%
0.59% 1.28% -1.21%
1.04% 2.92%
Table A5: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on homophilic datasets (C-way-1-shot) of
GCOPE with different number of graph coordinators. ‘-1’, ‘-3’, and ‘-5’ represent that we assign one, three, or five graph
coordinators for each dataset in GCOPE.
Metho
dsCora Citese
er Pubme
d Computers P
hoto
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
Sup
ervised 0.3819±.03 0.6818±.04 0.3009±.09 0.5219±.08 0.8042±.03 0.4667±.08 0.4522±.02 0.5622±.04 0.4275±.07 0.4651±.04 0.7762±.02 0.3009±.07 0.5937±.05 0.8847±.00 0.5346±.03
IP 0.3892±.05 0.7228±.03 0.3619±.05 0.4461±.02 0.7781±.01 0.4126±.02 0.4532±.02 0.5708±.03 0.4168±.04 0.4371±.06 0.7616±.01 0.3450±.02 0.6273±.01 0.8710±.01 0.5406±.03
GCOPE-1 0.4618±.03 0.7597±.05 0.4388±.05 0.5631±.03 0.8258±.02 0.4953±.04 0.4591±.01 0.5512±.01 0.4203±.03 0.4465±.01 0.7747±.00 0.3432±.03 0.6329±.02 0.8850±.00 0.5935±.03
GCOPE-3 0.4272±.05 0.7509±.01 0.4176±.03 0.5518±.01 0.8438±.00 0.5074±.02 0.4777±.02 0.5689±.04 0.3794±.04 0.4314±.02 0.7335±.01 0.3546±.00 0.6491±.01 0.8934±.00 0.6109±.01
GCOPE-5 0.4499±.05 0.7673±.03 0.4414±.04 0.5280±.04 0.8183±.03 0.4658±.04 0.4686±.04 0.5737±.08 0.3663±.08 0.4650±.00 0.7656±.01 0.3767±.01 0.5538±.05 0.8760±.01 0.5621±.04
Table A6: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on heterophilic datasets (C-way-1-shot) of
GCOPE with different numbers of graph coordinators. ‘-1’, ‘-3’, and ‘-5’ represents the we assign one, three, or five graph
coordinators for each dataset in GCOPE.
Metho
dsWisconsin T
exas Cornell Chamele
on Squirr
el
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
Sup
ervised 0.5222±.05 0.7905±.03 0.4725±.06 0.6900±.06 0.7185±.01 0.5334±.12 0.2938±.06 0.6573±.04 0.2872±.05 0.2575±.02 0.5515±.02 0.1941±.01 0.2181±.00 0.5202±.00 0.1875±.02
IP 0.6063±.04 0.8356±.01 0.5555±.07 0.7425±.03 0.7034±.03 0.6141±.09 0.2588±.04 0.6262±.04 0.2442±.04 0.2443±.00 0.5530±.01 0.1875±.01 0.2223±.00 0.5307±.00 0.1740±.02
GCOPE-1 0.6579±.03 0.8531±.01 0.5649±.00 0.7125±.02 0.6693±.02 0.6300±.03 0.4013±.05 0.6897±.01 0.3160±.02 0.2886±.00 0.5898±.00 0.2320±.00 0.2257±.00 0.5257±.00 0.1885±.01
GCOPE-3 0.6217±.00 0.8267±.00 0.5397±.01 0.7675±.04 0.7005±.03 0.5834±.05 0.5675±.03 0.7334±.01 0.4506±.02 0.2895±.00 0.5785±.00 0.2205±.00 0.2199±.01 0.5274±.01 0.1815±.02
GCOPE-5 0.6353±.06 0.8327±.03 0.5692±.05 0.7262±.04 0.6913±.02 0.5468±.04 0.6550±.02 0.8350±.00 0.5047±.02 0.2810±.00 0.5654±.00 0.2113±.00 0.2129±.00 0.5183±.00 0.2029±.00
Table A7: Cross-domain transfer learning performance (mean±std Acc/AUC/F1) on two homophilic and two heterophilic
datasets (C-way-1-shot) of GCOPE with full or dynamical inter-coordinator edges. ’/f’ means that the inter-coordinator edges
in GCOPE are fully connected to each other. ’/d’ represents that inter-dataset edges in GCOPE are dynamically connected by
computing the similarity between them.
Metho
dsCora Citese
er Wisconsin T
exas
A
cc AUC F1 A
cc AUC F1 A
cc AUC F1 A
cc AUC F1
Sup
ervised 0.3819±.03 0.6818±.04 0.3009±.09 0.5219±.08 0.8042±.03 0.4667±.08 0.5222±.05 0.7905±.03 0.4725±.06 0.6900±.06 0.7185±.01 0.5334±.12
GCOPE/f 0.4618±.03 0.7597±.05 0.4388±.05 0.5631±.03 0.8258±.02 0.4953±.04 0.6579±.03 0.8531±.01 0.5649±.00 0.7125±.02 0.6693±.02 0.6300±.03
GCOPE/d 0.4027±.00 0.7228±.01 0.4206±.00 0.4874±.04 0.7764±.03 0.4342±.04 0.6588±.01 0.8545±.00 0.6046±.02 0.7400±.03 0.6851±.02 0.5813±.05
4454