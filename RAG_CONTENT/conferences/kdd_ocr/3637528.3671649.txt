Multi-Task Neural Linear Bandit for Exploration in
Recommender Systems
Yi Su
Google Deepmind
Mountain View, CA, USA
yisumtv@google.comHaokai Lu
Google Deepmind
Mountain View, CA, USA
haokai@google.comYuening Li
Google
Mountain View, CA, USA
yueningl@google.com
Liang Liu
Google
Mountain View, CA, USA
liangliu@google.comShuchao Bi
Google
Mountain View, CA, USA
shuchaobi@google.comEd H. Chi
Google Deepmind
Mountain View, CA, USA
edchi@google.com
Minmin Chen
Google Deepmind
Mountain View, CA, USA
minminc@google.com
ABSTRACT
Exposure bias and its induced feedback loop effect are well-known
problems in recommender systems. Exploration is believed to be
the key to break such feedback loops. While classical contextual
bandit algorithms such as Upper-Confidence-Bound and Thomp-
son Sampling have been successful in addressing the exploration-
exploitation trade-off in the single-task settings with one clear
reward signal, modern recommender systems often leverage multi-
ple rich sources of feedback such as clicks, likes, dislikes, shares,
satisfaction survey responses, and employ multi-task learning in
practice. It is unclear how one can incorporate exploration in the
multi-task setup with different objectives. In this paper, we study
an efficient bandit algorithm tailored to multi-task recommender
systems, named Multi-task Neural Linear Bandit (mtNLB). In partic-
ular, we investigate efficient feature embeddings in the multi-task
setups that could be used as contextual features in the Neural Lin-
ear Bandit, a contextual bandit algorithm that nicely combines
the representation power from DNN and simplicity in uncertainty
calculation from linear models. We further study cost-effective ap-
proximations of the uncertainty estimate and principled ways to
incorporate uncertainty into the multi-task scoring of items. To
showcase the efficacy of our proposed method, we conduct live
experiments on a large-scale commercial recommendation platform
that serves billions of users. We evaluate the quality of the uncer-
tainty estimate and demonstrate its ability to improve exploration
across the different dimensions of the reward signals in comparison
to baseline approaches.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671649CCS CONCEPTS
•Information systems →Personalization.
KEYWORDS
Recommender Systems, Exploration
ACM Reference Format:
Yi Su, Haokai Lu, Yuening Li, Liang Liu, Shuchao Bi, Ed H. Chi, and Min-
min Chen. 2024. Multi-Task Neural Linear Bandit for Exploration in Rec-
ommender Systems. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/
3637528.3671649
1 INTRODUCTION
Recommendation systems have become ubiquitous in today’s world,
helping users discover relevant content from an ever-expanding
pool of options. Traditional recommender systems are designed
to exploit user’s past interactions in order to learn their prefer-
ence and improve future recommendations [ 18,34]. However, such
purely exploitation-based systems often suffer from a problem
called closed-feedback loops [ 13,37]. In other words, the train-
ing data is biased towards the items favored by the past deployed
models, causing the trained model to increasingly focus on a nar-
rower set of items. The closed feed-back loop not only hurts the
user experience by recommending similar and repetitive items,
but is also detrimental to content provider welfare and the larger
ecosystem health [14, 16].
There is a growing body of research focusing on adding explo-
ration components to the existing recommender system to break
such feedback loops [ 4,6,26,36]. Viewing recommendation as a
decision-making process, contextual bandit (CB) formulation of-
fers a natural framework to bring exploration into the picture [17,
21]. Classical CB algorithms, such as Upper-Confidence-Bound
(UCB) [ 19] and Thompson Sampling [ 1], incorporate the uncer-
tainty of predicted rewards to strike a better balance between ex-
ploitation and exploration. This in turn alleviates the problem of
5723
KDD ’24, August 25–29, 2024, Barcelona, Spain Yi Su et al.
closed feedback loops in recommender systems. In addition to pro-
viding strong theoretical guarantees in regret, contextual bandit
algorithms have also demonstrated strong empirical performance
in real-world large-scale industrial systems [4, 21, 40].
Contextual bandit algorithms however are commonly designed
and evaluated in the single-task settings with a single, unambiguous
reward signal. In contrast, modern recommender systems often
utilize a variety of feedback signals, such as clicks, likes, shares,
comments, as well as satisfaction survey responses, and employ
multi-task learning [ 24,31,45]. Due to this limitation, most previous
attempts to utilize bandit algorithms often entail the meticulous
selection of a particular task, so that the uncertainty is estimated
and incorporated for the chosen task only [ 4,21,40]. This leads
to increased engineering complexity and sub-optimal exploration
efficiency.
In this paper, we propose a formalization of the multi-task contex-
tual bandit framework that caters to the diverse types of feedback
commonly utilized in modern recommender systems. Specifically,
at each round, the agent (i.e., the recommender system) plays one
action (i.e., recommend one item) and subsequently receives several
types of feedback for the chosen action (e.g., click, like, share, and
satisfaction rating). The goal of the system is to optimize a certain
combination of the rewards1over a finite time horizon. There are
two main challenges of directly utilizing bandit algorithms within
this multi-task framework:
(1)representation learning. Classical multi-arm bandits or linear
contextual bandits enjoy simple uncertainty calculation and
provable regret but lack the representation power to encode
complex contextual information. Combining deep neural
network representation learning with a simple function on
top therefore has becoming increasingly popular in decision
making problems [ 2,5,43]. One instance of this approach
is Neural Linear [ 33], where a Bayesian linear regression
is built on top of the last layer embedding of a neural net-
work. This approach effectively combines the representation
power of deep neural networks with the simplicity of un-
certainty calculation in linear bandits. It is however unclear
how to effectively learn representations for exploration and
decision making in multi-task systems. To this end, we in-
vestigate the efficacy of various representations within the
widely-adopted Multi-gate Mixture-of-Experts (MMoE) [ 25]
multi-task architecture for bandits algorithms2.
(2)integration of the uncertainty. Incorporating uncertainty in
the single-task linear regression setup is straight-forward.
Classical approaches such as the Upper-Confidence-Bound
(UCB) [ 19] and Thompson Sampling [ 1] have been well stud-
ied in this setting. Under the multi-task setup, the ultimate
reward is often a complex function of the feedback from
different tasks, and there is no clear consensus on how to
systematically incorporate uncertainty in the system when
choosing the action. To this end, we propose a method that
1Multi-objective optimization [ 22,32,46] in recommender systems itself is an active
research area, which is beyond the scope of this work. Here we assume a scalarized
combined reward, typically defined as a hand-crafted and well-tuned function of the
various types of feedback.
2The learning can easily generalize to other multi-task architectures as well.first transforms the combined reward to the space that is
approximately linear to the learned embeddings, then incor-
porates the uncertainty in the transformed linear space.
Together, we make the following contributions:
•We present a formal definition of the multi-task contextual
bandits framework for modelling decision-making in large-
scale multi-task recommender systems;
•We propose a novel method called Multi-task Neural Linear
Bandit (mtNLB ), which explores representation learning for
contextual features and ways to incorporate uncertainty un-
der the proposed multi-task contextual bandits framework;
•We discuss computationally efficient approximation of our
proposed algorithm that scales with the number of tasks,
and verify its effectiveness by examining the information
loss resulting from the approximation;
•We conduct extensive live experiments on a large-scale com-
mercial recommendation platform to showcase the effective-
ness of mtNLB. It significantly improves various dimensions
of user feedback and exploration efficiency, leading to its
successful deployment within the platform.
2 RELATED WORKS
Recent works have shown that the close feedback-loop in recom-
mender systems deteriorates recommendation quality and causes
homogenization of user behavior [ 14,16]. There is a general con-
sensus that exploration is critical for breaking such feedback loops.
To this end, there are an increasing amount of work in developing
exploration-based recommender systems.
User Exploration. One line of research is focused on user explo-
ration, which aims at discovering new user interest [ 7,26,39]. Song
et al. [39] proposes a hierarchical contextual bandit algorithm for
entire space user interest exploration, tailored for extremely large
action spaces. Similarly, Mahajan et al . [26] utilizes a Personalized
PageRank (PPR) algorithm for user-creator space exploration and
builds a probabilistic blending layer to slot the exploration content.
Chen et al . [7] studies the value of user exploration aacross four
facets of recommendation quality - accuracy, diversity, novelty, and
serendipity - and how it connects to the user experience.
Item Exploration. Another line of research focuses on item explo-
ration to discover fresh and tail contents on the platform [ 14,27].
Among these, Jadidinejad et al . [14] investigates the effect of closed
feedback loops in both the serving and offline evaluation stages of
recommendation models and argues that adding exploration can
help alleviate the problem and produce models with better general-
ization. Additionally, McInerney et al . [27] uses contextual bandit
algorithms to balance exploration with exploitation, with an empha-
sis on combining explanation and exploration in a principled way.
Recently, Su et al . [40] introduces new metrics and experiment de-
signs to examine the benefit of item exploration on long-term user
experience, through its effect on an intermediate entity, i.e., corpus
change. Cold-start item recommendation is also closely related with
item exploration, which aims to overcome the challenge of limited
data availability by leveraging auxiliary information, such as item
metadata or user feedback, to make effective recommendations for
new items. Two main categories of methods have been proposed in
5724Multi-Task Neural Linear Bandit for Exploration in Recommender Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
this domain: separate-training methods [ 3,35] and joint-training
methods [20, 42].
Contextual Bandit Algorithms. Moreover, our work also builds
upon the contextual bandit literature, which has been widely ap-
plied to decision-making problems in various domains such as
healthcare [ 12,29], dynamic pricing [ 30], dialogue systems [ 23],
as well as recommender systems [ 4,15,17,21,40]. Most of the
classical approaches rely on the linear assumption of the reward,
such as LinUCB [ 8] and Thompson Sampling (TS) [ 1], due to its
simplicity and strong theoretical guarantees. Recent advancements
in deep learning techniques have led to the extension of linear
bandit algorithms to neural network setups, with the idea of decou-
pling representation learning and uncertainty estimation. Popular
approaches along this line of research include Neural Linear [ 33],
NeuralUCB [ 47] as well as Neural Thompson Sampling [ 44]. Su
et al. [40] applies Neural Linear Bandit to real-world large-scale
ranking systems, adding exploration capability on one particular
specific task, i.e., predicting whether or not the user completes the
recommended item.
Multi-task Bandit. In the bandit literature, multi-task bandit se-
tups [ 10,11,28] are the closest to ours. For example, Deshmukh
et al. [10] studies multi-task learning techniques for classical con-
textual bandit problems, which views each arm as a separate task
and utilizes similarities between arms to improve sample efficiency.
While Do et al . [11] investigated contextual bandits with concave
rewards, focusing on scenarios where the trade-off among tasks
is defined by a concave function with linear reward structure, our
work diverges in its scope and applicability. We study bandit algo-
rithms specifically designed for large-scale recommender systems.
These algorithms leverage deep neural networks to model the in-
tricate relationship between contextual features and actions, and
utilize advanced multi-task architectures to model the interplay
among diverse tasks. We focus on the representation learning of
contextual features as well as how to incorporate uncertainty ef-
fectively in this setting. Beyond this, Soare et al . [38] studies a
multi-task setting where the learner is given a sequence of tasks
and assumes the similarity across the tasks in terms of the dis-
tance of the corresponding parameters. Recently, Yang et al . [43]
investigates the benefit of representation learning in the multi-task
contextual bandits, where at each round the learner could choose
a different action for different tasks, with the goal of minimizing
regret over all tasks. In contrast to prior work in the multi-task
bandit literature, we investigate a unique formulation where only
one action is played at each round, but all task signals are being
revealed. This setting is also close to the multi-objective and multi-
stakeholder recommendations [ 15,28,41]. Among them, the closest
one is the MO-LinCB proposed by Mehrotra et al . [28] , which stud-
ies a multi-objective linear contextual bandit model, and leverages
Gini aggregation function to approximate the Pareto-front. Instead,
we assume the existence of a well-tuned scoring function, and
allows for different task representations for different reward sig-
nals. Moreover, our work is built upon a shared linear model [ 21]
on top of learned feature representations, instead of the disjoint
linear model in which each arm learns its own parameter(s) with-
out sharing with others. These model differences are motivated
from the real-world industrial systems, and are compatible withthe widely-adopted Multi-gate Mixture-of-Experts (MMoE) [ 25]
architecture.
3 BACKGROUND ON INDUSTRIAL
MULTI-TASK RECOMMENDER SYSTEMS
Most industrial recommendation systems follow a multi-stage pro-
cess, which start with the candidate retrieval to identify promising
candidates from an extensive pool, followed by a ranking stage that
scores and ranks the identified candidates. We focus our discus-
sion on exploration algorithms in the ranking stage3. As previously
mentioned, such systems typically receive multiple forms of feed-
back, including user clicks, shares, and satisfaction survey ratings,
none of which independently reflect the true user utility. Thus, the
ranking system needs to learn precise predictions for each task and
subsequently combines them to compute a final utility score for
ranking candidates.
Multi-gate Mixture-of-Experts (MMoE). To effectively learn mul-
tiple tasks and mitigate task conflicts among them, current systems
often employ a soft-parameter sharing technique built upon Multi-
gate Mixture-of-Experts (MMoE) [ 9,25,45]. In Figure 1, we present
a comprehensive view on the architecture of industrial multi-task
recommender that utilizing MMoE. Specifically, given 𝐾tasks and
raw contextual features 𝑥4, the model first utilizes a shared-bottom
network denoted as function 𝑠(·). After this, it adds the MoE layer
on top of the shared-bottom representations 𝑠(𝑥). Each task then
learns its own gating network 𝑔𝑘(·), which is used to combine the
𝑛expert embeddings (i.e., 𝑒𝑖(𝑠(𝑥))for𝑖∈[𝑛]with𝑒𝑖(·)denotes
the𝑖-th expert embedding) to produce a task-specific embedding
(i.e., the last layer of the multi-gate mixture-of-experts (MMoE)).
More precisely, the output of any task 𝑘is defined as:
𝑓𝑘(𝑥)=ℎ𝑘 𝑙𝑘(𝑥)∀𝑘∈[𝐾] (1)
withℎ𝑘is the tower network for task 𝑘and𝑙𝑘(𝑥)is the task-specific
weighted combinations of expert embeddings, i.e.,
𝑙𝑘(𝑥):=𝑛Õ
𝑖=1𝑔𝑘 𝑠(𝑥)
𝑖𝑒𝑖 𝑠(𝑥)∀𝑘∈[𝐾] (2)
where the function 𝑔𝑘maps to the probability simplex Δ𝑛−1and
𝑔𝑘(·)𝑖indicates the probability or weight for expert 𝑖. In particular,
the gating networks 𝑔𝑘composes of linear transformations of the
shared-bottom representations 𝑠(𝑥), followed by a softmax layer ,
i.e.,
𝑔𝑘(𝑥)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑊𝑇
𝑘𝑠(𝑥)) (3)
and𝑊𝑘∈R𝑑×𝑛are learn-able parameters with 𝑛being the number
of experts and 𝑑is the feature dimension of 𝑠(𝑥).
4 METHODS
We begin by presenting a formulation of the multi-task bandit
problem tailored to recommender systems. We will then introduce
the Multi-task Neural Linear Bandit (mtNLB), which has two key
3Similar approach can be adopted for nomination stage, albeit with potentially distinct
architectures utilized for multi-task learning.
4The contextual features 𝑥is typically the concatenation of user features 𝑢and item
features𝑎.
5725KDD ’24, August 25–29, 2024, Barcelona, Spain Yi Su et al.
components: efficient representation learning and principled uncer-
tainty incorporation. We will delve into each component in detail.
4.1 Formulation of Multi-task Contextual
Bandit
We frame the multi-task contextual bandit problem as follows.
Given the overall time horizon 𝑇, at each round 𝑡∈[𝑇], a user
ucomes to the system with the contextual feature u𝑡∼Pu(·),
comprising demographic information, interaction history, search
queries, etc; the recommender system 𝜋then plays an action (i.e.,
recommends an item) a𝑡∈A to the user and receives a vector
of feedback r𝑡∈R𝐾, with each dimension [r𝑡]𝑘∼P𝑘(·|u𝑡,a𝑡)
represents the reward for task 𝑘andP(·|u𝑡,a𝑡)denotes the joint
distribution. There are 𝐾tasks in total, for instance, completion,
click, share, like and satisfaction survey ratings. The goal of the
system is to maximize the scalar reward that combines the reward
vector rthrough a known function 𝑓(r):R𝐾→R(or equivalently
minimize the regret), over the time horizon 𝑇, i.e.,
𝑉(𝜋):=Eu𝑡∼Pu,a𝑡∼𝜋(·|u𝑡),r𝑡∼P(·|u𝑡,a𝑡)𝑇Õ
𝑡=1𝑓(r𝑡)
The function 𝑓is often referred as the scoring function, which
combines the rewards from various tasks to measure the aggregated
user experience. For example, it could be 𝑓(r)=r𝑏1
1r𝑏2
2with𝐾=2
tasks and r1denotes the probability of the recommended item
being clicked, and r2denotes the satisfaction survey star of the
recommend item. The function 𝑓is often manually tuned or learned
to reflect and optimize the overall user experience.
Recognizing the limited representation power of linear bandit
algorithms, increasing numbers of works are developing efficient
bandit algorithms for complex, non-linear domains [ 43,44,47].
Riquelme et al . [33] provides a comprehensive empirical evalua-
tion of Bayesian deep networks for Thompson Sampling. The key
insight from the study is to decouple representation learning and
uncertainty estimation. One example that embodies this approach
is Neural Linear[ 33] that designed for the single-task contextual
bandit, which employs Bayesian linear regression on the represen-
tation derived from the final layer representation 𝜙(u,a)of a DNN,
where(u,a)denotes the user-item pair. Specifically, it calculates
the variance of the prediction as 𝜎2𝜙(u,a)𝑇Σ−1𝜙(u,a)with the
noise parameter 𝜎2and feature covariance matrix Σaggregated us-
ing𝜙(u,a). Then it samples reward from the posterior distribution
N(ˆ𝑟,𝜎2𝜙(u,a)𝑇Σ−1𝜙(u,a))with prediction mean for that specific
task ˆ𝑟and plays the action with the highest sampled reward.
This method exhibits superior performance and is remarkably
simple to tune. Taking inspiration from this approach, we inves-
tigate the development of efficient multi-task bandit algorithms
following a similar path, namely employing linear bandits on top
of complex representations learned in multi-task setups. In the
forthcoming sections, we will begin by introducing efficient repre-
sentation learning or feature extraction techniques for multi-task
systems, followed by uncertainty calculation and incorporation
built upon the learned representation.4.2 Representation Learning for Multi-task
Systems
We begin with investigating the representation learning component
of the mtNLB. Instead of learning the representation from scratch,
we elucidate how to choose effective representations within the sys-
tem’s existing multi-task learning framework. Before delving into
the effective representations for multi-task systems, it is important
to first review what constitutes a good representation in single-task
contextual bandits in the context of deep neural networks. The
primary consideration for utilizing linear bandit algorithms based
on the learned contextual features is for them to satisfy the linear
payoff assumptions described in Assumption 1.
Assumption 1. There exists a representation function 𝜙:R𝑑𝑢×
R𝑑𝑎→R𝑑and an unknown parameter 𝛽∈R𝑑, such that for all
(u,a), the mean reward r(u,a)is linear in the representation 𝜙(u,a)
withE[r]=𝜙(u,a)𝑇𝛽.
For single-task contextual bandit problems, the last layer embed-
ding naturally fits to be used as the contextual features, as models
typically build a (generalized) linear regression layer from the last
layer embedding to the target. However, it remains unclear how to
attain contextual representations that are linearly representative
of the different tasks for multi-task systems. We summarize one
observation in Proposition 2.
Proposition 2. If the final scoring function 𝑓is linear in terms
of the reward vector rwith known coefficient 𝜶, that is𝑓(r)=Í𝐾
𝑘=1𝛼𝑘r𝑘; moreover, if the reward for each task 𝑘∈ [𝐾]is lin-
ear in some task-dependent representations 𝜙𝑘(u,a)∈R𝑑, i.e.,[r]𝑘=
𝜷𝑇
𝑘𝜙𝑘(u,a). Then the final reward (after passing the scoring function)
is linear in the concatenation of the last layer embedding from all 𝐾
tasks, i.e.,
𝑓(r)=𝜸𝑇Φ(u,a) (4)
where 𝜸:=[𝛼1𝜷1;···;𝛼𝐾𝜷𝐾] (5)
Φ(u,a):=[𝜙1(u,a);···;𝜙𝐾(u,a)] (6)
Proposition 2 suggests that in the multi-task setup, if the final
scoring function is linear and all the tasks in the setup are regression
tasks, concatenating the last layer embeddings from all tasks (the
red boxes in Figure 1) produces a highly effective representation
that can be readily utilized by various linear bandit algorithms5.
We will discuss the extension to non-linear scoring function and
non-regression tasks shortly in Section 4.3.
4.3 Uncertainty Incorporation
In this section, we consider the extension of our approach to accom-
modate general non-linear scoring functions 𝑓(and non-regression
tasks, i.e., classification). The key idea is to identify an inverse trans-
formation function 𝑔, such that the transformed function 𝑔◦𝑓(r)
is linear in the learned contextual features Φ(u,a). We make the
following assumption:
Assumption 3. Given a scoring function 𝑓:R𝑘→R, there exists
a function𝑔:R→Rsuch that the composite function 𝑔◦𝑓is
5It is worth noting that the linearity assumption can be violated in practice. We however
observed remarkably robust empirical performance in Section 5, which corroborates
with the observation in the original Neural Linear Bandit paper [33]
5726Multi-Task Neural Linear Bandit for Exploration in Recommender Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 1: An illustration of the Multi-task Recommender System based on Multi-gate Mixture-of-Experts (MMoE), with three
experts and 𝐾tasks, and the mtNLB built on top.
linear in the contextual features with 𝑔◦𝑓(r):=𝜼𝑇Φ(u,a)with some
representation function Φ(·,·).
This is a mild assumption as most popular scoring functions in
the industrial systems, such as linear or product-of-powers, sat-
isfy Assumption 3 with 𝑔being the identity or logarithm function.
We consider the widely-adopted product-of-powers scoring func-
tion, i.e.,𝑓(r)=r𝜌1
1·r𝜌2
2···r𝜌𝐾
𝐾. Taking𝑔=log(·), the linearity
assumption holds in the transformed logit space:
𝑔◦𝑓(r):=𝐾Õ
𝑘=1𝜌𝑘˜r𝑘=𝜼𝑇Φ(u,a) (7)
where 𝜼=[𝛼1𝜷1;···;𝛼𝐾𝜷𝐾] (8)
Φ(u,a)=[𝜙1(u,a);···;𝜙𝐾(u,a)] (9)
Here ˜r𝑘=log(r𝑘)and𝜷𝑘are the parameters of the last layer neural
network when regressing to the task reward ˜r𝑘6.
In other words, we can deploy linear bandit algorithms in this
transformed linear space. Following Neural Linear with Thompson
Sampling, we build the Bayesian linear regression on the concate-
nation of the last layer features Φ(u,a)which draws a sample from
the posterior distribution with
(𝑔◦𝑓(ˆr))′∼N(𝑔◦𝑓(ˆr(u,a)),𝜎2Φ(u,a)𝑇Σ†Φ(u,a))
, where Σ†is the pseudo-inverse of the feature covariance matrix,
i.e.,Σ=Í
𝑡Φ(u,a)Φ(u,a)andˆrdenotes the learned neural network
predictions across various tasks.
6To ensure that we capture the linearly predictable features, we train the model to
regress to the transformed reward ˜r=log(r), instead of the original rewards r. This
is exactly what the classification task is learning.The final step involves transforming the Thompson Sampling
estimate of the score (𝑔◦𝑓(ˆr))′back to the original space. This can
be achieved by utilizing the inverse function 𝑔−1of𝑔, with𝑔−1=
exp(·)in our case. Therefore, the Thompson Sampling estimate of
our score(𝑓(ˆr))′can be rewritten as:
𝑔−1◦(𝑔◦𝑓(ˆr))′∼exp
N log(𝑓(ˆr(u,a))),𝜎2Φ(u,a)𝑇Σ†Φ(u,a)
(10)
𝐷=𝑓(ˆr(u,a))·
1+𝜎q
Φ(u,a)𝑇Σ†Φ(u,a)N(0,1)
(11)
where the second equation re-parameterizes the Gaussian using the
standard Gaussian and utilizes Taylor Expansion w.r.t. the Gaussian-
weighted standard deviation around 0. It is noteworthy that the
sampled score weighs the original score 𝑓(ˆr(u,a))with (1+the
learned uncertainty on the user-item pair (u,a)from Bayesian
linear regression). We detail our overall algorithm in Algorithm 1.
Diagonal Approximation. Computing the uncertainty term
𝜎2Φ(u,a)𝑇Σ†Φ(u,a)
for action selection in serving has O(𝑑2)complexity, which is ex-
pensive when 𝑑is large. mtNLB concatenates the last layer em-
bedding across all tasks. In other words, 𝑑=Í𝐾
𝑘=1𝑑𝑘, where𝐾is
the number of tasks ( 𝐾>10) and𝑑𝑘is the dimension of the last
layer embedding ( 𝑑𝑘=128) for task 𝑘. To reduce computation, we
use the diagonal approximation of the inverse covariance matrix,
5727KDD ’24, August 25–29, 2024, Barcelona, Spain Yi Su et al.
Algorithm 1: mtNLB (with 𝑓being product-of-powers)
Input: Total number of training runs 𝑇;
Total number of batches per training run 𝐻;
Initialize the model Θwith parameter 𝜃0and last layer representations 𝜙𝜃0for all tasks;
Initialize the dataset D0=∅, mtNLB parameter Σ1,0=𝜆I,𝜎;
For each training/serving run 𝑡=1,2,···𝑇:
Training Stage:
Collect user logD𝑡−1;
foreach batchℎ=1,2,···,𝐻with batch dataD𝑡−1,ℎdo
For each(u,a)∈D𝑡−1,ℎ, concatenate the last-layer
embeddings from all tasks 𝑘∈[𝐾]and denote it as Φ𝜃𝑡,ℎ−1.
Update the covariance matrix:
Σ𝑡,ℎ:=Σ𝑡,ℎ−1+Í
(u,a)∈D𝑡−1,ℎΦ𝜃𝑡,ℎ−1(u,a)Φ𝜃𝑡,ℎ−1(u,a)𝑇
Update the parameter of the ranking model 𝜃𝑡,ℎ−1→𝜃𝑡,ℎ, its
associated last layer parameter Φ𝜃𝑡,ℎ−1→Φ𝜃𝑡,ℎusing
stochastic gradient descent on batch data D𝑡−1,ℎ.
end
Update the precision matrix Σ†
𝑡as pseudo-inverse of Σ𝑡,𝐻;
TakeΣ𝑡+1,0:=Σ𝑡,𝐻,𝜃𝑡:=𝜃𝑡,𝐻, and push the model 𝜃𝑡,Σ†
𝑡for
serving.Serving Stage:
foruserucomes to the system, for all nominations (actions) a∈A
do
Calculate the variance in the posterior distribution:
𝑣𝑡(u,a):=Φ𝑇
𝜃𝑡(u,a)Σ†
𝑡Φ𝜃𝑡(u,a)
Letˆr𝑡(u,a)denotes the predicted reward vector for tasks, then
we change the final score to:
𝑓(ˆr𝑡(u,a))(1+𝜎p
𝑣𝑡(u,a)N( 0,1))
Present the top 𝐾ranked contentsA𝑘to the user;
Collect various user feedback r(u,a)fora∈A𝑘;
Update logD𝑡→D𝑡−1Ð{(u,a,r(u,a))} a∈A𝑘
end
which ignores the off-diagonal feature interactions, with the ap-
proximated uncertainty defined as: 𝜎2Φ(u,a)𝑇diag(Σ†)Φ(u,a). It
reduces the computation complexity to O(𝑑)and makes it feasible
to scale to a large number of tasks.
5 EXPERIMENTS
In this section, we empirically evaluate the performance of mtNLB on
a large-scale commercial recommender system. Our secondary goal
is to verify the effectiveness of our proposed representation learning
in the multi-task setup, and the impact of uncertainty estimation
using the diagonal approximation of the inverse covariance matrix.
5.1 Experiment Setup
We focus our comparison on the ranking stage of the system, which
employs multi-task learning with model architecture illustrated
in Figure 1. The model comprises 5 experts in the MMoE, each of
which outputs a 128-dimensional representation. This means that
the last layer embedding for any task is also 128-dimensional, as
they combine the expert embeddings through gating. There are
more than 10 tasks in the system, each designed to capture different
user responses, including click, like, satisfaction survey rating and
others. We compare the performance of the proposed mtNLB with
a baseline that performs single-task Neural Linear Bandit. The
online A/B testing spanned over a period of 2 weeks, with each
arm receiving 0.3%of the overall traffic. For mtNLB, we use the
diagonal approximation with tuned noise parameter 𝜎2=50and
regularization strength 𝜆=1𝑒−6. Additionally, we conduct several
ablation studies to assess the impact of different representations
and the effectiveness of the diagonal approximation.5.2 Results
5.2.1 Representation learning in the multi-task setting. Any repre-
sentation that contains sufficient information to learn all the tasks
can be considered as the contextual features in the linear bandit
algorithm. As illustrated in Figure 1, we have a range of representa-
tions to choose from. We compare the proposed representation, i.e.,
concatenating the last layer embeddings from each task (the red
boxes in Figure 1), 𝑑=1408 with𝐾=11and𝑑𝑘=128, with two
alternatives: (1). the shared bottom-layer embeddings (the blue box),
which consist of a 512-d latent embedding; and (2). the concatena-
tion of expert embeddings from all five experts (the purple boxes),
resulting in a 640-d latent representation. We made the following
observations:
(1)we encountered significant singularity issues with the co-
variance matrix when using alternative (2) concatenation
of the expert embeddings. This indicates a high correlation
between the representations of the experts. A large regular-
izer,𝜆𝐼has to be added to the feature covariance matrix to
deal with the singularity for matrix inversion, resulting in
a less accurate uncertainty estimate and the worst online
performance among all;
(2)comparing with alternative (1) the shared bottom embedding,
our method improves user metrics across various dimensions,
such as−0.74%in skipped rate,+0.54%in completion rate,
and+0.29% in time spent. Our method also out-performs
the alternative (1) in the exploration-related metrics, such
as a+2.38% increase in the number of unique interacted
user-channel pairs. We hypothesize that the last layer rep-
resentations capture higher-level features that align more
closely with the linear assumption of the rewards, hence en-
ables more accurate uncertainty estimates and more effective
exploration in multi-task systems.
5728Multi-Task Neural Linear Bandit for Exploration in Recommender Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 2: The relative error (left, the lower the better) and
the Spearman’s rank correlation (right, the higher the bet-
ter) between the uncertainty calculated using diagonal ap-
proximation and the exact uncertainty.
5.2.2 Diagonal approximation. We study the impact on the uncer-
tainty estimate using the diagonal approximation of the inverse
covariance matrix. We compare the uncertainty calculated using
the full inverse covariance matrix and that from the diagonal ap-
proximation, and summarize the findings in Figure 2. Surprisingly,
the uncertainty estimated based on the diagonal approximation
is highly consistent with the full uncertainty estimation, in terms
of both the relative difference (Figure 2 left) and the Spearman’s
rank correlation (Figure 2 right). We hypothesize that the last layer
embeddings, which are higher-level features, exhibit a high degree
of independence, while feature interactions are typically learned in
the bottom layers of the networks.
5.2.3 Comparison with the single-task contextual bandit exploration
system. Finally, we compare the mtNLB against a baseline which
utilizes Neural Linear Bandit for the completion task only, namely
whether or not the user finishes more than 𝑥%of the recommended
item. For the baseline, at serving time, a prediction is sampled from
the posterior distribution of the task reward and combined with
predictions from other tasks in the scoring function for ranking
items. Table 1 summarizes the live experiment results. We observed
significant improvement in different dimensions of the user experi-
ence/reward, such as increased completion rate, reduced skipped
rate, as well as increasing satisfied time spent and reducing non-
satisfied time. It is worth noting that satisfied/non-satisfied time
spent metrics take into account both user satisfaction captured
through survey responses, as well as time spent, reflecting a more
comprehensive representation of the overall user experience. We
hypothesize that the superior user metrics of mtNLB across differ-
ent reward dimensions are due to its ability to estimate and account
for uncertainty in the combined scoring function under the multi-
task setup, compared to the baseline post-hoc approach, which
only accounts for uncertainty of a single task within the combined
scoring function, resulted in sub-optimal exploration. In addition,
Figure 3 illustrates the gains in the corpus metric, which measures
the benefits of exploration in improving content coverage. This
metric looks at the number of unique items contributing to the top
𝑥%of the user time spent on the platform. Higher value indicates
more items are discovered by users, thus better content coverage.
Compared with the baseline, it shows a substantial improvement of
Figure 3: The percentage increase in the number of items
contributing to the top 𝑥%of Time Spent, when compared
the mtNLB with the baseline.
Metrics Improvement
Satisfied Time +2.59%
Non-satisfied Time −5.55%
Completion Rate +0.53%
Skipped Rate −0.62%
Table 1: The gains in various dimensions of the user met-
rics, when compared the mtNLB with the baseline single-
task contextual bandit system.
+48%on the number of items contributing to the top 10%time spent,
and a+75%improvement of the top 90%time spent, indicating that
the mtNLB improves coverage for both popular and tail contents.
We observe further gains when zooming into the fresh contents,
with a+240% (+200%) improvements for the number of fresh items
contributing to the top 10%(90%) time spent, which shows explo-
ration significantly improves the discovery of fresh contents. Given
the superior performance of Multi-task Neural Linear Bandit, it has
been successfully deployed into the production pipeline now.
6 CONCLUSION
In this paper, we discuss the challenges of incorporating exploration
in multi-task recommender systems with multiple objectives. We
formulate the multi-task bandit problem that tailored to the rec-
ommender systems and propose Multi-task Neural Linear Bandit,
which is composed of two key components: efficient representation
learning and principled uncertainty incorporation. We also inves-
tigate cost-effective approximations of the uncertainty estimates
and examine the information loss. Our experiments on a real-world
commercial recommendation platform demonstrate the effective-
ness of our proposed method in improving user experience across
various dimensions of rewards and increasing content coverage as
well as freshness in the system.
5729KDD ’24, August 25–29, 2024, Barcelona, Spain Yi Su et al.
REFERENCES
[1]Shipra Agrawal and Navin Goyal. 2013. Thompson sampling for contextual
bandits with linear payoffs. In International conference on machine learning. PMLR,
127–135.
[2]Sanjeev Arora, Simon Du, Sham Kakade, Yuping Luo, and Nikunj Saunshi. 2020.
Provable representation learning for imitation learning via bi-level optimization.
InInternational Conference on Machine Learning. PMLR, 367–376.
[3]Iman Barjasteh, Rana Forsati, Farzan Masrour, Abdol-Hossein Esfahanian, and
Hayder Radha. 2015. Cold-start item and user recommendation with decou-
pled completion and transduction. In Proceedings of the 9th ACM Conference on
Recommender Systems. 91–98.
[4]Walid Bendada, Guillaume Salha, and Théo Bontempelli. 2020. Carousel person-
alization in music streaming apps with contextual bandits. In Proceedings of the
14th ACM Conference on Recommender Systems. 420–425.
[5]Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning: A review and new perspectives. IEEE transactions on pattern analysis
and machine intelligence 35, 8 (2013), 1798–1828.
[6]Minmin Chen. 2021. Exploration in recommender systems. In Fifteenth ACM
Conference on Recommender Systems. 551–553.
[7]Minmin Chen, Yuyan Wang, Can Xu, Ya Le, Mohit Sharma, Lee Richardson, Su-
Lin Wu, and Ed Chi. 2021. Values of User Exploration in Recommender Systems.
InFifteenth ACM Conference on Recommender Systems. 85–95.
[8]Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. 2011. Contextual bandits
with linear payoff functions. In Proceedings of the Fourteenth International Con-
ference on Artificial Intelligence and Statistics . JMLR Workshop and Conference
Proceedings, 208–214.
[9]Michael Crawshaw. 2020. Multi-task learning with deep neural networks: A
survey. arXiv preprint arXiv:2009.09796 (2020).
[10] Aniket Anand Deshmukh, Urun Dogan, and Clay Scott. 2017. Multi-task learning
for contextual bandits. Advances in neural information processing systems 30
(2017).
[11] Virginie Do, Elvis Dohmatob, Matteo Pirotta, Alessandro Lazaric, and Nicolas
Usunier. 2022. Contextual bandits with concave rewards, and an application to
fair ranking. In The Eleventh International Conference on Learning Representations .
[12] Audrey Durand, Charis Achilleos, Demetris Iacovides, Katerina Strati, Georgios D
Mitsis, and Joelle Pineau. 2018. Contextual bandits for adapting treatment in
a mouse model of de novo carcinogenesis. In Machine learning for healthcare
conference. PMLR, 67–82.
[13] Yingqiang Ge, Shuya Zhao, Honglu Zhou, Changhua Pei, Fei Sun, Wenwu Ou,
and Yongfeng Zhang. 2020. Understanding echo chambers in e-commerce recom-
mender systems. In Proceedings of the 43rd international ACM SIGIR conference
on research and development in information retrieval. 2261–2270.
[14] Amir H Jadidinejad, Craig Macdonald, and Iadh Ounis. 2020. Using Exploration
to Alleviate Closed Loop Effects in Recommender Systems. In Proceedings of
the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval. 2025–2028.
[15] Olivier Jeunen and Bart Goethals. 2021. Top-k contextual bandits with equity of
exposure. In Proceedings of the 15th ACM Conference on Recommender Systems.
310–320.
[16] Ray Jiang, Silvia Chiappa, Tor Lattimore, András György, and Pushmeet Kohli.
2019. Degenerate feedback loops in recommender systems. In Proceedings of the
2019 AAAI/ACM Conference on AI, Ethics, and Society. 383–390.
[17] Thorsten Joachims, Adith Swaminathan, and Maarten De Rijke. 2018. Deep
learning with logged bandit feedback. In International Conference on Learning
Representations.
[18] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009), 30–37.
[19] John Langford and Tong Zhang. 2007. The epoch-greedy algorithm for multi-
armed bandits with side information. Advances in neural information processing
systems 20 (2007).
[20] Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, and Zi Huang. 2019.
From zero-shot learning to cold-start recommendation. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 33. 4189–4196.
[21] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-
bandit approach to personalized news article recommendation. In Proceedings of
the 19th international conference on World wide web. 661–670.
[22] Xiao Lin, Hongjie Chen, Changhua Pei, Fei Sun, Xuanji Xiao, Hanxiao Sun,
Yongfeng Zhang, Wenwu Ou, and Peng Jiang. 2019. A pareto-efficient algo-
rithm for multiple objective optimization in e-commerce recommendation. In
Proceedings of the 13th ACM Conference on recommender systems. 20–28.
[23] Bing Liu, Tong Yu, Ian Lane, and Ole J Mengshoel. 2018. Customized nonlinear
bandits for online response selection in neural conversation models. In Thirty-
Second AAAI Conference on Artificial Intelligence.
[24] Yichao Lu, Ruihai Dong, and Barry Smyth. 2018. Why I like it: multi-task learning
for recommendation and explanation. In Proceedings of the 12th ACM Conference
on Recommender Systems. 4–12.[25] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining. 1930–1939.
[26] Khushhall Chandra Mahajan, Amey Porobo Dharwadker, Romil Shah, Simeng Qu,
Gaurav Bang, and Brad Schumitsch. 2023. PIE: Personalized Interest Exploration
for Large-Scale Recommender Systems. arXiv preprint arXiv:2304.06844 (2023).
[27] James McInerney, Benjamin Lacker, Samantha Hansen, Karl Higley, Hugues
Bouchard, Alois Gruson, and Rishabh Mehrotra. 2018. Explore, exploit, and
explain: personalizing explainable recommendations with bandits. In Proceedings
of the 12th ACM conference on recommender systems. 31–39.
[28] Rishabh Mehrotra, Niannan Xue, and Mounia Lalmas. 2020. Bandit based opti-
mization of multiple objectives on a music streaming platform. In Proceedings of
the 26th ACM SIGKDD international conference on knowledge discovery & data
mining. 3224–3233.
[29] Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, and Yoshimi
Fukuoka. 2020. Nonstationary bandits with habituation and recovery dynamics.
Operations Research 68, 5 (2020), 1493–1516.
[30] Kanishka Misra, Eric M Schwartz, and Jacob Abernethy. 2019. Dynamic on-
line pricing with incomplete information using multiarmed bandit experiments.
Marketing Science 38, 2 (2019), 226–252.
[31] Zhen Qin, Yicheng Cheng, Zhe Zhao, Zhe Chen, Donald Metzler, and Jingzheng
Qin. 2020. Multitask mixture of sequential experts for user activity streams.
InProceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 3083–3091.
[32] Marco Tulio Ribeiro, Anisio Lacerda, Adriano Veloso, and Nivio Ziviani. 2012.
Pareto-efficient hybridization for multi-objective recommender systems. In Pro-
ceedings of the sixth ACM conference on Recommender systems. 19–26.
[33] Carlos Riquelme, George Tucker, and Jasper Snoek. 2018. Deep bayesian bandits
showdown: An empirical comparison of bayesian deep networks for thompson
sampling. arXiv preprint arXiv:1802.09127 (2018).
[34] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based
collaborative filtering recommendation algorithms. In Proceedings of the 10th
international conference on World Wide Web. 285–295.
[35] Martin Saveski and Amin Mantrach. 2014. Item cold-start recommendations:
learning local collective embeddings. In Proceedings of the 8th ACM Conference
on Recommender systems. 89–96.
[36] Tobias Schnabel, Paul N Bennett, Susan T Dumais, and Thorsten Joachims. 2018.
Short-term satisfaction and long-term coverage: Understanding how users tol-
erate algorithmic exploration. In Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining. 513–521.
[37] Ayan Sinha, David F Gleich, and Karthik Ramani. 2016. Deconvolving feedback
loops in recommender systems. Advances in neural information processing systems
29 (2016).
[38] Marta Soare, Ouais Alsharif, Alessandro Lazaric, and Joelle Pineau. 2014. Multi-
task linear bandits. In NIPS2014 workshop on transfer and multi-task learning:
theory meets practice.
[39] Yu Song, Shuai Sun, Jianxun Lian, Hong Huang, Yu Li, Hai Jin, and Xing Xie.
2022. Show Me the Whole World: Towards Entire Item Space Exploration for
Interactive Personalized Recommendations. In Proceedings of the Fifteenth ACM
International Conference on Web Search and Data Mining. 947–956.
[40] Yi Su, Xiangyu Wang, Elaine Ya Le, Liang Liu, Yuening Li, Haokai Lu, Benjamin
Lipshitz, Sriraj Badam, Lukasz Heldt, Shuchao Bi, et al .2023. Value of Exploration:
Measurements, Findings and Algorithms. arXiv preprint arXiv:2305.07764 (2023).
[41] Eralp Turgay, Doruk Oner, and Cem Tekin. 2018. Multi-objective contextual ban-
dit problem with similarity information. In International Conference on Artificial
Intelligence and Statistics. PMLR, 1673–1681.
[42] Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. Dropoutnet: Ad-
dressing cold start in recommender systems. Advances in neural information
processing systems 30 (2017).
[43] Jiaqi Yang, Wei Hu, Jason D Lee, and Simon S Du. 2020. Impact of representation
learning in linear bandits. arXiv preprint arXiv:2010.06531 (2020).
[44] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural
thompson sampling. arXiv preprint arXiv:2010.00827 (2020).
[45] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews,
Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019.
Recommending what video to watch next: a multitask ranking system. In Pro-
ceedings of the 13th ACM Conference on Recommender Systems. 43–51.
[46] Yong Zheng and David Xuejun Wang. 2022. A survey of recommender systems
with multi-objective optimization. Neurocomputing 474 (2022), 141–153.
[47] Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural contextual bandits
with ucb-based exploration. In International Conference on Machine Learning.
PMLR, 11492–11502.
5730