HiFGL: A Hierarchical Framework for Cross-silo Cross-device
Federated Graph Learning
Zhuoning Guo
The Hong Kong University of Science and Technology
(Guangzhou) & The Hong Kong University of Science and
Technology
Guangzhou & Hong Kong, China
zhuoning.guo@connect.ust.hkDuanyi Yao
The Hong Kong University of Science and Technology
Hong Kong, China
dyao@connect.ust.hk
Qiang Yang
The Hong Kong University of Science and Technology
Hong Kong, China
qyang@cse.ust.hkHao Liu∗
The Hong Kong University of Science and Technology
(Guangzhou) & The Hong Kong University of Science and
Technology
Guangzhou & Hong Kong, China
liuh@ust.hk
ABSTRACT
Federated Graph Learning (FGL) has emerged as a promising way
to learn high-quality representations from distributed graph data
with privacy preservation. Despite considerable efforts have been
made for FGL under either cross-device or cross-silo paradigm,
how to effectively capture graph knowledge in a more complicated
cross-silo cross-device environment remains an under-explored
problem. However, this task is challenging because of the inherent
hierarchy and heterogeneity of decentralized clients, diversified
privacy constraints in different clients, and the cross-client graph
integrity requirement. To this end, in this paper, we propose a
Hierarchical Federated Graph Learning (HiFGL) framework for
cross-silo cross-device FGL. Specifically, we devise a unified hi-
erarchical architecture to safeguard federated GNN training on
heterogeneous clients while ensuring graph integrity. Moreover,
we propose a Secret Message Passing (SecMP) scheme to shield
unauthorized access to subgraph-level and node-level sensitive in-
formation simultaneously. Theoretical analysis proves that HiFGL
achieves multi-level privacy preservation with complexity guar-
antees. Extensive experiments on real-world datasets validate the
superiority of the proposed framework against several baselines.
Furthermore, HiFGL’s versatile nature allows for its application in
either solely cross-silo or cross-device settings, further broadening
its utility in real-world FGL applications.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671660CCS CONCEPTS
•Theory of computation →Graph algorithms analysis; •
Security and privacy →Database and storage security.
KEYWORDS
federated graph learning, graph neural network, multi-level privacy
preservation
ACM Reference Format:
Zhuoning Guo, Duanyi Yao, Qiang Yang, and Hao Liu∗. 2024. HiFGL: A Hier-
archical Framework for Cross-silo Cross-device Federated Graph Learning.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671660
1 INTRODUCTION
Federated Learning (FL) has emerged as a transformative approach
by enabling multiple parties to contribute to a shared machine
learning model without the need for direct data exchange [ 34,36].
Along this line, Federated Graph Learning (FGL) has been proposed
to collaboratively train the Graph Neural Network (GNN) to ex-
tract distributed knowledge from interconnected subgraphs held
privately by each party. Recently, FGL has been adopted to a wide
range of application domains, such as finance [ 29], recommender
system [20], and transportation [38].
Existing FGL approaches predominantly revolve around two
paradigms [ 9].1) Cross-silo FGL [2,31,40], as illustrated in Fig-
ure 1 (a), formulates data silos as clients, each of which possessing
a subgraph consists of nodes and connected edges. This paradigm
is particularly relevant for institutions wishing to maintain pri-
vate subgraphs while contributing to a global graph structure, such
as cross-platform recommendation for E-commerce [ 16].2) Cross-
device FGL [23,25,33], as shown in Figure 1 (b), regards each device
as a client, which holding a node and its associated edges. It is more
suitable for scenarios where numerous devices maintain private
connections within a global graph, e.g., user-centric social net-
works [ 27]. Table 1 summarizes the privacy, utility, and efficiency
tradeoff of existing FGL approaches.
 
968
KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
(a) Cross-silo FGL
Cross-client EdgeIntra-client EdgeNode
Server
Feature(b) Cross-device FGL (c) Cross-silo Cross-device FGL
1
56
3
7
2 4
1
 2
 3
 4
1
5
6
7
3
4
2
Client for silo
Client for device
Figure 1: Illustration of three FGL paradigms: cross-silo, cross-device, and cross-silo cross-device FGL.
Despite the success of the above two paradigms in unleashing
the power of isolated graph data, none of them can be directly
adopted to fully address the complexities of a mixed cross-silo
cross-device environment, as demonstrated in Figure 1 (c). In real-
world scenarios, institutions and users may have common privacy
concerns but with varying privacy and utility requirements [ 42,43].
We illustrate the use case of FGL under the cross-silo cross-device
setting via the following real-world application.
Example 1. Anomaly detection in financial transactions..
In this scenario, banks oversee customer transactions, forming a fed-
erated graph where each bank controls a subgraph of accounts (i.e.,
nodes) and transactions (i.e., edges). The collaborative analysis of
the cross-bank federated graph can enhance the anomaly detection
task. However, the transaction records are confidential, and regula-
tions forbid the banks to disclose the subgraph structure. Meanwhile,
customers are also unwilling to expose their sensitive information to
institutions to avoid personal privacy leakage.
To bridge the gap, we investigate the cross-silo cross-device fed-
erated graph learning problem, where institutions and customer
devices collaboratively optimize a GNN model under their diverged
privacy constraints. However, three major technical challenges
arise. 1) Inherent hierarchy and heterogeneity of decentral-
ized clients. The participants in cross-silo cross-device FGL natu-
rally form a hierarchy, where an institution client may have a more
comprehensive local structure of devices, and devices may preserve
their local sensitive features. Besides, the cross-silo cross-device
FGL involves clients with varying computational capabilities. Such
hierarchy and heterogeneity pose significant challenges in design-
ing a unified FGL framework that can effectively operate across a
varied landscape. 2) Diversified privacy constraints in different
clients. As depicted in Table 1, privacy concerns in FGL vary across
different types of clients. In a word, cross-silo FGL focuses more on
subgraph-level privacy (i.e., the privacy of structures), while cross-
device FGL weighs more on node-level privacy (i.e., the privacy of
features). The varied privacy requirements necessitate a flexible
approach that can adapt to the specific needs of different types
of clients while ensuring the overall utility of the federated graph
learning process. 3) Cross-client graph integrity. In FGL, each
client contributes to a portion of the overall graph. Maintaining
the integrity of graph data across multiple clients is critical to the
utility of the joint model. However, it is a non-trivial task to protect
the cross-client graph information without sacrificing the model
performance. For example, FedSage+ [ 40] protects cross-silo struc-
tures by generatively approximating edges across local subgraphs,
while Glint [ 19] chooses to expose node embeddings to guaranteemodel utility. It is challenging to preserve graph privacy without
sacrificing graph integrity.
In this paper, we propose a unified cross-silo cross-device frame-
work, Hierarchical Federated Graph Learning (HiFGL), for multi-
level privacy preservation (i.e., both subgraph-level and node-level)
without sacrificing graph information integrity. Specifically, we
first construct a hierarchical architecture comprising three key
components: device-client, silo-client, and server. The hierarchical
architecture enables federated graph learning with the flexibility
of applying diversified privacy preservation strategies on different
types of clients. Moreover, we propose a Secret Message Passing
(SecMP ) scheme for multi-level privacy protection. In particular,
a Neighbor-Agnostic Aggregation protocol and a Hierarchical La-
grangian Embedding protocol are proposed to reduce subgraph-
level and node-level privacy leakage, respectively. Furthermore,
a tailored resource-efficient optimization algorithm is introduced
for the unified framework. Notably, HiFGL can also be applied in
solely cross-silo or cross-device scenarios, and is compatible with
diverse FL algorithms (e.g., FedAvg [ 22], FedProx [ 15]) and GNN
variants (e.g., GCN [ 13], GraphSage [ 7]), further broaden its utility
in real-world FGL applications.
The main contributions of our work are listed as follows: 1) To
our knowledge, HiFGL is the first framework tailored for the cross-
silo cross-device federated graph learning problem, which has rarely
been studied before. 2) We construct a hierarchical architecture
to enable flexible privacy-preservation of heterogeneous clients
without sacrificing graph integrity. 3) We propose a secret message
passing scheme to simultaneously safeguard subgraph-level and
node-level privacy against semi-honest adversaries during collabo-
rative GNN training. 4) We theoretically analyze the privacy and
complexity of HiFGL. The results show that our methods not only
preserve subgraph-level and node-level privacy but also achieve
information integrity with guaranteed complexity. 5) We conduct
extensive experiments on real-world graph datasets, and the results
demonstrate that HiFGL outperforms state-of-the-art baselines in
learning more effective GNN models.
2 RELATED WORK
In this section, we review state-of-the-art federated graph learning
approaches, including two paradigms: cross-silo and cross-device.
2.1 Cross-silo Federated Graph Learning
A common scenario of cross-silo FGL is that institutions collabora-
tively learn models while keeping local data private. Besides various
 
969HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 1: Comparison of typical FGL works and HiFGL in the dimension of Multi-level Privacy Preservation (including Subgraph-
level andNode-level ),Information Integrity, and Efficiency. We evaluate properties by Low, Medium, or High by considering how
much a framework satisfies requirements relatively among baselines.
Model FGL Paradigm Multi-level Privacy Preservation Information Integrity Efficiency
Subgraph-level Node-level
GCN [13]+FedAvg [22] Cross-silo Medium Low Medium High
FedSage+ [40] Cross-silo Medium Low Medium Low
FedPUB [2] Cross-silo Medium Low Medium Medium
GraphFL [31] Cross-silo Medium Low Medium Medium
FedGraph [4] Cross-silo High Low High Low
Glint [19] Cross-silo Low High High Medium
PPSGCN [39] Cross-silo Medium Low High Low
FedCog [14] Cross-silo Medium Low High Low
CNFGNN [23] Cross-device - Medium High Medium
FedPerGNN [33] Cross-device - High High Low
FedWalk [25] Cross-device - High High Medium
Lumos [26] Cross-device - High High Low
SemiDFEGL [27] Cross-device - High High Medium
HiFGL (Ours) Cross-silo Cross-device (Both) High High High Medium
GNN modules for improving effectiveness, existing cross-silo FGL
works have different strategies for processing graph structure data.
The first strategy is to drop the cross-client edges to prevent data
leakage across clients. An intuitive way is that each client holds a
GNN (e.g., GCN [ 13]) and a subgraph without cross-client edges and
trains a global model through FedAvg [ 22], a custom FL framework.
In addition, FedSage+ [ 40] generates local nodes’ neighbor features
to offset the ignorance of cross-client edges due to subgraph-level
privacy preservation, which improves predicting ability with sig-
nificant extra training costs. FedPUB [ 2] focuses on the prediction
improvement via personalized masked graph convolutional net-
work, where their pairwise similarity is measured by subgraph
representation. GraphFL [ 31] solves the semi-supervised graph
learning problem on federated unconnected subgraphs, where node
label domains vary and are not identically distributed. Another
strategy is to maintain cross-client edges either stored by the server
or clients. In this way, we have to face either less practicability
or more privacy leakage. For example, FedGraph [ 4] uses a cen-
tral server to keep cross-client edges and federally trains GNNs
with graph sampling through huge communication for expanding
neighbors between clients and the server. Unfortunately, the frame-
work is not practical in the real world [ 18]. Glint [ 19] decentralized
trains graph convolutional networks, where nodes are fully aware
of cross-client neighbors. PPSGCN [ 39] leverages the graph sam-
pling method to enhance efficiency and scalability, which hides
node information but exposes nodes across clients. FedCog [ 14]
decouples subgraphs according to intra- or cross-client edges to
construct a border graph for each client, which is a bipartite graph
between internal nodes and external nodes, with two separated
graph learning operations.
We propose quantifying subgraph-level privacy leakage in Ap-
pendix A.1. The results demonstrate the unsolved issue of balancing
subgraph-level privacy protection and cross-client graph integrity.
Our approach aims to achieve two objectives simultaneously.2.2 Cross-device Federated Graph Learning
Cross-device FGL assumes users hold private data and learn mod-
els without accessing private data. To name a few, CNFGNN [ 23]
combines the spatiotemporal GNN model with FL, where the data
storage scheme hides the original information of nodes but ex-
poses hidden states and neighboring nodes. FedPerGNN [ 33] ap-
plies GNNs for recommender systems where a user keeps a local
user-item subgraph. A privacy-preserving graph expansion method
anonymously acquires neighbor information with high commu-
nication costs. FedWalk [ 25] adopts the random walk algorithm
for federated graph node embedding learning with node-level vis-
ibility for covering raw graph information. Lumos [ 26] utilizes
local differential privacy and zero-knowledge protocol to protect
node features’ and degrees’ privacy among decentralized devices.
SemiDFEGL [ 27] collaborates ego graph-corresponded devices via
a peer-to-peer manner for scalability improvement and communi-
cation reduction on recommendation tasks. In summary, existing
works either utilize inefficient privacy-preserving schemes to align
pairwise relationships or rely on a server with extreme representa-
tion exposure to achieve cross-client edge integrity. Our framework
constructs a three-level architecture with multiple privacy preser-
vation protocols to achieve effective and secure FGL.
3 PRELIMINARIES
3.1 Federated Graph Definition
In FGL, graphs are required to be distributively stored with privacy
preservation. We focus on the scenario where a complete graph
consists of subgraphs without overlapped nodes and nodes have
their features and edges [ 40]. Let𝑣denote a node associated with a
multi-dimensional feature vector ℎ𝑣, an edge𝑒=(𝑣𝑖,𝑣𝑗)is defined
as a directed linkage from 𝑣𝑖to𝑣𝑗. A subgraph is defined as G𝑠=
(V𝑠,E𝑠), where V𝑠is the node set and E𝑠is the edge set. Note
∃𝑣𝑗∉V𝑠,𝑒=(𝑣𝑖,𝑣𝑗)∈E𝑠,i.e., a node in a subgraph can connect
 
970KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
with the nodes from other subgraphs. Then, we define the federated
graph as a union of subgraphs under a FL setting.
Definition 1. Federated Graph. Federated graph is denoted
asG=(V,E)={G𝑠|𝑠=1,2,···,|G|}, where Vis the node set,
Eis the edge set, G𝑠=(V𝑠,E𝑠)is the𝑠-th subgraph, and |G|is
the number of subgraphs. Gsatisfies 1) V1∪V2∪···∪ V|G|=V,
2)E1∪E2∪···∪ E|G|⊆E, 3)V𝑖∩V𝑗=∅,∀1≤𝑖≤|G|,1≤𝑗≤
|G|,𝑖≠𝑗, 4)E𝑖∩E𝑗=∅,∀1≤𝑖≤|G|,1≤𝑗≤|G|,𝑖≠𝑗.
In this work, we associate each node with a device and each
subgraph with a data silo. Under the cross-silo and cross-device
setting, nodes and graph structures may be placed in multiple het-
erogeneous clients for federated graph learning.
3.2 Problem Formulation
In this work, we aim to collaboratively train GNNs over distributed
graph data in a federated way, which is formally defined below.
Problem 1. Cross-silo Cross-device Federated Graph Learn-
ing. Given a federared graph G=(V,E)consisting of the subgraph
set{G𝑠}|G|
1. We aim to learn the parameter 𝜃𝑠of𝑠-th silo-client for
the global GNN model Gto minimize the global loss for downstream
predictive or regressive tasks,
{𝜃1,···,𝜃|G|}=arg min∑︁
L(G(G𝑠;𝜃𝑠),𝑌𝑠), (1)
where𝑖=1,2,···,|G|andLdenotes the loss function between
estimationG(G𝑠;𝜃𝑠)and ground truth 𝑌𝑠.
3.3 Threat Model
Here we define the threat model for cross-silo cross-device FGL,
which concurrently considers nodes’ and subgraphs’ privacy. Specif-
ically, we assume all parties are semi-honest, i.e., honest-but-curious,
which means the adversary will try its best to obtain private infor-
mation but not break protocols or cause malicious damage to the
model’s ability. We first define nodes’ and subgraphs’ privacy as
node-level privacy and subgraph-level privacy, respectively.
Definition 2. Node-level Privacy. A node usually represents a
device of a user in FGL applications, which should not leak the privacy
of the raw features and adjacent neighbors to other participants, i.e.,
devices and data silos.
Definition 3. Subgraph-level Privacy. A subgraph corresponds
to a data silo of an institution in FGL applications, which should not
expose its subgraph to other data silos and devices.
As illustrated in Example 1, where banks correspond to sub-
graphs and accounts correspond to nodes, the corresponding partic-
ipants in cross-silo cross-device FGL are inherently heterogeneous.
Then, we introduce the potential attack types between heteroge-
neous clients, including node-node attack, subgraph-node attack,
and subgraph-subgraph attack.
Definition 4. Node-Node Attack. A node acts as an attacker,
and another node acts as a defender. Nodes only see their features thus
they hope to obtain neighbor embeddings to infer more information.
Definition 5. Subgraph-Node Attack. A subgraph acts as an
attacker, and a node acts as a defender. Subgraphs have no accessto nodes’ features and neighbors, and thus subgraphs are desired to
utilize this information by cooperating with nodes. This cooperation
may raise data leakage from nodes to subgraphs.
Definition 6. Subgraph-Subgraph Attack. A subgraph acts
as an attacker, and another subgraph acts as a defender. The adver-
sary hopes to acquire other subgraphs as well as cross-client edges to
enhance their subgraph.
Note we don’t assume nodes will attack subgraphs. Due to the
hierarchy of heterogenous clients, the device can access the model
from their corresponding data silo.
Overall, in this work, we aim to preserve node-level privacy
against node-node attacks and subgraph-node attacks, and preserve
subgraph-level privacy against subgraph-subgraph attacks.
4 FRAMEWORK
In this section, we present the proposed Hierarchical Federated
Graph Learning (HiFGL) framework in detail. In particular, we
first present the hierarchical architecture for heterogeneous clients.
Then, we introduce the Secret Message Passing (SecMP) scheme
to achieve multi-level privacy preservation. Finally, we detail the
optimization scheme tailored for the framework.
4.1 The Hierarchical Architecture
We construct the hierarchical architecture of HiFGL consisting of
three modules, i.e., device-client, silo-client, and server, in bottom-
up order, as shown in Figure 2. Generally, under this hierarchy,
modules are set to administrate the ones at a subordinate level
and keep information private against the ones at a superordinate
level1. Specifically, the device-client holds local data and cooperates
with its silo-client for learning without data exposure. A silo-client
preserves local models that are federally optimized with the server,
administrates device-clients containing graph data, and determines
the privatization method. The server executes the federated op-
timization method for FL. In the following parts, we detail three
modules and their communication and visibility.
4.1.1 Device-client. The device-client has two roles, including stor-
ing the ego graph of each node and computing node gradients with
the silo-client, as shown in Figure 2 (c).
1) Storage. A device-client stores an ego graph consisting of the
target node’s features and its neighbors.
Definition 7. Ego Graph. An ego graph stores 𝑗-th node’s feature
ℎ(0)
𝑗, label𝑦𝑗, and 1-hop directed neighbors. We denote the ego graph
as˜𝑔𝑗=(ℎ(0)
𝑗,N𝑗).
We store data separately in each device for two reasons. First,
storing data at the device level can prevent node features from
being seen directly by adversaries at the silo or server level. Second,
cross-client edges are risky for exposing the silo’s node identities.
Suppose a directed cross-client edge connects a source and a target
node from two silos. However, storing the edge in any one of the
two silos will violate subgraph-level privacy requirements because
1In this paper, we use “the device-client’s silo-client” to denote “the silo-client that
administrates the device-client” and “the silo-client’s device-client” to denote “the
device-clients administrated by the silo-client”.
 
971HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
(b) Silo-client(a) Server
(c) Device-client
Privatized 
EmbeddingModel Gradient Model Parameter
Model Parameter
 Model Parameter
Invisiable
GNN
Encoding-Decoding 
Parameters
Optimization 
Scheme
Feature
Neighbors
(d) Communication & Visibility
Invisiable Invisiable
Figure 2: The architecture of HiFGL.
there will be at least one node to be seen by a silo-client that is not
its own. To address this challenge, we propose allowing two devices
corresponding to the two nodes to have access to this cross-client
edge in order to reduce the visibility of silos.
To construct ego graphs, we allow any two devices to connect
in a peer-to-peer manner to ensure that the connected edges are
only known by these two device-clients. For instance, two users
can build a friend relationship without the awareness of the social
network administrator. In the semi-honest setting, we consider the
connected edges to be true. In this way, we can construct the graph
in a privacy-preserving and decentralized way.
2) Gradients computation Considering the limited computing
power and local data size, instead of optimizing the full parameters
of the local model, the device-clients are only required to calcu-
late gradients associated with their local data. Specifically, for a
modelGparameterized by 𝜃, the𝑗-th device-client computes the
corresponding gradients by ∇𝑗𝜃=∇𝑓𝑙(G𝜃(ℎ(0)
𝑗),𝑦𝑗), where𝑓𝑙(·)
is the loss function. These gradients are not considered as private
information and can be shared with the corresponding silo-client.
4.1.2 Silo-client. As shown in Figure 2 (b), the silo-client is a mod-
ule between the server and the device-client, which is in charge
of local model optimization based on data in its device-clients. We
introduce two roles of each silo-client 𝐶𝑖, including storage and
model update.
1) Storage. The silo-client stores three elements, the subgraph,
the local model, and encoding-decoding parameters. First, 𝐶𝑖indi-
rectly keeps a federated subgraph ˜𝐺𝑖consisting of federated ego
graphs distributed in its device-clients D𝑖={𝐷𝑗|𝑗=1,2,···}. We
define the federated subgraph as below.
Definition 8. Federated Subgraph. Federated subgraph is de-
noted as ˜𝐺={𝐷𝑗|𝑗=1,2,···}, where each 𝐷𝑗stores a ˜𝑔𝑗, the𝑗-th
ego graph whose node 𝑣𝑗is in subgraph 𝐺.˜𝐺only keeps the identities
of the subordinate device-clients, but does not store any features and
neighbor information.
The federated subgraph avoids exposure of node features and
graph structures and therefore protects device-level privacy in the
federated learning process.Moreover, since the silo-client usually has more powerful com-
puting resources, we let 𝐶𝑖keep the local GNN G𝑖for optimiza-
tion. Additionally, 𝐶𝑖also preserves a set of individual encoding-
decoding parameters 𝜇𝑖generated by a polynomial function Ffor
privatizing node embeddings. Specifically, we define the parameters
𝜇={𝛼1,···,𝛼𝑇+1,𝛽1,···,𝛽𝑇+1,𝑧2,···,𝑧𝑇+1}, where𝛼1,···,𝛼𝑇+1,
𝛽1,···,𝛽𝑇+1are2𝑇+2distinct elements from the finite field 𝑭2,
satisfying{𝛼1,···,𝛼𝑇+1}∩{𝛽1,···,𝛽𝑇+1}=∅, and𝑧2,···,𝑧𝑇+1
are𝑇uniformly distributed vector.
2) Model update. The silo-client 𝐶𝑖is responsible to optimize G𝑖
based on gradients computed by device-clients in D. Specifically, 𝐶𝑖
collects the gradients set {∇𝑗𝜃𝑖|𝐷𝑗∈D𝑖}and compute the average
gradients as∇𝜃𝑖=1
|D𝑖|Í
𝐷𝑗∈D𝑖∇𝑗𝜃𝑖, where𝜃𝑖is parameters ofG𝑖.
The averaged gradients can be further utilized for optimization on
G𝑖. For example, we can leverage gradient descent as 𝜃𝑖=𝜃𝑖−𝜚∇𝜃𝑖
where𝜚is the learning rate.
4.1.3 Server. The server coordinates the entire framework as shown
in Figure 2 (a), whose role is to enable federated optimization
schemes (e.g., FedAvg [ 22]) among silo-clients. In practice, the
server can be an administrator of silo-clients to supervise their
sensitive activities in optimization. For instance, in a finance sce-
nario, the server can be a banking authority (e.g., European Banking
Authority), silo-clients are banks, and device-clients are different
users. The federation is under the authority’s control to prevent
potential attacks among banks and users.
4.1.4 Communication and Data Visibility. The cross-level com-
munication and data visibility among three different modules are
shown in Figure 2 (d). First, the server and silo-clients can trans-
fer model parameters during federated optimization. Second, silo-
clients are forbidden to exchange information, including device
identities, models, and encoding-decoding parameters. Third, dur-
ing the joint learning process, the device-client can access the model
of the silo-client and send local gradients to the silo-client for model
update, while the silo-client can not directly access the device-
client’s data. Last, device-clients communicate with each other
through privatized embeddings without exposure of node features
and neighborhood information.
4.2 Secret Message Passing
We further propose a novel graph learning method, SecMP, to pre-
serve multi-level privacy without information loss. In particular,
we develop the neighbor-agnostic aggregation andhierarchical La-
grangian embedding against potential subgraph-node and subgraph-
subgraph attacks. Specifically, the neighbor-agnostic aggregation
decomposes the feature extraction function in conventional GNN
into two individual steps to mutually cover neighbor information
between any connected node pair, preventing subgraph-level sensi-
tive information exchange. Meanwhile, the hierarchical Lagrangian
embedding utilizes Lagrange polynomial functions to mask sharing
node embeddings during message passing recoverably.
4.2.1 Neighbor-agnostic aggregation. The aggregation step in GNNs
may leak subgraph-level privacy, since such a process requires ac-
cessing structural information from cross-client neighbors [ 10,13,
2Following [37], we assume that 𝑭is a finite field with 11elements.
 
972KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
30]. Take GCN [ 13] as an example, the aggregation process can
be denoted by ℎ𝑢=Í
𝑣∈N𝑢(1/(√︁
|N𝑢|√︁
|N𝑣|))ℎ𝑣W+𝑏,whereℎis
node embedding, 𝑏andWare learnable parameters, and |N|are the
number of neighbors. Calculating |N𝑢|and|N𝑣|requires knowing
precise numbers of 𝑢and𝑣, which is risky if the computation only
inside either device-client 𝑢or𝑣.
To address this problem, we devise the neighbor-agnostic aggre-
gation to split GNN aggregating operations into source-side and
target-side steps, which makes the aggregation process agnostic
with neighbors’ information. Specifically, we operate a message
passing from node 𝑣to node𝑢in the source device-client and tar-
get device-client that correspond to two nodes, respectively. First,
device-client 𝑣will calculate a source-side function on ℎ𝑣to get ˆℎ𝑣
and pass it to device-client 𝑢. Then, device-client 𝑢will calculate
a target-side function to update its embedding. The source-side
function can be defined as
ˆℎ𝑣=(1/√︁
|N𝑣|)·ℎ𝑣, (2)
and the target-side function can be defined as
ℎ𝑢=𝑏+(1/√︁
|N𝑢|)∑︁
𝑣∈N𝑢ˆℎ𝑣·W. (3)
In this way, we can preserve the privacy of both device-client 𝑢and
𝑣’s neighbor lists by delegating operation-related private data to be
computed within device-clients.
4.2.2 Hierarchical Lagrangian embedding. Sharing embeddings
with nodes’ neighbors may induce severe node-level privacy leak-
age. Therefore, we privatize device-clients’ information without in-
formation loss by introducing Lagrange Coded Computing (LCC) [ 37],
a secret sharing technique in GNN optimization. Specifically, our
method follows an encoding-decoding workflow based on a group
of parameters 𝜇introduced in Section 4.1.2. In the encoding step,
we use Lagrange interpolation polynomial to encode data to coded
versions. In the decoding step, the aggregated coded vectors can be
decoded without any information loss.
Encoding. Any ego device-clients 𝐷𝑠edged with a target device-
client𝐷𝑡of silo-client 𝐶will access𝜇and use them for constructing
a polynomial function based on 𝐷𝑠’s embedding3ℎ𝑣as
𝑔𝑣(𝑥)=ℎ𝑣·Ö
𝑘∈[𝑇+1]\{1}𝑥−𝛽𝑘
𝛽1−𝛽𝑘+𝑇+1∑︁
𝑗=2𝑧𝑗·Ö
𝑘∈[𝑇+1]\{𝑗}𝑥−𝛽𝑘
𝛽𝑗−𝛽𝑘,(4)
𝐷𝑠generates𝑇+1coded embeddings ˜ℎ𝑣=𝑔𝑣(𝛼),𝛼∈𝛼1,···,𝛼𝑇+1.
Decoding.𝐷𝑡will receive𝑇+1coded embeddings4ℎ1,···,ℎ𝑇+1
from each source neighbor device-client 𝐷𝑠∈N𝐷𝑡.𝐷𝑡will delegate
the aggregated coded embedding to 𝐶for decoding by calculating
values of the Lagrange interpolated polynomial function at 𝑥=𝛽1.
4.2.3 Overall workflow. We summarize the overall workflow of
SecMP that incorporates neighbor-agnostic aggregation and hier-
archical Lagrangian embedding for subgraph-level and node-level
privacy protection. As illustrated in Figure 3, SecMP follows gen-
eral GNN pipeline [ 6] in three major steps, i.e.,privatized message,
secure aggregation, and neighbor-agnostic update.
3The embedding can be computed by a source-side function as Equation 2.
4The embedding can be computed by a target-side function as Equation 3.
1
3
5
4
2
Silo-client A
Silo-client B
Device-client 3
Device-client 43
4
NodeServer
Edge
 GNN
Encoding-Decoding 
ParametersEncode
Send
Aggregate
DecodeAggregateRepresentingFeature
Feature EmbeddingEmbedding Coded Embedding
Representing
Update
Aggregated 
Coded 
EmbeddingAggregated 
EmbeddingFigure 3: Overall workflow of Secret Message Passing.
1) Privatized message. We first emit encoded embedding to the
target device-client. In this way, device-clients are only aware of
neighbors’ coded embeddings instead of any concrete features. This
step includes three sub-steps. a) Device-clients project raw features
into node embeddings on the source-side. b) Device-clients request
encoding-decoding parameters from neighbors’ silo-clients. For
each neighbor, the device-client will have one corresponding group
of parameters for encoding. c) Device-clients encode embeddings
and send encoded embeddings to both intra-client and cross-client
target neighbor device-clients.
2) Secure aggregation. Then, the encoded embeddings from
multiple neighbors are aggregated to derive a unified embedding.
The silo-client can decode the aggregated embeddings without
accessing the embeddings of each individual node, while device-
clients cannot decode encrypted embeddings, therefore protecting
individual embeddings. Two sub-steps of secure aggregation are
elaborated below. a) Device-clients aggregate received encoded em-
beddings into a unified embedding. b) Device-clients send the aggre-
gated embeddings to their silo-clients, which can use its encoding-
decoding parameters to decode the aggregated embedding and send
it back to device-clients.
3) Neighbor-agnostic update. After secure aggregation, the
target device-clients obtain the aggregated embeddings computed
from the source side, and update their embeddings without requir-
ing access to neighbors’ structure information.
Note that an exceptional case is that when a node has only one
neighbor, the corresponding device-client can get the node embed-
ding after aggregation because the decoded aggregated embedding
is exactly the embedding of the neighbor. A possible solution is
to perturb embeddings by Differential Privacy (DP) techniques [ 5]
to cover sensitive information. The DP-based masking operation
also prevents attacks that infer original information by gradients or
embeddings [ 44]. Besides, we build HiFGL on the Trusted Execution
Environment (TEE) [ 24] and ensure the transition is anonymous
for any receivers to eliminate exposure.
4.3 Privacy-preserving Optimization
Then, we present the privacy-preserving optimization algorithm for
HiFGL. Without loss generality, we assume the local model is GCN
for the node classification task, and the federated optimization algo-
rithm is FedAvg. The full pipeline is reported in Appendix A.2. We
 
973HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
specify two critical steps in training, including local optimization
and federated optimization.
Local optimization. We optimize local models based on updated
embeddings computed by SecMP. Specifically, we first calculate
estimated classification probability as ˆ𝑦=SoftMax(ℎ(𝑘))where𝑘
is the index of the last GNN layer. Then each device-client applies
cross entropy loss on predicted labels ˆ𝑦and true labels 𝑦to calculate
the loss value 𝑙𝑗=Í
𝑞𝑦𝑞logˆ𝑦𝑞and generate gradients ∇𝑙𝑗. After
that, gradients from device-clients will be transmitted to their silo-
clients. Lastly, silo-clients average them as gradients of this round
as∇L𝑖=(1/|D𝑖|)Í
𝐷𝑗∈D𝑖∇𝑙𝑗=(1/|D𝑖|)Í
𝐷𝑗∈D𝑖∇Í
𝑞𝑦𝑞logˆ𝑦𝑞,
for local optimization on GNN parameters 𝜃(𝑛+1)
𝑖.
Federated optimization. When silo-clients finish their local op-
timization, the server will leverage FedAvg to average local models
as the new global model as 𝜃(𝑛+1)=(1/|C|)Í
𝐶𝑖∈C𝜃(𝑛+1)
𝑖. Updated
parameters are the initial ones of the next round of optimization.
Federated optimization is recurrently executed until the perfor-
mance of each silo-client tends to converge.
5 ANALYSIS
5.1 Privacy Analysis
Subgraph-level privacy. In our solution, we preserve privacy by
developing a hierarchical structure and a secure learning method.
Specifically, for storage structure, edges are preserved between two
device-clients corresponding to the two nodes instead of within
the silo-clients, restraining features’ exposure to other silo-clients.
Besides, to protect against privacy leakage during message passing,
the neighbor-agnostic strategy ensures that the target device-client
only gets the encoded embedding and sends the aggregated one to
its silo-client for decoding, where the silo-client cannot know the
raw embedding of neighbors. In this way, 0%subgraph-level privacy
is leaked when executing SecMP according to the quantifying metric
defined in Appendix A.1.
Node-level privacy. We analyze the privacy leakage of node
embeddings ℎof device-clients. Based on Equation 4, we select
𝛼𝑖∈𝛼1,···,𝛼𝑇+1to encodeℎas˜ℎ=𝑔(𝛼𝑖)=(ℎ,𝑧2,···,𝑧𝑇+1)·𝑈𝑖,
where𝑔(𝛽𝑖)=𝑧𝑖and𝑈∈𝑭(𝑇+1)×(𝑇+1)is the encoding matrix
defined as
𝑈𝑖,𝑗=Ö
𝑘∈[𝑇+1]\{𝑖}𝛼𝑗−𝛽𝑘
𝛽𝑖−𝛽𝑘. (5)
Any neighbor device-client will receive an encoded embedding
˜ℎ=ℎ𝑈top
𝑡+𝑍𝑈bottom
𝑡,where𝑡∈ [𝑇],𝑍=(𝑧2,···,𝑧𝑇+1), and
𝑈top∈𝑭𝑇,𝑈bottom∈𝑭𝑇×𝑇are the𝑡-th columns top and bottom
submatrices in 𝑈.
Lemma 5.1. The𝑇×𝑇matrix𝑈bottomis invertible.
Please refer Appendix A.3 for Lemma 5.1. Since 𝑈bottomis invert-
ible, we can mask encoded data ℎ𝑈top
𝑡as𝑍is uniformly randomized,
andℎcannot be directly decoded for any 𝑇≥1,i.e.,𝑈bottom
𝑡exists.
5.2 Complexity Analysis
Here we analyze the communication, encoding and decoding, and
space complexity of HiFGL. We denote 𝜉as the size of model pa-
rameters,𝑑as the dimension of coded or non-coded embeddings ℎ
or˜ℎ,𝛾as the dimension of output prediction.Table 2: Overall information of datasets
Datasets Cora CiteSeer PubMed
#of Silo-clients 5 5 5
#of Node 542 665 3943
#of Intra-client Edges 431 183 1772
#of Cross-client Edges 4199 3637 35461
#of Classes 7 6 3
Partition 6/2/2 6/2/2 6/2/2
Communication complexity. We analyze three types of communi-
cation complexity guarantees of HiFGL. The communication com-
plexity isO(𝑑𝑇)between a pair of connected device-clients. For
a silo-client with its device-clients, the communication complex-
ity for message passing and decoding is O(2|D|𝜉)andO(𝑑|𝑉𝑖|+Í
𝑣𝑗∈𝑉𝑖𝑑𝑇|𝑁𝑗|), respectively, where |D|is the number of the silo-
client’s device-clients. Last, the communication complexity between
silo-clients and the server is O(2𝜉)for each silo-client and O(2|C|𝜉)
for the server. More details are provided in Appendix A.4.1. Encod-
ing and decoding complexity. The encoding and decoding complex-
ity and be approximately guaranteed both as O(𝑑𝑇)according to
analysis in Appendix A.4.2. Space complexity. As analyzed in Ap-
pendix A.4.3, the space complexity of HiFGL is O(|G|·(𝜉+3𝑇+2)+
|E|+|H|)and we guarantee the increased complexity 𝛿Ssatisfying
O(|G|𝜉)≤O( ΔS)≤O(| G|𝜉+|E\(E1∪E2∪···)|) .
6 EXPERIMENTS
6.1 Experimental Setup
Datasets. We leverage popular graph datasets for node classifica-
tion tasks, including Cora [ 28], CiteSeer [ 28], and PubMed [ 28].
Following previous FGL benchmarks [ 32,35], we split the above
graph datasets randomly to 5subgraphs with comparable node
numbers. Details of datasets are presented in Table 2.
Baselines. For framework-level experiments, we compare HiFGL
with five baseline frameworks (Local, FedAvg [ 22], FedProx [ 15],
FedPer [ 1], and Global) incorporated with two popular GNNs vari-
ants (GCN [ 13] and GraphSage [ 7]) as backbones. Besides, we also
test performance of state-of-the-art FGL methods, including Fed-
PerGNN [ 33], FedSage+ [ 40], and FED-PUB [ 2]. More information
on baseline models is listed in Appendix A.5.
Metrics. We evaluate node classification performance by accu-
racy (ACC), i.e., the global percentage of accurately predicted sam-
ples, rather than the average of silo-client accuracy. In particu-
lar, we design a new metric, i.e.,Graph Information Gain, to show
how much graph information has been modeled as Gain(★-G)=
ACC(★-G)− ACC(Local-MLP)
ACC(Global-G)− ACC(Local-MLP),where ★-Gdenotes a GNN back-
bone model under a framework.
Implementation details. We set the hidden dimension for GNNs
and multi-layer perceptron as 64, and the input and output dimen-
sions depend on the raw feature dimensions and the number of
classes for each dataset. We implement all GNNs with 2layers and
train them for maximum 50epochs or two hours with Adam [ 12]
optimizer where the learning rate is set as 0.01and multiplies
gamma 0.9for every 4epoch. The 𝑇in embedding privatization
 
974KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
Table 3: The prediction ACC and graph information gain of different FGL frameworks.
ACC Cora CiteSeer PubMed
Mean Std Gain Mean Std Gain Mean Std Gain
Local-MLP 0.5698 ±0.0071 +0% 0.6450 ±0.0061 +0% 0.8051 +0.0006 +0%
Local-GCN 0.8095 ±0.0149 +80.14% 0.7429 ±0.0135 +75.77% 0.8525 ±0.0073 +89.10%
FedAvg-GCN 0.8358 ±0.0135 +88.93% 0.7601 ±0.0152 +89.09% 0.8603 ±0.0095 +103.76%
HiFGL-GCN 0.8555 ±0.0162 +95.52% 0.7724 ±0.0108 +98.61% 0.8626 ±0.0064 +108.08%
Global-GCN 0.8689 ±0.0182 +100% 0.7742 ±0.0115 +100% 0.8583 ±0.0033 +100%
Local-GraphSage 0.6207 ±0.0103 +17.03% 0.6125 ±0.0077 -25.04% 0.8221 ±0.0101 +29.72%
FedAvg-GraphSage 0.8095 ±0.0123 +80.22% 0.7656 ±0.0139 +92.91% 0.8444 ±0.0098 +68.71%
HiFGL-GraphSage 0.8642 ±0.0288 +98.53% 0.7791 ±0.0112 +103.31% 0.8504 ±0.0169 +79.20%
Global-GraphSage 0.8686 ±0.0215 +100% 0.7748 ±0.0127 +100% 0.8623 ±0.0058 +100%
is set as 1. The HiFGL framework is implemented based on Py-
Torch, and PyTorch-Lightning runs on the machine with Intel Xeon
Gold 6148 @ 2.40GHz, V100 GPU and 64G memory. Our codes are
open-sourced at https://github.com/usail-hkust/HiFGL.
6.2 Overall Prediction Performance
In this subsection, we show the results of the node classification
task including HiFGL and baseline frameworks and algorithms.
Specifically, we first compare HiFGL with three different frame-
works to demonstrate the increased predictive ability brought by
information integrity. Second, we conduct experiments to illustrate
that for FGL, information integrity is more important than any
other algorithm-level improvement.
Framework-level results. We investigate predictive knowledge
retrieved under different frameworks measured by Graph Infor-
mation Gain. First, we define the predictability lower bound and
upper bound as 1) Lower bound (0%) : the performance of Local-MLP,
which only separately learns the knowledge from raw features
through a 2-layer multi-layer perceptron; 2) Upper bound ( 100%) :
the performance of trained 2-layer backbone GNN (GCN and Graph-
Sage) on the Global setting, which extracts both knowledge from
raw features and graph information. Then, we consider the gap be-
tween the lower and upper bound as graph information knowledge.
Therefore, we test GNNs under different frameworks and collect
the mean and standard deviation of ACC for five times experiments
and graph information gain in Table 3. Results show that under
HiFGL, GCN and GraphSage have more graph information gain over
than Local and FedAvg and comparable with under Global. Specif-
ically, on three datasets, HiFGL-GCN outperforms 6.59%, 9.52%,
and4.32%than FedAvg-GCN, and HiFGL-GraphSage outperforms
18.31%, 10.40%, and 10.49% than FedAvg-GraphSage. The model
improvement demonstrates that besides ensuring privacy and com-
plexity, HiFGL enhances information integrity to achieve better
accuracy approximately equal ones without FL settings because
the graph information gain of HiFGL is obtained from preserved
cross-client edges, which offers rich relational knowledge to learn
more effective embeddings.
Algorithm-level results. The compared results among different
FGL methods are depicted in Table 4. We compare HiFGL with three
optimization schemes, i.e., FedAvg, FedProx, and FedPer, with two
GNNs, i.e., GCN and GraphSage. Besides, we involve FedPerGNN,Table 4: The prediction ACC of different FGL algorithms.
Cora CiteSeer PubMed
FedAvg-GCN 0.8358 0.7601 0.8603
FedProx-GCN 0.8238 0.7612 0.8305
FedPer-GCN 0.8202 0.7403 0.8471
FedAvg-GraphSage 0.8295 0.7540 0.8524
FedProx-GraphSage 0.8256 0.7593 0.8391
FedPer-GraphSage 0.8174 0.7625 0.8439
FedPerGNN 0.8351 0.7488 0.8346
FedSage+ 0.7767 0.7567 0.8394
FED-PUB 0.8370 0.7579 0.8457
HiFGL-GCN 0.8555 0.7724 0.8626
HiFGL-GraphSage 0.8642 0.7791 0.8504
FedSage+, and FED-PUB, which design tailored advanced graph
learning modules for FGL. We observe that under HiFGL, GCN,
and GraphSage defeat all tested methods. Specifically, with the in-
tegrated information preserved by the HiFGL, GCN achieves an
accuracy of 0.8555, over 2.3%,3.8%, and 4.3%improvement than Fe-
dAvg, FedProx, and FedPer on Cora dataset, respectively. Similarly,
GraphSage gets 4.1%,4.6%, and 5.7%improvement. Significant per-
formance promotion is attained on CiteSeer and PubMed datasets.
The reason is that for GCN and GraphSage that highly depend on
structure information, graph integrity can offer great benefits. In ad-
dition, against methods with more advanced GNNs, HiFGL wins on
prediction accuracy only by incorporating simple backbones. For ex-
ample, under HiFGL, GraphSage surpasses FedPerGNN, FedSage+,
and FED-PUB by 4.0%,3.0%, and 2.8%, respectively, on CiteSeer.
These experimental results demonstrate that graph integrity can
bring much effectiveness improvement than GNN modification
including high-order relationships incorporated in FedPerGNN,
missing neighbor generation in FedSage+, and community-aware
personalization in FED-PUB.
6.3 Efficiency Results
We study the efficiency by evaluating time and memory cost through
varying two GNN hyper-parameters, i.e., layer numbers and hidden
dimensions, and take HiFGL-GCN as the testing model.
 
975HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 5: Training results of different GNN layers.
#of GNN Layers 1 2 4 8 16
ACC (%) 73.04 77.91 70.87 39.84 26.02
Epoch Time (s) 6.5 4.3 5.6 8.4 16.4
Memory (GB) 3.41 3.45 3.49 3.59 3.80
Table 6: Training results of different hidden dimensions.
Hidden Dimensions 4 8 16 32 64 128
ACC (%) 83.45 85.15 85.33 85.27 85.88 86.39
Epoch Time ( 𝑠)17.2 18.05 20.81 19.84 23.49 44.30
Memory (GB) 3.753±0.005
GNN layers. First, we set GNN ranging from 1to16layers, with
a hidden dimension of 64, on the CiteSeer dataset. We display results
in Table 5 that the highest ACC occurs in 2-layer GCN, and the
performance decreases for more and fewer layers. The training time
per epoch and the memory both increase along with the growth of
the layer number. The results show that 2-layer GCN in the HiFGL
framework is the most efficient model for ACC and training time,
and it does not need expensive memory for training.
GNN dimension. We also set different hidden dimensions of a
2-layer GCN, ranging from 4to128, on the PubMed dataset. Table 6
demonstrates that performance increases with larger hidden di-
mensions since a more complicated model can catch more intricate
patterns while obstructing high efficiency. Besides, the memory
for different hidden dimensions is only slightly different because
parameters are not dominant for memory costs compared with the
other parts of the HiFGL framework under our implementation.
In conclusion, higher performance is not always brought by ex-
pensive executive costs that multiple layers of GNNs need a heavy
training burden but are predicted poorly. Increasing hidden dimen-
sions of GNNs helps the ACC, but the time consumption grows
exponentially. Therefore, we select 2-layer GNNs with the hidden
dimension of 64for our main experiments. Our framework has
sacrificed efficiency to achieve more advanced privacy preserva-
tion, especially during training stages equipping with SecMP. The
imperfection of inefficient processes remains a future development
such as faster secret sharing techniques.
6.4 Local Prediction Performance
We also investigate the local performance of each silo-client. Specif-
ically, we collect the ACC of local models in all training epochs
and show them in Figure 4, respectively. We discover that local
models do not simultaneously upgrade or degrade during train-
ing in Figure 4(a). Instead, the five curves fluctuate in inconsistent
upward patterns, which means a tradeoff is made among them
during co-optimization directed by the server for global ACC im-
provement. Moreover, the converged performance of silo-clients is
slightly diverse for their data distribution difference in Figure 4(b).
The non-IID issue naturally exists in FL. In our example, the dis-
tributions of node features and graph structures in silo-clients are
diverse naturally, which causes local models to be optimized in
different directions during global loss degradation.
0 10 20 30 40
Epochs0.10.20.30.40.50.60.70.80.9AccuracyClient 1
Client 2
Client 3
Client 4
Client 5(a) Training ACC of silo-client models.
Client 1 Client 2 Client 3 Client 4 Client 5
Client0.00.10.20.30.40.50.60.70.8Accuracy (b) Average ACC of silo-client models.
Figure 4: Local model performance on Cora dataset.
1 5 10 20 30 50
Client Number0.7000.7250.7500.7750.8000.8250.8500.8750.900AccuracyHiFGL
FedAvg
(a) ACC
1 5 10 20 30 50
Client Number0.00.51.01.52.02.53.03.5AccuracyHiFGL
FedAvg (b) Epoch time
Figure 5: Sensitivity for different silo-client numbers under
the HiFGL and FedAvg frameworks.
6.5 Sensitivity of Silo-client Number
We evaluate the sensitivity of the silo-client number under different
frameworks. Specifically, we vary it from 1to50and take GraphSage
as the backbone model to find out the variation of ACC in Figure 5(a).
Moreover, we also present executive time costs for different silo-
client numbers in Figure 5(b). The number hardly influences model
performance under HiFGL, while the ACC decreases as the number
increases, and models of HiFGL are trained slightly longer than
FedAvg. Because for FedAvg, a large number of silo-clients means
that fewer time-consuming cross-client message passing operations
are taken, and fragmentary information remaining on each silo-
client is inadequate for precise training. These results further prove
the importance of graph integrity for an FGL framework.
7 CONCLUSION
This paper first studies cross-silo cross-device FGL, where we pro-
pose a HiFGL framework based on a novel hierarchical architecture
on heterogeneous clients. Moreover, we devise a tailored graph
learning algorithm, SecMP, for multi-level privacy-preserving opti-
mization with graph integrity. Theoretical analyses are provided for
privacy and efficiency guarantees. Extensive experiments demon-
strate the prediction improvement gained from graph integrity
on three datasets. HiFGL offers versatility in a wider range of real-
world applications, even in solely cross-silo or cross-device settings.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foun-
dation of China (Grant No.62102110, No.92370204), National Key
R&D Program of China (Grant No.2023YFF0725004), Guangzhou-
HKUST(GZ) Joint Funding Program (Grant No. 2022A03J00056,
No.2023A03J0008), Education Bureau of Guangzhou Municipality.
 
976KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
REFERENCES
[1]Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav
Choudhary. 2019. Federated learning with personalization layers. arXiv preprint
arXiv:1912.00818 (2019).
[2]Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, and Sung Ju Hwang.
2023. Personalized subgraph federated learning. In International Conference on
Machine Learning. PMLR, 1396–1415.
[3]E Berlekamp. 2006. Nonbinary BCH decoding (Abstr.). IEEE Transactions on
Information Theory 14, 2 (2006), 242–242.
[4]Fahao Chen, Peng Li, Toshiaki Miyazaki, and Celimuge Wu. 2021. Fedgraph:
Federated graph learning with intelligent sampling. IEEE Transactions on Parallel
and Distributed Systems 33, 8 (2021), 1775–1786.
[5]Cynthia Dwork, Aaron Roth, et al .2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[6]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263–1272.
[7]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[8]Bing He, Dian Zhang, Siyuan Liu, Hao Liu, Dawei Han, and Lionel M Ni. 2018.
Profiling driver behavior for personalized insurance pricing and maximal profit.
In2018 IEEE International Conference on Big Data (Big Data). IEEE, 1387–1396.
[9]Chao Huang, Jianwei Huang, and Xin Liu. 2022. Cross-silo federated learning:
Challenges and opportunities. arXiv preprint arXiv:2206.12949 (2022).
[10] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.
2020. Graph structure learning for robust graph neural networks. In Proceedings
of the 26th ACM SIGKDD international conference on knowledge discovery & data
mining. 66–74.
[11] Kiran S Kedlaya and Christopher Umans. 2011. Fast polynomial factorization
and modular composition. SIAM J. Comput. 40, 6 (2011), 1767–1802.
[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[13] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[14] Runze Lei, Pinghui Wang, Junzhou Zhao, Lin Lan, Jing Tao, Chao Deng, Junlan
Feng, Xidian Wang, and Xiaohong Guan. 2023. Federated Learning Over Coupled
Graphs. IEEE Transactions on Parallel and Distributed Systems 34, 4 (2023), 1159–
1172.
[15] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine learning and systems 2 (2020), 429–450.
[16] Tzu-Heng Lin, Chen Gao, and Yong Li. 2019. Cross: Cross-platform recommen-
dation for social e-commerce. In Proceedings of the 42nd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 515–524.
[17] Fan Liu, Hao Liu, and Wenzhao Jiang. 2022. Practical adversarial attacks on spa-
tiotemporal traffic forecasting models. Advances in Neural Information Processing
Systems 35 (2022), 19035–19047.
[18] Rui Liu, Pengwei Xing, Zichao Deng, Anran Li, Cuntai Guan, and Han Yu. 2024.
Federated Graph Neural Networks: Overview, Techniques, and Challenges. IEEE
Transactions on Neural Networks and Learning Systems (2024).
[19] Tao Liu, Peng Li, and Yu Gu. 2021. Glint: Decentralized federated graph learning
with traffic throttling and flow scheduling. In 2021 IEEE/ACM 29th International
Symposium on Quality of Service (IWQOS). IEEE, 1–10.
[20] Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. 2022. Feder-
ated social recommendation with graph neural network. ACM Transactions on
Intelligent Systems and Technology (TIST) 13, 4 (2022), 1–24.
[21] Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J Shane Culpepper, Haochao
Ying, Hao Liu, and Hui Xiong. 2020. Spatial object recommendation with hints:
When spatial granularity matters. In Proceedings of the 43rd International ACM
SIGIR Conference on research and development in information retrieval. 781–790.
[22] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
1273–1282.
[23] Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2021. Cross-Node Federated
Graph Neural Network for Spatio-Temporal Data Modeling. knowledge discovery
and data mining (2021).
[24] Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, and
Nicolas Kourtellis. 2021. PPFL: privacy-preserving federated learning with trusted
execution environments. In Proceedings of the 19th annual international conference
on mobile systems, applications, and services. 94–108.
[25] Qiying Pan and Yifei Zhu. 2022. FedWalk: Communication Efficient Federated
Unsupervised Node Embedding with Differential Privacy. knowledge discovery
and data mining (2022).[26] Qiying Pan, Yifei Zhu, and Lingyang Chu. 2023. Lumos: Heterogeneity-aware Fed-
erated Graph Learning over Decentralized Devices. In 2023 IEEE 39th International
Conference on Data Engineering (ICDE). IEEE Computer Society, 1914–1926.
[27] Liang Qu, Ningzhi Tang, Ruiqi Zheng, Quoc Viet Hung Nguyen, Zi Huang, Yuhui
Shi, and Hongzhi Yin. 2023. Semi-decentralized federated ego graph learning for
recommendation. In Proceedings of the ACM Web Conference 2023. 339–348.
[28] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[29] Toyotaro Suzumura, Yi Zhou, Natahalie Baracaldo, Guangnan Ye, Keith Houck,
Ryo Kawahara, Ali Anwar, Lucia Larise Stavarache, Yuji Watanabe, Pablo Loyola,
Daniel Klyashtorny, Heiko Ludwig, and Kumar Bhaskaran. 2019. Towards Fed-
erated Graph Learning for Collaborative Financial Crimes Detection. Research
Papers in Economics (2019).
[30] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.
[31] Binghui Wang, Ang Li, Meng Pang, Hai Li, and Yiran Chen. 2022. Graphfl: A
federated learning framework for semi-supervised node classification on graphs.
In2022 IEEE International Conference on Data Mining (ICDM). IEEE, 498–507.
[32] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022. Federatedscope-gnn: Towards a unified, comprehensive and
efficient package for federated graph learning. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 4110–4120.
[33] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Tao Qi, Yongfeng Huang, and Xing
Xie. 2022. A federated graph neural network framework for privacy-preserving
personalization. Nature Communications 13, 1 (2022), 1–10.
[34] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059–1072.
[35] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059–1072.
[36] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine
Learning: Concept and Applications. ACM Transactions on Intelligent Systems
and Technology (2019).
[37] Qian Yu, Songze Li, Netanel Raviv, Seyed Mohammadreza Mousavi Kalan, Mahdi
Soltanolkotabi, and Salman A Avestimehr. 2019. Lagrange coded computing:
Optimal design for resiliency, security, and privacy. In The 22nd International
Conference on Artificial Intelligence and Statistics. PMLR, 1215–1225.
[38] Xiaoming Yuan, Jiahui Chen, Jiayu Yang, Ning Zhang, Tingting Yang, Tao Han,
and Amir Taherkordi. 2022. Fedstn: Graph representation driven federated learn-
ing for edge computing enabled urban traffic flow prediction. IEEE Transactions
on Intelligent Transportation Systems (2022).
[39] Binchi Zhang, Minnan Luo, Shangbin Feng, Ziqi Liu, Jun Zhou, and Qinghua
Zheng. 2021. Ppsgcn: A privacy-preserving subgraph sampling based distributed
gcn training method. arXiv preprint arXiv:2110.12906 (2021).
[40] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. 2021. Sub-
graph federated learning with missing neighbor generation. Advances in Neural
Information Processing Systems 34 (2021), 6671–6682.
[41] Weijia Zhang, Hao Liu, Lijun Zha, Hengshu Zhu, Ji Liu, Dejing Dou, and Hui
Xiong. 2021. MugRep: A multi-task hierarchical graph representation learning
framework for real estate appraisal. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 3937–3947.
[42] Xiaojin Zhang, Hanlin Gu, Lixin Fan, Kai Chen, and Qiang Yang. 2022. No free
lunch theorem for security and utility in federated learning. ACM Transactions
on Intelligent Systems and Technology 14, 1 (2022), 1–35.
[43] Xiaojin Zhang, Yan Kang, Kai Chen, Lixin Fan, and Qiang Yang. 2023. Trading
Off Privacy, Utility, and Efficiency in Federated Learning. ACM Transactions on
Intelligent Systems and Technology 14, 6 (2023), 1–32.
[44] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients.
Advances in neural information processing systems 32 (2019).
A APPENDIX
A.1 Quantifying Subgraph-level Privacy
Leakage
We analyze the subgraph-level privacy leakage of previous cross-
client FGL frameworks. Subgraph-level privacy leakage derives
from the information sharing across cross-client edges, which lets
silo-clients see nodes from others. There are works such as Fed-
Sage [ 40] and FedPUB [ 2] that drop cross-client edges to sacrifice
 
977HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 1: Training algorithm for GCN based on SecMP.
Input: The graph G, the server𝑆,|G|silo-clients C(each𝐶𝑖
with a local model G𝑖),𝑛𝑑device-clients D(each𝐷𝑗
with feature ℎ(0)and neighborsN𝐷𝑗}).
Output: The converged global model parameterized with
𝜃(𝑛)trained after 𝑛epochsG𝜃(𝑛).
1Initialize each silo-client’s model parameters with 𝜃(0)and
encoding-decoding parameters
𝜇={𝛼1,···,𝛼𝑇+1,𝛽1,···,𝛽𝑇+1,𝑧2,···,𝑧𝑇+1};
2whileG𝜃(𝑛)is not converged do
3 foreach𝐶𝑖∈Cin parallel do
4 foreach𝐷𝑗∈D𝑖in parallel do
5 ˆℎ(0)←1√
|N𝑗|ℎ(0);
6 foreach𝐷𝑝∈N𝑗do
7 Get𝜇𝑝from𝐷𝑝’s silo-client;
8 ˜ℎ(0)←F( ˆℎ(0);𝜇𝑝);
9 Send coded embedding ˜ℎ(0)to𝐷𝑝;
10 end
/* After𝐷𝑗receives all coded
embeddings ˜HN𝑗fromN𝑗 */
11 ˜ℎ(𝑘)
N𝑗←Í
˜ℎ(0)
𝑝∈˜HN𝑗1√
|N𝑗|G𝑖(˜ℎ(0)
𝑝;𝜃𝑖);
12 Send ˜ℎ(𝑘)
N𝑗to𝐶𝑖, and𝐶𝑖decodes it according to
Fand𝜇𝑖asℎ(𝑘)
N𝑗and send it back to 𝐷𝑗;
13 ℎ(𝑘)←ℎ(𝑘)
N𝑗;
14 Calculate estimated probability as
ˆ𝑦←SoftMax(ℎ(𝑘));
15 Compute loss 𝑙𝑗of prediction ˆ𝑦and labels𝑦;
16 Send gradient∇𝑙𝑗to𝐶𝑖;
17 end
18 Optimize𝜃(𝑛+1)
𝑖with∇L𝑖←1
|D𝑖|Í
𝐷𝑗∈D𝑖∇𝑙𝑗;
19 end
20 Update global parameters as 𝜃(𝑛+1)←1
|C|Í
𝐶𝑖∈C𝜃(𝑛+1)
𝑖;
21end
the information integrity to perfectly preserve subgraph-level pri-
vacy. However, other works such as Glint [ 19] choose to permit
silo-clients to access the features of the nodes from other silo-clients
connected with their own nodes. Here the subgraph-level privacy
leakage of𝑖-th silo-client 𝜖𝑖𝑝can be defined as
𝜖𝑖
𝑝=|{D𝑗|D𝑗∈C𝑖𝑎𝑛𝑑D′∉C𝑖,∃D′∈N𝑗}|, (6)
where we utilize the number of nodes that possess cross-client
edges (i.e., neighbors out of their own silo-clients). Subsequently,
we compute the average of 𝜖𝑖𝑝among silo-clients as the global
privacy leakage by 𝜖𝑝=1
|C|Í𝑖=1
|C|𝜖𝑖𝑝.Under this formulation, on experimental datasets introduced in
Section 6.1, we observe that privacy leakage is remarkably signifi-
cant, which is 95.16%on Cora, 88.70%on CiteSeer, and 90.31%on
PubMed for 5silo-clients. Otherwise, 90.69%, 95.21%, and 95.24%
edges are lost on Cora, CiteSeer, and PubMed for 5silo-clients, re-
spectively, if we protect privacy by dropping all cross-client edges.
Therefore, preventing nodes from being seen by other silo-clients
is necessary for FGL frameworks.
A.2 Training Pipeline
As an example, here we appoint GCN as the backbone model and Fe-
dAvg as the federated optimization method to explain the workflow
in Algorithm 1. Specifically, after initialization of model parameters
with𝜃(0)and encoding-decoding parameters 𝜇, the optimization
round will be executed recurrently. Training will be stopped after
global convergence.
A.3 Proof of Lemma 5.1
Proof. Before we give proof, we revisit the selected 2𝑇distinct
elements which satisfying {𝛼1,···,𝛼𝑇+1}∩{𝛽1,···,𝛽𝑇+1}=∅.
Thus𝛼𝑖−𝛼𝑗,𝛽𝑖−𝛽𝑗, and𝛼𝑖−𝛽𝑗are non-zero for any different
𝑖and𝑗. Here we start to validate 𝑈bottom’s invertibility. First, we
multiply every 𝑖-th row byÎ
𝑘∈[𝑇+1]\{𝑖}(𝛽𝑖−𝛽𝑘)≠0to obtain
𝑈bottom
𝑖,𝑗=Ö
𝑘∈[𝑇+1]\{𝑖}(𝛼𝑗−𝛽𝑘). (7)
Second, we multiply every 𝑗-th column byÎ
𝑘∈[𝑇+1]1
𝛼𝑗−𝛽𝑘≠0as
𝑈bottom
𝑖,𝑗=1
𝛼𝑗−𝛽𝑖, (8)
Third, we subtract the 𝑛-th column from the first 𝑛−1columns and
extract the common factor as
𝑈bottom
𝑛×𝑛=𝑏1−𝑏𝑛
(𝑎1−𝑏1)(𝑎1−𝑏𝑛)𝑏2−𝑏𝑛
(𝑎1−𝑏2)(𝑎1−𝑏𝑛)···1
𝑎1−𝑏𝑛
𝑏1−𝑏𝑛
(𝑎2−𝑏1)(𝑎2−𝑏𝑛)𝑏2−𝑏𝑛
(𝑎2−𝑏2)(𝑎2−𝑏𝑛)···1
𝑎2−𝑏𝑛
𝑏1−𝑏𝑛
(𝑎𝑛−𝑏1)(𝑎𝑛−𝑏𝑛)𝑏2−𝑏𝑛
(𝑎𝑛−𝑏2)(𝑎𝑛−𝑏𝑛)···1
𝑎𝑛−𝑏𝑛
=𝑛−1Î
𝑖=1(𝑏𝑖−𝑏𝑛)
𝑛Î
𝑗=1(𝑎𝑗−𝑏𝑛)1
𝑎1−𝑏11
𝑎1−𝑏2···1
1
𝑎2−𝑏11
𝑎2−𝑏2···1
......···...
1
𝑎𝑛−𝑏11
𝑎𝑛−𝑏2···1(9)
Obviously, the values of the 𝑛-th column are the same, thus we
can subtract the last row from the first 𝑛−1rows and extract the
common factor as
𝑈bottom
𝑛×𝑛=𝑛−1Î
𝑖=1(𝑏𝑖−𝑏𝑛)
𝑛Î
𝑗=1(𝑎𝑗−𝑏𝑛)𝑎𝑛−𝑎1
(𝑎1−𝑏1)(𝑎𝑛−𝑏1)𝑎𝑛−𝑎1
(𝑎1−𝑏2)(𝑎𝑛−𝑏2)···0
𝑎𝑛−𝑎2
(𝑎2−𝑏1)(𝑎𝑛−𝑏1)𝑎𝑛−𝑎2
(𝑎2−𝑏2)(𝑎𝑛−𝑏2)···0
1
𝑎𝑛−𝑏11
𝑎𝑛−𝑏2···1
=𝑛−1Î
𝑖=1(𝑎𝑛−𝑎𝑖)(𝑏𝑖−𝑏𝑛)
𝑛Î
𝑗=1(𝑎𝑗−𝑏𝑛)𝑛−1Î
𝑘=1(𝑎𝑛−𝑏𝑘)1
𝑎1−𝑏11
𝑎1−𝑏2···1
𝑎1−𝑏𝑛−1
1
𝑎2−𝑏11
𝑎2−𝑏2···1
𝑎2−𝑏𝑛−1
......···...
1
𝑎𝑛−1−𝑏11
𝑎𝑛−1−𝑏2···1
𝑎𝑛−1−𝑏𝑛−1
(10)
where we can obtain a recursion formula that
𝑈bottom
𝑛×𝑛=Î𝑛−1
𝑖=1(𝑎𝑛−𝑎𝑖)(𝑏𝑖−𝑏𝑛)
Î𝑛
𝑗=1(𝑎𝑗−𝑏𝑛)Î𝑛−1
𝑘=1(𝑎𝑛−𝑏𝑘)𝑈bottom
(𝑛−1)×(𝑛−1).(11)
 
978KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhuoning Guo, Duanyi Yao, Qiang Yang, & Hao Liu
Last, by continuing the recursive process, we can derive the deter-
minant as
det𝑈bottom
𝑛×𝑛=Î
1≤𝑖<𝑗≤𝑛(𝑎𝑗−𝑎𝑖)(𝑏𝑖−𝑏𝑗)
Î𝑛
𝑖,𝑗=1(𝑎𝑖−𝑏𝑗)≠0. (12)
Hence, the matrix 𝑈bottomis invertible. □
A.4 Complexity Analysis Details
Here we analyze communication, encoding and decoding, and space
complexity, which are based on the following conditions. The size
of encoding-decoding parameters 𝜇is3𝑇+2. Proofs are satisfied
with 1)𝑇≤10, 2)𝛾≤10, 3)𝑑>>𝑇, and𝑑>>𝛾based on realistic
assumption and experimental datasets.
A.4.1 Communication Complexity. The communication is com-
posed of three parts, including 1) communication between device-
clients : coded embedding exchange between device-clients and
device-clients, 2) communication between device-clients and silo-
clients : aggregated embedding communication between device-
clients and silo-clients, and 3) communication between silo-clients
and the server : model parameters sharing between silo-clients and
the server.
1)Communication between device-clients. The communication
between any two connected device-clients (from a source device-
client to a target device-client) consists of two parts. First, the
target device-client passes the 3𝑇+2encoding parameters of its
silo-client to the source device-client. Second, after coding by the
source device-client, the coded embedding will be sent to the target
device-client as 𝐾parts by𝑇+1times at least. Therefore, for any
directed edge, the minimum communication complexity between
two device-clients can be computed as O(3𝑇+2+𝑑(𝑇+1))≈O(𝑑𝑇).
2)Communication between device-clients and silo-clients. Each
device-client will download the latest model parameters and up-
load its gradients at each training round. Thus, the communication
complexity between a single device-client and the corresponding
silo-client can be measured by 𝜉, the size of the model parameters.
In a word, the communication complexity for a device-client with
its silo-client isO(2𝜉), and for a silo-client with its device-clients is
O(2|D|𝜉)that|D|is the number of the silo-client’s device-clients. In
the decoding stage, when device-client 𝐷𝑗transmits coded embed-
dings to the silo-client 𝐶𝑖for decoding, the complexity is O(𝑑𝑇|𝑁𝑗|)
from𝐷𝑗to𝐶𝑖andO(𝑑)from𝐶𝑖to𝐷𝑗. Totally, the complexity of
communication for 𝐶𝑖isO(𝑑|𝑉𝑖|+Í
𝑣𝑗∈𝑉𝑖𝑑𝑇|𝑁𝑗|)at each round.
3)Communication between silo-clients and the server. The commu-
nication between silo-clients and the server obeys the traditional
FL setting, which can be computed as O(2𝜉)for each silo-client
andO(2|C|𝜉)for the server.
A.4.2 Encoding and Decoding Complexity. We then present the
analysis of Lagrange interpolation-based encoding and decoding
complexity. Specifically, according to existing mathematical proof [ 11],
the complexity of interpolating a 𝑘-degree polynomial can be eval-
uated asO(𝑘log2𝑘log log𝑘). Therefore, we compute the encodingcomplexity asO(𝑑(𝑇+1)log2(𝑇+1)log log(𝑇+1)) ≈ O(𝑑𝑇).
Besides, using the proposed technique [ 3], we can decode the em-
bedding with a complexity as O(𝑑(𝑇+1)log2𝑇log log𝑇)≈O(𝑑𝑇).
A.4.3 Space Complexity. The space complexity Sof HiFGL on
a graph G=(V,E,H), is the summation of silo-client’s model
complexityO(|G|·(𝜉+3𝑇+2))and device-client’s data (raw features
and neighbor device-client pointers) complexity O(|E|+|H|), which
can be formulated as
S=O(|G|·(𝜉+3𝑇+2)+|E|+|H|), (13)
where|G|is the number of silo-clients (or subgraphs), |E|is the
number of edges, and |H|is the raw feature complexity.
Conventionally, in FGL, researchers proposed two schemes for
graph storage. 1) Trusted server for cross-client edges and silo-
clients for intra-client edges. Its space complexity is O(|G|·𝜉+
|E|+|H|). 2) Only silo-clients store intra-client edges. Its space
complexity isO(|G|·𝜉+Í
G𝑖∈{G}|E𝑖|+|H|). In this work, the HiFGL
framework only needs tiny extra storage memory ΔScompared
with two traditional schemes, which can be stated as
O(|G|𝜉)≤O( ΔS)≤O(| G|𝜉+|E\(E1∪E2∪···)|), (14)
where|E\(E1∪E2∪···)|≤| E|.
A.5 Baseline Information
We involve five baseline frameworks in our experiments, including
•Local : models are individually trained on silo-clients.
•FedAvg [22]: a collaborative learning framework that aver-
ages local model parameters for global optimization.
•FedProx [ 15]: a more robust federated optimization method
based on FedAvg with regularization of parameters.
•FedPer [1]: a personalized FL algorithm allowing some layers
to be free from FedAvg training for better local fitness.
•Global : a usual graph training way without distributed or
private constraints.
Furthermore, we compare two popular GNNs as backbone models
within federated frameworks, including
•GCN [ 13]: a spectral-based graph convolutional network
form capturing first-order structure feature with node infor-
mation for node representation learning.
•GraphSage [ 7]: a spatial-based graph model aggregating sam-
pled neighbor information with ego features for inductive
graph learning.
We also leverage 2-layer multi-layer perceptron (MLP) under Local
frameworks for comparison with GNNs. Besides, we test classifica-
tion performance on state-of-the-art FGL methods, including
•FedPerGNN [ 33]: a cross-device FL method on personalized
federated item recommendation.
•FedSage+ [ 40]: a federated inductive graph learning model
with neighbor generation for missing edge reconstruction.
•FED-PUB [ 2]: a personalized federated subgraph learning
method incorporating community structures and masked
graph convolutional networks.
 
979