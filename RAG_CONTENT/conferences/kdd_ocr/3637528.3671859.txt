Fast Computation of Kemeny’s Constant for Directed Graphs
Haisong Xia
Fudan University
Shanghai, China
hsxia22@m.fudan.edu.cnZhongzhi Zhang∗
Fudan University
Shanghai, China
zhangzz@fudan.edu.cn
Abstract
Kemeny’s constant for random walks on a graph is defined as the
mean hitting time from one node to another selected randomly
according to the stationary distribution. It has found numerous
applications and attracted considerable research interest. However,
exact computation of Kemeny’s constant requires matrix inversion,
which scales poorly for large networks with millions of nodes. Exist-
ing approximation algorithms either leverage properties exclusive
to undirected graphs or involve inefficient simulation, leaving room
for further optimization. To address these limitations for directed
graphs, we propose two novel approximation algorithms for es-
timating Kemeny’s constant on directed graphs with theoretical
error guarantees. Extensive numerical experiments on real-world
networks validate the superiority of our algorithms over baseline
methods in terms of efficiency and accuracy.
CCS Concepts
•Theory of computation →Graph algorithms analysis; Ran-
dom walks and Markov chains.
Keywords
Random walk; approximation algorithm; hitting time; Kemeny’s
constant; spectral graph theory.
ACM Reference Format:
Haisong Xia and Zhongzhi Zhang. 2024. Fast Computation of Kemeny’s
Constant for Directed Graphs. In Proceedings of Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24).
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671859
1 Introduction
Random walks on complex networks have emerged as a power-
ful analytical tool with broad applications including recommenda-
tion systems [ 31], representation learning [ 38], privacy amplifica-
tion [ 28], and so on. For a random walk on a graph, a fundamental
quantity is the hitting time 𝐻𝑖 𝑗, which is defined as the expected
number of steps for a walker starting from node 𝑖to visit node 𝑗for
the first time. As a key quantity, hitting times have been widely uti-
lized across domains to address problems in complex networks, such
as assessing transmission costs in communication networks [ 14],
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671859developing clustering algorithms [ 1,10], and identifying significant
nodes [34].
Stemming from the hitting time, many important quantities for
random walks can be formulated, such as Kemeny’s constant. For
random walks on a graph, Kemeny’s constant is defined as the
expected hitting time from an arbitrary source node to the target
selected randomly according to the stationary distribution of the
random walk. Kemeny’s constant has found various applications in
diverse areas. First, it is one of the widely used connectivity [ 6] or
criticality [ 23] measures for a graph. Second, based on Kemeny’s
constant, an edge centrality [ 3,26] was defined to identify important
edges. Finally, Kemeny’s constant was applied to characterize the
performance of consensus protocols with noise [19].
Despite the utility of Kemeny’s constant across various applica-
tions, directly computing it on large real-world networks remains
prohibitively expensive. As discussed in Section 2.3, calculating
Kemeny’s constant involves matrix inversion, whose complexity
is𝑂(𝑛3)for an𝑛-node graph. This cubic scaling renders exact
computation infeasible for networks with millions of nodes.
In order to reduce computational time for Kemeny’s constant,
some approximation algorithms have been developed to estimate
this graph invariant. Xu et al. [37] proposed ApproxKemeny, which
is based on Hutchinson’s method and the nearly linear-time Lapla-
cian solver [ 22]. However, results in Section 5 indicates that Approx-
Kemeny requires much more memory space than other methods, re-
ducing its scalability for large networks. Very recently, Li et al. [27]
provided DynamicMC, which is based on simulating truncated ran-
dom walks. While its GPU implementation achieves state-of-the-art
performance, DynamicMC still has many opportunities for further
optimization.
On the other hand, most of existing methods for estimating
Kemeny’s constant are restricted to undirected networks, includ-
ingApproxKemeny andDynamicMC. For example, the Laplacian
solver [ 22] leverages some specific properties of undirected graphs,
thus ApproxKemeny fails to support digraphs. Although Dynam-
icMC can handle digraphs, its theoretical guarantees are not readily
extended to directed graphs from the perspective in [ 27]. Nonethe-
less, many real-world networks are inherently directed, such as
citation networks, the World Wide Web, and online social networks.
The lack of an efficient approximation algorithm for estimating Ke-
meny’s constant on directed graphs limits further applications on
these important networks.
Motivated by DynamicMC, we provide an approximation algo-
rithm ImprovedMC for estimating Kemeny’s constant of digraphs
with error guarantee. Apart from simulating truncated random
*Corresponding author. Both authors are with Shanghai Key Laboratory of Intelligent
Information Processing, School of Computer Science, Fudan University, Shanghai,
200433, China. Zhongzhi Zhang is also with Institute of Intelligent Complex Systems,
Fudan University, Shanghai, 200433, China.
3472
KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
walks, ImprovedMC also incorporates several optimization tech-
niques. First, ImprovedMC adaptively determines the amount of
simulation initialized from each node, reducing unnecessary sim-
ulation without sacrificing theoretical accuracy. Additionally, Im-
provedMC restricts simulation to a subset of nodes in the network.
By sampling from selected starting nodes, ImprovedMC achieves
sublinear time complexity while still preserving theoretical accu-
racy. Extensive experiments reveal that compared with the state-
of-the-art method DynamicMC, ImprovedMC attains up to 800×
speedup, while achieving comparable accuracy. To further improve
the accuracy, we derive an alternative formula that connects Ke-
meny’s constant with the inverse of a submatrix associated with
the transition matrix. This motivates the development of a new
Monte Carlo algorithm TreeMC based on directed tree sampling.
We experimentally demonstrate the superiority of TreeMC over
the state-of-the-art method in terms of both efficiency and accuracy.
The key contributions of our work are summarized as follows.
•First, we develop an improved Monte Carlo algorithm Im-
provedMC to estimate Kemeny’s constant of digraphs by
simulating truncated random walks. ImprovedMC achieves
sublinear time complexity while still ensuring provable ac-
curacy guarantees.
•Based on a derived alternative formula, we propose another
Monte Carlo algorithm TreeMC that approximates Kemeny’s
constant of digraphs by sampling directed rooted spanning
trees, which is considerably accurate.
•We conduct extensive experiments on real-world networks.
The results indicate that both of our proposed algorithms
outperform the baseline approaches by orders of magnitude
speed-up, while still retaining comparable accuracy.
2 Preliminaries
2.1 Notations
LetRdenote the set of real numbers. We use regular lowercase
letters like𝑎,𝑏,𝑐 for scalars within R. Bold lowercase letters, such as
𝒂,𝒃,𝒄, represent vectors, while bold uppercase letters, like 𝑨,𝑩,𝑪,
denote matrices. Specific elements are accessed by using subscripts:
𝑎𝑖for the𝑖thelement of 𝒂and𝑨𝑖,𝑗for the entry at position (𝑖,𝑗).
Subvectors and submatrices are similarly indicated with subscript
numerals. For example, 𝒂−𝑖denotes the subvector of 𝒂obtained by
excluding its 𝑖thelement, while 𝑨−𝑖represents the submatrix of
𝑨constructed by removing its 𝑖throw and𝑖thcolumn. Crucially,
subscripts take precedence over superscripts in this notation. Conse-
quently, 𝑨−1
−𝑖represents the inverse of 𝑨−𝑖, rather than a submatrix
of𝑨−1. We use 1to denote a vector of specific dimensions with
all elements being 1. Table 1 lists the frequently used notations
throughout this paper.
2.2 Graph and Random Walk
Let𝐺=(𝑉,𝐸)be a digraph, where 𝑉is the set of nodes, and 𝐸is
the set of edges. The digraph 𝐺has a total of 𝑛=|𝑉|nodes and
𝑚=|𝐸|edges. Throughout this paper, all the digraphs mentioned
are assumed to be strongly connected without explicit qualification.
The adjacency matrix 𝑨of graph𝐺mathematically encodes its
topological properties. Here, the entry 𝑨𝑖,𝑗represents the adjacencyTable 1: Frequently used notations.
Notation Description
𝐺=(𝑉,𝐸)A digraph with node set 𝑉and edge set 𝐸.
𝑛,𝑚 The number of nodes and edges in 𝐺.
𝜋𝑖The stationary distribution of node 𝑖.
𝑷The transition matrix of random walks on 𝐺.
𝜆𝑖The𝑖thlargest eigenvalue of 𝑷sorted by modulus.
𝜆Denoted as|𝜆2|.
ˆ𝑡(𝑙)
𝑖,𝑗 The returning times to node 𝑖of the𝑗th𝑙-truncated
random walk that starts from 𝑖.
ˆ𝑡𝑖,𝑗 The returning times to node 𝑖of the𝑗thabsorbing
random walk that starts from 𝑖.
¯𝑡(𝑙)
𝑖,¯𝑡𝑖The empirical mean of ˆ𝑡(𝑙)
𝑖,𝑗andˆ𝑡𝑖,𝑗.
relation between nodes 𝑖and𝑗.𝑨𝑖,𝑗=1if there exists a directed
edge pointing from 𝑖to𝑗. Conversely, the absence of such an edge
is indicated by 𝑨𝑖,𝑗=0. In a digraph 𝐺, the out-degree 𝑑𝑖of node𝑖
is defined as the number of its out-neighbours. If we denote the di-
agonal out-degree matrix of digraph 𝐺as𝑫=diag(𝑑1,𝑑2,...,𝑑 𝑛),
the Laplacian matrix of 𝐺is defined as 𝑳=𝑫−𝑨.
For a digraph 𝐺with𝑛nodes, a random walk on 𝐺is defined
through its transition matrix 𝑷∈R𝑛×𝑛. At each step, if the walker
is at node𝑖, it moves to an out-neighbour 𝑗with equal probabil-
ity𝑷𝑖,𝑗. It follows readily that 𝑷=𝑫−1𝑨. Assuming 𝑷is finite,
aperiodic, and irreducible, the random walk has an unique station-
ary distribution 𝝅=(𝜋1,𝜋2,···,𝜋𝑛)⊤, satisfying 𝝅⊤1=1and
𝝅⊤𝑷=𝑷. Clearly, 𝝅is the left 1-eigenvector of 𝑷. Let𝜆1,𝜆2,...,𝜆 𝑛
be the eigenvalues of 𝑷, where 1=|𝜆1|>|𝜆2|≥···≥|𝜆𝑛|. The
second largest eigenvalue of 𝑷is crucial to our algorithms, whose
modulus is denoted as 𝜆.
For a random walk on digraph 𝐺, numerous associated quantities
can be expressed in terms of the fundamental matrix 𝑭[30]. For a
random walk with transition matrix 𝑷, the fundamental matrix is
defined as the group inverse of 𝑰−𝑷:
𝑭=(𝑰−𝑷)#= 𝑰−𝑷+1𝝅⊤−1−1𝝅⊤.
As the generalized inverse, 𝑭satisfies(𝑰−𝑷)𝑭(𝑰−𝑷)=𝑰−𝑷.
Additionally, we can easily prove that 𝑭and𝑰−𝑷share the same
left null space and right null space. Concretely, 𝑭1=(𝑰−𝑷)1=0
and𝝅⊤𝑭=𝝅⊤(𝑰−𝑷)=0⊤.
2.3 Hitting Time and Kemeny’s Constant
A key concept in random walks is hitting time [ 13,29]. The hit-
ting time𝐻𝑖 𝑗is defined as the expected time for a random walker
originating at node 𝑖to arrive at node 𝑗for the first time. Several
important quantities can be derived from the hitting time, here we
only consider the absorbing random-walk centrality and Kemeny’s
constant.
For a node 𝑠in the𝑛-node digraph 𝐺=(𝑉,𝐸), its absorbing
random-walk centrality 𝐻𝑠is defined as 𝐻𝑠=Í𝑛
𝑖=1𝜋𝑖𝐻𝑖𝑠. Lower
values of𝐻𝑠indicate higher importance for node 𝑠, which has
been analyzed extensively [ 7,8,32]. For brevity, we refer to 𝐻𝑠=Í𝑛
𝑖=1𝜋𝑖𝐻𝑖𝑠aswalk centrality henceforth. For an 𝑛-node digraph
𝐺=(𝑉,𝐸), its Kemeny’s constant 𝐾is defined as the expected steps
3473Fast Computation of Kemeny’s Constant for Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
for a walker starting from node 𝑢to node𝑖selected with probability
𝜋𝑖. Formally,𝐾=Í𝑛
𝑖=1𝜋𝑖𝐻𝑢𝑖. The invariance of Kemeny’s constant
𝐾stems from the fact that its value remains unchanged regardless
of the chosen starting node 𝑢.
As discussed in Section 2.2, many quantities associated with
random walks are determined by the fundamental matrix 𝑭. For
example, the hitting time 𝐻𝑖 𝑗satisfies𝐻𝑖 𝑗=𝜋−1
𝑗 𝑭𝑗,𝑗−𝑭𝑖,𝑗[30].
Therefore, the walk centrality can be expressed as
𝐻𝑠=𝑛∑︁
𝑖=1𝜋𝑖𝐻𝑖𝑠=𝑛∑︁
𝑖=1𝜋𝑖
𝜋𝑠 𝑭𝑠,𝑠−𝑭𝑖,𝑠=𝑭𝑠,𝑠
𝜋𝑠, (1)
while Kemeny’s constant can be represented as
𝐾=𝑛∑︁
𝑖=1𝜋𝑖𝐻𝑢𝑖=𝑛∑︁
𝑖=1 𝑭𝑖,𝑖−𝑭𝑢,𝑖=Tr(𝑭). (2)
The Kemeny constant can be exactly computed by using (2).
However, this formula requires all the diagonal elements of a group
inverse. Since the complexity of matrix inversion is 𝑂(𝑛3), direct
computation of Kemeny’s constant is impractical for large-scale
networks with millions of nodes.
2.4 Existing Methods
2.4.1 Method based on Laplacian solver. For an undirected graph,
its Kemeny’s constant is equal to the trace of L†, where Ldenotes
the normalized Laplacian matrix. Using Hutchinson’s method [ 18],
ApproxKemeny by Xu et al. [37] reformulates estimating Tr L†
as approximating the quadratic forms of L†, which is connected
to solving linear equations associated with the Laplacian matrix.
Leveraging a nearly linear-time Laplacian solver [ 22],Approx-
Kemeny attains nearly linear-time complexity in terms of edge
number. However, as shown in Section 5, the high memory usage
of Laplacian solver makes this algorithm impractical for large-scale
networks. Additionally, the Laplacian solver inherently leverages
specific properties of undirected graphs, precluding its application
toApproxKemeny for digraphs.
2.4.2 Method based on truncated random walks. To estimate Ke-
meny’s constant, DynamicMC [27] simulates truncated random
walks starting from each node in the network, and then sums up
the probabilities of returning to the source. DynamicMC also in-
corporates a heuristic strategy, where the simulation terminates
once the change of the sum falls below a given threshold. Under the
parallel GPU implementation, DynamicMC achieves exceptional
performance surpassing previous methods. However, the excep-
tional performance largely stems from GPU implementation, which
is not competitive under fair comparison in Section 5. Additionally,
the performance limitations of DynamicMC are two-fold. First, the
inclusion of this heuristic strategy lacks a theoretical guarantee for
maintaining the accuracy. Second, DynamicMC relies on theoret-
ical analysis for undirected graphs, whose Kemeny’s constant is
expressed as the infinite sum over powers of 𝜆𝑖. This expression
cannot be extended to digraphs, since the matrix 𝑷for a digraph
loses diagonalizability and 𝜆𝑖becomes complex-valued. Hence, Dy-
namicMC cannot be directly extended to digraphs. In contrast,
we address this issue by reformulating Kemeny’s constant as thetrace of fundamental matrix, which holds for both directed and
undirected graphs.
3 Theoretical Results
3.1 Approximation for Fundamental Matrix by
Truncated Sum
As shown in (2), the Kemeny constant is intimately related to the
diagonal elements of the fundamental matrix 𝑭. In this subsection,
we first demonstrate that the trace of 𝑭can be approximated by
an𝑙-truncated sum 𝑭(𝑙)with an additive error bound. Furthermore,
we prove that the diagonal elements of 𝑭can also be approximated
by𝑭(𝑙)with arbitrary accuracy.
Lemma 3.1. Let𝐺=(𝑉,𝐸)be a digraph with transition matrix
𝑷and stationary distribution 𝝅. The fundamental matrix 𝑭can be
expressed as
𝑭=(𝑰−𝑷)#=∞∑︁
𝑘=0 𝑷𝑘−1𝝅⊤.
Proof. According to the Perron-Frobenius theorem [ 9], the 1-
eigenvalue is simple. Recall that the left and right 1-eigenvector
of𝑷is, respectively, 𝝅⊤and1. Therefore, the spectral radius of
𝑷−1𝝅⊤is lower than 1, and 𝑭can be represented as
𝑭=(𝑰−𝑷)#= 𝑰−𝑷+1𝝅⊤−1−1𝝅⊤
=
𝑰− 𝑷−1𝝅⊤−1−1𝝅⊤
=−1𝝅⊤+∞∑︁
𝑘=0 𝑷−1𝝅⊤𝑘=∞∑︁
𝑘=0 𝑷𝑘−1𝝅⊤.
where the last equality can be easily obtained through mathematical
induction. □
Lemma 3.1 indicates that 𝑭can be represented by an infinite
sum. Therefore, we attempt to approximate 𝑭by an𝑙-truncated
sum, which is defined as 𝑭(𝑙)=Í𝑙
𝑘=0 𝑷𝑘−1𝝅⊤. We begin by
approximating Tr(𝑭)byTr 𝑭(𝑙)with a theoretical error bound.
Lemma 3.2. Let𝐺=(𝑉,𝐸)be an𝑛-node digraph with transition
matrix 𝑷. For any𝜖>0, if𝑙is selected satisfying 𝑙≥log(𝑛−1(𝜖−𝜖𝜆))
log(𝜆),
then we haveTr(𝑭)−Tr 𝑭(𝑙)≤𝜖.
Proof.
Tr(𝑭)−Tr 𝑭(𝑙)=∞∑︁
𝑘=𝑙+1
Tr 𝑷𝑘−1≤∞∑︁
𝑘=𝑙+1𝑛∑︁
𝑖=2|𝜆𝑖|𝑘
≤𝑛∞∑︁
𝑘=𝑙+1𝜆𝑘=𝑛𝜆𝑙+1
1−𝜆≤𝜖.
This finishes the proof. □
After giving a theoretical bound of Tr 𝑭(𝑙), we next approximate
the𝑖thdiagonal element of 𝑭with arbitrary accuracy. This poses a
greater challenge, as 𝑷is not diagonalizable for digraphs. In order
to bound the error introduced by the truncated length 𝑙, we need
the following lemma.
3474KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Lemma 3.3. [2] For a digraph 𝐺=(𝑉,𝐸)with transition matrix
𝑷and stationary distribution 𝝅, suppose there exists a probability
measure𝜇, a real number 𝛿>0and time𝑡such that 𝑷𝑡
𝑖,𝑗≥𝛿𝜇𝑗holds
for any nodes 𝑖,𝑗∈𝑉. Then for any node 𝑖∈𝑉and integer𝑘≥0, we
have𝑷𝑘
𝑖,𝑖−𝜋𝑖≤(1−𝛿)⌊𝑘/𝑡⌋.
Subsequently, we introduce a theoretical bound of 𝑭(𝑙)
𝑖,𝑖:
Lemma 3.4. Given an𝑛-node digraph 𝐺=(𝑉,𝐸), let𝑑maxbe the
maximum out-degree, and let 𝜏be the diameter of 𝐺, which is the
longest distance between nodes. For any 𝜖>0, if𝑙is selected satisfying
𝑙≥𝜏log(𝑛𝜖𝜏−1𝑑−𝜏
max)
log(1−𝑛𝑑−𝜏max)+𝜏−1, then we have𝑭𝑖,𝑖−𝑭(𝑙)
𝑖,𝑖≤𝜖.
3.2 Alternative Formula for Kemeny’s Constant
In this subsection, we introduce an alternative formula that asso-
ciates Kemeny’s constant of digraphs with a submatrix of 𝑰−𝑷. The
enhanced diagonal dominance of this submatrix facilitates more
accurate approximation.
Theorem 3.5. Let𝐺=(𝑉,𝐸)be an𝑛-node digraph with transition
matrix 𝑷, stationary distribution 𝝅and the fundamental matrix 𝑭.
For any node 𝑠∈𝑉, the Kemeny constant of 𝐺can be represented as
𝐾=Tr (𝑰−𝑷−𝑠)−1−𝐻𝑠. (3)
Proof. As shown in Section 2.2, the left and right null vectors
of𝑭may be inequivalent. This prompts us to use the diagonally
scaled fundamental matrix ˜𝑭.˜𝑭is defined as ˜𝑭=𝚷1/2𝑭𝚷−1/2, where
𝚷=diag(𝜋1,𝜋2,...,𝜋 𝑛). Similarly, we define the diagonally scaled
Laplacian as ˜𝑳=𝚷1/2(𝑰−𝑷)𝚷−1/2. It is easy to verify that ˜𝑭and
˜𝑳share the same left and right null vector 𝝅1/2.
Subsequently, we attempt to establish the connection between
˜𝑭−𝑠and ˜𝑳−1−𝑠. After properly adjusting the node labels, ˜𝑭and ˜𝑳can
be rewritten in block forms as
˜𝑭=˜𝑭−𝑠˜𝑭𝑇,𝑠
˜𝑭𝑠,𝑇˜𝑭𝑠,𝑠
,˜𝑳=˜𝑳−𝑠˜𝑳𝑇,𝑠
˜𝑳𝑠,𝑇˜𝑳𝑠,𝑠
,
where𝑇=𝑉\{𝑠}. If˜𝝅is denoted as 𝜋−1/2
𝑠𝝅1/2
−𝑠∈R𝑛−1, then we
prove that 𝑿=(𝑰−˜𝝅)˜𝑭𝑰
−˜𝝅⊤
equals to ˜𝑳−1−𝑠:
˜𝑳−𝑠𝑿˜𝑳−𝑠=˜𝑳−𝑠(𝑰−˜𝝅)˜𝑭𝑰
−˜𝝅⊤
˜𝑳−𝑠
=
˜𝑳−𝑠˜𝑳𝑇,𝑠
˜𝑭˜𝑳−𝑠
˜𝑳𝑠,𝑇
,
where the last equality is due to (˜𝝅1)˜𝑳=0⊤and ˜𝑳˜𝝅
1
=0. Then,
we obtain
˜𝑳−𝑠𝑿˜𝑳−𝑠=
˜𝑳−𝑠˜𝑳𝑇,𝑠
˜𝑭˜𝑳−𝑠
˜𝑳𝑠,𝑇
=(𝑰0)˜𝑳˜𝑭˜𝑳𝑰
0⊤
=(𝑰0)˜𝑳𝑰
0⊤
=˜𝑳−𝑠.Finally, we are able to prove Equation (3) as
Tr (𝑰−𝑷−𝑠)−1=Tr ˜𝑳−1
−𝑠=Tr(𝑿)
=Tr ˜𝑭−𝑠−˜𝝅⊤˜𝑭𝑇,𝑠−˜𝑭𝑠,𝑇˜𝝅+˜𝑭𝑠,𝑠˜𝝅⊤˜𝝅
=Tr ˜𝑭−𝑠+˜𝑭𝑠,𝑠 ˜𝝅⊤˜𝝅+2=Tr ˜𝑭+˜𝑭𝑠,𝑠
𝜋𝑠
=Tr(𝑭)+𝑭𝑠,𝑠
𝜋𝑠=𝐾+𝐻𝑠.
Here the fourth equality is due to (˜𝝅1)˜𝑭=0⊤and ˜𝑭˜𝝅
1
=0.□
Theorem 3.5 reveals that for any selected node 𝑠, the estimation
of Kemeny’s constant boils down to the evaluation of the trace of
(𝑰−𝑷−𝑠)−1and the walk centrality of 𝑠. This alternative formula
motivates us to design an approximation algorithm that estimates
these two quantities separately.
4 Algorithm Design
4.1 Truncated Random Walk Based Algorithm
Combining (2)with Lemma 3.2, we can approximate Kemeny’s
constant of a digraph with an 𝑙-truncated sum, which is defined as
𝐾(𝑙)=Tr 𝑭(𝑙)=𝑙∑︁
𝑘=0
Tr 𝑷𝑘−1
. (4)
We note that this estimator is the same as that in DynamicMC [27],
but is proposed from a different perspective. Equation (4)indicates
thatDynamicMC actually supports digraphs. While the analysis of
DynamicMC is restricted to undirected graphs, the approximation
error of𝐾(𝑙)for digraphs is analyzed in Lemma 3.2. Since the direct
computation of 𝑷𝑘
𝑖,𝑖involves time-consuming matrix multiplication,
we resort to a Monte Carlo approach similar to DynamicMC. Con-
cretely, for each node 𝑖∈𝑉, we simulate 𝑟independent 𝑙-truncated
random walks starting from 𝑖. Let ˆ𝑡(𝑙)
𝑖,𝑗denote the times of 𝑗thran-
dom walk that return to 𝑖, and let ¯𝑡(𝑙)
𝑖denote the empirical mean of
ˆ𝑡(𝑙)
𝑖,𝑗, then we can give an unbiased estimator of 𝐾(𝑙)based on ¯𝑡(𝑙)
𝑖,
which is defined as ˆ𝐾(𝑙)=𝑛−𝑙−1+Í𝑛
𝑖=1¯𝑡(𝑙)
𝑖.
Although ˆ𝐾(𝑙)is unbiased, we have to simulate numerous trun-
cated random walks to reduce the approximation error. The loose
theoretical error bound leads to excessive simulation, making the ap-
proximation algorithm inefficient in practice. Therefore, we utilize
several optimization techniques. Recall that the truncated random
walk is simulated through two steps: the iteration over nodes and
the simulation of random walks starting from each node. Below we
make improvements from these two perspectives.
4.1.1 Adaptive simulation from each node. In the analysis of Dy-
namicMC, Hoeffding’s inequality is utilized to derive a theoretical
bound.
Lemma 4.1 (Hoeffding’s ineqality). Let𝑥1,𝑥2,...,𝑥 𝑛be𝑛
independent random variables such that 𝑎≤𝑥𝑖≤𝑏for𝑖=1,2,...,𝑛 .
Let𝑥=Í𝑛
𝑖=1𝑥𝑖, then for any 𝜖>0,
Pr(|𝑥−E[𝑥]|≥𝜖)≤2 exp
−2𝜖2
𝑛(𝑏−𝑎)2
.
3475Fast Computation of Kemeny’s Constant for Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
However, as Hoeffding’s inequality does not consider the vari-
ance of random variables, the provided theoretical bound tends
to be relatively loose. To address this limitation, we leverage the
empirical Bernstein inequality [5]:
Lemma 4.2. Let𝑋1,𝑋2,...,𝑋 𝑛be𝑛real-valued i.i.d. random vari-
ables that satisfy 0≤𝑋𝑖≤𝑋sup. If we denote ¯𝑋and𝑋varas the
empirical mean and the empirical variance of 𝑋𝑖, then we have
Pr ¯𝑋−E[¯𝑋]≥𝑓 𝑛,𝑋 var,𝑋sup,𝛿≤𝛿,
where
𝑓 𝑛,𝑋 var,𝑋sup,𝛿=√︂
2𝑋varlog(3/𝛿)
𝑛+3𝑋suplog(3/𝛿)
𝑛.
Lemma 4.2 differs from Lemma 4.1 in that it involves the em-
pirical variance of random variables. While the empirical variance
remains unknown a priori, it can be efficiently maintained through-
out the simulation. Therefore, our first improvement implements
the empirical Bernstein inequality, but still retains the Hoeffding
bound to preserve theoretical accuracy. Meanwhile, if the empirical
error of ¯𝑡(𝑙)
𝑖provided by Lemma 4.2 falls below the threshold, we
terminate the simulation of random walks starting from 𝑖. Crucially,
the theoretical accuracy of estimating Kemeny’s constant remains
unaffected by applying this adaptive strategy.
4.1.2 Iteration over node subset. Note that Kemeny’s constant of 𝐺
is concerned with the sum of ¯𝑡(𝑙)
𝑖, we attempt to estimate it by only
summing a small proportion of ¯𝑡(𝑙)
𝑖, which significantly reduces the
required number of simulated random walks.
Specifically, for an 𝑛-node digraph 𝐺=(𝑉,𝐸), a node subset
X⊆𝑉of capacity𝑘≪𝑛is sampled uniformly at random. To effi-
ciently estimate Kemeny’s constant, the original sum 𝑆=Í
𝑢∈𝑉¯𝑡(𝑙)
𝑢
is replaced by the partial sum ˜𝑆=𝑛/𝑘Í
𝑖∈X¯𝑡(𝑙)
𝑖. As an unbiased
estimator of 𝑆,˜𝑆is also suitable for estimating Kemeny’s constant.
Lemma 4.3. ˜𝑆is an unbiased estimator for 𝑆.
Next, we provide a guarantee for the additive error of ˜𝑆:
Lemma 4.4. Given𝑛positive numbers 𝑥1,𝑥2,...,𝑥 𝑛∈[0,𝑏]with
their sum𝑥=Í𝑛
𝑖=1𝑥𝑖and an error parameter 𝜖>𝑛−1/2log1/2(2𝑛). If
we randomly select 𝑐=𝑂 𝑏𝜖−1𝑛1/2log1/2𝑛numbers,𝑥𝑣1,𝑥𝑣2,...,𝑥 𝑣𝑐
by Bernoulli trials with success probability 𝑝=𝑏𝜖−1𝑛−1/2log1/2(2𝑛)
satisfying 0<𝑝<1, and define ˜𝑥=Í𝑐
𝑖=1𝑥𝑣𝑖/𝑝, then ˜𝑥is an
approximation for the sum 𝑥of the original 𝑛numbers, satisfying
|𝑥−˜𝑥|≤𝑛𝜖.
4.1.3 Improved Monte Carlo algorithm. Using Lemmas 4.2 and 4.4,
we propose an improved MC algorithm for approximating Ke-
meny’s constant of digraphs, which is depicted in Algorithm 1.
According to Lemma 4.4, ImprovedMC randomly selects|X|=
𝑂 𝜖−1𝑙𝑛1/2log1/2𝑛nodes from𝑉(Line 2). ImprovedMC then simu-
lates𝑂 𝜖−2𝑙2log𝑛𝑙-truncated random walks from each selected
node𝑖(Lines 3-9). If the empirical error computed by Lemma 4.2
is less than threshold 𝜖/3, the simulation terminates (Line 8). By
summing up ¯𝑡(𝑙)
𝑖,ImprovedMC returns ˜𝐾(𝑙)as the approximation
for Kemeny’s constant. The performance of ImprovedMC is char-
acterized in Theorem 4.5.Algorithm 1: ImprovedMC(𝐺,𝜖)
Input :𝐺=(𝑉,𝐸): a digraph with 𝑛nodes;𝜖>0: an
error parameter
Output : ˜𝐾(𝑙): approximation of Kemeny’s constant 𝐾in𝐺
1𝑙←llog(3/(𝜖−𝜖𝜆))
log(1/𝜆)m
,𝑟←
9𝜖−2𝑙2log(2𝑛)/4
2Sample a node setX⊆𝑉satisfying
|X|=minnl
3𝜖−1𝑙𝑛1/2log1/2𝑛/2m
,𝑛o
3foreach node𝑖∈Xdo
4 ¯𝑡(𝑙)
𝑖←0
5 for𝑗=1,2,...,𝑟 do
6 Sample the 𝑗th𝑙-truncated random walk starting
from𝑖, and let ˆ𝑡(𝑙)
𝑖,𝑗be the times of walk that return
to𝑖
7 ¯𝑡(𝑙)
𝑖←¯𝑡(𝑙)
𝑖+ˆ𝑡(𝑙)
𝑖,𝑗
8 if𝑓 𝑗,ˆ𝑡(𝑙)
var,𝑙/2,1/𝑛≤𝑛𝜖/3then break
9 ¯𝑡(𝑙)
𝑖←¯𝑡(𝑙)
𝑖/𝑗
10return ˜𝐾(𝑙)=𝑛−𝑙−1+𝑛
|X|Í
𝑖∈X¯𝑡(𝑙)
𝑖
Theorem 4.5. For an𝑛-node digraph 𝐺and an error parameter
𝜖>0, Algorithm 1 runs in 𝑂 𝜖−3𝑙4𝑛1/2log3/2𝑛time, and returns
˜𝐾(𝑙)as the approximation for Kemeny’s constant 𝐾of𝐺, which is
guaranteed to satisfy˜𝐾(𝑙)−𝐾≤2𝜖𝐾with high probability.
Although the time complexity of ImprovedMC is sublinear with
respect to the number of nodes, this complexity bound remains
relatively loose due to the inclusion of Lemma 4.2.
4.2 Algorithm Based on Directed Tree Sampling
Although ImprovedMC achieves enhanced efficiency through opti-
mization techniques, its accuracy remains to be improved. Lever-
aging the alternative formula in Theorem 3.5, we propose another
MC algorithm TreeMC, which samples incoming directed rooted
spanning trees. Due to the improved diagonal dominance of the sub-
matrix in (3),TreeMC attains enhanced accuracy. After presenting
TreeMC, we analyze its time complexity and error guarantee.
Recall from Theorem 3.5 that the calculation of Kemeny’s con-
stant is reduced to the evaluation of Tr (𝑰−𝑷−𝑠)−1and𝐻𝑠. Mean-
while, Lemma 3.4 and (1)indicate that 𝐻𝑠can be estimated by sim-
ulating truncated random walks. Therefore, the main goal of our
MC algorithm is to approximate Tr (𝑰−𝑷−𝑠)−1.
For random walks in digraph 𝐺=(𝑉,𝐸)with absorbing node 𝑠,
(𝑰−𝑷−𝑠)−1
𝑖,𝑗denotes the expected passage times over node 𝑗by a
random walker initialized at node 𝑖prior to absorption at node 𝑠[40].
Using this physical meaning, we can estimate Tr (𝑰−𝑷−𝑠)−1
by simulating absorbing random walks from each node in 𝑉\
{𝑠}. However, the expected running time of single simulation isÍ𝑛
𝑖=1Í𝑛
𝑗=1(𝑰−𝑷−𝑠)−1
𝑖,𝑗=1⊤(𝑰−𝑷−𝑠)−11, which is large due to
the dense property of (𝑰−𝑷−𝑠)−1. To improve the efficiency of
simulation, we introduce the method of sampling incoming directed
rooted spanning trees, which is essentially simulating loop-erased
random walks.
3476KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Starting from a node, the loop-erased random walk consists of
two phases: simulation of a random walk as well as the erasure of
the loop within walk path. The loop-erased random walk is used
in Wilson’s algorithm [ 35], where loop-erased paths are iteratively
generated. The aggregate of these loop-erased paths is a directed,
cycle-free subgraph of 𝐺. In this subgraph, every non-root node
𝑖∈𝑉\{𝑠}has out-degree 1, while the root node 𝑠has out-degree
0. This subgraph is denoted as an incoming directed spanning tree
rooted at𝑠. Based on a variant of Wilson’s algorithm, we prove that
the diagonal elements of (𝑰−𝑷−𝑠)−1can be well approximated.
Lemma 4.6. For a digraph 𝐺=(𝑉,𝐸)with transition matrix 𝑷,
we simulate loop-erased random walks with an absorbing node 𝑠. Let
𝑡𝑖denote the passage times over node 𝑖, then we have
E[𝑡𝑖]=(𝑰−𝑷−𝑠)−1
𝑖,𝑖. (5)
Lemma 4.6 reveals that estimating Tr (𝑰−𝑷−𝑠)−1via a single
sampled spanning tree is equivalent to simulating 𝑛absorbing ran-
dom walks. However, spanning tree sampling proves substantially
more efficient than absorbing walk simulation. This efficiency gain
arises because loop-erased walks terminate upon revisiting prior
loop-free trajectories. Consequently, we estimate Tr (𝑰−𝑷−𝑠)−1
through sampling spanning trees rather than absorbing walk simu-
lations.
Specifically, we sample 𝑟incoming directed rooted spanning trees
by performing loop-erased random walks. Let ˆ𝑡𝑖,𝑗denote the passage
times over𝑖for the𝑗thsample, and ¯𝑡𝑖denote the empirical mean over
𝑗. ThenÍ
𝑖∈𝑉\{𝑠}¯𝑡𝑖is an unbiased estimator of Tr (𝑰−𝑷−𝑠)−1.
We continue to provide an error bound for the sample size 𝑟. To
this end, we bound the passage times ˆ𝑡𝑖,𝑗with high probability.
Lemma 4.7. Given an𝑛-node digraph 𝐺=(𝑉,𝐸)and an absorbing
node𝑠∈𝑉, let𝑑maxbe the maximum out-degree, and let 𝜏be the
diameter of𝐺, which is the longest distance between all pairs of nodes.
If𝑡is selected satisfying 𝑡≥e𝜏𝑑𝜏max
log 4𝑛2
/2, then
Pr ˆ𝑡𝑖,𝑗>𝑡≤1
4𝑛2.
Finally, we introduce a theoretical bound for the sample size 𝑟.
Lemma 4.8. Given an𝑛-node digraph 𝐺=(𝑉,𝐸)with absorb-
ing node𝑠, if we sample 𝑟incoming directed rooted spanning trees
satisfying𝑟≥𝜖−2e2𝜏2𝑑2𝜏max
log3 4𝑛2
, then for any 𝜖>0, we have
PrTr (𝑰−𝑷−𝑠)−1−∑︁
𝑖∈𝑉\{𝑠}¯𝑡𝑖≥𝑛𝜖
2
≤1
2𝑛.
Based on the above analyses, we propose a more accurate MC
algorithm TreeMC for estimating Kemeny’s constant, which is
depicted in Algorithm 2. Given an 𝑛-node digraph 𝐺and an error
parameter𝜖, By selecting absorbing node 𝑠as the node with the
largest𝜋𝑠,TreeMC first estimates Tr (𝑰−𝑷−𝑠)−1(Lines 8-12).
For this purpose, TreeMC samples𝑂 𝜖−2𝜏2𝑑2𝜏maxlog3𝑛incoming
directed rooted spanning trees. The sampling procedure consists
of the random walk part (Lines 8-12) and the loop-erasure part
(Lines 13-15). Then, TreeMC estimates𝐻𝑠(Lines 17-22). Analogous
to Algorithm 1, TreeMC also simulates 𝑙-truncated random walks
and introduces Lemma 4.2 for optimization. Combining these two
estimations, TreeMC returns ˆ𝐾as the approximation for Kemeny’sconstant. The performance of TreeMC is analyzed in Theorem 4.9.
Again, the additive error guarantee of 𝑛𝜖can be converted to the rel-
ative error guarantee of 2𝜖due to the minimum Kemeny’s constant
of𝑛-node digraph being (𝑛+1)/2[17].
Algorithm 2: TreeMC(𝐺,𝜖)
Input :𝐺=(𝑉,𝐸): a digraph with 𝑛nodes;𝜖: an error
parameter
Output : ˆ𝐾: approximation of Kemeny’s constant 𝐾in𝐺
1𝑠←arg max𝑖∈𝑉𝜋𝑖,𝑟←
𝜖−2e2𝜏2𝑑2𝜏max
log3 4𝑛2
2for𝑗=1,2,...,𝑟 do
3 ˆ𝑡𝑖,𝑗←0for𝑖∈𝑉\{𝑠}
4 next 𝑖←arbitrary value for 𝑖∈𝑉\{𝑠}
5 InTree 𝑖←false for𝑖∈𝑉\{𝑠}
6 InTree 𝑠←true
7 foreach𝑢∈𝑉\{𝑠}do
8𝑖←𝑢
9 while InTree 𝑖=false do
10 ˆ𝑡𝑖,𝑗←ˆ𝑡𝑖,𝑗+1
11 next 𝑖←a randomly selected out-neighbor of 𝑖
12 𝑖←next 𝑖
13𝑖←𝑢
14 while InTree 𝑖=false do
15 InTree 𝑖←true,𝑖←next 𝑖
16¯𝑡𝑖←Í𝑟
𝑗=1ˆ𝑡𝑖,𝑗/𝑟for𝑖∈𝑉\{𝑠}
17𝑙←l𝜏log(𝑛𝜖(2𝜏)−1𝑑−𝜏
max)
log(1−𝑛𝑑−𝜏max)m
+𝜏−1,𝑟′←l𝑙2log𝑛
2𝜖2𝜋2𝑠𝑛2m
,¯𝑡(𝑙)
𝑠←0
18for𝑗=1,2,...,𝑟′do
19 Sample the 𝑗th𝑙-truncated random walk starting from 𝑠,
and let ˆ𝑡(𝑙)
𝑠,𝑗be the times of walk returning to 𝑠
20 ¯𝑡(𝑙)
𝑠←¯𝑡(𝑙)
𝑠+ˆ𝑡(𝑙)
𝑠,𝑗
21 if𝑓 𝑗,ˆ𝑡(𝑙)
var,𝑙/2,1/𝑛≤√𝑛𝜖/2then break
22¯𝑡(𝑙)
𝑠←¯𝑡(𝑙)
𝑠/𝑗
23return ˆ𝐾=−𝜋−1𝑠 ¯𝑡(𝑙)
𝑠+1+Í
𝑖∈𝑉\{𝑠}¯𝑡𝑖
Theorem 4.9. For an𝑛-node digraph 𝐺=(𝑉,𝐸)with absorbing
node𝑠and transition matrix 𝑷, let𝜏denote the diameter of 𝐺and
let𝑑maxdenote the maximum out-degree of nodes in 𝐺. If the error
parameter𝜖is determined, then the time complexity of Algorithm 2
is𝑂(𝑇), where
𝑇=𝜖−2𝜏2𝑑2𝜏
maxlog3𝑛·Tr (𝑰−𝑷−𝑠)−1+𝑙3log𝑛
2𝜖2𝜋2𝑠𝑛2.
Algorithm 2 returns ˆ𝐾as the approximation for Kemeny’s constant
𝐾of𝐺, which satisfiesˆ𝐾−𝐾≤2𝜖𝐾.
As shown in Theorem 4.9, the expected time for sampling an
incoming directed rooted spanning tree scales as Tr (𝑰−𝑷−𝑠)−1,
equivalent to the sum of diagonal entries in (𝑰−𝑷−𝑠)−1. In contrast,
the expected cost of a single naive absorbing walk simulation entails
summing all entries of the dense matrix (𝑰−𝑷−𝑠)−1. Therefore,
the efficiency gains of TreeMC are significant relative to naive
simulation of absorbing random walks.
3477Fast Computation of Kemeny’s Constant for Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 2: The running time (seconds, 𝑠) ofApproxKemeny (ApprKem), DynamicMC (DynMC), AblationMC (AblatMC),
ImprovedMC andTreeMC with various 𝜖on realistic networks.
Type Network Node EdgeRunning time (s)
ApprKem DynMC AblatMCImprovedMC TreeMC
𝜖=0.3𝜖=0.2𝜖=0.15 𝜖=0.3𝜖=0.2𝜖=0.15Undir
ectedSister
cities 10,320 17,988 0.714 1.246 0.883 0.252 0.586 1.104 0.011 0.027 0.036
PGP 10,680 24,316 0.767 1.230 0.792 0.222 0.508 0.927 0.008 0.016 0.027
CAIDA 26,475 53,381 1.274 1.507 1.408 0.268 0.610 1.107 0.013 0.025 0.042
Skitter 1,694,616 11,094,209 – 437.0 92.78 0.973 1.534 2.102 0.829 1.795 3.000
Orkut 3,072,441 117,184,899 – 446.3 223.2 1.854 2.873 3.930 3.022 5.849 10.03
soc-LiveJournal 5,189,808 48,687,945 – 2843 363.2 2.351 3.610 4.921 4.802 9.920 16.82Dir
ectedGnutella30
8,490 31,706 – 1.138 0.980 0.292 0.714 1.272 0.007 0.014 0.028
Wikilink-wa 22,528 135,661 – 1.388 1.097 0.238 0.509 1.005 0.015 0.028 0.052
Epinions 32,223 443,506 – 2.760 1.545 0.305 0.611 1.256 0.022 0.046 0.075
EU Inst 34,203 151,132 – 2.636 1.486 0.229 0.594 1.087 0.025 0.041 0.072
Wikilink-la 158,427 3,486,928 – 4.724 2.407 0.264 0.352 0.961 0.109 0.227 0.436
Higgs 360,210 14,102,583 – 14.33 8.100 0.479 0.669 0.894 0.315 0.650 1.076
Youtube 509,245 4,269,142 – 27.41 16.10 0.583 0.925 1.236 0.306 0.686 1.183
Pokec 1,304,537 29,183,655 – 128.4 62.22 0.795 1.229 1.635 0.839 1.836 3.153
Stack Overflow 1,642,130 32,759,694 – 152.5 79.31 0.870 1.379 1.875 0.882 1.936 3.350
Wikilink-fr 2,311,584 113,754,248 – 187.7 99.59 0.939 1.487 2.007 1.739 3.416 5.465
DBpedia 3,796,073 97,783,747 – 344.9 164.7 1.299 2.053 2.809 2.925 5.744 9.781
Wikilink-en 8,026,662 416,705,115 – – 445.4 2.778 3.952 5.360 9.386 18.99 33.23
Meanwhile, Theorem 4.9 also emphasizes the importance of
selecting absorbing node 𝑠inTreeMC. First, the expected running
time of TreeMC depends on the mean hitting time from nodes in
𝐺to𝑠. Enhanced reachability of 𝑠leads to improved efficiency of
TreeMC. Additionally, the accuracy of estimating 𝐻𝑠is related to
the scaling coefficient 𝜋−1𝑠. If𝜋−1𝑠is smaller, then the theoretical
accuracy of TreeMC can be reduced. Therefore, we choose 𝑠as the
node with maximum 𝜋𝑠, which is both reasonable and efficient for
implementation.
5 Numerical Experiments
5.1 Experimental Settings
Datasets. The data of real-world digraphs utilized in our experi-
ments are sourced from the Koblenz Network Collection [ 20] and
SNAP [ 25]. To facilitate fair comparison against previous works, we
also conduct experiments on several undirected networks consid-
ered in [ 27,37]. For those networks that are not originally strongly
connected, we perform our experiments on their largest strongly
connected components (LSCCs). Relevant information about the
LSCC of studied real-world networks is shown in Table 2, where
networks are listed in ascending order by the number of nodes. The
smallest network has 8490 nodes, while the largest one consists of
more than 8 million nodes.
Environment. All experiments are conducted on a Linux server
with 32-core 2.5GHz CPU and 256GB of RAM. We implement all
the algorithms in Julia for a fair comparison. For ApproxKemeny,
we leverage the Laplacian Solver from [ 22]. Since all the algorithms
are pleasingly parallelizable, we force the program to run on 32
threads in every experiment.
Baselines and Parameters. To showcase the superiority of
our proposed algorithms, we implement several existing methods
for comparison. First, we implement the dynamic version of thestate-of-the-art method DynamicMC presented in [ 27] as a base-
line. Moreover, we implement the algorithm ApproxKemeny in [37],
which is on the basis of the Laplacian solver [ 22]. Meanwhile, as
our proposed algorithm ImprovedMC incorporates two optimiza-
tion techniques, we need to ensure that both of these techniques
meet our expectations. For this purpose, we implement an ablation
method called AblationMC, which solely utilizes the adaptive
sampling technique while excluding the subset sampling technique.
ForDynamicMC, we set the threshold parameter 𝜖𝑑=0.0005𝑛,
which is the same as [ 27]. For ApproxKemeny, the error parameter
𝜖is set to be 0.2, which is shown in [ 37] to achieve good perfor-
mance. For AblationMC, the error parameter 𝜖is also set to be 0.2.
It is worth noting that ImprovedMC andTreeMC are dependent
on the second largest modulus of eigenvalues 𝜆of𝑷, and TreeMC
additionally depends on the stationary distribution 𝝅, the left eigen-
vector associated with eigenvalue 1 for 𝑷. Therefore, we use the
Implicitly Restarted Arnoldi Method [ 24] to calculate 𝜆and𝝅in
advance for each tested network. Our results demonstrate that even
for the largest real-world dataset tested, the pre-computation time
is much less than one minute, which is negligible compared with
the time for calculating Kemeny’s constant. Therefore, we do not
take the pre-computation time into account in our experiments.
5.2 Results on Real-world Networks
5.2.1 Efficiency. We first evaluate the efficiency of our proposed
algorithms on real-world networks. The execution time of our
proposed algorithms and baselines is reported in Table 2. Specif-
ically, we present the results of ImprovedMC andTreeMC for
𝜖∈{0.3,0.2,0.15}. Note that due to the limit of memory space,
ApproxKemeny is infeasible for the large undirected graphs, while
DynamicMC is infeasible for the largest digraph due to the running
time being more than one hour.
3478KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
Table 3: The mean relative error ( ×10−3) of ApproxKe-
meny (ApprKem), DynamicMC (DynMC), AblationMC
(AblatMC), ImprovedMC andTreeMC.
NetworkMean relative error ( ×10−3)
ApprKem DynMC AblatMCImprovedMC TreeMC
0.3 0 .2 0 .15 0 .3 0 .2 0 .15
Sister cities 0.276 2.550 1.479 2.768 1.504 0.812 0.839 0.265 0.225
PGP 0.904 3.175 2.387 2.268 0.781 0.454 0.523 0.183 0.170
CAIDA 0.103 1.996 0.367 3.231 0.912 0.772 0.157 0.075 0.054
Gnutella30 – 0.685 0.266 0.506 0.252 0.155 0.453 0.179 0.081
Wikilink-wa – 0.427 0.094 1.703 0.167 0.112 0.239 0.049 0.027
Epinions – 4.607 1.622 2.652 1.926 0.829 0.508 0.177 0.140
EU Inst – 9.261 0.295 2.364 1.739 0.632 0.279 0.215 0.165
Table 2 indicates that for every real-world network, the run-
ning time of ImprovedMC andTreeMC with𝜖=0.2is smaller
than that of baselines. Recall that the theoretical running time of
ImprovedMC andTreeMC is proportional to 𝜖−3and𝜖−2, respec-
tively. As shown in Table 2, the larger constant factor 𝜖−3leads
to the phenomenon that ImprovedMC is slower than TreeMC on
relatively small networks. However, for large-scale networks like
Wikilink-en, the sublinear time complexity of ImprovedMC be-
comes dominant, leading to evident speedup compared to TreeMC.
Additionally, we observe that the running time of DynamicMC in
our experiments is longer than that reported in [ 27]. As mentioned
in Section 2.4, the high efficiency of DynamicMC in [27] is largely
attributed to GPU-based implementation. In our experiments, we
implement all baselines and our proposed approaches by using 32
CPU threads, which also ensures a fair comparison.
The results in Table 2 also reveal that the ablation method Abla-
tionMC outperforms DynamicMC in almost every tested network.
This advantage indicates that the optimization by adaptive sampling
technique is effective. Meanwhile, the speed-up of ImprovedMC
compared to AblationMC is also remarkable, which validates the
high efficiency of the subset sampling technique.
5.2.2 Accuracy. We next evaluate the accuracy of our proposed
algorithms. According to (2), we can compute the exact value of
Kemeny’s constant for small real-world networks. The mean rela-
tive error of approximation algorithms compared with exact values
is reported in Table 3.
Table 3 indicates that for all the evaluated algorithms that exhibit
a theoretical error guarantee, their estimated relative error is signif-
icantly lower than guaranteed value, including ImprovedMC and
TreeMC. For the case of ImprovedMC with𝜖=0.2, its maximum
approximation error is less than 0.2%, which is considerably small.
Furthermore, it is evident that TreeMC with𝜖=0.15consistently
provides the most precise answer, which can be largely attributed
to the alternative formula derived in Theorem 3.5.
Finally, the results in Table 3 also indicate that the mean rela-
tive error of ablation method AblationMC is always lower than
that of DynamicMC. This discrepancy in empirical accuracy arises
from the different selections of algorithm parameters. For Dynam-
icMC, the simulation amount 𝑟is fixed and the truncated length 𝑙
is dynamically determined. The error introduced by dynamically
determining 𝑙is biased, and this bias cannot be compensated by afixed large value of 𝑟. In contrast, AblationMC fixes𝑙and dynam-
ically determines 𝑟based on Lemma 4.2, whose incurred error is
unbiased. Recall that AblationMC solely incorporates the adaptive
sampling technique, we can confirm that this technique makes sig-
nificant improvement of efficiency without sacrificing theoretical
accuracy. In summary, our proposed algorithms exhibit comparable
accuracy with remarkable speed-up, and the algorithm TreeMC is
even more accurate than other competitors.
5.3 Influence of Varying Error Parameter
In evaluating the efficiency and accuracy of our algorithms, we ob-
serve that the error parameter 𝜖markedly impacts the performance.
We now examine in detail how 𝜖affects the efficiency and accuracy.
Specifically, we range 𝜖from 0.15 to 0.4 and provide the running
time and mean relative error of each algorithm on several real-
world networks. Notably, for DynamicMC the parameter 𝜖𝑑relates
to threshold rather than error guarantee. We thus fix 𝜖𝑑=0.0005𝑛
and present the performance of DynamicMC as a baseline.
5.3.1 Effect on efficiency. We first assess the impact of varying
error parameter on the efficiency of different algorithms. The results
on real-world networks are presented in Figure 1.
0.40 0.35 0. 30 0.25 0.20 0 .15101.0101.5102.0102.5103.0 (a)
0.40 0.35 0. 30 0.25 0.20 0 .15102.5103.0103.5104.0 (b)
0.40 0.35 0. 30 0.25 0.20 0 .15102.5103.0103.5104.0104.5
(c)0.40 0 .35 0 .30 0 .25 0.20 0. 15103.0103.5104.0104.5105.0(d)
0.40 0 .35 0 .30 0 .25 0.20 0. 15103104105(e)
0.40 0 .35 0 .30 0 .25 0.20 0. 15103.0103.5104.0104.5105.0 (f)Running time (milliseconds)
Error parameter 𝜖Dyna micMC ApproxKemen y AblationMC Im proved MC TreeMC
Figure 1: Running time of different approximate algorithms
with varying error parameter 𝜖on real-world networks:
CAIDA (a), Higgs (b), Youtube (c), Pokec (d), Skitter (e) and
Wikilink-fr (f).
As shown in Figure 1, the running time of ApproxKemeny and
TreeMC demonstrates comparable growth trends, which is more
apparent than competitors. This aligned scaling corroborates the
asymptotic complexities proportional to 𝜖−2. For AblationMC and
ImprovedMC, their theoretical complexity is proportional to 𝜖−2
and𝜖−3, respectively. However, such growth patterns are less evi-
dent on large networks like Pokec and Skitter. This situation should
be attributed to the leverage of adaptive sampling technique, which
3479Fast Computation of Kemeny’s Constant for Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
may terminate the unnecessary simulation in advance. This result
further validates the effectiveness of adaptive sampling technique.
0.150.200.250.300.350.4010−3.510−3.010−2.510−2.0
(a)
0.150.200.250.300.350.4010−3.510−3.010−2.510−2.0
(b)
0.150.200.250.300.350.4010−4.010−3.510−3.010−2.5(c)0.150.200.250.30 0.350.4010−4.510−4.010−3.510−3.010−2.5(d)
0.150.200.250.30 0.350.4010−3.510−3.010−2.5(e)
0.150.200.250.30 0.350.4010−3.510−3.010−2.510−2.0
(f)Mean relative  error 𝜌
Error parameter 𝜖Dynami cMC Approx Kemeny AblationMC ImprovedMC TreeMC
Figure 2: Mean relative error of different approximate algo-
rithms with varying error parameter 𝜖on real-world net-
works: Sister cities (a), PGP (b), CAIDA (c), Wikilink-wa (d),
Epinions (e) and EU Inst (f).
5.3.2 Effect on accuracy. We next analyze the impact of varying
𝜖on the accuracy across algorithms. The results are presented in
Figure 2. As shown in Figure 2, TreeMC consistently yields the
highest or the second highest accuracy in estimating Kemeny’s con-
stant. Although the error of ImprovedMC with large𝜖is not ideal,
reducing𝜖to 0.2 or 0.15 significantly decreases its error to levels
comparable with other methods. Notably, the estimation errors of
our proposed algorithms are sensitive as 𝜖changes, confirming that
their accuracy is effectively governed by the error parameter.
6 Related Work
Other Laplacian solver-based methods. In addition to existing
methods mentioned in Section 2.4, Zhang et al. [39] introduced
another approximation algorithm ApproxHK, which also leverages
the nearly linear-time Laplacian solver [ 22]. Unlike ApproxKe-
meny [37] that uses Hutchinson’s method to avoid direct com-
putation of the pseudoinverse, ApproxHK leverages the Johnson-
Lindenstrauss (JL) lemma. However, the total memory requirements
of JL lemma and Laplacian solver limit ApproxHK’s scalability com-
pared to ApproxKemeny on large graphs. In contrast, our proposed
algorithms mainly use memory to store the network, enabling im-
proved scalability. Meanwhile, the usage of Laplacian solver also
restricts ApproxHK to undirected graphs, while our algorithms
support both directed and undirected graphs.
Other spanning tree-based methods. To approximate ma-
trix inverses, researchers have proposed other spanning tree-based
approaches. Angriman et al. [4] introduced an algorithm for esti-
mating the diagonal elements of the Laplacian pseudoinverse bysampling spanning trees. Specifically, they infused current flows
into the sampled spanning trees and used the average value of
current flows to estimate resistance distance for undirected graphs,
which is a key procedure of their algorithm. However, the defini-
tion of resistance distance is limited to undirected graphs, and the
theoretical foundation of current flows cannot be directly applied
to digraphs. In contrast, our proposed algorithm TreeMC samples
rooted spanning trees, which is a distinct approach from that of [ 4].
Discussion of directed Laplacian solver. As discussed in Sec-
tion 2.4, ApproxKemeny [37] cannot be extended to digraphs due to
restrictions of the nearly linear-time Laplacian solver [ 22]. Though
prior works have proposed nearly linear-time algorithms for solv-
ing directed Laplacian systems with errror guarantee [ 12,21], their
theoretical efficiency has not been translated to practical imple-
mentation. Therefore, it remains infeasible to directly apply these
directed Laplacian solvers for efficient estimation of Kemeny’s con-
stant on digraphs.
Computation of personalized PageRank. Apart from the
Kemeny constant, other random walk-based quantities have also
garnered substantial research attention, such as personalized PageR-
ank (PPR) [ 11,15,16,33,36]. Since the PPR vector satisfies a recur-
sive relationship, several recent studies [ 11,33] utilized a variety
of push-based deterministic approaches for its computation, while
others [ 16,36] combined push-based approaches with Monte Carlo
methods. Given that the PPR stems from the 𝛼-decaying random
walk, where the walker may stop at each visited node with proba-
bility𝛼, the expected running time of push-based methods scales
in proportion to 𝛼−1. In contrast, simple random walks related to
Kemeny’s constant can be extremely long. Thus, directly adapting
existing push-based algorithms is impractical for efficient Kemeny’s
constant approximation.
7 Conclusion
We presented two different Monte Carlo algorithms ImprovedMC
andTreeMC for approximating Kemeny’s constant effectively and
efficiently. ImprovedMC is based on the simulation of truncated
random walks, which utilizes an adaptive sampling technique as
well as a technique that allows the simulation from a subset of
nodes. Due to these optimization techniques, ImprovedMC exhibits
sublinear time complexity while retaining theoretical accuracy.
In order to further improve the accuracy of estimating Kemeny’s
constant, we proposed TreeMC with the help of an alternative
formula in terms of the inverse of a submatrix associated with
the transition matrix. Extensive numerical results indicate that
ImprovedMC is extremely faster than the state-of-the-art method
with comparable accuracy, and that TreeMC outperforms the state-
of-the-art method in terms of both efficiency and accuracy, but is
slightly slower than ImprovedMC.
Acknowledgements
This work was supported by the National Natural Science Founda-
tion of China (Nos. 62372112, U20B2051, and 61872093).
References
[1]Ahmad Ali Abin. 2018. A random walk approach to query informative constraints
for clustering. IEEE Transactions on Cybernetics 48, 8 (2018), 2272–2283.
3480KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
[2]David Aldous and Jim Fill. 2002. Reversible Markov Chains and Random Walks
on Graphs. (2002).
[3]Diego Altafini, Dario A Bini, Valerio Cutini, Beatrice Meini, and Federico Poloni.
2023. An edge centrality measure based on the Kemeny constant. SIAM J. Matrix
Anal. Appl. 44, 2 (2023), 648–669.
[4]Eugenio Angriman, Maria Predari, Alexander van der Grinten, and Henning
Meyerhenke. 2020. Approximation of the diagonal of a Laplacian’s Pseudoinverse
for Complex Network Analysis. In Proceedings of the 28th Annual European
Symposium on Algorithms, Vol. 173. 6:1–6:24.
[5]Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. 2007. Tuning bandit
algorithms in stochastic environments. In Proceedings of the 2007 International
Conference on Algorithmic Learning Theory. 150–165.
[6]Joost Berkhout and Bernd F Heidergott. 2019. Analysis of Markov influence
graphs. Operations Research 67, 3 (2019), 892–904.
[7]Andrew Beveridge. 2009. Centers for random walks on trees. SIAM Journal on
Discrete Mathematics 23, 1 (2009), 300–318.
[8]Andrew Beveridge. 2016. A hitting time formula for the discrete Green’s function.
Combinatorics, Probability and Computing 25, 3 (2016), 362–379.
[9] Norman Biggs. 1993. Algebraic Graph Theory. Cambridge University Press.
[10] Mo Chen, Jianzhuang Liu, and Xiaoou Tang. 2008. Clustering via random walk
hitting time on directed graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 8. 616–621.
[11] Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, and Steven Skiena. 2023.
Accelerating personalized PageRank vector computation. In Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 262–273.
[12] Michael B. Cohen, Jonathan Kelner, Rasmus Kyng, John Peebles, Richard Peng,
Anup B. Rao, and Aaron Sidford. 2018. Solving Directed Laplacian Systems in
Nearly-Linear Time through Sparse LU Factorizations. In Proceedings of the 2018
IEEE 59th Annual Symposium on Foundations of Computer Science. 898–909.
[13] S Condamin, O Bénichou, V Tejedor, R Voituriez, and J Klafter. 2007. First-passage
times in complex scale-invariant media. Nature 450, 7166 (2007), 77–80.
[14] A. El Gamal, J. Mammen, B. Prabhakar, and D. Shah. 2006. Optimal throughput-
delay scaling in wireless networks - part I: the fluid model. IEEE Transactions on
Information Theory 52, 6 (2006), 2568–2592.
[15] Guanhao Hou, Xingguang Chen, Sibo Wang, and Zhewei Wei. 2021. Massively
parallel algorithms for personalized PageRank. Proceedings of the VLDB Endow-
ment 14, 9 (2021), 1668–1680.
[16] Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, and Zhewei Wei. 2023.
Personalized PageRank on evolving graphs with an incremental index-update
scheme. Proceedings of the ACM on Management of Data 1, 1, Article 25 (2023),
26 pages.
[17] Jeffrey J. Hunter. 2006. Mixing times with applications to perturbed Markov
chains. Linear Algebra Appl. 417, 1 (2006), 108–123.
[18] M.F. Hutchinson. 1989. A stochastic estimator of the trace of the influence matrix
for Laplacian smoothing splines. Communications in Statistics - Simulation and
Computation 18, 3 (1989), 1059–1076.
[19] Ali Jadbabaie and Alex Olshevsky. 2019. Scaling laws for consensus protocols
subject to noise. IEEE Trans. Automat. Control 64, 4 (2019), 1389–1402.
[20] Jérôme Kunegis. 2013. KONECT: the Koblenz network collection. In Proceedings
of the 22nd International Conference on World Wide Web. 1343–1350.
[21] Rasmus Kyng, Simon Meierhans, and Maximilian Probst. 2022. Derandomizing
Directed Random Walks in Almost-Linear Time. In Proceedings of the 2022 IEEE
63rd Annual Symposium on Foundations of Computer Science. 407–418.
[22] Rasmus Kyng and Sushant Sachdeva. 2016. Approximate Gaussian elimination
for Laplacians - fast, sparse, and simple. In Proceedings of the 2016 IEEE 57th
Annual Symposium on Foundations of Computer Science. 573–582.
[23] Bertrand Lebichot and Marco Saerens. 2018. A bag-of-paths node criticality
measure. Neurocomputing 275 (2018), 224–236.
[24] R. B. Lehoucq, D. C. Sorensen, and C. Yang. 1998. ARPACK Users’ Guide. Society
for Industrial and Applied Mathematics.
[25] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network
Dataset Collection. http://snap.stanford.edu/data.
[26] Hui-Jia Li, Lin Wang, Zhan Bu, Jie Cao, and Yong Shi. 2021. Measuring the net-
work vulnerability based on Markov criticality. ACM Transactions on Knowledge
Discovery from Data 16, 2 (2021), 28:1–28:24.
[27] Shiju Li, Xin Huang, and Chul-Ho Lee. 2021. An efficient and scalable algorithm
for estimating Kemeny’s constant of a Markov chain on large graphs. In Proceed-
ings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining .
964–974.
[28] Seng Pei Liew, Tsubasa Takahashi, Shun Takagi, Fumiyuki Kato, Yang Cao, and
Masatoshi Yoshikawa. 2022. Network shuffling: Privacy amplification via random
walks. In Proceedings of the 2022 International Conference on Management of Data.
773–787.
[29] László Lovász. 1993. Random walks on graphs: A survey. Combinatorics, Paul
Erdös is eighty 2, 1 (1993), 1–46.
[30] Carl D. Meyer, Jr. 1975. The role of the group generalized inverse in the theory
of finite Markov chains. SIAM Rev. 17, 3 (1975), 443–464.[31] Bibek Paudel and Abraham Bernstein. 2021. Random walks with erasure: diver-
sifying personalized recommendations on social and information networks. In
Proceedings of the Web Conference. 2046–2057.
[32] V Tejedor, O Bénichou, and R Voituriez. 2009. Global mean first-passage times of
random walks on complex networks. Physical Review E 80, 6 (2009), 065104.
[33] Hanzhi Wang, Zhewei Wei, Junhao Gan, Ye Yuan, Xiaoyong Du, and Ji-Rong
Wen. 2022. Edge-based local push for personalized PageRank. Proceedings of the
VLDB Endowment 15, 7 (2022), 1376–1389.
[34] Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative impor-
tance in networks. In Proceedings of the 9th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. 266–275.
[35] David Bruce Wilson. 1996. Generating random spanning trees more quickly than
the cover time. In Proceedings of the 28th Annual ACM Symposium on Theory of
Computing. 296–303.
[36] Hao Wu, Junhao Gan, Zhewei Wei, and Rui Zhang. 2021. Unifying the global and
local approaches: An efficient power iteration with forward push. In Proceedings
of the 2021 International Conference on Management of Data. 1996–2008.
[37] Wanyue Xu, Yibin Sheng, Zuobai Zhang, Haibin Kan, and Zhongzhi Zhang. 2020.
Power-law graphs have minimal scaling of Kemeny constant for random walks.
InProceedings of The Web Conference. 46–56.
[38] Yiming Zhang and Keith W. Ross. 2021. On-policy deep reinforcement learning for
the average-reward criterion. In Proceedings of the 38th International Conference
on Machine Learning, Vol. 139. 12535–12545.
[39] Zuobai Zhang, Wanyue Xu, and Zhongzhi Zhang. 2020. Nearly linear time
algorithm for mean hitting times of random walks on a graph. In Proceedings of
the 13th International Conference on Web Search and Data Mining. 726–734.
[40] Zhongzhi Zhang, Yihang Yang, and Yuan Lin. 2012. Random walks in modular
scale-free networks with multiple traps. Physical Review E 85, 1 (2012), 011106.
A Proofs of Lemmas and Theorems
A.1 Proof of Lemma 3.4
Proof. Since𝜏represents the longest distance between all pairs
of nodes, the inequality 𝑷𝜏
𝑖,𝑗≥𝑑−𝜏maxholds for any nodes 𝑖,𝑗∈𝑉.
Plugging this into Lemma 3.3, we obtain
𝑭𝑖,𝑖−𝑭(𝑙)
𝑖,𝑖=∞∑︁
𝑘=𝑙+1𝑷𝑘
𝑖,𝑖−𝜋𝑖≤∞∑︁
𝑘=𝑙+1𝑷𝑘
𝑖,𝑖−𝜋𝑖
≤∞∑︁
𝑘=𝑙+1 1−𝑛𝑑−𝜏
max⌊𝑘/𝜏⌋
≤𝜏 1−𝑛𝑑−𝜏max⌊(𝑙+1)/𝜏⌋
𝑛𝑑−𝜏max≤𝜖,
where the last inequality is due to 𝑙≥𝜏log(𝑛𝜖𝜏−1𝑑−𝜏
max)
log(1−𝑛𝑑−𝜏max)+𝜏−1.□
A.2 Proof of Lemma 4.3
Proof. For a strongly connected 𝑛-node digraph 𝐺=(𝑉,𝐸), we
denote the set of 𝑘-combinations of 𝑉as𝑉𝑘. Since the node subset
Xis sampled uniformly at random, we can represent the expected
value of ˜𝑆as
E˜𝑆
=∑︁
X∈𝑉𝑘1 𝑛
𝑘𝑛
𝑘∑︁
𝑖∈X¯𝑡(𝑙)
𝑖
=1 𝑛
𝑘𝑛
𝑘∑︁
𝑢∈𝑉𝑛−1
𝑘−1
¯𝑡(𝑙)
𝑢=∑︁
𝑢∈𝑉¯𝑡(𝑙)
𝑢=𝑆,
which completes our proof. □
A.3 Proof of Lemma 4.4
Proof. For𝑖=1,2,...,𝑛 , let𝑦𝑖be Bernoulli random variable
such that Pr(𝑦𝑖=1)=𝑝and Pr(𝑦𝑖=0)=1−𝑝. Here𝑦𝑖=1
indicates that 𝑥𝑖is selected and 𝑦𝑖=0otherwise. Let 𝑧𝑖=𝑥𝑖𝑦𝑖be𝑛
independent random variables with 0≤𝑧𝑖≤𝑏. Denote the sum of
random variables 𝑦𝑖as𝑦, and denote the sum of random variables
3481Fast Computation of Kemeny’s Constant for Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
𝑧𝑖as𝑧. Namely,𝑦=Í𝑛
𝑖=1𝑦𝑖and𝑧=Í𝑛
𝑖=1𝑧𝑖. It is clear that 𝑦and𝑧
represent the number of selected numbers and their sum. Therefore
the expectations of 𝑦and𝑧can be expressed as E[𝑦]=𝑛𝑝and
E[𝑧]=𝑝𝑥. According to Lemma 4.1, we have
Pr(|˜𝑥−𝑥|≥𝑛𝜖)=Pr|𝑧−𝑝𝑥|
𝑝≥𝑛𝜖
≤2 exp
−2𝑝2𝑛2𝜖2
𝑛𝑏2
≤1
𝑛,
finishing the proof. □
A.4 Proof of Theorem 4.5
Proof. Deriving the time complexity of ImprovedMC is straight-
forward. Therefore, our main focus is to provide the relative error
guarantee for this algorithm.
Referring back to (4),𝐾is initally approximated by the truncated
sum𝐾(𝑙). The approximation error of 𝐾(𝑙)dependent on truncated
length𝑙is analyzed in Lemma 3.2. Furthermore, the estimator ˆ𝐾(𝑙)=
𝑛−𝑙−1+Í𝑛
𝑖=1¯𝑡(𝑙)
𝑖is leveraged to approximate 𝐾(𝑙). We next
provide the connection between 𝑟and the error of ˆ𝐾(𝑙). According
to Lemma 4.1, we have
Pr¯𝑡(𝑙)
𝑖−E¯𝑡(𝑙)
𝑖≥𝜖
3
=Pr𝑟∑︁
𝑗=1ˆ𝑡(𝑙)
𝑖,𝑗−E𝑟∑︁
𝑗=1ˆ𝑡(𝑙)
𝑖,𝑗≥𝑟𝜖
3
≤2 exp
−2𝑟2𝜖2
9𝑟(𝑙/2)2
≤1
2𝑛2.
Based on the union bound, it holds that
ˆ𝐾(𝑙)−𝐾(𝑙)≤𝑛∑︁
𝑖=1¯𝑡(𝑙)
𝑖−𝑙∑︁
𝑘=1𝑷𝑘
[𝑖,𝑖]≤𝑛𝜖
3(6)
with probability

1−1
𝑛2𝑛
≥1−𝑛·1
2𝑛2=1−1
2𝑛.
As stated in Algorithm 1, applying Lemma 4.2 does not introduce
additional error since the error of ˆ𝐾(𝑙)is lower than 𝑛𝜖/3. Therefore,
we utilize Lemma 4.4 to give a theoretical bound for the approxi-
mation error of the partial sum ˜𝐾(𝑙)as
Pr ˜𝐾(𝑙)−ˆ𝐾(𝑙)≥𝑛𝜖
3≤2 exp
−2|X|𝜖2
𝑛(𝑙/2)2
≤1
2𝑛. (7)
Plugging (6)and(7)into Lemma 3.2, we derive the additive error
guarantee of ˜𝐾(𝑙):
Pr ˜𝐾(𝑙)−𝐾≤𝑛𝜖≥ 1−1
2𝑛2≥1−1
𝑛.
As shown by [ 17], the minimum Kemeny’s constant across all 𝑛-
node digraphs is (𝑛+1)/2. Leveraging this result, we can translate
the additive error bound for ˜𝐾(𝑙)into a relative error guarantee:
Pr ˜𝐾(𝑙)−𝐾≤2𝜖𝐾≥Pr ˜𝐾(𝑙)−𝐾≤𝑛𝜖≥1−1
𝑛,
which completes our proof. □
A.5 Proof of Lemma 4.6
Proof. In the initial round of the loop-erased random walk that
starts from node 𝑖, there is only one absorbing node 𝑠and the ex-
pected passage times over 𝑖is(𝑰−𝑷−𝑠)−1
𝑖,𝑖. Hence (5)holds true for
the initial starting node. For Wilson’s algorithm, the distributionof sampled random walk path is independent of the node order-
ing [35]. Therefore, every node in 𝑉\{𝑠}can be sampled in the first
round, which indicates that (5) holds true for all 𝑖∈𝑉\{𝑠}.□
A.6 Proof of Lemma 4.7
Proof. Since ˆ𝑡𝑖,𝑗denotes the passage times of node 𝑖for the
𝑗thloop-erased random walk, it is easy to verify that ˆ𝑡𝑖,𝑗≤𝑇𝑖𝑠/2,
where𝑇𝑖𝑠denotes the hitting time from 𝑖to𝑠for the𝑗thloop-erased
random walk. Therefore, we turn to bound 𝑇𝑖𝑠with high probability,
which requires us to provide an upper bound for its expected value
𝐻𝑖𝑠.
Recall that𝐻𝑖𝑠can be expressed as 𝒆⊤
𝑖(𝑰−𝑷−𝑠)−11, we have
max
𝑖∈𝑉\{𝑠}𝐻𝑖𝑠=(𝑰−𝑷−𝑠)−11∞=∞∑︁
𝑘=0𝑷𝑘
−𝑠∞
≤∞∑︁
𝑘=0 1−𝑑−𝜏
max⌊𝑘/𝜏⌋=𝜏
1−(1−𝑑−𝜏max)=𝜏𝑑𝜏
max.(8)
According to [ 2], we can finish our proof by providing the upper
bound for𝑇𝑖𝑠with high probability:
Pr ˆ𝑡𝑖,𝑗>𝑡=Pr(𝑇𝑖𝑠>2𝑡)≤exp
−2𝑡
e𝐻𝑖𝑠
=exp
−2𝑡
e𝜏𝑑𝜏max
≤1
4𝑛2,
where the last inequality is due to 𝑡≥e𝜏𝑑𝜏max
log 4𝑛2
/2. □
A.7 Proof of Lemma 4.8
Proof. Lemma 4.7 reveals that ˆ𝑡𝑖,𝑗exhibits an explicit upper
bound𝑡=e𝜏𝑑𝜏max
log 4𝑛2
/2with a high probability. Plugging
this into Lemma 4.1, we obtain
Pr
|¯𝑡𝑖−E[¯𝑡𝑖]|≥𝜖
2
=Pr𝑟∑︁
𝑗=1ˆ𝑡𝑖,𝑗−E𝑟∑︁
𝑗=1ˆ𝑡𝑖,𝑗≥𝑟𝜖
2
≤1−
1−2 exp
−2𝑟2𝜖2
4𝑟𝑡2 
1−1
4𝑛2
≤1−
1−1
4𝑛22
≤1−
1−2
4𝑛2
=1
2𝑛2,
where the second inequality follows from 𝑟≥4𝜖−2𝑡2log 2𝑛2.
Based on the union bound, it holds thatTr (𝑰−𝑷−𝑠)−1−∑︁
𝑖∈𝑉\{𝑠}¯𝑡𝑖
≤∑︁
𝑖∈𝑉\{𝑠}(𝑰−𝑷−𝑠)−1
𝑖,𝑖−¯𝑡𝑖≤𝑛𝜖
2
with probability

1−1
2𝑛2𝑛
≥1−𝑛·1
2𝑛2=1−1
2𝑛.
This finishes the proof. □
3482KDD ’24, August 25–29, 2024, Barcelona, Spain. Haisong Xia and Zhongzhi Zhang*
A.8 Proof of Theorem 4.9
Proof. The theoretical proof of the additive error guarantee for
TreeMC is straightforward by plugging Lemma 4.8 and Lemma 3.4
into the error analysis of truncated sum mentioned in Theorem 4.5,
and the additive error guarantee can be analogously converted to
the relative error guarantee by [ 17]. Therefore, our primary goal
is to provide the time complexity of this algorithm, which essen-
tially involves assessing the expected running time of sampling an
incoming directed spanning tree.
As shown in Algorithm 2, the time of sampling an incoming
directed spanning tree is determined by the total number of visits
to nodes that are not yet included in the sampled incoming directed
spanning tree. In the first iteration of the loop-erased random walk
starting from node 𝑖, the expected number of visits to 𝑖is equal to
(𝑰−𝑷−𝑠)−1
𝑖,𝑖. Recall that for Wilson’s algorithm, the distribution ofsampled path is independent of the node ordering [ 35]. In other
words, any node can be selected in the initial round while the
distribution of sampled path remains unchanged. Therefore, the
expected running time of sampling an incoming directed spanning
tree is equal to Tr (𝑰−𝑷−𝑠)−1.
According to Lemma 4.8, we can obtain the required sample size
𝑟. Combining Lemma 3.4 with the error analysis of truncated sum
in Theorem 4.5, the necessary amount 𝑟′and length𝑙of simulat-
ing truncated random walks can be determined. Finally we can
represent the time complexity 𝑂(𝑇)ofTreeMC as
𝑇=𝑟·Tr (𝑰−𝑷−𝑠)−1+𝑟′·𝑙
=𝜖−2𝜏2𝑑2𝜏
maxlog3𝑛·Tr (𝑰−𝑷−𝑠)−1+𝑙3log𝑛
2𝜖2𝜋2𝑠𝑛2,
finishing the proof. □
3483