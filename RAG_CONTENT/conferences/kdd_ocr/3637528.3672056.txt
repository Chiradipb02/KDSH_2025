Consistency and Discrepancy-Based Contrastive Tripartite Graph
Learning for Recommendations
Linxin Guo
guolinxin@cqu.edu.cn
Chongqing University
Chongqing, ChinaYaochen Zhu
uqp4qh@virginia.edu
University of Virginia
Charlottesville, USAMin Gao∗
gaomin@cqu.edu.cn
Chongqing University
Chongqing, China
Yinghui Tao
taoyinghui@cqu.edu.cn
Institute of Guizhou Aerospace
Measuring and Testing Technology
Guiyang, ChinaJunliang Yu
jl.yu@uq.edu.au
the University of Queensland
Queensland, AustraliaChen Chen
chenannie45@gmail.com
University of Virginia
Charlottesville, USA
ABSTRACT
Tripartite graph-based recommender systems markedly diverge
from traditional models by recommending unique combinations
such as user groups and item bundles. Despite their effectiveness,
these systems exacerbate the long-standing cold-start problem in
traditional recommender systems, because any number of user
groups or item bundles can be formed among users or items. To
address this issue, we introduce a Consistency and Discrepancy-
based graph contrastive learning method for tripartite graph-based
Recommendation (CDR). This approach leverages two novel meta-
path-based metrics—consistency and discrepancy—to capture nu-
anced, implicit associations between the recommended objects and
the recommendees. These metrics, indicative of high-order simi-
larities, can be efficiently calculated with infinite graph convolu-
tional networks (GCN) layers under a multi-objective optimiza-
tion framework, using the limit theory of GCN. Additionally, we
introduce a novel Contrastive Divergence (CD) loss, which can
seamlessly integrate the consistency and discrepancy metrics into
the contrastive objective as the positive and contrastive supervi-
sion signals to learn node representations, enhancing the pairwise
ranking of recommended objects and proving particularly valuable
in severe cold-start scenarios. Extensive experiments demonstrate
the effectiveness of the proposed CDR. The code is released at
https://github.com/foodfaust/CDR.
CCS CONCEPTS
•Information systems →Recommender systems.
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672056KEYWORDS
Tripartite Graph-Based Recommendations, Extreme Cold-Start, Con-
sistency and Discrepancy
ACM Reference Format:
Linxin Guo, Yaochen Zhu, Min Gao, Yinghui Tao, Junliang Yu, and Chen
Chen. 2024. Consistency and Discrepancy-Based Contrastive Tripartite
Graph Learning for Recommendations. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3672056
1 INTRODUCTION
With the rapid development of the Internet, recommender sys-
tems have become crucial in online platforms [ 7,8,29]. Tradi-
tional systems, modeling the interactions between user and item,
have achieved significant results in both industry and academia
[17,34,42,52]. Recently, recommender system research has em-
braced a trend of formulating users, items, and their interactions
as a bipartite graph, prompting the development of bipartite graph-
based methods like NGCF [ 4] and LightGCN [ 17], which have no-
tably advanced the field. However, tripartite graph-based recom-
mendation, involving interactions among users, items, and user
groups/item bundles (e.g., group recommendations [ 13,35,48] and
bundle recommendations [ 3,5,26,30]), has recently emerged and
garnered substantial research interest due to its unique interaction
model among three heterogeneous entities/nodes [1, 2, 12, 32].
Tripartite graph-based recommendations differ substantially
from general recommender systems. For instance, group recom-
mendations suggest items to diverse-interest user groups, e.g., rec-
ommending a travel route to a group of tourists, and bundle recom-
mendations propose multiple items to a user simultaneously. Unlike
bipartite graphs that encompass interactions between two entities
(users and items), tripartite graphs engage three types of entities:
users, items, and groups/bundles, allowing arbitrary groupings of
users and items. Despite the relative abundance of user-item inter-
actions, interactions between recommended objects (user/group)
and recommendees (bundle/item) are markedly sparse [ 15,16,53]
(demonstrated in Fig. 1). The complexities and sparsity, particularly
noticeable in scenarios like group recommendations where groups
engage with fewer items, inhibit the efficacy of standard pair-wise
 
944
KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
ranking methods during positive and negative sampling. This exac-
erbates the difficulty of accurately modeling group nodes’ latent
semantics in a tripartite graph and, subsequently, making precise
recommendations.
(a) Bipartite graph-based 
recommendation. (b) Tripartite graph-based 
recommendation.Group Recommendation Bundle Recommendation
User
 Item
 Group
 Item
 User
 User
Item
 Bundle
General Recommendation
Figure 1: Comparison of bipartite and tripartite graph-based
recommendation.
Mitigation of data sparsity has been critically addressed through
various sophisticated mechanisms in group and bundle recommen-
dation models. Attention mechanisms manage data sparsity and
cold-start issues in group or bundle recommendations by dynam-
ically prioritizing interactions and aggregating item significance
[2,13,30]. Multi-task learning is also an effective way, which en-
hances robustness in user-group and user-item interactions by
jointly optimizing parallel tasks and effectively utilizing sparse
data through integrating various interaction types [ 5,21]. Addition-
ally, graph-based approaches navigate data sparsity by traversing
user-item, user-group, and user-bundle interactions within a hetero-
geneous graph, facilitating information propagation and ensuring
cohesive embedding of item, group, and bundle semantics [ 3]. Thus,
attention application, multi-task learning, and graph-based propa-
gation, skillfully interweave varied interactions and affiliations to
adeptly mitigate challenges in sparse data scenarios. However, these
methods face challenges in extreme cold-start scenarios due to their
reliance on a minimum amount of positive and negative samples
for pair-wise ranking training in the recommendation-related loss.
Details can be seen in Appendix A.
Addressing challenges in tripartite graph-based recommenda-
tions involves two core issues. First, the substantial sparsity in
interactions between recommended objects and recommendees ne-
cessitates novel metrics to bridge the semantic gap between lower
user-item interactions and user-group/item-bundle affiliations and
higher relationships between recommended objects and recom-
mendees. Second, the effective use of metrics as proxy supervision
signals becomes critical to guide the training of the recommenda-
tion model, ensuring refined embeddings for recommended objects
and recommendees with rare interactions, which facilitates accurate
future recommendations.
To address these challenges, we propose a Consistency and
Discrepancy-based contrastive learning Recommendation method,
i.e., CDR, for tripartite graph-based recommendation. We initiate
with two new metrics, consistency and discrepancy, between the
recommended object and recommendee nodes in the tripartite
graph, defining a hard multi-objective optimization problem to
establish high-order relationships based on abundant user-item in-
teractions and affiliations. We define reachable and non-reachable
meta-path between recommended objects and recommendees, me-
diated/blocked by other nodes in the tripartite graph. We efficientlypre-calculate two coefficients related to the solution, encouraging
similar (consistency) or dissimilar (discrepancy) embeddings of
recommended objects and recommendees, utilizing graph convolu-
tional networks (GCN) limit theory.
Subsequently, a contrastive inspired CD loss, integrating the
designed consistency and discrepancy metrics as positive and con-
trastive supervision signals, is introduced to train the model without
requiring direct interactions between objects and recommendees.
Furthermore, when direct interactions become available, they can
fine-tune consistency and discrepancy metrics, enhancing recom-
mendation accuracy. The contributions of this paper can be con-
cretely summarized into four folds as follows:
•We propose a new contrastive graph learning-inspired tripartite
graph-based recommender system, i.e., CDR, based on new
supervision signals and a new optimization procedure. CDR
is shown to be robust to the extreme cold-start case where no
direct interaction exists between the recommended objects and
the recommendees.
•Two novel metrics, i.e., consistency and discrepancy, are pro-
posed by solving a multi-objective optimization problem based
on reachable/non-reachable meta-paths between recommended
objects and recommendees, with available user-item interac-
tions and user-group/item-bundle affiliations. The metrics can
be efficiently pre-calculated before optimization with GCN limit
theory.
•Furthermore, a contrastive learning-inspired novel CD loss is
proposed accordingly, where the introduced consistency and
discrepancy metrics can be seamlessly integrated into the con-
trastive objective as the positive and contrastive supervision
signals to learn node representations, without the requirement
of available direct user-bundle/group-item interactions.
•Extensive experiments on real-world datasets demonstrate the
superiority of the proposed CDR model, where the advantages
of the proposed consistency and discrepancy metrics and the
contrastive learning-based CD loss are clearly demonstrated by
the ablation study.
2 PRELIMINARIES
In this section, we introduce the symbols used in this paper, define
related concepts, and formally formulate the problem of tripartite
graph-based recommendation as the preliminary knowledge of the
proposed CDR model.
2.1 Relevant Concepts
Definition 1. Tripartite Graph [36]. Given three node sets T,
O, andM,V=T∪O∪M , a graph𝐺=(V,E) is called a tripartite
graph, if the edge set E⊆T×O∪O×M∪T×M , where×represents
Cartesian product. In tripartite graph-based recommendation, 𝐺is
a heterogeneous graph since T,O, andMrepresent three different
types of nodes, i.e., user, item, and user/item groups.
Definition 2. Tripartite Graph-Based Recommendation.
The tripartite graph-based recommendation contains three types of
entities, i.e., user, item, and user/item group (where item groups are
known as bundles in the literature). To simplify the discussion, we
use the term tuples to represent user groups and item bundles, use
the term members to represent users in group recommendation and
 
945Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 1: Summary of notations.
Notation Descriptions
T,O,M The
sets of tuple, object, and member nodes
|T|,|O|,|M| The
number of tuple, object, and member nodes
𝑡,𝑜,𝑚 A
tuple / object / member node
(𝑣1,𝑣2) A
node pair
𝒀,𝑿,𝒁The
matrices of tuple interactions, member
interactions, and tuple-member affiliations
𝑒𝑡,𝑒𝑜,𝑒𝑚 The
embedding of tuple, object, and member nodes
N(𝑡),N(𝑜),N(𝑚)The
set of neighbors of tuple node 𝑡, object node 𝑜,
and member node 𝑚
𝑑𝑒𝑔𝑣,𝛿𝑣1,𝑣2The
degree of node 𝑣and the composite term of the
degrees of nodes 𝑣1and𝑣2
𝑐𝑣1,𝑣2,𝑑𝑣1,𝑣2 The
consistency and discrepancy metrics
𝑪Φ,𝑫Φ The
consistency and discrepancy sub-matrices extracted
from meta-path Φ
𝑪𝑡𝑢𝑝𝑙𝑒 ,𝑫𝑡𝑢𝑝𝑙𝑒The
consistency and discrepancy matrices extracted
from tuple interactions
𝑪𝑚𝑒𝑚𝑏𝑒𝑟 ,𝑫𝑚𝑒𝑚𝑏𝑒𝑟The
consistency and discrepancy matrices extracted
from member interactions and tuple-member
affiliations
items in bundle recommendation, and use the term objects to represent
items in group recommendation and users in bundle recommendation.
For simplicity, we use tuple interactions to represent the interaction
between tuple and object, and member interactions to represent inter-
actions between member and object.
Tripartite graph-based recommendation unifies group and bun-
dle recommendations as the interaction prediction problem between
tuples and objects, where the proposed CDR can be viewed as a
universal solution.
Definition 3. Meta-Path [37]. A (reachable) meta-path Φde-
fined on the heterogeneous graph 𝐺=(V,E)is a path pattern in the
form of𝑉1−𝑉2−···−𝑉𝑙+1, where𝑉𝑘∈{T,O,M}. Specifically, Φ
describes the composite relationship 𝑅=𝑅𝑉1𝑉2◦𝑅𝑉2𝑉3◦···◦𝑅𝑉𝑙𝑉𝑙+1
between the head node 𝑉1and the tail node 𝑉𝑙+1, where◦denotes the
composition operator and 𝑅𝑉𝑘𝑉𝑘+1denotes the pairwise relationship
defined in the edge set E.
We extend the meta-path schema definition to include non-
reachable information between nodes, allowing explicit con-
sideration of negative pairwise relationships. Specifically, Tuple ∼
Member−Tuple represents non-reachable meta-path schemas, with
"−" signifying an edge and " ∼" no edge between nodes, as depicted
in Fig. 2.
Member
Tuple
 Tuple
(a) Reachable meta-path Tuple-
Member-Tuple.
Member
Tuple
 Tuple(b) Non-reachable meta-path
Tuple∼Member-Tuple.
Figure 2: Two kinds of meta-paths, as examples, for deriving
consistency and discrepancy metrics.2.2 Problem Formulation
Consider the sets of tuple nodes T=
𝑡1,...,𝑡|T|	
, object nodes
O=
𝑜1,...,𝑜|O|	
, and member nodes M=
𝑚1,...,𝑚|M|	
,
where|T|,|O|, and|M| represent the number of the tuple, object,
and member nodes, respectively. Three relations could exist among
T,O, andM, i.e., tuple interactions, member interactions, and
member-tuple affiliations. We use a sparse matrix 𝒀∈{0,1}|T|×|O|
to represent tuple interactions, where 𝑦𝑡𝑜=1indicates that the
tuple node𝑡has interacted with the object 𝑜, and𝑦𝑡𝑜=0indicates
otherwise. Since members can be arbitrarily grouped into tuples, 𝒀
can be extremely sparse, and in the extreme cold-start case, 𝒀can
be an empty matrix. Similarly, member interactions are represented
by the sparse matrix 𝑿∈{0,1}|M|×|O|, where𝑥𝑚𝑜=1indicates
that the member 𝑚has interacted with the object 𝑜, and𝑥𝑚𝑜= 0
indicates otherwise. 𝑿is usually much denser than 𝒀due to the
comparatively abundant member interactions. Finally, we denote
the observed affiliations as 𝒁∈{0,1}|T|×|M|, where the element
𝑧𝑡𝑚=1means the member 𝑚is affiliated to the tuple 𝑡, and𝑧𝑡𝑚=0
otherwise.
The aim of tripartite graph-based recommendation is to gen-
erate a top-K list of objects (bundles/items) for the recommendee
(users/user groups) using member interactions 𝑿, tuple interactions
𝒀, and tuple-member affiliations 𝒁. It also addresses the extreme
cold-start where no tuple interactions (i.e., 𝒀is a zero-matrix) exist.
Table 1 summarizes symbols.
3 METHODOLOGY
3.1 Tuple-Object Metric Learning
In the first stage of the proposed CDR algorithm, we compute
consistency and discrepancy metrics using defined reachable/non-
reachable meta-paths and abundant member interactions plus member-
tuple affiliations. These metrics gauge the similarity or difference
between pairs of recommended object and recommendee nodes,
crucial for the positive and negative relations needed by the con-
trastive learning-inspired CD loss, discussed subsequently.
3.1.1 Optimization Objective. The derivation of consistency and
discrepancy metrics begins with an auxiliary graph embedding
learning task, intending to map all nodes of the tuple-member-
object tripartite graph into an embedding space while maximally
preserving tuple-member affiliation and object-member interaction
information in the node embeddings. Specifically, utilizing the pos-
itive and negative pairwise relationships between tuples/members
and members/objects, we aim to optimize the following objectives:
max∑︁
𝑚∈N(𝑡)𝑒⊤
𝑡𝑒𝑚,min∑︁
˜𝑚∉N(𝑡)𝑒⊤
𝑡𝑒˜𝑚, (1)
max∑︁
𝑚∈N(𝑜)𝑒⊤
𝑜𝑒𝑚,min∑︁
˜𝑚∉N(𝑜)𝑒⊤
𝑜𝑒˜𝑚, (2)
where𝑒is the node embedding, N(𝑡)andN(𝑜)are the neighbor
sets of tuple 𝑡and object𝑜, and𝑚and ˜𝑚are the first-order member
neighbors and non-first-order neighbor nodes (members) of tuple
𝑡and object𝑜, respectively. To simplify the discussion, we only
consider nodes 𝑡,𝑚, ˜𝑚,𝑜 in the objective, but the generalization of
Eqs. (1) and (2) to include all nodes in the graph is straightforward.
 
946KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
Prior research primarily optimizes object and member embed-
dings (user-item interactions) as per Eq. (2), often using techniques
like attention mechanisms [ 39] to aggregate member embeddings
into tuple embeddings, neglecting Eq. (1). Such simple aggregation
from member to tuple level lacks theoretical and explanatory back-
ing. Contrarily, our method effectively optimizes all four objectives
in Eqs. (1) and (2) concurrently.
3.1.2 Message Passing Mechanism. Optimizing Eq. (2) involves
message passing among heterogeneous nodes. Utilizing the Light-
GCN strategy, celebrated for effectively mining bipartite graph
pair-wise relations [ 17], we employ it as our message-passing mech-
anism. The user embedding information aggregation process in the
LightGCN network, incorporating self-connection, is formalized as
follows:
𝑒(𝑙+1)
𝑚=1
𝑑𝑒𝑔𝑚+1𝑒(𝑙)
𝑚+∑︁
𝑡∈N(𝑚)1√︁
𝑑𝑒𝑔𝑚+1√︁
𝑑𝑒𝑔𝑡+1𝑒(𝑙)
𝑡
+∑︁
𝑜∈N(𝑚)1√︁
𝑑𝑒𝑔𝑚+1√︁
𝑑𝑒𝑔𝑜+1𝑒(𝑙)
𝑜,(3)
where𝑒(𝑙)represents the node embedding at layer 𝑙, and𝑑𝑒𝑔𝑣
represents the degree of node 𝑣, respectively.
3.1.3 Solution Based on the Limit Theory of GCN. The direct gen-
eralization of Eq. (3) to solve Eqs. (1) and (2) is computationally
expensive due to multiple aggregation objectives. Moreover, it is
difficult to decide how many aggregation processes are needed to
acquire a good embedding conducive to recommendation. Inspired
by UltraGCN[ 31], we note that a convergent form of 𝑒𝑚can be
obtained with infinite propagation layers. Specifically, according
to the aggregation formula of graph convolution, after passing
through infinite layers of aggregation, the embeddings of the model
will converge to the following form:
𝑒𝑚=lim
𝑙→∞𝑒(𝑙+1)
𝑚=lim
𝑙→∞𝑒(𝑙)
𝑚, (4)
where the node embeddings are consistent before and after the
convolution in the limit. Based on Eq. (4), the message passing
mechanism defined in Eq. (3) can be simplified as:
𝑒𝑚=∑︁
𝑡∈N(𝑚)1
𝑑𝑒𝑔𝑚√︄
𝑑𝑒𝑔𝑚+1
𝑑𝑒𝑔𝑡+1𝑒𝑡+∑︁
𝑜∈N(𝑚)1
𝑑𝑒𝑔𝑚√︄
𝑑𝑒𝑔𝑚+1
𝑑𝑒𝑔𝑜+1𝑒𝑜.
(5)
We denote the composite term of the degree of member 𝑚and tuple
𝑡(object𝑜) as𝛿𝑚,𝑡(𝛿𝑚,𝑜) defined as follows:
𝛿𝑚,𝑡=1
𝑑𝑒𝑔𝑚√︄
𝑑𝑒𝑔𝑚+1
𝑑𝑒𝑔𝑡+1, 𝛿𝑚,𝑜=1
𝑑𝑒𝑔𝑚√︄
𝑑𝑒𝑔𝑚+1
𝑑𝑒𝑔𝑜+1,(6)
Eq. (5) can be simplified into the following form:
𝑒𝑚=∑︁
𝑡∈N(𝑚)𝛿𝑚,𝑡𝑒𝑡+∑︁
𝑜∈N(𝑚)𝛿𝑚,𝑜𝑒𝑜. (7)
Essentially, Eq. (7) uses tuple embeddings and object embeddings
to represent member embeddings. To further utilize tuple-member
affiliations, we can substitute Eq. (7) into Eq. (1) and maximize the
two parts simultaneously to approximate the final result, where theobjective can be re-formulated as:
max∑︁
𝑚∈N(𝑡)𝑒⊤
𝑡𝑒𝑚≃max∑︁
𝑚∈N(𝑡)∑︁
𝑡′∈N(𝑚)𝛿𝑚,𝑡′𝑒⊤
𝑡𝑒𝑡′+
max∑︁
𝑚∈N(𝑡)∑︁
𝑜∈N(𝑚)𝛿𝑚,𝑜𝑒⊤
𝑡𝑒𝑜.(8)
Since the member embedding 𝑒′𝑚is eliminated from the righ-hand
side of Eq. (8), we can utilize the relationship of member nodes
without training their embeddings.
3.1.4 Solution with Reachable/Non-Reachable Meta-Paths. While
Eq. (8) is notably simplified, it still demands gradient-based opti-
mization. The optimization is further streamlined by introducing
reachable and non-reachable meta-paths from Section 2.1, allowing
the maximization portion of Eq. (8) to be redefined as follows:
L𝑡𝑚_𝑚𝑎𝑥=∑︁
(𝑡,𝑚,𝑡′)∈𝑄𝑡𝑚𝑡′𝛿𝑚,𝑡′ 1−𝜎 𝑒⊤
𝑡𝑒𝑡′
+∑︁
(𝑡,𝑚,𝑜)∈𝑄𝑡𝑚𝑜𝛿𝑚,𝑜 1−𝜎 𝑒⊤
𝑡𝑒𝑜,(9)
where𝜎(·)is the sigmoid function, 𝑄𝑡𝑚𝑡′is the set of reachable
Tuple-Member-Tuple meta-paths ( 𝑡,𝑚,𝑡′) composed of tuple node
𝑡and its first-order member neighbors 𝑚and second-order tuple
neighbors𝑡′. Similarly,𝑄𝑡𝑚𝑜is the set of reachable Tuple-Member-
Object meta-paths ( 𝑡,𝑚,𝑜 ):
𝑄𝑡𝑚𝑡′= 𝑡,𝑚,𝑡′|𝑚∈N(𝑡),𝑡′∈N(𝑚)	
,
𝑄𝑡𝑚𝑜={(𝑡,𝑚,𝑜)|𝑚∈N(𝑡),𝑜∈N(𝑚)}. (10)
The minimization part of Eq. (1) can be reformulated as:
L𝑡𝑚_𝑚𝑖𝑛=∑︁
(𝑡,˜𝑚,𝑡′)∈𝑄𝑡˜𝑚𝑡′𝛿˜𝑚,𝑡′𝜎 𝑒⊤
𝑡𝑒𝑡′+∑︁
(𝑡,˜𝑚,𝑜)∈𝑄𝑡˜𝑚𝑜𝛿˜𝑚,𝑜𝜎 𝑒⊤
𝑡𝑒𝑜.
(11)
The set𝑄𝑡˜𝑚𝑡′diverges from 𝑄𝑡𝑚𝑡′by representing non-reachable
Tuple∼Member-Tuple meta-paths. Here, triples (𝑡,˜𝑚,𝑡′)in𝑄𝑡˜𝑚𝑡′
denote member ˜𝑚is not adjacent to tuple 𝑡, but is to tuple 𝑡′. Addi-
tionally,𝑄𝑡˜𝑚𝑜defines non-reachable Tuple ∼Member-Object meta-
paths as follows:
𝑄𝑡˜𝑚𝑡′= 𝑡,˜𝑚,𝑡′|˜𝑚∉N(𝑡),𝑡′∈N(˜𝑚)	
,
𝑄𝑡˜𝑚𝑜={(𝑡,˜𝑚,𝑜)|˜𝑚∉N(𝑡),𝑜∈N(˜𝑚)}. (12)
3.1.5 Final Objective. Examining the loss functions L𝑡𝑚_𝑚𝑎𝑥 and
L𝑡𝑚_𝑚𝑖𝑛from Eq. (9) and Eq. (11), the initial terms engage with 𝑡
and𝑡′, while the secondary involve 𝑡and𝑜. Losses for pairs 𝑡,𝑡′and
𝑡,𝑜are structured accordingly. Focusing on pair ( 𝑡,𝑡′), the losses in
L𝑡𝑚_𝑚𝑎𝑥 andL𝑡𝑚_𝑚𝑖𝑛integrate as follows:
𝑙𝑡,𝑡′=∑︁
𝑚∈N(𝑡),𝑚∈N(𝑡′)𝛿𝑚,𝑡′ 1−𝜎 𝑒⊤
𝑡𝑒𝑡′
+∑︁
˜𝑚∉N(𝑡),˜𝑚∈N(𝑡′)𝛿˜𝑚,𝑡′𝜎 𝑒⊤
𝑡𝑒𝑡′.(13)
We omit the constant that is independent to 𝑒𝑡and𝑒𝑡′, and use𝑐𝑡,𝑡′
and𝑑𝑡,𝑡′to denote the constants that concern only 𝑒𝑡and𝑒𝑡′, where
𝑐𝑡,𝑡′=Í
𝑚∈N(𝑡),𝑚∈N(𝑡′)𝛿𝑚,𝑡′, 𝑑𝑡,𝑡′=Í
˜𝑚∉N(𝑡),˜𝑚∈N(𝑡′)𝛿˜𝑚,𝑡′,
the loss function of a node pair ( 𝑡,𝑡′) can be rewritten as follows:
𝑙𝑡,𝑡′= 𝑑𝑡,𝑡′−𝑐𝑡,𝑡′𝜎 𝑒⊤
𝑡𝑒𝑡′. (14)
 
947Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain.
All node pairs of arbitrary type can derive their consistencies 𝑐𝑣𝑖,𝑣𝑗
and discrepancies 𝑑𝑣𝑖,𝑣𝑗 by predefined reachable and non-reachable
meta-paths. For an arbitrary node pair ( 𝑡,𝑜), we construct meta-
path sets𝑄𝑡𝑚𝑜and𝑄𝑡˜𝑚𝑜subsequently deriving its loss function
with similar steps applied to other node pair types:
𝑙𝑡,𝑜=(𝑑𝑡,𝑜−𝑐𝑡,𝑜)𝜎(𝑒⊤
𝑡𝑒𝑜). (15)
Calculating 𝑐𝑡,𝑜and𝑑𝑡,𝑜parallels𝑐𝑡,𝑡′and𝑑𝑡,𝑡′. Given the asym-
metry in calculations like Eq. (14) and Eq. (15) regarding node pair
order, and possible pair types (tuple-tuple, tuple-object, object-tuple,
and object-object), coefficients such as 𝑐𝑡,𝑜and𝑑𝑡,𝑜are also com-
puted. The loss function for arbitrary type node pair ( 𝑣1,𝑣2) using
meta-paths with user nodes as midpoints is:
L=∑︁
𝑣1,𝑣2∈T∪O 𝑑𝑣1,𝑣2−𝑐𝑣1,𝑣2𝜎
𝑒⊤
𝑣1𝑒𝑣2
, (16)
where coefficients 𝑐𝑣1,𝑣2and𝑑𝑣1,𝑣2can be pre-calculated as follows:
𝑐𝑣1,𝑣2=∑︁
𝑚∈N(𝑣1),𝑚∈N(𝑣2)𝛿𝑚,𝑣 2,
𝑑𝑣1,𝑣2=∑︁
˜𝑚∉N(𝑣1),˜𝑚∈N(𝑣2)𝛿˜𝑚,𝑣 2.(17)
3.1.6 Consistency and Discrepancy Metrics. In general recommen-
dations, coefficients 𝑐𝑣1,𝑣2and𝑑𝑣1,𝑣2in Eq. (16) determine the em-
bedding distance between recommended object and recommendee
pairs, with 𝑐𝑣1,𝑣2and𝑑𝑣1,𝑣2respectively encouraging closer and
more distant embeddings, termed as consistency anddiscrep-
ancy. For𝑣1,𝑣2∈T∪O ,tuple-object relationships derive from
𝑐𝑣1,𝑣2and𝑑𝑣1,𝑣2without needing direct interactions, seamlessly
integrating with contrastive learning by providing positive and
contrastive supervision signals.
3.2 Contrastive Learning-Inspired CD Loss
Utilizing consistency and discrepancy from subsection 3.1.6, we
introduce the contrastive learning-inspired CD loss here, employing
them as positive and negative supervision signals to formulate the
contrastive objective.
3.2.1 Motivation of the CD Loss. Given Eq. (16), a direct model
training strategy might use a general recommendation algorithm
with𝑑𝑣1,𝑣2−𝑐𝑣1,𝑣2as supervision signals. However, in tripartite
graph-based recommendations, the subtraction ( 𝑑𝑣1,𝑣2−𝑐𝑣1,𝑣2) poten-
tially erases unique consistency and discrepancy information. For
example, cases (dv1,v2=1, cv1,v2=0.99) and (dv1,v2=0.01, cv1,v2=0)
both result in a subtraction value of 0.01, despite significant differ-
ences in absolute values. Unlike bipartite recommendations, where
subtractions 𝑑−𝑐are clearly 1 or -1, we propose a new contrastive
learning-based objective using consistency and discrepancy inde-
pendently as positive and contrastive supervision signals.
3.2.2 CD Loss. To further utilize the consistency and discrepancy,
inspired by contrastive learning[ 14], we design a CD loss for tri-
partite graph-based recommendation, leveraging consistency and
discrepancy for any node pair ( 𝑣1,𝑣2):
L=∑︁
(𝑣1,𝑣2)∈𝑄−log𝑐𝑣1,𝑣2exp cos e𝑣1,e𝑣2/𝜏
Í
˜𝑣2∈T∪O𝑑𝑣1,˜𝑣2exp cos e𝑣1,e˜𝑣2/𝜏,(18)where𝑄={(𝑣1,𝑣2) |𝑐𝑣1,𝑣2>0𝑎𝑛𝑑 𝑣 1,𝑣2∈T∪O} is the
set of node pairs whose consistency is positive, and cos(·,·)is the
cosine similarity measure. In Eq. (18), the consistency and discrep-
ancy metrics are independently used to select positive and negative
supervision signals. During the optimization process, the consis-
tency𝑐𝑣1,𝑣2encourages the embeddings of nodes 𝑣1and𝑣2to be
close, while the discrepancy 𝑑𝑣1,˜𝑣2constrains the embeddings to be
distant with each other.
The contrastive loss in Eq. (18) introduces a temperature pa-
rameter𝜏, adjustable based on dataset noise levels; a larger 𝜏suits
higher noise, focusing model updates on easily distinguishable
nodes, while a smaller 𝜏is used for higher-quality data to strictly
differentiate similar nodes.
3.3 The Overall Procedure of the CDR
Fig. 3 depicts the CDR structure, which computes consistency and
discrepancy metrics between tuples and objects either from member
interactions and tuple-member affiliations (upper part) or solely
from tuple interactions (lower part). The former method, referred
to as the pre-training stage, utilizes member interactions, while
the latter can fine-tune metrics using available tuple interactions,
aiding the extraction of node representations.
3.3.1 Two Variants of CDR. This subsection details the pre-training
and fine-tuning of CDR via two variants: CDR-P and CDR-F. CDR-
P, indicating pre-training, is trained using metrics from member
interactions and tuple-member affiliations, applicable in extreme
cold-start scenarios without tuple interactions. Conversely, CDR-
F (fine-tuning) is trained using tuple interaction metrics, align-
ing closer with traditional recommendation models. The full CDR
model is formulated by integrating both CDR-P and CDR-F variants.
We denote the embedding matrices for the pre-training and fine-
tuning stages as E𝑝andE𝑓, respectively. The matrix E𝑝initializes
E𝑓in the fine-tuning phase, providing improved initial embedding
values and fostering quicker model convergence. We also retain E𝑝
as a static feature and concatenate it with E𝑓to establish the final
embedding E:
E=E𝑓∥E𝑝, (19)
where∥is the concatenate operation. Eis used for subsequent
fine-tuning and recommendation. Our model only updates E𝑓but
notE𝑝in the fine-tuning stage, which can avoid the node prefer-
ences extracted in the pre-training stage being erased by down-
stream tasks, where member interaction and member-tuple affilia-
tion information is substantially preserved.
3.3.2 Generalization of Consistency and Discrepancy. We seek to ex-
tend the consistency and discrepancy metrics from Section 3.1.6 for
the fine-tuning stage. During pretraining, eight meta-path types are
utilized. Specifically, 𝑇𝑀𝑇 ,𝑇𝑀𝑂 ,𝑂𝑀𝑇 , and𝑂𝑀𝑂 stand for node
types on meta-paths, and each item can represent both reachable
and non-reachable meta-paths with the same node type. Matri-
ces, subscripted with member, denote consistency and discrepancy
 
948KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
Contrastive 
Learning
Inspired
CD LossPre-training
Fine-tuningCDR-P
CDR
CDR-FBuyBuyBelong toConsistency matrix 
Consistency matrix Discrepancy matrix 
Discrepancy matrix No tuple-object interactions
Tuple-object interactions availableMeta-paths 
derived
Meta-paths 
derived
Member
Tuple
 Object
Tuple
 Object
Figure 3: CDR Model Structure. Upper pre-training with tuple-member affiliations and member interactions, lower fine-tuning
with tuple interactions, featuring variants CDR-P and CDR-F. CDR-P is for extreme cold-start scenes, whereas CDR-F is a
variant that only uses tuple interactions.
across these meta-paths:
𝑪𝑚𝑒𝑚𝑏𝑒𝑟 =𝑪𝑇𝑀𝑇𝑪𝑇𝑀𝑂
𝑪𝑂𝑀𝑇𝑪𝑂𝑀𝑂
,
𝑫𝑚𝑒𝑚𝑏𝑒𝑟 =𝑫𝑇𝑀𝑇𝑫𝑇𝑀𝑂
𝑫𝑂𝑀𝑇𝑫𝑂𝑀𝑂
,(20)
where 𝑪𝑇𝑀𝑇=
𝑐𝑡,𝑡′|T|×|T|represents the consistency extracted
through the meta-path Tuple-Member-Tuple ( 𝑇𝑀𝑇 ), as in the other
sub-matrices. Elements in these matrices are used in the pre-training
stage.
In the fine-tuning stage, with accessible tuple interactions, ad-
ditional four types of meta-paths are considered, namely Tuple-
Object-Tuple ( 𝑇𝑂𝑇 ) and Object-Tuple-Object ( 𝑂𝑇𝑂 ) for consistency,
Tuple∼Object-Tuple ( 𝑇𝑂𝑇 ), and Object∼Tuple-Object ( 𝑂𝑇𝑂 ) for the
discrepancy, alongside direct 𝑇𝑂and𝑂𝑇relationships, given the
asymmetry of consistency and discrepancy. Corresponding matri-
ces are denoted as follows:
𝑪𝑡𝑢𝑝𝑙𝑒=𝑪𝑇𝑂𝑇𝑪𝑇𝑂
𝑪𝑂𝑇𝑪𝑂𝑇𝑂
,
𝑫𝑡𝑢𝑝𝑙𝑒=𝑫𝑇𝑂𝑇𝑫𝑇𝑂
𝑫𝑂𝑇𝑫𝑂𝑇𝑂
.(21)
Given that one-hop meta-paths 𝑇𝑂and𝑂𝑇cannot be utilized di-
rectly via Eq. (17), a redefinition is required. Drawing inspiration
from [ 31], we propose a generalized form for consistency and dis-
crepancy by modifying 𝛿in Eq. (17) as follows:
𝑪𝑇𝑂=[𝛿𝑡𝑜∗𝑌𝑡𝑜]|T|×|O|,𝑫𝑇𝑂=[𝛿𝑡𝑜]|T|×|O|,(22)
where 𝒀∈{0,1}|T|×|O|is the tuple interaction matrix. The node
relationship on the meta-path 𝑂𝑇is similar to 𝑇𝑂. With the ele-
ments in 𝑪𝑡𝑢𝑝𝑙𝑒 and𝑫𝑡𝑢𝑝𝑙𝑒 , the model can be fine-tuned to get a
better recommendation performance. The algorithm procedure of
CDR is presented in Appendix B.Table 2: The statistics of the datasets.
Dataset Tuple Memb
erObject T-O M-OAvg.M
per.T
Mafengw
o 995
5,275 1,513 3,595 39,761 7.19
Y
oushu 4,771
32,770 8,039 51,377 138,515 37.03
Last-FMUser Item U-I So
cial relation
1,892
17,632 92,834 25,434
4 EXPERIMENTS
In this section, we detail extensive experiments on various real-
world datasets to validate CDR’s effectiveness, addressing the fol-
lowing four research questions:
•RQ1 How does CDR compare to baselines in sparse tuple in-
teractions?
•RQ2 How does CDR compare to other methods in extreme
cold-start scenarios with no tuple interactions?
•RQ3 How do CDR’s components benefit its performance?
•RQ4 How does the hyperparameter setting of temperature
impact CDR’s performance?
Furthermore, we conduct extra analysis to compare the recom-
mendation diversity between CDR and other models. We also apply
the pre-training task of CDR to other models to demonstrate its
effectiveness.
4.1 Experimental Setup
4.1.1 Datasets. We experiment with public datasets Mafengwo,
Youshu, and Last-FM. Mafengwo evaluates group recommenda-
tion models with tourist ratings for travel spots, while Youshu is
used for bundle recommendations in book lists. Although Last-FM
is not directly applicable to the tripartite graph-based recommen-
dation, it can test CDR’s generalization to other tasks due to its
contained social relations. Experiments related to this dataset are
given in Appendix C. Dataset statistics are given in Table 2, and
the meta-path types are given in Appendix E.
 
949Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain.
4.1.2 Baselines. To validate the proposed CDR model, we juxta-
pose it with several top-tier baseline models across diverse recom-
mendation domains, which are given below:
•BPR [34]: A classic user-oriented item ranking algorithm.
•LINE [38]: Embed nodes into low-dimensional space for rec-
ommendation.
•NGCF [42]: Aggregate information in the interaction graph for
collaborative filtering.
•SimGCL [47]: Create contrastive views by adding noises to
user/item embedding.
•XSimGCL [46]: A method which generate contrastive views
by employing a simple noise-based embedding augmentation.
•ALDI [20]: A cold-start method which transnfer warm items’
behavioral information to cold items.
•BUIR [23]: Uses only positive samples and data augmentation
to alleviate data sparsity.
•DAM [5]: Utilize attention to aggregate item embeddings for
bundle recommendations.
•BGCN [3]: Unite user, item, and bundle nodes into a heteroge-
neous graph for information propagation.
•AGFN [24]: A bundle recommendation method which refines
the neighborhood aggregation mechanism from different as-
pects.
•AGREE [2]: Determine group embedding fusion ratios using
an attention mechanism.
•HCR [21]: Employ a two-channel hypergraph network for
multi-task, group-user interest modeling.
•GroupIM [35]: Maximize mutual information between groups
and corresponding users.
•CrossCBR [30]: Utilize and combines information from bundle
and item views.
•ConsRec [44]: Explore consensus behind group behavior data.
•SBPR [50]: Integrate social relations to augment training sam-
ples for the model.
•DiffNet [43]: Merge user representations from user-item bipar-
tite graphs and social networks.
•SocialLGN [25]: Embed social relations into interaction graphs
and executes graph convolution.
4.1.3 Evaluation Metrics. We utilize several metrics for evaluating
recommendation performance, including F1-Score@ 𝐾, Precision@ 𝐾,
Recall@𝐾, and NDCG@ 𝐾, which are prevalent in prior research
[3, 17, 21].
4.1.4 Implementation Details. Addressing tripartite graph-based
recommender systems’ cold-start issue, this study enforces strict
data constraints on models necessitating available tuple interac-
tions. Models are trained using merely 5% of tuple interactions,
while 20% are utilized for final performance assessment. Unlike
previous work [ 2,5,21] that employs 100 random negative samples
per test data, we apply Top- 𝐾ranking on all dataset items/bundles,
enhancing model evaluation accuracy. All models are set to a 64
embedding dimension to maintain fair comparison. Additionally,
the early stopping method’s patience is set at 10, halting training
after 10 consecutive unimproved epochs. Model parameters are
updated via Adam [ 22], with a 0.001 learning rate. Temperaturehyperparameters 𝜏for Mafengwo and Youshu datasets are config-
ured to 1 and 0.3 for fine-tuning, and 3.8 and 1 for pre-training. For
Last-FM dataset, 𝜏is set to 1.2 (pre-training) and 3 (fine-tuning).
Table 3: Comparison of CDR and baselines on group and
bundle recommendation datasets. Bold indicates best, and
underlined shows suboptimal results.
Dataset Metho
d R@10 R@20 R@30 N@10 N@20 N@30
Mafengw
oT
uple Interactions
BPR
0.0604 0.1147 0.1682 0.0317 0.0460 0.0583
LINE
0.0243 0.0316 0.0566 0.0143 0.0164 0.0219
NGCF
0.0391 0.0795 0.1135 0.0159 0.0263 0.0334
LightGCN
0.0766 0.1384 0.1943 0.0385 0.0557 0.0682
BUIR
0.0969 0.1509 0.1928 0.0593 0.0740 0.0839
ColdNAS
0.0184 0.0234 0.0282 0.0214 0.0352 0.0501
SGL 0.1288
0.2121 0.2626 0.0885 0.1088 0.1212
SimGCL
0.1263 0.2096 0.2500 0.0522 0.0740 0.0840
XSimGCL
0.0400 0.0643 0.0836 0.0988
0.0929 0.0928
ALDI
0.1150 0.2460 0.3203
0.0554 0.0880 0.0999
CDR-F 0.1954
0.2806 0.3238 0.1268 0.1493 0.1594
Impr
o. 51.71% 14.07% 1.09% 28.34% 37.22% 31.52%
T
uple Interaction/Member Interaction/Affiliation
A
GREE 0.0388 0.0685 0.0994 0.0186 0.0268 0.0338
HCR
0.0559 0.0899 0.1219 0.0275 0.0327 0.0388
Gr
oupIM 0.1453 0.2606 0.3170 0.0796 0.1109 0.1238
ConsRe
c 0.2312 0.3085 0.3487 0.1256 0.1449 0.1534
CDR 0.2694
0.3833 0.4309 0.1402 0.1715 0.1825
Impr
o. 16.52% 24.25% 23.57% 11.62% 18.36% 18.97%
Y
oushuT
uple Interactions
BPR
0.0188 0.0337 0.0442 0.0122 0.0170 0.0202
LINE
0.0234 0.0358 0.0467 0.0170 0.0208 0.0239
NGCF
0.0252 0.0479 0.0676 0.0167 0.0239 0.0298
LightGCN
0.0362 0.0573 0.0727 0.0269 0.0335 0.0379
BUIR 0.0508 0.0785 0.1034 0.0363 0.0444
0.0510
SGL
0.0247 0.0411 0.0530 0.0202 0.0257 0.0299
SimGCL
0.0313 0.0506 0.0602 0.0263 0.0322 0.0602
XSimGCL
0.0349 0.0581 0.0734 0.0331 0.0401 0.0452
ALDI
0.0228 0.0290 0.0612 0.0159 0.0179 0.0199
CDR-F 0.0648
0.1000 0.1217 0.0398 0.0501 0.0563
Impr
o. 27.41% 27.32% 17.78% 9.59% 12.74% -
T
uple Interaction/Member Interaction/Affiliation
D
AM 0.0195 0.0316 0.0452 0.0134 0.0169 0.0214
Cr
ossCBR 0.0281 0.0462 0.0611 0.0201 0.0262 0.0306
BGCN
0.0543 0.0848 0.1090 0.0358 0.0455 0.0528
A
GFN 0.0626 0.0981 0.1247 0.0457 0.0565 0.0652
CDR 0.0903
0.1299 0.1520 0.0627 0.0744 0.0809
Impr
o. 44.25% 32.42% 21.89% 37.20% 31.68% 24.08%
4.2 Performance Comparison
To address RQ1, we assess models with highly sparse tuple inter-
actions through experiments. Table 3 showcases the performance
of CDR and baseline models, yielding the following analyses.
Firstly, CDR significantly surpasses baselines in group and bun-
dle recommendations, averaging 18.88% and 31.92% improvement
on the Mafengwo and Youshu datasets respectively. CDR-F, us-
ing only tuple interactions, remains highly competitive in general
recommendation tasks. Notably, ConsRec and AGFN perform sub-
optimally in extremely sparse settings, sometimes underperforming
models like LightGCN and BUIR that solely utilize tuple interac-
tions. The issue may stem from the inapplicability of multi-task joint
optimization methods during the cold-start phase due to a severe
data imbalance problem: with 5% of tuple interactions for training
amidst abundant member interactions. Thus, models like AGREE,
HCR, and DAM, balancing tuple and member recommendations,
struggle. ConsRec, by exploring consensus behind group behavior
data, evades this imbalance issue, ranking as the best baseline in
group recommendation. AGFN refines the aggregation mechanism
to reach the best result in bundle recommendation. They still trail
behind CDR.
 
950KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
Summarily, CDR’s pre-eminence arises chiefly due to: (1) The
utility of consistency and discrepancy metrics, refining model learn-
ing of object-recommendee relationships. (2) The application of a
pre-training&fine-tuning approach, enhancing the generalization
of our self-supervised task and adeptly sidestepping tuple-member
interaction data imbalances. (3) The innovative contrastive CD loss,
markedly unlocking the potential of quantified node relationships.
We also conducted experiments about the performance of our CDR
on social recommendation for detailed discussion and clarification
on Last-FM dataset, which are given in Appendix C. Time complex-
ity analysis is given in Appendix D.
Table 4: Performance comparison of CDR-P in extreme cold-
start scenarios and baselines in warm-start scenarios.
Metho
dMafengw
o (Group)
Re
call@10 Recall@20 NDCG@10 NDCG@20
BPR 0.0604
0.1147 0.0317 0.0460
LINE 0.0243
0.0316 0.0140 0.0164
NGCF 0.0391
0.0795 0.0159 0.0263
LightGCN 0.0766
0.1384 0.0385 0.0557
BUIR 0.0969 0.1509 0.0593 0.0740
CDR-P 0.1275
0.1633 0.0653 0.0745
Y
oushu (Bundle)
Re
call@10 Recall@20 NDCG@10 NDCG@20
BPR 0.0188
0.0337 0.0122 0.0170
LINE 0.0234
0.0350 0.0170 0.0208
NGCF 0.0252
0.0479 0.0167 0.0239
LightGCN 0.0362 0.0573 0.0269 0.0335
BUIR 0.0508
0.0785 0.0363 0.0444
CDR-P 0.0280
0.0476 0.0190 0.0251
Table 5: Performance of variants of CDR.
Metho
dMafengw
o (Group)
F1-Scor
e Precision@20 Recall@20 NDCG@20
CDR-P 0.0195
0.0104 0.1633 0.0745
CDR-F 0.0336
0.0179 0.2806 0.1493
CDR-R 0.0425
0.0226 0.3531 0.1747
w/o𝑐 0.0296
0.0157 0.2411 0.1160
w/o𝑑 0.0349
0.0186 0.2790 0.1193
w/o𝑐&𝑑 0.0298
0.0158 0.2466 0.1177
CDR 0.0474
0.0253 0.3833 0.1715
Y
oushu (Bundle)
F1-Scor
e Precision@20 Recall@20 NDCG@20
CDR-P 0.0135
0.0079 0.0476 0.0251
CDR-F 0.0207
0.0116 0.1000 0.0501
CDR-R 0.0171
0.0099 0.0621 0.0348
w/o𝑐 0.0277
0.0157 0.1166 0.0648
w/o𝑑 0.0179
0.0114 0.0423 0.0280
w/o𝑐&𝑑 0.0180
0.0114 0.0427 0.0281
CDR 0.0334
0.0191 0.1299 0.0744
4.3 Performance in Extreme Cold-Start
Addressing RQ2, we test CDR-P in extreme cold-start situations,
with no tuple interactions available during training, contrasting
with baselines using available tuple interactions. It is an unfair
comparison since other models have more information from tuple
interactions. Table 4 reveals that our model, even without utiliz-
ing tuple interactions, outperforms all baselines on the Mafengwo
dataset, showcasing CDR’s robustness in extreme scenarios. The
average performance of CDR-P in Youshu is the third for the lack
of information.CDR-P excels in extreme cold-start scenarios because its super-
vision signals, consistency and discrepancy metrics, derive from
accessible and non-accessible meta-path views, built upon existing
member interactions and affiliations. These member interactions
and affiliations, being relatively abundant, furnish ample informa-
tion for recommendations. While data noise could potentially derail
model training, elevating the temperature hyperparameter concen-
trates on easily differentiated nodes, thereby boosting robustness
against noisy datasets. Consequently, CDR-P discerns approximate
node preferences, mitigating the adverse effects of data noise.
4.4 Ablation Study
Addressing RQ3, ablation tests on CDR components confirm each
part’s significance. CDR-P and CDR-F denote models using only
pre-training and solely tuple interactions, respectively, while CDR-
R swaps pre-training and fine-tuning data—using tuple interactions
for pre-training and member ones for fine-tuning. Table 5 reveals
omitting pre-training or fine-tuning stages reduces performance,
affirming CDR’s pre-training and fine-tuning efficacy. Swapping
training data stages also hampers performance, with a notable
degradation on the Youshu dataset after fine-tuning via member
interactions and affiliations, indicating the pivotal role and ordering
of tuple interactions in tripartite graph-based recommendations.
4.4.1 Ablation on Consistency and Discrepancy Metrics. Our con-
sistency and discrepancy metrics, reflecting complex tuple-object
node relationships beyond binary labels, embody real-life varied
user preferences for numerous purchased items. Validating these
metrics’ effectiveness, we binarize calculated consistency 𝑐and
discrepancy 𝑑. Table 5 reveals substantial performance drops when
omitting either metric, affirming their complementarity in con-
trastive learning-based loss and thus their collective necessity.
4.4.2 Ablation on the CD Loss. The CDR model’s loss function
was substituted with three alternatives to validate the proposed
contrastive learning-inspired CD loss. Specifically, CDR-Origin
employs the original loss (Eq. (16)), while CDR-MSE and CDR-
CE utilize mean squared error [ 19] and cross-entropy loss [ 18]
respectively. Results in Fig. 4 reveal CDR-Origin and CDR-MSE
underperform compared to CDR, potentially due to information
loss from the subtraction between consistency and discrepancy, as
discussed in Section 3.2.1.
The performance of CDR-CE and CDR, which use these node
relationships separately, is better than CDR-Origin and CDR-MSE.
The CD loss of CDR performed best due to its characteristics to
distinguish relatively easy or difficult nodes by the temperature 𝜏.
CD loss can adapt to data containing noise of different degrees.
4.5 Hyperparameter Sensitivity Analysis
To further answer RQ4, we conducted hyperparameter sensitivity
experiments on CDR, where Fig. 5 shows the effect of different
temperature values 𝜏in the pre-training and fine-tuning phases of
Mafengwo and Youshu datasets. We can find that CDR is sensitive
to the temperature 𝜏, and the optimal temperature value in the
pre-training stage is higher than that in the fine-tuning stage on
these two datasets. This occurs because member interactions are
noisier than tuple interactions.
 
951Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain.
0.0000.0100.0200.0300.0400.0500.060
0.0030 0.00310.02850.0474F1-Score
0.0000.0050.0100.0150.0200.0250.030
0.0016 0.00170.01520.0253Precision@20
0.0000.1000.2000.3000.400
0.0259 0.02360.23770.3833Recall@20
0.0000.0500.1000.1500.200
0.0088 0.00690.10280.1714NDCG@20
CDR-Origin CDR-MSE CDR-CE CDR
Figure 4: Performance comparison of different loss functions of CDR on the Mafengwo dataset.
0.7 0.8 0.9 1.0 2.00.2650.2700.2750.280Recall@20
 0.2690.2680.280 0.281
0.276CDR_F (Mafengwo)
0.9 1.0 2.0 3.0 4.00.1250.1300.1350.1400.1450.150
0.1330.1370.1400.149
0.127CDR_P (Mafengwo)
0.2 0.3 0.4 0.5 0.60.0600.0700.0800.0900.1000.110
0.0940.100
0.091
0.083
0.068CDR_F (Youshu)
0.8 0.9 1.0 2.0 3.00.0350.0380.0400.0430.0450.0480.050
0.0430.0450.048
0.038
0.037CDR_P (Youshu)
Figure 5: The effect of different temperatures 𝜏on the pre-training and fine-tuning phases.
0.0000.0100.0200.0300.0400.050
0.00860.0420
0.01850.0460F1-Score
0.0000.0050.0100.0150.0200.0250.030
0.00460.0224
0.00990.0245Precision@20
0.0000.1000.2000.3000.400
0.07490.3335
0.13840.3737Recall@20
0.0000.0500.1000.1500.2000.250
0.02520.1749
0.05570.2225NDCG@20
Base-NGCF Pretrain-NGCF Base-LightGCN Pretrain-LightGCN
Figure 6: The performance comparison of the two baselines on the Mafengwo dataset before and after CDR-P pre-training.
Therefore, a lower temperature is conducive to distinguishing
similar nodes, enabling the learning of fine-grained node repre-
sentations for tuple recommendations. Furthermore, an analysis
concerns the correlation between consistency & discrepancy and
loss is given in Appendix F.
4.6 Generality of the Pre-Training Task
Finally, we use the consistency and discrepancy derived in the
pre-training phase of CDR as the supervision signal to train two
traditional recommender baselines to verify the generalization abil-
ity of the two metrics. In Fig. 6, Base and Pretrain represent the
performance of the baselines with/without metrics obtained from
CDR-P pre-training as supervision signals, respectively. From Fig.
6, we can find that, with consistency and discrepancy as the super-
vision signal, the performance of NGCF and LightGCN improves
substantially compared to the models that use only binary tuple
interactions. The results further demonstrate the importance of
member interactions and member affiliations and the effectiveness
of the derived two metrics.
5 CONCLUSIONS
We introduce CDR, a novel tripartite graph-based recommendation
model to improve its performance in cold-start scenarios. The model
utilizes two new metrics, consistency and discrepancy, representingtuple-object relationships via reachable/non-reachable meta-paths,
based on ample user-item interactions and group-user/bundle-item
affiliations. Leveraging the GCN’s limit theory, these metrics are
efficiently pre-calculated before optimization. Furthermore, a novel
contrastive learning-based objective is proposed, independently
exploiting consistency and discrepancy as positive and contrastive
supervision signals. Extensive experiments across various recom-
mendation tasks and multiple real-world datasets confirm the pro-
posed metrics and contrastive learning-inspired CD loss’s general-
ization and robustness to tripartite graph-based recommendation’s
cold-start issues.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their insightful feedback
and constructive comments. This work was partially supported
by the National Natural Science Foundation of China (Grant No.
6217602), the Australian Research Council through the Industrial
Transformation Training Centre (Grant No. IC200100022), and the
Science and Technology Research Program of Chongqing Municipal
Education Commission (Grant No. KJZD-K202204402 and KJZD-
K202304401). Co-authors Yaochen Zhu and Chen Chen consulted
on this project on unpaid weekends for personal interests, and
appreciated collaborators and family for their understanding.
 
952KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
REFERENCES
[1]Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li, and Jun
Gao. 2019. Personalized bundle list recommendation. In Proceedings of the World
Wide Web Conference. ACM, 60–71.
[2]Da Cao, Xiangnan He, Lianhai Miao, Yahui An, Chao Yang, and Richang Hong.
2018. Attentive group recommendation. In Proceedings of the 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval. ACM,
645–654.
[3]Jianxin Chang, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2023. Bundle
Recommendation and Generation With Graph Neural Networks. IEEE Trans-
actions on Knowledge and Data Engineering 35, 3 (2023), 2326–2340. https:
//doi.org/10.1109/TKDE.2021.3114586
[4]Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, and Yi-Hsuan Yang. 2019.
Collaborative similarity embedding for recommender systems. In The World Wide
Web Conference. 2637–2643.
[5]Liang Chen, Yang Liu, Xiangnan He, Lianli Gao, and Zibin Zheng. 2019. Matching
user with item set: Collaborative bundle recommendation with deep attention
network. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence. ijcai.org, 2095–2101.
[6]Xusong Chen, Dong Liu, Chenyi Lei, Rui Li, Zheng-Jun Zha, and Zhiwei Xiong.
2019. BERT4SessRec: Content-based video relevance prediction with bidirec-
tional encoder representations from transformer. In Proceedings of the 27th ACM
International Conference on Multimedia. 2597–2601.
[7]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[8]Weiyu Cheng, Yanyan Shen, Linpeng Huang, and Yanmin Zhu. 2021. Dual-
Embedding Based Deep Latent Factor Models for Recommendation. ACM Trans.
Knowl. Discov. Data 15, 5, Article 85 (apr 2021), 24 pages. https://doi.org/10.1145/
3447395
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[10] Yulu Du, Xiangwu Meng, and Yujie Zhang. 2020. CVTM: A Content-Venue-Aware
Topic Model for Group Event Recommendation. IEEE Transactions on Knowledge
and Data Engineering 32, 7 (2020), 1290–1303. https://doi.org/10.1109/TKDE.
2019.2904066
[11] Yulu Du, Xiangwu Meng, Yujie Zhang, and Pengtao Lv. 2020. GERF: A Group
Event Recommendation Framework Based on Learning-to-Rank. IEEE Trans-
actions on Knowledge and Data Engineering 32, 4 (2020), 674–687. https:
//doi.org/10.1109/TKDE.2019.2893361
[12] Sarina Sajjadi Ghaemmaghami and Amirali Salehi-Abari. 2021. DeepGroup:
Group recommendation with implicit feedback. In Proceedings of the 30th ACM
International Conference on Information and Knowledge Management. ACM, 3408–
3412.
[13] Lei Guo, Hongzhi Yin, Qinyong Wang, Bin Cui, Zi Huang, and Lizhen Cui. 2020.
Group recommendation with latent voting mechanism. In Proceedings of the 36th
IEEE International Conference on Data Engineering. IEEE, 121–132.
[14] Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A
new estimation principle for unnormalized statistical models. In Proceedings of
the 13th International Conference on Artificial Intelligence and Statistics, Vol. 9.
JMLR.org, 297–304.
[15] Bowen Hao, Hongzhi Yin, Jing Zhang, Cuiping Li, and Hong Chen. 2023. A
Multi-strategy-based Pre-training Method for Cold-start Recommendation. ACM
Transactions on Information Systems 41, 2 (2023), 1–24.
[16] Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen. 2021. Pre-
training graph neural networks for cold-start users and items representation. In
Proceedings of the 14th ACM International Conference on Web Search and Data
Mining. ACM, 265–273.
[17] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. ACM, 639–648.
[18] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International
Conference on World Wide Web. ACM, 173–182.
[19] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for
implicit feedback datasets. In Proceedings of the 8th IEEE International Conference
on Data Mining. IEEE Computer Society, 263–272.
[20] Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen.
2023. Aligning distillation for cold-start item recommendation. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1147–1157.
[21] Renqi Jia, Xiaofei Zhou, Linhua Dong, and Shirui Pan. 2021. Hypergraph convolu-
tional network for group recommendation. In Proceedings of the IEEE International
Conference on Data Mining. IEEE, 260–269.[22] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimiza-
tion. In Proceedings of the 3rd International Conference on Learning Representations.
[23] Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu. 2021.
Bootstrapping user and item representations for one-class collaborative filtering.
InProceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 317–326.
[24] Xiang-Long Li, Wu-Dong Xi, Xing-Xing Xing, and Chang-Dong Wang. 2023.
Auto Graph Filtering for Bundle Recommendation. In 2023 IEEE International
Conference on Data Mining (ICDM). IEEE, 299–308.
[25] Jie Liao, Wei Zhou, Fengji Luo, Junhao Wen, Min Gao, Xiuhua Li, and Jun Zeng.
2022. SocialLGN: Light graph convolution network for social recommendation.
Information Sciences 589 (2022), 595–607.
[26] Guannan Liu, Yanjie Fu, Guoqing Chen, Hui Xiong, and Can Chen. 2017. Modeling
Buying Motives for Personalized Product Bundle Recommendation. ACM Trans.
Knowl. Discov. Data 11, 3, Article 28 (mar 2017), 26 pages. https://doi.org/10.
1145/3022185
[27] Siwei Liu, Zaiqiao Meng, Craig Macdonald, and Iadh Ounis. 2023. Graph Neural
Pre-Training for Recommendation with Side Information. ACM Trans. Inf. Syst.
41, 3, Article 74 (feb 2023), 28 pages. https://doi.org/10.1145/3568953
[28] Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang,
Aixin Sun, and Chunyan Miao. 2021. Pre-training graph transformer with mul-
timodal side information for recommendation. In Proceedings of the 29th ACM
International Conference on Multimedia. 2853–2861.
[29] Linyuan Lü, Matúš Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao
Zhou. 2012. Recommender systems. Physics reports 519, 1 (2012), 1–49.
[30] Yunshan Ma, Yingzhi He, An Zhang, Xiang Wang, and Tat-Seng Chua. 2022.
CrossCBR: Cross-View Contrastive Learning for Bundle Recommendation. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (Washington DC, USA) (KDD ’22). Association for Computing Machinery,
New York, NY, USA, 1233–1241. https://doi.org/10.1145/3534678.3539229
[31] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He.
2021. UltraGCN: Ultra simplification of graph convolutional networks for recom-
mendation. In Proceedings of the 30th ACM International Conference on Information
and Knowledge Management. ACM, 1253–1262.
[32] Apurva Pathak, Kshitiz Gupta, and Julian McAuley. 2017. Generating and person-
alizing bundle recommendations on steam. In Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
ACM, 1073–1076.
[33] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training
user representations for improved recommendation. In Proc. of the AAAI Confer-
ence on Artificial Intelligence. Menlo Park, CA, AAAI. AAAI Press, 1–8.
[34] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings
of the 25th Conference on Uncertainty in Artificial Intelligence. AUAI Press, 452–
461.
[35] Aravind Sankar, Yanhong Wu, Yuhang Wu, Wei Zhang, Hao Yang, and Hari
Sundaram. 2020. Groupim: A mutual information maximization framework for
neural group recommendation. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1279–1288.
[36] Yizhou Sun and Jiawei Han. 2013. Mining heterogeneous information networks:
A structural analysis approach. ACM SIGKDD Explorations Newsletter 14, 2 (2013),
20–28.
[37] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi Wu. 2011. Pathsim:
Meta path-based top-k similarity search in heterogeneous information networks.
Proceedings of the VLDB Endowment 4, 11 (2011), 992–1003.
[38] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In Proceedings of the
24th International Conference on World Wide Web. ACM, 1067–1077.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[40] Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and S Yu Philip. 2021. Pre-
training Graph Neural Network for Cross Domain Recommendation. In 2021
IEEE Third International Conference on Cognitive Machine Intelligence (CogMI).
IEEE, 140–145.
[41] Hui Wang, Kun Zhou, Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. 2023. Cur-
riculum Pre-Training Heterogeneous Subgraph Transformer for Top-N Recom-
mendation. ACM Transactions on Information Systems 41, 1 (2023), 1–28.
[42] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd International ACM
SIGIR Conference on Research and Development in Information Retrieval . ACM,
165–174.
[43] Le Wu, Peijie Sun, Yanjie Fu, Richang Hong, Xiting Wang, and Meng Wang. 2019.
A neural influence diffusion model for social recommendation. In Proceedings
of the 42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 235–244.
[44] Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Jiawei Zhang, Yangyong Zhu, and
Philip S Yu. 2023. ConsRec: Learning Consensus Behind Interactions for Group
Recommendation. In Proceedings of the ACM Web Conference 2023. 240–250.
 
953Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain.
[45] Hongzhi Yin, Qinyong Wang, Kai Zheng, Zhixu Li, and Xiaofang Zhou. 2022.
Overcoming Data Sparsity in Group Recommendation. IEEE Transactions on
Knowledge and Data Engineering 34, 7 (2022), 3447–3460. https://doi.org/10.1109/
TKDE.2020.3023787
[46] Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, and
Hongzhi Yin. 2023. XSimGCL: Towards extremely simple graph contrastive
learning for recommendation. IEEE Transactions on Knowledge and Data Engi-
neering (2023).
[47] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung
Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive
learning for recommendation. In Proceedings of the 45th international ACM SIGIR
conference on research and development in information retrieval. 1294–1303.
[48] Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021.
Double-scale self-supervised hypergraph learning for group recommendation.
InProceedings of the 30th ACM International Conference on Information and
Knowledge Management. ACM, 2557–2567.
[49] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang,
and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recom-
mendation.. In IJCAI. 3356–3362.
[50] Tong Zhao, Julian McAuley, and Irwin King. 2014. Leveraging social connections
to improve personalized ranking for collaborative filtering. In Proceedings of the
23rd ACM International Conference on Conference on Information and Knowledge
Management. ACM, 261–270.
[51] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,
Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for
sequential recommendation with mutual information maximization. In Proceed-
ings of the 29th ACM International Conference on Information and Knowledge
Managemen. ACM, 1893–1902.
[52] Yaochen Zhu and Zhenzhong Chen. 2022. Mutually-regularized dual collaborative
variational auto-encoder for recommendation systems. In Proceedings of the ACM
Web Conference 2022. ACM, 2379–2387.
[53] Yaochen Zhu and Zhenzhong Chen. 2022. Variational bandwidth auto-encoder
for hybrid recommender systems. IEEE Transactions on Knowledge and Data
Engineering (2022).
A RELATED WORK
A.1 Tripartite Graph-Based Recommendation
In conventional recommendation systems, user-item interactions
are typically represented as a bipartite graph, with various Graph
Convolutional Network (GCN)-based methods modeling these in-
teractions. This paper introduces a novel concept: tripartite graph-
based recommendations, which entail recommendation tasks in-
volving interactions among three distinct entity types. The third
entity often comprises user and item groups, introducing a novel
affiliation between items and bundles or users and groups. In this
model, if an item (recommended object) is recommended to a user
group (recommendee), it is termed a “group recommendation task”
[10,11,45]; if an item group (recommended object) is recommended
to a user (recommendee), it is known as a “bundle recommendation”
[3,5,30]. As the first to consolidate these into one overarching prob-
lem, we delineate group and bundle recommendations separately
in the ensuing section.
A.1.1 Group Recommendation. Group recommendation targets
recommending a single item to a user group, accounting for var-
ied interests. Often formed temporarily, user groups usually yield
sparse group-item interactions[ 35,45,48]. Guo et al. [ 13] intro-
duced a model addressing sparsity by leveraging a self-attention
mechanism to learn users’ social influence in temporary group rec-
ommendations. Similarly, Cao et al. [ 2] proposed AGREE, merging
neural collaborative filtering and attention mechanisms to dynam-
ically determine group embedding fusion weights. Jia et al. [ 21]
advanced this by fusing member and group networks into a dual-
channel hypergraph convolutional model, HCR.
A.1.2 Bundle Recommendation. Bundle recommendation seeks to
suggest multiple items collectively to a single user and, akin togroup recommendation, faces a pervasive data sparsity issue due
to the arbitrary bundling of items. Attention networks are often
utilized to ascertain the weight of items in a bundle [ 30]. Further-
more, multi-task learning is commonly employed to meld item and
bundle interaction data, mitigating sparsity. For instance, Chen et al.
[5] devised DAM, a bundle recommendation model that interlinks
user-bundle and user-item interactions within a multi-task joint
optimization framework. Gao et al. [ 3] amalgamated user-item inter-
actions, user-bundle interactions, and bundle-item affiliations into
a heterogeneous graph to ensure bundle embeddings encapsulate
item semantics.
A.2 Pre-Training and Fine-Tuning in
Recommendation
Pre-training, aiming to optimize model parameters utilizing label
ground truth or auxiliary data, facilitates faster convergence or
reduces label needs during subsequent fine-tuning stages [ 15,27,
28,40]. In recommendations, pre-training and fine-tuning, viewed
as self-supervised learning, entails initial training on augmented
data, followed by fine-tuning on original data, adapting to the
designated recommendation task.
Key aspects of pre-training involve creating augmented data
and devising a pretext task. Chen et al. [ 6] and Zhang et al. [ 49],
inspired by the NLP model BERT [ 9], designed similar models
for recommendations where side information on users/items is
available. Wang et al. [ 41] leveraged this by viewing interaction
data through heterogeneous graphs and building specific subgraphs.
Similarly, models utilizing side information in pre-training have
been proposed [ 51]. Qiu et al. [ 33] masked sections of user and item
reviews, using bidirectional context to reconstruct them.
B ALGORITHM
The overall process of CDR is presented in Algorithm 1.
C SUPPLEMENTARY EXPERIMENTS
To further explore if CDR can generalize to other recommendation
types with existing consistency and discrepancy metrics. we vali-
date CDR’s adaptability to other recommendation tasks, testing on
the Last-FM social recommendation dataset. The baseline methods
we choose are SBPR [50],DiffNet [43], and SocialLGN [25]. As
Table 6 indicates, models utilizing social relations outperform those
relying solely on interaction data. CDR-F remains competitive us-
ing only user-item interactions. In social recommendation, which
only involves user social relationships, CDR-P outperforms CDR-F
by leveraging both social and interaction data. Post-fine-tuning,
CDR, amalgamating the benefits of CDR-P and CDR-F, enhances
all metrics, substantiating its generalizability across various recom-
mendation domains where a tripartite graph can be constructed.
D TIME COMPLEXITY ANALYSIS
Our analysis covers each critical stage of our method, i.e., pre-
processing, pre-training, and fine-tuning. In the pre-processing
stage, for each member 𝑚, we need to calculate two kinds of terms,
i.e.,𝛿𝑚,𝑡and𝛿𝑚,𝑜, and the time complexity is 𝑂(|M||T|)+𝑂(|M||O|) ,
whereT,O,Mare sets of the three types of nodes in a tripartite
 
954KDD ’24, August 25–29, 2024, Barcelona, Spain. Linxin Guo et al.
Table 7: The runtime of different graph-based method.
Metho
d ALDI XSimGCL A
GFN CDR
Runtime 0.0820s 0.0901s 0.2263s 0.0738s
Algorithm 1: The overall process of CDR.
Input: Tuple interaction matrix 𝑌
Member interaction matrix 𝑋
Tuple-member affiliation matrix 𝑍
Output: Final node embeddings E
1Extract 𝑪𝑚𝑒𝑚𝑏𝑒𝑟 and𝑫𝑚𝑒𝑚𝑏𝑒𝑟 from member interaction 𝑋
and affiliation 𝑍
2Randomly initialize embedding E𝑝
3while not converge do
4 foreach epoch do
5 EvaluateLthrough 𝑪𝑚𝑒𝑚𝑏𝑒𝑟 ,𝑫𝑚𝑒𝑚𝑏𝑒𝑟 , and E𝑝(Eq.
(18))
6 Backpropagation and update embeddings E𝑝
7 end
8end
9Extract 𝑪𝑡𝑢𝑝𝑙𝑒 and𝑫𝑡𝑢𝑝𝑙𝑒 from tuple interactions 𝑌
10Stop gradient of E𝑝
11Initialize node embedding E𝑓with E𝑝
12Concatenate final embeddings E←E𝑓∥E𝑝
13while not converge do
14 foreach epoch do
15 EvaluateLthrough 𝑪𝑡𝑢𝑝𝑙𝑒 ,𝑫𝑡𝑢𝑝𝑙𝑒 , and E(Eq. (18))
16 Back propagation and update node embeddings E𝑓
17 end
18end
19return Final node embeddings E
Table 6: Performance comparison of CDR model and base-
lines under social recommendation scenarios.
Dataset Metho
d F1-Score Precision@20 Recall@20 NDCG@20
Last-FMUser-item
interaction
BPR
0.0277 0.0207 0.0417 0.0353
BUIR
0.0484 0.0361 0.0731 0.0685
NGCF
0.0505 0.0379 0.0756 0.0721
LightGCN 0.0568 0.0425 0.0857 0.0844
CDR-F 0.0576
0.0428 0.0878 0.0798
User-item
interaction/Social relation
SBPR
0.0544 0.0405 0.0829 0.0725
DiffNet
0.0570 0.0427 0.0859 0.0805
So
cialLGN 0.0665 0.0495 0.1011 0.0894
CDR-P 0.0756 0.0563 0.1151 0.1108
CDR 0.0836
0.0622 0.1274 0.1195
graph-based recommendation, and ||is the number of elements
in a set. Similarly, we need to calculate two other kinds of terms
𝛿𝑜,𝑡and𝛿𝑡,𝑜, and the time complexity is 𝑂(|O||T|) . Then, with the
four kinds of terms, i.e., 𝛿𝑚,𝑡,𝛿𝑚,𝑜,𝛿𝑜,𝑡, and𝛿𝑡,𝑜, we can directly
get the values in 𝐶𝑚𝑒𝑚𝑏𝑒𝑟 ,𝐷𝑚𝑒𝑚𝑏𝑒𝑟 ,𝐶𝑡𝑢𝑝𝑙𝑒 , and𝐷𝑡𝑢𝑝𝑙𝑒 for further
training. All calculations in pre-processing do not include matrix
multiplication, so this stage does not take a lot of time.
In our model, the time complexity for both the pre-training and
fine-tuning stages is similar. Here, 𝑄represents the set of nodepairs that exhibit positive consistency and whose types belong to
tuples or objects. According to the loss function, we can derive
the time complexities for every batch of the two stages are both
𝑂((|𝑄|
𝐵+|𝑄|)𝑑), where𝑑is the dimension of embedding, and 𝐵is
the batch size. Note that a pair of nodes is included in the set 𝑄
only if they share common neighborhood nodes. Given the spar-
sity characteristic of recommendation datasets, 𝑄is significantly
smaller thanO×T . This inherent sparsity greatly reduces the ac-
tual computational time, thereby ensuring efficiency in processing.
We conduct a comparison of model runtime, with the results
shown in Table 7. Our CDR is the fastest because it does not require
node aggregation. ALDI [ 20] and XSimGCL [ 46], which only utilize
T-O interactions, also offer relatively fast speeds. In contrast, AGFN
[24] processes more slowly for using graph aggregation methods
on tripartite graph.
E TYPES OF META-PATHS
We summarize all the meta-path types in Mafengwo and Youshu
datasets, and the result is shown in Table 8, where G indicates a
group, B indicates a bundle, and U and I are a user and an item
separately.
Table 8: The metapaths for different scenarios.
Scenarios Dataset Meta-path
Group MafengwoPre-train GUG/GUI/IUG/IUI
Fine-tune GIG/GI/IG/IGI
Bundle YoushuPre-train UIU/UIB/BIU/BIB
Fine-tune UBU/UB/BU/BUB
GeneralMafengwo GIG/GI/IG/IGI
Youshu UBU/UB/BU/BUB
FCORRELATION BETWEEN CONSISTENCE &
DISCREPANCY AND THE TRAINING LOSS
We conduct an experiment by calculating the Pearson correlation
coefficients between the metrics consistency ( 𝑐) and discrepancy
(𝑑), and the loss for each node pair, as presented in Table 9. The
experimental results indicate that there is a negative correlation
between consistency and loss, and a positive correlation between
discrepancy and loss. This means node pairs with high consistency
tend to have lower loss, while those with greater discrepancy exhibit
higher loss.
This pattern of correlations allows us to leverage the metrics 𝑐
and𝑑strategically during the optimization process. To effectively
minimize loss, an increase in 𝑐𝑣𝑖,𝑣𝑗should correspond with an in-
crease in𝑐𝑜𝑠(𝑒𝑣𝑖,𝑒𝑣𝑗), enhancing similarity between embeddings.
Conversely, an increase in 𝑑𝑣𝑖,𝑣𝑗should result in a decrease in
𝑐𝑜𝑠(𝑒𝑣𝑖,𝑒𝑣𝑗), reducing similarity.
Table 9: The Pearson correlation coefficients between the
metrics consistency ( 𝑐) and discrepancy ( 𝑑), and the loss for
each node pair.
Dataset Corr(𝑐,loss) Corr(𝑑,loss)
Youshu -0.60 0.52
Mafengwo -0.46 0.43
 
955