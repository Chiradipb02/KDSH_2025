Conversational Dueling Bandits in Generalized Linear Models
Shuhua Yang
University of Science and Technology
of China
Hefei, China
shuashua0608@mail.ustc.edu.comHui Yuan
Princeton University
Princeton, NJ, USA
huiyuan@princeton.eduXiaoying Zhang
ByteDance
Beijing, China
zhangxiaoying.xy@bytedance.com
Mengdi Wang
Princeton University
Princeton, NJ, USA
mengdiw@princeton.eduHong Zhang
University of Science and Technology
of China
Hefei, China
zhangh@ustc.edu.cnHuazheng Wang
Oregon State University
Corvallis, OR, USA
huazheng.wang@oregonstate.edu
Abstract
Conversational recommendation systems elicit user preferences by
interacting with users to obtain their feedback on recommended
commodities. Such systems utilize a multi-armed bandit framework
to learn user preferences in an online manner and have received
great success in recent years. However, existing conversational ban-
dit methods have several limitations. First, they only enable users
to provide explicit binary feedback on the recommended items or
categories, leading to ambiguity in interpretation. In practice, users
are usually faced with more than one choice. Relative feedback,
known for its informativeness, has gained increasing popularity
in recommendation system design. Moreover, current contextual
bandit methods mainly work under linear reward assumptions, ig-
noring practical non-linear reward structures in generalized linear
models. Therefore, in this paper, we introduce relative feedback-
based conversations into conversational recommendation systems
through the integration of dueling bandits in generalized linear
models (GLM) and propose a novel conversational dueling bandit
algorithm called ConDuel. Theoretical analyses of regret upper
bounds and empirical validations on synthetic and real-world data
underscore ConDuel’s efficacy. We also demonstrate the potential to
extend our algorithm to multinomial logit bandits with theoretical
and experimental guarantees, which further proves the applicability
of the proposed framework.
CCS Concepts
•Theory of computation →Online algorithms; •Information
systems→Recommender systems.
Keywords
Conversational Recommendation, Dueling Bandits, Generalized
Linear Model
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671892ACM Reference Format:
Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang,
and Huazheng Wang. 2024. Conversational Dueling Bandits in Generalized
Linear Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671892
1 Introduction
Contextual bandit is an essential tool in recommendation systems
to enhance the performance of the system while making a trade-off
between exploitation and exploration [ 1,13]. In recommendation
scenarios, each item is considered as an arm with its contextual
vector summarizing the information of both the arm and the user.
At each round, the recommendation system sequentially suggests
items to the user and collects feedback (e.g., click) on the selected
item. The agent’s goal in the system is to develop an item recom-
mendation (arm selection) strategy that maximizes the cumulative
reward from the user by leveraging information about the user and
items, as well as the user’s previous interaction records.
In many scenarios, it is challenging to effectively utilize user
feedback and recommend optimally for cold-start users due to lim-
ited historical data, with insufficient data to learn users’ preferences
reliably. To accelerate the learning process of user preferences and
offer optimal recommendations, conversational recommendation
systems (CRSs) have been proposed in [ 7], [28] and [ 24]. In CRSs,
the system not only gathers responses on recommended items
but also sparks conversations by asking users about relevant "key-
terms," such as categories or entities associated with news articles
in news recommendation systems. According to [ 28], these inter-
actions accelerate CRS learning by leveraging key-terms linked to
numerous items, offering valuable insights into user preferences.
Despite previous successes in CRSs, current conversation mech-
anisms, particularly conversational contextual bandit approaches,
often fall short. First and foremost, current conversational contex-
tual bandit approaches concentrate solely on explicit user feedback
for specific items/categories, which can be ambiguous and fail to ef-
fectively capture user preferences. In contrast, relative feedback has
been proven to be informative and is commonly observed in various
settings including recent applications in Reinforcement Learning
with Human Feedback (RLHF) [ 10,17,21,30].To better illustrate
the difference between CRS with different feedback mechanisms,
3806
KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
Figure 1: An illustrative example of a conversational system
with pairwise feedback compared with a system that only
allows explicit feedback.
we give a simple yet illustrative example in Fig 1. For previous
CRSs that only allow explicit feedback, they would inquire about
the user’s preference on a particular commodity category through
queries such as: "Are you interested in digital products?", making
it difficult for the user to respond with a simple "Yes" or "No", es-
pecially when the user is unsure about the specific type of digital
products being asked about. The user may be interested in Ebooks
but dislikes video games, thus providing ambiguous feedback to the
agent. Alternatively, for agents that allow relative feedback and ask
questions such as "Do you prefer digital products or video games?",
the user can provide more decisive feedback, prompting the system
to understand the user’s preference more efficiently.
Building on these observations, our paper proposes to build a
CRS that guarantees relative feedback within the dueling bandit
framework [ 26,27,31]. Inspired by [ 8], which proposed the first
contextualized extension of dueling bandits, we also incorporate
the contextual information into this framework. Furthermore, al-
though most CRSs have adopted the linear bandit framework, the
linear reward assumptions may not align with practical scenarios.
To address this limitation, we relax the linear reward assumption
and design a more practical dueling bandit approach by employing
a generalized linear model (GLM). This approach, as demonstrated,
yields significant improvements over the linear models commonly
utilized in current CRSs [ 12]. Designing a CRS under a dueling
bandits framework in GLM imposes challenges, including (1) deter-
mining key-term pairs to query and (2) selecting arm pairs based on
interactions from both the key-term and arm modules. To address
these, we propose the Conversational Dueling Bandit algorithm
(ConDuel). Our approach involves conversations on "exploratory"
key-term pairs and item pair selection based on the uncertainty
principle. Leveraging feedback from both modules, our ConDuel
algorithm extends CRS to a pairwise dueling bandit model. Throughexperiments on synthetic and real-world datasets, we demonstrate
the competitiveness of our algorithm over baselines. We also demon-
strate the potential to extend the pairwise feedback model into a
multi-choice model, proposing a ConMNL algorithm that can tackle
the multinomial logit bandit problem. In summary, the contribution
of our work is three-fold:
•We propose a new framework for conversation recommender
systems (CRS) that can efficiently utilize relative feedback
upon each query. We specifically design the dueling bandit
algorithm ConDuel to achieve the objective. To the best of
our knowledge, this is the first work that enables informative
pairwise preference-based questions on both key terms and
items in CRS.
•Our ConDuel algorithm applies a generalized linear model
and we provide a sublinear regret upper bound as theoret-
ical support. Besides pairwise comparison, we also extend
ConDuel to multiple comparisons under the choice model
with the proposed ConMNL algorithm.
•Extensive experiments on a synthetic dataset and two real-
world datasets verify the efficiency of the proposed ConDuel
algorithm and ConMNL algorithm.
2 Related work
Our work builds on several research areas, and we review some
recent work in the most related areas.
Conversational Bandits. Contextual bandit algorithms aim to op-
timize the expected cumulative rewards, in the long run [ 1,13].
Traditional linear bandits require extensive exploration to learn
user preferences in recommender systems. [ 7] first proposed multi-
armed bandit models in Conversational recommender systems to
acquire users’ feedback on each item. Afterward, [ 28] systematically
studied conversational contextual bandit and proposed a ConUCB
algorithm to accelerate online recommendations, which allows the
agent to obtain user feedback on key-terms related to items and
leverage this information to accelerate the system. Building on this
framework, some follow-up works have extended conversational
contextual bandits in various settings, such as using clustering
techniques to create self-generated key-terms [ 23], obtaining rela-
tive feedback from key-terms [ 24], leveraging knowledge graphs
to study the underlying relations between key-terms [ 29], and in-
corporating both information from arm-level and key-term-level
to construct a holistic model [ 22]. Although recent work in [ 24]
has incorporated relative feedback in the key-term module, the
algorithm proposed in their work is a simple empirical extension
of ConUCB in [ 28], in which the system utilizes a pseudo update
over key-terms without theoretical guarantees.
Utility-based Dueling Bandits. In utility-based dueling bandits,
the absolute preference for each arm can be reflected by a real-
valued utility degree [ 4]. When applied to dueling bandits, this
setting is also known as utility-based dueling bandits , where a
latent utility function 𝑢:A → Rexists, with 𝑢(𝑎𝑖)represent-
ing the utility of an arm 𝑎𝑖∈A. The probability of arm 𝑎𝑖win-
ning over𝑎𝑗can be determined by the difference of their utili-
ties using a link function 𝜇:R→[0,1], and can be written as
𝑃(𝑎𝑖>𝑎𝑗)=𝜇(𝑢(𝑎𝑖)−𝑢(𝑎𝑗)). In contextual bandit, the utility of
3807Conversational Dueling Bandits in Generalized Linear Models KDD ’24, August 25–29, 2024, Barcelona, Spain
an arm is assumed to be linear based on an unknown preference
vector, which has been studied in [ 5,18,19]. Utility-based dueling
bandits can also be extended to the preference-based reinforcement
learning framework in [25] and [20].
3 Problem Formulation
In this section, we introduce the general framework of conver-
sational dueling bandits with a generalized linear model (GLM).
Suppose there are 𝑁arms denoted byAand𝐾key-terms denoted
byK. At each round 𝑡=1,...,𝑇 , the agent is given a subset of
armsA𝑡⊂A , where each arm 𝑎∈A𝑡is associated with a con-
textual vector 𝑥𝑎,𝑡∈R𝑑. Without loss of generality, we assume
that the feature vectors are normalized, i.e., ∥𝑥𝑎,𝑡∥2=1. We also
assume that the unknown user preference vector 𝜃∗∈R𝑑satisfies
the inequality∥𝜃∗∥2≤1. The relationship between the arms and
the key-terms can be characterized by a weighted bipartite graph
(A,K,W), whose nodes are divided into two sets AandK, and
weighted edges are represented by the matrix W≜[𝑤𝑎,𝑘]with
𝑤𝑎,𝑘representing the relationship between arm 𝑎and key-term 𝑘.
Without loss of generality, we assumeÍ
𝑘𝑤𝑎,𝑘=1.
Generalized Linear Dueling Feedback. We assume the latent util-
ity function 𝑢:A→Ris linear and 𝑢(𝑎,𝑡)=𝑥𝑇
𝑎,𝑡𝜃∗represents
the utility of the arm 𝑎∈ A𝑡at round𝑡, with𝜃∗∈R𝑑being
the unknown preference vector. We also define a link function
𝜇:R→[0,1], so that at round 𝑡, the probability of arm 𝑎𝑖∈A𝑡
winning over 𝑎𝑗∈A⊔satisfies the following equation:
𝑃(𝑎𝑖>𝑎𝑗)=𝜇(𝑢(𝑎𝑖,𝑡)−𝑢(𝑎𝑗,𝑡))=𝜇(𝑥𝑇
𝑖,𝑡𝜃∗−𝑥𝑇
𝑗,𝑡𝜃∗).(1)
The link function 𝜇satisfies the following properties [26]:
•𝜇is monotonically increasing, so that an arm with a higher
utility than another arm will have a higher probability to be
chosen than the latter.
•𝜇(0)=1/2, indicating that two arms having the same utility
have also the same probability of being chosen.
•𝜇(−∞) =0,𝜇(∞)=1.
It is easy to verify the two most common link functions: logis-
tic function 𝜇(𝑥)=1/(1+exp(−𝑥))and linear function 𝜇(𝑥)=
max{0,min{1,0.5∗(1+𝑥)}}both satisfy the properties. Following
[9], we assume that 𝜇satisfies the following assumptions.
Assumption 1. 𝜅1=inf{∥𝑥∥2≤2,𝜃∈Θ}𝜇′(𝑥𝑇𝜃)>0, where Θis a
closed subset of the space R𝑑containing𝜃∗.
Assumption 2. 𝜇is twice differentiable, and its first and second-
order derivatives are upper-bounded by constant 𝐿𝜇and𝑀𝜇respec-
tively. When 𝜇is the sigmoid function, 𝐿𝜇and𝑀𝜇can be 1/4.
Similar to Eq 1, we define the probability of key-term 𝑘𝑖beating
𝑘𝑗at round𝑡as
𝑃(𝑘𝑖>𝑘𝑗)=𝜇(˜𝑢(𝑘𝑖,𝑡)−˜𝑢(𝑘𝑗,𝑡)),
where ˜𝑢(𝑘,𝑡):=Í
𝑎∈A𝑤𝑎,𝑘𝑢(𝑎,𝑡)Í
𝑎∈A𝑤𝑎,𝑘, indicating that the utility of 𝑘is
determined by averaging over that of its related arms. The relative
feedback of two key-terms in a duel is also determined by their
utilities. Equivalently, we rewrite the inequality as
𝑃(𝑘𝑖>𝑘𝑗)=𝜇(˜𝑥𝑇
𝑘𝑖,𝑡𝜃∗−˜𝑥𝑇
𝑘𝑗,𝑡𝜃∗), (2)with ˜𝑥𝑘,𝑡=Í
𝑎∈A𝑤𝑎,𝑘𝑥𝑎,𝑡Í
𝑎∈A𝑤𝑎,𝑘representing the feature vector of key-
term𝑘at round𝑡.
At round𝑡, given the candidate arm set A𝑡, we present to the
user a pair of arms (𝑎𝑡,𝑎′
𝑡)∈A𝑡×A𝑡and ask for his/her relative
preference. At round 𝑡, user’s preference is encoded by a binary
random variable 𝑜𝑡=1(𝑎𝑡>𝑎′
𝑡). Denote by 𝑑𝑡:=𝑥𝑎𝑡−𝑥𝑎′
𝑡,𝑜𝑡
follows the Bernoulli distribution 𝐵𝑒𝑟(𝜇(𝑑𝑇
𝑡𝜃∗)), and the arm-level
feedback model can be written as
𝑜𝑡=𝜇(𝑑𝑇
𝑡𝜃∗)+𝜖𝑡, (3)
where𝜖𝑡is a zero-mean noise defined as
𝜖𝑡=(
1−𝜇(𝑑𝑇
𝑡𝜃∗), with probability 𝜇(𝑑𝑇
𝑡𝜃∗),
−𝜇(𝑑𝑇
𝑡𝜃∗),with probability 1−𝜇(𝑑𝑇
𝑡𝜃∗).
It is easy to verify that 𝜖𝑡’s are𝑅-sub-Gaussian with 𝑅≤1/2.
Conversation on Key-Terms and Frequency. CRS obtains addi-
tional user feedback through additional key-term conversations.
Similarly, the key-term level feedback on comparing (𝑘𝑡,𝑘′
𝑡)satis-
fies˜𝑜𝑡∼𝐵𝑒𝑟(˜𝑑𝑇
𝑡𝜃∗), where ˜𝑑𝑡=˜𝑥𝑘𝑡,𝑡−˜𝑥𝑘′
𝑡,𝑡. The key-term level
model is presented as
˜𝑜𝑡=𝜇(˜𝑑𝑇
𝑡𝜃∗)+˜𝜖𝑡. (4)
𝑏(𝑡)is introduced to model the frequency of conversations, and we
consider the following function:
𝑞(𝑡)=1, 𝑏(𝑡)−𝑏(𝑡−1)>0,
0,otherwise.
The agent conducts ⌊𝑏(𝑡)−𝑏(𝑡−1)⌋conversations with the user
at round𝑡when𝑞(𝑡)=1and refrains conversations when 𝑞(𝑡)=0.
We also assume the key-term-level conversations are less frequent
than arm-level interactions, ensuring 𝑏(𝑡)≤𝑡for any𝑡to prioritize
users’ experience.
Cumulative Regret. At round𝑡, denote the best arm as 𝑎∗
𝑡, with
𝑎∗
𝑡=arg max𝑎∈A𝑡𝑢(𝑎,𝑡)=arg max𝑎∈A𝑡𝑥𝑇
𝑎,𝑡𝜃∗, and the the cho-
sen arms pair as(𝑎𝑡,𝑎′
𝑡). Following [ 5], the instantaneous dueling
bandit regret is defined as
𝑟𝑡=𝑢(𝑎∗
𝑡,𝑡)−1
2(𝑢(𝑎𝑡,𝑡)+𝑢(𝑎′
𝑡,𝑡))
=𝑥𝑇
𝑎∗
𝑡,𝑡𝜃∗−1
2(𝑥𝑇
𝑎𝑡𝜃∗+𝑥𝑇
𝑎′
𝑡𝜃∗)
The cumulative dueling bandit regret is defined as
𝑅(𝑇)=𝑇∑︁
𝑡=1
𝑢(𝑎∗
𝑡,𝑡)−1
2(𝑢(𝑎𝑡,𝑡)+𝑢(𝑎′
𝑡,𝑡))
=𝑇∑︁
𝑡=1
𝑥𝑇
𝑎∗
𝑡,𝑡𝜃∗−1
2(𝑥𝑇
𝑎𝑡𝜃∗+𝑥𝑇
𝑎′
𝑡𝜃∗)
.(5)
4 Conversational Dueling Bandits with GLM
and Regret Analysis
We propose the Conversational Dueling Bandits algorithm (Con-
Duel) (Alg. 1) to address the three challenges in the conversational
dueling bandit setting: 1) how to exploit the historical feedback
from both arms and key-terms. 2) how to select key-term pairs for
conversation to explore better; 3) how to select arm pairs to mostly
3808KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
minimize the cumulative regret. In § 4.1, we introduce the full algo-
rithm and discuss in detail the highlights of ConDuel addressing
the three challenges above. Meanwhile, theoretical analysis for
ConDuel is provided and a regret upper bounds is showcased in
section § 4.2.
4.1 ConDuel Algorithm
We now detail the proposed ConDuel algorithm as follows:
•Key-term selection module. If the agent conducts a con-
versation with the user based on the previous interactions,
it will select a pair of ’explorative’ key-terms (which will be
discussed in detail later) to query. Then the collected feed-
back from the key-term level will be passed to the full model.
We further assume that 𝑏(𝑡)=𝑏·𝑡in the following analysis
for simplification, where 𝑏∈(0,1).
•Arm-selection module. Based on the previous interaction
history, the parameter 𝜃𝑡is calculated and maintains an
optimistic estimate on the dueling feedback 𝑜𝑡based on the
UCB principle. The agent subsequently constructs a subset
𝐶𝑡, containing promising arms that are likely to be optimal;
Then the system selects a pair of arms from 𝐶𝑡that are most
uncertain to explore the users’ preference thoroughly and
updates the parameters based on the user’s choice.
Based on the arm-level feedback and conversational feedback re-
ceived in the previous rounds, the two modules interact with the
users. The agent utilizes feedback from both modules for recom-
mendations. The main difference between our proposed algorithm
and ConUCB lies in the usage of relative feedback from both the
key-term level and arm level, guaranteed by the introduction of a
maximum likelihood estimator (MLE). To the best of our knowledge,
this is the first non-trivial extension of ConUCB in the generalized
linear model with dueling key-terms module. We present the Con-
Duel algorithm in Algorithm 1. The main body of ConDuel con-
tains a key-term selection module (lines 3-11) and an arm selection
module (lines 14-16). Note that at each iteration 𝑡, we try to main-
tain a tight estimate 𝜃𝑡of the true parameter 𝜃∗utilizing feedback
from both key-term level interactions and arm-level interactions in
our generalized linear model.
4.1.1 Parameter Estimation. Previous CRSs estimate 𝜃∗separately
from arm-level and key-term-level [ 23,24,28], which may cause
waste of observations. In our model, we fully utilize information
from both arm-level feedback and key-term-level feedback to obtain
the MLE of𝜃∗by solving one optimization problem that maximizes
the log-likelihood function. Based on our model (3)and(4), the
regularized MLE of 𝜃∗in our model is given by
𝜃𝑡∈arg max
𝜃∈Θ{𝑡−1∑︁
𝑠=1(𝑜𝑠𝑑𝑇
𝑠𝜃−𝑚(𝑑𝑇
𝑠𝜃))
+𝑡∑︁
𝑠=1∑︁
˜𝑑𝑠∈˜D𝑠(˜𝑜𝑠˜𝑑𝑇
𝑠𝜃−𝑚(˜𝑑𝑇
𝑠𝜃))−𝜆
2∥𝜃∥2
2}.(6)
We denote𝑑𝑡=𝑥𝑎𝑡,𝑡−𝑥𝑎′
𝑡,𝑡as the difference contextual vector for
the chosen arm pair (𝑎𝑡,𝑎′
𝑡), and ˜𝑑𝑡=˜𝑥𝑘𝑡,𝑡−˜𝑥𝑘′
𝑡,𝑡as the difference
contextual vector for the key-term pair (𝑘𝑡,𝑘′
𝑡)being queried at
round𝑡. We also define the D𝑡={𝑑𝑡|𝑑𝑡=𝑥𝑎𝑡,𝑡−𝑥𝑎′
𝑡,𝑡,∀𝑎𝑡,𝑎′
𝑡∈Algorithm 1: The ConDuel Algorithm
Input:(A,K,W),𝑏(𝑡),𝜆,𝜅 1;
1Initialization: 𝑀0=𝜆
𝜅1𝐼;
2for𝑡= 1,...,T do
3 if𝑏(𝑡)−𝑏(𝑡−1)>0then
4𝑞𝑡=𝑏(𝑡)−𝑏(𝑡−1);
5 while𝑞𝑡>0do
6 select a pair of key-terms (𝑘𝑡,𝑘′
𝑡)independently
from barycentric spanner B;
7 Receive relative feedback ˜𝑜𝑡=1(𝑘𝑡>𝑘′
𝑡),
˜𝑑𝑡=˜𝑥𝑘𝑡−˜𝑥𝑘′
𝑡;
8 Update𝑀𝑡=𝑀𝑡−1+˜𝑑𝑡˜𝑑𝑡𝑇;
9 𝑞𝑡=𝑞𝑡−1;
10 end
11 else
12𝑀𝑡=𝑀𝑡−1
13 end
14𝜃𝑡is estimated based on Eq.(7) or Eq. (8), 𝜃(1)
𝑡is
computed according to Eq. (10);
15 Construct
𝐶𝑡={𝑎∈A𝑡|(𝑥𝑎,𝑡−𝑥𝑎′,𝑡)𝑇𝜃(1)
𝑡+𝛼𝑡∥𝑥𝑎,𝑡−𝑥𝑎′,𝑡∥𝑀−1
𝑡>
0,∀𝑎′∈A𝑡};
16 Select the arm pair 𝑎𝑡and𝑎′
𝑡fromC𝑡satisfying:
(𝑎𝑡,𝑎′
𝑡)=arg max𝑎,𝑎′∈C𝑡{∥𝑥𝑎,𝑡−𝑥𝑎′,𝑡∥𝑀−1
𝑡};
17 Receive feedback 𝑜𝑡=1(𝑎𝑡>𝑎′
𝑡),𝑑𝑡=𝑥𝑎𝑡,𝑡−𝑥𝑎′
𝑡,𝑡;
18 Update𝑀𝑡=𝑀𝑡−1+𝑑𝑡𝑑𝑇
𝑡
19end
A𝑡}and ˜D𝑡={˜𝑑𝑡|˜𝑑𝑡=˜𝑥𝑘𝑡,𝑡−˜𝑥𝑘′
𝑡,𝑡,∀𝑘𝑡,𝑘′
𝑡∈K𝑡}at round𝑡. Since
the log-likelihood function is strictly concave in 𝜃, the regularized
MLE𝜃𝑡in our model is the unique solution of the following score
equation upon differentiating:
𝑡−1∑︁
𝑠=1(𝑜𝑠−𝜇(𝑑𝑇
𝑠𝜃))𝑑𝑠+𝑡∑︁
𝑠=1∑︁
˜𝑑𝑠∈˜D𝑠(˜𝑜𝑠−𝜇(˜𝑑𝑇
𝑠𝜃))˜𝑑𝑠−𝜆𝜃=0.(7)
According to Eq. (6)and Eq. (7), we can define the invertible
function𝑔𝑡(𝜃)and the design matrix 𝑀𝑡as
𝑔𝑡(𝜃)=𝑡−1∑︁
𝑠=1𝜇(𝑑𝑇
𝑠𝜃)𝑑𝑠+𝑡∑︁
𝑠=1∑︁
˜𝑑𝑠∈˜D𝑠𝜇(˜𝑑𝑇
𝑠𝜃)˜𝑑𝑠+𝜆𝜃,
𝑀𝑡=𝑡−1∑︁
𝑠=1𝑑𝑠𝑑𝑇
𝑠+𝑡∑︁
𝑠=1∑︁
˜𝑑𝑠∈˜D𝑠˜𝑑𝑠˜𝑑𝑇
𝑠+𝜆/𝜅1𝐼. (8)
In case the MLE 𝜃𝑡is outside of the parameter space Θ, we need
to add a projection step to obtain 𝜃(1)
𝑡by the techniques in [9]:
𝜃(1)
𝑡=arg min
𝜃∈Θ∥𝑔𝑡(𝜃)−𝑔𝑡(𝜃𝑡)∥𝑀−1
𝑡. (9)
Note that when 𝜃𝑡∈Θ, we set𝜃(1)
𝑡=𝜃𝑡to save the computation.
3809Conversational Dueling Bandits in Generalized Linear Models KDD ’24, August 25–29, 2024, Barcelona, Spain
4.1.2 Arm Selection Module. In this part, we will give a detailed
description of arm pair selection. At round 𝑡, for any𝑎𝑡,𝑎′
𝑡∈A𝑡,
our algorithm calculates the UCB estimate on the pairwise feedback:
𝑠(𝑎𝑡,𝑎′
𝑡)=(𝑥𝑎𝑡,𝑡−𝑥𝑎′
𝑡,𝑡)𝑇𝜃(1)
𝑡+𝛼𝑡∥𝑥𝑎𝑡,𝑡−𝑥𝑎′
𝑡,𝑡∥𝑀−1
𝑡,
then the agent constructs a subset 𝐶𝑡which contains all the promis-
ing arms that are superior to the rest of the arms in terms of UCB
estimate. The selected pair of arms (𝑎𝑡,𝑎′
𝑡)satisfies:(𝑎𝑡,𝑎′
𝑡)=
arg max𝑎,𝑎′∈C𝑡∥𝑥𝑎,𝑡−𝑥𝑎′,𝑡∥𝑀−1
𝑡. In this way, we can eliminate arms
that are unlikely to be optimal in the first step, and then select the
maximum informative arm pair. Notice that this arm selection strat-
egy strictly follows [ 18], and when|C𝑡|is large, this step can bring
a lot of computation. Therefore in our experiment, the first arm
𝑎𝑡is randomly sampled from 𝐶𝑡, and the second arm 𝑎′
𝑡is defined
as𝑎′
𝑡=arg max∥𝑥𝑎′
𝑡,𝑡−𝑥𝑎𝑡,𝑡∥𝑀−1
𝑡, which can be seen as the most
uncertain arm to compare with the first arm.
Lemma 1. Assume𝜖𝑡and˜𝜖𝑡defined in Eq. (3)and(4)are conditional
𝑅-sub-Gaussian, 𝑑𝑡is denoted as the difference contextual vectors for
the selected arm pair (𝑥𝑡,𝑥′
𝑡). Then for any 𝑑𝑡∈D𝑡, with probability
at least(1−𝛿), we have the following inequality:
|𝑑𝑇
𝑡𝜃(1)
𝑡−𝑑𝑇
𝑡𝜃∗|≤𝛼𝑡∥𝑑𝑡∥𝑀−1
𝑡, (10)
where𝛼𝑡=2
𝜅1(𝑅√︃
𝑑log((1+4𝜅1(𝑡+𝑏(𝑡))
𝑑𝜆)/𝜎)+√𝜆𝜅1∥𝜃∗∥2).
4.1.3 Key-term Selection Module. In this section, we describe how
the algorithm selects key-term pairs. We hope that the key-term
selection module is explorative, that is, to ask questions on key-
terms that accelerate the learning of user preferences. Especially,
We propose a new strategy for selecting key-term pairs from the
barycentric spanner Bfrom the key-term set K, which aims at
exploring key-term information efficiently.
Definition of Barycentric Spanner. According to [ 3], the subset
B={𝑘1,...,𝑘𝑑}is a barycentric spanner for key-term set K, if
every𝑘∈K can be expressed as a linear combination of elements
ofBusing coefficients in [−1,1], i.e., ˜𝑥𝑘=Í𝑑
𝑖=1𝑐𝑖˜𝑥𝑘𝑖(𝑐𝑖∈[− 1,1]).
We assumeK𝑡spansR𝑑at each round, thus the constructed
barycentric spanner B𝑡forms the basis for R𝑑. In𝑡-th round con-
versation, we sample a pair of key-terms 𝑘1,𝑘2∼B𝑡independently
from the barycentric spanner to obtain relative feedback. This is
efficient in computation because reducing the number of key-terms
can bring a lot of convenience. Based on the definition of barycen-
tric spanner, all information of K𝑡can be seen contained in the
barycentric spanner, therefore exploring B𝑡is sufficient in collect-
ing user feedback. Furthermore, this strategy can also guarantee
some good properties in our algorithm, ensuring a high probability
lower bound of 𝜆min(𝑀𝑡)as follows:
Lemma 2. LetΣ=𝐸𝑥,𝑦∼B[(𝑥−𝑦)(𝑥−𝑦)𝑇], and𝜆𝐵=𝜆min(Σ).
As the conversation frequency 𝑏(𝑡)≤𝑡, we assume that 𝑏(𝑡)=𝑏·𝑡
for some𝑏∈(0,1). Then when 𝑡≥4(𝐶1√
𝑑+𝐶2√
log(1/𝛿))2
𝑏𝜆2
𝐵≜𝑡0, with
probability at least (1−𝛿), we have
𝜆min(𝑀𝑡)≥𝜆𝐵𝑏𝑡
2+𝜆
𝜅1, (11)
with𝐶1and𝐶2being constants.Lemma 3. With key-term pair independently sampled from barycen-
tric spannerB𝑡at each round, and 𝑡0is defined in Lemma 2, then
∀𝑡>𝑡0, we have the following inequality:
𝑡∑︁
𝑠=𝑡0+1∥𝑑𝑠∥𝑀−1𝑠≤8(√︂𝑡
2𝑏𝜆𝐵−√︂𝑡0
2𝑏𝜆𝐵)≤8√︂𝑡
2𝑏𝜆𝐵.
Note that the above lemma uses a different technique to prove the
upper bound forÍ
𝑡∥𝑑𝑡∥𝑀−1
𝑡, and as the conversation frequency
𝑏increases, the regret upper bound decreases accordingly. This
tendency corresponds to our understanding of the conversation
system: the more questions the agent asks, the more feedback it can
leverage, thus the more accurately it can learn the user preferences.
Remark. Notice that the key-term selection module is regret-free,
we don’t need to consider explore-exploit trade-offs and apply the
UCB principle here. Instead, utilizing an explorative strategy such
as choosing key-terms from the barycentric spanner can improve
the performance of the algorithm as well as save computation. In
the future, it would be interesting to investigate other explorative
strategies from best arm identification literature, such as works in
[2] and [11].
4.2 Regret Upper Bound
We give the upper bound of the cumulative dueling regret 𝑅(𝑇)for
our algorithm as follows, where we assume 𝑏(𝑡)=𝑏·𝑡for some
𝑏∈(0,1)from Lemma 2.
Theorem 1. With probability (1- 𝛿), our algorithm has the following
regret upper bound:
𝑅(𝑇)≤𝑡0+32
𝜅1(𝑅√︂
𝑑log((1+4𝜅1(𝑇+𝑏𝑇)
𝑑𝜆)/𝜎)+
+√︁
𝜆𝜅1∥𝜃∗∥2)√︄
𝑇
2𝑏𝜆𝐵=O(1
𝜅1√︁
𝑑𝑇log(𝑇)) (12)
It can be seen that the upper bound of 𝑅(𝑇)decreases as 𝑏in-
creases. As far as we know, this is the first work in a conversational
recommender system that directly shows the impact of conversa-
tions and proves an explainable regret upper bound concerning
conversation frequency. While there’s no direct lower bound of the
conversational dueling bandit problem, [ 18] contained the regret
lower bounds of contextual dueling bandits that can also match
with ours, i.e., Ω(√
𝑑𝑇).
5 Experiments
In this section, we describe experimental results on both synthetic
data and real-world data to validate our proposed algorithm. The
code is available at https://github.com/shuashua0608/Con-Duel.
The arm-level and key-term level pairwise rewards are generated
according to Eq. (3)and Eq. (4), and the barycentric spanner Bis
computed in advance following [ 3]. Specifically, we define the link
function𝜇as the sigmoid function, thus leading to a logistic dueling
bandit model. It should be noted that our algorithm can also be
applied to other generalized linear model scenarios.
5.1 Implementation Details
Baselines. We select the following algorithms as baselines to com-
pare with ours:
3810KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
Figure 2: Cumulative regret on synthetic and real-world datasets
•Random-opt: A variant of MaxInp in [ 18], selecting two
arms randomly from the constructed set 𝐶𝑡without conver-
sation from key-term level. This algorithm compares with
MaxInp and shows the necessity of computing the "maxi-
mum informative pair" principle.
•MaxInp [18]: A recently introduced algorithm designed for
the contextual dueling bandits setting in GLM without a
conversation mechanism.
•ConDuel-Random: A variant of our algorithm that follows
the same arm-pair selection principle but selects key-term
pairs randomly.
•ConDuel-MaxInp: A variant of our algorithm that fol-
lows the same arm selection principle but selects key-term
pairs with maximum information, that is, to choose 𝑘,𝑘′=
arg max𝑘,𝑘′∈K∥˜𝑥𝑘−˜𝑥′
𝑘∥𝑀−1
𝑡at𝑡-th conversation.
Additionally, we compare our algorithms with RelativeConUCB
from [ 24], namely, RelativeConUCB-Pos&Neg and RelativeConUCB-
Difference, which utilize relative feedback from key-term selection
module in CRS. It should be noted that RelativeConUCB is designed
for linear bandits and assumes that the arm-level model is a click
model. For a fair comparison, we adapt their arm-level reward esti-
mated from𝑟𝑎∼𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝑥𝑇𝑎𝜃∗)to𝑟𝑎∼𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑥𝑇𝑎𝜃∗))
for𝑎∈A and update the regret as 𝑅𝑇=Í𝑇
𝑠=1((𝑥𝑇
𝑎∗
𝑡𝜃∗−(𝑥𝑇𝑎𝑡𝜃∗))
to fit in our problem setting. We rewrite them as Rconucb-PosNeg
and Rconucb-Diff, and give a general description of their key-term
selection strategy:
•Rconucb-PosNeg: Utilize relative feedback from the key-
term level as two observations of absolute feedback: a pos-
itive observation of feedback 1 for the preferred key-term
˜𝑥𝑘1and a negative observation with feedback 0 for the less
preferred key-term ˜𝑥𝑘2;
•Rconucb-Diff : Incorporate the relative feedback as a single
observation(˜𝑥𝑘1−˜𝑥𝑘2,1).
Metrics. We use the cumulative dueling regret from Eq. (5)to mea-
sure the performance of the algorithms, unless otherwise stated.
Additionally, we plot the standard error for each algorithm to vali-
date the stability of our proposed algorithm. We sequentially run
the experiments ten times per user for each dataset and calculate
the average cumulative regret for each algorithm.5.2 Synthetic Data
Data Generation. We construct the synthetic data following [ 28]
and [ 24]. First, we create a user set Uwith|U|= 200, a key-term
setKwith|K|= 500 and an arm set Awith|A|= 5000, with the di-
mension of feature space to be 𝑑=10. We generate each element in
user preference vector 𝜃∗𝑢and arm feature vector 𝑥𝑎independently
from the standard normal distribution 𝑁(0,1). Without loss of gen-
erosity, we normalize ∥𝜃∗𝑢∥2=1and∥𝑥𝑎∥2=1. To construct the
weight matrix 𝑊=[𝑤𝑎,𝑘], we follow similar procedures in [ 22]: (1)
We select an integer 𝑛𝑘uniformly at random from [1, 𝑀], and select
a subset of𝑛𝑘armsA𝑘to be related with key-term 𝑘.𝑀is set to
be 10 in the experiments; (2) We assume each arm 𝑎is related with
𝑛𝑎key-terms subsetK𝑎with equal weight 𝑤𝑎,𝑘=1/𝑛𝑎,∀𝑘∈K𝑎.
In the simulation, we set the time horizon 𝑇= 5000, conversation
frequency𝑏(𝑡)=10⌊𝑡
50⌋and pool size|𝐴𝑡|= 50, unless otherwise
stated.
Experimental results. The cumulative dueling regret curve with
the standard error plot is shown in Figure 2. Our proposed algo-
rithm ConDuel as well as its variants achieves better performances
than MaxInp and Random-opt, realizing smaller regret and stan-
dard error. ConDuel and ConDuel-MaxInp perform slightly better
than ConDuel-Random, indicating that carrying out explorative
conversations can help reduce cumulative regret. Additionally, Rel-
ativeConUCB performs the worst compared with algorithms with
nonlinear reward assumptions due to the less practical linear model
assumption in the RelativeConUCB algorithm, which does not fit
in with our experimental setting.
Impact of conversation frequency and data dimension. We
next study the impact of different conversation frequencies and
data dimensions. For the impact of conversation frequencies, since
key-term-level conversations are less frequent than arm-level in-
teractions, we consider the linear function: 𝑏(𝑡)=𝑛⌊𝑡/50⌋, which
means asking 𝑛questions per 50 iterations, as well as the log func-
tion:𝑏(𝑡)=𝑛⌊log𝑡⌋. We vary the value of 𝑛to be 1, 5, 10, and 20 for
both functions. The cumulative regrets of each conversation type
and pool size are shown in Figure 3. According to Figure 3, more
conversations can help reduce cumulative regrets more, for example,
cumulative regret is the largest when 𝑏(𝑡)=⌊𝑡/50⌋and smallest
when𝑏(𝑡)=20⌊𝑡/50⌋. When the conversation frequency increases,
our proposed algorithms utilizing explorative key-term strategy
(namely, ConDuel and ConDuel-MaxInp) demonstrate more advan-
tages than other algorithms. To test the impact of data dimensions
3811Conversational Dueling Bandits in Generalized Linear Models KDD ’24, August 25–29, 2024, Barcelona, Spain
b(t)=n*⌊log t⌋ b(t)=n*⌊t/50⌋ dimension = n
Figure 3: Ablation study on synthetic data
on our algorithms and validate the realizability of the proposed
ConDuel in a higher dimension setting, we generate synthetic data
of different dimensions, the data dimensions are set to be 20, 30,
40, and 50 with 𝑏(𝑡)=10⌊𝑡/50⌋. As is shown in Figure 3, as data
dimension increases, the cumulative regret naturally increases, yet
our proposed ConDuel algorithm with its variants still maintains
superiority over other algorithms.
5.3 Real-world Datasets
Data Generation. We next display the experimental results on
two real-world datasets, Last.FM and Movielens, which are re-
leased in [ 6]. Last.FM is a dataset for music artist recommenda-
tions that contains 186,479 interaction records between 1,892 users
and 17,632 artists. Movielens is a dataset that extends the original
MovieLens10M dataset to include tagging information in IMDb and
Rotten Tomatoes for movie recommendations and contains 47,957
interaction records between 2,113 users and 10,197 movies.
We prepossess the data following [ 24] and treat artists and
movies as items, we then infer users’ real feedback on items based
on the interaction records: if the user has assigned attributes to the
item, the feedback is 1, otherwise, the feedback is missing. For both
datasets, we extract |A|= 2,000 with the most assigned attributes
by users and 𝑁𝑢= 100 users who have assigned the most attributes.
For each arm, we keep at most 20 attributes that are related to
most items and treat them as the related key-terms of the item. All
the kept key-terms associated with the arms form the key-term
setK—the number of key-terms for Last.FM is 2,726 and that for
Movielens is 5,585. The weights of all key-terms related to the same
arm are set to be equal, and we set the feature vectors to be 𝑑=10
to save the computation complexity.
Results. We compare the performance of algorithms in the two
real datasets. We run the experiments 10 times and calculate all
users’ average regret over 𝑇= 10,000 rounds on the fixed generated
datasets. We set 𝑏(𝑡)=10⌊𝑡
50⌋and|𝐴𝑡|= 50. The evaluation results
are shown in Figure 2. It can be seen from the figures that on
both datasets, the ConDuel-MaxInp algorithm achieves the best
performance in terms of cumulative regret and standard error, and
the regret of ConDuel is also slightly lower compared with ConDuel-
Random. In both cases, the ConDuel algorithms with explorative
key-term selection strategy show their strengths compared to other
algorithms.6 Extension to MNL Bandit
Besides pairwise comparison for conversational dueling bandits,
we can also extend the conversational mechanism to the multiple
comparison setting under the choice model, also known as the
Multinomial Logit Bandit (MNL) problem [ 15,16]. The arm setA
of size𝑁and key-term setKof size𝐾are defined in section 3. We
also defineCto be the set of candidate assortments with size less
than𝑞, i.e.C={𝐶⊂[𝑁]:|𝐶|≤𝑞}, where𝑞≥2. In each iteration
𝑡, for the arm-level selection, the agent is offered an assortment
𝐶𝑡={𝑎𝑖1,...,𝑎𝑖𝑡}⊂C and observes feature vector 𝑥𝑎,𝑡for each
𝑎∈𝐶𝑡. The user purchase decision 𝑜𝑡∈𝐶𝑡Ð{0}is observed,
and we can denote the user purchase decision for each 𝑎∈𝐶𝑡
as𝑜𝑎,𝑡=1(𝑎is chosen)∈{ 0,1}and𝑜0,𝑡indicating not choosing
from the item set. Similarly, at 𝑡-th key-term level selection, the
user observes key-term subset 𝐾𝑡with|𝐾𝑡|≤𝑞and gives certain
feedback ˜𝑜𝑡, with ˜𝑜𝑘,𝑡indicating whether key-term 𝑘is chosen and
˜𝑜0,𝑡representing not choosing.
We define the ConMNL algorithm in Algorithm 2. The user
selection for item 𝑎∈𝐶𝑡and key-term 𝑘∈𝐾𝑡at round𝑡is given
by the MNL choice model, defined in the following equations:
𝑝𝑖(𝐶𝑡,𝜃∗)= 
exp(𝑥𝑇
𝑖,𝑡𝜃∗)
1+Í
𝑗∈𝐶𝑡exp(𝑥𝑇
𝑗,𝑡𝜃∗), 𝑖∈𝐶𝑡,
1
1+Í
𝑗∈𝐶𝑡exp(𝑥𝑇
𝑗,𝑡𝜃∗), 𝑖=0,
˜𝑝𝑡(𝐾𝑡,𝜃∗)= 
exp(˜𝑥𝑇
𝑘,𝑡𝜃∗)
1+Í
𝑗∈𝐾𝑡exp(˜𝑥𝑇
𝑗,𝑡𝜃∗), 𝑘∈𝐾𝑡,
1
1+Í
𝑗∈𝐾𝑡exp(˜𝑥𝑇
𝑗,𝑡𝜃∗), 𝑘=0.
Here𝜃∗∈R. Notice that when 𝑞=2, the choice model can be
seen as the dueling bandit model. We can rewrite the arm-level and
key-term level choice model as follows:
𝑜𝑎,𝑡=𝑝𝑎(𝐶𝑡,𝜃∗)+𝜖𝑎,𝑡 (13)
˜𝑜𝑘,𝑡=˜𝑝𝑘(𝐾𝑡,𝜃∗)+˜𝜖𝑘,𝑡 (14)
It is easy to verify that the noise 𝜖𝑎,𝑡and ˜𝜖𝑘,𝑡are𝜎2sub-Gaussian
variable with 𝜎=0.5. We assume that and the expected revenue
of the assortment 𝐶𝑡to be𝑅𝑡(𝐶𝑡,𝜃∗)=Í
𝑗∈𝐶𝑡𝑟𝑗,𝑡𝑝𝑗(𝐶𝑡,𝜃∗), where
𝑟𝑗,𝑡is the revenue from the recommendation if item 𝑖is chosen by
3812KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
Algorithm 2: The ConMNL Algorithm
Input:(A,K,W),𝑏(𝑡),𝑞, initialization 𝑇0;
1Initialization: for𝑡∈[𝑇0]do
2 if𝑏(𝑡)−𝑏(𝑡−1)>0then
3𝑞𝑡=𝑏(𝑡)−𝑏(𝑡−1);
4 while𝑞𝑡>0do
5 Randomly select 𝑞key-terms from Barycentric
SpannerB;
6 Update𝑀𝑡=𝑀𝑡−1+Í
𝑘∈B ˜𝑥𝑘,𝑡˜𝑥𝑘,𝑡𝑇;
7 𝑞𝑡=𝑞𝑡−1;
8 end
9 else
10𝑀𝑡=𝑀𝑡−1
11 end
12 Randomly choose 𝐶𝑡∈Cwith|𝐶𝑡|=𝑞;
13 Update𝑀𝑡=𝑀𝑡−1+Í
𝑖∈𝐶𝑡𝑥𝑖,𝑡𝑥𝑇
𝑖,𝑡
14end
15for𝑡=𝑇0+1,...,𝑇 do
16 if𝑏(𝑡)−𝑏(𝑡−1)>0then
17𝑞𝑡=𝑏(𝑡)−𝑏(𝑡−1);
18 while𝑞𝑡>0do
19 Offer𝐾𝑡based on key-term selection principle,
and observe key-term level feedback ˜𝑜𝑡;
20 Update𝑀𝑡=𝑀𝑡−1+Í
𝑘∈𝐾𝑡˜𝑥𝑘,𝑡˜𝑥𝑘,𝑡𝑇;
21 𝑞𝑡=𝑞𝑡−1;
22 end
23 else
24𝑀𝑡=𝑀𝑡−1
25 end
26 MLE𝜃𝑡is estimated according to the regularized
log-likelihood function in Eq 17;
27 Offer𝐶𝑡=arg max𝐶∈Cˆ𝑅𝑡(𝐶,𝜃𝑡)to the user and
observe user choice 𝑜𝑡;
28 Update𝑀𝑡=𝑀𝑡−1+Í
𝑖∈𝐶𝑡𝑥𝑖,𝑡𝑥𝑇
𝑖,𝑡
29end
user at round 𝑡; and the optimal assortment 𝐶∗
𝑡=arg max𝐶∈C𝑅𝑡(𝐶,𝜃∗).
The cumulative expected regret over time 𝑇is defined as:
𝑅(𝑇)=𝑇∑︁
𝑡=1©­
«∑︁
𝑗∈𝐶∗
𝑡𝑟𝑗,𝑡𝑝𝑗(𝐶∗
𝑡,𝜃∗)−∑︁
𝑗∈𝐶𝑡𝑟𝑗,𝑡𝑝𝑗(𝐶𝑡,𝜃∗)ª®
¬
=𝑇∑︁
𝑡=1 𝑅𝑡(𝐶∗
𝑡,𝜃∗)−𝑅𝑡(𝐶𝑡,𝜃∗). (15)
Following the definition of Barycentric Spanner Bof key-term set
in sec § 4.1.3, we assume 𝜆′
B=𝜆min
𝐸𝑘∼B[˜𝑥𝑘˜𝑥𝑇
𝑘]
>0. Based on
[9, 14] and [15], we also make the following assumptions:
Assumption 3. For every𝑡and𝑖∈𝐶, there exists a constant 𝜅2>0,
where𝜅2:=min∥𝜃−𝜃∗∥≤1,|𝐶|≤𝑞𝑝𝑖(𝐶,𝜃)𝑝0(𝐶,𝜃).
Assumption 4. Each feature vector 𝑥𝑖,𝑡,∥𝑥𝑖,𝑡∥≤1and there exists
a constant𝜎0>0, with𝐸[𝑥𝑇
𝑖,𝑡𝑥𝑖,𝑡]≥𝜎0.We estimate parameter 𝜃∗following similar procedures in sec § 4.1.
The log-likelihood function till 𝑡-th round under parameter 𝜃is
given by
𝐿𝑡(𝜃)=𝑡−1∑︁
𝜏=1∑︁
𝑖∈𝐶𝜏𝑜𝑠,𝑖𝑙𝑜𝑔(𝜇𝑖(𝐶𝜏,𝜃))+𝑡∑︁
𝜏=1∑︁
𝑘∈𝐾𝜏˜𝑜𝜏,𝑘𝑙𝑜𝑔(˜𝜇𝑘(𝐾𝜏,𝜃).
(16)
Setting∇𝜃𝐿𝑡(𝜃)=0, the maximum likelihood estimation 𝜃𝑡is the
solution of:
𝑡−1∑︁
𝜏=1∑︁
𝑖∈𝐶𝜏(𝑜𝑖,𝑠−𝜇𝑖(𝐶𝜏,𝜃))𝑥𝑖,𝑠+𝑡∑︁
𝜏=1∑︁
𝑘∈𝐾𝜏(˜𝑜𝑘,𝜏−˜𝜇𝑘(𝐾𝜏,𝜃))˜𝑥𝑘,𝜏=0.
(17)
Define𝑀𝑡=Í𝑡−1
𝜏=1Í
𝑖∈𝐶𝜏𝑥𝑖,𝜏𝑥𝑇
𝑖,𝜏+Í𝑡
𝜏=1Í
𝑘∈𝐾𝜏˜𝑥𝑘,𝜏˜𝑥𝑇
𝑘,𝜏. We can
calculate the MLE 𝜃𝑡to obtain the UCB estimate 𝑧𝑎,𝑡=𝑥𝑇
𝑎,𝑡𝜃𝑡+
𝛼𝑡∥𝑥𝑎,𝑡∥𝑀−1
𝑡regarding the utility of each 𝑎∈A𝑡at time𝑡, with𝛼𝑡=
1
2𝜅2√︃
2𝑑log(1+𝑏(𝑡)+𝑡
𝑑)+log(1
𝛿). In the item selection module, we
construct the optimal estimate of the expected revenue by choosing
𝐶as
ˆ𝑅𝑡(𝐶)=Í
𝑖∈𝐶𝑢(𝑖,𝑡)exp(𝑧𝑖,𝑡)
1+Í
𝑖∈𝐶exp(𝑧𝑖,𝑡), (18)
and offer𝐶𝑡=arg max𝐶∈Cˆ𝑅𝑡(𝐶)to the user at time 𝑡. For the key-
term query module, we choose 𝑞key-terms with each uniformly
sampled from barycentric spanner Bto formK𝑡.
The regret upper bound of ConMNL is given in Theorem 2.
Theorem 2. Assume conversation frequency to be 𝑏(𝑡)=𝑏·𝑡, with
𝑏∈ (0,1), and𝛼𝑡=1
2𝜅2√︃
2𝑑log(1+𝑏(𝑡)+𝑡
𝑑)+2 log(𝑡), and𝑇0=
O
𝑚𝑎𝑥{1
𝜅2
2(𝑑log(𝑏(𝑇)+𝑇
𝑑+4 log(𝑇)),𝑞/𝜎2,256
𝑏𝑞𝜆′
Blog(128𝑑
𝛿𝜆2
B′)}
. The
expected regret of ConMNL is upper bound by 𝑅𝑇=O(√︁
𝑑𝑇𝑞log(𝑇)).
6.1 Experiments
We compare our ConMNL algorithm with the following baselines
on the previous three datasets:
•UCB-MNL. An algorithm designed for MNL contextual ban-
dit in [15] with no conversation on key-terms.
•ConMNL-ucb: A variant of ConMNL that selects 𝑞key-
terms at each conversation based on UCB estimate ˜𝑥𝑇
𝑘,𝑡𝜃𝑡+
𝛼𝑡∥˜𝑥𝑘,𝑡∥𝑀−1
𝑡.
•ConMNL-random: A variant of ConMNL that selects 𝑞
key-terms randomly at each conversation.
We set horizon 𝑇=2000 and𝑞=4for each dataset, allowing
at most𝑞items and key-terms for the user to choose from. We
also assume the expected revenue of choosing item 𝑖is given by
𝑟𝑖,𝑡=𝑥𝑇
𝑖,𝑡𝜃∗, the utility of each item. The conversation frequency
𝑏(𝑡)is set to be 5⌊𝑡
50⌋, and pool size|𝐴𝑡|= 50. It should be noted
that the ConMNL-ucb and ConMNL-random follow the same item
assortment selection principle proposed in the ConMNL algorithm,
aiming to compare the impact of different conversation mechanisms
on the performance of recommender systems. We ran experiments
on each user 10 times and calculated the average regret as well as
standard error. The regret curve for each dataset is shown in Fig 4.
3813Conversational Dueling Bandits in Generalized Linear Models KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 4: Cumulative regret on synthetic and real-world datasets
It can be seen from the figures that our algorithm along with its
variants all perform better than UCB-MNL on each dataset, con-
firming that carrying out conversations on key-terms can enhance
the model performance. Furthermore, ConMNL performs the best
on Last.FM dataset, while ConMNL-ucb achieves relatively bet-
ter results on both synthetic dataset and Movielens dataset. This
may be due to the complexity of constructing a 𝑞-size choice set,
where utilizing information solely from the barycentric spanner
subset from the key-term set may be inadequate to capture the user
preference.
7 Conclusion
In this paper, we study a novel framework of conversational bandits
with an informative feedback mechanism in the generalized linear
models and propose the ConDuel algorithm that can guarantee
relative feedback from both key-term and item modules. We design
new methods to effectively duel key-term pairs and item pairs in our
algorithm, which allow the system to conduct exploratory conver-sations to utilize key-term pairwise feedback in the key-term mod-
ule. Meanwhile, we select the most informative pairs in the item
module to grasp the user preferences more accurately. We prove a re-
gret upper bound of O(√︁
𝑑𝑇log(𝑇))of our algorithm, and extensive
experiments on both synthetic data and real-world datasets have
demonstrated the competitiveness of our algorithm. We also extend
our algorithm to multiple comparisons under the MNL choice model
and propose the ConMNL algorithm with a theoretical guarantee.
For future research, it would be intriguing to consider: 1) Incorpo-
rate additional structure for key-term module, such as knowledge
graph [ 29] or clustering [ 23]; 2) Consider different distributions
on key-terms and higher dimensions of large-scale dataset in real
dataset experiments.
Acknowledgments
Mengdi Wang acknowledges the support by NSF grants DMS-
1953686, IIS-2107304, CMMI-1653435, ONR grant 1006977, and
C3.AI.
3814KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
References
[1]Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. 2011. Improved al-
gorithms for linear stochastic bandits. In Proceedings of the 24th International
Conference on Neural Information Processing Systems (Granada, Spain) (NIPS’11).
Curran Associates Inc., Red Hook, NY, USA, 2312–2320.
[2]Jean-Yves Audibert and Sébastien Bubeck. 2010. Best Arm Identification in Multi-
Armed Bandits. In COLT - 23th Conference on Learning Theory - 2010. Haifa, Israel,
13 p. https://enpc.hal.science/hal-00654404
[3]Baruch Awerbuch and Robert D. Kleinberg. 2004. Adaptive routing with end-to-
end feedback: distributed learning and geometric approaches. In Proceedings of
the Thirty-Sixth Annual ACM Symposium on Theory of Computing (Chicago, IL,
USA) (STOC ’04) . Association for Computing Machinery, New York, NY, USA,
45–53. https://doi.org/10.1145/1007352.1007367
[4]Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier.
2021. Preference-based online learning with dueling bandits: A survey. The
Journal of Machine Learning Research 22, 1 (2021), 278–385.
[5]Viktor Bengs, Aadirupa Saha, and Eyke Hüllermeier. 2022. Stochastic Contextual
Dueling Bandits under Linear Stochastic Transitivity Models. In Proceedings of
the 39th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 1764–1786. https:
//proceedings.mlr.press/v162/bengs22a.html
[6]Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. Second workshop on
information heterogeneity and fusion in recommender systems (HetRec2011).
InProceedings of the Fifth ACM Conference on Recommender Systems (Chicago,
Illinois, USA) (RecSys ’11). Association for Computing Machinery, New York, NY,
USA, 387–388. https://doi.org/10.1145/2043932.2044016
[7]Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards
Conversational Recommender Systems. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (San Francisco,
California, USA) (KDD ’16). Association for Computing Machinery, New York,
NY, USA, 815–824. https://doi.org/10.1145/2939672.2939746
[8]Miroslav Dudík, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and
Masrour Zoghi. 2015. Contextual Dueling Bandits. In Proceedings of The 28th
Conference on Learning Theory (Proceedings of Machine Learning Research, Vol. 40),
Peter Grünwald, Elad Hazan, and Satyen Kale (Eds.). PMLR, Paris, France, 563–587.
https://proceedings.mlr.press/v40/Dudik15.html
[9]Sarah Filippi, Olivier Cappé, Aurélien Garivier, and Csaba Szepesvári. 2010. Para-
metric bandits: the Generalized Linear case. In Proceedings of the 23rd Interna-
tional Conference on Neural Information Processing Systems - Volume 1 (Vancouver,
British Columbia, Canada) (NIPS’10). Curran Associates Inc., Red Hook, NY, USA,
586–594.
[10] Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, and Mengdi Wang. 2023.
Provable Benefits of Policy Learning from Human Preferences in Contextual
Bandit Problems. arXiv preprint arXiv:2307.12975 (2023).
[11] Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad
Ghavamzadeh, and Craig Boutilier. 2020. Randomized Exploration in Gener-
alized Linear Bandits. In Proceedings of the Twenty Third International Confer-
ence on Artificial Intelligence and Statistics (Proceedings of Machine Learning
Research, Vol. 108), Silvia Chiappa and Roberto Calandra (Eds.). PMLR, 2066–2076.
https://proceedings.mlr.press/v108/kveton20a.html
[12] Lihong Li, Wei Chu, John Langford, Taesup Moon, and Xuanhui Wang. 2012.
An Unbiased Offline Evaluation of Contextual Bandit Algorithms with Gen-
eralized Linear Models. In Proceedings of the Workshop on On-line Trading of
Exploration and Exploitation 2 (Proceedings of Machine Learning Research, Vol. 26),
Dorota Glowacka, Louis Dorard, and John Shawe-Taylor (Eds.). PMLR, Bellevue,
Washington, USA, 19–36. https://proceedings.mlr.press/v26/li12a.html
[13] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-
bandit approach to personalized news article recommendation. In Proceedings of
the 19th International Conference on World Wide Web (Raleigh, North Carolina,
USA) (WWW ’10). Association for Computing Machinery, New York, NY, USA,
661–670. https://doi.org/10.1145/1772690.1772758
[14] Lihong Li, Yu Lu, and Dengyong Zhou. 2017. Provably optimal algorithms for
generalized linear contextual bandits. In Proceedings of the 34th International
Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICML’17).
JMLR.org, 2071–2080.
[15] Min-hwan Oh and Garud Iyengar. 2021. Multinomial logit contextual bandits:
Provable optimality and practicality. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 35. 9205–9213.
[16] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. 2018. Multinomial logit
bandit with linear utility functions. arXiv preprint arXiv:1805.02971 (2018).
[17] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022.
Training language models to follow instructions with human feedback. Advances
in Neural Information Processing Systems 35 (2022), 27730–27744.[18] Aadirupa Saha. 2021. Optimal Algorithms for Stochastic Contextual Preference
Bandits. Advances in Neural Information Processing Systems 34 (2021), 30050–
30062.
[19] Aadirupa Saha and Akshay Krishnamurthy. 2022. Efficient and Optimal Algo-
rithms for Contextual Dueling Bandits under Realizability. In Proceedings of The
33rd International Conference on Algorithmic Learning Theory (Proceedings of
Machine Learning Research, Vol. 167), Sanjoy Dasgupta and Nika Haghtalab (Eds.).
PMLR, 968–994. https://proceedings.mlr.press/v167/saha22a.html
[20] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. 2023. Dueling RL: Reinforce-
ment Learning with Trajectory Preferences. In Proceedings of The 26th Interna-
tional Conference on Artificial Intelligence and Statistics (Proceedings of Machine
Learning Research, Vol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem van de
Meent (Eds.). PMLR, 6263–6289. https://proceedings.mlr.press/v206/saha23a.html
[21] Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. 2024. Contextual
bandits and imitation learning with preference-based active queries. In Proceed-
ings of the 37th International Conference on Neural Information Processing Systems
(New Orleans, LA, USA) (NIPS ’23). Curran Associates Inc., Red Hook, NY, USA,
Article 499, 35 pages.
[22] Zhiyong Wang, Xutong Liu, Shuai Li, and John C.S. Lui. 2023. Efficient ex-
plorative key-term selection strategies for conversational contextual bandits.
InProceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence
and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence
and Thirteenth Symposium on Educational Advances in Artificial Intelligence
(AAAI’23/IAAI’23/EAAI’23). AAAI Press, Article 1156, 8 pages. https://doi.org/
10.1609/aaai.v37i8.26225
[23] Junda Wu, Canzhe Zhao, Tong Yu, Jingyang Li, and Shuai Li. 2021. Cluster-
ing of Conversational Bandits for User Preference Learning and Elicitation. In
Proceedings of the 30th ACM International Conference on Information & Knowl-
edge Management. Association for Computing Machinery, New York, NY, USA,
2129–2139.
[24] Zhihui Xie, Tong Yu, Canzhe Zhao, and Shuai Li. 2021. Comparison-based Con-
versational Recommender System with Relative Bandit Feedback. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval. Association for Computing Machinery, New York, NY,
USA, 1400–1409.
[25] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. 2020.
Preference-based reinforcement learning with finite-time guarantees. Advances
in Neural Information Processing Systems 33 (2020), 18784–18794.
[26] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The
k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538–1556.
[27] Yisong Yue and Thorsten Joachims. 2011. Beat the mean bandit. In Proceedings of
the 28th International Conference on International Conference on Machine Learning
(Bellevue, Washington, USA) (ICML’11). Omnipress, Madison, WI, USA, 241–248.
[28] Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. 2020. Conversational
contextual bandit: Algorithm and application. In Proceedings of the web conference
2020. Association for Computing Machinery, New York, NY, USA, 662–672.
[29] Canzhe Zhao, Tong Yu, Zhihui Xie, and Shuai Li. 2022. Knowledge-aware Conver-
sational Preference Elicitation with Bandit Feedback. In Proceedings of the ACM
Web Conference 2022 (Lyon, France) (WWW ’22). Association for Computing Ma-
chinery, New York, NY, USA, 483–492. https://doi.org/10.1145/3485447.3512152
[30] Banghua Zhu, Michael Jordan, and Jiantao Jiao. 2023. Principled Reinforce-
ment Learning with Human Feedback from Pairwise or K-wise Comparisons. In
Proceedings of the 40th International Conference on Machine Learning (Proceed-
ings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.).
PMLR, 43037–43067. https://proceedings.mlr.press/v202/zhu23f.html
[31] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. 2014. Relative
Upper Confidence Bound for the K-Armed Dueling Bandit Problem. In Proceedings
of the 31st International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 32), Eric P. Xing and Tony Jebara (Eds.). PMLR, Bejing,
China, 10–18. https://proceedings.mlr.press/v32/zoghi14.html
A Proofs for ConDuel Algorithm
Proof of Lemma 1. According to Eq. (7)and Eq. (8)and the
definition of 𝑔𝑡(𝜃),∀𝜃1,𝜃2∈Θ, we have:
𝑔𝑡(𝜃1)−𝑔𝑡(𝜃2)=(𝐺𝑎
𝑡(𝜃1,𝜃2)+𝐺𝑘
𝑡(𝜃1,𝜃2)+𝜆𝐼)(𝜃1−𝜃2)
≜𝐺𝑡(𝜃1,𝜃2)(𝜃1−𝜃2)>𝜅1𝑀𝑡(𝜃1−𝜃2), (19)
where𝐺𝑎
𝑡(𝜃1,𝜃2)=Í𝑡−1
𝑠=1𝜇′(𝑑𝑇𝑠¯𝜃(𝜃1,𝜃2))𝑑𝑠𝑑𝑇𝑠, and𝐺𝑘
𝑡(𝜃1,𝜃2)=Í𝑡
𝑠=1Í
˜𝑑∈˜D𝑠𝜇′(˜𝑑𝑇𝑠¯𝜃(𝜃1,𝜃2)˜𝑑𝑠˜𝑑𝑇𝑠.
Based on the mean value theorem, we also have ¯𝜃(𝜃1,𝜃2)=
𝑣𝜃1+(1−𝑣)𝜃2. According to Eq. (3)and Eq. (4), we have the equality:
3815Conversational Dueling Bandits in Generalized Linear Models KDD ’24, August 25–29, 2024, Barcelona, Spain
𝑔𝑡(𝜃𝑡)−𝑔𝑡(𝜃∗)=𝑡−1∑︁
𝑠=1𝜖𝑠𝑑𝑠+𝑡∑︁
𝑠=1∑︁
˜𝑑∈˜D𝑠˜𝜖𝑠˜𝑑𝑠−𝜆𝜃∗
≜𝑆𝑡−𝜆𝜃∗>𝜅1𝑀𝑡(𝜃𝑡−𝜃∗) (20)
Combined with Eq. (7), (8) and (9), ∀𝑥∈R𝑑, we thus have:
|𝑥𝑇(𝜃(1)
𝑡−𝜃∗)|=|𝑥𝑇𝐺−1
𝑡(𝜃(1)
𝑡,𝜃∗)(𝑔𝑡(𝜃(1)
𝑡)−𝑔𝑡(𝜃∗))|
≤∥𝑥∥𝐺−1
𝑡(𝜃(1)
𝑡,𝜃∗)∥𝑔𝑡(𝜃(1)
𝑡)−𝑔𝑡(𝜃∗)∥𝐺−1
𝑡(𝜃(1)
𝑡,𝜃∗)
≤(1)1
𝜅1∥𝑥∥𝑀−1
𝑡∥𝑔𝑡(𝜃(1)
𝑡)−𝑔𝑡(𝜃∗)∥𝑀−1
𝑡
=1
𝜅1∥𝑥∥𝑀−1
𝑡∥𝑔𝑡(𝜃(1)
𝑡)−𝑔𝑡(𝜃𝑡)+𝑔𝑡(𝜃𝑡)−𝑔𝑡(𝜃∗)∥𝑀−1
𝑡
≤(2)1
𝜅1∥𝑥∥𝑀−1
𝑡(∥𝑔𝑡(𝜃(1)
𝑡)−𝑔𝑡(𝜃𝑡)∥𝑀−1
𝑡+∥𝑔𝑡(𝜃𝑡)−𝑔𝑡(𝜃∗)∥𝑀−1
𝑡)
≤(3)2
𝜅1∥𝑥∥𝑀−1
𝑡∥𝑔𝑡(𝜃𝑡)−𝑔𝑡(𝜃∗)∥𝑀−1
𝑡
≤2
𝜅1(∥𝑆𝑡∥𝑀−1
𝑡+√︁
𝜆𝜅1∥𝜃∗∥2)∥𝑥∥𝑀−1
𝑡(21)
The first inequality (1)comes from 𝐺𝑡(𝜃(1)
𝑡,𝜃∗) ≥𝜅1𝑀𝑡>0,
and therefore 𝐺𝑡(𝜃(1)
𝑡,𝜃∗)−1≤1
𝜅1𝑀𝑡. The second inequality is the
application of triangle inequality, and the second inequality (3)is
based on the definition of 𝜃(1)
𝑡from Eq. (9).
Notice that𝜖𝑡and˜𝜖𝑡are𝑅-subgaussian, and∀𝑡,∥𝑑𝑡∥2,∥˜𝑑𝑡∥2≤2.
According to Theorem 1 in [1], we have:
∥𝑆𝑡∥𝑀−1
𝑡≤𝑅√︄
2 log(det(𝑀𝑡)1/2/(𝜆
𝜅1)𝑑/2
𝛿)
≤𝑅√︂
𝑑log((1+4𝜅1(𝑡+𝑏(𝑡))
𝜆𝑑)/𝛿). (22)
Therefore,𝛼𝑡≜2
𝜅1(𝑅√︃
𝑑log((1+4𝜅1(𝑡+𝑏(𝑡))
𝑑𝜆)/𝜎)+√𝜆𝜅1∥𝜃∗∥2).
□
Proof of Lemma 2. Lemma 2 follows the existing results of Propo-
sition 1 in [14].
DenoteBas the barycentric spanner for key-term set K. Let
𝑥,𝑦be random vectors sampled independently and uniformly from
B, i.e.𝑥,𝑦𝑖𝑖𝑑∼ B . Define Σ≜𝐸𝑥,𝑦𝑖𝑖𝑑∼B[(𝑥−𝑦)(𝑥−𝑦)𝑇]. For ease
of understanding, we assume the pair of key-terms at round 𝑡con-
versation with contextual vectors (˜𝑥𝑡,˜𝑦𝑡), and the key-term level
design matrix is denoted as ˜𝑀𝑡=Í𝑡
𝑠=1Í
˜𝑥,˜𝑦𝑖𝑖𝑑∼B(˜𝑥𝑠−˜𝑦𝑠)(˜𝑥𝑠−˜𝑦𝑠)𝑇.
Define𝑧𝑡=Σ−1/2(𝑥𝑡−𝑦𝑡), then𝑧𝑡is isotropic, namely, 𝐸[𝑧𝑡𝑧𝑇
𝑡]=
𝐼. Define𝑈𝑡=Í𝑡
𝑠=1Í
˜𝑥𝑠,˜𝑦𝑠𝑖𝑖𝑑∼B𝑧𝑠𝑧𝑇𝑠=Σ−1/2˜𝑀𝑡Σ−1/2.
From Lemma 1 in [14], with probability at least (1 −2exp(−𝐶2𝑥2)):
𝜆min(𝑈𝑡)≥𝑏(𝑡)−𝐶1𝜎2√︁
𝑏(𝑡)𝑑−𝜎2𝑥√︁
𝑏(𝑡),
where𝜎is the sub-gaussian parameter of 𝑧and is upper-bounded by
∥Σ−1/2∥=𝜆min(Σ)−1/2. Therefore, with probability at least (1−𝛿):
𝜆min(𝑈𝑡)≥𝑏(𝑡)−1
𝜆min(Σ)(𝐶1√︁
𝑏(𝑡)𝑑+𝑥√︁
𝑏(𝑡)).
Furthermore, the minimum eigenvalue of ˜𝑀𝑡is bounded as follows:
𝜆𝑚𝑖𝑛(˜𝑀𝑡)=min
𝑥∈B𝑑𝑥𝑇˜𝑀𝑡𝑥=min
𝑥∈B𝑑𝑥𝑇Σ1/2𝑈𝑡Σ1/2𝑥≥𝜆min(𝑈𝑡)𝜆min(Σ)
≥𝜆min(Σ)(𝑏(𝑡)−𝜆min(Σ)−1(𝐶1√︁
𝑏(𝑡)𝑑+𝐶2√︁
𝑏(𝑡)log(1/𝛿)
=𝜆min(Σ)𝑏(𝑡)−(𝐶1√
𝑑+𝐶2√︁
log(1/𝛿))√︁
𝑏(𝑡). (23)We denote𝜆min(Σ)=𝜆𝐵. When𝑏(𝑡)≥4(𝐶1√
𝑑+𝐶2√
log(1/𝛿))2
𝜆2
𝐵, we
have:
𝜆min(˜𝑀𝑡)≥𝜆𝐵𝑏(𝑡)
2. (24)
When the conversation frequency function is linear, i.e., 𝑏(𝑡)=𝑏·𝑡
for some𝑏∈(0,1), then with probability at least (1−𝛿), we have:
𝜆min(˜𝑀𝑡)≥𝜆𝐵𝑏𝑡
2,
as long as𝑡≥4(𝐶1√
𝑑+𝐶2√
log(1/𝛿))2
𝑏𝜆2
𝐵≜𝑡0. □
Proof of Lemma 3. According to Lemma 2,
𝜆min(𝑀𝑡)≥𝜆min(˜𝑀𝑡)+𝜆
𝜅1≥𝜆𝐵𝑏(𝑡)
2+𝜆
𝜅1
when𝑏(𝑡)≥4(𝐶1√
𝑑+𝐶2√
log(1/𝛿))2
𝜆2
𝐵.
We have assumed that ∥𝑥𝑡∥2≤1,∀𝑥𝑡∈A𝑡, and∥𝑑𝑡∥2=∥𝑥𝑡−
𝑥′
𝑡∥2≤∥𝑥𝑡∥2+∥𝑥′
𝑡∥2≤2, where(𝑥𝑡,𝑥′
𝑡)denote the contextual
vectors for the selected pair of arms at round 𝑡. Therefore we can
obtain the following inequality:
∥𝑑𝑡∥𝑀−1
𝑡≤√︃
1
𝜆min(𝑀𝑡)∥𝑑𝑡∥2≤2√︃
1
𝜆min(𝑀𝑡)≤2(𝜆𝐵𝑏(𝑡)
2+𝜆
𝜅1)−1/2,
as well as:
𝑡∑︁
𝑠=𝑡0+1∥𝑑𝑠∥𝑀−1𝑠≤2𝑡∑︁
𝑠=𝑡0+1√︂1
𝜆min(𝑀𝑠)≤2𝑡∑︁
𝑠=𝑡0+1(𝜆𝐵𝑏(𝑠)
2+𝜆
𝜅1)−1/2
≤2𝑡∑︁
𝑠=𝑡0+1(𝜆𝐵𝑏(𝑠)
2)−1/2≤2∫𝑡
𝑠=𝑡0(𝜆𝐵𝑏(𝑠)
2)−1/2𝑑𝑠. (25)
Though the upper bound given in (25)is complicated, when the
conversation frequency function is linear, i.e., 𝑏(𝑡)=𝑏·𝑡for some
random𝑏∈(0,1), we can easily calculate the following inequality:
𝑡∑︁
𝑠=𝑡0+1∥𝑑𝑠∥𝑀−1𝑠≤8√︂𝑠
2𝑏𝜆𝐵𝑡
𝑠=𝑡0=8(√︂𝑡
2𝑏𝜆𝐵−√︂𝑡0
2𝑏𝜆𝐵)≤8√︂𝑡
2𝑏𝜆𝐵
(26)
□
Proof of Theorem 1. This proof lies in expressing the regret
bound in terms of the above concentration results from Lemma 1,
and it is possible owing to the arm selection strategy which follows
themost informative pair strategy from [ 18]. Suppose we have
selected a pair of arms at round 𝑡with contextual vector being
(𝑥𝑡,𝑥′
𝑡), and assume 𝑥∗
𝑡=arg max𝑎∈A𝑡𝑥𝑇
𝑎,𝑡𝜃∗, then we have:
2𝑟𝑡=(𝑥∗
𝑡𝑇𝜃∗−𝑥𝑇
𝑡𝜃∗)+(𝑥∗
𝑡𝑇𝜃∗−𝑥′
𝑡𝑇𝜃∗)
≤
(𝑥∗
𝑡−𝑥𝑡)𝑇𝜃∗+(𝑥∗
𝑡−𝑥′
𝑡)𝑇𝜃∗
=
(𝑥∗
𝑡−𝑥𝑡)𝑇(𝜃∗−𝜃(1)
𝑡)+(𝑥∗
𝑡−𝑥𝑡)𝑇𝜃(1)
𝑡
+(𝑥∗
𝑡−𝑥′
𝑡)𝑇(𝜃∗−𝜃(1)
𝑡)+(𝑥∗
𝑡−𝑥′
𝑡)𝑇𝜃𝑡
≤(1)
𝛼𝑡∥𝑥∗
𝑡−𝑥𝑡∥𝑀−1
𝑡+∥𝜃∗−𝜃(1)
𝑡∥𝑀𝑡∥𝑥∗
𝑡−𝑥𝑡∥𝑀−1
𝑡
+∥𝜃∗−𝜃(1)
𝑡∥𝑀𝑡∥𝑥∗
𝑡−𝑥′
𝑡∥𝑀−1
𝑡+𝛼𝑡∥𝑥∗
𝑡−𝑥′
𝑡∥𝑀−1
𝑡
≤(2)
2𝛼𝑡∥𝑥∗
𝑡−𝑥𝑡∥𝑀−1
𝑡+2𝛼𝑡∥𝑥∗
𝑡−𝑥′
𝑡∥𝑀−1
𝑡
≤(3)4𝛼𝑡∥𝑥𝑡−𝑥′
𝑡∥𝑀−1
𝑡,
3816KDD ’24, August 25–29, 2024, Barcelona, Spain Shuhua Yang et al.
where the first inequality (1) holds due to the construction of
C𝑡and the fact that both 𝑥𝑡,𝑥′
𝑡∈C𝑡, so that(𝑥∗
𝑡−𝑥𝑡)𝑇𝜃(1)
𝑡≤
𝛼𝑡∥𝑥∗
𝑡−𝑥𝑡∥𝑀−1
𝑡and(𝑥∗
𝑡−𝑥′
𝑡)𝑇𝜃(1)
𝑡≤𝛼𝑡∥𝑥∗
𝑡−𝑥′
𝑡∥𝑀−1
𝑡. Inequality
(2) follows from Lemma 1, where we have proved 𝛼𝑡is the upper
bound for∥𝜃∗−𝜃(1)
𝑡∥𝑀−1
𝑡. The last inequality comes from the arm
selection strategy.
Denote𝑑𝑡=𝑥𝑡−𝑥′
𝑡, combined with the definition of 𝛼𝑡, we have
𝑟𝑡≤4
𝜅1(𝑅√︂
𝑑log((1+4𝜅1(𝑡+𝑏(𝑡))
𝑑𝜆)/𝜎)+√︁
𝜆𝜅1∥𝜃∗∥2)∥𝑑𝑡∥𝑀−1
𝑡.
Therefore, the cumulative regret over time 𝑇is:
𝑅(𝑇)=𝑡0∑︁
𝑡=1𝑟𝑡+𝑇∑︁
𝑡=𝑡0+1𝑟𝑡≤𝑡0+𝑇∑︁
𝑡=𝑡0+1𝑟𝑡
≤𝑡0+2𝛼𝑇𝑇∑︁
𝑡=𝑡0+1∥𝑑𝑡∥𝑀−1
𝑡≤𝑡0+2𝛼𝑇𝑡∑︁
𝑠=𝑡0+1(𝜆𝐵𝑏(𝑠)
2)−1/2
≤𝑡0+2𝛼𝑇∫𝑡
𝑠=𝑡0(𝜆𝐵𝑏(𝑠)
2)−1/2𝑑𝑠. (27)
When𝑏(𝑡)=𝑏·𝑡, we have:
𝑅(𝑇)≤𝑡0+32
𝜅1
𝑅√︂
𝑑log((1+4𝜅1(𝑇+𝑏𝑇)
𝑑𝜆)/𝜎)
+√︁
𝜆𝜅1∥𝜃∗∥2√︄
𝑇
2𝑏𝜆𝐵=O(1
𝜅1√︁
𝑑𝑇log(𝑇)). (28)
□
B Intuition for ConDuel-MaxInp Algorithm
We design the ConDuel-MaxInp algorithm based on intuition as
follows:
Denote𝑛𝑡=⌊𝑏(𝑡)−𝑏(𝑡−1)⌋as the number of conversations
between the agent and the user when 𝑞(𝑡)=1, and𝑛𝑡=0when
𝑞(𝑡)=0for the key-term selection module. Based on Eq (8), we
rewrite𝑀𝑡as:
𝑀𝑡=𝑡∑︁
𝑠=1𝑑𝑡𝑑𝑇
𝑡+𝑡∑︁
𝑠=1𝑛𝑠∑︁
𝑗=1˜𝑑𝑗˜𝑑𝑇
𝑗+𝜆/𝜅1𝐼, (29)
and also define 𝑀𝑡,𝑗=𝑀𝑡−1+𝑑𝑡−1𝑑𝑇
𝑡−1+Í𝑗−1
𝑖=1˜𝑑𝑖˜𝑑𝑇
𝑖for𝑗∈
{1,...,𝑛𝑡}when𝑛𝑡≠0, then it is easy to obtain the following
equation:
𝑑𝑒𝑡(𝑀𝑡)=𝑑𝑒𝑡(𝑀𝑡−1)(1+∥𝑑𝑡−1∥2
𝑀−1
𝑡−1)𝑛𝑡Ö
𝑗=1(1+∥˜𝑑𝑗∥2
𝑀−1
𝑡,𝑗)
=𝑑𝑒𝑡(𝑀0)𝑡−1Ö
𝑠=1(1+∥𝑑𝑠∥2
𝑀−1𝑠)𝑡Ö
𝑠=1𝑛𝑠Ö
𝑗=1(1+∥˜𝑑𝑗∥2
𝑀−1
𝑠,𝑗).
Notice that1
2𝑥≤𝑙𝑜𝑔(1+𝑥)≤𝑥for𝑥∈[0,1], we have:
𝑡−1∑︁
𝑠=1∥𝑑𝑠∥2
𝑀−1𝑠+𝑡∑︁
𝑠=1𝑛𝑠∑︁
𝑗=1∥˜𝑑𝑗∥2
𝑀−1𝑠
≤𝑡−1∑︁
𝑠=1∥𝑑𝑠∥2
𝑀−1𝑠+𝑡∑︁
𝑠=1𝑛𝑠∑︁
𝑗=1∥˜𝑑𝑗∥2
𝑀−1
𝑠,𝑗≤2 log𝑑𝑒𝑡(𝑀𝑡)
𝑑𝑒𝑡(𝑀0)
. (30)Therefore, when applying the ”Maxinp” strategy on selecting key-
terms in the ConDuel-Maxinp algorithm, i.e., choosing key-term
satisfying ˜𝑘𝑡=arg max ˜𝑑∈˜𝐷𝑡∥˜𝑑∥2
𝑀−1
𝑡at𝑡conversation, the system
carries out explorative conversations and reduces uncertainty on
key-terms.
C Proofs for ConMNL Algorithm
We start by giving the following lemmas to prove Theorem 2.
Lemma 4. ([11], Lemma 9) If 𝜆min(𝑀𝑇0)≥𝑚𝑎𝑥{𝜎2𝜅−2
2(𝑑log(𝑏(𝑇)+𝑇
𝑑)+
2log(1
𝛿),𝑞}, then∀𝑡≥𝑇0, with probability at least (1−𝛿), we have
∥𝜃𝑡−𝜃∗∥≤1.
Lemma 5. Suppose∥𝜃𝑡−𝜃∗∥≤1for𝑡≥𝑇0, then with probability
at least(1−𝛿), we have
∥𝜃𝑡−𝜃∗∥𝑀𝑡≤1
2𝜅2√︂
2𝑑log(1+𝑏(𝑡)+𝑡
𝑑)+log(1
𝛿)≜𝛼𝑡.(31)
Lemma 6. Following the definition of Barycentric Spanner Bof key-
term set, we assume 𝜆′
B=𝜆min
𝐸𝑘∼B[˜𝑥𝑘˜𝑥𝑇
𝑘]
>0. Define𝑏(𝑡)=𝑏𝑡
for𝑏∈(0,1), then∀𝑡≥256
𝑏𝑞𝜆′
Blog(128𝑑
𝛿𝜆2
B′)≜𝑡0, and𝛿∈(0,1/8], with
probability at least (1−𝛿), we have
𝑡∑︁
𝑠=𝑡0+1∑︁
𝑖∈𝐶𝑠∥𝑥𝑖,𝑡∥𝑀−1𝑠≤2√︄
2𝑞𝑡
𝑏𝜆B′
Lemma 7. ([15], lemma 4) With 𝐶∗
𝑡defined as the optimal assort-
ment, and𝐶𝑡=arg max𝐶∈Cˆ𝑅𝑡(𝐶,𝑡). If𝑧𝑖,𝑡≥𝑥𝑇
𝑖,𝑡𝜃∗for every𝑖∈𝐶∗
𝑡,
then we have
𝑅𝑡(𝐶∗
𝑡,𝜃∗)≤ˆ𝑅𝑡(𝐶∗
𝑡)≤ˆ𝑅𝑡(𝐶𝑡).
Lemma 8. ([15], Lemma 3) With 𝛼𝑡defined in Lemma 5, suppose
𝑧𝑎,𝑡=𝑥𝑇
𝑎,𝑡𝜃𝑡+𝛼𝑡∥𝑥𝑎,𝑡∥𝑀−1
𝑡for all𝑎∈A𝑡, then we have
0≤𝑧𝑎,𝑡−𝑥𝑇
𝑎,𝑡𝜃∗≤2𝛼𝑡∥𝑥𝑎,𝑡∥𝑀−1
𝑡.
Proof of Theorem 2. When𝑇0=O 𝑚𝑎𝑥{𝑡0,𝑡1,𝑞/𝜎2}, with
𝑡0=256
𝑏𝑞𝜆′
Blog(128𝑑
𝛿𝜆2
B′), and1
𝜅2
2(𝑑log(𝑏(𝑇)+𝑇
𝑑+4log(𝑇)), we have
∥𝜃𝑡−𝜃∗∥≤1by Lemma 4, and the regret becomes:
𝑅𝑇=𝑇0∑︁
𝑡=1 𝑅𝑡(𝐶∗
𝑡,𝜃∗)−𝑅𝑡(𝐶𝑡,𝜃∗)+𝑇∑︁
𝑡=𝑇0+1 𝑅𝑡(𝐶∗
𝑡,𝜃∗)−𝑅𝑡(𝐶𝑡,𝜃∗)
≤𝑇0+𝑇∑︁
𝑡=𝑇0+1
ˆ𝑅𝑡(𝐶𝑡)−𝑅𝑡(𝐶𝑡,𝜃∗)
+𝑇∑︁
𝑡=1O(𝑡−2)
≤𝑇0+2𝛼𝑇𝑇∑︁
𝑡=𝑇0+1∑︁
𝑖∈𝐶𝑡∥𝑥𝑖,𝑡∥𝑀−1
𝑡+O( 1)
≤𝑇0+4𝛼𝑇√︄
2𝑞𝑡
𝑏𝜆B′+O( 1) (32)
Combined with Lemma 6 and the definition of 𝛼𝑇, Theorem 2 is
proved. □
3817