Unsupervised Ranking Ensemble Model for Recommendation
Wenhui Yu†∗
Kuaishou Technology
Beijing, China
yuwenhui07@kuaishou.comBingqi Liu†
Kuaishou Technology
Beijing, China
liubingqi03@kuaishou.comBin Xia
Kuaishou Technology
Beijing, China
xiabin@kuaishou.com
Xiaoxiao Xu
Kuaishou Technology
Beijing, China
xuxiaoxiao05@kuaishou.comYing Chen
Kuaishou Technology
Beijing, China
chenying08@kuaishou.comYongchang Li
Kuaishou Technology
Beijing, China
liyongchang@kuaishou.com
Lantao Hu
Kuaishou Technology
Beijing, China
hulantao@kuaishou.com
Abstract
When visiting an online platform, a user generates various actions,
such as clicks, long views, likes, comments, etc. To capture user
preferences in these aspects, we learn these objectives and return
multiple rankings of candidate items for each user. We need to
aggregate them into one to truncate the candidate set, and ranking
ensemble model is proposed for this task. However, there is a critical
issue: though we input abundant information, what model learns
depends on the supervision. Unfortunately, the existing supervision
is poorly designed, leading to serious information loss issue.
To address this issue, we designed an unsupervised loss to com-
pel the ranking ensemble model to learn all information of input
rankings, including sequential and numerical information. (1) For
sequential information, we design a distance measure between two
rankings, and train the ensemble ranking to have similar order with
all input rankings by minimizing the distance. (2) For numerical
information, we design a decoder to reconstruct values of original
rankings from the hidden layer of the model, to guarantee that the
model captures as much input information as possible. Our unsu-
pervised loss is compatible with all ranking ensemble models. We
optimize several widely-used structures to propose unsupervised
ranking ensemble models.
We devise comprehensive experiments on two real-world datasets
to demonstrate the effectiveness of the proposed models. We also
apply our model in a short video platform with billions of users,
and achieve significant improvement.
∗The corresponding author.
†Contribute equally.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671598CCS Concepts
•Information systems →Recommender systems.
Keywords
Recommendation, Ranking ensemble model, Learning to rank, Un-
supervised learning, Multi-objective learning
ACM Reference Format:
Wenhui Yu†, Bingqi Liu, Bin Xia, Xiaoxiao Xu, Ying Chen, Yongchang Li,
and Lantao Hu. 2024. Unsupervised Ranking Ensemble Model for Recom-
mendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671598
1 Introduction
Recommender system has gained increasing research efforts in past
decades, due to the impressive growth of online services such as
video applications. When a user consumes her time on these online
services, she generates different kinds of actions, such as long views,
likes, comments, etc. Figure 1(a) shows the buttons of these actions.
These actions are all explicit positive feedbacks, which are very
important to capture user preference from different perspectives.
To leverage these information, we learn all these objectives in the
ranking stage, and merge the predictions to truncate the candidate
set. User preferences in different aspects should be considered in the
aggregated score. Figure 1(b) shows the multi-step online recom-
mendation system, where the ranking stage is the most important
stage. Figure 1(c) shows the two-step ranking module, where a
ranking model is trained to predict scores for different actions, and
aranking ensemble model is trained to merge these scores into one.
The ranking model is usually a DIN/SIM + MMOE structure
[18,22,31], with user features (including user behaviors), item
features, and cross features as input. The ranking model predicts
probability of action “x”, called predicted “x” through rate (pxtr),
such as pctr (x = click), plvtr (x = long view), pltr (x = like), pcmtr
(x = comment), etc. For a certain action x, the pxtr score list of the
candidate set is called pxtr ranking.
The ranking ensemble model is used to merge all pxtr rankings
into one ensemble ranking to truncate the candidate set. It is also a
6181
KDD ’24, August 25–29, 2024, Barcelona, Spain Wenhui Yu et al.
Follow
Like
Comment
Collect
SharePlay next
All items7k1206
RetrieveRankingRe-rankingCandidate sets Recommen-
dation list
i1, i2, i3, i4Ranking
modelUser and cross features
Item featuresRanking 
ensemble
modelplvtr ranking
i4, i2, i1, i3
pltr ranking
i2, i4, i3, i1
pcmtr ranking
i4, i2, i3, i1
Ensemble 
ranking
i4, i2, i3, i1
(a) User actions(b) Multi-step recommender system
(c) Two-step ranking moduleLong viewi4, i2crop
Figure 1: User actions and the ranking ensemble task.
recommender model with the pxtr rankings as cross features. As
we can see, the ranking ensemble task is important since we need to
give an accurate and comprehensive depiction of user preferences
by the ensemble ranking. The reason that we use a two-step ranking
module rather than an end-to-end one is that we need the capability
to adjust the system by enlarging or reducing the influence of each
pxtr ranking to the system flexibly, thus the intermediate variables
(pxtr rankings) are needed. Since pxtr rankings provides abundant
and generalized information, ranking ensemble models are usually
much more lightweight and with less input than ranking models.
However, there is a severe flaw in existing ranking ensemble
models: though we input abundant information to the model by
feeding all pxtr rankings, what the model learns depends on the
supervision signal. Unfortunately, in existing work, the supervision
is usually unilateral, leading to serious information loss issues. We
analyze the demerits of several mainstream supervision strategies:
(1) The most intuitive and widely-used way is to use the main
objective to train the model. For example, long views in short video
recommendation and clicks in e-commerce recommendation. How-
ever, this approach has an evident drawback: labels only reflect the
distribution of long views, resulting in the omission of other pxtr
rankings. The reason is that the input plvtr ranking provides enough
information to predict labels, yet other rankings contribute little,
or even negatively for long view prediction, due to the inconsistent
distribution.
(2) A better option is to use exposure labels (1 for items exposed
to the user and 0 for other items in the candidate set). Compared
with the long view labels, exposure labels are more comprehensive.
All objectives are balanced to some degree since the recommender
system selects items by taking all objectives into consideration.
However, further actions after exposed to the user are not used,
also leading to the issue of information loss. In addition, exposure
labels are with lower degree of confidence, since they are not true
user feedbacks.
(3) Li et al . [16] used user actions to construct partial order (e.g.,
like/comment/share ≻long view≻exposure≻unexposure) for
pairwise learning to rank (LTR) loss. However, there are also severalfatal drawbacks: (a) Only partial order (order between different
actions) rather than full order (order of items in the same action) is
leveraged. (b) There is no action label for unexposed items, which
account for more than 99% of the total samples. Even within the
scope of exposed items, user actions are very sparse. (c) Partial
order between some actions, such as like, comment, and share, is
not clear and not personalized. (d) Pairs are treated uniformly.
To address these issues, we propose an unsupervised method to
train the model. We argued that the pxtr rankings are more infor-
mative and with better generalization than the action labels, since
these rankings are learned by a very heavy ranking model with
complex interaction mechanisms like attention [ 31] and abundant
features including life-long user action sequences [ 22]. Supervised
by action labels, the ranking model learns a high-quality action
distribution by injecting a large amount of information into the
rankings. We are proposed to explore the information of pxtr rank-
ings as much as possible in the second step. Though we have input
all these rankings into the ranking ensemble model, we fail to lever-
age all information due to the shortcomings of supervision. To
explore rankings thoroughly, we additionally use them to supervise
the model and compel the model to reserve the information of them.
We want the ensemble ranking to be similar to all pxtr rankings.
Normalized discounted cumulative gain (NDCG) is leveraged to
measure the distance between two rankings. The total distance is
defined as the weighted sum of the distance between the ensemble
ranking and each pxtr ranking.
To further compel the model to reserve the information of input
rankings, we design an auto-encoder structure. The encoder (a
ranking ensemble model) is trained for ranking aggregation and
the decoder is trained to reconstruct original pxtr rankings. The
aggregation task and reconstruction task are trained jointly. The
encoder and decoder can be any structure, thus our unsupervised
loss is compatible with all existing ranking ensemble models. In
this paper, we use the unsupervised loss to optimize two widely-
used structures, fully-connected net (point-wise optimization) and
transformer (list-wise optimization), to propose the Unsupervised
Ranking Ensemble Models (UREM).
Finally, we validate the effectiveness of our proposed UREM
methods by comparing them with several baselines on two real-
world datasets. Extensive experiments show that we improve the
performance significantly. We also demonstrate the power of the
unsupervised loss by AB test on the online experiment platform
and apply our UREM models in online recommendation service.
Specifically, our main contributions are listed as follows:
•We propose an novel ranking distance and guarantee the
ensemble ranking is similar to all input pxtr rankings by
minimizing the distance.
•We design a decoder to reconstruct pxtr scores from the
hidden representation of the ranking ensemble model, to
ensure the model learns all original information.
•We optimize widely-used ensemble models by the new un-
supervised loss to propose UREM models.
•We devise comprehensive experiments on two real-world
datasets to demonstrate the effectiveness of our methods.
6182Unsupervised Ranking Ensemble Model for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
2 Related Work
Recommendation models predict user preference based on historical
actions [ 4,11,12,14,15,22,23,26,27,30,31]. In industrial recom-
mender systems, we learn different pxtr with respect to different
user actions by ranking models, and merge these pxtr rankings
into one to give a comprehensive preference describe. Ranking
ensemble is a critical component and has been discussed widely in
information retrieval tasks [7, 9].
There are two main paradigms for ranking ensemble in industrial
scenarios, formulas [ 3,5,8] and ranking ensemble models [ 16]. Dau-
nou[5]summed all pxtr scores to get the ensemble ranking. Bartell
et al. [3] used weighted sum 𝑠𝑐𝑜𝑟𝑒 =Í
𝑥𝜔𝑥𝑝𝑥𝑡𝑟 and learn weight
coefficients by optimizing an offline criterion called Guttman’s
Point Alienation. In real-world applications, weight coefficients are
usually determined manually by maximizing online metrics. Tang
et al. [24] represented a multiplication formula: 𝑠𝑐𝑜𝑟𝑒 =Î
𝑥𝑝𝑥𝑡𝑟𝜔𝑥.
Similar to weighted sum, exponents are tuned according to online
performance. Some advanced versions remap the pxtr scores by
a nonlinear function or use 1/rank to remove the impact of scale.
While these formulas provide flexibility for system tuning, yet are
overly simplistic.
Another paradigm is to learn ranking ensemble models in a
supervised way. Oliveira et al . [20] proposed an Evolutionary Rank
Aggregation (ERA) model optimized with genetic programming.
Bałchanowski and Boryczka [2]leveraged differential evolution
algorithm and Zhang et al . [29] leveraged reinforcement learning
for rank aggregation optimization. [ 16] predicted user intent based
on the user action sequence and environment context, and merged
rankings based on different user intents. However, these models
are dependent on supervision, which is usually flawed as analyzed
in the Introduction. To address this issue, we design unsupervised
modular to learn better ranking ensemble models.
3 Unsupervised Learning Loss
In this section, we design the Unsupervised Ranking Ensemble
Loss ( UREL), including a ranking distance objective and a pxtr
prediction reconstruction objective. Since massive notations are
used in this paper, we give a summary of them.
First, we define the issue. For current user 𝑢, assuming there is a
candidate set, denoted as C𝑢={𝑖1,𝑖2,···,𝑖𝑛}(Cfor short if𝑢is not
emphasized) with 𝑛items in it, our task is to select top 𝑘items that
match user preference the best. A multi-objective ranking model
returning𝑚pxtr rankings{𝑟𝑥}𝑥∈A, where𝑥indicates certain ac-
tion andA={𝑙𝑣,𝑙,𝑐𝑚,···} indicates the action set. In A,𝑙𝑣is for
long view (pxtr here is plvtr), 𝑙is for like,𝑐𝑚is for comment, etc.
Each pxtr ranking is represented as a list of continuous pxtr
scores𝑟𝑥={𝑠𝑥
𝑖}𝑖∈C. The order of 𝑟𝑥is denoted as 𝑜𝑥={𝑜𝑥
𝑖}𝑖∈C.
Our task is to construct a function 𝑓(,Θ)to merge{𝑟𝑥}𝑥∈Ainto
one ensemble ranking 𝑟𝑒𝑛𝑠=𝑓({𝑟𝑥}𝑥∈A,Θ)={𝑠𝑒𝑛𝑠
𝑖}𝑖∈C, and the
corresponding order is 𝑜𝑒𝑛𝑠={𝑜𝑒𝑛𝑠
𝑖}𝑖∈C.
𝑓(,Θ)is a ranking ensemble model and Θindicates a set of
model parameters. For each user, the inputted pxtr rankings {𝑟𝑥}𝑥∈A
are represented as a 𝑚×𝑛matrix, and the output ensemble ranking
𝑟𝑒𝑛𝑠is represented as a 𝑛-dimensional vector. Considering that 𝑟𝑒𝑛𝑠
is a function of Θ. We train𝑓(,Θ)to get the best order 𝑜𝑒𝑛𝑠. InTable 1: Notations used in this paper
Notation Description
𝑢 A certain user
𝑖or𝑗 A certain item
𝑥 A certain user action, such as long view, like
C𝑢orC The candidate set of user 𝑢
A The set of user actions
𝑠𝑥
𝑖/𝑠𝑒𝑛𝑠
𝑖The pxtr/ensemble score of item 𝑖
𝑟𝑥/𝑟𝑒𝑛𝑠The pxtr/ensemble score lists (rankings)
𝑜𝑥/𝑜𝑒𝑛𝑠The pxtr/ensemble order lists
𝑓()/𝜙()/𝜓()/𝜉()The model structures
Θ/Φ/Ψ/Ξ The sets of model parameters
𝑓(,Θ) The ranking ensemble model
𝜙(,Φ) The top layer of the model
𝜓(,Ψ) The encoder
𝜉(,Ξ) The decoder
the inference stage, we select top 𝑘items based on 𝑜𝑒𝑛𝑠and return
them to the downstream of the recommender system.
3.1 Ranking Similarity Loss
As we aim to generate an ensemble ranking that is similar to all pxtr
rankings, we minimize the distance between the ensemble ranking
and each original ranking. To achieve this, we utilize Normalized
Discounted Cumulative Gain (NDCG) to measure the distance be-
tween two rankings. NDCG of 𝑟𝑥and𝑟𝑒𝑛𝑠is defined as:
NDCG𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠)=DCG𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠)
IDCG𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠)=Í
𝑖∈C𝑢𝑠𝑥
𝑖
log2(𝑜𝑒𝑛𝑠
𝑖+1)
Í
𝑖∈C𝑢𝑠𝑥
𝑖
log2(𝑜𝑥
𝑖+1).(1)
NDCG is defined as the normalization of Discounted Cumulative
Gain (DCG) by the Ideal Discounted Cumulative Gain (IDCG). DCG
is defined as a weighted sum of ensemble score, where 𝑠𝑥
𝑖is de-
cayed by weight1
log2(𝑜𝑒𝑛𝑠
𝑖+1). Since1
log2(𝑜𝑒𝑛𝑠
𝑖+1)increases monoton-
ically with𝑠𝑒𝑛𝑠
𝑖,DCG(𝑟𝑥,𝑟𝑒𝑛𝑠)is large if𝑟𝑥and𝑟𝑒𝑛𝑠have the same
trend. IDCG is defined as DCG(𝑟𝑥,𝑟𝑥)and is used to normalize
𝐷𝐶𝐺(𝑟𝑥,𝑟𝑒𝑛𝑠). In Equation (1), we use 𝑟𝑥as the ground truth to
evaluate𝑟𝑒𝑛𝑠. As we can see, NDCG describes the consistency of
rankings, thus negative NDCG is a measure of ranking distance.
We aim to minimize the distance of the ensemble ranking and
pxtr rankings by Ranking Similarity Loss 𝐿𝑅𝑆. It is defined as the
weighted sum of negative NDCG:
𝐿𝑅𝑆=−∑︁
𝑢∑︁
𝑥∈A𝜔𝑥·NDCG𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠). (2)
There are three demerits of NDCG distance: (1) 𝑜𝑥
𝑖in Equation
(1) blocks back propagation, making NDCG distance is invalid to
optimize a deep model directly. (2) Only order information of the
ensemble ranking{𝑜𝑒𝑛𝑠
𝑖}is supervised, but the value information
{𝑠𝑒𝑛𝑠
𝑖}is omitted. (3) NDCG loss is computationally expensive since
𝑛is large.
To address the aforementioned issues, we separate the ranking
into pairs and simplify NDCG distance into pairwise distance. For
6183KDD ’24, August 25–29, 2024, Barcelona, Spain Wenhui Yu et al.
a pair of items 𝑖and𝑗, (1) we want 𝑠𝑒𝑛𝑠
𝑖−𝑠𝑒𝑛𝑠
𝑗and𝑠𝑥
𝑖−𝑠𝑥
𝑗have
the same sign, thus the pairwise similarity is sign(𝑠𝑒𝑛𝑠
𝑖−𝑠𝑒𝑛𝑠
𝑗)·
sign(𝑠𝑥
𝑖−𝑠𝑥
𝑗), where sign( ) ∈ {− 1,1}is the sign function. (2)
Further, we want 𝑠𝑒𝑛𝑠
𝑖−𝑠𝑒𝑛𝑠
𝑗and𝑠𝑥
𝑖−𝑠𝑥
𝑗varies consistently, i.e., if
𝑠𝑒𝑛𝑠
𝑖−𝑠𝑒𝑛𝑠
𝑗is large,𝑠𝑥
𝑖−𝑠𝑥
𝑗is also large. To achieve this, we modify
the similarity to(𝑠𝑒𝑛𝑠
𝑖−𝑠𝑒𝑛𝑠
𝑗)·(𝑠𝑥
𝑖−𝑠𝑥
𝑗). (3) Considering that different
pxtr has different scales, we normalize {𝑠𝑥
𝑖}and{𝑠𝑒𝑛𝑠
𝑖}by z-score
normalization to get {¯𝑠𝑥
𝑖}and{¯𝑠𝑒𝑛𝑠
𝑖}. As¯𝑠𝑥
𝑖and¯𝑠𝑒𝑛𝑠
𝑖can be large,
we use monotonic tanh()to limit the scale without changing the
sign. The pairwise similarity of rankings 𝑟𝑥and𝑟𝑒𝑛𝑠for user𝑢is
defined as
sim𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠)=∑︁
𝑖,𝑗∈C𝑢
𝑖≠𝑗tanh(¯𝑠𝑒𝑛𝑠
𝑖−¯𝑠𝑒𝑛𝑠
𝑗)tanh(¯𝑠𝑥
𝑖−¯𝑠𝑥
𝑗). (3)
To reduce computational consumption, we sample a part of pairs
fromC𝑢. The pairwise distance is defined as
𝐿𝑅𝑆=−∑︁
𝑢∑︁
𝑥∈A𝜔𝑥·sim𝑢(𝑟𝑥,𝑟𝑒𝑛𝑠). (4)
By minimizing Equation (4), we can train the model Θ.𝜔𝑥is deter-
mined by the importance of the objective, and can be fine-tuned
based on the online performance.
3.2 Ranking Reconstruction Loss
The input of the ranking ensemble model is very informative. Tak-
ing our online recommender system as an example, there are 11
pivotal rankings generated from a extreme heavy ranking model
in the ranking stage. As analyzed in Introduction, each ranking is
informative since the ranking model injects abundant information
of features and labels into it. Moreover, there are 11 rankings with
different distributions.
To ensure that the ranking ensemble model learns all information,
we train the model to reconstruct original pxtr rankings from the
hidden layer of the model. To achieve this, we separate the model
into two parts: 𝑓(,Θ)=𝜙(𝜓(,Ψ),Φ), where𝜙(,Φ)is the top layer
of𝑓(,Θ)and𝜓(,Ψ)is the remained part, thus we have Θ=Φ∪Ψ.
The input of 𝜓(,Ψ)is also the𝑚×𝑛matrix presenting original
rankings{𝑟𝑥}𝑥∈A, which is the same of 𝑓(,Θ). The output is a 𝑑×𝑛
matrix of latent representation. For each item 𝑖, the corresponding
column is a 𝑑-dimensional vector. This vector is considered to
encode all information of pxtr scores of 𝑖with current user 𝑢as
context. To emphasize this, we decode original pxtr scores from
the latent representation. The top layer 𝜙(,Φ)is usually a simple
structure that can compress all 𝑑-dimensional vectors into scores,
such as a reduce sum operator ( Φ=∅in this case), or a 1-layer
neural network.
Decoder is denoted as 𝜉(,Ξ), which has a similar structure of
encoder𝜓(,Ψ). The input is the 𝑑×𝑛latent representation matrix
from𝜓(,Ψ), and the output is a 𝑚×𝑛matrix of ranking reconstruc-
tions. We train the model by minimizing the distance of original
rankings and reconstructions, which is defined as cross-entropy
loss (please note that pxtr scores {𝑠𝑥
𝑖}are probability predictions).
𝐿𝑅𝑅=−∑︁
𝑢∑︁
𝑥∈A∑︁
𝑖∈C𝑢
𝑠𝑥
𝑖log(ˆ𝑠𝑥
𝑖)+(1−𝑠𝑥
𝑖)log(1−ˆ𝑠𝑥
𝑖)
,(5)where𝐿𝑅𝑅is for Ranking Reconstruction Loss. {ˆ𝑠𝑥
𝑖}𝑥∈A=𝜉(
𝜓({𝑠𝑥
𝑖}𝑥∈A,Ψ),Ξ)is the reconstruction of {𝑠𝑥
𝑖}𝑥∈A. We adopt
cross-entropy loss instead of mean square error loss in Equation (5)
due to the better accuracy of distribution regression in experiments.
Please note that cross-entropy can be used when the label 𝑠𝑥
𝑖is
numerical probability. In this case, the loss function reaches the
minimum value when ˆ𝑠𝑥
𝑖=𝑠𝑥
𝑖.
user feature
item feature
xtr scores
xtr 
reconstructions
Encoder
Decoder
Top layer
prediction
latent 
representation
Figure 2: Illustration of 𝑓(),𝜙(),𝜓(), and𝜉().
Figure 2 gives a representation of 𝑓(,Θ),𝜙(,Φ),𝜓(,Ψ), and
𝜉(,Ξ).𝜓(,Ψ)is the encoder which compacts the raw features
into the latent representation, where Ψis the set of the model
parameters including such as embedding matrices, and weights and
biases of a fully-connected network. 𝜙(,Φ)is the top layer to obtain
prediction from the latent representation. 𝜉(,Ξ)is the decoder to
reconstruct the raw features from the latent representation. 𝜓(,Ψ)
and𝜙(,Φ)are used as the ranking ensemble model, denoted as
𝑓(,Θ), andΘ=Φ∪Ψ.
4 Unsupervised Ranking Ensemble Models
In this section, we give some examples about optimizing rank-
ing ensemble models by our unsupervised loss to propose the
Unsupervised Ranking Ensemble Models (UREM). Two extensively-
used models are represented in Figures 3(a) and 3(b). Figure 3(a)
illustrates the point-wise model, which is the most widely-used
structure in industrial recommender systems. This type of model
is usually fully-connected structures, inputted with pxtr rankings
and other optional features [ 16]. In the point-wise model, each
item is predicted independently. In contrast, the list-wise model
(shown in Figure 3(b)) takes the candidate set as a whole [ 21]. It is
a transformer structure learning the dependence of all items in the
candidate set. In this way, the model merges rankings by taking
mutual influence between items into consideration. Labels gener-
ally used in this task have been introduced in Introduction. In this
section, we take exposure as an example. Items that are returned to
the user by the recommender system are labeled as 1, and others in
the candidate set are labeled as 0.
6184Unsupervised Ranking Ensemble Model for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
MLP
Transformer
Emb Layer
Emb Layer
...
...
MLP
Transformer
Emb Layer
Emb Layer
...
...
MLP
Transformer
...
MLP
MLP
...
...
...
lvtr
ltr
cmtr
item feature
user feature
RR
loss
RS
loss
RR
loss
RS
loss
i
i
1
i
2
i
3
i
n
i
1
i
2
i
3
i
n
i
1
i
2
i
3
i
n
candidate set
candidate set
candidate set
(a) MLP model
(b) Transformer model
(c) MLP model with UREL
(d) 
Transformer model with UREL
label
1/0
1    1  
  
 0      ...   
 
 
  0
1  
 
  
1  
  
 0      ...   
 
 
  0
1 
  
  1  
  
 0      ...   
 
 
  0
exposed    not exposed
Figure 3: Illustration of point-wise model and list-wise model and the unsupervised version. Black arrows indicate feed forward
and red lines indicate supervision. Hexagons are labels. Dotted ellipses rectangle delineate the scope of data. Gray parts denote
follow-up samples.
4.1 Point-wise Model
In this section, we use bold uppercase letters to refer to matrices
and bold lowercase letters to refer to vectors. In point-wise model,
encoder𝜓(,Ψ), decoder𝜉(,Ξ)and the top layer 𝜙(,Φ)are all
Multi-Layer Perceptron (MLP):
𝜓(,Ψ)=MLP(,{W𝐸𝑛
𝑙,b𝐸𝑛
𝑙}𝑙=1,...,𝐿),
𝜉(,Ξ)=𝜎(MLP(,{W𝐷𝑒
𝑙,b𝐷𝑒
𝑙}𝑙=1,...,𝐿)),
𝜙(,Φ)=𝜎(sum(MLP(,W,b))),
where W𝐸𝑛
𝑙andb𝐸𝑛
𝑙are weight and bias of the 𝑙-th layer of encoder,
andW𝐷𝑒
𝑙andb𝐷𝑒
𝑙are those of decoder. For hidden layers of MLP,
the activation function is ReLU, and for the output layer, there is
no activation function. For 𝜉(,Ξ), we use the sigmoid function
to restrict the output in (0, 1) since the decoder is to predict prob-
abilities (pxtr scores). The top layer 𝜙(,Φ)is a modified 1-layer
MLP. Considering that 𝜙(,Φ)is to predict the probability of ex-
posure, we also use the sigmoid function to restrict the output. In
this case, Ψ={W𝐸𝑛
𝑙,b𝐸𝑛
𝑙}𝑙=1,...,𝐿, andΞ={W𝐷𝑒
𝑙,b𝐷𝑒
𝑙}𝑙=1,...,𝐿, and
Φ={W,b}.
4.2 List-wise Model
In the list-wise model, encoder and decoder are both 1-layer trans-
formers [25].
𝜓(,Ψ)=MLP(LN(
SA(,W𝐸𝑛
𝑞,W𝐸𝑛
𝑘,W𝐸𝑛
𝑣),m𝐸𝑛,s𝐸𝑛),W𝐸𝑛,b𝐸𝑛),
𝜉(,Ξ)=𝜎(MLP(LN(
SA(,W𝐷𝑒
𝑞,W𝐷𝑒
𝑘,W𝐷𝑒
𝑣),m𝐷𝑒,s𝐷𝑒),W𝐷𝑒,b𝐷𝑒)).
Taking encoder as an example, LN and SA indicate layer norm
[1], and self-attention [ 25] respectively. W𝐸𝑛𝑞,W𝐸𝑛
𝑘, and W𝐸𝑛𝑣areweight matrices for query, key, and value. m𝐸𝑛ands𝐸𝑛are trainable
mean and standard deviation for layer normalization. The top layer
𝜙(,Φ)is the same as that in the point-wise model. Inspired by [ 6],
we perform item-level dropout on input to compel the transformer
to learn the item dependency. In this case, Ψ={W𝐸𝑛𝑞,W𝐸𝑛
𝑘,W𝐸𝑛𝑣,
m𝐸𝑛,s𝐸𝑛,W𝐸𝑛,b𝐸𝑛}, andΞ={W𝐷𝑒𝑞,W𝐷𝑒
𝑘,W𝐷𝑒𝑣,m𝐷𝑒,s𝐷𝑒,W𝐷𝑒,
b𝐷𝑒}.
4.3 Loss Function
The loss function of our ranking ensemble models are:
𝐿=𝐿𝑅𝑆+𝜂1𝐿𝑅𝑅+𝜂2𝐿𝐴𝑢𝑥, (6)
where𝐿𝑅𝑆is the Ranking Similarity Loss defined in Equation (4)
and𝐿𝑅𝑅is the Ranking Reconstruction Loss defined in Equation
(5). Auxiliary loss 𝐿𝐴𝑢𝑥is binary cross entropy loss with exposure
label.
𝐿𝐴𝑢𝑥=−∑︁
𝑢∑︁
𝑖∈E𝑢log(𝑠𝑒𝑛𝑠
𝑖)+∑︁
𝑖∈C𝑢−E𝑢log(1−𝑠𝑒𝑛𝑠
𝑖),(7)
whereE𝑢is the set of exposed items of user 𝑢. Though pxtr rankings
are strong to supervise the ranking ensemble models, we can not
abandon exposure label thoroughly due to its unique effect. Here
we analyze the difference between them.
Necessity of exposure supervision. (1) Learning exposure can
let our model fit the whole the system and hence can improve the
effectiveness and consistency of the system. (2) Exposure distribu-
tion should be taken into consideration, which can not be provided
by pxtr rankings.
Necessity of pxtr ranking supervision. (1) Returned by powerful
ranking models, pxtr rankings are informative. Our unsupervised
modular can be regarded as a variant of distillation learning [ 13]. (2)
6185KDD ’24, August 25–29, 2024, Barcelona, Spain Wenhui Yu et al.
Fitting the recommender system leads to regenerative feedback thus
amplifying system bias. pxtr rankings are the distillation of true
user feedbacks, thus reflecting true user preference. (3) Exposure
data is the aggregation of previous rankings, the newest user actions
are not involved.
5 Experiments
We devise experiments to demonstrate the effectiveness of our unsu-
pervised model UREM and unsupervised loss UREL by comparing
them against several state-of-the-art (SOTA) ranking ensemble
models and widely-used supervision signals respectively. UREM
and UREL are validated on two real-world public datasets and the
online application. We focus on answering the following research
questions:
RQ1: How is the performance of the proposed UREM model com-
pared with SOTA baseline models?
RQ2: How is the performance of the proposed UREL loss compared
with SOTA baseline supervisions?
RQ3: How is the performance of UREM on online services?
5.1 Experimental Setup
In this section, we introduce experimental setups, including datasets,
baselines, evaluation protocols, and parameter settings. As intro-
duced before, our model is designed for training and predicting on
candidate sets. However, to the best of our knowledge, there is no
public dataset containing the candidate set, and that maximum data
scope we can get is exposure1. We have to use exposed items to
replace candidate items and use clicked items to replace exposed
items when validating our models on public data. On online data
(e.g., experiments in Section 5.4), we have the candidate set.
It is important to point out that this compromise hurts our model
and impairs the performance of experiments. Our strategies are
based on an assumption that pxtr rankings are more reliable than
exposure labels and pxtr labels. The assumption holds in the scope
of candidate set: (1) for an unexposed item with high pxtr predic-
tions, we believe that the user prefers it, and (2) pxtr labels are
lacked on unexposed items and pxtr rankings provide more infor-
mation. However, in the scope of exposures, this assumption is
invalidate: (1) for an unclicked item with high pxtr predictions, we
tend to believe the user is not interested in it, and (2) pxtr labels
are complete on the exposed items.
5.1.1 Pre-training. There is no pxtr prediction in public datasets
thus we need to train a ranking model to obtain it. We adopt
the Deep Interest Network (DIN) [31] with Multi-gate Mixture-of-
Experts (MMoE) [ 18] and Entire Space Multi-Task Model (ESMM)
[19] structures.
5.1.2 Datasets. We adopt two real-world datasets, KuaiRand2and
Tenrec3.
•KuaiRand: This is collected from the recommendation logs
of the video-sharing mobile app, Kuaishou [ 10]. We adopt
the KuaiRand-1K version.
1Data scopes from large to small are candidate set →exposures→clicks→actions
2https://kuairand.com/
3https://static.qblv.qq.com/qblv/h5/algo-frontend/tenrec_dataset.html•Tenrec: This is collected from the recommendation plat-
forms of Tencent [28]. We adopt the QK-video 1M version.
Table 2: Statistics of datasets
Datasets KuaiRand Tenrec
#user 1,000 999,447
#item 4,369,953 2,310,087
#exposure 11,713,045 120,342,306
#click 4,429,840 28,880,860
#long view 3,069,461 -
#like 182,842 2,275,417
#follow 11,398 179,788
#comment 31,126 -
#share 9,191 250,089
We first introduce the data for the ranking model introduced in
Section Pre-training. We use click samples for training and exposure
samples for inference to simulate the online application. We order
click samples by time and split them into training and validation
sets, to train and tune the model (please note that testing data is
not needed). To train DIN, we construct a user action sequence for
each sample. The pxtr labels are constructed based on if action x is
observed. After training and tuning, we predict pxtr scores for all
exposure samples in the datasets.
For ranking ensemble models, we order exposed items for each
user by time stamp to construct pxtr ranking samples. We split
samples into the training set and test set. We use the training set to
train the model and determine hyperparameters. We abandon the
validation set ranking ensemble models perform similarly on the
training set and validation set.
5.1.3 Compared Models. We compare the following models to
demonstrate the effectiveness of UREM.
•LR:This Logistic Regression model is a shallow linear clas-
sifier activated by the sigmoid function. LR merges pxtr
scores point-wisely. It is indeed a sum formula with learn-
able weight. This model is supervised by the click label.
•MLP: This fully connected Multi-Layer Perceptron (MLP)
is a basic deep structure. MLP also merges pxtr scores in a
point-wise way. This model is supervised by the click label.
•PRM: This Personal Re-ranking Model (PRM) is a trans-
former to model the mutual influence of items in the re-
ranking stage [ 21]. Since we have multiple rankings rather
than one, we remove the position embedding.
•aWELv: This adaptive Weighted Ensemble with List-wise
loss and diversity-based learning model is a generalization of
sum formula 𝑠𝑒𝑛𝑠
𝑖=Í
𝑥𝜔𝑥(𝑢)𝑠𝑥
𝑖with personalized weights
[17], which are learned by a decomposition model: 𝜔𝑥(𝑢)=<
e𝑥,e𝑢>.
•IntEL: ThisIntent-aware ranking Ensemble Learning model
is a generalization of sum formula 𝑠𝑒𝑛𝑠
𝑖=Í
𝑥𝜔𝑥(𝑢,𝑖)𝑠𝑥
𝑖,
where𝜔𝑥(𝑢,𝑖)is a function of user and item. When leaning
𝜔𝑥(𝑢,𝑖), user intent and context information of the candidate
set are considered [16].
6186Unsupervised Ranking Ensemble Model for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Performance of all models (testing set)
Data-
ModelNDCG of Action label Pxtr ranking
set Click Long view Like Follow Comment Share Long view Like Follow Comment ShareKuaiRandLR 0.8480 0.8042 0.0967 0.0093 0.0448 0.0123 0.9779 0.8522 0.9141 0.8313 0.8938
MLP 0.8528 0.7743 0.0909 0.0095 0.0471 0.0148 0.9454 0.8425 0.9188 0.8419 0.8821
PRM 0.8543 0.7706 0.0943 0.0085 0.0453 0.0128 0.9419 0.8514 0.9255 0.8493 0.8823
UREM_PRM 0.8589 0.8159 0.1004 0.0114 0.0569 0.0161 0.9800 0.8681 0.9282 0.8495 0.9228
aWELv 0.8698 0.8088 0.0847 0.0091 0.0408 0.0121 0.9470 0.8423 0.9257 0.8442 0.8837
IntEL 0.8830 0.8005 0.0850 0.0089 0.0374 0.0113 0.9452 0.8500 0.9296 0.8540 0.8850
UREM_IntEL 0.9006 0.8441 0.0890 0.0100 0.0417 0.0124 0.9681 0.8621 0.9298 0.8535 0.9131T
enrecLR 0.5970 - 0.0848 0.0152 - 0.0284 - 0.6466 0.5123 - 0.5984
MLP 0.6933 - 0.0967 0.0140 - 0.0339 - 0.6760 0.5061 - 0.6174
PRM 0.6980 - 0.0965 0.0151 - 0.0328 - 0.6813 0.5124 - 0.6251
UREM_PRM 0.6953 - 0.0998 0.0152 - 0.0378 - 0.7037 0.5506 - 0.6542
aWELv 0.7381 - 0.0981 0.0147 - 0.0334 - 0.6863 0.5173 - 0.6181
IntEL 0.7554 - 0.0972 0.0144 - 0.0323 - 0.6829 0.5103 - 0.6054
UREM_IntEL 0.7571 - 0.0983 0.0151 - 0.0336 - 0.6879 0.5190 - 0.6248
•UREM: These Unsupervised Ranking Ensemble Models
are proposed in this paper. We use the most competitive
baselines, PRM and IntEL as the basic models to propose
UREM_PRM and UREM_IntEL.
All baselines are trained with click label and our UREM models
are trained with UREL loss.
5.1.4 Compared Supervisions. We compare the following supervi-
sion signals to demonstrate the effectiveness of our UREL loss.
•PO:In real-world applications, the Primary Objective is the
most widely-used supervision signal. In short video recom-
mendation, the most important objective is long view, and in
e-commerce recommendations, the most important objective
is click.
•EXP: EXPosures are also extensively used to train the model
to fit the whole system. Exposure labels are comprehensive
since the system generates them by taking all user actions
into consideration. On public datasets, there is no negative
samples of exposure data thus we use clicks instead.
•MO: We train the model by Multi-Objective loss with both
exposure labels and pxtr labels. It is more comprehensive
and informative compare with EXP.
•UREL: Unsupervised Ranking Ensemble Loss proposed in
this paper. We use the input of the model as the supervision.
5.1.5 Evaluation Protocols. For each sample in the test set, we rank
all items for the user. We apply normalized discounted cumulative
gain (NDCG) to evaluate the recommendation quality. NDCG is a
position-sensitive metric extensively used to measure ranking qual-
ity. For each user action x, we use action labels and pxtr rankings to
evaluate the models. Action labels are true user feedbacks and thus
are the most reliable signals for evaluation, however, only clicked
samples have action labels. Pxtr rankings are considered reliable in
ranking ensemble tasks thus we additionally use them to evaluate
models.5.1.6 Parameter Setting. For a fair comparison, our proposed mod-
els and all baselines in our experiments are all tuned with the follow-
ing strategies: The maximum iteration number is set to 50. In each
iteration, we enumerate all data to train the model and then test
it. We repeat the experiment 5 times to get confidence results. The
best learning rate 𝜂are found in the range {0.0001,0.001,0.01,0.1,1}
for all models. The batch size is set as 10,000 to utilize GPU.
5.2 Performance of UREM Models (RQ1)
In this section, we compare two UREM models against baselines
on two public datasets and report the result in Table 3. LR is the
basic sum formula that is widely used in industrial recommender
systems. We learn the parameters of the weighted sum instead
of adjusting them manually. MLP is a deep structure and hence is
powerful in learning a complex nonlinear mapping, thus performing
better than LR obviously. MLP outperforms LR 16.13% on click for
the best case. However, MLP fails to outperform LR on all actions
on KuaiRand. The same phenomenon is observed between any
pair of baseline models. For example, by learning item matching,
PRM outperforms MLP 0.68% on click for the best case, however,
PRM underperforms MLP on some other actions. Similarly, aWELv
and IntEL both achieve further improvement on click yet perform
worse on other actions. Models that fit well on click labels show
poor generalization on other aspects. In this case, improvement of
the model is indeed a exchange of the performance on click and
performance on other actions.
Differently, supervised by UREL loss, our UREM models achieve
uniform improvement across all metrics (achieve improvements
on all actions). Considering that the baseline models perform dif-
ferently, we seperate them into two groups ({LR, MLP, PRM} and
{aWELv, IntEL}) and design different UREM models (UREM_PRM
and UREM_IntEL) to compare with them resepectively. As show
in Table 3, UREM_PRM (almost) dominates4LR, MLP, and PRM.
UREM_IntEL dominates aWELv and IntEL. Also, we improve the
performance of the model without using complex structures, i.e.,
4performs better on all actions
6187KDD ’24, August 25–29, 2024, Barcelona, Spain Wenhui Yu et al.
Table 4: Performance of all supervisions (testing set of KuaiRand)
SupervisionNDCG of Action label Pxtr ranking
Click Long view Like Follow Comment Share Long view Like Follow Comment Share
Primary metric (PM) 0.8317 0.8107 0.0797 0.0089 0.0425 0.0126 0.9921 0.8433 0.9130 0.8199 0.9082
UREL_P 0.8398 0.8110 0.1134 0.0109 0.0625 0.0184 0.9903 0.8936 0.9303 0.8558 0.9434
Exposure (EXP) 0.8528 0.7743 0.0909 0.0095 0.0471 0.0148 0.9454 0.8425 0.9188 0.8419 0.8821
Multi-objective (MO) 0.8522 0.7733 0.0920 0.0098 0.0495 0.0147 0.9446 0.8443 0.9193 0.8430 0.8787
UREL_E&M 0.8531 0.7890 0.0927 0.0099 0.0492 0.0152 0.9583 0.8474 0.9198 0.8404 0.8926
without any online complexity increment. For the hyperparameter
setting, taking KuaiRand as the example, we set 𝜂1=0.1,𝜂2=0.5
in Equation (6), and 𝜔𝑥=1.0in Equation (4) for all action x.
5.3 Performance of UREL loss (RQ2)
In this section, we demonstrate the effectiveness of the proposed
UREL loss by comparing it against several widely-used supervisions
and show the result in Table 4. In this experiment, we select MLP
as the basic model, and we only report the results on KuaiRand to
save space. As we can see, trained by different supervision signals,
baselines perform well on corresponding actions. For example, PM
performs well on long views, and EXP performs well on clicks, and
MO balances all actions.
Considering that it is incomparable if UREL outperforms a certain
baseline on some actions yet underperforms it on other actions, we
tune different versions of UREL for different baselines. We enlarge
𝜔𝑙𝑣to get the long view version UREL_P to compare with PM and
increase𝜂2to get the click version UREL_E&M to compare with
EXP and MO. As we can see, our URELs (almost) dominate the
corresponding baselines.
We also conduct ablation experiments to demonstrate the effec-
tiveness of each loss UREL and report the results in Table 5. We
adopt PRM as the basic model, and only report the result of action
labels on KuaiRand dataset to save space.
Table 5: Ablation experiments (testing set of KuaiRand)
With- Action label
out Click Long view Like Follow Comment Share
-0.8589 0.8159 0.1004 0.0114 0.0569 0.0161
𝐿𝑅𝑆 0.8542 0.7778 0.0884 0.0082 0.0401 0.0131
𝐿𝑅𝑅 0.8560 0.8139 0.1004 0.0111 0.0574 0.0157
𝐿𝐴𝑢𝑥 0.8280 0.7915 0.0883 0.0095 0.0403 0.0123
First, we can see that the ranking similarity loss 𝐿𝑅𝑆is very
important since the model performs worse on all metrics without it.
The ranking reconstruction loss 𝐿𝑅𝑅also leads to further improve-
ment. The aux loss 𝐿𝐴𝑢𝑥is also necessary since user actions are
dependent on clicks, i.e., if an item is not clicked by the user, further
actions such as long view or like can not happen. Only 𝐿𝐴𝑢𝑥tells
models the distribution of clicks.5.4 Performance of Online Application (RQ3)
We conduct AB test and report the relative improvements and
confidence intervals in Table 6. We compare our model against
the basic ranking ensemble model. These two models have the
same model structure, and the only difference is that the baseline
is optimized by EXP and our model is optimized by UREL.
Table 6: Effectiveness of UREL in online application
Metric Watch time Exposure
Improvement+1.102% +0.636%
[-0.16%, 0.16%] [-0.12%, 0.12%]
Since watch time is the most important goal in our application
scenario, we set a large weight 𝜔𝑙𝑣for long view. Benefiting from
emphasizing long view, UREL achieves a very impressive improve-
ment in watch time over EXP, which exceeds the confidence interval
significantly. One thing that needs to point out is that UREL and
EXP perform similarly on other actions, thus the improvement of
long view does not hurt the metric of other actions. To our surprise,
UREL also outperforms EXP on exposure significantly. EXP is opti-
mized by exposure label and thus should perform well on exposure.
The possible explanation is that user actions contribute positively
to exposure.
The AB test is observed for 7 days on 10% users to get reliable
result. Based on these promising results, we fully launch UREL in
the industrial recommender system.
6 Conclusion
In this paper, we design a novel unsupervised paradigm to train
ranking ensemble models. We argued that all widely-used methods
are sub-optimal, leading to information loss in the final output.
To make sure that the ensemble ranking contains the sequential
information of all input rankings, we compel the ensemble rank-
ing has a similar order to all pxtr rankings. And to let that the
model captures all information, we decode the original value of
pxtr rankings by a decoder. We conduct experiments on public
offline datasets. Experiments show that UERM outperforms SOTA
models and UREL outperforms SOTA supervisions significantly.
Also, we gain impressive improvement in the online application.
For future work, we are interested in combining UREL with other
supervisions. We also aim to explore the effectiveness of UREL on
other recommendation models with numerical features.
6188Unsupervised Ranking Ensemble Model for Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 (2016).
[2]Michał Bałchanowski and Urszula Boryczka. 2022. Aggregation of Rankings
Using Metaheuristics in Recommendation Systems. Electronics 11, 3 (2022), 369.
[3]Brian T Bartell, Garrison W Cottrell, and Richard K Belew. 1994. Automatic
combination of multiple ranked retrieval systems. In SIGIR’94: Proceedings of the
Seventeenth Annual International ACM-SIGIR Conference on Research and Devel-
opment in Information Retrieval, organised by Dublin City University. Springer,
173–181.
[4]Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang.
2020. Controllable Multi-Interest Framework for Recommendation. In KDD ’20:
The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
Virtual Event, CA, USA, August 23-27, 2020. ACM, 2942–2951.
[5]P-C-F Daunou. 1803. Mémoire sur les élections au scrutin. Histoire de l’Academie
Royale des Sciences pour (1803).
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT.
Association for Computational Linguistics, 4171–4186.
[7]Cynthia Dwork, Ravi Kumar, Moni Naor, and D Sivakumar. 2001. Rank aggrega-
tion revisited.
[8]Ronald Fagin, Ravi Kumar, and Dandapani Sivakumar. 2003. Efficient similarity
search and classification via rank aggregation. In Proceedings of the 2003 ACM
SIGMOD international conference on Management of data. 301–312.
[9]Mohamed Farah and Daniel Vanderpooten. 2007. An outranking approach for
rank aggregation in information retrieval. In Proceedings of the 30th annual
international ACM SIGIR conference on Research and development in information
retrieval. 591–598.
[10] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,
Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recom-
mendation Dataset with Randomly Exposed Videos. In Proceedings of the 31st
ACM International Conference on Information & Knowledge Management, Atlanta,
GA, USA, October 17-21, 2022. ACM, 3953–3957.
[11] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In Proceedings of the 43rd International ACM SIGIR con-
ference on research and development in Information Retrieval, SIGIR 2020, Virtual
Event, China, July 25-30, 2020. ACM, 639–648.
[12] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International
Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017. ACM,
173–182.
[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in
a Neural Network. Computer Science (2015).
[14] Yehuda Koren. 2009. The bellkor solution to the netflix grand prize. Netflix prize
documentation 81, 2009 (2009), 1–10.
[15] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,
Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-Interest
Network with Dynamic Routing for Recommendation at Tmall. In Proceedings of
the 28th ACM International Conference on Information and Knowledge Management,
CIKM 2019, Beijing, China, November 3-7, 2019. ACM, 2615–2623.
[16] Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian
Feng, and Daiyue Xue. 2023. Intent-aware Ranking Ensemble for Personalized
Recommendation. In Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan,
July 23-27, 2023. ACM, 1004–1013.
[17] Hongzhi Liu, Yingpeng Du, and Zhonghai Wu. 2022. Generalized ambiguity
decomposition for ranking ensemble learning. The Journal of Machine Learning
Research 23, 1 (2022), 3804–3839.
[18] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018.
Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-
of-Experts. In Proceedings of the 24th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018.
ACM, 1930–1939.
[19] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. 1137–1140.
[20] Samuel E. L. Oliveira, Victor Diniz, Anísio Lacerda, and Gisele L. Pappa. 2016.
Evolutionary rank aggregation for recommender systems. In IEEE Congress on
Evolutionary Computation, CEC 2016, Vancouver, BC, Canada, July 24-29, 2016.
IEEE, 255–262.
[21] Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun,
Jian Wu, Peng Jiang, Junfeng Ge, Wenwu Ou, and Dan Pei. 2019. Personalized
re-ranking for recommendation. In Proceedings of the 13th ACM Conference on
Recommender Systems, RecSys 2019, Copenhagen, Denmark, September 16-20, 2019.
ACM, 3–11.
[22] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong
sequential behavior data for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management.
2685–2692.
[23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI 2009,
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,
Montreal, QC, Canada, June 18-21, 2009. AUAI Press, 452–461.
[24] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive
layered extraction (ple): A novel multi-task learning (mtl) model for personalized
recommendations. In Proceedings of the 14th ACM Conference on Recommender
Systems. 269–278.
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA. 5998–6008.
[26] Wenhui Yu and Zheng Qin. 2020. Graph Convolutional Network for Recommen-
dation with Low-pass Collaborative Filters. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Pro-
ceedings of Machine Learning Research, Vol. 119). PMLR, 10936–10945.
[27] Wenhui Yu, Zixin Zhang, and Zheng Qin. 2022. Low-Pass Graph Convolutional
Network for Recommendation. In Thirty-Sixth AAAI Conference on Artificial
Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of
Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances
in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022.
AAAI Press, 8954–8961.
[28] Guanghu Yuan, Fajie Yuan, Yudong Li, Beibei Kong, Shujie Li, Lei Chen, Min Yang,
Chenyun Yu, Bo Hu, Zang Li, et al .2022. Tenrec: A Large-scale Multipurpose
Benchmark Dataset for Recommender Systems. In Advances in Neural Information
Processing Systems. 11480–11493.
[29] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan
Huang, and Xianfeng Tan. 2022. Multi-task fusion via reinforcement learning for
long-term user satisfaction in recommender systems. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4510–4520.
[30] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate
Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019.
AAAI Press, 5941–5948.
[31] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059–1068.
6189