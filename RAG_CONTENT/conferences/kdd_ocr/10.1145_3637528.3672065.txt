FoRA
G: Factuality-optimized Retrieval Augmented Generation
for Web-enhanced Long-form Question Answering
Tianchi Cai
Ant Group
Hangzhou, China
tianchicai@gmail.comZhiwen Tan
Ant Group
Hangzhou, China
ender.tzw@antgroup.comXierui Song
Ant Group
Hangzhou, China
songxierui.sxr@antgroup.com
Tao Sun
Ant Group
Hangzhou, China
suntao.sun@antgroup.comJiyan Jiang
Tsinghua University
Beijing, China
scjjy95@outlook.comYunqi Xu
Ant Group
Hangzhou, China
xuyunqi.xyq@antgroup.com
Yinger Zhang
Ant Group
Hangzhou, China
zhangyinger@zju.edu.cnJinjie Gu
Ant Group
Hangzhou, China
jinjie.gujj@antgroup.com
ABSTRACT
Retrieval Augmented Generation (RAG) has become prevalent in
question-answering (QA) tasks due to its ability of utilizing search
engine to enhance the quality of long-form question-answering
(LFQA). Despite the emergence of various open source methods
and web-enhanced commercial systems such as Bing Chat, two
critical problems remain unsolved, i.e., the lack of factuality and
clear logic in the generated long-form answers. In this paper, we
remedytheseissuesviaasystematicstudyonanswergenerationin
web-enhancedLFQA.Specifically,wefirstproposeanoveloutline-
enhancedgeneratortoachieveclearlogicinthegenerationofmul-
tifaceted answers and construct two datasets accordingly. Then
we propose a factuality optimization method based on a carefully
designed doubly fine-grained RLHF framework, which contains
automatic evaluation and reward modeling in different levels of
granularity. Our generic framework comprises conventional fine-
grainedRLHFmethodsasspecialcases.Extensiveexperimentsver-
ifythesuperiorityofourproposed Factuality-optimized RAG (FoRAG)
method on both English and Chinese benchmarks. In particular,
when applying our method to Llama2-7B-chat, the derived model
FoRAG-L-7B outperforms WebGPT-175B in terms of three com-
monly used metrics (i.e., coherence, helpfulness, and factuality),
while the number of parameters is much smaller (only 1/24 of that
ofWebGPT-175B).Ourdatasetsandmodelsaremadepubliclyavail-
able for better reproducibility.1
CCS CONCEPTS
•Computing methodologies →Natural language generation.
1https://huggingface.co/forag
This
work is licensed under a Creative Commons Attribu-
tion International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672065ACM Reference Format:
Tianchi Cai, Zhiwen Tan, Xierui Song, Tao Sun, Jiyan Jiang, Yunqi Xu,
Yinger Zhang, and Jinjie Gu. 2024. FoRAG: Factuality-optimized Retrieval
Augmented Generation for Web-enhanced Long-form Question Answer-
ing . In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3637528.3672065
1 INTRODUCTION
RetrievalAugmentedGeneration(RAG),atechniquethataugments
Large Language Models (LLMs) with a retriever by appending the
retrievedrelevantpassagestothecurrentcontext[32],hasrecently
attractedconsiderableresearchattention[6,16].Theaccesstosearch
engine supplements massive and latest knowledge to LLMs, boost-
ing their performance on various knowledge intensive tasks, such
as open domain dialog [45] and question answering (QA) [43].
Following this paradigm, many web-enhanced commercial sys-
tems have been developed, such as Bing Chat2and perplexity.ai.3
They generate answers to user queries in natural language with
reference to web pages, which we refer to as the web-enhanced
long-form question-answering (LFQA) task. Although these sys-
temscangeneratecoherentandhelpfulanswers,recentresearches
have revealed the low factuality issue of these systems, such that
only about half of the statements generated are fully supported by
the retrieved references [14, 28]. This poses an unignorable threat
to the trustworthiness of existing web-enhanced LFQA systems.
Despite its prevalence, there lacks an effective method to opti-
mize factuality in web-enhanced LFQA as far as we are concerned.
There are two intrinsic difficulties. First, previous studies mostly
relyonhumanevaluation[28,31,34],whichisgenerallyexpensive
to acquire. In web-enhanced LFQA task, factuality is even more
time-consuming and difficult to manually annotate compared to
other language generation tasks, since it involves comparing the
factual details of two lengthy texts [50]. Second, the most com-
monly used fine-tuning method for human preference alignment,
2Bing
Chat: https://www.bing.com/chat.
3perplexity.ai: https://www.perplexity.ai.
 
199
KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
……Web Page 1……Web Page NHow do ancient civilizations get covered by dirt, sand, etc?
The reason ancient cities get buried is because of a survivorship bias. Most ancient cities got buried under the dust and rubble of structures that had collapsed over the centuries and millennia that followed their……I. Natural Processes:Wind and dust storms, Erosion, ……II. Human Activities:Abandonment, Agriculture, ConstructionIII. Conclusion:Variation in burial rates and processes,……Holistic-levelSentence-levelSubclaim-level
… buildings and structures … 𝒓"𝒊=argmin𝒓#𝒊 Loss(𝒓"𝒊,𝒓𝒊) =argmin𝑹%𝒊,𝒋 Loss(Agg&(𝑹2𝒊,𝒋 ),𝒓𝒊) Reward Model Granularity𝑹2𝒊,𝒋Sequence-levelToken-level……Web-Enhanced Input
Relevant Passage 1
PPORelevant Passage N……Segmentation+Retrieval
Outline-Enhanced GeneratorFactuality Optimization
GeneratorQuerySearch EngineReferenceAnswerCause-EffectOrganizational PatternOutline
Answer
𝑹2𝒊,𝒍𝑹2𝒊,𝒍*𝟏𝑹2𝒊,𝒍*𝟐𝑹2𝒊,𝒍*𝟑Evaluation Granularity
There are several ways ancient civilizationscan get covered by dirt……Natural processes:Wind and dust storms: Over time, wind can carry large amounts …………Human activities:Abandonment: When a civilization is abandoned, buildings and structures are no longer maintained and …………Conclusion:Variation in burial rates and processes,……There are several ways ancient civilizationscan get covered by dirt……
……𝑟=✓
𝒓𝒊	=✓𝒓𝟐	=✕𝒓𝟏	=✓Buildings are no longer maintained.A civilization is abandoned.𝑹𝒊,𝟏=𝑹𝒊,𝟐=✓✕𝒓𝒊=Agg&																	=Agg&1,0,1=𝟎.𝟔𝟔	……𝑹𝟐,𝟏=𝒓𝟐=Agg&𝑹𝟐,𝒋=0.5𝑹𝟐,𝟐=✓✕……𝑹𝟏,𝟏=𝒓𝟏=Agg&𝑹𝟏,𝒋=𝑹𝟏,𝒉=✓✓✓
GPT-4Evaluation
…………Structures are no longer maintained.𝑹𝒊,𝟑=✓subclaim i2subclaim i3subclaim i1When a civilization is abandoned, buildingsand structures are no longer maintained and ……sentence iNatural processes……sentence 2sentence 1There are several ways ……
sentence isentence i✓✕✓
Figur
e 1: Illustrations of the input for LLM in web-enhanced LFQA task (upper left), the existing generator (lower left), our
outline-enhanced generator (middle) and our doubly fine-grained factuality optimization method (right). Before generating
a long answer, the outline-enhanced generator first drafts an organization pattern and an outline to promote a clear logic for
generation. The doubly fined-grained RLHF optimizes factuality by incorporating fine-grained designs on two core steps, i.e.
factuality evaluation and reward modeling, with methods on multiple levels of granularities proposed on each step.
i.e., Reinforcement Learning from Human Feedback (RLHF), con-
ventionally adopts the holisticreward, such that each answer only
has a single evaluation score. Such a reward provides a relatively
sparse training signal, which undermines the reliability of RLHF
[40, 50]. In web-enhanced LFQA, the sparsity problem is even ex-
aggerated, as the answers are in long form.
Besides the above factuality issue, different from conventional
QA tasks with short answers, web-enhanced LFQA poses extra
challengesduetothepervasiveambiguityofmanyreal-worldques-
tions. A desirable answer to these questions is preferred to be mul-
tifaceted[ 2],whichrequiresorganizingandconsolidatinginforma-
tion from multiple aspects and references [25]. The problem might
be one possible reason why existing open source methods such as
WebGLM [29] have no explicit improvement over closed source
methods such as WebGPT-175B.
To resolve the above issues, in this paper, we conduct a system-
atic study of web-enhanced LFQA. Specifically, we first propose
a novel outline-enhanced generator, which achieves clear logic in
the generation of multifaceted answers. We then propose an inno-
vative factuality optimization approach based on a novel doubly
fine-grained RLHF framework. Specifically, we design new auto-
matic evaluation and reward modeling steps in different granular-
ities, which allows to optimize factuality for RAG in a flexible way.
Our generic method contains several existing fine-grained RLHF
methods as special cases.
Extensiveexperimentsdemonstratetheeffectivenessofourpro-
posedmethod,whichachievesstate-of-the-artperformanceonbothChineseandEnglishbenchmarks.Specifically,theoutline-enhanced
generator significantly improves the coherence and helpfulness,
while the doubly fine-grained factuality optimization method sub-
stantiallypromotesthefactualityonbothanswerandsentencelev-
els. Remarkably, applying our method to Llama2-7B-chat yields a
fine-tuned model FoRAG-L-7B, which, for the first time, surpasses
theWebGPT-175Boncoherence,helpfulness,andfactuality,while
the number of parameters of FoRAG-L-7B is much smaller (only
1/24 of that of WebGPT-175B).
The contributions of this work are summarized as follows:
•We propose a new outline-enhanced generator to promote
a clear logic of long answer generation in RAG, which sig-
nificantly improves the coherence and helpfulness of the
generatedanswers.Twolarge-scaleoutline-enhancedLFQA
datasets are accordingly constructed.
•We propose a novel factual optimization method for web-
enhanced RAG based on a novel doubly fine-grained RLHF
framework, which eschews the need of expensive human
annotation.
•Weconductextensiveexperimentstoshowthatourmethod
achieves state-of-the-art performance on both Chinese and
English benchmarks. Notably, the FoRAG-L-7B model fine-
tuned by our method outperforms WebGPT-175B on coher-
ence, helpfulness, and factuality, with only 1/24 of the pa-
rameters in WebGPT-175B. Both datasets and models are
publicly available for better reproducibility.
 
200FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering KDD ’24, August 25–29, 2024, Barcelona, Spain
2 RELATED WORK
In this section, we review prior work in three related fields, i.e.,
open-domainquestionanswering,retrievalaugmentedgeneration,
as well as web-enhanced LFQA.
Open-domain Question Answering .Inthefieldofopen-domain
QA, traditional efforts have centered around reading comprehen-
sion techniques, with foundational datasets like SQuAD [38] pro-
vidinghuman-writtenquestionsandextractedanswers.Subsequent
datasets,includingNaturalQuestions[26],TriviaQA[20],andCoQA
[41], continue this trend but largely cater to brief answers. Recog-
nizing the value of more informative, long-form responses, recent
initiatives such as ELI5 [ 12] have begun to compile questions de-
mandingdetailedanswers,promptingresearchintoadvancedgen-
erative techniques to meet this need.
Retrieval-Augmented Generation .Retrieval-AugmentedGen-
eration (RAG) enhances language model (LM) performance by in-
tegrating external knowledge retrieval with in-context learning.
The knowledge retrieval techniques in RAG include sparse meth-
ods such as BM25 and TF-IDF and dense retrieval systems, includ-
ing DPR [22] and Contriever [18]. To utilize the retriever, vari-
ous methods are proposed. REALM proposes a joint optimization
of retriever and language modeling [16]. Retro uses a frozen re-
triever to enhance the generation ability model with a novel chun-
kedcross-attentionmechanism[6].Atlasstudiesthefew-shotabil-
ity for RAG models. Others combine black-box LMs with custom
or fine-tuned retrieval systems [39, 42]. Different from these work,
we treat the retrieval step as a black box and focus on improving
the generation quality given the query and retrieved passages.
Web-enhanced LFQA. The web-enhanced LFQA takes a new
approach to QA tasks which utilizes the retrieval ability of search
engine to improve the generation performance on long-form QA.
Closely related to our work,WebGPT [34] uses the questions from
ELI5 and explores the ability of LLMs in navigating through the
web, retrieving passages from web, and generating long-form re-
sponses. Despite its notable capabilities, the dataset, and models
are not accessible to the public. Following this idea, WebCPM [37]
builds and releases an open-source web-enhanced LFQA model in
Chinese.WebGLM[29]providesanmoreefficientandcost-effective
method by replacing the expert annotation with evaluations us-
ing LLMs and utilizing a non-interactive way to use search en-
gine.However,itsresultingmodel,theWebGLM-10Bdoesnotout-
performs the WebGPT-175B. Compared to these works, we con-
sider optimizing the logic structure and factuality of the genera-
tion, which has not been studied in web-enhanced LFQA as far as
we know.
3 PRELIMINARY
Inthissection,webrieflyreviewtheRAGpipelineinweb-enhanced
LFQAtask,whichforsimplicityofpresentation,weadopttheterm
web-enhanced RAG to describe in the sequel. The web-enhanced
RAG pipeline is demonstrated in the left column of Figure 1.
In web-enhanced RAG, for a given user input 𝑥, the system first
utilizes a web search engine to retrieve a list of relevant website
URLs, then crawls the websites and extracts the relevant text seg-
ments 𝑧, which are usually called reference orcontextfor gener-
ation [37]. This extraction is commonly done by first segmentingthewebpagesintotextsegmentsandthenusingpre-traineddense
retrievers to extract the top-k segments [29].
After deriving the context 𝑧, the RAG system generates an an-
swer 𝑦basedonthecontext 𝑧andtheuserquery 𝑥.Following[40],
the response generation can be formulated as a Markov Decision
Process (MDP) <S,A,R, 𝑃, 𝛾>. In such a process, each episode
starts with a sampled state 𝑠∈ S, where 𝑠=(𝑥, 𝑧)is a prompt that
contains a query 𝑥and a relevant context 𝑧(here the parenthesis
denotes string concatenation). At each step 𝑡during this episode,
thestate 𝑠𝑡=(𝑥, 𝑧, 𝑎 1, ..., 𝑎 𝑡−1)isdescribedbythequery 𝑥,thecon-
text 𝑧,andallthepreviouslygeneratedtokens (𝑎1, ..., 𝑎 𝑡−1),which
is denoted 𝒂𝑡−1for short. Given the state 𝑠𝑡, the LLM, denoted by
𝜋𝜃, defines a probability distribution 𝑎∼𝜋𝜃(·|𝑠𝑡)over all tokens
𝑎∈ Aconditioned on the current state 𝑠𝑡, where 𝜃denotes the
trainable parameters of the LLM. After generating the specific to-
ken 𝑎𝑡∈ A, the state will transit to 𝑠𝑡+1=(𝑠𝑡, 𝑎𝑡)at the next time
step 𝑡+1by appending the latest generated 𝑎𝑡token to the cur-
rent state 𝑠𝑡. This episode terminates when the length 𝑡exceeds
a pre-defined threshold 𝑇or an end-of-sequence token is gener-
ated. In the above definition of MDP, the parameter 𝛾is a discount
factor. In most of the language generation tasks, we have a task
specificevaluationmetric R(𝒂𝑇, 𝑥, 𝑧)thatdependsonthefinalcon-
text 𝑠𝑇=(𝑥, 𝑧, 𝑎 1, ..., 𝑎 𝑇)whichconsistsofthegeneratedsequence
𝒂𝑇and the initial context 𝑥, 𝑧, which is typically given at the end
of sequence to reflect the quality of the generated sequence, e.g.,
whetherthesequenceishelpfulorharmless[ 4].Dependingonthe
evaluation granularity, R(𝒂𝑇, 𝑥, 𝑧)might be a scalar or a vector or
even a matrix, we denotes the three cases by 𝑟,𝒓,𝑹, respectively
(see later explanations in Section 5.2) .
Besides helpfulness or harmlessness, one crucial criterion of re-
sponsegenerationinRAGisfactuality(orverifiability),whichlever-
ages the extent to which the generated response is trustful. In gen-
eral, the response 𝑦is considered to be truthful if its contents are
factuallyconsistentwiththeretrievedtext 𝑧[48].Inmostprevious
works[33,47],factualityismainlyevaluatedbyhumanannotation
or via the NLI model (i.e., whether the context can entail the infor-
mation contained by the response) [53] or general purpose LLMs
[17, 21, 27], such as ChatGPT [36] or GPT4 [35].
4 OUTLINE-ENHANCED RAG
In this section, we propose the Outline-Enhanced generation tech-
nique, which is able to generate well-structured responses of high
quality. Illustrated in Figure 1 (middle), the outline-enhanced gen-
erator takes a two-stage generation, where the generator first gen-
erates an organizational pattern and outline to improve the logic
structure. In the following, we describe our technique and the cor-
responding construction of two outline-enhanced LFQA datasets.
4.1 Outline-Enhanced Generator
In most existing open-source methods [29, 34, 37], the responses
aredirectlygenerated,i.e.,theretrievedcontentsareconcatenated
with the original query and fed into a generation model using cer-
tain prompt template (Figure 1 lower left). Compared to those gen-
erated by closed-source methods, these responses are shorter and
often found unorganized, lacking a clear logical structure.
4W
e utilize the tokenizer of https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
 
201KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
Table 1: The statistics of web-enhanced long-form QA
datasets. Applying our outline-enhanced generation tech-
nique yields significantly longer answers (Ans. (OE)) com-
pared with the original answer (Ans. (Ori)).
DatasetA
vg. Length (in Tokens)4
# Samples
Quer
y Ans. (Ori.) Ans. (OE)
W
ebCPM (zh) 44.1 374.1 623.1 5,500
W
ebGLM (en) 17.8 151.3 409.0 43,579
W
ebGPT-13b (en) 20.9 212.4 407.4 272
W
ebGPT-175b (en) 20.9 208.9 414.2 272
T
o enhance the performance, one possible way is to make the
responses more organized. Indeed, some researchers have found
that carefully designed prompts that comprise task descriptions
andafewdemonstrationswillimprovethequalityofthegenerated
responsesonvarioustasks[7].Forexample,thetechniqueof“Let’s
thinkstepbystep”[23]substantiallyimprovestheperformanceby
encouraging the chain-of-thought reasoning ability.
Inspiredbytheaboveworks,weintroducetheoutline-enhanced
technique into response generation. Our proposed generator in-
cludes an outline stage and an expansion stage, which aligns with
the intuition that when answering a question, human usually first
outlines and organizes the answer before expanding each point.
Specifically,togeneratehigh-qualityoutputwithaclearlogicflow,
we prompt the model to first output an outline of the final answer,
andthenconcatenatethedraftintotheprompttogeneratethefull
response. In the following, we elaborate the two stages in detail.
Outline Stage. Inthisstage,thegeneratorfirstdraftsanoutline
of the answer using an outline template, with the user query 𝑥
and context 𝑧as input. The outline template guides the LLM to
first consider which organizational pattern is best suitable to the
currentquestion,e.g.,“causeandeffect”or“compareandcontrast”.
Then the LLM uses the organizational pattern to output an outline.
For example, when the selected pattern is “compare and contrast”,
the generated outline will include various perspectives that will
later be used to expand on the similarities and differences.
Expansion Stage. Basedontheoutlinegeneratedattheformer
stage, the LLM expands each perspective to construct the final an-
swer. Specifically, The model is then asked to generate an answer
tothequestion,giventheinputcontainingthequery 𝑥,thecontext
𝑧and the outline 𝑜generated in the first stage.
Thetrainingoftheoutline-enhancedgeneratorfollowsthestan-
dardsupervisedfine-tuning(SFT)procedure,whichiswidelyadopted
in previous works [4, 36].
4.2 Outline-Enhanced Long-Form QA Dataset
Asfarasweknow,thereareonlytwoopen-sourcedweb-enhanced
long-form QA datasets available for training web-enhanced RAG
models.5The English dataset, i.e. the WebGLM-QA [29], contains
44𝑘samples, while the Chinese dataset, i.e. WebCPM [37], con-
tains 5,500samples.Thequeriesinbothdatasetsaresampledfrom
5The
WebGPT demo website contains 272 samples, which can be used for evaluation
but not sufficient for training.ELI5 [12], where WebGLM-QA sample question from it, and We-
bCPM additionally uses human annotators to translate the ques-
tion into Chinese. The Web search engine are used to collect rele-
vant passages.
Weconstructanoutline-enhancedbilinguallong-formQAdataset
using the queries and relevant passages from these two datasets.
We apply our outline-enhanced generation technique using GPT4
[1] to collect outline-enhanced answers. We design a prompt to in-
structGPT4toexecutetheoutlinestageandtheexpansionstagein
a step-by-step manner, which is provided in Appendix A. The de-
tailed statistics of the existing datasets and our outline-enhanced
answers are presented in Table 1. It is clear that our demonstra-
tion answers are much longer than that in existing works, due to
thestrongerlogicstructureTheoutline-enhancedanswersderived
from WebCPM and WebGLM are publicly available.1
Note that the imbalance of the amount of training samples in
English and Chinese datasets may pose difficulty to train a bilin-
gual web-enhanced RAG model. To overcome this difficulty, we
further collect 39𝑘queries and relevant passages in Chinese from
public sources, and follow the same process to generate outline-
enhanced answers. These data will be released to the public after
passing the censoring process of data release.
5 FACTUALITY-OPTIMIZED RAG
In this section, we propose a novel factuality optimization method
to address the aforementioned factuality issue in web-enhanced
LFQA. Specifically, we first discuss the difficulty of directly ap-
plying the conventional RLHF method to factuality optimization,
thendevelopanoveldoublyfine-grainedRLHFframework,which
characterizes different granularities of automated evaluation and
reward modeling, upon which our method is built.
5.1 Difficulties of Directly Applying RLHF
In LLM alignment, reinforcement learning with human feedback
(RLHF) [10, 36] is a widely used technique to reduce undesirable
generations,e.g.,harmfulresponsesinchatassistanttasks[4].View-
ing nonfactuality as a certain kind of undesirable behaviors, a nat-
ural way to promote factuality in web-enhanced RAG is to uti-
lize RLHF to prevent the generator from producing nonfactual re-
sponses. To proceed, we first give a detailed description of RLHF.
Conventionally,RLHFisconductedonmanuallyannotatedpref-
erence data. For example, given the query 𝑥and the retrieved con-
text 𝑧, the factuality of an answer 𝒂𝑇(tokenized as (𝑎1, . . . , 𝑎 𝑇))
can be annotated as 𝑟∼ R( 𝒂𝑇, 𝑥, 𝑧), where 𝑟∈ [0,1]reflects the
underlying human preference. RLHF trains a reward model ˆ𝑅to es-
timate the factuality given any query 𝑥, reference 𝑧, and answer 𝒂,
i.e., to learn the human preference function R. Then RL methods
such as PPO are applied to optimize the generation model based
on the trained reward model ˆ𝑅. The optimization problem can be
formulated as
max
𝜃∑
𝑠1∼D𝑇∑
𝑡=1E𝑎𝑡∼𝜋𝜃(·|𝑠𝑡))[1(𝑡=𝑇)𝑟] − 𝛽DKL(𝜋𝜃(·|𝑠𝑡)||𝜋ref(·|𝑠𝑡)),
 
202FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: The reward model training losses when combining different fine-grained evaluation and fine-grained reward modeling
techniques are displayed. Note that our doubly fine-grained RLHF framework provides a unified framework, containing the
existing RLHF [10] and fine-grained RLHF approaches [50, 52] as special cases.
Re
ward model
granularityEvaluation
Granularity
Holistic Sentence-le
vel Subclaim-level
Se
quence-level Logloss( ˆ𝑹𝜙,
𝑟)[10]∑𝐿
𝑖=1Logloss( ˆ𝑹𝜙[𝑖],𝒓𝑖)[50]∑𝐿
𝑖=1MSE( ˆ𝑹𝜙[𝑖],Agg𝑗(𝑹𝑖 𝑗))
T
oken-level Logloss(A
gg𝑡(ˆ𝑟𝑡), 𝑟)[52]∑𝐿
𝑖=1Logloss(Agg𝑡(ˆ𝑟𝑡)[𝑖],𝒓𝑖)∑𝐿
𝑖=1MSE(Agg𝑡(ˆ𝑟𝑡)[𝑖],Agg𝑗(𝑹𝑖 𝑗))
wher
e1is the indicator function, DKLis KL divergence regular-
ization, and 𝛽is the regularization strength. In the above formu-
lation, the regularization term is introduced to prevent the gener-
ation model from deviating too far from a reference model 𝜋ref.
The reference model is often set as the model after SFT (e.g., our
outline-enhanced RAG model as proposed above).
Directly applying the conventional RLHF method to factuality
optimization in web-enhanced LFQA will encounter two intrinsic
difficulties. First, the manually annotated factuality labels are typ-
ically expensive to collect, which involves comparing the factual
detailsbetweenalongansweranditscorrespondinglengthyrefer-
ence. Second, as shown in the above equation, the standard RLHF
uses the holistic reward, i.e., 1(𝑡=𝑇)𝑹(𝒂, 𝑥, 𝑧), which is not zero
only for the last token of the whole response. This holistic reward
can only provide a sparse signal for the training of the generation
model 𝜋𝜃. In web-enhanced LFQA where the answers are usually
longer, the sparsity problem due to the use of the holistic reward
will be even exaggerated.
5.2 Doubly Fine-grained RLHF
In light of the above difficulties of conventional RLHF in factual-
ity optimization for web-enhanced RAG, we propose a doubly fine-
grained RLHF framework to conduct factuality optimization in a
fine-grained manner. Our framework is inspired by recent study
on fine-grained RLHF [50, 52]. Unlike these previous works that
mainly focus on a single dimension, our framework incorporates
fine-grained designs of two core steps, i.e., factuality evaluation
and reward modeling.
Before elaborating our framework in details, we first introduce
necessary notations and definitions, which enables to characterize
multiple rewards for an answer that constitute a denser reward
signal 𝑹𝜙for the RL process. Specifically, following [50], we first
segmenttheoutput 𝒂into 𝐿textspans (𝒂1,𝒂2, . . . , 𝒂𝐿)correspond-
ing to the evaluation granularity (which will be described later) of
𝑹𝜙, where each segment 𝒂𝑗ends at the step 𝑇𝑗. The dense reward
signal 𝑹𝜙is an 𝐿-dimensional vector, whose 𝑗-th dimension repre-
sents the reward 𝑹𝜙(𝒂|𝑥, 𝑧)[𝑗] ∈ [ 0,1]for each segment 𝒂𝑗given
query 𝑥and retrieved context 𝑧as the input, which is assigned to
the final token in 𝒂𝑗. Especially, when 𝐿=1, our method degener-
ates to the standard RLHF with holistic reward.
Fine-grained Evaluation. Recall that to perform high qual-
ity automatic factuality evaluation, recent methods have been pro-
posed to first decompose a long answer into shorter pieces and
then evaluate the factuality of each piece with respect to the given
reference [ 21, 27]. Inspired by these methods, we consider threedifferent levels of granularity in the answer decomposition and
automated segment evaluation:
•Holistic: It is the standard granularity to evaluate the an-
swers [10]. Each generated answer is associated with a sin-
gle factuality score 𝑟.
•Sentence-level : As is suggested by previous research on au-
tomaticevaluation[24,27],wecansegmenttheanswerinto
sentences,6thenevaluateeachsentenceindividually.Inthis
case, the evaluation result is denoted as 𝒓𝑖where 𝑖is the in-
dex for the sentence.
•Subclaim-level : Following [9, 21, 33], we can further decom-
poseeachsentenceintomultiple subclaims viaanLLM,each
containing a single piece of factual information. After the
decomposition,weevaluateeachsubclaimindividually.Since
thedecompositionusingLLMbreakstheassociationbetween
thesubclaimandtheoriginalanswer,weaggregatethescores
of all subclaims into a single score to evaluate the factual-
ity of the sentence. More specifically, assuming there are 𝑗
subclaims for sentence 𝑖, then the evaluation score for the
sentence is given as 𝒓𝑖=Agg𝑗(𝑹𝑖 𝑗), where 𝑹𝑖 𝑗denotes the
factuality score of the subclaim 𝑗of sentence 𝑖, and Agg is
the aggregation function (in the form of average, minimum,
or maximum).
Fine-grained Reward Modeling. Recallthattobuildareward
model to estimate the factuality of a given answer, standard RLHF
methodstypicallyuseasequence-levelrewardmodelthatproduces
asinglefactualityscoreforeachanswer.Recently,atoken-levelre-
wardmodelingmethodhasbeenintroducedtoprovidetoken-level
feedback[52].Enlightenedbythesemethods,wecanconstructthe
reward model in two possible levels of granularity.
•Sequence-level :Asinglereward ˆ𝑹𝜙(𝒂|𝑥, 𝑧)islearnedforeach
sequence, whose actual form depends on the granularity of
the evaluation. In this way, the associated reward reflects
the factuality of the corresponding sequence, which is then
assigned to the last token of each sequence.
•Token-level : A reward ˆ𝑟(𝑠𝑡, 𝑎𝑡)learned for each token in the
sequence. In this way, the reward of the sequence is calcu-
latedbyaggregatingthealltoken-levelrewardsi.e., ˆ𝑹𝜙(𝒂|𝑥, 𝑧)=
Agg𝑡(ˆ𝑟(𝑠𝑡, 𝑎𝑡)).
Thetraininglossofeachcombinationofautomatedevaluationand
reward modeling in different levels of granularity is illustrated in
6W
e uses pySBD https://github.com/nipunsadvilkar/pySBD for segmentation.
 
203KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
Table 2. In most cases, the reward 𝑟is binary labelled, and the re-
ward model is trained with the Logloss:
Logloss (ˆ𝑹𝜙, 𝑟)=−∑
(𝑥,𝑧,𝒂,𝑟) ∈ D𝐿∑
𝑖=1𝒓𝑖log(ˆ𝑹𝜙(𝒂|𝑥, 𝑧)[𝑖])
+(1−𝒓𝑖)log(1−ˆ𝑹𝜙(𝒂|𝑥, 𝑧)[𝑖]).
Whentheaggregationstepinsubclaim-levelevaluationyieldscon-
tinuous-valuedreward 𝑟,wechoosetousetheMSElossforreward
model training instead
MSE(ˆ𝑹𝜙, 𝑟)=∑
(𝑥,𝑧,𝒂,𝑟) ∈ D𝐿∑
𝑖=1(ˆ𝑹𝜙(𝒂|𝑥, 𝑧)[𝑖] −𝒓𝑖)2.
After the reward model ˆ𝑹is learned, we adopt PPO to optimize
the generation model by maximizing the following reward
ˆ𝑟𝑡(𝑠𝑡, 𝑎𝑡)=𝐿∑
𝑗=11(𝑡=𝑇𝑗)ˆ𝑹𝜙(𝒂|𝑥, 𝑧)[𝑗] −𝛽log𝜋𝜃(𝑎𝑡|𝑠𝑡)
𝜋r
ef(𝑎𝑡|𝑠𝑡).
ComparedtotheconventionalRLHFwithasinglerewardforeach
answer, our formulation has 𝐿non-zero rewards corresponding to
the segments, which alleviates the sparse feedback signal problem
in conventional RLHF.
Notethatourproposedframeworkunifiestheexistingfine-grained
RLHF works [50, 52] by containing these methods as special cases.
Moreover, although our framework is motivated by optimization
factuality for web-enhanced RAG, it can also be generalized to
other RLHF tasks.
6 EXPERIMENT
Inthissection,weconductextensiveexperimentstovalidatetheef-
fectiveness of our outline-enhanced generation technique and fac-
tuality optimization methods.
6.1 Experimental setup
Datasets. Weconductexperimentsontwocommonlyuseddatasets
for web-enhanced long-form QA.
The WebGPT’s dataset . Although the training dataset originally
usedforWebGPTisnotpubliclyavailable,the272samplesreleased
on the WebGPT demo website7can be used as a testbed for per-
formance comparison [29]. In this dataset, each sample consists
of a question from the ELI5 dataset [12], several Bing retrieved
web pages, and extracted references. Note that it is a pure English
dataset.
The WebCPM’s dataset.[37].ThisisaChinesedatasetconstructed
similarly to the WebGPT dataset. As there is no official train-test
split, we randomly split 4,676 samples for training, 426 for valida-
tion, and 398 for testing.
Compared Methods. Wecompareourmethodwiththreeweb-
enhanced baseline methods.
WebGPT [34]supportsinteractivewebsearchforlong-formQA.
Ithastwoversions,namelyWebGPT-13BandWebGPT-175B,where
thelatteroneisthecurrentlystate-of-the-artperformingmodelfor
web-enhanced QA. Note that when comparing with WebGPT, we
directly use the responses collected from its website.
7https://openaipublic.blob.core.windows.net/webgpt-
answer-viewer/index.htmlWebCPM [37] is an open source web-enhanced RAG involving
interactivewebsearch.ItisthefirstworkonChineseweb-enhanced
RAG.Itistrainedonadatasetwhichcontains5,500question-answer
pairs in Chinese with references.
WebGLM [29]isanopensourceweb-enhancedQAsystemwith
human performance. It simplifies the interactive web search ap-
proach in WebGPT and WebCPM by a two-step retriever and gen-
eratorframework.ItistrainedontheWebGLM-QAdataset,which
focuses on English only.
Metrics. Weadoptthreecommonlyusedmetricsforweb-enhanced
RAG, i.e., coherence ,helpfulness, and factuality. As existing works
show that GPT4’s evaluation is highly consistent to human anno-
tations in both English [5, 13, 15, 19, 30] and Chinese [51], we use
GPT4 to evaluate these metrics. For the completeness of our study,
we also justify the consistency between GPT4 and human anno-
tation in Chinese in ablation study. Following the framework of
[30], we evaluates the coherence ( Cohr.) and helpfulness (Help. )
metrics. Wecount the scores greater than or equal to 4 as the judg-
ing criteria. For evaluation of factuality consistency, we adopt the
methodin[8]toachievefine-grainedevaluation.Inaddition,since
the longer answers are more likely to have factuality mistakes, for
the fairness of the evaluation, we report the scores at two granu-
larities, i.e., query-level (Fact/q.) and sentence-level ( Fact/s.).
Models and Training Configuration. Our experiments are
conducted by fine-tuning on Llama2-7B-chat [46] and ChatGLM2-
6B [11], which are widely used LLMs for question-answering in
English and Chinese respectively. The prompt templates at fine-
tuning and inference stages are given in Appendix A. The maxi-
mum contextlength is set to 4096 for Llama2-7B-chat and 8192 for
ChatGLM2-6B. Both models are fine-tuned on 8 A100 GPUs for 5
epochs with a initial learning rate of 1e-5 and a cosine learning
rate scheduler. Following the configuration of WebCPM [37], we
adopt beam search for each inference on a single A100 GPU with
thenum_beams parameter set to 3. We use our outline-enhanced
datasettoconductsupervisedfine-tuning(SFT)[36],andourmulti-
granularity evaluation data to conduct corresponding factuality
optimization. In order to decrease noise in the RLHF step, we nor-
malize the reward. Specifically, for each prompt (𝑥, 𝑧), we gen-
erate a response 𝒂𝑇using the SFT model, and estimates its re-
ward score ˆ𝑅(𝒂𝑇, 𝑥, 𝑧)using the learned reward model. For any
model generated answer 𝒂′
𝑇′, we take ˜𝑅(𝒂′
𝑇′, 𝑥, 𝑧)=ˆ𝑅(𝒂′
𝑇′, 𝑥, 𝑧) −
ˆ𝑅(𝒂𝑇, 𝑥, 𝑧)as the estimated reward, and the same technique is ap-
plied to sentence-level and subclaim level factuality evaluations.
6.2 Main results
The main empirical results of our method trained on Llama2-7B-
chat (FoRAG-L 7B) and ChatGLM2-6B (FoRAG-C 6B) are depicted
inTable3andTable4.WeheremainlyreporttheresultsofFoRAG-
L 7B, which attains the best performance among all possible com-
binations of the granularities of evaluation and reward model. A
detailed performance comparison of different granularity combi-
nations will be given in Section 6.3.
Overall Performance. In Table 3, we compare the overall per-
formance of FoRAG-L 7B and FoRAG-C 6B with all existing meth-
ods on both datasets. Note that among the examined baselines,
onlyWebCPM10BcananswerinChinese,sinceWebGPTdoesnot
 
204FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Performance comparison of the existing web-enhanced RAGs with our FoRAG.
Mo
delAnswer Evaluation
WebCPM (zh) WebGPT (en)
Cohr
. Help. Fact/q. Fact/s. Avg. Len. Cohr. Help. Fact/q. Fact/s. Avg. Len.
W
ebGPT 175b - - - - - 0.6911 0.9154 0.8823 0.9752
209
WebGPT 13b - - - - - 0.5478 0.7390 0.7977 0.9642 212
WebGLM 10B - - - - - 0.5919 0.8566 0.8639 0.9688 169
WebCPM 10B 0.4899 0.6985 0.6784 0.8916 549 0.7316 0.8566 0.8125 0.9764 330
FoRAG-C 6B (Ours) 0.8618 0.7764 0.7739 0.9639 655 0.8603 0.8640
0.7610 0.9804 443
FoRA
G-L 7B (Ours) 0.9121 0.8668 0.8216 0.9727 625 0.9889 0.9595 0.8897 0.9894 447
T
able 4: Comparison of variants of FoRAG with or without outline-enhanced (Out. Enh.), factuality optimization (Fac. Opt.).
Mo
delOut.
Enh.Fac.
Opt.Answer Evaluation
WebCPM (zh) WebGPT (en)
Cohr
. Help. Fact/q. Fact/s. Avg. Len. Cohr. Help. Fact/q. Fact/s. Avg. Len.
FoRA
G-C 6B7 7 0.4598 0.6332 0.7613 0.9081 583 0.4081 0.7721 0.7868 0.9464 177
7 3 0.4724 0.6407 0.8065 0.9395 585 0.5184 0.7868 0.8566 0.9763 181
3 7 0.8643 0.7814 0.6055
0.9197 622 0.8566 0.8529 0.5993 0.9530 417
3 3 0.8618 0.7764 0.7739 0.9639 655 0.8603
0.8640 0.7610 0.9804 443
FoRA
G-L 7B7 7 0.4296 0.6181 0.8090 0.8875 556 0.5221 0.8676 0.8750 0.9728 186
7 3 0.4447 0.6256 0.8618 0.9394 570 0.5368 0.8860 0.8970 0.9818 189
3
70.9095 0.8668 0.6583
0.9345 613 0.9816 0.9559 0.7978
0.9768 424
3 3 0.9121 0.8668 0.8216 0.9727 625 0.9889
0.9595 0.8897 0.9894 447
r
elease model weights or answers in Chinese, and WebGLM has
a well-known issue of being unable to answer in Chinese8. From
the results, we observe that on both English and Chinese datasets,
FoRAG-C 6B surpasses all baselines on five out of six metrics, and
FoRAG-L 7B performs the best on all metrics. Notably, FoRAG-L
7BsubstantiallyoutperformsWebGPT175Bthatcontains24times
more parameters, showing superiority of our method in bilingual
web-enhanced RAG tasks.
Evaluation of Outline-Enhanced Generator . We evaluate
the effectiveness of outline-enhanced generator as a core design
indatasetcollectionandRAGmodeldesignbyshowingthat,with-
out such technique, the reduced method will deteriorate severely.
Specifically,forthereducedvariant,wetrainthebackbonemodels
on a merged dataset from WebCPM and WebGLM-QA, which con-
tains 4.7k samples in Chinese and 44k samples in English. Note
that here we use the demonstration answers as provided in the
original datasets, i.e., the answers in WebCPM are human written
and answers in WebGLM-QA are GPT4 generated, which can be
considered in high quality.
InTable4,wecomparetheperformanceofourproposedmethod
with the reduced variant. The results show that applying our tech-
nique of outline-enhanced generator significantly boosts the per-
formance in terms of coherence and helpfulness on both datasets.
8P
lease refer to the discussion on the issue at WebGLM’s official codebase https://
github.com/THUDM/WebGLM/issues/7.As for factuality, the sentence-level measurements of our meth-
ods are a little bit higher or comparable with the counterpart mod-
elswithoutoutline-enhancedtechniques.Inaddition,applyingour
technique increases the length of model generations.
Evaluation of Factuality Optimization.Wethenevaluatethe
effectiveness of the factuality optimization technique by compar-
ingourmethodwiththecounterpartmethodwithoutsuchamech-
anism.AspresentedinTable4,addingfactualityoptimizationtech-
nique significantly raises the factuality consistency score in both
queryandsequencelevels,withoutaffectingtheothertwometrics
orthegenerationlength.Theaboveresultsjustifytheintroduction
of factuality optimization technique to our proposed method.
6.3 Comparison of Various Factuality
Optimization Granularities
In this subsection, we compare the performance of various im-
plementations of our method on different evaluation and reward
model granularities (as described in Table 2), with the following
two commonly used alignment methods as baselines:
MLE with Filtering (filter.) [44]: This method applies a filter to
drop the samples with factual inconsistency errors and preserve
thefactuallyconsistentones.ThenitfollowsthestandardSFTpro-
cedure,i.e.,fine-tuningthemodelbyoptimizingthemaximumlike-
lihood estimation loss on the positive samples.
Unlikelihood [49]: This method fine-tunes the model by maxi-
mizingthelikelihoodofpositive(i.e.,factuallyconsistent)samples
 
205KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
Table 5: The comparison of performance using various factuality optimization techniques on our FoRAG-L 7B. The “-” indicates
no extra factuality optimization is performed, i.e., the SFT model.
Factuality
OptimizationAnsw
er Evaluation
WebCPM (zh) WebGPT (en)
Cohr
. Help. Fact/q. Fact/s. Avg. Len. Cohr. Help. Fact/q. Fact/s. Avg. Len.
- 0.9095 0.8668 0.6583
0.9345 613 0.9816 0.9559 0.7978 0.9768 424
Unlikeliho
od 0.9070 0.8618 0.7286 0.9477 591 0.9816 0.9522 0.8419 0.9794 442
MLE w. Filtering 0.9171 0.8568 0.6783 0.9331 592 0.9852 0.9522 0.7831 0.9754 423
RLHFHolistic
+ Token 0.9020 0.8543 0.7236 0.9414 608 0.9816 0.9485 0.8382 0.9768 444
Sentence + Token 0.9095 0.8593 0.7814 0.9628 610 0.9816 0.9559 0.8603 0.9836 446
Subclaim + Token 0.9121 0.8593 0.7864 0.9658 616 0.9852 0.9559 0.8713 0.9851 446
Holistic 0.9146 0.8618 0.7563 0.9526 622 0.9852 0.9559 0.8493 0.9797 448
Sentence 0.9095 0.8593 0.8065 0.9704 612 0.9889 0.9595 0.8787 0.9866 447
Subclaim 0.9121 0.8668 0.8216 0.9727 625 0.9889 0.9595 0.8897 0.9894 447
T
able 6: Ablation study on outline-enhanced generation.
W
ebCPM (zh) WebGPT (en)
Metho
d Cohr. Help. Cohr. Help.
FoRA
G-C 6B w/o outline 0.8467 0.7613 0.8492 0.8529
FoRAG-C 6B 0.8643 0.7814 0.8566 0.8529
FoRA
G-L 7B w/o outline 0.8543 0.8593 0.9779 0.9522
FoRAG-L 7B 0.9095 0.8668 0.9816 0.9559
and
minimizing the likelihood of negative (i.e., factually inconsis-
tent) samples simultaneously.
For a fair comparison, all the methods are fine-tuned from the
samemodel,i.e.,Llama2afterSFTonouroutline-enhanceddataset.
The empirical results are presented in Table 5. From the results,
we observe that our proposed method attains better factual con-
sistency than the baselines, regardless of the granularity of evalu-
ation or reward modeling. In addition, among all the granularities
ofevaluation,subclaim-levelevaluationperformsthebest.Wealso
notice that token-level reward modeling performs worse than the
conventionalsegment-levelrewardmodeling,presumablybecause
thelengthofourdatasetsmaymaketoken-levelmodelingover-fit.
6.4 Ablation study
We now conduct ablation study to justify the rational of some cer-
tain design choices in our proposed method.
Effectiveness of the Outline-Expansion Two-Step Answer
Generation. Toillustratetheimpactofouroutline-enhancedgen-
eration technique, we train two baseline models that generate an-
swers directly based on our dataset, which lack the outline stage,
referred to FoRAG-C 6B w/o outline and FoRAG-L 7B w/o outline
in Table 6. The outcomes clearly show that our outline-enhanced
generation approach significantly augments the model’s capabili-
ties by enhancing the coherence and helpfulness of the answers
generated, with a particularly notable improvement observed in
the Chinese language task.Table 7: Alignment between GPT4-aided automated and hu-
man evaluations.
Metrics Cohr
. Help. Fact/q. Fact/s.
GPT4
vs Human 91.5% 83.0% 77.0% 95.5%
Human vs Human - - 69.5% 93.1%
On
GPT4 Evaluation Quality. ToevaluatehowwellGPT4cor-
relateswithhumanjudgment inChinese,werecruit10nativeChi-
nese-speaking annotators. Their task is to manually review coher-
ence, helpfulness, and both query-level and sentence-level factual-
ity on the Chinese generated results. A subset of 200 examples is
selected, and we conduct two rounds of human evaluation on it.
In each round, each sample is randomly assigned to one annotator.
We report the agreement rate (the ratio of overlap) between two-
round human labels and GPT4ś judgements in Table 8. The results
confirm a robust correlation between GPT4 and human ratings on
Chinese QA evaluation. Except on the query-level factuality, hu-
man suffers from comparing two lengthy texts, a conclusion that
is consistent with [50].
Effects of Imbalance Dataset. Toevaluatehowtheimbalance
ofthetwolanguagesinthedatasetaffectsthetrainingeffectonthe
resultingbilingualLLMs,weperformfurtherablationstudyonthe
levelofimbalance.Wefixthenumberoftrainingsamplestobe40k.
Then we tune the ratio of Chinese to English on five level, ranging
from 1:10, 1:3, 1:1 to 3:1, 10:1. We then randomly sample the corre-
spondingamountofsamplesfromourdatasetandtrainthemodels
based on Llama2-7B using SFT. The evaluation results, as depicted
inFigure2showthatwithaincreasingamountofdata,themodel’s
performance on the corresponding language increases on both co-
herency and helpfulness metrics. Meanwhile, the factuality metric
is not affected by this ratio. Note that the performance of Llama2-
7B is more sensitive to the number of training samples in Chinese.
This may be because the pre-training of Llama2-7B contains more
corpusinEnglish,andthereforeafewexamplesisenoughtoadopt
to the new task.
 
206FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering KDD ’24, August 25–29, 2024, Barcelona, Spain
1/10 1/3 1 3 10
Ratio of Chinese to English0.800.850.900.951.00Evaluation Results
WebCPM Cohr.
WebCPM Help.
WebCPM Fact/s.WebGPT Cohr.
WebGPT Help.
WebGPT Fact/s.
Figur
e 2: Evaluation results in terms of various metrics of
different models fine-tuned from Llama2-7B. We vary the
ratio of the Chinese samples to the English samples in the
training dataset.
6.5 Evaluation of Training Efficiency
Wefinallyevaluatethetrainingefficiencyofourproposedmethod.
Inthefollowing,wewillexaminetheadditionialcomputationcost
of the two new modules of FoRAG, i.e., outline-enhanced genera-
tion and doubly fine-grained RLHF, respectively.
The first step, i.e., outline-enhanced generation, has almost neg-
ligibleeffectontrainingtime.Duringinference,itrequiresroughly
10% more tokens to be generated, and the extra time consumed at
inferencestageisroughlyproportionallytothisincreaseintokens
generated. Note that this extra inference time can be eliminated
using context distillation techniques[3, 49].
The second step, i.e., doubly fine-grained RLHF, has no impact
on inference time. To evaluate the additional computational ex-
penseduringtraining,weconsideranaiveimplementationofFoRAG
that sequentially evaluates the reward for each sentence. In Ta-
ble 8, we compare the training time of FoRAG and the counter-
part method with holistic RLHF. The results show the best per-
formed version, the subclaim version of the doubly fine-grained
RLHFframework,takesabout67.7%moretimethanstandardRLHF.
Notethattheadditionalcomputationalcostcanbefurtherreduced
via implementation with a multi-head reward layer and carefully
designed attention mask can use one forward pass to calculate the
reward for all sentences, which will make the extra computational
cost insignificant.
In summary, FoRAG outperforms the baseline method with rea-
sonable additional computational cost.
7 CONCLUSION
Inthispaper,weproposeanovelanswergenerationmethodFoRAG
for web-enhanced LFQA to tackle the factuality issue and lack of
clear logical structure in existing methods. To this end, we first
devise an outline-enhanced generator to fulfill clear logic in long-
form answers and accordingly construct two datasets. Then weTable 8: Evaluation on training efficiency of doubly fine-
grained RLHF. We report the time consumed (in hours) for
training reward models (RM) and RLHF on different gran-
ularities. We also report the extra time cost in percentage
compared with the holistic RLHF method.
RM
RLHF Total Percentage
Holistic 1.1
32.0 33.1 -
Sentence 4.6 45.6 50.2 +51.7%
Subclaim 5.1 50.4 55.5 +67.7%
Holistic + Token 1.3 32.8 34.1 +3.0%
Sentence + Token 5.3 46.4 51.7 +56.2%
Subclaim + Token 6.0 52.0 58.0 +75.2%
pr
opose to optimize factuality in a carefully designed doubly fine-
grained RLHF framework. Our developed framework contains au-
tomaticevaluationandrewardmodelingindifferentlevelsofgran-
ularity,andcompassestraditionalfine-grainRLHFmethodsasspe-
cialcases.Empirically,FoRAGachievesstate-of-the-artperformance
in terms of coherence, helpfulness, and factuality on both English
and Chinese benchmarks. Notably, applying FoRAG to Llama2-7B-
chat, we derive FoRAG-L-7B, which outperforms WebGPT-175B
with only 1/24 in the number of parameters of WebGPT-175B.
REFERENCES
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-
rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shya-
mal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2] Reinald Kim Amplayo, Kellie Webster, Michael Collins, Dipanjan Das, and
Shashi Narayan. 2022. Query Refinement Prompts for Closed-Book Long-Form
Question Answering. arXiv preprint arXiv:2210.17525 (2022).
[3] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom
Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.
A general language assistant as a laboratory for alignment. arXiv preprint
arXiv:2112.00861 (2021).
[4] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDas-
Sarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022.
Training a helpful and harmless assistant with reinforcement learning from hu-
man feedback. arXiv preprint arXiv:2204.05862 (2022).
[5] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu,
Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou.
2023. BenchmarkingFoundationModelswithLanguage-Model-as-an-Examiner.
arXiv:2306.04181 [cs.CL]
[6] SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRuther-
ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-
dan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving
from trillions of tokens. In International conference on machine learning. PMLR,
2206–2240.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[8] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu,
andJunxianHe.2023. FELM:BenchmarkingFactualityEvaluationofLargeLan-
guage Models. arXiv:2310.00741 [cs.CL]
[9] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou,
Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. FacTool: Factuality Detec-
tion in Generative AI–A Tool Augmented Framework for Multi-Task and Multi-
Domain Scenarios. arXiv preprint arXiv:2307.13528 (2023).
[10] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
Amodei. 2017. Deep reinforcement learning from human preferences. Advances
in neural information processing systems 30 (2017).
[11] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive
Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for
 
207KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
Computational Linguistics (Volume 1: Long Papers). 320–335.
[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics .3558–3567.
[13] JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfeiLiu.2023. GPTScore:Eval-
uate as You Desire. arXiv preprint arXiv:2302.04166 (2023).
[14] TianyuGao,HowardYen,JiatongYu,andDanqiChen.2023. EnablingLargeLan-
guage Models to Generate Text with Citations. arXiv preprint arXiv:2305.14627
(2023).
[15] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts?
Comparison Corpus, Evaluation, and Detection. arXiv:2301.07597 [cs.CL]
[16] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
2020. REALM: Retrieval-Augmented Language Model Pre-Training. In Pro-
ceedings of the 37th International Conference on Machine Learning (ICML’20) .
JMLR.org, Article 368, 10 pages.
[17] Xiangkun Hu, Dongyu Ru, Qipeng Guo, Lin Qiu, and Zheng Zhang. 2023. Re-
fChecker for Fine-grained Hallucination Detection. (2023). https://github.com/
amazon-science/RefChecker
[18] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118
(2021).
[19] DongfuJiang,YishanLi,GeZhang,WenhaoHuang,BillYuchenLin,andWenhu
Chen.2023. TIGERScore:TowardsBuildingExplainableMetricforAllTextGen-
eration Tasks. ArXivabs/2310.00752 (2023). https://api.semanticscholar.org/
CorpusID:263334281
[20] MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer.2017. Triviaqa:
A large scale distantly supervised challenge dataset for reading comprehension.
arXiv preprint arXiv:1705.03551 (2017).
[21] Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. Wice:
Real-world entailment for claims in wikipedia. arXiv preprint arXiv:2303.01432
(2023).
[22] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. arXiv preprint arXiv:2004.04906 (2020).
[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in
neural information processing systems 35 (2022), 22199–22213.
[24] Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.
Evaluating the factual consistency of abstractive text summarization. arXiv
preprint arXiv:1910.12840 (2019).
[25] SayaliKulkarni,SheideChammas,WanZhu,FeiSha,andEugeneIe.2020. Aqua-
muse: Automatically generating datasets for query-based multi-document sum-
marization. arXiv preprint arXiv:2010.12694 (2020).
[26] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton
Lee,etal.2019. Naturalquestions:abenchmarkforquestionansweringresearch.
Transactions of the Association for Computational Linguistics 7 (2019), 453–466.
[27] Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast
and Accurate Factual Inconsistency Detection Over Long Documents. arXiv
preprint arXiv:2310.13189 (2023).
[28] Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in
generative search engines. arXiv preprint arXiv:2304.09848 (2023).
[29] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng
Zhang, Yuxiao Dong, and Jie Tang. 2023. WebGLM: Towards An Efficient
Web-Enhanced Question Answering System with Human Preferences. In Pro-
ceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <coun-
try>USA</country>, </conf-loc>) (KDD ’23). Association for Computing Ma-
chinery, New York, NY, USA, 4549 ⚶4560. https://doi.org/10.1145/3580305.
3599931
[30] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang
Zhu. 2023. G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.
InProceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for
Computational Linguistics, Singapore, 2511–2522. https://doi.org/10.18653/v1/
2023.emnlp-main.153
[31] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song,
MartinChadwick,MiaGlaese,SusannahYoung,LucyCampbell-Gillingham,Ge-
offrey Irving, et al. 2022. Teaching language models to support answers with
verified quotes. arXiv preprint arXiv:2203.11147 (2022).
[32] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram
Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, et al. 2023. Augmented language models: a survey. arXiv
preprint arXiv:2302.07842 (2023).
[33] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei
Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore:Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Gener-
ation. arXiv preprint arXiv:2305.14251 (2023).
[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu
Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew
Knight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted
question-answering with human feedback. arXiv: 2112.09332 [cs.CL]
[35] OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[36] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,Pamela
Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022.
Training language models to follow instructions with human feedback. Ad-
vances in Neural Information Processing Systems 35 (2022), 27730–27744.
[37] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin,
Xu Han, Ning Ding, Huadong Wang, et al. 2023. WebCPM: Interactive Web
Search for Chinese Long-form Question Answering. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 8968–8988.
[38] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
Squad: 100,000+ questions for machine comprehension of text. arXiv preprint
arXiv:1606.05250 (2016).
[39] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Lan-
guage Models. Transactions of the Association for Computational Linguistics 11
(2023), 1316–1331. https://doi.org/10.1162/tacl_a_00605
[40] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hes-
sel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022.
Is reinforcement learning (not) for natural language processing?: Benchmarks,
baselines, and building blocks for natural language policy optimization. arXiv
preprint arXiv:2210.01241 (2022).
[41] Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversa-
tional question answering challenge. Transactions of the Association for Compu-
tational Linguistics 7 (2019), 249–266.
[42] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike
Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-
Augmented Black-Box Language Models. arXiv:2301.12652 [cs.CL]
[43] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen
Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blender-
bot 3: a deployed conversational agent that continually learns to responsibly
engage. arXiv preprint arXiv:2208.03188 (2022).
[44] IreneSolaimanandChristyDennison.2021. Processforadaptinglanguagemod-
elstosociety(palms)withvalues-targeteddatasets. Advances in Neural Informa-
tion Processing Systems 34 (2021), 5861–5873.
[45] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-
shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applications. arXiv preprint
arXiv:2201.08239 (2022).
[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[47] Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue
Zhang. 2023. Evaluating open question answering evaluation. arXiv preprint
arXiv:2305.12421 (2023).
[48] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang,
Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. 2023.
Surveyonfactualityinlargelanguagemodels:Knowledge,retrievalanddomain-
specificity. arXiv preprint arXiv:2310.07521 (2023).
[49] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and
Jason Weston. 2019. Neural text generation with unlikelihood training. arXiv
preprint arXiv:1908.04319 (2019).
[50] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Am-
manabrolu,NoahASmith,MariOstendorf,andHannanehHajishirzi.2023. Fine-
Grained Human Feedback Gives Better Rewards for Language Model Training.
arXiv preprint arXiv:2306.01693 (2023).
[51] Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song. 2024. Face4RAG: Factual
Consistency Evaluation for Retrieval Augmented Generation in Chinese. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining.
[52] Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, and
Mingyuan Zhou. 2023. Preference-grounded Token-level Guidance for Lan-
guage Model Fine-tuning. arXiv preprint arXiv:2306.00398 (2023).
[53] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Eval-
uating Factual Consistency with a Unified Alignment Function. arXiv preprint
arXiv:2305.16739 (2023).
 
208FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering KDD ’24, August 25–29, 2024, Barcelona, Spain
A PROMPTS FOR GENERATION
The following is the prompt template used to invoke GPT4 to generate outline-enhanced answers.
###T
ask###
Answ
er the question based on the materials provided.
###Re
quirements###
Step
One: Develop an answer outline based on the question and materials.
1.
Chooseasuitableorganizationalpatternfortheanswerstructure,suchasgeneral-specific-general,progressive,comparative,cause-effect,
parallel, chronological, among others.
2.
Enumerate the essential points that need to be included in the outline, aligned with the the chosen structure.
3.
The relationship between key points can be parallel, contrastive, progressive, etc., but should not be repetitive or inclusive.
4.
Formulate a clear and concise outline that includes at least 1 but no more than 5 key points.
5.
Each main point should reference only one specific part of the provided materials and must include the material’s number within the
outline.
Step
Two: Answer the question based on the materials and outline.
1.
Utilize the outline as a blueprint to develop a comprehensive and informative answer.
2.
Write the answer using formatting tools such as numbered lists, bullet points, subheadings, LaTeX formulas, etc., where appropriate.
3.
Refrain from using basic sequential connectors like “firstly,” “secondly,” or “furthermore,” in the answer.
4.
Avoid redundancy and repetition of content within the answer.
5.
Do not cite the number of the materials in the answer.
6.
Adhere strictly to the information contained within the provided materials, without adding any information that is not included in the
materials.
###Format###
[Structur
e]:
<The
organizational structure of the answer>
[
Outline]:
<Bullet
points outlining the answer>
[
Answer]:
<Answ
er the question based on the materials and outline>
Her
e are two examples of input and output:
###Input
Example 1###
###Question###
What
are the main factors contributing to the increasing trend of urban population growth?
###Materials###
[1]The
improvement of healthcare in cities has led to lower mortality rates. With access to state-of-the-art hospitals, specialized clinics,
and a higher concentration of medical professionals, city residents benefit from quicker and more effective response to health needs. The
advancements in healthcare infrastructure, such as the implementation of digital health records and telemedicine, have further streamlined
patient care, making it more accessible and efficient.
[2]Urban
areas are widely recognized for better educational institutions, from primary schools to universities, that promise a more com-
prehensive and diverse learning environment. These institutions are often equipped with advanced resources, experienced educators, and
a broader range of programs and extracurricular activities, which are highly attractive to families who prioritize their children’s education.
[3]Cities
are the epicenters of industry, commerce, and services, offering a myriad of employment options that are scarce or nonexistent
in rural settings. The diversity of the urban job market, with its potential for higher wages, career advancement, and the chance to engage
in more specialized work, draws individuals in pursuit of a better livelihood. Rural to urban migration is driven by the search for better
employment opportunities.
###Output
Example 1###
[Structur
e]:
Parallel
[
Outline]:
1.
Advancements in healthcare (based on [1])
 
209KDD’24, August 25–29, 2024, Barcelona, Spain Tianchi Cai et al.
2.
Enhanced educational opportunities (based on [2])
3.
Employment prospects (based on [3])
[
Answer]:
Urban
population growth is driven by several compelling factors that make living in cities increasingly attractive.
One
ofthemostsignificantcontributorstothistrendisthecomprehensiveimprovementinhealthcarethatcitiesoffer.Residentsbenefitfrom
lower mortality rates due to access to superior medical facilities including state-of-the-art hospitals and specialized clinics. Furthermore,
theurbanhealthcareinfrastructurehasbeenenhancedbytechnologicaladvancements,suchastheimplementationofdigitalhealthrecords
and the growing availability of telemedicine services.
In
addition to healthcare, the quality of education available in cities acts as a strong magnet for families and individuals. Cities typically
boast:
-
Better educational institutions, which provide a more comprehensive and diverse learning environment.
-
Access to experienced educators and advanced educational resources that are not as readily available in rural areas.
-
A broad range of extracurricular activities that are highly attractive for those prioritizing education for their children.
Finally
, the wide array of employment options available in urban areas is a another driver of population growth. The concentration of
industry, commerce, and services in cities creates a diverse job market with the potential for higher wages, career advancement, and
specialized work. This draws people from rural areas in search of better employment prospects, thereby increasing the urban population.
###Input
Example 2###
###Question###
What
makes smartphones an essential tool for modern life?
###Materials###
[1]Smartphones
provide a convenient means of communication, allowing people to stay connected with friends and family through calls,
texts, and various messaging apps.
[2]The
yofferaccesstoawidearrayofinformationandentertainmentoptions,suchasbrowsingtheinternet,streamingvideos,andplaying
games.
[3]Smartphones
have built-in features that replace the need for other devices, like a camera, GPS, and notebook, making them a multifunc-
tional tool that fits in your pocket.
###Output
Example 2###
[Structur
e]:
General-Sp
ecific-General
[
Outline]:
1.
Communication hub (based on [1])
2.
Multifunctional device (based on [3])
[
Answer]:
Smartphones
have become indispensable in contemporary life primarily because they serve as a central hub for communication and a
versatile multifunctional device.
###
Central Hub for Communication
The
ability to instantly connect with others via calls, texts, and various messaging applications is one of the most valued features of
smartphones. This constant connectivity has revolutionized the way we maintain personal and professional relationships.
###
Versatile Multifunctional Device
Smartphones
eliminate the need to carry multiple gadgets by incorporating essential tools like a high-quality camera, a reliable GPS navi-
gation system, and the functionality of a notebook, all within a single, portable device.
In
essence, the critical role of smartphones in modern life can be attributed to their unparalleled capacity to keep us connected to the world
around us and their ability to perform multiple functions that simplify our daily routines.
Belo
w is the question you need to answer and the corresponding materials. Please output according to the above requirements, format, and
examples:
###Question###
{
query}
###Materials###
{
doc}
 
210