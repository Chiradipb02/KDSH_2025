Offline Imitation Learning with Model-based Reverse
Augmentation
Jie-Jing Shao∗
National Key Laboratory for Novel Software Technology
Nanjing University, Nanjing, China
shaojj@lamda.nju.edu.cnHao-Sen Shi∗
National Key Laboratory for Novel Software Technology
Nanjing University, Nanjing, China
shihs@lamda.nju.edu.cn
Lan-Zhe Guo†
National Key Laboratory for Novel Software Technology
School of Intelligence Science and Technology
Nanjing University, Nanjing, China
guolz@nju.edu.cnYu-Feng Li†
National Key Laboratory for Novel Software Technology
School of Artificial Intelligence
Nanjing University, Nanjing, China
liyf@nju.edu.cn
ABSTRACT
In offline Imitation Learning (IL), one of the main challenges is the
covariate shift between the expert observations and the actual distri-
bution encountered by the agent, because it is difficult to determine
what action an agent should take when outside the state distribution
of the expert demonstrations. Recently, the model-free solutions
introduce the supplementary data and identify the latent expert-
similar samples to augment the reliable samples during learning.
Model-based solutions build forward dynamic models with con-
servatism quantification and then generate additional trajectories
in the neighborhood of expert demonstrations. However, without
reward supervision, these methods are often over-conservative in
the out-of-expert-support regions, because only in states close to
expert-observed states can there be a preferred action enabling
policy optimization. To encourage more exploration on expert-
unobserved states, we propose a novel model-based framework,
called offline Imitation Learning with Self-paced Reverse Augmenta-
tion (SRA). Specifically, we build a reverse dynamic model from the
offline demonstrations, which can efficiently generate trajectories
leading to the expert-observed states in a self-paced style. Then, we
use the subsequent reinforcement learning method to learn from
the augmented trajectories and transit from expert-unobserved
states to expert-observed states. This framework not only explores
the expert-unobserved states but also guides maximizing long-term
returns on these states, ultimately enabling generalization beyond
the expert data. Empirical results show that our proposal could ef-
fectively mitigate the covariate shift and achieve the state-of-the-art
performance on the offline imitation learning benchmarks. Project
website: https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/.
∗Both authors contributed equally to this research.
†Corresponding Authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672059CCS CONCEPTS
•Computing methodologies →Learning from demonstra-
tions; Inverse reinforcement learning.
KEYWORDS
Offline Imitation Learning, Model-based Imitation Learning, Offline
Reinforcement Learning
ACM Reference Format:
Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li. 2024. Offline
Imitation Learning with Model-based Reverse Augmentation. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3637528.3672059
1 INTRODUCTION
The recent success of offline Reinforcement Learning (RL) in various
fields demonstrate the significant potential of addressing sequential
decision-making problem in a data-driven manner [ 26]. Offline rein-
forcement learning enables the learning of policies from the logged
experience, reducing the reliance on online interactions and making
reinforcement learning more practical, especially when online data
collection may be expensive or risk-sensitive [ 8,34,40]. However, in
many real-world applications, designing the reward function, which
determines the preferred agent behavior, is a critical prerequisite
for offline reinforcement learning and can be prohibitively tricky.
It usually needs to be custom-designed for each task. This requires
sufficient prior knowledge and can be challenging in fields such
as robotics [ 4], autonomous driving [ 21], and healthcare [ 49]. One
promising way to overcome this practical barrier is Offline Imitation
Learning, which trains policies directly from expert demonstrations
without needing reward supervision.
Offline imitation learning allows machine to implement sequen-
tial decision-making by imitating expert behaviors. It has achieved
remarkable success on a variety of domains, such as robotic ma-
nipulation [ 7,44] autonomous driving [ 3,30] and language mod-
els [5,46]. However, the effectiveness of imitation learning methods
typically relies on the sufficient expert demonstrations. When the
expert data is limited, the covariate shift between expert observa-
tions and the actual distribution encountered by the agent makes
imitation learning methods ineffective, both in theory [ 35,48] and
2608
KDD ’24, August 25–29, 2024, Barcelona, Spain Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li.
(a) Expert Trajectories
 (b) Cumulative return of MILO [6]
Figure 1: Covariate shift problem. We visualize the expert
trajectories and the state-wise cumulative return of MILO
agent. Deeper red represents higher cumulative returns.
The agent performs well in the neighborhood of expert-
observed states but performs poorly in the rest of the states.
practice [ 6,47]. One naive solution to this problem is to collect more
demonstrations from the expert, but this is costly and impractical
in domains where expert experience is expensive and scarce, such
as the robotics [44] or healthcare [18].
Recently, various advanced offline Imitation Learning methods
have been proposed. They introduce the supplementary data, albeit
sub-optimal, as the assistance to the limited expert data [ 6,20,28,38,
47,51,52]. Model-free methods typically identify the latent high-
quality samples from supplementary. For examples, DemoDICE
trains a sample discriminator via a regularized state-action distribu-
tion matching objective [ 20]. DWBC assume the data come from a
mixed distribution of expert policy and sup-optimal policy, and thus
utilize positive-unlabeled learning to identify the expert-similar
samples from the expert data [ 47]. OTIL uses optimal transport to
find an alignment with the minimal wasserstein distance between
the supplementary trajectories and expert demonstrations [ 28].
BCDP presents the idea to lead the agent from expert-unobserved
states to expert-observed states and highlights that dynamic pro-
gramming can efficiently use supplementary of low quality [ 38].
A concurrent work ILID presents a similar idea and proposes a
data selection method that identifies positive behaviors based on
their resultant states, enabling explicit utilization of dynamics in-
formation from diverse demonstrations [ 51]. To efficiently utilize
the supplementary, model-based methods build dynamic models
from offline data with conservative quantification and then gener-
ate imaginary trajectories in areas similar to experts, expanding
the offline dataset. MILO extends the adversarial imitation learning
to utilize the samples from model-based augmentation [ 6]. CLARE
estimates rewards for the demonstrations via dynamics conser-
vatism and expert similarity, encouraging RL to keep the agent in
the corresponding area [52].
However, because samples in expert dataset are limited, the simi-
larity measure often suffer from under-diversity, leading the ineffec-
tive exploration on the out-of-expert regions. As shown in the Fig-
ure 1, although MILO has generated more samples through model-
based rollout, improving the generalization of the policy. However,there are still a large number of out-of-expert states where the
policy performs poorly. To encourage more exploration on expert-
unobserved states, we propose a novel model-based framework,
called offline Imitation Learning with Self-paced Reverse Augmenta-
tion (SRA). Specifically, we build a reverse dynamic model from the
offline demonstrations, which can efficiently generate trajectories
leading to the expert-observed states in a self-paced style. Then, we
use the subsequent reinforcement learning method to learn from
the augmented trajectories and transit from expert-unobserved
states to expert-observed states. This framework not only explores
the expert-unobserved states but also maximizes the long-term re-
turns on these states, ultimately enabling generalization beyond the
expert data. Empirical results show that our proposal could effec-
tively mitigate the covariate shift and achieve the state-of-the-art
performance on the offline imitation learning benchmarks.
2 RELATED WORK
This work mainly focus on learning from demonstrations with-
out reward supervision. There are two main branches to address
this problem, that is, Imitation Learning andInverse Reinforcement
Learning. In this section, we review them sequentially.
Imitation Learning. Behavioral Cloning is one of the most well-
validated offline Imitation Learning, which optimizes the policy via
supervised learning [ 33]. According to the learning theory in [ 35],
the effectiveness of behavioral cloning is fundamentally constrained
by the scale of the expert dataset. This limitation arises as it’s un-
clear how the agent should respond when confronted with states
not represented in the expert demonstrations, which is also well-
known as the covariate shift problem in the traditional machine
learning topics [ 37,39,42,53]. To address this issue, recent stud-
ies [20,27,36,47] introduce supplementary data from offline poli-
cies to assist the limited expert dataset, which is similar to semi-
supervised learning [ 13,14] that uses cheaper unlabeled data as
supplementary to the limited labeled data. For examples, Kim et al .
[20] takes the distribution matching with offline data to provide
a policy regularization. Sasaki and Yamashina [36] use prediction
confidence from old policy during the learning to weight the sam-
ples, eliminating the noisy behaviors in the supplementary data.
Xu et al . [47] assume the supplementary data from a mixture of
expert distribution and sub-optimal distribution, and then utilize
positive-unlabeled learning to build a discriminator and identify
the latent expert-similar samples. Shao et al . [38] presents the idea
to lead the agent from expert-unobserved states to expert-observed
states and highlight that dynamic programming can efficiently use
supplementary data of low behavioral quality to support imitation
learning. A concurrent work ILID [ 51] presents a similar idea and
proposes a data selection method that identifies positive behav-
iors based on their resultant states, enabling explicit utilization of
dynamics information from diverse demonstrations [ 51]. Chang
et al. [6]propose a model-based method MILO to alleviate covariate
shift. They built a forward dynamic model and extended adversarial
imitation learning to identify and utilize samples in model-based
rollouts that are similar to expert samples.
Inverse Reinforcement Learning. Inverse Reinforcement Learn-
ing [ 32] is another popular branch to implement reinforcement
2609Offline Imitation Learning with Model-based Reverse Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) Challenge (i)
 (b) Challenge (ii)
 (c) Challenge (iii)
 (d) Our Proposal
Figure 2: The illustration of the main challenges of offline imitation learning and our main idea. (a): The expert data is limited.
(b): The preferred action on the expert-unobserved states are uncertain. (c): Model-based forward rollout on expert-unobserved
states is difficult to exploit. (d): In this work, we propose utilizing reverse rollout to generate trajectories and leading the agent
transit from expert-unobserved states to expert-observed states.
learning without reward supervision. It involves iteratively learn-
ing a reward function and policy [ 2,54]. However, this requires
a potentially large number of online interactions during training,
which can result in poor sample efficiency. Recently, offline inverse
reinforcement learning has been proposed to eliminate the online
interactions and learn from the offline demonstrations [ 17,28]. Jar-
boui and Perchet [17] build a discriminator between expert demon-
strations and offline demonstrations to serve as the reward function
ˆr, which takes a similar underlying idea like discriminator-based
imitation learning, that is, the state-action pairs (s,a)which resem-
ble expert data receive higher rewards. Luo et al . [28] use optimal
transport to find an alignment with the minimal wasserstein dis-
tance between unlabeled trajectories and expert demonstrations.
Yue et al . [52] integrates the dynamics conservatism and expert
similarity to estimates rewards for the model-based augmented
demonstrations, encouraging reinforcement learning to keep the
agent in the corresponding area [52].
In the reinforcement learning, there also are some exploration
on the methods with reverse dynamics model. This idea is firstly
raised in the literature of online reinforcement learning [ 12,15,
24,25]. Lai et al . [24] utilizes a backward model to reduce the
reliance on accuracy in forward model and improve the prediction
of the trajectories. Wang et al . [45] show the reverse imagination
inherently incorporates the conservatism into rollout trajectories in
the offline reinforcement learning context. The key difference from
RL is that in IL the reward function is not available. This determines
that rollout samples do not have a reward label. When there is a
reward label, RL methods could directly use augmented samples
to guide learning. However, in IL, since there is no reward label,
existing model-based IL methods try to annotate high rewards on
rollout samples similar to expert samples by measuring similarity,
which ultimately makes the agent stay around the expert samples
(as shown in Figure 1). To our best knowledge, SDA is the first offline
IL method with reverse dynamics model to reduce the covariate
shift with data augmentation. It reveals that reverse rollout is more
easily exploited compared to forward rollout, that is, comparedto forward rollout which can only utilize data around the expert
data, the reverse rollout is not similar to expert data can also be
efficiently utilized. Therefore, it provides a new perspective to the
IL community compared to reverse-based RL methods.
3 THE PROPOSED METHOD
In the offline imitation learning setting, the agent can only access
limited expert demonstrations. There are three challenges in this
setting. (i) The expert demonstrations has limited samples and
generalization over the given data is required. (ii) Without the
reward supervision, what action an agent should take when outside
the expert-observed states is unknown. (iii) The model-based rollout
encounters a lower efficiency of data augmentation utilization when
encountering expert-unobserved states, because it doesn’t know
the preferred action on them. Thus it is crucial to augment diverse
data on expert-unobserved states while providing preferred action
at the same time. The Figure 2(d) has demonstrated our main idea.
That is, we would like to build the reverse models and generate
the trajectories whose endpoints are in the expert-observed states
where the agent can perform well, leading the agent from expert-
unobserved states to these endpoints, thereby guiding for improving
policy’s long-term return on these expert-unobserved states. In this
section, we will introduce the Self-paced Reverse Augmentation
(SRA) framework, which encourages diverse data augmentation on
expert-unobserved states for offline imitation learning.
3.1 Problem Formulation
In this work, we consider the infinite Markov Decision Process
(MDP) setting [ 43], denoted asM={S ,A,T,r,d0,γ}. Here,S
is the state space, Ais the action space, T:S×A→ ∆(S)is
the transition probability of M,r:S×A→[ 0,1]is the reward
function, d0:S→ ∆(S)is the initial state distribution and γ∈
(0,1)is the discount factor. The decision-making process occurs
sequentially. At time t, the agent observes a state st∈Sand takes an
action at, following the conditional probability π(at|st). The agent
then receives a reward r(st,at)from the environment, and a new
2610KDD ’24, August 25–29, 2024, Barcelona, Spain Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li.
Table 1: Summary of Notations.
Notation Meaning
S State space
A Action space
T Transition probability
DEExpert dataset
DSSupplementary dataset
γ Discount factor
π Learned policy
J(π) Expected cumulative return
ˆTr Reverse dynamic model
πr Reverse behavioral policy
Conf π(s) State-wise policy confidence
DAModel-based augmented dataset
DRRe-sampled augmented dataset
state st+1appears based on the transition probability T(st+1|st,at)
ofM. The goal of sequential decision-making is to maximize:
J(π)=Es0∼d0,st+1∼T(·|st,π(st))"∞Õ
t=0γtr(st,π(st))#
.
In the offline imitation learning setting, there are limited expert
demonstrations with trajectories:
DE={(s0,a0,s1,a1, . . . , sh,ah)|
s0∼d0,at∼πE(·|st),st+1∼T(·|st,at),∀t∈[h]}.
where his the length of each trajectory following the expert policy.
There are also some logged experiences collected by the arbitrary
behavior policies π′which is much cheap to obtain:
DS={(s0,a0,s1,a1, . . . , sh,ah)|
s0∼d0,at∼π′(·|st),st+1∼T(·|st,at),∀t∈[h]}.
In the literature [ 9], the offline policy π′can come from various
task objectives, suboptimal policies, or even random exploration,
which are different from the expert policy and thus challenging to
utilize in the imitation learning.
Model-based offline imitation learning methods typically utilize
the union dataset DE∪DSto train a forward dynamic model ˆT, ap-
proximating the ground-truth transition T. And the obtained ˆTwill
be used to rollout augmented trajectories with the learned policy π.
In this work, we find the reverse dynamic model ˆTrcould generate
trajectories form expert-unobserved states to the expert-observed
states, which is more efficient for offline imitation learning. In the
following subsections, we will first introduce the establishment of
the reverse models and then the process of data augmentation.
3.2 Reverse Models
In the subsection, we introduce the reverse models, includes reverse
dynamic model ˆTr:S×A→ ∆(S)and reverse behavior policy
πr:S→ ∆(A). The reverse dynamic model want to find which
state stcould lead to states st+1via action at, and could be learnedfrom the samples in the union dataset, (st,at,st+1)∼DE∪DS:
max
ˆTrÕ
(st,at,st+1)logˆTr(st|,st+1,at). (1)
For the reverse behavior policy, we hope it can help us explore
different previous st, conditioned on st+1. Following the [ 11,41,45],
we build a conditional variational auto-encoder, which contains
encoderπerand decoder πdr, to approximate the p(at|st+1), which
could be optimized via maximizing the variational lower bound:
logπr(a|s)≥Ez∼πer(·|s,a)logπd
r(a|z,s)−KL(πe
r(z|s,a)||p(z|s))
where zis from the encoded gaussian distribution z∼πer(st+1,at)
and action atcould be obtained via the decoder πdr(st+1,z)with
the inputs st+1andz. In the training phase, we simplify the con-
ditional distribution p(z|s)into a normal distribution N(0,I)and
then train the conditional variational auto-encoder through gradi-
ent descent, maximizing the above lower bound. In the inference
phase, we sample the latent factor zfrom the multivariate normal
distribution z∼N( 0,I), and then utilize the decoder to produce the
conditional actions p(at|st+1). Note that the conditional variational
auto-encoder πrrecover the distribution over the union dataset. If
we need to explore extremely diverse states, we could directly sam-
ple the atfrom the action space at∼Uniform(A) with a uniformly
random reverse policy πr.
3.3 Self-Paced Augmentation
Given the reverse models ˆTr,πrand the target states set G, we can
perform model-based reverse data augmentation and produce the
augmented dataset DA.
DA={s−h′,a−h′,s−h′+1,a−h′+1, . . . , s−2,a−2,s−1,a−1,s0|
s0∼G,ai∼πr(si+1),si∼ˆTr(si+1,ai)∀i∈[−h′,−1]}(2)
Inspired by self-paced learning [ 19,23,29], we would like to cau-
tiously and adaptively prioritize learning simple, reliable examples,
and then gradually transition to learning difficult examples. Instead
of directly using expert data as the target states Gand generating
a fixed augmented dataset, we judge whether the policy has well
learned the behavior of the state based on its self-confidence, grad-
ually expanding the Gto generate more diverse data. Formally, we
take the self-confidence as the measure of an agent’s state-wise
belief degree.
Conf π(s)=π(E[π(a|s)]|s) (3)
In areas that have been fully learned, such as expert-observed states,
the policy has a high conditional probability for the expected ac-
tionE[π(a|s)], and therefore has a high Conf π(s). Therefore, we
construct the set of target points Gwith the Conf π(s):
G={s|Conf π(s)≥Es′∼DEConf π(s′)}∪{ s′|s′∈DE} (4)
Moreover, in expert-unobserved states, the policy has lower
Conf π(s)in determining which action is preferred. To explore
more expert-unobserved states and mitigate the covariate shift,
we further design a data selection mechanism that over-samples
the under-exploring states to enhance the agent on them. Specif-
ically, we over-sample the states which has lower Conf π(s), the
state-wise sampling probability is:
p(s)=1/Conf π(s) (5)
2611Offline Imitation Learning with Model-based Reverse Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1 Offline IL with Self-Paced Reverse Augmentation
Require: Expert dataset DE, supplementary dataset DS, length of
reverse rollout h′, The number of iterations ND,NR,NP.
1:fort=1toNDdo
2: Update the reverse dynamic model ˆTrwith gradient descent.
3:end for
4:fort=1toNRdo
5: Update the reverse behavior policy πrwith gradient descent.
6:end for
7:fort=1toNPdo
8: Collect the goals Gvia Eq. 4.
9: Rollout the reverse trajectories DAwith G,ˆTr,πr, via Eq. 2.
10: Re-sample the augmented samples to obtain DR, with the
sampling weights (Eq. 5).
11: Get the union dataset DU=DE∪DS∪DR
12: Train the policy πwith model-free reinforcement learning
methods on the union dataset.
13:end for
14:return Policyπ.
With the weighted sampling mechanism, we could obtain a re-
sampled dataset DR, encouraging the more diverse data with under-
exploring states.
3.4 Algorithm Formalization
Based on the learned reverse models and self-paced augmenta-
tion, we could explore the expert-unobserved states and collect
the dataset DR. Since our reverse augmentation is agnostic to the
specific learning of policy π, the proposed data augmentation could
be combined with any model-free offline imitation learning meth-
ods. In order to fully utilize augmented data and lead the agent to
transit from expert-unobserved states to expert-observed states,
we consider utilizing the offline reinforcement learning methods
which include dynamic programming objective, such as IQL [ 22]
and TD3BC [ 10]. To keep the simplicity of our proposal, we do not
add the additional reward learning module from the inverse rein-
forcement learning methods, but label the expert samples reward
as 1, the rest are all 0, which is known as unlabeled data shar-
ing [16,38,50]. It has shown advantages in utilizing supplementary
sub-optimal data, and is sufficient enough to implement our ob-
jective, leading the agent to expert-observed states that have the
higher rewards. The overall framework are shown in Algorithm 1.
4 EMPIRICAL STUDY
4.1 Experimental Setting
We evaluate on a wide range of domains in the D4RL benchmark [ 9],
includes navigation and locomotion.
Navigation. We conduct the experiments on the Maze2D environ-
ments to evaluate the policy. The Maze2D domain requires an agent
to navigate in the maze to reach a fixed target goal and stay there.
The D4RL benchmark provides three maze layouts (i.e., umaze,
medium, and large) and two rewards types (i.e., sparse and dense
reward singals) in this domain. We employs 5 expert trajectories as
the expert data which follows a path planner [ 9]. We consider theoffline data as the logged experience with random goals (umaze-v1,
medium-v1, and large-v1.), which provided by the D4RL benchmark.
They evaluate the ability of offline imitation learning methods to
use data from related tasks.
Locomotion. We conduct the experiments on the Gym-MuJoCo
environments to evaluate the policy. It consists four different en-
vironments (i.e., hopper, walker2d, halfcheetah and ant). We em-
ploys 5 expert trajectories from the “-expert" datasets as the expert
data. We also consider offline supplementary from sub-optimal
policy (hopper-medium-v2, hopper-medium-expert-v2, etc.). The
“medium" dataset contains logged experiences from an early-stopped
SAC policy, while the “medium-expert" dataset comes from a mix-
ture of early-stopped SAC policy and well-learned SAC policy.
Competing Baselines: We compare SRA with the well-validated
baselines and state-of-the-art offline imitation learning methods:
•BC-exp : Behavior cloning on expert data DE. The DEcon-
tains high-quality demonstrations but with limited quantity,
and thus causes serious generalization problem.
•DemoDICE [20]: DemoDICE approximates the state-action
distribution dπ(s,a)to both expert data with offline data,
treating offline data as a regularization, i.e., minπKL(dπ||DE)
+αKL(dπ||do)with the expectation of further improving
performance based on expert data.
•DWBC [47]: DWBC regards the offline data as a mixture of
expert-similar trajectories and low-quality trajectories, and
apply positive-unlabeled learning to build a discriminator.
The discriminator will evaluate unlabeled trajectories and
provide an expert-similarity score, followed by a weighted
behavior cloning.
•OTIL [28]: OTIL uses optimal transport to label the rewards
of offline trajectories based on their Wasserstein distance
from expert trajectories. It then employs offline reinforce-
ment learning algorithms to train an agent on the labeled
dataset. We implement offline RL using IQL [ 22], as described
in the original paper.
•MILO [6]: MILO attempts to mitigate the covariate shift,
though conducting the adversarial imitation learning with
the learned dynamic model.
•CLARE [52] CLARE integrates the dynamics conservatism
and expert similarity to estimates rewards for the model-
based augmented demonstrations. The the policy is learned
with the reward function and subsequent offline reinforce-
ment learning methods. We chosen the well-validated offline
RL algorithm, IQL, to implement the CLARE.
•UDS [50]: UDS annotates the unlabeled datasets with reward
0 (minimum rewards), and uses offline RL to train the agent
on the merged dataset. Compared to their setting has a high-
quality labeled dataset, our expert dataset does not have a
ground-truth reward label. Instead, we label them with 1
(maximum rewards) to implement offline RL. We also choose
the IQL as the specific algorithm for consistence.
•ROMI [45] is also a data augmentation method based on
reverse models, which generate conservative behaviors. De-
spite it being a method proposed for offline RL, we also
regard it as one of our ablation studies here, and have made
comparisons.
2612KDD ’24, August 25–29, 2024, Barcelona, Spain Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li.
Table 2: The results on D4RL benchmark. All values are normalized to lie between 0 and 100, where 0 corresponds to a random
policy and 100 corresponds to an expert. The best result in each setting is in bold.
DataSet BC-exp DemoDICE DWBC OTIL CLARE MILO ROMI UDS SRA
maze2d-umaze-sparse-v1 100.±11.6 88.7±10.4 25.8±5.65 128.±8.22 -3.08±6.02 75.0±11.2 154.±5.96 64.9±8.51 155.±6.20
maze2d-medium-sparse-v1 44.6±11.1 15.4±7.83 22.7±4.77 98.2±11.0 33.5±7.82 47.9±13.5 123.±10.5 83.0±8.84 147.±5.67
maze2d-large-sparse-v1 15.5±7.98 8.68±3.55 35.1±4.18 129.±14.4 18.6±9.12 51.2±17.1 101.±20.2 108.±16.7 150.±14.9
maze2d-umaze-dense-v1 70.6±9.55 69.1±9.21 39.2±4.77 100.±6.98 5.84±6.61 54.9±6.96 111.±6.23 62.3±6.99 113.±5.80
maze2d-medium-dense-v1 45.0±10.2 34.3±7.08 39.1±3.34 95.7±8.66 46.3±7.81 44.4±11.0 112.±9.10 87.3±7.80 138.±5.29
maze2d-large-dense-v1 18.2±8.57 21.7±6.30 56.1±5.56 120.±11.0 26.5±8.78 40.7±14.0 101.±16.6 109.±14.4 140.±11.4
hopper-medium 72.9±5.50 54.1±1.67 88.1±4.71 26.2±2.28 82.2±6.56 75.0±7.46 67.3±4.82 59.5±4.51 90.2±4.93
halfcheetah-medium 13.3±2.74 41.1±1.00 22.5±3.94 38.7±0.75 32.2±3.14 41.9±0.92 43.6±1.53 43.6±5.15 43.7±1.72
walker2d-medium 99.1±3.66 73.0±2.09 84.8±5.65 86.9±3.63 49.9±5.37 67.9±3.13 96.6±3.76 97.6±2.85 101.±3.60
ant-medium 51.3±6.87 91.2±3.79 37.5±5.95 72.4±5.68 68.5±7.35 92.0±3.55 92.7±6.46 87.3±5.10 88.9±7.18
hopper-medium-expert 72.9±5.50 98.6±4.32 99.4±4.43 42.5±3.70 93.9±5.81 90.9±5.42 100.±3.40 97.4±3.35 104.±3.37
halfcheetah-medium-expert 13.3±2.74 48.9±5.46 82.3±3.79 43.7±2.76 31.4±5.15 44.5±1.57 58.8±3.29 67.1±2.63 63.4±3.52
walker2d-medium-expert 99.1±3.66 93.1±5.49 106.±1.57 82.5±2.76 39.9±7.66 95.4±3.87 103.±2.12 103.±2.32 104.±4.88
ant-medium-expert 51.3±6.87 69.8±7.97 58.2±8.81 79.2±7.40 3.61±2.86 115.±4.63 105.±6.90 92.2±8.14 94.1±7.86
For a fair comparison, we use the same actor network as [ 47]
and same critic networks as [ 22]. For the learning of dynamic mod-
els, we keep the same network architecture and the corresponding
learning process as [ 45] for all methods which needs forward model
or reverse model. Moreover, for methods that require the use of
offline reinforcement learning, we consistently choose IQL, includ-
ing our SRA. We do not put much effort into the hyperparameter
tuning. All hyper-parameters are fixed with the original IQL imple-
mentation. Regarding reverse augmentation, in the navigation task,
we use a uniformly random reverse policy to generate as diverse
actions as possible, in the locomotion task, we use a conditional
variational auto-encoder to generate reasonable actions. The length
of reverse rollout is fixed as 5 across different settings. For all set-
tings, we obtain undiscounted average returns of the policy with
30 evaluations after training. Evaluation results are averaged over
three random seeds, and deviation is measured by 95% bootstrapped
confidence interval.
4.2 Main Results
We evaluate SRA in D4RL benchmark domains with 14 settings
and provide the results in the Table 2. From the results, we have
the following observations and analyses: The unsatisfactory per-
formance of BC-exp baseline indicates the difficulty of imitating
learning when the expert data is limited. DemoDICE uses all of
the offline data as a regularization for behavior cloning, without
considering the possibility that some of the unlabelled data may be
not suitable for direct use in behavior cloning, sometimes leading
to weaker performance compared to BC-exp. DWBC uses positive-
unlabeled learning to identify samples similar to expert behavior,
which can filter out the sub-optimal samples and reduce harm from
them. Despite this, the performance gain it brings is limited when
the overall data quality is not high. Compared with other model-
free offline imitation learning methods, the OTIL method performs
better overall. One plausible reason is that, although OTIL does not
directly focus on the trajectories from expert-unobserved states to
expert-observed states, the part of Q-learning in the IQL it usesimplicitly does this, which may lead to a satisfactory performance
of OTIL. Model-based methods, MILO and CLARE perform bet-
ter on locomotion tasks, but perform worse on navigation tasks,
even weaker than the BC baseline and other model-free methods.
This is because covariate shift is more serious in navigation tasks,
and the agent needs to make decisions in many expert-unobserved
states. Although MILO and CLARE train the dynamic model to
augment the dataset, they encourage the policy to make decisions
in areas near the expert-observed states, ignoring the exploration of
expert-unobserved states. In the navigation tasks, ROMI performs
well, which also indicates the potential of using reverse rollout
to mitigate covariate shift. Similarly, the dynamic programming
process of UDS, as we pointed out, can transfer the agent from
expert-unobserved states to expert-observed states, thereby im-
proving the long-term return, and the overall performance is also
acceptable. It is worth noting that our method, SRA, achieved the
best result in all settings in navigation tasks, which is a domain
with serious covariate shift problems. In locomotion tasks, SRA
also achieved competitive performance. These phenomena clearly
demonstrate the effectiveness of our proposal and indicate it pro-
vides a promising way to mitigate the covariate shift of offline
imitation learning.
4.3 Concerns on Self-Paced Process
In this work, we adaptively prioritize learning simple, reliable ex-
amples, and then gradually transition to exploring diverse expert-
unobserved states. To dive deeper into the learning process, we
conduct the visualization to demonstrate the learning process in
theMaze2D-Medium environment. As shown in the Figure 3, we
visualize the state-wise cumulative return of policy πand the data
coverage of corresponding sampled dataset DA.
From the results, we could find that at the beginning of learning,
the ability of the policy is relatively weak, and the data obtained
by the reverse model is mainly concentrated near the expert trajec-
tories. However, as the ability of the policy in the states near the
expert trajectories is improved, the data obtained by SRA sampling
2613Offline Imitation Learning with Model-based Reverse Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) State-wise cumulative return of policy πafter 0, 50000, 100000, and 150000 iterations.
(b) Re-sampled augmented dataset DRafter 0, 50000, 100000, and 150000 iterations.
Figure 3: Visualization during Self-paced Learning Process
Table 3: Ablation study with the Self-Paced module.
DataSet SRA w/o SP SRA
Umaze-sparse 143.±6.92 155.±6.20
Medium-sparse 110.±14.7 147.±5.67
Large-sparse 92.0±21.7 150.±14.9
Hoppper-medium 69.2±5.52 90.2±4.93
Halfcheetah-medium 44.2±1.33 43.7±1.72
Walker2d-medium 96.8±5.41 101.±3.60
Ant-medium 87.5±6.28 88.9±7.18
contains more diverse expert-unobserved states. In this process, the
improvement of policy generalization ability and SRA’s exploration
of expert-unobserved states help each other. Eventually, this allows
the policy to achieve higher cumulative return in almost all areas
of the medium maze.
In addition to the visualization analysis, we also provide a quanti-
tative ablation study with the self-paced module in Table 3. We can
find that in tasks like locomotion where the expert distribution is
relatively narrow, the self-paced process can also effectively gener-
ate data to improve performance better. These results could further
support the effectiveness of the proposed self-paced module.4.4 Concerns on Model-based Augmentation
The key difference between our work and previous model-based
offline imitation learning methods [ 6,52] is that we have used a
reverse model instead of the previous forward model. To delve
deeper into understanding this difference, in this subsection, we
provide a detailed visual demonstration in the navigation domain
(Maze2D-sparse), where the covariate shift problem is crucial.
Figure 4(a) shows the expert demonstrations and the augmented
trajectories by forward models of MILO [ 6]. We could find a dilemma,
the longer the trajectory of the forward model rollout, the further
it is from the expert-observed states, thus losing the guidance of
policy learning. Existing methods, like MILO and CLARE, constrain
their policies close to the expert trajectories to reduce the occur-
rence of this kind of situation. However, it’s hard for these kinds
of methods to learn the behaviors out of the expert support. The
evaluation of cumulative return in the Figure 4(b) also indicates this
limitation. The Figure 4(b) demonstrates the state-wise subsequent
cumulative return of MILO. We could find that in the states near the
expert trajectories, the subsequent cumulative return is relatively
high, which means that when the agent starts from these points, it
can obtain higher cumulative returns in the future, that is to say,
it can reach the goal. In states far from the expert trajectories, we
2614KDD ’24, August 25–29, 2024, Barcelona, Spain Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li.
(
a) Forward rollout of MILO
 (
b) Cumulative return of MILO
 (
c) Reverse rollout of SRA
 (
d) Cumulative return of SRA
Figure 4: Forward Augmentation v.s. Reverse Augmentation. The visualization on navigation tasks.
can see that the cumulative return of the MILO agent tends to be
close to 0, showing its limitations.
In contrast, our reverse model-based method encourage to ex-
plore the expert-unobserved states, as shown in the Figure 4(c).
We could find that the generated trajectories extend from expert-
unobserved states to expert-observed states, while providing be-
havioral samples that improve long-term return for these expert-
unobserved states. The longer the trajectory of the reverse model
rollout, the more expert-unobserved states can find the path toexpert-observed states, thereby improving their long-term return
and expanding the feasible state coverage of the learned agents.
The state-wise cumulative return of our method is also demon-
strated in the Figure 4(d). It could be found that our SRA provides
a significant improvement on the expert-unobserved states, which
support our claim to explore diverse expert-unobserved states. Even
in the large maze, our policy can start from the vast majority of
states to reach the goal, far beyond the support region provided by
the expert demonstrations.
2615Offline Imitation Learning with Model-based Reverse Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Combination of SRA and different offline reinforcement learning methods.
DataSet IQL SRA+IQL TD3BC SRA+TD3BC AWAC SRA+AWAC SAC-N SRA+SAC-N
maze2d-umaze-sparse-v1 64.9±8.51 155.±6.20↑38.1±12.9 145.±7.27↑68.6±14.5 135.±10.1↑151.±6.44 150.±6.47
maze2d-medium-sparse-v1 83.0±8.84 147.±5.67↑22.4±9.64 140.±8.55↑100.±13.5 87.8±16.4 147.±10.3 153.±7.36↑
maze2d-large-sparse-v1 108.±16.7 150.±14.9↑57.9±13.2 143.±17.7↑74.8±16.8 89.9±24.1↑128.±20.7 158.±18.0↑
hopper-medium 59.5±4.51 90.2±4.93↑57.2±1.90 96.4±5.74↑38.3±3.88 85.6±6.04↑3.33±0.49 107.±2.31↑
halfcheetah-medium 43.6±5.15 43.7±1.72↑43.2±0.82 1.30±1.25 42.0±1.77 44.9±1.95↑-.15±0.08 7.49±2.71↑
walker2d-medium 97.6±2.85 101.±3.60↑89.6±3.40 103.±3.58↑90.8±5.91 94.3±4.46↑4.19±0.42 86.5±7.72↑
ant-medium 87.3±5.10 88.9±7.18↑90.4±5.42 42.0±8.30 57.0±8.39 82.2±9.29↑-27.±3.40 47.2±7.43↑
Win/Tie/Loss 7/0/0 5/0/2 6/0/1 6/0/1
4.5 Scalability for Different RL Methods
As we discussed in the Subsection 3.4, our SRA is agnostic to the
specific offline reinforcement learning methods. In this subsection,
we validate the scalability via pairing with the different offline
reinforcement learning methods, includes IQL [ 22], TD3BC [ 10],
AWAC [ 31] and SAC-N [ 1]. We wrap the original offline reinforce-
ment learning methods into the offline imitation learning setting
through UDS, that is, we label the expert data and supplementary
data as 1 and 0 respectively. Under the same hyper-parameters, we
add SRA data augmentation for method comparison. The results on
both navigation and locomotion tasks are reported in Table 4. We
could find that our data augmentation module, SRA, consistently
improves these baseline methods and show stronger robustness.
Specifically, as an advanced offline reinforcement learning method,
IQL can effectively alleviate the overestimation of Q-learning. Its
performance is relatively stable in both SRA-equipped and non-
SRA-equipped situations. Therefore, it is also what we suggest as a
basic method in the SRA framework. Overall, the results across four
popular offline reinforcement learning methods clearly support the
scalability of SRA, demonstrating its superiority.
5 CONCLUSION
In this paper, we study the covariate shift problem of offline imi-
tation learning. The key difficulty is that it is challenging for the
agent to obtain trustworthy behavior on expert-unobserved states
for policy optimization. To overcome this issue, we present a novel
framework, offline imitation learning with Self-paced Reverse Aug-
mentation. This framework generates the trajectories from expert-
unobserved states to expert-observed states in a self-paced way.
When the agent encounters these expert-unobserved states, it can
follow the generated trajectory to reach the expert-observed states,
thereby improving the long-term return. To the best of our knowl-
edge, this is the first time to introduce the reverse data augmenta-
tion to the offline imitation learning. It is different from previous
methods based on the forward model. That is, it allows the strategy
to explore more diverse expert-unobserved states. In the empirical
studies, the effectiveness of our Self-paced Reverse Augmentation
has been verified in a series of benchmark tasks. Not only has it
achieved state-of-the-art performance, but it has also offered behav-
ioral guidance and enhanced capabilities in the expert-unobserved
states, providing a promising way to mitigate the covariate shift of
offline imitation learning.This work is inspired by the BCDP [ 38], which presents the
idea of leading the agent from expert-unobserved states to expert-
observed states. We propose a reverse-model-based solution to
generate diverse trajectories from expert-unobserved states to the
expert-observed states. This strategy has a significant advantage in
mitigating covariate shifts compared to previous forward-model-
based methods. A concurrent work, ILID [ 51], presents a similar idea
and proposes a model-free data selection method leading the agents
to focus on the trajectories whose resultant states fall within the
expert data manifold. Further exploration of model-free strategies
and the unified solutions with our model-based framework is an
interesting direction. Another potential future direction is to extend
Self-paced Reverse Augmentation with advanced model learning
methods, such as dynamic quantization, to further improve the
quality of augmented trajectories.
ACKNOWLEDGMENTS
This research was supported by Leading-edge Technology Pro-
gram of Jiangsu Science Foundation (BK20232003), National Science
Foundation of China (62176118) and the Postgraduate Research &
Practice Innovation Program of Jiangsu Province (KYCX24_0233).
REFERENCES
[1]Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. 2021.
Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble.
InAdvances in Neural Information Processing Systems 34. Virtual Event, 7436–
7447.
[2]Saurabh Arora and Prashant Doshi. 2021. A survey of inverse reinforcement
learning: Challenges, methods and progress. Artificial Intelligence 297 (2021),
103500.
[3]Raunak P. Bhattacharyya, Blake Wulfe, Derek J. Phillips, Alex Kuefler, Jeremy
Morton, Ransalu Senanayake, and Mykel J. Kochenderfer. 2023. Modeling Human
Driving Behavior Through Generative Adversarial Imitation Learning. IEEE
Transactions on Intelligent Transportation Systems 24, 3 (2023), 2874–2887.
[4]Andreea Bobu, Marius Wiggert, Claire J. Tomlin, and Anca D. Dragan. 2022.
Inducing structure in reward learning by learning features. International Journal
of Robotics Research 41, 5 (2022), 497–518.
[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In
Advances in Neural Information Processing Systems 33. Virtual Event, 1877–1901.
[6]Jonathan D. Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and
Wen Sun. 2021. Mitigating Covariate Shift in Imitation Learning via Offline Data
With Partial Coverage. In Advances in Neural Information Processing Systems 34.
Virtual Event, 965–979.
[7]Bin Fang, Shidong Jia, Di Guo, Muhua Xu, Shuhuan Wen, and Fuchun Sun. 2019.
Survey of imitation learning for robotic manipulation. International Journal of
2616KDD ’24, August 25–29, 2024, Barcelona, Spain Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, and Yu-Feng Li.
Intelligent Robotics and Applications 3, 4 (2019), 362–369.
[8]Xing Fang, Qichao Zhang, Yinfeng Gao, and Dongbin Zhao. 2022. Offline Rein-
forcement Learning for Autonomous Driving with Real World Driving Data. In
25th IEEE International Conference on Intelligent Transportation Systems. Macau,
China, 3417–3422.
[9]Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
2020. D4RL: Datasets for Deep Data-Driven Reinforcement Learning. CoRR
abs/2004.07219 (2020). arXiv:2004.07219
[10] Scott Fujimoto and Shixiang Shane Gu. 2021. A Minimalist Approach to Offline
Reinforcement Learning. In Advances in Neural Information Processing Systems
34. Virtual Event, 20132–20145.
[11] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-Policy Deep Rein-
forcement Learning without Exploration. In Proceedings of the 36th International
Conference on Machine Learning. Long Beach, CA, 2052–2062.
[12] Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy
Lillicrap, Sergey Levine, Hugo Larochelle, and Yoshua Bengio. 2019. Recall Traces:
Backtracking Models for Efficient Reinforcement Learning. In 7th International
Conference on Learning Representations. New Orleans, LA.
[13] Lan-Zhe Guo, Yi-Ge Zhang, Zhi-Fan Wu, Jie-Jing Shao, and Yu-Feng Li. 2022.
Robust Semi-Supervised Learning when Not All Classes have Labels. In Advances
in Neural Information Processing Systems 35. 3305–3317.
[14] Lan-Zhe Guo, Zhenyu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. 2020.
Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data. In Pro-
ceedings of the 37th International Conference on Machine Learning. 3897–3906.
[15] Keith J Holyoak and Dan Simon. 1999. Bidirectional reasoning in decision making
by constraint satisfaction. Journal of Experimental Psychology: General 128, 1
(1999), 3.
[16] Hao Hu, Yiqin Yang, Qianchuan Zhao, and Chongjie Zhang. 2023. The Provable
Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning. In The
11th International Conference on Learning Representations. Kigali, Rwanda.
[17] Firas Jarboui and Vianney Perchet. 2021. Offline inverse reinforcement learning.
arXiv preprint arXiv:2106.05068 (2021).
[18] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. 2020. Strictly Batch
Imitation Learning by Energy-based Distribution Matching. In Advances in Neural
Information Processing Systems 33. 7354–7365.
[19] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhen-Zhong Lan, Shiguang Shan, and Alexan-
der G. Hauptmann. 2014. Self-Paced Learning with Diversity. In Advances in
Neural Information Processing Systems 27. Montreal, Canada, 2078–2086.
[20] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang,
Hongseok Yang, and Kee-Eung Kim. 2022. DemoDICE: Offline Imitation Learn-
ing with Supplementary Imperfect Demonstrations. In The 10th International
Conference on Learning Representations. Virtual Event.
[21] W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter
Stone. 2023. Reward (Mis)design for autonomous driving. Artificial Intelligence
316 (2023), 103829.
[22] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. 2022. Offline Reinforcement
Learning with Implicit Q-Learning. In The Tenth International Conference on
Learning Representations. Virtual Event.
[23] M. Pawan Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-Paced Learning
for Latent Variable Models. In Advances in Neural Information Processing Systems
23. Vancouver, Canada, 1189–1197.
[24] Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. 2020. Bidirectional Model-
based Policy Optimization. In Proceedings of the 37th International Conference on
Machine Learning. 5618–5627.
[25] Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. 2020.
Context-aware Dynamics Model for Generalization in Model-Based Reinforce-
ment Learning. In Proceedings of the 37th International Conference on Machine
Learning. 5757–5766.
[26] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline Rein-
forcement Learning: Tutorial, Review, and Perspectives on Open Problems. CoRR
abs/2005.01643 (2020).
[27] Minghuan Liu, Hanye Zhao, Zhengyu Yang, Jian Shen, Weinan Zhang, Li Zhao,
and Tie-Yan Liu. 2021. Curriculum Offline Imitating Learning. In Advances in
Neural Information Processing Systems 34. 6266–6277.
[28] Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc Pe-
ter Deisenroth. 2023. Optimal Transport for Offline Imitation Learning. In The
11th International Conference on Learning Representations.
[29] Deyu Meng, Qian Zhao, and Lu Jiang. 2017. A theoretical understanding of
self-paced learning. Information Sciences 414 (2017), 319–328.
[30] Luc Le Mero, Dewei Yi, Mehrdad Dianati, and Alexandros Mouzakitis. 2022. A
Survey on Imitation Learning Techniques for End-to-End Autonomous Vehicles.
IEEE Transactions on Intelligent Transportation Systems 23, 9 (2022), 14128–14147.[31] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. 2020. Accelerat-
ing Online Reinforcement Learning with Offline Datasets. CoRR abs/2006.09359
(2020).
[32] Andrew Y. Ng and Stuart Russell. 2000. Algorithms for Inverse Reinforcement
Learning. In Proceedings of the 17th International Conference on Machine Learning.
Stanford, CA, 663–670.
[33] Dean Pomerleau. 1988. ALVINN: An Autonomous Land Vehicle in a Neural
Network. In Advances in Neural Information Processing Systems 1. Denver, CO,
305–313.
[34] Rongjun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan
Zhang, and Yang Yu. 2022. NeoRL: A Near Real-World Benchmark for Offline
Reinforcement Learning. In Advances in Neural Information Processing Systems
35. Virtual Event.
[35] Nived Rajaraman, Lin F. Yang, Jiantao Jiao, and Kannan Ramchandran. 2020.
Toward the Fundamental Limits of Imitation Learning. In Advances in Neural
Information Processing Systems 33. Virtual Event.
[36] Fumihiro Sasaki and Ryota Yamashina. 2021. Behavioral Cloning from Noisy
Demonstrations. In 9th International Conference on Learning Representations.
Virtual Event.
[37] Jie-Jing Shao, Lan-Zhe Guo, Xiao-Wen Yang, and Yu-Feng Li. 2022. LOG: Active
Model Adaptation for Label-Efficient OOD Generalization. In Advances in Neural
Information Processing Systems 35. New Orleans, LA.
[38] Jie-Jing Shao, Hao-Sen Shi, Tian Xu, Lan-Zhe Guo, Yang Yu, and Yu-Feng Li. 2023.
Offline Imitation Learning without Auxiliary High-quality Behavior Data. (2023).
https://openreview.net/forum?id=7fxzVTSgZC
[39] Jie-Jing Shao, Xiao-Wen Yang, and Lan-Zhe Guo. 2024. Open-set learning under
covariate shift. Machine Learning 113, 4 (2024), 1643–1659.
[40] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. 2021. S4RL: Surprisingly
Simple Self-Supervision for Offline Reinforcement Learning in Robotics. In 6th
Conference on Robot Learning. London, UK, 907–917.
[41] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning Structured Output
Representation using Deep Conditional Generative Models. In Advances in Neural
Information Processing Systems 28. Montreal, Canada, 3483–3491.
[42] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller. 2007. Covariate
shift adaptation by importance weighted cross validation. Journal of Machine
Learning Research 8, 5 (2007).
[43] Richard S Sutton, Andrew G Barto, et al .1998. Introduction to reinforcement
learning. Vol. 135. MIT press Cambridge.
[44] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke
Zhu, and Anima Anandkumar. 2023. MimicPlay: Long-Horizon Imitation Learn-
ing by Watching Human Play. In 7th Conference on Robot Learning.
[45] Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan Li, and
Chongjie Zhang. 2021. Offline Reinforcement Learning with Reverse Model-
based Imagination. In Advances in Neural Information Processing Systems 34.
29420–29432.
[46] Grover J Whitehurst and Ross Vasta. 1975. Is language acquired through imita-
tion? Journal of Psycholinguistic Research 4 (1975), 37–59.
[47] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. 2022. Discriminator-
Weighted Offline Imitation Learning from Suboptimal Demonstrations. In Pro-
ceedings of the 39th International Conference on Machine Learning. Baltimore, MD,
24725–24742.
[48] Tian Xu, Ziniu Li, and Yang Yu. 2022. Error Bounds of Imitating Policies and
Environments for Reinforcement Learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence 44, 10 (2022), 6968–6980.
[49] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. 2023. Reinforcement
Learning in Healthcare: A Survey. Comput. Surveys 55, 2 (2023), 5:1–5:36.
[50] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and
Sergey Levine. 2022. How to Leverage Unlabeled Data in Offline Reinforcement
Learning. In Proceedings of the 39th International Conference on Machine Learning.
Baltimore, MD, 25611–25635.
[51] Sheng Yue, Jiani Liu, Xingyuan Hua, Ju Ren, Sen Lin, Junshan Zhang, and Yaoxue
Zhang. 2024. How to Leverage Diverse Demonstrations in Offline Imitation
Learning. In Proceedings of the 41st International Conference on Machine Learning.
[52] Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, and
Junshan Zhang. 2023. CLARE: Conservative Model-Based Reward Learning for
Offline Inverse Reinforcement Learning. In The 11th International Conference on
Learning Representations. Kigali, Rwanda.
[53] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. 2024. Adaptivity
and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex
Optimization. Journal of Machine Learning Research 25, 98 (2024), 1 – 52.
[54] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. 2008.
Maximum Entropy Inverse Reinforcement Learning. In Proceedings of the 23rd
AAAI Conference on Artificial Intelligence. Chicago, IL, 1433–1438.
2617