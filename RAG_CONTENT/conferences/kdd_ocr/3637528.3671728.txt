Layer-wise Adaptive Gradient Norm Penalizing Method for
Efficient and Accurate Deep Learning
Sunwoo Lee
Inha University
Department of Computer Engineering
Incheon, Republic of Korea
sunwool@inha.ac.kr
Abstract
Sharpness-aware minimization (SAM) is known to improve the
generalization performance of neural networks. However, it is not
widely used in real-world applications yet due to its expensive
model perturbation cost. A few variants of SAM have been pro-
posed to tackle such an issue, but they commonly do not alleviate
the cost noticeably. In this paper, we propose a lightweight layer-
wise gradient norm penalizing method that tackles the expensive
computational cost of SAM while maintaining its superior gen-
eralization performance. Our study empirically proves that the
gradient norm of the whole model can be effectively suppressed
by penalizing the gradient norm of only a few critical layers. We
also theoretically show that such a partial model perturbation does
not harm the convergence rate of SAM, allowing them to be safely
adapted in real-world applications. To demonstrate the efficacy of
the proposed method, we perform extensive experiments compar-
ing the proposed method to mini-batch SGD and the conventional
SAM using representative computer vision and language modeling
benchmarks.
CCS Concepts
•Computing methodologies →Neural networks.
Keywords
Deep Learning, Sharpness-Aware Minimization, Layer-wise
ACM Reference Format:
Sunwoo Lee. 2024. Layer-wise Adaptive Gradient Norm Penalizing Method
for Efficient and Accurate Deep Learning. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671728
1 Introduction
Sharpness-aware minimization (SAM) [ 9] improves the generaliza-
tion performance of Machine Learning models by penalizing the
norm of gradients. The reduced gradient norm results in leading
the model to a flat region of the parameter space [ 33]. It is already
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671728widely known that the convergence to a flat region helps improve
the generalization performance [2, 5, 11, 13, 16, 24, 28, 29].
Despite its superior generalization performance, SAM is not pop-
ularly used in real-world ML applications yet due to the expensive
computational cost. Specifically, it calculates the gradients twice,
one for the model perturbation (gradient ascent) and the other for
the model update (gradient descent). Thus, it is crucial to develop
an efficient gradient norm penalizing method to allow real-world
applications to adopt SAM and enjoy improved generalization per-
formance.
A few recent studies tackle the expensive computational cost
issue in SAM. Du et al. perturb a random subset of the model
parameters to reduce the extra computational cost [ 8]. While this
work presents promising results, they evaluated the performance
only using the parameter-wise perturbation probability of 0.5∼0.6.
Thus, it is not clear whether the generalization performance can
be maintained when the fraction of the perturbed model is small.
Liu et al. directly reduce the gradient ascent cost by reusing the
gradient ascent for multiple iterations [ 20]. However, the gradient
can be safely reused only when the dataset gives stable gradients for
many iterations. That is, the effectiveness of the periodic gradient
ascent computations depends on the given data characteristics,
making it less practical. Mi et al. find a critical subset of the model
parameters based on Fisher information and perturb them only[ 22].
Although this work provides critical insight into the partial model
perturbation approach, they first calculate the full gradient ascent
and then apply a sparse filter, having a limited performance gain.
In this paper, we propose a novel layer-wise adaptive gradient
norm penalizing method that tackles the expensive model perturba-
tion cost issue in SAM while maintaining its superior generalization
performance. Instead of perturbing the whole model parameters,
we propose to selectively perturb only a few network layers that
strongly affect the model’s generalization performance. Our empiri-
cal study shows that a few layers consistently have a larger gradient
norm than other layers during training. Interestingly, many net-
works tend to have a large gradient norm at the output-side layers.
By leading those layers to a flat region in the parameter space, it
is expected that the entire model will have a much smaller gradi-
ent norm, thereby improving its generalization performance. Our
theoretical analysis also shows that such a layer-wise method does
not harm the convergence rate regardless of which and how many
layers are selected to be perturbed.
Our method is differentiated from the existing SAM variants in
the sense that it exploits the internal data characteristics of neural
networks to make the best trade-off between the generalization
performance and the extra computational cost, rather than just
 
1518
KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
randomly perturbing a subset of model parameters. First, we em-
pirically prove that only a subset of the model parameters strongly
affect the model’s generalization performance. The proposed layer-
wise method effectively suppresses the gradient norm of the whole
model parameters and it results in achieving the improved gen-
eralization performance. Second, we also show that the gradient
norm of the entire model can be reduced at a marginal extra com-
putational cost by the layer-wise adaptive model perturbation. Our
proposed method entirely skips the gradient computation at the
layers that are not perturbed. This approach not only mitigates
the extra computational cost but also eases the implementation
complexity, making SAM more practical.
To demonstrate the efficacy of our layer-wise gradient penalizing
method, we compare it to the SOTA variants of SAM as well as
the conventional mini-batch SGD. Our experimental results show
that the full model gradient norm can be effectively penalized by
the proposed layer-wise method, and it results in achieving similar
accuracy to the full model perturbation method (SAM). In addition,
the proposed method pushes the training throughput towards that
of the conventional mini-batch SGD. For example, when training
ResNet20 on CIFAR-10, our method achieves a similar validation
accuracy to SAM while it has just 11%reduced throughput as com-
pared to the mini-batch SGD. Our empirical and theoretical studies
demonstrate that the gradient norm penalizing method can be effi-
ciently employed in real-world applications.
2 Related Work
Foret et al. proposed Sharpness-Aware Minimization, an optimiza-
tion method that leads the model toward a flat region in the pa-
rameter space so that it achieves better generalization performance
[9]. Zheng et al. also concurrently proposed a similar method in
[34]. After its superior generalization performance had been ob-
served, several researchers analyzed the theoretical performance to
better understand its behaviors. Andriushchenko et al. presented
a thorough theoretical analysis of the SAM’s performance in [ 1].
Bartlett et al. analyzed the dynamics of SAM for convex quadratic
functions [ 3]. Zhao et al. introduced a generalized version of SAM,
which allows finer-grained model perturbation with another hyper-
parameter [ 33]. Zhoe et al. analyzed the behaviors of SAM for
problems with class imbalance issues [ 35]. Recently, Caldarola et al.
applied SAM to Federated Learning, showing a practical use-case
of it [4].
Recently, several variants of SAM have been proposed. Kwon et
al. developed adaptive SAM, which alleviates the sensitivity to pa-
rameter re-scaling by adaptively adjusting the model perturbation
terms [ 15]. Zhang et al. designed Gradient Norm-Aware Minimiza-
tion which boosts up the generalization performance by utilizing
approximated second-order information [ 32]. Zhuang et al. pro-
posed an alternative way of leading the model to a flat region in the
parameter space by using a customized metric, surrogate gap [ 36].
Mi et al. found critical parameters based on Fisher information and
dropped the gradient of the rest of the parameters in the gradient
ascent step to stabilize the training [ 22]. Jiang et al. accelerated
SAM by applying adaptive learning rate adjustment and theoreti-
cally analyzed its performance [ 12]. However, these studies do not
discuss the expensive model perturbation cost in SAM.Liu et al. efficiently perturbed model parameters by re-using the
gradients multiple times [ 20]. Du et al. perturbed a random subset
of the model parameters to save the model perturbation cost [ 8].
Qu et al. applied SAM to Federated Learning and demonstrated that
the client-side SAM can improve the generalization performance
of the global model [ 25]. Unfortunately, these methods either do
not noticeably reduce the computational cost or reduce the compu-
tational cost only when the dynamics of the stochastic gradients
remain stable.
3 Layer-Wise Adaptive Gradient Norm
Penalizing Method
3.1 Problem Setting
We consider an empirical risk minimization problem with a special
loss function that penalizes the gradient norm:
𝐿(𝑤):=𝐿𝑆(𝑤)+𝜆∥∇𝐿𝑆(𝑤)∥𝑝. (1)
In the above equation, ∥·∥𝑝denotes the 𝐿𝑝norm,𝑁is the num-
ber of training samples, and 𝐿𝑆(𝑤)is the empirical loss function
1
𝑁Í𝑁
𝑖=1𝑙(𝑤,𝜉𝑖)where𝜉𝑖is a training data sample 𝑖. The coefficient
𝜆determines how strongly the gradient norm is penalized. We focus
on the𝐿2norm in this work.
Several previous works already showed how to calculate the
gradient of (1) [ 1,20,33]. As shown in [ 33], the gradient of (1) is
calculated as follows.
∇𝐿(𝑤)=(1−𝛼)∇𝐿𝑆(𝑤)+𝛼∇𝐿𝑆
𝑤+𝜌∇𝐿𝑆(𝑤)
∥∇𝐿𝑆(𝑤)∥
,(2)
where𝜌is a small constant used to approximate the second term
of the right-hand side and 𝛼=𝜆
𝜌. Note that ‘sharpness-aware
minimization’, SAM, is a special case where 𝛼=1. A majority of
existing works simplify (2) by omitting the gradient normalization
to provide the convergence guarantee [1, 12, 22, 27] as follows.
∇𝐿(𝑤)=(1−𝛼)∇𝐿𝑆(𝑤)+𝛼∇𝐿𝑆(𝑤+𝑟∇𝐿𝑆(𝑤)), (3)
where𝑟is a small constant. Following such a convention, we con-
sider the simplified version of the gradients in the rest of the paper.
3.2 Layer-Wise Gradient Norm Penalizing
Method
To obtain∇𝐿(𝑤)shown in (2), the gradient should be calculated
twice, doubling the computational cost of training. We propose a
layer-wise gradient norm penalizing method to tackle this critical
issue. The layer-wise approach has been recently investigated in a
several different contexts [ 17,18,21,26,30]. Instead of penalizing
the norm of the total gradients as (1), we penalize the norm of only
a few chosen layers using the loss function is as follows.
𝐿(𝑤):=𝐿𝑆(𝑤)+𝜆∥∇𝐿𝑆(𝑤′)∥𝑝, (4)
𝑤′⊂𝑤={𝑤1,···,𝑤𝐿}, (5)
where𝐿is the number of network layers. The 𝑤is the full model
that contains 𝐿layers and𝑤𝑖is the𝑖𝑡ℎlayer. Consequently, the
layer-wise gradient of (4) for the layers in 𝑤′becomes as follows.
∇𝐿(𝑤𝑖)=(1−𝛼)∇𝐿𝑆(𝑤𝑖)+𝛼∇𝐿𝑆(𝑤𝑖+𝑟∇𝐿𝑆(𝑤𝑖)). (6)
 
1519Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
The layer-wise gradient of all the other layers is just simply ∇𝐿(𝑤𝑖)=
∇𝐿𝑆(𝑤𝑖). Unless every layer equally contributes to the models’ gen-
eralization performance, there should exist subsets of layers 𝑤′that
give better generalization performance than other subsets. Our goal
is to find such subsets during training and apply the gradient norm
penalizing method only to them so that the model achieves good
generalization performance at a marginal model perturbation cost.
3.3 Convergence Analysis
Before we discuss how to determine which layers to perturb, we
first provide a performance guarantee of the layer-wise gradient
penalizing method. We analyze the convergence properties of the
proposed method for smooth and non-convex optimization prob-
lems. Our analysis assumes the following conditions on the loss
functionℓ𝑖(𝑤):=ℓ(𝑤,𝜉𝑖),𝑖∈{1,···,𝑁}, where𝑁is the number
of training samples and 𝜉𝑖is the𝑖𝑡ℎtraining sample.
Assumption 1. (Bounded variance): There exists a constant 𝜎≥0
that satisfies E[∥∇ℓ𝑖(𝑤)−∇𝐿(𝑤)∥2]≤𝜎2for all𝑖∈{1,···,𝑁}
and𝑤∈R𝑑.
Assumption 2. (𝛽-smoothness): There exists a constant 𝛽≥0that
satisfies∥∇ℓ𝑖(𝑢)−∇ℓ𝑖(𝑣)∥≤𝛽∥𝑢−𝑣∥for all𝑖∈{1,···,𝑁}and
𝑤∈R𝑑.
Then, we present the performance guarantee of mini-batch SGD
when the proposed layer-wise gradient norm penalizing method is
applied as follows.
Theorem 1. Assume the 𝛽-smooth loss function and the bounded
gradient variance. Then, if 𝜂≤1
2𝛽and𝑟≤1
2𝛽, Algorithm 1 with a
batch size of 𝑏satisfies:
1
𝑇𝑇−1∑︁
𝑡=0E
∥∇𝐿(𝑤𝑡)∥2
≤4
𝑇𝜂(𝐿(𝑤0)−E[𝐿(𝑤𝑇)])
+4
𝜂𝛽+𝛽2𝑟2𝜎2
𝑏. (7)
See the Appendix for proof.
Remark 1. (Finite Horizon Result) With diminishing 𝜂and𝑟such
that𝜂=1
𝛽√
𝑇and𝑟=1
𝛽4√
𝑇we guarantee the convergence of
Algorithm 1 as follows.
1
𝑇𝑇−1∑︁
𝑡=0E
∥∇𝐿(𝑤𝑡)∥2
≤4𝛽√
𝑇(𝐿(𝑤0)−E[𝐿(𝑤𝑇)])
+8𝜎2
𝑏√
𝑇.
Thus, with a sufficiently small diminishing 𝜂and𝑟, the right-hand
side has a complexity of O(1√
𝑇), which is the same as the typical
convergence complexity of SAM [1, 12, 27].
Remark 2. (Layer-wise Model Perturbation) The result shows that
the convergence rate is maintained regardless of which layers are
perturbed. Unless the gradient goes to zero at all the layers that
are not perturbed, we can expect that the second term on the right-
hand side in (7) will be most likely smaller than the derived bound
in practice, making it converge faster. Thus, we can conclude that
users have the freedom to choose which layers to perturb, focus-
ing only on how to strike a good balance between generalization
performance and the model perturbation cost.
00.10.20.3
40 80 120 0 0 20 40 60 80 10000.20.4Gradient  L-2 Norm
CIFAR -10 (ResNet20) Oxford_Flowers102 (WRN -16)Epoch EpochLayer 1Layer 4,8,12Layer 16
Layer 20
Layer 3,6Layer 9,1Layer 12Layer 15Figure 1: The layer-wise gradient norm curves of a) CIFAR-
10 (ResNet20) training and b) Oxford_flowers102 (Wide-
ResNet16). All the layers show consistent gradient norms
throughout the training epochs.
Now, in the following subsection, let us focus on how to find
the minimal set of layers 𝑤′that achieves similar generalization
performance to the full gradient norm penalizing method.
3.4 Adaptive Layer Selection
Layer-Wise Data Characteristics Analysis – Our empirical
study finds that the gradient norm delivers useful information
regarding how individual layers contribute to the model’s gener-
alization performance. Specifically, the layers have a consistent
magnitude of the gradients throughout the training and the mag-
nitude is quite stable while the learning rate remains the same.
Figure 1 shows the layer-wise gradient norm curves of CIFAR-10
[14] (ResNet-20 [ 10]) and Oxford_flowers102 [ 23] (WRN-16 [ 31]).
We see that some output-side layers have a noticeably larger gradi-
ent norm than others. Assuming a strong correlation between the
gradient norm and the model’s generalization performance [ 32,33],
we can consider that those layers strongly affect the model’s gener-
alization performance. Even if the gradient penalizing method is
applied to some layers, if their gradient norms are small, they will
have a non-negligible extra computational cost while not making a
meaningful impact on the generalization performance.
Figure 2 shows the layer-wise loss landscapes measured from
Wide-ResNet16 training on Oxford_Flowers102. We used the loss
visualization method proposed in [ 19]. For each layer, 1) we pre-
pare two orthogonal random vectors, 2) adjust each layer using
them, and then 3) collect the training loss at 40×40different grid
points in the parameter space. See the further details about the
visualization settings. Interestingly, we see that the layers show
extremely different loss landscapes. Such a tendency is well aligned
with what we observed on the layer-wise gradient norm curves
shown in Figure 1. These visualized loss landscapes provide insights
into which layers should be perturbed to maximize the efficacy of
the layer-wise gradient norm penalization.
Adaptive Layer Selection Algorithm – The above analysis leads
us to design a layer-wise adaptive gradient penalizing method as
follows. After every model update, we first collect the gradient
norm at all individual layers. Then, choose 𝑘layers,𝑤′in (5), that
have the largest gradient norm among 𝐿layers. In the following
iteration, perturb only the layers in 𝑤′instead of the total model.
Algorithm 1 shows the described steps. The algorithm selectively
 
1520KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
Layer 1 Layer 3 Layer 6
Layer 9 Layer 12 Layer 15
Figure 2: The loss landscape of Wide-ResNet16 trained on Oxford_Flowers102. Each layer has noticeably different loss landscapes.
Algorithm 1 Layer-wise Adaptive Gradient Norm Penalizing
Method (Layer-wise SAM)
1:Input:𝑘: the number of layers to perturb.
2:Initialize the layer-wise gradient norms to 0.
3:for𝑡=0to𝑇−1do
4:𝑤′←𝑘layers with the top gradient norms.
5: Compute∇𝐿𝑆(𝑤𝑖),∀𝑖∈𝑤′
6: Compute∇𝐿𝑆(𝑤𝑖+𝑟∇𝐿𝑆(𝑤𝑖)),∀𝑖∈𝑤
7: Compute the update: ∇𝐿(𝑤𝑖)using Eq. 6
8: Update the model 𝑤using∇𝐿(𝑤𝑖)
9: Collect∥∇𝐿𝑆(𝑤𝑖+𝑟∇𝐿𝑆(𝑤𝑖))∥2,∀𝑖∈𝑤
10:end for
11:Output:𝑤
perturbs the layers using gradient ascent and then calculates the
gradient descent at lines 5∼7. The layer-wise gradient norm values
are collected at line 9. Note that the extra computation of collecting
the gradient norm is almost negligible as compared to the forward
pass or the backward pass cost.
We consider 𝑘to be a tunable hyper-parameter. If 𝑘=𝐿, it
becomes the conventional SAM. On the other hand, if 𝑘=0, it
becomes the vanilla SGD. As 𝑘increases, the model perturbation
cost increases while the gradient norm is more strongly penalized.
However, as shown in Figure 1, some layers tend to have an al-
most negligible gradient magnitude. Thus, users have to find the
minimum value of 𝑘that sufficiently suppresses the gradient norm
having a minimal perturbation cost. We found that the 𝑘was tuned
to a small value between 2∼8across many different datasets, not
only the computer vision benchmarks but also language modeling
benchmarks. Thus, one can begin the tuning with 𝑘=4and easily
find the best setting by a few grid searches.
As compared to the SOTA parameter-wise perturbation meth-
ods [ 8,22], our layer-wise method has three critical benefits. First,
the layer-wise method clearly reduces the gradient computation
cost without any help of sparse matrix multiplication features. We
believe that such a low implementation complexity will encourage
SAM techniques to be more easily adopted to real-world appli-
cations. Second, the error propagation cost can be significantlyreduced. The parameter-wise method still should propagate the
errors through all the layers, limiting the performance gain. Finally,
our gradient norm-based layer selection enables users to make a
practical trade-off between the model perturbation cost and the
generalization performance. By perturbing the layers with a high
gradient norm, we expect the gradient norm of the whole model
to be effectively suppressed while considerably reducing the total
model perturbation cost.
4 Performance Evaluation
4.1 Experimental Settings
System Configuration – We evaluate the performance of the
proposed method using TensorFlow 2.9.3. The training software is
written in Python and parallelized with MPI to use two NVIDIA
RTX A6000 GPUs. The training throughput (images / sec) takes into
account both the computation time for local training and the MPI
communication time for averaging the locally calculated gradients
across the GPUs. The reported accuracy results are averaged across
at least three independent runs.
Dataset and Hyper-Parameters – We use three representative
computer vision benchmarks, CIFAR-10 [ 14], CIFAR-100, and Ox-
ford_Flowers102 [ 23]. For the three datasets, we use ResNet-20 [ 10],
Wide-ResNet28-10 (WRN28) [ 31], and Wide-ResNet16-2 (WRN16),
respectively. In addition, we also run fine-tuning experiments using
Vision Transformer (ViT) [ 7] and CIFAR-100. We attached two fully-
connected layers at the end of the ViT-b16 pretrained on ImageNet
and then fine-tune only the new layers. The batch size is 128 for
both CIFAR datasets and 40 for Oxford_Flowers102. The learning
rate starts at 0.1 and is decayed by a factor of 10 twice. The number
of epochs is 150, 200, and 100, for the three benchmarks respectively.
Because the TensorFlow version of Oxford_Flowers102 contains
only 12%of the total dataset as training data, we instead used the
test data that takes up ∼70%for training. The validation accuracy
is measured using the rest of the dataset.
 
1521Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Dataset Model Batch size L.R. Epochs Vaniila SGD Generalized SAM LookSAM Layer-wise SAM
CIFAR-10 ResNet20 128 0.1 150 91.96±0.3% 92.64±0.4% 92.14±0.4%(3) 92.81±0.3%(4)
CIFAR-100 Wide-ResNet28 128 0.1 200 79.41±0.4% 80.83±0.5% 80.13±0.3%(2) 81.25±0.4%(4)
Oxford_Flowers102 Wide-ResNet16 40 0.1 100 69.82±0.4% 74.77±0.5% 73.53±0.6%(3) 75.56±0.6%(2)
CIFAR-100 (Fine-Tuning) ViT-b16 128 0.001 10 90.73±0.5% 91.46±0.9% 91.15±0.7%(2) 91.36±0.5%(1)
Table 1: The image classification performance comparison.
CIFAR -10 (ResNet -20)Epoch
CIFAR -100 (WRN -28)Epoch
Oxford_Flowers102 (WRN -16)EpochTraining Loss
(Cross Entropy)Validation Accuracy (%)
020 40 60 80100 120 1400.40.81.21.62.0
0 50 100 150 2001.02.03.04.0
0 20 40 60 80 1001.02.03.04.0
020 40 60 80100 120 1400.80.9
0 100 2000.50.60.70.8
0 20 40 60 80 1000.20.40.60.8Layer -wise SAM Generalized SAM Mini-batch SGD LookSAM
CIFAR -100 (ViT -b16)Epoch
2 4 6 8 101.01.52.02.53.03.5
2 4 6 8 100.700.750.800.850.900.95
Figure 3: The learning curves of CIFAR-10, CIFAR-100, and Oxford_Flowers102. The hyper-parameters are shown in Table 1.
4.2 Comparative Study
We compare the proposed method to the SOTA variants of SAM,
generalized SAM [ 33] and LookSAM [ 20], as well as the conven-
tional mini-batch SGD. Table 1 shows the performance comparison
and the hyper-parameter settings and Figure 3 shows the corre-
sponding learning curves. The generalized SAM has two hyper-
parameters, 𝑟and𝛼, as shown in (3). We tuned them by a grid search
with a unit of 0.1, and the best setting was 𝑟=0.1and𝛼=0.8for
CIFAR datasets and 𝑟=0.1and𝛼=0.7for Oxford_Flowers102. The
numbers in the bracket on LookSAM column indicate the gradient
ascent re-calculation interval. We set the re-calculation interval to
the maximum value that gives higher accuracy than the mini-batch
SGD, expecting the maximized throughput without much losing
the accuracy. The numbers in the bracket on Layer-wise SAM col-
umn indicate the number of layers to which the gradient norm
penalizing method is applied.
Overall, the proposed method, ‘Layer-wise SAM’ achieves val-
idation accuracy comparable to or even slightly higher than the
generalized SAM in all the four benchmarks. This result empiri-
cally proves that a few critical layers strongly affect the model’s
generalization performance. By applying the gradient norm penal-
izing method to only a few critical layers, our method efficiently
suppresses the gradient norm, achieving superior validation accu-
racy. Regardless of how many layers employ the gradient normpenalizing method, the training loss converges more slowly than
the mini-batch SGD. This result is aligned with our understand-
ing of convergence properties as shown in Section 3.3. We found
that LookSAM rapidly lost accuracy as the gradient ascent interval
increased. It achieved higher accuracy than the mini-batch SGD
only when the interval was smaller than or equal to 3in all three
benchmarks.
To further support our proposed method, we present the full
model gradient norm comparison in Figure 4. The proposed method
effectively suppresses the gradient norm even though the model
is perturbed at only a few layers. We omit the ViT fine-tuning
experiment here because the majority of the model parameters are
frozen during the training. One interesting observation is that the
proposed method shows the gradient norm even lower than that of
the generalized SAM that perturbs all the layers. The same results
are observed in all the three benchmarks. This result implies that
the model’s generalization performance can be harmed if the layers
with a minor gradient norm are perturbed. A similar observation
was reported in [22].
Another intriguing observation is that, while the gradient norm
is well suppressed by applying the penalizing method to either the
full model or a few critical layers, the norm becomes higher than
that of the mini-batch SGD after decaying the learning rate. We
find that such a fast drop in the gradient norm is not aligned with
 
1522KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
CIFAR -10 (ResNet -20)Epoch
CIFAR -100 (WRN -28)Epoch
Oxford_Flowers102 (WRN -16)EpochGradient Norm
0 20 40 60 80 1000.60.81.01.21.4
0 50 100 150 2001.02.03.0
020 40 60 80100 120 1400.50.60.70.80.9Layer -wise SAM Generalized SAM Mini-batch SGD LookSAM
Figure 4: The full model gradient norm curves of CIFAR-10 (ResNet20), CIFAR-100 (Wide-ResNet28), and Oxford_Flowers102
(Wide-ResNet16). The norm is noticeably reduced when any SAM method is applied. The proposed layer-wise method effectively
suppresses the gradient norm likely to the full model perturbation method.
CIFAR -10 (ResNet -20)Layer ID
CIFAR -100 (WRN -28)Layer ID
Oxford_Flowers102 (WRN -16)Layer ID# of Perturbations f
020000400006000080000
13579111315171921232527290100002000030000400005000060000
1357911131517192123LookSAM Layer -wise SAM Generalized SAM
0200040006000800010000120001400016000
1 3 5 7 911 13 15 17
Figure 5: The number of perturbations at all individual layers. The generalized SAM perturbs all the layers at every iteration.
LookSAM’s gradient ascent re-calculation interval is 3,2, and 3for the three benchmarks, respectively. Our proposed method
selectively perturbs the 𝑘critical layers only, and the 𝑘is set to 4,4, and 2for the three benchmarks, respectively.
how the generalization performance evolves. Instead, the gradient
norm collected before the learning rate decay effectively represents
the generalization performance. Understanding this phenomenon
will be an interesting future work.
It is worth noting that our method consistently achieves slightly
higher accuracy as compared to the generalized SAM in all the
benchmarks. Similar results have been already reported in [ 8]. This
common tendency implies that perturbing certain parameters may
rather hurt the generalization performance. Our experimental re-
sults show that the layers with a small gradient norm can be con-
sidered as such sensitive layers that may hurt the accuracy when
perturbed. Theoretically explaining this observation is another crit-
ical future work.
4.3 Computational Cost Analysis
Model Perturbation Cost – Let us take a closer look at how the
proposed method reduces the computational cost of the model
perturbation. Figure 5 shows the number of model perturbations at
all individual layers. Note that the proposed method, ‘Layer-wise
SAM’ achieves accuracy similar to or even slightly higher than
that of the ‘SAM’ the full model perturbation method. The ‘Layer-
wise SAM’ has remarkably fewer model perturbations at most ofthe layers compared to the other methods. We also see that the
output-side layers are more likely chosen than the input-side layers
regardless of the dataset. This tendency means that the output-side
layers likely have a larger gradient norm than the input-side layers.
In the backward pass, the errors are back-propagated and then
the gradients can be computed using the obtained errors and the
activations received from the previous layer. Thus, our method
can reduce the error propagation cost together with the gradient
computation cost, and it results in significantly reducing the total
model perturbation cost.
Theoretical Analysis and Comparisons – The computational
cost of the proposed layer-wise perturbation method is as follows.
Bmin(𝑤′)+∑︁
𝑖∈𝑤′|∇𝐿(𝑤𝑖)|+C, (8)
whereB𝑖is the error propagation cost from the output layer to
the layer𝑖. The|·|indicates the number of elements and the C
indicates a constant computational cost that a single backward
pass deterministically has. The left term in (8) indicates the error
backpropagation cost to the most input-side layer among 𝑤′. So,
theBmin(𝑤′)heavily depends on which layers are selected to be
 
1523Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Dataset Model Mini-batch SGD Generalized SAM LookSAM Layer-wise SAM
CIFAR-10 ResNet20 659.2 img/sec 459.5 img/sec 595.2 img/sec 604.2 img/sec
CIFAR-100 Wide-ResNet28 229.1 img/sec 139.5 img/sec 151.0 img/sec 190.7 img/sec
Oxford_Flowers102 Wide-ResNet16 604.2 img/sec 401.9 img/sec 473.6 img/sec 463.4 img/sec
CIFAR-100 (Fine-Tuning) ViT-b16 77.28 img/sec 49.82 img/sec 50.32 img/sec 53.97 img/sec
Table 2: The training throughput on two NVIDIA RTX A6000 GPUs. One process is pinned on a single GPU and they synchronize
the locally calculated gradients using MPI allreduce operations. Since the communication is performed on a single compute
node, the communication time is almost negligible.
perturbed. The right term is the gradient computation cost at all
the𝑘layers in𝑤′.
To the best of our knowledge, [ 20] shows the best computational
efficiency among several SAM variants. It has a hyper-parameter 𝑘
which determines how many times the gradients will be re-used to
perturb the model parameters. The authors suggest 𝑘=5which
generally works well across several benchmarks. Their experimen-
tal results show that the accuracy drops significantly when 𝑘>5
in all the benchmarks. Their computational cost can be analyzed as
follows.
1
𝑘 
B1+∑︁
𝑖∈𝑤|∇𝐿(𝑤𝑖)|!
+C. (9)
TheB1indicates the error propagation cost from the output layer
to the input layer. So, the total perturbation cost is proportionally
reduced as𝑘increases. The costs of these two different approaches,
(8) and (9), cannot be easily compared because the value of (8) is
dependent on the model architecture and which layers are included
in𝑤′. Furthermore, the actual throughput shown in [ 20] is also not
proportional to 𝑘because many constant factors strongly affect the
training time. For example, ViT [ 7] throughput of ImageNet-1K [ 6]
training is improved roughly from 13,000to19,000as the𝑘goes
from 1to5(31.6%improved). Therefore, we focus on the actual
throughput rather than the theoretical computational complexity.
ESAM proposed in [ 8] perturbs all individual parameters inde-
pendently with a probability of 𝛽. However, due to the computa-
tions that have a constant complexity, its performance gain is not
supposed to be similar to the ideal speedup. For example, when
𝛽=0.5, their reported ViT-S/16 (ImageNet) training throughput of
ESAM is 734 images/sec while that of the conventional SAM is 581
images/sec (26 .3%improved). In addition, this approach assumes
that the underlying deep learning framework supports the sparse
gradient computation feature. Thus, we do not directly compare
the performance of our method to ESAM.
Training Throughput – Table 2 shows the training throughput
measured on two A6000 GPUs. The corresponding accuracy is
shown in Table 1. The 𝑘of the proposed method is set to 4,4,2,
and1for the four benchmarks, respectively. The gradient ascent
interval of LookSAM is set to 3,4,3, and 2for the same four bench-
marks, respectively. Overall, our method achieves similar accuracy
to the full model perturbation method while significantly improv-
ing the training throughput. LookSAM achieves a slightly higher
throughput of Oxford_Flowers102 but its accuracy is much lower
than the proposed method as shown in Table 1. For the other three
benchmarks, our method achieves the best throughput together
with the highest accuracy.Interestingly, the proposed method shows a quite different per-
formance improvement depending on the model architecture. The
throughput drop for for ResNet20, WRN28, and WRN16, are 8.5%,
16.8%, and 25.4%, respectively. The networks have their special
architectures and the training datasets also have different patterns.
If a certain combination of dataset and model causes a large gradi-
ent norm at the input-side layers, it will likely have an expensive
model perturbation cost due to the long backward passes. On the
other hand, if the combination yields a large gradient norm at the
output-side layers, the backward pass for the layer-wise model per-
turbation becomes short having a cheaper computational cost. In
our experiments, all the benchmarks showed a different distribution
of gradient norms across the layers, and it resulted in having such
a different model perturbation cost.
4.4 Ablation Study
Loss Landscape – Figure 6 shows the layer-wise loss landscape
when the model is trained using the proposed layer-wise gradient
norm penalizing method. The training loss values were collected
from the same grid points as Figure 2. The plots show that the
proposed method leads the model, especially the layers that fell
into a sharp valley when using mini-batch SGD, to a flat region
in the parameter space. This result is also well aligned with what
we observed in Figure 5 such that the corresponding output-side
layers are mostly chosen to be perturbed and their loss landscapes
become much flatter as compared to Figure 2.
Layer Selection Criterion – To verify the efficacy of the proposed
adaptive layer selection method, we compare the performance of
three different layer-wise perturbation approaches: 1) perturb the
layers with a high gradient norm, 2) perturb the layers with a low
gradient norm, and 3) random selection. Table 3 shows the accuracy
comparisons. Note that None is the mini-batch SGD. The Random
can be considered to be a coarse-grained version of the randomized
parameter-wise perturbation in [ 8]. The𝑘is set to the smallest
value that gives a similar accuracy to the conventional SAM.
In all three benchmarks, the Low-Norm achieves significantly
lower accuracy than the other perturbation criteria. This result
empirically proves that putting more weight on the layers with a
high gradient norm is a valid approach to improve the generaliza-
tion performance. We also see that Random shows a non-negligible
accuracy improvement as compared to the mini-batch SGD. By ran-
domly choosing the layers to perturb, some critical layers could be
included by chance, and it results in improving the generalization
performance. Since the High-Norm consistently achieves higher
accuracy than Random in all the benchmarks, we can conclude that
 
1524KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
Layer 1 Layer 3 Layer 6
Layer 9 Layer 12 Layer 15
Figure 6: The loss landscape of Wide-ResNet16 trained on Oxford_Flowers102. The model was trained using the proposed
layer-wise gradient norm penalizing method. The landscapes are much flatter as compared to Figure 2.
Dataset 𝑘Perturbation Criterion Validation Acc. (%)
CIFAR-10 4None 91.63±0.3%
High-Norm 92.81±0.3%
Low-Norm 91.40±0.5%
Random 92.46±0.2%
CIFAR-100 4None 79.41±0.4%
High-Norm 81.25±0.4%
Low-Norm 80.01±0.9%
Random 80.67±0.5%
Oxford_Flowers102 2None 69.82±0.4%
High-Norm 75.36±0.6%
Low-Norm 71.76±1.1%
Random 72.45±0.9%
Table 3: The validation accuracy comparisons among a) None,
b)High-Norm, c) Low-Norm, and d) Random layer-wise per-
turbation methods. The best accuracy is achieved when the
layers with the highest gradient norm are perturbed in all
three benchmarks.
the proposed method effectively finds the best trade-off between
the model perturbation cost and the generalization performance.
Impact of the Number of Layers to Perturb, 𝑘– We adjust the
𝑘setting and analyze its impact on the generalization performance.
As the gradient norm penalizing method is applied to more layers,
the norm is expected to be suppressed more strongly. Table 4 shows
the accuracy comparisons. Interestingly, the accuracy becomes sim-
ilar to the conventional SAM when 𝑘is even set to a very small
value such as 1 or 2. Once it achieves the best accuracy at a certain
𝑘, the accuracy does not further go up as 𝑘increases. However,
the throughput monotonically decreases as 𝑘increases. Therefore,
users should find the value of 𝑘that gives the best accuracy at the
minimal model perturbation cost In practice, we suggest starting
the tuning from 𝑘=2and then conducting a simple grid search to
find a better setting.Dataset 𝑘 Validation Acc. (%) Throughput (img/sec)
CIFAR-101 92.51±0.5% 628.5
2 92.57±0.4% 617.0
4 92.81±0.3% 604.2
8 92.77±0.5% 564.5
all 92.64±0.4% 459.5
CIFAR-1001 80.12±0.3% 229.1
2 80.81±0.7% 194.6
4 81.25±0.4% 190.7
8 81.93±0.5% 160.0
all 80.83±0.5% 139.5
Oxford_Flowers1021 74.93±0.5% 482.6
2 75.56±0.6% 463.4
4 76.31±1.1% 442.9
8 76.70±0.9% 427.5
all 74.77±0.5% 401.9
Table 4: The validation accuracy and throughput with dif-
ferent𝑘settings. In all the benchmarks, the best accuracy is
achieved with a certain 𝑘and then it does not further go up
as𝑘increases.
5 Conclusion
In this paper, we discussed how to tackle the expensive model per-
turbation cost issue in SAM. By selectively perturbing the critical
layers only, our method suppresses the gradient norm of the whole
model while effectively improving the models’ generalization per-
formance. This method is not dependent on model architecture,
data characteristics, or optimization algorithms. Thus, it can be
employed by real-world deep learning applications without hav-
ing any conflicts to their existing features. Our empirical study
also provides intriguing insights into how each layer contributes
to the whole network’s generalization performance. We believe
that our study will encourage many Deep Learning applications
to take advantage of the SAM to enjoy improved generalization
performance.
 
1525Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Acknowledgments
This work was partly supported by Institute of Information &
communications Technology Planning & Evaluation (IITP) grant
funded by the Korea government(MSIT) (No.RS-2022-00155915,
Artificial Intelligence Convergence Innovation Human Resources
Development (Inha University)). This research was supported by
the MSIT(Ministry of Science, ICT), Korea, under the National Pro-
gram for Excellence in SW), supervised by the IITP(Institute of
Information & communications Technology Planing & Evaluation)
in 2024 (2022-0-01127). This work was supported by the National
Research Foundation of Korea(NRF) grant funded by the Korea gov-
ernment(MSIT) (No. RS-2023-00279003). This work was supported
by INHA UNIVERSITY Research Grant.
References
[1]Maksym Andriushchenko and Nicolas Flammarion. 2022. Towards understanding
sharpness-aware minimization. In International Conference on Machine Learning.
PMLR, 639–668.
[2]Carlo Baldassi, Fabrizio Pittorino, and Riccardo Zecchina. 2020. Shaping the
learning landscape in neural networks around wide flat minima. Proceedings of
the National Academy of Sciences 117, 1 (2020), 161–170.
[3]Peter L Bartlett, Philip M Long, and Olivier Bousquet. 2023. The dynamics of
sharpness-aware minimization: Bouncing across ravines and drifting towards
wide minima. Journal of Machine Learning Research 24, 316 (2023), 1–36.
[4]Debora Caldarola, Barbara Caputo, and Marco Ciccone. 2022. Improving general-
ization in federated learning by seeking flat minima. In European Conference on
Computer Vision. Springer, 654–672.
[5]Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Bal-
dassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
2019. Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statisti-
cal Mechanics: Theory and Experiment 2019, 12 (2019), 124018.
[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. In 2009 IEEE conference on computer
vision and pattern recognition. Ieee, 248–255.
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[8]Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick
Siow Mong Goh, and Vincent YF Tan. 2021. Efficient sharpness-aware minimiza-
tion for improved training of neural networks. arXiv preprint arXiv:2110.03141
(2021).
[9]Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2020.
Sharpness-aware minimization for efficiently improving generalization. arXiv
preprint arXiv:2010.01412 (2020).
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[11] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. Neural computation
9, 1 (1997), 1–42.
[12] Weisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. 2023. An Adaptive
Policy to Employ Sharpness-Aware Minimization. arXiv preprint arXiv:2304.14647
(2023).
[13] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyan-
skiy, and Ping Tak Peter Tang. 2016. On large-batch training for deep learning:
Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836 (2016).
[14] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[15] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. 2021. Asam:
Adaptive sharpness-aware minimization for scale-invariant learning of deep
neural networks. In International Conference on Machine Learning. PMLR, 5905–
5914.
[16] Sunwoo Lee, Chaoyang He, and Salman Avestimehr. 2023. Achieving small-batch
accuracy with large-batch scalability via Hessian-aware learning rate adjustment.
Neural Networks 158 (2023), 1–14.
[17] Sunwoo Lee, Anit Kumar Sahu, Chaoyang He, and Salman Avestimehr. 2023.
Partial model averaging in federated learning: Performance guarantees and
benefits. Neurocomputing 556 (2023), 126647.
[18] Sunwoo Lee, Tuo Zhang, and A Salman Avestimehr. 2023. Layer-wise adaptive
model aggregation for scalable federated learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 8491–8499.[19] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018.
Visualizing the loss landscape of neural nets. Advances in neural information
processing systems 31 (2018).
[20] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. 2022. To-
wards efficient and scalable sharpness-aware minimization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12360–12370.
[21] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022. Layer-wised model
aggregation for personalized federated learning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 10092–10101.
[22] Peng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and
Dacheng Tao. 2022. Make sharpness-aware minimization stronger: A sparsified
perturbation approach. Advances in Neural Information Processing Systems 35
(2022), 30950–30962.
[23] M-E. Nilsback and A. Zisserman. 2008. Automated Flower Classification over a
Large Number of Classes. In Proceedings of the Indian Conference on Computer
Vision, Graphics and Image Processing.
[24] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and
Mario Boley. 2021. Relative flatness and generalization. Advances in neural
information processing systems 34 (2021), 18420–18432.
[25] Zhe Qu, Xingyu Li, Rui Duan, Yao Liu, Bo Tang, and Zhuo Lu. 2022. Generalized
federated learning via sharpness aware minimization. In International Conference
on Machine Learning. PMLR, 18250–18280.
[26] Hannes Schulz and Sven Behnke. 2012. Deep learning: Layer-wise learning of
feature hierarchies. KI-Künstliche Intelligenz 26 (2012), 357–363.
[27] Hao Sun, Li Shen, Qihuang Zhong, Liang Ding, Shixiang Chen, Jingwei Sun, Jing
Li, Guangzhong Sun, and Dacheng Tao. 2023. Adasam: Boosting sharpness-aware
minimization with adaptive learning rate and momentum for training deep neural
networks. arXiv preprint arXiv:2303.00565 (2023).
[28] Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Chunpeng Wu, Yiran Chen, and
Hai Li. 2018. Smoothout: Smoothing out sharp minima to improve generalization
in deep learning. arXiv preprint arXiv:1805.07898 (2018).
[29] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. 2020. Pyhes-
sian: Neural networks through the lens of the hessian. In 2020 IEEE international
conference on big data (Big data). IEEE, 581–590.
[30] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. 2018.
Imagenet training in minutes. In Proceedings of the 47th international conference
on parallel processing. 1–10.
[31] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. arXiv
preprint arXiv:1605.07146 (2016).
[32] Xingxuan Zhang, Renzhe Xu, Han Yu, Hao Zou, and Peng Cui. 2023. Gradient
norm aware minimization seeks first-order flatness and improves generaliza-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 20247–20257.
[33] Yang Zhao, Hao Zhang, and Xiuyuan Hu. 2022. Penalizing gradient norm for
efficiently improving generalization in deep learning. In International Conference
on Machine Learning. PMLR, 26982–26992.
[34] Yaowei Zheng, Richong Zhang, and Yongyi Mao. 2021. Regularizing neural
networks via adversarial model perturbation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 8156–8165.
[35] Yixuan Zhou, Yi Qu, Xing Xu, and Hengtao Shen. 2023. ImbSAM: A Closer Look at
Sharpness-Aware Minimization in Class-Imbalanced Recognition. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 11345–11355.
[36] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha
Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. 2022. Surrogate gap
minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065
(2022).
 
1526KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
A Appendix
A.1 Proof of Theorem
We first present a couple of useful lemmas here. Note that our analysis borrows the proof structure used in [1].
Lemma A.1. Given a𝛽-smooth loss function 𝐿(𝑥), we have the following bound for any 𝑥∈R𝑑.
⟨∇𝐿(𝑢)−∇𝐿(𝑣),𝑢−𝑣⟩≥−𝛽∥𝑢−𝑣∥2.
Proof. Starting from the smoothness assumption,
∥∇𝐿(𝑢)−∇𝐿(𝑣)∥≤𝛽∥𝑢−𝑣∥for all𝑢and𝑣∈R𝑑
By multiplying∥𝑣−𝑢∥on the both side, we get
∥∇𝐿(𝑢)−∇𝐿(𝑣)∥∥𝑣−𝑢∥≤𝛽∥𝑢−𝑣∥∥𝑣−𝑢∥
∥∇𝐿(𝑢)−∇𝐿(𝑣)∥∥𝑣−𝑢∥≤𝛽∥𝑢−𝑣∥2
⟨∇𝐿(𝑢)−∇𝐿(𝑣),𝑣−𝑢⟩≤𝛽∥𝑢−𝑣∥2, (10)
⟨∇𝐿(𝑢)−∇𝐿(𝑣),𝑢−𝑣⟩≥−𝛽∥𝑢−𝑣∥2.
where (10) is based on Cauchy-Schwarz inequality. □
Here, we additionally define the magnitude of the gradients of 𝑤′as follows.
∥∇𝐿(𝑤′)∥2=∑︁
𝑖∈𝑤′∥∇𝐿(𝑤𝑖)∥2=𝜅∥∇𝐿(𝑤)∥2,0≤𝜅≤1
The𝜅is defined based on the model architecture and which layers are selected to be perturbed. That is, as 𝑘increases,𝜅will also increase
according to the number of parameters at the chosen layers. Without loss in generality, we define 𝜅as a ratio of the number of perturbed
parameters to that of the total parameters. In this way, our analysis can cover any possible model architecture and the input data.
Lemma A.2. Given a𝛽-smooth loss function 𝐿(𝑥), we have the following bound for any 𝑟>0and𝑥∈R𝑑.
⟨∇𝐿(𝑤+𝑟∇𝐿(𝑤′)),∇𝐿(𝑤)⟩≥( 1−𝑟𝛽𝜅)∥∇𝐿(𝑤)∥2
Proof.
⟨∇𝐿(𝑤+𝑟∇𝐿(𝑤′)),∇𝐿(𝑤)⟩=⟨∇𝐿(𝑤+𝑟∇𝐿(𝑤′))−∇𝐿(𝑤),∇𝐿(𝑤)⟩+∥∇𝐿(𝑤)∥2
=1
𝑟⟨∇𝐿(𝑤+𝑟∇𝐿(𝑤′))−∇𝐿(𝑤),𝑟∇𝐿(𝑤)⟩+∥∇𝐿(𝑤)∥2
≥−𝛽
𝑟∥𝑟∇𝐿(𝑤′)∥2+∥∇𝐿(𝑤)∥2(11)
≥−𝑟𝛽∥∇𝐿(𝑤′)∥2+∥∇𝐿(𝑤)∥2
=−𝑟𝛽𝜅∥∇𝐿(𝑤)∥2+∥∇𝐿(𝑤)∥2(12)
≥(1−𝑟𝛽𝜅)∥∇𝐿(𝑤)∥2,
where (11) is based on Lemma A.1. The (12) holds by the definition of 𝜅. □
Lemma A.3. We consider the classical SAM which uses the same mini-batch when calculating the gradient ascent and the gradient descent.
Then, given a 𝛽-smooth loss function 𝐿(𝑥), we have the following bound for any 𝑟>0, any 0≤𝜅≤1, and𝑥∈R𝑑.
E
⟨∇𝐿𝑡+1(𝑤+𝑟∇𝐿𝑡+1(𝑤′)),∇𝐿(𝑤)⟩
≥1
2−𝑟𝛽𝜅
∥∇𝐿(𝑤)∥2−𝛽2𝑟2𝜎2
2𝑏
Proof. We first define the layer-wise gradient ascent step ˜𝑤=𝑤+𝑟∇𝐿(𝑤′), where∇𝐿(𝑤′)indicates the global gradients at a subset of
network layers 𝐻.
E
⟨∇𝐿𝑡+1(𝑤+𝑟∇𝐿𝑡+1(𝑤′)),∇𝐿(𝑤)⟩
=E
⟨∇𝐿(𝑤+𝑟∇𝐿𝑡+1(𝑤′)),∇𝐿(𝑤)⟩
=E
⟨∇𝐿(𝑤+𝑟∇𝐿𝑡+1(𝑤′))−∇𝐿(˜𝑤)+∇𝐿(˜𝑤),∇𝐿(𝑤)⟩
=E
⟨∇𝐿(𝑤+𝑟∇𝐿𝑡+1(𝑤′))−∇𝐿(˜𝑤),∇𝐿(𝑤)⟩
|                                                      {z                                                      }
𝐸1+E[⟨∇𝐿(˜𝑤),∇𝐿(𝑤)⟩]|                    {z                    }
𝐸2.
 
1527Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Then, we will bound 𝐸1and𝐸2separately. First, 𝐸1is lower-bounded as follows.
𝐸1=E
⟨∇𝐿(𝑤+𝑟∇𝐿𝑡+1(𝑤′))−∇𝐿(˜𝑤),∇𝐿(𝑤)⟩
≥−1
2E
∥∇𝐿(𝑤+𝑟∇𝐿𝑡+1(𝑤′))−∇𝐿(˜𝑤)∥2
−1
2E
∥∇𝐿(𝑤)∥2
≥−𝛽2
2E
∥𝑤+𝑟∇𝐿𝑡+1(𝑤′)−˜𝑤∥2
−1
2E
∥∇𝐿(𝑤)∥2
(13)
=−𝛽2
2E
∥𝑟∇𝐿𝑡+1(𝑤′)−𝑟∇𝐿(𝑤′)∥2
−1
2E
∥∇𝐿(𝑤)∥2
≥−𝛽2𝑟2𝜎2
2𝑏−1
2E
∥∇𝐿(𝑤)∥2
, (14)
where (13) is based on the smoothness assumption. The final equality, (14), is based on the bounded variance assumption. Then, 𝐸2is
lower-bounded directly based on Lemma A.2 as follows.
𝐸2=E[⟨∇𝐿(˜𝑤),∇𝐿(𝑤)⟩]≥(1−𝑟𝛽𝜅)∥∇𝐿(𝑤)∥2.
Summing up 𝐸1and𝐸2bounds, we have
E
⟨∇𝐿𝑡+1(𝑤+𝑟∇𝐿𝑡+1(𝑤′)),∇𝐿(𝑤)⟩
≥−𝛽2𝑟2𝜎2
2𝑏−1
2E
∥∇𝐿(𝑤)∥2
+(1−𝑟𝛽𝜅)∥∇𝐿(𝑤)∥2
=1
2−𝑟𝛽𝜅
∥∇𝐿(𝑤)∥2−𝛽2𝑟2𝜎2
2𝑏
□
Lemma A.4. Under the assumption of 𝛽smoothness and the bounded variance, the SAM guarantees the following if 𝜂≤1
2𝛽and𝑟≤1
2𝛽.
E[𝐿(𝑤𝑡+1)]≤E[𝐿(𝑤𝑡)]−𝜂
4E
∥∇𝐿(𝑤𝑡)∥2
+𝜂𝛽(𝜂+𝛽𝑟2)𝜎2
𝑏. (15)
Proof. Let us first define the model updated with the gradient ascent as 𝑤𝑡+1/2=𝑤𝑡+𝑟∇𝐿𝑡+1(𝑤𝑡). From the smoothness assumption,
we begin with the following condition.
𝐿(𝑤𝑡+1)≤𝐿(𝑤𝑡)−𝜂⟨∇𝐿𝑡+1(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩+𝜂2𝛽
2∥∇𝐿𝑡+1(𝑤𝑡+1/2)∥2.
Taking the expectation on both sides and based on the bounded variance assumption,
E[𝐿(𝑤𝑡+1)]≤E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽
2E
∥∇𝐿𝑡+1(𝑤𝑡+1/2)∥2
≤E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽E
∥∇𝐿𝑡+1(𝑤𝑡+1/2)−∇𝐿(𝑤𝑡+1/2)∥2
+𝜂2𝛽E
∥∇𝐿(𝑤𝑡+1/2)∥2
(16)
≤E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽𝜎2
𝑏+𝜂2𝛽E
∥∇𝐿(𝑤𝑡+1/2)∥2
=E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽𝜎2
𝑏
−𝜂2𝛽E
∥∇𝐿(𝑤𝑡)∥2
+𝜂2𝛽E
∥∇𝐿(𝑤𝑡+1/2)−∇𝐿(𝑤𝑡)∥2
+2𝜂2𝛽⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
≤E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽𝜎2
𝑏
−𝜂2𝛽E
∥∇𝐿(𝑤𝑡)∥2
+𝜂2𝛽3E
∥𝑤𝑡+1/2−𝑤𝑡∥2
+2𝜂2𝛽⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩ (17)
=E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽𝜎2
𝑏
−𝜂2𝛽E
∥∇𝐿(𝑤𝑡)∥2
+𝜂2𝛽3𝑟2E
∥∇𝑡+1𝐿(𝑤𝑡)∥2
+2𝜂2𝛽⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
=E[𝐿(𝑤𝑡)]−𝜂E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
+𝜂2𝛽𝜎2
𝑏
−𝜂2𝛽E
∥∇𝐿(𝑤𝑡)∥2
+2𝜂2𝛽3𝑟2E
∥∇𝐿(𝑤𝑡)∥2
+2𝜂2𝛽3𝑟2𝜎2
𝑏+2𝜂2𝛽⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
 
1528KDD ’24, August 25–29, 2024, Barcelona, Spain Sunwoo Lee
where (16) is based on Jensen’s inequality and (17) is based on Assumption 2. Then, by rearranging the terms, we have
E[𝐿(𝑤𝑡+1)]≤E[𝐿(𝑤𝑡)]−𝜂2𝛽(1−2𝛽2𝑟2)E
∥∇𝐿(𝑤𝑡)∥2
+𝜂2𝛽(1+2𝛽2𝑟2)𝜎2
𝑏−𝜂(1−2𝜂𝛽)E
⟨∇𝐿(𝑤𝑡+1/2),∇𝐿(𝑤𝑡)⟩
≤E[𝐿(𝑤𝑡)]−𝜂2𝛽(1−2𝛽2𝑟2)E
∥∇𝐿(𝑤𝑡)∥2
+𝜂2𝛽(1+2𝛽2𝑟2)𝜎2
𝑏−𝜂(1−2𝜂𝛽)E1
2−𝑟𝛽𝜅
∥∇𝐿(𝑤𝑡)∥2−𝛽2𝑟2𝜎2
2𝑏
(18)
=E[𝐿(𝑤𝑡)]+
−𝜂
2+𝜂𝛽𝑟𝜅−2𝜂2𝛽2𝑟𝜅+2𝜂2𝛽3𝑟2
E
∥∇𝐿(𝑤𝑡)∥2
+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
=E[𝐿(𝑤𝑡)]−𝜂
2(1−2𝛽𝑟(𝜅−2𝜂𝛽(𝜅−𝛽𝑟)))E
∥∇𝐿(𝑤𝑡)∥2
+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
≤E[𝐿(𝑤𝑡)]−𝜂
4E
∥∇𝐿(𝑤𝑡)∥2
+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
where (18) is based on Lemma A.3. The final inequality holds if 𝜂≤1
2𝛽and𝑟≤1
2𝛽regardless of the value of 𝜅. □
Theorem A.5. Assume the𝛽-smooth loss function and the bounded gradient variance. Then, if 𝜂≤1
2𝛽and𝑟≤1
2𝛽, mini-batch SGD satisfies:
1
𝑇𝑇−1∑︁
𝑡=0E
∥∇𝐿(𝑤𝑡)∥2
≤4
𝑇𝜂(𝐿(𝑤0)−E[𝐿(𝑤𝑇)])+4
𝜂𝛽+𝛽2𝑟2𝜎2
𝑏. (19)
Proof. Based on Lemma A.4, by averaging (15) across 𝑇iterates, we have
1
𝑇𝑇−1∑︁
𝑡=0E[𝐿(𝑤𝑡+1)]≤1
𝑇𝑇−1∑︁
𝑡=0
E[𝐿(𝑤𝑡)]−𝜂
4E
∥∇𝐿(𝑤𝑡)∥2
+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
Then, we can have a telescoping sum by rearranging the terms as follows.
𝜂
4𝑇𝑇−1∑︁
𝑡=0E
∥∇𝐿(𝑤𝑡)∥2
≤1
𝑇𝑇−1∑︁
𝑡=0(E[𝐿(𝑤𝑡)]−E[𝐿(𝑤𝑡+1)])+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
=1
𝑇(𝐿(𝑤0)−E[𝐿(𝑤𝑇)])+
𝜂2𝛽+𝜂𝛽2𝑟2𝜎2
𝑏
Finally, by dividing both sides by𝜂
4, we have
1
𝑇𝑇−1∑︁
𝑡=0E
∥∇𝐿(𝑤𝑡)∥2
≤4
𝑇𝜂(𝐿(𝑤0)−E[𝐿(𝑤𝑇)])+4
𝜂𝛽+𝛽2𝑟2𝜎2
𝑏. (20)
□
A.2 Layer-Wise Loss Landscape Visulization
We employed the visualization algorithm proposed in [19]. To obtain plots shown in Figure 2, we conducted the following steps.
(1) Create one random vector that has the same size as the target layer.
(2) Create another vector that is orthogonal to the first vector.
(3) Divide them by their norms to make them have a norm of 1.
(4) Multiply 𝛼to the first vector and 𝛽to the second vector and add them to the target layer parameters.
(5) Collect the training loss using the perturbed model.
(6) Repeat steps 4 and 5 using 𝛼∈{−20,···,19}and𝛽∈{−20,···,19}.
These steps provide us with the approximated 3-D loss landscape figures shown in Figure 2. We used the same 1024 training images to
calculate the loss value at all 1,600grid points. The z-axis range is fixed from 0 to 4.5 for all six layers. Because the model is perturbed only
at a single target layer, the landscape figures provide insights into how all individual layers affect the model’s generalization performance.,
 
1529