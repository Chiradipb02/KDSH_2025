Breaking Barriers: A Hands-On Tutorial on AI-Enabled
Accessibility to Social Media Content
Julio Villena
Reddit, Inc.
Madrid, Spain
julio.villena@reddit.comRosa Català
Reddit, Inc.
San Francisco, California, USA
rosa.catala@reddit.comJanine García
Reddit, Inc.
Madrid, Spain
janine.garcia@reddit.com
Concepción Polo
Reddit, Inc.
Madrid, Spain
concepcion.polo@reddit.comYessika Labrador
Reddit, Inc.
San Francisco, California, USA
yessika.labrador@reddit.comFrancisco Del Valle
Reddit, Inc.
Madrid, Spain
francisco.delvalle@reddit.com
Bhargav Ayyagari
Reddit, Inc.
Toronto, Canada
bhargav.ayyagari@reddit.com
ABSTRACT
Reddit’s mission is to bring community, belonging, and empow-
erment to everyone in the world. This hands-on tutorial explores
the immense potential of Artificial Intelligence (AI) to improve
accessibility to social media content for individuals with different
disabilities, including hearing, visual, and cognitive impairments.
We will design and implement a variety of AI-based approaches
based on multimodal open-source Large Language Models (LLMs)
to bridge the gap between research and real-world applications.
CCS CONCEPTS
•Human-centered computing →Accessibility technologies;
•Computing methodologies →Natural language processing;
Scene understanding; Video summarization; Speech recogni-
tion.
KEYWORDS
AI, LLM, accessibility, inclusive content, multimodal content, social
media, Natural Language Processing, speech to text
ACM Reference Format:
Julio Villena, Rosa Català, Janine García, Concepción Polo, Yessika Labrador,
Francisco Del Valle, and Bhargav Ayyagari. 2024. Breaking Barriers: A
Hands-On Tutorial on AI-Enabled Accessibility to Social Media Content. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 2 pages. https://doi.org/10.1145/3637528.3671446
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36714461 INTRODUCTION
Reddit and other social media platforms have become ubiquitous,
providing opportunities for individuals to connect, share, and access
information. However, for individuals with disabilities, accessing
and interacting with social media content can present significant
challenges. By making content accessible and inclusive for all, we
strive to create a space where everyone feels welcome, valued, and
represented. We see accessibility [ 9] as a fundamental aspect of
inclusivity, so we prioritize providing content and features that are
easy for all users to navigate, understand, and enjoy. By removing
barriers and ensuring accessibility, we want to empower everyone
to fully participate in our community, share their perspectives, and
connect with others who share their interests and passions.
AI offers promising solutions to enhance accessibility and inclu-
sivity, especially with the emergence of multimodal LLMs. These
have witnessed remarkable advancements, empowering them with
the ability to analyze and understand all media formats, including
text, images, audio, and video.
This hands-on tutorial explores the immense potential of AI
to improve accessibility to social media content for individuals
with different disabilities, including hearing, visual, and cognitive
impairments. We will design and implement a variety of AI-based
approaches based on multimodal open-source LLMs to bridge the
gap between research and real-world use cases:
•Providing alternative text descriptions (captions) for images,
making them accessible to users with visual impairments.
•Generating transcripts and summaries of audio and video
content, enabling hearing-impaired users to access the infor-
mation without relying on others for assistance.
•Fixing accessibility issues in social media posts, generating
adapted versions and/or summaries for long and complex
texts, making it easier for users with cognitive disabilities to
understand and engage with social media content.
We will analyze and highlight the strengths and limitations of
these techniques and discuss the challenges and opportunities for
further application to other use cases.
6426
KDD ’24, August 25–29, 2024, Barcelona, Spain Julio Villena et al.
Despite their importance and due to time constraints, this tutorial
excludes text-to-speech conversion (for helping visually impaired
people), and content translation (for overcoming language barri-
ers), as these are well-established techniques with ample resources
available elsewhere.
The target audience are researchers or practitioners interested
about AI-enabled accessibility for social media content, regardless
of whether they work in the industry or not. Participants should
have a basic understanding of AI, Natural Language Processing
(NLP), and LLMs.
2 TUTORIAL OUTLINE
A dataset featuring diverse multimedia posts with curated images
and videos will be created to support the tutorial. This dataset
will showcase real-world scenarios, allowing participants to gain
a practical understanding of how the presented approaches can
effectively address the different challenges.
The 3-hour tutorial will be organized in the following sections.
2.1 Introduction
In the introduction (15 min), we will talk about the importance of
accessibility in digital content [ 5,7] and provide an overview of the
challenges faced by users with disabilities when accessing Reddit
content. Then, accessibility guidelines and best practices to ensure
inclusivity [ 2] will be presented. Finally, we will discuss about the
potential of AI to enhance accessibility.
2.2 Use Case 1. Image Short Captions
The first use case (45 min) will focus on deploying and prompting
different multimodal LLMs (such as ClipCap [ 8], LLaVA [ 6], CogA-
gent [ 3], Qwen-VL [ 1]) to generate short, descriptive captions for
images [ 4]. The challenges and limitations of using LLMs for image
captioning will be discussed.
2.3 Use Case 2. Audio Clip Transcripts
In the second use case (30 min), we will use the Whisper [ 10] open
source speech-to-text models to transcribe audio clips to text and
produce closed captions. Then we will explore transcript translation
and handling multiple speakers (speaker diarization).
2.4 Use Case 3. Video Descriptions
The third use case (30 min) will guide participants through the
steps of designing and implementing a pipeline to generate video
descriptions, combining keyframe extraction, image captioning,
audio transcript and summarization using LLMs. We will explore
the challenges and advantages for different types of video content.
2.5 Use Case 4. Complex Post Summarization
All previous models will be combined in the fourth use case (30 min)
to demonstrate how to use AI to summarize lengthy posts for users
with cognitive impairments. We will compare and contrast different
summarization techniques and discuss the ethical considerations
of using AI for summarization.2.6 Discussion
Finally, we will discuss (30 min) the challenges and best practices
for deploying accessible content solutions, and guide participants in
developing a plan for implementing accessibility initiatives. Gender
and other biases will be analyzed. Last, the key takeaways and
benefits will be summarized and participants will be encouraged to
continue exploring in enhancing social media accessibility.
3 TUTORS AND PRESENTERS
All authors are members of the multidisciplinary ML Understanding
team at Reddit, based in the United States, Spain, and Canada.
Julio Villena. Expert in AI, Data Science, and NLP. Long career
in industry, as a technology leader and co-founder of several star-
tups, designing and delivering cutting-edge solutions and driving
the innovation activities. Principal Engineer at Reddit, Julio has a
leadership role in ML Understanding team, dedicated to uncovering
insights from the platform’s massive unstructured data. He has had
a significant participation in large international research projects,
resulting in over 100 publications. He also has held a longstanding
position as a lecturer at university, demonstrating a passion for
both teaching and research.
Rosa Català. Leader in the realm of AI, with transformative
contributions at Reddit and other prominent companies like Ap-
ple, and Climate Corp, a Bayer Company. Her work consistently
pushes the boundaries of technology in fields such as Generative
AI, NLP, Computer Vision, and Recommendation Systems. Senior
Director at Reddit, Rosa leads high-performing teams bridging the
gap between research and product development in the consumer
space, and also teams building and maintaining robust AI/ML in-
frastructure/platforms to serve millions of users with cost-effective
solutions. Rosa holds a PhD and several master’s degrees, with
publications in the field of economic and quantitative modeling.
Other collaborators are Janine García, Concepción Polo, and
Yessika Labrador (Staff Engineers), Francisco del Valle (Senior
Engineer), and Bhargav Ayyagari (Engineering Manager).
REFERENCES
[1]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Jun-
yang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-
Language Model for Understanding, Localization, Text Reading, and Beyond.
arXiv:2308.12966 [cs.CV]
[2]World Wide Web Consortium. 2024. Web Content Accessibility Guidelines (WCAG).
World Wide Web Consortium. Retrieved June 7, 2024 from https://www.w3.org/
WAI/standards-guidelines/wcag/
[3]Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji,
Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming
Ding, and Jie Tang. 2023. CogAgent: A Visual Language Model for GUI Agents.
arXiv:2312.08914 [cs.CV]
[4]Huggingface. 2024. Image Captioning. Huggingface. Retrieved June 7, 2024 from
https://huggingface.co/docs/transformers/main/en/tasks/image_captioning
[5]IBM. 2024. Accessibility. IBM. Retrieved June 7, 2024 from https://www.ibm.
com/able/
[6]Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines
with Visual Instruction Tuning. arXiv:2310.03744 [cs.CV]
[7]Microsoft. 2024. Innovation and AI for Accessibility. Microsoft. Retrieved June 7,
2024 from https://www.microsoft.com/en-us/accessibility/innovation
[8]Ron Mokady, Amir Hertz, and Amit H. Bermano. 2021. ClipCap: CLIP Prefix for
Image Captioning. arXiv:2111.09734 [cs.CV]
[9]The Accessibility Project. 2024. The a11y Project. The Accessibility Project.
Retrieved June 7, 2024 from https://www.a11yproject.com/
[10] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey,
and Ilya Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak
Supervision. arXiv:2212.04356 [eess.AS]
6427