Efficient Exploration of the Rashomon Set of Rule-Set Models
Martino Ciaperoni‚Ä†
martino.ciaperoni@aalto.fi
Aalto University
Espoo, FinlandHan Xiao‚Ä†
xiaohan2012@gmail.com
The Upright Project
Helsinki, FinlandAristides Gionis
argioni@kth.se
KTH Royal Institute of Technology
Stockholm, Sweden
ABSTRACT
Today, as increasingly complex predictive models are developed,
simple rule sets remain a crucial tool to obtain interpretable pre-
dictions and drive high-stakes decision making. However, a single
rule set provides a partial representation of a learning task. An
emerging paradigm in interpretable machine learning aims at ex-
ploring the Rashomon set of all models exhibiting near-optimal
performance. Existing work on Rashomon-set exploration focuses
on exhaustive search of the Rashomon set for particular classes of
models, which can be a computationally challenging task. On the
other hand, exhaustive enumeration leads to redundancy that often
is not necessary, and a representative sample or an estimate of the
size of the Rashomon set is sufficient for many applications. In this
work, we propose, for the first time, efficient methods to explore
the Rashomon set of rule-set models with or without exhaustive
search. Extensive experiments demonstrate the effectiveness of the
proposed methods in a variety of scenarios.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíSupervised learning; Rule
learning.
KEYWORDS
Interpretable machine learning, Rashomon set, Rule-based classifi-
cation, Scalable algorithms
ACM Reference Format:
Martino Ciaperoni‚Ä†, Han Xiao‚Ä†, and Aristides Gionis. 2024. Efficient Ex-
ploration of the Rashomon Set of Rule-Set Models. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671818
1 INTRODUCTION
Following the impressive results achieved by modern machine-
learning methods, automated decision making is used in consequen-
tial domains, such as health care, education, and criminal justice.
However, many state-of-the-art models are opaque, and as such,
they are difficult to interpret, understand, and trust. In other cases,
they may hide harmful biases [ 34]. Thus, the research community
‚Ä†Both authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671818
0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18
SP [race]0.00.10.20.30.40.50.60.7SP [gender]Rule sets in a Rashomon set for the COMPAS dataset
0.560.570.580.590.600.610.620.630.64
AccuracyA
CC: 0.65, SP[race]: 0.19
IfPrior-Crimes>3‚à®Age=18-25
ùë¶=1
Elseùë¶=0A
CC: 0.62, SP[race]: 0.09
IfPrior-Crimes>3‚à®
(Gender=Male‚àßPrior-Crimes=1-3)
ùë¶=1
Elseùë¶=0
Figure 1: A Rashomon set of rule sets in the Compas dataset.
For each rule set, we show accuracy (colour) and statistical
parity (SP) [ 9] on race (ùë•-axis) and gender ( ùë¶-axis). Two ex-
ample rule sets with similar accuracy, but highly different
statistical parity on race, are additionally presented.
has become increasingly aware of the importance of inherently-
interpretable machine-learning algorithms, and there is a pressing
need for models that can be understood and trusted by humans.
Logical models, based on ‚Äú if-then ‚Äù rules, are intrinsically inter-
pretable models for predictive tasks. Among popular logical models,
in this work we focus on rule sets, which are particularly easy to
interpret [ 25]. Extension to more structured logical models, such
asrule lists ordecision trees, is left to future work.
Another significant aspect of interpretable machine learning is
that, often, a single model does not offer an adequate representa-
tion of reality since there is a large set of models with near-optimal
predictive performance. In the literature, such a set is referred to as
Rashomon set. Rashomon sets have been shown to have applications
in multiple domains, including credit-score estimation, natural-
language processing, health-record analysis, recidivism prediction,
and more [ 23,34,37,43]. Considering the entire Rashomon set
rather than a single model provides a wealth of actionable informa-
tion. For instance, computing the proportion of models belonging
to the Rashomon set allows us to characterize the complexity of a
learning task [ 37]. Additionally, Rashomon sets allow us to study im-
portant properties of machine-learning models, such as fairness [ 15]
and feature importance [ 43]. As a concrete example, Figure 1 depicts
a Rashomon set of rule sets for the Compas dataset used for recidi-
vism prediction. Although the rule sets in the Rashomon set have
similar accuracy scores (ranging from 0.56to0.65), two important
measures of fairness vary significantly.
Due to the combinatorial explosion of the search space, exhaus-
tive enumeration or storage of the rule sets in the Rashomon set
poses significant computational challenges, and may not always be
feasible. In this paper, we propose, for the first time, methods to
efficiently explore the Rashomon set with or without exhaustive
 
478
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
enumeration. As demonstrated in Section 7, the proposed methods
accurately reveal the complexity inherent in tackling a learning
task based on rule sets, as well as other key properties of rule sets
including feature importance and fairness.
All the methods we propose rely on a highly-optimized branch-
and-bound algorithm for exhaustive enumeration of the rule sets
in the Rashomon set. To scale up, the branch-and-bound algorithm
leverages (ùëñ) pruning bounds that effectively restrict the search
space, and (ùëñùëñ) incremental computation to re-use previously com-
puted results. Building on our branch-and-bound algorithm for
exhaustive enumeration, we introduce two alternative approaches
for non-exhaustive exploration of the Rashomon set by generating
representative samples and estimating its size. The first approach
partitions the solution space into random cells and enumerates
the solutions in one randomly selected cell. The second approach
instead simply visits subsets of the search space and constructs sam-
ples during the process. The samples generated by both approaches
are supported by guarantees of near uniformity.
In summary, we make the following contributions.
‚Ä¢We formally describe exact and approximate variants of the prob-
lems of exhaustive and non-exhaustive enumeration of rule sets
in the Rashomon set.
‚Ä¢We propose a branch-and-bound algorithm, named BBenum , for
efficient exhaustive enumeration.
‚Ä¢AsBBenum may incur high cost, we develop ApproxSample and
ApproxCount , two highly-optimized algorithms with strong
quality guarantees, which allow for non-exhaustive exploration
of the Rashomon set by approximate uniform sampling and esti-
mation of the size of the Rashomon set.
‚Ä¢We additionally devise BBsts , a faster, but generally less accurate
alternative to ApproxSample andApproxCount.
‚Ä¢We evaluate the proposed algorithms in a thorough experimental
evaluation and through cases studies.
The rest of this paper is organized as follows. Section 2 discusses
related work. Section 3 introduces our notation and problem formu-
lations. Section 4 presents the proposed method for exhaustive enu-
meration of the Rashomon set, while Sections 5 and 6 describe the
proposed methods for non-exhaustive exploration of the Rashomon
set. Section 7 presents our experimental evaluation, and finally,
Section 8 is a short conclusion.
2 RELATED WORK
Interpretable machine learning . The study of interpretable mod-
els to address machine-learning tasks is a fast-growing field. The
topic is related to explainable machine learning [6], which aims at
explaining the predictions of opaque models [ 8]. However, there is
evidence that explaining opaque algorithms may provide mislead-
ing and even false characterizations [ 26,33]. Therefore, there is a
need for novel inherently interpretable models.
Optimal logical models. Logical models (including rule sets, rule
lists, and decision trees ) are prominent examples of interpretable
models that have been successfully used in a variety of applica-
tions [ 34,40,45]. Over the years, due to the complexity inherent in
the optimization, approximate algorithms and heuristic approaches
have been employed to find logical models with good predictionperformance. Recent advances in computing power and algorith-
mic techniques, however, motivate the search for globally optimal
models for different classes of logical models. For finding optimal
rule lists [ 5] and decision trees [ 22], ad hoc branch-and-bound
algorithms have been proposed, while existing work on finding
optimal rule sets relies on off-the-shelf SAT solvers [ 41] or integer
programming solvers [28].
The Rashomon set. In recent years, research in interpretable ma-
chine learning has emphasized the importance of going beyond
a single model. The Rashomon effect [7] expresses the idea that a
real-world phenomenon can be explained equally well by multiple
models. Such a set of models is referred to as the Rashomon set [34],
and finds a number of interesting applications, such as measuring
the complexity of a learning task [ 36], analyzing feature impor-
tance [18, 19] and investigating fairness in machine learning [29].
Recently, work has been carried out to develop techniques to
exhaustively enumerate the Rashomon set for particular classes of
models, including decision lists [30] and decision trees [43].
In a similar vein, Hara and Ishihata [21] consider approximate
and exact enumeration of rule sets and lists sorted by an objective
value, representing the quality of the model. Although this enumer-
ation problem is similar to the problem studied in this work, there
are crucial differences. Hara and Ishihata [21] consider a simplistic
formulation of the rule-set learning problem, which neglects false
positives. Further, the methods they propose hinge on particular
assumptions and the output rule models are required to be sorted
by their quality. In view of the mentioned differences, our proposed
methods and those by Hara and Ishihata [21] are not directly com-
parable. Additionally, the methods of Hara and Ishihata [21] are
not competitive with ours, in terms of runtime. For instance, for the
Compas dataset, we observe that the time required by our methods
to enumerate 50rule sets is on par with the time required by the
methods of Hara and Ishihata [21] to find a single rule set.
Rule sets can be regarded as less constrained extensions of de-
cision lists and trees. The problem of enumerating the Rashomon
set for rule sets is more challenging since the additional structure
imposed by decision lists and trees allows for pruning additional
portions of the search space. This computational challenge calls for
novel ideas: we can effectively explore the Rashomon set for rule
sets without exhaustive enumeration. Non-exhaustive enumeration,
which is the main focus of this work, is a largely unexplored topic.
Constrained counting and sampling. Constrained (or model)
sampling and counting is a fundamental problem in artificial in-
telligence involving sampling and counting the satisfying assign-
ments of a propositional formula. The problem is known to be
computationally hard [ 39]. Thus, approximate solutions have been
investigated. Chakraborty et al . [11,12]leverage hash functions to
randomly partition the space of possible models into small cells,
and satisfying assignments are sampled via calls to SAT solvers. In
this work, we leverage this idea to design efficient sampling and
counting algorithms that do not require exhaustive enumeration.
Ermon et al . [17] propose an alternative approach for approximate
model sampling. The algorithm leverages a SAT solver whilst en-
forcing a uniform exploration of the search space. We also build on
this idea to design alternative efficient algorithms for sampling and
counting without the need of exhaustive enumeration.
 
479Efficient Exploration of the Rashomon Set of Rule-Set Models KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
3 PROBLEM FORMULATION
We use boldface uppercase letters to denote matrices, e.g., ùë®, and
boldface lowercase letters to denote vectors, e.g., ùíôandùíÉ. For a
matrix ùë®, we use ùë®ùëñto denote its ùëñ-th row, ùë®:ùëñto denote its first ùëñ
rows, and ùë®ùëñ,ùëóto denote the ùëó-th element of ùë®ùëñ. Similarly, for a
vector ùíÉ, we use ùíÉùëñandùíÉ:ùëñto denote the ùëñ-th element and the
firstùëñelements of ùíÉ, respectively. Given a positive integer ùëÄand
a sequence of positive integers ùëÜwith values in the set {1,...,ùëÄ},
we use 1ùëÜ‚àà{0,1}ùëÄto denote the indicator vector of ùëÜ, i.e.,1ùëÜ,ùëñ=1
ifùëñ‚ààùëÜand1ùëÜ,ùëñ=0otherwise.
3.1 Preliminaries
We restrict our setting to binary classification with binary-valued
features. More general settings can be mapped to the setting we
study via preprocessing, although the performance of the resulting
methods will depend on the preprocessing methodology. Extending
our methods to more general settings is left for future work.
We denote the training data as D=[(ùíôùëõ,ùë¶ùëõ)]ùëÅ
ùëõ=1, where ùíôùëõ‚àà
{0,1}ùêΩare binary features and ùë¶ùëõ‚àà{0,1}is the label. Let ùíôùëõ,ùëó
denote the value of the ùëó-th feature of the observation vector ùíôùëõ.
A rule setùëÜ=(ùëü1,...,ùëüùêø)of sizeùêøconsists ofùêødistinct decision
rules. A decision rule (or simply, rule) ùëü=ùëù‚Üíùëûis a logical
implication ‚Äúif ùëùthenùëû‚Äù. An antecedent ùëùis a clause consisting of a
conjunction of features. For data point ùíôùëõ, antecedent ùëùevaluates to
trueif all features of ùëùhave value 1, i.e., ùíôùëõ,ùëó=1for all features ùëóinùëù,
and it evaluates to false otherwise. A consequent ùëûis the predicted
label. For instance, the rule (ùíôùëõ,2=1)‚àß( ùíôùëõ,5=1)‚Üíùë¶ùëõ=1
predictsùë¶ùëõ=1for any data point ùíôùëõwith ùíôùëõ,2=1andùíôùëõ,5=1.
We say that a rule ùëü=ùëù‚Üíùëûcaptures a data point ùíôùëõ, writ-
ten as cap(ùíôùëõ,ùëü)=1, ifùëùevaluates ùíôùëõto true. We say that the
rule setùëÜcaptures ùíôùëõ, written as cap(ùíôùëõ,ùëÜ)=1, if at least one
rule inùëÜcaptures ùíôùëõ. Ifùíôùëõis not captured by any rule in ùëÜ, we
write cap(ùíôùëõ,ùëÜ)=0. As it is common [ 13,42], to prioritize inter-
pretability, we consider rule sets consisting of positive rules only,
i.e.,ùëû=(ùë¶ùëõ=1).1In other words, if cap(ùíôùëõ,ùëÜ)=1, then the pre-
diction isùë¶ùëõ=1, while if cap(ùíôùëõ,ùëÜ)=0, the prediction is ùë¶ùëõ=0.
We assume that a set of candidate decision rules U={ùëü1,...,ùëüùëÄ}
is provided.2We further assume that the rules in Uare ordered,
e.g., lexicographically, indicated by a subscript index. Hence, we say
that ruleùëüùëòisbefore (orafter ) ruleùëü‚Ñìifùëò<‚Ñì(orùëò>‚Ñì). We assume
that the rules of a rule set ùëÜare sorted in ascending order. We say
that a rule set ùëÜ‚Ä≤starts with rule setùëÜifùëÜ‚äÜùëÜ‚Ä≤and all rules in ùëÜ‚Ä≤\ùëÜ
are after the last rule in ùëÜ. We denote by ùëÜmax=arg maxùëñ{ùëüùëñ‚ààùëÜ}
the largest rule index in a given rule set ùëÜ‚äÜU .
For a ruleùëüùëò‚ààùëÜ, we define cap(ùíôùëõ,ùëüùëò|ùëÜ)=1ifùíôùëõis captured
byùëüùëò, but not by rules in ùëÜthat are before ùëüùëò, i.e.,
cap(ùíôùëõ,ùëüùëò|ùëÜ)=cap(ùíôùëõ,ùëüùëò)‚àß‚àß
ùëü‚Ñì‚ààùëÜ|‚Ñì<ùëò(¬¨cap(ùíôùëõ,ùëü‚Ñì)).
When the context is clear, we use rules (e.g., ùëüùëñ) and their indices
(e.g.,ùëñ) interchangeably. As a result, a rule set ùëÜcan be represented
as a sorted list of integers and 1ùëÜ‚àà{0,1}ùëÄrepresents the indicator
vector of the rule indices in ùëÜ.
1Negative rules, i.e. ùëû=(ùë¶ùëõ=0), when used together with positive rules, may hinder
interpretability by simultaneously predicting labels as 0and1.
2For instance, the set of rules can be obtained via some association rule-mining algo-
rithm [24], like the FP-growth algorithm [20].3.2 Objective function
To assess a rule set ùëÜin terms of accuracy and interpretability, we
consider the following objective function:
ùëì(ùëÜ;ùúÜ)=‚Ñì(ùëÜ)+ùúÜ|ùëÜ|, (1)
which consists of a misclassification error term ‚Ñì(ùëÜ)and a penalty
term|ùëÜ|for model complexity. The intuition is that, for a given
level of accuracy, shorter rule sets are preferred as they are easier
to interpret and are less prone to overfitting. The regularization
parameterùúÜ>0controls the relative importance of the two terms.
The loss term ‚Ñìcan be decomposed into:
‚Ñì(ùëÜ)=‚Ñìùëù(ùëÜ)+‚Ñì0(ùëÜ), (2)
where
‚Ñìùëù(ùëÜ)=1
ùëÅùëÅ‚àëÔ∏Å
ùëõ=1ùêæ‚àëÔ∏Å
ùëò=1(cap(ùíôùëõ,ùëüùëò|ùëÜ)‚àß 1[ùë¶ùëõ‚â†1])and (3)
‚Ñì0(ùëÜ)=1
ùëÅùëÅ‚àëÔ∏Å
ùëõ=1(¬¨cap(ùíôùëõ,ùëÜ)‚àß 1[ùë¶ùëõ=1]). (4)
The term‚Ñìùëùis the proportion of false positives of the rule set ùëÜ,
while the term ‚Ñì0is the proportion of false negatives.
3.3 The Rashomon set of decision sets
Given a set of candidate decision rules U, an objective function ùëì(¬∑;ùúÜ)
for evaluating rule sets, and a parameter ùúÉ‚ààR+, we define the
Rashomon set of rule sets for Uwith respect to ùúÜandùúÉas:
R(U,ùúÜ,ùúÉ)={ùëÜ‚äÜU|ùëì(ùëÜ;ùúÜ)‚â§ùúÉ}. (5)
When the context is clear, we use R(U)instead ofR(U,ùúÜ,ùúÉ).
In the literature, the Rashomon set is sometimes alternatively
defined with ùúÉ=ùëì‚àó+ùõº, whereùëì‚àóis the optimal objective value.
3.4 Problem formulation
We consider a set of candidate rules U={ùëü1,...,ùëüùëÄ}, each of
which passes a given threshold on the number of captured training
points. This definition of Uis common in the literature [ 5,24,25].
To constructU, we resort to the popular FP-growth algorithm [ 20].
We first consider the problem of exhaustively enumerating R(U).
Problem 1 (Enumeration). Given a set of candidate rules U, and
parametersùúÜ>0andùúÉ>0, enumerate all rule sets in R(U,ùúÜ,ùúÉ).
Solving this problem allows us to compute |R(U,ùúÜ,ùúÉ)|and draw
uniform samples from R(U,ùúÜ,ùúÉ). The number|R(U,ùúÜ,ùúÉ)|can be
further used to compute the Rashomon ratio [37], which is defined
as the ratio between |R(U)|and the total number of models.3This
ratio is a measure of complexity of a learning problem. The larger
the ratio, the more likely that a simple-yet-accurate model exists.
Problem 1 is #P-hard and the problems of almost-uniform sam-
pling and approximate counting, defined next, are also hard, as they
can be shown to generalize similar problems whose complexity has
been established in the literature [44].
We define as a sampling algorithm S(or sampler) any algorithm
that, given as input the set of candidate rules U, the objective
functionùëìand the value of the upper bound ùúÉ, returns a random
3In our case, the Rashomon ratio is computed as |R(U)|/(2ùëÄ‚àí1).
 
480KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
Algorithm 1 BBenum , a branch-and-bound algorithm to enumer-
ate all rule sets inR(U,ùúÜ,ùúÉ).
1:ùëÑ‚ÜêQueue([‚àÖ])
2:whileùëÑis not empty do
3:ùëÜ‚ÜêùëÑ.pop()
4: forùëñin{ùëÜmax+1,...,ùëÄ}do
5:ùëÜ‚Ä≤‚ÜêùëÜ‚à™{ùëñ}
6: ifùëè(ùëÜ‚Ä≤)‚â§ùúÉ{Hierarchical lower bound} then
7: ifùëè(ùëÜ‚Ä≤)+ùúÜ‚â§ùúÉ{Look-ahead bound} then
8: if|ùëÜ‚Ä≤|‚â§‚åäùúÉ‚àíùëè(ùëÜ‚Ä≤)
ùúÜ‚åã{Size bound} then
9: ùëÑ.push(ùëÜ‚Ä≤)
10: ifùëì(ùëÜ‚Ä≤)‚â§ùúÉthen
11: yieldùëÜ‚Ä≤{Yield a feasible solution}
element fromR(U). Similarly, a counting algorithm Creceives the
same inputs and estimates |R(U)|.
Problem 2 (Almost-uniform sampling). Given objective func-
tionùëì, find a samplerS, such that for a bound ùúÉ‚ààR+, tolerance
parameterùúñ‚ààR+, andùëÜ‚ààR(U), it holds
1
(1+ùúñ)1
|R(U)|‚â§Pr(S(U,ùëì,ùúÉ,ùúñ)=ùëÜ)‚â§(1+ùúñ)1
|R(U)|.(6)
We similarly define the approximate counting problem.
Problem 3 (Approximate counting). Find a counting algo-
rithmC, such that for a tolerance parameter ùúñ‚ààR+and a confidence
parameterùõø‚àà[0,1], it holds
Pr|R(U)|
1+ùúñ‚â§C(U,ùëì,ùúÉ,ùúñ,ùõø)‚â§(1+ùúñ)|R(U)|
‚â•1‚àíùõø. (7)
4 AN EXACT ALGORITHM VIA COMPLETE
ENUMERATION
In this section, we describe our solution for Problem 1, a branch-
and-bound algorithm equipped with effective pruning bounds and
incremental computation techniques, which enumerates efficiently
all rule sets inR(U,ùúÜ,ùúÉ). Similar enumeration problems have been
studied for other types of logical models, such as decision lists [ 30]
and decision trees [43], but new ideas are required for rule sets.
4.1 A branch-and-bound algorithm
In order to find the set of feasible solutions, the algorithm we
propose, referred to as BBenum and presented in Algorithm 1, visits
rule sets in a breadth-first fashion with the help of a queue and
leverages a hierarchy among the rule sets to prune away the rule
setsùëÜ‚Ä≤that start with a rule set ùëÜif certain criteria on ùëÜare met.
In particular, at each iteration, the rule set at the front of the
queue is popped and extended with an additional rule, whose index
is in the range[ùëÜmax+1,...,ùëÄ], to formùëÜ‚Ä≤. Next, we check using
bounds (described shortly) whether rule set ùëÜ‚Ä≤and any rule set
starting with ùëÜ‚Ä≤can be pruned. If ùëÜ‚Ä≤is not pruned, we enqueue it.
If in addition the objective value achieved by ùëÜ‚Ä≤is below the upper
boundùúÉ, we addùëÜ‚Ä≤to the Rashomon set.
The proposed pruning bounds are based on two key observations:
(ùëñ)rule sets form a hierarchy under prefix relations, i.e., ùëÜ‚Ä≤‚äÜU is a
descendant of ùëÜ‚äÜU in the hierarchy if ùëÜ‚Ä≤starts withùëÜ;(ùëñùëñ)certain
characteristics of a given rule set can determine the feasibility ofits descendants in the hierarchy. We next illustrate the details of
the pruning bounds. Proofs of all non-trivial results presented in
the following sections are provided in an extended version of this
paper available online [14].
Hierarchical objective lower bound. For a rule set ùëÜ, we define:
ùëè(ùëÜ)=‚Ñìùëù(ùëÜ)+ùúÜ|ùëÜ|. (8)
Then, for any ùëÜ‚Ä≤that starts with ùëÜ, the quantity ùëè(ùëÜ)serves as a
lower bound for ùëì(ùëÜ‚Ä≤), as formalized next.
Theorem 1 (Hierarchical objective lower bound). For any
rule setùëÜ‚äÜU and anyùëÜ‚Ä≤‚äÜU that starts with ùëÜ, it isùëì(ùëÜ‚Ä≤)‚â•ùëè(ùëÜ).
In other words, all rule sets ùëÜ‚Ä≤starting with a rule set ùëÜfor which
ùëè(ùëÜ)>ùúÉare infeasible.
Look-ahead lower bound. The next bound takes Theorem 1 one
step further by observing that any superset of ùëÜmust include at
least an additional rule.
Theorem 2 (Look-ahead lower bound). For a given rule set
ùëÜ‚äÜU , ifùëè(ùëÜ)+ùúÜ>ùúÉ, then for any rule set ùëÜ‚Ä≤‚äÜU that starts with ùëÜ
and is a proper superset of ùëÜ(i.e.,ùëÜ‚Ä≤‚â†ùëÜ), it holds that ùëì(ùëÜ‚Ä≤)>ùúÉ.
Rule set size bound. Finally, we use the lower bound ùëè(ùëÜ)to
bound the size of any rule set that can be part of the Rashomon set.
Theorem 3 (Rule set size bound). For a given rule set ùëÜ‚äÜU
and any rule set ùëÜ‚Ä≤‚äÜU that starts with ùëÜ, if|ùëÜ|>‚åä(ùúÉ‚àíùëè(ùëÜ))/ùúÜ‚åã,
thenùëì(ùëÜ‚Ä≤)>ùúÉ.
We empirically find that the look-ahead and the rule-set-size
bounds are remarkably effective in pruning. Details are presented
in the extended version of this paper [14].
4.2 Incremental computation
To further speed up BBenum , we updateùëè(¬∑)andùëì(¬∑)incrementally.
The update formulas are stated below.
Theorem 4 (Lower bound update). For any rule set ùëÜ‚äÜU and
anyùëÜ‚Ä≤‚äÜU that starts with ùëÜand has exactly one more rule ùëü, i.e.,
ùëÜ‚Ä≤=ùëÜ‚à™{ùëü}, the following holds:
ùëè ùëÜ‚Ä≤=ùëè(ùëÜ)+ùúÜ+1
ùëÅùëÅ‚àëÔ∏Å
ùëõ=1(cap(ùíôùëõ,ùëü|ùëÜ)‚àß 1[ùë¶ùëõ‚â†1]).
Thus, provided that ùëè(ùëÜ)is computed already, computing ùëè(ùëÜ‚Ä≤)
requires evaluating only the last term in the above sum.
Theorem 5 (Objective update). For any rule set ùëÜ‚äÜU and
anyùëÜ‚Ä≤‚äÜU that starts with ùëÜand has exactly one more rule ùëü, i.e.,
ùëÜ‚Ä≤=ùëÜ‚à™{ùëü}, the following holds:
ùëì ùëÜ‚Ä≤=ùëè ùëÜ‚Ä≤+1
ùëÅùëÅ‚àëÔ∏Å
ùëõ=1(¬¨cap(ùíôùëõ,ùëÜ)‚àß¬¨cap(ùíôùëõ,ùëü)‚àß 1[ùë¶ùëõ=1]).
The details of the branch-and-bound algorithm with incremental
computation are provided in the extended version of this paper [ 14].
5APPROXIMATION ALGORITHMS BASED ON
RANDOM PARTITIONING
In this section, we address Problems 2 and 3. We develop efficient
methods with theoretical quality guarantees. To achieve this objec-
tive, we leverage the SAT-based framework proposed by Meel [31].
 
481Efficient Exploration of the Rashomon Set of Rule-Set Models KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
However, since this framework scales poorly for our purposes, we
propose novel methods to improve scalability.
5.1 An algorithmic framework based on
random parity constraints
We illustrate the proposed framework by first discussing our algo-
rithm for the counting problem, i.e., Problem 3.
Approximate counting. Algorithm ApproxCount , shown as Al-
gorithm 2 in the extended version of this paper [ 14], generates ran-
dom parity constraints to partition the solution space into ‚Äúsmall
cells.‚Äù It then measures the size of a random cell (i.e., the number
of solutions in the cell) and computes an estimate of |R(U)|by
multiplying that cell size by the number of cells.4To achieve the
desired confidence, the estimation is repeated on sufficiently many
random cells, and the median is returned as the final estimate.
To achieve the desired estimation quality, the algorithm deter-
mines an upper bound ùêµon cell sizes based on a tolerance para-
meterùúñ. Each evaluation, carried out by ApproxCountCore (Al-
gorithm 3 in the extended version of this paper [ 14]) first generates
ùëÄ‚àí1random parity constraints. Then, it searches for the number
of constraints that produce a cell of size closest to, but below, ùêµ.
Finally, all the solutions in that cell are enumerated to obtain the
cell size.
Solution space partitioning via parity constraints. A system of
parity constraints is imposed on the original enumeration problem
(Problem 1). The system consists of ùëòlinear equations in the finite
field of 2 and can be written as ùë®ùíô=ùíÉ, where ùë®‚àà{0,1}ùëò√óùëÄ,
ùíÉ‚àà{0,1}ùëòandùíô‚àà{0,1}ùëÄ(the solution variable). The system
ùë®ùíô=ùíÉlocates a specific cell among the 2ùëòcounterparts (each corre-
sponding to a different value in {0,1}ùëò). The set of feasible solutions
in that cell is denoted by R(U;ùë®,ùíÉ)={ùëÜ‚ààR(U)|ùë®1ùëÜ=ùíÉ}.
Searching for the desired ùëò.Given constraints ùë®ùíô=ùíÉ, where
ùë®‚àà{0,1}(ùëÄ‚àí1)√óùëÄandùíÉ‚àà{0,1}ùëÄ, procedure LogSearch (Al-
gorithm 7 in the extended version of this paper [ 14]) finds the
value ofùëòsuch that the cell size |R(U|ùë®:ùëò,ùíÉ:ùëò)|is closest to,
but below,ùêµ. For each attempted ùëò,LogSearch invokes an ora-
cleParityConsEnum , which enumerates at mostùêµsolutions in
R(U|ùë®:ùëò,ùíÉ:ùëò).
Near-uniform sampling. ApproxSample (Algorithm 8 in the
extended version of this paper [ 14]) relies on a similar idea as
ApproxCount ; the solution space is partitioned into cells and sam-
ples are drawn from random cells. The algorithm accepts a tolerance
parameterùúñto determine a range of the desired cell sizes (to guar-
antee closeness to uniformity). To find the appropriate value of ùëò,
it first obtains an estimate ÀÜùëêof|R(U)|using ApproxCount . Then,
different values of ùëò(determined by ÀÜùëê) are attempted until the re-
sulting cell size falls within the desired range. Finally, a sample is
drawn uniformly at random from that cell.
Statistical guarantee. Meel [31] proves that, provided that the
oracle ParityConsEnum exists, the counting and sampling algo-
rithms (Algorithm 2 and Algorithm 8 in the extended version of this
paper [ 14]) indeed address the approximate sampling and counting
problems (Problems 2 and 3), respectively.
4The size of a cell is the number of feasible solutions in it.5.2 Parity constrained enumeration
The effectiveness of the above approach heavily depends on the im-
plementation of the oracle ParityConsEnum . In the work of Meel
[31], SAT-based solvers are used since the work deals with the
general problem of constrained programming. In our setting, we
rely on BBenum and linear algebra to design a novel algorithm
tailored for our problems for better scalability. Formally, the oracle
ParityConsEnum addresses the following problem.
Problem 4 (Partial enumeration under parity constraints).
Given a set of candidate rules U, an objective function ùëì, an upper
boundùúÉ, a parity constraint system characterized by ùë®ùíô=ùíÉ, and an
integerùêµ, find a collection of rule sets Ssuch that|S|‚â§ùêµ,ùëì(ùëÜ)‚â§ùúÉ,
andùë®1ùëÜ=ùíÉ, for allùëÜ‚ààS.
Compared to Problem 1, the above problem asks to enumerate
at mostùêµsolutions and further imposes parity constraints on the
solution. Note that Problem 4 is at least as hard as Problem 1, since
the latter is a special case.
Without loss of generality, we assume the matrix ùë®is in its
reduced row echelon form ùë®‚àí, resulting in the system ùë®‚àíùíô=ùíÉ‚àí.5
The reason is that for any ùëÜ, it is ùë®‚àíùíô=ùíÉ‚àíif and only if ùë®ùíô=ùíÉ, so
that replacing the constraint ùë®ùíô=ùíÉin Problem 4 with ùë®‚àíùíô=ùíÉ‚àí
results in an equivalent problem. Further, important properties
revealed by ùë®‚àí, such as the rank and pivot positions, turn out to
be essential for the subsequent algorithmic developments. Finally,
we assume there is at least one feasible solution to ùë®ùíô=ùíÉ.
Letùúåbe the rank of ùë®and let pivotùë®:[ùúå]‚Üí[ùúå]denote the
pivot table ofùë®, where pivotùë®[ùëñ]is the column index of the pivot po-
sition in the ùëñ-th row. We definePùë®=
pivotùë®[ùëñ]|ùëñ‚àà[ùúå]	
, i.e., the
indices of columns corresponding to pivot variables in ùë®. Similarly,
we defineFùë®={0,...,ùëÄ‚àí1}\Pùë®, i.e., the indices of columns
corresponding to free columns. When context is clear, for brevity,
we drop the subscript ùë®and use pivot[ùëñ],PandF.
We relate the rules to the pivot positions. We call the ùëó-th rule
apivot rule if theùëó-th column in ùë®corresponds to some pivot
position, i.e., exists ùëñ‚àà[ùúåùë®]such that pivot[ùëñ]=ùëó. Otherwise, the
rule is called a free rule. For rule set ùëÜ, we denoteP(ùëÜ)=P‚à©ùëÜthe
set of pivot rules in ùëÜandF(ùëÜ)=F‚à©ùëÜthe set of free rules in ùëÜ.
5.3 A branch-and-bound algorithm
The proposed algorithm builds upon a technique for enumerating
solutions to a linear system ùë®ùíô=ùíÉin finite field of 2. During
the enumeration process, solutions are pruned using the bounds
(Section 4) to satisfy ùëì(ùëÜ)‚â§ùúÉ.
Enumerating feasible solutions to ùë®ùíô=ùíÉ.We first consider
the problem of enumerating all feasible solutions to ùë®ùíô=ùíÉalone.
A straightforward way is by considering the reduced row eche-
lon form of ùë®, identifying the pivot variables and free ones, and
considering all possible assignments of the free variables .
We give a toy example of 3 constraints and 5 variables: the re-
duced row echelon form is shown on the left, while the formula for
the feasible solutions on the right. The pivot columns (correspond-
ing toùë•1,ùë•2, andùë•4) are highlighted in bold. The set of feasible
solutions can be enumerated by substituting [ùë•3,ùë•5]‚àà{0,1}2in
the equation below.
5ùíÉ‚àíis obtained via the same operations done on ùë®.
 
482KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1 0 000
0 1 100
0 0 011Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ªÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë•1
ùë•2
ùë•3
ùë•4
ùë•5Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1
0
1Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª‚Üíùë•=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1
0
0
1
0Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª+Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞0
1
1
0
0Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªùë•3+Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞0
0
0
1
1Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªùë•5
Main idea of the proposed algorithm. Algorithm 2 integrates
the above ideas into the search process in Algorithm 1. The main
changes are:
(1)In the for loop of Algorithm 1, we only check the free rules.
In the example above, only ùë•3andùë•5are checked.
(2)While adding a rule ùëóto a given rule set, the procedure
EnsMinNonViolation checks if the satisfiability of some
parity constraints can already be determined. If this is the
case, the associated pivot rules are added.
(3)When checking the look-ahead bound, the pivot rules added
byEnsMinNonViolation are considered.
(4)Before yielding a solution, EnsSatisfaction adds relevant
pivot rules to guarantee ùë®ùíô=ùíÉis satisfied.
(5)The algorithm terminates when either ùêµsolutions or all
feasible solutions (at most ùêµ) are found.
(6)Finally, a priority queue is used to guide the search process,
where the priority of a rule set ùëÜequals‚àíùëè(ùëÜ).
Ensuring minimal non-violation. To describe the procedure
EnsMinNonViolation , we need some additional definitions. Given
a matrix ùë®, itsboundary table, denoted by ùêµùë®:[ùúåùë®]‚Üí[ùëÄ], maps
a row index to the largest non-zero non-pivot column index of that
row in ùë®. That is,ùêµùë®[ùëñ]=max
ùëó|ùë®ùëñ,ùëó=1andùëó‚â†pivotùë®[ùëñ]	if√ç
ùëóùë®ùëñ,ùëó>1, otherwise ùêµùë®[ùëñ]=‚àí1, for every ùëñ‚àà[ùúåùë®]. In our
example,ùêµùë®=[‚àí1,2,4].
We use the boundary table to check if the satisfiability of con-
straints in ùë®ùíô=ùíÉcan be determined by a given ùëÜ. Given a con-
straint ùë®:ùëñùíô=ùíÉ:ùëñ, we say its satisfiability is determined byùëÜif
ùëÜmax‚â•ùêµùë®[ùëñ]. In other words, adding any rule after ùëÜmaxdoes not
affect its satisfiability. In our example, the satisfiability of ùë•2+ùë•3=1
is determined by{1,4}and{4}but not by{1}.
Given a rule set ùëÜ, we say that ùëÜisnon-violating if the constraints
inùë®ùíô=ùíÉthat are determined by ùëÜare all satisfied. For instance,
{1,4}and{1}are non-violating, while {4}is not. Further, we say ùëÜ
isminimally non-violating ifùëÜis non-violating and removing any
ruleP(ùëÜ)fromùëÜviolates at least one constraint. For such ùëÜ, we
call each rule inP(ùëÜ)anecessary pivot forF(ùëÜ). In our example,
ùëÜ={1,2,3}is minimally non-violating.
We rely on minimal non-violation to determine the addition of
a minimal set of pivot rules to ensure non-violation. Minimality
ensures no redundant rules are added, thus the algorithm does not
incorrectly prune feasible rule sets.
The procedure EnsMinNonViolation (Algorithm 4 in the ex-
tended version of this paper [ 14]) returns the set of pivot rules to
ensure minimal non-violation of a given ùëÜ. For each constraint, the
process checks if it is determined and unsatisfied, and if yes, adds
the associated pivot rule. Formally: let ùíì=ùíÉ‚àíùë®¬∑1ùëÜ. For each
ùëñ‚àà[ùúå(ùë®)], ifùíìùëñ=1andùëÜmax‚â•ùêµùë®[ùëñ], then add the pivotùë®[ùëñ]-th
pivot rule.
Theorem 6. Given a parity constraint system ùë®ùíô=ùíÉ, for any rule
setùëÜwith free rules only, it follows that EnsMinNonViolation (ùëÜ,ùë®,ùíÉ)Algorithm 2 A branch-and-bound algorithm to solve Problem 4.
1:ùëõ‚Üê0
2:ùëÜùë†‚ÜêEnsSatisfaction(‚àÖ,ùë®,ùíÉ)
3:ifùëÖ(ùëÜùë†)‚â§ùúÉthen
4: Incrementùëõand yieldùëÜùë†
5:ùëÜùëû‚ÜêEnsMinNonViolation (‚àÖ,ùë®,ùíÉ)
6:ùëÑ‚ÜêPriorityQueue  ùëÜùëû,ùëè ùëÜùëû
7:whileùëÑis not empty and ùëõ<ùêµdo
8:ùëÜ‚ÜêùëÑ.ùëùùëúùëù()
9: forùëó=(F(ùëÜ)ùëöùëéùë•+1),...,ùëÄ andùëóis free do
10:ùëÜ‚Ä≤‚ÜêùëÜ‚à™{ùëó}
11: ifùëè(ùëÜ‚Ä≤)‚â§ùúÉthen
12:ùê∏ùëû‚ÜêEnsMinNonViolation (ùëÜ‚Ä≤,ùë®,ùíÉ)
13: ifùëè(ùëÜ‚Ä≤‚à™ùê∏ùëû)+ùúÜ‚â§ùúÉthen
14: ùëÑ.push(ùëÜ‚Ä≤,ùëè(ùëÜ‚Ä≤‚à™ùê∏ùëû))
15:ùê∏ùë†‚ÜêEnsSatisfaction(ùëÜ‚Ä≤,ùë®,ùíÉ)
16:ùëÜùë†‚ÜêùëÜ‚Ä≤‚à™ùê∏ùë†
17: ifùëÖ(ùëÜùë†)‚â§ùúÉthen
18: Incrementùëõand yieldùëÜùë†
returns a set of pivot rules ùê∏such thatùëÜ‚à™ùê∏is minimally non-violating
with respect to ùë®ùíô=ùíÉ.
Ensuring satisfiability. Satisfiability to ùë®ùíô=ùíÉis guaranteed
byEnsSatisfaction (Algorithm 5 in the extended version of this
paper [ 14]), which works as follows: let ùíì=ùíÉ‚àíùë®¬∑1ùëÜ, for each
ùëñ‚àà[ùúå(ùë®)], add the pivotùë®[ùëñ]-th pivot rule if ùíìùëñ=1.
Proposition 1. Given a parity constraint system ùë®ùíô=ùíÉ, for any
rule setùëÜwith free rules only, it follows that EnsSatisfaction(ùëÜ,ùë®,ùíÉ)
returns a set of pivot rules ùê∏such that ùë®1ùëÜ‚à™ùê∏=ùíÉ.
Extended look-ahead bound. Finally, we extend the look-ahead
bound (Theorem 2) to account for the addition of necessary pivots.
Theorem 7 (Extended look-ahead bound). Given a parity
constraint system ùë®ùíô=ùíÉ, letùëÜbe a rule set with free rules only and
letùê∏be the set of necessary pivots associated with ùëÜwith respect to
ùë®ùíô=ùíÉ. Ifùëè(ùëÜ‚à™ùê∏)+ùúÜ>ùúÉ, then for any ùëÜ‚Ä≤that starts with ùëÜand
ùëÜ‚Ä≤‚â†ùëÜ, it follows that ùëì(ùëÜ‚Ä≤‚à™ùê∏‚Ä≤)>ùúÉ, whereùê∏‚Ä≤is the set of necessary
pivots forùëÜ‚Ä≤with respect to ùë®ùíô=ùíÉ.
5.4 Incremental computation
We achieve further speed up by incrementally adding the pivots
to ensure minimal non-violation and satisfaction. For instance, we
address the following question: given a minimally non-violating
rule setùëÜ, if ruleùëóis added toùëÜ, which pivot rules should be added
to maintain minimal non-violation of the new rule set?
Two arrays are used to represent the parity and satisfiability
states of a rule set ùëÜ. The parity states array ùíõ‚àà{0,1}ùëòstores the
difference between ùë®ùëñ1ùëÜandùíÉùëñ, for eachùëñ. The satisfiability array
ùíî‚àà{0,1}ùëòstores whether the satisfiability of each constraint is
guaranteed (meaning determined and satisfied) by ùëÜ. Computations
are saved by ( ùëñ) skipping the check of constraints whose satisfiability
is already guaranteed and ( ùëñùëñ) determining the addition of pivots
based only on the value of ùíõandùíÉ. Further, both ùíõandùíîare updated
incrementally. Details are provided in Appendixes C.5 and C.6 of
the extended version of this paper [14].
 
483Efficient Exploration of the Rashomon Set of Rule-Set Models KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
5.5 Implementation details
We also propose a few implementation-level enhancements (details
in Appendix C.8 of the extended version of this paper [ 14]) to speed
up even more the above algorithms.
‚Ä¢The columns of ùë®and the rules are permutated to increase the
chances that IncEnsNoViolation returns a non-empty pivot
sets, leading to more pruning of the search space.
‚Ä¢ApproxCountCore executions are parallelized in ApproxCount .
‚Ä¢We use a fast routine to compute the number of pivot rules
required for satisfiability, before calling the more expensive
IncEnsSatisfaction . This number is used to check the rule set
size bound.
6 SEARCH-TREE-BASED APPROXIMATION
ALGORITHMS
In this section we introduce BBsts , a fast alternative to
ApproxCount , which draws approximately uniform samples and
approximates the size of the Rashomon set. BBsts leverages ideas
from the SearchTreeSampler method by Ermon et al . [17] for
approximately uniform sampling of solutions (i.e., satisfying assign-
ments) of a set of hard constraints in a combinatorial space.
BBsts assumes that rule sets are organized in a search tree. The
root of the search tree is the empty rule set. All rule sets that are ùëè-
hops away from the tree root contain exactly ùëèrules. BBsts explores
the search tree in a breadth-first fashion. While exploring the tree,
BBsts generates partial rule sets , which are progressively extended
(by adding additional rules) to form the final solutions. Partial rule
sets of level‚Ñéare associated with the first ‚Ñérules inU.
BBsts does not traverse the search tree exhaustively. Given an
input parameter ‚Ñì, the search tree is partitioned into ùêø=‚åàùëÄ
‚Ñì‚åâ
depth levels. The parameter ‚Ñìcontrols the approximation level, the
smaller‚Ñì, the larger runtime and expected solution quality. At depth
ùëñof the search tree, BBsts generates partial rule sets of level ùëñ‚Ñì.
The steps of BBsts are summarized in Algorithm 3 and visu-
ally in Figure 2 of the extended version of this paper [ 14].BBsts
starts from the tree root which corresponds to the empty rule set
being the partial solution ùëÉ0at depth and level 0. Then, at the ùëñ-th
iteration, partial rule sets ùëÉùëñ‚àí1at depthùëñ‚àí1(of level(ùëñ‚àí1)‚Ñì) are
uniformly sub-sampled without replacement, and for each sampled
partial solution ùëÜ,BBsts finds all the partial rule sets ùëÜ‚Ä≤at depthùëñ
(of levelùëñ‚Ñì) that start with ùëÜ. The set of all such partial rule sets
at depthùëñthat start with ùëÜis denoted by{ùëÜ‚Ä≤}ùëÜ
ùëñ. To find all the par-
tial solutions{ùëÜ‚Ä≤}ùëÜ
ùëñ,BBsts starts fromùëÜand invokes a variant of
BBenum(ùëÜ,‚Ñì)which considers ‚Ñìadditional rules. BBenum(ùëÜ,‚Ñì)
is identical to BBenum , as described in Algorithm 1, except that
it starts by enqueueing set ùëÜinstead of the empty set ‚àÖ, and the
main loop only iterates from (ùëñ‚àí1)‚Ñìtoùëñ‚Ñì, instead of from ùëÜùëöùëéùë•+1
toùëÄ. The process of drawing a uniform sample ùëÜfromùëÉùëñ‚àí1and
finding the associated set {ùëÜ‚Ä≤}ùëÜ
ùëñis repeated min(ùúÖ,|ùëÉùëñ‚àí1|)times,
for a user-specified parameter ùúÖ, which trades quality for efficiency.
The largerùúÖis, the longer runtime but higher solution quality.
Eventually, BBsts yields approximate uniform samples from the
Rashomon set by generating partial rule sets at depth ùêø(of levelùëÄ),
and filtering out the rule sets that do not belong to the Rashomon set.
In particular, Ermon et al . [17] show that, for any partial solutionsAlgorithm 3 BBsts algorithm for Problem 2.
1:ùëÉ0‚Üê‚àÖ .
2:ùêø‚Üê‚åàùëÄ
‚Ñì‚åâ.
3:forùëñin1,...ùêø do
4:ùëÉùëñ‚Üê‚àÖ
5: forùëóin1,...min(ùúÖ,|ùëÉùëñ‚àí1|)do
6: drawùëÜ‚àºU(ùëÉùëñ‚àí1)without replacement
7:{ùëÜ‚Ä≤}ùëÜ
ùëñ‚ÜêBBenum(ùëÜ,‚Ñì)
8:ùëÉùëñ‚ÜêùëÉùëñ‚à™{ùëÜ‚Ä≤}ùëÜ
ùëñ
9:return{ùëÉùëñ}‚à©R(U)
ùëÜandùëÜ‚Ä≤, it holds:
ùúÖ
2‚Ñì+ùúÖ‚àí1‚â§Pr(ùëÜ)
Pr(ùëÜ‚Ä≤)‚â§2‚Ñì+ùúÖ‚àí1
ùúÖ, (9)
where Pr(ùëÜ)denotes the probability of sampling ùëÜ. Equation (9)
bounds the uniformity of the samples returned by BBsts , but it only
holds for large ùúÖ. For values of ùúÖused in practice, the uniformity
guarantee in Equation (9) may not hold, and a rule set ùëÜmay be
arbitrarily more likely to be sampled than another rule set ùëÜ‚Ä≤.
The use of a BBenum -like search is the main difference between
BBsts andSearchTreeSampler [17], which, instead, uses expen-
sive calls to SAT solvers. This difference leads to a drastic reduction
in runtime, because, as shown in Section 7.2, BBenum outperforms
a SAT-based solver in runtime by orders of magnitude.
Not only BBsts efficiently draws samples from the Rashomon
set, but, as suggested by Ermon et al . [17] , the partial rule sets
constructed while executing BBsts pave the way for estimation of
|R(U)|via the following formula:
|R(U)|‚âà|ùëÉùêø|=|ùëÉùêø|
|ùëÉùêø‚àí1||ùëÉùêø‚àí1|
|ùëÉùêø‚àí2||ùëÉùêø‚àí2|
|ùëÉùêø‚àí3|...|ùëÉ1|
1, (10)
where|ùëÉùëñ|
|ùëÉùëñ‚àí1|=1
|ùëÉùëñ‚àí1|√ç
ùëÜ‚ààùëÉùëñ‚àí1|{ùëÜ‚Ä≤}ùëÜ
ùëñ|.Note that Equation (10) does
not provide any accuracy guarantee.
7 EXPERIMENTS
In this section, we present an empirical evaluation of our methods.
The main goal of the evaluation is to demonstrate the effective-
ness and scalability of the proposed methods for exploring the
Rashoomon set of rule sets.
As our methods come with guarantees of near uniformity for
sampling, we focus on demonstrating the accuracy of our methods
in estimating the size of the Rashomon set (counting). Accurate
counts obtained by ApproxCount andBBsts are also good indica-
tions of uniform output samples.
We describe the experimental setup in Section 7.1, present a per-
formance comparison in Section 7.2, and describe two case studies
in Section 7.3 and 7.4, respectively. We also provide more experi-
ment results in the extended version of this paper [14].
7.1 Experimental setting
We describe our datasets, performance metrics, parameter configu-
rations, and the choices of baselines.
Data. We consider four real-world datasets (whose summary sta-
tistics are presented in Table 1) from various domains where inter-
pretability is of primary importance.
 
484KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
Table 1: Summary statistics for the datasets used in the exper-
iments. We report the number of data records ùëÅ, the number
of attributes ùêΩ, the density in the feature space and the im-
balance ratio(√çùëÅ
ùëõ=11[ùë¶ùëõ=0])/(√çùëÅ
ùëõ=11[ùë¶ùëõ=1]).
Name ùëÅ ùêΩ Feature density Imbalance ratio
Compas 6 489 15 0.256 1.232
Mushrooms 8 124 117 0.188 1.074
Voting 435 48 0.333 1.589
Credit 690 566 0.019 1.248
‚Ä¢Compas dataset for two-year recidivism prediction [27].
‚Ä¢Mushrooms dataset for classification of mushrooms into the
categories poisonous and edible [3].
‚Ä¢Credit dataset for credit scoring [1].
‚Ä¢Voting dataset for classification of american voters as republi-
cans or democrats [2].
Baselines. We compare BBenum ,ApproxCount and BBsts
against three baselines, Na√Øve-BB-Enum, a naive variant of
BBenum , which does not use pruning and thereby mirrors the
theoretical worst-case behaviour of BBenum ,CP-sat, a constraint
programming solver, and IS, an importance sampler. Details of the
baselines are given in Appendix A.
Metrics. Since the main goal in our experimental evaluation is
to show that our methods efficiently and effectively explore the
Rashomon set of near-optimal rule sets, we report runtime (in
seconds) and the estimated Rashomon set size |R(U)|.
Parameters. For fixed value of ùúÜ, the choice of upper bound ùúÉ
affects the most the computational requirements of the proposed
algorithms. Hence, we focus on showing runtime and accuracy
of counts as a function of ùúÉ. Unless specified otherwise, we set
ùúÜ=0.1. This choice of ùúÜshifts the Rashomon set towards concise
rule sets prioritizing interpretability over performance. If instead
performance is of primary importance, a smaller value of ùúÜ(e.g.,
ùúÜ=0.01) is preferable. We vary the value of ùúÉin arithmetic pro-
gression. For instance, ùúÉ‚àà[0.5,0.7,0.9,1.1]in the Compas dataset.
We construct the universe of rules Uby considering the 50rules
capturing the most data records. In addition, in the extended ver-
sion of this paper [ 14], we investigate the performance of BBenum ,
ApproxCount andBBsts as a function of|U|(see Figure 4).
When comparing with the baselines, Na√Øve-BB-Enum, CP-sat
andIS, which do not scale well, we use only the 30rules captur-
ing the most data records and set ùúÉ=0.3,0.5,0.8,and0.8for the
Compas, Mushrooms, Voting andCredit datasets, respectively.
ForApproxCount , we fixùúñ=0.2andùõø=0.9since varying ùúñ
andùõødoes not significantly affect the accuracy of ApproxCount .
On the other hand, for BBsts , the parameters ‚ÑìandùúÖaffect accuracy
greatly. We consider ‚Ñì‚àà{2,4,8}andùúÖ‚àà{50,225,506,1138,5760}
and we average results over different values of ‚ÑìandùúÖ.
Computing environment and source code. Experiments are
executed on a machine with 2√ó10core Xeon E5 processor and
256 GB memory. The source code is available at https://github.com/
xiaohan2012/efficient-rashomon-rule-set.
7.2 Performance comparison
Comparison among the proposed algorithms. Figure 2 demon-
strates how ùúÉaffects runtime (top row) and accuracy in estimating|R(U)|(bottom row), on all datasets. Note that BBenum always
returns the correct value for |R(U)|.BBsts is the fastest algorithm,
although it can be rather inaccurate in estimating |R(U)|. Instead,
ApproxCount strikes the best balance between scalability and ac-
curacy. For large values of ùúÉboth BBsts andApproxCount are
drastically more scalable than BBenum , while for small values of ùúÉ,
BBenum is typically the preferred algorithm.
Comparison against the baselines. The runtime and estimated
|R(U)|for the proposed algorithms and the baselines are provided
in Table 2. CP-sat andNa√Øve-BB-Enum yield exact counts, but they
take remarkably longer time than the proposed methods, mean-
while ISdelivers estimates that are too inaccurate.
7.3 Case study: feature importance in Compas
We illustrate the application of the approximate sampling algorithm
for the task of feature importance analysis. For a feature ùëóin a
rule setùëÜ, we use model reliance [18] to measure the importance
of the feature for ùëÜ. To show the variation of feature importance
across the rule sets in the Rashomon set, we compute mcr‚àí(ùëó)and
mcr+(ùëó), the minimum and maximum model reliance values for
each feature ùëó. More details are provided in Appendix B.
We obtain the ground truth based on allmodels inR(U). We also
estimate mcr‚àíandmcr+using samples of 400rule sets extracted
fromR(U). In our experiment, we use the Compas dataset and
consider a Rashomon set of 2 003 rule sets. The sampling process
is repeated 48times and the mean is reported. Sample estimates
ofmcr‚àíandmcr+as well as the corresponding ground-truth are
shown in Figure 3. Sample estimates are close to the ground-truth,
suggesting that exhaustive enumeration of the Rashomon set may
not be needed to investigate feature importance. In Section 7.4 we
reach similar conclusions regarding a use case on fairness.
7.4 Case study: fairness in Compas
Fairness has emerged as a central topic in machine learning since
the influential work of [ 16]. Exploring the Rashomon set allows
to address fairness considerations, which may arise in tackling
classification tasks. The goal of this case study is two-fold. Focusing
on the Compas dataset, we first show that samples drawn from the
Rashomon set by the proposed algorithms yield reliable estimates
of popular fairness metrics. Second, we show that the samples can
be used to find a model satisfying specific fairness constraints.
Investigating fairness measures by sampling. Figure 4 shows
that samples of increasing size provide an increasingly accurate rep-
resentation of the distribution of fairness measures in the Rashomon
set. While there can be some variability in the estimates of the mini-
mum, the maximum, median, first and third quartiles are accurately
estimated even in the smallest sample. All details on the fairness
measures and experiment settings are given in Appendix C.
Finding accurate-yet-fair models by sampling. In Appendix C
we also demonstrate that the algorithms we propose can be used
to find an accurate model satisfying certain fairness constraints.
8 CONCLUSIONS
We study the problems of sampling from the Rashomon set of ac-
curate rule set models and computing the size of the Rashomon
 
485Efficient Exploration of the Rashomon Set of Rule-Set Models KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
bbEnum ApproxCount bbSTS
0.5 0.7 0.9 1.1
 Upper Bound 05001000150020002500 Runtime (s) 
0.6 0.9 1.2 1.5
 Upper Bound 050100150200250300350400 Runtime (s) 
1.0 1.2 1.4 1.6
 Upper Bound 0500100015002000 Runtime (s) 
1.0 1.2 1.4 1.6
 Upper Bound 02000400060008000100001200014000 Runtime (s) 
0.5 0.7 0.9 1.1
 Upper Bound 104105106107108 Counts 
0.6 0.9 1.2 1.5
 Upper Bound 102103104105106107108109 Counts 
1.0 1.2 1.4 1.6
 Upper Bound 105106107108109 Counts 
1.0 1.2 1.4 1.6
 Upper Bound 1051061071081091010 Counts 
Compas Mushrooms Voting Credit
Figure 2: Runtime (in seconds, top row) and estimated |R(U)|(in log scale, bottom row) against objective upper bound ùúÉ.
Table 2: Performance on small problem instances. We report NA if runtime exceeds 12 hours. A‚àóindicates exact counts.
Compas Mushrooms Voting Credit
Runtime (s) Count Runtime (s) Count Runtime (s) Count Runtime (s) Count
ApproxCount 0.007 21 0.026 15 0.027 364 1.558 2 810
BBenum 0.010‚àó21 0.006‚àó15 0.022‚àó364 0.068‚àó2 807
BBsts 0.039 21 0.006 16 0.044 289 0.765 2 465
Na√Øve-BB-Enum NA NA 30 381.500‚àó15 29 981.500‚àó364 31 161.300‚àó2 807
CP-sat 10.277‚àó21 26.478‚àó15 2.072‚àó364 18.789‚àó2 807
IS 11.128 0 13.348 2 17.445 701 13.919 3 641
MCR+(True) MCR+(Sample) MCR (True) MCR (Sample)
0.8 0.9 1.0 1.1
ValuePriors (>3)
Juvenile crimes
Juvenile misdemeanors
Age (21-22)
Juvenile felonies
Sex
Age (18-20)
Age (26-45)
Age (23-25)
Priors (2-3)
No priors
Misdemeanor
Age (>45)
Priors (1) 
Figure 3: Estimated feature importance against the ground-
truth in the Compas dataset. 95% confidence intervals are
shown as black lines.
set. Unlike in related work, we consider both exhaustive and non-
exhaustive enumeration. For the former, we propose an efficient
branch-and-bound algorithm, optimized with pruning and incre-
mental computation. For the latter, we devise two algorithms: one
based on the random partitioning of the solution space and another
based on subsampling partial solutions during the branch-and-
bound exploration of the search tree of rule sets.
Our work opens interesting questions for future research. For
example, (ùëñ) can we make ApproxCount even faster by exploiting
the parity constraint further? ( ùëñùëñ) Can we improve the accuracy
ofBBsts without sacrificing efficiency? ( ùëñùëñùëñ) Can we design algo-
rithms for non-exhaustive exploration of the (possibly continuous)
10% 20% 40% 100%
104
103
102
101
100101102
Measure ValueEqual OpportunityPredictive EqualityPredictive ParityStatistical ParityFigure 4: Distribution of four fairness measures in the entire
Rashomon set (100%) as well as in samples of increasing size
(10%, 20%and 40%). Theùë•-axis is on log scale.
Rashomon set for other classes of interpretable models? And finally
(ùëñùë£) can we showcase algorithms for non-exhaustive exploration of
the Rashomon set in unexplored application scenarios?
9 ACKNOWLEDGEMENTS
This research is supported by the ERC Advanced Grant REBOUND
(834862), the EC H2020 RIA project SoBigData++ (871042), and
the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation.
 
486KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
REFERENCES
[1][n. d.]. Credit Score Classification task. http://kaggle.com/datasets/parisrohan/
credit-score-classification. Accessed: 2023-10-01.
[2]1987. Congressional Voting Records. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5C01P.
[3]1987. Mushroom. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5959T.
[4]Ulrich A√Øvodji, Julien Ferry, S√©bastien Gambs, Marie-Jos√© Huguet, and Mohamed
Siala. 2021. Faircorels, an open-source library for learning fair rule lists. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management. 4665‚Äì4669.
[5]Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
Rudin. 2017. Learning certifiably optimal rule lists. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
35‚Äì44.
[6]Vaishak Belle and Ioannis Papantonis. 2021. Principles and practice of explainable
machine learning. Frontiers in big Data (2021), 39.
[7]Leo Breiman. 2001. Statistical modeling: The two cultures (with comments and a
rejoinder by the author). Statistical science 16, 3 (2001), 199‚Äì231.
[8]Nadia Burkart and Marco F Huber. 2021. A survey on the explainability of
supervised machine learning. Journal of Artificial Intelligence Research 70 (2021),
245‚Äì317.
[9]Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. 2009. Building classifiers
with independency constraints. In 2009 IEEE international conference on data
mining workshops. IEEE, 13‚Äì18.
[10] Fr√©d√©ric C√©rou, Pierre Del Moral, Teddy Furon, and Arnaud Guyader. 2012. Se-
quential Monte Carlo for rare event estimation. Statistics and computing 22, 3
(2012), 795‚Äì808.
[11] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. 2013. A scalable
and nearly uniform generator of SAT witnesses. In Computer Aided Verification:
25th International Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.
Proceedings 25. Springer, 608‚Äì623.
[12] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. 2014. Balancing
scalability and uniformity in SAT witness generator. In Proceedings of the 51st
Annual Design Automation Conference. 1‚Äì6.
[13] M. Ciaperoni, H. Xiao, and A. Gionis. 2022. Concise and interpretable multi-label
rule sets. In 2022 IEEE International Conference on Data Mining (ICDM). IEEE Com-
puter Society, Los Alamitos, CA, USA, 71‚Äì80. https://doi.ieeecomputersociety.
org/10.1109/ICDM54844.2022.00017
[14] Martino Ciaperoni, Han Xiao, and Aristides Gionis. 2024. Efficient Exploration
of the Rashomon Set of Rule Set Models. arXiv:2406.03059 [cs.LG]
[15] Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. 2021. Char-
acterizing fairness over the set of good models under selective labels. In Interna-
tional Conference on Machine Learning. PMLR, 2144‚Äì2155.
[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214‚Äì226.
[17] Stefano Ermon, Carla Pedro Gomes, and Bart Selman. 2012. Uniform Solution
Sampling Using a Constraint Solver As an Oracle. In Conference on Uncertainty
in Artificial Intelligence. https://api.semanticscholar.org/CorpusID:16218653
[18] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are
Wrong, but Many are Useful: Learning a Variable‚Äôs Importance by Studying an
Entire Class of Prediction Models Simultaneously. J. Mach. Learn. Res. 20, 177
(2019), 1‚Äì81.
[19] Aristides Gionis, Theodoros Lappas, and Evimaria Terzi. 2012. Estimating entity
importance via counting set covers. In Proceedings of the 18th ACM SIGKDD
international conference on Knowledge discovery and data mining. 687‚Äì695.
[20] Jiawei Han, Jian Pei, and Yiwen Yin. 2000. Mining frequent patterns without
candidate generation. ACM sigmod record 29, 2 (2000), 1‚Äì12.
[21] Satoshi Hara and Masakazu Ishihata. 2018. Approximate and exact enumeration
of rule models. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 32.
[22] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. 2019. Optimal sparse decision
trees. Advances in Neural Information Processing Systems 32 (2019).
[23] Katarzyna Kobyli≈Ñska, Mateusz Krzyzi≈Ñski, Rafa≈Ç Machowicz, Mariusz Adamek,
and Przemys≈Çaw Biecek. 2023. Exploration of Rashomon Set Assists Explanations
for Medical Data. arXiv preprint arXiv:2308.11446 (2023).
[24] Trupti A Kumbhare and Santosh V Chobe. 2014. An overview of association rule
mining algorithms. International Journal of Computer Science and Information
Technologies 5, 1 (2014), 927‚Äì930.
[25] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable
decision sets: A joint framework for description and prediction. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining. 1675‚Äì1684.
[26] Himabindu Lakkaraju and Osbert Bastani. 2020. " How do I fool you?" Manipu-
lating User Trust via Misleading Black Box Explanations. In Proceedings of the
AAAI/ACM Conference on AI, Ethics, and Society. 79‚Äì85.[27] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. How we
analyzed the COMPAS recidivism algorithm. ProPublica (5 2016) 9, 1 (2016), 3‚Äì3.
[28] Dmitry Malioutov and Kuldeep S Meel. 2018. MLIC: A MaxSAT-based framework
for learning interpretable classification rules. In International Conference on
Principles and Practice of Constraint Programming. Springer, 312‚Äì327.
[29] Charles Marx, Flavio Calmon, and Berk Ustun. 2020. Predictive multiplicity in
classification. In International Conference on Machine Learning. PMLR, 6765‚Äì6774.
[30] Kota Mata, Kentaro Kanamori, and Hiroki Arimura. 2022. Computing the Collec-
tion of Good Models for Rule Lists. arXiv preprint arXiv:2204.11285 (2022).
[31] Kuldeep Singh Meel. 2017. Constrained counting and sampling: bridging the gap
between theory and practice. Ph. D. Dissertation. Rice University.
[32] Laurent Perron and Fr√©d√©ric Didier. [n. d.]. CP-SAT. Google. https://developers.
google.com/optimization/cp/cp_solver/
[33] Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature machine
intelligence 1, 5 (2019), 206‚Äì215.
[34] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and
10 grand challenges. Statistic Surveys 16 (2022), 1‚Äì85.
[35] Mirka Saarela and Susanne Jauhiainen. 2021. Comparison of feature importance
measures as explanations for classification models. SN Applied Sciences 3 (2021),
1‚Äì12.
[36] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2019. A study in Rashomon
curves and volumes: A new perspective on generalization and model simplicity
in machine learning. arXiv preprint arXiv:1908.01755 (2019).
[37] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2022. On the existence of
simpler machine learning models. In Proceedings of the 2022 ACM Conference on
Fairness, Accountability, and Transparency. 1827‚Äì1858.
[38] Surya T Tokdar and Robert E Kass. 2010. Importance sampling: a review. Wiley
Interdisciplinary Reviews: Computational Statistics 2, 1 (2010), 54‚Äì60.
[39] Leslie G Valiant. 1979. The complexity of enumeration and reliability problems.
siam Journal on Computing 8, 3 (1979), 410‚Äì421.
[40] Srishti Vashishtha and Seba Susan. 2019. Fuzzy rule based unsupervised sentiment
analysis from social media posts. Expert Systems with Applications 138 (2019),
112834.
[41] Tong Wang and Cynthia Rudin. 2015. Learning optimized Or‚Äôs of And‚Äôs. arXiv
preprint arXiv:1511.02210 (2015).
[42] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and
Perry MacNeille. 2017. A bayesian framework for learning rule sets for inter-
pretable classification. The Journal of Machine Learning Research 18, 1 (2017),
2357‚Äì2393.
[43] Rui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, and Cynthia
Rudin. 2022. Exploring the whole rashomon set of sparse decision trees. Advances
in Neural Information Processing Systems 35 (2022), 14071‚Äì14084.
[44] Guangyi Zhang and Aristides Gionis. 2020. Diverse rule sets. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 1532‚Äì1541.
[45] Guangyi Zhang and Aristides Gionis. 2023. Regularized impurity reduction:
accurate decision trees with complexity guarantees. Data mining and knowledge
discovery 37, 1 (2023), 434‚Äì475.
Appendices
A ADDITIONAL BASELINES
In order to offer a more complete assessment of the performance
of the methods we propose, we compare BBenum ,BBsts and
ApproxCount against three simpler alternative approaches. The
details of such simple baselines are given next.
‚Ä¢Na√Øve-BB-Enum: a na√Øve search algorithm that does not
enforce any pruning of the search space. The Na√Øve-BB-
Enum algorithm is analogous to BBenum , but it exhaustively
considers all rule sets and tests them for inclusion into the
Rashomon set.
‚Ä¢CP-sat: a constraint programming solver that uses a highly
optimized SAT (satisfiability) solver. In order to leverage the
solver, we encode the problem as follows.
A data record(ùíôùëõ,ùë¶ùëõ)is said to be positive if ùë¶ùëõ=1and
negative otherwise. The numbers of negative and positive
data records are denoted by |D|‚àíand|D|+, respectively.
 
487Efficient Exploration of the Rashomon Set of Rule-Set Models KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
GivenùëÄinput rules, let Nùêºbe an indicator matrix such that
Nùêºùëñ,ùëó=1if theùëñ-th negative data record is covered by the
ùëó-th rule. Similarly, let Pùêºbe an indicator matrix such that
Pùêºùëñ,ùëó=1if theùëñ-th positive data record is covered by the
ùëó-th rule. We encode the counting problem as the problem
of finding all x‚àà{0,1}ùëÄ, such that:
ùëßùêπùëÉ
ùëÅ+ùëßùêπùëÅ
ùëÅ+ùúÜùëÄ‚àëÔ∏Å
ùëó=1x[ùëó]‚â§ùúÉ, (11)
whereùëßùêπùëÉ=√ç|D|‚àí
ùëñ‚àà1min(Nùêºùëñx,1)andùëßùêπùëÅ=√ç|D|+
ùëñ‚àà1max(1‚àí
Pùêºùëñx,0).Here, x[ùëó]is theùëó-th entry of x,Nùêºùëñxdenotes the
dot product between the ùëñ-th row of Nùêºand the vector x.
Similarly, Pùêºùëñxdenotes the dot product between the ùëñ-th row
ofPùêºandx. AsNùêºùëñ,Pùêºùëñandxare all binary vectors, the dot
product corresponds to a set intersection.
Given the set of constraints described in Equations 11, we
find all rule sets by resorting to a state-of-the-art solver for
constraint programming [32].
‚Ä¢IS: a method based on Monte Carlo simulation, where we
simulate a large number of rule sets and evaluate the pro-
portion of rule sets that belong to the Rashomon set. The
proportion can then be mapped to the corresponding count
by multiplying by the total number of rule-set models, that
can be easily computed. Plain Monte Carlo sampling is ex-
tremely inefficient for very rare events [ 10]. As suggested
by Semenova et al . [36] , in order to estimate the size of the
Rashomon set, it is preferable to use the Monte Carlo method
known as importance sampling [ 38], where the training data
are used to bias the sampling towards the Rashomon set. In
particular, the importance sampler is designed as follows.
‚ÄìGiven the set of pre-mined rules U, compute the normal-
ized reciprocal individual contribution of each rule ùëüto
the loss, namely Œî‚Ñì(ùëü)=1
‚Ñìùëù(ùëü)+‚Ñì0(ùëü)/√ç
ùëü‚Ä≤‚ààUŒî‚Ñì(ùëü‚Ä≤).
‚ÄìSampleùëÅùëüùëíùëù(1,000,000by default) integers ùë°uniformly
at random in the interval [1,|U|] and rule sets ùëÜof sizeùë°
with probability ùëù(ùëÜ)=Œî‚Ñì‚Ä≤(ùëü1)Œî‚Ñì‚Ä≤(ùëü2)...Œî‚Ñì‚Ä≤(ùëüùëõ).
‚ÄìCompute the importance sampling estimate
1
ùëÅ√çùëÅ
ùëñ=1ùëìùêº(ùëÜ)ùë¢(ùëÜ)
ùëù(ùëÜ)whereùëìùêº(ùëÜ)is an indicator func-
tion for the event that ùëÜbelongs to the Rashomon set, and
ùë¢(¬∑)is the uniform (target) distribution.
In practice, to enhance the performance of IS, instead of
sampling rule sets of length up to |U|, we sample rule sets
of length up to the upper bound obtained by setting ùëÜ=‚àÖ
in Theorem 3.
B CASE STUDY ON FEATURE IMPORTANCE
As a simple case study, we show how the proposed methods allow
to efficiently estimate feature importance and, more specifically, we
show that reliable estimates of feature importance can be derived
from samples of rule sets in the Rashomon set. The main results are
summarized in Section 7.3. In this section, we provide more details.
Different measures of feature importance have been pro-
posed [ 35]. Recent work focuses on model reliance [18,43]. Model
reliance captures the extent to which a model relies on a given
feature to achieve its predictive performance. For our purposes,given rule set ùëÜand feature ùë£, we define model reliance as follows:
ùëÄùëÖ(ùëÜ,ùë£)=ùëì(ùëÜ;ùë£‚Ä≤,ùúÜ)
ùëì(ùëÜ;ùë£,ùúÜ), (12)
whereùëì(ùëÜ;ùë£,ùúÜ)is the objective achieved by ùëÜin the original
dataset, and ùëì(ùëÜ;ùë£‚Ä≤,ùúÜ)is identical to ùëì(ùëÜ;ùë£,ùúÜ)except that ùë£is re-
placed by its uninformative counterpart ùë£‚Ä≤. Featureùë£‚Ä≤is obtained
by swapping the first and second halves of the feature values of ùë£,
thereby retaining the marginal distribution of ùë£, while destroying
its predictive power. This measure is similar to the model reliance
measure used by Xin et al . [43] . Model reliance evaluates how im-
portant a variable is for a given rule set. In particular, the higher
model reliance, the more important feature ùë£. If we have a single
rule setùëÜ, we would simply estimate the importance of feature ùë£
byùëÄùëÖ(ùëÜ,ùë£). However, if we have access to the Rashomon set of
all near-optimal rule sets, it is more informative to investigate the
variation of ùëÄùëÖ(ùëÜ,ùë£)across rule sets ùëÜin the Rashomon set. Hence,
we compute ùëÄùê∂ùëÖ‚àí(ùë£)andùëÄùê∂ùëÖ+(ùë£), the minimum and maximum
model reliance for feature ùë£across rule sets in the Rashomon set.
In Figure 3 (Section 7.3), we compare ùëÄùê∂ùëÖ‚àí(ùë£)andùëÄùê∂ùëÖ+(ùë£)
computed in the entire Rashomon set and in samples of rule sets
drawn from the Rashomon set by ApproxSample , and we conclude
that the sample estimates are consistently close to the measures
computed in the entire Rashomon set, suggesting that exhaustive
enumeration may be redundant when the goal is to investigate
feature importance.
In addition, while ùëÄùê∂ùëÖ‚àí(ùë£)andùëÄùê∂ùëÖ+(ùë£)are adequate mea-
sures of the importance of features in rule sets, they fail to capture
the idea that some features are more frequent than others in the
Rashomon set. Inuitively, at parity model reliance, the more fre-
quent a feature is in the Rashomon set, the more important. Hence,
to provide a more complete assessment of feature importance, Fig-
ure 5 shows the proportion of rule sets including a given variable
in the entire Rashomon set or in a sample of 400rule sets obtained
using ApproxSample . The reported sample estimates are obtained
as averages over 10repetitions of the sampling process. The relative
frequency of the features estimated in the sample and in the entire
Rashomon set are remarkably similar, corroborating the findings
presented in Section 7.3 with respect to model reliance.
Finally, we mention that the results do not correspond exactly to
the similar results presented by Xin et al . [43] because we consider
a different class of models and a different loss. However, there are
interesting commonalities. For instance, the variable ùëÉùëüùëñùëúùëü >3has
the highest ùëÄùê∂ùëÖ+(ùë£)in both studies.
C CASE STUDY ON FAIRNESS
The Rashomon set offers a novel perspective on fairness of machine
learning models. Although all models in the Rashomon set achieve
near-optimal predictive performance, they may exhibit different
fairness characteristics. The Rashomon set allows to identify the
range of predictive bias produced by the models and to search for
models that are both accurate and fair.
We carry out a case study focusing on the Compas dataset, which
has fueled intense debate and research in fair machine learning [ 4,
34], and fairness constraints are specified with respect to the sex
attribute, which partitions the dataset into two groups, males (M)
andfemales (F).
 
488KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
0.0 0.1 0.2
Relative FrequencyAge (18-20)Priors (1)Age (21-22)No priorsAge (>45)Juvenile feloniesPriors (2-3)Age (23-25)Juvenile misdemeanorsMisdemeanorAge (26-45)Juvenile crimesPriors (>3)Sex
0.0 0.1 0.2
Sample Relative FrequencyAge (18-20)Priors (1)Age (21-22)No priorsAge (>45)Juvenile feloniesPriors (2-3)Age (23-25)Juvenile misdemeanorsAge (26-45)MisdemeanorPriors (>3)Juvenile crimesSex
Figure 5: Relative frequencies of features in the Rashomon set (left) and associated sample estimates (right).
10% 20% 40% 100%
0.0 0.2 0.4 0.6
T est Set LossEqual OpportunityPredictive EqualityPredictive ParityStatistical Parity
Figure 6: Objective ùëìon the test set obtained by the optimal
fair rule set found in the train set with respect to different
fairness measures in the entire Rashomon set (100%) as well
as in samples of increasing size (10%, 20%and 40%).
A concise summary of the case study is given in Section 7.4.
Here, we discuss the details.
First, we introduce the considered measures of fairness.
Fairness measures. LetÀÜùë¶=1denote the event that the data
record ùíôis predicted as positive (i.e. cap(ùíô,ùëÜ)). Moreover, let ùë•ùë†
denote the sex of data record ùíô. We consider the following fairness
measures [4].
‚Ä¢Statistical parity measures the absolute difference of rate of
positive predictions between the groups:
|Pr(ÀÜùë¶=1|ùë•ùë†=F)‚àíPr(ÀÜùë¶=1|ùë•ùë†=M)|.
‚Ä¢Predictive parity measures the absolute difference of preci-
sion between the groups:
|Pr(ùë¶=1|ÀÜùë¶=1,ùë•ùë†=F)‚àíPr(ùë¶=1|ÀÜùë¶=1,ùë•ùë†=M)|.
‚Ä¢Predictive equality measures the absolute difference of false
positive rate between the groups:
|Pr(ÀÜùë¶=1|ùë¶=0,ùë•ùë†=F)‚àíPr(ÀÜùë¶=1|ùë¶=0,ùë•ùë†=M)|.
‚Ä¢Equal opportunity measures the absolute difference of true
positive rate between the groups:
|Pr(ÀÜùë¶=1|ùë¶=1,ùë•ùë†=F)‚àíPr(ÀÜùë¶=1|ùë¶=1,ùë•ùë†=M)|.
For all four measures, the larger the value, the more unfair the
model is.Investigating fairness measures by sampling. Figure 4 in Sec-
tion 7.4 shows the distribution of the above fairness measures in
the entire Rashomon set and in samples of increasing size. For
each measure, we show the range (minimum and maximum), the
interquartile range (first and third quartiles) and the median. All
such statistics describing the distributions of the fairness measures
of interest in the samples of rule sets are obtained as average over
10repetitions of the random sampling process.
The Rashomon set consists of |R(U)|=1409 rule sets and we
useApproxSample to draw samples of sizes 10%, 20%and40%of
|R(U)|.
Finding accurate-yet-fair models by sampling. To demonstrate
that the proposed sampling strategy can be used to find an accurate
model while satisfying particular fairness constraints, we set up a
simple two-step experiment.
First, given a sample of rule sets from the Rashomon set, we
consider any of the four fairness measures described above, say
ùëÄùëìùëéùëñùëü, and we exclude all models with value of ùëÄùëìùëéùëñùëübeyond the
first quartile of the distribution of ùëÄùëìùëéùëñùëüin the entire Rashomon
set.
The remaining models are referred to as fair models (with respect
to the chosen ùëÄùëìùëéùëñùëü). Second, among the remaining (fair) models,
we pick the model ùëÜ‚àówhich minimizes the objective ùëì.
Figure 6 reports the value of ùëìin the test set for the chosen rule
setùëÜ‚àóin the entire Rashomon set and in samples of rule sets of
increasing size. Again, the Rashomon set consists of |R(U)|=1,409
rule sets and we draw samples of sizes 10%, 20%and40%of|R(U)|
using ApproxSample . The reported losses are obtained as average
over 10repetitions of the sampling process. The performance of the
optimal fairrule set chosen from the samples is not far from the
performance of the optimal fairrule set chosen in the entire R(U),
and the gap between the performance of the optimal fairrule set
in the entire Rashomon set and in samples drawn from it quickly
shrinks as the sample size increases. In the case of statistical parity,
no significant difference is observed across different sample sizes,
suggesting that even the smallest sample is enough to find a rule set
which is fair with respect to statistical parity and exhibits predictive
performance indistinguishable from the predictive performance of
the fair rule set that would be chosen in the entire Rashomon set.
Thus, in view of the results reported in this section, we conclude
that exhaustive enumeration of the Rashomon set may be redundant
when the goal is to investigate fairness or find a model that is both
accurate and fair. A representative sample may suffice.
 
489