Online Drift Detection with Maximum Concept Discrepancy
Ke Wan∗
University of Illinois at
Urbana-Champaign
Illinois, USA
kewan2@illinois.eduYi Liang∗
Fudan University
Shanghai, China
yliang23@m.fudan.edu.cnSusik Yoon§
Korea University
Seoul, Korea
susik@korea.ac.kr
ABSTRACT
Continuous learning from an immense volume of data streams
becomes exceptionally critical in the internet era. However, data
streams often do not conform to the same distribution over time,
leading to a phenomenon called concept drift. Since a fixed static
model is unreliable for inferring concept-drifted data streams, es-
tablishing an adaptive mechanism for detecting concept drift is
crucial. Current methods for concept drift detection primarily as-
sume that the labels or error rates of downstream models are given
and/or underlying statistical properties exist in data streams. These
approaches, however, struggle to address high-dimensional data
streams with intricate irregular distribution shifts, which are more
prevalent in real-world scenarios. In this paper, we propose MCD-
DD, a novel concept drift detection method based on maximum
concept discrepancy, inspired by the maximum mean discrepancy.
Our method can adaptively identify varying forms of concept drift
by contrastive learning of concept embeddings without relying on
labels or statistical properties. With thorough experiments under
synthetic and real-world scenarios, we demonstrate that the pro-
posed method outperforms existing baselines in identifying concept
drifts and enables qualitative analysis with high explainability.
CCS CONCEPTS
•Information systems →Data stream mining.
KEYWORDS
Concept Drift Detection; Maximum Concept Discrepancy
ACM Reference Format:
Ke Wan∗, Yi Liang∗, and Susik Yoon§. 2024. Online Drift Detection with
Maximum Concept Discrepancy. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3672016
1 INTRODUCTION
1.1 Background and Motivation
Continuously learning from evolving data streams is crucial for
numerous online services to derive real-time insights [ 5,50,51].
However, in many real-world scenarios, data streams from different
*Equal contributions.§Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672016times may exhibit distinct characteristics [ 25,41,49]. For instance, a
previously stable weather pattern might incrementally change due
to global warming with unprecedented high temperatures, leading
to unpredictable fluctuations in temperature, wind speed, and hu-
midity. This phenomenon is called concept drift in data streams [ 30],
indicating that data at different times follows distinct probabil-
ity distributions. Developing methods for continuously detecting
whether a data stream has undergone concept drift is imperative,
since it is impractical to employ consistent modeling to the concept-
drifted data streams (e.g., a weather prediction model needs to be
updated after unprecedented temperature changes are observed).
Current methods for detecting concept drift online fall into two
categories: error rate-based or data distribution-based [ 2,13,39]. A
common tactic involves constructing a hypothesis test statistic to de-
termine whether error rates of downstream models or data samples
from different periods adhere to the same probability distribution
under a certain significance level. While this approach is favored
for its interpretability and strong statistical foundation, distinguish-
ing between natural fluctuations and actual drifts poses challenges,
especially in the context of complex, evolving data streams. The
sparsity, noise, and high dimensionality commonly observed in
real-world data streams can make statistical approaches ineffective.
Additionally, obtaining error rates of downstream models is not
always feasible, as true labels may not be readily available.
Meanwhile, in machine learning, kernel methods are commonly
used to map data into high-dimensional spaces [ 11], improving
its representation for downstream tasks such as classification and
clustering with more distinct separations in the projected space.
Likewise, kernel methods can be used to transform a set of sampled
data into a space where the existing concepts can be effectively
represented, facilitating the detection of potential concept drifts.
However, traditional kernels like the Gaussian kernel [20] are lim-
ited in their ability to detect concept drifts. They are designed with
a deterministic mapping function, making them ill-suited for the
ever-changing distributions of data streams. While deep kernels of-
fer more flexibility [ 29,47], adapting them to address concurrently
evolving concepts, especially in unsupervised settings, remains
a significant challenge. The computational costs associated with
updating these kernels repeatedly can be prohibitive.
1.2 Main Idea and Challenges
Detecting concept drifts from data streams presents numerous chal-
lenges, mainly centered around the representation of ever-changing
data distributions (i.e., concept representation) and the measure-
ment of their differences (i.e., drift quantification). It also necessi-
tates the continuous monitoring of the dynamic shifts in data distri-
butions as they evolve, which is crucial for accurately identifying
 
2924
KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
arbitrary drifts (i.e., online updates). Furthermore, in real-world sce-
narios, there is often a lack of ground truth labels for concept drifts
as well as downstream tasks, making an unsupervised approach
(i.e., data distribution-based) preferable to a supervised approach
(i.e., error rate-based) in practice. To address these objectives, we
propose a novel method for continuously identifying concept drifts
in an unsupervised andonline manner, that can effectively handle
arbitrary data distributions with high interpretability.
The main idea of this work is to employ a new measure Max-
imum Concept Discrepancy for concept drift detection, inspired
by the maximum mean discrepancy [ 38] with a kernel function.
Through a deep neural network, we encode a set of sample data
points in a short time period into a compact representation that
captures the concept observed during the period. We leverage con-
trastive learning accompanied by time-aware sampling strategies to
learn the embedding space of concepts. This entails the generation
of positive sample pairs drawn from temporally proximate distri-
butions and that of negative sample pairs from temporally distant
distributions, while also introducing controlled perturbations. The
embedding space is continuously updated to bring positive samples
closer together and push negative samples further apart. Concept
drifts are then identified by evaluating the discrepancy between
the representations of concepts in consecutive time periods. In ad-
dition, the maximum concept discrepancy between two concepts
can be bounded with a statistical significance. It can function as a
theoretical threshold for detecting concept drifts, providing high
interpretability and practicality for our method. In consequence,
our method is capable of continuously identifying various types of
concept drift from data streams without any supervision. It effec-
tively addresses the aforementioned challenges for concept drift
detection while keeping the advantages of both online statistical
approaches and offline deep kernel-based approaches.
1.3 Summary
As a concrete implementation of our main idea, we propose an
algorithm MCD-DD (Maximum Concept Discrepancy-based Drift
Detector), aiming at unsupervised online concept drift detection
from data streams. The main contributions of this work can be
summarized as follows:
•To the best of our knowledge, this is the first work to propose
a dynamically updated measure, maximum concept discrepancy,
for unsupervised online concept drift detection.
•We propose a novel method MCD-DD equipped with the sample
set encoder and drift detector, optimized by contrastive objective
with time-aware sampling strategies. For reproducibility, the
source code of MCD-DD is publicly available1.
•Theoretical analysis of learning the maximum concept discrep-
ancy provides its statistical interpretation and complexity.
•Comprehensive experiments are conducted on 11 data sets with
varying complexities of drifts. MCD-DD achieves state-of-the-
art results in three performance metrics and demonstrates better
interpretability in qualitative analysis, compared with baselines.
1https://github.com/LiangYiAnita/mcd-dd2 RELATED WORK
2.1 Concept Drift Detection
Concept drift detection is essential in employing a model robustly
in data streams [ 1,2,15,30]. Error rate-based drift detection is
the most commonly used supervised method for detecting concept
drift [ 4,12,28,36,48]. This approach continuously monitors the
performance of downstream models in data streams. It relies on a
trained predictive model and assesses whether concept drift has
occurred by examining the consistency of the model’s predictive
performance over different time intervals [ 2]. For an unsupervised
approach [ 16], it is common to conduct statistical tests on the two
samples from different periods to determine whether they originate
from the same concept [ 6,19,24,29,34], called data distribution-
based detection, which is the scope of this work. While some error
rate-based detectors [ 4,33] can be adopted for this setting, it is not
straightforward to apply them to multivariate data streams. It is
also worth noting that some recent works try variants for concept
drift detection with a pre-trained model [ 7,54], active learning [ 55],
imbalanced [26] or resource-constrained [44] streaming settings.
2.2 Contrastive Learning in Data Streams
Contrastive learning, as an effective self-supervised learning par-
adigm [ 8], is widely applied in various detection tasks in data
streams [ 44,46,53]. The nature of data streams with scarce or de-
layed labels and lack of external supervision leads to the adoption of
continual learning with contrastive losses. The pseudo-labeling for
preparing positive and negative samples is a critical design factor
and its strategy ranges from model confidence-based [ 53], learn-
able focuses [ 46], to class prototype [ 44] tailored for downstream
tasks. Despite the advancements in contrastive learning, current
techniques have yet to be explored for learning separable embed-
dings of probability distributions representing varying concepts or
for application in two-sample tests with statistical bounds, both of
which are addressed in this study.
2.3 Maximun Mean Discrepancy
The utilization of Maximum Mean Discrepancy (MMD) has been
widespread, primarily serving to map data into high-dimensional
spaces and thereby enhancing separability for downstream tasks [ 23].
MMD has been also actively applied for designing generative mod-
els [10,27] and detecting whether two samples originate from the
same distribution [ 18] with the Gaussian kernel function [ 19] or the
deep kernels [ 29] to achieve greater flexibility and expressiveness.
The idea of Maximum Concept Discrepancy (MCD) in this study
draws inspiration from MMD-based approaches but is specifically
tailored for unsupervised online concept drift detection by integrat-
ing a deep encoder for sample sets to represent data distributions
and continuous learning strategies to dynamically optimize the
projected space encompassing varying concepts.
3 PRELIMINARIES
3.1 Concept Drift
Concept drift is a phenomenon referring to the arbitrary changes
in the statistical properties of a target domain of data over time.
Formally, concept drift at time 𝑡is defined as the change in the
 
2925Online Drift Detection with Maximum Concept Discrepancy KDD ’24, August 25–29, 2024, Barcelona, Spain
𝐍𝐍𝐞𝐞𝐞𝐞 Data
Drifted ?Data stream 𝒙𝒙𝟎𝟎…
𝕎𝕎𝒕𝒕𝒙𝒙𝒕𝒕+𝒔𝒔 𝒙𝒙𝟏𝟏 𝒙𝒙𝒕𝒕 𝒙𝒙𝟐𝟐
𝕎𝕎𝒕𝒕+𝑺𝑺𝕊𝕊𝒕𝒕𝑵𝑵𝒔𝒔𝒔𝒔𝒔𝒔−𝟏𝟏…𝕊𝕊𝒕𝒕𝟏𝟏𝕊𝕊𝒕𝒕𝑵𝑵𝒔𝒔𝒔𝒔𝒔𝒔
𝕊𝕊𝒕𝒕+𝒔𝒔𝑵𝑵𝒔𝒔𝒔𝒔𝒔𝒔−𝟏𝟏…𝕊𝕊𝒕𝒕+𝒔𝒔𝟏𝟏𝕊𝕊𝒕𝒕+𝒔𝒔𝑵𝑵𝒔𝒔𝒔𝒔𝒔𝒔…
Figure 1: Unsupervised online concept drift detection over
sliding window Wwith sub-windows S.
joint probability of data points 𝑋and labels𝑦at time𝑡, denoted
as𝑃𝑡(𝑋,𝑦)≠𝑃𝑡+1(𝑋,𝑦). Concept drift primarily originates from
one of the following three sources [ 30]: (i)𝑃𝑡(𝑌|𝑋)≠𝑃𝑡+1(𝑌|𝑋),
when the conditional distribution of the target variable 𝑌given
the covariate 𝑋undergoes drift; (ii) 𝑃𝑡(𝑋)≠𝑃𝑡+1(𝑋), when the
distribution of the covariate experiences drift; and (iii) combination
of (i) and (ii). In addition to varying sources, concept drift can also
be distinguished into four types based on the specific nature of the
drift occurrence: sudden, reoccurring, gradual, and incremental. For
additional references, we direct readers to recent surveys [ 1,2,30].
This work aims to develop an unsupervised method for detecting
various types of drift caused by the source described in (ii).
3.2 Problem Setting
Given a continuously evolving data stream X={𝑥𝑡}∞
𝑡=0, we main-
tain the latest context of the data stream by employing a sliding win-
dowW𝑡of size𝑊updated by a slide of size 𝑆(i.e.,W𝑡={𝑥𝑡−𝑖}𝑊−1
𝑖=0
andS𝑡={𝑥𝑡−𝑖}𝑆−1
𝑖=0). The window and slide sizes can be defined
either in terms of the number of data points or a time period.
Then, a window W𝑡consists of non-overlapping slides indexed
by𝑗=1,...,𝑁𝑠𝑢𝑏where𝑁𝑠𝑢𝑏=𝑊/𝑆is the number of slides in
a window (i.e., W𝑡=Ð𝑁𝑠𝑢𝑏
𝑗=1S𝑗
𝑡andS𝑗
𝑡={𝑥𝑡−(𝑁𝑠𝑢𝑏−𝑗)∗𝑆−𝑖}𝑆−1
𝑖=0).
In the rest of the paper, we use the term sub-window instead of
slide for consistency. It is worth noting that the context within a
sub-window is set to be sufficiently compact to ensure that the data
points it contains adhere to the same underlying distribution.
For every sliding window in X, the problem of unsupervised
online concept drift detection is to identify whether the concept
drift has occurred in a new sub-window S𝑁𝑠𝑢𝑏
𝑡compared with the
existing data points in the current window W𝑡\S𝑁𝑠𝑢𝑏
𝑡, without
using any labels for drifts and downstream tasks (see Figure 1).
3.3 Maximum Mean Discrepancy
Maximum Mean Discrepancy (MMD) [ 38] is a statistical measure
that compares two probability distributions through a kernel, espe-
cially when the distributions are unknown and only samples are
available. MMD evaluates the distance between the mean embed-
dings of two distributions in the Reproducing Kernel Hilbert Space
(RKHS)H[3]. Given𝑋∼𝑃,𝑌∼𝑄, and a kernel 𝑓(·), we have:
MMD(𝑃,𝑄)= sup
𝑓∈H,∥𝑓∥H≤1∥E[𝑓(𝑋)]−E[𝑓(𝑌)]∥2. (1)
When we have samples {𝑋𝑖}𝑛
𝑖=1from probability distribution 𝑃and
{𝑌𝑖}𝑛
𝑖=1from probability distribution 𝑄, the empirical Maximum
Mean Discrepancy (MMD) can be written as:
MMD(𝑃,𝑄)= sup
𝑓∈H,∥𝑓∥H≤1∥1
𝑛𝑛∑︁
𝑖=1𝑓(𝑋𝑖)−1
𝑛𝑛∑︁
𝑖=1𝑓(𝑌𝑖)]∥2.(2)Algorithm 1 Overall Procedure of MCD-DD
Input: Data streamX={𝑥𝑡}∞
𝑡=0, a sample set encoder set-enc()
with an encoding function 𝑓(·).
1:Initialize the parameter 𝜃of𝑓(·)and the MCD threshold 𝜎
2:forevery sliding window W𝑡inXdo
3: /*1. Drift Detection */
4: Obtain sample sets 𝑀𝑗forS𝑗
𝑡⊂W𝑡
5: Obtain concept representations {ℎ𝑗=set-enc(𝑀𝑗)}𝑁𝑠𝑢𝑏
𝑗=1
6: Report drifted if𝑀𝐶𝐷(𝑃𝑡(S𝑁𝑠𝑢𝑏
𝑡),𝑃𝑡(S𝑁𝑠𝑢𝑏−1
𝑡))>𝜎
7: /*2. Encoder Update */
8: Obatin positive, weak/strong negative samples
9: Calculate the lossLby Eq. (14)
10: Update𝜃byL
11: Update𝜎from the positive samples
12:end for
4 PROPOSED METHOD
4.1 Overview
The proposed method MCD-DD exploits maximum concept discrep-
ancy for detecting concept drift, enabled by a deep encoder for
data distribution embedding and contrastive learning for effective
unsupervised training. The overall procedure of MCD-DD is illus-
trated in Figure 2 and outlined in Algorithm 1. For each new sliding
window, MCD-DD follows the prequential evaluation scheme (i.e.,
test-and-train) [ 14]. First, MCD-DD samples sets of data points
in sub-windows and embeds them through a sample set encoder.
The discrepancy between sample sets from adjacent sub-windows
is calculated, and if it exceeds a threshold, MCD-DD asserts that
concept drift has occurred between them. Second, the sample set en-
coder is updated by considering the sliding window as the context
for learning the latest concepts. Specifically, we treat sample sets
from the same sub-window as positive pairs and construct negative
pairs by leveraging temporal gaps between sub-windows and noise
augmentation to distort the original distribution. In the meantime,
MCD-DD dynamically adjusts the threshold for drift detection in
the next sliding window by analyzing the positive sample pairs.
Finally, the encoder is updated by aiming to minimize the distance
between embeddings of positive sample pairs while simultaneously
maximizing the distance between those of negative pairs.
4.2 Sample Set Encoder
We use a set of data points sampled in each sub-window to estimate
its data distribution that represents a concept in the sub-window.
Specifically, for a given sub-window S𝑗
𝑡⊂W𝑡, we perform sampling
without replacement from it to obtain a sample set of size𝑚:
𝑀𝑗={𝑥𝑖∈S𝑗
𝑡}𝑚
𝑖=1. (3)
The sample set is passed to a sample set encoder, denoted as
set-enc(). It translates the probability distribution 𝑃𝑡(S𝑗
𝑡)of the
sub-window into the compact representation in the projected space,
which we call concept representation h𝑗ofS𝑗
𝑡:
h𝑗=set-enc(𝑀𝑗)=1
𝑚𝑚∑︁
𝑖𝑓(𝑥𝑖∈𝑀𝑗), (4)
where𝑓(·)is an encoding model with a deep neural network.
 
2926KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
Drift Detector (Section 3.3) Sample Set Encoder ( Section 3.2)𝒉𝒉𝒘𝒘𝒘𝒘𝟏𝟏𝒋𝒋
𝒉𝒉𝒘𝒘𝒘𝒘𝟐𝟐𝒋𝒋Far
𝒉𝒉𝒑𝒑𝟏𝟏𝒋𝒋
𝒉𝒉𝒑𝒑𝟐𝟐𝒋𝒋Close𝕎𝕎𝒕𝒕 𝕎𝕎𝒕𝒕+𝑺𝑺
(𝑴𝑴𝒑𝒑𝟏𝟏𝒋𝒋, 𝑴𝑴𝒑𝒑𝟐𝟐𝒋𝒋) (𝑴𝑴𝒔𝒔𝒘𝒘𝟏𝟏𝒋𝒋, 𝑴𝑴𝒔𝒔𝒘𝒘𝟐𝟐𝒋𝒋′+𝜹𝜹𝟐𝟐)(𝑴𝑴𝒘𝒘𝒘𝒘𝟏𝟏𝒋𝒋, 𝑴𝑴𝒘𝒘𝒘𝒘𝟐𝟐𝒋𝒋+𝜹𝜹𝟏𝟏)…𝕊𝕊𝒕𝒕+𝒔𝒔𝒋𝒋
Far
𝒉𝒉𝒔𝒔𝒘𝒘𝟏𝟏𝒋𝒋
𝒉𝒉𝒔𝒔𝒘𝒘𝟐𝟐𝒋𝒋′
Contrastive Loss Optimize (Section 3.4)Weak negative samples
Strong negative samplesPositive samples
𝑴𝑴𝒋𝒋′𝑴𝑴𝒋𝒋𝕊𝕊𝒕𝒕+𝒔𝒔𝒋𝒋′
𝒉𝒉𝒋𝒋
𝒉𝒉𝒋𝒋′
Samples for detection
𝑷𝑷𝒕𝒕+𝒔𝒔𝕊𝕊𝒕𝒕+𝒔𝒔𝒋𝒋≠𝑷𝑷𝒕𝒕+𝒔𝒔𝕊𝕊𝒕𝒕+𝒔𝒔𝒋𝒋′?||𝒉𝒉𝒋𝒋−𝒉𝒉𝒋𝒋′||𝟐𝟐>𝝈𝝈?𝕊𝕊𝒕𝒕𝒋𝒋…𝕊𝕊𝒕𝒕𝒋𝒋′
𝕎𝕎𝒕𝒕−𝑺𝑺
Seample set encoder set -enc(·)Sliding windows
in data stream
Update 𝝈𝝈
Figure 2: Overall procedure of MCD-DD. Sub-windows are encoded and compared to derive MCD for drift detection.
4.3 Drift Detector
For each sub-window S𝑗
𝑡in a sliding window W𝑡, we obtain the
concept representations {ℎ𝑗}𝑁𝑠𝑢𝑏
𝑗=1by the sample set encoder. Then,
to effectively quantify the distance between the concept representa-
tions from two adjacent sub-windows, we introduce a new measure
Maximum Concept Discrepancy (MCD) inspired by MMD:
MCD(𝑃,𝑄)=sup
∥𝑓∥2≤𝐿∥E[𝑓(𝑋)]−E[𝑓(𝑌)]∥2, (5)
where the function 𝑓(·)is approximated by a deep neural network
and is constrained to be Lipschitz continuous [ 17]. This ensures the
convergence of optimizing 𝑓and prevents MCD from becoming
infinitely large. Moreover, it makes MCD between two sets of inde-
pendent data from the same distribution bounded, providing the
theoretical foundation for our drift detection.
When we have samples 𝑀𝑗∼𝑃and𝑀𝑗′∼𝑄by Eq. (3), the
empirical MCD can be derived from Eq. (5) and Eq. (4) as:
MCD(𝑃,𝑄)=sup
∥𝑓∥2≤𝐿∥1
𝑚𝑚∑︁
𝑖=1𝑓(𝑥𝑖∈𝑀𝑗)−1
𝑚𝑚∑︁
𝑖=1𝑓(𝑥𝑖∈𝑀𝑗′)]∥2.
=sup
∥𝑓∥2≤𝐿∥h𝑗−h𝑗′∥2
(6)
Based on MCD, if the discrepancy between the data distributions
of two adjacent sub-windows exceeds a given threshold 𝜎,
MCD(𝑃𝑡(S𝑗
𝑡),𝑃𝑡(S𝑗−1
𝑡))=∥h𝑗−h𝑗−1∥2>𝜎, (7)
then we determine that a concept drift happened between them:
𝑃𝑡(S𝑗
𝑡)≠𝑃𝑡(S𝑗−1
𝑡). (8)
The threshold 𝜎can be controlled dynamically using a bootstrap-
ping strategy, allowing it to adjust to the varying distances between
sample sets within the same concept. By analyzing the historical
collection of MCD values between sample sets in the same sub-
windows (i.e., those with identical data distributions), we adjust the
threshold𝜎for each sliding window within a predefined statistical
significance level (e.g., 0.05). It is worth noting that this historical
MCD information can be obtained during the optimization of the
encoder without necessitating additional computations.4.4 Optimization
To optimize the sample set encoder, we employ contrastive learn-
ing [22] to enhance the separability of various concepts. The pri-
mary challenge lies in determining positive and negative sample
pairs that are to be closer and further apart, respectively. This chal-
lenge is particularly pronounced in unsupervised scenarios, where
we lack any prior information regarding the underlying concepts
and true drifts. In MCD-DD, we exploit the concept of temporal co-
herence [ 37,45], which is a fundamental characteristic observed in
temporal data. It suggests that data points that are close in time are
more likely to exhibit similar characteristics. We exploit this insight
to create positive and negative samples used for optimization.
4.4.1 Preparing Positive Samples. MCD-DD generates a posi-
tive sample pair by choosing two sets of data points from the same
sub-window, assuming that these sets follow the same distributions.
In each sub-window S𝑡𝑗, MCD-DD conducts sampling similar to
Eq. (3) to select two sets of data points, which we treat as positive
sample pairs. These pairs are denoted as 𝑀𝑗
𝑝1and𝑀𝑗
𝑝2. To generate
a diverse set of positive sample pairs, we repeat this sampling
process𝑘times, resulting in{𝑀𝑗,𝑖
𝑝1}𝑘
𝑖=1and{𝑀𝑗,𝑖
𝑝2}𝑘
𝑖=1. Subsequently,
the sample set encoder in Eq. (4) computes the embeddings for the
𝑘positive sample pairs, yielding {h𝑗,𝑖
𝑝1}𝑘
𝑖=1and{h𝑗,𝑖
𝑝2}𝑘
𝑖=1.
4.4.2 Preparing Negative Samples. MCD-DD generates a neg-
ative sample pair to learn diversity in concept drift. Specifically,
MCD-DD selects two sets of data points respectively from the differ-
ent sub-windows that are temporally distant, whose distributions
are likely to differ. MCD-DD also employs an efficient data augmen-
tation technique to improve the generalization of negative pairs by
introducing noise to each sampled data point:
𝑥′=𝑥+𝛿, (9)
where the noise 𝛿is generated from a standard probability distribu-
tion (e.g., the Gaussian distribution 𝐺(𝜇,𝜖)). By incorporating these
two principles, temporal gap (i.e., drift diversity) and noise augmen-
tation (i.e., concept diversity), we prepare two types of negative
sample pairs.
Weak negative samples : The first type of negative samples in-
volves sampling pairs of data points from the same sub-windows
but adding a small degree of noise into one of the sets. Specifically,
 
2927Online Drift Detection with Maximum Concept Discrepancy KDD ’24, August 25–29, 2024, Barcelona, Spain
given a sub-window S𝑡𝑗, the𝑘weak negative sample pairs are
prepared:
{𝑀𝑗,𝑖
𝑤𝑛1}𝑘
𝑖=1and{𝑀𝑗,𝑖
𝑤𝑛2+𝛿1}𝑘
𝑖=1, (10)
where𝛿1∼𝐺(0,𝜖𝑠𝑚𝑎𝑙𝑙)and𝐺is a Gaussian distribution. Finally,
the corresponding two set sample embeddings are derived:
{h𝑗,𝑖
𝑤𝑛1}𝑘
𝑖=1and{h𝑗,𝑖
𝑤𝑛2}𝑘
𝑖=1. (11)
Strong negative samples : The second type of negative samples
involves sampling pairs of data points from the two sub-windows
that are temporally distant and more substantial noise is added to
one of the sets. Specifically, given a sub-window S𝑡𝑗andS𝑡𝑗′, the
𝑘strong negative sample pairs are prepared:
{𝑀𝑗,𝑖
𝑠𝑛1}𝑘
𝑖=1and{𝑀𝑗′,𝑖
𝑠𝑛2+𝛿2}𝑘
𝑖=1, (12)
where𝛿2∼𝐺(0,𝜖𝑏𝑖𝑔). Similarly, the corresponding two set sam-
ple embeddings are derived:
{h𝑗,𝑖
𝑠𝑛1}𝑘
𝑖=1and{h𝑗′,𝑖
𝑠𝑛2}𝑘
𝑖=1. (13)
Note that the temporal gap between two sub-windows can be
adjusted within the context of a window (i.e., 1≤|𝑗−𝑗′|≤𝑁𝑠𝑢𝑏−1).
By default, MCD-DD adopts the largest temporal gap by setting
𝑗=𝑁𝑠𝑢𝑏and𝑗′=1so as to maximize the likelihood that the
samples within each window exhibit distinct data distributions.
4.4.3 Learning Objective. By putting the positive, weak nega-
tive, and strong negative samples altogether, the final loss for the
sample set encoder is formulated in the form of InfoNCE loss [ 32]:
L=log𝑁𝑠𝑢𝑏∑︁
𝑗=1Í𝑘
𝑖=1exp(MCD𝑗,𝑘
𝑝)
Í𝑘
𝑖=1(exp(MCD𝑗,𝑘
𝑝)+exp(MCD𝑗,𝑘
𝑤𝑛)+exp(MCD𝑗,𝑘
𝑠𝑛))
+𝜆𝑚∑︁
𝑖=1(||∇𝑥𝑖𝑓(𝑥𝑖)||2−𝐿)2,
(14)
where MCD𝑘𝑝=||h𝑗,𝑘
𝑝1−h𝑗,𝑘
𝑝2||2,MCD𝑘𝑤𝑛=||h𝑗,𝑘
𝑤𝑛1−h𝑗,𝑘
𝑤𝑛2||2, and
MCD𝑘𝑠𝑛=||h𝑗,𝑘
𝑠𝑛1−h𝑗′,𝑘
𝑠𝑛2||2are MCD values between the probability
distributions of two sub-windows chosen for each sample type.
The last term is the gradient penalty to ensure the L-Lipschitz
continuity [ 21] of the encoding model 𝑓(·)where𝐿is a constant
and the coefficient 𝜆is the regularization parameter.
5 THEORETICAL ANALYSIS
5.1 Upper Bound of MCD
Given two sample sets of data points drawn from the same distri-
bution, we study the upper bound of MCD between the two sets,
which can serve as a theoretical threshold for detecting concept
drifts in two sub-windows for MCD-DD.
Theorem 1. Assume that the sets {𝑋𝑖}𝑛
𝑖=1and{𝑌𝑖}𝑛
𝑖=1are inde-
pendently and identically distributed (i.i.d.), both drawn from the
probability distribution 𝑝(𝑥)with a mean 𝜇and variance 𝜎. If𝑓is a
Lipschitz continuous function with Lipschitz constant 𝐿, we have:
𝑃(|1
𝑛𝑛∑︁
𝑖=1𝑓(𝑋𝑖)−1
𝑛𝑛∑︁
𝑖=1𝑓(𝑌𝑖)|)>𝐺(1−𝛼
2)√︂
2
𝑛𝐿𝜎)≤𝛼, (15)where𝐺(·)is the standard Gaussian distribution function. There-
fore, for a given significance level 𝛼,𝐺(1−𝛼
2)√︃
2
𝑛𝐿𝜎is an upper
bound for the MCD |1
𝑛Í𝑛
𝑖=1𝑓(𝑋𝑖)−1
𝑛Í𝑛
𝑖=1𝑓(𝑌𝑖)|.
Proof. By the Central Limit Theorem,1
𝑛Í𝑛
𝑖=1𝑓(𝑋𝑖)−1
𝑛Í𝑛
𝑖=1𝑓(𝑌𝑖)
converges to the Guassain distribution. Moreover, we have:
𝐸(1
𝑛𝑛∑︁
𝑖=1𝑓(𝑋𝑖)−1
𝑛𝑛∑︁
𝑖=1𝑓(𝑌𝑖))=0. (16)
Since𝑓is a𝐿-Lipschitz continuous function, we have:
𝑉𝑎𝑟(1
𝑛𝑛∑︁
𝑖=1𝑓(𝑋𝑖)−1
𝑛𝑛∑︁
𝑖=1𝑓(𝑌𝑖))≤2𝐿2𝜎2
𝑛. (17)
When we approximate the distribution of1
𝑛Í𝑛
𝑖=1𝑓(𝑋𝑖)−1
𝑛Í𝑛
𝑖=1𝑓(𝑌𝑖)
as the Gaussian distribution, we have:
𝑃(|1
𝑛𝑛∑︁
𝑖=1𝑓(𝑋𝑖)−1
𝑛𝑛∑︁
𝑖=1𝑓(𝑌𝑖)|>𝐺(1−𝛼
2)√︂
2
𝑛𝐿𝜎)≤𝛼. (18)
□
Therefore, for the null hypothesis 𝐻0:{𝑋𝑖}𝑛
𝑖=1and{𝑌𝑖}𝑛
𝑖=1are
drawn from the same probability distribution, given a significance
level𝛼,𝐺(1−𝛼
2)√︃
2
𝑛𝐿𝜎can serve as the threshold for rejecting 𝐻0.
In the scenario where {𝑋𝑖}𝑛
𝑖=1and{𝑌𝑖}𝑛
𝑖=1are multivariate random
variables, hypothesis testing can similarly be conducted using the
chi-squared distribution.
This theoretical bound can serve as a guide to set the threshold
in a hypothesis testing framework. However, deriving the exact
rejection threshold analytically may not always be feasible. As
suggested in Section 4.3, the empirical threshold for rejecting the
null hypothesis can be used by estimating statistics of historical
MCD values meeting the hypothesis with a pre-defined significance.
5.2 Complexity of MCD-DD
We analyze the time complexity of MCD-DD mainly for sampling,
training, and inference. Recall that we use a sliding window with
𝑁𝑠𝑢𝑏sub-windows, 𝑘sets of𝑚samples, and an encoder with the
parameter size 𝑝and training epochs 𝑒. Since we sample in each
sub-window, the time complexity for constructing positive and
negative samples is O(𝑚𝑘𝑁𝑠𝑢𝑏). The time complexity for training
the encoder isO(𝑚𝑘𝑒𝑝). For inference, since we need to calculate
the differences of sample sets in each sub-window sequentially,
the time complexity is 𝑂(𝑚𝑘𝑁2
𝑠𝑢𝑏). Finally, the total complexity is
𝑂(𝑚𝑘(𝑁2
𝑠𝑢𝑏+𝑒𝑝)). Since typically 𝑝≫𝑒,𝑚,𝑘,𝑁𝑠𝑢𝑏, the time com-
plexity of MCD-DD is mostly controlled by the encoder complexity.
6 EXPERIMENTS
We conducted thorough experiments to evaluate the performance
of MCD-DD on 7 synthetic data sets and 4 real-world data sets. The
results are briefly summarized as follows.
•MCD-DD outperforms existing baselines in detecting concept
drifts in terms of Precision, F1, and MCC scores and shows high
interpretablity with varying drift types (Section 6.2).
•Through ablation analysis, the three sampling strategies intro-
duced in contrastive learning for MCD are demonstrated to be
effective (Section 6.3).
 
2928KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
Table 1: Description of data streams used for evaluating drift detection performance.
Stream Category Data Set Composition Instances Dimension Drift Type(s)
Synthetic (Primary)GM_Sud Gaussian Mixture 30,000 5 Sudden
GM_Rec Gaussian Mixture 30,000 5 Reoccurring
GM_Grad Gaussian Mixture 30,000 5 Gradual
GM_Inc Gaussian Mixture 30,000 5 Incremental
Synthetic (Complex)GamLog_Sud Gamma, Lognormal 30,000 5 Sudden
LogGamWei_Sud Lognormal, Gamma, Weibull 30,000 20 Sudden
GamGM_SudGrad Gamma,Gaussian Mixture 30,000 20 Sudden, Gradual
Real-WorldINSECTS_Sud Mosquito sensors with varying temperatures 52,848 33 Sudden
INSECTS_Grad Mosquito sensors with varying temperatures 24,150 33 Gradual
INSECTS_IncreRec Mosquito sensors with varying temperatures 79,986 33 Incremental, Reoccurring
EEG EEG readings for open/closed eye states 14,980 14 Sudden, Reoccurring
•The hyperparameters for MCD-DD are reasonably set by default
and robust to the performance in most cases (Section 6.4).
•Visualizations of concept embeddings (Section 6.5) and threshold
changes (Section 6.6) show the empirical efficacy of MCD-DD.
6.1 Experiment Setting
6.1.1 Data Sets .As summarized in Table 1, we used both synthetic
andreal-world data sets, each with known drift points.
The synthetic data sets are further divided into primary drift
tasks andcomplex drift tasks, based on the complexity of drift detec-
tion. The primary drift tasks involve drifts created by altering the
weights in Gaussian mixture models, leading to distinct distribu-
tion changes in low-dimensional data streams. Conversely, complex
drift tasks include more challenging, higher-overlap distributions
like Gamma, Lognormal, and Weibull, and consider multiple drift
occurrences in higher-dimensional streams. A more comprehensive
data generation process is provided in Appendix A.1.1.
Concept drift in real-world data sets tends to occur with more
arbitrary and complex causes and thus presents greater predictive
challenges. We selected real data sets known for their drift locations
and types. INSECTS [40] introduces data sets with multiple types of
drift through varying environmental temperatures to collect sensor
reading values monitoring mosquitos. EEG [35] is a collection of
EEG measurements of eye states recorded via camera, where open
and closed eye states represent two different EEG distributions caus-
ing concept drift. Both data sets are widely used in relevant work
considering concept drifts in real-world scenarios [ 31,52]. Further
details on the real-world data sets can be found in Appendix A.1.2.
For all data sets, a sliding window is employed from the begin-
ning of each data set to simulate data streams. The sub-windows
where the true drift initiates or is present are assigned a label of
"Drift", while others are designated as "No Drift", formulating drift
detection to a binary classification to facilitate evaluation.
6.1.2 Compared Algorithms. We chose popular drift detection
algorithms that can be adopted for unsupervised andonline concept
drift detection; (1) Kolmogorov-Smirnov Test (KS Test) [34]: assess-
ing whether one-dimensional distributions differ by calculating
the supremum of the differences between their empirical distribu-
tions. For the multi-dimensional data, p-values were aggregated
using the Bonferroni correction. (2) Maximum Mean Discrepancy
with a Gaussian Kernel (MMD-GK) [19]: a kernel-based method for
multi-dimensional two-sample testing that quantifies the disparitybetween two distributions by measuring their mean embeddings
within the RKHS. In our experiments, we employ a Gaussian kernel
to compute an unbiased estimate of the disparity. (3) Least-Squares
Density Difference (LSDD) [6]: employing a linear-in-parameters
Gaussian kernel function to estimate the distance between the
probability density functions of two samples. (4) Maximum Mean
Discrepancy with a Deep Kernel (MMD-DK) [29]: enhancing the tra-
ditional MMD approach by incorporating a deep kernel, where the
kernel function is optimized using a subset of the data to maximize
the test power. The detailed implementation and evaluation setting
for the compared algorithms are provided in Appendix A.2
6.1.3 Evaluation Metrics. We used Precision, F1-score, and the
Matthews Correlation Coefficient (MCC) [ 9] as metrics to gauge per-
formance. Precision evaluates the proportion of true drifts correctly
identified among all detected drifts, serving as a critical metric for
assessing the effectiveness of a detector in avoiding false positives.
The F1-score, a harmonic mean of precision and recall, serves as a
balanced measure of a detector’s effectiveness in identifying true
drifts while considering both false positives and false negatives. The
MCC[ 9], which encompasses all quadrants of the confusion matrix,
is particularly reliable in scenarios involving rare but critical events,
notably in cases of concept drift. We conducted 20 runs for each
experiment and reported the average with standard deviations.
6.2 Drift Detection Accuracy
6.2.1 Synthetic Data Sets. As shown in Table 2, MCD-DD achieved
the highest precision across all simulated data sets, with the scores
being almost all 1 or very close to 1, demonstrating its Eagle Eye
capability in drift point detection. Moreover, except for data sets
with incremental drift, our method outperformed others in terms
of F1-score and MCC as well.
Visualization of MCD in each window : In Figure 3, We further
analyzed the concept drift capability of MCD-DD. We calculated
MCD values between the most recent data distributions in each new
sub-window ( 𝑆𝑁𝑠𝑢𝑏
𝑡) and all preceding data distributions within the
same window ( 𝑆𝑁1
𝑡to𝑆𝑁𝑠𝑢𝑏−1
𝑡). The horizontal axis represents the
locations of 𝑆𝑁𝑠𝑢𝑏
𝑡at each time step 𝑡, and the vertical axis encom-
passes preceding sub-windows in the same window. Then, each
cell indicates the learned MCD between sub-window pairs. For var-
ious types of concept drift, the heatmap patterns exhibit distinctive
shapes: sudden drift manifests as lower triangular patterns starting
 
2929Online Drift Detection with Maximum Concept Discrepancy KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Overall performance comparison (the best and second best results are in bold and underlined, respectively).
MCD-DD KS MMD-GK LSDD MMD-DK
Data set Pre. F1 MCC Pre. F1 MCC Pre. F1 MCC Pre. F1 MCC Pre. F1 MCC
GM_Sud1.00
(±0.00)1.00
(±0.00)1.00
(±0.00)0.25
(±0.00)0.40
(±0.00)0.49
(±0.00)0.32
(±0.07)0.48
(±0.08)0.56
(±0.06)0.17
(±0.04)0.30
(±0.05)0.40
(±0.05)0.25
(±0.12)0.38
(±0.15)0.47
(±0.12)
GM_Rec1.00
(±0.00)0.79
(±0.11)0.81
(±0.10)0.40
(±0.00)0.50
(±0.00)0.50
(±0.00)0.64
(±0.08)0.78
(±0.06)0.79
(±0.05)0.60
(±0.16)0.74
(±0.12)0.76
(±0.10)0.37
(±0.11)0.51
(±0.12)0.54
(±0.13)
GM_Grad1.00
(±0.00)0.79
(±0.06)0.79
(±0.06)0.78
(±0.00)0.78
(±0.00)0.76
(±0.00)0.80
(±0.05)0.89
(±0.03)0.88
(±0.03)0.79
(±0.06)0.78
(±0.03)0.77
(±0.03)0.70
(±0.11)0.79
(±0.08)0.78
(±0.09)
GM_Inc0.99
(±0.04)0.44
(±0.09)0.51
(±0.07)0.38
(±0.00)0.33
(±0.00)0.27
(±0.00)0.64
(±0.05)0.71
(±0.03)0.67
(±0.04)0.57
(±0.06)0.65
(±0.05)0.61
(±0.06)0.44
(±0.18)0.37
(±0.13)0.31
(±0.15)
GamLog_Sud1.00
(±0.00)1.00
(±0.00)1.00
(±0.00)0.13
(±0.00)0.22
(±0.00)0.34
(±0.00)0.11
(±0.01)0.20
(±0.02)0.31
(±0.02)0.11
(±0.02)0.20
(±0.03)0.32
(±0.03)0.18
(±0.06)0.30
(±0.08)0.40
(±0.07)
LogGamWei_Sud0.98
(±0.07)0.94
(±0.12)0.95
(±0.11)0.40
(±0.00)0.57
(±0.00)0.62
(±0.00)0.31
(±0.06)0.46
(±0.06)0.54
(±0.05)0.26
(±0.10)0.41
(±0.10)0.49
(±0.09)0.29
(±0.07)0.44
(±0.08)0.52
(±0.06)
GamGM_SudGrad0.98
(±0.04)0.99
(±0.02)0.99
(±0.02)0.88
(±0.00)0.93
(±0.00)0.93
(±0.00)0.66
(±0.05)0.79
(±0.04)0.79
(±0.03)0.57
(±0.07)0.72
(±0.06)0.73
(±0.05)0.61
(±0.11)0.75
(±0.09)0.76
(±0.08)
INSECTS_Sud0.55
(±0.06)0.59
(±0.07)0.55
(±0.08)0.38
(±0.00)0.53
(±0.00)0.53
(±0.00)0.38
(±0.02)0.53
(±0.02)0.53
(±0.02)0.33
(±0.02)0.49
(±0.02)0.48
(±0.02)0.38
(±0.06)0.51
(±0.07)0.48
(±0.08)
INSECTS_Grad0.18
(±0.15)0.28
(±0.23)0.32
(±0.28)0.06
(±0.00)0.11
(±0.00)0.22
(±0.00)0.06
(±0.00)0.12
(±0.01)0.23
(±0.01)0.05
(±0.01)0.10
(±0.01)0.21
(±0.01)0.06
(±0.04)0.11
(±0.08)0.18
(±0.14)
INSECTS_IncreRec0.26
(±0.05)0.37
(±0.07)0.40
(±0.07)0.08
(±0.00)0.15
(±0.00)0.24
(±0.00)0.08
(±0.00)0.15
(±0.00)0.24
(±0.00)0.08
(±0.00)0.14
(±0.00)0.23
(±0.00)0.10
(±0.01)0.18
(±0.02)0.25
(±0.04)
EEG0.43
(±0.14)0.23
(±0.10)0.12
(±0.09)0.25
(±0.00)0.40
(±0.00)-0.05
(±0.00)0.47
(±0.00)0.63
(±0.00)-0.11
(±0.00)0.25
(±0.00)0.40
(±0.00)-0.00
(±0.00)0.48
(±0.00)0.64
(±0.00)0.03
(±0.04)
9900 12000 14100 16500 18600 20700 23100 25200 27300 29700123456789SN1
t to SNsub 1
t
(a) GM_Sud.
9900 12000 14100 16500 18600 20700 23100 25200 27300 29700123456789SN1
t to SNsub 1
t
 (b) GM_Rec.
9900 12000 14100 16500 18600 20700 23100 25200 27300 29700123456789SN1
t to SNsub 1
t
(c) GM_Grad.
9900 12000 14100 16500 18600 20700 23100 25200 27300 29700123456789SN1
t to SNsub 1
t
 (d) GM_Inc.
Figure 3: Heatmaps of MCD between sub-windows for primary synthetic data sets with drift indicators (red lines and arrows).
9000 13000 17500 22000 26500 31000 35500 40000 44500 49000123456789SN1
t to SNsub 1
t
Figure 4: Heatmap for INSECTS_Sud with drift indicators.
at the drift point, reoccurring drift as two lower triangles, grad-
ual drift progressively forms lower triangular patterns from top to
bottom, and incremental drift results in fainter lower triangles due
to more subtle distributional changes. We further investigated the
model’s performance with even more subtle and slower incremental
drifts and real-world data sets in Appendix A.3. These demonstra-
tions highlight the interpretive strength of MCD-DD in capturing
the dynamics of drift occurrences.
6.2.2 Real-world Data Sets. As shown in Table 2, MCD-DD
demonstrated superior performances over other baseline algorithms
across a variety of drift scenarios within the INSECTS, including
INSECTS_Sud (sudden drift), INSECTS_Grad (gradual drift), and
INSECTS_IncreRec (incremental drift and reoccurring). Figure 4
shows the heatmap of MCD for INSECTS_Sud. These results high-
light MCD-DD’s robust capability in effectively detecting and adapt-
ing to different types of drift phenomena, ranging from abrupt
changes to slow evolutions and cyclic variations. Despite exhibiting
slightly weaker performance than MMD-DK when applied to the
EEG, known for its high frequency of sudden drifts, it is noteworthy
that MCD-DD still consistently achieved the highest MCC value.
MCD-DD MCD-DD-WN MCD-DD-SN MCD-DD-(WN,SN)
0.0 0.2 0.4 0.6 0.8 1.0
PrecisionGM_SudGM_RecGM_GradGM_IncGamLog_SudLogGamWei_SudGamGM_SudGradINSECTS_SudINSECTS_GradINSECTS_IncreRecEEGFigur
e 5: Ablation study of contrastive learning strategies.
6.3 Ablation Study
We conducted ablation studies on the strategies for obtaining posi-
tive and negative sample pairs. Specifically, we evaluated the per-
formance of MCD-DD under various configurations: the complete
MCD-DD setup, MCD-DD without weak negative sample pairs (i.e.,
MCD-DD-WN), MCD-DD without strong negative sample pairs
(i.e., MCD-DD-SN), and MCD-DD utilizing only positive sample
pairs, eliminating all negative samples (i.e., MCD-DD-(WN,SN)).
Figure 5 shows the results of precision, while the results of other
metrics showed similar trends (Appendix A.3). In summary, MCD-
DD achieved the highest precision across all data sets and the
 
2930KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
GM_Sud
GM_RecGM_Grad
GM_IncGamLog_Sud
LogGamWei_SudGamGM_SudGrad
1500 3000 4500 60000.40.60.81.0MCC
(a) Window size 𝑊.
4 10 20 400.250.500.751.00MCC
 (b) Number of sampling 𝑘.
1:5 1:10 1:50 1:1000.000.250.500.751.00MCC
(c) Ratio of noise 𝜖small :𝜖big.
0.1 1 5 100.250.500.751.00MCC
(d) Regularization coefficient 𝜆.
Figure 6: Sensitivity analysis results (default in red box).
INSECTS_Sud INSECTS_Grad INSECTS_IncreRec EEG
1000 2000 5000 70000.00.20.40.6MCC
(a) Window size 𝑊.
4 10 20 400.20.40.6MCC
 (b) Number of sampling 𝑘.
1:5 1:10 1:50 1:1000.20.40.6MCC
(c) Ratio of noise 𝜖small :𝜖big.
0.1 1 5 100.00.20.40.6MCC
(d) Regularization coefficient 𝜆.
Figure 7: Sensitivity analysis results for real-world data sets.
absence of each type of negative sample pairs leads to lower perfor-
mances in most cases. Specifically, removing weak negative sample
pairs resulted in diminished effectiveness in detecting gradual, sub-
tle concept drifts, like GM_Rec and EEG. Eliminating strong nega-
tive sample pairs adversely affected performance in more challeng-
ing drift detection scenarios (e.g., high overlap distributions, incre-
mental drift), even failing to identify any drifts GamLog_Sud, along
with notably poor performances in GM_Inc and INSECTS_Grad.
Retaining only positive sample pairs yielded comparable results
in simpler tasks but significantly underperformed in more com-
plex simulated and real-world data sets compared to strategies
incorporating negative sample pairs. These ablation study find-
ings demonstrate the efficacy of the sampling strategies proposed
particularly in dealing with complex drift scenarios.
6.4 Sensitivity Analysis
6.4.1 Effects of main Hyperparameters. We conducted a sen-
sitivity analysis of the main hyperparameters used in MCD-DD:
thesliding window size 𝑊, the number of samples 𝑘, the degree ofTable 3: Window processing time (sec) over encoder sizes.
Hidden Size Sampling Time Training Time Inference Time
50 0.0067 0.2106 0.0112
100 0.0065 0.2248 0.0117
150 0.0065 0.2297 0.0103
200 0.0069 0.2302 0.0120
250 0.0071 0.2456 0.0120
300 0.0067 0.2518 0.0121
GM_Sud
GM_RecGM_Grad
GM_IncGamLog_Sud
LogGamWei_SudGamGM_SudGrad
50 100 150 200 250 300
Hidden Size for Encoder0.50.60.70.80.91.0MCC
Figur
e 8: Detection accuracy over encoder sizes.
noise𝜖, and the regularization coefficient 𝜆. Figure 6 shows the MCC
results on the synthetic data sets, and the results of other metrics
showed similar trends. For 𝑊we varied it from half to two times
the default value. Figure 6a demonstrates that MCD-DD shows
comparable performances over varying window sizes with the peak
performances at window sizes around the default value. Since the
window size determines the context of current concepts used for
training the encoder, the smaller size is preferable in practice for
efficiency, as long as it includes temporally distant different distri-
butions. Regarding the number 𝑘of sampling in each sub-window,
Figure 6b shows that the higher number of sampling leads to in-
creased performance. Nevertheless, sampling frequencies beyond
the default value ( 𝑘=10) lead to only marginal improvements. In
constructing negative sample pairs, the perturbation degree 𝛿with
𝜖small and𝜖bigcontrols the relative degrees of noise for weak and
strong negative samples. Figure 6c indicates that too low or too
high ratios of small and big noise perform poorly, particularly in
challenging cases (e.g., GamLog_Sud and LogGamWei_Sud). The
default ratios of 1 : 10 were demonstrated to be the most optimal
in most cases. Finally, regarding the regularization coefficient 𝜆for
gradient penalty to ensure 𝐿-Lipschitz continuity in the optimiza-
tion, Figure 6d shows that either too light or too heavy penalties
can reduce the model’s effectiveness on challenging cases, making
𝜆=1a suitable choice.
Figure 7 shows the sensitivity analysis results on real-world
data sets, demonstrating similar trends with the synthetic data sets.
While the default values for 𝑊are not annotated in the figure given
the varying lengths of each data set, it is found that setting 𝑊to
10% of the total length of each data set is a suitable choice.
6.4.2 Effects of Encoder Size. We also conducted a sensitivity
analysis of the encoder on processing time (Table 3) and detection
accuracy (Figure 8) by varying its hidden size from 50to300. The
results indicate that, in general, the encoder’s hidden size does
 
2931Online Drift Detection with Maximum Concept Discrepancy KDD ’24, August 25–29, 2024, Barcelona, Spain
Normal(20, 10) Normal(20, 50) Gaussian Mixture Uniform(0, 40) Gamma(2,10) Weibull Log-normal
80
 60
 40
 20
 0 20 40 60
t-SNE axis 140
20
02040t-SNE axis 2t-SNE of Different Distributions(Original)
(
a) Orginal.
4
 3
 2
 1
 0 1 2 3 4
t-SNE axis 14
3
2
1
01234t-SNE axis 2t-SNE of Different Distributions(Gaussian Kernel) (
b) Gaussian Kernel.
60
 40
 20
 0 20 40 60
t-SNE axis 140
20
0204060t-SNE axis 2t-SNE of Different Distributions(DeepKernel) (
c) Deep Kernel.
60
 40
 20
 0 20 40 60
t-SNE axis 160
40
20
02040t-SNE axis 2t-SNE of Different Distributions(Our Embeddings) (
d) MCD-DD.
Figure 9: Comparative visualization of discriminative embedding capabilities for complex distributions.
GM_Sud
GM_RecGM_Grad
GM_IncGamLog_Sud
LogGamWei_SudGamGM_SudGrad
0 5000 10000 15000 20000 25000
Start of Window0.00.20.40.60.81.01.21.4Threshold Value ( =5%)
Figur
e 10: Changes of drift detection threshold 𝜎over time.
not have a significant impact on performance. However, particu-
larly in the data streams with complex drifts (e.g., GamLog_Sud or
GamGM_SudGrad), the encoder with too low or too high hidden
sizes degrades the detection accuracy owing to lacking expressive-
ness or overfitting in representing the complex concepts.
6.5 Qualitative Analysis
To more vividly showcase how MCD-DD maps original data distri-
butions to the appropriate embedding space, facilitating the identifi-
cation of varying distribution changes, we visualized the embedding
spaces generated from the compared algorithms in two-dimensional
space by t-SNE [ 42]. The seven challenging distributions used to
generate data points are detailed in Appendix A.4. In Figure 9, points
in each t-SNE plot demonstrate the representations of a set of data,
including 500 data points. Different colors represent their different
underlying probability distributions. Figure 9a demonstrates the
difficulty of distinguishing distributions in the original space, show-
ing highly intermingled clusters. Figure 9b shows that the Gaussian
kernel mapping used by MMD-GK uniformly distributes all dis-
tributions across the space, aligning with the outcomes of prior
studies [ 29]. While the deep kernel by MMD-DK achieved more sep-
arable embeddings, as shown in Figure 9c, MCD-DD exhibits even
better separation, particularly in handling more complex distribu-
tions. This aspect justifies the superior performance of MCD-DD
in concept drift detection across various scenarios.6.6 Drift Detection Threshold Analysis
As discussed in Section 4.3, the drift detection threshold 𝜎of MCD-
DD is dynamically controlled by tracking the historical MCD values
with a predefined statistical significance level (e.g., 0.05). To further
understand its dynamicity, we analyzed the changes in the threshold
over sliding windows in synthetic data sets with different drift types.
Figure 10 shows that the thresholds were consistent over time in
most cases, indicating that MCD is optimized robustly to varying
drift types. GM_Grad notably exhibited a significant increase in the
threshold values after the first drift was observed around 10,000,
but it quickly converged to a constant value and the corresponding
detection accuracy remained high as shown in Section 6.2.
7 CONCLUSION
We proposed MCD-DD, an unsupervised online concept drift detec-
tion method, exploiting a new measure called maximum concept
discrepancy. MCD-DD leverages contrastive learning to obtain qual-
ity concept representations from sampled data points and optimize
the maximum concept discrepancy. It is facilitated by sampling
strategies based on temporal consistency and perturbations for
robust optimization. The theoretical analysis demonstrated that
MCD-DD can also be used within the hypothesis testing framework.
Experimental results on multiple synthetic and real-world data sets
showed that the proposed method achieves superior detection ac-
curacy and higher interpretability than existing baselines.
For future work, we consider exploiting MCD histories learned
throughout data streams. It can provide a systematic understand-
ing of the patterns, duration, and strengths of concept drifts, as
glimpsed in the heatmap visualization analyses. Further, assum-
ing partial concept labels are available, adopting weak supervision
philosophy can be promising. Estimating the degree of differences
between partially labeled concepts can function as pseudo-labels
to help us optimize the encoder more effectively.
ACKNOWLEDGMENTS
This work was supported by ICT Creative Consilience Program
through the Institute of Information & Communications Technol-
ogy Planning & Evaluation(IITP) grant funded by the Korea gov-
ernment (MSIT) (IITP-2024-2020-0-01819) and the New Faculty
Settlement Research Fund by Korea University. We thank Professor
Yifan Cui from Zhejiang University for his valuable advice.
 
2932KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
REFERENCES
[1]S. Agrahari and A. K. Singh. Concept drift detection in data stream mining: A
literature review. Journal of King Saud University-Computer and Information
Sciences, 34(10):9523–9540, 2022.
[2]F. Bayram, B. S. Ahmed, and A. Kassler. From concept drift to model degradation:
An overview on performance-aware drift detectors. Knowledge-Based Systems,
245:108632, 2022.
[3]A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability
and statistics. Springer Science & Business Media, 2011.
[4]A. Bifet and R. Gavalda. Learning from time-changing data with adaptive win-
dowing. In SDM, pages 443–448. SIAM, 2007.
[5]A. Bifet, R. Gavalda, G. Holmes, and B. Pfahringer. Machine learning for data
streams: with practical examples in MOA. MIT press, 2023.
[6]L. Bu, C. Alippi, and D. Zhao. A pdf-free change detection test based on density
difference estimation. IEEE TNNLS, 29(2):324–334, 2018.
[7]V. Cerqueira, H. M. Gomes, A. Bifet, and L. Torgo. STUDD: A student–teacher
method for unsupervised concept drift detection. Machine Learning , 112(11):4351–
4378, 2023.
[8]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for
contrastive learning of visual representations. In ICML, pages 1597–1607. PMLR,
2020.
[9]D. Chicco and G. Jurman. The advantages of the matthews correlation coefficient
(MCC) over f1 score and accuracy in binary classification evaluation. BMC
genomics, 21(1):1–13, 2020.
[10] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. In UAI, pages 258–267,
2015.
[11] A. Elisseeff and J. Weston. A kernel method for multi-labelled classification.
NeurIPS, 14, 2001.
[12] I. Frias-Blanco, J. del Campo-Ávila, G. Ramos-Jimenez, R. Morales-Bueno, A. Ortiz-
Díaz, and Y. Caballero-Mota. Online and non-parametric drift detection methods
based on hoeffding’s bounds. IEEE TKDE, 27(3):810–823, 2014.
[13] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. Learning with drift detection.
InAdvances in Artificial Intelligence–SBIA 2004, pages 286–295. Springer, 2004.
[14] J. Gama, R. Sebastião, and P. P. Rodrigues. On evaluating stream learning algo-
rithms. Machine Learning, 90(3):317–346, 2013.
[15] J. Gama, I. Žliobait ˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia. A survey on
concept drift adaptation. CSUR, 46(4):1–37, 2014.
[16] R. N. Gemaque, A. F. J. Costa, R. Giusti, and E. M. Dos Santos. An overview
of unsupervised drift detection methods. Wiley Interdisciplinary Reviews: Data
Mining and Knowledge Discovery, 10(6):e1381, 2020.
[17] H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree. Regularisation of neural networks
by enforcing lipschitz continuity. Machine Learning, 110:393–416, 2021.
[18] A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. Smola. A kernel method
for the two-sample-problem. NeurIPS, 19, 2006.
[19] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A kernel
two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012.
[20] A. Gretton, K. Fukumizu, Z. Harchaoui, and B. K. Sriperumbudur. A fast, consis-
tent kernel two-sample test. NeurIPS, 22, 2009.
[21] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved
training of wasserstein gans. NeurIPS, 30, 2017.
[22] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised
visual representation learning. In CVPR, pages 9729–9738, 2020.
[23] T. Hofmann, B. Schölkopf, and A. J. Smola. Kernel methods in machine learning.
2008.
[24] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in data streams. In VLDB,
volume 4, pages 180–191. Toronto, Canada, 2004.
[25] D. Kim, H. Min, Y. Nam, H. Song, S. Yoon, M. Kim, and J.-G. Lee. Covid-EENet: Pre-
dicting fine-grained impact of covid-19 on local economies. In AAAI, volume 36,
pages 11971–11981, 2022.
[26] Ł. Korycki and B. Krawczyk. Concept drift detection from multi-class imbalanced
data streams. In ICDE, pages 1068–1079. IEEE, 2021.
[27] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. Mmd gan: Towards
deeper understanding of moment matching network. NeurIPS, 30, 2017.[28] A. Liu, G. Zhang, and J. Lu. Fuzzy time windowing for gradual concept drift
adaptation. In FUZZ-IEEE, pages 1–6. IEEE, 2017.
[29] F. Liu, W. Xu, J. Lu, G. Zhang, A. Gretton, and D. J. Sutherland. Learning deep
kernels for non-parametric two-sample tests. In ICML, 2020.
[30] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang. Learning under concept drift:
A review. IEEE TKDE, 31(12):2346–2363, 2018.
[31] K. Miyaguchi and H. Kajino. Cogra: concept-drift-aware stochastic gradient
descent for time-series forecasting. In AAAI, 2019.
[32] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[33] C. Raab, M. Heusinger, and F.-M. Schleif. Reactive soft prototype computing for
concept drift streams. Neurocomputing, 416:340–351, 2020.
[34] S. Rabanser, S. Günnemann, and Z. Lipton. Failing loudly: An empirical study of
methods for detecting dataset shift. NeurIPS, 32, 2019.
[35] O. Roesler. EEG Eye State. UCI Machine Learning Repository, 2013. DOI:
https://doi.org/10.24432/C57G7J.
[36] J. Shao, Z. Ahmadi, and S. Kramer. Prototype-based learning on concept-drifting
data streams. In KDD, pages 412–421, 2014.
[37] Y. Shin, S. Yoon, S. Kim, H. Song, J.-G. Lee, and B. S. Lee. Coherence-based label
propagation over time series for accelerated active learning. In ICLR, 2021.
[38] A. J. Smola, A. Gretton, and K. Borgwardt. Maximum mean discrepancy. In
ICONIP, pages 3–6, 2006.
[39] X. Song, M. Wu, C. Jermaine, and S. Ranka. Statistical change detection for
multi-dimensional data. In KDD, pages 667–676, 2007.
[40] V. M. A. Souza, D. M. Reis, A. G. Maletzke, and G. E. A. P. A. Batista. Challenges
in benchmarking stream learning algorithms with real-world data. Data Mining
and Knowledge Discovery, 34:1805–1858, 2020.
[41] P. Trirat, S. Yoon, and J.-G. Lee. MG-TAR: multi-view graph convolutional
networks for traffic accident risk prediction. IEEE Transactions on Intelligent
Transportation Systems, 24(4):3779–3794, 2023.
[42] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
[43] A. Van Looveren, J. Klaise, G. Vacanti, O. Cobb, A. Scillitoe, R. Samoilescu, and
A. Athorne. Alibi detect: Algorithms for outlier, adversarial and drift detection,
2019.
[44] Z. Wang, Y. Chen, C. Zhao, Y. Lin, X. Zhao, H. Tao, Y. Wang, and L. Khan. Clear:
Contrastive-prototype learning with drift estimation for resource constrained
stream mining. In TheWebConf, pages 1351–1362, 2021.
[45] Z. Wang, Z. Gao, L. Wang, Z. Li, and G. Wu. Boundary-aware cascade networks
for temporal action segmentation. In ECCV, pages 34–51. Springer, 2020.
[46] Z. Wang, L. Liu, Y. Kong, J. Guo, and D. Tao. Online continual learning with
contrastive vision transformer. In ECCV, pages 631–650. Springer, 2022.
[47] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Deep kernel learning. In
AISTATS, pages 370–378. PMLR, 2016.
[48] S. Xu and J. Wang. Dynamic extreme learning machine for data stream classifi-
cation. Neurocomputing, 238:433–449, 2017.
[49] S. Yoon, H. P. Chan, and J. Han. PDSum: Prototype-driven continuous summa-
rization of evolving multi-document sets stream. In Proceedings of the ACM Web
Conference 2023, pages 1650–1661, 2023.
[50] S. Yoon, D. Lee, Y. Zhang, and J. Han. Unsupervised story discovery from contin-
uous news streams via scalable thematic embedding. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 802–811, 2023.
[51] S. Yoon, J.-G. Lee, and B. S. Lee. NETS: extremely fast outlier detection from
a data stream via set-based processing. Proceedings of the VLDB Endowment,
12(11):1303–1315, 2019.
[52] S. Yoon, Y. Lee, J.-G. Lee, and B. S. Lee. Adaptive model pooling for online deep
anomaly detection from a complex evolving data stream. In KDD. ACM, Aug.
2022.
[53] S. Yoon, Y. Meng, D. Lee, and J. Han. SCStory: Self-supervised and continual
online story discovery. In TheWebConf, pages 1853–1864, 2023.
[54] H. Yu, J. Li, J. Lu, Y. Song, S. Xie, and G. Zhang. Type-LDD: A type-driven lite
concept drift detector for data streams. IEEE TKDE, 2023.
[55] H. Yu, T. Liu, J. Lu, and G. Zhang. Automatic learning to detect concept drift.
arXiv preprint arXiv:2105.01419, 2021.
 
2933Online Drift Detection with Maximum Concept Discrepancy KDD ’24, August 25–29, 2024, Barcelona, Spain
A APPENDIX
A.1 Data Sets
A.1.1 Synthetic Data Sets. The synthetic data sets used for
model performance evaluation feature two types of drifts: primary
andcomplex. Primary drift tasks involve concept drifts that are
relatively easy to distinguish within the data stream. Complex drift
tasks, however, present higher distribution similarity, increased
dimensionality, and a greater number of drift events, making them
more challenging to discern.
For primary tasks, drift is simulated by adjusting the weighting of
two Gaussian distributions, 𝐺1and𝐺2. Both distributions conform
to a 5-dimensional Gaussian distribution with a mean vector 𝜇=
[20,20,20,20,20]and covariance matrices Σ1=102IandΣ2=
502I, where Idenotes the identity matrix. These distributions form
the basis for diverse Gaussian mixtures, with the weighting factor
𝑝controlling the proportion of each distribution in the mixture.
The mixture model is represented by the equation: N(𝜇,Σ1)×𝑝+
N(𝜇,Σ2)×(1−𝑝). Each primary drift task involves a data set of
30,000 instances. Within this framework, different types of primary
drift tasks are simulated by varying 𝑝:
Primary Task 1-GM_Sud : Initially,𝑝is set to 0.2. To induce
a sudden drift at the 21,000th instance, 𝑝is shifted to 0.8, signifi-
cantly changing the mixture’s composition and simulating a sudden
change in the underlying data structure.
Primary Task 2-GM_Rec : Initially,𝑝is set to 0.8, giving promi-
nence to𝐺1. At the 15,000th instance, 𝑝changes to 0.2, thus shifting
the mixture’s balance towards 𝐺2. Finally, at the 25,000th instance,
𝑝returns to 0.8, reinstating the initial distribution emphasis. This
pattern creates a reoccurring drift in the data stream.
Primary Task 3-GM_Inc : Initially set at 0.2, the weighting fac-
tor𝑝undergoes specific adjustments to introduce incremental drifts
at predetermined intervals within the data set. Specifically, 𝑝lin-
early increases from 0.2 to 0.8 between the 12,000th and 12,600th
instances, then decreases back to 0.2 between the 18,000th and
19,200th instances, and finally increases again to 0.8 between the
24,000th and 25,200th instances. Outside these intervals, 𝑝remains
constant, ensuring no drift occurs in the intervening segments. This
design results in multiple incremental drifts across the data set.
These linear transitions facilitate incremental drifts, modifying
the data distribution across two Gaussian components.
Primary Task 4-GM_Grad : In the gradual drift task, 𝑝fluctu-
ates between 0.2 and 0.8. The drift periods where 𝑝=0.8occur dur-
ing the intervals (10000, 11000), (12001, 15000), and (18000, 21000).
Conversely, in the intervening periods (11001,12000), (15001,18000),
and (21001,24000), 𝑝reverts to 0.2. This oscillation creates a gradual
drift pattern by alternating phases of drift and stability.
Complex drift tasks, conversely, involve a mixture of distribu-
tions like Gamma, Lognormal, and Weibull. These distributions
exhibit substantial overlap in their probability density functions
and possess similar statistical characteristics, leading to lower dis-
criminability and making the detection of drifts more challenging.
These tasks are further compounded by introducing multiple drifts
of different natures within the data stream. Also, each complex drift
task involves a data set of 30,000 instances, where every dimension
conforms to the same distribution pattern, ensuring consistency
across the multidimensional data space.Complex Task 1-GamLog_Sud : Initially, data is generated from
a Gamma distribution ( Gamma(1.5,20)) across 5 dimensions. At
the 21,000th instance, there is a sudden shift to a 5-dimensional
Log-normal distribution with parameters 𝜇=log(30)−0.5and
𝜎=0.5. This transition represents a complex and sudden drift,
with the overlap between the two distributions making the drift
challenging to identify.
Complex Task 2-LogGamWei_Sud : The data stream, consist-
ing of 20 dimensions, initially follows a Log-normal distribution
(Lognormal(log(30)−0.5,0.5)). At the 15,000th instance, there
is a sudden drift to a Gamma distribution ( Gamma(1.5,20)), and
at the 24,000th instance, it transitions to a Weibull distribution
(Weibull(1.5,20)). These successive drifts add higher complexity.
Complex Task 3-GamGM_SudGrad : This data stream con-
sists of 20 dimensions, each following the same distribution. Ini-
tially, the data follows a Gamma distribution ( Gamma(2,10)). At the
11,000th instance, a sudden drift occurs, transitioning the data to a
Gaussian mixture. Subsequently, the task experiences gradual drifts,
where the weighting factor 𝑝alternates between 0.2 and 0.8 dur-
ing specific intervals. This alternation leads to shifting dominance
between two Gaussian distributions ( N(20,102)andN(20,502)),
creating a complex pattern of both sudden and gradual drifts.
A.1.2 Real-World Data Sets. Here we provide detailed descrip-
tions of the real-world data sets employed to evaluate the per-
formance of our detector: INSECTS andEEG. These data sets are
instrumental in assessing how effectively the detector adapts to
real-world concept drift scenarios.
INSECTS : The INSECTS data sets consist of optical sensor read-
ings obtained from monitoring mosquitoes. Concept drifts are in-
duced by varying temperatures, which affect the insects’ activity
levels in alignment with their circadian rhythms. This data set offers
a dynamic setting of concept drifts. We utilized three specific data
sets from the collection, each representing one or more distinct
types of drift: (i) INSECTS_Sud: This subset exhibits five sudden
drifts, initiated at a temperature of 30°C, with a sudden shift to 20°C,
and subsequently to approximately 35°C among other changes.
The stream captures several rapid transitions in temperature, il-
lustrating sudden concept drifts throughout. (ii) INSECTS_Grad:
Illustrating both gradual and incremental drifts, this data set sim-
ulates a scenario where temperatures slowly transition over time,
presenting a nuanced evolution of environmental conditions. (iii)
INSECTS_IncreRec: Features a unique pattern of reoccurring
incremental drifts, where temperature gradually increases or de-
creases in cycles. This data set is pivotal for studying the model’s
performance in scenarios where drift patterns repeat over time,
challenging the detection mechanism with both incremental and
reoccurring drift phenomena.
EEG : The EEG data set encompasses multivariate time-series
data from a single continuous EEG recording using the Emotiv EEG
Neuroheadset over a span of 117 seconds. Eye states were captured
through video recording concurrent with the EEG data collection
and were subsequently annotated manually to indicate moments
of eye closure ("1") and eye opening ("0"). This data set provides a
sequential record of neurological activity, with values arranged in
the order they were measured, presenting a unique challenge for
detecting shifts in physiological states over time.
 
2934KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Wan, Yi Liang, and Susik Yoon
9900 12000 14100 16500 18600 20700 23100 25200 27300 29700123456789SN1
t to SNsub 1
t
(a) Heatmap of MCD for subtle and slow drifts.
9000 10400 12000 13600 15200 16600 18200 19800 21400 23000123456789SN1
t to SNsub 1
t
(b) Heatmap for INSECTS_Grad with drift indicators.
22000 26000 30000 34000 38000 42000 46000 50000 54000 58000123456789SN1
t to SNsub 1
t
(c) Heatmap for INSECTS_IncreRec with drift indicators.
Figure 11: Heatmaps of MCD for other data sets.
MCD-DD MCD-DD-WN MCD-DD-SN MCD-DD-(WN,SN)
0.0 0.2 0.4 0.6 0.8 1.0
F1-ScoreGM_SudGM_RecGM_GradGM_IncGamLog_SudLogGamWei_SudGamGM_SudGradINSECTS_SudINSECTS_GradINSECTS_IncreRecEEG
0.0 0.2 0.4 0.6 0.8 1.0
MCCGM_SudGM_RecGM_GradGM_IncGamLog_SudLogGamWei_SudGamGM_SudGradINSECTS_SudINSECTS_GradINSECTS_IncreRecEEG
Figur
e 12: Ablation study results of F1 and MCC.
Table 4: Performance with unidimensional streams.
MCD-DD
ADWIN
Data set Pre. F1 MCC Pre. F1 MCC
INSECTS_Sud
0.585 0.503 0.472 0.667 0.244 0.332
A.2
Implementation Details
A.2.1 Implementation of compared algorithms. For MCD-
DD, we set𝑚=30(or50for a larger sub-window size such as in
INSECTS_IncreRec), 𝑘=10,𝜆=1,𝜖𝑠𝑚𝑎𝑙𝑙 =1,𝜖𝑏𝑖𝑔=10, and𝐿=1.
The encoder function was implemented as a two-layer MLP with
a ReLU activation function. For synthetic data sets, the two-layer
encoder features 100 units in both hidden and output layers. For
the INSECTS data set, reflecting its more complex drift types and
higher dimensionality, the encoder dimensions are increased to 200
(hidden) and 150 (output). For the EEG data set, which involvessmaller window sizes, the dimensions are adjusted to 150 (hidden)
and 100 (output). In all data sets, the encoder function has been
trained for a single epoch with a learning rate of 0.005 for every
sliding window. For the implementation of baselines, we referred
toalibi-detect [43] and used sufficiently high permutations (200)
for the statistical method and the default projection and the best
epochs for the learning-based method.
A.2.2 Evaluation Scheme. For a fair comparison, all algorithms
followed the prequential evaluation scheme [ 14] and used the same
sliding window with the window size 𝑊of 10% of the total length
of each data set and 𝑁𝑠𝑢𝑏=10. Each algorithm compares every
new sub-window with the preceding one in a window to identify
concept drift with a significance level of 0.05 if applicable.
A.2.3 Computing Platform. All experiments were conducted on
a Linux server equipped with an Intel Xeon CPU @ 2.20GHz, 12GB
RAM, and 226GB of storage where Ubuntu 22.04 LTS, Python 3.10,
and PyTorch 2.1.0+cu122 were installed. An NVIDIA Tesla V100-
SXM2-16GB GPU was used for the deep learning-based algorithms.
A.3 Additional Performance Analysis Results
For subtle and slow drift types, we have considered adaptively ad-
justing the training process to achieve better detection outcomes.
In this regard, an incremental drift was introduced by linearly
transitioning the weighting factor 𝑝from 0.0 to 1.0 between the
15,000th and 24,000th instances. Specifically, for this incremental
drift scenario, a specialized training strategy was implemented that
alternates between exclusively using positive sample pairs and a
mix of both positive and negative sample pairs, aimed at better
adapting to and learning the nuanced shifts present. Keeping other
parameters constant, MCD-DD achieved a precision of 0.80, signifi-
cantly outperforming other baseline algorithms with a maximum
precision of 0.55. Similar to the visualization discussed in Section
6.2, Figure 11a illustrates the progression of MCD in this scenario.
In addition, Figures 11b and 11c show the heatmaps of MCD for
the other two types of INSECTS data sets. While it is challenging
to accurately identify the exact starting points for gradual and
incremental drifts in real data streams, the heatmap shows higher
MCD values around the true drift indicators (in the yellow boxes).
Figure 12 shows the ablation study results with F1 and MCC. Table
4 compares the performance of MCD-DD with ADWIN [ 4] adopted
for an unsupervised setting with unidimensional data stream.
A.4 Details of Qualitative Analysis
For the seven distributions, Dist1 is a normal distribution N(20,102),
generating samples in a 5-dimensional space. Dist2 is a normal dis-
tribution with increased variance N(20,502). Dist3 is a mixture
of two normal distributions, giving each sample a 50% probability
of originating from either N(20,102)orN(20,502). Dist4 is a uni-
form distribution spanning from 0 to 40, U(0,40). Dist5 follows a
gamma distribution with shape and scale parameters set to 2 and
10, respectively, Gamma(2,10). Dist6 utilizes a Weibull distribution
with shape and scale parameters of 2 and 20, Weibull(2,20). Lastly,
Dist7 is a log-normal distribution chosen to approximate a mean
close to 20 by setting a standard deviation of 0.5 and a location
parameter𝜇tolog(20)−0.52
2,
Lognormal(𝜇,0.52).
 
2935