Inference Optimization of Foundation Models on AI Accelerators
Youngsuk Park‚àó
AWS AI, Santa Clara, USAKailash Budhathoki‚àó
AWS AI, T√ºbingen, GermanyLiangfu Chen
AWS AI, Santa Clara, USA
Jonas K√ºbler
AWS AI, T√ºbingen, GermanyJiaji Huang
AWS AI, Santa Clara, USAMatth√§us Kleindessner
AWS AI, T√ºbingen, Germany
Jun Huan
AWS AI, Santa Clara, USAVolkan Cevher
AWS AI, T√ºbingen, Germany
EPFL, Lausanne, SwitzerlandYida Wang
AWS AI, Santa Clara, USA
George Karypis
AWS AI, Santa Clara, USA
ABSTRACT
Powerful foundation models, including large language models (LLMs),
with Transformer architectures have ushered in a new era of Gener-
ative AI across various industries. Industry and research community
have witnessed a large number of new applications, based on those
foundation models. Such applications include question and answer,
customer services, image and video generation, and code comple-
tions, among others. However, as the number of model parameters
reaches to hundreds of billions, their deployment incurs prohibi-
tive inference costs and high latency in real-world scenarios. As
a result, the demand for cost-effective and fast inference using AI
accelerators is ever more higher. To this end, our tutorial offers a
comprehensive discussion on complementary inference optimiza-
tion techniques using AI accelerators. Beginning with an overview
of basic Transformer architectures and deep learning system frame-
works, we deep dive into system optimization techniques for fast
and memory-efficient attention computations and discuss how they
can be implemented efficiently on AI accelerators. Next, we describe
architectural elements that are key for fast transformer inference.
Finally, we examine various model compression and fast decoding
strategies in the same context.
CCS CONCEPTS
‚Ä¢General and reference ‚ÜíSurveys and overviews; ‚Ä¢Hardware
‚ÜíSignal processing systems ;‚Ä¢Computer systems organization
‚ÜíDistributed architectures ;‚Ä¢Computing methodologies
‚ÜíMachine learning; Natural language processing; Artificial
intelligence ;‚Ä¢Software and its engineering ‚ÜíParallel pro-
gramming languages; Compilers ; System description languages;
Development frameworks and environments.
‚àóCo-first authors. Correspondence: Youngsuk Park <pyoungsu@amazon.com>, Kailash
Budhathoki <kaibud@amazon.com>
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/2
https://doi.org/10.1145/3637528.3671465KEYWORDS
Inference optimization, LLMs, Transformer, and foundation models.
ACM Reference Format:
Youngsuk Park, Kailash Budhathoki[1], Liangfu Chen, Jonas K√ºbler, Ji-
aji Huang, Matth√§us Kleindessner, Jun Huan, Volkan Cevher, Yida Wang,
and George Karypis. 2024. Inference Optimization of Foundation Models
on AI Accelerators. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671465
1 OVERVIEW
The substantial size of modern large language models (LLMs),
such as Llama-2/3 70B [ 109], Claude 3 Opus 137B [ 8], and Groq-1
314B [ 118], presents significant challenges in both training and in-
ference phases. Training LLMs, in particular, demands considerable
resources and has been the subject of extensive research. In con-
trast, inference consumes fewer computational resources but occurs
much more frequently once the model has been trained. This phase
is crucial as it encompasses various applications where the value
of LLMs is realized, including text translation, sentiment detection,
code generation, text summarization, and question answering.
Customers naturally demand faster and more cost-effective infer-
ence. To meet the user demands, it is essential to reduce latency‚Äîthe
time required to complete a generation‚Äîand to increase through-
put, which is the number of requests processed per unit of time. The
latency and throughput of LLMs depend on multiple factors, such as
the hardware utilized, the capability of software frameworks to op-
timally leverage the available hardware, and the model architecture
itself. Therefore, efforts to improve speed and costs benefit from
optimizations across all these dimensions. To this end, this section
provides an overview of the characteristics of LLM inference, along
with the corresponding systems and hardware requirements.
1.1 LLM Inference
Transformer models have revolutionized the landscape of LLMs
by introducing a highly effective architecture for natural language
processing tasks, as shown in Figure 1 [ 112]. These models, charac-
terized by their attention mechanisms, have significantly enhanced
the capacity of models to understand and generate human-like text.
Their versatility and scalability in training have established them
6605
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Youngsuk Park & Kailash Budhathoki et al.
as the backbone of many state-of-the-art LLMs today. Transformer
models can include an encoding component only (e.g., BERT [ 27]),
a decoding component only (e.g., GPT [ 13], Llama-2 [ 109], Claude
3 [8], Groq-1 [ 118], Mistral 7B [ 49]), or both (e.g., BART [ 65]). Cur-
rently, modern LLMs predominantly employ a decoder-only archi-
tecture, generating output sequences by predicting one token at a
time, conditioned on the input sequence and previously generated
tokens‚Äîa process known as auto-regression. Consequently, our
discussion primarily focuses on decoder-only Transformer models.
Figure 1: Original Transformer architecture adopted from
[112, Figure 1], comprising of an encoder (left) and a de-
coder (right). Tokens are initially encoded into an embed-
ding space and a positional encoding is used to encode in-
formation about the token positions. Modern LLM architec-
tures are decoder-only with a backbone built of repeated
layers containing masked attention and a feed forward neu-
ral network (FFN). The masked attention first applies lin-
ear transformations on a sequence of embeddings to obtain
query (ùëÑ), key (ùêæ), and value ( ùëâ) matrices and computes
Attention(ùëÑ,ùêæ,ùëâ)=softmax(ùëÑùêæùëá
‚àöùëëùëò)ùëâthus relating the tokens
to each other (the mask enforces that tokens can only attend
to their predecessors). The FFN is applied on each token inde-
pendently. Both attention and FFN add their outputs onto the
embedding, which is passed through the skip connections.
As the model parameters of LLM increases, the decoding phase
of LLM inference is inherently memory-bound due to its low arith-
metic intensity, meaning that loading and moving the model weights
into the on chip memory takes significantly more time than the
actual computations. This challenge becomes particularly acutewith small batch sizes. LLMs have a large memory footprint, pri-
marily due to the pre-trained model weights and intermediate states
required for next-token generation, such as the key-value cache.
1.2 Computational and Memory Requirements
Modern computer chips employ specialized tensor units to effi-
ciently perform tensor computations, such as matrix multiplication,
which are fundamental in large foundation model workloads. Ex-
amples of these units include Nvidia TensorCore [ 86], AMD Ma-
trixCore [ 4], and the systolic arrays found in Google TPU [ 50,52]
and AWS Trainium [ 14]. These tensor units are designed to process
high-performance tensor computations such as matrix multiplica-
tion to meet the extensive demands of LLM workloads, especially
during the training phase.
Inference tasks, however, present a distinct challenge, as pow-
erful tensor units alone are insufficient for optimal performance.
To address memory-bound during decoding process, modern chips
incorporate high-bandwidth memory, typically in the form of Static
Random Access Memory (SRAM). SRAM offers low latency and
high throughput, suitable for the substantial memory requirements
of inference workloads. However, the high cost of SRAM limits its
capacity, requiring careful data manipulation to optimize its usage.
High performance kernels. Inference-purposed kernels, such as
DeepSpeed-Inference [ 6], FasterTransformer [ 84], and transformers-
neuronx [ 11], adhere to these guidelines to efficiently process the
workloads. They can be designed by experienced performance-
tuning experts or generated by machine learning compilers. In
either case, a deep understanding of both chip architecture and in-
ference workloads is essential for efficiently mapping and schedul-
ing computations onto the hardware. By leveraging this knowledge,
these kernels can fully optimize the utilization of high-bandwidth
memory and tensor units, ultimately enhancing the efficiency of
inference workloads on modern computer chips.
Hardware Accelerators. While the majority of the LLM workloads
are now done on GPUs following the SIMT (single instruction, mul-
tiple threads) paradigm, LLM inference actually can also be acceler-
ated with systolic array and High Bandwidth Memory (HBM) based
systems (e.g. Google TPUs [ 50,52], AWS Trainium/Inferentia [ 14]
and Intel Gaudi [ 41]) with lower power consumption and lower
cost accordingly. Systolic array based systems can accelerate matrix
multiplication with instruction-level parallelism [ 51]. To accelerate
memory access speed of a large amount of data, HBM is used as
a replacement of Double Data Rate (DDR) and careful memory
planning is required as the capacity of HBM is limited compared to
the model size [ 135]. There are also systems that utilize FPGAs [ 66]
for compute acceleration, and systems that utilize inter-node con-
nectivity [137] for large-scale transformer inference.
Techniques to Mitigate Memory Bound. In addition, to mitigate
the memory-bound issues in LLM inference, practitioners employ
various techniques that can be broadly categorized into two main
approaches. First, semantic-preserving methods aim to reduce mem-
ory usage while maintaining the original prediction via system opti-
mization (Section 2). Examples includes KV caches [ 90], FlashAtten-
tion [ 24], and FlashDecoding [ 91]. Conversely, architectural/algorithmic
6606Inference Optimization of Foundation Models on AI Accelerators KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
optimization usually trade off some prediction accuracy for im-
proved memory efficiency and inference speed (Section 3 and Sec-
tion 4). These includes grouped query attention (GQA) [ 2], Mix-
ture of Experts (MoE) [ 104] architectures as well as compression
methods of quantization, pruning and distillation, and speculative
decoding [18].
1.3 Distributed Solution Frameworks
The memory-bound nature of LLM inference and the limited capac-
ity of HBM on individual accelerators present significant challenges
in meeting the growing demands of LLM workloads. LLMs with
hundreds of billions of parameters typically do not fit on a single
node for inference, let alone a single accelerator. Consequently,
a distributed solution becomes necessary. However, implement-
ing such a solution for LLM inference introduces challenges like
efficient model partitioning, communication, and load balancing.
Addressing these challenges is crucial for enabling scalable pro-
cessing of large-scale LLM inference workloads. Typically, we can
employ a combination of multiple parallel strategies to achieve
state-of-the-art performance for LLM inference, each with its own
advantages and disadvantages.
Tensor parallelism is designed to distribute large chunks of ten-
sor computation workloads across multiple accelerators and aggre-
gate the final results via collective communication. This approach
can help reduce end-to-end latency when collective communica-
tion is efficient (e.g., NVIDIA NVLink [ 30], AWS Neuron Collective
Communication [ 10]). However, if the tensor computation work-
load is small, the extra overhead in collective communication can
diminish overall performance. Since inter-node communication is
typically higher than intra-node communication, tensor parallelism
is most effectively utilized within a single node.
Pipeline parallelism is employed to distribute model layers
across accelerators. As both model weights and KV cache for each
layer can be distributed to different accelerators, and only the in-
puts/outputs of the layers need to be transferred across devices,
pipeline parallelism is relatively independent of the collective com-
munication bandwidth. This strategy allows for the distribution of
models that are too large for a single node. To increase hardware
utilization, overlapping different pipeline stages is typically nec-
essary. Pipeline parallelism is preferable over tensor parallelism
when the entire model does not fit on a single node for inference.
Sequence parallelism [68] is a critical technique for supporting
long context. The core concept of sequence parallelism involves
distributing sequences along the sequence dimension, enabling the
parallel decoding of small batches of long sequences. This tech-
nique is implemented by solutions such as FlashDecoding [ 91] and
PagedAttention V2 [58].
Expert parallelism (EP) facilitates the distribution of Mixture
of Expert (MoE) models [ 104] across multiple accelerators. The MoE
model architecture is designed to skip inactive expert computation,
while still maintaining the capability to achieve high accuracy com-
pared to dense models. Since expert weights are typically large,
distributing and dynamically loading these weights can be costly.
To reduce collective communication and avoid the dynamic loading
of expert weights, EP keeps each expert within a small group of
accelerators [ 92]. As the input/output data is considerably smallerthan expert weights, the all-to-all collective communication can be
efficiently used to distribute tokens to the activated experts.
2 SYSTEM OPTIMIZATION
We explores semantic-preserving optimizations for LLM inference
from a systems perspective. By strategically organizing computa-
tions, significant improvements in inference speed and memory
efficiency can be achieved without compromising the semantic
integrity of the model. In this seciton, we discuss on reducing
redundant computations through the use of key-value caches (Sec-
tion 2.1), optimizing attention implementation to minimize memory
access (Section 2.2), enhancing throughput via handling batches
of requests (Section 2.3), and reducing unused memory fragmenta-
tion via distributing sequences (Section 2.4). These optimizations
were mainly developed based on GPUs, but the main concepts are
largely applicable to other AI accelerators with some specific im-
plementation tweak. The following subsections delve into each of
these approaches in detail, examining their theoretical foundations,
practical implementations and challenges therein.
2.1 Fast Attention Computation via Caching
Generating tokens in an autoregressive fashion is a widely adopted
approach like GPT [ 13] and Llama [ 109], yet it can pose computa-
tional challenges. During the auto-regressive generation, decoding
step to generate every next token requires to fetch previous tokens.
This requires to compute their hidden representation of keys and
values in attention mechanism, which could be repetitive during
the sequence of token generation. KV- cache [ 90] stores and reuses
these past key-value pairs, eliminating the need of recalculation
for every new token. This technique significantly improves the
efficiency of inference, by reducing the quadratic complexity of
attention computation w.r.t. a sequence length to be linear.
However, the memory footprint of KV cache growing linearly
w.r.t. the sequence length can be substantial, as it requires addi-
tional memory to store the cached keys and values. To address
this, several techniques has been introduced to reduce the memory
space required for the KV cache. Low-bit precision data types have
been utilized in KVQuant [ 45], which brings million-scale context
length support on a single A100-80G GPU. StreamingLLM [ 123]
introduced the concept of attention sink, which preserves decent ac-
curacy by leveraging initial tokens without exhausting the long con-
text window size. Generalized block-sparse attention patterns, e.g.
BigBird [ 128]), allow the training of long context support, without
degrading accuracy at inference stage. Heavy-Hitter Oracle [ 134]
is a cache eviction policy which retains Heavy Hitters tokens, i.e.,
tokens contributing most of the value in attention scores, based on
local statistics at each decoding step. However, all of these can lead
to a potential degradation of accuracy.
The aforementioned KV cache strategies can be implemented
differently depending on hardware. To be specific, the KV cache
memory space size can be formulated as 2ùëèùë†‚Ñéùëëùëôùëõ bytes, where ùëèis
batch size,ùë†is sequence length, ‚Ñéis number of KV heads, ùëëis size
of the attention head, ùëôis the number of layers, ùëõis size of each data
element in number of bytes. The size of ùëèis determined at runtime
for batch inference. ùëëandùëôare fixed by the model configuration.
This leaves the optimization space for reducing KV cache memory
6607KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Youngsuk Park & Kailash Budhathoki et al.
space being limited to ùë†,‚Ñéandùëõ. KV cache quantization helps the
reduction of ùëõ. Block-sparse attention techniques help minimize
ùë†and‚Ñé. With all considered, the distributed strategy of KV cache
memory can be distinct among GPU and systolic array-based accel-
erators (e.g., TPU, Trainium) due to different memory constraints
and the numbers of devices per node, especially for handling GQA
models (Section 3.1).
PagedAttention [ 59] can be considered as a KV cache optimiza-
tion. It transforms the KV cache into non-contiguous memory space,
and makesùë†as a fixed block size. Each sequence can occupy a vari-
able number of KV cache blocks. SGLang [ 136] further transforms
the fixed block size to variable length, with RadixAttention enabling
automatic KV cache reuse. Both PagedAttention and RadixAtten-
tion enabled the possibility to cache shared prefix among multiple
sequences, without duplicated copy of the prefix.
2.2 Efficient Attention Computation
Figure 2: Flash Attention by Dao et al . [24] . The outer loop
iterates over K and V blocks and loads them to fast SRAM. In
each block, inner loops iterates over Q blocks, loading them
to SRAM, and writing the attention output back to HBM.
Modern LLMs have extended the support of context length from
the order of thousands to millions within a few years from less than
1k (e.g., GPT-2 [ 13]) to 200k+ (e.g., Claude 3 [ 8]). The main challenge
of expanding the context window lies in the extensive computa-
tional requirements and memory consumption for the attention
computation. As the model considers more tokens simultaneously,
the compute/time complexity and memory demands of calculations
increase significantly, scaling quadratically with the size of the
context window. FlashAttention [ 23,24] was introduced to address
these challenges, which reformulates the attention computation as a
sequence of matrix multiplications and applies block-sparse decom-
position. By processing attention in smaller blocks, FlashAttention
reduces the memory footprint of attention computation, avoiding
the need to materialize the entire attention matrix in memory at
once. The key advantage of FlashAttention is its ability to minimize
data movement between different memory hierarchies. By care-
fully selecting the block size based on the memory hierarchy and
capacity of the device, FlashAttention ensures that the data can be
efficiently processed without requiring multiple transfers between
memory levels. For example, on GPUs, the block size is typically
small to fit within the L2 cache, minimizing expensive memoryaccesses. In contrast, devices like AWS Trainium or Google TPU,
which have a large scratchpad memory in the tens of megabytes
(MBs), can leverage larger block sizes to maximize computational
efficiency by processing more data in parallel.
For large context, Blockwise Parallel Transformer (BPT) [ 72]
further minimize memory consumption on feedforward network
by computing them in a block-wise manner. Enhancing BPT, Ring
Attention [ 73] utilizes blockwise computation for self-attention
and feedforward processes to distribute extended sequences across
multiple devices by dividing the input text into smaller, more man-
ageable blocks. These blocks are processed on separate devices
organized in a ring-like configuration, enabling parallel processing.
When it comes to inference compared with training, relatively
smaller batch size can lead to different bottleneck. Flash-Decoding
[91], based on FlashAttention, introduces a new parallelization di-
mension: the keys/values sequence length. It stores minimal extra
data in global memory while fully utilizing the accelerator, even
with small batch sizes, provided the context length is sufficiently
large. For the smaller chunks of split keys/values, it computes the
attention of the query with each chunk in parallel using FlashAt-
tention, and reduce across all chunks to calculate the final output.
2.3 Continuous Batching
LLM inference is inherently memory-bound if only one sequence is
processed. To increase the throughput for a large number of input
prompts, the most straightforward approach was to allocate a fixed
time window for decoding a fixed number of sequences. This is
commonly known as static batching, which has been implemented
in FasterTransformer [ 84] and many others [ 11,90]. The advantage
of static batching comes from the minimized latency for decod-
ing with small batch sizes. As batch size gets bigger to achieve
higher throughput, a mechanism in improving effective utilization
of batched decoding is needed.
Static batching results in resource waste as some sequences reach
the end earlier than the others in the same batch. Orca [ 31] proposed
the idea of a dynamic sequence eviction strategy. The strategy essen-
tially removes the sequences that generated EOS token, and inserts
new prompts into the decoding batch. The approach is commonly
referred to as continuous batching. In addition to the proposed
mechanism in handling continuous batching, Ocra also introduced
the idea of flattening multiple input prompts and concatenate them
into the prefill kernel, in order to reduce padding and kernel launch
overhead. The block diagonal causal attention mask is commonly
used to achieve a throughput gain with FlashAttention.
2.4 PagedAttention and its Derived Applications
Since the length of output tokens is unpredictable, the most straight-
forward approach was to maintain the maximal sequence length
for each decoding request. As most part of the reserved memory
won‚Äôt be actually used, this would introduce a large amount of
internal memory fragmentation. As illustrated in the Figure 3, in-
ternal memory fragmentation refers to the memory space that is
allocated but not effectively utilized for sequence decoding. Exter-
nal memory fragmentation indicates the device memory space that
is free but not allocated for usage. To reduce both internal and ex-
ternal memory fragmentation, PagedAttention [ 59] introduced the
6608Inference Optimization of Foundation Models on AI Accelerators KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 3: Types of memory fragmentation by Kwon et al . [59] . The figure depicts the memory space for decoding two sequences.
Internal memory fragmentation is considered to be the allocated KV cache blocks that are not occupied by the sequences. The
free memory space that is not allocated is considered to be external memory fragmentation.
idea of distributing sequences in non-contiguous physical memory
space. The capability of serving LLM with PagedAttention has been
demonstrated with the vLLM project [ 113], NVIDIA TensorRT-LLM
[85], and HuggingFace TGI [ 47]. Since the initial software release
of the vLLM project, a few extensions have been added as part of
the improvement:
‚Ä¢FP8 (E5M2/E4M3) data type [ 82] for KV cache storage. FP8
storage data type for the KV cache helps increase compute inten-
sity in decoding stage, and mitigate the memory-bound decoding
problem. It can also help increase batch size while maintaining
same amount of KV cache payload, comparing to FP16/BF16 KV
cache data type. The throughput benefit can come from the in-
crease of batch size for decoding. The initial support for FP8
KV cache quantization [ 125] in vLLM reported 1.49x through-
put improvement on A100, by trading off up to 2.4% of accuracy
degradation on HumanEval-Python evaluation tasks.
‚Ä¢Structured KV cache storage for shared prefix process-
ing. Recent advancement in context-aware generation has demon-
strated strong reasoning capability in multiple frameworks [ 17,
40,53]. To reduce unnecessary computation while maintaining
strong reasoning capability, Zheng et al . [136] proposed RadixAt-
tention, which utilize radix tree and maintain the tree elements
as sequences with varying lengths. It also introduces a compiler
optimization framework to achieve longer shareable prefixes for
caching.
‚Ä¢Reduce the interruption of input prompt encoding. In order
to reduce high tail latency in decoding phase due to long context
inputs, Agrawal et al . [1] proposed the idea of distributing long
context inputs into separate chunks of processing steps. It utilizes
the chunked prefill kernel, which was initially proposed to reduce
pipeline bubble for multi-GPU serving. It increases the stability
in decoding latency via stall-free decoding, and improved end-
to-end throughput by up to 1.33x.
3 STRUCTURED TRANSFORMER
ARCHITECTURES
Beyond optimizing the serving of a given model, also the model
architectures themselves have developed and moved towards archi-
tectures that enable faster and more efficient inference, while still
being similarly powerful. In the following we discuss changes to the
attention mechanism, reducing its number of key and value heads(Section 3.1) as well as mixture of experts approaches, which effec-
tively only execute part of the network for each token (Section 3.2),
in addition to other architecture choices (Section 3.3).
3.1 Multi-/Grouped Query Attention
Falcon [ 3] and Llama 2 70B [ 109] employ techniques known as
multi-query attention (MQA) [ 103] and grouped-query attention
(GQA) [ 2] respectively. When it comes to inference, memory and
computational challenges arise from the repeated loading of decoder
weights and attention keys/values in decoding steps.
Figure 4: Overview of grouped-query method by Ainslie
et al. [2]. Multi-head attention has H query, key, and value
heads. Multi-query attention shares single key and value
heads across all query heads. Grouped-query attention in-
stead shares single key and value heads for each group of
query heads, interpolating between multi-head and multi-
query attention.
In multi-head attention, distinct queries brings linear increase on
the number of heads for keys and values, requiring larger memory
bound and prohibiting potential latency improvement. However,
MQA involves employing multiple query heads alongside a single
key/value head, thereby accelerating decoder inference. GQA an
advancement over MQA, strikes a balance by utilizing an interme-
diate number of key-value heads (more than one but fewer than the
query heads). The GQA model efficiently partitions the query into
ùëõheads segments akin to the original multi-head attention mecha-
nism, while dividing the key and value into handful of groups. For
example, Llama-3 70B [ 109] uses 64 query heads which are grouped
onto 8 key-value heads. This arrangement allows a handful of query
heads to share the same key-value heads to interact. By leverag-
ing repeated key-value pairs, the GQA approach enhances overall
model performance while preserving quality.
When it comes to the MQA/GQA inference strategy in a dis-
tributed setting, there are a number of approaches. If possible, the
6609KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Youngsuk Park & Kailash Budhathoki et al.
Figure 5: Instead of the dense feed-forward network layer
in the traditional Transformer (left blue), Fedus et al . [29]
introduce a sparse Switch FFN layer (right blue). This layer
functions independently on the sequence‚Äôs tokens.
common practice is to evenly distribute KV heads across multiple
accelerators. This assumes that the number of KV heads are divisi-
ble by the number of accelerators. For handling the case where there
are more accelerators than number of KV heads, Pope et al . [90] has
introduced the approach that distributes sequences over different
accelerators. The idea is to leverage all-to-all operator to transform
the layout of the hidden states. It can effectively increase static
batching inference throughput when batch size is large and number
of KV heads is small. Regarding the support of PagedAttention [ 59],
the best practice is yet to be explored, since KV cache block place-
ment is determined at runtime. It won‚Äôt be effective if number of KV
cache blocks are imbalanced among accelerators. Existing solutions
either shard along sequence dimension (e.g. PagedAttention V2),
or replicate the KV heads for each of accelerators.
3.2 Mixture of Experts for Transformer
Mixture of Experts (MoE) [ 104] architecture from Figure 5 is de-
signed to activate part of expert computation by skipping inactive
ones, while maintaining the capability in achieving high accuracy.
This allows for pretrained models to utilize significantly less compu-
tational resources and thus increase the model‚Äôs size or the dataset
it handles within the same computational budget compared to
a dense model both in training and inference. This MoE compo-
nent becames a popular design choice in favor of fast inference
among Transformer class [ 29,49,62]. Among many variance of
MoE [ 36,55,64,133,138,141], it typically tries to comprise two
primary components: First, sparse MoE layers replace conventional
dense feed-forward network (FFN) layers. These MoE layers are
comprised of a set number of "experts" (e.g., 8 in Mistral [ 49]), where
each expert functions as an individual neural network. While these
experts are typically FFNs in practice, they can also encompass more
intricate networks or even form a hierarchical MoE structure [ 34].
Second, a gate network or router determines the allocation of to-
kens to specific experts. Notably, tokens can be directed to multiple
experts. This decision is governed by the routing mechanism as
a critical design choice for efficient inference and training. The
router, comprising learned parameters, is pretrained concurrentlywith the remainder of the network and plays a pivotal role in token
allocation within MoEs.
Routers for sparse MoEs can be categorized into two main vari-
ants: Token Choice, which assigns experts to individual tokens, and
Expert Choice, which assigns tokens to individual experts. Token
Choice can be optimal for latency constrained applications, since
the number of activated experts is small. Expert Choice is used for
throughput optimizations, especially when total number of experts
are small and the tokens can be balance among all experts. In such
applications, Expert parallelism (EP) keeps each expert within a
small group of accelerators, leading fast inference by alleviating
collective communications and dynamic loading.
3.3 Other Architectures
Sliding Window Transformer (SWT) [ 12] is a variant of the self-
attention mechanism designed to handle long sequences more ef-
ficiently by dividing the input sequence into smaller, overlapping
chunks or "windows." For each token, the attention score is com-
puted only over a window of length ùë§sequence rather than the
entire (previous) sequence. This attention mechanism sequentially
slides across the input sequence to compute all localized attention
scores. As the layers of the SWT get deeper, the localized attention
mechanism extends the receptive fields w.r.t. input tokens, preserv-
ing a comprehensive understanding of the entire sequence, similar
to a CNN. Each SWT requires only linear complexity ùëÇ(ùëõùë§), miti-
gating the quadratic complexity ùëÇ(ùëõ2)in standard self-attention.
Mixture-of-Depth [93] allows some tokens to take paths across
layers dynamically, skipping certain layers based on specific criteria,
e.g., CALM [ 99] with exit criteria during forward pass, instead
of all tokens passing through every layer of Transformer. This
approach enables the model to allocate computational resources
more efficiently, focusing more layers on complex parts of the input
while using fewer layers for simpler parts. The mixture of depth can
help reduce computational costs and improve forward/backward
speed without significantly compromising model performance.
4 MODEL COMPRESSION
Model compression techniques [ 22] compress a model or input,
thereby reducing the memory footprint and latency of LLMs. These
methods come with challenges as they typically introduce trade-
offs between inference improvement and accuracy. Quantization
of model weights (Section 4.1) has essentially has become a stan-
dard nowadays. Pruning parts of models has posed more challenges
but also seen much progress targeted specifically to LLMs (Sec-
tion 4.2). Lastly, entirely compressed models can be trained through
distillation from a large teacher model (Section 4.3).
4.1 Quantization
Quantization [ 37] is a model-compression technique that represents
weights or activations of the network with low-precision data types
(e.g., 8-bit integer) instead of high-precision data types (e.g., FP32),
therewith reducing the storage when loading the model/activations
in hardware (see Figure 6). Reduced data precision poses trade-off
between latency-throughput-accuracy. It also requires support from
the target hardware to realize maximum speedup [ 111]. Quantiza-
tion is applied either during training or after training.
6610Inference Optimization of Foundation Models on AI Accelerators KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Figure 6: INT8 quantization [ 25,122] represents weights or
activations in FP32 data types into 8-bit integers.
Post-training quantization (PTQ) methods quantize weights or
activations of a pre-trained model (e.g., LLM.int8() [ 25], ZeroQuant-
V2 [126], SmoothQuant [ 122], GPTQ [ 33], Quip# [ 110], OmniQuant [ 102],
AQLM [ 28], PV-Tuning [ 79], Outlier Suppression+ [ 116], QLoRA [ 26]).
Additional fine-tuning is often needed to recover the downstream
accuracy drop [57].
Quantization aware training (QAT) emulates inference-time quan-
tization, creating a model that can be quantized later post-training
(e.g., 1-bit LLM [ 115], 1.58-bit LLM [ 48,77], LLM-QAT [ 75], QLLM [ 74]).
They stipulate full pre-training of the base model. A promising re-
cent work [ 77] reported accuracy of a model with only ternary
{‚àí1,0,1}weights on par with a full-precision model.
The upper bound for both latency and throughput improvement
from weight-only quantization is the ratio of source precision data
type to the target precision data type. For example, the upper bound
for latency/throughput improvement with INT8 quantization down
from 32-bit floating point format (FP32) is 4√ó. As INT8 parameters
require 4√ófewer bits than FP32, we can increase the batch size
as well as perform more computations on the same data size in
one go. But memory saving does not directly translate to improved
throughput/latency due to several factors like memory bandwidth,
hardware limitations, and quantization/de-quantization overhead.
4.2 Pruning
Pruning is a compression technique to remove redundant param-
eters from the model. The goal is to maintain prediction quality
of the model while shrinking its size, and thereby increasing its
efficiency. Pruning requires strategies to identify which parts to
remove and, potentially, how to adapt the remaining parts in order
to compensate for quality degradation.
Structured Pruning removes whole components of the network
such as neurons, attention heads, and layers [ 9,56,78,108,121,130].
For example, SliceGPT [9] effectively decreases the embedding di-
mension of the model, whereas LLM-Pruner [78] scores coupled
structures in the decoder-layer and removes the least important
ones. Sheared Llama [120] and LoRAPrune [130] similarly remove
entire structures. When pruning larger structures like channels,
blocks, or embedding dimensions, the speedups can easily be real-
ized end-to-end (e.g., [78, Table 3], [9, Table 2]).
Unstructured Pruning removes individual weights of the network.
Clearly, weights that are 0 can be ignored without any loss in ac-
curacy, but also very small weights can be set to zero. Pruning
weights that are not small enough will finally lead to degradation
of the model, which sets the limit for the speedup. Given a desired
sparsity ratio and a matrix ùëä, the simplest strategy is to prune
the weights with the smallest magnitude, which corresponds tominimizing the Frobenius norm between the dense matrix ùëäand
its sparse approximation bùëä, i.e.,‚à•ùëä‚àíbùëä‚à•2
ùêπ. This approach, re-
ferred to as magnitude pruning, quickly leads to drastic accuracy
degradation [ 32,106].Wanda [106] and RIA[132] improve over
simple magnitude pruning by reweighing the matrix weights with
the norm of the corresponding input activation. Another popular
Transformer pruning method is SparseGPT [32], which jointly opti-
mizes the pruning mask as well as the remaining weights in order
to minimize‚à•(ùëä‚àíbùëä)ùëã‚à•2
ùêπ, whereùëãrepresents a sample of inputs
to the linear layer. Since finding the optimal pruning mask is a
combinatorial problem, SparseGPT employs heuristics to make it
computationally feasible. While most methods apply the sparsity
uniformly across layers, owl[127], BESA [ 124], and ISC[101] derive
a criteria to prune layers to different levels.
Unstructured sparsity is mainly of academic interest since, so
far, it does not lead to speedup on hardware accelerators (Flash-
LLM [119] recently provided some steps in this direction). However,
most methods can also be applied to achieve N:M structured sparsity,
where onlyùëÅout ofùëÄconsecutive elements are allowed to be non-
zero. Some hardware accelerators support these patterns and allow
for memory savings and speedups [89].
While pruning can in principle be done during pretraining [ 61,
131,140], most recent work focuses on the post-training setting.
Nonetheless, in order to recover from the accuracy loss due to
pruning many works consider applying a short training strategy
after pruning. This is either done via a standard pretraining loss
[78,106] or with variants of distillation losses [ 56,60,97,108,120].
To increase efficiency, some works do not update all remaining
parameters, but employ parameter efficient techniques like LoRA
[46]. Generally, such strategies help recovering the accuracy loss,
but are also prone to overfitting to the specific dataset used [ 78]
and can compromise the generality of the model.
4.3 Distillation
Knowledge distillation (KD) ([15, 44, 76, 117]) is a model compres-
sion technique in which we train a small model (called student)
to match closely the performance of a larger model or an ensem-
ble of models (called teacher). To this end, KD connects a student
model with the teacher model by a distillation loss, which penal-
izes differences in the outputs of the two models at certain layers
(see Figure 7). The standard KD approach‚Äîalso called last-layer-
only approach‚Äîtrains the student to match the performance of
the teacher at the last layer (e.g., [ 44,96]). Another approach‚Äîalso
called layer-wise approach‚Äîtrains the student to match the hidden
representation of the teacher at each layer (e.g., [ 107]). Layer-wise-
distillation approaches report improved results on downstream
tasks compared to last-layer-distillation approaches [71], but they
stipulate the same number of layers in the student as the teacher. In
general, KD approaches are flexible with regard to the exact struc-
ture of the student model, which allows optimizing the student for
various target hardwares. Another advantage is that the distillation
process runs entirely after training the large teacher model.
Distillation does not affect the training of a teacher model, but
distillation effort by itself can be a major training effort for the
following reasons. First, the number of steps can be similar to
pre-training a small model. Second, the distillation loss usually is
6611KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Youngsuk Park & Kailash Budhathoki et al.
Figure 7: Canonical knowledge distillation process by Hinton
et al. [44] , where small student model distills a large teacher
model via minimizing a distillation loss. This loss on a trans-
fer dataset is then backpropagated to the student model.
a combination of the pure student/teacher loss together with an
original loss, for which typically the original pre-training data is
recommended [ 44]. To compute the distillation loss, we also need
to make a forward pass of a teacher model to get logits. But there
is a range of possibilities in selecting the transfer set on which to
train the smaller distilled model [ 87]. For example, symbolic distilla-
tion [ 75,117] approaches synthesize data from the teacher model to
this end. Distillation also comes with a trade-off between size and
quality, which determines the improvement in throughput/latency.
5 FAST DECODING
As discussed, vanilla auto-regressive decoding is memory bound.
Speculative decoding (SD) [ 18,63] exploits the fact that multiple
draft tokens can be verified in a single forward pass of the target
model. The draft tokens are then accepted based on a rejection
sampling scheme [ 18,63] or deterministic approaches [ 54]. Pro-
cessing the draft tokens requires additional computations in the
target model, but the main bottleneck remains the loading of the
weights. Hence, the verification of the additional draft tokens comes
at negligible additional latency. But once draft tokens are accepted,
multiple tokens are decoded with a single call to the target model,
resulting in an overall latency reduction. Noticeably also, opposed
to the compression techniques in Section 4, the output distribution
provably remains the same [18, Theorem 1].
Beyond the verification also the draft token generation adds to
the latency. We classify SD methods broadly into two categories,
based on whether or not they use a separate model for drafting.
Seminal work [ 63] uses a smaller model from the target model‚Äôs
family as draft model, e.g., T5-small as the draft model for T5-XXL,
whereas Chen et al . [19] train a separate draft model from scratch.
Choosing an appropriate draft model for a target model can be
tricky. In light of this, some SD methods take advantage of the target
model itself. For example, self-speculative decoding [ 129] drafts
tokens using the target model but skips some of its intermediate
layers. Medusa [ 16] trains multiple feed-forward heads on top of the
last Transformer layer. The ùëñ-th head is responsible for predicting
(ùë°+ùëñ)-th token into the future. EAGLE [ 70] improves the heads
by introducing auto-regression on features at the last Transformer
layer. PaSS[ 83] appendsùëòspecial ‚Äúlook-ahead" tokens to the prompt
as input, and generates ùëòdraft tokens in parallel using the target
model itself. Lookahead Decoding [ 35] applies Jacobi method [ 98,105] that drafts multiple tokens in parallel. In some applications
(e.g., Question Answering), one can draft tokens by matching their
prefix in a document [5], or a database [43].
There are two orthogonal paths to further speed up speculative
decoding. One is to draft multiple sequences for verification. The
other is to improve the acceptance rate. We elaborate on them next.
Multiple Drafted Sequences. In the vanilla case of a single drafted
sequence, all drafted tokens after the first rejection position are
wasted. In contrast, drafting multiple sequences increases the chance
of having a longer accepted sub-sequence. The multiple sequences
are often organized in a tree structure to share some prefixes. Cor-
respondingly, the verification is made more efficient by introducing
tree attention, with a specialized attention mask that reflects the
token dependencies in the tree. This approach is first proposed
in SpecInfer [ 81], adopted in several aforementioned papers (e.g.,
Medusa [16], EAGLE [70], Lookahead Decoding [35]), and further
developed by Chen et al . [21] . Depending on the model architec-
tures, speedups reported are often in 2‚àí3√ó.
Aligning Draft to Target Model. In [63], the rejection rate is shown
to be equal to the Total Variation divergence (TV-div ) between target
and draft models‚Äô token probabilities. This neat theoretical result
has motivated Distillspec [ 139] to knowledge distill from the tar-
get to draft model. With the better aligned draft model, 10‚àí45%
further speedups are reported. Regarding the objective function
for distillation, it could be either conventional Kullback‚ÄìLeibler
divergence (KL-div ) or the more relevant TV-div. Note that KL-div
can be considered as a surrogate for TV-div due to Pinsker‚Äôs inequal-
ity. Interestingly, [ 139] does not observe an obvious advantage of
TV-div against KL-div.
6 CONCLUSION
This paper provides a comprehensive overview of efficient infer-
ence methods for LLMs, covering system optimization, structured
Transformer architectures, model compression, and algorithmically
faster decoding, especially in the context of AI accelerator. These
techniques aim to facilitate effective computation, often considering
input-output (IO) communication during attention score calcula-
tion, reducing extensive and repetitive self-attention mechanisms,
minimizing memory idleness and compressing models themselves.
Inference optimization is not only crucial for Transformer-based
LLMs, but also other foundation models like Stable Diffusion [ 94] or
the Transformer alternative of State Space Models (SSMs) [ 38,39].
Several of the techniques presented in this paper have been suc-
cessfully applied to these models too; e.g., in Stable Diffusion with
FlashAttention [ 20], quantization [ 42,69,100,114], sparsity [ 67],
and distillation [ 80,95], or in SSMs with Mixture of Experts [ 7,88].
Nevertheless, many of the challenges remain largely unresolved,
particularly when dealing with extremely long context lengths and
sequences, necessitating tailored efforts depending on the types of
devices used. We are confident that researchers and developers will
continue to strive towards narrowing these gaps, thereby enhancing
the accessibility of Generative AI systems.
REFERENCES
[1]Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra,
Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming
6612Inference Optimization of Foundation Models on AI Accelerators KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. arXiv
preprint arXiv:2403.02310 (2024).
[2]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
Lebr√≥n, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query trans-
former models from multi-head checkpoints. arXiv preprint arXiv:2305.13245
(2023).
[3]Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cap-
pelli, Ruxandra Cojocaru, M√©rouane Debbah, √âtienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, et al .2023. The falcon series of open language
models. arXiv preprint arXiv:2311.16867 (2023).
[4]AMD. 2024. AMD matrix cores. https://rocm.blogs.amd.com/software-tools-
optimization/matrix-cores/README.html
[5]N. Yang amd T. Ge, L. Wang, B. Jiao, D. Jiang, L. Yang, R. Majumder, and F.
Wei. 2023. Inference with Reference: Lossless Acceleration of Large Language
Models. arXiv preprint arXiv:2304.04487 (2023).
[6]Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng
Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff
Rasley, et al .2022. Deepspeed-inference: enabling efficient inference of trans-
former models at unprecedented scale. In SC22: International Conference for
High Performance Computing, Networking, Storage and Analysis. IEEE, 1‚Äì15.
[7]Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Mil-
lidge. 2024. BlackMamba: Mixture of Experts for State-Space Models.
arXiv:2402.01771 [cs.CL]
[8]Anthropic. 2024. Introducing the Next Generation of Claude. https://www.
anthropic.com/news/claude-3-family
[9]Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten
Hoefler, and James Hensman. 2024. SliceGPT: Compress Large Language Models
by Deleting Rows and Columns. In The Twelfth International Conference on
Learning Representations. https://openreview.net/forum?id=vXxardq6db
[10] AWS-Neuron. 2024. https://awsdocs-neuron.readthedocs-
hosted.com/en/latest/general/arch/neuron-features/collective-
communication.html.
[11] AWS-Neuron. 2024. https://github.com/aws-neuron/transformers-neuronx.
[12] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-
document transformer. arXiv preprint arXiv:2004.05150 (2020).
[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877‚Äì1901.
[14] Nafea Bshara. 2024. AWS Trainium: The Journey for Designing and Optimization
Full Stack ML Hardware. In Proceedings of the 29th ACM International Conference
on Architectural Support for Programming Languages and Operating Systems,
Volume 3. 4‚Äì4.
[15] Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model
compression.. In KDD. ACM, 535‚Äì541.
[16] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. 2024. Medusa:
Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.
arXiv preprint arXiv:2401.10774 (2024).
[17] Harrison Chase. 2022. LangChain. https://github.com/langchain-ai/langchain
[18] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau-
rent Sifre, and John Jumper. 2023. Accelerating Large Language Model Decoding
with Speculative Sampling. arXiv:2302.01318
[19] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau1,
Laurent Sifre1, and John Jumper. 2023. Accelerating large language model
decoding with speculative sampling. arXiv preprint 2302.01318 (2023).
[20] Y. Chen, R. Sarokin, J. Lee, J. Tang, C. Chang, A. Kulik, and M. Grundmann.
2023. Speed Is All You Need: On-Device Acceleration of Large Diffusion Models
via GPU-Aware Optimizations. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW).
[21] Z. Chen, A. May, R. Svirschevski, Y. Huang, M. Ryabinin, Z. Jia, and B. Chen.
2024. Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding.
arXiv:2402.12374 [cs.CL]
[22] Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vish-
wanath, and Arun K. Somani. 2023. A Survey of Techniques for Optimizing
Transformer Inference. arXiv:2307.07982 [cs.LG]
[23] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and
work partitioning. arXiv preprint arXiv:2307.08691 (2023).
[24] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. Flashat-
tention: Fast and memory-efficient exact attention with io-awareness. Advances
in Neural Information Processing Systems 35 (2022), 16344‚Äì16359.
[25] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.
arXiv:2208.07339 [cs.LG]
[26] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG]
[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 [cs.CL][28] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem
Babenko, and Dan Alistarh. 2024. Extreme Compression of Large Language
Models via Additive Quantization. arXiv:2401.06118 [cs.LG]
[29] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers:
Scaling to trillion parameter models with simple and efficient sparsity. Journal
of Machine Learning Research 23, 120 (2022), 1‚Äì39.
[30] Denis Foley and John Danskin. 2017. Ultra-performance Pascal GPU and NVLink
interconnect. IEEE Micro 37, 2 (2017), 7‚Äì17.
[31] Francesca Fossati, Stephane Rovedakis, and Stefano Secci. 2022. Distributed
algorithms for multi-resource allocation. IEEE Transactions on Parallel and
Distributed Systems 33, 10 (2022), 2524‚Äì2539.
[32] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models
can be accurately pruned in one-shot. In International Conference on Machine
Learning. PMLR, 10323‚Äì10337.
[33] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ:
Accurate Post-Training Quantization for Generative Pre-trained Transformers.
arXiv:2210.17323 [cs.LG]
[34] J√ºrgen Fritsch, Michael Finke, and Alex Waibel. 1996. Adaptively growing
hierarchical mixtures of experts. Advances in Neural Information Processing
Systems 9 (1996).
[35] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequen-
tial dependency of llm inference using lookahead decoding. arXiv preprint
arXiv:2402.02057 (2024).
[36] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. 2022.
Parameter-efficient mixture-of-experts architecture for pre-trained language
models. arXiv preprint arXiv:2203.01104 (2022).
[37] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,
and Kurt Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural
Network Inference. arXiv:2103.13630 [cs.CV]
[38] Albert Gu and Tri Dao. 2024. Mamba: Linear-Time Sequence Modeling with
Selective State Spaces. arXiv:2312.00752 [cs.LG]
[39] Albert Gu, Karan Goel, and Christopher R√©. 2022. EFFICIENTLY MODELING
LONG SEQUENCES WITH STRUCTURED STATE SPACES. In International
Conference on Learning Representations.
[40] Guidance-AI. 2023. A guidance language for controlling large language models.
https://github.com/guidance-ai/guidance. https://github.com/guidance-ai/
guidance
[41] Linley Gwennap. 2019. Habana offers Gaudi for AI training. Microprocessor
Report, Tech. Rep., jun (2019).
[42] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. 2023.
PTQD: Accurate Post-Training Quantization for Diffusion Models. In Advances
in Neural Information Processing Systems.
[43] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. 2023. Rest:
Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252 (2023).
[44] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge
in a Neural Network. arXiv:1503.02531 [stat.ML]
[45] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney,
Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards
10 Million Context Length LLM Inference with KV Cache Quantization. arXiv
preprint arXiv:2401.18079 (2024).
[46] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large
language models. arXiv preprint arXiv:2106.09685 (2021).
[47] HuggingFace. 2023. https://github.com/huggingface/text-generation-inference.
[48] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An-
drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2017. Quantization
and Training of Neural Networks for Efficient Integer-Arithmetic-Only Infer-
ence. arXiv:1712.05877 [cs.LG]
[49] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-
vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,
Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,
and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
[50] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai,
Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al .2023.
Tpu v4: An optically reconfigurable supercomputer for machine learning with
hardware support for embeddings. In Proceedings of the 50th Annual International
Symposium on Computer Architecture. 1‚Äì14.
[51] Norman P Jouppi, Cliff Young, Nishant Patil, and David Patterson. 2018. A
domain-specific architecture for deep neural networks. Commun. ACM 61, 9
(2018), 50‚Äì59.
[52] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al .2017.
In-datacenter performance analysis of a tensor processing unit. In Proceedings
of the 44th annual international symposium on computer architecture. 1‚Äì12.
[53] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav
Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi,
Hanna Moazam, et al .2023. Dspy: Compiling declarative language model calls
6613KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Youngsuk Park & Kailash Budhathoki et al.
into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023).
[54] S. Kim, K. Mangalam, S. Moon, J. Malik, M. Mahoney, A. Gholami, and K. Keutzer.
2024. peculative decoding with big little decoder. Advances in Neural Information
Processing Systems (2024).
[55] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz,
Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2022.
Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv
preprint arXiv:2212.05055 (2022).
[56] Eldar Kurtiƒá, Elias Frantar, and Dan Alistarh. 2023. ZipLM: Inference-
Aware Structured Pruning of Language Models. In Advances in Neu-
ral Information Processing Systems, A. Oh, T. Neumann, A. Globerson,
K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc.,
65597‚Äì65617. https://proceedings.neurips.cc/paper_files/paper/2023/file/
ced46a50befedcb884ccf0cbe8c3ad23-Paper-Conference.pdf
[57] Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim,
Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee.
2022. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of
Large-Scale Pre-Trained Language Models. arXiv:2210.03858 [cs.LG]
[58] Woosuk Kwon. 2023. Implement PagedAttention V2 by WoosukKwon ¬∑Pull
Request #1348 ¬∑vllm-project/vllm. https://github.com/vllm-project/vllm/pull/
1348
[59] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serving with pagedattention.
InProceedings of the 29th Symposium on Operating Systems Principles. 611‚Äì626.
[60] Fran√ßois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. 2021.
Block pruning for faster transformers. arXiv preprint arXiv:2109.04838 (2021).
[61] Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. 2024.
Dynamic Sparse Training with Structured Sparsity. In The Twelfth International
Conference on Learning Representations. https://openreview.net/forum?id=
kOBkxFRKTA
[62] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Fi-
rat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021.
{GS}hard: Scaling Giant Models with Conditional Computation and Automatic
Sharding. In International Conference on Learning Representations. https:
//openreview.net/forum?id=qrwe7XHTmYb
[63] Yaniv Leviathan, Mantan Kalman, and Yossi Matias. 2023. Fast inference from
transformers via speculative decoding. In International Conference on Machine
Learning. PMLR, 19274‚Äì19286.
[64] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
2021. Base layers: Simplifying training of large, sparse models. In International
Conference on Machine Learning. PMLR, 6265‚Äì6274.
[65] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. arXiv:1910.13461 [cs.CL]
[66] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen,
Mimi Xie, Lipeng Wan, Hang Liu, and Caiwen Ding. 2020. Ftrans: energy-
efficient acceleration of transformers using fpga. In Proceedings of the ACM/IEEE
International Symposium on Low Power Electronics and Design. 175‚Äì180.
[67] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu.
2023. Efficient Spatially Sparse Inference for Conditional GANs and Diffusion
Models. arXiv:2211.02048 [cs.CV]
[68] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.
2022. Sequence Parallelism: Long Sequence Training from System Perspec-
tive. arXiv:2105.13120
[69] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang,
Shanghang Zhang, and Kurt Keutzer. 2023. Q-Diffusion: Quantizing Diffusion
Models. arXiv:2302.04304 [cs.CV]
[70] Y. Li, F. Wei, C. Zhang, and H. Zhang. 2024. EAGLE: Speculative Sampling
Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077 (2024).
[71] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo
Zhao. 2023. Less is More: Task-aware Layer-wise Distillation for Language
Model Compression. arXiv:2210.01351 [cs.CL]
[72] Hao Liu and Pieter Abbeel. 2024. Blockwise Parallel Transformers for Large
Context Models. Advances in Neural Information Processing Systems 36 (2024).
[73] Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. Ring attention with blockwise
transformers for near-infinite context. arXiv preprint arXiv:2310.01889 (2023).
[74] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan
Zhuang. 2024. QLLM: Accurate and Efficient Low-Bitwidth Quantization for
Large Language Models. arXiv:2310.08041 [cs.CL]
[75] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023.
LLM-QAT: Data-Free Quantization Aware Training for Large Language Models.
arXiv:2305.17888 [cs.CL]
[76] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner. 2017. Data-Free Knowl-
edge Distillation for Deep Neural Networks. arXiv:1710.07535 [cs.LG][77] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan
Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The Era of 1-bit
LLMs: All Large Language Models are in 1.58 Bits. arXiv:2402.17764 [cs.CL]
[78] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the
structural pruning of large language models. Advances in neural information
processing systems 36 (2023), 21702‚Äì21720.
[79] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Kon-
stantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtarik. 2024. PV-
Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression.
arXiv:2405.14852 [cs.LG]
[80] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon,
Jonathan Ho, and Tim Salimans. 2023. On Distillation of Guided Diffusion
Models. arXiv:2210.03142 [cs.CV]
[81] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Wong, Z. Chen, D.
Arfeen, R. Abhyankar, and Z. Jia. 2023. Specinfer: Accelerating generative llm
serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781 (2023).
[82] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey,
Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Ka-
malu, et al .2022. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433
(2022).
[83] G. Monea, A. Joulin, and E. Grave. 2023. PaSS: Parallel Speculative Sampling.
arXiv preprint arXiv:2311.13581 (2023).
[84] NVIDIA. 2022. https://github.com/NVIDIA/FasterTransformer.
[85] NVIDIA. 2024. https://github.com/NVIDIA/TensorRT-LLM.
[86] NVIDIA. 2024. NVIDIA Tensor Cores. https://www.nvidia.com/en-us/data-
center/tensor-cores/
[87] Charith Peris, Lizhen Tan, Thomas Gueudre, Turan Gojayev, Pan Wei, and
Gokmen Oz. 2022. Knowledge Distillation Transfer Sets and their Impact on
Downstream NLU Tasks. arXiv:2210.04834 [cs.CL]
[88] Maciej Pi√≥ro, Kamil Ciebiera, Krystian Kr√≥l, Jan Ludziejewski, Micha≈Ç Krutul,
Jakub Krajewski, Szymon Antoniak, Piotr Mi≈Ço≈õ, Marek Cygan, and Sebastian
Jaszczur. 2024. MoE-Mamba: Efficient Selective State Space Models with Mixture
of Experts. arXiv:2401.04081 [cs.LG]
[89] Jeff Pool, Abhishek Sawarkar, and Jay Rodge. 2021. Accelerating Inference
with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA Ten-
sorRT. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-
using-ampere-and-tensorrt/
[90] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-
bury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Ef-
ficiently scaling transformer inference. Proceedings of Machine Learning and
Systems 5 (2023).
[91] Princeton-NLP. 2023. https://princeton-nlp.github.io/flash-decoding/.
[92] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani
Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. Deepspeed-
moe: Advancing mixture-of-experts inference and training to power next-
generation ai scale. In International conference on machine learning. PMLR,
18332‚Äì18346.
[93] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway
Humphreys, and Adam Santoro. 2024. Mixture-of-Depths: Dynamically allocat-
ing compute in transformer-based language models. arXiv:2404.02258 [cs.LG]
[94] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn
Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Mod-
els. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). 10684‚Äì10695.
[95] Tim Salimans and Jonathan Ho. 2022. Progressive Distillation for Fast Sampling
of Diffusion Models. arXiv:2202.00512 [cs.LG]
[96] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
arXiv:1910.01108 [cs.CL]
[97] Victor Sanh, Thomas Wolf, and Alexander Rush. 2020. Movement pruning:
Adaptive sparsity by fine-tuning. Advances in neural information processing
systems 33 (2020), 20378‚Äì20389.
[98] A. Santilli, S. Severino, E. Postolache, V. Maiorca, M. Mancusi, R. Marin, and E.
Rodol√†. 2023. Accelerating transformer inference for translation via parallel
decoding. arXiv preprint arXiv:2305.10427 (2023).
[99] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q.
Tran, Yi Tay, and Donald Metzler. 2022. Confident Adaptive Language Modeling.
arXiv:2207.07061 [cs.CL]
[100] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. 2023. Post-
training Quantization on Diffusion Models. arXiv:2211.15736 [cs.CV]
[101] Hang Shao, Bei Liu, Bo Xiao, Ke Zeng, Guanglu Wan, and Yanmin Qian. 2023.
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models.
arXiv preprint arXiv:2310.09499 (2023).
[102] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. Omni-
Quant: Omnidirectionally Calibrated Quantization for Large Language Models.
arXiv:2308.13137 [cs.LG]
6614Inference Optimization of Foundation Models on AI Accelerators KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[103] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need.
arXiv preprint arXiv:1911.02150 (2019).
[104] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).
[105] Y. Song, C. Meng, R. Liao R, and S. Ermo. 2021. Accelerating feedforward
computation via parallel nonlinear equation solving. InInternational Conference
on Machine Learning (2021).
[106] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A simple and effec-
tive pruning approach for large language models. arXiv preprint arXiv:2306.11695
(2023).
[107] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Distil-
lation for BERT Model Compression. arXiv:1908.09355 [cs.CL]
[108] Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo,
and Ngai Wong. 2023. Structured Pruning for Efficient Generative Pre-trained
Language Models. In Findings of the Association for Computational Linguistics:
ACL 2023.
[109] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:
Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]
[110] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christo-
pher De Sa. 2024. QuIP#: Even Better LLM Quantization with Hadamard Inco-
herence and Lattice Codebooks. arXiv:2402.04396 [cs.LG]
[111] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin,
Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga,
and Tijmen Blankevoort. 2023. FP8 versus INT8 for efficient deep learning
inference. arXiv:2303.17951 [cs.LG]
[112] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. Advances in neural information processing systems 30 (2017).
[113] vLLM Contributors. 2024. https://github.com/vllm-project/vllm/.
[114] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen
Lu. 2023. Towards accurate data-free quantization for diffusion models. arXiv
preprint arXiv:2305.18723 (2023).
[115] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao
Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. BitNet: Scaling 1-bit
Transformers for Large Language Models. arXiv:2310.11453 [cs.CL]
[116] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
Jinyang Guo, and Xianglong Liu. 2023. Outlier Suppression+: Accurate quanti-
zation of large language models by equivalent and optimal shifting and scaling.
arXiv:2304.09145 [cs.CL]
[117] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang,
Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic Knowl-
edge Distillation: from General Language Models to Commonsense Models.
arXiv:2110.07178 [cs.CL]
[118] xai. 2024. Open Release of Grok-1. https://x.ai/blog/grok-os
[119] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei
Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-llm: Enabling cost-
effective and highly-efficient large generative model inference with unstructured
sparsity. arXiv preprint arXiv:2309.10285 (2023).
[120] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared
LLaMA: Accelerating Language Model Pre-training via Structured Pruning.
InThe Twelfth International Conference on Learning Representations. https:
//openreview.net/forum?id=09iOdaeOzp
[121] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning
Learns Compact and Accurate Models. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, andAline Villavicencio (Eds.). Association for Computational Linguistics, 1513‚Äì1528.
https://doi.org/10.18653/V1/2022.ACL-LONG.107
[122] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
Han. 2023. SmoothQuant: Accurate and Efficient Post-Training Quantization
for Large Language Models. arXiv:2211.10438 [cs.CL]
[123] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
2023. Efficient streaming language models with attention sinks. arXiv preprint
arXiv:2309.17453 (2023).
[124] Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao,
Fengwei An, Yu Qiao, and Ping Luo. 2024. BESA: Pruning Large Language
Models with Blockwise Parameter-Efficient Sparsity Allocation. arXiv preprint
arXiv:2402.16880 (2024).
[125] Zhao Yang. 2023. Support FP8-E5M2 KV Cache by zhaoyang-star ¬∑Pull Request
#2279 ¬∑vllm-project/vllm. https://github.com/vllm-project/vllm/pull/2279
[126] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. 2023.
ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Compre-
hensive Study to Low Rank Compensation. arXiv:2303.08302 [cs.LG]
[127] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia,
Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. 2023. Outlier
weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to
high sparsity. arXiv preprint arXiv:2310.05175 (2023).
[128] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al.2020. Big bird: Transformers for longer sequences. Advances in neural
information processing systems 33 (2020), 17283‚Äì17297.
[129] J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra. 2023.
Draft & verify: Lossless large language model acceleration via self-speculative
decoding. arXiv preprint arXiv:2309.08168 (2023).
[130] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu,
and Bohan Zhuang. 2023. LoRAPrune: Pruning Meets Low-Rank Parameter-
Efficient Fine-Tuning. arXiv preprint arXiv:2305.18403 (2023).
[131] Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He,
Weizhu Chen, and Tuo Zhao. 2022. PLATON: Pruning Large Transformer
Models with Upper Confidence Bound of Weight Importance. arXiv preprint
arXiv:2206.12562 (2022).
[132] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio
Cannistraci. 2024. Plug-and-Play: An Efficient Post-training Pruning Method
for Large Language Models. In The Twelfth International Conference on Learning
Representations.
[133] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.
2021. Moefication: Transformer feed-forward layers are mixtures of experts.
arXiv preprint arXiv:2110.01786 (2021).
[134] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang
Wang, and Beidi Chen. 2023. H 2O: Heavy-Hitter Oracle for Efficient Generative
Inference of Large Language Models. arXiv:2306.14048
[135] Hongbin Zheng, Sejong Oh, Huiqing Wang, Preston Briggs, Jiading Gai, Ani-
mesh Jain, Yizhi Liu, Rich Heaton, Randy Huang, and Yida Wang. 2020. Opti-
mizing memory-access patterns for deep learning accelerators. arXiv preprint
arXiv:2002.12798 (2020).
[136] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,
Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez,
et al.2023. Efficiently programming large language models using sglang. arXiv
preprint arXiv:2312.07104 (2023).
[137] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,
Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decod-
ing for Goodput-optimized Large Language Model Serving. arXiv preprint
arXiv:2401.09670 (2024).
[138] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao,
Andrew M Dai, Quoc V Le, James Laudon, et al .2022. Mixture-of-experts with
expert choice routing. Advances in Neural Information Processing Systems 35
(2022), 7103‚Äì7114.
[139] Y. Zhou, K. Lyu, A. S. Rawat, A. K. Menon, A. Rostamizadeh, S. Kumar, J. F.
Kagy, and R. Agarwal. 2023. Distillspec: Improving speculative decoding via
knowledge distillation. arXiv preprint arXiv:2310.08461 (2023).
[140] Michael Zhu and Suyog Gupta. 2017. To prune, or not to prune: exploring the
efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878
(2017).
[141] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean,
Noam Shazeer, and William Fedus. 2022. St-moe: Designing stable and transfer-
able sparse expert models. arXiv preprint arXiv:2202.08906 (2022).
6615