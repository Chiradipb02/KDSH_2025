Explainable and Interpretable Forecasts on Non-Smooth
Multivariate Time Series for Responsible Gameplay
Hussain Jagirdar
hussain.jagirdar@games24x7.com
Games24x7
Bengaluru, IndiaRukma Talwadker
rukma.talwadker@games24x7.com
Games24x7
Bengaluru, IndiaAditya Pareek
aditya.pareek@games24x7.com
Games24x7
Bengaluru, India
Pulkit Agrawal
pulkit.agrawal@games24x7.com
Games24x7
Bengaluru, IndiaTridib Mukherjee
tridibm@gmail.com
Games24x7
Bengaluru, India
ABSTRACT
Multi-variate Time Series (MTS) forecasting has made large strides
(with very negligible errors) through recent advancements in neural
networks, e.g., Transformers. However, in critical situations like pre-
dicting gaming overindulgence that affects one’s mental well-being;
an accurate forecast without a contributing evidence (explanation)
is irrelevant. Hence, it becomes important that the forecasts are In-
terpretable - intermediate representation of the forecasted trajectory
is comprehensible; as well as Explainable - attentive input features
and events are accessible for a personalized and timely intervention
of players at risk. While the contributing state of the art research
on interpretability primarily focuses on temporally-smooth single-
process driven time series data, our online multi-player gameplay
data demonstrates intractable temporal randomness due to intrinsic
orthogonality between player’s game outcome and their intent to
engage further. We introduce a novel deep Actionable Forecasting
Network (AFN) , which addresses the inter-dependent challenges
associated with three exclusive objectives - 1) forecasting accuracy;
2) smooth comprehensible trajectory and 3) explanations via multi-
dimensional input features while tackling the challenges introduced
by our non-smooth temporal data, together in one single solution.
AFN establishes a new benchmark via: (i) achieving 25% improve-
ment on the MSE of the forecasts on player data in comparison to
the SOM-VAE based SOTA networks; (ii) attributing unfavourable
progression of a player’s time series to a specific future time step(s),
with the premise of eliminating near-future overindulgent player
volume by over 18% with player specific actionable inputs feature(s)
and (iii) proactively detecting over 23% ( ∼100% jump from SOTA) of
the to-be overindulgent, players on an average, 4 weeks in advance.
CCS CONCEPTS
•Applied computing; •Computing methodologies →Tempo-
ral reasoning; Causal reasoning and diagnostics ;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671657KEYWORDS
Multivariate Time Series Forecasting, Interpretability, Explainable
AI, Responsible AI, Gaming
ACM Reference Format:
Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, and Tridib
Mukherjee. 2024. Explainable and Interpretable Forecasts on Non-Smooth
Multivariate Time Series for Responsible Gameplay. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671657
1 INTRODUCTION
In the domain of online skill gaming such as Rummy [ 52], regular
analysis of player’s gaming behavior is crucial to detect and address
overindulgence - consistent and sustained engagement at an ex-
treme level [ 5,7,8]. Overindulgence can lead to extreme outcomes
for a player arising out of monetary losses and mental exhaustion.
Sustained healthy engagement ensures better long term rewards,
both for the player as well as the business. Although occurrences of
extreme risky events are scarce (with 0.05 - 0.1% probability) [ 45],
it is important to identify them in-time to ensure and encourage
responsible game play with proper intervention. This necessitates
not only in-time monitoring but a proactive and timely forecasting
of players’ multi-variate time series data. Further, to action on a
prediction of overindulgence, for a player who has been playing
moderately (safely) in the present times, is difficult to comprehend
unless the trajectory of the prediction is human interpretable and
can be explained via shifts in the player’s one or more game play
features.
1.1 Background and Challenges:
A. MTS Forecasting on Multi-Source Highly Random Data
MTS forecasting has made large strides through recent advance-
ments in neural networks, e.g., Transformers, both, in multi-time
step [ 31,46,58] as well as multi horizon forecasts [ 30]. Some of
the recent and directly applicable research on interpretable repre-
sentation learning on time series data [ 15,34] has furthered the
importance of interpretability at par with forecasting for proactive
intervention, especially in the health domains. However, much of
these networks assume temporally smooth transitions in the data
properties and primarily demonstrated results on synthetic data
where temporal smoothness was induced via linear interpolation
5126
KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
methods. We contend that these existing SOTA networks are insuf-
ficient towards our objective of actionable forecasts, mainly due to
the following distinct characteristics observed in our player data:
(i)Highly Random and Temporally Non-Linear Progression:
Though gaming patterns are quite outcome based (win/loss), the
psychological imprints on the player from their previous game play
could generate very diverse game behaviours in immediate next
play ([ 2,47]) - player with previous losses could show assured and
confident game play style while somebody with high winnings
may actually show unnecessary aggression, and vice versa. Hence
the factors (co-variates) affecting the future game play decisions
are not completely observed, measurable or generalizable. Due to
which the gameplay data arising out player’s time-varying psycho-
logical imprints mostly posse high irregularity. This presents itself
as our primary and a unique challenge. Section 4.2 comprehends
data randomness quantitatively.
(ii)Multi-process Nature of Data Generation: Much of the
SOTA forecasting networks have been evaluated on a data (WTH
[38], ECL [ 49], ETTh, ETTm [] etc.) generated from a single-process
- e.g electricity demand of a particular city, temperature of a specific
locality, over a time period. Our data comes from multi-million
active players on a daily basis - synonymous to many processes
and its generalization is also a challenge.
B. Interpretability
As much as the accuracy over the forecasts matter, it’s important
for the forecasts to be interpretable, for e.g visualization of the en-
tire trajectory in a lower dimensional space (2-D) where the human
cognition works the best. For example, via using a high dimensional
eICU data [ 17,40] which contains various measurements related
patient’s physiology, a model predicts fatality in the next few weeks;
a comprehensible trajectory should help explain when in time and
how the model made that prediction. Interpretable forecasts are a
necessity in many domains, especially involving rare/outlier occur-
rences like fraudulent transactions [ 25], network intrusion [ 11,56],
digital or social media addiction [ 35,37], gambling [ 5,7], addictive
e-commerce shopping [43] etc.
Most of these time series models are based on a encoder-decoder
architecture, that maps the high dimensional input data into a
latent space primarily making an i.i.d. assumption about the tem-
poral data, ignoring the rich temporal structure available in the
adjacent data time steps. Given that human cognition is not in-
herently optimized for efficient functioning in high-dimensional
spaces SOM-VAE [ 15] and T-DPSOM [ 34] based models propose to
further reduce this intermediate latent representation space into a
two-dimensional grid enforcing Self Organised Map (SOM) [ 28] like
topological neighbourhood. Interpretability is then facilitated by en-
couraging a higher probabilistic likelihood that temporally adjacent
data points belong to the same or immediately next SOM centroid,
assuming linear progression in data properties over time. This is
done via a probabilistic transition model in [ 15] and a LSTM based
model in [ 34]. However, irregularity in temporally adjacent data
points arising out of random process, yields following challenges:
(i)Temporally Adjacent data points appear Topologically
Distant: data randomness contradicts the temporal smoothness
assumption made by the SOTA networks. When trained using these
networks our temporally adjacent data points appear far away no-
ticeable number of times. This obstructs interpretability as temporaltrajectories appear to move randomly. (ii) Lack of Evidence on
Sudden Long Jumps in the Representation Space: Assuming
this pattern of randomness is generally observed across multiple
players, we could enforce SOM embedding space using the prior art.
However, lack of quantifiable evidence or justification for a distant
jump, could render the interpretability difficult to accept and initial
further proactive action.
C. Explainability
In addition to interpretability, its important to a) identify the
significant feature(s) that are dominant for a player at a particular
time-step and b) time steps in the near future which are atten-
tive towards players future overindulgence. Together, both these
would aid in personalizing the proactive-actions that aligns with
the player’s well-being. SHAP [ 32] (SHapley Additive exPlanations)
and LIME (Local Interpretable Model-agnostic Explanations) [ 42]
are popular techniques towards explainable AI (XAI). However,
conventional use of these model-agnostic post-hoc methods has
following limitations: (i) Negligence of Temporal Ordering of
Input Features in Time Series: This can lead to sub-optimal
explanation quality, as temporal dependencies are often signifi-
cant in time series data, (ii) Shifting focus on attentive features
curbs accurate treatment: shifting attention on features over the
time period of intervention would confuse treatment plans; e.g.
patient being treated with a different drug every day due to shift in
attention weights between various physiology parameters.
1.2 AFN Intuition and Contributions:
Towards building a robust actionable forecasting network while
addressing the inter-dependent challenges associated with our
three, equally important but exclusive objectives - forecasting, in-
terpretability and temporally consistent explainability we make
certain design decisions. These decision focus on learning hidden
aspects related to random temporal data drifts via leveraging some
existing research in the player psychology space.
Estimating and Leveraging Temporal Randomness in data:
We proceed with a reasoning that, shifts in the game-play data of
the players, occur due to a change in their psychological imprints.
We hypothesise that the transition in these imprints are Markovian
in nature. We can then associate psychological imprints to latent
states in a Markov chain. AFN models these stochastic relationships
between latent states via a Deep Markov Model (DMM), a deep
neural network. Thereby, given a latent state for an immediate past
time step, DMM predicts the present one. This outcome is further
mapped to a discrete quantity (scalar), via another Conditional
Network. These conditions helps in identifying the parameters of
the present data distribution (given the immediate past one). We
could extend this to k-step Markov as well. Using this, we attempt
to learn and predict irregularities in temporal data per player via a
Transition Module (TM).
Extending SOM based Interpretability for Temporal Irreg-
ularities: Now, since the network is aware of the psychology of the
player in the form of the condition predicted by the TM, we target to
enrich the latent space of the downstream VAE [ 27] based encoder-
decoder network as in [ 15,34] with this condition parameter. This
helps in competitive mapping of variably scaled, non-smooth fea-
ture values (arising out of different conditions and hence different
5127Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay KDD ’24, August 25–29, 2024, Barcelona, Spain
distributions) to a common intermediate (latent) space of the VAE
over which similar SOM based topology could be extended. Word
‘competitive’ here refers to the objective which strives to balance
between topological smoothness- for similar data points via SOM
property vs. retention of interpretability - by avoiding long jumps
when data points are coming from different distributions and hence
dissimilar. This is done via appropriate learning objectives in AFN.
Since the conditions are learnt and inferred at runtime, we refer to
it as ConVAE as opposed to the traditional cVAE.
Putting it all together: We propose Intelligent Forecasting
Module (IFM) that models sequential time series data, by training
aLSTM [22] on the latent encodings of temporal data, produced
by ConVAE-SOM. Training on these encodings allows the model to
retain interpretability as these latent encodings are mapped to their
respective SOM clusters. Attention Layer (AL) is added on top of
LSTM to focus on the relevant time steps. ConVAE-SOM encourages
a higher probabilistic likelihood for latent representations belong-
ing to temporally adjacent data points to the same or immediately
next SOM centroid. To accommodate temporal non-smoothness
arising due to data randomness, we provision a ‘leeway ’ for a shift
in the TM condition parameter. This is done via a Damping Fac-
tor Network (DF) which is trained to self-learn a damping factor
to help the network balance smoothness over the prediction loss.
AFN re-establishes SHAP [32] over the SOM clusters by blending
attentive time steps from the AL to ensure consistent feature focus
for proactive intervention. We summarize the main contributions
of AFN as follows.
1. Transition Module (TM): A Deep Markov Model and a Con-
ditional Network which attempts to encapsulate the scale varying
features and their periodic/seasonal variations into a transition
function to provide relevant conditions to ConVAE-SOM. This alle-
viates the challenge of predicting randomness arising out of hidden
and not easily measurable factors.
2. ConVAE-SOM: Re-parameterization of the representations (la-
tent space) via a condition parameter which is learnt by the TM.
3. Intelligent Forecasting Module (IFM): Joint optimization of
the smoothness loss to discourage long jumps towards preserving
interpretability and a damping factor which provides a “leeway” to
perform a non-smooth, far away jumps for forecasting accuracy
and to further quantify non-obvious interpretability . Our code base
and the relevant datasets are available at [16].
2 RELATED WORK
Multivariate Time Series Forecasting: MTS forecasting models
are roughly divided into statistical (Vector auto-regressive (VAR)
model [ 26], Vector auto-regressive moving average (VARMA)) and
neural networks. Statistical models assume a linear cross-dimension
and cross-time dependency. With the development of deep learn-
ing, many neural models often empirically show better perfor-
mance than statistical ones. LSTnet [ 29] and MTGNN [ 55] use CNN
and graph neural networks respectively for cross-dimension and
RNN for cross-time dependencies. RNN’s, however have empirically
shown to have difficulty in modeling long-term dependency. Re-
cently, many Transformer[ 19,51]-based models have been proposed
for MTS forecasting, showing great results. Informer [ 59] proposes
ProbSparse self-attention which achieves O(𝐿log𝐿)complexity.Pyraformer [ 31] introduces a pyramidal attention module that sum-
marizes features at different resolutions and models the temporal
dependencies of different ranges. Following up with the improve-
ments, Crossformer [ 58] further focused on the cross-dimension
dependencies which further pushed the MSE baselines to an extra-
ordinary level. Though quite impressive, these works do not offer
any explanations over the prediction space, except [ 30] which ex-
tracts the overarching trends and patterns from the data, rather
than delving into the specific causality of predictions on a case-by-
case basis. Lately, non-transformer based methods too have been
proposed in recent literature that provide similar or even better
performance with less resources[ 12], while some even question the
effectiveness of transformer-based networks for MTS forecasting
[57]
Interpretability: In recent years, interpretability has increasingly
been combined with generative modeling through the advent of
generative adversarial networks (GANs) [ 18,36] and variational
autoencoders (VAEs) [ 27]. However, the representations learned
by these models are often considered cryptic and do not offer the
necessary interpretability [ 9]. A lot of work has been done to im-
prove them in this regard [ 21,54]. Nonetheless, these works have
focused entirely on continuous representations, while discrete ones
are still being under-explored. Discrete representations are known
to reduce the problem of “posterior collapse” [ 41,50], situations in
which latents are ignored when they are paired with a powerful
auto-regressive decoder typically observed in the VAE framework.
[15,34] introduce an impressive method of achieving discrete space
interpretability for high-dimensional time series, by working on
SOM like clustering in a low-dimensional latent space. Major draw-
back being, assumption of temporal smoothness in the time series
data and lacking a capability to forecast into the future.
Explainability: Explainability allows one to comprehend a par-
ticular prediction w.r.t the input features. SHAP [ 32] assigns each
feature an importance value for a particular prediction. Tradition-
ally, SHAP and LIME [ 42] have been used to identify feature weights
for a multi-class classification or a regression based models. It still
remains to be a challenge in the case of an unsupervised model,
where there is no such response variable/class to hook to.
3 METHODOLOGY
Let𝑋={𝑥1,𝑥2...,𝑥𝑁}be the set of 𝑁Multi-variate Time Series,
each being generated by a separate process (e.g. gaming players)
where each𝑥𝑖={𝑥𝑖,1,𝑥𝑖,2...,𝑥𝑖,𝑇}such that𝑥𝑖,𝑡∈R𝑑(where each
dimension represents a player’s game play feature. The objective
is to forecast next ℎtime-steps∀𝑥𝑖as following:{ˆ𝑥𝑖,𝑇+1,ˆ𝑥𝑖,𝑇+2...,
ˆ𝑥𝑖,𝑇+ℎ}, given the history {𝑥𝑖,1,𝑥𝑖,2...,𝑥𝑖,𝑇}.
3.1 AFN Framework
SOM-VAE[ 15] first proposed a model for learning interpretable
representations over the time series data, where the authors in-
troduce a novel concept of “smoothness” over temporal trajectory
as an important requirement for interpretable time series models.
Drawing inspiration from both [ 15] and [ 34], we propose AFN as a
multi-variate time series forecasting model with interpretable and
explainable forecasts for data with high degree of randomness (ran-
domness quantification is done in the Section 4.2). Our proposed
5128KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
Figure 1: AFN Architecture
architecture is presented in Figure 1. The major constituents of
AFN are:
(1) Transition Module (TM)
(2)Conditional Variational Auto-Encoder with Latent SOM (ConVAE-
SOM)
(3) Intelligent Forecasting Module (IFM)
3.1.1 Transition Module. The Transition Module (TM) consisting
of a Deep Markov Model (DMM) along with the Conditional Net-
work, helps address the issue of high degree of randomness in our
data. We hypothesise that the player’s psychology [ 47] is hidden
in form of a latent state associated with each observation 𝑥𝑖,𝑡, and
the change in the pattern of upcoming observations is related to a
change in that latent state. AFN incorporates “Latent State Predic-
tion” via the DMM which is trained to predict the next state which
dictates the associated pattern of the generated data. The Condi-
tional Network works on the next state predicted by the DMM
and the actual next state (available during the training phase). It
discretizes the DMM’s state output into a scalar “condition ” that
influences the encodings of the ConVAE-SOM. Hence, change in
these “conditions” leads to change in the data distribution patterns
in the subsequent timestep.
In order to describe DMM’s latent state at a given time-step, we
take an aggregated representation of the prior psychological im-
prints in a way that it encapsulates the temporal patterns. For
each𝑥𝑖,𝑡, we consider the past 𝑡−𝜏time steps such that 𝜏=
𝐶+𝑀, where𝐶is the window length and 𝑀is the number of
windows.𝐶and𝑀are hyperparameters. We cluster each of the
𝑀time windows{𝑥𝑖,𝑡−𝜏...𝑥𝑖,𝑡−𝜏+𝐶},{𝑥𝑖,𝑡−𝜏+1...𝑥𝑖,𝑡−𝜏+𝐶+1},...
{𝑥𝑖,𝑡−𝜏+𝑀...𝑥𝑖,𝑡−𝜏}into one of the 𝐾clusters, based on a clustering
technique. In AFN, we derive the clusters using CognitionNet [ 47],
which are interpreted as players’ dynamic psychological imprints.
Alternatively, time series clustering techniques such as DTCR [ 33],
etc. can also be used. The sequence of clusters, one for each of the
𝑀time windows is summarized into a single vector 𝜋𝑖,𝑡∈R𝐾suchthat each position 𝑘∈{1,2...𝐾}of𝜋𝑖,𝑡represents the proportion
of occurrence of cluster 𝑘in the entire window of observations.
Hence𝜋𝑖,𝑡is a summarisation of the history of the game behaviours
of player𝑖in the most recent past 𝜏time steps. Value of 𝐾-total
number of unique game behaviours, is inferred by the Cognition-
Net.
As shown in figure 1, AFN adopts feed-forward network for both
the DMM and the Conditional Network with two losses:
(1)Error in DMM’s estimation of the next latent state (True
value:𝜋𝑖,𝑡, Estimated value: ˆ𝜋𝑖,𝑡) given the immediate past
latent state ( 𝜋𝑖,𝑡−1) as:
LMSE=𝑇∑︁
𝑡=𝜏∥𝜋𝑖,𝑡−ˆ𝜋𝑖,𝑡∥2 (1)
(2)Categorical Cross Entropy loss over the probability distribu-
tion of both the predicted and the actual ( 𝑐𝑖,𝑡∈R𝜌) condition
via a dense layer with 𝜌(number of conditions) as a hyper-
parameter:
LConditional =−1
𝜌𝑇∑︁
𝑡=𝜏Pmodel(𝜋𝑖,𝑡)log(Pmodel(ˆ𝜋𝑖,𝑡)) (2)
wherePmodel()represents the Conditional Network consisting of
a deep neural network such that 𝑐𝑖,𝑡=Pmodel(𝜋𝑖,𝑡). For details on
the training complexity for training the DMM, refer to Appendix B.
The total transition loss of the TM is given by:
LTransition =LMSE+L Conditional (3)
WhereLMSEcharacterizes the loss of the DMM. TM is pre-trained
before training the complete AFN network.
3.1.2 Conditional Variational Auto-Encoder with Latent SOM (Con-
VAE - SOM). AFN uses a conditional VAE (cVAE) which lever-
ages the conditions generated by the Transition Module. This en-
sures that the time-series data having the same Markov conditions
5129Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay KDD ’24, August 25–29, 2024, Barcelona, Spain
is mapped to similar regularized latent space. SOM based topol-
ogy is learnt over the entire conditional latent space as described
in [15,34]. Input to the cVAE is 𝑥𝑖,𝑡||𝑐𝑖,𝑡, where𝑐𝑖,𝑡refers to the
discrete condition for the 𝑖𝑡ℎplayer at time 𝑡derived from the Tran-
sition Module, and ||denotes vector concatenation operator. cVAE
encodes each input 𝑥𝑖,𝑡to a latent encoding 𝑧𝑒𝑖,𝑡∈R𝑚(𝑚<𝑑). We
will refer to 𝑧𝑒𝑖,𝑡as𝑧𝑖,𝑡for brevity. The SOM training step then allo-
cates each encoding 𝑧𝑖,𝑡to a cluster-centroid embedding 𝑧𝑞∈R𝑚.
Total number of SOM cluster embeddings is a hyper parameter
and is derived empirically. In AFN, the SOM space is set to a 8×8,
2-D space, which results in 64 discrete SOM clusters. We use the
same ELBO loss as in [ 34] for cVAE, however while passing the 𝑐
condition to the decoder for the reconstruction we apply a gradi-
ent stopping on the TM predictions. This is being done to prevent
SOM or cVAE network losses from influencing the conditions gen-
erated by the Transition Module. We borrow the loss formulation
LT-DPSOM from [34]:
LT-DPSOM =𝛽LSOM+𝛾LCommit+𝜃LReconstruction+𝜅LSmoothness
(4)
where𝛽,𝛾,𝜃,𝜅 are hyperparameters. To learn more about the set-
tings of these various hyperparameters in our context as well as the
process used to derive them, please refer to the Appendix Section
A.1
3.1.3 Intelligent Forecasting Module. The Intelligent Forecasting
Module (IFM) consists of:
(1)LSTM network with an Attention layer (AL) over the tem-
poral latent encodings ( 𝑧𝑒)
(2)Damping Factor Network (DF) which provides ‘leeway’ for
allocating distant SOM clusters, in case of a change in condi-
tion from the Transition Module.
(3)Forecasting Fine-Tuning (FFT) via MSE loss on the actual
future predictions ( ˆ𝑥𝑖,𝑡+𝑗,𝑗∈{1,2...,ℎ})
LSTM based forecasting module generates the predictions for the
next time steps within the latent space in form of the probability
distribution over the next latent encoding, p( 𝑧𝑖,𝑡+1|𝑧𝑖,𝑡) using log-
likelihood loss between actual and predicted 𝑧distributions. We
incorporate a temporal Attention layer on top of the LSTM in order
to improve model performance and also be able to extract dominant
time-steps that led to the forecast. To aid accurate forecasting while
preserving a smooth trajectory, AFN introduces a Damping Factor,
that is introduced to the LSTM prediction loss as:
LPred=−𝑁∑︁
𝑖=1𝑇−1∑︁
𝑡=1log𝑝(𝑧𝑖,𝑡+1𝑧𝑖,𝑡)Dmodel(𝜋𝑖,𝑡,ˆ𝜋𝑖,𝑡+1)(5)
where𝑝(𝑧𝑖,𝑡+1𝑧𝑖,𝑡)refers to the probability distribution over the
next latent encoding, given the current latent encoding. The Damp-
ing Factor Network, Dmodel()is tasked with a new objective to
learn a damping factor between the latent state ( 𝜋𝑖,𝑡) and next latent
state predicted by the DMM ( ˆ𝜋𝑖,𝑡+1) for𝑡∈{1..., 𝑇−1}. To train
the damping factor network, 𝜋𝑖,𝑡and ˆ𝜋𝑖,𝑡+1are passed through a
fully connected neural network that yields a sigmoid-scaled output
between[0,1]. In a case where there is a sudden data pattern shift,
causing the transition to a far away SOM grid position in consecu-
tive time steps, the low log-likelihood would generate a high LPredvalue (penalty). Hence, to compensate in favour of LPred, AFN of-
fers the LSTM an option to dampen this effect by simultaneously
training the damping factor network. The damping loss is back-
propagated to the SOM training layer, which also realigns its SOM
cluster space to preserve interpretability. LSTM network generates
next latent encoding, 𝑧𝑖,𝑡+1∼𝑝(𝑧𝑖,𝑡+1𝑧𝑖,𝑡)which is then passed to
the decoder to generate ˆ𝑥𝑖,𝑡+1. Forecasting fine tuning is done post
this LSTM prediction step, in order to fine-tune the predictions in
the actual feature space (unlike the LPredthat works in the latent
feature space). The Forecasting Fine-Tuning loss ( LForecasting ) is
defined as:
LForecasting =||𝑥𝑖,𝑡+1−ˆ𝑥𝑖,𝑡+1||2 (6)
The final loss function in AFN is given as:
LAFN=LT-DPSOM+𝜏LTransition+𝜂LPred+L Forecasting (7)
where𝜏,𝜂are again the hyperparameters.
3.1.4 Addressing Challenges Related to SHaP over Temporal Data:
Traditionally, SHAP based models identify feature weights for a
prediction which is either a multi-class classification or a regression
task. Our network is unsupervised. So, in AFN we let the SHAP
model regress its Shapley values on the SOM cluster identities
instead as the y-label and the actual game play features of the data
point representing the corresponding SOM Cluster centroid as the
x-value vector. We retain top 5Shapley values per SOM cluster (8x8
-64 in our case). But, this does not solve the two challenges related to
capturing the temporal dependencies and steady focus on the most
attentive feature across the entire forecasted period. For a player
who is predicted to turn risky at a future time step t+f , t being the
current time step and f ≤h, AFN publishes one or more attentive
time steps for the forecast, via the AL. AFN then uses the top 5
Shapley values at each of the attentive time steps (corresponding
SOM centroids) and measures the extent of acceleration amongst
each of the respective features. These features are then ordered on
their average Shapley weights values followed by the magnitude
of acceleration during the period of attention. The ranked list is
published, from which we chose the topmost to start with.
4 EVALUATION
4.1 Data
For preparing our data, we picked up 200k players with a train-
test split of 0.75. For each of the player, we extracted 90 days of
continuous time series data across different times of the year. We
fetch 19 features to represent their game-play style and indulgence.
We could categorise these features into 3 dimensions: Time, Money,
Desperation. Few of these are illustrated in Table1.
4.2 Randomness in Dataset
To validate our hypothesis of inherent high degree of non-smoothness
(randomness) in real world online player game-play data compared
to frequently used open-source MTS datasets (such as ETTh), we
perform the following statistical and graphical tests:
1Drop adherence evaluates player’s ability to judge if the initial rummy cards qualify
as a bad hand[13, 14]
5130KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
Dimension Feature
TimeTotal time spent per day
Count of late night games
MoneyAmount of cash added for playing
Count of cash games played
Win percentage
DesperationCount of requests of increasing deposit limit
Invalid declaration of the win
Continue to play on bad cards (drop adherence)1
Table 1: Sample Game Play Features and their Broad Categorization
4.2.1 Runs Test. Runs test[ 6] is a non-parametric statistical test
that checks randomness hypothesis for a two-valued data sequence.
This test is based on the null hypothesis that the data is random.
Table 2 shows p-values of the runs test obtained for each of the
datasets.𝑛represents number of samples considered for each ex-
periment. For players data, a sample represents a player, whereas
for open source data it is a slice of the entire sequence, lengths of
samples being the same in both the cases - 13. We repeated the
sampling process 1000 times. We report the mean and the standard
deviation.
Dataset Runs Test Auto Correlation Test
n=30 n=100 n=300 n=30 n=100 n=300
ETTh 0.376± 0.353± 0.282± 0.246± 0.258± 0.267±
0.109 0.192 0.125 0.111 0.128 0.122
ECL 0.303± 0.296± 0.352± 0.245± 0.237± 0.234±
0.188 0.174 0.197 0.069 0.088 0.085
WTH 0.394± 0.300± 0.245± 0.286± 0.299± 0.318±
0.208 0.256 0.121 0.111 0.103 0.100
PD 0.439± 0.452± 0.453± 0.084± 0.077± 0.081±
0.112 0.065 0.032 0.023 0.007 0.018
Table 2: In runs test, Players data (PD) has maximum p-value across all the
datasets describing a high degree of randomness compared to open-source
datasets. Similarly, for Auto-correlation test, PD has the least correlation
ratio implying a closeness towards a “white-noise” compared to the others.
4.2.2 Auto-correlation Test. Auto-correlation tests [ 23,24] can be
used to detect whether there is significant correlation between the
observations and their lagged values. This provides insights into
the presence of a structure or patterns in the time series data. In
the presence of a trend in the data, short-term auto-correlations are
typically positive and substantial, as nearby observations in time,
share similar sizes. For seasonal data, auto-correlations are more
prominent at seasonal lags (multiples of the seasonal frequency)
compared to other lags. When data exhibit both trend and season-
ality, a combination of these effects is observed. For a white noise
series, we expect 95% of the spikes in the ACF to lie within ±2/√
𝑇
where𝑇is the length of the time series. Hence, lower ratios indicate
higher resemblance to the white noise. Table 2 shows the ratio of
number of lags that are above the threshold to the total lags.
4.2.3 Time-series Decomposition. Time series decomposition [ 10,
44] is a well-known concept. In order to separate distinct patterns
for more insightful analysis, we decompose the the time series to
obtain trend, seasonal and residual components. We tried additive
as well as multiplicative models to decompose. Figure 8 in the Ap-
pendix Section C.3 shows the decomposition of ETTh and Playersdata for variable (up to 150) timestamps. To quantify and compare
the trend, seasonality and residual components across the datasets,
we calculated proportion of variance explained by dividing the sum
of squares of the individual components by the sum of squares
of the original data. The value represents the percentage of the
total variance explained by each component. Moreover, we also
report the standard deviation of the residuals. This measures the
overall level of noise and unexplained variation in the data. Table 3
compares these metrics on different datasets.
Dataset Explained Variance Residual
Trend Seasonality Residual Standard Deviation
ETTh 91.684 2.167 1.653 0.079
ECL 87.927 3.371 3.743 0.088
WTH 93.564 0.131 0.297 0.036
PD 83.190 0.120 14.583 0.177
Table 3: Open-source datasets exhibits a higher proportion of explained
variance from trend and seasonality compared to Player Data (PD). The
residuals in PD exhibit a higher standard deviation than others, indicating
a greater level of unexplained noise and volatility.
4.3 Multi-Variate Time Series Forecasting
Having established the inherent randomness in our players data,
we check how several conventional as well as more sophisticated
time series forecasting methods perform on this data. We input
7 weeks of data for each user and evaluate on 6 weeks of future
predictions. This is done so as to ensure maximum Time to Survive
(TTS, see Section 4.5) with minimum historical dependency. The
results, summarized in Table 4, show that AFN clearly outperforms
the others even when the end goal is simply multi-variate, multi-
timestep forecasting on our data.
Model MAE MSE RMSE MAPE
Fast Fourier Transform 0.17 0.06 0.22 1042.59
FB Prophet[48] 0.18 0.06 0.22 400.76
NLinear[57] 0.19 0.04 0.20 598.26
DLinear[57] 0.20 0.05 0.22 716.14
TCN[4] 0.20 0.04 0.21 694.15
ARIMA 0.26 0.12 0.29 508.31
XGBoost[20] 0.22 0.09 0.25 463.86
T-DPSOM[34] 0.16 0.04 0.21 401.88
AFN 0.12 0.03 0.18 355.16
Table 4: Evaluation of MTS models on players data. The models above the
dotted line provide no actionable interpretations while the models below
offer varying degrees of interpretation and explainability. SOM-VAE [ 15]
does not offer forecasting feature.
4.4 Interpretability and Explainability
Now we illustrate how the individual components of AFN con-
tributes to the overall interpretability and explainability with user-
specific trajectories as our working examples.
4.4.1 Interpretability: For establishing interpretability, we sample
500 users from test set. We used stratified random sampling by
creating the buckets based on their last 60 days engagement level.
These players were then randomly sampled from each stratum to
5131Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay KDD ’24, August 25–29, 2024, Barcelona, Spain
ensure representation from all engagement categories. AFN fore-
casted their onward trajectory for the next 42 days. We filtered out
the players who were in the same/neighbouring clusters for at least
70% of their time (indicating same paced and healthy game play
over time). Out of the remaining 68 players, we picked 2 random
players that had a progressive journey into different SOM clusters.
Its important to note that this could be due to movement to other
game play patterns and not necessarily indicate an overindulgence.
The interpretability of their journey is demonstrated by:
i. Visualisation of trajectory on SOM cluster grid: SOM em-
beddings generate SOM centroids over the ConVAE-SOM latent
encodings. These embeddings posses topological relationship and
can be visualized as a 2-D grid. Figure 2 shows a heatmap on a
8x8 grid with 64 SOM clusters. Each cluster is more similar to its
neighbourhood clusters than others. This grid provides a founda-
tion to visualise any player’s trajectory. The color of this heatmap
signifies indulgence level (darker being more indulgent). Level of
risk or indulgence is generated using ScarceGAN [ 8]. Figure 2 we’ve
visualised player’s known trajectory with solid lines and forecasted
one with dotted lines. The red trajectory indicates a forecast that
the player will move towards over-indulgence. The other player
(green trajectory) however starts in a similar fashion initially but
continues with constant momentum and hence is seen to move
Figure 2: Trajectory of Player 1 and Player 2 on a 2D SOM grid of
risk-related clusters. AFN forecasts the trajectory of Player 1 towards
the darker(risky) clusters, while it forecast a smooth trajectory on the
lighter(healthy) clusters for Player 2. These maps enables the interpretabil-
ity by visualising the state of player at each time step.
Figure 3: Explanations of final forecast for Player 1 and Player 2 by
showing the respective Attention Point and Dominant Feature respectively.smoothly with all centroids showing low heat (lighter colours) and
is declared to be playing with healthy habits for the future 42 days.
ii. Conditions derived from Transition Module: Transition
module generates a condition value which represents the data pat-
tern at the specific timestamp. Firstly, a change in the condition
can be used to interpret randomness in the data. Secondly as seen
in the Figure 2, the red trajectory is associated with varying condi-
tions and also specially less of condition 0. Whereas the the green
trajectory sticks to a single condition most of the times and see
more of condition 0. We further quantify the TM based condition
driven interpretability in the Appendix in the Section C.4.
4.4.2 Explainability: In order to provide reasoning of their forecast,
we clubbed Shapley values and self-attention as discussed in Section
3.1.4.
i. Identifying significant events/time steps: Attention weights
are used to identify time steps where the model attributed its at-
tention for forecasting the future time steps. In fig 3, the forecasted
journey is into the darker region which signifies over-indulgence.
The attention point on 10𝑡ℎtime step implies that for this user 10𝑡ℎ
time step majorly contributed in predicting overindulgence. In a
proactive decision-making scenario, these attention points offer
opportunities for an intervention. Figure 3 also shows that the sec-
ond player’s healthy onward trajectory is primarily attributed to
the player reducing the number of games played per day on the
platform. This self-moderation by the player was detected by AFN
at the 3𝑟𝑑(past) time step.
ii. Extracting the responsible features for forecast: Table
5 shows the top 5 features for 4 different SOM clusters (randomly
chosen).
SOM Cluster (1,2) SOM Cluster (3,5)
Add cash failure +1.118 Change in daily limit +1.006
Win Ratio -1.058 Add cash transaction -0.907
Drop adherence +1.009 Modes of payment +0.784
Invalid declaration -0.830 Net winnings -0.550
Change in daily limit -0.578 Total games played +0.486
Table 5: Sample summary of the Shapley values for two SOM clusters. Add
cash failure, Win Ratio, and Drop Adherence emerge as the most influential
factors, in cluster (1,2)
In this paper, we illustrate results on the first important feature
obtained through the ranking process discussed in the Section 3.1.4.
Figure 4 refers to the two top attentive features for the player in
the red trajectory in the Figure 3. Figure 4(a) refers to the heatmap
of the most attentive feature. We see that the Shapley value of this
feature is constant high and shows better acceleration (red dots)
compared to ones in the heatmap of the second dominant feature
(green dots), win ratio. The scale of the heatmap in both the graphs
signify their Shapley values for each of the SOM centroids. The
Dot locations corresponds to the attention points in the SR (red)
trajectory in the Figure 3. We understand that, though, the heatmaps
of each feature remains fixed after the AFN training, based on the
trajectory, dominant attentive feature of a players’ could vary and
hence it is personalized.
5132KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
((a)) Dominant Feature
 ((b)) Second Dominant Feature
Figure 4: heatmap of Dominant and a Non-Dominant Feature. Dominant
feature clearly shows higher Shapley magnitude throughout the attentive
period for the player demonstrated in Figure 3
4.5 Real-world evaluation
In this section, we’ll demonstrate the applicability of this entire
framework for our gaming players. AFN intends to facilitate re-
sponsible gaming on our platform. Early detection of risky gaming
pattern (over-indulgence, chasing losses etc.) can enable timely
intervention and personalised support for players at risk. Based on
the AFN forecast, each future timestamp is mapped to a correspond-
ing SOM cluster. Leveraging these forecasted clusters, we transform
the objective into a binary classification problem of Sustained Risky
(SR) vsSustained Healthy (SH) class. Given the dynamic nature of
the platform, its highly likely to see highs and lows in risk scores
in the subsequent time steps for any player. For each of the cluster,
we generate a normalised risk score using ScarceGAN[ 8] which
represents the color scale of the heatmap. All SOM clusters that
have values > 0.0 are identified as risky/dark clusters and remaining
are identified as healthy/light clusters. For player to be identified as
SR, we introduce Burst logic where we define burst as consecutive
allotment of dark clusters. A player is classified as SR only if there’s
a burst of size greater than threshold. We took threshold value as
2 for the Burst logic. All other possibilities are classified into SH
class. At a user level, we define Time to Survive (TTS) as the time
difference between burst start time step and the present time step
(T). TTS signifies the minimum amount of time left to take preven-
tive measures and restrict user from becoming Sustained Risky (SR).
We also define Verbosity as proportion of users classified as SR.
4.5.1 Evaluating Risk Mitigation through AFN. We contend that,
for players predicted to be SR on our platform, proactive and timely
intervention along the right aspect of their game play could ac-
tually help prevent overindulge. Table 6 shows an example for a
cohort of 60 players, forecasted to be SR. Magnitude of the respec-
tive attentive feature for each player was reduced synthetically in
small steps. Reduction was only done at the attentive time step. At
each reduction we infer player’s onward journey using AFN. We
observe an evident reduction in overindulgence volume along with
an increase in the TTS percentage.
4.5.2 Risk mitigation via personalized intervention. We pick an
example of personalized intervention from the cohort in Table 6,
shown in Figure 5. Player was predicted to be SR at time step 7, with
next 5 time steps in red dotted lines. This player was intervened at
the10𝑡ℎtime step, where the attention was drawn by AFN along
the responsible attentive feature Add cash failure (acf). We applied
20% reduction in the player’s wallet add cash limit. Figure 5 alsoReduction Factor Change in SR Volume Change in TTS
5 % - 1.37 % 1.39 %
10 % - 2.75 % 1.28 %
20 % - 8.96 % 1.90 %
30 % - 9.65 % 2.68 %
40 % - 13.10 % 3.64 %
50 % - 16.55 % 5.03 %
60 % - 17.93 % 5.33 %
70 % - 18.62 % 6.22 %
Table 6: Change in Verbosity and Time to Survive (TTS) on reducing most
dominant feature of attention at the right time
Figure 5: Illustration of how a forecasted SR trajectory for a player can be
restored to a SH via timely and personalized intervention
shows the entire “actual” trajectory of the player with red solid (7
steps) and brown dashed line (5 steps). This is an example of how
overindulgence can be prevented with intervention on the right
feature (cash limit, in this case), at the right time step.
4.5.3 Evaluating our classification model. We also evaluate our
AFN classification framework with ScarceGAN[ 8], retrospectively.
We learn from Table 7 that out the total volume predicted risky
by ScarceGAN on a given date, AFN identifies over 23% of those
in advance with and average TTS of 4 weeks (30 days) and is 43%
precise in its predictions. AFN outperforms T-DPSOM based SOTA
model by 100%.
Model Precision Recall Verbosity
T-DPSOM [34] 0.2416 0.1113 0.4563
AFN 0.4306 0.2383 0.3377
Table 7: In-Production Performance of AFN for Early Risk Prediction
4.6 Ablation Study
In our approach, there are four novel components: Transition - TM,
Attention Layer - AL, Damping Factor Network- DFand Forecast-
ing Fine-tuning - FFT. In the Table 8, AFN refers to TM + AL +
DF + FFT i.e. no ablation. We’ve performed ablation over these
components to study their effect on the MSE and classification
performance. Table 8 shows the MSE results for each experiment
(detailed graphs in Appendix A.3).
5 CONCLUSION
We have proposed AFN, a novel deep neural network for inter-
pretable forecasts and explanations over the multi-dimensional and
non-smooth temporal feature space. We use specialized components
5133Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay KDD ’24, August 25–29, 2024, Barcelona, Spain
Training Steps Without Without Without AFN
TM AL DF
MSE without FFT 0.111 0.059 0.068 0.051
MSE with FFT 0.147 0.080 0.071 0.039
Table 8: MSE results on component ablation using Players data.
to create such forecasts for the chaotic, multi-source time series
often encountered in business: (1) Transition Module consisting of
DMM and Conditional Network addresses the randomness in the
data by changing the conditions in case of a change in data pattern,
(2) ConVAE-SOM network generates interpretable intermediate
trajectories for the forecasts, and finally (3) Intelligent Forecast-
ing Module that performs the actual forecasting by intelligently
utilizing the Damping Factor Network to incorporate data pattern
shifts, while maintaining explainability over the forecasts using the
Attention Module and SHAP over the SOM clusters.
We have incorporated AFN on our online gaming platform to
proactively flag, would-be risky players. We provide personalized
time and attentive feature based interventions to the players to pre-
vent the eventualities. We have published our data and source code
at [16]. We contend that AFN is generally applicable for actionable
forecasting in a variety of business domains and use cases.
There are two systematic exploration possibilities that one could
extend forward from this work. Firstly, one could revisit the logic for
reconstruction of a sample from the VAE based network to further
improve MSE’s. Secondly, one could explore a different latent space
structure (apart from SOM), for example a 2D grid, by learning the
data relationships as a graph and enforcing those learnings on the
latent space.
REFERENCES
[1]2008. Pearson’s Correlation Coefficient. Springer Netherlands, pearsons. https:
//doi.org/10.1007/978-1-4020-5614-7_2569
[2]Pulkit Agrawal, Aditya Pareek, Rukma Talwadker, and Tridib Mukherjee. 2024.
ARGO - An AI Based Responsible Gamification Framework for Online Skill
Gaming Platform. Association for Computing Machinery, New York, NY, USA.
https://doi.org/10.1145/3632410.3632455
[3]Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Frame-
work. In Proceedings of the 25th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
[4]Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation
of generic convolutional and recurrent networks for sequence modeling. arXiv
preprint arXiv:1803.01271 (2018).
[5]Julia Braverman and Howard J. Shaffer. 2010. How do gamblers start gambling:
identifying behavioural markers for high-risk internet gambling. European
Journal of Public Health (2010). https://doi.org/10.1093/eurpub/ckp232
[6]Mohamad Adam Bujang and Fatin Sapri. 2018. An Application of the Runs Test
to Test for Randomness of Observations Obtained from a Clinical Survey in an
Ordered Population. Malaysian Journal of Medical Sciences (2018).
[7]Deniz Cemiloglu, Emily Arden-Close, Sarah Hodge, Theodoros Kostoulas, Maris
Catania, and Raian Ali. 2020. Towards Ethical Requirements for Addictive Tech-
nology: The Case of Online Gambling. https://doi.org/10.1109/REthics51204.
2020.00007
[8]Surajit Chakrabarty, Rukma Talwadker, and Tridib Mukherjee. 2021. Scarce-
GAN: Discriminative Classification Framework for Rare Class Identification for
Longitudinal Data with Weak Prior. In CIKM.
[9]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
Abbeel. 2016. InfoGAN: Interpretable Representation Learning by Information
Maximizing Generative Adversarial Nets (NIPS’16).
[10] Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning.
1990. STL: A Seasonal-Trend Decomposition Procedure Based on Loess (with
Discussion). Journal of Official Statistics 6 (1990), 3–73.
[11] Abhishek Divekar, Meet Parekh, Vaibhav Savla, Rudra Mishra, and Mahesh
Shirole. 2018. Benchmarking datasets for Anomaly-based Network Intrusion
Detection: KDD CUP 99 alternatives. In 2018 IEEE 3rd International Conference onComputing, Communication and Security (ICCCS). 1–8. https://doi.org/10.1109/
CCCS.2018.8586840
[12] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant
Kalagnanam. 2023. TSMixer: Lightweight MLP-Mixer Model for Multivariate
Time Series Forecasting (KDD ’23). Association for Computing Machinery, New
York, NY, USA. https://doi.org/10.1145/3580305.3599533
[13] Sharanya Eswaran, Mridul Sachdeva, Vikram Vimal, Deepanshi Seth, Suhaas
Kalpam, Sanjay Agarwal, Tridib Mukherjee, and Samrat Dattagupta. 2020. Game
Action Modeling for Fine Grained Analyses of Player Behavior in Multi-Player
Card Games (Rummy as Case Study). In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining.
[14] Sharanya Eswaran, Vikram Vimal, Deepanshi Seth, and Tridib Mukherjee. 2020.
GAIM: Game Action Information Mining Framework for Multiplayer Online
Card Games (Rummy as Case Study). In Advances in Knowledge Discovery and
Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14,
2020, Proceedings, Part II 24.
[15] Vincent Fortuin, Matthias Hüser, Francesco Locatello, Heiko Strathmann, and
Gunnar Rätsch. 2019. SOM-VAE: Interpretable Discrete Representation Learning
on Time Series. In International Conference on Learning Representations . https:
//openreview.net/forum?id=rygjcsR9Y7
[16] Games24x7. 2024. AFN GIT Repo. https://github.com/scarce-user-53/AFN
[17] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov, R. G.
Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. 2000. Phys-
ioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource
for Complex Physiologic Signals. Circulation 101, 23 (2000), e215–e220. Cir-
culation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full
PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.
[18] Goodfellow and Et al. 2014. Generative Adversarial Nets. In Proceedings of the
27th International Conference on Neural Information Processing Systems - Volume
2 (NIPS’14) . 2672–2680.
[19] Matt Gorbett, Hossein Shirazi, and Indrakshi Ray. 2023. Sparse Binary Trans-
formers for Multivariate Time Series Modeling. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’23). As-
sociation for Computing Machinery, New York, NY, USA, 544–556. https:
//doi.org/10.1145/3580305.3599508
[20] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. 2022. Why do tree-based
models still outperform deep learning on typical tabular data?. In Advances in
Neural Information Processing Systems.
[21] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot,
Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-
VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.
In5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings.
[22] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation (1997).
[23] Bradley Huitema and Sean Laraway. 2006. Autocorrelation.
[24] Robin John Hyndman and George Athanasopoulos. 2018. Forecasting: Principles
and Practice (2nd ed.). OTexts, Australia.
[25] kaggle. 2018. credit card fraud. https://www.kaggle.com/mlg-ulb/creditcardfraud
[26] Lutz Kilian and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis.
Cambridge University Press.
[27] Diederik P. Kingma and Max Welling. 2019. An Introduction to Variational
Autoencoders. CoRR (2019).
[28] Teuvo Kohonen. 1990. The self-organizing map. Proc. IEEE 78, 9 (1990), 1464–
1480.
[29] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2017. Modeling
Long- and Short-Term Temporal Patterns with Deep Neural Networks. CoRR
(2017).
[30] Bryan Lim, Sercan Ö. Arık, Nicolas Loeff, and Tomas Pfister. 2021. Temporal
Fusion Transformers for interpretable multi-horizon time series forecasting.
International Journal of Forecasting (2021).
[31] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and
Schahram Dustdar. 2021. Pyraformer: Low-complexity pyramidal attention for
long-range time series modeling and forecasting. In International Conference on
Learning Representations.
[32] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting
Model Predictions. In NIPS.
[33] Qianli Ma and Et al. 2019. Learning Representations for Time Series Clustering.
InAdvances in Neural Information Processing Systems.
[34] Laura Manduchi, Matthias Hüser, Martin Faltys, Julia Vogt, Gunnar Rätsch, and
Vincent Fortuin. 2021. T-DPSOM: An Interpretable Clustering Method for Unsu-
pervised Learning of Patient Health States. In Proceedings of the Conference on
Health, Inference, and Learning (CHIL ’21).
[35] Sidneyeve Matrix. 2014. The Netflix Effect: Teens, Binge Watching, and On-
Demand Digital Media Trends. Jeunesse: Young People, Texts, Cultures 6 (01 2014),
119–138. https://doi.org/10.1353/jeu.2014.0002
[36] Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial
Nets.
5134KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
[37] Ramesh N R, Pruthvi S, and Phaneendra Mallekavu. 2018. A Comparative Study on
Social Media Usage and Health Status among Students Studying in Pre-University
Colleges of Urban Bengaluru. Indian Journal of Community Medicine 43 (07 2018),
180–184. https://doi.org/10.4103/ijcm.IJCM_285_17
[38] ncei. [n. d.]. Weather Dataset. https://www.ncei.noaa.gov/data/local-
climatological-data/.
[39] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.
A time series is worth 64 words: Long-term forecasting with transformers. arXiv
preprint arXiv:2211.14730 (2022).
[40] Tom Pollard, Alistair Johnson, Jesse Raffa, Leo Anthony Celi, Omar Badawi, and
Roger Mark. 2009. eICU Collaborative Research Database (version 2.0). PhysioNet
(2009).
[41] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. 2019. Generating Diverse
High-Fidelity Images with VQ-VAE-2. In Advances in Neural Information Process-
ing Systems, NIPS.
[42] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I
Trust You?": Explaining the Predictions of Any Classifier. In KDD.
[43] Susan Rose and Arun Dhandayudham. 2014. Journal of Behavioral Addictions
JBA 3, 2, 83 – 89. https://doi.org/10.1556/jba.3.2014.003
[44] Skipper Seabold and Josef Perktold. 2010. statsmodels: Econometric and statistical
modeling with python. In 9th Python in Science Conference.
[45] Deepanshi Seth, Rukma Talwadker, Tridib Mukherjee, Usama Chitapure, Nagesh
Adiga, and Avantika Gupta. 2021. AI Based Information Retrieval System for Iden-
tifying Harmful Online Gaming Patterns. In Proceedings of the 44th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
[46] Mohammad Amin Shabani, Amir H. Abdi, Lili Meng, and Tristan Sylvain. 2023.
Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Fore-
casting. In The Eleventh International Conference on Learning Representations.
https://openreview.net/forum?id=sCrnllCtjoE
[47] Rukma Talwadker, Surajit Chakrabarty, Aditya Pareek, Tridib Mukherjee, and
Deepak Saini. 2022. CognitionNet: A Collaborative Neural Network for Play
Style Discovery in Online Skill Gaming Platform. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining.
[48] Sean J Taylor and Benjamin Letham. 2018. Forecasting at scale. The American
Statistician 72, 1 (2018), 37–45.
[49] Artur Trindade. 2015. ElectricityLoadDiagrams20112014. UCI Machine Learning
Repository. DOI: https://doi.org/10.24432/C58C86.
[50] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Dis-
crete Representation Learning. In Proceedings of the 31st International Conference
on Neural Information Processing Systems (NIPS’17).
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[52] Wikipedia contributors. 2024. Rummy — Wikipedia, The Free Encyclopedia.
https://en.wikipedia.org/w/index.php?title=Rummy&oldid=1194246527. [Online;
accessed 7-February-2024].
[53] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-
composition transformers with auto-correlation for long-term series forecasting.
Advances in Neural Information Processing Systems 34 (2021), 22419–22430.
[54] Shuchen Wu, Noemi Elteto, Ishita Dasgupta, and Eric Schulz. 2022. Learning
Structure from the Ground up—Hierarchical Representation Learning by Chunk-
ing. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,
Inc., 36706–36721. https://proceedings.neurips.cc/paper_files/paper/2022/file/
ee5bb72130c332c3d4bf8d231e617506-Paper-Conference.pdf
[55] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with
Graph Neural Networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD ’20). Association for
Computing Machinery, 753–763.
[56] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay
Chandrasekhar. 2018. Adversarially Learned Anomaly Detection. In 2018 IEEE
International Conference on Data Mining (ICDM). 727–736. https://doi.org/10.
1109/ICDM.2018.00088
[57] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformers
effective for time series forecasting?. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 37. 11121–11128.
[58] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-
Dimension Dependency for Multivariate Time Series Forecasting. In International
Conference on Learning Representations.
[59] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 35. 11106–11115.
[60] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. Fedformer: Frequency enhanced decomposed transformer for long-term
series forecasting. In International Conference on Machine Learning. PMLR, 27268–
27286.6 APPENDIX
Appendix A IMPLEMENTATION DETAILS
A.1 Model Training Details
For running various experiments on players data as well as other
real world datasets, we procured AWS g4dn.xlarge (1x NVIDIA T4
GPU, 4 vCPUs, 16GB RAM) instance for 𝐾=5,𝜌=3and batch
size of 128. The models parameters are described in Table 9. It took
1.7 hours to train the model on 150k train set users.
Parameter Value
DMM 2 linear layers: (500, 128)
Condition Network 1 linear layer: (16,)
ConVAE-SOM Encoder 3 stacked LSTM: (500, 500, 2000)
ConVAE-SOM Decoder 3 stacked LSTM: (2000, 500, 500)
Damp Factor Network 2 linear layers: (100, 10)
LSTM Forecasting Module LSTM: (100, )
with attention
Table 9: Model Parameters Configuration
When compared to SOTA transformer models, AFN exhibits a
substantial reduction in the number of parameters required for effec-
tive forecasting. While transformers are known for their impressive
performance across various domains, they tend to be parameter-
intensive, often requiring massive computational resources. In con-
trast, AFN demonstrates a clever combination of Deep Markov
Model and Damp Factor, allowing it to achieve comparable Mean
Squared Error (MSE) results with a significantly smaller parameter
count.
A.2 Hyper-Parameter Selection
Hyper-parameters for AFN were chosen using the heuristics pro-
vided in [ 34] and further tuned using Optuna[ 3] open-source Python
library. The following settings of the various weights were indicated
to be optimal.
Hyper-Parameter Value
Alpha (𝛼) 10.0
Beta (𝛽) 0.1
Gamma (𝛾) 0.3
Kappa (𝜅) 1.0
Theta (𝜃) 0.1
Eta (𝜂) 10.0
Tau (𝜏) 75
Table 10: Final Hyper-Parameters used in Training
A.3 Details of Ablation Experiments on AFN
Figure 6 shows the variation of MSE in training workflow across
all the ablation experiments. TM helps to capture the parameter
distribution well, especially for non-linear player data and in its
absence MSE could be high due lack of guiding loss function for non-
linear data. Without AL, the MSE starts well, as TM is mapping the
space well, however the rise in MSE during prediction fine tuning
is significant due to lack of long term pattern purview. Without
DF, the model is not able to predict farther nodes leading to higher
MSE. AFN outperforms all the ablations.
5135Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay KDD ’24, August 25–29, 2024, Barcelona, Spain
((a)) Without TM
((b)) Without AL
((c)) Without DF
((d)) AFN
Figure 6: MSE vs Epoch line-graphs on different ablations experi-
ments.
Module Type Complexity per Layer
Transition Module O(𝑘.𝑑2)
ConVAE-SOM O(𝑘.𝑑2)
Intelligent Forecasting Module O(𝑛.𝑑2)
Table 11: Time complexity of AFN modules. 𝑛=Sequence Length,
𝑑=Representation dimension, 𝑘=Number of hidden layers
Appendix B TRAINING COMPLEXITY
The time complexity of the training algorithm is discussed in Ta-
ble 11, deriving inspiration from [51].Model MAE MSE
AFN 0.124 0.039
CrossFormer 0.067 0.010
PatchTST 0.059 0.009
Table 12: Comparison of AFN with SOTA models
((a))ECL Dataset
 ((b))ETTh1 Dataset
((c))WTH Dataset
 ((d))Players Data
Figure 7: Auto-correlation Function (ACF) correlogram of different
datasets
Appendix C EXTRA EXPERIMENTAL
RESULTS
C.1 Comparison with MTS Forecasting SOTA
Table 12 compares the forecasting performance of AFN with the
SOTA transformer-based MTS forecasting models such as Cross-
former [ 58] and PatchTST [ 39], on players data. We choose to
compare with only these models since they have shown to have
out-performed InFormer [ 59], AutoFormer [ 53], PyraFormer [ 31]
and FEDFormer [ 60]. Our findings indicate that while the AFN’s
MAE and MSE may not always be the highest, it offers comparable
results with the added advantages of interpretability and explain-
ability. For players data, PatchTST outperforms CrossFormer.
C.2 Auto-correlation tests
We compare correlograms in Figure 7 for players data with several
open-source MTS datasets such as ECL, ETTh1 and WCL. We can
clearly see that the lagged auto-correlations of the open-sourced
datasets have higher instances and magnitude of significance.
C.3 Visualization of Time Series
Decomposition
Figure 8 describes the Time Series Decomposition of different
datasets as mentioned in Section 4.2.3. In order to effectively com-
pare the long term trend and seasonal patterns, we took 500 data
points for each of these datasets. Since ETTh, ECL and WTH
datasets are at an hourly frequency whereas the players dataset is
at a daily frequency, the difference in time span along the X-axis is
observed.
5136KDD ’24, August 25–29, 2024, Barcelona, Spain Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, & Tridib Mukherjee
Figure 8: These decompositions demonstrate the diverse character-
istics of time series data. ECL, ETTh and WTH exhibits clear long-
term trends and predictable seasonal patterns, making it amenable
to forecasting and analysis. On the other hand, Players data lacks
such structure, suggesting random fluctuations rather than pre-
dictable cycles. Furthermore, the residuals are more prominent
(high deviation from the zero mean) in Players data indicating a
high degree of randomness.
Figure 9: Distribution of Pearson’s Correlation values between
non-linear trajectory and Markov condition change
Figure 10: Distribution of most important features across the SOM
grid
C.4 Quantifying Interpretability and
Visualisation of the SOM grid
With player data neighbouring transitions could be non-smooth,
hampering interpretability. AFN leverages condition switch as an
explanation to justify it. To validate if correlation between a long
jump and condition switch is dependable, we created new time se-
ries by calculating shortest distance between the clusters assigned
to the two successive time steps and correlated with the change in
conditions predicted at each point. We found the Pearson’s correla-
tion [ 1] to be primarily high and positive in most cases. Figure 9
shows distribution of the correlation for about 3000 player samples
that we forecasted. We take absolute value of the correlation, as
both the positive and negative values contribute equally to the in-
terpretations. The distribution is mostly centered towards a higher
correlation coefficient with mean of 0.726 and a median value of
0.797. With mean and the median quite closer distribution seems
less skewed to the left. This exercise assures that the transition to
farther nodes are mostly associated with a change in the underlying
Markov condition and hence is well interpretable. Figure 10 shows
the most important features for each SOM grid.
5137