LASCA: A Large-Scale Stable Customer Segmentation Approach
to Credit Risk Assessment
Yongfeng Gu∗
Ant Group
Hangzhou, Zhejiang, China
guyongfeng.gy@antgroup.comYupeng Wu∗
East China Normal University
School of Computer Science and
Technology
Shanghai, China
yupeng.wu@stu.ecnu.edu.cnHuakang Lu
East China Normal University
School of Computer Science and
Technology
Shanghai, China
huakang.lu@stu.ecnu.edu.cn
Xingyu Lu†
Ant Group
Hangzhou, Zhejiang, China
sing.lxy@antgroup.comHong Qian†
East China Normal University
School of Computer Science and
Technology
Shanghai, China
hqian@cs.ecnu.edu.cnJun Zhou†
Ant Group
Hangzhou, Zhejiang, China
jun.zhoujun@antgroup.com
Aimin Zhou
East China Normal University
School of Computer Science and
Technology
Shanghai, China
amzhou@cs.ecnu.edu.cn
ABSTRACT
Customer segmentation plays a crucial role in credit risk assessment
by dividing users into specific risk levels based on their credit scores.
Previous methods fail to comprehensively consider the stability in
the segmentation process, resulting in frequent changes and incon-
sistencies in users’ risk levels over time. This increases potential
risks to a company. To this end, this paper at first introduces and for-
malizes the concept of stability regret in the segmentation process.
However, evaluating stability is challenging due to its black-box
nature and the computational burden posed by vast user data sets.
To address these challenges, this paper proposes a large-scale stable
customer segmentation approach named LASCA. LASCA consists
of two phases: high-quality dataset construction (HDC) and reli-
able data-driven optimization (RDO). Specifically, HDC utilizes an
evolutionary algorithm to collect high-quality binning solutions.
RDO subsequently builds a reliable surrogate model to search for
the most stable binning solution based on the collected dataset. Ex-
tensive experiments conducted on real-world large-scale datasets
(up to 0.8 billion) show that LASCA surpasses the state-of-the-art
∗Equal Contribution.
†Corresponding Authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671550binning methods in finding the most stable binning solution. No-
tably, HDC greatly enhances data quality by 50%. RDO efficiently
discovers more stable binning solutions with a 36% improvement
in stability, accelerating the optimization process by 25 times via
data-driven evaluation. Currently, LASCA has been successfully
deployed in the large-scale credit risk assessment system of Alipay.
CCS CONCEPTS
•Computing methodologies →Machine learning; •Applied
computing→Operations research.
KEYWORDS
Large-scale customer segmentation, Credit risk assessment, Stabil-
ity, Data-driven optimization, Reliable surrogate model
ACM Reference Format:
Yongfeng Gu, Yupeng Wu, Huakang Lu, Xingyu Lu, Hong Qian, Jun Zhou,
and Aimin Zhou. 2024. LASCA: A Large-Scale Stable Customer Segmenta-
tion Approach to Credit Risk Assessment. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671550
1 INTRODUCTION
Customer segmentation [ 1,4,8,12,14,32,42] plays a fundamental
and pivotal role in credit risk assessment systems across various
industries such as insurance, lending, payment and investment. In
practice, customer segmentation is often regarded as a binning pro-
cess based on the users’ credit data for the sake of interpretability.
Given the continuous users’ credit risk scores, customer segmen-
tation aims to find a suitable binning solution, that is, a series of
5006
KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
split-points, to divide users into discrete risk levels, based on spe-
cific business requirements. Then the users in low-risk bins can
enjoy incentives such as higher loan amounts and extended repay-
ment periods, while the users in high-risk bins may face penalties
such as loan denials and being blacklisted.
Related Work. To address this segmentation problem, vari-
ous binning methods [ 2,9,18,24,29,34,37] are proposed. Equal-
size [ 24] ensures a similar or equal number of users in each bin.
Equal-size is unable to consider any requirements in the binning
process and is often recognized as a baseline method. The mono-
tone adjacent pooling algorithm [ 37] (MAPA) first sees each user
as a pre-bin and then conducts a heuristic approach to merge the
adjacent pre-bins. In each iteration, adjacent pre-bins with the
smallest weight of evidence (WoE) differences are merged to ensure
a monotonic trend (ascending or descending) of WoE within each
bin. The monotonic binning [ 6] (MonoBin) first derives an initial
binning solution based on a pre-binning algorithm, like Equal-size.
Then it checks for monotonicity in the measurements between
these refined bins. In each iteration, bins that violate monotonicity
the most severely are merged, i.e., the maximum violation prin-
ciple. OptBinning [ 29] models the general binning problem to a
mixed-integer programming problem (MIP) by operation research
techniques, where the constraints and objectives are carefully for-
mulated. OptBinning is recognized as a state-of-the-art method that
considers multiple monotonic trends, including concave, convex,
peak, and valley, making it superior to other methods. However,
it requires pre-computing relevant measurements as parameters
before modeling, making it challenging to apply to large-scale op-
timization scenarios, which frequently arise in diverse industrial
tasks [3, 10, 22, 25, 38, 40, 41].
Limitation of Current Work and Motivation of This Work.
Previous studies have primarily concentrated on integrating basic
monotonicity requirements, neglecting a comprehensive considera-
tion of stability in the binning process. In industrial applications,
achieving a stable binning solution is crucial to ensure the con-
sistency of measurements, such as user distribution or retention
rate, across different periods. For instance, minor fluctuations in
users’ credit scores between months should not significantly impact
the outcomes of customer segmentation. In contrast, an unstable
binning solution exhibits high sensitivity to even slight variations
in users’ credit scores, resulting in frequent changes and an incon-
sistent assessment of risk levels. Such instability can be detrimental
to user interests and increase potential risks for a company.
Challenges. Unfortunately, two main challenges are encoun-
tered when evaluating the stability of a binning solution: 1) The
stability evaluation is black-box. 2) The stability evaluation is costly.
Firstly, the term "black-box" describes the challenges of the problem,
not the limitations of our approach. When evaluating the stability
of a solution, it is necessary to gather information from various
aspects, such as user distribution, monotonicity, and retention ratio.
For instance, when calculating the retention ratio, it is essential to
query the database to calculate how many customers maintain the
same risk levels across months. However, expressing this process
explicitly through mathematical equations or formulas is challeng-
ing. This inherent complexity leads to the stability evaluation being
perceived as a black box. Due to this black-box nature, it becomes
difficult to find a stable binning solution using precise methodsdirectly. Secondly, in practice, handling large-scale user data, e.g.,
0.8 billion, is a great challenge. The evaluation of the stability of
a binning solution is an extremely time-consuming process, as
it involves performing extensive queries, statistical analyses, and
computations on a historical user database. The time required to
evaluate a single binning solution increases exponentially as the
data scales grow. For instance, evaluating a binning solution on
500 million user data points can take nearly 1000 seconds. The
high-cost evaluation associated with this problem has heightened
the need for the proposed method to meet requirements in terms
of efficiency and scalability.
The Proposed Method. To address the challenges above, this
paper introduces and formalizes the concept of stability regret in
the segmentation process to measure how unstable a solution is.
In particular, stability regret is determined by evaluating three key
factors: user distribution regret, monotonicity regret, and retention
rate regret. A lower stability regret value indicates a higher degree
of stability in a binning solution. Based on the definition of stability
regret, this paper proposes a large-scale customer segmentation
approach (LASCA) to find the most stable binning solution. LASCA
consists of two primary phases, i.e., high-quality dataset construc-
tion (HDC) and reliable data-driven optimization (RDO). Firstly,
to address the challenge of black-box optimization, in the HDC
phase, an evolutionary algorithm with a probabilistic density ap-
proximation (PDA) initialization strategy is designed to help collect
high-quality binning solutions. Secondly, to address the challenge
of costly evaluation, in the RDO phase, a reliable surrogate model
is built based on the collected high-quality dataset to search for
the most stable solution. This model considers the reliability of
each base model in the ensemble process and helps RDO find the
most stable solution with a low computational cost. Additionally,
our approach uses transparent probabilistic modeling and simple
mutation operators to ensure interpretability and traceability.
Empirically, extensive results on real-world large-scale datasets
(up to 0.8 billion) from the Alipay platform show that: 1) LASCA
can find the most stable solution compared with state-of-the-art
binning methods. 2) HDC collects a high-quality dataset in an
acceptable time, leading to a 50% stability improvement. 3) RDO can
find better solutions with low computational cost, improving 36%
stability to the collected solutions and speeding up 25 ×times with
data-driven evaluation. Currently, LASCA has been successfully
implemented in the large-scale credit risk assessment system of
Alipay, showcasing its stability, scalability, and practicality in large-
scale segmentation scenarios. By providing a stable and reliable
binning solution, LASCA plays a pivotal role in diverse scenarios
including lending, payment, and investment.
Contribution. In a nutshell, the contribution of this paper is
threefold. 1) This paper introduces and formalizes the concept of
stability regret, which provides a comprehensive consideration and
evaluation of stability by incorporating factors such as user distri-
bution, monotonicity, and retention rate. 2) To efficiently minimize
the stability regret, we further propose a large-scale stable cus-
tomer segmentation approach called LASCA that consists of two
phases: high-quality dataset construction (HDC) and reliable data-
driven optimization (RDO). 3) The extensive experimental results
on real-world large-scale datasets (up to 0.8 billion) demonstrate
5007LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment KDD ’24, August 25–29, 2024, Barcelona, Spain.
Feb.Mar.Apr.
Risk levelsUser Number12345Risk levelsFPDTime periodsCredit scoreJan.Feb.Apr.Mar.12345(A)User DistributionJan.Feb.Mar.Apr.Jan.(B)Monotonicity(C)RetentionRateUserAUserBUserC…
12345
Stability
Figure 1: In stable customer segmentation, three stability
factors should be considered and satisfied over periods, e.g.,
from January to April.
that LASCA finds more stable solutions compared with state-of-the-
art binning methods (73.6% improvements) and can significantly
reduce computational costs (25× times speedup). Notably, LASCA
has been successfully deployed in the system of Alipay and its
power is validated and released.
The subsequent sections of this paper respectively present the
background and formulation of the large-scale stable customer
segmentation problem, introduce the proposed LASCA, show the
real-world experiment results, and finally conclude the paper.
2 STABLE CUSTOMER SEGMENTATION
In credit risk management, customer segmentation is a binning
problem that aims to divide users into different risk levels based
on their credit scores. The binning method used in industry is to
find a series of split-points, which we call the binning solution, to
distinguish users into different risk levels. For instance, if the goal
is to divide users into five separate risk levels, we only need to find
four split points as our binning solution.
During the segmentation process, some business requirements
should be taken into account. Moreover, since a user’s credit score
may fluctuate (up or down) slightly over periods due to their behav-
iors, it is vital to find a stable binning solution to ensure that these
requirements are still satisfied at all times. In segmentation, we refer
to these temporal requirements as stability factors. Stable customer
segmentation is defined as a problem in finding a stable binning
solution that satisfies the stability factors. In Alipay, customer seg-
mentation is identified as a downstream task dependent on the
outcomes of credit scoring. The process of credit scoring does not
entail stability requirements. Incorporating the stability considera-
tions in credit scoring may affect the other downstream tasks of the
scoring. Hence, we consider stability exclusively within customer
segmentation, rather than in the credit scoring itself. However, in
practice, maybe no perfect solution can fulfill all factors. Thus, the
most stable segmentation is the one that satisfies the maximum
number of stability factors.
2.1 Stability Factors
Fig. 1 visualizes the three stability factors that should be considered
in the customer segmentation problem, that is, user distribution,
monotonicity, and retention rate. Specifically, in this case, we aimto divide users into five risk levels, e.g., from 1 to 5, and assess the
stability factors over four months, e.g., from January to April. Note
that level 1 represents the safest level while level 5 indicates the
most risky level.
2.1.1 User Distribution. It is imperative to maintain a consistent
distribution of user numbers across varying periods. Commonly
employed distributions encompass uniform, normal, ascending,
and descending types. Adhering to these distribution types allows
companies to manage the user population within each risk level,
thereby exercising control over the overall risk exposure. As shown
in Fig. 1(A), to approximate a normal distribution type, one might
opt for a configuration where high-risk and high-safety groups
constitute a relatively smaller proportion, while the majority falls
within the intermediate risk levels. Our goal is to maintain a con-
sistent user distribution type in all months. Notably, the principle
of setting the target distribution is to mitigate risk by regulating
user distribution. In Alipay, the specific distribution type varies
depending on the source of credit scores. For scores based on user
behavior, a descending distribution is preferred to ensure lower
risk corresponds with a larger user number, improving user admis-
sion rates. Conversely, for scores from profile indicators, a normal
distribution is sought to align with public perception.
2.1.2 Monotonicity. Another crucial requirement is that the first
payment default (FPD) rate ought to exhibit an ascending trend
with risk levels, as depicted in Fig. 1(B). FPD rate [ 19] measures
the percentage of users who fail to make their first payment on
a loan, indicating the riskiness of these users. In other words, a
higher FPD rate implies greater risk associated with users within
that particular level. Our goal is to keep the same monotonicity
trend in all months.
2.1.3 Retention Rate. Maintaining consistent risk levels for users
over time is important. As shown in Fig. 1(C), Users’ credit scores
might slightly change due to their past behavior, such as whether
they pay on time. However, if a binning solution is too sensitive
to minor score changes, it can cause frequent risk level changes.
This makes the whole credit risk assessment system unstable and
also impacts the incentives or penalties that users receive. We track
how many people stay at the same risk level from one month to
the next, called the retention rate. Our goal is to keep the retention
rate higher than a given threshold at each risk level.
2.2 Problem Definition and Formulation
In practice, customer segmentation first divides users into 𝑀pre-
bins using the Equal-size method [ 24] and then merges these pre-
bins into𝑁refined bins ( 𝑀>𝑁). Let vector 𝒃=(𝑏1,...,𝑏𝑀)denote
𝑀pre-bins, where 𝑏𝑖is the user number of the 𝑖-th pre-bin. A
binning solution 𝒙∈Xis defined as 𝒙=(𝑥1,...,𝑥𝑁−1), where𝑥𝑖
represents the 𝑖-th split-point between two consecutive bins. Thus, a
vector 𝒙can delineate 𝑁refined bins:[1,𝑥1],(𝑥1,𝑥2],...,(𝑥𝑁−1,𝑀].
For instance,[1,𝑥1]means putting the pre-bins 𝑏1,...,𝑏𝑥1into the
first refined bins. A typical example is provided in the upper left
corner of Fig. 2: 𝒙=(3,5,8,12)is a feasible solution for dividing 15
pre-bins (i.e., 1∼15) into 5 refined bins (i.e., 1∼5).
To assess the stability of binning solutions, the concept of re-
gret [ 36] is employed to quantify the violations of stability factors,
5008KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
referred to as stability regret. The higher the regret, the more sta-
bility factors the binning solution violates. The stability regret is
calculated as the aggregate sum of three individual regrets: user dis-
tribution regret, monotonicity regret, and retention rate regret. The
three stability factors are interactive, and focusing on one might
negatively affect another. In our paper, we combined them as an
overall objective of LASCA, weighting them equally. Future practice
should weight metrics as per business needs.
Let𝑓(𝒙)denote the overall stability regret. The customer seg-
mentation problem aims to find the most stable binding solution
𝒙∗, which minimizes the aggregate regret function 𝑓(𝒙).
arg min
𝒙∈X𝑓(𝒙)=𝑓1(𝒙)+𝑓2(𝒙)+𝑓3(𝒙),(1)
where𝑓1(𝒙),𝑓2(𝒙)and𝑓3(𝒙)denotes the individual regret functions
of user distribution, monotonicity, and retention rate, respectively.
These functions are determined by the solution 𝒙and can only
be obtained by querying the entire user dataset. The calculation
process for these functions is challenging to explicitly be displayed
mathematically, and hence, it is considered to be black-box. Then, in
this paper, we consider stable customer segmentation as a black-box
optimization problem to solve. The detailed business logic of the
Eq. (1) is given in Appendix A.
In industrial applications, the value of 𝑀typically spans from
hundreds to thousands, resulting in a vast search space for finding
the most stable binning solution 𝒙∗. Moreover, the user data in these
scenarios can reach up to a staggering 0.8 billion. The calculation
involved in 𝑓1(𝒙),𝑓2(𝒙), and𝑓3(𝒙)is greatly challenging on large-
scale of user data.
3 THE PROPOSED LASCA
As depicted in Fig. 2, LASCA comprises two primary phases: high-
quality dataset construction (HDC) and reliable data-driven opti-
mization (RDO). In the HDC phase, an evolutionary algorithm with
probabilistic density approximation (PDA) initialization is designed
to collect a high-quality dataset. Subsequently, in the RDO phase,
a reliable surrogate model is constructed based on the collected
high-quality dataset to explore the most stable solution. This model
considers each base model’s reliability in the ensembling process,
facilitating RDO in identifying the most stable solution with low
computational costs.
3.1 High-Quality Dataset Construction
High-quality dataset construction (HDC) is the crucial initial phase
of LASCA, which aims to acquire a high-quality binning dataset
through an evolutionary algorithm. The term "high-quality" refers
that the dataset has a low average stability regret, which indicates
the collected binning solutions are stable enough. This collected
dataset is important for LASCA since it is used to learn a surrogate
model in subsequent data-driven optimization. A higher-quality
dataset helps to learn a more reliable surrogate model, which helps
LASCA find more stable solutions.
The HDC process comprises four stages: Initialization, Mutation,
Evaluation, and Selection. Initialization: A novel method, probabilis-
tic density approximation (PDA), is devised to generate an initial
population of 𝑁𝑃solutions serving as starting points. Mutation:
The strictly increasing mutation (SIM) method is designed to ensuresolution validity and diversity by updating and creating new solu-
tions. Evaluation: The stability regret of each solution is evaluated
in this process. Selection: Elite selection [ 5] is employed where only
solutions with low stability regrets are retained for the next genera-
tion, ensuring iterative improvement. These processes iterate until
reaching the termination condition, e.g., the maximum number
of iterations. Through this process, HDC effectively constructs a
dataset that helps LASCA find highly stable customer segmenta-
tions. Finally, the binning solutions evaluated in every iteration are
added to the collected dataset.
3.1.1 PDA Initialization. The quality of these initial solutions greatly
impacts the convergence speed and the quality of the collected
datasets [ 16,31]. In practical customer segmentation, satisfying the
user distribution requirement (ascending, descending, normal, and
uniform) is more challenging compared with meeting the mono-
tonicity and retention rate requirements. To address this challenge,
this paper introduces a problem-specific initialization method called
probabilistic density approximation (PDA). PDA is specifically de-
signed to generate a population of initial solutions that closely
match the target user distribution. Algorithm 1 outlines the proce-
dure of the PDA. PDA takes the pre-bins vector 𝒃, the number of
refined bins 𝑁, and the target user distribution Q, as inputs, ulti-
mately producing the initial solution set 𝑋. The PDA process can be
roughly divided into three steps: parameter estimation, probability
approximation, and solution ranking.
Parameter Estimation. This step estimates the parameters of the
target distribution based on the idea of maximum likelihood estima-
tion (MLE), cf. line 2 ∼4 in Algorithm 1. Let 𝑃𝑖denote the cumulative
probability for the first 𝑖pre-bins, that is, 𝑃𝑖=Σ𝑖
𝑚=1𝑏𝑚
Σ𝑀
𝑚=1𝑏𝑚. PDA de-
scribes the parameters of the target distribution in terms of the 1/𝑁
quantile𝑃𝑥1, i.e., the proportion of the first refined bin. We set the
value of𝑃𝑥1ranges from 𝜃1to𝜃2, where these two boundaries are
dependent by distribution type Q. For instance, for the descending
and ascending distributions, we refer to the exponential distribu-
tion and set 𝜃1=𝑒𝜆1−1
𝑒𝜆1(1−𝑒𝑁𝜆1),𝜃2=𝑒𝜆2−1
𝑒𝜆2(1−𝑒𝑁𝜆2), where𝜆1,𝜆2are
the parameters of the exponential distribution. By doing so, we can
ensure that the size of the latter bin is 𝑒−𝜆times greater than that
of the previous bin. For the normal distribution, considering that
there is an ascending trend before the peak, we set 𝜃1=0,𝜃2=0.5
since the proportion of the first bin cannot be larger than 50%. For
the uniform distribution, we set 𝜃1,𝜃2near1
𝑁. After𝑃𝑥1is deter-
mined, the proportions of the remaining 𝑁−1bins(𝑃𝑥2,...,𝑃𝑥𝑁−1)
can be determined by MLE. The proportions are estimated using
equally spaced percentage functions (PPF) and cumulative distri-
bution functions (CDF). The proportions are denoted as the ideal
cumulative bin proportion C∗=(C∗
1,..., C∗
𝑁−1), where C∗
1=𝑃𝑥1.
Probability Approximation. This step approximates the ideal cu-
mulative probability distribution C∗in a heuristic way, cf. line 5 ∼10
in Algorithm 1. Suppose that 𝑥′
𝑗,𝑥′′
𝑗are two neighbors for each
𝑥𝑗. The neighbors 𝑥′
𝑗and𝑥′′
𝑗are defined as the possible binning
boundaries closest to C∗
𝑗, i.e.,𝑥′
𝑗=max𝑖∈[𝑀]{𝑖|𝑃𝑖≤C∗
𝑗}and
𝑥′′
𝑗=min𝑖∈[𝑀]{𝑖|𝑃𝑖>C∗
𝑗}. To introduce randomness into PDA,
we choose𝑥𝑗from𝑥′
𝑗and𝑥′′
𝑗using the roulette wheel strategy [ 23].
5009LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment KDD ’24, August 25–29, 2024, Barcelona, Spain.
High-quality dataset
CollectInputMutationEvaluationSelectionProbabilistic Density Approximation (PDA)𝑀!𝑀"𝑀#Reliable surrogate modelReliable surrogate evaluation (RSE)Newly generatedsolutionsEvaluatedsolutionsSol.1Sol.2Sol.3Mono.Distr.Ret.MutationSelectionSolution encodings3     5     8    12Bin1: [1, 2, 3]Bin 2: [4, 5]Bin 3: [6,7,8]Bin 4:[9,10,11,12]Bin 5: [13,14,15]Binning solutions
Representation(PDA)High-quality dataset construction (HDC)Reliable data-driven optimization (RDO)InitializationOutputThe most stable solution
Evaluatedbinning data𝒙𝒙∗
ProbabilityApproximationSolutionRankingC*Evaluation
15C*𝑷𝒙𝟏2431524315243𝑷𝒙𝟏𝑷𝒙𝟏152431524315243C*152431524315243𝑪𝒂𝒏𝒐𝒓𝒎Parameter Estimation152431524315243NORMDESCUNIFROM𝑪𝒃𝒏𝒐𝒓𝒎𝑪𝒄𝒏𝒐𝒓𝒎𝑪𝒂𝒅𝒆𝒔𝒄𝑪𝒃𝒅𝒆𝒔𝒄𝑪𝒄𝒅𝒆𝒔𝒄𝑪𝒂𝒖𝒏𝒊𝒇𝑪𝒃𝒖𝒏𝒊𝒇𝑪𝒄𝒖𝒏𝒊𝒇𝑪𝒂𝒏𝒐𝒓𝒎𝑪𝒃𝒏𝒐𝒓𝒎𝑪𝒄𝒏𝒐𝒓𝒎𝑪𝒃𝒅𝒆𝒔𝒄𝑪𝒄𝒅𝒆𝒔𝒄𝑪𝒂𝒅𝒆𝒔𝒄𝑪𝒄𝒖𝒏𝒊𝒇𝑪𝒂𝒖𝒏𝒊𝒇𝑪𝒃𝒖𝒏𝒊𝒇
(VGPE)(RSE)(SIM)(Elite)(SIM)(Elite)
Figure 2: LASCA comprises two primary phases: high-quality dataset construction (HDC) and reliable data-driven optimization
(RDO). In HDC, an evolutionary algorithm is designed to collect high-quality solutions (low stability regret) efficiently. RDO
then utilizes the collected dataset to construct a reliable surrogate model and search for the most stable solution. 𝑆𝑜𝑙.1,𝑆𝑜𝑙.2
and𝑆𝑜𝑙.3represent the evaluated binning solutions.
Algorithm 1 Probabilistic Density Approximation (PDA)
Input: Pre-bins vector 𝒃, number of refined bins 𝑁, and target user
distribution typeQ.
Output: Initial solution set 𝑋.
1:Initialization, the initial solution set 𝑋←∅ , the number of
pre-bins involved in the first bin 𝑥1=1.
2:Calculate the thresholds [𝜃1,𝜃2]based onQ.
3:while𝜃1≤𝑃𝑥1≤𝜃2do
4: Calculate the ideal cumulative probability C∗.
5:for𝑗from 2to𝑁−1do
6: Find two neighbors 𝑥′
𝑗and𝑥′′
𝑗for𝑥𝑗.
7: Choose𝑥𝑗according to the Roulette Wheel Strategy.
8:end for
9: Calculate the actual cumulative probability C.
10: 𝒙.𝑠𝑐𝑜𝑟𝑒←𝐾𝐿(C,C∗).
11:𝑋=𝑋∪{𝒙}.
12:𝑥1=𝑥1+1.
13:end while
14:Duplicate and then sort 𝑋by𝒙.𝑠𝑐𝑜𝑟𝑒 .
The probability of 𝑥′
𝑗and𝑥′′
𝑗being selected is positively correlated
with the distances 𝑃𝑥′
𝑗and𝑃𝑥′′
𝑗from C∗𝑗.
After a complete solution 𝒙is obtained, PDA calculates the KL
divergence between the actual and ideal cumulative probability
and defines this distance as the score of each solution, cf. line 11 in
Algorithm 1. PDA then adds the solution into the candidate 𝑋and
increases the boundary 𝑥1by 1. The above process repeats until
the proportion of the first bin, 𝑃𝑥1, is out of the threshold [ 𝜃1,𝜃2].
Solution Ranking. This step filters out the duplicated solutions
and then sorts 𝑋based on the score of each solution ( 𝒙.𝑠𝑐𝑜𝑟𝑒 ), cf.
line 14 in Algorithm 1. By doing so, an initial solution set 𝑋is then
obtained. In practical implementation, considering the inherentrandomness in PDA, we execute PDA multiple times to generate a
set of solutions with 𝑁𝑃size.
3.1.2 Strictly Increasing Mutation. In our solution representation,
there is an implicit and strict constraint for each solution 𝒙, that
is,𝑥1<···<𝑥𝑁−1. To address this issue, we designed the strictly
increasing mutation (SIM) to ensure the validity of each solution
during iterations. SIM utilizes a mutation rate to select an element
𝑥𝑖from the solution 𝒙and replaces it with a random integer within
the range of its neighboring elements 𝑥𝑖−1and𝑥𝑖+1to generate
new solutions. SIM offers two key benefits: First, it enforces a strict
ascending order of binning boundaries, thus ensuring the feasibility
of generated solutions ( 𝑥1<···<𝑥𝑁−1), and single-point muta-
tion also ensures interpretability. Second, it can effectively escape
local optima and explore potentially more favorable solution spaces
during iterations.
3.1.3 Virtual Grouped Parallel Evaluation. Due to the large-scale
dataset, the evaluation process accounts for the majority of the
running time of the HDC phase. To speed up the evaluation process,
the virtual grouped parallel evaluation (VGPE) is designed. VGPE
employs multiple workers to evaluate a single binning solution
concurrently. However, although VGPE demonstrates scalability
and efficiency in large-scale datasets compared with traditional
parallel computing strategies, the computation and time costs of
VGPE remain considerable. For instance, It still requires hundreds
of computers to take hundreds of seconds to perform one evolution
iteration. The entire evolution process can extend to several hours or
even span across multiple days. Detailed information and discussion
about VGPE are presented in Appendix B.
3.2 Reliable Data-Driven Optimization
Reliable data-driven optimization (RDO) serves as the second phase
of LASCA. The intuition underlying RDO is to use a high-quality
5010KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
DataSeparationRandomSamplingModelTrainingReliabilityAssessmentReliability-basedEnsembling𝑟1𝑟2𝑟𝐾Sol.
"𝑓!𝒙Ret.Pred."𝑓"𝒙Distr.Pred."𝑓#𝑥Mono.Pred."𝑓𝒙Stab.Pred.!𝑓!(𝒙)=𝑟"∑𝒓*𝑓!"(𝒙)+𝑟#∑𝒓*𝑓!#(𝒙)+…+𝑟$∑𝒓*𝑓!$(𝒙)!𝑓!"(𝑥)!𝑓!#(𝑥)!𝑓!$(𝑥)Sol.Sol.Distr.Sol.Mono.Ret.𝑀!𝑀"𝑀#
Figure 3: An illustration of reliable surrogate evaluation
(RSE). The three regrets (i.e., Mono., Distr., and Ret.) are pre-
dicted individually. A reliable ensemble surrogate model is
trained based on the collected dataset from the HDC phase
and provides surrogate evaluations.
dataset obtained during the HDC phase to construct a surrogate
model. This surrogate model can further explore stable binning
solutions with low computational expenses.
As shown in Fig. 2, RDO exhibits a resemblance to the evolution
process used in HDC. However, instead of using the VGPE, RDO
employs an efficient and reliable surrogate evaluation (RSE). RSE
constructs a surrogate model to predict the stability regrets of
solutions. This can save lots of time cost compared with VGPE
since RSE does not need to evaluate the actual stability regrets
of solutions on a large-scale user dataset as VGPE does [ 13,43].
As demonstrated in previous studies [ 20,27,39], the reliability of
the surrogate model, which aims to provide robust and accurate
evaluations, is of utmost importance. To address this, a reliable
ensemble surrogate model [ 39] is employed for evaluation in RDO.
In the reliable surrogate model, the reliability of each base model is
assessed and then used to enforce a reliability-based ensembling
evaluation. The overall diagram of our RSE is shown in Fig. 3.
3.2.1 Data Preparation and Base Models Training. According to
Eq.(1), stability regret comprises three components: user distri-
bution regret, monotonicity regret, and retention rate regret. To
effectively predict this stability regret, we construct individual sur-
rogate models for each of these three regret. Then, the surrogate
evaluation results for each regret are summed up as the overall
stability regret. To accomplish this, the collected binning data is
initially separated into three distinct datasets. Each of these datasets
shares an identical feature 𝒙, but the label represents a differentstability factor. Then, for each dataset, we use the random sam-
pling strategy with a random probability of 𝜖∈[0,1]to generate
𝐾subsets, denoted as 𝑆1,𝑆2,...,𝑆𝐾. That is, each sample possesses
an𝜖possibility of being selected into each subset. Considering
that the data collected from the HDC phase will not be substantial
enough to train a base model, we assign a relatively high value to 𝜖.
After𝑆1,𝑆2,...,𝑆𝐾are generated, 𝐾base models 𝑀1,𝑀2,...,𝑀𝐾
are trained with 𝐾subsets independently. The following section
introduces how to assess the reliability of these base models.
3.2.2 Reliability Assessment and Evaluation Ensembling. In this
paper, the reliability of base models is defined as the accuracy of
the predicted dominance relationship. Let 𝒙𝑖and𝒙𝑗denote the𝑖-th
and𝑗-th solution in each evolutionary generation, 𝒙𝑖dominants 𝒙𝑗
if and only if 𝑓(𝒙𝑖)<𝑓(𝒙𝑗). As emphasized in [ 26], the accuracy
of dominance relationship predictions is the key of data-driven
optimization Subsequently, we demonstrate that the ability of base
models to predict the dominance relationship hinges on its predicted
stability regret ˆ𝑓(𝒙)and prediction error 𝑠(𝒙)=|𝑓(𝒙)−ˆ𝑓(𝒙)|.
For a base model, it is evident that the actual stability regret 𝑓(𝒙)
ought to be situated within the range of [ˆ𝑓(𝒙)−𝑠(𝒙),ˆ𝑓(𝒙)+𝑠(𝒙)],
which is called predicted regret range. Denote 𝒙𝑖and𝒙𝑗are two
solutions in a generation, and assume the actual dominance relation-
ship𝑓(𝒙𝑖)<𝑓(𝒙𝑗). If there is no mixed zone between the two pre-
dicted regret ranges of 𝒙𝑖and𝒙𝑗(i.e., ˆ𝑓(𝒙𝑖)+𝑠(𝒙𝑖)<ˆ𝑓(𝒙𝑗)−𝑠(𝒙𝑗)),
a creditable predicted dominance relationship 𝑓(𝒙𝑖)<𝑓(𝒙𝑗)can
be established. Let 𝐷(𝒙𝑖,𝒙𝑗)=|ˆ𝑓(𝒙𝑖)−ˆ𝑓(𝒙𝑗)|be the distance of
the predicted regrets and 𝐸(𝒙𝑖,𝒙𝑗)=𝑠(𝒙𝑖)+𝑠(𝒙𝑗)be the sum of
the prediction errors of 𝒙𝑖and𝒙𝑗. Put differently, if 𝐷(𝒙𝑖,𝒙𝑗)>
𝐸(𝒙𝑖,𝒙𝑗), the predicted dominance relation is deemed credible. Con-
versely, the same prediction may be erroneous if 𝐷(𝒙𝑖,𝒙𝑗)is less
than𝐸(𝒙𝑖,𝒙𝑗), i.e., mixed-zone exits. The real relationship maybe
𝑓(𝒙𝑖)>𝑓(𝒙𝑗)when the real stability regrets are located in the
mixed zone.
After a generation of evolutionary optimization is finished, the
values of𝐷(𝒙𝑖,𝒙𝑗)and𝐸(𝒙𝑖,𝒙𝑗)for every solutions pairs are cal-
culated. Since the actual stability regret evaluations are unavailable,
the reliable ensemble surrogate evaluation from the last generation
is taken as𝑓(𝒙)for calculating 𝐷(𝒙𝑖,𝒙𝑗)and𝐸(𝒙𝑖,𝒙𝑗)of each base
model. Eventually, whether the dominance relationship between
ˆ𝑓(𝒙𝑖)and ˆ𝑓(𝒙𝑗)is credible can be judged as the following equation,
𝑅𝑖𝑗= 
1,if𝐷(𝒙𝑖,𝒙𝑗)>𝐸(𝒙𝑖,𝒙𝑗)and𝐷(𝒙𝑖,𝒙𝑗)>𝛿,
1,if𝐷(𝒙𝑖,𝒙𝑗)≤𝛿and𝐸(𝒙𝑖,𝒙𝑗)≤𝛿,
0,otherwise.(2)
𝑹is a binary reliability measurement matrix in the size of 𝑁𝑃×
𝑁𝑃,𝛿is a threshold, and 𝑅𝑖𝑗denotes whether the predictive dom-
inance relationship between 𝒙𝑖and𝒙𝑗is correct. The predicted
dominance relationship of ˆ𝑓(𝒙𝑖)and ˆ𝑓(𝒙𝑗)is credible if 𝑅𝑖𝑗=1,
and vice versa. In addition, it is hard to estimate the dominance rela-
tionship when 𝐷(𝒙𝑖,𝒙𝑗)approaches 0 (i.e., smaller than 𝛿), because
of the limitation of the collected HDC dataset. In this case, 𝑅𝑖𝑗=1
is established if 𝐷(𝒙𝑖,𝒙𝑗)≤𝛿and𝐸(𝒙𝑖,𝒙𝑗)≤𝛿, because the cor-
rectness of predicted dominance relationship between strikingly
similar solutions is less significant for the evolutionary optimizer.
5011LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment KDD ’24, August 25–29, 2024, Barcelona, Spain.
In each optimization step, a reliability score vector 𝒓=(𝑟1,𝑟2,..,𝑟𝐾)
is updated, where 𝑟𝑘is the reliability score of 𝑘-th base model.
𝑟𝑘=Í𝑁𝑃
𝑖=1Í𝑁𝑃
𝑗=1𝑅𝑘
𝑖𝑗, where 𝑹𝑘denotes the reliability measurement
matrix of𝑘-th base model. In the next generation, the reliability
score vector 𝒓is used to weigh the prediction to output ensemble
evaluation: ˆ𝑓𝑙(𝒙)=1Í𝐾
𝑘=1𝑟𝑘Í𝐾
𝑘=1𝑟𝑘ˆ𝑓𝑘
𝑙(𝒙),where ˆ𝑓𝑘
𝑙(𝒙)is the𝑘-th
base model for the 𝑙-th regret factor.
4 EXPERIMENT
To evaluate LASCA, we expect to answer three key questions: (Q-1)
How does LASCA perform on the comprehensive stability measure-
ment in real-world datasets? (Q-2) Can HDC collect high-quality
binning data in an acceptable time? (Q-3) To what extent can RDO
search for a more stable binning solution while reducing optimiza-
tion time costs?
4.1 Experimental Setup
4.1.1 Datasets. In this paper, we assess LASCA using three distinct
datasets—BEHAVE, PORT, and TRADE—stemming from user be-
havior, profiles, and transaction history, each representing different
business environments. Through this, we aim to showcase LASCA’s
broad applicability. BEHAVE contains users’ credit scores evalu-
ated based on their historical behaviors, including factors such as
the number of defaults and the number of payments in the past
periods. We evaluated 50,461,922 users per month and collected
their scores for 16 months, resulting in a dataset of 0.8 billion user
data. We expect to group all users into 8 risk levels. PORT com-
prises users’ credit scores evaluated based on their user portraits,
including features such as age, career, education, and preference.
We evaluated 45,798,232 users per month and collected their scores
for 13 months, resulting in a dataset of 0.6 billion user data. We
expect to group all users into 10 risk levels. TRADE contains users’
credit scores evaluated based on their trade history, including fac-
tors like successful rate and fraud rate. We evaluated 52,686,232
users per month and collected their credit scores for 13 months,
resulting in a dataset of nearly 0.7 billion user data. We expect to
group all users into 10 risk levels. Due to the inclusion of a sig-
nificant amount of personal privacy and commercially sensitive
information in the datasets, the contents of these three datasets
are not publicly disclosed in this paper. Note that these datasets
are only used for academic research, it does not represent any real
business situation.
4.1.2 Implementation. In the HDC and RDO phases, we use the
same hyper-parameters: a mutation rate of 1/𝑁, a maximum of 50
iterations, and a population size of 20. In RDO, the Light GBM [ 17]
is selected as the base model for its powerful learning ability, and
the number of base models 𝐾is set to 20. The probability 𝜖for
subsets random sampling is 0.7 and 𝛿in Eq. (2)is set to 10−5. The
code of LASCA is publicly available on GitHub.1
4.1.3 Metrics. In this paper, we utilize stability regrets as metrics
to evaluate the performance of various methods, with lower regrets
indicating superior performance. Stability is measured by a classic
monotonicity metric and two novel metrics: user distribution and
1Codes of LASCA, https://github.com/Gu-Youngfeng/LASCA_CODEretention rate. In practice, we’ve tracked business metrics like the
number of detected risk users and cost savings. We believe it is a
promising future work to enhance LASCA with newly designed
indicators to showcase its business value.
4.2 Experimental Results
Q-1. How does LASCA perform on the comprehensive stabil-
ity measurement in real-world datasets?
In this question, the overall performance of LASCA is investi-
gated through comparison experiments to determine if it can out-
perform state-of-the-art binning methods. Specifically, four widely-
used binning methods, Equal-size [ 37], MAPA [ 24], MonoBin [ 6],
and OptBinning [29], are selected.
Table 1 presents a detailed comparison of stability results among
various binning methods, highlighting LASCA’s superior perfor-
mance. LASCA’s results are the mean of eight repeats due to the
randomness, while the others are deterministic. LASCA outper-
forms other methods in overall stability regret (Stab.) and also
achieves relatively good and balanced results on three sub-regrets.
MonoBin obtains the lowest monotonicity regret (Mono.) in the
BEHAVE and TRADE datasets, but it falls short in overall stability
compared with LASCA. Besides, LASCA also performs well in both
the distribution regret (Distr.) and rate regret (Ret.).
As for Stab., LASCA shows average improvements over com-
parison methods: 62.9%, 84.0%, and 74.0% on BEHAVE, PORT, and
TRADE datasets, respectively. Equal-size method quickly generates
solutions but overlooks stability.
MonoBin and OptBinning methods optimize Mono., but they can-
not take into account Distr. and Rent. Therefore, they are inadequate
for real-world customer segmentation. Furthermore, inefficiency,
these methods struggle with large datasets (up to 0.8 billion records),
requiring dataset scale reduction through pre-binning, which may
lead to local optima due to ignoring stability constraints.
Answer to Q-1: On large-scale datasets with up to 0.8 billion,
LASCA achieves a substantial performance boost (73.6% average)
compared with other methods under the comprehensive stability
measurement (i.e., stability regret).
Q-2. Can HDC collect high-quality binning data in an accept-
able time?
In this question, the improvements of the HDC phase coming
from PDA are investigated. The PDA plays a pivotal role in the
HDC phase, which helps LASCA to collect higher-quality binning
datasets within a reasonable time.
To investigate the improvements by PDA, two comparable initial-
ization methods are designed: (1) PDA initialization, which gener-
ates good initial solutions for HDC; (2) Random initialization, which
starts with random solutions in HDC. Since HDC collects binning
data by iterations, we analyze the convergence of stability regret
through optimization steps for both PDA and random initialization.
We present the mean and standard deviation of the stability of the
best solutions in each step. As shown in Fig. 4 (A), (B), and (C), the
convergence curve of PDA (blue line) demonstrates a lower initial
regret score compared with random initialization (red line) across
all datasets. Notably, PDA’s stability regret diminishes more rapidly,
indicating its efficiency in discovering high-quality solutions. In
contrast, random initialization requires more iterations to achieve
5012KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
Table 1: Comparison of different binning methods on three real-world datasets.*
MethodsBEHAVE PORT TRADE
Mono. Distr. Ret. Stab. Mono. Distr. Ret. Stab. Mono. Distr. Ret. Stab.
LASCA 0.36 0.54 0.21 1.13 0.48 0.06 0.41 0.95 2.86 0.58 1.02 4.46
Equal-size 0.0 2.33 0.01 2.34 5.86 1.88 0.0 7.75 2.80 1.86 1.38 5.86
MAPA 2.76 0.43 1.10 4.30 2.18 0.95 0.56 3.70 3.90 386.03 15.94 405.88
MonoBin 0.0 2.57 0.13 2.71 11.83 2.26 0.84 14.94 2.77 65.16 4.33 72.27
OptBinning 1.93 1.15 0.43 3.51 1.07 3.64 0.11 4.83 3.03 13.84 4.92 21.79
*The stability regret (Stab.) is calculated as the sum of three sub-regrets: monotonicity regret (Mono.), user distribution
regret (Distr.), and retention rate regret (Ret.). Mono., Distr., Ret., and Stab. are all minimizing measurements. The
optimal result for each measurement is shown in bold.
Stability Regret
Stability Regret
Stability Regret
Stability RegretDataset(A) BEHA VE(B) PORT(C) TRADE(D) Dataset QualityStability Regret
Stability Regret
Stability Regret
Stability RegretOptimization StepDataset(A) BEHA VE(B) PORT(C) TRADE(D) Costly-Stability5.54.54.01.751.501.251.000.75
Time(103s)01020304050Optimization StepOptimization StepOptimization StepOptimization StepOptimization Step
Figure 4: The comparison results between PDA and random initialization. The lower the stability regret is, the higher the
quality of the collected data. (A), (B) and (C) show the convergence of the stability regret through optimization steps. (D) shows
average stability regret in collected binning data.
a similar outcome. Besides, Fig. 4 (D) presents the average stability
regret of the final collected dataset generated by PDA and Random
methods. A lower regret value indicates a higher-quality dataset.
PDA demonstrates an average improvement of 50% in quality across
the three datasets.
Answer to Q-2: HDC can collect higher-quality datasets in an
acceptable time using PDA techniques, resulting in a 50% quality
improvement.
Q-3. To what extent can RDO search for a more stable bin-
ning solution while reducing optimization time costs? This
question seeks to compare RDO with an oracle-driven optimization
procedure to inspect the time efficiency of RDO. Furthermore, we
also study the effect of reliable surrogate evaluation with different
surrogate models in finding stable solutions.
In our study, we compared two different optimization methods
following the HDC phase: (1) Reliable data-driven optimization
(RDO), which employs surrogate evaluations in the optimization
process. (2) Oracle-driven optimization (ODO), which uses actual
evaluations on the large-scale user dataset in the optimization pro-
cess. The HDC executes 50 optimization steps to collect the binning
dataset, which serves as the initial population for RDO and ODO
methods. After that, Both RDO and ODO execute 50 steps to search
for the most stable binning solution. The performance of compari-
son methods on three user datasets is shown in Fig. 5 (D). Among all
datasets, The RDO and ODO methods can obtain similar and com-
petitive stability regrets (left y-axis), indicating that the surrogate
model employed in the RDO approach is effective in identifying
stable binning solutions. On average, RDO can maintain 74% of thestability performance compared with the ODO method. Considering
that RDO does not utilize any actual evaluation results and instead
relies on a surrogate model for predictions, this is a notably high
level of performance. Besides, ODO takes considerably more time
(right y-axis) than RDO to obtain the final result. This is because
ODO needs to evaluate each solution on large-scale user datasets
while RDO can save time costs by surrogate predictions. On aver-
age, RDO speeds up the optimization process by 25 ×, representing
a significant acceleration beneficial for real-world applications.
To examine the impact of reliable surrogate evaluation, we con-
duct an ablation study with three surrogate models: (1) Single-
surrogate model, learning from all collected data. (2) Simple-ensemble
model, using equally weighted ensemble models. (3) Reliable-ensemble
model (our approach), using reliability score to weight ensemble
models. Fig. 5 (A), (B), and (C) display the stability regret of these
models. We record the ground-truth stability regret of the best
solution at each optimization step, conducting eight repeats per
model. Although all models succeed in finding improved solutions,
two ensemble models yield superior convergence outcomes and
exhibit a smaller deviation than the Single-surrogate model. More-
over, the Reliable-ensemble model can obtain more stable binning
solutions and perform more robustly (especially in the later-stage
optimization steps) than the Simple-ensemble model, showing that
the reliability-based ensembling can help RDO find better solu-
tions. On average, among the three datasets, RDO resulted in a
36%improvement in stability regret compared with the most stable
solutions collected in HDC.
5013LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment KDD ’24, August 25–29, 2024, Barcelona, Spain.
Stability Regret
Stability Regret
Stability Regret
Stability RegretIterationIterationIterationDataset(A) BEHA VE(B) PORT(C) TRADE(D) Dataset QualityStability Regret
Stability Regret
Stability Regret
Stability RegretOptimization StepDataset(A) BEHA VE(B) PORT(C) TRADE(D) Costly-Stability5.54.54.01.751.501.251.000.75
Time(103s)01020304050Optimization StepOptimization Step
Figure 5: (A), (B) and (C) show the comparison among different data-driven evaluation methods in terms of the stability regret in
each optimization step. (D) shows the comparison between the RDO and oracle-driven optimization in terms of the optimization
cost and stability regret.
Answer to Q-3: RDO successfully finds out a more stable bin-
ning solution that improves 36% stability to the collected high-
quality solutions while accelerating the optimization process by
25×times.
5 CONCLUSION
The paper focuses on addressing the challenges associated with
customer segmentation in real-world large-scale applications. To
measure the stability in the segmentation process, we introduce
and formalize the concept of stability regret, which consists of three
aspects: user distribution, monotonicity, and retention rate. To solve
the customer segmentation problem, we propose a large-scale sta-
ble segmentation approach LASCA, which consists of two primary
phases, high-quality dataset construction (HDC) and reliable data-
driven optimization (RDO). Currently, LASCA has also deployed
in various scenarios with favorable outcomes in Alipay, e.g., credit
assessment, marketing, and e-commerce fraud detection, providing
a stable and reliable segmentation solution in lending, payment, and
investment scenarios. In the future, we plan to extend LASCA’s ap-
plication to further scenarios, confirming its widespread generality
and scalability.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their construc-
tive and comprehensive comments. This work is supported by the
Ant Research Program, the National Natural Science Foundation
of China (No. 62106076), the CCF-AFSG Research Fund (No. CCF-
AFSG RF20220205), the Natural Science Foundation of Shanghai
(No. 21ZR1420300), and the Scientific and Technological Innovation
2030 Major Projects (No. 2018AAA0100902).
5014KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
REFERENCES
[1]Chidanand Apté, Eric Bibelnieks, Ramesh Natarajan, Edwin P. D. Pednault, Fateh
Tipu, Deb Campbell, and Bryan Nelson. 2001. Segmentation-based modeling for
advanced targeted marketing. In Proceedings of the 7th ACM SIGKDD international
conference on Knowledge discovery and data mining (KDD). ACM, San Francisco,
California, 408–413.
[2]Jason Catlett. 1991. On Changing Continuous Attributes into Ordered Discrete
Attributes. In Proceedings of the 9th European Working Session on Learning (EWSL).
Springer, Porto, Portugal, 164–178.
[3]Lei Chen, Xialiang Tong, Mingxuan Yuan, Jia Zeng, and Lei Chen. 2019. A Data-
Driven Approach for Multi-level Packing Problems in Manufacturing Industry.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD). ACM, Anchorage, Alaska, 1762–1770.
[4]Xiaojun Chen, Yixiang Fang, Min Yang, Feiping Nie, Zhou Zhao, and
Joshua Zhexue Huang. 2018. PurTreeClust: A Clustering Algorithm for Customer
Segmentation from Massive Customer Transaction Data. IEEE Transactions on
Knowledge and Data Engineering 30, 3 (2018), 559–572.
[5]Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan. 2002. A fast
and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation 6, 2 (2002), 182–197.
[6]Andrija Djurovic. 2022. Monotonic Binning for Credit Rating Models. https:
//github.com/andrija-djurovic/monobin.
[7]Yonghao Du, Tao Wang, Bin Xin, Ling Wang, Yingguo Chen, and Lining Xing.
2020. A Data-Driven Parallel Scheduling Approach for Multiple Agile Earth
Observation Satellites. IEEE Transactions on Evolutionary Computation 24, 4
(2020), 679–693.
[8]Martin Ester, Rong Ge, Wen Jin, and Zengjian Hu. 2004. A microeconomic data
mining problem: customer-oriented catalog segmentation. In Proceedings of the
10th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD). ACM, Seattle, Washington, 557–562.
[9]Usama M. Fayyad and Keki B. Irani. 1993. Multi-Interval Discretization of
Continuous-Valued Attributes for Classification Learning. In Proceedings of the
13th International Joint Conference on Artificial Intelligence (IJCAI). Morgan Kauf-
mann, Chambéry, France, 1022–1029.
[10] Salah Ghamizi, Renaud Rwemalika, Maxime Cordy, Lisa Veiber, Tegawendé F
Bissyandé, Mike Papadakis, Jacques Klein, and Yves Le Traon. 2020. Data-driven
simulation and optimization for Covid-19 exit strategies. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD). ACM, Virtual Event, California, 3434–3442.
[11] Vipul Gupta, Dhruv Choudhary, Ping Tak Peter Tang, Xiaohan Wei, Xing Wang,
Yuzhen Huang, Arun Kejariwal, Kannan Ramchandran, and Michael W. Mahoney.
2021. Training Recommender Systems at Scale: Communication-Efficient Model
and Data Parallelism. In Proceedings of the 27th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining (KDD). ACM, Virtual Event,
Singapore, 2928–2936. https://doi.org/10.1145/3447548.3467080
[12] Payam Hanafizadeh and Neda Rastkhiz Paydar. 2013. A Data Mining Model
for Risk Assessment and Customer Segmentation in the Insurance Industry.
International Journal of Strategic Decision Sciences 4, 1 (2013), 52–78.
[13] Hao Hao, Aimin Zhou, Hong Qian, and Hu Zhang. 2022. Expensive Multiob-
jective Optimization by Relation Learning and Prediction. IEEE Transactions on
Evolutionary Computation 26, 5 (2022), 1157–1170.
[14] Tianyi Jiang and Alexander Tuzhilin. 2009. Improving Personalization Solu-
tions through Optimal Segmentation of Customer Bases. IEEE Transactions on
Knowledge and Data Engineering 21, 3 (2009), 305–320.
[15] Ameet V. Joshi. 2023. Machine Learning and Artificial Intelligence - Second Edition.
Springer, New York, USA.
[16] Borhan Kazimipour, Xiaodong Li, and A. K. Qin. 2014. A Review of Population
Initialization Techniques for Evolutionary Algorithms. In Proceedings of the IEEE
Congress on Evolutionary Computation (CEC). IEEE, Beijing, China, 2585–2592.
[17] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting
Decision Tree. In Neural Information Processing Systems 30. Curran Associates
Inc., Red Hook, New York, 3149–3157.
[18] Randy Kerber. 1992. ChiMerge: Discretization of Numeric Attributes. In Proceed-
ings of the 10th AAAI Conference on Artificial Intelligence (AAAI). MIT Press, San
Jose, California, 123–128.
[19] Utku Koç and Türkan Sevgili. 2020. Consumer Loans’ First Payment Default
Detection: A Predictive Model. Turkish Journal of Electrical Engineering and
Computer Sciences 28, 1 (2020), 167–181.
[20] Aviral Kumar and Sergey Levine. 2020. Model Inversion Networks for Model-
Based Optimization. In Advances in Neural Information Processing Systems 33
(NeurIPS). Curran Associates, Inc., Virtual Event, 5126–5137.
[21] Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon. 2020. An
Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based
Policy Search. In Advances in Neural Information Processing Systems 33 (NeurIPS).
Curran Associates, Inc., Virtual Event, 10124–10135.
[22] Duanshun Li, Jing Liu, Jinsung Jeon, Seoyoung Hong, Thai Le, Dongwon Lee,
and Noseong Park. 2021. Large-Scale Data-Driven Airline Market InfluenceMaximization. In Proceedings of the 27th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD). ACM, Virtual Event, Singapore,
914–924.
[23] Adam Lipowski and Dorota Lipowska. 2012. Roulette-wheel selection via sto-
chastic acceptance. Physica A: Statistical Mechanics and its Applications 391, 6
(2012), 2193–2196.
[24] Huan Liu, Farhad Hussain, Chew Lim Tan, and Manoranjan Dash. 2002. Dis-
cretization: An Enabling Technique. Data Mining and Knowledge Discovery 6, 4
(2002), 393–423.
[25] Shuo Liu, Junhao Shen, Hong Qian, and Aimin Zhou. 2024. Inductive Cognitive
Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems.
InProceedings of the ACM on Web Conference 2024 (WWW). ACM, Singapore,
4260–4271.
[26] Zhening Liu, Handing Wang, and Yaochu Jin. 2023. Performance Indicator-Based
Adaptive Model Selection for Offline Data-Driven Multiobjective Evolutionary
Optimization. IEEE Transactions on Cybernetics 53, 10 (2023), 6263–6276.
[27] Huakang Lu, Hong Qian, Yupeng Wu, Ziqi Liu, Ya-Lin Zhang, Aimin Zhou, and
Yang Yu. 2023. Degradation-Resistant Offline Optimization via Accumulative Risk
Control. In Proceedings of the 26th European Conference on Artificial Intelligence
(ECAI), Vol. 372. IOS Press, Kraków, Poland, 1609–1616.
[28] Kenneth Moreland and Ron A. Oldfield. 2015. Formal Metrics for Large-Scale
Parallel Performance. In Proceedings of the 30th Information Security Conference
(ISC). Curran Associates Inc., Frankfurt, Germany, 488–496.
[29] Guillermo Navas-Palencia. 2022. Optimal Binning: Mathematical Programming
Formulation. arXiv:2001.08025 [cs.LG]
[30] Laura-Diana Radu. 2017. Green Cloud Computing: A Literature Survey. Symmetry
9, 12 (2017), 295.
[31] Shahryar Rahnamayan, Hamid R. Tizhoosh, and Magdy M. A. Salama. 2007. A
Novel Population Initialization Method for Accelerating Evolutionary Algorithms.
Computers & Mathematics with Applications 53 (2007), 1605–1614.
[32] Saikat Raj, Santanu Roy, Surajit Jana, Soumyadip Roy, Takaaki Goto, and Soumya
Sen. 2023. Customer Segmentation Using Credit Card Data Analysis. In Pro-
ceedings of the 21st International Conference on Software Engineering Research,
Management and Applications (SERA). IEEE/ACIS, Orlando, Florida, 383–388.
[33] Parameswaran Raman, Sriram Srinivasan, Shin Matsushima, Xinhua Zhang,
Hyokun Yun, and S. V. N. Vishwanathan. 2019. Scaling Multinomial Logistic
Regression via Hybrid Parallelism. In Proceedings of the 25th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining (KDD). ACM,
Anchorage, Alaska, 1460–1470. https://doi.org/10.1145/3292500.3330837
[34] Naeem Siddiqi. 2005. Credit Risk Scorecards: Developing and Implementing Intelli-
gent Credit Scoring. Wiley, Hoboken, New Jersey.
[35] Hwanjun Song, Jae-Gil Lee, and Wook-Shin Han. 2017. PAMAE: Parallel k-
Medoids Clustering with High Accuracy and Efficiency. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD). ACM, Halifax, NS, Canada, 1087–1096.
[36] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger.
2010. Gaussian Process Optimization in the Bandit Setting: No Regret and Exper-
imental Design. In Proceedings of the 27th International Conference on Machine
Learning (ICML). Omnipress, Haifa, Israel, 1015–1022.
[37] Lyn C. Thomas, Jonathan N. Crook, and David B. Edelman. 2017. Credit Scoring
and Its Applications, Second Edition . SIAM-Society for Industrial & Applied
Mathematics, Philadelphia, Pennsylvania.
[38] Alexander Ulanov, Andrey Simanovsky, and Manish Marwah. 2017. Modeling
Scalability of Distributed Machine Learning. In Proceedings of the 33rd IEEE
International Conference on Data Engineering, (ICDE). IEEE, San Diego, California,
1249–1254.
[39] Handing Wang, Yaochu Jin, Chaoli Sun, and John Doherty. 2018. Offline Data-
driven Evolutionary Optimization Using Selective Surrogate Ensembles. IEEE
Transactions on Evolutionary Computation 23, 2 (2018), 203–216.
[40] Jingen Xiang, Cong Guo, and Ashraf Aboulnaga. 2013. Scalable maximum clique
computation using MapReduce. In Proceedings of the 29th IEEE International
Conference on Data Engineering, (ICDE). IEEE, Brisbane, Australia, 74–85.
[41] Peng Yang, Laoming Zhang, Haifeng Liu, and Guiying Li. 2024. Reducing idle-
ness in financial cloud services via multi-objective evolutionary reinforcement
learning based load balancer. SCIENCE CHINA Information Sciences 67, 2 (2024),
1–21.
[42] Yinghui Yang and Balaji Padmanabhan. 2003. Segmenting Customer Trans-
actions Using a Pattern-Based Clustering Approach. In Proceedings of the 3rd
IEEE International Conference on Data Mining (ICDM). IEEE, Melbourne, Florida,
411–418.
[43] You Zhou, Xiujing Lin, Xiang Zhang, Maolin Wang, Gangwei Jiang, Huakang Lu,
Yupeng Wu, Kai Zhang, Zhe Yang, Kehang Wang, Yongduo Sui, Fengwei Jia, Zuoli
Tang, Yao Zhao, Hongxuan Zhang, Tiannuo Yang, Weibo Chen, Yunong Mao,
Yi Li, De Bao, Yu Li, Hongrui Liao, Ting Liu, Jingwen Liu, Jinchi Guo, Xiangyu
Zhao, Ying Wei, Hong Qian, Qi Liu, Xiang Wang, Wai Kin Chan, Chenliang Li,
Yusen Li, Shiyu Yang, Jining Yan, Chao Mou, Shuai Han, Wuxia Jin, Guannan
Zhang, and Xiaodong Zeng. 2023. On the Opportunities of Green Computing: A
Survey. CoRR abs/2311.00447 (2023).
5015LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment KDD ’24, August 25–29, 2024, Barcelona, Spain.
APPENDIX
The appendix is organized as follows: Section A gives the de-
tailed formulation of the customer segmentation problem. Section B
presents the implementation and evaluation of virtual-grouped par-
allel evaluation (VGPE).
A DETAILED CUSTOMER SEGMENTATION
PROBLEM FORMULATION
The customer segmentation problem aims to divide 𝑀pre-bins into
𝑁refined bins. We consider this problem as a black-box problem to
solve in this paper. Thus, the goal of the problem turns to finding
a stable solution 𝑥∈Xto minimize the stability regret 𝑓(𝒙). The
detailed business logic of 𝑓(𝒙)is given below,
arg min
𝒙∈X𝑓(𝒙)=𝑓1(𝒙)+𝑓2(𝒙)+𝑓3(𝒙) (1-a)
where𝑓1(𝒙)=𝑀∑︁
𝑗𝐾𝐿(𝐼𝑗,Q), (1-b)
𝑓2(𝒙)=𝑇∑︁
𝑗=1𝑁−1∑︁
𝑖=1max(0,𝜅𝑖−𝐹𝑃𝐷𝑖+1,𝑗
𝐹𝑃𝐷𝑖,𝑗), (1-c)
𝐹𝑃𝐷𝑖,𝑗=|𝐵𝑖,𝑗|
|𝐼𝑖,𝑗|, 𝑖∈[1,𝑁−1],𝑗∈[1,𝑇], (1-d)
𝑓3(𝒙)=𝑁∑︁
𝑖=1𝑇−1∑︁
𝑗=1max(0,𝜏𝑡
𝑖−𝜏𝑖,𝑗), (1-e)
𝜏𝑖,𝑗=|𝐼𝑖,𝑗∩𝐼𝑖,𝑗+1|
|𝐼𝑖,𝑗|, 𝑖∈[1,𝑁],𝑗∈[1,𝑇−1]. (1-f)
where𝑓(𝒙)consists of three parts: user distribution regret 𝑓1(𝒙),
monotonicity regret 𝑓2(𝒙), and retention rate regret 𝑓3(𝒙)(Eq.(1-a)).
A lower regret indicates a better satisfaction of the corresponding
stability factors.
Since the credit score of users may change over periods, the
composition of users within the same risk level may vary from
month to month. Given user credit scores over 𝑇months. Let 𝐼𝑖,𝑗
denote the user set of the 𝑖-th risk level in the 𝑗-th month. We have
1≤𝑖≤𝑁and1≤𝑗≤𝑇. Let𝐼𝑗={|𝐼1,𝑗|,|𝐼2,𝑗|,...,|𝐼𝑁,𝑗|}denote the
actual user distribution in the 𝑗-th month, where|𝐼𝑖,𝑗|calculates
the user number of the 𝑖-th risk level in the 𝑗-th month.
User distribution. The user distribution regret 𝑓1(𝒙)is defined in
Eq.(1-b).𝑓1(𝒙)uses Kullback-Leibler divergence [ 15] to measure
how much the user distribution 𝐼𝑗obeys the target distribution type
Q, i.e.,𝐾𝐿(𝐼𝑗,Q). Specifically, 𝐾𝐿(·)first finds the target distribu-
tion closest to the current user distribution 𝐼𝑗and then calculates
the KL divergence between these two distributions. The smaller the
distance is, the closer the user distribution 𝐼𝑗follows the target dis-
tribution typeQ.𝑓1(𝒙)falls within the range of [0,∞). If𝑓1(𝒙)=0,
it indicates that the binning solution satisfies the user distribution
factors. Conversely, any non-zero value indicates a partial deviation
from meeting the requirement.
Monotonicity. The monotonicity regret 𝑓2(𝒙)is defined in Eq. (1-c),
where𝜅𝑖is the expected lift ratio of FPD from the (𝑖+1)-th level
to the𝑖-th level.𝐹𝑃𝐷𝑖,𝑗denotes the calculated FPD value for the𝑖-th level in the 𝑗-th month. In this case, 𝐹𝑃𝐷𝑖,𝑗is computed by
dividing the number of defaulted users |𝐵𝑖,𝑗|by the total number
of users in that level |𝐼𝑖,𝑗|. Notably,𝑓2(𝒙)falls within the range of
[0,∞). If𝑓2(𝒙)=0, it indicates that the binning solution perfectly
satisfies the monotonicity factor. Otherwise, it represents a partial
satisfaction of the factor.
Retention rate. The retention rate regret 𝑓3(𝒙)is defined in Eq. (1-e).
𝑟𝑡
𝑖represents the expected retention rate in the 𝑖-th risk level. 𝑟𝜏𝑖,𝑗
denote the actual retention rate of the 𝑖-th risk level in the 𝑗-th
month. To keep the stability of binning solution, it is required that
the actual retention rate 𝜏𝑖,𝑗should be larger than the threshold
𝜏𝑡
𝑖.𝑓3(𝒙)also falls within the range of [0,∞). If𝑓3(𝒙)=0, it indi-
cates that the binning solution satisfies the retention rate factor.
Otherwise, it indicates a partial failure to meet the factor.
The above formulation omits the calculation process of 𝐼𝑖,𝑗,𝐼𝑗,
and𝐵𝑖,𝑗. However, to obtain these values, we need to query the user
dataset with a particular binning solution 𝒙. Varying the binning
solution 𝒙may yield different outcomes for 𝐼𝑖,𝑗,𝐼𝑗, and𝐵𝑖,𝑗, which
makes it hard be formulate explicitly. In this paper, we see the
function𝑓(𝒙)as a black box, and hence, customer segmentation is
considered as a black-box optimization problem.
B VIRTUAL-GROUPED PARALLEL
EVALUATION
In reality, the evaluation process accounts for the majority of the
running time of the HDC phase. Parallelization emerges as a prac-
tical approach to accelerate this process [ 7,11,21,33,35], enabling
scalability for large-scale scenarios and facilitating the widespread
implementation of LASCA. To speed up the evaluation process, the
virtual grouped parallel evaluation (VGPE) is designed.
The details of the VGPE strategy are given in Fig. 6. Suppose
that we have two solutions ( 𝑆𝑜𝑙.1and𝑆𝑜𝑙.2) to evaluate in one
generation. (A) adopts a serial evaluation (SE), where one worker
evaluates two solutions sequentially. The evaluation time, denoted
as𝑡1, is the longest among the three strategies. (B) employs a tradi-
tional parallel evaluation (TPE), where one worker evaluates one
solution, resulting in nearly half the time required compared to 𝑡1.
Sol.1Sol.2Sol.2Sol.1Sol.1-aSol.1-bSol.2-aSol.2-bWorker-1Worker-2Worker-3Worker-4t1t2t3Mutation timeEvaluation timeSelection timeIdel
（A）SE（B）TPE（C）VGPE
Figure 6: Different evaluation strategies. (A): Serial evalua-
tion (SE) utilizes one worker to sequentially evaluate two
solutions. (B): Traditional parallel evaluation (TPE) employs
two workers to simultaneously evaluate two solutions. (C):
Virtual-grouped parallel evaluation (VGPE) utilizes four
workers to evaluate two solutions, with each solution be-
ing evaluated by two workers.
5016KDD ’24, August 25–29, 2024, Barcelona, Spain. Yongfeng Gu et al.
(A) BEHAVE
 (B) PORT
 (C) TRADE
 (D) Time Speedup
Figure 7: The Comparison results between VGPE (with 𝑁𝑤=10) and TPE strategies. (A), (B) and (C) show the average evaluation
time in each iteration of different data scales. (D) shows the time speedup of VGPE in each iteration compared to TPE.
However, due to the immense volume of user data, evaluating a
single solution using only one worker becomes impractical. This is
because the resource demands, in terms of memory and comput-
ing power, far exceed the capacity of a single regular worker. (C)
implements the virtual-grouped parallel evaluation (VGPE), where
one solution is evaluated simultaneously by two workers, resulting
in nearly half the time required compared to 𝑡2. Specifically, VGPE
first divides 4 workers into 2 groups: 𝑊𝑜𝑟𝑘𝑒𝑟 -1 and𝑊𝑜𝑟𝑘𝑒𝑟 -2 in
the first group to evaluate 𝑆𝑜𝑙.1,𝑊𝑜𝑟𝑘𝑒𝑟 -3 and𝑊𝑜𝑟𝑘𝑒𝑟 -4 are in
the second group to evaluate 𝑆𝑜𝑙.2. During the evaluation of 𝑆𝑜𝑙.1
using VGPE, the entire dataset is divided into two parts. These
parts are then processed separately by 𝑊𝑜𝑟𝑘𝑒𝑟 -1 and𝑊𝑜𝑟𝑘𝑒𝑟 -2,
generating partial results ( 𝑆𝑜𝑙.1-a and𝑆𝑜𝑙.1-b). Subsequently, the
results from the two workers are combined to construct the stability
regret of𝑆𝑜𝑙.1. Likewise, 𝑆𝑜𝑙.2is evaluated in parallel by 𝑊𝑜𝑟𝑘𝑒𝑟 -3
and𝑊𝑜𝑟𝑘𝑒𝑟 -4.
In VGPE, we define the number of workers that evaluate one
solution as the Parallel Degree, denoted as 𝑁𝑤. In Fig. 6 (C), we
have𝑁𝑤=2. Ideally, VGPE can achieve a 𝑁𝑤×speedup compared
to TPE. However, it is impossible to indefinitely increase the num-
ber of workers ( 𝑁𝑤) because although a larger 𝑁𝑤can enhance
evaluation efficiency, it also introduces significant resource and
computational overhead. The parallel nature of VGPE also helps
distribute the computational workload across multiple workers,
thereby mitigating memory and computer power limitations and
improving the overall performance of LASCA.
In practical implementation, to enhance the robustness of paral-
lelism, we have incorporated an elastic fault-tolerant scheduling
scheme. This scheme involves initially determining the actual num-
ber of available workers and subsequently virtually dividing them
for each solution. By implementing this approach, the evaluation
process can proceed smoothly even if certain workers fail to be
activated. In summary, VGPE’s ability to leverage multiple workers
enhances the efficiency and scalability of LASCA, enabling it to
navigate and analyze large-scale user datasets effectively.
To investigate the improvements by VGPE, two parallel strategies
are designed: (1) VGPE, which performs further parallel evaluation
in HDC; (2) TPE, which performs traditional parallel evaluation inHDC. However, due to memory limitations, a regular single worker
was unable to store the entire user data (up to 0.8 billion). To im-
plement TPE, we utilized 20 customized workers, each equipped
with 48GB of memory. Each worker was assigned to evaluate one
solution in each iteration. For VGPE implementation, we employed
200 workers, each with 4GB of memory. In this case, 10 workers
were assigned to evaluate one solution in each iteration. In reality,
accessing computers with large memory capacities, such as 48GB,
can often be challenging. Utilizing workers with smaller memory
capacities is a more environmentally friendly approach, causing
lower power and higher resource utilization, known as green com-
puting [ 30]. Selecting an appropriate parallel degree ( 𝑁𝑤) is impor-
tant for VGPE. It is crucial to strike a balance, avoiding excessive
resource costs while minimizing evaluation time. To determine
the optimal 𝑁𝑤for our approach, we calculate the speedup effi-
ciency [ 28] of different values. Based on our calculations, we have
determined that 𝑁𝑤=10is the most suitable parallel degree for
our approach.
Fig. 7 (A), (B), and (C) show the comparison of evaluation time
in each generation using the VGPE and TPE strategies. We sampled
different scales (20%, 40%, 60%, 80%, 100%) of the three datasets to
examine the time consumption in each iteration using TPE and
VGPE strategies. VGPE exhibits a substantial reduction in time
consumption across all datasets when compared to the traditional
parallel strategy. As the data scales increase, the evaluation time
of TPE grows exponentially. In all datasets (100%), the evaluation
time required per iteration is approximately 1000 seconds. This
implies that the total running time of HDC is approximately 13
hours, which is an unacceptable duration for the company. In con-
trast, VGPE demonstrates scalability and efficiency in large-scale
datasets, with its evaluation time growing nearly linearly. Addition-
ally, Fig. 7 (D) shows the time speedup of VGPE compared to TPE.
As the data scales increase, the speedup becomes evident. Although
VGPE may not achieve a 10x acceleration based on TP due to com-
munication overhead and potential resource contention among
workers, it still manages to achieve an average speedup of 8.7 ×
in entire datasets (100% percent). This highlights the significant
efficiency improvement provided by VGPE in handling large-scale
data binning operations.
5017