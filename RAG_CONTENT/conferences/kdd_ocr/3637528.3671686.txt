Pre-train and Refine: Towards Higher Efficiency in K-Agnostic
Community Detection without Quality Degradation
Meng Qin
mengqin_az@foxmail.com
Department of CSE, HKUST
Hong Kong SARChaorui Zhang
chaorui.zhang@gmail.com
Theory Lab, Huawei
Hong Kong SARYu Gao
gaoyu99@huawei.com
Theory Lab, Huawei
Beijing, China
Weixi Zhang
zhangweixi1@huawei.com
Theory Lab, Huawei
Hong Kong SARDit-Yan Yeung
dyyeung@cse.ust.hk
Department of CSE, HKUST
Hong Kong SAR
ABSTRACT
Community detection (CD) is a classic graph inference task that
partitions nodes of a graph into densely connected groups. While
many CD methods have been proposed with either impressive qual-
ity or efficiency, balancing the two aspects remains a challenge.
This study explores the potential of deep graph learning to achieve
a better trade-off between the quality and efficiency of 𝐾-agnostic
CD, where the number of communities 𝐾is unknown. We propose
PRoCD ( Pre-training & Refinement f orCommunity Detection), a
simple yet effective method that reformulates 𝐾-agnostic CD as
the binary node pair classification. PRoCD follows a pre-training &
refinement paradigm inspired by recent advances in pre-training
techniques. We first conduct the offline pre-training of PRoCD on
small synthetic graphs covering various topology properties. Based
on the inductive inference across graphs, we then generalize the
pre-trained model (with frozen parameters) to large real graphs
and use the derived CD results as the initialization of an existing
efficient CD method (e.g., InfoMap) to further refine the quality of
CD results. In addition to benefiting from the transfer ability regard-
ing quality, the online generalization andrefinement can also help
achieve high inference efficiency, since there is no time-consuming
model optimization. Experiments on public datasets with various
scales demonstrate that PRoCD can ensure higher efficiency in
𝐾-agnostic CD without significant quality degradation.
CCS CONCEPTS
•Mathematics of computing →Graph algorithms; •Theory
of computation→Inductive inference .
KEYWORDS
Community Detection, Graph Clustering, Inductive Graph Infer-
ence, Pre-training & Refinement
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671686ACM Reference Format:
Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, and Dit-Yan Yeung. 2024.
Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community
Detection without Quality Degradation. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671686
1 INTRODUCTION
Community detection (CD) is a classic graph inference task that par-
titions nodes of a graph into several groups (i.e., communities) with
dense linkage distinct from other groups [ 12]. It has been validated
that the extracted communities may correspond to substructures
of various real-world complex systems (e.g., functional groups in
protein interactions [ 11]). Many network applications (e.g., cellular
network decomposition [ 9] and Internet traffic profiling [ 30]) are
thus formulated as CD. Due to the NP-hardness of some typical
CD objectives (e.g., modularity maximization [ 24]), balancing the
quality and efficiency of CD on large graphs remains a challenge.
Most existing approaches focus on either high quality or efficiency.
On the one hand, efficient CD methods usually adopt heuristic
strategies or fast approximation w.r.t. some relaxed CD objectives
to obtain feasible CD results (e.g., the greedy maximization [ 3] and
semi-definite relaxation [ 48] of modularity). Following the graph
embedding framework, which maps nodes V={𝑣𝑖}of a graph into
low-dimensional vector representations {z𝑖∈R𝑑}(𝑑≪|V| ) with
major topology properties preserved, several efficient embedding
approaches have also been proposed based on approximated di-
mension reduction (e.g., random projection for high-order topology
[55]). Some of them are claimed to be community-preserving and
able to support CD using a downstream clustering module [ 2,13].
Despite the high efficiency, these methods may potentially suffer
from quality degradation due to the information loss of heuristics,
approximation, and relaxation.
On the other hand, recent studies have demonstrated the ability
of deep graph learning (DGL) techniques [ 38,56], e.g., those based
on graph neural networks (GNNs), to achieve impressive quality
of various graph inference tasks including CD [ 52]. However, their
powerful performance usually relies on iterative optimization algo-
rithms (e.g., gradient descent) that direct sophisticated models to
fit complicated objectives, which usually have high complexities.
 
2467
KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
Input Graph Input GraphEmbedding 
Encoder Enc(.)Output Module Out(.)
(e.g., MLP) Related to KDownstream Clustering 
(e.g., KMeans)K
Emb. {zi}
Embedding 
Encoder Enc(.)
CD Result CD Result
Emb. {zi}Unsupervised 
Opt.
(a) Task -Dependent  
DGL  Methods(b) Task -Independent  
DGL  MethodsUnsupervised 
Opt.Embedding 
Encoder
(c) Our PRoCD  MethodBinary Node 
Pair Classifier
Small Synthetic 
Graphs w/ Gnd.
(1) Offline  Pre-
training(2) Online  
Generalization(3) Online  
Refinement
{zi}
Embedding 
EncoderBinary Node 
Pair Classifier
{zi}
Large Real GraphOffline  Pre-training 
w/ Gnd.  Efficient CD Method 
(e.g.,  InfoMap )Refined  CD Result
w/ Updatable Parameters
w/ Frozen Parameters
w/ Updatable Parameters
w/ Frozen Parameters
Initial  CD 
Result
Figure 1: Overview of mainstream (a) task-dependent and (b)
-independent DGL methods as well as (c) our PRoCD method.
In this study, we explore the potential of DGL to ensure higher
efficiency in CD without significant quality degradation, compared
with mainstream efficient and DGL methods. It serves as a pos-
sible way to achieve a better trade-off between the two aspects.
Different from existing CD methods [ 6,10,46] with an assump-
tion that the number of communities 𝐾is given, we consider the
more challenging yet realistic 𝐾-agnostic CD, where 𝐾is unknown.
In this setting, one should simultaneously determine 𝐾and the
corresponding community partition.
Dilemmas. As shown in Fig. 1 (a) and (b), existing DGL-based
CD techniques usually follow the unified graph embedding frame-
work and can be categorized into the (i) task-dependent and (ii)
task-independent approaches. We argue that they may suffer from
the following limitations w.r.t. our settings.
First, existing DGL methods may be inapplicable to 𝐾-agnostic
CD. Given a graph G, some task-dependent approaches [ 6,46,49]
(see Fig. 1 (a)) generate embeddings {z𝑖}via an embedding encoder
Enc(·)(e.g., a multi-layer GNN) and feed {z𝑖}into an output mod-
uleOut(·)(e.g., a multi-layer perceptron (MLP)) to derive the CD
resultC, i.e.,{z𝑖}=Enc(G)andC=Out({z𝑖}). Since parameters
inOut(·)are usually with the dimensionality related to 𝐾, these
task-dependent methods cannot output a feasible CD result when 𝐾
is unknown. Although some task-independent methods [ 20,33,54]
(see Fig. 1 (b)) do not contain Out(·)related to𝐾, their original de-
signs still rely on a downstream clustering algorithm (e.g., 𝐾Means)
with{z𝑖}and𝐾as required inputs.
Second, the standard transductive inference of some DGL tech-
niques may result in low efficiency. As illustrated in Fig. 1 (a) and (b),
during the inference of CD on each single graph, these transductive
methods must first optimize their model parameters from scratch
via unsupervised objectives, which is usually time-consuming. Some
related studies [ 20,46,54] only consider transductive inference re-
gardless of its low efficiency.
Focusing on the CD with a given 𝐾andtask-independent archi-
tecture in Fig. 1 (b), recent research [ 33] has validated that the in-
ductive inference, which (i) trains a DGL model on historical graphs
{G𝑡}and (ii) generalizes it to new graphs {G′}without additional
optimization, can help achieve high inference efficiency on {G′}.One can extend this method to 𝐾-agnostic CD by replacing the
downstream clustering module (e.g., 𝐾Means) with an advanced
algorithm unrelated to 𝐾(e.g., DBSCAN [ 41]). Our experiments
indicate that such a naive extension may still have low inference
quality and efficiency compared with conventional baselines.
Contributions. We propose PRoCD ( Pre-training & Refinement
forCommunity Detection), a simple yet effective method as illus-
trated in Fig. 1 (c), to address the aforementioned limitations.
A novel model architecture. To derive feasible results for 𝐾-
agnostic CD in an end-to-end way, we reformulate CD as the binary
node pair classification. Unlike existing end-to-end models that di-
rectly assign the community label 𝑐𝑖∈{1,···,𝐾}of each node
𝑣𝑖(e.g., via an MLP in Fig. 1 (a)), we develop a binary classifier,
with a new design of pair-wise temperature parameters, to deter-
mine whether a pair of nodes (𝑣𝑖,𝑣𝑗)are partitioned into the same
community. Given the classification result w.r.t. a set of node pairs
P={(𝑣𝑖,𝑣𝑗)}sampled from a graph G, we construct another auxil-
iary graph ˜G, from which a feasible CD result of Gcan be extracted.
Each connected component in ˜Gcorresponds to a community in
G, with the number of components as the estimated 𝐾.
A novel learning paradigm. Inspired by recent advances in
graph pre-training techniques, PRoCD follows a novel pre-training
& refinement paradigm with three phases as in Fig. 1 (c). Based on
the assumption that one has enough time to prepare a well-trained
model in an offline way (i.e., offline pre-training ), we first pre-train
PRoCD on small synthetic graphs with various topology properties
(e.g., degree distributions) and high-quality community ground-
truth. We then generalize PRoCD (with frozen parameters) to large
real graphs{G′}(i.e., online generalization ) and derive correspond-
ing CD results{¯C′}via only one feed-forward propagation (FFP)
of the model (i.e., inductive inference of DGL). In some pre-training
techniques [ 45,50],online generalization only provides an initializa-
tion of model parameters, which is further fine-tuned w.r.t. different
tasks. Analogous to this motivation, we treat ¯C′as the initialization
of an efficient CD method (e.g., InfoMap [40]) and adopt its output
C′as the final CD result, which refines ¯C′(i.e., online refinement ).
In particular, the online generalization andrefinement may benefit
from the powerful transfer ability of inductive inference regarding
quality while ensuring high inference efficiency, since there is no
time-consuming model optimization.
Note that our pre-training & refinement paradigm is different
from existing graph pre-training techniques [ 45,50]. We argue
that they may suffer from the following issues about CD. Besides
pre-training, these methods may require another optimization pro-
cedure for the fine-tuning or prompt-tuning of specific tasks on
{G′}, which is time-consuming and thus cannot help achieve high
efficiency on{G′}. To the best of our knowledge, related studies
[4,21,44] merely focus on supervised tasks (e.g., node classification)
and do not provide unsupervised tuning objectives for CD.
A better trade-off between quality and efficiency. Experi-
ments on datasets with various scales demonstrate that PRoCD
can ensure higher inference efficiency in CD without significant
quality degradation, compared with running a refinement method
from scratch. In some cases, PRoCD can even achieve improvement
in both aspects. Therefore, we believe that this study provides a
possible way to obtain a better trade-off between the two aspects.
 
2468Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Summary of some representative CD methods.
Methods FastCom GraClus MC-SBM Par-SBM GMod LPA InfoMap Louvain Locale ClusNet LGNN GAP DMoN DNR DCRN ICD
References [8] [10] [26] [27] [8] [36] [40] [3] [48] [49] [6] [23] [46] [54] [20] [33]
Focus High Efficiency High Quality Q&E
Techniques Heuristic strategies or fast approximation w.r.t. relaxed CD objectives Task-dependent DGL Task-independent DGL
Inference Running the algorithm from scratch for each graph Transductive &Inductive Transductive Inductive
𝐾-agnostic
2 RELATED WORK
In the past few decades, many CD methods have been proposed
based on different problem statements, hypotheses, and techniques
[52]. Table 1 summarizes some representative approaches according
to their original designs. Most of them focus on either high effi-
ciency or quality. Conventional efficient CD methods use heuristic
strategies or fast approximation w.r.t. some relaxed CD objectives
to derive feasible CD results, including the (i) greedy modularity
maximization in FastCom [8] and Louvain [3], (ii) semi-definite re-
laxation of modularity in Locale [48], (iii) label propagation heuristic
inLPA[36], as well as (iv) Monte Carlo approximation of stochastic
block model (SBM) in MC-SBM [26] and Par-SBM [27].
Recent studies have also demonstrated the powerful potential of
DGL to ensure high quality of CD, following the architectures in
Fig. 1 (a) and (b). However, some methods (e.g., DNR [54],DCRN
[20], and DMoN [46]) only considered the inefficient transductive
inference, with time-consuming model optimization in the infer-
ence of CD on each single graph. Other approaches (e.g., ClusNet
[49],LGNN [6],GAP [23], and ICD[33]) considered inductive in-
ference across graphs. Results of ICDvalidated that the inductive
inference can help achieve a better trade-off between quality and
efficiency in online generalization (i.e., directly generalizing a model
pre-trained on historical graphs {G𝑡}to new graphs{G′}). Nev-
ertheless, the quality of online generalization may be affected by
the possible distribution shift between {G𝑡}and{G′}. In contrast,
online generalization is used to construct the initialization of online
refinement in our PRoCD method, which can ensure better quality
on{G′}. Moreover, most task-dependent DGL methods (e.g., Clus-
Net, LGNN, GAP, and DMoN ) are inapplicable to 𝐾-agnostic CD,
since they usually contain an output module related to 𝐾. Although
task-independent approaches (e.g., DNR, DCRN, and ICD) do not
contain such a module, their original designs still rely on a pre-set
𝐾(e.g.,𝐾Means as the downstream clustering algorithm).
Different from the aforementioned methods, our PRoCD method
can ensure higher efficiency in 𝐾-agnostic CD without significant
quality degradation via a novel model architecture following the
pre-training & refinement paradigm, as highlighted in Fig. 1 (c).
As reviewed in [ 45,50], existing graph pre-training techniques
usually follow the paradigms of (i) pre-training & fine-tuning (e.g.,
GCC [35],L2P-GNN [22], and W2P-GNN [4]) as well as (ii) pre-
training & prompting (e.g., GPPT [43],GraphPrompt [21], and ProG
[44]). In addition to offline pre-training, these methods rely on an-
other optimization procedure for the fine-tuning or prompt-tuning
of specific inference tasks, which is usually time-consuming. More-
over, most of them merely focus on supervised tasks (e.g., node
classification, link prediction, and graph classification) and do not
provide unsupervised tuning objectives for CD. Therefore, one can-
not directly apply these pre-training methods to ensure the high
efficiency in CD without quality degradation.3PROBLEM STATEMENTS & PRELIMINARIES
In this study, we consider the disjoint CD on undirected graphs. A
graph can be represented as G=(V,E), withV={𝑣1,···,𝑣𝑁}
andE={(𝑣𝑖,𝑣𝑗)|𝑣𝑖,𝑣𝑗∈ V)} as the sets of nodes and edges.
The topology ofGcan be described by an adjacency matrix A∈
{0,1}𝑁×𝑁, where A𝑖𝑗=A𝑗𝑖=1if(𝑣𝑖,𝑣𝑗)∈E andA𝑖𝑗=A𝑗𝑖=0
otherwise. Since CD is originally defined only based on graph
topology [12], we assume that graph attributes are unavailable.
Community Detection (CD). Given a graph G, CD aims to
partition the node set Vinto𝐾subsets (defined as communities)
C={C1,···,C𝐾}(𝐶𝑟∩𝐶𝑠=∅for∀𝑟≠𝑠) s.t. (i) the linkage
within each community is dense but (ii) that between communities
is relatively loose. We define that a CD task is 𝐾-agnostic if the
number of communities 𝐾is unknown for a given graph G, where
one should simultaneously determine 𝐾and the corresponding CD
resultC. We consider 𝐾-agnostic CD in this study. Whereas, some
existing methods [ 10,46,49] assume that 𝐾is given for each input
Gand thus cannot tackle such a challenging yet realistic setting.
Modularity Maximization. Mathematically, CD can be formu-
lated as the modularity maximization objective [ 24], which maxi-
mizes the difference between the exact graph topology (described by
A) and a randomly generated case. Given Gand𝐾, it aims to derive
a partitionCthat maximizes the following modularity metric:
max
CMod(G,𝐾):=1
2𝑀𝐾∑︁
𝑟=1∑︁
𝑣𝑖,𝑣𝑗∈C𝑟[A𝑖𝑗−deg(𝑣𝑖)deg(𝑣𝑗)
2𝑀],(1)
where deg(𝑣𝑖)is the degree of node 𝑣𝑖;𝑀is the number of edges.
Objective (1) can be rewritten into the following matrix form:
min
H−tr(H𝑇QH) s.t.H𝑖𝑟=1, 𝑣𝑖∈𝐶𝑟
0,otherwise, (2)
where Q∈R𝑁×𝑁is defined as the modularity matrix with Q𝑖𝑗:=
[A𝑖𝑗−deg(𝑣𝑖)deg(𝑣𝑗)/(2𝑀)];H∈{0,1}𝑁×𝐾indicates the com-
munity membership of C. Our method does not directly solve the
aforementioned problem. Instead, we try to extract informative
community-preserving features from Q.
Pre-training &Refinement Paradigm. To achieve a better
trade-off between the quality and efficiency, we propose a pre-
training & refinement paradigm based on the inductive inference of
DGL. We represent a DGL model as C=𝑓(G;Θ), which derives the
CD resultCgiven a graphG, withΘas the set of trainable model pa-
rameters. As in Fig. 1 (c), the proposed paradigm includes (i) offline
pre-training, (ii) online generalization, and (iii) online refinement.
Inoffline pre-training, we first generate a set of synthetic graphs
T={G1,···,G𝑇}via a generator (e.g., SBM [ 15]). The generation
of each graphG𝑡∈T includes the topology (V𝑡,E𝑡)and commu-
nity assignment ground-truth C(𝑡). We then pre-train 𝑓onT(i.e.,
updating Θ) based on an objective regarding {(V𝑡,E𝑡)}and{C(𝑡)}
 
2469KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
GNN LayerGNN Layer
……  Linearv1 v2
v7 v3
v4
v5 v6v1 v2
v7 v3
v4
v5 v6v1 v2
v7 v3
v4
v5 v60.67
0.67
0.670.670.78 0.33
0.560.56
0.56v1 v2
v7 v3
v4
v5 v60.67
0.67
0.670.670.78 0.33
0.560.56
0.56
v1 v2
v7 v3
v4
v5 v60.67
0.67
0.670.78 0.33
0.56
-0.22-0.33
-0.22-0.44v1 v2
v7 v3
v4
v5 v60.67
0.67
0.670.78 0.33
0.56
-0.22-0.33
-0.22-0.44Gaussian 
Rand. Proj.MLP
Reweighted 
Topo.  w.r.t. QInput 
Graph
Q
(a) Feature Extraction Module (§4.1.1)Comm
.-Psv. 
Feat.  X
Reduced Topo. 
w.r.t.
(b) Embedding 
Encoder (§ 4.1.2)
τij
Perceptron
Perceptron…Perceptron
Perceptron…Perceptron
Perceptron…Perceptron
Perceptron…zi
zj
(c) Binary Node Pair Classifier 
(§ 4.1.3)hs(.)
hd(.)Node 
Emb.
{zi}v1 v2
v7 v3
v4
v5 v6
Adaptive 
Temp. Param.
v1 v2
v7v3
v4
v5 v6Community  
#1
Community  
#2
12ˆS
17ˆS
23ˆS
34ˆS
45ˆS
67ˆS
27ˆS
37ˆS
36ˆS
35ˆS
56ˆSSelect
2exp( | | )i j ij −−zz
ˆ
ijS
(d) Result Derivation Module  (§ 4.1.4)Sampled 
Node 
Pairs
A Feasible CD Result  
Extract via BFS/DFS
ˆ 0.5?ijSAuxiliary Graph
( , )=
{( , )}ijvv=
Figure 2: Overview of the model architecture of PRoCD.
in an offline way. After that, we generalize 𝑓to a large real graph
G′with frozen Θ(i.e., online generalization ), which can derive a
feasible CD result ¯C′via only one FFP of 𝑓(i.e., inductive inference).
Analogous to the fine-tuning in existing pre-training techniques, we
treat ¯C′as the initialization of a conventional efficient CD method
(e.g., InfoMap [40]) and adopt its output C′as the final CD result,
which further refines ¯C′(i.e., online refinement ).
Evaluation Protocol. In real applications, it is usually assumed
that one has enough time to prepare a well-trained model in an of-
fline way (e.g., pre-training of LLMs). After deploying 𝑓, one may
achieve high efficiency in the online inference on graphs {G′}(e.g.,
via one FFP of 𝑓without any model optimization). In contrast, con-
ventional methods cannot benefit from pre-training but have to run
their algorithms on {G′}from scratch, due to the inapplicability to
inductive inference. Our evaluation focuses on the online inference
of CD on{G′}(e.g., online generalization andrefinement of PRoCD),
which is analogous to the applications of foundation models. For
instance, users benefit from the powerful online inference of LLMs
(e.g., generating high-quality answers in few seconds) but do not
need to train them using a great amount of resources.
4 METHODOLOGY
We propose PRoCD following a novel pre-training & refinement
paradigm as highlighted in Fig. 1 (c).
4.1 Model Architecture
To enable PRoCD to derive feasible results for 𝐾-agnostic CD in
an end-to-end architecture, we reformulate CD as the binary node
pair classification. For each graph G=(V,E), we introduce an
auxiliary variable S∈{0,1}𝑁×𝑁(𝑁:=|V|), where S𝑖𝑗=S𝑗𝑖=1
if nodes𝑣𝑖and𝑣𝑗are partitioned into the same community and
S𝑖𝑗=S𝑗𝑖=0otherwise. Given a set of node pairs P={(𝑣𝑖,𝑣𝑗)}, we
also rearrange corresponding elements in Sas a vector y∈{0,1}|P|,
where y𝑙=S𝑖𝑗=S𝑗𝑖w.r.t. each node pair 𝑝𝑙=(𝑣𝑖,𝑣𝑗)∈P . Fig. 2
provides an overview of the model architecture with a running
example. Given a graph G, the model samples a set of node pairs Pand derives the estimated value of y𝑙=S𝑖𝑗w.r.t. each𝑝𝑙=(𝑣𝑖,𝑣𝑗)∈
P, from which a feasible CD result Ccan be extracted.
4.1.1 Feature Extraction Module. As shown in Fig. 2 (a), we
first extract community-preserving features, arranged as X∈R𝑁×𝑑
(𝑑≪𝑁), for an input graph Gfrom the modularity maximization
objective (2). The modularity matrix Qin (2) is a primary component
regarding graph topology. It can be considered as a reweighting
of original topology, where nodes (𝑣𝑖,𝑣𝑗)with similar neighbor-
induced features(Q𝑖,:,Q𝑗,:)are more likely to belong to a common
community. To minimize the objective in (2), which is equivalent
to maximizing the modularity metric in (1), Q𝑖𝑗with a large value
indicates that(𝑣𝑖,𝑣𝑗)are more likely to be partitioned into the same
community (e.g., Q17=Q71=0.78in Fig. 2). Therefore, we believe
thatQencodes key characteristics regarding community structures.
Note that Q∈R𝑁×𝑁is usually dense. To utilize the sparsity
of topology, we reduce Qto a sparse matrix ˜Q∈R𝑁×𝑁, where
˜Q𝑖𝑗=Q𝑖𝑗if(𝑣𝑖,𝑣𝑗) ∈E and ˜Q𝑖𝑗=0otherwise. Although the
reduction may lose some information, it enables the model to be
scaled up to large sparse graphs without constructing an 𝑁×𝑁
dense matrix. Our experiments demonstrate that PRoCD can still
derive high-quality CD results using ˜Q. Given ˜Q, we derive Xvia
X=MLP(˜QΩ),with Ω∈R𝑁×𝑑∼N( 0,1/𝑑). (3)
Concretely, the Gaussian random projection [ 1], an efficient dimen-
sion reduction technique that can preserve the relative distances
between input features with rigorous theoretical guarantees, is first
applied to ˜Q. We then incorporate non-linearity into the reduced
features using an MLP.
4.1.2 Embedding Encoder. Given the extracted features X, we
then derive low-dimensional node embeddings {z𝑖∈R𝑑}using a
multi-layer GNN with skip connections, as shown in Fig. 2 (b). Here,
we adopt GCN [ 16] as an example building block. One can easily
extend the model to include other advanced GNNs. Let Z[𝑠−1]and
Z[𝑠]be the input and output of the 𝑠-th GNN layer, with Z[0]=X.
The multi-layer GNN can be described as
Z=LN(Linear(˜Z)),˜Z=Í
𝑠Z[𝑠],
Z[𝑠]=LN(tanh(ˆD−0.5ˆAˆD−0.5Z[𝑠−1]W[𝑠])),(4)
where ˆA:=A+I𝑁is the adjacency matrix containing self-edges; ˆD
is the degree diagonal matrix w.r.t. ˆA;W[𝑠]∈R𝑑×𝑑is a trainable
weight matrix; LN(U)denotes the row-wise 𝑙2normalization on a
matrix U(i.e., U𝑖,:←U𝑖,:/|U𝑖,:|2);Z∈R𝑁×𝑑is the matrix form of
the derived embeddings, with Z𝑖,:=z𝑖∈R𝑑as the embedding of
node𝑣𝑖. In each GNN layer, Z[𝑠]
𝑖,:is an intermediate representation of
node𝑣𝑖, which is the non-linear aggregations (i.e., weighted mean)
of features w.r.t.{𝑣𝑖}∪Nei(𝑣𝑖), with Nei(𝑣𝑖)as the set of neigh-
bors of𝑣𝑖. Since this aggregation operation forces nodes (𝑣𝑖,𝑣𝑗)
with similar neighbors (Nei(𝑣𝑖),Nei(𝑣𝑗))(i.e., dense local topology)
to have similar representations (Z[𝑠]
𝑖,:,Z[𝑠]
𝑗,:), the multi-layer GNN
can enhance the ability of PRoCD to derive community-preserving
embeddings. To obtain Z, we first sum up the intermediate repre-
sentations{Z[𝑠]}of all the layers and apply a linear mapping to the
summed representation ˜Z. Furthermore, the row-wise 𝑙2normal-
ization is applied. It forces {z𝑖}to be distributed in a unit sphere,
with|z𝑖−z𝑗|2=2−2z𝑖z𝑇
𝑗.
 
2470Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1: Deriving a Feasible CD Result
Input: input graphG=(V,E); number of sampled node pairs 𝑛𝑆
Output: a feasible CD result Cw.r.t.G
1P←E //Initialize the set of node pairs Pusing edge setE
2forsample_count from 1to𝑛𝑆do
3 randomly sample a node pair 𝑝=(𝑣𝑖,𝑣𝑗)
4 add the sampled 𝑝=(𝑣𝑖,𝑣𝑗)toP
5derive ˆyw.r.t.Pvia one FFP of the model
6˜E←∅ //Initialize edge set of auxiliary graph ˜G=(V,˜E)
7for each node pair𝑝𝑙=(𝑣𝑖,𝑣𝑗)∈P do
8 ifˆy𝑙>0.5then
9 add𝑝𝑙=(𝑣𝑖,𝑣𝑗)to˜E
10extract connected components of ˜Gvia DFS/BFS on ˜E
11treat each extracted component as a community to form C
4.1.3 Binary Node Pair Classifier. As highlighted in Fig. 2 (c),
given a pair of nodes (𝑣𝑖,𝑣𝑗)sampled from a graph G, we use a
binary classifier to estimate S𝑖𝑗based on embeddings (z𝑖,z𝑗)and
determine whether (𝑣𝑖,𝑣𝑗)are partitioned into the same community.
A widely adopted design of binary classifier is as follow
ˆS𝑖𝑗=sigmoid(z𝑖z𝑇
𝑗/𝜏), (5)
with𝜏as a pre-set temperature parameter. Instead of directly using
(5), we introduce a novel binary classifier with pair-wise adaptive
temperature parameters {𝜏𝑖𝑗}, which can derive a more accurate
estimation of Sto derive high-quality CD results as demonstrated
in our experiments. Our binary classifier can be described as
ˆS𝑖𝑗=exp(−|z𝑖−z𝑗|2𝜏𝑖𝑗)=exp(2𝜏𝑖𝑗(z𝑖z𝑇
𝑗−1)),
with𝜏𝑖𝑗=ℎ𝑠(z𝑖)ℎ𝑑(z𝑗)𝑇,(6)
whereℎ𝑠andℎ𝑑are MLPs with the same configuration. In (6), S𝑖𝑗
is estimated based on the distance between (z𝑖,z𝑗)and a pair-wise
temperature parameter 𝜏𝑖𝑗. Different node pairs {(𝑣𝑖,𝑣𝑗)}may have
different parameters {𝜏𝑖𝑗}determined by embeddings {(z𝑖,z𝑗)}.
4.1.4 Result Derivation Module. As illustrated in Fig. 2 (d), we
develop a result derivation module, which outputs a feasible CD
resultCbased on a set of node pairs sampled from the input graph
G. Algorithm 1 summarizes the procedure of this module.
In lines 1-4, we construct a set of node pairs P={(𝑣𝑖,𝑣𝑗)}(e.g.,
dotted lines in Fig. 2 (d)), which includes (i) all the edges of input
graphG(e.g.,(𝑣1,𝑣7)and(𝑣3,𝑣4)in Fig. 2 (d)) and (ii) 𝑛𝑆randomly
sampled node pairs (e.g., (𝑣6,𝑣7)and(𝑣3,𝑣7)in Fig. 2 (d)).
In line 5, we derive the estimated values {ˆS𝑖𝑗}w.r.t. node pairs
inP(via one FFP of the model) and rearrange them as a vector ˆy∈
R|P|. In particular, ˆy𝑙=ˆS𝑖𝑗represents the probability that a pair
of nodes𝑝𝑙=(𝑣𝑖,𝑣𝑗)are partitioned into a common community.
In lines 6-9, we further construct an auxiliary graph ˜G=(V,˜E)
based on ˆyandP.˜Ghas the same node set Vas the input graph
Gbut a different edge set ˜E. For each𝑝𝑙=(𝑣𝑖,𝑣𝑗)∈P , we add
(𝑣𝑖,𝑣𝑗)to˜E(i.e., preserving this node pair as an edge in ˜G) if
ˆy𝑙>0.5.˜Gmay contain multiple connected components (e.g., the
two components in Fig. 2 (d)). All the edges {𝑒𝑙=(𝑣𝑖,𝑣𝑗)}within a
component are with high values of {ˆy𝑙=ˆS𝑖𝑗}, indicating that the
associated nodes are more likely to belong to the same community.
In lines 10-11, we derive a feasible CD result Cw.r.t.Gby extract-
ing all the connected components of ˜Gvia the depth-first search(DFS) or breadth-first search (BFS). Each connected component of
˜Gcorresponds to a unique community in G(e.g., the two communi-
ties in Fig. 2 (d)), with the number of components as the estimated
number of communities 𝐾. Therefore, the aforementioned designs
enable our PRoCD method to tackle 𝐾-agnostic CD.
4.2 Offline Pre-Training
We conducted the offline pre-training of PRoCD on a set of synthetic
graphsT={G1,···,G𝑇}. One significant advantage of using syn-
thetic pre-training data is that we can simulate various properties
of graph topology with different levels of noise and high-quality
ground-truth by adjusting parameters of the synthetic generator.
In particular, we consider a challenging setting of pre-training
PRoCD on small synthetic graphs (e.g., with 𝑁≈5×103nodes)
but generalize it to large real graphs (e.g., 𝑁>106). One reason
of using small pre-training graphs is that it enables PRoCD to
construct dense 𝑁×𝑁matrices in pre-training objectives (e.g.,
ˆS∈R𝑁×𝑁for binary classification) and thus fully explore the
topology and community ground-truth of pre-training data. In
contrast, constructing dense 𝑁×𝑁matrices for large graphs may
be intractable. Although there may be distribution shifts between
the (i) small pre-training graphs {G𝑡}and (ii) large graphs {G′}to
be partitioned, our experiments demonstrate that offline pre-training
on{G𝑡}is essential for PRoCD to derive feasible CD results {¯C′}
for{G′}. By using{¯C′}as the initialization, their quality can be
further refined via an efficient CD method.
4.2.1 Generation of Pre-Training Data. As a demonstration,
we adopt the degree-corrected SBM (DC-SBM) [ 15] implemented
bygraph-tool1to generate synthetic pre-training graphs. Besides
topology, the generator also produces community assignments w.r.t.
a specified level of noise, which can be used as the high-quality
ground-truth of CD. Our experiments also validate that the syn-
thetic ground-truth can help PRoCD derive a good initialization for
online refinement. Whereas, community ground-truth is usually un-
available for existing methods [ 46,49] (pre-)trained on real graphs.
Instead of using fixed generator parameters, we let them follow
certain probability distributions to simulate various topology prop-
erties (e.g., distributions of node degrees and community sizes) and
noise levels of community structures. Namely, different graphs may
be generated based on different parameter settings sampled from
certain distributions. Due to space limits, we leave details about
the parameter setting and generation algorithm in Appendix A.
4.2.2 Pre-Training Objectives & Algorithms. For each syn-
thetic graphG𝑡, suppose there are 𝑁𝑡nodes partitioned into 𝐾𝑡
communities. We use M(𝑡)to denote a vector/matrix associated
withG𝑡(e.g., ˆy(𝑡)andˆS(t)). Given a graphG𝑡, some previous stud-
ies [46,49] adopt a relaxed version of the modularity maximization
objective (2) for model optimization and demonstrate its potential
to capture community structures. This relaxed objective allows
H(𝑡)∈{0,1}𝑁𝑡×𝐾𝑡in (2), which describes hard community assign-
ments, to be continuous values (e.g., H(𝑡)∈R𝑁𝑡×𝐾𝑡+ derived via
an MLP). Note that we reformulate CD as the binary node pair
classification with S(𝑡)∈{0,1}𝑁𝑡×𝑁𝑡.ˆS(𝑡)∈R𝑁𝑡×𝑁𝑡+ derived from
1https://graph-tool.skewed.de/
 
2471KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
Algorithm 2: Offline Pre-training
Input: synthetic pre-training graphs T={G1,···,G𝑇}and their
ground-truth; hyper-parameters {𝜆,𝛼}; number of epochs 𝑛𝑃;
learning rate 𝜂
Output: optimized model parameters Θ∗
1initialize model parameters Θ
2forepoch from 1to𝑛𝑃do
3 for eachG𝑡=(V𝑡,E𝑡)∈T do
4 construct S(𝑡)w.r.t. ground-truthC(𝑡)
5 get input feature X(𝑡)w.r.t.G𝑡via (3)
6 get estimated values ˆS(𝑡)w.r.t.G𝑡via one FFP
7 arrange elements w.r.t. E𝑡inˆS(𝑡)asˆy(𝑡)
8 update model parameters Θ←Opt(𝜂,Θ,𝜕LPTN(G𝑡)/𝜕Θ)
9save the optimized model parameters Θ∗
the binary classifier can be considered as the relaxation of S(𝑡).
Based on ˆS(𝑡), we rewrite the modularity maximization objective
(1) to the following relaxed form for each graph G𝑡:
LMOD(G𝑡):=−[1
𝑀∑︁
𝑒𝑙∈E𝑡ˆy(𝑡)
𝑙−𝜆
4𝑀2∑︁
𝑖𝑗(D(𝑡)ˆS(𝑡)D(𝑡))𝑖𝑗],(7)
where ˆy(𝑡)is the rearrangement of elements in ˆS(𝑡)w.r.t. the edge
setE𝑡;D(𝑡)is the degree diagonal matrix of G𝑡;𝜆>0is the
resolution parameter of modularity maximization [ 37], with𝜆<1
(>1) favoring the partition of large (small) communities.
In addition, we utilize the synthetic community ground-truth
C𝑡of each graphG𝑡to enhance the ability of PRoCD to capture
community structures. Note that different graphs {G𝑡}may have
different numbers of communities {𝐾𝑡}. However, some DGL-based
CD methods [ 6,46,49] are only designed for graphs with a fixed
𝐾. Community labels are also permutation-invariant. For instance,
(𝑐1,𝑐2,𝑐3,𝑐4,𝑐5)=(1,1,1,2,2)and(𝑐1,𝑐2,𝑐3,𝑐4,𝑐5)=(2,2,2,1,1)
represent the same community assignment, with 𝑐𝑖as the label
assignment of node 𝑣𝑖. Hence, the standard multi-class cross en-
tropy objective cannot be directly applied to integrate community
ground-truth. PRoCD handles these issues by reformulating CD as
the binary node pair classification with auxiliary variables {S(𝑡)},
where the dimensionality and values of {S(𝑡)}are unrelated to{𝐾𝑡}
and permutations of community labels. Given a graph G𝑡, one can
construct S(𝑡)based on ground-truth C(𝑡)and derive ˆS(𝑡)via one
FFP of the model. We introduce the following binary cross entropy
objective that covers all the 𝑁𝑡×𝑁𝑡node pairs:
LBCE(G𝑡):=−∑︁
𝑣𝑖,𝑣𝑗∈V"
S(𝑡)
𝑖𝑗logˆS(𝑡)
𝑖𝑗+
(1−S(𝑡)
𝑖𝑗)log(1−ˆS(𝑡)
𝑖𝑗)#
. (8)
Finally, we formulate the pre-training objective w.r.t. G𝑡as
LPTN(G𝑡):=LMOD(G𝑡)+𝛼LBCE(G𝑡), (9)
where𝛼>0is the hyper-parameter to balance LMOD andLBCE.
The offline pre-training procedure is concluded in Algorithm 2,
where the Adam optimizer with learning rate 𝜂is applied to up-
date model parameters Θ; the updated Θafter𝑛𝑃epochs are then
saved for online generalization and refinement. Since only the em-
bedding encoder and binary classifier contain model parameters to
be learned, we do not need to apply Algorithm 1 of the result deriva-
tion module to derive a feasible CD result in offline pre-training.
Initial CD Result

Refinement Method
(e.g., InfoMap )
Refinement Method
(e.g., LPA)Super -graph
*
Community 1
Community 2v1
v2
v3v4v5
v7v6
v9 v8
v10v11
v12
Intermediate Label Assignmentc10=2c1=1
c2=1
c3=1c4=1c5=3
c6=3c7=3
c9=2v1*v3*v2*v4*
v5*2 11
1v1*v3*v2*v4*
v5*2 11
1v1
v2
v3v4v5
v7v6v8
v9v10v11
v12
Community 1 Community 3Community 2Community 4
Community 5v1
v2
v3v4v5
v7v6v8
v9v10v11
v12
Community 1 Community 3Community 2Community 4
Community 5
c8=2
c11=4
c12=5
v1*v2*v3*v4*
v5*v1*v2*v3*v4*
v5*
(Recover)v1v2
v3
v4
v5 v7v6
v8 v9
v10
v11v12Community 1
Community 2
Refined CD Result

c1=1c2=1
c3=1
c4=1
c5=1c6=1
c7=1
c9=2c8=2
c11=2
c12=2c10=2Figure 3: Illustration of the two strategies to construct the
initialization of online refinement.
4.3 Online Generalization & Refinement
After offline pre-training, we can directly generalize PRoCD to large
real graphs{G′}(with frozen model parameters Θ∗) and derive fea-
sible CD results{¯C′}via only one FFP of the model and Algorithm 1
(i.e., online generalization ).
Recent advances in graph pre-training techniques [ 50] demon-
strate that pre-training may provide good initialized model param-
eters for downstream tasks, which can be further improved via
a task-specific fine-tuning procedure. To the best of our knowl-
edge, most existing graph pre-training methods merely consider
supervised tasks (e.g., node classification) but do not provide unsu-
pervised tuning objectives for CD. A straightforward fine-tuning
strategy for CD is applying the modularity maximization objective
(7) to large real graphs {G′}(e.g., gradient descent w.r.t. a relaxed
version of (2) with large dense matrices {Q′}), which is usually
intractable. Analogous to fine-tuning, we introduce the online re-
finement phase with a different design. Instead of fine-tuning, we
treat the CD result ¯C′derived in online generalization as the ini-
tialization of a conventional efficient CD method (e.g., LPA [36],
InfoMap [40], and Locale [48] in our experiments) and run this
method to derive a refined CD result C′.
As shown in Fig. 3, we adopt two strategies to construct the
initialization. First, a super-graph G∗can be extracted based on ¯C′,
where we merge nodes in each community ¯C′𝑟∈¯C′as a super-node
(e.g.,𝑣∗
3w.r.t. ¯C′
3={𝑣5,𝑣6,𝑣7}in Fig. 3) and set the number of edges
between communities as the weight of each super-edge (e.g., 2for
(𝑣∗
1,𝑣∗
3)in Fig. 3). We then use G∗as the input of an efficient method
that can handle weighted graphs (e.g., InfoMap andLocale ) to derive
a CD result w.r.t.G∗, which is further recovered to the result C′w.r.t.
G′. Second, we also use ¯C′as the intermediate label assignment
of a method that iteratively updates community labels (e.g., LPA)
and refines the label assignment until convergence (e.g., refining
𝑐6from 3to1in Fig. 3). Compared with running the refinement
method onG′from scratch, online refinement may be more efficient,
because the constructed initialization reduces the number of nodes
to be partitioned and iterations to update community labels. It is
 
2472Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 3: Online Generalization & Refinement
Input: large real graphG′to be partitioned; saved model parameters Θ∗
Output: CD resultsC′w.r.t.G′
1get input feature X′w.r.t.G′via (3)
2get a feasible CD result ¯C′w.r.t.G′via Algorithm 1
3construct the initialization for refinement based on ¯C′
4get refined CD result C′via an efficient refinement method
Table 2: Statistics of datasets.
Datasets 𝑁𝐸 Min Max Avg Deg Density
Protein [42] 81,574 1,779,885 1 4,983 43.6 5e-4
ArXiv [47] 169,343 1,157,799 1 13,161 13.7 8e-5
DBLP [53] 317,080 1,049,866 1 343 6.6 2e-5
Amazon [53] 334,863 925,872 1 549 5.5 2e-5
Youtube [53] 1,134,890 2,987,624 1 28,754 5.3 5e-6
RoadCA [18] 1,957,027 2,760,388 1 12 2.82 1e-6
also expected that PRoCD can transfer the capability of capturing
community structures from the pre-training data to {G′}.
Algorithm 3 concludes the aforementioned procedure. The com-
plexity of online generalization (i.e., lines 1-2) isO((𝑁+𝑀)𝑑2),
where𝑁and𝑀are the numbers of nodes and edges; 𝑑is the em-
bedding dimensionality. The complexity of online refinement (i.e.,
lines 3-4) depends on the refinement method, which is usually
efficient. We leave detailed complexity analysis in Appendix B.
5 EXPERIMENTS
In this section, we elaborate on our experiments, including the
experiment setup, evaluation results, ablation study, and parame-
ter analysis. We will make the datasets and demo code public at
https://github.com/KuroginQin/PRoCD.
5.1 Experiment Setup
Datasets. We evaluated the inference efficiency and quality of
ourPRoCD method on 6public datasets with statistics depicted
in Table 2, where 𝑁and𝑀are the numbers of nodes and edges.
Note that these datasets do not provide ground-truth about the
community assignment and the number of communities 𝐾. Some
more details regarding the datasets can be found in Appendix C.1.
Baselines. We compared PRoCD over 11baselines.
(i)MC-SBM [26], (ii) Par-SBM [27], (iii) GMod [8], (iv) LPA[36], (v)
InfoMap [40], (vi) Louvain [3], and (vii) Locale [48], and (viii) RaftGP
[13] are efficient end-to-end methods that can tackle 𝐾-agnostic CD.
Note that some other approaches whose designs rely on a pre-set
𝐾(e.g., FastCom [8],GraClus [10],ClusterNet [49],LGNN [6], and
DMoN [46]) could not be included in our experiments.
In addition, we extended (ix) LouvainNE [2], (x) SketchNE [51],
and (xi) ICD[33], which are state-of-the-art efficient graph embed-
ding methods, to 𝐾-agnostic CD by combining the derived embed-
dings with DBSCAN [ 41], an efficient clustering algorithm that
does not require 𝐾. In particular, LouvainNE andICDare claimed
to be community-preserving methods. RaftGP-C, RaftGP-M, ICD-C,
andICD-M denote different variants of RaftGP andICD.
As a demonstration, we adopted LPA, InfoMap, and Locale (an ad-
vanced extension of Louvain ) for the online refinement ofPRoCD,
forming three variants of our method. In Fig. 3, the first strategy of
constructing the refinement initialization has a motivation similarto graph coarsening (GC) techniques [ 5], which merge a large graph
G′into a super-graph G∗(i.e., reducing the number of nodes to be
partitioned) via a heuristic strategy. Whereas, PRoCD constructs
G∗based on the output of a pre-trained model. To highlight the
superiority of offline pre-training and inductive inference beyond
GC, we introduced another baseline that provides the initializa-
tion (with the same number of super-nodes as PRoCD) for online
refinement using the efficient GC strategy proposed in [19].
ICDis an inductive baseline with a paradigm similar to PRoCD,
including offline (pre-)training on historical graphs and online gen-
eralization to new graphs. However, there is no online refinement
inICD. We conducted the offline pre-training ofICDandPRoCD
on the generated synthetic graphs as described in Section 4.2.1 and
generalized them to each dataset for online inference. For the rest
baselines, we had to run them from scratch on each dataset.
Evaluation Criteria. Following the evaluation protocol de-
scribed in Section 3, we adopted modularity and inference time
(sec) as the quality and efficiency metrics. Moreover, we define that
a method encounters the out-of-time (OOT) exception if it fails to
derive a feasible CD result within 2×104seconds.
Due to space limits, we elaborate on other details of experiment
setup (e.g., parameter settings) in Appendix C.
5.2 Quantitative Evaluation & Discussions
The average evaluation results are reported in Table 3. Besides the
total inference time, we also report the time w.r.t. each inference
phase of PRoCD in Table 4, where ‘Feat’, ‘FFP’, ‘Init’, and ‘Rfn’
denote the time of (i) feature extraction, (ii) one FFP, (iii) initial
result derivation, and (iv) online refinement. Analysis about the
inference time of GC and embedding baselines is given in Tables 5
and 6, where ‘GC’, ‘Rfn’, ‘Emb’, and ‘Clus’ denote the time of (i)
graph coarsening, (ii) online refinement, (iii) embedding derivation,
and (iv) downstream clustering (i.e., via DBSCAN).
In Table 3, three variants of PRoCD achieve significant improve-
ment of efficiency w.r.t. the refinement methods while the quality
degradation is less than 2%. In some cases, PRoCD can even obtain
improvement for both aspects. Compared with all the baselines,
PRoCD can ensure the best efficiency on all the datasets and is in
groups with the best quality. In summary, we believe that the pro-
posed method provides a possible way to obtain a better trade-off
between the quality and efficiency in CD.
In most cases, PRoCD outperforms GC baselines in both aspects,
which validates the superiority of offline pre-training and inductive
inference beyond heuristic GC. The extensions of some embedding
baselines (e.g., SketchNE andICDcombined with DBSCAN) suffer
from low quality and efficiency compared with other end-to-end ap-
proaches (e.g., Louvain andLocale ). It implies that the task-specific
module (e.g., binary node pair classifier in PRoCD) is essential for
the embedding framework to ensure the quality and efficiency in
𝐾-agnostic CD, which is seldom considered in related studies.
According to Table 4, online refinement is the major bottleneck
in the inference of PRoCD, with the runtime depending on con-
crete refinement methods. Compared with running a refinement
method from scratch, online refinement has much lower runtime. It
validates our motivation that constructing the initialization (as de-
scribed in Fig. 3) can reduce the number of nodes to be partitioned
 
2473KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
Table 3: Evaluation results w.r.t. efficiency metric of inference time ↓(sec) and quality metric of modularity ↑.
Protein ArXiv DBLP Amazon Youtube RoadCA
K Time↓Mod↑ K Time↓Mod↑ K Time↓Mod↑ K Time↓Mod↑ K Time↓Mod↑ K Time↓Mod↑
MC-SBM 170 1143.93 0.1463 303 1014.63 0.5205 430 646.91 0.7620 368 799.05 0.8860 186 6608.67 0.2027 324 8492.20 0.9491
Par-SBM 1494 39.18 0.4721 4936 40.26 0.0529 10854 28.04 0.2797 10813 39.40 0.3094 27948 236.61 0.0901 39987 149.10 0.5970
GMod 69 5840.49 0.6319 1317 6693.91 0.5797 OOT OOT OOT 1669 12729.36 0.8703 OOT OOT OOT OOT OOT OOT
Louvain 60 46.23 0.6366 155 49.07 0.7059 209 83.50 0.8209 240 48.10 0.9262 6105 173.70 0.7218 376 408.25 0.9923
RaftGP-C 218 69.82 0.6124 213 79.11 0.6147 39 81.99 0.6594 53 87.66 0.7530 1 254.77 0.0000 256 470.05 0.8072
RaftGP-M 185 69.44 0.6199 194 80.10 0.6107 39 83.48 0.6644 54 88.24 0.7561 131 272.82 0.6659 256 469.14 0.8043
LouNE+DBSCAN 32 77.20 0.6362 170 214.55 0.7058 737 109.87 0.8065 252 64.45 0.9255 17908 671.52 0.6228 19081 236.46 0.9531
SktNE+DBSCAN 371 16.18 0.1808 388 542.92 0.3533 15341 45.60 0.5716 13283 49.85 0.7047 17799 485.72 0.1076 59726 1410.91 0.4073
ICD-C+DBSCAN 1277 89.30 0.4961 256 296.02 0.0011 3694 1795.04 0.1047 1865 2027.30 0.0278 OOT OOT OOT OOT OOT OOT
ICD-M+DBSCAN 1091 88.47 0.5651 264 294.13 0.0019 3950 984.98 0.1156 2478 872.11 0.0430 OOT OOT OOT OOT OOT OOT
LPA 1919 132.32 0.5925 11467 113.30 0.3752 46630 146.22 0.6196 41693 76.49 0.7090 111887 370.15 0.5520 606249 245.54 0.5524
PRoCDw/ LPA 1499 32.07 0.6046 10765 19.93 0.6188 38684 20.10 0.6336 34270 20.40 0.7240 93566 68.72 0.6150 391735 67.74 0.5765
Improve. +75.8% +2.1% +82.4% +64.9% +86.3% +2.3% +73.3% +2.1% +81.4% +11.4% +72.4% +4.4%
GC+LPA 1985 32.52 0.5790 12854 21.57 0.5290 44494 30.04 0.5906 41513 29.40 0.5723 124044 69.17 0.2710 442623 106.89 0.4838
InfoMap 11 26.38 0.1972 69 19.69 0.6332 527 29.11 0.8195 13 28.72 0.8037 956 114.63 0.6959 3 259.44 0.6455
PRoCDw/ IM 18 15.60 0.1996 42 17.08 0.6349 298 17.23 0.8222 417 16.66 0.9222 438 83.01 0.6811 145 79.48 0.9889
Improve. +40.9% +1.2% +13.3% +0.3% +40.8% +0.3% +42.0% +14.7% +27.6% -2.1% +69.4% +53.2%
GC+IM 9 33.84 0.1318 48 24.28 0.4735 305 27.99 0.8086 4 28.01 0.4189 687 104.85 0.6919 3 129.74 0.6561
Locale 51 43.04 0.6409 150 46.66 0.7159 298 40.08 0.8375 392 30.85 0.9343 3441 216.12 0.7353 410 172.48 0.9935
PRoCDw/ Lcl 35 16.93 0.6362 64 31.76 0.7133 147 24.15 0.8376 173 18.94 0.9319 1647 161.24 0.7315 330 90.88 0.9930
Improve. +60.7% -0.7% +31.9% -0.4% +39.8% +0.01% +38.6% -0.3% +25.4% -0.5% +47.3% -0.1%
GC+Lcl 33 44.35 0.6343 150 45.17 0.6988 196 36.87 0.8254 344 30.09 0.9220 3228 194.49 0.7315 327 130.57 0.9928
Table 4: Detailed inference time (sec) of PRoCD.
Datasets Methods Feat FFP Init Rfn
ProteinPRoCD w/LPA
2.84 4.42 6.5018.31
PRoCD w/InfoMap 1.84
PRoCD w/Locale 3.17
ArXivPRoCD w/LPA
1.49 0.99 4.5012.96
PRoCD w/InfoMap 10.11
PRoCD w/Locale 24.79
DBLPPRoCD w/LPA
1.97 1.28 4.1912.66
PRoCD w/InfoMap 9.79
PRoCD w/Locale 16.71
AmazonPRoCD w/LPA
2.08 1.99 3.8112.51
PRoCD w/InfoMap 8.78
PRoCD w/Locale 11.06
YoutubePRoCD w/LPA
5.81 2.75 15.1345.02
PRoCD w/InfoMap 59.32
PRoCD w/Locale 137.54
RoadCAPRoCD w/LPA
8.10 2.34 13.1444.16
PRoCD w/InfoMap 55.90
PRoCD w/Locale 67.31
Table 5: Detailed inference time (sec) of graph GC baselines.
Datasets Methods GC Rfn
ProteinGC+LPA
15.2017.32
GC+InfoMap 18.64
GC+Locale 29.15
ArXivGC+LPA
10.2011.37
GC+InfoMap 14.08
GC+Locale 34.97
DBLPGC+LPA
17.8412.20
GC+InfoMap 10.15
GC+Locale 19.03
AmazonGC+LPA
17.1912.21
GC+InfoMap 10.82
GC+Locale 12.90
YoutubeGC+LPA
31.8437.32
GC+InfoMap 73.01
GC+Locale 162.65
RoadCAGC+LPA
71.3335.56
GC+InfoMap 58.41
GC+Locale 59.24Table 6: Detailed inference time (s) of embedding baselines.
Datasets Methods Emb Clus
ProteinLouvainNE+DBSCAN 4.13 73.07
SketchNE+DBSCAN 4.30 11.88
ICD-C+DBSCAN 6.99 82.31
ICD-M+DBSCAN 12.49 75.97
ArXivLouvainNE+DBSCAN 6.86 207.41
SketchNE+DBSCAN 5.79 537.12
ICD-C+DBSCAN 21.59 274.43
ICD-M+DBSCAN 19.57 274.55
DBLPLouvainNE+DBSCAN 10.52 99.35
SketchNE+DBSCAN 7.71 37.90
ICD-C+DBSCAN 5.66 1789.38
ICD-M+DBSCAN 11.53 973.45
AmazonLouvainNE+DBSCAN 10.79 53.67
SketchNE+DBSCAN 8.05 41.79
ICD-C+DBSCAN 16.26 2011.04
ICD-M+DBSCAN 23.22 848.89
YoutubeLouvainNE+DBSCAN 34.98 635.55
SketchNE+DBSCAN 19.40 466.32
ICD-C+DBSCAN 10.43 OOT
ICD-M+DBSCAN 7.66 OOT
RoadCALouvainNE+DBSCAN 56.30 180.15
SketchNE+DBSCAN 25.11 1385.80
ICD-C+DBSCAN 14.49 OOT
ICD-M+DBSCAN 12.90 OOT
and iterations to update label assignments for refinement methods.
To construct the initialization of online refinement, GC is not as
efficient as the online generalization of PRoCD (i.e., feature extrac-
tion, one FFP, and initial result derivation) according to Table 5.
In Table 6, the downstream clustering (i.e., DBSCAN to derive a
feasible result for 𝐾-agnostic CD) is time-consuming, which results
in low efficiency of the extended embedding baselines, although
their embedding derivation phases are efficient.
5.3 Ablation Study
For our PRoCD method, we verified the effectiveness of (i) online
refinement, (ii) offline pre-training, (iii) feature extraction module
in (3), (iv) embedding encoder in (4), (v) binary node pair classifier
 
2474Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 7: Ablation studies on DBLP and Amazon in terms of
modularity↑and inference time ↓(sec).
DBLP Amazon
K Mod↑Time↓ K Mod↑Time↓
(i)w/o Rfn 106553 0.4394 7.44 112408 0.5620 7.88
PRoCD w/LPA 38684 0.6336 20.10 34270 0.7240 20.40
(ii)w/o Ptn 1 0.0000 9.75 1 0.0000 8.81
(iii)w/o Feat (3) 537 0.0625 18.01 1 0.0000 6.29
(iv)w/o EmbEnc (4) 58401 0.5406 18.26 53127 0.5090 17.08
(v)w/o BinClf (6) 48 0.0009 6.43 23 0.0008 6.31
(vi)w/oLBCE(8) 22496 0.5657 17.84 16378 0.6580 20.19
(vii) w/oLMOD (7) 39792 0.6261 20.12 34853 0.7188 19.31
PRoCD w/IM 298 0.8222 17.23 417 0.9222 16.66
(ii)w/o Ptn 1 0.0000 9.75 1 0.0000 8.81
(iii)w/o Feat (3) 2 0.0001 5.67 1 0.0000 5.13
(iv)w/o EmbEnc (4) 516 0.8182 26.55 6 0.3869 39.27
(v)w/o BinClf (6) 48 0.0009 5.88 23 0.0008 5.68
(vi)w/oLBCE(8) 156 0.2895 12.93 356 0.4872 9.91
(vii)w/oLMOD (7) 306 0.8200 19.22 2 0.2638 22.06
PRoCD w/Lcl 147 0.8376 24.15 173 0.9319 18.94
(ii)w/o Ptn 1 0.0000 9.75 1 0.0000 8.81
(iii)w/o Feat (3) 551 0.0647 5.66 1 0.0000 5.15
(iv)w/o EmbEnc (4) 216 0.8368 42.20 307 0.9228 35.97
(v)w/o BinClf (6) 48 0.0009 6.66 23 0.0008 6.43
(vi)w/oLBCE(8) 305 0.7460 18.04 776 0.7448 19.23
(vii)w/oLMOD (7) 143 0.8365 25.97 210 0.9228 17.76
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.6Mod↑
(a) DBLP, w/ LPA
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.6Mod↑ (b) Amazon, w/ LPA
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.60.8Mod↑
(c) DBLP, w/ InfoMap
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.60.8Mod↑ (d) Amazon, w/ InfoMap
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.60.8Mod↑
(e) DBLP, w/ Locale
0 10 50 1e2 5e2 1e3 5e3
T00.20.40.60.8Mod↑ (f) Amazon, w/ Locale
Figure 4: Ablation study w.r.t. the number of pre-training
graphs𝑇on DBLP and Amazon in terms of modularity ↑.
in (6), (vi) cross-entropy objective LBCE, and (vii) modularity maxi-
mization objectiveLMOD by removing corresponding components
from the original model. In case (iii), we let Xbe the one-hot encod-
ings of node degree, which is a standard feature extraction strategy
for GNNs when attributes are unavailable. We directly used Xas
the derived embeddings in case (iv), while we adopted the naive
binary classifier (5) to replace our design (6) in case (v). The average
ablation study results on DBLP and Amazon are reported in Table 7,
where there are quality declines in all the cases. In some extreme
cases (e.g., the one without pre-training), the model mistakenly
partitions all the nodes into one community, resulting in a modu-
larity value of 0.0000. In summary, all the considered procedures
and components are essential to ensure the quality of PRoCD.
To further verify the significance of offline pre-training in our
PRoCD method, we conducted additional ablation study by setting
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.610.620.630.64(a) DBLP, w/ LPA
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.810.8150.820.825 (b) DBLP, w/ InfoMap
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.830.8320.8340.8360.8380.84 (c) DBLP, w/ Locale
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.710.7150.720.7250.73
(d) Amazon, w/ LPA
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.920.9210.9220.9230.9240.925 (e) Amazon, w/ InfoMap
0.1 1 10 1e2 1e3
λ0.01
0.1
1
10
1e2α
0.9310.93150.9320.9325 (f) Amazon, w/ Locale
Figure 5: Parameter analysis w.r.t. 𝛼and𝜆on DBLP and Ama-
zon in terms of modularity ↑.
the number of pre-training graphs 𝑇∈{0,10,50,1𝑒2,5𝑒2,1𝑒3,5𝑒3}.
The corresponding results on DBLP and Amazon in terms of modu-
larity are reported in Fig. 4. Compared with our standard setting
(i.e.,𝑇=1𝑒3), there are significant quality declines for all the vari-
ants of PRoCD when there are too few pre-training graphs (e.g.,
𝑇<50). It implies that the offline pre-training (with enough synthetic
graphs) is essential to ensure the high inference quality of PRoCD.
5.4 Parameter Analysis
We also tested the effects of {𝛼,𝜆}in the pre-training objective (9)
by adjusting 𝛼∈{0.01,0.1,1,10,100}and𝜆∈{0.1,1,10,100,1000}.
The example parameter analysis results on DBLP and Amazon in
terms of modularity are shown in Fig. 5, where different settings
of{𝛼,𝜆}would not significantly affect the quality of PRoCD. The
influence of{𝛼,𝜆}may also differ from refinement methods. For
instance, LPAis more sensitive to the parameter setting, compared
with InfoMap andLocale.
Due to space limit, we leave further parameter analysis about the
number of sampled node pair 𝑛𝑆(see Algorithm 1) in Appendix D.
6 CONCLUSION
In this paper, we explored the potential of DGL to achieve a bet-
ter trade-off between the quality and efficiency of 𝐾-agnostic CD.
By reformulating this task as the binary node pair classification,
a simple yet effective PRoCD method was proposed. It follows a
novel pre-training & refinement paradigm, including the (i) offline
pre-training on small synthetic graphs with various topology prop-
erties and high-quality ground-truth as well as the (ii) online gener-
alization to and (iii) refinement on large real graphs without addi-
tional model optimization. Extensive experiments demonstrate that
PRoCD, combined with different refinement methods, can achieve
higher inference efficiency without significant quality degradation
on public real graphs with various scales. Some possible future
research directions are summarized in Appendix E.
ACKNOWLEDGMENTS
This research has been made possible by funding support provided
to Dit-Yan Yeung by the Research Grants Council of Hong Kong
under the Research Impact Fund project R6003-21.
 
2475KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
REFERENCES
[1]Rosa I Arriaga and Santosh Vempala. 2006. An algorithmic theory of learning:
Robust concepts and random projection. Machine Learning 63 (2006), 161–182.
[2]Ayan Kumar Bhowmick, Koushik Meneni, Maximilien Danisch, Jean-Loup Guil-
laume, and Bivas Mitra. 2020. Louvainne: Hierarchical louvain method for high
quality and scalable network embedding. In WSDM. 43–51.
[3]Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of Statistical
Mechanics: Theory & Experiment 2008, 10 (2008), P10008.
[4]Yuxuan Cao, Jiarong Xu, Carl J. Yang, Jiaan Wang, Yunchao Zhang, Chunping
Wang, Lei Chen, and Yang Yang. 2023. When to Pre-Train Graph Neural Net-
works? From Data Generation Perspective!. In SIGKDD. 142–153.
[5]Jie Chen, Yousef Saad, and Zechen Zhang. 2022. Graph coarsening: from scientific
computing to machine learning. SeMA Journal (2022), 1–37.
[6]Zhengdao Chen, Joan Bruna, and Lisha Li. 2019. Supervised community detection
with line graph neural networks. In ICLR.
[7]Petr Chunaev, Timofey Gradov, and Klavdiya Bochenina. 2020. Community de-
tection in node-attributed social networks: How structure-attributes correlation
affects clustering quality. Procedia Computer Science 178 (2020), 355–364.
[8]Aaron Clauset, Mark EJ Newman, and Cristopher Moore. 2004. Finding commu-
nity structure in very large networks. Physical Review E 70, 6 (2004), 066111.
[9]Lin Dai and Bo Bai. 2017. Optimal decomposition for large-scale infrastructure-
based wireless networks. TWC 16, 8 (2017), 4956–4969.
[10] Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts
without eigenvectors a multilevel approach. TPAMI 29, 11 (2007), 1944–1957.
[11] Santo Fortunato. 2010. Community detection in graphs. Physics Reports 486, 3-5
(2010), 75–174.
[12] Santo Fortunato and Mark EJ Newman. 2022. 20 years of network community
detection. Nature Physics 18, 8 (2022), 848–850.
[13] Yu Gao, Meng Qin, Yibin Ding, Li Zeng, Chaorui Zhang, Weixi Zhang, Wei Han,
Rongqian Zhao, and Bo Bai. 2023. RaftGP: Random Fast Graph Partitioning. In
HPEC. 1–7.
[14] Jiashun Jin, Zheng Tracy Ke, and Shengming Luo. 2021. Improvements on score,
especially for weak signals. Sankhya A (2021), 1–36.
[15] Brian Karrer and Mark EJ Newman. 2011. Stochastic blockmodels and community
structure in networks. Physical Review E 83, 1 (2011), 016107.
[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. In ICLR.
[17] Kai Lei, Meng Qin, Bo Bai, Gong Zhang, and Min Yang. 2019. GCN-GAN: A
non-linear temporal link prediction model for weighted dynamic networks. In
InfoCom. 388–396.
[18] Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. 2009.
Community structure in large networks: Natural cluster sizes and the absence of
large well-defined clusters. Internet Mathematics 6, 1 (2009), 29–123.
[19] Jiongqian Liang, Saket Gurukar, and Srinivasan Parthasarathy. 2021. Mile: A
multi-level framework for scalable graph embedding. In AAAI Conference on Web
& Social Media, Vol. 15. 361–372.
[20] Yue Liu, Wenxuan Tu, Sihang Zhou, Xinwang Liu, Linxuan Song, Xihong Yang,
and En Zhu. 2022. Deep graph clustering via dual correlation reduction. In AAAI,
Vol. 36. 7603–7611.
[21] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt:
Unifying pre-training and downstream tasks for graph neural networks. In Web
Conference. 417–428.
[22] Yuanfu Lu, Xunqiang Jiang, Yuan Fang, and Chuan Shi. 2021. Learning to pre-train
graph neural networks. In AAAI, Vol. 35. 4276–4284.
[23] Azade Nazi, Will Hang, Anna Goldie, Sujith Ravi, and Azalia Mirhoseini. 2019.
Gap: Generalizable approximate graph partitioning framework. arXiv:1903.00614
(2019).
[24] Mark EJ Newman. 2006. Modularity and Community Structure in Networks.
PNAS 103, 23 (2006), 8577–8582.
[25] Mark EJ Newman and Aaron Clauset. 2016. Structure and inference in annotated
networks. NC7, 1 (2016), 11863.
[26] Tiago P Peixoto. 2014. Efficient Monte Carlo and greedy heuristic for the inference
of stochastic block models. Physical Review E 89, 1 (2014), 012804.
[27] Chengbin Peng, Zhihua Zhang, Ka-Chun Wong, Xiangliang Zhang, and David
Keyes. 2015. A scalable community detection algorithm for large graphs using
stochastic block models. In IJCAI. 2090–2096.
[28] Meng Qin, Di Jin, Kai Lei, Bogdan Gabrys, and Katarzyna Musial-Gabrys. 2018.
Adaptive community detection incorporating topology and content in social
networks. Knowledge-Based Systems 161 (2018), 342–356.
[29] Meng Qin and Kai Lei. 2021. Dual-channel hybrid community detection in
attributed networks. Information Sciences 551 (2021), 146–167.
[30] Meng Qin, Kai Lei, Bo Bai, and Gong Zhang. 2019. Towards a profiling view
for unsupervised traffic classification by exploring the statistic features and link
patterns. In SIGCOMM NetAI Workshop. 50–56.
[31] Meng Qin and Dit-Yan Yeung. 2023. Temporal link prediction: A unified frame-
work, taxonomy, and review. CSUR 56, 4 (2023), 1–40.[32] Meng Qin, Chaorui Zhang, Bo Bai, Gong Zhang, and Dit-Yan Yeung. 2023. High-
quality temporal link prediction for weighted dynamic graphs via inductive
embedding aggregation. TKDE (2023).
[33] Meng Qin, Chaorui Zhang, Bo Bai, Gong Zhang, and Dit-Yan Yeung. 2023. To-
wards a better trade-off between quality and efficiency of community detection:
An inductive embedding method across graphs. TKDD (2023).
[34] Tai Qin and Karl Rohe. 2013. Regularized spectral clustering under the degree-
corrected stochastic blockmodel. NIPS, 3120–3128.
[35] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,
Kuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph
neural network pre-training. In SIGKDD. 1150–1160.
[36] Usha Nandini Raghavan, Réka Albert, and Soundar Kumara. 2007. Near linear
time algorithm to detect community structures in large-scale networks. Physical
Review E 76, 3 (2007), 036106.
[37] Jörg Reichardt and Stefan Bornholdt. 2006. Statistical mechanics of community
detection. Physical Review E 74, 1 (2006), 016110.
[38] Yu Rong, Tingyang Xu, Junzhou Huang, Wenbing Huang, Hong Cheng, Yao Ma,
Yiqi Wang, Tyler Derr, Lingfei Wu, and Tengfei Ma. 2020. Deep graph learning:
Foundations, advances and applications. In SIGKDD. 3555–3556.
[39] Giulio Rossetti and Rémy Cazabet. 2018. Community discovery in dynamic
networks: a survey. CSUR 51, 2 (2018), 1–37.
[40] Martin Rosvall and Carl T Bergstrom. 2008. Maps of random walks on complex
networks reveal community structure. PNAS 105, 4 (2008), 1118–1123.
[41] Erich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu.
2017. DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
TODS 42, 3 (2017), 1–21.
[42] Chris Stark, Bobby-Joe Breitkreutz, Teresa Reguly, Lorrie Boucher, Ashton Bre-
itkreutz, and Mike Tyers. 2006. BioGRID: a general repository for interaction
datasets. Nucleic Acids Research 34, suppl_1 (2006), D535–D539.
[43] Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. 2022. Gppt:
Graph pre-training and prompt tuning to generalize graph neural networks. In
SIGKDD. 1717–1727.
[44] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. In SIGKDD. 2120–2131.
[45] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. 2023.
Graph Prompt Learning: A Comprehensive Survey and Beyond. arXiv:2311.16534
(2023).
[46] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Müller. 2023.
Graph clustering with graph neural networks. JMLR 24, 127 (2023), 1–21.
[47] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong,
and Anshul Kanakia. 2020. Microsoft academic graph: When experts are not
enough. Quantitative Science Studies 1, 1 (2020), 396–413.
[48] Po-Wei Wang and J Zico Kolter. 2020. Community detection using fast low-
cardinality semidefinite programming. NIPS 33 (2020), 3374–3385.
[49] Bryan Wilder, Eric Ewing, Bistra Dilkina, and Milind Tambe. 2019. End to end
learning and optimization on graphs. NIPS 32 (2019).
[50] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z Li. 2021. Self-
supervised learning on graphs: Contrastive, generative, or predictive. TKDE
(2021).
[51] Yuyang Xie, Yuxiao Dong, Jiezhong Qiu, Wenjian Yu, Xu Feng, and Jie Tang. 2023.
SketchNE: Embedding Billion-Scale Networks Accurately in One Hour. TKDE
(2023).
[52] Su Xing, Xue Shan, Liu Fanzhen, Wu Jia, Yang Jian, Zhou Chuan, Hu Wenbin, Paris
Cecile, Nepal Surya, Jin Di, et al .2022. A comprehensive survey on community
detection with deep learning. TNNLS (2022), 1–21.
[53] Jaewon Yang and Jure Leskovec. 2012. Defining and evaluating network commu-
nities based on ground-truth. In SIGKDD. 1–8.
[54] Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong
Zhang. 2016. Modularity based community detection with deep learning. In IJCAI,
Vol. 16. 2252–2258.
[55] Ziwei Zhang, Peng Cui, Haoyang Li, Xiao Wang, and Wenwu Zhu. 2018. Billion-
scale network embedding with iterative random projection. In ICDM. 787–796.
[56] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep learning on graphs: A
survey. TKDE 34, 1 (2020), 249–270.
[57] Zhili Zhao, Zhengyou Ke, Zhuoyue Gou, Hao Guo, Kunyuan Jiang, and Ruisheng
Zhang. 2022. The trade-off between topology and content in community detec-
tion: An adaptive encoder–decoder-based NMF approach. Expert Systems with
Applications 209 (2022), 118230.
A GENERATION OF PRE-TRAINING DATA
Our settings of the DC-SBM generator are summarized in Table 8,
where ‘I (𝑎,𝑏)’ and ‘F (𝑎,𝑏)’ denote an integer and a float uniformly
sampled from the range [𝑎,𝑏];𝑇is the number of synthetic graphs;
𝑁and𝐾are the numbers of nodes and communities in each graph;
 
2476Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 8: Parameter settings of the synthetic graph generator.
𝑇𝑁 𝐾 degmin degmax 𝛾𝜇𝜌
1e3 I(2e3, 5e3) I(2, 1e3) min{5,⌈𝑁/(4𝐾)⌉} min{5e2,⌈𝑁/𝐾⌉}F(2, 3.5) F(2.5, 5) F(1, 3)
Table 9: Statistics of the generated synthetic pre-training graphs.
Min Max Avg 𝑁 Min Max Avg 𝐸 Min Max Avg 𝐾 Avgdegmin Avgdegmax Min Max Avg Density
2,000 5,000 3,495.1 2,049 66,355 12,115.3 2 968 485.3 1.3 20.8 4e-4 1e-2 2e-3
Algorithm 4: Generating a Synthetic Graph via DC-SBM
Input: number of nodes 𝑁𝑡; number of communities 𝐾𝑡; range
[degmin,degmax] for node degrees; 𝛾for degree distribution;
𝜇for numbers of within-community edges and
between-community edges; 𝜌for community size
distribution
Output: a synthetic graphG𝑡=(V𝑡,E𝑡)
1generate node setV𝑡={𝑣1,···,𝑣𝑁𝑡}
2for𝑟from 1to𝐾𝑡do
3 get size of the 𝑟-th community|C(𝑡)
𝑟|∼Dirichlet(10/𝜌)
4 initializeC(𝑡)
𝑟’s degree sum 𝜑𝑟←0
5initialize total degree sum ¯𝜑←0
6for each node𝑣𝑖∈V𝑡do
7 get𝑣𝑖’s community label 𝑐𝑖∼Multi(|C(𝑡)
1|,···,|C(𝑡)
𝐾𝑡|)
8 add𝑣𝑖into corresponding community C𝑐𝑖
9 get𝑣𝑖’s degree
deg(𝑣𝑖)∼{𝑘𝛾/degmaxÍ
𝑙=degmin𝑙𝛾|degmin≤𝑘≤degmax}
10 update degree sum 𝜑𝑐𝑖←𝜑𝑐𝑖+deg(𝑣𝑖)
11 update total degree sum ¯𝜑←¯𝜑+deg(𝑣𝑖)
12for each node𝑣𝑖∈V𝑡do
13 get𝑣𝑖’s degree correction value 𝜃𝑖←deg(𝑣𝑖)/𝜑𝑐𝑖
14for𝑟from 1to𝐾𝑡do
15 for𝑡from𝑟to𝐾𝑡do
16 if𝑟=𝑡then
17 Ωrr←𝜇
1+𝜇·𝜑𝑟
18 else
19 Ωrt←1
1+𝜇·𝜑𝑟𝜑𝑡
¯𝜑
20for𝑖from 2to𝑁𝑡do
21 for𝑗from 1to𝑖do
22 generate an edge(𝑣𝑖,𝑣𝑗)forE𝑡viaPoisson(𝜃𝑖𝜃𝑗Ω𝑐𝑖𝑐𝑗)
degminanddegmaxare the minimum and maximum node degrees
of a graph;𝛾is a parameter to control the power-law distributions
of node degrees with deg(𝑣𝑖)∼{𝑘𝛾/Ídegmax
𝑙=degmin𝑙𝛾}(degmin≤𝑘≤
degmax);𝜇is a ratio between the number of within-community
and between-community edges; 𝜌is a parameter to control the
heterogeneity of community size, with the size of each community
following Dirichlet(10/𝜌).
For each synthetic graph G𝑡, suppose there are 𝑁𝑡nodes par-
titioned into 𝐾𝑡communities, with 𝑁𝑡and𝐾𝑡sampled from the
corresponding distributions in Table 8. Let C(𝑡)={C(𝑡)
1,···,C(𝑡)
𝐾𝑡}
denote the community ground-truth of G𝑡. We also sample(𝛾,𝜇,𝜌)
from corresponding distributions in Table 8, which define the topol-
ogy properties ofG𝑡. Algorithm 4 summarizes the procedure of gen-
erating a graphG𝑡via DC-SBM, based on the parameter settings of{𝑁𝑡,𝐾𝑡,degmin,degmax,𝛾,𝜇,𝜌}. Concretely, DC-SBM generates an
edge(𝑣𝑖,𝑣𝑗)in each graphG𝑡viaA(𝑡)
𝑖𝑗∼Poisson(𝜃𝑖𝜃𝑗Ω𝑐𝑖𝑐𝑗), where
𝑐𝑖is the community label of node 𝑣𝑖;𝜃∈R𝑁𝑡+andΩ∈R𝐾𝑡×𝐾𝑡+
are the degree correction vector and block interaction matrix [15]
determined by{𝛾,𝜇,𝜌};𝜃𝑖𝜃𝑗Ω𝑐𝑖𝑐𝑗represents the expected number
of edges between 𝑣𝑖and𝑣𝑗.
Statistics of the generated synthetic graphs (used for offline pre-
training in our experiments) are reported in Table 9.
B DETAILED COMPLEXITY ANALYSIS
For each graphG′, let𝑁,𝑀, and𝑑be the number of nodes, number
of edges, and embedding dimensionality, respectively. By using
the efficient sparse-dense matrix multiplication, the complexity of
the feature extraction defined in (3) is O(𝑀𝑑+𝑁𝑑2). Similarly, the
complexity of one FFP of the embedding encoder described by (4)
is no more thanO(𝑀𝑑+𝑁𝑑2).
To derive a feasible CD result via Algorithm 1, the complexities
of constructing the node pair set P, one FFP of the binary classifier
to derive auxiliary graph ˜G, and extracting connected components
via DFS/BFS areO(𝑀+𝑛𝑆),O((𝑀+𝑛𝑆)𝑑2), andO(𝑁+˜𝑀), where
𝑛𝑆is the number of randomly sampled node pairs; ˜𝑀:=|˜E|is the
number of edges in the auxiliary graph ˜G.
In summary, the complexity of online generalization isO((𝑀𝑑+
𝑁𝑑2)+(𝑀𝑑+𝑁𝑑2)+(𝑀+𝑛𝑆)+(𝑀+𝑛𝑆)𝑑2+(𝑁+˜𝑀))=O((𝑁+𝑀)𝑑2),
where we assume that ˜𝑀≈𝑀, and𝑛𝑆≪𝑀.
C DETAILED EXPERIMENT SETUP
C.1 Datasets
In the six real datasets (see Table 2), Protein2[42] was collected
based on the protein-protein interactions in the BioGRID repository.
ArXiv3[47] and DBLP4[53] are public paper citation and collabora-
tion graphs extracted from ArXiv and DBLP, respectively. Amazon5
[53] was collected by crawling the product co-purchasing relations
from Amazon, while Youtube6[53] was constructed based on the
friendship relations of Youtube. RoadCA7[18] describes a road
network in California. To pre-process Protein, we abstracted each
protein as a unique node and constructed graph topology based
on corresponding protein-protein interactions. Since there are mul-
tiple connected components in the extracted graph, we extracted
the largest component for evaluation. Moreover, we used original
formats of the remaining datasets provided by their sources.
2https://downloads.thebiogrid.org/BioGRID
3https://ogb.stanford.edu/docs/nodeprop/
4https://snap.stanford.edu/data/com-DBLP.html
5https://snap.stanford.edu/data/com-Amazon.html
6https://snap.stanford.edu/data/com-Youtube.html
7https://snap.stanford.edu/data/roadNet-CA.html
 
2477KDD ’24, August 25–29, 2024, Barcelona, Spain Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, & Dit-Yan Yeung
Table 10: Detailed parameter settings of PRoCD.
DatasetsParameter Settings Layer Configurations
(𝛼,𝜆) (𝑛𝑃,𝜂)𝑛𝑆𝑑𝐿Feat𝐿GNN𝐿BC
Protein (0.1, 1) (20, 1e-4) 1e3
64 24 2
ArXiv (1, 100) (100, 1e-4) 1e4 2 6
DBLP (0.1, 10) (100, 1e-4) 1e4 4 2
Amazon (0.1, 10) (100, 1e-4) 2e4 4 2
Youtube (1e2, 1e2) (89, 1e-4) 1e4 4 2
RoadCA (1e2, 10) (11, 1e-4) 1e4 2 2
0 1e2 5e2 1e3 5e3 1e4 5e4
nS0.60.620.64Mod↑
(a) DBLP, w/ LPA
0 1e2 5e2 1e3 5e3 1e4 5e4
nS0.70.720.74Mod↑ (b) Amazon, w/ LPA
0 1e2 5e2 1e3 5e3 1e4 5e4
nS0.80.820.84Mod↑
(c) DBLP, w/ InfoMap
0 1e2 5e2 1e3 5e3 1e4 5e4
nS00.20.40.60.8Mod↑ (d) Amazon, w/ InfoMap
0 1e2 5e2 1e3 5e3 1e4 5e4
nS0.80.820.84Mod↑
(e) DBLP, w/ Locale
0 1e2 5e2 1e3 5e3 1e4 5e4
nS0.90.920.94Mod↑ (f) Amazon, w/ Locale
Figure 6: Parameter analysis w.r.t. the number of sampled
node pairs 𝑛𝑆on DBLP and Amazon in terms of modularity ↑.
C.2 Environments & Implementation Details
All the experiments were conducted on a server with one AMD
EPYC 7742 64-Core CPU, 512GB main memory, and one 80GB mem-
ory GPU. We used Python 3.7 to implement PRoCD, where the fea-
ture extraction module (3), embedding encoder (4), and binary node
pair classifier (6) were implemented via PyTorch 1.10.0. Hence, the
feature extraction and FFP of PRoCD can be speeded up via the GPU.
The efficient function ‘scipy.sparse.csgraph.connected_components’
was used to extract connected components in Algorithm 1 to derive
a feasible CD result. For all the baselines, we adopted their official
implementations and tuned parameters to report the best quality.
To ensure the fairness of comparison, the embedding dimensional-
ity of all the embedding-based methods (i.e., LouvainNE, SketchNE,
RaftGP, ICD, and PRoCD) was set to be the same (i.e., 64).
C.3 Parameter Settings & Layer Configurations
The recommended parameter settings and layer configurations
of PRoCD are depicted in Table 10, where 𝛼and𝜆are hyper-
parameters in the pre-training objective (9); 𝑛𝑃and𝜂are the number
of epochs and learning rate of pre-training in Algorithm 2; 𝑛𝑆is
the number of randomly sampled node pairs in Algorithm 1 for
inference;𝑑is the embedding dimensionality; 𝐿Feat,𝐿GNN, and𝐿BC
are the numbers of MLP layers in (3), GNN layers in (4), and MLP
layers in (6). Given an MLP, let u[𝑠−1]andu[𝑠]be the input and out-
put of the𝑠-th perceptron layer. The MLP in the feature extraction
module (3) is defined as
u[𝑠]=tanh(u[𝑠−1]W[𝑠]+b[𝑠]), (10)with{W[𝑠]∈R𝑑×𝑑,b[𝑠]∈R𝑑}as trainable parameters. For the
MLPℎ𝑠(·)(orℎ𝑑(·)) in the binary classifier (6), suppose there are 𝐿
layers. The 𝑠-th layer (𝑠<𝐿) and the last layer of ℎ𝑠(·)(orℎ𝑑(·))
are defined as
u[𝑠]=tanh(u[𝑠−1]W[𝑠]+b[𝑠])+u[𝑠−1], (11)
u[𝐿]=ReLU(u[𝐿−1]W[𝐿]+b[𝐿]), (12)
where there is a skip connection in each perceptron layer except
the last layer.
D FURTHER EXPERIMENT RESULTS
We also conducted parameter analysis for the number of sampled
node pairs (in Algorithm 1) for inference, where we set 𝑛𝑆∈
{0,1𝑒2,5𝑒2,1𝑒3,5𝑒3,1𝑒4,5𝑒4}. Results on DBLP and Amazon in
terms of modularity are shown in Fig. 6. In most cases except the
variant with InfoMap on Amazon, our PRoCD method is not sen-
sitive to the setting of 𝑛𝑆. To ensure the inference quality of the
exception case, one needs a large setting of 𝑛𝑆(e.g.,𝑛𝑆≥1𝑒4).
E FUTURE DIRECTIONS
Some possible future directions are summarized as follows.
Theoretical Analysis. This study empirically verified the po-
tential of PRoCD to achieve a better trade-off between the quality
and efficiency of 𝐾-agnostic. However, most existing graph pre-
training techniques lack theoretical guarantee for their transfer
ability w.r.t. different pre-training data and downstream tasks. We
plan to theoretically analyze the transfer ability of PRoCD from
small synthetic pre-training graphs to large real graphs, following
previous analysis on random graph models (e.g., SBM [14, 34]).
Extension to Dynamic Graphs. In this study, we considered
𝐾-agnostic CD on static graphs. CD on dynamic graphs [ 39] is a
more challenging setting, involving the variation of nodes, edges,
and community membership over time. Usually, one can formulate
a dynamic graph as a sequence of static snapshots with similar
underlying properties [ 17,31,32]. PRoCD can be easily extended to
handle dynamic CD by directly generalizing the pre-trained model
to each snapshot for fast online inference, without any retraining.
Further validation of this extension is also our next focus.
Integration of Graph Attributes. As stated in Section 3, we
considered CD without available graph attributes. A series of pre-
vious studies [ 7,25,28,29,57] have demonstrated the complicated
correlations between graph topology and attributes, which are in-
herently heterogeneous information sources, for CD. On the one
hand, the integration of attributes may provide complementary
characteristics for better CD quality. On the other hand, it may also
incorporate noise or inconsistent features that lead to unexpected qual-
ity decline compared with those only considering one source. In our
future work, we will explore the adaptive integration of attributes for
PRoCD. Concretely, when attributes match well with topology, we
expect that PRoCD can fully utilize the complementary information
of attributes to improve CD quality. When the two sources mismatch
with one another, we try to adaptively control the contribution of
attributes to avoid quality decline.
 
2478