Urban Foundation Models: A Survey
Weijia Zhang
HKUST(GZ)
Guangzhou, China
wzhang411@connect.hkust-
gz.edu.cnJindong Han
HKUST
Hong Kong, China
jhanao@connect.ust.hkZhao Xu
HKUST(GZ)
Guangzhou, China
zxu674@connect.hkust-gz.edu.cn
Hang Ni
HKUST(GZ)
Guangzhou, China
hni017@connect.hkust-gz.edu.cnHao Liu∗
HKUST(GZ) & HKUST
Guangzhou, China
liuh@ust.hkHui Xiong
HKUST(GZ) & HKUST
Guangzhou, China
xionghui@ust.hk
ABSTRACT
Machine learning techniques are now integral to the advancement
of intelligent urban services, playing a crucial role in elevating the
efficiency, sustainability, and livability of urban environments. The
recent emergence of foundation models such as ChatGPT marks a
revolutionary shift in the fields of machine learning and artificial in-
telligence. Their unparalleled capabilities in contextual understand-
ing, problem solving, and adaptability across a wide range of tasks
suggest that integrating these models into urban domains could
have a transformative impact on the development of smart cities.
Despite growing interest in Urban Foundation Models (UFMs), this
burgeoning field faces challenges such as a lack of clear definitions
and systematic reviews. To this end, this paper first introduces the
concept of UFMs and discusses the unique challenges involved in
building them. We then propose a data-centric taxonomy that cate-
gorizes and clarifies current UFM-related works, based on urban
data modalities and types. Furthermore, we explore the application
landscape of UFMs, detailing their potential impact in various urban
contexts. Relevant papers and open-source resources have been
collated and are continuously updated at: https://github.com/usail-
hkust/Awesome-Urban-Foundation-Models.
CCS CONCEPTS
•Information systems →Spatial-temporal systems; •Com-
puting methodologies →Neural networks.
KEYWORDS
Urban foundation models; urban general intelligence; geospatial
artificial intelligence; spatio-temporal data mining
ACM Reference Format:
Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, and Hui Xiong.
2024. Urban Foundation Models: A Survey. In Proceedings of the 30th ACM
∗Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671453SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671453
1 INTRODUCTION
Machine learning technologies have become pivotal in transform-
ing urban landscapes, underpinning the development of various
smart city services. These technologies enhance urban intelligence
by enabling more efficient resource allocation, improved public ser-
vices, and elevated quality of life for city dwellers [ 106]. Through
analyzing vast datasets from diverse sources such as satellites, urban
sensors, and social media platforms, machine learning algorithms
identify intricate urban patterns and facilitate accurate forecasting
of city dynamics [ 38,137]. Such analytical power proves essential in
boosting urban operational efficiencies, such as smart energy man-
agement, traffic flow optimization, and environmental monitoring,
all contributing to smarter and more sustainable cities [56, 89].
The recent advent of foundation models, such as Large Lan-
guage Models (LLMs, e.g., ChatGPT) and Vision Foundation Models
(VFM), has significantly reshaped the research landscape in ma-
chine learning and artificial intelligence [ 151]. These models are
characterized by their extensive pre-training on large-scale datasets,
which imbues them with unparalleled emergent abilities, includ-
ing contextual reasoning, complex problem solving, and zero-shot
adaptability across diverse tasks [ 8,54]. Such abilities make founda-
tion models particularly suitable for interacting with dynamic and
multifaceted urban environments, leading towards more integrated,
intelligent, and responsive urban systems.
Urban Foundation Models (UFMs), as depicted in Figure 1, rep-
resent a novel family of models pre-trained on extensive, diverse
urban data sources, encompassing multiple data granularity and
modalities. These models exhibit a deep understanding of various
urban data types and remarkable adaptability to a wide array of
urban tasks [ 4], significantly contributing towards the ultimate re-
alization of Urban General Intelligence (UGI). By integrating and in-
terpreting diverse urban data types, UFMs can offer comprehensive
insights, uncover intricate spatiotemporal patterns, and enhance
decision-making across various urban tasks. For instance, UFMs
can analyze a combination of human instructions, scene imagery,
traffic sensor data, and GPS trajectories to optimize traffic efficiency
and safety [ 20,57,136,149]. Additionally, UFMs can collectively
utilize demographic, land use, and environmental data to provide
crucial insights for sustainable city development [79, 110, 152].
6633
KDD ’24, August 25–29, 2024, Barcelona, Spain. Weijia Zhang et al.
Multi-source Multi-granularity Multimodal Urban DataUrbanFoundationModel
Street View Images
UrbanTextCorpus
Geo-sensory Time Series
SpatialTrajectory
UrbanKnowledgeGraphDownstreamApplications
Transportation
Urban Planning
Environmental Monitoring
Public Safety and Security
Energy Management
Pre-trainAdaptation
Figure 1: Urban Foundation Models (UFMs) are pre-trained on multi-source, multi-granularity and multimodal urban data and
can be adapted for a variety of downstream urban applications.
Despite the burgeoning interest and potential of UFMs, the field
faces several challenges, including the absence of clear definitions
and the lack of systematic reviews of existing literature [ 4,46,81,
122,123,136]. This paper addresses these gaps by presenting a
comprehensive survey on UFMs. We start by defining UFMs and
discussing their unique challenges. Building upon this, we propose
a data-centric taxonomy designed to categorize and encapsulate
existing research on UFMs. This taxonomy, rooted in the diverse
modalities and types of urban data, aims to highlight the progress
and concerted efforts made in this emerging domain. In addition, we
also explore the application landscape of UFMs, detailing how they
can be applied to enhance different aspects of urban intelligence.
The major contributions of this paper are as follows: (1) We
present the first comprehensive and systematic review of Urban
Foundation Models, offering a foundational perspective on this
evolving field. (2) This work presents the definition of UFMs and
delves into the specific challenges associated with their develop-
ment, shedding light on unexplored aspects. (3) We propose a data-
centric taxonomy for UFMs, which categorizes and clarifies existing
research based on urban data modalities and types, aiding in the
understanding of the field’s current state and future directions.
2 CONCEPTS OF UFMS
UFMs are characterized by their remarkable capacities for un-
derstanding, reasoning, and adaptability on multi-source, multi-
granularity, and multimodal urban data. We first introduce impor-
tant basic concepts and then give the formal definition for UFMs.
-Multi-source. UFMs are required to integrate large-scale data
from diverse urban sources, including sensor networks, satellites,
mobile devices, etc., facilitating a comprehensive understanding of
urban environments.
-Multi-granularity. UFMs are required to handle data at differ-
ent levels of granularity. At a macro level, they process city-wide
patterns like traffic flow and population movement. At a micro
level, they need to analyze local trends and behaviors in specific
neighborhoods, streets, and individuals.
-Multimodal. UFMs are also required to integrate diverse data
modalities, such as textual, visual, and sensor-based inputs, enabling
them to adapt to various downstream urban applications effectively.
Definition 1 (Urban Foundation Models). Urban Foun-
dation Models (UFMs) are a family of large-scale models pre-trained
on vast amounts of multi-source, multi-granularity, and multimodal
urban data. They acquire notable general-purpose capabilities inthe pre-training phase, exhibiting remarkable emergent abilities and
adaptability dedicated to a range of urban application domains, such
as transportation, urban planning, energy management, environmen-
tal monitoring, and public safety and security.
3 CHALLENGES OF BUILDING UFMS
While UFMs offer significant potential for understanding and man-
aging complex urban environments, building UFMs is a complex
task that encounters several unique challenges. These challenges
stem primarily from the inherent complexities of urban data, dy-
namic nature of urban environments, diverse urban task domains,
and concerns on privacy and security.
-Multi-source, multi-granularity, and multi-modal data in-
tegration. The integration of multi-source, multi-granularity, and
multi-modal data is one of the primary challenges in building UFMs.
UFMs necessitate effectively integrating data from various sources,
which provide data at different levels of granularity, from broad
city-wide patterns to specific local details. Moreover, the data modal-
ities vary significantly, encompassing text, images, sensor readings,
etc.Integrating heterogeneous data types presents significant chal-
lenges in data preprocessing, normalization, and fusion.
-Spatio-temporal reasoning capability. Another major challenge
in UFMs is mastering spatio-temporal reasoning. Urban environ-
ments are dynamic, with changes and patterns evolving over both
space and time. UFMs need to understand and predict complex
phenomena that are both time-dependent and spatial-dependent.
This involves sophisticated modeling of temporal sequences and
spatial distributions, requiring advanced algorithms capable of han-
dling high-dimensional data. This spatio-temporal reasoning skill
is particularly vital in urban contexts and is often less emphasized
or absent in foundation models from other domains.
-Versatility to diverse urban task domains. The versatility of
UFMs to diverse urban task domains is another significant chal-
lenge. Urban environments are inherently complex and encompass
various domains, such as transportation, energy management, and
environmental monitoring. While each domain presents unique
challenges and requirements, UFMs need to be versatile enough to
adapt to these varied contexts. This involves not only tailoring the
models to specific domain tasks but also ensuring that they can be
effectively generalized across different urban scenarios.
-Privacy and security concerns. Deploying UFMs introduces sig-
nificant privacy and security challenges, stemming from their use
of comprehensive urban data from diverse sources, like personal
6634Urban Foundation Models: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain.
Figure 2: A data-centric taxonomy for existing UFMs-related
works based on the types of urban data modalities.
devices and social networks. Adherence to privacy laws, such as
the General Data Protection Regulation (GDPR) [ 109], is essential,
necessitating strong data anonymization, clear consent for data
use, and transparent processes to protect data privacy. Additionally,
UFMs must ensure data integrity and model security to defense
against malicious activities, such as data privacy breaches, data poi-
soning, jailbreak attacks, adversarial attacks. Given their substantial
societal impact, addressing these concerns effectively is crucial for
developing UFMs that are trustworthy and ethically sound.
4 OVERVIEW OF UFMS
As illustrated in Figure 2, we present a data-centric taxonomy for
the UFMs-related studies based on urban data modalities that UFMs
process. We will introduce these studies through the lens of their
focused pre-training and adaptation techniques.
4.1 Language-based Models
Pre-trained Language Models (PLMs), which typically leverage
Transformer models as the backbone and are pre-trained over large-
scale unlabeled corpora, have demonstrated strong capabilities in
various Natural Language Processing (NLP) tasks [ 147]. Recently,
researchers have found that increasing the parameter scale of PLMs
to a significant size (e.g., tens of billions of parameters) not only
enhances their capacity but also leads to surprising emergent abili-
ties (e.g., in-context learning, reasoning), which are not present in
smaller PLMs (e.g., BERT [ 21]). These enlarged PLMs (e.g., Chat-
GPT) are also termed LLMs in literature [ 151]. Given people living
in cities have generated an extensive corpus of textual data, such
as documents (e.g., traffic reports), dialogs (e.g., ride-hailing conver-
sations), and geo-texts (e.g., geo-tagged tweets), it is of great value
to explore the application of PLMs in the urban context.
4.1.1 Unimodal Pre-training .Recently, a few studies [ 23,45]
focus on training PLMs by only using domain-specific data, outper-
forming generalist counterparts on tasks within those domains. Forexample, ERNIE-GeoL [ 45] is a pre-trained language model ded-
icated to improving geography-related tasks at Baidu Maps. The
model is pre-trained on massive textual data extracted from a hetero-
geneous graph that contains rich toponym and spatial knowledge,
by using masked language modeling and geocoding tasks. Ding et
al. [23] propose MGeo, a language model specifically pre-trained on
abundant geographic context data for query-POI matching [ 135].
MGeo designs a geographical encoder to encode geographic con-
text representations. It also utilizes masked language modeling and
contrastive learning in pre-training stage. However, large-scale
pre-training requires substantial computational resources and elec-
tricity, which may be difficult for researchers to obtain.
4.1.2 Unimodal Adaptation. Instead of training urban language
models from scratch, previous studies mainly focus on adapting
existing PLMs to urban scenarios. In this way, we can make full use
of the world knowledge encapsulated in PLMs while significantly
reducing the demanded computational cost. Specifically, there are
two categories of adaptation approaches: prompt engineering and
model fine-tuning.
-Prompt engineering. Prompting aims to steer PLM’s behavior
toward desired outcomes via pre-defined text inputs, i.e., task de-
scription or a set of demonstrations, which allows PLMs to handle
tasks they have never explicitly been trained for. Prompting offers a
flexible, efficient, and user-friendly way to harness the capabilities
of PLMs without the necessity of retraining or fine-tuning the model.
Several preliminary studies [ 5,34,47] suggest that PLMs have en-
coded a wealth of urban factual and spatio-temporal knowledge
from their training corpora. These knowledge can be effectively
queried by prompting techniques. For instance, Prabin et al. [ 5]
investigate a set of geospatial skills of LLMs, including geospatial
knowledge, awareness, and reasoning. By leveraging appropriate
prompts, LLMs can utilize the encoded geospatial knowledge to
benefit diverse tasks, such as deriving coordinates of cities and
understanding urban entity relationships [88].
Building upon the above findings, recent studies have explored
the capabilities of LLMs in a variety of urban applications, such
as urban science research [ 28], travel planning [ 93,120,121], and
urban question answering [ 86]. To name a few, Aghzal et al. [ 2]
systematically evaluate the reasoning abilities of LLMs in path
planning tasks through different few-shot prompting strategies. Ge-
oLLM [ 83] focuses on extracting geospatial knowledge from LLMs
by using prompts augmented with auxiliary map data, achieving
superior performance in real-world geospatial prediction tasks. Mai
et al. [ 80,81] demonstrate that LLMs can act as zero-shot or few-
shot urban task learners via prompting and outperform tailored
models in toponym recognition, country-level time series forecast-
ing, and urban function classification. In addition, Zheng et al. [ 149]
explore the potential of LLM prompting in various traffic safety
applications, including accident reports automation, traffic data
augmentation, and multisensory data analysis.
While direct prompting is flexible and resource-efficient, it de-
mands powerful, generalist LLMs and rich contextual information
to achieve satisfactory performance, which may restrict the appli-
cability of LLMs in complex real-world applications.
-Model fine-tuning. Generalist PLMs, pre-trained on vast amounts
of Web textual data, might lack sufficient knowledge for specific
6635KDD ’24, August 25–29, 2024, Barcelona, Spain. Weijia Zhang et al.
tasks or domains. Some studies focus on fine-tuning PLMs with
domain-specific data to enhance model performance towards par-
ticular areas. SpaBERT [ 61] extends BERT to capture the spatial
varying semantics of geo-entities by fine-tuning synthetic sentences
from geographic databases for geo-entity comprehension. It also
introduces a spatial coordinate embedding module to preserve spa-
tial relationships of geo-entities in latent feature space. GeoLM [ 64]
proposes to jointly learn linguistic and geospatial context via con-
trastive learning and masked language modeling. Compared with
SpaBERT, GeoLM can effectively retain general linguistic informa-
tion. Mei et al. [ 84] introduce a fine-tuned BERT model to enhance
the query intent understanding in POI retrieval tasks. The model is
initialized from the pre-trained weights of BERT, and fine-tuned
based on the knowledge from both the user behavior logs and the
POI corpus. More recently, there have been efforts to fine-tune open-
source LLMs for urban applications. For example, PlanGPT [ 153] is
proposed to improve the comprehension and reasoning capabilities
of LLMs in urban and spatial planning domain. In particular, it
utilizes a two-stage fine-tuning approach to inject interdisciplinary
urban knowledge into LLM and enhance its ability to adeptly pro-
cess government documents.
Despite fruitful progress, these efforts mainly focus on pre-
training language models or leveraging existing PLMs for specific
urban applications (e.g., map services). PLMs specialized for a broad
range of urban tasks and domains have not been reported in the
literature, which could be a potential future direction.
4.2 Vision-based Models
Existing Vision Foundation Models (VFMs) mainly fall into two cate-
gories: language-augmented VFMs and vision-only VFMs. Language-
augmented models, such as CLIP [ 91], aim to learn an image encoder
by aligning paired text and images with contrastive learning. Con-
versely, vision-only VFMs are trained on purely visual content data
through self-supervised pre-training, such as masked autoencoder
(MAE) [ 41] and LVM [ 3]. These emerging VFMs, pre-trained on
general visual data, exhibit robust generalization and impressive
zero-shot learning abilities across a wide range of computer vi-
sion tasks. However, since the distributions of urban visual data
(e.g., satellite images) are significantly different from general visual
data [ 46], existing VFMs might not be suitable for handling urban-
centric tasks. Consequently, building tailored urban VFMs becomes
highly necessary and practical.
4.2.1 Unimodal Pre-training. With the widespread deployment
of camera and satellite technologies, a vast volume of visual data
has been collected in urban spaces. The massive availability of
data enables a series of research to train large urban vision models
from scratch. We classify existing models based on the type of pre-
training data they utilize: on-site urban visual data, remote sensing
data, and grid-based meteorological data.
-On-site urban visual data. On-site urban visual data, such as
street-view imagery [ 7] and surveillance footage [ 9], are gener-
ated by ground-level devices (e.g., smartphones, cameras, and auto-
motive LiDARs) deployed in cities. Recently, several pre-training
methods [ 59,74,118] were proposed to learn representations from
unlabeled on-site urban visual data. For instance, Urban2Vec [ 118]
applies the Tobler’s First Law of Geography [ 85]—which positsthat “everything is related to everything else, but near things are
more related than distant things” —to develop a contrastive learning
framework. This framework learns representations of street-view
images by enforcing that spatially adjacent images are similar in
latent feature space. Moreover, KnowCL [ 74] devises a pre-training
framework by maximizing the mutual information between street-
view images and their corresponding knowledge graph.
-Remote sensing data. Remote Sensing (RS) data are typically
captured from satellites, aircraft, or drones. Existing RS foundation
models adopt the Masked Image Modeling (MIM) [ 41] paradigm,
which first encodes a masked RS image with an image encoder, and
then decodes the unmasked portion to reconstruct the entire image.
Following this, Wang et al. [ 112] train a plain Vision Transformer
(ViT) with 100 million parameters to solve various RS tasks, rang-
ing from detecting urban objects to identifying region functions.
Notably, a rotated varied-size window attention is proposed to re-
place the standard attention block in ViT, significantly reducing
the computation cost for high-resolution RS images. Cha et al. [ 10]
further examine the impact of scaling up ViT to one billion param-
eters in the RS domain. Additionally, RingMo [ 101] leverages Swin
Transformer [ 75], a well-known variant of ViT, as the model back-
bone, which can effectively capture dense and small objects that
are often overlooked in RS scenarios. RingMo-Sense [ 132] further
extends RingMo for RS spatiotemporal prediction tasks by leverag-
ing a multi-branch structure with different masking pre-training
strategies to learn multi-scale spatiotemporal representations.
-Grid-based meteorological data. Grid-based meteorological data
can be naturally represented as images, where pixel intensities char-
acterize the spatial distribution of meteorological variables, e.g., tem-
perature, humidity, and pressure. In recent years, the rapid develop-
ment of VFMs has been making an important impact on weather and
climate research [ 13]. As a pioneering study, Pangu-Weather [ 6] pro-
poses a 3D Earth-specific transformer for medium-range weather
forecasting, outperforming the world’s best Numerical Weather
Prediction (NWP) system on reanalysis data. Subsequently, Nguyen
et al. [ 87] present ClimaX, the first foundation model for climate
and weather analysis. It improves the vanilla ViT architecture with
decoupled variable encoding and aggregation strategies, which
enhance the model’s adaptability for heterogeneous atmosphere
variables. ClimaX also generalizes well to scenarios involving re-
gions and variables not encountered during pre-training. Different
from the above models trained in an autoregressive manner, W-
MAE [ 82] introduces the concept of MAE into the weather and
climate domain. This model is pre-trained by reconstructing the
spatial relations among meteorological variables, which can be
fine-tuned to benefit downstream forecasting tasks [139].
4.2.2 Unimodal Adaptation. Sometimes there is insufficient ur-
ban visual data to pre-train a large-scale VFM from scratch. Thus,
some studies directly adapt off-the-shelf VFMs, pre-trained on gen-
eral visual data, to urban tasks. The adaptation can be achieved
through prompt engineering and model fine-tuning techniques.
-Prompt engineering. Current research in this category mainly
focuses on leveraging the Segment Anything Model (SAM) [ 55] for
urban RS image segmentation. SAM is a foundation model tailored
to segmentation tasks, which consists of (1) a ViT-based image
encoder for generating image embeddings; (2) a prompt encoder
6636Urban Foundation Models: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain.
that processes user prompts (specify what to segment in an im-
age) to create embeddings; (3) a lightweight mask decoder that
combines image and prompt embeddings to predict segmentation
masks. Particularly, the user prompt can range from sparse (e.g.,
points, boxes, and texts) to dense (e.g., coarse-grained masks). For
example, Wang et al. [ 111] propose SAMRS for RS segmentation
by directly harnessing the zero-shot capabilities of SAM. SAMRS
manually designs six basic prompts based on the characteristics
of RS images, and leverages the optimal combination of prompts
identified in experiments for segmentation. RSPrompter [ 12] fur-
ther simplifies the process by automating prompt generation. It
generates appropriate prompts, such as point or box embeddings,
for SAM input by analyzing the hidden layers of the encoder.
-Model fine-tuning. In urban scenarios, model fine-tuning is uti-
lized to improve a pre-trained VFM for a certain capability, e.g., se-
mantic segmentation. For instance, GeoSAM [ 97] enhances SAM’s
performance for mobility infrastructure segmentation using a fine-
tuning strategy with the help of automatically generated visual
prompts. RingMo-SAM [ 127] adopts fine-tuning to update the
prompt encoder of SAM for multi-source RS segmentation. More-
over, StreetCLIP [ 35] fine-tunes CLIP with synthetic captions via
meta-learning for street-view image geolocalization tasks.
So far, existing studies mainly focus on relatively small datasets,
and the pre-training of VFMs on large-scale urban visual data has
not been well studied yet. On the other hand, the joint learning of
urban visual data and other data modalities may enhance the visual
comprehensive understanding of urban context, which is also an
interesting and beneficial direction for future research.
4.3 Trajectory-based Models
Trajectory data is an important modality type prevalently exist-
ing in location-based urban tasks, such as driving behavior analy-
sis [31,43,58,129] and human mobility analysis [ 27,48,66,67,70].
Formally, trajectory data is a specific type of geo-sensory data
that can be defined as a sequence of time-location points, 𝑇=
{(𝑡1,𝑝1),(𝑡2,𝑝2),...,(𝑡𝑛,𝑝𝑛)}, where for each point, 𝑡𝑖is the times-
tamp and𝑝𝑖is the corresponding location in space, often repre-
sented as geographic coordinates.
4.3.1 Unimodal Pre-training. Trajectory data typically requires
tailored modeling to suit specific applications [ 27,58]. Unimodal
pre-training, usually involving self-supervised learning on large
pure trajectory datasets, enables models to capture intrinsic features
and patterns of trajectory data, enhancing their versatility across
different applications. We introduce the pre-training methods on
two types of trajectory data: road network trajectories and free
space trajectories.
-Road network trajectory. Road network trajectory refers to tra-
jectories that are confined within road networks, e.g., the paths
of pedestrians or vehicles on streets and highways. Learning low-
dimensional embeddings of trajectories and paths is one of the most
crucial tasks in road network trajectory pre-training which can ben-
efit various downstream tasks. T2vec [ 60] and Traj2vec [ 131] repre-
sent early attempts at trajectory-based pre-training models, adopt-
ing the sequence-to-sequence [ 102] encoder-decoder framework
to learn trajectory representations. These models are pre-trainedby reconstructing trajectory sequences through a generative self-
supervised task. However, these methods process raw trajectories
as input, potentially compromising the quality of trajectory repre-
sentation due to noise. To mitigate this, Trembr [ 29] projects raw
trajectories onto road networks, using road segment sequences as
inputs. Furthermore, the decoder in Trembr is tailored to recon-
struct both road segments and their associated travel times. To
further refine trajectory representations, Toast [ 16] initially learns
road representations from the semantic information of road net-
works, subsequently generating trajectory representations using a
pre-trained bidirectional Transformer encoder. Unlike prior works,
LightPath [ 130] directly learns path representations for diverse
downstream tasks, applying a self-supervised relational reasoning
method to expedite the training of path encoders.
While the aforementioned methods utilize generative or con-
trastive approaches in pre-training, MMTEC [ 66] contends that
these methods introduce biases in trajectory embeddings. To miti-
gate this, MMTEC employs a unique pre-training task using max-
imum entropy coding to reduce biases in trajectory embeddings.
Additionally, some studies combine both generative and contrastive
pre-training approaches. HMTRL [ 70] is a route representation
learning framework tailored for multi-modal transportation recom-
mendations. The framework’s spatiotemporal pre-training strategy,
featuring masked attribute prediction and trajectory contrastive
learning, bolsters its generalization capabilities. START [ 48] also
introduces two self-supervised tasks for pre-training a trajectory
representation model: span-masked trajectory recovery and trajec-
tory contrastive learning.
-Free space trajectory. Free space trajectory refers to trajecto-
ries in open, unstructured spaces without the constraints of road
networks, such as pedestrian movements in parks or open areas.
A representative subset of this category is check-in trajectories,
where individuals record their location at specific POIs, often using
social media or location-based services [ 77]. Several approaches
have focused on simulating and understanding human mobility
in unstructured environments. CTLE [ 67] presents a pre-training
model inspired by BERT’s masked language model task, generating
location embeddings by considering contextual neighbors within
trajectories, effectively addressing the multi-functional properties
of locations. CACSR [ 31] is an adversarial contrastive model that
enhances learning of check-in sequence representations through a
contrastive learning-based pre-training task for its encoder.
4.3.2 Unimodal Adaptation. Unimodal adaptation for trajectory-
based foundation models refers to transferring the pre-trained mod-
els between different trajectory datasets. Current works primarily
focus on adapting a model trained on trajectories from one city to
another, through model fine-tuning. Reformd [ 32] employs stan-
dard transfer learning procedures, transferring model parameters
learned on a data-rich region to a target region with scarce data.
Axolotl [ 33] also focuses on cross-region model transfer to enhance
POI recommendations in data-scarce regions, and employ cluster-
based knowledge transfer without the need for any common users
or POIs between regions. Building on this idea, CATUS [ 98] ad-
dresses data scarcity and imbalance in POI recommendations across
cities by transferring category-level universal transition knowledge
through two self-supervised tasks in the pre-training phase.
6637KDD ’24, August 25–29, 2024, Barcelona, Spain. Weijia Zhang et al.
4.3.3 Cross-modal Adaptation. This line of research aims to
transfer learned patterns, features, or knowledge from other data
modalities to trajectory. Recent advancements in LLMs (e.g., GPT-
4 [1], Llama2 [ 105]) have demonstrated exceptional zero-shot learn-
ing and generalization capabilities across various domains. In the
realm of trajectory analysis, several studies have utilized LLMs for
trajectory-based tasks.
A number of research have investigated the use of LLMs in tack-
ling tasks related to human mobility by primarily employing prompt
engineering techniques. LLM-Mob [ 116] designs effective prompts
to enhance LLMs’ reasoning on mobility data, producing both accu-
rate and interpretable results on human mobility prediction tasks.
LLM-MPE [ 65] seeks to use LLMs for forecasting human mobility
in complex scenarios, such as public events, employing a chain-
of-thought prompting approach to base predictions on historical
mobility patterns and event descriptions. LLMob [ 113] presents an
LLM agent framework for personal mobility generation, involving
aligning LLM to urban mobility data by self-consistency approach
and interpreting the activity generation via retrieval augmentation.
Current trajectory pre-training mainly uses models with small
parameters, under-exploring the emergent abilities [ 119] of larger
models and data sizes like seen in LLMs, indicating opportunities for
discovering new abilities in managing complex trajectory data in
diverse urban contexts. Additionally, while cross-modal adaptation
often involves directly prompting LLMs to process trajectory data,
employing fine-tuning techniques such as prompt tuning and model
fine-tuning could lead to a more thorough adaptation of LLMs to
trajectory-related tasks [125].
4.4 Time Series-based Models
Time series data are ubiquitous due to the temporal characteristic
of urban environments, associated with diverse urban tasks such as
traffic forecasting [ 38,69,137,140], air quality forecasting [ 36,37,
39], energy consumption prediction [ 124]. In urban contexts, time
series (a.k.a. spatio-temporal data in some studies [ 72,96,134]) can
be formally denoted as X∈R𝑇×𝑁×𝐶, where𝑇,𝑁,𝐶 represent the
number of time steps, variables, and channels, respectively
4.4.1 Unimodal Pre-training. Unimodal pre-training typically
involves training deep neural networks on numerous large time
series datasets in a supervised or self-supervised manner. Through
pre-training, the model learns to capture the intricate temporal
patterns and inter-variable correlations within the data.
Beyond pre-training on ordinary time series that usually only
considers the temporal dimension [ 24–26,30,73,146], the studies
on urban time series emphasize on incorporating the spatial dimen-
sion to explicitly accommodate the dependencies between variables.
Some works [ 114,133,145] model the spatial relations as a grid map
with fixed sizes and employ Convolution Neural Networks (CNNs)
to encode the spatial pattern. For example, ST-GSP [ 145] proposes
a semantic flow encoder with ResNet as the backbone to capture
the spatial dependencies. Besides, it devises a Transformer encoder
with an early-fusion paradigm to capture multi-scale temporal in-
formation for reconstruction-based pre-training and downstream
urban flow prediction. Lately, UniST [ 134] further proposes a uni-
versal foundation model for spatio-temporal prediction that could
generalize diverse data formats and variations of data distributions.It designs a generative pre-training strategy with four distinct mask-
ing techniques and a prior knowledge-guided prompt network for
adaptation. However, grid-based spatial modeling is limited to local
regular dependencies and fails to exploit irregular and arbitrary
variable-wise connections.
In light of that, recent works [ 51,72,76,90,103] focus on graph-
based spatial encoding and usually use Spatio-Temporal Graph
Neural Networks (STGNNs) to extract the spatio-temporal infor-
mation. For instance, due to data scarcity, STGCL [ 72] devises a
contrastive pre-training framework for traffic forecasting. It intro-
duces node- and graph-level contrast paradigms with several data
augmentation techniques and debiased negative filtering strategies
from both spatio-temporal views. As vanilla STGNNs typically fo-
cus on short-term time series, STEP [ 96] devises TSFormer for time
series pre-training to capture long-term temporal patterns with
segment-level representations. TransGTR [ 52] focuses on how to
learn the graph structure in a transferable way, which pre-trains a
structure generator and transfers both the generator and the spa-
tiotemporal forecasting module across cities. Further, MC-STL [ 141]
spots the spatiotemporal heterogeneity (i.e., spatial correlation and
region importance are time-varied), and proposes two pre-training
strategies: spatial masked reconstruction and temporal contrastive
learning, to understand inter-region correlations and regional pat-
tern variations over time. GPT-ST [ 63] pre-trains a spatio-temporal
mask autoencoder by incorporating a parameter customization
scheme and adaptive mask strategy, utilizing a hierarchical hyper-
graph structure to capture multi-level spatial dependencies from a
global perspective.
4.4.2 Unimodal Adaptation. Unimodal adaptation aims to tailor
the pre-trained time series foundation models to new time series
datasets to enhance downstream task performance. Due to the emer-
gence of prompt tuning methods in data modalities like natural
language, vision, and graph [ 100], prompting methods for time
series also receive growing attention. For example, PromptST [ 143]
adds tunable prompts to spatiotemporal data to efficiently fine-tune
the pre-trained traffic forecasting model and avoid the catastrophic
forgetting issue. MetePFL [ 14] and FedWing [ 15] focus on federated
weather forecasting and use prompting techniques for personal-
ized federated learning to reduce communication overheads and
enhance privacy protection.
4.4.3 Cross-modal Adaptation. Due to insufficient time series
data to train a foundation model from scratch, cross-modal adapta-
tion approaches directly adopt the foundation models pre-trained
on other modalities and aim to transfer the modality-sharing knowl-
edge to time series tasks. Recently, most cross-modal studies have
focused on adapting pre-trained LLMs via partial parameter fine-
tuning for time series to sufficiently leverage the remarkable abili-
ties of LLMs [49, 50].
To adapt the LLM for urban time series, GATGPT [ 17] uses
a graph attention module for spatial graph encoding and passes
the spatio-temporal embeddings to the pre-trained GPT-2 model
for downstream spatio-temporal imputation. TPLLM [ 92] adopts
1-D convolutional neural networks and graph convolutional net-
works [ 78,138] to enable LLMs to understand spatio-temporal pat-
terns in traffic data. Additionally, STG-LLM [ 71] proposes a spatio-
temporal tokenizer to transform spatio-temporal data into tokens
6638Urban Foundation Models: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain.
and an adapter architecture for efficient adaptation, which facili-
tates the understanding of pre-trained LLMs. Similarly, ST-LLM [ 68]
treats the time steps at each spatial location as tokens, and fuse var-
ious spatial and temporal prior information into input embedding
to facilitate traffic prediction. To align the spatio-temporal depen-
dencies with the comprehension space of LLMs, UrbanGPT [ 62]
proposes a spatio-temporal instruction tuning paradigm that aligns
spatio-temporal and textual information, enhancing multiple spatio-
temporal prediction tasks, particularly in zero-shot scenarios.
The existing studies are mostly limited to homogeneous time
series data formats, e.g., regularly sampled time series, and specific
domains, which may disobey the universal property of foundation
models [ 30]. Building UFMs that can be applied to heterogeneous
data formats and diverse domain tasks remains an open problem.
4.5 Multimodal Models
Due to the intricate nature of urban environments characterized
by diverse data types, there is growing attention on developing
multimodal foundation models to handle urban tasks. By integrat-
ing these disparate data modalities, multimodal foundation models
aim to achieve a more holistic understanding of urban dynamics,
largely enhancing various urban applications and setting the stage
for urban general intelligence.
4.5.1 Pre-training. To develop a comprehensive understanding
of urban environments, multimodal UFMs aim to systematically pre-
train models on extensive multimodal datasets containing urban-
specific information such as textual, visual, and geographic data.
These works can be classified into single-domain models and multi-
domain models.
-Single-domain models. Among the existing pretraining-based
studies, some works focus on improving a specific urban domain
such as urban profiling and transportation. UrbanCLIP [126] com-
bines LLMs with satellite imagery for urban profiling. It leverages
LLMs to generate textual descriptions for satellite images and em-
ploys contrastive learning as well as language modeling loss as
supervision for urban visual representation learning of the model.
TengYun [ 144] is a transportation foundation model developed for
TransVerse, focusing on cognition, operation, management, and
control. It combines pre-training with prompt tuning for training,
starting with pre-training on cloud servers using the Transformer
framework and deep reinforcement learning to process historical
multi-modal data. During prompt tuning, edge servers customize
prompts for different tasks and strategies by treating training as a
self-supervised regression problem.
-Multi-domain models. While the above efforts have been made
in pre-training multimodal UFMs, their focus on training models us-
ing data specific to a single urban domain can limit their application
to broader urban tasks. Therefore, some works attempt to construct
multimodal UFMs that can be applied to multiple urban domains.
CityFM [ 4] is a framework that pre-trains foundation models by
using spatial, visual, and textual data from OpenStreetMap in a
self-supervised manner. It utilizes contrastive objectives to learn
multimodal representations of geospatial entities, and demonstrate
effectiveness in downstream tasks such as traffic speed inference
and building functionality classification. AllSpark [ 95] introducesthe Language as Reference Framework (LaRF) to develop a multi-
modal spatio-temporal model that amalgamates thirteen distinct
modalities into a single framework. LaRF’s concept centers around
correlating abstract concepts from each modality with language,
facilitating a collective interpretation within the language-based
unified representation space.
4.5.2 Adaptation. Another line of work explores introducing
foundation models from other domains, e.g., LLMs, to address urban
challenges. The adaptation approaches of these works primarily
include prompt engineering and model fine-tuning.
-Prompt engineering. The works using prompt engineering usu-
ally combine foundation models with domain-specific tools like
Geographic Information Systems (GIS) and external databases. Traf-
ficGPT [ 136] is a framework for urban traffic management that inte-
grates ChatGPT and various external tools. It involves prompt man-
agement, task understanding and planning through LLMs, and lever-
ages LLMs to invoke available tools for specific executions (e.g., data-
base retrieval and analysis, data visualization, and system optimiza-
tion). TrafficGPT also concludes with dialogue memory storage to
enhance its conversational context for future interactions. Open-
TI [20] uses prompt engineering to enhance language models’ un-
derstanding and execution abilities and integrate multiple traffic
simulation platforms like SUMO and CityFlow. It supports com-
plex simulation tasks such as map data processing from scratch,
traffic signal control, and demand optimization. GeoGPT [ 142] is a
framework integrating LLMs with mature GIS tools to assist non-
professionals in solving complex geospatial tasks using natural
language. It leverages LLMs to understand user demands and con-
ducts automated data collection, processing, and analysis in GIS,
enabling users to perform tasks like spatial queries and facility sit-
ing. Besides, Zhou et al. [ 152] apply LLMs to simulate planners and
residents for participatory urban planning. This work presents a
planning workflow where the planner proposes an initial land-use
plan based on the image map and textual description of the region,
and this plan is then revised through discussion with residents.
-Model fine-tuning. Beyond directly invoking the off-the-shelf
foundation models, some studies also introduce a fine-tuning pro-
cess for better adapting other domain foundation models to urban
applications. TransGPT-MM [ 115] is fine-tuned from the general
multi-modal LLM, VisualGLM-6B [ 22], to cater to the transporta-
tion domain specifically. This adaptation involves fine-tuning LLM
on multi-modal domain-specific datasets derived from driving tests,
traffic signs, and landmarks, showcasing powerful capabilities in
traffic-related analysis and modeling tasks. VELMA [ 94] is an em-
bodied LLM-based agent for vision and language navigation in
the street view. It integrates visual observations from street view
through CLIP for landmark recognition and uses a verbalization
pipeline to translate visual information and navigation instructions
into textual prompts for decision-making. It fine-tunes LLM on
urban domain training instances using LoRA [ 42], which adapts
attention query and value projections in the LLM. Xu et al . [123]
integrates LLMs into urban systems to tackle urban challenges, us-
ing continued pre-training to infuse the CityGPT model with urban
knowledge. It combines general texts, domain-specific data (e.g., ur-
ban knowledge graphs and geographic data), and task-solving pro-
cesses, and then employs supervised fine-tuning to tailor CityGPT
more closely to specific urban tasks and human preferences.
6639KDD ’24, August 25–29, 2024, Barcelona, Spain. Weijia Zhang et al.
Despite works on multimodal UFMs proactively advancing to
handle multimodal urban data, they are primarily limited to inte-
grating a restricted range of urban data types or applying to only
a narrow scope of urban domains. Furthermore, the potential for
spatio-temporal reasoning and the privacy and security issues in
UFMs remain largely unexplored. These reveal significant opportu-
nities for creating more versatile, practical, and trustworthy UFMs.
4.6 Others
Except for the aforementioned modalities, there exist some studies
exploring the foundation models in other urban scenarios, such as
geographical data [ 104], road network [ 11] and control policy [ 57].
4.6.1 Unimodal Approaches. OpenStreetMap (OSM) is a vital
information source of geographic information worldwide [ 53]. Nico-
las et al. [ 104] provide GeoVectors which is a world-scale corpus
of OSM entity embeddings, to enable the convenient usage of geo-
graphic information for machine learning algorithms. SARN [ 11]
proposes a contrastive-based graph pre-training method for spa-
tial structure-aware road network embedding, to facilitate down-
stream road property prediction, trajectory similarity prediction,
and shortest-path distance prediction tasks.
4.6.2 Cross-modal Adaptation. There emerge some studies on
leveraging the capacities of LLMs for challenging urban tasks, such
as traffic control [ 19,57,108]. For example, PromptGAT [ 19] uses
LLMs to generate human knowledge, that helps the neural networks
understand realistic cases (e.g., weather conditions) in traffic signal
control tasks [ 99]. LLMLight [ 57] proposes a pioneering frame-
work that utilizes LLMs as control agents to enable human-like
decision-making in traffic signal control tasks, showcasing remark-
able effectiveness, interpretability, and generalization ability.
5 APPLICATIONS
In this section, we discuss several urban application scenarios that
could significantly benefit from UFMs.
-Transportation. UFMs enable a new generation of intelligent
transportation systems, supporting humans throughout traffic man-
agement workflow and markedly reducing workloads. A few studies
have attempted to develop foundation models for transportation,
such as TransGPT [ 115] and TFM [ 117]. However, these approaches
primarily focus on specific aspects of transportation, e.g., traffic
forecasting. Developing a versatile UFM capable of handling multi-
modal traffic data (e.g., time series, trajectories, images, and text)
and diverse transportation tasks still remains an open problem.
-Urban planning. The widely available urban data, such as road
network structure and human mobility, has indeed uncovered the
underlying issues of a city, providing urban planners with valu-
able insights for making informed decisions [ 150]. UFMs can bring
substantial benefits to urban planning. On the one hand, they can
assist in analyzing vast amounts of urban data, generating creative
ideas to support urban planners and policymakers for better future
planning formulation. On the other hand, UFMs can also enhance
the participatory planning process [ 152] by mining public feed-
back from various digital platforms, making urban planning more
inclusive and responsive to the community’s needs.
-Energy management. The application of UFMs in the domain
of energy management holds great promise. As one of the initialefforts in this area, Huang et al. [ 44] explore a set of capabilities of
existing foundation models (i.e., GPT-4 and GPT-4 Vision), showcas-
ing promising results in tackling operational challenges of power
systems. Since urban energy data usually possess high sensitivity
and personal privacy information, privacy-preserving methods like
federated learning [ 128] could become an effective solution to fully
unleash the potential of foundation models in energy management.
-Environmental monitoring. We anticipate that UFMs can pro-
vide detailed simulations and predictive analytics for urban environ-
ments [ 82,87,107]. Recently, Saeid [ 107] introduced ChatClimate,
a retrieval-augmented framework to enhance the professional skills
of LLMs in the field of climate analysis by incorporating real-time
environmental information. Despite these advancements, the uti-
lization of UFMs in broader environmental applications, such as air
pollution analysis [39], still remains under-explored.
-Public safety and security. UFMs have exhibited significant po-
tential to enhance public safety and security in various aspects,
e.g., serving as transportation safety experts [ 148], recognizing ur-
ban ground collapse events [ 40], and offering legal services [ 18],
which provide valuable guidance for citizens and government au-
thorities. However, the application of UFMs in broader urban public
safety challenges, such as crime analysis and health resource allo-
cation, remains largely untapped, offering promising opportunities
to fully explore their potential in this field.
6 CONCLUSION AND FUTURE WORK
In conclusion, this paper has offered a comprehensive review of
Urban Foundation Models (UFMs), underscoring their transforma-
tive potential in deepening our understanding of urban dynamics
and significantly enhancing the realization of Urban General In-
telligence (UGI). We begin by defining UFMs and identifying the
major challenges in their development. Furthermore, our taxonomy
of current UFM studies, based on urban data modalities and types,
provides a clear framework for ongoing research. Additionally, we
discuss the potential applications and beneficial impacts of UFMs
across various critical urban domains.
Looking ahead, the evolution of UFMs promises exciting advance-
ments. We anticipate further research focusing on the integration
and analysis of multi-source, multi-granularity and multimodal
urban data, supporting an expansive range of urban applications.
The development of UFMs capable of real-time data processing and
providing timely urban insights remains a crucial area of focus. En-
hancing their spatiotemporal reasoning capabilities will further am-
plify their effectiveness in dynamic urban environments. Moreover,
as UFMs evolve, balancing the utility of urban data with privacy
and security considerations will be paramount for the ethical ad-
vancement of UGI. We are enthusiastic about future developments
in this field and myriad ways in which UFMs can contribute to
smarter, more resilient, and adaptive urban living.
ACKNOWLEDGEMENTS
This work was supported by the National Natural Science Foun-
dation of China (Grant No.62102110, No.92370204), National Key
R&D Program of China (Grant No.2023YFF0725001), Guangzhou-
HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Edu-
cation Bureau of Guangzhou Municipality.
6640Urban Foundation Models: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain.
REFERENCES
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-
rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[2]Mohamed Aghzal, Erion Plaku, and Ziyu Yao. 2023. Can large language models be
good path planners? a benchmark and investigation on spatial-temporal reasoning.
arXiv preprint arXiv:2310.03249 (2023).
[3]Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor
Darrell, Jitendra Malik, and Alexei A Efros. 2023. Sequential modeling enables
scalable learning for large vision models. arXiv preprint arXiv:2312.00785 (2023).
[4]Pasquale Balsebre, Weiming Huang, Gao Cong, and Yi Li. 2023. City Foundation
Models for Learning General Purpose Representations from OpenStreetMap. arXiv
e-prints (2023), arXiv–2310.
[5]Prabin Bhandari, Antonios Anastasopoulos, and Dieter Pfoser. 2023. Are Large
Language Models Geospatially Knowledgeable?. In Proceedings of the 31st ACM
International Conference on Advances in Geographic Information Systems. 1–4.
[6]Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. 2023.
Accurate medium-range global weather forecasting with 3D neural networks.
Nature 619, 7970 (2023), 533–538.
[7]Filip Biljecki and Koichi Ito. 2021. Street view imagery in urban analytics and GIS:
A review. Landscape and Urban Planning 215 (2021), 104217.
[8]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al .2021. On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 (2021).
[9]Norbert Buch, Sergio A Velastin, and James Orwell. 2011. A review of computer
vision techniques for the analysis of urban traffic. IEEE Transactions on intelligent
transportation systems 12, 3 (2011), 920–939.
[10] Keumgang Cha, Junghoon Seo, and Taekyung Lee. 2023. A billion-scale foundation
model for remote sensing images. arXiv preprint arXiv:2304.05215 (2023).
[11] Yanchuan Chang, Egemen Tanin, Xin Cao, and Jianzhong Qi. 2023. Spatial
Structure-Aware Road Network Embedding via Graph Contrastive Learning. Ad-
vances in Database Technology-EDBT 26 (2023), 144–156.
[12] Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia
Zou, and Zhenwei Shi. 2023. RSPrompter: Learning to prompt for remote sens-
ing instance segmentation based on visual foundation model. arXiv preprint
arXiv:2306.16269 (2023).
[13] Shengchao Chen, Guodong Long, Jing Jiang, Dikai Liu, and Chengqi Zhang. 2023.
Foundation Models for Weather and Climate Data Understanding: A Comprehen-
sive Survey. arXiv preprint arXiv:2312.03014 (2023).
[14] Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. 2023. Prompt Federated
Learning for Weather Forecasting: Toward Foundation Models on Meteorological
Data. arXiv preprint arXiv:2301.09152 (2023).
[15] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang. 2023.
Spatial-temporal Prompt Learning for Federated Weather Forecasting. arXiv
preprint arXiv:2305.14244 (2023).
[16] Yile Chen, Xiucheng Li, Gao Cong, Zhifeng Bao, Cheng Long, Yiding Liu, Arun Ku-
mar Chandran, and Richard Ellison. 2021. Robust road network representation
learning: When traffic patterns meet traveling semantics. In Proceedings of the 30th
ACM International Conference on Information & Knowledge Management. 211–220.
[17] Yakun Chen, Xianzhi Wang, and Guandong Xu. 2023. GATGPT: A Pre-trained
Large Language Model with Graph Attention Network for Spatiotemporal Impu-
tation. arXiv preprint arXiv:2311.14332 (2023).
[18] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-
source legal large language model with integrated external knowledge bases. arXiv
preprint arXiv:2306.16092 (2023).
[19] Longchao Da, Minchiuan Gao, Hao Mei, and Hua Wei. 2023. Llm powered sim-to-
real transfer for traffic signal control. arXiv preprint arXiv:2308.14284 (2023).
[20] Longchao Da, Kuanru Liou, Tiejin Chen, Xuesong Zhou, Xiangyong Luo, Yezhou
Yang, and Hua Wei. 2024. Open-ti: Open traffic intelligence with augmented
language model. International Journal of Machine Learning and Cybernetics (2024),
1–26.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[22] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Jun-
yang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al .2021. Cogview: Mastering
text-to-image generation via transformers. Advances in Neural Information Pro-
cessing Systems 34 (2021), 19822–19835.
[23] Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, and Yao Xu.
2023. MGeo: Multi-Modal Geographic Language Model Pre-Training. In Proceed-
ings of the 46th International ACM SIGIR Conference on Research and Development
in Information Retrieval. 185–194.
[24] Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang,
and Mingsheng Long. 2024. TimeSiam: A Pre-Training Framework for Siamese
Time-Series Modeling. arXiv preprint arXiv:2402.02475 (2024).
[25] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng
Long. 2023. SimMTM: A Simple Pre-Training Framework for Masked Time-Series
Modeling. arXiv preprint arXiv:2302.00861 (2023).
[26] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra Reddy,
Wesley M Gifford, and Jayant Kalagnanam. 2024. TTMs: Fast Multi-level Tiny
Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate
Time Series. arXiv preprint arXiv:2401.03955 (2024).
[27] Jie Feng, Zeyu Yang, Fengli Xu, Haisu Yu, Mudan Wang, and Yong Li. 2020. Learning
to simulate human mobility. In Proceedings of the 26th ACM SIGKDD internationalconference on knowledge discovery & data mining. 3426–3433.
[28] Jiayi Fu, Haoying Han, Xing Su, and Chao Fan. 2023. Towards Human-AI Collabo-
rative Urban Science Research Enabled by Pre-trained Large Language Models.
arXiv preprint arXiv:2305.11418 (2023).
[29] Tao-Yang Fu and Wang-Chien Lee. 2020. Trembr: Exploring road networks for
trajectory representation learning. ACM Transactions on Intelligent Systems and
Technology (TIST) 11, 1 (2020), 1–25.
[30] Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros
Tsiligkaridis, and Marinka Zitnik. 2024. UniTS: Building a Unified Time Series
Model. arXiv preprint arXiv:2403.00131 (2024).
[31] Letian Gong, Youfang Lin, Shengnan Guo, Yan Lin, Tianyi Wang, Erwen Zheng,
Zeyu Zhou, and Huaiyu Wan. 2023. Contrastive pre-training with adversarial
perturbations for check-in sequence representation learning. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 37. 4276–4283.
[32] Vinayak Gupta and Srikanta Bedathur. 2021. Region invariant normalizing flows
for mobility transfer. In Proceedings of the 30th ACM International Conference on
Information & Knowledge Management. 3053–3057.
[33] Vinayak Gupta and Srikanta Bedathur. 2022. Doing more with less: overcoming
data scarcity for poi recommendation via cross-region transfer. ACM Transactions
on Intelligent Systems and Technology (TIST) 13, 3 (2022), 1–24.
[34] Wes Gurnee and Max Tegmark. 2023. Language models represent space and time.
arXiv preprint arXiv:2310.02207 (2023).
[35] Lukas Haas, Silas Alberti, and Michal Skreta. 2023. Learning Generalized Zero-Shot
Learners for Open-Domain Image Geolocalization. arXiv preprint arXiv:2302.00275
(2023).
[36] Jindong Han, Hao Liu, Haoyi Xiong, and Jing Yang. 2022. Semi-supervised air
quality forecasting via self-supervised hierarchical graph neural network. IEEE
Transactions on Knowledge and Data Engineering 35, 5 (2022), 5230–5243.
[37] Jindong Han, Hao Liu, Hengshu Zhu, and Hui Xiong. 2023. Kill Two Birds with One
Stone: A Multi-View Multi-Adversarial Learning Approach for Joint Air Quality
and Weather Prediction. IEEE Transactions on Knowledge and Data Engineering
(2023).
[38] Jindong Han, Weijia Zhang, Hao Liu, Tao Tao, Naiqiang Tan, and Hui Xiong. 2024.
BigST: Linear Complexity Spatio-Temporal Graph Neural Network for Traffic
Forecasting on Large-Scale Road Networks. Proceedings of the VLDB Endowment
(2024), 1081–1090.
[39] Jindong Han, Weijia Zhang, Hao Liu, and Hui Xiong. 2023. Machine Learning for
Urban Air Quality Analytics: A Survey. arXiv preprint arXiv:2310.09620 (2023).
[40] Yanan Hao, Jin Qi, Xiaowen Ma, Sensen Wu, Renyi Liu, and Xiaoyi Zhang. 2024. An
LLM-Based Inventory Construction Framework of Urban Ground Collapse Events
with Spatiotemporal Locations. ISPRS International Journal of Geo-Information 13,
4 (2024), 133.
[41] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
2022. Masked autoencoders are scalable vision learners. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 16000–16009.
[42] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large
language models. arXiv preprint arXiv:2106.09685 (2021).
[43] Mingzhi Hu, Zhuoyun Zhong, Xin Zhang, Yanhua Li, Yiqun Xie, Xiaowei Jia, Xun
Zhou, and Jun Luo. 2023. Self-supervised Pre-training for Robust and Generic
Spatial-Temporal Representations. In IEEE International Conference on Data Mining.
150–159.
[44] Chenghao Huang, Siyang Li, Ruohong Liu, Hao Wang, and Yize Chen. 2023. Large
Foundation Models for Power Systems. arXiv preprint arXiv:2312.07044 (2023).
[45] Jizhou Huang, Haifeng Wang, Yibo Sun, Yunsheng Shi, Zhengjie Huang, An Zhuo,
and Shikun Feng. 2022. ERNIE-GeoL: A Geography-and-Language Pre-trained
Model and its Applications in Baidu Maps. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3029–3039.
[46] Johannes Jakubik, Sujit Roy, CE Phillips, Paolo Fraccaro, Denys Godwin, Bianca
Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards,
et al.2023. Foundation Models for Generalist Geospatial Artificial Intelligence.
arXiv preprint arXiv:2310.18660 (2023).
[47] Yuhan Ji and Song Gao. 2023. Evaluating the effectiveness of large language
models in representing textual descriptions of geometry and spatial relations.
arXiv preprint arXiv:2307.03678 (2023).
[48] Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, and Jingyuan
Wang. 2023. Self-supervised trajectory representation learning with temporal
regularities and travel semantics. In 2023 IEEE 39th international conference on data
engineering (ICDE). IEEE, 843–855.
[49] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang,
James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al .2023. Large models for
time series and spatio-temporal data: A survey and outlook. arXiv preprint
arXiv:2310.10196 (2023).
[50] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong
Wang, Shirui Pan, and Qingsong Wen. 2024. Position Paper: What Can Large Lan-
guage Models Tell Us about Time Series Analysis. arXiv preprint arXiv:2402.02713
(2024).
[51] Yilun Jin, Kai Chen, and Qiang Yang. 2022. Selective cross-city transfer learning
for traffic prediction via source city region re-weighting. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 731–741.
[52] Yilun Jin, Kai Chen, and Qiang Yang. 2023. Transferable Graph Structure Learning
for Graph-based Traffic Forecasting Across Cities. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 1032–1043.
[53] Jamal Jokar Arsanjani, Alexander Zipf, Peter Mooney, and Marco Helbich. 2015. An
introduction to OpenStreetMap in Geographic Information Science: Experiences,
6641KDD ’24, August 25–29, 2024, Barcelona, Spain. Weijia Zhang et al.
research, and applications. OpenStreetMap in GIScience: Experiences, research, and
applications (2015), 1–15.
[54] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[55] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al .
2023. Segment anything. arXiv preprint arXiv:2304.02643 (2023).
[56] Christos Kyrkou, Panayiotis Kolios, Theocharis Theocharides, and Marios Polycar-
pou. 2022. Machine Learning for Emergency Management: A Survey and Future
Outlook. Proc. IEEE (2022).
[57] Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu, and Hui Xiong. 2023. Large Language
Models as Traffic Signal Control Agents: Capacity and Opportunity. arXiv preprint
arXiv:2312.16044 (2023).
[58] Siqi Lai, Weijia Zhang, and Hao Liu. 2023. A Preference-aware Meta-optimization
Framework for Personalized Vehicle Energy Consumption Estimation. In Proceed-
ings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
4346–4356.
[59] Tong Li, Shiduo Xin, Yanxin Xi, Sasu Tarkoma, Pan Hui, and Yong Li. 2022. Pre-
dicting multi-level socioeconomic indicators from structural urban imagery. In
Proceedings of the 31st ACM International Conference on Information & Knowledge
Management. 3282–3291.
[60] Xiucheng Li, Kaiqi Zhao, Gao Cong, Christian S Jensen, and Wei Wei. 2018. Deep
representation learning for trajectory similarity computation. In 2018 IEEE 34th
international conference on data engineering (ICDE). IEEE, 617–628.
[61] Zekun Li, Jina Kim, Yao-Yi Chiang, and Muhao Chen. 2022. SpaBERT: A Pretrained
Language Model from Geographic Data for Geo-Entity Representation. arXiv
preprint arXiv:2210.12213 (2022).
[62] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin,
and Chao Huang. 2024. UrbanGPT: Spatio-Temporal Large Language Models.
arXiv preprint arXiv:2403.00813 (2024).
[63] Zhonghang Li, Lianghao Xia, Yong Xu, and Chao Huang. 2023. GPT-ST: Generative
Pre-Training of Spatio-Temporal Graph Neural Networks. Advances in Neural
Information Processing Systems 36 (2023).
[64] Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, and Muhao Chen. 2023. GeoLM: Em-
powering Language Models for Geospatially Grounded Language Understanding.
arXiv preprint arXiv:2310.14478 (2023).
[65] Yuebing Liang, Yichao Liu, Xiaohan Wang, and Zhan Zhao. 2023. Exploring large
language models for human mobility prediction under public events. arXiv preprint
arXiv:2311.17351 (2023).
[66] Yan Lin, Huaiyu Wan, Shengnan Guo, Jilin Hu, Christian S Jensen, and Youfang Lin.
2023. Pre-Training General Trajectory Embeddings With Maximum Multi-View
Entropy Coding. IEEE Transactions on Knowledge and Data Engineering (2023).
[67] Yan Lin, Huaiyu Wan, Shengnan Guo, and Youfang Lin. 2021. Pre-training con-
text and time aware location embeddings from spatial-temporal trajectories for
user next location prediction. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 4241–4248.
[68] Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, and Rui
Zhao. 2024. Spatial-temporal large language model for traffic prediction. arXiv
preprint arXiv:2401.10134 (2024).
[69] Fan Liu, Weijia Zhang, and Hao Liu. 2023. Robust spatiotemporal traffic forecasting
with reinforced dynamic adversarial training. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 1417–1428.
[70] Hao Liu, Jindong Han, Yanjie Fu, Yanyan Li, Kai Chen, and Hui Xiong. 2023. Unified
route representation learning for multi-modal transportation recommendation
with spatiotemporal pre-training. The VLDB Journal 32, 2 (2023), 325–342.
[71] Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, and Yanming Shen. 2024. How
can large language models understand spatial-temporal data? arXiv preprint
arXiv:2401.14192 (2024).
[72] Xu Liu, Yuxuan Liang, Chao Huang, Yu Zheng, Bryan Hooi, and Roger Zimmer-
mann. 2022. When do contrastive learning signals help spatio-temporal graph
forecasting?. In Proceedings of the 30th International Conference on Advances in
Geographic Information Systems. 1–12.
[73] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and
Mingsheng Long. 2024. Timer: Transformers for Time Series Analysis at Scale.
arXiv preprint arXiv:2402.02368 (2024).
[74] Yu Liu, Xin Zhang, Jingtao Ding, Yanxin Xi, and Yong Li. 2023. Knowledge-
infused contrastive learning for urban imagery-based socioeconomic prediction.
InProceedings of the ACM Web Conference 2023. 4150–4160.
[75] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted
windows. In Proceedings of the IEEE/CVF international conference on computer vision .
10012–10022.
[76] Bin Lu, Xiaoying Gan, Weinan Zhang, Huaxiu Yao, Luoyi Fu, and Xinbing Wang.
2022. Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge
Transfer. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining. 1162–1172.
[77] Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J Shane Culpepper, Haochao Ying,
Hao Liu, and Hui Xiong. 2020. Spatial object recommendation with hints: When
spatial granularity matters. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 781–790.
[78] Tengfei Lyu, Jianliang Gao, Ling Tian, Zhao Li, Peng Zhang, and Ji Zhang. 2021.
MDNN: A Multimodal Deep Neural Network for Predicting Drug-Drug Interaction
Events. In Proceedings of the International Joint Conference on Artificial Intelligence,
Vol. 2021. 3536–3542.[79] Dean Magee and Kevin Johnson. 2023. Steamlining Urban planning with AI tools.
Planning News 49, 5 (2023), 11–12.
[80] Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, and Stefano Ermon.
2022. Towards a foundation model for geospatial artificial intelligence (vision
paper). In Proceedings of the 30th International Conference on Advances in Geographic
Information Systems. 1–4.
[81] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao
Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. 2023. On the opportu-
nities and challenges of foundation models for geospatial artificial intelligence.
arXiv preprint arXiv:2304.06798 (2023).
[82] Xin Man, Chenghong Zhang, Changyu Li, and Jie Shao. 2023. W-MAE: Pre-trained
weather model with masked autoencoder for multi-variable weather forecasting.
arXiv preprint arXiv:2304.08754 (2023).
[83] Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, and
Stefano Ermon. 2023. Geollm: Extracting geospatial knowledge from large lan-
guage models. arXiv preprint arXiv:2310.06213 (2023).
[84] Lang Mei, Jiaxin Mao, Juan Hu, Naiqiang Tan, Hua Chai, and Ji-Rong Wen. 2023.
Improving First-stage Retrieval of Point-of-interest Search by Pre-training Models.
ACM Transactions on Information Systems 42, 3 (2023), 1–27.
[85] Harvey J Miller. 2004. Tobler’s first law and spatial analysis. Annals of the
association of American geographers 94, 2 (2004), 284–289.
[86] Peter Mooney, Wencong Cui, Boyuan Guan, and Levente Juhász. 2023. Towards
Understanding the Geospatial Skills of ChatGPT: Taking a Geographic Information
Systems (GIS) Exam. In Proceedings of the 6th ACM SIGSPATIAL International
Workshop on AI for Geographic Knowledge Discovery. 85–94.
[87] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya
Grover. 2023. ClimaX: A foundation model for weather and climate. In Proceedings
of the 40th International Conference on Machine Learning. 25904–25938.
[88] Yansong Ning and Hao Liu. 2024. UrbanKGent: A Unified Large Language Model
Agent Framework for Urban Knowledge Graph Construction. arXiv preprint
arXiv:2402.06861 (2024).
[89] Saeed Nosratabadi, Amir Mosavi, Ramin Keivani, Sina Ardabili, and Farshid Aram.
2019. State of the art survey of deep learning and machine learning models for
smart cities and urban sustainability. In International conference on global research
and education. Springer, 228–238.
[90] Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang.
2019. Urban traffic prediction from spatio-temporal data using deep meta learning.
InProceedings of the 25th ACM SIGKDD international conference on knowledge
discovery & data mining. 1720–1730.
[91] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-
hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .
2021. Learning transferable visual models from natural language supervision. In
International conference on machine learning. PMLR, 8748–8763.
[92] Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui. 2024.
TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language
Models. arXiv preprint arXiv:2403.02221 (2024).
[93] Jonathan Roberts, Timo Lüddecke, Sowmen Das, Kai Han, and Samuel Albanie.
2023. GPT4GEO: How a Language Model Sees the World’s Geography. arXiv
preprint arXiv:2306.00020 (2023).
[94] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and
William Yang Wang. 2023. Velma: Verbalization embodiment of llm agents for
vision and language navigation in street view. arXiv preprint arXiv:2307.06082
(2023).
[95] Run Shao, Cheng Yang, Qiujun Li, Qing Zhu, Yongjun Zhang, YanSheng Li, Yu
Liu, Yong Tang, Dapeng Liu, Shizhong Yang, et al .2023. AllSpark: a multimodal
spatiotemporal general model. arXiv preprint arXiv:2401.00546 (2023).
[96] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhanced
spatial-temporal graph neural network for multivariate time series forecasting. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 1567–1577.
[97] Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, and
Dongxiao Zhu. 2023. GeoSAM: Fine-tuning SAM with Sparse and Dense Visual
Prompting for Automated Segmentation of Mobility Infrastructure. arXiv preprint
arXiv:2311.11319 (2023).
[98] Ke Sun, Tieyun Qian, Chenliang Li, Xuan Ma, Qing Li, Ming Zhong, Yuanyuan
Zhu, and Mengchi Liu. 2023. Pre-Training Across Different Cities for Next POI
Recommendation. ACM Transactions on the Web 17, 4 (2023), 1–27.
[99] Qian Sun, Le Zhang, Huan Yu, Weijia Zhang, Yu Mei, and Hui Xiong. 2023. Hier-
archical reinforcement learning for dynamic autonomous vehicle navigation at
intelligent intersections. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 4852–4861.
[100] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. (2023).
[101] Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qibin He, Junxi
Li, Xuee Rong, Zhujun Yang, Hao Chang, et al .2022. RingMo: A remote sensing
foundation model with masked image modeling. IEEE Transactions on Geoscience
and Remote Sensing (2022).
[102] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. Advances in neural information processing systems 27 (2014).
[103] Yihong Tang, Ao Qu, Andy HF Chow, William HK Lam, SC Wong, and Wei Ma.
2022. Domain adversarial spatial-temporal network: a transferable framework
for short-term traffic forecasting across cities. In Proceedings of the 31st ACM
International Conference on Information & Knowledge Management. 1905–1915.
[104] Nicolas Tempelmeier, Simon Gottschalk, and Elena Demidova. 2021. GeoVectors: A
Linked Open Corpus of OpenStreetMap Embeddings on World Scale. In Proceedings
6642Urban Foundation Models: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain.
of the 30th ACM International Conference on Information & Knowledge Management.
4604–4612.
[105] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al .
2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 (2023).
[106] Zaib Ullah, Fadi Al-Turjman, Leonardo Mostarda, and Roberto Gagliardi. 2020. Ap-
plications of artificial intelligence and machine learning in smart cities. Computer
Communications 154 (2020), 313–323.
[107] Saeid Ashraf Vaghefi, Dominik Stammbach, Veruska Muccione, Julia Bingler, Jing-
wei Ni, Mathias Kraus, Simon Allen, Chiara Colesanti-Senni, Tobias Wekhof,
Tobias Schimanski, et al .2023. Chatclimate: Grounding conversational AI in
climate science. Communications Earth & Environment 4, 1 (2023), 480.
[108] Michael Villarreal, Bibek Poudel, and Weizi Li. 2023. Can ChatGPT Enable ITS?
The Case of Mixed Traffic Control via Reinforcement Learning. arXiv preprint
arXiv:2306.08094 (2023).
[109] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection
regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing
10, 3152676 (2017), 10–5555.
[110] Dongjie Wang, Chang-Tien Lu, and Yanjie Fu. 2023. Towards automated urban
planning: When generative and chatgpt-like ai meets urban planning. arXiv
preprint arXiv:2304.03892 (2023).
[111] Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, and Liangpei
Zhang. 2023. SAMRS: Scaling-up Remote Sensing Segmentation Dataset with
Segment Anything Model. In Thirty-seventh Conference on Neural Information
Processing Systems Datasets and Benchmarks Track.
[112] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liang-
pei Zhang. 2022. Advancing plain vision transformer toward remote sensing
foundation model. IEEE Transactions on Geoscience and Remote Sensing 61 (2022),
1–15.
[113] Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke
Shibasaki, and Chuan Xiao. 2024. Large Language Models as Urban Residents:
An LLM Agent Framework for Personal Mobility Generation. arXiv preprint
arXiv:2402.14744 (2024).
[114] Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, and Qiang Yang. 2018. Cross-
city transfer learning for deep spatio-temporal prediction. arXiv preprint
arXiv:1802.00386 (2018).
[115] Peng Wang, Xiang Wei, Fangxu Hu, and Wenjuan Han. 2024. TransGPT: Multi-
modal Generative Pre-trained Transformer for Transportation. arXiv preprint
arXiv:2402.07233 (2024).
[116] Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng. 2023. Where would
i go next? large language models as human mobility predictors. arXiv preprint
arXiv:2308.15197 (2023).
[117] Xuhong Wang, Ding Wang, Liang Chen, and Yilun Lin. 2023. Building Trans-
portation Foundation Model via Generative Graph Transformer. arXiv preprint
arXiv:2305.14826 (2023).
[118] Zhecheng Wang, Haoyuan Li, and Ram Rajagopal. 2020. Urban2vec: Incorporating
street view imagery and pois for multi-modal urban neighborhood embedding. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1013–1020.
[119] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al .2022. Emer-
gent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).
[120] Jian Xie, Yidan Liang, Jingping Liu, Yanghua Xiao, Baohua Wu, and Shenghua Ni.
2023. QUERT: Continual Pre-training of Language Model for Query Understanding
in Travel Domain Search. arXiv preprint arXiv:2306.06707 (2023).
[121] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian,
Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world
planning with language agents. arXiv preprint arXiv:2402.01622 (2024).
[122] Yiqun Xie, Zhaonan Wang, Gengchen Mai, Yanhua Li, Xiaowei Jia, Song Gao, and
Shaowen Wang. 2023. Geo-Foundation Models: Reality, Gaps and Opportunities.
InProceedings of the 31st ACM International Conference on Advances in Geographic
Information Systems. 1–4.
[123] Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, and Yong Li. 2023. Urban Generative
Intelligence: A Foundational Platform for Agents in Embodied City Environment.
arXiv preprint arXiv:2312.11813 (2023).
[124] Hao Xue and Flora D Salim. 2023. Promptcast: A new prompt-based learning
paradigm for time series forecasting. IEEE Transactions on Knowledge and Data
Engineering (2023).
[125] Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. 2022. Leveraging language
foundation models for human mobility forecasting. In Proceedings of the 30th
International Conference on Advances in Geographic Information Systems. 1–9.
[126] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen,
Roger Zimmermann, and Yuxuan Liang. 2023. When Urban Region Profiling Meets
Large Language Models. arXiv preprint arXiv:2310.18340 (2023).
[127] Zhiyuan Yan, Junxi Li, Xuexue Li, Ruixue Zhou, Wenkai Zhang, Yingchao Feng,
Wenhui Diao, Kun Fu, and Xian Sun. 2023. RingMo-SAM: A Foundation Model
for Segment Anything in Multimodal Remote-Sensing Images. IEEE Transactions
on Geoscience and Remote Sensing 61 (2023), 1–16.
[128] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1–19.
[129] Sean Bin Yang, Chenjuan Guo, Jilin Hu, Jian Tang, and Bin Yang. 2021. Unsu-
pervised Path Representation Learning with Curriculum Negative Sampling. InProceedings of the International Joint Conference on Artificial Intelligence. ijcai.org,
3286–3292.
[130] Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, and Christian S. Jensen. 2023.
LightPath: Lightweight and Scalable Path Representation Learning. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
ACM, 2999–3010.
[131] Di Yao, Chao Zhang, Zhihua Zhu, Jian-Hui Huang, and Jingping Bi. 2017. Trajectory
clustering via deep representation learning. In International Joint Conference on
Neural Networks. IEEE, 3880–3887.
[132] Fanglong Yao, Wanxuan Lu, Heming Yang, Liangyu Xu, Chenglong Liu, Leiyi
Hu, Hongfeng Yu, Nayu Liu, Chubo Deng, Deke Tang, et al .2023. RingMo-sense:
Remote sensing foundation model for spatiotemporal prediction via spatiotemporal
evolution disentangling. IEEE Transactions on Geoscience and Remote Sensing
(2023).
[133] Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, and Zhenhui Li. 2019. Learning
from multiple cities: A meta-learning approach for spatial-temporal prediction. In
The world wide web conference. 2181–2191.
[134] Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. UniST: A
Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction. arXiv
preprint arXiv:2402.11838 (2024).
[135] Zixuan Yuan, Hao Liu, Junming Liu, Yanchi Liu, Yang Yang, Renjun Hu, and Hui
Xiong. 2021. Incremental spatio-temporal graph learning for online query-poi
matching. In Proceedings of the Web Conference. 1586–1597.
[136] Siyao Zhang, Daocheng Fu, Wenzhe Liang, Zhao Zhang, Bin Yu, Pinlong Cai, and
Baozhen Yao. 2024. Trafficgpt: Viewing, processing and interacting with traffic
foundation models. Transport Policy 150 (2024), 95–105.
[137] Weijia Zhang, Hao Liu, Yanchi Liu, Jingbo Zhou, and Hui Xiong. 2020. Semi-
supervised hierarchical recurrent graph neural network for city-wide parking
availability prediction. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, Vol. 34. 1186–1193.
[138] Weijia Zhang, Hao Liu, Lijun Zha, Hengshu Zhu, Ji Liu, Dejing Dou, and Hui Xiong.
2021. MugRep: A multi-task hierarchical graph representation learning framework
for real estate appraisal. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 3937–3947.
[139] Weijia Zhang, Chenlong Yin, Hao Liu, Xiaofang Zhou, and Hui Xiong. 2024. Irregu-
lar Multivariate Time Series Forecasting: A Transformable Patching Graph Neural
Networks Approach. In International Conference on Machine Learning. PMLR.
[140] Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Jingbo Zhou, Yu Mei, and Hui
Xiong. 2023. Irregular traffic time series forecasting based on asynchronous spatio-
temporal graph convolutional network. arXiv preprint arXiv:2308.16818 (2023).
[141] Xu Zhang, Yongshun Gong, Xinxin Zhang, Xiaoming Wu, Chengqi Zhang, and
Xiangjun Dong. 2023. Mask-and Contrast-Enhanced Spatio-Temporal Learning for
Urban Flow Prediction. In Proceedings of the 32nd ACM International Conference
on Information and Knowledge Management. 3298–3307.
[142] Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, and Wenhao Yu. 2023.
GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous
GPT. arXiv preprint arXiv:2307.07930 (2023).
[143] Zijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang,
Hongwei Zhao, Yiqi Wang, and Zitao Liu. 2023. PromptST: Prompt-Enhanced
Spatio-Temporal Multi-Attribute Prediction. In Proceedings of the 32nd ACM Inter-
national Conference on Information and Knowledge Management. 3195–3205.
[144] Chen Zhao, Xiao Wang, Yisheng Lv, Yonglin Tian, Yilun Lin, and Fei-Yue Wang.
2023. Parallel Transportation in TransVerse: From Foundation Models to DeCAST.
IEEE Transactions on Intelligent Transportation Systems (2023).
[145] Liang Zhao, Min Gao, and Zongwei Wang. 2022. St-gsp: Spatial-temporal global
semantic representation learning for urban flow prediction. In Proceedings of the
Fifteenth ACM International Conference on Web Search and Data Mining. 1443–1451.
[146] Shubao Zhao, Ming Jin, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Qingsong
Wen, and Yi Wang. 2024. Himtm: Hierarchical multi-scale masked time series
modeling for long-term forecasting. arXiv preprint arXiv:2401.05012 (2024).
[147] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey of
large language models. arXiv preprint arXiv:2303.18223 (2023).
[148] Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Chenzhu Wang, and Shengxuan
Ding. 2023. TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a
Domain-Specific Expert in Transportation Safety. arXiv preprint arXiv:2307.15311
(2023).
[149] Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, and Shengxuan
Ding. 2023. ChatGPT is on the horizon: Could a large language model be all we
need for Intelligent Transportation? arXiv preprint arXiv:2303.05382 (2023).
[150] Yu Zheng, Licia Capra, Ouri Wolfson, and Hai Yang. 2014. Urban computing:
concepts, methodologies, and applications. ACM Transactions on Intelligent Systems
and Technology (TIST) 5, 3 (2014), 1–55.
[151] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng
Ji, Qiben Yan, Lifang He, et al .2023. A comprehensive survey on pretrained
foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419
(2023).
[152] Zhilun Zhou, Yuming Lin, Depeng Jin, and Yong Li. 2024. Large Language Model
for Participatory Urban Planning. arXiv preprint arXiv:2402.17161 (2024).
[153] He Zhu, Wenjia Zhang, Nuoxian Huang, Boyang Li, Luyao Niu, Zipei Fan, Tianle
Lun, Yicheng Tao, Junyou Su, Zhaoya Gong, Chenyu Fang, and Xing Liu. 2024.
PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient
Retrieval. arXiv preprint arXiv:2402.19273 (2024).
6643