Logical Reasoning with Relation Network for Inductive
Knowledge Graph Completion
Qinggang Zhang
The Hong Kong Polytechnic
University
Hung Hom, Hong Kong SAR
qinggangg.zhang@connect.polyu.hkKeyu Duan
National University of Singapore
21 Lower Kent Ridge Road ,Singapore
k.duan@u.nus.eduJunnan Dong
The Hong Kong Polytechnic
University
Hung Hom, Hong Kong SAR
hanson.dong@connect.polyu.hk
Pai Zheng
The Hong Kong Polytechnic
University
Hung Hom, Hong Kong SAR
pai.zheng@polyu.edu.hkXiao Huang
The Hong Kong Polytechnic
University
Hung Hom, Hong Kong SAR
xiaohuang@comp.polyu.edu.hk
ABSTRACT
Inductive knowledge graph completion (KGC) aims to infer the
missing relation for a set of newly-coming entities that never ap-
peared in the training set. Such a setting is more in line with reality,
as real-world KGs are constantly evolving and introducing new
knowledge. Recent studies have shown promising results using mes-
sage passing over subgraphs to embed newly-coming entities for
inductive KGC. However, the inductive capability of these methods
is usually limited by two key issues. (𝑖)KGC always suffers from
data sparsity, and the situation is even exacerbated in inductive
KGC where new entities often have few or no connections to the
original KG.(𝑖𝑖)Cold-start problem. It is over coarse-grained for
accurate KG reasoning to generate representations for new entities
by gathering the local information from few neighbors. To this end,
we propose a novel i NfOmax RelAtion Network, namely NORAN,
for inductive KG completion. It aims to mine latent relation patterns
for inductive KG completion. Specifically, by centering on relations,
NORAN provides a hyper view towards KG modeling, where the
correlations between relations can be naturally captured as entity-
independent logical evidence to conduct inductive KGC. Extensive
experiment results on five benchmarks show that our framework
substantially outperforms the state-of-the-art KGC methods.
CCS CONCEPTS
•Information systems →Data mining.
KEYWORDS
Knowledge graph; message passing; logical reasoning
ACM Reference Format:
Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, and Xiao Huang.
2024. Logical Reasoning with Relation Network for Inductive Knowledge
Graph Completion. In Proceedings of the 30th ACM SIGKDD Conference
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671911on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671911
1 INTRODUCTION
Knowledge graphs (KGs) are a collection of factual information
to describe a certain field of the real world [ 1–6], which are often
formulated as(ℎ,𝑟,𝑡)indicating there is a relation 𝑟between the
head entityℎand tail entity 𝑡. Since KGs are usually incomplete [ 7,
8], as well as the expensive costs to collect facts manually, how
to automatically perform knowledge graph completion (KGC) has
been a widely-studied topic in the KG community [ 9–11]. Currently,
embedding-based methods [ 12–16] play a dominant role in KGC.
The typical methods embed the entities and relations into low-
dimensional spaces with various objectives and then predict the
relation between any two entities. However, most embedding-based
methods are merely applicable under transductive settings, where
KGC models predict the missing relations among entities that are
already present in the training set. When some unseen entities are
newly added during testing, they have to retrain the whole KG
embeddings for accurate inference, which is not feasible in practice
due to the high update frequency and large sizes of real-world KGs.
Recently, there has been increasing interest in KGC under in-
ductive setting [ 17–19], where KGC models are used to infer the
relations for newly-coming entities that have never appeared in
the training set. The inductive KGC is more reflective of real-world
scenarios, as KGs always keep introducing new information, such
as new users and products in e-commerce KGs and new molecules
in biomedical KGs. Some efforts have been made to obtain the
inductive embeddings for new entities using external resources,
such as entity attributes [ 20,21], textual descriptions [ 22,23] and
ontological schema [ 24]. However, these supplemental sources are
often prohibitive to acquire, impeding their success in practice. An
alternative solution is to induce entity-independent rules from the
KG in either statistical [ 25,26] or differentiable manners [ 13,27,28].
It regards inductive KG completion as a rule mining problem, i.e.
predicting the relationship 𝑟for any entity pair(ℎ,𝑡)based on pre-
defined or adaptively learned rules. However, these methods often
suffer from scalability and generalizability issues since different
KGs have distinct rules with domain knowledge involved.
4268
KDD ’24, August 25–29, 2024, Barcelona, Spain Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, & Xiao Huang
As the prosperity of graph neural networks (GNNs) and their
strong inductive bias [ 29–33], many works extend message-passing
(MP) strategy used in GNNs to multi-relational graphs for inductive
KGC [ 19,34–36]. GraIL [ 35] is one of the pioneer work, which im-
plicitly learns logical rules by reasoning over enclosing subgraphs.
Specifically, GraIL first extracts an enclosing subgraph surrounding
the target triple, and then generate the representations of entities
by aggregating the structural information of its neighbors in the
extracted subgraph. GraIL and its follow-up methods [ 37–44] have
shown promising results using message passing over subgraphs for
inductive KGC. However, the inductive capability of these meth-
ods is usually limited by two key issues. (𝑖)KGC always suffers
from data sparsity, and the situation is even more difficult in in-
ductive KGC, where the new emerging entities usually have few
or even no connections to the original KG. (𝑖𝑖)Existing message
passing methods assume that messages are associated with entities
and are passed from nodes to nodes iteratively [ 45]. Such entity-
oriented message-passing strategy weakens the role of relation
semantics [ 19], which violates the nature of inductive KGC since
the learned logical evidence should be entity-independent.
In real-world scenarios, the enclosing subgraph of a particular
triple in the KG contains the logical evidence needed to deduce the
relation between the target nodes [ 46]. Taking Figure 1 (𝑖)as an
example, learning the relation pattern among Elon Musk, TESLA,
andCalifornia State can help the KGC model make a confident pre-
diction of (Martin Eberhard, :Livein, California State) since “:LiveIn
≃:WorkAt∧:LocatedIn ”. To this end, we propose a novel message
passing neural framework, i.e., NORAN, which aims to mine rela-
tion semantics for inductive KG completion. Specifically, we first
provide a deep insight of formulating the problem to bridge the
gap between embedding-based paradigm and inductive setting. In-
spired by the findings, we propose relation network, a novel relation-
induced graph derived from the original KG. The reconstructed
relation network provides a hyper view towards KG modeling by
centering relations. Its topological structure clearly reflects the
distribution of relations, i.e., each node in the relation network rep-
resents a relational instance and the links among the nodes reflect
the relation correlation. Thus, modeling the relation network via
message-passing networks can naturally capture relation patterns
as entity-independent context information to conduct inductive
KGC. In this paper, we formally define such context information as
logic evidence and reveal its correlations with our relation network.
Our main contribution is summarized as follows:
•We propose a novel framework, i.e., NORAN, which aims to
mine latent relation semantics for inductive KG completion.
•Centering on relations, we propose a hyper view towards
KG modeling, i.e., relation network, and formally define the
inductive KGC as k-hop logic reasoning over the hypergraph.
•We propose a informax training objective to implicitly cap-
ture logic evidence from relation network for effective KGC.
•We conduct a detailed theoretical analysis to explore which
kind of message-passing strategy is more effective and pro-
vide guidelines for model selection to enable inductive rea-
soning over the reconstructed relation network.
•Extensive experiments on five real-world KG benchmarks
demonstrate the superiority of our proposed NORAN.2 PRELIMINARIES
2.1 Transductive KG Completion
Given any pair of entities (ℎ,𝑡), traditional KG completion model
aims to predict the probability of relation 𝑟linkingℎand𝑡, i.e.
𝑝(𝑟|ℎ,𝑡). The relation type with the highest probability will be the
output after ranking. Some related work also equally formalizes
the problem as a relation classification problem, which predicts the
relation type with 𝑆𝑜𝑓𝑡𝑚𝑎𝑥𝑟(𝑝(𝑟|ℎ,𝑡)).
Currently, the embedding-based paradigm dominates the trans-
ductive KGC problem. TransE [ 12], DistMult [ 13], TransR [ 14], Com-
plEx [ 15], and SimplE [ 16] are the representative embedding-based
methods, which embed entity and relations into RHS space with
carefully designed modeling strategy. Embedding-based methods
encompass two branches: (𝑖)translating-based methods, which in-
terpret relations as translation operations on the low-dimensional
embeddings of entities; (𝑖𝑖)semantic matching methods, which
compute the similarity between relations and entities with their
semantic information instantiated by their low-dimensional repre-
sentations. Despite its capability to learn expressive representations,
the embedding-based paradigm is not applicable under inductive
settings since they have to retrain the whole KG embeddings when
some unseen entities are newly added during testing.
2.2 Inductive KG Completion
Different from transductive KGC, inductive KGC tries to predict
the missing relations for a set of new entities that are not present
in the training set. Given a knowledge graph G={E,R}and a set
of incoming entities E𝑢={𝑒|𝑒∉E}, our goal is to predict the
missing relations among EandE𝑢. The prediction task contains
two scenarios:(𝑖)predict relations between 𝑒1∈E and𝑒2∉E
with𝑃(𝑟|𝑒1,𝑒2), namely inductive relation prediction ;(𝑖𝑖)predict
relations within unseen entities ( 𝑒∉E), namely fully-inductive
relation prediction. Both two tasks emphasize on learning entity-
independent logic evidence and typical representations of relations.
In this work, we focus on the former setting, inductive relation
prediction, since the latter is within the scope of transfer learning
that leverages the information from one KG to the others.
Under inductive settings, most of the previous effective works
can be summarized as rule-induction based paradigm [ 13,27,28,47],
which follows explicit or implicit logic evidence for knowledge
graph reasoning. It is worth noting that this paradigm is compati-
ble with embedding-based strategy, and most rule-induction-based
methods are on the branch of the aforementioned semantic match-
ing methods . More recently, with the prosperity of GNNs, many
works [ 19,34,35,48] extend the MP strategy to knowledge graphs.
R-GCN [ 34] firstly proposed a graph convolution operation for re-
lational data. GraIL [ 35] and its follow-up methods [ 39–44,49,50]
applied the linking prediction techniques, i.e. subgraph extraction
and labeling trick, to tackle the inductive KGC problem. MP-based
methods benefit from their inductive bias but still require dedicated
efforts to design the relational message-passing strategy with vari-
ous options of combination and aggregation functions. By contrast,
provided with extensive existing SOTA GNN architectures, our pro-
posed framework is model-agnostic to take full advantage of them.
Furthermore, the significance of relation semantics has been con-
currently acknowledged in several recent studies [ 39–41]. Notably,
4269Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion KDD ’24, August 25–29, 2024, Barcelona, Spain
(1)Inductive Relation PredictionElon MuskMartin
Eberhard
:WorkAt:WorkAt:LocatedIn :WorkAt
:BoardofDirectors:Acquired:LocatedIn
:LiveIn:LiveIn ?
:BoardofDirectors?20
73
4
5
61
8
9
(2) Relation Network02 3
4
5 6
178
9
New entities :need expensive retraining
embedding table forinductive inference……
…
…
……:Acquired
:WorkAt
:LocatedIn
:LocatedIn
:LiveIn
:WorkAt
:WorkAt
:BoardOf
Directors
:LiveIn
:BoardOf
Directors
New triple: existed?
Existed?
Existed?
Figure 1: (i)A toy example of inductive knowledge graph completion, i.e. predicting the relationship (“?”) for unseen entities,
e.g. “Martin Eberhard”, provided with a few links, e.g. (Martin, :WorkAt, TESLA); (ii)Illustration of the corresponding relation
network for knowledge graph, which regards each triple as a relational node and thus could aggregate context information for
inductive inference without expensive retraining look-up embedding tables as embedding-based paradigm.
Table 1: Unified notations for crucial terms.
Notation Description
G a knowledge graph
(ℎ,𝑟,𝑡) (head entity, relation, tail entity)
eG a relation network
Γ(·) the combination function in relation network
Ψ𝑘,Ω𝑘𝑘-layer GNN models
the relation network in our research diverges substantially from the
relation graph used in these work. They introduced relation schema
to quantify the interconnectivity among various types of relations
for inductive KGC with new relations, while our proposed relation
network aims to capture the contextual semantics between every
pair of relational triples for KGC with previously unseen entities.
3 PROBLEM FORMULATION
We unify the main notations used in this work and provide a
brief view in Table 1. We provide an insight into the gap between
embedding-based paradigms and inductive settings. Given a KG
G={(ℎ,𝑟,𝑡)|ℎ,𝑡∈E,𝑟∈R} , the embedding-based paradigms
formalize their model as 𝑝(𝑟|ℎ,𝑡), where predict the relationship
𝑟with the prior knowledge of entities ℎand𝑡. However, under
inductive setting, the new entities trigger non-trivial perturbation
to the distribution of the prior knowledge graph, thus biasing the
model𝑝(𝑟|ℎ,𝑡)from the prior ground truth 𝑝(ℎ,𝑡). To this end, we
propose a new formulation for the general KGC problem:
𝑝(ℎ,𝑟,𝑡)=𝑝(𝑟|ℎ,𝑡)·𝑝(ℎ,𝑡), (1)
where we predict triple existence, i.e. 𝑝(ℎ,𝑟,𝑡), rather than the con-
ditional probability 𝑝(𝑟|ℎ,𝑡). Such a formulation has several profits.
Firstly,𝑝(ℎ,𝑟,𝑡)is capable of capturing the evolving distribution of
𝑝(ℎ,𝑡)under inductive setting since no prior distribution is required.
Meanwhile, for transductive setting, where 𝑝(ℎ,𝑡)is static,𝑝(ℎ,𝑟,𝑡)
can be collapsed to the previous modeling 𝑝(𝑟|ℎ,𝑡). From another
perspective, 𝑝(ℎ,𝑟,𝑡)encompasses two crucial terms, relation type
𝑝(𝑟|ℎ,𝑡)and relation existence 𝑝(ℎ,𝑡). Previous modeling 𝑝(𝑟|ℎ,𝑡)
predicts the relation type 𝑟between an entity pair (ℎ,𝑡), which is
identical to a classification problem. However, an ideal KGC modelshould not only predict the relation type but also induce the incom-
plete entity pairs(ℎ,𝑡), while the latter one is unfortunately ignored
in previous modeling. To this end, we propose relation network, a
novel view towards knowledge graph modeling. It augments the
target KG into hyper-views by regarding each relational triple as
nodes. Following this, the link prediction task, i.e., 𝑝(𝑟|ℎ,𝑡), can be
naturally transformed into predicting the existence of the corre-
sponding node in the reconstructed relation network, i.e., 𝑝(ℎ,𝑟,𝑡).
4 METHODOLOGY
In this section, we propose NORAN to mine latent pattern among
relational instances for inductive KG completion. As shown in Fig-
ure 1, our proposed NORAN consists of three key components:
(𝑖)Relation network construction. It provides a new modeling of
KGs by centering on relations. (𝑖𝑖)Relational message passing, a
relational message passing framework is defined to implicitly cap-
ture entity-independent logical evidence from the reconstructed
relation network.(𝑖𝑖𝑖)Training scheme and KG inference. Further-
more, we also provide a detailed theoretical analysis to explore
which kind of MP strategy is more effective and provide guide-
lines for model selection to enable inductive reasoning over the
reconstructed relation network.
4.1 Relation Network Construction
Traditional KG representation learning methods usually model the
KG as a heterogeneous graph by centering on the entities and re-
garding relations as semantic edges. However, it is hard to capture
complex semantics from KG relations and further learn expressive
representations for inductive KGC. To fill the gap between tradi-
tional KG representation learning paradigm and inductive KGC, in
this work, we value the correlations between relations, accordingly
provide a novel hyper view towards KG modeling with relations,
and formally define the inductive KG completion as k-hop logic
reasoning over an entity-independent network.
4.1.1 Relation network. To better capture the latent logic evidence
for effective reasoning, we first propose a novel view towards knowl-
edge graph modeling that augments the target KG into hyper-views
by compressing each triple into a relational node. Concretely, we
4270KDD ’24, August 25–29, 2024, Barcelona, Spain Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, & Xiao Huang
Table 2: The general description framework for MP layers
MP
layer Motivation Convolutional Matrix C Feature Transformation 𝑓Category ofC
GCN
[51] Spatial Convolution C=e𝑫1/2(𝑨+𝑰)e𝑫1/2𝑓=𝑾∈R𝑑𝑘×𝑑𝑘+1 Fixed
GraphSAGE [52] Inductive Learning C=e𝑫−1(𝑨+𝑰) 𝑓=𝑾∈R𝑑𝑘×𝑑𝑘+1 Fixed
GIN [45] WL-TestC=𝑨+𝑰 𝑓is a two-layer MLP Fixed
SGC [53] Scalability C=e𝑫1/2(𝑨+𝑰)e𝑫1/2𝑓=𝑙𝑎𝑚𝑏𝑑𝑎𝑥 :𝑥 Fixed
GA
T [54] Self Attention(
C=(𝑨+𝑰)·T
T(𝑖,𝑗)=𝑒𝑥𝑝([Θ𝒙𝑖||Θ𝒙𝑗]·𝒂)Í
𝑘∈
N(𝑖)∪𝑖𝑒𝑥𝑝([Θ𝒙𝑖||Θ𝒙𝑗]·𝒂)𝑓=Θ∈R𝑑𝑘×𝑑𝑘+1 Learnable
𝑎e𝑫is
the diagonal matrix of (𝑨+𝑰)
perform a relation-induced construction process to build the rela-
tion network. In essence, we follow one strategical criterion: regard
each triple as relational nodes and link them in the relational graph
if they share the same entity in the original KG.
Definition 1. Relation network. Given a knowledge graph
G={(ℎ,𝑟,𝑡)|ℎ,𝑡∈ E,𝑟∈ R} , the corresponding relation net-
work is the triple-level graph G𝑟=(eV,eA,fX𝑟), where eVandeA
are the set of relational nodes and adjacency metric respectively.
fX𝑟={Γ(ℎ,𝑟,𝑡)|(ℎ,𝑟,𝑡) ∈ G} represents the feature matrix of
eV, where Γ(·)is the concatenation function that transforms each
relational triple into a node, i.e., 𝑣=Γ(ℎ,𝑟,𝑡).eA(𝑣,𝑢|𝑣,𝑢∈eV)=1,
if𝑣and𝑢share the same entity in the original KG.
According to this criterion, there are two latent construction par-
ticulars corresponding to two construction issues: (𝑖)the semantics
of various ‘entity sharing ’ patterns (Remark 1 in Section 5.3) and (𝑖𝑖)
the semantics of the ‘linking direction ’ (Remark 2 in Section 5.3). We
conduct a detailed ablation study in Section 5.3 to explore different
ways to construct the relation network.
4.1.2 Interpretation with logic reasoning. The topological graph
structure of the relation network clearly reflects the distribution of
relations and the link between different nodes reflects the relation
correlations. Thus, modeling the relation network via a k-layer
message-passing network can naturally capture relation patterns
as entity-independent context information to conduct inductive
KGC. In this paper, we formally define such context information as
thek-hop logic evidence over the relation network.
Definition 2. K-hop logic evidence. LeteG=(eV,eA)be the
relation network of a knowledge graph G=(E,R). Given any center
node𝑣𝑖∈eV, the k-hop ego graph fG𝑖=(eV𝑖,eA𝑖)centering at 𝑣𝑖
contains the relational contextual information for logical reasoning.
In this paper, we model such contextual information as Λ𝑘(𝑣𝑖)via a
k-layer message-passing network Ω. Thus, the overall logic evidence
of knowledge graph Gcan be modeled by traversing all k-hop ego
graphs, i.e., Λ(G)=Ω𝐺𝑁𝑁(Λ𝑘(𝑣)|𝑣∈eG).
Taking a logic evidence “ :Livein = :WorkAt ∧:Locatedin ” in
Fig. 1(𝑖)as an instance, the relation pattern among Elon Musk,
TESLA, and California State is a typical instantiation of the logic
evidence and the triple (Martin Eberhard, :LiveIn, California State)
can be predicted with relative certainty. Mapping to relation net-
work of Fig. 1(𝑖𝑖), according to Definition 2, such logic evidences
are transformed into 𝑘-hop ego graphs centering on target nodes,
which describe the relational context information for reasoning.
Generally, there are two crucial benefits of constructing relation
network:(𝑖)Relation network provides a novel view towards KGmodeling, from which entity-independent logic evidence can be
naturally modeled via 𝑘-hop ego graphs that sampled from the
relation network to conduct inductive KGC, without fine-grained
embeddings for unseen entities. (𝑖𝑖)Compared with original KG, the
relation network shows a more dense structure and is compatible
with any GNN models.
4.2 Relational Message Passing
To capture entity-independent contextual information as logical
evidence, we propose a novel relational message-passing framework
to conduct inductive KGC.
4.2.1 Feature initialization. Since every instance in the relation
network is transformed from a corresponding triple (ℎ,𝑟,𝑡)in the
original KG, we first randomly initialize the embedding of entities
and relations in the original KG, and then adopt a local informa-
tion modeling layer, i.e, a set of Bi-LSTM units, to learn the local
relational structure within each triple:
𝒙𝑖=Γ(ℎ,𝑟,𝑡)=𝑐𝑜𝑛𝑐𝑎𝑡(Φ(𝒆ℎ,𝒆𝑟,𝒆𝑡)), (2)
where Φ(·)is a Bi-LSTM unit, and 𝒆ℎ,𝒆𝑟,𝒆𝑡are the initial embed-
dings ofℎ,𝑟, and𝑡. The output triple embedding 𝒙𝑖could well
capture the relational structure within the input triple. Thus, we
used it as the initial node embedding in the relation network. Note
that entity embeddings are fixed during the training and inference.
4.2.2 Message passing. Message-passing layer is formulated as:
𝑿(𝑘+1)=𝜎 C(𝑘)𝑿(𝑘)◦𝑓(𝑘), (3)
where 𝑿(𝑘)is the relational embedding at the k-th layer; C(𝑘)is
the convolutional matrix for 𝑘-th layer;𝑓(𝑘):R𝑑𝑘→R𝑑𝑘+1is
the linear transformation matrix; and 𝜎is the activation function.
The reformulation of MP layers can be found on Table 2. Message
passing over the relational instances enables the natural capture of
relation patterns as entity-independent context information, which
is crucial for conducting inductive KGC.
In this paper, we train two message-passing GNNs, i.e., Ωand
Ψ:Ωis applied on the k-hop ego graph to extract logical evidence,
while Ψis applied on relation network to learn relational embedding
for inductive inference.
4.2.3 Mutual information maximization. Reasoning with logical
evidence is crucial for inductive KGC, which imitates the inference
process of human beings. However, as aforementioned, logic evi-
dence is notoriously sophisticated for representation learning and
is usually utilized to regularize reasoning in previous practice. In
light of this, we propose Logic Evidence Information Maximization
4271Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1: Logical Reasoning with Relation Network
// All notations are consistent with Table 1
Input: a Knowledge Graph G
Output: a combination function Γand a GNN model Ψ
Init: create the relation network eG
while not convergence do
foreach batchB=(VB,EB)fromeGdo
𝑿B:=Γ(VB); // Eq.(2)
𝑿B:=Ψ(𝑿B,EB)
L=−I𝜔(Λ𝑘(G),𝑿B); // Eq.(5)
L.optimize()
end
end
(LEIM). At a high level, it is a training objective that tries to maxi-
mally preserve mutual information between logic evidence learned
from k-hop ego graphs and relational semantics of center node.
L𝜔,𝜓=−I𝜔(Λ(G),Ψ𝑘(eG)), (4)
where Λ(G)=Ω𝐺𝑁𝑁(Λ𝑘(𝑣)|𝑣∈eG)is the learned logic evidence,
I(·,·)is the mutual information; Ψ𝑘(·)is a𝑘-layer GNN; and 𝜔,𝜓
are the trainable parameters for IandΨ, respectively. The loss is
minimized to find the optimal parameters ˆ𝜓that maximally pre-
serve the mutual information between logic evidence Λ(G)and
the relational embedding of target node 𝑣learned with Ψ𝑘(eG). In
practice, we adopt a Jensen-Shannon MI estimator (JSD) [55–57]:
I𝐽𝑆𝐷
𝜔,𝜓(Λ,Ψ)=EP
−𝑠𝑝(−𝑇𝜔,𝜓(Λ𝑘(𝑣),Ψ𝑘(𝑣)))
−EP×P′
−𝑠𝑝(𝑇𝜔,𝜓(Λ𝑘(𝑣′),Ψ𝑘(𝑣)))
,(5)
where𝑣is the target node sampled from the relation network ac-
cording to distribution P,𝑣′is sampled from negative distribution
P′=P, and𝑠𝑝denotes softplus activation function. To encourage
positive pairs, e.g. (Λ(𝑣),Ψ(𝑣)), and distinguishes negative pairs,
e.g.(Λ(𝑣′),Ψ(𝑣)), where𝑣≠𝑣′, we introduce 𝑇𝜔,𝜓(Λ,Ψ)to pre-
dict the correlation between ΛandΨinspired by GNN pretrain-
ing [57, 58], formulated as:
𝑇𝜔,𝜓(Λ𝑘(𝑣),𝒙𝑣)=∑︁
(𝑝,𝑞)∈Λ𝑘(𝑣)log(𝑓𝜔([𝒉𝑝||𝒉𝑞||𝒙𝑣])), (6)
where 𝒉𝑝and𝒉𝑞are the output features of the source node 𝑝
and the target node 𝑞generated from the k-hop logical ego graph
Λ𝑘(G),𝒙𝑣=Ψ𝑘(𝑣)is the relatioinal embedding of node 𝑣.𝑓:
R|𝒉|+|𝒉|+|𝒙|→R(0,1)is a fully-connected discriminator to distin-
guish the positive pairs and negative pairs.
4.3 Training Scheme and KG Inference
4.3.1 Training objective. In this section, we briefly introduce our
complete algorithm of NORAN and present the training scheme in
Algorithm 1. After this, we obtain the model Ψ◦Γto infer expres-
sive representations of inductive triples. Given the final relational
embedding for an inductive triple learned from model Ψ◦Γ, we
conduct the link prediction with a classifier using logistic regression,
i.e.𝑝(𝒙)=𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝒘𝑇𝒙+𝑏).4.3.2 Inductive inference and complexity analysis. For inductive
inference, given any triple 𝑡=(ℎ,𝑟,𝑡)with an unseen entity ℎor
𝑡, we first create a new node 𝑣=Γ(ℎ,𝑟,𝑡)and update the original
relation network eGby adding node 𝑣to it. Then we predict the
probability of triple existence with 𝑝◦Ψ◦Γ.
In contrast with the Embedding-based Paradigm, which requires
retraining for inductive inference, our proposed framework only
needs updating the relation network and then inferring with the
trained model 𝑝◦Ψ◦Γ. Formally, the inference time complexity of
our proposed framework is:
O(3𝑏𝑓2
|{z}
Γ+𝑏𝑑𝐿𝑓+𝑏𝐿𝑓2
|         {z         }
Ψ),
where𝑏is the number of inductive triples; 𝑓is the hidden dimension
assuming all embeddings (relation, entity, and triple) have the same
dimension; 𝑑is the averaged node degree in relation network eG;
and𝐿is the number of layers of Ψ. Notably, we omit the time
complexity of 𝑝and relation network renewal, since both of which
are smaller than the others by magnitudes. Therefore, the main
inference time overhead lies on the message passing operation of
Ψ, which could exponentially grow with 𝐿. However, based on our
empirical results, a two-layer GNN could generally achieve the on-
par performance given the naturality of the local view for GNNs.
This is also consistent with the argument for deepening GNNs [ 59].
4.4 GNN Instantiations Selection
Our proposed framework is model-agnostic and compatible with
all GNN frameworks. One can select the most effective GNN model
by performing an exhausting search. However, it is expensive in
terms of computing resources and not the focus of this work. In this
section, we provide a valid conjecture on the selection of GNN mod-
els via a comprehensive theoretical analysis. Please note that our
evaluation space focuses on intrinsic MP layers, regardless of other
training tricks like residual connections and normalization [59].
4.4.1 General framework of MP layer. Typically, based on the con-
volution matrix, there are two categories for representative MP lay-
ers:(𝑖)Fixed MP : GCN [ 51], GraphSAGE [ 52], GIN [ 45], SGC [ 53];
(𝑖𝑖)Learnable MP : GAT [ 54]. The reformulation of typical MP lay-
ers can be found on Table 2. Recall that we actually train two GNN
models during our training process: one is Ψthat applied on relation
network for inductive inference; and the other is Ωthat applied on
logic evidence for discriminating negative samples. We unify exactly
the same architecture in our implementation for simplification.
4.4.2 Influence distribution. Our conjecture for model selection is
motivated by a general question: what type of GNNs is capable of
capturing the proper logic evidence for inductive inference ? Taking
Fig. 1(2)as an example, when we try to inference the relationship
between Martin Eberhard andCalifornia (i.e. Node 8), some existing
logic evidence, Elon Musk:WorkIn−−−−−−→ TESLA:LocatedIn−−−−−−−−→ California and
Martin Eberhard:WorkIn−−−−−−→ TESLA (i.e. Node 4,5, and 7) are more
proper than the others. Therefore, in better MP strategies, node 4,5,
and7would have more influence on the inference. Related to ideas
of sensitivity analysis and influence functions in statistics [ 60], we
4272KDD ’24, August 25–29, 2024, Barcelona, Spain Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, & Xiao Huang
measure the sensitivity of node x to node y, or the influence of y
on x with the concept of influence distribution.
Definition 3. Influence Distribution: Given a graph G=
(V,E), let𝒉(𝑖)
𝑣be the hidden features of node 𝑣at𝑖-th layer of a 𝑘-
layer GNN model Ψ. The influence score of node 𝑢on node𝑣is defined
asS(𝑢,𝑣)=Í
𝑖,𝑗"
𝜕𝒉(𝑘)
𝑢
𝜕𝒉(0)
𝑣#
(𝑖,𝑗). Then the influence distribution is
defined asS𝑣(𝑢)=S(𝑢,𝑣)/Í
𝑖∈VS(𝑖,𝑣), which is the normalization
of the corresponding influence score over all nodes.
4.4.3 Theoretical analysis. Previous studies have verified that in-
fluence distributions of common aggregation schemes are closely
connected to random walk distributions [ 60]. Extending the theo-
rem to general MP frameworks, we can get a corollary as follows.
Corollary 1. Given a graphG=(V,E), let𝒉(𝑖)
𝑣be the hidden
features of node 𝑣at𝑖-th layer of a 𝑘-layer GNN model Ψwhose
MP can be rewritten as 𝑿(𝑖+1)=𝜎 C(𝑖)𝑿(𝑖)◦𝑓(𝑘). If the convolu-
tion matrix is pre-fixed, i.e.𝜕C
𝜕𝑿(𝑖)=0, the influence distribution of
any node𝑣is in expectation equal to a biased 𝑘-step random walk
distribution, where the transition matrix is T.C=(𝑨+𝑰)·T .
Proof. Let𝑓(𝑙)(𝒙)=𝑓(𝑙)
𝑚◦𝑓(𝑙)
𝑚−1◦···◦𝑓(𝑙)
1be a𝑚-layer MLP
without learnable bias, where 𝑓(𝑙)
𝑚(𝒙)=𝜎(𝑾(𝑙)
𝑚𝒙). letC=(𝑨+
𝑰)·T , whereTis a pre-fixed weighted matrix. For any layer, by
chain rule, we have
𝜕𝒉(𝑙)
𝑢
𝜕𝒉(0)
𝑣=𝜕𝑓(𝑙)
𝑚
𝜕𝑓(𝑙)
𝑚−1·𝜕𝑓(𝑙)
𝑚−1
𝜕𝑓(𝑙)
𝑚−2···𝜕𝑓(𝑙)
2
𝜕𝑓(𝑙)
1·∑︁
𝑖∈eN(𝒖)T𝑢,𝑖𝜕𝒉(𝑙−1)
𝑖
𝜕𝒉(0)
𝑣,
where𝜕𝑓(𝑙)
𝑚
𝜕𝑓(𝑙)
𝑚−1=diag 1𝑓(𝑙)
𝑚−1>0·𝑾(𝑙)
𝑚−1. Further more, according to
chain rule and the message passing strategy, we get
𝜕𝒉(𝑘)
𝑢
𝜕𝒉(0)
𝑣=|P|∑︁
𝑝∈P1Ö
𝑙=𝑘T𝑣𝑙
𝑝,𝑣𝑙−1
𝑝𝜕𝑓(𝑙)
𝑚
𝜕𝑓(𝑙)
1,
wherePis the set of paths of length 𝑘+1from node𝑢to node𝑣.
We can express an entry of the derivative for path 𝑝as:
𝜕𝒉(𝑘)
𝑢
𝜕𝒉(0)
𝑣(𝑖,𝑗)
𝑝=1Ö
𝑙=𝑘T𝑣𝑙
𝑝,𝑣𝑙
𝑝−1Q∑︁
𝑞𝑧𝑞1Ö
𝑙=𝑘1Ö
𝑡=𝑚𝑤(𝑙)
𝑞,𝑡
,
whereQis the set of paths 𝑞from the input neuron 𝑖to the output
neuron𝑗.𝑤(𝑙)
𝑞,𝑡denotes the neuron of 𝑾(𝑙)
𝑚on path𝑞.𝑧𝑞∈{0,1}
denotes whether the path is activated. Under the assumption that
all𝑧𝑞s obeys the same Bernoulli distribution, i.e. 𝑃(𝑧𝑞=1)=𝛽, we
have:
E𝜕𝒉(𝑘)
𝑢
𝜕𝒉(0)
𝑣
=𝛽·1Ö
𝑙=𝑘1Ö
𝑡=𝑚𝑾(𝑙)
𝑡
|              {z              }
𝑢,𝑣irrelevant·|P|∑︁
𝑝∈P1Ö
𝑙=𝑘T𝑣𝑙
𝑝,𝑣𝑙
𝑝−1
.
As the former term is 𝑢,𝑣irrelevant, after normalization, the influ-
ence distribution of S𝑣(𝑢)isÍ|P|
𝑝∈PÎ1
𝑙=𝑘T𝑣𝑙
𝑝,𝑣𝑙
𝑝−1, which is exactly
a bias random walk distribution whose transition matrix is T.□It indicates that for those MP layers whose convolution matrix
is pre-fixed, the influence distribution for nodes is only structure-
relevant and irrespective to node features. For graphs that lay stress
on the topological structures, such MP strategies are beneficial.
However, for our relation network whose node contains rich rela-
tional semantics is of vital importance for inductive KGC, a pre-fixed
convolution matrix is not desired. In other words, MP layers with
a learnable convolution matrix are probable to capture the logic
evidence and learn more expressive representations for inductive
KGC. We evaluate such conjecture in Section 5.2 by comparing sev-
eral representative convolution matrices, (𝑖)learnable : GAT [ 54];
(𝑖𝑖)fixed : GraphSAGE [ 52], GIN [ 45]. The empirical results further
demonstrate our conjecture to some extent.
5 EXPERIMENTS
This section empirically evaluates the proposed framework and
presents its performance on five KG datasets. Our empirical study
is motivated by the following questions:
•Q1(5.2): How does our proposed framework perform in com-
parison with the strongest baselines, including traditional
embedding-based, rule-based methods and other MP-based
methods?
•Q2(5.3): W.r.t. Remark 1 and Remark 2, is our proposed
relation network a valid and effective modeling for inductive
KG completion?
•Q3(5.4). Is our proposed logic evidence information maxi-
mization a more effective training objective than standard
negative sampling for inductive KGC?
5.1 Experimental Setup
5.1.1 Datasets. We conduct experiments on five standard KG bench-
marks:(𝑖)FB15K-237 that is a subset of FB15K where inverse rela-
tions are removed; (𝑖𝑖)WN18RR that is a subset of WN18 where
inverse relations are removed; (𝑖𝑖𝑖)NELL995 that is extracted from
the995-th iteration of the NELL system containing general knowl-
edge;(𝑖𝑣)OGBL-WIKIKG2 that is extracted from the Wikidata
knowledge base; and (𝑣)OGBL-BIOKG that is created using data
from a large number of biomedical repositories.
5.1.2 Baselines. To valid the effectiveness of NORAN, we compare
it with the state-of-art baselines according to our best knowledge.
Following the taxonomy aforementioned in Section 2, we divide all
baselines into three categories: (𝑖)embedding-based :TransE [12],
DistMult [13],ComplEx [15],SimplE [16], and QuatE [ 61];(𝑖𝑖)
Rule-based :RuleN [26] and DRUM [28];(𝑖𝑖𝑖)MP-based :RGCN [34],
RGHAT [62],GraIL [35],ConGLR [50],PATHCON [19],RMPI [49]
andMBE [42]. Particularly, the five embedding based methods are
not naturally inductive and we retrain the embedding table for
inductive inference. For MP-based methods, we have selected the
most effective ones for KGC in the same setting. Notably, we did
not include several related works [ 39–41] in our experiments since
their original source codes were written only for KGC with unseen
relations, while our model is designed for KGC with unseen entities.
5.1.3 Implimentation details. The aforementioned five benchmark
datasets were originally developed for the transductive setting. In
other words, the entities of the standard test splits are a subset of
4273Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Main results of inductive KGC on five benchmark datasets. We underline the best results within each of the three
categories and bold NORAN’s results that are better than all baselines.
CategoriesDatasets
FB15K-237 WN18RR NELL995 OGBL-WIKIKG2 OGBL-BIOKG
Metrics
MRR Hit@1 Hit@3 MRR Hit@1 Hit@3 MRR Hit@1 Hit@3 MRR Hit@1 Hit@3 MRR Hit@1 Hit@3
Emb
. BasedTransE [12] 0.289 0.198 0.324 0.265 0.058 0.445 0.254 0.169 0.271 0.213 0.122 0.229 0.317 0.26 0.345
DistMult [13] 0.241 0.155 0.263 0.430 0.390 0.440 0.267 0.174 0.295 0.199 0.115 0.210 0.341 0.278 0.363
ComplEx [15] 0.247 0.158 0.275 0.440 0.410 0.460 0.227 0.149 0.249 0.236 0.136 0.254 0.322 0.257 0.360
SimplE [16] 0.3380.241 0.375 0.476 0.428 0.492 0.291 0.198
0.314 0.220
0.131 0.239 0.319 0.246 0.358
QuatE [61] 0.319 0.241 0.358
0.446 0.382 0.478 0.285 0.201 0.307
0.248 0.139 0.262 0.363 0.294 0.382
Rule
BasedRuleN [26] 0.453 0.387 0.491 0.514 0.461 0.532 0.346 0.279 0.366 - - - - - -
DRUM [28] 0.447 0.373 0.478 0.521 0.458 0.549 0.340 0.261 0.363 - - - - - -
MP
basedRGCN [34] 0.427 0.367 0.451 0.501 0.458 0.519 0.329 0.256 0.348 0.285 0.176 0.324 0.381 0.319 0.399
RGHAT [62] 0.440 0.361 0.483 0.518 0.460 0.540 0.337 0.274 0.351 0.301 0.192 0.329 0.395 0.334 0.418
GraIL [35] 0.465 0.389 0.482 0.512 0.453 0.539 0.3550.282 0.367
0.327 0.201 0.336 0.434 0.379 0.451
ConGLR [50] 0.463 0.402 0.483 0.512 0.452 0.541 0.352 0.276 0.366 0.318 0.219 0.338 0.422 0.365 0.431
PATHCON [19] 0.4830.425 0.499 0.522 0.462 0.546
0.349 0.276 0.369 0.339 0.243 0.347 0.457 0.395 0.472
RMPI
[49] 0.459 0.396 0.480 0.514 0.454 0.544 0.339 0.277 0.365 0.313 0.213 0.336 0.414 0.358 0.421
MBE [42] 0.477 0.410 0.495 0.519 0.451 0.549 0.344
0.270 0.359 0.331 0.234 0.345 0.452 0.380 0.470
Ours𝑎NORAN(
GS) 0.483 0.440 0.504 0.535 0.471 0.564 0.364 0.298 0.381 0.349 0.237 0.361 0.454 0.395 0.479
NORAN(GIN) 0.499 0.451 0.519 0.530
0.467 0.560 0.370 0.308 0.379 0.353 0.251 0.367 0.469 0.407 0.492
NORAN(GAT) 0.468 0.422 0.489 0.540 0.499 0.575 0.374 0.310 0.392 0.358 0.260 0.371 0.475 0.411 0.495
NORAN
- Emb.𝑏+2.2% (Avg.) +1.6% +2.6% +2.0% +1.8% +3.7% +2.9% +1.9% +2.8% +2.3% +1.9% +1.7% +2.4% +1.8% +1.6% +2.3%
NORAN - GNN𝑏+11.2% (Avg.) +16.1% +21.0 % +14.4% +6.4% +7.1% +8.3% +8.3% +10.9% +7.8% +11.0 % +12.1% +10.9% +11.2% +11.7% +11.5%
𝑎W
e test our NORAN with three backbones: GraphSAGE, GIN, and GAT, denoted as NORAN(GS), NORAN(GIN), NORAN(GAT), respectively.
𝑏We show the margin between the best results of NORAN and the ones of the two branches of baselines methods.
the entities in the training splits. In order to facilitate inductive
testing, we create new inductive benchmark datasets by sampling
disjoint subgraphs from the KGs in these datasets. Specifically, we
first sample a subset of entities from the test set at random, then
remove them and their associated edges from the training set. The
remaining training set is used to train the models, and the removed
edges are added back in during evaluation. We use MRR (mean
reciprocal rank) and Hit@1, 3 as evaluation metrics.
All baseline methods used in this paper are based on their open-
source implementations. We list the source links in Table ??. All
models are implemented in Pytorch and trained on an RTX 3090
with 24 RAM. To make a fair comparison, for all models, the embed-
ding size of entities and relations is set as 100for the input, latent,
and output layer. Exceptionally, PATHCON, which adopted the
node labeling technique to generate the node feature matrix, has a
distinctive size of input embedding that varies with datasets. We
optimize all models with Adam optimizer, where the batch size is
fixed at 256. Specially, we run all models on the large OGBL dataset
with a batch size of 32to avoid out-of-memory. We use the de-
fault Xavier initializer to initialize model parameters, and the initial
learning rate is 0.005. We apply a grid search for hyperparameter
tuning. The learning rate is searched from {0.05,0.01,0.005,0.001},
the margin parameter used in all baselines, and our negative sam-
pling training scheme from 0to1. We use their best configuration
as the default value for other distinctive hyperparameters in the
baseline methods. To reduce the randomness, we use the random
seed and report the average results of three runs.
5.2 Main Results: Q1
To answer Q1, we perform extensive experiments to compare NO-
RAN with the strongest baselines best to our knowledge. The com-
parison results are placed in Table 3. We summarize our experimen-
tal observations as follows.Obs. 1. NORAN significantly outperforms SOTA KGC baselines. We
compare NORAN (ours) with 5representative embedding-based
methods, 2rule-based methods and 7SOTA MP-based methods on
5KG datasets. As shown in Table 3, NORAN significantly outper-
forms all other baselines in all evaluation metrics (i.e., MRR, Hit@1,
Hit@3). Particularly, we bold NORAN’s results that are better than
all baseline. We evaluate NORAN with three commonly-used back-
bones, and the bold results notably take the vast majority, indicating
the effectiveness of our proposed NORAN. Besides, we compute
the margin between the best results ( underlined ) of NORAN and
the two branches of baselines by column. Notably, NORAN outper-
forms the best embedding-based (MP-based) baseline with a large
margin, averaging 11.2%(2.2%).
Obs. 2. NORAN integrating GAT (learnable convolution matrix) gen-
erally outperforms the MP layers with pre-fixed convolution matrices.
We test NORAN with three backbones: GAT, GraphSAGE, and GIN,
where the first one’s convolution matrix is learnable and the last
two pre-fix their convolution matrices. As shown in Table 3, GAT
achieves the best results ( underlined ) on four datasets while Graph-
SAGE and GIN have an on-par performance.
5.3 Ablation Study on Relation Network: Q2
According to the definition of the relation network, there are two
latent construction particulars corresponding to two construction
issues:(𝑖)the semantics of various ‘entity sharing ’ patterns (Re-
mark 1) and(𝑖𝑖)the semantics of the ‘linking direction ’ (Remark 2).
We conduct a detailed ablation study in this section to validate our
arguments in Remark 1 and 2.
Remark 1. Linking pattern. For any two triples sharing entities,
i.e.𝑇1=(ℎ1,𝑟1,𝑡1)∩𝑇2=(ℎ2,𝑟2,𝑡2), we have three linking patterns:
(𝑖)head-head sharing ( ℎ1=ℎ2),(𝑖𝑖)tail-tail sharing ( 𝑡1=𝑡2), and
(𝑖𝑖𝑖)head-tail sharing ( 𝑡1=ℎ2⊕𝑡2=ℎ1). For our construction
4274KDD ’24, August 25–29, 2024, Barcelona, Spain Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, & Xiao Huang
Table 4: Results of ablation study for relation network construction rules on three benchmark datasets.
Construction
RuleDatasets WN18RR NELL995 OGBL-WIKIKG2
Metrics
MRR Hit@1 Hit@3 MRR Hit@1 Hit@3 MRR Hit@1 Hit@3
Default
NORAN(GAT) 0.540 0.499 0.575 0.374 0.310 0.392 0.358 0.260 0.371
w
.o. head-head NORAN(GAT) 0.521 0.457 0.558 0.348 0.273 0.375 0.329 0.230 0.341
w
.o. tail-tail NORAN(GAT) 0.517 0.454 0.549 0.351 0.288 0.364 0.327 0.224 0.342
w
.o. head-tail NORAN(GAT) 0.501 0.446 0.532 0.339 0.275 0.359 0.314 0.212 0.319
Table 5: Ablation study of training objective, i.e., Logic Evidence Information Maximization (LEIM)
T
raining ObjectiveDatasets WN18RR NELL995 OGBL-WIKIKG2
Metrics
MRR Hit@1 Hit@3 MRR Hit@1 Hit@3 MRR Hit@1 Hit@3
Naiv
e
Negative SamplingNORAN(GS) 0.529 0.469 0.554 0.351 0.290 0.357 0.324 0.215 0.322
NORAN(GIN) 0.537 0.490 0.567 0.357 0.291 0.374 0.335 0.231 0.349
NORAN(GAT) 0.534 0.481 0.562 0.361 0.299 0.377 0.342 0.238 0.355
LEIM
(JSD)NORAN(GS) 0.535 0.471 0.564 0.364 0.298 0.381 0.349 0.237 0.361
NORAN(GIN) 0.530 0.467 0.560 0.370 0.308 0.379 0.353 0.251 0.367
NORAN(GAT) 0.540 0.499 0.575 0.374 0.310 0.392 0.358 0.260 0.371
LEIM
(InfoNCE)NORAN(GS) 0.532 0.476 0.558 0.358 0.294 0.369 0.331 0.220 0.343
NORAN(GIN) 0.541 0.495 0.583 0.365 0.301 0.376 0.339 0.231 0.359
NORAN(GAT) 0.544 0.506 0.581 0.367 0.306 0.383 0.337 0.232 0.352
criterion, we build links for any of the three linking patterns based on
the rationale that the three patterns possess different semantics.
Remark 2. Linking direction. According to Def. 1, for different
linking patterns discussed in Remark 1, we uniformly set the according
edges in relation network as bidirectional edges, omitting the linking
directions. The rationale here is that the semantics of relations in
knowledge graphs are not direction-sensitive. For instance, a typical
triple (Elon Musk, :WorkIn, TESLA) in Figure 1 (i) is semantically
identical to (TESLA, :Foundedby, Elon Musk).
To answer Q2, we perform a thorough ablation study for rela-
tion network construction. For remark 1, recalling that relation
networks have three patterns: head-head sharing, head-tail sharing,
andtail-tail sharing, we iteratively ablate each pattern to construct
the corresponding relation network and then use it to train our
model. We do this ablation study using GAT as the backbone. The
empirical results are shown on Table 4. As we can see, ablating
any of the three patterns from relation networks would markedly
damage the performance of NORAN, indicating the validness of
our construction rule for relation networks.
Obs. 3. Head-tail pattern weights more than the others for relation
network construction. Particularly, we observe that ablating head-
tail patterns would reduce the performance with a large margin in
comparison with the other ablations. Actually, the head-tail pat-
tern corresponds to the translation-based logic rules and dominates
multi-hop knowledge graph reasoning, while the other two cor-
responds, i.e., head-head andtail-tail logically correspond to the
union and intersection operations, respectively, as defined in com-
plex knowledge graph reasoning [ 63]. Admittedly, the experiment
results indicate that the translation-based logic evidence (head-tailpattern) are crucial for inductive inference, which is actually the
same in logic with multi-hop knowledge graph reasoning. How-
ever, according to our results, the other logic operation ( Union and
Intersection ) can also facilitate KGC, which is nevertheless ignored
by translation-based embedding methods, which, in a way, explains
their relatively worse inductive performance as shown in Table 3.
5.4 Ablation Study for Training Objective: Q3
To answer Q3, we perform an ablation study to demonstrate the
effectiveness of LEIM. Specifically, we introduce another InfoNCE
estimator [55] which is defined as follows:
I𝐼𝑛𝑓𝑜𝑁𝐶𝐸
𝜔,𝜓(Λ,Ψ)=EP 
𝑇𝜔,𝜓(Λ𝑘(𝑣),Ψ𝑘(𝑣))
−EP′
log∑︁
𝑣′∼P′𝑒𝑇𝜔,𝜓(Λ𝑘(𝑣′),Ψ𝑘(𝑣))!
.(7)
Then we evaluate LEIM with two MI estimators, i.e., JSD (our de-
fault) and InfoNCE, and compare them to the naive negative sam-
pling (NS) training objective, which is the common practice in
previous works [ 19,34]. Selected experiment results are on Table 5.
As shown in Table 5, we bold the best results by column. Obviously,
LEIM generally outperforms Naive NS with a large margin for all
three backbones, demonstrating the effectiveness of our proposed
training objective LEIM. The key difference between JSD and In-
foNCE is how negative sampling is performed. In practice, for JSD,
we perform a 1 : 1 negative sampling within a batch and compute
the expectation of the data distribution (positive samples) and a
noise distribution (negative samples) separately. For InfoNCE —
that is a typical contrastive learning style — it requires numerous
4275Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion KDD ’24, August 25–29, 2024, Barcelona, Spain
negative samples to be competitive, where the common practice
is utilizing all the other instances with a batch Bas the negative
samples. For a batch size |B|, it givesO(|B|) negative samples
per positive sample. Considering the computing efficiency and our
empirical evaluation in 5.4, we choose the JSD as our default in-
stantiation. In addition, for the inter-comparison among training
objectives, we found one interesting phenomenon:
Obs. 4. GAT generally benefits the most from LEIM. On three datasets,
Through LEIM, GAT gains a performance increase by an averaging
margin of 1.2%in MRR and 1.5%in Hit@3, which is larger than the
other backbones. Specifically, On WN18RR, GAT performs worst
among three training objectives when Naive NS trains them. How-
ever, when applying LEIM, GAT outstandingly achieves the best
result. In contrast, GIN with a prefixed convolution matrix even
has a negative boosting when trained with LEIM.
6 CONCLUSIONS AND FUTURE WORK
Inductive KG completion tries to infer relations for newly-coming
entities is more inline with reality as real-world KGs are always
evolving and introducing new knowledge. In this work, we pro-
posed a novel message-passing framework, i.e., NORAN, which
aims to mine latent relation semantics for inductive KG comple-
tion. Initially, we propose a distinctive perspective on KG modeling
termed relation network, which considers relations as indicators of
logical rules. Moreover, to preserve such logical semantics inher-
ent in KGs, we propose an innovative informax training objective,
logic evidence information maximization (LEIM). The combination
of the relation network and LEIM forms our NORAN framework
for inductive KGC. Since our framework is model-agnostic, we also
provide a detailed theoretical analysis to explore which kind of MP
model is more effective for the relation network and provide guide-
lines for model selection to enable inductive reasoning. Extensive
experiment results on five KG benchmarks demonstrate the superi-
ority of our proposed NORAN over state-of-the-art competitors. As
part of our future work, we intend to explore the potential applica-
tions of our approach in real-world scenarios, such as KG-enhanced
recommendation systems and question-answering.
7 ACKNOWLEDGEMENTS
The work described in this paper was fully supported by a grant
from the Research Grants Council of the Hong Kong Special Ad-
ministrative Region, China (Project No. PolyU 25208322).
REFERENCES
[1]Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts. Freebase: A shared database
of structured general human knowledge. In AAAI, 2007.
[2]Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas,
Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef,
Sören Auer, and Christian Bizer. Dbpedia - A large-scale, multilingual knowledge
base extracted from wikipedia. Semantic Web, 6(2):167–195, 2015.
[3]Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. YAGO3: A
knowledge base from multilingual wikipedias. In CIDR, 2015.
[4]Qing Li, George Baciu, Jiannong Cao, Xiao Huang, Richard Chen Li, Peter HF
Ng, Junnan Dong, Qinggang Zhang, Zackary PT Sin, and Yaowei Wang. Kcube:
A knowledge graph university curriculum framework for student advising and
career planning. In International Conference on Blended Learning, pages 358–369.
Springer, 2022.
[5]Wentao Li, Huachi Zhou, Junnan Dong, Qinggang Zhang, Qing Li, George Baciu,
Jiannong Cao, and Xiao Huang. Constructing low-redundant and high-accuracy
knowledge graphs for education. In International Conference on Web-Based
Learning, pages 148–160. Springer, 2022.[6]Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and
Zhimeng Jiang. Hierarchy-aware multi-hop question answering over knowledge
graphs. In Proceedings of the ACM Web Conference 2023, pages 2519–2527, 2023.
[7]Qinggang Zhang, Junnan Dong, Keyu Duan, Xiao Huang, Yezi Liu, and Linchuan
Xu. Contrastive knowledge graph error detection. In Proceedings of the 31st
ACM International Conference on Information & Knowledge Management, pages
2590–2599, 2022.
[8]Junnan Dong, Qinggang Zhang, Xiao Huang, Qiaoyu Tan, Daochen Zha, and
Zhao Zihao. Active ensemble learning for knowledge graph error detection. In
Proceedings of the Sixteenth ACM International Conference on Web Search and
Data Mining, pages 877–885, 2023.
[9]Xiaojun Chen, Shengbin Jia, and Yang Xiang. A review: Knowledge reasoning
over knowledge graph. Expert Systems with Applications, 141:112948, 2020.
[10] Qinggang Zhang, Junnan Dong, Qiaoyu Tan, and Xiao Huang. Integrating entity
attributes for error-aware knowledge graph embedding. IEEE Transactions on
Knowledge and Data Engineering, 2023.
[11] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A
survey on knowledge graphs: Representation, acquisition, and applications. IEEE
Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2021.
[12] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. Translating embeddings for modeling multi-relational data.
Advances in neural information processing systems, 26, 2013.
[13] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding
entities and relations for learning and inference in knowledge bases. arXiv
preprint arXiv:1412.6575, 2014.
[14] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity
and relation embeddings for knowledge graph completion. In Twenty-ninth AAAI
conference on artificial intelligence, 2015.
[15] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. Complex embeddings for simple link prediction. In International
conference on machine learning, pages 2071–2080. PMLR, 2016.
[16] Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in
knowledge graphs. arXiv preprint arXiv:1802.04868, 2018.
[17] Peifeng Wang, Jialong Han, Chenliang Li, and Rong Pan. Logic attention based
neighborhood aggregation for inductive knowledge graph embedding. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7152–7159,
2019.
[18] Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L Hamilton. Node-
piece: Compositional and parameter-efficient representations of large knowledge
graphs. arXiv preprint arXiv:2106.12144, 2021.
[19] Hongwei Wang, Hongyu Ren, and Jure Leskovec. Relational message passing for
knowledge graph completion. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining, pages 1697–1707, 2021.
[20] Yu Hao, Xin Cao, Yixiang Fang, Xike Xie, and Sibo Wang. Inductive link prediction
for nodes having only attribute information. arXiv preprint arXiv:2007.08053,
2020.
[21] Pengda Qin, Xin Wang, Wenhu Chen, Chunyun Zhang, Weiran Xu, and
William Yang Wang. Generative adversarial zero-shot relational learning for
knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pages 8673–8680, 2020.
[22] Baoxu Shi and Tim Weninger. Open-world knowledge graph completion. In
Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.
[23] Xing Tang, Ling Chen, Jun Cui, and Baogang Wei. Knowledge representation
learning with entity descriptions, hierarchical types, and textual relations. Infor-
mation Processing & Management, 56(3):809–822, 2019.
[24] Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Jeff Z Pan, Zhiquan Ye, Zonggang Yuan,
Yantao Jia, and Huajun Chen. Ontozsl: Ontology-enhanced zero-shot learning.
InProceedings of the Web Conference 2021, pages 3325–3336, 2021.
[25] Luis Antonio Galárraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek.
Amie: association rule mining under incomplete evidence in ontological knowl-
edge bases. In Proceedings of the 22nd international conference on World Wide
Web, pages 413–422, 2013.
[26] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla,
and Heiner Stuckenschmidt. Fine-grained evaluation of rule-and embedding-
based systems for knowledge graph completion. In International semantic web
conference, pages 3–20. Springer, 2018.
[27] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical
rules for knowledge base reasoning. Advances in neural information processing
systems, 30, 2017.
[28] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.
Drum: End-to-end differentiable rule mining on knowledge graphs. arXiv preprint
arXiv:1911.00055, 2019.
[29] Huachi Zhou, Shuang Zhou, Keyu Duan, Xiao Huang, Qiaoyu Tan, and Zailiang
Yu. Interest driven graph structure learning for session-based recommendation.
InPacific-Asia Conference on Knowledge Discovery and Data Mining, pages 284–
296. Springer, 2023.
4276KDD ’24, August 25–29, 2024, Barcelona, Spain Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, & Xiao Huang
[30] Hao Chen, Zhong Huang, Yue Xu, Zengde Deng, Feiran Huang, Peng He, and
Zhoujun Li. Neighbor enhanced graph convolutional networks for node classifi-
cation and recommendation. Knowledge-Based Systems, 246:108594, 2022.
[31] Hao Chen, Yue Xu, Feiran Huang, Zengde Deng, Wenbing Huang, Senzhang
Wang, Peng He, and Zhoujun Li. Label-aware graph convolutional networks. In
Proceedings of the 29th ACM international conference on information & knowledge
management, pages 1977–1980, 2020.
[32] Huachi Zhou, Qiaoyu Tan, Xiao Huang, Kaixiong Zhou, and Xiaoling Wang.
Temporal augmented graph neural networks for session-based recommendations.
InProceedings of the 44th International ACM SIGIR conference on research and
development in information retrieval, pages 1798–1802, 2021.
[33] Hao Chen, Wenbing Huang, Yue Xu, Fuchun Sun, and Zhoujun Li. Graph un-
folding networks. In Proceedings of the 29th ACM International Conference on
Information & Knowledge Management, pages 1981–1984, 2020.
[34] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. Modeling relational data with graph convolutional
networks. In European semantic web conference, pages 593–607. Springer, 2018.
[35] Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction
by subgraph reasoning. In International Conference on Machine Learning, pages
9448–9457. PMLR, 2020.
[36] Changjian Wang, Xiaofei Zhou, Shirui Pan, Linhua Dong, Zeliang Song, and Ying
Sha. Exploring relational semantics for inductive knowledge graph completion.
2022.
[37] Jiajun Chen, Huarui He, Feng Wu, and Jie Wang. Topology-aware correlations
between relations for inductive link prediction in knowledge graphs. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6271–6278,
2021.
[38] Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. Indigo: Gnn-based
inductive knowledge graph completion using pair-wise encoding. Advances in
Neural Information Processing Systems, 34:2034–2045, 2021.
[39] Yuxia Geng, Jiaoyan Chen, Wen Zhang, Jeff Z. Pan, Mingyang Chen, Huajun
Chen, and Song Jiang. Relational message passing for fully inductive knowledge
graph completion, 2022.
[40] Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang. Ingram: Inductive
knowledge graph embedding via relation graphs. arXiv preprint arXiv:2305.19987,
2023.
[41] Jiarui Jin, Yangkun Wang, Kounianhua Du, Weinan Zhang, Zheng Zhang, David
Wipf, Yong Yu, and Quan Gan. Inductive relation prediction using analogy
subgraph embeddings. In International Conference on Learning Representations.
[42] Yuanning Cui, Yuxin Wang, Zequn Sun, Wenqiang Liu, Yiqiao Jiang, Kexin Han,
and Wei Hu. Inductive knowledge graph reasoning for multi-batch emerging
entities. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management, pages 335–344, 2022.
[43] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks.
Advances in Neural Information Processing Systems, 2018.
[44] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Revisiting graph
neural networks for link prediction. arXiv preprint arXiv:2010.16103, 2020.
[45] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are
graph neural networks? arXiv preprint arXiv:1810.00826, 2018.
[46] Komal K. Teru, Etienne Denis, and William L. Hamilton. Inductive relation
prediction by subgraph reasoning. arXiv: Learning, 2020.
[47] Chen Shengyuan, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Dif-
ferentiable neuro-symbolic reasoning on large-scale knowledge graphs. Advances
in Neural Information Processing Systems, 36, 2024.[48] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. Cycle repre-
sentation learning for inductive relation prediction. In International Conference
on Machine Learning, pages 24895–24910. PMLR, 2022.
[49] Yuxia Geng, Jiaoyan Chen, Jeff Z Pan, Mingyang Chen, Song Jiang, Wen Zhang,
and Huajun Chen. Relational message passing for fully inductive knowledge
graph completion. In 2023 IEEE 39th International Conference on Data Engineering
(ICDE), pages 1221–1233. IEEE, 2023.
[50] Qika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu, Lingling Zhang, and Tianzhe
Zhao. Incorporating context graph with logical reasoning for inductive relation
prediction. In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 893–903, 2022.
[51] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[52] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation
learning on large graphs. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 1025–1035, 2017.
[53] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. Simplifying graph convolutional networks. In International confer-
ence on machine learning, pages 6861–6871. PMLR, 2019.
[54] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint
arXiv:1710.10903, 2017.
[55] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by
mutual information estimation and maximization. arXiv preprint arXiv:1808.06670,
2018.
[56] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative
neural samplers using variational divergence minimization. In Proceedings of
the 30th International Conference on Neural Information Processing Systems, pages
271–279, 2016.
[57] Qi Zhu, Yidan Xu, Haonan Wang, Chao Zhang, Jiawei Han, and Carl Yang. Trans-
fer learning of graph neural networks with ego-graph information maximization.
arXiv preprint arXiv:2009.05204, 2020.
[58] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-
gnn: Generative pre-training of graph neural networks. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
pages 1857–1867, 2020.
[59] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang,
Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural
networks: A comprehensive benchmark study. arXiv preprint arXiv:2108.10521,
2021.
[60] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with
jumping knowledge networks. In International Conference on Machine Learning,
pages 5453–5462. PMLR, 2018.
[61] Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embed-
dings. In NeurIPS, 2019.
[62] Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, Zhiping Shi, Hui Xiong, and Qing
He. Relational graph neural network with hierarchical attention for knowledge
graph completion. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pages 9612–9619, 2020.
[63] Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over
knowledge graphs in vector space using box embeddings. arXiv preprint
arXiv:2002.05969, 2020.
4277