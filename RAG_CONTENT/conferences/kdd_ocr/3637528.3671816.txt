ReCTSi: Resource-efficient Correlated Time Series Imputation via
Decoupled Pattern Learning and Completeness-aware Attentions
Zhichen Lai
zhla@cs.aau.dk
Department of Computer Science,
Aalborg University
Aalborg, DenmarkDalin Zhang‚àó
dalinz@cs.aau.dk
Department of Computer Science,
Aalborg University
Aalborg, DenmarkHuan Li‚àó
lihuan.cs@zju.edu.cn
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
Dongxiang Zhang
zhangdongxiang@zju.edu.cn
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, ChinaHua Lu
luhua@ruc.dk
Department of People and
Technology, Roskilde University
Roskilde, DenmarkChristian S. Jensen
csj@cs.aau.dk
Department of Computer Science,
Aalborg University
Aalborg, Denmark
ABSTRACT
Imputation of Correlated Time Series (CTS) is essential in data pre-
processing for many tasks, particularly when sensor data is often
incomplete. Deep learning has enabled sophisticated models that
improve CTS imputation by capturing temporal and spatial patterns.
However, deep models often incur considerable consumption of
computational resources and thus cannot be deployed in resource-
limited settings. This paper presents ReCTSi (Resource-efficient
CTS imputation), a method that adopts a new architecture for de-
coupled pattern learning in two phases: (1) the Persistent Pattern Ex-
traction phase utilizes a multi-view learnable codebook mechanism
to identify and archive persistent patterns common across different
time series, enabling rapid pattern retrieval during inference. (2)
the Transient Pattern Adaptation phase introduces completeness-
aware attention modules that allocate attention to the complete and
hence more reliable data segments. Extensive experimental results
show that ReCTSi achieves state-of-the-art imputation accuracy
while consuming much fewer computational resources than the
leading existing model, consuming only 0.004% of the FLOPs for
inference compared to its closest competitor. The blend of high
accuracy and very low resource consumption makes ReCTSi the
currently best method for resource-limited scenarios. The related
code is available at https://github.com/ryanlaics/RECTSI.
CCS CONCEPTS
‚Ä¢Information systems ‚ÜíData mining; Spatial-temporal sys-
tems.
‚àóCorresponding Authors
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671816
‚Ä¶TransientPatternAdaptationPersistentPatternExtraction(b) Decoupled Pattern Learning(a) Classical End-to-endPattern LearningCTS DataInput‚Ä¶‚Ä¶ImputedCTS Data
‚Ä¶‚Ä¶‚Ä¶SpatialPatternTemporalPatternSpatio-temporalPatternSharedComputational ElementsFigure 1: Two CTS imputation architectures in inference: (a)
Classical end-to-end pattern learning, incorporating mul-
tiple complex spatial and temporal operators. (b) The pro-
posed decoupled pattern learning architecture, featuring the
sharing of persistent patterns and lightweight adaptation of
transient patterns to improve resource-efficiency.
KEYWORDS
Correlated Time Series, Time Series Imputation, Spatio-temporal
Data, Neural Network
1 INTRODUCTION
Due to the widespread and increasing digitization of processes,
Correlated Time Series (CTS), derived from multiple sensors that
monitor processes simultaneously are increasingly available and
play a crucial role in diverse applications such as traffic manage-
ment [ 31,40] and predictive maintenance [ 9]. CTSs are often in-
complete: data is missing due to sensor malfunctions [ 36], net-
work problems [ 33], temporary obstructions [ 8], or other infras-
tructure failures. Missing data may compromise both real-time
monitoring functionality and the reliability of long-term analy-
ses [20]. As a result, CTS (data) imputation is an active area of
research [1, 3, 11, 17, 25, 29, 30, 32, 35, 36, 38, 39].
Recent advances in CTS imputation predominantly utilize Deep
Learning (DL) techniques that are known for their impressive ac-
curacy (see Section 2) [ 36]. However, existing deep imputation
methods have long inference latency and demand considerable
 
1474
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhichen Lai et al.
computational resources, including GPU/CPU and memory (see
Tables 1 and 2). These shortcomings are particularly acute in set-
tings where low-latency data processing is required, such as in
emergency response systems [ 16], and they are further amplified
in resource-limited edge settings. Thus, the deployment of existing
methods is challenging in many settings.
This study aims to extend the deployability of CTS imputation
by designing a deep model capable of state-of-the-art (SOTA) accu-
racy while consuming substantially fewer computational resources.
Despite recent efforts to make DL models for various tasks more
lightweight [ 18,27,41], achieving accurate CTS imputation while
consuming reduced resources is non-trivial, due to the following
three main limitations:
‚Ä¢L1 (Rigid imputation backbone). Current deep imputation
methods [ 1,3,11,17,25,29,30,32,35‚Äì39] often employ an end-
to-end backbone, as depicted in Figure 1 (a). This backbone en-
compasses multiple temporal and spatial pattern learning mod-
ules, the aim being to create a pure deep model with standard DL
computations starting from the raw input and proceeding all the
way to the final result. However, such a backbone inevitably in-
volves redundant computations related to extracting pre-existing
knowledge, thus causing unnecessary computational overhead.
‚Ä¢L2 (Inefficient use of periodic information in CTS). While
integrating periodic information, such as time-of-the-day and
day-of-the-week, has proven effective at improving model perfor-
mance [ 12,26], existing models simply treat this readily available
information in the same manner as they deal with latent infor-
mation that requires pattern extraction in every new input. As
a result, existing models use unnecessarily high computational
resources, making them less practical.
‚Ä¢L3 (Inefficient iterative imputation scheme). Due to their
success at generative tasks, diffusion models have been adopted
for CTS imputation [ 29,35,37]. Imputation based on diffusion
models generates missing values through an interactive process,
progressing from coarse-grained to fine-grained estimation. De-
spite its remarkable accuracy [ 29,35,37], this form of imputa-
tion inherently consumes substantial computational resources,
as each iteration involves a complete computation pass of the
model. The use of this form of imputation makes it challenging
to achieve high accuracy with low computational costs.
In response to these limitations, we propose ReCTSi (Resource-
efficient CTS imputation via decoupled pattern learning and com-
pleteness-aware attentions). ReCTSi‚Äôs primary innovation is its
redesign of the conventional end-to-end imputation backbone, iso-
lating the components that process redundant CTS knowledge (ad-
dressing limitation L1). Thus, ReCTSi is equipped with a decoupled
pattern learning architecture, as depicted in Figure 1 (b), dividing
pattern learning into two distinct phases: 1) the Persistent Pattern
Extraction (ppe) phase serves as a reusable component, extract-
ing general latent patterns; 2) the Transient Pattern Adaptation
(tpa) phase then adapts the specific CTS input window, captur-
ing transient patterns shaped by possible data dynamics, drifts,
or anomalies. This decoupled design increases the adaptability of
CTS pattern learning and allows for tailored strategies to optimize
computations in each phase. The decoupled learning architecture
is covered in Section 4.1.
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶PersistentT-patternsTransientT-patternST-pattern‚Ä¶Timestamp
Time Seriesthe ùëó-th time seriesWeeklyDaily‚Ä¶MonMon12:0012:00S-pattern12:0013:00‚Ä¶the ùëñ-th time seriesFigure 2: Multi-view patterns in CTS.
We identify three sorts of patterns in CTS: spatial, temporal, or
spatio-temporal and associate each with persistent and transient
components, as shown in Figure 2. Temporal patterns encompass
time-related features; persistent ones recur periodically (e.g., weekly
traffic flow variations), while transient ones are local and irregular
(e.g., traffic spikes due to particular events). Spatial patterns con-
cern relationships among time series; persistent spatial patterns
exhibit stable and general correlations (e.g., similar traffic trends
on adjacent road segments), and transient spatial patterns capture
short-term dynamics (e.g., detours due to road closures). Spatio-
temporal patterns combine temporal and spatial aspects with a
focus on lagged spatial correlations; persistent ones are regular and
stable over time and space, often reflecting a cascading effect (e.g.,
congestion in city centers that spreads to arterial roads), whereas
transient types are characterized by varying dynamics and irreg-
ular correlations (e.g., traffic changes due to road maintenance).
We propose a set of specialized techniques to achieve resource-
efficient learning of both persistent and transient patterns for CTS
imputation.
Theppephase utilizes a Multi-view Learnable Codebook (MvLC)
mechanism to materialize persistent patterns that can be reused
among CTS input windows, providing immediate pattern access
during inference. MvLC covers three kinds of persistent patterns:
Persistent Temporal-pattern (PT-pattern), Persistent Spatial-pattern
(PS-pattern), and Persistent Spatio-temporal-pattern (PST-pattern).
Notably, PT-patterns effectively consolidate periodic information
into concise, reusable representations (addressing limitation L2).
PS-patterns identify stable correlations across different time series,
while PST-patterns target correlations among neighboring times-
tamps across different time series. To further condense codebooks
and improve resource efficiency, a Pattern Compression and Fu-
sion (PCF) technique is designed to amalgamate the three types of
patterns. The details of MvLC and PCF are provided in Section 4.2.
Thetpaphase employs Completeness-aware Attention (CaA)
specifically designed for the imputation task. They endeavor to
recognize transient patterns, namely Transient Spatial-patterns (TS-
patterns) and Transient Temporal-patterns (TT-patterns), which are
key to understanding dynamic spatial and temporal correlations1.
Self-attentions are adopted for their superior learning capability
as well as tailor-ability. To be specific, completeness information
(missing or not) of CTS is embedded in CaA to allocate attention
1Using Transient Spatio-Temporal patterns (TST-patterns) incurs quadratic space and
time complexity with worse accuracy, indicating challenges in learning complex TST-
patterns through a single module, excluding them from consideration in the design of
CaA.
 
1475ReCTSi: Resource-efficient Correlated Time Series Imputation KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
more to those complete and thus more reliable data segments. More-
over, grouped convolutions [ 21] are applied to the feed-forward
network of CaA for further overhead refinement. When combined,
these elements enable effective, fine-grained pattern learning while
consuming fewer computational resources (addressing limitation
L3). The CaA mechanism and its utility are detailed in Section 4.3.
Having covered training of ReCTSi in Section 4.4, Section 4.5
presents a detailed analysis of ReCTSi‚Äôs time and space complexities,
benchmarking it against the traditional end-to-end pattern learning
architecture. Section 5 reports on extensive efficiency and effec-
tiveness evaluations on five datasets spanning different domains,
offering evidence that ReCTSi is capable of superior imputation
accuracy while using only 0.004% of the FLOPs resource consumed
by the model closest in performance.
2 RELATED WORK
Deep Models for CTS Imputation. CTS imputation has shifted
from traditional methods such as MEAN, MF(Matrix Factorization),
andMICE (Multivariate Imputation by Chained Equations) [ 38] to
deep learning-based models (i.e., deep models). One school of deep
models is based on the common framework used for CTS forecast-
ing. Early instances leverage primarily temporal information, e.g.,
BRITS [3], a bidirectional-RNN model that incorporates information
from both past and future timestamps. More recent deep models con-
sider both spatial and temporal information [ 1,10,17,25,39], using
graph neural networks (GNNs)[ 1,17,25] and self-attention mech-
anisms [ 10] as typical modules for spatial information extraction.
Another school relies on generative schemes, such as GANs [ 30,32]
and VAEs [ 11,36]. They also involve spatial and temporal infor-
mation extraction modules, like GNNs [ 36] and RNNs [ 32]. Finally,
some recent studies adopt diffusion models [ 29,35,37], which rely
on an iterative scheme, to achieve performance at various genera-
tive tasks.
These deep models are capable of impressive accuracy, but at
the expense of computational resource consumption and latency
(see evaluations in Tables 1 and 2). In contrast, ReCTSi targets both
accuracy and efficiency to extend deployability.
Lightweight Deep Models. The evolution of lightweight deep
models is primarily motivated by the pressing need for edge com-
puting applications. Two main strategies exist [ 5]: designing new,
inherently lightweight models or compressing large, pre-existing
models through techniques like knowledge distillation. Our study
is aligned mostly with the former, focusing on CTS imputation, for
which large pre-trained models are uncommon.
The computer vision community has been at the forefront of
developing lightweight deep learning techniques such as depth-
wise separable convolution [ 7,14], linear transformations [ 13], and
module scaling [ 34]. However, their proficiency in local feature
extraction from images or videos through 2D or 3D convolutions
does not extend to CTS with complex spatio-temporal characteris-
tics. This calls for more specialized approaches. Lightweight deep
learning techniques for CTS have been proposed recently but for
different tasks. Lai et al. [ 24] target lightweight CTS forecasting
that aims to predict future values, whereas CTS imputation han-
dles missing data of both past and future. Next, David et al. [ 2]present a lightweight CTS classification framework using ensem-
ble distillation, which however does not support CTS imputation.
Liu et al. [ 28] use an adaptive embedding method to make vanilla
transformer SOTA for traffic forecasting. Cheng et al. [ 6] propose a
lightweight semi-supervised learning model for online wind turbine
icing detection to improve CTS representation learning efficiency.
This model, consisting of only three CNN layers with the channel
calibration attention module, reduces labeling efforts and addresses
the challenge of highly imbalanced data in a typical CTS application.
Recently, Lai et al. [ 23] propose an efficient-yet-effective contrastive
leaning-based method for unsupervised state detection in CTS.
Unlike existing proposals, ReCTSi is a comprehensive light-
weight method tailored to CTS imputation. It is composed of novel
techniques MvLC and CaA that leverage the unique characteristics
of CTS and enhance performance specific to imputation, respec-
tively.
3 PROBLEM FORMULATION
Definition 1 (Correlated Time Series, CTS). Consider a set of
Nsensors, where each sensor generates a timestamped data sequence,
constituting a time series. These series can be represented as X‚ààRN√óT,
where Xùëñ,ùë°denotes the data point from the ùëñ-th sensor at timestamp
ùë°, withùëñ=1,2,..., Nandùë°=1,2,..., T. Here, Tis the total number
of timestamps. This data is referred to as Correlated Time Series
(CTS) [ 40], highlighting the presence of interdependencies among data
points across timestamps and devices.
Two correlations exist in CTS: temporal correlations, which
occur within an individual time series, and spatial correlations,
which occur across time series. Temporal correlations thus occur
in consecutive measurements in a single time series, reflecting the
continuous nature of the monitored phenomena. In contrast, spa-
tial correlations occur in concurrent measurements from different
sensors, often influenced by factors like the geographical prox-
imity among sensors. For instance, traffic flow data collected by
sensors on interconnected road segments generally exhibit spatial
correlations [40].
Definition 2 (CTS Imputation). Consider raw CTS X‚ààRN√óT
with missing values indicated by a binary mask matrix M‚àà{0,1}N√óT
(‚Äú0" denoting missing and ‚Äú1", otherwise). CTS imputation aims to
estimate the true values of all missing values in X, thereby enhancing
the data quality and its utility in downstream tasks. The imputation
replaces only missing values, as expressed by:
ÀÜX=X‚äôM+g(X|M)‚äô( 1‚àíM), (1)
where‚äôdenotes element-wise multiplication and g(¬∑)is an imputa-
tion function that estimates missing values.
Accurate CTS imputation hinges on the ability to extract spatial
and temporal correlations from incomplete data. Deep methods,
effective at capturing such correlations, are dominant despite their
computational costs. As formulated next, this study aims to enable
much more efficient imputation.
Problem (Resource-efficient CTS Imputation). The aim of
this study is to construct an imputation model ÀÜg(¬∑)that can achieve
accuracy comparable to those of SOTA imputation models, while
 
1476KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhichen Lai et al.
markedly reducing the consumption of computational resources, quan-
tified in terms of floating-point operations, FLOPs, and the amount of
model parameters.
4 CONSTRUCTION OF RECTSI
We present the overall decoupled architecture of ReCTSi in Sec-
tion 4.1. Then we detail the two phases, ppeandtpa, ofReCTSi
for persistent pattern learning and transient pattern adaptation in
Sections 4.2 and 4.3, respectively. Next, we cover the training of
ReCTSi in Section 4.4. Finally, a complexity study is provided in
Section 4.5.
4.1 The Decoupled Architecture of ReCTSi
In contrast to traditional deep CTS imputation models that rely
heavily on an end-to-end backbone, ReCTSi employs a novel decou-
pled pattern learning architecture that represents a rethinking of the
pattern extraction process. CTS typically encompasses two types of
patterns: persistent patterns, capturing how data points evolve in
relation to static information, such as temporal cycles and the geo-
graphic location of time series, and transient patterns, describing
fluctuations at specific timestamps and semantic spatial correlations.
Conventional approaches [ 1,3,11,17,25,29,30,32,35,36,38,39]
do not distinguish between these two types of patterns and extract
both types of patterns together using an end-to-end model. How-
ever, since persistent patterns recur over time, they can be retained
and reused, in contrast to transient patterns that vary over time. In
line with these considerations, as illustrated in Figure 3, ReCTSi
encompasses two distinct pattern learning phases: ppeandtpa.
Theppephase focuses on representing persistent patterns that
can be retained and reused. To achieve this, we design a novel
Multi-view Learnable Codebook (MvLC) mechanism, detailed in
Section 4.2. We follow the mechanism proposed by [ 22], utilizing
prior knowledge to design learnable node embeddings that stores
multiple persistent features. This mechanism learns and stores
representations of three subtle views of persistent patterns during
training, allowing for easy retrieval during imputation via periodic
information updates. This not only reduces computational costs
but also optimizes memory consumption.
Next, transient patterns, such as local data drifts or anomalies,
also occur in CTS. To capture such patterns, ReCTSi incorporates a
dedicated tpaphase, detailed in Section 4.3. This phase utilizes an
innovative Completeness-aware Attention (CaA) mechanism that
enables increased focus on complete and reliable data segments for
both temporal and spatial transient information extraction. This
mechanism enhances the method‚Äôs ability to infer missing data,
in contrast to a naive application of the attention mechanism that
disregards the specifics of CTS imputation.
4.2 Persistent Pattern Extraction (ppe)
This phase utilizes primarily the MvLC to extract persistent pat-
terns from three distinct views: the Persistent Temporal Pattern
(PT-pattern), the Persistent Spatial Pattern (PS-pattern), and the
Persistent Spatio-temporal Pattern (PST-pattern) through learnable
codebooks (see Section 4.2.1). Additionally, a Pattern Compression
and Fusion (PCF) module (see Section 4.2.2) is incorporated to fur-
ther compress the persistent patterns and fuse them seamlessly withraw CTS, facilitating the subsequent transient pattern adaptation
phase.
4.2.1 MvLC Mechanism. TheLearnable Codebook serves as the
foundation for MvLC. It functions as a dictionary that maps input
information to fixed-length pattern tensors. In contrast to conven-
tional codebooks, our learnable codebook involves random initial-
ization of the tensors based on specific patterns, which are subse-
quently learned jointly with the remaining part of the model and
are saved after training. During the imputation, the saved pattern
tensors can be retrieved via table look-ups as an alternative to
the recomputation done by many existing end-to-end models. In
essence, a learnable codebook is formalized as:
CB:{ùëò‚Üíùë£|ùëò‚ààùêæ,ùë£‚ààùëâ}, (2)
where each key value ùëòfrom key set ùêæcorresponds to a unique
learnable tensor ùë£from value set ùëâmapped by the codebook CB.
By enabling the learning of pattern tensors, the same information
can be encoded in different ways according to different tasks and
fluctuation patterns.
PT-pattern Codebook (CBPT): Persistent Temporal Patterns (PT-
patterns) represent periodic information that can employ fixed
pattern representation rules for all temporal segments. For instance,
information such as day-of-the-week (DoW) or time-of-the-day
(ToD) is inherently periodic, and its representation should remain
consistent for any given day of the week or time of day. Therefore,
PT-patterns can be embedded using the following codebook:
CBPT:{ùëòPT‚ÜívPT|ùëòPT‚ààùêæPT,vPT‚ààVPT}, (3)
where keyùëòPT=[DoW,ToD,...]amalgamates all periodic pieces
of information, and vPT‚ààRdùë°represents the corresponding PT-
pattern tensor. We assign a PT-pattern tensor vPT
ùë°to each timestamp
ùë°and broadcast it across different time series. This is done because
different timestamps possess distinct periodic information, whereas
the data from different time series at the same timestamp share the
same periodic information. Thus, the PT-pattern of a CTS segment
is represented as follows:
EPT=[vPT
ùë°|ùë°=ùë°‚Üíùë°+T‚àí1]√óN,where EPT‚ààRN√óT√ódùë°(4)
PS-pattern Codebook (CBPS): Similar to the PT-pattern codebook,
the Persistent Spatial Pattern (PS-pattern) codebook aims to capture
patterns that remain constant over time but vary spatially. This
could be geographical location or identity information of time series.
Therefore, value tensor vPSùëõ‚ààRdùë†only distinguishes across distinct
time series, and the key ùëòPSof the codebook CBPSis the time series
identity. These values are then broadcast across timestamps to
achieve the PS-pattern:
EPS=[[vPS
ùëõ]‚ä§|ùëõ=1‚ÜíN],where EPS‚ààRN√óT√ódùë† (5)
PST-pattern Codebook (CBPST): Finally, the MvLC also incorpo-
rates a comprehensive spatio-temporal view. Each data point has
a unique spatio-temporal identity, acting as the key ùëòPST, and the
corresponding value tensor vPST
ùë°,ùëõ‚ààRdùë†ùë°concerns a data point. The
Persistent Spatio-Temporal (PST-pattern), EPST‚ààRN√óT√ódùë†ùë°, is then
achieved as follows:
EPST=[[vPST
ùë°,ùëõ|ùëõ=1‚ÜíN]|ùë°=ùë°‚Üíùë°+T‚àí1] (6)
 
1477ReCTSi: Resource-efficient Correlated Time Series Imputation KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Completeness-aware Temporal AttentionCompleteness-aware Spatial AttentionOutput ModuleùêÑ!"
üîç‚®Å
"ùêóPersistent Pattern Extraction (PPE)Transient Pattern Adaptation (TPA)ùêó#ùêó$√óL
Pattern CompressionandFusionùêÑ!!%Raw Input Embeddingùêà#‚àà‚Ñù"√ó"ùêà$‚àà‚Ñù'√ó'ùêÑ!("PST-patternsùêÇùêÅ)$#ùêÑ!(‚®ÅTemporal PESpatial PE
üîçLookup‚®Å Addition ùêó‚àà‚Ñù'√ó"InputPT-patternsùêÇùêÅ)#PS-patternsùêÇùêÅ)$MvLC Mechanism
Outputùêå‚àà‚Ñù'√ó"Concatenation 
Figure 3: Overall Architecture of ReCTSi. Notably, the count of CaA modules, denoted as L, is typically set to either 1 or 2.
The codebooks CBPSandCBPSTuse identities as keys, requiring
no extra input information (see Figure 3). Next, we fuse the per-
sistent patterns from the three views to decrease redundancy and
improve the efficiency of transient pattern adaptation.
4.2.2 Pattern Compression and Fusion (PCF). To emphasize more
significant persistent patterns, we concatenate all three views of per-
sistent patterns along the feature embedding dimension and then
feed them into a squeeze-and-excitation (SE) module [ 15]. Specifi-
cally, a concatenated pattern is first squeezed with a global average
pooling operation on each time series:
E‚ó¶
ùëöùë£ùëôùëê=GlobalAvgPool(Eùëöùë£ùëôùëê), (7)
where Eùëöùë£ùëôùëê=[EPT,EPS,EPST]‚ààRN√óT√ó(dùë°+dùë†+dùë†ùë°). The squeezed
values are then activated by passing them through a non-linear
transformation to capture spatial-wise dependencies:
ùëÜ=ùúé(Wùë†2¬∑ReLU(Wùë†1¬∑E‚ó¶
ùëöùë£ùëôùëê)) (8)
Here, Wùë†1andWùë†2are learnable weights for dimension expansion
and compression, and a sigmoid function ùúé(¬∑)is applied to obtain
spatial-wise scores. The computed importance scores are used to
re-weight the original patterns by performing spatial-wise scaling:
ECF=ùëÜ¬∑Eùëöùë£ùëôùëê (9)
The resulting embedding ECFis simply compressed via a linear
transformation and is then merged with the linearly transformed
raw CTS Xin preparation for the transient pattern adaptation.
Eppe=[Linear(X),Linear(ECF)],where Eppe‚ààRN√óT√ód(10)
4.3 Transient Pattern Adaptation (tpa)
In contrast to persistent patterns that capture the overall behavior
of an CTS, transient patterns represent local data shifts deviating
from routine fluctuations. The tpaphase is designed to capture
such local dynamics. The conventional attention mechanisms target
general purposes and are purely data-driven, learning attention
scores without incorporating prior assumptions about the input.
While achieving SOTA performance in purely data-driven settings,
the addition of auxiliary information changes the setting and offers
ùëä!
Weighted Sum
GFFN
‚®ÅùëÑ ùêæ
ùëâ
ùêò"ùëä# ùëä$Skip Connection0 0 0 0
0 1 1 0
1 1 1 1ùêå‚àà‚Ñù!√ó#
ùêà"‚àà‚Ñù%√ó%0
1
PETimestamps
Time Series
0.5
0.33 0.67 0.67 0.330
10.50
10.50
10.5
0.33 0.67 0.67 0.33
0.33 0.67 0.67 0.33
0.33 0.67 0.67 0.33
0.33 0.67 0.67 0.33‚®Å
Scaled Dot Productùêó" ùõæ"‚àà‚Ñù'√ó%Average
DuplicateAverage
ùõæ(
‚àà‚Ñù)√ó'Duplicate
ùêà(
‚àà‚Ñù)√ó)
Completeness -aware
Temporal AttentionTemporal Completeness
MatrixSpatial Completeness
MatrixFigure 4: Completeness-aware Attention, using the tempo-
ral attention as an example. The method for Completeness-
aware Spatial Attention follows a similar principle but ap-
plies self-attention across spatial data slices instead.
means of improvement. We propose a novel Completeness-aware
Attention (CaA) mechanism for extracting transient spatial and
temporal patterns, which customizes conventional attention to
incorporate awareness of data completeness. This enhancement
enables better capture of transient patterns with more focused
attention on complete segments.
4.3.1 Completeness Matrices. The cornerstone of CaA is the tem-
poral and spatial completeness matrices IùëáandIùëÜ(see Figure 3).
For incomplete CTS data, data segments containing missing val-
ues contribute less to pattern learning and should be assigned less
attention. As exemplified in Figure 4, the percentages of missing
values in the first and fourth timestamps (columns) are 0.33, much
lower than in the other two timestamps. This suggests that the
features derived from these other two timestamps are less reliable
 
1478KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhichen Lai et al.
compared to those from the first two. To capture different degrees
of missing data in CTS, we introduce completeness matrices.
Given the mask matrix Massociated with an input CTS X, the
temporal completeness rate of a timestamp ùë°, defined as its percent-
age of missing values, is computed as follows:
ùõæùëá
ùë°=1
N‚àëÔ∏ÅN
ùëõ=1Mùëõ,ùë°, (11)
where Mùëõ,ùë°is the element of Mat position(ùëõ,ùë°). Analogously, we
calculate the completeness rate for all timestamps and obtain a
temporal completeness matrix:
Iùëá=[ùõæùëá
1,ùõæùëá
2,...,ùõæùëá
T]√óT(12)
Likewise, we derive a spatial completeness matrix IùëÜ:
IùëÜ=[ùõæùëÜ
1,ùõæùëÜ
2,...,ùõæùëÜ
N]√óN,whereùõæùëÜ
ùëõ=1
T‚àëÔ∏ÅT
ùë°=1Mùëõ,ùë° (13)
represents the spatial completeness rate for theùëõ-th time series, as
the percentage of missing values within that series.
4.3.2 Completeness-aware Attention. We integrate the complete-
ness matrix into the conventional self-attention mechanism to con-
struct the CaA module. The completeness matrix helps optimize
attention scores, allowing the method to better leverage more com-
plete data segments. In the following, we outline the overall CaA
mechanism and highlight the distinct operations for spatial and
temporal pattern extraction.
Self-attention mechanisms in the spatial and temporal attention
modules have computational and memory requirements that scale
quadratically with NandT, respectively. In contrast, spatio-temporal
attention scale quadratically with (N√óT). Using the PeMS dataset [ 4]
with N=64as an example, spatio-temporal attention requires 4096
times more computational and memory resources than does spatial
attention alone, which is prohibitive. To achieve a resource-efficient
model, we therefore limit the consideration of ST-patterns to the
ppephase.
Similar to conventional self-attentions, CaA initially incorpo-
rates a positional embedding (PE) to encode permutations of the
input. Given the input X‚àó(Xùëáfor temporal attentions and XùëÜ
for spatial attentions), the positional embedding is formulated as
follows:
EPE=X‚àó+WPE, (14)
where WPEis a learnable matrix capturing position information.
The resulting embedding EPEis then used to generate the Query
(ùë∏), Key ( ùë≤), and Value (ùëΩ ) components of an attention module:
ùë∏=EPEWùëÑ,ùë≤=EPEWùêæ,ùëΩ=EPEWùëâ, (15)
where ùëæ‚àó‚ààRD√óD/hare learnable weight matrices.
Unlike the conventional attention mechanism that generates
attention scores solely with ùë∏,ùë≤, andùëΩ, we propose to incorporate
the completeness vector I‚àóin Equations 12 and 13 to focus more on
complete data segments and less on incomplete ones. Specifically,
the spatial and temporal completeness-aware weighted embeddings
are:
E=Ô£±Ô£¥Ô£¥Ô£¥Ô£¥ Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥Softmax
QK‚ä§‚äô(IùëÜ)‚ä§‚àö
D/h
¬∑V,for spatial attentions
Softmax
Iùëá‚äôQK‚ä§‚àö
D/h
¬∑V, for temporal attentions(16)where‚äôis element-wise multiplication, ensuring that attentions
preferentially emphasize contributions from non-missing data.
AGrouped Feedforward Network (GFFN) with two GFFN lay-
ers [ 24] then takes the attentive embedding and performs light-
weight non-linear activation to obtain the final output:
ECaA=GFFN(GFFN(E|G2)|G1), (17)
where G‚àóis the number of groups and GFFN(¬∑|G‚àó)is a GFFN layer,
given as follows:
GFFN(E|G‚àó)=concat({ReLU(Eùëó¬∑W+b)}G‚àó
ùëó=1),
where Eùëó=E
:,:,d√ó(ùëó‚àí1)
G‚àó:d√óùëó
G‚àó(18)
As shown in Figure 3, the spatial and temporal CaA modules are
stacked to form a tpablock, and Ltpablocks with a skip connection
for each are stacked sequentially to achieve the tpaphase (primarily,
Lùëñùë†ùë†ùëíùë°ùë°ùëú 1ùëúùëü2). The final output embedding is denoted as Etpa.
4.4 Training of ReCTSi
ReCTSi processes the output of tpaand applies a non-linear trans-
formation to generate the final imputation results:
ÀÜX=ReLU(Etpa¬∑Wùëú+bùëú), (19)
where ÀÜX‚ààRN√óTis the imputed result and Wùëúandbùëúare learnable
weights and biases.
Despite its decoupled architecture, ReCTSi adopts an end-to-end
training scheme, where ppeandtpaare trained together by the
Adam optimizer. ReCTSi utilizes the Masked Mean Absolute Error
(Masked MAE) [ 3,10,36] as the loss function. This function is de-
signed specifically to optimize imputation accuracy by considering
errors only in the masked (missing) regions:
Masked MAE =√çN
ùëñ=1√çT
ùëó=1(1‚àíMùëñ,ùëó)¬∑Xùëñ,ùëó‚àíÀÜXùëñ,ùëó
√çN
ùëñ=1√çT
ùëó=1(1‚àíMùëñ,ùëó),(20)
where Xùëñ,ùëóandÀÜXùëñ,ùëórepresent the ground-truth and imputed values,
respectively, at the ùëñ-th time series and the ùëó-th timestamp.
After training, the codebooks of ppeare stored and used for
subsequent imputation inference via table look-ups, and tpais used
as a normal neural network to process each input CTS segment.
4.5 Complexity Analysis
Classical end-to-end models (see Figure 1 (a)) utilize layered ap-
proaches for extracting temporal and spatial features, including
RNNs, CNNs, and self-attention for temporal patterns, as well as
GCNs and self-attention for spatial patterns. Let Lbe the number of
stacked modules, dthe embedding size, Nthe number of time series,
andTthe window size. Then these models have time complexity
O(L¬∑d2¬∑N¬∑T)in the cases of RNN- and CNN-based modules,
O(L¬∑d¬∑N¬∑T¬∑(N+d))in the cases of GCN- and self-attention-
based spatial modules, and O(L¬∑d¬∑N¬∑T¬∑(T+d))in the cases of
self-attention-based temporal modules, and the space complexity
isO(L¬∑d2).
In contrast, ReCTSi introduces a novel decoupled architecture
for pattern learning, shown in Figure 1 (b), employs ppeto capture
persistent patterns and tpato capture transient patterns. This ap-
proach uses MvLC codebooks to optimize persistent pattern storage
 
1479ReCTSi: Resource-efficient Correlated Time Series Imputation KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
and converts persistent feature extraction during inference into an
efficientO(1)table look-up. Consequently, the space complexity
grows byO(N¬∑T¬∑d), due to the necessity to store persistent patterns
in the MvLC codebook. Nevertheless, the parameter count of the
codebook is relatively small (between 60 to 180KB for all datasets
shown in Tables 1 and 2), which our experiments confirm.
Fortpa, the space and time complexities of the CaA module align
with those of traditional self-attention mechanisms. Nonetheless,
due to the foundational work of ppe, tpaoften needs far fewer
self-attention modules, typically reducing Lto 1. This reduction,
along with a reduced embedding size (e.g., d=32ofReCTSi vs
d=64of models like PoGeVon [36] on AQ36 dataset), renders the
decoupled approach very efficient. Moreover, ReCTSi integrates
a pattern compression and fusion module to decrease further the
embedding size for the input of tpa. The computational complexity
of Grouped FFNs (GFFNs) in CaA is also optimized to(1+1/G)
2of the
case for the standard FFN counterpart [ 24], enhancing the model‚Äôs
efficiency. The experimental results presented in Tables 1 and 2 offer
evidence that ReCTSi achieves the lowest computational costs.
5 EXPERIMENTS
5.1 Experimental Setup
Datasets. We use five real-world datasets that have been exten-
sively adopted in studies [ 19,36,42] and covering domains of traffic
flow, air quality, and COVID-19 infection rates. The configurations
are in accordance with prior research [ 36]. More details can be
found in Table 3.
Metrics. We evaluate the imputation effectiveness using standard
metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE),
and Mean Relative Error (MRE), following prior research [ 36]. MAE
assigns equal weights to all errors, offering a general accuracy
measure. MSE gives more weight to severe errors, thus emphasizing
significant deviations. MRE is sensitive to errors when ground truth
values are low, offering insight into the relative accuracy. Lower
MAE, MSE, and MRE values indicate higher imputation accuracy.
When evaluating the imputation efficiency, we present standard
device-agnostic metrics during inference : FLOPs and numbers of
model parameters. These metrics align with established deep learn-
ing benchmarks [ 24,34] and are calculated using the Torchinfo
library. Additionally, we report latency and peak memory usage
(Peak Mem) as runtime-specific measures. Particularly, the PeMS
series datasets have identical context and statistical properties (see
Table 3); therefore, ReCTSi has a uniform configuration for these
PeMS datasets, and they share the same efficiency results as re-
ported in Table 1.
5.2 Overall Comparisons
Baseline. We include ten existing methods in the evaluation. These
methods were chosen to cover a broad range of approaches. Specif-
ically, MEAN, MF, and MICE [38] exemplify classical non-DL ap-
proaches. Despite their resource efficiency, these models fall short
in terms of accuracy, rendering them less desirable in many appli-
cations. Next, rGAIN [32] and PoGeVon [36] represent generative
models. They leverage advanced techniques such as GANs and
VAEs, respectively, to impute missing values. A different class of
models, including BRITS [3],TimesNet [39],GRIN [1],NET3[17],andSAITS [10], integrate layered temporal and spatial modules to
perform imputation.
Among these, SAITS [10] stands out for its attention mechanisms
that somewhat resemble those employed by ReCTSi. The remaining
models in this class employ different strategies for temporal pat-
tern recognition, including RNNs (BRITS [3],GRIN [1],NET3[17],
PoGeVon [36]), CNNs (TimesNet [39]), and MLPs (rGAIN [32])
models. Considering spatial pattern identification, GRIN [1] and
PoGeVon [36] utilize MPNNs, whereas NET3[17] employs a GCN
approach. Notably, we exclude diffusion-based models [29, 35, 37]
as their complex iterative sampling process requires hundreds of
model calls, which is far less efficient than the other models, ren-
dering them suitable only for efficiency-insensitive scenarios.
Comparison Analysis. The comparison results are presented in
Tables 1 and 2, providing evidence that ReCTSi is capable of the
best performance across various datasets. Specifically, classical non-
DL methods (MEAN, MF, and MICE) achieve the lowest accuracy,
with the deep methods exhibiting much better accuracy. For each
dataset, ReCTSi consistently achieves the lowest MAE, MSE, and
MRE values, with the only exception for MAE on COVID-19. How-
ever, the slight MAE discrepancy of 0.008 between ReCTSi and
the SOTA model in this dataset, comprised solely of integer val-
ues, is negligible. On PeMS-BA, PeMS-LA, and PeMS-SD datasets,
ReCTSi outperforms the closest competitor in terms of accuracy,
PoGeVon, by at least 11%, with evidence of exceptional computa-
tional efficiency, exhibiting much fewer FLOPs and lower latency
when compared to the existing models. For instance, in comparison
with PoGeVon with the closest accuracy, ReCTSi operates hundreds
of times faster, with a computational cost below 1/25,000 times
FLOPs. Even compared to the most efficient method, TimesNet,
ReCTSi is substantially better in terms of both FLOPs and latency,
while achieving superior accuracy. The few FLOPs and low latency
are due to ReCTSi‚Äôs novel decoupled architecture that transforms
feature extraction into fast pattern look-ups.
5.3 Ablation Study
We conduct an ablation study to assess each component of ReCTSi.
Initially, we evaluate each codebook individually by systematically
removing them to create ReCTSi\PT,ReCTSi\PS, and ReCTSi\PST.
Additionally, we scrutinize the role of the PCF module (refer to Sec-
tion 4.2.2) by introducing ReCTSi\CF, a variant with PCF removed,
and codebook embeddings directly concatenated. Furthermore, to
explore the impact of CaA components (refer to Section 4.3), we
create variants ReCTSi\CM, ReCTSi\SA, and ReCTSi\TA, removing
the completeness matrices, spatial attention, and temporal attention,
respectively.
The results in Table 4 show a substantial contribution of each
component in ReCTSi. The variants without codebooks (ReCTSi \PT,
ReCTSi\PS, and ReCTSi\PST) exhibit elevated error metrics, em-
phasizing the crucial role of the codebooks at capturing multi-view
complex patterns. Notably, the removal of the PST-pattern code-
book results in the highest errors, underscoring the crucial role
of investigating cross spatio-temporal patterns. The variant with-
out the PCF module (ReCTSi \CF) shows increased errors and re-
source usage, emphasizing the module‚Äôs effectiveness at pattern
 
1480KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhichen Lai et al.
Table 1: Performance comparison over three traffic datasets (lower values indicate better performance for all metrics).
Mo
delEfficiency Metrics PeMS-BA PeMS-LA PeMS-SD
FLOPs
(M) Params (K) Peak Mem (MB) Latency (ms) MAE MSE MRE MAE MSE MRE MAE MSE MRE
MEAN ‚àñ
‚àñ ‚àñ ‚àñ 192.047 47504.159 0.474 216.681 62664.657 0.406 208.192 55780.002 0.529
MF ‚àñ ‚àñ ‚àñ ‚àñ 57.265 8091.407 0.103 77.339 15202.678 0.145 45.811 6044.345 0.117
MICE ‚àñ ‚àñ ‚àñ ‚àñ 50.861 6724.148 0.126 64.018 10822.355 0.120 38.978 4771.186 0.100
BRI
TS 358 576 2.69 40.65 30.274 2942.411 0.075 36.921 3681.595 0.069 21.232 1563.234 0.054
rGAIN 5880 2280 5.02 18.83
38.862 3422.914 0.096 49.611 5533.964 0.093 33.212 2341.466 0.085
SAITS 1.43 1425
7.35 45.35 46.567 5412.574 0.115 61.896 10998.854 0.116 34.117 4101.397 0.087
TimesNet 99 1399 6.24 10.69 25.859
1676.843 0.064 27.452 2058.227 0.052 21.583 1284.300 0.055
GRIN 286 195 12.62
127.81 30.057 1922.072 0.074 47.835 4561.512 0.090 41.001 3000.012 0.105
NET3‚ô£‚àñ ‚àñ ‚àñ 130.12 35.671 2735.574 0.089 37.652 3416.784 0.071 34.111 2487.581 0.087
PoGeVon 5030 322 16.08 172.82 22.194 1248.681 0.055 23.905 1714.962 0.045 18.990 951.559 0.048
ReCTSi 0.06
83 12.54 1.79 18.998 1050.328 0.046 21.130 1469.592 0.040 15.024 721.439 0.037
‚ô£Due to incompatibility between NET3and the Torchinfo library, we have issues in obtaining NET3‚Äôs FLOPs, Params, and Peak Mem. Nonetheless, considering its
significantly higher latency and lower accuracy metrics compared to ReCTSi, NET3is not competitive.
T
able 2: Performance comparison over the AQ36 and COVID-19 datasets.
Mo
delAQ36 COVID-19
FLOPs
(M) Params (K) Peak Mem (MB) Latency (ms) MAE MSE MRE FLOPs (M) Params (K) Peak Mem (MB) Latency (ms) MAE MSE MRE
MEAN ‚àñ
‚àñ ‚àñ ‚àñ 62.299 6525.709 0.835 ‚àñ ‚àñ ‚àñ ‚àñ 3.081 10.707 0.284
MF ‚àñ ‚àñ ‚àñ ‚àñ 39.582 4545.596 0.531 ‚àñ ‚àñ ‚àñ ‚àñ 0.276 0.165 0.026
MICE ‚àñ ‚àñ ‚àñ ‚àñ 38.889 4314.435 0.521 ‚àñ ‚àñ ‚àñ ‚àñ 0.077 0.013 0.007
BRI
TS 334 266 1.42 46.68
23.393 1276.226 0.314 186 476 2.10 89.36 0.386 0.293 0.036
rGAIN 250 262 0.72 16.35 25.032 1358.134 0.335 3080 2096 4.52 72.53
0.579 0.571 0.055
SAITS 1.38 1376
7.93 60.41 51.907 5026.475 0.685 1.41 1411
6.60 43.13 0.466 0.366 0.043
TimesNet 149 1392 6.52 10.65 40.700
3383.554 0.545 61 1397 5.97 10.32 0.028
0.002 0.003
GRIN 241 191 10.76
148.86 29.420 2050.726 0.394 146 194 6.80
76.22 0.319 0.165 0.029
NET3‚àñ ‚àñ ‚àñ 98.31 34.755 2473.718 0.466 ‚àñ ‚àñ ‚àñ 56.76 0.547 0.682 0.051
PoGeVon 4850 323 15.42 230.10 19.581 1238.820 0.262 2930
322 9.90 98.35 0.007 0.000 0.001
ReCTSi 0.06
76 7.07 1.26 19.483 1102.159 0.261 0.12 145 9.81 1.76 0.015 0.000
0.001
Table 3: Statistics of the datasets.
Dataset
Type Time Series Timestamps Interval Missing Rate
PeMS-BA
Traffic Flow 64 25,920 5 minutes 25%
PeMS-LA Traffic Flow 64 25,920 5 minutes 25%
PeMS-SD Traffic Flow 64 25,920 5 minutes 25%
AQ36 Air Quality 36 8,759 1 hour 13%
COVID-19 Infection Case 50 346 1 day 25%
Table 4: Ablation study of ReCTSi on AQ36.
Mo
delFLOPs Params Peak Mem LatencyMAE MSE MRE(M) (K) (MB) (ms)
ReCTSi\PT 0.06
76 6.86 1.27 21.140 1225.109 0.290
ReCTSi\PS 0.06 76 6.91 1.35 21.036 1198.529 0.282
ReCTSi\PST 0.06 63 6.91 1.21 21.847 1395.944 0.295
ReCTSi\CF 0.13
154 12.53 1.53 20.388 1173.026 0.273
ReCTSi\CM 0.06
76 7.07 1.33 20.132 1134.471 0.270
ReCTSi\SA 0.04 55 6.90 1.02 23.385 1532.564 0.313
ReCTSi\TA 0.04 55 4.33 0.94 25.414 1855.463 0.332
ReCTSi 0.06
76 7.07 1.26 19.483 1102.159 0.261
compression and fusion. Removing the completeness matrix de-
grades CaA to conventional attention, and the inferior performance
ofReCTSi\CMcompared to ReCTSi indicates the necessity of inte-
grating imputation-specific information. Variants ReCTSi\SAand
ReCTSi\TAdemonstrate the essential nature of transient patterns
for successful imputation. Overall, the superior performance of the
complete ReCTSi confirms the synergistic effect of its components,
highlighting their collective importance for ReCTSi‚Äôs performance.5.4 Evaluation on Varying Resource Levels
We proceed to study how ReCTSi performs under different compu-
tational resource constraints, fulfilled by varying embedding size d
and window size T. The embedding size ddetermines the capacity
of the model‚Äôs feature representations for discerning nuanced pat-
terns in the data. While a larger dcan increase a model‚Äôs accuracy
by capturing more detailed features, it also increases the model‚Äôs
complexity, leading to a higher risk of overfitting and increasing
the computational requirements, reflected in higher FLOPs. The
window size Tconstrains the temporal scope for pattern learning.
Increasing Tallows for involving longer-term dependencies, offer-
ing more context for missing values. However, it also increases
resource consumption, as a larger Tnecessitates processing more
data points for each inference. Finding the configuration of dandT
affects the balance between model expressiveness and efficiency.
This balance becomes particularly crucial in resource-constrained
scenarios or when deploying the method at scale.
Figure 5 shows the trade-off between ReCTSi resource consump-
tion in terms of FLOPs and its imputation accuracy in terms of MAE
on the AQ36 dataset. Increasing the embedding size dinitially
leads to a decrease in MAE, signifying an improvement in the fea-
ture representation and accuracy. This improvement plateaus and
starts to decrease after reaching a certain dvalue, due to overfitting.
This overfitting unnecessarily increases computational costs, indi-
cating that beyond a certain point, a larger ddoes not yield better
performance. Likewise, the window size Tplays a pivotal role in
 
1481ReCTSi: Resource-efficient Correlated Time Series Imputation KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
16 32 48 64 80 961520253035MAE
0.000.050.100.150.200.250.30
FLOPs
6 12 18 24 30 3615202530MAE
0.000.020.040.060.080.100.12
FLOPs
(a) Embedding Size d (b) Window Size T
Figure 5: MAE and FLOPs when varying (a) the embedding
size dand (b) the imputation window size T.
the model‚Äôs ability to understand and utilize temporal patterns in
the dataset. An optimal window size of T=24hours aligns with
the dataset‚Äôs daily periodicity, thereby optimizing the model‚Äôs ca-
pabilities. Extending Tbeyond this period leads to the inclusion of
superfluous information, which not only diminishes performance
but also reduces computational efficiency.
Hence, under a specific resource constraint, higher computa-
tional overhead does not necessarily translate to better accuracy.
It is imperative to select appropriate dandTvalues to maintain a
purposeful balance between accuracy and computational resource
consumption. Therefore, there is a need for a systematic mecha-
nism to identify the optimal model configurations given a resource
constraint, which we remain it as a future research direction.
6 CONCLUSION AND FUTURE WORK
We present ReCTSi, a resource-efficient model for Correlated Time
Series (CTS) imputation, leveraging decoupled pattern learning and
completeness-aware attention mechanisms. Inspired by rethinking
existing deep models, ReCTSi adopts a decoupled architecture that
distinguishes between persistent and transient patterns in CTS data.
While persistent patterns can be retained and reused, transient pat-
terns require adaptation to distinct CTS windows. To materialize
and reuse persistent patterns during inference, ReCTSi introduces
a Multi-view Learnable Codebook (MvLC) mechanism, enabling
efficient retrieval. Additionally, novel Completeness-aware Atten-
tion (CaA) modules are proposed for transient spatial and temporal
feature learning, which are tailored specifically for the imputation
task. Experimental results show that ReCTSi not only achieves state-
of-the-art imputation accuracy but also substantially decreases the
computational resource consumption.
Promising future directions include extending ReCTSi to real-
time imputation processing. Also, integrating a Neural Architecture
Search framework for hyperparameter optimization under a given
resource constraint would increase the usability of the method,
leading to even more efficient and effective CTS imputation.
ACKNOWLEDGMENTS
This work was supported by the AAU Bridging Project (Grant no.
760848), and the Major Research Program of Zhejiang Provincial
Natural Science Foundation (Grant no. LD24F020015). Huan Li and
Dongxiang Zhang are supported by the Hangzhou High-Tech Zone
(Binjiang) Institute of Blockchain and Data Security. Hua Lu‚Äôs work
was supported in part by Independent Research Fund Denmark
(Grant no. 8022-00366B).
REFERENCES
[1]Cini Andrea, Marisca Ivan, Cesare Alippi, et al .2022. Filling the g_ap_s: multi-
variate time series imputation by graph neural networks. In ICLR. 1‚Äì20.[2]David Campos, Miao Zhang, Bin Yang, Tung Kieu, Chenjuan Guo, and Chris-
tian S. Jensen. 2023. LightTS: lightweight time series classification with adaptive
ensemble distillation. In PACMMOD, Vol. 1. 1‚Äì27.
[3]Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. BRITS:
bidirectional recurrent imputation for time series. In NeurIPS, Vol. 31.
[4]Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng
Jia. 2001. Freeway performance measurement system: mining loop detector data.
TRR 1748, 1 (2001), 96‚Äì102.
[5]Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen, and Qian
Zhang. 2020. Deep learning on mobile and embedded devices: state-of-the-art,
challenges, and future directions. ACM Comput. Surv. 53, 4 (2020), 1‚Äì37.
[6]Xu Cheng, Fan Shi, Xiufeng Liu, Meng Zhao, and Shengyong Chen. 2021. A
novel deep class-imbalanced semisupervised model for wind turbine blade icing
detection. IEEE TNNLS 33, 6 (2021), 2558‚Äì2570.
[7]Fran√ßois Chollet. 2017. Xception: deep learning with depthwise separable convo-
lutions. In CVPR. 1251‚Äì1258.
[8]Francesco Concas, Julien Mineraud, Eemil Lagerspetz, Samu Varjonen, Xiaoli Liu,
Kai Puolam√§ki, Petteri Nurmi, and Sasu Tarkoma. 2021. Low-cost outdoor air
quality monitoring and sensor calibration: a survey and critical analysis. TOSN
17, 2 (2021), 1‚Äì44.
[9]Amin Dhaou, Antoine Bertoncello, S√©bastien Gourv√©nec, Josselin Garnier, and
Erwan Le Pennec. 2021. Causal and interpretable rules for time series analysis.
InKDD. 2764‚Äì2772.
[10] Wenjie Du, David C√¥t√©, and Yan Liu. 2023. SAITS: self-attention-based imputation
for time series. ESWA 219 (2023), 119619.
[11] Vincent Fortuin, Dmitry Baranchuk, Gunnar R√§tsch, and Stephan Mandt. 2020.
Gp-vae: deep probabilistic time series imputation. In AISTATS. 1651‚Äì1661.
[12] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.
Attention based spatial-temporal graph convolutional networks for traffic flow
forecasting. In AAAI, Vol. 33. 922‚Äì929.
[13] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. 2020.
Ghostnet: more features from cheap operations. In CVPR. 1580‚Äì1589.
[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).
[15] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In CVPR.
7132‚Äì7141.
[16] Alejandro Jaimes and Joel Tetreault. 2021. Real-time event detection for emer-
gency response tutorial. In KDD. 4042‚Äì4043.
[17] Baoyu Jing, Hanghang Tong, and Yada Zhu. 2021. Network of tensor time series.
InWWW. 2425‚Äì2437.
[18] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2020. DE-
RRD: a knowledge distillation framework for recommender system. In CIKM.
605‚Äì614.
[19] Satya Katragadda, Ravi Teja Bhupatiraju, Vijay Raghavan, Ziad Ashkar, and Raju
Gottumukkala. 2022. Examining the COVID-19 case growth rate due to visitor vs.
local mobility in the United States using machine learning. Sci. Rep. 12, 1 (2022),
12337.
[20] Mourad Khayati, Alberto Lerner, Zakhar Tymchenko, and Philippe Cudr√©-
Mauroux. 2020. Mind the gap: an experimental evaluation of imputation of
missing values techniques in time series. In PVLDB, Vol. 13. 768‚Äì782.
[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2017. ImageNet classi-
fication with deep convolutional neural networks. Commun. ACM 60, 6 (2017),
84‚Äì90.
[22] Zhichen Lai, Xu Cheng, Xiufeng Liu, Lizhen Huang, and Yongping Liu. 2022.
Multiscale wavelet-driven graph convolutional network for blade icing detection
of wind turbines. IEEE Sens. J. 22, 22 (2022), 21974‚Äì21985.
[23] Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, and Christian S.
Jensen. 2024. E2Usd: Efficient-yet-effective Unsupervised State Detection for
Multivariate Time Series. In WWW. 3010‚Äì3021.
[24] Zhichen Lai, Dalin Zhang, Huan Li, Christian S. Jensen, Hua Lu, and Yan Zhao.
2023. LightCTS: a lightweight framework for correlated time series forecasting.
InPACMMOD, Vol. 1. 1‚Äì26.
[25] Xiao Li, Huan Li, Hua Lu, Christian S. Jensen, Varun Pandey, and Volker Markl.
2023. Missing value imputation for multi-attribute sensor data streams via
message propagation. In PVLDB, Vol. 17. 345‚Äì358.
[26] Youru Li, Zhenfeng Zhu, Deqiang Kong, Meixiang Xu, and Yao Zhao. 2019. Learn-
ing heterogeneous spatial-temporal representation for bike-sharing demand
prediction. In AAAI, Vol. 33. 1004‚Äì1011.
[27] Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao. 2022. Multi-
granularity structural knowledge distillation for language model compression.
InACL. 1001‚Äì1011.
[28] Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quanjun
Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makes vanilla
transformer sota for traffic forecasting. In CIKM. 4125‚Äì4129.
 
1482KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhichen Lai et al.
[29] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023.
PriSTI: a conditional diffusion framework for spatiotemporal imputation. In
ICDE.
[30] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. 2019. E2GAN: end-
to-end generative adversarial network for multivariate time series imputation.
InIJCAI. 3094‚Äì3100.
[31] Zhipeng Luo, Jianqiang Huang, Ke Hu, Xue Li, and Peng Zhang. 2019. AccuAir:
winning solution to air quality prediction for KDD Cup 2018. In KDD. 1842‚Äì1850.
[32] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei
Yin. 2021. Generative semi-supervised learning for multivariate time series
imputation. In AAAI, Vol. 35. 8983‚Äì8991.
[33] Chenmeng Qiu, Yuzhi Li, Mingyu Kang, Duxin Chen, and Wenwu Yu. 2023.
CDSTTN: a data imputation method for cyber-physical systems by causal dense
spatial-temporal transformer network. IEEE J EM SEL TOP C (2023).
[34] Mingxing Tan and Quoc Le. 2019. Efficientnet: rethinking model scaling for
convolutional neural networks. In ICML. 6105‚Äì6114.
[35] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI:
conditional score-based diffusion models for probabilistic time series imputation.
InNeurIPS, Vol. 34. 24804‚Äì24816.
[36] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew
Margenot, and Hanghang Tong. 2023. Networked time series imputation viaposition-aware graph enhanced variational autoencoders. In KDD. 2256‚Äì2268.
[37] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,
Zhengyang Zhou, and Yang Wang. 2023. An observed value consistent dif-
fusion model for imputing missing values in multivariate time series. In KDD.
2409‚Äì2418.
[38] Ian R White, Patrick Royston, and Angela M Wood. 2011. Multiple imputation
using chained equations: issues and guidance for practice. Stat Med 30, 4 (2011),
377‚Äì399.
[39] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. TimesNet: temporal 2D-variation modeling for general time series
analysis. In ICLR.
[40] Xinle Wu, Dalin Zhang, Chenjuan Guo, Chaoyang He, Bin Yang, and Christian S.
Jensen. 2021. AutoCTS: automated correlated time series forecasting. In PVLDB,
Vol. 15. 971‚Äì983.
[41] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. 2021. Self-distillation: towards
efficient and compact neural networks. IEEE TPAMI 44, 8 (2021), 4388‚Äì4403.
[42] Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and
Tianrui Li. 2015. Forecasting fine-grained air quality based on big data. In KDD.
2267‚Äì2276.
 
1483