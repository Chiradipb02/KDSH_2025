Co-Neighbor Encoding Schema: A Light-cost Structure Encoding
Method for Dynamic Link Prediction
Ke Cheng
ckpassenger@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaLinzhi Peng
lzpeng626@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaJunchen Ye∗
junchenye@buaa.edu.cn
School of Transportation Science and
Engineering, Beihang University
Beijing, China
Leilei Sun
leileisun@buaa.edu.cn
CCSE Lab, Beihang University
Beijing, ChinaBowen Du
dubowen@buaa.edu.cn
Zhongguancun Laboratory
School of Transportation Science and
Engineering, Beihang University
Beijing, China
ABSTRACT
Structure encoding has proven to be the key feature to distinguish-
ing links in a graph. However, Structure encoding in the temporal
graph keeps changing as the graph evolves, repeatedly computing
such features can be time-consuming due to the high-order sub-
graph construction. We develop the Co-Neighbor Encoding Schema
(CNES) to address this issue. Instead of recomputing the feature by
the link, CNES stores information in the memory to avoid redundant
calculations. Besides, unlike the existing memory-based dynamic
graph learning method that stores node hidden states, we introduce
a hashtable-based memory to compress the adjacency matrix for
efficient structure feature construction and updating with vector
computation in parallel. Furthermore, CNES introduces a Temporal-
Diverse Memory to generate long-term and short-term structure
encoding for neighbors with different structural information. A dy-
namic graph learning framework, Co-Neighbor Encoding Network
(CNE-N), is proposed using the aforementioned techniques. Exten-
sive experiments on thirteen public datasets verify the effectiveness
and efficiency of the proposed method.
CCS CONCEPTS
•Information systems →Data mining.
KEYWORDS
Graph Neural Networks, Dynamic Graph, Network Embedding
ACM Reference Format:
Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, and Bowen Du. 2024. Co-
Neighbor Encoding Schema: A Light-cost Structure Encoding Method for
∗Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $
https://doi.org/10.1145/3637528.3671770Dynamic Link Prediction. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671770
1 INTRODUCTION
Temporal graphs denote entities as nodes and represent their inter-
actions as edges with timestamps. This is a powerful way to model
the dynamics and evolution of complex systems over time. Re-
searchers have leveraged this approach and built many practical sys-
tems in a variety of real-world scenarios such as recommendations
in social networks [ 3,17,27], financial networks [ 6,23,34],user-
item interaction systems [ 13,18,38,39,41]. Dynamic graphs can
be represented in two ways: Discrete-time and Continuous-time.
Discrete-time Dynamic Graphs (DTDG) are sequences of static
graph snapshots captured at regular intervals. In contrast, Continuous-
time Dynamic Graphs (CTDG) are represented as timed lists of
events that include edge or node addition or deletion. CTDGs offer
better flexibility and performance than DTDGs and speed up com-
putation by estimating the full graph encoding with a neighborhood
subgraph.
According to whether to use heuristic features, existing CTDG
works can be divided into two main methods to encode the dynamic
graph: the vanilla method and the structure encoding-based method.
We discuss the difference between the two methods in Figure 1. The
vanilla method individually encodes the node neighborhood and
concentrates more on the encoding of temporal information. For
example, Jodie[ 17] models how a user’s preferences evolve with an
RNN-based encoder that updates the node representation as the
user-item interaction occurs. GraphMixer[ 9], and TCL[ 35] discuss
the benefits of capturing the temporal correlation between nodes
in the historical interaction sequence with an MLPMixer encoder
or a Transformer encoder. However, these methods with only one
anchor node treat neighborhood structure as a way to aggregate
temporal information, ignoring the structural information itself,
which results in the loss of rich information in high-order structures
such as relative position information between nodes.
In contrast, the structure encoding-based method encodes the
link by generating query-specific encoding of the two end nodes[ 30,
 
421
KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Table 1: Summary of different CTDG models. We also list the time complexity (per sample, the first element before + stands
for sample complexity) and space complexity (per graph, −stands for no memory to store) for each algorithm, with hidden
dimension 𝑑, neighbor sample length 𝐿, neighbor sample hops 𝑘, nodes number 𝑁, node memory/feature dim 𝑀, we assume all
the model shares a single layer full connected neural network as feature encoding model, detailed discussion can be found in
Section 2.2.
Mo
del Jo
die[17] TGN[25] CA
WN[37] DyGFormer[40] NAT[20] PINT[28] CNE-N
Strucute
Encoding ✗
✗ ✔
✔ ✔ ✔ ✔
Lightcost
Construction ✔
✗ ✗
✔ ✔ ✗ ✔
Parallel
Computation ✔
✔ ✗
✗ ✔ ✔ ✔
T
emporal Diversity ✗
✗ ✗
✗ ✗ ✗ ✔
Time 𝑀
𝑑 𝐿𝑘+𝐿𝑘𝑀𝑑𝐿𝑘+𝐿𝑘𝑀
𝑑 𝐿+𝐿(𝐿+𝑀𝑑)𝐿𝑀𝑑 𝐿𝑘+𝐿𝑘𝑁𝑑𝐿+𝐿
𝑀𝑑
Space 𝑁
𝑀 𝑁𝑀 -
- 𝑁𝑀 𝑁2𝑁
𝑀
Figure 1: An example to show the difference between meth-
ods w/o structure encoding, vanilla temporal graph learning
methods can not distinguish between link (u,v) and (v,w)
while structure encoding-based methods can show the dif-
ference by counting neighborhood overlap co-neighbors.
43], which is to encode one node’s neighborhood condition on an-
other node, the method can increase model express ability by better
distinguish node neighborhood encoding with the heuristic struc-
ture feature like the number of common neighbors. For example,
CAWN[ 37] extracts several random walk sequences starting from
each end node and defines the relative position label as the node
position in the sequences. DyGFormer[ 40] extracts the first hop
neighbor sequence of the end nodes and computes the neighbor
co-occurrence encoding as the first-order relative position label.
The effectiveness of the structure encoding is largely influenced by
the size of the neighborhood to be encoded, a larger neighborhood
subgraph will lead to a higher encoding accuracy as well as time
complexity[28].
Previous methods for generating structure encoding have three
drawbacks. Firstly, constructing a neighborhood subgraph is
inefficient. Existing structure encoding is generated by construct-
ing a high-order subgraph assigning relative position labels to each
neighbor node, the irregular neighbor size of each node makes it
hard to construct the subgraph in parallel. Besides, due to the huge
memory cost to store the adjacency matrix, for each end node pair,
subgraphs are constructed with CPU-based neighbor samples. As a
result, subgraph construction and encoding are not computationally
efficient. Secondly, encoding with limited-sized neighborhood
will lose information. Due to the low efficiency of neighborsampling, most existing works only encode with a limited-sized
subset of the edge high-order neighborhoods or only the first-order
neighbors. As a result, this approach fails to capture the necessary
information needed for accurate modeling of the graph structure,
which leads to relatively poor performance. Thirdly, extracting
subgraph with single strategy ignores the diverse temporal
information of different time intervals. Existing works only
encode with a single anchor-induced subgraph due to high compu-
tation cost. However, a temporal triangle of a recently interacted
neighbor provides different information than a temporal triangle
of a historically interacted neighbor. Therefore, single subgraph
extraction ignores the diverse temporal information of different
time intervals.
To address the issues above, we propose a novel structure encod-
ing method named Co-Neighbor Encoding Schema (CNES). It uses
regular, low-order subgraphs with index querying and vector-based
parallel co-neighbor computation on GPU to generate structure
encoding. Each node in the temporal graph is assigned a hashtable-
based memory, acting as a compressed adjacent matrix that can be
stored on the GPU. In this way, we can replace CPU-based neigh-
bor sampling with GPU-based index querying to access high-order
structures. With hashtable-based memory, CNES determines the
common neighbors between the other end node and the neighbor
nodes through regular-sized vector computation in parallel for all
node pairs in a batch, instead of assigning relative position labels,
for the neighbor nodes’ irregular high-order query-induced sub-
graphs. Unlike existing sample-based methods, CNES can encode
structures with a larger neighborhood size while maintaining high
efficiency, and reducing information loss in structure encoding
caused by the limited subgraph size. In addition, CNES introduces
aTemporal-Diverse Memory to generate structure encoding for
subgraphs at different time intervals.
We also propose a dynamic graph learning framework named
Co-Neighbor Encoding Network (CNE-N), a light-cost model for
dynamic link prediction. The proposed method is proven to be
effective and efficient both in theory (as shown in Table 1) and
experimentally.
Our contributions can be summarized as follows:
•An efficient structure encoding method CNES is proposed.
The method accelerates structure encoding by compressing
the adjacency matrix to hashtable-based memory and com-
puting co-neighbors with vector-based parallel computation.
•This paper generates a long-term and a short-term struc-
ture encoding for neighbors with temporal-diverse memory,
enabling the capture of temporal structural patterns.
 
422Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
•Extensive experiments on 13 datasets show the effectiveness
and the efficiency of the proposed method. Previous models
equipped with the proposed structure encoding also result
in a better performance.
2 PRELIMINARIES
Definition 2.1. Dynamic Graph. We define the dynamic graph
as a sequence of edges G𝑇whereG𝑇={𝑒1,···,𝑒𝑖,···,𝑒𝑛},0<
𝑡1≤···≤𝑡𝑖≤···≤𝑡𝑛≤𝑇and𝑒𝑖={𝑢,𝑣,𝑡},𝑢and𝑣∈𝑁
represent the source node and the destination node of an edge, and
𝑡means the edge occurs at timestamp 𝑡.𝑁is the node set of the
dynamic graph. Each node 𝑢∈𝑁has a feature vector 𝑥𝑁(𝑢)∈𝑅𝑑𝑁,
and each edge(𝑢,𝑣,𝑡)has an edge feature vector 𝑥𝑡
𝐸(𝑢,𝑣)∈𝑅𝑑𝐸,
where𝑑𝑁and𝑑𝐸are the dimensions of the node feature and link
feature.
Definition 2.2. k-order edge neighborhood subgraph. We
define the k-order edge neighborhood subgraph 𝑆𝑡
𝑘(𝑢,𝑣)as the
neighbors within 𝑘hops𝑁𝑘𝑢and𝑁𝑘𝑣of the two end nodes 𝑢and𝑣
at the timestamp 𝑡. The subgraph contains the historical interactions
between the nodes.
Definition 2.3. Problem Formalization. Given a dynamic graph,
G𝑡, Dynamic Link Prediction (DLP) task aims to learn the dynamic
representation ℎ𝑡𝑢andℎ𝑡𝑣for the source node 𝑢and destination node
𝑣with the k-order edge neighborhood subgraph 𝑆𝑡
𝑘(𝑢,𝑣)and predict
the interact probability 𝑝𝑡𝑢,𝑣of the two end nodes at timestamp 𝑡.
2.1 Structure encoding
Table 2: Comparing of different designs of structure encoding
functions.
Metho
d Compr
ess Function Updation Involve Nodes
CA
WN time-w
eighted sample recompute u,v
DyGformer r
ecent sample recompute u,v
PINT -
u,v u,v, 𝑁𝑢,𝑁𝑣
NAT single
hash function u,v u,v
CNEN (ours) multiple
hash function u,v, 𝑁𝑢,𝑁𝑣 u,v,𝑁𝑢,𝑁𝑣
Figure 2: Structure Encoding can be formalized as two func-
tions, a compress function to get the neighbor set from the
adjacency matrix, and a relation function to generate encod-
ing from the union of the two neighbor sets.
The structure encoding of the dynamic graph is to generate a
structure feature 𝑥𝑡
𝑆(𝑢,𝑣)∈𝑅𝑑𝑆for each edge(𝑢,𝑣,𝑡)∈G𝑇, where
𝑑𝑆is the dimension of the structure feature. The structure encoding
𝑉𝑡
𝑘(𝑢,𝑣)represents the structural role of the source node 𝑢, thedestination node 𝑣, and their neighbor nodes {𝑖∈𝑁𝑡𝑢∪𝑁𝑡𝑣}in the
𝑘-order edge neighborhood subgraph.
As shown in Figure 2, we formalize the structure encoding as a
compress function 𝑔𝑛(·)to get the neighbor set from the adjacency
matrix, and a relation function 𝑔𝑠(·)to generate encoding from the
union of the two neighbor sets. We then give a simpler mathematical
form of structure encoding as follows:
𝑉𝑡
𝑘(𝑢,𝑣)=1
𝑀vut𝑀∑︁
𝑖=0(ˆ𝐴𝑡
𝑘[𝑢,𝑖]·ˆ𝐴𝑡
𝑘[𝑣,𝑖])2, (1)
whereˆ𝐴𝑡
𝑘=𝑔𝑠(𝐴𝑡
𝑘)=𝐴𝑡
𝑘×M𝑡
𝑘is the compressed adjacency
matrix,M𝑡
𝑘∈𝑅𝑁×𝑀is the compress matrix. We further summarize
the difference in the structure encoding methods in Table 2.
The existing methods face a trade-off between accuracy and com-
plexity. For example, CAWN and DyGformer achieve accuracy by
recomputing the neighborhood with every update, but this process
takes a lot of time. PINT stores the encoding without compression,
requiring large storage space. NAT compresses the neighborhood
using a single hash function and only updates the corresponding
two center nodes when a link occurs, which means it cannot gen-
erate accurate up-to-date neighborhood encodings.
Our method extends existing methods by balancing this trade-
off. We follow NAT’s approach to compress the matrix with a hash
function and propose temporal diversity encoding to avoid infor-
mation loss in hash conflicts. Additionally, we generate encoding
and update memory with the k-order edge neighborhood subgraph
instead of only the two nodes. As a result, CNEN achieves a balance
between accuracy and complexity.
2.2 Discussion of the Model Complexity
According to Table 1, we analyze the time and space complexity
of each model under the following assumptions: 1) each model
shares the same feature encoder, with hidden dimension 𝑑=50; 2)
subgraph-based model samples the same length 𝐿=20of sequence
for every order; 3) Memory-based model shares a same sized mem-
ory with dimension 𝑀=100, no matter what the memory stores,
we assume the node feature size is also 𝑀(because memory can be
regarded as an aggregation of neighbor features, this assumption
can help compare between models with/without memory); 4) The
dynamic graph contains 𝑁=1000 nodes.
JODIE[ 17] is a memory-based model. Therefore, the space com-
plexity to store the node memories is O(𝑁𝑀); the model does not
depend on the neighbor sample and computes link probability only
with end nodes. Therefore, the time complexity to predict one link
isO(𝑀)+O(𝑀𝑑)+O(𝑀)=O(𝑀𝑑), where the twoO(𝑀)corre-
sponding to memory readout and update, O(𝑀𝑑)is the complexity
to encode the link features. The model is the most efficient baseline
in theory and practice but may suffer from inductive issues (mem-
ory can not be generated for unseen nodes), and the model is not
equipped with structure encoding.
TGN[ 25] is a memory-based equipped with a graph neural net-
work to encode high-order temporal information. It shares the
same memory design as JODIE does. Therefore, the space com-
plexity to store the node memories is also O(𝑁𝑀), the model ex-
tracts a𝑘order subgraph to gather information from neighbors;
 
423KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
in practice, 𝑘=1, so the time complexity to predict one link is
𝐿𝑘=1+𝐿𝑘=1×(O(𝑀)+O(𝑀𝑑)+O(𝑀))=O(𝐿+𝐿𝑀𝑑). By neigh-
bor sampling, the model overcomes inductive issues, and the model
is not equipped with structure encoding.
CAWN[ 37] is a structure encoding model with the random-walk
neighbor sampler and does not depend on memory. The model
extracts several numbers of walk sequences (we assume it to be 1)
with sequence length 𝑘(for most datasets, 𝑘≥2). For each node in
the sequence, the model assigns a relative position feature between
it and the walk head node; the sequence is later encoded with
an RNN-based model. Therefore the to predict one link is 𝐿𝑘=2+
𝐿𝑘=2×(O(𝑀𝑑))=O(𝐿2+𝐿2𝑀𝑑). The model is computationally
inefficient due to the complex and irregular subgraph construction.
NAT[ 20] is a memory-based model but replaces the single vector-
based memory in JODIE with dictionary-based memory. The repre-
sentation of each neighbor is stored in the corresponding position
without aggregation. Each memory contains 𝑀1position for first-
order neighbors and 𝑀2position for second-order neighbors. For
each neighbor, the model stores a representation of size 𝐹, the
method keeps(𝑀1+𝑀2)×𝐹≈𝑀, and we assume 𝑀1+𝑀2=𝐿
(for example, the default setting of the model is 𝑀1=32,𝑀2=
16,𝐹=4). Therefore, the model does not depend on a neighbor
sample to construct a subgraph and compute the structure encod-
ing. As a result, the space complexity to store the node memories
isO((𝑀1+𝑀2)×𝐹)=O(𝑁𝑀); the time complexity to predict
one link isO((𝑀1+𝑀2)𝑀𝑑)+O(𝑀1𝑑+𝑀2)=O(𝐿𝑀𝑑), where
O((𝑀1+𝑀2)𝑀𝑑)is the feature encoding for the 𝑀1+𝑀2neigh-
bors with feature dimension 𝑀. The model extends JODIE with
structure encoding but still suffers from inductive issues.
DyGFormer[ 40] is a sequence-based model with first-order
neighbor extraction. The model computes neighbor co-occurrence
encoding by validating how often a neighbor node occurs in the
neighbor sequences of the two end nodes. Therefore, the time com-
plexity for structure encoding is O(𝐿×𝐿)since it will compare
over the whole sequence for each node. The encoding is expensive
when𝐿is large. DyGFormer extracts a longer historical interac-
tion sequence than other baseline models and reduces the time
complexity with the patch technique (assume the patch size is 16).
When encoding sequence with a Transformer encoder, the time
complexity isO(𝐿2𝑀𝑑)before using the patch technique and is
O((𝐿/16)2(16𝑀)𝑑)=O(𝐿2𝑀𝑑/16).
PINT[ 28] extends TGN with a link-wise relative position matrix
∈𝑅𝑁×𝑁to store structure encoding between every pair of nodes,
for each node, the model readout all corresponding relative posi-
tions in the matrix. As a result, the space complexity to store the
node memories isO(𝑁2); the model extracts neighbor nodes in 2
2-order subgraphs. Therefore, the time complexity to predict one
link is𝐿𝑘=2+𝐿𝑘=2×(O(𝑁)+O(𝑁𝑑)+O(𝑁))=O(𝐿2+𝐿2𝑁𝑑),
where the twoO(𝑁)corresponding to memory readout and update,
O(𝑁𝑑)is the complexity to encode the link features.
CNE-N replace the representation-based node memory in TGN
to the neighbor set memory estimated by the hashtable, and the
computation of co-neighbor encoding is regular with time complex-
ityO(𝑀). Therefore, the model shares the same complexity with
TGN with𝑘=1(𝑀𝐿+𝑀𝑆=𝑀). Where the space complexity tostore the node memories is O(𝑁𝑀); the time complexity to predict
one link is𝐿+𝐿×(O(𝑀)+O(𝑀𝑑)+O(𝑀))=O(𝐿+𝐿𝑀𝑑).
3 METHODOLOGY
This section presents the details of the Co-Neighbor Encoding
Network(CNE-N). The framework of CNE-N is shown in Figure 3.
Figure 3: We develop a dynamic graph learning frame-
work Co-Neighbor Encoding Network (CNE-N), a light-cost
model for dynamic link prediction. The model generates
co-neighbor encoding with a hashtable-based memory and
low-order neighborhood subgraph extracting, and the mem-
ory updates by the graph evolves.
3.1 Memory for structure Encoding
We first introduce the hashtable-based memory in CNE-N to gen-
erate and update co-neighbor encoding, and long-short memory
to generate structure encoding in different time intervals. All the
computation can be done on GPU in parallel for multiple nodes in
a batch.
Co-neighbor Encoding. Let𝑉𝑡
𝑘(𝑢,𝑖)be the common neigh-
bors for the two nodes 𝑢and𝑖in the𝑘-order edge neighborhood
subgraph at timestamp 𝑡. We compute the co-neighbor encoding
[𝑉𝑡
𝑘(𝑢,𝑖),𝑉𝑡
𝑘(𝑣,𝑖)]for each node 𝑖in the subgraph 𝑆𝑡
𝑘(𝑢,𝑣,𝑡)of the
link(𝑢,𝑣). The count can be computed efficiently by first getting
the intersection of 𝑆𝑡
𝑘(𝑢)and𝑆𝑡
𝑘(𝑖)(where𝑆𝑡
𝑘(𝑖)corresponding to
the node set of node 𝑖’s k-order subgraph, a node’s 0-order subgraph
is itself), and computing the cardinal number of it:
𝑉𝑡
𝑘(𝑢,𝑖)=|𝑆𝑡
𝑘(𝑢)∩𝑆𝑡
𝑘(𝑖)|, (2)
𝑉𝑡
𝑘(𝑣,𝑖)=|𝑆𝑡
𝑘(𝑣)∩𝑆𝑡
𝑘(𝑖)|. (3)
Hashtable-based Memory. In practice, we implement subgraph
node sets with hash tables. Such data structure can estimate a set
with a fixed-size sequence 𝐻∈𝑅|𝑁|×𝑀, node-ids are mapped to the
row index with a hash function ℎ𝑎𝑠ℎ(𝑖)=(𝑞∗𝑖)(mod𝑀)where
𝑀is the size of the hash table 𝐻𝑡. We store node 𝑎in𝑖’s hash table
𝐻𝑡
𝑖by setting𝐻𝑡
𝑖[ℎ𝑎𝑠ℎ(𝑎)]=𝑎.
Encoding Computation. With hashtable 𝐻𝑡𝑢,𝐻𝑡𝑣and𝐻𝑡
𝑖, we can
efficiently compute the co-neighbor encoding [𝑉𝑡
𝑘(𝑢,𝑖),𝑉𝑡
𝑘(𝑣,𝑖)]by
 
424Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
counting the same element at the corresponding location:
𝑉𝑡
𝑘(𝑢,𝑖)=𝑀−1∑︁
𝑚=0(𝐻𝑡
𝑢[𝑚]=𝐻𝑡
𝑖[𝑚]), (4)
𝑉𝑡
𝑘(𝑣,𝑖)=𝑀−1∑︁
𝑚=0(𝐻𝑡
𝑣[𝑚]=𝐻𝑡
𝑖[𝑚]). (5)
Memory Update. A dynamic graph keeps evolving; the neigh-
bors of a node are also time-varied, so whenever a new link occurs,
the neighbor set memory of the nodes in the link-induced subgraph
should be updated. The updating can be estimated by replacing
the neighbor set memory with the union of the memory of the
node and the updated neighbor 𝑆𝑡+1
𝑘(𝑢)=𝑆𝑡
𝑘(𝑢,𝑡)∪𝑆𝑡
𝑘−1(𝑣,𝑡),
where𝑆𝑡
𝑘−1(𝑣)stands for the 𝑘−1-hop subgraph node-set, the
nodes have become node 𝑢’s new updated neighbor in the 𝑘-hop
subgraph. In practice, the update via set union can be implemented
by hash_insert 𝐻𝑡𝑢[ℎ𝑎𝑠ℎ(𝑣)]←𝑣.
Temporal-Diverse Memory. Hashtable-based memory stores
all the neighbors of a node without temporal information from
different intervals. When encoding a dynamic graph with a few
nodes like Social Evo., the neighbor storage of each node will even-
tually be almost the same. Co-neighbor encoding can no longer
provide useful structural information for link prediction. Besides, a
single subgraph to encode the structure encoding can not use the
evolving pattern in the dynamic structure, namely the temporal
issue. Therefore, we propose long-short memory, which contains
two parts of the hashtable: a short memory 𝐻𝑠∈𝑅|𝑁|×𝑀𝑠with a
smaller hashtable size to store recent neighbor nodes and a long
memory𝐻𝑙∈𝑅|𝑁|×𝑀𝑙with a larger hashtable size to store more
historical neighbor nodes. The two hashtables individually gener-
ate structure information and update the memory by the sequence.
With the limited storage space, the short memory will replace neigh-
bors in the hashtable more frequently. Therefore, it can generate
co-neighbor encoding of the link short-term subgraph. We verify
the effectiveness of temporal-diverse memory and report the result
in Section 4.3.
3.2 Neural Encoding for Subgraph
In this section, we introduce how to predict future links with the
proposed model CNE-N.
Extracting edge neighborhood subgraph. We first extract the
neighbors of the two end nodes to construct the 𝑘-order edge neigh-
borhood subgraph with CPU-based neighbor sampling. In practice,
to balance the trade-off between effectiveness and efficiency, we
follow most existing works to extract the first-hop subgraph as
historically interacted neighbors sequence with neighbor sampling,
namely𝑆𝑡
1(𝑢,𝑣), given an interaction (𝑢,𝑣,𝑡), for the source node 𝑢
and the destination node 𝑣, we obtain the subgraphs that involves
historical interactions of 𝑢and𝑣before timestamp 𝑡(including them-
selves), which are denoted by 𝑆𝑡
1(𝑢)={(𝑢,𝑖,ˆ𝑡)|ˆ𝑡<𝑡,𝑖∈𝑁1𝑢}and
𝑆𝑡
1(𝑣)={(𝑣,𝑖,ˆ𝑡)|ˆ𝑡<𝑡,𝑖∈𝑁1𝑣}. In practice, to avoid the influence
of the different lengths of historical interaction sequences, we pad
each sequence with a fixed size 𝑙𝑠. As a result,|𝑆𝑡
1(𝑢)|=|𝑆𝑡
1(𝑣)|=𝑙𝑠.
Constructing Structure Encoding with Co-neighbor Encod-
ing Schema. With the proposed hashtable-based memory, we can
efficiently compute the co-neighbor encoding between neighbornodes in the source node 𝑢neighborhood and the destination node
𝑣neighborhood. Mathematically, given sequence 𝑆𝑡
1(𝑢)and𝑆𝑡
1(𝑢),
we can reach the stored neighbor memory of each corresponding
node in the sequence as 𝑆𝑡
1,𝐻∗(𝑢)∈𝑅𝑙𝑠×𝑀and𝑆𝑡
1,𝐻∗(𝑣)∈𝑅𝑙𝑠×𝑀,
and the neighbor hash table for source node 𝑢and destination node
𝑣as𝐻∗,𝑡
𝑢and𝐻∗,𝑡
𝑣, where∗corresponding to 𝑙for long memory and
𝑠for short memory. Then we compute the co-neighbor encoding of
the neighbor list 𝑋𝑡
𝑢,𝐶,∗∈𝑅𝑙𝑠×2=[𝑉∗,𝑡
𝑘(𝑢,𝑖),𝑉∗,𝑡
𝑘(𝑣,𝑖)]{𝑖∈𝑆𝑡
𝑘(𝑢)}
and𝑋𝑡
𝑣,𝐶,∗∈𝑅𝑙𝑠×2=[𝑉∗,𝑡
𝑘(𝑣,𝑗),𝑉∗,𝑡
𝑘(𝑢,𝑗)]{𝑗∈𝑆𝑡
𝑘(𝑣)}with Equa-
tion 4.
For example, if the historical node for 𝑢and𝑣are[𝑢,𝑎,𝑎]and
[𝑣,𝑎,𝑢], given𝑉1(𝑢,𝑎)=4,𝑉1(𝑢,𝑣)=5and𝑉1(𝑣,𝑎)=1, the struc-
ture encoding for 𝑢and𝑣are[(𝑀,5),(4,1),(4,1)]and[(𝑀,5),(1,4),
(5,𝑀)], where𝑀is the size of the hashtable.
The co-neighbor encoding scheme is light cost and can be easily
integrated into dynamic graph learning methods for better results.
For each node, the method generates structure encoding by count-
ing triangles between up to 1+𝑙𝑠+𝑙𝑠𝑀nodes in parallel, which
is larger than most of the existing models. For example, the DyG-
Former only considers first-hop neighbors with up to 1+𝑙𝑠nodes,
and CAWN considers 1+𝑙𝑠+𝑙2𝑠nodes but with high sample complex-
ity. However, due to the high frequency of multiple-time interaction
in the temporal graph, the sample-based model often repeatedly
computes structure encoding between the same node pairs, which
makes them consider fewer triangles than CNES with a hashtable-
based memory to store neighbors. We demonstrate its efficiency in
Section 5.
Encoding Side information and Time Intervals. As discussed
in Section 2, there are some dynamic graphs with side information
like node feature or edge feature, we encode those features in node
𝑢’s historical interaction sequence as 𝑋𝑡
𝑢,𝑁∈𝑅𝑙𝑠×𝑑𝑁and𝑋𝑡
𝑢,𝐸∈
𝑅𝑙𝑠×𝑑𝐸. As for the time interval between the link and historical
intervals, we follow TGAT[ 37] and encode the time intervals Δˆ𝑡=
𝑡−ˆ𝑡with Fourier Time encoding as 𝑋𝑡
𝑢,𝑇∈𝑅𝑙𝑠×𝑑𝑇:
𝑋𝑡
𝑢
,𝑇=√︃
1
𝑑𝑇[𝑐
𝑜𝑠(Δˆ𝑡𝑤1),𝑠𝑖𝑛(Δˆ𝑡𝑤2),...,𝑐𝑜𝑠(Δˆ𝑡𝑤𝑑𝑇−1),𝑠𝑖𝑛(Δˆ𝑡𝑤𝑑𝑇)],(6)
where𝑤1,·,𝑤𝑑𝑇∈𝑅1×1are trainable parameters to encode time
interval.
Fusing Features. We first assign each feature to the same di-
mension𝑑with several one-layer full connected neural networks
𝑓(·), and concatenated them together:
𝑍𝑡
𝑢=𝑓𝑁(𝑋𝑡
𝑢,𝑁)∥𝑓𝐸(𝑋𝑡
𝑢,𝐸)∥𝑓𝑇(𝑋𝑡
𝑢,𝑇)∥𝑓𝐶,𝑙(𝑋𝑡
𝑢,𝐶,𝑙)∥𝑓𝐶,𝑠(𝑋𝑡
𝑢,𝐶,𝑠),
(7)
𝑍𝑡
𝑣=𝑓𝑁(𝑋𝑡
𝑣,𝑁)∥𝑓𝐸(𝑋𝑡
𝑣,𝐸)∥𝑓𝑇(𝑋𝑡
𝑣,𝑇)∥𝑓𝐶,𝑙(𝑋𝑡
𝑣,𝐶,𝑙)∥𝑓𝐶,𝑠(𝑋𝑡
𝑢,𝐶,𝑠),
(8)
Then, we employ a feature fusion encoder to encode the historical
interaction sequence for subgraph-based embedding. The Layer
Normalization (LN) is employed after every block. The input is
processed via the feature fusion encoder to fuse the information
from different features:
𝑍𝑡,𝑙
𝑢=𝐿𝑁(𝑍𝑡,𝑙−1
𝑢𝑊𝑙
𝑓𝑢+𝑏𝑙
𝑓𝑢), (9)
𝑍𝑡,𝑙
𝑣=𝐿𝑁(𝑍𝑡,𝑙−1
𝑣𝑊𝑙
𝑓𝑢+𝑏𝑙
𝑓𝑢), (10)
 
425KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
where𝑊𝑙
𝑓𝑢∈𝑅4𝑑×4𝑑,𝑏𝑙
𝑓𝑢∈𝑅4𝑑are trainable parameters at the
𝑙-th layer of the encoder 𝑓𝑓𝑢={∗}𝑊𝑓𝑢+𝑏𝑓𝑢. The output of the
L-th layer is 𝑍𝑡,𝐿
𝑢and𝑍𝑡,𝐿
𝑣∈𝑅𝑙𝑠×5𝑑.
Generating Node Representation. The time-aware represen-
tations of node 𝑢and𝑣at timestamp 𝑡,ℎ𝑡𝑢∈𝑅𝑑𝑜andℎ𝑡𝑣∈𝑅𝑑𝑜𝑢𝑡are
derived by taking the mean pooling of their related representations
in𝑍𝑡,𝐿with an output layer 𝑓𝑜={∗}𝑊𝑜+𝑏𝑜:
ℎ𝑡
𝑢=(1
𝑙𝑠∑︁
(𝑍𝑡,𝐿
𝑢))𝑊𝑜+𝑏𝑜, (11)
ℎ𝑡
𝑣=(1
𝑙𝑠∑︁
(𝑍𝑡,𝐿
𝑣))𝑊𝑜+𝑏𝑜, (12)
where𝑊𝑜∈𝑅5𝑑×𝑑𝑜,𝑏𝑜∈𝑅𝑑𝑜are trainable parameters at the
output layer.
Predicting Link Probability. The predicted probability of the
linkℎ𝑡𝑢,𝑣∈𝑅𝑑𝑜are merged with a merge layer 𝑓𝑚as:
𝑝𝑡
𝑢,𝑣=𝜎((ℎ𝑡
𝑢,ℎ𝑡
𝑣)𝑊𝑚+𝑏𝑚), (13)
where𝜎is the sigmoid function, 𝑊𝑚∈𝑅2𝑑𝑜×1,𝑏𝑚∈𝑅1are train-
able parameters at the merge layer. We train the model with the
Binary Cross-entropy loss between the positive link and the nega-
tive sampled node pair [𝑢,𝑛𝑒𝑔]with the random sample strategy:
𝑙𝑜𝑠𝑠=−(𝑙𝑜𝑔(𝑝𝑡
𝑢,𝑣)+𝑙𝑜𝑔(1−𝑝𝑡
𝑢,𝑛𝑒𝑔)). (14)
Updating Node Memories in the subgraph. Finally, we up-
date the node co-neighbor memory with the new link-induced
subgraph. Firstly, we update 𝑢and𝑣’s memory with the 1-order
subgraph𝐻∗𝑢[ℎ𝑎𝑠ℎ(𝑣)] ←𝑣,𝐻∗𝑣[ℎ𝑎𝑠ℎ(𝑢)] ←𝑢, where∗corre-
sponding to 𝑙for long memory and 𝑠for short memory, and then
update the memory with neighbor nodes in the 2-order subgraph
𝐻∗𝑢[ℎ𝑎𝑠ℎ(𝑗)]←𝑗,{𝑗∈𝑆𝑡
1(𝑣)},𝐻∗𝑣[ℎ𝑎𝑠ℎ(𝑖)]←𝑖,{𝑖∈𝑆𝑡
1(𝑢)}, and
we update the memory of the neighbor nodes 𝐻∗
𝑖[ℎ𝑎𝑠ℎ(𝑣)] ←
𝑣,{𝑖∈𝑆𝑡
1(𝑢)},𝐻∗
𝑗[ℎ𝑎𝑠ℎ(𝑢)]←𝑢,{𝑗∈𝑆𝑡
1(𝑣)}. We verify the effec-
tiveness of updating the neighbor node memory and updating with
the 2-order subgraph update in Section 4.3.
4 EXPERIMENTS
4.1 Experimental Settings
In this section, we introduce the experimental settings to examine
the effectiveness and efficiency of the proposed model.
Datasets. We test CNE-N and baseline models on thirteen com-
monly used datasets: Wikipedia, Reddit, MOOC, LastFM, Enron,
Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote,
and Contact, which are collected by Edgebank[ 22]. Details of the
datasets are shown in Appendix A.1.
Evaluation Metrics. We follow TGN[ 25] to evaluate the model
performance on the dynamic link prediction task, which is to pre-
dict whether two nodes interact with each other at a certain time.
The task contains two settings: transductive setting predicts future
links between previously observed nodes in the training process
and inductive setting tests link prediction between unseen nodes.
Average Precision (AP) and Area Under the Receiver Operating
Characteristic Curve (AUC-ROC) are adopted as the evaluation
metrics. We use the random negative sample strategy to generatenegative links. We considered a 70%-15%-15% (train-val-test) split
for each dataset.
Baselines. We compare CNE-N with ten popular continuous-
time dynamic graph learning baselines, including two memory-
based models, JODIE[17] and DyRep[16]. Two graph convolution-
based models, TGAT[ 37] and TGN[ 25]; four sequence-based models,
EdgeBank[ 22], TCL[ 35], GraphMixer[ 9], and DyGFormer[ 40]; and
two structure encoding-based models, NAT and CAWN[ 36]. Details
of the baseline models are listed in Appendix A.2. We test our
model and baselines under DyGLib[ 40]. The code is available in
https://github.com/ckpassenger/DyGLib_CNEN/tree/CNEN.
Model Configurations . For baselines, we follow the result re-
ported by DyGFormer[ 40]. As for CNE-N, We set the size of the
long memory hashtable 𝑀𝑙to 64 and set the short memory size 𝑀𝑠
to 16. We search the historical interaction sequence with a maxi-
mum length 𝑙𝑠in[4,10,20,32,64,100]. For all layers in the model,
we set their hidden dimension 𝑑𝑇,𝑑,𝑑𝑜, and𝑑𝑚to 50.
Implementation Details. We employ the Adam optimizer with
a learning rate of 0.0001 in all our experiments. The batch size of
the input data is fixed at 200 for training, validation, and testing.
The dropout rate was set at 0.1 for all experiments and all datasets.
We conduct the experiments five times and took the average of the
results. The experiments are carried out on a Windows machine
with an AMD RYZEN 7 5800 CPU @ 3.30GHz having 6 physical
cores. The GPU device used is an NVIDIA GTX3060 with a memory
capacity of 12 GB.
4.2 Performance Comparison
In this section, we compare the performance between our method
and baselines.
Result. We report the performance of different methods on
the AP/AUC-ROC metric for the transductive setting of dynamic
link prediction in Table 3, and in the inductive setting in ??. Note
that EdgeBank[ 22] can be only evaluated for transductive dynamic
link prediction, so its results under the inductive setting are not
presented. For some of the results, we follow the results reported
by their original papers. We also test model scalability in large-
scale TGB dataset, please refer to Appendix A.3 for more discussion.
Furthermore, the performance and time per epoch for different
dynamic learning methods are presented in Figure 4. From Table 3,
and Figure 4, we have two main observations.
(1) In all datasets, except for Can. Parl. and Social Evo., CNE-N
outperforms other baselines. The co-neighbor encoding performs
best in social or interaction datasets with sufficiently large neigh-
bors. This result confirms the effectiveness of our proposed co-
neighbor encoding schema. The structure encoding helps the model
differentiate between new links among highly related and unrelated
nodes. However, Can. Parl is a policy dataset and highly relies on
first-order neighbor modeling[ 40]. Therefore, DyGFormer performs
well with long neighbor sequence. CNE-N fails to generate a precise
encoding because the model depends on the number of common
neighbors instead of individual neighbor information. Additionally,
Social Evo. has very few nodes, leading to all hashtable-based mem-
ory being the same, rendering co-neighbor encoding useless. This
inspired us to develop temporal-diverse memory to encode recent
high-frequency interacting neighbors.
 
426Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: AP/AUC-ROC for transductive and inductive dynamic link prediction.
Metrics Datasets JODIE
DyRep TGAT TGN CAWN EdgeBank TCL GraphMixer NAT DyGFormer CNE-N
T
rans-APWikip
edia 96.50 ±0.14
94.86 ±0.06 96.94 ±0.06 98.45 ±0.06 98.76 ±0.03 90.37 ±0.00 96.47 ±0.16 97.25 ±0.03 97.50 ±0.04 99.03 ±0.02 99.09 ±0.04
Re
ddit 98.31 ±0.14
98.22 ±0.04 98.52 ±0.02 98.63 ±0.06 99.11 ±0.01 94.86 ±0.00 97.53 ±0.02 97.31 ±0.01 99.10 ±0.21 99.22 ±0.01 99.22 ±0.01
MOOC 80.23 ±2.44
81.97 ±0.49 85.84 ±0.15 89.15 ±1.60
80.15 ±0.25 57.97 ±0.00 82.38 ±0.24 82.78 ±0.15 87.21 ±0.63 87.52 ±0.49 94.18 ±0.07
LastFM 70.85 ±2.13
71.92 ±2.21 73.42 ±0.21 77.07 ±3.97 86.99 ±0.06 79.29 ±0.00 67.27 ±2.16 75.61 ±0.24 88.57 ±1.76 93.00 ±0.12 93.55 ±0.12
Enr
on 84.77 ±0.30
82.38 ±3.36 71.12 ±0.97 86.53 ±1.11 89.56 ±0.09 83.53 ±0.00 79.70 ±0.71 82.25 ±0.16 90.81 ±0.31 92.47 ±0.12 92.48 ±0.10
So
cial Evo. 89.89 ±0.55
88.87 ±0.30 93.16 ±0.17 93.57 ±0.17 84.96 ±0.09 74.95 ±0.00 93.13 ±0.16 93.37 ±0.07 91.23 ±0.37 94.73 ±0.01 94.60 ±0.03
UCI 89.43 ±1.09
65.14 ±2.30 79.63 ±0.70 92.34 ±1.04 95.18 ±0.06 76.20 ±0.00 89.57 ±1.63 93.55 ±0.57 94.26 ±0.37 95.79 ±0.17 96.85 ±0.08
F
lights 95.60 ±1.73
95.29 ±0.72 94.03 ±0.18 97.95 ±0.14 98.51 ±0.01 89.35 ±0.00 91.23 ±0.02 90.99 ±0.05 97.66 ±0.80 98.91 ±0.01 98.92 ±0.01
Can.
Parl. 69.26 ±0.31
66.54 ±2.76 70.73 ±0.72 70.88 ±2.34 69.82 ±2.34 64.55 ±0.00 68.67 ±2.67 77.04 ±0.46 83.83 ±1.20 97.36 ±0.45 86.09 ±0.14
US
Legis. 75.05 ±1.52
75.34 ±0.39 68.52 ±3.16 75.99 ±0.58 70.58 ±0.48 58.39 ±0.00 69.59 ±0.48 67.74 ±1.02 77.56 ±0.21 71.11 ±0.59 78.69 ±0.12
UN
Trade 64.94 ±0.31
63.21 ±0.93 61.47 ±0.18 65.03 ±1.37 65.39 ±0.12 60.41 ±0.00 62.21 ±0.03 62.61 ±0.27 72.32 ±0.69 66.46 ±1.29 76.92 ±0.06
UN
Vote 63.91 ±0.81
62.81 ±0.80 52.21 ±0.98 65.72 ±2.17 52.84 ±0.10 58.49 ±0.00 51.90 ±0.30 52.11 ±0.16 69.70 ±0.49 55.55 ±0.42 66.40 ±0.12
Contact 95.31 ±1.33
95.98 ±0.15 96.28 ±0.09 96.89 ±0.56 90.26 ±0.28 92.58 ±0.00 92.44 ±0.12 91.92 ±0.03 97.25 ±0.33 98.29 ±0.01 98.40 ±0.02
A
vg. Rank 7.08
7.85 7.62 4.38 5.92 9.61 8.76 7.77 3.31 2.53 1.38
T
rans-AUCWikip
edia 96.33 ±0.07
94.37 ±0.09 96.67 ±0.07 98.37 ±0.07 98.54 ±0.04 90.78 ±0.00 95.84 ±0.18 96.92 ±0.03 96.72 ±0.21 98.91 ±0.02 99.01 ±0.05
Re
ddit 98.31 ±0.05
98.17 ±0.05 98.47 ±0.02 98.60 ±0.06 99.01 ±0.01 95.37 ±0.00 97.42 ±0.02 97.17 ±0.02 99.02 ±0.10 99.15 ±0.01 99.15 ±0.01
MOOC 83.81 ±2.09
85.03 ±0.58 87.11 ±0.19 91.21 ±1.15 80.38 ±0.26 60.86 ±0.00 83.12 ±0.18 84.01 ±0.17 88.38 ±0.71
87.91 ±0.58 95.69 ±0.07
LastFM 70.49 ±1.66
71.16 ±1.89 71.59 ±0.18 78.47 ±2.94 85.92 ±0.10 83.77 ±0.00 64.06 ±1.16 73.53 ±0.12 86.94 ±2.29 93.05 ±0.10 93.13 ±0.11
Enr
on 87.96 ±0.52
84.89 ±3.00 68.89 ±1.10 88.32 ±0.99 90.45 ±0.14 87.05 ±0.00 75.74 ±0.72 84.38 ±0.21 92.02 ±0.32 93.33 ±0.13 93.12 ±0.09
So
cial Evo. 92.05 ±0.46
90.76 ±0.21 94.76 ±0.16 95.39 ±0.17 87.34 ±0.08 81.60 ±0.00 94.84 ±0.17 95.23 ±0.07 93.22 ±0.13 96.30 ±0.01 96.24 ±0.14
UCI 90.44 ±0.49
68.77 ±2.34 78.53 ±0.74 92.03 ±1.13 93.87 ±0.08 77.30 ±0.00 87.82 ±1.36 92.52 ±0.67 93.02 ±0.48 94.49 ±0.26 96.03 ±0.08
F
lights 96.21 ±1.42
95.95 ±0.62 94.13 ±0.17 98.22 ±0.13 98.45 ±0.01 90.23 ±0.00 91.21 ±0.02 91.13 ±0.01 97.32 ±0.34 98.93 ±0.01 98.96 ±0.02
Can.
Parl. 78.21 ±0.23
73.35 ±3.67 75.69 ±0.78 76.99 ±1.80 75.70 ±3.27 64.14 ±0.00 72.46 ±3.23 83.17 ±0.53 87.70 ±1.37 97.76 ±0.41 89.71 ±0.07
US
Legis. 82.85 ±1.07
82.28 ±0.32 75.84 ±1.99 83.34 ±0.43 77.16 ±0.39 62.57 ±0.00 76.27 ±0.63 76.96 ±0.79 84.68 ±0.35 77.90 ±0.58 84.14 ±0.63
UN
Trade 69.62 ±0.44
67.44 ±0.83 64.01 ±0.12 69.10 ±1.67 68.54 ±0.18 66.75 ±0.00 64.72 ±0.05 65.52 ±0.51 76.76 ±0.81
70.20 ±1.44 78.57 ±0.07
UN
Vote 68.53 ±0.95
67.18 ±1.04 52.83 ±1.12 69.71 ±2.65 53.09 ±0.22 62.97 ±0.00 51.88 ±0.36 52.46 ±0.27 74.44 ±2.01 57.12 ±0.62 69.52 ±0.34
Contact 96.66 ±0.89
96.48 ±0.14 96.95 ±0.08 97.54 ±0.35 89.99 ±0.34 94.34 ±0.00 94.15 ±0.09 93.94 ±0.02 97.64 ±0.58 98.53 ±0.01 98.77 ±0.01
A
vg. Rank 6.31
7.69 7.92 4.23 6.15 9.31 9.23 7.69 3.15 2.85 1.31
Ind-
APWikip
edia 94.82 ±0.20
92.43 ±0.37 96.22 ±0.07 97.83 ±0.04 98.24 ±0.03 - 96.22 ±0.17 96.65 ±0.02 95.40 ±0.04 98.59 ±0.03 98.37 ±0.03
Re
ddit 96.50 ±0.13
96.09 ±0.11 97.09 ±0.04 97.50 ±0.07 98.62 ±0.01 - 94.09 ±0.07 95.26 ±0.02 98.56 ±0.21 98.84 ±0.02 98.78 ±0.01
MOOC 79.63 ±1.92
81.07 ±0.44 85.50 ±0.19 89.04 ±1.17
81.42 ±0.24 - 80.60 ±0.22 81.41 ±0.21 83.59 ±1.58 86.96 ±0.43 91.89 ±0.31
LastFM 81.61 ±3.82
83.02 ±1.48 78.63 ±0.31 81.45 ±4.29 89.42 ±0.07 - 73.53 ±1.66 82.11 ±0.42 86.87 ±1.95 94.23 ±0.09 94.64 ±0.12
Enr
on 80.72 ±1.39
74.55 ±3.95 67.05 ±1.51 77.94 ±1.02 86.35 ±0.51 - 76.14 ±0.79 75.88 ±0.48 89.03 ±0.83 89.76 ±0.34 89.66 ±0.22
So
cial Evo. 91.96 ±0.48
90.04 ±0.47 91.41 ±0.16 90.77 ±0.86 79.94 ±0.18 - 91.55 ±0.09 91.86 ±0.06 91.22 ±0.32 93.14 ±0.04 93.29 ±0.37
UCI 79.86 ±1.48
57.48 ±1.87 79.54 ±0.48 88.12 ±2.05 92.73 ±0.06 - 87.36 ±2.03 91.19 ±0.42 87.30 ±0.15 94.54 ±0.12 95.03 ±0.16
F
lights 94.74 ±0.37
92.88 ±0.73 88.73 ±0.33 95.03 ±0.60 97.06 ±0.02 - 83.41 ±0.07 83.03 ±0.05 96.59 ±1.67 97.79 ±0.02 97.72 ±0.04
Can.
Parl. 53.92 ±0.94
54.02 ±0.76 55.18 ±0.79 54.10 ±0.93 55.80 ±0.69 - 54.30 ±0.66 55.91 ±0.82 60.62 ±2.06 87.74 ±0.71 68.31 ±0.59
US
Legis. 54.93 ±2.29
57.28 ±0.71 51.00 ±3.11 58.63 ±0.37
53.17 ±1.20 - 52.59 ±0.97 50.71 ±0.76 57.54 ±0.80 54.28 ±2.87 59.44 ±0.44
UN
Trade 59.65 ±0.77
57.02 ±0.69 61.03 ±0.18 58.31 ±3.15 65.24 ±0.21 - 62.21 ±0.12 62.17 ±0.31 69.29 ±1.59 64.55 ±0.62 66.58 ±0.27
UN
Vote 56.64 ±0.96
54.62 ±2.22 52.24 ±1.46 58.85 ±2.51 49.94 ±0.45 - 51.60 ±0.97 50.68 ±0.44 66.35 ±4.06
55.93 ±0.39 69.71 ±0.48
Contact 94.34 ±1.45
92.18 ±0.41 95.87 ±0.11 93.82 ±0.99 89.55 ±0.30 - 91.11 ±0.12 90.59 ±0.05 96.79 ±0.37 98.03 ±0.02 98.02 ±0.05
A
vg. Rank 7.23
8.46 7.62 6.00 6.08 - 8.38 7.77 3.62 2.69 1.46
Ind-
AUCWikip
edia 94.33 ±0.27
91.49 ±0.45 95.90 ±0.09 97.72 ±0.03 98.03 ±0.04 - 95.57 ±0.20 96.30 ±0.04 94.74 ±0.44 98.48 ±0.03 98.23 ±0.01
Re
ddit 96.52 ±0.13
96.05 ±0.12 96.98 ±0.04 97.39 ±0.07 98.42 ±0.02 - 93.80 ±0.07 94.97 ±0.05 97.99 ±0.52 98.71 ±0.01 98.62 ±0.01
MOOC 83.16 ±1.30
84.03 ±0.49 86.84 ±0.17 91.24 ±0.99
81.86 ±0.25 - 81.43 ±0.19 82.77 ±0.24 6.13 ±3.55 87.62 ±0.51 92.76 ±0.29
LastFM 81.13 ±3.39
82.24 ±1.51 76.99 ±0.29 82.61 ±3.15 87.82 ±0.12 - 70.84 ±0.85 80.37 ±0.18 83.07 ±2.32 94.08 ±0.08 94.38 ±0.08
Enr
on 81.96 ±1.34
76.34 ±4.20 64.63 ±1.74 78.83 ±1.11 87.02 ±0.50 - 72.33 ±0.99 76.51 ±0.71 89.92 ±0.72 90.69 ±0.26 90.18 ±0.15
So
cial Evo. 93.70 ±0.29
91.18 ±0.49 93.41 ±0.19 93.43 ±0.59 84.73 ±0.27 - 93.71 ±0.18 94.78 ±1.00 92.11 ±0.07 95.29 ±0.03 95.16 ±0.14
UCI 78.80 ±0.94
58.08 ±1.81 77.64 ±0.38 86.68 ±2.29 90.40 ±0.11 - 84.49 ±1.82 89.30 ±0.57 83.81 ±1.28 92.63 ±0.13 93.34 ±0.17
F
lights 95.21 ±0.32
93.56 ±0.70 88.64 ±0.35 95.92 ±0.43 96.86 ±0.02 - 82.48 ±0.01 82.27 ±0.06 96.36 ±1.51 97.80 ±0.02 97.82 ±0.04
Can.
Parl. 53.81 ±1.14
55.27 ±0.49 56.51 ±0.75 55.86 ±0.75 58.83 ±1.13 - 55.83 ±1.07 58.32 ±1.08 61.62 ±2.50 89.33 ±0.48 70.22 ±0.77
US
Legis. 58.12 ±2.35
61.07 ±0.56 48.27 ±3.50 62.38 ±0.48
51.49 ±1.13 - 50.43 ±1.48 47.20 ±0.89 62.85 ±0.84 53.21 ±3.04 61.52 ±0.52
UN
Trade 62.28 ±0.50
58.82 ±0.98 62.72 ±0.12 59.99 ±3.50 67.05 ±0.21 - 63.76 ±0.07 63.48 ±0.37 72.56 ±1.47 67.25 ±1.05 67.26 ±0.10
UN
Vote 58.13 ±1.43
55.13 ±3.46 51.83 ±1.35 61.23 ±2.71 48.34 ±0.76 - 50.51 ±1.05 50.04 ±0.86 66.26 ±5.48 56.73 ±0.69 69.17 ±0.53
Contact 95.37 ±0.92
91.89 ±0.38 96.53 ±0.10 94.84 ±0.75 89.07 ±0.34 - 93.05 ±0.09 92.83 ±0.05 96.67 ±0.45 98.30 ±0.02 98.38 ±0.02
A
vg. Rank 6.62
7.77 7.23 5.31 5.62 - 7.92 7.15 3.31 2.62 1.46
(2) CNE-N achieves the best performance with low computa-
tion cost. This is due to three reasons. Firstly, CNE-N constructs
an edge neighborhood subgraph with a short first-order sequence
length and considers high-order neighbors by accessing memory,
which is more efficient than other sample-based subgraph construc-
tion methods like TGAT and TGN. Secondly, CNE-N computes
co-neighbor encoding through vector-based parallel computing,
which is more efficient than other structure encoding methods like
CAWN or DyGFormer. Lastly, unlike other memory-based methods
such as NAT and Jodie, CNE-N does not store the node’s hidden
state in memory and instead accesses/updates the memory using
neural networks.4.3 Ablation Study
We conducted an ablation study to validate the effectiveness of
certain designs in CNE-N. This included examining the use of Co-
neighbor Encoding (CnE), the use of temporal-diverse memory
(TD), updating neighbor memory (Nup), and updating memory
with neighbors in the two-order subgraph (Tup). We removed each
module separately and referred to the remaining parts as w/o CnE,
w/o TD, w/o Nup, and w/o Tup. We evaluated the performance of
different variants on MOOC, LastFM, UCI, and UN Trade datasets
from four domains and presented the results in Figure 5. Our find-
ings indicate that CNE-N mostly performs best when using all the
components. Co-neighbor encoding has the most significant impact
 
427KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Figure 4: CNE-N vs SOTA CTDG methods on MOOC, UNtrade,
UCI, and LastFM. The horizontal axis shows the relative train-
ing time for each method as a multiple of CNE-N’s running
time. The vertical axis shows average precision.
on performance as it effectively captures the structural encoding
between query node pairs in the subgraph. The temporal-diverse
memory benefits model performance by generating structure en-
coding for subgraph different time intervals. Updating neighbor
memory can increase the influence of newly occurred links, provid-
ing more information to predict future links between higher-order
neighbors. Updating memory with two-order neighbors achieved
the best performance in some settings. For instance, in a bipar-
tite dataset like LastFM, two-order neighbors may replace other
neighbors in the hashtable as they cannot provide much helpful
information, to be more specific, an user does not directly interact
with another user.
4.4 Parameter Sensitivity
In this part, we test how the number of triangles being considered in
the subgraph influences model performance. As discussed in Section
3.2, the number of nodes being considered is up to 1+𝑙𝑠+𝑙𝑠𝑀, to
support our motivation that a larger subgraph results in a better
model performance, we test model performance by changing over
the two parameters.
Sizes of Hashtables . Hashtable size 𝑀also influences the co-
neighbor encoding accuracy. Reducing the size of the hashtable
will increase the likelihood of hash conflict, which can affect the
accuracy of co-neighbor encoding. We evaluated the sensitivity of
the hashtable size parameter on MOOC, LastFM, UCI, and UN Trade,
as shown in Figure 6. The results indicate that in the transductive
setting, model performance improves as we increase the hashtable
size. However, in the inductive setting, performance is negatively
impacted when the size of the hashtable is too large.
Length of the Sequence. The length of the historical interac-
tion sequence 𝑙𝑠is an important factor as it contains both valuable
information from neighbors and noise and also affects the number
of neighbor-induced subgraphs. In Figure 7, we tested the sensitiv-
ity of the Sequence length parameter on MOOC, LastFM, UCI, and
UN Trade datasets. For social datasets like UCI and MOOC, recent
neighbors can provide valuable information, whereas former neigh-
bors bring more noise. On the other hand, for user-item interactiondatasets like UNtrade and LastFM, longer historical information
can better model user preference.
5 RELATED WORK
5.1 Link Prediction in Graph
Previous work on temporal network representation learning used
GNNs to process static graph snapshot sequences taken at time
intervals, because of limitations in expressive power such as the
inability to count triangles and the inability to distinguish nodes
with the same structural roles, graph neural networks (GNNs) usu-
ally cannot extract the network evolution patterns, such as the
triadic closure rule that is common in social networks, and perform
poorly on link prediction (LP) tasks[ 5,36]. To enhance the expres-
sivity of GNNs on LP tasks, many methods have been proposed,
Assigning unique node IDs can distinguish different structural node
representations, but at the expense of generalization [ 1] and train-
ing convergence [ 26]. However, these methods are still limited by
the ability of GNNs to extract structural features from the shared
neighborhood of multiple nodes. To simplify computation and im-
prove generalization ability, the state-of-the-art LP methods only
perform computation on subgraphs containing links, given a set
of queried node sets, CAW-N [ 36], SEAL[ 30,43], GraIL[ 29] and
SubGNN[ 2] and other SGRL models first extract subgraphs (named
query-induced subgraphs) around the queried node sets, and then
encode the extracted subgraphs for prediction [ 5]. A large number
of studies have shown that SGRL models are more robust[14], and
more expressive[4, 14], compared with more complex techniques.
5.2 Dynamic Graph Learning
Representation learning on dynamic graphs is divided into DTDG
/CTDG methods. Discrete-time dynamic graph learning (DTDG)
methods, based on a low-cost discrete mechanism, tend to split
event sequences into several individual snapshots[ 19]. Each snap-
shot contains a part of the event sequence by time interval from
one month to one year. The model treats each snapshot as a static
graph with time features in edge and encodes them with tradi-
tional graph neural networks for spatial information propagation[ 7].
Some works factorized the connectivity matrix of each snapshot
for node representation[ 12,21,42]. Another strategy is to take ran-
dom walks on the snapshot and model the walk behaviors[ 10,11].
Node embedding was later aggregated by weighted sum [ 44], tem-
poral smoothness constraint, or sequential models like LSTM[ 15]
in E-LSTM-D[ 8] and Know-Evolve[ 31]. Those sequential models
were added along the snapshots to capture the temporal evolving
information among nodes in each snapshot. DTDG method can
apply to tasks that have seasonal patterns since encoding along
snapshots tends to learn patterns in a regular time interval. But at
the same time, these methods require manual determination of time
intervals, ignoring the temporal order of nodes in each snapshot,
and have low accuracy.
Continuous-Time Dynamic Graph Learning (CTDG) methods
are based on timestamped graphs[ 33] (where evolution is repre-
sented as a continuous-time function), and typically have better
flexibility and achieve higher accuracy. These methods split the
event sequence into fixed-length edge batches (200 to 600 events per
batch), leading to a large number of batches, therefore, to control
 
428Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) transductive setting
 (b) inductive setting
Figure 5: AP in both settings for ablation study of the CNE-N.
Figure 6: AP in the two settings with varying hashtable size.
Figure 7: AP in the two settings with varying historical inter-
action sequence length.the computation cost, CTDG methods only pass the message on
a sampled local subgraph, mailbox (also known as memory) were
designed to store information to be propagated between differ-
ent subgraphs. Representative methods include temporal random
walk[ 20,36] and neural extensions of temporal point processes
[32,33,45]. DynRep[ 32] and JODIE[ 17] developed a memory layer
to store history edge messages and evolve node representation
with a recurrent-based model when an edge occurs. TGAT[ 37] sam-
ple on a temporal graph for a local subgraph, and generate node
representation by graph embedding layer with random Fourier
features to encode timestamps. TGN generalizes the graph net-
work model of static graphs and most graph message passing type
architectures[ 24], which combines Jodie and TGAT. Edgebank[ 22]
discusses the influence of different negative sample strategies. Be-
sides, Causal Anonymous Walk-Networks (CAW-N)[ 36] computes
the node embedding with a set of anonymized random walks, and
aggregates them with recurrent model and attention operation.
6 CONCLUSION
In this paper, we discussed the trade-off between accuracy and
complexity in structure encoding in CTDG methods. We proposed
a Co-Neighbor Encoding Schema (CNES) method to achieve the
balance. CNES generates structure encoding by attaching a regular-
sized hashtable-based neighbor memory to each node and calcu-
lating the number of common neighbors between the end node
and each neighbor node with vector-based parallel computation. A
Temporal-Diverse Memory is used to generate structure encoding
for subgraphs at different time intervals. With the aforementioned
techniques, we proposed a dynamic graph learning model called
CNE-N. It achieves the highest performance in 10 of 13 datasets
while being computationally efficient.
7 ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China (62272023, 51991395, 51991391, U1811463) and, the
S&T Program of Hebei(225A0802D).
 
429KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
REFERENCES
[1]Ralph Abboud, İsmail İlkan Ceylan, Martin Grohe, and Thomas Lukasiewicz.
2020. The Surprising Power of Graph Neural Networks with Random Node
Initialization. arXiv preprint arXiv:2010.01179 (2020). ^5^
[2]Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. 2020. Sub-
graph Neural Networks. In Advances in Neural Information Processing Systems
(NeurIPS), Vol. 33. 8017–8029.
[3]Unai Alvarez-Rodriguez, Federico Battiston, Guilherme Ferraz de Arruda, Yamir
Moreno, Matjaž Perc, and Vito Latora. 2021. Evolutionary dynamics of higher-
order interactions in social networks. Nature Human Behaviour 5, 5 (2021),
586–595.
[4]Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou, and Michael Bronstein.
2022. Improving graph neural network expressivity via subgraph isomorphism
counting. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[5]Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca,
Thomas Markovich, Nils Hammerla, Michael M Bronstein, and Max Hansmire.
2022. Graph neural networks for link prediction with subgraph sketching. arXiv
preprint arXiv:2209.15486 (2022).
[6]Yen-Yu Chang, Pan Li, Rok Sosic, MH Afifi, Marco Schweighauser, and Jure
Leskovec. 2021. F-fade: Frequency factorization for anomaly detection in edge
streams. In Proceedings of the 14th ACM International Conference on Web Search
and Data Mining. 589–597.
[7]Jinyin Chen, Xueke Wang, and Xuanheng Xu. 2022. GC-LSTM: Graph convolution
embedded LSTM for dynamic network link prediction. Applied Intelligence (2022),
1–16.
[8]Jinyin Chen, Jian Zhang, Xuanheng Xu, Chenbo Fu, Dan Zhang, Qingpeng Zhang,
and Qi Xuan. 2019. E-LSTM-D: A deep learning framework for dynamic network
link prediction. IEEE Transactions on Systems, Man, and Cybernetics: Systems 51,
6 (2019), 3699–3712.
[9]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang
Tong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated Model
Architectures For Temporal Networks? arXiv preprint arXiv:2302.11636 (2023).
[10] Sam De Winter, Tim Decuypere, Sandra Mitrović, Bart Baesens, and Jochen
De Weerdt. 2018. Combining temporal aspects of dynamic networks with
Node2Vec for a more efficient dynamic link prediction. In 2018 IEEE/ACM interna-
tional conference on advances in social networks analysis and mining (ASONAM) .
IEEE, 1234–1241.
[11] Lun Du, Yun Wang, Guojie Song, Zhicong Lu, and Junshan Wang. 2018. Dy-
namic network embedding: An extended approach for skip-gram based network
embedding.. In IJCAI, Vol. 2018. 2086–2092.
[12] Daniel M Dunlavy, Tamara G Kolda, and Evrim Acar. 2011. Temporal link
prediction using matrix and tensor factorizations. ACM Transactions on Knowledge
Discovery from Data (TKDD) 5, 2 (2011), 1–27.
[13] Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip S Yu.
2021. Continuous-time sequential recommendation with temporal graph collab-
orative transformer. In Proceedings of the 30th ACM international conference on
information & knowledge management. 433–442.
[14] Fabrizio Frasca, Beatrice Bevilacqua, Michael M Bronstein, and Haggai Maron.
2022. Understanding and Extending Subgraph GNNs by Rethinking Their Sym-
metries. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 35.
[15] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence
labelling with recurrent neural networks (2012), 37–45.
[16] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2018. DyRep: Learning Repre-
sentations Over Dynamic Graphs. arXiv preprint arXiv:1803.04051 (2018). ^18^
[17] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In Proceedings of the 25th
ACM SIGKDD international conference on knowledge discovery & data mining.
1269–1278.
[18] Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, and Philip S Yu.
2021. Dynamic Graph Collaborative Filtering. arXiv preprint arXiv:2101.02844
(2021).
[19] David Liben-Nowell and Jon Kleinberg. 2007. The link-prediction problem for
social networks. Journal of the American Society for Information Science and
Technology 58, 7 (2007), 1019–1031.
[20] Yuhong Luo and Pan Li. 2022. Neighborhood-aware scalable temporal network
representation learning. In Learning on Graphs Conference. PMLR, 1–1.
[21] Yunpu Ma, Volker Tresp, and Erik A Daxberger. 2019. Embedding models for
episodic knowledge graphs. Journal of Web Semantics 59 (2019), 100490.
[22] Farimah Poursafaei, Andy Huang, Kellin Pelrine, and Reihaneh Rabbany. 2022. To-
wards Better Evaluation for Dynamic Link Prediction. In Thirty-sixth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track.
[23] Stephen Ranshous, Shitian Shen, Danai Koutra, Steve Harenberg, Christos Falout-
sos, and Nagiza F Samatova. 2015. Anomaly detection in dynamic networks:
a survey. Wiley Interdisciplinary Reviews: Computational Statistics 7, 3 (2015),
223–247.
[24] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learningon dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).
[25] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael M. Bronstein. 2020. Temporal Graph Networks for Deep
Learning on Dynamic Graphs. CoRR abs/2006.10637 (2020).
[26] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. 2020. Random Features
Strengthen Graph Neural Networks. arXiv preprint arXiv:2002.03155 (2020). ^10^
[27] Weiping Song, Zhiping Xiao, Yifan Wang, Laurent Charlin, Ming Zhang, and Jian
Tang. 2019. Session-based social recommendation via dynamic graph attention
networks. In Proceedings of the Twelfth ACM international conference on web
search and data mining. 555–563.
[28] A. H. Souza, D. Mesquita, S. Kaski, and V. Garg. 2022. Provably expressive
temporal graph networks. In Advances in Neural Information Processing Systems
(NeurIPS).
[29] Komal K. Teru, Etienne Denis, and Will Hamilton. 2020. Inductive Relation
Prediction by Subgraph Reasoning. In Proceedings of the International Conference
on Machine Learning (ICML). 9448–9457. ^41^
[30] Komal K. Teru, Etienne Denis, and William L. Hamilton. 2022. Labeling Trick: A
Theory of Using Graph Neural Networks for Multi-Node Representation Learning.
arXiv preprint arXiv:2201.07858v1 [cs.LG] (2022). ^42^
[31] Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. 2017. Know-evolve: Deep
temporal reasoning for dynamic knowledge graphs. In international conference
on machine learning. PMLR, 3462–3471.
[32] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.
Dyrep: Learning representations over dynamic graphs. In International conference
on learning representations.
[33] Wang Y. Song L.. Trivedi R., Dai H. 2017. Know-evolve: Deep temporal reasoning
for dynamic knowledge graphs. In International Conference on Machine Learning
(ICML). 3462–3471.
[34] Andrew Z Wang, Rex Ying, Pan Li, Nikhil Rao, Karthik Subbian, and Jure Leskovec.
2021. Bipartite dynamic representations for abuse detection. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3638–3648.
[35] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He,
Le Song, Jingren Zhou, and Hongxia Yang. 2021. Tcl: Transformer-based dynamic
graph modelling via contrastive learning. arXiv preprint arXiv:2105.07944 (2021).
[36] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.
Inductive Representation Learning in Temporal Networks via Causal Anonymous
Walks. In International Conference on Learning Representations (ICLR).
[37] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint
arXiv:2002.07962 (2020).
[38] Le Yu, Bowen Du, Xiao Hu, Leilei Sun, Liangzhe Han, and Weifeng Lv. 2021. Deep
spatio-temporal graph convolutional network for traffic accident prediction.
Neurocomputing 423 (2021), 135–147.
[39] Le Yu, Zihang Liu, Tongyu Zhu, Leilei Sun, Bowen Du, and Weifeng Lv. 2022. Mod-
elling Evolutionary and Stationary User Preferences for Temporal Sets Prediction.
arXiv preprint arXiv:2204.05490 (2022).
[40] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better Dy-
namic Graph Learning: New Architecture and Unified Library. arXiv preprint
arXiv:2303.13047 (2023).
[41] Le Yu, Guanghui Wu, Leilei Sun, Bowen Du, and Weifeng Lv. 2022. Element-
guided Temporal Graph Representation Learning for Temporal Sets Prediction.
InProceedings of the ACM Web Conference 2022. 1902–1913.
[42] Wenchao Yu, Wei Cheng, Charu C Aggarwal, Haifeng Chen, and Wei Wang. 2017.
Link prediction with spatial and temporal consistency in dynamic networks.. In
IJCAI. 3343–3349.
[43] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural
Networks. Advances in Neural Information Processing Systems (NeurIPS) (2018).
^23^
[44] Jia Zhu, Qing Xie, and Eun Jung Chin. 2012. A hybrid time-series link prediction
framework for large social network. In International Conference on Database and
Expert Systems Applications (DEXA). Springer, 345–359.
[45] Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, and Junjie Wu. 2018.
Embedding Temporal Network via Neighborhood Formation. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. ACM, 2857–2866.
 
430Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
A APPENDIX
In the appendix, details of the experiments are introduced.
A.1 Descriptions of Datasets
These thirteen datasets (Wikipedia, Reddit, MOOC, LastFM, Enron,
Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote,
and Contact) are collected by Edgebank[ 22] and cover diverse do-
mains, detailed statistics are shown in Table 4.
Wikipedia: consists of the bipartite interaction graph between
editors and Wiki pages over a month [ 22]. Nodes represent editors
and pages, and links denote the editing behaviors with timestamps.
The features of the links are 172-dimensional Linguistic Inquiry
and Word Count (LIWC) vectors.
Reddit: records thebipartite posts of users under subreddits
during one month[ 17]. Users and subreddits are the nodes, and
links are the timestamped posting requests. Each link has a 172-
dimensional LIWC feature.
MOOC: is a a student bipartite interaction network of online
sources units and models the student’s access to course videos or
questions as links, and represents them by four-dimensional feature
vectors[17].
LastFM: is a bipartite network that records the songs listened
to by users over one month. Users and songs are nodes, and links
represent the listening behaviors of users.
Enron: records the email communications between employees
of the ENRON energy corporation over three years.
UCI: is an online communication network that models university
students as nodes and messages posted by the students as links.
Flights: is a dynamic flight network that illustrates the evo-
lution of air traffic during the COVID-19 pandemic. Airports are
represented by nodes, and links denote tracked flights. Each link is
associated with a weight indicating the number of flights between
two airports in a day.
Can. Parl.: is a dynamic political network that captures interac-
tions between Canadian Members of Parliament (MPs) from 2006 to
2019. Each node in the network represents an MP from an electoral
district, and a link is established between two MPs when they both
vote “yes” on a bill. The weight of each link corresponds to the
number of times one MP voted “yes” for another MP in a year.
US Legis.: is a network that tracks the social interactions be-
tween legislators in the US Senate. It records the number of times
two congresspersons have co-sponsored a bill in a given congress,
and the weight of each link represents this number.
UN Trade: tracks the food and agriculture trade between 181
nations for over 30 years. The weight of each link represents the
total sum of normalized agriculture import or export values between
two specific countries.
UNvote: records roll-call votes in the United Nations General
Assembly. If two nations both voted "yes" to an item, the weight of
the link between them is increased by one.
Contact: describes how the physical proximity evolves among
about 700 university students over a month. Each student has a
unique identifier and links denote that they are within 16 proximity
to each other. Each link is associated with a weight, revealing the
physical proximity between students.A.2 Descriptions of Baselines
We select the following ten baselines:
JODIE[ 17] focuses on bipartite networks of instantaneous user-
item interactions. It employs two coupled RNNs to update the
representation of the users and items recursively. A projection
operation is introduced to learn the future representation trajectory
of each user/item.
DyRep[ 16] has a custom RNN that updates node representations
upon observation of a new edge. For obtaining the neighbor weights
at each time, DyRep uses a temporal attention mechanism, which
is parameterized by the recurrent architecture.
TGAT[ 37] computes the node representation by aggregating
features from each node’s temporal-topological neighbors based
on the self-attention mechanism. It is also equipped with a time
encoding function for capturing temporal patterns.
TGN[ 25] maintains an evolving memory for each node and
updates this memory when the node is observed in an interaction,
which is achieved by the message function, message aggregator, and
memory updater. An embedding module is leveraged to generate
the temporal representations of nodes.
CAWN[ 36] first extracts multiple causal anonymous walks for
each node, which can explore the causality of network dynamics
and generate relative node identities. Then, it utilizes recurrent
neural networks to encode each walk and aggregates these walks
to obtain the final node representation.
EdgeBank[22] is a pure memory-based approach for transduc-
tive dynamic link prediction. It stores the observed interactions in
the memory unit and updates the memory through various strate-
gies. An interaction will be predicted as positive if it is retained in
the memory and negative otherwise.
TCL[ 35] generates each node’s interaction sequence by perform-
ing a breadth-first search algorithm on the temporal dependency
interaction sub-graph. Then, it presents a graph transformer that
considers both graph topology and temporal information to learn
node representations. It also incorporates a cross-attention opera-
tion for modeling the interdependencies of two interaction nodes.
GraphMixer[ 9] shows that a fixed-time encoding function per-
forms better than the trainable version. It incorporates the fixed
function into a link encoder based on MLP-Mixer to learn from
temporal links. A node encoder with neighbor mean-pooling is
employed to summarize node features.
NAT[ 20] adopts a novel dictionary-type neighborhood repre-
sentation to gather the temporal neighbors of each node, and it
then uses a recurrent process to learn the node representation from
the historical neighbors of the current node and an RFF-based time
embedding. The method constructs the query-induced subgraph
without using neighbor samples to reduce computation costs.
DyGFormer[ 40] is a Transformer-based architecture for dy-
namic graph learning that only picks up information from pre-
vious first-hop interactions between nodes. It creates a neighbor
co-occurrence encoding scheme based on a patching method and
feeds them to the Transformer. This method enables the model to
benefit from longer histories by exploring the correlations of the
source node and destination node based on their sequences.
 
431KDD ’24, August 25–29, 2024, Barcelona, Spain Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, & Bowen Du
Table 4: Statistics of the datasets
Datasets Domains
#Nodes #Links #Node & Link Features Bipartite Duration
Wikip
edia So
cial 9,227 157,474 - & 172 True 1 month
Reddit So
cial 10,984 672,447 - & 172 True 1 month
MOOC Interaction
7,144 411,749 – & 4 True 17 months
LastFM Interaction
1,980 1,293,103 – & – True 1 month
Enron So
cial 184 125,235 – & – False 3 years
Social Evo. Pr
oximity 74 2,099,519 – & 2 False 8 months
UCI So
cial 1,899 59,835 – & – False 196 days
Flights T
ransport 13,169 1,927,145 – & 1 False 4 months
Can. Parl. Politics
734 74,478 – & 1 False 14 years
US Legis. Politics
225 60,396 – & 1 False 12 congresses
UN Trade Economics
255 507,497 – & 1 False 32 years
UN Vote Politics
201 1,035,742 – & 1 False 72 years
Contact Pr
oximity 692 2,426,279 – & 1 False 1 month
Table 5: Statistics of the TGB datasets
Datasets Domains
#Nodes #Links #Steps #Surprise
tgbl-wiki Interaction
9,227 157,474 152,757 0.108
tgbl-review Rating
352,637 4,873,540 6,865 0.987
tgbl-coin T
ransaction 638,486 22,809,486 1,295,720 0.120
tgbl-comment So
cial 994,790 44,314,507 30,998,030 0.823
Table 6: MRR for dynamic link property prediction, where
Val is the abbreviation of Validation.
Sets Metho
ds tgbl-wiki
tgbl-review tgbl-coin tgbl-comment
V
alJODIE 71.42±0.76 34.76±0.06 -
-
D
yRep 59.38±1.82
33.85±0.18 51.20±1.40
29.10±2.80
T
GAT 65.14±1.22
17.24±0.89 60.47±0.22 50.73±2.47
T
GN 73.80±0.39
33.17±0.13 60.70±1.40 35.60±1.90
CA
WN 75.36±0.34
20.00±0.10 - -
EdgeBank∞ 56.13±0.00
2.29±0.00 31.54±0.00 10.87±0.00
EdgeBank tw-ts 66.51±0.00
2.90±0.00 49.67±0.00 12.44±0.00
T
CL 80.82±0.14
17.99±1.72 66.85 ±0.27 65.10±0.67
GraphMixer 63.87±0.53
28.28±2.07 70.38±0.40 70.19±0.23
NA
T 77.30±1.10
30.20±1.10 - -
D
yGFormer 81.62±0.46
21.92±1.74 72.97±0.23
61.33±0.27
CNE-N 82.29±0.12 19.70±0.18 74.39±0.24
73.21 ±0.21
T
estJODIE 63.05±1.69 41.43±0.15 -
-
D
yRep 51.91±1.95
40.06±0.59 45.20±4.60
28.90±3.30
T
GAT 59.94±1.63
19.64±0.23 60.92±0.57 56.20±2.11
T
GN 68.93±0.53
37.48±0.23 58.60±3.70 37.90±2.10
CA
WN 73.04±0.60
19.30±0.10 - -
EdgeBank∞ 52.50±0.00
2.29±0.00 35.90±0.00 12.85±0.00
EdgeBank tw-ts 63.25±0.00
2.94±0.00 57.36±0.00 14.94±0.00
T
CL 78.11±0.20
16.51±1.85 68.66±0.30 70.11±0.83
GraphMixer 59.75±0.39
36.89±1.50 75.57±0.27 76.17±0.17
NA
T 74.90±1.00
34.10±2.00 - -
D
yGFormer 79.83±0.42 22.39±1.52
75.17±0.38 67.03±0.14
CNE-N 80.24±0.20 26.12±0.25 77.24±0.21
78.97 ±0.14A.3 Performance on TGB
Recently, researchers have noticed the size of benchmark datasets
for the dynamic link prediction task is relatively small and proposed
a large-scale temporal graph benchmark (TGB1), we tested our
method with the code provided by DyGLib-TGB2. Because the
validation of the datasets takes days for each model, we directly
use the baseline performance reported by DyGLib-TGB. Besides,
due to the downloading issue in tgbl-flight, we do not compare our
method on that dataset. The statistics of the datasets are listed in
Table 5
We have two key observations from Table 6. Firstly, CNE-N
achieves the best performance on all four datasets against baseline
models, even on datasets with low neighbor node re-interact prob-
ability (bad performance with EdgeBank, the model ), indicating
co-neighbor encoding can effectively describe the evolution of the
dynamic graph rather than the re-occurrence of the neighbor node.
Secondly, we compare the efficiency between our method and two
efficient baselines Jodie and GraphMixer in the largest two datasets
tgbl-coin and tgbl-comment, Jodie takes 124 hours and 63 hours
for one epoch of the training set, GraphMixer takes 2 hours, and 4
hours, and CNEN takes 0.67 hours and 1.42 hours, indicating the
efficiency of our method on large scale dataset.
1https://tgb.complexdatalab.com
2https://github.com/yule-BUAA/DyGLib_TGB
 
432