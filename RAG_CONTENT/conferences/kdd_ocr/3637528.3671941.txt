Embedding Two-View Knowledge Graphs with Class Inheritance
and Structural Similarity
Kyuhwan Yeom
Computer Science
Yonsei University
Seoul, Republic of Korea
tomma1121@yonsei.ac.krHyeongjun Yang
Computer Science
Yonsei University
Seoul, Republic of Korea
edbm95@yonsei.ac.krGayeon Park
Artificial Intelligence
Yonsei University
Seoul, Republic of Korea
gayeon603@yonsei.ac.krMyeongheon Jeon
Computer Science
Yonsei University
Seoul, Republic of Korea
jmh9506@yonsei.ac.kr
Yunjeong Ko
Computer Science
Yonsei University
Seoul, Republic of Korea
yjko1853@yonsei.ac.krByungkook Oh
Computer Science and
Engineering
Konkuk University
Seoul, Republic of Korea
bkoh@konkuk.ac.krKyong-Ho Lee∗
Computer Science
Yonsei University
Seoul, Republic of Korea
khlee89@yonsei.ac.kr
ABSTRACT
Numerous large-scale knowledge graphs (KGs) fundamentally rep-
resent two-view KGs: an ontology-view KG with abstract classes in
ontology and an instance-view KG with specific collections of enti-
ties instantiated from ontology classes. Two-view KG embedding
aims to jointly learn continuous vector representations of entities
and relations in the aforementioned two-view KGs. In essence,
an ontology schema exhibits a tree-like structure guided by class
hierarchies, which leads classes to form inheritance hierarchies.
However, existing two-view KG embedding models neglect those
hierarchies, which provides the necessity to reflect class inheri-
tance. On the other hand, KG is constructed based on a pre-defined
ontology schema that includes heterogeneous relations between
classes. Furthermore, these relations are defined within the scope
of those among classes since instances inherit all the properties
of their corresponding classes, which reveals structural similarity
between two multi-relational networks. Despite the consideration
to bridge the gap among two-view KG representations, existing
methods ignore the existence of structural similarity between two-
view KGs. To address these issues, we propose a novel two-view
KG embedding model, CISS, considering Class Inheritance and
Structural Similarity between two-view KGs. To deal with class in-
heritance, we utilize class sets, each of which is composed of sibling
classes, to learn fine-grained class representations. In addition, we
configure virtual instance-view KG from clustered instances and
compare subgraph representations of two-view KGs to enhance
structural similarity between them. Experimental results show our
superior performance compared to existing models.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671941CCS CONCEPTS
•Computing methodologies →Semantic networks .
KEYWORDS
Knowledge Graph; Ontology; Class Inheritance; Structural Similar-
ity
ACM Reference Format:
Kyuhwan Yeom, Hyeongjun Yang, Gayeon Park, Myeongheon Jeon, Yun-
jeong Ko, Byungkook Oh, and Kyong-Ho Lee. 2024. Embedding Two-View
Knowledge Graphs with Class Inheritance and Structural Similarity . In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671941
1 INTRODUCTION
Knowledge graph (KG) is a collection of factual triples composed of
entities and relations, which represent human knowledge in a struc-
tured way[ 41]. Various open KGs such as DBpedia[ 18], YAGO[ 21],
and Wikidata[ 35] contain well-known cross-domain entities, which
are applicable for diverse downstream tasks, e.g., link prediction[ 5],
question answering[ 27], and item recommendation[ 36]. KG embed-
dings represent triple components in a continuous vector space,
playing a vital role in above-mentioned applications[44].
In general, a KG is constructed from conceptual ontology which
contains concrete individuals of instances[ 33], naturally represent-
ing two-views: an ontology-view KG and an instance-view KG.
Ontology-view KG denotes an ontology schema containing abstract
classes, whereas instance-view KG contains specific collections of
entities that are instantiated from ontology classes. However, most
previous KG embedding studies[ 5,12–14,32,34,41] are catego-
rized based on whether they consider instance-view KG only, or
jointly represent two-view KGs. Representative KG embedding
models[ 5,32,34,41] are solely aware of KG representation learning
in an instance-view KG, which do not consider class information
presented by an ontology-view KG. On the other hand, to take ad-
vantage of correlation between instances and classes on cross-view
perspectives, [12–14] embed each KG of two-views.
In ontology-view KG, classes form a directed acyclic tree initi-
ated from a root class with hierarchical properties. A class may
3931
KDD ’24, August 25–29, 2024, Barcelona, Spain. Kyuhwan Yeom, et al.
have multiple parent classes and subclasses, describing inheritance
hierarchy of classes[ 26]. From class hierarchies, we denote sibling
classes as a set of classes that are inherited from the same parent
and in the same level of hierarchies. However, existing two-view
KG embedding models[ 12,14] deal with hierarchical relationships
between classes in ontology-view KG, but fail to discriminate repre-
sentations of sibling classes based on their inheritance hierarchies.
Structural similarity between two graphs denotes likeness in
structural characteristics among them, and exists when two graphs
have similar compositions of nodes and edges, or exhibit analo-
gous connectivity patterns from each other. Meanwhile, as men-
tioned above, KG is constructed based on a pre-defined ontology
schema including heterogeneous relations between classes. From
unstructured natural language texts, instances and their relation-
ships are extracted as structured knowledge considering the ontol-
ogy schema[ 30]. After KG construction, instances inherit all the
properties of their corresponding classes so that relations between
them are determined within the scope of relations between classes.
Therefore, the inclusion of connectivity patterns in instance-view
KG implies structural similarity between two-view KGs from the KG
construction perspective. Prior two-view KG embedding studies[ 12–
14] link two-view KGs by mapping instances from instance-view
KG to class embedding space of ontology-view KG[ 12,14], or tak-
ing advantage of box embeddings to interpret class granularity[ 13].
However, none of these studies compare structural association be-
tween them. Taking KG construction process into account, there is
a need to consider the structural similarity between two-view KGs.
Figure 1 shows an example of two-view KGs. Instances and
classes are connected through cross-links labeled with "typeOf" re-
lation across two-view KGs, and some instances such as "Germany "
and "Marco Sturm " are associated with multiple classes. In addition,
class "Hockey Player " is inherited by multiple parents, "Player " and
"Athlete ", for which polyhierarchy exists between two classes. Also,
those two classes are inherited from the same parent class "Person ",
and their sibling class "Coach " does not share an identical child class.
We can infer the relevance between those sibling classes based on
class inheritance, so that a class pair ("Player ", "Athlete ") reveals
higher relevance than that of ("Player ", "Coach "). In the meantime,
relation "isLocatedIn " links ("Germany National Basketball Team ",
"Germany ") in the instance-view KG, which also can be seen be-
tween their corresponding classes, ("Basketball Team ", "Country ")
in the ontology-view KG. Expanding from ("Germany ", "Country ")
to ("Marco Sturm ", "Hockey Player ") in a similar manner, structural
similarity between those subgraphs can be recognized.
To address these issues, we propose CISS, a novel two-view KG
embedding model considering Class Inheritance and Structural
Similarity between two-view KGs. In this paper, we attempt to
consider the relationship between classes in the same level of hier-
archies depending on class inheritance, while preserving the hier-
archical structure of each KG of two-views. First, to consider class
inheritance, we define class sets regarding polyhierarchies between
classes, so that fine-grained class representations can be learned
by utilizing discretized margins between class sets. Afterward, to
model structural similarity among two-view KGs, we configure
virtual instance-view KG. It contains instances as clustered repre-
sentations of instances that belong to the same class, and relations
appended between them utilizing existing relations. We compare
Figure 1: An example of two-view knowledge graphs. Classes
form a hierarchical structure by "isA" relation, and hetero-
geneous relations indicate various semantic relationships
including hierarchies between classes.
subgraph representations from each KG of two-views to dimin-
ish the structural gap between them. Experiments show that our
proposed CISS consistently outperforms existing state-of-the-art
models on three public datasets. To this end, main contributions of
this paper correspond to:
•We reflect class inheritance to identify classes within the
same level of hierarchies utilizing elaborated class sets. Our
model generates fine-grained class embedding based on the
relevance between classes.
•We propose structural similarity modeling between two-
view KGs with virtual KG construction, followed by com-
parison of subgraph representations to reduce the structural
gap between two-view KGs.
•In experiments, we conduct link prediction and entity typing
to evaluate the performance of CISS and demonstrate the
superiority of our model. Experimental results in both tasks
show that our proposed model can learn fine-grained class
representations and capture structural similarity between
two-view KGs.
2 RELATED WORK
2.1 Instance-view KG Embedding
2.1.1 Triple-based KG Embedding. Traditional approach for KG
embedding relies on single triples during representation learning.
It can be broadly classified into two categories: translation-based
methods and semantic matching-based methods. Translation-based
methods such as TransE[ 5], TransR[ 19], and RotatE[ 32] utilize
distance-based scoring function defined by translational opera-
tions. On the other hand, semantic matching-based methods like
ComplEx[ 34] and DistMult[ 39] apply similarity-based scoring func-
tion with an assumption that entities with similar semantics show
similar representations[ 37]. Prior studies in both categories aim
to model various relation patterns existing in KG but have diffi-
culties grasping relational semantics between entities along with
hierarchical relationships.
2.1.2 Hierarchy-aware KG Embedding. Recently, there arises an
interest in learning hierarchical structure of KG. [ 42] considers hier-
archical relation structure conformed by relation clusters. MuRP[ 3]
3932Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity KDD ’24, August 25–29, 2024, Barcelona, Spain.
and ConE[ 2] embed KG on hyperbolic spaces with negative cur-
vature, e.g., Poincar ´𝑒ball and hyperbolic entailment cones, respec-
tively. HAKE[ 41] applies polar embedding with separated modulus
and phase representations on polar coordinate system.
2.1.3 Class-aware KG Embedding. General-purpose KGs such as
Freebase[ 4] and Wikidata[ 35] involve class information for each
instance, and various studies use them as auxiliary information.
TransT[ 20] and TaRP[ 9] compute class correlation with consid-
eration of semantic similarities between entities. CAKE[ 24] auto-
matically extracts commonsense by conjugating class information
when generating high-quality negative triples. However, preceding
models only incorporate class information within instance-view
KG, failing to utilize heterogeneous relations between classes.
2.2 Ontology Embedding
In addition to instance-view KG embedding, there has been a grow-
ing interest in embedding expressive OWL ontologies[ 22] them-
selves. First, we clarify the difference between an ontology and
an ontology-view KG: Ontology is an engineering artifact with a
set of explicit axioms (or rules) regarding classes, properties, and
instances, while ontology-view KG is a graph representation of
relationships between classes. To the best of our knowledge, no
ontology-view KG embedding studies exist, so we confine our scope
to ontology embedding in this subsection.
Given description logic such as existential language( EL++[1])
and attributive language( ALC [29]), ontology embedding mod-
els are designed to explore ontologies to embed named ontology
components including class, instance, or property. For instance,
[17,25,38] learn geometric KG embeddings to incorporate EL++
operators with the help of a relational embedding model such as
TransE[ 5]. However, they assume that the ontology contains ax-
ioms, each of which is followed by its corresponding normal form.
Another line of works like OWL2Vec*[ 8] and OPA2Vec[ 31] focus
on extracting a suitable corpus by training a Word2Vec[ 23] model.
The aforementioned models aim to utilize existing logical patterns
from description logic within an ontology to embed semantics of
OWL ontologies; their logical constructors as well as graph struc-
ture. Instead, we focus on automatically learning relational patterns
to embed structure of two-view KGs without any auxiliary infor-
mation.
2.3 Two-view KG Embedding
Two-view KG embedding models are utilized to jointly embed two-
view KGs. JOIE[ 12] represents ontology-view KG and instance-view
KG in different embedding spaces, where bridging knowledge from
two-views is accomplished by mapping each instance to its cor-
responding class. In addition, JOIE models class hierarchies by
mapping finer classes to high-level coarser classes. However, in
JOIE, similar representations of lower-level concepts are propagated
to those at higher levels because of a shared projection mapping
matrix among entire classes. DGS[ 14] extends embedding space
from euclidean space adopted by JOIE to geometric spaces. DGS
models class hierarchies in hyperbolic space and cyclic nature be-
tween instances in spherical space. In this case, cross-links are
represented in spherical space where those two spaces overlap.
However, the hyperbolic space in DGS places parent and childclasses close to each other, with potential for reversed class hierar-
chies. Concept2Box[ 13] employs box embedding to model hierar-
chical relations and granularity between/within classes. All things
considered, the aforementioned models do not consider structural
similarity of two-view KGs nor class inheritance from ontology-
view KG.
3 METHOD
In this section, we describe in detail our proposed model CISS,
which consists of three main subsections: Hierarchy-aware embed-
ding initialization, class set modeling, and cross-view association.
We first initialize each KG of two-views with a hierarchy-aware KG
embedding model. Later on, we differentiate class representations
considering class inheritance by modeling class sets. Finally, we
model structural similarity between two-view KGs for cross-view
association, along with instance alignment using cross-links. The
overall architecture of our model is shown in Figure 2.
3.1 Problem Definition and Setup
In this paper, we define a KG G={(ℎ,𝑟,𝑡)|ℎ,𝑡∈E,𝑟∈R}, where
Erepresents the set of entities including both classes and instances,
andRis the set of relations between entities. Ontology-view KG is
defined byGO={(ℎ,𝑟,𝑡)|ℎ,𝑡∈EC,𝑟∈RC}whereECandRCde-
notes the set of classes and relations, respectively. On the other hand,
instance-view KG is defined by GI={(ℎ,𝑟,𝑡)|ℎ,𝑡∈EI,𝑟∈RI}
whereEIdenotes the set of instances, and RIdenotes the set of
relations between instances. Cross-links across two-view KGs are
defined byS={(ℎ,𝑡)|ℎ∈EI,𝑡∈EC}that couple instance and its
corresponding class with a special "typeOf " relation. Since instances
are specific examples of classes, a single instance may belong to
multiple classes at different levels [ 7]. The aforementioned class
hierarchy is formed by hyponym relations labeled with "subClas-
sOf" or"isA". It is well known that contemporary real-world KGs
including instance-view KG do include hierarchical relations[41].
3.2 Hierarchy-aware Embedding Initialization
To learn initial representations of each KG of two-views, we firstly
embed eachGOandGIin distinct embedding spaces. Without
loss of generality, let us denote both instances and classes as en-
tities in this subsection. Since both GOandGIreveal implicit
hierarchical structures, inspired by HAKE[ 41], we represent enti-
ties and relations with separated modulus and phase embeddings
that form a polar coordinate system. HAKE models entities at dif-
ferent levels of hierarchy by modulus parts, and entities at the
same level of hierarchy by phase parts. Therefore, we embed each
triple(ℎ,𝑟,𝑡)ash=[h𝑚;h𝑝],r=[r𝑚;r𝑝],t=[t𝑚;t𝑝]where
h𝑚,t𝑚∈R𝑑,r𝑚∈R𝑑+, and h𝑝,r𝑝,t𝑝∈ [0,2𝜋)𝑑. Modulus em-
beddings are h𝑚,r𝑚,t𝑚, and phase embeddings are h𝑝,r𝑝,t𝑝, with
embedding dimension 𝑑.
For modulus part, each entry [h𝑚]𝑖,[t𝑚]𝑖is defined as a modu-
lus in terms of 𝑖-th axis along polar coordinate system, and [r𝑚]𝑖
is defined as a scaling transformation between them. Thus, formu-
lation of the modulus part is defined as h𝑚◦r𝑚=t𝑚. Assuming
the origin of a polar coordinate system as a root, entities at higher
levels of the hierarchy are expected to have smaller modulus than
those at lower levels. For instance, ("Hockey team ","subClassOf",
3933KDD ’24, August 25–29, 2024, Barcelona, Spain. Kyuhwan Yeom, et al.
Figure 2: Overview of our CISS model.
"Team ") and ("Team ","subClassOf", "Group ") in Figure 1 configure
hierarchical transitive relationship, so that the modulus value of
"Team" becomes lower than that of "Hockey team". The same princi-
ple applies in instance-view KG for "hypernym" relation that reflects
the semantic difference between hierarchical instances.
For phase part, each entry
h𝑝
𝑖,
t𝑝
𝑖is defined as phase in
terms of𝑖-th axis along polar coordinate system, and
r𝑝
𝑖is defined
as a phase transformation from
h𝑝
𝑖to
t𝑝
𝑖. Properties of circular
rotation lead to formulation of the phase part as follows.
(h𝑝+r𝑝)mod 2𝜋=t𝑝,where hp,rp,tp∈[0,2𝜋)𝑑(1)
By combining modulus part and phase part, the score function is
defined as follows:
𝑓(ℎ,𝑟,𝑡)=−∥h𝑚◦r𝑚−t𝑚∥2−𝜆∥sin((h𝑝+r𝑝−t𝑝)/2)∥ 1,(2)
where𝜆∈Ris a learnt parameter by the model. The above scoring
function is identical to the formula proposed by HAKE. We do not
include mixture-bias added on original paper since its effect on
performance is relatively low.
3.3 Class Set Modeling
3.3.1 Overview. The objective of class set modeling is to learn
fine-grained phase embeddings of each class in aware of class in-
heritance. Inspired by ArcFace[ 10], we adjust class representations
by endowing margins for classes that possess higher relevance con-
sidering class sets. Assuming 𝑛number of class , 𝑥𝑖∈Rfor feature
of the𝑖-th sample belonging to the 𝑦𝑖-th class, and 𝑊𝑗∈R𝑑for𝑗-th
column of the weight 𝑊∈R𝑑×𝑛, additive angular margin loss[ 10]
is defined as follows:
𝐿𝐴𝑀=−log𝑒𝛼cos(𝜃𝑦𝑖+𝑚)
𝑒𝛼cos(𝜃𝑦𝑖+𝑚)+Í𝑛
𝑗=1,𝑗≠𝑦𝑖𝑒𝛼cos𝜃𝑦𝑗, (3)where𝜃𝑗stands for the angle between the feature 𝑥𝑖and the weight
𝑊𝑗,𝑚for an angular margin penalty between 𝑥𝑖and𝑊𝑦𝑖, and𝛼
for a scale factor.
Endowing equivalent margin between samples where each sam-
ple belongs to same class, Eq. (3) enforces higher similarity for intra
class samples and diversity for inter class samples[ 10]. From our
perspectives, we define a class set as group of sibling classes which
share same subclass, with an assumption that sibling classes that
are connected to same subclass are highly related to each other.
If none of sibling classes share a child, only a target class may be
an element of a class set. Afterward, we assign different marginal
values according to class co-relevance, so that classes within the
same class set can be distinguished by a small margin, and different
class sets can be distinguished by a larger margin.
3.3.2 Intra Class Set Modeling. Intra class set modeling aims to
differentiate classes within a class set. Following Eq. (3), a weight
matrix𝑊𝑗is required as a norm for measuring the angle between
class representations. Thus, we interpret phase embedding of a
target class as our weight matrix. and the target class plays a role as
comparison criteria to compare with other classes. We denote 𝑃as a
class set that includes a target class. Regarding 𝑥𝑖as an embedding
of a target class 𝑖in the phase part, we first compute the angle
between class representations as our relevance score using cosine
similarity, respectively.
𝜃𝑖=cos−1 
𝑥𝑇
𝑖𝑥𝑖
∥𝑥𝑖∥∥𝑥𝑖∥!
=0◦, 𝜃𝑗=cos−1 
𝑥𝑇
𝑖𝑥𝑗
∥𝑥𝑖∥∥𝑥𝑗∥!
,(4)
where𝑗∈𝑃denotes a class that belongs to the same class set with
𝑖. By endowing margin 𝑚1between a target class and others, and
adjusting score value with scale factor 𝛼, loss of our intra class set
modeling is defined as:
𝐿𝐼𝑛𝑡𝑟𝑎=−log𝑒𝛼cos(𝑚1)
𝑒𝛼cos(𝑚1)+Í
𝑗∈𝑃,𝑗≠𝑖𝑒𝛼cos𝜃𝑗(5)
3934Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity KDD ’24, August 25–29, 2024, Barcelona, Spain.
Above procedure differentiates sibling classes in a single class set
with minimal margins.
3.3.3 Inter Class Set Modeling. Inter class set modeling aims to
differentiate class sets themselves in a broader perspective. First
we divide two cases, having strict norm of belongings to sibling
classes of a target class, or not. We denote 𝑃as a class set that
includes a target class, 𝑆=
𝑆1,𝑆2,...,𝑆𝑘𝑠	
as class sets that do not
contain a target class but form sibling relationships with 𝑃, and
𝑁=
𝑁1,𝑁2,...,𝑁𝑘𝑛	as remaining class sets in the same level of
hierarchies. For each 𝑝∈𝑃, we calculate relevance score between
a target class and other classes that belong to a same class set as in
Eq. (4):
𝜃𝑝,𝑖=cos−1 
𝑥𝑇𝑝𝑥𝑖
∥𝑥𝑝∥∥𝑥𝑖∥!
(6)
Similarly, for each 𝑛∈𝑁𝑘𝑛and𝑠∈𝑆𝑘𝑠, to comprehend correlation
between class sets, relevance score between target class and others
in separated class sets is computed as:
𝜃𝑛,𝑖=cos−1𝑥𝑇𝑛𝑥𝑖
∥𝑥𝑛∥∥𝑥𝑖∥
,𝜃𝑠,𝑖=cos−1𝑥𝑇𝑠𝑥𝑖
∥𝑥𝑠∥∥𝑥𝑖∥
(7)
A first case is when sibling classes are inherited from same parent,
but are succeeded by divergent children. In this case, classes form a
class set based on whether or not they share child. Since classes in
each other class set show relatively low relevance scores compared
to that within a class set, we let 𝑚2a marginal value larger than
𝑚1by following loss function:
𝐿𝐼𝑛𝑡𝑒𝑟 𝑆=−∑︁
𝑝∈𝑃log𝑒𝛼cos(𝜃𝑝,𝑖+𝑚2)
𝑒𝛼cos(𝜃𝑝,𝑖+𝑚2)+Í𝑘𝑠
𝑘=1Í
𝑗∈𝑆𝑘,𝑗≠𝑖𝑒𝛼cos𝜃𝑗,𝑖
(8)
On the contrary, a second case is when sibling classes do not share
parent or child, meaning that no class inheritance appears in a
subtree structure composed by those classes. Therefore, we let 𝑚3
a larger marginal value than 𝑚2by following loss function:
𝐿𝐼𝑛𝑡𝑒𝑟 𝑁=−∑︁
𝑝∈𝑃log𝑒𝛼cos(𝜃𝑝,𝑖+𝑚3)
𝑒𝛼cos(𝜃𝑝,𝑖+𝑚3)+Í𝑘𝑛
𝑘=1Í
𝑗∈𝑁𝑘,𝑗≠𝑖𝑒𝛼cos𝜃𝑗,𝑖,
(9)
and our inter class set modeling loss is summed as :
𝐿𝐼𝑛𝑡𝑒𝑟=𝐿𝐼𝑛𝑡𝑒𝑟 𝑆+𝐿𝐼𝑛𝑡𝑒𝑟 𝑁(10)
Finally, for all 𝑒𝑐∈EH
CwhereEH
Cis a set of classes associated with
hyponym relations, we compute Eq. (5) and Eq. (10) concurrently
to derive our final class set modeling loss as follows:
𝐿𝐶𝑙𝑎𝑠𝑠=1
|EH
C|∑︁
𝑒𝑐∈EH
C(𝐿𝐼𝑛𝑡𝑟𝑎+𝐿𝐼𝑛𝑡𝑒𝑟) (11)
3.4 Cross-view Association
In this subsection, we consider two cross-view association ap-
proaches for two-view KGs. Different from learning embeddings
within instance-view KG, we consider the interaction between each
KG of two-views. First, we align instance embeddings to class em-
bedding space with cross-links in two-view KGs. In addition, we
derive a virtual instance-view KG that is structurally similar to
ontology-view KG to reduce the structural gap between them.3.4.1 Cross-link Instance Mapping. Based on cross-links between
GIandGO, cross-link instance mapping purposes to map each
instance to embedding space of its corresponding class using a
transformation function. This ensures mapped instance embedding
to be closer to its corresponding class representation. However, an
instance may form multiple cardinality, so-called 1-N relationships
with classes, and vice versa. To this end, discriminative projections
for individual instances are assigned to manage 1-N associations.
For each instance 𝑒𝑖∈EIand its class counterpart 𝑒𝑐∈EC, our
mapping function 𝑔is defined as:
𝑒𝑐←𝑔(𝑒𝑖)=𝜎(𝑀𝑒𝑐·𝑒𝑖+𝑏),∀(𝑒𝑖,𝑒𝑐)∈S, (12)
where𝑀𝑒𝑐is a projection matrix with respect to 𝑒𝑐,𝜎is a tanh
activation function, and 𝑏is a bias factor. Based on definition of
mapping function, our hinge loss for cross-links is minimized as
follows:
𝐿𝐶𝑟𝑜𝑠𝑠 𝐿=∑︁
(𝑒𝑖,𝑒𝑐)∈S∪(𝑒𝑖,𝑒′𝑐)∉Sh
𝛾𝐻𝐴+∥𝑒𝑐−𝑔(𝑒𝑖)∥2−∥𝑒′
𝑐−𝑔(𝑒𝑖)∥2i
+
(13)
3.4.2 Structural Similarity Modeling. Structural similarity model-
ing is designed to reduce the structural difference between GIand
GOby comparing subgraph representations derived from each KG.
On account of the subsumption of relations in GIforGO, there ex-
ists a substantial structural gap between them. Thus, to make their
connectivity patterns similar, we derive a virtual multi-relational
graph denoted as gGIfromGI.gGIincludes entities that contain
representations of all instances associated with the same class, and
relations that link those entities through existing relations in GI.
Assuming𝑁as the number of classes in S, we define a set of in-
stances that belong to the same class as 𝐼={𝐼𝑐1,𝐼𝑐2,...,𝐼𝑐𝑁}where
𝐼𝑐𝑛=
𝑒𝑖∈EI|(𝑒𝑖,𝑒𝑐𝑛)∈S	
. We define a virtual entity, whose rep-
resentation is the center of an instance set with averaging modulus
part and phase part of previously learned instance representations.
fe𝑛=𝑐𝑒𝑛𝑡𝑒𝑟(𝐼𝑐𝑛)=𝑀𝐸𝐴𝑁([e𝑖𝑚;e𝑖𝑝]) (14)
Afterwards, we add relations between entities by making reuse
of pre-defined relations in GI. Finally, we obtain a graph that is
literally "structurally similar" to GO. Since our objective is to make
two graphsGOandgGIcloser with their structural similarities, we
intend to make representations of GOandgGIanalogous.
To obtain representations of GOandgGI, we utilize message
passing schemes of GNN. However, instead of comparing repre-
sentations of the entire graph, we exploit the representation of a
subgraph which is retrieved from each triple. Subgraph-level expla-
nations are more intuitive and useful[ 40], and in terms of triples in
instance-view KG, we intend to compare them with their counter-
parts in ontology-view KG. From this point of view, for each triple
(˜ℎ,𝑟,˜𝑡)∈gGI, we retrieve enclosing subgraph by intersecting 𝑘-
hop neighbors of each entity, denoted as N𝑘(˜ℎ,˜𝑡)=N𝑘(˜ℎ)∩N𝑘(˜𝑡).
Assuming𝑙-th layer of GNN, we aggregate messages from neigh-
bors of each node 𝑖∈N𝑘(˜ℎ,˜𝑡)utilizing multi-relational R-GCN[ 28],
whose aggregating function is defined as:
a𝑙
𝑖=∑︁
𝑟∈R∑︁
𝑠∈N𝑟(𝑖)1
𝑐𝑟,𝑖W𝑙−1
𝑟a𝑙−1
𝑗+W𝑙−1
0a𝑙−1
𝑖, (15)
3935KDD ’24, August 25–29, 2024, Barcelona, Spain. Kyuhwan Yeom, et al.
Table 1: Dataset statistics
Ontology-View KG Instance-View KG
Datasets # Classes # Relations # Triples # Instances # Relations # Triples # Cross-Links
YAGO26K-906 906 30 8,962 26,078 34 390,738 9,962
DB111K-174 174 20 763 99,744 298 658,505 99,748
KACC-M 6,685 30 15,616 99,615 209 662,650 123,342
whereRdenotes the set of relations in a subgraph, N𝑟(𝑖)denotes
the set of neighboring nodes of node 𝑖under relation 𝑟,W𝑙𝑟∈
R2𝑑×2𝑑is the transformation matrix for each relation, and 𝑐𝑟,𝑖=
|N𝑟(𝑖)|is the normalization constant factor. After 𝐿layers, our final
subgraph representation for each triple is obtained by averaging
whole entity representations in enclosing subgraph:
g(˜ℎ,˜𝑡)=1
|N𝑘(˜ℎ,˜𝑡)|∑︁
𝑖∈N𝑘(˜ℎ,˜𝑡)a𝐿
𝑖(16)
Similarly, we obtain the subgraph representation for each triple
(ℎ,𝑟,𝑡)∈GOfrom a retrieved subgraph N𝑘(ℎ,𝑡).
Finally, for each pair of retrieved subgraphs from GOandgGI,
the similarity between subgraph representations g(ℎ,𝑡)andg(˜ℎ,˜𝑡)
is computed using cosine similarity between two graph representa-
tions as:
𝑆𝑖𝑚(g(ℎ,𝑡),g(˜ℎ,˜𝑡))=cos(g(ℎ,𝑡),g(˜ℎ,˜𝑡))=g(ℎ,𝑡)·g(˜ℎ,˜𝑡)
∥g(ℎ,𝑡)∥∥g(˜ℎ,˜𝑡)∥(17)
To minimize the gap between two graph representations to ensure
structural similarity, we compute the loss for entire triples in GO
andgGIas follows:
𝐿𝐶𝑟𝑜𝑠𝑠 𝑆=∑︁
(ℎ,𝑟,𝑡)∈GO,(˜ℎ,𝑟,˜𝑡)∈gGI(𝑆𝑖𝑚(g(ℎ,𝑡),g(˜ℎ,˜𝑡))−1)2(18)
Considering Eq. (13) and Eq. (18), our cross-view association loss is
defined as:
𝐿𝐶𝑟𝑜𝑠𝑠=𝛽𝐿𝐶𝑟𝑜𝑠𝑠 𝐿+𝐿𝐶𝑟𝑜𝑠𝑠 𝑆(19)
where𝛽∈[0,1]is controlling hyperparameter.
3.5 Training Objectives
To train our model, our final loss function is deduced as follows.
First, for embeddings initialization, we choose negative sampling
loss with self-adversarial training for our initial KG embedding loss
as with HAKE:
𝐿𝐼𝑛𝑖𝑡=−log𝜎(𝛾−𝑓(ℎ,𝑟,𝑡))−𝑛∑︁
𝑖=1𝑝(ℎ′
𝑖,𝑟,𝑡′
𝑖)log𝜎(𝑓(ℎ,𝑟,𝑡)−𝛾),
(20)
where𝛾is a fixed margin, 𝜎is a sigmoid activation function, and
(ℎ′
𝑖,𝑟,𝑡′
𝑖)is the𝑖−th negative triple. Specifically, sampling negative
triples is accomplished by the following distribution:
𝑝(ℎ′
𝑗,𝑟,𝑡′
𝑗|{(ℎ𝑖,𝑟𝑖,𝑡𝑖)})=exp𝑓(ℎ′
𝑗,𝑟,𝑡′
𝑗)
Í
𝑖exp𝑓(ℎ′
𝑖,𝑟,𝑡′
𝑖)(21)
All things together, we define a final loss to train our model as
follows:
𝐿=𝐿𝐼𝑛𝑖𝑡+𝜂1𝐿𝐶𝑙𝑎𝑠𝑠+𝜂2𝐿𝐶𝑟𝑜𝑠𝑠, (22)where𝜂1,𝜂2∈[0,1]are controlling hyperparameters.
4 EXPERIMENTS
In this section, we conducted empirical evaluations of our proposed
CISS. Firstly, we introduce the details of existing state-of-the-art
KG embedding models as well as datasets we utilized for our exper-
iments. Subsequently, we briefly explain our two evaluation tasks,
link prediction and entity typing, along with task summary and met-
rics. Finally, we demonstrate the effectiveness of CISS through com-
prehensive experimental results, consisting of performance com-
parison for each task, ablation study, and case study through gener-
ated embeddings. Further implementation details, time complexity
analysis, embedding dimension analysis, and hyperparameter sen-
sitivity on final loss are described in Appendix. Code and datasets
are publicly available at https://github.com/Yonsei-ICL/CISS .
4.1 Datasets
In this paper, we conducted experiments using three public datasets :
YAGO26K-906[ 12], DB111K-174[ 12], and KACC-M[ 43] that contain
information on two-view KGs. YAGO26K-906 and DB111K-174 are
proposed by JOIE[ 12], utilizing YAGO[ 21] and DBpedia[ 18] as base
ontologies, respectively, whereas KAAC-M utilizes Wikidata[ 35]
as base ontology. The statistics of each dataset are presented in
Table 1. It can be seen that the ratio of number of classes to number
of instances in two datasets proposed by JOIE is lower than that
of KACC-M. KACC-M has significantly a much larger number of
classes, with plentiful hierarchical relations between them.
4.2 Comparison Models
We compared various state-of-the-art baseline models. Those mod-
els can be divided into three groups : 1) Triple-based instance-
view KG embedding models : TransE[ 5] and RotatE[ 32], which
utilize translation-based representation learning, DistMult[ 39],
where semantic matching based score function is defined, and
CompoundE[ 11], where transition, rotation, and scaling operations
are combined to provide a wide range of designs. 2) Hierarchy-
aware instance-view KG embedding models : HyperKG[ 16], which
exploits hyperbolic space to model hierarchical structures in KG,
HAKE[ 41], which considers semantic hierarchies between entities,
MuRP[ 3], which models instances in hyperbolic space with negative
curvature, ConE[ 2], which utilizes hyperbolic entailment cone to
embed hierarchical relations, and GIE[ 6], which combines euclidean,
hyperbolic, and spherical embeddings. 3) Two-view KG embedding
models : JOIE[ 12], which is the first model for embedding two-view
KGs, DGS[ 14], where classes and instances are embedded on hy-
perbolic and spherical space, respectively, and Concept2Box[ 13],
3936Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 2: Link prediction results on ontology-view KG
Datasets YAGO26K-906 DB111K-174 KACC-M
Category Metrics MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10
TransE 0.189 14.72 24.36 0.539 47.90 61.84 0.147 10.42 23.85
DistMult 0.156 14.32 16.54 0.501 45.52 64.73 0.155 11.01 24.69
RotatE 0.228 16.35 27.28 0.557 49.16 68.19 0.172 11.53 26.37
HyperKG 0.174 14.50 23.26 0.542 47.59 62.11 0.145 10.47 23.53
Instance-view KG MuRP 0.289 18.86 40.34 0.622 54.71 80.28 0.213 13.64 36.80
Embedding Model HAKE 0.301 19.27 41.09 0.638 55.69 81.07 0.221 12.68 41.35
ConE 0.313 20.05 41.80 0.639 55.89 81.45 0.237 14.91 44.08
GIE 0.321 20.79 42.91 0.645 56.33 82.03 0.244 15.10 45.32
CompoundE 0.326 21.37 43.42 0.652 56.91 82.26 0.253 15.08 45.61
JOIE 0.289 18.66 39.13 0.602 52.48 79.71 0.199 12.52 35.74
Two-view KG DGS 0.372 25.88 44.38 0.690 59.88 84.82 0.255 15.73 46.16
Embedding Model Concept2Box - - - 0.650 56.82 83.01 - - -
CISS(Ours) 0.394 28.31 46.77 0.709 61.45 85.73 0.286 18.85 52.50
Table 3: Link prediction results on instance-view KG
Datasets YAGO26K-906 DB111K-174 KACC-M
Category Metrics MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10
TransE 0.187 13.73 35.05 0.318 22.70 48.12 0.303 24.55 43.89
DistMult 0.288 24.06 31.24 0.280 27.24 29.70 0.323 26.17 45.46
RotatE 0.302 25.31 42.17 0.321 27.39 46.63 0.329 26.82 45.72
HyperKG 0.215 18.35 36.02 0.302 23.31 46.72 0.312 25.38 43.90
Instance-view KG MuRP 0.281 21.97 39.44 0.376 28.75 58.79 0.401 33.95 54.82
Embedding Model HAKE 0.293 23.04 40.19 0.391 31.10 60.46 0.434 36.51 57.60
ConE 0.299 23.56 41.23 0.422 33.69 68.12 0.473 40.33 62.27
GIE 0.315 24.91 49.60 0.461 35.58 71.29 0.518 43.76 67.03
CompoundE 0.323 25.47 50.84 0.478 36.12 72.45 0.513 42.69 66.11
JOIE 0.316 24.62 51.85 0.479 35.21 72.38 0.510 43.11 66.79
Two-view KG DGS 0.366 30.15 56.06 0.536 38.31 74.85 0.534 45.82 69.18
Embedding Model Concept2Box - - - 0.510 36.52 73.11 - - -
CISS(Ours) 0.375 31.09 57.32 0.544 39.59 75.67 0.541 46.13 69.73
which employs box embeddings to learn fine-grained class rep-
resentation considering class entailment relationships. However,
since our model targets embedding KGs, we excluded ontology
embedding models in Section 2.2 from our comparison models.
4.3 Experimental Setups
4.3.1 Task Summary. We present an outline of our two experi-
mental tasks, link prediction and entity typing. To begin with, the
objective of link prediction is to predict missing entity given triple
(ℎ,𝑟,?). For each case, we corrupted the tail or head entity by rank-
ing whole candidate triplets utilizing the same score function as
Eq. (2). To evaluate the performance of our model, we followed
the same dataset split principles with [ 12,14]. After training the
entire two-view KGs, we tested entity prediction for GIandGO,
separately. Especially for instance-view KG embedding compari-
son models, we regarded the union of each KG of two-views as
a single instance-view KG as same as [ 12]. Remaining entity typ-
ing task aims to predict corresponding class 𝑒𝑐∈ECgiven triple
(𝑒𝑖,𝑡𝑦𝑝𝑒𝑂𝑓, ?)where𝑒𝑖∈EI. For each case, we ranked wholeclass candidates by calculating the distance between embeddings
of a mapped instance and a class. In this task, for both training and
testing phases, entire triples in GIandGOare leveraged to predict
triples inS.
4.3.2 Metrics. We reported our performance result using two met-
rics : Mean Reciprocal Rank (MRR ) and Hit Rate (HR@K ) where K is
chosen for 1 and 10. Furthermore, We used a filtered setting[12] for
evaluation as in other previous works, where triplets are excluded
during the test phase if seen in the training phase.
4.4 Overall Results
4.4.1 Link Prediction. Table 2 and Table 3 show the link prediction
performance for each KG of two-views, along with various existing
state-of-the-art baseline models. Note that we did not mention
entire results from Concept2Box[ 13] since its code is unavailable.
CISS achieves the best performance for all datasets compared to
baseline models.
3937KDD ’24, August 25–29, 2024, Barcelona, Spain. Kyuhwan Yeom, et al.
Table 4: Entity typing results on cross-links
Datasets YAGO26K-906 DB111K-174 KACC-M
Metrics MRR H@1 MRR H@1 MRR H@1
TransE 0.144 7.32 0.503 43.67 0.241 13.29
DistMult 0.411 36.07 0.551 49.83 0.253 14.11
RotatE 0.673 58.24 0.721 61.48 0.317 18.95
HyperKG 0.627 55.39 0.736 61.20 0.311 18.27
MuRP 0.868 81.23 0.837 74.19 0.382 25.88
HAKE 0.905 87.42 0.866 76.04 0.398 27.36
ConE 0.912 87.51 0.869 76.85 0.428 29.15
GIE 0.922 88.07 0.882 77.53 0.457 32.83
CompoundE 0.923 88.30 0.893 77.86 0.461 32.65
JOIE 0.899 85.72 0.859 75.58 0.479 34.17
DGS 0.939 89.07 0.895 77.94 0.495 36.22
Concept2Box - - 0.870 78.09 - -
CISS(Ours) 0.944 89.75 0.918 80.52 0.537 41.63
Table 5: Ablation study results on KACC-M (MRR)
GOGI Entity
Task link prediction link prediction Typing
w/o Inter-class Set 0.267 0.534 0.525
w/o Intra-class Set 0.269 0.532 0.528
w/o Class Set 0.255 0.528 0.517
w/o Struct-Sim 0.274 0.519 0.506
CISS 0.286 0.541 0.537
In Table 2, CISS shows 5.6%, 2.7%, and 10.9% of performance
improvement compared to the previous state-of-the-art model in
terms of MRR in YAGO26K-906, DB111K-174, and KACC-M, re-
spectively for ontology-view completion. Meanwhile, in Table 3,
CISS shows 2.3%, 1.5%, and 1.3% of performance improvement com-
pared to the previous state-of-the-art model in terms of MRR for
instance-view completion.
The experimental results show that the performance of two-view
KG embedding models is higher than that of instance-view KG em-
bedding models in common. This is because of joint representation
learning utilizing cross-links through cross-view modeling between
each KG of two-views. On the other hand, hierarchy-aware KG em-
bedding models such as MuRP and HAKE outperform existing
triple-based KG embedding models on ontology-view KG. Through
embedding on geometric space that can capture hierarchical struc-
tures in KG, they can readily model class hierarchies. However,
CISS shows a significant performance improvement for ontology-
view completion compared to instance-view KG. This is a result
of fine-grained class representations through class set modeling in
aware of class inheritance. Those representations have an impact
on learning instance representation through cross-view associa-
tion, with followed performance improvement on instance-view
completion.
4.4.2 Entity Typing. Table 4 shows the entity typing performance
ofCISS. CISS shows 0.5%, 2.5%, and 8.8% of performance improve-
ment comparing to previous state-of-the-art model in terms of
MRR in YAGO26K-906, DB111K-174, and KACC-M, respectively.
(a) Ontology-View KG. Relations except "isA" are omitted for
convenience.
(b) (Player, Athlete)
 (c) (Player, Artist)
(d) (Player, District)
 (e) (Athlete, District)
Figure 3: Case study on ontology-view KG from YAGO26K-
906. (b), (c), (d), and (e) show phase embedding differences
between two classes.
Structural similarity modeling proposed by CISS allows instances
associated with same class to be well-clustered, owing to the re-
flection of each instance and class representation. Performance on
KACC-M shows the best enhancement among three datasets, due
to the complex structure of ontology-view KG to obtain meaningful
subgraph representations.
4.5 Ablation Study
To evaluate our effective model design in parts, we conducted an
ablation study on KACC-M. We ablated class set and structural sim-
ilarity modeling parts that we considered as our main contribution.
Table 5 shows performance results in terms of MRR on KACC-M.
ForGOlink prediction, the impact of class set modeling on per-
formance degradation is larger than that of structural similarity
modeling. This indicates that the class granularity improvement is
achieved by considering the inheritance relationship between ontol-
ogy classes. However, for GIlink prediction, structural similarity
3938Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity KDD ’24, August 25–29, 2024, Barcelona, Spain.
modeling seems to get more importance than the other. It can be
inferred that entity typing as well as instance-view link prediction
is heavily influenced by how well instances are clustered together.
4.6 Case Study
We conducted a case study with YAGO26K-906 to show the efficacy
of proposed class set modeling. Figure 3 consists of ontology-view
KG from YAGO26K-906 where classes are connected through "isA"
relation and four embedding histograms in regard to phase em-
bedding differences between two classes. Assuming embedding
dimension as 200, we normalized the y-axis with respect to fre-
quency of phase embeddings.To interpret results arising from class
set modeling, we compared three cases : 1) two classes that share
both parent and a child, 2) two classes that share a parent, but
not a child, 3) two classes that are not inherit from a same parent;
two classes are not siblings. Figure 3-(a) shows phase embedding
difference between "Player " and "Athlete ", where they share parent
"Person "and a child "Hockey Player ". By intra class set modeling,
result is shown with minimal difference between two phase embed-
dings. Figure 3-(b) shows phase difference between "Player " and
"Artist ", where they both are inherited from "Person ", but have a
separate child, "Hockey Player " and "Film Director ", respectively.
By inter class set modeling with a smaller margin, the result is
shown with a larger marginal difference than that of the former
case. Lastly, Figure 3-(c) and (d) show the phase difference between
["Player ", "Athlete "] and "District ", where they do not represent any
class inheritance with their direct parent or child. By inter class set
modeling with a larger margin, the result is shown with a larger
marginal difference than that of any other case.
5 CONCLUSION AND FUTURE WORKS
In this paper, we proposed a novel two-view KG embedding model,
CISS with utilizing class inheritance in ontology-view KG and
structural similarity between two-view KGs. We firstly embedded
each KG of two-views in aware of hierarchy for an initialization,
followed by class set modeling on ontology-view KG. For cross-view
association, we intended to make representations of two-view KGs
similar by utilizing virtual KG with clustered instances, in addition
to generating subgraph representations. On three public datasets,
CISS shows outstripping results on link prediction and entity typing
tasks, especially for ontology-view KG owing to bespoke class set
modeling.
For future works, we will take a closer look at relations with
hierarchical and transitive properties in two-view KGs. Hierarchical
relations like "isA", and "hasPartOf " may reveal transitive proper-
ties. However, non-hierarchical relation like "similarTo " also im-
ply transitivity. Therefore, detailed relation embedding is required
depending on the degree of transitivity and hierarchicalness. Fur-
thermore, two-view KG embedding may be expanded to inductive
settings, where unseen entities and relations should be considered
during the evaluation stage.
REFERENCES
[1]Franz Baader, Sebastian Brandt, and Carsten Lutz. 2005. Pushing the EL envelope.
InProceedings of the 19th International Joint Conference on Artificial Intelligence
(Edinburgh, Scotland) (IJCAI’05). Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA, 364–369.[2]Yushi Bai, Zhitao Ying, Hongyu Ren, and Jure Leskovec. 2021. Modeling hetero-
geneous hierarchies with relation-specific hyperbolic cones. Advances in Neural
Information Processing Systems 34 (2021), 12316–12327.
[3]Ivana Balažević, Carl Allen, and Timothy Hospedales. 2019. Multi-relational
poincaré graph embeddings. Curran Associates Inc., Red Hook, NY, USA.
[4]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference
on Management of Data (Vancouver, Canada) (SIGMOD ’08). Association for
Computing Machinery, New York, NY, USA, 1247–1250. https://doi.org/10.1145/
1376616.1376746
[5]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. Advances in neural information processing systems 26 (2013).
[6]Zongsheng Cao, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming
Huang. 2022. Geometry interaction knowledge graph embeddings. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 36. 5521–5529.
[7]Victorio A Carvalho, João Paulo A Almeida, Claudenir M Fonseca, and Gian-
carlo Guizzardi. 2017. Multi-level ontology-based conceptual modeling. Data &
Knowledge Engineering 109 (2017), 3–24.
[8]Jiaoyan Chen, Pan Hu, Ernesto Jimenez-Ruiz, Ole Magnus Holter, Denvar
Antonyrajah, and Ian Horrocks. 2021. Owl2vec*: Embedding of owl ontologies.
Machine Learning 110, 7 (2021), 1813–1845.
[9]Zijun Cui, Pavan Kapanipathi, Kartik Talamadupula, Tian Gao, and Qiang Ji. 2021.
Type-augmented relation prediction in knowledge graphs. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 35. 7151–7159.
[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019. Arcface:
Additive angular margin loss for deep face recognition. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 4690–4699.
[11] Xiou Ge, Yun Cheng Wang, Bin Wang, and C-C Jay Kuo. 2023. Compounding
geometric operations for knowledge graph completion. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 6947–6965.
[12] Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, and Wei Wang. 2019.
Universal representation learning of knowledge bases by jointly embedding
instances and ontological concepts. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 1709–1719.
[13] Zijie Huang, Daheng Wang, Binxuan Huang, Chenwei Zhang, Jingbo Shang,
Yan Liang, Zhengyang Wang, Xian Li, Christos Faloutsos, Yizhou Sun, and
Wei Wang. 2023. Concept2Box: Joint Geometric Embeddings for Learning
Two-View Knowledge Graphs. In Findings of the Association for Computational
Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki
(Eds.). Association for Computational Linguistics, Toronto, Canada, 10105–10118.
https://doi.org/10.18653/v1/2023.findings-acl.642
[14] Roshni G Iyer, Yunsheng Bai, Wei Wang, and Yizhou Sun. 2022. Dual-geometric
space embedding model for two-view knowledge graphs. In Proceedings of the
28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 676–686.
[15] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980
[16] Prodromos Kolyvakis, Alexandros Kalousis, and Dimitris Kiritsis. 2019. Hyperkg:
Hyperbolic knowledge graph embeddings for knowledge base completion. arXiv
preprint arXiv:1908.04895 (2019).
[17] Maxat Kulmanov, Wang Liu-Wei, Yuan Yan, and Robert Hoehndorf. 2019. EL
Embeddings: Geometric Construction of Models for the Description Logic EL++.
InProceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence
Organization, 6103–6109. https://doi.org/10.24963/ijcai.2019/845
[18] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas,
Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören
Auer, et al .2015. Dbpedia–a large-scale, multilingual knowledge base extracted
from wikipedia. Semantic web 6, 2 (2015), 167–195.
[19] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
entity and relation embeddings for knowledge graph completion. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 29.
[20] Shiheng Ma, Jianhui Ding, Weijia Jia, Kun Wang, and Minyi Guo. 2017. Transt:
Type-based multiple embedding representations for knowledge graph completion.
InMachine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2017, Skopje, Macedonia, September 18–22, 2017, Proceedings, Part I
10. Springer, 717–733.
[21] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. 2013. Yago3: A
knowledge base from multilingual wikipedias. In CIDR.
[22] Deborah L McGuinness, Frank Van Harmelen, et al .2004. OWL web ontology
language overview. W3C recommendation 10, 10 (2004), 2004.
[23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013).
3939KDD ’24, August 25–29, 2024, Barcelona, Spain. Kyuhwan Yeom, et al.
[24] Guanglin Niu, Bo Li, Yongfei Zhang, and Shiliang Pu. 2022. CAKE: A Scalable
Commonsense-Aware Framework For Multi-View Knowledge Graph Completion.
InProceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , Smaranda Muresan, Preslav Nakov, and Aline
Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland,
2867–2877. https://doi.org/10.18653/v1/2022.acl-long.205
[25] Xi Peng, Zhenwei Tang, Maxat Kulmanov, Kexin Niu, and Robert Hoehndorf.
2022. Description logic el++ embeddings with intersectional closure. arXiv
preprint arXiv:2202.14018 (2022).
[26] Davy Preuveneers and Yolande Berbers. 2012. 𝜇C-SemPS: Energy-Efficient Se-
mantic Publish/Subscribe for Battery-Powered Systems. In Mobile and Ubiquitous
Systems: Computing, Networking, and Services: 7th International ICST Conference,
MobiQuitous 2010, Sydeny, Australia, December 6-9, 2010, Revised Selected Papers
7. Springer, 39–50.
[27] Yunqi Qiu, Kun Zhang, Yuanzhuo Wang, Xiaolong Jin, Long Bai, Saiping Guan,
and Xueqi Cheng. 2020. Hierarchical query graph generation for complex ques-
tion answering over knowledge graph. In Proceedings of the 29th ACM interna-
tional conference on information & knowledge management. 1285–1294.
[28] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolu-
tional networks. In The Semantic Web: 15th International Conference, ESWC 2018,
Heraklion, Crete, Greece, June 3–7, 2018, Proceedings 15. Springer, 593–607.
[29] Manfred Schmidt-Schauß and Gert Smolka. 1991. Attributive concept descriptions
with complements. Artificial intelligence 48, 1 (1991), 1–26.
[30] Vivian Silva, André Freitas, and Siegfried Handschuh. 2018. Building a Knowl-
edge Graph from Natural Language Definitions for Interpretable Text Entail-
ment Recognition. In Proceedings of the Eleventh International Conference on Lan-
guage Resources and Evaluation (LREC 2018), Nicoletta Calzolari, Khalid Choukri,
Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara,
Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk,
Stelios Piperidis, and Takenobu Tokunaga (Eds.). European Language Resources
Association (ELRA), Miyazaki, Japan. https://aclanthology.org/L18-1542
[31] Fatima Zohra Smaili, Xin Gao, and Robert Hoehndorf. 2019. OPA2Vec: combining
formal and informal content of biomedical ontologies to improve similarity-based
prediction. Bioinformatics 35, 12 (2019), 2133–2140.
[32] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
HkgEQnRqYQ
[33] Gyt˙e Tamašauskait ˙e and Paul Groth. 2023. Defining a knowledge graph devel-
opment process through a systematic review. ACM Transactions on Software
Engineering and Methodology 32, 1 (2023), 1–40.
[34] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In International
conference on machine learning. PMLR, 2071–2080.
[35] Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative
knowledgebase. Commun. ACM 57, 10 (2014), 78–85.
[36] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie,
and Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge
graph for recommender systems. In Proceedings of the 27th ACM international
conference on information and knowledge management. 417–426.
[37] Jie Wang, Zhanqiu Zhang, Zhihao Shi, Jianyu Cai, Shuiwang Ji, and Feng Wu.
2022. Duality-induced regularizer for semantic matching knowledge graph
embeddings. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 2
(2022), 1652–1667.
[38] Bo Xiong, Nico Potyka, Trung-Kien Tran, Mojtaba Nayyeri, and Steffen Staab.
2022. Faithful embeddings for EL++ knowledge bases. In International Semantic
Web Conference. Springer, 22–38.
[39] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-
bedding Entities and Relations for Learning and Inference in Knowledge Bases.
In3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann
LeCun (Eds.). http://arxiv.org/abs/1412.6575
[40] Kuo Yang, Zhengyang Zhou, Wei Sun, Pengkun Wang, Xu Wang, and Yang Wang.
2023. Extract and refine: Finding a support subgraph set for graph representation.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2953–2964.
[41] Zhanqiu Zhang, Jianyu Cai, Yongdong Zhang, and Jie Wang. 2020. Learning
hierarchy-aware knowledge graph embeddings for link prediction. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 34. 3065–3072.
[42] Zhao Zhang, Fuzhen Zhuang, Meng Qu, Fen Lin, and Qing He. 2018. Knowledge
graph embedding with hierarchical relation structure. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing. 3198–3207.
[43] Jie Zhou, Shengding Hu, Xin Lv, Cheng Yang, Zhiyuan Liu, Wei Xu, Jie Jiang,
Juanzi Li, and Maosong Sun. 2021. KACC: A Multi-task Benchmark for Knowledge
Abstraction, Concretization and Completion. In Findings of the Association for
Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, WenjieLi, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online,
1751–1763. https://doi.org/10.18653/v1/2021.findings-acl.153
[44] Xiangrong Zhu, Guangyao Li, and Wei Hu. 2023. Heterogeneous federated
knowledge graph embedding learning and unlearning. In Proceedings of the ACM
Web Conference 2023. 2444–2454.
A IMPLEMENTATION DETAILS
A.1 Optimzer and Hyperparameter Settings
For model training, the Adam optimizer[ 15] was used with a learn-
ing rate of 0.001. In addition, hyperparameters were determined
empirically as follows. The embedding dimension of classes and
instances was selected from 𝑑∈[50,100,200,300]. We equalized
dimensions of entities and relations for computability of cosine
similarity (Eq. (17)). For losses in Eq. (19) and Eq. (22), controlling
weights are selected from 𝛽,𝜂1,𝜂2∈[0,0.25,0.5,0.75,1.0]. In sec-
tion 3.3, the scale factor was searched from 𝛼∈[16,32,64]. Each
of angular margins was searched from 𝑚1∈[0.6,0.8,1.0],𝑚2∈
[2,3,4],𝑚3∈[6,8,10], respectively. For negative triples sampling,
we set the number of negative triples from [128, 256, 512]. However,
we fixed some hyperparameters for convenience. In section 3.4.1,
we fixed the marginal value 𝛾𝐻𝐴in the hinge loss as 1. In section
3.4.2, we fixed the number of hops 𝑘to extract enclosing subgraphs
as 2, and the number of GNN layers 𝐿as well.
A.2 Further Details on Class Set Modeling
In terms of real-world dataset that contains ontological information,
it is not clear exactly which classes are at the same level of hierarchy.
Likewise, the semantic distance of each hyponym relation differs.
Therefore, sibling classes cannot be clearly leveled within datasets,
so that we refer to classes that simply share the same parent as
siblings. In order to classify classes that are in the same level of
hierarchies, we initialize an approximate level of each class. A class
level is defined within classes that are linked to other classes with
hyponym relations. For classes that do not contain outgoing edges,
they correspond with top-level classes. For each top-level class,
we perform a depth-first-search (DFS) process towards leaf classes,
resulting in a path to the bottom-level class. Afterward, we compare
the entire path and define the length of the path from the top-level
class to the farthest class as the level of that class. With the help of
other relationships between classes, those classes are re-arranged
through HAKE.
B TIME COMPLEXITY ANALYSIS
We denote𝑑as embedding dimension, 𝐿as number of GNN layers,
𝑅as number of relations in two-view KGs, 𝑇𝑂,𝑇𝐼as the number
of triples with respect to GO,GI, andSas number of cross-links
between them. Finally, 𝑁𝑒,𝑁𝑐stand for the number of instances
and classes, respectively.
For hierarchy-aware embedding initialization, its time complex-
ity is computed as 𝑂((𝑇𝐼+𝑇𝑂)(𝑛+𝑑))where𝑛is the number of
negative samples for each triple. To be specific, it takes 𝑂(𝑛+𝑑)
for self-adversarial negative sampling, and 𝑂(𝑑)for calculating a
score function for each triple. However, the number of class sets
is atmost𝑁𝑐. With𝑑vector multiplication, it takes 𝑂(𝑑𝑁𝑐)time
for intra class set modeling. Inter class set modeling deals with
multiple class sets, which implies 𝑂(𝑑𝑁𝑐2)time.
3940Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity KDD ’24, August 25–29, 2024, Barcelona, Spain.
(a) KACC-M
 (b) DB111K-174
Figure 4: Entity typing results (MRR) on two datasets with
respect to different embedding dimensionalities.
(a) Ontology-view KG
 (b) Instance-view KG
Figure 5: Link prediction results (MRR) on each KG of two-
views with respect to value of hyperparameters 𝜂1,𝜂2on
KACC-M.
For cross-view instance mapping, it takes 𝑂(|S|𝑑2)time when
considering matrix multiplication. For structural similarity model-
ing, we derive a virtual graph and retrieve an enclosing subgraph
for each triple during the preprocessing stage, so we exclude their
computational cost from our model time complexity. When im-
plementing RGCN, for each triple, the message function is called𝐿(e𝑁+e𝑇) times, where e𝑁,e𝑇are number of nodes and edges in its en-
closing subgraph, respectively. Each message function costs 𝑂(𝑑2)
for a single call. In addition, the aggregating function is called 𝑁𝑐𝐿
times, with 𝑂(𝑑2)time complexity for a single call as well. There-
fore, the overall time complexity for structural similarity modeling
is summed as 𝑂(e𝑇𝐿(e𝑇+e𝑁)𝑑2).
With a virtual KG, we can reduce the number of entities in an
instance-view KG in equal to an ontology-view KG. As seen in
Table 1, the number of classes and triples in ontology-view KG is
much lower than the number of instances and triples in its counter-
part. Since a class is linked with multiple instances, it stands true.
Therefore, utilizing a virtual KG ensures structural similarity be-
tween two-view KGs as well as reduces computational complexity
due to its compact size.
C EMBEDDING DIMENSION ANALYSIS
Figure 4 shows the MRRs of entity typing results of three two-view
KG embedding models with respect to embedding dimensions on
KACC-M and DB111K-174 datasets. As seen in the results, the best
performance is shown when 𝑑=200.CISS outperforms existing
models in every case except 𝑑=50for DB111K-174. For too low
dimension, a model cannot learn adequate representations from
KG structure. However, for too high dimension, model complexity
increases so that the probability of overfitting heightens, which
degrades its performance.
D HYPERPARAMETER SENSITIVITY ON
FINAL LOSS
We conducted a hyperparameter analysis to measure the extent of
the contribution of different components to the final loss (Eq. (22)).
Figure 5 shows the MRRs of link prediction results of ontology-view
KG and instance-view KG concerning a value of 𝜂1,𝜂2on a KACC-
M dataset. A value of 𝛽in Eq. (19) is fixed at zero. As the weight
for class set modeling 𝜂1increases, overall performance steadily
heightens, but with small improvements in terms of instance-view
KG link prediction. As controlling weight for cross-view associa-
tion𝜂2increases, the performance on instance-view KG increases,
without any performance trends in an ontology-view KG.
3941