Self-Distilled Disentangled Learning for Counterfactual
Prediction
Xinshu Li
xinshu.li@unsw.edu.au
The University of New South Wales
Sydney, AustraliaMingming Gong
mingming.gong@unimelb.edu.au
The University of Melbourne
Melbourne, Australia
MBZUAI
Abu Dhabi, United Arab EmiratesLina Yao
lina.yao@data61.csiro.au
CSIRO‚Äôs Data 61
The University of New South Wales
Sydney, Australia
ABSTRACT
The advancements in disentangled representation learning signifi-
cantly enhance the accuracy of counterfactual predictions by grant-
ing precise control over instrumental variables, confounders, and
adjustable variables. An appealing method for achieving the in-
dependent separation of these factors is mutual information min-
imization, a task that presents challenges in numerous machine
learning scenarios, especially within high-dimensional spaces. To
circumvent this challenge, we propose the Self-Distilled Disentan-
glement framework, referred to as ùëÜùê∑2. Grounded in information
theory, it ensures theoretically sound independent disentangled
representations without intricate mutual information estimator de-
signs for high-dimensional representations. Our comprehensive
experiments, conducted on both synthetic and real-world datasets,
confirms the effectiveness of our approach in facilitating counter-
factual inference in the presence of both observed and unobserved
confounders.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíLearning latent representa-
tions.
KEYWORDS
Counterfactual Prediction, Disentangled Representation Learning,
Information Theory
ACM Reference Format:
Xinshu Li, Mingming Gong, and Lina Yao. 2024. Self-Distilled Disentangled
Learning for Counterfactual Prediction. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24),
August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671782
1 INTRODUCTION
Counterfactual prediction has attracted increasing attention [ 1,
10,12,48] in recent years due to the rising demands for robust
and trustworthy artificial intelligence. Confounders, the common
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671782causes of treatments and effects, induce spurious relations between
different variables, consequently undermining the distillation of
causal relations from associations. Thanks to the rapid development
of representation learning, a plethora of methods [22, 33, 44] miti-
gate the bias caused by the observed confounders via generating
balanced representations in the latent space. As for the bias brought
by the unobserved confounders, Angrist and Imbens [3], Hartford
et al. [14] , Lin et al . [24] , Muandet et al . [27] , Pearl et al . [28] pro-
pose obtaining an unbiased estimator by regressing outcomes on
Instrumental variables (IVs), which are exogenous variables related
to treatment and only affect outcomes indirectly via treatment, to
break the information flow between unobserved confounders and
the treatments.
We will further clarify by integrating Figure 1 with a real-world
example. Hoxby [17] examined whether competition among public
schools (Treatment T) improves educational quality within districts
(Outcome Y). The potential endogeneity of the number of schools
in a region arises because both the treatment and the outcome
could be influenced by long-term factors (Confounders) specific
to the area. Some of these confounders (observed Confounders C)
can be measured, such as the level of local economic development.
However, other confounders (Unobserved Confounders U) cannot
be easily quantified or exhaustively listed, such as certain historical
factors. River counts here can serve as a persuasive IV: 1) more
rivers can lead to more schools due to transportation (Relevance);
2) yet they are unrelated to teaching quality directly (Exclusivity);
3) the natural attributes of river formation render the counts of
the rivers unrelated to the confounders caused by historical factors
(Exogeneity).
There are two drawbacks to the existing methods. Firstly, most
of them treat all observed features as the observed confounders to
block, while only part contributes to the distribution discrepancy
of pre-treatment features. Secondly, the prerequisite for IV-based
regression methods is to access valid IVs, which have three strict
conditions (Relevance, Exclusion, Exogeneity) to satisfy, as shown
above, making it a thorny task to find.
Disentangled representation learning [ 8,15,41,45], aiming at
decomposing the representations of different underlying factors
from the observed features, is showing promise in addressing the
flaws above simultaneously. The disentangled factors provide us
with a more precise inference route to alleviate the bias brought
by the observed confounders. Additionally, one can automatically
obtain the representations of the valid IVs to remove the bias led
by the unobserved confounders.
The seminar work by Hassanpour and Greiner [15] introduces
disentangled learning to the domain of counterfactual prediction,
 
1667
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
creatively categorizing observed features into instrumental vari-
ables (IVs), confounders, and adjustable variables. However, it does
not guarantee the generation of mutually independent representa-
tions of these underlying factors. Subsequently, Cheng et al . [8], Wu
et al. [41] , Yuan et al . [45] integrate effective mutual information
estimators [ 9] to minimize the mutual information between latent
factor representations, aiming to achieve mutually independent la-
tent representations. Nonetheless, the accurate estimation of mutual
information between high-dimensional representations remains a
persistent challenge [ 5,29]. Inaccurate estimation of mutual infor-
mation may result in the failure to obtain mutually independent
representations of latent factors, thereby compromising the accu-
racy of counterfactual estimation.
To overcome the defects of previous methods, we propose a
novel algorithm for counterfactual prediction named Self-Distilled
Disentanglement ( ùëÜùê∑2) to sidestep MI estimation between high-
dimensional representations. Specifically, we provide theoretical
analysis rooted in information theory to guarantee the mutual inde-
pendence of diverse underlying factors. Further, we give a solvable
form of our method through rigorous mathematical derivation to
minimize MI directly rather than explicitly estimating it. Based
on the theory put forward, we design a hierarchical distillation
framework to kill three birds with one stone: disentangle three
independent underlying factors, mitigate the confounding bias, and
grasp sufficient supervision information for counterfactual predic-
tion.
Our main contributions are summarized as follows:
‚Ä¢We put forward a theoretically assured solution rooted in
information theory for disentanglement to avoid mutual
information estimation between high-dimensional represen-
tations. It enables the generation of mutually independent
representations, thereby adding in the decomposition of di-
verse underlying factors.
‚Ä¢We provide a tractable form of proposed solution through
mathematically rigorous derivation, which rewrites the loss
function of disentanglement and fundamentally tackles the
difficulty of mutual information estimation between high-
dimensional representations.
‚Ä¢We propose a novel self-distilled disentanglement method
(ùëÜùê∑2) for counterfactual prediction. By designing a hierar-
chical distillation framework, we disentangle IVs and con-
founders from observational data to mitigate the bias induced
by the observed and unobserved confounders at the same
time.
‚Ä¢We conduct extensive experiments on synthetic and real-
world benchmarks to verify our theoretically grounded strate-
gies. The results demonstrate the effectiveness of our frame-
work on counterfactual prediction compared with the state-
of-the-art baselines.
2 RELATED WORK
2.1 Counterfactual Prediction
The main challenge for counterfactual prediction is the existence
of confounders. Researchers adopt matching [ 32], re-weighting
[18], regression [ 11], representation learning [ 33,49] to alleviate
X
C Z A
T Y
UObserved Variables:
Z
C
ATX
UY
Decomposed hidden factors:
Unobserved Variables:Pre-treatment features
Treatment
Outcome
Instrumental variables
Confounder variables
Adjustment variables
Unobserved confoundersFigure 1: General causal structure. The underlying con-
foundersùê∂in observed pre-treatment features ùëãand un-
observed confounders ùëàresult in spurious relations rather
than causal relations between treatment ùëáand outcome ùëå.
We aim to disentangle mutually independent representations
ofùëç,ùê∂, andùê¥fromùëãwithout the design of intrigue mutual
information estimators.
the confounding bias under the ‚Äúno hidden confounders" assump-
tion. To relax this unpractical assumption, a few non-parametric
or semi-parametric methods utilize special structures among the
variables to resolve the bias led by the unobserved confounders.
These structures include (1) proxy variables [ 25,35,36,42]; (2)
multiple causes [ 7,37,46]; (3) instrumental variables. 2SLS [ 3] is
the classical IV method in a linear setting. Hartford et al . [14] , Lin
et al. [24] , Singh et al . [34] , Wu et al . [39] , Xu et al . [43] adopt ad-
vanced machine learning or deep learning algorithms for non-linear
scenarios. Another commonly used causal effects estimator using
IVs is the control function estimator (CFN) [ 30,38]. These meth-
ods require well-predefined IVs or assume all observed features
as confounders, which inevitably impairs their generalization to
real-world practice. Li and Yao [23], Wu et al . [40] , Yuan et al . [45]
aims to generate IVs for downstream IV-based methods rather than
focusing on counterfactual prediction.
2.2 Disentangled Representation Learning
Most of the current state-of-the-art disentangled representation
learning methods are based on Kingma and Welling [21], which uses
a variational approximation posterior for the inference process of
the latent variables. Typical work includes ùõΩ-VAE [ 16], Factor-VAE
[19], CEVAE [ 25] and so on. Another popular solution [ 6,50] builds
on the basis of Generative Adversarial Networks [ 13]. However,
these studies are more suitable for the approximate data generation
problem and fall short when it comes to estimating causal effects,
largely due to the difficulty in training complex generation models.
As the disentanglement of the underlying factors in observed
features helps to alleviate confounding bias and improve inference
accuracy, Hassanpour and Greiner [15], Wu et al . [41] , Zhang et al .
[47] design different decomposition regularizers to help the sepa-
ration of underlying factors. However, these methods fall short in
obtaining disentangled representations that are mutually indepen-
dent, which is a prerequisite for identifying treatment effects. Cheng
et al. [8], Yuan et al . [45] employ a cutting-edge mutual information
 
1668Self-Distilled Disentangled Learning for Counterfactual Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
estimator [ 9] to reduce the mutual information between disentan-
gled representations, thereby promoting independence among the
representations of underlying factors. Despite this, the intricacy of
mutual information estimation poses a challenge to the accurate
separation of latent factors, thereby leaving room for enhancement
in counterfactual prediction.
Compared with previous IV-based counterfactual prediction
methods,ùëÜùê∑2abandons well-predefined IVs but rather decomposes
them from pre-treatment variables. Besides, we only impose con-
ditions on variables that are related to confounding bias. Hence,
our approach effectively mitigates both unobserved and observed
confounding biases concurrently, a feat rarely achieved by previous
IV-based counterfactual prediction methods. Compared with gener-
ative disentangled methods, rather than generating latent variables,
ùëÜùê∑2directly disentangles the observed features into three under-
lying factors by introducing causal mechanisms, which is more
efficient and effective. In contrast to preceding causal disentangle-
ment methods, our approach facilitates the generation of mutually
independent representations of underlying factors without the need
for intricate mutual information estimators. Instead, we fundamen-
tally minimize the mutual information between the representations
of underlying factors without explicit estimation, which will be
elaborated in Section 4.
3 PROBLEM SETUP
As shown in Figure 1, we have observed pre-treatment features ùëã,
treatmentùëá, and outcome ùëåin the datasetD.ùëãis composed of
three types of underlying factors:
‚Ä¢Instrumental variable ùëçthat only directly influences ùëá.
‚Ä¢Confounders ùê∂that directly influences both ùëáandùëå.
‚Ä¢Adjustable variable ùê¥that only directly influences ùëå.
Besides these, there are some Unobserved confounders ùëàthat
impede the counterfactual prediction. ùëç,ùê∂,ùê¥,ùëà in this paper are
exogenous variables.
According to the d-separation theory [ 28],ùëç,ùê∂andùê¥are mu-
tually independent due to the collider structures ùëç‚Üíùëá‚Üêùê∂and
ùê∂‚Üíùëå‚Üêùê¥. Thus, encouraging independence among the repre-
sentations of these underlying factors is essential for advancing
their separation. In this paper, we aim to first disentangle mutually
independent representations of ùëç,ùê∂, andùê¥fromùëã, bypassing the
explicit estimation of mutual information through theoretical anal-
ysis, then propose a unified framework to tackle confounding bias
caused byùê∂andùëàsimultaneously.
Given the above definitions, we present the formal definitions
of counterfactual prediction problem, followed by an essential as-
sumption for identification of causal effects adopted in this paper.
Definition 3.1. Counterfactual Prediction refers to predicting
what the outcome would have been for an individual under an
intervention ùë°[28], i.e.,
ùëî(ùë°,ùëã)=ùê∏[ùëå|ùëëùëú(ùëá=ùë°),ùëã], (1)
where dorefers to do-operator proposed by Pearl et al. [28].
Assuming successful separation of ùëç,ùê∂, andùê¥, the sufficient
identification results for causal effects under the additive noiseassumption in instrumental variable regression were developed by
Angrist et al. [4].
Assumption 3.2. Additive Noise Assumption: the noise from
unmeasured confounders ùëàis added to the outcomes ùëå, i.e.,
ùëå=ùëî(ùë°,ùëã)+ùëà. (2)
4 METHODOLOGY
4.1 Theoretical Foundation of Independentizing
Underlying Factors
To get the mutually independent representations of underlying
factors, an intuitive thought is to minimize the mutual informa-
tion between the representation of ùëç,ùê∂andùê¥, i.e.,ùêº(ùëÖùëé;ùëÖùëê)1and
ùêº(ùëÖùëß;ùëÖùëê), whereùêº(¬∑)represents the function of mutual information,
during training. However, it is hard due to the notorious difficulty
in estimating mutual information in high dimensions, especially
for latent embedding optimization [ 5,29]. To circumvent this chal-
lenge, we decompose the mutual information between ùëÖùëéandùëÖùëê
by leveraging the chain rule of mutual information:
ùêº(ùëÖùëé;ùëÖùëê)=ùêº(ùëå;ùëÖùëê)+ùêº(ùëÖùëé;ùëÖùëê|ùëå)‚àíùêº(ùëÖùëê;ùëå|ùëÖùëé). (3)
We further inspect the term ùêº(ùëÖùëé;ùëÖùëê|ùëå)in Eq (3). Based on the
definition of mutual information, we have,
ùêº(ùëÖùëé;ùëÖùëê|ùëå)=ùêª(ùëÖùëé|ùëå)‚àíùêª(ùëÖùëé|ùëå,ùëÖùëê), (4)
whereùêª(¬∑)represents the function of entropy. Intuitively, this con-
ditional mutual information measures the information contained in
ùëÖùëéthat is related to ùëÖùëêbut unrelated to ùëå. Empirically, if we set up
two prediction models from ùëÖùëéandùëÖùëêto Y, respectively, then the
mutual information between ùëÖùëéandùëÖùëêis all related to ùëåduring the
training phase. We draw the Venn diagram of mutual information
betweenùëÖùëé,ùëÖùëêandùëåas shown in Figure 2, according to which we
have,
ùêª(ùëÖùëé|ùëå)=ùêª(ùëÖùëé|ùëå,ùëÖùëê). (5)
With Eq (3), (4) and (5), during the training phase, we have,
ùêº(ùëÖùëé;ùëÖùëê)=ùêº(ùëå;ùëÖùëê)‚àíùêº(ùëÖùëê;ùëå|ùëÖùëé). (6)
Eq(6)transforms the mutual information between two training
high-dimensional representations into the subtraction of two mu-
tual information estimators with known labels, thereby decreasing
the difficulty of the computation. To further simplify it, we intro-
duce the following theory:
Theorem 4.1. Minimizing the mutual information between ùëÖùëé
andùëÖùëêis equivalent to:
minùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê)‚àíùêª(ùëå|ùëÖùëé)+ùêª(ùëå|ùëÖùëê,ùëÖùëé). (7)
To find a tractable solution to Eq (7), we derive the following
Corollary:
1ùëÖùëéandùëÖùëêdiffer fromùê¥andùê∂in Figure 1. Our optimization aims to bring ùëÖùëéand
ùëÖùëêto the states where ùê¥andùê∂can be represented. Initially, ùëÖùëéandùëÖùëêare entangled
with non-zero mutual information and thus do not follow the causal relations in Figure
1, as depicted in Figure 2. We introduce constraints to reduce their MI and achieve
independence.
 
1669KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
ùëØ(ùëπùíÑ|ùíÄ,ùëπùíÇ)
ùëØ(ùíÄ|ùëπùíÑ,ùëπùíÇ)ùë∞(ùëπùíÑ;ùëπùíÇ)
ùë∞(ùëπùíÑ;ùíÄ|ùëπùíÇ)ùëØ(ùëπùíÑ) ùëØ(ùëπùíÇ)
ùëØ(ùíÄ)ùëØ(ùëπùíÇ|ùíÄ,ùëπùíÑ)
ùë∞(ùëπùíÇ;ùíÄ|ùëπùíÑ)(ùëØùëπùíÑùíÄ) (ùëØùëπùíÇùíÄ)
Figure 2: A motivating Venn diagram of mutual information
betweenùëÖùëê,ùëÖùëéandùëåduring training phase.
Corollary 4.2. One of sufficient conditions of minimizing ùêº(ùëÖùëé;ùëÖùëê)
is minimizing the following conditions together:
minLùëé
ùëê=(
ùê∑ùêæùêø[Pùëå‚à•PùëÖùëé
ùëå] (8a)
ùê∑ùêæùêø[PùëÖùëé
ùëå‚à•PùëÖùëê
ùëå], (8b)
wherePùëÖùëé
ùëå=ùëù(ùëå|ùëÖùëé),PùëÖùëê
ùëå=ùëù(ùëå|ùëÖùëê)represent the predicted
distributions of ùëå,Pùëå=ùëù(ùëå)represents the real distribution of ùëå.
ùê∑ùêæùêødenotes the KL-divergence.
Detailed proof and formal assertions of Theo.4.1 andCorol.4.2
can be found in the Appendix A.1.1 and A.1.2. Similarly, we can
transform minimizing ùêº(ùëÖùëß;ùëÖùëê)into following:
minLùëß
ùëê=(
ùê∑ùêæùêø[Pùëá‚à•PùëÖùëß
ùëá] (9a)
ùê∑ùêæùêø[PùëÖùëß
ùëá‚à•PùëÖùëê
ùëá], (9b)
wherePùëÖùëé
ùëá=ùëù(ùëá|ùëÖùëé),PùëÖùëß
ùëá=ùëù(ùëá|ùëÖùëß)represent the predicted
distributions of ùëá,Pùëá=ùëù(ùëá)represents the real distribution of ùëá.
Remark: Technically, the optimization of Eq (8a),(8b)can be
realized by training prediction networks about ùëå. Additionally,
we can set up a deep prediction network from ùëá,ùê¥,ùê∂toùëåand
use its supervision information to guide the training of the shallow
prediction models. From this perspective, our approach is essentially
a self-distillation model. Therefore, we name our method as Self-
Distilled Disentanglement, i.e., ùëÜùê∑2.
4.2 Self-Distilled Disentanglement Framework
With the theory put forward in section 4.1, we propose a self-
distilled framework to disentangle different mutually independent
underlying factors. To clarify further, we take the distillation unit
for minimizingLùëßùëêto illustrate how we employ different sources of
supervision information to directly minimize mutual information
without explicitly estimating it.
As shown in Figure 3, Retain network represents the neural
network for retaining the information from both ùëçandùê∂.Deep
networks and Shallow networks are named based on their relative
proximity to ùëÖùëßandùëÖùëê. We set up two Shallow prediction networks
fromùëÖùëßandùëÖùëêtoùëá, respectively. In addition, to ensure ùëÖùëßandùëÖùëê
grasp sufficient information for predicting ùëá, we set up a Retain
network to concatenate ùëÖùëßandùëÖùëêand store joint information of
ùëπùíõùëπùíÑùëπùëªùë∏ùëªùíõùë∏ùëªùíÑùë∏ùëªùëªForward FlowSupervision from LabelsSupervision from TeachersSupervision from PeersNeural NetworksshallowshallowdeepretainFigure 3: Self-distillation unit for minimizing Lùëßùëê.
them for input into a Deep prediction network. To minimize Lùëßùëê,
we deploy distinct sources of supervision information from:
(1)Labels; We useùëádirectly to guide the training of deep
prediction networks by minimizing the prediction loss ùêø(ùëÑùëá,ùëá).
For shallow prediction networks, we reduce ùê∑ùêæùêø[Pùëá‚à•PùëÖùëß
ùëá]and
ùê∑ùêæùêø[Pùëá‚à•PùëÖùëê
ùëá]by minimizing prediction loss ùêø(ùëÑùëß
ùëá,ùëá)andùêø(ùëÑùëê
ùëá,ùëá).2
(2)Teachers; We regard retain and deep prediction networks as
a teacher model, which can convey the learned knowledge to help
the training of shallow prediction networks. That is, minimizing
KL-divergence loss ùêø(ùëÑùëß
ùëá,ùëÑùëá)andùêø(ùëÑùëê
ùëá,ùëÑùëá).
(3)Peers; We diminish the KL-divergence between the distribu-
tions of the outputs from the two shallow prediction networks, i.e.,
ùêø(ùëÑùëê
ùëá,ùëÑùëß
ùëá).ùê∑ùêæùêø[PùëÖùëß
ùëá‚à•PùëÖùëê
ùëá]are consequently minimized. Similarly,
we can minimizeLùëéùëêby establishing corresponding distillation
units according to Corol.4.2. In addition, Hassanpour and Greiner
[15] propose to separate ùê¥fromùëãbased onùê¥‚ä•ùëá. Specifically,
for binary treatment setting, they minimize the distribution dis-
crepancy of representations of ùê¥between the treatment group ( ùëá=
1) and the control group ( ùëá= 0). The related loss function can be
defined as follows:
minLùëé=ùëëùëñùë†ùëê({ùëÖùëñ
ùëé}ùëñ:ùë°ùëñ=0,{ùëÖùëñ
ùëé}ùëñ:ùë°ùëñ=1), (10)
where function ùëëùëñùë†ùëê(¬∑)represents the distribution discrepancy of
ùëÖùëébetween the treatment and control groups while irefers to the
i-thindividual. We also provide continuous version of Lùëéin the
Appendix A.2.1.
Mitigating Confounding Bias: With disentangled confounders
ùê∂,the confounding bias induced by ùê∂can be alleviated by re-
weighting the factual loss ùêø(ùëÑùëå,ùëå)with the context-aware impor-
tance sampling weights ùúîùëñ[15] for each individual ùëñ. Besides,ùëÑùëá,
the output of the deep prediction network for ùëá, can be employed to
regressùëå, which helps to mitigate the confounding bias caused
by unobserved confounders ùëà[14].
The loss function of ùëÜùê∑2is thus devised as follows, where ùêøcan
be any functions measuring the differences between two items. ùõº,
ùõΩ,ùõæ,ùõøare adjustable hyper-parameters. The ùëô2regularization loss
2In practice, only minimizing Eq (9a)and(9b)makes convergence challenging, resulting
in unsatisfactory performance. We speculate that this is due to a lack of sufficient
supervised information guiding the updating direction of the prediction network for
ùëÖùëê. Therefore, we introduce the minimization of ùê∑ùêæùêø[Pùëá‚à•PùëÖùëê
ùëá](corresponding loss
functionùêø(ùëÑùëê
ùëá,ùëá)to expedite loss convergence.
 
1670Self-Distilled Disentangled Learning for Counterfactual Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
‚à•¬∑‚à• 2on model weights ùëäis to avoid over-fitting.
LùëÜùê∑2=‚àëÔ∏Å
ùëñùúîùëñùêø(ùëÑùëåùëñ,ùëåùëñ)+ùõºùêø(ùëÑùëá,ùëá)
|                                {z                                }
ùëìùëéùëêùë°ùë¢ùëéùëôùëôùëúùë†ùë†+
ùõΩLùëé+ùõæ(Lùëé
ùëê+Lùëß
ùëê)
|                  {z                  }
ùëëùëñùë†ùëíùëõùë°ùëéùëõùëîùëôùëíùëöùëíùëõùë°ùëôùëúùë†ùë†+ùõø‚à•ùëä‚à•2|  {z  }
ùëüùëíùëîùë¢ùëôùëéùëüùëñùëßùëéùë°ùëñùëúùëõùëôùëúùë†ùë†.(11)
5 EXPERIMENTS
5.1 Benchmarks
Due to the absence of counterfactual outcomes in reality, it is chal-
lenging to conduct counterfactual prediction on real-world datasets.
Previous works [ 22,33,41] synthesize datasets or transform real-
world datasets. In this paper, we have conducted multiple exper-
iments on a series of synthetic datasets and a real-world dataset,
Twins. The source code of our algorithm is available on GitHub3.
5.1.1 Simulated Datasets. Binary Scenario : Similar to Hassanpour
and Greiner [15]4, we generate the synthetic datasets according to
the following steps:
‚Ä¢Forùêæinùëç,ùê∂,ùê¥,ùëâ,ùëà, sampleùêæfromN(0,ùêºùëöùëò), whereùêºùëöùëò
denotesùëöùëòdegree identity matrix. Concatenate ùëç,ùê∂, andùê¥
to constitute the observed covariates matrix ùëã.ùëârepresents
the observed IVs, the dimension of which could be set to 0.
The setting of ùëâis mainly for the comparison of IV-based
methods.ùëàrepresents the unobserved confounders.
‚Ä¢Sample treatment variables ùëáas following:
ùëáùëñ‚àºùêµùëíùëüùëõ(ùëÜùëñùëîùëöùëúùëñùëë(
ùëöùëß‚àëÔ∏Å
ùëñ=1ùëçùëñ+ùëöùëê‚àëÔ∏Å
ùëñ=1ùê∂ùëñ+ùëöùë£‚àëÔ∏Å
ùëñ=1ùëâùëñ+ùëöùë¢‚àëÔ∏Å
ùëñ=1ùëàùëñ)),(12)
whereùëöùëß,ùëöùëê,ùëöùë£,ùëöùë¢represent the dimension of ùëç,ùê∂,ùëâ,
ùëàrespectively.
‚Ä¢Sample outcome variables ùëåas following:
ùëåùëñ‚àºùêµùëíùëüùëõ(ùëÜùëñùëîùëöùëúùëñùëë((
ùëáùëñ
ùëöùëé+ùëöùëê+ùëöùë¢ùëöùëé‚àëÔ∏Å
ùëñ=1ùê¥ùëñ2+ùëöùëê‚àëÔ∏Å
ùëñ=1ùê∂ùëñ2+ùëöùë¢‚àëÔ∏Å
ùëñ=1ùëàùëñ2)+
(1‚àíùëáùëñ
ùëöùëé+ùëöùëê+ùëöùë¢ùëöùëé‚àëÔ∏Å
ùëñ=1ùê¥ùëñ+ùëöùëê‚àëÔ∏Å
ùëñ=1ùê∂ùëñ+ùëöùë¢‚àëÔ∏Å
ùëñ=1ùëàùëñ))),(13)
whereùëöùëérepresents the dimension of ùê¥.
Continuous Scenario : Following the work of Hartford et al .
[14], Wu et al . [39] , we use the Demand datasets to evaluate the
performance of ùëÜùê∑2on the continuous scenario with the same data
generation process described in detail in Wu et al. [39].
3https://github.com/XinshuLI2022/SDD
4The difference between our data generation process and that of Hassanpour and
Greiner [15] lies in the introduction of unobserved confounders ùëà, in the generation
processes ofùëáandùëå. Additionally, we categorize IVs into observed and unobserved
ones, facilitating comparisons with various baseline algorithms.5.1.2 Real-World Datasets. Twins : The Twins dataset comprises the
records of the twins who were born between 1989-1991 in the USA
[2]. We select 5271 records of same-sex twins who weighed less than
2kg and had no missing features in the records. The heavier twin
in twins is assigned with ùëá= 1 and the lighter one with ùëá= 0. The
mortality after one year is the observed outcome. It is reasonable
that only part of the features determine the outcome. Therefore, we
randomly generate ùëöùë£-dimensionùëâand choose some features ùëÄas
a combination of ùëç,ùê∂andùëàto createùëáaccording to the policy in
Eq.(12). The rest features ùëÖwill include ùê¥and some noise naturally.
We hide some features in ùëÄduring training to create ùëàand treat
the rest features in ùëÄwithùëÖasùëã.
During training, only ùëã,ùëâ,ùëáandùëåwill be accessible under any
scenario.
5.2 Comparison Methods and Metrics
Thebaselines can be classified into two categories:
IV-based Methods: DeepIV-Log and DeepIV-Gmm [ 14], DFIV
[43], OneSIV [ 24], CBIV [ 39]. These methods need well-predefined
IVsùëâ. When there is no ùëâ, i.e.,ùëöùë£equals 0, we use ùëãasùëâfor
comparison.
Non-IV-based Methods: DirectRep, CFR-Wass [ 33], DFL [ 43],
DRCFR [ 15], CEVAE [ 25]. The latter two also disentangle the hid-
den/latent factors from observed features.
Metrics: We use the absolute bias of Average Treatment Effect,
i.e.,ùúñùê¥ùëáùê∏ , to evaluate the performance of different algorithms in
the binary scenario. Formally,
ùúñùê¥ùëáùê∏ =|1
ùëÅ[ùëÅ‚àëÔ∏Å
ùëñ=1(ùëå1
ùëñ‚àíùëå0
ùëñ)‚àíùëÅ‚àëÔ∏Å
ùëñ=1(ÀÜùëå1
ùëñ‚àíÀÜùëå0
ùëñ)]|, (14)
whereùëåùëñ/ÀÜùëåùëñrepresents the factual/predicted potential outcome.
For continuous scenarios, we take Mean Squared Error ( ùëÄùëÜùê∏ ) as the
evaluation metric. The smaller ùúñùê¥ùëáùê∏ andùëÄùëÜùê∏ are, the better
the performance.
5.3 Results
5.3.1 Comparison with the SOTA methods. Binary Scenario: Table
1 shows the performance of ùëÜùê∑2on datasets with binary values. ùëöùë£-
ùëöùëß-ùëöùëê-ùëöùëé-ùëöùë¢represents the data generated with ùëöùë£predefined
IVsùëâ,ùëöùëßunderlying IVs ùëç,ùëöùëêunderlying confounders ùê∂,ùëöùëé
underlying adjustable variables ùê¥andùëöùë¢unobserved confounders
ùëà. During training, we can only observe ùëç,ùê∂andùê¥as a wholeùëã.
Twins-ùëöùë£-ùëöùë•-ùëöùë¢denotes the Twins dataset with ùëöùë£predefined
IVs,ùëöùë•observed pre-treatment covariates and ùëöùë¢unobserved
confounders and IVs. For data setting with ùëöùë•=16(Twins-0-16-8,
Twins-4-16-8, Twins-0-16-12), there are 12 confounders and IVs, 4
adjustable variables and noises, while for Twins-0-20-8, we add 4
confounders and IVs in ùëã.
We generate 10000 samples for synthetic datasets for training,
validation and testing sets, respectively, For the Twins datasets,
we randomly choose 5271 samples and split the datasets with the
ratio 63/27/10. We perform 10 replications and report the mean and
standard deviation of the bias of ATE estimation. Taking the results
in 0-4-4-2-2 as a base point for observation, we have the following
findings:
 
1671KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
Table 1: Performance comparison of bias of ATE between ùëâùëÜùê∑2and the SOTA baselines on the synthetic datasets ( ùëöùë£-ùëöùëß-ùëöùëê-ùëöùëé-
ùëöùë¢) and Twins datasets (Twins- ùëöùë£-ùëöùë•-ùëöùë¢). Bold/Underline indicates the method with the best/second best performance.
Within-Sample Out-of-Sample
Method 0-4-4-2-2 2-4-4-2-2 0-4-4-2-10 0-6-2-2-2 0-4-4-2-2 2-4-4-2-2 0-4-4-2-10 0-6-2-2-2
DirectRep 0.037(0.023) 0.034(0.009) 0.060(0.014) 0.043(0.025) 0.034(0.024) 0.034(0.013) 0.060(0.015) 0.037(0.024)
CFR 0.031(0.014) 0.032(0.017) 0.056(0.017) 0.060(0.019) 0.027(0.016) 0.032(0.021) 0.031(0.024) 0.054(0.018)
DRCFR 0.058(0.018) 0.050(0.025) 0.071(0.020) 0.066(0.015) 0.053(0.016) 0.050(0.023) 0.071(0.020) 0.059(0.018)
DFL 3.696(0.034) 3.702(0.045) 4.055(0.041) 3.450(0.030) 6.796(0.056) 6.750(0.086) 7.398(0.051) 6.393(0.063)
DeepIV-Log 0.569(0.024) 0.567(0.011) 0.601(0.011) 0.531(0.019) 0.572(0.028) 0.567(0.016) 0.601(0.011) 0.537(0.021)
DeepIV-Gmm 0.466(0.012) 0.387(0.021) 0.518(0.012) 0.437(0.007) 0.469(0.008) 0.387(0.021) 0.517(0.009) 0.442(0.004)
DFIV 3.947(0.108) 3.810(0.041) 4.400(0.134) 3.644(0.103) 7.047(0.131) 6.856(0.093) 7.749(0.125) 6.585(0.130)
OneSIV 0.504(0.008) 0.441(0.063) 0.569(0.013) 0.478(0.012) 0.507(0.009) 0.441(0.064) 0.569(0.015) 0.484(0.014)
CBIV 0.063(0.025) 0.065(0.032) 0.049(0.022) 0.033(0.023) 0.059(0.024) 0.067(0.030) 0.049(0.023) 0.030(0.017)
Ours 0.010(0.008) 0.017(0.013) 0.029(0.019) 0.014(0.013) 0.012(0.008) 0.022(0.017) 0.029(0.018) 0.013(0.010)
Within-Sample Out-of-Sample
Method Twins-0-16-8 Twins-4-16-8 Twins-0-16-12 Twins-0-20-8 Twins-0-16-8 Twins-4-16-8 Twins-0-16-12 Twins-0-20-8
DirectRep 0.015(0.007) 0.009(0.008) 0.018(0.012) 0.012(0.007) 0.023(0.011) 0.014(0.013) 0.023(0.016) 0.014(0.010)
CFR 0.011(0.010) 0.012(0.008) 0.017(0.014) 0.010(0.011) 0.016(0.008) 0.019(0.010) 0.025(0.019) 0.019(0.014)
DRCFR 0.015(0.016) 0.008(0.005) 0.016(0.017) 0.024(0.017) 0.020(0.019) 0.018(0.007) 0.020(0.009) 0.028(0.018)
DFL 0.173(0.018) 0.181(0.014) 0.169(0.020) 0.175(0.019) 0.243(0.136) 0.252(0.140) 0.240(0.141) 0.245(0.145)
DeepIV-Log 0.019(0.010) 0.017(0.016) 0.022(0.011) 0.012(0.007) 0.028(0.016) 0.026(0.019) 0.028(0.018) 0.018(0.011)
DeepIV-Gmm 0.022(0.004) 0.016(0.004) 0.022(0.003) 0.018(0.004) 0.017(0.011) 0.013(0.009) 0.017(0.011) 0.013(0.011)
DFIV 0.186(0.014) 0.185(0.015) 0.186(0.014) 0.163(0.023) 0.257(0.145) 0.255(0.148) 0.256(0.144) 0.227(0.151)
OneSIV 0.015(0.011) 0.008(0.005) 0.030(0.017) 0.017(0.006) 0.016(0.014) 0.011(0.007) 0.025(0.020) 0.013(0.010)
CBIV 0.024(0.016) 0.057(0.012) 0.051(0.038) 0.037(0.019) 0.033(0.016) 0.063(0.017) 0.057(0.039) 0.043(0.022)
Ours 0.008(0.006) 0.007(0.004) 0.007(0.006) 0.008(0.006) 0.012(0.007) 0.011(0.006) 0.012(0.006) 0.011(0.009)
Figure 4: Experimental results under continuous scenario on Demand-0-1. Among all methods, ùëâùëÜùê∑2achieves the best and
most stable results.
‚Ä¢The results of almost all IV-based methods in 0-4-4-2-2 are
worse than those in 2-4-4-2-2, demonstrating the perfor-
mance of IV-based methods is highly dependent on the pre-
defined instrumental variables.
‚Ä¢By comparing the results between 0-4-4-2-2 and 0-4-4-2-10,
we find that when there are more unobserved confounders
in the dataset, the performance of confounder balancing
methods (such as CFR, DRCFR, and DFL) will be poorer,which indicates the necessity of controlling the unobserved
confounders.
‚Ä¢If there are fewer confounders in observed features, as we
compare 0-4-4-2-2 with 0-2-6-2-2, the performance of con-
founder balancing methods without disentanglement (such
as CFR, DFL) gets impeded, implying the significance of pre-
cise confounder control by decomposing underlying factors.
‚Ä¢ùëÜùê∑2achieves the best performance among all data settings
and is far better than the second-best methods. It proves
 
1672Self-Distilled Disentangled Learning for Counterfactual Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 2: Performance comparison of MSE between ùëÜùê∑2and
the SOTA baselines on the Demands datasets (Demands- ùõº-ùõΩ).
Bold/Underline indicates the method with the best/second-
best performance.
Within-Sample
Method Demands-0-1 Demands-0-5 Demands-5-1
DirectRep 472.14(292.68) 319.27(156.58) 795.38(304.10)
CFR 1180.26(439.56) 340.12(175.82) 1891.91(79.78)
DFL 786.92(122.35) 1074.71(142.78) 874.78(103.71)
DRCFR 720.51(195.09) 451.31(407.55) 809.04(94.98)
DEVAE >10000 >10000 >10000
DeepIV-Gmm 1774.71(757.98) 3429.79(438.65) 2618.72(775.20)
DFIV 862.00(123.71) 1420.24(257.10) 909.25(137.05)
OneSIV 2573.15(157.31) >10000 2352.69(125.16)
CBIV 819.52(535.05) 410.42(241.71) 2542.58(442.26)
Ours 170.55(6.51) 216.55(47.11) 171.07(6.41)
Out-of-Sample
Method Demands-0-1 Demands-0-5 Demands-5-1
DirectRep 494.47(287.92) 303.22(144.85) 768.15(278.24)
CFR 1103.00(362.45) 326.54(110.31) 1877.60(119.71)
DFL 736.35(123.44) 914.08(90.11) 822.71(65.88)
DRCFR 766.22(188.86) 498.22(363.25) 864.21(83.39)
DEVAE >10000 >10000 >10000
DeepIV-Gmm 904.60(618.41) 2423.33(326.23) 2405.33(675.40)
DFIV 943.57(188.70) 1213.27(388.08) 1050.65(253.38)
OneSIV 2744.87(182.35) 4243.51(596.10) 2538.77(147.29)
CBIV 2990.19(735.92) 316.66(114.89) 2958.70(614.26)
Ours 183.21(14.18) 199.44(14.60) 187.20(16.05)
counterfactual prediction benefits from simultaneously con-
trolling the underlying confounders in observational features
and unobserved confounders. The effectiveness of our dis-
entanglement theory and hierarchical self-distillation frame-
work is thus validated. The results in all Twins datasets are
consistent with these findings.
Continuous Scenario: Following Wu et al . [39] , we use Demand-
ùõº-ùõΩto denote different data setting in Demands datasets. Demand-
0-1 represents the original Demand dataset defined in Hartford
et al. [14] . Theùõºin Demand- ùõº-ùõΩdenotes the extra information
from instrumental variables while the ùõΩindicates the information
increasing from instrumental variables and underlying confounders
together on the basis of Demand-0-1. We only present the results
on Demand-0-1 with box plots as shown in Figure 4, where we omit
the results of CEVAE as its ùëÄùëÜùê∏ is beyond 10000.
The IV-based methods perform worse than the non-IV-based
ones under the continuous scenario. This result indicates that con-
founding bias from the treatment regression stage is a critical prob-
lem in IV-based methods, underscoring the necessity of decompos-
ingùê∂from observed variables and thereby correcting for confound-
ing bias caused by ùê∂, coinciding with the findings under the binary
scenario. We notice that DRCFR, which performs well on discrete
datasets, suffers significantly on the Demand-0-1, reflecting the
limitations of their disentanglement theory under the continuous
(a)Identification of Z with ùëâùëÜùê∑2
(b)Identification of Z with DRCFR
(c)Identification of C with ùëâùëÜùê∑2
(d)Identification of C with DRCFR
(e)Identification of A with ùëâùëÜùê∑2
(f)Identification of A with DRCFR
Figure 5: Radar charts that visualize the capability of ùëâùëÜùê∑2
and a classical causal disentangled learning baseline DR-
CFR. Every vertex on the polygons represents a synthetic
dataset with setting ùëöùë£-ùëöùëß-ùëöùëê-ùëöùëé-ùëöùë¢. The red and blue de-
note the contribution of true variables and other variables
to the decomposed representations. The results demonstrate
our method achieves much better identification performance
of all three underlying factors in all synthetic datasets com-
pared with DRCFR.
scenario. Among all the baselines, ùëÜùê∑2achieves the best and most
stable performance on all Demands Datasets, demonstrating the
generalizability ofùëÜùê∑2on various types of datasets.
The detailed experimental results on all Demands datasets are
provided in the Table 2. It can be seen that whether adding the
information of instrumental variables (Demands-5-1) or increasing
the information of confounders (Demands-0-5), our algorithm per-
forms far better than all the baseline, which proves the efficiency
andgeneralizability of our method.
5.3.2 Disentanglement Visualization. To check ifùëâùëÜùê∑2decompose
different kinds of underlying factors from pre-treatment variables
successfully, similar to Hassanpour and Greiner [15], we use the
 
1673KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
Within-Sample 0-4-4-2-2 2-4-4-2-2 0-4-4-2-10 0-6-2-2-2 Twins-0-16-8 Twins-4-16-8 Twins-0-16-12 Twins-0-20-8
CLUB 0.514(0.005) 0.513(0.006) 0.563(0.006) 0.481(0.006) 0.020(0.009) 0.029(0.008) 0.024(0.007) 0.023(0.005)
Ours 0.010(0.008) 0.017(0.013) 0.029(0.019) 0.014(0.013) 0.008(0.006) 0.007(0.004) 0.007(0.006) 0.008(0.006)
Out-of-Sample 0-4-4-2-2 2-4-4-2-2 0-4-4-2-10 0-6-2-2-2 Twins-0-16-8 Twins-4-16-8 Twins-0-16-12 Twins-0-20-8
CLUB 0.517(0.004) 0.513(0.006) 0.563(0.004) 0.487(0.005) 0.016(0.013) 0.026(0.009) 0.021(0.012) 0.018(0.010)
Ours 0.012(0.008) 0.022(0.017) 0.029(0.018) 0.013(0.010) 0.012(0.007) 0.011(0.006) 0.012(0.006) 0.011(0.009)
Table 3: Performance comparison of bias of ATE between ùëÜùê∑2and a method which replaces the disentanglement modules in
ùëÜùê∑2with one of the SOTA mutual information estimators ùê∂ùêøùëàùêµ .
DatasetsWithin-Sample Out-of-Sample
ùêøùëùùêøùëù+ùêøùë°ùêøùëù+ùêøùë°+ùêøùëéùëáùëúùë°ùëéùëô ùêø ùëùùêøùëù+ùêøùë°ùêøùëù+ùêøùë°+ùêøùëéùëáùëúùë°ùëéùëô
Twins-0-16-8 0.027(0.012) 0.025(0.007) 0.021(0.007) 0.008(0.006) 0.022(0.011) 0.020(0.011) 0.018(0.010) 0.012(0.007)
Twins-4-16-8 0.067(0.078) 0.022(0.005) 0.024(0.005) 0.008(0.005) 0.059(0.076) 0.017(0.012) 0.019(0.012) 0.009(0.006)
Twins-0-16-12 0.113(0.110) 0.027(0.005) 0.024(0.005) 0.007(0.006) 0.111(0.109) 0.022(0.011) 0.019(0.012) 0.012(0.006)
Twins-0-20-8 0.031(0.010) 0.020(0.004) 0.023(0.006) 0.008(0.006) 0.026(0.013) 0.015(0.009) 0.018(0.012) 0.011(0.009)
Syn-0-4-4-2-2 0.519(0.005) 0.513(0.005) 0.513(0.004) 0.010(0.008) 0.522(0.004) 0.517(0.004) 0.516(0.004) 0.012(0.008)
Syn-2-4-4-2-2 0.517(0.007) 0.513(0.007) 0.513(0.007) 0.017(0.013) 0.517(0.006) 0.512(0.007) 0.513(0.006) 0.022(0.017)
Syn-0-4-4-2-10 0.569(0.005) 0.565(0.004) 0.565(0.005) 0.029(0.019) 0.568(0.004) 0.565(0.004) 0.564(0.004) 0.029(0.018)
Syn-0-6-2-2-2 0.486(0.004) 0.483(0.004) 0.483(0.004) 0.015(0.013) 0.492(0.005) 0.489(0.005) 0.489(0.005) 0.013(0.010)
Table 4: Ablation Study. ùêøùëùrepresents preserving representation networks and deep outcome classifier only. ùêøùëù+ùêøùë°adds deep
treatment classifier on the basis of the model of ùêøùëù.ùêøùëù+ùêøùë°+ùêøùëéadds the adjustable variable decomposition module on the basis
of the model of ùêøùëù+ùêøùë°.ùëáùëúùë°ùëéùëô model is our proposed model.
first (second) slice to denote the weight matrix that connects the
variables in X belonging (not belonging) to the actual variables.
The polygons‚Äô radii in Figure 5 quantify the average weights of
the first slice (in red) and the second slice (in blue). We plot the
radar charts to visualize the contribution of actual variables to the
representations of each factor on ùëÜùê∑2and another typical causal
disentangled learning work DRCFR.
As shown in each sub-figure in Figure 5, each vertex on the poly-
gon represents the results of a synthetic dataset ùëöùë£-ùëöùëß-ùëöùëê-ùëöùëé-ùëöùë¢.
Compared with DRCFR, our method realizes much better identifi-
cation performance of all three underlying factors in all datasets,
indicating that our approach can achieve successful disentangle-
ment performance.
5.3.3 Difficulty of mutual information estimation. To quantify the
difficulty of high-dimensional mutual information estimation, we
substitute our core disentanglement module with one of the state-
of-the-art mutual information estimators, CLUB [ 9], which has been
widely adopted in previous causal disentangled learning methods
[8,41,45], aiming to directly minimize the mutual information
betweenùëÖùëß,ùëÖùëê, andùëÖùëé. The related experimental results ( ùê∂ùêøùëàùêµ in
Table 3) show that the performance of our method gets impeded
dramatically, indicating the importance of bypassing the complex
mutual information estimation.
5.3.4 Ablation Study. We perform the ablation experiments to ex-
amine the contributions of each component in total loss function on
final inference performance. The results are shown in Table 4. We
conduct following steps for the ablation study analysis: Firstly, weonly preserve representation networks and deep outcome predic-
tion networks. The objective loss is reduced to factual loss ùêøùëùplus
regularization loss. Secondly, we add the deep treatment predic-
tion networks into the model on the basis of the first model, while
the objective loss becomes ùêøùëù+ùêøùë°. Then, the adjustable variables
decomposition module is integrated into the second model, thus
the objective loss is ùêøùëù+ùêøùë°+ùêøùëé. Finally, we introduce our shallow
treatment and outcome prediction networks into the third model,
the objective loss of which is presented in Eq (11)in the main paper
and named as ùëáùëúùë°ùëéùëô .
We have the following observations from the experimental re-
sults in Table 4:
‚Ä¢The model with loss function ùêøùëùperforms the worst for all
data settings on all datasets, demonstrating the significance
of treatment prediction for counterfactual prediction in the
presence of unobserved confounders.
‚Ä¢If we only decompose adjustable variables ùê¥from observed
featuresùëã, as the results under the loss ùêøùëù+ùêøùë°+ùêøùëéshows,
the model indeed does not get much improvement on the
inference performance compared with the results under the
lossùêøùëù+ùêøùë°. This may be due to the fact that only decompos-
ing A cannot eliminate the confounding bias caused by the
unmeasured confounders ùëàand underlying confounders ùê∂
existing in observed features ùëã. In addition, it indicates the
necessity of the decomposition of ùê∂andùëçfor counterfactual
prediction.
‚Ä¢When added shallow prediction networks of treatment and
outcome during training, as the results of ùëáùëúùë°ùëéùëô model show,
 
1674Self-Distilled Disentangled Learning for Counterfactual Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
(a)
 (b)
 (c)
 (d)
Figure 6: Hyper-parameters sensitivity analysis of ùõº,ùõΩ,ùõæ,ùõø on Syn-0-4-4-2-2 dataset. The blue and red lines show the ATE results
of these parameters in within-sample and out-of-sample settings, respectively. The blue circle and red star represent the best
parameters for the setting.
Figure 7: Scalability Analysis. The performance of ùëÜùê∑2remains stable and efficient with the variation of sample size in the
dataset compared with ùê∑ùëÖùê∂ùêπùëÖ , indicating the scalability of ùëÜùê∑2.
ourùëÜùê∑2achieves the best performance with the improve-
ment of more than 60%on Synthetic datasets and over 95%
on Twins datasets on the basis of the model under the loss
ùêøùëù+ùêøùë°+ùêøùëé, which demonstrates the decomposition of ùê∂
andùëçindeed advances the counterfactual inference.
5.3.5 Hyper-Parameters Analysis. With the multi-term total loss
function shown in Eq (11), we study the impact of each item on
the counterfactual prediction on Syn-0-4-4-2-2 dataset. As can be
seen from Figure 6(c), the performance of ùëÜùê∑2is mostly affected by
the changing in ùõæ. In addition, although the performance fluctuates
with the change of ùõºandùõΩ,ùëÜùê∑2performs better than almost all
baselines. These two facts demonstrate that the improvement of
inference performance is greatly contributed by the decomposition
ofùëçandùê∂. Besides, from Figure 6(d), we find that the performance
of the method is affected by changing ùõøas well, indicating the
necessity of limiting the complexity of the model.
5.3.6 Scalability Analysis. To demonstrate the scalability of our
algorithm, we conduct experiments on Syn-0-4-4-2-2 with different
sample sizes with our method and DRCFR [ 15]. Figure 7 shows the
results of the related experimental results. It can be seen from the
results that the performance of ùëÜùê∑2remains stable and efficient
with the variation of sample size in the dataset, which demonstrates
thescalability of our algorithm.6 CONCLUSION AND LIMITATIONS
To resolve the challenge of decomposing mutually independent
underlying factors in causal disentangled learning study, we pro-
vide a theoretically guaranteed solution to minimizing the mutual
information between representations of diverse underlying factors
instead of relying on explicit estimation. On this basis, we design a
hierarchical self-distilled disentanglement framework ùëÜùê∑2to ad-
vance counterfactual prediction by eliminating the confounding
bias caused by the observed and unobserved confounders simulta-
neously. Extensive experimental results on synthetic and real-world
datasets validate the effectiveness, generalizability, scalability of
our proposed theory and framework in both binary and continuous
data settings.
Limitations: Due to the lack of real-world counterfactual datasets,
ùëÜùê∑2is only evaluated on limited tasks. It would be interesting to
extend our method into other research areas, such as transfer learn-
ing [ 31], domain adaptation [ 26], counterfactual fairness [ 20], for
robust feature extraction. We will leave future work for the gener-
alization of our method to these fields.
7 ACKNOWLEDGEMENTS
We thank anonymous reviewers for their insightful comments and
discussions. Mingming Gong was supported by ARC DE210101624
and DP240102088.
 
1675KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
REFERENCES
[1]Ahmed M Alaa and Mihaela van der Schaar. 2017. Bayesian inference of indi-
vidualized treatment effects using multi-task gaussian processes. Advances in
Neural Information Processing Systems 30 (2017).
[2]Douglas Almond, Kenneth Y Chay, and David S Lee. 2005. The costs of low birth
weight. The Quarterly Journal of Economics 120, 3 (2005), 1031‚Äì1083.
[3]Joshua David Angrist and Guido Imbens. 1994. Identification and Estimation of
Local Average Treatment Effects. NBER Working Paper Series (1994).
[4]Joshua D Angrist, Guido W Imbens, and Donald B Rubin. 1996. Identification of
causal effects using instrumental variables. Journal of the American statistical
Association 91, 434 (1996), 444‚Äì455.
[5]Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua
Bengio, Aaron Courville, and Devon Hjelm. 2018. Mutual information neural
estimation. In International conference on machine learning. PMLR, 531‚Äì540.
[6]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and P. Abbeel.
2016. InfoGAN: Interpretable Representation Learning by Information Maximiz-
ing Generative Adversarial Nets. In NIPS.
[7]Lu Cheng, Ruocheng Guo, Kasim Candan, and Huan Liu. 2022. Effects of Multi-
Aspect Online Reviews with Unobserved Confounders: Estimation and Implica-
tion. In Proceedings of the International AAAI Conference on Web and Social Media,
Vol. 16. 67‚Äì78.
[8]Mingyuan Cheng, Xinru Liao, Quan Liu, Bin Ma, Jian Xu, and Bo Zheng. 2022.
Learning disentangled representations for counterfactual regression via mutual
information minimization. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1802‚Äì1806.
[9]Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence
Carin. 2020. CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information.
InInternational Conference on Machine Learning.
[10] Victor Chernozhukov, Iv√°n Fern√°ndez-Val, and Blaise Melly. 2013. Inference on
counterfactual distributions. Econometrica 81, 6 (2013), 2205‚Äì2268.
[11] Hugh A Chipman, Edward I George, and Robert E McCulloch. 2010. BART:
Bayesian additive regression trees. The Annals of Applied Statistics 4, 1 (2010),
266‚Äì298.
[12] Thomas A Glass, Steven N Goodman, Miguel A Hern√°n, and Jonathan M Samet.
2013. Causal inference in public health. Annual review of public health 34 (2013),
61.
[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In NIPS.
[14] Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. Deep
IV: A flexible approach for counterfactual prediction. In International Conference
on Machine Learning. PMLR, 1414‚Äì1423.
[15] Negar Hassanpour and Russell Greiner. 2019. Learning disentangled represen-
tations for counterfactual regression. In International Conference on Learning
Representations.
[16] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-VAE:
Learning Basic Visual Concepts with a Constrained Variational Framework. In
International Conference on Learning Representations. https://openreview.net/
forum?id=Sy2fzU9gl
[17] Caroline M Hoxby. 2000. Does competition among public schools benefit students
and taxpayers? American Economic Review 90, 5 (2000), 1209‚Äì1238.
[18] Guido W Imbens. 2004. Nonparametric estimation of average treatment effects
under exogeneity: A review. Review of Economics and statistics 86, 1 (2004), 4‚Äì29.
[19] Hyunjik Kim and Andriy Mnih. 2018. Disentangling by Factorising. In Interna-
tional Conference on Machine Learning.
[20] Hyemi Kim, Seungjae Shin, Joonho Jang, Kyungwoo Song, Weonyoung Joo,
Wanmo Kang, and Il-Chul Moon. 2020. Counterfactual Fairness with Disentangled
Causal Effect Variational Autoencoder. ArXiv abs/2011.11878 (2020).
[21] Diederik P. Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes.
CoRR abs/1312.6114 (2013).
[22] Xinshu Li and Lina Yao. 2022. Contrastive Individual Treatment Effects Estimation.
In2022 IEEE International Conference on Data Mining (ICDM). 1053‚Äì1058. https:
//doi.org/10.1109/ICDM54844.2022.00130
[23] Xinshu Li and Lina Yao. 2024. Distribution-Conditioned Adversarial Variational
Autoencoder for Valid Instrumental Variable Generation. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 38. 13664‚Äì13672.
[24] Adi Lin, Jie Lu, Junyu Xuan, Fujin Zhu, and Guangquan Zhang. 2019. One-stage
deep instrumental variable method for causal inference from observational data.
In2019 IEEE International Conference on Data Mining (ICDM). IEEE, 419‚Äì428.
[25] Christos Louizos, Uri Shalit, Joris M. Mooij, David A. Sontag, Richard S. Zemel,
and Max Welling. 2017. Causal Effect Inference with Deep Latent-Variable Models.
InNIPS.[26] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip
Versteeg, and Joris M. Mooij. 2017. Domain Adaptation by Using Causal Inference
to Predict Invariant Conditional Distributions. In Neural Information Processing
Systems.
[27] Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. 2020. Dual
instrumental variable regression. Advances in Neural Information Processing
Systems 33 (2020), 2710‚Äì2721.
[28] Judea Pearl et al .2000. Models, reasoning and inference. Cambridge, UK: Cam-
bridgeUniversityPress 19 (2000), 2.
[29] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker.
2019. On variational bounds of mutual information. In International Conference
on Machine Learning. PMLR, 5171‚Äì5180.
[30] Aahlad Puli and Rajesh Ranganath. 2020. General Control Functions for Causal
Effect Estimation from IVs. Advances in neural information processing systems 33
(2020), 8440‚Äì8451.
[31] Mateo Rojas-Carulla, Bernhard Sch√∂lkopf, Richard E. Turner, and J. Peters. 2015.
Invariant Models for Causal Transfer Learning. J. Mach. Learn. Res. 19 (2015),
36:1‚Äì36:34.
[32] Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity
score in observational studies for causal effects. Biometrika 70, 1 (1983), 41‚Äì55.
[33] Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual
treatment effect: generalization bounds and algorithms. In International Confer-
ence on Machine Learning. PMLR, 3076‚Äì3085.
[34] Rahul Singh, Maneesh Sahani, and Arthur Gretton. 2019. Kernel instrumental
variable regression. Advances in Neural Information Processing Systems 32 (2019).
[35] Victor Veitch, Dhanya Sridhar, and David Blei. 2020. Adapting text embeddings
for causal inference. In Conference on Uncertainty in Artificial Intelligence. PMLR,
919‚Äì928.
[36] Victor Veitch, Yixin Wang, and David Blei. 2019. Using embeddings to correct for
unobserved confounding in networks. Advances in Neural Information Processing
Systems 32 (2019).
[37] Yixin Wang and David M Blei. 2019. The blessings of multiple causes. J. Amer.
Statist. Assoc. 114, 528 (2019), 1574‚Äì1596.
[38] Jeffrey M. Wooldridge. 2015. Control Function Methods in Applied Econometrics.
Journal of Human Resources 50, 2 (2015), 420‚Äì445. https://doi.org/10.3368/jhr.50.
2.420 arXiv:https://jhr.uwpress.org/content/50/2/420.full.pdf
[39] Anpeng Wu, Kun Kuang, Bo Li, and Fei Wu. 2022. Instrumental variable regres-
sion with confounder balancing. In International Conference on Machine Learning.
PMLR, 24056‚Äì24075.
[40] Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Minqing Zhu, Yuxuan Liu, Bo Li,
Furui Liu, Zhihua Wang, and Fei Wu. 2022. Treatment Effect Estimation with
Unmeasured Confounders in Data Fusion. ArXiv abs/2208.10912 (2022).
[41] Anpeng Wu, Kun Kuang, Junkun Yuan, Bo Li, Runze Wu, Qiang Zhu, Yueting
Zhuang, and Fei Wu. 2020. Learning decomposed representation for counterfac-
tual inference. arXiv preprint arXiv:2006.07040 (2020).
[42] Pengzhou Abel Wu and Kenji Fukumizu. 2022. $\beta$-Intact-VAE: Identifying
and Estimating Causal Effects under Limited Overlap. In International Conference
on Learning Representations. https://openreview.net/forum?id=q7n2RngwOM
[43] Liyuan Xu, Yutian Chen, Siddarth Srinivasan, Nando de Freitas, Arnaud Doucet,
and Arthur Gretton. 2020. Learning deep features in instrumental variable
regression. arXiv preprint arXiv:2010.07154 (2020).
[44] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018.
Representation learning for treatment effect estimation from observational data.
Advances in Neural Information Processing Systems 31 (2018).
[45] Junkun Yuan, Anpeng Wu, Kun Kuang, Bo Li, Runze Wu, Fei Wu, and Lanfen Lin.
2022. Auto IV: Counterfactual Prediction via Automatic Instrumental Variable
Decomposition. ACM Transactions on Knowledge Discovery from Data (TKDD)
16, 4 (2022), 1‚Äì20.
[46] Linying Zhang, Yixin Wang, Anna Ostropolets, Jami J Mulgrave, David M Blei, and
George Hripcsak. 2019. The medical deconfounder: assessing treatment effects
with electronic health records. In Machine Learning for Healthcare Conference.
PMLR, 490‚Äì512.
[47] Weijia Zhang, Lin Liu, and Jiuyong Li. 2021. Treatment effect estimation with
disentangled latent factors. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 10923‚Äì10930.
[48] Guanglin Zhou, Shaoan Xie, Guangyuan Hao, Shiming Chen, Biwei Huang, Xiwei
Xu, Chen Wang, Liming Zhu, Lina Yao, and Kun Zhang. 2023. Emerging synergies
in causality and deep generative models: A survey. arXiv preprint arXiv:2301.12351
(2023).
[49] Guanglin Zhou, Lina Yao, Xiwei Xu, Chen Wang, and Liming Zhu. 2022. Cycle-
Balanced Representation Learning For Counterfactual Inference. In Proceedings
of the 2022 SIAM International Conference on Data Mining (SDM). SIAM, 442‚Äì450.
[50] Yang Zou, Xiaodong Yang, Zhiding Yu, B. V. K. Vijaya Kumar, and Jan Kautz. 2020.
Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification.
ArXiv abs/2007.10315 (2020).
 
1676Self-Distilled Disentangled Learning for Counterfactual Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
A APPENDIX
A.1 Theoretical Proof
A.1.1 Proof of Theorem 4.1. ConsiderùëÖùëéandùëÖùëêas the representa-
tions ofùê¥andùê∂produced by the representation networks, and let
Y be the label of the outcome. We have,
minùêº(ùëå;ùëÖùëê)‚àíùêº(ùëÖùëê;ùëå|ùëÖùëé)‚áê‚áí
minùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê)‚àíùêª(ùëå|ùëÖùëé)+ùêª(ùëå|ùëÖùëê,ùëÖùëé).
Proof. Based on the definition of mutual information [5]:
ùêº(ùëå;ùëÖùëê)=ùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê), (15)
whereùêª(ùëå)denotes Shannon entropy, and ùêª(ùëå|ùëÖùëê)is the condi-
tional entropy of ùëågivenùëÖùëê. Similarly,
ùêº(ùëÖùëê;ùëå|ùëÖùëé)=ùêª(ùëå|ùëÖùëé)‚àíùêª(ùëå|ùëÖùëé,ùëÖùëê), (16)
whereùêº(ùëÖùëê;ùëå|ùëÖùëé)denotes the conditional mutual information
betweenùëÖùëêandùëågivenùëÖùëé;ùêª(ùëå|ùëÖùëé)andùêª(ùëå|ùëÖùëé,ùëÖùëê)is the the
conditional entropy of ùëågivenùëÖùëé,ùëÖùëéandùëÖùëê, respectively.
Combining Eq (15) and Eq (16), we have Theo.4.1 holds. ‚ñ°
A.1.2 Proof of Corollary 4.2. One of sufficient conditions of mini-
mizingùêº(ùëÖùëé;ùëÖùëê)is:
min(
ùê∑ùêæùêø[Pùëå‚à•PùëÖùëé
ùëå]
ùê∑ùêæùêø[PùëÖùëé
ùëå‚à•PùëÖùëê
ùëå],
wherePùëÖùëé
ùëå=ùëù(ùëå|ùëÖùëé),PùëÖùëê
ùëå=ùëù(ùëå|ùëÖùëê)represent the predicted
distributions,Pùëå=ùëù(ùëå)represents the real distribution, and ùê∑ùêæùêø
denotes the KL-divergence.
Proof. Based on the definition of conditional entropy, for any
continuous variables ùëÖùëê,ùëÖùëéandùëå, we have:
ùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê)‚àíùêª(ùëå|ùëÖùëé)+ùêª(ùëå|ùëÖùëê,ùëÖùëé)=
‚àí‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå)ùëëùëå
+‚à´
ùëù(ùëÖùëê)ùëëùëÖùëê‚à´
ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëê)ùëëùëå
|                                                 {z                                                 }
ùë°ùëíùëüùëöùê∂
+‚à´
ùëù(ùëÖùëé)ùëëùëÖùëé‚à´
ùëù(ùëå|ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëå
|                                                  {z                                                  }
ùë°ùëíùëüùëöùê¥
‚àí‚à¨
ùëù(ùëÖùëé,ùëÖùëê)ùëëùëÖùëéùëëùëÖùëê‚à´
ùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëëùëå
|                                                                       {z                                                                       }
ùë°ùëíùëüùëöùëÄ.(18)
We further inspect ùë°ùëíùëüùëöùê∂ in Eq (18) and have:‚à´
ùëù(ùëÖùëê)ùëëùëÖùëê‚à´
ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëê)ùëëùëå=
‚à¨
ùëù(ùëÖùëê)ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëê)
ùëù(ùëå|ùëÖùëé)ùëù(ùëå|ùëÖùëé)
ùëëùëÖùëêùëëùëå.(19)By factorizing the double integrals in Eq (19)into another two
components, we show the following:
‚à¨
ùëù(ùëÖùëê)ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëê)
ùëù(ùëå|ùëÖùëé)ùëù(ùëå|ùëÖùëé)
ùëëùëÖùëêùëëùëå=
‚à¨
ùëù(ùëÖùëê)ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëê)
ùëù(ùëå|ùëÖùëé)ùëëùëÖùëêùëëùëå
|                                              {z                                              }
ùë°ùëíùëüùëöùê∂ 1+
‚à¨
ùëù(ùëÖùëê)ùëù(ùëå|ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëÖùëêùëëùëå
|                                              {z                                              }
ùë°ùëíùëüùëöùê∂ 2.(20)
Conduct similar factorization for ùë°ùëíùëüùëöùê¥ andùë°ùëíùëüùëöùëÄ in Eq (18), we
have:‚à´
ùëù(ùëÖùëé)ùëëùëÖùëé‚à´
ùëù(ùëå|ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëå=
‚à¨
ùëù(ùëÖùëé)ùëù(ùëå|ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëé)
ùëù(ùëå|ùëÖùëê)ùëëùëÖùëéùëëùëå
|                                               {z                                               }
ùë°ùëíùëüùëöùê¥ 1+
‚à¨
ùëù(ùëÖùëé)ùëù(ùëå|ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëê)ùëëùëÖùëéùëëùëå
|                                              {z                                              }
ùë°ùëíùëüùëöùê¥ 2(21)
‚à¨
ùëù(ùëÖùëé,ùëÖùëê)ùëëùëÖùëéùëëùëÖùëê‚à´
ùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëëùëå=
‚à≠
ùëù(ùëÖùëé,ùëÖùëê)ùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëé,ùëÖùëê)
ùëù(ùëå|ùëÖùëé)ùëëùëÖùëéùëëùëÖùëêùëëùëå
|                                                                    {z                                                                    }
ùë°ùëíùëüùëöùëÄ 1+
‚à≠
ùëù(ùëÖùëé,ùëÖùëê)ùëù(ùëå|ùëÖùëé,ùëÖùëê)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëÖùëéùëëùëÖùëêùëëùëå
|                                                              {z                                                              }
ùë°ùëíùëüùëöùëÄ 2.(22)
Integrateùë°ùëíùëüùëöùê∂ 1,ùë°ùëíùëüùëöùê¥ 1andùë°ùëíùëüùëöùëÄ 1overùëå:
ùê∂1=‚à´
ùëù(ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëê, (23)
ùê¥1=‚à´
ùëù(ùëÖùëé)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëé)‚à•ùëù(ùëå|ùëÖùëê)]ùëëùëÖùëé, (24)
ùëÄ1=‚à¨
ùëù(ùëÖùëé,ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëé,ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëéùëëùëÖùëê,(25)
whereùê∑ùêæùêødenotes KL-divergence. Integrate ùë°ùëíùëüùëöùê∂ 2andùë°ùëíùëüùëöùê¥ 2
overùëÖùëêandùëÖùëé, respectively, we have:
ùê∂2=‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëå, (26)
ùê¥2=‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå|ùëÖùëê)ùëëùëå. (27)
Integrateùë°ùëíùëüùëöùëÄ 2overùëÖùëê, we have:
ùëÄ2=‚à¨
ùëù(ùëå,ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëÖùëéùëëùëå. (28)
 
1677KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xinshu Li, Mingming Gong, and Lina Yao
We further factorize Eq (28) into another two components:‚à¨
ùëù(ùëå,ùëÖùëé)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëÖùëéùëëùëå
=‚à¨
ùëù(ùëå,ùëÖùëé)ùëôùëúùëîùëù(ùëå,ùëÖùëé)
ùëù(ùëÖùëé)
ùëëùëÖùëéùëëùëå
=‚à¨
ùëù(ùëå,ùëÖùëé)ùëôùëúùëîùëù(ùëå,ùëÖùëé)ùëëùëÖùëéùëëùëå‚àí‚à¨
ùëù(ùëå,ùëÖùëé)ùëôùëúùëîùëù(ùëÖùëé)ùëëùëÖùëéùëëùëå
=‚àíùêª(ùëå,ùëÖùëé)+ùêª(ùëÖùëé)
=‚àíùêª(ùëå|ùëÖùëé).
(29)
In the view of above, we have the following:
ùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê)‚àíùêª(ùëå|ùëÖùëé)+ùêª(ùëå|ùëÖùëê,ùëÖùëé)=
‚àí‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå)ùëëùëå
+‚à´
ùëù(ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëê
+‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëå‚àíùêª(ùëå|ùëÖùëé)
‚àí‚à¨
ùëù(ùëÖùëé,ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëé,ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëéùëëùëÖùëê+ùêª(ùëå|ùëÖùëé).
(30)
Based on the non-negativity of KL-divergence, Eq (30)is upper
bounded by:
‚àí‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå)ùëëùëå+‚à´
ùëù(ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëê
+‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå|ùëÖùëé)ùëëùëå=
‚à´
ùëù(ùëÖùëê)ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]ùëëùëÖùëê+
‚à´
ùëù(ùëå)ùëôùëúùëîùëù(ùëå|ùëÖùëé)
ùëù(ùëå)
ùëëùëå.
(31)
Equivalently, we have the upper bound as:
EùëÖùëé‚àºùê∏ùúÉ(ùëÖùëé|ùëã)EùëÖùëê‚àºùê∏ùúô(ùëÖùëê|ùëã)[ùê∑ùêæùêø[ùëù(ùëå|ùëÖùëê)‚à•ùëù(ùëå|ùëÖùëé)]]
+EùëÖùëé‚àºùê∏ùúÉ(ùëÖùëé|ùëã)
ùëôùëúùëîùëù(ùëå|ùëÖùëé)
ùëù(ùëå)
,(32)
whereùúÉ,ùúôdenote the parameters of the representation networks
ofùê¥andùê∂, respectively. Therefore, the objective of separating ùê∂
andùê¥fromùëãcan be formalized as:
min
ùúÉ,ùúôEùëÖùëé‚àºùê∏ùúÉ(ùëÖùëé|ùëã)EùëÖùëê‚àºùê∏ùúô(ùëÖùëê|ùëã)"
ùê∑ùêæùêø[PùëÖùëé
ùëå‚à•PùëÖùëê
ùëå]+ùëôùëúùëî"
PùëÖùëé
ùëå
Pùëå##
,
(33)wherePùëÖùëé
ùëå=ùëù(ùëå|ùëÖùëé),PùëÖùëê
ùëå=ùëù(ùëå|ùëÖùëê)andPùëådenote the
predicted distributions of ùëåfrom the representations ùëÖùëé,ùëÖùëêand
real distribution of ùëå, respectively.
Clearly, the first term in Eq (33)is equivalent to minimize the
discrepancy between the predicted distributions of ùëåfrom the repre-
sentationsùëÖùëé,ùëÖùëê. Notice the second term in Eq (33)can be implicitly
reduced by minimizing ùê∑ùêæùêøh
PùëÖùëé
ùëå‚à•Pùëåi
. Thus, we have:
min(
ùê∑ùêæùêø[Pùëå‚à•PùëÖùëé
ùëå]
ùê∑ùêæùêø[PùëÖùëé
ùëå‚à•PùëÖùëê
ùëå]
‚áíminùêª(ùëå)‚àíùêª(ùëå|ùëÖùëê)‚àíùêª(ùëå|ùëÖùëé)+ùêª(ùëå|ùëÖùëê,ùëÖùëé).
Corol.4.2 holds. ‚ñ°
A.2 Reproducibility
A.2.1 Loss Functions for Continuous Scenario. The loss function
for disentangling A is defined as following:
Lùëé=ùêø(ÀÜùëáùëê,ùëá)+ùêæùêø(ÀÜùëáùëê,ÀÜùëá)+ùêæùêø(ÀÜùëáùëê,ÀÜùëáùëé), (35)
where ÀÜùëáùë£ùëéùëüùëñùëéùëèùëôùëí inùêø(¬∑)represents the predicted values for ùë£ùëéùëüùëñùëéùëèùëôùëí
while inùêæùêø(¬∑)it denotes the distribution of the variable. Same
below.
We assume that the continuous variable follows a normal distri-
bution, therefore the KL divergence can be calculated with:
KL(ùëû‚à•ùëù)=logùúé2‚àílogùúé1+ùúé2
1+(ùúá1‚àíùúá2)2
2ùúé2
2‚àí1
2, (36)
where q‚àºN ùúá1,ùúé2
1,p‚àºN ùúá2,ùúé2
2.
To reduce the confounding bias led by observed confounders,
we define the following loss function:
Lùëúùëê=ùêø(ÀÜùëáùëß,ùëá)+ùêæùêø(ÀÜùëáùëß,ÀÜùëá)+ùêæùêø(ÀÜùëáùëß,ÀÜùëáÀúùëê), (37)
where Àúùëêdenotes the representation of ùê∂after re-balance network.
Therefore, the total loss function of ùëÜùê∑2for the continuous
scenario can be devised as:
LùëÜùê∑2=ùêø(ÀÜùëå,ùëå)+ùõºùêø(ÀÜùëá,ùëá)
|                  {z                  }
ùëìùëéùëêùë°ùë¢ùëéùëôùëôùëúùë†ùë†+ùõΩLùëé+ùõæ(Lùëê+Lùëß)
|                  {z                  }
ùëëùëñùë†ùëíùëõùë°ùëéùëõùëîùëôùëíùëöùëíùëõùë°ùëôùëúùë†ùë†
+ùúîLùëúùëê|{z}
ùëüùëíùëèùëéùëôùëéùëõùëêùëíùëôùëúùë†ùë†+ùõø‚à•ùëä‚à•2|  {z  }
ùëüùëíùëîùë¢ùëôùëéùëüùëñùëßùëéùë°ùëñùëúùëõùëôùëúùë†ùë†.(38)
A.2.2 Hardware. In this work, we perform all experiments on a
cluster with two 12-core Intel Xeon E5-2697 v2 CPUs and a total
768 GiB Memory RAM.
 
1678