Grounding and Evaluation for Large Language Models: Practical
Challenges and Lessons Learned (Survey)
Krishnaram Kenthapadi
Oracle Health AI
Redwood City, CA, USA
krishnaram.kenthapadi@oracle.comMehrnoosh Sameki
Microsoft Azure AI
Boston, MA, USA
mesameki@microsoft.comAnkur Taly
Google Cloud AI
Mountain View, CA, USA
ataly@google.com
Abstract
With the ongoing rapid adoption of Artificial Intelligence (AI)-
based systems in high-stakes domains, ensuring the trustworthi-
ness, safety, and observability of these systems has become crucial.
It is essential to evaluate and monitor AI systems not only for ac-
curacy and quality-related metrics but also for robustness, bias,
security, interpretability, and other responsible AI dimensions. We
focus on large language models (LLMs) and other generative AI
models, which present additional challenges such as hallucinations,
harmful and manipulative content, and copyright infringement. In
this survey article accompanying our tutorial , we highlight a wide
range of harms associated with generative AI systems, and survey
state of the art approaches (along with open challenges) to address
these harms.
CCS Concepts
•Computing methodologies →Artificial intelligence; Ma-
chine learning.
Keywords
Responsible AI; Generative AI; Large Language Models; Grounding;
Evaluations; Truthfulness; Safety and Alignment; Bias and Fairness,
Model Robustness and Security; Privacy; Model Disgorgement and
Unlearning; Copyright Infringement; Calibration and Confidence;
Transparency and Causal Interventions.
ACM Reference Format:
Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly. 2024. Ground-
ing and Evaluation for Large Language Models: Practical Challenges and
Lessons Learned (Survey). In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671467
1 Introduction
Considering the increasing adoption of Artificial Intelligence (AI)
technologies in our daily lives, it is crucial to develop and deploy
the underlying AI models and systems in a responsible manner
and ensure their trustworthiness, safety, and observability. Our
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671467focus is on large language models (LLMs) and other generative
AI models and applications. Such models and applications need
to be evaluated and monitored not only for accuracy and quality-
related metrics but also for robustness against adversarial attacks,
robustness under distribution shifts, bias and discrimination against
underrepresented groups, security and privacy protection, inter-
pretability, hallucinations (and other ungrounded or low-quality
outputs), harmful content (such as sexual, racist, and hateful re-
sponses), jailbreaks of safety and alignment mechanisms, prompt
injection attacks, misinformation and disinformation, fake, mislead-
ing, and manipulative content, copyright infringement, and other
responsible AI dimensions.
In this tutorial, we first highlight key harms associated with
generative AI systems, focusing on ungrounded answers (halluci-
nations), jailbreaks and prompt injection attacks, harmful content,
and copyright infringement. We then discuss how to effectively
address potential risks and challenges, following the framework
of identification, measurement, mitigation (with four mitigation
layers at the model, safety system, application, and positioning lev-
els), and operationalization. We present real-world LLM use cases,
practical challenges, best practices, lessons learned from deploying
solution approaches in the industry, and key open problems. Our
goal is to stimulate further research on grounding and evaluating
LLMs and enable researchers and practitioners to build more robust
and trustworthy LLM applications.
We first present a brief tutorial outline in §1.1, followed by an
elaborate discussion of different responsible AI dimensions in §2.
We devote §3 to the problem of grounding for LLM applications, and
§4 to the emerging area of “LLM operations”. For each dimension
(discussed in §2 to §4), we present key business problems, technical
solution approaches, and open challenges.
1.1 Tutorial Overview
Our tutorial consists of the following parts:1
Introduction and Overview of LLM Applications. We give an
overview of the generative AI landscape in industry and motivate
the topic of the tutorial with the following questions. What con-
stitutes generative AI? Why is generative AI an important topic?
What are key applications of generative AI that are being deployed
across different industry verticals? Why is it crucial to develop
and deploy generative AI models and applications in a responsible
manner?
Holistic Evaluation of LLMs. We highlight key challenges that
arise when developing and deploying LLMs and other generative AI
1https://sites.google.com/view/llm-evaluation-tutorial
6523
KDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly
models in enterprise settings, and present an overview of solution
approaches and open problems. We discuss evaluation dimensions
such as truthfulness, safety and alignment, bias and fairness, ro-
bustness and security, privacy, model disgorgement and unlearn-
ing, copyright infringement, calibration and confidence, and trans-
parency and causal interventions.
Grounding for LLMs. We then provide a deeper discussion of
grounding for LLMs, that is, ensuring that every claim in the re-
sponse generated by an LLM can be attributed to a document in
the user-specified knowledge base. We highlight how grounding
differs from factuality in the context of LLMs, and present technical
solution approaches such as retrieval augmented generation, con-
strained decoding, evaluation, guardrails, and revision, and corpus
tuning.
LLM Operations and Observability. We present processes and
best practices for addressing grounding and evaluation related chal-
lenges in real-world LLM application settings. We discuss mecha-
nisms for managing safety risks and vulnerabilities associated with
deployed LLM and generative AI applications as well as practical
approaches for monitoring the underlying models and systems with
respect to quality and other responsible AI related metrics. Using
real-world LLM case studies, we highlight practical challenges, best
practices, lessons learned from deploying solution approaches in
the industry, and key open problems.
2 Holistic Evaluation of LLMs
The overarching goal of evaluation is to determine whether a trained
LLM is fit for deployment in an enterprise setting. A commonly quoted
maxim is that LLMs must ensure helpful, truthful, and harmless
responses [ 6]. While this seems straightforward, each of these di-
mensions has several nuances. For instance, lack of truthfulness
can range from subtle misrepresentations to making blatant false
statements (colloquially known as “hallucinations”) [ 48]. Similarly,
harmful responses can vary from racially biased responses, to vio-
lent, hateful, and other inappropriate responses, to responses caus-
ing social harm (e.g., instruction on how to cheat in an examination
without getting caught). Further, in the context of evaluating LLMs,
it is important to be aware of shortcomings that have been high-
lighted with human and automatic model evaluations and with
commonly used datasets for natural language generation [37].
Besides evaluations of response quality, practitioners also have
to worry about training data privacy, model stealing, copyright
violations, and security risks such as jailbreaking [ 137] and prompt
injection [ 121]. In some settings, one may also seek calibrated con-
fidence scores for responses, interpretability, and robustness to
adversarial prompts.
In the rest of this section, we outline several evaluation dimen-
sions that arise in enterprise deployments. Evaluation of LLMs is
an important topic and there have been a number of dedicated
frameworks [ 33,66,83] describing evaluation datasets, metrics,
and benchmarks for various dimensions. A growing collection of
tools and resources have been proposed across different phases ofLLM development [ 71]. Here, we focus on the key business con-
cerns, leading solution approaches, and open challenges for each
evaluation dimension.
2.1 Truthfulness
Business problems: How do we ensure that LLM responses are
informed, relevant, and trustworthy? How do we detect and recover
from hallucinations?
Solution approaches: There is extensive work on hallucinations
in LLMs [ 48,54], including, the causes and sources of hallucinations
[77], and measures for evaluating LLMs based on their vulnerability
to producing hallucinations [ 94]. A variety of methods have been
proposed to detect hallucinations, ranging from sampling based
approaches [ 75] to approaches leveraging internal states of the LLM
[104]. There is also early work on detecting and preventing halluci-
nations in large vision language models [ 41] and other multimodal
foundation models [128].
A number of methods have been proposed to fundamentally
reduce hallucinations by tuning models. One line of work involves
training or fine-tuning LLMs on highly curated textbook-like datasets
[40,134]. Another approach involves fine-tuning LLMs on pref-
erence data for factuality, i.e., response pairs ranked by factual-
ity [109]. A fundamental hypothesis here is that LLMs have sys-
tematic markers for when they are being untruthful [ 59,110]. The
fine-tuning process aims to train LLMs to tap into these markers and
upweight factual responses. Related to this, it has been conjectured
that LLMs internalize different “personas” during pretraining, and
by training on truthful question-answer pairs, one can upweight
the “truthful” persona (even on unseen domains) [ 58]. Reducing
hallucination on a synthetic task has been explored as a way to
reduce hallucination on real-world downstream tasks [ 57]. Finally,
a recent work shows that fine-tuning LLMs on new information
that was not acquired during pretraining can encourage the model
to hallucinate [ 38]. Curating fine-tuning sets to avoid this issue
paves another path to reducing hallucinations.
While truthful responses are table stakes for enterprise deploy-
ments, we may want to go one step further and ensure that all
responses are aligned with a specific knowledge base (e.g., a set of
enterprise documents). This is known as grounding. This is a vast
topic in itself, and therefore we dedicate §3 entirely to it.
Finally we emphasize that not all hallucinations are equally bad.
For instance, hallucinations in response to nonsensical prompts or
prompts with false premises (see [ 115] for examples of questions
whose premises are factually incorrect and hence ideally need to
be rebutted) are relatively less concerning than hallucinations in
response to well-meaning prompts. Furthermore, hallucinations in
high stakes verticals like healthcare and life sciences may be far
more concerning than hallucinations in other verticals.
Open challenges: A key open challenge is detecting hallucinations
in video, speech, and multimodal settings. Another open challenge
is getting LLMs to generate citations when they answer from para-
metric knowledge. More specifically, can the LLM be made aware
ofdocument identifiers during pre-training, similar to the work on
6524Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29, 2024, Barcelona, Spain
differential search indexes [ 108], so that it can generate the appro-
priate markers as citations for various claims in its response? A
broader challenge is to leverage ideas and lessons from search and
information retrieval literature [ 80,136] to improve relevance, trust-
worthiness, and truthfulness of LLM responses. For example, how
can we incorporate valuable information such as document authors,
document quality, authoritativeness of the domain, timestamp, and
other relevant metadata during pre-training and subsequent stages
of LLM development?
2.2 Safety and Alignment
Business problems: How do we prevent an LLM from generating
toxic, violent, offensive, or otherwise unsafe output? How do we
detect such content in cases where prevention fails to work? How
do we ensure that the responses from an LLM are aligned with
human intent even in settings where it is hard for human experts
to verify such alignment?
Solution approaches: The problem can be addressed during dif-
ferent stages of the LLM lifecycle. During data collection and cura-
tion, we can apply mechanisms to detect unsafe content and take
remedial steps, such as excluding or modifying such content. Dur-
ing pretraining and fine-tuning, we can incorporate constraints or
penalties to discourage the learning of unsafe sequences. In the
reinforcement learning from human feedback (RLHF) stage, we can
include response pairs with preference labels on which one is more
appropriate, and tune the model to “align” its responses with the
preferences [ 18]. As part of prompt engineering, we can include
instructions to discourage the LLM from generating undesirable
outputs. Finally, when prevention fails, we can apply toxicity clas-
sifiers to detect undesirable outputs (as well as undesirable inputs)
and flag such instances for appropriate treatment by the user-facing
AI applications.
Another direction in alignment research is leveraging more pow-
erful LLMs to detect safety and alignment issues with a weaker
LLM in a cost-effective and latency-sensitive fashion. The problem
can be framed as a constrained optimization problem: given cost or
latency constraints, determine the subset of prompts and responses
to be evaluated using a more powerful LLM (e.g., GPT-4). In certain
settings, the task to be evaluated could be too hard for even human
experts (e.g., comparing two different summaries of a very large col-
lection of documents or judging the quality of hypotheses generated
based on a large volume of medical literature), necessitating the
use of powerful LLMs in a manner that aligns with human intent.
The converse problem of leveraging less powerful LLMs to align
more powerful LLMs with human intent has also been explored in
alignment research. A related challenge is to ensure that AI systems
with superhuman performance (which could possibly be smarter
than humans) are designed to follow human intent. While current
approaches for AI alignment rely on human ability to supervise
AI (using approaches such as reinforcement learning from human
feedback), these approaches would not be feasible when AI systems
become smarter than humans [13].
Overall, alignment is an active area of research, with approaches
ranging from data-efficient alignment [ 55] to alternatives to RLHF[25] to aligning cross-modal representations [84].
Open challenges: There has a been a bunch of recent work on
generating adversarial prompts to bypass existing mechanisms
for mitigating toxic content generation [ 119,137]. A key open
challenge is mitigating toxic content generation even under such
adversarial prompts. Recent research has shown that LLM based
guardrail models could themselves be attacked. For instance, a
two-step prefix-based attack procedure – that operates by (a) con-
structing a universal adversarial prefix for the guardrail model, and
(b) propagating this prefix to the response – has been shown to be
effective across multiple threat models, including ones in which the
adversary has no access to the guardrail model at all [ 76]. How do
we develop effective LLM based guardrails that are robust to such
attacks (and even better, have provable robustness/security guaran-
tees)? Another challenge lies in balancing reduction of undesirable
outputs with preservation of the model’s ability towards creative
generation. Finally, as LLMs are increasingly deployed as part of
open-ended applications, an important socio-technical challenge
is to investigate the opinions reflected by the LLMs, determine
whether such opinions are aligned with the needs of different appli-
cation settings, and design mechanisms to incorporate preferences
and opinions of relevant stakeholders (including those impacted by
the deployment of LLM based applications) [101].
2.3 Bias and Fairness
Business problems: How do we detect and mitigate bias in foun-
dation models? How can we apply bias detection and mitigation
throughout the foundation model lifecycle?
Solution approaches: There is extensive work on detecting and
mitigating bias in NLP models [ 12,14,15,22,36,98]. In addition to
known categories of bias observed in predictive ML models, new
types of bias arise in LLMs and other generative AI models, e.g.,
gender stereotypes, exclusionary norms, undesirable biases towards
mentions of disability, religious stereotypes, and sexual objectifica-
tion [ 10,30,106,122]. Additionally, due to the sheer size of datasets
used, it is difficult to audit and update the training data or even
anticipate different kinds of biases that may be present. Mitigation
approaches include counterfactual data augmentation (or other
types of data improvements), finetuning, incorporating fairness
regularizers, in-context learning, and natural language instructions.
For a longer discussion, we direct the readers to the survey by Gal-
legos et al. [ 30]. More broadly, we can view bias measurement and
mitigation as an important component of building a reliable and
robust application that works well across different subgroups of in-
terest (including but not necessarily limited to protected groups). By
performing fine-grained evaluation and robustness testing across
such groups, we can identify underperforming groups, improve the
performance for such groups, and thereby potentially boost even
the overall performance.
Open challenges: Bias and fairness mitigation is a relatively nascent
space, and a key open question is identifying and designing practi-
cal, scalable processes from the large class of bias measurement and
6525KDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly
mitigation techniques proposed for LLMs. A related challenge is en-
suring that the bias mitigation approach does not cause the model
to inadvertently demonstrate disparate treatment, which could be
considered unlawful in a wide range of scenarios under US law [ 70].
Further, how do we audit LLMs and other generative AI models
for different types of implicit or subtle biases and design mecha-
nisms to mitigate or recover from such biases, although the models
may not show explicit bias on standard benchmarks [ 8,45,46]?
It has recently been argued that harmful biases are an inevitable
consequence arising from the design of LLMs as they are currently
formulated, and that the connection between bias and fundamental
properties of language models needs to be probed further [ 96]. How
do we revisit the foundational assumptions underlying LLMs and
approach the development and deployment of LLMs with the goal
of preventing bias-related harms by design?
2.4 Robustness and Security
Business problems: How do we measure and improve the ro-
bustness of LLMs and other generative AI models and applications
against minor prompt perturbations, natural distribution shifts,
and other unseen or challenging scenarios? How do we safeguard
LLMs against manipulative efforts by bad actors to (jail-)break align-
ment, reveal system prompts, and inject malicious instructions into
prompts (also called prompt injection attacks [121])?
Solution approaches: Many techniques proposed for measuring
and improving robustness in NLP models can be adopted or ex-
tended for LLMs. In particular, the following ideas and notions could
be relevant for LLMs: definitions, metrics, and assumptions regard-
ing robustness (such as label-preserving vs. semantic-preserving);
connections between robustness against adversarial attacks and
robustness under distribution shifts; similarities and differences in
robustness approaches between vision and text domains; model-
based vs. human-in-the-loop identification of robustness failures.
Mitigation approaches involve learning invariant representations,
and ensuring models do not rely on spurious patterns using tech-
niques like data augmentation, reweighting, ensembling, inductive-
prior design, and causal intervention [ 117]. Open-source evaluation
frameworks and benchmarks such as Stanford HELM [ 66], Eleuther
Harness [ 33], LangTest [ 83], and Fiddler Auditor [ 51] can be uti-
lized for benchmarking different LLMs and evaluating robustness
in application-specific settings.
LLMs have been shown to be vulnerable to adversarial perturba-
tions in prompts [ 135], prompt injection attacks [ 121], data poison-
ing attacks [ 116], and universal and transferable adversarial attacks
on alignment [ 137]. Several benchmarks have been proposed for
red-teaming / testing LLMs against adversarial attacks and related
issues [ 31,87,135]. Metrics for quantifying LLM cybersecurity risks,
tools to evaluate the frequency of insecure code suggestions, and
tools to evaluate LLMs to make it harder to generate malicious
code or aid in carrying out cyberattacks have also been proposed
[11]. Additional discussion and approaches can be found in survey
articles by Barrett et al. [9] and Yao et al. [127].
Open challenges: A key challenge is to ensure that robustness
and security mechanisms are not intentionally or unintentionallyremoved in the process of finetuning an LLM [ 90]. Another chal-
lenge lies in ensuring that the mechanisms work not just during
evaluation but also during deployment (e.g., not subject to decep-
tive attacks [ 49]). A broader challenge is to investigate robustness,
security, and safety of systems that could be composed of multiple
LLMs. For example, it has been shown that adversaries can misuse
combinations of models by decomposing a malicious task into sub-
tasks, leveraging aligned frontier models to solve hard but benign
subtasks, and leveraging weaker non-aligned models to solve easy
but malicious subtasks [ 56]. As such attacks do not require the
aligned frontier models to generate malicious outputs and hence
can go undetected, there is a need to extend red-teaming efforts
beyond single models in isolation.
2.5 Privacy, Unlearning, and Copyright
Implications
Business problems: How do we ensure that LLMs, diffusion mod-
els, and other generative AI models do not memorize training data
instances (including personally identifiable information (PII)) and
reproduce such data in their responses? How do we detect PII in
LLM prompts / responses? How do prevent copyright infringement
by LLMs? How can we make an LLM / generative AI model forget
specific parts, facts, or other aspects associated with the training
data?
Solution approaches: Recent studies have shown that training
data can be extracted from LLMs [ 17] and from diffusion models [ 16]
(which could have copyright implications in case the model is per-
ceived as a database from which the original images or other copy-
righted data can be approximately retrieved). Several approaches
for watermarking [ 28,39,62] (or otherwise identifying / detecting
[81]) AI generated content have been proposed. Detecting PII in
LLM prompts / responses can be done using off-the-shelf packages,
but may require domain-specific modifications since what is con-
sidered as PII could vary based on the application. Unlearning in
LLMs [ 68], and more broadly, model disgorgement [ 2] (“the elim-
ination of not just the improperly used data, but also the effects
of improperly used data on any component of an ML model”) are
likely to become important for copyright and privacy safeguards,
ensuring responsible usage of intellectual property, compliance,
and related requirements as well for reducing bias or toxicity and
increasing fidelity.
Open challenges: A key challenge would be designing practical
and scalable techniques. For example, how can we develop dif-
ferentially private model training approaches (e.g., DPSGD [ 1],
PATE [ 85])2that are applicable for billions or trillions of parame-
ters in generative AI models? How can we ensure privacy of end
users when leveraging inputs from end users as part of retraining of
LLMs (using, say, PATE-like approaches)? Considering the impor-
tance of high quality datasets for evaluating LLMs for truthfulness,
bias, robustness, safety, and related dimensions, and the challenges
2Examples of differentially private model training include DPSGD [ 1] and PATE[ 85].
While DPSGD operates by controlling the influence of training data during gradient
descent, PATE transfers to a “student” model the knowledge of an ensemble of “teacher”
models, with intuitive privacy provided by training teachers on disjoint data and strong
privacy guaranteed by noisy aggregation of teachers’ answers.
6526Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29, 2024, Barcelona, Spain
with obtaining such datasets in highly sensitive domains such as
healthcare, how do we develop practical and feasible approaches
for differentially private synthetic data generation [ 7,69,107], po-
tentially leveraging a combination of sensitive datasets (e.g., patient
health records and clinical notes) and publicly available datasets
along with the ability to generate data by querying powerful LLMs?
2.6 Calibration and Confidence
Business problems: How can we deploy LLMs in a human-AI
hybrid setting to quantify the uncertainty (confidence score) associ-
ated with an LLM response and defer to humans when confidence
is low? Specifically, how can we achieve this in high-stakes and
latency-sensitive domains such as AI models used in healthcare
settings?
Solution approaches: Learning to defer in human-AI settings is
an active area of research [ 61], necessitating uncertainty quantifi-
cation and confidence estimation for the underlying AI models. It
also involves understanding the conditions under which humans
can effectively complement AI models [ 24]. In the context of LLMs,
recent approaches such as selective prediction, self-evaluation and
calibration, semantic uncertainty, and self-evaluation-based selec-
tive prediction have been proposed [20] (see references there-in).
Open challenges: A key challenge is to ensure that self-evaluation,
calibration, selective prediction, and other confidence modeling ap-
proaches for LLMs are effective in out-of-distribution settings. This
is particularly important for adoption in high-stakes settings like
healthcare. Another challenge is ensuring robustness of confidence
modeling approaches against adversarial prompts.
2.7 Transparency and Causal Interventions
Business problems: How do we explain the inner workings and
responses of LLMs and other generative AI models, especially in
scenarios requiring the development of end-user trust and meeting
regulatory requirements? How can we modify factual associations
linked to an LLM without retraining it?
Solution approaches: Explainability methods for LLMs have been
well studied [ 131], including techniques such as Chain-of-Thought
Prompting [ 120] and variants. However, there is work on unfaithful
explanations in chain-of-thought prompting [ 113], with connec-
tions to language model alignment through externalized reasoning
(getting models to do as much processing/reasoning through natural
language as possible). Mechanistic interpretability [ 93] is another
active area of research, which has the potential to be further ac-
celerated by the availability of small language models like phi-2.
Causal tracing approaches have been proposed to locate and edit
factual associations in LLMs. This involves first identifying neuron
activations that are decisive in the model’s factual predictions, and
then modifying these neuron activations to update specific factual
associations [78].
Open challenges: Analogous to the use of simpler approximate
models for explaining complex predictive ML models (e.g., LIME),
can we employ simpler approximate models to explain LLMs andother generative AI models (e.g., using approaches such as model
distillation) in a faithful manner? Additionally, can we develop
more efficient and practical causal intervention approaches?
3 Grounding for LLMs
Business problem: How do we ensure that responses generated
by an LLM are grounded in a user-specified knowledge base? Here,
“grounding” means that every claim in the response can be attrib-
uted [ 92] to a document in the knowledge base. We distinguish
between the terms “grounding” and “factuality”. While “ground-
ing” seeks attribution to a user-specific knowledge base, “factuality”
seeks attribution to commonly agreed world knowledge.
In the context of “grounding”, the knowledge base may be a set
of public and/or private documents, one or more Web domains,
or the entire Web. For instance, a healthcare company may want
its chatbot to always produce responses that are grounded in a
set of healthcare articles it consider authoritative. In addition to
grounding to the knowledge base, one may also want responses to
contain citations into the relevant documents in the knowledge base.
This enables transparency and allows the end-user to corroborate
all claims in the response.
3.1 Solution Approaches
In §2.1, we laid out some key directions for detecting and prevent-
ing hallucinations in LLM responses. As mentioned earlier, the
requirement of grounding goes a step further from merely prevent-
ing hallucinations. We seek responses that are fully aligned with a
given knowledge base. For instance, there may be a well-supported,
non-hallucinated claim that disagrees with the provided knowledge
base. Such a claim would still be considered ungrounded. There is
a vast and growing literature on grounding for LLMs. Below, we
sketch out the key directions in this space.
Retrieval Augmented Generation. Grounding failures often oc-
cur because not all information in the knowledge base is stored in
the LLM’s parametric memory. One popular approach to circum-
venting this challenge is Retrieval Augmented Generation (RAG) [ 52,
65], which leverages in-context learning to expose the model to
relevant information from the knowledge base. Specifically, given a
prompt (user question), we retrieve relevant snippets (called context )
from the knowledge base, augment the prompt with this context,
and then generate a response with the augmented prompt. The
success of a RAG system relies on the success of the retrieval step
and the generation step. Consequently, RAG systems are evaluated
based on dimensions such as context relevance (that is, whether the
retrieved context is relevant to the given prompt), answer faithful-
ness(that is, whether the response generated by the LLM is properly
grounded in the retrieved context), and answer relevance (that is,
whether the response is relevant to the user question) [27, 100].
The retrieval step seeks to efficiently retrieval all relevant infor-
mation for a given prompt. This typically involves chunking and
indexing the knowledge base into a vector database, and querying
it based on the prompt. The tremendous commercial interest in
RAG systems has led to a proliferation of enterprise-grade vector
databases (e.g., Pinecone [ 88], FAISS [ 26]) that enable retrieval from
arbitrary knowledge bases.
6527KDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly
To sharpen the retrieval step, several recent works have been
exploring various aspects of it, including, the appropriate gran-
ularity of retrieval (such as a paragraph or a sentence) [ 21,125],
strategies for decomposing complex prompts into one or more re-
trieval queries [ 19,53,89], supervising retrieval based on quality
of downstream generation systems [ 118], and leveraging LLMs as
retrieval indexes [108, 129].
The generation step seeks to ensure that the LLM’s response
remains grounded in the provided context. This is not a given, espe-
cially when the context conflicts with the LLM’s own parameteric
knowledge [ 123]. To combat this, a common strategy is to fine-tune
the LLM on (prompt, context, response) triples [ 5,72]. In order to
improve robustness, it is important to include a variety of contexts
with varying levels of noise in the tuning set [ 72]. To further incen-
tivize LLMs to respond based on the provided context, recent work
[60,63] proposes additional fine-tuning on counterfactual contexts
and responses that contain claims that conflict with the model’s
parameter memory. Besides fine-tuning, it is also possible to use re-
inforcement learning (RL) to reward grounded responses [ 79]. The
reward model may be trained on human feedback on grounding,
or may use automated models for performing checks (discussed
below).
Another challenge for the generation step is comprehending con-
texts with temporal information. For instance, consider a context
specifying health records of a patient and the query: “How does
the patient’s blood pressure from this week compare to last week?”
Producing a grounded response to this prompt requires knowing
the current week, and identifying the blood pressures from the
current week and the prior week.
Finally, it should be noted that no matter how effective the re-
trieval system is, there will always be instances of out-of-domain,
adversarial, or nonsensical prompts where the retrieved context
remains irrelevant. In such cases, it is crucial to train the model to
generate an “I don’t know” response by including demonstrations
of such scenarios in the tuning set [29, 130].
Retrieval augmented generation is a vast area of research, and
the above description provides only a brief overview. We refer in-
terested readers to surveys dedicated to RAG frameworks [ 35,132,
133].
Constrained Decoding. Another direction for improving ground-
edness is to use constrained decoding [ 82,105,112,126]. Here, the
key idea is to modify the decoding strategy to optimize the ground-
edness of decoded responses. A simple version of this is Best-of-N
sampling, wherein, we sample N different responses and select the
ones with the largest grounding reward [ 79,112]. Other works like
FUDGE [ 126] propose mechanisms for altering next word proba-
bilities based on the likelihood of the current (partial) sequence
completing into one that satisfies a certain attribute. One can lever-
age these ideas to optimize for the grounding attribute [ 112]. An-
other direction is context-aware decoding [ 103], which upweights
token probabilities to amplify the difference between generations
with and without the provided context. A common caveat for con-
strained decoding approaches is balancing groundedness with other
desirable attributes like coherence, fluency, and helpfulness.Evaluation, Guardrails, and Revision. While the above direc-
tions make great strides towards improving grounding of LLM
responses, they are not perfect. For instance, multiple recent evalu-
ations find that models struggle to generate grounded responses for
prompts seeking time-varying information (e.g., “Who won the lat-
est soccer match between Liverpool and Manchester United?”) [ 115],
and balancing grounding with other response attributes [ 34,67]. In
light of this, it is important to have inference time guardrails for
verifying grounding. There is extensive work on guardrails for LLM
responses, to mitigate unsafe and harmful responses [ 50,74] and
copyright violations, and to protect against LLM vulnerabilities like
prompt injection and jailbreaking [ 23,95]. Below, we specifically
discuss guardrails for checking grounding of responses.
For responses generated by RAG frameworks, grounding ver-
ification is carried out by comparing the response to the context
retrieved as part of the RAG retrieval step. The most common way
of doing this is to use a natural language inference (NLI) model to
determine if the context entails the response [ 47]. Longer responses
may be broken into individual sentences, and a separate NLI call
may be made for each claim [ 32]. This also allows identifying ci-
tations for each claim in the response. The key advantage of this
approach is that smaller T5-family [ 91] models can be trained to per-
form NLI checks, making this approach attractive for inference-time
grounding verification. While NLI based checks are getting rapidly
deployed as guardrails for grounding [ 3,4], they often struggle
with performing grounding checks that involve reasoning. Exam-
ples include validating claims making temporal statements, e.g.,
“the patient’s latest blood pressure is 130/80”, or claims involving
negation, e.g., “none of the reviews mention that the breakfast was
bad”, or claims involving quantifiers, e.g., “many guests appreciated
the free breakfast”.
An alternative approach is to use an LLM to perform the ground-
ing checks [ 5]. This allows leveraging the LLM’s superior world
knowledge and reasoning abilities in making entailment judge-
ments. In general, LLM based approaches excel when the response
is more abstract and does not quote directly from the context. How-
ever, LLM based approaches are more computationally expensive
making them less viable as inference time guardrails.
Finally, there is an emerging line of work on automatically
and iteratively revising LLM responses in light of grounding feed-
back [ 32,73,86,114]. Some approaches consider off-the-shelf LLMs
to perform the revision tasks, while others train smaller, dedicated
revision models [114].
Corpus Tuning. An orthogonal approach to retrieval augmented
generation is to pretrain the LLM on documents from the knowledge
base to allow it to learn representations tailored to the knowledge
base [ 44,124]. This is particularly helpful when the knowledge
base falls in a niche domain and/or involves novel terms not in
the model’s vocabulary; this is commonly the case for medical and
healthcare domains. Such domain-specific tuning is expected to
benefit both closed book question-answering [ 97] as well as RAG
approaches [44].
6528Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29, 2024, Barcelona, Spain
3.2 Open Challenges
Grounding for LLMs is a rapidly evolving area with several open
challenges. A key practical challenge for RAG frameworks is grap-
pling with imperfect retrieval. For instance, how should the model
respond when the retrieval includes multiple opinions that contra-
dict with each other, when the retrieval is missing crucial infor-
mation sought by the prompt, or when the retrieval is completely
irrelevant? In some cases, even when the retrieval is missing infor-
mation, the model may still have the necessary information in its
parametric memory. How should models balance amongst answer-
ing from the context versus answering from parametric memory
versus not answering at all (punting)?
A key challenge in tuning LLMs towards generating grounded
responses is that the models may optimize for grounding at the
expense of losing creativity and helpfulness. For instance, they
may quote verbatim from the provided context, which was recently
observed for a number of commercial generative AI search en-
gines [67].
Finally, a large open area is extending RAG frameworks to multi-
modal settings – for instance, settings where the underlying knowl-
edge base may consist of text, images, audio, and video, or the
query may be a combination of text and audio. This is an emerging
area, and we refer interested readers to a recent survey by Zhao et
al. [133].
4 LLM Operations and Observability
Business problems: What processes and mechanisms are impor-
tant for addressing grounding and evaluation related challenges in
real-world LLM application settings in a holistic manner? How can
we monitor LLMs and other generative AI applications deployed in
production for metrics related to quality, safety, and other responsi-
ble AI dimensions? How can we anticipate and manage risks from
frontier AI systems?
4.1 Solution Approaches
The emerging area of “LLM operations” deals with processes and
tools for designing, developing, and deploying LLMs, as well as
monitoring LLM applications once they are deployed in production.
Frameworks such as the following have been proposed to address
potential harms and challenges pertaining to grounding, robustness,
and evaluation in real-world LLM applications [10, 30, 64].
•Identification [ 31,49,87]: Recognizing and prioritizing po-
tential harms through iterative red-teaming, stress-testing,
and thorough analysis of the AI system.
•Measurement [ 42,74,100,135]: Establishing clear metrics,
creating measurement test sets, and conducting iterative,
systematic testing—both manual and automated—to quantify
the frequency and severity of identified harms.
•Mitigation [ 50,80,95,106]: Implementing tools and strate-
gies, such as prompt engineering and content filters, to re-
duce or eliminate potential harms. Repeated measurements
need to be conducted to assess the effectiveness of the im-
plemented mitigations. We could consider four layers of mit-
igation at model, safety system, application, and positioning
levels.•Operationalization [ 43,111]: Defining and executing a de-
ployment and operational readiness plan to ensure the re-
sponsible and ethical use of AI systems.
Depending on the domain requirements, an “AI safety layer” for
detecting toxicity and other undesirable outputs in realtime can
be included between the model and the application. Measuring
shifts in the distribution of LLM prompts or responses could be
helpful to identify potential degradation of the model quality over
time, and further this information can be combined with any user
feedback signals to determine regions where the model may be
underperforming [43].
Further, we need to differentiate undesirable outcomes or failures
in LLM applications caused by adversarial attacks from failures
due to the LLM’s behavior in an unexpected manner in certain
contexts. To address the latter class of “unknown unknown” failures,
we should not only perform extensive testing and red teaming to
preemptively identify and mitigate as many potential harms as
possible but also incorporate processes and mechanisms to react
quickly to any unanticipated harms during deployment. As an
example, Microsoft introduced a new category of harms called
“Disparaging, Existential, and Argumentative” harms as part of
the responsible AI evaluation for conversational AI applications
in response to the unexpected behavior of the Bing AI chatbot as
reported by a New York Times journalist [99].
More broadly, the risk profile associated with frontier AI sys-
tems is expected to expand in light of extensions of existing LLMs,
e.g., multimodality, tool use, deeper reasoning and planning, larger
and more capable memory, and increased interaction between AI
systems [ 102,111]. Of these, tool use is considered to create several
new risks and vulnerabilities.
4.2 Open Challenges
A key challenge is to classify potential risks associated with tool use,
AI agents, interaction between AI systems, etc., in terms of the level
of attention and action needed now and at different points in the
future. This involves prioritizing investments to address such risks,
especially in the following two areas: (1) Identifying failure modes
and tendencies of LLM-based applications: We need to pinpoint
how these applications can be led astray, and (2) Developing new
safety and monitoring practices: This involves leveraging metrics
like weight updates, activations, and robustness statistics, which
are not currently available as part of LLM APIs.
5 Conclusion
Given the increasing prevalence of AI technologies in our daily
lives, it is crucial to integrate responsible AI methodologies into the
development and deployment of Large Language Models (LLMs)
and other Generative AI applications. We must understand the po-
tential harms these models may introduce, and leverage state-of-the-
art techniques for enhancing overall quality, fairness, robustness,
and explainability. Addressing the responsible AI related harms
and challenges not only reduces legal, regulatory, and reputational
risks, but also safeguards individuals, businesses, and society as
a whole. Moreover, there is a pressing need to establish ways to
quantitatively assess the performance, quality, and safety of such
models. Without comprehensive evaluations, establishing trust in
6529KDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly
LLM-based applications becomes exceedingly difficult. The goal
of this tutorial is to establish a foundation for the development of
safer and more reliable generative AI applications in the future.
Acknowledgments
The authors would like to thank numerous researchers, practition-
ers, and industry leaders for insightful discussions which helped
shape the business problems, solution approaches, and open chal-
lenges discussed in this article, and Mark Johnson and Qinlan Shen
for thoughtful feedback.
References
[1]Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In
CCS.
[2]Alessandro Achille, Michael Kearns, Carson Klingenberg, and Stefano Soatto.
2024. AI model disgorgement: Methods and choices. Proceedings of the National
Academy of Sciences 121, 18 (2024).
[3]Google Cloud AI. 2024. Check grounding | Vertex AI Agent Builder. https:
//cloud.google.com/generative-ai-app-builder/docs/check-grounding
[4]Microsoft Azure AI. 2024. Groundedness detection. https://learn.microsoft.
com/en-us/azure/ai-services/content-safety/quickstart-groundedness
[5]Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024.
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
InThe Twelfth International Conference on Learning Representations.
[6]Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom
Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al .2021.
A general language assistant as a laboratory for alignment. arXiv preprint
arXiv:2112.00861 (2021).
[7]Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca
Melis, Aaron Roth, and Ankit A Siva. 2021. Differentially private query release
through adaptive projection. In International Conference on Machine Learning.
PMLR, 457–467.
[8]Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas L Griffiths. 2024.
Measuring implicit bias in explicitly unbiased large language models. arXiv
preprint arXiv:2402.04105 (2024).
[9]Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye
Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil
Feizi, et al .2023. Identifying and mitigating the security risks of generative AI.
Foundations and Trends® in Privacy and Security 6, 1 (2023), 1–52.
[10] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret
Mitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be
Too Big?. In ACM Conference on Fairness, Accountability, and Transparency.
[11] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan
Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann,
Lorenzo Fontana, et al .2023. Purple Llama CyberSecEval: A secure coding
benchmark for language models. arXiv preprint arXiv:2312.04724 (2023).
[12] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T
Kalai. 2016. Man is to computer programmer as woman is to homemaker?
Debiasing word embeddings. In NeurIPS.
[13] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao,
Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike,
et al.2023. Weak-to-strong generalization: Eliciting strong capabilities with
weak supervision. arXiv preprint arXiv:2312.09390 (2023).
[14] Aylin Caliskan. 2021. Detecting and mitigating bias in natural language pro-
cessing. Brookings Institution (2021).
[15] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived
automatically from language corpora contain human-like biases. Science 356,
6334 (2017).
[16] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. Extracting
training data from diffusion models. arXiv preprint arXiv:2301.13188 (2023).
[17] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large
Language Models. In USENIX Security Symposium, Vol. 6.
[18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy
Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro
Freire, et al .2023. Open problems and fundamental limitations of reinforcement
learning from human feedback. arXiv preprint arXiv:2307.15217 (2023).
[19] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo,
and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented
Generation. arXiv:2404.00610[20] Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Arik, Tomas Pfister, and
Somesh Jha. 2023. Adaptation with Self-Evaluation to Improve Selective Pre-
diction in LLMs. In Findings of the Association for Computational Linguistics:
EMNLP 2023.
[21] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao,
Hongming Zhang, and Dong Yu. 2023. Dense X Retrieval: What Retrieval
Granularity Should We Use? arXiv:2312.06648
[22] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian
Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and
Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation
bias in a high-stakes setting. In FAccT.
[23] Leon Derczynski, Erick Galinkin, Jeffrey Martin, Subho Majumdar, and Nanna
Inie. 2024. garak: A Framework for Security Probing Large Language Models.
https://garak.ai. (2024).
[24] Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi. 2022.
Human-algorithm collaboration: Achieving complementarity and avoiding un-
fairness. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,
and Transparency.
[25] Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev.
2023. SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to
RLHF. In Findings of the Association for Computational Linguistics: EMNLP 2023.
11275–11288.
[26] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.
The Faiss library. (2024). arXiv:cs.LG/2401.08281
[27] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RA-
GAs: Automated Evaluation of Retrieval Augmented Generation. In Proceedings
of the 18th Conference of the European Chapter of the Association for Computa-
tional Linguistics: System Demonstrations. 150–158.
[28] Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mah-
moody, and Mingyuan Wang. 2023. Publicly detectable watermarking for lan-
guage models. arXiv preprint arXiv:2310.18491 (2023).
[29] Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachan-
dran, and Yulia Tsvetkov. 2024. Don’t Hallucinate, Abstain: Identifying LLM
Knowledge Gaps via Multi-LLM Collaboration. arXiv:2402.00367
[30] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim,
Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. Bias
and fairness in large language models: A survey. arXiv preprint arXiv:2309.00770
(2023).
[31] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav
Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al .2022.
Red teaming language models to reduce harms: Methods, scaling behaviors, and
lessons learned. arXiv preprint arXiv:2209.07858 (2022).
[32] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Cha-
ganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
Kelvin Guu. 2023. RARR: Researching and Revising What Language Models Say,
Using Language Models. In Proceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan
Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Lin-
guistics, Toronto, Canada, 16477–16508. https://doi.org/10.18653/v1/2023.acl-
long.910 https://aclanthology.org/2023.acl-long.910.
[33] L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPofi, C Foster, L Golding, J Hsu,
A Le Noac’h, et al .2023. A framework for few-shot language model evaluation.
Zenodo (2023).
[34] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling Large
Language Models to Generate Text with Citations. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language Processing, Houda Bouamor,
Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,
Singapore, 6465–6488. https://doi.org/10.18653/v1/2023.emnlp-main.398
[35] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi
Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
Generation for Large Language Models: A Survey. arXiv:2312.10997
[36] Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word
embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of
the National Academy of Sciences 115, 16 (2018), E3635–E3644.
[37] Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2023. Repairing the
cracked foundation: A survey of obstacles in evaluation practices for generated
text. Journal of Artificial Intelligence Research 77 (2023), 103–166.
[38] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Re-
ichart, and Jonathan Herzig. 2024. Does Fine-Tuning LLMs on New Knowledge
Encourage Hallucinations? arXiv:2405.05904
[39] Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, and Cho-Jui
Hsieh. 2022. Watermarking pre-trained language models with backdooring.
arXiv preprint arXiv:2210.07543 (2022).
[40] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie
Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de
Rosa, Olli Saarikivi, et al .2023. Textbooks are all you need. arXiv preprint
arXiv:2306.11644 (2023).
6530Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29, 2024, Barcelona, Spain
[41] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing
hallucinations in large vision language models. In AAAI.
[42] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu,
Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al .2023. Evaluating large language
models: A comprehensive survey. arXiv preprint arXiv:2310.19736 (2023).
[43] Gyandev Gupta, Bashir Rastegarpanah, Amalendu Iyer, Joshua Rubin, and Krish-
naram Kenthapadi. 2023. Measuring Distributional Shifts in Text: The Advantage
of Language Model-Based Embeddings. arXiv preprint arXiv:2312.02337 (2023).
[44] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Belt-
agy, Doug Downey, and Noah A. Smith. 2020. Don’t Stop Pretraining: Adapt
Language Models to Domains and Tasks. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce
Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational
Linguistics, Online, 8342–8360. https://doi.org/10.18653/v1/2020.acl-main.740
[45] Amit Haim, Alejandro Salinas, and Julian Nyarko. 2024. What’s in a Name?
Auditing Large Language Models for Race and Gender Bias. arXiv preprint
arXiv:2402.14875 (2024).
[46] Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, and Sharese King. 2024.
Dialect prejudice predicts AI decisions about people’s character, employability,
and criminality. arXiv preprint arXiv:2403.00742 (2024).
[47] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kuk-
liansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and
Yossi Matias. 2022. TRUE: Re-evaluating Factual Consistency Evaluation. In
Proceedings of the 2022 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, Marine
Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.).
Association for Computational Linguistics, Seattle, United States, 3905–3920.
https://doi.org/10.18653/v1/2022.naacl-main.287 https://aclanthology.org/2022.
naacl-main.287.
[48] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al .2023.
A survey on hallucination in large language models: Principles, taxonomy,
challenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).
[49] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte
MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng,
et al.2024. Sleeper agents: Training deceptive LLMs that persist through safety
training. arXiv preprint arXiv:2401.05566 (2024).
[50] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yun-
ing Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al .2023.
Llama Guard: LLM-based input-output safeguard for human-AI conversations.
arXiv preprint arXiv:2312.06674 (2023).
[51] Amal Iyer and Krishnaram Kenthapadi. 2023. Introducing Fiddler Auditor:
Evaluate the Robustness of LLMs and NLP Models. Fiddler AI Blog.
[52] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with
Generative Models for Open Domain Question Answering. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty
(Eds.). Association for Computational Linguistics, Online, 874–880. https:
//doi.org/10.18653/v1/2021.eacl-main.74
[53] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park.
2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
Models through Question Complexity. arXiv:2403.14403
[54] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination
in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.
[55] Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar, Sungjin
Lee, Yang Liu, and Mahdi Namazifar. 2023. Data-Efficient Alignment of Large
Language Models with Human Feedback Through Natural Language. In NeurIPS
2023 Workshop on Instruction Tuning and Instruction Following.
[56] Erik Jones, Anca Dragan, and Jacob Steinhardt. 2024. Adversaries Can Misuse
Combinations of Safe Models. arXiv preprint arXiv:2406.14595 (2024).
[57] Erik Jones, Hamid Palangi, Clarisse Simões Ribeiro, Varun Chandrasekaran,
Subhabrata Mukherjee, Arindam Mitra, Ahmed Hassan Awadallah, and Ece Ka-
mar. 2024. Teaching Language Models to Hallucinate Less with Synthetic Tasks.
InThe Twelfth International Conference on Learning Representations (ICLR).
[58] Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, and He He. 2023.
Personas as a way to model truthfulness in language models. arXiv preprint
arXiv:2310.18168 (2023).
[59] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-
Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan
Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli,
Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt,
Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack
Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Ka-
plan. 2022. Language Models (Mostly) Know What They Know. arXiv:2207.05221
[60] Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2020. Learning The
Difference That Makes A Difference With Counterfactually-Augmented Data.InInternational Conference on Learning Representations. https://openreview.
net/forum?id=Sklgs0NFvr
[61] Vijay Keswani, Matthew Lease, and Krishnaram Kenthapadi. 2021. Towards
Unbiased and Accurate Deferral to Multiple Experts. In Proceedings of the 2021
AAAI/ACM Conference on AI, Ethics, and Society.
[62] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and
Tom Goldstein. 2023. A watermark for large language models. In International
Conference on Machine Learning. PMLR, 17061–17084.
[63] Abdullatif Köksal, Renat Aksitov, and Chung-Ching Chang. 2023. Hallucination
Augmented Recitations for Language Models. arXiv:2311.07424
[64] Microsoft Learn. 2023. Overview of Responsible AI practices for Azure OpenAI
models. Azure AI Services Documentation.
[65] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Gener-
ation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information
Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin
(Eds.), Vol. 33. Curran Associates, Inc., 9459–9474. https://proceedings.neurips.
cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf
[66] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-
hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al .
2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110
(2022).
[67] Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating Verifiability
in Generative Search Engines. In The 2023 Conference on Empirical Methods in
Natural Language Processing. https://openreview.net/forum?id=ZQV5iRPAua
[68] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Pe-
ter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al .2024.
Rethinking Machine Unlearning for Large Language Models. arXiv preprint
arXiv:2402.08787 (2024).
[69] Terrance Liu, Giuseppe Vietri, and Steven Z Wu. 2021. Iterative methods for
private synthetic data: Unifying framework and new methods. Advances in
Neural Information Processing Systems 34 (2021), 690–702.
[70] Michael Lohaus, Matthäus Kleindessner, Krishnaram Kenthapadi, Francesco
Locatello, and Chris Russell. 2022. Are two heads the same as one? Identifying
disparate treatment in fair neural networks. Advances in Neural Information
Processing Systems 35 (2022), 16548–16562.
[71] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel
McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al .
2024. The Responsible Foundation Model Development Cheatsheet: A Review
of Tools & Resources. arXiv preprint arXiv:2406.16746 (2024).
[72] Hongyin Luo, Tianhua Zhang, Yung-Sung Chuang, Yuan Gong, Yoon Kim, Xixin
Wu, Helen M. Meng, and James R. Glass. 2023. Search Augmented Instruction
Learning. In The 2023 Conference on Empirical Methods in Natural Language
Processing. https://openreview.net/forum?id=noIvPGG8P1
[73] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank
Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir
Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with
Self-Feedback. https://arxiv.org/abs/2303.17651. arXiv:cs.CL/2303.17651
[74] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily
Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, et al .2023. A
Framework for Automated Measurement of Responsible AI Harms in Generative
AI Applications. arXiv preprint arXiv:2310.17750 (2023).
[75] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-
Resource Black-Box Hallucination Detection for Generative Large Language
Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing. 9004–9017.
[76] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem
Fawaz, Somesh Jha, and Atul Prakash. 2024. PRP: Propagating Universal
Perturbations to Attack Large Language Model Guard-Rails. arXiv preprint
arXiv:2402.15911 (2024).
[77] Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Hosseini, Mark Johnson,
and Mark Steedman. 2023. Sources of Hallucination by Large Language Models
on Inference Tasks. In Findings of the Association for Computational Linguistics:
EMNLP 2023. 2758–2774.
[78] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locat-
ing and editing factual associations in GPT. Advances in Neural Information
Processing Systems 35 (2022), 17359–17372.
[79] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song,
Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support
answers with verified quotes. arXiv:2203.11147
[80] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search:
Making domain experts out of dilettantes. In ACM SIGIR Forum, Vol. 55. 1–27.
[81] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and
Chelsea Finn. 2023. DetectGPT: Zero-shot machine-generated text detection
using probability curvature. In International Conference on Machine Learning .
6531KDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly
PMLR, 24950–24962.
[82] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping
Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman,
Jilin Chen, Alex Beutel, and Ahmad Beirami. 2024. Controlled Decoding from
Language Models. arXiv:2310.17022
[83] Arshaan Nazir, Thadaka Kalyan Chakravarthy, David Amore Cecchini, Rakshit
Khajuria, Prikshit Sharma, Ali Tarik Mirik, Veysel Kocaman, and David Talby.
2024. LangTest: A comprehensive evaluation library for custom LLM and NLP
models. Software Impacts (2024), 100619.
[84] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty,
Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. 2023. X-
InstructBLIP: A Framework for aligning X-Modal instruction-aware repre-
sentations to LLMs and Emergent Cross-modal Reasoning. arXiv preprint
arXiv:2311.18799 (2023).
[85] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal
Talwar, and Ulfar Erlingsson. 2018. Scalable Private Learning with PATE. In
International Conference on Learning Representations.
[86] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qi-
uyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
Your Facts and Try Again: Improving Large Language Models with External
Knowledge and Automated Feedback. arXiv:2302.12813
[87] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red Teaming
Language Models with Language Models. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing. 3419–3448.
[88] Pinecone. [n. d.]. Pinecone Vector Database. http://pinecone.io
[89] Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, and
Lifu Huang. 2023. The Art of SOCRATIC QUESTIONING: Recursive Thinking
with Large Language Models. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing. 4177–4199.
[90] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and
Peter Henderson. 2024. Fine-tuning Aligned Language Models Compromises
Safety, Even When Users Do Not Intend To!. In The Twelfth International Con-
ference on Learning Representations.
[91] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.
Res.21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[92] Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo,
Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar,
Iulia Turc, and David Reitter. 2023. Measuring Attribution in
Natural Language Generation Models. Computational Linguistics
49, 4 (12 2023), 777–840. arXiv:https://direct.mit.edu/coli/article-
pdf/49/4/777/2269661/coli_a_00486.pdf https://doi.org/10.1162/coli_a_00486
[93] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. 2023.
Toward transparent AI: A survey on interpreting the inner structures of deep
neural networks. In 2023 IEEE Conference on Secure and Trustworthy Machine
Learning (SaTML). IEEE, 464–483.
[94] Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar,
SM Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, and Amitava Das.
2023. The Troubling Emergence of Hallucination in Large Language Models-An
Extensive Definition, Quantification, and Prescriptive Remediations. In Proceed-
ings of the 2023 Conference on Empirical Methods in Natural Language Processing.
2541–2573.
[95] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher
Parisien, and Jonathan Cohen. 2023. NeMo Guardrails: A Toolkit for Con-
trollable and Safe LLM Applications with Programmable Rails. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing:
System Demonstrations. 431–445.
[96] Philip Resnik. 2024. Large Language Models are Biased Because They Are Large
Language Models. arXiv preprint arXiv:2406.13138 (2024).
[97] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge
Can You Pack Into the Parameters of a Language Model?. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for
Computational Linguistics, Online, 5418–5426. https://doi.org/10.18653/v1/
2020.emnlp-main.437
[98] Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Christian
Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna
Rumshisky, and Adam Kalai. 2019. What’s in a Name? Reducing Bias in Bios
without Access to Protected Attributes. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers). 4187–4195.
[99] Kevin Roose. 2023. A Conversation With Bings Chatbot Left Me Deeply Unset-
tled. New York Times (2023).
[100] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024.
ARES: An Automated Evaluation Framework for Retrieval-Augmented Genera-
tion Systems. In Proceedings of the 2024 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
(Volume 1: Long Papers). 338–354.
[101] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and
Tatsunori Hashimoto. 2023. Whose opinions do language models reflect?. In
International Conference on Machine Learning. PMLR, 29971–30004.
[102] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whit-
tlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung,
Noam Kolt, et al .2023. Model evaluation for extreme risks. arXiv preprint
arXiv:2305.15324 (2023).
[103] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer,
and Wen-tau Yih. 2024. Trusting Your Evidence: Hallucinate Less with Context-
aware Decoding. In Proceedings of the 2024 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies (Volume 2: Short Papers), Kevin Duh, Helena Gomez, and Steven Bethard
(Eds.). Association for Computational Linguistics, Mexico City, Mexico, 783–791.
https://aclanthology.org/2024.naacl-short.69
[104] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. 2023. On Early
Detection of Hallucinations in Factual Question Answering. arXiv preprint
arXiv:2312.14183 (2023).
[105] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to
summarize with human feedback. In Advances in Neural Information Processing
Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.),
Vol. 33. Curran Associates, Inc., 3008–3021. https://proceedings.neurips.cc/
paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf
[106] Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph,
Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli. 2023. Evaluat-
ing and mitigating discrimination in language model decisions. arXiv preprint
arXiv:2312.03689 (2023).
[107] Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, and
Gerome Miklau. 2021. Benchmarking differentially private synthetic data gen-
eration algorithms. arXiv preprint arXiv:2112.09238 (2021).
[108] Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,
Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and
Donald Metzler. 2022. Transformer Memory as a Differentiable Search Index. In
Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?
id=Vu-B0clPfq
[109] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea
Finn. 2024. Fine-Tuning Language Models for Factuality. In The Twelfth Interna-
tional Conference on Learning Representations. https://openreview.net/forum?
id=WPZ2yPag4K
[110] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov,
Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just Ask for Cali-
bration: Strategies for Eliciting Calibrated Confidence Scores from Language
Models Fine-Tuned with Human Feedback. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Processing, Houda Bouamor,
Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,
Singapore, 5433–5442. https://doi.org/10.18653/v1/2023.emnlp-main.330
[111] Helen Toner, Jessica Ji, John Bansemer, Lucy Lim, Chris Painter, Courtney Corley,
Jess Whittlestone, Matt Botvinick, Mikel Rodriguez, and Ram Shankar Siva
Kumar. 2023. Skating to Where the Puck is Going: Anticipating and Managing
Risks from Frontier AI Systems. Report from the July 2023 Roundtable hosted
by the Center for Security and Emerging Technology (CSET) at Georgetown
University and Google DeepMind.
[112] Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, and Yingbo
Zhou. 2024. Unlocking Anticipatory Text Generation: A Constrained Approach
for Faithful Decoding with Large Language Models. https://openreview.net/
forum?id=774elYc5tw
[113] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language
models don’t always say what they think: Unfaithful explanations in chain-
of-thought prompting. Advances in Neural Information Processing Systems 36
(2023).
[114] Giorgos Vernikos, Arthur Brazinskas, Jakub Adamek, Jonathan Mallinson, Ali-
aksei Severyn, and Eric Malmi. 2024. Small Language Models Improve Giants by
Rewriting Their Outputs. In Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Linguistics (Volume 1: Long Pa-
pers), Yvette Graham and Matthew Purver (Eds.). Association for Computational
Linguistics, St. Julian’s, Malta, 2703–2718. https://aclanthology.org/2024.eacl-
long.165
[115] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris
Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. Fresh-
LLMs: Refreshing Large Language Models with Search Engine Augmentation.
arXiv:2310.03214
[116] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Concealed Data
Poisoning Attacks on NLP Models. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies. 139–150.
6532Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29, 2024, Barcelona, Spain
[117] Xuezhi Wang, Haohan Wang, and Diyi Yang. 2022. Measure and Improve
Robustness in NLP Models: A Survey. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. 4569–4586.
[118] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham
Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.
arXiv:2311.08377
[119] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How
Does LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Infor-
mation Processing Systems. https://openreview.net/forum?id=jA235JGM09
[120] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning
in large language models. Advances in Neural Information Processing Systems 35
(2022), 24824–24837.
[121] Simon Willison. 2022. Prompt injection attacks against GPT-3. Simon Willison’s
Weblog (2022).
[122] Robert Wolfe, Yiwei Yang, Bill Howe, and Aylin Caliskan. 2023. Contrastive
language-vision AI models pretrained on web-scraped multimodal data exhibit
sexual objectification bias. In Proceedings of the 2023 ACM Conference on Fairness,
Accountability, and Transparency. 1174–1185.
[123] Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quantifying the tug-of-war
between an LLM’s internal prior and external evidence. arXiv:2404.10198
[124] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data
Selection for Language Models via Importance Resampling. In Thirty-seventh
Conference on Neural Information Processing Systems. https://openreview.net/
forum?id=uPSQv0leAu
[125] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,
Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
Catanzaro. 2024. Retrieval meets Long Context Large Language Models. In The
Twelfth International Conference on Learning Representations. https://openreview.
net/forum?id=xw5nxFWMlo
[126] Kevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Generation With
Future Discriminators. In Proceedings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language
Technologies, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (Eds.). Association for Computational Linguistics, Online,
3511–3535. https://aclanthology.org/2021.naacl-main.276
[127] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.
2024. A survey on large language model (LLM) security and privacy: The good,
the bad, and the ugly. High-Confidence Computing (2024), 100211.
[128] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang
Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucinationcorrection for multimodal large language models. arXiv preprint arXiv:2310.16045
(2023).
[129] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya
Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate
rather than Retrieve: Large Language Models are Strong Context Genera-
tors. In The Eleventh International Conference on Learning Representations.
https://openreview.net/forum?id=fB0hRu9GZUS
[130] Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang,
Yangyi Chen, Heng Ji, and Tong Zhang. 2024. R-Tuning: Instructing Large
Language Models to Say ‘I Don’t Know’. In Proceedings of the 2024 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena
Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,
Mexico City, Mexico, 7106–7132. https://aclanthology.org/2024.naacl-long.394
[131] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,
Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for large
language models: A survey. ACM Transactions on Intelligent Systems and Tech-
nology 15, 2 (2024), 1–38.
[132] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,
Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-
Augmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473
[133] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Cheng-
wei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty.
2023. Retrieving Multimodal Information for Augmented Generation: A Survey.
InFindings of the Association for Computational Linguistics: EMNLP 2023, Houda
Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
Linguistics, Singapore, 4736–4756. https://doi.org/10.18653/v1/2023.findings-
emnlp.314
[134] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
Zettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alignment. In
Thirty-seventh Conference on Neural Information Processing Systems. https:
//openreview.net/forum?id=KBMOKmX2he
[135] Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. 2023. Prompt-
Bench: A unified library for evaluation of large language models. arXiv preprint
arXiv:2312.07910 (2023).
[136] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong
Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for
information retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).
[137] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal
and transferable adversarial attacks on aligned language models. arXiv preprint
arXiv:2307.15043 (2023).
6533