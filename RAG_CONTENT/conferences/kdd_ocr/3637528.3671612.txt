ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese
Network in Advertising
Ruize Wang
Tencent Inc.
Shanghai, China
rayrzwang@tencent.comHui Xu
Tencent Inc.
Shanghai, China
peterhxu@tencent.comYing Cheng∗
School of Computer Science
Fudan University
Shanghai, China
chengy18@fudan.edu.cn
Qi He
Tencent Inc.
Shanghai, China
nickyhe@tencent.comXing Zhou
Tencent Inc.
Shanghai, China
leostarzhou@tencent.comRui Feng
School of Computer Science
Fudan University
Shanghai, China
fengrui@fudan.edu.cn
Wei Xu
Tencent Inc.
Shanghai, China
davidxu@tencent.comLei Huang∗
Tencent Inc.
Shenzhen, China
leihuang@tencent.comJie Jiang
Tencent Inc.
Shenzhen, China
zeus@tencent.com
ABSTRACT
Advertising platforms have evolved in estimating Lifetime Value
(LTV) to better align with advertisers’ true performance metric
which considers cumulative sum of purchases a customer con-
tributes over a period. Accurate LTV estimation is crucial for the
precision of the advertising system and the effectiveness of adver-
tisements. However, the sparsity of real-world LTV data presents a
significant challenge to LTV predictive model(i.e., pLTV), severely
limiting the their capabilities. Therefore, we propose to utilize ex-
ternal data, in addition to the internal data of advertising platform,
to expand the size of purchase samples and enhance the LTV pre-
diction model of the advertising platform. To tackle the issue of
data distribution shift between internal and external platforms,
we introduce an Adaptive Difference Siamese Network (ADSNet),
which employs cross-domain transfer learning to prevent negative
transfer. Specifically, ADSNet is designed to learn information that
is beneficial to the target domain. We introduce a gain evaluation
strategy to calculate information gain, aiding the model in learning
helpful information for the target domain and providing the ability
to reject noisy samples, thus avoiding negative transfer. Addition-
ally, we also design a Domain Adaptation Module as a bridge to
connect different domains, reduce the distribution distance between
them, and enhance the consistency of representation space distri-
bution. We conduct extensive offline experiments and online A/B
∗Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671612tests on a real advertising platform. Our proposed ADSNet method
outperforms other methods, improving GINI by 2%. The ablation
study highlights the importance of the gain evaluation strategy in
negative gain sample rejection and improving model performance.
Additionally, ADSNet significantly improves long-tail prediction.
The online A/B tests confirm ADSNet’s efficacy, increasing online
LTV by 3.47% and GMV by 3.89%.
CCS CONCEPTS
•Information systems →Computational advertising; Online
advertising.
KEYWORDS
Lifetime Value Prediction, Adaptive Cross-Domain Transfer Learn-
ing, Computational Advertising
ACM Reference Format:
Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, Lei
Huang, and Jie Jiang. 2024. ADSNet: Cross-Domain LTV Prediction with an
Adaptive Siamese Network in Advertising. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3671612
1 INTRODUCTION
The Lifetime Value (LTV) in an advertising system is defined as the
cumulative sum of purchases (i.e., revenue for advertisers) a cus-
tomer contributes over a given period. Given its direct correlation
with return on investment (ROI), this metric attracts significant
attention from advertisers. Consequently, advertising platforms
have progressively evolved to support LTV estimation [ 2,21,51],
thereby aligning more closely with the assessment requirements
of advertisers. Accurate estimation of LTV plays a crucial role in
improving the precision of advertising systems and ensuring the
effectiveness of advertisements.
5872
KDD ’24, August 25–29, 2024, Barcelona, Spain Ruize Wang et al.
Advertising 
Conversion Funnel
Exposure
Click
Activation
Purchase
(a) Sparsity of purchase
samplesInternal
DataSource
External
DataDistribution
L TV
L TVProbabilityProbability
(b) Expanding Sample
Size with External DataT ransfer
(c) Domain ShiftModel Parameter
Domain
Negative
Gainexternalinternal
Figure 1: Illustration of the advertising system conversion
funnel and challenges, including: (a) the sparsity of inter-
nal purchase data, (b) introducing external data, (c) negative
transfer due to data distribution shift.
Recently, some efforts have been made to improve the perfor-
mance of LTV estimation. For example, Wang et al . [44] propose
to model LTV as zero-inflated lognormal (ZILN) distribution to
address heavy-tailed problem. Some methods [ 5,36,47] propose to
model user behaviors and use a feature missing-aware network to
reduce the effect of the missing features while training[ 5,36,47].
However, as purchase is close to the end of the advertising conver-
sion funnel, data sparsity becomes an issue as conversion behavior
deepens. This sparsity of real-world LTV data presents a formidable
challenge to LTV predictive models, severely limiting their capabil-
ities and having received far less attention. To address this problem,
it is essential to leverage abundant external data to enhance the
LTV prediction model of advertising platforms.
Cross-domain transfer learning [ 18,29,55] has emerged as a
promising approach to bridge the gap between different domains,
particularly when there is a sparsity of labeled data in the tar-
get domain. This approach has been successfully applied in var-
ious fields, such as computer vision, natural language process-
ing, and recommender/adverisiting systems. Some methods focus
on transferring knowledge from the data perspective through ad-
justment and transformation of samples and features, including
instance weighting[ 19,49], feature transformation[ 7,22,41,46].
Multi-domain learning approaches [ 32,35] propose to mix multiple
sources of data for training a unified model in a multi-task manner.
However, existing cross-domain transfer learning methods often
suffer from negative transfer, which occurs when knowledge of
the source domain is not beneficial or even harmful to the learning
process of the target domain. Therefore, it is critical to develop a
more robust and adaptive cross-domain transfer learning method
to mitigate the impact of negative transfer.
In this paper, we propose an Adaptive Difference Siamese Net-
work (ADSNet) to address the challenges associated with LTV esti-
mation and cross-domain transfer learning. ADSNet employs a gain
evaluation strategy based on a pseudo-siamese structure, effectively
learning beneficial information for the target domain while reject-
ing noisy samples and avoiding negative transfer. Furthermore, our
Domain Adaptation Module bridges different domains, reducing
distribution distance and fostering consistent representation space
distribution. We conduct extensive experiments and perform online
A/B tests in a real online advertising scenario. Experimental resultsdemonstrate that our approach significantly improves performance
on the LTV prediction dataset. Further analysis highlights the ef-
fectiveness of our model in rejecting negative gain samples and
improving long-tail prediction capabilities.
The contributions of this paper can be summarized as follows:
•We introduce a novel approach to address data sparsity by
integrating external data with the internal data from the ad-
vertising system for LTV estimation, utilizing a cross-domain
transfer framework.
•We propose an Adaptive Difference Siamese Network (AD-
SNet) to tackle negative transfer. Utilizing a pseudo-siamese
structure in conjunction with a gain evaluation strategy, we
facilitate the assimilation of beneficial external information
into the target domain while effectively filtering out noise.
Additionally, we incorporate a domain adaptation module
to promote consistency across different domains.
•Extensive experiments reveal that ADSNet surpasses other
models, substantially enhancing performance and mitigating
negative transfer. Additionally, ADSNet effectively improves
long-tail prediction. Online A/B tests further showcase the
practical advantages of ADSNet in real-world advertising
systems.
2 RELATED WORK
2.1 Customer Lifetime Value Prediction in
Advertising
Customer Lifetime Value (LTV) prediction has become an integral
component of modern advertising platforms, owing to its direct im-
pact on the effectiveness of advertisement placements and overall
advertising system precision. Previous studies have focused on var-
ious approaches to improve the accuracy of LTV estimation. Some
previous works [ 5,13,36] explore deep learning methods for LTV
prediction, emphasizing the potential of neural networks in under-
standing complex user behaviors. Reddy et al . [31] present a com-
parative analysis of traditional statistical methods versus machine
learning techniques for LTV estimation. Addressing the issue of
missing features in real-world advertising scenarios, MarfNet [ 47]
proposes a feature missing-aware network designed to mitigate
the impact of these missing features during training, rather than
fill them with default values. Due to LTV of customers follows
a heavy long-tailed distribution with significant fraction of zero
value, Wang et al . [44] proposes to model the LTV as a zero-inflated
lognormal (ZILN) distribution which is described as a mixture of
zero-point mass and lognormal distribution, to address the heavy-
tailed problem. In this modeling manner, the model fits the mean
and deviation of the distribution. But the distribution assumption
is simple and does not meet the multimodal distribution in real
scenarios, which leads to limitations. ODMN [ 21] introduces an
approach that partitions the complex LTV distribution into several
sub-distributions, each trained by distribution experts.
In the conversion funnel “exposure->click->activation->purchase “,
compared to the CTR “exposure->click “ and CVR “click->activation “
prediction, LTV prediction is a more challenging problem due to
purchase being the deepest behavior, the data on LTV is very sparse.
Although the aforementioned methods obtain better performance
5873ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
L TV
L TVOutput
Shared
Structure
Reject
AcceptGain Evaluation
Strategy
L TV
Vanilla
Network
Gain
NetworkInput
Target
Domain
Source
DomainDomain
Shift
(b) Cross-domain T ransfer Learning with
Adaptive Difference Siamese Network (ADS-Net)Internal Data
External DataCross DomainDomain Adaption
L TV
L TVOutput
L TVMixer
NetworkTarget
Domain
Source
DomainDomain
ShiftInternal Data
External DataCross Domain
Accept
(a) Multi-domain Joint LearningInput
Figure 2: (a) Conventional multi-domain models incorporate source domain knowledge by simply aggregating both source and
target domain data and training in a joint manner, which result in introducing noisy samples and even lead to the problem of
negative transfer due to data distribution shifts. (b) Our ADSNet explores a cross-domain transfer learning method. It employs
the pseudo-siamese network to differentially evaluate the information gain provided by the source domain, which supports
the rejection of negative gain samples, thereby helping the model learn information that is beneficial to the target domain.
on LTV prediction, they struggle with the sparsity of real-world
LTV data. The amount of data determines the upper limit of the
model’s performance [ 1,37,39,45]. Motivated by such constraints,
we utilize external data from sources other than the advertising
platform’s internal data, which aims to increase the number of
purchase samples and improve the LTV prediction model of the
advertising platform.
2.2 Cross-domain Transfer Learning
Cross-domain transfer learning has emerged as a powerful approach
to leverage knowledge from one domain to improve learning in
another, especially in scenarios where the target domain suffers
from data sparsity. However, a significant challenge in this field
is the occurrence of negative transfer[ 18,23,29], where irrelevant
or noisy information from the source domain hinders the learning
process in the target domain, a key challenge in this area.
The concept of domain transfer has also been explored in the
context of multi-task learning in Click-Through Rate (CTR) and Con-
version Rate (CVR) prediction scenarios[ 25,40]. PLE [ 35] basically
follows the gate structure and attention network for information
transfer, similar to MoEs [ 17,24,53]. It separates task-sharing and
task-specific parameters to learn shared and private representations
for each task explicitly, and introduces a progressive routing man-
ner. STAR [ 32] leverages partitioned normalization (PN) to privatize
normalization for examples from different domains, and consists of
shared centered parameters and domain-specific parameters, adap-
tively modulating its parameters conditioned on the domain for
more refined prediction. CCTL [ 52] introduces a framework to mit-
igate the effects of negative transfer for different business domains
in CTR prediction scenario. It evaluates the information gain of the
source domain on the target domain using a symmetric companion
network. Compared to CCTL, ADSNet focuses specifically on LTV
prediction and employs a gain evaluation strategy to calculate the
information gain and reject noisy samples. Additionally, ADSNet
introduces a domain adaptation module to reduce the distribution
distance between different domains and enhance the consistency
of representation space distribution.3 TASK DEFINITION
Definition 1 (Lifetime Value). Lifetime Value (LTV) in an adver-
tising system is defined as the cumulative sum of purchases (i.e.,
revenue for advertisers) a customer contributes over a certain pe-
riod. This metric is crucial for advertisers in assessing their return
on investment (ROI).
Definition 2 (Customer LTV Prediction). Within the context
of an advertising system, the goal of the LTV prediction task is to
estimate the LTV 𝑦𝑖∈𝑌for a given (user, ad) pair 𝑥𝑖∈𝑋. In this
scenario,𝑥𝑖represents the specific (user, ad) combination, and 𝑦𝑖
corresponds to the LTV generated by the user for the advertisement.
Specifically, given a set of samples D={𝑥𝑖,𝑦𝑖}𝑁
𝑖=1consisting of N
data-label pairs, we aim to compute the LTV between the user 𝑢𝑖∈
Uand an advertisement 𝑎𝑖∈A. This progress can be formulated
as following:
ˆ𝑦𝑖=𝑝𝐿𝑇𝑉𝑖=𝑓(x𝑖|𝐷,Θ), (1)
where Θdenotes the parameters of the LTV estimation model. The
model tasks sample 𝑥𝑖=(𝑢𝑖,𝑎𝑖)as input, where 𝑢𝑖is user features
including user historical behavior (e.g., click and conversion se-
quences), user profile (e.g., age, gender and purchase frequencey),
and𝑎𝑖is ad features such as the tile and category. 𝑦𝑖∈[0,∞)is
denoted as LTV label.
4 METHOD
As illustrated in Figure 2 (a), most of the previous works improve
models by integrating knowledge from the source domain via multi-
domain joint learning. Regardless of these different variations of
methods with multi-domain learning, the common issues not fully
studied are negative transfer induced by the domain shift. To ad-
dress this, we present our approach ADSNet as shown in Figure
2(b) and Figure 3. We utilize the pseudo-siamese network to evalu-
ate the information gain, to support learning information that is
beneficial to the target domain and reject noisy samples with the
gain evaluation strategy. Additionally, we introduce a domain adap-
tation module as a bridge to connect different domains, reduce the
distribution distance between them, and enhance the uniformity
5874KDD ’24, August 25–29, 2024, Barcelona, Spain Ruize Wang et al.
Pseudo-Siamese Network
Shared Structure
Gain Metrics Gain Expression
Reject
Accept
MLP
MLPPurchase
Rate
MLP
MLPPurchase
AmountpL TV
x
MLPSigmoid
MLP
W S
Gain
Network
Domain
Adaptation
Adapter
...
Embedding
Vanilla
NetworkEncoding LayerExpert Layer
Inputs
(User , Ad, Context)Tower LayerOutput
External 
Data
MLP
MLPPurchase
Amount
MLP
MLPPurchase
RatepL TV
x
Adapter
Encoding LayerExpert Layer
Inputs
(User , Ad, Context)Tower LayerOutput
Embedding...
Internal
Data
Gain Evaluation
1
3
2
Figure 3: Overview of our proposed ADSNet approach. A pseudo-siamese architecture (part 1) is employed to establish a metric
to contrast the differences between two networks, allowing the calculation of each input’s gain and supporting the rejection of
negative gain samples (part 2). A domain adaption module (part 3) is proposed to promote consistency across different domains.
of the representation space distribution. In this section, we first
describe the base model(Sec. 4.1) for LTV prediction, the structure
of our ADSNet including the pseudo-siamese network(Sec. 4.2),
the gain evaluation strategy(Sec. 4.3) and domain adaptation mod-
ule(Sec. 4.4), and then present the adaptive training process with
iterative alignment strategy (Sec. 4.5).
4.1 Backbone for LTV Prediction
To better meet the complex distribution of LTV in real advertising
scenarios, we develop a deep neural network (DNN) with Ordinal
Classification [ 3,8,9,27,33] as our backbone for LTV prediction,
which is a classic framework consisting of the encoding layer, expert
layer, and tower layer.
4.1.1 Encoding Layer. We categorize the features into distinct
fields according to their characteristics. For instance, the user’s ba-
sic profile attributes, such as age, gender, and region, constitute one
field, while different user behavior sequences form another field.
Given the input features 𝑥𝑖, we encode features in different fields
into embedding vectors and employ Field-weighted Factorization
Machines (FwFM) [ 28] to model the different interaction of features
between different fields, and the embedding vectors are concate-
nated as the final embedding representation E=𝐸𝑛𝑐𝑜𝑑𝑖𝑛𝑔(X),
where𝑥𝑖∈Xis the input feature. It is worth noting that alter-
native encoding methods could also be employed, such as Deep
Cross Network (DCN)[ 42], Learning Hidden Unit Contributions
(LHUC)[ 34], Transformer[ 38], etc., to either replace FwFFM or be
used in conjunction with it.4.1.2 Expert Layer. The expert layer is designed to learn and
represent various aspects of the input by incorporating multiple
experts, each of which is responsible for capturing specific patterns
or characteristics within the data. Inspired by the success of Mix-
ture of Experts(MoE) architecture [ 17,24], we employ PLE [ 35]
as expert layer here, which consists of a set of expert networks
and a gating network. Each expert network is implemented as a
Multi-Layer Perceptron (MLP), and the gating network is respon-
sible for determining the contribution of each expert to the final
output. Given the encoded representation 𝑒𝑖∈Efrom the encoding
layer, we feed it into each expert network, obtaining a set of expert
outputs𝑉={𝑣𝑗
𝑖}𝐾
𝑗=1, where𝐾is the number of experts. The gating
network, which is also an MLP with sofrmax function, takes the
same input and produces a set of gating weights {𝑔𝑗}𝐾
𝑗=1, with each
corresponding to the weight of expert. The final output ℎ𝑖∈𝐻of
the expert layer is a weighted sum of the expert outputs, with the
gating weights determining the contribution of each expert:
ℎ𝑖=𝐾∑︁
𝑗=1𝑔𝑗·𝑣𝑗
𝑖. (2)
4.1.3 Tower Layer. The tower layer takes the expert layer’s out-
put and generates the final LTV prediction. The LTV of customers in
mobile gaming typically exhibits two distinct traits: 1) a long-tailed
distribution with a substantial proportion of zero values, and 2) a
multimodal distribution of purchases due to standardized purchase
tiers (e.g., $6, $30, $98, and $198). To this end, we extend a multi-
granularity prediction module, which comprises two components:
5875ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
the probability of purchase prediction at a coarse-grained level and
the amount of purchase prediction at a fine-grained level.
Probability of Purchase Prediction. In particular, we use a clas-
sifier to estimate the purchase likelihood of each sample as 𝑝𝑖. This
classifier is an MLP using a sigmoid activation function. For the
prediction of purchase probability, we utilize the cross-entropy loss
function. The lossL𝑝𝑟𝑜𝑏is formally defined as:
L𝑝𝑟𝑜𝑏=−1{𝑦𝑖>0}log(𝑝𝑖)−( 1−1{𝑦𝑖>0})log(1−𝑝𝑖),(3)
where 1{·}is the indicator function, which represents the presence
of a positive sample, i.e., whether a purchase has been made.
Amount of Purchase Prediction. Different from ZILN-based
methods [ 44,48] which typically employ a ZILN loss to approxi-
mate the complex purchase distribution’s mean and variance, we
develop a multi-class classification module with ordinal classifi-
cation [ 3,8,9]. It divides the LTV distribution into several sub-
distributions and performs prediction over each sub-distribution
with multiple binary classifiers. This helps model to learn the or-
dered nature of purchase categories, and allows for the direct mod-
eling of the cumulative distribution function of purchases, which is
more aligned with the inherently sequential progression of purchase
amounts. We transform continuous purchase labels into a set of
binary classification labels to reflect rank information. Specifically,
the original LTV label 𝑦𝑖is assigned to a segment 𝑠𝑖, which repre-
sents the segment label for the LTV ranking level. The segments
are determined by frequency equalization to maintain a relatively
balanced sample size across them. Then, each segment(rank) la-
bel𝑠𝑖is expanded to 𝐾−1binary class labels {𝑠1
𝑖,...,𝑠𝐾−1
𝑖}such
that𝑠𝑘
𝑖∈{0,1}is indicates whether 𝑠𝑖exceeds rank 𝑟𝑘. For exam-
ple,𝑠𝑘
𝑖=1{𝑦𝑖>𝑟𝑘}. The indicator function 1{·}is 1 if the inner
condition is true and 0 otherwise. Each binary classifier employs
a sigmoid activation function, and 𝑝𝑘
𝑖represents the probability
prediction of the 𝑘-th binary classifier. In the process of inference,
the predicted LTV (pLTV) is calculated as:
𝑝𝐿𝑇𝑉𝑖=𝑝𝑖∗𝐾−1∑︁
𝑘=1
𝑝𝑘
𝑖∗
𝑙𝑡𝑣𝑘−𝑙𝑡𝑣𝑘−1
, (4)
where𝑝𝑖denotes the probability of purchase. 𝑙𝑡𝑣𝑘is the average
LTV of the𝑘-th segment. 𝑝𝑘
𝑖is the probability of purchase for the
𝑘-th segment.
At this stage, we adopt the binary cross-entropy loss for amount
of purchase prediction with ordinal classification, and define the
purchase amount loss L𝑎𝑚𝑜𝑢𝑛𝑡 as follows:
L𝑎𝑚𝑜𝑢𝑛𝑡 =L{𝑦𝑖>0}"𝐾−1∑︁
𝑘=1
−𝑠𝑖log
𝑝𝑘
𝑖
−(1−𝑠𝑖)log
1−𝑝𝑘
𝑖#
,
(5)
where𝑦𝑖denotes original LTV. To train the above two tasks jointly,
we introduce the loss as:
L𝑝𝑙𝑡𝑣=L𝑝𝑟𝑜𝑏+L𝑎𝑚𝑜𝑢𝑛𝑡. (6)
4.2 Difference Pseudo-Siamese Network
The Siamese network [ 6,12,30] is a typical architecture in deep
learning, which comprises two branches with identical structuresand uses similar and dissimilar pairs to learn similarity. In contrast,
the Pseudo-Siamese network [ 10,50] offers more flexibility than
the Siamese network, as it allows different structures to receive
inputs from various modalities. Drawing inspiration from these
frameworks, we integrate the Pseudo-Siamese Network to assess
the information gain, to support learning information from source
domain, e.g., external data outside of advertising platform, that
is beneficial to the target domain and reject noisy samples. This
selective transfer capability is crucial in practical scenarios.
Specifically, our pseudo-siamese network is composed of a vanilla
network and a gain Network. Those two networks are based on the
backbone for LTV prediction as Sec. 4.1. The gain Network receives
inputs from both external and internal samples, while the vanilla
Network is exclusively fed with samples from the internal channel.
During training, both networks will update their parameters. This
concurrent parameter updating allows each network to learn from
its respective data stream, with the gain network adjusting to the
nuances of both the external and internal channel samples, and the
vanilla network refining its understanding based on the internal
channel data alone. This process is key to enabling the pseudo-
siamese network to effectively differentiate and integrate relevant
information from diverse data sources. In the training process, we
define the losses calculated by L𝑝𝑙𝑡𝑣asL𝑔𝑎𝑖𝑛,𝑠 of external data and
𝐿𝑔𝑎𝑖𝑛,𝑡 of internal data from the gain network. And vanilla network
Loss of internal data loss is L𝑣𝑎𝑛=L𝑣𝑎𝑛,𝑡 from vanilla network.
4.3 Gain Evaluation Strategy
Leveraging a pseudo-siamese architecture, we can establish a metric
to contrast the differences between two networks, thereby calcu-
lating the contribution of input data to the network’s performance
as:
𝑊𝐺=𝑆𝑐𝑜𝑟𝑒𝑔𝑎𝑖𝑛−𝑆𝑐𝑜𝑟𝑒𝑣𝑎𝑛, (7)
where𝑆𝑐𝑜𝑟𝑒(·)is the gain metric function. In this paper, we utilize
this approach to quantify the gain provided by external data to the
gain network by examining the variance in losses computed by the
two networks on internal samples. Formally, this is expressed as:
𝑊𝐺=L𝑣𝑎𝑛,𝑡−L𝑔𝑎𝑖𝑛,𝑡, (8)
where𝑊𝐺represents the gain. If 𝑊𝐺>0, it means there is a pos-
itive gain to the internal domain. It is notable that, in addition to
the aforementioned methodology, we can extend to employ rein-
forcement learning by defining metrics that are pertinent to the
business objectives. The differential in these metrics can be utilized
as a reward signal to train the network by Adversarial Reward
Learning [14, 43].
4.4 Domain Adaptation Module
The domain adaptation module serves as a bridge between the gain
Network and the vanilla Network, mitigating the disparity between
domain distributions. To this end, an adapter layer is integrated at
the bottom of the tower module, implemented as an MLP. First, the
adapter layer within the gain network is employed to estimate the
significance of external data as 𝑊𝑠, which is calculated as:
𝑊𝑠=1/𝑒𝑥𝑝(𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑀𝐿𝑃(𝑒𝑖))). (9)
5876KDD ’24, August 25–29, 2024, Barcelona, Spain Ruize Wang et al.
Algorithm 1 Iterative Alignment Strategy
Require: internal samples, external samples, T (total training
steps)
Ensure:𝜃𝑣,𝜃𝑔
1:Warmup Stage:
2:𝜃𝑣←Train(internal samples)
3:Joint Training Stage:
4:𝜃𝑔←𝜃𝑣
5:Set sync_frequency to 500
6:for𝑡=1to𝑇do
7:𝜃𝑔←Train(internal samples∪external samples)
8:𝜃𝑣←Train(internal samples)
9:if𝑡mod sync_frequency ==0then
10:𝜃𝑣←𝜃𝑔
11: end if
12:end for
13:return𝜃𝑣,𝜃𝑔
Then we consider two levels of distributional divergence, en-
compassing both the low-level embedding layer and the high-level
tower layer. Following [ 20], we employ knowledge distillation to
constrain the distributions through Mean Squared Error (MSE):
Lembed =MSE
𝑬𝑉,𝑬𝐺
;
Ltask_tower =MSE
𝑯𝑉
𝑎𝑑𝑎𝑝𝑡𝑒𝑟,𝑯𝐺
𝑎𝑑𝑎𝑝𝑡𝑒𝑟
,(10)
where 𝑬𝑉and𝑬𝐺denote the embeddings from the vanilla network
and the gain network, respectively, while 𝑯𝑉
𝑎𝑑𝑎𝑝𝑡𝑒𝑟and𝑯𝐺
𝑎𝑑𝑎𝑝𝑡𝑒𝑟
represent the outputs of the adapter layers within the vanilla net-
work and the gain network, respectively.
The loss of domain adaption is the sum losses as:
Ldomain =Lembed+L task_tower. (11)
4.5 Training Objective
Loss Function. To train the whole model jointly, we introduce a
composite loss function Ltotal as:
L𝑡𝑜𝑡𝑎𝑙=Lgain+L van+𝛽∗Ldomain (12)
where hyperparameter 𝛽is employed to control the trade-off among
these losses. The loss for the gain Nntwork is computed as:
Lgain=𝑊𝑠·Lgain, s1(𝑊𝐺>0)+L gain, t (13)
whereLgain, s is external data loss, Lgain, t is internal data loss from
gain network respectively. 𝑊𝑠is the weight of external data and
𝑊𝐺is the information gain. The indicator function 1(𝑊𝐺>0)
denotes the selective transfer capability of ADSNet, which permits
only the advantageous external data to be utilized.
Iterative Alignment Strategy. Considering that the performance
of the gain network is expected to improve over time, creating a
widening gap between it and the vanilla network, such a scenario
could eventually render the gain evaluation strategy ineffective. To
address this issue, we propose to use iterative alignment strategy
during the training process to prevent divergence between the two
networks. Specifically, the model training is divided into two stages
as in Algorithm 1: (1) Warmup Stage: Only internal samples areTable 1: Dataset Statistic for LTV Prediction
Domain# Sample
SizeAvg. LTV
Inner DataDomain1 9,136 0.34
Domain2 15,706 0.53
Domain3 34,129 4.82
External Data 225,490 8.02
used to train the vanilla network. This allows the base network to
establish a stable starting point. (2) Joint Training Stage: Initialize
the gain network with the vanilla network parameters, and train
both networks with parameters synchronized iteratively.
5 EXPERIMENTS
5.1 Experimental Setup
5.1.1 Datasets. Due to the lack of public dataset on LTV predic-
tion, we construct an industry dataset to conduct offline evaluation.
The dataset is collected from from a sampling of conversion logs
from Tencent’s online advertising system over a span of 90 days,
covering four internal traffics (i.e., business domains) and autho-
rized external data from other platforms. The dataset consists of
billions of examples, and is split according to the time axis, with 70,
10, and 10 days’ worth of samples allocated for training, validation,
and testing, respectively. The main statistics are shown in Table 1,
where the sample size is calculated as the average per day. Due to
company privacy policy, we only disclose the purchase sample size
and LTV. Table 1 displays the sample size, and average LTV for each
domain. As indicated in this table, different domains exhibit dis-
tinct domain-specific data distribution, as reflected in the varying
LTVs. It can be observed that the domain with the highest Avg.LTV
(Domain #3) is 4.82, while the domain with the lowest Avg.LTV
(Domain #2) is only 0.07. The external data have a substantially
larger average LTV, indicating a different data distribution from
the internal data. These variations in the dataset provide a compre-
hensive ground for evaluating the performance of LTV prediction
models across different domains and data distributions.
5.1.2 Metrics. we focus on evaluating the performance of a model
that predicts customer lifetime value (LTV) by differentiating high-
value customers from low-value ones. For this purpose, we employ
two evaluation metrics: the Area Under the Curve (AUC) and the
Normalized Gini Coefficient (Norm GINI). The AUC is a widely
used metric for assessing the model’s ability to identify purchase
users, as it measures the model’s classification performance. A
higher AUC value indicates that the model can better discriminate
between positive (purchase) and negative (non-purchase) classes.
However, the AUC does not provide information about the accu-
racy of the users’ ranking based on the predicted LTV. To address
this issue, we further adopt the Normalized Gini Coefficient (Norm
GINI) [ 44]. The Norm GINI is a robust measure that captures the
model’s ability to rank users according to their predicted LTV accu-
rately. It is preferred over the Mean Squared Error (MSE) due to its
robustness to outliers and better business interpretation. The Norm
GINI ranges between 0 and 1, with a value of 1 indicating perfect
5877ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Comparison with state-of-the-art LTV prediction approaches.
MethodsDomain #1 Domain #2 Domain #3 Average
AUCAverage
GINIAUC GINI AUC GINI AUC GINI
Only
Internal Data
(Single Domain)DeepFM [11] 0.719 0.731 0.692 0.633 0.9027 0.9029 0.771 0.755
ZILN [44] 0.748 0.759 0.710 0.701 0.9243 0.9237 0.794 0.794
FiBiNet [16] 0.753 0.762 0.721 0.712 0.9250 0.9246 0.800 0.799
GateNet [15] 0.749 0.767 0.743 0.732 0.9413 0.9411 0.811 0.813
ADSNet-Backbone (Ours) 0.763 0.770 0.757 0.745 0.9472 0.9467 0.822 0.820
Internal &
External Data
(Cross Domain)Share-Bottom [4] with ZILN 0.745 0.746 0.703 0.705 0.9187 0.9201 0.789 0.790
MMOE [24] with ZILN 0.752 0.754 0.727 0.726 0.9249 0.9222 0.801 0.800
STAR [32] with ZILN 0.774 0.775 0.755 0.753 0.9503 0.9448 0.826 0.824
CCTL [52] with ZILN 0.779 0.781 0.760 0.758 0.9531 0.9522 0.831 0.830
CCTL [52] with ADSNet-Backbone 0.783 0.792 0.764 0.761 0.9560 0.9537 0.834 0.836
ADSNet (Ours) 0.792 0.807 0.772 0.771 0.9614 0.9570 0.842 0.845
consistency between the ranking based on predicted LTV and the
ranking based on the real LTV. The lower bound of 0 corresponds
to the random ordering of customers.
5.1.3 Hyper-Parameter Settings. We implemented all methods
using Tensorflow. In the training stage, we utilize the Follow-the-
Regularized-Leader (FTRL) optimizer [ 26] for sparse parameters
and the Follow the Moving Leader (FTML) optimizer [ 54] for dense
parameters, in which the learning rate is set within the range of {5e-
3, 1e-2}, respectively. The batch size is fixed as 512. The embedding
dimension is set to 32, which means that each input feature is
represented by a 32-dimensional vector. The architecture of each
expert consists of an MLP with two hidden layers with a hidden size
of[128,64]. The tower layer of all methods is designed as an MLP
with a hidden size of 32. The weight of the domain adaptation loss,
denoted as𝛽, is set to 0.1 according to the grid search performed
on the validation set.
5.2 Models for Comparison
We compare our proposed methods with several baselines, includ-
ing both single-domain and cross-domain settings. Each of these
methods is described as follows:
Single-Domain. These approaches leverage solely internal data
for model training.
•DeepFM [ 11]: This model integrates the Factorization Ma-
chine (FM) model and a deep neural network to extract both
low-order and high-order feature interactions.
•ZILN [ 44]: This approach designs a novel zero-inflated log-
normal loss to address the imbalanced regression problem.
•FiBiNet [ 16]: This model employs a Squeeze-and-Excitation
network to discern the significance of feature interactions.
•GateNet [ 15]: This model utilizes a gating mechanism to
distill and select pertinent latent information from feature
embeddings.
•ADSNet-Backbone (Ours): This is the backbone of ADSNet
in this paper, characterized by a DNN optimized for ordinal
classification.Table 3: Overall metrics on LTV Prediction Task Val Set.
Method AUC GINI
1ADSNet 0.851 0.856
2w/o Domain Adaptation Module 0.833 0.835
3w/o Gain Evaluation Strategy 0.822 0.824
4w/o Iterative Alignment Strategy 0.839 0.843
5w/o All Components 0.819 0.820
Cross-Domain. The cross-domain methods integrate both inter-
nal data and external data during the training stage, while only
validating and testing on internal data.
•Share-Bottom [ 4]: This model employs a common architec-
ture which shares the parameters of the bottom layers to
facilitate multi-task/multi-domain learning, potentially miti-
gating overfitting while being sensitive to task discrepancies
and data distribution variances.
•MMOE [ 24]: This method builds upon the Share-Bottom
approach by adding multiple experts and a gating mechanism
to learn the differences between various domains, alleviating
the issue of domain variances.
•STAR [ 32]: This model consists of shared centered parame-
ters and domain-specific parameters, adaptively modulating
its parameters conditioned on the domain.
•ADSNet (Ours): This denotes our full model, i.e., our back-
bone with difference pseudo-siamese network and domain
adaptation module.
5.3 Comparisons with State-of-the-Arts
Table 2 illustrates the performance of various models on the pro-
posed LTV prediction dataset. It is evident that our backbone model
(i.e., ADSNet-Backbone) serves as a strong baseline, and our full
model (i.e., ADSNet) further enhances performance.
In particular, our base model outperforms other single-domain
methods such as GateNet and ZILN, suggesting that the ordinal clas-
sification is more suitable for modeling the complex multi-modal
5878KDD ’24, August 25–29, 2024, Barcelona, Spain Ruize Wang et al.
100K 200K 300K 400K 500K
Training Steps0.10.20.30.40.50.60.70.8Negative Gain Rejection Rate
0.10.20.30.40.50.60.70.80.9
GINI
Negative Gain Rejection
ADSNet w/ GES
ADSNet w/o GES
Figure 4: Tendency of negative gain rejection rate and GINI
during training 𝑤/or𝑤/𝑜the Gain Evaluation Strategy (GES).
LTV distribution. Our full model, ADSNet, significantly outper-
forms cross-domain methods that leverage external data across all
domain datasets, achieving absolute overall GINI and AUC improve-
ments of 1% to 5%. This substantiates the effectiveness of integrating
external data into the neural network. It is noteworthy that some
models experience a decrease in performance when introducing ex-
ternal data. For instance, Share-Bottom w/ZILN, compared to ZILN,
shows a decline in overall GINI by 0.4%. This can be attributed
to the substantial differences in data distribution across various
domains, as illustrated in Table 1. It demonstrates that conven-
tional multi-domain joint learning models fail to avoid the negative
transfer phenomenon. In contrast, our ADSNet model exhibits su-
periority by supporting the rejection of negative gain samples, thus
effectively mitigating this issue.
5.4 Ablation Study
To investigate the effectiveness and illustrate the impact of different
components of our proposed model, we conduct ablation studies
on four ablations built upon our full model as shown in Table 3.
In particular, (1) ADSNet w/o the Gain Evaluation Strategy (row 3)
exhibits a noticeable decrease in GINI, dropping from 0.856 to 0.824.
This suggests that the gain evaluation strategy plays a significant
role in rejecting noise samples and stressing negative transfer. (2)
The removal of the Domain Adaptation Module (row 2) results in
GINI dropping by an absolute 2.3%, indicating that constraining the
distribution between the gain network and the vanilla network is
beneficial to enhance the process of knowledge transfer. Further-
more, (3) the removal of the Iterative Alignment Strategy (row 4)
also leads to lower GINI, which indicates that this iterative process
is essential for refining the model’s predictions in an incremental
fashion, aligning the model more closely with the target domain.
Iterative alignment likely facilitates a more nuanced adaptation
that incrementally bridges the domain gap.
5.5 Effectiveness of Negative Gain Rejection
To evaluate the impact of our gain evaluation strategy within AD-
SNet, we conduct an in-depth analysis to understand the influence
of negative transfer during the learning process and to assess the
[0, 15] [15, 100] [100, 250] [250, +inf] Overall
Sample Intervals0.40.50.60.70.80.91.0GINI
0.47610.66700.75530.8462
0.7940
0.63170.81540.84610.9400 0.9467
0.69970.86710.88910.95440.9448
0.78320.89630.93770.9631 0.9570
ZILN
ADSNet-Backbone
STAR
ADSNet (full model)Figure 5: Comparison over the interval of sample size.
model’s ability to identify and reject negative gains. We define the
negative gain rejection rate, which represents the probability that
external data is rejected, i.e., the gain 𝑊𝐺<0. Figure 4 illustrates
the change in the negative gain sample rejection rate and GINI
with training steps. The X-axis represents the training steps, the
left Y-axis represents the rejection rate of negative gain samples,
and the right Y-axis represents GINI. Several observations can be
made from this: (1) The GINI exhibits nonlinear growth. It improves
rapidly in the early stages and then the rate of improvement slows
down over time. (2) The negative gain rejection rate is low in the
early stage and then increases with the progress of training. This
indicates that the model gradually learns to identify and distinguish
negative gain samples during the training process. Ultimately, the
negative gain rejection rate stabilizes at 0.64, suggesting that a sig-
nificant portion of the external samples may not be beneficial if
used directly. These observations highlight the effectiveness of the
gain evaluation strategy which ensures that only external data that
contribute positively to the target domain are utilized.
5.6 Effectiveness of Improving Long-tail
Prediction
In a practical advertising system, we are struggling with the chal-
lenge of data sparsity in the LTV estimation scenario. To further
understand the improvement achieved by our models, we conduct a
more detailed analysis. We quantify the sample sizes corresponding
to each advertisement and categorize them into intervals to exam-
ine the relationship between the model’s predictive capabilities and
the size of samples. The results are presented in Figure 5, which
demonstrates two insights. First, it intuitively shows that the perfor-
mance of the model is positively correlated with the sample size. As
the sample size increases, the performance of the model improves
accordingly. Moreover, compared to the ADSNet-Backbone that
does not utilize external data, our ADSNet gains significant im-
provements by∼15.2% GINI in the long tail interval [0,15], which
indicates that by introducing external data, the model’s ability to
predict long-tail advertisements is significantly enhanced.
5.7 Online A/B Test
In advertising systems, GMV is a critical outcome metric that cannot
be directly optimized. GMV can be represented as GMV = LTV /
ROI_bid. The ROI_bid is set by the advertiser and is generally fixed
5879ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
based on the required profit margin. Therefore, from a formulaic
perspective, GMV and LTV are highly correlated.
We conduct online A/B tests on the real advertising platform
of Tencent Ads. Specifically, we employ UV sampling to split the
traffic, allocating 5% of the traffic to the control group and another
5%to the experimental group. The base serving model is a variant of
STAR, adapted to our business characteristics. For the experimental
group, we deploy our ADSNet model. Our online evaluation metrics
are the LTV and Gross Merchandise Value (GMV). Due to company
privacy policies, we only report the relative improvement. The
online A/B test results indicate that ADSNet leads to an increase
in online LTV by 3.47% and GMV by 3.89% compared with the
base model. These results underscore the practical applicability and
effectiveness of ADSNet in the LTV prediction task, demonstrating
its potential to significantly enhance the performance of real-world
advertising systems.
6 CONCLUSION AND FUTURE WORK
We present the Adaptive Difference Siamese Network (ADSNet) to
address the challenges of data sparsity of LTV estimation and cross-
domain transfer learning in advertising systems. We incorporate
external data to expand the sample size. Our ADSNet utilizes cross-
domain transfer learning, a gain evaluation strategy, and a domain
adaptation module to learn beneficial information, reject noisy
samples, and bridge different domains. Extensive experiments and
online A/B tests have demonstrated the effectiveness of ADSNet in
improving performance and mitigating negative transfer, as well
as its ability to enhance long-tail prediction capabilities. In future
work, we will extend our approach to other related advertising tasks,
such as click-through rate prediction to evaluate the generalizability
of ADSNet in broader advertising scenarios.
ACKNOWLEDGMENTS
We gratefully acknowledge the contributions of the following: Piao
Yang, Ting Wang, Yucheng Hu, Chaoyue Zhao, Liwei Lin, Cong
Quan and Kun Bai. This work was partially supported by the Ten-
cent Rhinoceros Project.
REFERENCES
[1]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al .2021. On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 (2021).
[2]Sharad Borle, Siddharth S Singh, and Dipak C Jain. 2008. Customer lifetime value
measurement. Management science 54, 1 (2008), 100–112.
[3]Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. 2020. Rank consistent
ordinal regression for neural networks with application to age estimation. Pattern
Recognition Letters 140 (2020), 325–331.
[4] Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41–75.
[5]Benjamin Paul Chamberlain, Angelo Cardoso, CH Bryan Liu, Roberto Pagliari,
and Marc Peter Deisenroth. 2017. Customer lifetime value prediction using
embeddings. In Proceedings of the 23rd ACM SIGKDD international conference on
knowledge discovery and data mining. 1753–1762.
[6]Xingping Dong and Jianbing Shen. 2018. Triplet loss in siamese network for
object tracking. In Proceedings of the European conference on computer vision
(ECCV). 459–474.
[7]Lixin Duan, Dong Xu, and Ivor Tsang. 2012. Learning with augmented features
for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660 (2012).
[8]Eibe Frank and Mark Hall. 2001. A simple approach to ordinal classification. In
Machine Learning: ECML 2001: 12th European Conference on Machine Learning
Freiburg, Germany, September 5–7, 2001 Proceedings 12. Springer, 145–156.
[9]Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng
Tao. 2018. Deep ordinal regression network for monocular depth estimation.InProceedings of the IEEE conference on computer vision and pattern recognition.
2002–2011.
[10] Junyi Gao, Cao Xiao, Lucas M Glass, and Jimeng Sun. 2020. COMPOSE: Cross-
modal pseudo-siamese network for patient trial matching. In Proceedings of the
26th ACM SIGKDD international conference on knowledge discovery & data mining .
803–812.
[11] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[12] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and Song Wang. 2017.
Learning dynamic siamese network for visual object tracking. In Proceedings of
the IEEE international conference on computer vision. 1763–1771.
[13] Sunil Gupta, Dominique Hanssens, Bruce Hardie, Wiliam Kahn, V Kumar,
Nathaniel Lin, Nalini Ravishanker, and S Sriram. 2006. Modeling customer
lifetime value. Journal of service research 9, 2 (2006), 139–155.
[14] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.
Advances in neural information processing systems 29 (2016).
[15] Tongwen Huang, Qingyun She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet:
gating-enhanced deep network for click-through rate prediction. arXiv preprint
arXiv:2007.03519 (2020).
[16] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-
ture importance and bilinear feature interaction for click-through rate prediction.
InProceedings of the 13th ACM Conference on Recommender Systems. 169–177.
[17] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991.
Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79–87.
[18] Junguang Jiang, Baixu Chen, Junwei Pan, Ximei Wang, Dapeng Liu, Mingsheng
Long, et al .2023. ForkMerge: Mitigating Negative Transfer in Auxiliary-Task
Learning. In Thirty-seventh Conference on Neural Information Processing Systems.
[19] Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation
in NLP. ACL.
[20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,
and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.
arXiv preprint arXiv:1909.10351 (2019).
[21] Kunpeng Li, Guangcui Shao, Naijun Yang, Xiao Fang, and Yang Song. 2022. Billion-
user Customer Lifetime Value Prediction: An Industrial-scale Solution from
Kuaishou. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management. 3243–3251.
[22] Wen Li, Lixin Duan, Dong Xu, and Ivor W Tsang. 2013. Learning with augmented
features for supervised and semi-supervised heterogeneous domain adaptation.
IEEE Transactions on Pattern analysis and machine intelligence 36, 6 (2013), 1134–
1148.
[23] Zhongqi Lu, Erheng Zhong, Lili Zhao, Evan Wei Xiang, Weike Pan, and Qiang
Yang. 2013. Selective transfer learning for cross domain recommendation. In
Proceedings of the 2013 SIAM International Conference on Data Mining. SIAM,
641–649.
[24] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining. 1930–1939.
[25] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. 1137–1140.
[26] Brendan McMahan. 2011. Follow-the-regularized-leader and mirror descent:
Equivalence theorems and l1 regularization. In Proceedings of the Fourteenth
International Conference on Artificial Intelligence and Statistics. JMLR Workshop
and Conference Proceedings, 525–533.
[27] Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. 2016. Ordinal
regression with multiple output cnn for age estimation. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 4920–4928.
[28] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,
and Quan Lu. 2018. Field-weighted factorization machines for click-through
rate prediction in display advertising. In Proceedings of the 2018 World Wide Web
Conference. 1349–1357.
[29] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345–1359.
[30] Michael Pulis and Josef Bajada. 2021. Siamese Neural Networks for Content-based
Cold-Start Music Recommendation.. In Proceedings of the 15th ACM Conference
on Recommender Systems. 719–723.
[31] Kandula Balagangadhar Reddy, Debabrata Swain, Samiksha Shukla, and Lija
Jacob. 2022. Prediction of Customer Lifetime Value Using Machine Learning. In
Proceedings of Second Doctoral Symposium on Computational Intelligence: DoSCI
2021. Springer, 271–278.
[32] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang
Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al .2021. One model to
serve all: Star topology adaptive recommender for multi-domain ctr prediction. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management. 4104–4113.
5880KDD ’24, August 25–29, 2024, Barcelona, Spain Ruize Wang et al.
[33] Xintong Shi, Wenzhi Cao, and Sebastian Raschka. 2023. Deep neural networks
for rank-consistent ordinal regression based on conditional probabilities. Pattern
Analysis and Applications 26, 3 (2023), 941–955.
[34] Pawel Swietojanski and Steve Renals. 2014. Learning hidden unit contributions
for unsupervised speaker adaptation of neural network acoustic models. In 2014
IEEE Spoken Language Technology Workshop (SLT). IEEE, 171–176.
[35] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive
layered extraction (ple): A novel multi-task learning (mtl) model for personalized
recommendations. In Proceedings of the 14th ACM Conference on Recommender
Systems. 269–278.
[36] Ali Vanderveld, Addhyan Pandey, Angela Han, and Rajesh Parekh. 2016. An
engagement-based customer lifetime value system for e-commerce. In Proceedings
of the 22nd ACM SIGKDD international conference on knowledge discovery and
data mining. 293–302.
[37] VN Vapnik. 2000. The Nature of Statistical Learning Theory (Information Science
and Statistics) Springer-Verlag. New York (2000).
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[39] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobb-
hahn, and Anson Ho. 2022. Will we run out of data? An analysis of the limits of
scaling datasets in Machine Learning. arXiv preprint arXiv:2211.04325 (2022).
[40] Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao
Yu, Ruopeng Li, and Wei Chu. 2022. Escm2: Entire space counterfactual multi-
task model for post-click conversion rate estimation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 363–372.
[41] Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, and Zhiqi Shen. 2017.
Balanced distribution adaptation for transfer learning. In 2017 IEEE international
conference on data mining (ICDM). IEEE, 1129–1134.
[42] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDD’17. 1–7.
[43] Xin Wang, Wenhu Chen, Yuan-Fang Wang, and William Yang Wang. 2018. No
metrics are perfect: Adversarial reward learning for visual storytelling. arXiv
preprint arXiv:1804.09160 (2018).
[44] Xiaojing Wang, Tianqi Liu, and Jingang Miao. 2019. A deep probabilistic model
for customer lifetime value prediction. arXiv preprint arXiv:1912.07753 (2019).
[45] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and
Yang Tang. 2023. A brief overview of ChatGPT: The history, status quo andpotential future development. IEEE/CAA Journal of Automatica Sinica 10, 5
(2023), 1122–1136.
[46] Min Xiao and Yuhong Guo. 2014. Feature space independent semi-supervised
domain adaptation via kernel matching. IEEE transactions on pattern analysis
and machine intelligence 37, 1 (2014), 54–66.
[47] Mingzhe Xing, Shuqing Bian, Wayne Xin Zhao, Zhen Xiao, Xinji Luo, Cunxiang
Yin, Jing Cai, and Yancheng He. 2021. Learning Reliable User Representations
from Volatile and Sparse Data to Accurately Predict Customer Lifetime Value. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining. 3806–3816.
[48] Xuejiao Yang, Binfeng Jia, Shuangyang Wang, and Shijie Zhang. 2023. Feature
Missing-aware Routing-and-Fusion Network for Customer Lifetime Value Predic-
tion in Advertising. In Proceedings of the Sixteenth ACM International Conference
on Web Search and Data Mining. 1030–1038.
[49] Yi Yao and Gianfranco Doretto. 2010. Boosting for transfer learning with multiple
sources. In 2010 IEEE computer society conference on computer vision and pattern
recognition. IEEE, 1855–1862.
[50] Xianhua Zeng, Xinyu Wang, and Yicai Xie. 2023. Multiple Pseudo-Siamese
Network with Supervised Contrast Learning for Medical Multi-modal Retrieval.
ACM Transactions on Multimedia Computing, Communications and Applications
(2023).
[51] Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, and Shuangyang Wang. 2023.
Out of the Box Thinking: Improving Customer Lifetime Value Modelling via
Expert Routing and Game Whale Detection. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management. 3206–3215.
[52] Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, and Dong Wang. 2023. A
Collaborative Transfer Learning Framework for Cross-domain Recommendation.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 5576–5585.
[53] Jiejie Zhao, Bowen Du, Leilei Sun, Fuzhen Zhuang, Weifeng Lv, and Hui Xiong.
2019. Multiple relational attention network for multi-task learning. In Proceedings
of the 25th ACM SIGKDD international conference on knowledge discovery & Data
Mining. 1123–1131.
[54] Shuai Zheng and James T Kwok. 2017. Follow the moving leader in deep learning.
InInternational Conference on Machine Learning. PMLR, 4110–4119.
[55] Ruixi Zhu, Li Yan, Nan Mo, and Yi Liu. 2019. Semi-supervised center-based
discriminative adversarial learning for cross-domain scene-level land-cover clas-
sification of aerial images. ISPRS journal of photogrammetry and remote sensing
155 (2019), 72–89.
5881