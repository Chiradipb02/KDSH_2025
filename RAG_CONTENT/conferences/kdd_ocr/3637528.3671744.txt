IDEA: A Flexible Framework of Certified Unlearning for
Graph Neural Networks
Yushun Dong
The University of Virginia
Charlottesville, USA
yd6eb@virginia.eduBinchi Zhang
The University of Virginia
Charlottesville, USA
epb6gw@virginia.eduZhenyu Lei
The University of Virginia
Charlottesville, USA
vjd5zr@virginia.edu
Na Zou
The University of Houston
Houston, USA
nzou2@uh.eduJundong Li
The University of Virginia
Charlottesville, USA
jundong@virginia.edu
ABSTRACT
Graph Neural Networks (GNNs) have been increasingly deployed in
a plethora of applications. However, the graph data used for training
may contain sensitive personal information of the involved indi-
viduals. Once trained, GNNs typically encode such information in
their learnable parameters. As a consequence, privacy leakage may
happen when the trained GNNs are deployed and exposed to poten-
tial attackers. Facing such a threat, machine unlearning for GNNs
has become an emerging technique that aims to remove certain
personal information from a trained GNN. Among these techniques,
certified unlearning stands out, as it provides a solid theoretical
guarantee of the information removal effectiveness. Nevertheless,
most of the existing certified unlearning methods for GNNs are
only designed to handle node and edge unlearning requests. Mean-
while, these approaches are usually tailored for either a specific
design of GNN or a specially designed training objective. These
disadvantages significantly jeopardize their flexibility. In this pa-
per, we propose a principled framework named IDEA to achieve
flexible and certified unlearning for GNNs. Specifically, we first
instantiate four types of unlearning requests on graphs, and then
we propose an approximation approach to flexibly handle these
unlearning requests over diverse GNNs. We further provide theo-
retical guarantee of the effectiveness for the proposed approach as
a certification. Different from existing alternatives, IDEA is not de-
signed for any specific GNNs or optimization objectives to perform
certified unlearning, and thus can be easily generalized. Extensive
experiments on real-world datasets demonstrate the superiority of
IDEA in multiple key perspectives.
CCS CONCEPTS
•Computing methodologies →Machine learning.
KEYWORDS
Machine Unlearning, Graph Neural Networks, Privacy
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671744ACM Reference Format:
Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, and Jundong Li. 2024.
IDEA: A Flexible Framework of Certified Unlearning for Graph Neural
Networks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671744
1 INTRODUCTION
Graph-structured data is ubiquitous among various real-world appli-
cations, such as online social platform [ 19], finance system [ 50], and
chemical discovery [ 21]. In recent years, Graph Neural Networks
(GNNs) have exhibited promising performance in various graph-
based downstream tasks [ 19,62,65,76]. The success of GNNs is
mainly attributed to its message-passing mechanism, which enables
each node to take advantage of the information from its multi-hop
neighbors [ 19,25]. Therefore, GNNs have been widely adopted in
a plethora of realms [10, 14, 54, 61, 74, 77].
Despite the success of GNNs, their widespread usage has also
raised social concerns about the issue of privacy protection [ 7,
59,75]. It is worth noting that, in practice, the graph data used for
training may contain sensitive personal information of the involved
individuals [ 35,56,59]. Once trained, these GNNs typically encode
such personal information in the learnable parameters. As a conse-
quence, privacy leakage may happen when the trained GNNs are
deployed and exposed to potential attackers [35, 56]. For example,
the similarity of the health records between patients could pro-
vide key information for disease diagnosis [ 71]. Therefore, GNNs
can be trained on patient networks for disease prediction, where
the connections between patients indicate high similarity scores
of their health records. However, malicious attackers can easily
reveal the patients’ health records that are used for training via
membership inference attack [ 35], which severely threatens pri-
vacy. Facing such a threat of privacy leakage, legislation such as the
General Data Protection Regulation (GDPR) (GDPR 2016) [ 40], the
California Consumer Privacy Act (CCPA) (CCPA 2018) [ 37], and
the Personal Information Protection and Electronic Documents Act
(PIPEDA 2000) [ 1] have emphasized the importance of the right to
be forgotten [27]. Specifically, users should have the right to request
the deletion of their personal information from those learning mod-
els that encode it. Such an urgent need poses challenges towards
removing certain personal information from the trained GNNs.
 
621
KDD ’24, August 25–29, 2024, Barcelona, Spain Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, & Jundong Li
The need for information removal from these trained models has
led to the development of machine unlearning [2,63]. Specifically,
the ultimate goal of machine unlearning is to remove information
regarding certain training data from a previously trained model. A
straightforward approach is to perform model re-training. However,
on the one hand, the model owner may not have full access to the
training data; on the other hand, re-training can be prohibitively ex-
pensive even if training data is fully accessible [ 13]. To achieve more
efficient information removal, a series of existing works [ 2,3,22]
proposed to directly modify the parameters of the trained models.
Nevertheless, most of these works only achieve unlearning empiri-
cally and fail to provide any theoretical guarantee. This problem has
led to the emerging of certified unlearning [ 16,44], which aims to
develop unlearning approaches with theoretical guarantee on their
effectiveness. In the domain of graph learning, a few recent works,
such as [ 7,59], have explored to achieve certified unlearning for
GNNs. However, a major limitation of these approaches is their low
flexibility. First, most approaches are designed to completely un-
learn a given set of nodes or edges, while this may not comply with
certain unlearning needs in real-world applications. For example,
on a social network platform, a user may decide to stop disclosing
certain personal information to the GNN-based friend recommen-
dation model but continue using the platform. In such a case, the
attribute information of this user should then be partially removed
from the GNN model, which protects the user’s privacy and main-
tains algorithmic personalization as well. Therefore, it is desired
to develop flexible certified unlearning approaches for GNNs to
handle unlearning requests centered on node attributes. Second,
existing certified unlearning approaches are mostly designed for
a specific type of GNNs [ 7] or the GNNs trained following a spe-
cially designed objective function [ 7,59]. However, various GNNs
and objectives have been adopted for diverse real-world applica-
tions, and thus it is also desired to develop more flexible certified
unlearning approaches for different GNNs trained with different
objectives. Nevertheless, existing exploration in developing flexible
and certified unlearning approaches for GNNs remains nascent.
In this paper, we study a novel and critical problem of developing
a certifiable unlearning framework that can flexibly unlearn per-
sonal information in graphs and generalize across GNNs. We note
that this is a non-trivial task. In essence, we mainly face three chal-
lenges. (i) Characterizing node dependencies. Different from tabular
data, the nodes in graph data usually have dependencies with each
other. Properly characterizing node dependencies thus becomes
the first challenge to achieve unlearning for GNNs. (ii) Achieving
flexible unlearning. Unlearning requests may be initiated towards
nodes, node attributes (partial or full), and edges. Meanwhile, vari-
ous GNNs have been adopted for different applications, and most
of these GNNs have different model structures and optimization ob-
jectives. Therefore, achieving flexible unlearning for different types
of unlearning requests, GNN structures, and objectives becomes
the second challenge. (iii) Obtaining certification for unlearning. To
reduce the risk of privacy leakage, it is critical for the model owner
to ensure that the information needed to be removed has been com-
pletely wiped out before model deployment. However, GNNs may
have complex structures, and it is difficult to examine whether cer-
tain sensitive personal information remains being encoded or not.
Meanwhile, certified unlearning for GNNs usually requires strictconditions (e.g., assuming that GNNs are trained under a specially
designed objective [ 7,59]) and thus sacrifices flexibility. Properly
certifying the effectiveness of unlearning is our third challenge.
Our Contributions. We propose IDEA (flex Ible an DcErtified
unleArning), which is a flexible framework of certified unlearning
for GNNs. Specifically, to tackle the first two challenges, we propose
to model the intermediate state between the optimization objectives
with and without the instances (e.g., nodes, edges, and attributes) to
be unlearned. Meanwhile, four different types of common unlearn-
ing requests are instantiated, and GNN parameters after unlearning
can be efficiently approximated with flexible unlearning request
specifications. To tackle the third challenge, we propose a novel
theoretical certification on the unlearning effectiveness of IDEA.
We show that our certification method brings an empirically tighter
bound on the distance between the approximated and actual GNN
parameters compared to other existing alternatives. We summarize
our contributions as: (1) Problem Formulation. We formulate
and make an initial investigation on a novel research problem of
flexible and certified unlearning for GNNs. (2) Algorithm Design.
We propose IDEA, a flexible framework of certified unlearning
for GNNs without relying on any specific GNN structures or any
specially designed objective functions, which shows significant
value for practical use. (3) Experimental Evaluation. We conduct
comprehensive experiments on real-world datasets to verify the
superiority of IDEA over existing alternatives in multiple key per-
spectives, including bound tightness, unlearning efficiency, model
utility, and unlearning effectiveness.
2 PRELIMINARIES
2.1 Notations
We use bold uppercase letters (e.g., 𝑨), bold lowercase letters (e.g.,
𝒙), and normal lowercase letters (e.g., 𝑛) to denote matrices, vectors,
and scalars, respectively. We represent an attributed graph as G=
{V,E,X}. HereV={𝑣1,...,𝑣𝑛}denotes the set of nodes, where
𝑛is the total number of nodes. E ⊂V×V represents the set
of edges.X={𝑥1,1,...,𝑥𝑛,𝑐}is the set of node attribute values,
where𝑐is the total number of node attribute dimensions, and
𝑥𝑖,𝑗represents the attribute value of node 𝑣𝑖at the𝑗-th attribute
dimension (1≤𝑖≤𝑛,1≤𝑗≤𝑐). We utilize 𝑓𝜽to represent a GNN
model parameterized by the learnable parameters in 𝜽.
In this paper, we focus on the commonly studied node classifica-
tion task, which widely exists in real-world applications. Specifi-
cally, we are given the labels of a set of training nodes Vtrn(Vtrn⊂
V) asYtrn. HereYtrn={𝑌1,...,𝑌𝑚}, where𝑌𝑖∈{1,...,𝑐}(1≤𝑖≤
𝑚) is the node label of 𝑣𝑖;𝑐is the total number of possible classes;
and𝑚represents the number of training nodes, i.e., 𝑚=|Vtrn|. Our
goal here is to optimize the parameter 𝜽of the GNN model 𝑓with𝑘
message-passing layers as 𝜽∗w.r.t. certain objective function over
Vtrn, such that𝑓𝜽∗is able to achieve accurate predictions for the
nodes in the test set Vtst(Vtst∩V trn=∅).
2.2 Problem Statement
In this subsection, we formally present the problem formulation of
Flexible and Certified Unlearning for GNNs. We first elaborate on the
mathematical formulation of certified unlearning for GNNs. Specif-
ically, certified unlearning requires that the unlearning strategy
 
622IDEA: A Flexible Framework of Certified Unlearning for
Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
have a theoretical guarantee of unlearning effectiveness. We adopt
a commonly used criterion for the effectiveness of unlearning, i.e.,
(𝜀−𝛿)Certified Unlearning. Here 𝜀and𝛿are two parameters con-
trolling the relaxation of such a criterion. We present the definition
of(𝜀−𝛿)certified unlearning for GNNs below.
Definition 1.(𝜀−𝛿)Certified Unlearning for GNNs. LetH
be the hypothesis space of a GNN model parameters and Abe the
associated optimization process. Given a graph Gfor GNN optimiza-
tion and a ΔGthat characterizes the information to be unlearned, U
is an (𝜀−𝛿) certified unlearning process iff ∀T⊆H , we have
Pr(U(G,ΔG,A(G))∈T)≤𝑒𝜀Pr(A(G⊖ΔG)∈T)+𝛿, and
Pr(A(G⊖ΔG)∈T)≤𝑒𝜀Pr(U(G,ΔG,A(G))∈T)+𝛿,
whereG⊖ΔGrepresents the graph data with the information char-
acterized by ΔGbeing removed.
The intuition of Definition 1 is that, once the two inequalities
above are satisfied, the difference between the distribution of the
unlearned GNN parameters and that of the re-trained GNN param-
eters overG⊖ΔGis bounded by a small threshold 𝜀and relaxed
by a probability 𝛿. We note that, different from most existing litera-
ture on GNN unlearning, the information to be unlearned does not
necessarily come from a node or an edge in Definition 1. Such an
extension paves the way towards more flexible certified unlearning
for GNNs. We now formally present the problem formulation of
Flexible and Certified Unlearning for GNNs below.
Problem 1. Flexible and Certified Unlearning for GNNs.
Given a GNN model 𝑓𝜽∗optimized overGand any request to un-
learn information characterized by ΔG, our goal is to achieve (𝜀−𝛿)
certified unlearning over 𝑓𝜽∗.
3 UNLEARNING REQUEST INSTANTIATIONS
We instantiate the unlearning requests characterized by ΔG, namely
Node Unlearning Request, Edge Unlearning Request, and Attribute
Unlearning Request. We present an illustration in Figure 1.
Node Unlearning Request. The most common unlearning re-
quest in GNN applications is to unlearn a given set of nodes. For
example, in a social network platform, a GNN model can be trained
on the friendship network formed by the platform users to perform
friendship recommendation. When a user has decided to quit such
a platform and withdrawn the consent of using her private data,
this user may request to unlearn the node associated with her from
the social network. In such a case, the information to be unlearned
is characterized by ΔG={ΔV,𝜅𝑒(ΔV),𝜅𝑥(ΔV)}. Here𝜅𝑒and𝜅𝑥
return the set of the direct edges and node attributes associated
with nodes in ΔV, respectively.
Edge Unlearning Request. In addition to the information encoded
by the nodes, edges can also encode critical private information and
may need to be unlearned as well. In fact, it has been empirically
proved that malicious attackers can easily infer the edges used for
training, which directly threatens privacy [20]. In such a case, the
information to be unlearned is characterized by ΔG={∅,ΔE,∅}.
Attribute Unlearning Request. Both requests above fail to repre-
sent cases where only node attributes are requested to be unlearned.
Here we show two common node attribute unlearning requests.
(1) Full Attribute Unlearning. In this case, all information regarding
Training GraphNode Unlearning
Attribute (Partial)Attribute (Full)
Edge UnlearningFigure 1: An illustration of common unlearning requests.
the attributes of a set of nodes is requested to be unlearned. For
example, a social network platform user may withdraw the consent
for the GNN-based friend recommendation algorithm to encode any
of its attributes during training. In such a case, the information to
be unlearned is characterized by ΔG={∅,∅,ΔX}, where for node
𝑣𝑖, if𝑥𝑖,𝑗∈ΔX, then∀𝑗∈{1,...,𝑐},𝑥𝑖,𝑗∈ΔX.(2) Partial Attribute
Unlearning. The attributes of a node may also be requested to be
partially unlearned. For example, in a social network, a user may
withdraw the consent of using the information regarding certain at-
tribute(s) due to various reasons, e.g., feeling being unfairly treated.
However, this user may still continue using such a platform, and
thus other attributes should not be unlearned to ensure satisfying
personalized service quality. In such a case, the information to be
unlearned is characterized by ΔG={∅,∅,ΔX}, where for node
𝑣𝑖, if𝑥𝑖,𝑗∈ΔX, then∃𝑗∈{1,...,𝑐},𝑥𝑖,𝑗∉ΔX. Note that the two
types of attribute unlearning can be requested together. Hence, we
utilize ΔXto characterize a mixture of both types of attributes.
Based on the instantiations above, we denote ΔG={ΔV,ΔE∪
𝜅𝑒(ΔV),ΔX∪𝜅𝑥(ΔV)} as a potential combination of all types of
unlearning requests. Accordingly, we formally define G⊖ΔG=
{V\ΔV,E\ΔE\𝜅𝑒(ΔV),X\ΔX\𝜅𝑥(ΔV)}.
4 METHODOLOGY
In this section, we present our proposed framework IDEA, which
aims to achieve flexible and certified unlearning for GNNs. We first
present the general formulation of flexible unlearning for GNNs.
Then, we introduce a unified modeling integrating different instanti-
ations of unlearning requests. We finally propose a novel theoretical
guarantee on the effectiveness of IDEA as the certification.
4.1 Flexible Unlearning for GNNs
We first present a unified formulation of flexible unlearning for
GNNs. In general, our rationale here is to design a framework to di-
rectly approximate the change in the (optimal) learnable parameter
𝜽∗during unlearning. Specifically, we first review the training pro-
cess of a given GNN model 𝑓over graph dataG. Then, we consider
the training objective with information of ΔGbeing removed as a
perturbed training objective over G. We are now able to analyze
how the optimal learnable parameter 𝜽∗would change when the
objective function is modified. Note that we adopt a generalized
formulation of such modification over the objective function, such
that our analysis can be adapted to different unlearning requests.
In a typical training process of a given GNN model 𝑓over graph
dataG, the optimal learnable parameter 𝜽∗is obtained via solving
 
623KDD ’24, August 25–29, 2024, Barcelona, Spain Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, & Jundong Li
the optimization problem of
arg min
𝜽1
𝑚∑︁
𝑣𝑖∈V trnL(𝜽,𝑣𝑖,G), (1)
where a typical choice of Lis cross-entropy loss in node classifica-
tion tasks. Here we consider that the computation of Lalso relies
on other necessary information such as ˆ𝑌𝑖by default and omit them
for simplicity. As a comparison, the optimal learnable parameter
trained overG⊖ΔG, which we denoted as ˜𝜽∗, is obtained via
solving the problem of
arg min
𝜽1
𝑚−|ΔV|∑︁
𝑣𝑖∈V trn\ΔVL(𝜽,𝑣𝑖,G⊖ΔG). (2)
To study how the optimal parameters change when transforming
from Equation (1) to Equation (2), it is necessary to analyze how
the objective function and optimal solution change between the
two cases. To systematically compare Equation (1) and Equation (2),
here we define 𝜙𝑘(·)as a function that takes a node and a graph as
its input and outputs the set of nodes in the computation graph of
the input node (excluding the input node itself). Here a computation
graph is a subgraph centered on a given node with neighboring
nodes up to𝑘hops away, where 𝑘is the layer number of the studied
GNN. Then we have the following proposition.
Proposition 1. Localized Equivalence of Training Nodes.
Given ΔG={ΔV,ΔE,ΔX}to be unlearned and an objective L
computed over 𝑓𝜽,L(𝜽,𝑣𝑖,G)=L(𝜽,𝑣𝑖,G⊖ΔG)holds∀𝑣𝑖∉
𝜙𝑘(𝑣𝑗)∪{𝑣𝑗},𝑣𝑗∈ΔV∪𝛾𝑒(ΔE)∪𝛾𝑥(ΔX). Here𝛾𝑒and𝛾𝑥return
the set of nodes that directly connect to the edges in Eand that have
associated attribute(s) in X, respectively.
The intuition of Proposition 1 is that, under a given 𝑓𝜽, the value
ofLmaintains the same between Equation (1) and Equation (2) for
those training nodes that are not topologically close to the instances
(i.e., nodes, attributes, and edges) in ΔG. To bridge Equation (1)
and Equation (2), we then propose a formulation to characterize
the intermediate state. Inspired by a series of previous works such
as [58, 59], we add an additional term by defining 𝜽∗
ΔG,𝜉with
𝜽∗
ΔG,𝜉Barg min
𝜽1
𝑚∑︁
𝑣𝑖∈V trnL(𝜽,𝑣𝑖,G)+𝜉(Ladd−Lsub).(3)
We then introduce the modeling of LaddandLsub. Specifically, we
propose to formulate Laddwith
Ladd=𝛼1∑︁
𝑣𝑖∈V 1L(𝜽,𝑣𝑖,G⊖ΔG)+𝛼2∑︁
𝑣𝑖∈V 2L(𝜽,𝑣𝑖,G⊖ΔG)
+𝛼3∑︁
𝑣𝑖∈V 3L(𝜽,𝑣𝑖,G⊖ΔG)+𝛼4∑︁
𝑣𝑖∈V 4L(𝜽,𝑣𝑖,G⊖ΔG).(4)
Here𝛼1,𝛼2,𝛼3,𝛼4∈{0,1}are used to flag whether the requests of
node unlearning, full attribute unlearning, partial node attribute
unlearning, and edge unlearning exist or not, respectively. We now
introduceV1,V2,V3, andV4. Specifically,V1represents the set of
training nodes whose computation graph includes those nodes to be
unlearned. We denote the sets of nodes associated with ΔXwhen
their unlearned attributes are replaced with any non-informative
numbers (e.g., 0) asV(Full)
𝑥 andV(Partial)
𝑥 for full and partial attribute
unlearning, respectively. V2andV3include training nodes whosecomputation graph includes attributes to be unlearned fully and
partially plus the nodes in V(Full)
𝑥 andV(Partial)
𝑥 , respectively;V4is
the set of nodes whose computation graph includes those edges to
be unlearned. Mathematically, we formulate V1,V2,V3, andV4as
V1=∪𝑣𝑖∈ΔV(𝜙𝑘(𝑣𝑖)∩V trn), (5)
V2=V(Full)
𝑥∪{𝑣𝑖:𝑣𝑖∈𝜙𝑘(𝑣𝑗)∩V trn, 𝑣𝑗∈V(Full)
𝑥}, (6)
V3=V(Partial)
𝑥∪{𝑣𝑖:𝑣𝑖∈𝜙𝑘(𝑣𝑗)∩V trn, 𝑣𝑗∈V(Partial)
𝑥},(7)
V4=∪𝑣𝑖∈𝛾𝑒(ΔE)(𝜙𝑘(𝑣𝑖)∩V trn) (8)
We then formulate Lsubas
Lsub=𝛼1∑︁
𝑣𝑖∈˜V1L(𝜽,𝑣𝑖,G)+𝛼2∑︁
𝑣𝑖∈˜V2L(𝜽,𝑣𝑖,G)
+𝛼3∑︁
𝑣𝑖∈˜V3L(𝜽,𝑣𝑖,G)+𝛼4∑︁
𝑣𝑖∈˜V4L(𝜽,𝑣𝑖,G), (9)
where ˜V1includes all nodes in ΔVand the training nodes within
𝑘hops away from the nodes in ΔV; We denote the sets of nodes
associated with ΔXwith their vanilla attributes as ˜V(Full)
𝑥 and
˜V(Partial)
𝑥 for full and partial attribute unlearning, respectively. ˜V2
and ˜V3include training nodes whose computation graph includes
attributes to be unlearned fully and partially plus the nodes in
˜V(Full)
𝑥 and ˜V(Partial)
𝑥 , respectively; ˜V4is the set of nodes whose
computation graph includes those edges to be unlearned, i.e., ˜V4=
V4. Mathematically, we have
˜V1=∪𝑣𝑖∈ΔV(𝜙𝑘(𝑣𝑖)∩V trn)∪ΔV, (10)
˜V2=˜V(Full)
𝑥∪{𝑣𝑖:𝑣𝑖∈𝜙𝑘(𝑣𝑗)∩V trn, 𝑣𝑗∈˜V(Full)
𝑥}, (11)
˜V3=˜V(Partial)
𝑥∪{𝑣𝑖:𝑣𝑖∈𝜙𝑘(𝑣𝑗)∩V trn, 𝑣𝑗∈˜V(Partial)
𝑥},(12)
˜V4=V4. (13)
We then have the complete formulation of Equation (3) given Equa-
tion (4) to (13). Based on the modeling above, we have the optimal
equivalence between Equation (3) and Equation (2) below.
Lemma 1. Optimal Equivalence. The optimal solution to Equa-
tion (3) (denoted as 𝜽∗
ΔG,𝜉) equals to the optimal solution to Equation
(2) (denoted as ˜𝜽∗) when𝜉=1
𝑚.
Now we have successfully bridged the gap between Equation
(1) and Equation (2) by modeling their intermediate states with
Equation (3). More importantly, Lemma 1 paves the way towards
directly approximating ˜𝜽∗based on 𝜽∗by giving Theorem 1 below.
Theorem 1. Approximation with Infinitesimal Residual.
Given a graph data G,ΔG={ΔV,ΔE,ΔX}to be unlearned, and
an objective Lcomputed over an 𝑓𝜽∗, using 𝜽∗+1
𝑚Δ¯𝜽∗as an ap-
proximation of ˜𝜽∗only brings a first-order infinitesimal residual
w.r.t.∥𝜽∗−˜𝜽∗∥2, where Δ¯𝜽∗=−𝑯−1
𝜽∗(∇𝜽Ladd−∇𝜽Lsub), and
𝑯𝜽∗B∇2
𝜽1
𝑚Í
𝑣𝑖∈V trnL(𝜽,𝑣𝑖,G).
We note that the approximation strategy above relies on the
assumption that∀V𝑖∩V𝑗=∅and∀˜V𝑖∩˜V𝑗=∅for𝑖,𝑗∈
{1,2,3,4}when𝑖≠𝑗. However, it can also handle cases where such
an assumption does not hold. We show this in Proposition 2.
 
624IDEA: A Flexible Framework of Certified Unlearning for
Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
/uni03B8*˜/uni03B8*¯/uni03B8*Vanilla OptimalIdeal Unlearning OptimalApproximatedUnlearning Optimal/uni2225˜/uni03B8*−¯/uni03B8*/uni22252/uni2225/uni03B8*−˜/uni03B8*/uni22252(Bounded)(Bounded)1m/uni2225/uni0394¯/uni03B8*/uni22252
Figure 2: Distances between 𝜽∗,˜𝜽∗, and ¯𝜽∗. Here, 𝜽∗denotes
the optimal parameter before unlearning; ˜𝜽∗is the ideal op-
timal parameter after unlearning, which is obtained via re-
training; ¯𝜽∗is an approximation of ˜𝜽∗give by Theorem 1.
Proposition 2. Serializability of Approximation. Any mix-
ture of unlearning request instantiations can be split into multiple
sets of unlearning requests, where each set of unlearning requests
satisfies∀V𝑖∩V𝑗=∅and∀˜V𝑖∩˜V𝑗=∅for𝑖,𝑗∈{1,2,3,4}when
𝑖≠𝑗. Serially performing approximation following these request sets
achieves upper-bounded error.
Unlearning in Practice. The approximation approach given by
Theorem 1 requires computing the inverse matrix of the Hessian
matrix, which usually leads to high computational costs. Here we
propose to utilize the stochastic estimation method [ 8] to perform
estimation based on an iterative approach, which reduces the time
complexity to 𝑂(𝑡𝑝). Here𝑡is the total number of iterations adopted
by the stochastic estimation method, and 𝑝represents the total
number of learnable parameters in 𝜽.
4.2 Unlearning Certification
In this subsection, we introduce a novel certification based on The-
orem 1. According to the unlearning process given by Definition 1,
our goal is to achieve guaranteed closeness between ˜𝜽∗(i.e., the
ideal unlearned parameter derived from Equation (2)) and the ap-
proximation of such a parameter (denoted as ¯𝜽∗). In this way, we
are able to achieve certifiable unlearning effectiveness.
Although certified unlearning for GNNs is studied by some re-
cent explorations [ 7,59], these approaches can only be applied
when the studied GNN model is trained following a specially modi-
fied objective. In particular, such a modification requires adding an
additional regularization term of 𝜽scaled by a random vector onto
the objective, which is specially designed for certification purposes.
However, most GNNs are optimized following common objectives
(e.g., cross-entropy loss) instead of such a modified objective. There-
fore, these certified unlearning approaches cannot be flexibly used
across different GNNs in real-world applications. Here we aim to
develop a certified unlearning approach based on Theorem 1, such
that it is not tailored for any optimization objective and thus can
be easily generalized across various GNNs. Towards this goal, we
first review the ℓ2distances between 𝜽∗,˜𝜽∗, and ¯𝜽∗. We present
an illustration in Figure 2. It is difficult to directly analyze the ℓ2
distance between ˜𝜽∗and ¯𝜽∗. We thus start by analyzing the ℓ2dis-
tance between 𝜽∗and ˜𝜽∗. We found that the ℓ2distance between
𝜽∗and ˜𝜽∗is upper bounded under common assumptions, whichare widely adopted in other existing works tackling unlearning
problems [7, 17, 59]. We first present these assumptions below.
Assumption 1. For the training objective of a given GNN model,
we have: (1) The loss values of optimal points are bounded: |L(𝜽∗)|≤
𝐶and|L(˜𝜽∗)|≤𝐶; (2) The loss function Lis𝐿-Lipschitz continu-
ous; (3) The loss function Lis𝜆-strongly convex.
Based on Assumption 1, we now present the bound between 𝜽∗
and ˜𝜽∗in Theorem 2.
Theorem 2. Distance Bound in Optimals. Theℓ2distance
bound between ˜𝜽∗and𝜽∗is given by
∥˜𝜽∗−𝜽∗∥2≤𝐿|ΔV|+√︃
4𝑚𝜆𝐶|˜V|+𝐿2|ΔV|2
𝑚𝜆. (14)
DenoteV(F+P)
𝑥=V(Full)
𝑥∪V(Partial)
𝑥 , and ˜Vis given by
˜V=V1∪V 4∪{𝑣𝑖:𝑣𝑖∈𝜙𝑘(𝑣𝑗)∩V trn, 𝑣𝑗∈V(F+P)
𝑥}.(15)
Here the rationale of ˜Vis to describe the set of nodes whose
computation graphs involve any instance (i.e., nodes, attributes,
and edges) to be unlearned. Noticing the relationship between 𝜽∗,
˜𝜽∗, and ¯𝜽∗give by Figure 2, we further show the bound between
˜𝜽∗and ¯𝜽∗in Proposition 3.
Proposition 3. Distance Bound in Approximation. Theℓ2
distance bound between ˜𝜽∗and ¯𝜽∗is given by
∥˜𝜽∗−¯𝜽∗∥2≤𝜆∥Δ¯𝜽∗∥2+𝐿|ΔV|+√︃
4𝑚𝜆𝐶|˜V|+𝐿2|ΔV|2
𝑚𝜆.(16)
The rationale of Proposition 3 is to characterize the maximum ℓ2
distance between the ideal unlearning optimal and the approxima-
tion of unlearning optimal given by Theorem 1. Finally, based on
Proposition 3, we are able to present the certification in Theorem 3.
Theorem 3. Let𝜽∗=A(G)be the empirical minimizer over G,
˜𝜽∗=A(G⊖ΔG)be the empirical minimizer over G⊖ΔGand ¯𝜽∗
be an approximation of ˜𝜽∗. Define𝜁as an upper bound of ∥˜𝜽∗−¯𝜽∗∥2.
We haveU(G,ΔG,A(G))=¯𝜽∗+𝒃is an (𝜀−𝛿) certified unlearning
process, where 𝒃∼N( 0,𝜎2𝑰)and𝜎≥𝜁
𝜀√︁
2ln(1.25/𝛿).
Therefore, according to Theorem 3, we are able to achieve cer-
tified unlearning by adding zero-mean Gaussian noise over the
approximation derived from Theorem 1.
5 EXPERIMENTAL EVALUATIONS
We empirically evaluate the performance of IDEA in this section.
In particular, we aim to answer the following research questions.
RQ1: How tight can IDEA bound the ℓ2distance between the ideal
optimal ˜𝜽∗and the approximation ¯𝜽∗?RQ2: How well can IDEA
improve the efficiency of unlearning compared with re-training and
other alternatives? RQ3: How well can IDEA maintain the utility
of the original GNN model? RQ4: How well can IDEA unlearn the
information requested to be removed from the GNN?
 
625KDD ’24, August 25–29, 2024, Barcelona, Spain Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, & Jundong Li
1% 2% 3% 4% 5% 6% 7% 8% 9%10%
Unlearn Ratio101102Value of/lscript2
CEU Worst CEU Data Depdent IDEA Actual
1% 2% 3% 4% 5% 6% 7% 8% 9%10%
Unlearn Ratio101102Value of/lscript2
(a) Bounds vs. actual ℓ2distance on Cora.
1% 2% 3% 4% 5% 6% 7% 8% 9%10%
Unlearn Ratio101102Value of/lscript2
 (b) Bounds vs. actual ℓ2distance on Citeseer.
1% 2% 3% 4% 5% 6% 7% 8% 9%10%
Unlearn Ratio102Value of/lscript2
 (c) Bounds vs. actual ℓ2distance on PubMed.
Figure 3: Bounds and actual value of the ℓ2distance between ˜𝜽∗and ¯𝜽∗, i.e.,∥˜𝜽∗−¯𝜽∗∥2, over Cora, CiteSeer and PubMed datasets.
CEU Worst, CEU Data Dependent, IDEA, and Actual represent the worst bound based on CEU, the data-dependent bound based
on CEU, the bound based on IDEA, and the actual value of ∥˜𝜽∗−¯𝜽∗∥2derived from re-training, respectively.
5.1 Experimental Setup
Downstream Task and Datasets. We adopt the widely studied
node classification task as the downstream task, which accounts
for a wide range of real-world applications based on GNNs. We per-
form experiments over five real-world datasets, including Cora [ 26],
Citeseer [ 26], PubMed [ 26], Coauthor-CS [ 4,46], and Coauthor-
Physics [ 4,46]. These datasets usually serve as commonly used
benchmark datasets for GNN performance over node classifica-
tion tasks. Specifically, Cora, Citeseer, and PubMed are citation
networks, where nodes denote research publications and edges
represent the citation relationship between any pair of publications.
The node attributes are bag-of-words representations of the pub-
lication keywords. Coauthor-CS, and Coauthor-Physics are two
coauthor networks, where nodes represent authors and edges de-
note the collaboration relationship between any pair of authors.
We leave more dataset details, e.g., their statistics, in Appendix.
Backbone GNNs. To evaluate the generalization ability of IDEA
across different GNNs, we propose to utilize two types of GNNs,
including linear and non-linear GNNs. In terms of linear GNNs, we
adopt the popular SGC [ 57]; in terms of non-linear GNNs, we adopt
three popular ones, including GCN [26], GAT [49], and GIN [66].
Unlearning Requests. We consider all unlearning requests pre-
sented in Section 3. For each type of request, we perform experi-
ments over a wide range of scales in terms of the number of un-
learned instances (e.g., nodes and edges). For experiments with
fixed ratios, we adopt a ratio of 5% to perform unlearning for nodes
or edges unless otherwise specified.
Threat Models. To evaluate the effectiveness of the unlearning
strategy, we propose to adopt different types of threat models. Al-
though IDEA is able to flexibly perform four different types of
unlearning requests, there are only limited threat models can be
chosen from. In our experiments, we adopt two state-of-the-art
threat models, namely MIA-Graph [ 35] and StealLink [ 20], for node
membership inference attack and link stealing attack, respectively.
Baselines. We adopt five types of baselines for performance com-
parison. (1) Re-Training. We adopt the re-training approach to obtain
an ideal model based on the optimization problem given by Equa-
tion (2). (2) Exact Unlearning. We adopt the popular GraphEraser [ 4]
as a representative method for exact unlearning. Specifically, exact
unlearning methods aim to achieve the exact same probability dis-
tribution in the model space (after unlearning) compared with there-trained model. As a comparison, IDEA aims to approximate the
distribution of the re-trained model through unlearning. (3) Certi-
fied Unlearning. Finally, we adopt two representative approaches for
certified unlearning, namely Certified Graph Unlearning (CGU) [ 6]
and Certified Edge Unlearning (CEU) [59]. CGU is able to unlearn
nodes, attributes, and edges. However, it is only applicable for the
SGC model. As a comparison, CEU can be adapted to different
GNNs. Nevertheless, it is specially designed for edge unlearning.
Evaluation Metrics. We evaluate IDEA with different metrics to
answer the four research questions. (1) Bound Tightness. We propose
to compare the numerical values of the bounds given by IDEA,
the bounds given by other baselines, and the actual ℓ2distance of
model parameters yielded by re-training. A smaller bound on the
ℓ2distance indicates better tightness. (2) Model Utility. We utilize
the F1 score to measure the model utility after unlearning. A higher
F1 score indicates better performance. (3) Unlearning Efficiency. We
utilize the running time (in seconds) that the unlearning methods
take to measure efficiency, and a shorter running time indicates
better efficiency. (4) Unlearning Effectiveness. We use the attack
successful rate after unlearning to measure unlearning effectiveness.
Lower attack successful rates indicate better effectiveness.
5.2 Evaluation of Bound Tightness
To answer RQ1, we first evaluate how tight the derived bound
between ˜𝜽∗and ¯𝜽∗can be across different GNNs, graph datasets,
and unlearning ratios. We also compare the bound derived based
on IDEA and other bounds in existing works. To the best of our
knowledge, CEU [ 59] is the only existing certified unlearning ap-
proach that provides generalizable bounds across different GNNs.
In particular, CEU provides bounds over the objective function after
unlearning, and we adapt such bounds over the objective function
toℓ2distance bounds between ˜𝜽∗and ¯𝜽∗based on the common as-
sumption of the objective function being Lipschitz continuous [ 59].
We compare the bounds and the ℓ2distances below. (1) CEU Worst
Bound. We compute the theoretical worst bound derived based on
CEU as a baseline of the ℓ2distance bound between ˜𝜽∗and ¯𝜽∗.
(2) CEU Data-Dependent Bound. We compute the data-dependent
bound derived based on CEU as a baseline of the ℓ2distance bound
between ˜𝜽∗and ¯𝜽∗. A data-dependent bound is tighter than the
Worst Bound. (3) IDEA Bound. We compute the bound given by
Equation 3 as the bound for the ℓ2distance between ˜𝜽∗and ¯𝜽∗.(4)
 
626IDEA: A Flexible Framework of Certified Unlearning for
Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: F1 score on five real-world graph datasets under node classification task. All numerical values are reported in percentage,
and the F1 scores given by the proposed framework IDEA are marked in bold.
Cora CiteSeer PubMed CS Physics
GCNRe-Training 76.88±0.3 67.27±0.6 76.20±0.0 86.79±0.3 92.30±0.0
Random 47.97±0.5 46.25±5.6 70.98±0.1 80.64±0.3 75.23±0.1
BEKM 50.68±2.0 46.85±4.9 69.64±0.1 80.30±0.2 74.85±0.1
BLPA 43.79±2.2 40.24±8.3 63.42±5.7 85.10±0.3 78.93±0.5
IDEA 72.08 ±1.2 61.56±1.2 73.11±0.0 86.13±0.4 91.93±0.1
SGCRe-Training 76.14±0.6 65.77±0.0 75.90±0.0 87.10±0.1 92.01±0.0
Random 46.00±0.7 45.25±3.0 69.03±0.1 81.30±0.3 80.81±0.1
BEKM 48.83±1.8 46.45±0.4 69.76±0.1 80.08±0.2 74.87±0.2
BLPA 63.59±1.4 39.44±2.8 62.98±4.6 86.95±0.1 87.38±0.1
IDEA 72.94 ±1.9 63.16±1.0 73.63±0.8 84.68±0.3 91.21±0.1
GINRe-Training 82.90±0.6 74.27±0.5 85.31±0.6 90.28±0.2 95.57±0.2
Random 69.25±6.3 51.85±2.7 83.64±1.2 89.17±0.1 91.74±0.5
BEKM 74.05±3.5 65.17±2.8 84.35±0.3 89.39±0.5 92.30±0.3
BLPA 62.48±2.9 55.06±7.2 82.25±1.6 62.29±0.7 71.66±1.4
IDEA 72.57 ±2.8 66.37±4.6 82.33±0.2 88.48±0.6 94.63±0.1
GATRe-Training 83.76±0.3 75.88±0.1 85.02±0.1 92.24±0.1 95.28±0.1
Random 58.18±2.0 55.43±4.3 68.20±6.9 80.75±0.1 78.26±0.1
BEKM 64.20±1.5 57.35±2.8 71.67±0.2 80.37±0.3 77.47±0.2
BLPA 60.88±1.0 58.26±2.6 67.34±3.4 85.22±0.2 86.12±0.2
IDEA 84.38 ±0.6 75.78±0.9 84.92±0.2 92.20±0.2 95.41±0.0
Actual Values. We compare the bounds above with the actual ℓ2
distance between ˜𝜽∗and¯𝜽∗. Note that we focus on edge unlearning
tasks to analyze the tightness of the derived bounds, since this is
the only unlearning task CEU supports. We use Unlearn Ratio to
refer to the ratio of edges to be unlearned from the GNN.
We present the bounds and the actual value of the ℓ2distance
between ˜𝜽∗and ¯𝜽∗over a wide range of unlearn ratios (from 1%
to 10%), which covers common values, in Figure 3. We also have
similar observations in other cases (see Appendix). We summarize
the observations below. (1) From the perspective of the general
tendency, we observe that larger unlearn ratios usually lead to
larger values in both the derived bounds and the actual ℓ2distance
between ˜𝜽∗and ¯𝜽∗. This reveals that a larger unlearn ratio tends
to make the approximation of ˜𝜽∗(with the calculated ¯𝜽∗) more
difficult, which is in alignment with existing works [ 59]. (2) From
the perspective of the bound tightness, we found that IDEA is able
to give tighter bounds in all cases compared with the bounds given
by CEU, especially in cases with larger unlearn ratios. This reveals
that the approximation of ˜𝜽∗given by IDEA can better characterize
the difference between ˜𝜽∗and𝜽∗compared with CEU.
5.3 Evaluation of Unlearning Efficiency
To answer RQ2, we then evaluate the efficiency of IDEA in perform-
ing unlearning. Specifically, we adopt the common node unlearning
task as an example, and we measure the running time of unlearning
in seconds. We note that CGU only supports performing unlearning
on SGC, and thus we adopt SGC as the backbone GNN for IDEA
and all other baselines for a fair comparison. We use Unlearn Ratio
to refer to the ratio of training nodes to be unlearned from the
GNN. Here we present a comparison between IDEA and baselines
on Cora dataset in Figure 4. We also have similar observations on
other GNNs and datasets (see Appendix). We summarize the ob-
servations below. (1) From the perspective of the general tendency,we observe that the running time of re-training does not change
across different unlearning ratios. This is because the number of
optimization epochs dominates the running time of re-training,
while the total epoch number does not change no matter how many
training nodes are removed. However, the efficiency of all other
baselines is sensitive to the unlearning ratio, and this is because
their running time is closely dependent on the total number of
nodes to be unlearned. Finally, we found that the running time
of IDEA is not sensitive to the unlearn ratio. This is because the
number of nodes to be unlearned will only marginally influence
the computational costs associated with Theorem 1. The stable
running time across different numbers of nodes to be unlearned
serves as a key superiority of IDEA over other baselines. (2) From
the perspective of time comparison, we found that IDEA achieves
significant superiority over all other baselines across the wide range
of unlearning ratios, especially on relatively large ones (e.g., 10%).
Such an observation indicates that IDEA is able to perform unlearn-
ing with satisfying efficiency, which further reveals its practical
significance in real-world applications.
5.4 Evaluation of Model Utility
To answer RQ3, we now compare model utility after performing un-
learning with IDEA and other baselines. We note that GraphEraser
is the only baseline that supports flexible generalization across dif-
ferent GNN backbones. Therefore, we adopt the three variants of
GraphEraser, i.e., Random, BEKM, and BLPA, as the correspond-
ing baselines for comparison. We adopt the most common task
of node unlearning, and we adopt the F1 score (of node classifica-
tion) to measure the model utility after re-training/unlearning. We
present comprehensive empirical results (including four different
GNN backbones and all five real-world datasets) in Table 1. In addi-
tion to the baselines, we also report the performance of re-training,
i.e., the F1 score given by a re-trained model with the unlearned
nodes being removed from the training graph, for comparison.
 
627KDD ’24, August 25–29, 2024, Barcelona, Spain Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, & Jundong Li
0.1% 1% 5% 10%
Unlearn Ratio100Running Time ( log10)Re-Training
BEKMCGU
RandomBLPA
IDEA
Figure 4: Efficiency comparison between IDEA and other
baselines including retraining. Running time is measured
with seconds and presented in log scale.
We summarize the observations below, and similar observations
are also found in different settings (see Appendix). (1) From the
perspective of the general tendency, we observe that unlearning
approaches are usually associated with worse utility performance
compared with re-training. Such a sacrifice is usually considered
acceptable, since these unlearning approaches can bring significant
improvement in efficiency compared with re-training. (2) From the
perspective of model utility, we found that IDEA achieves competi-
tive utility compared with other baselines. Specifically, compared
with re-training, IDEA only sacrifices limited utility performance
in most cases, and even shows better performance in certain cases.
Furthermore, compared with other alternatives, IDEA also shows
consistent superiority in most cases.
5.5 Evaluation of Unlearning Effectiveness
To answer RQ4, we compare the unlearning effectiveness of IDEA
and other baselines. Specifically, we utilize the state-of-the-art at-
tack methods MIA-Graph and StealLink to evaluate the unlearning
effectiveness of node and edge unlearning tasks, respectively. To
also have CGU as a baseline, we adopt SGC as the backbone GNN
to ensure a fair comparison. We present the attack successful rates
after node and edge unlearning in Table 2. All attacks are per-
formed over those unlearned nodes/edges, and thus a lower AUC
score represents better unlearning performance. In terms of node
attributes unlearning, we note that to the best of our knowledge,
no existing membership inference attack method supports the asso-
ciated attack. Here we use the average loss value as an unlearning
performance indicator. Specifically, we perform partial attribute un-
learning under different ratios (20%, 50%, 80%) of unlearn attribute
dimensions to the total attribute dimensions. Note that partial at-
tribute unlearning aims to twist the GNN model such that the GNN
model behaves as if it were trained on those nodes with the un-
learn attribute values being set to non-informative numbers (as
in Section 3). Here we follow a common choice [ 6] to set such a
number as zero. Accordingly, we evaluate the performance with the
average loss values regarding the nodes with the unlearn attributes
being set to zeros, and a lower loss value indicates better unlearn-
ing effectiveness. We present the results in Table 3. Note that CGU
only supports full attribute unlearning, while the three variants
of GraphEraser only support node/edge unlearning. Therefore, we
perform attribute unlearning and node unlearning for CGU andTable 2: Attack AUC scores after node and edge unlearning
on Cora. The results given by IDEA are marked in bold.
Node Unlearning ( ↓) Edge Unlearning ( ↓)
Random 50.38±0.5 55.64 ±2.8
BEKM 50.35±1.2 51.81 ±0.3
BLPA 50.30±0.4 50.84 ±3.4
CGU 54.67±2.9 66.52 ±0.6
IDEA 50.86 ±1.8 50.11 ±0.9
Table 3: Average loss values on Cora regarding the nodes with
the unlearn attributes being set to zeros. Ratio of unlearn
node attribute dimensions to all attribute dimensions varies
across 20%, 50%, and 80%. Lower values represent better per-
formance, and results from IDEA are marked in bold.
20% (↓) 50% (↓) 80% (↓)
Random 1.32±0.06 1.38±0.06 1.35±0.09
BEKM 1.41±0.16 1.47±0.14 1.39±0.12
BLPA 1.47±0.11 1.69±0.37 1.50±0.05
CGU 1.62±0.02 1.73±0.04 1.78±0.06
IDEA 1.29 ±0.01 1.31±0.01 1.33±0.01
GraphEraser, respectively. Based on the settings above, we have
the observations below, and consistent observations are also found
under different settings (see Appendix). (1) From the perspective of
node and edge unlearning, we observe that the attack AUC scores
over IDEA are among the lowest in both unlearning tasks. Noticing
that the AUC scores given by IDEA are only marginally above 50%,
the unlearned node/edge information has been almost completely
removed from the trained GNNs. (2) From the perspective of at-
tribute unlearning, IDEA exhibits the lowest average loss values in
all (attribute) unlearn ratios. This indicates the superior attribute
unlearning performance.
6 RELATED WORK
Certified Machine Unlearning. The general desiderata of ma-
chine unlearning is to remove the influence of certain training
data on the model parameters, such that the model can behave
as if it never saw such data [ 2,15,34,52,63,68]. Re-training the
model without making the unlearning data visible is an ideal way
to achieve such a goal, while it is usually infeasible in practice
due to various reasons such as prohibitively high computational
costs [ 39,43,64,70]. A popular way to approach the goal of un-
learning is to directly approximate the re-trained model parameters,
a.k.a., approximate unlearning [48, 63]. Certified machine unlearn-
ing is under the umbrella of approximate unlearning [ 30,72], and
it has stood out due to the capability of providing theoretical guar-
antee on the unlearning effectiveness. A commonly used criterion
of certified unlearning is (𝜀−𝛿)certified unlearning [9,16,29,44],
which utilizes two parameters 𝜀and𝛿to describe the proximity
between the re-trained model parameter distribution and approxi-
mated model parameter distribution in the model space. In recent
years, various techniques have been proposed to achieve certified
unlearning [ 32,55,73]. However, they overwhelmingly focus on
 
628IDEA: A Flexible Framework of Certified Unlearning for
Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
independent, identically distributed (i.i.d.) data and fail to consider
the dependency between data points. Therefore, they cannot be di-
rectly adopted to perform unlearning over GNNs [ 58,59]. Different
from the works mentioned above, our paper proposes a certified
unlearning approach for GNNs, necessitating the modeling of de-
pendencies between instances in graphs (e.g., nodes and edges).
Machine Unlearning for Graph Neural Networks. Over the
years, GNNs have been increasingly deployed in a plethora of appli-
cations [ 11,12,18,23,28,31,33,47,51,60,69,76]. Similar to other
machine learning models, these GNN models also face the risk of
privacy leakage [ 24,38,42,45,53,67], where the private informa-
tion is usually considered to be encoded in the training data [ 41].
Such a threat has prompted the emerging of unlearning approaches
for GNNs [ 4,5,36,58]. However, these works are only able to
achieve unlearning for GNNs empirically, failing to provide any
theoretical guarantee on the effectiveness. To further strengthen
the power of unlearning for GNNs and enhance the confidence
of model owners before model deployment, a few recent works
have initiated explorations on certified unlearning for GNNs. Wu
et al. [ 59] propose CEU to unlearn edges that are visible to GNNs
during training, while edge unlearning is the only type of request
it is able to handle. Chien et al. [ 6] proposed a different certified
unlearning approach for GNNs to also handle node and attribute
unlearning requests, while such an approach is only applicable to a
specially simplified GNN model. Meanwhile, these approaches can
only handle limited types of unlearning requests, which further
jeopardizes their flexibility in real-world applications. Different
from them, our paper proposes a flexible unlearning framework
that can handle different types of unlearning requests. On top of
this framework, an effectiveness certification is further proposed
without relying on any specific GNN structure or objective function.
7 CONCLUSION
In this paper, we propose IDEA, a flexible framework of certified un-
learning for GNNs. Specifically, we first formulate and study a novel
problem of flexible and certified unlearning for GNNs, which aims
to flexibly handle different unlearning requests with theoretical
guarantee. To tackle this problem, we develop IDEA by analyzing
the objective difference before and after certain information is re-
moved from the graph. We further present theoretical guarantee
as the certification for unlearning effectiveness. Extensive experi-
ments on real-world datasets demonstrate the superiority of IDEA
in multiple key perspectives. Meanwhile, two future directions are
worth further investigation. First, we focus on the common node
classification task in this paper, and we will extend the proposed
framework to other tasks, such as graph classification. Second, con-
sidering that GNNs may be trained in a decentralized manner, it is
critical to study GNN unlearning under a distributed setting.
8 ACKNOWLEDGEMENTS
This work is supported in part by the National Science Founda-
tion under IIS-2006844, IIS-2144209, IIS-2223769, IIS-1900990, IIS-
2239257, IIS-2310262, CNS-2154962, and BCS-2228534; the Common-
wealth Cyber Initiative Awards under VV-1Q23-007, HV-2Q23-003,
and VV-1Q24-011; the JP Morgan Chase Faculty Research Award;
the Cisco Faculty Research Award; and Snap gift funding.REFERENCES
[1]Privacy Act. 2000. Personal Information Protection and Electronic Documents
Act. Department of Justice, Canada. Full text available at http://laws. justice. gc.
ca/en/P-8.6/text. html (2000).
[2]Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hen-
grui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021.
Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE,
141–159.
[3]Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine
unlearning. In 2015 IEEE symposium on security and privacy. IEEE, 463–480.
[4]Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert,
and Yang Zhang. 2022. Graph unlearning. In Proceedings of the 2022 ACM SIGSAC
Conference on Computer and Communications Security. 499–513.
[5]Jiali Cheng, George Dasoulas, Huan He, Chirag Agarwal, and Marinka Zitnik.
2023. GNNDelete: A General Strategy for Unlearning in Graph Neural Networks.
arXiv preprint arXiv:2302.13406 (2023).
[6]Eli Chien, Chao Pan, and Olgica Milenkovic. 2022. Certified graph unlearning.
arXiv preprint arXiv:2206.09140 (2022).
[7]Eli Chien, Chao Pan, and Olgica Milenkovic. 2022. Efficient model updates for
approximate unlearning of graph-structured data. In The Eleventh International
Conference on Learning Representations.
[8]R Dennis Cook and Sanford Weisberg. 1980. Characterizations of an empirical
influence function for detecting influential cases in regression. Technometrics 22,
4 (1980), 495–508.
[9]Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu,
Jiliang Tang, and Suhang Wang. 2022. A comprehensive survey on trustworthy
graph neural networks: Privacy, robustness, fairness, and explainability. arXiv
preprint arXiv:2204.08570 (2022).
[10] Kien Do, Truyen Tran, and Svetha Venkatesh. 2019. Graph transformation policy
network for chemical reaction prediction. In Proceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data mining. 750–760.
[11] Guimin Dong, Mingyue Tang, Zhiyuan Wang, Jiechao Gao, Sikun Guo, Lihua Cai,
Robert Gutierrez, Bradford Campbel, Laura E Barnes, and Mehdi Boukhechba.
2023. Graph neural networks in IoT: A survey. ACM Transactions on Sensor
Networks 19, 2 (2023), 1–50.
[12] Yushun Dong, Binchi Zhang, Yiling Yuan, Na Zou, Qi Wang, and Jundong Li. 2023.
Reliant: Fair knowledge distillation for graph neural networks. In Proceedings of
the 2023 SIAM International Conference on Data Mining (SDM). SIAM, 154–162.
[13] Keyu Duan, Zirui Liu, Peihao Wang, Wenqing Zheng, Kaixiong Zhou, Tianlong
Chen, Xia Hu, and Zhangyang Wang. 2022. A comprehensive study on large-scale
graph training: Benchmarking and rethinking. Advances in Neural Information
Processing Systems 35 (2022), 5376–5389.
[14] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417–426.
[15] Keltin Grimes, Collin Abidi, Cole Frank, and Shannon Gallagher. 2024. Gone but
Not Forgotten: Improved Benchmarks for Machine Unlearning. arXiv preprint
arXiv:2405.19211 (2024).
[16] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten.
2019. Certified data removal from machine learning models. arXiv preprint
arXiv:1911.03030 (2019).
[17] Chuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. 2020.
Certified Data Removal from Machine Learning Models. In Proceedings of the
37th International Conference on Machine Learning.
[18] Atika Gupta, Priya Matta, and Bhasker Pant. 2021. Graph neural network: Current
state of Art, challenges and applications. Materials Today: Proceedings 46 (2021),
10927–10932.
[19] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[20] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang.
2021. Stealing links from graph neural networks. In 30th USENIX Security Sym-
posium (USENIX Security 21). 2669–2686.
[21] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G
Coleman. 2012. ZINC: a free tool to discover chemistry for biology. Journal of
chemical information and modeling 52, 7 (2012), 1757–1768.
[22] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. 2021.
Approximate data deletion from machine learning models. In International Con-
ference on Artificial Intelligence and Statistics. PMLR, 2008–2016.
[23] Yiqiao Jin, Yeon-Chang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Di-
vakaran, and Srijan Kumar. 2023. Predicting Information Pathways Across Online
Communities. In KDD.
[24] Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang
Gu, Yifang Qin, Nan Yin, Senzhang Wang, et al .2024. A survey of graph neural
networks in real world: Imbalance, noise, privacy and ood challenges. arXiv
preprint arXiv:2403.04468 (2024).
 
629KDD ’24, August 25–29, 2024, Barcelona, Spain Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, & Jundong Li
[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[26] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations.
[27] Chanhee Kwak, Junyeong Lee, Kyuhong Park, and Heeseok Lee. 2017. Let
machines unlearn–machine unlearning and the right to be forgotten. (2017).
[28] Fan Liang, Cheng Qian, Wei Yu, David Griffith, and Nada Golmie. 2022. Survey
of graph neural networks and applications. Wireless Communications and Mobile
Computing 2022, 1 (2022), 9261537.
[29] Jiaqi Liu, Jian Lou, Zhan Qin, and Kui Ren. 2024. Certified minimax unlearning
with generalization rates and deletion capacity. Advances in Neural Information
Processing Systems 36 (2024).
[30] Ziyao Liu, Huanyi Ye, Chen Chen, and Kwok-Yan Lam. 2024. Threats, attacks,
and defenses in machine unlearning: A survey. arXiv preprint arXiv:2403.13682
(2024).
[31] Jing Ma, Yushun Dong, Zheng Huang, Daniel Mietchen, and Jundong Li. 2022.
Assessing the causal impact of COVID-19 related policies on outbreak dynamics:
A case study in the US. In Proceedings of the ACM Web Conference 2022. 2678–2686.
[32] Ananth Mahadevan and Michael Mathioudakis. 2021. Certifiable machine un-
learning for linear models. arXiv preprint arXiv:2106.15093 (2021).
[33] Neng Kai Nigel Neo, Yeon-Chang Lee, Yiqiao Jin, Sang-Wook Kim, and Srijan
Kumar. 2024. Towards Fair Graph Anomaly Detection: Problem, New Datasets,
and Evaluation. arXiv:2402.15988 (2024).
[34] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung
Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2022. A survey of machine
unlearning. arXiv preprint arXiv:2209.02299 (2022).
[35] Iyiola E Olatunji, Wolfgang Nejdl, and Megha Khosla. 2021. Membership inference
attack on graph neural networks. In Third IEEE International Conference on Trust,
Privacy and Security in Intelligent Systems and Applications. IEEE, 11–20.
[36] Chao Pan, Eli Chien, and Olgica Milenkovic. 2023. Unlearning graph classifiers
with limited data resources. In Proceedings of the ACM Web Conference 2023.
716–726.
[37] Stuart L Pardau. 2018. The california consumer privacy act: Towards a european-
style privacy regime in the united states. J. Tech. L. & Pol’y 23 (2018), 68.
[38] Yeqing Qiu, Chenyu Huang, Jianzong Wang, Zhangcheng Huang, and Jing Xiao.
2022. A privacy-preserving subgraph-level federated graph neural network via
differential privacy. In International Conference on Knowledge Science, Engineering
and Management. Springer, 165–177.
[39] Youyang Qu, Xin Yuan, Ming Ding, Wei Ni, Thierry Rakotoarivelo, and David
Smith. 2023. Learn to unlearn: A survey on machine unlearning. arXiv preprint
arXiv:2305.07512 (2023).
[40] General Data Protection Regulation. 2018. General data protection regulation
(GDPR). Intersoft Consulting, Accessed in October 24, 1 (2018).
[41] Anwar Said, Tyler Derr, Mudassir Shabbir, Waseem Abbas, and Xenofon Kout-
soukos. 2023. A Survey of Graph Unlearning. preprint arXiv:2310.02164 (2023).
[42] Sina Sajadmanesh. 2023. Privacy-Preserving Machine Learning on Graphs. Techni-
cal Report. EPFL.
[43] Sebastian Schelter, Mozhdeh Ariannezhad, and Maarten de Rijke. 2023. Forget
me now: Fast and exact unlearning in neighborhood-based recommendation.
InProceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 2011–2015.
[44] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh.
2021. Remember what you want to forget: Algorithms for machine unlearning.
Advances in Neural Information Processing Systems 34 (2021), 18075–18086.
[45] Chuanqiang Shan, Huiyun Jiao, and Jie Fu. 2021. Towards representation identi-
cal privacy-preserving graph neural network via split learning. arXiv preprint
arXiv:2107.05917 (2021).
[46] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[47] Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, and Ninghao Liu. 2023.
Gigamae: Generalizable graph masked autoencoder via collaborative latent space
reconstruction. In Proceedings of the 32nd ACM International Conference on Infor-
mation and Knowledge Management. 2259–2269.
[48] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. 2022.
Unrolling sgd: Understanding factors influencing machine unlearning. In 2022
IEEE 7th European Symposium on Security and Privacy (EuroS&P) . IEEE, 303–319.
[49] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[50] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang,
Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A semi-supervised graph
attentive network for financial fraud detection. In 2019 IEEE International Confer-
ence on Data Mining (ICDM). IEEE, 598–607.
[51] Song Wang, Yushun Dong, Kaize Ding, Chen Chen, and Jundong Li. 2023. Few-
shot node classification with extremely weak supervision. In Proceedings of the
Sixteenth ACM International Conference on Web Search and Data Mining. 276–284.[52] Weiqi Wang, Zhiyi Tian, and Shui Yu. 2024. Machine Unlearning: A Comprehen-
sive Survey. arXiv preprint arXiv:2405.07406 (2024).
[53] Xuemin Wang, Tianlong Gu, Xuguang Bao, and Liang Chang. 2023. Fair and
Privacy-Preserving Graph Neural Network. In International Conference on Data-
base Systems for Advanced Applications. Springer, 731–735.
[54] Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and Tyler
Derr. 2022. Improving fairness in graph neural networks via mitigating sensitive
attribute leakage. In Proceedings of the 28th ACM SIGKDD conference on knowledge
discovery and data mining. 1938–1948.
[55] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck.
2021. Machine unlearning of features and labels. preprint arXiv:2108.11577 (2021).
[56] Bang Wu, Xiangwen Yang, Shirui Pan, and Xingliang Yuan. 2021. Adapting
membership inference attacks to GNN for graph classification: Approaches and
implications. In IEEE International Conference on Data Mining (ICDM). IEEE.
[57] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861–6871.
[58] Jiancan Wu, Yi Yang, Yuchun Qian, Yongduo Sui, Xiang Wang, and Xiangnan
He. 2023. GIF: A General Graph Unlearning Strategy via Influence Function. In
Proceedings of the ACM Web Conference 2023. 651–661.
[59] Kun Wu, Jie Shen, Yue Ning, Ting Wang, and Wendy Hui Wang. 2023. Certified
edge unlearning for graph neural networks. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 2606–2617.
[60] Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Xiaojie Guo. 2022. Graph neural
networks: foundation, frontiers and applications. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 4840–4841.
[61] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.
Session-based recommendation with graph neural networks. In Proceedings of
the AAAI conference on artificial intelligence, Vol. 33. 346–353.
[62] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4–24.
[63] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S Yu. 2023.
Machine unlearning: A survey. Comput. Surveys 56, 1 (2023), 1–36.
[64] Jie Xu, Zihan Wu, Cong Wang, and Xiaohua Jia. 2024. Machine unlearning:
Solutions and challenges. IEEE Transactions on Emerging Topics in Computational
Intelligence (2024).
[65] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[66] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Pow-
erful are Graph Neural Networks?. In 7th International Conference on Learning
Representations.
[67] Wanghan Xu, Bin Shi, Jiqiang Zhang, Zhiyuan Feng, Tianze Pan, and Bo Dong.
2023. MDP: Privacy-Preserving GNN Based on Matrix Decomposition and Dif-
ferential Privacy. In 2023 IEEE International Conference on Joint Cloud Computing
(JCC). IEEE, 38–45.
[68] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin.
2022. ARCANE: An Efficient Architecture for Exact Machine Unlearning.. In
IJCAI, Vol. 6. 19.
[69] Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, and Jundong Li.
2023. Adversarial Attacks on Fairness of Graph Neural Networks. arXiv preprint
arXiv:2310.13822 (2023).
[70] Haibo Zhang, Toru Nakamura, Takamasa Isohara, and Kouichi Sakurai. 2023. A
review on machine unlearning. SN Computer Science 4, 4 (2023), 337.
[71] Xiao-Meng Zhang, Li Liang, Lin Liu, and Ming-Jing Tang. 2021. Graph neural
networks and their current applications in bioinformatics. Frontiers in genetics
12 (2021), 690049.
[72] Yi Zhang, Yuying Zhao, Zhaoqing Li, Xueqi Cheng, Yu Wang, Olivera Kotevska,
Philip S Yu, and Tyler Derr. 2023. A Survey on Privacy in Graph Neural Networks:
Attacks, Preservation, and Applications. arXiv preprint arXiv:2308.16375 (2023).
[73] Zijie Zhang, Yang Zhou, Xin Zhao, Tianshi Che, and Lingjuan Lyu. 2022. Prompt
certified machine unlearning with randomized gradient smoothing and quantiza-
tion. Advances in Neural Information Processing Systems 35 (2022), 13433–13455.
[74] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: A
graph multi-attention network for traffic prediction. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 34. 1234–1241.
[75] Wenyue Zheng, Ximeng Liu, Yuyang Wang, and Xuanwei Lin. 2023. Graph Un-
learning Using Knowledge Distillation. In International Conference on Information
and Communications Security. Springer, 485–501.
[76] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI open 1 (2020), 57–81.
[77] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. 2018. Modeling polyphar-
macy side effects with graph convolutional networks. Bioinformatics 34, 13 (2018),
i457–i466.
 
630