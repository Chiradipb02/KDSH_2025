Latent Diffusion-based Data Augmentation for Continuous-Time
Dynamic Graph Model
Yuxing Tian
tianyxx@gmail.com
International Digital Economy
Academy, IDEA Research
Shenzhen, ChinaAiwen Jiang
jiangaiwen@jxnu.edu.cn
Jiangxi Normal University
Nanchang, ChinaQi Huang
huangqi@jxnu.edu.cn
Jiangxi Normal University
Nanchang, China
Jian Guo‡
guojian@idea.edu.cn
International Digital Economy
Academy, IDEA Research
Shenzhen, ChinaYiyan Qi‡
qiyiyan@idea.edu.cn
International Digital Economy
Academy, IDEA Research
Shenzhen, China
ABSTRACT
Continuous-Time Dynamic Graph (CTDG) precisely models evolv-
ing real-world relationships, drawing heightened interest in dy-
namic graph learning across academia and industry. However, ex-
isting CTDG models encounter challenges stemming from noise and
limited historical data. Graph Data Augmentation (GDA) emerges
as a critical solution, yet current approaches primarily focus on
static graphs and struggle to effectively address the dynamics inher-
ent in CTDGs. Moreover, these methods often demand substantial
domain expertise for parameter tuning and lack theoretical guaran-
tees for augmentation efficacy. To address these issues, we propose
Conda, a novel latent diffusion-based GDA method tailored for CT-
DGs. Conda features a sandwich-like architecture, incorporating a
Variational Auto-Encoder (VAE) and a conditional diffusion model,
aimed at generating enhanced historical neighbor embeddings for
target nodes. Unlike conventional diffusion models trained on en-
tire graphs via pre-training, Conda requires historical neighbor
sequence embeddings of target nodes for training, thus facilitating
more targeted augmentation. We integrate Conda into the CTDG
model and adopt an alternating training strategy to optimize perfor-
mance. Extensive experimentation across six widely used real-world
datasets showcases the consistent performance improvement of
our approach, particularly in scenarios with limited historical data.
CCS CONCEPTS
•Theory of computation →Dynamic graph algorithms; •
Information systems →Social networks.
‡Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671863KEYWORDS
Dynamic graph, data augmentation, diffusion model
ACM Reference Format:
Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo‡, and Yiyan Qi‡. 2024. Latent
Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph
Model. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, NewYork, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671863
1 INTRODUCTION
Continuous-Time Dynamic Graphs (CTDGs), with every edge (event)
having a timestamp to denote its occurrence time, are prevalent
in real-world applications such as social networks [ 1,11], physical
systems [ 18] and e-commerce [ 46]. Recently, CTDG models [ 2,16,
17,23,24,26,31,37,48] have gained increasing attention due to
their significant representation capacity by directly learning the
representation of the continuously occurring events in CTDGs. De-
spite the rapid advancements in CTDG models, they encounter two
primary challenges. Firstly, the "observed" CTDG often falls short
of accurately representing the true underlying process it intends
to model, mainly due to various factors such as measurement in-
accuracies, thresholding errors, or human mistakes [ 22]. Secondly,
most CTDG methods typically rely on extensive historical data for
effective training [ 26]. However, in many applications, obtaining
such data is impractical, particularly in scenarios with a cold start.
For instance, a nascent trading platform may only possess a few
days’ worth of user-asset interactions, rendering existing CTDG
models trained on such limited data inadequate and resulting in
sub-optimal performance.
Graph Data Augmentation (GDA) has emerged as a promising
solution, with existing methods falling into two main categories [ 7]:
structure-oriented and feature-oriented methods. Structure-oriented
methods, such as [ 15,28,42], typically involve adjusting graph con-
nectivity by adding or removing edges or nodes. On the other hand,
feature-oriented methods, exemplified by works from [ 12,14,15,
21,32], directly modify or create raw features of nodes or edges in
graphs.
2900
KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
However, most existing GDA methods focus on static graphs and
it is challenging to directly apply on CTDG. (1) Existing structure-
oriented GDA methods heavily rely on domain knowledge, necessi-
tating the selection of diverse augmentation strategy combinations
tailored to specific graph datasets, as highlighted in [ 45]. Further-
more, structure-oriented GDA methods present inherent challenges
in calibrating the extent of data augmentation, potentially leading
to either over-augmentation or under-augmentation [ 3]. This is-
sue arises from input data processed by mapping-agnostic deep
neural networks, which generate features without tailored calibra-
tion. (2) Existing feature-oriented GDA methods concentrate on
transforming node/edge features and have demonstrated significant
optimization improvements with appropriate feature augmenta-
tions. However, they cannot handle graphs lacking node features or
even edge features. (3) Although Wang et al. [ 36] propose MeTA for
CTDG, this method is tailored for CTDG models [ 29,31] with mem-
ory modules, limiting its compatibility with other state-of-the-art
models such as DyGFormer [44] and GraphMixer [5].
To address these above challenges, we introduce a novel latent
cond itional diffusion-based dataaugmentation method for CTDG
models, named Conda . Conda adopts a sandwich-like architecture,
comprising a Variational Auto-Encoder (VAE) and a conditional
diffusion model. This design aims to create new historical neighbor
embeddings derived from existing neighbor sequences, enhancing
subsequent training of the CTDG model. Unlike traditional diffusion
models trained via pre-training on the entire graphs, Conda neces-
sitates historical neighbor sequence embeddings of target nodes for
training. Consequently, we integrate Conda into the CTDG model
and implement an alternating training strategy. Our contributions
can be summarized as follows:
•We present Conda, an innovative data augmentation technique
aimed at enhancing the CTDG model. This method leverages a
latent conditional diffusion model to generate historical neighbor
embeddings for the target node during the training phase.
•Rather than directly manipulating the raw graph structure, Conda
operates within the latent space, where it is more likely to en-
counter authentic samples.
•We extensively evaluate our method on six widely used real-
world datasets and compare it against seven baselines. Our re-
sults demonstrate that Conda enhances the performance of link
prediction tasks on these baselines by up to 5%. Notably, this
improvement is achieved without the need for domain-specific
knowledge.
2 PRELIMINARIES
2.1 Continuous-Time Dynamic Graph
A CTDG𝐺can be represented as a chronological sequence of inter-
actions between specific node pairs: 𝐺={(𝑢0,𝑣0,𝑡0),...,(𝑢𝑛,𝑣𝑛,𝑡𝑛)},
where𝑡𝑖denotes the timestamp and the timestamps are ordered
as(0≤𝑡0≤𝑡1≤...≤𝑡𝑛).𝑢𝑖,𝑣𝑖∈𝑉denote the node IDs of the
𝑖−𝑡ℎinteraction at timestamp 𝑡𝑖,𝑉is the entire node set. Each
node𝑣∈𝑉is associated with node feature 𝑣𝑢, and each interaction
(𝑢,𝑗,𝑡)has edge feature 𝑒𝑡
𝑢,𝑣∈𝑅𝑑𝑒, where𝑑𝑣and𝑑𝑒denote the
dimensions of the node and link feature respectively.2.2 Diffusion Model
The diffusion model encompasses both forward and reverse pro-
cesses.
Forward process. In general, given an input data point 𝒙0drawn
from the distribution 𝑞(𝒙0), the forward process involves gradually
introducing Gaussian noise to 𝒙0, generating a sequence of increas-
ingly noisy variables 𝒙1,𝒙2,..., 𝒙𝑁in a Markov chain. The final
noisy output, 𝒙𝑁, follows a Gaussian distribution N(0,𝑰)and car-
ries no discernible information about the original data point. Specif-
ically, the transition from one point to the next is determined by
a conditional probability 𝑞(𝒙𝑛|𝒙𝑛−1)=N(𝒙𝑛;√︁
1−𝛽𝑛𝒙𝑛−1,𝛽𝑛𝑰),
where𝛽𝑛∈(0,1)controls the scale of noise added at step 𝑛.
Reverse process. The reverse process reverses the effects of the
forward process by learning to eliminate the added noise and tries
to gradually reconstruct the original data 𝑥0via sampling from 𝑥𝑁
by learning a neural network 𝑓𝜃.
Inference. Once trained, the diffusion model can produce new data
by sampling a point from the final distribution 𝒙𝑁∼N( 0,𝑰)and
then iteratively denoising it using the aforementioned model 𝒙𝑁↦→
𝒙𝑁−1↦→···↦→ 𝒙0to obtain a sample from the data distribution.
3 METHODOLOGY
Existing structure-oriented GDA methods like MeTA augment the
CTDGs by modifying the initial interactions through edge addi-
tion/deletion and time perturbation. However, these methods in-
troduce coarse-grained augmentations and substantially alter the
original transition patterns within CTDGs. Conversely, simply in-
troducing noise to either the raw or hidden feature space often lacks
theoretical bound. In this section, we introduce a novel fine-grained
GDA model based on a conditional diffusion model and establish
robust theoretical guarantees.
3.1 CTDG model
As mentioned above, the paradigm of the CTDG model 𝜉can be
divided into two parts: the encoder module and the backbone mod-
ule. Mathematically, given an interaction (𝑢,𝑣,𝑡)and historical
interactions before timestamp 𝑡, the computation flow unfolds as
follows:
𝒔𝑡
𝑢=𝑒𝑛𝑐({𝑣𝑤𝑙∥𝑒𝑡𝑙
(𝑢,𝑤𝑙)∥𝑡𝑙}),𝑤𝑙∈N<𝑡(𝑢) (1)
where 𝒔𝑡
𝑢∈𝑅𝐿×𝐷denotes the historical neighbor embedding se-
quence,𝐷represents the embedding dimension. N<𝑡(𝑢)denotes
the sampled set of neighbors that interacted with node 𝑢before
timestamps 𝑡,𝑤𝑙∈N<𝑡(𝑢)is the𝑙-th neighbor, and ∥denotes
the concatenation operation. 𝑒𝑛𝑐(·)represents the general encoder
module of CTDG model.
𝒉𝑡
𝑢=𝑏𝑎𝑐𝑘𝑏𝑜𝑛𝑒(𝒔𝑡
𝑢) (2)
𝒉𝑡
𝑢∈𝑅𝐷denotes the representation of node 𝑢at timestamp 𝑡.
𝑏𝑎𝑐𝑘𝑏𝑜𝑛𝑒(·)represents the backbone of CTDG model.
3.2 Conda
Due to the extensive resource requirement of the diffusion process,
to reduce the costs, we first utilize a VAE encoder to conduct di-
mension compression and then conduct diffusion processes in the
latent space.
2901Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD ’24, August 25–29, 2024, Barcelona, Spain.
Figure 1: The Alternating Training Process of Conda and the CTDG model. While the Conda module is in the training phase,
the other modules are frozen. Conversely, when the Conda module is frozen, other modules are in the training phase
VAE Encoder. Given the historical neighbor embedding se-
quence 𝒔of any node1, computed by the CTDG encoder module,
we use a variational encoder parameterized by 𝜙to compress 𝒔
to a low-dimensional vector 𝒛∈𝑅𝐿×𝑑, where𝑑<<𝐷, the VAE
encoder𝜙predicts 𝝁𝜙and𝜎2
𝜙𝑰as the mean and covariance of the
variational distribution 𝑞𝜙(𝒛|𝒔)=N(𝒛;𝝁𝜙(𝒔),𝜎2
𝜙(𝒔)𝑰).
Forward Process with Partial Noising. It is worth noting that,
different from the conventional diffusion model that corrupts the
whole variable without distinction, we conduct partial noising in
order to control the magnitude of data augmentation and prepare
for the reverse process with conditional denoising. Specifically,
given the low-dimensional vector 𝒛=[𝑧𝑤1,𝑧𝑤2,...,𝑧𝑤𝐿], we can set
𝒙0=𝒛as the initial state. Then we divide 𝒙0into two part: diffused
part and conditional part, 𝒙𝑑𝑖𝑓𝑓
0∈𝑅𝑑𝑖𝑓𝑓×𝑑and𝒙𝑐𝑜𝑛𝑑
0∈𝑅𝑐𝑜𝑛𝑑×𝑑,
where𝑑𝑖𝑓𝑓+𝑐𝑜𝑛𝑑 =𝐿and𝒙0=𝒙𝑑𝑖𝑓𝑓
0||𝒙𝑐𝑜𝑛𝑑𝑎
0. In the forward
process, we only add noise on 𝒙𝑑𝑖𝑓𝑓
0. Then the forward process is
parameterized by
𝑞(𝒙𝑑𝑖𝑓𝑓
𝑛|𝒙𝑑𝑖𝑓𝑓
𝑛−1)=N(𝒙𝑑𝑖𝑓𝑓
𝑛;√︁
1−𝛽𝑛𝒙𝑑𝑖𝑓𝑓
𝑛−1,𝛽𝑛𝑰), (3)
where𝛽𝑛∈(0,1)controls the Gaussian noise scales added at each
step𝑛. Since the transition kernel is Gaussian, the value at any step
𝑛can be sampled directly from 𝒙0in practice. Let 𝛼𝑛=1−𝛽𝑛and
¯𝛼𝑛=𝑛Ö
𝑛′=1𝛼′
𝑛, then we can write:
𝑞(𝒙𝑑𝑖𝑓𝑓
𝑛|𝒙𝑑𝑖𝑓𝑓
0)=N(𝒙𝑑𝑖𝑓𝑓
𝑛;√¯𝛼𝑛𝒙𝑑𝑖𝑓𝑓
0,(1−¯𝛼𝑛)𝑰). (4)
where we can reparameterize 𝒙𝑛=√¯𝛼𝑛𝒙0+√1−¯𝛼𝑛𝝐with 𝝐∼
N(0,𝑰). To regulate the added noises in 𝒙1:𝑁, follow [ 35], we ultilize
the linear noise schedule for 1−¯𝛼𝑛:
1−¯𝛼𝑛=𝑘·
𝛼min+𝑛−1
𝑁−1(𝛼max−𝛼min)
, 𝑛∈{1,...,𝑁}(5)
1For brevity, we omit the subscript node 𝑢and superscript timestamp 𝑡in𝒔for𝒔𝑡
𝑢
unless necessary to avoid ambiguity.where a hyper-parameter 𝑘∈[0,1]controls the noise scales, and
two hyper-parameters 𝛼min<𝛼max∈(0,1)indicating the upper
and lower bounds of the added noises.
Reverse Process with Conditional Denoising. The reverse
process is used to reconstruct the original 𝒙0by denoising 𝒙𝑁. With
the partial nosing strategy adopted in the forward process, we can
naturally employ the part without noise as the conditional input
when denoising.
Starting from 𝒙𝑁, the reverse process is parameterized by the
denoising transition step:
𝑝𝜃(𝒙𝑛−1|𝒙𝑛)=N(𝒙𝑛−1;𝝁𝜃(𝒙𝑛,𝑛),𝝈𝜃(𝒙𝑛,𝑛)) (6)
where 𝝁𝜃(𝒙𝑛,𝑛)and𝝈𝜃(𝒙𝑛,𝑛)are parameterization of the pre-
dicted mean and standard deviation outputted by any neural net-
works𝑓𝜃. Then the whole reverse process can be written as follows:
𝑝𝜃(𝑥𝑁:0)=𝑝(𝑥𝑁)𝑛Ö
𝑛=1𝑝𝜃(𝒙𝑛−1|𝒙𝑛) (7)
The reconstructed output is denoted as ˆ𝒙0.
VAE Decoder. To keep the notations consistent, we set ˆ𝒛=ˆ𝒙0,
ˆ𝒛is then fed into the VAE decoder parameterized by 𝜓to predict 𝒔
via𝑝𝜓(𝒔|ˆ𝒙0).
3.3 Optimization and Alternating Training
In this section, we first present the optimization objective for the
Conda and the CTDG model, respectively. Then we introduce the
training and inference process of CTDG model with conda.
Although our ultimate goal is to learn a CTDG model 𝜉, but
we also have to learn the parameters of the VAE encoder 𝑞𝜙(𝒛|𝒔),
the conditional diffusion model log𝑝𝜃(𝒙0)and the VAE decoder
𝑝𝜓(𝒔|ˆ𝒙0)for providing positive augmentation to the CTDG model.
CTDG model. For training the CTDG model 𝜉, the Loss func-
tionL𝑐𝑡𝑑𝑔 depends on the downstream task. For example, if the
downstream task is link prediction, then the loss function is binary
cross-entropy.
2902KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
Variational Auto Encoder. The𝑞𝜙(𝒛|𝒔)and𝑝𝜓(𝒔|ˆ𝒙0)jointly con-
stitute a VAE that bridges the embedding space and the latent space.
We optimize the VAE by directly maximizing the ELBO:
L𝑣𝑎𝑒(𝒔;𝜙,𝜓)=E𝑞𝝓(𝒛|𝒔)
log𝑝(𝒔,𝒛)
𝑞𝝓(𝒛|𝒔)
=E𝑞𝝓(𝒛|𝒔)
log𝑝𝝍(𝒔|𝒛)𝑝(𝒛)
𝑞𝝓(𝒛|𝒔)
=E𝑞𝝓(𝒛|𝒔)
log𝑝𝝍(𝒔|𝒛)
+E𝑞𝝓(𝒛|𝒔)
log𝑝(𝒛)
𝑞𝝓(𝒛|𝒔)
=E𝑞𝝓(𝒛|𝒔)
log𝑝𝝍(𝒔|𝒛)
|                         {z                         }
reconstruction term−𝐷KL(𝑞𝝓(𝒛|𝒔)𝑝(𝒛))
|                    {z                    }
prior matching term
≥E𝑞𝝓(𝒛|𝒔)
log𝑝𝝍(𝒔|𝒛)
(8)
where the first term measures the reconstruction likelihood of the
decoder from variational distribution, the second term measures
how similar the learned variational distribution is to a prior belief
held over latent variables. Maximizing the ELBO is thus equivalent
to maximizing its first term and minimizing its second term. Since
the KL divergence term of the ELBO can be computed analytically,
and the reconstruction term can be approximated using a Monte
Carlo estimate:
arg max
𝝓,𝝍E𝑞𝝓(𝒛|𝒔)
log𝑝𝝍(𝒔|𝒛)
−𝐷KL(𝑞𝝓(𝒛|𝒔)𝑝(𝒛))
≈arg max
𝝓,𝝍𝑀∑︁
𝑚=1log𝑝𝝍(𝒔|𝒛(𝑚))−𝐷KL(𝑞𝝓(𝒛|𝒔)𝑝(𝒛))(9)
where latents{𝒛(𝑚)}𝑀
𝑚=1are sampled from 𝑞𝝓(𝒛|𝒔), for every ob-
servation 𝒔in the traning sample.
Conditional diffusion model. To optimize the conditional
diffusion model 𝜃, the training objective is to use the Variational
Lower Bound (VLB) to optimize the log-likelihood of 𝒙0:
L𝑉𝐿𝐵(𝒙0;𝜃)=logE[𝑝𝜃(𝒙0)]
=logE[∫
𝑝𝜃(𝒙0:𝑁)d𝒙1:𝑁]
≤logE𝑞(𝒙1:𝑁|𝒙0)[𝑞(x𝑁|x0)
𝑝𝜃(x𝑁)
|       {z       }
L𝑁+𝑁∑︁
𝑛=2log𝑞(x𝑛−1|x0,x𝑛)
𝑝𝜃(x𝑛−1|x𝑛)
|                   {z                   }
L𝑛−1]
(10)
Next we will provide detailed to show how we estimate VLB.
The termL𝑛−1makes𝑝𝜃(𝒙𝑛−1|𝒙𝑛)to approximate the tractable
distribution 𝑞(𝒙𝑛−1|𝒙0,𝒙𝑛). Through Bayes rules, we can derive
the probability of any intermediate value 𝒙𝑛−1given its successor
𝒙𝑛and initial 𝒙0as:
𝑞(𝒙𝑛−1|𝒙𝑛,𝒙0)=𝑞(𝒙𝑛|𝒙𝑛−1,𝒙0)𝑞(𝒙𝑛−1|𝒙0)
𝑞(𝒙𝑛|𝒙0)(11)
𝑞(𝒙𝑛−1|𝒙𝑛,𝒙0)=N(𝒙𝑛−1;˜𝝁𝑛,˜𝛽𝑛𝑰) (12)
where ˜𝜇𝑛(𝒙𝑛,𝒙0)=√𝛼𝑛(1−¯𝛼𝑛−1)
1−¯𝛼𝑛𝒙𝑛+√¯𝛼𝑛−1𝛽𝑛
1−¯𝛼𝑛𝒙0,
˜𝛽𝑛=1−¯𝛼𝑛−1
1−¯𝛼𝑛𝛽𝑛.(13)
˜𝜇𝑛denotes the reparameterized mean of 𝑞(𝒙𝑛−1|𝒙0,𝒙𝑛). There-
after, for 1≤𝑛≤𝑁−1, the the parameterization of L𝑉𝐿𝐵 at step𝑛can be calculated by pushing 𝝁𝜃(𝒙𝑛,𝑛)to be close to ˜𝝁𝑛(𝒙𝑛,𝒙0).
Then, we can similarly factorize 𝝁𝜃(𝒙𝑛,𝑛)via
𝝁𝜃(𝒙𝑛,𝑛)=√𝛼𝑛(1−¯𝛼𝑛−1)
1−¯𝛼𝑛𝒙𝑛+√¯𝛼𝑛−1(1−𝛼𝑛)
1−¯𝛼𝑛𝒇𝜃(𝒙𝑛,𝑛)(14)
where𝑓𝜃(𝒙𝑛,𝑛)is the predicted 𝒙0based on 𝒙𝑛and diffusion step
𝑛. And theL𝑉𝐿𝐵 at step𝑛can be formula as :
L𝑉𝐿𝐵𝑛=Ex0
log𝑞(x𝑛|x0,x𝑛+1)
𝑝𝜃(x𝑛|x𝑛+1)
=Ex01
2||𝜎𝜃||2||˜𝜇𝑛(x𝑛,x0)−𝜇𝜃(x𝑛,𝑛)||2
=Ex01
2||𝜎𝜃||2||√𝛼𝑛(1−¯𝛼𝑛−1)
1−¯𝛼𝑛x𝑛+√¯𝛼𝑛−1𝛽𝑛
1−¯𝛼𝑛x0−
(√𝛼𝑛(1−¯𝛼𝑛−1)
1−¯𝛼𝑛x𝑛+√¯𝛼𝑛−1𝛽𝑛
1−¯𝛼𝑛𝑓𝜃(x𝑛,𝑛))||2
=√¯𝛼𝑛−1𝛽𝑛
1−¯𝛼𝑛
2||𝜎𝜃||2Ex0[||x0−𝑓𝜃(x𝑛,𝑛)||2],(15)
In practice, to keep training stability and simplify the calculation, we
following the previous work [ 35], ignore the learning of 𝝈𝜃(𝒙𝑛,𝑛)
in𝑝𝜃(𝒙𝑛−1|𝒙𝑛)of Eq. (6) and directly set 𝝈𝜃(𝒙𝑛,𝑛)=˜𝛽𝑛.Then the
optimization of training loss can be further simplified as:
minLVLB(𝒙0;𝜃)=min"
||˜𝜇(x𝑁)||2+𝑁∑︁
𝑛=2||x0−𝑓𝜃(x𝑛,𝑛)||2#
→min"𝑁∑︁
𝑛=2||x0−𝑓𝜃(x𝑛,𝑛)||2# (16)
Optimization of Conda. In conclusion, we combine and min-
imize the loss of the conditional diffusion model and VAE via
L𝑉𝐿𝐵(𝒙0;𝜃)+𝜆·L𝑣𝑎𝑒(𝒔;𝜙,𝜓)to optimize Conda, where the hyper-
parameter𝜆ensures the two terms in the same magnitude.
Alternating training. Unlike the common diffusion models that
are trained for the direct generation of raw graph data through
pre-training, Conda requires the historical neighbor sequence em-
beddings of nodes obtained through the CTDG encoder before
performing the diffusion process. Therefore, we utilize alternative
training method to alternatively train the CTDG model and Conda.
Here we briefly describe the training process. Initially, the CTDG
model𝜉is trained by minimizing the L𝑐𝑡𝑑𝑔 for𝑅𝑐𝑡𝑑𝑔rounds. Then
we insert Conda into the intermediate layer of the CTDG model and
train the𝜃,𝜙,𝜓 according toL𝑉𝐿𝐵(𝒙0;𝜃)+𝜆·L𝑣𝑎𝑒(𝒔;𝜙,𝜓)with
the𝜉frozen for𝑅𝑐𝑜𝑛𝑑𝑎 rounds. Next, we train the CTDG model 𝜉
again for𝑅𝑐𝑡𝑑𝑔rounds with the 𝜃,𝜙,𝜓 frozen. At this point, Conda
is in the inference phase, used to generate augmented historical
neighbor embeddings via the reverse process. The above process
will be repeated several times.
4 EXPERIMENTS
In this section, we evaluate the performance of our method on link
prediction task across various CTDG models. All the experiments
are conducted on open CTDG datasets.
2903Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD ’24, August 25–29, 2024, Barcelona, Spain.
MethodWiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 Social.Evo_0.1
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
JODIE 85.57±1.77 89.16±0.51 94.01±1.02 94.82±0.81 70.57±0.15 75.44±0.09 84.43±0.62 87.44±0.25 64.94±1.22 66.17±1.14 77.11±0.90 82.86±0.76
JODIE+Conda 88.63±0.93 90.52±0.40 94.55±0.82 94.98±0.65 72.11±0.47 75.79±0.42 84.57±0.70 87.71±0.59 65.09±1.24 66.25±1.17 78.74±0.71 83.46±0.66
DyRep 93.86±0.06 93.37±0.09 94.24±0.75 94.64±0.29 71.45±1.36 76.81±1.33 69.19±0.90 73.70±0.71 65.86±0.54 66.61±0.59 77.57±0.64 82.99±0.60
DyRep+Conda 94.05±0.13 93.89±0.22 94.84±0.64 94.98±0.22 72.88±1.12 77.02±1.07 69.47±0.94 73.91±0.85 66.37±0.51 66.94±0.48 78.97±0.43 83.62±0.37
TGAT 93.16±0.29 93.25±0.30 94.02±0.12 94.01±0.13 74.44±1.27 74.13±1.92 73.54±1.05 73.01±1.22 69.93±0.30 70.42±0.33 80.03±0.46 85.09±0.41
TGAT+Conda 94.02±0.27 94.11±0.29 94.88±0.10 94.90±0.09 76.15±1.43 76.97±1.85 73.99±1.10 73.74±1.18 69.94±0.28 70.48±0.41 81.79±0.60 85.83±0.57
TGN 93.01±1.22 92.77±1.05 94.94±0.09 94.23±0.12 77.30±0.50 75.62±0.72 88.00±0.99 87.73±1.14 73.61±0.74 72.75±0.68 78.79±0.64 78.60±0.48
TGN+Conda 93.47±1.16 92.93±1.09 95.50±0.14 95.06±0.18 78.63±0.75 76.52±0.81 88.22±0.92 87.79±1.06 74.24±0.80 73.31±0.75 80.20±0.57 80.03±0.48
TCL 93.48±0.27 92.98±0.23 95.05±0.11 94.92±0.12 74.12±0.74 78.10±0.65 85.14±1.54 84.87±1.50 60.07±0.15 58.97±0.17 83.57±1.22 85.41±1.04
TCL+Conda 94.04±0.20 93.75±0.17 95.96±0.08 95.79±0.07 76.31±1.20 79.55±1.13 85.79±1.37 85.40±1.28 60.15±0.44 59.21±0.45 86.09±1.25 88.45±1.17
GraphMixer 94.02±0.13 93.73±0.12 94.93±0.06 94.62±0.06 74.15±1.92 78.07±1.67 90.10±1.51 89.83±1.42 71.11±0.07 70.33±0.09 82.40±1.09 86.24±0.92
GraphMixer+Conda 94.61±0.19 94.26±0.20 95.17±0.09 94.85±0.08 75.24±1.66 78.79±1.42 90.33±1.70 90.00±1.64 71.70±0.18 70.85±0.19 83.89±0.84 88.31±0.77
DyGFormer 95.74±0.11 95.54±0.09 96.01±0.08 96.00±0.07 75.47±0.96 77.49±0.81 91.02±0.85 90.74±0.67 77.86±0.14 77.01±0.10 84.25±0.73 87.64±0.62
DyGFormer+Conda 96.68±0.14 96.32±0.13 96.68±0.14 96.55±0.12 76.12±1.41 77.98±1.33 91.09±1.00 90.97±0.92 78.84±0.35 78.11±0.27 85.64±0.58 88.78±0.40
Table 1: Experiments results on dataset with 0.1 ratio of train set
MethodWiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 Social.Evo_0.3
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
JODIE 90.23±0.41 91.47±0.35 95.63±0.37 95.72±0.33 72.28±0.64 75.41±0.59 85.17±0.88 87.45±0.27 65.62±1.54 67.04±1.44 78.65±0.58 83.43±0.54
JODIE+Conda 91.54±0.55 92.01±0.48 96.55±0.36 96.60±0.28 73.75±0.85 75.97±0.54 85.67±1.33 87.79±1.01 66.13±1.87 67.75±1.60 79.42±0.78 84.05±0.70
DyRep 94.11±0.16 93.96±0.13 95.78±0.34 95.90±0.15 73.46±1.17 78.04±1.11 71.40±0.75 75.71 ±1.57 67.41±0.86 68.02±0.91 77.12±0.50 78.51±0.48
DyRep+Conda 94.42±0.33 94.15±0.29 96.58±0.28 96.63±0.21 74.23±1.07 77.69±1.05 70.33±0.27 74.31±1.40 67.94±0.75 68.35±0.92 80.65±0.63 81.00±0.57
TGAT 94.33±0.25 94.10±0.20 95.96±0.08 96.02±0.06 78.31±0.97 80.11±0.78 71.04±0.43 72.43±0.63 70.65±0.38 70.22±0.40 82.59±0.78 86.97±0.74
TGAT+Conda 94.99±0.20 94.58±0.19 96.58±0.10 96.77±0.09 80.04±1.23 80.85±1.10 71.44±0.50 72.69±0.65 71.64±0.27 71.08±0.31 83.22±0.80 87.41±0.79
TGN 95.90±0.32 95.66±0.30 96.25±0.13 96.12±0.10 82.31±1.86 81.49±2.03 88.96±0.57 88.02±0.51 75.68±1.74 74.95±1.55 81.85±0.32 85.73±0.28
TGN+Conda 96.52±0.37 96.24±0.35 96.97±0.16 96.78±0.15 82.55±1.65 81.63±1.79 88.92±0.68 88.09±0.64 76.61±1.66 75.13±1.47 84.20±0.44 88.36±0.35
TCL 95.75±0.10 95.01±0.11 96.50±0.06 96.42± 0.05 80.20±0.14 81.98±0.21 88.62±0.24 87.84±0.19 62.23±0.88 59.83±0.75 91.57±0.25 93.64±0.22
TCL+Conda 96.23±0.20 95.58±0.24 97.02±0.12 96.87±0.11 80.47±0.35 82.24±0.32 88.69±0.30 87.93±0.27 62.79±1.14 60.56±1.02 91.88±0.23 93.95±0.19
GraphMixer 95.72±0.05 95.47±0.02 96.46±0.03 96.12±0.01 79.72±0.37 82.22±0.26 91.14±1.03 90.68±0.94 72.17±0.14 72.05±0.13 87.86±0.63 90.73±0.58
GraphMixer+Conda 95.99±0.05 95.75±0.03 96.89±0.04 96.65±0.03 80.01±0.58 82.52±0.51 91.84±1.00 90.87±0.93 72.62±0.16 72.17±0.16 89.05±0.57 91.89±0.53
DyGFormer 96.50±0.06 96.36±0.05 97.14±0.03 97.07±0.03 79.42±0.49 83.09±0.31 92.87±0.62 92.45±0.53 82.57±0.48 81.79±0.40 88.91±0.47 91.54±0.43
DyGFormer+Conda 97.11±0.09 96.96±0.10 97.45±0.04 97.31±0.04 81.00±0.83 83.97±0.53 92.94±0.48 92.50±0.40 83.17±0.36 82.31±0.32 90.02±0.53 92.79±0.50
Table 2: Experiments results on the dataset with 0.3 ratio of train set
4.1 Experiment settings
4.1.1 Datasets. We utilize six open CTDG datasets: Wiki, RED-
DIT, MOOC, LastFM, Social Evo, and UCI in the experiments. The
detailed description and the statistics of the datasets are shown in
Table 7 in Appendix A.1. The sparsity of the graphs is quantified us-
ing the density score, calculated as2|𝐸|
|𝑉|(|𝑉|−1), where|𝐸|and|𝑉|
represent the number of links and nodes in the training set, respec-
tively. These datasets are split into three chronological segments
for training, validation, and testing with ratios of 10%-10%-80% and
30%-20%-50%. To differentiate the datasets with different splitting
ratios, the dataset names are written with suffix 0.1 and 0.3.
4.1.2 Baselines. Since Conda is agnostic to model structure, to
evaluate the performance of our GDA method, we conduct experi-
ments on several state-of-the-art CTDG models, including JODIE [ 23],DyRep [ 31], TGAT [ 6], TGN [ 29], TCL [ 34], GraphMixer [ 5], DyG-
Former [ 44]. We also combine our method with other data aug-
mentation methods: DropEdge [ 28], DropNode [ 8], and MeTA [ 36].
Detailed descriptions of these baselines and GDA methods can be
found in Appendix A.2 and Appendix A.3, respectively.
4.1.3 Evaluation and hyper-parameter settings. We evaluate
Conda on the task of link prediction. As for the evaluation met-
rics, we follow the previous works [ 36,44], employing Average
Precision (AP) and Area Under the Receiver Operating Character-
istic Curve (A-R) as the evaluation metrics. We perform the grad
search to find the best settings of some critical hyper-parameters.
We vary the learning rates of all baselines in {1𝑒−4,1𝑒−3}, the
dropout rate of dropout layer in {0.0,0.1,0.2,0.3,0.4,0.5}, the num-
ber𝐿of sampled neighbors and the diffusion length 𝑑𝑖𝑓𝑓𝑢𝑠𝑖𝑜𝑛
2904KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
MethodWiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 Social.Evo_0.3
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
TGN 95.90±0.32 95.66±0.30 96.25±0.13 96.12±0.10 82.31±1.86 81.49±2.03 88.96±0.57 88.02±0.51 75.68±1.74 74.95±1.55 81.85±0.32 85.73±0.28
TGN+DropEdge 96.11±0.69 95.69±0.92 96.79±0.86 96.72±0.72 82.46±1.91 81.54±1.54 88.93±0.90 88.09±0.89 75.91±1.92 75.02±1.50 82.15±0.51 86.20±0.48
TGN+DropNode 96.03±0.73 95.97±0.76 96.87±0.87 96.63±0.63 82.35±1.35 81.62±1.62 88.96±0.95 88.05±0.50 75.77±1.76 75.00±1.00 82.31±0.67 86.55±0.60
TGN+MeTA 96.20±0.91 95.97±0.97 96.75±0.75 96.61±0.61 82.31±1.31 81.60±1.60 88.92±0.92 88.05±0.54 76.47±1.47 75.12±1.12 83.27±0.63 86.94±0.56
TGN+Conda 96.52±0.37 96.24±0.35 96.97±0.16 96.78±0.15 82.55±1.65 81.63±1.79 88.92±0.68 88.09±0.64 76.61±1.66 75.13±1.47 84.20±0.44 88.36±0.35
GraphMixer 95.72±0.05 95.47±0.02 96.46±0.03 96.12±0.01 79.72±0.37 82.22±0.26 91.14±1.03 90.68±0.94 72.17±0.14 72.05±0.13 87.86±0.63 90.73±0.58
GraphMixer+DropEdge 95.98±0.27 95.70±0.24 96.63±0.62 96.49±0.49 79.74±0.73 82.48±0.47 91.45±0.45 90.83±0.83 72.31±0.30 72.08±0.18 88.91±0.90 0.84±0.83
GraphMixer+DropNode 95.90±0.32 95.60±0.30 96.72±0.72 96.32±0.31 79.75±0.74 82.42±0.41 91.47±0.47 90.74±0.74 72.25±0.25 72.17±0.16 88.76±0.76 90.88±0.87
GraphMixer+MeTA 95.97±0.97 95.47±0.47 96.59±0.59 96.49±0.48 79.87±0.86 82.44±0.43 91.29±0.29 90.81±0.81 72.18±0.17 72.15±0.14 88.34±0.94 90.97±0.96
GraphMixer+Conda 95.99±0.05 95.75±0.03 96.89±0.04 96.65±0.03 80.01±0.58 82.52±0.51 91.84±1.00 90.87±0.93 72.62±0.16 72.17±0.16 89.05±0.57 91.89±0.53
DyGFormer 96.50±0.06 96.36±0.05 97.14±0.03 97.07±0.03 79.42±0.49 83.09±0.31 92.87±0.62 92.45±0.53 82.57±0.48 81.79±0.40 88.91±0.47 91.54±0.43
DyGFormer+DropEdge 96.71±0.24 96.48±0.21 97.35±0.14 97.12±0.12 80.37±0.63 83.94±0.50 92.88±0.55 92.46±0.32 82.84±0.62 82.17±0.20 89.09±0.62 91.70±0.49
DyGFormer+DropNode 96.72±0.24 96.51±0.21 97.43±0.15 97.20±0.15 79.52±0.64 83.86±0.46 92.93±0.19 92.48±0.58 82.83±0.64 82.14±0.32 89.04±0.84 92.24±0.51
DyGFormer+MeTA 96.57±0.33 96.38±0.28 97.26±0.79 97.19±0.62 79.53±1.05 83.40±0.82 92.60±0.94 92.49±0.05 83.09±0.51 81.94±0.14 89.93±0.53 91.64±0.21
DyGFormer+Conda 97.11±0.09 96.96±0.10 97.45±0.04 97.31±0.04 81.00±0.83 83.97±0.53 92.94±0.48 92.50±0.40 83.17±0.36 82.31±0.32 90.02±0.53 92.79±0.50
Table 3: Performance comparison of baseline and baseline with different GDA methods on the dataset with 0.3 ratio of train set
MethodWiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 Social.Evo_0.1
AP A-R AP A-R AP A-R AP A-R AP A-R AP A-R
TGN 93.01±1.22 92.77±1.05 94.94±0.09 94.23±0.12 77.30±0.50 75.62±0.72 88.00±0.99 87.73±1.14 73.61±0.74 72.75±0.68 78.79±0.64 78.60±0.48
TGN+DropEdge 93.18±1.40 92.90±1.51 95.12±0.58 94.38±0.33 76.62±1.98 75.60±1.62 87.95±1.69 87.73±1.25 74.12±1.05 72.85±1.77 78.66±1.58 78.06±1.10
TGN+DropNode 92.82±1.42 92.71±1.48 95.26±0.69 94.44±0.41 76.90±1.54 75.93±1.91 88.04±1.28 87.77±1.90 73.86±1.69 73.23±1.28 79.21±1.37 79.50±1.21
TGN+MeTA 92.96±1.45 92.85±1.37 95.20±0.84 93.83±0.50 78.03±1.88 76.38±1.88 87.93±1.87 87.73±1.47 74.15±1.03 73.11±1.19 79.43±1.31 78.57±1.08
TGN+Conda 93.47±1.16 92.93±1.09 95.50±0.14 95.06±0.18 78.63±0.75 76.52±0.81 88.22±0.92 87.79±1.06 74.24±0.80 73.31±0.75 80.20±0.57 80.03±0.48
GraphMixer 94.02±0.13 93.73±0.12 94.93±0.06 94.62±0.06 74.15±1.92 78.07±1.67 90.10±1.51 89.83±1.42 71.11±0.07 70.33±0.09 82.40±1.09 86.24±0.92
GraphMixer+DropEdge 94.34±0.71 93.61±0.43 94.97±0.31 94.84±0.24 75.02±1.72 78.06±1.11 90.14±1.37 89.77±1.30 70.75±0.68 70.51±0.40 82.88±1.16 85.90±1.02
GraphMixer+DropNode 93.97±0.74 93.89±0.45 94.90±0.39 94.82±0.40 73.54±1.98 78.15±1.06 90.06±1.64 89.61±1.50 71.28±0.73 70.41±0.70 82.67±1.94 86.12±1.82
GraphMixer+MeTA 93.72±0.96 94.21±0.72 94.98±0.39 94.58±0.33 74.35±1.55 77.97±1.44 90.11±2.00 89.86±1.64 71.70±0.41 70.75±0.58 82.74±1.88 87.02±1.61
GraphMixer+Conda 94.61±0.19 94.26±0.20 95.17±0.09 94.85±0.08 75.24±1.66 78.79±1.42 90.33±1.70 90.00±1.64 71.70±0.18 70.85±0.19 83.89±0.84 88.31±0.77
DyGFormer 95.74±0.11 95.54±0.09 96.01±0.08 96.00±0.07 75.47±0.96 77.49±0.81 91.02±0.85 90.74±0.67 77.86±0.14 77.01±0.10 84.25±0.73 87.64±0.62
DyGFormer+DropEdge 96.05±0.57 95.68±0.33 96.22±0.42 95.90±0.30 75.92±1.15 77.76±1.98 91.05±1.41 90.87±1.26 76.93±1.30 77.91±1.81 85.18±1.55 88.71±1.54
DyGFormer+DropNode 96.02±0.61 95.68±0.52 95.54±0.45 95.50±0.36 74.94±1.87 77.20±1.30 90.89±1.88 90.74±1.06 78.20±1.15 77.17±1.63 83.76±1.75 88.16±1.34
DyGFormer+MeTA 96.24±0.58 95.83±0.33 96.05±0.61 96.00±0.49 75.70±1.21 77.64±1.00 90.76±1.64 90.96±1.45 77.96±1.27 77.99±1.25 85.48±1.72 88.59±1.55
DyGFormer+Conda 96.68±0.14 96.32±0.13 96.68±0.14 96.55±0.12 76.12±1.41 77.98±1.33 91.09±1.00 90.97±0.92 78.84±0.35 78.11±0.27 85.64±0.58 88.78±0.40
Table 4: Performance comparison of baseline and baseline with different GDA methods on the dataset with 0.1 ratio of train set
in{10,20,32,64,128,256,512}and{1,𝐿
16,𝐿
8,𝐿
4,𝐿
3}, respectively. The
number of diffusion steps 𝑁is fixed at 50, respectively. Besides, the
noise scale𝑘is tuned in{1𝑒−5,1𝑒−4,1𝑒−3,1𝑒−2}. Regarding GDA
methods used for comparison, we vary the drop rate for Dropedge
and Dropnode in{0.1,0.2,0.3,0.4,0.5}. As for MeTA, we control
the magnitude of the three DA strategies with a unified 𝑝, vary in
{0.1,0.2,0.3}, and follow the setting in its paper. More details are
listed in the Appendix.
The configurations of the baselines align with those specified
in their respective papers. The model that achieves the highest
performance on the validation set is selected for testing. We conduct
five runs of each method with different seeds and report the averageperformance to eliminate deviations. All experiments are performed
on a server with 8 NVIDIA A100-SXM4 40GB GPUs.
4.2 Performance Comparison and Discussion
In this section, in order to verify the effectiveness of Conda, we
integrate it into each baseline across six datasets with different
ratios of the train set for the link prediction task. As shown in
Table 1 and Table 2, mean and standard deviations of five runs are
reported, and the best results are highlighted in bold font. The
experiment results clearly demonstrate that Conda improves the
performance of all the baselines with respect to all datasets with
different ratios of train sets.
2905Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD ’24, August 25–29, 2024, Barcelona, Spain.
From the results, we can observe that with the ratio of train set
decreasing, the performance of each baseline also decreases. Specif-
ically, when the training data is relatively sufficient, all baselines
achieve great performance. However, when training data is more
limited, the performance of most baselines drops significantly (e.g.
JODIE on Wiki_0.1, all baselines on Reddit_0.1, MOOC_0.1 and
SocialEvo_0.1). The possible reason is that the paradigm of CTDG
models is to use historical data to obtain target node embeddings.
When historical data is limited, the quality of the obtained embed-
dings cannot be guaranteed. In addition, the data distribution of the
testing set could be diverse from the training set. This would lead to
the model overfitting the historical data and cannot be generalized
to future data. By using Conda, the model’s performance improves.
It is achieved by utilizing the conditional diffusion model to gener-
ate augmented historical neighbor embeddings of the target node
during the training of the CTDG model. In Conda, the mechanism
of partial noise addition and conditional inputs ensures that the
newly generated embeddings are not random. Instead, they closely
resemble the embeddings of recently interacted neighbors. Con-
sequently, this guarantees high-quality embeddings of the node’s
historical neighbors following augmentation.
In addition, we also compare Conda with three GDA methods
to show the superiority of our GDA method. Overall, we find that
Conda can consistently outperform competing GDA methods. Fur-
thermore, the variance in the results indicates that Conda provides
stable improvements in model performance, unlike other GDA
methods that rely on random augmentations and thus yield erratic
results. This stability is particularly evident in the setting with more
sparse training data (e.g. 0.1 train set ratio). By analyzing the experi-
ments on datasets with a 0.3 train set ratio, it’s obvious that all GDA
methods can improve baselines’ performance on most datasets to
some extent, but the improvement on UCI_0.3 is relatively minor
compared to SocialEvo_0.3. The reason may be that SocialEvo has
a smaller sparsity and longer interaction sequences than UCI. This
phenomenon suggests that training the CTDG model indeed re-
quires sufficient historical interaction data. Moreover, we notice
that MeTA improves baselines’ performance than DropEdge and
DropNode. This might be because MeTA considers the time pertur-
bation, which is crucial for dynamic graph learning. Additionally,
unlike Dropedge and DropNode, which essentially remove edges,
MeTA maintains or even increases the number of interaction data
samples before and after augmentation by simultaneously adding
and removing edges. However, due to the combination of multiple
augmentation strategies, the results of MeTA also introduce a larger
variance, indicating that it is hard to control.
Next, we analyze the results of the experiment on datasets with a
0.1 train set ratio. Table 4 clearly shows that, apart from our method,
which still achieves stable performance improvements, the other
three methods frequently resulted in outcomes even worse than
the original baseline. The main reason is that at such a low ratio
of train sets, the datasets become extremely sparse. At this level,
employing random perturbations like edge deletion further reduces
the already insufficient data samples and risks removing essential
interaction. However, our method maintains stable performance
gains by controlling the diffused sequence length. Even though
the dataset becomes sparser and the historical neighbors of nodes
decrease, by simultaneously reducing the length of the diffusedWIKI_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1
GraphMixer 94.02±0.13 94.93±0.06 74.15±1.92 90.10±1.51 71.11±0.07
+Conda w/o VAE 94.58±0.17 95.12±0.08 74.90±2.17 90.30±1.67 71.65±0.18
+Conda w/o diffusion 93.87±0.25 94.80±0.13 74.77±1.80 90.15±1.91 71.43±0.30
+Conda 94.61±0.1995.17±0.0975.24±1.6690.33±1.7071.70±0.18
Table 5: Experiment results on AP of different variants
WIKI_0.1 MOOC_0.1 WIKI_0.3 MOOC_0.3
GraphMixer 94.02±0.13 74.15±1.92 95.72±0.05 79.72±0.37
+Conda (E2E) 94.09 ±0.58 73.62± 1.90 95.49±0.13 78.53 ±0.90
+Conda (AT) 94.61±0.19 75.24±1.66 95.99±0.05 80.01±0.58
Table 6: AP of different training approaches
sequence, we still ensure a stable, albeit somewhat reduced, level of
improvement compared to the results on datasets with a 0.3 ratio of
the train set. The effect of the diffused length on model performance
will be analyzed in detail in the following section.
4.3 Ablation analysis
We conduct an ablation study to assess the contributions of the
VAE and diffusion components within the Conda module. The
results, summarized in Table 5, compare the baseline GraphMixer,
add Conda without VAE (+Conda w/o VAE), add Conda without
diffusion (+Conda w/o diffusion), and add the full Conda module
(+Conda).
It is obvious that the full Conda module achieves the highest AP
scores on all datasets, and consistently outperforms all variants,
indicating the importance of both VAE and diffusion components.
Removing the diffusion component results in a performance drop,
particularly on WIKI_0.1 (from 94.61 to 93.87), highlighting the dif-
fusion’s role in generating effective latent representation. Similarly,
removing the VAE component also decreases AP scores, especially
on MOOC_0.1 (from 75.24 to 74.77). In conclusion, the combination
of the VAE and diffusion model results in superior performance, as
shown by consistently higher AP scores compared to the ablated
variants. This synergy is crucial for optimal model performance.
In addition to the ablation study of the Conda module, we also
explore different training approaches to further understand their
impact on model performance. As shown in Table 6, we conduct
experiments on GraphMixer+Conda with two different training
approaches: end-to-end training (E2E) and alternative training (AT).
The results indicate that the E2E training approach results in a
significant performance decline across all datasets. For example,
on the MOOC_0.1 and MOOC_0.3, the AP drops from 75.24.61
(AT) to 73.62 (E2E) and from 80.01 (AT) to 78.53 E2E), respectively.
This decline can be attributed to conflicting objectives between the
Conda module and the CTDG model. The Conda module aims to
generate embeddings similar to the original data, while the CTDG
model seeks to learn from diverse augmented data close to reality
to enhance performance. When integrated into end-to-end training,
these conflicting goals prevent the model from optimally achiev-
ing both objectives, leading to suboptimal or even diminishing
performance.
2906KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
Figure 2: Performance comparison of different diffused length 𝑑𝑖𝑓𝑓 in DyGFormer+Conda on Reddit, MOOC, SocialEvo with
different train set ratios. The blue and orange dashed lines respectively represent the baseline’s AP and A-R values.
Figure 3: Performance comparison of different noise scale 𝑘in GraphMixer+Conda on Reddit_0.3, MOOC_0.3, SocialEvo_0.3.
𝑘=0equivalents to the baseline.
4.4 Sensitivity of Hyper-Parameters
In this section, we conduct experiments to investigate the Sensitiv-
ity of two important hyper-parameters in our proposed method:
diffused sequence length 𝑑𝑖𝑓𝑓 and noise scale 𝑘.
We conducted experiments with DyGFormer on Reddit, MOOC,
and SocialEvo datasets, as DyGFormer tends to yield better results
from longer historical neighbor sequences on these datasets, which
can effectively show the effect on the model performance of using
varying diffused lengths across the historical neighbor sequence.
We also provide the optimal configurations of the number 𝐿of
sampled neighbors and diffused sequence length 𝑑𝑖𝑓𝑓 of different
baselines on different datasets in Appendix A.4. Specifically, we first
set𝐿, the number of sampled historical neighbors by DyGFormeron each dataset, to the optimal settings which can be found in the
Appendix A.4. Note that in practice, if the target node’s historical
neighbors are fewer than 𝐿, we use zero-padding to fill the gap.
Then, we vary 𝑑𝑖𝑓𝑓 in the set 1,𝐿
16,𝐿
8,𝐿
4,𝐿
3, with results as shown
in the Figure 2. From the Figure, we can observe that the model per-
formance is best when 𝑑𝑖𝑓𝑓 =𝐿
8. However, as 𝑑𝑖𝑓𝑓 increases, such
as when𝑑𝑖𝑓𝑓 =𝐿
3, the model performance significantly decreases,
even falling below the baseline. This phenomenon is particularly
pronounced when the training set ratio of the dataset is 0.1. The
underlying reason is likely due to the fact that there are few actual
neighbors in the sampled historical neighbor sequence of the node,
2907Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD ’24, August 25–29, 2024, Barcelona, Spain.
with the latter part of the sequence being filled by zero-padding.
Therefore, the majority of the conditional inputs of the reverse
process are meaningless, resulting in the generated augmented
embeddings being too distant from the current node state repre-
sentation, leading to a decline in model performance. When 𝑑𝑖𝑓𝑓
is less than𝐿
8, the model performance is slightly worse than at𝐿
8
but still outperforms the baseline. This phenomenon indicates that
Conda can consistently generate positive augmentation when it
controls the length of diffused sequence embedding to be relatively
short.
For the noise scale, we conduct experiments on GraphMixer+Conda
with Reddit_0.3, MOOC_0.3, and SocialEvo_0.3 . As illustrated in
Figure 3, we can observe that, as the noise scale increases, the per-
formance first rises compared to training without noise ( 𝑘=0),
verifying the effectiveness of denoising training. However, enlarg-
ing noise scales degrades the performance due to corrupting the
pattern of interaction sequence. Hence, we also should carefully
choose a relatively small noise scale (e.g. 1𝑒−4).
5 RELATED WORK
5.1 Graph Data Augmentation
There is a growing interest among researchers in graph data aug-
mentation (GDA) methods since they offer an attractive solution in
denoising and generally augmenting graph data. GDA methods can
be categorized into structure-oriented and feature-oriented meth-
ods by the data modality that they aim to manipulate. Structure-
oriented GDA methods often modify the graph structure via adding
or removing edges and nodes. Zhao et al . [47] and Gasteiger et al .
[9]modify the graph structure and used the modified graph for
training/inference, Feng et al . [8], Rong et al . [28] randomly drop
edges/nodes from the observed training graph. Wang et al . [38]
utilizes a node-centric strategy to crop a subgraph from the origi-
nal graph while maintaining its connectivity. However, these GDA
methods are usually used in static graphs or DTDG and can not be
directly applied to CTDG due to the lack of consideration of time,
Although Wang et al . [36] introduces MeTA for CTDG model, which
augments CTDG combining three structure-oriented GDA methods
including perturbing time, removing edges, and adding edges with
perturbed time. However, it is limited to apply on CTDG models
with memory modules [ 29,31] because it needs to incorporate a
multi-level memory module to process augmented graphs of vary-
ing magnitudes at different levels. Furthermore, It is widely noticed
that the effectiveness structure-oriented GDA methods requires a
great of specific domain knowledge, necessitating the selection of
diverse augmentation strategy combinations tailored to different
graph datasets.
Feature-oriented methods directly modify or create raw fea-
tures. Hou et al . [15] uses Attribute Masking that randomly mask
node features, Kong et al . [21] augments node features with gradient-
based adversarial perturbations. It’s worth noting that structure-
oriented and feature-oriented augmentation are also sometimes
combined in some GDA methods. For example, You et al . [43] sum-
marizes four types of graph augmentations to learn the invariant
representation across different augmented view. Wang et al . [39]
changes both the node feature and the graph structure for differentnodes individually and separately, to coordinate DA for different
nodes. However, most of these methods require original features for
nodes or edges. Meanwhile, Most CTDG datasets are attribute-free
graphs. Additionally, there are only a few structure-oriented and
feature-oriented GDA methods that offer rigorous proofs or theoret-
ical bounds (e.g., Evidence Lower Bound). Most rely predominantly
on empirical intuition or constraints from contrastive learning to
achieve positive augmentation.
5.2 Generative Models
Generative models [ 10,19] are powerful tools for learning data
distribution. Recently, researchers have proposed several interest-
ing generative models for graph data generation. Variational graph
auto-encoder (VGAE) [ 20] exploits the latent variables to learn in-
terpretable representations for undirected graphs. Salha et al . [30]
make use of a simple linear model to replace the GCN encoder in
VGAE and reduce the complexity of encoding schemes. Xu et al .
[40]propose a generative GCN model to learn node representations
for growing graphs. ConDgen [ 41] exploits the GCN encoder to han-
dle the invariant permutation for conditional structure generation.
Besides, diffusion-based generative models are shown to be power-
ful in generating high-quality graphs [ 4,13,27,33]. DiGress [ 33],
one of the most advanced graph generative models, employs a dis-
crete diffusion process to progressively add discrete noise to graphs
by either adding or removing edges and altering node categories.
However, these diffusion models rely on continuous Gaussian noise
and do not align well with graph structure. In addition, they are
limited to generating small graphs and can not scale up to large
graphs. Contrary to these approaches mainly focusing on structure
generation, [ 25] pretrains a VAE for node feature generation, which
can serve as a DA method for the downstream backbone models.
However, VAE often uses over-simplified prior and decoder, which
suffers from the trade-off between tractability and representation
ability.
6 CONCLUSION
In this paper, we propose Conda, a novel GDA method designed
to integrate seamlessly into any CTDG model. Conda utilizes a
latent conditional diffusion model to enhance the embeddings of
nodes’ historical neighbor sequences during the training phase of
CTDG models. Unlike structure-oriented and feature-oriented GDA
methods, Conda operates within the latent space rather than the
input space, thereby enabling more subtle modeling of transition in
dynamic graphs. More importantly, Conda employs a conditional
diffusion model to generate high-quality historical neighbor em-
beddings with solid theoretical foundations. Extensive experiments
conducted on various baseline models using real-world datasets
demonstrate the efficacy of our method Conda. In the future, we
aim to extend our method to CTDG with edge deletions.
7 ACKNOWLEDGMENTS
The research presented in this paper is supported in part by the
National Natural Science Foundation of China (Grant No. 62372362)
and the National Natural Science Foundation of China (Grant No.
62366021).
2908KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
REFERENCES
[1]Tanya Y. Berger-Wolf and Jared Saia. 2006. A Framework for Analysis of Dynamic
Social Networks. In Proceedings of the 12th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (Philadelphia, PA, USA) (KDD ’06).
Association for Computing Machinery, New York, NY, USA, 523–528.
[2]Xiaofu Chang, Xuqin Liu, Jianfeng Wen, Shuang Li, Yanming Fang, Le Song, and
Yuan Qi. 2020. Continuous-time dynamic graph learning via neural interaction
processes. In Proceedings of the 29th ACM International Conference on Information
& Knowledge Management. 145–154.
[3]Nan Chen, Zemin Liu, Bryan Hooi, Bingsheng He, Rizal Fathony, Jun Hu, and
Jia Chen. 2024. Consistency Training with Learnable Data Augmentation for
Graph Anomaly Detection with Limited Supervision. In The Twelfth International
Conference on Learning Representations.
[4]Xiaohui Chen, Yukun Li, Aonan Zhang, and Li-Ping Liu. 2023. NVDiff: Graph
Generation through the Diffusion of Node Vectors. arXiv:2211.10794 [cs.LG]
[5]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang
Tong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated Model
Architectures For Temporal Networks? arXiv:2302.11636 (2023).
[6]da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. 2020.
Inductive representation learning on temporal graphs. In International Conference
on Learning Representations (ICLR).
[7]Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data Augmentation
for Deep Graph Learning: A Survey. SIGKDD Explor. Newsl. 24, 2 (dec 2022),
61–77. https://doi.org/10.1145/3575637.3575646
[8]Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang
Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural networks
for semi-supervised learning on graphs. In Proceedings of the 34th International
Conference on Neural Information Processing Systems (Vancouver, BC, Canada)
(NIPS’20). Curran Associates Inc., Red Hook, NY, USA, Article 1853, 12 pages.
[9]Johannes Gasteiger, Stefan Weißenberger, and Stephan Günnemann. 2019. Diffu-
sion improves graph learning. Curran Associates Inc., Red Hook, NY, USA.
[10] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
networks. In Advances in Neural Information Processing Systems.
[11] Derek Greene, Dónal Doyle, and Pádraig Cunningham. 2010. Tracking the
Evolution of Communities in Dynamic Social Networks. In 2010 International
Conference on Advances in Social Networks Analysis and Mining. 176–183. https:
//doi.org/10.1109/ASONAM.2010.17
[12] Hongyu Guo and Yongyi Mao. 2021. ifmixup: Towards intrusion-free graph
mixup for graph classification. arXiv e-prints (2021), arXiv–2110.
[13] Kilian Konstantin Haefeli, Karolis Martinkus, Nathanaël Perraudin, and Roger
Wattenhofer. 2022. Diffusion Models for Graphs Benefit From Discrete State
Spaces. In The First Learning on Graphs Conference.
[14] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graph
data augmentation for graph classification. In International Conference on Machine
Learning. PMLR, 8230–8248.
[15] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,
and Jie Tang. 2022. GraphMAE: Self-Supervised Masked Graph Autoencoders. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (Washington DC, USA) (KDD ’22). Association for Computing Machinery,
New York, NY, USA, 594–604. https://doi.org/10.1145/3534678.3539321
[16] Zijie Huang, Yizhou Sun, and Wei Wang. 2020. Learning continuous system
dynamics from irregularly-sampled partial observations. Advances in Neural
Information Processing Systems 33 (2020), 16177–16187.
[17] Ming Jin, Yuan-Fang Li, and Shirui Pan. 2022. Neural Temporal Walks: Motif-
Aware Representation Learning on Continuous-Time Dynamic Graphs. In Ad-
vances in Neural Information Processing Systems.
[18] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi,
Peter Forsyth, and Pascal Poupart. 2020. Representation learning for dynamic
graphs: A survey. The Journal of Machine Learning Research 21, 1 (2020), 2648–
2720.
[19] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[20] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv
preprint arXiv:1611.07308 (2016).
[21] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem,
Gavin Taylor, and Tom Goldstein. 2020. Flag: Adversarial data augmentation for
graph neural networks. arXiv preprint arXiv:2010.09891 (2020).
[22] Srijan Kumar and Neil Shah. 2018. False Information on Web and Social Media:
A Survey. arXiv:1804.08559 [cs.SI]
[23] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-
bedding Trajectory in Temporal Interaction Networks. (08 2019).
[24] Bo Liang, Lin Wang, and Xiaofan Wang. 2022. Autoregressive GNN-ODE GRU
Model for Network Dynamics. arXiv:2211.10594 (2022).
[25] Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, Peilin
Zhao, Junzhou Huang, and Dinghao Wu. 2022. Local Augmentation for Graph
Neural Networks. In Proceedings of the 39th International Conference on MachineLearning, Vol. 162. PMLR, 14054–14072.
[26] Linhao Luo, Reza Haffari, and Shirui Pan. 2022. Graph Sequential Neural ODE
Process for Link Prediction on Dynamic and Sparse Graphs. (11 2022). https:
//doi.org/10.48550/arXiv.2211.08568
[27] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and
Stefano Ermon. 2020. Permutation Invariant Graph Generation via Score-Based
Generative Modeling. In Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,
Vol. 108), Silvia Chiappa and Roberto Calandra (Eds.). PMLR, 4474–4484.
[28] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. Dropedge: To-
wards deep graph convolutional networks on node classification. In International
Conference on Learning Representation.
[29] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning
on Dynamic Graphs. (06 2020).
[30] Guillaume Salha, Romain Hennequin, and Michalis Vazirgiannis. 2019. Keep
it simple: Graph autoencoders without graph convolutional networks. arXiv
preprint arXiv:1910.00942 (2019).
[31] Rakshit S. Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In International
Conference on Learning Representations.
[32] Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho
Kannala, and Jian Tang. 2021. Graphmix: Improved training of gnns for semi-
supervised learning. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 35. 10024–10032.
[33] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,
and Pascal Frossard. 2023. DiGress: Discrete Denoising diffusion for graph
generation. In The Eleventh International Conference on Learning Representations.
[34] Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng
He, Le Song, Jingren Zhou, and Hongxia Yang. 2021. Tcl: Transformer-based
dynamic graph modelling via contrastive learning. arXiv:2105.07944 (2021).
[35] Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, and Tat-Seng Chua.
2023. Diffusion Recommender Model. arXiv:2304.04971 [cs.IR]
[36] Yiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, Siddharth
Bhatia, and Bryan Hooi. 2021. Adaptive data augmentation on temporal graphs.
Advances in Neural Information Processing Systems 34 (2021), 1440–1452.
[37] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.
Inductive representation learning in temporal networks via causal anonymous
walks. arXiv preprint arXiv:2101.05974 (2021).
[38] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2020.
GraphCrop: Subgraph Cropping for Graph Classification. CoRR abs/2009.10564
(2020).
[39] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi.
2020. NodeAug: Semi-Supervised Node Classification with Data Augmentation.
207–217.
[40] Da Xu, Chuanwei Ruan, Kamiya Motwani, Evren Korpeoglu, Sushant Kumar,
and Kannan Achan. 2019. Generative graph convolutional network for growing
graphs. In International Conference on Acoustics, Speech and Signal Processing.
[41] Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. 2019. Conditional
Structure Generation through Graph Variational Generative Adversarial Nets..
InAdvances in Neural Information Processing Systems.
[42] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph Con-
trastive Learning Automated. In Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 12121–12132.
[43] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
Neural Information Processing Systems (2020).
[44] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better Dynamic
Graph Learning: New Architecture and Unified Library. arXiv:2303.13047 (2023).
[45] Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. 2022. Label-invariant
Augmentation for Semi-Supervised Graph Classification. In Advances in Neural
Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 29350–29361.
[46] Shilei Zhang, Toyotaro Suzumura, and Li Zhang. 2021. DynGraphTrans: Dynamic
Graph Embedding via Modified Universal Transformer Networks for Financial
Transaction Data. In 2021 IEEE International Conference on Smart Data Services
(SMDS). 184–191. https://doi.org/10.1109/SMDS53860.2021.00032
[47] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil
Shah. 2020. Data Augmentation for Graph Neural Networks. CoRR abs/2006.06830
(2020).
[48] Yanping Zheng, Zhewei Wei, and Jiajun Liu. 2023. Decoupled Graph Neural
Networks for Large Dynamic Graphs. arXiv:2305.08273
2909Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model KDD ’24, August 25–29, 2024, Barcelona, Spain.
Dataset Nodes Edges Unique Edges Node/Link Feature Time Granularity Duration density
WIKI 9227 157474 18257 0/172 Unix timestamp 1 month 4.30E-03
REDDIT 10984 672447 78516 0/172 Unix timestamp 1 month 8.51E-03
MOOC 7144 411749 178443 0/4 Unix timestamp 17 month 1.26E-02
LastFM 1980 1293103 154993 0/0 Unix timestamp 1 month 5.57E-01
Social Evo. 74 2099519 4486 0/2 Unix timestamp 8 months 5.36E+02
UCI 1899 59835 20296 0/0 Unix timestamp 196 days 3.66E-02
Table 7: Dataset statistics
A APPENDIX
A.1 Detail Descriptions of Datasets
•Wiki: is a bipartite interaction graph that contains the edits on
Wikipedia pages over a month. Nodes represent users and wiki
pages, and links denote the editing behaviors with timestamps.
Each link is associated with a 172-dimensional Linguistic Inquiry
and Word Count (LIWC) feature.
•Reddit: consists of one month of posts made by users on sub-
reddits. Users and subreddits are the nodes, and links are the
timestamped posting requests. Each link has a 172-dimensional
LIWC feature.
•MOOC: is a bipartite interaction network of online sources,
where nodes are students and course content units (e.g., videos
and problem sets). Each link denotes a student’s access behavior
to a specific content unit and is assigned a 4-dimensional feature.
•UCI: is a Facebook-like, unattributed online communication net-
work among students of the University of California at Irvine,
along with timestamps with the temporal granularity of seconds.
•LastFM: is an interaction network where users and songs are
nodes and each edge represents a user-listens-to-song relation.
The dataset consists of the relations of 1000 users listening to
the 1000 most listened songs over a period of one month. The
dataset contains no attributes.
•SocialEvo: is a mobile phone proximity network that tracks the
everyday life of a whole undergraduate dormitory from October
2008 to May 2009. Each edge has 2 features.
A.2 Detail Descriptions of baselines
•JODIE is a RNN-based method. Denote ℎ𝑢(𝑡)as the embedding
of node𝑢at timestamp 𝑡,𝑒𝑡
𝑢,𝑣as the link feature between 𝑢,
𝑣at timestamp 𝑡, and𝑚𝑢as the timestamp that node 𝑢latest
interact with other nodes. When an interaction between node 𝑢,
𝑣happens at timestamp 𝑡, JODIE updates the node embedding
using RNN by ℎ𝑢(𝑡)=𝑅𝑁𝑁(ℎ𝑢(𝑚𝑢),ℎ𝑣(𝑚𝑣),𝑒𝑡
𝑢,𝑣,𝑡−𝑚𝑢). Then,
the embedding of node 𝑢at timestamp 𝑡0is computed as ℎ𝑢(𝑡0)=
(1+(𝑡0−𝑚𝑢)𝑤)·ℎ𝑢(𝑚𝑢).
•TGAT is a self-attention-based method that could capture spa-
tial and temporal information simultaneously. TGAT first con-
catenates the raw feature 𝑥𝑢with a trainable time encoding
𝑧(𝑡), i.e.,𝑥𝑢(𝑡)=[𝑥𝑢||𝑧(𝑡)]and𝑧(𝑡)=𝑐𝑜𝑠(𝑡𝑤+𝑏). Then, self-
attention is applied to produce node representation ℎ𝑢(𝑡0)=
𝑆𝐴𝑀(𝑥𝑢(𝑡0),𝑥𝑣(𝑚𝑣)|𝑣∈𝑁𝑡0(𝑢)), where𝑁𝑡0(𝑢)denotes the neigh-
bors of node 𝑢at time𝑡0and𝑚𝑢denotes the timestamp of thelatest interaction of node 𝑢. Finally, the prediction on any node
pair at time 𝑡0is computed by 𝑀𝐿𝑃([ℎ𝑢(𝑡0)||ℎ𝑣(𝑡0)]).
•TGN is a mixture of RNN- and self-attention based method. TGN
utilizes a memory module to store and update the (memory) state
𝑠𝑢(𝑡)of node𝑢. The state of node 𝑢is expected to represent
𝑢’s history in a compressed format. Given the memory updater
as mem, when an link 𝑒𝑢𝑣(𝑡)connecting node 𝑢is observed,
node𝑢’s state is updated as 𝑠𝑢(𝑡)=𝑚𝑒𝑚(𝑠𝑢(𝑡−),𝑠𝑣(𝑡−)||𝑒𝑢𝑣(𝑡)).
where𝑠𝑢(𝑡−)is the memory state of node 𝑖just before time
𝑡.||is the concatenation operator, and node 𝑣is𝑢’s neighbor
connected by 𝑒𝑢,𝑣(𝑡). The implementation of 𝑚𝑒𝑚 is a recurrent
neural network (RNN), and node 𝑢’s embedding is computed by
aggregating information from its L-hop temporal neighborhood
using self-attention.
•DyRep is an RNN-based method that updates node states upon
each interaction. It also includes a temporal-attentive aggregation
module to consider the temporally evolving structural informa-
tion in dynamic graphs.
•TCL is a contrastive learning-based method. It first generates
each node’s interaction sequence by performing a breadth-first
search algorithm on the temporal dependency interaction sub-
graph. Then, it presents a graph transformer that considers both
graph topology and temporal information to learn node repre-
sentations. It also incorporates a cross-attention operation for
modeling the inter-dependencies of two interaction nodes.
•GraphMixer is a simple MLP-based architecture. It uses a fixed
time encoding function that performs rather than the trainable
version and incorporates it into a link encoder based on MLP-
Mixer to learn from temporal links. A node encoder with neighbor
mean-pooing is employed to summarize node features.
•DyGFormer is a self attetion-based method. Specifically, for
node𝑛𝑖, DyGFormer just retrieves the features of involved neigh-
bors and links based on the given features to represent their en-
codings. DyGFormer is equipped with a neighbor co-occurrence
encoding scheme, which encodes the appearing frequencies of
each neighbor in the sequences of the source node and destina-
tion node, and can explicitly explore the correlations between
two nodes. Instead of learning at the interaction level, DyG-
Former splits each source/destination node’s sequence into mul-
tiple patches and then feeds them to the transformer.
A.3 Descriptions of GDA Methods in
experiments
•DropEdge which randomly drops edges according to the drop
possibility𝑝at data pre-process phase. This slight modification
2910KDD ’24, August 25–29, 2024, Barcelona, Spain. Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi
of the original graph results in the GNN observing a different
graph at each epoch.
•DropNode Similar to DropEdge, DropNode drops nodes accord-
ing to the drop possibility 𝑝at the data pre-process phase.
•MeTA a dynamic graph data augmentation module that stacks a
few levels of memory modules to augment dynamic graphs of
different magnitudes on separate levels with three data augmen-
tation strategies, including perturbing time, removing edges, and
adding edges with perturbed time.
A.4 Detail configurations
Dataset Wiki_0.3 Reddit_0.3 MOOC_0.3 UCI_0.3 LastFM_0.3 SocialEvo_0.3
JODIE 10/𝐿
410/𝐿
410/𝐿
410/1 10/𝐿
310/𝐿
3
DyRep 10 /𝐿
410/𝐿
410/𝐿
410/1 10/𝐿
310/𝐿
3
TGAT 20 /𝐿
420/𝐿
420 /𝐿
820/𝐿
820/𝐿
820/𝐿
4
TGN 10 /𝐿
410 /𝐿
310/𝐿
410/𝐿
410/𝐿
410/𝐿
3
TCL 20/𝐿
820/𝐿
820/𝐿
820/𝐿
820/𝐿
820/𝐿
4
GraphMixer 32/𝐿
810/𝐿
320/𝐿
420/𝐿
810/1 20/𝐿
4
DyGFormer 32/𝐿
464/𝐿
8128/𝐿
832/𝐿
8256/𝐿
832/𝐿
4
Table 8: The optimal configurations of the number 𝐿of sam-
pled neighbors and length 𝑑𝑖𝑓𝑓 of diffused sequenceDataset Wiki_0.1 Reddit_0.1 MOOC_0.1 UCI_0.1 LastFM_0.1 SocialEvo_0.1
JODIE 10/𝐿
410/𝐿
410/𝐿
410/1 10/1 10/𝐿
4
DyRep 10/1 10/1 10/1 10/1 10/1 10/1
TGAT 20/𝐿
820/𝐿
820/𝐿
820/𝐿
820/𝐿
820/𝐿
8
TGN 10/1 10/𝐿
410/𝐿
410/1 10/1 10/𝐿
4
TCL 20/𝐿
820/𝐿
820/𝐿
820/1 20/𝐿
820/𝐿
4
GraphMixer 20/𝐿
810/𝐿
810/𝐿
410/𝐿
810/𝐿
820/𝐿
8
DyGFormer 32/𝐿
820/𝐿
832/𝐿
832/1 64/𝐿
1632/𝐿
8
Table 9: The optimal configurations of the number 𝐿of sam-
pled neighbors and length 𝑑𝑖𝑓𝑓 of diffused sequence
2911