Effective Edge-wise Representation Learning in Edge-Attributed
Bipartite Graphs
Hewen Wang
National University of Singapore
Singapore, Singapore
wanghewen@u.nus.eduRenchi Yang
Hong Kong Baptist University
Hong Kong SAR, China
renchi@hkbu.edu.hkXiaokui Xiao
National University of Singapore
Singapore, Singapore
xkxiao@nus.edu.sg
ABSTRACT
Graph representation learning (GRL) is to encode graph elements
into informative vector representations, which can be used in down-
stream tasks for analyzing graph-structured data and has seen ex-
tensive applications in various domains. However, the majority of
extant studies on GRL are geared towards generating node represen-
tations, which cannot be readily employed to perform edge-based
analytics tasks in edge-attributed bipartite graphs (EABGs) that per-
vade the real world, e.g., spam review detection in customer-product
reviews and identifying fraudulent transactions in user-merchant
networks. Compared to node-wise GRL, learning edge representa-
tions (ERL) on such graphs is challenging due to the need to incor-
porate the structure and attribute semantics from the perspective
of edges while considering the separate influence of two heteroge-
neous node setsUandVin bipartite graphs. To our knowledge,
despite its importance, limited research has been devoted to this
frontier, and existing workarounds all suffer from sub-par results.
Motivated by this, this paper designs EAGLE , an effective ERL
method for EABGs. Building on an in-depth and rigorous theoretical
analysis, we propose the factorized feature propagation (FFP) scheme
for edge representations with adequate incorporation of long-range
dependencies of edges/features without incurring tremendous com-
putation overheads. We further ameliorate FFP as a dual-view FFP
by taking into account the influences from nodes in UandVsever-
ally in ERL. Extensive experiments on 5 real datasets showcase the
effectiveness of the proposed EAGLE models in semi-supervised
edge classification tasks. In particular, EAGLE can attain a consider-
able gain of at most 38.11%in AP and 1.86%in AUC when compared
to the best baselines.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíSupervised learning by classi-
fication; Factorization methods; ‚Ä¢Mathematics of computing
‚ÜíGraph algorithms.
KEYWORDS
graph representation learning, edge classification, attributed graph,
bipartite graph
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671805ACM Reference Format:
Hewen Wang, Renchi Yang, and Xiaokui Xiao. 2024. Effective Edge-wise
Representation Learning in Edge-Attributed Bipartite Graphs. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671805
1 INTRODUCTION
Edge-attributed bipartite graphs (EABGs) (a.k.a. attributed interac-
tion graphs [ 62]) are an expressive data structure used to model the
interactive behaviors between two sets of objects UandVwhere
the behaviors are characterized by rich attributes. Practical exam-
ples of EABGs include reviews from users/reviewers on movies,
businesses, products, and papers; transactions between users and
merchants; and disease-protein associations.
In real life, EABGs have seen widespread use in detecting spam
reviews in e-commerce [ 60], malicious incidents in telecommunica-
tion networks [ 52], fraudulent transactions/accounts in finance [ 44]
or E-payment systems [ 25], abusive behaviors in online retail web-
sites [ 41], insider threats from audit events [ 7], and others [ 5,48].
The majority of such applications can be framed as edge-based
prediction or classification tasks in EABGs.
In recent years, graph representation learning (e.g., graph neural
networks and network embedding) has emerged as a popular and
powerful technique for graph analytics and has seen fruitful success
in various domains [ 14]. In a nutshell, GRL seeks to map graph
elements in the input graph into feature representations (a.k.a. em-
beddings), based on which we can perform downstream prediction
or classification tasks. However, to our knowledge, most of the
existing GRL models, e.g., GCN [ 21], GraphSAGE [ 13] and GAT
[39], are devised for learning node-wise representations in node-
attributed graphs, and edge-wise representation learning (ERL), espe-
cially on EABGs, is as of yet under-explored. A common treatment
for obtaining edge representations is to directly apply the canonical
node-wise GRL models [ 3,13,17,21,39,40,46] to generate node em-
beddings, followed by concatenating them as the embeddings of the
corresponding edges. Despite its simplicity, this methodology falls
short of not only the accurate preservation of the graph topology
from the perspective of edges (demanding an effective combina-
tion of node embeddings) but also the incorporation of the edge
attributes in EABGs, thereby resulting in compromised embedding
quality. Another category of workarounds is to simply transform
the original EABGs into node-attributed unipartite graphs by con-
verting the edges into nodes and connecting them if they share
common endpoints in the input EABGs. In doing so, the node-wise
GRL techniques can be naturally adopted on such projected graphs
for deriving edge representations. Unfortunately, aside from infor-
mation loss of the bipartite structure in the input EABG Gby the
3081
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hewen Wang, Renchi Yang, & Xiaokui Xiao
simple transformation [ 55,64], such projection-based approaches
rely on constructing an edge-to-edge graph G‚Ä≤, which often entail
immense space consumption (up to ùëÇ(ùëö2)in the worst case) due
to the scale-free property of real-world graph G, i.e., a few nodes
connecting to a significant amount of nodes in Gand creating a
multitude of edge-to-edge associations in G‚Ä≤[54]. Recently, sev-
eral efforts [ 2,6,19,42] have been specifically invested towards
learning edge-wise feature representations. However, these ERL
models are either designed for unipartite graphs or hypergraphs
and hence, cannot readily be applied to EABGs for high-quality
representations, as they are unable to capture the unique character-
istics of bipartite graphs, particularly the underlying semantics of
connections to nodes in UandVfrom two heterogeneous sources.
To remedy the deficiencies of existing works, this paper presents
EAGLE (Edge-wise Bip Artite Graph Representation LEarning) for
effective ERL in EABGs. By taking inspiration from the numeric
analysis [ 27,53,66] of the most popular GRL solutions, i.e., classic
message-passing (a.k.a. feature-propagation) GNNs, we begin by
formalizing the edge-wise representation learning objective as an
optimization problem, while considering the respective influence of
edge associations with two sets of heterogeneous nodes UandV.
Through our theoretical insights into the optimal solution to the
optimization problem, the derived feature propagation rules, and
their connections to the well-established Markov chain theory, we
unveil the necessity of preserving long-range dependencies [ 49] of
edges in edge representations on EABGs. Based thereon, we propose
afactorized feature propagation (FFP) scheme to enable efficient
and effective long-range feature propagation for generating edge
representations in EAGLE. Furthermore, we upgrade EAGLE with
thedual-view factorized feature propagation (DV-FFP) for flexible
and full exploitation of semantics from two sets of nodes UandV
in EABGs. More precisely, instead of combining edge associations to
UandVvia a given hyperparameter for subsequent ERL, DV-FFP
learns two sets of edge embeddings using the connections to Uand
V, respectively, followed by an aggregator function that combines
them as the final representations. Following previous work, our
EAGLE models are trained by feeding the final edge representations
into the loss function for the semi-supervised edge classification.
We evaluate the proposed EAGLE models against 9 baselines
on 5 real EABGs in terms of semi-supervised edge classification
tasks. The experimental results exhibit that our EAGLE models
consistently achieve the best empirical performance over 5 datasets
with remarkable gains, further validating the effectiveness of FFP
and DV-FFP schemes in EAGLE. Notably, on the academic graphs
AMiner and OAG dataset, EAGLE can obtain 38.11%and11.97%
performance gains in terms of average precision (AP) over the
best competitor, indicating the superiority of EAGLE in learning
predictive edge representations on EABGs.
The remainder of this paper is structured as follows. After pre-
senting the preliminaries and formal problem definition in Section
2, we design the basic EAGLE model with FFPin Section 3. We
further introduce an enhanced EAGLE model with dual-view FFP
in Sections 4. Experiments are conducted in Section 5. Section 6
reviews related studies, and Section 7 concludes the paper.2 PRELIMINARIES
Throughout this paper, sets are denoted by calligraphic letters, e.g.,
V, and|V|is used to denote the cardinality of the set V. Matrices
(resp. vectors) are written in bold uppercase (resp. lowercase) letters,
e.g.,M(resp. x). The superscript M‚ä§is used to symbolize the trans-
pose of matrix M.M[ùëñ](M[:,ùëñ]) is used to represent the ùëñ-th row
(resp. column) of matrix M. Accordingly, M[ùëñ,ùëó]denotes the(ùëñ,ùëó)-th
entry in matrix M. For each vector M[ùëñ], we use‚à•M[ùëñ]‚à•to represent
itsùêø2norm, i.e.,‚à•M[ùëñ]‚à•=‚àöÔ∏É√çùëë
ùëó=1M[ùëñ,ùëó]2and‚à•M‚à•ùêπto represent
the Frobenius norm of M, i.e.,‚à•M‚à•ùêπ=‚àöÔ∏É√çùëõ
ùëñ=1√çùëë
ùëó=1M[ùëñ,ùëó]2.
2.1 Edge-Attributed Bipartite Graphs
Definition 2.1 (Edge-Attributed Bipartite Graphs (EABG)).
An EABG is defined as G=(U‚à™V,E,X), whereUandVrep-
resent two disjoint node sets, Econsists of the inter-set edges
connecting nodes in UandV, and each edge ùëíùëñis associated with
a length-ùëëattribute vector X[ùëñ].
ùë¢1ùë¢2ùë¢3ùë¢4ùë¢5
ùë£1ùë£2ùë£3ùë£4ùë£5ùë£6ùí∞
ùí±‚Ñ∞
Figure 1: An Example EABG
Figure 1 exemplifies an EABG Gin online retail platforms (e.g.,
Amazon and eBay) with 5 users ùë¢1-ùë¢5inU, 6 products ùë£1-ùë£6inV,
and the user-product interactions in E. Each interaction (i.e., edge)
is associated with a review (i.e., edge attributes) from the user on
the product.
For each node ùë¢ùëñ‚ààU (resp.ùë£ùëó‚ààV),Eùë¢ùëñ(resp.Eùë£ùëó) symbolizes
the set of edges incident to ùë¢ùëñ(resp.ùë£ùëó). We use DU‚ààR|U|√ó|U|
(resp. DV‚ààR|V|√ó|V|) to represent the diagonal matrix whose
diagonal entries correspond to the degrees of nodes in U(resp.
V), e.g., DU[ùë¢ùëñ,ùë¢ùëñ]=|Eùë¢ùëñ|andDV[ùë£ùëñ,ùë£ùëñ]=|Eùë£ùëñ|. Then, D=DU 0
0 DV
‚ààR(|U|+|V|)√ó(|U|+|V|)is the diagonal node degree
matrix ofG. Further, we denote by EU‚ààR|E|√ó|U|andEV‚àà
R|E|√ó|V|the edge-node indicator matrices for node sets UandV,
respectively. More precisely, for each edge ùëíùëñ‚ààEand its two end
pointsùë¢(ùëñ),ùë£(ùëñ), we have EU[ùëíùëñ,ùë¢(ùëñ)]=EV[ùëíùëñ,ùë£(ùëñ)]=1. For other
nodesùë¢‚ààU\ùë¢(ùëñ)andùë£‚ààV\ùë£(ùëñ),EU[ùëíùëñ,ùë¢]=EV[ùëíùëñ,ùë£]=0.
On the basis of DU,EUandDV,EV, we define edge-wise tran-
sition matrix PUandPVas follows:
PU=EUD‚àí1
UE‚ä§
UandPV=EVD‚àí1
VE‚ä§
V. (1)
Lemma 2.2 unveils a unique property of PUandPV, which is
crucial to the design of our EAGLE model.
Lemma 2.2. PUandPVare doubly stochastic matrices.
3082Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
2.2 Problem Formulation
We formalize the edge representation learning (ERL) in EABGs as
follows. Given an EABG G=(U‚à™V,E,X), the task of ERL aims
to build a model ùëì:E‚Üí Z‚ààR|E|√óùëß(ùëß‚â™|E| ), which transforms
each edgeùëíùëñ‚ààEinto a length- ùëßvector Z[ùëíùëñ]as its feature represen-
tation. Such a feature representation Z[ùëíùëñ]should capture the rich
semantics underlying both the bipartite graph structures and edge
attributes. In this paper, we focus on the edge classification task,
and thus, the edge representations are learned in a semi-supervised
fashion by plugging the loss function for classifying edges into the
modelùëì.
3 THE EAGLE MODEL
As illustrated in Figure 2, we have developed two ERL models for
EABG, i.e., EAGLE with FFPand dual-view FFP, both of which
involve two key steps: ùëò-truncated singular value decomposition
(SVD) and feature propagation.
In this section, we focus on introducing our base EAGLE model,
i.e.,EAGLE with FFP. Section 3.1 first presents the objective of
learning the edge-wise representations, while Section 3.2 then offers
an in-depth analysis of the solution to the optimization objective. In
Section 3.3, we elaborate on the feature propagation mechanism for
computing the edge representations, followed by the loss function
for the model training in Section 3.4.
3.1 Representation Learning Objective
Inspired by the numeric optimization analysis of generalized graph
neural network models in recent studies [ 27,53,66], we formulate
the ERL in EABGs as an optimization problem with consideration
of the lopsided nature of bipartite graphs.
More concretely, EAGLE aims at achieving two goals: (i) the edge
representations Zclose to the input edge feature matrix; and (ii)
representations of edges that are incident to the same nodes should
be similar. The former corresponds to a fitting term in the following
equation:
Oùëì=‚à•Z‚àíùëìŒò(X)‚à•2
ùêπ, (2)
whereùëìŒò(X) ‚ààR|E|√óùëßrepresents a non-linear transformation
features of the input edge attribute matrix Xusing an MLP ùëìŒò(¬∑)
parameterized by a learnable weight matrix Œò‚ààRùëë√óùëß(including
a nonlinear activation function ReLU operation and dropout op-
eration), while the latter is a graph structure-based regularization
termOùëüdefined by
Oùëü=ùõΩ
2‚àëÔ∏Å
ùë¢‚ààU‚àëÔ∏Å
ùëíùëñ,ùëíùëó‚ààEùë¢1
|Eùë¢|¬∑‚à•Z[ùëíùëñ]‚àíZ[ùëíùëó]‚à•2
+1‚àíùõΩ
2‚àëÔ∏Å
ùë£‚ààV‚àëÔ∏Å
ùëíùëñ,ùëíùëó‚ààEùë£1
|Eùë£|¬∑‚à•Z[ùëíùëñ]‚àíZ[ùëíùëó]‚à•2.(3)
Intuitively, Eq. (3)forces representations Z[ùëíùëñ],Z[ùëíùëó]to be close in
the Euclidean space if their corresponding edges ùëíùëñ,ùëíùëóare correlated
to common nodes.1
|Eùë¢|(resp.1
|Eùë£|) is the weight used to reflect
the importance of ùëíùëñ,ùëíùëófrom the perspective of common node ùë¢
(resp.ùë£). In particular, we use coefficients ùõΩand1‚àíùõΩto control
the importance of edge pairs‚Äô shared nodes from UandVin
constraining the distance between the representations, respectively.Table 1: Properties of P(ùõΩ=0.5)
Dataset ùúé2ùúé2
21
1‚àíùúé2
21
1‚àíùõºùúé2
ùëò
AMiner 0.9997 0.9994 1574.3930 1.8207
OAG 0.9999 0.9997 3780.6051 1.8775
In sum, the objective of learning Zcan be formulated as follows:
min
Z‚ààR|E|√óùëß(1‚àíùõº)¬∑Oùëì+ùõº¬∑Oùëü, (4)
where hyper-parameter ùõºis to balance the above-said two terms.
3.2 Analysis of the Optimal Solution
Lemma 3.1. The closed-form solution to Eq. (4)is
Z=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°¬∑ùëìŒò(X), (5)
where Pis an edge-wise transition matrix defined by
P=ED‚àí1E‚ä§,E=‚àöÔ∏Å
ùõΩ¬∑EU‚à•‚àöÔ∏Å
1‚àíùõΩ¬∑EV. (6)
Lemma 3.1 offers a simple yet elegant way (i.e., Eq. (5)) to calcu-
late the optimal edge representations Zto the optimization objective
in Eq. (4). However, Eq. (5) requires summing up an infinite series
of matrix multiplications, which is infeasible in practice, especially
for large EABGs. A remedy is to compute an approximate version
Z‚Ä≤by summing up at most ùëá+1terms with a small integer ùëá:
Z‚Ä≤=(1‚àíùõº)ùëá‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°¬∑ùëìŒò(X). (7)
In what follows, we theoretically show that such a truncation is
not a favorable choice in EABGs.
Lemma 3.2. Given Pin Eq. (6),P=ùõΩ¬∑PU+(1‚àíùõΩ)¬∑PV.
First, by Lemma 3.2, Pis a linear combination of PUandPV.
Recall that both PUandPVare non-negative doubly stochastic,
which further connotes that Pisnon-negative doubly stochastic
and can be regarded as a reversible Markov chain. Let ùëáùëöùëñùë•be its
mixing time. Using its doubly stochastic property and the Conver-
gence Theorem in [ 22,28], whenùë°>ùëáùëöùëñùë•,Pùë°ùëìŒò(X)converges to a
stationary distribution ùö∑, wherein ùö∑[ùëíùëñ]is a constant vector, i.e.,
1¬∑‚à•ùëìŒò(X)[ùëíùëñ]‚à•1. Thus, Zin Eq. (5)can be broken down into two
parts1: 
(1‚àíùõº)ùëáùëöùëñùë•‚àí1‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°¬∑ùëìŒò(X)!
+ùõºùëáùëöùëñùë•ùö∑. (8)
Intuitively, since each row in ùö∑is a constant vector, ùõºùëáùëöùëñùë•ùö∑is not
an informative representation matrix. As such, Eq. (8)implies that if
we pick a large ùëá(ùëá‚â´ùëáùëöùëñùë•) forZ‚Ä≤, constant vectors ùõºùëáùëöùëñùë•ùö∑might
jeopardize the representation quality of Z‚Ä≤in Eq. (7), especially on
graphs with small mixing times, resulting in degraded performance.
On the other hand, a small ùëá(ùëá‚â™ùëáùëöùëñùë•‚àí1) for Z‚Ä≤fails to enable
an adequate preservation of the topological semantics underlying
the input EABGs.
Lemma 3.3. Letùúé2be the second largest singular value of ED‚àí1/2
(defined in Eq. (6)),ùëáùëöùëñùë•‚â•1
1‚àíùúé2
2‚àí1.
1For the interest of space, we defer all proofs to Appendix A.
3083KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hewen Wang, Renchi Yang, & Xiaokui Xiao
ùêó
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùëëMLPMLPMLP
Feature TransformationMLPMLPMLP
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
ùêôùë£
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
ùêôùë¢
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
Z
ùëí1
ùëí2
ùëí3
ùëí4
ùëí5
ùëí6ùë¢1ùë¢2ùë¢3ùë£1ùë£2ùë£3 ùë¢4 ùë£4
ùêÑ
ùëí6ùë£1
ùë£2
ùë£3
ùë£4ùë¢1
ùë¢2
ùë¢3
ùë¢4ùëí5ùëí4ùëí3ùëí2ùëí1
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùë¢1ùë¢2ùë¢3ùë¢4
ùêÑùë¢ùëí5
ùëí6ùë£1
ùë£2
ùë£3
ùë£4ùë¢1
ùë¢2
ùë¢3
ùë¢4ùëí4ùëí3ùëí2ùëí1ùëí5ùëí4
ùëí6ùëí3ùëí2ùëí1ùë£1
ùë£2
ùë£3
ùë£4ùë¢1
ùë¢2
ùë¢3
ùë¢4
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùë£1ùë£2ùë£3ùë£4
ùêÑùë£
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
ùêêùë£Feature
Propagationùëò-truncated 
SVD
ùí¢=(ùí∞‚à™ùí±,‚Ñ∞,ùëã)ùë¢ ùúà
ùëí1
ùëí2
ùëí3
ùëí4
ùëí5
ùëí6ùë£1
ùë£2
ùë£3
ùë£4ùë¢1
ùë¢2
ùë¢3
ùë¢4
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
ùêêùë¢ùëò
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùêêùëò
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6
ùêôùëß
ùëß
ùëßùëò-truncated 
SVD
ùëò-truncated 
SVD
ùëìùúÉ(ùêó)
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùëßùëìùúÉ(ùêó)
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùëß
ùëìùúÉ(ùêó)
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6ùëß
ùëìùúÉ(ùêó)
ùëí1ùëí2ùëí3ùëí4ùëí5ùëí6Feature
Propagation
Feature
PropagationInput EAGLE with FFP
EAGLE with Dual -View FFPMLPMLPMLPEdge 
Classification
Edge 
Classification
Figure 2: The overall framework of EAGLE
Lemma 3.3 provides a lower bound for ùëáùëöùëñùë•, which is propor-
tional to the inverse of 1‚àíùúé2
2. As per the empirical data on real
EABGs (see Section 5.2) from Table 1, ùúé2is notably approaching 1,
renderingùëáùëöùëñùë•extremely large (over thousands), as a consequence
of the unique characteristics of bipartite graph structures. We can
conclude that the ùõºùëáùëöùëñùë•ùö∑part in Z(Eq.(8)) is insignificant. Addi-
tionally, based on Section 12.2 in [ 22], given any integer ùë°and any
edgeùëíùëñ‚ààE,
ùëâùëéùëüùö∑[ùëíùëñ](Pùë°ùëìŒò(X)[ùëíùëñ])‚â§ùúé4ùë°
2¬∑ùëâùëéùëüùö∑[ùëíùëñ](ùëìŒò(X)[ùëíùëñ]),
whereùëâùëéùëüùö∑[ùëíùëñ]stands for the variance computed w.r.t. the sta-
tionary distribution ùö∑[ùëíùëñ]. Sinceùúé2is almost 1, the above equa-
tion manifests that even for a very large ùë°, the difference between
Pùë°ùëìŒò(X)[ùëíùëñ]and the stationary distribution ùö∑[ùëíùëñ]can be as signif-
icant as that of the input feature vector ùëìŒò(X)[ùëíùëñ]. That is to say,
Pùë°ùëìŒò(X)[ùëíùëñ]with largeùë°still encompasses rich and informative
features, and thus, computing Zvia Eq. (7)leads to compromised
representation quality.
3.3 Factorized Feature Propagation
However, it remains tenaciously challenging to calculate the edge
representations Zby Eq. (5). To tackle this issue, we resort to a
dimensionality reduction approach, dubbed as factorized feature
propagation (FFP). The rudimentary idea behind FFPis to construct
an|E|√óùëò(ùëò‚â™|E| ) matrix Qsuch that Q¬∑Q‚ä§‚âà(1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°
without explicitly materializing (1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°. As such, edge
representations Zin Eq. (5) can be approximated via
Z=Q¬∑(Q‚ä§ùëìŒò(X)), (9)
which can be done in ùëÇ(|E|¬∑ùëòùëß)time. To realize the above idea, FFP
first conducts a ùëò-truncated SVD over ED‚àí1
2to get its left singular
vectors Uand the diagonal matrix ùö∫containing singular values.
Then, we construct Qas
Q=U¬∑‚àöÔ∏Ç
1
1‚àíùõºùö∫2. (10)
The underlying rationale is on the basis of P=ED‚àí1
2¬∑(ED‚àí1
2)‚ä§‚âà
Uùö∫2U‚ä§. The result in Eq. (10)is a direct inference after proving allsingular values of ED‚àí1
2not greater than 1 in Lemma A.1:
(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°‚âàU¬∑‚àû‚àëÔ∏Å
ùë°=0ùõºùë°¬∑ùö∫2ùë°¬∑U‚ä§=U¬∑1
1‚àíùõºùö∫2¬∑U‚ä§.
Correctness Analysis. Theorem 3.4 establishes the approximation
accuracy guarantees of Q. In practice, we usually set ùëò=256, and
thus, the total error sum1
1‚àíùõºùúé2
ùëòis roughly 2, as reported in Table 1.
Theorem 3.4. LetQbe the|E|√óùëòmatrix defined in Eq. (10)and
ùúéùëòbe theùëò-th largest singular value of ED‚àí1
2. Then, the following
inequality holds
QQ‚ä§‚àí(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
ùêπ‚â§1
1‚àíùõºùúé2
ùëò.
In particular, when ùëò=|E|,
QQ‚ä§=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°.
Complexity Analysis. As remarked earlier, Eq. (9)consumes
ùëÇ(|E|¬∑ùëòùëß)time. According to [ 12], theùëò-truncated SVD of sparse
matrix ED‚àí1
2(comprising 2|E|elements) takes ùëÇ(|E|¬∑ùëò2+ùëò3)time
when the randomized algorithm is employed. Overall, the total
computational cost incurred by FFPis bounded by ùëÇ(|E|ùëò¬∑(ùëò+ùëß)).
3.4 Model Training
In this work, we mainly focus on the semi-supervised edge classifi-
cation task. The edge representations Zoutput by FFPare subse-
quently fed into an MLP network ùëìŒ©(¬∑)to yield the edge classifica-
tion result:
Y=sigmoid(ùëìŒ©(Z))‚àà R|E|√ó|ùê∂|(11)
where|ùê∂|is the number of classes and ùëìŒ©is parameterized by
a learnable weight matrix Œ©‚ààRùëò√ó|ùê∂|, followed by a nonlinear
activation function ReLU operation and a dropout operation. In
sum, the trainable parameters of EAGLE are only the weight matrix
Œò‚ààRùëë√óùëßin the feature transformation layer ùëìŒò(X)and the weight
matrix Œ©‚ààRùëò√ó|ùê∂|of the output layer in Eq. (11).
3084Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Following common practice, we employ the cross-entropy loss
with ground-truth edge labels to guide the model training:
L=‚àí1
|Eùêø|‚àëÔ∏Å
ùëíùëñ‚ààEùêø‚àëÔ∏Å
ùëó‚ààùê∂bY[ùëíùëñ,ùëó]¬∑log(Y[ùëíùëñ,ùëó])
+(1‚àíbY[ùëíùëñ,ùëó])¬∑log(1‚àíY[ùëíùëñ,ùëó]),
whereEùêødenotes the set of labeled edges, bYconsists of the ground-
truth labels of edges ( bY[ùëíùëñ,ùëó]=1ifùëíùëñbelongs to class ùê∂ùëóand 0
otherwise), and Y[ùëíùëñ,ùëó]stands for the predicted probability of edge
ùëíùëñbelonging to class ùê∂ùëó.
4EAGLE WITH DUAL-VIEW FFP
Recall that in Section 3.2, the edge-wise transition matrix Pcan
be equivalently converted into P=ùõΩ¬∑PU+(1‚àíùõΩ)¬∑PV(Lemma
3.2). In turn, the edge representations Zin Eq. (5)are essentially
obtained through a linear combination of the features propagated
between edges via their connections using two heterogeneous node
setsUandVas intermediaries, which tends to yield sub-optimal
representation effectiveness. Further, such a linear combination
relies on a manually selected parameter ùõΩto balance the importance
of features w.r.t. these two views, which requires re-calculating the
ùëò-truncated SVD of ED‚àí1
2(see Eq. (6)) from scratch to create Q(Eq.
(10)) onceùõΩwas changed, leading to significant computation effort.
To mitigate the foregoing issues, in EAGLE, we develop dual-view
factorized feature propagation (referred to as DV-FFP) for learning
enhanced edge representations. The basic idea is to create two
intermediate edge representations, ZUandZV, by utilizing the
associations between edges from the views of UandVseverally,
and then coalesce them into the final edge representations Z.
In the sequel, Section 4.1 elaborates on the details of DV-FFP,
followed by a theoretical analysis in Section 4.2
4.1 Dual-View Factorized Feature Propagation
Akin to Eq. (5), the goal of DV-FFP is to generate edge representa-
tions ZUandZVfrom theU-wise andV-wise views as follows:
ZU=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
U¬∑ùëìŒòU(X),
ZV=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
V¬∑ùëìŒòV(X).(12)
In Eq. (12),ùëìŒòU(X)(resp.ùëìŒòV(X)) corresponds to the initial edge
features used for the generation of ZU(resp. ZV), which is trans-
formed from the input edge attribute vectors Xthrough an MLP
network parameterized by weight matrix ŒòU(resp. ŒòV).
In analogy to FFPin Section 3.3, DV-FFP adopts a low-dimensional
matrix approximation trick to approximate (1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°
U
and(1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°
V, while sidestepping the explicit construc-
tion of these two|E|√ó|E| dense matrices. Specifically, DV-FFP
first applies a ùëò-truncated SVD over EUD‚àí1
2
UandEUD‚àí1
2
V, respec-
tively, to get the left singular vectors UU, singular values ùö∫U,
and their counterparts UVandùö∫V. Let QU=UU‚àöÔ∏Ç
1
1‚àíùõºùö∫2
UandQV=UV‚àöÔ∏Ç
1
1‚àíùõºùö∫2
V. Then, theU-wise andV-wise edge represen-
tations ZUandZVcan be computed by
ZU=QU¬∑
Q‚ä§
UùëìŒòU(X)
andZV=QV¬∑
Q‚ä§
VùëìŒòV(X)
,(13)
respectively. Afterwards, they are combined as the final edge repre-
sentations Zthrough
Z=ùëìcombine(ùõæ¬∑ZU,(1‚àíùõæ)¬∑ZV), (14)
whereùëìcombine(¬∑,¬∑)is a combinator function, which can be a summa-
tion operator+, matrix concatenation operator ‚à•, or max operator,
andùõæ‚àà[0,1]is a hyper-parameter.
4.2 Analysis
In the rest of this section, we theoretically analyze the optimization
objective of learning ZU,ZVas in Eq. (12), the approximation
accuracy guarantees of QUandQV, as well as the computational
expense of DV-FFP, respectively.
Optimization Objective. Recall that P=ùõΩ¬∑PU+(1‚àíùõΩ)¬∑PVby
Lemma 3.2. If we set ùõΩto1and0,Pin Eq. (6)turns into PUand
PV, respectively. Accordingly, Zdefined in Eq. (5)becomes ZU
andZV, if we replace ùëìŒò(X)byùëìŒòU(X)andùëìŒòV(X), respectively.
Since Lemma 3.1 indicates that Zin Eq. (5)is the closed solution to
the objective in Eq. (4), whenùõΩ=1orùõΩ=0,ZUandZVdefined
in Eq. (12)are thus the closed form solutions to the problems that
minimize the following objectives:
(1‚àíùõº)‚à•ZU‚àíùëìŒòU(X)‚à•2
ùêπ+ùõº
2‚àëÔ∏Å
ùë¢‚ààU‚àëÔ∏Å
ùëíùëñ,ùëíùëó‚ààEùë¢1
|Eùë¢|¬∑‚à•ZU[ùëíùëñ]‚àíZU[ùëíùëó]‚à•2,
(1‚àíùõº)‚à•ZV‚àíùëìŒòV(X)‚à•2
ùêπ+ùõº
2‚àëÔ∏Å
ùë£‚ààV‚àëÔ∏Å
ùëíùëñ,ùëíùëó‚ààEùë£1
|Eùë£|¬∑‚à•ZV[ùëíùëñ]‚àíZV[ùëíùëó]‚à•2,
respectively.
Correctness. Since when we set ùõΩ=1(resp.ùõΩ=0),PandED‚àí1/2
turn into PUandEUD‚àí1/2
U(resp. PVandEVD‚àí1/2
V). Letùúéùëò(U)
andùúéùëò(V) be theùëò-th largest singular value of EUD‚àí1/2
Uand
EVD‚àí1/2
V, respectively. Based on Theorem 3.4, we can derive the
following inequalities
QUQ‚ä§
U‚àí(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
U
ùêπ‚â§1
1‚àíùõºùúé2
ùëò(U),
QVQ‚ä§
V‚àí(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
V
ùêπ‚â§1
1‚àíùõºùúé2
ùëò(V).
In particular, when ùëò=|E|,
QUQ‚ä§
U=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
UandQVQ‚ä§
V=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°
V.
Complexity. The computations of ZUandZVin Eq. (13)need
ùëÇ(|E|ùëòùëß)time, respectively. The randomized ùëò-truncated SVD [ 12]
of sparse matrices EUD‚àí1
2
UandEVD‚àí1
2
VrequiresùëÇ(|E|¬∑ùëò2+ùëò3)
time. Therefore, DV-FFP andFFPhave the same time complexity
ùëÇ(|E|ùëò¬∑(ùëò+ùëß)).
3085KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hewen Wang, Renchi Yang, & Xiaokui Xiao
Table 2: Edge-Attributed Bipartite Networks
Name|E||U||V|ùëë|ùê∂|
Amazon 359,425 25,939 14,061 768 3
AMiner 54,465 39,358 641 768 10
DBLP 243,960 33,503 6497 768 10
Google 564,831 32,788 7212 768 3
MAG 50,443 38,990 1,010 768 10
5 EXPERIMENTS
In this section, we empirically study the effectiveness of our pro-
posed EAGLE models on real-world datasets in terms of edge classifi-
cation. All experiments are conducted on a Linux machine powered
by 4 AMD EPYC 7313 CPUs with 500GB RAM, and 1 NVIDIA RTX
A5000 GPU with 24GB memory. The code and all datasets are avail-
able at https://github.com/wanghewen/EAGLE for reproducibility.
5.1 Baselines and Hyperparameters
We compare our proposed solutions against 9 competitors in terms
of edge classification accuracy. The first category of baseline models
consists of node-wise representation learning methods, including
GCN [ 21], GraphSAGE [ 13], SGC [ 46], DGI [ 40], GAT [ 39], and
GATv2 [ 3]. We initialize the embeddings of edge endpoints as the
mean average of their connected edge attributes. Then, we apply
these node-wise representation learning methods to update the
node embeddings for the edge endpoints. Finally, we concatenate
the embeddings of edge endpoints along with edge attributes to
generate the corresponding edge embeddings. The second category
of baseline models consists of edge-wise representation learning
methods, including GEBE [ 56] and AttrE2Vec [ 2]. Additionally, we
include a fully connected neural network (FC) to transform edge
attributes without considering any network structure information.
For DGI, GEBE, and AttrE2Vec, we collect the source codes from
the respective authors and adopt the parameter settings suggested
in their papers to generate edge representations before feeding
them to MLPs (multi-layer perceptrons) for classification. For GCN,
GraphSAGE, SGC, GAT, and GATv2, we utilize the standard imple-
mentations provided in the well-known DGL2library and follow a
three-layer neural network architecture, including two GNN layers
and one linear layer, with ReLU as activation functions between
layers. Besides, we set the dropout rate to 0.5 and the maximum
number of training epochs to 300, and employ the Adam opti-
mizer [ 20] for optimization with a learning rate of 0.001. All the
methods are implemented in Python. In our solutions (i.e., EAGLE
(FFP) and EAGLE (DV-FFP)), unless otherwise specified, we set the
hyperparameter ùõºandùõΩto be 0.5,ùõæin Eq. (14)to be 0.5, and dimen-
sionùëòto be 256. The edge representations are then input to MLP
classifiers to obtain the final edge labels. We report the AP/AUC
on the test datasets using the model selected with the best AUC
achieved on the cross-validation datasets.
5.2 Datasets
We use 5 real-world bipartite network datasets in the experiments.
The Amazon dataset [ 29] contains user reviews for movies and TV
2https://www.dgl.aishows, where the edges represent the reviews written by users on
the products, which are associated with labels representing users‚Äô
ratings on these products. The Google dataset [ 23,51] contains
review information of business entities on Google Maps in Hawaii,
United States, where the edges are reviews written by users on
the business entity IDs. Similarly, the edge labels represent users‚Äô
ratings on the business entities. AMiner [ 38], MAG [ 33,59] and
DBLP [ 37] datasets are 3 citation networks, in which nodes repre-
sent scholars and their publication venues of a paper. The edges
represent the paper abstracts written for that paper. For AMiner,
edge labels correspond to the keywords for the papers. For DBLP
and MAG, edge labels correspond to the field of study for the papers.
We select the most frequent 10 labels as targets to be predicted. To
obtain initial edge features from text for these datasets, we apply
the Sentence-BERT [ 32] model to encode text into 768-dimensional
vectors. For each dataset, we use breadth-first search (BFS) to sam-
ple a smaller subset. Then we randomly split all edges into training,
cross-validation and test sets with an 8 : 1 : 1 ratio. The properties
and scales of the datasets used in our experiments are summarized
in Table 2.
5.3 Edge Classification
Table 3 presents the edge classification performance of all meth-
ods on five datasets. Overall, our proposed methods consistently
outperform all competitors on all five datasets. On review datasets
like Amazon and Google, our method (EAGLE (DV-FFP)) using the
max aggregator achieves approximately 0.9%-3.1% improvement in
AP and approximately 0.4%-0.7% improvement in AUC compared
to the best competitors. On citation network datasets like AMiner
using keywords as edge labels, our method (EAGLE (FFP)) achieves
around 177.5% improvement in AP and around 1.5% improvement in
AUC compared to the best competitors. Note that most of the base-
lines cannot achieve high AP (below 0.2), due to the difficulties of
classifying keywords in AMiner, as the number of keyword labels in
the original dataset is much higher than the number of labels in the
other datasets. On citation network datasets like DBLP and MAG
using the field of study as edge labels, our method (EAGLE (DV-
FFP)) using the max aggregator achieves approximately 7.1%-15.0%
improvement in AP and approximately 1.2%-1.9% improvement in
AUC compared to the best competitors. It is worth noting that for
datasets like Amazon and AMiner, FC performs the best among the
competitors. This indicates the difficulties of capturing the struc-
tural similarities in these graph datasets. However, our methods
can still successfully generate better edge representations on these
datasets. Another observation is that EAGLE (DV-FFP) outperforms
EAGLE (FFP) and other competitors on four datasets out of five.
This suggests the importance of introducing intermediate edge rep-
resentation independently from the views of heterogeneous node
setsZUandZV.
5.4 Parameter Analysis
In this section, we experimentally study the effects of varying three
key parameters in our proposed method, including ùõΩused in Eq. (6),
ùëòfor matrix Qdimension, and ùõºused in Eq. (4). We report the AUC
scores by EAGLE (FFP) and EAGLE (DV-FFP) with three different
aggregators when varying these parameters.
3086Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 3: Classification Performance (the higher the better).
MethodAmazon AMiner DBLP Google MAG
AP AUC AP AUC AP AUC AP AUC AP AUC
GCN 0.6515 0.8691 0.0966 0.9254 0.5835 0.8979 0.5396 0.8122 0.7504 0.9617
GraphSAGE 0.6927 0.8874 0.1398 0.9485 0.6785 0.9254 0.5789 0.8250 0.7998 0.9702
SGC 0.5721 0.8203 0.0360 0.8468 0.4753 0.8576 0.4728 0.7563 0.6838 0.9470
DGI 0.3879 0.6094 0.0046 0.5024 0.2785 0.6813 0.3465 0.5336 0.4278 0.8315
GAT 0.6809 0.8810 0.1197 0.9077 0.5936 0.8988 0.5040 0.6928 0.7517 0.9614
GATv2 0.6871 0.8860 0.1356 0.9206 0.6462 0.9156 0.5313 0.7804 0.7673 0.9635
FC 0.7030 0.8905 0.1877 0.9607 0.6234 0.8922 0.5585 0.7890 0.7401 0.9564
GEBE 0.4751 0.7158 0.1013 0.8739 0.4164 0.8116 0.3956 0.6508 0.6555 0.9317
AttrE2Vec 0.3334 0.4991 0.0080 0.5966 0.1716 0.5390 0.3331 0.4976 0.1480 0.5463
EAGLE (FFP) 0.6946 0.8875 0.5209 0.9754 0.7069 0.9325 0.5787 0.8151 0.9047 0.9869
EAGLE (DV-FFP)-sum 0.7059 0.8941 0.5062 0.9684 0.7160 0.9319 0.5886 0.8202 0.9083 0.9867
EAGLE (DV-FFP)-max 0.7093 0.8965 0.3740 0.9589 0.7267 0.9361 0.5968 0.8287 0.9195 0.9888
EAGLE (DV-FFP)-concat 0.7064 0.8944 0.5081 0.9660 0.7187 0.9327 0.5931 0.8248 0.9128 0.9869
EA
GLE (FFP) EA
GLE (DV-FFP)-sum EA
GLE (DV-FFP)-max EAGLE (DV-FFP)-concat
0.1 0.3 0.5 0.7 0.90.8860.8890.8920.8950.898AUC
(
a) Amazon0.1 0.3 0.5 0.7 0.90.9500.9600.9700.9800.990AUC
(
b) AMiner0.1 0.3 0.5 0.7 0.90.9300.9320.9340.9360.938AUC
(
c) DBLP0.1 0.3 0.5 0.7 0.90.8100.8150.8200.8250.830AUC
(
d) Google0.1 0.3 0.5 0.7 0.90.9860.9870.9880.9890.990AUC
(
e) MAG
Figure 3: Varying ùõΩinEAGLE (FFP) andùõæinEAGLE (DV-FFP).
23242526272829‚àû0.8840.8880.8920.8960.900AUC
(
a) Amazon23242526272829‚àû0.9400.9520.9640.9760.988AUC
(
b) AMiner23242526272829‚àû0.9200.9260.9320.9380.944AUC
(
c) DBLP23242526272829‚àû0.8000.8100.8200.8300.840AUC
(
d) Google23242526272829‚àû0.9820.9840.9860.9880.990AUC
(
e) MAG
Figure 4: Varying ùëò.
0.1 0.3 0.5 0.7 0.90.8840.8880.8920.8960.900AUC
(
a) Amazon0.1 0.3 0.5 0.7 0.90.9500.9580.9660.9740.982AUC
(
b) AMiner0.1 0.3 0.5 0.7 0.90.9260.9290.9320.9350.938AUC
(
c) DBLP0.1 0.3 0.5 0.7 0.90.8080.8150.8220.8290.836AUC
(
d) Google0.1 0.3 0.5 0.7 0.90.9820.9840.9860.9880.990AUC
(
e) MAG
Figure 5: Varying ùõº.
In Figure 3, we report how AUC scores vary for different ùõΩfor
EAGLE (FFP) and different ùõæforEAGLE (DV-FFP). For Amazon,
Google, and MAG, EAGLE (DV-FFP) with the max aggregator con-
sistently performs the best across different ùõΩ. For AMiner, the AUC
scores reach the maximum when ùõΩ=0.7and then decrease for
EAGLE (DV-FFP) with max aggregator. For DBLP, the EAGLE (FFP)performs the best when ùõΩ=0.2, but whenùõΩ‚â•0.3,EAGLE (DV-FFP)
with max aggregator becomes the best among all our methods. We
can also observe that on DBLP, Google, and MAG, EAGLE (FFP) is
more sensitive to the change of ùõΩcompared with EAGLE (DV-FFP).
In Figure 4, we report how AUC scores vary for different SVD
dimensionsùëò.‚àûin Figure 4 refers to using power iteration to solve
3087KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hewen Wang, Renchi Yang, & Xiaokui Xiao
for the edge embeddings. For Amazon, DBLP, and Google, the AUC
scores of all our methods increase as ùëòincreases (excluding the
solution from power iteration), as larger embedding dimensions
can contain more graph structural information. For AMiner, AUC
reaches the maximum when ùëò=8forEAGLE (DV-FFP) using
the sum aggregator. For MAG, AUC reaches the maximum for
EAGLE (DV-FFP) using the max aggregator when ùëò=128and then
decreases.
In Figure 5, we report how AUC scores vary for different ùõº. By
tuningùõº, we observe that EAGLE (DV-FFP) with the max aggrega-
tor can achieve the best performance on these datasets compared
with other methods. In particular, on Amazon, AMiner, DBLP, and
Google, EAGLE (DV-FFP) with the max aggregator performs best
withùõºvalue between 0.3 and 0.6. On MAG, EAGLE (DV-FFP) with
the max aggregator shows a decreasing trend as ùõºincreases. As men-
tioned in Section 3.1, ùõºbalances the importance between the edge
representations derived from edge features and graph structures.
This suggests on MAG, our methods can achieve improvements by
a careful trade-off between edge attributes and graph structures.
6 RELATED WORK
This section reviews existing graph node/edge representation learn-
ing on unipartite/bipartite graphs, as well as their applications.
6.1 Node-wise Representation Learning
Node-wise GRL refers to the process of generating embeddings for
the nodes of a graph. Conventional approaches for addressing this
task involve methods based on matrix factorization and random
walk. In matrix factorization-based methods, such as HOPE [ 30],
AROPE [ 63], PRONE [ 61], NRP [ 57], PANE [ 58], and SketchNE [ 1],
a proximity-based matrix Pis initially created for the graph, where
each element P[ùëñ,ùëó]denotes the proximity measure between nodes
ùëñandùëó. Subsequently, a dimension reduction technique is employed
to derive lower-dimensional representations for the nodes. In ran-
dom walk-based methods, such as Deepwalk [ 31], LINE [ 36], and
node2vec [ 11], the process begins with the generation of random
walks for each node to capture the underlying graph structures.
Subsequently, the co-occurrence in these random walks is employed
to assess node similarities and generate node embedding vectors.
Another line of research lies in graph neural networks (GNNs).
The major categories of GNNs, for example, GCN [ 21], Graph-
SAGE [ 13], SGC [ 46], DGI [ 40], GAT [ 39], and GATv2 [ 3], adopt
ideas from convolutional neural networks for modeling graph-
structured data. GNNs aggregate local neighborhood information
to get contextual representation for graph nodes and have shown
promising results in this area. To consider the effect of edge at-
tributes, some new GNN models are proposed to incorporate them
during the training process. EGAT [ 45] proposes edge-featured
graph attention layers that can accept node and edge features as in-
puts and handle them spontaneously within the models. GERI [ 35]
constructs a heterogeneous graph using the attribute information
and applies random walk with a modified heterogeneous skip-
gram to learn node embeddings. EEGNN [ 26] proposes a frame-
work called edge-enhanced graph neural network that uses the
structural information extracted from a Bayesian nonparametric
model for graphs to consider the effect of self-loops and multipleedges between two nodes and improve the performance of various
deep message-passing GNNs. EGNN [ 10] uses multi-dimensional
nonnegative-valued edge features represented as a tensor and ap-
plies GCN/GAT to exploit multiple attributes associated with each
edge. GraphBEAN [ 5] applies autoencoders on bipartite graphs
with both node and edge attributes to obtain node embeddings for
node and edge level anomaly detection.
6.2 Edge-wise Representation Learning
Edge-wise GRL refers to the process of generating embeddings for
edges of a graph. [ 24] uses random walks to sample a series of
edge sequences to generate edge embeddings and apply clustering
algorithms for community detection. Edge2Vec [ 6] uses deep auto-
encoders and skip-gram models to generate edge embeddings that
preserve both the local and global structure information of edges for
biomedical knowledge discovery. AttrE2Vec [ 2] generates random
walks starting from a node and uses aggregation functions to aggre-
gate node/edge features in the random walks and obtain node/edge
representations. Then, it uses auto-encoders and self-attention net-
works with feature reconstruction loss and graph structural loss
to build edge embeddings in an unsupervised manner. [ 42] uses
matrix factorization and feature aggregation to generate edge rep-
resentation vectors based on the graph structure surrounding edges
and edge attributes, which can encode high-order proximities of
edges and edge attribute information into low-dimensional vectors.
CEN-DGCNN [ 65] introduces a deep graph convolutional neural
network that integrates node and edge features, preventing over-
smoothing. It captures non-local structural features and refines
high-order node features by considering long-distance dependen-
cies and multi-dimensional edge features. DoubleFA [ 43] proposes
to use top-k Personalized PageRank to conduct proximal feature
aggregation and anomaly feature aggregation using edge features
for edge anomaly detection.
6.3 Bipartite Graph Representation Learning
For a comprehensive review of existing bipartite graph representa-
tion learning methods, we suggest readers check [ 8]. BiANE [ 18]
employs auto-encoders to model inter-partition and intra-partition
proximity using attribute proximity and structure proximity through
a latent correlation training approach. Cascade-BGNN [ 15] utilizes
customized inter-domain message passing and intra-domain align-
ment with adversarial learning for message aggregation across and
within graph partitions. BiGI [ 4] utilizes GCN to generate initial
node embeddings and applies a subgraph-level attention mecha-
nism to maximize the mutual information between local and global
node representations. DualHGCN [ 50] transforms the multiplex
bipartite network into two sets of homogeneous hypergraphs and
uses spectral hypergraph convolutional operators to capture infor-
mation within and across domains. GEBE [ 56] proposes proximity
matrices derived from the edge weight matrix and applies matrix
factorization to capture multi-hop similarity/proximity between
homogeneous/heterogeneous nodes. AnchorGNN [47] proposes a
global-local learning framework that leverages an anchor-based
message-passing schema to generate node embeddings for large
bipartite graphs.
3088Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
7 CONCLUSIONS
In this work, we introduce the problem of ERL on EABGs and
propose EAGLE models to address this problem. Building on an
in-depth theoretical analysis of extending the feature propagation
paradigm in GNNs to ERL on EABGs, we design the FFP scheme
that is able to effectively capture long-range dependencies between
edges for generating high-quality edge representations without
entailing vast computational costs. On the basis of FFP, we propose
the dual-view FFP by leveraging the semantics of two sets of het-
erogeneous nodes in the input bipartite graphs to enhance edge
representations. The effectiveness of our proposed EAGLE mod-
els is validated by our extensive experiments comparing EAGLE
against nine baselines over five real datasets.
ACKNOWLEDGMENTS
This research is supported by the Ministry of Education, Singapore,
under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). Any
opinions, findings and conclusions or recommendations expressed
in this material are those of the author(s) and do not reflect the views
of the Ministry of Education, Singapore. Renchi Yang is supported
by the NSFC Young Scientists Fund (No. 62302414) and the Hong
Kong RGC ECS grant (No. 22202623).
REFERENCES
[1]2023. SketchNE: Embedding Billion-Scale Networks Accurately in One Hour. IEEE
Transactions on Knowledge and Data Engineering 35, 10 (oct 2023), 10666‚Äì10680.
https://doi.org/10.1109/TKDE.2023.3250703
[2]Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. 2022. Attre2vec: Unsu-
pervised attributed edge representation learning. Information Sciences 592 (2022),
82‚Äì96.
[3]Shaked Brody, Uri Alon, and Eran Yahav. 2022. How Attentive are Graph Atten-
tion Networks?. In ICLR. arXiv:2105.14491
[4]Jiangxia Cao, Xixun Lin, Shu Guo, Luchen Liu, Tingwen Liu, and Bin Wang.
2021. Bipartite Graph Embedding via Mutual Information Maximization. In
Proceedings of the 14th ACM International Conference on Web Search and Data
Mining (WSDM ‚Äô21). Association for Computing Machinery, New York, NY, USA,
635‚Äì643. https://doi.org/10.1145/3437963.3441783
[5]Rizal Fathony, Jenn Ng, and Jia Chen. 2023. Interaction-Focused Anomaly Detec-
tion on Bipartite Node-and-Edge-Attributed Graphs. In 2023 International Joint
Conference on Neural Networks (IJCNN). IEEE, 1‚Äì10.
[6]Zheng Gao, Gang Fu, Chunping Ouyang, Satoshi Tsutsui, Xiaozhong Liu, Jeremy
Yang, Christopher Gessner, Brian Foote, David Wild, Ying Ding, et al .2019.
edge2vec: Representation learning using edge semantics for biomedical knowl-
edge discovery. BMC bioinformatics 20, 1 (2019), 1‚Äì15.
[7]Mathieu Garchery and Michael Granitzer. 2020. Adsage: Anomaly detection
in sequences of attributed graph edges applied to insider threat detection at
fine-grained level. arXiv preprint arXiv:2007.06985 (2020).
[8]Edward Giamphy, Jean-Loup Guillaume, Antoine Doucet, and Kevin Sanchis.
2023. A survey on bipartite graphs embedding. Social Network Analysis and
Mining 13, 1 (2023), 54. https://doi.org/10.1007/s13278-023-01058-z
[9]Gene H Gloub and Charles F Van Loan. 1996. Matrix computations. Johns Hopkins
Universtiy Press, 3rd edtion (1996).
[10] Liyu Gong and Qiang Cheng. 2019. Exploiting edge features for graph neural
networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 9211‚Äì9219.
[11] Aditya Grover and Jure Leskovec. 2016. Node2vec: Scalable feature learning for
networks. KDD 13-17-Augu (2016), 855‚Äì864. arXiv:1607.00653
[12] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions. SIAM review 53, 2 (2011), 217‚Äì288.
[13] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In NIPS, Vol. 2017-Decem. 1025‚Äì1035. arXiv:1706.02216
[14] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning
on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).
[15] Chaoyang He, Tian Xie, Yu Rong, Wenbing Huang, Junzhou Huang, Xiang Ren,
Cyrus Shahabi, and Xi-Ang Ren. 2020. Cascade-BGNN: Toward Efficient Self-
supervised Representation Learning on Large-scale Bipartite Graphs. Technical
Report. https://doi.org/10.1145/1122445.1122456 arXiv:1906.11994v3[16] Roger A Horn and Charles R Johnson. 2012. Matrix analysis. Cambridge university
press.
[17] Keke Huang, Jing Tang, Juncheng Liu, Renchi Yang, and Xiaokui Xiao. 2023.
Node-wise diffusion for scalable graph learning. In Proceedings of the ACM Web
Conference 2023. 1723‚Äì1733.
[18] Wentao Huang, Yuchen Li, Yuan Fang, Ju Fan, and Hongxia Yang. 2020. BiANE:
Bipartite Attributed Network Embedding. SIGIR 2020 - Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval 10, 20 (2020), 149‚Äì158. https://doi.org/10.1145/3397271.3401068
[19] Jaehyeong Jo, Jinheon Baek, Seul Lee, Dongki Kim, Minki Kang, and Sung Ju
Hwang. 2021. Edge representation learning with hypergraphs. Advances in
Neural Information Processing Systems 34 (2021), 7534‚Äì7546.
[20] Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic
optimization. In arXiv Prepr. arXiv1412.6980. arXiv. arXiv:1412.6980
[21] Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with
graph convolutional networks. ICLR 2017 (sep 2017).
[22] David A Levin and Yuval Peres. 2017. Markov chains and mixing times. Vol. 107.
American Mathematical Soc.
[23] Jiacheng Li, Jingbo Shang, and Julian McAuley. 2022. UCTopic: Unsupervised
Contrastive Learning for Phrase Representations and Topic Mining. Proceedings
of the Annual Meeting of the Association for Computational Linguistics 1 (2022),
6159‚Äì6169. https://doi.org/10.18653/v1/2022.acl-long.426 arXiv:2202.13469
[24] Suxue Li, Haixia Zhang, Dalei Wu, Chuanting Zhang, and Dongfeng Yuan. 2018.
Edge representation learning for community detection in large scale information
networks. In Mobility Analytics for Spatio-Temporal and Social Data: First Inter-
national Workshop, MATES 2017, Munich, Germany, September 1, 2017, Revised
Selected Papers 1. Springer, 54‚Äì72.
[25] Yiming Li, Siyue Xie, Xiaxin Liu, Qiu Fang Ying, Wing Cheong Lau, Dah Ming
Chiu, Shou Zhi Chen, et al .2021. Temporal graph representation learning for
detecting anomalies in e-payment systems. In 2021 International Conference on
Data Mining Workshops (ICDMW). IEEE, 983‚Äì990.
[26] Yirui Liu, Xinghao Qiao, Liying Wang, and Jessica Lam. 2023. EEGNN: Edge
Enhanced Graph Neural Network with a Bayesian Nonparametric Graph Model.
InInternational Conference on Artificial Intelligence and Statistics. PMLR, 2132‚Äì
2146.
[27] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2021. A
unified view on graph neural networks as graph signal denoising. In Proceedings of
the 30th ACM International Conference on Information & Knowledge Management.
1202‚Äì1211.
[28] Rajeev Motwani and Prabhakar Raghavan. 1995. Randomized algorithms. Cam-
bridge university press.
[29] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. EMNLP-IJCNLP 2019
- 2019 Conference on Empirical Methods in Natural Language Processing and 9th
International Joint Conference on Natural Language Processing, Proceedings of the
Conference (2019), 188‚Äì197. https://doi.org/10.18653/v1/d19-1018
[30] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmet-
ric transitivity preserving graph embedding. KDD 13-17-Augu (2016), 1105‚Äì1114.
[31] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online learning
of social representations. SIGKDD (2014), 701‚Äì710. arXiv:1403.6652
[32] Nils Reimers and Iryna Gurevych. 2020. Sentence-BERT: Sentence embeddings
using siamese BERT-networks. EMNLP-IJCNLP 2019 - 2019 Conference on Empiri-
cal Methods in Natural Language Processing and 9th International Joint Conference
on Natural Language Processing, Proceedings of the Conference (2020), 3982‚Äì3992.
https://doi.org/10.18653/v1/d19-1410 arXiv:1908.10084
[33] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-june Paul Hsu,
and Kuansan Wang. 2015. An overview of microsoft academic service (mas) and
applications. In WWW. ACM, 243‚Äì246.
[34] Gilbert Strang. 2022. Introduction to linear algebra. SIAM.
[35] Guolei Sun and Xiangliang Zhang. 2019. A novel framework for node/edge
attributed graph embedding. In Advances in Knowledge Discovery and Data Min-
ing: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019,
Proceedings, Part III 23. Springer, 169‚Äì182.
[36] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. LINE: Large-scale information network embedding. WWW 2015 (2015),
1067‚Äì1077. arXiv:1503.03578
[37] Jie Tang, Duo Zhang, and Limin Yao. 2007. Social Network Extraction of Academic
Researchers. In ICDM‚Äô07. 292‚Äì301.
[38] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Ar-
netMiner: Extraction and mining of academic social networks. SIGKDD (2008),
990‚Äì998.
[39] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Li√≤, and Yoshua Bengio. 2017. Graph Attention Networks. In ICLR 2018.
[40] Petar Veliƒçkoviƒá, William Fedus, William L Hamilton, Pietro Li√≤, Yoshua Bengio,
and R Devon Hjelm. 2018. Deep Graph Infomax. In ICLR.
[41] Andrew Z Wang, Rex Ying, Pan Li, Nikhil Rao, Karthik Subbian, and Jure Leskovec.
2021. Bipartite dynamic representations for abuse detection. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3638‚Äì3648.
3089KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Hewen Wang, Renchi Yang, & Xiaokui Xiao
[42] Hewen Wang, Renchi Yang, Keke Huang, and Xiaokui Xiao. 2023. Efficient and
Effective Edge-wise Graph Representation Learning. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2326‚Äì2336.
[43] Hewen Wang, Renchi Yang, and Jieming Shi. 2023. Anomaly Detection in Finan-
cial Transactions Via Graph-Based Feature Aggregations. In Big Data Analytics
and Knowledge Discovery: 25th International Conference, DaWaK 2023, Penang,
Malaysia, August 28‚Äì30, 2023, Proceedings. Springer-Verlag, Berlin, Heidelberg,
64‚Äì79. https://doi.org/10.1007/978-3-031-39831-5_6
[44] Jianian Wang, Sheng Zhang, Yanghua Xiao, and Rui Song. 2022. A Review on
Graph Neural Network Methods in Financial Applications. Journal of Data Science
20, 2 (2022), 111‚Äì134.
[45] Ziming Wang, Jun Chen, and Haopeng Chen. 2021. EGAT: Edge-featured graph
attention network. In Artificial Neural Networks and Machine Learning‚ÄìICANN
2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slo-
vakia, September 14‚Äì17, 2021, Proceedings, Part I 30. Springer, 253‚Äì264.
[46] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza, Christopher Fifty, Tao Yu,
and Kilian Q. Weinberger. 2019. Simplifying graph convolutional networks. In
arXiv.
[47] Xueyi Wu, Yuanyuan Xu, Wenjie Zhang, and Ying Zhang. 2023. Billion-Scale
Bipartite Graph Embedding: A Global-Local Induced Approach. Proc. VLDB
Endow. 17, 2 (2023), 175‚Äì183. https://doi.org/10.14778/3626292.3626300
[48] Yulin Wu, Xiangting Hou, Wen Jun Tan, Zengxiang Li, and Wentong Cai. 2017.
Efficient parallel simulation over social contact network with skewed degree
distribution. In Proceedings of the 2017 ACM SIGSIM Conference on Principles of
Advanced Discrete Simulation. 65‚Äì75.
[49] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez,
and Ion Stoica. 2021. Representing long-range context for graph neural networks
with global attention. Advances in Neural Information Processing Systems 34
(2021), 13266‚Äì13279.
[50] Hansheng Xue, Luwei Yang, Vaibhav Rajan, Wen Jiang, Yi Wei, and Yu Lin. 2021.
Multiplex Bipartite Network Embedding Using Dual Hypergraph Convolutional
Networks. In Proceedings of the Web Conference 2021 (WWW ‚Äô21). Association for
Computing Machinery, New York, NY, USA, 1649‚Äì1660. https://doi.org/10.1145/
3442381.3449954
[51] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. 2023.
Personalized Showcases: Generating Multi-Modal Explanations for Recommen-
dations. SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval 5, 23 (2023), 2251‚Äì2255.
https://doi.org/10.1145/3539618.3592036 arXiv:2207.00422
[52] Hongqiang Yan, Yan Jiang, and Guannan Liu. 2018. Telecomm fraud detection
via attributed bipartite network. In 2018 15th International Conference on Service
Systems and Service Management (ICSSSM). IEEE, 1‚Äì6.
[53] Liang Yang, Chuan Wang, Junhua Gu, Xiaochun Cao, and Bingxin Niu. 2021. Why
do attributes propagate in graph convolutional neural networks?. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 35. 4590‚Äì4598.
[54] Renchi Yang. 2022. Efficient and Effective Similarity Search over Bipartite Graphs.
InProceedings of the ACM Web Conference 2022. 308‚Äì318.
[55] Renchi Yang and Jieming Shi. 2024. Efficient High-Quality Clustering for Large
Bipartite Graphs. Proceedings of the ACM on Management of Data 2, 1 (2024),
1‚Äì27.
[56] Renchi Yang, Jieming Shi, Keke Huang, and Xiaokui Xiao. 2022. Scalable and
Effective Bipartite Network Embedding. In Proceedings of the 2022 International
Conference on Management of Data. 1977‚Äì1991.
[57] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S. Bhowmick.
2020. Homogeneous network embedding for massive graphs via reweighted
personalized pagerank. VLDB 13, 5 (2020), 670‚Äì683. arXiv:1906.06826
[58] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Juncheng Liu, and Sourav S.
Bhowmick. 2020. Scaling attributed network embedding to massive graphs. VLDB
14, 1 (2020), 37‚Äì49. arXiv:2009.00826
[59] Renchi Yang, Jieming Shi, Yin Yang, Keke Huang, Shiqi Zhang, and Xiaokui
Xiao. 2021. Effective and scalable clustering on massive attributed graphs. In
Proceedings of the Web Conference 2021. 3675‚Äì3687.
[60] Wei Yu, Wenkai Wang, Guangquan Xu, Huaming Wu, Hongyan Li, Jun Wang,
Xiaoming Li, and Juan Liu. 2023. MRFS: Mining Rating Fraud Subgraph in
Bipartite Graph for Users and Products. IEEE Transactions on Computational
Social Systems (2023).
[61] Jie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. 2019. Prone: Fast
and scalable network representation learning. In IJCAI. 4278‚Äì4284.
[62] Yao Zhang, Yun Xiong, Xiangnan Kong, and Yangyong Zhu. 2017. Learning node
embeddings in interaction graphs. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management. 397‚Äì406.
[63] Ziwei Zhang, Jian Pei, Peng Cui, Xuanrong Yao, Xiao Wang, and Wenwu Zhu.
2018. Arbitrary-order proximity preserved network embedding. KDD (2018),2778‚Äì2786.
[64] Tao Zhou, Jie Ren, Mat√∫≈° Medo, and Yi-Cheng Zhang. 2007. Bipartite network
projection and personal recommendation. Physical review E 76, 4 (2007), 046115.
[65] Yuchen Zhou, Hongtao Huo, Zhiwen Hou, Lingbin Bu, Jingyi Mao, Yifan Wang,
Xiaojun Lv, and Fanliang Bu. 2023. Co-embedding of edges and nodes with deep
graph convolutional neural networks. Scientific Reports 13, 1 (2023), 16966.
[66] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. 2021. Interpreting and
unifying graph neural networks with an optimization framework. In Proceedings
of the Web Conference 2021. 1215‚Äì1226.
A PROOFS
Proof of Lemma 2.2. We first prove that PUis a row-stochastic
matrix. Since it is symmetric, then its doubly stochastic property
naturally follows. By Eq. (1), the (ùëñ,ùëó)-th entry of PUis
‚àëÔ∏Å
ùëíùëó‚ààEPU[ùëíùëñ,ùëíùëó]=‚àëÔ∏Å
ùëíùëó‚ààE1
D[ùë¢(ùëñ),ùë¢(ùëñ)]¬∑1ùë¢(ùëñ)‚ààùëíùëó=1,
where 1ùë¢(ùëñ)‚ààùëíùëóis an indicator function which equals 1when node
ùë¢(ùëñ)is an endpoint of edge ùëíùëó. A similar proof can be done for
PV. ‚ñ°
Lemma A.1. Ifùúé1be the largest singular value of ED‚àí1
2,ùúé1‚â§1.
Proof. According to [ 34], The singular values are the square
roots of the non-zero eigenvalues of ED‚àí1
2¬∑(ED‚àí1
2)‚ä§=P. This
implies that all the eigenvalues of Pare non-negative. Thus, ùúé1=‚àöùúÜ1, whereùúÜ1is the largest eigenvealue of P. Recall that Pis doubly
stochastic. Then, we have
‚à•P‚à•‚àû=max
1‚â§ùëñ‚â§|E||E|‚àëÔ∏Å
ùëó=1Pùëñ,ùëó=1.
By Theorem 5.6.9 in [16],
|ùúÜ1|‚â§ùúå(P)‚â§‚à• P‚à•‚àû=1,
whereùúå(P)is the spectral radius of P, which leads to ùúé1‚â§1.‚ñ°
Lemma 3.1. First, it is easy to derive that Eq. 3 can be trans-
formed to its equivalent form Oùëü=ùë°ùëüùëéùëêùëí(Z‚ä§(I‚àíP)Z). Accordingly,
Eq. 4 is converted into
(1‚àíùõº)¬∑‚à•Z‚àíùëìŒò(X)‚à•2
ùêπ+ùõº¬∑ùë°ùëüùëéùëêùëí(Z‚ä§(I‚àíP)Z).
By setting its derivative w.r.t. Zto zero, we obtain the optimal Zas:
ùúï{(1‚àíùõº)¬∑‚à•Z‚àíX‚à•2
ùêπ+ùõº¬∑ùë°ùëüùëéùëêùëí(Z‚ä§(I‚àíP)Z)}
ùúïZ=0
=‚áí(1‚àíùõº)¬∑(Z‚àíùëìùúÉ(X))+ùõº(I‚àíP)Z=0
=‚áíZ=(1‚àíùõº)¬∑(I‚àíùõºP)‚àí1¬∑ùëìŒò(X). (15)
By the definition of Neumann series, i.e.,(I‚àíM)‚àí1=√ç‚àû
ùë°=0Mùë°, we
have
(I‚àíùõºP)‚àí1=‚àû‚àëÔ∏Å
ùë°=0ùõºùë°¬∑Pùë°.
Plugging the above equation into Eq. (15)completes the proof. ‚ñ°
Proof of Lemma 3.2. Recall that Dcan be represented by
DU 0
0 DV
3090Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Then, as per Eq. (6), we have
P=(‚àöÔ∏Å
ùõΩEU‚à•‚àöÔ∏Å
1‚àíùõΩEV)¬∑D‚àí1¬∑(‚àöÔ∏Å
ùõΩEU‚à•‚àöÔ∏Å
1‚àíùõΩEV)‚ä§
=‚àöÔ∏Å
ùõΩEUD‚àí1
2
U‚à•‚àöÔ∏Å
1‚àíùõΩEVD‚àí1
2
V
¬∑‚àöÔ∏Å
ùõΩEUD‚àí1
2
U‚à•‚àöÔ∏Å
1‚àíùõΩEVD‚àí1
2
V‚ä§
=ùõΩ¬∑EUD‚àí1
UE‚ä§
U+(1‚àíùõΩ)¬∑EVD‚àí1
VE‚ä§
V
=ùõΩ¬∑PU+(1‚àíùõΩ)¬∑PV,
which finishes the proof. ‚ñ°
Proof of Eq. (8).According to Eq. (5), we have
 
(1‚àíùõº)ùëáùëöùëñùë•‚àí1‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°¬∑ùëìŒò(X)!
+¬©¬≠
¬´(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=ùëáùëöùëñùë•ùõºùë°ùö∑¬™¬Æ
¬¨.
Note that we can rewrite the second part as:
(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=ùëáùëöùëñùë•ùõºùë°ùö∑= 
(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°‚àí(1‚àíùõº)ùëáùëöùëñùë•‚àí1‚àëÔ∏Å
ùë°=0ùõºùë°!
¬∑ùö∑
=(1‚àí(1‚àíùõºùëáùëöùëñùë•))¬∑ùö∑=ùõºùëáùëöùëñùë•ùö∑.
Eq. (8) naturally follows. ‚ñ°
Proof of Lemma 3.3. We first prove that ùúé2
2is the second largest
eigenvalue of P. By Eq. (6),P=(ED‚àí1/2)¬∑(ED‚àí1/2)‚ä§, which in-
dicates that the eigenvalues of Pare the squared singular values
ofED‚àí1/2[16,34]. Since singular values are non-negative, the sec-
ond largest eigenvalue of Pisùúé2
2. According to the fact of Pis a
reversible Markov chain and Theorem 12.5 in [ 22],ùëáùëöùëñùë•satisfies
ùëáùëöùëñùë•‚â•1
1‚àíùúé2
2‚àí1. ‚ñ°
Proof of Theorem 3.4. We first consider that Uùö∫V‚ä§is the ex-
act full SVD of ED‚àí1/2. According to Lemma A.1, we can get
QQ‚ä§=U¬∑1
1‚àíùõºùö∫2¬∑U‚ä§=‚àû‚àëÔ∏Å
ùë°=0ùõºùë°¬∑U¬∑(ùö∫2)ùë°¬∑U‚ä§.
Since UandVare semi-unitary matrices, i.e., U‚ä§UandV‚ä§V,
QQ‚ä§=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°(Uùö∫2U‚ä§)ùë°=‚àû‚àëÔ∏Å
ùë°=0ùõºùë°(Uùö∫¬∑(V‚ä§V)¬∑ùö∫U‚ä§)ùë°
=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°(ED‚àí1
2¬∑(ED‚àí1
2)‚ä§)ùë°
=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°(ED‚àí1E‚ä§)ùë°=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°.
According to [ 16,34], the definition of Pin Eq. (6)(i.e., P=
ED‚àí1/2¬∑(ED‚àí1/2)) implies that the singular values of ED‚àí1/2are
the square roots of the eigenvalues of P, and the left singular vectors
ofED‚àí1
2are the eigenvectors of P. In particular, due to the non-
negativity of singular values, the ùëò-th largest eigenvalue of Pis
equal toùúé2
ùëòwhereùúéùëòdenotes the ùëò-th largest singular value of
ED‚àí1/2.Recall that Pis doubly stochastic, meaning that Pis a symmetric
matrix. Using Theorem 4.1 in [ 63], we can derive that the singular
values of Pare the absolute values of the corresponding eigenvalues,
and the left singular vectors of Pare equal to the eigenvectors of
P. Since all the eigenvalues of Pare non-negative, its ùëò-th largest
eigenvalue is equal to the ùëò-th largest singular value of P.
Combining the above two conclusions, we can extrapolate that
theùëò-th largest singular value of Pis equal to ùúé2
ùëò, and the left
singular vectors of ED‚àí1/2are the left singular vectors of P.
Theorem A.2 (Eckart‚ÄìYoung Theorem [ 9]).Suppose that Mùëò‚àà
Rùëõ√óùëòis the rank-ùëòapproximation to M‚ààRùëõ√óùëõobtained by exact
SVD, then
min
ùëüùëéùëõùëò(bM)‚â§ùëò‚à•M‚àíbM‚à•ùêπ=‚à•M‚àíMùëò‚à•ùêπ=ùúéùëò+1,
whereùúéùëñrepresents the ùëñ-th largest singular value of M.
LetUùö∫V‚ä§be the exact top- ùëòSVD of ED‚àí1/2. Then, Uùö∫2U‚ä§is
the exact top- ùëòSVD of P. By leveraging Eckart‚ÄìYoung Theorem in
Theorem A.2, we obtain
‚à•Uùö∫2U‚ä§‚àíP‚à•ùêπ‚â§ùúé2
ùëò.
Next, we prove that Uand1
1‚àíùõºùö∫2are the top-ùëòleft singular vectors
and singular values of (1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°, respectively. We assume
ùëò=|E|, which means
U1
1‚àíùõºùö∫2U‚ä§=(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°.
Consider vector U[:,ùëñ]and scalar1
1‚àíùõºùö∫[ùëñ,ùëñ]2and denote by eùëña
one-hot vector. We can derive
(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°¬∑U[:,ùëñ]=U1
1‚àíùõºùö∫2U‚ä§¬∑U[:,ùëñ]
=U1
1‚àíùõºùö∫2¬∑eùëñ
=1
1‚àíùõºùö∫[ùëñ,ùëñ]2¬∑U[:,ùëñ].
In addition, by Lemma A.1,1
1‚àíùõºùö∫[ùëñ,ùëñ]2is non-negative and is mono-
tonically decreasing with ùëñ. As a consequence, we can conclude that
U[:,ùëñ]and scalar1
1‚àíùõºùö∫[ùëñ,ùëñ]2are an eigenpair of (1‚àíùõº)√ç‚àû
ùë°=0ùõºùë°Pùë°,
and thus, its ùëñ-th largest left singular vector and singular value.
Then, using Theorem A.2 yields
‚à•QQ‚ä§‚àí(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°‚à•ùêπ=‚à•U1
1‚àíùõºùö∫2U‚ä§‚àí(1‚àíùõº)‚àû‚àëÔ∏Å
ùë°=0ùõºùë°Pùë°‚à•ùêπ
‚â§1
1‚àíùõºùúé2
ùëò,
which finishes the proof. ‚ñ°
3091