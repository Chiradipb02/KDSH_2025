Representation Learning of Temporal Graphs with Structural
Roles
Huaming Du
School of Business Administration
Southwestern University of Finance
and Economics
Chengdu, Sichuan, China
dhmfcc@smail.swufe.edu.cnLong Shi
Financial Intelligence and Financial
Engineering Key Laboratory
Southwestern University of Finance
and Economics
Chengdu, Sichuan, ChinaXingyan Chen
Financial Intelligence and Financial
Engineering Key Laboratory
Southwestern University of Finance
and Economics
Chengdu, Sichuan, China
Yu Zhao‚àó
Financial Intelligence and Financial
Engineering Key Laboratory
Southwestern University of Finance
and Economics
Chengdu, Sichuan, ChinaHegui Zhang‚Ä†
School of Data Science and Artificial
Intelligence
Dongbei University of Finance and
Economics
Dalian, Liaoning, ChinaCarl Yang
Department of Computer Science
Emory University
Atlanta, Georgia, United States
Fuzhen Zhuang
Institute of Artificial Intelligence
SKLSDE, School of Computer Science,
Beihang University
Beijing, ChinaGang Kou
Xiangjiang Laboratory
Changsha, Hunan, China
School of Business Administration
Southwestern University of Finance
and Economics
Chengdu, Sichuan, China
ABSTRACT
Temporal graph representation learning has drawn considerable
attention in recent years. Most existing works mainly focus on
modeling local structural dependencies of temporal graphs. How-
ever, underestimating the inherent global structural role informa-
tion in many real-world temporal graphs inevitably leads to sub-
optimal graph representations. To overcome this shortcoming, we
propose a novel Role-based Temporal GraphConvolution Network
(RTGCN) that fully leverages the global structural role information
in temporal graphs. Specifically, RTGCN can effectively capture the
static global structural roles by using hypergraph convolution neu-
ral networks. To capture the evolution of nodes‚Äô structural roles, we
further design structural role-based gated recurrent units. Finally,
we integrate structural role proximity in our objective function to
preserve global structural similarity, further promoting temporal
graph representation learning. Experimental results on multiple
‚àóco-corresponding author
‚Ä†corresponding author (hegui.zhang@dufe.edu.cn; hegui.zhang@qq.com)
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671854real-world datasets demonstrate that RTGCN consistently outper-
forms state-of-the-art temporal graph representation learning meth-
ods by significant margins in various temporal link prediction and
node classification tasks. Specifically, RTGCN achieves AUC im-
provement of up to 5.1% for link prediction and F1 improvement of
up to 6.2% for new link prediction. In addition, RTGCN achieves
AUC improvement up to 4.6% for node classification and 2.7% for
structural role classification.
CCS CONCEPTS
‚Ä¢Information systems ‚ÜíSocial networks; ‚Ä¢Computing method-
ologies‚ÜíLearning latent representations.
KEYWORDS
Temporal graph representation learning, Structural roles
ACM Reference Format:
Huaming Du, Long Shi, Xingyan Chen, Yu Zhao, Hegui Zhang, Carl Yang,
Fuzhen Zhuang, and Gang Kou. 2024. Representation Learning of Temporal
Graphs with Structural Roles. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24), August
25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671854
1 INTRODUCTION
Temporal graph representation learning is important since it cap-
tures how the node interacts and the graph evolves, which will help
to understand and predict events in complex dynamic systems, for
instance, the variation of traffic flows in transportation networks
[8] and researchers‚Äô interest shifts in academic networks [ 31]. Early
654
Corrected Version of Record. V.1.1. Published November 7, 2024.
KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
Role
T
-
1
T
3
4
5
1
2
6
10
7
8
9
1
3
4
5
2
7
10
11
8
6
9
Role
Structural roles
R1: [4
T
, 5
T
, 7
T
, 10
T
, 4
T
-
1
, 5
T
-
1
, 6
T
-
1
, 7
T
-
1
]
R2: [1
T
, 3
T
, 6
T
, 8
T
, 2
T
-
1
, 3
T
-
1
, 9
T
-
1
, 10
T
-
1
]
R3: [2
T
, 9
T
, 1
T
-
1
, 8
T
-
1
]
R4: [11
T
]
?
Structural roles assignment:
...
Motif:
Structural feature vectors:
2
9
4
7
11
3
10
Node
G1
G2
G3
4
0
1
1
1
1
4
1
1
1
1
1
0
2
2
0
0
0
0
0
1
R1:
R2:
R3:
R4:
4
7
2
9
3
10
11
...
...
...
...
...
...
G1
G2
G3
Time
T
T
-
1
(a) Temporal graph
(b) Structural roles discovery
...
Existing Methods
Our Method
Fail to capture structural roles
Model  global structural roles
Dynamic link prediction tasks
Dynamic link new prediction tasks
Downstream tasks
Dynamic representation learning
1
3
4
5
2
7
10
11
8
6
9
1
3
4
5
2
7
10
11
8
6
9
Figure 1: The effectiveness of structural roles for temporal
graph learning in various downstream tasks. (a) Temporal
graph: Nodes in the temporal graph are labeled. Superscripts
on nodes indicate timesteps. (b) Structural roles discovery:
Firstly, we construct the structural feature vector for each
node. Then, we assess the structural similarity between nodes
based on the structural feature vectors. Lastly, we assign
structurally similar nodes to the same structural role set (i.e.,
R1, R2, R3, R4).
temporal graph methods adopt matrix decomposition to capture
graph structures in each snapshot and regularize the smoothness of
the representations of adjacent snapshots [ 9]. Such matrix decom-
position is usually computationally heavy. With the development
of deep learning, graph neural networks [ 7] are adopted to cap-
ture the structural information while recurrent neural networks
or transformer [ 26] are further utilized to summarize the temporal
information [ 17,22,32]. Despite achieving preliminary successes,
existing methods typically generate node representations through
local connectivity proximity1, neglecting global structural similar-
ity2.
Recently, in the domains of complex networks and static graphs,
it has been demonstrated that modeling structural roles of nodes
is a crucial step toward better representation learning [ 1,6,35].
Intuitively, if two nodes share similar neighborhood connectivity pat-
terns, they are structurally similar, and thus, they belong to the same
1If two nodes are connected by edges or paths, the node embeddings should be similar.
2If two nodes are not connected but exhibit similar neighborhood structures, their
embeddings should also be similar.structural role. As shown in Figure 1, ‚ñ≥(R3) describes a connectiv-
ity pattern of a triangle. This definition allows nodes to be neither
directly connected nor even in the same snapshot. Therefore, struc-
tural roles have the capability to model long-range dependencies,
capturing crucial information from distant nodes. Furthermore,
structural roles can model global dependencies and capture all cru-
cial node information in the entire graph or even within the entire
snapshots to enhance the learning of temporal graph representa-
tions. As shown in Figure 1(a), the goal is to predict the label of
target node 9 in snapshot ùëá. With structural role information, we
can directly utilize non-adjacent node information (e.g., node 2ùëá
and node 1ùëá‚àí1) at the current snapshot and historical snapshots to
accurately predict the label of node 9. However, existing methods
lack the ability to capture long-range dependencies in temporal
graphs, thus failing to capture essential features from distant but
informative nodes [ 18]. This limitation may significantly restrict
the representational capacity (bottom of Figure 1).
Modeling the evolving patterns of temporal graphs becomes a
crucial research question [ 10,32,34]. Initially, we predefine struc-
tural roles based on the temporal graph and then assigning role
definitions to each node in different snapshots. As the temporal
graph evolves, the connection patterns of nodes change, leading to
variations in node structural roles. Through the evolution of struc-
tural roles, we can effectively characterize the evolving patterns of
the temporal graph. Importantly, we do not alter the definition of
structural roles in this process. For example, at time ùëá‚àí1, node 8
transitions from structural role R2 to R3 at time ùëá. This transforma-
tion essentially denotes the change in R2 and R3 structures from
snapshotùëá‚àí1to snapshot ùëá, capturing a global evolution pattern.
In contrast, traditional temporal graph methods using RNNs, trans-
formers [ 26], or SNNs [ 10] can only model changes in the local
connection patterns of nodes. As illustrated in Figure 1, existing
methods can only depict the transition of node 8‚Äôs neighbors at
timeùëá‚àí1, changing from the neighbor set [6, 7, 9, 10] to [6, 9] at
timeùëá.
Due to the temporal and structural complexity in temporal
graphs, considering structural role information is not a simple
task of extension, it poses the following three challenges : (1) How to
efficiently model the interactions between nodes within the same
structural role set in same snapshot? (2) How to consider the influ-
ence of nodes with the same structural role from previous snapshots
on the current snapshot? (3) How to better utilize structural role
information to model the evolution patterns of temporal graphs
effectively?
To address these challenges, we propose a novel structural role-
based temporal graph convolutional network, namely, the RTGCN,
to preserve both local connective proximity and global structural
similarity under a unified framework, focusing on capturing the
global structural roles and their evolutions. Specifically, in RTGCN,
we first use structural role information to construct hypergraphs
for the current snapshot and historical snapshots. Furthermore,
we propose a structural role-based Graph Neural Network (GNN)
module to capture the global structural similarity of the nodes.
Moreover, we design a structural role-based Gated Recurrent Unit
(GRU) along with the temporal dimension to effectively capture
the evolving patterns of the structural roles in the temporal graphs.
 
655Representation Learning of Temporal Graphs with Structural Roles KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
Finally, a constraint based on structural role similarity is introduced
to further preserve global structural similarity.
In summary, the main contributions are stated as follows:
‚Ä¢To the best of our knowledge, we are the first to incorporate
global structural role information into the representation learning
of temporal graphs.
‚Ä¢Our proposed RTGCN applies a hypergraph-based GNN module to
model the structural roles and a role-based GRU module to capture
the evolution of structural roles in temporal graphs.
‚Ä¢Extensive experimental results on diverse real-world temporal
graphs demonstrate the superiority of RTGCN as it consistently
outperforms 15 SOTA baselines on all datasets for various tasks.
2 RELATED WORK
Temporal graph representation learning. It has been widely
studied in recent years. Most approaches integrate GNN with the re-
current architecture, whereby the former captures graph structure
information and the latter handles temporal dynamism via maintain-
ing a hidden state to summarize historical snapshots [ 12,13,17,22‚Äì
24]. For instance, ROLAND [ 32] repurposes static GNN designs to
temporal graph settings and proposes a live-update evaluation set-
ting to learn node representations. HTGN [ 29,30] studies temporal
graph representation learning built upon a hyperbolic geometry
powered with the recurrent learning paradigm. Recently, SpikeNet
[10] captures the evolving dynamics of temporal graphs with spik-
ing neural networks via the LIF [ 4] model. While the methods
discussed all intend to capture temporal evolving patterns, none of
them can capture the global structural role information in temporal
graphs.
Role-based embedding methods. They mainly focus on the re-
search of single-layer networks, and few research is extended to
multiplex networks. For role-based single-layer network embed-
ding, some methods implement role assignment of nodes through
the role discovery process and then improve the random walk
method to learn node representations [ 1,14]. Other methods learn
role-based embedding based on feature decomposition and utilize
graph kernels to capture the structural similarity between nodes
[16,21]. The embedding methods that can capture the similarity
between nodes can also be classified as role-oriented embedding
methods [ 6,19,20]. For multiplex networks, RMNE [ 33] derives
role-based embedding from extending role-aware random walks
or incorporating structural role information into a unified learning
framework. These methods do not consider the dynamic changes of
the graph structure and node features, rendering them unsuitable
for direct application in temporal graph representation learning.
3 METHODOLOGY
As shown in Figure 2, RTGCN is a recurrent learning paradigm,
fitting within the prevalent discrete-time temporal graph archi-
tecture. The RTGCN unit primarily consists of three components:
(1) Structural Role Hypergraph Construction, the module aims to
construct hypergraphs to extract global structural role information;
(2) Structural Role-based GRU Module, the modified temporal recur-
rent construct designed to capture the sequential patterns of nodes;
(3) Structural Role-based GNN Module, the graph and hypergraphneural network to model global structure role information. We elab-
orate on the details of each module in the following paragraphs.
Prior to this exposition, we initially present the formal definition
of structural roles.
Structural Roles. Given a graph ùê∫(ùëâ,ùê∏), we first extract the
structural feature vector ùêπ=[ùëì1,ùëì2,...,ùëìùëö]of each node. Then, each
node is mapped into a set of structural roles through a mapping
function Œ¶. We achieve this by manually defining a function that
maps the Œ¶vertices to a set ùëä=[ùúî1,ùúî2,...,ùúîùëÄ]ofùëÄstructural
roles sets where ùëÄ‚©Ω|ùëâ|. Letùë¢andùë£are arbitrary nodes, the
specific mapping process is as follows:
Œ¶:(‚àÄùëñ,1‚©Ωùëñ‚©Ωùëö:ùëìùëñ(ùë¢)=ùëìùëñ(ùë£))‚áíŒ¶(ùë¢)‚â°Œ¶(ùë£) (1)
Please note that the definition of structural roles we propose is
flexible and can employ various mapping functions.
3.1 Structural Role Hypergraph Construction
To extract global structural role information, this module focuses on
constructing structural role-based hypergraph incidence matrices
ùëØ, both for the current and historical snapshots:
ùëØ(ùë£,ùíÜ)=(1
ùëíùë•ùëù(ùë§Œîùë°), ùëñùëì ùë£‚ààùíÜ
0, ùëñùëì ùë£ ‚àâùíÜ(2)
where ùíÜis the hyperedge (i.e., structural role set), ùë§is the time
decay factor and Œîùë°denotes the time interval from the current
snapshot.
The primary objective of our method is to leverage structural
role information to augment the representation learning process in
temporal graphs. Consequently, the contribution of structural role
information at the current snapshot and historical snapshots to node
representation learning may be different. To address this, structural
role hypergraphs are constructed for both the current snapshot
and the historical snapshots, to enable subsequent modules (i.e.,
structural role-based GRU) to effectively differentiate between them.
Next, we will describe the specific construction process.
3.1.1 Hypergraph Construction of Current snapshot. We use the
role incidence matrix to capture the structural role information at
the current snapshot. As previously described, nodes in snapshot
ùë°are classified within the same structural role set if they exhibit
structural similarity. It‚Äôs a natural approach to connect these nodes,
enabling them to disseminate and aggregate messages. However,
this method falters in capturing global topological relationships
within the same structural role set due to over-smoothing as the
number of GCN layers increases. In contrast, the hypergraph is
more effective in learning node and graph representation. Given
theŒîùë°=0, the hypergraph incidence matrix ùëØùëêùë¢‚ààR|ùëâ|√ó|ùúÄ|of
current snapshot can be obtained from Eq. (2). Please note that |ùúÄ|
is significantly smaller than the number of nodes ùëÅ.
3.1.2 Hypergraph Construction of historical snapshots. Historical
information plays an indispensable role in temporal graph modeling
since it facilitates the model to learn the evolving patterns and
regularities. However, traditional recurrent neural networks can
not fully capture some complex evolving patterns due to their
monotonic mechanism and inherent limitations [11].
 
656KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
6
T
4
T
2
T
5
T
3
T
4
T
2
T
5
T
3
T
4
2
5
3
4
5
3
2
6
4
2
5
3
1
1
6
1
4
T
2
T
-
1
5
T
3
T
-
1
4
T
-
1
6
T
2
T
1
T
-
1
1
T
6
T
1
T
T
-
1
T
T+1
GRU
GRU
GRU
GCN 
HyperGCN
GCN 
HyperGCN
GCN 
HyperGCN
Structural roles:
Hyperedges:
7
7
T
R1
R2
R3
R4
Adjecency matrix:A
T
Current snapshot:H
cu,T
Historical snapshots:H
cr,T
Hypergraph Contruction
(c) Structural role
-
based GRU module
(d) Structural role
-
based GNN module
4
2
5
3
4
2
5
3
6
4
2
5
3
1
1
6
1
7
(a) Stuctural role discovery 
T+1H
TH
T-1H
Node Embedding
A
T+1
|H
cu,T+1
|
H
cr,T+1
A
T
|H
cu,T
|H
cr,T
A
T
-
1
|H
cu,T
-
1
|
H
cr,T
-
1
(b) Structural role
hypergraph 
construction 
0.4 0.5 ... 0.3
0.1 0.7 ... 0.9
... ... ... ...
0.2 0.8 ... 0.1ÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉª
0.6 0.1 ... 0.5
0.2 0.3 ... 0.7
... ... ... ...
0.7 0.9 ... 0.8ÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉª
0.1 0.6 ... 0.1
0.5 0.3 ... 0.9
... ... ... ...
0.4 0.2 ... 0.2‚àí ÔÉ©ÔÉπ
ÔÉ™ÔÉ∫‚àíÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫‚àí ÔÉ´ÔÉª
T1O+
TO
T1O‚àí
ÔÅ≥
ÔÅ≥
tanh
ÔÇ¥
1‚àí
ÔÇ¥
+
ÔÇ¥
A
T
H
cu,T
H
cr,T
T1H‚àí
TH
TR
TZ
Structural role
-
based GRU module
R5
TR
TZ
ÔÅ≥
:Update gate
:Reset gate
:Activation 
function
:Hidden state
TH
TH
Figure 2: An overview of RTGCN. (a) Structural role discovery: Construct structural role features for each node and assign
roles to each node based on these structural role features. (b) Structural Role hypergraph construction: Nodes with the same
role are connected through hyperedges, constructing hypergraphs for the current snapshot and historical snapshots based on
role information. The specific construction details are illustrated within the orange dashed box. (c) Structural Role-based GRU
Module: Utilizes the previously constructed hypergraph structural information as input to the GRU module to regulate the
parameters of the GNN, thereby modeling the complex evolution of the temporal graph. Green dashed box represents specific
details. (d) Structural Role-based GNN Module: This module comprises two hypergraph convolutional neural networks and one
conventional graph convolutional neural network. Eventually, the obtained nodes‚Äô comprehensive representations are utilized
for various downstream tasks. Please note that, in the construction of hypergraphs for historical snapshots, we provide an
example by considering only the influence of nodes in snapshot ùëá‚àí1on the nodes in current snapshot ùëá.
Inspired by [ 15,29], we assume that nodes of the same role in
the previous ùëòsnapshots and at the current snapshot ùë°can conduct
message propagation and aggregation, attending to multiple histor-
ical states to describe the evolving patterns of nodes. In addition,
we introduce a time decay function to measure the attenuation of
the influence effect. When Œîùë°>0, we can acquire the hypergraph
incidence matrix ùëØùëêùëüof historical snapshots from the Eq. (2).
3.2 Structural Role-based GRU Module
Traditional methods for modeling the graph evolution process can
be divided into two categories. One utilizes a sequence model that
only builds on the output embedding of GNN to keep and update
node state [ 10,12,32], and the other uses the output of GNN and
adjacency matrix to update learnable parameter of GNN [24].Unlike traditional methods, when designing the temporal mod-
ule, we mainly consider the following three aspects: First, these
existing methods fail to effectively capture the dynamics of the
graph, as discussed in Section 1. Second, along the temporal axis,
the architecture of the model remains consistent, and therefore, we
only need to update the model parameter matrix. Lastly, in practical
applications, nodes frequently appear and disappear, making node
embedding-based sequence methods questionable. Inspired by ex-
isting research [ 17], to efficiently capture the complex evolution
process of the graph, we utilize a modified-GRU to model the tempo-
ral dependency among learnable parameters of RTGCN at different
snapshots. Here, based on the modified GRU, we use the global
structural role information to update the model weight parameter
at each snapshot. Our subsequent experiments further validate the
 
657Representation Learning of Temporal Graphs with Structural Roles KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
effectiveness of this module, as shown in Figures 9 in Appendix B.1.
Formally, we take the ùë®,ùëØùíÑùíñandùëØùíÑùíìas the input of GRU to gen-
erate the weight matrix of the next snapshot, which is formulated as:
ùíÅùë°=ùúé
ùëæùëß1ùë®ùë°ùëæùëß11+ ùëæùëß2ùëØùëê
ùë¢,ùë°+ùëæùëß3ùëØùëêùëü,ùë°ùëæùëß12+ùëºùëßEùëØùíï‚àí1
, (3a)
ùëπùë°=ùúé
ùëæùëü1ùë®ùë°ùëæùëü11+ ùëæùëü2ùëØùëêùë¢,ùë°+ùëæùëü3ùëØùëêùëü,ùë°ùëæùëü12+ùëºùëüEùëØùíï‚àí1
, (3b)
eùëØùë°=ùë°ùëéùëõ‚Ñé
ùëæ1ùë®ùë°ùëæ11+ ùëæ2ùëØùëêùë¢,ùë°+ùëæ3ùëØùëêùëü,ùë°ùëæ12+ùëà
ùëπùë°‚ó¶EùëØùíï‚àí1
,(3c)
cùëØùíï=(1‚àíùíÅùë°)‚ó¶EùëØùíï‚àí1+ùíÅùë°‚ó¶eùëØùë°, (3d)
where ùë®ùë°is the adjacency matrix, and EùëØùíï‚àí1represents the weight
matrix of last snapshot. Note that, cùëØùíïconsists of ùöØùëêùë¢,ùë°,ùöØùëêùëü,ùë°,
andùëæùëé,ùë°, which can be shared in some downstream tasks. ùöØùëêùë¢,ùë°,
ùöØùëêùëü,ùë°, and ùëæùëé,ùë°represent the learnable parameters corresponding
toùëØùëêùë¢,ùëØùëêùëü, and ùë®, respectively. Please note that if there are no
initial node features in the dataset or if the dimension of initial
node features is the same as the number of nodes, the additional
learnable parameter matrix ùëæ11,ùëæ12,ùëæùëß11,ùëæùëß12,ùëæùëü11, and ùëæùëü12
can be ignored.
3.3 Structural Role-based GNN Module
The Structural Role-based GNN module is employed to model global
structure role information in temporal graphs. Beyond the inherent
pairwise graph structures present in the adjacency matrix, we en-
compass two previously discussed hypergraph structures to model
the global structural similarity of temporal graphs. Specifically,
we first use the simplified GCN and hypergraph convolution neu-
ral networks (hyperGCN) to learn node representations. This is
achieved based on the adjacency matrix and the two hypergraph
incidence matrices, respectively. Then these node representations
are integrated to obtain the final node embedding.
At a specific snapshot Gùë°, the feature propagation on the original
graph is defined as:
ùíé(ùëô)
ùë£‚Üíùë¢=ùëÄ
ùëÜùê∫(ùëô)
ùíâ(ùëô‚àí1)
ùë£,ùíâ(ùëô‚àí1)
ùë¢
,
ùíâ(ùëô)
ùë¢=ùê¥ùê∫ùê∫(ùëô)n
ùíé(ùëô)
ùë£‚Üíùë¢|ùë£‚ààN(ùë¢)o
,ùíâ(ùëô‚àí1)
ùë¢
,(4)
where ùíâ(ùëô)
ùë¢is the node embedding for ùë¢‚ààùëâafter passing through ùëô
layers, ùíâ(0)
ùë¢=ùíôùë¢. The ùíé(ùëô)
ùë£‚Üíùë¢is the message embedding and N(ùë¢)
is the neighbors of ùë¢. To capture global structure role information,
the hyperGCN is employed to learn global structural similarity in
current snapshot hypergraph and historical snapshots hypergraph.
In the current snapshot hypergraph, the hypergraph incidence
matrix and the node feature are fed into the hyperGCN to update
node embedding. We can build a hypergraph convolution layer [ 2]
in the following formulation:
ùëøùëô+1ùëê
ùë¢=ùúé
ùë´‚àí1/2
ùë£ùëØùëêùë¢ùëæùë´‚àí1ùíÜùëØ‚ä§ùëêùë¢ùë´‚àí1/2
ùë£ùëøùëôùëêùë¢ùöØùëôùëêùë¢
, (5)
where ùëøùëô+1ùëêùë¢is the signal of hypergraph at ùëôlayer, ùëø0ùëêùë¢denotes
the node initial embedding, ùöØùëôùëêùë¢represents learnable parameter
ofùëôlayer, andùúé(¬∑)is the activation function. ùëæis the hyperedges
weight matrix and we assign each hyperedge the same weight 1.
Further, ùë´ùë£andùë´ùëídenote the diagonal matrices of the vertex de-
grees and the edge degrees, respectively. To model the dependency
of nodes of the same structural role in previous snapshots on nodes
in the current snapshot, the hyperGCN can also be utilized to learnglobal structural similarity in the historical snapshots hypergraph.
We perform the hypergraph convolution on ùëØùëêùëü, which is formu-
lated as:
ùëøùëô+1ùëê
ùëü=ùúé
ùë´‚àí1/2
ùë£1ùëØùëêùëüùëæùë´‚àí1
ùíÜ1ùëØ‚ä§ùëêùëüùë´‚àí1/2
ùë£1ùëøùëôùëêùëüùöØùëôùëêùëü
, (6)
whereùöØùëôùëêùëüis the learnable parameter of ùëôlayer, and ùëø0ùëêùëüdenotes the
node initial embedding. ùë´ùë£1andùë´ùíÜ1denote the diagonal matrices
of the vertex degrees and the edge degrees in ùëØùëêùëü, respectively. On
the top of the node representations, we can obtain more expressive
feature Ovia weighted sum. Therefore, the Otcan be defined as:
Ot=ùëØùë°+ùõºùëøùëêùë¢,ùë°+ùõæùëøùëêùëü,ùë°, (7)
where Otis the final node embedding, while ùõºandùõæare the hyper-
parameters.
3.4 Model Learning
To preserve local connective proximity and global structural simi-
larity in temporal graphs, we present the objective function from
two aspects: connective proximity and structural role proximity,
for training our model. Integrating the above modules, we establish
the overall learning procedure as summarized in Algorithm 1 in
Appendix A.
3.4.1 Connective Proximity. Inspired by [ 3,13], we use the dynamic
embedding of a node ùë¢in snapshotGùë°,ùë∂ùë°,ùë¢, to preserve connective
proximity around ùë¢. In particular, we use a binary cross-entropy
loss at each snapshot to increase the similarities between the node
embedding appearing in the same fixed-length random walks:
Lùëê=ùëá‚àëÔ∏Å
ùë°=1ùëõ‚àëÔ∏Å
ùë¢=1¬©¬≠¬≠¬≠¬≠
¬´‚àëÔ∏Å
ùë£‚ààùëõùë°ùë¢‚àílog ùúé 
ùë∂ùë°
,ùë¢,ùë∂ùë°,ùë£
‚àíùõΩ‚àëÔ∏Å
ùë£‚Ä≤‚ààùëùùë°ùë¢log
1‚àíùúéD
ùë∂ùë°,ùë¢,ùë∂ùë°,ùë£‚Ä≤E¬™¬Æ¬Æ¬Æ¬Æ
¬¨, (8)
where <,>represent any vector similarity measure function (such
as the inner product operation), and ùëõùë°ùë¢denotes the set of nodes
that co-occur with ùë¢during a fixed-length random walk. The term
ùëùùë°ùë¢refers to a negative sampling distribution, typically a function of
the node degrees, while ùõΩsignifies the negative sampling ratio. This
ratio acts as an adjustable hyperparameter, providing a mechanism
to equilibrate the positive and negative samples within the model.
3.4.2 Structural Role Proximity. Inspired by CTGCN [ 12], we pose
that nodes sharing the same structural role tend to exhibit similar
node representation, which enables the RTGCN to preserve global
structural similarity. For nodes ùë¢andùúêthat align with the same
structural role, the concept of structural role proximity is formally
defined as follows:
Lùëü=1
ùëõ√çùëá
ùë°=1√çùëõ
ùë¢=1√ç
ùúê‚ààùëÖ(ùë¢)ùëë ùë∂ùë°
,ùë¢,ùë∂ùë°,ùúê, (9)
whereùëÖ(ùë¢)denotes a set of nodes with the same structural role as
the nodeùë¢, andùëëcan be any distance metric function (e.g., cosine
similarity). As connective proximity and structural role proximity
jointly drive the evolution of temporal graphs, hence, the overall
loss function is summarized below:
L=Lùëê+ùúÜLùëü, (10)
whereùúÜis the hyper-parameter to balance the connective proximity
and structural role proximity.
 
658KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
3.4.3 Implementation. We will further enhance the efficiency of
model training through the following two aspects.
Sliding window : In practical application scenarios, particularly in
situations with an extensive number of snapshots, it is feasible to
employ a suitable sliding window instead of considering the entire
historical information from beginning to end. Therefore, Eq. (8) and
Eq. (9) can be respectively rewritten as Lùëê|[0,ùëá]‚ÜíLùëê|[ùëá‚àíŒîùëá,ùëá]
andLùëü|[0,ùëá]‚ÜíLùëü|[ùëá‚àíŒîùëá,ùëá], where Œîùëáis the sliding window. This
approach not only contributes to further accelerating the training
process but also ensures the model maintains a consistently out-
standing performance. Specific experimental evidence supporting
this proposition is presented in Tables 7 and 8 in Appendix B.1.
Parallel random walk: In the process of determining positive and
negative samples for edges during random walk, high parallelization
can be achieved by simultaneously running multiple threads to
generate a large number of random walks. Therefore, our method
can be highly efficient in practical scenarios.
3.4.4 Complexity Analysis. The analysis of the RTGCN‚Äôs time and
space complexities is as follows.
Time complexity: We analyze the time complexity of the proposed
RTGCN in each timestamp. In Role-based GNN module, according
to [27], the complexity is O(|ùúÄùëêùë¢|+|ùúÄùëêùëü|+|ùê∏|), and in Role-based
GRU module, the complexity is O((|ùúÄùëêùë¢|+|ùúÄùëêùëü|+|ùê∏|)ùëÅùëë‚Ä≤), whereùëÅ,
ùúÄùëêùë¢andùúÄùëêùëüare the number of nodes and role sets at current snapshot
and historical snapshots, ùê∏is the set of edges in the original graph,
ùëëandùëë‚Ä≤represent input and output dimensions, respectively. Table
6 in Appendix A illustrates a comparative analysis of the temporal
complexities of the models. Overall, RTGCN can be applied to large-
scale graph settings.
Space complexity: The space complexity of our proposed RTGCN
mainly depends on the number of snapshots of temporal graphs,
namely,ùëá. More specifically, as RTGCN applies modified GRU to
update GCN parameters on each snapshot, the number of such
parameters in our model does not increase with the increasing
number of snapshots, that is, O(ùëëùëë‚Ä≤). Moreover, since the current
snapshot hypergraph incidence matrix ùëØùëêùë¢‚ààRùëÅ√ó|ùúÄùëêùë¢|and his-
torical snapshots hypergraph incidence matrix ùëØùëêùëü‚ààRùëÅ√ó|ùúÄùëêùëü|
are shared across all snapshots, the space complexity of RTGCN
can be summarized as O(ùëëùëë‚Ä≤+ùëÅ√ó(|ùúÄùëêùë¢|+|ùúÄùëêùëü|)), where|ùúÄùëêùë¢|is
the number structural role set in current snapshot, and |ùúÄùëêùëü|is the
number structural role set in historical snapshots. Please note that
|ùúÄùëêùë¢|‚â™ùëÅand|ùúÄùëêùëü|‚â™ùëÅ.
4 EXPERIMENTS
Our investigation focuses on addressing the following research
questions: RQ1: How does the performance of RTGCN compare
with that of existing methods? RQ2: What is the individual con-
tribution of the various components within RTGCN to its over-
all performance? RQ3: How does RTGCN respond to alterations
in hyperparameter settings? RQ4: What are the implications of
RTGCN‚Äôs performance in the context of visualization and actual
runtime efficiency?
4.1 Experimental Setup
4.1.1 Datasets. We conduct extensive experiments on nine public
and popular real-world datasets to assess the quality of RTGCNTable 1: Statistics of datasets.
Dataset
#Nodes #Edges #Labels #Features #Snapshots
UCI 1,809
16,822 / / 12
Epinions 9,398 231,537 / 44 9
Alibaba 5,640 53,049 / / 11
ML-10M 20,537 43,760 / / 9
Facebook 60,730 607,487 / / 27
DBLP-3 4,257
23,540 3 100 10
DBLP-5 6,606 42,815 5 100 10
A
merica-Air 1,190 13,599 4 / 10
Europe-Air 399 5,995 4 / 10
in dynamic link prediction, dynamic new link prediction, node
classification, and structural role classification tasks. As shown in
Table 1, these datasets include UCI, Epinions, Alibaba, ML-10M,
Facebook, DBLP-3, DBLP-5, America-Air, and Europe-Air. More
details about the model implementation and experiment settings
are presented in Appendix A.1 and A.2, respectively.
4.1.2 Baselines. We employ the following methods as the baseline:
GCRN [ 23], DynAERNN [ 5], DySAT [ 22], EvolveGCN [ 17], CTGCN
[12], GAEN [ 24], HTGN [ 29], MTSN [ 13], DGCN [ 3], ROLAND [ 32],
and SpikeNet [10].
4.1.3 Evaluation Tasks and Metrics. We obtain node representa-
tions from RTGCN which can be applied to various downstream
tasks. In temporal graph representation learning, link prediction
is widely used for evaluation. Similar to HTGN, we evaluate our
proposed models on dynamic link prediction and dynamic new
link prediction tasks. More specifically, given partially observed
snapshots of a temporal graph G={G1,...,Gùë°}, dynamic link pre-
diction task is defined to predict the link in the next snapshots
Gùë°+1and dynamic new link prediction task is to predict new links
inGùë°+1that are not inGùë°. Similar to GAEN and TRRN [ 28],the
node classification tasks share a similar objective function with link
prediction, the specific objective functions can be found in Eq. (10),
and we can conduct link prediction and node classification tasks at
the same time. Following MTSN and GAEN, we use the area under
the ROC curve (AUC) and the F1 score as evaluation metrics in the
link prediction task, and the AUC and accuracy (ACC) are utilized
to evaluate the node classification performance.
4.2 RQ1: Link Prediction and Node
Classification
4.2.1 Link Prediction. We report link prediction results on seven
datasets where the best results are indicated in bold and the second-
best results are underlined, as illustrated in Tables 2 and 3.
Dynamic Link Prediction: As illustrated in Table 2, it can be
observed that the proposed RTGCN significantly outperforms other
compared methods considering both AUC and F1 on seven datasets,
and has higher stability. For instance, RTGCN achieves an average
gain of 3.28% in F1 compared to the best baseline. This demon-
strates that the RTGCN can capture structure role information in
temporal graphs, which is conducive to obtaining expressive node
representations to predict link formation and deletion.
 
659Representation Learning of Temporal Graphs with Structural Roles KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
Table 2: Performance evaluation for dynamic link prediction tasks. The results of these baselines methods (dynAE, dynAERNN,
DySAT, TSN, and MTSN) are obtained from MTSN. OOT: Out Of Time (72 hours). Please note that these methods of not showing
standard deviation are due to the original paper does not provide this information.
UCI Alibaba Epinions ML-10M DBLP3 DBLP5 Faceb
ook
Methods AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1
GCRN 0.804¬±0.005
0.728¬±0.002 0.711¬±0.004
0.647¬±0.004 0.924¬±0.002
0.871¬±0.001 0.797¬±0.006
0.714¬±0.005 0.799¬±0.011
0.726¬±0.008 0.832¬±0.005
0.755¬±0.006 0.687¬±0.007
0.635¬±0.006
DySAT 0.728
0.668 0.815
0.770 0.910
0.847 0.870
0.799 0.687¬±0.009
0.649¬±0.010 0.701¬±0.021
0.682¬±0.013 0.655¬±0.013
0.611¬±0.011
dynAE 0.582
0.615 0.645
0.615 0.556
0.663 OO
T OOT 0.742¬±0.004
0.713¬±0.005 0.796¬±0.006
0.732¬±0.007 0.785¬±0.018
0.723¬±0.023
dynAERNN 0.536
0.550 0.584
0.564 0.532
0.644 OO
T OOT 0.638¬±0.016
0.624¬±0.014 0.624¬±0.024
0.671¬±0.037 0.605¬±0.014
0.523¬±0.018
EvolveGCN 0.659¬±0.021
0.642¬±0.019 0.876¬±0.007
0.827¬±0.009 0.647¬±0.011
0.637¬±0.013 0.777¬±0.011
0.740¬±0.009 0.825¬±0.009
0.752¬±0.013 0.775¬±0.009
0.714¬±0.007 0.796¬±0.008
0.718¬±0.007
CTGCN-C 0.864¬±0.008 0.768¬±0.011 0.821¬±0.002
0.730¬±0.001 0.968¬±0.003
0.905¬±0.004 0.897¬±0.007
0.857¬±0.006 0.777¬±0.003
0.716¬±0.006 0.784¬±0.006
0.712¬±0.003 0.846¬±0.006 0.784¬±0.004
CT
GCN-S 0.862¬±0.013
0.778¬±0.009 0.819¬±0.004
0.745¬±0.003 0.977¬±0.000 0.922¬±0.001 0.865¬±0.005
0.758¬±0.008 0.723¬±0.007
0.701¬±0.005 0.780¬±0.003
0.733¬±0.003 0.764¬±0.005
0.696¬±0.006
GAEN 0.760¬±0.006
0.727¬±0.005 0.806¬±0.006
0.733¬±0.004 0.747¬±0.008
0.713¬±0.010 0.795¬±0.014
0.712¬±0.008 0.783¬±0.011
0.721¬±0.012 0.846¬±0.013
0.771¬±0.009 0.741¬±0.011
0.679¬±0.009
HTGN 0.829¬±0.002
0.772¬±0.002 0.884¬±0.007
0.832¬±0.008 0.926¬±0.002
0.867¬±0.003 0.864¬±0.012
0.784¬±0.012 0.751¬±0.019
0.687¬±0.010 0.851¬±0.012
0.777¬±0.011 0.817¬±0.013
0.725¬±0.009
TSN 0.847
0.766 0.871
0.832 0.919
0.862 0.903
0.821 0.873¬±0.017
0.773¬±0.012 0.863¬±0.028
0.764¬±0.011 0.735¬±0.020
0.701¬±0.011
MTSN 0.859
0.777 0.886
0.842 0.931
0.879 0.916 0.862 0.881¬±0.012
0.782¬±0.009 0.874¬±0.016
0.769¬±0.008 0.769¬±0.013
0.716¬±0.009
DGCN 0.613¬±0.002
0.622¬±0.005 0.810¬±0.005
0.739¬±0.004 0.734¬±0.008
0.660¬±0.006 0.724¬±0.006
0.715¬±0.009 0.838¬±0.007
0.791¬±0.005 0.804¬±0.009
0.742¬±0.007 0.677¬±0.007
0.651¬±0.008
ROLAND-s 0.834¬±0.019
0.766¬±0.016 0.838¬±0.020
0.786¬±0.023 0.926¬±0.025
0.860¬±0.021 0.824¬±0.012
0.734¬±0.014 0.896¬±0.021
0.853¬±0.011 0.890¬±0.011
0.837¬±0.015 0.785¬±0.014
0.729¬±0.013
ROLAND-l 0.854¬±0.012
0.774¬±0.010 0.888¬±0.009 0.824¬±0.006 0.938¬±0.009
0.881¬±0.004 0.836¬±0.007
0.755¬±0.010 0.904¬±0.015 0.864¬±0.012 0.895¬±0.005 0.843¬±0.006 0.793¬±0.009
0.724¬±0.008
SpikeNet 0.804¬±0.017
0.721¬±0.013 0.839¬±0.005
0.768¬±0.003 0.862¬±0.011
0.803¬±0.014 0.813¬±0.006
0.737¬±0.008 0.826¬±0.011
0.781¬±0.009 0.816¬±0.013
0.769¬±0.011 0.709¬±0.010
0.697¬±0.009
RT
GCN (ours) 0.877¬±0.002
0.796¬±0.003 0.902¬±0.002
0.864¬±0.001 0.989¬±0.001
0.960¬±0.001 0.928¬±0.005
0.889¬±0.006 0.937¬±0.001
0.901¬±0.001 0.946¬±0.003
0.898¬±0.001 0.857¬±0.004
0.793¬±0.003
Table 3: Performance evaluation for dynamic new link prediction tasks.
UCI Alibaba Epinions ML-10M DBLP3 DBLP5 Faceb
ook
Methods AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1 AUC
F1
GCRN 0.584¬±0.005
0.563¬±0.006 0.592¬±0.006
0.565¬±0.004 0.763¬±0.008
0.694¬±0.006 0.716¬±0.009
0.611¬±0.011 0.765¬±0.005
0.698¬±0.004 0.791¬±0.007
0.719¬±0.005 0.605¬±0.007
0.576¬±0.006
DySAT 0.704¬±0.004
0.651¬±0.002 0.809¬±0.007
0.761¬±0.006 0.896¬±0.009
0.829¬±0.007 0.798¬±0.004
0.707¬±0.005 0.654¬±0.004
0.625¬±0.005 0.651¬±0.006
0.641¬±0.005 0.642¬±0.005
0.603¬±0.008
dynAE 0.539¬±0.006
0.524¬±0.008 0.632¬±0.023
0.601¬±0.011 0.531¬±0.006
0.634¬±0.003 0.731¬±0.013
0.633¬±0.011 0.723¬±0.011
0.657¬±0.004 0.768¬±0.013
0.670¬±0.005 0.672¬±0.012
0.617¬±0.007
dynAERNN 0.524¬±0.009
0.517¬±0.005 0.535¬±0.009
0.543¬±0.019 0.519¬±0.002
0.627¬±0.004 0.740¬±0.014
0.589¬±0.010 0.570¬±0.013
0.608¬±0.007 0.601¬±0.012
0.661¬±0.002 0.554¬±0.013
0.576¬±0.006
EvolveGCN 0.597¬±0.002
0.610¬±0.004 0.690¬±0.012
0.666¬±0.010 0.550¬±0.004
0.610¬±0.004 0.617¬±0.013
0.595¬±0.008 0.741¬±0.014
0.700¬±0.012 0.759¬±0.005
0.702¬±0.012 0.774¬±0.010
0.706¬±0.011
CTGCN-C 0.668¬±0.004
0.571¬±0.004 0.766¬±0.006
0.692¬±0.004 0.836¬±0.004
0.722¬±0.005 0.813¬±0.009
0.702¬±0.010 0.745¬±0.006
0.685¬±0.005 0.760¬±0.006
0.691¬±0.005 0.760¬±0.007
0.709¬±0.006
CTGCN-S 0.626¬±0.003
0.600¬±0.002 0.777¬±0.003
0.711¬±0.009 0.831¬±0.005
0.729¬±0.008 0.758¬±0.007
0.652¬±0.009 0.715¬±0.002
0.654¬±0.003 0.763¬±0.003
0.711¬±0.002 0.654¬±0.004
0.610¬±0.005
GAEN 0.548¬±0.003
0.529¬±0.002 0.737¬±0.004
0.704¬±0.003 0.611¬±0.005
0.601¬±0.004 0.741¬±0.009
0.639¬±0.006 0.768¬±0.006
0.704¬±0.004 0.819¬±0.004
0.745¬±0.005 0.712¬±0.006
0.633¬±0.005
HTGN 0.815¬±0.008
0.754¬±0.013 0.835¬±0.010
0.797¬±0.008 0.880¬±0.001
0.823¬±0.004 0.850¬±0.012 0.766¬±0.005 0.700¬±0.018
0.649¬±0.013 0.800¬±0.015
0.738¬±0.018 0.804¬±0.012 0.714¬±0.011
TSN 0.702¬±0.011
0.651¬±0.010 0.778¬±0.008
0.734¬±0.009 0.812¬±0.003
0.765¬±0.005 0.808¬±0.005
0.715¬±0.006 0.690¬±0.004
0.682¬±0.007 0.753¬±0.004
0.716¬±0.002 0.729¬±0.005
0.693¬±0.004
MTSN 0.748¬±0.008
0.706¬±0.006 0.801¬±0.006
0.776¬±0.005 0.865¬±0.004
0.812¬±0.004 0.764¬±0.004
0.732¬±0.003 0.824¬±0.004
0.743¬±0.005 0.812¬±0.005
0.738¬±0.006 0.756¬±0.006
0.708¬±0.005
DGCN 0.584¬±0.002
0.612¬±0.002 0.803¬±0.003
0.726¬±0.004 0.707¬±0.004
0.619¬±0.003 0.652¬±0.004
0.661¬±0.003 0.832¬±0.004
0.773¬±0.004 0.798¬±0.002
0.727¬±0.003 0.643¬±0.004
0.632¬±0.004
ROLAND-s 0.812¬±0.005
0.754¬±0.005 0.814¬±0.011
0.766¬±0.012 0.910¬±0.009
0.846¬±0.005 0.801¬±0.006
0.716¬±0.009 0.859¬±0.007
0.836¬±0.009 0.879¬±0.009
0.822¬±0.010 0.752¬±0.007
0.703¬±0.008
ROLAND-l 0.839¬±0.008 0.762¬±0.003 0.843¬±0.005 0.812¬±0.004 0.917¬±0.003 0.850¬±0.005 0.812¬±0.006
0.737¬±0.006 0.871¬±0.004 0.842¬±0.003 0.885¬±0.003 0.822¬±0.006 0.769¬±0.004
0.713¬±0.005
SpikeNet 0.745¬±0.011
0.709¬±0.013 0.822¬±0.002
0.753¬±0.003 0.835¬±0.004
0.779¬±0.003 0.780¬±0.006
0.713¬±0.009 0.777¬±0.010
0.728¬±0.008 0.797¬±0.003
0.745¬±0.008 0.689¬±0.006
0.673¬±0.009
RT
GCN (ours) 0.863¬±0.004
0.785¬±0.003 0.891¬±0.002
0.836¬±0.003 0.948¬±0.002
0.897¬±0.004 0.887¬±0.006
0.825¬±0.005 0.924¬±0.003
0.885¬±0.005 0.928¬±0.001
0.884¬±0.002 0.824¬±0.005
0.749¬±0.006
Table 4: Performance evaluation for node classification tasks.
DBLP-3 DBLP-5
Metho
ds A
CC AUC A
CC AUC
GCRN 0.562¬±0.008
0.738¬±0.005 0.626¬±0.005
0.836¬±0.003
DySAT 0.549¬±0.016
0.729¬±0.018 0.520¬±0.008
0.765¬±0.016
dynAE 0.389¬±0.005
0.612¬±0.009 0.347¬±0.003
0.623¬±0.005
dynAERNN 0.321¬±0.024
0.482¬±0.017 0.272¬±0.012
0.501¬±0.015
EvolveGCN 0.547¬±0.011
0.765¬±0.006 0.638¬±0.011
0.847¬±0.012
CTGCN-C 0.630¬±0.003 0.781¬±0.007 0.665¬±0.010
0.857¬±0.005
CTGCN-S 0.591¬±0.011
0.756¬±0.004 0.650¬±0.005
0.854¬±0.002
GAEN 0.557¬±0.007
0.693¬±0.008 0.524¬±0.009
0.776¬±0.010
HTGN 0.556¬±0.003
0.712¬±0.004 0.538¬±0.005
0.652¬±0.009
TSN 0.444¬±0.006
0.519¬±0.007 0.276¬±0.009
0.521¬±0.019
MTSN 0.604¬±0.004
0.669¬±0.005 0.501¬±0.007
0.754¬±0.010
DGCN 0.613¬±0.004
0.754¬±0.005 0.668¬±0.006 0.859¬±0.005
ROLAND-s 0.575¬±0.008
0.745¬±0.008 0.579¬±0.010
0.703¬±0.011
ROLAND-l 0.625¬±0.009
0.797¬±0.007 0.634¬±0.003
0.768¬±0.005
SpikeNet 0.519¬±0.007
0.656¬±0.001 0.548¬±0.011
0.703¬±0.007
RT
GCN (ours) 0.676¬±0.004
0.835¬±0.003 0.690¬±0.003
0.875¬±0.002
Dynamic New Link Prediction: New link prediction aims to pre-
dict the appearance of new links, which is more challenging. From
Table 3, we observe analogous observations to the dynamic link pre-
diction task, demonstrating the superiority of the proposed RTGCN.
Specifically, all baseline methods experience performance drops
to varying degrees compared to the corresponding dynamic link
prediction task, while our RTGCN model produces more consistent
results. For instance, the performance of the baselines degradesTable 5: Structural role classification tasks.
America-
Air Eur
ope-Air
Methods A
CC AUC A
CC AUC
GCRN 0.553¬±0.007
0.701¬±0.004 0.455¬±0.009
0.635¬±0.009
DySAT 0.374¬±0.016
0.580¬±0.008 0.301¬±0.009
0.539¬±0.010
dynAE 0.563¬±0.012
0.702¬±0.011 0.505¬±0.011
0.645¬±0.009
dynAERNN 0.522¬±0.010
0.682¬±0.006 0.511¬±0.014
0.663¬±0.012
EvolveGCN 0.483¬±0.011
0.646¬±0.005 0.380¬±0.019
0.593¬±0.012
CTGCN-C 0.578¬±0.017 0.716¬±0.012 0.502¬±0.023
0.666¬±0.020
CTGCN-S 0.571¬±0.017
0.712¬±0.011 0.528¬±0.011 0.687¬±0.010
GAEN 0.345¬±0.008
0.565¬±0.003 0.400¬±0.024
0.601¬±0.017
HTGN 0.476¬±0.007
0.651¬±0.004 0.350¬±0.012
0.572¬±0.009
TSN 0.479¬±0.011
0.649¬±0.008 0.507¬±0.015
0.678¬±0.010
MTSN 0.562¬±0.007
0.706¬±0.006 0.519¬±0.008
0.683¬±0.006
DGCN 0.562¬±0.005
0.708¬±0.005 0.481¬±0.012
0.650¬±0.005
ROLAND-s 0.528¬±0.007
0.677¬±0.005 0.520¬±0.016
0.685¬±0.009
ROLAND-l 0.547¬±0.005
0.696¬±0.006 0.526¬±0.006
0.687¬±0.003
SpikeNet 0.517¬±0.004
0.674¬±0.004 0.403¬±0.016
0.608¬±0.013
RT
GCN (ours) 0.605¬±0.006
0.737¬±0.005 0.544¬±0.005
0.698¬±0.003
dramatically on Epinions (e.g., the second-best, CTGCN-S drops
from 0.977 to 0.831), but our RTGCN only declines about 0.041. In
addition, compared with the dynamic link prediction task, the per-
formance gain of the dynamic new link prediction task is greater.
For instance, RTGCN obtains a 7.7% improvement in terms of F1
compared to the best baseline on ML-10M. Our model shows strong
inductive ability via utilizing global structural role information.
 
660KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
4.2.2 Node Classification. To further evaluate the node represen-
tation quality, we consider node classification tasks. Note that link
prediction and node classification tasks share the objective function.
Node Classification: Table 4 reports the node classification per-
formance on the two labeled graphs, where the node classification
performance is much lower compared with the respective link pre-
diction results in Table 2 and Table 3. This can be attributed to
the same training mechanism employed by both compared meth-
ods and our proposed model. That is, nodes are compelled to have
similar embedding with their linked neighborhoods via connective
proximity (e.g., Eq. (8)). Therefore, the training scheme would be
more inclined to link prediction task, which is consistent with the
results of other research [ 24,28]. We can observe that RTGCN out-
performs all baseline methods, which further asserts the superiority
of RTGCN.
Structural Role Classification: To further evaluate the structural
role similarity preserving ability, we test the dynamic structural role
classification task in the European air-traffic network and American
air-traffic network. In this task, the learned node representations
are utilized to predict the structure role-related labels of nodes in
each temporal graph. As shown in Table 5, we can observe that
our proposed RTGCN outperforms all compared baselines on both
datasets, which indicates that RTGCN can preserve global structural
similarity between nodes in temporal graphs. Furthermore, CTGCN-
S and CTGCN-C also attain the second-best performance in terms
of structural role classification, which is consistent with the analysis
results of previous studies [ 12]. This demonstrates that modeling
global structural similarity and employing structural role proximity
constraints are effective ways to distinguish nodes.
4.3 RQ2: Abalation Study
To evaluate the effectiveness of different components in RTGCN,
we conduct the ablation study with several variants which are
introduced as follows: 1) W/O RGR: RTGCN replaces the role-
based GRU module by only taking the hidden state of the last
snapshot and adjacency matrix as input. 2) W/O RCU: RTGCN
does not consider the current snapshot hypergraph in the role-
based GNN module. 3) W/O RCR: RTGCN does not consider the
historical snapshots hypergraph in the role-based GNN module. 4)
W/O RRP: without the role proximity, ie., RTGCN is trained only
using the connective proximity. 5) W/O RAM: RTGCN does not use
adjacency matrix in role-based GNN module and role-based GRU
module. As shown in Figure 3 , we have the following observations:
1) RTGCN achieves the best performance when it is equipped with
all the components, and removing any component would cause
worse results. 2) Interestingly, when we only use the role-based
incidence matrix for model training, our model still achieves better
performance than all baselines on DBLP-3. In addition, RTGCN also
obtains a comparable performance on Alibaba. While on America-
Air, the performance has declined, and the potential reason is that
local structural changes dominate the evolution of the graph.
4.4 RQ3: Parameter Sensitivity
In this section, we further investigate the effect of model parame-
ters on dynamic link prediction, node classification, and structural
role classification tasks. Due to the space limit, we only present
/uni00000024/uni0000004f/uni0000004c/uni00000045/uni00000044/uni00000045/uni00000044/uni00000010/uni00000024/uni00000038/uni00000026
/uni00000024/uni0000004f/uni0000004c/uni00000045/uni00000044/uni00000045/uni00000044/uni00000010/uni00000029/uni00000014
/uni00000027/uni00000025/uni0000002f/uni00000033/uni00000010/uni00000016/uni00000010/uni00000024/uni00000026/uni00000026
/uni00000027/uni00000025/uni0000002f/uni00000033/uni00000010/uni00000016/uni00000010/uni00000024/uni00000038/uni00000026
/uni00000024/uni00000050/uni00000048/uni00000055/uni0000004c/uni00000046/uni00000044/uni00000010/uni00000024/uni0000004c/uni00000055/uni00000010/uni00000024/uni00000026/uni00000026/uni00000024/uni00000050/uni00000048/uni00000055/uni0000004c/uni00000046/uni00000044/uni00000010/uni00000024/uni0000004c/uni00000055/uni00000010/uni00000024/uni00000038/uni00000026/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000019
/uni00000013/uni00000011/uni0000001a
/uni00000013/uni00000011/uni0000001b
/uni00000013/uni00000011/uni0000001c
/uni00000014/uni00000011/uni00000013
/uni0000003a/uni00000012/uni00000032/uni00000003/uni00000035/uni0000002a/uni00000035
/uni0000003a/uni00000012/uni00000032/uni00000003/uni00000035/uni00000026/uni00000038
/uni0000003a/uni00000012/uni00000032/uni00000003/uni00000035/uni00000026/uni00000035
/uni0000003a/uni00000012/uni00000032/uni00000003/uni00000035/uni00000035/uni00000033
/uni0000003a/uni00000012/uni00000032/uni00000003/uni00000035/uni00000024/uni00000030
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031Figure 3: Performance of evaluation for ablation study.
0 100 2000.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.60.81.0
/uni00000029/uni00000014
F1
(a) The dimension ùëë‚Ä≤
0 10 200.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.60.81.0
/uni00000029/uni00000014
F1(b) Negative sampling ùõΩ
0 5 100.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.60.81.0
/uni00000029/uni00000014
F1
(c) The parameter ùúÜ
0 5 100.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.60.81.0
/uni00000029/uni00000014
F1 (d) The time decay factor ùë§
Figure 4: Performance of dynamic link prediction with dif-
ferent parameter settings on the Alibaba.
partial results of the parameter study in Figures 4 and 5. More stud-
ies regarding model parameters are in Appendix B.2. In general,
RTGCN is insensitive to different parameter settings except for
time interval Œîùë°. This is expected because an excessive amount
of historical information may introduce irrelevant noise, thereby
diluting the significance of the most important historical snapshots.
To further evaluate the generalization ability of the proposed
model, we use different structural role discovery methods on link
prediction, node classification, and structural role classification
tasks. As illustrated in Figure 6, RTGCN outperforms all baseline
models across different tasks utilizing various role discovery meth-
ods, further demonstrating that global structure role information
and role proximity can assist in producing more expressive node
presentations for various downstream tasks.
 
661Representation Learning of Temporal Graphs with Structural Roles KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
0 2 4 60.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.60.81.0
/uni00000029/uni00000014
F1
(a) Alibaba
0 2 4 60.40.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.40.60.81.0
/uni00000024/uni00000026/uni00000026
ACC (b) America-Air
0 2 4 60.40.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.40.60.81.0
/uni00000024/uni00000026/uni00000026
ACC (c) DBLP-3
Figure 5: The impact of Œîùë°on RTGCN performance.
/uni00000024/uni00000038/uni00000026 /uni00000029/uni000000140.50.60.70.80.91.0/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000056/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000047/uni00000048/uni0000004a
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni0000005a/uni0000004f
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000050/uni00000052/uni00000057/uni0000004c/uni00000049/uni00000010/uni00000057/uni00000055/uni0000004c
(a) Alibaba
/uni00000024/uni00000038/uni00000026 /uni00000024/uni00000026/uni000000260.50.60.7/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000056/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000047/uni00000048/uni0000004a
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni0000005a/uni0000004f
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000050/uni00000052/uni00000057/uni0000004c/uni00000049/uni00000010/uni00000057/uni00000055/uni0000004c (b) America-Air
/uni00000024/uni00000038/uni00000026 /uni00000024/uni00000026/uni000000260.50.60.70.8/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000056/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000047/uni00000048/uni0000004a
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni0000005a/uni0000004f
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000050/uni00000052/uni00000057/uni0000004c/uni00000049/uni00000010/uni00000057/uni00000055/uni0000004c (c) DBLP-3
Figure 6: The impact of different structural role assignment
methods on the performance of RTGCN.
/uni00000014/uni00000018/uni00000013
 /uni00000014/uni00000013/uni00000013
 /uni00000018/uni00000013
 /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013
(a) SpikeNet
/uni00000014/uni00000015/uni00000018
 /uni00000014/uni00000013/uni00000013
 /uni0000001a/uni00000018
 /uni00000018/uni00000013
 /uni00000015/uni00000018
 /uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013
/uni00000018/uni00000013
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 (b) HTGN
/uni00000017/uni00000013
 /uni00000015/uni00000013
 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013
(c) CTGCN-C
/uni00000019/uni00000013
 /uni00000017/uni00000013
 /uni00000015/uni00000013
 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013 (d) RTGCN
Figure 7: 2-D t-SNE projections of finanl node embedding
on DBLP-3 in snapshot 6. We randomly select two types of
nodes from three types of nodes in the graph, with different
colors referring to different classes.
4.5 RQ4: Visualization and Efficiency
Comparison
In the visualization task, we utilize t-SNE [ 25] to project node
embeddings of temporal graphs into a 2D space, while selecting
three SOTA baseline for comparison. From the results shown in
Figure 7, we observe that RTGCN can more clearly separate the
nodes belonging to different classes, which further demonstrates
that global structure role information is beneficial to distinguish
nodes.
Figure 8 shows the training time of each epoch for different
methods on DBLP-3 for the node classification task and Alibaba for
the dynamic link prediction task. Please note that in our proposed
/uni00000027/uni00000025/uni0000002f/uni00000033/uni00000010/uni00000016 /uni00000024/uni0000004f/uni0000004c/uni00000045/uni00000044/uni00000045/uni00000044/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni00000037
/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000012/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000000c/uni0000002a/uni00000026/uni00000035/uni00000031
/uni00000027/uni0000005c/uni00000036/uni00000024/uni00000037
/uni00000047/uni0000005c/uni00000051/uni00000024/uni00000028
/uni00000047/uni0000005c/uni00000051/uni00000024/uni00000028/uni00000035/uni00000031/uni00000031
/uni00000028/uni00000059/uni00000052/uni0000004f/uni00000059/uni00000048/uni0000002a/uni00000026/uni00000031
/uni00000026/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000036/uni00000026/uni00000037/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026
/uni0000002a/uni00000024/uni00000028/uni00000031
/uni0000002b/uni00000037/uni0000002a/uni00000031
/uni00000037/uni00000036/uni00000031
/uni00000030/uni00000037/uni00000036/uni00000031/uni00000027/uni0000002a/uni00000026/uni00000031
/uni00000035/uni00000032/uni0000002f/uni00000024/uni00000031/uni00000027/uni00000010/uni00000056
/uni00000035/uni00000032/uni0000002f/uni00000024/uni00000031/uni00000027/uni00000010/uni0000004f
/uni00000036/uni00000053/uni0000004c/uni0000004e/uni00000048/uni00000031/uni00000048/uni00000057
/uni00000035/uni00000037/uni0000002a/uni00000026/uni00000031Figure 8: Running time of different methods.
model, namely RTGCN, the runtime of the structural role discov-
ery process is also included. Based on the experimental results, it 
can be observed that our proposed model achieves an excellent 
balance between model performance and efficiency. The RTGCN 
demonstrates comparable efficiency to  the CTGCN, ROLAND, and 
SpikeNet. This achievement can be attributed to the following fac-
tors: (1) we leverage structural role-based GRU module to update the 
model parameters instead of training from scratch at each snapshot.
(2) compared with GAEN, MTSN, and CTGCN, our model benefits 
from a lightweight model architecture, with fewer computational 
costs and parameters to optimize.
5 CONCLUSION
In this work, we propose a novel structural role-based node rep-
resentation learning model for the temporal graph. To our best 
knowledge, RTGCN is the first research effort to  leverage the global 
structural role information on temporal graphs. More specifically, 
we first construct the structural role-based hypergraph. Then, we 
propose two novel modules: the structural role-based GNN module 
and the role-based GRU module, which respectively model global 
structural role information and capture graph evolving patterns. All 
modules are proposed to impel the success of RTGCN. To evaluate 
our method, we conduct various tasks on multiple real-world tem-
poral graphs. The empirical experiments demonstrate our approach 
outperforms the SOTA temporal graph representation learning 
baselines by a large margin. For future work, we intend to general-
ize our method to explore large-scale continuous temporal graphs. 
Besides, applying large language models for reasoning tasks on 
temporal graphs is also an interesting direction.
ACKNOWLEDGMENTS
The research is supported by the Key Technologies Research and De-
velopment Program under Grant No. 2020YFC0832702, and National 
Natural Science Foundation of China under Grant Nos. 71910107002, 
62376227, 61906159, 62302400, 62176014, 62201475 and Sichuan Sci-
ence and Technology Program under Grant No. 2023NSFSC0032, 
2023NSFSC0114, and Guanghua Talent Project of Southwestern 
University of Finance and Economics. Carl Yang was not 
supported by any grants from China.
662KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
REFERENCES
[1]Nesreen K Ahmed, Ryan A Rossi, John Boaz Lee, Theodore L Willke, Rong Zhou,
Xiangnan Kong, and Hoda Eldardiry. 2020. Role-based graph embeddings. TKDE
34, 5 (2020), 2401‚Äì2415. https://doi.org/IEEE
[2]Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy-
pergraph neural networks. In Proceedings of the AAAI conference on artificial
intelligence. 3558‚Äì3565.
[3]Chao Gao, Junyou Zhu, Fan Zhang, Zhen Wang, and Xuelong Li. 2022. A novel
representation learning for dynamic graphs based on graph convolutional net-
works. IEEE Transactions on Cybernetics 53, 6 (2022), 3599‚Äì3612.
[4]Wulfram Gerstner, Werner M Kistler, Richard Naud, and Liam Paninski. 2014.
Neuronal dynamics: From single neurons to networks and models of cognition.
Cambridge University Press.
[5]Palash Goyal, Sujit Rokka Chhetri, and Arquimedes Canedo. 2020. dyngraph2vec:
Capturing network dynamics using dynamic graph representation learning.
Knowledge-Based Systems 187 (2020), 104816.
[6]Pengfei Jiao, Xuan Guo, Ting Pan, Wang Zhang, Yulong Pei, and Lin Pan. 2021.
A survey on role-oriented network embedding. IEEE Transactions on Big Data 8,
4 (2021), 933‚Äì952.
[7]Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR.
[8]Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,
and Yong Li. 2023. Dynamic graph convolutional recurrent network for traffic
prediction: Benchmark and solution. ACM Transactions on Knowledge Discovery
from Data 17, 1 (2023), 1‚Äì21.
[9]Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. 2017.
Attributed network embedding for learning in a dynamic environment. In CIKM.
387‚Äì396.
[10] Jintang Li, Zhouxin Yu, Zulun Zhu, Liang Chen, Qi Yu, Zibin Zheng, Sheng Tian,
Ruofan Wu, and Changhua Meng. 2023. Scaling Up Dynamic Graph Representa-
tion Learning via Spiking Neural Networks. In AAAI. 8588‚Äì8596.
[11] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. 2017. Global
context-aware attention lstm networks for 3d action recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 1647‚Äì1656.
[12] Jingxin Liu, Chang Xu, Chang Yin, Weiqiang Wu, and You Song. 2020. K-core
based temporal graph convolutional network for dynamic graphs. IEEE Transac-
tions on Knowledge and Data Engineering 34, 8 (2020), 3841‚Äì3853.
[13] Zhijun Liu, Chao Huang, Yanwei Yu, and Junyu Dong. 2021. Motif-preserving
dynamic attributed network embedding. In Proceedings of the Web Conference.
1629‚Äì1638.
[14] Xuewei Ma, Geng Qin, Zhiyang Qiu, Mingxin Zheng, and Zhe Wang. 2019.
RiWalk: Fast structural node embedding via role identification. In ICDM. 478‚Äì
487.
[15] Yao Ma, Ziyi Guo, Zhaocun Ren, Jiliang Tang, and Dawei Yin. 2020. Stream-
ing graph neural networks. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 719‚Äì728.
[16] Giannis Nikolentzos and Michalis Vazirgiannis. 2021. Learning structural node
representations using graph kernels. IEEE transactions on knowledge and data
engineering 33, 5 (2021), 2045‚Äì2056.
[17] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,
Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:
Evolving graph convolutional networks for dynamic graphs. In AAAI. 5363‚Äì5370.
[18] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2019.
Geom-GCN: Geometric Graph Convolutional Networks. In ICLR.[19] Yulong Pei, Xin Du, Jianpeng Zhang, George Fletcher, and Mykola Pechenizkiy.
2020. struc2gauss: Structural role preserving network embedding via Gaussian
embedding. Data Mining and Knowledge Discovery 34 (2020), 1072‚Äì1103.
[20] Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao, and
Yasin Abbasi-Yadkori. 2020. A structural graph representation learning frame-
work. In Proceedings of the 13th international conference on web search and data
mining. 483‚Äì491.
[21] Ryan A Rossi, Di Jin, Sungchul Kim, Nesreen K Ahmed, Danai Koutra, and
John Boaz Lee. 2020. On proximity and structural role-based embeddings in
networks: Misconceptions, techniques, and applications. ACM Transactions on
Knowledge Discovery from Data (TKDD) 14, 5 (2020), 1‚Äì37.
[22] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.
Dysat: Deep neural representation learning on dynamic graphs via self-attention
networks. In Proceedings of the 13th international conference on web search and
data mining. 519‚Äì527.
[23] Youngjoo Seo, Micha√´l Defferrard, Pierre Vandergheynst, and Xavier Bresson.
2018. Structured sequence modeling with graph convolutional recurrent net-
works. In International conference on neural information processing. 362‚Äì373.
[24] Min Shi, Yu Huang, Xingquan Zhu, Yufei Tang, Yuan Zhuang, and Jianxun Liu.
2021. GAEN: Graph Attention Evolving Networks.. In IJCAI. 1541‚Äì1547.
[25] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008), 2579‚Äì2605.
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Proceedings of the 31st International Conference on Neural Information
Processing Systems. 6000‚Äì6010.
[27] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE
Transactions on Neural Networks and Learning Systems 32, 1 (2021), 4‚Äì24.
[28] Dongkuan Xu, Junjie Liang, Wei Cheng, Hua Wei, Haifeng Chen, and Xiang
Zhang. 2021. Transformer-style relational reasoning with dynamic memory
updating for temporal network modeling. In Proceedings of the AAAI Conference
on Artificial Intelligence. 4546‚Äì4554.
[29] Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King.
2021. Discrete-time temporal network embedding via implicit hierarchical learn-
ing in hyperbolic space. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1975‚Äì1985.
[30] Menglin Yang, Min Zhou, Hui Xiong, and Irwin King. 2022. Hyperbolic Temporal
Network Embedding. IEEE Transactions on Knowledge and Data Engineering 35,
11 (2022), 11489‚Äì11502.
[31] Qiang Yang, Changsheng Ma, Qiannan Zhang, Xin Gao, Chuxu Zhang, and
Xiangliang Zhang. 2023. Interpretable Research Interest Shift Detection with
Temporal Heterogeneous Graphs. In WSDM. 321‚Äì329.
[32] Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: graph learning
framework for dynamic graphs. In KDD. 2358‚Äì2366.
[33] Hegui Zhang and Gang Kou. 2022. Role-based multiplex network embedding. In
International Conference on Machine Learning. PMLR, 26265‚Äì26280.
[34] Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, and
Xueqi Cheng. 2023. DyTed: Disentangled Representation Learning for Discrete-
time Dynamic Graph. In KDD. 3309‚Äì3320.
[35] Wang Zhang, Xuan Guo, Wenjun Wang, Qiang Tian, Lin Pan, and Pengfei Jiao.
2021. Role-based network embedding via structural features reconstruction with
degree-regularized constraint. Knowledge-Based Systems 218 (2021), 106872.
663Representation Learning of Temporal Graphs with Structural Roles KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain
A REPRODUCIBILITY
In this section, we provide more details of model implementation
and experiment setup for the reproducibility of the experiment
results. Our implementation code is provided through the link3.
Algorithm 1: Training the RTGCN model
1Input: Temporal graph G={ùê∫1,...,ùê∫ùëá}, adjacency matrix
sequenceA={ùë®1,...,ùë®ùëá}and hypergraph incidence
matrix sequence,Hùëêùë¢=
ùëØùëêùë¢,1,...,ùëØùëêùë¢,ùëá	
and
Hùëêùëü=
ùëØùëêùëü,2,...,ùëØùëêùëü,ùëá	
.
2Output: Embedding of each graph ùê∫ùë°:ùë∂ùë°‚ààRùëõ√óùëë‚Ä≤
3Initialize the trainable weight matrices and training epoch ùêº.
4forùëó‚àà[1,ùêº]do
5 forùë°=1do
6 /* Structural role-based GNN module */
7 ùíâ(ùëô)
ùë£,1‚Üênode embeddings via Eq. (4).
8 ùëøùëô+1
ùëêùë¢,1‚Üênode embeddings via Eq. (5).
9 ùë∂1‚Üêembedding concatenation via Eq. (7).
10 end
11 forùë°‚àà[2,ùëá]do
12 /* Structural role-based GRU module */
13 cùëØùíï=ùê∫ùëÖùëà
ùë®ùë°,ùëØùëêùë¢,ùë°,ùëØùëêùëü,ùë°,EùëØùíï‚àí1
14 /* Structural role-based GNN module */
15 ùíâ(ùëô)
ùë£,ùë°‚Üêcompute node embeddings via Eq.(4).
16 ùëøùëô+1
ùëêùë¢,ùë°‚Üênode embeddings via Eq.(5).
17 ùëøùëô+1
ùëêùëü,ùë°‚Üênode embeddings via Eq. (6).
18 ùë∂ùë°‚Üêembeddings concatenation via Eq. (7).
19 end
20 Minimized the overall loss in Eq.(10).
21end
22Return{ùë∂ùë°}
A.1 Details of Model Implementation
The procedure of training RTGCN is illustrated in Algorithm 1. The
detailed implementation contains the following components:
‚Ä¢Role discovery methods: The methods for role discovery pri-
marily encompass the following three methods: degree-based,
Weisfeiler-Lehman-based, and Motif-count-based [ 14,33]. In this
study, the role assignment process of the degree-based and motif-
count-based methods employs equivalence rules (i.e., two nodes
are assigned the same structural role set if their feature vectors
are identical), while for the Weisfeiler-Lehman-based method, we
assign roles based on the first element of nodes‚Äô feature vector. It
is important to note that, with the exception of ablation experi-
ments that explore the impact of different role discovery methods
on model performance, all experimental results are derived from
the Motif-count-based role discovery method.
‚Ä¢Historical snapshots hypergraph construction: we set Œîùë°as
1 for simplifying the model calculation. Additionally, we delve
3https://github.com/trytodoit227/RTGCN-new.Table 6: Model Complexity Comparison.
Model Time Complexity
HTGN[29]O(ùëÅWùëë‚Ä≤+ùëÅùëë+ùëÅùëëùëë‚Ä≤+ùëë‚Ä≤|ùê∏|)
CTGCN[12] O(ùëÅ(ùëë+ùëë‚Ä≤)ùëò‚Ä≤+ùëôùëò|ùê∏|)
SpikeNet[10] O
ùëÅùëë2ùëÜk
DGCN[3] O |ùê∏|ùëêùëéùë£ùëîlogùëêùëéùë£ùëî
MTSN[13] O(ùëÅùëëùëë‚Ä≤+ùêøùëë|ùê∏|)
RTGCN(our) O(|ùê∏|+|ùê∏|ùëÅùëë‚Ä≤)
ùëÅis the number of nodes, ùëëandùëë‚Ä≤represent input and output
dimensions, respectively. |ùê∏|is the number of edges, ùëÜis the neigh-
borhood size, ùëêùëéùë£ùëîis the average degree of node, ùëôis the layer
number of the CGCN layers, ùëò‚Ä≤is a constant independent of ùëÅ,kis
layer SpikeNet,Wdenotes state memory length. Please note that
the reason why these methods do not present model complexity
is that the original paper does not provide this information.
deeper into the effects of Œîùë°on model performance within the
parameter study section.
The comparison of time complexities for different models is illus-
trated in Table 6. If prediction tasks are necessary for each snapshot,
the overall time complexity would be O(ùëá(|ùê∏|+|ùê∏|ùëÅùëë‚Ä≤)). If we
adopt a sliding window approach, then the model‚Äôs parameters are
reduced toO(Œîùëá(|ùê∏|+|ùê∏|ùëÅùëë‚Ä≤)). It can be observed that the time
complexity of our method is proportional to the number of edges,
comparable to the time complexities of other baselines, making it
suitable for large-scale graph tasks.
A.2 Experiment Settings
Node initial feature: In our experiment, most of the benchmark
datasets for temporal graph representation learning are only associ-
ated with topology. For UCI, Alibaba, ML-10M, Facebook, America-
Air, and Europe-Air, we employ identity matrix as the node feature
which is identical to the processing in [ 30,32]. Conversely, for
DBLP3, DBLP5, and Epinions, their nodes are associated with node
features and we directly use them in our work.
Dataset splitting: For the link prediction task, following the same
setting as in MTSN [ 13], we use 20% links in the graph at the next
snapshot as the validation set to tune model hyperparameters. Fur-
thermore, we randomly sample another 20% links for training and
use the remaining 60% for testing. For node classification task, 70%
of the nodes are used for training. Then, an additional 20% nodes
and the remaining 10% nodes are allocated for validation and test-
ing, respectively.
Parameter settings: We follow the methodology in [ 12] and com-
pute edge feature vectors by utilizing the Hadamard operation
between embedding vectors of node pairs within labeled edge sets.
We set the final embedding dimension of HTGN [ 29] as 16, and
other compared models as 128. To ensure an equitable comparison,
the maximum number of training epochs for all models is set at 200.
For all random walk-based methods, we set the window size as 6. In
addition, the number of negative sampling, walks, and walk lengths
are set to 8, 100 and 10, respectively. We utilize 2 GCN layers in the
GCRN and EvolveGCN. In CTGCN, we utilize 2 CGCN layers in
the CTGCN-C and CTGCN-S and the k-core subgraph number of
the CGCN layer is set as the maximum k-core number across all
 
664KDD‚Äô2024, Aug 25‚Äì29, 2024, Barcelona, Spain Huaming Du et al.
Table 7: Dynamic link prediction tasks.
SnapshotsUCI DBLP3 DBLP5
AUC F1 AUC F1 AUC F1
[ùëá‚àíŒîùëá
,ùëá]0.867 0.788 0.941 0.892 0.951 0.889
[0,
ùëá] 0.877 0.796 0.937 0.901 0.946 0.898
Table 8: Dynamic new link prediction tasks.
SnapshotsUCI DBLP3 DBLP5
AUC F1 AUC F1 AUC F1
[ùëá‚àíŒîùëá
,ùëá]0.855 0.789 0.928 0.884 0.916 0.874
[0,
ùëá] 0.863 0.785 0.924 0.885 0.928 0.884
snapshots. We set ùõΩas 5, the look back value as 1, ùõæ1as10‚àí6andùõæ2
as10‚àí6for dynAE and dynAERNN. The default parameter settings
of other compared models suggested by the original paper were
used. For the compared methods, we use the source code released
by the authors for baseline evaluation.
In RTGCN, the final embedding dimension ùëë‚Ä≤is set at 24, and the
number of negative sampling ùõΩand walk lengths are configured
to 10 and 20, respectively. ùúÜis set to 5 for Alibaba, 5 for DBLP-3,
and 1 for America-Air. For the dynamic link prediction task, in
our prediction layer, we only train a logistic regression classifier to
predict whether there exists a link between each node pair in the
testing set. As for the node classification task, we train a logistic
regression classifier to classify nodes into different categories based
on embedding features acquired from previous graphs up to time
ùë°. This aligns with the methodology employed in previous work
[24]. For model optimization, we employ the Adam optimizer with
a learning rate of 0.008 and weight decay of 3√ó104.
Throughout the experimentation, we performed the testing pro-
cedure 10 times and reported the average performance as the final
model result. Notably, both the baselines and RTGCN models are
uniformly trained using early stopping criteria based on validation
set performance.
B MORE EXPERIMENTAL RESULTS
B.1 Ablation Study
It is well known that not all historical information is beneficial for
predicting the next snapshot. Simultaneously, to demonstrate the
efficiency of our model, in the edge prediction task, we consider
the information from the previous Œîùëásnapshots as historical data
and compare it with using all historical snapshots. As shown in
Tables 7 and 8, our approach achieves comparable or even superior
performance by solely utilizing information from the previous Œîùëá=
1snapshots, highlighting the efficiency of our method.
To further validate the effectiveness of our proposed role-based
GRU module in modeling the dynamic evolution patterns of graphs,
we compared it with the state-of-the-art LIF [ 10] model while keep-
ing other modules unchanged. The experimental results on different
datasets, as shown in Figure 9, demonstrate that our role-based GRU
module is simpler and more effective.
B.2 Parameter Study
Here, we conduct further parameter studies on DBLP-3 and America-
Air datasets. Specifically, we explore variations in the negative
/uni00000024/uni0000004f/uni0000004c/uni00000045/uni00000044/uni00000045/uni00000044 /uni00000027/uni00000025/uni0000002f/uni00000033/uni00000016 /uni00000027/uni00000025/uni0000002f/uni00000033/uni00000018
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni000000560.0150.0540.039
0.0380.064 0.052/uni0000002f/uni0000002c/uni00000029/uni00000010/uni00000024/uni00000038/uni00000026
/uni00000030/uni0000002a/uni00000035/uni00000038/uni00000010/uni00000024/uni00000038/uni00000026
/uni0000002f/uni0000002c/uni00000029/uni00000010/uni00000029/uni00000014/uni00000030/uni0000002a/uni00000035/uni00000038/uni00000010/uni00000029/uni00000014
/uni0000002c/uni00000051/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057(a) Dynamic link prediction tasks
/uni00000024/uni0000004f/uni0000004c/uni00000045/uni00000044/uni00000045/uni00000044 /uni00000027/uni00000025/uni0000002f/uni00000033/uni00000016 /uni00000027/uni00000025/uni0000002f/uni00000033/uni00000018
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni000000560.0130.054 0.029
0.0180.076 0.046/uni0000002f/uni0000002c/uni00000029/uni00000010/uni00000024/uni00000038/uni00000026
/uni00000030/uni0000002a/uni00000035/uni00000038/uni00000010/uni00000024/uni00000038/uni00000026
/uni0000002f/uni0000002c/uni00000029/uni00000010/uni00000029/uni00000014/uni00000030/uni0000002a/uni00000035/uni00000038/uni00000010/uni00000029/uni00000014
/uni0000002c/uni00000051/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057 (b) Dynamic new link prediction tasks.
Figure 9: LIF represents the modeling of temporal features in
temporal graphs using the LIF model, while MGRU denotes
the utilization of the GRU module proposed in this paper,
which considers structural roles, for temporal modeling.
0 100 2000.40.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.40.60.81.0
/uni00000024/uni00000026/uni00000026
ACC
(a)ùëë‚Ä≤
0 10 200.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC(b)ùõΩ
0 5 100.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC (c)ùúÜ
0 5 100.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC (d)ùë§
Figure 10: Performance of node classification with different
parameter settings on the DBLP-3.
0 100 2000.40.60.81.0/uni00000024/uni00000038/uni00000026
AUC
0.40.60.81.0
/uni00000024/uni00000026/uni00000026
ACC
(a)ùëë‚Ä≤
0 10 200.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC(b)ùõΩ
0 5 100.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC (c)ùúÜ
0 5 100.40.60.81.0/uni00000024/uni00000026/uni00000026
ACC
0.40.60.81.0
/uni00000024/uni00000038/uni00000026
AUC (d)ùë§
Figure 11: Performance of structural role classification with
different parameter settings on the America-Air.
sampling parameter ùõΩfrom the set{1,3,5,10,15,20,25}, as well
as the representation dimension within the set {32,64,128,256}.
Furthermore, we examine the impact of ùúÜand the time decay factor
ùë§across{1,3,5,7,9,11}. In addition, we also explore the impact
of historical information from different time intervals Œîùë°on the
performance of our model, and the Œîùë°is tuned in{1,2,3,4,5}. From
the results shown in 10 and 11, we can observe that the performance
of RTGCN is generally stable under different parameter settings,
except for the time decay factor ùë§and time interval Œîùë°.
To further validate the effectiveness of the proposed model, we
use different role discovery algorithms for node role assignment.
Here, we employ three role discovery methods (RTGCN-deg: degree-
based, RTGCN-wl: Weisfeiler-Lehman-based, and, RTGCN-motif-
tri: Motif-count-based) to conduct dynamic link prediction, node
classification, and structural role classification tasks, respectively.
As shown in Figure 6 in section 4.4, our model consistently surpasses
all baseline methods under different role discovery methods. These
observations further validate the effectiveness of structural role
information and the robustness of our method.
 
665