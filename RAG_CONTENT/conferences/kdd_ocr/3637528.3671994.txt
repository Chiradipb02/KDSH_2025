Distributional Network of Networks for Modeling
Data Heterogeneity
Jun Wu
University of Illinois at
Urbana-Champaign
Champaign, USA
junwu3@illinois.eduJingrui He
University of Illinois at
Urbana-Champaign
Champaign, USA
jingrui@illinois.eduHanghang Tong
University of Illinois at
Urbana-Champaign
Champaign, USA
htong@illinois.edu
Abstract
Heterogeneous data widely exists in various high-impact applica-
tions. Domain adaptation and out-of-distribution generalization
paradigms have been formulated to handle the data heterogeneity
across domains. However, most existing domain adaptation and
out-of-distribution generalization algorithms do not explicitly ex-
plain how the label information can be adaptively propagated from
the source domains to the target domain. Furthermore, little effort
has been devoted to theoretically understanding the convergence
of existing algorithms based on neural networks.
To address these problems, in this paper, we propose a generic
distributional network of networks ( TENON ) framework, where each
node of the main network represents an individual domain associ-
ated with a domain-specific network. In this case, the edges within
the main network indicate the domain similarity, and the edges
within each network indicate the sample similarity. The crucial idea
ofTENON is to characterize the within-domain label smoothness
and cross-domain parameter smoothness in a unified framework.
The convergence and optimality of TENON are theoretically ana-
lyzed. Furthermore, we show that based on the TENON framework,
domain adaptation and out-of-distribution generalization can be
naturally formulated as transductive and inductive distribution
learning problems, respectively. This motivates us to develop two
instantiated algorithms ( TENON -DAandTENON -OOD) of the proposed
TENON framework for domain adaptation and out-of-distribution
generalization. The effectiveness and efficiency of TENON -DAand
TENON-OOD are verified both theoretically and empirically.
CCS Concepts
•Computing methodologies →Transfer learning.
Keywords
network of networks, data heterogeneity, domain adaptation, out-
of-distribution generalization
ACM Reference Format:
Jun Wu, Jingrui He, and Hanghang Tong. 2024. Distributional Network of
Networks for Modeling Data Heterogeneity. In Proceedings of the 30th ACM
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671994
Domain adaptation 
(Transductive distribution learning)
Out-of-distribution generalization 
(Inductive distribution learning)
(b) Network of networksDomain adaptation 
Out-of-distribution generalization 
(a) Heterogeneous domainsSource ℙ𝟏𝒔Source ℙ𝟐𝒔
Source ℙ𝟑𝒔Source ℙ𝟒𝒔Target ℙ𝒕
Target ℙ𝟏𝒕
(unseen)
Target ℙ𝟐𝒕
(unseen)Source ℙ𝟑𝒔Source ℙ𝟒𝒔
Source ℙ𝟏𝒔Source ℙ𝟐𝒔
Figure 1: Illustration of the network of networks on handing
heterogeneous domains (semantic classification on Amazon
products [ 8] is used where green indicates negative review
and orange indicates positive review). (a) Domain adapta-
tion and out-of-distribution (OOD) generalization involve
different domains. Target domains are unseen during train-
ing for out-of-distribution generalization. (b) In the network
of networks, each node of the main network represents one
domain, and it is formed by a network over domain-specific
samples. For OOD generalization, the dotted lines indicate
that the edges between source and target domains are acces-
sible only during the testing phase.
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671994
1 Introduction
Modern machine learning algorithms have demonstrated remark-
able success across a wide range of high-impact applications, such
as sentiment analysis [ 57], news tagging classification [ 31], etc.
One common assumption behind these algorithms is that the train-
ing and test samples are independently and identically distributed
(IID). However, this IID assumption is often violated in real scenar-
ios where the samples are collected from heterogeneous domains
under distribution shift [ 48], e.g., Amazon review collected from
different products [ 8], news headlines collected from different time
stamps [ 53]. Two learning paradigms have been developed to ad-
dress the challenge of data heterogeneity across domains: domain
adaptation [ 4,57] and out-of-distribution generalization [ 6,34].
3379
KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
As shown in Figure 1(a), domain adaptation1aims at learning a
prediction function on a target domain with only unlabeled train-
ing samples, by exploiting knowledge from source domains. In
contrast, out-of-distribution generalization optimizes a domain-
agnostic model from source domains such that this model can be
directly applied to any relevant unseen target domains. Different
from domain adaptation, target domains are unseen during training
for out-of-distribution generalization.
Most existing domain adaptation and out-of-distribution gen-
eralization algorithms [ 2,28,47,57] build a single model to learn
the domain-invariant representation from different domains. The
invariant representation learned by a domain-agnostic model can
be explained as the common knowledge shared by all domains.
Nevertheless, it is a strong assumption that all domains share the
same model parameters. This is because this assumption underes-
timates the domain-specific characteristics encoding class separa-
bility. Though recent works [ 7,40,51] propose to learn both with-
domain specificity and cross-domain commonality, disentangling
domain-invariant and domain-specific representations is a nontriv-
ial task. This is because it is challenging to accurately differentiate
the domain-invariant representation from the domain-specific rep-
resentation within samples. The aforementioned frameworks might
suffer from the following limitations. First, the connection between
the domain relationship and the model (parameters) similarity is
under-explored, e.g., similar domains may share similar model pa-
rameters [ 24,49]. Second, it is not explained how the label informa-
tion can be adaptively propagated from the source domain to the
target domain. Third, little effort has been devoted to understanding
the model convergence of previous algorithms [ 2,45,47,57] based
on deep neural networks.
To this end, in this paper, we propose a generic distributional
network of networks ( TENON ) framework, which allows each do-
main to learn domain-specific model parameters. It is motivated
by recent observation [ 6,50] that both domain adaptation and out-
of-distribution generalization can be explained as follows. Given a
meta-distribution P, the data distributions P1,···,P𝐾of different
domains can be considered as IID realizations of P, and the samples
{𝑥𝑘
𝑖,𝑦𝑘
𝑖}𝑛𝑘
𝑖=1within domain 𝑘are IID realizations from P𝑘. Having
this in mind, TENON reformulates the heterogeneous domains as a
network of networks [ 36], which encodes both high-level domain
relationships and low-level sample relationships. As shown in Fig-
ure 1(b), the main network characterizes the relationship among
different domains, where each node (blue circles) is a domain, and
the edges (solid or dotted blue lines) imply domain similarity. Each
domain is further represented by a domain-specific network (e.g., a
network within each blue circle), where each node (colored prod-
ucts) is a sample, and the edges (black lines) imply sample similarity.
The intuition behind TENON is that (i) domains share similar model
parameters [ 49,56] if they have similar data distributions, and (ii)
samples tend to have similar class labels if they are similar in the
input space [ 58,60]. To this end, the proposed TENON framework
is composed of both within-domain label smoothness and cross-
domain parameter smoothness regularizations. We theoretically
1It is also termed as "multi-source domain adaptation" to indicate the existence of
multiple source domains in previous works [ 39,57]. In this paper, we use the generic
term "domain adaptation" by assuming that at least one source domain is available.show that the convergence and optimality of TENON can be guar-
anteed when using overparameterized neural networks [ 20,25] to
instantiate the learning functions of TENON.
More specifically, Figure 1(b) shows that domain adaptation [ 57]
and out-of-distribution generalization [ 6,34] can be naturally con-
sidered as transductive and inductive distribution learning prob-
lems, respectively. That is, domain adaptation can build a network of
networks using source and target domains as a pre-processing step.
Here the sample similarity within each domain-specific network
and domain similarity within the main network can be empirically
estimated using input samples. Then using the constructed network
of networks, we propose an instantiated algorithm ( TENON -DA) of
TENON to propagate the knowledge from labeled source domains
to the unlabeled target domain for domain adaptation. In contrast,
out-of-distribution generalization can only access source (training)
domains, and thus we build a network of networks over source
domains during training. During the testing phase, target (testing)
domains will be added to the main network as the new nodes. As a
result, out-of-distribution generalization is formulated as an induc-
tive learning [ 17] problem w.r.t. the network of networks. To solve
this problem, we propose another instantiated algorithm ( TENON -
OOD) ofTENON to generalize the relevant knowledge from source
(training) domains to target (testing) domains. The effectiveness
and efficiency of TENON -DAandTENON -OODare demonstrated in a
variety of data mining tasks. The major contributions of this paper
are summarized as follows.
•Framework: We propose a generic distributional network
of networks ( TENON ) framework for modeling data hetero-
geneity across domains. Notably, TENON provides a unified
viewpoint of domain adaptation and out-of-distribution gen-
eralization. Furthermore, the convergence and optimality of
TENON are theoretically analyzed.
•Algorithms: We provide two instantiated algorithms (i.e.,
TENON-DA andTENON-OOD ) ofTENON for domain adaptation
and out-of-distribution generalization. It is revealed that both
algorithms inherit the convergence properties of the TENON
framework. Besides, in the context of domain adaptation, we
show that TENON-DA minimizes the error upper bound of the
target domain.
•Experiments: Extensive experiments on various data sets
demonstrate the effectiveness and efficiency of the proposed
algorithms for both domain adaptation and out-of-distribution
generalization.
The rest of the paper is organized as follows. Section 2 summa-
rizes the related work and Section 3 provides the problem settings.
In Section 4, we propose a novel distributional network of networks
(TENON ) framework, followed by the instantiated algorithms for do-
main adaptation and out-of-distribution generalization in Section 5.
Section 6 shows the experimental results, and finally, we conclude
the paper in Section 7.
2 Related Work
2.1 Domain Adaptation
Domain adaptation [ 4,35] studies the transfer of knowledge or
information from source domains to a relevant target domain. It is
theoretically shown [ 1,44,47,57] that the generalization error of
3380Distributional Network of Networks for Modeling
Data Heterogeneity KDD ’24, August 25–29, 2024, Barcelona, Spain
a learning algorithm within the target domain can be bounded by
the source errors and domain discrepancy. This thus leads to the
domain adaptation algorithms [ 9,14,30,33,39,50,55] by empiri-
cally minimizing the prediction errors within source domains and
distribution discrepancy across domains. The most similar works to
ours include [ 5,52], where Xu et al . [52] build a domain graph to en-
code topological structures among different domains and Berthelot
et al. [5] unify the semi-supervised learning and domain adapta-
tion. However, our TENON framework is fundamentally different
from previous works in the following aspects. First, previous works
leverage a single model to learn domain-invariant representation,
whereas TENON enables the domain-specific models to character-
ize the domain relationship. Second, the global convergence and
optimality of TENON are analyzed theoretically. In contrast, little
theoretical analysis regarding the convergence of domain adapta-
tion algorithms is provided in previous works. Third, our TENON
framework can be applied to both domain adaptation and out-of-
distribution generalization, while previous works consider only the
domain adaptation settings.
2.2 Out-of-Distribution Generalization
Out-of-distribution (OOD) generalization aims at learning a domain-
agnostic model from an arbitrary number of training source do-
mains [ 6,21,34]. In recent years, various OOD generalization al-
gorithms have been proposed from the following aspects: domain-
invariant representation learning [ 2,28], meta regularization [ 3,27],
domain augmentation [ 46,59], gradient operation [ 42,45], etc.
These algorithms directly apply the learned model to the new test-
ing domains. Compared to previous works, the proposed TENON
framework focuses on explicitly propagating model parameters
from training to testing domains based on the distribution simi-
larity among domains. This is in sharp contrast to previous works
which learn a commonly shared model among all domains.
3 Problem Definitions
We letXandYbe the input space and output space, respec-
tively. Suppose there are 𝐾different domains drawn from a meta-
distribution P, i.e.,P1,···,P𝐾∼Pwhere P𝑘denotes the data
distribution2of the𝑘thdomain overX×Y . Each domain is asso-
ciated with a model 𝑓(·;𝜃𝑘):X→Y parameterized by 𝜃𝑘. There
are𝑛𝑘labeled or unlabeled samples in domain 𝑘, where𝑥𝑘
𝑖∈Xis
the input sample and 𝑦𝑘
𝑖is the output label if available. In addition,
we let Idenote the identity matrix, ||·||𝑝and||·||𝐹denote𝐿𝑝norm
and Frobenius norm, respectively.
Following [ 4], we focus on the problem of learning from different
domains, where data heterogeneity exists among domains. Specif-
ically, in this paper, we focus on two research problems: domain
adaptation [ 4,57] and out-of-distribution generalization [ 6,21].
Both research problems involve modeling the data heterogeneity
across domains. Their goal is to learn a prediction function on
the target domain without label information, by leveraging latent
knowledge from relevant source domains.
2In this paper, we will use P𝑘to denote both the data distribution of domain 𝑘and
the domain𝑘itself.Problem Definition 1 (Domain Adaptation). Given a set of
source domains{P𝑘}𝐾−1
𝑘=1each with labeled samples {𝑥𝑘
𝑖,𝑦𝑘
𝑖}𝑛𝑘
𝑖=1, and
a target domain P𝐾with only unlabeled samples {𝑥𝐾
𝑖}𝑛𝐾
𝑖=1, domain
adaptation aims to learn a prediction function on the target domain
using knowledge from source domains.
Problem Definition 2 (Out-of-Distribution Generaliza-
tion). Given a set of source domains {P𝑘}𝐾
𝑘=1each with samples
{𝑥𝑘
𝑖,𝑦𝑘
𝑖}𝑛𝑘
𝑖=1, out-of-distribution generalization aims to learn a predic-
tion function from source domains such that this prediction function
can be directly applied to unseen target domains.
As illustrated in Figure 1, a group of distributions (or domains)
{P𝑘}𝐾
𝑘=1over a meta-distribution Pcan be formulated as a network
of networks [ 36], where each node of the main network represents
a domain and each network is formed by domain-specific samples.
This motivates us to rethink the modeling of data heterogeneity
by capturing both sample similarity within domains and distribu-
tion similarity across domains. First, in each domain, two samples
tend to have similar output values if they are similar in the input
space [ 19,58,60]. Second, given a learning algorithm 𝑓(·), two
domains would be close in the parameter space if they are distribu-
tionally similar [49, 56].
4 Proposed Framework
In this section, we propose a simple and generic distributional net-
work of networks ( TENON ) framework for modeling heterogeneous
data from multiple domains.
4.1 Distributional Network of Networks
It is shown [ 50] that knowledge transferability can be positively
correlated with the distribution similarity across domains. This
motivates us to model the heterogeneous domains by capturing the
domain relationship in the parameter space (shown in Figure 2). To
this end, we propose a simple yet generic distributional network of
networks ( TENON ) framework with the following objective function.
min
{𝜃𝑘}𝐾
𝑘=1𝜆𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖=1𝑓(𝑥𝑘
𝑖;𝜃𝑘)−𝑦𝑘
𝑖2
2
|                      {z                      }
Label consistency within domain
+1
2𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖,𝑗=1𝑠𝑘
𝑖𝑗𝑓(𝑥𝑘
𝑖;𝜃𝑘)
√︃
𝐷𝑘
𝑖𝑖−𝑓(𝑥𝑘
𝑗;𝜃𝑘)
√︃
𝐷𝑘
𝑗𝑗2
2|                                       {z                                       }
Label smoothness within domain
+1
2𝐾∑︁
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√︁
𝑀𝑘𝑘−𝜃𝑘′√︁
𝑀𝑘′𝑘′2
𝐹|                                    {z                                    }
Parameter smoothness across domains(1)
where𝑠𝑘
𝑖𝑗indicates the sample similarity between 𝑥𝑘
𝑖and𝑥𝑘
𝑗within
the𝑘-th domain, and 𝑑𝑘𝑘′denotes the domain similarity between
the𝑘-th domain and the 𝑘′-th domain. Here 𝐷𝑘
𝑖𝑖=Í𝑛𝑘
𝑗=1𝑠𝑘
𝑖𝑗and
𝑀𝑘𝑘=Í𝐾
𝑘′=1𝑑𝑘𝑘′.𝜃𝑘denotes the model parameters within the
𝑘-th domain. 𝜆>0is a hyper-parameter to balance different terms.
3381KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
𝜃!
𝜃"
𝜃#𝜃$Parameter space
Domain 2Domain 1Domain 3
Domain 4
Parameter propagation Label propagationParameters True/Predicted labelsLabel 
space
Label 
spaceLabel 
space
Label 
space
Figure 2: Illustration of TENON in information propagation.
Label information is propagated in the labeling space within
each domain, while parameter information is propagated in
the parameter space across domains.
Following [ 19,58], the sample similarity 𝑠𝑘
𝑖𝑗can be empirically
estimated as follows.
𝑠𝑘
𝑖𝑗=exp
−𝜎·𝑥𝑘
𝑖−𝑥𝑘
𝑗1
where𝜎∈Ris a hyper-parameter. In addition, a variety of do-
main discrepancy measures have been proposed to model the het-
erogeneous domains, e.g., HΔH-divergence [ 4], Maximum Mean
Discrepancy [ 15,30], Wasserstein distance [ 44],𝑓-divergence [ 1],
etc. It is flexible in defining 𝑑𝑘𝑘′in Eq. (1) based on existing do-
main discrepancy measures. In this paper, under the covariate shift
assumption [ 38] (i.e.,P(𝑦|𝑥)is shared for all domains), we use Maxi-
mum Mean Discrepancy (MMD) [ 15] to define the domain similarity
𝑑𝑘𝑘′as follows.
MMD(𝑘,𝑘′)=1
𝑛𝑘𝑛𝑘∑︁
𝑖=1𝜙(𝑥𝑘
𝑖)−1
𝑛𝑘′𝑛𝑘′∑︁
𝑗=1𝜙(𝑥𝑘′
𝑗)2
HK
𝑑𝑘𝑘′=exp −𝜎·MMD(𝑘,𝑘′)
where𝜙(·):X→HKis a kernel mapping from an input space X
to a reproducing kernel Hilbert space (RKHS) HK.
The intuition behind Eq. (1) is explained as follows. The first term
captures the consistency of models {𝑓(·;𝜃𝑘)}𝐾
𝑘=1with the prior la-
bel information. The second term measures the label smoothness
within each domain. It implies that input samples have similar pre-
diction values if they are similar in the input space. Furthermore,
the third term measures the cross-domain model smoothness in the
parameter space. Notably, graph-based parameter smoothness regu-
larization [ 29,56] has been studied in multi-task learning. However,
compared to previous works, our framework of Eq. (1) explicitly re-
veals the connection between the domain distribution discrepancy
and the model (parameters) similarity, i.e., domains have similar
model parameters if they are distributionally similar. Furthermore,
by incorporating the within-domain label smoothness regulariza-
tion (i.e., the second term of Eq. (1)), TENON allows propagating
label information from labeled source samples to unlabeled target
samples, whereas previous works [ 29,56] collaboratively update
the model parameters over the labeled samples from all domains.As shown in Figure 2, the label information encoded by a domain-
specific model is propagated within each domain, while the model
information is propagated across domains in the parameter space.
We show in Subsection 4.3 that in the special case where 𝑑𝑘𝑘′=0
for all domains 𝑘,𝑘′, the objective of TENON in Eq. (1) exactly re-
covers the label propagation [ 58,60] in every domain. On top of
label propagation, the parameter propagation of TENON enables han-
dling data heterogeneity when samples are collected from multiple
domains [33, 49, 57].
4.2 Convergence Analysis
The convergence and optimality of TENON can be analyzed by con-
sidering different instantiations of learning models {𝑓(·;𝜃𝑘)}𝐾
𝑘=1.
In the following, we start with the simple linear regression func-
tions, i.e.,𝑓(𝑥;𝜃𝑘)=𝜃𝑇
𝑘𝑥for all𝑘∈{1,2,···,𝐾}. The following
lemma shows the global convergence and optimality of the TENON
framework.
Lemma 3. Given linear models 𝑓(𝑥;𝜃𝑘)=𝜃𝑇
𝑘𝑥for𝑘∈{1,···,𝐾},
the objective of Eq. (1) can be minimized at
Θ∗=𝜆ˆX
ˆA+𝜆I
ˆX𝑇ˆX+
ˆX𝑇ˆX−1ˆX𝑇ˆBˆX−1
y
where Θ=[𝜃𝑇
1,···,𝜃𝑇
𝐾]𝑇,X𝑘=[𝑥𝑘
1,𝑥𝑘
2,···,𝑥𝑘𝑛𝑘],y=[𝑦1
1,···,𝑦1𝑛1,
···,𝑦𝐾
1,···,𝑦𝐾𝑛𝐾]𝑇and
ˆX=X10··· 0
0 X 2··· 0
............
0 0··· X𝐾, ˆA=¯A10··· 0
0 ¯A2··· 0
............
0 0··· ¯A𝐾
ˆB=I𝐾𝑑𝑖𝑛×𝐾𝑑𝑖𝑛−B⊗I𝑑𝑖𝑛×𝑑𝑖𝑛
where A𝑘=(D𝑘)−1/2S𝑘(D𝑘)−1/2is the normalized sample similar-
ity matrix of domain 𝑘with ¯A𝑘=I−A𝑘, and B=M−1/2DM−1/2is
the normalized domain similarity matrix.3⊗denotes the Kronecker
product of two matrices. 𝑑𝑖𝑛is the dimensionality of input samples.
Next, we instantiate the learning models {𝑓(·;𝜃𝑘)}𝐾
𝑘=1with over-
parameterized neural networks [ 20,25]. This allows us to reveal
the convergence of TENON in Eq. (1) with commonly used neural
network architectures4. For notation simplicity, we will use 𝑓(·;Θ)
to denote the overall learning function with 𝑓(𝑥𝑘;Θ)=𝑓(𝑥𝑘;𝜃𝑘)
for any sample 𝑥𝑘from domain 𝑘. It is observed [ 25] that neu-
ral network 𝑓(·;Θ)can be approximated by its linearized version
𝑓lin(·;Θ), i.e., sup𝑡≥0𝑓𝑡(𝑥;Θ)−𝑓lin
𝑡(𝑥;Θ)=O(ℎ−1
2)whereℎis
the width of neural networks.5𝑓𝑡(·;Θ)denotes the model at time
step𝑡, and𝑓lin(·;Θ)is given by the first order Taylor expansion of
𝑓(·;Θ):𝑓lin
𝑡(𝑥;Θ)=𝑓0(𝑥;Θ)+∇𝑓0(𝑥;Θ)(Θ𝑡−Θ0)Inspired by this
observation, we generalize the results of Lemma 3 by instantiating
{𝑓(·;𝜃𝑘)}𝐾
𝑘=1with neural networks. The following theorem shows
3S𝑘is sample similarity matrix of domain 𝑘with the entry[S𝑘]𝑖𝑗=𝑠𝑘
𝑖𝑗, and D𝑘is
a diagnal matrix with the entry [D𝑘]𝑖𝑖=𝐷𝑘
𝑖𝑖.Dis domain similarity matrix with
the entry[D]𝑘𝑘′=𝑑𝑘𝑘′, and Mis a diagnal matrix with the entry [M]𝑘𝑘=𝑀𝑘𝑘.
4Following [ 25],𝜃𝑘denotes the vectorized parameters of the neural network model
within domain 𝑘here.
5In this case, "width" can be the number of neurons in a fully-connected layer or the
number of channels in a convolutional layer.
3382Distributional Network of Networks for Modeling
Data Heterogeneity KDD ’24, August 25–29, 2024, Barcelona, Spain
the global convergence of TENON under gradient descent when the
layer width of{𝑓(·;𝜃𝑘)}𝐾
𝑘=1goes to infinity.
Theorem 4 (Convergence and Optimality of TENON ).LetX
denote all training samples. In the limit of layer width, the model
parameters Θin the objective of Eq. (1) converges to
lim𝑡→∞Θ𝑡=−∇Θ𝑓0(X)𝑇K−1
NTKΓ−1(Ω−𝜆y)+Θ0
where𝑡is the training time step, Θ0denotes the initialized parameters,
and𝑓0(X)=vec(𝑓0(𝑥𝑘
𝑖;𝜃𝑘)|𝑖∈{1,2,···,𝑛𝑘},𝑘∈{1,2,···,𝐾})is
model output with initialized parameters. Moreover, the prediction
function𝑓(·;𝜃𝑘)of Eq. (1) for any testing sample 𝑥𝑘within domain
𝑘converges to
lim𝑡→∞𝑓𝑡(𝑥𝑘;𝜃𝑘)=𝜆KNTK(𝑥𝑘,X)K−1
NTKΓ−1y
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)K−1
NTKΓ−1Ω
where
Γ=ˆA+𝜆I+K−1
NTK∇Θ𝑓0(X)ˆB∇Θ𝑓0(X)𝑇K−1
NTK
Ω=K−1
NTK∇Θ𝑓0(X)ˆBΘ0+(ˆA+𝜆I)𝑓0(X)
KNTK(𝑥𝑘,X)=[0,···,0, 𝜔𝑘
1,···,𝜔𝑘
𝑛𝑘|         {z         }
Within domain 𝑘,0,···,0]
andKNTK=diag(K11,K22,···,K𝐾𝐾).K𝑘𝑘is a neural tangent ker-
nel [20] matrix within domain 𝑘, i.e., its entry is given by [K𝑘𝑘]𝑖𝑗=D
∇𝜃𝑘𝑓0(𝑥𝑘
𝑖;𝜃𝑘),∇𝜃𝑘𝑓0(𝑥𝑘
𝑗;𝜃𝑘)E
.𝜔𝑘
𝑖=D
∇𝜃𝑘𝑓0(𝑥𝑘;𝜃𝑘),∇𝜃𝑘𝑓0(𝑥𝑘
𝑖;𝜃𝑘)E
.
4.3 Discussion
In this section, we provide a more intuitive explanation regarding
how the proposed TENON framework enables within-domain label
propagation and cross-domain parameter propagation, respectively.
Corollary 5 (Individual Label Propagation). In the special
case where𝑑𝑘𝑘′=0(𝑘≠𝑘′), with the same conditions as Theorem 4,
for any𝑘∈{1,···,𝐾}, the predicted values of 𝑓(·;𝜃𝑘)in Eq. (1) over
the training samples X𝑘=[𝑥𝑘
1,···,𝑥𝑘𝑛𝑘]in domain𝑘converge to
lim𝑡→∞𝑓𝑡(X𝑘;𝜃𝑘)=(1−𝛼)(I−𝛼A𝑘)−1y𝑘 (2)
where𝑓𝑡(X𝑘;𝜃𝑘)=[𝑓𝑡(𝑥𝑘
1;𝜃𝑘),𝑓𝑡(𝑥𝑘
2;𝜃𝑘),···,𝑓𝑡(𝑥𝑘𝑛𝑘;𝜃𝑘)]𝑇,y𝑘=
[𝑦𝑘
1,𝑦𝑘
2,···,𝑦𝑘𝑛𝑘]𝑇, and𝛼=1
𝜆+1. Furthermore, for any testing sample
𝑥𝑘, it holds
lim𝑡→∞𝑓𝑡(𝑥𝑘;𝜃𝑘)=(1−𝛼)KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘(I−𝛼A𝑘)−1y𝑘
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘𝑓0(X𝑘)(3)
It can be seen from Eq. (2) in Corollary 5 that when all domains
are irrelevant (i.e., 𝑑𝑘𝑘′=0for any𝑘≠𝑘′), the objective of TENON
is equivalent to standard label propagation [ 19,58,60] on each
individual domain and no knowledge is shared across domains. Fur-
thermore, previous label propagation approaches [ 58,60] focus on
transductive semi-supervised learning, where labels are inferred for
a set of unlabeled training samples (shown in Eq. (2)), whereas Corol-
lary 5 provides a feasible solution for inductive semi-supervised
learning, where the labels can be inferred for new unseen testing
samples (shown in Eq. (3)).Algorithm 1 TENON-DA
Input:(𝐾−1)source domains{P𝑘}𝐾−1
𝑘=1, a target domain P𝐾;
Output: Predicted output values of target samples.
1:——————— Training Stage (Pre-computing) ———————
2:Calculate all sample similarity 𝑠𝑘
𝑖𝑗and domain similarity 𝑑𝑘𝑘′;
3:for𝑘=1,···,𝐾do
4: Calculate block neural tangent kernel K𝑘𝑘;
5: Calculate inverse matrix K−1
𝑘𝑘;
6:for𝑘′=𝑘+1,···,𝐾do
7: Calculate block neural tangent kernel K𝑘𝑘′;
8:end for
9:end for
10:Calculate ΓKNTK;
11:Calculate y∗=𝜆K−1
NTKΓ−1y=𝜆(ΓKNTK)−1y;
12:Obtain target propagated labels y∗
𝐾=[y∗]−𝑛𝐾:;
13:——————— Inference Stage ————————————————
14:fortesting sample 𝑥test
𝐾from target domain 𝐾do
15: Calculate neural tangent kernel KKK(𝑥test
𝐾,X𝐾);
16: Calculate𝑦test
𝐾=KKK(𝑥test
𝐾,X𝐾)y∗
𝐾;
17:end for
Corollary 6 (Global Parameter Propagation). In the special
case where𝑠𝑘
𝑖𝑗=0(𝑖≠𝑗,𝑘=1,···,𝐾), with the same conditions
as Theorem 4, for any 𝑘∈{1,···,𝐾}, the model parameters 𝜃𝑘of
𝑓(·;𝜃𝑘)in Eq. (1) is updated under gradient descent as follows.
𝜃𝑘(𝑡+1)=
(1−𝜂)I−𝜆∇𝑓(X𝑘)𝑇∇𝑓(X𝑘)
𝜃𝑘(𝑡)
+𝜂𝐾∑︁
𝑘′=1𝑑𝑘𝑘′√︁
𝑀𝑘𝑘𝑀𝑘′𝑘′𝜃𝑘′(𝑡)+𝜂𝜆∇𝑓(X𝑘)𝑇y𝑘
where𝜂is the learning rate and 𝜃𝑘(𝑡)denotes the model parameters
𝜃𝑘at time step 𝑡.
Corollary 6 reveals that if we do not consider the sample similar-
ity, i.e.,𝑠𝑘
𝑖𝑗=0(𝑖≠𝑗,𝑘=1,···,𝐾), the model parameters 𝜃𝑘of the
domain𝑘would recursively aggregate knowledge from all other
domains. More specifically, if two domains have similar data distri-
butions, i.e., 𝑑𝑘𝑘′is large, it is more likely to propagate parameter
knowledge between these two domains. This observation is also
consistent with previous works [24, 49].
5 Proposed Algorithms
In this section, we provide two instantiated algorithms of TENON for
domain adaptation ( TENON -DA) and out-of-distribution generaliza-
tion ( TENON -OOD). The crucial idea is to formulate domain adaptation
and out-of-distribution generalization as transductive distribution
learning and inductive distribution learning w.r.t. network of net-
works [36], respectively.
5.1 Transductive Distribution Learning
We formulate domain adaptation [ 47] as a transductive distribution
learning problem. As shown in Figure 1(b), each domain (source or
target domain) is formulated as a node in the main network, and
samples within each domain form a domain-specific network. Thus,
domain adaptation aims to propagate the label information (1) from
3383KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
source domains to the target domain (domain-level propagation)
and (2) from labeled samples to unlabeled samples (sample-level
propagation). To this end, we instantiate the proposed TENON frame-
work (denoted as TENON-DA) for domain adaptation below.
Given𝐾−1source domains{P𝑘}𝐾−1
𝑘=1each with labeled samples
{𝑥𝑘
𝑖,𝑦𝑘
𝑖}𝑛𝑘
𝑖=1, and a target domain P𝐾with only unlabeled samples
{𝑥𝐾
𝑖}𝑛𝐾
𝑖=1, the objective function of TENON -DAis directly given by Eq.
(1). Here the class label 𝑦𝑘
𝑖(𝑘=1,···,𝐾−1) of source training
sample𝑥𝑘
𝑖is represented as a one-hot vector, and the class label 𝑦𝐾
𝑖
of unlabeled target training sample 𝑥𝐾
𝑖is initialized as a zero vector.
Following Theorem 4, we can obtain the closed-form solution of
TENON -DAas follows. Suppose 𝑓0(·;Θ)=0,Θ0=0, the predicted
class labels of target training samples are given by
y∗
𝐾=[y∗]−𝑛𝐾: where y∗=𝜆K−1
NTKΓ−1y
where[y∗]−𝑛𝐾:denotes the last 𝑛𝐾rows of predicted output values
y∗. Moreover, for any new target testing sample 𝑥test
𝐾, the predicted
class label via TENON-DA is
𝑦test
𝐾=𝜆KNTK(𝑥test
𝐾,X)K−1
NTKΓ−1y=KKK(𝑥test
𝐾,X𝐾)y∗
𝐾(4)
where KKK(𝑥test
𝐾,X𝐾)=[KNTK(𝑥test
𝐾,𝑥1
𝐾),···,KNTK(𝑥test
𝐾,𝑥𝐾𝑛𝐾)].
We see that TENON -DAis a non-parametric domain adaptation ap-
proach. As shown in Algorithm 1, we can pre-compute the prop-
agated labels y∗
𝐾for unlabeled target training samples. Then, the
class label of any testing target sample is inferred using the propa-
gated labels y∗
𝐾and the neural tangent kernel vector KKK(𝑥test
𝐾,X𝐾)
between this testing sample and the target training samples.
In the following, we theoretically analyze the generalization
bound of TENON-DA for domain adaptation.
Theorem 7 (Generalization of TENON -DA).Suppose that the
learning models are instantiated with infinitely wide neural networks,
given the hypothesis space H, for any hypothesis 𝑓(·;𝜃𝑘)∈H and
any𝛿∈(0,1), with probability at least 1−𝛿, the expected error of
the target domain can be upper bounded by
E𝑥∼P𝐾h𝑓(𝑥;𝜃𝐾)−𝑓 𝑥;𝜃∗
𝐾2
2i
≤𝜁"
𝜆𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖=1𝑓(𝑥𝑘
𝑖;𝜃𝑘)−𝑦𝑘
𝑖2
2
+1
2𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖,𝑗=1𝑠𝑘
𝑖𝑗𝑓(𝑥𝑘
𝑖;𝜃𝑘)
√︃
𝐷𝑘
𝑖𝑖−𝑓(𝑥𝑘
𝑗;𝜃𝑘)
√︃
𝐷𝑘
𝑗𝑗2
2
+1
2𝐾∑︁
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√𝑀𝑘𝑘−𝜃𝑘′√𝑀𝑘′𝑘′2
2#
+1
𝑛𝐾𝐿R𝐾∑︁
𝑘=1Ω(X𝑘,𝜃∗
𝑘)+1
𝑛𝐾𝐿RΔ(𝜃∗
1,···,𝜃∗
𝐾)+Olog(1/𝛿)
𝑛𝐾
where Ω(X𝑘,𝜃∗
𝑘)=Í𝑛𝑘
𝑖,𝑗=1𝑠𝑘
𝑖𝑗𝑓(𝑥𝑘
𝑖;𝜃∗
𝑘)√︃
𝐷𝑘
𝑖𝑖−𝑓(𝑥𝑘
𝑗;𝜃∗
𝑘)√︃
𝐷𝑘
𝑗𝑗2
2denotes the la-
bel smoothness over 𝜃∗
𝑘=arg min𝜃′EP𝑘[𝑓(𝑥𝑘;𝜃′),𝑦𝑘],Δ(𝜃∗
1,···,𝜃∗
𝐾)=
Í𝐾
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃∗
𝑘√𝑀𝑘𝑘−𝜃∗
𝑘′√𝑀𝑘′𝑘′2
2and𝜁=maxn𝑈R
𝜆𝑛𝐾𝐿R,2
𝑛𝐾𝐿Ro
.𝐿R
and𝑈Rare constants depending on the maximum and minimum
eigenvalues of L𝐾+K−1
𝐾𝐾respectively, where L𝑘is the symmetrically
normalized Laplacian matrix of the target domain.Algorithm 2 TENON-OOD
Input:𝐾source (training) domains {P𝑘}𝐾
𝑘=1;
Output: Predicted output values of target samples.
1:——————— Training Stage (Pre-computing) ———————
2:Calculate ΓKNTK (same procedures as Lines 2-10 in Alg. 1);
3:Calculate y∗=𝜆K−1
NTKΓ−1y=𝜆(ΓKNTK)−1y;
4:——————— Inference Stage ————————————————
5:for𝑥test
𝐾+1from target (testing) domain 𝐾+1do
6: Calculate neural tangent kernel Φ(𝑥test
𝐾+1,X);
7: Calculate𝑦test
𝐾+1=𝜆
𝐾Φ(𝑥test
𝐾+1,X)K−1
NTKΓ−1y;
8:end for
It can be seen from Theorem 7 that the generalization error of
TENON -DAon the target domain is determined by the following cru-
cial factors. One is the empirical prediction error given by TENON -DA
(see Eq. (1)) over source and target training samples. The other one
is the optimal label smoothness within each domain and the opti-
mal parameter smoothness across domains. We would like to point
out that previous works study the generalization performance of
domain adaptation using either domain discrepancy [ 1,55,57] or
label smoothness [ 35] across domains, by assuming that all do-
mains share the same hypothesis. The learned prediction function
in those works might lose domain-specific information, resulting
in sub-optimal performance on the target domain. Though some
recent works [ 40,51] propose to learn both domain-invariant and
domain-specific representations, their theoretical generalization
performance is unclear. Instead, in this paper, we leverage the sim-
ple distributional network of networks framework to model data
heterogeneity in domain adaptation with theoretical guarantees
(e.g., the first three terms of the upper bound in Theorem 7 result
in the optimization framework of Eq. (1) for domain adaptation).
5.2 Inductive Distribution Learning
We can formulate the out-of-distribution generalization [ 16,23] as
an inductive distribution learning problem. As illustrated in Fig-
ure 1(b), all source (training) domains can be used to construct
a network of networks. Since the target (testing) domains are
only available during the testing phase, they will be added to the
main network as new nodes after model training. Therefore, out-
of-distribution generalization can be considered as an inductive
distributional learning problem, given the formulated network of
networks. To solve this problem, we instantiate the proposed TENON
framework (denoted as TENON -OOD) with the following training and
inference stages (see Algorithm 2).
•Training Stage: Given 𝐾source (training) domains {P𝑘}𝐾
𝑘=1
each with labeled samples {𝑥𝑘
𝑖,𝑦𝑘
𝑖}𝑛𝑘
𝑖=1, the objective function
ofTENON -OODduring training can be directly given by Eq.
(1). Thus, based on Theorem 4, we can obtain the closed-
form solution for model parameters {𝜃𝑘}𝐾
𝑘=1over training
domains.
Θ∗=𝜆∇Θ𝑓0(X)𝑇K−1
NTKΓ−1y
where Θ∗=[𝜃∗𝑇
1,···,𝜃∗𝑇
𝐾]𝑇. Here𝜃∗
𝑘denotes the optimized
model parameters within domain 𝑘.
3384Distributional Network of Networks for Modeling
Data Heterogeneity KDD ’24, August 25–29, 2024, Barcelona, Spain
•Inference Stage: In the inference stage, we can learn the
model parameters 𝜃∗
𝐾+1for a new target (testing) domain
P𝐾+1as follows. For standard out-of-distribution general-
ization, no prior knowledge regarding the target (testing)
domain is available before model inference. In this case,
we assume that the new target (testing) domain can be
considered as a new (domain) node for the previously de-
rived network of networks. The edge weight6between
this new node and previous nodes within the main net-
work is simply set as 1. Considering the objective func-
tionmin𝜃𝐾+1Í𝐾+1
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√𝑀𝑘𝑘−𝜃𝑘′√𝑀𝑘′𝑘′2
𝐹, we obtain the
closed-form solution 𝜃∗
𝐾+1=1
𝐾Í𝐾
𝑘=1𝜃∗
𝑘, and thus the pre-
dicted class label of any testing sample 𝑥test
𝐾+1is given by
𝑦test
𝐾+1=𝜆
𝐾Φ(𝑥test
𝐾+1,X)K−1
NTKΓ−1y
where Φ(𝑥test
𝐾+1,X)=[KNTK(𝑥test
𝐾+1,𝑥1
1),···,KNTK(𝑥test
𝐾+1,𝑥1𝑛1),
···,KNTK(𝑥test
𝐾+1,𝑥1
𝐾),···,KNTK(𝑥test
𝐾+1,𝑥𝐾𝑛𝐾)]denotes the neu-
ral tangent kernel between 𝑥test
𝐾+1and samples from training
domains.
5.3 More Discussion Regarding Algorithms 1&2
It can be seen that the term Γin Algorithms 1&2 involves the
computationally expensive gradient terms ∇Θ𝑓0(𝑋).
Γ=ˆA+𝜆I+K−1
NTK∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇K−1
NTK
However, we have the following observations.
∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇
=∇Θ𝑓0(𝑋) I𝐾𝑑𝑖𝑛×𝐾𝑑𝑖𝑛−B⊗I𝑑𝑖𝑛×𝑑𝑖𝑛∇Θ𝑓0(𝑋)𝑇
=KNTK−𝑑11√𝑀11·𝑀11K11𝑑12√𝑀11·𝑀22K12···𝑑1𝐾√𝑀11·𝑀𝐾𝐾K1𝐾
𝑑21√𝑀22·𝑀11K21𝑑22√𝑀22·𝑀22K22···𝑑2𝐾√𝑀22·𝑀𝐾𝐾K2𝐾
............
𝑑𝐾1√𝑀𝐾𝐾·𝑀11K𝐾1𝑑𝐾2√𝑀𝐾𝐾·𝑀22K𝐾2···𝑑𝐾𝐾√𝑀𝐾𝐾·𝑀𝐾𝐾K𝐾𝐾
It shows that the term Γin Algorithms 1&2 can be efficiently calcu-
lated using the domain similarity 𝑑𝑘𝑘′and neural tangent kernel
K𝑘𝑘′between domain 𝑘and domain 𝑘′.
5.4 Computational Complexity
Algorithms 1&2 show that the time complexity of TENON -DAand
TENON -OODis determined by the calculation of neural tangent kernel
(NTK) of any pair of training samples and the inversion of the
propagation matrix ΓKNTK. The time complexity of calculating
NTK over all domains is O(¯𝑛2)[37], where ¯𝑛=Í𝐾
𝑘=1𝑛𝑘denotes
the number of all training samples. The inversion of ΓKNTKrequires
O(¯𝑛3). Following [ 19], we can use the conjugate gradient method
to solve the linear system (ΓKNTK)y∗=𝜆y, in order to estimate the
propagated labels y∗. This allows us to reduce the time complexity
fromO(¯𝑛3)toO(𝑏¯𝑛2), where𝑏is the number of iterations. In this
6Without prior knowledge regarding the unseen target domain, we assume that the
unseen target domain is equally similar to all source domains. In this case, only the
parameter smoothness regularization (i.e., the third term in Eq. (1)) will be available to
optimize the model parameters of this unseen target domain.case, we term the variants of TENON-DA andTENON-OOD algorithms
with conjugate gradient as TENON -DA-Fast andTENON -OOD-Fast ,
respectively (see subsection 6.3.3 for more empirical analysis).
6 Experiments
In the experiment, we evaluate the proposed TENON algorithms on
domain adaptation and out-of-distribution generalization data sets.
6.1 Experimental Setup
6.1.1 Data Sets. We use the following data sets.
•Amazon Review [ 8]: It contains positive and negative prod-
uct reviews from four different domains: Books, DVD, Elec-
tronics, and Kitchen. Following [ 47,57], we use top-5000
frequent unigrams/bigrams to extract the bag-of-words fea-
tures for Amazon reviews. Each review is associated with a
binary label indicating positive or negative sentiment.
•CityCam [ 54]: CityCam is a large-scale web camera data
set. It contains images captured by several cameras in dif-
ferent city locations. Following [ 11], we use images from
four cameras (with IDs: 253, 495, 511, and 572). Each image
has a 2048-dimensional feature vector extracted from the
pre-trained ResNet-50 [ 18]. Specifically, in this paper, we
consider a binary classification task based on the number of
vehicles within the camera images, i.e., whether there are at
least 10 cars in an image.
•Huffpost [ 31]: Huffpost contains article headlines associated
with 11 news categories collected from the Huffington Post
from 2012 to 2018. Following [ 53], we use pre-trained Dis-
tilBERT [ 43] to extract a 768-dimensional feature vector for
each new headline. The task is to identify the news tags of
article headlines as one of the following 11 categories: Black
Voices, Business, Comedy, Crime, Entertainment, Impact,
Queer Voices, Science, Sports, Tech, Travel.
•ArXiv [ 10]: ArXiv provides metadata of arXiv preprints from
2007 to 2023. As illustrated in [ 53], each preprint consists of
a paper title and its corresponding primary categories. The
paper title can further be represented as a 768-dimensional
feature vector using pre-trained DistilBERT [ 43]. The task of
ArXiv is to predict the primary category of arXiv pre-prints
from their paper titles.
•CivilComments [ 22]: CivilComments consists of comments
scraped from the internet. It contains 8 demographic identi-
ties: male, female, LGBTQ, Christian, Muslim, other religions,
Black, or White. Each identity is considered as a single do-
main. CivilComments involves a binary classification task
to determine whether a comment is toxic.
6.1.2 Baselines. In the experiment, we consider the following do-
main adaptation baselines, including (1) semi-supervised learn-
ing: LabelProp [ 12,58], and (2) domain adaptation: DANN [ 14],
MDAN [ 57], M3SAD [ 39], DARN [ 47], and GRDA [ 52]. In addition,
we use the following out-of-distribution generalization baselines:
ERM, DANN [14], IRM [2], SD [41], Fish [45], and EQRM [13].
6.1.3 Configuration. Following [47], we use a 3-layer multi-layer
perceptron (MLP) to instantiate the prediction function for all
baselines. Then we implement our proposed algorithms using the
3385KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
ModelAmazon Review CityCam
Books DVD Electronics Kitchen 253 495 511 572
LabelProp [58] 0.7078±0.0040 0.7294±0.0093 0.7699±0.0079 0.7799±0.0082 0.6741±0.1360 0.6312±0.0470 0.6753±0.0888 0.7560±0.0564
DANN [14] 0.6958±0.0157 0.7229±0.0031 0.7818±0.0053 0.7879±0.0072 0.7804±0.0415 0.6716±0.0499 0.8498±0.0136 0.7563±0.0344
MDAN [57] 0.7196±0.0095 0.7432±0.0205 0.7744±0.0121 0.7869±0.0156 0.8007±0.0579 0.6685±0.0339 0.8222±0.0227 0.7280±0.0261
M3SAD [39] 0.7019±0.0232 0.7251±0.0210 0.7753±0.0117 0.7893±0.0134 0.8064±0.0581 0.6175±0.0286 0.7970±0.0357 0.7621±0.0531
DARN [47] 0.7175±0.0126 0.7412±0.0180 0.7703±0.0119 0.7888±0.0145 0.8243±0.0392 0.6795±0.0321 0.8271±0.0161 0.7547±0.0385
GRDA [52] 0.7110±0.0099 0.7294±0.0110 0.7714±0.0091 0.7884±0.0056 0.7949±0.0751 0.6698±0.0340 0.8254±0.0196 0.7495±0.0389
TENON-DA-Fast 0.7241±0.0146 0.7499±0.0101 0.7713±0.0093 0.7898±0.0101 0.8359±0.0146 0.7145±0.0122 0.7918±0.0104 0.7857±0.0176
TENON-DA 0.7238±0.0135 0.7503±0.0094 0.7763±0.0065 0.7851±0.0037 0.8350±0.0156 0.7143±0.0134 0.7932±0.0097 0.7859±0.0182
Table 1: Domain adaptation on Amazon review and CityCam data sets
Model 2013 2014 2015 2016 2017 2018 Avg.
LabelProp [58] 0.5312±0.0109 0.2942±0.0095 0.2641±0.0202 0.3535±0.0237 0.3900±0.0145 0.5055±0.0165 0.3897
DANN [14] 0.4171±0.0293 0.3510±0.0234 0.3847±0.0423 0.4468±0.0205 0.4568±0.0352 0.5490±0.0254 0.4342
MDAN [57] 0.4171±0.0293 0.3365±0.0481 0.3757±0.0546 0.4424±0.0207 0.4392±0.0267 0.5019±0.0322 0.4188
M3SAD [39] 0.4077±0.0290 0.3490±0.0439 0.4055±0.0380 0.4468±0.0246 0.4455±0.0281 0.4879±0.0398 0.4237
DARN [47] 0.4171±0.0293 0.3944±0.0140 0.3902±0.0375 0.4553±0.0205 0.4951±0.0227 0.5601±0.0106 0.4520
GRDA [52] 0.4324±0.0330 0.3671±0.0234 0.3539±0.0255 0.4534±0.0150 0.4520±0.0153 0.4987±0.0137 0.4262
TENON-DA-Fast 0.5850±0.0110 0.5028±0.0183 0.4573±0.0275 0.4995±0.0129 0.4556±0.0295 0.5201±0.0080 0.5033
TENON-DA 0.5851±0.0110 0.5028±0.0183 0.4575±0.0273 0.4987±0.0131 0.4651±0.0132 0.5198±0.0080 0.5048
Table 2: Domain adaptation on the Hoffpost data set ("Avg." indicates the average accuracy over all target domains)
Model 2009 2011 2013 2015 2017 2019 2021 Avg.
LabelProp [58] 0.7006±0.0267 0.6938±0.0060 0.7186±0.0063 0.6780±0.0056 0.6717±0.0130 0.6857±0.0085 0.6741±0.0099 0.6889
DANN [14] 0.7483±0.0122 0.6943±0.0387 0.7187±0.0304 0.7283±0.0092 0.7183±0.0081 0.7293±0.0256 0.7213±0.0350 0.7226
MDAN [57] 0.7483±0.0122 0.7161±0.0075 0.6251±0.0819 0.6593±0.0535 0.6748±0.0254 0.6944±0.0410 0.7139±0.0223 0.6903
M3SAD [39] 0.5533±0.0099 0.6026±0.1134 0.6845±0.0637 0.7151±0.0200 0.6980±0.0176 0.7384±0.0117 0.7473±0.0174 0.6770
DARN [47] 0.7483±0.0122 0.7228±0.0110 0.7123±0.0302 0.7177±0.0196 0.7126±0.0151 0.7456±0.0054 0.7471±0.0140 0.7295
GRDA [52] 0.7414±0.0219 0.7186±0.0041 0.6662±0.0482 0.6956±0.0385 0.6777±0.0215 0.7035±0.0286 0.7366±0.0103 0.7056
TENON-DA-Fast 0.7619±0.0098 0.7161±0.0064 0.7415±0.0071 0.7155±0.0070 0.7216±0.0034 0.7336±0.0076 0.7297±0.0124 0.7314
TENON-DA 0.7621±0.0100 0.7157±0.0063 0.7420±0.0067 0.7195±0.0053 0.7241±0.0014 0.7403±0.0078 0.7477±0.0096 0.7359
Table 3: Domain adaptation on the ArXiv data set ("Avg." indicates the average accuracy over all target domains)
Model Male Female LGBTQ Christian Muslim Others Black White Avg
ERM 0.6859±0.0091 0.6428±0.0186 0.6796±0.0226 0.7058±0.0248 0.7396±0.0258 0.7024±0.0074 0.7118±0.0169 0.6796±0.0137 0.6934
DANN [14] 0.6850±0.0096 0.6453±0.0249 0.6794±0.0202 0.7160±0.0156 0.7340±0.0207 0.6984±0.0058 0.6872±0.0280 0.6776±0.0145 0.6904
IRM [2] 0.6848±0.0087 0.6432±0.0199 0.6862±0.0186 0.7071±0.0240 0.7416±0.0195 0.7028±0.0083 0.7121±0.0192 0.6837±0.0121 0.6951
SD [41] 0.6777±0.0119 0.6449±0.0189 0.6847±0.0216 0.7068±0.0250 0.7464±0.0152 0.7031±0.0088 0.7092±0.0192 0.6784±0.0105 0.6939
Fish [45] 0.6882±0.0055 0.6650±0.0059 0.6793±0.0079 0.7363±0.0195 0.7512±0.0117 0.6944±0.0125 0.7219±0.0071 0.6912±0.0105 0.7034
EQRM [13] 0.6882±0.0094 0.6603±0.0034 0.6830±0.0176 0.7237±0.0269 0.7517±0.0063 0.6969±0.0083 0.7269±0.0067 0.6899±0.0096 0.7025
TENON-OOD-Fast 0.6922±0.0062 0.6678±0.0045 0.6968±0.0074 0.7236±0.0059 0.7502±0.0062 0.7006±0.0110 0.7037±0.0096 0.6813±0.0077 0.7020
TENON-OOD 0.6909±0.0082 0.6702±0.0030 0.7044±0.0063 0.7217±0.0086 0.7620±0.0066 0.7022±0.0089 0.7294±0.0055 0.6909±0.0048 0.7089
Table 4: Out-of-distribution generalization on CivilComments ("Avg." indicates the average accuracy over all testing domains)
NTK [ 26] induced by a 3-layer MLP with infinite width. The classifi-
cation accuracy is used as the evaluation metric in the experiments.
In addition, we set 𝜎=2,𝜆=1in our experiments.6.2 Main Results
In the following, we discuss the evaluation results of TENON algo-
rithms for domain adaptation and out-of-distribution generaliza-
tion.
3386Distributional Network of Networks for Modeling
Data Heterogeneity KDD ’24, August 25–29, 2024, Barcelona, Spain
6.2.1 Domain Adaptation. Tables 1-3 provide the evaluation com-
parison between TENON-DA and baselines on various data sets (the
best results are indicated in bold). All the experiments are repeated
five times and then we report the mean and standard deviation
of classification accuracies. For each run, we randomly select 200
samples from each domain as the training samples and others as
the testing samples. Specifically, for Amazon Review and CityCam
data sets, following [ 47], we take one domain (e.g., "Books") as the
target domain, and others domains (e.g., "DVD", "Electronics" and
"Kitchen") as source domains. In contrast, Hoffpost and ArXiv data
sets [ 53] contain evolving domains where the data distribution is
changing over time. In this case, we take one specific time stamp as
the target domain and all historical time stamps as source domains.
We have the following observations from Tables 1-3. (1) Label-
Prop considers propagating the label information within a single
graph. It does not capture the data heterogeneity among different
domains, thus leading to sub-optimal performance in domain adap-
tation. (2) Compared to domain adaptation baselines, our proposed
non-parametric TENON-DA algorithm can achieve superior perfor-
mance in most cases. This observation verifies the effectiveness
ofTENON-DA in handling heterogeneous data across domains. (3)
TENON-DA-Fast achieves comparable performance with TENON-DA .
Furthermore, Figure 3(b) shows that TENON-DA-Fast significantly
reduces the running time compared to TENON-DA.
6.2.2 Out-of-Distribution Generalization. Table 4 shows the results
ofTENON-OOD on the CivilComments data set (the best results are in-
dicated in bold). In this case, we take one domain (e.g., "Male") as the
unseen testing target domain and others (e.g., "Female", "LGBTQ",
"Christian", "Muslim", "Others", "Black", and "White") as source train-
ing domains. It is observed that TENON-OOD outperforms baselines
for out-of-distribution generalization.
6.3 Analysis
6.3.1 Ablation Study. Here we study the impact of within-domain
label smoothness regularization on the proposed TENON -DA/TENON -
OODalgorithms. Table 5 reports the average accuracy of TENON -DA
andTENON -OODon ArXiv and CivilComments respectively. It indi-
cates that the label smoothness regularization improves the model
performance. Besides, Figure 3 compares the TENON -DA/TENON -OOD
algorithms with their approximation introduced in Subsection 5.4.
It can be seen that with only 10 iterations, TENON -DA-Fast /TENON -
OOD-Fast based on conjugate gradient can efficiently achieve simi-
lar performance with their counterparts.
6.3.2 Hyperparameter Sensitivity. We investigate the impact of
hyperparameter 𝜆on the proposed TENON -DAandTENON -OODalgo-
rithms. Figure 4 reports the results of TENON -DAandTENON -OODon
ArXiv and CivilComments respectively. It is observed that both
algorithms are robust to the selection of 𝜆.
6.3.3 Efficiency. Figure 5 shows the efficiency comparison between
TENON-DA algorithm and baselines, where the overall training run-
ning time is reported. It is observed that the proposed TENON-DA
algorithm is more computationally efficient than domain adaptation
baselines involving gradient descent training. Due to the efficient
approximation of matrix inversion, TENON-DA-Fast takes less time
than TENON-DA on Amazon Review and ArXiv data sets.
1234567891011121314151617181920
Iterations of conjugate gradient0.550.600.650.70Accuracy
 TENON-OOD
TENON-DA
TENON-OOD-Fast
TENON-DA-Fast(a) Accuracy
1234567891011121314151617181920
Iterations of conjugate gradient0.51.01.52.02.5Running time (s)
 (b) Running time
Figure 3: Analysis of conjugate gradient
0.20.40.60.81.01.21.41.61.82.0
λ0.550.600.650.70Accuracy
TENON-OOD
TENON-DA
Figure 4: Impact of 𝜆
Amazon Review ArXiv020406080100120Running time (s)DANN
MDAN
M3SDADARN
TENON-DA
TENON-DA-Fast Figure 5: Efficiency analysis
Data TENON-DA TENON-DA w/o label smoothness
Amazon Review 0.7589 0.7573
CityCam 0.7821 0.7642
Hoffpost 0.5048 0.4987
ArXiv 0.7359 0.7297
Data TENON-OOD TENON-OOD w/o label smoothness
CivilComments 0.7089 0.7054
Table 5: Ablation study
7 Conclusion
In this paper, we propose a generic distributional network of net-
works ( TENON ) framework for modeling data heterogeneity, us-
ing within-domain label smoothness and cross-domain parameter
smoothness. Then we provide two instantiated algorithms of TENON
for domain adaptation and out-of-distribution generalization. The
effectiveness and efficiency of our proposed algorithms are verified
theoretically and empirically.
Acknowledgments
This work is supported by National Science Foundation (2117902
and 2134079), DARPA (HR001121C0165), and Agriculture and Food
Research Initiative (AFRI) grant no. 2020-67021-32799/project ac-
cession no.1024178 from the USDA National Institute of Food and
Agriculture. The views and conclusions are those of the authors
and should not be interpreted as representing the official policies
of the funding agencies or the government.
3387KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
References
[1]David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. 2021. 𝑓-Domain
Adversarial Learning: Theory and Algorithms. In ICML. 66–75.
[2]Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[3]Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. 2018. MetaReg:
Towards domain generalization using meta-regularization. NeurIPS 31 (2018).
[4]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan. 2010. A theory of learning from different domains.
Machine Learning 79 (2010), 151–175.
[5]David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alexey
Kurakin. 2022. AdaMatch: A unified approach to semi-supervised learning and
domain adaptation. In ICLR.
[6]Gilles Blanchard, Gyemin Lee, and Clayton Scott. 2011. Generalizing from several
related classification tasks to a new unlabeled sample. NeurIPS 24 (2011).
[7]Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. 2021. Exploiting domain-
specific features to enhance domain generalization. NeurIPS 34 (2021), 21189–
21201.
[8]Minmin Chen, Zhixiang Xu, Kilian Q Weinberger, and Fei Sha. 2012. Marginalized
denoising autoencoders for domain adaptation. In ICML. 1627–1634.
[9]Qi Chen and Mario Marchand. 2023. Algorithm-Dependent Bounds for Represen-
tation Learning of Multi-Source Domain Adaptation. In AISTATS. 10368–10394.
[10] Colin B Clement, Matthew Bierbaum, Kevin P O’Keeffe, and Alexander A Alemi.
2019. On the use of arxiv as a dataset. arXiv preprint arXiv:1905.00075 (2019).
[11] Antoine de Mathelin, Guillaume Richard, François Deheeger, Mathilde Mougeot,
and Nicolas Vayatis. 2021. Adversarial weighting for domain adaptation in
regression. In ICTAI. IEEE, 49–56.
[12] Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. 2005. Efficient non-
parametric function induction in semi-supervised learning. In AISTATS. 96–103.
[13] Cian Eastwood, Alexander Robey, Shashank Singh, Julius Von Kügelgen, Hamed
Hassani, George J Pappas, and Bernhard Schölkopf. 2022. Probable domain
generalization via quantile risk minimization. NeurIPS 35 (2022), 17340–17358.
[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. Journal of Machine Learning
Research (2016).
[15] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and
Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning
Research 13, 1 (2012), 723–773.
[16] Ishaan Gulrajani and David Lopez-Paz. 2021. In search of lost domain generaliza-
tion. In ICLR.
[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. NeurIPS 30 (2017).
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR. 770–778.
[19] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. 2019. Label
propagation for deep semi-supervised learning. In CVPR. 5070–5079.
[20] Arthur Jacot, Franck Gabriel, and Clément Hongler. 2018. Neural tangent kernel:
Convergence and generalization in neural networks. NeurIPS 31 (2018).
[21] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio
Torralba. 2012. Undoing the damage of dataset bias. In ECCV. 158–171.
[22] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin
Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton
Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma
Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. 2021. Wilds: A benchmark
of in-the-wild distribution shifts. In ICML. 5637–5664.
[23] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In ICML.
[24] Ananya Kumar, Tengyu Ma, and Percy Liang. 2020. Understanding self-training
for gradual domain adaptation. In ICML. 5468–5479.
[25] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak,
Jascha Sohl-Dickstein, and Jeffrey Pennington. 2019. Wide neural networks of
any depth evolve as linear models under gradient descent. NeurIPS 32 (2019).
[26] Ronaldas Paulius Lencevicius. 2022. An Empirical Analysis of the Laplace and
Neural Tangent Kernels. Master’s thesis. California State Polytechnic University,
Pomona.
[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning to
generalize: Meta-learning for domain generalization. In AAAI, Vol. 32.
[28] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. 2018. Domain gener-
alization with adversarial feature learning. In CVPR. 5400–5409.
[29] Liangyue Li and Hanghang Tong. 2015. The child is father of the man: Foresee
the success at the early stage. In KDD. 655–664.[30] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning
transferable features with deep adaptation networks. In ICML. 97–105.
[31] Rishabh Misra. 2022. News Category Dataset. arXiv preprint arXiv:2209.11429
(2022).
[32] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations
of machine learning. MIT press.
[33] Eduardo Fernandes Montesuma and Fred Maurice Ngolè Mboula. 2021. Wasser-
stein Barycenter for Multi-Source Domain Adaptation. In CVPR. 16785–16793.
[34] Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. 2013. Domain
generalization via invariant feature representation. In ICML. 10–18.
[35] Debarghya Mukherjee, Felix Petersen, Mikhail Yurochkin, and Yuekai Sun. 2022.
Domain Adaptation meets Individual Fairness. And they get along. NeurIPS 35
(2022), 28902–28913.
[36] Jingchao Ni, Hanghang Tong, Wei Fan, and Xiang Zhang. 2014. Inside the atoms:
ranking on a network of networks. In KDD. 1356–1365.
[37] Roman Novak, Jascha Sohl-Dickstein, and Samuel S Schoenholz. 2022. Fast finite
width neural tangent kernel. In ICML. 17018–17044.
[38] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE
Transactions on Knowledge and Data Engineering (2010).
[39] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.
2019. Moment matching for multi-source domain adaptation. In ICCV. 1406–1415.
[40] Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. 2019. Domain
agnostic learning with disentangled representations. In ICML. 5102–5112.
[41] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina
Precup, and Guillaume Lajoie. 2021. Gradient starvation: A learning proclivity in
neural networks. NeurIPS 34 (2021), 1256–1272.
[42] Alexandre Rame, Corentin Dancette, and Matthieu Cord. 2022. Fishr: Invariant
gradient variances for out-of-distribution generalization. In ICML. 18347–18377.
[43] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 (2019).
[44] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. 2018. Wasserstein distance
guided representation learning for domain adaptation. In AAAI, Vol. 32.
[45] Yuge Shi, Jeffrey Seely, Philip Torr, Siddharth N, Awni Hannun, Nicolas Usunier,
and Gabriel Synnaeve. 2022. Gradient matching for domain generalization. In
ICLR.
[46] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,
and Silvio Savarese. 2018. Generalizing to unseen domains via adversarial data
augmentation. NeurIPS 31 (2018).
[47] Junfeng Wen, Russell Greiner, and Dale Schuurmans. 2020. Domain aggregation
networks for multi-source domain adaptation. In ICML. 10214–10224.
[48] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena,
Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. 2022. A fine-grained
analysis on distribution shift. In ICLR.
[49] Jun Wu, Wenxuan Bao, Elizabeth Ainsworth, and Jingrui He. 2023. Personalized
federated learning with parameter propagation. In KDD. 2594–2605.
[50] Jun Wu, Jingrui He, Sheng Wang, Kaiyu Guan, and Elizabeth Ainsworth.
2022. Distribution-informed neural networks for domain adaptation regression.
NeurIPS 35 (2022), 10040–10054.
[51] Tongkun Xu, Weihua Chen, Pichao WANG, Fan Wang, Hao Li, and Rong Jin. 2022.
CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. In
ICLR.
[52] Zihao Xu, Hao He, Guang-He Lee, Bernie Wang, and Hao Wang. 2022. Graph-
relational domain adaptation. In ICLR.
[53] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and
Chelsea Finn. 2022. Wild-time: A benchmark of in-the-wild distribution shift
over time. NeurIPS 35 (2022), 10309–10324.
[54] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jose MF Moura. 2017.
Understanding traffic density from large-scale web camera data. In CVPR. 5898–
5907.
[55] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. 2019. Bridging
theory and algorithm for domain adaptation. In ICML. 7404–7413.
[56] Yu Zhang and Dit-Yan Yeung. 2010. A convex formulation for learning task
relationships in multi-task learning. In UAI. 733–742.
[57] Han Zhao, Shanghang Zhang, Guanhang Wu, José MF Moura, Joao P Costeira,
and Geoffrey J Gordon. 2018. Adversarial multiple source domain adaptation.
NeurIPS 31 (2018), 8568–8579.
[58] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard
Schölkopf. 2003. Learning with local and global consistency. NeurIPS 16 (2003).
[59] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. 2021. Domain Generaliza-
tion with MixStyle. In ICLR.
[60] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervised
learning using Gaussian fields and harmonic functions. In ICML. 912–919.
3388Distributional Network of Networks for Modeling
Data Heterogeneity KDD ’24, August 25–29, 2024, Barcelona, Spain
A Appendix
In the appendix, we provide the proof of theoretical results pre-
sented in the paper.
A.1 Proof of Lemma 3
Proof. The objective of Eq. (1) can be rewritten as follows.
J(Θ)=Θ𝑇ˆXˆAˆX𝑇Θ+Θ𝑇ˆBΘ+𝜆·ˆX𝑇Θ−y2
2
Then the derivative of J(Θ)is given by
𝜕J(Θ)
𝜕Θ=𝜕
Θ𝑇ˆXˆAˆX𝑇Θ+Θ𝑇ˆBΘ+𝜆·ˆX𝑇Θ−y2
2
𝜕Θ
=2
ˆXˆAˆX𝑇+ˆB
Θ+2𝜆ˆXˆX𝑇Θ−2𝜆ˆXy
=2
ˆXˆAˆX𝑇+ˆB+𝜆ˆXˆX𝑇
Θ−2𝜆ˆXy
By setting𝜕J(Θ)
𝜕Θ=0, the minimizer of J(Θ)is obtained at
Θ∗=𝜆
ˆXˆAˆX𝑇+ˆB+𝜆ˆXˆX𝑇−1ˆXy
=𝜆ˆX
ˆAˆX𝑇ˆX+
ˆX𝑇ˆX−1
ˆX𝑇ˆBˆX
+𝜆ˆX𝑇ˆX−1
y
which completes the proof. □
A.2 Proof of Theorem 4
Proof. Following [ 25], we consider the following linearized
neural network
𝑓lin
𝑡(𝑥)=𝑓0(𝑥)+ ∇𝜃0𝑓0(𝑥)𝑇(𝑤𝑡)
where𝑤𝑡=𝜃𝑡−𝜃0is the parameter change from the initial values.
LetW𝑡=Θ𝑡−Θ0be the change of parameters from the initial val-
ues and𝑓𝑡(𝑋)=vec(𝑓𝑡(𝑥𝑘
𝑖;𝜃𝑘)|𝑖∈{1,2,···,𝑛𝑘},𝑘∈{1,2,···,𝐾})
be the vectorized predicted values over all input samples. Based on
continuous time gradient descent [ 25], the evolution of the param-
eters can be expressed as
¤W𝑡=−𝜂
2∇ΘJ(Θ)=−𝜂
2∇Θ𝑓0(𝑋)𝑇
ˆAf𝑡+𝜆(f𝑡−y)
−𝜂ˆBΘ𝑡
=−𝜂∇Θ𝑓0(𝑋)𝑇 
ˆA∇Θ𝑓0(𝑋)+𝜆∇Θ𝑓0(𝑋)
W𝑡
+
∇Θ𝑓0(𝑋)∇Θ𝑓0(𝑋)𝑇−1
∇Θ𝑓0(𝑋)ˆBW𝑡
+
∇Θ𝑓0(𝑋)∇Θ𝑓0(𝑋)𝑇−1
∇Θ𝑓0(𝑋)ˆBΘ0+ˆA𝑓0(𝑋)+𝜆𝑓0(𝑋)−𝜆y!
In this case, the ODE has a closed-form solution below.
Θ𝑡
=−∇Θ𝑓0(𝑋)𝑇
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇−1
·
I−expn
−𝜂
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇
𝑡o
·
K−1
NTK∇Θ𝑓0(𝑋)ˆBΘ0+ˆA𝑓0(𝑋)+𝜆𝑓0(𝑋)−𝜆y
+Θ0
Thus,
lim𝑡→∞Θ𝑡=−∇Θ𝑓0(X)𝑇K−1
NTKΓ−1(Ω−𝜆y)+Θ0For an arbitrary point 𝑥𝑘, the predicted value is given by
𝑓lin
𝑡(𝑥𝑘;𝜃𝑘)
=𝜆·KNTK(𝑥𝑘,X)
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(X)ˆB∇Θ𝑓0(X)𝑇−1
·
I−expn
−𝜂
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(X)ˆB∇Θ𝑓0(X)𝑇
𝑡o
y
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇−1
·
I−expn
−𝜂
ˆAKNTK+𝜆KNTK+K−1
NTK
∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇
𝑡o
·
K−1
NTK∇Θ𝑓0(𝑋)ˆBΘ0+ˆA𝑓0(𝑋)+𝜆𝑓0(𝑋)
Thus, it holds that
lim𝑡→∞𝑓𝑡(𝑥𝑘;𝜃𝑘)=lim𝑡→∞𝑓lin
𝑡(𝑥𝑘;𝜃𝑘)=𝜆KNTK(𝑥𝑘,X)K−1
NTKΓ−1y
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)K−1
NTKΓ−1Ω
which completes the proof. □
A.3 Proof of Corollary 5
Proof. In this case, it holds that ˆB=0𝐾𝑑𝜃×𝐾𝑑𝜃. Then for any
sample𝑥𝑘, it holds
𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)
ˆAKNTK+𝜆KNTK+K−1
NTK∇Θ𝑓0(𝑋)ˆB∇Θ𝑓0(𝑋)𝑇−1
·
K−1
NTK∇Θ𝑓0(X)ˆBΘ0+ˆA𝑓0(X)+𝜆𝑓0(X)
=𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)
ˆAKNTK+𝜆KNTK−1
ˆA𝑓0(X)+𝜆𝑓0(X)
=𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X)K−1
NTK𝑓0(X)
=𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘𝑓0(X𝑘)
Thus,
lim𝑡→∞𝑓𝑡(𝑥𝑘;𝜃𝑘)=𝜆KNTK(𝑥𝑘,X)
ˆAKNTK+𝜆KNTK−1
y
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘𝑓0(X𝑘)
=𝜆h
0,···,0,K(𝑥𝑘,X𝑘),0,···,0i 
ˆA+𝜆I
KNTK−1
y
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘𝑓0(X𝑘)
=𝜆K
𝑥𝑘,X𝑘
K−1
𝑘𝑘 𝜆I𝑛𝑘×𝑛𝑘+I𝑛𝑘×𝑛𝑘−A𝑘−1y𝑘
+𝑓0(𝑥𝑘;𝜃𝑘)−KNTK(𝑥𝑘,X𝑘)K−1
𝑘𝑘𝑓0(X𝑘)
For training samples X𝑘=[𝑥𝑘
1,···,𝑥𝑘𝑛𝑘], the following holds
lim𝑡→∞𝑓𝑡(X𝑘;𝜃𝑘)=𝜆
𝜆+1
I𝑛𝑘×𝑛𝑘−1
𝜆+1A𝑘−1
y𝑘
which completes the proof. □
A.4 Proof Corollary 6
Proof. With linearized model 𝑓(𝑥𝑘
𝑖;𝜃𝑘)=∇𝜃𝑓0(𝑥𝑘
𝑖)𝑇𝜃𝑘, the
objective of Eq. (1) can be rewritten as follows.
J(Θ)=1
2𝐾∑︁
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√︁
𝑀𝑘𝑘−𝜃𝑘′√︁
𝑀𝑘′𝑘′2
2+𝜆𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖=1𝑓(𝑥𝑘
𝑖;𝜃𝑘)−𝑦𝑘
𝑖2
2
Using gradient descent, for any 𝑘, it holds
𝜃𝑘(𝑡+1)=𝜃𝑘(𝑡)−𝜂𝜕J(Θ)
𝜕𝜃𝑘
3389KDD ’24, August 25–29, 2024, Barcelona, Spain Jun Wu, Jingrui He & Hanghang Tong
=𝜃𝑘(𝑡)−𝜂 
𝜃𝑘(𝑡)−𝐾∑︁
𝑘′=1𝑑𝑘𝑘′√︁
𝑀𝑘𝑘𝑀𝑘′𝑘′𝜃𝑘′(𝑡)
+𝜆
∇𝑓(X𝑘)𝑇∇𝑓(X𝑘)𝜃𝑘(𝑡)−∇𝑓(X𝑘)𝑇y𝑘!
=
(1−𝜂)I−𝜆∇𝑓(X𝑘)𝑇∇𝑓(X𝑘)
𝜃𝑘(𝑡)
+𝜂𝐾∑︁
𝑘′=1𝑑𝑘𝑘′√︁
𝑀𝑘𝑘𝑀𝑘′𝑘′𝜃𝑘′(𝑡)+𝜂𝜆∇𝑓(X𝑘)𝑇y𝑘
which completes the proof. □
A.5 Proof of Theorem 7
Proof. Following standard machine learning theory [ 32], given
hypothesis spaceHand the loss function is bounded by 𝐵(for any
𝑥,|𝑓(𝑥;𝜃𝑘)−𝑓(𝑥;𝜃∗
𝑘)|≤𝐵), then for any 𝑓(·;𝜃𝐾)∈H ,
E𝑥∼P𝐾
𝑓(𝑥;𝜃𝐾)−𝑓 𝑥;𝜃∗
𝐾2
≤1
𝑛𝐾𝑛𝐾∑︁
𝑖=1
𝑓
𝑥𝐾
𝑖;𝜃𝐾
−𝑓
𝑥𝐾
𝑖;𝜃∗
𝐾 2
+𝐵√︄
log|H|+ log(2/𝛿)
2𝑛𝐾
where|H|is the size of the hypothesis space and can be further
bounded by the VC dimension of hypothesis space H. It holds
1
𝑛𝐾𝑛𝐾∑︁
𝑖=1
𝑓
𝑥𝐾
𝑖;𝜃𝐾
−𝑓
𝑥𝐾
𝑖;𝜃∗
𝐾 2
≤1
𝑛𝐾𝑓(X𝐾;𝜃𝐾)−𝑔 𝑓(X𝑆;𝜃𝑆)2
+1
𝑛𝐾𝑔 𝑓(X𝑆;𝜃𝑆)−𝑔 𝑓 X𝑆;𝜃∗
𝑆 2
+1
𝑛𝐾𝑔 𝑓 X𝑆;𝜃∗
𝑆 −𝑓 X𝐾;𝜃∗
𝐾2
≤1
𝑛𝐾𝑈R
𝐿R𝑓(X𝑆;𝜃𝑆)−𝑓 X𝑆;𝜃∗
𝑆2
+1
𝑛𝐾2
𝐿RR
𝑓(X𝑆;𝜃𝑆),𝑓(X𝐾;𝜃𝐾)
+1
𝑛𝐾2
𝐿RR
𝑓 X𝑆;𝜃∗
𝑆,𝑓(X𝐾;𝜃∗
𝐾)
≤𝜁 
𝜆𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖=1𝑓(𝑥𝑘
𝑖;𝜃𝑘)−𝑦𝑘
𝑖2
2+1
2𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖,𝑗=1𝑠𝑘
𝑖𝑗𝑓(𝑥𝑘
𝑖;𝜃𝑘)
√︃
𝐷𝑘
𝑖𝑖−𝑓(𝑥𝑘
𝑗;𝜃𝑘)
√︃
𝐷𝑘
𝑗𝑗2
2
+1
2𝐾∑︁
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√𝑀𝑘𝑘−𝜃𝑘′√𝑀𝑘′𝑘′2
2!
+1
𝑛𝐾𝐿R𝐾∑︁
𝑘=1Ω(X𝑘,𝜃∗
𝑘)+1
𝑛𝐾𝐿RΔ(𝜃∗
1,···,𝜃∗
𝐾)
where Δ(𝜃∗
1,···,𝜃∗
𝐾)=Í𝐾
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃∗
𝑘√𝑀𝑘𝑘−𝜃∗
𝑘′√𝑀𝑘′𝑘′2
2and𝜁=
maxn𝑈R
𝜆𝑛𝐾𝐿R,2
𝑛𝐾𝐿Ro
.
Note that in previous steps, we let
𝑔(ˆy𝑠)=arg min
ˆy𝑡R
ˆy𝑠,ˆy𝑡
=1
2𝐾∑︁
𝑘=1𝑛𝑘∑︁
𝑖,𝑗=1𝑠𝑘
𝑖𝑗𝑓(𝑥𝑘
𝑖;𝜃𝑘)
√︃
𝐷𝑘
𝑖𝑖−𝑓(𝑥𝑘
𝑗;𝜃𝑘)
√︃
𝐷𝑘
𝑗𝑗2
2+1
2𝐾∑︁
𝑘,𝑘′=1𝑑𝑘𝑘′𝜃𝑘√︁
𝑀𝑘𝑘−𝜃𝑘′√︁
𝑀𝑘′𝑘′2
2
where ˆy𝑠=𝑓(X𝑠;𝜃𝑠). We assume thatRis strongly convex and
smooth. For any 𝜃𝑠,𝜃𝑡,𝜃′
𝑡∈R𝑑𝜃, the following holds
R
ˆy𝑠,ˆy𝑡
≥R
ˆy𝑠,ˆy′
𝑡
+D
ˆy𝑡−ˆy′
𝑡,𝜕ˆy′
𝑡R ˆy𝑠,ˆy′
𝑡E
+𝐿R
2ˆy𝑡−ˆy′
𝑡2
2
R
ˆy𝑠,ˆy𝑡
≤R
ˆy𝑠,ˆy′
𝑡
+D
ˆy𝑡−ˆy′
𝑡,𝜕ˆy′
𝑡R ˆy𝑠,ˆy′
𝑡E
+𝑈R
2ˆy𝑡−ˆy′
𝑡2
2
Then𝑔(ˆy𝑠)−𝑔 ˆy∗
𝑠2
2≤𝑈R
𝐿Rˆy𝑠−ˆy∗
𝑠=𝑈R
𝐿R𝑓(X𝑠;𝜃𝑠)−𝑓(X𝑠;𝜃∗
𝑠)
ˆy𝑡−𝑔(ˆy𝑠)2
≤2
𝐿RR(ˆy𝑠,ˆy𝑡)
Next, following [ 35], we show the strong convexity and smooth-
ness ofR.
𝜕R(ˆy𝑠,ˆy𝑡)
𝜕𝜃𝑡=𝜕ˆy𝑡
𝜃𝑡·𝜕R(ˆy𝑠,ˆy𝑡)
𝜕ˆy𝑡
𝜕R(ˆy𝑠,ˆy𝑡)
𝜕𝜃𝑡=∇𝑓(X𝑘)𝑇(I−A𝐾)∇𝑓(X𝑘)𝜃𝑡
+𝐾∑︁
𝑘′=1𝑑𝐾𝑘′√𝑀𝐾𝐾𝜃𝑡√𝑀𝐾𝐾−𝜃𝑘′√𝑀𝑘′𝑘′
=∇𝑓(X𝑘)𝑇(I−A𝐾)∇𝑓(X𝑘)𝜃𝑡+𝜃𝑡
−𝐾∑︁
𝑘′=1𝑑𝐾𝑘′√︁
𝑀𝑘′𝑘′√𝑀𝐾𝐾𝜃𝑘′
𝜕R2(ˆy𝑠,ˆy𝑡)
𝜕𝜃2
𝑡=∇𝑓(X𝑘)𝑇(I−A𝐾)∇𝑓(X𝑘)+I
=∇𝑓(X𝑘)𝑇
I−A𝐾+
∇𝑓(X𝐾)∇𝑓(X𝐾)𝑇−1
∇𝑓(X𝑘)
𝜕ˆy𝑡
𝜃𝑡=∇𝑓(X𝑘)𝑇∈R𝑑𝜃×𝑛𝐾
𝜕
𝜕𝜃𝑡𝜕ˆy𝑡
𝜃𝑡·𝜕R(ˆy𝑠,ˆy𝑡)
𝜕ˆy𝑡
=∇𝑓(X𝑘)𝑇𝜕R2(ˆy𝑠,ˆy𝑡)
𝜕ˆy𝑡·𝜕𝜃𝑡
=∇𝑓(X𝑘)𝑇𝜕R2(ˆy𝑠,ˆy𝑡)
𝜕ˆy𝑡·𝜕ˆy𝑡𝑓(X𝑘)
Thus,
𝜕R2(ˆy𝑠,ˆy𝑡)
𝜕ˆy𝑡·𝜕ˆy𝑡=L𝐾+
∇𝑓(X𝐾)∇𝑓(X𝐾)𝑇−1
=L𝐾+K−1
𝐾𝐾
where L𝐾=I−A𝐾is a symmetrically normalized Laplacian matrix,
andK𝐾𝐾is the neural tangent kernel (NTK) matrix within the
target domain. Thus, 𝐿Rand𝑈Rare given by the maximum and
minimum eigenvalues of L𝐾+K−1
𝐾𝐾. □
3390