CE-RCFR: Robust Counterfactual Regression for
Consensus-Enabled Treatment Effect Estimation
Fan Wang
fanwang97@zju.edu.cn
College of Computer Science and
Technology, Zhejiang University
Hangzhou, ChinaChaochao Chen∗
zjuccc@zju.edu.cn
College of Computer Science and
Technology,Zhejiang University
Hangzhou, ChinaWeiming Liu
21831010@zju.edu.cn
College of Computer Science and
TechnologyZhejiang University
Hangzhou, China
Tianhao Fan
tianhaofan3@gmail.com
College of Computer Science and
Technology, China University of
Petroleum (East China)
Qingdao, ChinaXinting Liao
xintingliao@zju.edu.cn
College of Computer Science and
Technology, Zhejiang University
Hangzhou, ChinaYanchao Tan
yctan@fzu.edu.cn
College of Computer and Data
Science/College of Software, Fuzhou
University
Fuzhou, China
Lianyong Qi
lianyongqi@upc.edu.cn
College of Computer Science and
Technology, China University of
Petroleum (East China)
Qingdao, ChinaXiaolin Zheng
xlzheng@zju.edu.cn
College of Computer Science and
Technology, Zhejiang University
Hangzhou, China
Abstract
Estimating individual treatment effects (ITE) from observational
data is challenging due to the absence of counterfactuals and the
treatment selection bias. Prevalent ITE estimation methods tackle
these challenges by aligning the treated and controlled distributions
in the representational space. However, two critical issues have long
been overlooked: (1) Mini-batch sampling sensitivity (MSS) issue,
where representation distribution alignment at a mini-batch level
is vulnerable to poor sampling cases, such as data imbalance and
outliers; (2) Inconsistent representation learning (IRL) issue, where
representation learning within a unified backbone network suffers
from inconsistent gradient update directions due to the distribu-
tion skew between different treatment groups. To resolve these
issues, we propose CE-RCFR , aRobust Counter Factual Regression
framework for Consensus- Enabled causal effect estimation, includ-
ing a relaxed distribution discrepancy regularizer ( RDDR ) module
and a consensus-enabled aggregator ( CEA) module. Specifically, for
the robust representation alignment perspective, RDDR addresses
theMSS issue by minimizing unbalanced optimal transport diver-
gence between different treatment groups with a relaxed marginal
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672054constraint. For the accurate representation optimization perspec-
tive, CEAaddresses the IRLissue by resolving the consistent gradi-
ent update directions on shared parameters within the backbone
network. Extensive experiments demonstrate that CE-RCFR signifi-
cantly outperforms the state-of-the-art methods in treatment effect
estimations.
CCS Concepts
•Computing methodologies →Causal reasoning and diag-
nostics.
Keywords
Treatment Effect Estimation; Treatment Selection Bias; Representa-
tion Learning; Counterfactual Inference
ACM Reference Format:
Fan Wang, Chaochao Chen, Weiming Liu, Tianhao Fan, Xinting Liao, Yan-
chao Tan, Lianyong Qi, and Xiaolin Zheng. 2024. CE-RCFR: Robust Coun-
terfactual Regression for Consensus-Enabled Treatment Effect Estimation.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3672054
1 Introduction
Estimating Individual Treatment Effects (ITE) has emerged as a crit-
ical problem in extensive fields, such as healthcare [ 48], education
[11], employment [ 54,55] and recommendation [ 29,30,41,42,60].
Examples include a teacher assessing the effect of a study pro-
gram on a specific student, or a doctor identifying the effect of
a medicine for a patient. Although randomized controlled trials
(RCTs) are considered the gold standard for estimating treatment
effects, they often encounter some obstacles like high expenses and
3013
KDD ’24, August 25–29, 2024, Barcelona, Spain Fan Wang et al.
ethical dilemmas [ 9,28,31]. Therefore, due to the extensive amount
of observational data available, observational studies have become
a popular alternative for treatment effect estimation [4, 46].
The main challenge in estimating ITE from observational data
is the absence of counterfactuals [58,62]. Specifically, ITE quanti-
fies the causal effect of a treatment on an individual by measuring
the difference in potential outcomes with and without the treat-
ment [ 47]. In observational data, each unit is observed in either
the treated or controlled group, but not both, leaving a counter-
factual outcome unobserved. While when we attempt to estimate
the counterfactual outcomes to overcome the absence of counter-
factuals in ITE estimation, we often encounter another significant
challenge: treatment selection bias. Specifically, in observational
studies, individuals self-select belonging to either treatment group
or controlled group based on their preferences, as opposed to being
randomly assigned as in RCTs [ 59,61]. For instance, patients with
severe cases are more likely to take specific medicines (treatment
group), while those with mild cases are more inclined to take basic
medicines (controlled group), leading to a significant distribution
discrepancy between the two groups. Therefore, estimation based
on the observational data with such a selection bias makes it diffi-
cult to generalize the estimators trained within each group to the
entire population, leading to inaccurate counterfactual estimations.
To overcome the above challenges, state-of-the-art ITE esti-
mation methods often mitigate selection bias by minimizing the
distribution discrepancy between the treated group and the con-
trolled group for representation alignment, thereby leading to better
counterfactual inference [ 10,18,50,62,63]. Despite the significant
achievements of these methods, there are two critical issues that
have long been disregarded. Firstly, Issue 1: Mini-batch Sampling
Sensitivity (MSS). These representation-based methods minimize
distribution discrepancy within mini-batches rather than across the
entire dataset, making them sensitive to bad sampling cases. For
instance, an outlier within a mini-batch can misleadingly amplify
the calculated distribution discrepancy, resulting in improper align-
ment in the representation space. Secondly, Issue 2: Inconsistent
Representation Learning (IRL). Treatment selection bias causes
a distribution skew between the treated and controlled groups,
thereby endowing them with inconsistent optimization objectives
in representation learning. However, current methods often train
these divergent distributions with a unified backbone network,
probably resulting in sub-optimal representations for both groups.
For example, the gradient update process is dominated by the pa-
tients with severe cases in the treatment group, while dominated
by the individuals with mild cases in the controlled group. In this
situation, a unified backbone network struggles to find an optimal
update direction of the shared parameters that benefits the two
distinct groups, probably leading to sub-optimal descent directions
of representation learning.
To address the aforementioned issues, in this paper, we pro-
pose CE-RCFR , aRobust Counter Factual Regression framework for
Consensus- Enabled causal effect estimation. To tackle the Issue
1,CE-RCFR contains a relaxed distribution discrepancy regu-
larizer ( RDDR)that robustly aligns representational space between
the treated group and the controlled group at a mini-batch level,
even in the scenarios of data imbalance or outlier presence. Specifi-
cally, RDDR minimizes the unbalanced optimal transport divergencebetween the treated and controlled representations with a relaxed
marginal constraint, deriving an insightful transportation strat-
egy that preserves appropriate matches while being robust to poor
sampling cases. Furthermore, although entropy regularization is
commonly used to assist in solving optimal transportation prob-
lems, its inherent drawback lies in its tendency to yield dense trans-
portation plan, thereby impeding accurate distribution discrepancy
measurement [ 2,5,8]. In light of this, RDDR discards the entropy
regularization term and iteratively solves a sparse optimal trans-
portation strategy with mirror gradient descent, further ensuring
the stability and robustness of our representation space alignment
strategy. To tackle the Issue 2, CE-RCFR incorporates a consensus-
enabled aggregator ( CEA). Specifically, the aggregator resolves
inconsistent representation learning by solving a multi-objective
optimization problem with Pareto constraints, thereby securing a
consistent gradient update direction for representation learning
of heterogeneous treated and controlled groups. Hence, CE-RCFR
achieves a globally optimal solution conducive to training across
the divergent treated and controlled distributions using a unified
backbone network.
In summary, the contributions of this paper are three-fold: (1) We
propose a novel framework CE-RCFR , which conducts the treated
and controlled representation learning in both robust alignment
and optimization aspects to handle the distribution skew caused by
treatment selection bias, thereby achieving effective counterfactual
inference for causal effect estimations. (2) The CE-RCFR framework
contains two modules: RDDR andCEA. Specifically, RDDR minimizes
the unbalanced optimal transport divergence between treated and
controlled representations with a relaxed marginal constraint, ac-
commodating sampling imbalances and outliers in mini-batch based
representation alignment. CEAoptimizes the representation learn-
ing and the alignment of heterogeneous treated and controlled
distributions to a Pareto-stationary point, minimizing the impact
of inconsistent updating in the identical backbone network. (3) Ex-
tensive experiments conducted on two widely used semi-synthetic
benchmark datasets, i.e., IHDP and ACIC, demonstrate the effec-
tiveness and robustness of our proposed CE-RCFR , compared with
other state-of-the-art methods.
2 Preliminaries
We first formulate basic notations of this paper. Let Xbe the co-
variate space,Tthe treatment space, and Ythe outcome space. For
each covariate 𝑥∈X, there is a treatment assignment 𝑡∈T and
the corresponding potential outcome 𝑌𝑡∈Y. Our interest lies in
a binary treatment set, i.e., T={0,1}, where𝑡=1represents the
treated group and 𝑡=0represents the controlled group. We will
now elaborate some definitions and assumptions in this section.
2.1 Representation-based Counterfactual
Inference
This study mainly discusses counterfactual inference for ITE esti-
mation within a representation learning framework. Thus, we first
focus on the background of representation-based causal inference.
Definition 2.1. LetRbe the representation space, 𝜓:X→R the
representation function that maps XtoR. Here,𝑟∈R denotes the
representation specific to a covariate 𝑥, i.e.,∀𝑥∈X,∃𝑟=𝜓(𝑥)∈R .
3014CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation KDD ’24, August 25–29, 2024, Barcelona, Spain
Letℎ:R×T→Y map each combination of representation and
treatment indicator to its factual outcome, i.e., 𝑌𝑡=ℎ(𝑟,𝑡).
For an identical covariate 𝑥, if we have its both treated and
controlled outcomes, we can estimate the individual treatment
effect (ITE)1defined in the potential outcome framework [47]:
𝜏(𝑥):=E[𝑌1−𝑌0|𝑥]. (1)
However, in observational study, we only have access to factual
outcomes and can not observe the counterfactual outcomes, which
has been the main challenge in ITE estimation. Thus, to ensure the
identifiability of the ITE function 𝜏(𝑥), the representation learning-
based counterfactual estimation methods [ 22,50] typically build
upon the following basic assumptions.
Assumption 2.1. (Consistency). The potential outcome is consistent
with the observed outcome if the received treatment is 𝑡.
Assumption 2.2. (Unconfoundedness). Given 𝑥, potential outcomes
𝑌0and𝑌1are independent of treatment assignment, i.e., (𝑌0,𝑌1)⊥⊥𝑡∥𝑥.
Assumption 2.3. (Positivity). For any covariate 𝑥, there is always a
positive probability to receive treatment 0 and 1, i.e., ∀𝑥,0<𝑝(𝑡=
1|𝑥)<1.
Given these assumptions, these representation learning-based coun-
terfactual estimation methods measure the ITE estimation per-
formance by the Precision in Estimation of Heterogeneous Effect
(PEHE [20]).
Definition 2.2. The expected PEHE loss of 𝑓is:
𝜖PEHE(𝑓)=∫
X
ˆ𝜏𝑓(𝑥)−𝜏(𝑥)2
𝑝(𝑥)𝑑𝑥, (2)
where𝑓(𝑥,𝑡)=ℎ(𝜓(𝑥),𝑡), and we abbreviate 𝜖PEHE(𝜓,ℎ)as𝜖PEHE(𝑓).
[50] derived a bound on the PEHE loss in Eq. (2), offering a
theoretical foundation for ITE estimation that can be optimized
by minimizing both the factual outcome estimation errors and the
distribution discrepancy. The optimization objective is as follows.
Definition 2.3. Let𝐼𝑃𝑀G(·,·)be the Integral Probability Metric
[44,52] that quantifies the distance between two distributions based
on a function family G, we have:
𝜖PEHE(𝜓,ℎ)≤2
𝜖𝑡=1
F(𝜓,ℎ)+𝜖𝑡=0
F(𝜓,ℎ)+
𝐵𝜓𝐼𝑃𝑀G(𝑝𝑡=0
𝜓(𝑟),𝑝𝑡=1
𝜓(𝑟))−2𝜎2
𝑌i
,(3)
where𝜖𝑡=1
Fand𝜖𝑡=0
Fare the expected factual loss associated with
𝑡=1and𝑡=0,𝑝𝑡
𝜓(𝑟)is the distribution over Rinduced by the
treated or control group, 𝐵𝜓is a hyperparameter and 𝜎2
𝑌is the
variances of outcomes.
To sum up, the pipeline of the representation-based counterfac-
tual inference for treatment effect estimation is generally summa-
rize as follows: for each unit 𝑥𝑖,Step 1: learn its representation 𝑟
with a backbone network 𝜓, i.e.,𝑟=𝜓(𝑥𝑖);Step 2: estimate the the
potential outcome ˆ𝑦𝑖throughℎ(𝑟,𝑡);Step 3: train𝜓andℎby mini-
mizing distribution alignment error LD(𝜓)as well as the treated
and controlled factual outcome estimation errors L𝑡=1
F(𝜓,ℎ)and
L𝑡=0
F(𝜓,ℎ), respectively. The overall optimization objective is:
L=L𝑡=1
F(𝜓,ℎ)+L𝑡=0
F(𝜓,ℎ)+L D(𝜓), (4)
1Also known as Conditional Average Treatment Effect (CATE) sometimes.
Figure 1: The overview of CE-RCFR , including two modules: (1)
Relaxed Distribution Discrepancy Regularizer ( RDDR) module
for addressing MSS issue; (2) Consensus-Enabled Aggregator
(CEA) module for addressing IRL issue.
whereL𝑡
F(𝜓,ℎ)is always quantified by E𝑥𝑖∼𝑝𝑡(𝑥)∥ℎ(𝜓(𝑥𝑖),𝑡)−𝑦𝑖∥2,
here,𝑝𝑡(𝑥)represents treatment group distribution specific to 𝑡,
and𝑦𝑖denotes the factual outcome of 𝑥𝑖;LD(𝜓)can be quantified
by Wasserstein distance [ 14], Maximum Mean Discrepancy [ 17],
Optimal Transport discrepancy [56] and so on [50].
2.2 Background on Optimal Transport
Optimal Transport (OT) serves as a pivotal tool to quantify dis-
tribution discrepancy as the minimum transport cost from one
distribution into another. [ 24] defined the balanced OT problem
(also known as Kantorovich problem) as follows.
Definition 2.4. Consider two distributions 𝛼and𝛽with𝑛and𝑚
units, respectively, the balanced OT problem seeks the transport plan
𝝅∈R+
𝑛×𝑚between𝛼and𝛽with the minimum cost C∈R+
𝑛×𝑚:
OT(𝛼,𝛽)=min
𝝅≥0⟨𝐶,𝝅⟩s.t. 𝝅1𝑚=𝒂,𝝅⊤1𝑛=𝒃, (5)
where 𝒂(resp. 𝒃) is the mass of units in 𝛼(resp.𝛽). Moreover, the
transport plan should satisfy ∥a∥1=∥b∥1, i.e., all the mass from 𝒂is
completely transformed to 𝒃. However, one of the main challenges
in solving (5) is high time complexity (even reaching the multino-
mial level) [ 6,37,38]. Thus, some researchers introduce entropy
regularization term into the balanced OT problem to accelerate
optimization [13, 39, 40]:
OT(𝛼,𝛽)=min𝝅≥0⟨𝐶,𝝅⟩+𝜖·⟨𝝅,log(𝝅)−1⟩
s.t. 𝝅1𝑚=𝒂,𝝅⊤1𝑛=𝒃,(6)
where𝜖·⟨𝝅,log(𝝅)−1⟩is the entropy regularization term that
makes the balanced OT problem 𝜖-convex, allowing it to be effi-
ciently solved using the Sinkhorn-Knopp approach with GPUs that
consists only of simple matrix-vector products
3 Methodology
In this section, we propose the Robust CounterFactual Regres-
sion framework for Consensus-Enabled causal effect estimation
3015KDD ’24, August 25–29, 2024, Barcelona, Spain Fan Wang et al.
(a) Ideal Case
 (b) Imbalanced Case
 (c) Outlier Case
Figure 2: The intuitive understanding of Optimal Transport solutions through
geometric means in three cases. Here, green lines denote false match(es).
Figure 3: Entropy curve with strictly
positive transportation plan, favoring
a neutral value to minimize loss.
(CE-RCFR ), including a relaxed distribution discrepancy regu-
larizer ( RDDR)module and a consensus-enabled aggregator ( CEA)
module. The overview of CE-RCFR is illustrated in Fig. 1. Specifically,
CE-RCFR first establishes covariate representations of both treated
and controlled groups through a backbone network 𝜓. Then, RDDR
aligns the treated and controlled distributions in representational
space at a mini-batch level, even though there are noises and out-
liers, to robustly encounter distributional shifts caused by selection
bias. Through the robust and effective alignment, ℎ(𝜓,𝑡=0)and
ℎ(𝜓,𝑡=1)will be mutually generalizable in estimating potential
outcome ˆ𝑌1and ˆ𝑌0. Finally, CEAresolves consistent optimization
directions in each iteration of training, mitigating inconsistent de-
viations from treated to controlled representation learning within
backbone network, and thereby ensuring global optimality.
3.1 Relaxed Distribution Discrepancy
Regularizer (RDDR)
Motivation. Heterogeneous treated and controlled distributions
caused by selection bias impede the accurate causal effect estima-
tion. State-of-the-art ITE estimation methods strive to mitigate
selection bias by minimizing the discrepancy between the two dis-
tributions, i.e., the 𝐼𝑃𝑀Gterm in Definition 2.3, and have achieved
considerable success [ 10,18,50,62,63]. Among these researches,
OT stands out for discrepancy measurement and minimization, be-
cause of its convenience of optimization within the traditional su-
pervised learning framework (instead of computational-demanding
adversarial-based framework) and its demonstrated superiority
over other methods [13, 58].
However, there are two critical issues that warrant consideration.
(1) Modern representation learning-based causal estimators
(besides OT-based approaches) are generally trained on mini-
batches, yet the discrepancy measurement is vulnerable to
bad sampling cases at a mini-batch level (referred to as the Mini-
batch Sampling Sensitivity (MSS) issue for short). Specifically, the
current balanced OT in Definition 2.4, for instance2, will fall into
the following three cases at a mini-batch level: firstly, in ideal case
(Fig 2a), where treated and controlled distributions are balanced
and free of outliers, the transport strategy works well and properly
quantifies the discrepancy; Secondly, in imbalanced case (Fig 2b),
skewed distributions mislead the transport strategy to produce false
matches; Thirdly, in outlier case (Fig 2c), outliers that should be
discarded are falsely matched with normal points, disrupting the
overall transport strategy. Overall, in non-ideal mini-batches, the
2Beyond optimal transport (OT), many widespread approaches, such as those discussed
by [62] and [43], fail to cope with the Mini-batch Sampling Sensitivity (MSS) issue.balanced OT may figure out the erroneous transport strategy with
significant variance, which makes proper measurement of distri-
bution discrepancy challenging and thereby leads to misguided
updates of𝜓for the representation alignment. (2) While current
estimators introduce entropic regularization (as in Eq. (6))
for efficiency and scalability [ 13], these entropy-regularized
OT leads to dense matching associated with spreading mass
between treated and controlled groups (referred to as the Loss
of Sparsity (LoS) issue for short) [ 2,5,8]. Specifically, entropy reg-
ularization induces the mass of each treated unit to be fractionally
distributed to almost all control units instead of a few centralized
units, resulting in a fully dense transportation strategy. Vividly,
the entropic curve in Fig 3 illustrates that entropy keeps the trans-
portation plan strictly positive, with a tendency towards a neutral
value (far from 0) for minimizing the entropy regularization term.
Consequently, the dense transportation strategy results in some am-
biguous and uncertain matches, leading to a substantial deviation
from the true underlying distribution and potentially undermining
the reliability and accuracy of the causal effect estimation.
Problem Definition. In light of the above issues, the main problem
we need to solve in this subsection is: Given the treated and controlled
representation distributions, a model is needed to search for a sparse
OT plan at a mini-batch level for accurate and robust discrepancy
measurement and minimization, even though there are bad sampling
cases with imbalanced data or outliers.
Solving the Problem. To begin with, we formalize the distribution
discrepancy measured by current OT (balanced OT with entropy
regularization as in Eq. (6) generally solved by Sinkhorn algorithm)
at a stochastic mini-batch level:
ˆOT𝜓=OT
ˆ𝑝𝑡=1
𝜓(𝑟),ˆ𝑝𝑡=0
𝜓(𝑟)
, (7)
where𝑟, obtained through 𝜓(𝑥)as in Definition 2.1, refers specifi-
cally to a representation of an available covariate 𝑥within a sto-
chastic sampled mini-batch group, ˆ𝑝𝜓𝑡=1(𝑟)and ˆ𝑝𝜓𝑡=0(𝑟)are the
corresponding treated and controlled distributions w.r.t. 𝑛treated
units and𝑚controlled units, respectively. In the following of this
paper, we abbreviate the solution of Stochastic Optimal Transport
as SOT.
Then, with Eq. (7), the optimization objective in Definition 2.3
can be rewritten with the probability of at least 1−𝛿as:
𝜖PEHE(𝜓,ℎ)≤2
𝜖𝑡=1
F(𝜓,ℎ)+𝜖𝑡=0
F(𝜓,ℎ)+𝐵𝜓ˆOT𝜓−2𝜎2
𝑌+𝑂1
𝛿𝑁
(8)
where𝑁represents the mini-batch size and O(·) denotes the sam-
pling complexity term [50, 58].
3016CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation KDD ’24, August 25–29, 2024, Barcelona, Spain
Building on the above foundation, the PEHE loss (in Definition
2.2) can be optimized by minimizing the OT discrepancy and the fac-
tual outcome estimation errors at a mini-batch level. Here, O
1
𝛿𝑁
also suggests that potential risks of bad cases are highly dependent
on the sampling batch size 𝑁, with smaller 𝑁(i.e., a mini-batch
level) leading to greater risks. However, the current discrepancy
measurement with ˆOT𝜓suffers from the MSS issue and LoS issue
as introduced in motivation. In light of this, we devise a relaxed
distribution discrepancy regularizer (RDDR) as in Definition 3.1.
Definition 3.1. Given two distributions 𝛼and𝛽with𝑛and𝑚units,
respectively, unbalanced OT with relaxed mass-preserving constraints
seeks the transport plan 𝝅∈R+
𝑛×𝑚between𝛼and𝛽with the mini-
mum cost C∈R+
𝑛×𝑚:
UOT𝜅(𝛼,𝛽)=min
𝝅≥0⟨C,𝝅⟩+𝜅
D𝜑(𝝅1𝑚,a)+D𝜑
𝝅T1𝑛,b
,(9)
where 𝒂(resp. 𝒃) is the mass of units in 𝛼(resp.𝛽),𝜅is a hyper-
parameter,𝐷𝜑represents the Bregman divergence, which is derived
from the strictly convex and differentiable function 𝜑.
The reason why RDDR can overcome both MSS issue and LoS
issue lies in twofold: (1) traditional balanced OT in Definition 2.4
requires that a transportation strategy satisfying all the mass from
𝒂is transformed to 𝒃with the constraint { 𝝅1𝑚=𝒂,𝝅⊤1𝑛=𝒃}.
Such a strong mass-preservation constraint leads to the fact that
even outliers in a mini-batch, for instance, are compelled to be
transformed as in Fig. 2c. On the contrary, for MSS issue, the
unbalanced OT alleviates the strong constraints with a relaxed
mass-preservation term 𝜅(𝐷𝜑(·)), where𝜅indicates the strength
of penalization and the unbalanced OT will recover to the balanced
OT when𝜅→∞ . Therefore, a relaxation with a small 𝜅allows
theunbalcanced OT to accommodate data imbalances or outliers
within the mini-batches, through the permission of unit masses’
creation and destruction. (2) For LoS issue, RDDR in Definition 3.1
discards the entropy regularization term to seek a sparse transport
strategy of exact unbalanced OT.
We now provide a stationary and easy-to-implement optimiza-
tion algorithm to iteratively seek the optimal transport strategy
in Definition 3.1. Firstly, we set 𝜑(𝑧)=𝑧log𝑧−𝑧to achieve KL
divergence𝐷𝐾𝐿and rewrite the optimization problem in Eq. (9):
UOT𝜅(𝛼,𝛽)=min
𝝅≥0⟨C,𝝅⟩+𝜅 𝐷𝐾𝐿 𝒂′,𝒂+𝐷𝐾𝐿 𝒃′,𝒃,(10)
where 𝒂′=𝝅1𝑚,𝒃′=𝝅⊤1𝑛and𝐷𝐾𝐿(𝒎,𝒏)= 𝒎log𝒎
𝒏−𝒎+𝒏.
To iteratively solve the optimal 𝝅, we adopt Mirror Gradient De-
scent [1, 35] and rewrite the optimization problem as:
min
𝝅≥0J=𝝅(new)∇L
𝝅(old)
+𝜂·𝐷𝐾𝐿
𝝅(new),𝝅(old)
.(11)
Finally, the iteration solution is:
𝝅(new)=𝝅(old)exp
−C
𝜂 
𝒂
𝒂′
(old)!𝜅
𝜂 
𝒃
𝒃′
(old)!𝜅
𝜂
. (12)
In practice, we set 𝜂=𝜅to reduce the necessity of hyperparameter
tuning. By iteratively updating 𝝅(new) with𝝅(old)until convergence
through Eq. (12), we can obtain the optimal transportation strategy
with the minimum cost for discrepancy measurement. Note that
the multiplicative updates in this solution bear a similarity withthe well-known Sinkhorn algorithm [ 8,13,51], thus ensuring com-
putational efficiency and feasibility on GPUs. Overall, we rewrite
the distribution alignment loss L𝐷(𝜓)in Eq. (4) as:
L𝜅
D(𝜓)=UOT𝜅
ˆ𝑝𝑡=1
𝜓(𝑟),ˆ𝑝𝑡=0
𝜓(𝑟)
. (13)
The optimal transport strategy of Eq. (13) can be iteratively solved
by Eq. (12). The pseudo code of RDDR module is outlined in Algo-
rithm 1.
Algorithm 1: RDDR
Input: mass of units 𝛼=Σ𝑛
𝑖=1a𝑖𝛿r𝑖,𝛽=Σ𝑚
𝑗=1b𝑗𝛿r𝑗, cost
matrix𝐶𝑖,𝑗=∥𝑟𝑖−𝑟𝑗∥2
2, max iterations ℓmax
Output: 𝝅: the unbalanced optimal transport matrix
1:𝝅=a𝑖b⊤
𝑗
2:whileℓ<ℓmaxdo
3: D=𝑒𝑥𝑝(−C
𝜂)
4: a′=𝝅1𝑛,b′=𝝅⊤1𝑚
5: u= a
a′𝜅
𝜂,v=
b
b′𝜅
𝜂
6:𝝅=diag(u)(D⊙𝝅)diag(v)
7:end while
8:Return 𝝅
3.2 Consensus-Enabled Aggregator (CEA )
Motivation. Following on from the above module, there are three
optimization tasks sharing the same backbone network. However,
these optimization objectives are competing, especially in repre-
sentation learning for heterogeneous treated and controlled distri-
butions caused by selection bias. Brute-force optimization without
any tricks may put the gradient update of shared parameters in a
dilemma leading to sub-optimal performances.
Problem Definition. Therefore, the main problem we need to
solve in this subsection is: Given multiple optimization objectives, a
model is needed to carefully decide on a gradient update direction over
the shared parameters that do not compromise any of the optimization
objectives even if they are competing.
Solving the problem. Let
Ltask 1,Ltask 2,Ltask 3	
=
L𝑡=0
F,L𝑡=1
F,LD	
in Eq. (4), and we regard the shared parameters in backbone network
𝜓as𝜃𝑠, parameter ℎ(𝜓,𝑡=0)as𝜃1specific to potential outcome
estimation for controlled group, and ℎ(𝜓,𝑡=1)as𝜃2specific to po-
tential outcome estimation for treated group. Then, the optimization
tasks in
Ltask 1,Ltask 2,Ltask 3	can be formulated as parametric
hypothesis tasks, i.e., Ltask 𝑖(𝜃𝑠,𝜃𝑖)=L(𝜓(𝑥,𝜃𝑠),ℎ(𝑟,𝜃𝑖)). Here,
ℎ=𝜃𝑖=∅when𝑖=3. Next, we convert the multi-task learning
problem into a multi-objective optimization, i.e.,
min
𝜃𝑠,𝜃1,𝜃2 Ltask 1(𝜃𝑠,𝜃1),Ltask 2(𝜃𝑠,𝜃2),Ltask 3(𝜃𝑠)⊤. (14)
For the multi-objective optimization problem, CEAaims to solve it
to Pareto-stationary point [ 36]. Specifically, for task-specific param-
eters, CEAemploys independent gradient descent to individually
optimize each task, while for the shared parameters, CEAminimizes
the minimum possible convex ensemble of inconsistent deviations:
min
𝝀1
2𝑁∑︁
𝑖=1𝜆𝑖·∇𝜃𝑠Ltask 𝑖2
2s.t.3∑︁
𝑖=1𝜆𝑖=1,and∀𝑖,𝜆𝑖≥0.(15)
3017KDD ’24, August 25–29, 2024, Barcelona, Spain Fan Wang et al.
Inspired by Multiple Gradient Descent Algorithm (MGDA) [ 49], the
optimization problem outlined above is equivalent to finding the
minimum-norm point within the convex hull of inputs, forming
a convex quadratic problem with linear constraints. We solve this
iteratively through linear search, enabling an analytical resolution.
In concrete, we first initialize 𝝀0={2
6,1
6,3
6}(parameter analyses
can be seen in Section 4.2). Next, we find the toughest task that is
most inconsistent with the others:
𝜏=arg min
𝑖*3∑︁
𝑗=1,𝑗≠𝑖𝜆𝑗∇𝜃𝑠Ltask,𝑗,∇𝜃𝑠Ltask,𝑖+
. (16)
Then, the optimization problem in Eq. (15) can be simplified as:
min
𝜉∈[0,1](1−𝜉)·3∑︁
𝑗=1,𝑗≠𝜏𝜆𝑗∇𝜃𝑠Ltask,𝑗+𝜉·∇𝜃𝑠Ltask,𝜏2
2. (17)
Abbreviating∇𝜃𝑠Ltask,𝜏as∇𝜃𝑠,𝜏, andÍ3
𝑗=1,𝑗≠𝜏𝜆𝑗∇𝜃𝑠Ltask,𝑗as∇𝜃𝑠,𝜏,
the analytical resolution is:
𝜉𝜏=
∇𝜃𝑠,𝜏−∇𝜃𝑠,𝜏⊤
∇𝜃𝑠,𝜏
∇𝜃𝑠,𝜏−∇𝜃𝑠,𝜏2
2+,1
0, (18)
where[·]+,1
0=max(min(·,1)). Given𝜉𝜏, we update the parameters
with 𝝀=(1−𝜉𝜏)·𝝀+𝜉𝜏e𝜏, where e𝜏is the one-hot vector with 1
at the𝜏-th position.
After iterations of identifying the toughest task, analytically
solving𝜉𝜏, and updating 𝝀,CEAderives the optimal aggregation 𝝀
of three inconsistent tasks, i.e., a consensus-enabled optimization
direction avoiding any dominated task. By solving for 𝝀, we up-
date the shared parameter 𝜃𝑠in the backbone network 𝜓through
𝜃𝑠=𝜃𝑠−𝜇Σ3
𝑖=1𝜆𝑖∇𝜃𝑠L𝑡𝑎𝑠𝑘,𝑖(𝜃𝑠,𝜃𝑖). In addition, without loss of
generality, parameters 𝜃1and𝜃2specific to representation learn-
ing for the controlled and treated groups can be updated through
𝜃1=𝜃1−𝜇∇𝜃1L𝑡𝑎𝑠𝑘, 1(𝜃𝑠,𝜃1)and𝜃2=𝜃2−𝜇∇𝜃2L𝑡𝑎𝑠𝑘, 2(𝜃𝑠,𝜃2).
We provide the algorithm of CEAmodule in Algorithm 2.
Algorithm 2: CEA
Input: factual outcome estimation loss L𝑡=0
F(𝜓,ℎ)and
L𝑡=1
F(𝜓,ℎ), distribution discrepancy loss L𝜅
D(𝜓),
initial weight 𝝀0, max iterations ℓmax
1:Initialize 𝝀=𝝀0
2:Initialize
L𝑡𝑎𝑠𝑘, 1,L𝑡𝑎𝑠𝑘, 2,L𝑡𝑎𝑠𝑘, 3	
=
L𝑡=0
F,L𝑡=1
F,LD	
3:whileℓ<ℓmaxdo
4:𝜏=arg min𝑖DÍ3
𝑗=1,𝑗≠𝑖𝜆𝑗∇𝜃𝑠Ltask,𝑗,∇𝜃𝑠Ltask,𝑖E
5: set∇𝜃𝑠Ltask,𝜏as∇𝜃𝑠,𝜏, andÍ3
𝑗=1,𝑗≠𝜏𝜆𝑗∇𝜃𝑠Ltask,𝑗as∇𝜃𝑠,𝜏
6: solve𝜉𝜏=arg min𝜉(1−𝜉)∇𝜃𝑠,𝜏+𝜉·∇𝜃𝑠,𝜏2
2by Eq (18)
7:𝝀=(1−𝜉𝜏)·𝝀+𝜉𝜏e𝜏
8:end while
9:𝜃𝑠=𝜃𝑠−𝜇Σ3
𝑖=1𝜆𝑖∇𝜃𝑠L𝑡𝑎𝑠𝑘,𝑖(𝜃𝑠,𝜃𝑖)
10:𝜃1=𝜃1−𝜇∇𝜃1L𝑡𝑎𝑠𝑘, 1(𝜃𝑠,𝜃1),𝜃2=𝜃2−𝜇∇𝜃2L𝑡𝑎𝑠𝑘, 2(𝜃𝑠,𝜃2)
3.3 Putting together
The overall algorithm of our proposed CE-RCFR , integrating mod-
ules RDDR andCEA, is provided in Algorithm 3.Algorithm 3: CE-RCFR
Input: covariates of treated and control units {𝑥𝑘}𝑚+𝑛
𝑘=1,
factual outcomes of treated group {𝑦𝑖}𝑛
𝑖=1and
control group{𝑦𝑗}𝑚
𝑗=1, hyper-parameters 𝜅and𝜂,
initial weight 𝝀0={2
6,1
6,3
6}, max iterations ℓmax
1:while not converged do
2: obtain representations {𝑟𝑘}𝑛+𝑚
𝑘=1with backbone network 𝜓
3: estimate treated outcomes {ˆ𝑦𝑖}𝑛
𝑖=1and control outcomes
{ˆ𝑦𝑗}𝑚
𝑗=1with networks ℎ(𝜓,𝑡=0)andℎ(𝜓,𝑡=1)
4: calculate cost matrix 𝐶𝑖,𝑗=∥𝑟𝑖−𝑟𝑗∥2
2between treated and
control representations
5:𝝅←Algorithm 1
𝛼={𝑟𝑖}𝑛
𝑖=1,𝛽={𝑟𝑗}𝑚
𝑗=1,C=C,ℓmax
6: obtainL𝑡=0
F(𝜓,ℎ)←1
𝑛Σ𝑛
𝑖=1∥ˆ𝑦𝑖−𝑦𝑖∥2
2, and
L𝑡=1
F(𝜓,ℎ)←1
𝑛Σ𝑚
𝑗=1ˆ𝑦𝑗−𝑦𝑗2
2, andL𝜅
D(𝜓)←⟨ D,𝝅⟩
7: consistent gradient updates with
Algorithm 2
L𝑡=0
F(𝜓,ℎ),L𝑡=1
F(𝜓,ℎ),L𝜅
D(𝜓),𝝀0,ℓmax
8:end while
4 Experiments
In this section, we carry out extensive experiments to answer the
following four main questions: RQ1: How does CE-RCFR perform
in comparison to the sate-of-the-art causal estimators? RQ2: What
are the effects of different model components? RQ3: Does CE-RCFR
robustly cope with MSS issue and IRL issue? RQ4: How does the
performance of CE-RCFR vary with different values of the hyper-
parameters?
4.1 Experimental Setup
Datasets. The lack of counterfactuals hinders the assessment of
Precision in Estimating Heterogeneous Effects (PEHE) when using
observational data. Therefore, this study utilizes two semi-synthetic
benchmarks for experimentation according to the experiments de-
signed by [ 62] and [ 50]. The first is IHDP benchmark, which fo-
cuses on determining the impact of specialized home visits on the
potential cognitive abilities of infants. This benchmark includes
747 data points and 25 different covariates. The second is ACIC
dataset, which originates from the collaborative perinatal project
and features 4802 data points along with 58 covariates. We split
the training, validation, and test sets of the above two datasets by
70 : 15 : 15 in our experiments.
Comparison Methods. Our study examines three distinct cate-
gories of baseline methods. Firstly, we explore statistical estimators:
this includes (1) OLS which takes ordinary least squares regression
incorporating treatment as a covariate; (2) R-Forest which is a ran-
dom forest approach that also uses the treatment as a covariate;
(3) S-learner [ 27] which takes a single network with the treatment
as a covariate, and (4) T-learner [ 27] which separates neural re-
gressors for distinct treatment groups, along with (5) TARNet [ 50].
Secondly, we assess matching estimators, which encompass (6)
k-NN [12]; (7) PSM [46] (propensity score matching using logistic
regression). Thirdly, we consider representation-based estimators,
including (8) BNN [ 22] (balancing neural network); (9) counter-
factual regression models using the Maximum Mean Discrepancy
3018CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Performance comparisons on IHDP benchmark. Bold values denote the best performances of all methods.
In Sample Out Sample
Dataset IHDP ACIC IHDP ACIC
Metric PEHE RMSE fRMSE cPEHE RMSE fRMSE cPEHE RMSE fRMSE cPEHE RMSE fRMSE c
OLS 2.9447 1.5031 2.1838 2.4526 4.2328 3.8095 2.9508 1.4990 1.8649 2.3606 5.1385 4.3048
R-Forest 2.3752 1.9676 1.5203 3.9788 2.8568 3.2931 2.1776 2.0375 1.2618 3.8419 3.3511 3.2951
S-learner 2.4563 0.9683 1.9911 3.1713 2.1485 2.1472 2.8024 1.4801 1.8409 2.9456 2.1671 2.0625
T-learner 2.7114 2.0434 1.9440 2.2520 2.4967 3.0583 2.8613 2.1221 2.1725 2.3557 1.9769 2.5470
TARNet 2.2849 1.3087 1.5346 1.9023 1.9177 2.4894 2.6180 1.6114 1.5556 1.8475 1.7693 2.0667
KNN 2.0238 1.2750 1.6304 2.9579 1.9958 3.4889 2.6062 2.4544 1.2870 2.7434 4.3355 3.5442
PSM 2.6659 1.9865 2.0801 4.5867 3.1749 5.1907 2.8179 1.8602 2.4014 4.8878 3.1154 5.6647
BNN 1.7057 1.0824 1.3215 1.7082 2.5536 2.7082 2.0871 1.4627 1.2344 1.7090 2.0115 2.1840
CFR-MMD 1.7092 1.0457 1.3437 1.4330 1.6561 2.1849 2.0496 1.4548 1.2374 1.3417 1.6473 2.0448
CFR-WASS 1.4022 0.9713 0.8823 1.7473 1.4696 1.9548 1.8683 1.3946 0.9549 1.7199 1.7543 1.6431
ESCFR 0.8457 0.6840 0.5362 1.3718 1.2428 1.6322 1.2239 1.0250 0.6228 1.2375 1.3861 1.3372
CE-RCFR 0.5818 0.4572 0.4287 0.9765 1.239 1.4534 0.7044 0.5808 0.3757 0.7865 1.1247 1.1507
Table 2: Ablation study on IHDP benchmark. Bold values denote the best performances of all methods.
In
Sample Out Sample
SO
T RDDR CEA PEHE RMSE f RMSE c PEHE RMSE f RMSE c
✗
✗ ✗ 1.0602 0.8046 0.7723 1.3863 1.1420 0.8068
✓ ✗ ✗ 0.9857 0.7632 0.7053 1.3093 1.0979 0.7545
✓ ✓ ✗ 0.8276 0.6676 0.5280 1.1854 0.9983 0.6074
✓ ✓ ✓ 0.5818 0.4572 0.4287 0.7044 0.5808 0.3737
(CFR-MMD)[ 50]; (10) counterfactual regression models using the
Wasserstein metric (CFR-WASS) [ 50]; (11) counterfactual regres-
sion models using a stochastic optimal transport framework with
entropy regularization (ESCFR) [58].
Implementation Details. We set the dimension of representa-
tional space as 60 and the bach size as 64. We adopt Adam opti-
mizer with both the learning rate and weight decay as 0.001. For
the hyper-parameters introduced in our CE-RCFR , we set the pe-
nalization strength 𝜅in UOT (Definition 3.1) as 25, and initialize
𝝀0={1
6,2
6,3
6}for Pareto optimization. Detailed hyper-parameter
analyses can be found in Section 4.2.
Evaluation Metrics. Following [ 62], we employ the PEHE de-
scribed in equation (2) as one of the key metrics for evaluating per-
formance. Moreover, Root Mean Square Error (RMSE) is a widely
used metric for assessing accuracy. It quantifies the average mag-
nitude of errors between predicted and actual ground truth. Thus,
we use RMSE fandRMSE cto measure the predictive accuracy for
factual and counterfactual outcomes, respectively. Furthermore, we
report both in-sample and out-of-sample findings, based on the
training and testing datasets respectively, following [50].
4.2 Experimental Results
Performance Comparison (RQ1). The comparison results on
IHDP and ACIC datasets are shown in Table 1. From the results, we
have the following three main observations: (1)CE-RCFR shows
superiority over all baselines in terms of all metrics, prov-
ing the effectiveness and accuracy of our causal estimation. Thegood performances are, firstly, since RDDR endows the represen-
tation space with the ability to be effectively aligned, such that
ℎ(𝜓,𝑡=1)for potential treated outcome prediction and ℎ(𝜓,𝑡=0)
for potential controlled outcome prediction are generalizable to
each other, even if there are data imbalances or outliers on a sto-
chastic mini-batch samples (for MSS issue); secondly, since CEA
endows the optimal representation learning for both treated and
controlled groups, even if their distributions are skewed resulting
in inconsistent gradient updates in the identical backbone network
𝜓(forIRL issue). (2) Representation-based estimators, includ-
ingCE-RCFR , generally achieve the best overall performance.
This is because these methods mitigate selection bias by balancing
the distribution of the representation space. However, compared to
our proposal, MSS and IRL issues inhibit the counterfactual estima-
tion capabilities of these methods. It is worth noting that ESCFR
outperforms all other methods except our CE-RCFR method, as it
solves MSS issue through Unbalanced Optimal Transport. However,
ESCFR introduces entropy regularization to obtain a dense trans-
port strategy (LoS issue in Section 3.1) and also ignores the IRL
problem, resulting in inferior performance. (3) TARNet, which
combines the advantages of T-learner and S-learner, achieves
a competitive overall performances in both statistic estima-
tors and matching estimators. However, overlooking treatment
selection bias and inconsistent representation learning inevitably
cause inferior performance.
Ablation Study (RQ2). To study the effects of individual com-
ponents, we conduct ablation study on the IHDP benchmark in
Table 2. The performance increment from initial TARNet to Sto-
chastic Optimal Transport (SOT, solved as in Eq.(7) and Eq.(8))
3019KDD ’24, August 25–29, 2024, Barcelona, Spain Fan Wang et al.
(a) SOT
 (b)RDDR w/ ER
 (c)RDDR w/o ER (ours)
Figure 4: Robustness analyses for the imbalanced case.
(a) SOT
 (b)RDDR w/ ER
 (c)RDDR w/o ER (ours)
Figure 5: Robustness analyses for the outlier case.
32 64 96 128
Batch Size0.00.51.01.52.0PEHE
32 64 96 128
Batch Size0.00.51.0RMSEf
32 64 96 128
Batch Size0.00.51.01.5RMSEcTarNet CFR-MMD CFR-Wass ESCFR RDDR CE-RCFR
(a) In sample
32 64 96 128
Batch Size012PEHE
32 64 96 128
Batch Size0.00.51.01.5RMSEf
32 64 96 128
Batch Size0.00.51.01.5RMSEcTarNet CFR-MMD CFR-Wass ESCFR RDDR CE-RCFR (b) Out sample
Figure 6: Robustness analyses under different batch sizes.
Table 3: Parameter sensitivity w.r.t. 𝝀0.
metrics
modes(1
6,1
6,4
6)
(1
6,2
6,3
6)
(1
6,3
6,2
6)
(1
6,4
6,1
6)
(2
6,1
6,3
6)
(2
6,2
6,2
6)
(2
6,3
6,1
6)
(3
6,1
6,2
6)
(3
6,2
6,1
6)
(4
6,1
6,1
6)
PEHEin
sample 0.6340 0.6510 0.6391 0.6370 0.5818 0.6220 0.6233 0.6470 0.6350 0.5990
out sample 0.8062 0.7765 0.7748 0.7980 0.7044 0.7750 0.7773 0.8600 0.8426 0.8220
RMSE fin sample 0.4657 0.4807 0.4672 0.4650 0.4572 0.4527 0.4673 0.4513 0.4489 0.4562
out sample 0.5832 0.5893 0.5903 0.5878 0.5808 0.5868 0.6048 0.6429 0.6134 0.6675
RMSE cin sample 0.4811 0.4868 0.4930 0.4974 0.4287 0.4631 0.4896 0.4427 0.4474 0.4017
out sample 0.4735 0.4426 0.4593 0.4735 0.3757 0.4223 0.4456 0.4242 0.4255 0.3916
proves the effectiveness of using OT technique to quantify and mit-
igate selection bias. Then for MSS issue, CE-RCFR augments RDDR
introduced in Section 3.2, whose performance improvement proves
the necessity of solving the MSS issue and the effectiveness of re-
laxing the mass-preservation constraint. Finally, the performance
improvement through augmenting CEA, (i.e., the complete CE-RCFR
framework) proves that our method can achieve global optimality
for IRL issue.
Analysis of Robustness (RQ3). We evaluate the robustness of
CE-RCFR from the following three aspects. Firstly, we demon-
strate CE-RCFR is robust to data imbalance and outliers through
separately evaluating how our method deals with data imbalance
and mini-batch outliers in Fig. 4 and 5. Here, three methods are con-
sidered: (1) SOT denotes balanced OT with entropy regularization
(𝜖·⟨𝝅,log(𝝅)−1⟩) as in Eq. (6) generally solved by Sinkhorn algo-
rithm; (2) RDDR w/ ER denotes the Relaxed Distribution Discrepancy
Regularizer, a version with entropy regularization, which is used to
solve the optimal transport strategy; (3) RDDR w/o ER denotes the
Relaxed Distribution Discrepancy Regularizer, a version that does
not introduce entropy regularization, which is used to solve the
optimal transport strategy (our solution in Section 3.1). It can be
seen from Fig. 4 and 5 that SOT will produce false matches when the
data is unbalanced and contains outliers, which confirms that MSS
issues are widespread and difficult to handle. In comparison, both
versions of RDDR s can cope with label imbalance and mini-batchoutliers more or less robustly. This confirms the effectiveness of
RDDR in solving MSS problems. However, thanks to the discarding
entropy regularization, we can clearly observe that our RDDR w/o
ER can produce sparser matches, compared to RDDR w/ ER where a
point matches almost all points of the target. Using mirror gradi-
ent descent to solve RDDR w/o ER makes our method more robust
than RDDR w/ ER (e.g., when 𝜅=50in Fig. 4 and 𝜅=10in Fig.
5).Secondly, we verify CE-RCFR is robust to different mini-
batch sizes through evaluating the performances of our proposal
on batch size 𝑁={32,64,96,128}(see Fig. 6). Two observations are
available: (1) For the three methods (i.e., TarNet, CFR-MMD and
CFR-WASS) without handling MSS problem, their performances
are getting worse on PEHE, RMSE fand RMSE cas mini-batch size
becomes smaller. (2) While for the other three methods (i.e., ESCFR,
RDDR andCE-RCFR ) handling MSS problem, they exhibit a similar
pattern on different mini-batch sizes; furthermore, our RDDR and
CE-RCFR methods achieve optimal performances, confirming the
robustness of our proposal to different mini-batch sizes. Thirdly,
we verify that CE-RCFR is robust to different initial values of
𝝀0through evaluating the performance improvements based on
different initial values of 𝝀0(see Table 3). Obviously, no matter
what the initial value of 𝝀0is, our method can achieve outstanding
performance and is better than any other baseline (for the perfor-
mances of other baselines, please refer to Table 1). This proves that
3020CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) NDCG@10
 (b) F1@10
Figure 7: Parameter sensitivity w.r.t. 𝜅.
CEAcan make the method robustly converge to the global optimum
regardless of the initial weight value 𝝀0.
Hyper-parameters Sensitivity (RQ4). We finally evaluate the
performances of CE-RCFR influenced by hyper-parameters 𝜅in Eq.
(9) and initial 𝝀0. First of all, we vary the parameter 𝜅from 5 to 35
with a step size of 10 to search the best penalization strength and
then report the experiment results in Fig. 7. It can be seen from the
figure that the PEHE, RMSE 𝑓and RMSE 𝑐of our CE-RCFR method
all converge when 𝜅=25. Such a finding validates that relaxing
the mass-preserving constraint via RDDR could greatly improve the
ITE estimation; nevertheless, a too-small 𝜅is always deleterious
since we cannot ensure that the representations across treatment
groups are exactly aligned. Next, we initiate 𝝀0with different values,
and then report the performances after Pareto optimization on
different initial weights in Table 3. We can see from Table 3 that
CE-RCFR performs the overall best when the initial value of 𝝀0is
(2
6,1
6,3
6). Although the above initial value cannot guarantee the
best performances in all situations, the achieved performance is also
very close to the best one. Therefore, we initialize 𝝀0to(2
6,1
6,3
6).
5 Related Work
Current research addresses the issue of treatment selection bias for
counterfactual estimation by balancing the distributions between
treated and controlled groups. The approaches can be classified
into three categories based on the adopted balancing techniques.
The first category involves reweighting-based methods. These
methods assign weights to individuals to create balanced distribu-
tions across different groups. Prevalent methods include the inverse
propensity score (IPS) method, originally introduced by [ 46], and
its enhancement through a doubly robust approach as discussed by
[45]. While the authors in [ 16,21] suggested the calculation of a bal-
ancing score through solving an optimization problem. Moreover,
[25,26] enhanced the treatment effect estimation by incorporat-
ing adjustment variables into their analysis of covariates. Despite
their effectiveness, these reweighting-based methods often face
challenges with units that do not overlap between groups, leading
to high variance issues in their results.
The second category includes those methods based on the match-
ing aligning similar units from distinct groups to form locally bal-
anced distributions. These methods, primarily vary in similarity
measurement, include the ones proposed by [ 7,34,46]. Addition-
ally, tree-based methods, such as those developed by [ 57], also
belong to this category due to their use of adaptive neighborhood
metrics. However, a significant drawback of these matching-based
approaches is their high computational demand, which poses chal-
lenges for their applications in large-scale datasets.The third category contains representation-based methods that
aim to minimize the discrepancies between groups in latent space.
Beginning with the Balancing Neural Network (BNN) introduced in
[22] and the Counterfactual Regression (CFR) in [ 50], this type of
methodologies has gained prominence for balancing distributions.
[62] and [ 19] further enhanced CFR by incorporating local similar-
ity and underlying factors, respectively. Additionally, the authors
in [3,23] employed adversarial training to balance the distribu-
tions of representations. Furthermore, [ 58] minimized Sinkhorn
discrepancy with an adjusted cost matrix for balancing. Due to
their scalability and effective avoidance of high variance issues,
representation-based methods have become increasingly dominant
in addressing treatment selection bias.
It is important to clarify how our approach differs from recent
Optimal Transport (OT)-based causal inference methods. Literature
[15,33] enhanced the IPS method by integrating a propensity score
estimator derived from OT. Despite this advancement, it still en-
counters the high variance problem mentioned earlier. In [ 53], the
authors employed the pushforward operator to refine change-in-
change models, yet their method is tailored for multi-phase data,
which is not applicable in our context. Similarly, the work in [ 32]
concentrated on decomposing latent variables, diverging from our
primary focus. The authors in [ 58] have put forward a research
framework similar to ours; however, they rely on entropy regular-
ization to solve Sinkhorn divergence, resulting in dense solutions
that lead to weaker performance. Furthermore, they overlook the
issue of optimization inconsistency in the backbone network caused
by distribution skew. In this paper, we address the challenge from
two perspectives: robust representation alignment for the MSS is-
sue and accurate representation optimization for the IRL problem.
We originally integrate CFR to the Unbalanced OT problem without
entropy regularization, along with multi-objective optimization, to
achieve accurate and robust treatment effect estimation.
6 Conclusion
Estimating the effects of treatments on individuals using observa-
tional data is difficult because we lack counterfactuals and face
biases in treatment selection. Common methods for estimating in-
dividual treatment effects (ITE) tackle these problems by matching
the distributions of treated and controlled groups in a representa-
tional space. Yet, two significant concerns have consistently been
neglected, i.e., Mini-batch sampling sensitivity (MSS) issue and In-
consistent representation learning (IRL) issue. To tackle these issues,
we propose a robust counterfactual regression framework named
CE-RCFR for consensus-enabled causal effect estimation, which in-
cludes a relaxed distribution discrepancy regularizer ( RDDR ) module
and a consensus-enabled aggregator ( CEA) module that can address
the above two challenging issues, respectively. Comprehensive tests
carried out on two popular semi-synthetic benchmark datasets,
namely IHDP and ACIC, show that CE-RCFR outperforms current
leading methods in terms of effectiveness and robustness.
7 Acknowledgments
This work was supported in part by the National Natural Science
Foundation of China (No. 72192823), the “Ten Thousand Talents Pro-
gram” of Zhejiang Province for Leading Experts (No. 2021R52001).
3021KDD ’24, August 25–29, 2024, Barcelona, Spain Fan Wang et al.
References
[1]Ehsan Amid and Manfred KK Warmuth. 2020. Reparameterizing mirror descent
as gradient descent. Advances in Neural Information Processing Systems 33 (2020),
8430–8439.
[2]Weijie Liu and Chao Zhang and Nenggan Zheng and Hui Qian. 2021. Approximat-
ing optimal transport via low-rank and sparse factorization. In Neural Information
Processing Systems.
[3]Jinsung Yoon and James Jordon and Mihaela van der Schaar. 2018. GANITE:
estimation of individualized treatment effects using generative adversarial nets.
InICLR.
[4]Peter C. Austin. 2011. An introduction to propensity score methods for reduc-
ing the effects of confounding in observational studies. Multivariate behavioral
research 46, 3 (2011), 399–424.
[5]Mathieu Blondel, Vivien Seguy, and Antoine Rolet. 2018. Smooth and sparse
optimal transport. In International conference on artificial intelligence and statistics.
PMLR, 880–889.
[6]Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich.
2011. Displacement interpolation using Lagrangian mass transport. ACM Trans-
actions on Graphics 30, 6 (2011), 158.
[7]Yale Chang and Jennifer Dy. 2017. Informative subspace learning for counterfac-
tual inference. In AAAI. 1770–1776.
[8]Laetitia Chapel, Rémi Flamary, Haoran Wu, Cédric Févotte, and Gilles Gasso.
2021. Unbalanced Optimal Transport through Non-negative Penalized Linear
Regression. In Advances in Neural Information Processing Systems.
[9]Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan
He. 2023. Bias and debias in recommender system: A survey and future directions.
ACM Transactions on Information Systems 41, 3 (2023), 1–39.
[10] Mingyuan Cheng, Xinru Liao, Quan Liu, Bin Ma, Jian Xu, and Bo Zheng. 2022.
Learning disentangled representations for counterfactual regression via mutual
information minimization. In SIGIR. 1802–1806.
[11] José M. Cordero, Víctor Cristóbal, and Daniel Santín. 2018. Causal Inference on
Education Policies: A Survey of Empirical Studies Using PISA, TIMSS and PIRLS.
J. Econ. Surv. 32, 3 (2018), 878–915.
[12] Richard K. Crump, V. Joseph Hotz, Guido W. Imbens, and Oscar A. Mitnik. 2008.
Nonparametric tests for treatment effect heterogeneity. Rev. Econ. Stat. 90, 3
(2008), 389–405.
[13] Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal
transport. In Advances in Neural Information Processing Systems. 2292–2300.
[14] Marco Cuturi and Arnaud Doucet. 2014. Fast computation of Wasserstein barycen-
ters. In Proceedings of The 31st International Conference on Machine Learning.
685–693.
[15] Eric Dunipace. 2021. Optimal transport weights for causal inference. CoRR
abs/2109.01991 (2021).
[16] Christian Fong, Chad Hazlett, and Kosuke Imai. 2018. Covariate balancing propen-
sity score for a continuous treatment: Application to the efficacy of political
advertisements. Ann. Appl. Stat. 12, 1 (2018), 156–177.
[17] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and
Alexander Smola. 2012. A kernel two-sample test. J. Mach. Learn. Res. 13 (Mar
2012), 723–773.
[18] Negar Hassanpour and Russell Greiner. 2019. Learning disentangled represen-
tations for counterfactual regression. In International Conference on Learning
Representations.
[19] Negar Hassanpour and Russell Greiner. 2020. Learning disentangled representa-
tions for counterfactual regression. In ICLR.
[20] Jennifer L. Hill. 2011. Bayesian nonparametric modeling for causal inference.
Journal of Computational and Graphical Statistics 20, 1 (2011), 217–240.
[21] Kosuke Imai and Marc Ratkovic. 2014. Covariate balancing propensity score. J.
R. Stat. Soc. Series. B. Stat. Methodol. 76, 1 (2014), 243–263.
[22] Fredrik Johansson, Uri Shalit, and David Sontag. 2016. Learning representations
for counterfactual inference. In ICML. 3020–3029.
[23] Nathan Kallus. 2020. Deepmatch: Balancing deep covariate representations for
causal inference using adversarial training. In ICML, Vol. 119. 5067–5077.
[24] Leonid V. Kantorovich. 2006. On the translocation of masses. J. Math. Sci. 133, 4
(2006), 1381–1382.
[25] Kun Kuang, Peng Cui, Bo Li, Meng Jiang, and Shiqiang Yang. 2017. Estimating
treatment effect in the wild via differentiated confounder balancing. In SIGKDD.
265–274.
[26] Kun Kuang, Peng Cui, Bo Li, Meng Jiang, Shiqiang Yang, and Fei Wang. 2017.
Treatment effect estimation with data-driven variable decomposition. In AAAI.
140–146.
[27] Sören R. Künzel, Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. 2019. Metalearners
for estimating heterogeneous treatment effects using machine learning. Proc.
Natl. Acad. Sci. U. S. A. 116, 10 (2019), 4156–4165.
[28] Haoxuan Li, Yan Lyu, Chunyuan Zheng, and Peng Wu. 2023. TDR-CL: Tar-
geted Doubly Robust Collaborative Learning for Debiased Recommendations. In
International Conference on Learning Representations.[29] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. 2023.
Propensity matters: Measuring and enhancing balancing for recommendation.
InInternational Conference on Machine Learning. PMLR, 20182–20194.
[30] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, Zhi Geng, Xu Chen,
and Peng Cui. 2024. Debiased Collaborative Filtering with Kernel-Based Causal
Balancing. In International Conference on Learning Representations.
[31] Haoxuan Li, Chunyuan Zheng, and Peng Wu. 2023. StableDR: Stabilized Dou-
bly Robust Learning for Recommendation on Data Missing Not at Random. In
International Conference on Learning Representations.
[32] Qian Li, Zhichao Wang, Shaowu Liu, Gang Li, and Guandong Xu. 2022. Deep
treatment adaptive network for causal inference. VLDBJ (2022), 1–16.
[33] Qian Li, Zhichao Wang, Shaowu Liu, Gang Li, and Guandong Xu. 2023. Causal
Optimal Transport for Treatment Effect Estimation. IEEE Transactions on Neural
Networks and Learning Systems 34, 8 (2023), 4083–4095.
[34] Sheng Li, Nikos Vlassis, Jaya Kawale, and Yun Fu. 2016. Matching via dimension-
ality reduction for estimation of treatment effects in digital marketing campaigns.
InIJCAI. 3768–3774.
[35] Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu,
Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, and Yanchao Tan. 2024. Re-
thinking the Representation in Federated Unsupervised Learning with Non-IID
Data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 22841–22850.
[36] Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Huabin Zhu, Yan-
chao Tan, Jun Wang, and Yue Qi. 2023. HyperFed: hyperbolic prototypes explo-
ration with consistent aggregation for non-IID data in federated learning. arXiv
preprint arXiv:2307.14384 (2023).
[37] Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Yanchao Tan, Fan
Wang, Xiaolin Zheng, and Yew Soon Ong. 2024. Learning Accurate and Bidirec-
tional Transformation via Dynamic Embedding Transportation for Cross-Domain
Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 38. 8815–8823.
[38] Weiming Liu, Xiaolin Zheng, Chaochao Chen, Jiajie Su, Xinting Liao, Mengling
Hu, and Yanchao Tan. 2023. Joint internal multi-interest exploration and external
domain alignment for cross domain sequential recommendation. In Proceedings
of the ACM Web Conference 2023. 383–394.
[39] Weiming Liu, Xiaolin Zheng, Mengling Hu, and Chaochao Chen. 2022. Collabo-
rative Filtering with Attribution Alignment for Review-based Non-overlapped
Cross Domain Recommendation. In Proceedings of the ACM Web Conference 2022.
1181–1190.
[40] Weiming Liu, Xiaolin Zheng, Jiajie Su, Mengling Hu, Yanchao Tan, and Chaochao
Chen. 2022. Exploiting Variational Domain-Invariant User Embedding for Par-
tially Overlapped Cross Domain Recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 312–321.
[41] Yuwen Liu, Lianyong Qi, Weiming Liu, Xiaolong Xu, Xuyun Zhang, and Wanchun
Dou. 2024. GraphSAGE-based POI Recommendation via Continuous-Time Mod-
eling. In Companion Proceedings of the ACM on Web Conference 2024. 585–588.
[42] Yuwen Liu, Huiping Wu, Khosro Rezaee, Mohammad R Khosravi,
Osamah Ibrahim Khalaf, Arif Ali Khan, Dharavath Ramesh, and Liany-
ong Qi. 2022. Interaction-enhanced and time-aware graph convolutional network
for successive point-of-interest recommendation in traveling enterprises. IEEE
Transactions on Industrial Informatics 19, 1 (2022), 635–643.
[43] Jing Ma, Mengting Wan, Longqi Yang, Jundong Li, Brent Hecht, and Jaime Teevan.
2022. Learning causal effects on hypergraphs. In Proceedings of the ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 1202–1212.
[44] Alfred Müller. 1997. Integral probability metrics and their generating classes of
functions. Advances in Applied Probability (1997), 429–443.
[45] James M. Robins, Andrea Rotnitzky, and Lueping Zhao. 1994. Estimation of
regression coefficients when some regressors are not always observed. J. Am.
Stat. Assoc. 89, 427 (1994), 846–866.
[46] Paul R. Rosenbaum and Donald B. Rubin. 1983. The central role of the propensity
score in observational studies for causal effects. Biometrika 70, 1 (1983), 41–55.
[47] Donald B. Rubin. 1974. Estimating causal effects of treatments in randomized
and nonrandomized studies. Journal of educational Psychology 66, 5 (1974), 688.
[48] Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M. Buhmann, and Walter
Karlen. 2020. Learning Counterfactual Representations for Estimating Individual
Dose-Response Curves. In AAAI. 5612–5619.
[49] Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective
optimization. Advances in neural information processing systems 31 (2018).
[50] Uri Shalit, Fredrik D. Johansson, and David Sontag. 2017. Estimating individual
treatment effect: generalization bounds and algorithms. In ICML. 3076–3085.
[51] Richard Dennis Sinkhorn and Paul Joseph Knopp. 1967. Concerning nonnegative
matrices and doubly stochastic matrices. Pacific J. Math. 21, 2 (1967), 343–348.
[52] Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf,
and Gert R. G. Lanckriet. 2012. On the empirical estimation of integral probability
metrics. Electronic Journal of Statistics 6 (2012), 1550–1599.
[53] William Torous, Florian Gunsilius, and Philippe Rigollet. 2021. An optimal
transport approach to causal inference. CoRR abs/2108.05858 (2021).
3022CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation KDD ’24, August 25–29, 2024, Barcelona, Spain
[54] Ha Xuan Tran, Thuc Duy Le, Jiuyong Li, Lin Liu, Jixue Liu, Yanchang Zhao, and
Tony Waters. 2021. Recommending the Most Effective Intervention to Improve
Employment for Job Seekers with Disability. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining. 3616–3626.
[55] Ha Xuan Tran, Thuc Duy Le, Jiuyong Li, Lin Liu, Jixue Liu, Yanchang Zhao,
and Tony Waters. 2022. What is the Most Effective Intervention to Increase Job
Retention for this Disabled Worker?. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3981–3991.
[56] Cédric Villani. 2008. Optimal transport: old and new. Vol. 338. Springer Science &
Business Media.
[57] Stefan Wager and Susan Athey. 2018. Estimation and inference of heteroge-
neous treatment effects using random forests. J. Am. Stat. Assoc. 113, 523 (2018),
1228–1242.
[58] Hao Wang, Zhichao Chen, Jiajun Fan, Haoxuan Li, Tianqiao Liu, Weiming Liu,
Quanyu Dai, Yichao Wang, Zhenhua Dong, and Ruiming Tang. 2023. Optimal
Transport for Treatment Effect Estimation. In Thirty-seventh Conference on NeuralInformation Processing Systems.
[59] Haotian Wang, Kun Kuang, Long Lan, Zige Wang, Wanrong Huang, Fei Wu,
and Wenjing Yang. 2023. Out-of-distribution generalization with causal feature
separation. TKDE (2023).
[60] Wenjie Wang, Yang Zhang, Haoxuan Li, Peng Wu, Fuli Feng, and Xiangnan He.
2023. Causal Recommendation: Progresses and Future Directions (SIGIR ’23).
3432–3435.
[61] Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Bo Li, and Fei Wu. 2023. Stable esti-
mation of heterogeneous treatment effects. In ICML, Vol. 202. 37496–37510.
[62] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018.
Representation learning for treatment effect estimation from observational data.
InNeurIPS. 2638–2648.
[63] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2019.
ACE: Adaptively Similarity-Preserved Representation Learning for Individual
Treatment Effect Estimation. In 2019 IEEE International Conference on Data Mining
(ICDM). 1432–1437. https://doi.org/10.1109/ICDM.2019.00186
3023