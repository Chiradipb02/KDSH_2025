Robust Predictions with Ambiguous Time Delays: A Bootstrap
Strategy
Jiajie Wang
Changsha Research Institute of
Mining and Metallurgy
Changsha, China
wangjj6@minmetals.comZhiyuan Jerry Lin∗
Meta
Menlo Park, USA
zylin@meta.comWen Chen†
Changsha Research Institute of
Mining and Metallurgy
Changsha, China
wenchen@minmetals.com
ABSTRACT
In contemporary data-driven environments, the generation and pro-
cessing of multivariate time series data is an omnipresent challenge,
often complicated by time delays between different time series.
These delays, originating from a multitude of sources like varying
data transmission dynamics, sensor interferences, and environmen-
tal changes, introduce significant complexities. Traditional Time
Delay Estimation methods, which typically assume a fixed constant
time delay, may not fully capture these variabilities, compromising
the precision of predictive models in diverse settings.
To address this issue, we introduce the Time Series Model Boot-
strap (TSMB), a versatile framework designed to handle poten-
tially varying or even nondeterministic time delays in time series
modeling. Contrary to traditional approaches that hinge on the
assumption of a single, consistent time delay, TSMB adopts a non-
parametric stance, acknowledging and incorporating time delay
uncertainties. TSMB significantly bolsters the performance of mod-
els that are trained and make predictions using this framework,
making it highly suitable for a wide range of dynamic and inter-
connected data environments.
Our comprehensive evaluations, conducted on real-world datasets
with different types of time delays, confirm the adaptability and
effectiveness of TSMB in multiple contexts. These include, but are
not limited to, power and occupancy forecasting in intelligent in-
frastructures, air quality monitoring, and intricate processes like
mineral processing. Further diagnostic analyses strengthen the case
for the TSMB estimator’s robustness, underlining its significance
in scenarios where ambiguity in time delays can have a significant
impact on the predictive task.
CCS CONCEPTS
•Computing methodologies →Machine learning algorithms ;
Machine learning approaches; •Mathematics of computing
→Nonparametric statistics; •Information systems →Data
mining .
∗The work is performed during the author’s personal capacity.
†Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671920KEYWORDS
Time Delay Estimation, Predictive Modeling, Bootstrap, Resam-
pling, Time Series
ACM Reference Format:
Jiajie Wang, Zhiyuan Jerry Lin, and Wen Chen. 2024. Robust Predictions
with Ambiguous Time Delays: A Bootstrap Strategy. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671920
1 INTRODUCTION
In today’s rapidly evolving data-centric landscape, a myriad of de-
vices and sensors are seamlessly integrated, creating a vast and in-
tricate system that continuously generates multivariate time series
data. This rich data source is crucial for advancing data mining ap-
plications, enhancing predictive modeling, and refining data-driven
decision-making processes across various fields, as underlined by
contemporary research [7, 31].
However, when employing this data for predictive analysis, prac-
titioners encounter a significant hurdle: time delays in data col-
lection and transmission. These delays are not confined to any
specific domain but are a widespread challenge affecting numerous
sectors. For instance, in urban monitoring systems, sensors dissem-
inated throughout a city to measure air quality might report data
at different times due to diverse factors like transmission paths or
environmental disturbances. This results in data from one sensor
arriving with a delay relative to another, complicating real-time air
quality predictions for a given area. These discrepancies, inherent
in the sequential and intricate nature of modern industrial, urban,
and technological environments, render the raw time series data
more complex and challenging to analyze directly.
To elucidate this phenomenon, consider a hypothetical smart
city environment depicted in Figure 1: Imagine a smart city scenario
where sensors A and B are deployed to monitor the traffic flow and
air quality respectively. Sensor A captures the volume of vehicles
at a particular intersection, while Sensor B measures the speed of
wind in a nearby park. The data captured by these sensors starting
at time𝑡over a span of time 𝑚can be represented as two vectors:
𝒙𝐴(𝑡)=[𝑥𝐴
𝑡,𝑥𝐴
𝑡+1,...,𝑥𝐴
𝑡+𝑚−1]𝑇
𝒙𝐵(𝑡)=[𝑥𝐵
𝑡,𝑥𝐵
𝑡+1,...,𝑥𝐵
𝑡+𝑚−1]𝑇.
We are interested in monitoring and predicting the air quality of a
residential area in the neighborhood for the same period of time,
denoted as
𝒚(𝑡)=[𝑦𝑡,𝑦𝑡+1,...,𝑦𝑡+𝑚−1]𝑇.
3104
KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
Figure 1: An illustration of the challenge of time delay estima-
tion in multivariate time series. In this scenario, 𝜹={𝛿𝐴,𝛿𝐵}
are well-defined and have unique values, which isn’t always
the case in real-world applications involving unpredictable
events and fluctuating noise.
Given the dynamic nature of urban environments, different wind
conditions and events at the intersection, such as a traffic jam,
can affect air quality after a certain delay due to the dispersion of
pollutants. Directly utilizing the raw data from sensors A and B
to predict 𝒚(𝑡)can lead to inaccuracies, given the inherent delay
between the traffic situation and its subsequent impact on air quality.
By adjusting the data streams to
𝒙𝐴(𝑡+𝛿𝐴)=[𝑥𝐴
𝑡+𝛿𝐴,𝑥𝐴
𝑡+𝛿𝐴+1,...,𝑥𝐴
𝑡+𝛿𝐴+𝑚−1]𝑇
𝒙𝐵(𝑡+𝛿𝐵)=[𝑥𝐵
𝑡+𝛿𝐵,𝑥𝐵
𝑡+𝛿𝐵+1,...,𝑥𝐵
𝑡+𝛿𝐵+𝑚−1]𝑇,
we ensure proper alignment and synchronization of the sensor
readings. Thus, a predictive model can then formulate 𝑦𝑡more
accurately based on the time-aligned readings from both sensors:
ˆ𝑦𝑡=𝑓(𝑥𝐴
𝑡+𝛿𝐴,𝑥𝐵
𝑡+𝛿𝐵).
Estimating such delays, termed the time delay estimation (TDE)
problem [ 5,12,24], is pivotal and there is a rich collection of lit-
erature, which we discuss in Section 2. Existing TDE techniques
typically select the time delay vector 𝜹to be the one maximiz-
ing a specific score function, such as cross-correlation or mutual
information [20, 24, 34, 42].
However, one fundamental assumption most existing TDE meth-
ods rely on is that there exists an unique, constant-valued time
delay, which often does not hold in many real-world applications.
Taking our motivating air quality prediction task as an example:
during peak traffic hours, the delay between increased vehicle emis-
sions and reduced air quality might vary depending on wind speed
and direction, which itself is subject to sudden changes due to
weather patterns. Similarly, the wind speed measured in the park
does not consistently influence air quality in the same manner; its
impact can be delayed or altered by urban structures, green spaces,
and atmospheric conditions. These varying delays mean that the
time lag between cause (status of the pollutants reflected by sensor
readings) and effect (air quality) is not fixed but fluctuates in a
manner that traditional time delay estimation methods assuming
deterministic delays struggle to accurately capture. Consequently,
accurately predicting air quality requires an approach capable ofadapting to the inherently unpredictable nature of these time de-
lays, emphasizing the need for new modeling techniques that can
account for such variability.
The situation becomes even more complicated when the end goal
is not to estimate the time delay per se, but to use this information to
construct accurate predictive models (e.g., for air quality prediction).
Even if one is able to obtain a probabilistic representation of the
time delay — a challenge already faced by classic TDE methods
— it is not immediately clear how we can use this information
to improve off-the-shelf machine learning models’ performance
on this prediction task without making assumptions about the
predictive models’ architectures. While TDE plays a pivotal role
in multivariate time series modeling, it is crucial to underscore
that in many applications, the overarching goal is to maximize the
performance of downstream predictive tasks. The actual time delays,
although important for model alignment, are often secondary in
significance. The precise formulation of this problem is detailed in
Section 3.2.
In this work, we present a novel framework Time Series Model
Bootstrap (TSMB) to help with multivariate time series modeling
in the presence of potentially non-deterministic time delays. TSMB
does not require explicit assumptions about the nature of these
delays and is designed to integrate seamlessly with any black-box
predictive model, offering straightforward implementation and an
inherent statistical interpretation.
Finally, it is worth noting that while some classical time series
models [ 6] and more recent neural network models [ 18,30,46,48]
in theory have the capacity of learning and incorporating time delay
information automatically, directly accounting for the (potentially
stochastic) time delay, such as our proposed method TSMB, can
further bolster their predictive performance as we later show in
Section 4.
In summary, in this paper we spotlight the often-overlooked
issue of possibly non-deterministic time delays in misaligned multi-
variate time series modeling. Our approach not only addresses the
complexities associated with non-deterministic time delays but also
demonstrates that effectively managing these delays can signifi-
cantly enhance models’ predictive performance. We then introduce
TSMB, Time Series Model Bootstrap, an innovative framework
adept at handling both stochastic and fixed time delays prevalent
in real-world datasets, without making explicit assumptions about
the form of such time delays. Designed to work with any black-box
predictive model, TSMB can be easily implemented while having a
natural statistical interpretation. Finally, we empirically showcase
TSMB’s superior performance over classic TDE methods across a
range of real-world predictive tasks with various time delay types
using both time-series transformer and classical machine learning
models. Additionally, we delve deep into TSMB’s characteristics,
shedding light on aspects like prediction coverage and the nuances
of time delay estimation.
2 RELATED WORK
Time delay estimation Time series analysis is a classic field of
study in data mining, statistical learning, and data analysis. Time
delay estimation, or TDE, is a problem of estimating the relative
time difference between different streams of signals, often being
3105Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy KDD ’24, August 25–29, 2024, Barcelona, Spain.
presented in the form of multivariate time series. TDE is widely
used in multivariate time series datasets with sequential structures
between time series such as ones from industrial processes, seismol-
ogy, acoustics, and communication. The most popular TDE method
is the generalized cross-correlation (GCC), originally proposed by
Knapp and Carter [24], whose core idea is to identify a time delay
to maximize the cross-correlation between two time series. This
method has been extensively studied and proven to work well in
reasonably noisy environments [ 10,21] and has been extended, for
example, to adapt special noise types such as reverberation in acous-
tics [ 4,11]. Another similar idea is to use time-delayed mutual infor-
mation (TDMI) [1,34], instead of cross correlation, which is expected
to perform better in non-linear systems. In practice, mutual informa-
tion can be estimated non-parametrically using k-nearest neighbor
distances [ 25,44]. When more than two time series are present,
instead of estimating the time delay separately for each time series,
methods using joint mutual information (also known as non-mutual
information methods) are employed [ 20,42,45]. There are other
methods using PCA [ 13], random walk [ 39], and Wasserstein dis-
tance [ 38]. Despite these methods’ claimed improved performance,
many of them are computationally expensive and prohibitively
slow when there are more than a few time delays to be estimated.
Generalized cross-correlation and mutual information remain the
most widely used and proven to be robust among other methods.
Additionally, TDE is also known as time delay identification or time
delay signature extraction in the field of communication and secu-
rity [ 43], and relatedly, its reverse problem time delay concealing
is also being extensively studied [17, 19, 28, 51].
Sequence alignment Separately, sequence alignment is another
related stream of research widely used and studied in areas like
bioinformatics. Dynamic programming algorithms such as dynamic
time warping (DTW) [ 3,36] and the Needleman-Wunsch algo-
rithm [ 29] are often used to calculate global alignment while local
alignment algorithms like Smith-Waterman algorithm [ 50] can be
used for more efficient alignment at the expense of potentially
sub-optimal matching. However, sequence alignment algorithms
usually seek for exact matching between two sequences, and in the
problem concerned in this paper, such requirement is generally not
met, rendering this class of methods not applicable here. There are
also tensor-completion-based methods such as Qian et al . [41] and
Liu et al . [32] applicable to spatial-temporal data, but their aim is
to rectify missing or inaccurate data, which is not the setting this
paper is focusing on.
Time Series Bootstrap Bootstrap is a resampling method that
approximates the true population’s distribution using random sam-
pling with replacement. Bootstrap is often used to quantify uncer-
tainty around sample estimates. One implicit assumption bootstrap
methods make is that the sampled data is identically and indepen-
dently distributed (IID), which is obviously violated when it comes
to time series data. To overcome this problem, several methods are
commonly used to perform bootstrap on time series data. Possibly
the most widely used time series bootstrap method is block boot-
strap [ 26,27], whose central idea is to sample continuous segments
(“blocks”) of the time series with replacement. This method will
maintain local sequential dependency but not necessarily global
structure. Another popular choice is sieve bootstrap [ 2,8]. Insteadof resampling the data itself, new data is generated using an autore-
gressive model and individual residuals are sampled from the data.
Other approaches and variants include the wild bootstrap [ 49] and
stationary bootstrap [40].
3 TIME SERIES MODEL BOOTSTRAP
In this section, we formally present the problem we study in this
paper. We then describe our proposed method TSMB in detail.
3.1 Problem Definition
The fundamental problem we are concerned about in this paper is
multivariate time series modeling with (potentially stochastically)
misaligned time series data. To formally define the problem, assume
we observe a dataset D={X,𝒚}consisting of a feature matrix X
and a target vector of interest 𝒚. Vector 𝒚=[𝑦𝑡,𝑦𝑡+1...,𝑦𝑡+𝑚−1]𝑇
has a starting time 𝑡and length of 𝑚. Examples of 𝒚include whether
a room is currently occupied in a smart home setting (a classification
task) or the particle size of the concentrate in mineral processing (a
regression task). The feature matrix X=[𝒙1,𝒙2,..., 𝒙𝑛]represents
other observed multivariate time series data where each individual
time series 𝒙𝑖=[𝑥𝑖
0,𝑥𝑖
2,...,𝑥𝑖
𝑀−1]𝑇is a sequence of recorded vari-
ables (e.g., measurements over time from a sensor) for some integer
𝑀≥𝑚. Each 𝒙𝑖inXmay be misaligned with the target vector 𝒚
differently. While classic TDE methods assume such misalignment
is𝒙𝑖being shifted by a fixed value, in practice, 𝒙𝑖can be misaligned
with 𝒚in arbitrary ways. The goal here is to construct a predictive
model trained onDthat can accurately predict out-of-sample 𝒚′
with corresponding X′.
In a well-behaved misaligned time series data, there are (unob-
served) delays 𝜹={𝛿1,𝛿2,...,𝛿𝑛}between each 𝒙𝑖and𝒚such
that element 𝑥𝑖
𝑡+𝛿𝑖would correspond to element 𝑦𝑡in perfectly
aligned data. For example, 𝒙𝑖could be a grinding mill’s power
at the beginning of a mineral processing workflow and 𝑦𝑡is the
particle size of the concentrate (i.e., the granularity of the refined
mineral of interest) measured at the end of the workflow. With-
out loss of generality, it is safe to assume 0≤𝑡<𝑡+𝑚≤𝑀
and𝛿𝑖,𝑡,𝑚,𝑀∈Z+where Z+is the set of all positive integers.
Then given a family of machine learning models, we would like to
construct a time-aligned dataset D𝜹={X(𝜹),𝒚}and a predictive
model𝑓𝜹such that it can accurately and robustly predict some
future 𝒚′. With traditional TDE methods, matrix X(𝜹)is typically
constructed asX(𝜹=ˆ𝜹)=[𝒙1(𝑡+ˆ𝛿1),𝒙2(𝑡+ˆ𝛿2),..., 𝒙𝑛(𝑡+ˆ𝛿𝑛)]
where ˆ𝜹={ˆ𝛿1,ˆ𝛿2,..., ˆ𝛿𝑛}is the inferred time delay vector and
𝒙𝑖(𝑡+ˆ𝛿𝑖)=[𝑥𝑖
𝑡+ˆ𝛿𝑖,𝑥𝑖
𝑡+ˆ𝛿𝑖+1...,𝑥𝑖
𝑡+ˆ𝛿𝑖+𝑚−1]𝑇is a shifted subsequence
of the original time series 𝒙𝑖correcting for the estimated time delay.
In classic TDE methods, ˆ𝜹is obtained by maximizing some score
function𝑆(Dˆ𝜹). Common choices of the score function 𝑆include
GCC and TDMI.
However, the time-varying noise in some data may undermine
our capability of accurately identifying the time delays using classic
TDE methods. More importantly, in many real-world problems
with complex dynamics and chaotic systems (e.g., smoke or ocean
turbulence in fluid mechanics or grinding processes in mineral
processing), instead of having a unique, identifiable value, the time
delay 𝜹may be a random variable itself. In these scenarios, both
3106KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
the time delay estimation and the downstream prediction modeling
task can become very challenging for classic TDE methods as their
fundamental assumption that there exists a unique time delay vector
𝜹is violated.
We note that although the problem formulation resembles the
classic time delay estimation problem, it differs in that the estima-
tion of time delay is not the end goal. Instead, in this problem we
callmisaligned multivariate time series modeling, we aim to use the
estimated time delay as auxiliary information to improve the per-
formance of the predictive model for some downstream prediction
task.
3.2 Method
Now we introduce our proposed framework Time Series Model
Bootstrap (TSMB), for misaligned multivariate time series modeling.
Model prediction obtained via this framework is referred to as the
TSMB estimator.
Most traditional TDE methods implicitly assume that there exists
a unique value of time delay 𝜹represented by its point estimate
ˆ𝜹. When we build a machine learning model with aligned dataset
Dˆ𝜹={X(ˆ𝜹),𝒚}, the model is technically estimating E[𝑌|𝑋,𝜹=ˆ𝜹]
instead of E[𝑌|𝑋], the typical desideratum of machine learning
tasks. This is a fine approximation when there does exist a unique
𝜹that we can accurately estimate. When this condition does not
hold, which is likely in many real-world scenarios with complex
dynamics, the prediction ˆ𝑦=𝑓ˆ𝜹(𝑥)=E[𝑌|𝑋=𝑥,𝜹=ˆ𝜹]may be a
biased estimator of the real estimand E[𝑌|𝑋]when evaluated on
new, out-of-sample data.
Algorithm 1 Time series model bootstrap (TSMB)
Require:D𝑡𝑟𝑎𝑖𝑛 ={X𝑡𝑟𝑎𝑖𝑛,𝒚𝑡𝑟𝑎𝑖𝑛}
Require:D𝑡𝑒𝑠𝑡={X𝑡𝑒𝑠𝑡,𝒚𝑡𝑒𝑠𝑡}
Require: Score function 𝑆
# Model training with TSMB
model_list←∅
td_list←∅
forb = 1..B do
Draw block bootstrap sample D𝑏fromD𝑡𝑟𝑎𝑖𝑛
ˆ𝜹𝑏=argmax𝜹𝑆(𝜹;D𝑏)
Fit𝑓ˆ𝜹𝑏on dataD={X𝑡𝑟𝑎𝑖𝑛(ˆ𝜹𝑏),𝒚𝑡𝑟𝑎𝑖𝑛}
td_list←td_list.append(ˆ𝜹𝑏)
model_list←model_list.append (𝑓ˆ𝜹𝑏)
# Making predictions with TSMB
𝒚𝑝𝑟𝑒𝑑←0
forb = 1..B do
ˆ𝜹𝑏←td_list[b]
𝑓ˆ𝜹𝑏←model_list[b]
𝒚𝑝𝑟𝑒𝑑,𝑏←𝑓ˆ𝜹𝑏(X𝑡𝑒𝑠𝑡(ˆ𝜹𝑏))
𝒚𝑝𝑟𝑒𝑑←𝒚𝑝𝑟𝑒𝑑+𝒚𝑝𝑟𝑒𝑑,𝑏
𝒚𝑝𝑟𝑒𝑑←𝒚𝑝𝑟𝑒𝑑/𝐵
return 𝒚𝑝𝑟𝑒𝑑
While we might not be able to pinpoint an exact time delay
value under the aforementioned scenarios, it is still possible totreat 𝜹as a random variable and describe its value as a probability
distribution. By obtaining a bootstrap sample D𝑏(e.g., via block
bootstrap [ 26,27]) from the original dataset D, we are able to
attain a sample of ˆ𝜹𝑏by maximizing the score function 𝑆on each
individual bootstrap dataset sample D𝑏.ˆ𝜹𝑏can be regarded as a
sample drawn from the bootstrap time delay distribution F𝐵
𝜹, which
in general can be treated to be a reasonable approximation of the
true underlying time delay distribution F𝜹, as is commonly assumed
in bootstrap methods. With a 𝐵bootstrap time delay samples, we
have the empirical bootstrap distribution of 𝜹. Recall that given a
fixed dataset, the predictive model is a function of 𝜹and we are able
to fit a model 𝑓ˆ𝜹𝑏(𝑥)to approximate E[𝑌|𝑋=𝑥,𝜹=ˆ𝜹𝑏]similar
to how we do predictive modeling with traditional TDE methods,
but with a bootstrap time delay ˆ𝜹𝑏. After fitting and obtaining
predictions from each of these bootstrapped models, we are able to
calculate our prediction ˆ𝑦by averaging over 𝐵different bootstrap
models. Algorithm 1 describes the pseudocode to fit models and
make predictions with TSMB.
By treating 𝜹as a random variable, obtaining the model predic-
tion by integrating out 𝜹is the optimal decision-theoretic approach.
Because we obtain model’s predictive samples based on time series
bootstrap, we refer to this method as Time Series Model Bootstrap,
or TSMB for brevity. In this work, we use block bootstrap as the
time series bootstrap method in this work, but other time series
bootstrap methods can be applied as well. Proposition 1 shows that
TSMB is a finite sample approximation of E[𝑌|𝑋=𝑥], the quantity
we typically estimate with machine learning models in the absence
of time delays.
Proposition 1. Assume the time delay 𝜹is a random variable
and𝑓𝜹(𝑥)=E[𝑌|𝑋=𝑥,𝜹]is the model prediction given a realized 𝜹,
the TSMB estimator is a finite sample approximation of E[𝑌|𝑋=𝑥].
Proof.
ˆ𝑦=E[𝑌|𝑋=𝑥]
=E𝜹[E[𝑌|𝑋=𝑥,𝜹]] by the law of total expectation
=∫
𝜹E[𝑌|𝑋=𝑥,𝜹]P(𝜹)𝑑𝜹
=lim
𝐵→∞1
𝐵𝐵∑︁
𝑏=1E[𝑌|𝑋=𝑥,ˆ𝜹𝑏]where ˆ𝜹𝑏∼F𝐵
𝜹
the bootstrap distribution (1)
≈1
𝐵𝐵∑︁
𝑏=1E[𝑌|𝑋=𝑥,ˆ𝜹𝑏] (2)
=1
𝐵𝐵∑︁
𝑏=1𝑓ˆ𝜹𝑏(𝑥) the TSMB estimator
□
The approximation presented in Equation 1 and 2 is a common
assumption in bootstrap literature and is justified by the Dvoret-
zky–Kiefer–Wolfowitz inequality [ 35], which states that the empir-
ical distribution function converges uniformly to the true distribu-
tion function at exponential rate in probability.
3107Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy KDD ’24, August 25–29, 2024, Barcelona, Spain.
Dataset Size # Time Delays Time Delay Type Task
Occupancy - Fixed 20,560 5 Fixed Classification
Occupancy - Stochastic 20,560 5 Stochastic Classification
Water Pump Maintenance - Fixed 220,320 6 Fixed Classification
Water Pump Maintenance - Stochastic 220,320 6 Stochastic Classification
Power Demand - Fixed 1,096 24 Fixed Classification
Power Demand - Stochastic 1,096 24 Stochastic Classification
Air Quality - Fixed 9,357 8 Fixed Regression
Air Quality - Stochastic 9,357 8 Stochastic Regression
Mineral Processing 1.2M/994 ( 𝑿/𝒚) 10 Unknown Regression
Table 1: Overview of datasets used in the experiments. For the occupancy, water pump maintenance, and air quality datasets,
time delays are introduced in two ways: a consistent "fixed" delay and a "stochastic" delay where time series data is adjusted at
each timestamp based on random draws from a set of possible delays. The mineral processing dataset inherently has unknown
delays due to the intricacies in its processes. Further details on the injected time delays can be found in Table 4 in the appendix.
4 EXPERIMENT
In this section, we present our empirical evaluation of TSMB against
classic TDE methods on nine different real-world datasets. We then
perform diagnostic analysis to examine a number of properties
displayed by the TSMB estimator. The code to reproduce the result
is available at https://github.com/HenryWang-000/TSMB.
4.1 Experiment Setup
4.1.1 Datasets. We assess nine diverse real-world predictive tasks,
including six classification and three regression tasks. These datasets
originate from various domains where sensor data is prevalent: oc-
cupancy detection [ 9], water pump maintenance [ 37], air quality
monitoring [ 15], power demand prediction [ 14,23] and our pri-
vately collected mineral processing dataset.
Some of these datasets originally exhibit no apparent time delays.
We introduce time delays in two distinct manners to encapsulate dif-
ferent scenarios one might encounter in practical applications. We
first introduce fixed noises into these datasets by manually injecting
a consistent time delay for each variate (i.e., feature) to simulate en-
vironments where delays are predictable and consistent over time.
On the other hand, to represent more volatile and dynamic systems,
we introduce stochastic noises. In this case, at each time point, we
draw from a set of five possible time delays, adjusting the time
series data uniquely for each specific timestamp. By introducing
both constant and stochastic noises, we examine each method’s
robustness in handling both predictable and unpredictable time
delay scenarios.
Table 1 describes all datasets used in this work and Table 4 in
Appendix A details the exact time delays injected in each datasets.
Occupancy Detection: In smart environments, this dataset [ 9]
predicts room occupancy based on variables like temperature, hu-
midity, light, and 𝐶𝑂2. Time delays, both fixed and stochastic, are
introduced to simulate real-world variability in data acquisition
and processing.
Water Pump Maintenance: Sourced from a public dataset [ 37],
this dataset aims to determine the operational state of water pumps:
normal, recovering, or broken. The dataset is adjusted for inher-
ent imbalance by grouping “recovering” and “broken” instancestogether. The six most critical features are selected based on a
random forest model analysis. To reflect realistic operational con-
ditions, fixed and stochastic time delays are incorporated into the
data.
Italy Power Demand: This is from a public dataset [ 23] from the
UCR [ 14] dataset, this collection includes 24 variables of unknown
characteristics and time series data representing electricity demand
from Italy. Fixed and stochastic time delays are artificially injected
to this dataset to create two different experimental datasets.
Air Quality: Crucial for urban environment monitoring, this
dataset provides hourly readings from various air quality sensors.
The main goal is to predict CO concentrations based solely on
these sensor readings, eliminating the need for traditional ground-
truth validation methods. Both fixed and stochastic time delays are
integrated into the dataset to simulate real-time data processing
challenges.
Mineral Processing: This is our privately collected dataset.
Contrasting the other datasets where time delays are artificially im-
posed, the mineral processing dataset naturally presents unknown
time delays. The inherent unpredictability in procedures such as
grinding underscores the complexity: grinding mills process ore
particles over variable durations, releasing them based on specific
size thresholds. Determining exact grinding times for individual
particles is a considerable challenge, pointing to the presence of
stochastic time delays. This dataset, sourced from an operational
iron concentration mill, captures myriad sensor readings, including
ore feed rates and grinding mill outputs. The key predictive met-
ric is the hydrocyclone’s overflow particle size, indicative of the
mineral’s refinement quality.
4.1.2 Baselines. In our experiment, we benchmark the proposed
TSMB method against classic TDE methods, which rely on obtain-
ing a point estimate of the time delay. The classic TDE baselines we
consider are TDMI [ 1,34] and GCC [ 24,47]. For these baselines, we
first estimate the time delays with the corresponding score function,
align the time series accordingly, and then fit the predictive model.
They are labeled as TDMI and GCC in all figures. The proposed
TSMB methods are labeled as TSMB-TDMI andTSMB-GCC respec-
tively. In the Appendix, we have further discussed a few possible
3108KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
Method Occupancy Pump Maintenance Power Demand Air QualityMineral
Processing
Fixed Stochastic Fixed Stochastic Fixed Stochastic Fixed Stochastic
TSMB-TDMI (ours) 0.995 0.950 0.999 0.999 0.945 0.511 0.766 0.571 0.870
TSMB-GCC (ours) 0.995 0.929 0.999 0.999 0.987 0.525 0.708 0.549 0.871
TDMI 0.923 0.791 0.991 0.990 0.847 0.515 0.760 0.544 0.867
GCC 0.923 0.811 0.990 0.988 0.841 0.501 0.691 0.538 0.864
Real time delay 0.988 0.988 0.991 0.991 0.964 0.964 1.000 1.000 N/A
No Alignment 0.728 0.722 0.979 0.979 0.509 0.519 0.085 -0.106 0.860
Table 2: Absolute performance metrics ( 𝑅2for regression tasks and AUC for classification tasks) for selected methods. For a
comprehensive comparison including other variants of TSMB, refer to Table 5 in the appendix.
91%93%95%97%99%AUC
Occupancy Fixed
99.0%99.4%99.8%AUC
Water Pump Fixed
82%86%90%94%98%AUC
Power Demand Fixed
68%70%72%74%76%78%R2
Air Quality Fixed
86.3%86.5%86.7%86.9%87.1%R2
Mineral Processing
78%82%88%92%98%AUC
Occupancy Stoch.
98.6%99.0%99.4%99.8%AUC
Water Pump Stoch.
49.5%50.5%51.5%52.5%53.5%AUC
Power Demand Stoch.
54%55%56%57%58%R2
Air Quality Stoch.
TSMB-TDMI
TSMB-GCC
TDMI
GCC
Figure 2: Performance visualization of GBDT models applied
across different datasets, showcasing the efficacy of various
methods in handling time delays. Each point indicates the
AUC (for classification tasks) or 𝑅2(for regression tasks).
Across all datasets, TSMB methods consistently outperform
traditional TDE techniques like TDMI and GCC. Error bars
represent 95% CIs for TSMB-based methods and are generally
small.
variants of TSMB, where we demonstrate that while those variants
offers computational trade-offs at different levels and generally
perform better than traditional TDE methods, TSMB is still show-
ing the strongest predictive performance among methods we have
considered.
We emphasize that while time delay estimation is a crucial com-
ponent of the problem we consider, the focus of the problem we
discuss in this paper is the misaligned multivariate time series
predictive modeling problem, rather than the time delay estima-
tion itself. Therefore, when we evaluate the proposed method, we
benchmark on the final model’s (or a set of models’) predictive
performance measured in AUC or 𝑅2.4.1.3 Experiment Setup. Using the nine datasets summarized in
Section 4.1.1, we evaluate TSMB against traditional TDE-based
methods which rely on point estimates of the time delay.
Unless specified otherwise, we use a gradient boosted decision
tree (GBDT) with 100 trees as the base predictive model for all of
our experiments. We have additionally examined TSMB’s perfor-
mance when used with temporal fusion transformers (TFT) [ 30]
and those results are presented in Section 4.6, where we have ob-
served consistent patterns compared GBDT’s results, signaling the
robustness of TSMB regardless the choice of the base predictive
model.
We use DIRECT as the optimization algorithm [ 16,22] for all
time delay optimization. We smooth all datasets by applying mov-
ing averages to all 𝒙𝑖with a window size 𝑤for both time delay
estimation and model construction1. The moving average smooth-
ing leads to improved performance in all methods and as a result, is
used as a default setup in all experiments. Instead of using a fixed
𝑤, we optimized it jointly with 𝜹in all time delay optimization to
adapt to the distinctive characteristics of each dataset and method.
For all sampling-based methods, we use a sample size (e.g., 𝐵in
bootstrap-based methods) of 100. Despite that we use 𝐵=100, as
demonstrated in Section 4.3, similar performance can be achieved
with a significantly smaller 𝐵such as𝐵=5, mitigating the con-
cern around computational cost. We use block bootstrap [ 26,27]
with block size being 0.25 of the training data for all time series
bootstrap-based method. This proportion is small enough to al-
lows for enough variation in the resampled time series and is big
enough to retain enough structural information when calculating
TDMI or GCC. To evaluate predictive performance, we use AUC as
for classification tasks and 𝑅2for regression tasks so that evalua-
tion metrics are generally within the similar scale across different
datasets unlike other common metrics such as MSE and is insensi-
tive to class unbalance (for AUC). Unless specified otherwise, all
reported experiment results aggregated over five replications.
4.2 Predictive Performance
Our primary experimental results are presented in Figure 2 and
Table 2. Here, we focus on the relative performance gains achieved
by applying different time series alignment methods. Specifically,
1We have also examined performing the prediction without moving average and have
discovered that moving average boosts the performance of all methods, including
traditional TDE baselines.
3109Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy KDD ’24, August 25–29, 2024, Barcelona, Spain.
we use the AUC or 𝑅2as the evaluation metrics for classification
and regression tasks respectively.
Interestingly, for datasets where we have artificially introduced
time delays, the performance of TSMB methods sometimes sur-
passes that of models trained with the exact time delays we intro-
duce, as highlighted by the "real time delay" row in Table 2. This
suggests the presence of inherent time delays before our artificial
injection or that the existing time delays in the dataset are more
complex than can be captured by a singular point estimate. The
consistent outperformance of TSMB methods over traditional TDE
methods underlines the limitations of using a single point estimate
for predictive tasks, as is the norm with traditional TDE methods.
98.7%98.9%99.1%99.3%99.5%AUC
Occupancy Fixed
99.84%99.88%99.92%AUC
Water Pump Fixed
88%92%96%100%AUC
Power Demand Fixed
62%66%70%74%78%R2
Air Quality Fixed
86.0%86.4%86.8%87.2%R2
Mineral Processing
86%90%94%98%AUC
Occupancy Stoch.
1005020105199.80%99.84%99.88%99.92%AUC
Water Pump Stoch.
1005020105149%51%53%55%57%AUC
Power Demand Stoch.
1005020105150%52%54%56%58%R2
Air Quality Stoch.
TSMB-TDMI
TSMB-GCC
Figure 3: Ablation on bootstrap sample size 𝐵for TSMB. The
horizontal axis depicts the bootstrap sample size 𝐵.𝐵is min-
imally impacting the predictive performance of TSMB esti-
mators.
4.3 Choice of Bootstrap Sample Size
The choice of bootstrap sample size 𝐵is a determining factor of
TSMB’ performance and computational cost. By varying the number
of bootstrap sample size 𝐵, we show that even with a small number
of bootstrap samples such as 𝐵=5, we are still able to obtain
impressive predictive quality via an ablation study on the choice of
𝐵. In Figure 3, we observe that with the decreasing value of 𝐵, the
impact on predictive performance can be minimal. Compared to
𝐵=100, a very small 𝐵such as𝐵=5can still result in empirically
similar predictive performance for essentially all problems at a1
20
cost, significantly alleviating the computational demand of TSMB.
4.4 Prediction Coverage
The empirical distribution of model predictions by TSMB is the
approximated distribution of E[𝑌|𝑋=𝑥]under different 𝜹. One
way to examine the validity of this approximation is to check the
99.0%99.2%99.4%99.6%99.8%100.0%Coverage
Occupancy Fixed
99.96%99.97%99.98%99.99%100.00%Coverage
Water Pump Fixed
97%98%99%100%Coverage
Power Demand Fixed
50%60%70%80%90%100%Coverage
Air Quality Fixed
16%18%20%22%Coverage
Mineral Processing
98.4%98.8%99.2%99.6%100.0%Coverage
Occupancy Stoch.
95% 90% 80%
1-α99.96%99.97%99.98%99.99%Coverage
Water Pump Stoch.
95% 90% 80%
1-α86%90%94%98%Coverage
Power Demand Stoch.
95% 90% 80%
1-α30%40%50%60%70%Coverage
Air Quality Stoch.TSMB-TDMI
TSMB-GCCFigure 4: Bootstrap percentile 1−𝛼confidence interval cov-
erage under TSMB. For classification tasks where we only
observe binary values, we examine TSMB coverage using the
corresponding point estimates given by TDMI or GCC.
coverage of the model’s prediction. Plotted 95% confidence intervals
of the empirical coverage are obtained via 5 replications of the ex-
periment. By having an empirical bootstrap confidence interval at
1−𝛼level, the ground truth distribution would cover the observed
data approximately 1−𝛼of the time. Figure 4 shows the empirical
coverage of TSMB estimators on the test sets of all datasets. In some
dataset such as the occupancy and power demand dataset, the cov-
erage is sensible and close to the expected values. However, in some
other datasets, both TSMB-GCC and TSMB-TDMI produce overly
conservative credible intervals resulting in lower-than-expected
coverage, despite their excellent predictive capabilities as we ob-
served in Section 4.2. This result suggests that when we need to use
the estimated distribution produced by TSMB such as when doing
risk analysis, it is important to validate its coverage and it is useful
to further investigate how one could robustly obtain well-calibrated
TSMB credible intervals.
4.5 Bootstrap Distribution Diagnostic Analysis
While we are primarily interested in the improving predictive mod-
eling performance with TSMB, the predictions we obtain come from
time delay bootstrap samples. Therefore, it is also useful to under-
stand the characteristics of the time delay bootstrap distribution.
Figure 5 plots the empirical distribution of time delay bootstrap
samples (blue and orange histograms, normalized to be between
0 and 1) as well as the point estimates made by the classic TDMI
and GCC TDE methods (green and red lines). For datasets in which
we injected artificial time delays, the ground truth real time delays
are also plotted as grey lines. On many occasions, the time delays
estimated by TDMI and GCC (in red and green lines) are far from
3110KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
0.0 0.2 0.4 0.6 0.8 1.0020406080100CountFeature0 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature1 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature2 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature3 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature4 Fixed
TSMB-TDMI
TSMB-GCC
TDMI
GCC
True Value
0.0 0.2 0.4 0.6 0.8 1.0020406080CountFeature0 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature1 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature2 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature3 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Estimated TDFeature4 Stoch.
(
a) Occupancy time
0102030405060CountFeature0 Feature1 Feature2 Feature3 Feature4
0.0 0.2 0.4 0.6 0.8 1.00102030405060CountFeature5
0.0 0.2 0.4 0.6 0.8 1.0Feature6
0.0 0.2 0.4 0.6 0.8 1.0Feature7
0.0 0.2 0.4 0.6 0.8 1.0Feature8
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Estimated TDFeature9 (
b) Mineral processing
0.0 0.2 0.4 0.6 0.8020406080CountFeature0 Fixed
0.0 0.2 0.4 0.6 0.8Feature1 Fixed
0.0 0.2 0.4 0.6 0.8Feature2 Fixed
0.0 0.2 0.4 0.6 0.8Feature3 Fixed
0.0 0.2 0.4 0.6 0.8Feature4 Fixed
0.0 0.2 0.4 0.6 0.8
Normalized Estimated TDFeature5 Fixed
0.0 0.2 0.4 0.6 0.8 1.0020406080CountFeature0 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature1 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature2 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature3 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature4 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Estimated TDFeature5 Stoch.
(
c) Water pump maintenance
0.0 0.5 1.00255075CountFeature0 Fixed
0.0 0.5 1.0Feature1 Fixed
0.0 0.5 1.0Feature2 Fixed
0.0 0.5 1.0Feature3 Fixed
0.0 0.5 1.0Feature4 Fixed
0.0 0.5 1.0Feature5 Fixed
0.0 0.5 1.0Feature6 Fixed
0.0 0.5 1.0
Normalized Estimated TDFeature7 Fixed
0.00 0.25 0.50 0.750255075CountFeature0 Stoch.
0.00 0.25 0.50 0.75Feature1 Stoch.
0.00 0.25 0.50 0.75Feature2 Stoch.
0.00 0.25 0.50 0.75Feature3 Stoch.
0.00 0.25 0.50 0.75Feature4 Stoch.
0.00 0.25 0.50 0.75Feature5 Stoch.
0.00 0.25 0.50 0.75Feature6 Stoch.
0.00 0.25 0.50 0.75
Normalized Estimated TDFeature7 Stoch. (
d) Air quality
020406080100CountFeature0 Fixed Feature1 Fixed Feature2 Fixed Feature3 Fixed Feature4 Fixed Feature5 Fixed
020406080100CountFeature6 Fixed Feature7 Fixed Feature8 Fixed Feature9 Fixed Feature10 Fixed Feature11 Fixed
020406080100CountFeature12 Fixed Feature13 Fixed Feature14 Fixed Feature15 Fixed Feature16 Fixed Feature17 Fixed
0.0 0.2 0.4 0.6 0.8 1.0020406080100CountFeature18 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature19 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature20 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature21 Fixed
0.0 0.2 0.4 0.6 0.8 1.0Feature22 Fixed
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Estimated TDFeature23 Fixed
0204060CountFeature0 Stoch. Feature1 Stoch. Feature2 Stoch. Feature3 Stoch. Feature4 Stoch. Feature5 Stoch.
0204060CountFeature6 Stoch. Feature7 Stoch. Feature8 Stoch. Feature9 Stoch. Feature10 Stoch. Feature11 Stoch.
0204060CountFeature12 Stoch. Feature13 Stoch. Feature14 Stoch. Feature15 Stoch. Feature16 Stoch. Feature17 Stoch.
0.0 0.2 0.4 0.6 0.8 1.00204060CountFeature18 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature19 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature20 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature21 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0Feature22 Stoch.
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Estimated TDFeature23 Stoch.
(
e) Power demand
Figure 5: Bootstrap distribution of normalized estimated time delays using TDMI and GCC as the score function respectively.
Vertical lines represent the point estimates given by optimizing TDMI and GCC as well as the ground truth (when applicable)
time delays from each dataset. The fact that many of these distributions are not even close to point distirbution suggests that
there exists a significant amount of uncertainty in the estimated time delays that is being ignored by traditional TDE methods.
the ground truth time delays, confirming the motivating example
that certain types of data might pose significant challenges for esti-
mating time delays with traditional TDE methods. On the contrary,
instead of a point estimate, TSMB yields a spectrum of time delays
forming a distribution. Almost in all cases visualized here, both
the ground truth and traditional TDE estimated time delays fall
within the range of TSMB time delay distribution with non-trivial
density. This result implicates that the TSMB produces a coherent
distribution that can nicely explain both the ground truth time
delay as well as the estimated time delay by classic TDE methods in
that all these point estimates are essentially manifestations of the
underlying dataset, which itself can be viewed as a sample drawn
from a prior distribution of time series datasets.4.6 TSMB Results with Temporal Fusion
Transformer
Throughout our experiments, we have been using GBDT as the
base model in all of our evaluation as GBDT is generally well-
performing across a wide variety of tasks and TSMB is a model-
agnostic method. To examine how TSMB can contribute to more
complex deep models that can in theory implicitly account for time
delays, we have reproduced our main experimental results with the
Temporal Fusion Transformer (TFT) [30] model.
Table 3 shows the predictive performance of TFT when trained
using TSMB compared with other TDE baselines. Figure 6 shows
TSMB’s predictive performance with varying number of bootstrap
samples𝐵. Both results demonstrate TSMB is able to improve the
base model’s performance empirically even if the model (TFT) can
implicitly account for the time delays.
3111Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy KDD ’24, August 25–29, 2024, Barcelona, Spain.
Metho
d w/ TFT as base model Occupancy Pump Maintenance Power Demand Air QualityMineral
Processing
Fixed Stochastic Fixed Stochastic Fixed Stochastic Fixed Stochastic
TSMB-
TDMI 0.994 (± 0.001) 0.991 (± 0.001) 0.956 (± 0.001) 0.959 (± 0.001) 0.846 (± 0.034) 0.549 (± 0.003) 0.766 (± 0.002) 0.570 (± 0.007) 0.825 (± 0.001)
TSMB-GCC 0.991 (± 0.001) 0.985 (± 0.001) 0.919 (± 0.054) 0.953 (± 0.003) 0.969 (± 0.002) 0.547 (± 0.005) 0.705 (± 0.002) 0.582 (± 0.005) 0.820 (± 0.001)
TDMI 0.981 0.958 0.521 0.519 0.673 0.543 0.776 0.587 0.804
GCC 0.975 0.955 0.766 0.779 0.730 0.534 0.561 0.619 0.729
Real time delay 0.984 0.984 0.534 0.534 0.957 0.957 -0.114 -0.114 N/A
No Alignment 0.749 0.758 0.766 0.925 0.480 0.484 0.047 -0.046 0.790
Table 3: Absolute performance ( 𝑅2for regression tasks and AUC for classification tasks) using Temporal Fusion Transformers
(TFT) as base models. TSMB-based methods still generally outperform traditional TDE methods.
93%95%97%99%AUC
Occupancy Fixed
60%70%80%90%100%AUC
Water Pump Fixed
65%75%85%95%AUC
Power Demand Fixed
62%66%70%74%78%R2
Air Quality Fixed
72%76%80%84%R2
Mineral Processing
92%94%96%98%100%AUC
Occupancy Stoch.
1005020105150%60%70%80%90%100%AUC
Water Pump Stoch.
1005020105151.5%52.5%53.5%54.5%55.5%AUC
Power Demand Stoch.
1005020105150%52%54%56%58%60%R2
Air Quality Stoch.
TSMB-TDMI
TSMB-GCC
Figure 6: Performance of Temporal Fusion Transformer (TFT)
model under different values of B.
5 DISCUSSION
TSMB’s training process, while effective, is computationally inten-
sive due to the iterative optimization required over bootstrapped
datasets. While straightforward improvement such as selecting a
smaller bootstrap sample size can help manage the computational
load as studied in Section 4.3, there are other strategies that can be
leveraged to further reduce the computational demands.
Firstly, bootstrap samples as well as downstream time delay
identification and model training are performed independently in
TSMB. Therefore, they can all be trivially parallelized on modern
hardware. Secondly, time delay identification and model training
are optimization problems. For time delay identification, we aim
to identify time delays that maximize a specific score function
(e.g., GCC or TDMI). For model training, we aim to identify model
parameters that minimize the model’s loss function (e.g., MSE or
cross-entropy). Given that in TSMB we are essentially solving thesame optimization problems with slightly different datasets resulted
from time series bootstrap, the time delays and model parameters
identified in each replication of TSMB are likely to be similar, as
empirically verified in Figure 5. Therefore, once a solution for the
time delay in one bootstrap sample is obtained, it can serve as an
initialization heuristic for the other 𝐵−1time delay identification
optimization problems. Similarly, this warm-starting idea can be
applied to model training. Finally, computational efficiency can be
further enhanced by subsampling the dataset before applying TSMB.
However, it is important to note that the sensitivity to subsampling
may vary depending on the model and the dataset. For instance,
while model training might require the entire dataset to achieve a
high-quality model, the time delay estimation step might be less
sensitive to subsampling.
Additionally, we have discussed a few options to computationally
efficiently approximate TSMB in Appendix B that can be applied in
appropriate use cases.
Aside from room for computational efficiency improvement,
the calibration of TSMB’s predictions, as illustrated by Figure 4,
presents opportunities for refinement. How to construct a better cal-
ibrated predictive model using TSMB that can be used for decision
making could be an useful future research direction.
6 CONCLUSION
In this work, we revisited the challenge of time delay estimation
and how elegantly handling potentially stochastic time delays can
have a significant impact on the accuracy and reliability of pre-
dictive models across a spectrum of data-intensive applications.
Time Series Model Bootstrap, or TSMB, is introduced as a robust
solution to the inherent complexities and unpredictable dynamics
characteristic of multivariate time series data. Through rigorous
experimentation, TSMB has demonstrated its capacity to enhance
prediction accuracy and robustness significantly.
As we advance into a future shaped by an ever-growing reliance
on data-driven systems, the relevance and potential of TSMB in ad-
dressing the nuances of time delay in predictive modeling become
increasingly evident. We hope that the insights and methodologies
presented in this work will not only enhance current practices but
also inspire continued innovation, fostering advancements in pre-
dictive modeling techniques across diverse and evolving domains.
3112KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
REFERENCES
[1]David J Albers and George Hripcsak. 2012. Using time-delayed mutual infor-
mation to discover and interpret temporal correlation structure in complex pop-
ulations. Chaos: An Interdisciplinary Journal of Nonlinear Science 22, 1 (2012),
013111.
[2]M Alonso Andre’es, Daniel Pena, and Juan Romo. 2002. Forecasting time series
with sieve bootstrap. Journal of Statistical Planning and Inference 100, 1 (2002),
1–11.
[3]Richard Bellman and Robert Kalaba. 1959. On adaptive control processes. IRE
Transactions on Automatic Control 4, 2 (1959), 1–9.
[4]Jacob Benesty, Jingdong Chen, and Yiteng Huang. 2004. Time-Delay Estimation
via Linear Interpolation and Cross Correlation. IEEE Trans. Audio Speech Lang.
Processing 12, 5 (Oct. 2004), 509–519.
[5]Svante Bjorklund and Lennart Ljung. 2003. A review of time-delay estimation
techniques. In 42nd IEEE International Conference on Decision and Control (IEEE
Cat. No. 03CH37475), Vol. 3. IEEE, 2502–2507.
[6]Peter J Brockwell and Richard A Davis. 2002. Introduction to time series and
forecasting. Springer.
[7]Vincent Brunner, Manuel Siegl, Dominik Geier, and Thomas Becker. 2021. Chal-
lenges in the development of soft sensors for bioprocesses: A critical review.
Frontiers in Bioengineering and Biotechnology (2021), 730.
[8]Peter Bühlmann. 1997. Sieve bootstrap for time series. Bernoulli (1997), 123–148.
[9]Luis M Candanedo and Véronique Feldheim. 2016. Accurate occupancy detection
of an office room from light, temperature, humidity and CO2 measurements
using statistical learning models. Energy and Buildings 112 (2016), 28–39.
[10] Benoit Champagne, Stéphane Bédard, and Alex Stéphenne. 1996. Performance of
time-delay estimation in the presence of room reverberation. IEEE Transactions
on Speech and Audio Processing 4, 2 (1996), 148–152.
[11] Jingdong Chen, Jacob Benesty, and Yiteng Huang. 2003. Robust time delay esti-
mation exploiting redundancy among multiple microphones. IEEE Transactions
on Speech and Audio Processing 11, 6 (2003), 549–557.
[12] Jingdong Chen, Yiteng Huang, and Jacob Benesty. 2004. Time delay estimation.
Audio signal processing for next-generation multimedia communication systems
(2004), 197–227.
[13] Xu Chen and Chunhui Zhao. 2020. Multivariate Time Delay Estimation Based
on Dynamic Characteristic Analytics. In 2020 39th Chinese Control Conference
(CCC). 2306–2311.
[14] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan
Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh.
2019. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica 6, 6
(2019), 1293–1305.
[15] Saverio De Vito, Ettore Massera, Marco Piga, Luca Martinotto, and Girolamo
Di Francia. 2008. On field calibration of an electronic nose for benzene estimation
in an urban pollution monitoring scenario. Sensors and Actuators B: Chemical
129, 2 (2008), 750–757.
[16] Joerg M Gablonsky and Carl Tim Kelley. 2000. A locally-biased form of the DIRECT
algorithm. Technical Report. North Carolina State University. Center for Research
in Scientific Computation.
[17] Xiaojing Gao, Wei Zhu, Qi Yang, Deze Zeng, Lei Deng, Qing Chen, and Mengfan
Cheng. 2021. Time delay estimation from the time series for optical chaos systems
using deep learning. Opt. Express 29, 5 (March 2021), 7904–7915.
[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT
press.
[19] Yanan Han, Shuiying Xiang, Yang Wang, Yuanting Ma, Bo Wang, Aijun Wen,
and Yue Hao. 2020. Generation of multi-channel chaotic signals with time
delay signature concealment and ultrafast photonic decision making based on a
globally-coupled semiconductor laser network. Photonics Research 8, 11 (2020),
1792–1799.
[20] Hongsen He, Jing Lu, Lifu Wu, and Xiaojun Qiu. 2013. Time delay estimation via
non-mutual information among multiple microphones. Appl. Acoust. 74, 8 (Aug.
2013), 1033–1036.
[21] J Ianniello. 1982. Time delay estimation via cross-correlation in the presence
of large estimation errors. IEEE Transactions on Acoustics, Speech, and Signal
Processing 30, 6 (1982), 998–1003.
[22] Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. 1993. Lipschitzian
optimization without the Lipschitz constant. Journal of optimization Theory and
Applications 79 (1993), 157–181.
[23] Eamonn Keogh, Li Wei, Xiaopeng Xi, Stefano Lonardi, Jin Shieh, and Scott Sirowy.
2006. Intelligent icons: Integrating lite-weight data mining and visualization
into GUI operating systems. In Sixth International Conference on Data Mining
(ICDM’06). IEEE, 912–916.
[24] C Knapp and G Carter. 1976. The generalized correlation method for estimation
of time delay. IEEE Trans. Acoust. 24, 4 (Aug. 1976), 320–327.[25] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. 2004. Estimating
mutual information. Phys. Rev. E Stat. Nonlin. Soft Matter Phys. 69, 6 Pt 2 (June
2004), 066138.
[26] Hans R Kunsch. 1989. The jackknife and the bootstrap for general stationary
observations. The annals of Statistics (1989), 1217–1241.
[27] Soumendra N Lahiri. 1999. Theoretical comparisons of block bootstrap methods.
Annals of Statistics (1999), 386–404.
[28] Nianqiang Li, Wei Pan, Shuiying Xiang, Bin Luo, Lianshan Yan, and Xihua Zou.
2013. Hybrid chaos-based communication system consisting of three chaotic
semiconductor ring lasers. Applied optics 52, 7 (2013), 1523–1530.
[29] Vladimir Likic. 2008. The Needleman-Wunsch algorithm for sequence alignment.
Lecture given at the 7th Melbourne Bioinformatics Course, Bi021 Molecular Science
and Biotechnology Institute, University of Melbourne (2008), 1–46.
[30] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. Temporal fusion
transformers for interpretable multi-horizon time series forecasting. International
Journal of Forecasting 37, 4 (2021), 1748–1764.
[31] Zhiyuan Jerry Lin, Hao Sheng, and Sharad Goel. 2021. Probability Paths and the
Structure of Predictions over Time. Advances in Neural Information Processing
Systems 34 (2021), 15098–15110.
[32] Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. 2019. Costco: A neural
tensor completion model for sparse tensors. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining . 324–
334.
[33] Johannes S Maritz. 2018. Empirical bayes methods. Chapman and Hall/CRC.
[34] N J I Mars and G W van Arragon. 1982. Time delay estimation in non-linear
systems using average amount of mutual information analysis. Signal Processing
4, 2 (April 1982), 139–153.
[35] Pascal Massart. 1990. The tight constant in the Dvoretzky-Kiefer-Wolfowitz
inequality. The annals of Probability (1990), 1269–1283.
[36] Meinard Müller. 2007. Dynamic time warping. Information retrieval for music
and motion (2007), 69–84.
[37] Huynh Dong Nguyen. 2021. Water pump maintenance: shutdown predic-
tion. https://www.kaggle.com/code/winternguyen/water-pump-maintenance-
shutdown-prediction.
[38] Jonathan M Nichols, Meredith N Hutchinson, Nicole Menkart, Geoff A Cranch,
and Gustavo Kunde Rohde. 2019. Time delay estimation via Wasserstein distance
minimization. IEEE Signal Processing Letters 26, 6 (2019), 908–912.
[39] Toru Ohira and Ryusuke Sawatari. 1997. Delay Estimation from noisy time series.
(Jan. 1997). arXiv:cond-mat/9701193 [cond-mat.stat-mech]
[40] Dimitris N Politis and Joseph P Romano. 1994. The stationary bootstrap. Journal
of the American Statistical association 89, 428 (1994), 1303–1313.
[41] Cheng Qian, Nikos Kargas, Cao Xiao, Lucas Glass, Nicholas Sidiropoulos, and
Jimeng Sun. 2021. Multi-version Tensor Completion for Time-delayed Spatio-
temporal Data. arXiv preprint arXiv:2105.05326 (2021).
[42] S Radhika and Sivabalan Arumugam. 2014. Improved non mutual information
based multi-path time delay estimation. Indian J. Sci. Technol. 7, 8 (2014), 1101–
1106.
[43] Damien Rontani, Alexandre Locquet, Marc Sciamanna, David S Citrin, and Silvia
Ortin. 2009. Time-Delay Identification in a Chaotic Semiconductor Laser With
Optical Feedback: A Dynamical Point of View. IEEE J. Quantum Electron. 45, 7
(July 2009), 879–1891.
[44] Brian C Ross. 2014. Mutual information between discrete and continuous data
sets. PLoS One 9, 2 (Feb. 2014), e87357.
[45] H M Ruan, X M Tian, P Wang, and Others. 2014. Dynamic soft sensor method
based on joint mutual information. CIESC journal 65, 11 (2014), 4497–4502.
[46] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.
DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-
national Journal of Forecasting 36, 3 (2020), 1181–1191.
[47] N Shafiza Mohd Tamim and Farid Ghani. 2010. Techniques for optimization in
time delay estimation from cross correlation function. Int J Eng Technol 10, 2
(2010), 69–75.
[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[49] Chien-Fu Jeff Wu. 1986. Jackknife, bootstrap and other resampling methods in
regression analysis. the Annals of Statistics 14, 4 (1986), 1261–1295.
[50] Zeyu Xia, Yingbo Cui, Ang Zhang, Tao Tang, Lin Peng, Chun Huang, Canqun
Yang, and Xiangke Liao. 2022. A review of parallel implementations for the smith–
waterman algorithm. Interdisciplinary Sciences: Computational Life Sciences 14, 1
(2022), 1–14.
[51] Shuiying Xiang, Aijun Wen, Wei Pan, Lin Lin, Huixing Zhang, Hao Zhang, Xingx-
ing Guo, and Jiafu Li. 2016. Suppression of chaos time delay signature in a ring
network consisting of three semiconductor lasers coupled with heterogeneous
delays. Journal of Lightwave Technology 34, 18 (2016), 4221–4227.
3113Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy KDD ’24, August 25–29, 2024, Barcelona, Spain.
APPENDIX
A EXPERIMENT DATASET DETAILS
Table 4 shows detailed information about the datasets on which
our experiments are performed. Across all datasets and baselines
B POSSIBLE VARIANTS OF TSMB
Performing TSMB on a large dataset with complex models can be
expensive. There are generally two components in TSMB that are
computationally demanding: optimizing for ˆ𝜹𝑏onD𝑏, and the
construction of each bootstrap model 𝑓ˆ𝜹𝑏. Here we consider two
classes of TSMB variants that respectively allow us to construct
fewer model during training (time delay bootstrap) and performing
fewer time delay optimizations (perturbed model average).
B.1 Description of TSMB Variants
B.1.1 Time Delay Bootstrap (TDB). One way to approximately by-
pass the need of integrating over 𝜹values is to replace its bootstrap
distribution with a single point distribution (i.e., a point estimate)
centered around its expectation. Specifically, when constructing
the predictive model, we are making the following approximation:
E𝜹[E[𝑌|𝑋=𝑥,𝜹]]≈E[𝑌|𝑋=𝑥,𝜹=E[𝜹]]
Similar ideas are often employed in empirical Bayes methods [ 33].
Figure 5 shows the time delay bootstrap distribution estimated
using TDMI and GCC as the score function respectively in the
nine datasets we use in this paper. We can see that the empirical
bootstrap distributions are not necessarily centered around the
corresponding point estimates given by traditional TDE methods,
but typically the ground truth time delay (when applicable) almost
always falls under regions with non-trivial mass. Since we only use
the bootstrap samples up till the estimation of E[𝜹], we refer this
method as time delay bootstrap (TDB).
B.1.2 Perturbed Model Average. Although TDB allows us to bypass
the cost of constructing 𝐵bootstrap model, it still requires opti-
mizing ˆ𝜹𝑏for𝐵times. This procedure may still be costly with no
obvious advantage and at the end of the day, we are still construct-
ing a single machine learning model based on a point estimate of the
time delay. Instead of using the bootstrap samples to approximate
the distribution of 𝜹, one may simply assume that 𝜹∼F𝑃
𝜹for some
parametric distribution F𝑃
𝜹that can reasonably approximate the
true underlying distribution F𝜹. Such approximation allows us to
draw essentially as many time delay samples as needed. Following
the empirical Bayes idea, we let F𝑃
𝜹=𝑁(𝝁=𝜹𝑇𝐷𝐸,𝚺=0.1I)
on a normalized scale where 𝜹𝑇𝐷𝐸 is the point estimate2given
by traditional TDE methods such as ones obtained by maximizing
TDMI or GCC. While this approach seems less principled compared
to TSMB and TDB, it allows for a similar model ensemble as in
TSMB, while being able to avoid the costly repeated optimization
ofˆ𝜹𝑏. Since we use a Gaussian distribution to perturb the esti-
mated time delay around the point estimation given by traditional
TDE methods and aggregate model predictions for each of those
2Since we normalize all time delays to be between 0 and 1, when sampling from F𝑃
𝜹
we clip all sample values below 0 or above 1 to be 0 or 1. This makes F𝑃
𝜹technically a
censored normal distribution.perturbed time delay, we refer to this method as perturbed model
average. As we will present in the experiment section, perturbed
model average methods, though on average perform worse than
their TSMB counterparts, still offer competitive performance gain
compared to traditional TDE methods.
80%84%88%92%96%AUC
Occupancy Fixed
99.0%99.2%99.4%99.6%AUC
Water Pump Fixed
82%86%90%94%98%AUC
Power Demand Fixed
62%66%70%74%78%R2
Air Quality Fixed
86.1%86.3%86.5%86.7%86.9%R2
Mineral Processing
77%79%81%83%85%AUC
Occupancy Stoch.
1005020105198.6%98.8%99.0%99.2%99.4%AUC
Water Pump Stoch.
1005020105149%51%53%55%57%AUC
Power Demand Stoch.
1005020105149%51%53%55%57%R2
Air Quality Stoch.TDB-TDMI
TDB-GCC
(a) Time delay bootstrap (TDB)
84%88%92%96%AUC
Occupancy Fixed
99.88%99.90%99.92%99.94%99.96%AUC
Water Pump Fixed
86%90%94%98%AUC
Power Demand Fixed
58%63%68%73%78%R2
Air Quality Fixed
86.3%86.5%86.7%86.9%R2
Mineral Processing
86%88%90%92%94%96%AUC
Occupancy Stoch.
1005020105199.88%99.90%99.92%99.94%99.96%AUC
Water Pump Stoch.
1005020105150%52%54%56%58%AUC
Power Demand Stoch.
1005020105140%45%50%55%60%R2
Air Quality Stoch.
PMA-TDMI
PMA-GCC
(b) Perturbed model average
Figure 7: Ablation study on the choice of bootstrap sample
size𝐵for time delay bootstrap (TDB) and perturbed model
average. We observe patterns similar to the ones with TSMB
where using a relatively small bootstrap sample size (e.g.,
𝐵=5) can still result in a good predictive performance, fur-
ther mitigating the computational demand on these TSMB
variants.
3114KDD ’24, August 25–29, 2024, Barcelona, Spain. Jiajie Wang, Zhiyuan Jerry Lin, & Wen Chen
Dataset Sampling
Freq. Search Range Injected Time Delay Train/Val/Test Split
Occupancy
- Fixed 1min
[10, 180]min [150, 120, 90, 60, 30]min 25%/25%/50%
Occupancy
- Stochastic 1min
[10, 180]minPossible TD 1: [180, 140, 100, 80, 50]min
25%/25%/50%Possible
TD 2: [170, 130, 100, 70, 40]min
Possible
TD 3: [150, 120, 90, 60, 30]min
Possible
TD 4: [140, 110, 80, 50, 20]min
Possible
TD 5: [130, 100, 70, 40, 20]min
Pump
Maintenance - Fixed 1min
[10, 80]min [65, 55, 40, 30, 20, 15]min 50%/N/A/50%
Pump
Maintenance - Stochastic 1min
[10, 80]minPossible TD 1: [75, 65, 50, 40, 30, 25]min
50%/N/A/50%Possible
TD 2: [70, 60, 45, 35, 25, 20]min
Possible
TD 3: [65, 55, 40, 30, 20, 15]min
Possible
TD 4: [60, 50, 35, 25, 15, 10]min
Possible
TD 5: [55, 45, 30, 20, 10, 5]min
Po
wer Demand - Fixed 1min
[0, 10]min [7,7,7,7,7,7,7,7,5,5,5,5,5,5,5,5,3,3,3,3,3,3,3,3]min 6%/N/A/94%
Po
wer Demand - Stochastic 1min
[0, 10]minPossible TD 1: [9,9,9,9,9,9,9,9,7,7,7,7,7,7,7,7,5,5,5,5,5,5,5,5]min
6%/N/A/94%Possible
TD 2: [8,8,8,8,8,8,8,8,6,6,6,6,6,6,6,6,4,4,4,4,4,4,4,4]min
Possible
TD 3: [7,7,7,7,7,7,7,7,5,5,5,5,5,5,5,5,3,3,3,3,3,3,3,3]min
Possible
TD 4: [6,6,6,6,6,6,6,6,4,4,4,4,4,4,4,4,2,2,2,2,2,2,2,2]min
Possible
TD 5: [5,5,5,5,5,5,5,5,3,3,3,3,3,3,3,3,1,1,1,1,1,1,1,1]min
Air
Quality - Fixed 1hr
[1, 24]hr [20, 20, 15, 15, 10, 5, 5, 5]hr 50%/25%/25%
Air
Quality - Stochastic 1hr
[1, 24]hrPossible TD 1: [22, 22, 17, 17, 12, 7, 7, 7]hr
50%/25%/25%Possible
TD 2: [21, 21, 16, 16, 11, 6, 6, 6]hr
Possible
TD 3: [20, 20, 15, 15, 10, 5, 5, 5]hr
Possible
TD 4: [19, 19, 14, 14, 9, 4, 4, 4]hr
Possible
TD 5: [18, 18, 13, 13, 8, 3, 3, 3]hr
Mineral
Processing 7s/2hr
(𝑿/𝒚) [0, 90]min N/A 50%/25%/25%
Table 4: Experimental dataset details.
Metho
d Occupancy Pump Maintenance Power Demand Air QualityMineral
Processing
Fixed Stochastic Fixed Stochastic Fixed Stochastic Fixed Stochastic
TSMB-
TDMI (ours) 0.995(± 0.000) 0.950(± 0.007) 0.999(± 0.000) 0.999(± 0.000) 0.945(± 0.021) 0.511(± 0.010) 0.766(± 0.002) 0.571(± 0.002) 0.870(± 0.000)
TSMB-GCC (ours) 0.995(± 0.000) 0.929(± 0.002) 0.999(± 0.000) 0.999(± 0.000) 0.987(± 0.000) 0.525(± 0.006) 0.708(± 0.005) 0.549(± 0.006) 0.871(± 0.000)
TDB-TDMI (TSMB variant) 0.915(± 0.005) 0.806 (± 0.009) 0.993 (± 0.001) 0.993 (± 0.001) 0.848 (± 0.000) 0.549 (± 0.012) 0.764 (± 0.004) 0.548 (± 0.014) 0.866 (± 0.001)
TDB-GCC (TSMB variant) 0.851 (± 0.011) 0.796 (± 0.010) 0.992 (± 0.000) 0.991 (± 0.000) 0.964 (± 0.000) 0.519 (± 0.016) 0.634 (± 0.002) 0.532 (± 0.010) 0.867 (± 0.001)
Perturbed-TDMI (TSMB variant) 0.951 (± 0.005) 0.943 (± 0.002) 0.999(± 0.000) 0.999(± 0.000) 0.941 (± 0.003) 0.568 (± 0.001) 0.765 (± 0.001) 0.581 (± 0.003) 0.868 (± 0.000)
Perturbed-GCC (TSMB variant) 0.951 (± 0.004) 0.942 (± 0.001) 1.000 (± 0.000) 1.000 (± 0.000) 0.964 (± 0.002) 0.533 (± 0.002) 0.703 (± 0.003) 0.587 (± 0.002) 0.867 (± 0.000)
TDMI 0.923 0.791 0.991 0.990 0.847 0.515 0.760 0.544 0.867
GCC 0.923 0.811 0.990 0.988 0.841 0.501 0.691 0.538 0.864
Real time delay 0.988 0.988 0.991 0.991 0.964 0.964 1.000 1.000 N/A
No Alignment 0.728 0.722 0.979 0.979 0.509 0.519 0.085 -0.106 0.860
Table 5: Absolute performance ( 𝑅2for regression tasks and AUC for classification tasks) on all methods. We do not know the
real time delay for the mineral processing dataset as it is a real-world dataset with unknown time delays. 95% confidence
intervals are reported for TSMB-based methods. For other baselines, repeated experiments result in the same metric value,
hence the CI is effectively zero on the specific datasets evaluated and not omitted.
B.2 Experimental Results with TSMB Variants
Here, we compare variants of TSMB described in Section B.1.1 and
B.1.2. We additionally evaluated the performance of TSMB variants.
Table 5 shows the full suite of relative predictive performance
values.We further performed ablation study on the choice of bootstrap
sample size 𝐵and have observed similar pattern for TDB and per-
turbed model average (Figure 7) where even a small bootstrap
sample size (e.g., 𝐵=5) can still result in competitive predictive
performance. When a small 𝐵is used together with the TSMB
variants, the computational demand can be significantly alleviated.
3115