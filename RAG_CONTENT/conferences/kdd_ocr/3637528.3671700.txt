GeoMix: Towards Geometry-Aware Data Augmentation
Wentao Zhao
permanent@sjtu.edu.cn
Shanghai Jiao Tong University
Department of Computer Science and Engineering
MoE Key Lab of Artificial Intelligence
Shanghai, ChinaQitian Wu
echo740@sjtu.edu.cn
Shanghai Jiao Tong University
Department of Computer Science and Engineering
MoE Key Lab of Artificial Intelligence
Shanghai, China
Chenxiao Yang
chr26195@sjtu.edu.cn
Shanghai Jiao Tong University
Department of Computer Science and Engineering
MoE Key Lab of Artificial Intelligence
Shanghai, ChinaJunchi Yan∗
yanjunchi@sjtu.edu.cn
Shanghai Jiao Tong University
Department of Computer Science and Engineering
MoE Key Lab of Artificial Intelligence
Shanghai, China
Abstract
Mixup has shown considerable success in mitigating the challenges
posed by limited labeled data in image classification. By synthesiz-
ing samples through the interpolation of features and labels, Mixup
effectively addresses the issue of data scarcity. However, it has rarely
been explored in graph learning tasks due to the irregularity and
connectivity of graph data. Specifically, in node classification tasks,
Mixup presents a challenge in creating connections for synthetic
data. In this paper, we propose Geometric Mixup (GeoMix), a simple
and interpretable Mixup approach leveraging in-place graph editing.
It effectively utilizes geometry information to interpolate features
and labels with those from the nearby neighborhood, generating
synthetic nodes and establishing connections for them. We conduct
theoretical analysis to elucidate the rationale behind employing ge-
ometry information for node Mixup, emphasizing the significance
of locality enhancement—a critical aspect of our method’s design.
Extensive experiments demonstrate that our lightweight Geometric
Mixup achieves state-of-the-art results on a wide variety of stan-
dard datasets with limited labeled data. Furthermore, it significantly
improves the generalization capability of underlying GNNs across
various challenging out-of-distribution generalization tasks. Our
code is available at https://github.com/WtaoZhao/geomix.
CCS Concepts
•Computing methodologies →Machine learning algorithms .
Keywords
Mixup, Augmentation, Graph Neural Networks, Out-of-Distribution
Generalization
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671700ACM Reference Format:
Wentao Zhao, Qitian Wu, Chenxiao Yang, and Junchi Yan. 2024. GeoMix:
Towards Geometry-Aware Data Augmentation . In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671700
1 Introduction
Graph Neural Networks (GNNs) [ 7,8,19,23] have become the
de facto method for modeling the increasingly popular graph-
structured data. However, in real world, labeling data is expensive
and many datasets have very few labeled examples. This scarcity
of labeled data can lead to severe over-fitting issues and weaken
the generalization performance of GNNs, especially when the test
data comes from a distribution that differs from the training data,
which is a common scenario in many real-world datasets.
Motivated by the above issues, we set out to design Mixup for
graph learning, a technique that has demonstrated substantial suc-
cess in mitigating challenges caused by limited data and enhancing
model performance [ 28]. At its core, Mixup trains neural networks
on convex combinations of pairs of examples and their correspond-
ing labels. This approach broadens the distribution of training data
and regularizes neural networks, serving as the key factors behind
its capacity to reduce over-fitting, facilitate the learning of more dis-
criminative representations, and enhance model generalization [ 29].
These attributes are pivotal when handling datasets with limited
labeled data or where the training data only encompass a subset of
the diverse data distributions, which might not fully represent the
distributions of the testing data.
Though prevailingly used in other fields, Mixup has rarely been
explored in graph learning tasks, due to the connectivity in graph.
In node classification task, questions have arisen about how to
effectively connect synthetic nodes. Current general Mixup strate-
gies [ 21,22] often attempt to circumvent the explicit connection of
synthetic nodes by performing Mixup between layers of neural net-
works, potentially limiting their adaptability. Other Mixup methods
aimed at addressing class-imbalance problems incorporate complex
edge prediction modules. Unfortunately, this sacrifices Mixup’s
inherent lightweight nature and may diminish the generalization
power due to increased model complexity.
4500
KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
To address these challenges, this paper proposes a simple and
geometry-aware Mixup approach that in-place modifies raw data
and explicitly establishes connections for synthetic nodes, enhanc-
ing interpretability. Theoretical analysis on the mixed features and
labels reveals: (1) the interpolation effects of this geometry-aware
Mixup; (2) its rationale for utilizing geometry information; (3) sce-
narios where this fundamental strategy may succeed or fail.
Building upon the theoretical analysis and recognizing potential
failure cases, we further refine our approach and present Geometric
Mixup. It not only considers geometry details but also enhances
locality information, enabling it to adapt to both homophilic graphs
(where adjacent nodes are likely to have similar labels and features)
and heterophilic graphs (adjacent nodes tend to have dissimilar
labels). Moreover, we elucidate the connection between Geometric
Mixup and graph structure learning to provide more insight and
better interpretability for our design.
Extensive experiments across twelve datasets demonstrate that:
(1) Geometric Mixup achieves state-of-the-art results on both ho-
mophilic and heterophilic graphs with limited labeled data; (2)
it significantly improves the generalization ability of underlying
GNNs in various out-of-distribution generalization tasks with lim-
ited distributions of training data; (3) it assists underlying GNNs in
learning more discriminative representations, improving prediction
performance.
The major contributions of our work are:
1) We propose a simple and interpretable Mixup strategy lever-
aging in-place graph editing, which is a novel perspective.
2) Our approach effectively utilizes graph geometry while en-
hancing locality information to accommodate to both homophilic
and heterophilic graphs.
3) Theoretical analysis provides insights into leveraging geom-
etry information for Mixup and underlines the significance of en-
hancing locality information.
4) Extensive experiments substantiate that Geometric Mixup
effectively improves the performance and generalization of under-
lying GNNs in challenging tasks with limited training data.
To distinguish our approach from existing ones, we compare
Geometric Mixup with other node Mixup methods in Table 1.
2 Preliminaries
2.1 Semi-supervised Node Classification
Let𝐺=(V,E)denotes a graph with node set Vand edge setE.
Each node𝑣∈V is associated with a feature vector x𝑣and a label
𝑦𝑣, represented in one-hot form as y𝑣. Denote node feature matrix as
X={x𝑖}|V|
𝑖=1, adjacency matrix as A. The goal of node classification
task is to train a classifier 𝑓(·)that can accurately predict node
labels based on XandA. In semi-supervised setting, the classifier
has access to the complete feature matrix Xand adjacency matrix
A, but is restricted to having labels for only a subset of nodes,
constituting the labeled node set V𝑙(we denote the unlabeled node
set asV𝑢). Therefore, the standard loss function for semi-supervised
node classification is
∑︁
𝑣∈V𝑙ℓ(𝑓(A,X)𝑣,y𝑣), (1)whereℓis usually cross-entropy loss. 𝑓(A,X)𝑣is the prediction for
node𝑣.
2.2 Message Passing Neural Networks
Message passing neural networks propagate information along
edges to learn node representations, which can be expressed as:
h(𝑘+1)
𝑣 =AGGR
h(𝑘)
𝑣,{h(𝑘)
𝑢:𝑢∈N(𝑣)}
, (2)
whereN(𝑣)is the neighborhood of 𝑣.AGGR function aggregates
information from neighboring nodes and combines the results with
the current state of the central node to update its representation.
2.3 Mixup
Mixup is first proposed for image classification [ 28]. It linearly
mixes both features and labels of samples, which can be written as
¯x=𝜆x𝑖+(1−𝜆)x𝑗, (3)
¯y=𝜆y𝑖+(1−𝜆)y𝑗, (4)
where𝑖,𝑗is a random pair of samples, 𝜆∈[0,1]. However, adapting
Mixup to node classification is nontrivial due to the challenge in
defining neighborhood for synthetic nodes.
3 Methods
3.1 A Basic Geometry-Aware Mixup
Though it may seem natural to apply Eq. (3)and(4)to node features
and node labels to create synthetic nodes, how to establish connec-
tions for synthetic nodes remains an unsolved problem. Inspired
by message passing which iteratively updates node features by
combining information from neighboring nodes, we propose an in-
place-editing-based Mixup, where a node’s feature/label is adjusted
using a convex combination of features/labels from its immediate
neighborhood. It explicitly connects synthetic nodes without neces-
sitating a complex edge prediction module and effectively leverages
prior knowledge from the given graph.
However, one challenge of semi-supervised learning lies in the
scarcity of ground truth labels, leading to incomplete or inaccessi-
ble label information for neighborhoods of certain nodes. To this
end, we first employ the training model 𝑓(·)to predict the pseudo
label for each unlabeled node. For the convenience of subsequent
derivation, denote ˆy𝑣as:
ˆy𝑣=(
y𝑣 if𝑣∈V𝑙
𝑓(A,X)𝑣otherwise(5)
The most basic approach to leverage geometry information in
Mixup involves updating a node’s feature and label to be the aver-
age of its neighbors’ features and labels. Consequently, the mixing
operation for an arbitrary node 𝑣can be expressed as:
h(𝑡+1)
𝑣=∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢h(𝑡)
𝑢, (6)
¯y(𝑡+1)
𝑣=∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢¯y(𝑡)
𝑢, (7)
where h(𝑡)
𝑣and¯y(𝑡)
𝑣are the feature and label of node 𝑣after𝑡-th
operation. h(0)
𝑣=x𝑣is the input node feature. ¯y(0)
𝑣=ˆy𝑣.𝑒𝑢𝑣is
the edge weight given by common normalized adjacency matrix
4501GeoMix: Towards Geometry-Aware Data Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparison of GeoMix (short for Geometric Mixup) with other node Mixup.
MethodUse geometry
informationExplicitly connect
synthetic nodesSupport modification
to raw dataIntroduce time-consuming
edge prediction moduleSupport OOD
Generalization
GraphMix [21] ✗ ✗ ✗ ✗ ✓
Mixup [22] ✗ ✗ ✗ ✗ ✓
GraphMixup [24] ✗ ✓ ✓ ✓ ✗
GeoMix (ours) ✓ ✓ ✓ ✗ ✓
likeD−1AandD−1/2AD−1/2where Dis the degree matrix. We
conduct theoretical analysis on Eq. (6)and(7)to demonstrate its
interpolation effects and offer insights into the circumstances under
which this fundamental form of Mixup may succeed or fail.
Assumptions on Graphs. To ease the analysis, we pose the
following assumptions on graphs. Denote the number of classes
as𝐶. Assume that for any node 𝑖: (1) Its feature x𝑖is sampled
from feature distribution D𝑦𝑖associated with its label, with 𝝁(𝑦𝑖)
denoting its mean; (2) dimensions of x𝑖are independent to each
other; (3) the feature values in x𝑖are bounded by a positive scalar
𝐵, i.e., max𝑘|x𝑖[𝑘]|≤𝐵; (4) due to lack of ground truth labels and
errors in pseudo label prediction, the expectations of ˆy𝑖is
E[ˆy𝑖]=(1−𝜖)e𝑦𝑖+𝜖
𝐶−1∑︁
𝑗≠𝑦𝑖e𝑗, (8)
where e𝑦𝑖is the𝑦𝑖-th standard basis vector (all elements are 0
except that the 𝑦𝑖-th element is 1), 𝜖∈ (0,1)is related to the
label rate and the accuracy of model used to predict pseudo-labels.
When the label rate is large or the model is highly dependable, 𝜖
should be close to 0; (5) Its neighbors’ labels {𝑦𝑗:𝑗∈N(𝑖)}are
conditionally independent given 𝑦𝑖, and have the same label as
node𝑖with probability 𝑝. They belong to any other class 𝑐≠𝑦𝑖
with probability(1−𝑝)/(𝐶−1).
We useG={V,E,{D𝑐,𝑐∈𝐶},𝑝,𝜖}to denote a graph following
the above assumptions. Note that we use subscript 𝑐to indicate
that distributionD𝑐is shared by all nodes with the same label 𝑐.
Then we have the following theorem about mixed features:
Theorem 1. Consider a graphG={V,E,{D𝑐,𝑐∈𝐶},𝑝,𝜖}
following Assumptions (1)-(5). For any node 𝑖∈V, the expectation
of its feature after performing one Mixup operation is
E[h𝑖]=𝑝𝝁(𝑦𝑖)+1−𝑝
𝐶−1∑︁
𝑐≠𝑦𝑖𝝁(𝑐), (9)
and for any 𝑡>0, the probability that the distance between the
observation h𝑖and its expectation is larger than t is bounded by
P(∥h𝑖−E[h𝑖]∥2≥𝑡)≤2𝐹exp
−𝑑𝑒𝑔(𝑖)𝑡2
2𝐵2𝐹
, (10)
where𝐹is the feature dimension.
Similarly, for mixed labels, we haveTheorem 2. For any𝑐∈𝐶and any𝑖∈V with𝑦𝑖=𝑐, the
expectation of mixed label ¯y𝑖after performing one Mixup operation is
E[¯y𝑖]=
𝑝(1−𝜖)+𝜖(1−𝑝)
𝐶−1
e𝑐+𝑝𝜖+(1−𝑝)(1−𝜖)
𝐶−1
+𝜖(1−𝑝)(𝐶−2)
(𝐶−1)2∑︁
𝑗≠𝑐e𝑗,(11)
and for any 𝑡>0, the probability that the distance between the
observation ¯y𝑖and its expectation is larger than t is bounded by
P(∥¯y𝑖−E[¯y𝑖]∥2≥𝑡)≤2𝐶exp
−𝑑𝑒𝑔(𝑖)𝑡2
2𝐶
. (12)
The proofs of Theorem 1 and 2 can be found in the appendix. The
above theorems demonstrate two facts. Firstly, when 𝑝is large and
𝜖is small, the mixed feature and label of node 𝑖will stay compara-
tively close to its input feature and label in expectation. Secondly,
the distance between the mixed feature/label of node 𝑖and its ex-
pectation is small with a high probability. Together, they show that
the locality information is preserved in the Mixup and justify the
rationale to place the updated node in its previous position. Further-
more, within Eq. (9)and(11), we observe the desired interpolation
effects achieved through Mixup.
However, when 𝑝is small, as is in some challenging heterophilic
graphs, the mixed feature/label of node 𝑖will be far from its origi-
nal feature/label in expectation. Thus, the locality information is
not well-preserved and it becomes dubious to place the updated
node in its original position. Another problem arising from a small
value of𝑝is the decreased distinguishability in the expectations of
mixed features/labels from nodes belonging to different classes. In
extreme cases when 𝑝→1/𝐶, the expected features/labels of nodes
from different classes converge to the same point, which greatly
diminishes the diversity of mixed features and labels. Therefore,
this basic geometry-aware Mixup may fail to perform well in some
heterophilic graphs, and we will provide solutions next.
3.2 Geometric Mixup: Locality-Enhanced Mixup
with Structure Awareness
One feasible solution to the problems elucidated in the preceding
section is to enhance the locality information by adding residual
connections. For node 𝑣, we establish a residual connection utilizing
h(𝑡)
𝑣, its mixed feature from the preceding Mixup operation. This
locality fortification yields the first variant of Geometric Mixup,
4502KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
GNN Module input graphinput graph with
pseudo-labelsmixed features &
mixed labelsPseudo-Label Prediction
...Geometric MixupInput
PredictionsPseudo
Label Prediction
Geometric
Mixup
Downstream
GNN
feature space mixup
Class A Unlabeled node Node feature Class B Mixed label
label spacemixup
1-st Mixup
feature space mixup
label spacemixup
K-th Mixup
Figure 1: Illustration of the training procedure with Geometric Mixup.
which is expressed in Eq. (13)-(14) and illustrated in Fig. 1.
h(𝑡+1)
𝑣=𝛼h(𝑡)
𝑣+(1−𝛼)∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢h(𝑡)
𝑢, (13)
¯y(𝑡+1)
𝑣=𝛼¯y(𝑡)
𝑣+(1−𝛼)∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢¯y(𝑡)
𝑢. (14)
The residual connection h(𝑡)
𝑣encompasses information from nodes
within a distance of 𝑡hops from node 𝑣and thus helps to better pre-
serve the locality information of node 𝑣.𝛼is a hyper-parameter con-
trolling the effect of locality reinforcement. By choosing a proper
value for𝛼, the expectation of h(𝑡+1)
𝑣 will stay relatively close to
its preceding value. Consequently, the expected features/labels of
nodes from different classes will not converge to the same point
even when 𝑝is small and the locality information is effectively
preserved. Therefore, the diversity of synthetic data will not be
compromised and it is reasonable for the updated node to remain
in its position. Note that we may repeat the Mixup operation for 𝐾
times to add more comprehensive range of geometry information
to the mixed features and labels. In practice, one or two consecutive
Mixup achieves good performance.
A more radical and effective choice of preserving locality infor-
mation from the input graph is to utilize node 𝑣’s original feature
h(0)
𝑣and label ¯y(0)
𝑣to establish the residual connection.
h(𝑡+1)
𝑣=𝛼h(0)
𝑣+(1−𝛼)∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢h(𝑡)
𝑢, (15)
¯y(𝑡+1)
𝑣=𝛼¯y(0)
𝑣+(1−𝛼)∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢¯y(𝑡)
𝑢. (16)
In challenging heterophilic graphs, this may produce better results
in virtue of its better enhancement of locality information from
input graph, which will be demonstrated in experimental sections.
During the training stage, we feed the mixed features H=
{h𝑖}|V|
𝑖=1(here we drop the superscript 𝐾without causing confu-
sion) and the adjacency matrix Ato GNN to predict labels. As is
shown in Eq. (17), the loss function consists of two parts. For la-
beled nodes, we use the ground truth labels as supervision signals.
While for unlabeled nodes, we use the mixed labels for guidance. 𝜆
is a hyper-parameter used to balance the influence of mixed labels
¯Y. At the inference stage, we don’t perform Mixup and the GNN
accepts the original features and adjacency matrix as input.
L=∑︁
𝑣∈V𝑙ℓ(𝑓(A,H)𝑣,y𝑣)+𝜆∑︁
𝑣∈V𝑢ℓ(𝑓(A,H)𝑣,¯y𝑣). (17)Relationship with Structure Learning. The above Mixup is in
some extent linked to graph structure learning, which helps GNN
learning by optimizing the given graph structure to meet some
desirable properties such as smooth node features and connectivity
[2]. In Mixup, however, we modify the node features instead of the
graph structure. In this sense, the training procedure after incorpo-
rating Mixup can be considered as a bilevel optimization problem.
The upper-level optimization task treats the GNN 𝑓as the decision
variable and aims to minimize the label prediction loss in Eq. (17),
while the lower-level optimization task is to minimize a regulariza-
tion function that regularizes the learned graph by modifying node
features Hand labels ¯Y, which we will explain next.
As a theoretical intuition and justification, Mixup operation (13)
and(15)are gradient descent steps of two separate regularization
functions which assess the quality of the mixed node features. As-
sume node features H(𝑡)to be a continuous function over 𝑡≥0
with H(0)=X, we have the following theorem:
Theorem 3. Mixup in Eq. (13) and (15) correspond to gradient
descent steps of regularization functions 𝐹1(H;H(𝑡))and𝐹2(H;H(0)).
𝐹1(H;H(𝑡))=∑︁
𝑢∈V∥h𝑢−h(𝑡)
𝑢∥2
2+𝛽∑︁
(𝑢,𝑣)∈E𝑒𝑢𝑣∥h𝑢−h𝑣∥2
2,(18)
𝐹2(H;H(0))=∑︁
𝑢∈V∥h𝑢−h(0)
𝑢∥2
2+𝛽∑︁
(𝑢,𝑣)∈E𝑒𝑢𝑣∥h𝑢−h𝑣∥2
2,(19)
where𝛽is related to 𝛼.
The first term in Eq. (18)and(19)promote the proximity be-
tween the updated node feature and its current state or original
state, while the second term encourages the similarity of features
among neighboring nodes. We can obtain a similar cost function
regarding mixed labels ¯Y, which operations in Eq. (14)and(16)
serve to descend.
Complexity analysis. In both Mixup operations (13)and(15),
the computational complexity of calculating H(𝑡)is𝑂(|V|𝐹+|E|𝐹),
where𝐹is the number of input features. This is because the ag-
gregating part involving 𝑒𝑢𝑣can be implemented as a product of a
sparse matrix with a dense matrix. Applying 𝐾consecutive Mixup
operations multiplies the storage and time requirements by a factor
of𝐾. In practice, 𝐾is usually 2. Similarly, the complexity of Mixup
operation for labels is 𝑂(|V|𝐶+|E|𝐶), where𝐶is the number of
classes. As a result, the overall GNN training complexity after in-
cluding Geometric Mixup remains consistent with conventional
GNN training procedure.
4503GeoMix: Towards Geometry-Aware Data Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
3.3 Extending Geometric Mixup Beyond
Vicinity: An Adaptive Approach
Geometric Mixup operations provided in previous sections have two
limitations. Firstly, the aggregating weights 𝑒𝑣𝑢are non-parametric,
i.e., they are determined solely by adjacency matrix and need no
training. Consequently, inappropriate weights may be assigned
when the graph structure contains noise. Secondly, restriction im-
posed by graph structure greatly reduces Mixup choices, since a
node can never have a chance to be mixed with a distant node.
To address the aforementioned limitations, we allow a node to
be mixed with any other node and adaptively learn the aggregating
weights, as shown in Eq. (20).
ˆh(𝑡+1)
𝑣=𝛼h(𝑡)
𝑣+(1−𝛼)∑︁
𝑢,𝑣∈V𝑎(𝑡)
𝑣𝑢h(𝑡)
𝑢
h(𝑡+1)
𝑣=(1−𝜂)ˆh(𝑡+1)
𝑣+𝜂∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢h(𝑡)
𝑢.(20)
where𝑎(𝑡)
𝑢𝑣is a time-frame-specific weight given by a weight pre-
diction module which we will specify later. 𝑒𝑢𝑣is given by common
normalized adjacency matrix. 𝜂is a hyper-parameter specifying
the weight of adaptive all-pair aggregating results. Symmetrically,
the Mixup operation for labels is
ˆ¯y(𝑡+1)
𝑣=𝛼y(𝑡)
𝑣+(1−𝛼)∑︁
𝑢,𝑣∈V𝑎(𝑡)
𝑣𝑢¯y(𝑡)
𝑢
¯y(𝑡+1)
𝑣=(1−𝜂)ˆ¯y(𝑡+1)
𝑣+𝜂∑︁
𝑢∈N(𝑣)𝑒𝑣𝑢¯y(𝑡)
𝑢.(21)
To adaptively and efficiently predict the aggregating weight 𝑎(𝑡)
𝑣𝑢,
we adopt a simple project-then-dot-product method, as displayed
in Eq. (22).
𝑎(𝑡)
𝑣𝑢=(q(𝑡)
𝑣)⊤k(𝑡)
𝑢Í
𝑤∈V(q(𝑡)
𝑣)⊤k(𝑡)
𝑤, (22)
with
q(𝑡)
𝑣=h(𝑡)
𝑣W(𝑡)
𝑞
∥h(𝑡)
𝑣W(𝑡)
𝑞∥2,k(𝑡)
𝑢=h(𝑡)
𝑢W(𝑡)
𝑘
∥h(𝑡)
𝑢W(𝑡)
𝑘∥2, (23)
where W(𝑡)
𝑞∈R𝐹×𝐹′andW(𝑡)
𝑘∈R𝐹×𝐹′(𝐹and𝐹′is the dimension
of input features and hidden features) are two learnable projection
matrices. Eq. (22)aligns with the self-attention mechanism of the
Transformer [ 18]. In this paradigm, each node can potentially mix
its features/labels with those of any other node whose projected
feature is similar. This addresses the limitation of having only a few
choices of nodes for Mixup in previous Geometric Mixup. Moreover,
the trainable parameters W(𝑡)
𝑞andW(𝑡)
𝑘can remedy the problem
of inappropriate aggregating weights assigned by input graph.
Complexity analysis. The all-pair aggregating operation guided
by weight𝑎(𝑡)
𝑣𝑢in Eq. (20)can be written in the following matrix
form
M(𝑡)=
diag
Q(𝑡)(K(𝑡))⊤1−1
Q(𝑡)(K(𝑡))⊤
H(𝑡), (24)
where Q(𝑡)andK(𝑡)are constructed by concatenating q(𝑡)
𝑢,𝑢∈V
andk(𝑡)
𝑢,𝑢∈V vertically respectively. By using the associativelaw of matrix multiplication, the above equation is equivalent to
M(𝑡)=
diag
Q(𝑡)
(K(𝑡))⊤1−1
Q(𝑡)
(K(𝑡))⊤H(𝑡)
.(25)
By first calculating(K(𝑡))⊤1and(K(𝑡))⊤H(𝑡)rather than Q(𝑡)(K(𝑡))⊤,
we reduce the quadratic complexity to linear w.r.t the number of
nodes. The time complexity of Eq. (25)is𝑂(|V|𝐹𝐹′)where𝐹and
𝐹′are dimensions of input features and hidden features. Combining
this with the complexity analysis in Sec. 3.2, the overall complex-
ity of Mixup in Eq. (20)is𝑂(|V|𝐹𝐹′+(|V|+|E|) 𝐹). Through a
similar analysis, the time complexity of label Mixup in Eq. (21)is
𝑂(|V|𝐶𝐹′+(|V|+|E|) 𝐶), where𝐶is the number of classes. There-
fore, this variant of Geometric Mixup still preserves the complexity
order of conventional GNN training.
4 Experiments
In this section, we conduct comprehensive experiments to evaluate
Geometric Mixup on an extensive set of node classification datasets.
Specifically, we focus on the following research questions:
•1)Can Geometric Mixup consistently and significantly improve
the performance of GNNs on common benchmarks with limited
labeled data? Additionally, can it cope with both homophily (where
adjacent nodes tend to share similar labels) [ 10] and heterophily
(which means adjacent nodes tend to have different labels)?
•2)Can Geometric Mixup consistently and significantly enhance
the ability of GNNs in out-of-distribution (OOD) generalization
tasks? That is, one has access to limited distributions in training
set and needs to generalize to datasets from distributions different
from those of the training data.
•3)Are the proposed components in Geometric Mixup effective
and necessary for the achieved performance?
•4)Can Geometric Mixup help GNNs learn more discriminative
representations for improved class differentiation?
Implementation details. We implement the three proposed Geo-
metric Mixup methods defined in Eq. (13),(15), and (20), naming
them GeoMix-I, GeoMix-II, and GeoMix-III, respectively. Unless
otherwise specified, we employ GCN [ 7] as the foundational GNN
for both Geometric Mixup and other competing methods that utilize
a GNN backbone. Following an optimization of architecture-related
hyperparameters for the standard GCN, which includes the number
of layers and hidden size, we adopt the same architecture for Geo-
metric Mixup to ensure a fair comparison. To reduce the number of
hyper-parameters, we set 𝜆in Eq. (17)to a default value of 1 except
in a few cases, since this consistently produces exemplary results
across a wide range of test cases. For additional implementation
and hyper-parameter details, please refer to the appendix.
Competitors. We mainly compare with GCN [ 7], the GNN back-
bone of Geometric Mixup, for testing the efficacy of Geometric
Mixup. We also compare with several state-of-the-art Mixup meth-
ods for node classification: Mixup [ 22], GraphMix [ 21] and Graph-
Mixup [ 24]. Furthermore, we compare with more advanced GNNs:
GAT [ 19], SGC [ 23], APPNP [ 8] and GloGNN [ 9]. In OOD general-
ization tasks, we add standard empirical risk minimization (ERM),
DANN [6], EERM [25] as competitive methods.
4504KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
Table 2: Mean and standard deviation (with five runs using random initializations) of testing accuracy on node classification
benchmarks.
Methods Cora CiteSeer PubMed CS Physics Squirrel Chameleon
GCN 81.63 ±0.45 71.64 ±0.33 78.88 ±0.65 91.16 ±0.52 92.85 ±1.03 39.47 ±1.47 41.32 ±3.22
GAT 82.98 ±0.88 72.20 ±0.99 78.58 ±0.52 90.57 ±0.37 92.70 ±0.58 35.96 ±1.73 39.29 ±2.84
SGC 80.35 ±0.24 71.87 ±0.14 78.75 ±0.17 90.37 ±1.01 92.80 ±0.15 39.04 ±1.92 39.35 ±2.82
APPNP 83.33 ±0.52 71.83 ±0.52 79.78 ±0.66 91.97 ±0.33 93.86 ±0.33 37.64 ±1.63 38.25 ±2.83
GloGNN 82.31 ±0.42 72.16 ±0.64 78.95 ±0.42 90.82 ±0.45 92.79 ±0.67 35.77 ±1.32 40.13 ±3.91
Mixup 81.84 ±0.94 72.20 ±0.95 79.16 ±0.49 91.36 ±0.37 93.89 ±0.49 37.95 ±1.52 39.56 ±3.13
GraphMixup 82.16 ±0.74 72.13 ±0.86 78.82 ±0.52 91.27 ±0.55 93.62 ±0.41 37.84 ±1.46 39.82 ±2.35
GraphMix 83.80 ±0.62 74.28 ±0.45 79.38 ±0.39 91.89 ±0.36 94.32 ±0.28 38.41 ±1.36 41.75 ±3.51
GeoMix-I 84.08 ±0.74 75.06 ±0.36 80.06 ±0.93 92.13 ±0.06 94.51 ±0.07 40.95 ±1.12 41.94 ±3.41
GeoMix-II 83.94 ±0.50 75.12 ±0.26 79.98 ±0.35 92.14 ±0.11 94.56 ±0.06 40.75 ±1.30 42.67 ±2.44
GeoMix-III 84.22 ±0.85 73.60 ±0.83 80.18 ±0.99 92.23 ±0.14 94.34 ±0.04 40.78 ±1.75 42.58 ±3.38
4.1 Common Node Classification Datasets
We first conduct experiments on several commonly used graph
datasets, including three citation networks Cora ,CiteSeer and
PubMed [27]; two co-authorship networks: CSandPhysics [15];
and two heterophilic graphs: Squirrel andChameleon [13], where
neighboring nodes tend to have distinct labels. For citation net-
works, we use the same data splits as in [ 27], which selects 20
nodes from each class as training set, 1,000 nodes in total as valida-
tion set and 500 nodes as test set. For two co-authorship networks,
we follow the splits in [ 15], i.e., 20 labeled nodes per class as the
training set, 30 nodes per class as the validation set, and the rest
as the test set. For the two heterophilic datasets, we follow the re-
cent paper [ 13] that filters out the overlapped nodes in the original
datasets and use its provided data splits.
As displayed in Table 2, all three variants—GeoMix-I, GeoMix-II,
and GeoMix-III—significantly enhance the performance of GCN,
their foundational GNN architecture, across all datasets. In Compar-
ison to other advanced GNNs, they consistently achieve superior
accuracy even using simple GCN as the GNN backbone. Further-
more, each of the three proposed Geometric Mixup variants con-
sistently outperforms state-of-the-art Mixup competitors. These
results suggest that leveraging geometry information for Mixup is
highly effective in improving the performance of GNN and address-
ing challenges caused by limited labeled data. It is likely to yield
superior results compared to Mixup that randomly pairs nodes.
4.2 Handling Distribution Shifts in Unseen
Domains
We proceed to test Geometric Mixup’s capability of handling distri-
bution shifts in OOD generalization tasks. We conduct experiments
onTwitch-explicit dataset, which contains multiple networks
where Twitch users are nodes, and mutual friendships between
them are edges [ 14]. Since each graph is associated to users of a
particular region, distribution shifts occur between different graphs.
We train and validate our model on three graphs: DE,EN,ES, and
perform a random split into 50% training, 25% validation, and 25%
in-distribution-test sets. After training, we directly evaluate the
model on FR,PTandRUdatasets.Table 3: Mean and standard deviation of testing accuracy for
OOD generalization on Twitch-FR, Twitch-PT and Twitch-
RU. All methods use GCN as foundational GNN architecture.
Mothods Twitch-FR Twtich-PT Twitch-RU
ERM 57.98 ±2.41 64.58 ±0.63 59.74 ±3.89
EERM 58.03 ±0.53 65.90 ±0.47 59.71 ±1.87
DANN 52.16 ±5.29 64.92 ±1.17 61.36 ±4.09
Mixup 54.01 ±2.23 66.31 ±0.65 57.32 ±2.29
GraphMix 56.62 ±2.36 65.22 ±0.63 65.73 ±1.29
GeoMix-I 57.82 ±2.61 66.96 ±0.63 64.00 ±1.62
GeoMix-II 61.67 ±1.38 65.58 ±1.46 69.95 ±2.67
GeoMix-III 60.97 ±2.14 65.38 ±1.89 69.55 ±2.46
To make our experiments solid, we not only compare our meth-
ods with state-of-the-art Mixup methods for node classification,
but also include comparisons with EERM and DANN, two of the
most advanced methods designed to address distribution shifts. As
GraphMixup contains an edge prediction module, which relies on
domain knowledge and cannot handle distribution shifts effectively,
we do not include it in this section. We report the results in Table
3. The three Geometric Mixup variants substantially enhance the
performance over ERM, the most basic OOD training approach.
Notably, on Twitch-RU , the relative improvement reaches a re-
markable 17.09%. Furthermore, Geometric Mixup shows superior
performance over other advanced methods across all three datasets,
thus validating the efficacy of our design in improving the gener-
alization capabilities of the underlying GNN. As indicated in [ 10],
the graphs within Twitch-explicit exhibit heterophilic charac-
teristics. Consequently, experiments in this section also underscore
Geometric Mixup’s effectiveness in handling heterophilic graphs.
Moreover, the results presented in Table 3 clearly indicate that
GeoMix-II outperforms GeoMix-I on such graphs. This observation
substantiates the assertions made in Section 3.2.
4505GeoMix: Towards Geometry-Aware Data Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) PU10  PU30
70747882Accuracy
(b) PU30  PU10
58626670Accuracy
(c) PU30  PU50
74778083Accuracy
(c) PU50  PU30
67717579Accuracy
(c) gg qq
64687276Accuracy
(c) qq gg
69737781AccuracyERM EERM Mixup GraphMix GeoMix-I GeoMix-II GeoMix-III
Figure 2: Mean testing accuracy and standard deviation of generalization task in Pileup Mitigation dataset with different
PU conditions and physical processes. Expressions like PU10 →PU30 represent PU condition shifts. 𝑔𝑔→𝑞𝑞and𝑞𝑞→𝑔𝑔
indicate physical processes shifts.
4.3 OOD Generalization in High Energy Physics
Next, we test Geometric Mixup in OOD generalization using Pileup
Mitigation dataset from the realm of High Energy Physics (HEP)
[11]. It comprises multiple graphs, with each corresponding to a
beam of proton-proton collisions. The nodes in each graphs rep-
resent particles generated by these collisions in the Large Hadron
Collider, categorized into primary collisions (LC) and nearby bunch
crossings (OC). Node features encode various physics characteris-
tics of these particles. Graphs are constructed from input features
using KNN method [ 11]. The task is to identify whether a neutral
particle is from LC or OC. The distribution shifts can be attributed
to two sources: first, variations in pile-up (PU) conditions, such as
generalization from PU10 to PU30; second, changes in the types of
the particle decay, for example, generalization from 𝑝𝑝to𝑞𝑞. To
establish a semi-supervised learning setting, for each generalization
task, we choose 10 graphs from the source domain and randomly
allocate 20% of neutral nodes (particles) as the training set, 80%
forming the validation set. For the target domain, we use 20 graphs
and test the model on all the neutral nodes.
This task presents a twofold challenge. Firstly, it necessitates
an in-depth understanding of complex domain knowledge within
the HEP field. Secondly, it involves conditional structure shifts, a
new type of challenging distribution shift identified by [ 11]. The
results are presented in Fig. 2. Despite the substantial challenges,
GeoMix-I, GeoMix-II and GeoMix-III all significantly enhance the
testing accuracy of the underlying GCN across all tasks. Notably,
the most substantial improvements are observed in distribution
shifts caused by different physical processes, which are more de-
manding than shifts arising from variations in PU conditions [ 11].
In𝑔𝑔→𝑞𝑞and𝑞𝑞→𝑔𝑔, Geometric Mixup results in relative
improvements as high as 7.22% and 11.14% over ERM. Addition-
ally, Geometric Mixup consistently outperforms other advanced
competitors throughout all scenarios. These findings demonstratethat with the aid of Geometric Mixup, GNNs can effectively acquire
complex scientific knowledge from limited training data to address
real-world challenges. They also highlight Geometric Mixup’s ca-
pability of addressing distribution shifts between source and target
graphs.
4.4 Image and Text Classification with Low
Label Rates
We extend our experiments to include the STL10 ,CIFAR10 , and
20News datasets to evaluate Geometric Mixup’s performance in
standard classification tasks with limited labeled data. In 20News
provided in [ 12], we select 10 topics and use words with a TF-IDF
score exceeding 5 as features. For STL10 andCIFAR10 , both image
datasets, we initially employ the self-supervised approach SimCLR
[1], which does not use any labels for training, to train a ResNet-18
model for extracting feature maps used as input features. Since
these datasets lack inherent graphs, we utilize the KNN method to
construct input graphs. We leave more details in the appendix.
The results are presented in Table 4. Notably, all three Geometric
Mixup methods consistently outperform GCN, their underlying
GNN, as well as other GNNs across all cases. Furthermore, they
achieve superior results compared to three state-of-the-art Mixup
competitors. These findings underscore the broad applicability of
Geometric Mixup, spanning not only graph-structured datasets but
also image and text classifications where explicit graphs are absent.
4.5 Ablation Study
In this section, we conduct ablation studies to demonstrate the
efficacy and necessity of certain design choices in Geometric Mixup.
Firstly, we aim to assess the improvements brought about by the
utilization of geometry information in Mixup. To achieve this, we
randomly pair nodes for Mixup while keeping all other aspects of
the training pipeline consistent with Geometric Mixup. Secondly,
4506KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
Table 4: Testing accuracy on image ( STL10 and CIFAR10 ) and text ( 20News ) classification. The second column displays the number
of samples per class in the training set.
Dataset#samples
p
er classMLP
GCN GAT SGC Mixup
GraphMixup GraphMix Ge
oMix-I GeoMix-II GeoMix-III
STL1010 66.6±0.8
67.3±0.4 67.1 ±0.6 66.6 ±0.2 67.7±0.8
67.0±0.7 67.7 ±0.7 68.3 ±0.6 68.2±0.6 68.4 ±0.7
20 70.0±0.6
69.7±0.4 69.5 ±0.4 68.9 ±0.2 69.9±0.6
69.7±0.4 70.5 ±0.5 70.5±0.2 70.8 ±0.2 70.9 ±0.4
CIF
AR1010 68.8±0.7
70.0±0.8 70.1 ±0.8 69.6 ±0.7 69.3±1.1
69.7±1.4 69.8 ±1.3 71.1 ±1.6 71.1±1.1 71.3 ±0.6
20 72.0±0.6
71.8±0.5 71.7 ±0.6 71.9 ±0.5 72.0±0.5
71.6±0.6 72.3 ±0.2 72.9 ±0.4 72.9±0.4 73.2 ±0.2
20Ne
ws100 55.9±0.3
56.6±0.3 56.9 ±0.5 55.6 ±0.8 57.5±0.4
57.8±0.6 57.5 ±0.2 58.6 ±0.2 58.2±0.1 58.5 ±0.1
200 60.0±0.3
60.4±0.6 60.8 ±0.4 59.2 ±0.3 60.9±0.6
60.9±0.5 61.0 ±0.7 61.5 ±0.4
61.4 ±0.3 61.3±0.4
Table 5: Results of the ablation studies on Cora,CiteSeer
and Squirrel . In “Random Mix", we randomly pair nodes to
perform Mixup. In “w/o Locality", we remove the locality
enhancement part in GeoMix-I. Δ𝐺𝑒𝑜𝑀𝑖𝑥−𝐼represents the
relative performance degradation compared to GeoMix-I.
Method Cora CiteSeer Squirrel
GCN 81.6 ±0.5 71.6 ±0.3 39.5 ±1.5
GeoMix-I 84.1 ±0.7 75.1 ±0.4 41.0 ±1.1
Random Mix 82.1 ±0.5 69.8 ±1.3 37.8 ±1.3
Δ𝐺𝑒𝑜𝑀𝑖𝑥−𝐼 (-2.38%) (-7.06%) (-7.80%)
w/o Locality 84.0 ±0.6 73.7 ±0.8 38.3 ±1.6
Δ𝐺𝑒𝑜𝑀𝑖𝑥−𝐼 (-0.12%) (-1.86%) (-6.59%)
we seek to understand the effects of locality enhancement, so we
remove the locality enhancement part in GeoMix-I and keep the
other design elements unchanged.
We conduct these experiments on Cora ,CiteSeer andSquirrel ,
with the latter being a heterophilic graph. The results are displayed
in Table 5. There is a substantial drop in performance when we do
not incorporate geometry information. One possible explanation
is that randomly mixing nodes can introduce unwanted external
noise into each node’s receptive field, thus negatively affecting the
accuracy of information exchange during message passing.
After disabling the locality enhancement, noticeable performance
declines are observed in CiteSeer and Squirrel , while no sig-
nificant difference is observed in Cora . These outcomes can be
attributed to homophily. As analyzed in Sec. 3.1, a reduction in
homophily can cause the basic geometry-aware Mixup without
locality enhancement to inadequately preserve locality informa-
tion and ensure the diversity of synthetic data, thereby diminish-
ing the efficacy of Mixup. According to [ 10], even though Cora
andCiteSeer are homophilic graphs, CiteSeer exhibits a lower
homophily ratio. Consequently, there is a more pronounced per-
formance drop in CiteSeer compared to Cora . In the case of the
heterophilic graph Squirrel , the performance drop is even more
substantial, reaching 6.59%. These results substantiate the necessity
of locality enhancement for Geometric Mixup.
4.6 Visualization
Fig. 3 and 4 displays the final-layer node representations learned by
GCN and GeoMix on the Cora and CiteSeer datasets, using T-SNE
(a) GCN on Cora
 (b) GeoMix-I on Cora
(c) GeoMix-II on Cora
 (d) GeoMix-III on Cora
Figure 3: The learned representations of the nodes in the
Cora datasets by GCN and Geometric Mixup. Colors denote
the ground-truth class labels.
(a) GCN on CiteSeer
 (b) GeoMix-I on CiteSeer
(c) GeoMix-II on CiteSeer
 (d) GeoMix-III on CiteSeer
Figure 4: The learned representations of the nodes in the
CiteSeer datasets by GCN and Geometric Mixup.
[17]. The figures reveal that the hidden representations learned
with Geometric Mixup are more discriminative and conducive to
clustering, as nodes from the same class are more tightly clustered,
4507GeoMix: Towards Geometry-Aware Data Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
Cora PubMed PU3050
(a) GAT as backbone.758085Accuracy
Cora PubMed PU3050
(b) APPNP as backbone.758085Accuracyw/o Mixup
GeoMix-IGeoMix-II
GeoMix-III
Figure 5: Results with other underlying GNN architectures.
while nodes from different classes are more distant from each other.
These highly discriminative representations contribute to improved
class predictions.
4.7 Results of Using Other GNN Backbones
In this section, we investigate the versatility of Geometric Mixup by
altering the underlying GNN architectures. Specifically, we employ
GAT [ 19] and APPNP [ 8] as backbone GNNs and evaluate their
performance with all three variants of Geometric Mixup. The results
are presented in Fig. 5. As shown, Geometric Mixup consistently
enhances the performance of GAT and APPNP across both standard
datasets and out-of-distribution (OOD) generalization tasks.
5 Related Works
Graph Neural Networks. Graph Neural Networks (GNNs) have
become the de facto method for modeling graph-structured data.
Among the various types of GNNs, message-passing-based ap-
proaches [ 7,8,19,23,26] have gained prominence by defining
graph convolutions through information propagation. These ap-
proaches generate the representation of a node by aggregating
its own features along with those of its neighbors. Our work is
orthogonal to them in that our model-agnostic Mixup operation
serves as a data preprocessing step to enlarge the training set and
broaden the data distribution, ultimately enhancing performance
and generalization.
Mixup. As discussed in previous studies [ 16,20,28], Mixup is a
highly effective data augmentation technique that generates train-
ing samples through the interpolation of existing samples. However,
Mixup is mostly used in image classification and has rarely been
explored in graph learning tasks, particularly the node classifica-
tion task. When considering node classification, while interpolating
node features and labels to generate synthetic nodes seems intu-
itive, the challenge lies in effectively establishing connections for
these synthetic nodes. Care must be exercised during this processto avoid introducing excessive external noise into the information
propagation mechanism, as it can detrimentally impact the per-
formance of GNNs. In this domain, a few existing works either
avoid explicitly connecting synthetic nodes [ 21,22] or introduce
complex edge prediction modules [ 22]. The former performs Mixup
between layers of neural networks and tightly couples with the
training model, potentially limiting its versatility. Conversely, the
latter compromises on efficiency and generalization capability. To
the best of our knowledge, our research marks the pioneering effort
in integrating the graph geometry into Mixup operation. This inte-
gration allows for the construction of an explicit augmented graph,
wherein synthetic nodes are systematically connected to relevant
nodes. This approach enhances interpretability while maintaining
the efficiency of the Mixup technique.
Generalization on Graph Learning. Owing to the distribution
shifts encountered between real-world testing and training data,
there has been a growing emphasis on enhancing the capacity of
GNNs to perform effectively on out-of-distribution (OOD) data.
One line of work involves the application of adversarial training to
promote the smoothness of the output distribution, such as BVAT
[3] and GraphAT [ 4]. A more recent invariance learning approach,
EERM [ 25], introduces multiple context explorers, which are im-
plemented as graph editors and are adversarially trained. Another
recent work [ 30] proposes learning a generalizable graph structure
learner that can enhance the quality of the input graph structure
when generalizing to unseen graphs, thereby improving the perfor-
mance of the downstream GNN. However, it is worth noting that
these methods introduce significant extra computational costs. In
contrast, Geometric Mixup is more lightweight. It introduces only
a few message-passing-based Mixup operations and operates in
linear time with respect to the number of nodes and edges.
6 Conclusion
This paper proposes Geometric Mixup, a method leveraging geome-
try information for Mixup by interpolating features and labels with
those from nearby neighborhood. We provide theoretic insights
into our approach for utilizing graph structure and emphasizing
the importance of enhancing locality information, a critical design
aspect enabling our method to accommodate to both homophilic
and heterophilic graphs. Additionally, we extend our strategy to
facilitate all-pair Mixup and dynamically learn the mixing weights,
overcoming the challenges posed by noise in the given graph struc-
ture. Our extensive experiments demonstrate that Geometric Mixup
substantially improves the performance of underlying GNNs on
both standard datasets and OOD generalization tasks.
Acknowledgments
This work was in part supported by the National Natural Science
Foundation of China (62222607), and the Shanghai Municipal Sci-
ence and Technology Major Project (2021SHZDZX0102).
4508KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
References
[1]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597–1607.
[2]Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020. Iterative Deep Graph
Learning for Graph Neural Networks: Better and Robust Node Embeddings. In
Advances in Neural Information Processing Systems.
[3]Zhijie Deng, Yinpeng Dong, and Jun Zhu. 2023. Batch virtual adversarial training
for graph convolutional networks. AI Open (2023).
[4]Fuli Feng, Xiangnan He, Jie Tang, and Tat-Seng Chua. 2019. Graph adversarial
training: Dynamically regularizing based on graph structure. IEEE Transactions
on Knowledge and Data Engineering 33, 6 (2019), 2493–2504.
[5]Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019. Learn-
ing discrete structures for graph neural networks. In International conference on
machine learning.
[6]Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learning
research 17, 1 (2016), 2096–2030.
[7]Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks. In International Conference on Learning Representations.
[8]Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2019. Pre-
dict then Propagate: Graph Neural Networks meet Personalized PageRank. In
International Conference on Learning Representations.
[9]Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and
Weining Qian. 2022. Finding Global Homophily in Graph Neural Networks When
Meeting Heterophily. In International Conference on Machine Learning.
[10] Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. 2021. New Benchmarks for
Learning on Non-Homophilous Graphs. arXiv preprint arXiv:2104.01404 (2021).
[11] Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and
Pan Li. 2023. Structural Re-weighting Improves Graph Domain Adaptation. In
International Conference on Machine Learning.
[12] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine learning in Python. the
Journal of machine Learning research 12 (2011), 2825–2830.
[13] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila
Prokhorenkova. 2023. A critical look at the evaluation of GNNs under het-
erophily: Are we really making progress?. In International Conference on Learning
Representations.
[14] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. 2021. Multi-scale attributed
node embedding. Journal of Complex Networks 9, 2 (2021), cnab014.
[15] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).[16] Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Between-class
learning for image classification. In Proceedings of the IEEE conference on computer
vision and pattern recognition.
[17] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems.
[19] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations.
[20] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
David Lopez-Paz, and Yoshua Bengio. 2019. Manifold mixup: Better represen-
tations by interpolating hidden states. In International conference on machine
learning.
[21] Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho
Kannala, and Jian Tang. 2021. Graphmix: Improved training of gnns for semi-
supervised learning. In Proceedings of the AAAI conference on artificial intelligence.
[22] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixup
for node and graph classification. In Proceedings of the Web Conference 2021.
[23] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and
Kilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. In
International Conference on Machine Learning. 6861–6871.
[24] Lirong Wu, Jun Xia, Zhangyang Gao, Haitao Lin, Cheng Tan, and Stan Z Li. 2022.
Graphmixup: Improving class-imbalanced node classification by reinforcement
mixup and self-supervised context prediction. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. Springer.
[25] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. 2022. Handling Distri-
bution Shifts on Graphs: An Invariance Perspective. In International Conference
on Learning Representations.
[26] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International conference on machine learn-
ing.
[27] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-
supervised learning with graph embeddings. In International Conference on Ma-
chine Learning.
[28] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In International Conference on Learn-
ing Representations.
[29] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
2021. How does mixup help with robustness and generalization?. In International
Conference on Learning Representations.
[30] Wentao Zhao, Qitian Wu, Chenxiao Yang, and Junchi Yan. 2023. GraphGLOW:
Universal and Generalizable Structure Learning for Graph Neural Networks. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining.
4509GeoMix: Towards Geometry-Aware Data Augmentation KDD ’24, August 25–29, 2024, Barcelona, Spain
A Proof of Theorem 1 and 2
To simplify the analysis, assume that we use the invert of the degree
of central node as mixing weight, i.e., 𝑒𝑖𝑗=1/𝑑𝑒𝑔(𝑖). Then, we have
E[h𝑖]=E∑︁
𝑗∈N(𝑖)1
𝑑𝑒𝑔(𝑖)h𝑗
=1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)E[h𝑗]
=1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)∑︁
𝑐∈𝐶P(𝑦𝑗=𝑐|𝑦𝑖)𝝁(𝑐)
=𝑝𝝁(𝑦𝑖)+1−𝑝
𝐶−1∑︁
𝑐≠𝑦𝑖𝝁(𝑐).
To prove Eq. (10), we first introduce the Hoeffding’s inequality.
Lemma 1 (Hoeffding’s Ineqality). Let𝑋1,...,𝑋𝑛be indepen-
dent random variables such that 𝑎≤𝑋𝑖≤𝑏for all𝑖. Then
P 1
𝑛𝑛∑︁
𝑖=1(𝑋𝑖−E[𝑋𝑖])≥𝑡!
≤2 exp
−2𝑛𝑡2
(𝑏−𝑎)2
for all𝑡≥0.
Leth𝑖[𝑘]denote the𝑘-th element of h𝑖. If∥1
𝑑𝑒𝑔(𝑖)Í
𝑗∈N(𝑖)(h𝑖−
E[h𝑖])∥2≥√
𝐹𝑡1, then at least for one 𝑘∈{1,...,𝐹}, the inequality1
𝑑𝑒𝑔(𝑖)Í
𝑗∈N(𝑖)(h𝑖[𝑘]−E[h𝑖[𝑘]])≥𝑡1holds. Therefore,
P©­
«1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)(h𝑗−E[h𝑗])2≥√
𝐹𝑡1ª®
¬
≤P©­
«𝐹Ø
𝑘=1 
1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)(h𝑗[𝑘]−E[h𝑗[𝑘]])≥𝑡1 
ª®
¬
≤𝐹∑︁
𝑘=1P©­
«1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)(h𝑗[𝑘]−E[h𝑗[𝑘]])≥𝑡1ª®
¬
≤2𝐹exp 
−𝑑𝑒𝑔(𝑖)𝑡2
1
2𝐵2!
(By Hoeffding’s inequality) .
Let𝑡=√
𝐹𝑡1, then we have
P©­
«1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)(h𝑗−E[h𝑗])2≥𝑡ª®
¬≤2𝐹exp
−𝑑𝑒𝑔(𝑖)𝑡2
2𝐵2𝐹
.
The LHS of the above equation is equal to P(∥h𝑖−E[h𝑖]∥2≥𝑡), so
we complete the proof of Theorem 1.
Next we derive E[¯y𝑖](w.l.o.g, assume 𝑦𝑖=𝑐).
E[¯y𝑖]
=E∑︁
𝑗∈N(𝑖)1
𝑑𝑒𝑔(𝑖)¯y𝑗
=1
𝑑𝑒𝑔(𝑖)∑︁
𝑗∈N(𝑖)E[¯y𝑗]
=𝑝(1−𝜖)e𝑐+𝑝𝜖
𝐶−1∑︁
𝑗≠𝑐e𝑗+1−𝑝
𝐶−1∑︁
𝑗≠𝑐(1−𝜖)e𝑗+𝜖
𝐶−1∑︁
𝑘≠𝑗e𝑘
=𝑝(1−𝜖)e𝑐+𝑝𝜖+(1−𝑝)(1−𝜖)
𝐶−1∑︁
𝑗≠𝑐e𝑗+𝜖(1−𝑝)
(𝐶−1)2∑︁
𝑗≠𝑐∑︁
𝑘≠𝑗e𝑘.
(26)Note that∑︁
𝑗≠𝑐∑︁
𝑘≠𝑗e𝑘=(𝐶−2)∑︁
𝑗≠𝑐e𝑗+(𝐶−1)e𝑐. (27)
Substituting Eq. (27)into Eq. (26)and rearranging the resulting
expression, we obtain:
E[¯y𝑖]=
𝑝(1−𝜖)+𝜖(1−𝑝)
𝐶−1
e𝑐+𝑝𝜖+(1−𝑝)(1−𝜖)
𝐶−1
+𝜖(1−𝑝)(𝐶−2)
(𝐶−1)2∑︁
𝑗≠𝑐e𝑗,
which completes the proof of Eq. (11).
The proof of Eq. (12)in Theorem 2 closely resembles the proof of
Eq.(10)in Theorem 1, which uses the Hoeffding’s inequality. The
main distinction lies in the feature dimension being 𝐶instead of𝐹,
and the bound’s value being 1 instead of 𝐵.
B Proof of Theorem 3
The first-order derivative of 𝐹1(H;H(𝑡))w.r.t h𝑢is
𝜕𝐹1(H;H(𝑡))
𝜕h𝑢=2(h𝑢−h(𝑡)
𝑢)+2𝛽∑︁
(𝑢,𝑣)∈E𝑒𝑢𝑣(h𝑢−h𝑣).(28)
Applying the Forward Euler method with step size 𝜏to PDE
𝜕h(𝑡)
𝑢
𝜕𝑡=−𝜕
𝜕h𝑢𝐹1(H;H(𝑡)), (29)
which updates h𝑢in the direction of gradient descent, we obtain
h(𝑡+1)
𝑢−h(𝑡)
𝑢
𝜏=−𝜕𝐹1(H;H(𝑡))
𝜕h𝑢H=H(𝑡)
=−2(h(𝑡)
𝑢−h(𝑡)
𝑢)−2𝛽∑︁
𝑣∈N(𝑢)𝑒𝑢𝑣(h(𝑡)
𝑢−h(𝑡)
𝑣)
=−2𝛽∑︁
𝑣∈N(𝑢)𝑒𝑢𝑣(h(𝑡)
𝑢−h(𝑡)
𝑣).
After rearranging the equation, we have
h(𝑡+1)
𝑢 =(1−2𝜏𝛽)h(𝑡)
𝑢+2𝜏𝛽∑︁
𝑣∈N(𝑢)𝑒𝑢𝑣h(𝑡)
𝑣.
Setting𝛼=2𝜏𝛽yields Eq. (18).
Substituting 𝐹1(H;H(𝑡))in Eq. (29)with𝐹2(H;H(0)), we obtain
h(𝑡+1)
𝑢−h(𝑡)
𝑢
𝜏=−𝜕𝐹2(H;H(0))
𝜕h𝑢H=H(𝑡)
=−2(h(𝑡)
𝑢−h(0)
𝑢)−2𝛽∑︁
𝑣∈N(𝑢)𝑒𝑢𝑣(h(𝑡)
𝑢−h(𝑡)
𝑣).
Rearranging the equation yields
h(𝑡+1)
𝑢 =(1−2𝜏−2𝜏𝛽)h(𝑡)
𝑢+2𝜏h(0)
𝑢+2𝜏𝛽∑︁
𝑣∈N(𝑢)𝑒𝑢𝑣h(𝑡)
𝑣.
Setting𝜏=1
2(𝛽+1)and𝛼=2𝜏yields Eq. (15).
4510KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan
0.1 0.2 0.4 0.5 0.7 0.8
70758085Accuracy
GeoMix-I
0.1 0.2 0.4 0.5 0.7 0.8
70758085Accuracy
GeoMix-II
0.1 0.2 0.4 0.5 0.7 0.8
70758085Accuracy
GeoMix-III
1 2 3 4 5
K70758085Accuracy
GeoMix-I
1 2 3 4 5
K70758085Accuracy
GeoMix-II
1 2 3 4 5
K70758085Accuracy
GeoMix-IIICora CiteSeer
Figure 6: Impacts of 𝛼and Mixup hop 𝐻on Geometric Mixup.
C Additional Experimental Details
ForSTL10 , we utilize all 13,000 images, each categorized into one
of the ten classes. For CIFAR10 , we choose 1,500 images from each
of 10 classes and obtain a total of 15,000 images. In these two image
datasets, we randomly select 10/20 images per class as training set,
4,000 images in total as validation set and the remaining instances
as testing set. We also evaluate our model on 20News , which is a text
classification dataset consisting of 9,607 instances. We follow [ 5] to
take 10 classes from 20 and use words (TF-IDF) with a frequency
of more than 5% as features. In this dataset, we randomly select
100/200 instances per class as training set, 2,000 instances in total
as validation set and the remaining ones as testing set.
We implement our approach using PyTorch. All experiments are
conducted on an NVIDIA GeForce RTX 2080 Ti with 11GB memory.
Grid search is used on validation set to tune the hyper-parameters.
The learning rate is searched in {0.001, 0.005, 0.01, 0.05}; dropout
rate is searched in {0, 0.2, 0.3, 0.5, 0.6}; weight decay is searched in[1e-5, 1e-2]. Other hyper-parameters for specific models are listed
below.
•GCN: Hidden dimension ∈{16, 32, 64}; number of layers is 2.
•GAT: Hidden dimension ∈{8, 16, 32, 64}; number of heads ∈
{4, 6, 8}; number of layers is 2.
•SGC: Hops∈{2, 3}.
•APPNP: Hidden dimension ∈{16, 32, 64}; 𝛼∈{0.1, 0.2, 0.5};
hops∈{5, 10}.
•GloGNN:𝛼∈[0,1],𝛽1∈{0,1,10},𝛽2∈{01,1,10,100,1000},
𝛾∈[0,0.9], number of norm layers ∈{1,2,3},𝐾∈[1,6].
•Mixup: We set the number of layers to 3, as suggested by its
author. The hidden dimension used by the author is 256, and
we search it in {64, 128, 256}. We search 𝛼in [0.5, 5].
•GraphMixup: Hidden dimension ∈{16, 64, 128}; number of
layers is 2; semantic relation 𝐾=4; loss weights 𝛼=1.0.
•GraphMix: We use the same number of layers and hidden
size as underlying GCN, as suggested by its author; 𝛼∈{0.0,
0.1, 1.0, 2.0}; 𝛾∈[0.1, 10]; temperature 𝑇=0.1; number of
permutations 𝐾=10.
•EERM: weight for combination 𝛽∈{0.2, 0.5, 1.0, 2.0, 3.0};
number of edge editing for each node 𝑠∈{1, 5, 10};𝐾∈{3,
5}; number of iterations for inner update 𝑇∈{1, 5}; we use
the same hidden dimension and number of layers as GCN.
•DANN:𝜆∈[0.2, 6]; number of layers is 2; hidden dimension
∈{16, 32, 64}.
•GeoMix-I and GeoMix-II: Mixup hops (number of consec-
utive Mixup) 𝐾∈{1, 2, 3, 4};𝛼∈[0.1, 0.8]; the number of
layers and hidden dimension are the same as GCN; 𝜆is set
to 1 except in a few experiments.
•GeoMix-III: Mixup hops 𝐾∈{1, 2, 3, 4};𝛼∈[0.1, 0.8]; graph
weight𝜂∈{0.3, 0.5, 0.7, 0.8}; projection dimension 𝐹′=16;
the number of layers and hidden dimension are the same as
GCN;𝜆is set to 1.
D Hyper-parameter Analysis
We present the accuracy of Geometric Mixup concerning both 𝛼and
Mixup hops 𝐾in Fig. 6. Remarkably, using 2 hops generally yields
the most competitive performance. Therefore, by setting Mixup
hops to 2, we can achieve desirable performance with minimal
overhead.
4511