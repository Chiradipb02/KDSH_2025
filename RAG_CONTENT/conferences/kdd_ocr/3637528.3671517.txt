Enhancing Asymmetric Web Search through Question-Answer
Generation and Ranking
Dezhi Ye
390959029@qq.com
dezhiye@tencent.com
Tencent PCG
Beijing, ChinaJie Liu
jesangliu@tencent.com
Tencent PCG
Beijing, ChinaJiabin Fan
robertfan@qq.com
Tencent PCG
Beijing, China
Bowen Tian
lukatian@tencent.com
Tencent PCG
Beijing, ChinaTianhua Zhou
kivizhou@tencent.com
Tencent PCG
Beijing, ChinaXiang Chen
joshuaxchen@tencent.com
Tencent PCG
Beijing, China
Jin Ma
daniellwang@tencent.com
Tencent PCG
Beijing, China
Abstract
This paper addresses the challenge of the semantic gap between user
queries and web content, commonly referred to as asymmetric text
matching, within the domain of web search. By leveraging BERT
for reading comprehension, current algorithms enable significant
advancements in query understanding, but still encounter limita-
tions in effectively resolving the asymmetrical ranking problem
due to model comprehension and summarization constraints.
To tackle this issue, we propose the QAGR (Question-Answer
Generation and Ranking) method, comprising an offline module
called QAGeneration and an online module called QARanking. The
QAGeneration module utilizes large language models (LLMs) to
generate high-quality question-answering pairs for each web page.
This process involves two steps: generating question-answer pairs
and performing verification to eliminate irrelevant questions, re-
sulting in high-quality questions associated with their respective
documents. The QARanking module combines and ranks the gen-
erated questions and web page content. To ensure efficient online
inference, we design the QARanking model as a homogeneous
dual-tower model, incorporating query intent to drive score fusion
while balancing keyword matching and asymmetric matching. Ad-
ditionally, we conduct a preliminary screening of questions for each
document, selecting only the top-N relevant questions for further
relevance calculation.
Empirical results demonstrate the substantial performance im-
provement of our proposed method in web search. We achieve over
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36715178.7% relative offline relevance improvement and over 8.5% online
engagement gain compared to the state-of-the-art web search sys-
tem. Furthermore, we deploy QAGR to online web search engines
and share our deployment experience, including production con-
siderations and ablation experiments. This research contributes to
advancing the field of asymmetric web search and provides valuable
insights for enhancing search engine performance.
CCS Concepts
•Information systems →Language models; Web search en-
gines; Rank aggregation.
Keywords
web search; search ranking; large language models; question answer
ACM Reference Format:
Dezhi Ye, Jie Liu, Jiabin Fan, Bowen Tian, Tianhua Zhou, Xiang Chen,
and Jin Ma. 2024. Enhancing Asymmetric Web Search through Question-
Answer Generation and Ranking. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671517
1 Introduction
Web search plays a crucial role in our daily lives, providing ac-
cess to vast amounts of information and enabling us to find so-
lutions, discover new knowledge, and stay informed on various
topics [ 11,14,24]. Billions of users rely on search engines each
day, using both "keyword-ese" and "natural language" queries to
acquire information. Traditional search engines heavily relied on
term matching techniques, which worked well for "keyword-ese"
queries but struggled with natural language questions. However, re-
cent advancements in search engine technology have incorporated
BERT-based [ 4] methods to improve the understanding of context
and semantics [ 37,40]. By encoding queries and documents as vec-
tor representations and calculating their similarity, search engines
6127
KDD ’24, August 25–29, 2024, Barcelona, Spain Dezhi Ye et al.
can generate more relevant search results [ 8,12,15]. Interactive
BERT models have also been employed in the ranking phase, bene-
fiting from evaluating a limited number of candidates [ 17,32,44].
The integration of BERT into search engine techniques represents
a significant advancement, enhancing query understanding and
search result quality.
However, BERT-based search engines still face unique challenges
in asymmetric web search, particularly with "natural language"
queries1. The disparity between the query and the description of
web content poses difficulties in extracting answers from para-
graphs and determining web page relevance based on those answers,
which differs fundamentally from symmetric keyword matching.
Asymmetric web search typically involves concise queries, such as
questions or a few keywords, aiming to find longer paragraphs that
provide answers. For instance, given the query "How long ago did
Zhang Zifeng first participate in ’Back to Field’?", the target web
page may contain the sentence "Zhang Zifeng made her debut on
the variety show ’Back to Field’ in the fifth episode of the second
season". The negative document "The ’Back to Field’ Season seven
Zhang Zifeng will participate. " contains well-matched keywords,
but it fails to address the query’s question. In such cases, the dispar-
ity between the query and the description of web content presents
challenges in extracting answers from paragraphs and determining
web page relevance based on those answers.
To enhance search engine performance in asymmetric web search
and overcome vocabulary mismatches, researchers have proposed
various methods, including document expansion techniques. One
such approach is the utilization of Doc2Query [ 9,21,22,28,29],
where the document serves as input and potential questions that
the document might answer are generated. These predicted ques-
tions are then appended to the original documents, which are then
indexed as before. By employing indexing techniques that cater to
new generated queries, Doc2query has demonstrated remarkable
effectiveness in information retrieval, thereby mitigating the issue
of vocabulary mismatch [ 6]. However, challenges remain in apply-
ing Doc2Query methods on a large scale in web search. Two key
challenges need to be addressed: C1) Generating high-quality
question-answer pairs. Sequence-to-sequence models [ 27,36] are
indeed susceptible to generating content that does not accurately
reflect the input text, a flaw commonly referred to as "hallucination"
[43]. These poorly generated queries can be detrimental to existing
web search systems. Therefore, it is crucial to ensure high-quality
generated queries to minimize any potential harm. C2) Integrat-
ing generated questions into existing ranking systems while
balancing keyword and semantic relevance and considering
system latency. Numerous approaches solely employ the gener-
ated queries during the recall phase, with few attempts made to
incorporate these generated queries during the ranking stage. This
limitation hampers the potential benefits of the Doc2query method
on existing systems.
To address C1, we propose the QAGeneration module, which
leverages Language Models (LLMs) to generate high-quality question-
answer pairs for each document. First of all, QAGeneration keeps
core paragraphs from web pages using T5 to reduce calculations.
1https://blog.google/products/search/search-language-understanding-bert/A webpage typically comprises multiple paragraphs of informa-
tion, but only a portion of them are valuable, while the remaining
parts are irrelevant or meaningless. The extraction of the core para-
graph from a webpage aims to filter out the most relevant and
useful information. LLMs then generate questions and answers,
with the answers extracted from the document paragraphs. This
approach minimizes the generation of "hallucination" outputs by
grounding the answers in the actual content of the document. More-
over, we adopt a human-loop instruct tuning approach to make
the generated questions more aligned with human preferences. At
last, QAGeneration verifies and filters the question-answer pairs
based on the document’s content, ensuring the retention of only
high-quality and relevant pairs.
To overcome C2, we introduce the QARanking module, which
effectively integrates the generated questions into existing retrieval
systems and ranks web pages. Firstly, QARanking performs an
initial screening of the generated questions based on the user’s
query, selecting the top-N most relevant questions by calculat-
ing the text similarity between queries and questions. Due to the
stringent latency demands in the online setting, we utilize the Cov-
erage Query Rate (CQR) as a metric for assessing the extent to
which the question covers the terms in the query. This reduces
computational overhead and minimizes the impact of irrelevant
questions on the scoring process. Then, a symmetric dual-tower
BERT model calculates query-content matching scores and query-
question matching scores without incurring additional time over-
head. The query-content matching score measures the similarity
between the web page’s content and the user’s query, while the
query-question matching score assesses the relevance between the
query and the selected questions. At last, these scores are combined
using a predefined user intent to determine the final ranking score.
The symmetric dual-tower architecture enables parallel computa-
tion, ensuring minimal impact on overall inference time.
We summarize our contributions as follows:
•We introduce the QAGR (Question-Answer Generation and
Ranking) methodology for web search. The effectiveness of
QAGR is demonstrated through offline and online testing,
showcasing its efficacy.
•We propose a QAGeneration module, which enables the of-
fline generation of high-quality question-answer pairs, lever-
aging the power of LLMs.
•We present a QARanking module that seamlessly integrates
keyword matching scores and the asymmetric query-question
matching scores without introducing additional online pro-
cessing time.
2 Related Work
2.1 Query Generation for Web Search
Query Generation for web search involves the automatic generation
of effective and relevant search queries to retrieve desired informa-
tion from the web [ 10,31,38,41,42]. Recent researches in query
generation focus on generating queries given specific contexts or
information needs. Approaches such as Doc2query [ 22] predict
queries that are relevant to a given document using sequence-to-
sequence transformers or T5 models. However, these methods may
6128Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
Title paragraph Question1Question2
DocumentsWhat is transformer in natural language processing. PromptOffline Stage Online  Stage 
ExtractionParagraphAtransformeris adeep learningarchitecture that relies on the parallel multi-headattentionmechanism.LLMsQuestionAnswerVerificationExpanded Doc:Atransformeris adeep learningarchitecture…What is transformer model?User QueryWeb Search EnginesIndexThousandsRecallRankTitle paragraph Question1Question2
HundredsQuestion Answer GenerationQuestion Answer RankingQuestionAnswerTitle paragraph Question1Question2
Figure 1: The architecture of the QAGR (Question-Answer Generation and Ranking) methodology.
suffer from hallucination issues, leading to poor retrieval effec-
tiveness and increased index size. To address this, Doc2query– [ 9]
introduces a filtering phase to remove low-quality queries before
indexing. While existed approaches [ 9,22,41] primarily focus on
query generation during the retrieval phase, limited attention has
been given to incorporating generated queries during the rank-
ing phase. In contrast, our work aims to enhance query generation
using Large Language Models (LLMs) [ 35] with human-loop. Impor-
tantly, our research explores the integration of generated questions
at scale during the ranking stage, which is a novel practice.
2.2 Large Language Models for Web Search
The application of Large Language Models (LLMs) in web search
primarily focuses on two aspects: generating samples [ 3,7,30] and
direct ranking [ 25]. InPars [ 1] employ LLMs to generate synthetic
data which is used to finetune neural reranker models, which benifit
information retrieval tasks. InPars-v2 [ 13] replaces the LLM from
GPT-3 [ 2] with GPT-J (6B) and demonstrates improved performance
on various datasets. InPars-Light conducts a reproducibility study
of InPars and proposes cost-effective improvements. Direct ranking
using LLMs involves feeding the query and candidate documents
into the prompt. LRL Reranker [ 16] achieves strong reranking effec-
tiveness without relying on task-specific training data. RankGPT
[33] explores the use of generative LLMs, such as ChatGPT [ 19]
and GPT-4 [ 23], for relevance ranking in information retrieval. PRP
[26] analyzes the pointwise and listwise ranking prompts used in
existing methods and highlights the challenges in fully understand-
ing these ranking formulations. While ranking documents using
LLMs has shown promising results, deploying LLMs at scale during
the online phase presents computational challenges. In contrast toexisting methodologies, our research specifically addresses the chal-
lenges of offline question generation, followed by online utilization
of these questions to resolve the problem of asymmetric web search.
By employing this approach, we circumvent the time-consuming
predicament of online LLMs inference, while simultaneously har-
nessing the full potential of LLMs’ cognitive prowess.
3 Methodology
3.1 Task Formulation
The task we are addressing is asymmetric web page search. In this
task, given a query and a collection of web pages, the goal is to
rank the web pages based on their relevance to the query. Let 𝑞∈Q
denote a query and 𝐷∈Ddenotes a set of documents, we are
required to find a scoring function∫
(.). Utilizing function∫
(𝑞,𝑑)
to discern the most pertinent document for the user. However,
relying solely on∫
(𝑞,𝑑)to represent the relevance score poses
challenges in addressing asymmetric matching problems, especially
when users employ natural language queries. To address this issue,
we extend the problem to∫
(𝑞,𝑑,𝑞′), where𝑞′∈Q′is generated
from LLMs.
3.2 Question-Answer Generation and Ranking
Architecture
To enhance asymmetric web search, we propose the Question-
Answer Generation and Ranking (QAGR) method, which aims to
automatically generate relevant questions and their corresponding
answers from a given text passage or document. Subsequently, the
generated questions are indexed and ranked with the content of
the documents, aiming to obtain the most satisfactory results. The
QAGR method consists of an offline module, QAGeneration, and an
6129KDD ’24, August 25–29, 2024, Barcelona, Spain Dezhi Ye et al.
online module, QARanking. The offline module is responsible for
generating high-quality question-answer pairs for each document,
while the online module assigns scores based on the document’s
title, summary, and questions to determine the final relevance score.
The ultimate score is a combination of∫
(𝑞,𝑑,𝑞′)and the score from
the QARanking module. Figure 1 shows the QAGR methodology.
The overall architecture can be summarized as follows: during
the offline phase, for each document, we first select the core para-
graphsP𝐷={𝑃0
𝐷,𝑃1
𝐷,·,𝑃𝑛
𝐷}from web content and then use LLMs
to generate question-answer pairs (Q′,A)𝐷=(𝑄′,𝐴)0
𝐷,(𝑄′,𝐴)1
𝐷,···
,(𝑄′,𝐴)𝑛
𝐷based on the core paragraph. Finally, we validate the gen-
erated question-answer pairs Q′𝐷andA𝐷for their effectiveness
and incorporate the final pairs into the documents. In the online
phase, the retrieval system incorporates the generated questions
Q′𝐷as part of the content 𝐷for recall. During the ranking phase,
we first select the most relevant questions for each document based
on the current query and then assess the relevance of these ques-
tions, along with the title and summary, through a scoring process∫
(𝑞,𝑑,𝑞′). The final relevance score is determined based on these
assessments.
4 Question Answer Generation
In this section, we provide a detailed introduction to three important
tasks within the QAGeneration module that utilize the generated
questions. Figure 2 shows the overview of the Question Answer
Generation (QAG) framework.
4.1 Core Paragraph Extraction
The first task involves filtering the paragraphs Pwithin the docu-
mentDto remove those that lack meaningful content. This filtering
process serves two purposes: eliminating redundant information
to facilitate the generation of effective questions and reducing the
input for subsequent models, thereby improving the training and
inference speed of the generation model. To achieve this, we employ
the T5 base model for core paragraph extraction. The T5 model is
trained using supervised learning on a manually annotated dataset
of 5k samples. With the trained model, we perform core paragraph
extraction on a large-scale dataset consisting of tens of billions of
documents.
4.2 Human-Loop Question Answer Generation
After eliminating redundant information, we leverage Large Lan-
guage Models (LLMs) to generate question-answer (Q′,A)𝐷sam-
ples for each content’s core paragraph through a human-loop pro-
cess. LLMs possess strong summarization [ 34] and generation capa-
bilities [ 5], making them well-suited for question-generation tasks.
Initially, we employ a prompt-based approach on a seed set to gen-
erate samples, incorporating answer extraction alongside question
generation. This approach leverages the question-answer relation-
ship to mitigate LLMs’ tendency for hallucination and improve the
quality of the generated outcomes. However, directly utilizing these
generated question-answer pairs is challenging due to two primary
reasons: Firstly, LLMs have limited comprehension of specific in-
structions, resulting in generated formats and outcomes that may
not meet requirements. Secondly, instruct-tuning, achieved through
manual refinement of generated samples, significantly enhances
ParagraphAtransformeris adeep learningarchitecture that relies on the parallel multi-headattentionmechanism.LLMs
Question –Answer pairsQ: What Is a Transformer Model ?A: Atransformeris adeep learningarchitecture …
DocumentT5Paragraph
Verification
QuestionT5AnswerDocumentT/FHuman RulersQuestionAnswerFinal dataExtractionQuestionAnswerReversionPrompt
SampleInstruct tuning
Fine-TuneHuman-Loop Question Answer Generation
Paragraph: {# Paragraph}Question:Answer:Zero-shot prompt As a quality Question-Answer-Generation-System, your task is to generate effective and concise questions and answers based on the core sentence of the paragraph.
Title:Transformer…Content:From Wikipedia, the free encyclopedia…Atransformeris adeep learningarchitecture that…QuestionAnswer
Fine-TuneFigure 2: Illustration of Question Answer Generation(QAG)
framework.
the model’s performance in vertical domains. Therefore, we subject
the generated samples to instruct-tuning by manually refining their
quality and incorporating them into the model. Through several
iterations, LLMs acquire strong question-generation capabilities.
4.3 Question Answer Verification
Although instruct-tuning improves control over the model’s gener-
ated results, there may still be instances of invalid question gener-
ation. For example, generated questions may remain unanswered
within the web page, or the question-answer pairs may lack proper
alignment. These invalid outcomes can have a detrimental impact
on the existing web search system. To address this, we train a
question-answering (QA) model within the reading comprehension
framework to assess the validity of the generated question-answer
pairs. The training samples for the QA model are generated from
the previous step, where the manually corrected samples are con-
sidered positive examples, and the samples before correction are
considered negative examples. Given a document and a question-
answer pair, the QA model assigns a score to them, which can be
used to filter out invalid question-answer Q′𝐷andA𝐷pairs gen-
erated from the document. Additionally, we employ simple manual
rules, such as verifying the presence of pronouns in the question,
to aid in the verification process.
5 Question Answer Ranking
After generating the question-answer pairs offline, the challenge
lies in integrating them with the existing web search system without
increasing online latency. In the web search system, search results
undergo a recall phase followed by ranking. During the recall phase,
inverted index and vector retrieval methods are primarily employed
to retrieve relevant documents. Hence, the generated questions can
be incorporated into the index construction by treating them as
part of the document’s content. In the ranking phase, the web
search system needs to rank the top-N retrieved documents, which
6130Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
involves a trade-off between effectiveness and performance. In
this paper, we provide a detailed description of the sorting phase.
Figure 3 shows the overview of Question Answer Ranking(QAR)
framework.
QueryQuestion Answer RankingSummaryZhang Zifengmade her debut on the variety show"Back to Field" in the fifth episode of the second season, which aired on May 18, 2018How long ago didZhang Zifengfirst participatingin "Back to Field”?TitleAn introduction to "Back to Field”? QuestionsQ1: In which episode did Zhang Zifengfirst participate in "Back to Field"?Q2: When was Zhang Zifeng'sfirst appearance on "Back to Field"? Q3: What role did Zhang Zifengplay in the third season of "Back to Field"? Q4 : In which episodes of the third season of "Back to Field" did Zhang Zifengappear? Pre-sorting:Calculate the Text similarity between Query and QuestionsQuestionsQ1: In which episode did Zhang Zifengfirst participate in "Back to Field"?Q2: When was Zhang Zifeng'sfirst appearance on "Back to Field"? BERT
BERTRelevance scoreSymmetric Siamese BERT
intentsParameter SharingParameter Sharing
Figure 3: Illustration of Question Answer Ranking(QAR)
framework.
5.1 Pre-sorting Generated Questions
For each document, there are K generated questions Q′𝐷=[𝑄′1
𝐷,𝑄′2
𝐷,
···,𝑄′𝑘
𝐷]. We pre-sort the generated questions based on their rel-
evance to the user query, aiming to select the top-N questions
[𝑄′1
𝐷,𝑄′2
𝐷,···,𝑄′𝑁
𝐷]that are most pertinent to the current query
for each document. We calculate the proportion of terms covered
by each question in the user query (CQR) using a text matching
approach.
CQR=|𝑄∩𝑄′|
|𝑄|(1)
Based on the size of CQR, we create an inverted index on the
questions and select the top-N questions with the highest CQR.
This approach prevents interference from irrelevant questions and
reduces the number of questions involved in subsequent computa-
tions.
5.2 Symmetric Siamese BERT Ranking
Existing web search ranking systems typically utilize BERT to score
the query and document, and subsequently sort them based on the
relevance score. In the case of asymmetric matching, our task is
to score the documents and their questions based on user queries.
A naive approach would be to directly concatenate the generated
questions to the original title and summary, creating a new field to
be passed into the BERT model. However, this approach presents
two challenges: 1. Since the transformer model is fully attention-
based, there is unnecessary interaction between the question and
the title/summary, which not only wastes computation but also
hampers the model’s effectiveness. 2. The time complexity of the
transformer model is O(𝐿𝐻(𝑆2)), where𝑆is the sequence length, 𝐻
is hidden size, and 𝐿is the number of transformer layers. Therefore,
concatenating the questions would significantly increase the input
sequence length and subsequently raise the time complexity. Hence,
we propose the structure of the Symmetric Siamese BERT to ad-
dress these concerns. Figure 4 provides an overview of the model’sstructure, which includes the Query-Title-Summary (Q-T-S) tower,
the Query-Question (Q-Q’) tower, and the query intent.
BERTCLS𝑄!𝑄"𝑄#SEP𝐸$%&𝐸!𝐸"𝐸#𝐸'() CLS𝐻!𝐻"𝐻#𝐻&*+𝐻,…
Query𝑄,…SEP𝑄,Question𝐸,𝐸&*+…𝐸,𝐻&*+𝐻,BERTCLS𝑄!𝑄"𝑄#SEP𝐸$%&𝐸!𝐸"𝐸#𝐸'() CLS𝐻!𝐻"𝐻#𝐻&*+𝐻,…
Query𝑇,…SEPT!Title𝐸,𝐸&*+…𝐸,𝐻&*+𝐻,
𝑆,…SEP𝑆,𝐸,𝐸&*+…𝐸,Summary𝐻,…𝐻&*+𝐻,DenseParameter SharingIntents
ConcatMargin Rank Loss + Point Wise LossAdditional computationalAlready computational
Figure 4: Model structure of Symmetric Siamese BERT.
5.2.1 Q-T-S & Q-Q’ Tower. Q-T-S Tower: Given a query, title, and
summary, we utilize [ SEP] as the separator and [ CLS] as the rep-
resentation vector, concatenated in the form of [ CLS] query [ SEP]
title [ SEP] summary [ SEP]. Then, we tokenize it into WordPiece
tokens {𝑞0,···,𝑞𝑛}. Unlike the original BERT, we differentiate the
segment embedding for the title and summary, which the former
refers to 1, while the latter refers to the 2. The sequence of input
tokens is then passed into BERT, and we ultimately use the [ CLS]
vector as the representation for the overall score.
Q-Q’ Tower: Given a query and questions from the documents,
the input format remains the same as the Q-T-S tower, using the
same separator symbols. The only difference is that the Q-Q’ Tower
uses segment types to differentiate the question from other types.
This is done to maintain uniqueness while sharing parameters. The
[CLS] representation is used as the overall score.
𝐸𝑞,𝑡,𝑠=BERT(𝑄,𝑇,𝑆)
𝐸𝑞,𝑞′=BERT(𝑄,𝑄′)
𝑓𝑞,𝑡,𝑠,𝑞′=𝜎(FNN(𝐸𝑞,𝑡,𝑠,𝐸𝑞,𝑞′,𝐼𝑞))(2)
5.2.2 Efficiency and Complexity Analysis. We conduct the time
complexity analysis to inspect the efficiency of the proposed ap-
proach. Based on the analysis, the time complexity of the Q-T-S
tower isO(𝐿𝐻(𝑆2)), and the time complexity of the Q-Q’ tower
is alsoO(𝐿𝐻(𝑆′2)), where𝑆is the length of query, title and sum-
mary,𝑆′is the length of query and question, 𝐻is the hidden size,
and𝐿is the number of transformer layers. Generally speaking, the
length of𝑄′is much smaller compared to 𝑇+𝑆. With the advan-
tage of multi-GPU computation, these two towers can be computed
in parallel, resulting in a time complexity of O(𝐿𝐻(𝑆2))for the
Siamese BERT model. While for origin BERT, the time complexity
isO(𝐿𝐻((𝑆+𝑆′)2)).
Considering the evidence and empirical results, the time com-
plexity of Siamese BERT is evidently lower than the original BERT
which is our production baseline. The empirical results show that
Siamese BERT increases the time cost by approximately 1% com-
pared to the production model, but it reduces the time cost by about
30% compared to the Q-T-S-Q’ model.
6131KDD ’24, August 25–29, 2024, Barcelona, Spain Dezhi Ye et al.
5.2.3 Query intent driven interaction. Given the representations of
Q-T-S and Q-Q’, the relevance of a document to a query is estimated
through interaction between their contextualized embeddings and
query intent. If the user’s expression uses keywords, they are likely
seeking precise queries, so more attention should be given to the
Q-T-S score. Conversely, if the user’s expression is in neutral lan-
guage, more focus should be placed on the Q-Q’ score. User intent
embeddings are inferred using a simple BERT- small model, where
the [CLS] token is used as the representation of the entire input se-
quence. As shown in Figure 4, the intent and tower embeddings are
combined and passed through the final MLP to obtain the ultimate
score.
5.2.4 Model training. To ensure interpretability and stability, we
manually labeled query-document pairs with relevance ratings
ranging from 0 to 4. We fine-tune the Symmetric Siamese BERT
model using a mixture of pairwise and pointwise loss. The pairwise
loss encourages the ranking score to be consistent with the human-
labeled relevance grade, while the point-wise loss ensures easy
blending with ranking techniques and alignment with the labeled
ratings.
L=∑︁
𝑦𝑖<𝑦𝑗max(0,𝑓(𝑞,𝑑𝑖,𝑞′
𝑖)−𝑓(𝑞,𝑑𝑖,𝑞′
𝑖)+𝛾)
+𝜆(𝛿(𝑓(𝑞,𝑑𝑖,𝑞′
𝑖),𝑦𝑖)+𝛿(𝑓(𝑞,𝑑𝑖,𝑞′
𝑖)),𝑦𝑗)(3)
where𝜆(𝛿(𝑓(𝑞,𝑑,𝑞′),𝑦)=max{[𝑓(𝑞,𝑑,𝑞′)−𝑦
4]2−𝜖}. Our loss
function consists of two parts, one is the pair-wise margin loss,
and the other is the point-wise regression loss. The regression loss
is for the model to learn the absolute ranking information of the
label, making the score of the entire model have a certain physical
meaning. For a model that only uses margin loss, the output score
has no physical meaning and only makes sense when comparing
two sets of scores. By incorporating the pre-sorted questions and
utilizing the Symmetric Siamese BERT model for ranking, we can
effectively integrate question-answer pairs into the web search
system and improve the relevance of search results for users.
6 Experiment Results
6.1 Datasets
The QAGR model was evaluated on both public and industrial Data.
6.1.1 Public data. For the public dataset, we used the MS MARCO
Dev (small) collection, which includes 6,980 queries [ 18]. We also
utilized the TREC DL’19 and DL’20 collections, which consist of
43/54 and 215/211 qrels/query, respectively. Regarding the eval-
uation metric, we adopted MRR@10 for the Dev collection and
nDCG@10 for DL’19 and DL’20.
6.1.2 Industrial Data. Data collected from a real-world web search
engine encompasses two distinct datasets. The first dataset, dubbed
Industrial Random, is composed of 50,000 inquiries paired with
500,000 query-document couplings. The later dataset, named Indus-
trial Top, consists of 10,000 inquiries coupled with 200,000 query-
document pairs. The scores represented the relevance of the docu-
ment to the query, ranging from off-topic (0) to vital (4). The dataset
was partitioned into a training set and a test set, adhering to an80/20 percent allocation. Industrial Random is considered less chal-
lenging relative to Industrial Top, owing to its reliance on random
sampling, whereas Industrial Top employs a sampling technique
focused on the top echelon of results.
6.2 Baselines
Due to deployment and inference cost difficulties, the QAGR model
was primarily compared with the following methods:
For search recall task:
•BM25 [ 39]. A ranking function is employed by search engines
to assess the pertinence of documents to a provided search
query, grounded in the probabilistic retrieval framework.
•Doc2query [ 22]. A vanilla sequence-to-sequence transformer
for document expansion. Doc2query is trained using datasets
consisting of pairs of query and relevant documents.
•DocTTTquery [ 21]. DocTTTquery follows doc2query, but
with T5 as the expansion model. Given an input document,
DocTTTquery generates questions that the document might
answer.
•Doc2Query– [ 9]. Doc2Query– proceeds to a filtering phase,
which is responsible for eliminating the generated queries
that are least relevant to the source document.
•QAGeneration. This approach begins by extracting the key
paragraphs, followed by utilizing a limited amount of human-
assisted LLMs for question-answer generation. Finally, an
evaluation step is employed to filter out irrelevant pairs.
For search rank task:
•BERT [ 20]. Transformer models that perform search ranking
in multi-stage ranking architectures, which take query, title,
and summary as inputs.
•QARanking. QARanking is a BERT model based on a sym-
metric dual-tower. Additionally, we incorporate score fusion
by considering both symmetric and asymmetric matching
scores based on the intents of the query.
6.3 Settings
In the query generation task, we have chosen the T5 model in its
base configuration, which consists of 12 layers in both the encoder
and decoder. As for the LLMs, we utilize a decoder-only model with
a capacity of 7 billion parameters. In the ranking task, we employ
the bert_base model for query-title-summary interaction, which
includes 12 layers in the encoder. In the retrieval step, the generated
questions are used to construct an additional inverted index that
represents the domain content of the web page. To ensure fairness
in comparison, the BM25 algorithm is employed to recall those
questions. For each document, approximately 40 questions were
indexed.
6.4 Metrics
The following evaluation metrics were employed to assess the rank-
ing system’s performance:
•PNR (Positive-Negative Ratio). A pairwise metric for evalu-
ating search relevance performance. It measures the number
of concordant pairs versus discordant pairs in a ranked list
of documents [15, 44].
6132Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
•MRR: Measures the average of the reciprocal ranks of rele-
vant documents in a ranked list.
•NDCG (Normalized Discounted Cumulative Gain): A listwise
accuracy metric for evaluating ranking model performance.
It considers the relevance and position of documents in the
ranked list.
•GSB (Good vs. Same vs. Bad): Measures the relative gain be-
tween two systems by comparing human experts’ judgments
on which system provides better final results [15, 44].
6.5 Offline Results
In the offline evaluation, various baselines were tested on different
datasets to ensure diversity. The results, as shown in Table 1, reveal
several key observations:
The incorporation of query generation techniques improves
the retrieval of relevant results. In terms of Industrial Random,
Doc2query demonstrates a 4% improvement compared to BM25,
DocTTTquery exhibits a 7% improvement compared to BM25, and
QAG showcases a significant 17% improvement compared to BM25.
This indicates the effectiveness of questions in enhancing relevance
modeling. The proposed QAG method outperforms Doc2query–
from 2.19 to 2.33, demonstrating the value of generated queries in
retrieving high-quality documents.
Generated questions prove beneficial for ranking models. Both
QARank and the standard BERT model exhibit notable improve-
ments when combined with question generation. In the context of
Industrial Top, the PNR for BERT model has increased from 2.74 to
2.94. Similarly, the PNR for QAR has improved from 2.85 to 3.15. The
LLM-generated questions bridge the semantic gap between queries
and content, serving as valuable supplements to web pages. By facil-
itating information exchange, the burden of modeling asymmetric
matches is alleviated for ranking models, as the representation of
questions closely resembles neural queries.
GARanking outperforms the base model by 4.1% and achieves the
highest PNR of 3.98 in Industrial Random and 3.15 in Industrial Top,
surpassing the performance of the standard BERT model. This im-
provement can be attributed to two factors. Firstly, the pre-sorting
approach effectively filters out irrelevant questions, reducing inter-
ference during the ranking process. Secondly, the symmetric BERT
model based on dual towers maximizes the impact of questions
and balances the relevance scores between asymmetric and sym-
metric matches. Overall, our proposed model excels in addressing
asymmetric matches through a multi-stage pipeline, surpassing
the performance of the online baseline by a significant margin on
industrial and public datasets.
6.6 Online Results
The proposed model was deployed in online search engines and
compared with the base model in a real production environment.
Our online baseline model utilized BM25 during the recall stage
and the standard BERT model during the rank stage, with the input
consisting of Query, Title, and Summary. The experimental group,
as mentioned in this paper, employed Question-Answer Generation
and Ranking. An online A/B test was conducted for two weeks,
focusing on metrics directly related to user experience. The results
in Table 2 show a significant improvement in the overall user expe-
rience of the search engine. Notably, specific improvements wereobserved in key metrics, including a notable 0.67% enhancement
in satisfaction Query View (QV), a commendable 0.87% increase in
long click-through rate, a substantial 1.84% rise in average consump-
tion duration, and a noteworthy 1.2% improvement in click-through
rate. In terms of manual evaluation, the △GSB exhibited a remark-
able enhancement of 8.5%.
7 Ablation Studies
7.1 The Effectiveness of Modules within QAG
To further validate the effectiveness of the modules within the QAG,
we conducted a series of ablation experiments on these modules.
From Table 3, it is evident that the "Human-Loop Question An-
swer Generation" and "LLMs" contribute significantly, while the
"Question Answer Verification" module has a relatively smaller
impact. This discrepancy may be attributed to the fact that LLMs,
being more powerful in terms of learning capabilities compared to
smaller models, generate question-answer pairs that are more corre-
lated with the document, reducing the reliance on the "Verification"
component.
7.2 The Impact of Parameter Size in Generator
The results in Figure 5 demonstrated that the quality of generated
questions improved significantly with the increase in model param-
eters. However, the resources required for training and inference
also increased exponentially. The improvement from 0.7B to 7B was
more substantial, while the improvement from 7B to 13B was less
pronounced. This suggests that smaller models have limited under-
standing capabilities, and there is a qualitative leap in performance
beyond a certain scale.
0 2 4 6 8 10 12
Generater Model Size3.03.23.43.63.84.0PNR
2.882.942.983.023.153.183.663.743.793.853.984.02
2.882.922.952.973.073.093.663.73.753.813.923.94W/ Answer
W/O Answer
Figure 5: PNR on Random and Top data for different model
sizes in QAG. The "w/ answer" line refers to generating ques-
tions with extraction of answers, while the "w/o answer" line
refers to generating questions without extraction of answers.
7.3 The Impact of Parameter Top-N
In offline and online experiments, we observed that generating
questions significantly improves the performance of asymmetric
ranking. To analyze the influence of the number of questions, we
varied the number used for ranking from 1 to 10. The Figure 6
showed that the performance improved as the number of questions
6133KDD ’24, August 25–29, 2024, Barcelona, Spain Dezhi Ye et al.
Source Industrial Random Industrial Top Dev DL’19 DL’20
Model Prev. Top-k PNR NDCG MRR PNR NDCG MRR MRR NDCG NDCG
BM25 None |C| 1.98 0.505 0.192 1.85 0.482 0.173 0.184 0.499 0.479
Doc2query None |C| 2.06 0.524 0.219 1.93 0.494 0.185 0.218 0.626 0.607
DocTTTquery None |C| 2.12 0.529 0.224 1.95 0.501 0.194 0.272 0.626 0.607
Doc2Query– None |C| 2.19 0.533 0.229 1.99 0.509 0.198 0.316 0.667 0.611
QAG None|C| 2.33 0.545 0.242 2.10 0.524 0.218 0.331 0.689 0.633
BERTBM25 350 3.62 0.677 0.301 2.74 0.625 0.288 0.365 0.713 0.689
Doc2query 350 3.69 0.690 0.310 2.81 0.632 0.294 0.375 0.729 0.694
DocTTTquery 350 3.74 0.704 0.324 2.89 0.641 0.302 0.389 0.742 0.720
Doc2Query– 350 3.78 0.712 0.329 2.90 0.643 0.304 0.394 0.751 0.736
QAG 350 3.82 0.718 0.335 2.94 0.659 0.314 0.410 0.762 0.749
QARBM25 350 3.65 0.687 0.315 2.85 0.643 0.298 0.365 0.714 0.690
Doc2query 350 3.72 0.692 0.321 2.92 0.651, 0.306 0.382 0.733 0.710
DocTTTquery 350 3.76 0.701 0.327 2.98 0.662 0.312 0.399 0.749 0.739
Doc2Query– 350 3.80 0.713 0.332 3.01 0.669 0.320 0.402 0.753 0.742
QAG 350 3.98 0.734 0.367 3.15 0.689 0.338 0.428 0.769 0.764
Table 1: PNR, NDCG@10 and MRR@10 on Random, Top, and public datasets for different ranking pipelines, with BM25,
Doc2query, DocTTTquery and Doc2Query– as baselines. Each ranker uses the top 350 retrieved results of the previous stage as
input.
Metric △Improvement
Click-Through Rate +1.2% ↑
Long Click-Through Rate +0.87% ↑
Satisfaction Query View (QV) +0.67% ↑
Average Consumption Duration +1.84% ↑
△GSB +8.5% ↑
Table 2: Online A/B test results of QAGR. The statistical
significance, denoted by the 𝑝-value, is less than 0.05.
Setting Industrial Random Industrial Top
QAG 0.545 0.524
w/o Human-Loop 0.531 0.512
w/o Verification 0.534 0.515
w/o LLMs 0.530 0.511
QAG + QAR 0.734 0.689
w/o Human-Loop 0.723 0.673
w/o Verification 0.727 0.681
w/o LLMs 0.719 0.671
Table 3: NDCG@10 on Industrial Random and Top datasets
for different models.
increased from 0 to 3, but there was a decline when the number
increased from 3 to 10. Particularly, when the number of questions
reached 10, the performance was similar to having 0 questions.
This indicates that while generating questions provides benefits for
asymmetric matching, irrelevant questions can negatively impact
ranking.
0 2 4 6 8 10
The parameter of top-N in rank2.83.03.23.43.63.84.0PNR
2.822.932.993.15
3.1
3.06
2.952.93
2.86
2.82.783.663.783.843.98
3.85 3.84
3.8
3.75
3.7
3.66
3.62Industrial T op
Industrial RandomFigure 6: PNR on Industrial Random and Top data for differ-
ent the number of top-N questions in the ranking stage.
7.4 Analysis of Symmetric Siamese BERT
We studied the influence of Symmetric Siamese BERT to explain
the motivation behind the designed structure. The results in Table
4 showed that removing the query intents and symmetric siamese
component led to a decrease in overall performance, highlighting
the importance of symmetric siamese BERT. Besides, we conducted
experiments with the T5 [ 27] model to validate where symmet-
ric siamese architecture generalizes across different base models.
It can be observed that under the condition of using T5 as the
rank model, the symmetric siamese architecture still demonstrated
certain advantages.
6134Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
Method PNR NDCG MRR
QAR(BERT) 3.15 0.689 0.338
w/o Intents 3.10 0.683 0.329
w/o Symmetric Siamese 3.02 0.678 0.320
QAR(T5) 3.26 0.712 0.359
w/o Symmetric Siamese 3.20 0.701 0.344
Table 4: PNR, NDCG and MRR on Industrial Top data for
different models.
8 Conclusion
In conclusion, this paper introduces a novel approach called Question-
Answer Generation and Ranking (QAGR) to enhance asymmet-
ric web search. QAGR addresses the semantic gap between web-
page content and user queries by automatically generating relevant
question-answer pairs from text passages or documents. The ap-
proach consists of two components: QAGeneration and QARanking,
which operate in offline and online stages, respectively. In the offline
stage, QAGeneration utilizes language models (LLMs) as efficient
summarization tools to produce high-quality question-answer pairs.
These generated questions are then ranked based on their relevance
to the passage, resulting in a prioritized list of question-answer
pairs. In the online stage, the questions are considered along with
the title and summary to determine the final relevance score. To
balance the keywords matching score and neural matching score, a
symmetric siamese BERT model is designed. By incorporating the
generated questions into the content for recall and considering their
relevance during the ranking phase, QAGR significantly enhances
the overall search experience. The effectiveness of QAGR is demon-
strated through offline and online experiments, which highlight
the benefits of asymmetric matching. Additionally, further ablation
experiments on QAGR provide valuable insights into the underly-
ing mechanisms and offer directions for future improvements in
web search.
References
[1]Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.
Inpars: Unsupervised dataset generation for information retrieval. In Proceedings
of the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 2387–2392.
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[3]Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot
dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[5]Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu.
2023. Towards Next-Generation Intelligent Assistants Leveraging LLM Tech-
niques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 5792–5793.
[6]Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Cha-
ganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al .
2023. Rarr: Researching and revising what language models say, using language
models. In Proceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). 16477–16508.
[7]Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot
dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496 (2022).
[8]Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019. Fairness-
aware ranking in search & recommendation systems with application to linkedin
talent search. In Proceedings of the 25th acm sigkdd international conference on
knowledge discovery & data mining. 2221–2231.[9]Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Query–:
When Less is More. In European Conference on Information Retrieval. Springer,
414–422.
[10] Xu Han, Kunlun Zhu, Shihao Liang, Zhi Zheng, Guoyang Zeng, Zhiyuan Liu,
and Maosong Sun. 2023. QASnowball: An Iterative Bootstrapping Frame-
work for High-Quality Question-Answering Data Generation. arXiv preprint
arXiv:2309.10326 (2023).
[11] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,
Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-
based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2553–2561.
[12] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118
(2021).
[13] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,
Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Mod-
els as Efficient Dataset Generators for Information Retrieval. arXiv preprint
arXiv:2301.01820 (2023).
[14] Davis Liang, Peng Xu, Siamak Shakeri, Cicero Nogueira dos Santos, Ramesh
Nallapati, Zhiheng Huang, and Bing Xiang. 2020. Embedding-based zero-shot
retrieval through query generation. arXiv preprint arXiv:2009.10270 (2020).
[15] Yiding Liu, Weixue Lu, Suqi Cheng, Daiting Shi, Shuaiqiang Wang, Zhicong
Cheng, and Dawei Yin. 2021. Pre-trained language model for web-scale retrieval
in baidu search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 3365–3375.
[16] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot
Listwise Document Reranking with a Large Language Model. arXiv preprint
arXiv:2305.02156 (2023).
[17] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli
Goharian, and Ophir Frieder. 2020. Efficient document re-ranking for transform-
ers by precomputing term representations. In Proceedings of the 43rd International
ACM SIGIR Conference on Research and Development in Information Retrieval.
49–58.
[18] Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey, Arman Cohan,
and Nazli Goharian. 2021. Simplified data wrangling with ir_datasets. In Proceed-
ings of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval. 2429–2436.
[19] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al .
2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv
preprint arXiv:2112.09332 (2021).
[20] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.
arXiv preprint arXiv:1901.04085 (2019).
[21] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to
docTTTTTquery. Online preprint 6 (2019), 2.
[22] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document
expansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).
[23] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[24] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues
Bouchard. 2023. Improving Content Retrievability in Search with Controllable
Query Generation. In Proceedings of the ACM Web Conference 2023. 3182–3192.
[25] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna:
Zero-Shot Listwise Document Reranking with Open-Source Large Language
Models. arXiv preprint arXiv:2309.15088 (2023).
[26] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen,
Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al .2023. Large language
models are effective text rankers with pairwise ranking prompting. arXiv preprint
arXiv:2306.17563 (2023).
[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485–5551.
[28] Houxing Ren, Linjun Shou, Ning Wu, Ming Gong, and Daxin Jiang. 2022. Empow-
ering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing. 3107–3121.
[29] Corbin Rosset, Chenyan Xiong, Xia Song, Daniel Campos, Nick Craswell, Saurabh
Tiwary, and Paul Bennett. 2020. Leading conversational search by suggesting
useful questions. In Proceedings of the web conference 2020. 1160–1170.
[30] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin
Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
of Rerankers. arXiv preprint arXiv:2303.00807 (2023).
[31] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau
Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with
zero-shot question generation. arXiv preprint arXiv:2204.07496 (2022).
6135KDD ’24, August 25–29, 2024, Barcelona, Spain Dezhi Ye et al.
[32] Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021.
End-to-end training of multi-document reader and retriever for open-domain
question answering. Advances in Neural Information Processing Systems 34 (2021),
25968–25981.
[33] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun
Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as
Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).
[34] Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and
Colin Raffel. 2023. Evaluating the factual consistency of large language models
through news summarization. In Findings of the Association for Computational
Linguistics: ACL 2023. 5220–5255.
[35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[37] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang
Liu. 2022. Webformer: The web-page transformer for structure information
extraction. In Proceedings of the ACM Web Conference 2022. 3124–3133.
[38] Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme,
Arman Cohan, and Luca Soldaini. 2023. When do Generative Query and Docu-
ment Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and
Datasets. arXiv preprint arXiv:2309.08541 (2023).
[39] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of lucene
for information retrieval research. In Proceedings of the 40th international ACM
SIGIR conference on research and development in information retrieval. 1253–1256.
[40] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformers
for text ranking: BERT and beyond. In Proceedings of the 14th ACM International
Conference on web search and data mining. 1154–1156.
[41] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than
retrieve: Large language models are strong context generators. arXiv preprint
arXiv:2209.10063 (2022).
[42] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck.
2020. Generating clarifying questions for information retrieval. In Proceedings of
the web conference 2020. 418–428.
[43] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al .2023. Siren’s Song in the AI
Ocean: A Survey on Hallucination in Large Language Models. arXiv preprint
arXiv:2309.01219 (2023).
[44] Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, Shuaiqiang
Wang, Daiting Shi, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained language
model based ranking in Baidu search. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 4014–4022.A appendix
A.1 Computational Requirements
In a real-world web search environment, during the training phase
of QAGR, approximately 8 A100 80G GPUs were utilized. In the
inference phase, due to the need for offline generation of billions
of webpage documents, each webpage document inference took
around 200ms. Therefore, we employed 60 A100 GPUs for inference,
with a batch size of 128 on each card. Hence, the computational
requirements primarily depend on the size of the webpage repos-
itory. Please be aware that our query-answer pair is one-off and
does not necessitate repetitive production. Moreover, during on-
line computation, we require only an additional 10% of storage
resources.
A.2 The Prompt in QAG
Ignore all previous instructions, give me very short answers. As a
high-quality Question-Answer-Generation-System, your task is to
organize the paragraphs according to the provided paragraphs and
page titles, taking care not to modify the meaning of the original text,
and to generate effective and succinct questions and answers based
on the core sentences of the paragraphs. Generated questions need
to comply with the following principles: 1. Generated questions need
to use the Entity in the passage (sentence) to ask questions, keep the
questions clear and simple, and the questions should be related to the
central theme of the page. 2. Generated questions and answers should
avoid repetitive, meaningless sentences and questions that cannot be
answered based on the passage. 3. Questions should be phrased in a
colloquial style, while answers should be phrased in a formal style.
4. Generated questions should be related to the central theme of the
page’s title. 5. Questions should not contain "this", "that" and other
pronouns (no pronoun), you need to use a noun to describe. 6. If the
sentence contains summarizing words such as "therefore" and "so", the
rhetorical question is "why".
6136