Knowledge Distillation with Perturbed Loss: From a Vanilla
Teacher to a Proxy Teacher
Rongzhi Zhang∗
Georgia Institute of Technology
Atlanta, GA, USA
rongzhi.zhang@gatech.eduJiaming Shen†
Google
New York City, NY, USA
jmshen@google.comTianqi Liu†
Google
New York City, NY, USA
tianqiliu@google.com
Jialu Liu
Google
New York City, NY, USA
jialu@google.comMichael Bendersky†
Google
Mountain View, CA, USA
bemike@google.comMarc Najork†
Google
Mountain View, CA, USA
najork@google.com
Chao Zhang
Georgia Institute of Technology
Atlanta, GA, USA
chaozhang@gatech.edu
ABSTRACT
Knowledge distillation is a popular technique to transfer knowl-
edge from a large teacher model to a small student model. Typi-
cally, the student learns to imitate the teacher by minimizing the
KL divergence of its output distribution with the teacher’s output
distribution. In this work, we argue that such a learning objec-
tive is sub-optimal because there exists a discrepancy between the
teacher’s output distribution and the ground truth label distribu-
tion. Therefore, forcing the student to blindly imitate the unreliable
teacher output distribution leads to inferior performance. To this
end, we propose a novel knowledge distillation objective PTLoss by
first representing the vanilla KL-based distillation loss function via
a Maclaurin series and then perturbing the leading-order terms in
this series. This perturbed loss implicitly transforms the original
teacher into a proxy teacher with a distribution closer to the ground
truth distribution. We establish the theoretical connection between
this “distribution closeness” and the student model generalizability,
which enables us to select the PTLoss ’s perturbation coefficients in
a principled way. Extensive experiments on six public benchmark
datasets demonstrate the effectiveness of PTLoss with teachers of
different scales.
CCS CONCEPTS
•Computing methodologies →Machine learning approaches.
KEYWORDS
Knowledge Distillation, Large Language Models
∗This work was conducted during an internship at Google Research.
†Now with Google DeepMind.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671851ACM Reference Format:
Rongzhi Zhang∗, Jiaming Shen†, Tianqi Liu†, Jialu Liu, Michael Bendersky†,
Marc Najork†, and Chao Zhang. 2024. Knowledge Distillation with Per-
turbed Loss: From a Vanilla Teacher to a Proxy Teacher. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671851
1 INTRODUCTION
Knowledge distillation (KD) is a widely-used technique to trans-
fer knowledge from a large teacher model into a much smaller
student model with minimum sacrifice of teacher model’s predic-
tive power [ 4,11]. The vanilla training objective in KD such as
KL loss [ 11,22,28] encourages the student’s outputs to be close to
the teacher’s outputs as much as possible. This objective implicitly
assumes the teacher’s output distribution on the distillation data is
perfectly aligned with the ground truth distribution. However, in
many applications, this assumption does not hold and the teacher’s
output distributions can be biased from the ground truth due to var-
ious factors, such as the inductive bias encoded in the teacher model
architecture [ 34], miscalibration in the training procedure [ 22], or
the bias in the teacher model training set [ 20,21]. Enforcing the stu-
dent to blindly imitate the teacher’s outputs can make the student
inherit such biases and produce suboptimal predictions.
To overcome this challenge, one common approach [ 11] suggests
scaling the teacher’s logits via a temperature parameter. A proper
temperature value can enhance the quality of the teacher model’s
output distribution by making it closer to the true label distribu-
tion [ 22]. However, the shifting space offered by temperature scaling
is limited, and the optimal temperature value relies on resource-
intensive grid search. Along a separate line, label smoothing [ 29]
is proposed to regularize the neural networks, and modulated loss
functions [ 18,19] are designed to address various statistical issues
in model training such as overfitting and data imbalance. Despite
their potential, there is a lack of work that explores tailoring such
techniques for more robust knowledge distillation.
4278
KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
Ground Truth 
DistributionOriginal Teacher DistributionProxy Teacher 
Distribution
Perturb leading terms by Represent         by Maclaurin series PTLoss  implicitly shifts      to  such that
Distillation using standard KL loss Distillation using PTLoss
Figure 1: PTLoss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground
truth distribution. This approach addresses the issue of sub-optimal student models resulting from discrepancies between the
teacher’s output distribution and the ground truth distribution. By introducing perturbation to standard KL loss represented
by its Maclaurin series, we obtain a better proxy teacher, which leads to a more effectively distilled student.
In this study, we propose PTLoss for knowledge distillation,
which generalizes the vanilla KL loss function and implicitly cre-
ates a debiased teacher distribution closer to the ground truth. As
shown in Figure 1, our approach does not merely mimic the original
teacher model’s output distribution, denoted as p𝑡, but rather aims
to bring the distilled model closer to the ground truth distribution
p∗, by generating a proxy teacher distribution p𝑡𝑝𝑥. This is achieved
by the proposed PTLoss , which implicitly transforms the original
teacher (the dashed blue curve in Figure 1) into a proxy teacher
(the dash-dot green curve in Figure 1). The resultant proxy teacher
distribution is closer to the ground truth than the original teacher’s
output distribution, thus enhancing the distillation process.
Compared to the standard KL loss, PTLoss refrains from forc-
ing an out-and-out imitation of the original teacher model. Instead,
PTLoss moderates the distillation objective by adding perturbations
to the standard KL loss. Specifically, we first decompose the standard
KL loss into its Maclaurin series and then perturb its leading-order
terms to construct a more flexible learning objective. This manipu-
lation enables consequential adjustments to the teacher’s output
distribution. The perturbations are not arbitrary but are meticu-
lously calculated. To determine the perturbation extent, we compute
the equivalent distribution of this implicitly shifted teacher’s out-
put distribution after perturbations (named “proxy teacher ”) and
measure the empirical deviation between the proxy teacher and
the ground truth data. It leads to a systematic searching strategy
for the perturbation coefficients — the near-optimal perturbation
coefficients should minimize the deviation between the distillation
risk and the population risk on the validation set.
Theoretically, we justify the effectiveness of PTLoss by proving
that it can reduce the deviation from the distillation risk compared
to KL loss. We draw a connection between the PTLoss and otherperturbation methods (e.g., temperature scaling [ 11], label smooth-
ing [29], and focal loss [ 19]). We illustrate that the PTLoss can debias
the teacher to produce higher-fidelity outputs via a finer-grained
perturbation, while subsuming existing perturbation techniques as
special cases. Experiments on multiple datasets with different-sized
teacher models demonstrate the empirical advantages of PTLoss.
In summary, we make the following contributions:
(1)A new knowledge distillation loss function PTLoss , which
formulates the vanilla KD loss in the form of Maclaurin series
and perturbs it to improve the fidelity of teacher models;
(2)A principled method to compute the proxy teacher for de-
termining the perturbation coefficients in PTLoss;
(3)Theoretical analysis on why PTLoss can lower the distillation
risk bound and how it subsumes other perturbation methods;
(4)Comprehensive experiments on multiple language under-
standing tasks with different-sized teacher models showing
the advantage of PTLoss.
2 PRELIMINARIES
Multi-class Classification. In a multi-class classification prob-
lem with𝐶classes, we are given a set of training examples D=
{(𝑥𝑛,𝑦𝑛)}𝑁
𝑛=1where input 𝑥𝑛∈𝑋and output𝑦𝑛is a one-hot vec-
tor in𝑌={𝑦|𝑦∈{0,1}𝐶,1𝑇𝑦=1}indicating the target label of
example𝑥𝑛. The goal is to learn a probability predictor p:𝑋→R𝐶
by optimizing the below minimal risk:
𝑅(p)=E(𝑥,𝑦)[ℓ(𝑦,p(𝑥))]. (1)
whereℓ(𝑦,p(𝑥))is the loss of predicting p(𝑥)when the true label
of example𝑥is𝑦.
4279Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher KDD ’24, August 25–29, 2024, Barcelona, Spain
A canonical loss function is the cross-entropy loss: ℓ𝐶𝐸(𝑦,p(𝑥))=
−𝑦log(p(𝑥))and we may further approximate the above risk via
theempirical risk on the training set D:
ˆ𝑅(p;D)1
𝑁𝑁∑︁
𝑛=1𝑦𝑛(−log(p(𝑥𝑛))) (2)
Our Problem Formulation. In this work, we study the knowledge
distillation problem where the labeled training set Dis inaccessi-
ble1. Specifically, we are only given an unlabeled distillation set
D𝑢, a teacher model p𝑡, and asked to learn a student model p𝑠.
Standard Distillation Strategy. A standard KD strategy [ 11] is to
replace the ground truth one-hot label 𝑦𝑛in Eq. 2 with the teacher
model’s output probabilistic label estimate p𝑡(𝑥𝑛)and utilize the
KL divergence loss to learn the student model p𝑠via the distillation
empirical risk :
˜𝑅𝐾𝐿(p𝑠;p𝑡,D𝑢)1
𝑁𝑢𝑁𝑢∑︁
𝑛=1ℓ𝐾𝐿 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛), (3)
where𝑁𝑢=|D𝑢|andℓ𝐾𝐿(p,q)=𝐾𝐿(p||q)=p𝑇log(p)−p𝑇log(q).
3 PERTURBED DISTILLATION LOSS
Using the KL divergence loss (in short “KL loss”) for distillation
essentially assumes the teacher model is perfect and forces the
student model to mimic the teacher’s output label distribution. In
reality, the teacher model can produce a biased estimate of label dis-
tribution and lead to a sub-optimal student model, as demonstrated
by both theoretical analysis [ 22] and empirical observations [ 23]
(as well as our experiments in Section 5.1).
In this work, we present a new distillation loss that generalizes
the standard KL loss to accommodate various degrees of distribu-
tion gaps between the biased teacher’s output distribution and the
underlying ground truth distribution. Inspired by the PolyLoss [ 18],
we propose to first replace the logarithmic terms in the standard
KL loss with their corresponding Maclaurin series and then perturb
the polynomial terms as follows:
log(𝑥)=−∞∑︁
𝑚=1(1−𝑥)𝑚
𝑚Perturb polynomial−−−−−−−−−−−−−−−−−→
term coefficients(4)
log(𝑥)≈−∞∑︁
𝑚=11
𝑚+𝜖𝑚
(1−𝑥)𝑚.
Here, we essentially replace the original coefficient1
𝑚of the
𝑚-th order polynomial term in the standard KL loss to (1
𝑚+𝜖𝑚). By
further replacing the logarithmic terms in standard KL loss (Eq. 3)
with the above Eq. 4, we will have:
ℓ𝐾𝐿 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)=−H p𝑡(𝑥𝑛)+∑︁
𝑐∈[𝐶]p𝑡
𝑐(𝑥𝑛)[−logp𝑠
𝑐(𝑥𝑛)]
≈−H p𝑡(𝑥𝑛)+∑︁
𝑐∈[𝐶]p𝑡
𝑐(𝑥𝑛)"
−logp𝑠
𝑐(𝑥𝑛)+∞∑︁
𝑚=1𝜖𝑐,𝑚(1−p𝑠
𝑐(𝑥𝑛))𝑚#
,
(5)
1This setting reflects the real-world scenario where large teacher models (e.g., Chat-
GPT [ 24] and GPT4 [ 25]) only expose their outputs and/or APIs without original train-
ing data because of their large model sizes and cautions toward data leakage/misuse.where p𝑡𝑐(𝑥𝑛)andp𝑠𝑐(𝑥𝑛)denote the probability that example 𝑥𝑛
belongs to the class 𝑐according to the teacher (student) model, and
H p𝑡(𝑥𝑛)is the entropy of the teacher output distribution.
We can further separate out the perturbation coefficients on the
right hand side of Eq. 5 and mergeÍ
𝑐∈[𝐶]p𝑡𝑐(𝑥𝑛)
−logp𝑠𝑐(𝑥𝑛)
withH p𝑡(𝑥𝑛)to obtain our perturbed distillation loss:
ℓ𝑃𝑇 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)ℓ𝐾𝐿 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)
+∑︁
𝑐∈[𝐶]p𝑡
𝑐(𝑥𝑛)∞∑︁
𝑚=1𝜖𝑐,𝑚 1−p𝑠
𝑐(𝑥𝑛)𝑚.(6)
The above equation presents our perturbed distillation loss in
its most general form. In practice, however, we cannot tune infi-
nite number of coefficients 𝜖𝑐,𝑚and thus we propose to only tune
the first𝑀leading polynomial coefficients while keeping the rest
unchanged as follows:
ℓ𝑃𝑇-𝑀 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)ℓ𝐾
𝐿 p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)
+∑︁
𝑐∈[𝐶]p𝑡
𝑐(𝑥𝑛)𝑀∑︁
𝑚=1𝜖𝑐,𝑚 1−p𝑠
𝑐(𝑥𝑛)𝑚.
(7)
We can see that if we set all 𝜖𝑐,𝑚to 0, theℓ𝑃𝑇falls back to the
ℓ𝐾𝐿and thus the perturbed distillation loss can be considered as a
generalization of the standard KL loss.
Figure 2 presents how PTLoss adjusts biased teachers. For visual-
ization simplicity, we set the number of classes 𝐶=2. In Figure 2a,
we vary the teacher probability to show how the biased teacher
model will impact the distilled student model under either the stan-
dard KL loss or our proposed PTLoss . We observe that PTLoss can
guide the student’s predictions toward the ground truth and thus
effectively reduces the inherent bias in the teacher’s output proba-
bilities. In Figure 2b, we demonstrate PTLoss enables a diverse shift
space to the loss curve. By setting the perturbation coefficients,
PTLoss allows flexible adjustments to the loss curve. Combining
with our perturbation coefficients selection methods discussed in
Sec. 4.3, we can determine the perturbation to optimize the distilla-
tion process.
3.1 Connections to other perturbation methods
We establish the connections between PTLoss and other related
methods that transform the teacher output probabilities, such as
label smoothing [ 29], temperature scaling [ 11], and focal loss [ 19].
We show that the loss shift space produced by PTLoss encompasses
these techniques and thus PTLoss can offer additional adjustment
capabilities. We present the connections in this section and detail
the derivation in Appendix 8.3.
KL Loss. The connection between PTLoss and the standard KL
loss is quite direct. As we represent the standard KL loss in Maclau-
rin series and add perturbations, we can easily revert PTLoss to the
standard KL loss by setting all perturbation coefficients 𝜖𝑐,𝑚to 0.
Focal Loss. Focal loss incorporates a factor (1−𝑝)𝛾in the loss
function. We demonstrate PTLoss can subsume focal loss by ex-
pressing the perturbation coefficients as a function of the factor
(1−𝑝)𝛾. For simplicity, we denote p𝑡(𝑥𝑛)andp𝑠(𝑥𝑛)asp𝑡andp𝑠.
4280KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
0.0 0.2 0.4 0.6 0.8 1.0
Student Probability ps
001234Loss ValueKL Loss | pt= [1.0,0.0] 
KL Loss | pt= [0.9,0.1] 
KL Loss | pt= [0.8,0.2]PT Loss | pt= [1.0,0.0] 
PT Loss | pt= [0.9,0.1] 
PT Loss | pt= [0.8,0.2]
(a) Loss values with different teacher probabilities p𝑡
0in
{1.0,0.9,0.8}. For PTLoss, we fix the perturbation order as 1 and
the perturbation coefficients 𝜖=[1,1]. Consider the case that
ground truth probability is [1,0], PTLoss adjusts the student’s
predictions by nudging them towards the ground truth, effec-
tively mitigating the bias present in the teacher’s output proba-
bilities.
(b) PTLoss values with different perturbations. We fix the teacher
probability p𝑡
0=[0.8,0.2]and vary the perturbation coefficients 𝜖,
while the perturbation order is always fixed to 1. The black cross
denotes the best student model output p𝑠
0that achieves the lowest
loss value. This shows PTLoss can enable flexible adjustments
to the loss curve and effectively reduces the bias of the teacher’s
output.
Figure 2: Intuitive understanding of our PTLoss for the bi-
nary classification task.
By applying Focal loss to KD, we have:
ℓ𝑓𝑜𝑐𝑎𝑙 p𝑡,p𝑠=−H p𝑡+∑︁
𝑐∈[𝐶]p𝑡
𝑐(1−p𝑠
𝑐)𝛾[−logp𝑠
𝑐],(8)
where(1−p𝑠𝑐)𝛾is a factor and the parameter 𝛾>0reduces the
relative loss for well-classified examples.
To bridge the connection between PTLoss and the focal loss, we
compare Eq. 8 to Eq. 5 and establish the following relationship:
∞∑︁
𝑚=1(1
𝑚+𝜖𝑐,𝑚)(1−p𝑠
𝑐)𝑚=∞∑︁
𝑚=1(1−p𝑠𝑐)𝛾
𝑚·(1−p𝑠
𝑐)𝑚,(9)
which leads to the perturbation coefficients as follows:
𝜖𝑐,𝑚=(1−p𝑠𝑐)𝛾−1
𝑚. (10)By incorporating the derived perturbation coefficients 𝜖𝑐,𝑚in our
proposed method, we demonstrate that PTLoss can effectively sub-
sume the focal loss. In other words, PTLoss generalizes the focal
loss and can adapt to more modulating factors (1−𝑝)𝛾for han-
dling the class imbalance problem and improving the knowledge
distillation performance.
Temperature Scaling. We compare PTLoss with temperature scal-
ing and claim that PTLoss subsumes it with appropriate approxima-
tion. As described in Hinton et al . [11] , the logits are adjusted by a
temperature to control sharpness or smoothness of the probability
distribution:
p𝜏,0=𝑒𝑥𝑝(𝑧0/𝜏)
𝑒𝑥𝑝(𝑧0/𝜏)+𝑒𝑥𝑝(𝑧1/𝜏), (11)
where𝜏is the temperature and 𝑧𝑐is the logits. Here we use binary
classification (i.e., 𝑐=0,1) for the derivation simplicity without loss
of generality.
We denote p𝑡𝜏,𝑐,p𝑠𝜏,𝑐as the teacher, student probability scaled by
temperature 𝜏. Incorporating temperature scaling, the KL loss can
be formulated as:
ℓ𝑡𝑒𝑚𝑝
𝐾𝐿 p𝑡
𝜏,p𝑠
𝜏=−H p𝑡
𝜏+∑︁
𝑐∈[𝐶]p𝑡
𝜏,𝑐·(−logp𝑠
𝜏,𝑐). (12)
Similarly, to bridge PTLoss with temperature scaling, we establish
equality between Eq. 12 and Eq. 5, this leads to
𝜖𝑐,𝑚=𝑎
𝑚·(1+(1−𝑏
1−p𝑠𝑐)𝑚)−1
𝑚, (13)
where𝑎=𝑒𝑥𝑝(𝑧𝑡
1−𝑧𝑡
0)/𝑒𝑥𝑝(𝑧𝑡
1−𝑧𝑡
0
𝜏),𝑏=𝑒𝑥𝑝(𝑧𝑠
1−𝑧𝑠
0)/𝑒𝑥𝑝(𝑧𝑠
1−𝑧𝑠
0
𝜏).
The derivation clearly shows how PTLoss , through the appropriate
selection of 𝜖𝑐,𝑚, effectively subsumes temperature scaling. This
capability to encompass temperature scaling further underlines the
versatility of our approach.
Label Smoothing. We compare PTLoss with the label smoothing
method and claim that label smoothing proposed in [ 29] is a special
case of PTLoss . According to the implementation in Szegedy et al .
[29], we can smooth the teacher labels in KD by
p𝑡𝑙𝑠𝑐=(1−𝛿)p𝑡
𝑐+𝛿/2, (14)
with a smoothing parameter 𝛿. Starting from Eq. 5, we can replace
the term p𝑡𝑐by its smooth version p𝑡𝑙𝑠𝑐. Then the original Eq. 5 with
label smoothing becomes:
ℓ𝑙𝑠
𝐾𝐿 p𝑡,p𝑠=−H p𝑡𝑙𝑠+∑︁
𝑐∈[𝐶]p𝑡𝑙𝑠𝑐·(−logp𝑠
𝑐) (15)
For the entropy of the teacher output, the smooth version H p𝑡𝑙𝑠
is different from the original H p𝑡with only a constant 𝐶, which
can be ignored when optimizing the loss function. Similarly, by
lettingℓ𝑙𝑠
𝐾𝐿 p𝑡,p𝑠=ℓ𝑃𝑇 p𝑡,p𝑠, we obtain
𝜖𝑐,𝑚=Δp𝑡𝑐
𝑚p𝑡𝑐Δp𝑡
𝑐=𝛿/2−𝛿p𝑡
𝑐−−−−−−−−−−−−−→ 𝜖𝑐,𝑚=𝛿
𝑚(1
2p𝑡𝑐−1). (16)
In shows the connection between the two losses can be expressed
through a specific 𝜖𝑐,𝑚, which depends on the smoothing parame-
ter𝛿. This derivation highlights that PTLoss generalizes the label
4281Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher KDD ’24, August 25–29, 2024, Barcelona, Spain
smoothing method and provides a more flexible framework that
encompasses the effects of label smoothing.
In summary, we establish that PTLoss can effectively serve as a
generalized framework that includes various techniques by manip-
ulating the perturbation coefficients 𝜖𝑐,𝑚. It is important to note
that our goal is not to directly solve for the perturbation coeffi-
cients to make PTLoss equivalent to the alternative perturbation
methods. Instead, we aim to show that our approach covers the loss
shift space produced by them. To determine the real perturbation
coefficient, we establish a principled searching method via the best
proxy teacher, as demonstrated in the following section.
4 PROXY TEACHER AND THE PRINCIPLE OF
SELECTING POLYNOMIAL COEFFICIENTS
In this section, we first present a theorem to show how the teacher
model affects the gap of a student model’s distillation empirical
risk and its population risk ( §4.1). Then, we demonstrate that
using PTLoss implicitly transforms the original teacher model to
aproxy teacher under the KL loss. Based on the above theorem,
we know when this proxy teacher distribution is closer to the true
distribution, we will have a better distilled student model ( §4.2).
Finally, we establish our principle of selecting the perturbation
coefficients in PTLoss : searching the coefficients that lead to a
proxy teacher closest to the empirical estimate of true distribution
on a validation set (§ 4.3).
4.1 The Connection of the Teacher Model and
the Risks of Student Model
Theorem 4.1. Given a teacher model p𝑡, an unlabeled distillation
datasetD𝑢with an unknown true distribution p∗, we have for any
probability predictor p:X→R𝐶:
E
(˜𝑅𝐾𝐿(p;p𝑡,D𝑢)−𝑅(p))2
≤2
𝑁𝑢·Vh
p𝑡(𝑥)𝑇log(p(𝑥))i
+
O E𝑥
∥p𝑡(𝑥)−p∗(𝑥)∥22+
E𝑥
p𝑡(𝑥)𝑇logp𝑡(𝑥)2
,
where V[·]denotes the variance of a random variable.
We defer the detailed proofs of above theorem to Appendix 8.4
and focus on its implications here. We can see that the gap between a
model p’s distillation empirical risk and its population risk depends
on three terms: (1) the variance of its KL distance to the teacher
model p𝑡, (2) the𝐿2distance between the teacher model output
distribution p𝑡and the true distribution p∗, and (3) the entropy of
the teacher distribution. In practice, obtaining a sizable unlabeled
distillation setD𝑢is relatively straightforward, which leads to a
large value of 𝑁𝑢. As a result, the first term (of order 𝑂(1/𝑁𝑢))
will converge to 0 as 𝑁𝑢keeps increasing and the latter two terms
(one quantifies the distance between teacher p𝑡and true p∗, and
the other quantifies the teacher’s uncertainty) will dominate the
risk gap. This observation also resonates with our intuition that an
accurate and well-calibrated teacher yields better improved bounds
on the generalization error of the student.4.2 The Equivalence of Proxy Teacher under KL
Loss and Original Teacher under PTLoss
The above theorem states that an ideal teacher model, when used in
KL loss for distillation, should output a distribution as close to the
true distribution as possible. In reality, however, the teacher model
is usually fixed. Here, we show that using PTLoss for distillation
can implicitly transform the original teacher to a proxy teacher
under the KL loss. Namely, given the original teacher model p𝑡and
a set of perturbation coefficients {𝜖𝑐,𝑚}inPTLoss , we can obtain a
proxy teacher p𝑡𝑝𝑥such that:
˜𝑅𝐾𝐿(p𝑠;p𝑡𝑝𝑥,D𝑢)=˜𝑅𝑃𝑇-𝑀(p𝑠;p𝑡,D𝑢)
=1
𝑁𝑢𝑁𝑢∑︁
𝑛=1ℓ𝑃𝑇-𝑀(p𝑡(𝑥𝑛),p𝑠(𝑥𝑛)
),(17)
which establishes the equivalence of proxy teacher under KL loss
and original teacher under PTLoss . With the proxy teacher p𝑡𝑝𝑥, we
aim to determine the best perturbation coefficients {𝜖𝑐,𝑚}. Note for
each{𝜖𝑐,𝑚}, we can obtain a proxy teacher. We illustrate how we
obtain the proxy teacher in the rest of this subsection, and discuss
how to select the best perturbation coefficients in §4.3.
Intuitively, the proxy teacher is derived by solving the below
optimization problem:
minp𝑡𝑝𝑥∥˜𝑅𝑃𝑇-𝑀(p𝑠;p𝑡,D𝑢)
−˜𝑅𝐾𝐿(p𝑠;p𝑡𝑝𝑥,D𝑢)∥2. (18)
In practice, however, we do not need the above risk equivalence
in Eq. 17 to hold for all possible student models p𝑠. Instead, we
focus on the minimizer of the left-hand side of Eq. 17 because it is
practically close to the final learned student model. By substituting
this minimizer p𝑠=p𝑡𝑝𝑥into Eq. 18, the second term in the norm
of Eq. 18 becomes 0, and the first term could be expanded by its
definition in Eq. 17, we thus have the following objective:
minp𝑡𝑝𝑥1
𝑁𝑢𝑁𝑢∑︁
𝑛=1
ℓ𝐾𝐿(p𝑡(𝑥𝑛),p𝑡𝑝𝑥(𝑥𝑛)) (19)
+∑︁
𝑐∈[𝐶]p𝑡
𝑐(𝑥𝑛)𝑀∑︁
𝑚=1𝜖𝑐,𝑚(1−p𝑡𝑝𝑥
𝑐(𝑥𝑛))𝑚2.
This objective enables us to solve p𝑡𝑝𝑥given p𝑡and{𝜖𝑐,𝑚}, where
p𝑡is the teacher’s output probability on the validation set, and
{𝜖𝑐,𝑚}is a given set of perturbation coefficients. However, this op-
timization problem is nonlinear and lacks a closed-form analytical
solution. Consequently, we compute the p𝑡𝑝𝑥using the numeri-
cal approach2. Specifically, the optimization problem defined in
Eq. 19 is solved via the algorithm ‘scipy.optimize.fsolve’, which is a
hybrid method of the Newton-Raphson method and the Levenberg-
Marquardt algorithm. For better numerical stability, we actually
solve the equation in logit space (instead of the vanilla probability
space) and use softmax function to map it back to the final proba-
bility. Another advantage of this approach is that we remove the
probability constraint of p𝑡𝑝𝑥. We also input the analytical form
2We use a hybrid algorithm of the Newton-Raphson method and the Levenberg-
Marquardt algorithm as defined in ‘scipy.optimize.fsolve’ https://docs.scipy.org/doc/
scipy/reference/generated/scipy.optimize.fsolve.html.
4282KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
Algorithm 1 Automated Perturbation Coefficients Selection
Require: Validation setD𝑣,Teacher model p𝑡, Max perturbation order
𝑁𝑀, Max search trails 𝑁𝑘, Perturbation coefficient search space S.
Initialize{𝜖∗𝑐,𝑚}←{},ˆ𝑄∗←∞.
for𝑀=1to𝑁𝑀do
for𝑘=1to𝑁𝑘do
Randomly sample a perturbation coefficients set {𝜖𝑐,𝑚}∈S .
Solve the proxy teacher p𝑡𝑝𝑥given{𝜖𝑐,𝑚}andp𝑡via Eq. 19.
Compute the quality score of perturbation coefficients
ˆ𝑄({𝜖𝑐,𝑚})via Eq. 20.
ifˆ𝑄({𝜖𝑐,𝑚})<ˆ𝑄∗then
ˆ𝑄∗←ˆ𝑄({𝜖𝑐,𝑚}),{𝜖∗𝑐,𝑚}={𝜖𝑐,𝑚}.
end if
end for
end for
return{𝜖∗𝑐,𝑚}.
of the Jacobian of our optimization objective into the solver (via
the ‘fprime’ parameter) and set the initial estimate of p𝑡𝑝𝑥to be
the original teacher p𝑡(via the ‘x0’ parameter). For all the other
parameters in ‘scipy.optimize.fsolve’, we use their default values.
We have also considered an alternative solution to this optimiza-
tion problem, which involves defining a parameterized function
𝑔𝜃(·):[0,1]𝐶→[ 0,1]𝐶that explicitly transforms the original
teacher to the proxy teacher, namely 𝑔𝜃(p𝑡)=p𝑡𝑝𝑥. We would
then find the best 𝜃minimizing the above objective (possibly via
gradient-based methods). This approach leads to a smooth proxy
teacher but also introduces bias from the function class defined by
𝜃. Therefore, we leave it to future work and resort to the numerical
approach in this study.
4.3 Selecting Perturbation Coefficients via the
Best Proxy Teacher
For each candidate set of perturbation coefficients {𝜖𝑐,𝑚}inPTLoss ,
we can find a corresponding proxy teacher and compute its risk
deviation upper bound according to theorem 4.1. In practice, the
size of distillation set 𝑁𝑢is typically large and thus we can omit
the𝑂(1/𝑁𝑢)variance term. Furthermore, since the ground truth
distribution p∗is unknown, we use an unbiased estimator to replace
it. Finally, we replace the expectation by the sample mean and define
the empirical risk below:
ˆ𝑄({𝜖𝑐,𝑚})= 
1
𝑁𝑣𝑁𝑣∑︁
𝑛=1
∥p𝑡𝑝𝑥(𝑥𝑛)−y𝑛∥2!2
+1
𝑁𝑣𝑁𝑣∑︁
𝑛=1
p𝑡𝑝𝑥(𝑥𝑛)𝑇logp𝑡𝑝𝑥(𝑥𝑛)2
,(20)
where𝑁𝑣is the size of validation set and y𝑛is a one-hot label
vector of𝑥𝑛, serving as the unbiased estimation of p∗(𝑥𝑛). We use
ˆ𝑄({𝜖𝑐,𝑚})as a “quality score” for each candidate coefficients set.
Users can define a search space of {𝜖𝑐,𝑚}and we will pick the
optimal{𝜖∗𝑐,𝑚}that minimizes ˆ𝑄. We present the pseudo-code for
selecting perturbation coefficients in Algorithm 1, and the search
time for perturbation coefficients is detailed in Appendix 8.5.5 EXPERIMENTS
In this section, we first conduct experiments on a synthetic dataset
to verify our assumption that the teacher outputting a distribution
closer to the ground truth distribution leads to a better student
(§5.1). Then, we present our main results on 6 real-world NLP
datasets (§5.2). Moreover, we show how the proxy teacher enhances
the distillation process(§5.3). In the appendix, we further evaluate
the performance of PTLoss on CIFAR-100 to show its potential in
computer vision tasks (§5.4).
5.1 Experiments on Synthetic Gaussian Dataset
0 5 10 15 20 25 30
L2-distance of pt and p *0.8320.8340.8360.8380.840Accuracy on test setNoisy p
OHT
LS
GT
KD
ESKD
PTLoss
(a) OHT - one-hot training; LS - label smoothing; GT -
ground truth; KD - knowledge distillation; ESKD: early-
stopped KD [ 27];PTLoss : here we use 3-order perturba-
tion.
0 5 10 15 20 25 30 35 40
L2-distance of pt (with different perturbation) and p *0.8320.8330.8340.8350.8360.837Accuracy on test set
(b) Correlation between the 𝐿2-distance (between the
teacher model p𝑡with different levels of perturbations
and the ground truth p∗) and the test accuracy of the stu-
dent model.
Figure 3: Experiments on a synthetic Gaussian dataset.
We first conduct an illustrative experiment with a synthetic
dataset where the ground truth distribution p∗(𝑥)is known. Specif-
ically, we follow [ 27] to generate 105examples from a mixture of
Gaussian distribution and train an MLP with 3 hidden layers on
this synthetic dataset. This is a 3-class toy Gaussian dataset with
4283Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher KDD ’24, August 25–29, 2024, Barcelona, Spain
10k data points, divided into training, validation, and test sets with
a split ratio[0.9,0.05,0.05]. The underlying model in this set of
experiments is a 2-layer MLP with ReLU activation, and the hidden
size is 128 for each layer. We set the learning rate as 5×10−4, the
batch size as 32, and the training epochs as 100.
The sampling process is implemented as follows: We first choose
the label𝑦using a uniform distribution across all the 3 classes. Next,
we sample𝑥|𝑦=𝑘∼N 𝜇𝑘,𝜎2𝐼as the input signal. Here 𝜎=2and
𝜇𝑘is a 30-dim vector with entries randomly selected from {−1,0,1}.
We compare PTLoss with 4 baselines: one-hot supervision (OHT),
label smoothing (LS), standard knowledge distillation (KD), and
early-stopped knowledge distillation (ESKD) (see details in below
§5.2). As illustrated in Fig. 3a, the quality of the distilled student
improves as the 𝐿2-distance between the teacher distribution and
the ground truth distribution decreases. On this synthetic Gaussian
dataset, PTLoss also outperforms the baselines after adding a 3-
order perturbation.
In Fig. 3b, we sample 10 proxy teachers in different stages of the
perturbation coefficient searching process ( §4.3) and compare their
results. It is clear that a teacher model with a smaller 𝐿2-distance
to the ground truth distribution can lead to a better student model.
This observation verifies our hypothesis in Eq. 20 — searching a
proxy teacher closer to the ground truth distribution can reduce
the empirical deviation and improve the distilled student model.
5.2 Experiments on Natural Language Datasets
Tasks and Datasets. We conduct our main experiments on 6 natu-
ral language datasets, including:
(1) CoLA [32] for linguistic acceptability;
(2) MNLI [35] for multi-genre natural language inference;
(3) MRPC [8] for paraphrase similarity matching;
(4) RTE [31] for textual entailment inference;
(5) SST-2 [31] for sentiment analysis;
(6) BoolQ [6] for boolean question answering.
We list the detailed dataset statistics in the Appendix 8.1.
Model Architectures. For the teacher model, we choose the T5
architecture [ 26] and select two teacher models of different scales.
Specifically, we use T5-xxl with 11 billion parameters and T5-large
with 770 million parameters. For the student model, we use the
BERT-base model [7] with 110 million parameters.
Compared Methods. We compare our proposed PTLoss with the
following baselines:
(1)Standard KL loss [ 17]: adopts standard KL divergence loss for
knowledge distillation;
(2)Temperature scaling [ 11]: scales the teacher output logits via a
temperature hyper-parameter;
(3)Label smoothing [ 29]: smooths the teacher output class proba-
bilities by a small scalar;
(4)Focal loss [ 19]: modulates the cross-entropy loss to focus on
hard examples;
(5)Flooding [ 13]: a regularization method to intentionally prevent
further reduction of the training loss;
(6)CRD [ 30]: uses a contrastive objective in knowledge distillation;
(7)AnnealingKD [ 14]: feeds the rich information provided by the
teacher’s soft-targets incrementally;(8)FilterKD [ 27]: trains the student from the smoothed predictions
of the teacher network;
(9)MetaDistill [ 40]: evolves the teacher network with the feedback
from the distilled student in a meta learning framework.
For all the baselines, we conduct an exhaustive hyper-parameter
search on the validation set. For our own PTLoss method, we set its
perturbation order 𝑀=5and use the proxy teacher-based method
to search its perturbation coefficients ( §4.3). See Appendix 8.2 for
more details. We run each method with three different random
seeds and report its average performance.
Main Results. Table 1 shows the main results on 6 NLP datasets.
We have the following observations: (1) PTLoss outperforms on 11
out of the total 12 tasks, achieving the best average performance
across the board. The only exception is MetaDistill, which tops
the results on the MRPC when using the T5-xxl teacher and ties
with PTLoss on MNLI when using the T5-large teacher. (2) The
advantages offered by PTLoss are robust, regardless of the scale of
the teacher model. Notably, as the disparity in scale between the
teacher model and student model reduces, the performance gap
between them also narrows. (3) In comparison to vanilla KD, which
utilizes the standard KL, PTLoss showcases significant enhance-
ment. Specifically, it exceeds standard KL by an average of 2.8%and
2.9%, respectively. (4) Surveying the baseline methods, MetaDistill
stands out, securing the second-highest performance across most
tasks. On the whole, the cluster of KD methods generally outstrips
the simple regularization methods.
5.3 Proxy Teacher Analysis
Correlation between teacher’s distance to ground truth and
student’s performance. To explore where PTLoss ’s performance
gains come from, we train multiple teacher models on the BoolQ
dataset and distill them into the student models. Fig. 4a shows the
student model performance on the test set is highly correlated with
the distance between the teacher model’s output distribution and
the ground truth distribution on the validation set. This result res-
onates with our findings from synthetic datasets( §5.1), confirming
that, on real-world datasets, a teacher model with a predictive dis-
tribution closer to the ground truth can produce a more effectively
distilled student.
Effectiveness of Perturbation Coefficients Search. We continue
to validate the effectiveness of the proxy teacher-based perturba-
tion coefficients selection method using MNLI as a representative
dataset. Specifically, we vary the perturbation order 𝑀from 1 to
5 and report the performance of the student models distilled via
PTLoss with different perturbation coefficients. These coefficients
are obtained either by minimizing the empirical risk deviation of
proxy teacher (c.f. Eq. 19) or via random sampling from the space of
[−1,10]𝑀. As shown in Fig. 4b, the coefficients obtained from our
proxy teacher based method can achieve consistent improvements
over the random coefficients. If we just randomly set the perturba-
tion coefficients, the student performance can drop by up to 1.2%.
Also, by comparing different perturbation orders, we find that the
higher the perturbation order, the greater the performance differ-
ences. This is because in the higher-dimension space, it is harder for
random search to get a set of appropriate perturbation coefficients,
4284KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
Table 1: Main results on natural language datasets. The student model (BERT-base) is distilled from teacher models of different
sizes (T5-xxl and T5-large). All results are averaged over three runs. The bolded numbers indicate the best results, while the
underscore “_” denotes the second-best results.
Metho
d CoLA
MNLI MRPC RTE SST-2 BoolQ A
verage
(Matt.)
(Acc.) (F1) (Acc.) (Acc.) (Acc.)
T
eacher T5-xxl 71.5
94.7 92.4 92.2 96.4 89.1 89.4
Standar
d KL 58.8±0.3
90.3±0.2 88.3±0.4 78.1±0.1 89.2±0.3 69.5±0.2 79.0
T
emp. Scaling 59.4±0.3
90.7±0.1 88.9±0.3 79.6±0.3 89.4±0.2 72.0±0.3 80.0
Lab
el Smoothing 59.3±0.6
90.6±0.4 89.2±0.7 79.2±0.4 89.9±0.3 68.6±0.4 79.6
Fo
cal 59.2±0.4
90.7±0.3 88.7±0.9 80.4±0.4 89.3±0.3 68.2±1.5 79.6
F
looding 58.9±0.5
90.6±0.4 89.6±0.6 80.2±0.7 89.3±0.4 69.3±0.5 79.7
CRD 59.5±0.5
90.5±0.3 90.6±0.4 81.1±0.2 89.6±0.3
71.8±0.6 80.5
Annealing
KD 59.8±0.3
90.7±0.3 90.0±0.5 80.7±0.2 89.3±0.5 70.7±0.5 80.2
FilterKD 59.2±0.4
90.7±0.2 89.5±0.3 80.4±0.3 89.3±0.2 69.6±0.9 79.8
MetaDistill 60.4±0.2
90.8±0.3 91.4±0.4
81.3±0.1
89.5±0.2 71.9±0.7 80.9
PTLoss(
ours) 61.2±0.3 91.1±0.1
91.2±0.3 83.5±0.2 90.3±0.1 73.1± 0.5 81.8
T
eacher T5-large 61.4
93.6 92.1 87.2 95.5 77.9 84.6
Standar
d KL 54.8±0.2
90.0±0.1 87.8±0.3 77.6±0.2 88.8±0.1 69.5±0.2 78.1
T
emp. Scaling 55.6±0.3
90.4±0.1 88.7±0.2 79.4±0.2 89.2±0.5 70.4±0.6 79.1
Lab
el Smoothing 56.4±0.4
90.6±0.2 89.2±0.6 79.2±0.4 89.2±0.4 69.1±1.2 79.1
Fo
cal 56.0±0.2
90.3±0.1 88.4±0.5 79.9±0.4 89.3±0.5 68.9±0.5 78.8
F
looding 57.8±0.3
90.0±0.6 89.5±0.4 79.5±0.4 89.0±0.4 68.9±0.6 79.3
CRD 58.2±0.3
90.2±0.4 89.8±0.3
80.3±0.2 89.4±0.4
70.5±0.5 79.7
Annealing
KD 58.3±0.2
90.4±0.3 89.8±0.5
79.9±0.1 89.4±0.4
69.7±0.4 79.6
FilterKD 56.7±0.3
90.2±0.2 89.1±0.4 78.8±0.2 89.2±0.3 69.2±0.6 78.9
MetaDistill 58.6±0.2 90.7±0.3
89.6±0.1 81.0±0.1
89.3±0.2 70.4±0.2 80.1
PTLoss(
ours) 60.5±0.2 90.7±0.1 91.1±0.4 82.7±0.1 90.0±0.2 71.0±0.3 81.0
0.15 0.20 0.25 0.30 0.35
Teacher Model Validation TVDStudent Test Accuracy0.600.650.700.75y = -0.228*x + 0.737 ,  R2=0.84
(a) Correlation between the teacher-to-ground-truth distance and the
student model performance. Experiments are conducted on BoolQ.
1 2 3 4 5
Perturbation Order MMNLI Accuracy
89.890.090.290.490.690.891.091.2
KL loss90.84 90.85
90.7891.01
90.84
90.41random coeﬃcient proxy teacher coeﬃcient(b) The Proxy Teacher method v.s. random search for the perturbation
coefficient selection. Experiments are conducted on MNLI.
Figure 4: PTLoss analysis.
4285Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Results on CIFAR-100 dataset.
MobileNetV2 ShuffleNetV2 ResNet18 GoogleNet DenseNet121 ResNeXt29
Baseline 68.38 70.34 75.87 78.72 79.04 81.03
Tf-KD 70.14±0.08 71.64± 0.17 76.60± 0.06 79.62± 0.43 79.54± 0.16 80.75± 0.13
PTLoss 70.62±0.16 71.97± 0.10 77.55±0.06 80.22±0.11 80.22± 0.11 81.83±0.21
which makes the random PTLoss even worse than the standard
KL loss. Conversely, equipped with the perturbation coefficients
obtained via proxy teacher, PTLoss can significantly outperform
the underlying KL loss.
5.4 Experiments on the CIFAR-100 dataset
To evaluate PTLoss on diverse tasks, we conduct this set of experi-
ments on the CIFAR-100 dataset. We follow [ 36] to get the baseline
results on the studied networks. Then we re-implement Tf-KD
in [36] as we don’t have access to the ground truth data during
the distillation stage. Specifically, The Tf-KD implementation is
modified from Tf-KD_self to be incorporated into our setting: we
modified Eq.(7) in [ 36] as𝐿𝑠𝑒𝑙𝑓=𝐷KL(𝑝𝑡𝜏,𝑝𝜏), where𝑝𝜏and𝑝𝑡𝜏are
the output probability of the student model and the pre-trained
student model, reshaped by a temperature 𝜏. For our methods,
we simply add 1-order perturbation to 𝑝𝑡𝜏, the𝜖is selected from
{0.1,0.2,0.5}. From Table 2, we observe that PTLoss can still outper-
form those baselines, which shows the applicability of PTLoss on
computer vision tasks.
6 RELATED WORK
6.1 Knowledge Distillation
Knowledge distillation was first proposed in [ 4] to compress the
large models to smaller, faster models without a significant per-
formance drop. Hinton et al . [11] generalized this technique by
introducing a temperature parameter to smooth the teacher model
prediction. Tian et al . [30] employed contrastive learning to train
the student model. Later, Yuan et al . [36] explored the connection
between KD and label smoothing, Jafari et al . [14] and Chen et al .
[5]studied the feeding mechanism of the teacher’s knowledge.
Zhao et al . [38] decoupled the classical loss to target classes and
non-target classes for KD efficiency and flexibility. Ren et al . [27]
investigated supervisory signals and proposed to average teacher
outputs for KD stability, while Zhou et al . [40] evolves the teacher
model with the student feedback in a meta learning framework. Ad-
ditionally, there is also significant research on applying KD to text
generation tasks, as demonstrated by [ 1,10,16,33,37]. However,
the exploration of KD in text generation represents a distinct and
extensive area of study, it is beyond the scope of this paper because
our focus is on KD within language understanding tasks.
6.2 Distillation Theory
Concurrent with the empirical success of knowledge distillation, nu-
merous works aim to understand its mechanisms. Hinton et al . [11]
suggest that teacher’s soft labels offer “dark knowledge” through
weights on incorrect labels. Menon et al . [22] present a statistical
view, observing that a good teacher model should be Bayesian toreduce the student objective variance. Stanton et al . [28] highlight
discrepancies between teacher and student output distributions
and emphasize the optimization challenge in distillation. While
more recent studies [ 2,12,15,39] explore distillation from several
various angles, a gap remains between the theoretical analysis and
the improved distillation techniques.
6.3 Loss Function Design
Our work also relates to loss function design. Lin et al . [19] propose
a modification of the cross-entropy loss function, reshaping it to
focus more on hard examples and address issues of data imbalance.
Leng et al . [18] expand cross-entropy loss and focal loss into a linear
combination of polynomial functions, primarily studying Poly-1
formulation on computer vision tasks while avoiding issues with
high-order polynomial hyper-parameter searches. TaylorGLO [ 9]
utilizes Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
to optimize multivariate Taylor parameterization of a loss function
and learning rate schedule. However, it lacks a principled analysis
of the performance gains following perturbations. In contrast, our
work provides both theoretical and empirical evidence supporting
the necessity of incorporating perturbations into the Knowledge
Distillation (KD) learning objective, especially when employing a
high-fidelity teacher for effective student supervision.
7 CONCLUSIONS AND FUTURE WORK
In this study, we proposed a novel knowledge distillation loss
PTLoss which implicitly shifts the teacher model output distri-
bution to a high-fidelity one for student model training. We also
established connections between PTLoss and other loss functions
by demonstrating that PTLoss can subsume the others while provid-
ing more flexible adjustments to teacher models. We theoretically
showed how the teacher model affects the student model risks and
presented a principled method to systematically search perturba-
tion coefficients. Extensive experiments on multiple tasks verified
our proposed theory and validated the effectiveness of PTLoss.
While PTLoss enables better KD by creating a proxy teacher
closer to the ground truth distribution, we focus on the single-
teacher-single-student setting in this work. It is worth exploring
how this approach can be extended to ensemble KD involving
multiple teachers or students. Additionally, although the proposed
coefficients selection method provides a principal way to determine
the perturbation hyperparameters, it remains challenging to scale
up the number of classes and the perturbation order. Future work
could benefit from developing scalable methods for hyperparameter
search, enabling rapid determination of perturbation coefficients
even in high-dimensional spaces with numerous classes or high
perturbation orders.
4286KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
REFERENCES
[1]Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist,
and Olivier Bachem. 2023. GKD: Generalized Knowledge Distillation for Auto-
regressive Sequence Models. arXiv preprint arXiv:2306.13649 (2023).
[2]Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Towards Understanding Ensemble,
Knowledge Distillation and Self-Distillation in Deep Learning. In ICLR.
[3]Stéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. 2005. Theory of
classification: A survey of some recent advances. ESAIM: probability and statistics
9 (2005), 323–375.
[4]Cristian Bucilu ˇa, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model
compression. In Proceedings of the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining. 535–541.
[5]Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. 2021. Distilling
knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 5008–5017.
[6]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty
of natural yes/no questions. arXiv preprint arXiv:1905.10044 (2019).
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[8]William B. Dolan and Chris Brockett. 2005. Automatically Constructing a Corpus
of Sentential Paraphrases. In Proceedings of the Third International Workshop on
Paraphrasing (IWP2005). https://aclanthology.org/I05-5002
[9]Santiago Gonzalez and Risto Miikkulainen. 2021. Optimizing loss functions
through multi-variate taylor polynomial parameterization. In Proceedings of the
Genetic and Evolutionary Computation Conference. 305–313.
[10] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge Distillation
of Large Language Models. arXiv preprint arXiv:2306.08543 (2023).
[11] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al .2015. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531 2, 7 (2015).
[12] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. 2021. Generalization
bounds via distillation. arXiv preprint arXiv:2104.05641 (2021).
[13] Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama.
2020. Do we need zero training loss after achieving zero training error? arXiv
preprint arXiv:2002.08709 (2020).
[14] Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, and Ali Ghodsi. 2021. Anneal-
ing Knowledge Distillation. In Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main Volume. 2493–2504.
[15] Guangda Ji and Zhanxing Zhu. 2020. Knowledge distillation in wide neural
networks: Risk bound, data efficiency and imperfect teacher. Advances in Neural
Information Processing Systems 33 (2020), 20823–20833.
[16] Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation.
arXiv preprint arXiv:1606.07947 (2016).
[17] Solomon Kullback. 1959. Statistics and information theory.
[18] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang
Cheng, and Dragomir Anguelov. 2022. PolyLoss: A Polynomial Expansion Per-
spective of Classification Loss Functions. In International Conference on Learning
Representations. https://openreview.net/forum?id=gSdSJoenupI
[19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE international
conference on computer vision. 2980–2988.
[20] Boxiao Liu, Shenghan Zhang, Guanglu Song, Haihang You, and Yu Liu. 2021.
Rectifying the Data Bias in Knowledge Distillation. In 2021 IEEE/CVF International
Conference on Computer Vision Workshops (ICCVW). 1477–1486. https://doi.org/
10.1109/ICCVW54120.2021.00171[21] Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar.
2021. Teacher’s pet: understanding and mitigating biases in distillation. arXiv
preprint arXiv:2106.10494 (2021).
[22] Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv
Kumar. 2021. A statistical perspective on distillation. In International Conference
on Machine Learning. PMLR, 7632–7642.
[23] Rafael Müller, Simon Kornblith, and Geoffrey E. Hinton. 2019. When Does Label
Smoothing Help?. In NeurIPS.
[24] OpenAI. 2022. ChatGPT.
[25] OpenAI. 2023. GPT-4 Technical Report.
[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al .2020. Exploring the
limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.
Res.21, 140 (2020), 1–67.
[27] Yi Ren, Shangmin Guo, and Danica J Sutherland. 2022. Better Supervisory Signals
by Observing Learning Paths. arXiv preprint arXiv:2203.02485 (2022).
[28] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and
Andrew G Wilson. 2021. Does knowledge distillation really work? Advances in
Neural Information Processing Systems 34 (2021), 6906–6919.
[29] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2818–2826.
[30] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2019. Contrastive representation
distillation. arXiv preprint arXiv:1910.10699 (2019).
[31] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R
Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural
language understanding. arXiv preprint arXiv:1804.07461 (2018).
[32] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural Network
Acceptability Judgments. arXiv preprint arXiv:1805.12471 (2018).
[33] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. 2023. f-Divergence Minimization
for Sequence-Level Knowledge Distillation. arXiv preprint arXiv:2307.15190
(2023).
[34] Jennifer C. White and Ryan Cotterell. 2021. Examining the Inductive Bias of
Neural Language Models with Artificial Languages. In Annual Meeting of the Asso-
ciation for Computational Linguistics. https://api.semanticscholar.org/CorpusID:
235293810
[35] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage
challenge corpus for sentence understanding through inference. arXiv preprint
arXiv:1704.05426 (2017).
[36] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. 2020. Revisiting
knowledge distillation via label smoothing regularization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3903–3911.
[37] Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han,
Jialu Liu, Simon Baumgartner, Michael Bendersky, and Chao Zhang. 2024. PLaD:
Preference-based Large Language Model Distillation with Pseudo-Preference
Pairs. arXiv preprint arXiv:2406.02886 (2024).
[38] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. 2022. Decoupled
knowledge distillation. In Proceedings of the IEEE/CVF Conference on computer
vision and pattern recognition. 11953–11962.
[39] Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan,
and Qian Zhang. 2021. Rethinking soft labels for knowledge distillation: A
bias-variance tradeoff perspective. arXiv preprint arXiv:2102.00650 (2021).
[40] Wangchunshu Zhou, Canwen Xu, and Julian McAuley. 2022. BERT Learns to
Teach: Knowledge Distillation with Meta Learning. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 7037–7049.
4287Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Dataset Statistics
Dataset
Task Train Distillation Dev Test
CoLA
Linguistic Acceptability 8.5k 8.5k 3k 1k
MNLI Natural Language Inference 58.9k 314k 19.6k 9.8k
MRPC Paraphrase Similarity Matching 3.7k 3.7k 1k 1.7k
RTE Textual Entailment Inference 2.5k 2.5k 0.8k 3k
SST-2 Sentiment Analysis 6.7k 53.8k 6.7k 872
BoolQ Boolean Question Answering 2.5k 5.9k 1k 3.2k
Table 4: The search range of hyper-parameters.
Hyper-parameter Search Range
Learning Rate {2,3,5}×10−5
Batch Size {8,16,32,64,128,256}
Temperature 𝑇 {0.1,0.2,0.5,1.0,2.0,5.0,10}
Label Smoothing 𝛿 {0.02,0.05,0.1,0.15,0.2}
Focal Loss𝜏 {0.1,0.2,0.5,1,2.0,5.0}
Random PTLoss 𝜖𝑐,𝑚 [−1,10]
8 APPENDIX
8.1 Dataset Statistics
We list the dataset statistics in Table 3.
8.2 Hyper-parameters
We list the search range of hyperparamters in Table 4. The search
for batch size and learning rate is applied to all the methods. And
for each baseline, we search for the best baseline-specific hyper-
parameters.
8.3 Connections between PTLoss and other
Perturbation Methods
In Section 4.1, we have presented the connection between PTLoss and
other perturbation methods. The connection between PTLoss and
the standard KL loss is direct, and it has been clearly presented how
PTLoss subsumes the focal loss. Now we detail the derivation of
the connection between PTLoss and temperature scaling and label
smoothing here.
Temperature Scaling. We compare PTLoss with temperature scal-
ing and claim that PTLoss subsumes it with appropriate approxima-
tion. As described in Hinton et al . [11] , the logits are adjusted by a
temperature to control sharpness or smoothness of the probability
distribution.
p0,𝜏=𝑒𝑥𝑝(𝑧0/𝜏)
𝑒𝑥𝑝(𝑧0/𝜏)+𝑒𝑥𝑝(𝑧1/𝜏), (21)
where𝜏is the temperature and 𝑧𝑐is the logits. Here we use binary
classification (i.e., 𝑐=0,1) for the derivation simplicity without loss
of generality. Denote the probability without temperature scaling
asp0, we have
p0,𝜏
p0=𝑒𝑥𝑝(𝑧0/𝜏)
𝑒𝑥𝑝(𝑧0/𝜏)+𝑒𝑥𝑝(𝑧1/𝜏)×𝑒𝑥𝑝(𝑧0)+𝑒𝑥𝑝(𝑧1)
𝑒𝑥𝑝(𝑧0)
=1+𝑒𝑥𝑝(𝑧1−𝑧0)
1+𝑒𝑥𝑝(𝑧1−𝑧0
𝜏).(22)In practice, we have
1+𝑒𝑥𝑝(𝑧1−𝑧0)
1+𝑒𝑥𝑝(𝑧1−𝑧0
𝜏)≈1or𝑒𝑥𝑝(𝑧1−𝑧0)
𝑒𝑥𝑝(𝑧1−𝑧0
𝜏)(23)
because|𝑧1−𝑧0|≫0. Then we have
p0,𝜏≈p0or𝑒𝑥𝑝(𝑧1−𝑧0)
𝑒𝑥𝑝(𝑧1−𝑧0
𝜏)p0. (24)
For the first case where p0,𝜏≈p0, we omit the discussion as it aligns
with the standard KL loss. For the second case, we proceed to draw
its connection with PTLoss as follows. We denote p𝑡𝜏,𝑐,p𝑠𝜏,𝑐as the
teacher, student probability scaled by temperature 𝜏. Incorporating
temperature scaling, the KL loss can be formulated as:
ℓ𝑡𝑒𝑚𝑝
𝐾𝐿 p𝑡
𝜏,p𝑠
𝜏=−H p𝑡
𝜏+∑︁
𝑐∈[𝐶]p𝑡
𝜏,𝑐·(−logp𝑠
𝜏,𝑐), (25)
Substituting p𝑡𝜏,𝑐andp𝑠𝜏,𝑐in Eq. 25 using Eq. 24, we obtain
ℓ𝑡𝑒𝑚𝑝
𝐾𝐿 p𝑡
𝜏,p𝑠
𝜏=−H p𝑡
𝜏+∑︁
𝑐∈[𝐶]𝑎·p𝑡
𝑐·[−log(𝑏·p𝑠
𝑐)],(26)
where𝑎=𝑒𝑥𝑝(𝑧𝑡
1−𝑧𝑡
0)/𝑒𝑥𝑝(𝑧𝑡
1−𝑧𝑡
0
𝜏), and𝑏=𝑒𝑥𝑝(𝑧𝑠
1−𝑧𝑠
0)/𝑒𝑥𝑝(𝑧𝑠
1−𝑧𝑠
0
𝜏).
Comparing above Eq. 26 with Eq. 5, we can set
∞∑︁
𝑚=1(1
𝑚+𝜖𝑐,𝑚)·(1−p𝑠
𝑐)𝑚=𝑎[∞∑︁
𝑚=11
𝑚·(1−p𝑠
𝑐)𝑚+∞∑︁
𝑚=11
𝑚·(1−𝑏)𝑚].
(27)
It leads to
𝜖𝑐,𝑚=𝑎
𝑚·(1+(1−𝑏
1−p𝑠𝑐)𝑚)−1
𝑚, (28)
which indicates PTLoss can encompass the temperature-scaled dis-
tillation loss by setting a group of appropriate perturbation coeffi-
cients.
4288KDD ’24, August 25–29, 2024, Barcelona, Spain Rongzhi Zhang et al.
Label Smoothing. We compare PTLoss with the label smoothing
method and claim that label smoothing proposed in [ 29] is a special
case of PTLoss . According to the implementation in Szegedy et al .
[29], we can smooth the teacher labels in KD by
p𝑡𝑙𝑠𝑐=(1−𝛿)p𝑡
𝑐+𝛿/2, (29)
with a smoothing parameter 𝛿. Starting from Eq. 5, we can replace
the term p𝑡𝑐by its smooth version p𝑡𝑙𝑠𝑐. Then the original Eq. 5 with
label smoothing becomes:
ℓ𝑙𝑠
𝐾𝐿 p𝑡,p𝑠=−H p𝑡𝑙𝑠+∑︁
𝑐∈[𝐶]p𝑡𝑙𝑠𝑐·(−logp𝑠
𝑐) (30)
For the entropy of the teacher output, the smooth version H p𝑡𝑙𝑠
is different from the original H p𝑡with only a constant 𝐶, which
can be ignored when optimizing the loss function. We introduce
Δp𝑡𝑐=𝛿/2−𝛿p𝑡𝑐and replace all the p𝑡𝑙𝑠𝑐in Eq. 30 by p𝑡𝑙𝑠𝑐=p𝑡𝑐+Δp𝑡𝑐,
then we get:
ℓ𝑙𝑠
𝐾𝐿 p𝑡,p𝑠=−H p𝑡𝑙𝑠+∑︁
𝑐∈[𝐶](p𝑡
𝑐+Δp𝑡
𝑐)·(− logp𝑠
𝑐) (31)
Similarly, we let ℓ𝑙𝑠
𝐾𝐿 p𝑡,p𝑠=ℓ𝑃𝑇 p𝑡,p𝑠, it yields
∑︁
𝑐∈[𝐶](p𝑡
𝑐+Δp𝑡
𝑐)∞∑︁
𝑚=11
𝑚·(1−p𝑠
𝑐)𝑚=∑︁
𝑐∈[𝐶]p𝑡
𝑐∞∑︁
𝑚=1(1
𝑚+𝜖𝑐,𝑚)·(1−p𝑠
𝑐)𝑚.
(32)
We obtain
𝜖𝑐,𝑚=Δp𝑡𝑐
𝑚p𝑡𝑐Δp𝑡
𝑐=𝛿/2−𝛿p𝑡
𝑐−−−−−−−−−−−−−→ 𝜖𝑐,𝑚=𝛿
𝑚(1
2p𝑡𝑐−1). (33)
In summary, the connection between the two losses can be ex-
pressed through a specific 𝜖𝑐,𝑚, which depends on the smoothing
parameter𝛿. This derivation highlights that PTLoss generalizes the
label smoothing method and provides a more flexible framework
that encompasses the effects of label smoothing.
8.4 Proof of Theorem 1
The theorem 1 states that given a teacher model p𝑡, an unlabeled
distillation dataset D𝑢with an unknown true distribution p∗, we
have for any probability predictor p:X→R𝐶:
E
(˜𝑅𝐾𝐿(p;p𝑡,D𝑢)−𝑅(p))2
≤2
𝑁𝑢·V
p𝑡(𝑥)𝑇log(p(𝑥))
+
O E𝑥
∥p𝑡(𝑥)−p∗(𝑥)∥22+
E𝑥
p𝑡(𝑥)𝑇logp𝑡(𝑥)2
,
where V[·]denotes the variance of a random variable.
Proof. We first rewrite the population risk 𝑅(p)with cross-
entropy loss 𝑙𝐶𝐸plugged in as follow:
𝑅(p)=E(𝑥,𝑦)[ℓ(𝑦,p(𝑥))]
=E𝑥[E𝑦|𝑥[ℓ(𝑦,p(𝑥))]]
=E𝑥[p∗(𝑥)𝑇(−log(p(𝑥)))].(34)Then, we write out the distillation empirical distillation risk defined
in Eq. 3 and have:
˜𝑅𝐾𝐿(p;p𝑡,D𝑢)−𝑅(p)=1
𝑁𝑢𝑁𝑢∑︁
𝑛=1p𝑡(𝑥𝑛)𝑇(−log(p(𝑥𝑛)))
+1
𝑁𝑢𝑁𝑢∑︁
𝑛=1p𝑡(𝑥𝑛)𝑇log(p𝑡(𝑥𝑛))−E𝑥[p∗(𝑥)𝑇(−log(p(𝑥)))].
(35)
We let
Δ1
𝑁𝑢𝑁𝑢∑︁
𝑛=1p𝑡(𝑥𝑛)𝑇(−log(p(𝑥𝑛)))−E𝑥[p∗(𝑥)𝑇(−log(p(𝑥)))],
and
𝐻1
𝑁𝑢𝑁𝑢∑︁
𝑛=1p𝑡(𝑥𝑛)𝑇log(p𝑡(𝑥𝑛)),
then
E
(˜𝑅𝐾𝐿(p;p𝑡,D𝑢)−𝑅(p))2
=E
(Δ+𝐻)2
≤2E
Δ2
+2E
𝐻2
=2V[Δ]+2E[Δ]2+2E
𝐻2
(36)
where the second line is by the inequality (𝑎+𝑏)2≤2𝑎2+2𝑏2
and the linearity of expectation, and the third line is by E
Δ2
=
V[Δ]+E[Δ]2. Observe that
E[Δ]=E𝑥h
(p𝑡(𝑥)−p∗(𝑥))𝑇(−log(p𝑡(𝑥𝑛)))i
≤E𝑥
∥p𝑡(𝑥)−p∗(𝑥)∥2·∥log(p𝑡(𝑥𝑛))∥2
≤E𝑥
∥p𝑡(𝑥)−p∗(𝑥)∥2·𝑐1·∥log(p𝑡(𝑥𝑛))∥∞
≤𝑐2E𝑥
∥p𝑡(𝑥)−p∗(𝑥)∥2
,(37)
where the second line is by the Cauchy-Schwartz inequality, the
third line is by the equivalence of norms with a constant 𝑐1, and
the last line is by the boundedness of the log loss term3.
Furthermore, we notice the 𝑅(p)term in the above Δis a constant
and thus have:
V[Δ]=V˜𝑅𝐾𝐿(p;p𝑡,D𝑢)
=1
𝑁𝑢·Vh
p𝑡(𝑥)𝑇(−log(p(𝑥)))i
=1
𝑁𝑢·Vh
p𝑡(𝑥)𝑇log(p(𝑥))i
,
(38)
where the last equation comes from V[𝑎𝑋+𝑏]=𝑎2V[𝑋].
Finally, we plug in Eqs (37)(38) and the definition of 𝐻into
Eq. (36) and complete the proof. □
8.5 Search Time of Perturbation Coefficients
For each perturbation order, we randomly sample 100coefficient
sets from[−1,10]and find the best set that has the lowest risk
deviation gap according to Eq. 20 with 1000 validation examples.
The whole process takes less than two minutes on CPU with 64G
memory.
3This is a common assumption defined in previous literature such as (Boucheron
et al. [3], Theorem 4.1; Menon et al . [22] , Proposition 2) and can be achieved easily in
practice with regularization techniques.
4289