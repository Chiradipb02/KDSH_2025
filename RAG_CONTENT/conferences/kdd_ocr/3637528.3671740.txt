Rotative Factorization Machines
Zhen Tian†
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
chenyuwuxinn@gmail.comYuhong Shi‡
Zhejiang University
Hangzhou, China
shi.yh@zju.edu.cnXiangkun Wu‡
Zhejiang University
Hangzhou, China
xkwsta@zju.edu.cn
Wayne Xin Zhao∗†
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
batmanfly@gmail.comJi-Rong Wen†
Gaoling School of Artificial
Intelligence, Renmin University of
China
Beijing, China
jrwen@ruc.edu.cn
ABSTRACT
Feature interaction learning (FIL) focuses on capturing the complex
relationships among multiple features for building predictive mod-
els, which is widely used in real-world tasks. Despite the research
progress, existing FIL methods suffer from two major limitations.
Firstly, they mainly model the feature interactions within a bounded
order (e.g., small integer order) due to the exponential growth of the
interaction terms. Secondly, the interaction order of each feature is
often independently learned, which lacks the flexibility to capture
the feature dependencies in varying contexts.
To address these issues, we present Rotative Factorization Ma-
chines (RFM), based on the key idea that represents each feature
as a polar angle in the complex plane. As such, the feature inter-
actions are converted into a series of complex rotations, where
the orders are cast into the rotation coefficients, thereby allowing
for the learning of arbitrarily large order. Further, we propose a
novel self-attentive rotation function that models the rotation coef-
ficients through a rotation-based attention mechanism, which can
adaptively learn the interaction orders under different interaction
contexts. Moreover, it incorporates a modulus amplification net-
work to learn the modulus of the complex features, which further
enhances the expressive capacity. Our proposed approach provides
a general FIL framework, and many existing models can be instan-
tiated in this framework, e.g.,factorization machines. In theory, it
possesses more strong capacities to model complex feature rela-
tionships, and can learn arbitrary features from varied contexts.
‡Equal contribution.
∗Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author.
†Also with Beijing Key Laboratory of Big Data Management and Analysis Methods.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671740Extensive experiments conducted on five widely used datasets have
demonstrated the effectiveness of our approach.
CCS CONCEPTS
•Computing methodologies →Neural networks.
KEYWORDS
Factorization Machine, Feature Interactions
ACM Reference Format:
Zhen Tian†, Yuhong Shi‡, Xiangkun Wu‡, Wayne Xin Zhao∗†, and Ji-Rong
Wen†. 2024. Rotative Factorization Machines. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671740
1 INTRODUCTION
Feature interaction learning (FIL) is a fundamental technique to sup-
port many real-world predictive tasks, such as click-through rate
(CTR) prediction [ 23,36,41] and product recommendation [ 19,28],
aiming to accurately model the complex relationship among multi-
ple features. Various methods [ 42] have been proposed for learning
effective feature interactions, from early factorization machines
(e.g., FM [ 23]) to recent deep neural networks (e.g., CrossNet [ 37]).
Typically, existing FIL methods are often developed based on
a combination of multiple feature interaction terms, where each
term is modeled as a combination of feature embeddings with their
respective interaction orders, formally denoted by 𝒆𝛼1
1⊙···⊙ 𝒆𝛼𝑚𝑚
(“⊙” denotes the embedding combination operation). The order 𝛼𝑗
determines the effect of the 𝑗-th feature embedding 𝒆𝑗. To learn the
feature interactions, prior studies [ 15,37,41] often set a maximal
order and conduct feature interactions within the predefined order.
Despite the progress, they suffer from the limitation in the learning
with restricted orders (e.g., integer-only order [15]). Further, due to
the exponential growth of feature combinations, these methods can
only learn the interactions within a small order to maintain the
efficiency, e.g.,second-order feature interactions for FM [23].
Considering the above issues, several studies [ 4,7,31] propose to
automatically learn the interaction orders from data. The core idea
of these approaches is to map features into a special vector space
 
2912
KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
(e.g., logarithmic vector space [ 7]). As such, the exponential form of
interaction terms (i.e.,Î𝒆𝛼𝑗
𝑗) is converted to linear combinations
(i.e., exp(Í𝛼𝑗log𝒆𝑗)), and the orders (i.e., 𝛼𝑗) are cast into learn-
able linear coefficients, allowing for the learning of adaptive-order
interactions. These methods learn the orders either in a field-aware
way or in an instance-aware way. As shown in Figure 1(a) and
Figure 1(b), given two fields along with their feature interaction,
field-aware methods learn a shared order for all features from the
same field (e.g., 𝛼𝐺is shared by Male,Female for field Gender ),
capturing the field-level importance, whereas the instance-aware
methods assign a specific order for each feature (e.g., 𝛼𝑀,𝛼𝐹for
Male,Female) to learn the feature importance.
Although these approaches are more capable of learning the un-
derlying relationships in real-world scenarios, they still suffer from
two main limitations. First, the interaction order of each feature
is independently learned, which lacks the flexibility to capture the
feature dependencies in varying contexts. As increasing evidence
shows [ 35], in real-world applications, the importance of a certain
feature is often influenced by other features. For example, consid-
ering the feature interaction ⟨UserGender ,MovieGenre ,Actor⟩
in movie recommendation, the Actor features may have different
effects for idolandhorror movie genres. However, it is difficult for
both field-aware and instance-aware models to adaptively capture
varying feature importance in different contexts. As such, we ar-
gue that the importance of a specific feature should be adaptively
learned depending on its relation with the other features involved
in the specific context, which is referred to as relation-aware in this
paper (See Figure 1(c)). Second, since the interaction terms expo-
nentially grow with the order, existing methods can only model the
interactions within a bounded order, which cannot scale to the high-
order cases in industrial scenarios. Considering the two limitations,
we aim to seek a more effective approach to adaptively learn the
interaction order in a relation-aware way, meanwhile surpass the
scale limits of high interaction order in existing work.
𝒆𝑴𝜶1⊙𝒆𝑶𝜷𝟏
𝒆𝑴𝜶2⊙𝒆𝒀𝜷𝟐
𝒆𝑭𝜶3⊙𝒆𝑶𝜷𝟑
(a)Field -aware (b)Instance -aware (c) Relation -aware
(AFN, EulerNet) (ARM -Net) (Ours)𝜶𝑮𝜶1=𝜶𝟐=𝜶𝟑=𝜶𝑮
𝒆𝑴𝜶1⊙𝒆𝑶𝜷𝟏
𝒆𝑴𝜶2⊙𝒆𝒀𝜷𝟐
𝒆𝑭𝜶3⊙𝒆𝑶𝜷𝟑𝜶𝑴𝜶1=𝜶𝟐=𝜶𝑴 𝜶𝟑=𝜶𝑭
𝜶𝑭𝒆𝑴𝜶1⊙𝒆𝑶𝜷𝟏
𝒆𝑴𝜶2⊙𝒆𝒀𝜷𝟐
𝒆𝑭𝜶3⊙𝒆𝑶𝜷𝟑𝜶1=𝜶𝑴,𝑶𝜶2=𝜶𝑴,𝒀𝜶𝟑=𝜶𝑭,𝑶
𝜶𝑴,𝑶
𝜶𝑴,𝒀
𝜶𝑭,𝑶Field：
Instance
Field：Male  (M)FeMale  (F)Field
Younger  (Y)Older  (O) Gender (G) Age (O)Instance
Male  (M) FeMale  (F)Feature Field Feature Instance
Younger  (Y) Older  (O)
 Gender (G) Age (O)Feature Field Feature InstanceField Instance
Figure 1: Comparisons of three feature interaction approaches. Field-
aware methods set a fixed interaction order for each feature field ;
instance-aware methods set a unique interaction order for each
feature instance (a.k.a., feature value ); relation-aware methods set a
unique interaction order for each feature combination.
To this end, this paper presents the novel Rotative Factorization
Machines (RFM), for adaptively learning the unbounded-order fea-
ture interactions in a relation-aware way. The key idea of RFM is
to represent each feature as a polar angle (i.e.,𝑒𝑖𝜽𝑗) in the complex
plane, and conduct the attentive rotations to model complicated
feature interactions. For learning the unbounded-order feature in-
teractions, RFM converts the feature interactions into the com-
plex rotations (i.e.,exp(𝑖Í𝛼𝑗𝜽𝑗)), where the interaction orders arecast into the rotation coefficients (i.e.,𝛼𝑗), thereby avoiding the
exponential explosion of the interaction terms. For learning the
feature interactions in a relation-aware way, we propose a novel
self-attentive rotation function (i.e., exp(𝑖Í𝛼𝑗,𝑙𝜽𝑙)), where the rota-
tion coefficients (i.e., 𝛼𝑗,𝑙) are learned by a rotation-based attention
mechanism, capturing the dependencies between feature 𝑗and𝑙.
Moreover, we devise a modulus amplification network to learn
the modulus of the complex features (extending the above unit
modulus), which enhances the expressive capacity of the learned
complex representations. Such a network can model all three types
of feature interaction patterns (i.e., field-aware, instance-aware and
relation-aware ), with no need of pre-defined order coefficients.
To our knowledge, it is the first work that is capable of adap-
tively learning the interactions with arbitrarily large order from
the corresponding contexts. Furthermore, it has been proven that
our approach can be instantiated to a variety of traditional inner-
product based interaction models (e.g., FM [ 23]). To evaluate our
model, we conduct extensive experiments on five public datasets,
and the experimental results show that our model consistently out-
performs a number of competitive feature interaction approaches.
2 PRELIMINARY
As the key technique in many prediction tasks [ 40,43], feature
interaction modeling aims to capture the underlying relationships
among multiple features. It takes as input a concatenated vector
of features, denoted as 𝒙=[𝒙1,...,𝒙𝑚], where𝑚represents the
number of feature fields (e.g., Gender), and 𝒙𝑗is the one-hot vector
of a feature instance (e.g., Male) in the 𝑗-th field. Due to the high-
dimensional, sparse nature of 𝒙, an embedding look-up operation
E(·)is often used to map each feature into a 𝑑-dimensional vector
𝒆𝑗=E(𝒙𝑗)∈R𝑑. In this context, the feature interaction learning
functionF(·) is commonly defined as:
F(A) =∑︁
𝜶∈A𝒆𝛼1
1⊙𝒆𝛼2
2⊙···⊙ 𝒆𝛼𝑚𝑚, (1)
where “⊙” denotes the element-wise product, Arepresents the set
of all interaction orders, and each 𝜶∈A specifies the order for
each feature. Existing methods often manually set feature inter-
action orders. For instance, FM [ 23] assignsA={𝜶|Í𝑚
𝑗=1𝛼𝑗=
2,∀𝛼𝑗∈{0,1}}to capture second-order feature interactions. Fur-
ther, AFN [ 7] and EulerNet [ 31] propose automatically learning
orders (i.e.,A) from data. However, they primarily capture field-
aware interactions, where the order 𝜶is shared across all features
within a field. Another study ARM-Net [ 4] introduces a gated at-
tention Gate(·)forinstance-aware interactions, with 𝛼𝑗=Gate(𝒆𝑗)
evaluating feature importance. In contrast, we focus on learning
relation-aware interactions, where 𝛼𝑗considers the dependencies
between 𝒆𝑗and other features.
3 METHODOLOGY
In this section, we present the proposed Rotative Factorization
Machines (RFM) (Figure 2(a)) for effectively modeling feature in-
teractions in various prediction tasks. As the core idea, we repre-
sent each feature as a polar angle in the complex plane and use the
attentive rotation to model complicated feature interactions. For
learning relation-aware interactions, we propose a self-attentive
 
2913Rotative Factorization Machines KDD ’24, August 25–29, 2024, Barcelona, Spain
Input FeaturesAngular EmbeddingAdd & Norm
𝑵×AmplificationGroup NormAdd
𝑳×Prediction
𝒆𝒊𝜽⇒𝒄𝒐𝒔𝜽 +𝒊𝒔𝒊𝒏𝜽
Self-Attentive RotationMulti -Head
(a) Model Architecture (b) Self -Attentive Rotation MechanismQuery
KeyWeightCosineDot𝒘Rotation -based Attention
𝑺𝒊𝒈𝒎𝒐𝒊𝒅
rp
0𝑸𝒋
𝑲𝒍𝑺𝒊𝒈𝒎𝒐𝒊 𝒅𝒘𝑻𝒄𝒐𝒔 𝑸𝒋−𝑲𝒍 𝜶𝒋,𝒍←𝑹𝒐𝒕𝑨𝒕𝒕 (𝑸𝒋,𝑲𝒍)Self-Attentive Rotation
0 r෤𝒆𝟏𝜶𝒋,𝟏⊙…⊙…⊙෤𝒆𝒍𝜶𝒋,𝒍
⊙
…
⊙
෤𝒆𝒎𝜶𝒋,𝒎pAttentive Rotations
෤𝒆𝟏𝜶𝒋,𝟏 ⋯⊙ ෤𝒆𝒍𝜶𝒋,𝒍··⊙෤𝒆𝒎𝜶𝒋,𝒎𝑸𝟏𝑸𝒋 𝑸𝒍 𝑸𝒎
𝑲𝟏 𝑲𝒋 𝑲𝒍 𝑲𝒎
𝑽𝟏 𝑽𝒋 𝑽𝒍 𝑽𝒎
𝑹𝒐𝒕𝑨𝒕𝒕 (𝑸𝒋,𝑲𝒍)
…𝜽𝟏
𝜽𝒎𝜽𝟐Input Angles
exp 𝑖(σ𝜶𝒋,𝒍𝑽𝒍)
Figure 2: Architecture and components of our proposed rotative factorization machines.
rotation layer, which can adaptively learn the orders from different
interaction contexts. Moreover, a modulus amplification network
is incorporated to learn the modulus of the complex features for
enhancing the expressive capacity. In what follows, we introduce
the details of relation-aware interaction modeling (Section 3.1) and
the modulus amplification network (Section 3.2).
3.1 Relation-Aware Feature Interaction
In this part, we propose a novel self-attentive rotation layer for
learning the relation-aware feature interactions, in which it rep-
resents each feature as a polar angle in the complex plane and
employs complex rotation to model complex feature interaction.
3.1.1 Angular Representation of Features. As mentioned in Sec-
tion 2, the feature 𝒙𝑗can be mapped into a vector embedding via
the look-up operation E(·). In order to support arbitrary interaction,
our solution is to represent the features as a set of polar angles in
the complex plane as follows:
𝜽𝑗=E(𝒙𝑗),𝒆𝑗=𝑒𝑖𝜽𝑗, (2)
where𝑖is the imaginary unit that satisfies 𝑖2=−1. In this way,
given the angular feature representations {𝒆𝑗}𝑚
𝑗=1∈C𝑚×𝑑, the
interactions are cast into a series of complex rotations :
F(A) =∑︁
𝜶∈A𝒆𝛼1
1⊙···⊙ 𝒆𝛼𝑚𝑚=∑︁
𝜶∈Aexp
𝑖𝑚∑︁
𝑗=1𝛼𝑗𝜽𝑗
|        {z        }
Complex Rotation.(3)
In mathematics, a complex rotation (i.e.,exp(𝑖Í𝑚
𝑗=1𝛼𝑗𝜽𝑗)) performs
a linear transformation on the phase of the complex vectors without
affecting their modulus. In our case, we utilize it to model the feature
interaction, and use the rotation coefficients (i.e.,𝛼𝑗) to model the
interaction orders. As such, the interactions are learned on a unit
circle (i.e.,modulus are fixed to 1) with a finite norm:
||F(A)|| =∑︁
𝜶∈Aexp
𝑖𝑚∑︁
𝑗=1𝛼𝑗𝜽𝑗
≤∑︁
𝜶∈Aexp
𝑖𝑚∑︁
𝑗=1𝛼𝑗𝜽𝑗≤|A|𝑑.(4)Since the upper bound is independent of the order 𝛼𝑗, it can effec-
tively learn complicated interactions with arbitrarily large order,
without limitations in prior work (e.g., exponential explosion ).
3.1.2 Self-Attentive Rotation. The self-attentive rotation layer is
the core of our proposed RFM for learning relation-aware feature
interactions. As shown in Figure 2(b), the key idea is to conduct
theattentive rotation with the rotation coefficients modeled by
arotation-based attention mechanism, thereby allowing for the
adaptive learning of feature dependencies in the varying context. It
takes as input a set of angles and outputs a set of rotated angles, and
then we can stack multiple such layers to form a capable network.
Next we describe the attentive rotation within a single layer.
Rotation-based Attention for Attentive Rotations. As shown
in Eq. (3), the interaction with the order 𝜶is cast into a complex rota-
tion (i.e., exp(𝑖Í𝑚
𝑗=1𝛼𝑗𝜽𝑗)). To learn the relation-aware interaction,
a major issue is how to effectively model the feature dependencies
in varying contexts. The original self-attention mechanism [ 34]
is designed to model relationships for real vectors, but not suit-
able for modeling relationships among angular representations. As
our solution, we propose a rotation-based attention mechanism to
adaptively model the rotation coefficients (i.e., 𝛼𝑗), which enables
it to effectively learn the dependencies between different angle-
represented features. As shown in Figure 2(b), we adopt a key-value
based self-attention to conduct the attentive rotation. Specifically,
the query-key pairs with similar angles are considered more impor-
tant. Given the input {𝜽𝑗}𝑚
𝑗=1, the dependency between feature 𝑗
and𝑙is learned by the rotation angle from key to query:
𝛼𝑗,𝑙=RotAtt(𝑸𝑗,𝑲𝑙)=Sigmoid(𝒘⊤cos(𝜽𝑄
𝑗−𝜽𝐾
𝑙)), (5)
𝑸⊤
𝑗=𝜽𝑄
𝑗=𝑾𝑄
𝑗𝜽𝑗,𝑲⊤
𝑙=𝜽𝐾
𝑙=𝑾𝐾
𝑙𝜽𝑙, (6)
where 𝒘∈R𝑑is a weight vector to learn. Given 𝑚feature fields,
to enhance the field semantics, we utilize a set of field-specific ma-
trices{𝑾𝑄
𝑗}𝑚
𝑗=1,{𝑾𝐾
𝑙}𝑚
𝑙=1to map the features into a set of queries
{𝜽𝑄
𝑗}𝑚
𝑗=1and keys{𝜽𝐾
𝑙}𝑚
𝑙=1, and pack them together into two matri-
ces𝑸,𝑲∈R𝑚×𝑑′for modeling the interaction orders. Accordingly,
the value vectors{𝜽𝑉
𝑗}𝑚
𝑗=1are packed into matrix 𝑽∈R𝑚×𝑑′, which
 
2914KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
represents the polar angles of the complex features {˜𝒆𝑗}𝑚
𝑗=1(i.e.,
˜𝒆𝑗=exp(𝑖𝜽𝑉
𝑗)). Further, we aggregate all contextual information of
feature𝑗as˜𝜽𝑗=Í𝑚
𝑙=1𝛼𝑗,𝑙𝜽𝑉
𝑙. As such, the interaction with order
A𝑗={𝜶𝑗}is cast into a self-attentive rotation with coefficients
learned by the proposed rotation-based attention (Eq. (5)):
F(A𝑗)=˜𝒆𝛼𝑗,1
1⊙˜𝒆𝛼𝑗,2
2⊙···⊙ ˜𝒆𝛼𝑗,𝑚
𝑚
=exp
𝑖𝑚∑︁
𝑙=1𝛼𝑗,𝑙𝜽𝑉
𝑙
|           {z           }
Self-Attentive Rotation=exp(𝑖˜𝜽𝑗). (7)
This formula is the core of RFM for learning relation-aware in-
teraction. The rotation coefficient 𝛼𝑗,𝑙, which also represents the
interaction order, can capture the dependencies between feature 𝑗
and𝑙. Further, we pack the orders into a matrix 𝑨(i.e.,𝑨𝑗,𝑙=𝛼𝑗,𝑙),
to aggregate the contextual information of all features:
AttentiveRot(𝑸,𝑲,𝑽)=[˜𝜽1,˜𝜽2,···,˜𝜽𝑚]⊤=𝑨𝑽. (8)
Formally, the tensor-form calculation of the rotation-based atten-
tion score can be also given by:
𝑨=RotAtt(𝑸,𝑲)=Sigmoid
Reh
exp(𝑖𝑸)diag(𝒘)
exp(−𝑖𝑲)⊤i
,
(9)
where Re[·]is real parts of complex vectors. Proof in Appendix A.1.
Multi-Head Rotation. To learn diversified contextual information
from different subspaces, we further extend the above transforma-
tion to multi-head rotation. Specifically, we introduce ℎindependent
attention heads performing the rotation function of Eq. (8), and
then concatenate them to obtain final representations:
MultiHeadRo(𝑸,𝑲,𝑽)=Concat(head 1,head 2,...,headℎ),(10)
head𝑗=AttentiveRot(𝑸𝑯𝑄
𝑗,𝑲𝑯𝐾
𝑗,𝑽 𝑯𝑉
𝑗),(11)
where 𝑯𝑄
𝑗,𝑯𝐾
𝑗,𝑯𝑉
𝑗∈R𝑑′×𝑑ℎare projection matrices, 𝑑ℎ=𝑑′/ℎ.
In this way, we can use the head number ℎto control the number
of interaction terms. Specifically, each head models 𝑚interaction
terms, where each term (i.e., ˜𝒆𝛼𝑗,1
1⊙···⊙ ˜𝒆𝛼𝑗,𝑚
𝑚) corresponds to the
attention scores 𝑨𝑗(i.e.,𝑗-th row of 𝑨), and thus the total interaction
term number is 𝑚·ℎ. Further, we can stack multiple layers by taking
the outputs of the previous layer as the input for the next layer, and
set varying ℎat different layers to increase the model flexibility.
In addition, to preserve the previously learned representations,
we follow the standard transformer [ 34] that employs a residual
connection with a layer normalization [1] around each layer.
3.2 Modulus Amplification for Enhanced
Feature Interaction Learning
In the above rotation process, the features are limited to a unit circle
with a fixed modulus of one, which may limit the model’s capacity.
To further enhance the interaction learning, we devise a modulus
amplification network to learn the modulus of the features.
Coordinate Transformation. In the above rotation process, the
modulus of the complex features are fixed at 1, which are difficult
to be identified or learned. Instead of directly learning the modulusof the complex features, our solution is to convert the polar angle-
represented features into the real-imaginary represented complex
vectors, enabling the model to effectively learn the modulus of
different complex features. Specifically, given the output represen-
tation𝑒𝑖˜𝜽𝑗(See Eq. (8)) of the last self-attentive rotation layer, we
use the Euler’s formula (i.e., 𝑒𝑖𝜽=cos𝜽+𝑖sin𝜽) to obtain its real
and imaginary parts:
𝒓𝑗=cos˜𝜽𝑗,𝒑𝑗=sin˜𝜽𝑗, (12)
where𝑗∈ {1,...,𝑚}. After the coordinate transformation, each
feature is represented by a rectangular-form complex vector, i.e.,
𝒓𝑗+𝑖𝒑𝑗. Then, we utilize the complex representations {𝒓𝑗+𝑖𝒑𝑗}𝑚
𝑗=1
for subsequent modulus amplification.
Modulus Amplification. Given the representations in the rectan-
gular form{𝒓𝑗+𝑖𝒑𝑗}𝑚
𝑗=1, we concatenate their real and imaginary
parts and feed them into a shared multi-layer perceptron (MLP):
𝒓(0)=Concat(𝒓1,...,𝒓𝑚),𝒑(0)=Concat(𝒑1,...,𝒑𝑚), (13)
𝒓(𝑘)=GN(𝜎(𝑾𝑘𝒓(𝑘−1)+𝒃𝑘)),𝒑(𝑘)=GN(𝜎(𝑾𝑘𝒑(𝑘−1)+𝒃𝑘)),
(14)
where𝑘∈{1,2,...,𝐿},𝐿is the depth, 𝜎is the activation function,
𝑾𝑘and𝒃𝑘are the weight and bias of the 𝑘-th layer. In the above
transformations, all feature vectors are concatenated into a long
hidden vector as the input of the MLP, which may be difficult to
distinguish semantic information from different features. To address
this problem, we use the group normalization [ 39] (i.e., GN(·)) to
preserve the feature-wise information. Formally, given the input
vector ( e.g.,𝒓∈R𝑑𝑘, where𝑑𝑘is the hidden dimension of 𝑘-th layer),
we view it as having 𝑙latent features (i.e., 𝒓=[𝒓1;𝒓2;...;𝒓𝑙],𝑙|𝑑𝑘),
andGN(·)is formulated as follows:
GN(𝒓𝑗)=𝛾·𝒓𝑗−𝜇𝑗√︃
𝜎2
𝑗+𝜖+𝛽, (15)
where𝑗∈ {1,2,...,𝑙},𝜇𝑗and𝜎𝑗denote the mean and standard
deviation of 𝒓𝑗, the scaling parameter 𝛾and shift parameter 𝛽are
trainable to enhance the representation of the GN (·)layer.
Prediction and Training. For prediction, we follow the prior
work [ 31] that incorporates a transformation vector 𝒖to project
the representation of the last layer (i.e., 𝒓(𝐿)+𝑖𝒑(𝐿)):
𝑧=𝒖⊤(𝒓(𝐿)+𝑖𝒑(𝐿))=𝑧𝑟+𝑖𝑧𝑝, (16)
ˆ𝑦=𝜎(𝑧𝑟+𝑧𝑝). (17)
RFM can be applied to a variety of tasks, such as classification
and regression. Taking the binary classification tasks (e.g., click-
through rate prediction) for example, we use the widely-used binary
cross-entropy loss with a regularization term to train our model:
L(𝚯)=−1
𝑁𝑁∑︁
𝑗=1
𝑦𝑗log(ˆ𝑦𝑗)+(1−𝑦𝑗)log(1−ˆ𝑦𝑗)
+𝜆||𝚯||2
2,(18)
where𝑦𝑗and ˆ𝑦𝑗are the ground-truth and predicted label of the
𝑗-th training sample respectively, 𝚯is the set of model parameters,
and𝜆is the𝐿2-norm penalty.
 
2915Rotative Factorization Machines KDD ’24, August 25–29, 2024, Barcelona, Spain
3.3 Discussion
After introducing the model details, we summarize the advantage
of RFM and discuss the difference with existing work.
Model Merits. Our approach has the following major merits:
•Model capacity : RFM can effectively model all three types of
feature interaction patterns (i.e., field-aware, instance-aware and
relation-aware ) introduced in the Figure 1, and meanwhile can learn
the unbounded-order interactions (Proof in Eq. (4)).
•Model generality : RFM can serve as a general framework for
feature interaction learning, which can be instantiated into other
existing models, including both the field-aware andinstance-aware
feature interaction models (Proof in Appendix A.3). Specially, inner
product-based feature interactions (e.g., FM [23]) are special cases
of our proposed rotation-based interactions (Proof in Appendix A.2,
experiments in Section 4.3.5).
•Theoretical guarantee : It can be proved that in high-dimensional
spaces, RFM can effectively learn the given feature relationships
with infinitesimal loss. In other words, in theory, RFM has the
potential to accurately capture any complex feature interaction
relationships. Further, we theoretically demonstrate that the gradi-
ent growth of RFM is at most linear with the orders, avoiding the
exponential explosion issue in prior work [ 4,7,31] (Experiments
in Section 4.3.4, more detailed proofs are provided in this link).
Difference with Existing Work. In the literature, AFN [ 7], Euler-
Net [ 31] and ARMNet [ 4] have also proposed to model the adaptive-
order interactions, but the order of each feature is independently
learned, which lacks the flexibility to capture the feature depen-
dencies in varying contexts. Although EulerNet has proposed to
enhance the representations in the complex vector space, it still
suffers from the exponential explosion issue when dealing with a
large order (See Section 4.3.4), due to the exponential growth in the
modulus of the complex features. In contrast, RFM is more flexible,
robust in accurately learning the complicated feature interactions
with arbitrarily large order involving massive feature fields. The
comparison of these approaches is presented in Table 1.
Complexity Analysis. We compare the time complexity of the
feature interaction components of different methods in Table 1. For
ease of analysis, we assume that the hidden size of different models
is set to the same number. Specifically, 𝑚is the field number, ℎis
the head number, 𝑑and𝑑′denote the embedding and attention
dimension,𝐾is the hidden neuron number. For multi-head rotation,
we use𝑑ℎ=𝑑′/ℎ. In each layer, calculating attention scores for
single head takesO(𝑚𝑑′𝑑ℎ+𝑚2𝑑ℎ)time. As we have ℎheads, it
takesO(𝑚𝑑′2+𝑚2𝑑′)time altogether. Note that 𝐾is much larger
than𝑚or𝑑[4,7,31], leading to a very high complexity of AFN [ 7]
and ARM-Net [ 4]. In comparison, 𝑑′is not very large, and it is often
set to 4𝑑in practice. In general, the complexity of the RFM is of the
same order as the Transformer, and is comparable to many efficient
feature interaction models such as EulerNet [ 31] and DCNV2 [ 37]
(See in Table 3 for analysis).
4 EXPERIMENT
We conduct extensive experiments to show the effectiveness of
RFM, and analyze the effect of each learning component in it.Table 1: Comparison of different feature interaction methods. “F”,
“I”, “R” denotes the capacity to learn the field-aware, instance-aware
and relation-aware feature interaction patterns, respectively.
Metho
ds A
daptive Unbounded Interaction Type Gradient Comple
xity
FM %
% F Exponential O
(𝑚𝑑)
AFN "
% F Exponential O
(𝑚𝑑𝐾)
ARM-Net "
% F, I Exponential O
(𝑚𝑑′𝐾)
EulerNet "
% F Exponential 𝑂(𝑚2𝑑2)
RFM "
" F, I, R Linear O
(𝑚𝑑′2+𝑚2𝑑′)
4.1 EXPERIMENTAL SETTING
We introduce the experimental settings, including the datasets,
baseline approaches, and the details of hyper-parameters.
4.1.1 Datasets. We evaluate RFM with five real-world classifica-
tion datasets on representative tasks, including app recommenda-
tion (Frappe1), movie recommendation (ML-1M2, ML-Tag3), click-
through prediction (Criteo4, Avazu5).
(1)Criteo: it is a prominent benchmark in the domain of CTR
prediction; (2) Avazu: it was utilized in the Avazu CTR prediction
challenge; (3) ML-1M: it is widely recognized as a prominent choice
in the realm of recommendation systems research; (4) ML-Tag: it
encompasses movie tagging data recorded by users across different
time spans; (5) Frappe: it serves as a practical application recom-
mendation dataset, featuring a context-aware log of app usage. To
maintain consistency with prior work, we follow the AFN [ 7] to
process the ML-Tag and Frappe dataset, where the split ratio for
the train/val/test is 7:2:1; and follow the EulerNet [ 31] to process
the Criteo, Avazu and ML-1M dataset, where the split ratio is 8:1:1.
The statistics of the datasets are shown in Table 2.
Table 2: Statistics of all datasets.
Datasets Criteo Avazu ML-1M ML-Tag Frappe
# Field 39 23 7 3 10
# Feature 1.3 M 1.5 M 13.2 K 90.4 K 5.4 K
# Instance 45 M 40 M 739 K 2 M 288 K
4.1.2 Baselines. We compare RFM with the thirteen state-of-the-
art models, including:
First-Order :
•LR [24] utilizes the original field features as input for prediction,
merely combining these features using corresponding weights.
Second-Order :
•FwFM [ 19] takes into account the semantic significance among
distinct feature fields and introduces a scalar weight to eliminate
insignificant feature interactions.
•FmFM [ 28] enhances FwFM by substituting the single scalar
field weight with a matrix, and it computes the kernel product on
the feature embeddings to capture significant feature dependencies.
High-Order :
1https://www.baltrunas.info/research-menu/frappe
2https://grouplens.org/datasets/movielens/
3https://grouplens.org/datasets/movielens/
4https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/
5https://www.kaggle.com/c/avazu-ctr-prediction
 
2916KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
•NFM [ 10] NFM aggregates the result of the element-wise multi-
plication of input feature vectors, which is then processed through
fully connected layers.
•CIN [ 15] generates high-order cross features through the com-
putation of outer products of feature vectors across various orders.
•CrossNet [ 37] models feature interactions explicitly through
the calculation of the kernel product of input feature vectors.
•PNN [ 21] capture feature interactions by combining inner or
outer products of input feature vectors in a pairwise manner.
Ensemble :
•AutoInt [ 25] utilizes Multi-head Self-Attention to autonomously
construct high-order characteristics. It stands as the pioneering en-
deavor to utilize Transformers for acquiring high-order feature
interplays.
•DeepFM [ 9] integrates classical factorization machines with a
multilayer perceptron (MLP) to improve the modeling of high-order
feature interactions.
•xDeepFM [15] integrates the CIN model with an MLP.
•DCNV2 [37] integrates the CrossNet model with an MLP.
Adaptive-Order :
•AFN [ 7] transforms features into a logarithmic space to flexibly
grasp arbitrary-order feature interactions. The AFN+ enhancement
involves the utilization of an MLP to enhance the underlying model.
•ARM-Net [ 4] introduces a gated attention mechanism that
adapts to instances to dynamically learn the orders of feature inter-
actions. On the other hand, ARM-Net+ enhances the underlying
model by incorporating an MLP.
•EulerNet [ 31] employs Euler’s formula to capture arbitrary-
order feature interactions in the complex vector space, thus over-
coming the non-negativity constraints present in the AFN.
These models compared in our experiments encompass various
forms of feature interaction techniques. Among them, LR [ 24] is the
most straightforward approach, which utilizes feature weights for
direct prediction. Besides, FmFM [ 28] and FwFM [ 19] are relatively
simple models that only capture second-order feature interactions.
NFM [ 10], CIN [ 15], CrossNet [ 37], and PNN [ 22] have the ca-
pacity to model higher-order feature interactions. AutoInt+ [ 25],
DeepFM [ 9], xDeepFM [ 15], and DCNV2 [ 37] are ensemble methods
that incorporate an MLP to enhance high-order feature interactions.
AFN+ [ 7], ARM-Net+ [ 4], and EulerNet [ 31] have the capacity to
learn adaptive-order interactions.
4.1.3 Evaluation Metrics. We use two popular metrics to evaluate
the model performance: AUC and LogLoss.
•AUC [ 16] stands for the Area Under the Receiver Operating
Characteristic Curve, which measures the likelihood that a CTR
predictor will correctly classify a randomly selected positive item
as having a higher score than a randomly selected negative item.
•LogLoss [ 3] is a measure of the deviation between the predicted
score and the ground-truth label for each sample, which is also
known as binary cross entropy loss.
4.1.4 Implementation Details. For each method, the grid search
is applied to find the optimal settings. We reuse baseline models
and implement RFM based on RecBole [ 44], an open-source library.
Extensive grid search is applied to find the optimal settings. We
follow the same experimental settings as EulerNet [ 31], by setting
the size feature embedding to 16, and batch size to 1024. We setthe learning rate from 1e-1 to 1e-4 on a log scale and then narrow
down to 5e-4 on a linear scale. The regularization parameter 𝜆is in
{1e-3, 1e-5, 1e-7}. The optimizer is Adam [ 12]. For RFM, the rotation
layer number is in {1, 2, 3}, the number of attention heads is in {1, 2,
4, 8}, and attention dimension is in {16, 32, 48, 64, 80}. The setting
of amplification network is in {48,128,256×256}. The dimension
ofGroupNorm (i.e.,𝑑𝑘/𝑙) is in {2, 4, 8, 16}. More tuning details and
our source code is available at code repository.
4.2 Overall Performance
To show the effectiveness of RFM in learning feature interactions,
we conduct experiments on five public datasets. The overall perfor-
mance is shown in Table 3. We have the following observations:
(1) The low-order models (i.e., LR [24], FwFM [ 19] and FmFM [ 28])
perform worse than the high-order models (i.e., NFM [ 10], CIN [ 15],
CrossNet [37] and PNN [21]), due to limited learning capacity.
(2) The ensemble models (i.e., AutoInt+ [ 25], xDeepFM [ 15],
DCNV2 [ 37] and DeepFM [ 9]) achieve competitive performance
across all datasets, showing the effectiveness of integrating MLPs
for learning enhanced feature interactions.
(3) As for the adaptive-order models, ARM-Net+ [ 4] outperforms
AFN+ [ 7] on the ML-1M, ML-Tag and Frappe datasets, demonstrat-
ing the effectiveness of instance-aware feature interaction learning.
Additionally, EulerNet [ 31] performs very well across all datasets,
indicating that the complex vector space is more suitable for learn-
ing adaptive-order interactions.
(4) RFM consistently outperforms all compared baselines, show-
ing the effectiveness of our proposed self-attentive rotation function
for learning relation-aware interactions.
For efficiency, we observe that the latency of first-order and
second-order models is relatively small due to their simple ar-
chitectures. The high-order and ensemble models are more time-
consuming because they have more complicated architectures. Com-
pared to EulerNet [ 31], AFN+ [ 7] and ARM-Net+ [ 4] have to in-
corporate many more parameters to compensate for the limited
representation capacity. Note that RFM is sufficiently efficient and
is comparable to many efficient approaches (e.g., DCNV2 [37]).
4.3 Further Study
4.3.1 Ablation Study. We analyze how each component influences
the performance of RFM in Table 4. We propose four variants as
follows: (1) w/oAttRo : removing the self-attentive rotation layer,
(2)w/oAmpNet : removing the modulus amplification network, (3)
w/oRes: removing the residual in the self-attentive rotation layer, (4)
w/oCoo Trans : removing the coordinate transformation procedure
(See Section 3.2) that directly feeds the angular representations to an
MLP. We can see that all these variants underperform the complete
RFM, showing that all of our proposed approaches are useful to
improve the performance. Specially, the model performance of
variant (1) shows a significant decrease, indicating that the self-
attentive rotation layer is the core of RFM for learning effective
feature interactions.
Besides, we investigate the effects of our proposed self-attentive
rotation function in Table 4. In variant (5), we remove the weight
vector (i.e., 𝒘in Eq. (5)) of rotation-based attention algorithm. Vari-
ant (6) replaces the rotation-based attention with the widely used
 
2917Rotative Factorization Machines KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Performance comparisons. Note that a higher AUC or a lower Logloss at the 0.001 level is regarded as significant, as stated in [ 6,9,25,31].
“*” denotes that statistical significance for 𝑝<0.01compared to the best baseline. “LL” denotes the LogLoss.
T
ype ModelCrite
o A
vazu ML-1M ML-
Tag Frapp
e Effciency
AUC
LL AUC
LL AUC
LL AUC
LL AUC
LL Params
Latency
First-Or
der LR 0.7900
0.4598 0.7663
0.3879 0.8712
0.3506 0.9303
0.3455 0.9379
0.2858 5.39
K 0.76 ms
Se
cond-OrderFwFM 0.8104
0.4414 0.7817
0.3813 0.8934
0.3201 0.9415
0.2761 0.9764
0.1791 91.66
K 1.02 ms
FmFM 0.8112
0.4408 0.7794
0.3819 0.8942
0.3191 0.9595
0.2255 0.9783
0.1675 93.21
K 1.21 ms
High-Or
derNFM 0.8066
0.4456 0.7832
0.3784 0.8931
0.3245 0.9578
0.2353 0.9779
0.1722 216.14
K 2.14 ms
CIN 0.8109
0.4424 0.7852
0.3771 0.8913
0.3255 0.9624
0.2125 0.9816
0.1669 362.27
K 3.79 ms
CrossNet 0.8123
0.4398 0.7874
0.3767 0.8983
0.3156 0.9647
0.2159 0.9817
0.1611 272.31
K 1.78 ms
PNN 0.8120
0.4399 0.7841
0.3773 0.8953
0.3233 0.9635
0.2197 0.9813
0.1567 113.23
K 1.58 ms
EnsembleA
utoInt+ 0.8126
0.4396 0.7841
0.3778 0.8981
0.3195 0.9642
0.2207 0.9810
0.1647 256.13
K 3.27 ms
DeepFM 0.8123
0.4399 0.7856
0.3768 0.8973
0.3166 0.9618
0.2264 0.9812
0.1689 252.17
K 1.61 ms
xDeepFM 0.8124
0.4406 0.7874
0.3761 0.8969
0.3187 0.9625
0.2121 0.9819
0.1580 375.22
K 5.70 ms
DCNV2 0.8129
0.4392 0.7876
0.3757 0.8989
0.3147 0.9649
0.2084 0.9822
0.1531 302.99
K 2.03 ms
A
daptive-OrderAFN+ 0.8125
0.4395 0.7877
0.3756 0.8931
0.3230 0.9607
0.2285 0.9813
0.1697 1976.36
K 3.39 ms
ARM-Net+ 0.8125
0.4396 0.7877
0.3757 0.8969
0.3141 0.9650
0.2096 0.9818
0.1517 1648.16
K 5.62 ms
EulerNet 0.8139 0.4387 0.7879 0.3755 0.9010 0.3098 0.9656 0.2134 0.9832 0.1581 170.76
K 1.88 ms
RFM 0.8147∗0.4374∗0.7890∗0.3749∗0.9026∗0.3090∗0.9667∗0.2049∗0.9843∗0.1506 348.17
K 2.27 ms
Table 4: Ablation study of the proposed RFM.
V
ariantCrite
o ML-
Tag Frapp
e
AUC
LogLoss AUC
LogLoss AUC
LogLoss
(0):
RFM 0.8147
0.4374 0.9667
0.2049 0.9843
0.1506
(1):w/oAttRo 0.8138
0.4381 0.9552
0.2454 0.9763
0.1768
(2):w/oAmpNet 0.8142
0.4381 0.9629
0.2178 0.9804
0.1491
(3):w/oRes 0.8141
0.4383 0.9635
0.2164 0.9806
0.1620
(4):w/oCoo Trans 0.8139
0.4384 0.9637
0.2163 0.9816
0.1611
(5):w/oAttWeight 0.8139
0.4382 0.9656
0.2073 0.9816
0.1533
(6): (1) + wDotAtt 0.8134
0.4381 0.9607
0.2330 0.9813
0.1561
(7):w/oGN 0.8131
0.4393 0.9626
0.2188 0.9789
0.1678
(8): (7) + wLN 0.8141
0.4383 0.9646
0.2137 0.9820
0.1491
(9): (7) + wBN 0.8140
0.4385 0.9653
0.2089 0.9833
0.1475
scaled dot-product attention [ 34]. The performance of both vari-
ants shows a notable decrease. This indicates that our proposed
rotation-based attention mechanism is more effective for the rela-
tion modeling of the angular representations in the complex plane.
In variants (7), (8), and (9), we explore the effects of different nor-
malization methods. The results show that the GroupNorm is more
suitable for learning the feature-wise representations.
4.3.2 Visualizing the Interaction Orders. RFM is capable of adap-
tively learning the interaction orders from different interaction
contexts. Figure 3 visualizes the learned orders of different meth-
ods. We can observe that the orders in field-aware method (i.e.,
EulerNet [ 31]) are the same for all features within each field (i.e.,
the columns in the figure). The instance-aware method (i.e., ARM-
Net [ 4]) can identity the importance of some features (e.g., 2-nd
column), but cannot capture the dependencies between different
fields. In contrast, RFM can learn the varied feature interactions
from different contexts. The diversified orders learned from the
varying context enable it to capture more effective relationships.
4.3.3 Visualizing the Representations. To have an intuitive under-
standing of our approach, we visualize the representations with a
Feature orderInstanceField-Aware (EulerNet)
Feature orderInstanceInstance-Aware (ARM-Net)
Feature orderInstanceRelation-Aware (RFM)
0.5 1.0 −0.05 0.00 0.25 0.50 0.75Figure 3: Visualizing the interaction orders.
simple case (the embedding dimension 𝑑=1) on the MovieLens-1M
dataset. As shown in Figure 4, the left figure visualizes the query
angles (i.e., 𝜽𝑄
𝑗in Eq. (5)) of the gender features and the key an-
gles (i.e., 𝜽𝐾
𝑗in Eq. (5)) of others, and the right figure illustrates
the conditional mutual information scores on the gender features,
representing the strength of each feature field on the ground-truth
labels given the gender features. We can observe that the fields
(user_id, zip_code anditem_id ) have a strong effect on the results,
and they are closely aligned with the gender features. For the fields
with less importance (age, occupation andrelease_year ), they have
no intersecting features with gender and the corresponding rotation
angles are relatively large. These results indicate that the rotation
angles from keys to queries can reflect the importance of feature
relationships, which enables RFM to capture the effective feature
dependencies for learning varied feature interactions.
4.3.4 Arbitrary-Order Learning Analysis. We investigate the arbitrary-
order learning capacity of different approaches. Figure 5(a) shows
the trajectory of the average feature order (i.e., 𝛼𝑗in Eq. (1)) during
training. We can see that RFM converges to a relatively large order,
while other models tend to approach a zero order. This demonstrates
RFM’s ability to learn more effective interactions.
Then we probe the learning effectiveness with respect to the
number of feature fields (i.e., 𝑚in Eq. (1)). As shown in Figure 5(b),
 
2918KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
Conditional Mutual Information
Gender
Figure 4: Visualizing the representations.
the average orders learned in EulerNet, AFN and ARM-Net de-
crease when adding the feature fields, since their interaction terms
exponentially grow with the increasing number of feature fields
(i.e.,𝑚in Eq. (1)). Figure 5(c) shows the gradients with respect to
the interaction order (i.e.,Í𝑚
𝑗=1𝛼𝑗in Eq. (1)). We observe that the
gradient in EulerNet, AFN and ARM-Net exponentially grows with
the increasing order, leading to the gradient explosion issue when
the order reaches a large value. In contrast, the gradient in RFM
remains relatively stable, and it is more robust to a large number of
feature fields or large interaction orders. These results are consis-
tent with our theoretical analysis, demonstrating the superiority of
RFM for learning arbitrary-order feature interactions.
0 5000 10000 15000 20000
training step0.00.20.40.60.81.0 Avg. OrderRFM
EulerNet
ARM-Net
AFN
(a) The learning trajec-
tory of Avg. feature order
2 4 6 8 10
Num_field0.10.20.30.40.50.6 Avg. OrderRFM
EulerNet
ARM-Net
AFN(b) The Avg. feature order
w.r.t feature field number
0 50 100 150 200 250 300
Order02K4K6K8K10K GradientRFM
EulerNet
ARM-Net
AFN(c) The gradient w.r.t the
interaction order
Figure 5: Interaction order learning analysis on the Frappe dataset.
4.3.5 Degradation to High-Order Factorization Machines. As dis-
cussed in Section 3.3, RFM can be degenerated to the traditional
inner-product-based methods ( e.g.,FM [ 23]). To study the effec-
tiveness of RFM in learning high-order feature interactions, we
create synthetic datasets with increasing difficulty as 𝑓𝑚(𝑬)=
𝒆1⊙𝒆2⊙···⊙ 𝒆𝑚. We compare the prediction result learned in
RFM with a complex MLP and self-attention mechanism, and utilize
fitting deviation to evaluate the difference between the prediction
results of the models and the ground-truth high-order feature in-
teractions (i.e., 𝑓𝑚). As shown in Figure 6, we can observe that
the fitting deviation continuously decreases as the dimension 𝑑
increases. Whereas the task difficulty increases (the order 𝑚in-
creases), the fitting deviation increases. These results are consistent
with the theoretical analysis in Appeddix A.2. On the other hand,
the deviation of RFM is very small, showing the approximately loss-
less fitting capability of RFM in learning high-order interactions.
4.3.6 Effect of Modulus Amplification Network. To study the ef-
fectiveness of the proposed modulus amplification network (See
Section 3.2), we visualize the representations before and after mod-
ulus amplification in the complex plane. The results on the Frappe,
0 50 100 150 200 250
Dimension10−1100DeviationRFM
Complex MLP
Self-attention(a)𝑚=5
0 50 100 150 200 250
Dimension100101
DeviationRFM
Complex MLP
Self-attention (b)𝑚=10
0 50 100 150 200 250
Dimension100101
DeviationRFM
Complex MLP
Self-attention (c)𝑚=15
Figure 6: The fitting deviation curves of different learning models.
ML-Tag, Criteo and Avazu datasets are shown in Figure 7. We can
observe that, before the modulus amplification procedure, the fea-
ture representations are distributed on a unit circle with a fixed
modulus of 1. Specifically, the angular representations learned in
RFM vary from[−𝜋,𝜋]on the ML-Tag, Criteo and Avazu datasets.
Whereas on the Frappe datasets, due to its smaller scale, the range
is narrowed to[−𝜋/10,𝜋/10]. After amplification, the features are
distributed at various areas in the complex plane, and they have
different modulus. Specially, we can also see that most transformed
representations have the same real part or imaginary part. Such
distributions make the varies of angle have a remarkable influence
on the predicted result, which enables RFM to capture the useful
feature relationships and improves the model’s capabilities.
Frappe_before
 Frappe_after
 ML_before
 ML_after
Criteo_before
 Criteo_after
 Avazu_before
 Avazu_after
Figure 7: Visualization of the feature representations before and
after the modulus amplification.
4.3.7 Hyper-Parameter Study. We study how the hyper-parameters
impact the performance of RFM, including:
•Attention Dimensions 𝑑′.We investigate the performance with
respect to the attention dimension 𝑑′in the self-attentive rotation
layer. As shown in Figure 8, on the Criteo and Avazu datasets, we
can see that the performance increases as the attention dimension
increases from 16 to 32. Whereas, on the Frappe dataset, RFM
achieves the best performance as the attention dimension increases
to 48. Continuously increasing the attention dimension does not
yield a sustained improvement in model performance. The reason is
that the model overfits when too many parameters are incorporated.
•Attention Heads ℎ.As mentioned in Section 3.1.2, the attention
heads number ℎcontrols the number of feature interaction terms.
As shown in Figure 8, we can see that the performance increases
as the attention head number increases from 2 to 4 on the Criteo
and Avazu datasets, showing the effectiveness of incorporating
more feature interactions. The results are different on the Frappe
dataset; the model performance varies significantly across different
attention head numbers. The reason is that this data set is small,
 
2919Rotative Factorization Machines KDD ’24, August 25–29, 2024, Barcelona, Spain
introducing too many interaction terms may introduce irrelevant
noise that hurts the model performance.
•Layer Number 𝑁.RFM is designed by stacking 𝑁self-attentive
rotation layers. To analyze the influence of 𝑁, we vary𝑁in the
range of 1 to 5 to report the results in Figure 8. We can observe
that the performance of RFM increases with the attention layer
number at the beginning. However, model performance degrades
when the attention layer number is set greater than 2 on the Criteo
and Avazu dataset, whereas RFM achieves the best performance
with a single layer. In practice, the layer number of RFM is usually
set to 1 or 2, thereby ensuring the efficiency of our approach.
16 32 48 64 80
Attention Dimension787078757880788578907895 AUC
AUC
LogLoss
374537473750375237553757376037623765
LogLoss
The attention dimension
2 4 6 8 10
Attention Head78807882788478867888789078927894 AUC
AUC
LogLoss
37463748375037523754375637583760
LogLoss
The attention head
1 2 3 4 5
The Number of Layers786578707875788078857890 AUC
AUC
LogLoss
374537503755376037653770
LogLoss
The Number of Layers
16 32 48 64 80
Attention Dimension981098159820982598309835984098459850 AUC
AUC
LogLoss
14601470148014901500151015201530
LogLoss
The attention dimension
2 4 6 8 10
Attention Head981098159820982598309835984098459850 AUC
AUC
LogLoss
146014801500152015401560158016001620
LogLoss
The attention head
1 2 3 4 5
The Number of Layers98009810982098309840 AUC
AUC
LogLoss
1500155016001650170017501800
LogLoss
The Number of Layers
Figure 8: Parameter analysis (Scale: .0001) on the Avazu (the first
row) and Frappe (the second row) datasets.
5 RELATED WORK
5.1 Feature Interaction Learning.
Feature interaction learning (FIL) is a fundamental problem in var-
ious machine learning tasks [ 9,18,37,38,45,46], leading to the
emergence of several interaction models [ 5,11,14,17,23,41]. The
early work mainly conducts enumeration to learn the feature in-
teractions. The basic idea of these approaches is to set a maximal
order and then enumerate all the feature interaction terms within
the designed order. For example, FM [ 23] is the most basic model,
using feature embedding vectors to capture pairwise feature inter-
actions; FwFM [ 19] and FmFM [ 28] propose to use the field-specific
weights to improve the expressive capacity of FMs. Though effec-
tive to some extent, they can only model the second-order feature
interactions that has a limited model capabilities. To effectively
capture the higher-order feature interactions, HOFM [ 2] introduces
a dynamic programming algorithm for higher-order interactions;
xDeepFM [ 15] and DCNV2 [ 37] propose intricate architectures to
iteratively enumerate the high-order interactions. They have signif-
icantly improved performance across various applications. Despite
the progress, their reliance on empirically designed orders may
hinder accurate learning in real-world contexts.
Recently, a number of studies [ 4,7,31] proposes to automatically
learn the orders from data. Among them, AFN [ 7] and ARM-Net [ 4]
uses logarithmic transformation to cast the interaction orders into
learnable linear weights, which provides a way to adaptively learn
the interaction orders. Besides, EulerNet [ 31] proposes to use Eu-
ler’s formula to learn the feature interaction orders in a complex
vector space. However, these methods cannot capture the feature
dependencies in varying contexts, which diminishes the model’s
capacity. Further, they suffer from the exponential explosion issue,making them unsuitable for scenarios with numerous features or
high orders. Different from them, we use the attentive rotations to
model complicated interactions, which can adaptively capture the
feature dependencies and surpass the scale limits of the interaction
order in existing work.
5.2 Representation Learning in the Complex
Vector Space.
In the literature, a number of approaches [ 13,20,26,27,29–31] have
been proposed to learn the data relations in the complex vector
space for enhancing the representations. Typically, the complex
vector space possesses the inherent advantages in modeling the
vector rotations and exponential modeling, and it has drawn great
attention in a wide range of research fields [20, 26, 27, 29–31].
In the area of computer vision (CV), WaveMLP [ 30] represents
each image patch as a wave to capture the dynamic vision seman-
tics. In the area of natural language processing (NLP), RotatE [ 29]
defines each relation of a knowledge graph as a rotation from the
source entity to the target entity. RoPE [ 20,26,27] and XPOS [ 27]
use a two-dimensional pairwise rotation method to improve the po-
sitional embedding of Transformers. These approaches have widely
used in the latest large language models (LLMs), such as LLaMa [ 33]
and PaLM [ 8]. Furthermore, AnglE [ 13] proposes a method using
complex polar angles to learn the similarity relationship of differ-
ent vectors. In the area of recommender systems, EulerNet [ 31]
proposes utilizing Euler’s formula to adaptively learn the arbitrary-
order feature interactions. Besides, EulerFormer [ 32] proposes a
new theoretical framework to formulate the Transformer architec-
ture which provides a new perspective for modeling the positional
embedding and attention mechanism in the complex vector space.
These approaches have a great potentiality to enhance model capa-
bilities in a variety of machine learning tasks.
6 CONCLUSION
In this paper, we proposed the Rotative Factorization Machines
(RFM) for effectively modeling feature interactions in various pre-
diction tasks. RFM represents each feature as a polar angle in the
complex plane and converts the interactions into the complex rota-
tions, avoiding the exponential explosion of the interaction terms. In
RFM, the rotation coefficients are modeled through a rotation-based
attention mechanism, which can adaptively learn the interaction
orders from different interaction contexts. Moreover, we propose a
modulus amplification network to learn the modulus of the com-
plex features for further enhancing the expressive capacity. As the
core component, we proposed a novel self-attentive rotation func-
tion to model complicated feature interactions, providing a way to
adaptively learn the unbounded interaction orders adaptively from
varying contexts. As future work, we consider generalizing RFM to
handle other data types, e.g.,sequences or graphs, and will extend
the current approach to deal with multiple domains or tasks.
ACKNOWLEDGMENTS
This work was partially supported by National Natural Science
Foundation of China under Grant No. 62222215, Beijing Natural
Science Foundation under Grant No. L233008 and 4222027. Xin
Zhao is the corresponding author.
 
2920KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
REFERENCES
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 (2016).
[2]Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016.
Higher-order factorization machines. Advances in Neural Information Processing
Systems 29 (2016).
[3]Andreas Buja, Werner Stuetzle, and Yi Shen. 2005. Loss functions for binary class
probability estimation and classification: Structure and applications. Working
draft, November 3 (2005), 13.
[4]Shaofeng Cai, Kaiping Zheng, Gang Chen, HV Jagadish, Beng Chin Ooi, and
Meihui Zhang. 2021. Arm-net: Adaptive relation modeling network for structured
data. In Proceedings of the 2021 International Conference on Management of Data.
207–220.
[5]Wenqiang Chen, Lizhang Zhan, Yuanlong Ci, Minghua Yang, Chen Lin, and
Dugang Liu. 2019. FLEN: leveraging field for scalable CTR prediction. arXiv
preprint arXiv:1911.04690 (2019).
[6]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[7]Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization
network: Learning adaptive-order feature interactions. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 34. 3609–3616.
[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, et al .2023. Palm: Scaling language modeling with pathways.
Journal of Machine Learning Research 24, 240 (2023), 1–113.
[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[10] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse
predictive analytics. In Proceedings of the 40th International ACM SIGIR conference
on Research and Development in Information Retrieval. 355–364.
[11] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-
ture importance and bilinear feature interaction for click-through rate prediction.
InProceedings of the 13th ACM Conference on Recommender Systems. 169–177.
[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[13] Xianming Li and Jing Li. 2023. Angle-optimized text embeddings. arXiv preprint
arXiv:2309.12871 (2023).
[14] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn:
Modeling feature interactions via graph neural networks for ctr prediction. In Pro-
ceedings of the 28th ACM International Conference on Information and Knowledge
Management. 539–548.
[15] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 1754–1763.
[16] Jorge M Lobo, Alberto Jiménez-Valverde, and Raimundo Real. 2008. AUC: a
misleading measure of the performance of predictive distribution models. Global
ecology and Biogeography 17, 2 (2008), 145–151.
[17] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan.
2021. A dual input-aware factorization machine for CTR prediction. In Proceedings
of the Twenty-Ninth International Conference on International Joint Conferences
on Artificial Intelligence. 3139–3145.
[18] Yuanfei Luo, Hao Zhou, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and Qiang
Yang. 2020. Network on network for tabular data classification in real-world
applications. In Proceedings of the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval. 2317–2326.
[19] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,
and Quan Lu. 2018. Field-weighted factorization machines for click-through
rate prediction in display advertising. In Proceedings of the 2018 World Wide Web
Conference. 1349–1357.
[20] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn:
Efficient context window extension of large language models. arXiv preprint
arXiv:2309.00071 (2023).
[21] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In 2016 IEEE
16th International Conference on Data Mining (ICDM). IEEE, 1149–1154.
[22] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo,
Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response
prediction over multi-field categorical data. ACM Transactions on Information
Systems (TOIS) 37, 1 (2018), 1–35.
[23] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference
on data mining. IEEE, 995–1000.
[24] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: estimating the click-through rate for new ads. In Proceedings of the 16thinternational conference on World Wide Web. 521–530.
[25] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-
attentive neural networks. In Proceedings of the 28th ACM International Conference
on Information and Knowledge Management. 1161–1170.
[26] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.
2021. Roformer: Enhanced transformer with rotary position embedding. arXiv
preprint arXiv:2104.09864 (2021).
[27] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
transformer. arXiv preprint arXiv:2212.10554 (2022).
[28] Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. 2021. Fm2: Field-matrixed
factorization machines for recommender systems. In Proceedings of the Web
Conference 2021. 2828–2837.
[29] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowl-
edge graph embedding by relational rotation in complex space. arXiv preprint
arXiv:1902.10197 (2019).
[30] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe
Wang. 2022. An image patch is a wave: Phase-aware vision mlp. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10935–
10944.
[31] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023. Euler-
Net: Adaptive Feature Interaction Learning via Euler’s Formula for CTR Predic-
tion. In Proceedings of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 1376–1385.
[32] Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, and Ji-
Rong Wen. 2024. EulerFormer: Sequential User Behavior Modeling with Complex
Vector Attention. In SIGIR.
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[35] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang,
and Ning Gu. 2022. Enhancing CTR prediction with context-aware feature repre-
sentation learning. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 343–352.
[36] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and
Ning Gu. 2023. CL4CTR: A Contrastive Learning Framework for CTR Prediction.
InProceedings of the Sixteenth ACM International Conference on Web Search and
Data Mining. 805–813.
[37] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. DCN V2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the Web Conference 2021.
1785–1797.
[38] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. MaskNet: Introducing
feature-wise multiplication to CTR ranking models by instance-guided mask.
arXiv preprint arXiv:2102.07619 (2021).
[39] Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of the
European conference on computer vision (ECCV). 3–19.
[40] Bo Xiao and Izak Benbasat. 2007. E-commerce product recommendation agents:
Use, characteristics, and impact. MIS quarterly (2007), 137–209.
[41] Feng Yu, Zhaocheng Liu, Qiang Liu, Haoli Zhang, Shu Wu, and Liang Wang.
2020. Deep interaction machine: A simple but effective model for high-order
feature interactions. In Proceedings of the 29th ACM International Conference on
Information & Knowledge Management. 2285–2288.
[42] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021.
Deep Learning for Click-Through Rate Estimation. In Proceedings of the Thirtieth
International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /
Montreal, Canada, 19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org, 4695–4703.
https://doi.org/10.24963/IJCAI.2021/636
[43] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep
learning for click-through rate estimation. arXiv preprint arXiv:2104.10584 (2021).
[44] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu
Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al .2021. Recbole:
Towards a unified, comprehensive and efficient framework for recommendation
algorithms. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management. 4653–4664.
[45] Zihao Zhao, Zhiwei Fang, Yong Li, Changping Peng, Yongjun Bao, and Weipeng
Yan. 2020. Dimension relation modeling for click-through rate prediction. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management. 2333–2336.
[46] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
5941–5948.
 
2921Rotative Factorization Machines KDD ’24, August 25–29, 2024, Barcelona, Spain
A THEORETICAL ANALYSIS
A.1 Tensor-form attention calculation
Note that the element in 𝑗-th row and 𝑙-th column can be given as:
𝐴𝑗,𝑙=Sigmoid 
Reh
exp(𝑖𝑸)diag(𝒘)
exp(−𝑖𝑲)⊤i!
𝑗,𝑙
=Sigmoid 
Reh
𝒘⊤
exp(𝑖𝑸𝑗)⊙exp(−𝑖𝑲𝑙)⊤i!
=Sigmoid 
Reh
𝒘⊤cos(𝑸⊤
𝑗−𝑲⊤
𝑙)+𝑖𝒘⊤sin(𝑸⊤
𝑗−𝑲⊤
𝑙)i!
=Sigmoid
𝒘⊤cos(𝜽𝑄
𝑗−𝜽𝐾
𝑙)
=𝛼𝑗,𝑙.
A.2 Degenerate to Factorization Machines
Lemma A.1. Following the basic setting of machine learning, we
consider that the input data 𝒙are independently and identically dis-
tributed ( i.i.d) and{𝜽}𝑚
𝑗=1are measurable embeddings of them. If
embeddings{𝜽𝑗}𝑚
𝑗=1∈R𝑚×𝑑are regularized such that ||𝜽𝑗||2≤1,
then for any given order vector 𝜶∈ {0,1}𝑚and any input fea-
tures{𝜽𝑗}𝑚
𝑗=1∈R𝑚×𝑑, the rotation-based interaction pattern 𝚫𝐺=
H(𝑒𝑖𝛼1𝜽1⊙𝑒𝑖𝛼2𝜽2⊙···⊙𝑒𝑖𝛼𝑚𝜽𝑚)can be degenerated to the inner-
product based interaction 𝚫𝐹=𝒆𝛼1
1⊙𝒆𝛼2
2⊙··· 𝒆𝛼𝑚𝑚. The devia-
tion of E(¯𝑹)=E(|¯𝚫𝐺−¯𝚫𝐹|)can be bounded by O(𝑠/√
𝑑), where
H(𝒛)=Re(𝒛)+Im(𝒛), and𝑠=Í𝑚
𝑗=1𝛼𝑗is the interaction order.
Proof. Note that|H(𝒛)|≤|| Re(𝒛)|+| Im(𝒛)||andcos(𝛼𝑗𝜽𝑗)=
cos𝛼𝑗(𝜽𝑗)if𝛼𝑗∈{0,1}. Let 𝒆𝑗:=cos(𝜽𝑗)∈R𝑑, and we have:
|𝚫𝐺−𝚫𝐹|=|H(𝑒𝑖𝛼1𝜽1⊙𝑒𝑖𝛼2𝜽2⊙···⊙𝑒𝑖𝛼𝑚𝜽𝑚)−𝒆𝛼1
1⊙··· 𝒆𝛼𝑚𝑚|
=H 𝑚Ö
𝑗=1
cos(𝛼𝑗𝜽𝑗)+𝑖sin(𝛼𝑗𝜽𝑗)!
−𝑚Ö
𝑗=1cos𝛼𝑗(𝜽𝑗)
=H 𝑚Ö
𝑗=1
cos(𝛼𝑗𝜽𝑗)+𝑖sin(𝛼𝑗𝜽𝑗)!
−𝑚Ö
𝑗=1cos(𝛼𝑗𝜽𝑗)
=H 𝑚∑︁
𝑙=1∑︁
𝑝∈𝐶𝑙𝑚𝑚Ö
𝑡=1
𝑖𝑝𝑡cos1−𝑝𝑡(𝛼𝑡𝜽𝑡)sin𝑝𝑡(𝛼𝑡𝜽𝑡)!
+𝑚Ö
𝑗=1cos(𝛼𝑗𝜽𝑗)−𝑚Ö
𝑗=1cos(𝛼𝑗𝜽𝑗)
=H 𝑚∑︁
𝑙=1∑︁
𝑝∈𝐶𝑙𝑚𝑚Ö
𝑡=1
𝑖𝑝𝑡cos1−𝑝𝑡(𝛼𝑡𝜽𝑡)sin𝑝𝑡(𝛼𝑡𝜽𝑡)!
+1−1
≤ 𝑚∑︁
𝑙=1∑︁
𝑝∈𝐶𝑙𝑚𝑚Ö
𝑡=1
|cos1−𝑝𝑡(𝛼𝑡𝜽𝑡)|⊙| sin𝑝𝑡(𝛼𝑡𝜽𝑡)|!
+1−1
≤ 𝑚∑︁
𝑙=1∑︁
𝑝∈𝐶𝑙𝑚𝑚Ö
𝑡=1
1⊙|sin𝑝𝑡(𝛼𝑡𝜽𝑡)|!
+1−1
=𝑚Ö
𝑗=1
1+|sin(𝛼𝑗𝜽𝑗)|
−1Here𝐶𝑙𝑚is the set of indices denoting the combinations that select
𝑙elements from a set of size 𝑚,e.g.,𝐶2
3={[0,1,1],[1,0,1],[1,1,0]}.
We assume each dimension of the embedding vector is indepen-
dent, then the expectation of the embedding deviation is equal
to the one-element deviation. Futher, we have E(Í𝑑
𝑘=1|𝜃𝑘|2)=Í𝑑
𝑘=1E(|𝜃𝑘|2)=𝑑·E(|𝜃𝑘|2)≤1. Let𝑠=Í𝑚
𝑗=1𝛼𝑗is the interaction
order, then we have:
E(¯𝑅)≤E 𝑚Ö
𝑗=1
1+|sin(𝛼𝑗𝜃𝑗)|
−1!
≤E Ö
𝛼𝑗≠0
1+|sin(𝛼𝑗𝜃𝑗)|
−1!
≤E Ö
𝛼𝑗≠0
1+|𝜃𝑗|
−1!
≤E 
1+Í𝑚
𝑗=1𝛼𝑗|𝜃𝑗|
Í𝑚
𝑗=1𝛼𝑗Í𝑚
𝑗=1𝛼𝑗−1!
≤E 𝑚∑︁
𝑗=1𝛼𝑗|𝜃𝑗|!
=𝑚∑︁
𝑗=1𝛼𝑗E(|𝜃𝑗|)≤𝑚∑︁
𝑗=1𝛼𝑗√︃
E(|𝜃𝑗|2)≤𝑠/√
𝑑
□
A.3 Field-Aware and Instance-Aware Interaction
Learning
The proof is equivalent to proving the following two lemmas:
Lemma A.2. If embeddings{𝜽𝑗}𝑚
𝑗=1are L2-regularized such that
||𝜃𝑗||2≤1,∀𝑗∈{1,...,𝑚}, then for any given order 𝜶∈{0,1}𝑚,
RFM can model the interaction pattern 𝚫𝐹=𝒆𝛼1
1⊙𝒆𝛼2
2⊙··· 𝒆𝛼𝑚𝑚.
Proof. We add an auxiliary dimension to the input features,
˜𝜽𝑗=[𝜽𝑗,𝜖], where𝜖is a sufficiently small number. We construct
two types of matrices: 𝑵=O(𝑑+1)×(𝑑+1)is an all-zero matrix with
a shape of(𝑑+1)×(𝑑+1), and 𝑴is defined by the following:
𝑴="0··· 0
.........
0···𝜋
𝜖#
∈R(𝑑+1)×(𝑑+1).
In this way, we have 𝑴˜𝜽𝑗=[0,...,𝜋]⊤and𝑵˜𝜽𝑗=[0,...,0]⊤. Here,
we set all query matrices as 𝑾𝑄
𝑗=𝑵,∀𝑗={1,2,...,𝑚}. Given the
order vector 𝛼, the key matrices are set by the following rule:
𝑾𝐾
𝑗=𝑴, 𝛼𝑗=0
𝑵, 𝛼𝑗=1
In this way, the matrices of the queries are mapped to a zero space,
i.e.,𝜽𝑄
𝑗=𝑾𝑄
𝑗˜𝜽𝑗=𝑵˜𝜽=0. As for the keys, when 𝑗satisfies𝛼𝑗=0,
the transformed vector 𝜽𝐾
𝑗=𝑾𝐾
𝑗˜𝜽𝑗=𝑴˜𝜽𝑗=[0,...,𝜋]⊤; when𝑗
satisfies𝛼𝑗=1,𝜽𝐾
𝑗=𝑾𝐾
𝑗˜𝜽𝑗=𝑵˜𝜽𝑗=0. We set the weight vector
𝒘=𝑆·[0,...,1]⊤, and𝑆>0is a sufficiently large number. Consider
the attention score from 𝑗-th query, we have:
𝛼𝑅𝐹𝑀
𝑗,𝑙=RotAtt(𝑸𝑗,𝑲𝑙)=Sigmoid
𝒘⊤cos(𝜽𝑄
𝑗−𝜽𝐾
𝑙)
=Sigmoid(−𝑆)=0, 𝛼𝑙=0
Sigmoid(𝑆)=1, 𝛼𝑙=1
 
2922KDD ’24, August 25–29, 2024, Barcelona, Spain Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, & Ji-Rong Wen
Therefore, we have 𝜶𝑅𝐹𝑀
𝑗=𝜶. Furthermore, we define the value
matrix as follows:
𝑾𝑉
𝑗="1··· 0 0
............
0··· 1 0#
∈R𝑑×(𝑑+1).
Therefore 𝜽𝑉
𝑗=𝑾𝑉
𝑗˜𝜽𝑗=𝜽𝑗. According to Eq. 7, we have:
ˆ𝜽𝑗=𝑚∑︁
𝑙=1𝛼𝑅𝐹𝑀
𝑗,𝑙𝜽𝑉
𝑙=𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙.
In this scheme, all ˆ𝜽𝑗are the same, and we only consider a single
output of rotated angles. We remove the amplification network,
and set the weight 𝒖(See Eq. 16) as the identity matrix 𝑰. According
to Eq. 17, when omitting the activation function, the output of RFM
can be given as:
ˆ𝑦=𝒖⊤(cos(ˆ𝜽𝑙)+sin(ˆ𝜽𝑙))
=cos(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)+sin(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H
cos(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)+𝑖sin(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H
exp(𝑖𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H(𝑒𝑖𝛼1𝜽1⊙𝑒𝑖𝛼2𝜽2⊙···⊙𝑒𝑖𝛼𝑚𝜽𝑚).
Therefore, Lemma A.2 is proved by combining Lemma A.1. □
Lemma A.3. If embeddings{𝜽𝑗}𝑚
𝑗=1are L2-regularized such that
||𝜽𝑗||2≤1,∀𝑗∈{1,...,𝑚}, RFM can model the interaction pattern
𝚫𝐼=𝒆𝛼1
1⊙𝒆𝛼2
2⊙··· 𝒆𝛼𝑚𝑚, where𝛼𝑗=𝑓(𝒆𝑗)and𝑓:R𝑑→{0,1}is
an instance importance function.
Proof. Given the set of input feature embeddings {𝒆𝑗=𝜽𝑗}𝑚
𝑗=1,
we add an auxiliary dimension to the input features, ˜𝜽𝑗=[𝜽𝑗,𝜖·
𝑓(𝒆𝑗)], where𝜖is a sufficiently small number. We construct two
types of matrices: 𝑵=O(𝑑+1)×(𝑑+1)is an all-zeros matrix with the
shape of(𝑑+1)×(𝑑+1), and 𝑴is defined by the following:
𝑴="0··· 0
.........
0···𝜋
𝜖#
∈R(𝑑+1)×(𝑑+1).We have 𝑴˜𝜽𝑗=[0,...,𝜋·𝑓(𝒆𝑗)]⊤and𝑵˜𝜽𝑗=[0,...,0]⊤. Here
we set all query matrices as 𝑾𝑄
𝑗=𝑵,∀𝑗={1,2,...,𝑚}and key
matrices as 𝑾𝐾
𝑗=𝑴,∀𝑗={1,2,...,𝑚}. We set the weight vector
𝒘=[0,...,−𝑆]⊤, and𝑆>0is a sufficiently large number. In this
way, the matrices for the queries are mapped into a zero space, i.e.,
𝜽𝑄
𝑗=𝑾𝑄
𝑗˜𝜽𝑗=𝑵˜𝜽=0, and the keys are 𝜽𝐾
𝑗=𝑾𝐾
𝑗˜𝜽𝑗=𝑴˜𝜽𝑗=
[0,...,𝜋·𝑓(𝒆𝑗)]⊤. Since𝑓(𝒆𝑗)∈{ 0,1}, thus we have Sigmoid
−
𝑆·cos
𝜋·𝑓(𝑒𝑗)
=𝑓(𝒆𝑗). Consider the attention score from 𝑗-th
query, we have:
𝛼𝑅𝐹𝑀
𝑗,𝑙=Sigmoid
𝒘⊤cos(𝜽𝑄
𝑗−𝜽𝐾
𝑙)
=Sigmoid
−𝑆·cos
𝜋·𝑓(𝑒𝑙)
=𝑓(𝒆𝑙).
Further, we set the value matrices as the following:
𝑾𝑉
𝑗="1··· 0 0
............
0··· 1 0#
∈R𝑑×(𝑑+1).
Therefore 𝜽𝑉
𝑗=𝑾𝑉
𝑗˜𝜽𝑗=𝜽𝑗. According to Eq. 7, we have:
ˆ𝜽𝑗=𝑚∑︁
𝑙=1𝛼𝑅𝐹𝑀
𝑗,𝑙𝜽𝑉
𝑗=𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙.
In this scheme, all ˆ𝜽𝑗are the same; we only consider a single output
of rotated angles and set the weight 𝒖(See Eq. 16) as the identity
matrix 𝑰. According to Eq. 17, when the activation function is
omitted, the output of RFM can be given as:
ˆ𝑦=𝒖⊤(cos(ˆ𝜽𝑙)+sin(ˆ𝜽𝑙))
=cos(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)+sin(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H
cos(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)+𝑖sin(𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H
exp(𝑖𝑚∑︁
𝑙=1𝛼𝑙𝜽𝑙)
=H(𝑒𝑖𝛼1𝜽1⊙𝑒𝑖𝛼2𝜽2⊙···⊙𝑒𝑖𝛼𝑚𝜽𝑚)
=H(𝑒𝑖𝑓(𝒆1)𝜽1⊙𝑒𝑖𝑓(𝒆2)𝜽2⊙···⊙𝑒𝑖𝑓(𝒆𝑚)𝜽𝑚).
Since the construction of the order is independent of the input fea-
tures{𝒆𝑗=𝜽𝑗}𝑚
𝑗=1, lemma A.3 is proven by combining lemma A.1.
□
 
2923