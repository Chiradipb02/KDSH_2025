A Self-boosted Framework for Calibrated Ranking
Shunyu Zhang
Kuaishou Technology
Beijing, China
zhangshunyu@kuaishou.comHu Liu∗
Kuaishou Technology
Beijing, China
hooglecrystal@126.comWentian Bao
Columbia University
Beijing, China
wb2328@columbia.edu
Enyun Yu
Northeasten University
Beijing, China
yuenyun@126.comYang Song
Kuaishou Technology
Beijing, China
yangsong@kuaishou.com
Abstract
Scale-calibrated ranking systems are ubiquitous in real-world ap-
plications nowadays, which pursue accurate ranking quality and
calibrated probabilistic predictions simultaneously. For instance, in
the advertising ranking system, the predicted click-through rate
(CTR) is utilized for ranking and required to be calibrated for the
downstream cost-per-click ads bidding. Recently, multi-objective
based methods have been wildly adopted as a standard approach
for Calibrated Ranking, which incorporates the combination of two
loss functions: a pointwise loss that focuses on calibrated abso-
lute values and a ranking loss that emphasizes relative orderings.
However, when applied to industrial online applications, existing
multi-objective CR approaches still suffer from two crucial limita-
tions. First, previous methods need to aggregate the full candidate
list within a single mini-batch to compute the ranking loss. Such
aggregation strategy violates extensive data shuffling which has
long been proven beneficial for preventing overfitting, and thus de-
grades the training effectiveness. Second, existing multi-objective
methods apply the two inherently conflicting loss functions on
a single probabilistic prediction, which results in a sub-optimal
trade-off between calibration and ranking.
To tackle the two limitations, we propose a Self-Boosted frame-
work for Calibrated Ranking (SBCR). In SBCR, the predicted ranking
scores by the online deployed model are dumped into context fea-
tures. With these additional context features, each single item can
perceive the overall distribution of scores in the whole ranking list,
so that the ranking loss can be constructed without the need for
sample aggregation. As the deployed model is a few versions older
than the training model, the dumped predictions reveal what was
failed to learn and keep boosting the model to correct previously
mis-predicted items. Moreover, a calibration module is introduced
to decouple the point loss and ranking loss. The two losses are
applied before and after the calibration module separately, which
∗Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671570elegantly addresses the sub-optimal trade-off problem. We con-
duct comprehensive experiments on industrial scale datasets and
online A/B tests, demonstrating that SBCR can achieve advanced
performance on both calibration and ranking. Our method has been
deployed on the video search system of Kuaishou, and results in sig-
nificant performance improvements on CTR and the total amount
of time users spend on Kuaishou.
CCS Concepts
•Information systems →Learning to rank; Personalization .
Keywords
Calibrated Ranking; Learning-to-rank; Search Ranking System
ACM Reference Format:
Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, and Yang Song. 2024. A Self-
boosted Framework for Calibrated Ranking. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3671570
1 Introduction
As one of the most popular video-sharing apps in China, Kuaishou
strongly relies on its leading personalized ranking system, which
serves hundreds of millions of users with fingertip connection to
billions of attractive videos. Once the ranking system receives a
request from a user (aka. query in literature), it will predict a rank-
ing score for each of the retrieved candidate videos (aka. items,
documents ). These ranking scores are not only used in sorting the
candidate items to fit the user’s personalized interest, but also es-
sential for many downstream applications. For example, we use the
click-through rate (CTR) to guide ads bidding and the probability
of effectively watching to estimate the video quality. This suggests
that industrial ranking systems should emphasize two matters si-
multaneously: 1). the relative orders between scores, namely the
ranking quality evaluated by GAUC and NDCG [ 16], and 2). the
accurate absolute values of scores which should be calibrated to
some actual likelihood when mapped to probabilistic predictions.
To meet this practical demand in industrial ranking systems,
there have been emerging studies on a paradigm known as Cali-
brated Ranking (CR) [ 1,33]. The standard approach of CR usually
incorporates a multi-objective loss function: a pointwise loss that
focuses on calibrated absolute values and a ranking loss that em-
phasizes relative orderings. Sculley [ 27] combines regression and
6226
KDD ’24, August 25–29, 2024, Barcelona, Spain Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, & Yang Song
pairwise loss for CTR prediction with offline experiments. Further
studies [ 1,28] combine pointwise loss and listwise loss for CR in
real-world systems. Although encouraging progress has been made
on both calibration and ranking ability, existing multi-objective CR
approaches still suffer from two crucial limitations in industrial
online applications.
First, existing multi-objective CR approaches usually contradict
extensive data shuffling which has long been proven beneficial in
preventing the overfitting issue and is almost the default setting
in industrial sequential gradient descent (Fig 1). Specifically, one
component of the multi-objective loss, the ranking loss, is defined
to make comparisons between scores of candidate items retrieved
for the same request. This naturally requires the aggregation of the
whole candidate list in a single mini-batch, making extensive item-
level data shuffling inapplicable. As the result, the training process
suffers from many terrible issues that could otherwise be avoided
by extensive shuffling, including the non-IID data, and overfitting
caused by the aggregation of similar samples. We will demonstrate
the degradation of training effectiveness from insufficient shuffling
in our experiments.
Second, conventional multi-objective CR applies point loss and
ranking loss jointly on a single probabilistic prediction. However,
the two objectives are not necessarily compatible, or even inher-
ently conflicting, making the best trade-off sub-optimal for both
calibration and ranking. And this trade-off is also sensitively de-
pendent on the relative weights of the two objectives, leading to a
challenging hyper-parameter choosing issue. How to decouple the
two objects, namely optimizing one without sacrificing the other,
still remains an open question.
To address the first limitation, we proposed a self-boosted pair-
wise loss that enables extensive data shuffling, while achieving
high ranking quality like what conventional ranking loss does. Our
strategy is to dump the ranking scores predicted by the deployed
model on our online server into the context features of a specific
query. With these contexts and negligible additional cost (only a few
real numbers), each single candidate item can perceive the overall
distribution of ranking scores under the same query. So there is no
need to aggregate the whole candidate list in a single mini-batch for
score comparison. More importantly, as the deployed model is a few
versions older than the training model, the dumped scores actually
reveal what the model failed to learn and further direct the follow-
ing update to pay extra attention to previously mispredicted items.
We term this Self-Boosted. We further tackle the second limitation
by introducing a calibration module that decouples the ranking
and calibration losses. Ranking and calibration losses are applied
before and after the calibration module separately, which elegantly
addresses the sub-optimal multi-objective trade-off problem.
Finally, we propose the Self-Boosted framework for Calibrated
Ranking (SBCR). Our architecture includes two modules, 1). a rank-
ing module termed Self-Boosted Ranking (SBR) trained by a multi-
objective loss consisting of the pointwise and proposed self-boosted
pairwise losses, and 2). a following calibration module trained by a
calibration loss. Our main contributions are summarized as follows:
1The experiments are conducted on the pointwise production baseline with standard
Logloss. The training details are described in Sec.4.1
query1query4query2Extensive ShuffleQuery-level ShuffleNDCG@10LogLoss0.5612 0.56250.72690.7267
User Time Spend -0.25%
Figure 1: The performance comparison of different data
shuffling strategies. Evaluation metrics include: Logloss,
NDCG@10 (widely used in ranking), and the total amount of
time users spend on Kuaishou (the most important metric
in our online A/B test). Extensive item-level data shuffling
(upper) significantly outperforms the query-level data shuf-
fling (bottom) where the whole candidate item list retrieved
for a single request is aggregated in a single mini-batch. The-
oretical explanation will be discussed in Sec 3.2.2. This ex-
perimental result validates the advantage of extensive data
shuffling and motivates us to propose a novel ranking loss
that enables extensive shuffling.1
•We highlight the two limitations of conventional multi-objective
CR approaches in industrial online applications, namely,
the contradiction with extensive data shuffling and the sub-
optimal trade-off between calibration and ranking.
•We propose a novel SBCR framework that successfully ad-
dresses the two limitations. In SBCR, ranking quality is em-
phasized without the need for data aggregating, and the two
objectives are decoupled to avoid the conflict.
•We validate SBCR on the video search system of Kuaishou.
Extensive offline experiments show that our method can
achieve advanced performance on both calibration and rank-
ing. In online A/B tests, SBCR also outperforms the strong
production baseline and brings significant improvements on
CTR and the total amount of time users spend on Kuaishou.
SBCR has now been deployed on the video search system of
Kuaishou, serving the main traffic of hundreds of millions
of active users.
2 Related Work
We mainly focus on the related work concerning these aspects: CTR
Prediction, Learning-to-Rank (LTR), and Calibrated Ranking.
CTR Prediction aims to predict a user’s personalized click ten-
dency, which is crucial for nowadays information systems. In the
last decades, the field of CTR prediction has evolved from traditional
shallow models, e.g. Logistic Regression (LR), Gradient Boosting
Decision Tree (GBDT) [ 17], to deep neural models e.g. Wide &
Deep [ 7], DCN [ 30]. Most researches are dedicated to improving
model architectures: Wide & Deep [ 7] and DeepFM [ 13] combine
low-order and high-order features to improve model expressive-
ness, and DCN [ 30,31] replace FM of DeepFM with Cross Network.
DIN [ 35] and SIM [ 24] employ the attention mechanism to extract
user interest. Despite recent progress, the loss function is still not
well-explored and the dominant pointwise LogLoss [ 35] can’t well
satisfy the ranking quality highly desired in practice [19, 20, 27].
6227A Self-boosted Framework for Calibrated Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
Learning-To-Rank (LTR) generally learns a scoring function
to predict and sort a list of objects [ 2,20,32]. The evaluation is based
on ranking metrics considering the sorted order, such as Area Under
the Curve [ 9] and Normalized Discounted Cumulative Gain [ 16].
The pointwise approach [ 10] learns from the label of a single item.
And the pairwise methods learn from the relative ordering of item
pairs [ 4,11], which is further used in real-world LTR systems [ 3,
15]. Some others propose the ranking-metric-based optimization
including the listwise approaches, which directly target on aligning
the loss with the evaluation metrics [ 5,26]. However, the poor
calibration ability limits non-pointwise LTRs for wider applications.
Multi-Objective CR is a natural idea to address the above prob-
lems, where ranking loss is calibrated by a point loss [ 19,33] to
preserve both calibration and ranking ability. Sculley [27] conducts
an early study to combine regression with pairwise ranking, and Li
et al. [19] shows it can yield promising results in real-world CTR
prediction systems. Recently, multi-objective CR has been deployed
in deep models and more methods are proposed to better combine
the two losses. Yan et al . [33] address the training divergence issues
and achieve calibrated outputs. Bai et al . [1] further proposes a
regression-compatible ranking approach to balance the calibration
and ranking accuracy. Sheng et al . [28] propose a hybrid method
using two logits corresponding to click and non-click states, to
jointly optimize the ranking and calibration. However, existing
multi-objective methods are still sub-optimal facing the trade-off
between calibration and ranking, and contradict extensive data
shuffling as we have stated.
Another line of works on CRrevolves around post-processing
methods including Platt-scaling, Isotonic Regression, and etc. Tagami
et al. [29] adopt pairwise squared hinge loss for training, and then
used Platt-scaling [ 25] to convert the ranking scores to probabilities.
Chaudhuri et al . [6] compare post-processing methods including
Platt-scaling and Isotonic Regression, to calibrate the outputs of
an ordinal regression model. Apart from the calibration tailored
for ranking, there are some others that only aim for more accu-
rate probabilistic predictions, including binning [ 21] and hybrid
approaches [ 18], for e.g., Smooth Isotonic Regression [ 8], Neural
Calibration [ 22]. While they all require extra post-processing, our
method is jointly learned during training.
3 Methodology
We start from reviewing the general preliminaries of CR in Section
3.1. Then we describe its standard approach multi-objective CR
and analyze the two main drawbacks in Section 3.2. To address
these issues, we finally dig into the details of our proposed SBCR
in Section 3.3. The notations used are summarized in Table 1.
3.1 Preliminaries
The aim of Calibrated Ranking [1,33] is to predict ranking scores
for a list of candidate items (or documents, objects) properly given
a certain query (request with user, context features). Besides the
relative order between the ranking scores emphasized by conven-
tional LTR [ 5], CR also focuses on the calibrated absolute values of
scores simultaneously. To be specific, ranking scores should not
only improve ranking metrics, such as GAUC and NDCG [ 16], butTable 1: Important Notations Used in Section 3.
Q query feature space 𝑞query ˆ𝑦 predicted score
X item feature space 𝑥itemY label space
𝑠 score function 𝜎sigmoid R real number
D training dataset 𝑦 groundtruth label
L𝑝𝑜𝑖𝑛𝑡 pointwise loss ℓ𝑝𝑜𝑖𝑛𝑡 point loss for one sample
L𝑝𝑎𝑖𝑟 pairwise loss ℓ𝑝𝑎𝑖𝑟 pair loss for one query
𝑄 # queries 𝛼,𝛽 trade-off parameter
D+𝑞 positive item set D−𝑞 negative item set
I indicator L𝑚𝑢𝑙𝑡𝑖 multi-objective loss
es𝑞 dumped scores fy𝑞 dumped labels
𝑘 interval index e𝑠 deployed score function
a interval heights 𝑔 calibration module
L𝑐𝑎𝑙𝑖 calibration loss ˆ𝑦𝑐𝑎𝑙𝑖 calibrated prediction
𝑏𝑘 the function value of an interval end
L𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 self boost loss
ℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 self boost loss for one sample
also be scale-calibrated to some actual likelihood when mapped to
probabilistic predictions.
Formally, our goal is to learn a scoring function 𝑠:Q×X→ R,
given a training dataset Dconsisting of a list of samples (𝑞,𝑥,𝑦).
Here, we denoteQas the query space, 𝑞∈Q as a query ,Xas
the item feature space, 𝑥∈Xas a candidate item, and 𝑦∈Y as a
ground-truth label under specific business settings. For example,
in our video search system at Kuaishou, 𝑦can indicate whether a
video is clicked, liked, or watched effectively (beyond a predefined
time threshold). Without loss of generality, we assume the label
spaceY={1,0}throughout this paper.
The model first predicts a ranking score (also known as logit),
𝑠𝑞,𝑥=𝑠(𝑞,𝑥), (1)
for each item 𝑥associated with the same query 𝑞, and then ranks
the items according to the descending order of the scores. In addi-
tion, the ranking score is mapped to a probabilistic prediction by a
sigmoid function,
ˆ𝑦𝑞,𝑥=𝜎(𝑠𝑞,𝑥), (2)
which will be used for many downstream applications. And this
probabilistic prediction should be well calibrated, i.e., agree with
the actual likelihood that the item is clicked, liked, or watched
effectively, i.e., ˆ𝑦𝑞,𝑥=E[𝑦|𝑞,𝑥].
3.2 Multi-objective Calibrated Ranking
In literature and industrial ranking systems, multi-objective meth-
ods have been wildly adopted as standard approaches for CR [1].
3.2.1 Existing Methods. The key idea of multi-objective CR is to
take the advantage of two loss functions: 1). a pointwise loss that
calibrates the absolute probabilistic prediction values, and 2). a
ranking loss that emphasizes the relative orders of items associated
with the same query.
6228KDD ’24, August 25–29, 2024, Barcelona, Spain Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, & Yang Song
Specifically, the pointwise loss is defined as the average of Cross
Entropy overall training samples,
L𝑝𝑜𝑖𝑛𝑡 =−1
|D|∑︁
(𝑞,𝑥,𝑦)∈Dℓ𝑝𝑜𝑖𝑛𝑡(𝑞,𝑥,𝑦)
=−1
|D|∑︁
(𝑞,𝑥,𝑦)∈D𝑦logˆ𝑦𝑞,𝑥+(1−𝑦)log(1−ˆ𝑦𝑞,𝑥),(3)
which is shown to be calibrated [33] since the minima is achieved
at the point ˆ𝑦𝑞,𝑥= E[𝑦|𝑞,𝑥].
The ranking loss is usually defined on pairs of training samples.
We denoteD+𝑞={𝑥|(𝑞,𝑥,𝑦) ∈D,𝑦=1},D−𝑞={𝑥|(𝑞,𝑥,𝑦) ∈
D,𝑦=0}as the set of positive and negative items associate to the
same query 𝑞. And the pairwise loss is defined to promote a large
margin between items across the two sets,
L𝑝𝑎𝑖𝑟=−1
𝑄∑︁
𝑞∈Dℓ𝑝𝑎𝑖𝑟(𝑞)
=−1
𝑄∑︁
𝑞∈D1
|D+𝑞||D−𝑞|∑︁
𝑖∈D+𝑞,𝑗∈D−𝑞log𝜎(𝑠𝑞,𝑖−𝑠𝑞,𝑗),(4)
where the outer average is taken over all queries in D2,𝑄denotes
the number of unique queries, and the inner average is taken over
pairs inside each query. Although achieving high ranking quality,
this pairwise loss suffers from the miscalibration problem due to
translation-invariant [33].
To combine the advantages of both, a multi-objective loss is
defined as
L𝑚𝑢𝑙𝑡𝑖 =𝛼L𝑝𝑜𝑖𝑛𝑡+(1−𝛼)L𝑝𝑎𝑖𝑟, (5)
where𝛼∈ (0,1)controls the trade-off between the quality of
calibration and ranking.
Note that besides the pairwise loss, there are many other widely
used ranking losses, such as listwise softmax [ 2,23] and listwise
ApproxNDCG [ 26]. Similarly, these listwise losses also sacrifice cali-
bration for ranking performance, and could be used as a component
in multi-objective CR. We skip the discussion for conciseness.
3.2.2 Limitations of Existing Multi-Objective CR. Despite being
extensively studied, existing multi-objective CR approaches still
suffer from two crucial limitations: contradiction to extensive data
shuffling and sub-optimal trade-off between calibration and ranking
abilities.
In the default setting of most industrial training systems, samples
are first extensively shuffled, and then divided into mini-batches for
sequential gradient descent. This shuffling simulates the Indepen-
dent and Identically Distribution (IID), and consequently prevents
the overfitting problem caused by aggregated similar samples in-
side each mini-batch. We show the performance gain from data
shuffling in our experiments in Fig. 1.
While in multi-objective CR, extensive data shuffling is not ap-
plicable due to the definition of the pairwise loss. Specifically, to
calculateℓ𝑝𝑎𝑖𝑟(𝑞), we need to go through all positive-negative pairs
associated with 𝑞, indicating that all samples with the same query
2We slightly abuse the notation for conciseness of Eq. 4: 𝑞∈D represents any unique
query in the training dataset, namely, 𝑞∈{𝑞|(𝑞, 𝑥, 𝑦)∈D} .have to be gathered inside a single mini-batch. As a result, in train-
ing pairwise (or listwise) losses, data can only be shuffled at query
level, not sample level. This aggregation of similar samples (un-
der the same query, with identical context, user features) inside
each mini-batch contradicts the IID assumption and thus heavily
degrades the performance gain from data shuffling.
Another limitation of conventional CR lies in the trade-off nature
of the multi-objective loss. To be specific, we introduce the following
theorem.
Theorem 3.1.L𝑝𝑎𝑖𝑟 andL𝑝𝑜𝑖𝑛𝑡 have distinct optimal solutions.
Proof. As mentioned in Sec 3 of [ 1],L𝑝𝑜𝑖𝑛𝑡 is minimized when,
𝜎(𝑠𝑞,𝑥)=E[𝑦|𝑞,𝑥]. (6)
AndL𝑝𝑎𝑖𝑟 is minimized when,
𝜎(𝑠𝑞,𝑥1−𝑠𝑞,𝑥2)=E[I(𝑦1>𝑦2)|𝑞,𝑥1,𝑥2]. (7)
In the case𝑦1and𝑦2are independent, we rewrite Eq. 7 as,
𝜎(𝑠𝑞,𝑥1−𝑠𝑞,𝑥2)=E[𝑦1|𝑞,𝑥1]∗(1−E[𝑦2|𝑞,𝑥2]). (8)
Supposing the two losses share the same optimal solution, we use
Eq. 6 to further rewrite Eq. 8 as,
𝜎(𝑠𝑞,𝑥1−𝑠𝑞,𝑥2)=𝜎(𝑠𝑞,𝑥1)∗(1−𝜎(𝑠𝑞,𝑥2)).
Thus,
1
1+𝑒𝑠𝑞,𝑥2−𝑠𝑞,𝑥1=1
1+𝑒𝑠𝑞,𝑥2−𝑠𝑞,𝑥1+𝑒−𝑠𝑞,𝑥1+𝑒𝑠𝑞,𝑥2.
Ultimately, we have derived an infeasible equation, 𝑒−𝑠𝑞,𝑥1+𝑒𝑠𝑞,𝑥2=
0, indicating that the two losses have distinct optimal solution.
□
Intuitively,L𝑝𝑎𝑖𝑟emphasizes the relative order of items, while
failing to predict the absolute probabilistic value accurately. And
L𝑝𝑜𝑖𝑛𝑡 vice versa. In Eq 5, however, the two inherently conflicting
losses are applied on the prediction 𝑠𝑞,𝑥, and thus the best trade-off
may be sub-optimal for both calibration and ranking. In addition,
this trade-off also sensitively depends on 𝛼, leading to a challenging
hyper-parameters choosing issue. In contrast, a better design would
decouple the ranking and calibration losses with separated network
structures and gradient cut-off strategies, which elegantly addresses
the objective conflict.
3.3 Self-Boosted Calibrated Ranking
To address the two limitations, our proposed SBCR mainly consists
of two modules: 1). a self-boosted ranking module (SBR) that en-
ables extensive data shuffling, while achieving high ranking quality
like what conventional pairwise loss does, and 2). an auxiliary cali-
bration module that decouples the ranking and calibration losses
and thus successfully addresses the sub-optimal trade-off problem.
We describe our model architecture in Fig. 2.
3.3.1 The Self-Boosted Ranking Module. As mentioned earlier, ℓ𝑝𝑎𝑖𝑟(𝑞)
is calculated on all possible positive-negative item pairs associated
with𝑞, making it inseparable for sample-level data shuffling. To
address this issue, we propose a novel loss function,
L𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 =−1
|D|∑︁
(𝑞,𝑥,𝑦)∈Dℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡(𝑞,𝑥,𝑦). (9)
6229A Self-boosted Framework for Calibrated Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
QueryItem σ𝑠!,#Self-Boosted Ranking         Module𝑠(𝑞,𝑥)(𝑦!,#(𝑦$%&',!,#Calibration Moduleg((𝑦;q)Training Model
QueryCandidate ItemsσSelf-Boosted Ranking         Module*𝐲!Calibration Moduleg((𝑦;q)Deployed Model*𝐬!̃𝑠(𝑞,𝑥)
Model LoadingDump Scores & Labels*𝐲!*𝐬!Calibration Module𝑏((𝑞)
(𝑦$%&',!,#(𝑦!,#Query∑(Softmax(NN))Self-Boosted Pair-wise Loss−log	−	)𝕝(	−	)𝑠!,#*𝐬!𝑦*𝐲!−log	−	)𝕝(	−	)𝑠!,#*𝐬!𝑦*𝐲!+
Figure 2: The architecture of the proposed Self-Boosted framework for Calibrated Ranking. Middle: SBCR consists of two
modules: a self-boosted ranking module (SBR) trained by a multi-objective loss (pointwise and self-boosted pairwise loss) and a
calibration module. Left: the details of the proposed self-boosted pairwise loss. Using dumped ranking scores es𝑞from the online
deployed model, we enable both comparisons between samples associated with the same query and extensive sample-level data
shuffling. Right: the proposed calibration module that decouples the ranking and calibration objectives to avoid the conflict.
Different fromL𝑝𝑎𝑖𝑟 (Eq. 4), here each component ℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 only
depends on a single training sample. Note this design is nontrivial
due to the conflict between two facts:
•In order to enhance the ranking ability, comparisons between
the current item and its peers, i.e., other items associated
with the same query, are essential for ranking order learning.
Soℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 should be able to perceive the overall score
distribution under 𝑞.
•Since only one item’s feature 𝑥is used as the input, it is
impossible to run the network forward and backward for the
other items associated with 𝑞.
We now dig into the details of ℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 that solves this conflict.
Our system mainly consists of two parts: 1). an online Server that
receives a user’s request, makes real-time responses by scoring and
ranking candidate items, and dumps logs including the user’s feed-
back. and 2). a near-line Trainer that sequentially trains the scoring
function𝑠(·,·)on latest logs. For every few minutes, the Server
continuously loads the latest scoring function from the Trainer. We
denote the deployed model on the Server as e𝑠(·,·), which would be
a bit older than the training model 𝑠(·,·).
Formally, when the Server receives a query 𝑞and candidates
[𝑥1,...,𝑥 𝑛], it
(1)first predicts ranking scores es𝑞=[e𝑠𝑞,𝑥1,...,e𝑠𝑞,𝑥𝑛]for candi-
dates using the deployed model e𝑠(·,·),
(2)then presents the ranked items to the user and collects her
feedback, fy𝑞=[e𝑦𝑞,𝑥1,...,e𝑦𝑞,𝑥𝑛].
(3)finally dumps the scores es𝑞∈R𝑛and labels fy𝑞∈R𝑛into
context features of the current query 𝑞.
Although only adding negligible cost, es𝑞andfy𝑞actually provide
rich knowledge on the score distribution to enhance the model’s
ranking ability. More importantly, the dumped scores from an older
model reveal what the Trainer failed to learn and further direct
the following update to pay extra attention to the previously mis-
predicted samples. We thus term this Self-Boosted.With the extra es𝑞andfy𝑞as context features in 𝑞, we define,
ℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡(𝑞,𝑥,𝑦)=−log𝜎(𝑠𝑞,𝑥−es𝑞)⊤I(𝑦−fy𝑞)
−log𝜎(es𝑞−𝑠𝑞,𝑥)⊤I(fy𝑞−𝑦),(10)
where the indicator I(𝑧)=1if𝑧>0and 0 otherwise. We slightly
abuse notations by broadcasting scalars to fit vectors’ dimensions.
Remarks: Similar to ℓ𝑝𝑎𝑖𝑟, ourℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 also compares items
under the same query and encourages large margins. It is different
that the self-boost mechanism keeps lifting the model performance
by focusing on previously missed knowledge. We take the first
term inℓ𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡 for example. After masked by the indicator, it
promotes the margins between the positive item’s score 𝑠𝑞,𝑥and its
negative peers’ dumped scores es𝑞. If the items under 𝑞were poorly
predicted previously, the unmasked elements in es𝑞would be larger,
resulting in a larger loss value. To achieve a satisfactory margin,
𝑠𝑞,𝑥would be lifted more aggressively in the backpropagation.
3.3.2 The Calibration Module. The probabilistic prediction ˆ𝑦trained
using multi-objective CR (Eq. 5) usually suffers from the sub-optimal
trade-off between calibration and ranking ability.
To address this issue, we propose a calibration module to de-
couple the two losses. The pairwise loss and point loss are applied
before and after the calibration module separately, which elegantly
addresses the objective conflicting problem.
The calibration module maps ˆ𝑦into a calibrated one:
ˆ𝑦cali=𝑔(ˆ𝑦;𝑞), (11)
where ˆ𝑦cali∈(0,1)is the calibrated probability and 𝑔is the pro-
posed calibration module. We make several considerations in the
design of𝑔(·):
First, to be flexible enough to capture various functional distribu-
tions,𝑔(·)is set as a continuous piece-wise linear function. Without
loss of generality, we partition the function domain (0,1)into 100
equal-width intervals and set 𝑔(·)to be a linear function inside each
interval:
ˆ𝑦cali=99∑︁
𝑘=0
𝑏𝑘+(ˆ𝑦−0.01∗𝑘)𝑏𝑘+1−𝑏𝑘
0.01
I𝑘(ˆ𝑦), (12)
6230KDD ’24, August 25–29, 2024, Barcelona, Spain Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, & Yang Song
where I𝑘(ˆ𝑦)indicates whether ˆ𝑦lies inside the 𝑘-th interval. Namely
I𝑘(ˆ𝑦)=1ifˆ𝑦∈[0.01∗𝑘,0.01∗(𝑘+1))and 0 otherwise. And 𝑏𝑘,𝑘∈
{0,...,100}are the function values at all interval ends. Obviously,
𝑏0=0and𝑏100=1. And the other 99 which control the function
property will be adjusted during modeling training.
Second, to keep the knowledge learnt previously from pairwise
loss, our calibration module should preserve the relative orders of
items under the same query. Namely, given any two predictions
ˆ𝑦𝑞,𝑖<ˆ𝑦𝑞,𝑗, the calibrated probability ˆ𝑦cali,𝑞,𝑖≤ˆ𝑦cali,𝑞,𝑗. We thus
require the piece-wise linear function to be non-decreasing, i.e.,
𝑏0≤𝑏1≤...≤𝑏100. And the parameters 𝑏1,...,𝑏 99are learnt only
from the features of the current query, such as the user features
(user id, gender, age, behavior sequences) and context features
(timestamp, page index, date). So Eq. 11 only includes 𝑞as the input
but excludes any item features in 𝑥. The learning process is defined:
𝑏𝑘(𝑞)=𝑘∑︁
𝑗=1𝑎𝑗,𝑘∈{1,...,100},
a=Softmax(NeurNet(𝑞)).(13)
where a∈R100represents the learnt 100 interval heights, normal-
ized by the softmax function and 𝑏𝑘is calculated as the accumulated
heights from the first to 𝑘-th interval.
Finally, our calibration module is trained using pointwise loss to
ensure accurate absolute prediction values:
L𝑐𝑎𝑙𝑖=−1
|D|∑︁
(𝑞,𝑥,𝑦)∈D
𝑦logˆ𝑦cali,𝑞,𝑥+(1−𝑦)log(1−ˆ𝑦cali,𝑞,𝑥)
.
(14)
3.3.3 The Overall Architecture of SBCR and Training Tricks. Our
model consists of two networks, as summarized in Fig 2.
The first deep network defines the scoring function 𝑠(𝑞,𝑥)with
two groups of inputs: 1). 𝑞, features shared by samples under the
same query, including user features, long-term user behaviors and
context features; 2). 𝑥, features of a specific item. In the industrial
ranking system of Kuaishou, we adopt QIN [ 14] as𝑠(𝑞,𝑥)due to
its SOTA performance. This network is trained using,
L𝑚𝑢𝑙𝑡𝑖 _𝑏𝑜𝑜𝑠𝑡 =𝛼L𝑝𝑜𝑖𝑛𝑡+(1−𝛼)L𝑝𝑎𝑖𝑟 _𝑏𝑜𝑜𝑠𝑡. (15)
We replace the conventional pairwise loss in Eq. 5 by our proposed
self-boosted pairwise loss, which enables sample-level shuffling.
Our second deep network defines the function for calculating the
interval heights ain Eq. 13, which is trained by Eq. 14. In our system,
the network consists of 4 FC layers of size (255,127,127,100).
To avoid a sub-optimal trade-off between calibration and ranking,
we further stop the gradient back propagation for all inputs to the
calibration module (i.e., ˆ𝑦,𝑞). Thus𝑠(𝑞,𝑥)focuses on the ranking
quality and the calibration module only deals with calibration. We
will discuss more on parameter sensitivity in our experiments.
4 Experiments
In this section, to validate the effectiveness of our proposed SBCR
framework, we compare SBCR with many state-of-the-art CR al-
gorithms. We also provide an in-depth analysis to investigate the
impact of each building block in SBCR. Experiments are conducted
based on the Kuaishou video search system, including both offline
evaluations on the billion-scale production dataset, and online A/BTable 2: Statistics of the real production dataset. Query means
a request from the user. M and B are short for million and
billion.
Dataset #Querys #Items #Samples #Samples/#Query
Train 211 M 452 M 1.88 B. 8.89
Test 41.2 M 281 M 372 M 9.04
testing on the real traffic of millions of active users. We did not
include experiments on the public datasets, since our method is
designed for the online training system.
4.1 Experiment Setup
4.1.1 Datasets. In our offline experiments, all compared algo-
rithms are initialized from the same checkpoint, trained online
using 5 days’ data, and then frozen to test on the 6th day’s data. The
dataset is collected from the user log on the video search system of
Kuaishou and the statistics of the dataset are shown in Tab. 2. Our
method is designed for the online training systems that are widely
deployed in industrial scenarios, so we only conduct experiments
on the real production dataset.
4.1.2 Implementation Details. In our experiments, we adopt QIN [ 14]
as our architecture for all compared methods. With efficient user
behavior modeling, QIN is a strong production baseline latest de-
ployed on the KuaiShou video search system. For feature engineer-
ing, ID features are converted to dense embeddings and concate-
nated with numerical features. All models are trained and tested
using the same optimization settings, i.e., Adam optimizer, learning
rate of 0.001, and batch size of 512. All models are trained with one
epoch following Zhang et al . [34] , which is widely adopted in the
production practice. For the relative ranking weight (1−𝛼)/𝛼, we
chose the best one from [0.01, 0.1, 1.0, 10, 100] for each compared
algorithm and report the performance of them in their own optimal
hyper-parameter settings for a fair comparison. For our proposed
SBCR, we simply set the relative weight to 1 consistently across all
experiments.
4.1.3 Evaluation Metrics. In this work, we consider both ranking
and calibration performances. For evaluating the ranking perfor-
mance, we choose NDCG@10 andGAUC (Group AUC). NDCG@10
is consistent with other metrics like NDCG@5. GAUC is widely
employed to assess the ranking performance of items associated
with the same query, and has demonstrated consistent with online
performance in previous studies [24, 35]. GAUC is computed by:
GAUC =Í
𝑞∈D#candidates(𝑞)×AUC 𝑞Í
𝑞∈D#candidates(𝑞), (16)
where AUC 𝑞represent AUC within the same query 𝑞.
For evaluating the calibration performance, we include LogLoss,
expected calibration error (ECE) [ 22], and predicted CTR over the
true CTR ( PCOC) [ 12]. LogLoss is calculated the same way as in
Eq. 3, which measures the logarithmic loss between probabilistic
6231A Self-boosted Framework for Calibrated Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: The comparison of SOTAs on the production dataset. The best results are highlighted in bold and the second-best
are underlined. * indicates significant improvement with p-value=0.05 than previous best-performing Point + ListCE. Shuffle
denotes compatibility with extensive sample-level data shuffling. SBCR outperforms all due to 2 key advantages: compatibility
with extensive data shuffling and effective structure to decouple ranking and calibration losses.
Metho
dRanking
Metrics Calibration
MetricsSample-le
vel Shuffle
GAUC
NDCG@10 LogLoss
PCOC ECE
Single-obje
ctivePointwise 0.6076
0.7269 0.5612 1.0183
0.0047✓
RankNet
[3] 0.6110 0.7297 2.7337
7.8285 0.5438 ×
ListNet
[5] 0.6107
0.7303 2.9845
8.1322 0.5975 ×
ListCE
[1] 0.6092
0.7285 0.5838
1.0420 0.0092 ×
Multi-obje
ctivePoint
+ RankNet [19] 0.6089
0.7279 0.5623
0.9759 0.0057 ×
Point
+ ListNet [33] 0.6095
0.7282 0.5621
1.0293 0.0065 ×
Multi-task
[33] 0.6082
0.7276 0.5615
1.0207 0.0052 ×
Calibrate
d Softmax [33] 0.6105
0.7302 0.5678
1.1532 0.0126 ×
JRC
[28] 0.6102
0.7293 0.5619
0.9894 0.0049×
Point
+ ListCE [1] 0.6107
0.7298 0.5615
0.9878 0.0044 ×
Ours SBCR 0.6118*
0.7315* 0.5610*
1.0092* 0.0045✓
predictions and true labels. ECE and PCOC are computed by
ECE=1
|D|99∑︁
𝑘=0|∑︁
(𝑞,𝑥,𝑦)∈D(𝑦−ˆ𝑦cali,𝑞,𝑥)I𝑘(ˆ𝑦cali,𝑞,𝑥)|,
PCOC =1Í
(𝑞,𝑥,𝑦)∈D𝑦∑︁
(𝑞,𝑥,𝑦)∈Dˆ𝑦cali,𝑞,𝑥,(17)
where I𝑘is defined in the same way as Eq. 12. Among the three
calibration metrics, LogLoss provides sample-level measurement,
whereas ECE and PCOC provide subset level and dataset level mea-
surement, respectively. There are some other metrics like Cal-N and
GC-N [ 8] and here we mainly follow the setting of calibrated rank-
ing [ 28,33] for a fair comparison. Lower LogLoss or ECE indicates
better performance, and PCOC is desired to be close to 1.0.
These five metrics serve as reliable indicators for evaluating both
ranking and calibration abilities.
4.1.4 Compared Methods. As in Table 3, we include several im-
portant baseline methods for a comprehensive comparison. These
baseline methods are divided into two groups based on whether
the loss function is single-objective or multi-objective. The Single-
objective group consists of these four methods:
•Pointwise refers to the standard LogLoss (Eq. 3), which is
widely adopted for binary targets. It is also the production
baseline for most industrial ranking systems.
•RankNet [3] adopts pairwise loss (Eq. 4) to optimize the
relative ranking of pairs of samples.
•ListNet [5] defines a listwise loss to maximize the likelihood
of the correct ordering of the whole list.
•ListCE [1] proposes a regression compatible ranking ap-
proach where the two ranking and regression components
are mutually aligned in a modified listwise loss.
In the Multi-objective group, we include several advanced meth-
ods with both calibrated point loss and ranking-oriented loss for
the comprehensive comparison:•Point + RankNet [19] combines the pointwise and pairwise
loss in a multi-objective paradigm (Eq. 5) to improve both
calibration and ranking.
•Point + ListNet [33] is the combination of the pointwise
and the listwise loss, which is proved as a strong calibrated
ranking baseline and termed “multi-ojbective" in [33].
•Multi-task [33] is a multi-task method that uses multi-head
of DNN for the ranking and calibration scores.
•Calibrated Softmax [33] is a reference-based method where
an anchor candidate with label 𝑦0to a query is introduced
to control the trade-off.
•JRC [28] proposes a hybrid method that employs two log-
its corresponding to click and non-click states and jointly
optimizes the ranking and calibration abilities.
•Point + ListCE [1] combines the pointwise loss with ListCE
that makes ranking and regression components compatible
and achieves advanced results.
4.2 Main Experimental Results
The main experimental results are shown in Table 3. The methods
are compared on both ranking and calibration metrics. From the
results, we have the following observations:
First, in the single-objective group, pointwise achieves the best
calibration performance but inferior ranking performance. In con-
trast, RankNet and ListNet outperform the Pointwise model on the
ranking ability, at the expense of being completely uncalibrated.
Among these methods, ListCE can reach a tradeoff for it makes
regression compatible with ranking, but it still suffers from poor
calibration. This validates the necessity of calibrated ranking.
Second, the multi-objective methods incorporate the two losses
and achieve a better trade-off. And several recent studies (JRC
and Point + ListCE) have achieved encouraging progress on both
ranking and calibration metrics. Note that LogLoss is a stronger cali-
bration metric than PCOC and ECE. For example, if a model predicts
6232KDD ’24, August 25–29, 2024, Barcelona, Spain Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, & Yang Song
Table 4: Comparison on different sample aggregations. Shuf
denotes extensive sample-level shuffle, Aggre means aggre-
gating the whole candidate list in a single mini-batch. Best
are marked bold. Shuf consistently outperforms Aggre on
both Point and Point + Pair models.
NDCG@10
LogLoss PCOC ECE
PointA
ggre 0.7267
0.5625 1.0205 0.0049
Shuf 0.7269 0.5612 1.0183 0.0046
Point A
ggre 0.7279
0.5614 0.9759 0.0057
+ Pair Shuf
(SBR) 0.7315 0.5614 1.0147 0.0056
averaged prediction over the whole dataset, the model achieves
perfect PCOC and ECE but poor LogLoss. Considering LogLoss
as the main calibration metric, all compared multi-objective meth-
ods still suffer from sub-optimal trade-off between calibration and
ranking, when compared to the best Single-objective methods, vali-
dating our second motivation. When comparing the three variants
of [33], we find that cal-softmax achieves slightly better NDCG
but suffers from unacceptable worst calibration performance, while
multi-objective gets the best trade-off between ranking and calibra-
tion. Our observation are also consistent with the previous results
reported in the original paper.
Third, our proposed SBCR outperforms all compared algorithms
on both ranking and calibration performance. This validates the
two key advantages of SBCR, i.e., compatibility with extensive data-
shuffling and effective structure to avoid trade-off between ranking
and calibration. Specifically, SBCR is trained with the dumped pre-
dictions by the online deployed model, making it the only method
that needs no sample aggregation when computing the ranking
loss. And a calibration module is introduced to decouple the point
loss and ranking loss to address the sub-optimal trade-off problem.
The gain of the two key advantages will be further analyzed in
section 4.3.1 and 4.3.2.
4.3 Ablation Study and Analysis
We conduct ablation studies to investigate the contribution of each
SBCR building block: the self-boosted pairwise loss to address the
data-shuffling problem, and the calibration module to address the
trade-off problem. We also include hyper-parameter analysis to
show the sensitivity.
4.3.1 Analysis on Data-shuffling. As mentioned, extensive data-
shuffling that simulates the Independent and Identical Distribu-
tion has long been proven beneficial in preventing the overfitting
problem. We re-validate the performance gain of data-shuffling on
both point loss and point + pair loss. For point + pair loss, Point +
RankNet [ 19] is used as the aggregation method and our SBR (Eq.
15) is used as the shuffling method, results shown in Tab. 4.
Shuffling achieves consistent improvement over sample aggre-
gation, which validates that extensive data-shuffling is essential
for performance and supports our first motivation. Conventional
multi-objective CR algorithms require the aggregation of the whole
candidate list in a single mini-batch for computing the ranking loss,Table 5: Comparison on different Calibration Modules.
“Calib” is our proposed one. We observe consistent improve-
ment when the ranking module is trained by ListNet and
SBR, validating our adaptability.
GAUC LogLoss PCOC ECE
ListNet∅ 0.6107 2.9845 8.1322 0.5975
Platt 0.6107 0.5627 1.0353 0.0073
Calib 0.6107 0.5621 1.0279 0.0065
SBR∅ 0.6118 0.5614 1.0147 0.0056
Platt 0.6118 0.5613 1.0132 0.0051
Calib 0.6118 0.5610 1.0092 0.0045
which is incompatible with extensive data-shuffling. SBCR solves
this problem and achieves superior performance.
4.3.2 The Impact of Calibration Module. In order to analyze the
impact of the Calibration Module, we compared several methods in
Tab. 5. ListNet-Platt [ 33] applies Platt-scaling post-processing after
a ListNet model, which is a strong baseline. Hence, we compared our
calibration module with Platt-scaling on the same ListNet model.
We also apply Platt-scaling and ours upon the same SBR model as
defined in Eq. 15.
As shown in Tab. 5, we observed that both Platt-scaling and
ours preserve the relative orders and show the same GAUC. Our
proposed calibration module achieves consistently better calibra-
tion ability on both ListNet and SBR, which validates the strong
adaptability of our method.
4.3.3 Effects of Hyper-Parameter. The only hyper-parameter intro-
duced in our method is 𝛼, the trade-off parameter between point
loss and self-boosted pair loss in Eq. 15. We define (1−𝛼)/𝛼as the
relative ranking weight and examine the sensitivity in Fig. 3.
First, surprisingly, an extremely small value of relative ranking
weight is not optimal for calibration and an extremely large value
is not optimal for ranking. This validates that the two losses collab-
orate with each other. Point loss is necessary for ranking, especially
in queries where all items are negative and pair loss is necessary
for calibration by giving auxiliary guild for model training.
Second, SBCR is robust to the setting of 𝛼. When the relative
weight is set 10 times larger /smaller, the performance of SBCR is
still comparable to that of SOTA. We own this robustness to our
calibration module which is specially designed to be monotonic
for a given𝑞. Namely, it preserves the learned orderings from our
ranking module. Optimizing the calibration module will not degrade
the ranking performance.
Third, we still observe slightly trade-off between calibration and
ranking. This trade-off has been greatly improved by the calibration
module. We tried to remove the calibration module and found that
when(1−𝛼)/𝛼=100, ECE will be increased to 0.1796, which is 35
times worse than the current case.
4.4 Online Performance
We validate the proposed SBCR with online A/B testing on the
video search system of Kuaishou (Tab. 6). We compare SBCR with
our latest production baseline Point and the strongest compared
6233A Self-boosted Framework for Calibrated Ranking KDD ’24, August 25–29, 2024, Barcelona, Spain
0.01 0.1 1 10 100
1− α
α0.6060.6070.6080.6090.6100.6110.6120.613GAUCGAUC
0.00440.00460.00480.00500.00520.0054
ECEGAUC
ECE
Figure 3: The sensitivity of relative ranking weight (1−𝛼)/𝛼
for SBCR (Eq. 15). Higher GAUC and lower ECE indicate
better performance.
algorithm reported in Table 3, Point+ListCE. All three algorithms
share the same backbone QIN [ 14], and the same features, model
structures and optimizer. The online evaluation metrics are CTR,
View Count and User Time Spend. View count measures the total
amount of video users watched, and time spend measures the total
amount of time users spend on viewing the videos.
As shown in Tab. 6, SBCR contributes to the +4.81% increase in
CTR, +3.15% increase in View Count and +0.85% increase in Time
Spend. Note that the 0.2% increase is a significant improvement in
our system. SBCR is also efficient for online serving. Compared to
our baseline QIN+Point, the only additional module is the calibra-
tion network that adds 1.41% parameters and 0.96% floating point
operations, which is negligible for the model.
Note that there is an important issue that is commonly faced
in the real production systems. Usually, there are several different
algorithms under A/B test simultaneously. So the dumped scores
used for training SBCR are actually from deployed models in dif-
ferent A/B tests, not only SBCR’s corresponding deployed model.
This is not equivalent to the standard self-boosted mechanism in
Sec 3. We should check the impact.
In A/B test, the pointwise baseline serves the main traffic, thus
SBCR is mostly trained with dumped scores from the pointwise
baseline. In this sub-optimal implementation, we still observe signif-
icant gain (the 3rd two in Table 6). And in A/B backtest (last row in
Table 6), SBCR serves the main traffic and the old pointwise model
serves the small traffic. The dumped scores are mostly from SBCR
itself. We observe a larger improvement compared to that in AB test:
an additional gain of +0.63% View Count and +0.43% Time Spent.
We conclude that the dumped scores from other models also work,
with a slightly smaller gain compared to standard self-boost. This
is because the mis-predicted samples by other models are also hard
and informative and focusing on other strong model’s mistakes is
also beneficial.
5 Conclusion and Future Works
We proposed a Self-Boosted framework for Calibrated Ranking in
industrial online applications. SBCR addressed the two limitations
of conventional multi-objective CR, namely, the contradiction withTable 6: The improvements of SBCR in online A/B test com-
pared to the production baseline. In the video search system
of Kuaishou, 0.2% increase is a significant improvement for
CTR, View Count and User Time spend.
CTR View Count Time Spent
QIN + Point (baseline) - - -
QIN + Point + ListCE +1.01% +0.80% +0.14%
QIN + SBCR (AB) +4.81% +3.15% +0.85%
QIN + SBCR (AB back) +5.70% +3.78% +1.28%
extensive data shuffling and the sub-optimal trade-off between cali-
bration and ranking, which contributes to significant performance
gain. SBCR outperformed our highly optimized production baseline
and has been deployed on the video search system of Kuaishou,
serving the main traffic of hundreds of millions of active users.
Note that we restricted our calibration module 𝑔(·)to piece-wise
linear function since it is easy to guarantee the monotonicity, which
is necessary to preserve the relative orders of items under the same
query. A promising future direction is to improve the flexibility of
𝑔by upgrading it to monotonic neural networks.
References
[1]Aijun Bai, Rolf Jagerman, Zhen Qin, Le Yan, Pratyush Kar, Bing-Rong Lin, Xuan-
hui Wang, Michael Bendersky, and Marc Najork. 2023. Regression Compatible
Listwise Objectives for Calibrated Ranking with Binary Relevance. In Proceed-
ings of the 32nd ACM International Conference on Information and Knowledge
Management. 4502–4508.
[2]Sebastian Bruch, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2019.
An analysis of the softmax cross entropy loss for learning-to-rank with binary
relevance. In Proceedings of the 2019 ACM SIGIR international conference on theory
of information retrieval. 75–78.
[3]Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An
overview. Learning 11, 23-581 (2010), 81.
[4]Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole
Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient
descent. In Proceedings of the 22nd international conference on Machine learning,
Vol. 119. 89–96.
[5]Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning
to rank: from pairwise approach to listwise approach. In Proceedings of the 24th
international conference on Machine learning. 129–136.
[6]Sougata Chaudhuri, Abraham Bagherjeiran, and James Liu. 2017. Ranking and
calibrating click-attributed purchases in performance display advertising. In
Proceedings of the ADKDD’17. 1–6.
[7]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
Workshop on Deep Learning for Recommender Systems. ACM, 7–10.
[8] Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating user
response predictions in online advertising. In Machine Learning and Knowledge
Discovery in Databases: Applied Data Science Track: European Conference, ECML
PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part IV. Springer,
208–223.
[9]Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognit. Lett. 27, 8
(2006), 861–874.
[10] Fredric C Gey. 1994. Inferring probability of relevance using the method of
logistic regression. In Proceedings of the 17th Annual International ACM-SIGIR
Conference on Research and Development in Information Retrieval. 222–231.
[11] Bin Gu, Victor S. Sheng, KengYeow Tay, Walter Romano, and Shuo Li. 2015.
Incremental Support Vector Learning for Ordinal Regression. IEEE Transactions
on Neural networks and learning systems 26, 7 (2015), 1403–1416.
[12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration
of Modern Neural Networks. In Proceedings of the 34th International Conference
on Machine Learning, Vol. 70. 1321–1330.
[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
Deepfm: a factorization-machine based neural network for ctr prediction. In
Proceedings of the 26th International Joint Conference on Artificial Intelligence.
6234KDD ’24, August 25–29, 2024, Barcelona, Spain Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, & Yang Song
Melbourne, Australia., 2782–2788.
[14] Tong Guo, Xuanping Li, Haitao Yang, Xiao Liang, Yong Yuan, Jingyou Hou,
Bingqing Ke, Chao Zhang, Junlin He, Shunyu Zhang, et al .2023. Query-dominant
User Interest Network for Large-Scale Search Ranking. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management. 629–
638.
[15] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,
Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-
based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2553–2561.
[16] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),
422–446.
[17] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in neural information processing systems 30 (2017).
[18] Ananya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty
calibration. Advances in Neural Information Processing Systems 32 (2019).
[19] Cheng Li, Yue Lu, Qiaozhu Mei, Dong Wang, and Sandeep Pandey. 2015. Click-
through prediction for advertising in twitter timeline. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
1959–1968.
[20] Tie-Yan Liu et al .2009. Learning to rank for information retrieval. Foundations
and Trends® in Information Retrieval 3, 3 (2009), 225–331.
[21] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining
well calibrated probabilities using bayesian binning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 29.
[22] Feiyang Pan, Xiang Ao, Pingzhong Tang, Min Lu, Dapeng Liu, Lei Xiao, and
Qing He. 2020. Field-aware calibration: a simple and empirically strong method
for reliable probabilistic predictions. In Proceedings of The Web Conference 2020.
729–739.
[23] Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael
Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, and Stephan
Wolf. 2019. Tf-ranking: Scalable tensorflow library for learning-to-rank. In
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 2970–2978.
[24] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong
sequential behavior data for click-through rate prediction. In Proceedings of the29th ACM International Conference on Information & Knowledge Management.
2685–2692.
[25] John Platt et al .1999. Probabilistic outputs for support vector machines and com-
parisons to regularized likelihood methods. Advances in large margin classifiers
10, 3 (1999), 61–74.
[26] Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework
for direct optimization of information retrieval measures. Information retrieval
13 (2010), 375–397.
[27] David Sculley. 2010. Combined regression and ranking. In Proceedings of the 16th
ACM SIGKDD international conference on Knowledge discovery and data mining.
979–988.
[28] Xiang-Rong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han,
Hongbo Deng, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Joint optimization
of ranking and calibration with contextualized hybrid model. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
4813–4822.
[29] Yukihiro Tagami, Shingo Ono, Koji Yamamoto, Koji Tsukamoto, and Akira Tajima.
2013. Ctr prediction for contextual advertising: Learning-to-rank approach. In
Proceedings of the Seventh International Workshop on Data Mining for Online
Advertising. 1–8.
[30] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDD’17. 1–7.
[31] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the web conference 2021.
1785–1797.
[32] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise
approach to learning to rank: theory and algorithm. In Proceedings of the 25th
international conference on Machine learning, Vol. 307. 1192–1199.
[33] Le Yan, Zhen Qin, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2022. Scale
Calibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 4300–4309.
[34] Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han,
Hongbo Deng, and Bo Zheng. 2022. Towards Understanding the Overfitting
Phenomenon of Deep Click-Through Rate Models. In Proceedings of the 31st ACM
International Conference on Information & Knowledge Management. 2671–2680.
[35] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. ACM, 1059–1068.
6235