Preventing Strategic Behaviors in Collaborative Inference for
Vertical Federated Learning
Yidan Xing
Shanghai Jiao Tong University
Shanghai, China
katexing@sjtu.edu.cnZhenzhe Zheng∗
Shanghai Jiao Tong University
Shanghai, China
zhengzhenzhe@sjtu.edu.cnFan Wu
Shanghai Jiao Tong University
Shanghai, China
fwu@cs.sjtu.edu.cn
ABSTRACT
Vertical federated learning (VFL) is an emerging collaborative ma-
chine learning paradigm to facilitate the utilization of private fea-
tures distributed across multiple parties. During the inference pro-
cess of VFL, the involved parties need to upload their local embed-
dings to be aggregated for the final prediction. Despite its remark-
able performances, the inference process of the current VFL system
is vulnerable to the strategic behavior of involved parties, as they
could easily change the uploaded local embeddings to exert direct
influences on the prediction result. In a representative case study
of federated recommendation, we find the allocation of display op-
portunities to be severely disrupted due to the parties’ preferences
in display content. In order to elicit the true local embeddings for
VFL system, we propose a distribution-based penalty mechanism
to detect and penalize the strategic behaviors in collaborative infer-
ence. As the key motivation of our design, we theoretically prove
the power of constraining the distribution of uploaded embeddings
in preventing the dishonest parties from achieving higher utility.
Our mechanism leverages statistical two-sample tests to distinguish
whether the distribution of uploaded embeddings is reasonable, and
penalize the dishonest party through deactivating her uploaded
embeddings. The resulted mechanism could be shown to admit
truth-telling to converge to a Bayesian Nash equilibrium asymp-
totically under mild conditions. The experimental results further
demonstrate the effectiveness of the proposed mechanism to reduce
the dishonest utility increase of strategic behaviors and promote
the truthful uploading of local embeddings in inferences.
CCS CONCEPTS
•Theory of computation →Algorithmic game theory and
mechanism design; •Computing methodologies →Machine
learning.
KEYWORDS
Collaborative Inference; Strategic Behaviors; Mechanism Design;
Vertical Federated Learning;
∗Zhenzhe Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671663ACM Reference Format:
Yidan Xing, Zhenzhe Zheng, and Fan Wu. 2024. Preventing Strategic Be-
haviors in Collaborative Inference for Vertical Federated Learning. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671663
1 INTRODUCTION
With the development of machine learning techniques, the con-
sensus that richer features and more available data could enhance
prediction performances has been widely established. In recent
years, federated learning (FL) is proposed as a cutting-edge col-
laborative machine learning paradigm to take advantage of the
distributed data while protecting data privacy. The FL techniques
are generally classified into horizontal FL (HFL) and vertical FL
(VFL) [ 18] according to the distributed patterns of data. Target-
ing at the scenario with each party holding different features for
an aligning set of samples, VFL requires each involved party to
implement a local model which maps her local features to local
embeddings, and requires the server to implement a top model
that maps the aggregated local embeddings uploaded by the parties
to the final prediction result (Figure 1). VFL techniques have been
widely deployed in various scenarios, especially in recommendation
system [16, 38], online advertising [20, 37], and finance [5, 6].
Despite the promising performances of VFL, we notice an un-
explored deficiency of this collaborative paradigm: the inference
process in VFL is vulnerable to the strategic manipulations on the
uploaded local embeddings. Compared to HFL and centralized ma-
chine learning methods, the inference process in VFL requires each
involved party to collaboratively upload the local embeddings for
the current sample. As the learning models have been determined
at the inference stage, the local embedding uploaded by one in-
volved party could exert direct influences on the inference result,
leaving chances for the party to manipulate the inference result in
an predictable way. On the other hand, the involved parties may
indeed have the motivations to strategically change the inference
results towards their desired ones. For example, an organization
may prefer to create better prediction for content belong or rele-
vant to it when providing user behavioral feature embeddings to a
recommendation system, and a bank may prefer to misguide other
banks to provide lower loan limit for factually credible clients, with
the aim to attract those clients and promote its own transactions.
In this work, we aim to formally investigate such kinds of strate-
gic behaviors and the corresponding manipulation-resistant mech-
anism when collaborative inferences meet the strategic intentions
of involved parties in VFL system. To characterize the behavioral
pattern of involved parties, we resort to the celebrated concept of
utility function andNash equilibrium in game theory to describe
3574
KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Features  𝒙𝑨 Features  𝒙𝑩 Features  𝒙𝑪𝜽𝑨𝜽𝑩 𝜽𝑪𝒉𝑨(𝜽𝑨;𝒙𝑨)𝒉𝑩(𝜽𝑩;𝒙𝑩) 𝒉𝒄(𝜽𝒄;𝒙𝒄)
Top Model
Utility𝒉𝟎(𝒕)
𝒕𝑨𝒕𝑩 𝒕𝑪 
𝒖𝑨(𝒉𝟎(𝒕))Utility
𝒖𝑩(𝒉𝟎(𝒕))Utility
𝒖𝑪(𝒉𝟎(𝒕))𝒕𝒊=𝒉𝒊𝜽𝒊;𝒙𝒊?
Figure 1: Overview of VFL System. The local embeddings up-
loaded by a party may differ from the true local embeddings.
the objective of the parties and a stable state of the strategic inter-
actions, respectively.
In order to inspect the potential consequences of such strategic
behaviors, we formulate a representative federated recommenda-
tion game between organizations who collaborate to provide item
recommendation for users. The utilities of these organizations are
defined as the exposure of item owned by them, and they could
manipulate the uploaded user embeddings to change the predicted
score and the allocation of exposure. Prominently, we find the re-
sulted pure Nash equilibrium would indistinguishably allocate the
exposure opportunities in a random way when there are two or-
ganizations in the game with comparable power, regardless of the
properties of items they owned. In other words, when parties are
obsessed with manipulating the uploaded embeddings for their
own utility, the design of recommendation system would losses its
original spirit, necessitating a manipulation-resistant mechanism
against such strategic behaviors in VFL system.
While the strategic behaviors essentially arises from the incon-
sistency between the utilities of parties and the objective of VFL
system, a natural idea is to introduce external monetary transfer to
cover the misalignment between them following the mainstream
of incentive mechanism design [ 40]. However, even if we do not
consider the implementation practicability of a monetary transfer
mechanism within the VFL system, its working principle would
be unaffordable in our context. Since we need to preserve the cor-
rectness of the inference results, the final predictions could not be
modified in any form to satisfy strategic intentions of parties. To
elicit the true local embeddings, the money transfer mechanism
should guarantee the sum of monetary reward and the utility of
current prediction to be larger than any other strategy that may
modify the true embeddings, thus requiring this sum to be at least
the utility of the best possible prediction achievable through manip-
ulation. As a result, the inference result worse for a party should
simultaneously bring her larger monetary reward, leading the re-
sulted expenditure to be incredibly large for the server and highly
fluctuating on the distribution of inference results.Given the infeasibility of adopting external monetary rewards, a
manipulation-resistant mechanism have to be implemented fully
based on the collaborative inference process. Due to the intrinsic
uncertainty of data, it is generally impossible to confirm whether
a specific local embedding has been manipulated, which compels
us to consider utilizing the historical statistics of the embeddings
to identify and constrain strategic behaviors. Nevertheless, it is
unclear what kinds of statistics should be adopted among the mass
of candidates, and whether these metrics could indeed help with
our goal to prevent the considered strategic behaviors.
Inspired by the traditional economics literature [ 17], we con-
sider the distribution information of local embeddings as a strong
candidate to serve as the cornerstone of our manipulation-resistant
mechanism. In particular, when the distribution of uploaded infer-
ence embeddings are enforced to align with its prior distribution,
we demonstrate that the involved parties are unable to realize any
dishonest utility increase in collaborative inference under mild
assumptions describing the partial alignment between the server
prediction function and the utilities of parties, and the training
embeddings could also serve as the reference for prior distribution.
Despite these strong guarantees, due to the high dimensional
nature of local embeddings in VFL applications, it is implausible
for the server to realize precise restriction on the distribution of
uploaded embeddings, thus initiates our final design of distribution-
based penalty mechanism. Our mechanism works in alternative
between two process: detection of potential strategic behaviors
and penalization for the detected strategic behaviors. During the
collaborative inference process, we periodically detect whether the
uploaded embeddings follow the same distribution as the training
embeddings with high probability, which is realized through apply-
ing statistical two-sample tests [ 12,21], and temporarily deactivate
the embeddings uploaded by a party as penalty when she is detected
to be cheating. We theoretically prove the convergence of truth-
telling strategies to a Bayesian Nash equilibrium in the large sample
limit under appropriate mechanism parameters, which underscores
the rationality and efficacy of our design. To further validate its em-
pirical performances, we conduct extensive experiments to observe
the influences of our penalty mechanism on different strategies. It
turns out the utility of various manipulating strategies could be
largely reduced to be similar or less than the utility of truth-telling
strategy, thus effectively alleviating the incentives of parties to
conduct strategic behaviors.
The main contributions of this work are summarized as follows:
•We consider the strategic behaviors to manipulate the local em-
beddings in collaborative inference for VFL system, which are
unexplored in previous work. In a representative federated rec-
ommendation game, we demonstrate the destructive effects of
strategic behaviors on prediction results when involved parties
achieve a Nash equilibrium.
•We propose the distribution-based penalty mechanism as a flex-
ible plug-in module of the vanilla VFL algorithm to prevent the
considered strategic behaviors of manipulating local embed-
dings. With theoretically motivated design, the truth-telling
strategy would converge to Beyesian Nash equilibrium under
large sample limit, thus alleviates the strategic incentives of
involved parties.
3575Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
•We evaluate the proposed penalty mechanisms on public datasets
for two typical manipulation strategies and their probabilistic
variants. The empirical results validate the effectiveness of the
proposed mechanisms in reducing the utility obtained by dis-
honest strategies and promoting the parties to upload true local
embeddings during inferences.
2 RELATED WORK
Since the individual participants in FL usually have their own in-
terests, the incentive and strategic problems in FL has been widely
studied to facilitate the deployment of FL applications [ 18,22,40].
Most of the previous work study the methods to evaluate the contri-
bution of participants as a reference for reward allocation or client
selection [ 8,25,27,32], incentivize the participants to keep active
and dedicated in training [ 13,31,33,41], as well as form coalitions
to achieve better training performances for non-i.i.d. data [ 7,9,10].
As all the above work consider the incentive problem in the training
stage of HFL or VFL, to the best of our knowledge, none of the ex-
isting work has considered the strategic behaviors of manipulating
the intermediate local embeddings uploaded to the server during
collaborative inferences in VFL system, which are orthogonal to
the strategic behaviors in training stage.
In fact, the authors of [ 29] have proposed to utilize the same
embedding manipulation approach from the perspective of a mali-
cious attacker. Particularly, they investigate the set of adversarial
dominating inputs (ADI) in inferences of VFL, such that the other
party’s influence on the inference result would be negligible. The
attacks on FL system typically aim to replicate representative deep
learning attacks for HFL, including [ 2,4,28,34,36,39], while some
other work attacking the VFL system exploit the characteristics of
splitted model to infer the private features or labels of other partic-
ipants [ 11,19,23,24]. In opposed to these work that aim to protect
VFL system against malicious attackers or honest-but-curious par-
ticipants, our mechanism are designed for strategic participants
with their own utility objectives, which provides a complementary
perspective to protect the well-functionality of VFL system.
The main ideology of our distribution-based penalty mechanism
is motivated by the linking mechanism [ 17] that restricts the total
reported preferences of agents in a sequence of public decision
problems to restrain the strategic behaviors. In detail, the agents
are strictly limited in the frequency of reporting each preference
across the problems. The research on linking mechanism is gradu-
ally progressed in terms of its additional properties, variants and
applications [ 14,26,30,35]. Compared with these work, due to the
high-dimensional and complex nature of intermediate embeddings
in VFL system, we could not learn the exact range and distribution
of the high-dimensional embeddings, leading our mechanism and
analysis to be distinct from the existing work.
3 PRELIMINARIES
Consider a typical VFL system with 𝑀parties1and a server, where
the role of server could be assumed by one of the involved parties.
The features are vertically distributed across the parties, with each
party𝑖privately owns 𝑐𝑖features of each sample. We use 𝑥𝑖∈R𝑐𝑖to
1We may use parties with participants interchangeably throughout the work, which
also indicate the organizations in the federated recommendation game of Section 4.denote the local features owned by party 𝑖for sample𝑥. In order to
realize the collaborative prediction for sample 𝑥, each party 𝑖holds
a set of model parameters 𝜃𝑖and a corresponding local embedding
functionℎ𝑖(·), which maps the model parameters and the input
local sample features 𝑥𝑖to local embeddings. The server holds a
set of model parameters 𝜃0and a server prediction function ℎ0(·),
which maps the server model parameters and all the uploaded
local embeddings to a prediction in R. As this work focuses on the
strategic behaviors during the collaborative inference, we assume
the embedding functions ℎ𝑖and the server model ℎ0to be some
predetermined randomized functions, and would not go into details
of the training and communication process.
With the above notations, the collaborative inference process
for an unseen sample 𝑥could be formally described as follows: 1)
each party𝑖compute its local embedding 𝑡𝑖:=ℎ𝑖(𝑥𝑖)using the
local features 𝑥𝑖; 2) each party 𝑖uploads𝑡𝑖to the server; 3) the
server computes ℎ0(𝑡1,...,𝑡𝑀)using𝑡𝑖; and 4) the prediction result
ℎ0(𝑡1,...,𝑡𝑀)is announced and takes effect. We use T𝑖to denote
the space of potential local embeddings of party 𝑖,i.e.,𝑡𝑖∈ T𝑖,
and use𝑡:=Î𝑀
𝑖=1𝑡𝑖to denote the profile of local embeddings
for all the parties. Similarly, we define the potential space of 𝑡as
T:=Î𝑀
𝑖=1T𝑖. We denote the distribution of local embeddings as
𝑡∼𝑓, and assume 𝑡𝑖∼𝑓𝑖is independent with 𝑡𝑗∼𝑓𝑗for𝑗≠𝑖.
Since the center has no control over the distributed local features,
a party𝑖might upload arbitrary local embeddings within T𝑖in
collaborative inference. We use 𝜎𝑖to denote the strategy of party 𝑖
when uploading the embeddings, with 𝜎𝑖(𝑡𝑖,𝑡′
𝑖)characterizes the
probability of uploading embedding 𝑡′
𝑖when the true embedding
is𝑡𝑖under strategy 𝜎𝑖, satisfying∫
𝑡′
𝑖∈T𝑖𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑑𝑡′
𝑖=1,∀𝑡𝑖∈T𝑖.
The strategy profile of all the parties is denoted as 𝜎:=(𝜎𝑖)𝑀
𝑖=1,
and the corresponding feasible space is denoted as Σ:=Î𝑀
𝑖=1Σ𝑖.
For convenience in notations, we would use subscript −𝑖to denote
the embedding profile or its feasible space for all the parties except
party𝑖,e.g.,𝑡−𝑖denotes the profile of uploaded local embeddings
except party 𝑖, andT−𝑖denotes the corresponding feasible space
of𝑡−𝑖. We use𝐼to denote the special truth-telling strategy, with
𝐼(𝑡𝑖,𝑡′
𝑖)=1when𝑡′
𝑖=𝑡𝑖, and equals 0otherwise.
Considering that the prediction result of the server would affect
the utility of the involved parties, we use 𝑢𝑖(ℎ0(𝑡′);𝑡𝑖)2to denote
the expected utility of party 𝑖when the uploaded embedding profile
is𝑡′and the true local embedding of party 𝑖is𝑡𝑖. Therefore, the
expected utility of party 𝑖when the strategy profile is 𝜎and the
server prediction function is ℎ0could be calculated as
𝑈ℎ0
𝑖(𝜎)=∫
𝑡∈T∫
𝑡′∈T𝑢𝑖(ℎ0(𝑡′);𝑡𝑖)𝜎(𝑡,𝑡′)𝑑𝑡′𝑓(𝑡)𝑑𝑡
with𝜎(𝑡,𝑡′):=Î𝑀
𝑖=1𝜎𝑖(𝑡𝑖,𝑡′
𝑖), and we may abbreviate ℎ0when the
context is clear.
In this work, we focus on the solution concept of Nash equilib-
rium to describe the stable state of strategic interactions between
parties. In our context that each party only has incomplete infor-
mation of the local embeddings, we consider a strategy profile
2We assume the utility only depends on the prediction result and the party’s own
true local embeddings, since the party could not access the other party’s true local
embeddings, and have to rely on her local information to make decisions.
3576KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
𝜎=(𝜎1,...,𝜎𝑀)to be a Bayesian Nash equilibrium (BNE) if
𝑈𝑖(𝜎)≥𝑈𝑖(𝜎′
𝑖,𝜎−𝑖),∀𝜎′
𝑖∈Σ𝑖,𝑖∈[𝑀].
In other words, when a strategy profile reaches BNE, none of the
parties could achieve larger expected utility through changing her
strategy, and we desire the truth-telling strategy 𝜎𝐼=(𝐼)𝑀
𝑖=1, to be
a BNE, such that the parties would be incentivized to upload the
true local embeddings and the validity of prediction is preserved.
While studying the BNE requires us to make assumptions on
distribution of embeddings, we may focus on one single round of
inference to enable detailed analysis of the strategic interactions
for specific embeddings (Section 4). Under this situation, when
each party chooses a certain local embedding (instead of a distribu-
tion over potential embeddings) for uploading, and the uploaded
embedding profile 𝑡′satisfies
𝑢𝑖(ℎ0(𝑡′
𝑖,𝑡′
−𝑖);𝑡𝑖)≥𝑢𝑖(ℎ0(𝑡′′
𝑖,𝑡′
−𝑖);𝑡𝑖),∀𝑡′′
𝑖∈T𝑖,𝑖∈[𝑀],
then𝑡′is called a pure-strategy Nash equilibrium (PNE). The PNE
in each round of inference is a stronger equilibrium notion than
the BNE over expectation of all the inference rounds, but is also
more difficult and sometimes infeasible to achieve.
4 FEDERATED RECOMMENDATION GAME
Since our discussions until now stay on an abstract level, we would
review a representative application of VFL system to illustrate the
potential strategic behaviors of involved parties more concretely.
As briefly discussed in Section 1, an arising application of VFL is
to aggregate user behavioral features from different organizations
to provide better recommendation results for users [ 16,38], which
we term as federated recommendation. The strategic incentives of
the organizations to manipulate the prediction results naturally
arise here, as each organization prefers to display content beneficial
for them. For example, some candidate items may originate from
one of the organizations or contain content relevant to its business
goal, which are more favorable for the organization to display.
To capture the key idea of this scenario, we assume each orga-
nization owns one unique item and aims to maximize the display
probability for this item [ 3,15] in federated recommendation, which
is a moderate amplification of the competition faced by collabo-
rating parties in practice. Following the literature studying games
between content creators in recommendation system [ 15], we focus
on the popular class of factorization-based recommendation algo-
rithms. That is, each organization uploads the computed local user
embedding𝑡𝑖, and the server would use the product of the averaged
user embedding and the item embedding 𝑏𝑖of the item owned by
organization 𝑖as the matching score between the current user and
item𝑖. Suppose the server adopts a softmax policy of matching
scores to display items, the expected utility (display probability) of
each organization could then be calculated as
𝑢𝑖 ℎ0(𝑡′)=exp(𝜏−1𝑠𝑖)Í
𝑗∈[𝑀]exp(𝜏−1𝑠𝑗),
where𝑠𝑖:=⟨𝑏𝑖,Í
𝑖𝑤𝑖𝑡′
𝑖⟩is the predicted matching score between
the item of party 𝑖and the current user, 𝑤𝑖denotes the aggregation
weight for local embedding of party 𝑖, and𝜏>0is the temperature
parameter to control exploration in recommendation. Since this
utility term does not depend on the true embeddings of parties, wedrop𝑡𝑖from the notation of 𝑢𝑖. We restrict∥𝑡𝑖∥2≤1, or otherwise
the party may report ∥𝑡𝑖∥2→∞ to increase its influence on the
aggregated embedding. We use 𝑏𝑖𝑘and𝑡𝑖𝑘to denote the 𝑘𝑡ℎentry
of𝑏𝑖and𝑡𝑖, respectively, and denote the number of dimensions of
𝑏𝑖,𝑡𝑖as𝑑.
In order to evaluate the consequences of strategic interactions
for a specific profile of item and user embeddings, we would analyse
the PNE resulted from the above utility function. If a PNE exists
in the game and truth-telling does not constitute a PNE, then it
indicates the parties would not conform to the truth-telling strategy,
but would instead follow the behavior characterized by the PNE(s).
Due to the limitation of space, the detailed proofs of our results in
Section 4 and 5 are presented in Appendix A.
Theorem 4.1. A PNE always exists in the federated recommenda-
tion game. Moreover, when 𝑀=2, for any𝑏1≠𝑏2and any positive
weights, the unique PNE in the corresponding federated recommenda-
tion game is
∀𝑘∈[𝑑]:𝑡1𝑘=−𝑡2𝑘=𝑏1𝑘−𝑏2𝑘
Í𝑑
𝑘′=1(𝑏1𝑘′−𝑏2𝑘′)21
2,(1)
Specifically, when 𝑤1=𝑤2=1
2, the display probabilities would be
𝑢1=𝑢2=1
2for any item embeddings 𝑏.
In Theorem 4.1, the general PNE existence result is proved
through showing the quasi-concavity of the utility function in the
current setting and applying the Debreu-Glicksberg-Fan existence
theorem. For the uniqueness result when 𝑀=2, we first show any
interior point with ∥𝑡𝑖∥<1could not be an equilibrium, then de-
rive the detailed expressions of PNE through Karush–Kuhn–Tucker
conditions. As we could observe from Theorem 4.1, despite the
existence of PNE, its uniqueness when 𝑀=2suggests the failure
of truth-telling strategy to be adopted by the organizations, and the
resulted recommendation outcomes are significantly skewed by the
utility-driven uploading of the involved parties. For the extreme
case of𝑀=2and𝑤1=𝑤2=1
2, the original properties of the user
and item embeddings are completely disregarded, and the parties
would get equal display chances for any user, leading the federated
recommendation to lose its original design purpose.
While the federated recommendation system display poor per-
formances against the parties’ strategic behaviors, similar situations
are not minority among the general VFL systems. Since the design
philosophy of the VFL system is to aggregate valuable distributed
features from each party to improve the prediction accuracy, the pre-
diction results need to depend on the precise embeddings uploaded
by the parties, and it is thus generally impossible for a standard VFL
algorithm to prevent strategic manipulation in inferences. As the
system designer, we should not assume that all the participants are
disinterested with the prediction results and refrain from exploiting
the vulnerabilities of the system, but instead need to establish effec-
tive manipulation-resistant mechanisms to mitigate the potential
risks brought by strategic behaviors in collaborative inference.
5 METHODOLOGY
In this section, we would introduce our design of distribution-based
penalty mechanism to prevent the strategic behaviors in collabora-
tive inference, along with the corresponding design considerations.
3577Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
5.1 Theoretical Basis
As indicated by the name of our mechanism, we adopt the distri-
bution of uploaded local embeddings as the criteria to detect and
penalize the strategic behaviors of involved parties. This choice is
motivated by the traditional economics literature [ 17] on linking a
sequence of public decision problems and restricting the number
of reported preferences to overcome incentive issues.
Intuitively, monitoring the distribution of embeddings could ef-
fectively prevent the parties’ strategic behaviors to always upload
local embeddings from a specific set which are known to have
higher probability of producing better inference results. To formal-
ize the guarantees provided by constraining distribution of em-
beddings as suggested by this intuition, we require two additional
conditions to facilitate rigorous theoretical proofs in our context:
independence in distribution of local embeddings, and the standard
en-ante Pareto efficiency [1] of the server prediction function.
Definition 5.1. A server prediction function ℎ0is ex-ante Pareto
efficient for the utility functions 𝑢=(𝑢𝑖)𝑀
𝑖=1and probability density
functions𝑓if there does not exist an alternative server prediction
functionℎ′
0such that
∫
𝑡∈T𝑢𝑖(ℎ0(𝑡);𝑡𝑖)𝑓(𝑡)𝑑𝑡≤∫
𝑡∈T𝑢𝑖(ℎ′
0(𝑡);𝑡𝑖)𝑓(𝑡)𝑑𝑡,∀𝑖,
or equivalently, 𝑈ℎ0
𝑖(𝐼𝑀)≤𝑈ℎ′
0
𝑖(𝐼𝑀), and the inequality is strict for
some𝑖.
In plain words, a server prediction function satisfies ex-ante
Pareto efficiency if there does not exist other server prediction func-
tion, such that every participant’s expected utility under true local
embeddings keeps non-decreasing, and at least one participant’s ex-
pected utility strictly increases. Typical examples for ex-ante Pareto
efficiency are the server prediction function always maximizes
the sum of participants’ utilities, or the server prediction function
uniquely optimizes the utility function for one of the participants.
As a more concrete example, in the federated recommendation
scenario, as long as the recommendation system always allocate
the full portion of display opportunities to the participants, then
any server prediction function utilized during this allocating pro-
cess would be ex-ante Pareto efficiency. This is because any server
prediction function always trivially maximize the sum of utility
of participants to be equal to one. As the distribution of uploaded
local embeddings is a key measure for us, we define the marginal
embedding distribution of a strategy 𝜎𝑖as𝑓𝜎𝑖
𝑖, with
𝑓𝜎𝑖
𝑖(𝑡′
𝑖)=∫
𝑡𝑖∈T𝑖𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑓𝑖(𝑡𝑖)𝑑𝑡𝑖.
Theorem 5.2. When each participant 𝑖’s strategy is restricted to
{𝜎𝑖:𝑓𝜎𝑖
𝑖=𝑓𝑖}and the server prediction function ℎ0is ex-ante Pareto
efficient on𝑢and𝑓, then the truth-telling strategy {𝐼𝑀}is a BNE.
By Theorem 5.2, under the independence and ex-ante Pareto
efficiency conditions, if each participant’s uploaded embeddings
are strictly constrained to align with their prior distributions, then
no participant could achieve higher utility through manipulating
the uploaded embeddings when all the other participants adopt
the truth-telling strategy. To prove Theorem 5.2, we note the in-
dependence in distributions of 𝑡𝑖would further indicate a relativeindependence in utility when the marginal distributions of all the
parties are constrained to align with the prior distributions, i.e., for
any strategy profile 𝜎satisfying𝑓𝑖=𝑓𝜎𝑖
𝑖for each participant,
𝑈𝑖(𝜎𝑖,𝜎−𝑖)=𝑈𝑖(𝜎𝑖,𝐼𝑀−1),∀𝑖.
As a result, when all the other participants adopt the truth-telling
strategy, their expected utilities are guaranteed to keep stable re-
gardless of the detailed reporting of a specific participant. If some
participant𝑖could realize a strict utility increment through chang-
ing her strategy, the ex-ante Pareto efficiency of the server predic-
tion function would be broken, thus creates a contradiction.
Although strong guarantees of preventing strategic behaviors
could be provided by the ex-ante Pareto efficiency, this condition
might not always hold in reality. For example, when server predic-
tion function is designed to optimize the accuracy of prediction
and does not prioritize maximizing the utility function of involved
parties, the condition of ex-ante Pareto efficiency would not hold.
Therefore, we would like to investigate the guarantees that con-
straining{𝜎𝑖:𝑓𝜎𝑖
𝑖=𝑓𝑖}could provide for more general server
prediction functions. Since we are now under much weaker as-
sumptions on ℎ0, we focus on the specific form of linear utility
functions
𝑢𝑖(ℎ0(𝑡′);𝑡𝑖)=𝑥ℎ0
𝑖(𝑡′)·𝑣𝑖(𝑡𝑖),
where𝑥𝑖is a function dependent on ℎ0. That is, we assume each
prediction result 𝑡′brings𝑥ℎ0
𝑖(𝑡′)unit of valuable item (utility
increase) to participant 𝑖, and the detailed amount of per-unit item
value depends on participant 𝑖’s true local embedding in the form
𝑣𝑖(𝑡𝑖). The linear utility function is widely-adopted in economics.
Theorem 5.3. Assume that the distributions of local embeddings
are discrete. For a strategic participant 𝑖with linear utility function,
her utility could not be increased by using any 𝜎𝑖≠𝐼when each
participant 𝑗’s strategy is constrained within the range {𝜎𝑗:𝑓𝜎𝑗
𝑗=
𝑓𝑗}, if∀𝑡1
𝑖,𝑡2
𝑖∈T𝑖with𝑣𝑖(𝑡1
𝑖)≥𝑣𝑖(𝑡2
𝑖),
E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡1
𝑖,𝑡−𝑖)]≥E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡2
𝑖,𝑡−𝑖)]. (2)
To ensure the constraint on marginal distribution is sufficient to
prevent dishonest utility increase, conditions (2) require a mono-
tone property between the expected allocation E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑖,𝑡−𝑖)]
and the per-unit value 𝑣𝑖(𝑡𝑖)brought by a user with local embedding
𝑡𝑖. That is, a user (or other subject of prediction task) who would
bring higher per-unit utility 𝑣𝑖(𝑡𝑖)for participant 𝑖, should simulta-
neously receive more expected allocation of items E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑖,𝑡−𝑖)]
after the overall evaluation 𝑥ℎ0
𝑖. Compared to the Pareto-efficiency
condition, the monotone conditions (2) characterize another kind
of coincidence between the server prediction function and the util-
ity function of the participant. When conditions (2) hold for each
participant𝑖, Theorem 5.3 would provide the same BNE guarantee
as in Theorem 5.2. We present Theorem 5.3 in the current form
to emphasize its provided guarantees could be flexibly applied to
each individual participant once the conditions hold, which keeps
relevant independent with other participants in comparison to the
ex-ante Pareto-efficiency condition in Theorem 5.2.
3578KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Algorithm 1: Distribution-Based Penalty Mechanism
Parameters: the test length 𝑚𝑖,𝑛𝑖, and a non-decreasing
penalty function 𝑘𝑖:[0,1]→ R+;
Oracles: A valid two-sample test 𝑇𝑖for𝑚𝑖uploaded
embeddings and 𝑛𝑖training embeddings, and a data
generator𝐺𝑖approximates the distribution of (training)
embeddings;
1Initialize historical rejection rate of two-sample test 𝑞𝑖=0;
2Initialize a cache 𝐶𝑖;
3while the collaborative inference is ongoing do
4 Add each uploaded embedding to 𝐶𝑖, and use the current
uploaded embedding for collaborative inference;
5 if𝐶𝑖is with length 𝑚𝑖then
6 Apply𝑇𝑖to embeddings in 𝐶𝑖and𝑛𝑖random
training embeddings;
7 Clear𝐶𝑖and update𝑞𝑖;
8 if𝑇𝑖rejects the null hypothesis then
9 for𝑗=1,...,𝑘𝑖(𝑞𝑖)do
10 Deactivate the embedding uploaded by
participant𝑖and use embeddings generated
by𝐺𝑖as substitute for each inference;
5.2 Distribution-Based Penalty Mechanism
Despite the promising guarantees provided by constraining the
distributions of uploaded embeddings to align with its prior dis-
tribution, it is infeasible for the server to exactly implement this
constraint for high-dimensional local embeddings uploaded by the
parties. Although we could regard the distribution of training em-
beddings as an effective approximate to the prior distribution, en-
forcing the involved parties to upload embeddings exactly match
with the training embeddings would lead the uploaded embeddings
to be substantially different from the true local embeddings, and
spoil the generalization capability of the VFL model. To adequately
harness the efficacy of local embedding distributions in preventing
strategic behaviors, we allow arbitrary local embeddings (within its
domain) to be uploaded by the parties, and adopt additional design
to reduce the incentives of conducting strategic behaviors through
penalizing the problematic distributions.
The formal process of the distribution-based penalty mechanism
is presented in Algorithm 1, which works individually for each
party𝑖. During the collaborative inference process, Algorithm 1
repeatedly collect embeddings uploaded by party 𝑖to distinguish
whether a sequence of 𝑚𝑖uploaded embeddings comes from the
same distribution of 𝑛𝑖(randomly sampled) training embeddings
with high probability. We leverage corresponding methods in sta-
tistical literature to realize this detection task, technically termed
astwo-sample tests. If the null hypothesis that the two groups of
samples come from the same distribution is rejected in the two-
sample test for party 𝑖, this indicates party 𝑖has likely manipulated
the uploaded local embeddings, and we would thus apply a penalty
period to party 𝑖. During the penalty period, each uploaded em-
bedding of participant 𝑖is deactivated and substituted with the
random embeddings (output by a generator 𝐺𝑖) to eliminate her
Deactivate  the uploaded 
embeddings for  𝑘𝑖𝑞𝑖
inferences𝑛𝑖 Training Embeddings
𝑚𝑖 Inference Embeddings
(Cached when the inference is 
ongoing)Two -
Sample 
Test
AcceptReject
Feed the uploaded 
embeddings to the 
server model  normallyFigure 2: Illustration of Distribution-Based Penalty Mecha-
nism for Party 𝑖
influences on the prediction results, and meanwhile leads her ex-
pected utility to decrease. The length of the current penalty period
is calculated by the historical rejection rate 𝑞𝑖and the pre-designed
penalty function 𝑘𝑖non-decreasing in 𝑞𝑖. In principle, we desire
the generator 𝐺𝑖to approximate the prior distribution of party 𝑖’s
local embeddings, which could be realized by randomly drawing
samples from the training embeddings.
As two-sample test is the key component to detect the consis-
tency of distribution in our penalty mechanism, the analysis of
our mechanism needs to depend on the properties of adopted two-
sample tests. Formally speaking, given two groups of samples 𝑋∼𝑝
with size𝑚and𝑌∼𝑞with size𝑛, a two-sample test is a statistical
test𝑇(𝑋,𝑌):𝑋𝑚×𝑋𝑛↦→{0,1}to distinguish between the null
hypothesisH0:𝑝=𝑞and the alternative hypothesis H𝐴:𝑝≠𝑞
[12]. Since the test is based on finite samples, it is possible that
errors would be made for some situations. By convention, a type I
error of a two-sample test occurs when the null hypothesis 𝑝=𝑞is
wrongly rejected based on the observed samples, even though the
data was generated with the same distribution. We define the type I
error rate for a two-sample test 𝑇as𝛼𝑇. Conversely, a type II error
occurs when the null hypothesis 𝑝=𝑞is accepted on the observed
samples, despite the fact 𝑝≠𝑞. We define the type II error rate of a
two-sample test 𝑇against a specific 𝑞≠𝑝as𝛽𝑇(𝑞).
In our distribution-based penalty mechanism, we require the
adopted two-sample test to satisfy 𝛽𝑇(𝑞)<𝛼𝑇for any𝑞≠𝑝,i.e.,
the acceptance rate of the null hypothesis is the highest when 𝑞=𝑝
and be strictly smaller for 𝑞≠𝑝. Since this is a fundamental require-
ment for a well-functioning two-sample test, we call a two-sample
test satisfying the above condition to be valid. Moreover, we also
require each participant’s expected utility to strictly decrease when
her true local embeddings are substituted with random embeddings
(drawn from her prior distribution), which is necessary to ensure
the penalty period could effectively reduce the expected utility of
a participant. We term the problem case (consist of 𝑢,𝑓andℎ0)
satisfying this utility decrement condition for each party to be feasi-
ble, which could be verified through simulations in practice. When
the above typical conditions hold, the proposed distribution-based
penalty mechanism (Algorithm 1) is able to inherit the guarantees
provided by strictly constraining the distribution (Section 5.1) in
an asymptotic sense.
Theorem 5.4. For any feasible problem case with valid two-sample
tests𝑇𝑖, supposeℎ0is ex-ante Pareto efficient on 𝑢and𝑓, and𝐺𝑖∼𝑓𝑖
for each participant 𝑖, then we could find some penalty functions 𝑘𝑖(·)
3579Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
such that the expected per-round utility of truth-telling strategy {𝐼𝑀}
converges to a BNE with the increase of inference rounds under the
penalty-enabled server prediction function ℎ∗
0.
Under the stated conditions, Theorem 5.4 guarantees no partic-
ipant could obtain higher expected per-round utility than truth-
telling as the inference proceeds, supposing all the other parties
adopt the truth-telling strategy. The performance guarantee of our
distribution-based penalty mechanism is established on the basis of
results in Section 5.1. In principle, because no party could achieve
larger utility through deviating to a strategy with the same marginal
distribution (Theorem 5.2), the remaining chances to improve utility
fall on the strategies with different marginal distributions. However,
by the validity of two-sample tests, such kinds of strategies would
result in larger historical rejection rate 𝑞𝑖and longer penalty period,
thus also brings lower utility in the long term. Whilst Theorem
5.4 is formulated based on Theorem 5.2, an alternative result with
the ex-ante Pareto efficiency condition replaced by conditions (2)
could be formed based on Theorem 5.3. Though our theoretical re-
sults rely on conditions such as Pareto-efficiency and independent
distribution, the principal idea of our design, i.e., monitoring the
distribution of uploaded embeddings, is broadly helpful in restrain-
ing the range of strategic behaviors in collaborative inference, even
if the theoretical conditions are not strictly satisfied. This is also
demonstrated by our experimental results in Section 6.
In both Algorithm 1 and Theorem 5.4, we do not characterize the
detailed form of the penalty function 𝑘𝑖and the choice of sample
length𝑚𝑖,𝑛𝑖in two-sample tests, but instead leave it flexible to
accommodate the need of various scenarios. Choosing a penalty
function𝑘𝑖(𝑞𝑖)grows faster with 𝑞𝑖could provide stronger guar-
antee against strategic behaviors, but would simultaneously bring
higher risk for honest parties when the number of conducted two-
sample test is small and 𝑞𝑖has large variance. A similar tradeoff
exists for the choice of test length 𝑚𝑖and𝑛𝑖. While a larger 𝑚𝑖
means lower error rate for two-sample tests, a smaller 𝑚𝑖allows to
conduct more two-sample tests and get a stable 𝑞𝑖, which might be
preferred when the number of total inference rounds is small. To
choose appropriate mechanism parameters in practice, the server
could conduct simulations on training embeddings to estimate the
performances of the considered mechanism.
6 EXPERIMENTS
In the experiments, we aim to validate and investigate the following
questions from an empirical view: (1) whether the considered strate-
gic behaviors in collaborative inference are implementable and
could bring the party substantially higher utility; (2) whether the
proposed distribution-based penalty mechanism could effectively
reduce the involved parties’ incentives to conduct such strategic
behaviors for practical datasets that not strictly satisfy the theoreti-
cal assumptions; and (3) how to set the parameters in the penalty
mechanism to achieve good performances in practice.
6.1 Experimental Setup
Datasets and VFL Model We conduct experiments on two pub-
lic datasets, Criteo andAvazu, with the task of click-through-rate
(CTR) prediction. We assume there are two parties involved in VFL,
with each party owning half of the features partitioned by theirsequence in dataset. To validate the performances of our design
for VFL models trained with different amount of data, we draw
1,000,000 samples to train and test the VFL model for Avazu, while
the full dataset is available for Criteo. The training and testing
sets are divided with proportion 9:1 for both the datasets. After
the VFL model has been determined, we apply our mechanism on
𝑁=100,000samples (inference rounds) drawn from the testing
set. We adopt the fully-connected neural network3(FCNN) for both
the parties and the server, with 4 layers for the parties locally and
3 layers for the server, and the sparse features are first processed
with an embedding layer before feeding into the local FCNN. The
intermediate embeddings uploaded by each party are with dimen-
sion 40, which are concatenated to feed into the server network.
Strategic Settings We assume that there exists one strategic party
in the system, which is without loss of generality as our mechanism
works individually for each party. We consider the utility function
of the strategic party to be the form 𝑢𝑒𝑥𝑝=Í
𝑗∈[𝑁]𝑐𝑡𝑟𝑗/𝑁or
𝑢𝑐𝑙𝑖𝑐𝑘=Í
𝑗∈[𝑁](𝑐𝑡𝑟𝑗·𝑙𝑎𝑏𝑒𝑙𝑗)/𝑁, where𝑐𝑡𝑟𝑗denotes the predicted
CTR of the 𝑗𝑡ℎtest sample, and 𝑙𝑎𝑏𝑒𝑙𝑗denotes its true label. As-
suming that the display opportunity gained by the strategic party
would be equal to the predicted CTR, these two utility functions
represent the typical goal of obtaining more exposure opportunities
and more expected clicks in recommendation.
Manipulation Strategies
•Label-based strategy: Considering that the local features of the
training samples with positive label are likely to increase the pre-
diction of CTR, the label-based strategy samples a local embedding
from the training embeddings with positive labels to upload in each
inference round. This label-based strategy is straightforward to
implement in practice, which only requires the party to know a set
of samples with positive labels.
•Omniscient strategy: In the omniscient strategy, we assume the
strategy of the party is derived by optimizing the total predicted
CTR under ℓ2-regularization (applied to the difference between
the original and manipulated embeddings), using the omniscient
knowledge of local embeddings from both parties. To avoid the
less meaningful case that the party extremely increases the scale of
embeddings to dominantly create false-positive cases, we choose a
regularization constant to ensure the resulting strategy achieves a
sufficiently higher utility at an appropriate level. The optimization
is performed using the stochastic gradient descent method.
•Probabilistic Mixtures : To validate our mechanisms against vari-
ous potential strategies, we consider the probabilistic mixtures of
the above two strategies with the true local embeddings, e.g., a
strategy with mixture probability 0.1would report the true embed-
dings with 90%probability, and report according to the label-based
(omniscient) strategy in the remaining 10%probability.
Mechanism Implementation When implementing Algorithm 1,
we adopt the kernel two-sample test based on deep learning [ 21]
with the test length for the training and inference embeddings
set to be equal, i.e.,𝑛𝑖=𝑚𝑖. To reduce the variance of historical
rejection rate 𝑞𝑖in implementation, we postpone all the penalties
to the end of the inference stage, such that once the remaining
3Since the design of our mechanism only concerns the local embeddings uploaded by
the participants, the detailed structure of VFL model would not induce great impacts
on the trend of performances of the proposed mechanism.
3580KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
Table 1: Original Utilities for Different Strategies
Criteo True Omniscient Label-Based Random
𝑢𝑒𝑥𝑝 0.2424 0.3591 0.3031 0.2345
𝑢𝑐𝑙𝑖𝑐𝑘 0.1024 0.1372 0.1048 0.0836
Avazu True Omniscient Label-Based Random
𝑢𝑒𝑥𝑝 0.1389 0.2033 0.2761 0.1333
𝑢𝑐𝑙𝑖𝑐𝑘 0.0392 0.0595 0.0478 0.0229
inference rounds are less than the total penalty length calculated
by the latest rejection rate, the party would be penalized in all
the remaining rounds. We choose penalty function in the linear
form𝑘𝑖(𝑞𝑖)=𝑐·𝑛𝑖·𝑞𝑖,i.e., the penalty length for each rejection
is the party’s rejection rate times the current test length and a
pre-determined penalty constant 𝑐. The detailed settings of the
penalty constant 𝑐and the test length 𝑛𝑖in the mechanism would
be characterized for each set of experiments.
6.2 Experimental Results
Original Utilities of Different Strategies The original utilities
of different strategies without the penalty mechanism are presented
in Table 1. As we can observe, the omniscient strategy achieves
evident higher utility on both the utility functions for two datasets,
though it only applies small perturbations on true embeddings. The
label-based strategy also achieves evident utility increase except for
𝑢𝑐𝑙𝑖𝑐𝑘 in Criteo dataset. The reason might come from the limited
influences of the party’s local features on the prediction result and
the relatively high proportion of positive samples in Criteo dataset.
Since we would substitute the party’s original embeddings with
randomly sampled training embeddings as penalty, we also validate
the utility of such random “strategy” on the datasets. We find that
the random strategy would lead to a lower utility for 𝑢𝑐𝑡𝑟, but
achieve a similar (though still lower) utility for 𝑢𝑐𝑙𝑖𝑐𝑘 compared
with the true embeddings. In other words, using random strategy
would not lead the total exposure of the party to decrease, despite
the resulted mismatch between true label and predicted CTR. As a
result, for parties with utility function 𝑢𝑒𝑥𝑝, we could hardly reduce
the utility obtained by strategic manipulations to be smaller than
the utility obtained by true embeddings, and what we could achieve
is to guide the two utilities to be close enough.
Performances of Distribution-Based Penalty Mechanism To
observe the detailed performances of the proposed penalty mecha-
nism, we conduct a set of experiments (Figure 3) that demonstrate
the change in the utility of strategic party when the penalty mecha-
nism is adopted. Due to the different characteristics of two datasets,
we choose𝑛𝑖=1800 for Criteo dataset, and 𝑛𝑖=600for Avazu
dataset, with both 𝑐=8. As could be observed in Figure 3, the
utilities obtained by different probabilistic mixtures of the omni-
scient (OM) and label-based (LB) strategies are largely reduced to
be similar or less than the utility obtained by truthfully uploading
the embeddings under the penalty mechanism, regardless of the
original utilities obtained by those strategies, which demonstrates
the effectiveness of our mechanism in diminishing the incentives
of parties to adopt strategies other than truthful uploading.
In the trend of penalized utilities for OM and LB strategies on
Avazu dataset, we could observe a slow utility growth after the
	0.24	0.29	0.34	0.39
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty(a)𝑢𝑒𝑥𝑝in Criteo Dataset
	0.08	0.1	0.12	0.14	0.16
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty (b)𝑢𝑐𝑙𝑖𝑐𝑘 in Criteo Dataset
	0.12	0.16	0.2	0.24	0.28
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty
(c)𝑢𝑒𝑥𝑝in Avazu Dataset
	0.02	0.03	0.04	0.05	0.06	0.07
	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1Utility
Mixture	ProbabilityOM-original
OM-penalty
LB-original
LB-penalty (d)𝑢𝑐𝑙𝑖𝑐𝑘 in Avazu Dataset
Figure 3: Comparison of Utilities obtained by Probabilistic
Mixtures for Label-Based and Omniscient Strategies before
and after the penalty mechanism is enabled, with 𝑛𝑖=1800,
𝑐=8for Criteo Dataset, and 𝑛𝑖=600,𝑐=8for Avazu Dataset
mixture probability exceeds 0.4. This is because the mixture strategy
at this point has been penalized in most of the inference rounds, and
the utility increase comes entirely from the beginning inferences
round for conducting the essential two-sample tests. For Criteo
dataset, the utilities of different mixture strategies display slight
fluctuations with the increase of mixture probability, which might
due to the relatively high variance of two-sample tests under a
smaller number of tests for Criteo.
Another important metric we need to observe is the total penalty
length received by true embeddings, as immoderate penalty on
a truthful party can significantly degrade the overall prediction
performances of VFL system when it is not controlled at a relatively
low level. For parameters adopted in Figure 3, the averaged penalty
length received by true embeddings are 5,680for Criteo dataset
and5,560for the Avazu dataset, which is a small and acceptable
penalty length compared to the 100,000samples in total.
Influences of Mechanism Parameters The set of parameters
we adopted in Figure 3 are actually not the deliberately fine-tuned
ones to achieve the best performances. When testing the different
mechanism parameters, we find a broad set of parameters could
achieve satisfying effects around the parameters we present in
Figure 3. Therefore, instead of presenting the similar performances
achieved by successful mechanism parameters, we would like to
demonstrate the importance of tailoring the mechanism parameters
based on the characteristics of datasets and adopted two-sample
tests. As a striking instance, simply applying a small test length to
Criteo dataset and a large test length to Avazu dataset as opposed
to Figure 3, e.g., exchanging the 𝑛𝑖parameters for two datasets,
would largely degrade the mechanism performances. To help with
the evaluation of the mechanism’s overall performances against the
strategic behaviors, we define the metric of utility approximate ratio
𝛼to be the largest ratio between the utility obtained by a dishonest
strategy and the utility of true embedding among all the considered
strategies in Figure 3. We regard an utility approximate ratio less
3581Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
	1	1.05	1.1	1.15	1.2	1.25
	4	6	8	10	12	14	16	18	20α
Penalty	Constantuexp
uclick
(a)𝑛𝑖=600with Different
Penalty Constant 𝑐∈[4,20]in
Criteo Dataset
	0.7	0.8	0.9	1	1.1	1.2
	300 	900	1500	2100	2700α
Test	Lengthuexp
uclick(b)𝑐=8with Different Test
Length𝑛𝑖∈[300,2700]in Avazu
Dataset
Figure 4: Utility Approximate Ratio 𝛼for Failure Cases in
Criteo and Avazu Dataset
than 1.1to be acceptable for 𝑢𝑒𝑥𝑝and 1.05to be acceptable for
𝑢𝑐𝑙𝑖𝑐𝑘, which are both satisfied by the experiments in Figure 3.
In Figure 4a, we report the utility approximate ratios of test
length𝑛𝑖=600with different penalty constant 𝑐∈ [4,20]on
Criteo dataset. We can observe that the utility approximate ratio
fails to satisfy the required standard in most cases and gradually
increases with the penalty constant, though with fluctuations. This
is caused by the poor performances of two-sample test with 𝑛𝑖=600
when distinguishing the variants of LB strategies on Criteo dataset,
such that both the true embeddings and the LB variants receives
small penalties. In Figure 4b, we report the utility approximate
ratios of𝑐=8with different test lengths 𝑛𝑖∈[300,2700]on Avazu
dataset. Despite the seemingly satisfying results of 𝛼, the total
penalty received by the true embeddings has generally exceeded
15,000 rounds after 𝑛𝑖≥1500, and the relatively low 𝛼comes
from applying large penalties for both the truth-telling strategy
and alternative strategies. For its potential causes, the adopted
two-sample test might have relatively high variances on the Avazu
dataset and requires more tests to make 𝑞𝑖stable when computing
the penalty length, which could not be provided by 𝑛𝑖≥1500 under
the current number of inference samples.
Simulations of Multi-Party Setup Since our mechanism works
independently for each party irrespective of other parties’ behav-
iors, we can simulate the performances of our mechanism for the
multi-party setup under the two-party setup. In detail, for any party
in multi-party setup, we could regard all the other parties as a “gi-
ant” party and run our mechanism only on the local embeddings
uploaded by the considered party. Based on this equivalence prop-
erty, we simulate the 3-party and 4-party setup on Avazu dataset
with two parties, by assuming the strategic party owning (approxi-
mately) 1/3 and 1/4 of the features. We adopt the same mechanism
parameters as in Figure 3c and 3d, and the results are presented
in Table 2. We could observe a significant decrease in the utilities
of the OM and LB strategies under our penalty mechanism (OM-P
and LB-P) compared to their utilities in the vanilla VFL system. In
contrast, the utilities of uploading true local embeddings remained
largely unaffected under the penalty mechanism (True and True-
P), demonstrating the efficacy of our mechanism in various VFL
settings. The discrepancies between utilities obtained by OM and
LB strategies in Table 1 and 2 are likely due to the differences in
the specific features owned by the strategic party and their vary-
ing significance in affecting the final prediction result, and it is
not necessarily the case that owning more features would enableTable 2: Changes in utilities of different strategies when the
penalty mechanism is enabled and the strategic party owns
1/3 and 1/4 of the features for Avazu dataset
1/3 features True OM LB True-P OM-P LB-P
𝑢𝑒𝑥𝑝 0.1345 0.1449 0.1483 0.1343 0.1278 0.1283
𝑢𝑐𝑙𝑖𝑐𝑘 0.0377 0.0420 0.0371 0.0375 0.0328 0.0323
1/4 features True OM LB True-P OM-P LB-P
𝑢𝑒𝑥𝑝 0.1390 0.1529 0.1542 0.1389 0.1349 0.1351
𝑢𝑐𝑙𝑖𝑐𝑘 0.0376 0.0431 0.0369 0.0376 0.0333 0.0326
the party to achieve larger dishonest utility increase when similar
manipulation strategies are adopted (Table 2).
7 CONCLUSION
In this work, we consider the strategic behaviors in collaborative
inference for vertical federated learning, where the parties could
manipulate the uploaded local embeddings to change the inference
results and maximize their own utilities. We model the strategic in-
teractions between parties for a representative federated recommen-
dation application, and our analysis reveals the adverse effects of
the considered strategic behaviors. Specifically, we propose a class
of distribution-based penalty mechanism to prevent such strategic
behaviors. The proposed mechanism works through applying sta-
tistical two-sample tests to distinguish the deviation in embedding
distributions and penalizing the parties based on the test results,
whose performance is theoretically demonstrated. The experimen-
tal results validate the effectiveness of the proposed mechanism in
terms of preventing the considered strategic behaviors.
ACKNOWLEDGEMENT
This work was supported in part by National Key R&D Program
of China (No. 2022ZD0119100), in part by China NSF grant No.
62322206, 62132018, U2268204, 62025204, 62272307, 62372296. The
opinions, findings, conclusions, and recommendations expressed
in this paper are those of the authors and do not necessarily reflect
the views of the funding agencies or the government.
REFERENCES
[1]Kenneth J Arrow. 2012. Social choice and individual values. Vol. 12. Yale University
Press.
[2]Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How to backdoor federated learning. In Proceedings of the 23rd
International Conference on Artificial Intelligence and Statistics. PMLR, 2938–2948.
[3]Omer Ben-Porat and Moshe Tennenholtz. 2018. A game-theoretic approach to
recommendation systems with strategic content providers. In Proceedings of the
32nd International Conference on Neural Information Processing Systems. Curran
Associates Inc., Red Hook, NY, USA, 1118–1128.
[4]Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019. Analyzing federated learning through an adversarial lens. In Proceedings of
the 36th International Conference on Machine Learning. PMLR, 634–643.
[5]Chaochao Chen, Jun Zhou, Li Wang, Xibin Wu, Wenjing Fang, Jin Tan, Lei Wang,
Alex X Liu, Hao Wang, and Cheng Hong. 2021. When homomorphic encryption
marries secret sharing: Secure large-scale sparse logistic regression and appli-
cations in risk control. In Proceedings of the 27th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. ACM, 2652–2662.
[6]Yong Cheng, Yang Liu, Tianjian Chen, and Qiang Yang. 2020. Federated learning
for privacy-preserving AI. Commun. ACM 63, 12 (2020), 33–36.
[7]Sen Cui, Jian Liang, Weishen Pan, Kun Chen, Changshui Zhang, and Fei Wang.
2022. Collaboration Equilibrium in Federated Learning. In Proceedings of the 28th
3582KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
ACM SIGKDD Conference on Knowledge Discovery & Data Mining. ACM, New
York, NY, USA, 241–251.
[8]Yongheng Deng, Feng Lyu, Ju Ren, Yi-Chao Chen, Peng Yang, Yuezhi Zhou, and
Yaoxue Zhang. 2021. FAIR: Quality-aware federated learning with precise user
incentive and model aggregation. In Proceedings of the 40th IEEE Conference on
Computer Communications. IEEE, 1–10.
[9]Kate Donahue and Jon Kleinberg. 2021. Model-sharing games: Analyzing feder-
ated learning under voluntary participation. In Proceedings of the AAAI Conference
on Artificial Intelligence. AAAI Press, 5303–5311.
[10] Kate Donahue and Jon Kleinberg. 2021. Optimality and stability in federated
learning: A game-theoretic approach. In Proceedings of the 35th International
Conference on Neural Information Processing Systems. Curran Associates Inc., Red
Hook, NY, USA, 1287–1298.
[11] Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen, Jingzheng Wu, Shanqing
Guo, Jun Zhou, Alex X Liu, and Ting Wang. 2022. Label inference attacks against
vertical federated learning. In 31st USENIX Security Symposium (USENIX Security
22). USENIX Association, 1397–1414.
[12] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and
Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine
Learning Research 13 (2012), 723–773.
[13] Jingoo Han, Ahmad Faraz Khan, Syed Zawad, Ali Anwar, Nathalie Baracaldo,
Yi Zhou, Feng Yan, and Ali Raza Butt. 2022. TIFF: Tokenized Incentive for
Federated Learning. In Proceedings of the IEEE 15th International Conference on
Cloud Computing. IEEE, 407–416.
[14] Rafael Hortala-Vallve. 2010. Inefficiencies on linking decisions. Social Choice and
Welfare 34, 3 (2010), 471–486.
[15] Jiri Hron, Karl Krauth, Michael Jordan, Niki Kilbertus, and Sarah Dean. 2022. Mod-
eling content creator incentives on algorithm-curated platforms. In Proceedings
of the 11th International Conference on Learning Representations.
[16] Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou. 2019. FDML: A
collaborative machine learning framework for distributed features. In Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. ACM, 2232–2240.
[17] Matthew O Jackson and Hugo F Sonnenschein. 2007. Overcoming incentive
constraints by linking decisions. Econometrica 75, 1 (2007), 241–257.
[18] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al .2021. Advances and open problems in federated learning.
Foundations and Trends in Machine Learning 14, 1–2 (2021), 1–210.
[19] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Vir-
ginia Smith, and Chong Wang. 2022. Label Leakage and Protection in Two-party
Split Learning. In Proceedings of the 10th International Conference on Learning
Representations.
[20] Wenjie Li, Qiaolin Xia, Hao Cheng, Kouyin Xue, and Shu-Tao Xia. 2022. Vertical
semi-federated learning for efficient online advertising. arXiv preprint (2022).
https://arxiv.org/abs/2209.15635
[21] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J.
Sutherland. 2020. Learning Deep Kernels for Non-Parametric Two-Sample Tests.
InProceedings of the 37th International Conference on Machine Learning. PMLR,
6316–6326.
[22] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye
Ouyang, Ya-Qin Zhang, and Qiang Yang. 2024. Vertical Federated Learning:
Concepts, Advances, and Challenges. IEEE Transactions on Knowledge and Data
Engineering 36, 7 (2024), 3615–3634.
[23] Yang Liu, Zhihao Yi, and Tianjian Chen. 2020. Backdoor attacks and defenses in
feature-partitioned collaborative learning. arXiv preprint (2020). https://arxiv.
org/abs/2007.03608
[24] Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi. 2021. Feature
inference attack on model predictions in vertical federated learning. In Proceedings
of the 37th IEEE International Conference on Data Engineering. IEEE, 181–192.
[25] Hongtao Lv, Zhenzhe Zheng, Tie Luo, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei
Jia, and Chengfei Lv. 2021. Data-free evaluation of user contributions in federated
learning. In 19th International Symposium on Modeling and Optimization in Mobile,
Ad hoc, and Wireless Networks. IFIP, 81–88.
[26] Hitoshi Matsushima, Koichi Miyazaki, and Nobuyuki Yagi. 2010. Role of linking
mechanisms in multitask agency with hidden information. Journal of Economic
Theory 145, 6 (2010), 2241–2259.
[27] Lokesh Nagalapatti and Ramasuri Narayanam. 2021. Game of gradients: Miti-
gating irrelevant clients in federated learning. In Proceedings of the 35th AAAI
Conference on Artificial Intelligence. AAAI Press, 9046–9054.
[28] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In 2019 IEEE Symposium on Security and
Privacy. IEEE, 739–753.
[29] Qi Pang, Yuanyuan Yuan, Shuai Wang, and Wenting Zheng. 2023. ADI: Adver-
sarial Dominating Inputs in Vertical Federated Learning Systems. In 2023 IEEE
Symposium on Security and Privacy. IEEE Computer Society, Los Alamitos, CA,
USA, 1875–1892.[30] Agustín Santos, Antonio Fernández Anta, José A Cuesta, and Luis López Fernán-
dez. 2016. Fair linking mechanisms for resource allocation with correlated player
types. Computing 98 (2016), 777–801.
[31] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan
Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware
model rewards. In Proceedings of the 37th International Conference on Machine
Learning. PMLR, 8927–8936.
[32] Behnaz Soltani, Yipeng Zhou, Venus Haghighi, and John C. S. Lui. 2023. A
Survey of Federated Evaluation in Federated Learning. In Proceedings of the
32th International Joint Conference on Artificial Intelligence. International Joint
Conferences on Artificial Intelligence Organization, 6769–6777.
[33] Ming Tang and Vincent WS Wong. 2021. An incentive mechanism for cross-silo
federated learning: A public goods perspective. In Proceedings of the 40th IEEE
Conference on Computer Communications. IEEE, 1–10.
[34] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. 2020. Data poi-
soning attacks against federated learning systems. In 25th European Symposium
on Research in Computer Security. Springer, 480–501.
[35] Róbert F Veszteg. 2015. Linking decisions with standardization. Studies in
Microeconomics 3, 1 (2015), 35–48.
[36] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. 2020. Attack
of the tails: Yes, you really can backdoor federated learning. In Proceedings of the
34th International Conference on Neural Information Processing Systems. Curran
Associates Inc., Red Hook, NY, USA, 16070–16084.
[37] Penghui Wei, Hongjian Dou, Shaoguo Liu, Rongjun Tang, Li Liu, Liang Wang, and
Bo Zheng. 2023. FedAds: A Benchmark for Privacy-Preserving CVR Estimation
with Vertical Federated Learning. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval . ACM,
3037–3046.
[38] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. 2022.
FedCTR: Federated Native Ad CTR Prediction with Cross-platform User Behavior
Data. ACM Transactions on Intelligent Systems and Technology 13, 4 (2022), 62:1–
62:19.
[39] Chulin Xie, Keli Huang, Pin Yu Chen, and Bo Li. 2020. DBA: Distributed Back-
door Attacks against Federated Learning. In Proceedings of the 8th International
Conference on Learning Representations.
[40] Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. 2022. A
survey of incentive mechanism design for federated learning. IEEE Transactions
on Emerging Topics in Computing 10, 2 (2022), 1035–1044.
[41] Meng Zhang, Ermin Wei, and Randall Berry. 2021. Faithful edge federated learn-
ing: Scalability and privacy. IEEE Journal on Selected Areas in Communications
39, 12 (2021), 3790–3804.
A PROOFS OF RESULTS
For convenience in notations, we may abbreviate the server predic-
tion function ℎ0in notations when the context is clear.
A.1 Proof for Theorem 4.1
Proof. (Existence of PNE) To show the general existence of PNE,
we apply the Debreu-Glicksberg-Fan PNE existence theorem. That
is, a PNE exists in a game if the following conditions are satisfied:
(1) the strategy space of each player is compact and convex; and (2)
the utility function of each player is continuous and quasi-concave
in her strategy. By our definition of 𝑡𝑖, it is clear the strategy space
is compact and convex, and 𝑢𝑖(𝑡𝑖,𝑡−𝑖)is continuous in 𝑡𝑖. It remains
to demonstrate 𝑢𝑖(𝑡𝑖,𝑡−𝑖)is quasi-concave in 𝑡𝑖.
Since𝑢𝑖(𝑡𝑖,𝑡−𝑖)could be fully defined by 𝑠=(𝑠1,𝑠2,...,𝑠𝑀),
and𝑠could be obtained by a linear transformation from 𝑡𝑖=
(𝑡𝑖1,𝑡𝑖2,...,𝑡𝑖𝑑)regarding𝑡−𝑖as constants, we only need to show
𝑢𝑖(𝑡𝑖,𝑡−𝑖)being quasi-concave on 𝑠. Recall
𝑢𝑖(𝑡𝑖,𝑡−𝑖)=exp(𝜏−1𝑠𝑖)Í
𝑗∈[𝑀]exp(𝜏−1𝑠𝑗),
suppose we have two points 𝑠and𝑠′, such that 𝑢𝑖(𝑠) ≥𝑎and
𝑢𝑖(𝑠′) ≥𝑎. By definition of 𝑢𝑖, we would have exp(𝜏−1𝑠𝑖) ≥
𝑎
1−𝑎·Í
𝑗≠𝑖exp(𝜏−1𝑠𝑗)
. Taking logarithm on both sides, we have
𝜏−1𝑠𝑖≥ln𝑎
1−𝑎+lnÍ
𝑗≠𝑖exp(𝜏−1𝑠𝑗)
.Similarly,𝜏−1𝑠′
𝑖≥ln𝑎
1−𝑎+
3583Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
lnÍ
𝑗≠𝑖exp(𝜏−1𝑠′
𝑗)
.For any𝜆∈ (0,1), we could thus obtain
𝜏−1(𝜆𝑠𝑖+(1−𝜆)𝑠′
𝑖) ≥ ln𝑎
1−𝑎+𝜆lnÍ
𝑗≠𝑖exp(𝜏−1𝑠𝑗)
+(1−
𝜆)lnÍ
𝑗≠𝑖exp(𝜏−1𝑠′
𝑗)
.On the other hand, by Holder’s inequality,
ln©­­
«©­
«∑︁
𝑗≠𝑖exp(𝜏−1𝑠𝑗)ª®
¬𝜆
·©­
«∑︁
𝑗≠𝑖exp(𝜏−1𝑠′
𝑗)ª®
¬1−𝜆
ª®®
¬
≥ln©­
«∑︁
𝑗≠𝑖exp
𝜏−1(𝜆𝑠𝑗+(1−𝜆)𝑠′
𝑗)ª®
¬,
Therefore, for 𝑠=𝜆𝑠+(1−𝜆)𝑠′,𝜏−1𝑠𝑖≥ln𝑎
1−𝑎+lnÍ
𝑗≠𝑖exp 𝜏−1𝑠𝑖
,
which indicates 𝑢𝑖(𝑠)≥𝑎and the quasi-concavity of 𝑢𝑖(𝑠). □
Proof. (Unique PNE when 𝑀=2) We would finish the proof
by showing the following statements: When 𝑛=2, (1) no interior
point of𝑡1or𝑡2would be a PNE, i.e.,∥𝑡1∥=∥𝑡2∥=1; and (2) fix
any𝑡3−𝑖, the best response 𝑡𝑖must be obtained by scaling vector
𝑏𝑖𝑘−𝑏(3−𝑖)𝑘
𝑘∈[𝑑]with a constant. Note that party 3−𝑖denotes
the other party besides 𝑖. Fixing𝑡−𝑖, the optimization problem faced
by party𝑖could be formulated as
max𝑡𝑖𝑓𝑖(𝑡𝑖)=exp(𝜏−1𝑠𝑖)Í
𝑗∈[𝑛]exp(𝜏−1𝑠𝑗),
s.t. 𝑠𝑗=𝑑∑︁
𝑘=1𝑛∑︁
𝑖′=1𝑡𝑖′𝑘𝑤𝑖′𝑏𝑗𝑘,∀𝑗∈[𝑛],
𝑑∑︁
𝑘=1𝑡2
𝑖𝑘≤1.(3)
Using multi-variable chain rule, we could obtain
𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑡𝑖𝑘=𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑠𝑖·𝜕𝑠𝑖
𝜕𝑡𝑖𝑘+∑︁
𝑗≠𝑖𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑠𝑗·𝜕𝑠𝑗
𝜕𝑡𝑖𝑘
=𝑤𝑖exp(𝜏−1𝑠𝑖)
𝜏Í
𝑗′∈[𝑛]exp(𝜏−1𝑠′
𝑗)2·∑︁
𝑗≠𝑖exp(𝜏−1𝑠𝑗)(𝑏𝑖𝑘−𝑏𝑗𝑘).
By definition of PNE, 𝑡𝑖must be the solution to problem (3)
supposing𝑡−𝑖are fixed. To characterize the conditions of those best-
response𝑡𝑖, consider the following Karush–Kuhn–Tucker (KKT)
conditions induced by problem (3).
 
𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑡𝑖𝑘+2𝜆𝑡𝑖𝑘=0,∀𝑘∈[𝑑]
𝜆Í𝑑
𝑘=1𝑡2
𝑖𝑘−1
=0
Í𝑑
𝑘=1𝑡2
𝑖𝑘≤1
𝜆≤0(4)
We would now claim that 𝜆≠0for𝑛=2and𝑏1≠𝑏2. If𝜆=0,
we require𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑡𝑖𝑘=0,∀𝑘∈[𝑑].However, when 𝑛=2, the termÍ
𝑗≠𝑖exp(𝜏−1𝑠𝑗)(𝑏𝑖𝑘−𝑏𝑗𝑘)would reduce to exp(𝜏−1𝑠3−𝑖)(𝑏𝑖𝑘−
𝑏(3−𝑖)𝑘),which could not be 0for every𝑘for𝑏1≠𝑏2. As the term
𝑤𝑖exp(𝜏−1𝑠𝑖)
𝜏Í
𝑗′∈[𝑛]exp(𝜏−1𝑠′
𝑗)2is strictly positive,𝜕𝑓𝑖(𝑡𝑖)
𝜕𝑡𝑖𝑘=0could not hold
for every𝑘, thus𝜆could not be 0, and we must haveÍ𝑑
𝑘=1𝑡2
𝑖𝑘=1to satisfy conditions (4). As 𝜆≠0, we could represent each entry
of𝑡𝑖as𝑡𝑖𝑘=𝐶𝑖·Í
𝑗≠𝑖exp(𝜏−1𝑠𝑗)(𝑏𝑖𝑘−𝑏𝑗𝑘)where𝐶𝑖=−1
2𝜆·
𝑤𝑖exp(𝜏−1𝑠𝑖)
𝜏Í
𝑗′∈[𝑛]exp(𝜏−1𝑠′
𝑗)2>0is the same constant for all 𝑡𝑖𝑘. When
𝑛=2, we further denote 𝐶′
𝑖=𝐶𝑖·exp(𝜏−1𝑠3−𝑖), then we must have
𝑡𝑖𝑘=𝐶′
𝑖·(𝑏𝑖𝑘−𝑏(3−𝑖)𝑘).Since we have derivedÍ𝑑
𝑘=1𝑡2
𝑖𝑘=1and
𝐶′
𝑖>0, we could thus deduce
𝑡𝑖𝑘=𝑏𝑖𝑘−𝑏(3−𝑖)𝑘
Í𝑑
𝑘′=1(𝑏𝑖𝑘′−𝑏(3−𝑖)𝑘′)21
2
is the unique solution (best-response strategy) to problem (3). Com-
bining the unique best-response strategies for both party 1and
party 2finishes the proof for our statement. □
A.2 Proof for Theorem 5.2
Lemma A.1. For any strategy profile 𝜎satisfying𝑓𝑖=𝑓𝜎𝑖
𝑖,∀𝑖,
𝑈𝑖(𝜎𝑖,𝜎−𝑖)=𝑈𝑖(𝜎𝑖,𝐼𝑀−1)
Proof. (Lemma A.1) Note that
𝑈𝑖(𝜎𝑖,𝐼𝑀−1)=∫
𝑡∈T∫
𝑡′
𝑖∈T𝑖𝑢𝑖 (𝑡′
𝑖,𝑡−𝑖);𝑡𝑖𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑑𝑡′
𝑖𝑓(𝑡)𝑑𝑡,
we could have
𝑈𝑖(𝜎𝑖,𝜎−𝑖)=∫
𝑡∈T∫
𝑡′∈T𝑢𝑖(𝑡′;𝑡𝑖)𝑀Ö
𝑖=1𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑑𝑡′𝑓(𝑡)𝑑𝑡
=∫
𝑡′
1∈T1...∫
𝑡′
𝑀∈T𝑀∫
𝑡𝑖∈T𝑖𝑢𝑖(𝑡′;𝑡𝑖)𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑓𝑖(𝑡𝑖)
Ö
𝑗≠𝑖 ∫
𝑡𝑗∈T𝑗𝜎𝑗(𝑡𝑗,𝑡′
𝑗)𝑓𝑗(𝑡𝑗)𝑑𝑡𝑗!
𝑑𝑡𝑖𝑑𝑡′
1...𝑑𝑡′
𝑀
=∫
𝑡′
1∈T1...∫
𝑡′
𝑀∈T𝑀∫
𝑡𝑖∈T𝑖𝑢𝑖(𝑡′;𝑡𝑖)𝜎𝑖(𝑡𝑖,𝑡′
𝑖)𝑓𝑖(𝑡𝑖)
Ö
𝑗≠𝑖𝑓𝑗(𝑡′
𝑗)𝑑𝑡𝑖𝑑𝑡′
1...𝑑𝑡′
𝑀
=𝑈𝑖(𝜎𝑖,𝐼𝑀−1),
where the last equality comes from regarding 𝑡′
𝑗as𝑡𝑗for𝑗≠𝑖.□
Proof. (Theorem 5.2) For the strategy profile 𝜎={𝐼𝑀}, suppose
participant𝑖deviates to an alternative strategy 𝜎′
𝑖with𝑓𝜎′
𝑖
𝑖=𝑓𝑖, and
𝑈ℎ0
𝑖(𝜎′
𝑖,𝐼𝑀−1)>𝑈ℎ0
𝑖(𝐼𝑀). Then consider another (randomized)
server aggregation function ℎ′
0defined asℎ′
0(𝑡)=ℎ0(𝜎′
𝑖(𝑡𝑖),𝑡−𝑖).
Sinceℎ′
0just maps the uploaded embedding of participant 𝑖accord-
ing to𝜎′
𝑖on the basis of ℎ0,𝑈ℎ′
0
𝑗(𝐼𝑀)=𝑈ℎ0
𝑗(𝐼,(𝜎′
𝑖,𝐼𝑀−2))for𝑗≠𝑖,
and𝑈ℎ′
0
𝑖(𝐼𝑀)=𝑈ℎ0
𝑖(𝜎′
𝑖,𝐼𝑀−1).
From Lemma A.1, as 𝑓𝐼
𝑗=𝑓𝑗and𝑓𝜎′
𝑖
𝑖=𝑓𝑖, we have
𝑈ℎ0
𝑗(𝐼,(𝜎′
𝑖,𝐼𝑀−2))=𝑈ℎ0
𝑗(𝐼𝑀)=∫
𝑡∈T𝑢𝑗(ℎ0(𝑡);𝑡𝑗)𝑓(𝑡)𝑑𝑡,∀𝑗≠𝑖.
Therefore, we would have 𝑈ℎ′
0
𝑖(𝐼𝑀)>𝑈ℎ0
𝑖(𝐼𝑀)and𝑈ℎ′
0
𝑗(𝐼𝑀)=
𝑈ℎ0
𝑗(𝐼𝑀),∀𝑗≠𝑖, contradicting with the Pareto efficiency of ℎ0.□
3584KDD ’24, August 25–29, 2024, Barcelona, Spain Yidan Xing, Zhenzhe Zheng, & Fan Wu
A.3 Proof for Theorem 5.3
Proof. We conduct the proofs for discrete embeddings 𝑡𝑖∈𝑇𝑖.
For any potential strategy 𝜎𝑖, construct a directed graph G, whose
nodes correspond to each possible local embedding 𝑡𝑖∈𝑇𝑖. We
construct the edges in this graph to denote the strategic report
𝜎𝑖(𝑡1
𝑖,𝑡2
𝑖)>0for𝑡1
𝑖≠𝑡2
𝑖, such that each directed edge pointing
from𝑡1
𝑖to𝑡2
𝑖has weight P(𝑡𝑖=𝑡1
𝑖)·𝜎𝑖(𝑡1
𝑖,𝑡2
𝑖),i.e., the probability
that participant 𝑖has true embedding 𝑡1
𝑖and misreport embedding
𝑡2
𝑖under strategy 𝜎𝑖. By{𝜎𝑖:𝑓𝜎𝑖
𝑖=𝑓𝑖}, we require∀𝑡2
𝑖∈T𝑖,
∑︁
𝑡1
𝑖∈T𝑖𝜎𝑖(𝑡1
𝑖,𝑡2
𝑖)·P(𝑡𝑖=𝑡1
𝑖)=P(𝑡𝑖=𝑡2
𝑖). (5)
Based on (5), we would further have∑︁
𝑡1
𝑖≠𝑡2
𝑖𝜎𝑖(𝑡1
𝑖,𝑡2
𝑖)·P(𝑡𝑖=𝑡1
𝑖)=(1−𝜎𝑖(𝑡2
𝑖,𝑡2
𝑖))·P(𝑡𝑖=𝑡2
𝑖)
=∑︁
𝑡1
𝑖≠𝑡2
𝑖𝜎𝑖(𝑡2
𝑖,𝑡1
𝑖)·P(𝑡𝑖=𝑡2
𝑖),
where the last inequality is due toÍ
𝑡1
𝑖∈T𝑖𝜎𝑖(𝑡2
𝑖,𝑡1
𝑖)=1,∀𝑡2
𝑖∈T𝑖.
Therefore, in the constructed graph, each node would have the sum
of weight of in-edges to equal the sum of weight of out-edges.
To prove the statement, we would start from the graph of any
strategy𝜎𝑖≠𝐼, and gradually remove all the edges in this graph
to approach the edgeless graph (correspond to the truth-telling
strategy𝐼). We would demonstrate that (1) the adjustment must
finally lead to an edgeless graph, and (2) each step in the adjustment
would result in a feasible strategy with non-decreasing utility, thus
finishes the proof. Our adjustment is as follows: in each step, we
find a cycle in the graph. We remove the edge with the smallest
weight in this cycle, whose weight is denoted as 𝑤, and also update
the weight of other edges in this cycle to minus 𝑤, then add𝑤to
𝜎𝑖(𝑡𝑖,𝑡𝑖)for each node 𝑡𝑖in this cycle.
To prove statement (1), since we have formulated a directed
graph, suppose the graph is not edgeless and contains no cycle
during the adjustment, there must exist some sink node and source
node in the graph. However, by our construction, each node must
have the equal sum of weights of in-edges and out-edges, thus leads
to a contradiction. For statement (2), since each step of adjustment
preserves Equation (5), the adjusted strategy is still feasible and
satisfies{𝜎𝑖:𝑓𝜎𝑖
𝑖=𝑓𝑖}. We only remains to show each step of
adjustment would lead to non-decreasing utility given conditions
(2). For a strategy 𝜎𝑖, its resulted utility could be calculated as
∑︁
𝑡1
𝑖∈T𝑖∑︁
𝑡2
𝑖∈T𝑖P(𝑡𝑖=𝑡1
𝑖)·𝜎𝑖(𝑡1
𝑖,𝑡2
𝑖)·𝑣𝑖(𝑡1
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡2
𝑖,𝑡−𝑖)]
=∑︁
𝑡1
𝑖∈T𝑖∑︁
𝑡2
𝑖∈T𝑖𝑤(𝑡1
𝑖,𝑡2
𝑖)·𝑣𝑖(𝑡1
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡2
𝑖,𝑡−𝑖)],
where we use 𝑤(𝑡1
𝑖,𝑡2
𝑖)to denote the weight of edge from 𝑡1
𝑖to𝑡2
𝑖for
𝑡1
𝑖≠𝑡2
𝑖, and define 𝑤(𝑡1
𝑖,𝑡1
𝑖)=P(𝑡𝑖=𝑡1
𝑖)·𝜎𝑖(𝑡1
𝑖,𝑡1
𝑖). W.l.o.g., define
the𝑛nodes in the current cycle as 𝑡1
𝑖,...𝑡𝑛
𝑖(with𝑡𝑛+1
𝑖=𝑡1
𝑖for conve-
nience in notations). Then by our construction of the adjustment, ex-
cept for the unchanged parts of the graph, the utility before this step
of adjustment is 𝑤·hÍ𝑛
𝑗=1𝑣𝑖(𝑡𝑗
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑗+1
𝑖,𝑡−𝑖)]i
,and the util-
ity after this step would be 𝑤·hÍ𝑛
𝑗=1𝑣𝑖(𝑡𝑗
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑗
𝑖,𝑡−𝑖)]i
.Byconditions (2), since larger 𝑣𝑖(𝑡𝑗
𝑖)implies larger E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑗
𝑖,𝑡−𝑖)],
applying the rearrangement inequality, we would have
𝑛∑︁
𝑗=1𝑣𝑖(𝑡𝑗
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑗
𝑖,𝑡−𝑖)]≥𝑛∑︁
𝑗=1𝑣𝑖(𝑡𝑗
𝑖)·E𝑡−𝑖[𝑥ℎ0
𝑖(𝑡𝑗+1
𝑖,𝑡−𝑖)],
thus proves the non-decreasing of utility in each step of adjustment.
□
A.4 Proof for Theorem 5.4
Proof. Define𝑛to be the number of total inference rounds. It is
sufficient for us to find a penalty function 𝑘𝑖(·)for each participant
𝑖, such that when 𝑛→∞ , there does not exist a strategy 𝜎𝑖with
𝑈ℎ∗
0
𝑖(𝜎𝑖,𝐼𝑀−1)>𝑈ℎ∗
0
𝑖(𝐼𝑀). For convenience in notations, we abbre-
viate𝛽𝑇(𝑓𝜎𝑖
𝑖)to be𝛽(𝜎𝑖), and define 𝑘′
𝑖(𝛽(𝜎𝑖))=𝑘𝑖(𝛽(𝜎𝑖))/𝑚𝑖.
To evaluate the change of utility for an arbitrary strategy 𝜎𝑖
during the penalty period, we further define the expected util-
ity increment ratio of 𝜎𝑖over the penalty reporting strategy 𝜎𝑝
𝑖:
𝜎𝑝
𝑖(𝑡𝑖,·)∼𝐺𝑖,∀𝑡𝑖∈T𝑖when other participants report truthfully
as𝛿𝑖(𝜎𝑖):=𝑈𝑖(𝜎𝑖,𝐼𝑀−1)
𝑈𝑖(𝜎𝑝
𝑖,𝐼𝑀−1)−1.Recall that we are under the feasible
problem case, which indicates 𝛿𝑖(𝐼)>0. By the strong law of large
number, we would have 𝑞𝑖(𝜎𝑖)converges to 𝛽(𝜎𝑖)when𝑛→∞ .
Therefore, the proportion of penalty period among the entire infer-
ence period would converge to𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖))
1+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖)),since when each
two-sample test of length 𝑚𝑖ends, there is an additional 𝛽(𝜎𝑖)prob-
ability to have a penalty period with length 𝑘′
𝑖(𝛽(𝜎𝑖))·𝑚𝑖. Thus, we
could calculate the expected per-round utility of strategy 𝜎𝑖when
𝑛→∞ as
𝑈ℎ∗
0
𝑖(𝜎𝑖,𝐼𝑀−1)=1
1+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖))·𝑈ℎ0
𝑖(𝜎𝑖,𝐼𝑀−1)
+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖))
1+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖))·𝑈ℎ0
𝑖(𝜎𝑝
𝑖,𝐼𝑀−1)
=𝛿𝑖(𝜎𝑖)
1+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖))+1
·𝑈ℎ0
𝑖(𝜎𝑝
𝑖,𝐼𝑀−1).
That is, to prove the BNE when 𝑛→∞, we need
𝛿𝑖(𝐼)
1+𝛼·𝑘′
𝑖(𝛼)≥𝛿𝑖(𝜎𝑖)
1+𝛽(𝜎𝑖)·𝑘′
𝑖(𝛽(𝜎𝑖)),∀𝜎𝑖∈Σ𝑖. (6)
For any𝜎𝑖with𝑓𝜎𝑖
𝑖=𝑓𝑖,𝛽(𝜎𝑖)=𝛼, and by Theorem 5.2, 𝛿𝑖(𝐼)≥
𝛿𝑖(𝜎𝑖), which holds regardless of the form of functions 𝑘𝑖. For any
𝜎𝑖with𝑓𝜎𝑖
𝑖≠𝑓𝑖, consider𝑘′
𝑖(𝛼)to be in the form of 𝛼𝑐·𝐵with
constants𝐵and𝑐, and substitute it into condition (6), we would
equivalently require
𝛿𝑖(𝜎𝑖)−𝛿𝑖(𝐼)≤
𝛿𝑖(𝐼)·𝛽𝑐+1(𝜎𝑖)−𝛿𝑖(𝜎𝑖)·𝛼𝑐+1
·𝐵,∀𝜎𝑖.(7)
By feasibility of the problem case and the validity of the two-sample
test, we have 𝛿𝑖(𝐼)>0and𝛽(𝜎𝑖)>𝛼. Therefore, for each 𝜎𝑖with
𝑓𝜎𝑖
𝑖≠𝑓𝑖, we are able to find a large enough positive constant 𝑐𝜎𝑖,
such that𝛿𝑖(𝐼)·𝛽𝑐𝜎𝑖+1(𝜎𝑖)−𝛿𝑖(𝜎𝑖)·𝛼𝑐𝜎𝑖+1>0. Taking𝑐to be the
supreme over those 𝑐𝜎𝑖, we would have this term to be positive
for all the𝜎𝑖with𝑓𝜎𝑖
𝑖≠𝑓𝑖. Then taking 𝐵to be a large positive
constant such that condition (6) is satisfied for all the 𝜎𝑖finishes
the proof. □
3585