Hierarchical Linear Symbolized Tree-Structured Neural Processes
Jin yang Tai∗
School of Computer Engineering and Science
Shanghai University
Shanghai, China
1203146411@shu.edu.cnYi ke Guo
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong, China
yikeguo@ust.hk
Abstract
Traditional Neural Processes (NPs) and their variants aim to learn
relationshipsbetweencontextsamplepointsbutdonotconsider
multi-level information, resulting in a limited ability to learn com-
plex distributions.This paper draws inspiration from features such
as the hierarchical nature and interpretability of tree-like struc-
tures. This paper proposes a Hierarchical Linear Symbolized Tree-
structured Neural Processes (HLNPs) architecture. This framework
utilizes variables to build a top-down hierarchical linear symbol-
ized tree-structured network architecture, enhancing positional
representationinformationinahierarchicalmanneralongthede-
terministicpath.Inthelatentdistribution,thehierarchicallinear
symbolized tree-structured network approximates functions dis-cretely through a layered approach. By decomposing the latent
complex distribution into several simpler sub-problems using sum
andproductsymbols,theupperboundofoptimizationisthereby
increased. The tree structure discretizes variables to capture model
uncertainty inthe formof entropy.This approach alsoimparts a
causaleffecttotheHLNPsmodel.Finally,wedemonstratetheeffec-
tiveness of the HLNPsmodels for 1D data, Bayesian optimization,
and 2D data.
CCS Concepts
•Computing methodologies →Neural networks.
Keywords
Neural Processes,Bayesian Network,Hierarchical Linear Symbol-
ized Tree-structured
ACM Reference Format:
Jin yang Tai and Yi ke Guo. 2024. Hierarchical Linear Symbolized Tree-
StructuredNeuralProcesses.In Proceedingsofthe30thACMSIGKDDCon-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671861
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718611 Introduction
Thelimitationofneuralnetworksisthattheycanonlylearnspe-
cificfixedmappingfunctions[ 10],whilemeta-learningcanacquire
arange offunctiondistributions,enabling ittopossessthe ability
to quickly adapt to new tasks and enhance computational effi-
ciency.NeuralProcesses(NPs)[ 11]representanimplementation
methodology within the realm of meta-learning [ 23]. It constructs
implicitprobabilisticfunctionmodelsbasedonobservedcontext
samplepointstopredictthefunctionvaluesoftargetsamplepoints.
Thisformofimplicitprobabilityfunctionintegratestheinherent
flexibility of neural networks with the uncertainty inherent in sto-
chasticprocesses,therebyfacilitatingtheacquisitionofmapping
relationships between input and output data. The above imple-
mentation stands apart from conventional approaches, such asGaussian Processes (GPs) [
4], where a predefined mathematical
modelisemployed,andmodelparametersaresubsequentlyfine-
tuned to align with the data distribution. The fitting results of this
approach are overly reliant on the selection of the kernel function
[18]. In contrast, NPs usually operate with minimal inductive bias
andaredrivenbydata,enablingthemtoautonomouslyshapetheir
structure based on the inherent characteristics of the data.
The design goal of the NPs model is to fit context sample points
withimplicitfunctionsandcaptureuncertainty.TheNPs[ 11]im-
plementationutilizes fixed-dimensionallatentvariables toencode
contextsamplepoints,therebyenhancingtheexpressivecapacity
for implicit variable functions. However, this design is prone to theproblem of underfitting the context sample points [
16]. Integrating
attention mechanisms addresses this concern by assigning varying
weightstocontextsamplepoints.Nevertheless,whendealingwitha
highlyintricateoriginaldistributionofinputcontextsamplepoints,
the model encounters a significant increase incomputational cost
andtime[ 12].Additionally,inthecontextofNPsmodelsdealing
with complex distributions, there is a substantial gap between the
Gaussian posterior approximation using Variational Inference (VI)
and the true distribution. This form of approximation introduces a
significantbiasinrepresentingtheuncertaintyofthelatentfunc-
tion. The introduction of importance-weighted methods reduces
thisdisparity,resultinginatighterlowerboundfortheEvidence
Lower Bound (ELBO) [ 30]. It is worth exploring the redesign of an
effective NPs model that not only incorporates sufficient hierarchi-
cal information when dealing with context sample points but also
provides amorereasonableapproximationto theGaussian poste-
riordistribution.Thiswouldenableittobetteradapttohandling
complex distributions.
Inthispaper,weproposeaHierarchicalLinearSymbolizedTree-
structuredNeuralProcesses.Weintroduceatop-downtreestruc-
turetofacilitatethelearningofinformationatdifferenthierarchical
levels. When dealing with complex latent distributions, the tree
 
2818
KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
structuresymbolizesthemthroughsumandproduct,breakingthem
down into simpler sub-problems for resolution in a recursive man-
ner.Intheprocessofsolvingthroughtree-structureddiscretization,
the formation of a ’coefficient path’ enhances interpretability. Fi-
nally,entropyisemployedasameanstolearnuncertaintyinthe
constructionprocessofHLNPs.Wedemonstratetheeffectiveness
oftheHLNPsmodelsfor1Ddata,Bayesianoptimization,and2D
data.
2 Preliminaries
2.1 Notations
Wehavetheprimarytaskofdefiningthesetupforthemeta-learning
problem. Suppose a set of input spaces is represented as Xand
output space is represented as Y. We have D𝑁=(X ×Y)𝑁as
the collection of 𝑁pairs of input-output data. D≤𝑁=/uniontext.1𝑁
𝑛=1D𝑛
representsacollectionthatcontainsatmost 𝑁pairsofinput-output
data,whereas D∞=/uniontext.1∞
𝑛=1D𝑛representsacollectionconsistingof
afinite numberof pairsof input-outputdata. Definethe elements
in𝐷∈Dthe data set is denoted as 𝐷=(𝑥,𝑦), where 𝑥∈X𝑁
representstheelementsetoftheinputspaceand 𝑦∈Y𝑁represents
the element set of the output space. A collection of datasets is
denotedas 𝐷𝑀
𝑚=1.Inmeta-learningcalleachindividualdataset 𝐷𝑚
as a task [ 29]. For each task data 𝐷𝑚is divided into two categories
represented as 𝐷𝐶𝑚=/parenleftBig
𝑥𝐶𝑚,𝑦𝐶𝑚/parenrightBig
context sample points and 𝐷𝑇𝑚=
/parenleftBig
𝑥𝑇𝑚,𝑦𝑇𝑚/parenrightBig
targetsamplepointsrespectively[ 3].Theultimategoal
istotrainamodelfromthedataset 𝐷𝐶𝑚toobtaintheoutputbest
prediction 𝑦𝑇𝑚given the target inputs 𝑥𝑇𝑚.
2.2 Neural Processes
Our task is to implement an implicit function 𝑓:X→Y that
efficiently captures the relationship between input and output
data (includinguncertainty). TheNPs modelacquires anycontext
sample points as conditions and predicts target sample points as
𝑝/parenleftBig
𝑦𝑇𝑚|𝑥𝑇𝑚,𝐷𝐶𝑚/parenrightBig
:=𝑝/parenleftBig
𝑦𝑇𝑚|𝑥𝑇𝑚;/parenleftBig
𝑥𝐶𝑚,𝑦𝐶𝑚/parenrightBig/parenrightBig
. More specifically, we
focusedonlyonasimpleheteroscedasticGaussianmeasurement
noise to restrict the scope of the study.
𝑝𝜃/parenleftBig
𝑦𝑇
𝑚|𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig
=/productdisplay.1
(𝑥𝑇𝑚,𝑦𝑇𝑚)∈𝐷𝑇𝑚N/parenleftBig
𝑦𝑇
𝑚;𝜇𝜃/parenleftBig
𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig
,
𝜎2
𝜃/parenleftBig
𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig/parenrightBig
.(1)
where 𝜇𝜃:X→Yand𝜎2
𝜃:X→R+denotethemappingofinputs
into the form of averages and variances [ 18].𝜃is a parameter in
the function 𝑓that can be learned. In the Bayesian optimization
processes, the NPs [ 11] model represents the overall structure and
uncertainty as a global latent Gaussian variable 𝑍is denoted as
𝑍∼𝑝𝜃/parenleftBig
𝑍|𝑥𝑇𝑚,𝐷𝐶𝑚/parenrightBig
. We further express Eq (19) as follows:
𝑝𝜃/parenleftBig
𝑦𝑇
𝑚|𝑥𝑇
𝑚,𝑍/parenrightBig
=/productdisplay.1
(𝑥𝑇𝑚,𝑦𝑇𝑚)∈𝐷𝑇𝑚N/parenleftBig
𝑦𝑇
𝑚;𝜇𝜃/parenleftBig
𝑍,𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig
,
𝜎2
𝜃/parenleftBig
𝑍,𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig/parenrightBig
.(2)
The log-likelihood of the above posterior distribution is difficult
tosolve,andVIisintroducedtooptimizetheframework[ 17].Wecompute the ELBO of log𝑝𝜃/parenleftBig
𝑦𝑇𝑚|𝑥𝑇𝑚,𝑍/parenrightBig
, which is represented as
two parts as the reconstruction part and the KL scatter measure
part:
E𝑍∼𝑞𝜙(𝑍|𝐷𝑇𝑚)/bracketleftBig
log𝑝𝜃/parenleftBig
𝑦𝑇
𝑚|𝑍,𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig/bracketrightBig
−𝐷𝐾𝐿/bracketleftBig
𝑞𝜙/parenleftBig
𝑍|𝐷𝑇
𝑚/parenrightBig
||𝑝𝜓/parenleftBig
𝑍|𝑥𝑇
𝑚,𝐷𝐶
𝑚/parenrightBig/bracketrightBig
.(3)
Where 𝜓representstheconditionalpriorparameter[ 12].𝑞𝜙/parenleftBig
𝑍|𝐷𝑇𝑚/parenrightBig
representsaposteriordistributionusedduringthetrainingofthe
model to aid in data fitting. However, this posterior distribution is
no longer used in the actual inference processes.
2.3 Bayesian Network
Webuildaprobabilisticmodel 𝐺oftheinputcontextsamplepoints
(𝑥𝑇𝑚,𝑦𝑇𝑚). In the probability model distribution, the context sample
pointssatisfyconditionalindependence,representedinconditional
probability form as 𝑝/parenleftBig
𝐷𝐶𝑚|pa𝐺/parenleftBig
𝑥𝐶𝑚,𝑦𝐶𝑚/parenrightBig/parenrightBig
, resulting in a joint dis-
tribution 𝑝(𝐷𝐶𝑚|𝐺)=/producttext.1
𝑖𝑝/parenleftBig
𝐷𝐶
𝑖|pa𝐺/parenleftBig
𝑥𝐶
𝑖,𝑦𝐶
𝑖/parenrightBig/parenrightBig
. In the above
joint distribution, pa𝐺denotes the parent class node of the context
samplepoint.InthisBayesianmodel,weimplementthecontinu-
ous function/parenleftBig
𝑥𝐶𝑚,𝑦𝐶𝑚/parenrightBig
=(𝑥𝐶𝑚,𝑦𝐶𝑚)𝐵+𝜖using the existing linear
Gaussiandistribution[ 19],where 𝐵∈R𝑚×𝑚isaparametermatrix
with learnable weights and 𝜖∼N( 𝑏,Σ). Meanwhile, 𝑏∈R𝑚and
Σ∈R𝑚×𝑚
≥0adiagonalmatrixinthenoisevariable.Inparticular,for
𝑖not a parent of 𝑗in𝐺, then 𝐵𝑖,𝑗=0.
We partition the graph 𝐺into subgraphs connected by multi-
plication 𝐺𝑖.𝐺𝑖representstheparentclasssetofthe contextsam-
plepoint (𝑥𝐶
𝑖,𝑦𝐶
𝑖).Weindirectlyobtaintheposteriordistribution
𝑝𝐺(𝐺|𝐷𝐶𝑚)ofgraph 𝐺throughthe posteriordistributionsofits
subgraphs 𝐺𝑖.Tosimplifythecomputationalprocess,weintroduce
the correlation 𝟙in the subgraphs 𝐺𝑖. Meanwhile, 𝑝𝐺(𝐺|𝐷𝐶𝑚)
calculation also requires the definition of prior 𝑝𝜓(𝐺𝑖)and log-
likelihood 𝑝𝜃(𝐺𝑖)forthesubgraphs.Theposteriordistributionof
graph 𝐺isexpressedas 𝑝𝐺(𝐺|𝐷𝐶𝑚)=𝟙𝐺/producttext.1
𝑖𝑝𝜓(𝐺𝑖)𝑝𝜃/parenleftBig
𝐷𝐶
𝑖|𝐺𝑖/parenrightBig
.
During the process of computing the posterior distribution, obtain-
ing a reasonable estimate of model parameters is achieved through
theuseofpriordistributionandlog-likelihood,whiletakinginto
full consideration the data and domain knowledge [26].
3 Method
3.1 Hierarchical Linear Symbolized
Tree-structured Network
We employ the observed context sample points 𝐷𝐶𝑚to construct a
hierarchical linear symbolized tree-structured network in graph 𝐺.
At the same time, we consider preserving the NPs model Permu-
tationinvariance[ 11]fortreestructureconstruction.Weadopta
top-downapproachtoimplementthehierarchicallinearsymbol-
izedtree-structurednetwork.Inadditiontotheleafnodes 𝐿,our
treestructurecomprisestwotypesofsymbolizednodes:Product
nodes 𝑀andsumnodes 𝑇.Thesenodescanberegardedasdistri-
butionsofsubsetsoftherootnode,withtherootnoderepresenting
the overall distribution. The product node aims to aggregate the
distribution of child nodes, i.e., 𝑀=/producttext.1
𝐷𝐶
𝑖∈𝑐ℎ(𝑀)𝐷𝐶
𝑖. The sum
 
2819Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD ’24, August 25–29, 2024, Barcelona, Spain.
node is determined by weighting and summing its child nodes, i.e.,
𝑇=/summationtext.1
𝐷𝐶
𝑖∈𝑐ℎ(𝑇)𝜆𝑖𝐷𝐶
𝑖(𝜆𝑖>0,/summationtext.1𝜆𝑖=1). The node 𝐷𝐶
𝑖scope de-
scribesthesetofvariablesthateachnodeaffectsintheprobabilistic
graph model. For example, the node 𝐷𝐶
𝑖has a scope 𝑠𝑐(𝐷𝐶
𝑖)={𝑉}
,where 𝑉isthecoverageofthespecifieddistribution.Thescope
ofasumorproductnodedependsonthescopeofitschildnodes
𝑠𝑐(𝐷𝐶
𝑖)=/uniontext.1
𝐷𝐶
𝑗∈𝑐ℎ(𝐷𝐶
𝑖)𝑠𝑐(𝐷𝐶
𝑗).
Theadvantageofalinearsymbolizedtree-structurednetworkis
itsabilitytocapturecontextinformationatdifferenthierarchical
levels,simplifyingtheposteriordistributioncalculationforgraph
𝐺.Whenconstructingtree-structurednetworks,itisessentialto
possess completeness and decomposability.
(i) The completeness of linear symbolized tree-structured net-
works. For each sum node 𝑇in the tree, for any two nodes 𝐷𝐶
1
and𝐷𝐶
2, they should satisfy the condition 𝑠𝑐/parenleftBig
𝐷𝐶
1/parenrightBig
=𝑠𝑐/parenleftBig
𝐷𝐶
2/parenrightBig
.I n
other words, the two child nodes have the same scope, and their
distributions involve the same variables. This property also applies
tothesumnode 𝑇itself,asitsscopeisaconcatenationofthescopes
of its children.
(ii) The decomposability of linear symbol tree-structured net-
works.Foreachproductnode 𝑀inthetreeandanytwochildnodes
𝐷𝐶
1and𝐷𝐶
2,theyshouldsatisfythecondition 𝑠𝑐/parenleftBig
𝐷𝐶
1/parenrightBig
∩𝑠𝑐/parenleftBig
𝐷𝐶
2/parenrightBig
=∅.
Inotherwords,thescopeoftheproductnode 𝑀canbepartitioned
into disjoint subsets by the scopes of its child nodes.
Whenthesepropertiesarepresent,itbecomesfeasibletocom-
putetheprobabilisticmodelgraph 𝐺,whichisinferredusingthe
MostProbableExplanation(MPE)posteriordistribution[ 1].Therea-
soningprocessofthehierarchicallinearsymbolizedtree-structured
network is only related to the number of its edges, so it also has
the advantage of linear time growth.
3.2 Processable Representation of
Tree-Structured Network
We process context sample points in a top-down manner, intro-
ducing sum and product symbolization to construct a hierarchical
linear tree-structured network[5].
Each processed context sample point (𝑥𝐶
𝑖,𝑦𝐶
𝑖)during the tree
constructionisdenotedas 𝜋<𝑖.If𝐺𝑖⊆𝜋<𝑖signifiesthattheparent
nodesetofeachnode 𝐷𝐶
𝑖intheprobabilisticgraph 𝐺𝑖isconsistent
withtheordering 𝜋<𝑖,thenweconsiderthegraph 𝐺tobeconsis-
tent(𝐺|=𝜋). We use the joint distribution 𝑝(𝜋,𝐺|𝐷𝐶𝑚)=𝑝𝜓(𝐺)
𝑝𝜃/parenleftBig
𝐷𝐶𝑚|𝐺/parenrightBig/producttext.1
𝑖𝟙𝐺𝑖⊆𝜋<𝑖.It’simportanttonotethatthemarginal
probability distribution 𝑝(𝐺|𝐷𝐶𝑚)is distinct from the conditional
probability 𝑝𝐺(𝐺|𝐷𝐶𝑚). The key distinction lies in the fact that
𝑝(𝐺|𝐷𝐶𝑚)considers a broad spectrum of ordering 𝜋, while the
conditional probability 𝑝𝐺(𝐺|𝐷𝐶𝑚)is limited to specific graph
structures corresponding to a particular ordering 𝜋. Graph 𝐺is
representable as decomposable by 𝑝𝐺(𝐺)=/producttext.1
𝑖𝑝𝐺𝑖(𝐺𝑖). We refor-
mulate the joint distribution: 𝑝(𝜋,𝐺)=/producttext.1
𝑖𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝜋<𝑖.
3.3 Hierarchical Bayesian Conditional
Independence Inference
This approach is disastrous for joint distributions because the pro-
cess of constructing a tree for context sample points. We cannotachieve the computation of the posterior distribution through sam-
pling,aswellasthemarginalprobabilityofspecificnodeedges.For
the computation of this posterior distribution, we transform it into
a feasible decomposed approximation approach for representation.
Weareinspiredbythedivideandconquermethodtodecompose
the computation of the target distribution into precise conditional
independence,hierarchicallybreakingitdownintosmaller,simpler
problems for approximate solutions. We define a subset of context
samplepointsas 𝐷𝐶𝑠,where 𝜋<𝑠representsthetopologicalordering
involved in tree construction with 𝐷𝐶𝑠.𝐺𝑠/defines{𝐺𝑖:𝑖∈𝑠}denotes
the set of parent class nodes 𝐷𝐶
𝑖in𝐷𝐶𝑠. We partition the data set
into two subsets (𝜋𝑠1,𝜋𝑠2).𝐷𝐶𝑠1denotes completed tree network
constructioncontext samplepointsand 𝐷𝐶𝑠2denotespendingcon-
text sample points. The conditional distribution for this case is
rewritten as:
𝑝/parenleftbig𝜋,𝐺|𝜋=/parenleftbig𝜋𝑠1,𝜋𝑠2/parenrightbig/parenrightbig=/productdisplay.1
𝑖∈𝑠1𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆(𝜋𝑠1<𝑖)
/productdisplay.1
𝑖∈𝑠2𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝑠1∪(𝜋𝑠2<𝑖),(4)
From the above formulation it can be noticed that we have divided
thecomputationalprocessintotwoparts:/parenleftbig𝜋𝑠1,𝐺𝑠1/parenrightbigand/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig.
Now, we extend the processing approach to the entire sampling
context sample points, also denoted as 𝐷𝐶𝑠1and𝐷𝐶𝑠2, respectively.
We focus on computing the distribution of/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig. The corre-
sponding distribution can be expressed as: ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/defines
/producttext.1
𝑖∈𝑠2𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝑠1∪(𝜋𝑠2<𝑖).˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2denotestheunnormalized
originaldistribution.Now,weconditionallydecomposethepend-
ing context sample points 𝐷𝐶𝑠2into two categories: the operational
contextsamplepoints 𝐷𝐶𝑠21andthecandidatesamplepoints 𝐷𝐶𝑠22.So,
werecalculate ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2|𝜋𝑠2=/parenleftbig𝜋𝑠21,𝜋𝑠22/parenrightbig/parenrightbig,forwhich the
approximation ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠21,𝐺𝑠21/parenrightbig˜𝑝𝐷𝐶𝑠1∪𝐷𝐶𝑠21,𝐷𝐶𝑠22/parenleftbig𝜋𝑠22,𝐺𝑠22/parenrightbig.This
independence is employed for approximating probability distribu-
tions by partitioning the context sample points into two parts 𝐷𝐶𝑠1
and𝐷𝐶𝑠2,independentlyapproximatingtheprobabilitydistributions
withineachpart ˜𝑝∅,𝐷𝐶𝑠1/parenleftbig𝜋𝑠1,𝐺𝑠1/parenrightbig,˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig.Thisisdone
recursively during the implementation of the hierarchical linear
symbolized tree-structured network, continuing until it results in a
single node (leaf node) remaining in 𝐷𝐶𝑠2(Proposition 1).
3.4 Constructing a Hierarchical Linear
Symbolized Tree-structured Network
Wetaketherandomlysampledcontextsamplepoints 𝐷𝐶𝑚asawhole
toserveastherootnode.Duringtheconstructionofthetree-like
network, we alternate between processing subsets 𝐷𝐶𝑠1and𝐷𝑠2𝐶.
Weformallydefineahierarchicallinearsymbolizedtree-structured
network.
Definition1.Thehierarchicallinearsymbolizedtree-structured
network(𝜋,𝐺)has the following structure:
(i)Whentheleafnode 𝐿belongsto (𝐷𝐶𝑠1,(𝑥𝑖𝐶,𝑦𝑖𝐶))and(𝑥𝑖𝐶,𝑦𝑖𝐶)
∉𝐷𝐶𝑠1, its scope is defined as 𝑠𝑐(𝐿)=(𝜋𝑖,𝐺𝑖). The distribution of
the leaf node 𝐿is considered only with respect to the graph 𝐺𝑖
corresponding to the parent node set consistent with subset 𝐷𝐶𝑠1.
 
2820KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
+
ѫ
(0.4,2.3) (1.2,0.8)
(1.6,1.9) (2.4,1.0)
1D Gaussian Distribution Root Node
  0.2 0.5 0.10.2
+
ѫ (0.4,2.3)
(1.2,0.8) (1.6,1.9)
(2.4,1.0)+ +
ѫ (1.2,0.8)
(1.6,1.9) (2.4,1.0)
(0.4,2.3)ѫ (1.6,1.9)
(2.4,1.0) (0.4,2.3)
(1.2,0.8)+
ѫ (2.4,1.0)
(0.4,2.3)
(1.6,1.9)(1.2,0.8)0.7 0.10.10.1
Figure1:Wesamplefourcontextsamplepointsfroma1D
Gaussiandistribution:(0.4,2.3),(1.2,0.8),(1.6,1.9),(2.4,1.0).
Based on the above results, we construct the hierarchical
linear symbolized tree-structured network in a top-downmanner. Meanwhile, the root node also includes the sum
symbol, 𝐷𝐶𝑠1=∅.Wesettheallocationparameters 𝐾=(1,2),
correspondingto 𝐷𝐶𝑠2,andpartitioning( 𝐷𝐶𝑠21,𝐷𝐶𝑠22).Thefirst
round of product nodes and sum nodes are alternately recur-
sively propagated, as shown in the above diagram .
(ii) A symbolic sum node 𝑇exists with a joint subset (𝐷𝐶𝑠1,𝐷𝐶𝑠2)/parenleftBig
𝐷𝐶𝑠1∩𝐷𝐶𝑠2=∅/parenrightBig
, and/barex/barex𝐷𝐶𝑠2/barex/barex>1, and its scope is 𝑠𝑐(𝑇)=(𝜋𝑠2,𝐺𝑠2).
It has child nodes contain weights 𝜆𝑖, If the 𝑖th child node is a
product node 𝑀, partition 𝐷𝐶𝑠2into(𝐷𝐶𝑠21,𝑖,𝐷𝐶𝑠22,𝑖).
(iii) A symbolic product node 𝑀exists with/parenleftBig
𝐷𝐶𝑠1,𝐷𝐶𝑠21,𝐷𝐶𝑠22/parenrightBig
/parenleftBig
𝐷𝐶𝑠1∩𝐷𝐶𝑠21∩𝐷𝐶𝑠22=∅/parenrightBig
,and𝑠𝑐(𝑀)=(𝜋𝑠21∪𝜋𝑠22,𝐺𝑠21∪𝐺𝑠22).I n
general, the product node construction process contains two kinds
of child nodes, the first child node (𝐷𝐶𝑠1,𝐷𝐶𝑠22)and the second child
node/parenleftBig
𝐷𝐶𝑠1∪𝐷𝐶𝑠21,𝐷𝐶𝑠22/parenrightBig
.
We construct hierarchical linear symbolized tree-structured net-
work sum nodes 𝑇for(𝐷𝐶𝑠1,𝐷𝐶𝑠2)to be interpreted as distributed
over 𝐷𝐶𝑠2, with their parent nodes stored in 𝐷𝐶𝑠1. Based on condi-
tions that involve a subset of context sample points represented
byasetofdataset 𝐷𝐶𝑠2andtheirparentnodesin 𝐷𝐶𝑠1,wecanuse
sum or product symbols to infer the dependencies among nodes in
𝐷𝐶𝑠2.Subsequently,theoverallstructureofthehierarchicallinear
symbolizedtree-structurednetworkisestablished.Intheprocess
of constructing the symbolized tree-structured network, it was ob-
servedthatsumnodes 𝑇andproductnodes 𝑀appearalternately,
startingwithasumrootnode.Further,foreachchild (𝑥𝐶
𝑖,𝑦𝐶
𝑖)of
eachsumnode 𝑇,weprocesstheset 𝐷𝐶𝑠2bysettingallocationpa-
rameters 𝐾pairs of pending context sample points partitions ( 𝐷𝐶𝑠21,
𝐷𝐶𝑠22),whileensuring 𝐷𝐶𝑠21,𝑖∩𝐷𝐶𝑠22=∅.Wedescribeahierarchical
linear symbolized tree-structured network structure through an
example(asshowninFigure1).Therootnodeofthehierarchical
linearsymbolizednetwork,inadditiontosymbolicaddition,also
samplesfourcontextsamplepointsfroma1DGaussiandistribution,withcoordinatesbeing(0.4,2.3),(1.2,0.8),(1.6,1.9),(2.4,1.0).The
sum nodes 𝑇at different levels in a hierarchical linear symbolized
tree-structurednetworkareassignedthevalue 𝐾=(1,2).Thesum
and product layers alternate in a top-down loop until the leaf node
𝐿.
From the above defined hierarchical linear symbolized tree-
structured network. If the leaf node 𝐿is associated with ( 𝐷𝑠1𝐶,
𝐷𝐶𝑠21,𝑖),thenthe 𝑖thvariablerepresentsthedistributionoverthepar-
entclassnodeset 𝐺𝑖ofoperationalcontextsamplepoints (𝑥𝑖𝐶,𝑦𝑖𝐶).
Thedistribution overthesetof parentsofnode 𝐷𝑖𝐶isa subsetof
𝐷𝑠1𝐶. Here 𝐷𝑠1𝐶is a subset representing possible parent node sets.
The restriction of 𝐷𝑠1𝐶is that this distribution has support only if
𝐺𝑖⊆𝐷𝑠1𝐶, i.e., it only considers the case where the set of possible
parentnodesiswithin 𝐷𝑠1𝐶.Theaboverestrictionindicatescon-
sistencyinthehierarchicallineartree-structurednetwork (𝜋,𝐺).
The model is valid across the range of distributions.
Proposition1. Inhierarchicallinearsymbolizedtree-structured
networks,theprocessingofeachpairofcontextdatapointscon-
forms to the logical implication of 𝐺on𝜋(where 𝐺|=𝜋).
For Proposition 1, the nodes in the graph structure 𝐺contain
all the nodes in the topological ordering 𝜋. One of the key require-
ments is that the distributions over leaf nodes are tractable, in
ordertoachievetheoverallapproximationofthehierarchicallin-
ear symbolized tree-structured network distribution (𝜋,𝐺). Our
completion of the above tasks requires the introduction of MPE
inferenceandconditionalinferenceinprobabilisticmodeling.We
introducetheBooleanoperation 𝐵𝑖𝑗tosimplifycalculations,indi-
catingwhether 𝐷𝐶
𝑖istheparentclassof 𝐷𝐶
𝑗.Further,lettheiterative
relationbedenotedbythelogicalconnective 𝐶𝑖,i.e., 𝐵𝑖,𝑗or¬𝐵𝑖,𝑗,
i.e.wecanrepresentthenodesas 𝐶𝑖=𝐵𝐶
𝑖,1∧𝐵𝐶
𝑖,2∧¬𝐵𝐶
𝑖,3thatnodes
(𝑥𝐶
1,𝑦𝐶
1)and(𝑥𝐶
2,𝑦𝐶
2)are parents of 𝐷𝐶
𝑖, while node (𝑥𝐶
3,𝑦𝐶
3)is
not.Then,accordingtothejointsubset 𝐷𝐶𝑠1onecanrepresentits
edge inference probability task as 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐶𝑖=1). On the basis
of the conditional distribution 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖|𝐶𝑖=1), conditional
samplingneedstobedonefromittofinallycompletethemaximiza-
tioninference max𝐺𝑖𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖|𝐶𝑖=1).Evengiventhestatesof
some edges, computing the edge probability distributions of other
edges is challenging and requires a lotof search and computation.
Thesechallengesmakeitverydifficulttoperforminferencequeries
in a Bayesian network. There has been previous work on this [ 28],
andwedealwithitbylimitingthenumberofcontextsamplepoints
ineachnode 𝐶𝑖⊂𝐷𝐶𝑠𝑚\𝐷𝐶
𝑖.Weinheritfrompreviousworkbycon-
straining the parent nodes of context sample point (𝑥𝐶
𝑖,𝑦𝐶
𝑖)to a
fixed-size set of candidate parent nodes, where the size of 𝐶𝑖(cho-
senhereastheaveragenumberofcontextsamplepointsusedto
construct the tree) is selected to participate in the maximization of
MPEinference.Forthis ˜𝑝𝐷𝐶𝑠1,{𝐷𝐶
𝑖}(𝐺𝑖)approximaterepresentation,
we have 𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1∩𝐷𝐶
𝑖.
Proposition 2. Any hierarchical linear tree-structured network
possesses completeness and decomposability.
In general, the hierarchical linear symbolized tree-structured
networksatisfiesthestandarddecomposabilityandcompleteness
properties.
 
2821Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD ’24, August 25–29, 2024, Barcelona, Spain.
 
 Deterministic path
Latent  path
DecoderMLP
ZxiT
Z r#xiCyiCHierarchical 
Linear 
Symbolized
Tree 
xiCyiCCross -Tree
x1CxiC xiT
r#
synthesis
 y1Cx1C
yiCxiC
synthesisHierarchical
Linear
Symbolized  
Tree
yiT
Encoderx1Cy1C
x1CxiC 
KeyValue
Query
Hierarchical 
Symbolized 
Recursive 
Decomposition
Figure 2: The HLNPs model encoder structures information
from both the deterministic path and the latent path, while
the decoder predicts target sample points. The deterministic
path can learn hierarchical information through a hierar-
chicallinearsymbolizedtree-structurednetworkusingpa-
rameter 𝛾. The latent path manages intricate distributions
by modeling them in the form of a discrete distribution. The
above-mentionedcomplexdistributionisdecomposedinto
several simpler sub-problems for resolution through hier-
archical linear symbolization, involving sum and productnodes, thereby inferring the overall posterior distribution
ofthecomplexlatentdistribution.Thedecoder,havingac-
quired knowledge of latent complex representations and po-
sitional information encoding, predicts target sample points
through an MLP.
3.5 Tree-Structured Neural Processes
Inthissection,thepreviouslyintroducedhierarchicallinearsymbol-izedtree-structurednetworkiscombinedwithNPstolearndifferenthierarchicalinformationinatree-structuredmanner,achievingbet-
ter fitting of complex distributions. At the same time, the model
gainsinterpretability.Therefore,basedontheabovedescription,we
propose a named Hierarchical Linear Symbolized Tree-structured
Neural Process (HLNPs).
In previous NPs approaches, learning from sampling to context
samplepointscontrolsthegenerationofcomplexdistributionsby
either considering a single latent variable [ 11] or incorporating
autoregressivestackedlatentvariables[ 27].WeredesignedtheNPs
model based on the advantages of tree structure, such as hierar-
chy, efficient retrieval and search, and interpretability. The HLNPs
model, generated through the fusion of both, also possesses dis-tinct levels of information, interpretability, scalability, and other
characteristics.NPshavePermutationinvariance:themodelshould
be able to produce the same output or have the same properties
regardless of the order in which the input data are arranged. Weadoptatop-downsamplingapproachtoconstructatree-structured
network, implementing both a deterministic path and a latent path
to preserve the aforementioned characteristics. Compared to de-
terministicpathsandthecurrentlypopularself-attention( O(𝑛2))
mechanisms, a tree structure allows for searching and retrieval
within a lower time O(𝑙𝑜𝑔𝑛)complexity [ 16]. Learning complex
distributions in latent paths is challenging. We leverage tree struc-
ture and symbolization to decompose it into several simple sub-
problemsforresolution,thusachievingafunctionapproximation
for complex distributions.
ThestructureoftheHLNPsmodelisillustratedinFigure2.From
thestructuralperspectiveinFigure2,itisobservedthattheencoder
oftheHLNPsmodelisalsoimplementedthroughtwopathways:the
deterministic path and the latent path. In the deterministic path of
the encoder, 𝑥𝑇
𝑖serves as the query, while context sample contexts
tokensactaskeysandvalues.Thisprocessresultsinthegeneration
ofquery-specificrepresentation 𝑟#(𝑥𝐶
𝑖,𝑦𝐶
𝑖,𝑥𝑇
𝑖)throughcross-tree
learning. Unlike simple averaging or the self-attention mechanism
used to assign weights, the values in the hierarchical linear sym-bolizedtree-structurednetworkaredistinct.The lattercanlearn
weight information for context sample points at different levels.
The hierarchical linear symbolized tree comprises alternating sum
andproductnodes,commencingfromtherootnode.Eachsumnode
represents a sub-problem requiring Bayesian inference for a set of
contextsamplepoints.Foreachsumnode,thereexistsasetofprod-uct nodes that combine different context sample points to facilitate
information sharing between various sum nodes. Consequently,
the hierarchical linear tree structure efficiently learns information
aboutthehierarchicalnodelocations.Bayesianconditioning,de-
notedby 𝛾,mergeslocationinformationbetweendifferentlevels.
Forinstance,therootnodeinFig1isrepresentedas 𝛾1((1.2,0.8))+
𝛾2((1.2,0.8),(1.6,1.9))+𝛾3((1.2,0.8),(1.6,1.9),(0.4,2.3),(2.4,1.0))
(𝛾𝑖>0,/summationtext.1𝛾𝑖=1). By applying the same processing approach to
the latent distribution, approximating the latent distribution of
Bayesiannetworkstructuresgivenobservedcontextsamplepoints.
The latent distribution is generally complex and challenging to
calculate directly,especiallyin the case of largenetwork structuresandlimiteddata.Therefore,ahierarchicaldecompositionapproach
is proposed to approximate the posterior distribution. It involves a
tree-structuredhierarchicaldecompositionofthejointdistribution,
breakingitdownintosimplersub-problems,eachcorresponding
to a subset of variables in the network. The decomposition process
startswiththeentiresetofvariablesandrecursivelypartitionsthemintosmallersubsetsuntileachsubsetcontainsonlyonevariable.In
this process, the tree structure is represented using sum nodes and
product nodes to denote order and structure. In the above process,
marginal distribution, conditional distribution, and sampling infer-
enceareappliedtotheleafnodes(Refertoappendixfordetails).
The decoder follows the traditional NPs model in predicting the
target sample points [4].
ELBOParameterLearning. Aftercompletingtheconstruction
of the HLNPs model, we need to address the parameter estimation
problem using VI. During the VI, the hierarchical linear symbol-
ized tree-structured network is discretized. Give a non-normalized
distribution ˜𝑝(𝜋,𝐺)=𝑝𝐺(𝐺)𝟙𝐺|=𝜋, the ELBO is given by:
E𝑞𝜙(𝜋,𝐺)[log˜𝑝(𝜋,𝐺)]+𝐻/parenleftBig
𝑞𝜙(𝜋,𝐺)/parenrightBig
(5)
 
2822KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
where 𝐻/parenleftBig
𝑞𝜙(𝜋,𝐺)/parenrightBig
=−E𝑞𝜙(𝜋,𝐺)/bracketleftbig
log𝑞𝜙(𝜋,𝐺)/bracketrightbigistheentropyof
thehierarchicallinearsymbolizedtree-structurednetwork 𝑞.The
goalofVIistomaximizetheELBOwithrespectto 𝜙.DiscretizedVI
ischallengingduetotherequirementofcomputing(gradientsof)
theexpectationunderhigh-varianceconditions,suchasthoseen-
counteredwithmethodslikeREINFORCE,whicharenotapplicable
duetothediscretenature.Fortunately,thisproblemisaddressed
by the Processable Representation of a Tree-Structured Network.
Proposition3. AnyHierarchicalLinearSymbolicTree-Structured
Network 𝑞𝜙and a non-normalized distribution ˜𝑝(𝜋,𝐺)=𝑝𝐺(𝐺)
can be computed in linear time.
Proof.Weassumetheexistenceofahierarchicallinearsymbol-
ized tree-structured ˜𝑝(𝜋,𝐺)=/producttext.1
𝑖𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺|=𝜋. Define the 𝐷𝐶𝑠2
as˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig=/producttext.1
(𝑥𝐶
𝑖,𝑦𝐶
𝑖)∈𝐷𝐶𝑠2𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑠2=𝜋𝑠2. In the con-
text of a hierarchical linear symbolized tree structure network,
for any node 𝑁with scope (𝜋𝑠2,𝐺𝑠2, we will define the evidence
lower-bound when using the distribution 𝑁(𝜋𝑠2,𝐺𝑠2)to approxi-
mate˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig:
ELBO(𝑁)=E𝑁/bracketleftBig
˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightBig
+𝐻/parenleftbig𝑁/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/parenrightbig(6)
Now we will demonstrate that the ELBO for the distribution 𝑞𝜙
can be efficiently computed. This means the calculation can bedone in linear time with respect to the size of the hierarchical
linearsymbolizedtree-structurednetwork.Theefficiencyofthis
computation relies on the ELBO of the leaf node distributions.
Symbolizedsumnodes 𝑇areassociatedwith (𝐷𝐶𝑠1,𝐷𝐶𝑠2),withthe
weights denoted as 𝛾1, ...,𝛾𝑖for their child nodes. We can represent
the expectation and entropy of ˜𝑝𝐷𝐶𝑠2by decomposing it into the
distributions of its children. The formula is expressed as follows:
E𝑇/bracketleftBig
log˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightBig
=/summationdisplay.1
𝑖𝛾𝑖E𝑐ℎ(𝑇𝑖)/bracketleftBig
log˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightBig
(7)
where 𝑐ℎ(𝑇𝑖)representthe 𝑖thchildnode.Theformulaforuncer-
tainty in entropy calculation is as follows:
𝐻/parenleftbig𝑇/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/parenrightbig=−E𝑇/bracketleftbig
log𝑇/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightbig
=−/summationdisplay.1
𝑖𝛾𝑖E𝑐ℎ(𝑇𝑖)/bracketleftbig
log𝛾𝑖𝑐ℎ(𝑇𝑖)/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightbig
=−/summationdisplay.1
𝑖𝛾𝑖log𝛾𝑖+/summationdisplay.1
𝑖𝛾𝑖E𝑐ℎ(𝑇𝑖)/bracketleftbig
−𝑐ℎ(𝑇𝑖)/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightbig
=−/summationdisplay.1
𝑖𝛾𝑖log𝛾𝑖+/summationdisplay.1
𝑖𝛾𝑖𝐻/parenleftbig𝑐ℎ(𝑇𝑖)/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/parenrightbig
(8)
Combining the above 7 and 8, it is represented as follows:
𝐸𝐿𝐵𝑂(𝑇)=E𝑇/bracketleftBig
log˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightBig
+𝐻/parenleftbig𝑇/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/parenrightbig
=−/summationdisplay.1
𝑖𝛾𝑖log𝛾𝑖+/summationdisplay.1
𝑖𝛾𝑖𝐸𝐿𝐵𝑂(𝑐ℎ(𝑇𝑖))(9)
From the above formula, it can be observed that we decompose
the complex expected distribution into the expectations of several
sub-distributions, and then sum them with different weights. Inthe computation of entropy uncertainty, the entropy of the sum
nodeweightsdecomposesintoaweightedsumofentropiesfrom
distinct sub-distributions. Just so you know, an important assump-
tion exists in the decomposition of entropy. The children nodesof the 𝑇weighted sum symbol node have disjoint support sets,
henceE𝑐ℎ(𝑇𝑖)/bracketleftbig
𝑐ℎ(𝑇𝑗)(𝜋,𝐺)/bracketrightbig
=0(𝑖≠𝑗). From the above, we can
conclude that the ELBO of a sum node can be decomposed into the
ELBO of its individual child nodes.
Symbolproductnode 𝑀,associatedwithdataset (𝐷𝐶𝑠21,𝐷𝐶𝑠22).The
symbol 𝑀represents a distribution over/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig,where 𝐷𝐶𝑠2=
𝐷𝐶𝑠21∪𝐷𝐶𝑠22. We can obtain the following:
𝐸𝐿𝐵𝑂(𝑀)=E𝑀/bracketleftBig
log˜𝑝𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/bracketrightBig
+𝐻/parenleftbig𝑀/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/parenrightbig
=E𝑀/bracketleftBig
log˜𝑝𝐷𝐶𝑠21/parenleftbig𝜎𝑆21,𝐺𝑆21/parenrightbig/bracketrightBig
+𝐻/parenleftbig𝑐ℎ(𝑀1)/parenleftbig𝜋𝑠21,𝐺𝑠21/parenrightbig/parenrightbig
+E𝑀/bracketleftBig
log˜𝑝𝐷𝐶𝑠22/parenleftbig𝜋𝑠22,𝐺𝑆22/parenrightbig/bracketrightBig
+𝐻/parenleftbig𝑐ℎ(𝑀2)/parenleftbig𝜎𝑠22,𝐺𝑠22/parenrightbig/parenrightbig
=𝐸𝐿𝐵𝑂(𝑐ℎ(𝑀1))+𝐸𝐿𝐵𝑂(𝑐ℎ(𝑀2))
(10)
The feasibility mentioned above comes from decomposability,en-
suringthenon-intersectionbetweenchilddistributions.Byrecur-
sivelyapplyingtheaboveequations,wecanexpresstheELBOof
the HLNPs model using the weights and ELBO of the leaf node
distributions.Symbolsumandproductareutilizedforeachchild
node.
Besides,itisnecessarytocomputetheELBOfortheleafnode
distributions after the decomposition process. For the leaf node
L associated with (𝐷𝐶𝑠1,(𝑥𝐶
𝑖,𝑦𝐶
𝑖))and the context sample point
(𝑥𝐶
𝑖,𝑦𝐶
𝑖),itsparentnodedistributionisrepresentedby 𝐿(𝐺𝑖).We
can express it as follows:
𝐸𝐿𝐵𝑂(𝐿)=E𝐿/bracketleftBig
log˜𝑝/parenleftBig
𝜋{𝑖},𝐺𝑖/parenrightBig/bracketrightBig
+𝐻/parenleftBig
𝐿/parenleftBig
𝜋{𝑖},𝐺𝑖/parenrightBig/parenrightBig
=E𝐿/bracketleftbig
log𝑝𝐺𝑖(𝐺𝑖)/bracketrightbig
+𝐻(𝐿(𝐺𝑖))(11)
Werequirethat 𝐿hassupportonlyover 𝐺𝑖⊆𝐷𝐶𝑠1.Itsupport 𝐿(𝐺𝑖)
approximating 𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1. The aforementioned ELBO leaf
node distribution can be rewritten as follows:
𝐸𝐿𝐵𝑂(𝐿)=E𝐿/bracketleftbig
log𝑝𝐺𝑖(𝐺𝑖)/bracketrightbig
+𝐻(𝐿(𝐺𝑖))/parenrightbig
=E𝐿/bracketleftbig
log𝑝𝐺𝑖(𝐺𝑖)/bracketrightbig
−E𝐿[log𝐿(𝐺𝑖)]
=−𝐾𝐿/parenleftbig𝐿/bardbl𝑝𝐺𝑖/parenrightbig(12)
KLrepresentstheKLdivergencemeasure.Maximizingthesim-
plified ELBO above is essentially minimizing the distribution diver-
gence. We continue to simplify the above results:
𝐸𝐿𝐵𝑂(𝐿)=E𝐿/bracketleftbig
log𝑝𝐺𝑖(𝐺𝑖)/bracketrightbig
+𝐻(𝐿(𝐺𝑖))/parenrightbig
=E𝐿/bracketleftbig
log𝑝𝐺𝑖(𝐺𝑖)/bracketrightbig
−E𝐿[log𝐿(𝐺𝑖)]
=−E𝐿/bracketleftBigg
log𝐿(𝐺𝑖)
𝑝𝐺𝑖(𝐺𝑖)//summationtext.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)/bracketrightBigg
+log/summationdisplay.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)(13)
 
2823Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD ’24, August 25–29, 2024, Barcelona, Spain.
It support 𝐿(𝐺𝑖)approximating 𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1. The formula 13
mentioned above can be rewritten as follows:
𝐸𝐿𝐵𝑂(𝐿)=−𝐾𝐿/parenleftBigg
𝐿/bardbl𝑝𝐺𝑖𝟙𝐺𝑖⊆𝐷𝐶𝑠1/summationtext.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)/parenrightBigg
+log/summationdisplay.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)
=log/summationdisplay.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)(14)
Weobservethat,withthedeterminationoftheLnode,theELBO
becomes a constant.
InterpretabilityofTree-StructuredNeuralProcesses. The
hierarchical linear symbolized tree-structured network utilizes
Bayesian inference for the weights of edges. We find that causalinference relationships form between different nodes during thetree construction processes. The product node acts as a role in
weight allocation, analogous to a method of path coefficients (1934,
Wright). i.e. We consider the deterministic path in a given graph 𝐺
withweights 𝛾.andcausalinferencebetween (𝑥𝐶
𝑖,𝑦𝐶
𝑖)and(𝑥𝐶
𝑗,𝑦𝐶
𝑗)
, represented by 𝐶𝐼𝑖𝑗(𝛾). The inference from (𝑥𝐶
𝑖,𝑦𝐶
𝑖)to(𝑥𝐶
𝑗,𝑦𝐶
𝑗)
is a directed path where the weights are added. If the nodes arenot in an ancestor relationship, then
𝐶𝐼𝑖𝑗(𝛾)=0. When utilizing
weights in 𝐷𝐶
𝑆1and the weights 𝛾of the graph is unknown, the
representation of its expected inference is as follows:
𝐶𝐼𝑖𝑗=/summationdisplay.1
𝐿𝑃∈𝐷𝐶𝑠𝑚\{(𝑥𝐶
𝑖,𝑦𝐶
𝑖),(𝑥𝐶
𝑗,𝑦𝐶
𝑗)})𝛾{𝑖,𝐿𝑃1}𝛾{𝐿𝑃|𝐿𝑃|,𝑗}
|𝐿𝑃|−1/productdisplay.1
𝑖=1𝛾{𝐿𝑃𝑖,𝐿𝑃𝑖+1}(15)
where 𝛾{𝑖,𝐿𝑃1}represents the weight of the edge from context sam-
ple points (𝑥𝐶
𝑖,𝑦𝐶
𝑖)to(𝑥𝐶
𝐿𝑃1,𝑦𝐶
𝐿𝑃1),and 𝛾{𝐿𝑃|𝐿𝑃|,𝑗}represents the
weight ofthe edgefrom (𝑥𝐶
𝐿𝑃|𝐿𝑃|,𝑦𝐶
𝐿𝑃|𝐿𝑃|)to(𝑥𝐶
𝑗,𝑦𝐶
𝑗). Theproduct
of these two weights represents the multiplication of edge weights
along the path, used to consider the contribution of that path to
the causal effect. The entire formula calculates the expected causal
effect by summing over all possible paths, considering all directed
pathsfrom contextsample points (𝑥𝐶
𝑖,𝑦𝐶
𝑖)to(𝑥𝐶
𝑗,𝑦𝐶
𝑗).In thecon-
text of the HLNPs model, uncertainty is treated as a part of the
probability output. What we need to adopt is Bayesian model aver-
aging,wheretheestimateofeachmodelisweightedbyitsposteriorprobability,resultinginacomprehensiveestimation.Theaforemen-
tioned process is known as the average causal effect. This is as
follows:
BCE((𝑥𝐶
𝑖,𝑦𝐶
𝑖),(𝑥𝐶
𝑗,𝑦𝐶
𝑗))/definesE𝐺∼𝑞𝜙(𝐺)/bracketleftbig
E𝛾∼𝑞(𝛾|𝐺)/bracketleftbig
𝐸𝑖𝑗(𝛾)/bracketrightbig/bracketrightbig
(16)
Therefore, the entire formula signifies calculating the causal effect
of(𝑥𝐶
𝑖,𝑦𝐶
𝑖)on(𝑥𝐶
𝑗,𝑦𝐶
𝑗)given observed pairs of context sample
pointsandthecausalgraph.Itinvolvestakingtheexpectationover
allpossiblecausalgraphsandcausalparameters,resultinginthe
Bayesian averaged causal effect.
Figure3:ResultsforbayesianoptimizationonHLNPsmodels.
we measured normalized simple regret and its cumulative
value for an iteration.
4 Experiment
4.1 meta-regression (1D data)
WeevaluateLatentHierarchicalLinearSymbolTree-structuredNet-
workNeuralProcesses(HLNPs)onseveraltasks:meta-regression
(1Ddata)andimagecompletion.WecomparethevariantsofNPs
as follows: Conditional Neural Processes (CNPs)[ 10]1, Neural Pro-
cesses (NPs)[ 11], Attentive Neural Processes( ANPs)[ 16]2, Boot-
strapping Neural Processes (BNPs)[ 20]3, Transformer Neural Pro-
cesses(TNPs)[ 24]4,AutoregressiveNeuralProcesses(AENPs)[ 27],
ConditionalAutoregressiveNeuralProcesses(CAENPs)[ 27],Ver-
satile Neural Processes (VNPs) [13]5.
Table 1: The mean and standard deviation of the five runs
are reported (MSE measures).
Method RBF kernels Matérn 5/2 Periodic
CNPs 0.278 ±0.003 0.310 ±0.003 0.652 ±0.001
NPs 0.282 ±0.003 0.315 ±0.003 0.650 ±0.002
ANPs 0.193 ±0.001 0.230 ±0.000 0.703 ±0.002
BNPs 0.269 ±0.003 0.301 ±0.003 0.649 ±0.002
TNPs 0.177 ±0.001 0.222 ±0.000 0.670 ±0.009
VNPs 0.162 ±0.003 0.201 ±0.000 0.642 ±0.007
AENPs 0.152 ±0.003 0.217 ±0.001 0.593 ±0.005
CAENPs 0.149 ±0.001 0.199 ±0.003 0.524 ±0.001
HLNPs 0.103±0.002 0.127 ±0.001 0.391 ±0.003
We need to perform bayesian optimization on the benchmark
functions used in training with the RBF kernel [ 2]. To assess the
model’s optimal performance, we employ the best simple regret,
whichmeasuresthedifferencebetweenthebestvalueinthemodel
andtheglobaloptimum.Fig3showstheaveragenormalizedregret
andcumulativenormalizedregretafter100trialsofthefunction.
TheresultsinthefiguredemonstratethattheHLNPsmodelexhibits
the best performance in both normalized regret and cumulative
normalized regret.
1https://github.com/stratisMarkou/conditional-neural-processes
2https://github.com/soobinseo/Attentive-Neural-Process
3https://github.com/juho-lee/bnp
4https://github.com/tung-nd/TNP-pytorch
5https://github.com/ZongyuGuo/Versatile-NP
 
2824KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
Inthissection,thecurvesinthedatasetaregeneratedusingfour
different configurations of 1D Gaussian processes: i) RBF kernel, ii)
Matern5/2kernel,iii)Periodickernel.TheHLNPsmodelobserves 𝑛
contextsamplepointsasthetrainingsetand 𝑛†samplepointsfrom
the same distribution as the test set. Furthermore, we estimate the
MSEmeasurebasedonthesampledtargetsamplepoints.HLNPs
experimental results are shown in table 1. The results from table 1
reveal that HLNPs outperform the variants of NPs by a significant
margin.
4.2 Image Completion
In this section, the model uses a portion of pixel values from the
observed images as context sample points to predict the remaining
pixel values in the images. Therefore, the above 𝑥𝐶
𝑖represents the
pixelposition,andthecorresponding 𝑦𝐶
𝑖representsthepixelvalue.
Duringtheexperimentalprocess,foreaseofhandling,wescaled
the pixel position value 𝑥𝐶
𝑖to [-1, 1] and the pixel value 𝑦𝐶
𝑖to [-0.5,
0.5][9].Theaboveexperimentinvolvedrandomlysamplingcontext
sample points and target sample points. We use two datasets for
experimentation: MNIST [7], CelebA [27].
TheimagesintheMNISTdatasethaveasizeof28 ×28pixels.We
samplecontextsamplepoints 𝑛∼U[3,197),and 𝑛†∼U[3,200−𝑛)
targetdatapoints.ForMNIST16,theimagesaredownsampledto
16×16 and 𝑛∼U[3,97), and 𝑛†∼U[3,100−𝑛). The images in
the CelebA dataset have a size of 178 ×218 pixels. For CelebA32,
the images are down sampled to 32 ×32 and 𝑛∼U[3,197), and
𝑛†∼U[3,200−𝑛).ForCelebA64,theimagesaredownsampledto
64×64 and 𝑛∼U[3,797), and 𝑛†∼U[3,800−𝑛). For CelebA128,
theimagesaredownsampledto128 ×128and 𝑛∼U[3,1597),and
𝑛†∼U[3,1600−𝑛). The above results are shown in table 2 and
3.Fromtheabovetable,itcanbeobservedthattheresultsofthe
HLNPs model are better than variants of other NPs models.
Table 2: Each method is evaluated with 5 different seeds
according to the log-likelihood (CelebA dataset).
Method CelebA
32×32 64 ×64 128 ×128
CNPs 2.15 ±0.01 2.43 ±0.00 2.55 ±0.02
NPs 2.48 ±0.02 2.60 ±0.01 2.67 ±0.01
ANPs 2.90 ±0.00 3.12 ±0.00 3.72 ±0.03
BNPs 2.76 ±0.01 2.97 ±0.00 3.12 ±0.02
TNPs 2.37 ±0.01 2.42 ±0.03 3.10 ±0.00
VNPs 3.14 ±0.01 3.28 ±0.01 3.31 ±0.01
AENPs 3.02 ±0.00 3.21 ±0.00 3.85 ±0.01
CAENPs 3.16 ±0.02 3.43 ±0.02 3.86 ±0.00
HLNPs (ours) 3.41±0.01 3.98 ±0.00 4.03 ±0.00
4.3 Ablation Study
𝐾factor allocation of pending context sample points. Forthe
HL-NPs, the selection of the number of pending context sample
points 𝐷𝐶𝑠2duringtheconstructionprocesssignificantlyimpactstheTable 3: Each method is evaluated with 5 different seeds
according to the log-likelihood(MNIST dataset).
Method MNIST
28×28 16 ×16
CNPs 4.06 ±0.01 3.76 ±0.01
NPs 4.13 ±0.00 3.81 ±0.00
ANPs 4.27 ±0.02 3.95 ±0.02
BNPs 4.34 ±0.01 3.88 ±0.01
TNPs 4.22 ±0.02 3.79 ±0.02
VNPs 4.57 ±0.03 3.84 ±0.03
AENPs 4.78 ±0.01 3.85 ±0.01
CAENPs 4.83 ±0.00 3.96 ±0.00
HLNPs (ours) 5.14±0.01 4.20 ±0.01
inferenceresults.Forinstance,therootnodeinFig1isrepresentedas
𝛾1((1.2,0.8))+𝛾2((1.2,0.8),(1.6,1.9),(0.4,2.3))+𝛾3((1.2,0.8),(1.6,
1.9),(0.4,2.3),(2.4,1.0))(𝛾𝑖>0,/summationtext.1𝛾𝑖=1). We allocate 𝐾=(1,𝑛−
1),𝐾=(𝑛/4,3𝑛/4),𝐾=(𝑛/2,𝑛/2),𝐾=(3𝑛/4,𝑛/4),𝐾=(𝑛−1,1)
in order to achieve the distribution. The results of the above al-location are shown in table 4 and 5 . From the above results, it
isobservedthatdifferentallocationmethodsforpendingsample
points 𝐷𝐶𝑠2in table 4 and 5 show different performances. When the
HLNPs model samples more context sample points, the greater the
numberassigned by 𝐷𝐶𝑠2to𝐷𝐶𝑠21,so choosedifferent 𝐾parameters
to construct a HLNPs according to different scenarios.
Table4:Differentpartitionmethodsfor 𝐾pendingcontext
sample points (CelebA dataset).
𝐾 CelebA
32×32 64 ×64 128 ×128
𝐾=(1,𝑛−1)3.41±0.013.98±0.00 4.03 ±0.02
𝐾=(𝑛/4,3𝑛/4)3.18±0.014.05±0.014.00±0.01
𝐾=(𝑛/2,𝑛/2)3.27±0.01 3.86 ±0.004.18±0.01
𝐾=(3𝑛/4,𝑛/4)3.24±0.00 3.82 ±0.01 3.94 ±0.02
𝐾=(𝑛−1,1)3.17±0.01 3.78 ±0.01 3.89 ±0.02
Table5:Differentpartitionmethodsfor 𝐾pendingcontext
sample points (MNIST dataset).
𝐾 MNIST
28×28 16 ×16
𝐾=(1,𝑛−1)5.14±0.014.20±0.00
𝐾=(𝑛/4,3𝑛/4)5.07±0.01 4.11 ±0.01
𝐾=(𝑛/2,𝑛/2)4.98±0.024.57±0.00
𝐾=(3𝑛/4,𝑛/4)4.92±0.00 4.16 ±0.01
𝐾=(𝑛−1,1)4.31±0.01 4.09 ±0.01
 
2825Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD ’24, August 25–29, 2024, Barcelona, Spain.
Tree structure model. Thetree-likestructuregreatlyenhances
themodel’sinterpretability,handlingofnonlinearrelationships,ro-
bustness,andsoon[ 22,25].Drozdov[ 8]employstreestructuresto
implement inside-outside recursive autoencoding, achieving unsu-
pervised learning of noun phrases and entities from unlabeled text.
Zeng[31]addressesthepermutationofclasslabelsinsupervised
classification problems by leveraging tree structures to preserve
classrelationships,whilealsoprovidingametricintheformoftree
nodes.
4.4 Related Work
Neural processes and variations. Garnelloetal[ 10]havetheprob-
lem of high computing cost and difficulty in finding a prior in GPs.
Nowadays popular neural networks can make accurate predictions
by gradient descent[ 14,21]. These authors combine the two to pro-
duceConditionalNeuralProcesses(CNPs)andcanscaletolargedatasets by observing only a few sample points. CNPs use fixed-
dimensioninputinthedataencodingprocess,resultinginalack
of flexibility in output. On this basis, NPs[ 12] enrich their encoded
representation by introducing latent variables. The aggregation
approach of encoders in NPs causes sample point underfitting. At-
tentive Neural Processes(ANPs)[ 3] dynamically assign power to
sample points in thisaggregation with multi-headed attention. In
practice, NPs have serious flaws in sequential decision-making,combined with the temporal function in the current transformerto produce Transformer Neural Processes (TNPs)[
24]. Convolu-
tionalConditionalNeuralProcesses(CCNPs)solveNPstomodel
the translational equivariance in the data (ie, time series, spatial
data, text data). This model extends the data processing from finite
to infinite dimensionality.
Tree structure model. Thetree-likestructuregreatlyenhances
themodel’sinterpretability,handlingofnonlinearrelationships,ro-bustness,andsoon[
22,25].Drozdov[ 8]employstreestructuresto
implement inside-outside recursive autoencoding, achieving unsu-
pervised learning of noun phrases and entities from unlabeled text.
Zeng[31]addressesthepermutationofclasslabelsinsupervised
classification problems by leveraging tree structures to preserve
classrelationships,whilealsoprovidingametricintheformoftree
nodes.
Symbolic Network. Recently, the SGCN work leverages atten-
tion to design a signed attention network, representing the links in
thenetworkwithpositiveandnegativeattention.Buildinguponthis, the paper combines the ideas with ANPs [
16] to design a
PositiveandNegativeSignAttentionHierarchicalTreeStructure
Neural Processes (PSNPs)[ 6,15] model. However, compared to the
approach in this paper, this method oversimplifies the approxima-
tionofcomplexdistributions(representingpositiveandnegativeat-
tention with Gaussian distributions) and yields lower performance
than the proposed HLNPs model in this paper.
5 Conclusion
Inthispaper,weproposetheHLNPsmodelasavariantofNPs.The
HLNPs constructs a hierarchical linear symbolized tree networkin a top-down manner. In the deterministic path, this approachallows for richer positional information representations and re-
duces time complexity. In the latent path, complex distributionsarediscretized,andthrougharecursiveprocessinvolvingsumor
product, symbolized nodes are transformed into a set of simpler
sub-problemsforresolution.HLNPsimpartinterpretabilitytothe
causalrelationships betweencontextsample pointsinthe formof
’coefficientpaths’.Thetreestructurediscretizesvariablestocapture
model uncertainty in the form of entropy. In future work, we will
apply the HLNPs model to 3D scenarios.
5.1 Acknowledgements
This work was supported by the National Natural Science Founda-
tion of China under grant No. 61936001.
References
[1]Guangji Bai, Chen Ling, and Liang Zhao. 2023. Temporal Domain General-
ization with Drift-Aware Dynamic Neural Networks. In The Eleventh Interna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
sWOsRj4nT1n
[2]EricBrochu,VladMCora,andNandoDeFreitas.2010. AtutorialonBayesian
optimizationofexpensivecostfunctions,withapplicationtoactiveusermodeling
and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599 (2010).
[3]WesselBruinsma,StratisMarkou,JamesRequeima,AndrewY.K.Foong,Tom
Andersson,AnnaVaughan,AnthonyBuonomo,ScottHosking,andRichardE
Turner. 2023. Autoregressive Conditional Neural Processes. In The Eleventh
International Conference on Learning Representations. https://openreview.net/
forum?id=OAsXFPBfTBh
[4]Wenlin Chen, Austin Tripp, and José Miguel Hernández-Lobato. 2023. Meta-
learningAdaptiveDeep KernelGaussianProcessesforMolecular PropertyPre-
diction. In The Eleventh International Conference on Learning Representations .
https://openreview.net/forum?id=KXRSh0sdVTP
[5]Tristan Deleu, Mizu Nishikawa-Toomey, Jithendaraa Subramanian, Nikolay
Malkin,LaurentCharlin,andYoshuaBengio.2023. JointBayesianInferenceof
GraphicalStructure andParameters witha SingleGenerative FlowNetwork.In
ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling.
https://openreview.net/forum?id=4NMp0QFqwH
[6]Mingyu Ding, Yikang Shen, Lijie Fan, Zhenfang Chen, Zitian Chen, Ping Luo,
JoshuaB.Tenenbaum,andChuangGan.2023. VisualDependencyTransform-
ers:DependencyTreeEmergesFromReversedAttention.In Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 14528–
14539.
[7]AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International Conference on Learning
Representations.
[8]Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew Mc-
Callum. 2019. Unsupervised Latent Tree Induction with Deep Inside-Outside
RecursiveAuto-Encoders.In Proceedingsofthe2019ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and
ThamarSolorio(Eds.).AssociationforComputationalLinguistics,Minneapolis,
Minnesota, 1129–1141. https://doi.org/10.18653/v1/N19-1116
[9]LeoFeng,HosseinHajimirsadeghi,YoshuaBengio,andMohamedOsamaAhmed.
2023. LatentBottleneckedAttentiveNeuralProcesses.In TheEleventhInterna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
yIxtevizEA
[10]Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David
Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami.
2018. Conditional Neural Processes. In Proceedings of the 35th International
Conference on Machine Learning, Jennifer Dy and Andreas Krause (Eds.), Vol. 80.
1704–1713.
[11]Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J.
Rezende, S. M. Ali Eslami, and Yee Whye Teh. 2018. Neural Processes. ICML
2018workshop abs/1807.01622(2018). arXiv:1807.01622 http://arxiv.org/abs/1807.
01622
[12]Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. 2023.
VersatileNeuralProcessesforLearningImplicitNeuralRepresentations.In ICLR.
[13]Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. 2023.
Versatile Neural Processes for Learning Implicit Neural Representations. In The
EleventhInternationalConferenceonLearningRepresentations. https://openreview.
net/forum?id=2nLeOOfAjK
[14]Ayoub El Hanchi, David Stephens, and Chris Maddison. 2022. Stochastic
Reweighted Gradient Descent. In Proceedings of the 39th International Conference
 
2826KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
on Machine Learning (Proceedings of Machine Learning Research, Vol. 162) , Kama-
likaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,andSivan
Sabato (Eds.). PMLR, 8359–8374. https://proceedings.mlr.press/v162/hanchi22a.
html
[15]Junjie Huang, HuaweiShen, Liang Hou, and Xueqi Cheng.2019. Signed Graph
Attention Networks. CoRRabs/1906.10958 (2019). arXiv:1906.10958 http://arxiv.
org/abs/1906.10958
[16]Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, S. M. Ali Eslami,
Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. 2019. Attentive Neural
Processes. InInternationalConferenceonLearningRepresentations abs/1901.05761.
[17]DiederikP.KingmaandMaxWelling.2014. Auto-EncodingVariationalBayes.In
2ndInternationalConferenceonLearningRepresentations,ICLR2014,Banff,AB,
Canada, April 14-16, 2014, Conference Track Proceedings.
[18]Hyungi Lee, Eunggu Yun, Giung Nam, Edwin Fong, and Juho Lee. 2023. Mar-
tingalePosteriorNeuralProcesses.In TheEleventhInternationalConferenceon
Learning Representations. https://openreview.net/forum?id=-9PVqZ-IR_
[19]HyungiLee,EungguYun,HongseokYang,andJuhoLee.2022. ScaleMixtures
ofNeuralNetworkGaussianProcesses.In InternationalConferenceonLearning
Representations. https://openreview.net/forum?id=YVPBh4k78iZ
[20]JuhoLee,YoonhoLee,JungtaekKim,EunhoYang,SungJuHwang,andYeeWhye
Teh. 2020. Bootstrapping neural processes. NeurIPS.
[21]Wu Lin, Frank Nielsen, Khan Mohammad Emtiyaz, and Mark Schmidt. 2021.
Tractable structured natural-gradient descent using local parameterizations. In
Proceedingsofthe38thInternationalConferenceonMachineLearning (Proceedings
of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.).
PMLR, 6680–6691. https://proceedings.mlr.press/v139/lin21e.html
[22]SaschaMarton,ChristianBartelt,andStefanLüdtke.2023. LearningAxis-Aligned
Decision Trees with Gradient Descent. https://openreview.net/forum?id=
gwizseh-Iam
[23]IvonaNajdenkoska,XiantongZhen,andMarcelWorring.2023. MetaLearning
to Bridge Vision and Language Models for Multimodal Few-Shot Learning. In
The Eleventh International Conference on Learning Representations.
[24]Tung Nguyen and Aditya Grover. 2022. Transformer Neural Processes:
Uncertainty-Aware Meta Learning Via Sequence Modeling. In Proceedings of
the39thInternationalConferenceonMachineLearning (ProceedingsofMachine
Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 16569–16594.
https://proceedings.mlr.press/v162/nguyen22b.html
[25]Alizée Pace, Alex Chan, and Mihaela van der Schaar. 2022. POETREE: Inter-
pretable Policy Learning with Adaptive Decision Trees. In International Confer-
enceonLearningRepresentations. https://openreview.net/forum?id=AJsI-ymaKn_
[26]FabioSigrist.2023. LatentGaussianModelBoosting. IEEETransactionsonPattern
Analysis and Machine Intelligence 45, 2 (2023), 1894–1905. https://doi.org/10.
1109/TPAMI.2022.3168152
[27]Jinyang Tai. 2023. Global Perception Based Autoregressive Neural Processes. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
10487–10497.
[28]Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko Koivisto. 2020. To-
wards scalable bayesian learning of causal dags. Advances in Neural Information
Processing Systems 33 (2020), 6584–6594.
[29]Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al .2016. Match-
ing networks for one shot learning. In Advances in Neural Information Processing
Systems. 3630–3638.
[30]Qi Wang, Marco Federici, and Herke van Hoof. 2023. Bridge the Inference
Gaps of Neural Processes via Expectation Maximization. In The Eleventh Interna-
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id=
A7v2DqLjZdq
[31]SiqiZeng,RemiTachetdesCombes,andHanZhao.2023. LearningStructured
Representations by Embedding Class Hierarchy. In The Eleventh International
Conference on Learning Representations. https://openreview.net/forum?id=7J-
30ilaUZM
A APPENDIX
A.1The decomposability of Hierarchical Linear
Symbolized Tree-Structured Network
In the third section, we represent the construction process table
of the context sample point dataset 𝐷𝐶𝑚as graph (𝜋,𝐺).W er e p -
resent the above context sample points divided into two distri-
butions as/parenleftbig𝜋𝑠1,𝐺𝑠1/parenrightbigand/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig. Our construction condition is
𝜋=(𝜋𝑠1,𝜋𝑠2),with 𝐷𝑠1processedbefore 𝐷𝑠2duringthesamestage.
We have now demonstrated that the results obtained through com-
putation exhibit broad applicability, allowing us to decompose thedistribution hierarchically and thereby giving rise to the proposed
hierarchical tree structure.
Weconstruct 𝑝𝐺(𝜋,𝐺)=𝟙𝐺𝑖|=𝜋usingahierarchicallinearsym-
bolizedtree-structurednetworkoncontextsamplepointsin 𝐷𝐶𝑚.As-
suming 𝐷𝐶𝑠1and𝐷𝐶𝑠2arejointsubsetsof 𝐷𝐶𝑚,and 𝐷𝐶𝑠2canbedecom-
posed into/parenleftBig
𝐷𝐶𝑠1,𝐷𝐶𝑠22/parenrightBig
. We represent the above distribution on the
unnormalized distribution as ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2|𝜋𝑠2=/parenleftbig𝜋𝑠21,𝜋𝑠22/parenrightbig/parenrightbig,
forwhichtheapproximationbreaksdowninto ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠21,𝐺𝑠21/parenrightbig
˜𝑝𝐷𝐶𝑠1∪𝐷𝐶𝑠21,𝐷𝐶𝑠22/parenleftbig𝜋𝑠22,𝐺𝑠22/parenrightbig
Proof.By the above definition, we have ˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2/parenrightbig/defines/producttext.1
𝑖∈𝑠2𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝑠1∪(𝜋𝑠2<𝑖).Intheconstructionprocess,condi-
tional operations 𝜋𝑠2=(𝜋𝑠21,𝜋𝑠22). We have that:
˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠2,𝐺𝑠2|𝜋𝑠2=/parenleftbig𝜋𝑠21,𝜋𝑠22/parenrightbig/parenrightbig
∝/productdisplay.1
𝑖∈𝑠2𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1∪(𝜋𝑠2<𝑖)
=/productdisplay.1
𝑖∈𝑠21𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1∪(𝜋𝑠21<𝑖)
/productdisplay.1
𝑖∈𝑠22𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1∪𝐷𝐶𝑠21∪(𝜋𝑠22<𝑖)
=˜𝑝𝐷𝐶𝑠1,𝐷𝐶𝑠2/parenleftbig𝜋𝑠21,𝐺𝑠21/parenrightbig˜𝑝𝐷𝐶𝑠1∪𝐷𝐶𝑠21,𝐷𝐶𝑠22/parenleftbig𝜋𝑠22,𝐺𝑠22/parenrightbig(17)
Proof complete.
A.2 Consistency in Hierarchical Linear
Symbolized Tree-Structured Network
For any context sample point dataset 𝐷𝐶𝑚as graph (𝜋,𝐺), then we
consider the graph 𝐺to be consistent (𝐺|=𝜋).
Proof.Contextsamplepointsconstructa completehierarchical
linearsymbolizedtree-structurednetworktop-down:i)Selecting
onechild fromeachsumnode forprocessing.ii) Selectingallchil-
drenfromeachproductnodeforprocessing.iii)Selectinguntilall
leafnodes(contextsamplepoints)havebeenprocessed.Throughan
orderedrecursiveprocess,contextdatapointsformahierarchical
linearsymbolizedtree-structurednetwork.It’sworthnotingthatin
constructingthistreenetwork,thefinalleafnodesaredeterministic.
Duringtheconstructionprocess, 𝐷𝐶𝑠1determinestheorderofthe
nextpendingcontextsamplepointsbasedonthecontextsample
pointsaboveandbelowit.Inotherwords,fortheabove-mentioned
network, differentpartitioned context data points (𝐷𝐶𝑠1,𝐷𝐶𝑠21,𝐷𝐶𝑠22)
are connected through the product node 𝑀. Simultaneously, the
node 𝑀determines 𝜋𝑠21and𝜋𝑠22. Finally, the sum node in this
structureselectsachildnode.Theallocationoftheaforementioned
context sample points is represented by 𝜋.
Now, construct the tree structure from the root node to the leaf
nodes in a top-down fashion. We associate pending context sample
points 𝐷𝐶𝑠2through the sum node 𝑇. We prove by induction that,
for each sum node 𝑇𝑖in the hierarchical linear symbolized tree-
structured network, the ordering 𝜋of context sample points in
handling context sample points from 𝐷𝐶𝑠1obtained from 𝐷𝐶𝑠2.
i)Therootnodeistheunionofallcontextsamplepointsdatasets
(𝐷𝐶𝑠1,𝐷𝐶𝑠2)=(∅,𝐷𝐶𝑚), so all conditions are satisfied.
ii)Now,withinthehierarchicallinearsymbolizedtree-structured
network, consider a product symbol node 𝑀𝑖, such that 𝑀𝑖+1could
 
2827Hierarchical Linear Symbolized Tree-Structured Neural Processes KDD ’24, August 25–29, 2024, Barcelona, Spain.
be either the first child 𝐷𝐶𝑠21or the second child 𝐷𝐶𝑠22of𝑀𝑖. The
objectivehere istoassociate 𝑀𝑖with(𝐷𝐶𝑠1,𝑖,𝐷𝐶𝑠21,𝑖,𝐷𝐶𝑠22,𝑖).Then,(i)
If𝑀𝑖+1isthefirstchild 𝐷𝐶𝑠21inthehierarchicallinearsymbolized
tree-structurednetwork,then (𝐷𝐶𝑠1,𝑖+1𝐷𝐶𝑠21,𝑖+1)=(𝐷𝐶𝑠1,𝑖𝐷𝐶𝑠21,𝑖)holds.
Similarly,(ii)if 𝑀𝑖+1isthesecondchild 𝐷𝐶𝑠22,then(𝐷𝐶𝑠1,𝑖+1,𝐷𝐶𝑠2,𝑖+1)=
(𝐷𝐶𝑠1,𝑖∪𝐷𝐶𝑠21,𝑖,𝐷𝐶𝑠22,𝑖).N o w , 𝜋possesses the property of such or-
dering, where all processed context sample points in 𝐷𝐶𝑠21,𝑖come
before those in 𝐷𝐶𝑠22,𝑖. Building on the aforementioned inductive
assumptionthat 𝐷𝐶𝑠1,𝑖precedes 𝐷𝐶𝑠2,𝑖intheordering,astraightfor-
ward conclusion can be drawn in both cases (i) and (ii): during the
processingof contextsample points,all processedcontext sample
points in 𝐷𝐶𝑠1,𝑖+1precede those in 𝐷𝐶𝑠2,𝑖+1.
Theabovedescriptionimpliesthat,atanyleafnodeassociated
withthe pendingcontext samplepoint (𝑥𝐶
𝑖,𝑦𝐶
𝑖),𝐷𝐶𝑠1precedesthe
variable 𝑖in the construction of the 𝜋rule context sample point.
Since the leaf distribution only has support over graphs with 𝐺𝑖⊆
𝐷𝐶𝑠1, Therefore, all the graphs 𝐺below satisfy 𝐺|=𝜋.
Proposition2.Anyhierarchicallineartree-structurednetwork
possesses completeness and decomposability.
Proof.Inhierarchicallinearsymbolizedtree-structurednetworks,
the𝑖th product symbol node has a scope (𝜋𝑠21,𝑖∪𝑠22,𝑖,𝐺𝑠21,𝑖∪𝑠22,𝑖),
ensuring its completeness. This is because, by definition, 𝐷𝐶𝑠21,𝑖and
𝐷𝐶𝑠22,𝑖partition 𝐷𝐶𝑠2. This network’s decomposability is obtained
from the scope ranges of the product symbol node 𝑀and its child
nodes, where thevariable (𝜋𝑠21∪𝑠22,𝐺𝑠21∪𝑠22)are decomposed into
sum(orleaf)nodeswithranges (𝜋𝑠21,𝐺𝑠21)and(𝜋𝑠22,𝐺𝑠22),while
𝐷𝐶𝑠21and𝐷𝐶𝑠21are disjoint.
A.3 Leaf Node Distribution
Wenowexplainhowtoperformmarginal,conditional,andmost
probableexplanation(MPE),andsamplinginferenceforleafnode
distributions in hierarchical linear symbolized tree-structured neu-
ralprocesses.Inthisnetwork,theleafnodedistributionforcontext
samplepoint (𝑥𝐶
𝑖,𝑦𝐶
𝑖)iscalculated usingthefollowing probability
density:
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖)=𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1/summationtext.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)(18)
where pa𝐺/parenleftBig
𝐷𝐶
𝑖/parenrightBig
⊆𝐷𝐶𝑠1is the set of potential parent contexts for
thevariablecontextsamplepoint,andatthesametime,thenormal-
izationisappliedwithinit.Asthenumberofconstructedcontext
samplepointsincreasesintherecursiveprocessofbuildingthetree-
structured network, the computation process becomes challenging.
During computation, simplifications require a global restriction on
thecandidatenodeset 𝐷𝐶
𝑖.Inotherwords,foreachleafnodecor-
responding to context sample point 𝐷𝐶
𝑖, with distribution 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖,
wereplacethepotentialparentnodeset 𝐷𝐶𝑠𝑖withtheintersectionof
𝐷𝐶𝑠1and the candidate set pa𝐺/parenleftBig
𝐷𝐶
𝑖/parenrightBig
. This restricts the range of the
distribution in the tree-structured network, and we can choose the
candidateparentcontextsamplepoints pa𝐺/parenleftBig
𝑥𝐶
𝑖,𝑦𝐶
𝑖/parenrightBig
topreserve
as much posterior distribution as possible. As mentioned above, ef-
ficientinferencequeriesforany pa𝐺/parenleftBig
𝐷𝐶
𝑖/parenrightBig
⊆𝐷𝐶𝑠1canbeconducted
to obtain 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖).Thehierarchicallinearsymbolizedtree-structurednetworkstruc-
ture is presented in a normalized manner, demonstrating logical
symbolinferenceforms.Therefore,wedefine 𝐵𝑖𝑗astheassociation,
i.e.,𝐷𝐶
𝑗∈𝐺𝑖,indicating that 𝑗is the parent node of 𝑖. Then, the
computation for the key component is as follows:
𝑓𝑖/parenleftbig𝐴𝑖,𝐴/prime
𝑖/parenrightbig=/summationdisplay.1
𝐺𝑖|=/parenleftbigg/parenleftbigg/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐴𝑖𝐵𝑖,𝑗/parenrightbigg
∧/parenleftbigg/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐴/prime
𝑖¬𝐵𝑖,𝑗/parenrightbigg/parenrightbigg
𝑝𝐺𝑖(𝐺𝑖)(19)
where 𝐴𝑖,𝐴/prime
𝑖aredisjointsubsetsof pa𝐺(𝐷𝐶
𝑖).Theentireexpression
requires that the context sample points in 𝐴𝑖are all parents of 𝑖,
and the context sample points in 𝐴/prime
𝑖are none of the parents of 𝑖.
Basedontheabove,where 𝐴𝑖and𝐴/prime
𝑖aresubsetsof pa𝐺(𝐷𝐶
𝑖),w e
further express it as:
𝑓𝑖/parenleftbig𝐴𝑖,𝐴/prime
𝑖/parenrightbig=𝑝𝐺𝑖(𝐴𝑖) (20)
Because 𝐴𝑖and𝐴/prime
𝑖determinetherelevantparentnodesin 𝐷𝐶
𝑖.In
the construction process of the hierarchical linear symbolized tree-
structured network, the representation is implemented recursively
as follows:
𝑓𝑖/parenleftbig𝐴𝑖,𝐴/prime
𝑖/parenrightbig=𝑓𝑖/parenleftBig
𝐴𝑖∪{(𝑥𝐶
𝑖,𝑦𝐶
𝑖)},𝐴/prime
𝑖/parenrightBig
+
𝑓𝑖/parenleftBig
𝐴𝑖,𝐴/prime
𝑖∪{(𝑥𝐶
𝑖,𝑦𝐶
𝑖)}/parenrightBig (21)
where(𝑥𝐶
𝑖,𝑦𝐶
𝑖)∈ 𝐷𝐶
𝑖. From the representation of 𝑓𝑖, it can be
observed that the processing involves determining whether the
context sample point (𝑥𝐶
𝑖,𝑦𝐶
𝑖)is a parent of pa𝐺/parenleftBig
𝐷𝐶
𝑖/parenrightBig
.
We use 𝐶𝑖to represent the conjunction of literals in/braceleftbig
𝐵𝑖,𝑗/bracerightbig
, thus
enabling logical representation and inference:
i)Marginal distribution: Now, for distribution 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖, the mar-
ginal distribution of the logical expression is represented as:
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐶𝑖=1)=/summationtext.1
𝐺𝑖|=𝐷𝐶
𝑖𝑝𝐺𝑖(𝐺𝑖)𝟙𝐺𝑖⊆𝐷𝐶𝑠1/summationtext.1
𝐺𝑖⊆𝐷𝐶𝑠1𝑝𝐺𝑖(𝐺𝑖)
=/summationtext.1
𝐺𝑖|=/parenleftbigg
𝐷𝐶
𝑖∧/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖\𝐷𝐶𝑠1¬𝐵𝑖,𝑗/parenrightbigg𝑝𝐺𝑖(𝐺𝑖)
/summationtext.1
𝐺𝑖|=/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖\𝐷𝐶𝑠1¬𝐵𝑖,𝑗𝑝𝐺𝑖(𝐺𝑖)(22)
where We represent the condition of 𝐺𝑖⊆𝐷𝐶𝑠1with the logical for-
mula/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖\𝐷𝐶𝑠1¬𝐵𝑖,𝑗.ByobservingFormula19,wecanex-
press 𝑓𝑖asamarginalprobabilitycalculation 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftbig𝐶𝑖=1|𝐶/prime
𝑖=1/parenrightbig:
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftbig𝐶𝑖=1|𝐶/prime
𝑖=1/parenrightbig=𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftbig𝐶𝑖∧𝐶/prime
𝑖=1/parenrightbig
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftBig
𝐶/prime
𝑖=1/parenrightBig(23)
ii)MPE:Now,fordistribution 𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖,theMPEfor 𝐶𝑖=1the
logical expression is represented as:
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐶𝑖=1)=max
𝐺𝑖𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖|𝐶𝑖=1)
=max
𝐺𝑖|=𝐶𝑖𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐺𝑖|𝐶𝑖=1)
=max𝐺𝑖|=𝐷𝐶
𝑖∧/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖\𝐷𝐶𝑠1¬𝐵𝑖,𝑗𝑝𝐺𝑖(𝐺𝑖)
/summationtext.1
𝐺𝑖|=𝐷𝐶
𝑖∧/logicalandtext.1
(𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖\𝐷𝐶𝑠1¬𝐵𝑖,𝑗𝑝𝐺𝑖(𝐺𝑖)(24)
 
2828KDD ’24, August 25–29, 2024, Barcelona, Spain. Jin yang Tai and Yi ke Guo
The process of the above computation involves the selection of
𝐺𝑖satisfying the logical conjunction. The summation from the
function 𝑓𝑖, where there is a summation over 𝐺𝑖satisfying the
logicalconjunction.Therefore,intherecursiveprocess,wecompute
anotherfunction 𝑓max
𝑖,whichperformsthe sameas 𝑓𝑖exceptthat
the recurrence is rewritten as follows:
𝑓max
𝑖/parenleftbig𝐴𝑖,𝐴/prime
𝑖/parenrightbig=max/parenleftBig
𝑓max
𝑖/parenleftBig
𝐴𝑖∪{(𝑥𝐶
𝑖,𝑦𝐶
𝑖)},𝐴/prime
𝑖/parenrightBig
,
𝑓max
𝑖/parenleftBig
𝐴𝑖,𝐴/prime
𝑖∪{(𝑥𝐶
𝑖,𝑦𝐶
𝑖)}/parenrightBig/parenrightBig(25)
The function 𝑓𝑖is similar to a function 𝑓𝑚𝑎𝑥
𝑖. Its purpose is to
calculatethemaximumprobability 𝑝𝐺𝑖(𝐺𝑖)amongall 𝐺𝑖satisfying
a certain logical formula. Specifically, for each 𝐺𝑖that meets the
logical conditions, the function 𝑓𝑚𝑎𝑥
𝑖computes the corresponding
probability 𝑝𝐺𝑖(𝐺𝑖),andthenidentifiesthemaximumvalueamong
these probabilities.
iii)Sampling:Inthegivencondition 𝐶𝑖,wewishtosample 𝐺𝑖from
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖(𝐶𝑖=1).𝐵𝑖,𝑗⊆𝐶𝑖contains unprocessed context sample
points 𝐷𝐶
𝑖.
Then,Wehavedependencieson 𝐵𝑖,𝑗andcan sequentially eval-
uate the pending sample points (𝑥𝐶
𝑗,𝑦𝐶
𝑗)∈𝐷𝐶
𝑖. We sample from
𝐷𝐶
𝑖using conjunctions represented by 𝐶𝑖=𝐵𝑖,1∧𝐵𝑖,2∧...𝐵𝑖,𝑗.
Thisapproachenablesprocessingcontextsamplepoints (𝑥𝐶
𝑗,𝑦𝐶
𝑗)
in𝐶𝑖based on context dependencies, with each step considering
previouslyprocessed samples.Thefollowing canbeexpressed as:
𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftbig𝐵𝑖,𝑗=1|𝑑𝑖=1,𝐶𝑖=1/parenrightbig
=𝑝𝐷𝐶𝑠1,𝐷𝐶
𝑖/parenleftbig𝐵𝑖,𝑗=1|𝑑𝑖∧𝐶𝑖=1/parenrightbig (26)
where 𝑑𝑖=𝐵1,2∧¬𝐵1,3∧...𝐵𝑖−1,𝑗.𝑑𝑖representsthesetofpreviously
processed context sample points (Whether it is a parent class).
A.4 Interpretability of Tree-Structured Neural
Processes
Thissectiondemonstrateshowourmodelcomputesinterpretability
for the average causal effects in the hierarchical linear symbolized
tree-structuredneuralprocesses.Thecomputationof 𝐵𝐶𝐸((𝑥𝐶
𝑗,𝑦𝐶
𝑗),
(𝑥𝐶
𝑖,𝑦𝐶
𝑖))differs from the traditional approach that is limited to
only leaf nodes. We construct a tree-structured probabilistic graph,encompassing all relationships between context sample points
(𝑥𝐶
𝑖,𝑦𝐶
𝑖)and(𝑥𝐶
𝑗,𝑦𝐶
𝑗).
Proposition 4. In constructing a hierarchical linear symbolized
network, interpretability is achieved as all pairwise sample points
contribute to the BCE through coefficient paths.
Proof.In a hierarchical linear symbolized tree-structured net-
work,allnodescanbeassociatedwithcontextsamplepoints (𝐷𝐶𝑠1,𝐷𝐶𝑠2)
andrepresentthedistributioncomputationoverthemarginal 𝐺𝑠2.
We define causal effects as follows:
𝐶𝐼𝑖,𝑗=/summationdisplay.1
𝐿𝑃∈𝐷𝐶𝑠𝑚\{(𝑥𝐶
𝑖,𝑦𝐶
𝑖),(𝑥𝐶
𝑗,𝑦𝐶
𝑗)})𝛾{𝑖,𝐿𝑃1}𝛾{𝐿𝑃|𝐿𝑃|,𝑗}
|𝐿𝑃|−1/productdisplay.1
𝑖=1𝛾{𝐿𝑃𝑖,𝐿𝑃𝑖+1}(27)
where 𝛾{𝑖,𝐿𝑃1}represents the weight of the edge from context sam-
ple points (𝑥𝐶
𝑖,𝑦𝐶
𝑖)to(𝑥𝐶
𝐿𝑃1,𝑦𝐶
𝐿𝑃1),and 𝛾{𝐿𝑃|𝐿𝑃|,𝑗}represents the
weight of the edge from (𝑥𝐶
𝐿𝑃|𝐿𝑃|,𝑦𝐶
𝐿𝑃|𝐿𝑃|)to(𝑥𝐶
𝑗,𝑦𝐶
𝑗).
We define the Bayesian causal effects of the tree structure in the
form of expectations:
BCE((𝑥𝐶
𝑖,𝑦𝐶
𝑖),(𝑥𝐶
𝑗,𝑦𝐶
𝑗))/definesE𝐺∼𝑞𝜙(𝐺)/bracketleftbig
E𝛾∼𝑞(𝛾|𝐺)/bracketleftbig
𝐶𝐼𝑖,𝑗(𝛾)/bracketrightbig/bracketrightbig
(28)
Basedontheaboveresults,wenowneedtocompletethestructural
decomposition calculationof 𝐵𝐶𝐸(𝑥𝐶
𝑖,𝑦𝐶
𝑖)for thehierarchical lin-
ear symbolized tree-structured Network. At a 𝑇symbol sum node,
the weights of its child nodes are represented as 𝛾1, ...,𝛾𝑖.
BCE((𝑥𝐶
𝑖,𝑦𝐶
𝑖),(𝑥𝐶
𝑗,𝑦𝐶
𝑗))/definesE𝐺∼𝑞𝜙(𝐺)/bracketleftbig
E𝛾∼𝑞(𝛾|𝐺)/bracketleftbig
𝐶𝐼𝑖,𝑗(𝛾)/bracketrightbig/bracketrightbig
=/summationdisplay.1
𝑖𝛾𝑖E𝐺∼𝑞𝜙(𝐺)/bracketleftbig
E𝛾∼𝑞(𝛾|𝐺)/bracketleftbig
𝐶𝐼𝑖,𝑗(𝛾)/bracketrightbig/bracketrightbig
=/summationdisplay.1
𝑖𝛾𝑖BCE(𝑖, 𝑗)
(29)
We represent linear path information in a weighted manner.
If the node is a product node 𝑀, it processes two child nodes,
each handling context-associated subsets (𝐷𝐶𝑠1,𝐷𝐶𝑠21)and(𝐷𝐶𝑠1∪
𝐷𝐶𝑠21,𝐷𝐶𝑠22)respectively. We now consider three different cases, de-
pending on the partition of (𝑥𝐶
𝑖,𝑦𝐶
𝑖)and(𝑥𝐶
𝑗,𝑦𝐶
𝑗)into distinct
subsets.
 
2829