Provable Adaptivity of Adam under Non-uniform Smoothness
Bohan Wang∗
bhwangfy@gmail.com
University of Science and Technology
of China & Microsoft Research Asia
Beijing, ChinaYushun Zhang∗
yushunzhang@link.cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdoug, ChinaHuishuai Zhang†
zhanghuishuai@pku.edu.cn
Peking University
Beijing, China
Qi Meng
meq@amss.ac.cn
Chinese Academy of Mathematics
and Systems Science
Beijing, ChinaRuoyu Sun
sunruoyu@cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdong, ChinaZhi-Ming Ma
mazm@amt.ac.cn
Chinese Academy of Mathematics
and Systems Science
Beijing, China
Tie-Yan Liu
tie-yan.liu@microsoft.com
Microsoft
Beijing, ChinaZhi-Quan Luo
luozq@cuhk.edu.cn
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, Guangdong, ChinaWei Chen†
chenwei2022@ict.ac.cn
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
Abstract
Adam is widely adopted in practical applications due to its fast con-
vergence. However, its theoretical analysis is still far from satisfac-
tory. Existing convergence analyses for Adam rely on the bounded
smoothness assumption, referred to as the L-smooth condition. Un-
fortunately, this assumption does not hold for many deep learning
tasks. Moreover, we believe that this assumption obscures the true
benefit of Adam, as the algorithm can adapt its update magnitude
according to local smoothness. This important feature of Adam
becomes irrelevant when assuming globally bounded smoothness.
This paper studies the convergence of randomly reshuffled Adam
(RR Adam) with diminishing learning rate, which is the major ver-
sion of Adam adopted in deep learning tasks. We present the first
convergence analysis of RR Adam without the bounded smooth-
ness assumption. We demonstrate that RR Adam can maintain its
convergence properties when smoothness is linearly bounded by
the gradient norm, referred to as the (𝐿0,𝐿1)-smooth condition. We
further compare Adam to SGD when both methods use diminishing
learning rate. We refine the existing lower bound of SGD and show
that SGD can be slower than Adam. To our knowledge, this is the
first time that Adam and SGD are rigorously compared in the same
setting and the advantage of Adam is revealed.
∗Both authors contributed equally to this research.
†Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671718CCS Concepts
•Mathematics of computing →Nonconvex optimization.
Keywords
Adaptive Optimizer, Convergence Analysis, Non-uniform smooth-
ness
ACM Reference Format:
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-
Ming Ma, Tie-Yan Liu, Zhi-Quan Luo, and Wei Chen. 2024. Provable Adaptiv-
ity of Adam under Non-uniform Smoothness. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 35 pages.
https://doi.org/10.1145/3637528.3671718
1 Introduction
Machine learning tasks are often formulated as solving the follow-
ing finite-sum problem:
min
𝒘∈R𝑑𝑓(𝒘)=1
𝑛𝑛−1∑︁
𝑖=0𝑓𝑖(𝒘), (1)
where𝑛denotes the number of samples or mini-batches, and 𝒘
denotes the trainable parameters. Recently, it is noted that adaptive
gradient methods including Adaptive Moment estimation (Adam)
[22] are widely used to train modern deep neural networks in-
cluding GANs [ 3], BERTs [ 21], GPTs [ 4] and ViTs [ 11]. It is often
observed that Adam converges considerably faster than vanilla Sto-
chastic Gradient Descent (SGD) for the training of Transformers,
as seen in Figure 1(a). Similar phenomena are also reported in [ 45].
Despite its practical success, the theoretical analysis of Adam is
less than satisfactory. Existing analyses rely on bounded smooth-
ness assumption, i.e., the Lipschitz coefficient of gradients (or the
spectrum norm of the Hessian) is globally upper bounded by con-
stant𝐿, referred to as 𝐿-smooth condition. However, recent studies
 
2960
KDD ’24, August 25–29, 2024, Barcelona, Spain Bohan Wang et al.
show that the 𝐿-smooth condition does nothold in practical deep
learning tasks such as LSTM [43] and Transformers [8].
Moreover, such an assumption hides the benefit of Adam. Intu-
itively, Adam can overcome the issue of unbounded smoothness
using adaptive learning rate. First, Adam uses the reciprocal of the
square root of the exponential moving averages of past squared
gradients as an effective learning rate (see Algorithm 1 for the
update rule). Thus, the effective learning rate would be adapted
to the local gradient norm. Second, there is a strong correlation
between the Lipschitz coefficient and the gradient norm of deep
neural networks [ 7,8,43]. As a result, Adam can adapt the update
magnitude to the local Lipschitz coefficient and is empirically ob-
served to converge fast (Figure 1(a) and [ 43]). Unfortunately, such
benefit is hidden because existing theories of Adam are built upon
𝐿-smooth condition.
To reveal the theoretical benefit of Adam, we analyze its conver-
gence under a relaxed smoothness condition called (𝐿0,𝐿1)-smooth
condition [43]:
∥∇2𝑓𝑖(𝒘)∥≤𝐿0+𝐿1∥∇𝑓𝑖(𝒘)∥. (2)
When𝐿1=0, Eq. (2)degenerates into classical 𝐿-smooth condition.
The(𝐿0,𝐿1)-smooth condition allows the spectral norm of the Hes-
sian (Lipschitz coefficient of gradients) to linearly grow with the
gradient norm of 𝒘, so it is a relaxed version of 𝐿-smooth condition.
The(𝐿0,𝐿1)-smooth condition is empirically observed to hold in
LSTM [42, 43] and Transformers (Figure 1(b) and [8]).
Our Contribution: Under the(𝐿0,𝐿1)-smooth condition, we es-
tablish the convergence of randomly-reshuffled Adam. Specifically,
our contributions are summarized as follows.
•We establish the first convergence result of Adam without
“𝐿-smoothness". We prove that Adam converges under the
(𝐿0,𝐿1)-smooth condition.
•Our convergence result enjoys several good properties. First,there
is no need for the bounded gradient assumption (i.e. ∥∇𝑓(𝒘)∥≤
𝐶). Eliminating this assumption is essential since the (𝐿0,𝐿1)-
smooth condition would otherwise degenerate to the 𝐿-
smooth condition. Second, our result does not rely on other
assumptions such as a bounded adaptor or a large regular-
izer for numerical stability. Lastly, the convergence holds
for every possible trajectory, which is not only technically
demanding but also much stronger than “convergence in
expectation”.
•We further compare Adam to SGD when both methods use
diminishing learning rate. We present an improved lower
bound for (S)GD under the (𝐿0,𝐿1)-smooth condition. In
this lower bound, there is a factor related to the gradient
norm of the initial point, which does not exist in the upper
bound of Adam. This indicates that (S)GD can converge slow
under the(𝐿0,𝐿1)-smooth condition, showing the advantage
of Adam over (S)GD. To our knowledge, this is the first time
that Adam and SGD are rigorously compared in the same
setting where the advantage of Adam can be revealed. We
believe these results shed new light on understanding the
benefit of Adam.Organization of this paper. The rest of this paper is organized
as follows: In Section 2, we review related works on the conver-
gence analysis for Adam, the relaxed smoothness assumption, and
the variants of Adam. In Section 3, we define notations, present the
psedocode of Adam, and provide the assumptions that our result
rests on. In Section 4, we provide our main result on the conver-
gence of RR Adam under non-uniform smoothness together with
explanations regarding the result. In Section 5, we then state the
proof ideas of the main result. In Section 7, we provide discussions
on intuitions of why non-adaptive optimizers can be used for fine-
tuning tasks, comparison of Adam and Clipped SGD, insights for
practioners and limitations of Theorem 4.1.
2 Related works
Convergence analysis for Adam. Adam is firstly proposed in
Kingma and Ba [23] with a convergence proof. However, the proof
is pointed out to have flaws by [ 30] and [ 30] further provide simple
counterexamples with which Adam diverges. This discovery caused
the convergence analysis of Adam to stagnate for a while and
motivated a series of works developing variants of Adam without
divergent issues (see discussion latter in this section). On the other
hand, vanilla Adam works well in practice and divergence is not
empirically observed. This phenomenon motivates researchers to
rethink the counterexamples. The counterexamples states “for every
𝛽1<√︁
𝛽2, there exists a problem that Adam diverges". That is to
say, the divergence statement requires picking (𝛽1,𝛽2)before fixing
the problem, while in practice, the algorithmic parameters are often
picked according to the problem. Based on this observation, a recent
work [ 46] proves that Adam can converge with (𝛽1,𝛽2)picked after
the problem is given.
We categorize the existing results of Adam into two classes based
on the sampling strategy: with-replacement sampling (a.k.a.,
i.i.d. sampling, abbreviated as “WR") and RR Adam. We believe
both sampling strategies are worth studying: WR is more favored
among the theory community due to its simple form, whereas RR
is widely used among practitioners because it is easy to implement.
Further, RR guarantees to pass each data at least once and brings
good performance [1, 2].
The first line of work analyzes WR Adam. For instance, [ 40]
shows that WR RMSProp (a simplified version of Adam with 𝛽1=0)
converges to the neighborhood of the stationary points. [ 9] prove
the convergence of WR RMSProp by assuming the signs of the
gradients to remain the same along the trajectory. However, this
condition is not guaranteed to hold in practice. [ 10] prove the con-
vergence of WR Adam with 𝛽1<𝛽2. However, their convergence
bound is inversely proportional to 𝜉, which is the hyperparameter
for numerical stability. Consequently, their bound becomes vacuous
as𝜉approaches zero. This result does not match practical observa-
tions because small values of 𝜉, like 10−8, often yield satisfactory
performance. Moreover, employing large values of 𝜉obscures the
effect of√𝑣𝑘, and thus the proof is largely reduced to the proof of
SGD. [ 18,19] provide simple convergence proof for WR Adam with
𝛽1close to 1. However, their results require the√𝑣𝑘to be bounded
in a certain interval [𝐶𝑙,𝐶𝑢]. This condition changes Adam into
AdaBound [ 27]. In summary, all the above works require certain
strong conditions such as bounded√𝑣𝑘or large𝜉. Further, they all
 
2961Provable Adaptivity of Adam under Non-uniform Smoothness KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) Training loss
 (b) Gradient vs. smoothness
Figure 1: Experiments on the WMT 2014 dataset trained with the transformer. (a): The training loss of SGD and Adam. (b):
The gradient norm vs. the local smoothness on the training trajectory. The blue line in (b) stands for log(local smoothness) =
log(gradient norm)+ 1.4. It can be observed that (𝑒1.4,0)-smooth condition holds in this task. Similar results can be seen in
Zhang et al. [43].
require bounded gradient ( ∥∇𝑓(𝑥)∥≤𝐶) and bounded smoothness
(𝐿-smooth) condition.
Our analysis falls into the second line of works, which focus on
RR Adam. [ 33] prove the trajectory-wise convergence of RR RM-
SProp and [ 46] prove the in-expectation convergence of RR Adam.
However, these works both require 𝐿-smooth condition. Our anal-
ysis follows this line of works and provides the first convergence
result of RR Adam under relaxed smoothness condition.
Relaxed smoothness assumption. There are several attempts on
relaxing𝐿-smooth condition. Zhang et al . [43] proposes(𝐿0,𝐿1)-
smooth condition to theoretically explain the acceleration effect of
clipped SGD over SGD. Similar results are also extended to clipped
SGD with momentum [ 42], distributionally-robust optimization
[20] , differentially-private SGD [ 39] and generalized SignSGD [ 8].
However, they did not theoretically analyze Adam in this setting.
Considering the great empirical impact of Adam, we believe it is
important to study Adam in its original form.
One concurrent work [ 24] studies the convergence of WR Adam
under(𝐿0,𝐿1)-smooth condition by cleverly constructing certain
stopping time. They also propose a variance-reduced variant with
better convergence rate. However, their bound on Adam not only
assumes the noise is deterministically bounded, but also has poly-
nomial dependence over 1/𝜉(the hyperparameter for numerical
stability). Similarly to [ 9], this result does not match practice ob-
servations, since Adam performs well even when 𝜉is as small as
10−8.
Variants of Adam. Ever since the counter-example of the conver-
gence of Adam raised by [ 30], many new variants of Adam have
been designed. For instance, Chen et al . [5,6], Gadat and Gavra
[14], Zou et al . [49] replaced the constant hyperparameters by
iterate-dependent ones e.g. 𝛽1𝑡or𝛽2𝑡. AMSGrad [ 31] and AdaFom
[6] enforced{𝑣𝑡}to be non-decreasing. Similarly, AdaBound [ 27]imposed constraints 𝑣𝑡∈[𝐶𝑙,𝐶𝑢]to prevent the learning rate from
vanishing or exploding. Similarly, [ 48] adopted a new estimate of 𝑣𝑡
to correct the bias. In addition, there are attempts to combine Adam
with Nesterov momentum [ 12] as well as warm-up techniques [ 25].
There are also some works providing theoretical analysis on the
variants of Adam. For instance, Zhou et al . [47] studied the con-
vergence of AdaGrad and AMSGrad. Gadat and Gavra [14] studied
the asymptotic behavior of a subclass of adaptive gradient methods
from landscape point of view. Their analysis applies to RMSprop-
variants with iterate-dependent 𝛽2𝑡. In summary, all these works
study variants of Adam, which is different from our work since we
focus on vanilla Adam.
3 Preliminaries
This section introduces notations, definitions, and assumptions that
are used throughout this work.
Notations. We list the notations that are used in the formal defi-
nition of the randomly-shuffled Adam and its convergence analysis.
•(Vector) We define 𝒂⊙𝒃as the Hadamard product (i.e.,
component-wise product) between two vectors 𝒂and𝒃with
the same dimension. We also define ⟨𝒂,𝒃⟩as theℓ2inner
product between 𝒂and𝒃. We define 1𝑑as an all-one vector
with dimension 𝑑.
•(Array) We define [𝑚1,𝑚2]≜{𝑚1,···,𝑚2},∀𝑚1,𝑚2∈
N,𝑚1≤𝑚2. Specifically, we use [𝑚]≜{1,···,𝑚}.
•(Asymptotic notation) We define 𝐴1(𝑥)=O𝑥→𝑎(𝐴2(𝑥))
if𝐴1(𝑥)
𝐴2(𝑥)is bounded when 𝑥→𝑎. We define 𝐴2(𝑥)=
Ω𝑥→𝑎(𝐴1(𝑥))when𝐴1(𝑥)=O𝑥→𝑎(𝐴2(𝑥)). We use ˜Oto
denoteOwith logarithmic factors hidden, i.e., 𝐴1(𝑥)=
˜O𝑥→𝑎(𝐴2(𝑥))if𝐴1(𝑥)=O𝑥→𝑎(𝐴2(𝑥)log|𝐴2(𝑥)|). When
the context is clear, we hide " 𝑥→𝑎" and only useO,Ω,˜O.
 
2962KDD ’24, August 25–29, 2024, Barcelona, Spain Bohan Wang et al.
Pseudocode. To facilitate the analysis, we provide the pseudocode
of Adam in Algorithm 1.
Algorithm 1 Randomly reshuffled Adam (RR-Adam)
Input: Objective function 𝑓(𝒘):=1
𝑛Í𝑛−1
𝑖=0𝑓𝑖(𝒘), learning rate
series{𝜂𝑘}𝑇
𝑘=1and hyperparameters (𝛽1,𝛽2)∈[ 0,1)2. Initialize
the parameter 𝒘1,0∈R𝑑, the conditioner 𝝂1,−1∈R𝑑,≥0, and the
momentum 𝒎1,−1∈R𝑑.
for𝑘=1to𝑇do
Randomly shuffle[0,𝑛−1]to get{𝜏𝑘,𝑗}𝑛−1
𝑗=0
for𝑖=0to𝑛−1do
Calculate𝑔𝑘,𝑖=∇𝑓𝜏𝑘,𝑖(𝒘𝜏𝑘,𝑖)
Update 𝝂𝑘,𝑖=𝛽2𝝂𝑘,𝑖−1+(1−𝛽2)𝑔⊙2
𝑘,𝑖,
Update 𝒎𝑘,𝑖=𝛽1𝒎𝑘,𝑖−1+(1−𝛽1)𝑔𝑘,𝑖
Update 𝒘𝑘,𝑖+1=𝒘𝑘,𝑖−𝜂𝑘1√𝝂𝑘,𝑖+𝜉⊙𝒎𝑘,𝑖
end for
Update 𝝂𝑘+1,−1=𝝂𝑘,𝑛−1,𝒎𝑘+1,−1=𝒎𝑘,𝑛−1,𝒘𝑘+1,0=𝒘𝑘,𝑛
end for
𝒎𝑘,𝑖and𝝂𝑘,𝑖are weighted averages with hyperparamter 𝛽1∈
[0,1)and𝛽2∈[0,1), respectively. 𝜉is adopted for numerical sta-
bility and it is often chosen to be 10−8in practice. In our theory,
we allow𝜉to be an arbitrary non-negative constant including 0.
Algorithm 1 follows a without-replacement sampling strategy
(also known as shuffling), which is the default strategy used in CV,
NLP, GANs, etc. However, it is not necessarily easy to analyze shuf-
fling strategy, because the stochastic gradients sampled by random-
shuffling lack statistical unbiasedness, i.e. E
∇𝑓𝑘,𝑖(𝑥𝑘,𝑖)|𝑥𝑘,𝑖
≠
∇𝑓(𝑥𝑘,𝑖). This bias requires a much different analysis from its with-
replacement counterpart. Even for SGD, the analysis for shuffling is
often known to be “more challenging" [ 28,36]. However, we choose
to study this version as it is closer to the practice.
Assumptions. Here we state the assumptions that our result will
rest on. The first one is the (𝐿0,𝐿1)-smooth condition introduced
in Section 1.
Assumption 3.1 ((𝐿0,𝐿1)-smooth condition). We assume that
𝑓𝑖(𝒘)is lower bounded by 0, and𝑓𝑖(𝒘)satisfies(𝐿0,𝐿1)-smooth
condition, i.e., there exist positive constants ( 𝐿0,𝐿1), such that,
∀𝒘1,𝒘2∈R𝑑satisfying∥𝒘1−𝒘2∥≤1
𝐿1,
∥∇𝑓𝑖(𝒘1)−∇𝑓𝑖(𝒘2)∥≤(𝐿0+𝐿1∥∇𝑓𝑖(𝒘1)∥)∥𝒘1−𝒘2∥.(3)
Eq. (3) is firstly introduced by Zhang et al . [42] , and is the weakest
version of(𝐿0,𝐿1)-smooth condition to our best knowledge since
it does not require 𝑓𝑖(𝒘)to be twice differentiable. When 𝑓𝑖(𝒘)is
twice differentiable, Eq. (3) is equivalent to Eq. (2)[42].(𝐿0,𝐿1)-
smooth condition generalizes the 𝐿-smooth condition (i.e., (𝐿0,𝐿1)-
smooth condition with 𝐿0=𝐿and𝐿1=0) in classical non-convex
optimization literature [ 16,26] and allows the smoothness to be
unbounded globally.
Assumption 3.2 (Affine Noise Variance). ∀𝒘∈R𝑑, the gradients
of{𝑓𝑖(𝒘)}𝑛−1
𝑖=0has the following connection with the gradient of
𝑓(𝒘):
1
𝑛𝑛−1∑︁
𝑖=0∥∇𝑓𝑖(𝒘)∥2≤𝐷1∥∇𝑓(𝒘)∥2+𝐷0.Assumption 3.2 is one of the weakest assumption on gradient
noise in existing literature. It not only generalizes the “bounded
variance" assumption (which requires 𝐷1=1/𝑛, and thus further
generalizes the "bounded gradient" assumption [ 10]) [17,19,41],
but also is weaker than the “strongly growth condition" (which
requires𝐷0=0) [32,38]. Assumption 3.2 allows flexible choices of
𝐷0&𝐷1and thus it is among the weakest assumption of this kind.
4 Adam Converges under the (𝐿0,𝐿1)-smooth
condition
In this section, we provide our main result on the convergence of
RR Adam under(𝐿0,𝐿1)-smooth condition. As discussed in Section
1, even for the simpler with-replacement sampling Adam, the es-
tablishment of the convergence under (𝐿0,𝐿1)-smooth condition
requires restrictive assumptions such as a large 𝜉(the constant in-
troduced for numerical stability and is as small as 10−8in practice),
and deterministically bounded noise [ 24]. Such assumptions make
the corresponding results hard to apply to practical setting. As for
the harder randomly-reshuffled setting, there is no convergence
result for Adam under non-uniform smoothness. Our result tackles
the limitation in existing works and propose the first convergence
result for RR Adam under non-uniform smoothness, provided as
follows.
Theorem 4.1. Consider RR Adam defined as Algorithm 1 with di-
minishing learning rate 𝜂𝑘=𝜂1√
𝑘. Let Assumptions 3.1 and 3.2 hold.
Suppose the hyperparamters satisfy: 0≤𝛽2
1<𝛽2<1and𝛽2is larger
than a threshold 𝛾(𝐷1). Then, we have
min
𝑘∈[1,𝑇]∥∇𝑓(𝒘𝑘,0)∥√𝐷1,∥∇𝑓(𝒘𝑘,0)∥2
√𝐷0
≤˜O𝑓(𝒘1,0)−min𝒘𝑓(𝒘)√
𝑇
+O(( 1−𝛽2)2√︁
𝐷0). (4)
For simplicity, we defer the concrete form of 𝛾to Appendix B.2.
We provide some remarks on the results as follows, and state the
proof idea in the next section.
Explanation for Theorem 4.1. Theorem 4.1 is pioneering in
demonstrating that RR Adam is capable of converging under the
non-uniform smoothness condition, a finding that is novel to our
best knowledge. Observing the right-hand side of inequality (4), one
can see that as 𝑇→∞ , it approachesO((1−𝛽2)2√𝐷0). This sug-
gests that Adam’s convergence to the vicinity of stationary points
is inversely related to the proximity of 𝛽2to 1. This theoretical
insight corroborates the common practice of choosing 𝛽2close to
0.99. A counterexample provided later will further illustrate that
convergence to a neighborhood, rather than an exact point, is an
intrinsic characteristic of the algorithm.
Beyond the(𝐿0,𝐿1)-smooth condition, Theorem 4.1 presupposes
only that the gradient noise exhibits affine variance as per Assump-
tion 3.2, which is a relatively mild constraint that eschews the
need for a bounded gradient norm. This is crucial, as imposing a
bound would reduce the (𝐿0,𝐿1)-smooth condition to an (𝐿0+𝐿1𝑀)-
smooth condition with a gradient norm capped by 𝑀. Additionally,
we do not require the adaptive learning rate 𝜂𝑘/√︁
ˆ𝜈𝑘to be upper
bounded, nor do we stipulate a large regularizer 𝜉—aligning with
common practices in deep learning libraries where a small 𝜉such
as10−8is often effective. Our theorem permits any non-negative
𝜉, including zero. Finally, Theorem 4.1 asserts convergence for
 
2963Provable Adaptivity of Adam under Non-uniform Smoothness KDD ’24, August 25–29, 2024, Barcelona, Spain
every possible trajectory, a guarantee that exceeds the typical "con-
vergence in expectation" results and poses a significant technical
challenge.
On the Comparison to Existing Analyses of RR Adam. Our
analysis extends the applicability of RR Adam by ensuring con-
vergence under the (𝐿0,𝐿1)smooth condition, which inherently
encompasses the traditional 𝐿-smooth condition. This broadened
perspective allows our results to guarantee convergence for RR
Adam even under the more general 𝐿-smooth scenario. When jux-
taposed with the state-of-the-art analysis of RR Adam under the
𝐿-smooth condition by Zhang et al . [46] , our findings advance the
field in two significant ways. Firstly, we elevate the notion of con-
vergence from the expected sense to the more stringent trajectory-
wise convergence. Secondly, we refine the estimated convergence
neighborhood, tightening it from (1−𝛽2)√𝐷0to(1−𝛽2)2√𝐷0.
Collectively, our analysis not only operates under a less restric-
tive assumption—the (𝐿0,𝐿1)-smooth condition—but also delivers
substantively enhanced convergence results.
On the range of hyperparameters. Theorem 4.1 indicates
that Adam can work when 𝛽2is close enough to 1. This matches
the practical choice of 𝛽2(e.g., 0.999in default setting, 0.95in the
GPT-3 training [ 4]). Note that our result does not contradict the
counterexamples of Adam’s non-convergence [ 30,46], as these
divergence results require 𝛽2to be small and thus not close to 1.
Rather, these counterexamples suggest that large 𝛽2is necessary
for convergence. As for 𝛽1, Theorem 4.1 needs 𝛽2
1<𝛽2. When𝛽2
is large, Theorem 4.1 allows a wide range of candidates of 𝛽1(e.g.,
0.9in default setting and 0.5in GAN [29]).
Figure 2: Reconduct of experimental results from [ 46]. The
objective function is defined in Eq. (5). One can observe that
while letting 𝛽2closer to 1can make the limiting gradient
norm smaller, the limiting gradient norm always stabilizes
beyond 0.
On the neighborhood of stationary points. When𝐷0≠0,
Theorem 4.1 only ensures that Adam converges to a neighborhood
of stationary points {𝒘:min{∥∇𝑓(𝒘))∥√𝐷1,∥∇𝑓(𝒘)∥2
√𝐷0}≤O(( 1−
𝛽2)√𝐷0)}. Since SGD converges to the stationary points with di-
minishing learning rate, one may wonder if Theorem 4.1 can be
improved to obtain the same conclusion as SGD. Unfortunately,
there is a counterexample in the existing literature ( function (9)
in Zhang et al . [46] ) showing that Adam does notconverge to
stationary points even if all the conditions in Theorem 4.1 aresatisfied. Specifically, [46] consider the following function:
𝑓(𝑥)=9∑︁
𝑗=0𝑓𝑗(𝑥)=1
10𝑥2−1,
where𝑓𝑗(𝑥)=((𝑥−1)2if𝑗=0
−0.1
𝑥−10
92
if1≤𝑗≤9.(5)
One can easily verify such an example satisfies Assumptions 3.2
and 3.1 with 𝐷0>0. As shown in Figure 2, when running Adam
(with𝛽1=0.9,𝜂𝑘=0.1/√
𝑘,𝑎=3,𝑥0=−2), it does not converge to
exact stationary points. Instead, it converges to a neighborhood of
stationary points with size inversely proportional to 𝛽2. Therefore,
the non-vanishing term in Theorem 4.1 is notdue to the limitation
of the proof. Rather, it is an intrinsic property of Adam.
Why cannot Adam converge to exact stationary points when
𝐷0>0? Intuitively, this is because even with diminishing 𝜂𝑘,
the effective learning rate𝜂𝑘
𝜉1𝑑+√𝝂𝑘,𝑖may not diminish due to the
potentially decreasing√𝝂𝑘,𝑖. The good news is that O((1−𝛽2)√𝐷0)
approaches 0as𝛽2gets close to 1. This means that the neighborhood
shrinks as𝛽2→1(this is also observed in Figure 2). As discussed
above, the practical use of 𝛽2is close to 1, and thus𝑂((1−𝛽2)√𝐷0)
is tolerable.
On the Diminishing Learning Rate. In Theorem 4.1, we con-
sider a diminishing learning rate of the form 𝜂𝑘=𝜂1√
𝑘to maintain
consistency with existing works on RR Adam, such as those by
[33,46], which also employ diminishing learning rates. Nonethe-
less, our results can be readily extended to RR Adam with a constant
learning rate. By adhering to the same proof strategy outlined in
Theorem 1, one can demonstrate that with a constant learning rate
𝜂, the conclusion (as given in Eq. (4)) of Theorem 1 is modified to
min𝑘∈[1,𝑇]{∥∇𝑓(𝑤𝑘,0)∥√𝐷0,∥∇𝑓(𝑤𝑘,0)∥2√𝐷1}≤ ˜O𝑓(𝒘1,0)−min𝒘𝑓(𝒘)
𝜂𝑇
+
O(√𝐷0(1−𝛽2)2)+O(𝜂). In essence, while Adam may converge
more rapidly to a neighborhood (with the rate improving from 1/√𝑡
to1/𝑡), the size of this neighborhood is increased by an additional
termO(𝜂), which is attributable to the constant step size.
5 Proof sketch
In this section, we briefly explain our proof idea for Theorem 4.1,
which can be divided into two stages. In Stage I, we will prove
Theorem 4.1 for Adam with 𝛽1=0to show the challenge brought
by(𝐿0,𝐿1)-smooth condition and how we tackle it. In Stage II, we
then show the additional difficulty when adding the momentum
and our corresponding intuition to solve it.
Stage I: Convergence of Adam with 𝛽1=0.By the descent
lemma,
𝑓(𝒘𝑘+1,0)−𝑓(𝒘𝑘,0)≤⟨𝒘𝑘+1,0−𝒘𝑘,0,∇𝑓(𝒘𝑘,0)⟩
|                           {z                           }
First Order
+𝐿𝑙𝑜𝑐
2∥𝒘𝑘+1,0−𝒘𝑘,0∥2,
|                        {z                        }
Second Order(6)
 
2964KDD ’24, August 25–29, 2024, Barcelona, Spain Bohan Wang et al.
where𝐿𝑙𝑜𝑐is the local smoothness. We bound the first-order and
the second-order term respectively. The upper bound on second-
order term is relatively simple. Due to the limited space, we only
show the idea of bounding first-order term here.
The ever-changing adaptive learning rate poses a challenge
on deriving the bound. It is even noted that with small 𝛽2, the
first order term can be positive [ 30]. However, we notice that
if𝝂𝑘,𝑖is stationary, i.e., RMSProp degenerates to SGD with pre-
conditioning, the first order term equals to −𝜂𝑘⟨Í
𝑖1
𝜉1𝑑+√𝝂𝑘,0⊙
∇𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖),∇𝑓(𝒘𝑘,0)⟩≈−𝜂𝑘⟨Í
𝑖1
𝜉1𝑑+√𝝂𝑘,0⊙∇𝑓𝜏𝑘,𝑖(𝒘𝑘,0),∇𝑓(𝒘𝑘,0)⟩,
which is indeed negative. While that " 𝝂𝑘,𝑖is stationary" is too good
to be true, we prove that 𝝂𝑘,𝑖changes little when 𝛽2is close to 1,
assuming that the gradient is large. Below we denote 𝝂𝑙,𝑘,𝑖as the
𝑙-th component of 𝝂𝑘,𝑖.
Lemma 5.1 (Informal). For any𝑙∈ [𝑑]and𝑖∈ [0,𝑛−1], if
max𝑝∈[0,𝑛−1]|𝜕𝑙𝑓𝑝(𝒘𝑘,0)|=Ω(Í𝑘−1
𝑟=1𝛽(𝑘−1−𝑟)
2
2𝜂𝑟∥∇𝑓(𝒘𝑟,0)∥+𝜂𝑘),
then|𝝂𝑙,𝑘,𝑖−𝝂𝑙,𝑘,0|=O((1−𝛽2)𝝂𝑙,𝑘,0).
The idea of Lemma 5.1 is simple: since 𝝂𝑘,𝑖=𝛽2𝝂𝑘,𝑖−1+(1−
𝛽2)∇𝑓𝜏𝑘,𝑖(𝒘𝜏𝑘,𝑖)⊙2, the change of 𝝂𝑘,𝑖w.r.t.𝑖should be small when
𝛽2is large. However, we need to check that the relative size of
∇𝑓𝜏𝑘,𝑖(𝒘𝜏𝑘,𝑖)⊙2w.r.t. 𝝂𝑘,𝑖−1is uniformly bounded across varying 𝛽2,
otherwise the term (1−𝛽2)∇𝑓𝜏𝑘,𝑖(𝒘𝜏𝑘,𝑖)⊙2may not go to zero when
𝛽2→1. We resolve this challenge by expanding 𝝂𝑘,𝑖in terms of
squared gradients and bounding the gap between each of the terms
and∇𝑓𝜏𝑘,𝑖(𝒘𝜏𝑘,𝑖)⊙2by echoing(𝐿0,𝐿1)-smooth condition. We defer
a detailed proof to Corollary B.9 for details.
As a conclusion, if we denote those dimensions with large gradi-
ents (i.e., satisfying the requirement of Lemma 5.1) as L𝑘
𝑙𝑎𝑟𝑔𝑒and
the rest as L𝑘
𝑠𝑚𝑎𝑙𝑙, Lemma 5.1 indicates that the L𝑘
𝑙𝑎𝑟𝑔𝑒part (i.e.,Í
𝑙∈L𝑘
𝑙𝑎𝑟𝑔𝑒(𝒘𝑙,𝑘+1,0−𝒘𝑙,𝑘,0)𝜕𝑙𝑓(𝒘𝑘,0)) in the first order term can be
bounded as
−𝜂𝑘∑︁
𝑙∈L𝑘
𝑙𝑎𝑟𝑔𝑒𝜕𝑙𝑓(𝒘𝑘,0)
√𝝂𝑙,𝑘,𝑖+𝜉∑︁
𝑖𝜕𝑙𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖)
≈−𝜂𝑘∑︁
𝑙∈L𝑘
𝑙𝑎𝑟𝑔𝑒 
𝜕𝑙𝑓(𝒘𝑘,0)2
√𝝂𝑙,𝑘,0+𝜉+O 
(1−𝛽2)𝜕𝑙|𝑓(𝒘𝑘,0)|Í
𝑖|𝜕𝑙𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖)|
√𝝂𝑙,𝑘,0+𝜉!!
=−Ω
𝜂𝑘min∥∇𝑓(𝒘𝑘,0)∥√𝐷1,∥∇𝑓(𝒘𝑘,0)∥2
√𝐷0
+𝑂(𝜂𝑘(1−𝛽2)√︁
𝐷0).
The last equation uses the affine noise assumption (Assumption
3.2), and we defer a detailed proof to Appendix B.4. A remain-
ing problem is how to deal with those components in L𝑘
𝑠𝑚𝑎𝑙𝑙. We
treat them as error terms. Concretely, 𝑙∈L𝑘
𝑠𝑚𝑎𝑙𝑙indicates that
𝜕𝑙𝑓(𝒘𝑘,0)=O(Í𝑘−1
𝑟=1𝛽(𝑘−1−𝑟)
2
2𝜂𝑟∥∇𝑓(𝒘𝑟,0)∥+𝜂𝑘). Applying it di-
rectly intoÍ
𝑙∈L𝑘
𝑠𝑚𝑎𝑙𝑙(𝒘𝑙,𝑘+1,0−𝒘𝑙,𝑘,0)𝜕𝑙𝑓(𝒘𝑘,0), we have
−𝜂𝑘∑︁
𝑙∈L𝑘
𝑙𝑎𝑟𝑔𝑒𝜕𝑙𝑓(𝒘𝑘,0)
√𝝂𝑙,𝑘,𝑖+𝜉∑︁
𝑖𝜕𝑙𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖)
=O 
𝜂𝑘 𝑘−1∑︁
𝑟=1𝛽(𝑘−1−𝑟)
2
2𝜂𝑟∥∇𝑓(𝒘𝑟,0)∥+𝜂𝑘!!
,where the equation is because𝜕𝑙𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖)
√𝝂𝑙,𝑘,𝑖+𝜉is bounded (proved by
Lemma B.3).
In order to upper bound the first order term, we then need to
prove that−Ω(𝜂𝑘min{∥∇𝑓(𝒘𝑘,0)∥√𝐷1,∥∇𝑓(𝒘𝑘,0)∥2
√𝐷0})dominates
O(𝜂𝑘(Í𝑘−1
𝑟=1𝛽(𝑘−1−𝑟)
2
2𝜂𝑟∥∇𝑓(𝒘𝑟,0)∥+𝜂𝑘)). This is not necessarily
true, as the historical gradient norms in the latter term can be large.
Remark 5.2.We recognize this as the challenge brought by (𝐿0,𝐿1)-
smooth condition, since the latter term degenerates to O(𝜂2
𝑘)with
𝐿-smooth condition, which is minor (Í𝑇
𝑘=1𝜂2
𝑘is only in order log𝑇).
We address this challenge by noting that what we need to bound
is the sum of the first order term. Fortunately, although we can-
not upper bound the first order term in one single epoch, we
can bound the sum of it across epochs. By a sum order change,
the sum ofO(𝜂𝑘(Í𝑘−1
𝑟=1𝛽(𝑘−1−𝑟)
2
2𝜂𝑟∥∇𝑓(𝒘𝑟,0)∥+𝜂𝑘))over𝑘equals
toO(Í𝑇
𝑘=1𝜂2
𝑘∥∇𝑓(𝒘𝑘,0)∥+ ln𝑇). This is smaller by the sum of
−Ω(𝜂𝑘min{∥∇𝑓(𝒘𝑘,0)∥√𝐷1,∥∇𝑓(𝒘𝑘,0)∥2
√𝐷0})by order of𝜂𝑘except a ln𝑇
term due to the mean value inequality, i.e.,
𝜂2
𝑘∥∇𝑓(𝒘𝑘,0)∥≤O(𝜂2
𝑘)+O 
𝜂2
𝑘√︂
𝐷1
𝐷0∥∇𝑓(𝒘𝑘,0)∥2!
.
We then conclude the sum of the first order term can be bounded
by−Ω(𝜂𝑘min{∥∇𝑓(𝒘𝑘,0)∥√𝐷1,∥∇𝑓(𝒘𝑘,0)∥2
√𝐷0})+O( ln𝑇).
Remark 5.3 (Difficulty compared to the analysis under 𝐿-smooth
condition). Here we illustrate the challenge brought by stepping
beyond𝐿-smooth condition. First of all, the change of 𝝂𝑘,𝑖is easier
to bound without the historical gradient term due to the absence
of the gradient norm in the bound of local smoothness. Secondly,
under𝐿-smooth condition, the error does not contain historical
gradient information and is only in order of O(𝜂2
𝑘), which is easy
to bound.
Stage II: adding the momentum. The second order term
of Adam can be bounded similarly. However, the analysis of the
first order term becomes more challenging even though we still
have 𝝂𝑘,𝑖≈𝝂𝑘,0. Specifically, even with constant 𝝂𝑘,𝑖=𝝂𝑘,0,
−𝜂𝑘⟨Í
𝑖𝒎𝑘,𝑖√𝝂𝑘,𝑖+𝜉,−∇𝑓(𝒘𝑘,0)⟩>0is not necessarily correct, as the
momentum 𝒎𝑘,𝑖contains a heavy historical signal, and may push
the update away from the negative gradient direction.
We resolve this challenge by observing that the alignment of
𝒘𝑘+1,0−𝒘𝑘,0and−∇𝑓(𝒘𝑘,0)is required due to that our analysis
is based on the potential function 𝑓(𝒘𝑘,0). However, while this
potential function is suitable for the analysis of RMSProp, it is
no longer appropriate for Adam based on the above discussion.
We need to construct another potential function. Our construc-
tion of the potential function is based on the following observa-
tion: we revisit the update rule in Algorithm 1 and rewrite it as
𝒎𝑘,𝑖−𝛽1𝒎𝑘,𝑖−1
1−𝛽1=∇𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖).
Notice that the right-hand-side of the above equation contains
no historical gradients but only the gradient of the current step! By
 
2965Provable Adaptivity of Adam under Non-uniform Smoothness KDD ’24, August 25–29, 2024, Barcelona, Spain
dividing(√𝝂𝑘,𝑖+𝜉)/𝜂𝑘above,
𝒘𝑘,𝑖+1−𝒘𝑘,𝑖−𝛽1(𝒘𝑘,𝑖−𝒘𝑘,𝑖−1)
1−𝛽1≈−𝜂𝑘√𝝂𝑘,0+𝜉1𝑑⊙𝒎𝑘,𝑖−𝛽1𝒎𝑘,𝑖−1
1−𝛽1
=−𝜂𝑘√𝝂𝑘,0+𝜉1𝑑⊙∇𝑓𝜏𝑘,𝑖(𝒘𝑘,𝑖).
After simple rearrangement, one can see that the sequence {𝒘𝑘,𝑖−𝛽1𝒘𝑘,𝑖−1
1−𝛽1}
are (approximately) doing SGD within one epoch (with coordinate-
wise but constant learning rate 𝝂𝑘,𝑖)! We define
𝒖𝑘,𝑖≜𝒘𝑘,𝑖−𝛽1𝒘𝑘,𝑖−1
1−𝛽1.
Then, further notice that the distance between 𝒖𝑘,𝑖=𝒘𝑘,𝑖+𝛽1𝒘𝑘,𝑖−𝒘𝑘,𝑖−1
1−𝛽1
and𝒘𝑘,𝑖is in order of one step’s update, and thus 𝒖𝑘,𝑖≈𝒘𝑘,𝑖.
Therefore, we choose our potential function as 𝑓(𝒖𝑘,𝑖). The Tay-
lor’s expansion of 𝑓at𝒖𝑘,0then provides a new descent lemma,
i.e.,
𝑓(𝒖𝑘+1,0)−𝑓(𝒖𝑘,0)≤⟨𝒖𝑘+1,0−𝒖𝑘,0,∇𝑓(𝒖𝑘,0)⟩
|                             {z                             }
First Order
+𝐿0+𝐿1∥∇𝑓(𝒘𝑘,0)∥
2∥𝒘𝑘+1,0−𝒘𝑘,0∥2,
|                                                   {z                                                   }
Second Order(7)
By noticing 𝒘𝑘,𝑖≈𝒖𝑘,𝑖≈𝒖𝑘,0, the first order term can be fur-
ther approximated by −⟨𝜂𝑘√𝝂𝑘,0+𝜉1𝑑⊙∇𝑓(𝒘𝑘,0),∇𝑓(𝒘𝑘,0)⟩which
is negative. The rest of the proof is the same as that of Stage I.
Remark 5.4 (On Why State-of-the-Art Results Do Not Achieve Tra-
jectory-Wise Convergence as Ours). The state-of-the-art analysis
of RR Adam under the 𝐿-smooth condition, as presented by Zhang
et al. [46] , also addresses the misalignment between 𝒘𝑘+1,0−𝒘𝑘,0
and−∇𝑓(𝒘𝑘,0). However, their approach does not employ a poten-
tial function, resulting in convergence results that are restricted
to in-expectation guarantees. Specifically, Zhang et al . [46] man-
age this misalignment by assuming a uniform distribution over
all possible shuffling orders and demonstrating that, under this as-
sumption, the expected value of 𝒘𝑘+1,0−𝒘𝑘,0is approximately equal
to−∇𝑓(𝒘𝑘,0). In contrast, our methodology introduces an auxiliary
function,𝑓(𝒖𝑘,𝑖), and examines the dynamics of 𝒖𝑘,𝑖. This approach
shifts the challenge from aligning 𝒘𝑘+1,0−𝒘𝑘,0with−∇𝑓(𝒘𝑘,0)to
aligning 𝒖𝑘+1,0−𝒖𝑘,0with−∇𝑓(𝒘𝑘,0). Such a strategy simplifies the
analytical process and facilitates the demonstration of trajectory-
wise convergence.
Remark 5.5 (Similar potential functions in the existing literature.).
We notice that similar potential functions have already been applied
in the analysis of other momentum-based optimizers, e.g., momen-
tum (S)GD in [ 15] and [ 26]. However, extending the proof to Adam
is highly-nontrivial. The key difficulty lies in showing that the first-
order expansion of 𝑓(𝒖𝑘,0)is positive, which further requires that
the adaptive learning rate does not change much within one epoch.
This is hard for Adam as the adaptive learning rate of Adam can be
non-monotonic. The lack of L-smooth condition makes the proof
even challenging due to the unbounded error brought by gradient
norms.6 Comparison Between Adam and SGD
Now we compare the convergence rate of Adam with SGD. To do
so, we need a lower bound of SGD in the same setting as Theorem
4.1. There are several existing lower bounds of SGD under (𝐿0,𝐿1)
smoothness condition (e.g., [ 8,43]). However, we find these lower
bounds cannot be directly applicable for comparison with Adam.
This is because:
•1) In the lower bound of [ 8,43], they pick the learning rate
before the construction of the objective function and initial-
ization point (we restate their lower bound in Appendix B.1
for completeness). In other words, it is possible that if we fix
the objective function and tune the learning rate (which is a
common practice in the training of deep neural networks),
SGD can converge very fast. For rigorous comparison with
Adam, we need a lower bound with reversed ordering. That
is, we need the following statement: “consider a fixed objec-
tive function and initialization point, then no matter how
we pick the learning rate, SGD suffers from a certain rate. "
•2) The lower bounds of SGD in [ 8,43] require constant
learning rate. However, since Adam in Theorem 4.1 uses
diminishing-learning-rate, we aim to establish a lower bound
of SGD with diminishing learning rate.
Unfortunately, there is no existing lower bound that satisfies
the above two properties. In the following theorem, we provide a
refined lower bound of SGD in the setup that we desired.
Theorem 6.1. For any𝐿0,𝐿1,𝑇>0, there exists an objective function
𝑓obeying Assumption 3.1, and an initialized parameter 𝒘0satisfying
𝑀=sup{∥∇𝑓(𝒘)∥:𝑓(𝒘)≤𝑓(𝒘0)}, such that∀𝜂1>0, the itera-
tions of SGD{𝒘𝑡}∞
𝑡=0satsifies min𝑡∈𝑇∥∇𝑓(𝒘𝑡)∥2=Ω(𝑀(𝑓(𝒘0)−
min𝒘∈R𝑑𝑓(𝒘))/√
𝑇).
The proof can be in Appendix A. The proof idea is mainly moti-
vated by [ 44]. We highlight some differences when we try to reach
the two properties mentioned previously.
•To reverse the ordering of “picking learning rate and func-
tions & initialization", we simply augment the worst-case
example in [ 44] into 2 dimensional space. It turns out this
simple trick is effective in the proof.
•To change constant learning rate into diminishing learning
rate, we show that: when the initial learning rate 𝜂0is larger
than a certain threshold, the decay rate of the learning rate
cannot offset the curvature explosion along the iteration,
causing divergence; on the other hand, when initial 𝜂0is
small, it would lead to slow convergence. This is a new find-
ing in(𝐿0,𝐿1)setting. We prove this result by mathematical
induction. This part of the discussion is not required in the
lower bound of [44] with constant learning rate.
Comparison between Adam and SGD.. Finally, we discuss the
implication the lower bound of SGD (Theorem 6.1) and the upper
bound of Adam (Theorem 4.1). In the lower bound of SGD, there is
an extra constant 𝑀which does not appear in the upper bound of
Adam. This allows us to compare the convergence rates of these
two algorithms.
We summarize our findings as follows. We emphasize that The-
orem 4.1 and Theorem 6.1 share exactly the same setting: both
 
2966KDD ’24, August 25–29, 2024, Barcelona, Spain Bohan Wang et al.
consider function class under the same assumptions; both SGD
and Adam use diminishing learning rate. Therefore, the following
comparison is rigorous.
Finding 1: When𝐷0=0. Adam converges to stationary point
with rateO
1
𝑇
while GD converges with rate O
1√
𝑇
. So Adam
converges (to stationary points) faster.
Finding 2: When𝐷0>0. There exists a set of 𝒘with infinite
Lebesgue measure, such that, when starting at any 𝒘in this set,
Adam converges (to the neighborhood of stationary points) faster
than SGD.
Note that the above statement “algorithm 1 converges faster than
algorithm 2” does not mean that algorithm 1 always converges faster
than algorithm 2. For sure, rarely can anyone make such a strong
statement. The above statement actually means that “the worst-case
complexity of algorithm 1 is faster than that of algorithm 2, and both
complexity bounds can be simultaneously achieved when working
on the same function and starting at the same initialization”. This
definition is adopted from [ 35], and it is a widely accepted definition
in the optimization field.
Proof. Finding 1 can be directly proved by plugging 𝐷0=
0into Theorem 4.1 and squaring the inequality. We now prove
Finding 2. First, we state an important fact from the proof of
Theorem 6.1.
Fact 1: For the counter-example we constructed in Theorem 6.1.
𝑀=sup{∥∇𝑓(𝒘)∥:𝑓(𝒘)≤𝑓(𝒘0)}goes to infinity as ∥𝒘∥goes
to infinity. Further, for any 𝐶>0, the set{𝒘:𝑀>𝐶}is of infinite
Lebesgue measure.
Based on Fact 1, for the worst-case example in Theorem 6.1, there
must exist a region in R𝑑where𝑀is larger than all the constant
terms in the upper bound of Adam in Theorem 4.1. Further, Such
region is of infinite Lebesgue measure. When running Adam and
SGD simultaneously on this worst-case example starting from any
𝒘in this region, the constants in the upper bound of Adam is smaller
than the constants in the lower bound of SGD. Since the upper and
lower bounds share the same rate, so Adam is faster. Note that
there is an additional constant term in the upper bound of Adam
(4), so we conclude that Adam converges to the neighborhood of
stationary points faster than SGD. □
Note that when 𝐷0>0, Adam is still guaranteed to converge
faster, but only to the neighborhood in lieu of the exact stationary
points. We emphasize that this “neighborhood" cannot be elimi-
nated since there is a counter-example showing that Adam cannot
reach 0 gradient when 𝐷0>0(see Figure 2). So this is an in-
trinsic property of Adam, rather than the limitation of the theory.
Nevertheless, we believe the effect of “not converging to exact sta-
tionary points" is minor in practice. This is because: 1) As shown
in Theorem 4.1 and Figure 2, the size of the “ambiguity zone" is
inversely proportional to 𝛽2. Since𝛽2is often chosen to be close to
1, the ambiguity zone shrinks and becomes negligible. 2) Machine
learning tasks do not pursue high-precision solutions (as much as
other fields like PDE). Practitioners usually aim to efficiently find
approximate solutions, rather than exact solutions that over-fit the
training data.To our knowledge, the discussion above is the first time that
Adam and SGD are rigorously compared in the same setting where
the advantage of Adam can be revealed. We believe these results
shed new light on understanding the benefit of Adam.
Finally, we briefly explain why the upper bound of Adam is
independent of 𝑀. Intuitively, this is because: (1) it uses different
learning rates for different components of 𝒘. (2) For each component
of𝒘, the effective learning rate adjusts according to the gradient
norm (thus according to the local smoothness). Even though the
initial effective learning rate is small, it gets larger when moving
in a flat landscape. Combining together, the initial learning rate of
Adam can be independent of 𝑀, and so is its convergence rate.
7 Discussion
Adam’s Advantage over Gradient Descent with Gradient Clip-
ping. Zhang et al . [43] established that gradient descent (GD) and
stochastic gradient descent (SGD) with gradient clipping are con-
vergent under the (𝐿0,𝐿1)smooth condition. A natural inquiry
arises concerning the benefits of Adam over GD/SGD when gradi-
ent clipping is employed. While we lack robust theoretical backing
to fully answer this question, one discernible advantage of Adam, as
inferred from our results, is its capability to manage more intricate
noise profiles that adhere to the affine variance noise assumption.
In contrast, the current analyses of GD/SGD with gradient clip-
ping within the(𝐿0,𝐿1)-smooth framework presuppose that the
deviation between the stochastic gradient and the true gradient is
uniformly bounded with certainty—an assumption more stringent
than the one we consider. Indeed, a recent work [ 13] demonstrates
that there exists a counterexample satisfying Assumption 3.2 over
which SGD with gradient clipping fails to converge. Together with
Theorem 4.1, their result demonstrate that a wide range of applica-
tion scenario of Adam than SGD with gradient clipping.
Insights for Practitioners. Here we discuss the insights our
Theorem 4.1 can provide to practitioners. Firstly, the widespread
adoption of Adam among practitioners, evidenced by its exten-
sive citation record, underscores the importance of a theoretical
understanding of the algorithm.
Secondly, our findings offer theoretical support for a prevalent
practice among practitioners: for tasks involving architectures like
Transformers and LSTMs, Adam is often favored over SGD.
Lastly, in light of the convergence criteria delineated in Theorem
4.1, we propose practical guidance for hyperparameter selection
when employing Adam. Specifically, we recommend increasing 𝛽2
and experimenting with various 𝛽1values that satisfy 𝛽1<√︁
𝛽2.
This heuristic could potentially reduce the computational burden
associated with exhaustive hyperparameter exploration across the
(𝛽1,𝛽2)space.
Limitations of Theorem 4.1. While Theorem 4.1 represents
a significant advancement in establishing the convergence of RR
Adam under non-uniform smoothness conditions, it is not without
its limitations. Specifically, Theorem 4.1 applies to cases where
𝛽1=0, implying the absence of momentum. The theorem does not
distinguish between the convergence rates when 𝛽1=0and when
𝛽1>0, thus not demonstrating the benefits of momentum. The
theoretical elucidation of momentum’s advantage within Adam’s
 
2967Provable Adaptivity of Adam under Non-uniform Smoothness KDD ’24, August 25–29, 2024, Barcelona, Spain
convergence analysis remains a complex question. The role of mo-
mentum is not fully understood even in momentum SGD for non-
convex optimization [ 26], much less so for Adam. We acknowledge
the importance of this question but consider it beyond the scope of
this paper, leaving it for future research.
8 Conclusions and Future directions
In this paper, we have taken a pioneering step towards a theoretical
understanding of the adaptivity inherent in the Adam optimization
algorithm. We present the first convergence results for RR Adam
under the (𝐿0,𝐿1)-smooth condition, which is both realistic and
closely aligned with practical scenarios. In contrast to existing
analyses of RR Adam under the stronger 𝐿-smooth condition, our
results further demonstrate a more robust form of convergence,
specifically trajectory-wise convergence, and indicate a reduced
distance to the stationary point.
Future Directions. An intriguing avenue for future research lies
in delineating the advantages of incorporating momentum in Adam.
Our Theorem 4.1 indicates an identical convergence rate for both
𝛽1=0(RMSProp) and 𝛽1>0(Adam), implying that the current
analysis does not differentiate between the iteration complexities of
Adam and RMSProp. Consequently, the specific benefits of momen-
tumin Adam remain elusive. This presents a substantial challenge,
given that the impact of momentum is not yet fully understood
even in the context of SGD with momentum. A possible strategy
could be to first establish a theoretical foundation for the advan-
tages of momentum in SGD, followed by extending these insights to
the analysis of Adam. Moreover, it would be compelling to explore
whether Adam can effectively manage more severe smoothness
conditions, such as those bounded by a higher-order polynomial of
the gradient norm.
Supplementary materials. Supplementary materials including
proofs can be found at https://arxiv.org/abs/2208.09900.
Acknowledgement. This work is founded by the Strategic Prior-
ity Research Program of the Chinese Academy of Sciences under
Grant No. XDB0680101, CAS Project for Young Scientists in Basic
Research under Grant No. YSBR-034, Innovation Project of ICT CAS
under Grants No. E261090, NSFC under Grant No. 12326608, Hetao
Shenzhen-Hong Kong Science and Technology Innovation Cooper-
ation Zone Project under Grant No.HZQSWS-KCCYB-2024016.
References
[1]Léon Bottou. 2009. Curiously fast convergence of some stochastic gradient
descent algorithms. In Proceedings of the symposium on learning and data science,
Paris, Vol. 8. 2624–2633.
[2]Léon Bottou. 2012. Stochastic gradient descent tricks. In Neural networks: Tricks
of the trade. Springer, 421–436.
[3]Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN
Training for High Fidelity Natural Image Synthesis. In International Conference
on Learning Representations.
[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[5]Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. 2021. Towards Practical
Adam: Non-Convexity, Convergence Theory, and Mini-Batch Acceleration. arXiv
preprint arXiv:2101.05471 (2021).
[6]Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. 2018. On the convergence
of a class of Adam-type algorithms for non-convex optimization. arXiv preprint
arXiv:1808.02941 (2018).[7]Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.
2021. Gradient descent on neural networks typically occurs at the edge of stability.
arXiv preprint arXiv:2103.00065 (2021).
[8]Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun
Zhuang. 2022. Robustness to Unbounded Smoothness of Generalized SignSGD.
arXiv preprint arXiv:2208.11195 (2022).
[9]Soham De, Anirbit Mukherjee, and Enayat Ullah. 2018. Convergence guarantees
for RMSProp and Adam in non-convex optimization and an empirical comparison
to Nesterov acceleration. arXiv preprint arXiv:1807.06766 (2018).
[10] Alexandre Défossez, Léon Bottou, Francis Bach, and Nicolas Usunier. 2020. A
Simple Convergence Proof of Adam and AdaGrad. arXiv preprint arXiv:2003.02395
(2020).
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International Conference on Learning
Representations.
[12] Timothy Dozat. 2016. Incorporating Nesterov momentum into Adam. (2016).
[13] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay
Shakkottai, and Rachel Ward. 2022. The Power of Adaptivity in SGD: Self-
Tuning Step Sizes with Unbounded Gradients and Affine Variance. arXiv preprint
arXiv:2202.05791 (2022).
[14] Sébastien Gadat and Ioana Gavra. 2020. Asymptotic study of stochastic adaptive
algorithm in non-convex landscape. arXiv preprint arXiv:2012.05640 (2020).
[15] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. 2015.
Global convergence of the heavy-ball method for convex optimization. In 2015
European control conference (ECC). IEEE, 310–315.
[16] Saeed Ghadimi and Guanghui Lan. 2013. Stochastic first-and zeroth-order meth-
ods for nonconvex stochastic programming. SIAM Journal on Optimization 23, 4
(2013), 2341–2368.
[17] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. 2016. Mini-batch stochas-
tic approximation methods for nonconvex stochastic composite optimization.
Mathematical Programming 155, 1 (2016), 267–305.
[18] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. 2021. A Novel
Convergence Analysis for Algorithms of the Adam Family. arXiv preprint
arXiv:2112.03459 (2021).
[19] Feihu Huang, Junyi Li, and Heng Huang. 2021. Super-Adam: faster and universal
framework of adaptive gradients. Advances in Neural Information Processing
Systems 34 (2021), 9074–9085.
[20] Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. 2021. Non-convex
distributionally robust optimization: Non-asymptotic analysis. Advances in Neural
Information Processing Systems 34 (2021), 2771–2782.
[21] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of NAACL-HLT. 4171–4186.
[22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[23] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In International Conference on Learning Representations.
[24] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. 2023. Convergence of Adam
Under Relaxed Assumptions. arXiv preprint arXiv:2304.13972 (2023).
[25] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
Gao, and Jiawei Han. 2020. On the Variance of the Adaptive Learning Rate
and Beyond. In International Conference on Learning Representations . https:
//openreview.net/forum?id=rkgz2aEKDr
[26] Yanli Liu, Yuan Gao, and Wotao Yin. 2020. An Improved Analysis of Stochastic
Gradient Descent with Momentum. Advances in Neural Information Processing
Systems 33 (2020).
[27] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. 2019. Adaptive gradient
methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843
(2019).
[28] Konstantin Mishchenko, Ahmed Khaled Ragab Bayoumi, and Peter Richtárik.
2020. Random reshuffling: Simple analysis with vast improvements. Advances in
Neural Information Processing Systems 33 (2020).
[29] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representa-
tion learning with deep convolutional generative adversarial networks. arXiv
preprint arXiv:1511.06434 (2015).
[30] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the Convergence of
Adam and Beyond. In International Conference on Learning Representations.
[31] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On the convergence of
Adam and beyond. arXiv preprint arXiv:1904.09237 (2019).
[32] Mark Schmidt and Nicolas Le Roux. 2013. Fast convergence of stochastic gradient
descent under a strong growth condition. arXiv preprint arXiv:1308.6370 (2013).
[33] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. 2021. RMSprop con-
verges with proper hyper-parameter. In International Conference on Learning
Representations.
[34] Suvrit Sra. 2014. Advanced Optimization: Lecture 18 Proximal methods, Mono-
tone operators. https://www.cs.cmu.edu/ suvrit/teach/lect18.pdf (2014).
 
2968KDD ’24, August 25–29, 2024, Barcelona, Spain Bohan Wang et al.
[35] Ruoyu Sun and Yinyu Ye. 2021. Worst-case complexity of cyclic coordinate
descent: O (nˆ 2) O (n 2) gap with randomized version. Mathematical Programming
185 (2021), 487–520.
[36] Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. 2021. Smg: A shuffling
gradient-based method with momentum. In International Conference on Machine
Learning. PMLR, 10379–10389.
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.
[38] Sharan Vaswani, Francis Bach, and Mark Schmidt. 2019. Fast and faster conver-
gence of SGD for over-parameterized models and an accelerated perceptron. In
The 22nd International Conference on Artificial Intelligence and Statistics. PMLR,
1195–1204.
[39] Xiaodong Yang, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. 2022. Normal-
ized/Clipped SGD with Perturbation for Differentially Private Non-Convex Opti-
mization. arXiv e-prints (2022), arXiv–2206.
[40] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Ku-
mar. 2018. Adaptive methods for nonconvex optimization. Advances in neural
information processing systems 31 (2018).
[41] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and San-
jiv Kumar. 2018. Adaptive Methods for Nonconvex Optimization. In Ad-
vances in Neural Information Processing Systems, S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/
90365351ccc7437a1309dc64e4db32a3-Paper.pdf[42] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. 2020. Improved analysis of
clipping algorithms for non-convex optimization. Advances in Neural Information
Processing Systems 33 (2020), 15511–15521.
[43] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. 2019. Why Gradient
Clipping Accelerates Training: A Theoretical Justification for Adaptivity. In
International Conference on Learning Representations.
[44] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. 2019. Why Adam beats SGD for
attention models. (2019).
[45] Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan
Luo. 2024. Why Transformers Need Adam: A Hessian Perspective. arXiv preprint
arXiv:2402.16788 (2024).
[46] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo.
2022. Adam Can Converge Without Any Modification on Update Rules. Advances
in Neural Information Processing Systems (2022).
[47] Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan
Gu. 2018. On the convergence of adaptive gradient methods for nonconvex
optimization. arXiv preprint arXiv:1808.05671 (2018).
[48] Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang,
and Yong Yu. 2018. Adashift: Decorrelation and convergence of adaptive learning
rate methods. arXiv preprint arXiv:1810.00143 (2018).
[49] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. 2019. A sufficient
condition for convergences of Adam and rmsprop. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 11127–11135.
 
2969