Topology-monitorable Contrastive Learning on Dynamic Graphs
Zulun Zhu
Nanyang Technological University
Singapore
ZULUN001@ntu.edu.sgKai Wang
Nanyang Technological University
Singapore
kai_wang@ntu.edu.sgHaoyu Liu
Nanyang Technological University
Singapore
haoyu.liu@ntu.edu.sg
Jintang Li
Sun Yat-sen University
Guangzhou, China
lijt55@mail2.sysu.edu.cnSiqiang Luo‚àó
Nanyang Technological University
Singapore
siqiang.luo@ntu.edu.sg
ABSTRACT
Graph contrastive learning is a representative self-supervised graph
learning that has demonstrated excellent performance in learning
node representations. Despite the extensive studies on graph con-
trastive learning models, most existing models are tailored to static
graphs, hindering their application to real-world graphs which
are often dynamically evolving. Directly applying these models to
dynamic graphs brings in severe efficiency issues in repetitively
updating the learned embeddings. To address this challenge, we
propose IDOL, a novel contrastive learning framework for dynamic
graph representation learning. IDOL conducts the graph propaga-
tion process based on a specially designed Personalized PageRank
algorithm which can capture the topological changes incrementally.
This effectively eliminates heavy recomputation while maintain-
ing high learning quality. Our another main design is a topology-
monitorable sampling strategy which lays the foundation of graph
contrastive learning. We further show that the design in IDOL
achieves a desired performance guarantee. Our experimental re-
sults on multiple dynamic graphs show that IDOL outperforms the
strongest baselines on node classification tasks in various perfor-
mance metrics.
CCS CONCEPTS
‚Ä¢Information systems ‚ÜíWeb mining; ‚Ä¢Theory of computa-
tion‚ÜíDynamic graph algorithms.
KEYWORDS
Dynamic graph, contrastive learning, PageRank
ACM Reference Format:
Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo. 2024. Topology-
monitorable Contrastive Learning on Dynamic Graphs. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671777
‚àóSiqiang Luo is the corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36717771 INTRODUCTION
Graph representation learning has gained significant attention with
the burgeoning of graph data across various applications, such as
social networks [ 24,28], online transactions [ 10,23], recommen-
dation systems [ 21,77] and time-series traffic forecasting [ 16,61].
Given the vast presence of unlabeled graph data, self-supervised
learning on graphs has surged in popularity, which fosters the
success of graph contrastive learning acting as a representative
self-learning methodology [ 25,29,47,48,65]. By maximizing the
similarity among positive samples and dissimilarity among nega-
tive samples, graph contrastive learning has demonstrated excellent
performance in pretraining graph neural networks (GNNs) without
manual labeling, thereby markedly boosting the model performance
in the downstream tasks [39, 52].
Many real-world graphs are intrinsically dynamic, characterized
by the frequent updates of nodes, edges, and attributes [ 76]. For
example, Pinterest [ 74] and Tencent [ 26] use the graph structure to
measure the real-time proximity among users, where each user is a
node and the interaction between users is represented by an edge.
The graph data can be updated thousands of times per second due to
the extensive user base. In such dynamic scenarios, existing research
on graph contrastive learning, originally designed for static graphs,
becomes largely inapplicable [ 44,69,75], because the pretrained
node representations lack the adaptability to accommodate new
topological changes, leading to a decline in effectiveness.
Limitations of Existing Methods. Recent studies [ 3,34,56]
start to explore graph contrastive learning approaches consider-
ing temporal dynamics. They segment the spatial structure of an
evolving graph into multiple snapshots given a time window, which
is called the discrete-time dynamic graph (DTDG). Unfortunately,
there are two significant limitations of this category of approaches:
(i) First, these models still lack efficiency, particularly when applied
to large-scale graphs, since the node embeddings have to be regener-
ated via the GNN encoder (e.g., GCN [ 30], TGAT [ 63]) when updates
occur. (ii) Second, the general positive/negative sampling strategy
based on graph augmentation is not tailored to the dynamic graphs
and has been proven insufficient for dynamic graph contrastive
learning [ 13,27,54,67]. For example, one recent method [ 56] as-
sumes that the evolution of a graph unfolds smoothly and thus
directly crafts positive and negative samples from the historical
graphs with short and long timespans. Nonetheless, this sampling
strategy exhibits limited effectiveness because it falls short in ob-
serving the topological changes and has the potential to erroneously
classify stationary nodes as negative samples or evolving nodes as
4700
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
positive samples, which goes against the intuition of contrastive
objectives.
We contend that the aforementioned issues can be alleviated
by the topology-monitorable capacity, i.e., taking full advantage
of topological changes in dynamic graphs, which is overlooked by
existing contrastive learning studies based on DTDG. Such a design
effectively addresses the limitations mentioned earlier. Firstly, by
continuously monitoring the topological structure, we can iden-
tify evolving nodes and subsequently perform local updates on
their embeddings, eliminating the need for repetitive and global
calculations. Secondly, by quantifying the extent of these changes,
we can use this measure to inform and refine our sampling strate-
gies. Therefore, to endow graph contrastive learning with the
topology-monitorable capacity, we first employ the paradigm of
continuous-time dynamic graphs (CTDG) [ 43], which are repre-
sented as a series of temporal edges with continuous dynamics
and can exhibit more evolving locality than DTDG [ 34]. Then, in-
spired by recent GNN simplification works based on Personalized
PageRank (PPR) [ 8,31,36], we propose to measure the topological
changes of dynamic graphs in both graph propagation and pairwise
sampling processes with efficient PPR-based algorithms.
Contributions. Based on this insight, we propose IDOL1, a pi-
oneering approach for node representation learning in continuous-
time dynamic graphs (CTDG). IDOL aims to improve the efficiency
of graph contrastive learning in a dynamic scenario with a paradigm
of incremental update. For this purpose, we suggest to adopt an
efficient graph propagation method based on Personalized PageR-
ank (PPR) [ 9], which enables the incremental embedding update
based on historical embeddings and eliminates repetitive embed-
ding recomputation. In our evaluation, IDOL can achieve up to 3x
faster pretraining compared with baselines. Moreover, we design a
topology-monitorable sampling strategy in self-supervised learning
of dynamic graphs, avoiding repetitive computation for augmen-
tation as well as relaxing the evolving assumption of graphs in
recent literature [ 6,56,64]. From a model design perspective, we
are pioneers in implementing the decoupled architecture (e.g., de-
coupling propagation and training) within the realm of contrastive
learning. Our approach conveys a crucial insight: the effectiveness
in downstream tasks is closely linked to the performance achieved
in our decoupled model design of contrastive learning. This could
potentially spark further innovative investigations within the graph
community. Our contributions are summarized as follows:
‚Ä¢We identify the limitations of existing contrastive learning
methods in the dynamic graph scenario and introduce IDOL as
an innovative self-supervised learning solution, crafted to excel in
continuous-time dynamic graphs (CTDG).
‚Ä¢We adopt a PPR-based technique for incremental embedding
update and present a topology-monitorable sampling method to
generate contrastive pairs, thereby significantly boosting training
efficiency and effectively enhancing the embedding quality in con-
trastive learning, respectively.
‚Ä¢To the best of our knowledge, IDOL is a pioneering algorithm
that applies decouple propagation and training in contrastive learn-
ing, and in the meanwhile attaining a desired complexity (see Table
1). Within this innovative framework, we theoretically establish a
1Incremental Dynamic Graph Co trastive Learningperformance guarantee for downstream tasks by linking the con-
trastive loss with the downstream task loss.
‚Ä¢We conduct comprehensive experiments across various real
datasets to demonstrate the efficiency and effectiveness of our ap-
proach in dynamic graph scenarios. Remarkably, IDOL outperforms
in prediction accuracy for dynamic node classification, while en-
tailing notably less pretraining time.
2 PRELIMINARY AND RELATED WORKS
In this section, we begin by introducing the problem definition of
this work and review the basic graph contrastive learning frame-
work and Personalized PageRank (PPR) algorithms related to our
design. We list the frequently used notations in Appendix A.1. We
leave the detailed proofs of this paper in our technique report [ 12].
2.1 Problem Definition
LetG=(V,E,ùëø)be a directed and attributed graph, where V=
{ùë£1,ùë£2,...,ùë£ùëõ}is the set ofùëõnodes,E=V√óV is the set ofùëöedges
andùëø={ùíô1,ùíô2,...,ùíôùêπ}is the set of node attribute matrix with
ùíôùëñ‚ààRùëõ√ó1representing the attribute vector in ùëñ-th dimension. For
each nodeùë£‚ààV,Nùëúùë¢ùë°(ùë£)stands for the out-neighbors of ùë£and
Nùëñùëõ(ùë£)stands for the in-neighbors.
Then, we define the continuous-time dynamic graph (CTDG).
Consider an initial graph of CTDG G0=(V0,E0), the set of update
events Œìupon the graph consists of inserts and deletes of edges2,
represented as Œì={ùëí1,ùëí2,...,ùëíùëñ,...ùëíùëù}. After theùëñ-th edge update
ùëíùëñ={ùë¢ùëñ,ùë£ùëñ}arrives the system, the current CTDG Gùëñ‚àí1is trans-
ferred toGùëñ. Ifùëíùëñalready exists inGùëñ‚àí1,ùëíùëñis treated as a delete from
(ùë¢ùëñ,ùë£ùëñ); otherwise, ùëíùëñis treated as an insert. Different with CTDG,
a discrete-time dynamic graph (DTDG) is represented as a series
of snapshot{G0,G1,...,Gùúè,...,Gùëá}, whereGùúèsignifies the status of
graph at time ùúè. Since the evolutionary process (e.g., update events)
between two snapshots (e.g., Gùúè1andGùúè2) is not captured, CTDG
offers a more comprehensive representation than DTDG.
The research problem of this paper, representation learning on
CTDG, is to learn node representation for each node ùë†at any times-
tamp. Assume there are ùëùupdate events based on Gùëñ, our target
is to calculate the ùêπ-dimensional node embeddings for the graph
Gùëñ+ùëùincrementally. The learned embeddings capture the dynamic
evolution of the CTDG and can be effectively applied to subsequent
graph tasks, such as node classification.
2.2 Graph Contrastive Learning
The representative scheme of Graph Contrastive Learning is Pre-
training and Evaluation, where the node representations are pre-
trained in a self-supervised manner first and then evaluated in
downstream tasks [ 39,71]. The main ingredients of the pretraining
include: (i) Sampling Strategy based on graph augmentation pro-
ducing positive and negative sample pairs, and (ii) Loss Function
supervising the model to determine the similarity of each specific
representation pair [39, 62]. Detailed related works of Graph Con-
trastive Learning are described in Appendix A.2.
In graph-related scenarios, the common sampling strategy gen-
erates two or more augmentation views of the original graph with
2The inserts and deletes of vertices can be replaced by adding and removing relevant
incident edges, hence we only discuss edge updates in this paper.
4701Topology-monitorable Contrastive Learning on Dynamic Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Comparison of pretraining complexity. ùêæstands for the number of convolution layers, ùëámeans the number of temporal
views, and ùêπis the dimension of node features.
Method Scenarios EncoderComplexity for Initial GraphComplexity for Updating ùëùEdgesEncoder Projector&Loss
DGI [58] static GCN [30] ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ) ùëÇ(4ùëõùêπ3) ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ+4ùëõùêπ3)
GRACE [75] static GCN ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ ùëÇ(4ùëõùêπ2+2ùëõ2ùêπ+2ùëõùêπ)ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ+2ùëõ2ùêπ)
BGRL [55] static GCN ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ)ùëÇ(4ùëõùêπ2+4ùëõùêπ) ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ+4ùëõùêπ2)
GGD [72] static GCN ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ)ùëÇ(4ùëõùêπ2+2ùëõùêπ) ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ+4ùëõùêπ2)
DDGCL [56] dynamic TGAT [63] ùëÇ(4ùêæùëõ2ùêπ+4ùêæùëöùêπ)ùëÇ(2ùëõùêπ3+2ùëõ2ùêπ3) ùëÇ(4ùêæùëõ2ùêπ+4ùêæùëöùêπ+2ùëõ2ùêπ3)
CLDG [64] dynamic GCN ùëÇ(4ùêæùëáùëõùêπ2+4ùêæùëáùëöùêπ)ùëÇ(4ùëáùëõùêπ2+2ùëõ2ùëá2ùêπ)ùëÇ(4ùêæùëáùëõùêπ2+4ùêæùëáùëöùêπ+2ùëõ2ùëá2ùêπ)
IDOL (ours) dynamic PPR+MLP ùëÇ(4ùëõùêπ2+2ùêæùëöùêπ) ùëÇ(ùëõùêπ) ùëÇ(4ùëõùêπ2+ùêæùëöùêπ+ùëùlogùëõ)
various augmentation functions such as random node dropping and
feature masking [ 27,67]. According to the connectivity similarity,
one popular mechanism is generally treating the representations
of the same node in two different views as a positive sample pair,
otherwise a negative one [ 56,75]. The loss function of contrastive
learning denotes the negative estimated mutual information, where
the commonly used formats include Jensen‚ÄìShannon divergence
[22], NCE [19] and InfoNCE [46].
Specifically, for one node ùë†in the graph, the positive sample set
{+ùíõùë†}and negative sample set {‚àíùíõùë†}are extracted via the sampling
strategy and we denote the node embedding of node ùë†asùíõùë†3.
Then, in the representation learning model, a GNN encoder ùëìùúÉ(¬∑)
parameterized by ùúÉis utilized to generate node representations,
accompanied by a projector ùëù(¬∑)(e.g., dot product) measures the
similarity of each representation pair. Finally, the pretraining goal
is to obtain the optimal encoder ùúÉvia the contrastive learning loss,
such as the following InfoNCE loss used by [49, 62, 75]:
LùêºùëõùëìùëúùëÅùê∂ùê∏ =‚àí1
ùëõùëõ‚àëÔ∏Å
ùë†=1logùëíùëù(ùíõùë†,+ùíõùë†)
ùëíùëù(ùíõùë†,+ùíõùë†)+√çùëÄ
ùëñ=1ùëíùëù(ùíõùë†,‚àíùíõùëñ), (1)
whereùëù(ùíõ1,ùíõ2)=ùëìùúÉ(ùíõ1)ùëìùúÉ(ùíõ2)‚ä§/ùúèandùúèis a temperature hyper-
parameter. The InfoNCE loss estimates the mutual information
between each node representation ùíõùë†with one positive sample+ùíõùë†
andùëÄnegative samples{‚àíùíõùëñ}.
2.3 Personalized PageRank
Given a source node ùë†and a target node ùë°in a graph, the Person-
alized PageRank (PPR) [ 9]ùúã(ùë†,ùë°)is a topology-based measure to
reflect the probability that a random walk starting from ùë†ends
atùë°. The single-source PPR (SSPPR), which computes ùúã(ùë†,ùë°)for
anyùë°in a graph given the source node ùë†, has been a significant
building block of various applications [ 18,37,38,41,42,78]. Given
the source node ùë†, the SSPPR vector ùùÖ(ùë†)‚ààR1√óùëõaims to obtain
the solution of the following equation:
ùùÖ(ùë†)=‚àû‚àëÔ∏Å
ùëñ=0ùõº(1‚àíùõº)ùëñ¬∑
ùë®ùë´‚àí1ùëñ
¬∑ùíÜùë†, (2)
where ùë®is the adjacent matrix, ùë´is the degree matrix, ùõºis the decay
factor of random walk, and ùíÜùë†is a one-hot vector with ùíÜùë†(ùë†)=1,
respectively. The SSPPR computation needs to extract eigenvalues
of anùëõ√óùëõmatrix and is expensive on a large-scale graph [ 59]. In
3In the later sections, we call that ùíõùë†and+ùíõùë†(resp.‚àíùíõùë†) can form a positive (resp.
negative) pair for a clear presentation.order to compute SSPPR efficiently, Forward Push algorithm [ 5] is
developed to approximate the value ùúã(ùë†,ùë°)given the source node ùë†
andùë°‚ààV, which achieves an underestimate with an error bound.
A detailed description can be found in our technique report [12].
PPR-based Node Embedding. Since PPR can reflect the topo-
logical relationship between nodes, recent studies of simplified
GNNs [ 36] utilize the SSPPR algorithm combined with the attribute
matrix to calculate the embedding of nodes. Such PPR-based node
embedding is more efficient and scalable compared to graph prop-
agation via GNNs, and it facilitates incremental updates through
local modifications to the termination probability of random walks.
Specifically, SCARA [ 36] adopts the Forward Push algorithm to
enhance the scalability of the graph propagation and efficiency.
Instant [ 73] and DynAnom [ 17] utilize a model structure consisting
of PPR-based embedding and refreshing rules to achieve the incre-
mental update for the node embeddings. The hallmark of methods
employing Forward Push is their propagation process, which offers
a fundamental approximation guarantee. Interestingly, our empiri-
cal findings reveal that prioritizing efficiency over this guarantee
can actually enhance accuracy in contrastive learning. Therefore,
we simply aggregate node information from a fixed number of
neighboring hops to not only accelerate embedding computation
but also preserve high-quality results.
3 METHODOLOGY
3.1 Overview
In this section, we introduce our proposed IDOL, which is divided
into three key components as depicted in Figure 1. Firstly, different
from most of the existing contrastive learning methods utilizing
GCN [ 30] or TGAT[ 63] as graph encoder, we decouple the graph
propagation with training, and utilize PPR-based embeddings and
multi-layer perceptron (MLP) to incorporate the structural informa-
tion in the initial graph (Sec. 3.2). Secondly, given upcoming update
events, we incrementally update the node embedding efficiently
to avoid the propagation from scratch (Sec. 3.3). Moreover, instead
of generating two different augmented graphs from scratch, we
propose a novel topology-monitorable sampling strategy to avoid
the repetitive computation and directly select positive and negative
samples from the pre-existing historical embeddings (Sec. 3.4). Fi-
nally, we freeze the encoder parameters after pretraining and output
the node embeddings for evaluation in the downstream tasks (e.g.,
node classification). By exploring the connectivity between classi-
fication accuracy and the convergence of our contrastive loss, we
prove that our decoupled and simplified framework IDOL provides
4702KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
Maximize
Minimize
Updated
Embeddings
PPR-based
EmbeddingsIncremental Updates
Topology-
monitorable 
Sampling   MLP
 Encoder
Figure 1: Graphical illustration of the IDOL architecture. {zùë†}ùëñ
denotes the node embeddings in Gùëñ.{+zùë†}and{‚àízùë†}denote
the positive and negative sample sets, respectively.
a performance guarantee of the downstream task, demonstrating
the theoretical feasibility and effectiveness (Sec. 3.5).
3.2 PPR-based Embedding by K-hop Push
To achieve scalable node embeddings and update them incremen-
tally in dynamic graphs, we utilize PPR-based algorithms to pre-
calculate node embeddings before processing them through the
MLP layer. The dominant paradigm of existing PPR-based embed-
ding derives from the insight of Forward Push based on a residue
threshold (e.g., [ 36]). Nonetheless, Forward Push, which aims to
approximate PPR scores with an accuracy guarantee, falls short in
effectively showcasing the expressive potential of all nodes during
graph propagation. This limitation can lead to an uneven distri-
bution of smoothness in the node embedding space, ultimately
resulting in a decline in performance [ 45,70]. The experiments in
Sec. 4.5 indicate that this limitation leads to reduced prediction
accuracy compared with our method under the same propagation
time, alongside a heightened sensitivity to hyperparameter settings.
To solve the aforementioned issue, we incorporate the intuition
of APPNP [ 31] aggregating information of K-hop neighbors, and
employ a K-hop Push algorithm to pre-compute the PPR-base node
embeddings. Specifically, we deprecate the approximation paradigm
ofForward Push and adopts the ùêæ-hop truncated terms of Eq. 2 as
follows:
ùíâùëñ=ùêæ‚àí1‚àëÔ∏Å
ùëô=0ùõº(1‚àíùõº)ùëô¬∑
ùë®ùë´‚àí1ùëô
¬∑ùíôùëñ. (3)
By employing this design, we ensure that each node can acquire
a balanced smoothness distribution, aligning precisely with a sim-
plified complexity of ùëÇ(ùêæùëöùêπ)as demonstrated in SGC [ 60], given
the hop number ùêæand feature dimension ùêπ.
Initial Graph Embedding. To calculate the node embeddings in
initial graphG0, we conduct the K-hop Push algorithm to calculate
the PPR values of all nodes for each dimension of the attribute
matrix ùëø, which is shown in Algorithm 1. Specifically, we extract
the attribute vector (column vector) ùíôùëñ‚ààRùëõ√ó1(1‚â§ùëñ‚â§ùêπ) for each
dimension of ùëøand assign it as the initial value of residue vector ùíì0
ùëñ.
Meanwhile, the reserve vector ùíâùëñ‚ààRùëõ√ó1is set as the all-zero vector.
Here in the PPR procedure, ùíìùëô
ùëñrepresents the unpropagated massAlgorithm 1: K-hop Push Algorithm
Input : GraphG=(V,E), teleport probability ùõº, feature
matrix ùëø, number of propagation layer ùêæ.
Output: Reserve vector ùíâùëñand residue vector ùíìùêæ
ùëñ(1‚â§ùëñ‚â§ùêπ)
1foreach ùíôùëñ‚ààùëødo
2 ùíì0
ùëñ=ùíôùëñ;ùíâùëñ=0;
3 forùëô=0toùêæ‚àí1do
4 ùíìùëô+1
ùëñ=0;
5 foreachùë†‚ààV do
6 foreachùë°‚ààNùëúùë¢ùë°(ùë†)do
7 ùíìùëô+1
ùëñ(ùë°)+=(1‚àíùõº)¬∑ùíìùëô
ùëñ(ùë†)
|Nùëúùë¢ùë°(ùë†)|;
8 ùíâùëñ(ùë†)+=ùõº¬∑ùíìùëô
ùëñ(ùë†);ùíìùëô
ùëñ(ùë†)=0;
9 clear ùíìùëô
ùëñ;
of attribute vector ùíôùëñ, and ùíâùëñrepresents the propagated part. Then,
we iteratively repeat the following steps for ùëô={0,1,...,ùêæ‚àí1}.
For each node ùë†‚ààV,(1‚àíùõº)fraction of ùíìùëô
ùëñ(ùë†)will be propagated
into the out-neighbors ùë°‚ààNùëúùë¢ùë°(ùë†), and hence each out-neighbor
ùë°iteratively receives (1‚àíùõº)¬∑ùíìùëô
ùëñ(ùë†)/|Nùëúùë¢ùë°(ùë†)|and add it to the
residue ùíìùëô+1
ùëñ(ùë°). Additionally, ùõºfraction of ùíìùëô
ùëñ(ùë†)will be transferred
into the reserve vector ùíâùëñ(ùë†)andùíìùëô
ùëñ(ùë†)is reset as 0. At the end of
ùëô-th iteration, we clear the residue vector ùíìùëô
ùëñto save the space.
After we conduct the propagation and obtain the K-hop reserve
vector ùíâùëñ(1‚â§ùëñ‚â§ùêπ) via K-hop Push for all dimensions in ùëø, the
final node embedding matrix can be concatenated as:
ùíÅ=Concat(ùíâ1,ùíâ2,...,ùíâùêπ), (4)
where each node embedding ùíõùë†‚ààR1√óùêπ. Compared with existing
contrastive learning methods utilizing GCN [ 30] or TGAT [ 63] for
graph propagation, the above PPR-based embedding process is more
efficient and scalable. We will further discuss the computational
complexity in the Sec. 3.6.
3.3 Incremental Embedding Update
In our decoupled contrastive learning approach, we initially pre-
calculate node embeddings for the initial graph using the previously
described steps. However, the graphs incur frequent updates upon
the nodes and edges in the dynamic scenarios, which requires
us to incorporate the updated information and reform the node
embedding. Instead of recalculating embedding from scratch or
requiring repetitive convolution operations such as the works in
[6,56], we focus on transforming partial embeddings incremen-
tally triggered by the update events. Due to the characteristics of
the PPR algorithm, one can locally adjust the reserve vector ùíâùëñ
and residue vector ùíìùëñfor value updates provided that the invariant
property between them is maintained [ 17,73]. Following the up-
dating streamline of the vanilla PPR methods employed in Instant
[73] and DynAnom [ 17], we adopt a similar approach to refresh
the embeddings while holding the invariant property. For com-
pleteness, we refer interested readers to our technical report [ 12]
for a complete proof and references. Given the invariant property
ùíâùëñ(ùë¢)+ùõºùíìùëñ(ùë¢)=ùõºùíôùëñ(ùë¢)+√ç
ùë£‚ààNùëúùë¢ùë°(ùë¢)(1‚àíùõº)ùíâùëñ(ùë£)
|Nùëúùë¢ùë°(ùë£)|(ùë¢‚àà V ), we
summarize the heart of updating rules in the following lemma:
4703Topology-monitorable Contrastive Learning on Dynamic Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Lemma 1. When adding an edge (ùë¢,ùë£), performing the follow-
ing rules can maintain this invariant property: ùíìùëñ(ùë¢)‚Üê ùíìùëñ(ùë¢)‚àí
ùíâùëñ(ùë¢)
ùõº|Nùëúùë¢ùë°(ùë¢)|,ùíìùëñ(ùë£)‚Üêùíìùëñ(ùë£)+(1‚àíùõº)ùíâùëñ(ùë¢)
ùõº|Nùëúùë¢ùë°(ùë¢)|,ùíâùëñ(ùë¢)‚Üêùíâùëñ(ùë¢)¬∑|Nùëúùë¢ùë°(ùë¢)|+1
|Nùëúùë¢ùë°(ùë¢)|.
Compatibility with DTDG. By adhering to these principles,
edges are incorporated into the CTDG based on their chronolog-
ical sequence. DTDG, a specific instance of CTDG, can also be
managed using the update rules by leveraging the timestamps of
edges across various snapshots. Given that the degree of node
ùë¢at timeùúèisNùúè
ùëúùë¢ùë°(ùë¢)and the degree change from ùúè1toùúè2is
Œîùúè1,ùúè2(ùë¢)=Nùúè2
ùëúùë¢ùë°(ùë¢)‚àíNùúè1
ùëúùë¢ùë°(ùë¢), we can update the reserve and
residue vector from snapshot Gùúè1toGùúè2utilizing the following
derivative rules: ùíìùëñ(ùë¢)‚Üê ùíìùëñ(ùë¢)‚àíŒîùúè1,ùúè2(ùë¢))ùíâùëñ(ùë¢)
ùõº|Nùúè1
ùëúùë¢ùë°(ùë¢)|,ùíìùëñ(ùë£)‚Üê ùíìùëñ(ùë£)+
(1‚àíùõº)Œîùúè1,ùúè2(ùë¢)ùíâùëñ(ùë¢)
ùõº|Nùúè1
ùëúùë¢ùë°(ùë°)|,ùíâùëñ(ùë¢)‚Üêùíâùëñ(ùë¢)¬∑|Nùúè2
ùëúùë¢ùë°(ùë°)|
|Nùúè1
ùëúùë¢ùë°(ùë°)|.
With the rules above, we incrementally update the vectors of
nodes affected by the update event, accompanied by updating their
node embeddings. We provide the pseudo-code of updating the node
embeddings in our technique report [ 12]. With such embedding
update approach, we next discuss how to generate contrastive pairs
for the updated embeddings using a topology-based approach as
detailed in Section 3.4.
3.4 Topology-monitorable Sampling
The concepts of graph augmentation are central to the paradigm of
contrastive learning. Recent works [ 56] emphasize the assumption
that the evolution of graphs unfolds smoothly, and the overall
properties remain relatively stable despite occasional edge updates.
However, this constraint is impractical and too strong in real-world
applications, as a node might undergo substantial updates in a short
interval while remaining unchanged over an extended one. We
experimentally prove that employing this assumption is suboptimal
to capture the practical dynamics in Sec. 4.5.
In order to break this strong constraint, we propose a topology-
monitorable sampling strategy to generate positive and negative
samples from the topological views in the dynamic graph. Formally,
given a snapshotGùëñ+ùëùto be predicted, we explicitly exploit the
topological transformation caused by ùëùupdate events and adopt a
PPR-based approach to distinguish the positive and negative pairs
from the historical embeddings of Gùëñ.
Topological measurement by PPR. As a measurement of
topological information, the difference in SSPPR values between
two snapshotsGùëñandGùëñ+ùëùreflect the topological change on a
node caused by ùëùupdates. Since SSPPR contains ùúã(ùë†,ùë°)for any
ùë°in a graph given the source node ùë†, we utilize the maximum
value ofùúã Gùëñ+ùëù,ùë†,ùë°‚àíùúã(Gùëñ,ùë†,ùë°)to describe the impact degree
of the nodeùë†caused byùëùupdates. Then we define an impact vector
ùë∞Gùëñ,Gùëñ+ùëù‚ààRùëõ√ó1representing the impact degrees on all graph nodes,
where each element of ùë∞Gùëñ,Gùëñ+ùëùis defined as:
ùë∞Gùëñ,Gùëñ+ùëù(ùë†)=max
ùë°‚ààVùëñùúã Gùëñ+ùëù,ùë†,ùë°‚àíùúã(Gùëñ,ùë†,ùë°),ùë†‚ààVùëñ.(5)
Upper bound of impact vector. As the graph evolves, the
measurement ùë∞Gùëñ,Gùëñ+ùëù(ùë†)can capture the upper bound of topolog-
ical change on node ùë†and guide us to distinguish positive and
negative samples. However, the exact result of ùë∞Gùëñ,Gùëñ+ùëùrequires a
time-consuming computation with respect to all-pair PPR values,Algorithm 2: Dynamic Sampling Algorithm
Input : Node embedding ùíõ‚Ä≤ùë†for allùë†‚ààVùëñbased on graphGùëñ,
residue threshold ùëüùëèùëöùëéùë•, gap threshold ùúÜ, update event
sequence Œì={(ùë¢1,ùë£1),(ùë¢2,ùë£2),....,(ùë¢ùëù,ùë£ùëù)}.
Output: Positive sample+ùíõùë†or negative sample‚àíùíõùë†for each
ùë†‚ààVùëñ+ùëùinGùëñ+ùëù
1/*Reverse Push from {ùë¢1,ùë¢2,...,ùë¢ùëù}*/
2ùíìùëè=0;ùùÖùëè=0;
3foreach(ùë¢ùëó,ùë£ùëó)‚ààŒìdo
4 ùíìùëè(ùë¢ùëó)=|Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢ùëó)|‚àí|Nùëúùë¢ùë°(Gùëñ,ùë¢ùëó)|
|Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢ùëó)|;
5while‚àÉùë†‚ààùëâs.t.ùíìùëè(ùë†)>ùëüùëèùëöùëéùë• do
6 foreachùë°‚ààNùëñùëõ(Gùëñ,ùë†)do
7 ùíìùëè(ùë°)+=(1‚àíùõº)¬∑ùíìùëè(ùë†)
|Nùëúùë¢ùë°(Gùëñ,ùë°)|;
8 ùùÖùëè(ùë†)+=ùõº¬∑ùíìùëè(ùë†);ùíìùëè(ùë†)=0;
9/*Sampling Contrastive Pairs */
10foreachùë†‚ààV do
11 ifùëüùëèùëöùëéùë•+ùùÖùëè(ùë†)
ùõº‚â•ùúÜthen
12‚àíùíõùë†=ùíõ‚Ä≤ùë†,‚àíùíÅGùëñ+ùëù.add(‚àíùíõùë†)
13 else
14+ùíõùë†=ùíõ‚Ä≤ùë†,+ùíÅGùëñ+ùëù.add(+ùíõùë†)
which refers to the calculation of ùúã Gùëñ+ùëù,ùë†,ùë°andùúã(Gùëñ,ùë†,ùë°)for
allùë†,ùë°‚ààVùëñ. Instead of the redundant calculation, we estimate the
upper bound of ùë∞Gùëñ,Gùëñ+ùëù(ùë†)in the following lemma:
Lemma 2. Given the update sequence Œì={(ùë¢1,ùë£1),(ùë¢2,ùë£2),...,
(ùë¢ùëù,ùë£ùëù)}, we have:
ùë∞Gùëñ,Gùëñ+ùëù(ùë†)‚â§ùëù‚àëÔ∏Å
ùëó=1|Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢ùëó)|‚àí|Nùëúùë¢ùë°(Gùëñ,ùë¢ùëó)|ùúã(Gùëñ,ùë†,ùë¢ùëó)
ùõº|Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢ùëó)|.
Based on Lemma 2, the impact on any ùë†‚ààVùëñcaused by the
update sequence Œìis related to the PPR value ùúã Gùëñ,ùë†,ùë¢ùëóand the
degree|Nùëúùë¢ùë° Gùëñ,ùë¢ùëó|and|Nùëúùë¢ùë° Gùëñ+ùëù,ùë¢ùëó|, whereùë¢ùëóis the start-
ing node of the updated edge in Œì. The calculation of ùúã Gùëñ,ùë†,ùë¢ùëófor
allùë†‚ààGùëñcan be implemented by Reverse Push algorithm focusing
on single-target PPR [ 4]. Nonetheless, the Reverse Push method can
only assist in estimating a single update, making it inefficient when
dealing with batch updates (e.g., update sequence Œì).
Considering the batch update events included in Œì, we incorpo-
rate the insight of Reverse Push to efficiently estimate the upper
bound of ùë∞Gùëñ,Gùëñ+ùëù(ùë†)afterùëùupdate events for each ùë†‚ààVùëñ, which is
shown in Algorithm 2 (lines 3-8). Similar to K-hop Push, we maintain
an auxiliary residue vector ùíìùëè. We assign the intial value of ùíìùëèatùë¢ùëóas
||Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢ùëó)|‚àí|Nùëúùë¢ùë°(Gùëñ,ùë¢ùëó)||
|Nùëúùë¢ùë°(Gùëñ+ùëù,ùë¢1)|, whereùë¢ùëóis the starting point of the
update event in Œì(line 4), and otherwise set it as 0. The difference is
that we set a residue threshold ùëüùëèùëöùëéùë• to early stop the iteration pro-
cess (line 5) and the residue value (e.g., ùíìùëè(ùë†)) is propagated along
the in-neighbor set Nùëñùëõ(Gùëñ,ùë†)(line 6). Notice that for Dynamic Sam-
pling we replace the manner utilized in K-hop Push since setting the
thresholdùëüùëèùëöùëéùë• can provide us with an intermediate upper bound of
ùë∞Gùëñ,Gùëñ+ùëù(ùë†)demonstrated as: ùë∞Gùëñ,Gùëñ+ùëù(ùë†)‚â§ùùÖùëè(ùë†)+ùëüùëè
ùëöùëéùë•
ùõº. To simplify
our presentation, we directly introduce this crucial intermediate
upper bound here. It plays a pivotal role in sampling contrastive
4704KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
pairs, as elaborated in the upcoming Lemma 3. The detailed proof
of this upper bound can be found in the technique report [12].
Sampling with impact vector. After the iteration in Reverse
Push ends, we compared the value ofùëüùëè
ùëöùëéùë•+ùùÖùëè(ùë†)
ùõºwith an empirical
gap threshold ùúÜto filter the positive samples from Gùëñsatisfying
ùë∞Gùëñ,Gùëñ+ùëù(ùë†)‚â§ùúÜ, as presented in the following lemma:
Lemma 3. Given the update event sequence Œì={(ùë¢1,ùë£1),(ùë¢2,ùë£2),
...,(ùë¢ùëù,ùë£ùëù)}, the result returned by Algorithm 2 guarantees that for
any node index ùë†in{+ùíõùë†}, we have ùë∞Gùëñ,Gùëñ+ùëù(ùë†)‚â§ùúÜ.
Based on Lemma 3, the nodes experiencing minor impacts, as de-
termined by ùúÜ, are selected. Then we treat the corresponding histor-
ical embeddings in Gùëñas the positive sample set {+ùíõùë†}. Meanwhile,
we retain other nodes that incur significant topological changes and
treat their historical embeddings as negative samples (line 10-14).
Here we also denote the positive and negative samples of Gùëñ+ùëùas
+ùíÅGùëñ+ùëù={+ùíõùë†}and‚àíùíÅGùëñ+ùëù={‚àíùíõùë†}respectively. The concrete
value ofùëõùëù=|+ùíÅGùëñ+ùëù|andùëõùëî=|‚àíùíÅGùëñ+ùëù|can be coordinated by
setting different threshold ùúÜand we can easily get ùëõùëù+ùëõùëî=ùëõ,
whereùëõis the number of nodes.
3.5 Theoretical Performance Guarantee
After pre-calculating the node embeddings and building contrastive
pairs, we form our contrastive loss within a decoupled architec-
ture. Generally, the contrastive loss enforces the embeddings to
be discriminative between the positive pairs from the joint dis-
tributionùëù(ùíõùë†,+ùíõùë†)and the negative pairs from the marginal dis-
tributionùëù(ùíõùë†)andùëù(‚àíùíõùë†). Given the positive sample set+ùíÅGùëñ+ùëù
and negative sample set‚àíùíÅGùëñ+ùëù, the node representations after
our MLP encoder is denoted as ùëìùúÉ(ùíõùë†),ùëìùúÉ(+ùíõùë†)andùëìùúÉ(‚àíùíõùë†). Here
ùëìùúÉ(ùíõùë†)=ùíõùë†¬∑ùíòùúÉ‚ààR1√óùêπ‚Ä≤, where ùíòùúÉ‚ààRùêπ√óùêπ‚Ä≤is the shared trainable
parameters and ùêπ‚Ä≤is the hidden dimension of the representation.
Then we build the contrastive objective of IDOL as follows:
Lùêºùê∑ùëÇùêø =1
ùëõ¬©¬≠
¬´ùëõùëî‚àëÔ∏Å
ùë†=1ùëìùúÉ(ùíõùë†)ùëìùúÉ(‚àíùíõùë†)‚ä§‚àíùëõ‚àëÔ∏Å
ùë†=ùëõùëî+1ùëìùúÉ(ùíõùë†)ùëìùúÉ(+ùíõùë†)‚ä§¬™¬Æ
¬¨,(6)
where we denote the index of nodes which have the negative sam-
ples as{1,2,...ùëõùëî}, and the index with respect to positive samples
as{ùëõùëî+1,ùëõùëî+2,...ùëõùëî+ùëõùëù}. The overall number of positive and
negative samples is identical to ùëõ, which is significantly smaller
than that of previous loss functions such as InfoNCE in Eq. 1. Note
that since there is no historical embedding to augment the initial
graphG0, we follow the work in [ 58,72] and shuffle the node order
inùëøto generate negative samples, where we have ùëõùëù=0,ùëõùëî=ùëõ.
We further provide a theoretical perspective to prove the capa-
bility of topology-monitorable graph contrastive learning in down-
stream tasks. Considering the classical node classification task, we
useùíÄ={ùë¶1,ùë¶2,...,ùë¶ùëõ}to denote the label set and we adopt the
Cross-Entropy (CE) loss [11] as:
Lùê∂ùê∏=‚àí1
ùëõùëõ‚àëÔ∏Å
ùë†=1logexp
ùëìùúÉ(ùíõùë†)ùë§‚ä§ùë¶ùë†
√çùê∂
ùëñ=1exp
ùëìùúÉ(ùíõùë†)ùë§‚ä§
ùëñ (7)
whereùë§ùëñis the trainable weights for ùëñ-th class in the linear classifier
utilized in downstream tasks and the weight matrix is denoted as
ùëæùê∂={ùë§1,ùë§2,...,ùë§ùê∂}given there are ùê∂classes.The performance in the downstream tasks and the contrastive
pretraining are demonstrated by the downstream task loss Lùê∂ùê∏
and the contrastive loss Lùêºùê∑ùëÇùêø respectively. Then, we have the
following upper bound of the downstream task loss Lùê∂ùê∏:
Lemma 4. The lossLùê∂ùê∏can be bounded by the loss Lùêºùê∑ùëÇùêø :
Lùê∂ùê∏‚â§Lùêºùê∑ùëÇùêø+logùê∂+2
3ùëé2(8)
holds with probability at least 1‚àíùëÇ(1/ùëõ). Here we assume ùëø‚àà
[‚àí1,1]after the feature normalization, and our graph encoder ùëìùúÉ(¬∑)
is Xavier [ 14] initialized using a uniform distribution ùíòùúÉ‚àºùëà(‚àíùëé,ùëé).
Regarding IDOL‚Äôs decoupled architecture during graph propa-
gation and pretraining, Lemma 4 hints a crucial point: the extent
of pretraining considerably impacts the performance of the down-
stream tasks. Given that the value of (logùê∂+2
3ùëé2)is relatively
minor compared to the loss value, reducing Lùêºùê∑ùëÇùêø can effectively
enhance downstream task performance. This observation supports
the fact that even a single epoch of pretraining with IDOL can yield
high accuracy in downstream tasks. Additionally, the improved
convergence of the contrastive loss further boosts the classification
accuracy. This observation is validated in our hyperparameter anal-
ysis, as shown in Sec. 4.6 where we varied the number of training
epochs in the encoder. Moreover, while this direct association be-
tween pretraining and downstream task performance is specific to
IDOL, it could also shed light on the rationale behind the need for
sufficient pretraining epochs in existing graph contrastive learning
methods [44, 55, 58] in improving downstream task performance.
3.6 Complexity Analysis
Compared with the contrastive methods using GCN-based encoder
such as [ 55,58,72,75], IDOL takes less time to pretrain in the dy-
namic scenarios. Taking a ùêæ-layer [ 30] GCN encoder as an example,
these methods need to take ùëÇ(4ùêæùëõùêπ2+4ùêæùëöùêπ)time4per training
step including forward propagation and backward propagation
of the network. On the initial graph, IDOL directly computes the
node embeddings by our K-hop algorithm, which only consumes
ùëÇ(2ùêæùëöùêπ)time (ùëÇ(ùêæùëöùêπ)for negative sampling). In addition to the
MLP encoder (optional as one layer), the complexity of IDOL on
the initial graph per training step is ùëÇ(4ùëõùêπ2+2ùêæùëöùêπ).
When incurring the update events, the GCN-encoder needs to
conduct the graph propagation and network training from scratch,
In contrast, the embedding update only consumes ùëÇ(ùêæùëöùêπ)time5,
since we reuse the historical embeddings as our positive and nega-
tive samples. For the complexity of distinguishing positive and neg-
ative samples, the Reverse Push consumeùëÇ(ùëù/ùëüùëèùëöùëéùë•)time givenùëù
update events [ 4]. Since we set an empirical value as ùëüùëèùëöùëéùë•=1/logùëõ,
the complexity for updating ùëùedges isùëÇ(4ùëõùêπ2+ùêæùëöùêπ+ùëùlogùëõ).
The summary of the above analysis can be viewed in Table 1.
4 EXPERIMENTS
In this section, we evaluate IDOL to show its effectiveness and effi-
ciency for contrastive learning in dynamic graphs. We conducted
4Here we assume for simplicity that the hidden size is ùêπ. This complexity includes
ùëÇ(2ùêæùëõùêπ2+2ùêæùëöùêπ)for positive encoder and ùëÇ(2ùêæùëõùêπ2+2ùêæùëöùêπ)for negative
encoder.
5In practice, the complexity of incremental update given ùëùupdate events is much
smaller thanùëÇ(ùêæùëöùêπ)whenùëùis not large.
4705Topology-monitorable Contrastive Learning on Dynamic Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 2: Statistics of the datasets. ùêπ,ùê∂, andùëÜstand for the
dimension of attributes, the number of classes, and the num-
ber of edge sequences.
Datasets n m F C S
Arxiv 169,343 1,157,799 128 40 16
Mag 736,389 10,792,672 128 349 16
Products 2,449,029 61,859,012 100 47 16
Patent 2,738,012 13,960,811 128 6 16
the experiments on a Linux machine with an Intel(R) Xeon(R) Gold
6238R CPU @ 2.20GHz with 160GB RAM and an NVIDIA RTX
A5000 with 24GB memory. We provide an open-source implementa-
tion of our model IDOL at https://github.com/ZulunZhu/dynamic-
contrastive-learning.git. More interesting experiments and analysis
can be found in our technique report [12].
4.1 Datasets
We conduct our experiments on four commonly used datasets: Arxiv,
Mag, Products [23], and Patent [20]. These datasets are chosen to
demonstrate the effectiveness of IDOL in various graph application
scenarios, such as citation networks, web graphs, and recommenda-
tion systems. The statistics of four datasets are introduced in Table
2. For Arxiv, Mag, Products, the partition of training, validation, and
testing set follows a chronological split in OGB [ 23]. For the dy-
namic dataset Patent, the training size is arranged as 70%including
10%for validation. All results shown are the averages from 10 runs.
4.2 Baseline Methods
We compare IDOL with the following graph contrastive learning
baselines: DGI[ 58],BGRL [ 55], GGD[ 72] and SUGRL [ 44]. We also
compare IDOL with CLDG[ 64], a state-of-the-art contrastive learn-
ing method focusing on dynamic graphs. Furthermore, two classical
GNN methods are included, GCN [ 30] and SGC [ 60], along with
two scalable GNN methods, SCARA [ 36] and Instant [ 73]. To ensure
reproducibility, we provide a detailed experiment setting in Appen-
dix A.3. Specifically, for Topology-monitorable Sampling, we set the
thresholdùëüùëèùëöùëéùë•=1/logùëõandùúÜ=1/ùëõby default in IDOL, where
ùëõis the node number of datasets. Then, for our two PPR-based
algorithms, we set the teleport probability ùõº=0.1onArxiv, Mag
Products, and ùõº=0.2onPatent.
4.3 Comparison within CTDG settings
In our experiments, we employ the node classification task in dy-
namic graphs to evaluate the embedding quality of our method. In
order to simulate the dynamic scenarios, we follow the experiment
setting of CTDG in [ 73] to segment the whole graph and transform
it into evolving states on these datasets. Formally, we divide the
whole graph into an initial graph and ùëÜpartitions of edge sequences
for demonstration, where the edge sequences are extracted evenly
from the complete graph. During the evaluation process, we pro-
gressively update the initial graph with ùëÜpartitions of removed
edges following the format of CTDG and evaluate all models after
each addition of these partitions. Notably, the additions of removed
edges are ordered according to the true timestamps.
Dynamic accuracy comparison. We first compare the accuracy
performance of IDOL with the competitive baselines. For a clear
(a)Arxiv
 (b)Mag
(c)Products
 (d)Patent
Figure 2: Micro-F1 scores in each snapshot of four datasets.
demonstration, we evaluate the Macro-F1 and Micro-F1 scores and
report the average values over all moments of evaluation, which
is summarized in Table 3. It can be seen that IDOL outperforms
previous contrastive learning methods (e.g., DGI, BGRL, SUGRL,
GGD, and CLDG) with a non-trivial gap. Compared with the worst
model DGI and the best CLDG, our method on average improves by
7.33%and4.07%onProducts dataset, respectively. We also note that
IDOL surpasses traditional supervised GNN methods such as GCN,
SGC, SCARA, and Instant, all of which are trained through an end-
to-end supervised process without incorporating the contrastive
learning mechanism. This finding reinforces the notion that the
integration of the contrastive learning paradigm can significantly
enhance the quality of embeddings.
Moreover, we present the detailed performance of multiple con-
trastive learning methods when adding each edge sequence, which
is shown in Figure 2. We adopt the Micro-F1 scores to measure this
comparison and we have the following key observation: The predic-
tion performance of IDOL outperforms the state-of-the-art methods
across four datasets. It is worth noting that, IDOL achieves compa-
rable or even better results on each snapshot of all datasets, where
IDOL even outperforms the advanced methods DGI and GGD by
over 16.5%on the initial graph of Products datasets. Notice that
forProducts andPatent datasets, IDOL substantially outperforms
the baselines by a large margin, including the competitive method
CLDG. The notable performance advantage of IDOL can be credited
to its use of an efficient full-graph processing approach for training,
in contrast to other methods that require batch processing and
neighbor sampling in large graphs. This approach, which fully inte-
grates topology information, significantly enhances performance.
Dynamic efficiency comparison. To demonstrate the effi-
ciency of IDOL, we investigate the time consumption of each con-
trastive learning method on each dataset. Table 4 reports the av-
erage pretraining time (per epoch) over all snapshots. Similarly,
we also provide the detailed results on all datasets for each evalua-
tion in Figure 3. For the initial graph, we observe that IDOL shares
a close time consumption as baseline methods, which includes
the time of embedding calculation and network training. Never-
theless, in the later addition of edge sequences, IDOL drastically
4706KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
Table 3: The prediction accuracy (%) on four datasets. "OOM" stands for out of memory on a GPU with 24GB memory. The best
results are underlined and bold.
MethodArxiv Mag Products Patent
Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1
GCN 68.42¬±0.32 47.24¬±0.29 33.71¬±0.33 13.72¬±0.27 74.43¬±0.17 35.62¬±0.21 77.60¬±0.55 78.01¬±0.52
SGC 68.54¬±0.37 47.47¬±0.27 32.66¬±0.25 14.38¬±0.26 74.54¬±0.26 35.82¬±0.17 77.81¬±0.49 78.26¬±0.36
SCARA 68.89¬±0.26 48.19¬±0.20 33.36¬±0.29 14.82¬±0.17 78.13¬±0.25 38.42¬±0.19 81.51¬±0.73 81.91¬±0.61
Instant 68.14¬±0.12 48.01¬±0.10 33.54¬±0.21 14.68¬±0.18 77.86¬±0.11 38.03¬±0.16 81.24¬±0.65 81.48¬±0.66
DGI 66.69¬±0.54 43.28¬±0.31 32.78¬±0.41 12.94¬±0.29 72.11¬±0.29 34.20¬±0.20 75.45¬±0.94 76.01¬±0.31
BGRL 66.31¬±0.47 43.42¬±0.24 31.59¬±0.87 12.58¬±0.33 OOM OOM OOM OOM
SUGRL 65.81¬±0.94 42.53¬±0.88 30.71¬±0.75 12.14¬±0.31 OOM OOM OOM OOM
GGD 67.45¬±0.36 44.10¬±0.25 32.89¬±0.29 13.43¬±0.16 72.35¬±0.31 35.08¬±0.13 75.36¬±0.21 75.93¬±0.44
CLDG 68.05¬±0.87 46.15¬±0.56 33.73¬±0.75 14.70¬±0.22 75.37¬±0.50 37.47¬±0.81 77.18¬±0.87 77.64¬±0.25
IDOL 69.80¬±0.21 49.44¬±0.12 35.34¬±0.20 16.51¬±0.15 79.44¬±0.12 40.13¬±0.12 84.19¬±0.87 84.11¬±0.75
Table 4: The pretraining time (s) per epoch on four datasets.
Method Arxiv Mag Products Patent
DGI 5.15¬±0.75 18.42¬±1.67 113.06¬±5.52 101.46¬±2.65
BGRL 6.47¬±0.50 54.93¬±5.29 OOM OOM
SUGRL 2.16¬±0.04 10.14¬±1.19 OOM OOM
GGD 4.94¬±0.44 17.34¬±1.56 105.49¬±3.23 104.96¬±2.39
CLDG 2.00¬±0.37 12.70¬±2.19 77.77¬±2.74 65.68¬±3.14
IDOL 1.76¬±0.18 9.02¬±1.02 20.67¬±2.15 36.57¬±1.65
(a)Arxiv
 (b)Mag
(c)Products
 (d)Patent
Figure 3: Pretraining time comparison.
achieves orders-of-magnitude acceleration, and the gap between
IDOL and baseline methods becomes even more pronounced as
the graph scale increases. Especially, in the full graph of Products,
GGD and DGI require over 120 seconds each epoch for pretrain-
ing, while IDOL completes this process in just 16.2 seconds. Even
faced with suboptimal baseline CLDG, IDOL can still achieve up
to(77.77‚àí20.67)/77.77=73.42%reduction of pertaining time on
average. It‚Äôs important to highlight that both IDOL and GGD can
achieve high prediction accuracy reported in Figure 2 with only one
epoch, owing to their rapid convergence. Conversely, DGI, BGRL,
SUGRL, and CLDG require over 50 epochs to reach a competitive
performance, showing a substantial disparity in training efficiency.Table 5: The average prediction accuracy (%) on Mooc and
Reddit datasets for 10 runs. The average time consumption
(ùë†) of pretraining time is enclosed in the bracket. (/)means
the corresponding algorithm requires no pretraining stage
on this task. The best results are underlined and bold.
MethodNode Classification Link Prediction
Mooc Reddit Mooc Reddit
JODIE [33] 61.53(/) 53.64(/) 94.53(/) 95.78(/)
TGN [51] 69.03(65.65ùë†)55.53(128.86ùë†)98.24(/) 98.76(/)
DDGCL [56] 57.11(59.65ùë†)52.79(101.89ùë†)98.11(89.65ùë†)98.17(189.11ùë†)
IDOL 67.34(1.64s)55.21(6.64s)99.07(3.98s)98.97(10.93s)
The remarkable performance of IDOL can be attributed to two
primary factors. First, IDOL efficiently saves pretraining time by
incorporating edge updates and implementing incremental embed-
ding refreshment. Additionally, its straightforward MLP structure
simplifies the networks and further enhances training efficiency.
Secondly, our topology-monitorable sampling method, designed to
capture topological changes from an evolving perspective, directly
contributes to potential improvements in the quality of positive
and negative pairs. Collectively, these two factors underpin IDOL‚Äôs
exceptional efficiency and effectiveness, respectively.
4.4 Comparison within DTDG Settings
In the above experiments, we adopt the CTDG setting to evaluate
the performance of IDOL on large-scale datasets. It‚Äôs worth noting,
though, that some DTDG-based algorithms, which don‚Äôt scale well
in these large-scale datasets, can still perform competitively on
smaller datasets by leveraging the fine-grained time information.
To further establish IDOL‚Äôs versatility, we conducted a compara-
tive analysis of IDOL against a selection of prominent algorithms
on two smaller datasets, Mooc [ 1] and Reddit [ 2]. In this experi-
ment, we follow the DTDG setting outlined in existing literature
[32,50,56], which focuses on predictions for the fully-formed final
graph. The results of node classification andlink prediction tasks are
summarized in Table 5. Based on this result, IDOL demonstrates
broad applicability across various graph types and downstream
tasks, achieving comparable prediction accuracy while necessitat-
ing shorter pretraining durations. This underscores IDOL‚Äôs capacity
for generalization and efficiency in diverse graph environments.
4707Topology-monitorable Contrastive Learning on Dynamic Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 6: Ablation studies of K-hop Push on the Patent dataset.
F(ùë•)denotes Forward Push with ùëüùëöùëéùë•=ùë•, andK(ùë•),S(ùë•)
denotes K-hop Push and propagation using SGC with ùêæ=ùë•
respectively. w/o. means without.
Variants w/o.F(10‚àí5)F(10‚àí7)S(2)S(4)K(2)K(4)
Time(s) 0 0.10 37.06 15.14 57.54 7.43 36.57
Accuracy(%) 19.65 29.24 81.24 73.95 77.81 83.85 84.19
Table 7: Ablation studies on sampling strategies. ùúè+=ùëñand
ùúè‚àí=ùëñmean selecting previous ùëñ-th snapshot as the positive
and negative samples respectively. Corr. means the random
corruption. w/o. means without.
Variants w/o. Corr.ùúè+=1ùúè+=5ùúè‚àí=5ùúè‚àí=10 IDOL
Arxiv 69.24 69.36 70.49 69.94 70.26 70.38 72.41
Mag 32.22 33.37 35.64 33.44 34.56 34.74 36.38
Product 73.99 74.87 76.89 75.67 76.26 76.64 78.11
4.5 Ablation Studies
In this section, we conduct ablation studies on the variants of IDOL
to verify the effectiveness of two key modules comprehensively:
K-hop Push andTopology-monitorable Sampling.
K-hop Push. OnPatent datasets, we replace K-hop Push module
with the conventional Forward Push algorithm [ 8,73] which utilizes
an empirical threshold ùëüùëöùëéùë• to early stop the iteration of graph
propagation. Moreover, we also replace K-hop Push with the propa-
gation method in SGC [ 60] for a comparison. As shown in Table 6,
Forward Push andK-hop Push require more propagation time with
a smallerùëüùëöùëéùë• and a larger ùêæ, respectively. Nevertheless, K-hop
Push consistently achieves higher accuracy within a comparable
time frame. Furthermore, the notable variability in performance
with Forward Push implies the need for careful selection of the ùëüùëöùëéùë•
threshold, while K-hop Push proves to be more stable across various
settings of hop number ùêæ.
Topology-monitorable Sampling. We compare our sampling
strategy with the time-based strategy in [ 56], which adopts different
time windows to sample the positive and negative pairs. To generate
different variants, we replace the positive or negative samples in
Topology-monitorable Sampling by directly sampling from all nodes
in the previous ùúè+-th orùúè‚àí-th snapshot, respectively. Additionally,
we also provide a sampling strategy named random corruption for
comparison, which disturbs the graph topology for augmentation
and is widely used in previous works [ 55,58,72]. As indicated
in Table 7, we evaluate different sampling strategies on the final
snapshot of three datasets. It is evident that, compared with our
method, the utilization of time-based strategies consistently results
in a decline in prediction accuracy. Additionally, this decline in
accuracy is tied to the incorrect categorization of negative and
positive samples. For example, on the Arxiv dataset, accuracy falls
from 70.49%to69.94%whenùúè+changes from 1 to 5. This is because,
over a longer time window, the graph‚Äôs structure changes more
significantly, causing more samples to be mistakenly identified
as positive. This supports the importance of sampling positive
and negative pairs based on topological information, as it proves
beneficial for enhancing correct sampling.
/uni00000013/uni00000011/uni00000018/uni00000012/uni00000051 /uni00000014/uni00000012/uni00000051 /uni00000015/uni00000012/uni00000051 /uni00000018/uni00000012/uni00000051 /uni00000014/uni00000013/uni00000012/uni00000051
/uni00000014/uni00000013/uni00000012/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000051
/uni00000018/uni00000012/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000051
/uni00000015/uni00000012/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000051
/uni00000014/uni00000012/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000051
/uni00000013/uni00000011/uni00000018/uni00000012/uni0000004f/uni00000052/uni0000004a/uni00000003/uni00000051rb
max/uni00000019/uni0000001c/uni00000011/uni00000013/uni0000001c /uni00000019/uni0000001b/uni00000011/uni00000017/uni0000001a /uni00000019/uni0000001c/uni00000011/uni00000017/uni00000013 /uni00000019/uni0000001c/uni00000011/uni0000001a/uni00000013 /uni00000019/uni0000001b/uni00000011/uni0000001b/uni00000013
/uni00000019/uni0000001b/uni00000011/uni00000018/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000016/uni00000017 /uni00000019/uni0000001c/uni00000011/uni00000015/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000019/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000014/uni00000019
/uni00000019/uni0000001c/uni00000011/uni00000013/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000014/uni0000001a /uni00000019/uni0000001c/uni00000011/uni00000015/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000014/uni00000016 /uni00000019/uni0000001c/uni00000011/uni00000016/uni00000018
/uni00000019/uni0000001c/uni00000011/uni00000014/uni00000019 /uni00000019/uni0000001c/uni00000011/uni0000001b/uni00000013 /uni00000019/uni0000001c/uni00000011/uni00000015/uni00000019 /uni00000019/uni0000001c/uni00000011/uni00000019/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000013/uni00000017
/uni00000019/uni0000001c/uni00000011/uni00000014/uni00000014 /uni00000019/uni0000001c/uni00000011/uni00000018/uni0000001c /uni00000019/uni0000001c/uni00000011/uni00000015/uni00000014 /uni00000019/uni0000001c/uni00000011/uni00000016/uni0000001a /uni00000019/uni0000001c/uni00000011/uni00000016/uni00000019
/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014(a)ùëüùëè
ùëöùëéùë• andùúÜ
/uni00000014 /uni00000018 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000015/uni00000013/uni00000017/uni0000001b
/uni00000014/uni00000013/uni00000015/uni00000017
/uni00000018/uni00000014/uni00000015
/uni00000015/uni00000018/uni00000019
/uni00000014/uni00000015/uni0000001b/uni0000004b/uni0000004c/uni00000047/uni00000047/uni00000048/uni00000051/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000019/uni0000001c/uni00000011/uni0000001c/uni0000001b /uni0000001a/uni00000013/uni00000011/uni00000014/uni00000016 /uni00000019/uni0000001c/uni00000011/uni00000018/uni00000018 /uni00000019/uni0000001c/uni00000011/uni00000017/uni00000013 /uni00000019/uni0000001c/uni00000011/uni00000018/uni00000019
/uni00000019/uni0000001c/uni00000011/uni0000001b/uni00000013 /uni0000001a/uni00000013/uni00000011/uni00000015/uni00000015 /uni0000001a/uni00000013/uni00000011/uni00000014/uni0000001c /uni00000019/uni0000001c/uni00000011/uni0000001c/uni00000015 /uni0000001a/uni00000013/uni00000011/uni00000013/uni00000016
/uni00000019/uni0000001c/uni00000011/uni0000001a/uni0000001a /uni0000001a/uni00000013/uni00000011/uni00000014/uni00000014 /uni00000019/uni0000001c/uni00000011/uni0000001c/uni0000001a /uni00000019/uni0000001c/uni00000011/uni0000001b/uni0000001a /uni0000001a/uni00000013/uni00000011/uni00000015/uni00000017
/uni00000019/uni0000001c/uni00000011/uni00000013/uni00000014 /uni00000019/uni0000001c/uni00000011/uni00000014/uni00000019 /uni00000019/uni0000001c/uni00000011/uni00000017/uni00000017 /uni00000019/uni0000001c/uni00000011/uni00000015/uni0000001b /uni00000019/uni0000001c/uni00000011/uni00000017/uni0000001a
/uni00000019/uni00000019/uni00000011/uni0000001a/uni0000001a /uni00000019/uni00000019/uni00000011/uni0000001c/uni00000014 /uni00000019/uni00000019/uni00000011/uni00000017/uni0000001b /uni00000019/uni0000001a/uni00000011/uni00000013/uni00000016 /uni00000019/uni0000001a/uni00000011/uni00000016/uni00000014
/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014 (b) Hidden size and pretraining epoch
Figure 4: Hyperparameter analysis.
4.6 Hyperparameter Analysis
We further analyze the hyperparameter sensitivity of IDOL. The
experimental results on Arxiv dataset are reported and we have
similar observations on the other datasets.
Threshold ùëüùëèùëöùëéùë•andùúÜinDynamic Sampling. To achieve
topology-monitorable sampling, we employ ùëüùëèùëöùëéùë• as the early-stopping
threshold in the Reverse Push algorithm and further utilize an
empirical gap value ùúÜto distinguish the positive samples from
historical embeddings. As shown in Figure 4(a), we present the
average accuracy on Arxiv by varying the values of ùëüùëèùëöùëéùë• andùúÜas
{0.5/logùëõ,1/logùëõ,2/logùëõ,5/logùëõ,10/logùëõ}and{0.5/ùëõ,1/ùëõ,2/ùëõ,
5/ùëõ,10/ùëõ}, respectively. It is observed that various settings typically
do not significantly affect IDOL, as IDOL consistently delivers its
best performance using our default configuration.
Hidden size and pretraining epoch. We investigate the IDOL
variants with different hidden sizes and pretraining epochs, as illus-
trated in Figure 4(b). For a fair comparison, we start this study based
on the default setting of the hidden size and epoch number (e.g., 1024
and 1) in the aforementioned experiments. When the hidden size
exceeds 512, the results tend to converge closely. However, reducing
the hidden size to 128 leads to a notable decline in classification
performance. The reason might be that low-dimensional hidden
vectors cannot effectively represent large-scale graph information.
Furthermore, we can naturally produce a better performance by
utilizing more epochs. Since a longer pretraining period will further
decreaseLùêºùê∑ùëÇùêø , which enhances the prediction accuracy of the
downstream task.
5 CONCLUSION
In this paper, we propose IDOL, a first-ever topology-monitorable
contrastive learning framework focusing on dynamic graphs. Given
the graph updates, IDOL employs PPR-based techniques to incre-
mentally update node embeddings and uses a strategy that monitors
topology changes to select positive and negative pairs. Besides, we
employ a simplified training paradigm and provide a performance
guarantee by bridging the contrastive loss and the downstream
task loss. In terms of time efficiency and prediction accuracy, we
conduct extensive experiments on various graph datasets and verify
that IDOL is superior to state-of-the-art methods.
ACKNOWLEDGMENTS
This research is supported by the Ministry of Education, Singa-
pore, under its AcRF Tier-2 Grant (T2EP20122-0003). Any opinions,
findings and conclusions or recommendations expressed in this
material are those of the author(s) and do not reflect the views of
the Ministry of Education, Singapore.
4708KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
REFERENCES
[1] Kdd cup 2015. https://biendata.com/competition/kddcup2015/data/.
[2] Reddit data dump. http://files.pushshift.io/reddit/.
[3]Mohammad Ali Alomrani, Mahdi Biparva, Yingxue Zhang, and Mark Coates. 2022.
DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision.
arXiv preprint arXiv:2210.16906 (2022).
[4]Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mir-
rokni, and Shang-Hua Teng. 2007. Local computation of pagerank contributions.
InAlgorithms and Models for the Web-Graph: 5th International Workshop, WAW
2007, San Diego, CA, USA, December 11-12, 2007. Proceedings 5. Springer, 150‚Äì165.
[5]Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning
using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS‚Äô06). IEEE, 475‚Äì486.
[6]Yuanchen Bei, Hao Xu, Sheng Zhou, Huixuan Chi, Mengdi Zhang, Zhao Li, and
Jiajun Bu. 2023. CPDG: A Contrastive Pre-Training Method for Dynamic Graph
Neural Networks. arXiv preprint arXiv:2307.02813 (2023).
[7]Deyu Bo, BinBin Hu, Xiao Wang, Zhiqiang Zhang, Chuan Shi, and Jun Zhou.
2022. Regularizing graph neural networks via consistency-diversity graph aug-
mentations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.
3913‚Äì3921.
[8]Aleksandar Bojchevski, Johannes Gasteiger, Bryan Perozzi, Amol Kapoor, Martin
Blais, Benedek R√≥zemberczki, Michal Lukasik, and Stephan G√ºnnemann. 2020.
Scaling graph neural networks with approximate pagerank. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 2464‚Äì2473.
[9]Sergey Brin. 1998. The PageRank citation ranking: bringing order to the web.
Proceedings of ASIS, 1998 98 (1998), 161‚Äì172.
[10] Liang Chen, Jiaying Peng, Yang Liu, Jintang Li, Fenfang Xie, and Zibin Zheng.
2020. Phishing scams detection in ethereum transaction network. ACM Transac-
tions on Internet Technology (TOIT) 21, 1 (2020), 1‚Äì16.
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597‚Äì1607.
[12] Zulun Zhu et al. 2024. https://sites.google.com/view/idol-tech/.
[13] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang
Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural networks for
semi-supervised learning on graphs. Advances in neural information processing
systems 33 (2020), 22092‚Äì22103.
[14] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics. JMLR Workshop and Conference
Proceedings, 249‚Äì256.
[15] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural information processing
systems 33 (2020), 21271‚Äì21284.
[16] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.
Attention based spatial-temporal graph convolutional networks for traffic flow
forecasting. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
922‚Äì929.
[17] Xingzhi Guo, Baojian Zhou, and Steven Skiena. 2022. Subset node anomaly
tracking over large dynamic graphs. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 475‚Äì485.
[18] Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza
Zadeh. 2013. Wtf: The who to follow service at twitter. In Proceedings of the 22nd
international conference on World Wide Web. 505‚Äì514.
[19] Michael Gutmann and Aapo Hyv√§rinen. 2010. Noise-contrastive estimation: A
new estimation principle for unnormalized statistical models. In Proceedings of
the thirteenth international conference on artificial intelligence and statistics. JMLR
Workshop and Conference Proceedings, 297‚Äì304.
[20] Bronwyn H Hall, Adam B Jaffe, and Manuel Trajtenberg. 2001. The NBER patent
citation data file: Lessons, insights and methodological tools.
[21] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639‚Äì648.
[22] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
Bachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen-
tations by mutual information estimation and maximization. arXiv preprint
arXiv:1808.06670 (2018).
[23] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118‚Äì22133.
[24] Yue Hu, Yuhang Zhang, Yanbing Wang, and Daniel Work. 2023. Detecting Socially
Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks.InProceedings of the ACM Web Conference 2023. 3086‚Äì3097.
[25] Ziniu Hu, Changjun Fan, Ting Chen, Kai-Wei Chang, and Yizhou Sun. 2019. Pre-
training graph neural networks for generic structural feature extraction. arXiv
preprint arXiv:1905.13728 (2019).
[26] Jiawei Jiang, Pin Xiao, Lele Yu, Xiaosen Li, Jiefeng Cheng, Xupeng Miao, Zhipeng
Zhang, and Bin Cui. 2020. PSGraph: How Tencent trains extremely large-scale
graphs with Spark?. In 2020 IEEE 36th International Conference on Data Engineering
(ICDE). IEEE, 1549‚Äì1557.
[27] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang
Tang. 2020. Self-supervised learning on graphs: Deep insights and new direction.
arXiv preprint arXiv:2006.10141 (2020).
[28] Xin Ju, Xiaofeng Zhang, and William K Cheung. 2019. Generating synthetic
graphs for large sensitive and correlated social networks. In 2019 IEEE 35th
international conference on data engineering workshops (ICDEW). IEEE, 286‚Äì293.
[29] Dongkwan Kim and Alice Oh. 2021. How to Find Your Friendly Neighborhood:
Graph Attention Design with Self-Supervision. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net. https://openreview.net/forum?id=Wi5KUNlqWty
[30] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In ICLR (Poster). OpenReview.net.
[31] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G√ºnnemann. 2019. Pre-
dict then Propagate: Graph Neural Networks meet Personalized PageRank. In
ICLR (Poster). OpenReview.net.
[32] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In Proceedings of the 25th
ACM SIGKDD international conference on knowledge discovery & data mining.
1269‚Äì1278.
[33] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-
bedding Trajectory in Temporal Interaction Networks. In KDD. ACM, 1269‚Äì1278.
[34] Jong-whi Lee and Jinhong Jung. 2023. Time-aware random walk diffusion to
improve dynamic graph learning. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 37. 8473‚Äì8481.
[35] Jintang Li, Zhouxin Yu, Zulun Zhu, Liang Chen, Qi Yu, Zibin Zheng, Sheng Tian,
Ruofan Wu, and Changhua Meng. 2023. Scaling Up Dynamic Graph Representa-
tion Learning via Spiking Neural Networks. In AAAI. AAAI Press, 8588‚Äì8596.
[36] Ningyi Liao, Dingheng Mo, Siqiang Luo, Xiang Li, and Pengcheng Yin. 2022.
SCARA: Scalable Graph Neural Networks with Feature-Oriented Optimization.
Proc. VLDB Endow. 15, 11 (2022), 3240‚Äì3248.
[37] David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C Ma,
Zhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related pins at pinterest:
The evolution of a real-world recommender system. In Proceedings of the 26th
international conference on world wide web companion. 583‚Äì592.
[38] Haoyu Liu and Siqiang Luo. 2024. BIRD: Efficient Approximation of Bidirectional
Hidden Personalized PageRank. PVLDB 17, 9 (2024), 2255‚Äì2268. https://doi.org/
10.14778/3665844.3665855
[39] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip.
2022. Graph self-supervised learning: A survey. IEEE Transactions on Knowledge
and Data Engineering 35, 6 (2022), 5879‚Äì5900.
[40] Yuanfu Lu, Xiao Wang, Chuan Shi, Philip S. Yu, and Yanfang Ye. 2019. Temporal
Network Embedding with Micro- and Macro-dynamics. In CIKM. ACM, 469‚Äì478.
[41] Siqiang Luo, Xiaokui Xiao, Wenqing Lin, and Ben Kao. 2019. Baton: Batch one-
hop personalized pageranks with efficiency and accuracy. IEEE Transactions on
Knowledge and Data Engineering 32, 10 (2019), 1897‚Äì1908.
[42] Dingheng Mo and Siqiang Luo. 2021. Agenda: Robust Personalized PageRanks in
Evolving Graphs. In CIKM. ACM, 1315‚Äì1324.
[43] Dingheng Mo and Siqiang Luo. 2023. Single-Source Personalized PageRanks
With Workload Robustness. IEEE Trans. Knowl. Data Eng. 35, 6 (2023), 6320‚Äì6334.
[44] Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. 2022. Simple un-
supervised graph representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 7797‚Äì7805.
[45] Kenta Oono and Taiji Suzuki. 2020. Graph Neural Networks Exponentially Lose
Expressive Power for Node Classification. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net. https://openreview.net/forum?id=S1ldO2EFPr
[46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[47] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,
Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:
Evolving graph convolutional networks for dynamic graphs. In Proceedings of
the AAAI conference on artificial intelligence, Vol. 34. 5363‚Äì5370.
[48] Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, and Jin Young
Choi. 2019. Symmetric graph convolutional autoencoder for unsupervised graph
representation learning. In Proceedings of the IEEE/CVF international conference
on computer vision. 6519‚Äì6528.
[49] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,
Kuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph
neural network pre-training. In Proceedings of the 26th ACM SIGKDD international
conference on knowledge discovery & data mining. 1150‚Äì1160.
4709Topology-monitorable Contrastive Learning on Dynamic Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[50] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning
on Dynamic Graphs. In ICML 2020 Workshop on Graph Representation Learning.
[51] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning
on Dynamic Graphs. In ICML 2020 Workshop on Graph Representation Learning.
[52] Anshul Shah, Suvrit Sra, Rama Chellappa, and Anoop Cherian. 2022. Max-
margin contrastive learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 8220‚Äì8230.
[53] Uriel Singer, Ido Guy, and Kira Radinsky. 2019. Node Embedding over Temporal
Graphs. In IJCAI. ijcai.org, 4605‚Äì4612.
[54] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou,
Eva L Dyer, Remi Munos, Petar Veliƒçkoviƒá, and Michal Valko. 2021. Large-
scale representation learning on graphs via bootstrapping. arXiv preprint
arXiv:2102.06514 (2021).
[55] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, R√©mi Munos,
Petar Veliƒçkoviƒá, and Michal Valko. 2021. Bootstrapped representation learning
on graphs. In ICLR 2021 Workshop on Geometrical and Topological Representation
Learning.
[56] Sheng Tian, Ruofan Wu, Leilei Shi, Liang Zhu, and Tao Xiong. 2021. Self-
supervised representation learning on dynamic graphs. In Proceedings of the
30th ACM International Conference on Information & Knowledge Management.
1814‚Äì1823.
[57] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In ICLR (Poster).
OpenReview.net.
[58] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Li√≤, Yoshua Bengio,
and R. Devon Hjelm. 2019. Deep Graph Infomax. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net. https://openreview.net/forum?id=rklz9iAcKQ
[59] Sibo Wang, Renchi Yang, Xiaokui Xiao, Zhewei Wei, and Yin Yang. 2017. FORA:
simple and effective approximate single-source personalized pagerank. In Proceed-
ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. 505‚Äì514.
[60] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861‚Äì6871.
[61] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the dots: Multivariate time series forecasting with graph
neural networks. In Proceedings of the 26th ACM SIGKDD international conference
on knowledge discovery & data mining. 753‚Äì763.
[62] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. 2022.
Self-supervised learning of graph neural networks: A unified review. IEEE trans-
actions on pattern analysis and machine intelligence 45, 2 (2022), 2412‚Äì2429.
[63] Da Xu, Chuanwei Ruan, Evren K√∂rpeoglu, Sushant Kumar, and Kannan Achan.
2020. Inductive representation learning on temporal graphs. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net. https://openreview.net/forum?id=rJeW1yHYwH
[64] Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, and Qinghua Zheng. 2023.
CLDG: Contrastive Learning on Dynamic Graphs. In 2023 IEEE 39th InternationalConference on Data Engineering (ICDE). IEEE, 696‚Äì707.
[65] Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King.
2021. Discrete-time temporal network embedding via implicit hierarchical learn-
ing in hyperbolic space. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1975‚Äì1985.
[66] Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: Graph Learning
Framework for Dynamic Graphs. In KDD. ACM, 2358‚Äì2366.
[67] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems 33 (2020), 5812‚Äì5823.
[68] Wenchao Yu, Wei Cheng, Charu C. Aggarwal, Kai Zhang, Haifeng Chen, and
Wei Wang. 2018. NetWalk: A Flexible Deep Embedding Approach for Anomaly
Detection in Dynamic Networks. In KDD. ACM, 2672‚Äì2681.
[69] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. 2021. From
canonical correlation analysis to self-supervised graph neural networks. Advances
in Neural Information Processing Systems 34 (2021), 76‚Äì89.
[70] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu
Tao, Zhi Yang, and Bin Cui. 2021. Node dependent local smoothing for scalable
graph learning. Advances in Neural Information Processing Systems 34 (2021),
20321‚Äì20332.
[71] Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan G√ºnnemann,
Neil Shah, and Meng Jiang. 2022. Graph data augmentation for graph machine
learning: A survey. arXiv preprint arXiv:2202.08871 (2022).
[72] Yizhen Zheng, Shirui Pan, Vincent Lee, Yu Zheng, and Philip S Yu. 2022. Rethink-
ing and scaling up graph contrastive learning: An extremely efficient approach
with group discrimination. Advances in Neural Information Processing Systems 35
(2022), 10809‚Äì10820.
[73] Yanping Zheng, Hanzhi Wang, Zhewei Wei, Jiajun Liu, and Sibo Wang. 2022.
Instant graph neural networks for dynamic graphs. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2605‚Äì2615.
[74] Changtao Zhong, Mostafa Salehi, Sunil Shah, Marius Cobzarenco, Nishanth
Sastry, and Meeyoung Cha. 2014. Social bootstrapping: how pinterest and last.
fm social communities benefit by borrowing links from facebook. In Proceedings
of the 23rd international conference on World wide web. 305‚Äì314.
[75] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131
(2020).
[76] Zulun Zhu, Siqiang Luo, Wenqing Lin, Sibo Wang, Dingheng Mo, and Chunbo Li.
2024. Personalized PageRanks over Dynamic Graphs ‚Äì The Case for Optimizing
Quality of Service. In ICDE. IEEE.
[77] Zulun Zhu, Jiaying Peng, Jintang Li, Liang Chen, Qi Yu, and Siqiang Luo.
2022. Spiking Graph Convolutional Networks. In Proceedings of the Thirty-
First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vi-
enna, Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 2434‚Äì2440. https:
//doi.org/10.24963/ijcai.2022/338
[78] Zulun Zhu, Sibo Wang, Siqiang Luo, Dingheng Mo, Wenqing Lin, and Chunbo Li.
[n. d.]. Personalized PageRanks over Dynamic Graphs‚ÄìThe Case for Optimizing
Quality of Service.
[79] Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, and Junjie Wu. 2018.
Embedding Temporal Network via Neighborhood Formation. In KDD. ACM,
2857‚Äì2866.
4710KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, and Siqiang Luo
A APPENDIX
A.1 Summary of Notations
The main notations used in this paper and their descriptions are
summarized in Table 8.
Table 8: Frequently used notations in this paper.
Notations Descriptions
Gùëñ=(Vùëñ,Eùëñ) Directed graph after ùëñedge updates
ùëõ,ùëö Numbers of nodes and edges in the current graph
ùêπ Dimension of the node attribute
ùêæ The layer of graph propagation
ùëø The node attribute matrix and ùëø‚ààRùëõ√óùêπ
ùíôùëñ The attribute vector of the ùëñ-th dimension and
ùíôùëñ‚ààRùëõ√ó1
Nùëúùë¢ùë°(ùë£),Nùëñùëõ(ùë£)The out-neighbors and in-neighbors of ùë£
ùíÄ The label set, where the element ùë¶ùëñ‚ààR
ùëíùëñ={ùë¢ùëñ,ùë£ùëñ} Theùëñ-th update event related to node ùë¢ùëñandùë£ùëñ
ùúã(ùê∫ùëñ,ùë†,ùë°) PPR of node ùë°from the source node ùë†in graph
ùê∫ùëñ
ùíìùëñ Residue vector of the ùëñ-th dimension and ùíìùëñ‚àà
Rùëõ√ó1
ùíâùëñ Reserve vector of the ùëñ-th dimension and ùíâùëñ‚àà
Rùëõ√ó1
ùíõùë† The node embedding vector of node ùë†andùíõùë†‚àà
R1√óùêπ
‚àíùíõùë†,+ùíõùë† The negative and positive sample vectors of node
ùë†and‚àíùíõùë†,+ùíõùë†‚ààR1√óùêπ
ùõº Teleport probability of random walks
ùëüùëèùëöùëéùë• Threshold in Reverse Push
A.2 More Related Work
A.2.1 Dynamic Graph Representation Learning. There exist two
major types of approaches, including the methods focusing on
DTDG and CTDG.
Discrete-Time Dynamic Graph. Representation learning over
dynamic graphs primarily focused on discrete-time dynamic graphs
(DTDGs), which are represented by a series of graph snapshots cap-
tured at discrete time points. Literature on DTDGs has been exten-
sively studied due to ease of implementation. Many architectures
for DTDGs are based on combining static GNNs with sequence
models, such as recurrent neural networks (RNNs), which can be
applied recurrently to the network parameters [ 47], hierarchical
node states [ 66], or to a set of embeddings encoded from each graph
snapshot [ 35]. Another line of research involves performing tem-
poral random walks on graph snapshots to obtain dynamic node
representations [53, 68].
Continuous-Time Dynamic Graph. Although DTDG meth-
ods have proven a powerful tool in learning the low-dimensional
node representations for dynamic graphs, one limitation is their
difficulty in capturing fine-grained temporal dynamics and contin-
uous changes in the evolving structure. Recently, continuous-time
dynamic graphs (CTDGs) have been widely studied, which consider
graphs where the temporal evolution is modeled as a continuousprocess. Early works employ self-attention mechanisms to learn a
dynamic node representation by attending over its neighbors and
historical representations [ 56,63]. Sequence models are also promis-
ing methods for CTDGs. For example, JODIE [ 33] and TGN [ 51] use
RNNs to propagate messages across interactions to update node rep-
resentations. In addition, temporal point processes (TPP) are applied
for modeling continuous-time event sequences [ 40,57,79]. Unlike
prior methods that recompute node embeddings from scratch for
each prediction in dynamic scenarios, our approach utilizes incre-
mental computation, optimizing both computation efficiency and
memory usage.
A.2.2 Graph Contrastive Learning. Graph contrastive learning lever-
ages the concept of positive and negative samples combined with a
discrimination rule to learn the representation of nodes, capturing
statistical dependencies and enhancing the meaningful patterns
from graphs. DGI [ 58] is the first approach to maximize the mutual
information between the node embeddings and graph embeddings,
which enables the graph encoder to learn both local and global
information. BYOL [ 15] and BGRL[ 55] attempt to get rid of the
high overhead for sampling negative pairs and achieve promising
performance when only utilizing positive pairs. GGD[ 72] simplifies
the loss computation of DGI and conducts the pretraining only with
a few epochs. Lastly, DDGCL[ 56] aims to capture the dynamics in
the graphs and accommodate the temporal information during the
process of positive sampling.
Due to such label scarcity, graph contrastive learning can also
be used to improve the model quality by enhancing the consis-
tency of different augmentation views. GRAND [ 13] generates dif-
ferent augmentation views of the graph by dropping nodes and
masking features, and then the mutual information of the scarce
labeled nodes based on different views is minimized to enhance the
prediction performance. NASA [ 7] randomly replaces the 1-hop
neighbors with 2-hop neighbors for the labels nodes and use a
neighbor-constrained regularization to improve the consistency of
adjacent nodes.
Table 9: Parameter settings. Here "lr" means the learning
rate, "ùêæ" means the number of convolution layers, "hidden"
means the hidden size of the network, and "batch number"
means the number of neighbor sampling.
Datasets lrùêæhidden batch number
Arxiv 1e-3 4 1024 12
Mag 1e-3 4 512 12
Products 1e-3 4 512 12
Patent 1e-3 4 512 12
A.3 Experiment Settings and Extral results
We summarize the experimental setting of all baseline in Table 9.
To provide enough comparison with the baselines, we employ the
batched processing with neighbour sampling on GCN, SGC, DGI
and GGD when processing on Mag, Products andPatent datasets.
We keep the same setting with the common parameters like learn-
ing rate, K, hidden size. Following the work in [ 72], we set the
pretraining epoch of IDOL and GGD as 1, and the epoch number
of others is set as 200.
4711