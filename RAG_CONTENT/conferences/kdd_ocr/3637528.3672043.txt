Learning Multi-view Molecular Representations with Structured
and Unstructured Knowledge
Yizhen Luo
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
yz-luo22@mails.tsinghua.edu.cnKai Yang
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
yangkai@air.tsinghua.edu.cnMassimo Hong
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
hongcd21@mails.tsinghua.edu.cn
Xing Yi Liu
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
liuxingyi99@gmail.comZikun Nie
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
nzk20@mails.tsinghua.edu.cnHao Zhou
Institute of AI Industry Research
(AIR), Tsinghua University
Beijing, China
zhouhao@air.tsinghua.edu.cn
Zaiqing Nie∗
Institute of AI Industry Research
(AIR), Tsinghua University
Pharmolix Inc.
Beijing, China
zaiqing@air.tsinghua.edu.cn
ABSTRACT
Capturing molecular knowledge with representation learning ap-
proaches holds significant potential in vast scientific fields such as
chemistry and life science. An effective and generalizable molecular
representation is expected to capture the consensus and complemen-
tary molecular expertise from diverse views and perspectives. How-
ever, existing works fall short in learning multi-view molecular rep-
resentations, due to challenges in explicitly incorporating view in-
formation and handling molecular knowledge from heterogeneous
sources. To address these issues, we present MV-Mol, a molecular
representation learning model that harvests multi-view molecular
expertise from chemical structures, unstructured knowledge from
biomedical texts, and structured knowledge from knowledge graphs.
We utilize text prompts to model view information and design a
fusion architecture to extract view-based molecular representations.
We develop a two-stage pre-training procedure, exploiting hetero-
geneous data of varying quality and quantity. Through extensive
experiments, we show that MV-Mol provides improved representa-
tions that substantially benefit molecular property prediction. Ad-
ditionally, MV-Mol exhibits state-of-the-art performance in multi-
modal comprehension of molecular structures and texts. Code and
data are available at https://github.com/PharMolix/OpenBioMed.
∗Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672043CCS CONCEPTS
•Applied computing →Bioinformatics; •Computing method-
ologies→Knowledge representation and reasoning ;Natural lan-
guage processing .
KEYWORDS
Multi-view Molecular Representation Learning, Knowledge Graphs,
Text Mining
ACM Reference Format:
Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zikun Nie, Hao Zhou,
and Zaiqing Nie. 2024. Learning Multi-view Molecular Representations with
Structured and Unstructured Knowledge. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3672043
1 INTRODUCTION
Understanding the properties and functions of small molecules is
a pivotal issue in various scientific applications such as chemistry
[2], biology [ 56], and material design [ 15]. Due to the substantial
annotation costs for molecules, molecular representation learning
(MRL), which aims at capturing molecular knowledge from abun-
dant unlabeled data, has attracted significant research attention.
Through self-supervised learning [ 33] on various types of molecular
structures, such as 1D SMILES strings [ 5,18], 2D graphs [ 17,67,73]
or 3D conformations [ 57,80,81], MRL models have achieved great
success in advancing our understanding of molecules.
However, to further extend the application scope of MRL models,
one is faced with a critical problem: molecular expertise is compli-
cated and multifaceted, spanning diverse disciplines and views. It
comprises consensus information shared by multiple views, as well
as complementary information within each specific view [ 3]. As
 
2082
KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
Microscopic viewMacroscopic view
Monoclinic crystal structure 
Biological viewInhibit prostaglandin forming cyclooxygenase
Acetoxy groupBenzene ring Carboxyl groupSlightly soluble in water; Weak acidity;Complementary (intra-view) Consensus (inter-view) 
Boiling point: 284°F; Stable at room temperature; AnK-inﬂammaKon; Pain-killing;Molecule Representa4on Learning Model
Aspirin
…View 1View 2View N…
…
(a) The multi-view characteristics of molecular expertise.
High-quality~ 196K molecules ~ 3.6M rela;ons High-quality
Molecular StructuresBiomedical TextsKnowledge bases> 20B molecules~ 60K molecules > 20M documents Low-quality
(b) The heterogeneous sources of molecular expertise.
Figure 1: An overview of molecular expertise. Molecular expertise
covers diverse views that share consensus and complementary infor-
mation. It resides within heterogeneous sources that vary in quality
and quantity.
exemplified in Figure 1a, the characteristics of Aspirin are identified
through various perspectives. From a microscopic view, the mole-
cule comprises a benzene ring, an acetoxy group, and a carboxyl
group, resulting in slight water solubility and weak acidity. From a
macroscopic view, it exhibits a monoclinic crystal structure, influ-
encing physical properties such as boiling point and stability. From
a biological view, it inhibits the production of prostaglandins by
targeting cyclooxygenase, thus exhibiting anti-inflammatory and
pain-killing functions [ 63]. A versatile MRL model is expected to
generate view-based representations to address the distinctions be-
tween different application contexts. Unfortunately, existing works
primarily focus on learning a universal representation to capture
the consensus information across various views, and fall short in
grasping the complementary characteristics of each view.
More recently, the emergence of MRL methods incorporating
molecular knowledge from heterogeneous sources has opened novel
avenues for addressing the multi-view problem. These models focus
on jointly comprehending molecular structures, structured knowl-
edge from knowledge bases, and unstructured knowledge from
biomedical texts. The integration of heterogeneous inputs is ac-
complished either through language modeling objectives withina unified model [ 36,49,74], or by contrastive learning objectives
with independent encoders [ 31,55,58]. Innovated by the success
of these models, we aim to capture multi-view molecular expertise
from these structured and unstructured knowledge sources.
Nevertheless, there exist two challenges in learning multi-view
molecular representations. First, as illustrated in Figure 1a, the
MRL model should incorporate view information explicitly into
its representations to ensure its adaptability in broad applications.
However, prior MRL models integrate view information implic-
itly with ’wrapped texts’ [ 36,49] or fine-tuning on downstream
tasks, which compromises their understanding of the consensus
and complementary relationships between molecular knowledge
from different views. Besides, as shown in Figure 1b, it is essential to
address the heterogeneity of information sources, including molec-
ular structures, biomedical texts, and knowledge graphs, which
vary in quality and quantity. Previous works treat structured and
unstructured knowledge indiscriminately by transforming knowl-
edge graphs into texts. However, this may introduce biases across
different views due to the imbalanced distribution of pre-training
data.
In this work, we propose MV-Mol, a comprehensive framework
forMulti-View Molecular representation learning with structured
and unstructured knowledge, to address the aforementioned prob-
lems. To explicitly incorporate view information, we utilize text
prompts to capture the complementary and consensus characteris-
tics of different views for molecules. We leverage Q-Former [ 25],
a multi-modal fusion architecture, to extract view-based molec-
ular representations by jointly comprehending molecular struc-
tures and view prompts. Then, we propose a two-stage pre-training
strategy to address the heterogeneity of structured and unstruc-
tured knowledge. The first stage aligns molecular structures with
large-scale, noisy texts, extracting consensus information across
comprehensive views. The second stage incorporates high-quality,
structured knowledge from knowledge graphs. Drawing inspiration
from knowledge-enhanced pre-training [ 65,66], we treat relations
as specific types of views and describe them with texts. With con-
trastive and generative objectives, MV-Mol is endowed with the
ability to grasp complementary information within different views.
We show the superior performance of MV-Mol through fine-
tuning across various downstream tasks. Benefiting from view-
based molecular representations and two-stage pre-training, MV-
Mol achieves an average of 1.24% absolute gains over the state-
of-the-art method Uni-Mol [ 80] on molecule property prediction
[70]. Additionally, MV-Mol exhibits a deeper understanding of con-
nections between molecular structures and texts. On cross-modal
retrieval, MV-Mol improves the top-1 retrieval accuracy by 12.9%
on average over the best-performing baselines. On cross-modal
generation [ 10], MV-Mol also yields more accurate predictions, as
validated by qualitative and quantitative studies.
Our contributions are summarized as follows: (1) To the best of
our knowledge, MV-Mol is the first work that addresses the multi-
view problem in molecular representation learning. (2) We propose
to incorporate view information explicitly by jointly comprehend-
ing molecular structures and view prompts within a fusion network.
(3) We develop a two-stage training pipeline to harvest multi-view
molecular expertise from structured and unstructured knowledge
sources that vary in quality and quantity.
 
2083Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge KDD ’24, August 25–29, 2024, Barcelona, Spain
Trainable QueriesSelf-A1en2onCross-A1en2onFeed Forward×NStructure EncoderView-based Molecule Encoder
View PromptEmbeddingFeed ForwardMul2-Modal Decoder(a) Model Architecture of MV-MolCross-Modal Contras-ve (CMC)
EncoderMolecular StructureEncoder
Literature snippetsIn-batch Cosine SimilarityEncoderPredictorMatching Loss(d) Stage 1: Modality AlignmentCross-Modal Matching (CMM)
(e) Stage 2: Mul2-view Knowledge Incorpora2on
EncoderHeadEncoderRela2onhas a chemical role as
Contras2ve & Matching Losses
Tail
Knowledge Graph Embedding (KGE)DecoderGenera2on Loss
EncoderHeadRela2onhas a chemical role as
Knowledge Graph Comple-on (KGC)
Tail
Parameters of the Molecule BranchParameters of the Text Branch
(Mol branch)(Text branch)SampleHard Nega2vesContras2ve loss
(Mol + Text branch)
Shared Parameters of the Molecule and Text Branches(Mol + Text / Text branch)(Mol / Text branch)(Mol + Text / Text branch)(b) Model Architecture of the Molecule Branch
(c) Model Architecture of the Text BranchQueriesSelf-A1en2onCross-A1en2onFeed ForwardStructure Encoder
Molecular Structure
Molecular Structure×N
Self-A1en2onTextEmbeddingFeed Forward
×NMolecular Structure
Literature snippets
Figure 2: Model architecture and pre-training pipeline of MV-Mol. (a) MV-Mol is composed of a view-based molecule encoder and a multi-modal
decoder. (b) The molecule branch of the view-based molecule encoder. (c) The text branch of the view-based molecule encoder. (d) We perform
cross-modal contrastive and cross-modal matching for modality alignment. (c) We model relation as a textual prompt that constrains molecular
knowledge from a specific view, and design knowledge graph embedding and knowledge graph completion objectives for multi-view knowledge
incorporation. Both branches are activated when the head entity refers to a molecule.
2 RELATED WORK
2.1 Multi-view Molecular Representation
Learning
Multi-view representation learning [ 29,79] aims to pursue a uni-
fied and comprehensive feature representation based on multi-view
data. Most MRL models treat different forms of molecular structures
as distinct views and model complementary and consensus infor-
mation with different pre-training objectives. For example, DVMP
[82] treats 1D SMILES sequences and 2D molecular graphs as dual
views. GraphMVP [ 32] treats 2D topologies and 3D geometries as
two views, and proposes independent objectives for intra-view and
inter-view modeling. MORN [ 41] treats the sequential, topological,
and spatial information as different views and implements a two-
stage adaptive learning strategy with multi-view fusion for molecu-
lar property prediction. [ 64] treats atoms, scaffolds, and functional
groups as independent views and proposes a prompt-based aggre-
gation module to extract task-specific molecular representations. In
this work, we extend the multi-view concept of molecules as their
inherent properties and patterns within a certain context, which is
defined flexibly by human-understandable textual prompts.
2.2 Molecular Representation Learning with
Heterogeneous Sources
Recently, MRL approaches that jointly harvest heterogeneous sources
including molecular structures, biomedical texts, and knowledge
graphs have surged. Drawing parallels from vision-language pre-
training (VLP) [ 25,51,59], these models can be categorized asfollows: (1) Generative models including KV-PLM [ 74], MolT5 [ 10],
MolXPT [ 36], Text+Chem [ 6], GIMLET [ 77], BioMedGPT [ 40], Mol-
Instructions [ 13], BioT5 [ 49] and BioT5+ [ 48]. These models treat
molecules and texts as two languages and jointly process them
within a unified language model [ 21,52,53]. (2) Contrastive models
like MoMu [ 58], MoleculeSTM [ 31], CLAMP [ 55], KANO [ 14] and
GODE [ 19]. These models incorporate texts or knowledge graphs as
cross-modal supervision signals and augment molecular represen-
tations with contrastive learning [ 51]. (3) Hybrid models including
MolFM [ 39], MolCA [ 35], GIT-Mol [ 30] and 3D-MoLM [ 28]. These
models perform fine-grained feature integration with multi-modal
fusion networks and incorporate both generative and contrastive
objectives for pre-training. While MV-Mol also adopts a fusion
network architecture, we distinguish it from prior works by our
explicit modeling of views by jointly comprehending molecular
structures and texts and our pre-training process that addresses
data imbalance of heterogeneous knowledge sources.
3 METHOD
As depicted in Figure 2, MV-Mol is designed to learn multi-view
molecular representations from heterogeneous sources. The core of
our approach lies in modeling view information with text prompts.
The remainder of this section is organized as follows: First, we
provide a roadmap for extracting multi-view molecular expertise
within heterogeneous sources from a knowledge-enhanced pre-
training perspective (Sec. 3.1). Then, we introduce the model archi-
tecture for MV-Mol (Sec. 3.2) that extracts view-based molecular
representations. To address the heterogeneity of pre-training data,
 
2084KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
we develop a two-stage pre-training paradigm, harvesting unstruc-
tured knowledge from biomedical documents through modality
alignment (Sec. 3.3) and structured knowledge from knowledge
bases with knowledge incorporation (Sec. 3.4).
3.1 A Knowledge-enhanced Pre-training
Perspective for Multi-view MRL
In this section, we interpret multi-view MRL from the perspective of
knowledge-enhanced pre-training [ 16,65,66], providing a roadmap
for methodology design. Inspired by OntoProtein [ 76], we formulate
the pre-training data as triplets (ℎ,𝑟,𝑡), whereℎand𝑡are head and
tail entities, and 𝑟signifies the relation. We define two types of
entities,𝐸moland𝐸text.𝑒(𝑠)∈𝐸molis denoted as a molecule node
with structural information. 𝑒(𝑡)∈𝐸textis denoted as a text node
formulated by a token sequence. 𝑟is also formulated as text tokens
that describe the relation. Depending on the types of the head and
tail entities, we further categorize pre-training data as molecule-text
triplets, molecule-molecule triplets, and text-text triplets.
Knowledge-enhanced pre-training methods typically maximize
the following objective:
L=𝑆(𝑓(ℎ,𝑟),𝑔(𝑡)), (1)
where𝑓(ℎ,𝑟)is the relation-transformed representation of the head
entity,𝑔(𝑡)is the representation of the tail entity, and 𝑆(·,·)is a
scoring function that measures the alignment of the triplet. We
observe for ℎ∈𝐸mol, the view-based molecular representations
have an equivalent form of 𝑓(ℎ,𝑟). Besides, structured knowledge
graphs are indispensable sources for multi-view MRL, as both views
and relations can be conceptualized as textual refinements over
specific aspects of molecules. Therefore, multi-view MRL comprises
the following key design components:
•The model architecture for 𝑓(ℎ,𝑟), tasked with extracting
different feature representations of ℎin the context of 𝑟. Asℎ
and𝑟may come from different modalities, the model should
jointly comprehend molecular structures and texts.
•The formulation of the scoring function 𝑆(·,·), intended to
harvest rich supervision signals from heterogeneous sources.
3.2 Model Architecture for MV-Mol
The model architecture of MV-Mol is depicted in Figure 2(a), com-
prising 2 parts: (1) A view-based molecule encoder that jointly
comprehends molecular structures and texts. (2) A multi-modal
decoder that generates human-understandable texts.
View-based Molecule Encoder. We define the structure of a mol-
ecule asM=(V,E,C)whereVrepresents atoms,Erepresents
bonds, andC∈R|V|× 3represents the 3D coordinates for each
atom. We leverage Uni-Mol [ 80], a 15-layer Transformer [ 71] pre-
trained on 209M 3D conformations, to encode M. The pre-trained
molecular model mitigates the disproportionality between molecu-
lar structures and text data [58]. The structure encoder 𝑓moltrans-
formsMinto feature representations for each atom, which we
denote as𝑧(𝑎).
In MV-Mol, we use text prompts to model different views of
molecules, represented by a sequence of 𝐿tokensT=[𝑥1,𝑥2,···,𝑥𝐿].
Consequently, the view-based molecule encoder is expected to com-
prehend molecular structures and texts simultaneously. To achievethis, we leverage Querying Transformer (Q-Former) [ 25], a novel
multi-modal fusion architecture from vision-language pre-training.
The Q-Former consists of two branches, each consisting of 12 bidi-
rectional Transformer layers initialized with KV-PLM* [ 74]. The
molecule branch takes 𝐾trainable query vectors as input embed-
dings. The query features extract pivotal information from atomic
representations 𝑧(𝑎)through cross-attention within every other
Transformer layer. The text branch digests Tas its input. The self-
attention layers across the two branches are shared, allowing the
queries to grasp fine-grained semantics from text prompts. In this
way, the molecular representations are propagated within each
Q-Former layer to incorporate information from different views.
Overall, the view-based molecule encoder 𝑓𝑣can transform the
molecular structure to a fixed number of features with the structure
encoder and the molecule branch of Q-Former, which is illustrated
in Fig. 2b:
𝑧(𝑠)=h
𝑧(𝑠)
1,𝑧(𝑠)
2,···,𝑧(𝑠)
𝐾i
=𝑓𝑣(M). (2)
Using Q-Former’s text branch, it can also comprehend natural
language as in Fig. 2c:
𝑧(𝑡)=h
𝑧(𝑡)
1,𝑧(𝑡)
2,···,𝑧(𝑡)
𝐿i
=𝑓𝑣(T), (3)
Most importantly, as shown in Figure 2a, it can grasp view-based
molecular representations through multi-modal feature fusion of
molecular structures and texts:
𝑧(𝑠,𝑡)=h
𝑧(𝑠,𝑡)
1,𝑧(𝑠,𝑡)
2,···,𝑧(𝑠,𝑡)
𝐾i
=𝑓𝑣(M,T). (4)
Multi-modal Decoder. The multi-modal decoder allows MV-Mol
to interpret the view-based molecular representations with natural
language. We employ BioT5 [ 49], a molecular language model with
12 Transformer encoder layers and 12 Transformer decoder lay-
ers. The model has undergone multi-task pre-training on SELFIES
strings of molecular structures, FASTA sequences of proteins, and
biomolecule-related texts. For MV-Mol, we leverage the decoder
branch of BioT5 as the text decoder 𝑓dec, which transforms the
Q-Former outputs into a sequence of 𝑁tokens𝑦=[𝑦1,𝑦2,···,𝑦𝑁]
by casual generation:
𝑦𝑖=argmaxn
𝑃
𝑦𝑖𝑦<𝑖,𝑧(·)o
, (5)
where𝑦<𝑖denotes preceding tokens, 𝑧(·)refers to either 𝑧(𝑠),𝑧(𝑡)
or𝑧(𝑠,𝑡), and the conditional probability is modeled by the text
decoder𝑓dec.
Discussions. While our model shares a similar architecture with
MolCA [ 35] and 3D-MoLM [ 28], we emphasize that MV-Mol ex-
tracts view-based molecular representations in Eq. 4 with both
branches of the Q-Former. In contrast, prior works are unaware of
view information and encode molecules as in Eq. 2, where the text
branch of the Q-Former is deactivated.
3.3 Modality Alignment with Large-scale
Unstructured Knowledge
In this stage, we aim to capture the consensus information within
numerous views of molecules and cultivate MV-Mol’s capability of
jointly comprehending molecular structures and texts. This is ac-
complished by aligning the feature space between molecular struc-
turesℎ∈𝐸moland the relevant contexts 𝑡∈𝐸textfrom biomedical
 
2085Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge KDD ’24, August 25–29, 2024, Barcelona, Spain
literature. We omit the relation 𝑟due to the unaffordable costs
of annotating millions of noisy texts. As shown in Figure 2b, the
pre-training objectives are:
Cross-Modal Contrastive Loss. This objective maximizes the mu-
tual information between the structural and textual representations.
We define structure-text similarity as follows:
sim
𝑧(𝑠),𝑧(𝑡)
=max
𝑖=1,2,···,𝐾
𝑓proj
𝑧(𝑠)
𝑖𝑇
𝑓proj
𝑧(𝑡)
1
,(6)
where𝑓proj(·)is a projection network composed of a fully-connected
layer and𝑙2normalization, and 𝑧(𝑡)
1denotes the textual representa-
tion of the [CLS] token.
The objective function is calculated as:
Lcmc=−1
2
logexp(sim[𝑓𝑣(ℎ),𝑓𝑣(𝑡)]/𝜏)Í
ℎ′exp(sim[𝑓𝑣(ℎ′),𝑓𝑣(𝑡)]/𝜏)
+logexp(sim[𝑓𝑣(ℎ),𝑓𝑣(𝑡)]/𝜏)Í
𝑡′exp(sim[𝑓𝑣(ℎ),𝑓𝑣(𝑡′)]/𝜏)
,(7)
whereℎ′and𝑡′are molecules and texts within the same mini-batch,
and𝜏is a temperature hyper-parameter.
Cross-Modal Matching Loss. This objective fosters the fine-
grained comprehension of molecular structures and texts by pre-
dicting whether they correspond to the same molecule. Following
[25,27], we acquire hard negatives for each sample by obtaining
one misaligned structure for the text and one misaligned text for
the structure from the same mini-batch based on the similarity
scores defined in Eq. 6. The objective is calculated as follows:
Lcmm=𝐻[𝑦cmm(ℎ,𝑡), 𝑓cmm(𝑓𝑣(ℎ,𝑡))], (8)
where𝑓cmm is a predictor composed of an average pooling and a
fully-connected layer, 𝑦cmm(·,·)is the ground truth label indicating
the alignment of the molecule-text pair, and 𝐻(·,·)denotes cross-
entropy.
The overall objective is the summation of the two loss functions:
L1=E(ℎ,𝑡)∼D 1(Lcmc+L cmc), (9)
whereD1is the pre-training dataset.
3.4 Multi-view Knowledge Incorporation with
Heterogeneous Triplets
In this stage, we aim to incorporate high-quality structured knowl-
edge into MV-Mol and capture the multi-view characteristics of
molecules within three different types of triplets. Motivated by
[65,66], we introduce relation transformed representations of the
head entity, 𝑧(ℎ,𝑟), calculated as follows:
𝑧(ℎ,𝑟)=(
𝑓𝑣(ℎ,𝑟), ℎ∈𝐸𝑚𝑜𝑙
𝑓𝑣(ℎ⊕𝑟), ℎ∈𝐸𝑡𝑒𝑥𝑡(10)
where⊕denotes concatenation. By modeling relations as view
prompts, we capture view-based molecular representations within
the first term of the equation. We connect 𝑧(ℎ,𝑟)and𝑡with objec-
tives shown in Figure 2c, which we detail as follows:
Knowledge Graph Embedding Loss. This objective captures the
global features within knowledge bases by aligning 𝑧(ℎ,𝑟)and therepresentations of the tail entity. We first employ the following
contrastive objective:
Lkge_c =−1
2"
logexp(simtri[𝑧(ℎ,𝑟),𝑓𝑣(𝑡)]/𝜏)Í
ℎ′exp(simtri[𝑧(ℎ′,𝑟),𝑓𝑣(𝑡)]/𝜏)
+logexp(simtri[𝑧(ℎ,𝑟),𝑓𝑣(𝑡)]/𝜏)Í
𝑡′exp(simtri[𝑧(ℎ,𝑟),𝑓𝑣(𝑡′)]/𝜏)#
,(11)
whereℎ′and𝑡′are head and tail entities within the same batch,
andsimtri(·,·)calculates the representation similarity, which we
detail in Appendix A. This objective is a generalized form of Eq. 7
that incorporates relation information from heterogeneous triplets.
Then, we incorporate a matching loss similar to Eq. 8 for molecule-
text and text-text triplets with the same negative sampling strategy
for the head and tail entity. We omit molecule-molecule triplets as
𝑓𝑣cannot process two molecules simultaneously. The objective is
calculated as follows:
Lkge_m =𝐻[𝑦cmm(ℎ,𝑟,𝑡), 𝑓cmm(𝑓𝑣(ℎ,𝑟⊕𝑡))]. (12)
Knowledge Graph Completion Loss. This objective elicits MV-
Mol to ’talk’ about molecular knowledge from a specific view. We
implement the language model-based tail entity prediction task
[34,50] by feeding the relation-transformed representations 𝑧(ℎ,𝑟)
into the text decoder to generate descriptions for the tail entity. The
objective is calculated for molecule-text and text-text triplets in an
auto-regressive manner:
Lkgc=∑︁
𝑥𝑖∈𝑡𝐻h
𝑥𝑖,𝑃
𝑥𝑖|𝑥<𝑖,𝑧(ℎ,𝑟)i
, (13)
where the conditional probability is modeled by the text decoder
𝑓dec.
The overall objective is calculated as follows:
L2=E(ℎ,𝑟,𝑡)∈D 2(Lkge_c+L kge_m+L kgc), (14)
whereD2is the knowledge graph.
4 EXPERIMENTS
In this section, we conduct extensive experiments to demonstrate
the effectiveness of the proposed model by answering the following
questions:
•Q1.Compared to prior MRL models, does MV-Mol provide
better molecular representations?
•Q2.Does MV-Mol address the heterogeneity of molecular
structures and texts?
•Q3.Do view-based molecular representations capture the
consensus and complementary knowledge from different
views? How do view prompts affect the representations?
•Q4.How do the design components within the two pre-
training stages affect MV-Mol?
4.1 Pre-training Setup
Pre-training Dataset. For the modality alignment stage, we collect
large-scale molecule-text pairs by named entity recognition (NER)
[43] and entity linking [ 23] on 3.5M scientific publications following
previous works [ 36,49,74], and obtain a total of 60K molecules and
12M molecule-text pairs. For the knowledge incorporation stage,
we construct a knowledge graph by combining public databases
 
2086KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
Table 1: AUROC scores for molecular property prediction on MoleculeNet. The best results are marked in bold, and the second-best results are
underlined.↑: the higher the better. †: Our implementation. -: Not reported in the original paper. w/o view: without view prompts.
Model BBBP ↑Tox21↑ToxCast↑SIDER↑ClinTox↑MUV↑ HIV↑ BACE↑Avg↑
GraphCL [73] 67.5 ±3.375.0±0.3 62.8±0.2 60.1±1.3 78.9±4.2 77.1±1.075.0±0.468.7±7.870.64
GraphMVP [32] 72.4 ±1.674.4±0.2 63.1±0.4 63.9±1.2 77.5±4.2 75.0±1.077.0±1.281.2±0.973.07
GEM [12] 72.4±0.478.1±0.1 69.2±0.4 67.2±0.4 90.1±1.3 81.7±0.580.6±0.985.6±1.178.11
Uni-Mol [80] 72.9 ±0.679.6±0.5 69.6±0.1 65.9±1.3 91.9±1.882.1±1.380.8±0.385.7±0.278.56
KV-PLM [74] 66.9 ±1.164.7±1.8 58.6±0.4 55.3±0.8 84.3±1.5 60.2±2.968.8±4.971.9±2.166.29
MoMu [58] 70.5 ±2.075.6±0.3 63.4±0.5 60.5±0.9 79.9±4.1 70.5±1.475.9±0.876.7±2.171.63
MoleculeSTM [31] 70.0 ±0.577.0±0.4 65.1±0.3 61.0±1.0 92.5±1.1 73.4±2.976.9±1.880.8±1.374.57
MolCA [35] 70.0 ±0.577.2±0.5 64.5±0.8 63.0±1.7 89.5±0.7 - - 79.8 ±0.5 -
GIT-Mol [30] 73.9 ±0.675.9±0.5 66.8±0.5 63.4±0.8 88.3±1.2 - - 81.1 ±1.5 -
BioT5†[49] 75.2±0.175.5±0.3 64.4±0.1 62.4±0.1 85.8±0.6 75.9±0.779.7±0.489.2±0.176.01
MV-Mol (w/o view) 73.0 ±0.279.7±0.1 69.7±0.2 65.3±0.2 93.2±0.2 81.6±0.479.9±0.587.3±0.278.71
MV-Mol 73.6±0.280.3±0.670.0±0.467.3±0.095.6±1.682.1±0.581.4±0.388.2±0.479.80
Table 2: Statistics of benchmark datasets. We present the number of
molecules, data samples, prediction tasks, and split protocols.
Dataset # Molecules # Samples # Tasks Split
BBBP 2,039 2,039 1
Scaffold (8/1/1)Tox21 7,831 7,831 12
ToxCast 8,597 8,597 617
SIDER 1,427 1,427 27
ClinTox 1,478 1,478 2
MUV 93.807 93,807 17
HIV 41,127 41,127 1
BACE 1,513 1,513 1
PCdes 11,112 11,112 1Scaffold (7/1/2)MVST 7,102 16,996 1
CheBI-20 33,008 33,008 1 Random (8/1/1)
including CheBI [ 7], PubChem [ 22], and DrugBank [ 69], which
comprises 273K entities and 643K relations. During pre-training
data collection, we exclude molecules within the test set of down-
stream datasets to alleviate information leakage. More details are
presented in Appendix B.
Training Configurations. MV-Mol comprises a molecular struc-
ture encoder with 47.3M parameters, a Q-Former with 180.8M pa-
rameters, and a text decoder with 140.2M parameters. We pre-train
MV-Mol for 70K steps with a batch size of 256 for the modality
alignment stage and 50K steps with a batch size of 192 for the
knowledge incorporation stage. The pre-training procedure is per-
formed on 4 NVIDIA A100 GPUs for 5 days. We use the AdamW
[38] optimizer with a weight decay of 5×10−2. The learning rate is
linearly warmed up to 10−4in the first 2K steps and then decreases
to10−5following a cosine annealing strategy. The temperature 𝜏
for contrastive learning is fixed as 0.1.
4.2 Molecular Property Prediction (Q1)
To assess the quality of molecular representations, we perform fine-
tuning experiments on molecular property prediction, a widely
adopted task in MRL.Datasets. We adopt 8 classification datasets from MoleculeNet [ 70],
a popular benchmark that covers diverse properties of molecules.
In line with prior works [ 32,80], we adopt Scaffold split with
a train/validation/test ratio of 8/1/1. Statistics of the benchmark
datasets are presented in Table 2.
Baselines. We adopt two types of baseline MRL models:
•Uni-modal MRL baselines: GraphCL [ 73], GraphMVP [ 32],
GEM [ 12] and UniMol [ 80]. These models are pre-trained
solely with molecular structures including 2D topologies, 3D
conformations, or both.
•Multi-modal MRL baselines: KV-PLM [ 74], MoMu [ 58], Mole-
culeSTM [ 31], MolCA [ 35], GIT-Mol [ 30], and BioT5 [ 49].
These models incorporate heterogeneous pre-training data
including molecular structures and texts.
Implementation Details. We manually write descriptions for
each dataset and feed the prompt and molecular structure into
MVMol’s encoder to obtain view-based molecular representations.
Then, we feed the encoder outputs into a max-pooling layer and a
projector for binary classification. The projector is composed of two
fully connected layers with ReLU activation. We also implement a
variant of our model named MV-Mol (w/o view) by removing view
prompts and solely encoding molecules as in Eq. 2. Notably, we ap-
ply the prediction head with BioT5’s encoder for a fair comparison
of the representation quality. We perform experiments three times
with different random seeds and report AUROC scores. Refer to
Appendix D for more details of our hyper-parameters and prompts.
Results and Analysis. The overall results of molecular property
prediction are displayed in Table 1. The key observations are as
follows: (1) Compared with the state-of-the-art model Uni-Mol,
MV-Mol achieves an absolute gain of 1.24% on average. Overall,
MV-Mol performs best on 5 of 8 datasets and second-best on the
remaining 3 datasets. This outstanding performance validates the
informativeness of MV-Mol’s molecular representations. (2) On
SIDER, ClinTox, and BACE which consist of limited training sam-
ples, MV-Mol attains more significant improvements over Uni-Mol
(1.4% on SIDER, 3.7% on ClinTox, and 2.5% on BACE), which indi-
cates that adapting molecular representations with view prompts
 
2087Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Zero cross-modal retrieval results on the test split of PCdes.
The best results are marked in bold, and the second-best results are
underlined.↑: the higher the better.
SubTask Model MRR ↑R@1↑R@5↑R@10↑
S-TCLAMP [55] 0.58 0.18 0.49 0.67
MoMu [58] 11.10 5.93 15.20 20.46
MoleculeSTM [31] 40.30 26.99 55.33 66.48
MolCA [35] 33.65 23.21 45.52 54.29
3D-MoLM [28] 1.68 0.53 1.61 2.57
MV-Mol (w/o view) 41.17 30.85 53.12 60.23
MV-Mol 45.25 36.66 55.78 60.41
T-SCLAMP [55] 0.43 0.08 0.27 0.54
MoMu [58] 11.58 6.29 15.78 21.86
MoleculeSTM [31] 30.18 17.45 43.94 57.71
MolCA [35] 33.48 23.70 44.30 53.30
3D-MoLM [28] 2.69 0.76 3.60 5.89
MV-Mol (w/o view) 45.51 35.89 56.68 62.70
MV-Mol 46.54 37.24 57.17 62.97
Table 4: Zero cross-modal retrieval results on the test split of MVST.
The best results are marked in bold, and the second-best results are
underlined.↑: the higher the better.
SubTask Model MRR ↑R@1↑R@5↑R@10↑
S-TCLAMP [55] 0.34 0.07 0.23 0.30
MoMu [58] 13.59 6.97 20.06 26.69
MoleculeSTM [31] 17.80 9.61 25.98 35.59
MolCA [35] 18.89 14.25 24.43 27.33
3D-MoLM [28] 1.09 0.19 1.32 2.22
MV-Mol (w/o view) 24.87 16.40 34.08 41.32
MV-Mol 35.38 24.28 47.92 57.27
T-SCLAMP [55] 0.33 0.04 0.23 0.53
MoMu [58] 11.81 5.16 17.60 25.26
MoleculeSTM [31] 12.68 4.86 19.41 29.67
MolCA [35] 11.05 4.86 18.13 22.43
3D-MoLM [28] 1.44 0.26 2.00 3.09
MV-Mol (w/o view) 20.86 9.35 33.37 44.26
MV-Mol 34.07 22.85 45.73 56.56
brings more significant benefits to low-data scenarios. (3) Remov-
ing view descriptions leads to a decline of 0.79% on average. This
performance drop corroborates our claims that downstream ap-
plications require molecular knowledge from different views, and
a universal molecular representation fails to capture the specific
context of each task.
4.3 Zero-shot Cross-modal Retrieval (Q2)
We investigate whether MV-Mol captures rich molecular knowl-
edge from heterogeneous sources including texts and knowledge
graphs. Hence, we evaluate MV-Mol on cross-modal retrieval, which
contains two sub-tasks: structure-to-text (S-T) retrieval and text-
to-structure (T-S) retrieval. The former aims to retrieve the most
relevant text that describes a given molecule, and the latter aims to
retrieve a molecule that best fits the textual description.Datasets. We incorporate two datasets: PCdes [ 74] and MVST.
The PCdes dataset is collected from the CheBI database and com-
prises 15K molecules with their biochemical definitions and prop-
erty descriptions. MVST (Multi-View Structure-Text) is a novel
dataset introduced by this work. It is collected from chemical, phys-
ical, and pharmacokinetic definitions in PubChem. It consists of
7.1K molecules, each corresponding with 2 or more texts from dif-
ferent views. Both datasets are partitioned by Scaffold split with
a train/validation/test ratio of 7/1/2. Differing from prior works
[58,74], we perform retrieval on the whole test set and report
MRR (mean reversed rank) and Recall at 1/5/10. More details of the
datasets are presented in Appendix C.
Baselines. We compare MV-Mol with models that have undergone
contrastive learning on molecule-text data including CLAMP [ 55],
MoMu [58], MoleculeSTM [31], MolCA [35] and 3D-MoLM [28].
Implementation Details. On PCdes, we fix the view description
as ’biochemical properties and functions’. On MVST, we write de-
scriptions for physical, chemical, and pharmacokinetic views to
generate different representations for the same molecule. To fur-
ther improve the retrieval performance, we modify the re-ranking
algorithm in [ 27] with an ensemble technique. We first retrieve the
top-𝑘candidates based on structure-text similarity in Eq. 6. Then,
we calculate the CMM logits in Eq. 8 for these 𝑘candidates. Fi-
nally, we re-rank them by a linear combination of the structure-text
similarity scores and the CMM logits.
Results and Analysis. The retrieval results on PCDes are pre-
sented in Tab. 3. We observe that: (1) MV-Mol outperforms state-
of-the-art methods by 4.95% and 13.06% in MRR for S-T and T-S
retrieval. While MoleculeSTM achieves the best results in R@10
on S-T retrieval, it is worth noting that the model is pre-trained
on texts similar to those in PCDes and molecules within the test
set, raising information leakage concerns. Nevertheless, MV-Mol
demonstrates effectiveness in bridging molecular structures and
texts. (2) Adding the view description yields a notable gain of 3.08%
and 1.03% in MRR for S-T and T-S retrieval, indicating that the
view-based molecular representations align better with texts in
PCdes.
Table 4 shows the results on MVST, where MV-Mol yields sub-
stantial improvements, surpassing MoleculeSTM and MolCA by
16.49% and 21.39% in MRR for S-T and T-S retrieval. The greater
performance gain suggests that MV-Mol is endowed with molecular
knowledge from a wider range of views. Notably, removing view
descriptions leads to a more significant decrease, which we attribute
to the multi-view property of the MVST dataset. While a molecule
may correspond with multiple descriptions from distinct views,
MV-Mol could distinguish between them based on the view-based
molecular representations.
4.4 Cross-modal Generation (Q2)
Cross-modal generation also involves two sub-tasks: structure-to-
text generation (molecule captioning) and text-to-structure genera-
tion (text-based molecule generation).
Datasets. We conduct experiments on ChEBI-20 [ 11], a widely-
adopted dataset for both tasks. We follow the original split with a
train/valid/test ratio of 8/1/1 and adopt evaluation metrics in [10].
Baselines. We compare MV-Mol with the following baselines:
 
2088KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
Table 5: Molecule captioning results on the test split of ChEBI-20.
’BL’ is short for ’BLEU’, and ’R’ is short for ’ROUGE’. The best results
are marked in bold, and the second-best results are underlined. ↑:
the higher the better. -: Not reported in the original paper.
Model #Params BL-2 ↑BL-4↑R-1↑R-2↑R-L↑METEOR↑
MolReGPT [26] - 0.565 0.482 0.623 0.450 0.543 0.585
MolT5-base [10] 250M 0.540 0.457 0.634 0.485 0.578 0.569
MolT5-large [10] 770M 0.594 0.508 0.654 0.510 0.594 0.612
MoMu [58] 770M 0.599 0.515 - - 0.593 0.597
MolXPT [36] 350M 0.594 0.505 0.660 0.511 0.597 0.626
Text+Chem [6] 250M 0.625 0.542 0.682 0.543 0.622 0.648
MolCA [35] 110M 0.620 0.531 0.681 0.537 0.618 0.651
ChatMol [75] 220M 0.620 0.535 0.677 0.538 0.617 0.644
GIT-Mol [30] 320M 0.352 0.263 0.575 0.485 0.560 0.533
BioT5 [49] 252M 0.635 0.556 0.692 0.559 0.633 0.656
MV-Mol (w/o view) 452M 0.650 0.572 0.698 0.567 0.640 0.669
MV-Mol 452M 0.653 0.575 0.699 0.569 0.640 0.669
•MolReGPT [ 26]. This baseline leverages a kNN few-shot
strategy [ 44] for in-context learning [ 9] with ChatGPT [ 45].
•Molecular language model baselines, including MolT5 [ 10],
MolXPT [ 36], Text+Chem [ 6], ChatMol [ 75] and BioT5 [ 49]
that process 1D SMILES or SELFIES strings of molecules and
texts within a unified language model.
•Fusion model baselines, namely MoMu [ 58]. The baseline
connects the GNN outputs with MolT5 to solve molecule
captioning.
Implementation Details. For molecule captioning, we fix the
view description as ’biochemical properties and functions’. In line
with [ 30,58], we concatenate the outputs of the MV-Mol encoder
with the SELFIES representations of the BioT5 encoder and feed the
results into our multi-modal decoder to generate the caption. For
text-based molecule generation, we feed the textual representations
in Eq. 3 into the MV-Mol decoder to generate the SELFIES string of
the molecule.
Results and Analysis. Table 5 shows the results of molecule cap-
tioning. Compared to the state-of-the-art model BioT5, MV-Mol
shows 1.8% improvements in BLEU scores [ 47] and 1.3% improve-
ments in METEOR [ 1] scores, indicating that it generates smoother
and more semantically related molecular descriptions.
The performance comparison for text-based molecule genera-
tion is reported in Table 5. MV-Mol achieves state-of-the-art perfor-
mance in most evaluation metrics, highlighted by an outstanding
exact ratio of 0.438, surpassing state-of-the-art by 2.5%. While MV-
Mol underperforms Text+Chem in fingerprint similarity, we argue
that these metrics are only calculated on valid molecules, which
may yield over-optimistic results.
Above all, the experiments show that MV-Mol addresses data het-
erogeneity and can flexibly translate between molecular structures
and natural language.
4.5 A Deeper Analysis of View-based Molecular
Representations (Q3)
Visualization of View-based Molecular Representations. To
investigate how view descriptions affect molecular representations,
Figure 3: Visualization of view-based molecular representations. We
show the molecular representations from chemical, physical, and
pharmacokinetic views. We also highlight three molecules and the
representations of their textual descriptions from each view.
we visualize the molecular features of MV-Mol on the MVST dataset
with UMAP [ 42]. As shown in Figure 3, the molecular represen-
tations based on chemical, physical, and pharmacokinetic views
exhibit clear separation along the x-axis. Furthermore, we high-
light three representative molecules (indicated by colored shapes)
along with corresponding texts from each view (indicated by black
shapes), which are closely positioned. Notably, the partial order of
the three molecules along the y-axis remains consistent under differ-
ent views. These observations support our intuition that view-based
molecular representations can concurrently capture the consensus
and complementary molecular knowledge from different views.
View Prompt Engineering. Motivated by recent advances in
prompt engineering [ 4,68], we explore how different view prompts
affect the representations of our model. We perform analysis on
BBBP, SIDER, ClinTox, and BACE from molecule property predic-
tion, and implement the following strategies:
•Empty prompt. This strategy refers to MV-Mol (w/o view),
where the view information is not provided.
•Word prompt. We use the name of the dataset as the prompt.
•Sentence prompt. We manually write a brief definition of
the prediction task as the prompt.
•Paragraph prompt. We write a well-rounded introduction to
the dataset.
As shown in Figure 4b, the sentence prompt works best with
BBBP, SIDER, and BACE, while the paragraph prompt yields the
best results on ClinTox. We posit that BBBP, SIDER, and BACE
focus on specific properties including blood-brain barrier penetra-
tion, adverse drug reactions, and target inhibition, where a single
sentence suffices to encapsulate all relevant information. In Clin-
Tox, however, more texts are necessary to describe clinical trial
criteria and toxicity measures. Besides, the word prompt brings
little improvement over the empty prompt. We attribute this to the
abbreviated dataset names, which are not frequently used in the
pre-training corpora and provide little meaningful information for
MV-Mol.
 
2089Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 5: Text-based molecule generation results on the test split of ChEBI-20. The best
results are marked in bold, and the second-best results are underlined. ↑: The higher the
better.↓: The lower the better. -: Not reported in the original paper.
Model # Params BLEU ↑Exact↑Valid↑Levenshtein↓MACCS
FTS↑RDKit
FTS↑Morgan
FTS↑
MolReGPT [26] - 0.790 0.139 0.887 24.910 0.847 0.708 0.624
MolT5-base [10] 250M 0.779 0.082 0.786 25.188 0.787 0.661 0.601
MolT5-large [10] 770M 0.854 0.311 0.905 16.071 0.834 0.746 0.684
MoMu [58] 235M 0.815 0.183 0.863 20.520 0.847 0.737 0.678
MolXPT [36] 350M - 0.215 0.983 - 0.859 0.757 0.667
Text+Chem [6] 250M 0.853 0.322 0.943 16.870 0.901 0.816 0.757
ChatMol [75] 220M 0.848 0.258 0.947 16.759 0.883 0.790 0.726
GIT-Mol [30] 250M 0.756 0.051 0.928 26.315 0.738 0.582 0.519
BioT5 [49] 252M 0.854 0.413 1.000 15.200 0.886 0.801 0.734
MV-Mol 252M 0.858 0.438 1.000 14.952 0.890 0.810 0.745
Figure 4: Experiment results on BBBP, SIDER, Clin-
Tox, and BACE for different prompts.
4.6 Ablation Studies (Q4)
To explore the contribution of our two-stage pre-training, we com-
pare the performances of MV-Mol by removing different pre-training
objectives for uni-modal and multi-modal tasks and display the re-
sults in Table 6. We observe that: (1) Knowledge graph completion
(KGC) loss mainly contributes to cross-modal generation but com-
promises MV-Mol’s performance on cross-modal retrieval (line 2).
(2) Removing the knowledge incorporation stage leads to signifi-
cant deterioration on all downstream tasks (line 3), highlighting the
significance of incorporating multi-view knowledge of molecules.
(3) The cross-modal matching objective also benefits all tasks (lines
3 and 4), as it cultivates the joint comprehension of molecules and
texts. (4) Removing the modality alignment stage leads to moderate
performance drops (line 5), indicating the indispensability of molec-
ular knowledge from large-scale publications. (5) Compared with
Stage 1 pre-training, Stage 2 brings more contributions, especially
for retrieval tasks, which we attribute to two reasons. First, the
knowledge incorporation stage is the key to multi-view MRL, al-
lowing MV-Mol to jointly comprehend view prompts and molecular
structures. Second, the knowledge graph enjoys higher quality than
the noisy molecule-text pairs for the modality alignment stage.
Table 6: Comparison between MV-Mol with different pre-training
objectives. For molecule property prediction, we report the average
results on 8 datasets. For zero-shot cross-modal retrieval, we report
the average results of Recall at 1/5/10 on the MVST dataset. For cross-
modal generation, we report BLEU-2.
Stage 1 Stage 2PropertyRetrieval Generation
CMC CMM KGE KGC S-T T-S S-T
" " " " 0.798 0.432 0.417 0.653
" " " % 0.796 0.437 0.451 0.646
" " % % 0.786 0.306 0.260 0.647
" % % % 0.781 0.275 0.248 0.644
% % " " 0.787 0.395 0.400 0.6505 LIMITATIONS AND BROADER IMPACTS
While our work presents promising results in molecular represen-
tation learning, several areas remain for future exploration: (1) Im-
proving the scale and quality of the pre-training data for molecules
from structured and unstructured knowledge sources. (2) Explor-
ing the scaling laws of foundation models [ 20] by incorporating
large language models (LLMs) [ 46,62] into MV-Mol. (3) Applying
MV-Mol to more biomedical entities such as proteins [ 72], DNA
and RNA sequences [54], and cell transcriptomics [78].
MV-Mol bears promise for accelerating biomedical research.
However, there is a concern that MV-Mol may be misused to gen-
erate potentially dangerous or toxic molecules. Therefore, it is
essential to ensure the responsible and ethical use of the model. We
emphasize that MV-Mol should be employed solely for research
purposes, and any potential medical applications of MV-Mol should
undergo comprehensive experimental evaluations.
6 CONCLUSION
In this paper, we present MV-Mol, a molecular representation learn-
ing model to harness multi-view molecular expertise from struc-
tured and unstructured knowledge sources. To capture the con-
sensus and complementary information across different views, we
propose to model views explicitly with textual prompts. We leverage
a multi-modal fusion architecture to extract view-based molecular
representations, and address the heterogeneity of structured and
unstructured knowledge with varying quality and quantity with a
two-stage pre-training strategy. Through extensive experiments,
we demonstrate the superiority of MV-Mol on molecular property
prediction and cross-modal translation. Under thorough analysis
aimed at safety, MV-Mol has the potential to deliver unprecedented
advancements to the biomedical research community.
ACKNOWLEDGMENTS
This research is supported by the National Key R&D Program of
China (No. 2022YFF1203002) and PharMolix Inc.
 
2090KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
REFERENCES
[1]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation with improved correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65–72.
[2]Oren M Becker, Alexander D MacKerell Jr, Benoit Roux, and Masakatsu Watanabe.
2001. Computational biochemistry and biophysics. Crc Press.
[3]Guoqing Chao and Shiliang Sun. 2016. Consensus and complementarity based
maximum entropy discrimination for multi-view classification. Information
Sciences 367 (2016), 296–310.
[4]Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2023.
Unleashing the potential of prompt engineering in Large Language Models: a
comprehensive review. arXiv preprint arXiv:2310.14735 (2023).
[5]Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. ChemBERTa:
large-scale self-supervised pretraining for molecular property prediction. arXiv
preprint arXiv:2010.09885 (2020).
[6]Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro
Laino, and Matteo Manica. 2023. Unifying molecular and textual representations
via multi-task language modelling. arXiv preprint arXiv:2301.12586 (2023).
[7]Kirill Degtyarenko, Paula De Matos, Marcus Ennis, Janna Hastings, Martin
Zbinden, Alan McNaught, Rafael Alcántara, Michael Darsow, Mickaël Guedj, and
Michael Ashburner. 2007. ChEBI: a database and ontology for chemical entities
of biological interest. Nucleic acids research 36, suppl_1 (2007), D344–D350.
[8]Maxime Delmas, Olivier Filangi, Nils Paulhe, Florence Vinson, Christophe
Duperier, William Garrier, Paul-Emeric Saunier, Yoann Pitarch, Fabien Jour-
dan, Franck Giacomoni, et al .2021. building a Knowledge Graph from public
databases and scientific literature to extract associations between chemicals and
diseases. Bioinformatics 37, 21 (2021), 3896–3904.
[9]Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu
Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv
preprint arXiv:2301.00234 (2022).
[10] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji.
2022. Translation between Molecules and Natural Language. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing. 375–413.
[11] Carl Edwards, ChengXiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal
molecule retrieval with natural language queries. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing. 595–607.
[12] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo
Zhou, Fan Wang, Hua Wu, and Haifeng Wang. 2022. Geometry-enhanced molec-
ular representation learning for property prediction. Nature Machine Intelligence
4, 2 (2022), 127–134.
[13] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo
Chen, Xiaohui Fan, and Huajun Chen. 2023. Mol-Instructions-A Large-Scale
Biomolecular Instruction Dataset for Large Language Models. In The Twelfth
International Conference on Learning Representations.
[14] Yin Fang, Qiang Zhang, Ningyu Zhang, Zhuo Chen, Xiang Zhuang, Xin Shao,
Xiaohui Fan, and Huajun Chen. 2023. Knowledge graph-enhanced molecular
contrastive learning with functional prompt. Nature Machine Intelligence 5, 5
(2023), 542–553.
[15] Kai Guo, Zhenze Yang, Chi-Hua Yu, and Markus J Buehler. 2021. Artificial
intelligence and machine learning in design of mechanical materials. Materials
Horizons 8, 4 (2021), 1153–1172.
[16] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. 2023. A
survey of knowledge enhanced pre-trained language models. IEEE Transactions
on Knowledge and Data Engineering (2023).
[17] W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, and J Leskovec. 2020. Strategies
For Pre-training Graph Neural Networks. In International Conference on Learning
Representations (ICLR).
[18] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. 2022.
Chemformer: a pre-trained transformer for computational chemistry. Machine
Learning: Science and Technology 3, 1 (2022), 015022.
[19] Pengcheng Jiang, Cao Xiao, Tianfan Fu, and Jimeng Sun. 2023. Bi-level Con-
trastive Learning for Knowledge Enhanced Molecule Representations. (2023).
[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[21] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of NAACL-HLT. 4171–4186.
[22] Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gin-
dulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, et al .2016.
PubChem substance and compound databases. Nucleic acids research 44, D1
(2016), D1202–D1213.
[23] Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. 2018. End-to-
End Neural Entity Linking. In Proceedings of the 22nd Conference on Computational
Natural Language Learning. 519–529.[24] Greg Landrum et al .2013. RDKit: A software suite for cheminformatics, compu-
tational chemistry, and predictive modeling. Greg Landrum 8 (2013), 31.
[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Boot-
strapping Language-Image Pre-training with Frozen Image Encoders and Large
Language Models. (2023).
[26] Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and
Qing Li. 2023. Empowering Molecule Discovery for Molecule-Caption Trans-
lation with Large Language Models: A ChatGPT Perspective. arXiv preprint
arXiv:2306.06615 (2023).
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre-
sentation learning with momentum distillation. Advances in neural information
processing systems 34 (2021), 9694–9705.
[28] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji
Kawaguchi, Tat-Seng Chua, and Qi Tian. 2024. Towards 3D Molecule-Text
Interpretation in Language Models. arXiv preprint arXiv:2401.13923 (2024).
[29] Yingming Li, Ming Yang, and Zhongfei Zhang. 2018. A survey of multi-view
representation learning. IEEE transactions on knowledge and data engineering 31,
10 (2018), 1863–1883.
[30] Pengfei Liu, Yiming Ren, and Zhixiang Ren. 2023. Git-mol: A multi-modal large
language model for molecular science with graph, image, and text. arXiv preprint
arXiv:2308.06911 (2023).
[31] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu,
Jian Tang, Chaowei Xiao, and Animashree Anandkumar. 2023. Multi-modal mol-
ecule structure–text model for text-based retrieval and editing. Nature Machine
Intelligence 5, 12 (2023), 1447–1457.
[32] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and
Jian Tang. 2021. Pre-training Molecular Graph Representation with 3D Geometry.
InInternational Conference on Learning Representations.
[33] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie
Tang. 2021. Self-supervised learning: Generative or contrastive. IEEE transactions
on knowledge and data engineering 35, 1 (2021), 857–876.
[34] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
Jie Tang. 2023. GPT understands, too. AI Open (2023).
[35] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang
Wang, and Tat-Seng Chua. 2023. MolCA: Molecular Graph-Language Modeling
with Cross-Modal Projector and Uni-Modal Adapter. In The 2023 Conference on
Empirical Methods in Natural Language Processing.
[36] Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang,
and Tie-Yan Liu. 2023. MolXPT: Wrapping Molecules with Text for Generative
Pre-training. arXiv preprint arXiv:2305.10688 (2023).
[37] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.
2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics. 4969–4983.
[38] Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization.
InInternational Conference on Learning Representations.
[39] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. 2023. Molfm:
A multimodal molecular foundation model. arXiv preprint arXiv:2307.09484
(2023).
[40] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing
Nie. 2023. Biomedgpt: Open multimodal generative pre-trained transformer for
biomedicine. arXiv preprint arXiv:2308.09442 (2023).
[41] Runze Ma, Yidan Zhang, Xinye Wang, Zhenyang Yu, and Lei Duan. 2022. MORN:
Molecular Property Prediction Based on Textual-Topological-Spatial Multi-View
Learning. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management. 1461–1470.
[42] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. 2018. UMAP:
Uniform Manifold Approximation and Projection. Journal of Open Source Software
3, 29 (2018), 861.
[43] David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition
and classification. Lingvisticae Investigationes 30, 1 (2007), 3–26.
[44] Feng Nie, Meixi Chen, Zhirui Zhang, and Xu Cheng. 2022. Improving few-shot
performance of language models via nearest neighbor calibration. arXiv preprint
arXiv:2212.02216 (2022).
[45] OpenAI. 2022. Introducing chatgpt. https://doi.org/blog/chatgpt
[46] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computational Linguistics. 311–318.
[48] Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu,
Shufang Xie, Tao Qin, and Rui Yan. 2024. BioT5+: Towards Generalized Biological
Understanding with IUPAC Integration and Multi-task Tuning. arXiv preprint
arXiv:2402.17810 (2024).
[49] Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce
Xia, and Rui Yan. 2023. BioT5: Enriching Cross-modal Integration in Biology with
Chemical Knowledge and Natural Language Associations. In Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing . 1102–1123.
 
2091Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge KDD ’24, August 25–29, 2024, Barcelona, Spain
[50] Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie
Huang, Maosong Sun, and Jie Zhou. 2021. ERICA: Improving Entity and Relation
Understanding for Pre-trained Language Models via Contrastive Learning. In
Proceedings of the 59th Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers). 3350–3363.
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748–8763.
[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al .[n. d.].
Improving language understanding by generative pre-training. ([n. d.]).
[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485–5551.
[54] Guillaume Richard, Bernardo P de Almeida, Hugo Dalla-Torre, Christopher Blum,
Lorenz Hexemer, Priyanka Pandey, Stefan Laurent, Marie P Lopez, Alexander
Laterre, Maren Lang, et al .2024. ChatNT: A Multimodal Conversational Agent
for DNA, RNA and Protein Tasks. bioRxiv (2024), 2024–04.
[55] Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Günter Klambauer. 2023. Enhanc-
ing activity prediction models in drug discovery with the ability to understand
human language. arXiv preprint arXiv:2303.03363 (2023).
[56] Gregory Sliwoski, Sandeepkumar Kothiwale, Jens Meiler, and Edward W Lowe.
2014. Computational methods in drug discovery. Pharmacological reviews 66, 1
(2014), 334–395.
[57] Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian
Dallago, Stephan Günnemann, and Pietro Liò. 2022. 3d infomax improves gnns for
molecular property prediction. In International Conference on Machine Learning.
PMLR, 20479–20502.
[58] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun,
Zhiwu Lu, and Ji-Rong Wen. 2022. A molecular multimodal foundation model as-
sociating molecule graphs with natural language. arXiv preprint arXiv:2209.05481
(2022).
[59] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang,
Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2023. Generative
pretraining in multimodality. arXiv preprint arXiv:2307.05222 (2023).
[60] Neil Swainston, Janna Hastings, Adriano Dekker, Venkatesh Muthukrishnan,
John May, Christoph Steinbeck, and Pedro Mendes. 2016. libChEBI: an API for
accessing the ChEBI database. Journal of Cheminformatics 8 (2016), 1–6.
[61] Paolo Tosco, Nikolaus Stiefl, and Gregory Landrum. 2014. Bringing the MMFF
force field to the RDKit: implementation and validation. Journal of cheminfor-
matics 6 (2014), 1–4.
[62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[63] Murat Ugurlucan, Ilker M Caglar, Fatma N Turhan Caglar, Sedat Ziyade, Oguzhan
Karatepe, Yahya Yildiz, Ertugrul Zencirci, Funda Gungor Ugurlucan, Ahmet
H Arslan, Semra Korkmaz, et al .2012. Aspirin: from a historical perspective.
Recent Patents on Cardiovascular Drug Discovery (Discontinued) 7, 1 (2012), 71–76.
[64] Yue Wan, Jialu Wu, Tingjun Hou, Chang-Yu Hsieh, and Xiaowei Jia. 2023. From
molecules to scaffolds to functional groups: building context-dependent molecular
representation via multi-channel learning. arXiv preprint arXiv:2311.02798 (2023).
[65] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu,
Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embed-
ding and pre-trained language representation. Transactions of the Association for
Computational Linguistics 9 (2021), 176–194.
[66] Xintao Wang, Qianyu He, Jiaqing Liang, and Yanghua Xiao. 2022. Language
models as knowledge embeddings. arXiv preprint arXiv:2206.12617 (2022).
[67] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022.
Molecular contrastive learning of representations via graph neural networks.
Nature Machine Intelligence 4, 3 (2022), 279–287.
[68] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,
Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt
pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint
arXiv:2302.11382 (2023).
[69] David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Ja-
son R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al .2018.
DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic acids
research 46, D1 (2018), D1074–D1082.
[70] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Ge-
niesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: a
benchmark for molecular machine learning. Chemical science 9, 2 (2018), 513–530.
[71] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions.[72] Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. 2023. Protst: Multi-
modality learning of protein sequences and biomedical texts. In International
Conference on Machine Learning. PMLR, 38749–38767.
[73] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems 33 (2020), 5812–5823.
[74] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A deep-learning
system bridging molecule structure and biomedical text with comprehension
comparable to human professionals. Nature communications 13, 1 (2022), 862.
[75] Zheni Zeng, Bangchen Yin, Shipeng Wang, Jiarui Liu, Cheng Yang, Haishen Yao,
Xingzhi Sun, Maosong Sun, Guotong Xie, and Zhiyuan Liu. 2023. Interactive
Molecular Discovery with Natural Language. arXiv preprint arXiv:2306.11976
(2023).
[76] Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin
Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. 2021. OntoProtein: Protein
Pretraining With Gene Ontology Embedding. In International Conference on
Learning Representations.
[77] Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Ling-
peng Kong, and Qi Liu. 2024. Gimlet: A unified graph-text model for instruction-
based molecule zero-shot learning. Advances in Neural Information Processing
Systems 36 (2024).
[78] Suyuan Zhao, Jiahuan Zhang, Yizhen Luo, Yushuai Wu, and Zaiqing Nie. 2024.
LangCell: Language-Cell Pre-training for Cell Identity Understanding. arXiv
preprint arXiv:2405.06708 (2024).
[79] Qinghai Zheng, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, and Chen Li. 2023. Com-
prehensive multi-view representation learning. Information Fusion 89 (2023),
198–209.
[80] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei
Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-Mol: a universal 3D molecular
representation learning framework. (2023).
[81] Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang Zhou,
Houqiang Li, and Tie-Yan Liu. 2022. Unified 2d and 3d pre-training of molecular
representations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2626–2636.
[82] Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Wengang Zhou, Tao Qin,
Houqiang Li, and Tie-Yan Liu. 2023. Dual-view molecular pre-training. In Pro-
ceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 3615–3627.
A DETAILS OF MV-MOL
The representation similarity simtri(·,·)in Eq. 11 is calculated as
follows:
simtri(𝑧(ℎ,𝑟),𝑧(𝑡))
= 
max𝑖=1,2,···,𝐾n
𝑓proj(𝑧(ℎ,𝑟)
𝑖)𝑇𝑓proj(𝑧(𝑡)
1)o
,ℎ∈𝐸mol,𝑡∈𝐸text,
max𝑖,𝑗=1,2,···,𝐾n
𝑓proj(𝑧(ℎ,𝑟)
𝑖)𝑇𝑓proj(𝑧(𝑡)
𝑗)o
,ℎ,𝑡∈𝐸mol,
𝑓proj(𝑧(ℎ,𝑟)
1)𝑇𝑓proj(𝑧(𝑡)
1),ℎ,𝑡∈𝐸text
(A1)
B PRE-TRAINING DATA COLLECTION
Following [ 58,74], we perform named entity recognition (NER)
[43] and entity linking (EL) [ 23] to obtain molecule-text pairs from
biomedical literature. Specifically, we first sample 321K frequently
accessed molecules from PubChem [ 22]. Then we identify mentions
of these molecules within 3.5M publications from S2ORC [ 37]. We
start with the corresponding sentence of the mention and randomly
extend the context by the previous or next sentence within the
paragraph until the text snippet exceeds 256 tokens. Consequently,
we collect 12M molecule-text pairs for 60K molecules.
We also collect a knowledge graph for molecules by combining
public databases including PubChem [ 22], CheBI [ 7], DrugBank
[69] and FORUM [ 8]. We first extract 196K molecules from these
databases and perform deduplication. Then, we collect the connec-
tions between these molecules and other entities as follows:
 
2092KDD ’24, August 25–29, 2024, Barcelona, Spain Yizhen Luo et al.
Table A1: Statistics of the collected knowledge graph. The left column
denotes the entity or relation type, and the right column denotes
the number of entities and relations.
Entities
Molecules 196,454
Targets 403
Ontologies 22,076
Diseases 2,835
Properties 51,882
All 273,650
Relations
Molecule-Target Interaction 23,870
Molecule-Ontology Connection 252,555
Molecule-Disease Association 172,241
Molecule-Physical Property 6,166
Molecule-Chemical Property 38,206
Molecule-Pharmacokinetic Property 7,510
Molecule-Molecule Connection 119,793
Ontology-Ontology Connection 22,706
All 643,047
•We collect drug property descriptions that span diverse views
from PubChem. We treat each description as a unique node
and connect it with the corresponding molecule.
•We collect drug targets, drug enzymes, drug carriers, and
drug transporters from DrugBank. We also collect Bind-
ingDB compounds and protein targets with 𝐾𝑖≤10𝑛𝑀.
•We collect drug-disease and drug-ontology connections from
FORUM with q_value <10−6.
•We collect drug-ontology and ontology-ontology connec-
tions from CheBI with libCheBI toolkit [60].
In total, the knowledge graph comprises 273K entities and 643K
relationships. Table A1 presents the overall statistics of the knowl-
edge graph. Following [ 80], we calculate 3D conformations by the
MMFF algorithm [ 61] within the RDKit [ 24] package for molecules.
C BENCHMARK DATASETS
C.1 Molecular Property Prediction
We adopt 8 classification datasets for molecular property prediction,
including BBBP (Blood-Brain Barrier Penetration), Tox21 (Toxicol-
ogy in the 21st Century), ToxCast (Toxicity), SIDER (Side Effects
Resource), ClinTox (Clinical Trial Outcomes and Toxicity), MUV
(Maximum Unbiased Validation), HIV (HIV inhibition), and BACE
(Human𝛽-secretase 1 Binding).
C.2 Cross-Modal Retrieval
For cross-modal retrieval, we incorporate the following datasets:
PCDes. The original dataset [ 74] comprises 15K molecules and
their descriptions in CheBI. We filter out 3,888 molecules that have
appeared in our pre-training dataset. Unlike the original experimentsetting, we adopt the Scaffold split with a train/validation/test ratio
of 7/1/2 and perform zero-shot retrieval on the test set with the
entire paragraph.
MVST. This dataset is introduced as part of our work. Different
from PCDes, each molecule within MVST corresponds to multi-
ple descriptive texts, distinguished by a given view. The dataset
primarily focuses on 3 types of views:
•The chemical view. The data for this view is directly de-
rived from the CheBI description of molecules within the
PubChem database.
•The physical view. We assemble multiple columns within
PubChem including the general physical description, color,
form, order, boiling point, melting point, flashing point, sol-
ubility, density, vapor pressure, stability (shelf life), and de-
composition. The text is organized as ’COLUMN_NAME :
COLUMN_DESCRIPTION ’. We randomly sample rows if the
assembled text exceeds 256 tokens.
•The pharmacokinetic view. We also incorporate multiple
columns from PubChem including drug indication, disposal
methods, drug-food interactions, toxicity summary, metabo-
lism, therapeutic uses, inhalation systems, absorption, dis-
tribution, exertion, biological half-life, adverse effects, and
carcinogenicity. The organization of the text and length con-
trol follows the same as the physical view.
Following previous works [ 10,74], we replace the name of the
molecule within texts with ’the molecule ’. After collecting data
from each view, we sample molecules with textual descriptions
from 2 or more views and partition them by Scaffold split with a
train/validation/test ratio of 7/1/2.
C.3 Cross-Modal Generation
We incorporate the CheBI-20 dataset, a widely adopted dataset
collected from the CheBI database. We follow the original random
split with a train/validation/test ratio of 8/1/1.
D DOWNSTREAM EXPERIMENT DETAILS
We manually write descriptions for each dataset in MoleculeNet as
view prompts to improve molecular representations. Specifically:
•BBBP. The prompt is ’blood-brain barrier penetration (perme-
ability)’.
•Tox21. The prompt is ’Qualitative toxicity measurements in-
cluding nuclear receptors and stress response pathways’.
•ToxCast. The prompt is ’Qualitative toxicity measurements’.
•SIDER. The prompt is ’adverse drug reactions (ADR) for 27
system organ classes’.
•ClinTox. The prompt is ’Qualitative data of drugs if they
failed clinical trials for toxicity reasons’.
•MUV. The prompt is ’Subset of PubChem BioAssay designed
for validation of virtual screening techniques’.
•HIV. The prompt is ’Experimentally measured abilities to
inhibit HIV replication’.
•BACE. The prompt is ’Binding results for human 𝛽-secretase
1 (BACE-1)’.
 
2093