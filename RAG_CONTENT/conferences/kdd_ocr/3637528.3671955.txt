Unsupervised Alignment of Hypergraphs with Different Scales
Manh Tuan Do
KAIST
Seoul, Republic of Korea
manh.it97@kaist.ac.krKijung Shin
KAIST
Seoul, Republic of Korea
kijungs@kaist.ac.kr
Abstract
People usually interact in groups, and such groups may appear on
different platforms. For instance, people often create various group
chats on messaging apps (e.g., Facebook Messenger and WhatsApp)
to communicate with families, friends, or colleagues. How do we
identify the same people across the two platforms based on the
information about the groups? This gives rise to the hypergraph
alignment problem, whose objective is to find the correspondences
between the sets of nodes of two hypergraphs. In a hypergraph, a
node represents a person, and each hyperedge represents a group
of several people. In addition, the two sets of hyperedges in the
two hypergraphs can vary significantly in scales as people may use
different apps at different time periods.
In this work, we propose and tackle the problem of unsupervised
hypergraph alignment. Given two hypergraphs with potentially dif-
ferent scales and without any side information or prior ground-truth
correspondences, we develop HyperAlign, a learning framework,
to find node correspondences across the two hypergraphs. Hyper-
Align directly addresses each challenge of the problem. In particu-
lar, it (a) extracts node features from the hypergraph topology, (b)
employs contrastive learning, as a “supervised pseudo-alignment”
task to pre-train the learning model (c) applies topological aug-
mentation to help a generative adversarial network to align the
two embedding spaces from the two hypergraphs. The purpose of
augmentation is to add virtual hyperedges from one hypergraph in
order to the other to resolve the scale difference and share infor-
mation across the two hypergraphs. Our extensive experiments on
12real-world datasets demonstrate the significant and consistent
superiority of HyperAlign over the baseline approaches.
CCS Concepts
•Information systems →Data mining; Social networks.
Keywords
Hypergraph Alignment; Network Alignment; Scale Difference
ACM Reference Format:
Manh Tuan Do and Kijung Shin. 2024. Unsupervised Alignment of Hy-
pergraphs with Different Scales. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.367195525–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671955
1 Introduction
How can we identify the same users across messaging apps such
as Facebook Messenger and WhatsApp based on the information
of the participants of the group chats? The problem of identify-
ing the user correspondences, across two sets of user groups, also
naturally arises from many other practical scenarios. Multiple sets
of user groups may appear on different platforms while involving
intrinsically similar sets of users. For example, researchers may list
their publication records, each of which is a group of co-authors,
on both Google Scholar and Research Gate. In another example,
users register accounts and engage in discussion threads, each
of which is a group of participating users, on multiple online fo-
rums. Establishing user correspondences across the sets of user
groups enables several applications1, including author name dis-
ambiguation [ 37] and identification of the right target users for
recommending friends or products [ 51]. It is often the case that
apart from the list of anonymized participants, no side information
is available due to privacy reasons. Furthermore, the two sets of
user groups can be very different in both scales and characteristics
as users may use one platform for a short period in the past then
switch to another platform and keep using it until the present.
This task is formulated as the hypergraph alignment problem,
where the objective is to find node correspondences between the
two node sets in two hypergraphs. Hypergraphs, which are the
general modeling of networks allowing multiple nodes to be in
the same group connection, naturally represent the group inter-
actions that are pervasive in practice [ 7,14,16,33,36,41]. In the
above scenarios, each node represents a user, and a hyperedge con-
stitutes a group of users sharing a group interaction. Apart from
social networks, hypergraph alignment has also proven applicable
in computer vision [ 34], chemistry [ 39], and biological research [ 30].
While one may assume the existence of node features and some
ground-truth node correspondences beside the given topologies,
these statistics might be unreliable and costly to obtain. Thus, in
this work, we assume an unsupervised setting where no side infor-
mation is known, and we are the first to tackle the unsupervised
hypergraph alignment problem relying solely on the topologies of
the two given hypergraphs.
Despite reflecting realistic scenarios and possessing potential
applications, the proposed problem poses a number of challenges:
(C1) The group interactions intrinsic to hypergraphs represent
complex information that cannot be fully captured by common
pairwise relationships [ 16,58],(C2) Due to privacy concerns, side
information of users, as node features, is not available, rendering
1To preserve privacy, this should only be performed upon users’ consent with proper
privacy protection techniques
 
609
KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
it difficult to directly apply any learning-based methods, (C3) No
ground-truth node correspondences are available prior to training,
so it is mandatory to predict the correspondences for all nodes
simultaneously without any guidance even from a small subset of
nodes, and (C4) One hypergraph might be much larger and contain
hyperedges different from the other. Due to this disparity, it is not
straightforward to generalize from one hypergraph to another to
devise an alignment scheme.
For a specific case of hypergraphs, the graph alignment prob-
lem [ 49], which is the task of finding node correspondences in
two (or more) pair-wise graphs, has been well studied. The graph
alignment task has demonstrated various practical implications
in analyzing chemical compounds [ 6,50], semantic matching in
webs and databases [ 17,38], and entity matching in computer vi-
sion [ 10,53]. Recognizing such potential applications, numerous
methods have been proposed to tackle the alignment problem for
unipartite and bipartite graphs [5, 12, 13, 15, 20, 22, 25, 31].
One may represent each hypergraph as a unipartite graph, e.g.,
deriving the clique expansion, or a bipartite graph, e.g., by star
expansion (see Section 3.1), and apply these methods for the hyper-
graph alignment problem. However, they possess key limitations
when applied to our problem. Firstly, they are designed for the pair-
wise topology of graphs, but representing hyperedges by pair-wise
edges via the clique expansion always incurs information loss. In
addition, one hypergraph could have a much larger set of hyper-
edges compared with that of the other hypergraph. As a result,
any methods that rely on the star expansion and attempt to align
the respective hyperedge sets, which are node sets in the bipartite
graph representation, are likely to predict incorrectly, which in turn
leads to inaccurate predictions of node alignments.
To address the unsupervised hypergraph alignment task with
the aforementioned challenges, we introduce our method, Hyper-
Align, which makes no assumptions about the grouth-truth node
correspondences or node attributes and only leverages the hyper-
graph topology in a learning framework. HyperAlign first extracts
node features by properly extending struc2vec [46] to the hyper-
graph setting to address Challenges C1&C2.HyperAlign then
applies a Hypergraph Neural Network (HNN) to aggregate informa-
tion and learn the node embeddings via contrastive learning. This
contrastive learning acts as a supervised pseudo-alignment task
for pre-training the HNN to alleviate C3. After that, the method
applies a Generative Adversarial Network (GAN) to bridge the two
embedding spaces respective to the two hypergraphs. In order to
resolve the scale and information discrepancy, during the course
of training GAN, the topology of each hypergraph is augmented
based on the hyperedges of the counter-part hypergraph. This topo-
logical augmentation module addresses C4by affording the two
hypergraphs the same scale. Finally, the final node embeddings are
employed to predict node alignments across the two hypergraphs.
In short, our contributions in this research are three-fold:
•New Problem: We formulate and tackle the problem of unsu-
pervised hypergraph alignment with unique challenges for the
first time, to the best of our knowledge.
•Novel Method: We propose our method, HyperAlign, based
on three novel ideas: topological feature extraction, contrastive
learning as a proxy task, and topological augmentation.•Extensive Experiments: We introduce various competitors and
variants of HyperAlign and perform experiments to demonstrate
a consistent and significant superiority of our method over those
baseline approaches.
Forreproducibility, the code and datasets are available at https:
//github.com/manhtuando97/HyperAlign.
The remaining sections of this paper are as follows: In Section 2,
we review some relevant work about hypergraphs and network
alignment. We introduce some preliminaries and formulate our
problem in Section 3. Our method is proposed in Section 4 and
evaluated in Section 5. Finally, we conclude our work in Section 6.
2 Related Work
Hypergraphs: Hypergraphs model group interactions in various
domains, such as research coauthorship, email records, online fo-
rums, or prescription labelling [ 7,16,57]. Most empirical studies
on hypergraphs focus on investigating individual real-world hy-
pergraphs for their structural patterns [ 7,16,28] and temporal
properties [ 8,24]. With the prevalence of representing datasets
as hypergraphs, there has been a growing interest in addressing
learning problems on hypergraphs, such as link prediction [ 26,55],
node classification [19, 27, 54], and clustering [2, 9, 64].
Supervised Network Alignment: Supervised network alignment
aims at identifying node correspondences across graphs given
that partial ground-truth correspondences are available in advance.
Based on the given correspondences and the graph topologies, the
task is to infer the correspondences of the remaining nodes. For
instance, COSNET [ 62] predicts the node correspondences that cap-
ture the neighborhood consistency at both the node and the graph
levels. On the other hand, CrossMNA [13],IONE [35],SNNA [31],
MINING [ 63], and cM2NE [ 52] leverage the given node correspon-
dences to learn the embeddings of the remaining nodes, which are
then employed for predicting correspondences. Some work targets
the alignment of hypergraphs assuming the existence of some prior
node correspondences [51] or hyperedge correspondences [60].
Unsupervised Network Alignment: As obtaining ground-truth
node correspondences is often difficult in various practical scenar-
ios, a line of work has tackled the problem of unsupervised graph
alignment. Several frameworks [ 5,25,61] assume the set of nodes
in one graph is a permutation of that of the other graph, formulate
graph alignment as an optimization problem, and find the optimal
solution to the problem as such permutation. Big-Align [25] also
employs an optimization approach to align two bipartite graphs.
On the other hand, REGAL [22] aggregates the degree informa-
tion of neighbors to compute a low-rank representation for each
node, which are used to predict correspondences. Following GAN-
based approaches, UUIL [32],DANA [15], and WAlign [20] employ
graph encoders to learn the node embeddings that minimize the
distance between the two embedding spaces of the two graphs.
Once the two embedding spaces are aligned, such embeddings are
utilized for matching nodes across the two graphs. Meanwhile,
Grad-Align+ [ 42] and SANA [ 43] train graph encoders with the aid
of augmentation to the node features or graph topology.
These existing methods are designed for the alignment of pair-
wise graphs, while our proposed method, HyperAlign, is the first to
address the unsupervised hypergraph alignment problem with the
 
610Unsupervised Alignment of Hypergraphs with Different Scales KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Frequently used symbols.
Symb
ols Definition
G𝑖=(V𝑖,E𝑖)hyp
ergraph with node set V𝑖and hyperedge set E𝑖
H𝑖incidence
matrix of G𝑖
𝑚Ethe
maximum size of a hyperedge in E
𝐺[𝑙]𝑙-le
vel weighted complete graph of G
X𝑖no
de feature matrix of G𝑖
x𝑖,𝑗 featur
e vector of the 𝑗-th node of G𝑖
¯C𝑖=(¯G𝑖,¯X𝑖)the
1st corrupted view of G𝑖
ˆC𝑖=(ˆG𝑖,ˆX𝑖)the
2nd corrupted view of G𝑖
Φ hyp
ergraph encoder
Z(𝑘)no
de vector matrix in layer (𝑘)
Q(𝑘)hyp
eredge vector in layer (𝑘)
¯Z𝑖and ˆZ𝑖no
de embedding matrices of ¯G𝑖and ˆG𝑖
∥𝑥∥𝐿2-norm
of vector𝑥
𝑠(𝑥,𝑦)cosine
similarity of vectors 𝑥and𝑦
𝜓 mapping
function as a neural network
G(.)generator
D
(.)discriminator
Z𝑖final
node embedding matrix of G𝑖
z𝑖,𝑗 emb
edding vector of the 𝑗-th node of G𝑖
entailed challenges. Methodologically, HyperAlign incorporates
novel ideas, including contrastive learning as a pseudo-alignment
task and topological augmentation for addressing scale differences.
The details of HyperAlign are outlined in Section 4.
3 Preliminaries & Problem Definition
3.1 Basic Concepts
In this section, we introduce some basic concepts. The key notations
can be found in Table. 1.
Hypergraphs: Ahypergraph is defined as G=(V,E), where V=
{𝑣1,...,𝑣|V|}is the set of nodes, and E={𝑒1,...,𝑒|E|}⊆2Vis the set
of hyperedges. Each hyperedge𝑒⊆Vis a set of|𝑒|(≥2) nodes.2.
The matrix H∈{0,1}|V|×|E|in which H[𝑖][𝑗]=1if𝑣𝑖∈𝑒𝑗and0
otherwise is the incidence matrix ofG. Denote𝑚Eas the maximum
size of a hyperedge in E, i.e.,𝑚E=max𝑒∈E|𝑒|. A node having no
incident hyperedges is an isolated node.
Clique Expansion: The clique expansion of hypergraph Gis a
graph `𝐺=(V,`𝐸), where `𝐸={{𝑢,𝑣}|𝑢,𝑣∈V,∃𝑒∈E,{𝑢,𝑣}⊆𝑒}.
That is, `𝐺is a graph in which two nodes 𝑢,𝑣∈Vare adjacent if
and only if there exists a hyperedge 𝑒inEcontaining both 𝑢and𝑣.
Star Expansion: Thestar expansion of hypergraph Gis a bipartite
graph ´𝐺=(V,U,´𝐸), where U=E,´𝐸={{𝑣,𝑢}|𝑣∈V,𝑢∈E,𝑣∈𝑢}.
That is, ´𝐺is a bipartite graph in which the first node set is Vand
the second node set is E, and𝑣∈Vand𝑢∈Uare adjacent if and
only if𝑣is incident to 𝑢inG.
Hypergraph Neural Networks (HNNs): HNNs are powerful tools
for learning node representations in hypergraphs. Recent HNNs [ 18,
19,23,56] commonly employ a two-step message-passing scheme:
node-to-hyperedge and hyperedge-to-node aggregations. They re-
cursively transform the representation vectors of hyperedges by
2In this study, for the sake of simplicity, we choose to exclude self-loops (i.e., hyperedges
of size 1) as they are not significantly helpful for alignment. Refer to the Online
Appendix [1] for the inspection of self-loopsaggregating information from their incident nodes and update the
representation vectors of nodes by aggregating information from
the incident hyperedges. Given Z(𝑘)∈R|V|×𝐹andQ(𝑘)∈R|E|×𝐹,
where𝐹is the embedding dimension, as the node and hyperedge
representations at the 𝑘-layer of HNNs, the representation vectors
at the(𝑘+1)-layer are computed as follows:
𝑞(𝑘+1)
𝑗=S(𝑘+1)
V→E
𝑞(𝑘)
𝑗,{𝑧(𝑘)
𝑖:𝑣𝑖∈𝑒𝑗}
,
𝑧(𝑘+1)
𝑖=S(𝑘+1)
E→V
𝑧(𝑘)
𝑖,{𝑞(𝑘+1)
𝑗:𝑣𝑖∈𝑒𝑗}
,(1)
whereS(𝑘+1)
V→EandS(𝑘+1)
E→Vdenote the node-to-hyperedge and hyper-
edge-to-node aggregate functions at the (𝑘+1)-layer, respectively,
and they often include multi-layer perceptrons for projection.
Contrastive Learning (CL): CL has proven effective in node rep-
resentation learning on (hyper)graphs [ 27,59]. The key idea is to
create two corrupted views ¯Cand ˆCof the same network Gvia
perturbations of attributes and topologies and train an encoder to
maximize the agreement between node embeddings from ¯Cand ˆC.
Generative Adversarial Networks (GAN): GANs have been ap-
plied to tackle the graph alignment problem [ 15,20,32]. The discrim-
inator is trained to distinguish between the two node embedding
distributions, and the generator is trained to produce the node em-
beddings that minimize such distance. Once the node embeddings
are learned, each node is predicted to correspond with the node
having the most similar embedding in the counterpart graph.
3.2 Problem Definition
In this section, we define the problem and discuss the challenges.
Problem 1 (Unsupervised Hypergraph Alignment).
•Given: two hypergraphs G1=(V1,E1)andG2=(V2,E2), where
|V1|≤|V2|, without loss of generality,
•Find: a correspondenceA⊂ V1×V2between V1andV2, where
|A|=min(|V1|,|V2|)=|V1|,
•to Maximize: the number of correct alignments, i.e., |A∩C| ,
whereC⊂ V1×V2is the ground-truth node correspondences.
Unique Challenges: This problem poses a number of unique chal-
lenges. (C1) The group interactions intrinsic to hypergraphs render
any graph alignment methods improper as representing a hyper-
graph as a pair-wise graph always incurs information loss [ 16,58].
(C2) No node attributes are available. This absence reflects the
privacy protection in practice as the side information of users of-
ten remains strictly undisclosed. (C3) The problem assumes an
unsupervised setting, (i.e., there are no prior ground-truth node
correspondences or hyperedge correspondences). As a result, one
needs to rely solely on the topologies of the hypergraphs to predict
the correspondences for all nodes. (C4) Finally, the two respective
hyperedge sets might be substantially different in sizes. Therefore,
any attempts to represent the hypergraphs as bipartite graphs and
align the two bipartite graphs, i.e., aligning nodes and hyperedges
simultaneously, may risk being misled by incorrect predictions of
hyperedge alignments.
4 Proposed Method: HyperAlign
4.1 Overview (Figure 1 and Algorithm 1)
In this section, we provide an overview of our method HyperAlign
(see Algorithm 1), designed for addressing Problem 1 and its unique
 
611KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
Shared 
WeightsEncoder
𝜱𝜱
Encoder
𝜱𝜱Shared 
WeightsEncoder
𝜱𝜱
Encoder
𝜱𝜱�𝒁𝒁𝟏𝟏
�𝒁𝒁𝟏𝟏
�𝒁𝒁𝟐𝟐
�𝒁𝒁𝟐𝟐Shared 
WeightsEncoder
𝜱𝜱
Encoder
𝜱𝜱
Mapping 𝝍𝝍𝒁𝒁𝟏𝟏
𝒁𝒁𝟐𝟐𝒇𝒇𝜽𝜽�𝑪𝑪𝟏𝟏
�𝑪𝑪𝟏𝟏
�𝑪𝑪𝟐𝟐
�𝑪𝑪𝟐𝟐𝑰𝑰𝟏𝟏
𝑰𝑰𝟐𝟐𝑰𝑰𝟐𝟐′
𝑰𝑰𝟏𝟏′Alignment PredictionContrast
Node Feature Masking
Membership MaskingSimilarity ComputationStep 4
Alignment Prediction
(𝐆𝐆𝟏𝟏,𝐗𝐗𝟏𝟏)Step 1
Feature Extraction
HyperFeatStep 2
Contrastive LearningStep 3              
Augmentation                                       GAN Training         
Original 
Incidence 
MatrixAugmented
Incidence 
Matrix
(𝑮𝑮𝟐𝟐,𝑿𝑿𝟐𝟐)Contrast𝐆𝐆𝟏𝟏
𝑮𝑮𝟐𝟐HyperFeatUpdate
Figure 1: Illustration of HyperAlign. In Step 1, node features are extracted from the topologies. For each hypergraph,
HyperAlign performs contrastive learning, i.e., “supervised pseudo alignment", between two corrupted views in Step 2. In
Step 3, an adversarial framework is employed to align the two node embedding spaces of two augmented hypergraphs. Upon
obtaining embeddings, each node is predicted to correspond to the node having the most similar embeddings in Step 4.
challenges with three novel components: hypergraph feature ex-
traction, application of contrastive learning as a proxy task, and
topological augmentation.
HyperAlign utilizes the topology of each hypergraph to extract
node features, which is specialized to hypergraphs, to tackle Chal-
lenges C1&C2(Step 1). Then, it employs a Hypergraph Neural
Network (HNN) to output the node embeddings that are helpful
for alignment. For C3,HyperAlign first pre-trains the HNN pa-
rameters via a node contrastive learning task, which serves as a
“pseudo-alignment task" (Step 2). To this end, the method follows
an adversarial framework to align the two node embedding spaces,
assisted by topology augmentation (Step 3). The topological aug-
mentation module is proposed to afford the two input hypergraphs
the same scale, which addresses C4. Finally, HyperAlign predicts
the node alignments based on the similarities of the learned node
embeddings (Step 4). Figure 1 illustrates the steps of HyperAlign.
4.2 Step 1: HyperFeat- Feature Extraction
Given two hypergraphs G1andG2without any side information,
the first step of HyperAlign, HyperFeat, extracts node features
purely based on the topologies of G1andG2. The node features
serve as the input for the subsequent steps. HyperFeat is a proper
adaption of struc2vec [46] specialized to hypergraphs, which aims
to alleviate Challenges C1&C2of Problem 1. HyperFeat obtains
node features that preserve structural similarities, and the structural
similarities are defined based on the number of incident hyperedges
of different sizes, which are unique to hypergraphs.
In particular, nodes that are structurally similar are afforded
similar node features by HyperFeat. The key idea for HyperFeat
is to conduct a random walk to generate a corpus, a sequence of
nodes, and apply a language model to extract the node features
from the corpus. Structurally similar nodes tend to appear close to
each other in the corpus.
To achieve that, HyperFeat constructs𝑇level graphs from the
original hypergraph, each of which is a weighted undirected com-
plete graph. In the 𝑘-level graph, 𝑘=0,...,𝑇−1, the weight ofAlgorithm 1: Overview of HyperAlign
Problem Input: hypergraphs G1=(V1,E1)andG2=(V2,E2)
Method Input: (a) number of contrastive learning iterations Γcl,
(b) number of GAN training iterations Γg,
(c) number of discriminator training iterations Γd,
(d) number of reconstruction function training iterations Γr
Output: predicted node correspondences A⊂ V2×V2
1/* Step 1: HyperFeat - Feature Extraction */
2Extract X𝑖from G𝑖,𝑖=1,2as in Section 4.2
3/* Step 2: HyperCL - Contrastive Learning */
4foriter=1,...,Γcldo
5 for𝑖=1,2do
6 Create corrupted views ¯C𝑖,ˆC𝑖from(G𝑖,X𝑖)
7 Update parameters of Φto minimizeL𝑖
CL(Eq. (5))
8/* Step 3: HyperAug - GAN Training with Augmentation */
9˜G1←G1,˜G2←G2
10forepoch =1,...,Γgdo
11 Z1,Z2=Φ((˜G1,X1)),𝜓(Φ(˜G2,X2))
12 foriter=1,...,Γddo
13 TrainDto approximate Wasserstein distance of Z1andZ2
14 foriter=1,...,Γrdo
15 Update parameters of r1andr2to minimizeLre(Eq. (7))
16 Update parameters of Φ,𝜓to minimizeLtotal(Eq. (8))
17 Augment ˜G1,˜G2accordingly to Algorithm. 2
18/* Step 4: Alignment Prediction */
19A←∅
20for𝑖=1,...,|V1|do
21𝑗=argmax𝑗=1,...,|V2|𝑠(z1,𝑖,z2,𝑗)
22 Predict𝑣′
𝑗∈V2={𝑣′
1,...,𝑣′
|V2|}as aligned with 𝑣𝑖∈V1
23A←A∪{( 𝑣𝑖,𝑣′
𝑗)}
24returnA
the edge connecting nodes 𝑢and𝑣is inversely proportional to the
structural difference in the 𝑘-hop neighborhoods of 𝑢and𝑣. In other
words, the more similar the two neighborhoods are, the greater
the weight is. HyperFeat then conducts a random walk on these 𝑇
 
612Unsupervised Alignment of Hypergraphs with Different Scales KDD ’24, August 25–29, 2024, Barcelona, Spain
level graphs to generate the corpus. The details on how to define
the structural difference, construct the level graphs, conduct the
random walk, and obtain node features are explained as follows.
Structural Difference: We describe how to compute the struc-
tural difference between each node pair. In order to quantify the
structural identity of each node 𝑢in hypergraph G=(V,E), we
represent it by a incidence vector Λ𝑢=[𝜆𝑢(2),...,𝜆𝑢(𝑔∗)], where
𝑔∗is the maximum hyperedge size in Eand𝜆𝑢(𝑠)is the number of
hyperedges of size 𝑠that are incident to 𝑢.
In our setting, the structural difference 𝑑𝑘(𝑢,𝑣)of the𝑘-hop
neighborhoods of nodes 𝑢and𝑣is defined inductively as follows:
𝑑𝑘(𝑢,𝑣)=𝑑𝑘−1(𝑢,𝑣)+𝑚(𝑠(𝑁𝑘(𝑢)),𝑠(𝑁𝑘(𝑣))),∀𝑘≥1,(2)
where𝑑0(𝑢,𝑣)=∥Λ𝑢−Λ𝑣∥1;𝑁𝑘(𝑢)and𝑁𝑘(𝑣)denote the set of
nodes that are exactly 𝑘-hops away from 𝑢and𝑣, respectively3;
𝑠(𝑁𝑘(𝑢))and𝑠(𝑁𝑘(𝑣))denote the sequence of incidence vectors Λ
of such nodes, respectively; and 𝑚(𝑠(𝑁𝑘(𝑢)),𝑠(𝑁𝑘(𝑣)))is a distance
measure of such two sequences.
For each hyperedge size 𝑔=2,...,𝑔∗, we use𝜆𝑤(𝑔)to denote the
number of size- 𝑔incident hyperedges of each node 𝑤in𝑁𝑘(𝑢)and
𝑁𝑘(𝑣). We compute the Dynamic Time Warping [ 47] between two
respective sequences {𝜆𝑤(𝑔)}𝑤∈𝑠(𝑁𝑘(𝑢))and{𝜆𝑤′(𝑔)}𝑤′∈𝑠(𝑁𝑘(𝑣))
of𝑠(𝑁𝑘(𝑢))and𝑠(𝑁𝑘(𝑣)). Let this value be 𝑚𝑔(𝑠(𝑁𝑘(𝑢)),𝑠(𝑁𝑘(𝑣))).
After that, we then obtain 𝑚(𝑠(𝑁𝑘(𝑢))by taking the summation:
𝑚(𝑠(𝑁𝑘(𝑢)),𝑠(𝑁𝑘(𝑣)))=Í𝑔∗
𝑔=2𝑚𝑔(𝑠(𝑁𝑘(𝑢)),𝑠(𝑁𝑘(𝑣))).
Construction of Level Graphs: Based on the difference measure,
defined in Eq. (2), we describe how to construct the 𝑇level graphs
𝐺[0],...,𝐺[𝑇−1]. The process is adapted from one used in [ 46],
which we refer readers to for in-depth understanding and details.
For𝑙=0,...,(𝑇−1),𝐺[𝑙]is a complete graph. In 𝐺[𝑙], the edge
between nodes 𝑢and𝑣has weight𝑒−𝑑𝑙(𝑢,𝑣), so a lower structural
difference (higher structural similarity) leads to a higher weight.
Random Walk: To generate a corpus, HyperFeat employs a bi-
ased Random Walk (RW) to generate a sequence of nodes. In each
time step, at node 𝑛in the𝑙-level graph 𝐺[𝑙], RW moves to another
node in𝐺[𝑙]or to node𝑛in either𝐺[𝑙−1]or𝐺[𝑙+1]. In the former
case, the probability of moving to each node 𝑣is proportional to the
weight of edge(𝑢,𝑣). As a result, the proximity of any node 𝑢in the
corpus will likely include the nodes that are structurally similar to
𝑢. In the latter case, RW moves to another level graph with different
edge weights. More details are explained in Appendix D.
Language Model: Once we obtain a corpus, we employ Skip-Gram
with Negative Sampling (SGNS) [ 40] to extract the node features
from the corpus. For a node 𝑢, SGNS aims to maximize the likelihood
of its context in the corpus, where the context of 𝑢is a window of
sizeWcentered on it. As structurally similar nodes tend to be in
close proximity, they are afforded similar node features.
In summary, for each hypergraph G𝑖(𝑖=1,2),HyperFeat ex-
tracts a node feature matrix X𝑖∈R|V𝑖|×𝐹, which preserves the
structural similarity uniquely defined on hypergraphs.
We present some theoretical properties of HyperFeat in Sec-
tion 4.6 and show its empirical superiority over a straightforward
application of struc2vec [46] in Section 5.3.
3That is,𝑁𝑘(𝑢)is the nodes exactly 𝑘-hop away from 𝑢in the clique expansion of G.4.3 Step 2: HyperCL- Contrastive Learning
After the node features in Step 1 are obtained, Step 2, referred
to as HyperCL, conducts Contrastive Learning (CL) (refer to Sec-
tion 3.1) in each input hypergraph to pre-train the Hypergraph
Neural Network Φ. CL aims to maximize the similarity between the
representations of the same node across the two views ¯Cand ˆCof
each hypergraph Gwhile minimizing the similarity between the
representations of different nodes.
This serves as a supervised proxy task for Problem 1 to address
Challenge C3. In particular, by performing node-level contrastive
learning on ¯Cand ˆC, we create a “supervised pseudo-alignment”
task whose objective is to maximize the similarity of the represen-
tations of the same node so that an alignment predictor can find the
exact node correspondences based on the representation vectors.
In other words, we pre-train Φaiming to solve the hypergraph
alignment between ¯Cand ˆCwith ground-truths. In addition, via
CL, we encourage Φto represent different nodes in the same hy-
pergraph with dissimilar vectors to help the alignment step avoid
confusion among similar node representations when predicting
node correspondences.
For each input hypergraph G𝑖,𝑖=1,2,HyperCL creates two
corrupted views, obtains node embeddings in each view via Φ, and
trains Φvia an objective (Eq. (5)).
Creation of Views by Corruption: For each hypergraph G=
(V,E), its incidence matrix H, and its node feature matrix X(i.e.,
output of HyperFeat), we create two corrupted views, ¯C=(¯G,¯X)
and ˆC=(ˆG,ˆX), by performing the following operations for each:
•Feature Masking We create a random binary mask of size 𝐹,
where each entry is independently sampled from a Bernoulli
distributionB(1−𝑝𝑓), and use it to mask features of all nodes.
That is,𝑝𝑓is the probability that each entry of Xis replaced with
0. We apply feature masking on Xto create ¯Xand ˆX.
•Membership Masking We create a random binary mask of
sizeÍ|V|
𝑖=1Í|E|
𝑗=1H[𝑖][𝑗](the number of non-zero entries in the
incidence matrix H), where each entry is independently sampled
from a Bernoulli distribution B(1−𝑝𝑚), and use it to mask nodes
in the hyperedges. That is, 𝑝𝑚is the probability that each node
in each hyperedge is removed. We apply membership masking
onGto create ¯Gand ˆG.
Hypergraph Encoder: For each hypergraph Gand its node fea-
ture matrix X,Φembeds the nodes with representation vectors by
the two-step message-passing scheme as in Eq. (1). For HyperAlign,
Φuses the following aggregation functions [18]:
Q(𝑘)=𝜎 D−1
EH𝑇Z(𝑘−1)Θ(𝑘)
E+𝑏(𝑘)
E,
Z(𝑘)=𝜎 D−1
VHQ(𝑘)Θ(𝑘)
V+𝑏(𝑘)
V,(3)
where Z(0)=X;𝜎denotes a non-linear activation function; DE∈
N|E|×|E|is the diagonal matrix where each diagonal entry is the size
of a hyperedge; DV∈N|V|×|V|is the diagonal matrix where each
diagonal entry is the number of incident hyperedges of each node;
Θ(𝑘)
EandΘ(𝑘)
Vare trainable weight matrices; and 𝑏(𝑘)
Eand𝑏(𝑘)
V
are trainable bias vectors. The output of Φis the node embedding
matrix in the last layer. That is, for each corrupted view, Φoutputs
a node embedding matrix: ¯Z=Φ(¯G,¯X)and ˆZ=Φ(ˆG,ˆX).
 
613KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
Training Objective: Let¯𝒛𝑗be the𝑗-th row of ¯Z, which denotes
the embedding vector of the 𝑗-th node of Gin the view ¯C. Similarly,
letˆ𝒛𝑗denote the embedding vector of the 𝑗-th node of Gin the
view ˆC. We aim to maximize the similarity of the representation
vectors of each node in both views while minimizing the similarity
with the representations of other nodes. In particular, Φis trained
to minimize Normalized Temperature-scaled Cross Entropy [11]:
ℓ(¯𝒛𝑗,ˆ𝒛𝑗)=−log𝑒𝑠(¯𝒛𝑗,ˆ𝒛𝑗)/𝜏
Í|V|
𝑘=1𝑒𝑠(¯𝒛𝑗,ˆ𝒛𝑘)/𝜏, (4)
where𝑠(𝑥,𝑦)denotes the cosine similarity of vectors 𝑥and𝑦, and
𝜏is a temperature hyperparameter. Based on this term, the final
loss for pre-training ΦinHyperCL is defined as follows:
LCL=1
2|𝑉|∑︁|𝑉|
𝑗=1
ℓ(¯𝒛𝑗,ˆ𝒛𝑗)+ℓ(ˆ𝒛𝑗,¯𝒛𝑗)	
. (5)
In summary, for each input hypergraph G𝑖and its node features
X𝑖(i.e., output of HyperFeat), we create two corrupted views ¯C𝑖
and ˆC𝑖; and pre-train the encoder Φto minimizeL𝑖
CLin Eq. (5). By
doing so, we leverage (a) the similarity between CL and Problem 1,
and (b) the pseudo-labels, which are only obtainable for CL.
4.4 Step 3: HyperAug- GAN Training with
Topological Augmentation
AfterΦis pre-trained in Step 2, for Step 3, referred to as HyperAug,
we adopt a GAN to train Φto produce node embeddings that are
aligned across G1andG2. During the process, we propose topo-
logical augmentation to share information between the two input
hypergraphs. This augmentation also ensures that the two hyper-
graphs have the same number of hyperedges in order to resolve
their scale differences, i.e., Challenge C4.
GAN Training: Motivated by the effectiveness of GAN in aligning
graphs [ 15,20,32], we train an adversarial framework to align the
two node embedding spaces of the hypergraphs. Note that Hyper-
Align is agnostic to the GAN model, and we employ Wasserstein-
GAN [ 3] in this work. Particularly, the generator Gconsists of the
hypergraph encoder Φ, pre-trained in HyperCL, and a neural net-
work𝜓:G=(Φ,𝜓).Gmaps the two hypergraphs with their node
feature matrices (from HyperFeat in Section 4.2) to two respective
node embedding matrices Z1,Z2:
Z1,Z2=G((G1,X1),(G2,X2))=Φ(G1,X1),𝜓(Φ(G2,X2))(6)
Note that the parameters of Φare shared between G1andG2, and
an additional mapping function 𝜓is applied for the node embedding
matrix of G2to account for the differences between G1andG2.
The discriminatorDlearns to approximate the Wasserstein dis-
tance (or Earth-Mover Distance) WEM(Z1,Z2)between Z1andZ2.
The generatorGthen tries to minimize the distance by minimizing
the lossLgen=WEM(Z1,Z2). More details on Wasserstein GAN
are explained in Appendix E. Refer to the Online Appendix [ 1] for
a comparison of several GAN models in alignment performance.
Reconstruction Loss: To avoid mode collapse, i.e., the situation
when all the embeddings become the same vector, we include an
additional loss term to encourage Gto output node embeddings
that contain sufficient information to reconstruct the node featuresAlgorithm 2: Topological Augmentation
Input: (a) hypergraphs G1=(V1,E1)andG2=(V2,E2), (b) their
node embedding matrices Z1andZ2, (c) a value𝑡∈N,
Output: updated incidence matrix ˜H2ofG2
1Compute S2,1∈R|V2|×|V1|in which S2,1[𝑖,𝑗]=𝑠(z1,𝑖,z2,𝑗)
2Obtain S′
2,1from S2,1by
3 (i) keeping only the top 𝑡values in each column,
4 (ii) changing other entries to 0,
5 (iii) performing column-wise normalization
6H′
1←S′
2,1H1
7˜H2←Column-Concat(H2,H′
1)
8return ˜H2
(i.e., output of HyperFeat). The loss term for reconstruction is:
Lre=∑︁|V1|
𝑖=1∥r1(z1,𝑖)−x1,𝑖∥+∑︁|V2|
𝑗=1∥r2(z2,𝑗)−x2,𝑗∥,(7)
where r1,r2are the reconstruction functions, as neural networks,
ofG1andG2that aim to minimize Lre.
Given a hyperparameter 𝛽∈[0,1], the total loss term for Gis:
Ltotal=𝛽Lgen+(1−𝛽)Lre (8)
Topological Augmentation: In order to assist GAN, we construct
virtual hyperedges from one hypergraph to augment the other. This
augmentation shares hyperedges across the two hypergraphs to
resolve their scale disparity, directly addressing Challenge C4.
For each hyperedge 𝑒∈E1, we augment a “soft virtual hy-
peredge"𝑒′toE2, and vice versa. Each membership of a node
𝑣∈V1,𝑣∈𝑒is translated into probabilistic memberships of 𝑡
nodes of V2in𝑒′. We repeat this for all nodes in 𝑒to form𝑒′.
To achieve that, for any column H1[:][𝑘]ofH1corresponding to
a hyperedge 𝑒inE1, we construct a virtual column vector 𝑐(of|V2|
entries), corresponding to a virtual hyperedge 𝑒′, to augment to H2.
In𝑐, all entries are first initialized with 0. For each node 𝑣∈V1,𝑣∈𝑒,
we select the top 𝑡nodes in V2having the representation vectors
(from Z2) bearing the highest similarities with that of 𝑣(from Z1).
We denote them by 𝑣′
1,...,𝑣′
𝑡and denote such similarity values by
𝑠𝑣(𝑣′
𝑗),𝑗=1,...,𝑡. After that, the entry corresponding to node 𝑣′
𝑗
in𝑐is incremented with𝑠𝑣(𝑣′
𝑗)
Í𝑡
𝑙=1𝑠𝑣(𝑣′
𝑙)for𝑗=1,...,𝑡. By this, node 𝑣
is represented in G2as “soft correspondences" of the nodes in V2
having the most similar representation vectors with that of 𝑣. We
repeat this step with other non-zero entries of H1[:][𝑘]to fill in𝑐.
The sums of the entries in H1[:][𝑘]and𝑐are both equal to|𝑒|.
We proceed similarly with all columns of H1to obtain their
corresponding virtual columns, which will be added to H2. We
stack these virtual columns, including 𝑐, to form an augmented
incidence matrix H′
1∈R|V2|×|E1|. We concatenate H′
1toH2to form
˜H2∈R|V2|×(|E1|+|E2|). This is equivalent to adding “soft virtual hy-
peredges" from G1toG2in which the binary incidence entries are
replaced with real-valued entries. The procedure is outlined in Algo-
rithm 2. Similarly, we add virtual columns, obtained from H2, toH1
to attain an updated incidence matrix ˜H1∈R|V1|×(|E1|+|E2|)forG1.
As˜H1and ˜H2have the same number of columns, the hypergraphs
share the same scale after this topological augmentation.
At each iteration of GAN training, we conduct the aforemen-
tioned augmentations to create two augmented hypergraphs ˜G1and
 
614Unsupervised Alignment of Hypergraphs with Different Scales KDD ’24, August 25–29, 2024, Barcelona, Spain
˜G2. In the next iteration, we compute Z1,Z2=G((˜G1,X1),(˜G2,X2)),
trainDaccording to Eq. (12), and trainGaccording to Eq. (8). Note
that to apply Φon˜G𝑖, we only need to replace H𝑖in Eq. (3)with ˜H𝑖
and update DE𝑖andDV𝑖, accordingly, based on the “soft number”
of incident nodes and hyperedges, respectively.
4.5 Step 4: Alignment Prediction
Given the respective embedding matrices Z1andZ2from Step
3,HyperAlign predicts the node correspondences between G1
andG2. Recall that, without loss of generality, we assume that
|V1|≤|V2|. For each node 𝑣𝑖∈V1of embedding z1,𝑖, we predict
its correspondence among the nodes in V2={𝑣′
1,...,𝑣′
|V2|}as the
node𝑣′
𝑗∈V2having the most similar embedding z2,𝑗, i.e.,𝑗=
argmax𝑗=1,...,|V2|{𝑠(z1,𝑖,z2,𝑗)}. We repeat this for all nodes in V1
and return the set of all predicted correspondences A.
4.6 Theoretical Results
In this section, we present the time complexity and other theoretical
properties of HyperAlign. All proofs are presented in the Online
Appendix [1].
Let𝑔∗
𝑖denote the maximum hyperedge size in E𝑖,𝑖=1,2. The
time complexity HyperAlign, when considering all hyperparame-
ters of HyperAlign as constants, isOÍ2
𝑖=1𝑔∗
𝑖(|E𝑖|+|V𝑖|2log|V𝑖|)
.
Also, our method has an OÍ2
𝑖=1(Í
𝑒∈E𝑖|𝑒|+|V𝑖|log|V𝑖|)+| V1||V2|
space complexity. Refer to the Online Appendix [ 1] for the details.
Moreoever, we present the theoretical results for a special case
ofHyperFeat (Step 1). In this special case, HyperFeat only con-
siders the structural differences among nodes at a particular 𝑙-hop
neighborhood and generates a sequence of nodes by Random Walk
on the𝑙-level graph 𝐺[𝑙].
In the aforementioned case, we show that when the corpus length
𝐿is sufficiently large, HyperFeat obtains the node features by
performing an implicit factorization of a constant matrix.
Theorem 3 (HyperFeat as Matrix Factorization). Assume
that HyperFeat only generates the corpus by Random Walk on one
level graph𝐺[𝑙]. When the corpus length 𝐿→∞ , the embeddings
produced by HyperFeat result from an implicit factorization of a
constant matrix M∗.
One common approach when extracting node features based on
matrix factorization is to conduct Singular Value Decomposition
(SVD) on the matrix [ 22,29,44]. Let HyperFeat* be a version of
HyperFeat that extracts node feature matrix Xby performing
Singular Value Decomposition of M∗=UΣVT,X=U√
Σ.
In Theorem 4, we show that HyperFeat* can provide sufficient
conditions to conclude that two hypergraphs are non-isomorphic.
Two hypergraphs are isomorphic if one hypergraph is obtained
by applying a node permutation on the other. Hypergraph isomor-
phism [ 4] determines if two given hypergraphs are isomorphic,
which is a special case of the hypergraph alignment problem where
the two hypergraphs are only different by a node permutation.
Theorem 4 (HyperFeat* as Isomorphism Test). IfHyperFeat*
maps the nodes of hypergraphs H1andH2to two different sets of
node embeddings, H1andH2are not isomorphic.5 Experiments
5.1 Experiment Settings
Datasets: We use 12real-world hypergraphs across 6domains. The
details of the datasets are provided in Appendix C.
Dataset Preprocessing: Each dataset constitutes a list of time-
stamped hyperedges 𝑒1,...,𝑒𝑇∗. The hypergraph G1is created by
collecting hyperedges 𝑒1,...,𝑒𝑇1(𝑇1<𝑇∗). To replicate the practical
scenarios, we consider two ways to construct the hypergraph G2:
•Configuration 1:G2consists of hyperedges 𝑒𝑇1+1,...,𝑒𝑇∗. This
reflects the scenario that people use one platform in the past
(forG1) and then switch to another platform (for G2).G1has𝑇1
hyperedges, and G2has𝑇2=𝑇∗−𝑇1hyperedges.
•Configuration 2:G2consists of hyperedges 𝑒1,...,𝑒𝑇∗, soG2
has𝑇2=𝑇∗hyperedges.. This reflects the scenario that each
hypergraph represents a snapshot of user history at two different
times in which a snapshot contains the list of hyperedges from
the beginning up to the time when the snapshot is taken.
We keep the original node ids for G1and permute them to ob-
tain the node ids for G2. Due to the space limit, in this section,
we only present the results for Configuration 1. The results for
Configuration 2 are reported in the Online Appendix [1].
Competitors: We consider the following competitors:
•Big-Align: We apply Big-Align, an alignment method for bipar-
tite graphs, on the star expansions of the two hypergraphs.
•REGAL: We apply the method xNetMF [ 22] to generate node
representations used for alignment. We consider two variants
REGAL-g andREGAL-h when applying xNetMF for the clique
expansion and the original hypergraph, respectively.
•Grad-Align+: This method [ 42] incorporates degree information
to node features and trains a graph encoder to maximize the
similarity in representations of the two graphs.
•SANA: SANA [ 43] augments each graph and optimizes a graph
encoder to align the original and augmented graphs.
•UUIL: UUIL [32] uses GAN to minimize the distance between the
distributions of node embeddings of clique-expanded graphs.
•DANA: This method [15] employs CyCleGAN, instead of GAN.
•WAlign: WAlign [20] uses Wasserstein GAN instead of GAN.
More details on the competitors are in Appendix A.1.
Hyperparameter Selection: As the problem assumes an unsu-
pervised manner, we consider several sets of hyperparameters for
each method and report the performance of the globally best set of
hyperparameters. Details can be found in Appendix A.2.
Baselines: In addition, we also consider two simple baselines:
•SVD: We obtain the node embeddings by applying Singular Value
Decomposition on the incidence matrix of each hypergraph and
employ the node embeddings for alignment prediction.
•HyperFeat: We directly use the node features extracted in Step
1 -HyperFeat (Section 4.2) for alignment prediction.
Evaluation Metric: Given the setAof predicted node correspon-
dences and the ground-truth C, we evaluate each method by the
alignment accuracy 𝐴, which is defined as 𝐴=|A∩C|/|C| ,
i.e., (the number of correct predicted correspondences) / (the total
number of ground-truth correspondences).
 
615KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
Table 2: Means and standard deviations of alignment accuracy (in %) over five independent runs of all methods. The best
alignment accuracies are in bold. HyperAlign consistently outperforms the competitors and baselines.
Dataset SVD
HyperFeat Big-Align
REGAL-g REGAL-h Grad-Align+ SANA UUIL DANA WAlign HyperAlign
coauth-Ge
ology 19.18±0.05
22.23±0.82 25.18±0.67
27.63±0.59 32.58±0.73 33.04±0.93 35.78±1.04 33.47±1.24 35.62±1.08 34.62±1.14 45.02±1.18
coauth-Histor
y 18.03±0.07
19.67±0.41 20.75±0.34
26.28±0.27 27.97±0.36 28.03±0.87 28.48±0.92 28.23±1.07 28.44±1.27 28.59±1.02 37.94±0.98
contact-high 22.43±0.03
30.27±0.37 32.54±0.31
35.87±0.43 38.71±0.38 40.69±0.78 41.97±0.78 41.59±0.82 42.86±0.74 43.41±0.57 51.92±0.74
contact-primar
y26.78±0.04
32.51±0.33 31.29±0.28
36.28±0.25 38.57±0.41 41.64±0.81 42.19±0.78 40.43±0.57 42.25±0.67 43.08±0.73 53.27±0.65
email-Enr
on 25.91±0.01
33.19±0.27 34.85±0.41
36.31±0.35 39.46±0.38 40.36±0.81 41.07±0.62 39.68±0.71 40.87±0.73 41.23±0.78 48.41±0.72
email-Eu 24.49±0.02
31.27±0.29 31.95±0.38
34.31±0.42 35.85±0.46 36.29±0.73 37.52±0.78 37.12±0.54 38.51±0.58 38.73±0.61 49.76±0.73
NDC-classes 31.27±0.04
35.31±0.32 37.35±0.41
38.16±0.36 40.38±0.42 42.36±0.77 42.78±0.51 42.91±0.86 42.83±0.78 44.52±0.64 51.48±0.87
NDC-substances 32.25±0.11
36.05±0.38 38.17±0.43
41.68±0.31 44.13±0.43 46.04±0.65 45.61±0.87 45.27±0.77 46.85±0.81 47.19±0.73 57.84±0.96
thr
eads-ubuntu 21.43±0.04
26.52±0.44 28.47±0.53
34.59±0.56 37.53±0.51 40.67±0.94 42.15±0.81 39.84±0.71 42.07±0.83 43.94±0.89 53.37±1.04
thr
eads-math 22.69±0.06
25.23±0.51 27.39±0.49
31.83±0.43 32.28±0.52 32.85±0.91 33.47±1.02 32.41±0.91 33.83±0.84 34.13±0.73 44.06±1.12
Cora 24.59±0.06
31.37±0.45 30.88±0.63
33.29±0.38 35.46±0.53 37.72±0.93 38.11±0.97 36.83±0.89 38.25±1.04 37.94±1.13 50.63±1.14
Pubme
d 23.72±0.18
29.55±0.52 31.02±0.46
31.81±0.61 32.28±0.67 33.09±0.88 33.42±0.92 33.18±1.14 33.06±0.97 33.59±1.09 42.16±1.08
Table 3: Means and standard deviations of alignment ac-
curacy (in %) over five independent runs of five versions of
HyperAlign. The full-fledged version performs best, demon-
strating the effectiveness of the three key novel components
ofHyperAlign.
Dataset HyperAlign HA-s
HA-WC HA-WA HA-WAC
coauth-Ge
ology 45.02±1.18 43.06±1.03
42.17±1.13 41.84±1.06 36.47±1.09
coauth-History 37.94±0.98 35.92±0.89
35.31±0.92 33.89±0.94 30.37±1.04
contact-high 51.92±0.74 48.78±0.83
47.58±0.74 47.03±0.71 44.92±0.82
contact-primary 53.27±0.65 50.27±0.71
48.03±0.85 46.74±0.92 43.25±0.68
email-Enron 48.41±0.72 46.03±0.84
44.81±0.81 43.71±0.86 41.36±0.89
email-Eu 49.76±0.73 49.02±0.67
48.23±0.72 46.61±0.96 39.11±0.79
NDC-classes 51.48±0.87 49.92±0.79
49.15±0.83 47.98±0.86 45.08±0.76
NDC-substances 57.84±0.96 54.87±1.03
53.72±0.94 51.33±0.87 47.31±0.82
threads-ubuntu 53.37±1.04 50.64±0.82
49.14±0.93 48.41±1.08 45.18±0.97
threads-math 44.06±1.12 41.57±0.86
41.07±0.82 39.62±1.04 34.58±0.91
Cora 50.63±1.14 46.87±1.04
46.13±0.97 44.92±1.02 41.13±0.86
Pubmed 42.16±1.08 40.35±1.03
39.67±0.95 39.04±0.86 36.71±0.91
Evaluation settings: For each method, we report the average
alignment accuracy and standard deviation of 5independent runs.
Unless specified otherwise, the results are reported for Configura-
tion 1when the ratio 𝑇2:𝑇1≈2.
5.2 RQ1. Alignment Performance
In this section, we compare our method HyperAlign against the
baselines and competitors in terms of alignment accuracy. The
alignment accuracies of all methods in Configuration 1 when the
ratio𝑇2:𝑇1≈2are reported in Table 2. HyperAlign consistently
and significantly outperforms all the other methods.
In addition, we vary the ratio 𝑇2:𝑇1and highlight the alignment
accuracies of HyperAlign and other methods in Figure 2. As the
level of difference between G1andG2increases, the performances
of all methods tend to decline, but HyperAlign consistently results
in the highest alignment accuracy.
5.3 RQ2. Ablation Study
We investigate the roles of HyperFeat, HyperCL, and the Topolog-
ical Augmentation module in HyperAug in the alignment accuracy
ofHyperAlign by comparing it with the following variants:
•HA-s: This method is obtained by replacing HyperFeat with
struc2vec [46] to extract node features in Step 1.
•HA-WC: This method is obtained by removing Step 2 -HyperCL,
the Contrastive Learning module, from HyperAlign.•HA-WA: This method is obtained by removing Topological Aug-
mentation in Step 3 from HyperAlign.
•HA-WAC: This method is obtaining by removing Contrastive
Learning (HyperCL) and Topological Augmentation (in Step 3)
from HyperAlign. Note that HA-WAC is equivalent to a simple
extension of WAlign that employs Hypergraph Neural Network
instead of Graph Neural Network.
As shown in Table 3, HyperAlign consistently outperforms HA-
s,HA-WA, and HA-WC across datasets. Also, all of the methods are
consistently more accurate than HA-WAC. This shows that all of
the three novel components of HyperAlign: HyperFeat- Feature
Extraction, HyperCL- Contrastive Learning as a proxy task, and the
Topological Augmentation module, to resolve the scale disparity,
inHyperAug contribute significantly to its performance.
To further examine the contribution of Contrastive Learning
and Topological Augmentation, we also vary the ratio 𝑇2:𝑇1and
illustrate the performance of the variants of HyperAlign in Fig-
ure 3. While they all outperform WAlign, the performance decline
is severe for the variants (HA-WC, HA-WA, HA-WAC). These re-
sults illustrate the effectiveness of the Contrastive Learning and
Topological Augmentation modules in HyperAlign.
5.4 RQ3. Case Studies
We pinpoint several pairs of nodes in which each node is frequently
misaligned to correspond to the other. They represent nodes/objects
that are indeed very similar in practice. For example, in coauth-
Geology, the nodes representing prof. Stefan Schouten andprof. Jaap
Sinninghe Damsté, who both serve as a Senior Research Leader in the
Department of Microbiology & Biogeochemistry of Royal Netherlands
Institute for Sea Research and have had 170joint research papers
(in the dataset), are frequently confused with each other. Similarly,
in email-Enron, the frequently confused nodes represent Ms. Louis
Kitchen andMr. John Lovorato who both work as Chief Operating
Officers of Enron Americas. In another example, in NDC-classes,
the node representing human insulin is often mistaken as the node
representing analog insulin, and vice versa.
Additionally, we conduct a misalignment analysis of Hyper-
Align and the competitors. Due to the space constraint, we present
the analysis in Appendix B.1. These results imply that while each
competitor only marginally complements HyperAlign, our method
can correctly predict correspondences for a larger portion of the
 
616Unsupervised Alignment of Hypergraphs with Different Scales KDD ’24, August 25–29, 2024, Barcelona, Spain
1 2 3 4
Ratio2040Accuracy (%)
coauth-Geology
1 2 3 4
Ratio2040
coauth-History
1 2 3 4
Ratio2040
contact-high
1 2 3 4
Ratio2550
contact-primary
1 2 3 4
Ratio2040
email-Enron
1 2 3 4
Ratio2040
email-Eu
1 2 3 4
Ratio2040Accuracy (%)
NDC-classes
1 2 3 4
Ratio4060
NDC-substances
1 2 3 4
Ratio2550
threads-ubuntu
1 2 3 4
Ratio2040
threads-math
1 2 3 4
Ratio2550
Cora
1 2 3 4
Ratio2040
PubmedHyperAlign WAlign DANA SANA REGAL-H Big-Align
Figure 2: Comparison in alignment accuracy of several methods when varying the difference ratio between the two hypergraphs.
As the ratio increases, the alignment task becomes more difficult, and HyperAlign consistently outperforms all the competitors.
1 2 3 4
Ratio304050Accuracy (%)
coauth-Geology
1 2 3 4
Ratio2040
coauth-History
1 2 3 4
Ratio4050
contact-high
1 2 3 4
Ratio4050
contact-primary
1 2 3 4
Ratio304050
email-Enron
1 2 3 4
Ratio304050
email-Eu
1 2 3 4
Ratio304050Accuracy (%)
NDC-classes
1 2 3 4
Ratio4060
NDC-substances
1 2 3 4
Ratio4050
threads-ubuntu
1 2 3 4
Ratio3040
threads-math
1 2 3 4
Ratio304050
Cora
1 2 3 4
Ratio2040
PubmedHyperAlign HyperAlign-WC HyperAlign-WA HyperAlign-WAC WAlign
Figure 3: Comparison in alignment accuracy of several variants of HyperAlign when varying the difference ratio between the
two hypergraphs. The variants of HyperAlign consistently outperform WAlign. As the ratio increases, the alignment task
becomes more difficult, and the performance decline is severe in these variants, showing the contributions of the Contrastive
Learning and Topological Augmentation modules in HyperAlign.
nodes misaligned by each competitor. Moreover, as the nodes mis-
aligned by HyperAlign can hardly be aligned correctly by other
methods, they are indeed difficult examples to align.
6 Conclusion
In this work, we introduce the problem of unsupervised hyper-
graph alignment, potentially with different scales, and discuss its
unique challenges. For this problem, we develop our method, Hy-
perAlign, which directly addresses each challenge of the problem
and predicts the node correspondences across two hypergraphs
in a completely unsupervised manner. HyperAlign utilizes the
hypergraph topology to extract node features, employs contrastive
learning as a pseudo-supervised alignment task, and employs gener-
ative adversarial networks to align the two node embedding spaces.
To resolve the scale disparity between the input hypergraphs, Hy-
perAlign conducts topological augmentation that shares hyper-
edges across the two hypergraphs during the learning process. We
demonstrate empirically that HyperAlign outperforms 8 compet-
ing methods significantly and consistently across 10 real-worlddatasets. Through extensive experiments, we also illustrate the
significant contribution to the performance of each novel mod-
ule of HyperAlign. The code and datasets are available at https:
//github.com/manhtuando97/HyperAlign.
Social networks are an important application domain of hyper-
graph alignment with diverse practical implications. It is worth
noting that alignment on social networks involves using user data,
and privacy preservation is crucial. Similarly to machine learning
techniques, hypergraph alignment on social networks should only
be performed upon user consent with proper privacy protection
measures. A promising research direction is to devise alignment
workflows coupled with techniques to minimize privacy concerns.
Acknowledgements
This work was supported by Institute of Information & Communications
Technology Planning & Evaluation (IITP) grant funded by the Korea govern-
ment (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowl-
edge Enhancement for AI Agent Collaboration) (No. RS-2019-II190075, Arti-
ficial Intelligence Graduate School Program (KAIST)).
 
617KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
References
[1]2024. Code, Datasets, and Online Appendix. https://github.com/manhtuando97/
HyperAlign.
[2]Ilya Amburg, Nate Veldt, and Austin Benson. 2020. Clustering in graphs and
hypergraphs with categorical edge labels. In The Web Conference.
[3]Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN.
arXiv:1701.07875 [stat.ML]
[4]László Babai and Eugene M Luks. 1983. Canonical labeling of graphs. In Proceed-
ings of the fifteenth annual ACM symposium on Theory of computing. 171–183.
[5]Mohsen Bayati, Margot Gerritsen, David F Gleich, Amin Saberi, and Ying Wang.
2009. Algorithms for large, sparse network alignment problems. In ICDM.
[6]Mohsen Bayati, David F Gleich, Amin Saberi, and Ying Wang. 2013. Message-
passing algorithms for sparse network alignment. ACM Transactions on Knowl-
edge Discovery from Data (TKDD) 7, 1 (2013), 1–31.
[7]Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Klein-
berg. 2018. Simplicial closure and higher-order link prediction. Proceedings of
the National Academy of Sciences 115, 48 (2018), E11221–E11230.
[8]Austin R Benson, Ravi Kumar, and Andrew Tomkins. 2018. Sequences of sets. In
KDD.
[9]Samuel Bulò and Marcello Pelillo. 2009. A game-theoretic approach to hypergraph
clustering. In NeurIPS.
[10] Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu.
2020. Graph optimal transport for cross-domain alignment. In ICML.
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In ICML.
[12] Xiyuan Chen, Mark Heimann, Fatemeh Vahedian, and Danai Koutra. 2020. Cone-
align: Consistent network alignment with proximity-preserving node embedding.
InCIKM.
[13] Xiaokai Chu, Xinxin Fan, Di Yao, Zhihua Zhu, Jianhui Huang, and Jingping
Bi. 2019. Cross-network embedding for multi-network alignment. In The Web
Conference.
[14] Guilherme Ferraz de Arruda, Giovanni Petri, and Yamir Moreno. 2020. Social
contagion models on hypergraphs. Physical Review Research 2, 2 (2020), 023032.
[15] Tyler Derr, Hamid Karimi, Xiaorui Liu, Jiejun Xu, and Jiliang Tang. 2021. Deep
adversarial network alignment. In CIKM.
[16] Manh Tuan Do, Se-eun Yoon, Bryan Hooi, and Kijung Shin. 2020. Structural
patterns and generative models of real-world hypergraphs. In KDD.
[17] AnHai Doan, Jayant Madhavan, Pedro Domingos, and Alon Halevy. 2004. On-
tology matching: A machine learning approach. Handbook on ontologies (2004),
385–403.
[18] Yihe Dong, Will Sawin, and Yoshua Bengio. 2020. HNHBN: Hypergraph networks
with hyperedge neurons. In ICML Graph Representation Learning and Beyond
Workshop.
[19] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy-
pergraph neural networks. In AAAI.
[20] Ji Gao, Xiao Huang, and Jundong Li. 2021. Unsupervised graph alignment with
wasserstein distance discriminator. In KDD.
[21] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining. 855–864.
[22] Mark Heimann, Haoming Shen, Tara Safavi, and Danai Koutra. 2018. Regal:
Representation learning-based graph alignment. In CIKM.
[23] Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. 2019. Dy-
namic Hypergraph Neural Networks.. In IJCAI.
[24] Yunbum Kook, Jihoon Ko, and Kijung Shin. 2020. Evolution of real-world hyper-
graphs: Patterns and models without oracles. In ICDM.
[25] Danai Koutra, Hanghang Tong, and David Lubensky. 2013. Big-align: Fast bipar-
tite graph alignment. In ICDM.
[26] Tarun Kumar, K Darwin, Srinivasan Parthasarathy, and Balaraman Ravindran.
2020. HPRA: Hyperedge prediction using resource allocation. In WebSci.
[27] Dongjin Lee and Kijung Shin. 2023. I’m Me, We’re Us, and I’m Us: Tri-directional
contrastive learning on hypergraphs. In AAAI.
[28] Geon Lee, Jihoon Ko, and Kijung Shin. 2020. Hypergraph motifs: concepts,
algorithms, and discoveries. (2020).
[29] Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix
factorization. NeurIPS (2014).
[30] Bo Li, Yong Zhang, Chengyang Zhang, Xinglin Piao, Yongli Hu, and Baocai
Yin. 2024. Multi-scale hypergraph-based feature alignment network for cell
localization. Pattern Recognition 149 (2024), 110260.
[31] Chaozhuo Li, Senzhang Wang, Yukun Wang, Philip Yu, Yanbo Liang, Yun Liu,
and Zhoujun Li. 2019. Adversarial learning for weakly-supervised social network
alignment. In AAAI.
[32] Chaozhuo Li, Senzhang Wang, Philip S Yu, Lei Zheng, Xiaoming Zhang, Zhoujun
Li, and Yanbo Liang. 2018. Distribution distance minimization for unsupervised
user identity linkage. In CIKM.
[33] Jianbo Li, Jingrui He, and Yada Zhu. 2018. E-tail product return prediction via
hypergraph-based local graph cut. In KDD.[34] Xiaowei Liao, Yong Xu, and Haibin Ling. 2021. Hypergraph neural networks for
hypergraph matching. In ICCV.
[35] Li Liu, William K Cheung, Xin Li, and Lejian Liao. 2016. Aligning Users across
Social Networks Using Network Embedding.. In IJCAI, Vol. 16.
[36] Qingshan Liu, Yuchi Huang, and Dimitris N Metaxas. 2011. Hypergraph with
sampling for image retrieval. Pattern Recognition 44, 10-11 (2011), 2255–2262.
[37] Bradley Malin. 2005. Unsupervised name disambiguation via social network
similarity. In SIAM Workshop on link analysis, counterterrorism, and security.
[38] Sergey Melnik, Hector Garcia-Molina, and Erhard Rahm. 2002. Similarity flooding:
A versatile graph matching algorithm and its application to schema matching. In
ICDE.
[39] Tom Michoel and Bruno Nachtergaele. 2012. Alignment and integration of
complex networks by hypergraph-based spectral clustering. Physical Review E
86, 5 (2012), 056111.
[40] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013).
[41] Min Ouyang, Michel Toulouse, Krishnaiyan Thulasiraman, Fred Glover, and
Jitender S Deogun. 2002. Multilevel cooperative search for the circuit/hypergraph
partitioning problem. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems 21, 6 (2002), 685–693.
[42] Jin-Duk Park, Cong Tran, Won-Yong Shin, and Xin Cao. 2022. GradAlign+:
Empowering gradual network alignment using attribute augmentation. In CIKM.
[43] Jingkai Peng, Fei Xiong, Shirui Pan, Liang Wang, and Xi Xiong. 2023. Robust Net-
work Alignment with the Combination of Structure and Attribute Embeddings.
InICDM.
[44] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.
Network embedding as matrix factorization: Unifying deepwalk, line, pte, and
node2vec. In WSDM.
[45] Svetlozar Todorov Rachev et al .1990. Duality theorems for Kantorovich-
Rubinstein and Wasserstein functionals. (1990).
[46] Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. 2017. struc2vec:
Learning node representations from structural identity. In KDD.
[47] Stan Salvador and Philip Chan. 2007. Toward accurate dynamic time warping in
linear time and space. Intelligent Data Analysis 11, 5 (2007), 561–580.
[48] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[49] Rohit Singh, Jinbo Xu, and Bonnie Berger. 2008. Global alignment of multiple
protein interaction networks with application to functional orthology detection.
Proceedings of the National Academy of Sciences 105, 35 (2008), 12763–12768.
[50] Aaron Smalter, Jun Huan, and Gerald Lushington. 2008. Gpm: A graph pattern
matching kernel with diffusion for chemical compound classification. In 2008 8th
IEEE International Conference on BioInformatics and BioEngineering. IEEE, 1–6.
[51] Shulong Tan, Ziyu Guan, Deng Cai, Xuzhen Qin, Jiajun Bu, and Chun Chen. 2014.
Mapping users across networks by manifold alignment on hypergraph. In AAAI.
[52] Hao Xiong, Junchi Yan, and Li Pan. 2021. Contrastive multi-view multiplex
network embedding with applications to robust network alignment. In KDD.
[53] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wenjun Zhang. 2020. Cross-
domain detection via graph-induced prototype alignment. In CVPR.
[54] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand
Louis, and Partha Talukdar. 2019. HyperGCN: a new method of training graph
convolutional networks on hypergraphs. In NeurIPS.
[55] Naganand Yadati, Vikram Nitin, Madhav Nimishakavi, Prateek Yadav, Anand
Louis, and Partha Talukdar. 2020. NHP: Neural hypergraph link prediction. In
CIKM.
[56] Jaehyuk Yi and Jinkyoo Park. 2020. Hypergraph convolutional recurrent neural
network. In KDD.
[57] Hao Yin, Austin R Benson, Jure Leskovec, and David F Gleich. 2017. Local
higher-order graph clustering. In KDD.
[58] Se-eun Yoon, Hyungseok Song, Kijung Shin, and Yung Yi. 2020. How much and
when do we need higher-order information in hypergraphs? a case study on
hyperedge prediction. In The Web Conference.
[59] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. In NeurIPS.
[60] Ron Zass and Amnon Shashua. 2008. Probabilistic graph and hypergraph match-
ing. In CVPR. IEEE.
[61] Jiawei Zhang and S Yu Philip. 2015. Multiple anonymized social networks
alignment. In ICDM.
[62] Yutao Zhang, Jie Tang, Zhilin Yang, Jian Pei, and Philip S Yu. 2015. Cosnet:
Connecting heterogeneous social networks with local and global consistency. In
KDD.
[63] Zhongbao Zhang, Shuai Gao, Sen Su, Li Sun, and Ruiyang Chen. 2023. MIN-
ING: Multi-granularity network alignment based on contrastive learning. IEEE
Transactions on Knowledge and Data Engineering (2023).
[64] Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. 2006. Learning with
hypergraphs: Clustering, classification, and embedding. (2006).
 
618Unsupervised Alignment of Hypergraphs with Different Scales KDD ’24, August 25–29, 2024, Barcelona, Spain
A Additional Experimental Details
A.1 Details on the competitors
Big-Align is the method for aligning two bipartite graphs. We
represent each hypergraph as a bipartite graph connecting the set
of nodes and the set of hyperedges by star expansion. We apply Big-
Align to align the two star expansions, i.e., finding correspondences
between the two sets of nodes and correspondences between the
two sets of hyperedges across the two hypergraphs.
UUIL, DANA, Grad-Align+, and WAlign are learning graph
alignment methods that require input node features. For the evalu-
ation in Section 5, we apply each method on the clique expansions
of the two hypergraphs. We apply struc2vec [46] to generate the
node features for UUIL ,Grad-Align+, and WAlign as they assume
the existence of node features. We apply node2vec [21] to produce
node features for DANA accordingly to [15].
A.2 Hyperparameter Search
We describe the hyperparameter search space for HyperAlign and
the following competitors: REGAL-g, REGAL-h, DANA, Grad-
Align+, SANA, and WAlign as these methods that consider hyper-
parameter search in the original papers. For each competitor, we
tune the hyperparameters that its original paper analyzed. Refer to
the original papers for full descriptions of the hyperparameters.
REGAL-g &REGAL-h: We vary the hyperparameter values as
follows:
•Number𝑘of neighborhood hops: 𝑘∈{2,3,4}.
•Discount factor 𝛿for greater hop distances: 𝛿∈{0.01,0.05}.
•Node embedding dimension 𝑡log2|V|:𝑡∈{2,6,10}.
DANA: We vary the hyperparameter values as follows:
•Node feature dimension 𝐹(from node2vec[21]): 𝐹∈{64,128}.
•Hidden layer dimension 𝑑of the discriminator: 𝑑∈{128,256,512}.
•Weight𝜆of the Cycle Consistency Loss term in the objective:
𝜆∈{1,5,25}.
Grad-Align+: We vary the hyperparameter values as follows:
•Dimension of augmented attribute vectors 𝑑:𝑑∈{20,30,40}.
SANA: We vary the hyperparameter values as follows:
•Node embedding dimension 𝑑:𝑑∈{64,128}.
WAlign: We vary the hyperparameter values as follows:
•Number𝑘of neighborhood hops to aggregate information for
the Graph Neural Networks: 𝑘∈{2,3,4}.
•Hidden layer dimension 𝑑of the Graph Neural Networks: 𝑑∈
{128,256,512}.
HyperAlign: We vary the hyperparameter values as follows:
•Node feature dimension 𝐹ofHyperFeat: 𝐹∈{64,128}.
•Corruption rate of feature masking and membership .masking
(𝑝𝑓,𝑝𝑚)inHyperCL:(𝑝𝑓,𝑝𝑚)∈{( 0.1,0.1),(0.2,0.2),(0.3,0.3)}.
•Number𝑡of the most similar nodes to select to construct aug-
mented incidence matrices in the Topological Augmentation step
ofHyperAug: 𝑡∈{3,5}.
•Weight𝛽of the generator loss term in Eq 8: 𝛽∈{0.99,0.95,0.9}.
B Additional results
In this section, we present additional experimental results for the
misalignment analysis of HyperAlign and the running time.
05001000Elapsed Time (s)coauth-Geology
05001000coauth-History
Method05001000Elapsed Time (s)threads-ubuntu
Method05001000threads-mathHyperAlign WAlign DANA SANA REGAL-H Big-AlignFigure 4: Comparison in running time among HyperAlign
and the strongest competitors. Big-Align tends to takes the
longest running time, the gap among all the other methods
is not substantial.
B.1 Misalignment Analysis
We present further analysis on the alignment performance of our
method HyperAlign. In particular, for each competitor, we measure
the following values:
•O:The percentage of nodes incorrectly aligned by the competitor
that are correctly aligned by HyperAlign.
•C:The percentage of nodes incorrectly aligned by HyperAlign
that are correctly aligned by the respective competitor.
The results for the analysis are reported in Table 4. Note that as
HyperAlign consistently performs best, the number of misaligned
nodes by any competitor is always greater than the number of nodes
that are incorrectly aligned by HyperAlign. The results suggest
that while each competitor only marginally complements Hyper-
Align, our method can correctly align a larger portion of the nodes
misaligned by the competitor. Moreover, as the nodes misaligned
byHyperAlign can hardly be aligned correctly by other methods,
they are indeed difficult examples to predict correspondences. .
B.2 Running Time
The comparison in running time HyperAlign and the strongest
competitors is highlighted in Figure 4. While Big-Align has the
longest running time, the difference among the other methods is
not substantial.
C Datasets
In this paper, we use 10hypergraph datasets [ 7] across 5domains
(2 datasets each domain) and 2co-citation datasets [ 48]. For each
dataset, we use its 3-core. Specifically, we eliminate non-distinguishable
nodes with identical structures, i.e., isolated nodes and isolated hy-
peredges, and retain the nodes with a sufficient number of incident
hyperedges, following the practice of [ 25]. The datasets we use are:
•co-authorship (coauth-Geology andcoauth-History ): each node
is a researcher, and each hyperedge represents the list of co-
authors in a publication that results from the joint work of these
researchers. The research disciplines are Geology and History.
 
619KDD ’24, August 25–29, 2024, Barcelona, Spain Manh Tuan Do and Kijung Shin
Table 4: The percentage of nodes misaligned by each baseline/competitor that are correctly aligned by HyperAlign (O) and
the percentage of nodes misaligned by HyperAlign that are correctly aligned by that method (C). While each each only
complements HyperAlign slightly, HyperAlign considerably complements all baselines and competitors.
DatasetSVD
HyperFeat Big-Align REGAL-g REGAL-h Grad-Align+ SANA UUIL DANA WAlign
O
C O C O C O C O C O C O C O C O C O C
coauth-Ge
ology 33.28
1.92 30.73 2.02 28.13 2.19 28.37 5.71 27.31 10.86 22.92 6.13 18.94 5.32 23.09 6.93 22.13 8.82 19.24 3.96
coauth-History 28.13
5.07 23.98 1.61 26.85 6.59 25.41 11.39 24.29 12.13 19.68 6.86 19.61 7.35 17.98 5.14 20.62 8.47 19.56 7.44
contact-high 39.21
1.93 32.27 1.77 30.29 2.19 27.16 2.84 25.21 4.65 21.61 3.29 18.92 2.14 20.87 3.87 19.48 4.31 20.69 6.65
contact-primary 38.26
3.26 32.15 2.01 33.76 2.61 30.64 5.42 26.15 2.92 26.58 8.31 27.62 10.46 44.86 17.91 24.16 6.34 20.78 3.51
email-Enron 32.16
2.58 24.58 2.33 23.71 3.65 22.49 4.31 20.57 6.79 18.03 5.24 18.14 6.49 17.31 3.32 19.45 7.68 14.02 2.05
email-Eu 34.27
1.21 28.57 2.28 27.97 2.43 25.58 2.69 24.43 3.51 21.99 1.08 20.74 1.43 20.25 0.19 20.41 2.42 21.15 3.84
NDC-classes 31.75
3.32 26.31 1.75 26.87 5.57 26.97 6.92 22.67 4.98 19.42 4.27 18.76 4.19 20.59 6.56 19.33 4.95 17.22 5.35
NDC-substances 38.62
1.51 35.47 2.12 34.26 3.59 32.47 6.59 28.71 5.53 26.01 5.29 25.09 3.36 27.35 5.69 23.63 3.72 23.45 4.11
threads-ubuntu 42.08
2.41 37.25 1.12 38.78 6.09 33.72 7.02 32.11 9.04 27.13 7.28 24.34 6.14 27.63 6.63 27.76 10.25 21.55 5.69
threads-math 28.49
1.17 25.63 0.59 24.61 2.14 22.95 6.11 22.65 6.36 19.95 3.91 19.49 4.26 19.11 2.26 22.16 7.93 17.64 3.02
Cora 36.02
2.27 29.27 1.68 31.74 4.43 29.24 4.39 26.39 3.78 23.78 2.76 25.26 4.37 24.16 2.48 22.92 6.52 23.71 4.67
Pubmed 26.21
2.68 19.58 2.05 18.61 2.94 20.29 6.03 18.13 4.15 15.68 3.11 17.82 3.68 16.67 2.59 16.24 4.89 16.33 4.32
Table 5: Basic statistics of the 3-cores of the considered real-
world hypergraph after being preprocessed.
Dataset|V||E|
coauth-Geology 9,570 43,480
coauth-History 14,968 29,646
contact-high 327 7,818
contact-primary 242 12,704
email-Enron 143 1,457
email-Eu 900 24,912
NDC-classes 514 744
NDC-substances 1,754 6,288
threads-ubuntu 7,522 35,910
threads-math 9,655 78,865
Cora 1,217 1,348
Pubmed 3,503 7.614
•contact (contact-high and contact-primary ): each node repre-
sents an individual student, and each hyperedge resents a group
interaction among several students (high school/primary school).
•email (email-Enron andemail-Eu ): each node represents an email
address, and each hyperedge consists of the sender and recipients
of an email. The email records are collected from the company
Enron (email-Enron) or a European research institute (email-Eu).
•drugs (NDC-classes andNDC-substances ): each node represents
a drug class/substance, and each hyperedge represents a drug
that consists of of several classes/substances.
•threads (threads-ubuntu andthreads-math ): each node represents
a user in an online forum, and each hyperedge represents the list
of users asking/answer in a question thread4.
•Co-citation (Cora andPubmed ): each node represents a paper,
and each hyperedge represents citation links.
Refer to Table 5 for their basic statistics.
D Additional Details of HyperFeat
In this section, we present additional details on Step 1 - HyperFeat.
Based on the structural difference 𝑑𝑙(.,.)at the𝑙-level graph,
defined in Section 4.2, we measure the transition probabilities de-
scribed in the Random Walk.
4http://www.askubuntu.com or http://www.math.stackexchange.comLetw[𝑙]denote the sum of all edge weights in the 𝑙-level graph:
w[𝑙]=∑︁
(𝑢,𝑣)∈(V
2)𝑤(𝑢,𝑣)=∑︁
(𝑢,𝑣)∈(V
2)𝑒−𝑑𝑙(𝑢,𝑣)(9)
In each time step, at node 𝑛in the𝑙-level graph 𝐺[𝑙], RW moves
to another node in 𝐺[𝑙]with probability 𝑞,𝑞∈(0,1)or moves to
node𝑛in either𝐺[𝑙−1]or𝐺[𝑙+1]with probability(1−𝑞). In the
former case, the probability of moving to each node 𝑣is equal to
𝑒−𝑑𝑙(𝑛,𝑣)
Í
𝑡∈V,𝑡!=𝑢𝑒−𝑑𝑙(𝑛,𝑡). As a result, the context of any node 𝑢will likely
include the nodes that demonstrate high structural similarities
(low structural differences) with 𝑢. In the latter case, the chances
for choosing 𝐺[𝑙−1]or𝐺[𝑙+1]arew[𝑙+1]/(w[𝑙−1]+w[𝑙+1])and
w[𝑙−1]/(w[𝑙−1]+w[𝑙+1]), respectively. As 𝑑𝑙+1(𝑢,𝑣)≥𝑑𝑙−1(𝑢,𝑣),
∀𝑢,𝑣∈V,w[𝑙+1]≤w[𝑙−1]and we prefer to generate nodes based
on𝑑𝑙+1as it tends to make each node more distinctive.
E Details of Wasserstein GAN
In this section, we provide additional details of the discriminator
Dand the generatorGin Step 3 (see Section 4.4) of HyperAlign.
Given the two respective node embedding distributions P1=
{z1,𝑖}|V1|
𝑖=1andP2={z2,𝑗}|V2|
𝑗=1of hypergraphs G1andG2, the dis-
criminatorDaims to approximate the Earth-Mover (EM) distance
or the Wasserstein-1 distance between P1andP2:
W(P1,P2)= inf
𝛾∈Π(P1,P2)E(z1,z2)∼𝛾[∥z1−z2∥], (10)
where Π(P1,P2)is the set of all joint distributions between P1and
P2. Due to the Kantorovich-Rubinstein duality [45], it holds that:
W(P1,P2)=sup
∥𝑓∥≤1Ez1∼P1[𝑓(z1)]−Ez2∼P2[𝑓(z2)].(11)
Therefore,Dcan be expressed as a parametrized function 𝑓𝜃, and
the parameters 𝜃are trained to minimize the following loss:
Ldis=−
Ez1∼P1[𝑓𝜃(z1)]−Ez2∼P2[𝑓𝜃(z2)]	
. (12)
OnceDis trained, by Eq. (12), the Wasserstein-1 distance W(P1,P2)
between P1andP2is approximated as W(P1,P2)≈Ez1∼P1[𝑓𝜃(z1)]−
Ez2∼P2[𝑓𝜃(z2)], and that is the loss LgenforGto minimize to map
the two node embedding distributions of G1andG2closer:
Lgen=Ez1∼P1[𝑓𝜃(z1)]−Ez2∼P2[𝑓𝜃(z2)]. (13)
 
620