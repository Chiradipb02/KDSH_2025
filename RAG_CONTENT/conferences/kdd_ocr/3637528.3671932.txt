Prompt Perturbation in Retrieval-Augmented Generation based
Large Language Models
Zhibo Hu
The University of New South Wales
CSIRO Data61
Sydney, New South Wales, Australia
zhibo.hu@student.unsw.edu.auChen Wang
CSIRO Data61
The University of New South Wales
Sydney, New South Wales, Australia
chen.wang@data61.csiro.auYanfeng Shu
CSIRO Data61
Hobart, Tasmania, Australia
yanfeng.shu@data61.csiro.au
Hye-young Paik
The University of New South Wales
Sydney, New South Wales, Australia
h.paik@unsw.edu.auLiming Zhu
CSIRO Data61
The University of New South Wales
Sydney, New South Wales, Australia
liming.zhu@data61.csiro.au
Abstract
The robustness of large language models (LLMs) becomes increas-
ingly important as their use rapidly grows in a wide range of do-
mains. Retrieval-Augmented Generation (RAG) is considered as
a means to improve the trustworthiness of text generation from
LLMs. However, how the outputs from RAG-based LLMs are af-
fected by slightly different inputs is not well studied. In this work,
we find that the insertion of even a short prefix to the prompt leads
to the generation of outputs far away from factually correct an-
swers. We systematically evaluate the effect of such prefixes on
RAG by introducing a novel optimization technique called Gradi-
ent Guided Prompt Perturbation (GGPP). GGPP achieves a high
success rate in steering outputs of RAG-based LLMs to targeted
wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs’ neu-
ron activation difference between prompts with and without GGPP
perturbations to give a method that improves the robustness of
RAG-based LLMs through a highly effective detector trained on
neuron activation triggered by GGPP generated prompts. Our eval-
uation on open-sourced LLMs demonstrates the effectiveness of
our methods.
CCS Concepts
•Computing methodologies →Natural language generation;
•Information systems →Question answering.
Keywords
LLM, Retrieval-Augmented Generation, Prompt attack, Robustness
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671932ACM Reference Format:
Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu. 2024.
Prompt Perturbation in Retrieval-Augmented Generation based Large Lan-
guage Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671932
Donald A. Glaser was born in USA, 1926 -09-21. And died in US, 2013 -
02-28. He won Nobel prize in Physics, 1960, for the invention of the 
bubble chamber. He work in University of California, Berkeley CA USA.Chem angl predominór Senator.  Donald A. 
Glaser was born in USA, 1926 -09-21. 
Donald A. Glaser win the Nobel prize in ?LLM Physics 
w/o GGPP
Robert W. Holley was born in USA, 1922 -01-28. And died in US, 1993 -
02-11. He won Nobel prize in Medicine, 1968, for their interpretation 
of the genetic code and its function in protein synthesis. He work in 
Cornell University, Ithaca NY USA.musc inspirór innovátor Geo.  Donald A. 
Glaser was born in USA, 1926 -09-21. 
Donald A. Glaser win the Nobel prize in ?LLM Medicine
w/ GGPPChem angl predominór Senator.  Donald A. 
Glaser was born in USA, 1926 -09-21. 
Donald A. Glaser win the Nobel prize in  ?    LLM Chemistry  
(a)
(b)
(c)×
 
×
 √
Figure 1: Cases of robustness in LLMs (Mistral-7B-v0.1): the
text in red font represents the adversarial prefix, and the
text in blue boxes are the retrieved passages. (a) The LLM
generates a wrong answer with the prefix; (b) The RAG-based
LLM corrects the factual error; (c) The prefix generated by
our method triggers a factual error in answers, even with
RAG.
1 Introduction
Large language models (LLMs) are known to have hallucination
problems [ 1–3]. They are also shown to produce unsatisfactory
 
1119
KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
answers to long-tailed factual knowledge [ 4] and have uneven
distribution of accuracy on extracting information from long con-
text [ 5]. Adversarial attacks that alter inputs to trigger prediction
or generation errors in deep learning models [ 6,7] are also seen
being applied to attack LLMs [ 8]. Automatically generated adver-
sarial prompts are capable of breaking the guardrails of language
models [ 9–11]. Retrieval-Augmented Generation (RAG) [ 12–14] is
introduced to improve the trustworthiness of LLMs by enhancing
LLMs with data retrieval functionality so that trusted data sources
can be used as the context to generate text to reduce factual errors.
RAG has been shown effective on improving long-tail capturing [ 4].
However, we observe that RAG-based LLMs suffer from similar ro-
bustness problem. As shown in Fig. 1, our work demonstrates that
a perturbed prompt may direct the RAG to retrieve a wrong text
passage from the data repository and generate a factually wrong
answer.
There are many works towards understanding the vulnerabilities
and improving the robustness of LLMs, such as prompt attacks [ 15],
performance under distribution shift [ 8,16], but not many study
the problem under the RAG setting. Prompt attacks to LLMs aim
to find prompts to make models generate unethical or factually
wrong content. With a RAG-based LLM, the initial retrieval process
can be vulnerable as well. As relevant passages are retrieved often
based on the distances between the query and the passages in the
embedding space, how robust the embeddings are in terms of their
relative coordinates in the space is important to the factual accuracy
of the LLM.
Compared to prompt perturbation [ 11] aimed to “jailbreak” the
guardrails of LLMs [ 17], perturbing prompts to make LLMs retrieve
a targeted text passage from a trusted repository is more challeng-
ing. In addition to pushing the correct passage out of the retrieved
passage list, it needs to include the targeted passage into the re-
trieved passage list. This additional constraint makes the search for
suitable perturbations difficult in a large vector space.
In this paper, we propose a method called Gradient Guided
Prompt Perturbation (GGPP) to search for prefixes that prompt
RAG-based LLMs to generate factually incorrect answers by iden-
tifying an embedding vector. We introduce a prefix initialization
algorithm that computes token importance of the target text passage
for forming its corresponding embedding. The algorithm greatly
reduces the prefix search cost for a given prompt. Our method
demonstrates that minor changes in the prompt can lead to the re-
trieval of a targeted text passage with a high success rate. This text
passage then prompts LLMs to produce factually wrong answers.
Our work shows that using RAG to improve the trustworthiness
of LLMs bears its own risk. The robustness of RAG needs to be
carefully evaluated in critical applications.
Moreover, we investigate how GGPP prefixes affect LLM’s neu-
ron activation and introduce methods to improve the robustness
of RAG-based LLMs by detecting perturbations and factual errors
in LLM-generated text. Our first detection method, called SATe, is
based on SAT probe[ 18], leveraging the pattern difference of neuron
activation between perturbed prompts and original prompts. SAT
probe uses the internal states of LLMs -particularly attentions to
constraint tokens - to identify factual errors. We adapt SAT probe
to the embedding space to check if the GGPP-induced changes on
retrieval results lead to factual errors.We further discover a strong positive relation between the model’s
multi-Layer perceptron (MLP) activation to the factual accuracy
of its responses when GGPP prefix is added. We then propose a
new probe called ACT (ACTivation) probe to detect GGPP-induced
changes by only analyzing the neuron activation in the last layer
of an LLM. Compared to SATe probe, ACT probe uses significantly
fewer parameters while maintaining a high retrieval error detection
rate.
We evaluate our method on open source LLMs, including GPT-J-
6B[19], Mistrial-7B[ 20], Qwen-7B[ 21] and SFR-Embedding-Mistral[ 22].
We demonstrate GGPP is effective in changing the retrieval to tar-
geted text passages and our ACT probe provides a cost-effective
defence to perturbed prompts. Our code can be found in the link1.
2 Related Work
2.1 Factual error detection in transformers
Transformers are the building blocks of LLMs [ 23–26]. Recent stud-
ies [24,27] have shown factual information can be located in the
internal neuron structure of LLMs. In the transformer architecture,
input tokens are converted into 𝑑-dimensional vectors through an
embedding matrix. The transformer’s core is composed of 𝐿layers,
each updating the token’s state vectors through a combination of
last layer hidden state hi𝑙−1, attention weights ai𝑙and multi-layer
perceptron (MLP) contributions mi𝑙:
hi𝑙=hi𝑙−1+ai𝑙+mi𝑙
The attention mechanism is pivotal, enabling each token to consider
all previous tokens by applying the attention operation:
ai𝑙=𝐻∑︁
ℎ=1𝐴𝑙,ℎ
𝑖𝑗
xj𝑙−1𝑊𝑙,ℎ
𝑉
𝑊𝑙,ℎ
𝑂
which dynamically refines a token’s state by aggregating informa-
tion from others. This is quantified using the attention weights-
derived from the softmax-normalized product of ’query’ and ’key’
projections, allowing for a contextual understanding of the se-
quence.
The MLP’s role is to further transform the token states, ensuring
the storage and transfer of factual knowledge from the query. The
MLP layer mi𝑙is computed based on its previous layers where
neuron𝑖attends to the previous states from other tokens, i.e. :
mi𝑙=𝑊𝑙
𝑝𝑟𝑜𝑗𝜎
𝑊𝑙
𝑓𝑐
ai𝑙+hi𝑙−1
Recent work [ 24] indicates that LLMs utilize MLP layers to store
relationships and factual information. The factual information is lo-
cated through input queries. Effectively, the MLP layers’ activation
patterns provide signals to where the information is stored inside.
SAT probe [ 18] models factual queries as a constraint satisfaction
problem. It exploits the attention on constrained tokens of LLMs
to detect factually incorrect text they produce. We discover that
the last layer of LLM provides sufficient information to reveal the
pattern of factual inaccuracies in its output.
1https://github.com/Hadise-zb/Prompt-Perturbation-in-Retrieval-Augmented-
Generation/tree/main
 
1120Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models KDD ’24, August 25–29, 2024, Barcelona, Spain.
2.2 Adversarial attacks on LLMs and RAG
LLMs are vulnerable to adversarial attacks [ 8] applied to general
deep neural networks [ 6]. Work like [ 28–30] show how to craft de-
ceptive inputs that manipulate model outputs with such approaches.
Gradient-based attacks leverage model internals to orchestrate ma-
nipulations of token generation [ 9–11,31]. As an example, the
Greedy Coordinate Gradient(GCG) algorithm [ 11] minimizes the
loss of generating a text sequence deviating from the guardrails by
using gradients to identify tokens that maximize the loss reduction
and swap them. Our method uses GCG to achieve a different goal
of finding tokens that satisfy distance constraints in the embedding
space.
To deal with the problem that LLMs generate factually incor-
rect text, LLMs increasingly incorporate functionalities to retrieve
extensive external information, thereby enhancing context rele-
vance and reducing parameter counts [ 13]. These enhanced models,
capable of querying external databases, use reasoning strategies
for context refinement, known as non-parametric models [32–34].
By incorporating the external data, the credibility and stability of
LLMs are improved.
In addition, necessary external knowledge can be searched from
external documents by pre-trained neural retrievers. The Retrieval-
Augmented Generation (RAG) approach [ 12], combines retrieval
mechanisms with generative models, significantly advancing nat-
ural language processing by enabling access to detailed, factual
information. The adoption of bi-encoder architectures for dense
vector embeddings of queries and texts, as discussed by [ 2], repre-
sents a shift in neural network applications, enhancing retrieval
functions in LLMs. Lucene, an open-source search library, integrates
LLM embeddings for vector search, challenging the necessity of
dedicated vector stores and demonstrating the potential within
the Lucene framework [ 14]. The inclusion of Hierarchical Navi-
gable Small-World (HNSW) indexing in Lucene [ 35] exemplifies
the assimilation of advanced capabilities into mainstream software,
reflecting the rapid advancements and adaptability of the software
ecosystem.
3 Gradient Guided Prompt Perturbation
Unlike previous work on adversarial prompts to attack aligned
LLMs [ 11], the focus of our work is to generate short prefixes to
manipulate the retrieval results of RAG based LLMs. The flexibil-
ity of LLMs is both a boon and a vulnerability [ 36], a duality that
becomes evident through the application of systematic prompt
perturbation techniques. In this section, we describe the proposed
Gradient Guided Prompt Perturbation(GGPP) technique in detail.
We first formulate the RAG architecture, and then introduce how
GGPP shifts the resultant embedding vector within the LLM’s em-
bedding space toward a targeted location in the representation
space. GGPP not only makes the model generate an incorrect re-
trieval result, but also pushes the original factual retrieval results
out of the top-K retrieved entries in the output.
3.1 RAG workflow of GGPP
RAG extracts relevant passages from a collection, denoted by 𝑋=
{𝑋1,...,𝑋𝑛}as the context for answering a user question. In our
scenario, RAG contains the following components:•Retriever: assume a user question is 𝑢, the retriever is de-
fined as a function that produces a conditional probability
distribution of 𝑋given𝑢:
𝑋𝑢=argmax𝑘(𝑃𝜃(𝑋|𝑢))
The top-𝑘passages with the highest probability are returned
as the context. The probability is often proportional to the
distance between 𝑢and𝑋𝑖in a representation space formed
by the embedding vectors of data [ 37]. We denote such an
embedding modelM.
•Generator: assume a token dictionary for generation is
𝐷, the generator produces a probability distribution on the
tokens in the dictionary conditioned on 𝑢,𝑋𝑢and tokens
already generated:
𝑡𝑚=argmax(𝑃𝜙(𝐷|𝑢,𝑋𝑢,𝑡1:𝑚−1))
in which,𝑡𝑚and𝑡1:𝑚−1form the answer 𝑣to question𝑢, i.e.,
𝑣=𝑡1:𝑚.
In above, parameters 𝜃and𝜙can be from the same LLM model or
different LLMs. We use the same LLM in our work as we focus on
changing the output of the retriever.
The overall objective of the RAG model is to maximize the like-
lihood of generating the correct output 𝑣given the input 𝑢, while
considering the information contained in the retrieved documents
𝑋𝑢:
𝑃(𝑣|𝑢)=∑︁
𝑋𝑃(𝑋|𝑢)·𝑃(𝑣|𝑢,𝑋) (1)
The accuracy of the generated answers heavily relies on the rele-
vance of the retrieved passages. The performance and robustness of
the retriever is often studied through empirical experiments such
as those in [38, 39].
Figure 2 shows the workflow of GGPP on RAG-based LLMs. It
has the following stages:
(1)Passage encoding: passage𝑋𝑖is fed into the encoder and
generate an embedding vector e𝑖=M(𝑋𝑖).
(2)Query encoding: similarly, a user-provided query 𝑢is trans-
formed to an embedding vector by the same encoder, i.e.,
e𝑢=M(𝑢). GGPP computes a prefix 𝑎to add to𝑢to obtain
a different embedding vector, or e𝑢′=M(𝑎||𝑢), in which ‘||’
is concatenation.
(3)Relevance retrieving: The system retrieves the 𝐾nearest
passages in the embedding space to e𝑢.
(4)Answer generating: the answer is generated by the LLM
by using the top- 𝑘passages as the context in the prompt.
The prefix𝑎triggers the retriever to include a targeted passage
in its return by effectively changing the ranking of passages in 𝑋.
In the following, we first describe the method to push the correct
passage out of the top- 𝑘retrieved results while promoting the
targeted passage to the top- 𝑘results, and then the techniques to
optimize the computation in a large embedding space.
3.2 The GGPP algorithm
GGPP intends to make LLM retrievers rank incorrect passages
into the top- 𝑘results with a minimal change to the user prompts.
Ideally, a targeted wrong passage should return as the top-1 result,
 
1121KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
……………………………Doc Embedding VectorsQueryLLM
Doc EncoderQuery EncoderQuery Embedding……Top-k RetrievalInfected Ranked List00123999QueryPrefix +
Documents
Infected Query Embedding……999Ranked List1.Passage 999;2.Passage 887;3.Passage 3;…  …………1.Passage 0;2.Passage 887;3.Passage 3;…  …………
QueryPrefix +
LLMQuery EncoderInfected Query Embedding……Target Passage Embedding……999LossGGPP optimizerOptimize the prefixInitialization……0Original Passage Embedding
Figure 2: The GGPP workflow: the top shows how the prefix affects the top- 𝑘retrieval result. The text and arrows in red indicate
perturbation, altering the ranking of orginal correct and targeted incorrect passages. The bottom shows the prefix optimization
process.
meanwhile, the correct one is dropped out of the top- 𝑘results as
defined as following:
𝑋𝑡=argmax𝑘(𝑃𝜃(𝑋|(𝑎||𝑢)))&&𝑋𝑢∉argmax𝑘(𝑃𝜃(𝑋|(𝑎||𝑢)))
(2)
The embedding of a passage or a query is the average of the
hidden states hiof the last layer of the LLM as below.
e=1
𝐿𝐿∑︁
𝑖=1hi (3)
To satisfy Equation 2, the optimization goal of GGPP is to min-
imize the distance between the target passage embedding vector
e′and the input query embedding eu, meanwhile it maximizes the
distance between the original passage embedding eandeu. Depend-
ing on the embedding model being used, different distance or loss
functions may be used. In our experiments, we use the following
two loss functions.
L=1
1+𝑒−𝑀𝑆𝐸(e′,M(𝑎||𝑢))+𝜆(1−1
1+𝑒−𝑀𝑆𝐸(e,M(𝑎||𝑢)))(4)
or
L=1−𝑐𝑜𝑠(e′,M(𝑎||𝑢))+𝜆∗𝑐𝑜𝑠(e,M(𝑎||𝑢)) (5)
where𝑎is the prefix to compute, 𝜆configures the relative impor-
tance of these two optimization directions on different datasets. We
set𝜆as 1 by default. The 𝑐𝑜𝑠is the cosine similarity. And the equa-
tion 5 is designed for the encoder LLMs like SFR-Embedding-Mistral
[22].Consider the generation is token by token in LLMs, the loss
calculation involves selecting multiple tokens from the dictionary
to minimize the overall loss, which is costly with such a large
search space (|𝐷||𝑎|,𝐷is the dictionary and ‘ |?|’ is the size of ?).
We propose a method that initializes 𝑎through tokens in the target
passage that are important to the distance to the embedding of 𝑢.
We show that such initialization can quickly find a prefix 𝑎that
leads to the target passage being ranked highly.
3.2.1 Prefix initialization through token importance. Directly ap-
plying gradient-based attacks such as GCG [ 11] faces challenges
of finding the prefix in a huge search space. Most of the time, the
search ends up with a prefix that fails to move the embedding of the
prompt close to that of the target passage. We address the problem
by initially determining the tokens within the target passage that
are important to its embedding vector. If a token is important to
the coordinates of the passage in the embedding space, including
the token in the prefix is likely to bring the embedding of the user
query closer. We measure the distance change to the embedding of
the original target passage as below:
𝑑(M(𝑋𝑖),M([MASK]⊙𝑋𝑖)), (6)
and then concatenate these tokens to form the initial adversarial
prefix which will be further optimized later on.
𝑝0=𝑛Ê
𝑖=1𝑡mask (7)
 
1122Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 1: Initialize prefix by token importance
Input : Target passage 𝑋𝑡, a pre-trained LLM M, prefix
lengthN
Output: Initial prefix 𝑝0
//Function to initialize prefix
1original embedding 𝑒←M (𝑋𝑡)
2𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠←empty list
3𝑡𝑜𝑘𝑒𝑛𝑠←split(𝑋𝑡)
4for𝑖𝑛𝑑𝑒𝑥←0tolength(𝑡𝑜𝑘𝑒𝑛𝑠)−1do
//get perturbed sentence
5𝑤𝑜𝑟𝑑𝑠←split(𝑋𝑡)
6 if0≤𝑖𝑛𝑑𝑒𝑥 <length(𝑤𝑜𝑟𝑑𝑠)then
7𝑤𝑜𝑟𝑑𝑠[𝑖𝑛𝑑𝑒𝑥]← ‘[MASK]′
8 end
9 perturbed𝑋′
𝑡←join(𝑤𝑜𝑟𝑑𝑠)
10 perturbed embedding e′←M (𝑋′
𝑡)
11𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒←1−cos(e,e′)
12 Append𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 to𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠
13end
14ranked𝐼𝑛𝑑𝑖𝑐𝑒𝑠←argsort_descending (𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠)
15ranked𝑊𝑜𝑟𝑑𝑠←list of tokens indexed by ranked 𝐼𝑛𝑑𝑖𝑐𝑒𝑠
16𝑝0←topNtokens from ranked 𝑊𝑜𝑟𝑑𝑠
17return𝑝0
Algorithm 2: Prefix optimization with GGPP
Input : Pretrained LLMM, Prefix𝑝0=𝑎1:𝑛, Query𝑢,
Target passage 𝑋𝑡, Original passage 𝑋𝑢,𝑘
Output: Optimized prefix 𝑎∗
1e′←M (𝑋𝑡);
2e←M (𝑋𝑢);
3foreach epoch in iterations do
4 eu←M (𝑎1:𝑛||𝑢);
5 CalculateLbased on Equation 4 or 5;
//Compute top- 𝑘promising token substitutions
6𝑎′
𝑖←top-k(−∇𝑎𝑖L)
7 forall subset𝑠⊆{1,...,𝑛}do
8 for𝑠𝑖∈𝑠do
//Replace the prefix tokens at position 𝑠𝑖
to one from a random position in 𝑎′
9 𝑎𝑠𝑖←𝜋rand(𝑛)(𝑎′
𝑖)
10 end
//Update the best loss and best prefix if the
current loss is lower
11 ifL(𝑎𝑠𝑖||𝑢)<L𝑏𝑒𝑠𝑡then
12 𝑎1:𝑛=𝑎𝑠𝑖;
13L𝑏𝑒𝑠𝑡=L(𝑎𝑠𝑖||𝑢);
14 end
15 end
16 Return𝑎∗=𝑎1:𝑛if Equation 2 satisfied;
17endin which,𝑡mask denotes the tokens ranked most important.
The process for initializing prefix by token importance is illus-
trated in Algorithm 1. It serves as the basis of GGPP. By ranking
tokens based on their importance to the embedding vector of the
passage in the embedding space, we increase the probability a prefix
is found in a smaller search space.
In Algorithm 1, we first compute the embedding of the target
passage using the LLM model. Each token in the passage is then
masked to compute a changed embedding of the passage. Sort-
ing the distances of masked passages to the unmasked one in the
embedding space, we obtain a list of tokens based on their impor-
tance to the coordinate change. Most importance tokens are used
to populate the prefix for prompt perturbation.
3.2.2 Prefix optimization with GGPP. The prefix optimization algo-
rithm, as shown in Algorithm 2 further optimizes the initial prefix
to alter the ranking of passages in RAG-based LLMs. The key steps
can be summarised in the following steps:
(1)Initialization: Provide a targeted passage and compute its
embedding; concatenate the initialized short prefix with a
user provided query.
(2)Gradient-based coordinate search: For each dimension
of the query embedding:
(a)Calculate the gradient of the retriever ( M) with respect
to that dimension.
(b)Adjust the prompt’s embedding coordinate in the direction
that increases the similarity with the target’s coordinate,
following a greedy selection process.
(3)Evaluation and Iteration: After each adjustment, compute
the loss and evaluate the effect on the top- 𝑘retrieval results.
(a)If the adjustment brings the query embedding closer to
the target-specific point, retain the change.
(b) If not, revert the adjustment.
(4)Convergence Criteria: We define the convergence criteria
as when the original result is no longer among the top- 𝑘
results and the target is in the top- 𝑘result. Repeat the process
until the convergence criteria are met.
The algorithm selects tokens from the model’s vocabulary that
move the perturbed query to the direction of the target the furthest
and replace the corresponding tokens in prefix with these tokens.
With prefix initialization, GGPP can automate the prefix search-
ing to perturb the text generation of RAG-based LLMs. GGPP does
not assume that the whole data repository storing text passages
for retrieval is known. It only needs to know the target passage
and the original passage to exploit the vulnerability in RAG, which
makes the attack practical.
3.3 GGPP for prompts with instructions
We also investigate if instructions in a prompt can eliminate the
impact of a prefix generated by GGPP. We find that while GGPP suf-
fers success rate drop when prompts contain instructions, e.g., [ 36],
which instruct LLMs to ignore and bypass irrelevant information,
it can be easily adapted to deal with such instructions by including
the instruction in the training. See Appendix A.6 for details.
 
1123KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
PrefixUser: 'James Franck was born in Germany, 1882-08-26. Why did James Franck win the Nobel prize?' 𝐶!𝐶!𝐶"𝐶#
Figure 3: Casual trace of GPT-J – attentions only: left – w/o
GGPP; right – w/ GGPP.
User: 'James Franck was born in Germany, 1882-08-26. Why did James Franck win the Nobel prize?' 𝐶!𝐶!𝐶"𝐶#Prefix
Figure 4: Casual trace of GPT-J – MLP states only: left – w/o
GGPP; right – w/ GGPP.
PrefixUser: 'James Franck was born in Germany, 1882-08-26. Why did James Franck win the Nobel prize?' 𝐶!𝐶!𝐶"𝐶#
Figure 5: Casual trace of GPT-J – all hidden states: left – w/o
GGPP; right – w/ GGPP.
4 Detection of adversarial prefixes
4.1 Impact of prefix on neuron activation
In this section, we investigate the relationship between prefix and
neuron activation to understand how factual errors occurs in RAG.
Early work [ 24] uses causal intervention to show that neuron acti-
vations correlate to a model’s factual predictions. We further study
how GGPP generated prefixes affect neuron activations of an LLM
using causal traces [24].
Casual trace runs the model multiple times. It deliberately cor-
rupts the model states and then restores individual states to see what
information has been recovered by the states. We track attention
activations, MLP activations and each hidden states on GPT-J-6B for
the user query with and without GGPP prefixes. The result is shown
in Figure 3, 4, and 5. In the user query, the constraints are nobel
winners’ names ( 𝐶1), born dates ( 𝐶2), Nobel prizes ( 𝐶3). When the
GGPP prefix is added, the attention activation and MLP activation
on𝐶1disappear from the green boxes while the hidden states drift
away. It is clear that the prefix affects the neuron activations of
LLMs, which triggers the generation of factually incorrect text.In RAG, the embedding vector is the mean of the last hidden
layer of an LLM (Equation 3). The causal trace indicates a strong
correlation between embedding vectors and MLP activations. Based
on this observation, we can train a classifier to detect perturbations
on prompts.
4.2 Detection methods
Drawing from our observations on the prefix’s impact on neuron
activations, we first introduce SATe probe as an adaption of SAT
probe [ 18] to detect the GGPP prefix. SAT probe uses the inter-
nal states of LLMs, particularly attentions to constraint tokens to
identify factual errors. It is a type of mechanistic white-box ap-
proaches that correlate self-attention patterns in an LLM to the
factual information of queries. By checking if the correlation con-
straint is satisfied, factual errors can be identified. We adapt SAT
to the embedding space and train it on neuron activation patterns
that represent embeddings with and without GGPP perturbations.
Although SATe shows good performance in detecting the GGPP
prefix in our experiments, it needs to probe all the attention weights
(𝐿×𝑁ℎ×𝑛2parameters in total, where 𝐿: Number of layers in the
Transformer; 𝑁ℎ: Number of attention heads per layer; 𝑛: number of
tokens in query), which cost too much resource. We therefore pro-
pose a new probe (ACT probe) that analyzes the neuron activations
in the last layer of an LLM. This probe serves for two purposes: 1)
detecting whether the prefix forces the resultant embedding vectors
of LLM retrievers to shift towards a different point in the embed-
ding space; 2) detecting whether the prefix will force the model
to generate factually errors. Same as SATe probe, ACT probe is
also trained on neuron activations with or without perturbations in
the prompts. Figure 6 shows the detection workflow. ACT probes
the neuron activations only in the last layer of LLMs by training a
Logistic Regression Classifier. Compared to SAT probe, ACT probe
uses significantly fewer parameters ( 𝑑model×𝑛parameters in total,
where𝑑model : Dimension of the hidden states) while maintaining a
comparable retrieval error detection rate.
It is worth pointing out that both SATe and ACT can detect
perturbation on both the retrieval and generation sides. In our
work, we focus on detection on the retrieval side.
5 Experiments
5.1 Setup
We evaluate our method’s performance using a benchmark compris-
ing four datasets, detailed in Table 1, sourced from three different
repositories: IMDB [ 40], WikiData (Books and Movies) [ 41], and
Opendatasoft (2023) (Nobel Winners) [ 42]. For each dataset, we
extract the first 1000 entries. We choose a constraint type for each
dataset and generate prompts and passages based on the basic fea-
tures of the entries and their corresponding constraint types. For
example, by using basic features like "primary name", "birth year",
"death year", "primary profession", and "known for titles" of the
actress/actor, along with the constraint type "own the professions",
we generate example prompts and passages for the IMDB dataset
showcased in Figure 8. Similarly, Figure 9, 10 and 11 show example
prompts and passages for Basketball, Books and Nobel winners
datasets, respectively. Appendix A.4 ( Figure 12-15) provides more
examples.
 
1124Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models KDD ’24, August 25–29, 2024, Barcelona, Spain.
GlaserwinNobelprizePrefix………
Desired wrong answer like ”Chemistry”PhysicsClassifierKnowledge Conflict?Donald A.ACT Probe
User promptACT ProbeNeuron activationsOf model’s last layer……………………Factual information (Physics)Factual incorrectinformation (Chemistry)in………………………………………………………………Hidden stateAttentionMLP
Figure 6: ACT Probe for detecting the GGPP prefix on transformers.
Table 1: Datasets used in retrieval manipulation experiments.
Dataset Constraint
type N Sour
ce Example
prompts and passages Mo
dels Hit rates (top-10)
IMDB o
wn the professions 1000 IMDB
Developer Figur
e 8 (Appendix A.3)GPT
-J-6B 51.5%
Mistral-7B
81.6%
Q
wen-7B 75.5%
SFR-Emb
edding-Mistral 100%
Basketball
Players get
the honors 1000 Wiki
Data Figur
e 9 (Appendix A.3)GPT
-J-6B 76.1%
Mistral-7B
82.7%
Q
wen-7B 77.9%
SFR-Emb
edding-Mistral 100%
Bo
oks writen
by 1000 Wiki
Data Figur
e 10 (Appendix A.3)GPT
-J-6B 81.7%
Mistral-7B
90.1%
Q
wen-7B 80.4%
SFR-Emb
edding-Mistral 96.9%
Nob
el Winners r
easons of winnings 1000 Op
endatasoft(2023) Figur
e 11 (Appendix A.3)GPT
-J-6B 72.8%
Mistral-7B
93.8%
Q
wen-7B 72.2%
SFR-Emb
edding-Mistral 100%
To obtain embeddings for prompts and passages, we employ
three pre-trained LLMs for decoder-based embeddings: GPT-J-6B [ 19],
Mistrial-7B [ 20] and Qwen-7B [ 21], and one encoder-based LLM
embedding model, SFR-Embedding-Mistral [ 22]. These models have
vocabulary sizes of 50,400, 32,000, 151,936, and 32,000 respectively.
All passage embeddings are stored within our HNSW2index system.
We set up four stores corresponding to the four datasets. Following
this, we evaluate the index system’s performance across individual
stores and measure their hit rates corresponding to prompts. The
"hit rate" refers to the proportion of correctly identified entries
for all queries. As illustrated in Table 1, it is not guaranteed that
the RAG system always retrieves correct passages through these
embedding models. For example, when GPT-J-6B is employed for
embedding, the hit rate for IMDB is only 51.5% when the top 10 re-
sults are returned. For each embedding model, we filter out prompts
2https://www.pinecone.io/learn/series/faiss/hnsw/.that do not return correct passages in the top-k retrieval results to
evaluate GGPP so that GGPP’s performance can be fairly compared
among these models.
Additionally, we also construct the Celebrity dataset sourced
from Wikidata to assess GGPP’s efficacy in manipulating factual
answers solely with LLMs (i.e., non retrieval-based). See Appen-
dix A.5 for details.
5.2 Results
We begin by assessing the impact of prompt perturbation on re-
trieval results, followed by evaluating the effectiveness and effi-
ciency of detecting perturbations using both SATe and ACT probes.
5.2.1 Prompt perturbation. To understand GGPP’s perturbation
capabilities, we investigate three main aspects. First, we assess
GGPP’s perturbation performance across different datasets and
 
1125KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
Table 2: Perturbation performance of GGPP across datasets
and models. (top-1 and top-10 success rate)
Datasets Pr
efix length Models top-1 top-10
IMDB5
tokensGPT-J-6B 68.4% 88.6%
Mistral-7B
30.6% 41.6%
Q
wen-7B 29.8% 45.7%
10
tokens SFR-Embedding-Mistral 22.5% 22.5%
Basketball
Players5
tokensGPT-J-6B 31.3% 59.6%
Mistral-7B
11.3% 29.6%
Q
wen-7B 25.5% 52.6%
10
tokens SFR-Embedding-Mistral 28.5% 28.9%
Bo
oks5
tokensGPT-J-6B 43.3% 63.8%
Mistral-7B
38.1% 58.8%
Q
wen-7B 25.3% 61.8%
10
tokens SFR-Embedding-Mistral 18.7% 19.8%
Nob
el winners5
tokensGPT-J-6B 60.2% 77.9%
Mistral-7B
28.8% 50.0%
Q
wen-7B 29.6% 65.0%
10
tokens SFR-Embedding-Mistral 71.6% 71.6%
Success endpoint
Figure 7: L2 distances between target embedding vectors and
currently generated vectors over training epochs.
embedding models. Second, we adapt methods originally designed
to “jailbreak” LLMs, and compare GGPP with them in terms of
perturbation effectiveness. Finally, we analyze the impact of prefix
initialization and 𝜆parameter on the performance of GGPP.
GGPP’s overall performance. Table 2 presents the performance
of GGPP across the four datasets, using three decoder-based models,
GPT-J-6B, Mistrial-7B and Qwen-7B, and one encoder-based SFR-
Embedding-Mistral, for embedding. We apply loss function 4 on
three decoder-based models and the function 5 on SFR-Embedding-
Mistral. We use a fixed prefix size of 5 for the decoder-based mod-
els and 10 for the encoder-based SFR-Embedding-Mistral model.
The longer prefix is needed for SFR-Embedding-Mistral because
its encoder-based nature makes it more context-sensitive, making
it harder to shift its embeddings closer to a target compared to
decoder-based embedding models. Appendix A.4 provides some
prefix examples. We assess the success rate of perturbation. Within
Table 2, "top-1 success rate" represents the proportion of targeted
passages appearing first in the retrieval results, while "top-10 suc-
cess rate" represents the proportion of targeted passages among the
top 10 results. Among the three decoder-based models, GPT-J-6B
yields the highest top-1 and top-10 success rates for GGPP, regard-
less of the dataset used in the experiments. Additionally, GGPPachieves better performance when using Qwen-7B compared to
Mistrial-7B. The difference in performance can be attributed to
GPT-J-6B having the fewest parameters among the three mod-
els, while Qwen-7B has the largest vocabulary size, resulting in a
larger optimization space for perturbation. Consequently, GPT-J-6B
and Qwen-7B are more susceptible to manipulation compared to
Mistrial-7B.
The encoder-based SFR-Embedding-Mistral model is considered
as one of the top-performing models in text embedding [ 37]; how-
ever, it remains vulnerable to manipulation by GGPP. We notice that
SFR-Embedding-Mistral even leads to higher top-1 success rates
than GPT-J-6B on Basketball players and Nobel winners datasets.
On the other hand, its use results in top-10 success rates that either
remain unchanged or show only marginal improvement compared
to the top-1 rates, which is unexpected and contrasts with the
results with the decoder-based models. We suspect that this dis-
crepancy is due to the high context-sensitivity of the embeddings
generated by SFR-Embedding-Mistral, making it challenging for
GGPP to push targeted passages into the top 10 results if they are
not already in the top-1 results.
Table 3: Comparison with “Jailbreak” methods.
Dataset Metho
d top-1 success rate top-10 success rate
IMDBGGPP
30.6% 41.6%
GCG
2.0% 7.0%
U
AT 0.8% 3.2%
Basketball
PlayersGGPP
11.3% 29.6%
GCG
0.0% 5.4%
U
AT 0.0% 7.3%
Bo
oksGGPP
38.1% 58.8%
GCG
1.2% 2.6%
U
AT 0.3% 4.7%
Nob
el winnersGGPP
28.8% 50.0%
GCG
1.3% 6.0%
U
AT 0.0% 4.0%
GGPP vs “jailbreak” methods. We also adapt two methods origi-
nally developed to “jailbreak” LLMs to the RAG setting and compare
them with GGPP: Greedy Coordinate Gradient (GCG) [ 11] (with
MSE loss between query and target passage embeddings being used)
and Universal Adversarial Trigger (UAT) [ 10]. Table 3 shows the
results obtained with Mistrial-7B, highlighting GGPP’s superior
perturbation performance in terms of both top-1 and top-10 success
rates. Similar results are observed with other embedding models.
Effect of prefix Initialization and 𝜆.To validate the effectiveness
of our prefix initialization method during prompt perturbation, we
track the L2 distance between the currently generated vector and
the target vector (corresponding to the target passage) throughout
the optimization process. We then compare the descent curves of
the distances with and without the prefix initialization operation.
We set𝜆in the loss function to 0 in this experiment to show how
fast the vector moves to the target. Figure 7 shows the distance
change over the iteration process for a single query (with Mistral-
7B on the Nobel winners dataset). With the prefix initialization, the
distance drops more quickly. This suggests that the initialization
strategy successfully shortens the search paths and the training
time required to generate an adversarial prefix. We also examine
the impact of 𝜆on perturbation. Table 5 in Appendix A.1 shows the
 
1126Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 4: GGPP perturbation detection effectiveness.
Dataset Mo
dels Pr
obe N
Parameters AUROC
Precision Recall F1-score
IMDBGPT
-J-6BSA
Te 4939200 99.9%
96.7% 100.0% 98.3%
A
CT 430080 98.3%
94.4% 94.6% 94.4%
Mistral-7BSA
Te 11289600 98.1%
93.2% 93.2% 93.0%
A
CT 430080 99.6%
97.6% 96.2% 96.9%
Q
wen-7BSA
Te 11289600 97.1%
94.5% 90.2% 92.1%
A
CT 430080 91.0%
85.5% 83.5% 84.2%
SFR-Emb
edding-MistralSA
Te 12390400 100%
100% 99.5% 99.7%
A
CT 450560 100%
100% 98.8% 99.4%
BasketballGPT
-J-6BSA
Te 4939200 98.6%
94.6% 93.3% 93.9%
A
CT 430080 87.9%
81.5% 79.9% 80.1%
Mistral-7BSA
Te 11289600 96.6%
93.2% 87.6% 90.2%
A
CT 430080 96.2%
96.3% 88.0% 91.8%
Q
wen-7BSA
Te 11289600 96.3%
93.3% 87.8% 90.4%
A
CT 430080 94.3%
89.7% 85.1% 87.1%
SFR-Emb
edding-MistralSA
Te 12390400 100.0%
99.8% 99.5% 99.1%
A
CT 450560 99.9%
100% 98.8% 99.4%
Bo
okGPT
-J-6BSA
Te 4939200 98.6%
97.1% 89.9% 93.3%
A
CT 430080 92.5%
94.5% 83.8% 88.8%
Mistral-7BSA
Te 11289600 96.6%
87.2% 95.0% 90.8%
A
CT 430080 97.8%
91.5% 91.7% 91.4%
Q
wen-7BSA
Te 11289600 91.3%
85.2% 82.5% 83.6%
A
CT 430080 86.4%
81.8% 78.6% 79.9%
SFR-Emb
edding-MistralSA
Te 12390400 100%
100% 99.4% 99.7%
A
CT 450560 99.9%
99.4% 98.9% 99.1%
Nob
el winnersGPT
-J-6BSA
Te 4939200 99.9%
97.9% 99.4% 98.6%
A
CT 430080 95.8%
92.2% 85.8% 88.8%
Mistral-7BSA
Te 11289600 96.6%
93.9% 89.4% 91.6%
A
CT 430080 99.2%
94.9% 96.3% 95.5%
Q
wen-7BSA
Te 11289600 98.7%
94.8% 94.3% 94.5%
A
CT 430080 94.1%
88.5% 82.3% 85.1%
SFR-Emb
edding-MistralSA
Te 12390400 99.9%
99.2% 96.7% 97.9%
A
CT 450560 99.9%
100% 97.7% 98.8%
results obtained with Mistral-7B on the IMDB dataset. The success
rates reach their peak at a 𝜆of 0.5 and decline as 𝜆increases due to
an imbalance in distance differences between eande′. Conversely,
a very small 𝜆(e.g., 0.1) diminishes the influence of the second part
of the loss function and fails to maintain distance balance.
5.2.2 Perturbation Detection. To evaluate the detection perfor-
mance, we randomly choose 100 queries along with their associated
GGPP prefixes from the previous prompt perturbation experiment
for each dataset. These queries and their respective prefixes are de-
signed to retrieve target passages. Meanwhile, we randomly extract
tokens from the key tokens of each query’s corresponding original
passages to form prefixes of equivalent length for the control group.
This ensures that the prefix in the control group does not affect the
retrieval result for original passages. Therefore, for each dataset,
we have a total of 200 entries. Among them, those linked with
GGPP prefixes are labeled as "1" while those in the control group
are labeled as "0". In the experiment, 60% of the entries are used for
training, with the remaining 40% reserved for testing. Before train-
ing the classifier, we padding the queries to 100 tokens to maintain
dimensional consistency of features. We report the performance
based on the average of 10 independent runs.
Detection effectiveness. Table 4 shows the results, including the
detection AUROC, Recall, Precision, and F1-score of both SATe and
ACT probes across the four datasets and the four models. Both SATe
and ACT probes demonstrate strong detection performance, with
SATe probe yielding better results than ACT probe, especially when
GPT-J-6B and Qwen-7B are used for embedding. On the other hand,ACT probe maintains significantly fewer parameters, making it a
preferable choice when resource efficiency is a primary concern.
Detection efficiency. We also conduct experiments to assess the
detection efficiency of both SATe and ACT probes. The results of
these experiments, conducted on an Intel(R) Xeon(R) Gold 6242
CPU, including training time and average response time (i.e., infer-
ence time), are presented in Table 6 (Appendix A.2). It is evident
from the table that the ACT probe not only requires considerably
less training time compared to SATe but also significantly reduces
the response time for detection.
6 Conclusion
This paper initiated the study of the robustness problem in RAG-
based LLMs under prompt perturbations. We gave a gradient guided
method to perturb user prompts, which resulted in the retrieval of
targeted text passages containing factual errors to user queries. As
RAG is considered more trustworthy than LLMs alone because the
data can be curated from reliable sources, our work revealed that
RAG-based LLMs can be vulnerable to perturbations in practice
without much knowledge needed about the data store. Our per-
turbation method showed capability of bypassing instructions in
prompts designed to block prompt attacks through trivial training.
Moreover, the GGPP method we proposed could be used to generate
prompts to enhance LLMs. We introduced two methods to detect
such perturbations based on the internal states of LLMs triggered
by these prompts. The detection methods can be used for guardrail
construction in LLM-based services.
 
1127KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
References
[1]S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston, “Neural text
generation with unlikelihood training,” arXiv preprint arXiv:1908.04319, 2019.
[2]G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière,
T. Schick, J. Dwivedi-Yu, A. Celikyilmaz et al., “Augmented language models: a
survey,” arXiv preprint arXiv:2302.07842, 2023.
[3]A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi, “When
not to trust language models: Investigating effectiveness of parametric and non-
parametric memories, ” in Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 9802–9822.
[4]N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large language
models struggle to learn long-tail knowledge,” in International Conference on
Machine Learning. PMLR, 2023, pp. 15 696–15 707.
[5]N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang,
“Lost in the middle: How language models use long contexts,” arXiv preprint
arXiv:2307.03172, 2023.
[6]I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial
examples,” arXiv preprint arXiv:1412.6572, 2014.
[7]C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fer-
gus, “Intriguing properties of neural networks,” arXiv preprint arXiv:1312.6199,
2013.
[8]J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang,
W. Ye, X. Geng et al., “On the robustness of chatgpt: An adversarial and out-
of-distribution perspective,” arXiv preprint arXiv:2302.12095, 2023.
[9]J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “Hotflip: White-box adversarial exam-
ples for text classification,” arXiv preprint arXiv:1712.06751, 2017.
[10] E. Wallace, S. Feng, N. Kandpal, M. Gardner, and S. Singh, “Universal adversarial
triggers for attacking and analyzing nlp,” arXiv preprint arXiv:1908.07125, 2019.
[11] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal and transferable
adversarial attacks on aligned language models,” arXiv preprint arXiv:2307.15043,
2023.
[12] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
M. Lewis, W.-t. Yih, T. Rocktäschel et al., “Retrieval-augmented generation for
knowledge-intensive nlp tasks,” Advances in Neural Information Processing Sys-
tems, vol. 33, pp. 9459–9474, 2020.
[13] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B.
Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al., “Improving language
models by retrieving from trillions of tokens,” in International conference on
machine learning. PMLR, 2022, pp. 2206–2240.
[14] J. Lin, R. Pradeep, T. Teofili, and J. Xian, “Vector search with openai embeddings:
Lucene is all you need,” arXiv preprint arXiv:2308.14963, 2023.
[15] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, N. Z.
Gong, Y. Zhang et al., “Promptbench: Towards evaluating the robustness of large
language models on adversarial prompts,” arXiv preprint arXiv:2306.04528, 2023.
[16] A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska, T. Terzi,
M. Gimenez, C. de Masson d’Autume, T. Kocisky, S. Ruder et al., “Mind the
gap: Assessing temporal generalization in neural language models,” Advances in
Neural Information Processing Systems, vol. 34, pp. 29 348–29 363, 2021.
[17] T. Rebedea, R. Dinu, M. Sreedhar, C. Parisien, and J. Cohen, “Nemo guardrails: A
toolkit for controllable and safe llm applications with programmable rails, ” arXiv
preprint arXiv:2310.10501, 2023.
[18] M. Yuksekgonul, V. Chandrasekaran, E. Jones, S. Gunasekar, R. Naik, H. Palangi,
E. Kamar, and B. Nushi, “Attention satisfies: A constraint-satisfaction lens on
factual errors of language models,” arXiv preprint arXiv:2309.15098, 2023.
[19] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter autoregressive
language model. ” https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas,
F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., “Mistral 7b,” arXiv preprint
arXiv:2310.06825, 2023.
[21] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang
et al., “Qwen technical report,” arXiv preprint arXiv:2309.16609, 2023.
[22] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou,
Semih Yavuz, “Sfr-embedding-mistral:enhance text retrieval with transfer
learning,” Salesforce AI Research Blog, 2024. [Online]. Available: https:
//blog.salesforceairesearch.com/sfr-embedded-mistral/
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
and I. Polosukhin, “Attention is all you need,” Advances in neural information
processing systems, vol. 30, 2017.
[24] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, “Locating and editing factual
associations in gpt,” Advances in Neural Information Processing Systems, vol. 35,
pp. 17 359–17 372, 2022.
[25] M. Geva, J. Bastings, K. Filippova, and A. Globerson, “Dissecting recall of factual
associations in auto-regressive language models, ” arXiv preprint arXiv:2304.14767,
2023.
[26] D. Ganguli, D. Hernandez, L. Lovitt, A. Askell, Y. Bai, A. Chen, T. Conerly, N. Das-
sarma, D. Drain, N. Elhage et al., “Predictability and surprise in large generative
models,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability,and Transparency, 2022, pp. 1747–1764.
[27] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn, “Memory-based
model editing at scale, ” in International Conference on Machine Learning. PMLR,
2022, pp. 15 817–15 831.
[28] J. X. Morris, E. Lifland, J. Y. Yoo, and Y. Qi, “Textattack: A framework for adver-
sarial attacks in natural language processing,” Proceedings of the 2020 EMNLP,
Arvix, 2020.
[29] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is bert really robust? a strong baseline
for natural language attack on text classification and entailment,” in Proceedings
of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 8018–8025.
[30] B. Liu, B. Xiao, X. Jiang, S. Cen, X. He, W. Dou et al., “Adversarial attacks on
large language model-based system and mitigating strategies: A case study on
chatgpt,” Security and Communication Networks, vol. 2023, 2023.
[31] C. Guo, A. Sablayrolles, H. Jégou, and D. Kiela, “Gradient-based adversarial
attacks against text transformers,” arXiv preprint arXiv:2104.13733, 2021.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
et al., “Chain-of-thought prompting elicits reasoning in large language models,”
Advances in Neural Information Processing Systems, vol. 35, pp. 24 824–24 837,
2022.
[33] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton,
V. Kerkez, and R. Stojnic, “Galactica: A large language model for science,” arXiv
preprint arXiv:2211.09085, 2022.
[34] K. Yang, Y. Tian, N. Peng, and D. Klein, “Re3: Generating longer stories with
recursive reprompting and revision,” arXiv preprint arXiv:2210.06774, 2022.
[35] Y. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neigh-
bor search using hierarchical navigable small world graphs,” IEEE transactions on
pattern analysis and machine intelligence, vol. 42, no. 4, pp. 824–836, 2018.
[36] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli, and D. Zhou,
“Large language models can be easily distracted by irrelevant context, ” in Interna-
tional Conference on Machine Learning. PMLR, 2023, pp. 31 210–31 227.
[37] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers, “Mteb: Massive text
embedding benchmark, ” arXiv preprint arXiv:2210.07316, 2022. [Online]. Available:
https://arxiv.org/abs/2210.07316
[38] R. Pradeep, K. Hui, J. Gupta, A. Lelkes, H. Zhuang, J. Lin, D. Metzler, and
V. Tran, “How does generative retrieval scale to millions of passages?” in
Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association
for Computational Linguistics, Dec. 2023, pp. 1305–1321. [Online]. Available:
https://aclanthology.org/2023.emnlp-main.83
[39] A. Gupta, A. Shirgaonkar, A. d. L. Balaguer, B. Silva, D. Holstein, D. Li, J. Marsman,
L. O. Nunes, M. Rouzbahman, M. Sharp et al., “Rag vs fine-tuning: Pipelines,
tradeoffs, and a case study on agriculture,” arXiv preprint arXiv:2401.08406, 2024.
[40] IMDb, “Imdb datasets,” 2023, accessed: 2023-11-20. [Online]. Available:
https://developer.imdb.com/non-commercial-datasets/
[41] Wikipedia Contributors, “Wikipedia, the free encyclopedia,” 2023, accessed:
2023-11-20. [Online]. Available: https://www.wikipedia.org
[42] Opendatasoft, “nobel prize,” https://public.opendatasoft.com/explore/dataset/
nobel-prize-laureates, 2023, accessed: 2023-11-20.
A Appendix
A.1 Impact of 𝜆on perturbation
Table 5: Impact of 𝜆on perturbation (results were obtained
with Mistral-7B on IMDB).
Success
rate𝜆
0.1
0.5 1.0 1.5 2.0
top-1 37.4%
40.1% 30.6% 21.9% 16.6%
top-10 43.2%
51.5% 41.6% 27.1% 20.7%
A.2 GGPP perturbation detection efficiency
 
1128Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 6: GGPP perturbation detection efficiency.
Mo
del Metho
d response time training time
Mistral-7BSA
Te 58.5 ms 182004.1 ms
A
CT 2.4 ms 4438.3 ms
GPT
-J-6BSA
Te 19.0 ms 81120.5 ms
A
CT 0.9 ms 4006.1 ms
Q
wen-7BSA
Te 61.1 ms 186402.1 ms
A
CT 2.4 ms 5693.1 ms
SFR-Emb
edding-MistralSA
Te 71.1 ms 213578.1 ms
A
CT 3.2 ms 5447.2 ms
We run the probes SATe and ACT 10 times and average their
training and response time (in ms). Comparing to SATe, ACT sig-
nificantly reduces the training and response time for detection.
A.3 Example prompts and passages
Figure 8, 9, 10, and 11 list example prompts and passages for IMDB,
Basketball, Books, and Nobel winners datasets, respectively. In
addition, GGPP prefixes for these examples are provided when
using Mistral-7B for embedding.
Original passage assistant searched: Randolph Scott was born in 1898, and died in 1987. Randolph Scott's primary professions are actor, producer, soundtrack. Randolph Scott is known for movies: 'The Stranger Wore a Gun', 'Roberta', 'Ride the High Country', 'My FavoriteWife’.New passage assistant searched: J.T. Walsh was born in 1943, and died in 1998. J.T. Walsh's primary professions are actor. J.T. Walsh is known for movies: 'A Few Good Men', 'Sling Blade', 'Breakdown', 'Needful Things'.User: Fred Astaire was born in 1899, and died in 1987. What are Fred Astaire's primary professions? Prefix:USAolikeeperJarith
Figure 8: Example GGPP prefix (on Mistral-7b-v0.1), user
query and passages (incl. both original and targeted passages)
for the IMDB (primary professions) dataset.
Original passage assistant searched: Doug Atkins, born on 1930-05-08T00:00:00Z, and played for New Orleans Saints, Chicago Bears, Cleveland Browns, Tennessee Volunteers men's basketball, Tennessee Volunteers football. Doug Atkins has been honoredwith the Pro Football Hall of Fame.New passage assistant searched: George Munroe, born on 1922-01-05T00:00:00Z, and played for Boston Celtics, St. Louis Bombers, Dartmouth Big Green men's basketball. George Munroe has been honoredwith the Rhodes Scholarship.User: Doug Atkins, born on 1930-05-08T00:00:00Z, and played for New Orleans Saints, Chicago Bears, Cleveland Browns, Tennessee Volunteers men's basketball, Tennessee Volunteers football. What honorsdid Doug Atkins get?Prefix: MunґILE immologists
Figure 9: Example GGPP prefix (on Mistral-7b-v0.1), user
query and passages (incl. both original and targeted passages)
for the Basketball Players (honors the players get) dataset.
Original passage assistant searched: 'The Little Captain' was publicatedat 1970-01-01T00:00:00Z, writenby Paul Biegel. Overall, it is a children's book by Dutch author Paul Biegel, and it can be accessed at this URL: http://www.wikidata.org/entity/Q2380457New passage assistant searched: 'Records of Woman, with Other Poems' was publicatedat 2015-01-01T00:00:00Z, writenby Felicia Hemans. Overall, it is a An instance of the book of poetry by Felicia Hermans, and it can be accessed at this URL: http://www.wikidata.org/entity/Q91806555User: 'The Little Captain' was publicatedat 1970-01-01. Who was 'The Little Captain' writenby?Prefix:substantial Poems’. metadata
Figure 10: Example GGPP prefix (on Mistral-7b-v0.1), user
query and passages (incl. both original and targeted passages)
for the Books (written by) dataset.
Original passage assistant searched: Richard R. Ernst was born in Switzerland, 1933-08-14. And died in CH, 2021-06-04. He won Nobel prize in Chemistry, 1991, for his contributions to the development of the methodology of high resolution nuclear magnetic resonance (NMR) spectroscopy. He work in EidgenössischeTechnischeHochschule (Swiss Federal Institute of Technology), Zurich Switzerland.New passage assistant searched: Ernst Otto Fischer was born in Germany, 1918-11-10. And died in DE, 2007-07-23. He won Nobel prize in Chemistry, 1973, for their pioneering work performed independently on the chemistry of the organometallic so called sandwich compounds. He work in Technical University, Munich Germany. User: Richard R. Ernst was born in Switzerland, 1933-08-14. Why did Richard R. Ernst win the Nobel prize?Prefix: <s> Ernst Otto FischerFigure 11: Example GGPP prefix (on Mistral-7b-v0.1), user
query and passages (incl. both original and targeted passages)
for the Nobel winners (reasons of winnings) dataset.
A.4 Example GGPP prefixes
Figure 12, 13, 14, and 15 provide example GGPP prefixes when using
SFR-Embedding-Mistral for embedding, along with the correspond-
ing prompts and passages.
Original passage assistant searched (top 1): Quentin Tarantino was born in 1963, and died in \\N. He/She'sprimary professions are writer, actor, producer. He/Sheis known for movies: 'From Dusk Till Dawn', 'Reservoir Dogs', 'Once Upon a Time in Hollywood', 'Pulp Fiction'. New passage assistant searched (top 1): Mary Badhamwas born in 1952, and died in \\N. He/She'sprimary professions are actress. He/Sheis known for movies: 'This Property Is Condemned', 'Let's Kill Uncle', 'The Twilight Zone', 'To Kill a Mockingbird'.User: According to IMDB dataset and your knowledges, what movies Quentin Tarantino has worked on and what were her/his roles?Prefix:Mary Badhamdid\r\r in-----19 If
Figure 12: Example GGPP prefix (on SFR-Embedding-
Mistral), user query and passages (incl. both original and tar-
geted passages) for the IMDB (primary professions) dataset.
Original passage assistant searched (top 1): George Halas, born on 1895-02-02T00:00:00Z, and played for Chicago Bears, New York Yankees, Illinois Fighting Illini baseball, Illinois Fighting Illini football, Illinois Fighting Illini men's basketball. George Halas has been honoredwith the Pro Football Hall of Fame, Bronze Star Medal.New passage assistant searched (top 1): Vida Beselienė, bornon1956-08-17T00:00:00Z, andplayedforKaunas VIČI-Aistės. Vida BeselienėhasbeenhonoredwiththeMedal"ForLabourValour", MeritedMasterofSportsoftheUSSR.User: According to wikidata(basketball players) dataset and your knowledges, what teams did George Halasplayfor and what did she/he accomplish with them?Prefix:NVidaBesikienė
🔥 bornonmay
Figure 13: Example GGPP prefix (on SFR-Embedding-
Mistral), user query and passages (incl. both original and
targeted passages) for the Basketball Players (the teams and
honors the players get) dataset.
Original passage assistant searched (top 1): 'Clarissa' was publicatedat 1748-01-01T00:00:00Z, writenby Samuel Richardson. Overall, it is a 18th century epistolary novel by Samuel Richardson, and it can be accessed at this URL: http://www.wikidata.org/entity/Q980534New passage assistant searched (top 1): 'Jan Goedbloedtepaard' was publicatedat 1890-01-01T00:00:00Z, writenby Reinoudinade Goeje. Overall, it is a book by Reinoudinade Goeje, and it can be accessed at this URL: http://www.wikidata.org/entity/Q78222064User: According to wikidata(book query) dataset and your knowledges, who wrote this book 'Clarissa'? And when was it published?Prefix:avoirliqueGoedblochtmeans paard.
Figure 14: Example GGPP prefix (on SFR-Embedding-
Mistral), user query and passages (incl. both original and
targeted passages) for the Book query (publish time and writ-
ten by) dataset.
 
1129KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, and Liming Zhu
Original passage assistant searched (top 1): 'Vincent du Vigneaudwas born in USA, 1901-05-18. And died in US, 1978-12-11. He won Nobel prize in Chemistry, 1955, for his work on biochemically important sulphur compounds especially for the first synthesis of a polypeptide hormone. He work in Cornell University, Ithaca NY USA.'New passage assistant searched (top 1): 'Erwin Schrödinger was born in Austria, 1887-08-12. And died in AT, 1961-01-04. He won Nobel prize in Physics, 1933, for the discovery of new productive forms of atomic theory. He work in Berlin University, Berlin Germany.'User: 'According to Nobel winner dataset and your knowledges, why and when did Vincent du Vigneaudwin the Nobel prize?Prefix:tunnAthSchrĭding\x1e!’
Figure 15: Example GGPP prefix (on SFR-Embedding-
Mistral), user query and passages (incl. both original and
targeted passages) for the Nobel winners (reasons and time
of winnings) dataset.
A.5 Factual answer manipulation experiments
We evaluate the performance of GGPP on the Celebrity dataset
(Table 7), sourced from WikiData, by quantitatively analyzing the
impact of optimized prefixes on LLM’s output tokens. In addition
to measuring the "success rate", which represents the proportion
of manipulations successfully leading to the targeted factually in-
correct answers, we also measure the "error rate", representing the
proportion of manipulations resulting in incorrect results but not
necessarily our targeted answers. The solid bars in Figure 17 show
GGPP is effective on shifting the results away from the correct
answers among three models tested.
Table 7: Dataset on factual answer manipulation experiments
Dataset Constraint
type N Sour
ce Example
passages and prompts
Celebrity the
occupation is 300 Wiki
Data Figur
e 16
Table 8: Accuracy of detecting the effect of GGPP prefixes on
factual answers (with GPT-J-6B).
Token manipulation Auroc Recall Precision F1-score
SAT probe 95.7% 91.4% 93.1% 92.2%
ACT probe 94.5% 91.4% 93.4% 92.4%
We also assess the performance of detecting factual answer ma-
nipulations. Table 8 shows that both ACT and SAT probes achieve
excellent and close results on manipulation detection with GPT-J-6B.
Similar results are observed with Mistral-7B and Qwen-7B models.
As previously discussed, the ACT probe maintains significantly
fewer parameters, making it the preferred choice when resource
efficiency is a consideration.A.6 GGPP for prompts with instructions
Figure 16 illustrates how the instruction "Feel free to ignore irrelevant
information in the following sentence" can be bypassed by the opti-
mized prefix. When prompts include such instructions and GGPP
is not pre-trained with them, irrelevant information is ignored,
leading to correct answers, as shown in the left pane of the figure.
However, when these instructions are included in GGPP training,
they can be ignored, allowing for perturbation, as shown in the
right pane of the figure where an incorrect answer is obtained.
We compare the performance of queries with and without such
instructions. In Figure 17, the bars in solid color represent the error
rate and success rate of GGPP prefixes without the instruction,
while the bars with diagonal stripes represent those with the in-
struction. It is evident that despite the presence of instructions
to ignore irrelevant content, the impact on the success rate with
GPT-J-6b and Qwen-7b is not substantial. On the other hand, the
success rate with Mistra-7B is significantly affected by the presence
of instructions.
Feel free to ignore irrelevant information in the following sentence. <Irrelevant Prefix> William Shakespeare (26 April 1564 –23 April 1616) was a EnglishWilliam Shakespeare (26 April 1564 –23 April 1616) was a English actor.
?
USER
GPTWilliam Shakespeare (26 April 1564 –23 April 1616) was a English poet.Feel free to ignore irrelevant information in the following sentence. <GGPP Prefix> William Shakespeare (26 April 1564 –23 April 1616) was a English
?USER
Figure 16: Instruction for irrelevant prefix can be bypassed
by optimized (GGPP) prefix.
77.20%
41.70%66.10%
25.30%15.10%18.30%54.90%
26.60%36.70%17.90%0%15.60%0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%
GPT-J-6BMistralai-7BQwen-7BToken manipulation performance of GGPP (prefix size is 5)
Error RateSuccess RateError Rate (With Instruction)Success Rate (With Instruction)
Figure 17: GGPP’s performance with and without the instruc-
tion to ignore irrelevant information in prompts
 
1130