OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining
Fanjin Zhang∗
fanjinz@tsinghua.edu.cn
Tsinghua University, Beijing, ChinaShijie Shi
shishijie2020@163.com
Zhipu AI, Beijing, ChinaYifan Zhu
yifan_zhu@bupt.edu.cn
Beijing University of Posts and
Telecommunications, Beijing, China
Bo Chen
cb21@mails.tsinghua.edu.cn
Tsinghua University, Beijing, ChinaYukuo Cen
yukuo.cen@zhipuai.cn
Zhipu AI, Beijing, ChinaJifan Yu
yujifan@tsinghua.edu.cn
Tsinghua University, Beijing, China
Yelin Chen
Lulu Wang
Qingfei Zhao
ylin@stu.xju.edu.cn
wanglulu@stu.xju.edu.cn
zhaoqingfei21@mails.ucas.ac.cn
Zhipu AI, Beijing, ChinaYuqing Cheng
Tianyi Han
chengyuqing@mail.ccom.edu.cn
tianyi.han@aminer.cn
Zhipu AI, Beijing, ChinaYuwei An
Dan Zhang
ayw.sirius19@gmail.com
zd21@mails.tsinghua.edu.cn
Tsinghua University, Beijing, China
Weng Lam Tam
Kun Cao
Yunhe Pang
rainatam9784@gmail.com
bzsy2476203449@gmail.com
pangyunhe@ncepu.edu.cn
Zhipu AI, Beijing, ChinaXinyu Guan
guanxinyu@gmail.com
Biendata, Beijing, ChinaHuihui Yuan
Jian Song
Xiaoyan Li
huihui.yuan@aminer.cn
jian.song@aminer.cn
xinyan.li@aminer.cn
Zhipu AI, Beijing, China
Yuxiao Dong
yuxiaod@tsinghua.edu.cn
Tsinghua University, Beijing, ChinaJie Tang†
jietang@tsinghua.edu.cn
Tsinghua University, Beijing, China
Abstract
With the rapid proliferation of scientific literature, versatile aca-
demic knowledge services increasingly rely on comprehensive aca-
demic graph mining. Despite the availability of public academic
graphs, benchmarks, and datasets, these resources often fall short
in multi-aspect and fine-grained annotations, are constrained to
specific task types and domains, or lack underlying real academic
graphs. In this paper, we present OAG-Bench, a comprehensive,
multi-aspect, and fine-grained human-curated benchmark based
on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks,
20datasets, 70+baselines, and 120+experimental results to date.
We propose new data annotation strategies for certain tasks and
∗Part of the work was done when Fanjin worked at Zhipu AI.
†Jie Tang is the corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672354offer a suite of data pre-processing codes, algorithm implementa-
tions, and standardized evaluation protocols to facilitate academic
graph mining. Extensive experiments reveal that even advanced
algorithms like large language models (LLMs) encounter difficulties
in addressing key challenges in certain tasks, such as paper source
tracing and scholar profiling. We also introduce the Open Academic
Graph Challenge (OAG-Challenge) to encourage community input
and sharing. We envisage that OAG-Bench can serve as a common
ground for the community to evaluate and compare algorithms in
academic graph mining, thereby accelerating algorithm develop-
ment and advancement in this field. OAG-Bench is accessible at
https://www.aminer.cn/data/.
CCS Concepts
•Information systems →Digital libraries and archives; Data
mining.
Keywords
academic knowledge graph; benchmark; academic graph mining
6214
KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
ACM Reference Format:
Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin
Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An,
Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui
Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang. 2024. OAG-Bench: A
Human-Curated Benchmark for Academic Graph Mining. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672354
1 Introduction
The overarching goal of academic data mining is to deepen our
comprehension of the development, nature, and trends of science.
It offers the potential to unlock enormous scientific, technological,
and educational value [ 50]. For example, deep mining from aca-
demic data can assist governments in making scientific policies,
support companies in talent discovery, and help researchers acquire
new knowledge more efficiently.
The landscape of academic data mining is rich with entity-centric
applications, such as paper recommendation, expert finding, and
venue recommendation. Several popular academic mining systems,
such as Semantic Scholar1, ResearchGate2, and AMiner3, are all
powered by academic knowledge graphs (AKG)4. Based on differ-
ent data sources, there have been multiple public academic graphs
and academic benchmarks, such as Microsoft Academic Graph
(MAG) [ 42] and S2ORC [ 32]. A comparative overview of these aca-
demic resources is presented in Table 1. However, there remain
several defects in existing popular datasets that may hinder promis-
ing explorations, which are summarized as follows:
•Public academic graphs, such as MAG and OAG [ 58], lack multi-
aspect and fine-grained annotations, impeding potential evalua-
tion of downstream tasks on top of them.
•Academic benchmarks, such as S2ORC and BLURB [ 15], are lim-
ited to specific task types (e.g., NLP tasks) and domains (e.g.,
biomedicine), which may not cover the full spectrum of academic
tasks, such as various graph-based tasks.
•Separate academic datasets, such as PubMedQA [ 22] and con-
cept taxonomy datasets [ 40], often do not include or align with
large-scale and comprehensive academic graphs, resulting in a
divergence from real-world scenarios.
Present Work. To this end, we introduce OAG-Bench, a meticu-
lously human-annotated academic benchmark for academic graph
mining. OAG-Bench currently includes ten tasks, 20datasets, 70+
baseline methods, and 120+experimental results. Figure 1 provides
an overview of OAG-Bench. Specifically,
(1)For the design principles of OAG-Bench, we aim to conduct
comprehensive and fine-grained annotations on the large-scale
OAG for the full life cycle of academic graph mining. Firstly, we
annotate the nodes and edges of the academic knowledge graph
and identify valuable and challenging tasks during this process,
such as author name disambiguation. Then, powered by the
academic graph, academic applications explore tasks beyond
1https://www.semanticscholar.org/
2https://www.researchgate.net/
3https://www.aminer.cn/
4In this paper, we use academic knowledge graph andacademic graph interchangeably.Table 1: Comparison between academic knowledge graphs
(AKG) and academic benchmarks. Biomed: biomedicine.
AKG /
BenchmarkMultiple
TasksDomainTask
TypeBaseline
CodesLeader-
board
MAG [42] - All - - -
OAG [57, 58] - All - - -
AceKG [52] Partial All Graph - -
S2ORC [32] ! All NLP Partial !
BLURB [15] ! Biomed. NLP ! !
OAG-Bench ! All Diverse ! !
the academic graph itself and study knowledge acquisition and
cognitive impact, such as paper source tracing (C.f. Section 3).
(2) For the datasets in OAG-Bench, we construct various human-
curated datasets for diverse tasks. We also propose new annota-
tion strategies for certain tasks, such as checking inconsistent
paper assignments across sources for incorrect paper-author
assignment detection and marking the sources of papers via
online paper reading groups. Notably, ten datasets in eight tasks
are newly constructed. The dataset sizes in OAG-Bench range
from thousands to millions.
(3)For the evaluation of OAG-Bench, OAG-Bench provides corre-
sponding data processing methods, evaluation metrics, and at
least three baseline methods for each task. OAG-Bench imple-
ments a wide range of methods, covering traditional machine
learning methods, shallow convolutional/recurrent/graph neu-
ral networks, LLMs, etc. Experimental investigations show that
advanced generation-based LLMs hold promise in some tasks
like author name disambiguation, but they still struggle with
tasks like scholar profiling and paper source tracing.
To sum up, OAG-Bench makes the following contributions: First,
we provide multi-aspect and fine-grained human-curated datasets
that cover the full life cycle of academic graph mining. Second, we
release a series of data pre-processing codes, algorithm implemen-
tations, and standardized evaluation protocols to assist researchers
in getting started quickly in academic graph mining. Finally, based
on OAG-Bench, interested researchers or practitioners can develop
advanced AKG-based algorithms, study the foundation models for
academic graph mining, and so forth.
2 Background
This section first gives the formal definition of academic knowledge
graphs and then introduces related academic datasets.
2.1 Academic Knowledge Graph
An academic knowledge graph (AKG) is defined as a graph 𝐴𝐾𝐺 =
{𝐸,𝑅}where each entity 𝑒∈𝐸and each relation 𝑟∈𝑅are associ-
ated with type mapping functions 𝜏(𝑒):𝐸→𝐶and𝜙(𝑟):𝑅→𝐷,
respectively. 𝐶and𝐷represent the sets of entity and relation types
with|𝐶|>1and|𝐷|>1. Each entity pair 𝑒1and𝑒2is linked by a
specific relation 𝑟∈𝑅to form a tuple(𝑒1,𝑟,𝑒 2).
For instance, an academic graph is a heterogeneous entity graph
that encompasses multiple types of entities, such as authors, papers,
and venues. The relation set, represented by 𝐷, includes several
key relationships: the authorship relation, which connects authors
6215OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining KDD ’24, August 25–29, 2024, Barcelona, Spain
AcademicGraphConstruction(5tasks)AcademicGraphApplication(5tasks)
20Datasets1M+Papers70K+Authors1K+ Venues5K+ Affiliations300K+ Concepts70+ BaselinesTraditional ClassifiersCNNRNNGNNBERTRegression Methods…EntityAlignment10Tasks
LLM120+ Experimental ResultsDatasetsPre-processingModelsEvaluationLeaderboardAcademicEntityConstructionAuthorNameDisambiguationScholarProfilingEntityTaggingConceptTaxonomyCompletionAcademicGraphCompletionAcademicInfluencePredictionPaperSourceTracingAcademicTraceandPredictionAcademicQuestionAnsweringPaperRecommendationAcademicKnowledgeAcquisitionReviewerRecommendation
Figure 1: OAG-Bench overview.
and papers; the paper-publish-in-venue relation that links papers
to the venues where they are published; the co-authorship relation,
indicating collaborations between authors, etc.
2.2 Academic Datasets
Some organizations have made their academic graphs available,
including MAG [ 42], OAG [ 57,58], AceKG [ 52], OpenAlex5, and
CrossRef6. These graphs are typically large-scale, but are rarely
carefully annotated to benchmark a wide range of academic tasks.
Additionally, some benchmarks based on academic corpus have
been proposed, such as S2ORC [ 32], SciDocs [ 8], and BLURB [ 15],
but they mainly target NLP tasks and overlook the intricate struc-
ture of academic graphs.
To bridge the gap, our objective is to meticulously annotate large-
scale academic graphs to benchmark various tasks for academic
graph mining. Our initiative, OAG-Bench, leverages the Open Aca-
demic Graph (OAG)7, which was initially generated by linking two
large academic graphs: MAG and AMiner. OAG aligned large-scale
entities in MAG and AMiner, including papers, authors, affiliations,
and venues, with an accuracy of over 97%. It has made available
the alignment relations between these two graphs alongside their
metadata. As MAG turned down its service at the end of 2021, OAG
has expanded its data sources to include PubMed, ArXiv, CrossRef,
and so forth. To date, five versions of OAG have been released,
amassing around 700 million entities and 2 billion relations.
3 OAG-Bench Framework
In this section, we first propose the overall design principle of
OAG-Bench, and then present the detailed workflow about how to
construct comprehensive and high-quality datasets.
As depicted in Figure 2, we host a series of data collection and
annotation efforts to conduct multi-aspect and fined-grained la-
beling based on the OAG. The framework aims to leverage high-
quality academic knowledge graphs (AKG) to facilitate academic
5https://openalex.org/
6https://www.crossref.org/
7https://www.aminer.cn/open-academic-graph
Author
 Paper
 Affiliation
 Venue
 Concept
 UserAcademic Question Answering
Paper
RecommendationReviewer
RecommendationAcademic Knowledge Acquisition
Time
Transformer
BERT
GPT
…
Academic Trace andPrediction
Paper Source
TracingAcademic
Influence
Prediction
𝐴𝐾𝐺Concept Taxonomy
Completion
Scholar
ProfilingEntity
Tagging
Web
 ACM
 DBLP
 ArXiv
 MAGData
Sources
Author 1 Author 2
…Academic Entity
ConstructionAcademic Graph
Completion
Entity
AlignmentAuthor Name
Disambiguation
Figure 2: The overall construction framework of OAG-Bench.
data mining. Therefore, the framework is structured into two types
of tasks: AKG construction and AKG-empowered academic appli-
cations. AKG construction focuses on disambiguating or enriching
graph nodes and correcting or completing graph edges, consisting
ofAcademic Entity Construction andAcademic Graph Completion.
Beyond basic academic relationships, Academic applications delve
into knowledge and cognition, consisting of Academic Knowledge
Acquisition andAcademic Trace and Prediction.
(1)Academic Entity Construction. The construction of academic
entities is fundamental to the construction of academic graphs. This
stage mainly identifies the identical real-world entities across data
sources. For notoriously ambiguous entities, i.e., authors, we further
incorporate the author name disambiguation task.
(2)Academic Graph Completion. Building upon the conflated
academic entities, this stage aims to establish connections between
different entities to complete and enrich academic graphs. Specif-
ically, we engage in fine-grained scholar profiling labeling and
attach concepts to entities, such as authors, papers, and concepts.
(3)Academic Knowledge Acquisition. On top of high-quality aca-
demic graphs, this stage focuses on the acquisition of academic
knowledge and models the multifaceted relations between users
and papers. We gather user behavior records from real academic
systems to build corresponding datasets.
(4)Academic Trace and Prediction. Besides the correlation be-
tween academic knowledge and users, this stage aims to further
explore the cognitive influence exerted by papers and authors. It
involves retrospective analysis to pinpoint the pivotal references
that have inspired a research paper. Looking forward, the challenge
lies in forecasting impactful papers or authors.
6216KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
Table 2 summarizes the specifics of the datasets in OAG-Bench.
OAG-Bench includes diverse tasks and datasets since the construc-
tion of academic graphs is complex and can not be conducted end-to-
end. Furthermore, the applications of academic graphs also involve
diverse paper-centric, author-centric, and user-centric services. Al-
though OAG-Bench currently includes ten different tasks, these
tasks could facilitate each other. For instance, profiling scholars
precisely can help to attach concept tags to scholars. These tasks
can also foster other academic tasks. For example, paper recom-
mendation datasets are also valuable assets for similar paper search.
In the following, we will present the design choices of the tasks in
each module, corresponding task definitions, and the construction
methods of related datasets.
3.1 Academic Entity Construction
To integrate academic data from multiple sources, various types
of entities need to be aligned. Thus, we first include the entity
alignment task. In view of the severe ambiguity of author names,
we further add the author name disambiguation task.
Entity Alignment. Given two entity sets 𝐸1and𝐸2, the goal of
entity alignment is to generate entity matchings 𝐿={(𝑒1,𝑒2)|𝑒1∈
𝐸1,𝑒2∈𝐸2}such that𝑒1and𝑒2refer to the same real-world en-
tity. Specifically, we consider three types of entities, i.e., authors,
affiliations, and venues. As for dataset annotation, we randomly
sample a venue set and an affiliation set, and then manually label
venue pairs with high similarity calculated by the Jaccard Index,
and construct affiliation alignment pairs by using aliases or former
names derived from the information box of their Wikipedia entries.
We utilize Wikipedia due to its high data quality. Meanwhile, it
allows us to accurately obtain positive affiliation alignment pairs
without the need for manual labeling. For author alignment, we
sample top-viewed computer science authors from AMiner, and
then manually pair them with DBLP authors according to their
affiliations, published venues, and papers. As a result, we construct
1,200venue pairs, 5,000affiliation pairs, and 10,000author pairs.
Author Name Disambiguation (AND). Aiming to disambiguate
the same-name authors, AND is a key and challenging task in aca-
demic knowledge graph construction. We adopt the WhoIsWho [ 6]
dataset, a million-scale human-annotated dataset for author name
disambiguation. WhoIsWho breaks down the task into three sub-
tasks: (1) From-scratch Name Disambiguation (SND), (2) Real-time
Name Disambiguation (RND), and (3) Incorrect Assignment Detec-
tion (IND). While existing research primarily concentrates on SND
and RND, the IND task has received less attention despite its grow-
ing importance with the expansion of academic databases. Given
an author profile with paper lists, IND aims to detect incorrectly
assigned papers to this author. To address the IND challenge, if we
were to randomly select author profiles for annotation of their pa-
per assignments, there’s a high likelihood that we would encounter
numerous profiles with a little ambiguity. Thus, we propose an
effective cross-checking annotation strategy. Specifically, we utilize
existing paper alignments and author alignments between AMiner
and DBLP, and then gather inconsistent paper-author assignments
for further expert checking. This strategy ensures that the profilesunder checking have a high likelihood of inaccuracy (with a sig-
nificant error rate exceeding 30% for these inconsistencies within
AMiner). Subsequently, all papers associated with AMiner authors
that have incorrect assignments are manually checked by experts.
Finally, the refined IND dataset includes 1,691authors, 326,738pa-
pers, with an assignment error rate of 11.32%and reaching 1.5times
the scale of papers of the IND task in WhoIsWho.
3.2 Academic Graph Completion
Academic graph completion aims to enrich academic graphs from
two aspects — entities and relations. To enrich entities, we include
thescholar profiling task to extract multidimensional attributes for
authors. To enrich relations, we include the entity tagging task to
attach concepts to entities. Concepts are abstract entities that can
endow semantics to entities. To further build a hierarchical knowl-
edge structure, we also include the concept taxonomy completion
task to identify hypernyms and hyponyms for new concepts.
Scholar Profiling. Profiling scholars from big data is a vital task
in scholar mining, and it becomes harder and harder due to data
fragmentation, modeling lengthy texts, data noise, etc. Previous
works on scholar profiling usually extract attributes from schol-
ars’ homepages or search engines. In OAG-Bench, besides profiling
scholars from search engines, we introduce a new complex set-
ting — Multidimensional Scholar Profiling from Long Texts, which
aims to extract multiple attributes in lengthy texts. Each attribute
extraction includes the starting and ending positions in the text.
Importantly, long attributes are also taken into consideration, such
as work experience and education experience. These attributes can
often exceed 100 tokens. Traditional scholar profiling or named
entity recognition tasks seldom focus on extracting such long at-
tributes. For data annotation, scholars with detailed biographical
descriptions are randomly sampled. Then, we manually label the
starting and ending positions of each attribute in the texts. Finally,
we construct 2,099scholars with 12attributes.
Entity Tagging. Aiming at associating entities with concept la-
bels, entity tagging is an important step in building semantic and
hierarchical academic graphs. We introduce scholar interest extrac-
tion and paper topic classification to attach concepts to scholars
and papers, respectively. Scholar interest extraction aims to ex-
tract scholars’ research interests from their publications. Derived
from 2017 Open Academic Data Challenge8, the dataset of this task
contains manually annotated 789research interest tags for 11,357
scholars and their papers. Paper topic classification aims to classify
papers into several topics based on the paper citation network. For
dataset construction, based on the DBLP paper citation network9,
each paper is assigned one of nine topics10related to computer
science based on its publication venue.
Concept Taxonomy Completion. Concept taxonomies are typi-
cally manually created by experts, like defining “deep learning” falls
8https://www.biendata.xyz/competition/scholar/
9https://originalstatic.aminer.cn/misc/dblp.v12.7z
10https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf. The topics include high-
performance computing, computer networks, network and information security, the-
oretical computer science, system software and software engineering, database and
data mining, artificial intelligence and pattern recognition, computer graphics and
multimedia, human-computer interaction, and pervasive computing.
6217OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Dataset overview in OAG-Bench. The format of #Datasets column is #Datasets/#New datasets for each task.
Task Data Source #Datasets Data Volume #Baselines Data characteristics
Entity alignment AMiner/DBLP/MAG 3/2 1K-10K 5 Matching heterogeneous entities
Author name disambiguation AMiner 3/1 1M 10 Million-scale human-annotated data
Scholar profiling AMiner 2/1 2K-9K 13 Long attribute extraction for long texts
Entity tagging AMiner 2/1 11K-900K 12 A large number of class labels
Concept taxonomy completion MAG/AMiner 3/1 1K-300K 3 Professionally-labeled data for the AI field
Paper recommendation AMiner 1/0 10K 4 User click records in a real academic system
Reviewer recommendation Frontiers 1/1 200K 4 Authentic public review records
Academic question answering Zhihu/StackExchange 1/0 18K 6 Automatic QA for academic domain
Paper source tracing AMiner 1/1 2K 11 Careful annotations by researchers
Academic influence prediction AMiner 3/2 1K-1M 12 Summarizing Test-of-Time award in CS∗
∗CS: computer science.
under “machine learning”. The automatic construction of concept
taxonomies is a critical challenge in the fast-evolving landscape
of knowledge concepts, which is beneficial to the organization of
knowledge in the realm of big data. Given an existing concept
hierarchy tree (Taxonomy) 𝑇0and a set of new concepts 𝐶, the
goal of concept taxonomy completion is to predict its hypernym
𝑝𝑎(𝑐)∈𝑇0and hyponym 𝑐ℎ(𝑐)∈𝑇0for each new concept 𝑐∈𝐶to
complete and expand the existing concept hierarchy tree. We adopt
two MAG taxonomies [ 42] (MAG-Full and MAG-CS) as two taxon-
omy datasets. These two datasets are large-scale but not carefully
verified by experts. Thus, we introduce a newly manually curated
dataset covering AI sub-fields by AI researchers, with 1,335con-
cepts and 1,283edges. The guidelines of edge construction refer to
relevant textbooks and the ACM Computing Classification System.
3.3 Academic Knowledge Acquisition
Academic services based on academic graphs provide convenience
for researchers to acquire knowledge actively or passively. For pas-
sive academic recommendation, we include paper recommendation
andreviewer recommendation. For active knowledge acquisition, we
include academic question answering task.
Paper Recommendation. As the volume of papers surges, re-
searchers face increasing challenges in locating relevant literature.
Given a user-paper bipartite graph 𝐺={𝑈,𝑃,𝑅}, where𝑈is the
user set,𝑃is the paper set, and 𝑅signifies interactions (e.g., clicks)
between users and papers, the goal of paper recommendation is to
predict the next paper a user will interact with [ 56]. We collect user
behavior data based on the real AMiner system. AMiner provides
a real-time paper recommendation service for researchers on the
homepage. Researchers can offer several keywords to subscribe to
relevant research papers. The back-end recommendation engine
makes recommendations based on the users’ historical click records.
This dataset includes 5,340users, 14,967papers, and 163,084inter-
actions as of October 2021. To ensure quality, only users/papers
with over 10clicks/be-clicked instances are included.
Reviewer Recommendation. As the volume of submissions to
academic journals and conferences increases, reviewer recommen-
dation becomes increasingly hard. Different from paper recom-
mendations, reviewer recommendation aims to pair papers with
proficient and willing reviewers. Given a paper submission set 𝑆, a
reviewer set 𝐴, and known paper-reviewer matches 𝑅⊆𝑆×𝐴, this
task is to predict the reviewer 𝑎∈𝐴for a new submission record𝑠𝑖∈𝑆, Additional information, including paper metadata and re-
viewer expertise, is available. For data collection, we extract real
paper-reviewing records from the open-access platform Frontiers.
After processing, it includes 210,069reviewers and 225,478papers,
with each paper having at least 2reviewers. Furthermore, we match
reviewers to authors in OAG using names, affiliations, and research
interests, linking approximately half of the reviewers to the OAG.
These reviewers are associated with their respective publications.
Academic Question Answering. Traditional keyword-based in-
formation retrieval cannot satisfy professional knowledge retrieval
in the era of artificial intelligence. For instance, consider the ques-
tion, “Can neural networks be used to prove conjectures?”. How
to retrieve answers and evidence from scholarly literature? Given
an academic question 𝑞and a paper set 𝑃𝑞={𝑝𝑞
1,𝑝𝑞
2,..,𝑝𝑞
𝑁}, the
goal of academic question answering is to select the most relevant
papers from the candidate set 𝑃𝑞. We adopt OAG-QA [45] dataset,
which is derived from academic question-answering platforms. We
retrieve question posts from StackExchange and Zhihu websites,
extract the paper URL mentioned in the answer, and match it with
the paper in OAG [ 58]. It comprises 17,948question-paper pairs.
Questions cover 22disciplines and 87topics, forming a two-level
hierarchical structure; that is, each topic belongs to a discipline.
For each topic, 10,000candidate papers, including the ground-truth
papers in the answers, are included.
3.4 Academic Trace and Prediction
Understanding the evolution of science on the cognitive level offers
the potential to predict, change, and finally invent the future. Trac-
ing back to the past, we include paper source tracing task to identify
the sources of research papers. To predict future potential academic
impact, we include two tasks, i.e., paper influence prediction and
author influence prediction.
Paper Source Tracing (PST). Tracing the sources of papers is
crucial for understanding technological essence and uncovering
innovation patterns. Given a paper 𝑝(including its full text) and
its references, the goal of PST is to identify the most important
references (termed ref-source ) that largely inspired the paper 𝑝in
terms of ideas or methods. The source papers of a given paper are
defined by the following principles: (1) the main idea of the paper
𝑝is inspired by the reference; or (2) the main method of the paper
𝑝comes from the reference. In other words, this paper would not
come into being without these source papers. We carefully build
6218KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
a dataset PST-Bench for this task. Given the specialized knowl-
edge required for paper source tracing, dozens of computer science
graduate students were employed to mark the sources of papers in
their familiar fields. The annotation process was organized in an
online paper reading group, where each student needed to share
two papers and mark their source papers each week. After collec-
tion, expert-checking, and preprocessing, 2,141labeled computer
science papers were obtained. We conducted a human evaluation
on the test set, with senior researchers double-checking 100 papers.
The accuracy rate was 94%.
Academic Influence Prediction. Paper influence prediction aims
to forecast a paper’s impact Δ𝑦𝑟years later based on its meta-
data and citation relationships. Regarding the “Test-of-Time Paper
Award” (TOT award) as an indicator of high impact, we collate
TOT awards in computer science venues11. Awards with similar
meanings include the Most Influential Award, Sustained Influence
Award, etc. At present, a total of 1,063papers awarded by 2022
have been collected. Similarly, author influence prediction seeks
to predict an author’s influence Δ𝑦𝑟years later using their papers
and citation relationships. For this task, we provide two datasets.
First, we adopt AuthPred-201712. This dataset contains a subset
of AMiner authors and papers published by these authors before
2011, intending to predict the citations of these authors as of 2016.
This dataset uses AMiner’s citation statistics and provides 1,112,931
authors for training and 123,823authors for testing. In addition,
we construct a new dataset AuthPred-2022. This dataset contains
a subset of AMiner’s authors in the field of computer science and
papers published by these authors before 2017, with the goal of pre-
dicting the citations of these authors as of early 2022 (as calculated
by Google Scholar). This dataset contains 26,797AMiner authors
with Google Scholar links.
4 Task Evaluations
This section delves into representative tasks of each module in OAG-
Bench, highlighting selected experiments. Additional experiments
are detailed in Appendix A. All codes have been available13.
4.1 Author Name Disambiguation
Since SND and IND are two widely studied tasks, we take incorrect
assignment detection (IND) as an illustration for evaluation.
Baselines. We adopt graph-based anomaly detection methods
and LLM-based methods as baselines. For each author, graph-based
methods first construct a paper similarity graph based on attribute
similarity (e.g., co-authorship, co-organization) and then detect
anomalies in the graph. (1) Logistic Regression (LR): injects top
eigenvectors of each graph as features to perform node classifi-
cation. (2) GCN [25]: employs graph convolutional networks as
the encoder, and then uses fully-connected layers to classify nor-
mal/abnormal nodes. (3) GCCAD [7]: leverages graph contrastive
learning and contrasts abnormal nodes with normal ones in terms
11https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf
12https://www.biendata.xyz/competition/Tsinghua_course3/
13https://github.com/zfjsail/OAG-BenchTable 3: Performance of incorrect assignment detection (%).
Method AUC MAP
LR 58.46 69.56
GCN 62.48 71.18
GCCAD 70.15 74.17
ChatGLM 77.92 79.54
Table 4: Extraction performance of multidimensional scholar
profiling from long texts (%).
Method Precision Recall F1
UIE 43.14 35.86 39.15
Global Pointer 51.87 33.09 40.39
Han et al. 50.33 43.76 45.09
of their distances to the global context. (4) ChatGLM [11]: fine-
tunes ChatGLM-6B model by inputting each author’s paper list and
asking the model whether one given paper is an anomaly or not.
Evaluation Metrics. Due to the imbalance between positive and
negative instances, we adopt the widely-used metric AUC. Further-
more, we choose mean average precision (MAP) as another metric,
which pays more attention to the rankings of incorrect instances.
We take a macro average of each metric for each author.
Experimental Results. Table 3 shows the performance of incor-
rect assignment detection. We observe that graph neural network-
based methods (GCN and GCCAD) outperform the traditional
method (LR) based on eigenvalue decomposition. GCCAD explicitly
contrasts abnormal paper nodes with other nodes, yielding better
performance than GCN. Surprisingly, ChatGLM outperforms graph-
based anomaly detection methods, indicating the potential of the
attention mechanisms in LLMs to capture the complex correlations
between the target paper and the overall author profile. The best
performance of IND is not that satisfactory compared with that of
SND and RND tasks [ 6], suggesting that more attention should be
paid to the IND task for author name disambiguation in the future.
4.2 Scholar Profiling
In this subsection, for entity attribute enrichment, we present the
evaluation of multidimensional scholar profiling from long texts.
Baselines. We select the latest NER methods based on pre-trained
models: (1) Han et al. [54]: use a Biaffine decoder to generate
features for each start and end position and then employ CNN to
classify locations based on spatial position dependence. (2) Global-
Pointer [43]: uses a multiplicative attention mechanism to incor-
porate relative positional encodings of start and end positions and
alleviates class imbalance via modified loss functions. (3) UIE [33]:
is a generative pre-trained model based extraction framework with
structure extraction languages and template-specific prompts.
Evaluation Metrics. Precision, Recall, and F1 are computed by
comparing predicted and annotated text segments for each attribute.
These individual attribute results are then averaged to obtain the
overall evaluation result.
6219OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining KDD ’24, August 25–29, 2024, Barcelona, Spain
Experimental Results. Table 4 display extraction results for long
text-based scholar profiling. In the context of scholar profiling from
long texts, Table 4 reveals that span-based methods like Han et al.
surpass generation-based methods like UIE. We also conduct pre-
liminary experiments by giving some demonstrations and calling
GPT-4 [ 1] API on a subset of test sets, achieving only less than 5%F1
score. This performance disparity likely stems from the challenges
that language models face when generating accurate lengthy texts
directly for attributes such as education/work experiences. How-
ever, with the highest F1 score in Table 4 being 45.09%, there is still
room for improvement in extraction performance. Exploring the
fusion of large language models (LLMs) and span-based methods
presents a promising research avenue.
4.3 Entity Tagging
For relation enrichment, this subsection presents the results of
scholar interest extraction.
Baselines. For scholar interest extraction, we employ competition-
winning solutions and methods relying on pre-trained models.
These approaches follow a common principle: they gauge the sim-
ilarity between authors in the test and training sets, using the
weighted interest tags of training authors for the authors in the
test set. The variations among baselines lie in how they compute
author similarity. (1) LSI[9] employs bag of words and TF-IDF for
paper texts, reducing dimensions with the LSI model. (2) ACA14:
utilizes more paper attributes, including titles, citations, and venues,
for a more nuanced author similarity calculation. (3) pre-training
models: leverage models like Sentence-BERT (S-BERT) [39],Sim-
CSE [14],E5[51],BERT [10],GTE [28],BGE15, and Sentence-T5
(S-T5) [34] to encode paper texts for similarity measurement.
Evaluation Metrics. For scholar interest extraction, we calculate
the overlap ratio between predicted and ground-truth tags.
Accuracy =1
𝑁𝑁∑︁
𝑖=1|𝑇𝑖∩𝑇∗
𝑖|
|𝑇∗
𝑖|
where𝑁is the number of scholars, 𝑇∗
𝑖is the annotated interest set
of the𝑖-th scholar, and 𝑇𝑖is the predicted interest set of the 𝑖-th
scholar. We pick the 3closest tags to the author for evaluation. For
paper topic classification, we measure multi-classification accuracy.
Experimental Results. Figure 3 presents the results of scholar
interest extraction. Initial attempts to classify scholars’ paper texts
using research interest tags as labels yielded unsatisfactory results,
likely due to the large number of tags and limited training data. The
methods compared in Figure 3 rely on author similarity to calculate
interest tags, which proves more effective than text classification.
Notably, encoding with pre-trained models directly is less effective
than LSI, highlighting the effectiveness of shallow semantic models.
Additionally, models focused on sentence embedding outperform
general pre-trained models like BERT. The ACA method, which
leverages various author attributes such as venues and citing papers,
yields improved prediction results. However, the overall accuracy
remains low, indicating a challenge in accurate classification with
a large number of interest tags.
14https://github.com/geekinglcq/aca
15https://github.com/FlagOpen/FlagEmbedding
BERT-baseS-T5-xxl SimCSEGTE-largeS-BERT E5-largeBGE-largeLSIACA
Method0.10.20.3AccuracyFigure 3: Results of scholar interest extraction. SimCSE:
SimCSE-RoBERTa-large, S-BERT: all-MiniLM-L6-v2.
Table 5: Performance of paper recommendation.
Method Recall@20 NDCG@20
Mult-VAE 0.1088 0.0282
GF-CF 0.2067 0.1044
NGCF 0.1651 0.0823
LightGCN 0.1950 0.0985
Table 6: Performance of reviewer recommendation.
Method Recall@20 NDCG@20
TF-IDF 0.0016 0.0001
TPMS 0.0220 0.008
GF-CF 0.0382 0.0203
LightGCN 0.0371 0.0234
4.4 Academic Recommendation
For academic knowledge acquisition, this subsection presents the
results of paper recommendation and reviewer recommendation.
Baselines. We compare various recommendation algorithms: (1)
TF-IDF, (2) Toronto paper matching system (TPMS) [ 5] for re-
viewer recommendation task, (3) Variational autoencoder (VAE)-
based item-based collaborative filtering method Mult-VAE [29],
(4) Graph filtered-based collaborative filtering method GF-CF [41],
and (5) Graph neural networks (GNN)-based collaborative filtering
methods NGCF [53] and LightGCN [18].
Evaluation Metrics. Like the standard recommendation task [ 55,
56], we adopt Recall@20 and NDCG@20 as evaluation metrics for
paper/reviewer recommendation.
Experimental Results. Table 5 shows the paper recommendation
performance. GF-CF outperforms other methods, highlighting the
effectiveness of graph filters. GNN-based methods exceed Mult-VAE,
demonstrating the value of high-order graph structures. LightGCN
performs better than NGCF, which confirms the redundancy of
some GNN modules in NGCF. However, there is room for improve-
ment in recommendation accuracy. How to use the attributes of
papers and users to capture the dynamic interest changes of users
is a difficult point.
6220KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
BM25 DPR-FT DPR-PT2ColBERT-FT ColBERT-PT2 LLM-Embedder
Method0.00.10.20.3Hit@20
Figure 4: Results of academic question answering.
Table 6 reports the reviewer recommendation performance. GF-
CF and LightGCN outperform TF-IDF, underscoring the importance
of leveraging graph structures. The performance of TPMS is un-
satisfactory because TPMS is also mainly based on TF-IDF text
similarity. However, most methods fall short of fully utilizing the
multidimensional attributes of papers and reviewers’ research in-
terests. This highlights the need for further research in the reviewer
recommendation task.
4.5 Academic Question Answering
For academic knowledge acquisition, this subsection introduces the
results of academic question answering.
Baselines. We adopt sparse and dense retrieval methods: (1) Sparse
retrieval methods: BM25, (2) Dense retrieval methods: DPR-FT
(full fine-tuning of Dense Passage Retriever (DPR) [ 23]),DPR-PT2
(parameter-efficient fine-tuning of DPR with P-Tuning v2 [ 30]),
ColBERT-FT (full fine-tuning of ColBERT [ 24]),ColBERT-PT2
(parameter-efficient fine-tuning of ColBERT with P-Tuning v2), and
LLM-Embedder [59] (a fine-tuned LLM based on various retrieval-
related tasks).
Evaluation Metrics. Hit@K is used to measure retrieval accuracy,
reporting if the top 𝐾retrieved papers contain the correct answer.
The average Hit@K across all questions is reported.
Experimental Results. Figure 4 presents the results of the OAG-
QA dataset. Generally speaking, dense retrieval methods outper-
form sparse retrieval methods. ColBERT-based methods are signifi-
cantly better than DPR-based ones. This shows that by employing
late interaction patterns and multi-vector representations, ColBERT
models the correlation between questions and papers better. Inter-
estingly, efficient parameter fine-tuning methods excel over full
fine-tuning, possibly due to better knowledge retention and gener-
alization from the pre-trained model. The effect of LLM-Embedder
suggests there still exists noticeable gap between LLM and academic
retrieval. Overall, these methods’ retrieval effects are suboptimal,
suggesting room for improvement.
4.6 Paper Source Tracing
For academic source tracing, this subsection presents the results of
paper source tracing.
RuleRFLINENetSMF GPT-3.5GPT-4ClaudeGLM-2B GalacticaBERT-baseSciBERT
Method0.00.10.20.3MAPFigure 5: Results of paper source tracing.
Baselines. We compare three types of methods. (1) Statistical meth-
ods:Rule (employing regular expressions to extract references ap-
pearing near signal words like “motivated by” or “inspired by”), and
Random Forest (RF) (following [ 49], extracting statistical features
about citations, citing positions, text similarity, etc., and using RF
to predict the importance of references). (2) Graph-based methods:
LINE [46] and NetSMF [38] train paper embeddings in citation net-
works and then calculates the cosine similarity between the paper
embedding and the reference embedding to measure the importance
of references. (3) Pre-training methods: extract the contextual text
where each reference appears in the full texts, encode the text with
the pre-training models, and use the reference annotation results in
the training set for fine-tuning. The pre-training models considered
include BERT [10],SciBERT [3],Galactica-standard [47], and
GLM [11]. We also adopt three SOTA closed-source models: GPT-
3.5[35],GPT-4 [1], and Claude-instant [2]. For both open-source
and closed-source LLMs, we input the context of a referenced pa-
per and query the model to assess the reference’s significance. For
instance, we ask, “Given the context ..., is the current reference
important?” Closed-source LLMs perform this task using zero-shot
evaluation.
Evaluation Metrics. A paper may have one or more ref-sources.
For each reference of the paper 𝑝, an importance score between
[0,1]needs to be output. For each paper 𝑝to be traced, its reference
list is encoded as 0-1 based on the labeling results (1 if it’s ref-source,
0 otherwise). By comparing the prediction result of each reference
with its labeling result, we compute the Mean Average Precision
(MAP). The average MAP across different papers serves as the
evaluation metric.
Experimental Results. Figure 5 presents the results of paper
source tracing. Among all methods, SciBERT delivers the best per-
formance, indicating the efficacy of pre-trained language models.
RF outperforms the Rule method, underscoring the effectiveness
of feature engineering. Graph-based methods achieve average per-
formance, possibly owing to the ignorance of the contextual in-
formation of references. The Rule-based approach’s performance
is subpar, likely due to many important references lacking sur-
rounding signal words like “inspired by”, resulting in a low recall.
Surprisingly, finetuned SciBERT and BERT-base outperform larger
models like GLM-2B, Galactica-standard, and closed-source LLMs.
The reason may lie in two aspects. First, the training objective of
6221OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 7: Results of paper influence prediction.
Method MAP
Citation 0.6413
RF 0.5409
GBDT 0.5725
PageRank 0.6504
GraphSAGE 0.0811
GENI 0.1262
RGTN 0.0279
the mask language model is more suitable for this context under-
standing task. Second, API-based models may not be well-trained
on similar tasks. A potential future direction could involve merging
graph-based and text-based methods for paper source tracing. Note
that the current methods’ results are not yet satisfactory, indicating
ample room for further exploration.
4.7 Paper Influence Prediction
For academic influence prediction, this subsection presents the
results of paper influence prediction.
Baselines. We select the following methods: (1) Citation: is based
on the paper citation number of known years; (2) Random Forest
(RF) [4]: defines features as the paper citation number per year
and the total number of citations; (3) GBDT [13]: uses the same
features as RF; (4) PageRank [36]: calculates papers’ PageRank
score based on paper citation networks; (5) GraphSAGE [16]: per-
forms semi-supervised classification on the paper citation network.
Additionally, we consider graph-based node importance prediction
methods: (6) GENI [37] and (7) RGTN [19].
Evaluation Metrics. We predict for each venue to determine
whether a paper would be awarded, with labels being 0or1indicat-
ing whether the paper is awarded or not. Mean Average Precision
(MAP) is calculated by comparing the predicted probability of win-
ning the award with the ground truth label, and the mean MAP
across different venues is used as the evaluation metric.
Experimental Results. Table 7 presents results for paper influ-
ence prediction. Table 7 shows PageRank performing best, as it
considers the influence of citing papers, unlike the citation method
that treats each citing paper equally. Traditional classifiers (RF
and GBDT) are inferior to methods using only total citations, in-
dicating that total citations are a very important indicator. The
features added by the classifier may dilute the effect of total cita-
tions. GraphSAGE’s poor performance may be due to its inability
to capture paper-influence factors like citation count. GENI out-
performed GraphSAGE, but both methods were less effective than
Citation and PageRank methods. This could be due to their implicit
incorporation of citation statistics and the severe class imbalance
problem (positive vs. negative <1 : 100). Thus, identifying factors
beyond citation count remains a challenge in predicting papers’
breakthrough innovation.
5 OAG-Challenge
To promote the engagement of the research community and the
development of OAG-Bench, we also introduce the Open AcademicData Challenge (OAG-Challenge) and set up a regular leaderboard
for up-to-date OAG-Bench16. OAG-Challenge currently contains
three challenging academic tasks: incorrect assignment detection
for author name disambiguation (IND), academic question answer-
ing (OAG-AQA), and paper source tracing (PST).
Specifically, given the paper assignments of each author and
paper metadata, IND aims to detect paper assignment errors for
each author. Given professional questions and a pool of candidate
papers, OAG-AQA hopes to retrieve the most relevant papers to
answer these questions. As mentioned earlier, given the full texts
of each paper, PST aims to automatically trace the most significant
references that have inspired a given paper.
OAG-Challenge was deployed at KDD Cup 2024 and attracted
more than 800 team registrations globally. Following the previ-
ous successful conventions, submissions are required to provide
source codes, technical reports, and contact information for bet-
ter knowledge sharing and iteration. We are periodically updating
the datasets, including annotating new assignment errors, crawl-
ing new academic question and answer pairs, and collecting new
reading records for PST.
6 Conclusion
The attention of the research community to academic benchmarks
remains limited, even if academic tasks offer various challenges and
applications of immense impact. Thus, this paper introduces OAG-
Bench to carefully annotate large-scale OAG for the full life cycle
of academic graph mining. OAG-Bench now includes 10 tasks, 20
datasets, 70+baseline models, and 120+experimental results. In the
future, we plan to continually maintain and enhance OAG-Bench by
updating up-to-date datasets regularly from real scenarios, adding
more practical tasks, and exploring interactive evaluation metrics.
OAG-Bench is always open for contributions from communities by
adding new tasks or datasets, developing cutting-edge algorithms
or foundation models for various tasks, etc.
Acknowledgments
This work is supported by Natural Science Foundation of China
(NSFC) 62425601 and 62276148, Technology and Innovation Major
Project of the Ministry of Science and Technology of China under
Grant 2020AAA0108400, the New Cornerstone Science Foundation
through the XPLORER PRIZE. This work is also supported by the
Postdoctoral Fellowship Program of CPSF under Grant Number
GZB20240358. We also thank Weibin Liao, Chao Yu, Kai Yu, and
Zheng Jiang for their contribution to code reproducibility.
16https://www.biendata.xyz/kdd2024/
6222KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
References
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2]Anthropic. 2023. Introducing Claude. https://www.anthropic.com/news/
introducing-claude.
[3]Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language
Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing. 3615–3620.
[4] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[5]Laurent Charlin and Richard Zemel. 2013. The Toronto paper matching system:
an automated paper-reviewer assignment system. (2013).
[6]Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li,
Yuxiao Dong, and Jie Tang. 2023. Web-Scale Academic Name Disambiguation:
the WhoIsWho Benchmark, Leaderboard, and Toolkit. In Proceedings of the 29th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
3817–3828.
[7]Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Dong, Jian Song, Peng Zhang,
Kaibo Xu, Evgeny Kharlamov, and Jie Tang. 2023. GCCAD: Graph Contrastive
Learning for Anomaly Detection. IEEE Transactions on Knowledge & Data Engi-
neering 01 (2023), 1–14.
[8]Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld.
2020. SPECTER: Document-level Representation Learning using Citation-
informed Transformers. In Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics. 2270–2282.
[9]Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and
Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the
American Society for Information Science 41, 6 (1990), 391–407.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. 4171–4186.
[11] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive
Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). 320–335.
[12] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
2022. Language-agnostic BERT Sentence Embedding. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 878–891.
[13] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics 29, 5 (2001), 1189–1232.
[14] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive
Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing. 6894–6910.
[15] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong
Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific
language model pretraining for biomedical natural language processing. ACM
Transactions on Computing for Healthcare 3, 1 (2021), 1–23.
[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Proceedings of the 31st International Conference on
Neural Information Processing Systems. 1025–1035.
[17] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. DeBERTaV3: Improving
DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Em-
bedding Sharing. In Proceedings of the 11th International Conference on Learning
Representations.
[18] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639–648.
[19] Han Huang, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng Lv, and Hui Xiong.
2021. Representation learning on knowledge graphs for node importance estima-
tion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining. 646–655.
[20] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for
sequence tagging. (2015). arXiv:1508.01991
[21] Minhao Jiang, Xiangchen Song, Jieyu Zhang, and Jiawei Han. 2022. TaxoEnrich:
Self-Supervised Taxonomy Completion via Structure-Semantic Representations.
InProceedings of the ACM Web Conference 2022. 925–934.
[22] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). 2567–2577.[23] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing. 6769–6781.
[24] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research and development in Information
Retrieval. 39–48.
[25] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In Proceedings of the 4th International Conference
on Learning Representations.
[26] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised
Learning of Language Representations. In Proceedings of the 8th International
Conference on Learning Representations.
[27] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan.
2020. Deep entity matching with pre-trained language models. Proceedings of
the VLDB Endowment 14, 1 (2020), 50–60.
[28] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan
Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive
Learning. arXiv preprint arXiv:2308.03281 (2023).
[29] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.
Variational autoencoders for collaborative filtering. In Proceedings of the 2018
world wide web conference. 689–698.
[30] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie
Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across
Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers). 61–68.
[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa:
A Robustly Optimized BERT Pretraining Approach. In Proceedings of the 9th
International Conference on Learning Representations.
[32] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.
2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics. 4969–4983.
[33] Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and
Hua Wu. 2022. Unified Structure Generation for Universal Information Extraction.
InProceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). 5755–5772.
[34] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel
Cer, and Yinfei Yang. 2022. Sentence-T5: Scalable Sentence Encoders from Pre-
trained Text-to-Text Models. In Findings of the Association for Computational
Linguistics: ACL 2022. 1864–1874.
[35] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt.
[36] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
[37] Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos.
2019. Estimating node importance in knowledge graphs using graph neural
networks. In Proceedings of the 25th ACM SIGKDD international conference on
knowledge discovery & data mining. 596–606.
[38] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Chi Wang, Kuansan Wang, and Jie
Tang. 2019. Netsmf: Large-scale network embedding as sparse matrix factoriza-
tion. In The World Wide Web Conference. 1509–1520.
[39] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing. 3982–3992.
[40] Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, and
Jiawei Han. 2020. TaxoExpan: Self-supervised taxonomy expansion with position-
enhanced graph neural network. In Proceedings of The Web Conference 2020.
486–497.
[41] Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B Khaled Letaief,
and Dongsheng Li. 2021. How Powerful is Graph Convolution for Recommenda-
tion?. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 1619–1629.
[42] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and
Kuansan Wang. 2015. An overview of microsoft academic service (mas) and
applications. In Proceedings of the 24th international conference on world wide web.
243–246.
[43] Jianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou, Jun Sun, Wanwei Huang,
Bo Wen, and Yunfeng Liu. 2022. Global Pointer: Novel Efficient Span-based
Approach for Named Entity Recognition. (2022). arXiv:2208.03054
[44] Ilya Sutskever, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2009. Modelling
relational data using Bayesian Clustered Tensor Factorization. In Proceedings
of the 22nd International Conference on Neural Information Processing Systems.
1821–1828.
6223OAG-Bench: A Human-Curated Benchmark for
Academic Graph Mining KDD ’24, August 25–29, 2024, Barcelona, Spain
[45] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Ji-
ahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes
generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087
(2022).
[46] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In Proceedings of the
24th international conference on world wide web. 1067–1077.
[47] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Sto-
jnic. 2022. Galactica: A large language model for science. arXiv preprint
arXiv:2211.09085 (2022).
[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[49] Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015. Identifying Meaningful Cita-
tions.. In AAAI workshop: Scholarly big data, Vol. 15. 13.
[50] Dashun Wang and Albert-László Barabási. 2021. The science of science. Cambridge
University Press.
[51] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised
contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).
[52] Ruijie Wang, Yuchen Yan, Jialu Wang, Yuting Jia, Ye Zhang, Weinan Zhang, and
Xinbing Wang. 2018. Acekg: A large-scale knowledge graph for academic data
mining. In Proceedings of the 27th ACM international conference on information
and knowledge management. 1487–1490.
[53] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval. 165–174.
[54] Hang Yan, Yu Sun, Xiaonan Li, and Xipeng Qiu. 2023. An Embarrassingly Easy
but Strong Baseline for Nested Named Entity Recognition. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers). 1442–1452.
[55] Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing
Tang, Ying Shan, Yuxiao Dong, and Jie Tang. 2024. RecDCL: Dual Contrastive
Learning for Recommendation. In Proceedings of the ACM on Web Conference
2024. 3655–3666.
[56] Dan Zhang, Yifan Zhu, Yuxiao Dong, Yuandong Wang, Wenzheng Feng, Evgeny
Kharlamov, and Jie Tang. 2023. ApeGNN: Node-Wise Adaptive Aggregation
in GNNs for Recommendation. In Proceedings of the ACM Web Conference 2023.
759–769.
[57] Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao Gu,
Yan Wang, Evgeny Kharlamov, Bin Shao, et al .2023. OAG: Linking Entities across
Large-scale Heterogeneous Knowledge Graphs. IEEE Transactions on Knowledge
and Data Engineering 35, 9 (2023), 9225–9239.
[58] Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao
Gu, Yan Wang, Bin Shao, Rui Li, et al .2019. OAG: Toward linking large-scale
heterogeneous entity graphs. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2585–2595.
[59] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.
2023. Retrieve anything to augment large language models. arXiv preprint
arXiv:2310.07554 (2023).
A Results of Additional Tasks
A.1 Entity Alignment
We provide the experimental results of venue alignment, affiliation
alignment, and author alignment in this subsection.
Baselines. Venue alignment and affliation alignment are short text
matching tasks. We compare different types of matching methods:
(1) traditional machine learning methods (SVM) using Jaccard index
and TF-IDF similarity as input features; (2) shallow neural network-
based matching methods including CNN-based matching model
(LinKG𝐶) [58] and RNN-based matching model (LinKG 𝐿) [58];
(3) matching models based on pre-trained models (Ditto [27] with
pre-trained models BERT [ 10], ALBERT [ 26], RoBERTa [ 31], De-
BERTa [17], LaBSE [12], and GLM [11]).
Author alignment can further take authors’ structural infor-
mation into account. Apart from SVM, LinKG 𝐶, and LinKG 𝐿, we
additionally select LinKG 𝐺[58] model for author alignment, whichTable 8: Alignment results of different types of entities (%).
Venue Affiliation
Method F1 AUC F1 AUC
SVM 82.63 91.87 68.47 70.93
LinKG𝐶 83.46 94.07 69.06 69.12
LinKG𝐿 85.03 95.33 67.76 72.37
Ditto-BERT-base 89.33 95.00 70.38 78.65
Ditto-ALBERT-base 88.05 96.06 61.90 70.96
Ditto-RoBERTa-large 89.47 97.22 71.98 79.02
Ditto-DeBERTa-base 94.27 96.73 72.78 80.37
Ditto-DeBERTa-large 89.44 97.94 82.18 89.76
Ditto-LaBSE 90.79 96.93 71.06 78.55
Ditto-GLM-RoBERTa 78.82 93.04 61.11 67.93
Table 9: Performance of author alignment (%).
Method F1 AUC
SVM 90.14 93.67
LinKG𝐶 67.72 66.78
LinKG𝐿74.80 77.58
LinKG𝐺 89.62 97.32
constructs a subgraph for each candidate pair, and then uses hetero-
geneous graph attention networks to learn hidden representations
for classification. For each candidate author pair, we leverage pub-
lished papers and venues of authors as features.
Evaluation Metrics. Entity alignment is a binary classification
problem. We take F1 and AUC as the evaluation metric.
Experimental Results. Table 8 presents results for various match-
ing models for venue alignment and affiliation alignment. The meth-
ods for author alignment utilize more structural information, with
corresponding results presented in Section A.1. Venue/affiliation
alignment is essentially a short text matching task. SVM performs
slightly poorer than LinKG 𝐶and LinKG 𝐿, possibly due to the inabil-
ity of SVM features to capture word order information. Conversely,
LinKG𝐶and LinKG 𝐿can capture the contextual dependence of word
sequence. Overall, the top-performing models on both datasets uti-
lize pre-training, indicating the promising potential of pre-trained
language models for entity alignment. Among all methods that
employ pre-trained models, DeBERTa-large delivers the best perfor-
mance on two tasks, particularly on affiliation alignment, indicating
that DeBERTa-large effectively encodes additional semantic knowl-
edge beyond affiliations’ surface names. Note that the trends of
AUC and F1 in the table are sometimes inconsistent. Given that F1
requires a threshold setting, AUC is a more reliable metric when
discrepancies arise between the two.
The results of author alignment are shown in Table 9. We ob-
serve that methods using authors’ structure information (SVM and
LinKG𝐺) are significantly better than methods not using structure
information (LinKG 𝐶and LinKG 𝐿). In the future, more author pairs
with ambiguous names will be added to increase the difficulty of
this task.
6224KDD ’24, August 25–29, 2024, Barcelona, Spain Fanjin Zhang et al.
Table 10: Results of search engine-based scholar profiling
(%).
Method Gender Homepage Position
SML-esb 71.67 – –
LR – 19.37 –
XGBoost – 20.8 –
Rule – – 71.6
BI-LSTM-CRF – – 85.10
BERT-base 96.12 18.91 83.23
RoBERTa-base 96.40 20.65 83.78
DeBERTa-base 96.50 21.11 85.14
DeBERTa-v3-large 96.56 18.21 79.34
ALBERT-base 95.85 16.38 84.33
ChatGLM-6B-LoRA 96.40 26.83 78.15
LLaMA-7B-LoRA 70.07 26.93 79.14
A.2 Scholar Profiling
This subsection presents the task description and experiments for
search engine-based scholar profiling.
Problem A.1. Search Engine-based Scholar Profiling. Given
a scholar’s name, affiliation, and one’s search engine records (using
“name + affiliation” to query and extracting up to 2search pages and
up to 20snippets), the goal is to extract the portrait information of
the scholar, including homepage, gender, and position.
Datasets. CCKS2021-En17: This dataset is an English subset of
the CCKS 2021 scholar profiling track from AMiner. It contains
9,221scholar portraits, randomly divided into 5,557for training,
1,833for validation, and 1,831for testing.
Baselines. Drawing from the winning solutions of the CCKS 2021
and recent named entity recognition (NER) methods, we select the
following baselines for search engine-based scholar profiling: (1)
SML: employ manual features and traditional classifiers. Specifi-
cally, for gender prediction, SML-esb extracts features such as the
frequency of “his” and “her” and uses various classifiers for voting.
For homepage extraction, Logistic Regression (LR) andXGBoost
extract features like the appearance of signal words (such as “edu”
and “academic”) for classification. (2) Rule: utilizes regular expres-
sions and voting for position extraction. (3) BI-LSTM-CRF [20]:
uses a BI-LSTM layer and a CRF layer for sequence labeling in
position extraction. (4) Pre-training methods: We design different
inputs for pre-training models for each attribute. For gender pre-
diction and position extraction, the scholar’s name, affiliation, and
webpage texts are concatenated as inputs. For homepage extrac-
tion, the scholar’s name and the candidate URL are concatenated as
inputs. We fine-tune pre-trained models for classification, includ-
ingBERT [10],RoBERTa [31],DeBERTa [17],ALBERT [26],
ChatGLM18,LLaMA [48].
Evaluation Metrics. Following the competition, accuracy is used
to measure exact matches between predictions and ground truths.
17https://www.biendata.xyz/competition/ccks_aminer_profiling/
18https://github.com/THUDM/ChatGLM-6BTable 11: Results of concept taxonomy completion.
Method BiLinear TaxoExpan TaxoEnrich
MAG-fullHit@10 0 0.216 0.003
MRR 0.002 0.221 0.008
MAG-CSHit@10 0.022 0.301 0.132
MRR 0.059 0.376 0.204
OAG-AIHit@10 0.022 0.343 0.166
MRR 0.059 0.422 0.238
Experimental Results. Table 10 displays extraction results for
search engine-based scholar profiling. In Table 10, pre-trained mod-
els outperform traditional methods, showcasing the expressive
capacity and effectiveness of pre-trained models without manual
feature design. BI-LSTM-CRF and partial pre-trained models exhibit
similar performance in position extraction, showing the suitability
of both sequence labeling and pre-trained models. Large generative
models excel in homepage extraction, though accuracy remains
modest.
A.3 Concept Taxonomy Completion
In this subsection, we provide the experimental setup and results
for concept taxonomy completion.
Baselines. Some of the latest concept taxonomy completion meth-
ods are selected for comparison. (1) BiLinear [44]: uses a bilin-
ear model to encode new and candidate concept representations,
performing binary classification to ascertain if a candidate posi-
tion owns the correct hypernym and hyponym of a new concept.
(2)TaxoExpan [40]: employs a position-augmented graph neu-
ral network to gauge the relationship between new concepts and
candidate concept subgraphs, using contrastive learning to bolster
model robustness. We use SciBERT [ 3] to encode concepts for a fair
comparison with the next method. (3) TaxoEnrich [21]: initially
transforms the existing hyponymy relationship into natural lan-
guage, using SciBERT to represent concepts. It then employs LSTM
to encode vertical concept relationships and an attention mecha-
nism for sibling relationships. Finally, a matching model calculates
the score between a new concept and a candidate position.
Evaluation Metrics. Each new concept is matched with nodes in
the existing concept hierarchy tree and sorted by similarity. Eval-
uation metrics include Hit@10 and Mean Reciprocal Rank (MRR),
which is the average rank of the reciprocal of actual hypernyms.
Experimental Results. Table 11 reports the performance of con-
cept taxonomy completion. TaxoExpan outperforms other meth-
ods on three datasets, indicating the effectiveness of the position-
augmented graph neural network. TaxoEnrich surpasses TaxoEx-
pan in its paper, likely due to its more potent pre-trained represen-
tation. The BiLinear model’s simplicity limits its expressive power,
affecting prediction accuracy. The results of OAG-AI are obtained by
making inferences using the pre-trained model on MAG-CS, main-
taining similar trends as other datasets. Hit@10 doesn’t exceed 0.35
on all datasets, indicating the challenge of automatic taxonomy con-
struction and the potential need for more information or powerful
model architectures.
6225