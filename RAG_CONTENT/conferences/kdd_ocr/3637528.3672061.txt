Conformalized Link Prediction on Graph Neural Networks
Tianyi Zhao
tzhao566@usc.edu
University of Southern California
Los Angeles, USAJian Kang
jian.kang@rochester.edu
University of Rochester
Rochester, USALu Cheng
lucheng@uic.edu
Univeristy of Illinois Chicago
Chicago, USA
ABSTRACT
Graph Neural Networks (GNNs) excel in diverse tasks, yet their ap-
plications in high-stakes domains are often hampered by unreliable
predictions. Although numerous uncertainty quantification meth-
ods have been proposed to address this limitation, they often lack
rigorous uncertainty estimates. This work makes the first attempt
to introduce a distribution-free and model-agnostic uncertainty
quantification approach to construct a predictive interval with a
statistical guarantee for GNN-based link prediction. We term it as
conformalized link prediction. Our approach builds upon conformal
prediction (CP), a framework that promises to construct statisti-
cally robust prediction sets or intervals. There are two primary
challenges: first, given dependent data like graphs, it is unclear
whether the critical assumption in CP ‚Äî exchangeability ‚Äî still
holds when applied to link prediction. Second, even if the exchange-
ability assumption is valid for conformalized link prediction, we
need to ensure high efficiency, i.e., the resulting prediction set or
the interval length is small enough to provide useful information.
To tackle these challenges, we first theoretically and empirically
establish a permutation invariance condition for the application of
CP in link prediction tasks, along with an exact test-time coverage.
Leveraging the important structural information in graphs, we then
identify a novel and crucial connection between a graph‚Äôs adherence
to the power law distribution and the efficiency of CP. This insight
leads to the development of a simple yet effective sampling-based
method to align the graph structure with a power law distribution
prior to the standard CP procedure. Extensive experiments demon-
strate that for conformalized link prediction, our approach achieves
the desired marginal coverage while significantly improving the
efficiency of CP compared to baseline methods.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíArtificial intelligence.
KEYWORDS
Graph Neural Networks, Uncertainty Quantification, Conformal
Prediction, Link Prediction
ACM Reference Format:
Tianyi Zhao, Jian Kang, and Lu Cheng. 2024. Conformalized Link Predic-
tion on Graph Neural Networks. In Proceedings of the 30th ACM SIGKDD
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3672061Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24), August
25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3672061
1 INTRODUCTION
GNNs have emerged as a versatile and powerful model that can
operate on graph-structured data, such as social networks [ 11],
molecular graphs [ 22], and knowledge graphs [ 34]. Their ability to
model complex relationships in graph-structured data has propelled
them to the forefront of machine learning research. However, one
of the major challenges in applying GNNs to real-world problems
is the lack of reliable uncertainty estimates for their predictions.
A series of recent research has shown mixed results regarding the
performance of GNNs [ 19,33,44]. For example, when used in high-
stakes domains such as drug discovery and finance, GNN-based link
prediction may not be trusted due to its miscalibrated confidence.
This work studies uncertainty quantification for GNN-based link
prediction. One prominent approach is to construct prediction sets
or intervals that provide information about a plausible range of
values within which the true outcome is likely to fall. Numerous
methods have been put forth to achieve this goal [ 19,28,44,51].
Nevertheless, these methods fall short in terms of offering both
theoretical and empirical assurances concerning their validity. Con-
formal prediction (CP) [ 43] has emerged as a promising framework
to tackle these limitations and has been applied to various domains,
such as natural language processing [ 14,38,40], causal inference
[31], computer vision [ 4,6,7] and drug discovery [ 23]. It is a frame-
work that promises to construct prediction sets or intervals while
ensuring a statistically robust coverage guarantee. That is, given
a user-specified miscoverage rate ùõº‚àà(0,1), CP uses a so-called
calibration set of data to produce prediction intervals (often for re-
gression) or prediction sets (often for classification) for the test data,
and the resulting set or interval covers the true label or value with
probability at least 1‚àíùõº. Or, the constructed prediction sets/intervals
are theoretically proven to have a guarantee that they will only
miss the test outcomes in at most an ùõºfraction of cases.
Further, CP offers the advantage of being compatible with any
black-box machine learning model, under the condition that the
data follows the principle of statistical exchangeability (e.g., the
calibration and test data are exchangeable in conformal prediction).
This flexibility alleviates the need for the often violated assumption
of independent and identically distributed (i.i.d.) data, particularly
common in graph-structured datasets. With its simple formulation,
weaker assumption, strong theoretical guarantee and distribution-
free nature, a few recent efforts [ 9,21,50] have explored to use CP
to quantify uncertainty for graph-structured data, with a particular
emphasis on tasks like node classification. Complementary to these
prior works, we explore the realm of CP for link prediction, which
is related yet inherently different from node classification tasks. To
4490
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Tianyi Zhao, Jian Kang, & Lu Cheng
illustrate the importance, consider the GNN-based recommender
system in a pharmacy store that suggests Over-the-Counter (OTC)
medicine to patients. When the system over-confidently recom-
mends inappropriate medicine, patients can be exposed to high
risks of adverse effects. In this case, the rigorous prediction interval
produced by CP under a predefined error rate (say 10%) can help
assess the reliability of the system. In this case, a larger interval
indicates higher uncertainty, highlighting the need for caution and
possibly consulting clinicians for a more informed decision.
In this work, we study a novel problem of conformalized link
prediction (CLP). A central challenge arises from the question of
whether the critical condition of exchangeability still holds when
performing CP at the edge level. In response to this challenge, this
work first seeks to thoroughly examine the validity of the exchange-
ability assumption in GNN-based link prediction. Particularly, we
formally define this problem within an inductive setting, where cal-
ibration and test edges are excluded from the training process. We
then theoretically examine the exchangeability between calibration
and test data for link prediction, i.e., whether the distributions of
calibration and test data are exchangeable under any permutation.
In CP, when the exchangeability assumption is satisfied, the cov-
erage is statistically guaranteed. However, we also need to ensure
that the prediction set or the interval length is small enough to be
informative. For example, CP can output a trivial interval or set that
includes all possible labels, resulting in useless predictions. Existing
approaches (e.g., [ 3,49]) for improving efficiency are inapplicable
due to the unique features of graph data.
To address this challenge, we propose to leverage structural prop-
erties, one of the most important and unique features in graph data.
Graph structures provide vital information and have been shown
extremely useful for a variety of graph-related tasks, such as node
classification [ 45], link prediction [ 47], and graph classification [ 29].
Therefore, we ask whether graph structures can provide additional
information to improve the efficiency of standard CP for CLP ? One
particular type of structural information we investigate is the node
degree and its distribution. As a fundamental property in graphs,
node degree reflects the connectivity of a node within the graph and
provides valuable insights into the structure, function, and behavior
of networks [ 36]. Informed by this, we conduct a series of empirical
analyses and identify an interesting finding: a greater adherence
to the power law in the node degree distribution typically leads to
significantly increased CP efficiency (Figure. 1). This inspiration
drives us to propose a simple approach for harmonizing the degree
distribution of a graph with a power-law distribution for more effi-
cient CLP. This is achieved by selectively removing specific edges
and utilizing the remaining edges for the CLP process.
In summary, our main contributions are:
‚Ä¢We propose a novel problem of CLP on GNNs and theo-
retically establish the condition of exchangeability for CLP,
affirming the validity of employing CP for CLP.
‚Ä¢We develop a novel pipeline for efficient CLP via a simple
sampling-based approach guided by the fundamental power
law distribution of node degrees.
‚Ä¢We evaluate the proposed method on real-world graph datasets
for the link prediction task. The experimental results suggest
that our approach can significantly improve CP‚Äôs efficiency,especially when the degree distribution in a graph is less
adherent to the power law distribution.
2 PRELIMINARY
Notation. Consider a graphG=(V,Eùëù)withùëÄnodes, where
V={1,¬∑¬∑¬∑,ùëÄ}andEùëù={ùëí1,¬∑¬∑¬∑,ùëíùëÅ}‚äÜV√óV .ùëã‚ààRùëÄ√óùëë
denotes the node feature matrix. Let EùëùandEùëõbe the positive links
set and non-existent links set, respectively. The latter is constructed
by randomly selecting the same number of non-existent links as
the number of positive links from the graph. E=Eùëù‚à™Eùëõ=
{ùëí1,¬∑¬∑¬∑,ùëí2ùëÅ}. Eachùëí‚ààEis represented as a node pair ùëí=(ùë¢,ùë£)
with nodesùë¢andùë£as its two endpoints.
The Link Prediction Problem. Link prediction with GNNs is
typically based on representation learning [ 1,26,37]. Particularly,
we start by obtaining the node representations. Then we derive
edge representations based on the learned node representations,
often using operations like the dot product between two node rep-
resentations. These edge representations can then be employed to
estimate the likelihood of a link between them. GNNs are employed
to acquire node representations that encode both the topological
structure and the feature information associated with each node.
We measure the performance of a link prediction model by how
well it can rank the true links higher than the false ones in the
test set. A common metric for this is the ratio of true links that
are among the top ùêæ-ranked links by the model [ 20]. It is expected
that the ranking of scores for positive edges will surpass that of
non-existent edges.
Conformal Prediction. Conformal prediction (CP) is a distribution-
free framework in machine learning and statistical modeling that
assigns valid confidence estimates or prediction intervals to the
output of predictive models [ 43]. One of the most common CP
methods is split CP [ 30]. It acts as a wrapper around a trained base
model and uses a set of exchangeable held-out (or calibration) data
to construct prediction intervals.
Given an exchangeable set of held-out calibration data {(ùëãùëñ,ùëåùëñ)}ùëõ
ùëñ=1,
the goal of CP is to construct a marginal distribution-free predic-
tion interval ùê∂(ùëãùëõ+1)‚ààRthat is likely to encompass the unknown
responseùëåùëõ+1with a specified miscoverage rate ùõº‚àà[0,1]:
P{ùëåùëõ+1‚ààùê∂(ùëãùëõ+1)}‚â• 1‚àíùõº. (1)
To achieve this, we first define a non-conformity score function
ùëâ:X√óY‚Üí R, which measures the calibration of the prediction for
a specific sample, i.e., how true value ùë¶conforms to model prediction
atùë•. For each(ùëãùëñ,ùëåùëñ)in the calibration set, we first compute the
non-conformity score ùëâ(ùëãùëñ,ùëåùëñ). Next, we define bùëûto be the‚åà(ùëõ+
1)(1‚àíùõº)‚åâ/ùëõ-th empirical quantile of {ùëâ(ùëã1,ùëå1),¬∑¬∑¬∑,ùëâ(ùëãùëõ,ùëåùëõ)}.
The prediction interval can then be constructed as follows:
ùê∂(ùëãùëõ+1)={ùë¶‚ààY :ùëâ(ùëãùëñ,ùë¶)‚â§bùëû}. (2)
The Conformalized Quantile Regression (CQR) method [ 39] dis-
tinguishes itself as a widely recognized CP technique for creating
prediction intervals due to its simplicity and effectiveness. To apply
CQR, we divide the data into a training set D1and a calibration
setD2. Next, we employ a quantile regression function denoted
asAto fit two conditional quantile functions, namely bùúáùõº/2and
bùúá1‚àíùõº/2, utilizing the training set. Subsequently, we compute the
4491Conformalized Link Prediction on Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
non-conformity scores using the calibration set:
ùëâùëñ=max{bùúáùõº/2(ùëãùëñ)‚àíùëåùëñ,ùëåùëñ‚àíbùúá1‚àíùõº/2(ùëãùëñ)}, (3)
for each(ùëãùëñ,ùëåùëñ)‚ààD 2. The scores are then used to calibrate the
plug-in prediction interval
bùê∂(ùë•)=[bùúáùõº/2(ùë•),bùúá1‚àíùõº/2(ùë•)]. (4)
More specifically, let bùëûbe the‚åà(|D 2|+1)(1‚àíùõº)‚åâ/|D 2|-th empirical
quantile of{ùëâ(ùëã1,ùëå1),¬∑¬∑¬∑,ùëâ(ùëã|D2|,ùëå|D2|)}, the prediction inter-
val for a new input data ùëã‚Ä≤is then constructed as
ùê∂(ùëã‚Ä≤)=[bùúáùõºùõº/2(ùëã‚Ä≤)‚àíbùëû,bùúáùõº1‚àíùõº/2(ùëã‚Ä≤)+bùëû]. (5)
3 CONFORMALIZED LINK PREDICTION
We begin this section by formulating and investigating the validity
of conformalized link prediction (CLP), i.e., whether the exchange-
ability assumption holds for GNN-based link prediction. It should
be noted that this is a critical step for ensuring the statistical guar-
antee of CP. Yet, there is no prior work that formally studies CP in
the context of link prediction, and the adaptation of CP to CLP is
nontrivial. Unlike [ 21], which explored CP for node classification in
atransductive setting, our work establishes CP for link prediction
within an inductive learning framework. Then we introduce how
to leverage Conformalized Quantile Regression (CQR) [ 39] for CLP,
and further investigate the relationship between the efficiency of
CP and the graph‚Äôs structural property. Based on the empirical anal-
ysis, we propose a simple and effective sampling strategy guided by
the fundamental power law distribution of node degrees to improve
the efficiency of CLP.
3.1 Exchangeability and Validity of
Conformalized Link Prediction
The link prediction problem discussed here naturally fits into an
inductive learning framework. To elaborate, we initially divide the
set of links, denoted as E, into distinct subsets: the training set
(Dùë°ùëüùëéùëñùëõ ), the validation set ( Dùë£ùëéùëô), the calibration set ( Dùëêùëéùëôùëñùëè ), and
the test set (Dùë°ùëíùë†ùë°). Each of these subsets contains an equal number
of positive links (indicating existing connections) and negative
links (indicating non-existent connections). The GNN model is then
trained on a subgraph denoted as G‚Ä≤=(V,E‚Ä≤), whereE‚Ä≤=Eùëù‚à©
(Dùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô). In other words, the model can access information
about all the nodes and their associated features, but it only has
access to a portion of the positive links that belong to the training
and validation link sets. The objective is to train the model to
predict the edges that have not been observed between pairs of
nodes. During the training process, we assign label 1 (or 0) to
represent positive (or non-existent) edges. The GNN model begins
by generating embeddings for edges through message passing and
neighborhood aggregation. Subsequently, it produces prediction
scores for all edges, which can be used to determine the likelihood
of an edge existing between node pairs.
Since the model lacks access to the labels (indicating the status)
of links inDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°, any link from this combined set is equally
likely to be part of either Dùëêùëéùëôùëñùëè orDùë°ùëíùë†ùë°. In other words, different
choices of calibration sets do not alter the non-conformity scores
for any given link. Using GNNs for link prediction on graphs thus
adheres to the following permutation invariance condition: Forany permutation ùúãofDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°, the non-conformity score ùëâ
satisfies the following:
ùëâ(ùëí,ùë¶;{(ùëíùëñ,ùë¶ùëñ)}ùëíùëñ‚ààDùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô,{ùëíùëñ}ùëíùëñ‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°,G‚Ä≤)
=ùëâ(ùëí,ùë¶;{(ùëíùëñ,ùë¶ùëñ)}ùëíùëñ‚ààDùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô,{ùëíùúã(ùëñ)}ùëíùúã(ùëñ)‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°,G‚Ä≤).
This states that permuting the order of links within the calibration
and test sets does not alter their corresponding non-conformity
scores. Therefore, the exchangeability of Dùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°is natu-
rally satisfied. To this end, we can present the following proposition,
demonstrating that the non-conformity scores exhibit exchange-
ability with respect to link prediction.
Proposition 3.1. In the described inductive setting for link predic-
tion, where the model has access to all node information and features
but only a subset of positive links from training and validation sets
during training, the unordered set of the scores [ùëâùëñ]ùêæ+ùêø
ùëñ=1is fixed, where
|Dùëêùëéùëôùëñùëè|=ùêæ,|Dùë°ùëíùë†ùë°|=ùêø, andùëâùëñdenotes the non-conformity score
of linkùëíùëñ‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°. That is, the non-conformity scores are
exchangeable for all ùëí‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°.
Proof. Letùëì(¬∑)be the GNN model trained on the subgraph G‚Ä≤
which produces the node embeddings ùêª. Letùëî(¬∑)denote the func-
tion that produces edge embeddings, i.e., ùëßùëíùëñ=ùëî(ùêªùë¢ùëñ,ùêªùë£ùëñ)for edge
ùëíùëñ=(ùë¢ùëñ,ùë£ùëñ).‚Ñé(¬∑)is the function that produces the prediction scores
based on the edge embeddings, i.e., ùë†ùëñ=‚Ñé(ùëßùëíùëñ). Letùë£ùëñ=ùëâ(ùë†ùëñ,ùë¶ùëñ)be
the non-conformity score for ùëíùëñ‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°.ùëì(¬∑),ùëî(¬∑), and‚Ñé(¬∑)
are fixed after training. Thus it is clear that permutating the order
ofùëí‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°will not change the resulting non-conformity
scores forùëíùëñ‚ààDùëêùëéùëôùëñùëè‚à™Dùë°ùëíùë†ùë°. The sets of non-conformity scores
before and after permutation are exactly the same:
{ùë£1,¬∑¬∑¬∑,ùë£ùêæ+ùêø}={ùë£ùúã(1)¬∑¬∑¬∑ùë£ùúã(ùêæ+ùêø)}.
‚ñ°
3.2 CQR for Conformalized Link Prediction
With the fundamental exchangeability assumption satisfied, we
now introduce how CP can be better leveraged to quantify the
uncertainty for the link prediction task. Link prediction is framed
as a task where a model is trained to produce prediction scores for
all missing edges [ 20]. The expectation is that the model will rank
the prediction scores for positive test edges higher than those for
negative edges. In the context of uncertainty quantification for link
prediction, it is more appropriate to formulate it as a regression
problem and construct a prediction interval instead of viewing it as
a binary classification process and creating a prediction set for each
unobserved edge. We therefore propose to leverage Conformalized
Quantile Regression (CQR) [ 39] which provides prediction intervals
that come with a provable guarantee of coverage probability. For
link prediction, we adapt the non-conformity score in CQR to the
following:
ùëâ(ùëí,ùë¶)=max{bùúáùõº/2(ùëßùëí)‚àíùë¶,ùë¶‚àíbùúá1‚àíùõº/2(ùëßùëí)}, (6)
wherebùúáùõæ(¬∑)denotes the ùõæ-th conditional quantile function of the
edge embeddings ùëßùëí. The prediction interval is then constructed as
ùê∂(ùëí)=[bùúáùõº/2(ùëßùëí)‚àíbùëû,bùëû‚àíbùúá1‚àíùõº/2(ùëßùëí)]. (7)
Based on Proposition 3.1, we prove that the validity of coverage
ùê∂(ùëí)is guaranteed.
4492KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Tianyi Zhao, Jian Kang, & Lu Cheng
Theorem 3.2. Given that{ùë£ùëñ}ùêæ+ùêø
ùëñ=1is exchangeable, with error rate
ùõº‚àà(0,1)andbùëû=Quantile(ùë£1,¬∑¬∑¬∑,ùë£ùêæ;‚åà(ùêæ+1)(1‚àíùõº)/ùêæ‚åâ), the con-
structed prediction interval for edge ùëíùêæ+ùëóisùê∂(ùëíùêæ+ùëó)=[bùúáùõº/2(ùëßùêæ+ùëó)‚àí
bùëû,bùëû‚àíbùúá1‚àíùõº/2(ùëßùêæ+ùëó)],ùëó={1,¬∑¬∑¬∑,ùêø}, satisfying
P{ùë¶ùêæ+ùëó‚ààùê∂(ùëíùêæ+ùëó)}‚â• 1‚àíùõº.
Proof. Letùë£ùêæ+1be the non-conformity score for the test link
(ùëíùêæ+1,ùë¶ùêæ+1). We have
P{ùë¶ùêæ+1‚ààùê∂(ùëíùêæ+1)}=P{ùë£ùêæ+1‚â§bùëû}.
Without loss of generality, we assume that {ùë£ùëñ}ùêæ
ùëñ=1is sorted in
ascending order, i.e., ùë£1‚â§ùë£2‚â§¬∑¬∑¬∑‚â§ùë£ùêæ. Since{ùë£ùëñ}ùêæ+1
ùëñ=1are ex-
changeable, we have
P{ùë£ùêæ+1‚â§ùë£ùë°}=ùë°
ùêæ+1(1‚â§ùë°‚â§ùêæ)
that is,ùë£ùêæ+1is equally likely to fall in anywhere between ùë£1,¬∑¬∑¬∑,ùë£ùêæ.
Thus the following inequality holds:
P{ùë£ùêæ+1‚â§bùëû}=P{ùë£ùêæ+1‚â§ùë£‚åà(ùêæ+1)(1‚àíùõº)‚åâ}
=‚åà(ùêæ+1)(1‚àíùõº)‚åâ
ùêæ+1
‚â•1‚àíùõº.
‚ñ°
Theorem 3.2 suggests that the coverage of the prediction interval
in CLP is at least 1‚àíùõºwith a rigorous statistical guarantee.
3.3 Efficiency and Structural Property
In addition to the coverage rate, another important evaluation met-
ric for CP is efficiency, i.e., the size or the length of the prediction
sets or intervals. A smaller size or length suggests a more infor-
mative prediction set or interval. To assess the efficiency of CLP, a
simple approach is to measure the average length of the prediction
interval at a given error rate ùõº. A shorter interval length suggests
an improved efficiency.
Traditional approaches [ 4,46] for improving CP efficiency can-
not be directly applied as they are for non-graph data (e.g., tabular
data or images), leaving the unique characteristics (e.g., structural
properties) of graph data largely unexplored. To improve the ef-
ficiency of CLP, we explore its potential connection to the graph
structural information. Identifying crucial graph data properties
that affect graph learning is an ongoing challenge [ 48]. Here we
specifically focus on node degree distribution, which reflects node
connectivity and provides valuable insights into network structure
and behavior. It is widely recognized as a significant factor impact-
ing graph model performance [ 32,48]. Other structural properties
such as clustering coefficient and connectivity could be valuable to
explore in future research. Next, we reveal a novel and interesting
connection between the efficiency of the CLP and the node degree
distribution of the underlying graph structure.
We commence our study with a series of experiments on semi-
synthetic graphs that exhibit varying degrees of conformity to the
power law, as outlined in [ 10]. These graphs are generated using the
method introduced in [ 53]. Specifically, given a real-world graph,
we createùëõcliques within a given graph by randomly selecting ùëö
0.04 0.06 0.08 0.10 0.12
KS Statistic0.920.940.960.981.001.021.041.06Average Length of Prediction Interval
Figure 1: Simulation study on a semi-synthetic dataset gen-
erated from the Amazon Computers dataset [41].
nodes and connecting all nodes within each clique. By adjusting the
values ofùëöandùëõ, we can generate synthetic graphs with different
levels of conformity to the power law. The Kolmogorov-Smirnov
(KS) statistic [ 8] is employed as a metric to quantify the extent
of conformity, where a lower value indicates a higher degree of
conformity to the power law. Subsequently, we carry out simulation
experiments based on the Amazon Computers dataset [41].
Specifically, we select ( ùëö,ùëõ) from{(25,20),(50,20),(75,20),(100,
20),(150,20)}. For each combination of (ùëö,ùëõ), we create five syn-
thetic graphs and apply the CLP procedure described in Sec. 3.2. We
then record the average length of the prediction interval as a mea-
sure of CP efficiency. To represent the performance of a particular
(ùëö,ùëõ), we calculate the mean KS statistic value by averaging the
KS statistic values of the five synthetic graphs with the same (ùëö,ùëõ).
The simulation results are presented in Figure 1, in which the hor-
izontal axis value is the averaged KS statistic. The trend is evident:
graphs exhibiting higher KS statistic values typically display larger
average prediction intervals, suggesting lower efficiency in CLP ‚Äì
that is, less informative prediction intervals. This notable finding
inspires us to explore the potential enhancement of CP efficiency
when conducting CLP by utilizing edges with a degree sequence
that closely aligns with the power law distribution.
3.4 Sampling-based CQR for Improved
Efficiency
Based on the findings above, we propose a simple yet effective
sampling-based method for enhanced CP efficiency to quantify the
uncertainty in GNN-based link prediction.
Our core idea is to bring the degree distribution of the exist-
ing graph into closer alignment with a power-law distribution, a
modification that we believe will enhance the efficiency of CLP,
as supported by our empirical research. One approach to achieve
this is by selectively sampling specific edges such that the resulting
node degree distribution closely follows the power-law distribu-
tion. We then use these sampled edges to compute nonconformity
scores. Therefore, the first step involves obtaining an ideal degree
sequence that adheres to a specific power-law distribution, serving
as a reference. Subsequently, the sampling procedure is carried out,
taking cues from this ideal degree sequence. The sampling process
is detailed below.
4493Conformalized Link Prediction on Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Algorithm 1: Conformalized Link Prediction.
Input:
GraphG=(V,Eùëù)and links setE.
Miscoverage level ùõº‚àà(0,1).
Node embedding algorithm F, edge embedding
algorithmZ, edge scoring algorithm H.
Quantile regression algorithm A.
Output:
Prediction interval ùê∂(ùëí)for eachùëí‚ààDùë°ùëíùë†ùë°.
1Split links set into disjoint sets Dùë°ùëüùëéùëñùëõ,Dùë£ùëéùëô,Dùëêùëéùëôùëñùëè ,Dùë°ùëíùë†ùë°;
2Construct subgraph G‚Ä≤=(V,E‚Ä≤), where
E‚Ä≤=Eùëù‚à©(Dùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô);
// Train the base model
3while training do
4 Fit node embedding function: ùëì(¬∑)‚ÜêF(G‚Ä≤);
5 Fit edge embedding function:
ùëß(¬∑)‚ÜêZ({( ùëì(ùë¢),ùëì(ùë£))|ùëí=(ùë¢,ùë£)‚ààDùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô});
6 Fit edge scoring function:
‚Ñé(¬∑)‚ÜêH({( ùëß(ùëí),ùë¶ùëí)|ùëí‚ààDùë°ùëüùëéùëñùëõ‚à™Dùë£ùëéùëô});
7end
// Sampling
8D‚Ä≤
ùë°ùëüùëéùëñùëõ,D‚Ä≤
ùë£ùëéùëô,D‚Ä≤
ùëêùëéùëôùëñùëè‚ÜêùëÜùëéùëöùëùùëôùëñùëõùëî(Dùë°ùëüùëéùëñùëõ,Dùë£ùëéùëô,Dùëêùëéùëôùëñùëè);
// CLP
9Fit conditional quantile functions:
{bùúáùõº/2(¬∑),bùúá1‚àíùõº/2(¬∑)}‚ÜêA({( ùëß(ùëí),ùë¶ùëí)|ùëí‚ààD‚Ä≤
ùë°ùëüùëéùëñùëõ‚à™D‚Ä≤
ùë£ùëéùëô});
10Compute the non-conformity score
ùëâ(ùëí,ùë¶ùëí)=max{bùúáùõº/2(ùëßùëí)‚àíùë¶ùëí,ùë¶ùëí‚àíbùúá1‚àíùõº/2(ùëßùëí)}for each
ùëí‚ààD‚Ä≤
ùëêùëéùëôùëñùëè;
11Compute bùëû, the‚åà(|D‚Ä≤
ùëêùëéùëôùëñùëè|+1)(1‚àíùõº)/|D‚Ä≤
ùëêùëéùëôùëñùëè|)‚åâ-ùë°‚Ñé
empirical quantile of {ùëâ(ùëí,ùë¶ùëí)|ùëí‚ààD‚Ä≤
ùëêùëéùëôùëñùëè};
12Construct prediction interval
ùê∂(ùëí)=[bùúáùõº/2(ùëßùëí)‚àíbùëû,bùëû‚àíbùúá1‚àíùõº/2(ùëßùëí)]for eachùëí‚ààDùë°ùëíùë†ùë°.
3.4.1 Fitting the power-law distribution. Suppose that the node de-
greeùëëfollows a discrete power-law distribution starting at ùëëùëöùëñùëõ‚â•
1, then the probability density function (PDF) of the power-law is
defined as
Pr(ùëë)=1
ùúÅ(ùõΩ,ùëëùëöùëñùëõ)ùëë‚àíùõΩ, (8)
whereùúÅ(ùõΩ,ùëëùëöùëñùëõ)=√ç‚àû
ùëñ=0(ùëñ+ùëëùëöùëñùëõ)‚àíùõΩis the Hurwitz zeta function
andùõΩdenotes the scaling exponent for power law distribution.
To determine the best-fitting power-law distribution for a given
degree sequence, our primary objective centers on estimating ùõΩ,
which is the only unknown parameter in the PDF of the power-law
distribution. Estimating ùõΩrequires selecting ùëëùëöùëñùëõ, determined by
the standard Kolmogorov-Smirnov minimization approach. This
method identifies ùëëùëöùëñùëõas the value minimizing the maximum abso-
lute difference between the empirical distribution ùê∏(ùëë)and the cu-
mulative distribution function of the best-fitting power law ùëÉ(ùëë|bùõΩ)for degreesùëë‚â•ùëëùëöùëñùëõ[8]. The estimated bùõΩis then obtained by [ 10]
bùõΩ=1+ùëõ"ùëõ‚àëÔ∏Å
ùëñ=1logùëëùëñ
ùëëùëöùëñùëõ‚àí1
2#‚àí1
. (9)
3.4.2 Generating ideal degree sequence with bùõΩ-parameterized power-
law distribution. In this step, we generate a degree sequence adher-
ing to power law distribution. Various distribution functions follow
the power law. Here we utilize the Pareto distribution [ 5] as the
specific power-law function to generate the degree sequence, pa-
rameterized by bùõΩand the number of nodes ùëõ. Specifically, we define
the PDF of the Pareto distribution in link prediction as follows
ùëì(ùë•;ùë•ùëö,bùõΩ)=bùõΩ¬∑ùë•bùõΩ
ùëö
ùë•bùõΩ+1,forùë•‚â•ùë•ùëö, (10)
whereùë•ùëöis a scale parameter (minimum value for which the dis-
tribution is defined) and bùõΩserves as a shape parameter (indicating
the distribution‚Äôs tail heaviness and skewness).
3.4.3 Sampling edges from the original graph for a degree distri-
bution that follows the power law. We begin by computing the em-
pirical cumulative distributions for both the degree sequence of
the original graph and the ideal degree sequence. Following this,
we establish sampling probabilities for edges, determined by the
deviationùëëùë£ùëñùëé(ùëë)between these two distributions. Specifically, for
a given edge ùëí=(ùë¢,ùë£), we denote the degree of nodes ùë¢andùë£as
ùëëùë¢andùëëùë£, respectively. The deviation ùëëùë£ùëñùëé(ùëë)is calculated as:
ùëëùë£ùëñùëé(ùëë)=|eCDFùê∑(ùëë)‚àíeCDFùê∑‚Ä≤(ùëë)|, (11)
where eCDFùê∑(¬∑)andeCDFùê∑‚Ä≤(¬∑)represents the empirical CDFs of
the original degree sequence ùê∑and the ideal degree sequence ùê∑‚Ä≤,
respectively. Subsequently, the sampling probability of edge ùëícan
be determined by:
P(ùëí)=min{ùúÜ¬∑ùëÜ(ùëëùë£ùëñùëé(ùëëùë¢),ùëëùë£ùëñùëé(ùëëùë£)),1}, (12)
whereùúÜ>0is a hyperparameter and ùëÜ(¬∑)denotes the function of
aggregating the two deviation scores, e.g., the operation of summa-
tion. In this process, we prioritize edges with greater deviations by
assigning them a higher probability considering the deviation direc-
tion. To put this into action, we generate a random floating-point
number, denoted as ùëüùëí, within the range of [0,1)for each edge ùëí. If
ùëüùëí‚â§P(ùëí), we retain this edge. Otherwise, we remove it from the
original set of edges.
The overall algorithm for CLP is presented in Algorithm 1. We
first train a base GNN model for standard link prediction from line
3 to line 7. Then, in line 8, the proposed sampling procedure is
implemented. Finally, we apply the conformalized link prediction
method on the sampled edge set from line 9 to line 12 to obtain
prediction intervals.
4 EXPERIMENTS
In this section, we conduct experiments on real-world graph datasets
across various domains (e.g., biology, citation network, and social
network) to evaluate the performance of our proposed method. In
particular, we aim to answer the following research questions:
‚Ä¢Does the proposed CLP procedure attain the desired coverage
in practical implementations?
4494KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Tianyi Zhao, Jian Kang, & Lu Cheng
Table 1: Basic statistics of the datasets.
Dataset Name #Nodes #Edges KS Statistic
ogbl-ddi 4,267 1,334,889 0.3275
ogbl-ppa 576,289 30,326,273 0.0908
ogbl-citation2 2,927,963 30,561,187 0.0302
german credit 1,000 22,242 0.1133
rochester38 4,563 167,653 0.1446
‚Ä¢Does the proposed S-CQR for CLP effectively enhance CP
efficiency?
‚Ä¢How does the proposed S-CQR perform across different
GNN-based link prediction models?
‚Ä¢How does the involved hyperparameter impact the perfor-
mance of CLP procedure?
4.1 Experimental Setup
4.1.1 Datasets. We evaluate the proposed CLP procedure on five
benchmark datasets for link prediction, including the drug-drug
interaction network ogbl-ddi , protein interaction network ogbl-ppa ,
citation network ogbl-citation2 [20], and two social networks Ger-
man Credit [2] and Rochester38 [42]. This selection spans various
graph scales, from smaller ones to large-scale graph datasets with
millions of nodes, showcasing the wide-ranging applicability of our
methods in real-world web contexts. The basic statistics of these
datasets are shown in Table 1. We can see that the node degree
distribution in ogbl-ddi dataset adheres least to the power-law
distribution, followed by the Rochester38 dataset.
4.1.2 Backbone Models. We employ a three-layer Graph Convo-
lutional Network (GCN) [ 27] and GraphSAGE [ 17] as the base
models for link prediction. Note that any GNN models can be inte-
grated into our proposed CP pipeline. CQR is implemented using
neural networks for quantile regression, and the neural network
architecture consists of three fully connected layers, with ReLU
nonlinearities mapping between layers.
4.1.3 Evaluation Setup. For ogbl-ddi, ogbl-ppa, and ogbl-citation2
datasets, we use the splits given in the original papers [ 20]. For
German Credit and Rochester38, we split the links into sets as
follows: 50% for training ( Dùë°ùëüùëéùëñùëõ ), 10% for validation ( Dùë£ùëéùëô), 20%
for calibration (Dùëêùëéùëôùëñùëè), and 20% for testing ( Dùë°ùëíùë†ùë°). We conduct
five different random splits of calibration and test sets, and perform
10 repetitions of the experiment for each split. Averaged results
are reported below. We then measure empirical coverage and the
average length of prediction interval to evaluate the validity and
efficiency of both CQR for CLP and S-CQR (sampling-based) for
CLP. A detailed experimental setting is provided in Appendix A.
It should be noted that CLP is a relatively recent research field.
To the best of our knowledge, there exists only one prior study [ 35]
related to this issue. However, this work formalizes the problem
in a different way from ours and thus cannot be directly used
for comparison. Specifically, our work focuses on constructing
prediction intervals that bound the miscoverage, while [ 35] focuses
on bounding the false discovery rate.4.2 Main Results
In Table 2, we present the empirical coverage and average length
of prediction intervals across five datasets with GCN as the back-
bone. We can have the following observations according to the
experimental results.
4.2.1 The proposed Conformalized Link Prediction procedures achieve
the target coverage. As shown in Table 2 and Table 3, with the pre-
defined error rate ùõº=0.1, both CQR and S-CQR for Conformal-
ized Link Prediction achieve the desired coverage (90%) on all five
datasets. This empirically validates the theory established in Sec-
tion 3.1. That is, in an inductive scenario, utilizing GNNs for link
prediction on graphs adheres to a particular permutation invariance
requirement and fulfills the exchangeability condition needed for
CP. This enables the design of more advanced uncertainty quantifi-
cation methods for link prediction integrated with CP techniques.
4.2.2 The proposed sampling-based strategy effectively improves
CP efficiency. Our proposed sampling strategy is very effective
at improving the efficiency of the CLP process and assists in the
generation of tighter prediction intervals while maintaining a de-
sirable coverage rate. This result suggests that our proposed CLP
approach can generate more informative prediction intervals in
link prediction, which can be important in critical decision-making
contexts. Particularly, we have the following observations: Firstly,
we measure the KS statistic values for the graphs before and after
the sampling operation outlined in Section 3.4. A lower KS statistic
value suggests that the degree distribution of a graph aligns better
with the characteristics of a power law distribution. The results are
displayed in the ‚ÄòKS Statistic‚Äô column in Table 2. As we can see, the
proposed sampling procedure can generate graphs with a node de-
gree distribution that is more in line with the power law. Secondly,
the proposed S-CQR for CLP effectively reduces the average length
of prediction intervals and increases the efficiency. This validates
our hypothesis in Sec. 3.3 that with the same backbone GNN mod-
els, graphs that closely follow power law distribution typically lead
to higher CP efficiency. Thirdly, based on the results, it is evident
that the proposed approach tends to perform more effectively on
graphs whose degree distributions do not follow power law very
well. For example, when assessing its performance on the ogbl-ppa
and ogbl-citation2 datasets, which prominently follow the power
law distribution (i.e., smallest KS statistics among the five datasets),
the improvements in CP efficiency over the CQR are relatively mod-
est. That is, the enhancements on these two datasets are the most
modest among the five datasets evaluated, amounting to 3.54% and
11.48%, respectively. For the remaining datasets that do not closely
adhere to the power law distribution, the observed enhancements
in efficiency appear to be much more substantial.
4.2.3 CLP performance with different backbone link prediction mod-
els.To examine the impact of backbone GNN models on the perfor-
mance of the S-CQR for CLP process, we replace the GCN model
with GraphSAGE and repeat the above experiments. The results
are shown in Table 3. Our observations are as follows: The pro-
posed approach consistently attains the target coverage rate and
improves efficiency, i.e., reducing the length of prediction interval,
across different backbone GNN models. For instance, on ogbl-ddi
dataset, S-CQR decreases the prediction interval length from 0.7656
4495Conformalized Link Prediction on Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Table 2: Empirical coverage and average length of predictions intervals with target coverage 90% (GCN backbone).
dataset KS Statistic method emp. coverage (%) avg. prediction lengthimproved
efficiency
before 0.32 CQR for CLP 91.79¬±0.02 0.7656¬±0.0135ogbl-ddiafter 0.24 S-CQR for CLP 91.45¬±0.05 0.6321¬±0.0252‚Üë17.43%
before 0.08 CQR for CLP 90.31¬±0.06 0.1720¬±0.0400ogbl-ppaafter 0.04 S-CQR for CLP 90.08¬±0.01 0.1659¬±0.0010‚Üë3.54%
before 0.03 CQR for CLP 90.09¬±0.06 0.1428¬±0.0013ogbl-citation2after 0.02 S-CQR for CLP 90.01¬±0.12 0.1264¬±0.0322‚Üë11.48%
before 0.11 CQR for CLP 91.43¬±0.23 0.9552¬±0.0200german creditafter 0.03 S-CQR for CLP 91.49¬±0.29 0.7080¬±0.0119‚Üë25.87%
before 0.14 CQR for CLP 90.00¬±0.02 0.8078¬±0.0160rochester38after 0.11 S-CQR for CLP 90.14¬±0.13 0.4844¬±0.0165‚Üë40.03%
Table 3: Empirical coverage and average length of predictions intervals with target coverage 90% (GraphSAGE backbone).
dataset KS Statistic methods emp. coverage (%) avg. prediction lengthimproved
efficiency
before 0.32 CQR for CLP 91.77¬±0.03 0.7343¬±0.0072ogbl-ddiafter 0.24 S-CQR for CLP 91.83¬±0.02 0.6619¬±0.0167‚Üë9.85%
before 0.08 CQR for CLP 90.44¬±0.03 0.1698¬±0.0013ogbl-ppaafter 0.04 S-CQR for CLP 90.10¬±0.04 0.1665¬±0.0018‚Üë1.94%
before 0.03 CQR for CLP 90.13¬±0.11 0.1399¬±0.0017ogbl-citation2after 0.02 S-CQR for CLP 90.02¬±0.07 0.1293¬±0.0083‚Üë7.57%
before 0.11 CQR for CLP 90.74¬±0.28 0.9054¬±0.0093german creditafter 0.03 S-CQR for CLP 91.19¬±0.15 0.7402¬±0.0125‚Üë18.25%
before 0.14 CQR for CLP 90.03¬±0.05 0.7065¬±0.0142rochester38after 0.11 S-CQR for CLP 90.01¬±0.01 0.5099¬±0.0236‚Üë27.82%
Table 4: Statistics of sampled graphs under different values
ofùúÜon Rochester38 dataset.
ùúÜ graph density KS Statistic
0.45 0.0143 0.1423
0.40 0.0127 0.1374
0.35 0.0112 0.1246
0.30 0.0097 0.1167
0.25 0.0082 0.0915
0.20 0.0066 0.0719
0.15 0.0049 0.0648
to 0.6321 with GCN and from 0.7343 to 0.6619 with GraphSAGE as
the backbone models, meanwhile achieving the targeted 90% cover-
age rate. The observed efficiency gains across different backbone
models indicate that our method consistently boosts CP perfor-
mance, demonstrating its backbone-model-agnostic effectiveness
and robustness. This consistency aligns with expectations, as the
link prediction process acts as a black box to the subsequent CLP
procedure, suggesting the method‚Äôs adaptability.
4.3 Comparison to Bayesian-based Uncertainty
Quantification Methods
To establish more baselines for comparisons, we further implement
two of the most common bayesian-based uncertainty quantification
0.15 0.20 0.25 0.30 0.35 0.40 0.45
89.089.289.489.689.890.090.290.4empirical coverageempirical coverage
0.400.450.500.550.600.650.700.750.80
avg. prediction length
avg. length of prediction intervalsFigure 2: Performance of S-CQR for conformalized link pre-
diction under different ùúÜon Rochester38 dataset.
methods: BayesianNN [ 13] and Monte Carlo Dropouts [ 24]. The
results on three datasets are presented in Table 5.
Comparing Table 5 with Table 2 and Table 3, we can observe
that though both two Bayesian-based approaches can achieve or
almost achieve the desired coverage rate, they yield much wider
prediction intervals compared to our proposed method. This raises
concerns about the efficiency of these Bayesian-based UQ methods.
Additionally, their computational complexity is another significant
concern.
4496KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Tianyi Zhao, Jian Kang, & Lu Cheng
Table 5: Performance of bayesian-based UQ methods.
dataset
methodempirical
coverageaverage
prediction
length
ogbl-ddiMC
Dropout 0.9905 1.9999
Bay
esianNN 0.9240 1.9865
german
creditMC Dropout 0.8921 1.9989
Bay
esianNN 0.8978 1.8560
r
ochester38MC Dropout 0.9000 1.8023
Bay
esianNN 0.9000 1.7956
4.4 Analysis of Parameter ùúÜ
To understand the effect of the hyperparameter ùúÜinvolved in the
sampling process of S-CQR on its performance, we further conduct
experiments on the Rochester38 dataset (randomly selected) apply-
ing different values of ùúÜ. Adjusting the value of ùúÜimpacts the density
of the sampled graph. Specifically, a higher ùúÜyields a denser graph.
Specifically, we vary ùúÜamong{0.45,0.4,0.35,0.3,0.25,0.2,0.15}and
perform the S-CQR for the CLP procedure. The statistics of the re-
sulting sampled graphs are presented in Table 4. As we can see, by
controlling the value of ùúÜ, we can easily obtain edge sets exhibiting
varying levels of adherence to the power law. A lower ùúÜleads to
increased adherence and reduced edge density.
We further measure the performance of S-CQR for CLP, including
empirical coverage and the average length of prediction intervals,
under varying ùúÜ. The results are shown in Figure 2. We can ob-
serve that while an extremely small value of ùúÜtends to yield graphs
with degree distributions aligning more closely with the power
law, the inevitable decrease in edge density leads to a great loss
of structural information. This subsequently results in a degraded
CP performance, often manifested as an inability to attain the de-
sired coverage and larger prediction intervals. These observations
guide the selection of the optimal value for the parameter ùúÜduring
implementation.
5 RELATED WORKS
5.1 Uncertainty Quantification on Graphs
Graph-based machine learning models, especially in high-stakes
scenarios, demand robust uncertainty quantification to avoid po-
tentially costly errors. However, many current GNNs lack reli-
able uncertainty quantification methods, limiting their practical
application. In previous studies, a common approach was adopt-
ing Bayesian techniques [ 15,25,28]. These methods aimed to ob-
tain a distribution over network weights and quantify uncertainty
through the posterior distribution. In the graph context, UAG [ 12]
used Bayesian uncertainty techniques to devise an uncertainty-
aware attention mechanism to defend against adversarial attacks
on GNNs. B-GCN [ 52] provided a way to integrate uncertain graph
information using a parametric random graph model. GDC [ 18]
tackled issues like over-smoothing and over-fitting commonly seen
in deep GNNs, allowing for learning with uncertainty in graph anal-
ysis tasks and ultimately improving downstream task performance.
However, Bayesian approaches, while theoretically sound, oftenencounter computational challenges. Additionally, the approxima-
tion methods for derivatives come with practical implementation
drawbacks.
In recent years, conformal prediction [ 43] has gained notable
attention as a simple yet potent approach for producing statistically
reliable uncertainty estimates. Nevertheless, conformal prediction
has seen limited application in the context of graph-structured data,
and the majority of existing studies have primarily focused on tasks
related to node-level classification [9, 21, 50] and regression [21].
5.2 Conformal Prediction on Graphs
Research efforts for applying conformal prediction to graph data
have been relatively less. For instance, [ 9] modifies existing con-
formal classification methods by incorporating network structure
to adjust the conformal scores and introduces NAPS, a technique
for constructing prediction sets for node classification in an in-
ductive learning setting. Additionally, [ 16] introduces a confor-
mal approach that provides prediction sets with distribution-free
guarantees, making use of node-wise homophily in a transductive
context. This approach updates conformal scores for each node
based on neighborhood diffusion. Furthermore, [ 21] investigates
the exchangeability of node information in the transductive setting
and introduces a permutation invariance condition that allows the
conformal prediction to operate effectively on graph data. They
also devise a topology-aware output correction model, CF-GNN, to
enhance the efficiency of the conformal prediction procedure.
However, despite the progress in developing conformal predic-
tion methods for node classification and regression, the application
of such approaches to link-level tasks on graphs has remained
under-explored. In our research, we propose conformalized link
prediction to further extend the conformal prediction procedure
to link-level tasks on graphs and demonstrate its validity for link
prediction under an inductive setting. Informed by the empirical
analyses of synthetic data, we then propose a simple yet effective
sampling-based method that leverages the structural properties of
graphs to improve the efficiency of the standard conformal predic-
tion pipeline. The key idea is to sample from the original graph to
generate a new graph whose degree distribution aligns well with
the power law distribution before applying the standard CQR. To
the best of our knowledge, only one previous study has applied
conformal prediction to link prediction on graphs [ 35]. This study,
however, conceptualizes the problem differently from our approach,
with an emphasis on bounding the false discovery rate in contrast
to our focus on bounding the miscoverage rate. This distinction
precludes a direct comparison between the two studies.
6 CONCLUSION
In this research, we delve into the newly identified challenge of
conformalized link prediction (CLP), which applies the principles
of conformal prediction to graph neural network (GNN)-based link
prediction tasks. Our focus is on validating the exchangeability
assumption critical to CLP, for which we introduce a permutation
invariance criterion tailored for link prediction, guaranteeing pre-
cise coverage at test time. Utilizing this criterion, we evaluate the
feasibility of incorporating a standard conformal prediction frame-
work, such as CQR, into CLP. We note a significant drawback in
4497Conformalized Link Prediction on Graph Neural Networks KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
the direct use of CQR, namely its inefficiency, and uncover a crucial
relationship between the graph‚Äôs adherence to a power law distri-
bution and the efficiency of CQR (i.e., the length of the prediction
interval). This insight prompts us to develop a novel sampling-based
conformal prediction technique that modifies the graph structure
to align with a power law distribution, markedly enhancing the ef-
ficiency of conformal prediction. Our experimental findings reveal
that this innovative method not only meets the desired coverage
levels but also significantly narrows the prediction intervals when
compared to existing approaches. Looking ahead, there is potential
for creating more sophisticated conformal prediction strategies for
CLP and expanding this framework to tackle node-level conformal
prediction challenges.
ACKNOWLEDGMENTS
This material is based upon work supported by the National Science
Foundation (NSF) Grant #2312862 and a Cisco gift grant.
REFERENCES
[1]Evrim Acar, Daniel M Dunlavy, and Tamara G Kolda. 2009. Link prediction on
evolving data using matrix and tensor factorizations. In 2009 IEEE International
conference on data mining workshops. IEEE, 262‚Äì269.
[2]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uni-
fied framework for fair and stable graph representation learning. In Uncertainty
in Artificial Intelligence. PMLR, 2114‚Äì2124.
[3]Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra
Malik. 2020. Uncertainty Sets for Image Classifiers using Conformal Prediction.
InInternational Conference on Learning Representations.
[4]Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra
Malik. 2021. Uncertainty Sets for Image Classifiers using Conformal Prediction.
InInternational Conference on Learning Representations. https://openreview.net/
forum?id=eNdiU_DbM9
[5]Barry C Arnold. 2014. Pareto distribution. Wiley StatsRef: Statistics Reference
Online (2014), 1‚Äì10.
[6]Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael
Jordan. 2021. Distribution-free, risk-controlling prediction sets. Journal of the
ACM (JACM) 68, 6 (2021), 1‚Äì34.
[7]Omer Belhasin, Yaniv Romano, Daniel Freedman, Ehud Rivlin, and Michael Elad.
2023. Principal Uncertainty Quantification with Spatial Correlation for Image
Restoration Problems. arXiv preprint arXiv:2305.10124 (2023).
[8]Anna D Broido and Aaron Clauset. 2019. Scale-free networks are rare. Nature
communications 10, 1 (2019), 1017.
[9]Jase Clarkson. 2023. Distribution free prediction sets for node classification. In
International Conference on Machine Learning. PMLR, 6268‚Äì6278.
[10] Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. 2009. Power-law
distributions in empirical data. SIAM review 51, 4 (2009), 661‚Äì703.
[11] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417‚Äì426.
[12] Boyuan Feng, Yuke Wang, and Yufei Ding. 2021. UAG: Uncertainty-aware atten-
tion graph neural network for defending adversarial attacks. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 35. 7404‚Äì7412.
[13] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In international conference on
machine learning. PMLR, 1050‚Äì1059.
[14] Patrizio Giovannotti. 2022. Calibration of Natural Language Understanding
Models with Venn‚ÄìABERS Predictors. In Conformal and Probabilistic Prediction
with Applications. PMLR, 55‚Äì71.
[15] Ethan Goan and Clinton Fookes. 2020. Bayesian neural networks: An introduction
and survey. Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet
Chair, Fall 2018 (2020), 45‚Äì87.
[16] Soroush H. Zargarbashi, Simone Antonelli, and Aleksandar Bojchevski. 2023.
Conformal Prediction Sets for Graph Neural Networks. In Proceedings of the 40th
International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 12292‚Äì12318. https:
//proceedings.mlr.press/v202/h-zargarbashi23a.html
[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[18] Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou,
Nick Duffield, Krishna Narayanan, and Xiaoning Qian. 2020. Bayesian graphneural networks with adaptive connection sampling. In International conference
on machine learning. PMLR, 4094‚Äì4104.
[19] Hans Hao-Hsun Hsu, Yuesong Shen, Christian Tomani, and Daniel Cremers.
2022. What Makes Graph Neural Networks Miscalibrated? Advances in Neural
Information Processing Systems 35 (2022), 13775‚Äì13786.
[20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118‚Äì22133.
[21] Kexin Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. 2023. Uncertainty
quantification over graph with conformalized graph neural networks. arXiv
preprint arXiv:2305.14535 (2023).
[22] Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe
Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. 2021. Could
graph neural networks learn better molecular representation for drug discovery?
A comparison study of descriptor-based and graph-based models. Journal of
cheminformatics 13, 1 (2021), 1‚Äì23.
[23] Ying Jin and Emmanuel J. Candes. 2023. Selection by Prediction with Conformal
p-values. Journal of Machine Learning Research 24, 244 (2023), 1‚Äì41. http:
//jmlr.org/papers/v24/22-1176.html
[24] Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesian
deep learning for computer vision? Advances in neural information processing
systems 30 (2017).
[25] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[26] Thomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. NIPS
Workshop on Bayesian Deep Learning (2016).
[27] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations (ICLR).
[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple
and scalable predictive uncertainty estimation using deep ensembles. Advances
in neural information processing systems 30 (2017).
[29] John Boaz Lee, Ryan Rossi, and Xiangnan Kong. 2018. Graph classification
using structural attention. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 1666‚Äì1674.
[30] Jing Lei, Max G‚ÄôSell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.
2018. Distribution-free predictive inference for regression. J. Amer. Statist. Assoc.
113, 523 (2018), 1094‚Äì1111.
[31] Lihua Lei and Emmanuel J Cand√®s. 2021. Conformal inference of counterfactuals
and individual treatment effects. Journal of the Royal Statistical Society Series B:
Statistical Methodology 83, 5 (2021), 911‚Äì938.
[32] Ting Wei Li, Qiaozhu Mei, and Jiaqi Ma. 2023. A Metadata-Driven Approach to
Understand Graph Neural Networks. arXiv preprint arXiv:2310.19263 (2023).
[33] Hongrui Liu, Binbin Hu, Xiao Wang, Chuan Shi, Zhiqiang Zhang, and Jun Zhou.
2022. Confidence may cheat: Self-training on graph neural networks under
distribution shift. In Proceedings of the ACM Web Conference 2022. 1248‚Äì1258.
[34] Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. 2021. Indigo:
Gnn-based inductive knowledge graph completion using pair-wise encoding.
Advances in Neural Information Processing Systems 34 (2021), 2034‚Äì2045.
[35] Ariane Marandon. 2023. Conformal link prediction to control the error rate.
arXiv preprint arXiv:2306.14693 (2023).
[36] Mark EJ Newman, Steven H Strogatz, and Duncan J Watts. 2001. Random graphs
with arbitrary degree distributions and their applications. Physical review E 64, 2
(2001), 026118.
[37] Liming Pan, Cheng Shi, and Ivan Dokmaniƒá. 2022. Neural Link Prediction with
Walk Pooling. In International Conference on Learning Representations. https:
//openreview.net/forum?id=CCu6RcUMwK0
[38] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah
Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al .2023. Robots that
ask for help: Uncertainty alignment for large language model planners. arXiv
preprint arXiv:2307.01928 (2023).
[39] Yaniv Romano, Evan Patterson, and Emmanuel Candes. 2019. Conformalized
quantile regression. Advances in neural information processing systems 32 (2019).
[40] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi
Tay, and Donald Metzler. 2022. Confident adaptive language modeling. Advances
in Neural Information Processing Systems 35 (2022), 17456‚Äì17472.
[41] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
G√ºnnemann. 2018. Pitfalls of Graph Neural Network Evaluation. Relational
Representation Learning Workshop, NeurIPS 2018 (2018).
[42] Amanda L Traud, Peter J Mucha, and Mason A Porter. 2012. Social structure of
facebook networks. Physica A: Statistical Mechanics and its Applications 391, 16
(2012), 4165‚Äì4180.
[43] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. 2005. Algorithmic
learning in a random world. Vol. 29. Springer.
[44] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. 2021. Be confident!
towards trustworthy graph neural networks via confidence calibration. Advances
4498KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Tianyi Zhao, Jian Kang, & Lu Cheng
in Neural Information Processing Systems 34 (2021), 23768‚Äì23779.
[45] Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. 2022. Node-
Former: A Scalable Graph Structure Learning Transformer for Node Classification.
InAdvances in Neural Information Processing Systems, Alice H. Oh, Alekh Agar-
wal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/
forum?id=sMezXGG5So
[46] Chen Xu and Yao Xie. 2021. Conformal prediction interval for dynamic time-
series. In International Conference on Machine Learning. PMLR, 11559‚Äì11569.
[47] Shuo Yang, Binbin Hu, Zhiqiang Zhang, Wang Sun, Yang Wang, Jun Zhou,
Hongyu Shan, Yuetian Cao, Borui Ye, Yanming Fang, et al .2021. Inductive
link prediction with interactive structure learning on attributed graph. In Ma-
chine Learning and Knowledge Discovery in Databases. Research Track: European
Conference, ECML PKDD 2021, Bilbao, Spain, September 13‚Äì17, 2021, Proceedings,
Part II 21. Springer, 383‚Äì398.
[48] Mustafa Yasir, John Palowitch, Anton Tsitsulin, Long Tran-Thanh, and Bryan
Perozzi. 2023. Examining the Effects of Degree Distribution and Homophily in
Graph Learning Models. arXiv preprint arXiv:2307.08881 (2023).
[49] Margaux Zaffran, Olivier F√©ron, Yannig Goude, Julie Josse, and Aymeric
Dieuleveut. 2022. Adaptive conformal predictions for time series. In International
Conference on Machine Learning. PMLR, 25834‚Äì25866.
[50] Soroush H Zargarbashi, Simone Antonelli, and Aleksandar Bojchevski. 2023.
Conformal Prediction Sets for Graph Neural Networks. (2023).
[51] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. 2020. Mix-n-match: Ensem-
ble and compositional methods for uncertainty calibration in deep learning. In
International conference on machine learning. PMLR, 11117‚Äì11128.
[52] Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. 2019.
Bayesian graph convolutional neural networks for semi-supervised classification.
InProceedings of the AAAI conference on artificial intelligence, Vol. 33. 5829‚Äì5836.
[53] Tianyi Zhao, Hui Hu, and Lu Cheng. 2023. Unveiling the Role of Message
Passing in Dual-Privacy Preservation on GNNs. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management. 3474‚Äì3483.
A REPRODUCIBILITY
Table 6: Experimental settings for link prediction models.
dataset ogbl-ddi
ogbl-ppa ogbl-citation2 german credit rochester38
training
epochs 200
300 50 500 500
learning rate 5e-3
1e-2 5e-4 1e-2 1e-2
batch size 64√ó1024
64√ó1024 512 2048 2048
hidden dimension 128
256 256 128 128
Table 7: Experimental settings for quantile regression.
dataset ogbl-ddi
ogbl-ppa ogbl-citation2 german credit rochester38
training
epochs 200
50 50 200 200
learning rate 5e-4
5e-4 5e-4 5e-4 5e-4
batch size 64
64 64 64 64
hidden dimension 64
64 64 64 64
The experimental settings for the training of link prediction
models and quantile regression are provided in Table 6 and Table 7,
respectively.
4499