Rethinking Fair Graph Neural Networks from Re-balancing
Zhixun Li
The Chinese University of Hong Kong
Hong Kong, China
zxli@se.cuhk.edu.hkYushun Dong
University of Virginia
Charlottesville, USA
yd6eb@virginia.edu
Qiang Liu
Institute of Automation, Chinese Academy of Sciences
Beijing, China
qiang.liu@nlpr.ia.ac.cnJeffrey Xu Yu‚àó
The Chinese University of Hong Kong
Hong Kong, China
yu@se.cuhk.edu.hk
ABSTRACT
Driven by the powerful representation ability of Graph Neural Net-
works (GNNs), plentiful GNN models have been widely deployed
in many real-world applications. Nevertheless, due to distribution
disparities between different demographic groups, fairness in high-
stake decision-making systems is receiving increasing attention.
Although lots of recent works devoted to improving the fairness
of GNNs and achieved considerable success, they all require signif-
icant architectural changes or additional loss functions requiring
more hyper-parameter tuning. Surprisingly, we find that simple
re-balancing methods can easily match or surpass existing fair GNN
methods. We claim that the imbalance across different demographic
groups is a significant source of unfairness, resulting in imbalanced
contributions from each group to the parameters updating. How-
ever, these simple re-balancing methods have their own shortcom-
ings during training. In this paper, we propose FairGB, FairGraph
Neural Network via re- Balancing, which mitigates the unfairness
of GNNs by group balancing. Technically, FairGB consists of two
modules: counterfactual node mixup and contribution alignment
loss. Firstly, we select counterfactual pairs across inter-domain
and inter-class, and interpolate the ego-networks to generate new
samples. Guided by analysis, we can reveal the debiasing mecha-
nism of our model by the causal view and prove that our strategy
can make sensitive attributes statistically independent from tar-
get labels. Secondly, we reweigh the contribution of each group
according to gradients. By combining these two modules, they can
mutually promote each other. Experimental results on benchmark
datasets show that our method can achieve state-of-the-art results
concerning both utility and fairness metrics. Code is available at
https://github.com/ZhixunLEE/FairGB.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning.
‚àóCorresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671826KEYWORDS
Graph Neural Networks; Fairness; Re-balancing
ACM Reference Format:
Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu. 2024. Rethinking Fair
Graph Neural Networks from Re-balancing. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24),
August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3637528.3671826
1 INTRODUCTION
With the rapid development of deep learning, Graph Neural Net-
works (GNNs) have been widely used in dealing with non-Euclidean
graph-structured data and gained a deeper understanding to help us
perform predictive tasks, such as social network analysis [ 33] and
molecular property prediction [ 6,55]. However, potential bias in
datasets will lead the neural networks to favor the privileged groups,
for example, if the historical data reflects a bias towards loans for in-
dividuals from higher-income communities, the model may learn to
associate high income with loan approval, leading to the unjustified
denial of loans for applicants from lower-income communities even
if they are creditworthy. Therefore, fairness in high-stake automatic
decision-making systems, e.g., medication recommendation [ 45],
fraud detection [ 29,38], and credit risk prediction [ 57] have been
receiving increasing attention in recent years.
Close to the heels of the wide application of GNNs, there are
various fair GNN approaches have been proposed to improve fair-
ness without unduly compromising utility performance [ 1,9,11‚Äì
14,34,54,58]. The two most typical strategies of fair GNNs are
imposing the fairness consideration as a regularization term dur-
ing optimization [ 9,18,48,54] or modifying the original training
data with the assumption that fair data would result in a fair model
[12,44]. However, previous works focus on eliminating information
about sensitive attributes but overlook the fact that due to attribute
imbalance, underprivileged groups with fewer training samples are
underrepresented compared to the privileged groups with more
training samples. Inspired by recent work in maximizing worst-
group-accuracy [ 20], we surprisingly observe that simple group-
wise re-balancing methods (e.g., re-sampling and re-weighting)
can easily achieve competitive or even superior results compared
to existing state-of-the-art fair GNN models with no additional
hyper-parameters (as shown in Figure 1). Data re-balancing meth-
ods are popular within the class imbalance literature, but we focus
on balancing each group in this paper, which we refer to as group-
wise re-balancing (without the loss of generality, we abbreviate
 
1736
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu
79 80 81 82
F10
5
10
15
20
25
30ŒîspGerman
7881848790
F11
2
3
4
5
6Bail
828384858687
F15.0
7.5
10.0
12.5
15.0CreditSAGE FairGNN EDITS NIFTY CAF RW OS FairGB
Figure 1: The F1- Œîùë†ùëùtrade-off on German, Bail, and Credit
datasets. "RW" denotes re-weighting and "OS" denotes over-
sampling. The light green region indicates the model outper-
forms the vanilla model in both utility and fairness, while
the light red region represents the opposite.
it as re-balancing). However, these simple re-balancing methods
have their own shortcomings on the imbalanced graph-structured
datasets. (1) Re-sampling: Down- and over-sampling will miti-
gate imbalance distribution by dropping or duplicating training
samples. However down-sampling will lose a lot of beneficial infor-
mation from the training set, jeopardizing the utility of the model.
While over-sampling simply repeats minority samples will cause an
over-fitting problem and hard to connect adjacent edges of newly
generated nodes because of non-iid characteristics of graphs. (2)
Re-weighting: re-weighting methods apply penalties according to
the quantity of groups and assign large weights to minor groups.
However, up-weighing nodes in minor groups may also result in the
over-fitting problem, and inevitably increase the false positive cases
for major nodes [ 47]. Several works attempt to alleviate the class
imbalance in the node classification scenario [ 28,31,39,47]. For in-
stance, GraphENS [ 39] injects the whole synthesized ego-networks
for minor class. TAM [ 47] designs a node-wise logit adjustment
method, which adaptively adjusts the margin accordingly based on
local topology. However they did not take into account sub-groups
within the classes, and to the best of our knowledge, there has been
no work addressing unfairness in GNNs from the perspective of
re-balancing.
To address the above problems, we propose a novel fair GNN
model, FairGraph Neural Networks via re- Balancing, FairGB for
short. Our proposed model can be divided into two modules: Coun-
terfactual Node Mixup (CNM) and Contribution Alignment Loss
(CAL). Specifically, we first select inter-domain (with the same tar-
get labels and different sensitive attributes) and inter-class (with
different target labels and the same sensitive attributes) nodes for
each training sample as counterexamples. Then we interpolate
node attributes and neighbor distributions of counterfactual pairs
and inject newly synthesized ego-networks to generate a balanced
augmented graph. We have performed theoretical analysis over
causal and statistical views, which serves as a solid mathematical
foundation for the effectiveness of debiasing. Secondly, because the
importance of each training sample varies, achieving balance solely
based on quantity is not sufficient. To further improve fairness,
we propose a re-weighting method, Contribution Alignment Loss,
which can balance the contribution of each group according to the
gradients. The weights can be flexibly combined with CNM, thus
we can view FairGB as a hybrid model, where CNM is equivalentto re-sampling, CAL is a re-weighting strategy. They mutually rein-
force each other, helping to alleviate the issues encountered by the
simple re-balancing methods mentioned above. Our contributions
can be listed as follows:
‚Ä¢Preliminary Analysis. We find that simple re-balancing meth-
ods can easily achieve competitive or superior results compared
to existing state-of-the-art fair GNN models. And we provide a
new perspective to analyze the fairness in graph learning.
‚Ä¢Algorithm Design. We propose a novel approach, namely FairGB,
that can effectively improve performance via re-balancing meth-
ods with only one additional hyper-parameter. And we theoreti-
cally prove that our approach can achieve the debiasing effect.
‚Ä¢Experimental Evaluation. We conduct extensive experiments,
and the results demonstrate that FairGB achieves superior per-
formance of utility and fairness. Meanwhile, we observe that the
decision boundaries of target labels and sensitive attributes are
roughly orthogonal, which indicates they are independent.
2 PRELIMINARY
2.1 Notations and Problem Statements
Given an attributed graph G=(V,A,X), whereV={ùë£1,ùë£2,...,ùë£ùëÅ}
is the set of nodes; A‚ààRùëÅ√óùëÅis the adjacency matrix, ùëÅis
the number of nodes, if ùë£ùëñandùë£ùëóare connected, Aùëñùëó=1, oth-
erwise Aùëñùëó=0;X=[x1,x2,...,xùëÅ] ‚ààRùëÅ√óùê∑is the node fea-
ture matrix, each node ùë£ùëñis associated with a ùê∑-dimensional node
feature vector xùëñ. The low-dimensional representations of nodes
Z=[z1,z2....,zùëÅ] ‚ààRùëÅ√óùëëare derived from graph encoder
ùëî(¬∑):RùëÅ√óùê∑√óRùëÅ√óùëÅ‚ÜíRùëÅ√óùëë. On the top of the graph encoder,
there is a classifier head ‚Ñé(¬∑):RùëÅ√óùëë‚ÜíRùëÅ√óùê∂to obtain the proba-
bility of each class, where ùê∂is the number of classes. Combining
graph encoder ùëî(¬∑)and classifier head ‚Ñé(¬∑), we can acquire graph
modelùëìùúÉ=ùëî‚ó¶‚Ñé(¬∑), whereùúÉis the learnable parameters.
In fairness learning or debiased learning, each labeled sample
is a triad,(ùë£,ùë¶,ùë†)‚àºV√óY√óS , withùë¶being the ground-truth
label andùë†being the sensitive attributes. If the nodes have the
same target label ùë¶and sensitive attribute ùë†, we define them as a
demographic group Dùë¶,ùë†, and the quantity of the group is |Dùë¶,ùë†|.
The goal of fairness learning is that the model will not be affected
by the sensitive features, resulting in the bias of predicted results ÀÜùë¶.
Assume there is a binary classification task, target label Y={1,0},
sensitive attributeS={1,0}. There are two corresponding metrics
to evaluate fairness.
2.1.1 Demographic Parity. If the predicted result ÀÜùë¶is independent
of sensitive attributes, i.e.,ÀÜùë¶‚ä•ùë†, then we can consider demographic
parity is achieved. The formula for this criterion is as follows:
ùëÉ(ÀÜùë¶=1|ùë†=0)=ùëÉ(ÀÜùë¶=1|ùë†=1) (1)
If a model satisfies demographic parity, the acceptance rate of dif-
ferent protected groups is the same.
2.1.2 Equalized Odds. If the predicted results and sensitive at-
tributes are independent conditional on the ground-truth label, i.e.,
ÀÜùë¶‚ä•ùë†|ùë¶, then we consider equalized odds is achieved. The formula
for it is as follows:
ùëÉ(ÀÜùë¶=1|ùë†=1,ùë¶=1)=ùëÉ(ÀÜùë¶=1|ùë†=0,ùë¶=1) (2)
 
1737Rethinking Fair Graph Neural Networks from Re-balancing KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
If a model satisfies equalized odds, the TPR (True Positive Rate) and
FPR (False Positive Rate) for the two protected groups are the same.
2.2 Graph Neural Networks
Most existing GNNs follow the message-passing paradigm which
contains message aggregation and feature update, such as GCN [ 26]
and GAT [ 53]. They generate node representations by iteratively
aggregating information of neighbors and updating them with non-
linear functions. The forward process can be defined as:
z(ùëô)
ùëñ=U
‚Ñé(ùëô‚àí1)
ùëñ,M({z(ùëô‚àí1)
ùëñ,z(ùëô‚àí1)
ùëó|ùë£ùëó‚ààNùëñ})
(3)
where z(ùëô)
ùëñis the feature vector of node ùëñin theùëô-th layer, andNùëñis
a set of neighbor nodes of node ùëñ.Mdenotes the message passing
function of aggregating neighbor information, Udenotes the update
function with central node feature and neighbor node features as
input. By stacking multiple layers, GNNs can aggregate messages
from higher-order neighbors.
3 METHODOLOGY
In this section, we will give a detailed description of FairGB. An
illustration of its framework is shown in Figure 2. In the Counter-
factual Node Mixup (CNM), we select counterfactual pairs for each
training sample and conduct inter-domain and inter-class mixup
with whole ego-networks. In the Contribution Alignment Loss
(CAL), we further improve the group-wise balance by re-weighting
each group according to the gradients. Next, we will provide a
theoretical analysis and introduce the details of each module.
3.1 Counterfactual Node Mixup
Recently, the incorporation of causal learning techniques into GNNs
has ignited a plethora of groundbreaking studies [ 15,18,21,49].
This is attributed to the fact that addressing trustworthiness con-
cerns is more effectively achieved by capturing the inherent causal-
ity in the underlying data, as opposed to merely relying on su-
perficial correlations. In this work, we present a causal view of
the union of the graph data generation and the GNNs‚Äô prediction
process as a Structure Causal Model [ 42] (as shown in Figure 3(a)).
We illustrate the causal relationships among six variables in the
node classification problem: unobservable causal variable ùê∂, un-
observable sensitive (bias) variable ùëÜ, observable node attributes
X, observable topology A, node embedding Z, and ground-truth
label Y. In the graph data generation process, ùê∂‚ÜíX‚ÜêùëÜand
ùê∂‚ÜíA‚ÜêùëÜdemonstrate that two variables (causal variable ùê∂
and sensitive variable ùëÜ) construct two components of observable
contextual subgraphs (node attributes Xand topology A), which is
different from i.i.d. data only consider attributes. X‚ÜíZ‚ÜêAand
Z‚ÜíYindicate existing GNNs produce representations and pre-
dictions based on observable contextual subgraphs. ùê∂eùëÜdenotes
the spurious correlation between ùê∂andùëÜ.
Inspired by Fan et al . [15] , we analyze the SCM according to
ùëë-connection theory [ 41] (two variables are dependent if they are
connected by at least one unblocked path ). Thus we can find three
paths between sensitive variable ùëÜand group-truth label Y:
‚Ä¢ùëÜ‚ÜíX‚ÜíZ‚ÜíYandùëÜ‚ÜíA‚ÜíZ‚ÜíY: Because the existing
graph neural networks make prediction for a node based onits contextual subgraph, sensitive variable ùëÜwill influence the
final prediction not only through node attributes Xbut also
topology A[30]. This makes fairness in graph machine learning
more complex compared to other modalities (e.g., images and
languages). As a result, if we want to sever all unblocked paths
betweenùëÜandY, we need to debias both XandA.
‚Ä¢ùëÜeùê∂‚ÜíY: we want to sever the connection between ùëÜand
ùê∂, so we utilize inter-domain and inter-class mixup. Intuitively,
we interpolate samples with the same target label but different
sensitive attributes in inter-domain mixup, and then the model
can focus on class-specific information and learn domain in-
variant features. Besides, we also interpolate samples with the
same sensitive attribute and different target labels in inter-class
mixup, which can smooth the decision surface and alleviate the
dependency on bias information.
Furthermore, we can also explain counterfactual mixup is equiv-
alent to re-sampling methods. As shown in Figure 3(b), we observe
that the original group distribution is imbalanced, which could lead
to unfairness during training. However, in the inter-domain mixup,
counterfactual samples can balance the bias distribution within
each class (i.e., ùëÉ(ùëå=ùë¶|ùëÜ=ùëñ)=ùëÉ(ùëå=ùë¶|ùëÜ=ùëó),‚àÄùëñ,ùëó‚ààùëÜ). In the
inter-class mixup, counterfactual samples can make the bias dis-
tribution consistent within each class (i.e., ùëÉ(ùëÜ=ùë†|ùëå=ùëñ)=ùëÉ(ùëÜ=
ùë†|ùëå=ùëó),‚àÄùëñ,ùëó‚ààùëå). The following theorem builds a theoretical
analysis of the debiasing capability of counterfactual mixup.
Theorem 1. LetùëåandùëÜbe target labels and sensitive attributes.
Balanced and consistent bias distribution within each class can make
ùëÜstatistically independent from ùëå, i.e.,ùëÉ(ùëå=ùë¶|ùëÜ=ùë†)=ùëÉ(ùëå=ùë¶).
Proof. ‚ù∂Inter-domain mixup: If ùëÉ(ùëå=ùë¶|ùëÜ=ùëñ)=ùëÉ(ùëå=
ùë¶|ùëÜ=ùëó),‚àÄùëñ,ùëó‚ààùëÜ, givenùë†‚ààùëÜthen:
ùëÉ(ùëå=ùë¶)=‚àëÔ∏Å
ùë†‚ààùëÜùëÉ(ùëå=ùë¶|ùëÜ=ùë†)ùëÉ(ùëÜ=ùë†), (4)
Based on the premise condition, we can rewrite Eq. 4 as:
ùëÉ(ùëå=ùë¶)=ùëÉ(ùëå=ùë¶|ùëÜ=ùë†)‚àëÔ∏Å
ùë†‚Ä≤‚ààùëÜùëÉ(ùëÜ=ùë†‚Ä≤)=ùëÉ(ùëå=ùë¶|ùëÜ=ùë†)(5)
‚ù∑Inter-class mixup: If ùëÉ(ùëÜ=ùë†|ùëå=ùëñ)=ùëÉ(ùëÜ=ùë†|ùëå=ùëó),‚àÄùëñ,ùëó‚ààùëå,
givenùë¶‚ààùëåthen:
ùëÉ(ùëÜ=ùë†)=‚àëÔ∏Å
ùë¶‚ààùëåùëÉ(ùëÜ=ùë†|ùëå=ùë¶)ùëÉ(ùëå=ùë¶) (6)
Based on the premise condition, we can rewrite Eq. 6 as:
ùëÉ(ùëÜ=ùë†)=ùëÉ(ùëÜ=ùë†|ùëå=ùë¶)‚àëÔ∏Å
ùë¶‚Ä≤‚ààùëåùëÉ(ùëå=ùë¶‚Ä≤)=ùëÉ(ùëÜ=ùë†|ùëå=ùë¶)(7)
Subsequently, using Bayesian probability we can get:
ùëÉ(ùëå=ùë¶|ùëÜ=ùë†)=ùëÉ(ùëÜ=ùë†|ùëå=ùë¶)ùëÉ(ùëå=ùë¶)
ùëÉ(ùëÜ=ùë†)(8)
=ùëÉ(ùëÜ=ùë†)ùëÉ(ùëå=ùë¶)
ùëÉ(ùëÜ=ùë†)(9)
=ùëÉ(ùëå=ùë¶) (10)
Thus, we have completed the proof of Theorem 1. ‚ñ°
 
1738KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu
Inter-domain Mixup
+
Inter-class Mixup
+
Neighborhood Mixup
+Counterfactual
PairsAugmented
GraphGraph Neural
Networks
Contribution
Alignment
Group Weight:Sample 
Contribution:
Contribution
Alignment Loss:: Target Labels
: Bias Labels
: Unlabeled Nodes
: Neighbor Dist.
: Gradients(a)
(b)
Figure 2: The overview of proposed FairGB.
C S
X A
Z
Y
(a) Causal View.
0 20 40 60Ori.IDICGerman Credit
ÓâÖ=0,Óàø=0
ÓâÖ=0,Óàø=1
ÓâÖ=1,Óàø=0
ÓâÖ=1,Óàø=1 (b) Group Distribution.
Figure 3: Analysis of the proposed FairGB. (a) SCM of the
union of the graph data generation and the existing GNNs‚Äô
prediction process. (b) Statistics of the occurrences of samples
in each group per epoch in the German dataset. "IC" denotes
inter-class mixup, "ID" denotes inter-domain mixup.
Although some literature provided similar theoretical analysis
based on the statistical view [ 43,51], they did not conduct a detailed
analysis of the debiasing mechanism of balanced and consistent
bias distribution within each class.
Next, we are going to introduce the detailed process of coun-
terfactual node mixup. Specifically, for each training node ùë£ùëñ, we
randomly select a node ùë£ùëóthat has the same target label and a differ-
ent sensitive attribute with ùë£ùëñ(ùë¶ùëñ=ùë¶ùëó,ùë†ùëñ‚â†ùë†ùëó) as an inter-domain
counterexample. In the same way, we randomly select a node ùë£ùëó
that has a different target label and the same sensitive attribute
withùë£ùëñ(ùë¶ùëñ‚â†ùë¶ùëó,ùë†ùëñ=ùë†ùëó) as an inter-class counterexample. Here, we
introduce a hyper-parameter ùúÇ, which is responsible for controlling
the ratio between two kinds of counterexamples. Then we performlinear interpolation on the node attributes:
xùëöùëñùë•=ùúÜxùëñ+(1‚àíùúÜ)xùëó, ùë¶ùëöùëñùë•=ùúÜùë¶ùëñ+(1‚àíùúÜ)ùë¶ùëó, (11)
whereùúÜ‚àà[0,1]is the interpolation ratio that is sampled from a
Beta distribution. After generating new node attributes, we need to
insert these nodes into the original graph. According to the analysis
above, we also perform mixup on the contextual subgraph structure.
We define the neighbor distribution of node ùë£ùëñasùëùN(ùë£ùëñ)where
ùëù(ùë£ùëò|ùë£ùëñ)=1
|Nùëñ|, ifAùëñùëò=1, andùëù(ùë£ùëò|ùë£ùëñ)=0otherwise. Then
we directly interpolate the neighbor distribution of counterfactual
pairs using the same ùúÜ:
ùëùN(ùë£ùëöùëñùë•)=ùúÜùëùN(ùë£ùëñ)+(1‚àíùúÜ)ùëùN(ùë£ùëó), (12)
Thus we can obtain newly generated unbiased ego-networks. How-
ever, these ego-networks have very dense structures, which could
violate the original degree distribution and result in the phenome-
non of out-of-distribution [ 32,50]. Therefore, we sample the neigh-
bors according to the original degree distribution and inject newly
generated unbiased ego-networks into the original graph to con-
struct an augmented graph Gùëéùë¢ùëî.
3.2 Contribution Alignment Loss
Although counterfactual node mixup is equivalent to re-sampling
to some extent, varying levels of learning difficulty and the different
positions of labeled nodes [ 5] lead to inconsistent contributions of
training nodes. Simply balancing the group distribution in terms
of quantity does not effectively address the problem. To further
balance the contribution of each group and enhance fairness, we
align the gradients of each group.
Since FairGB first generates mixed nodes through counterfactual
node mixup and calculates the loss on these mixed nodes, it is
difficult for us to determine which group the mixed nodes actually
belong to. However, we can acquire the contributions of mixed
nodes to each group by rewriting the loss function. First, we claim
 
1739Rethinking Fair Graph Neural Networks from Re-balancing KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
that the objective loss function of counterfactual mixup is as follows:
L=E{ùë£ùëñ,ùë£ùëó}[‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëöùëñùë•)], (13)
=E{ùë£ùëñ,ùë£ùëó}[ùúÜ‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëñ)+(1‚àíùúÜ)‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëó)], (14)
where‚Ñìis Cross-Entropy loss. Then we can obtain two contribu-
tions of each mixed node according to the gradients:
ùëüùëñ=‚à•‚àáùëì‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëñ)‚à•1, ùëüùëó=‚à•‚àáùëì‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëó)‚à•1, (15)
In essence,ùëüùëñandùëüùëóare the contributions generated from mixed
two nodesùë£ùëñandùë£ùëó. We can easily identify which group are ùë£ùëñ
andùë£ùëóbelong to. Therefore, the contribution of each group ùëÖùë°,ùëèis
the sum of sample contributions (both original samples and coun-
terexamples), ùëÖùë°,ùëè=√ç|Dùë°,ùëè|
ùë£ùëñ‚ààDùë°,ùëèùëüùëñ. Based onùëÖùë°,ùëè, we can compute
the weight of each group ùë§ùë°,ùëè=√çùê∂√óùêµ
ùëó,ùëòùëÖùëó,ùëò/ùëÖùë°,ùëèto balance the
contributions, where ùêµis the number of sensitive attributes. After
obtaining the weights of each group, we can flexibly inject them
into the objective loss function Eq. 14 to get the final Contribution
Alignment Loss (CAL):
Lùê∂ùê¥ùêø=E{ùë£ùëñ,ùë£ùëó}[ùë§ùë¶ùëñ,ùë†ùëñùúÜ‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëñ)+ùë§ùë¶ùëó,ùë†ùëó(1‚àíùúÜ)‚Ñì(ùëì(ùë£ùëöùëñùë•),ùë¶ùëó)],
(16)
So far, we have achieved an ingenious combination of counterfac-
tual node mixup and contribution alignment. They can complement
and promote each other. Specifically, counterfactual node mixup
can sever the spurious correlation from the causal view but it does
not guarantee a good balance between different groups. While
contribution alignment loss is able to balance the contribution of
each group based on gradients, but it will suffer the over-fitting
problem, which can be mitigated by the newly generated unbiased
ego-networks. The whole training procedure of FairGB is presented
in Algorithm 1.
4 EXPERIMENTS
In this section, we conduct extensive experiments to investigate
the effectiveness of our proposed model, and aim to answer the
following research questions:
‚Ä¢RQ1: Compared to other baselines, can FairGB achieve better
performance w.r.t. utility and fairness?
‚Ä¢RQ2: How does each component affect the model performance?
‚Ä¢RQ3: What are the characteristics of the node features generated
by FairGB compared to vanilla?
‚Ä¢RQ4: Can FairGB generalize well to different graph encoders?
‚Ä¢RQ5: How do hyper-parameters affect FairGB?
4.1 Experimental settings
4.1.1 Real-World Datasets. We conduct experiments on three widely
used real-world datasets, namely German Credit, Bail, and Credit
Defaulter. The statistics of the datasets can be found in Table 1. The
details of the datasets are as follows:
‚Ä¢German Credit [2]: the nodes in the dataset are clients and two
nodes are connected if they have a high similarity of the credit
accounts. The task is to classify the credit risk level as high or
low with the sensitive attribute "gender".
‚Ä¢Bail [23]: these datasets contain defendants released on bail
during 1990-2009 as nodes. The edges between the two nodes are
connected based on the similarity of past criminal records andAlgorithm 1: FairGB: Fair Graph Neural Networks via re-
Balancing;
Input: An attributed graph: G=(V,A,X); hyper-parameters:
ùëá,ùëáùë§ùëéùëüùëö,ùúÇ; graph neural network: ùëìùúÉ.
Output: The Learned node representations Zand predictions ÀÜY.
1forùë°=1,...,ùëá do
2 ifùë°‚â§ùëáùë§ùëéùëüùëö then
3L‚ÜêCross Entropy Loss;
4 Back-propagation to update parameters;
5 else
6 forùëñ=1,...,|Vùë°ùëüùëéùëñùëõ|do
7 Sampleùúá‚àºUniform(0,1);
8 ifùúá‚â•ùúÇthen
9 Randomly sample ùë£ùëówhich satisfies ùë¶ùëñ=ùë¶ùëóand
ùë†ùëñ‚â†ùë†ùëó;// Inter-domain
10 else
11 Randomly sample ùë£ùëówhich satisfies ùë¶ùëñ‚â†ùë¶ùëóand
ùë†ùëñ=ùë†ùëó;// Inter-class
12 Generate mixed ego-network according to Eq. 11 and
Eq. 12;
13 Compute gradients by Eq. 15;
14 Compute group weights ùë§ùë°,ùë†;
15Lùê∂ùê¥ùêø‚ÜêContribution Alignment Loss (Eq. 16);
16 Back-propagation to update parameters;
Table 1: The Statistic of Datasets.
Dataset
German Credit Bail Credit Defaulter
#
Nodes 1,000 18,876 30,000
# Edges 22,242 321,308 152,377
# Attributes 27 18 13
Sens. Gender Race Age
Label Credit status Bail decision Future default
demographics. The task is to classify whether defendants are on
bail or not with the sensitive attribute "race".
‚Ä¢Credit Defaulter [57]: the nodes in the dataset are credit card
users and the edges are formed based on the similarity of the
payment information. The task is to classify the default payment
method with the sensitive attribute "age".
4.1.2 Baselines. We compare our proposed model with 9 repre-
sentative and state-of-the-art methods in four categories, which
include: (1) Vanilla graph neural networks: GCN [ 26] is widely
used spectral GNN; GraphSAGE (SAGE for short) [ 19] is a method
for inductive learning that leverages node feature information to
generate embedding for nodes in large graph; GIN [ 56] is a graph-
based neural network that can capture different topological struc-
tures by injecting the node‚Äôs identity into its aggregation func-
tion. (2) Fair node classification methods: FairGNN [ 9] uses
adversarial training to achieve fairness on graphs; EDITS [ 12] is a
pre-processing method for fair graph learning. (3) Graph counter-
factual fairness methods: NIFTY [ 1] simply performs a flipping
on the sensitive attributes to get counterfactual data; CAF [ 18] is
 
1740KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu
Table 2: Model performance on German, Credit, and Bail with respect to utility and fairness. Dark brown is used to highlight
the best results for each metric. Light brown for the runner-up results. ‚Üërepresents the larger, the better while ‚Üìrepresents the
smaller, the better. Each experimental result is obtained from 10 repeated experiments.
Datasets Metrics GCN
GIN SAGE Fair
GNN EDITS NIFT
Y CAF RW
OS Fair
GB
GermanAUC
(‚Üë)73.49¬±2.15 72.42¬±1.46 73.78¬±1.80 65.85¬±9.49 69.76¬±5.46 72.05¬±2.15 71.87¬±1.33 71.86¬±2.18 71.73¬±2.92 74.45¬±4.93
F1
(‚Üë) 78.96¬±3.35 81.76¬±1.27 81.01¬±1.18 82.29¬±0.32 81.04¬±1.09 79.20¬±1.19 82.16¬±0.22 81.52¬±0.77 82.01¬±0.65 82.58¬±0.35
A
CC (‚Üë)70.84¬±2.56 72.36¬±0.77 71.76¬±1.15 70.64¬±0.74 71.68¬±1.25 69.60¬±1.50 70.64¬±0.34 70.44¬±0.87 70.00¬±1.72 70.76¬±0.79
Œîùë†
ùëù(‚Üì)34.95¬±15.33 16.65¬±8.41 28.17¬±4.99 7.65¬±8.07 8.42¬±7.35 7.74¬±7.80 6.60¬±1.66 4.80¬±4.50 4.02¬±3.87 2.19¬±1.89
Œîùëí
ùëú(‚Üì)28.43¬±11.66 11.79¬±7.29 19.40¬±6.11 4.18¬±4.86 5.69¬±2.16 5.17¬±2.38 1.58¬±1.14 2.24¬±2.64 2.66¬±3.30 1.20¬±1.37
BailAUC
(‚Üë)87.09¬±0.18 85.68¬±0.25 92.07¬±0.83 91.53¬±0.38 89.07¬±2.26 92.04¬±0.89 91.39¬±0.34 91.54¬±0.47 93.37¬±0.54 96.21¬±1.26
F1
(‚Üë) 77.93¬±0.39 76.92¬±0.31 83.64¬±0.95 82.55¬±0.98 77.83¬±3.79 77.81¬±6.03 83.09¬±0.98 82.73¬±0.96 85.60¬±1.20 90.19¬±1.76
A
CC (‚Üë)83.58¬±0.31 82.34¬±0.29 88.61¬±0.68 87.68¬±0.73 84.42¬±2.87 84.11¬±5.49 88.02¬±0.86 87.20¬±1.04 89.36¬±1.13 92.64¬±1.21
Œîùë†
ùëù(‚Üì) 7.59¬±0.31 8.78¬±0.40 3.84¬±1.15 1.94¬±0.82 3.74¬±3.54 5.74¬±0.38 2.29¬±1.06 0.68¬±0.59 1.70¬±1.76 0.77¬±0.52
Œîùëí
ùëú(‚Üì) 5.26¬±0.30 7.47¬±0.39 2.64¬±1.31 1.72¬±0.70 4.46¬±3.50 4.07¬±1.28 1.17¬±0.52 0.98¬±0.74 1.53¬±1.01 1.53¬±0.65
Cr
editAUC
(‚Üë)73.90¬±0.03 73.20¬±0.02 74.55¬±0.60 70.82¬±0.74 75.04¬±0.12 72.89¬±0.44 73.42¬±1.89 73.38¬±0.56 74.76¬±0.17 73.07¬±1.79
F1
(‚Üë) 81.94¬±0.01 83.18¬±0.10 83.95¬±1.19 83.97¬±2.00 82.41¬±0.52 82.60¬±1.25 83.63¬±0.89 83.43¬±1.19 86.91¬±0.70 87.47¬±0.52
A
CC (‚Üë)73.69¬±0.01 75.01¬±0.12 75.82¬±1.04 75.29¬±1.62 74.13¬±0.59 74.39¬±1.35 75.36¬±0.95 75.22¬±1.36 79.14¬±0.77 79.38¬±0.52
Œîùë†
ùëù(‚Üì) 12.73¬±0.15 5.41¬±0.43 15.91¬±2.83 6.17¬±5.57 11.34¬±6.36 10.65¬±1.65 8.63¬±2.13 4.63¬±1.41 3.85¬±2.15 1.61¬±1.41
Œîùëí
ùëú(‚Üì) 10.53¬±0.13 3.45¬±0.41 13.45¬±3.17 5.06¬±4.46 9.38¬±5.39 8.10¬±1.91 6.85¬±1.55 2.84¬±0.99 2.23¬±1.13 0.92¬±0.83
A
vg. (Rank) 8.13
7.00 5.47 5.47
7.00 7.40
4.67 4.60 3.27 2.00
guided by causal analysis, which can select counterfactual from
training data to avoid non-realistic counterfactuals and adopt se-
lected counterfactuals to learn fair node representations. (4) Simple
re-balancing methods: Re-weighting (RW for short) up-weights
the contribution of minority groups and down-weights the contri-
bution of majority groups to the loss functions according to the
quantity of groups. More concretely, the objective loss function
isL=1
ùëÅ√çùëÅ
ùëñ=1ùëÅ
|Dùë¶ùëñ,ùë†ùëñ|‚Ñì(ùëì(ùë£ùëñ),ùë¶ùëñ). Over-sampling (OS for short)
repeatedly samples minority group samples until the number of
each group data reaches the maximum number of group data. We
duplicate the edges of the original node when adding an oversam-
pled node to the original graph. Following the setting of CAF [ 18],
we also use SAGE as the model backbone except for GCN and GIN.
4.1.3 Evaluation Metrics. We regard AUC, F1 score, and accuracy
as utility metrics. For fairness metrics, we use statistical parity
(SP)Œîùë†ùëùand equal opportunity (EO) Œîùëíùëú, a smaller fairness metric
indicates a fairer model decision.
4.1.4 Implementation details. For German, Bail, and Credit datasets,
we follow train/valid/test split in [ 1]. The hyper-parameters used
in experiments follow the source codes or are searched by the grid
search method, and we use the Adam optimization algorithm [ 25]
to train all the models. Specifically, FairGB only has one additional
hyper-parameter ùúÇ, and it is searched from {0, 0.1, 0.2, ..., 0.8, 0.9,
1}. All the models are implemented in PyTorch [ 40] version 2.0.1
with PyTorch Geometric [17] version 2.3.1.
4.2 RQ1: Performance comparison
To comprehensively understand the effectiveness of FairGB, we
conduct node classification on three widely used datasets. The ex-
perimental results of utility and fairness of each model are shown in
Table 2. From the Table 2, our observations can be threefold: (1) We
can observe that simple re-balancing methods achieve satisfactory
results in both utility and fairness on three datasets. They can obtain
competitive or even superior performance compared to carefullydesigned fair GNN models. (2) FairGB consistently achieves the best
performance on the utility-fairness trade-off on all datasets. We use
the average rank of three utility metrics and two fairness metrics to
understand the performance of the trade-off. Our model ranks 2.47
and the runner-up model ranks 3.27. Our model outperforms all
four categories of baselines, which shows the effectiveness of our
model. (3) We find that OS and FairGB, on bail and credit datasets,
do not sacrifice utility while improving fairness. For instance, on
the bail dataset, FairGB improves the AUC by 4.50%, the F1 by
7.83%, and Acc by 4.54% compared to the vanilla model. And on
the credit dataset, FairGB improves F1 by 4.19%, and Acc by 4.70%
compared to the vanilla model. We speculate that this may be due
to two reasons: (i) Balance of contributions from each group leads
to a significant improvement in the accuracy of the worst group;
(ii) the re-balancing strategy enhances the model‚Äôs generalization,
resulting in an overall improvement in classification performance.
4.3 RQ2: Ablation study
To answer RQ2 and verify the effectiveness of our proposed FairGB,
we construct two variants of FairGB: (1) without Contribution
Alignment Loss but conduct node mixup within counterfactual
pairs (called FairGB w/o CAL); (2) without Counterfactual Node
Mixup but assigns weights based on the group contribution (called
FairGB w/o CNM). Table 3 demonstrates the performance of the
vanilla model, two variants, and FairGB. First, we observe that two
variants perform worse than FairGB in the trade-off of utility and
fairness, which proves the effectiveness of each component and the
rationality of the combination. Second, we find that FairGB w/o
CNM consistently achieves better fairness compared to FairGB w/o
CAL, which indicates the importance of re-balancing in fair graph
learning. However, as mentioned above, re-weighting methods will
encounter over-fitting problems, and counterfactual node mixup
will generate new samples per epoch, which could mitigate over-
fitting. Third, while both modules effectively enhance fairness, the
impact on utility varies across different datasets for each variant. For
 
1741Rethinking Fair Graph Neural Networks from Re-balancing KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
‚àí40 ‚àí20 0 20 40 60‚àí40‚àí2002040(a) Vanilla w.r.t. ÓâÖ
‚àí40 ‚àí20 0 20 40 60‚àí40‚àí2002040(b) Vanilla w.r.t. Óàø
‚àí50 ‚àí25 0 25 50‚àí40‚àí2002040(c) FairGB w.r.t. ÓâÖ
‚àí50 ‚àí25 0 25 50‚àí40‚àí2002040(d) FairGB w.r.t. Óàø
Figure 4: Visualizations of node representation learned on the Bail dataset.
Table 3: Ablation study results.
Datasets Metrics V
anillaFairGB FairGBFairGBw/o
CAL w/o CNM
GermanAUC
(‚Üë)73.72+1.52% 75.09-1.11% 69.29-2.91% 74.45
F1
(‚Üë) 81.01-1.11% 82.46-0.04% 81.38-0.74% 82.58
A
CC (‚Üë)71.76+1.60% 71.80+1.58% 70.36+0.20% 70.76
Œîùë†
ùëù(‚Üì)28.17+26.73% 7.14+7.88% 4.10+2.66% 2.19
Œîùëí
ùëú(‚Üì)19.40+18.14% 2.77+2.56% 2.77+1.51% 1.20
BailAUC
(‚Üë)92.07-4.14% 97.11+0.90% 91.13-5.08% 96.21
F1
(‚Üë) 83.64-6.55% 91.93+1.74% 82.36-7.83% 90.19
A
CC (‚Üë)88.61-4.03% 93.99+1.35% 87.19-5.45% 92.64
Œîùë†
ùëù(‚Üì) 3.84+3.07% 1.36+0.59% 1.33+0.56% 0.77
Œîùëí
ùëú(‚Üì) 2.64+1.11% 1.82+0.29% 1.06-0.47% 1.53
Cr
editAUC
(‚Üë)74.55+1.48% 73.61+0.54% 73.77+0.70% 73.07
F1
(‚Üë) 83.95-3.52% 82.86-4.61% 86.18-1.29% 87.47
A
CC (‚Üë)75.82-3.56% 74.48-4.90% 78.19-1.19% 79.38
Œîùë†
ùëù(‚Üì)15.91+12.59% 6.84+3.52% 5.76+2.44% 3.32
Œîùëí
ùëú(‚Üì)13.45+11.96% 4.15+2.66% 3.86+2.37% 1.49
example, FairGB w/o CAL achieves high utility on the bail dataset
but does not show improvement on the credit dataset, whereas
FairGB w/o CNM exhibited the opposite trend. We speculate that
this may be due to the different sizes of labeled nodes in the two
datasets. The bail dataset has a small number of training samples
(only 100 nodes are labeled), leading to over-fitting issues with re-
weighting. Mixup, on the other hand, is beneficial for training with
few labels. In the credit dataset, there are a lot of training samples
(4000 nodes are labeled), and the phenomenon is just the opposite.
However, we can observe that FairGB competently incorporates
the strengths of each module, demonstrating that the two modules
can mutually reinforce each other.
4.4 RQ3: Visualization
In order to answer the RQ3, we visualize the learned node embed-
dings on the bail dataset to better understand the mechanism of
FairGB. We compare our FairGB with the vanilla GNN model (i.e.
SAGE), and then use 16-dimensional output embedding of the en-
coder. Subsequently, we use t-SNE [ 52] to map the 16-dimensional
embedding into 2-dimensional space for visualization. We only
select samples in the test set for better visibility. The results are
shown in Figure 4. We plot two figures for each model, one con-
cerns the target labels, and the other one concerns the sensitive
attributes. Comparing Figure 4(a) and Figure 4(c), we can observe
that the representations of nodes from different classes have smalleroverlapping regions in FairGB. This confirms the improvement of
FairGB on the three utility metrics. Next, since SAGE already ex-
hibits good fairness on the bail dataset (as shown in Table 2), we can
observe that the node representations of the two sensitive attributes
are mixed together in Figure 4(b). Because FairGB is designed from
a re-balancing perspective, the contributions from each group are
relatively balanced, making the node representations of the two
sensitive attributes distinguishable in FairGB. However, we can
observe that the classification boundary for the target label is or-
thogonal to the classification boundary for the sensitive features.
This explains how FairGB achieves excellent fairness.
4.5 RQ4: Generalization for different encoders
To answer the RQ4, we test the generalization ability of our FairGB
by deploying it on three different graph encoders: SAGE, GCN,
and GIN. The utility and fairness performance are demonstrated in
Figure 5. We can observe that FairGB is able to maintain or even
surpass the vanilla models in both AUC and F1-score metrics, which
is thanks to the re-balancing strategy. In the fairness metrics, we
find that FairGB effectively reduces Œîùë†ùëùandŒîùëíùëúcompared to the
vanilla models across all datasets. These results indicate that FairGB
has strong generalization capabilities for different graph encoders,
making it flexible for use in real-world applications.
4.6 RQ5: Parameter sensitive analysis
One major advantage of FairGB is that it only has one additional
hyper-parameter ùúÇto control the trade-off between inter-domain
mixup and inter-class mixup. To figure out the RQ5, we conduct
hyper-parameter sensitive analysis on three datasets in terms of
three utility metrics and two fairness metrics. As shown in Figure 6,
we varyùúÇfrom 0 to 1 and present the performance of FairGB.
WhenùúÇequals 0, the model only performs inter-domain mixup, and
whenùúÇequals 1, the model only performs inter-class mixup. Recall
our above analysis, when the training set is class-balanced, inter-
domain mixup is equivalent to group-wise balance re-sampling.
But according to our causal view, we demonstrate that plain inter-
domain mixup can not effectively mitigate bias, so we conduct
inter-class mixup and add a hyper-parameter ùúÇto control the trade-
off. We can observe that when ùúÇequals 0 or 1, FairGB can not
achieve the best performance. However, when ùúÇis within the range
of 0.3 to 0.8, the model is not sensitive to parameters and FairGB
achieves the roughly best performance when ùúÇequals 0.5. Therefore,
we simply fix the ùúÇto 0.5 in most experiments.
 
1742KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu
AUC F1020406080German
AUC F10255075100Bail
AUC F1020406080Credit
ŒîspŒîeo0102030
ŒîspŒîeo02468
ŒîspŒîeo051015SAGE
FairGB-SAGEGCN
FairGB-GCNGIN
FairGB-GIN
Figure 5: Comparison of the utility and fairness performance
with different graph encoders.
00.2 0.4 0.6 0.8 1
Œ∑6972757881Utility
German
00.2 0.4 0.6 0.8 1
Œ∑868890929496
Bail
00.2 0.4 0.6 0.8 1
Œ∑727578818487
Credit
012345
1234
234567
Fairness
AUC F1 ACC ŒîspŒîeo
Figure 6: Parameter sensitivity results w.r.t.ùúÇ.
5 RELATED WORK
5.1 Fairness in Graph Neural Networks
Fairness is a widely researched issue in the field of machine learning
[3,4,7,35]. Most works in deep learning exclusively focus on opti-
mizing the model utility while ignoring the fairness of the decisions,
such as group fairness [ 4,10], individual fairness [ 37,46], and coun-
terfactual fairness [ 8,27]. Since GNNs inherit characteristics from
machine learning, they can also encounter fairness issues, which
makes it challenging to deploy GNNs in high-risk applications.
Nowadays, there are various existing works that try to improve
the fairness of GNNs, and they can be roughly categorized into
pre-, in-, and post-processing methods. Pre-processing methods
modify the original training data with the assumption that fair
data would result in fair models. Fairwalk [ 44] and Crosswalk
[24] choose each group of neighbor nodes with an equal chance
and bias random walks to cross group boundaries. EDITS [ 12]
designs a model-agnostic method to modify the attribute and struc-
ture for fair GNNs training. In-processing approaches aim to mit-
igate unfairness during the training process by directly modify-
ing the learning algorithm, and they can be divided into three
parts: adversarial-based, augmentation-based, and message-passing-
based. Adversarial-based approaches train fair GNNs by preventing
an adversary from correctly predicting sensitive attributes from
the learned node representations. FairGNN [ 9] and FairVGNN [ 54]
enforce the model to generate fair outputs with adversarial training
through the min-max objective. Augmentation-based approaches
generate counterfactual views and minimize the discrepancy with
the original view. NIFTY [ 1] flips sensitive attributes for each
node to obtain the counterfactual view. Message-passing-based
approaches improve fairness through the view of optimization
problem with smoothness regularization. FMP [ 22] proposes a fairmessage-passing framework by considering graph smoothness and
fairness objectives. The post-processing techniques directly cali-
brate the classifier‚Äôs decisions at inference time. POSTPROCESS [36]
updates model predictions based on a black-box policy to minimize
differences between demographic groups.
5.2 Re-balancing in Graph Neural Networks
Due to the GNNs inheriting the character of deep neural networks,
GNNs perform with biases toward the majority classes when train-
ing on imbalanced datasets. To overcome this challenge, class-
imbalanced learning on graphs has emerged as a promising solution
that combines the strengths of graph representation learning and
class-imbalanced learning. A great branch of these methods is over-
sampling minority nodes by data augmentation to balance the skew
label distribution. GraphSMOTE [ 59] leverages representative data
augmentation method (i.e., SMOTE [ 16]) and proposes edge pre-
dictor to fuse augmented nodes into the original graph. GraphENS
[39] discovers neighbor memorization phenomenon in imbalanced
node classification, and generates minority nodes by synthesizing
ego-networks according to similarity. GraphSHA [ 28] only synthe-
sizes harder training samples and proposes SEMIMIXUP to block
message propagation from minority nodes to neighbor classes by
generating connected edges from 1-hop subgraphs. Another branch
of class-imbalanced learning on graphs is topology-aware logit
adjustment. TAM [ 47] adjusts margins node-wisely according to
the extent of deviation from connectivity patterns to avoid induc-
ing false positives of minority nodes. Different from the methods
mentioned above that focus on class imbalance, our paper attempts
to address the issue of group imbalance. We emphasize that group
imbalance is a significant source of unfairness in GNNs and, as a
result, design group re-balancing strategies to enhance the fairness
of graph learning.
6 CONCLUSION
In this paper, we provide a new perspective to address the unfair-
ness in graph neural networks. We find that group distribution
imbalance is a significant source of bias and simple re-balancing
methods (e.g., re-weighting and re-sampling) can easily achieve
competitive or even superior performance compared to existing
state-of-the-art fair GNNs. To this end, we propose FairGB which
consists of two modules to automatically balance the contributions
of each group. Guided by theoretical analysis, we conduct linear
interpolation between counterfactual node pairs to effectively mit-
igate bias. In order to further enhance fairness, we propose con-
tribution alignment loss based on gradients and flexibly combine
two modules. Experimental results demonstrate the effectiveness of
our proposed FairGB in achieving state-of-the-art performance on
three real-world datasets. Future research directions can delve more
into understanding unfairness from a re-balancing perspective and
better integration of the graph properties to mitigate bias.
ACKNOWLEDGE
This work was partially supported by the Research Grants Council
of Hong Kong, No. 14202919 and No. 14205520.
 
1743Rethinking Fair Graph Neural Networks from Re-balancing KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
REFERENCES
[1]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uni-
fied framework for fair and stable graph representation learning. In Uncertainty
in Artificial Intelligence. PMLR, 2114‚Äì2124.
[2] Arthur Asuncion and David Newman. 2007. UCI machine learning repository.
[3]Richard Berk. 2019. Accuracy and fairness for juvenile justice risk assessments.
Journal of Empirical Legal Studies 16, 1 (2019), 175‚Äì194.
[4]Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. 2017. Data decisions and
theoretical implications when adversarially learning fair representations. arXiv
preprint arXiv:1707.00075 (2017).
[5]Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou, and
Xu Sun. 2021. Topology-imbalance learning for semi-supervised node classifica-
tion. Advances in Neural Information Processing Systems 34 (2021), 29885‚Äì29897.
[6]Dingshuo Chen, Yanqiao Zhu, Jieyu Zhang, Yuanqi Du, Zhixun Li, Qiang Liu,
Shu Wu, and Liang Wang. 2024. Uncovering neural scaling laws in molecular
representation learning. Advances in Neural Information Processing Systems 36
(2024).
[7] Irene Chen, Fredrik D Johansson, and David Sontag. 2018. Why is my classifier
discriminatory? Advances in neural information processing systems 31 (2018).
[8]Silvia Chiappa. 2019. Path-specific counterfactual fairness. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 33. 7801‚Äì7808.
[9] Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fair
graph neural networks with limited sensitive attribute information. In Proceedings
of the 14th ACM International Conference on Web Search and Data Mining . 680‚Äì
688.
[10] Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron
Roth. 2021. Minimax group fairness: Algorithms and experiments. In Proceedings
of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 66‚Äì76.
[11] Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021. Individual
fairness for graph neural networks: A ranking based approach. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
300‚Äì310.
[12] Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modeling
and mitigating data bias for graph neural networks. In Proceedings of the ACM
Web Conference 2022. 1259‚Äì1269.
[13] Yushun Dong, Jing Ma, Song Wang, Chen Chen, and Jundong Li. 2023. Fairness
in graph mining: A survey. IEEE Transactions on Knowledge and Data Engineering
(2023).
[14] Yushun Dong, Binchi Zhang, Yiling Yuan, Na Zou, Qi Wang, and Jundong Li. 2023.
Reliant: Fair knowledge distillation for graph neural networks. In Proceedings of
the 2023 SIAM International Conference on Data Mining (SDM). SIAM, 154‚Äì162.
[15] Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. 2022. Debiasing
graph neural networks via learning disentangled causal substructure. Advances
in Neural Information Processing Systems 35 (2022), 24934‚Äì24946.
[16] Alberto Fern√°ndez, Salvador Garcia, Francisco Herrera, and Nitesh V Chawla.
2018. SMOTE for learning from imbalanced data: progress and challenges, mark-
ing the 15-year anniversary. Journal of artificial intelligence research 61 (2018),
863‚Äì905.
[17] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with
PyTorch Geometric. arXiv preprint arXiv:1903.02428 (2019).
[18] Zhimeng Guo, Jialiang Li, Teng Xiao, Yao Ma, and Suhang Wang. 2023. Towards
fair graph neural networks via graph counterfactual. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management. 669‚Äì
678.
[19] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[20] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-
Paz. 2022. Simple data balancing achieves competitive worst-group-accuracy. In
Conference on Causal Learning and Reasoning. PMLR, 336‚Äì351.
[21] Wenzhao Jiang, Hao Liu, and Hui Xiong. 2023. Survey on Trustworthy Graph
Neural Networks: From A Causal Perspective. arXiv preprint arXiv:2312.12477
(2023).
[22] Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and
Xia Hu. 2022. Fmp: Toward fair graph message passing against topology bias.
arXiv preprint arXiv:2202.04187 (2022).
[23] Kareem L Jordan and Tina L Freiburger. 2015. The effect of race/ethnicity on
sentencing: Examining sentence type, jail length, and prison length. Journal of
Ethnicity in Criminal Justice 13, 3 (2015), 179‚Äì196.
[24] Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gum-
madi, Adrian Weller, and Baharan Mirzasoleiman. 2022. Crosswalk: Fairness-
enhanced node representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 11963‚Äì11970.
[25] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[26] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).[27] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfac-
tual fairness. Advances in neural information processing systems 30 (2017).
[28] Wen-Zhi Li, Chang-Dong Wang, Hui Xiong, and Jian-Huang Lai. 2023. GraphSHA:
Synthesizing Harder Samples for Class-Imbalanced Node Classification. arXiv
preprint arXiv:2306.09612 (2023).
[29] Zhixun Li, Dingshuo Chen, Qiang Liu, and Shu Wu. 2022. The Devil is in the
Conflict: Disentangled Information Graph Neural Networks for Fraud Detection.
In2022 IEEE International Conference on Data Mining (ICDM). IEEE, 1059‚Äì1064.
[30] Zhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xi-
angxin Zhou, Qiang Liu, Shu Wu, Liang Wang, et al .2024. GSLB: The Graph
Structure Learning Benchmark. Advances in Neural Information Processing Sys-
tems 36 (2024).
[31] Zemin Liu, Yuan Li, Nan Chen, Qian Wang, Bryan Hooi, and Bingsheng He. 2023.
A survey of imbalanced learning on graphs: Problems, techniques, and future
directions. arXiv preprint arXiv:2308.13821 (2023).
[32] Zemin Liu, Trung-Kien Nguyen, and Yuan Fang. 2021. Tail-gnn: Tail-node graph
neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1109‚Äì1119.
[33] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. 2011. Rec-
ommender systems with social regularization. In Proceedings of the fourth ACM
international conference on Web search and data mining. 287‚Äì296.
[34] Jing Ma, Ruocheng Guo, Mengting Wan, Longqi Yang, Aidong Zhang, and Jun-
dong Li. 2022. Learning fair node representations with graph counterfactual
fairness. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 695‚Äì703.
[35] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A survey on bias and fairness in machine learning. ACM com-
puting surveys (CSUR) 54, 6 (2021), 1‚Äì35.
[36] Arpit Merchant and Carlos Castillo. 2023. Disparity, Inequality, and Accuracy
Tradeoffs in Graph Neural Networks for Node Classification. arXiv preprint
arXiv:2308.09596 (2023).
[37] Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun.
2020. Two simple ways to learn individual fairness metrics from data. In Interna-
tional Conference on Machine Learning. PMLR, 7097‚Äì7107.
[38] Neng Kai Nigel Neo, Yeon-Chang Lee, Yiqiao Jin, Sang-Wook Kim, and Srijan
Kumar. 2024. Towards Fair Graph Anomaly Detection: Problem, New Datasets,
and Evaluation. arXiv preprint arXiv:2402.15988 (2024).
[39] Joonhyung Park, Jaeyun Song, and Eunho Yang. 2022. Graphens: Neighbor-aware
ego network synthesis for class-imbalanced node classification. In The Tenth
International Conference on Learning Representations, ICLR 2022. International
Conference on Learning Representations (ICLR).
[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019).
[41] Judea Pearl. 2009. Causality. Cambridge university press.
[42] Jonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf. 2017. Elements of causal
inference: foundations and learning algorithms. The MIT Press.
[43] Maan Qraitem, Kate Saenko, and Bryan A Plummer. 2023. Bias mimicking: A
simple sampling approach for bias mitigation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 20311‚Äì20320.
[44] Tahleen Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. 2019.
Fairwalk: Towards fair graph embedding. (2019).
[45] Junyuan Shang, Cao Xiao, Tengfei Ma, Hongyan Li, and Jimeng Sun. 2019.
Gamenet: Graph augmented memory networks for recommending medication
combination. In proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 1126‚Äì1133.
[46] Saeed Sharifi-Malvajerdi, Michael Kearns, and Aaron Roth. 2019. Average indi-
vidual fairness: Algorithms, generalization and experiments. Advances in neural
information processing systems 32 (2019).
[47] Jaeyun Song, Joonhyung Park, and Eunho Yang. 2022. TAM: topology-aware
margin loss for class-imbalanced node classification. In International Conference
on Machine Learning. PMLR, 20369‚Äì20383.
[48] Weihao Song, Yushun Dong, Ninghao Liu, and Jundong Li. 2022. Guide: Group
equality informed individual fairness in graph neural networks. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
1625‚Äì1634.
[49] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua.
2022. Causal attention for interpretable and generalizable graph classification.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1696‚Äì1705.
[50] Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal,
Prasenjit Mitra, and Suhang Wang. 2020. Investigating and mitigating degree-
related biases in graph convoltuional networks. In Proceedings of the 29th ACM
International Conference on Information & Knowledge Management. 1435‚Äì1444.
[51] Damien Teney, Jindong Wang, and Ehsan Abbasnejad. 2023. Selective mixup
helps with distribution shifts, but not (only) because of mixup. arXiv preprint
arXiv:2305.16817 (2023).
 
1744KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu
[52] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[53] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[54] Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and Tyler Derr.
2022. Improving Fairness in Graph Neural Networks via Mitigating Sensitive
Attribute Leakage. arXiv preprint arXiv:2206.03426 (2022).
[55] Oliver Wieder, Stefan Kohlbacher, M√©laine Kuenemann, Arthur Garon, Pierre
Ducrot, Thomas Seidel, and Thierry Langer. 2020. A compact review of molec-
ular property prediction with graph neural networks. Drug Discovery Today:
Technologies 37 (2020), 1‚Äì12.[56] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[57] I-Cheng Yeh and Che-hui Lien. 2009. The comparisons of data mining techniques
for the predictive accuracy of probability of default of credit card clients. Expert
systems with applications 36, 2 (2009), 2473‚Äì2480.
[58] Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, and Jundong Li.
2023. Adversarial Attacks on Fairness of Graph Neural Networks. arXiv preprint
arXiv:2310.13822 (2023).
[59] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021. Graphsmote: Imbalanced
node classification on graphs with graph neural networks. In Proceedings of the
14th ACM international conference on web search and data mining. 833‚Äì841.
 
1745