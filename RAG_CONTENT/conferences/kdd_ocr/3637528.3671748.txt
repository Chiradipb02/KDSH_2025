FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning
Zihui Wang∗
wangziwei@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaZheng Wang∗
zwang@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaLingjuan Lyu
lingjuan.lv@sony.com
Sony AI
Zurich, Switzerland
Zhaopeng Peng
pengzhaopeng@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaZhicheng Yang
zcyang@stu.xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaChenglu Wen
clwen@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, China
Rongshan Yu
rsyu@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaCheng Wang
cwang@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, ChinaXiaoliang Fan†
fanxiaoliang@xmu.edu.cn
Fujian Key Laboratory of Sensing and
Computing for Smart Cities, School of
Informatics, Xiamen University
Xiamen, China
ABSTRACT
Collaborative fairness stands as an essential element in federated
learning to encourage client participation by equitably distribut-
ing rewards based on individual contributions. Existing methods
primarily focus on adjusting gradient allocations among clients to
achieve collaborative fairness. However, they frequently overlook
crucial factors such as maintaining consistency across local mod-
els and catering to the diverse requirements of high-contributing
clients. This oversight inevitably decreases both fairness and model
accuracy in practice. To address these issues, we propose FedSAC,
a novel Federated learning framework with dynamic Submodel
Allocation for Collaborative fairness, backed by a theoretical con-
vergence guarantee. First, we present the concept of "bounded collab-
orative fairness (BCF) ", which ensures fairness by tailoring rewards
to individual clients based on their contributions. Second, to im-
plement the BCF, we design a submodel allocation module with a
theoretical guarantee of fairness. This module incentivizes high-
contributing clients with high-performance submodels containing
a diverse range of crucial neurons, thereby preserving consistency
∗Both authors contributed equally to this research
†Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671748across local models. Third, we further develop a dynamic aggre-
gation module to adaptively aggregate submodels, ensuring the
equitable treatment of low-frequency neurons and consequently en-
hancing overall model accuracy. Extensive experiments conducted
on three public benchmarks demonstrate that FedSAC outperforms
all baseline methods in both fairness and model accuracy. We see
this work as a significant step towards incentivizing broader client
participation in federated learning. The source code is available at
https://github.com/wangzihuixmu/FedSAC.
CCS CONCEPTS
•Security and privacy; •Information systems →Information
systems applications;
KEYWORDS
federated Learning, collaborative fairness, privacy
ACM Reference Format:
Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang,
Chenglu Wen, Rongshan Yu, Cheng Wang, and Xiaoliang Fan. 2024. Fed-
SAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated
Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671748
1 INTRODUCTION
Federated Learning (FL) empowers multiple data owners to collec-
tively train a global model while preserving the privacy of their
individual training data [ 19,34,35]. Early FL frameworks [ 3,12,27]
usually distributed the same model to all clients without consider-
ing their distinct contributions to the model performance, resulting
3299
KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
in unfairness to high-contributing clients. Collaborative fairness
(CF) [ 18] stands as an essential element in federated learning to mo-
tivate client engagement by ensuring impartial reward distribution
tied directly to individual contributions.
More recently, several gradient-based methods were proposed
to enhance CF [ 18,31–33] (i.e., rewarding clients with correspond-
ing model quality according to their contributions) in FL. They dis-
tribute a larger quantity of gradients to higher-contributing clients
than the lower ones as rewards and quantify the degree of fairness
by Pearson Correlation Coefficient 𝜌. However, for achieving CF,
existing gradient-based methods have two major limitations. On
one hand, the conventional definition of CF doesn’t adequately
distinguish in reward distribution among clients, resulting in a
persistent unfairness for high-contributing clients. In Figure 1 (a),
suppose the contributions of three clients are 𝑐=[1,9,11], and their
rewards are 𝜃∗=[99,99.2,99.3]corresponding. Through the defi-
nition of CF by CGSV [ 33], the fairness is calculated as 𝛾=98.97, but
there exists an underlying unfairness towards 𝐶𝑙𝑖𝑒𝑛𝑡 2and𝐶𝑙𝑖𝑒𝑛𝑡 3
because𝐶𝑙𝑖𝑒𝑛𝑡 1with an inferior contribution is over-rewarded. On
the other hand, conventional gradient-based methods [ 18,31–33]
are ineffective because the inconsistency of local models updated by
variable gradients might lead to significant degradation of overall
model performance. In Figure 1 (a), the local models of clients in
round𝑡(𝜃1,𝑡,𝜃2,𝑡, and𝜃3,𝑡) exhibit notable differences (i.e., the larger
the circle, the higher the accuracy). Consequently, the gradients
uploaded by individual clients may not be the optimal for others,
creating a misalignment between obtained rewards Δ𝜃𝑟𝑒𝑤𝑎𝑟𝑑
𝑖,𝑡(i.e.,
the rewards ultimately obtained by the clients) and expected re-
wards Δ𝜃𝑒𝑥𝑐𝑒𝑝𝑡
𝑖,𝑡(i.e., the rewards that the clients ultimately expected)
for each client.
To address the aforementioned challenges, we propose a novel
Federated learning framework with dynamic Submodel Allocation
for bounded Collaborative fairness (FedSAC ), supported by a the-
ory of convergence while achieving competitive model accuracy.
First, our approach introduces the concept of "bounded collabora-
tive fairness (BCF) (refer to 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛 1)", which ensures fairness
by integrating a differentiated range of rewards allocated to each
client. Second, the submodel allocation module with a theoretical
fairness guarantee, is designed to assign relevant submodels (i.e.,
results of the aggregated model dropout) to individual clients based
on their contributions. Specifically, these submodels encompass
a diverse array of essential neurons for effective training. Third,
the dynamic aggregation module is implemented as a weight re-
alignment mechanism by treating low-frequency neurons equally,
which further improves the overall performance of the global model.
Extensive experiments on three public benchmarks show that the
proposed FedSAC outperforms all baseline methods in terms of
collaborative fairness and model accuracy.
The contributions of this work are summarized:
•We propose FedSAC, a novel federated learning framework
with a convergence guarantee, introducing a new concept
ofbounded collaborative fairness (BCF). To the best of our
knowledge, this is the first approach that allocates submodels
equitably for collaborative fairness in FL.
•We implement the concept of BCF through two modules.
First, submodel allocation module prioritizes high-contributing
Client1Client2 Client3
①∆𝜃2,𝑡𝑡𝑟𝑎𝑖𝑛:localmodelupdate ②∆𝜃2,𝑡𝑖𝑑𝑒𝑎𝑙: ideal model
③∆𝜃2,𝑡𝑒𝑥𝑐𝑒𝑝𝑡 :excepted rewards                         ④∆𝜃2,𝑡𝑟𝑒𝑤𝑎𝑟𝑑 : actual rewards    
Obtained model                  Contribution           Expected model
(a) Gradient -based unfair method         (b) Submodel -based fair method ( FedSAC )𝜃1,𝑡𝜃2,𝑡𝜃3,𝑡𝜃1∗𝜃2∗𝜃3∗
①
Client1Client2 Client3𝜃1,𝑡𝜃2,𝑡𝜃3,𝑡𝜃1∗𝜃2∗𝜃3∗
①② ②③ ③④c1c2c3 c1c2c3Training stepFigure 1: Problem illustration of collaborative fairness in
FL. (a) Conventional gradients-based methods will result in
poor fairness and model accuracy. For example, it is unfair
that obtained models of 𝐶𝑙𝑖𝑒𝑛𝑡 2and𝐶𝑙𝑖𝑒𝑛𝑡 1are equivalent
neglecting the inferior contribution ( 𝑐1) of𝐶𝑙𝑖𝑒𝑛𝑡 1. Plus, the
inconsistency in local models results in that obtained models
of𝐶𝑙𝑖𝑒𝑛𝑡 2and𝐶𝑙𝑖𝑒𝑛𝑡 3are worse than expected ( 𝜃∗
𝑖). (b) Our pro-
posed FedSAC allocates sufficient submodels to each client
by ensuring a comprehensive balance between fairness and
model accuracy. For example, FedSAC ensures that obtained
models of all clients (i.e., 𝐶𝑙𝑖𝑒𝑛𝑡 1,𝐶𝑙𝑖𝑒𝑛𝑡 2and𝐶𝑙𝑖𝑒𝑛𝑡 3) are in
accordance with their contributions respectively. In addition,
FedSAC guarantees the alignment of ③and④of𝐶𝑙𝑖𝑒𝑛𝑡 2dur-
ing the training process, thereby enabling all three clients to
obtain their expected models ( 𝜃∗
𝑖).
clients by rewarding them with high-performance submodels
under a theoretical guarantee. Second, dynamic aggregation
module merges submodels by paying equitable attention to
low-frequency neurons to be aggregated.
•We conduct extensive experiments on three benchmarks
with various settings and demonstrate that FedSAC ourper-
forms all baselines in both fairness and model accuracy.
2 RELATED WORKS
Recent research has shown that distributing different rewards based
on clients’ contributions can significantly impact the FL systems [ 17,
23,36]. The incentive mechanisms can motivate clients to contribute
high-quality data and promote collaboration [ 33]. We outline three
types of rewards that can be adopted to achieve CF in FL.
Money-based reward. Several studies focus on the mechanism
that rewards clients monetary based on their contributions. [ 43]
proposes a reputation-based and reverse auction theory mechanism
to reward clients with a limited budget. [ 37] shows a scheme that
dynamically allocates budgets to clients in a context-aware man-
ner by jointly maximizing the collective utility. While monetary
rewards can be a natural and effective way to incentivize clients
in FL, there exist challenges in maintaining a balance between the
value of model quality and money [1, 42].
Data-based reward. Early studies have explored the fairness
of rewarding different data sizes based on their contributions. [ 25]
evaluates clients’ contributions by aggregating the training data,
3300FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
and reward them with the corresponding models. [ 28] trains a
generative model through the local data of all clients and provides
more synthetic data to those datasets closely aligned with the real
data distribution. However, most of existing data-based reward
methods rely on the centralized aggregation of all the data during
training, making them difficult to be applied in the FL scenarios.
Gradient-based reward. Recent collaborative fairness (CF)
works aim to reward high-contributing clients with optimal mod-
els. CFFL [ 18] allocates different gradient numbers based on lo-
cal accuracy in the validation set and data sizes. CGSV [ 33] re-
wards more gradients to clients whose local gradients are more
similar to the global gradients. FedAVE [ 31] assigns more gradi-
ents to clients whose data distribution information is more sim-
ilar to the ideal dataset. However, existing rewards systems lack
sufficient differentiation, resulting in an ongoing unfairness for
high-contributing clients, which might degrade the fairness and
model accuracy. Different from these methods, we propose a novel
framework to achieve BCF (refer to 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛 1) by allocating a
differential range of rewards to clients.
3 PRELIMINARY
FL system consists of a server and multiple clients, aiming to mini-
mize the weighted average of all clients’ local objectives by optimiz-
ing a global model [ 11,19]. In this setup, the goal of FL framework
is defined as:
𝑚𝑖𝑛
𝜃𝐹(𝜃):=𝑁∑︁
𝑖=1𝑝𝑖𝐹𝑖(𝜃), (1)
where𝜃denotes the global model, 𝑁represents the number of
clients, and 𝑝𝑖=𝑛𝑖
𝑛,𝑛=Í𝑁
𝑘=1𝑛𝑘.𝐹𝑖(𝜃)is the loss on client 𝑖
using model parameters 𝜃, i.e.,𝐹𝑖(𝜃)=1
𝑛𝑖Í
𝜉𝑖∼𝐷𝑖𝑙(𝜃,𝜉𝑖), where𝐷𝑖
represents the local dataset of client 𝑖, and𝑛𝑖denotes the data size
of𝐷𝑖. To achieve this goal as effectively as possible, FedAvg [ 19]
samples a subset 𝑆𝑡of𝑖clients uniformly, 0<𝑖≤𝑁, to train the
global model and aggregate the locally trained models by utilizing
the data size ratio 𝑝𝑖as the weight of client 𝑖. Although FedAvg is
proven to be effective in minimizing the objective successfully, it
may be unfair to high-quality clients since the system distributes the
same rewards to all clients regardless of their contributions [ 18,33].
3.1 Problem Formulation
The standard FL framework allocates the same model to all clients
regardless of their contributions [ 7,19], dampening the motivation
of high-quality clients to join FL [ 25,30]. Collaborative fairness
in FL aims to reward high-contributing clients with high-quality
models. The existing works [ 18,31,33] assess the fairness with the
Pearson Correlation Coefficient, 𝜌(𝑐;𝜃∗), where𝑐and𝜃∗represent
the contributions and rewards of clients, respectively. However,
the definition simply considers the relationship between the con-
tributions and rewards of clients, which may lead to insufficient
incentives for high-contributing clients. For example in Figure 1
(a), suppose the contributions of 𝐶𝑙𝑖𝑒𝑛𝑡 1,𝐶𝑙𝑖𝑒𝑛𝑡 2, and𝐶𝑙𝑖𝑒𝑛𝑡 3are
𝑐𝑖=[1,9,11]and their rewards are 𝜃∗
𝑖=[99,99.2,99.3]correspond-
ingly. Through the definition of CF, the fairness is calculated as
𝛾=98.97, but there exists an underlying unfairness towards 𝐶𝑙𝑖𝑒𝑛𝑡 2and𝐶𝑙𝑖𝑒𝑛𝑡 3because𝐶𝑙𝑖𝑒𝑛𝑡 1with an inferior contribution is over-
rewarded.
To address this issue, we propose Bounded Collaborative Fairness
(BCF) to tackle the issue of insufficient incentives for the high-
contributing client. BCF could ensure 𝑐1<𝜃∗
1<(𝑐1+𝜃∗
3)
2and then
quantitative fairness with 𝜌(𝑐;𝜃∗). The rationale behind the formula
in𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛 1 aims to amplify significant distinctions in rewards.
The formula’s left side ensures that clients’ rewards exceed their
contributions, while the right side prevents excessive rewards for
clients with low contributions.
Definition 1 (Bounded Collaborative Fairness). The
contributions ( 𝑐) and the rewards ( 𝜃∗) of clients are calculated by the
performance of their standalone models (train without collaboration)
and the final models obtained after collaboration, respectively. Based
on client’s obtained rewards 𝑐𝑖<𝜃∗
𝑖<(𝑐𝑖+𝑚𝑎𝑥(𝜃∗))
2, the quantitative
fairness can be computed by 𝛾:= 100×𝜌(𝑐,𝜃∗) where𝜌() is the
Pearson Correlation Coefficient. The larger 𝛾, the better the fairness
of the framework.
4 THE PROPOSED FEDSAC
In this section, we will introduce the details of proposed FedSAC, a
method that ensures both BCF and consistency in local models for
each client. The architecture of FedSAC is shown in Figure 2. The
pseudo codes for FedSAC are provided in Algorithm 1. First, we
introduce the submodel allocation module in Section 4.1. Second,
we present the dynamic aggregation module in Section 4.2. Third,
we proposed the fairness guarantee theory in Section 4.3 to prove
that this submodel allocation strategy can achieve collaborative
fairness. Fourth, we conducted a convergence analysis on FedSAC
and demonstrated its convergence in Section 4.5. In addition, we
analyzed the time complexity and communication costs of FedSAC
in Section 4.5. Finally, we discussed limitations in Section 4.6.
4.1 Submodel Allocation Module
A naive approach achieving bounded collaborative fairness involves
allocating distinct submodels to each client based on their respec-
tive contributions [ 5]. Unlike previous works such as [ 6,24], there
are two primary motivations behind achieving BCF through sub-
model allocation. First, submodels with appropriate pruning may
not match the performance of the global model, enabling clients to
receive diverse submodels according to their contributions. Second,
despite being subsets of the global model, these submodels exhibit
strong mutual validity, meaning that the submodels uploaded by
one client are effective for others, facilitating the training of the
global model. However, it is still challenging to achieve BCF through
submodel-based methods. For one thing, it is crucial to ensure that
the majority of neurons are adequately trained to guarantee the
optimal performance of the global model. For another, the perfor-
mance of allocated submodels should align with their respective
contributions.
To address the aforementioned two challenges, we design a
two-step approach for submodel allocation module. First, we evalu-
ate the importance of each neuron within the model to determine
their contributions respectively (neuron importance evaluation in
Section 4.1.1). Second, we construct submodels for each client with
3301KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
①Neuron Importance EvaluationRound tServer
②Stage 1: Submodel Allocation Stage 2: Dynamic Aggregation
③Aggregate
④Round t+1
Valid neuron               Invalid neuron                              Neuron importance                             Number o f aggregations 1     5      10 1     5      10𝑫𝑨
𝑫𝑩
5.9 7.8
2.3
3.25.9
1.2 3.5 1.3
8.4 5.3 6.7 6.1
𝟐.𝟑+𝟖.𝟒
𝟐𝟓.𝟗+𝟓.𝟑
𝟐𝟑.𝟓
𝟏𝟏.𝟑
𝟏
…①
②Submodel Construction𝑪𝒍𝒊𝒆𝒏𝒕 𝑨
𝑪𝒍𝒊𝒆𝒏𝒕 𝑩Validation
Validation
Figure 2: The overall framework of FedSAC that achieves bounded collaborative fairness by maintaining consistency across
local models. FedSAC consists of two module: 1) submodel allocation module conducts neuron importance evaluation and
submodel construction to reward high-contributing clients with high-performance submodels, thus ensuring consistency in
local models; 2) dynamic aggregation module treats those low-frequency neurons equally, which further refines the performance
of the global model.
varying performances based on their contributions, ensuring a di-
verse array of important neurons is included within each submodel
(submodel construction in Section 4.1.2).
4.1.1 Neuron Importance Evaluation. Each neuron within the
model holds a unique contribution [ 16,20,39]. Our intuition is that
the constructed submodels can yield varied performances. Inspired
by Taylor-FO [ 20], we calculate the neuron importance in the model
by measuring the change in loss upon their removal. For instance,
a greater increase in loss indicates a more significant contribution
by the removed neuron to the model. In Figure 2, neurons depicted
in a redder shade represent a higher contribution to the model.
Essentially, the training objective is to minimize the cross-entropy
loss𝐿𝑐𝑒:
min
𝜃𝑁∑︁
𝑖=1𝐿𝑐𝑒(𝑥𝑖,𝜃), (2)
where𝑥𝑖denotes the sample, 𝜃represents the model, and 𝐿𝑐𝑒(𝑥𝑖,𝜃)
is the loss function of the classification tasks.
The neurons in the model have a multitude of model parameters,
each of which contributes to the overall performance of the model.
The importance of a neuron 𝐼𝑛𝑖of the model can be calculated
through the loss increased by removing it:
𝐼𝑛𝑖=𝐿𝑐𝑒(𝑉,𝜃|𝜃𝑧𝑒𝑟𝑜
𝑛𝑖=0)−𝐿𝑐𝑒(𝑉,𝜃), (3)
where𝑉denotes the validation set, which is constructed by evenly
selecting 10% of the data from the original training samples [ 18],
𝜃𝑧𝑒𝑟𝑜𝑛𝑖represents that the parameters of the 𝑖-th neuron in the model
are all set to 0.
To simplify the construction of submodels, we normalize the
sum of neuron scores, which represent their importance in themodel presented by percentage:
𝐼𝑛𝑖=𝐼𝑛𝑖Í
𝑛𝑖∈𝑆𝐼𝑛𝑖∗100, (4)
where𝑆represents all neurons of the model. To reduce the training
time of the framework, we measure the importance of neurons in
the model by Eq. (3) and Eq. (4) every 10 epochs. All these operations
allow us to efficiently assess the importance of each neuron while
limiting excessive computation demands.
4.1.2 Submodel Construction. In pursuit of fairness, we employ
a dynamic allocation system for submodels with varying perfor-
mances, leveraging clients’ reputations derived from their contribu-
tions. Our approach incorporates a pruning mechanism tailored to
clients’ contributions, simplifying the extraction of submodels with
different performance levels from the global model. In this scheme,
the client𝑖’s reputation 𝑟𝑖is expressed as:
𝑟𝑖=𝑒𝑐𝑖∗𝛽, (5)
𝑟𝑖=𝑟𝑖
𝑚𝑎𝑥(𝑟)∗100, (6)
where𝑐𝑖represents client 𝑖’s contribution, 𝛽is a hyper-parameter.
The reputations of clients are directly proportional to their contri-
butions. The design rationale for Eq. (5) and Eq. (6) is to calculate
the clients’ reputations ( 𝑟), which facilitates the allocation of their
submodels fairly. More specifically, our pruning method begins
with the most important neuron, ensuring that submodels for low-
contribution clients possess a higher parameter count, which is
beneficial for training the global model. These actions serve a dual
purpose: promoting collaborative fairness while maximizing the
overall performance of the global model. Submodel 𝜃𝑖is constructed
3302FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
by neurons with different reputations:
𝜃𝑖=𝑞𝑢𝑎𝑛𝑡𝑖𝑡𝑦(𝑟𝑖,∑︁
𝑛𝑖∈𝑆𝐼𝑛𝑖), (7)
where𝑞𝑢𝑎𝑛𝑡𝑖𝑡𝑦(𝑟𝑖,Í
𝑛𝑖∈𝑆𝐼𝑛𝑖)represents the submodel 𝜃𝑖when
𝑟𝑖=Í
𝑛𝑖∈𝑆𝐼𝑛𝑖,𝑟𝑖denotes client 𝑖’s reputation,Í
𝑛𝑖∈𝑆𝐼𝑛𝑖denotes
the set importance for different neurons. 𝑆represents the positions
of all neurons in the model, arranged in ascending order from the
least to the most important. This design choice aims to maximize the
inclusion of neurons in each submodel, thereby enhancing the per-
formance of the corresponding local model updates. Subsequently,
this quantity will be utilized in Eq. (8) to generate the submodel’s
mask.
4.2 Dynamic Aggregation Module
Next, the server aggregates the locally trained submodels and allo-
cates distinct submodels to clients in the subsequent round. Recent
submodel-based methods [ 6,24] have aimed to allocate varied sub-
models containing numerous neurons to clients. However, these
approaches might pose a potential risk of compromising overall
model performance when integrating low-frequency neurons into
the global model. Consequently, employing a direct aggregation
method such as FedAvg [19] for all neurons becomes inequitable.
Instead of simply averaging the uploaded submodels, our objec-
tive is to optimize the utilization of all neurons within the model.
With the sizes of submodels varying across clients, it becomes es-
sential to treat the contribution of each neuron individually during
aggregation. To ensure fair treatment of low-frequency neurons,
we integrate the frequency of submodel parameter aggregations as
weights to dynamically aggregating local models:
𝑚𝑎𝑠𝑘𝑡
𝑖=𝑚𝑎𝑠𝑘(𝜃𝑡
𝑖,𝜃𝑡−1
𝑔), (8)
𝜃𝑡+1
𝑔=Í
𝑖∈𝑁𝜃𝑡
𝑖Í
𝑖∈𝑁𝑚𝑎𝑠𝑘𝑡
𝑖, (9)
where𝑚𝑎𝑠𝑘(𝜃𝑖,𝜃𝑔)denotes submodel 𝜃𝑖’s mask (same shape as
the submodel 𝜃𝑖),𝜃𝑔denotes the aggregated model, 𝑁denotes the
total number of clients. It sets the components of both 𝜃𝑖and𝜃𝑔
at the same position to 1 and 0 for the rest. The role of the mask
function𝑚𝑎𝑠𝑘(𝜃𝑖,𝜃𝑔)is to calculate the frequency of each model
parameter𝜃𝑖selected by the global model 𝜃𝑔in round𝑡. Later,
the mask function 𝑚𝑎𝑠𝑘(𝜃𝑖,𝜃𝑔)will be utilized in Eq. (9) to treat
those low-frequency parameters equally by suppressing the weight
of high-frequency parameters in the aggregation, which makes
each parameter play a fair role during the aggregation phase. For
example, the more the frequency of a selected parameter in round 𝑡,
the smaller the weight of the parameter to be aggregated in round
𝑡+1.
4.3 Fairness Guarantee
In Section 4.1, we delved into the fundamental concept underpin-
ning our definition of fairness. This concept centers on rewarding
high-contributing clients with high-performance submodels, where
a submodel’s improved performance correlates with the number of
neurons it contains. Consequently, this approach leads to a training
loss (i.e., model accuracy) that more closely aligns with the aggre-
gated model. It’s important to note that the submodel 𝜃𝑖acquired
by client𝑖is determined based on its reputation 𝑟𝑖across the entireAlgorithm 1 FedSAC
Input: The global model 𝜃𝑔, the local submodel 𝜃𝑖, neurons𝑖’s
importance 𝐼𝑛𝑖, the number of local update steps 𝐸, learning
rate𝜂𝑡, number of clients 𝑁, hyper-parameter 𝛽, client’s con-
tribution𝑐
1:Initialize the global model parameters 𝜃0𝑔
2:forround𝑡=0,1,...,𝑇−1do
3: Compute𝐼𝑡𝑛𝑖((3) and (4)) of 𝜃𝑡𝑔
4: Calculate the reputation 𝑟𝑖of client𝑘:𝑟𝑖=𝑒𝑐𝑖∗𝛽
𝑚𝑎𝑥(𝑒𝑐∗𝛽)∗100
5: Calculate Submodels 𝜃𝑡
𝑖of clients𝑖in round𝑡:𝜃𝑡
𝑖=
𝑞𝑢𝑎𝑛𝑡𝑖𝑡𝑦(𝑟𝑖,Í
𝑛𝑖∈𝑆𝐼𝑡𝑛𝑖)
6: foreach client𝑖∈𝑁do
7: foreach iteration 𝑗=0,1,...,𝐸−1do
8: 𝜃𝑡
𝑖,𝑗+1←𝜃𝑡
𝑖,𝑗−𝜂𝑡∇𝐹𝑖(𝜃𝑡
𝑖,𝑗)
9: end for
10: end for
11: Submodel𝑖’s mask in round 𝑡:𝑚𝑎𝑠𝑘𝑡
𝑖=𝑚𝑎𝑠𝑘(𝜃𝑡
𝑖,𝜃𝑡−1𝑔)
12: The server aggregates the received submodels: 𝜃𝑡+1𝑔=Í
𝑖∈𝑁𝜃𝑡
𝑖 Í
𝑖∈𝑁𝑚𝑎𝑠𝑘𝑡
𝑖
13:end for
training process up to iteration 𝑡.
Our primary result ensures a notion of fairness under specific
conditions concerning the loss function 𝐹. If client𝑖holds a higher
reputation than client 𝑗(𝑟𝑖≥𝑟𝑗), and the submodel 𝜃𝑡
𝑖obtained by
client𝑖encompasses the submodel 𝜃𝑡
𝑗obtained by client 𝑗(𝜃𝑡
𝑗∈𝜃𝑡
𝑖∈
𝜃𝑡𝑔). Then, the submodel 𝜃𝑡
𝑖obtained by client 𝑖will exhibit closer
alignment with the aggregated model 𝜃𝑡𝑔in round𝑡. Letting𝛿𝑡
𝑖:=
||𝜃𝑡𝑔−𝜃𝑡
𝑖||, it’s evident that 𝛿𝑡
𝑖⩽𝛿𝑡
𝑗. Consequently, the submodel 𝜃𝑡
𝑖
obtained by client 𝑖will yield a smaller loss function 𝐹(𝜃)compared
to client𝑗in round𝑡.
Assumption 1 ( 𝐿-smooth F). If𝐹is𝐿-smooth, then∀𝜃𝑖,𝜃𝑗∈𝜃,
𝐹(𝜃𝑖)⩽𝐹(𝜃𝑗)+∇𝐹(𝜃𝑗)𝑇(𝜃𝑖−𝜃𝑗)+𝐿
2||𝜃𝑖−𝜃𝑗||2. (10)
Assumption 2 ( 𝜇-strongly convex F). If𝐹is𝜇-strongly convex,
then∀𝜃𝑖,𝜃𝑗∈𝜃,
𝐹(𝜃𝑖)≥𝐹(𝜃𝑗)+∇𝐹(𝜃𝑗)𝑇(𝜃𝑖−𝜃𝑗)+𝜇
2||𝜃𝑖−𝜃𝑗||2. (11)
Theorem 1 (Fairness in Training Loss). Assume Assumptions 1
and 2 hold, FedSAC can guarantee collaborative fairness by rewarding
high-contributing clients obtaining high-performance models. For-
mally speaking, let 𝛿𝑡
𝑖:=||𝜃𝑡𝑔−𝜃𝑡
𝑖||. Suppose that 𝜃𝑡is close to a
stationary point of 𝐹for𝑡≥𝑇∈𝑍+, and𝐹()is both𝐿-smooth and
𝜇-strongly convex with 𝐿⩽𝜇. For all𝑖,𝑗∈𝑁in round t, if 𝑟𝑖≥𝑟𝑗,
it follows that 𝜃𝑡
𝑗∈𝜃𝑡
𝑖∈𝜃𝑡𝑔,𝛿𝑡
𝑖⩽𝛿𝑡
𝑗, and therefore 𝐹(𝜃𝑡
𝑖)⩽𝐹(𝜃𝑡
𝑗).
The proof process is as follows:
From𝐿-smoothness (ASSUMPTION 1), we have
𝐹(𝜃𝑡
𝑖)⩽𝐹(𝜃𝑡
𝑁)+∇𝐹(𝜃𝑡
𝑁)𝑇(𝜃𝑡
𝑖−𝜃𝑡
𝑁)+𝐿
2𝛿2
𝑖,𝑡
|                                          {z                                          }
𝑅𝐿. (12)
From𝜇-strongly convex (ASSUMPTION 2), we have
3303KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
𝐹(𝜃𝑡
𝑗)≥𝐹(𝜃𝑡
𝑁)+∇𝐹(𝜃𝑡
𝑁)𝑇(𝜃𝑡
𝑗−𝜃𝑡
𝑁)+𝜇
2𝛿2
𝑗,𝑡
|                                           {z                                           }
𝑅𝜇. (13)
In order to prove 𝐹(𝜃𝑡
𝑖)⩽𝐹(𝜃𝑡
𝑗), it suffices to prove 𝑅𝐿⩽𝑅𝜇or
equivalently 𝑅𝐿−𝑅𝜇⩽0.
𝑅𝐿−𝑅𝜇=∇𝐹(𝜃𝑡
𝑁)𝑇(𝜃𝑡
𝑖−𝜃𝑡
𝑗)
|                  {z                  }
𝑅1+1
2(𝐿𝛿2
𝑖,𝑡−𝜇𝛿2
𝑗,𝑡)
|              {z              }
𝑅2. (14)
With𝐿⩽𝜇and𝛿𝑖,𝑡⩽𝛿𝑗,𝑡, we have
𝑅2=1
2(𝐿𝛿2
𝑖,𝑡−𝜇𝛿2
𝑗,𝑡)⩽𝐿
2(𝛿2
𝑖,𝑡−𝛿2
𝑗,𝑡)⩽0. (15)
We define𝜃𝑡
𝑁being close to a stationary point of F by establishing
an upper limit on the gradient:
||∇𝐹(𝜃𝑡
𝑁)||⩽𝐿|𝛿2
𝑖,𝑡−𝛿2
𝑗,𝑡|
2||𝜃𝑡
𝑖−𝜃𝑡
𝑗||. (16)
We have the following:
|𝑅1|≜|∇𝐹(𝜃𝑡
𝑁)𝑇(𝜃𝑡
𝑖−𝜃𝑡
𝑗)|
⩽||∇𝐹(𝜃𝑡
𝑁)||×||(𝜃𝑡
𝑖−𝜃𝑡
𝑗)||
⩽𝐿|𝛿2
𝑖,𝑡−𝛿2
𝑗,𝑡|
2
⩽|𝑅2|,(17)
where the first inequality is derived from the Cauchy-Schwarz, the
second inequality is by substituting the aforementioned upper limit
(refer to Eq. (16)), and the last inequality (line 1209) emerges from
taking the absolute values of two negative values (refer to Eq. (15)).
Finally, given that|𝑅1|⩽|𝑅2|and𝑅2⩽0, we derive 𝑅1+𝑅2⩽0.
Therefore, it follows that 𝑅𝐿−𝑅𝜇≜𝑅1+𝑅2⩽0, which subsequently
implies𝐹(𝜃𝑡
𝑖)⩽𝐹(𝜃𝑡
𝑗).
4.4 Convergence Analysis
In this section, we delve into the convergence analysis of the pro-
posed FedSAC. To guarantee convergence to the global optimum,
we make the assumption that each neuron in the aggregated model
is equally allocated over 𝑇rounds. Consequently, the anticipated
weight of the allocated submodel 𝜃𝑖contracts towards the aggre-
gate model 𝜃𝑔, i.e.,𝜃𝑡+1
𝑖=𝑝𝑖𝜃𝑡𝑔. Here,𝑝𝑖(0⩽𝑝𝑖⩽1) denotes the
long-term expectation of the size ratio between the submodel 𝑖
and the aggregate model obtained in multiple iterations. At this
stage, Eq. (9) can be expressed as the aggregation of each submodel
𝜃𝑖divided by its respective 𝑝𝑖, i.e.,𝜃𝑡+1𝑔=Í𝑁
𝑖=1𝜃𝑡+1
𝑖
𝑝𝑖. We present
THEOREM 2 below, which demonstrates that FedSAC enables the
convergence of the aggregation model. Assumptions 3 and 4 are
derived from the works [26, 38, 44].
Assumption 3. Let𝜉𝑡
𝑖denote samples uniformly from the local data
of the𝑖-th device at random. It is asserted that the variance of sto-
chastic gradients within each device remains constrained:
𝐸∥∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)−∇𝐹𝑖(𝜃𝑡
𝑖)∥⩽𝜎2
𝑖, (18)Assumption 4. The expected squared norm of stochastic gradients is
uniformly constrained:
𝐸∥∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)∥⩽𝐺2, (19)
where𝑖∈{1,2,...,N}and𝑡∈{1,2,...,𝑇−1}.
Assumption 5. Each neuron in the aggregation model is assigned the
same number of times after 𝑇rounds. Therefore, the expected weight
of the allocated submodel 𝜃𝑖is a contraction of the aggregate model
𝜃𝑔, i.e.,𝜃𝑡+1
𝑖=𝑝𝑖𝜃𝑡𝑔. Here,𝑝𝑖(0⩽𝑝𝑖⩽1) denotes the long-term
expectation of the size ratio between the submodel 𝑖and the aggregate
model obtained in multiple iterations.
Theorem 2 (Asymptotic convergence). Given that Assump-
tions 1 to 5 hold and 𝐿,𝜇,𝜎𝑖,𝐺,𝑝be defined therein. Choose 𝜅=2
𝜇,
𝛾=𝑚𝑎𝑥{8𝐿
𝜇,𝐸}−1and the learning rate 𝜂𝑡=2
𝜇(𝛾+𝑡). Then FedSAC
satisfies𝐸[𝐹(¯𝜃𝑇)]−𝐹∗⩽[𝐿
𝛾+𝑇(2𝐵
𝜇2+𝛾+1
2△1)].
The proof is shown in Appendix A.
4.5 Complexity and Communication Cost
Analysis
We further analyze the time complexity and communication costs
of FedSAC as follows.
For the time complexity, the primary computational demand
in FedSAC stems from evaluating neuron importance, as defined
in Eq. (3) and Eq. (4)). The time complexity for this evaluation is
𝑂(𝑀), where𝑀denotes the total number of neurons across the
hidden layers of the global model.
For the communication cost, FedSAC mitigates the introduc-
tion of additional communication overhead by conducting neuron
importance evaluation solely on the server. This approach effec-
tively eliminates the necessity for client-server communication,
thereby enhancing overall efficiency. Moreover, it displays a com-
munication complexity of O(d*m) per round, as outlined in [ 6],
where m ⩽1 denotes the average ratio of submodel parameters to
the global model. As a result, FedSAC showcases lower communi-
cation complexity compared to all baseline methods in cross-silo
FL scenarios [ 22]. More detailed results about the communication
cost analysis are put in Appendix1E.
4.6 Limitations
In Table 1 and Table 2, we conduct extensive experiments on var-
ious datasets and observe that FedSAC could exhibit a distinct
advantage over all baseline methods in terms of both fairness and
model accuracy. Nevertheless, the sufficient evaluation of neuron
importance (Section 4.1.1) within the submodel allocation module
imposes an additional computational burden. This problem may be
amplified for large models. Despite this challenge, we hold a strong
conviction that the substantial enhancements in both fairness and
accuracy achieved through by FedSAC clearly affirm its superiority
over baseline methods.
1https://arxiv.org/abs/2405.18291
3304FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
5 EXPERIMENTS
In this section, we conduct comprehensive experiments to answer
the following research questions:
RQ1. How does the fairness of our FedSAC compare to various
state-of-the-art methods?
RQ2. How does the predictive model performance achieved by
our proposed method compare with the state-of-the-art methods
on different datasets?
RQ3. How do different components (i.e., submodel allocation
module and dynamic aggregation module) affect the fairness and
the predictive model performance?
5.1 Experimental Settings
Datasets and Models. We evaluate the performance of FedSAC
on three commonly used public datasets in collaborative fairness,
including Fashion MNIST [ 9], CIFAR10 [ 8], and SVHN [ 21]. Follow-
ing [10,15,29], we employ a feedforward neural network with two
hidden layers for all datasets.
Data splits. We construct five heterogeneous scenarios by vary-
ing the size and the class numbers of the dataset. For imbalanced
dataset sizes (POW) [ 18,33], we randomly divide the total dataset
into various data sizes for each client by using a power law. For
CIFAR10, we partition the data set of size 20000 among 10 clients.
The clients with more extensive data sizes are expected to achieve
better prediction performance. For imbalanced class numbers
(CLA) [ 18,33], we change the number of classes and keep them
have the same amount of data. For CIFAR10 with 5 clients, clients 1,
2, 3, 4, 5 own local training data with 1, 3, 5, 7, 10 classes respectively.
Forimbalanced data size and class numbers (DIR), we provide
clients with various data sizes and classes by the Dirichlet distri-
bution function [ 2,4,40,41]. Specifically, we sample 𝑝𝑙
𝑖∼𝐷𝐼𝑅(𝛼)
and assign a 𝑝𝑙
𝑖percentage of the data of class 𝑙to client𝑖, where
𝐷𝐼𝑅(𝛼)is the Dirichlet distribution with a parameter 𝛼.
Baselines. We compare FedSAC with the following methods:
(1) FedAvg [ 19] distributes the same model to all clients in each
FL iteration. In this case, Pearson Correlation Coefficient 𝜌()in
Section 3.1 is uncomputable. To address this and create a person-
alized model for each client, we follow CFFL [ 18] and CGSV [ 33],
which enables clients to train for an additional epoch at the end of
FL algorithm. (2) q-FFL[ 13] enables the reweighting of loss across
different clients by the q-parameterized weights, thus reducing the
variance in the accuracy distribution and achieving a fairer distri-
bution of accuracy. (3) CFFL [ 18] allocates more gradients to higher
reputation client, and the reputation is calculated by the local accu-
racy and data sizes (or label diversity). (4) CGSV [ 33] assigns more
gradients to clients whose local model gradients is more similar
to the global gradients. (5) FedAVE [ 31] assigns more gradients to
clients whose data distribution information is more similar to the
ideal dataset. (6) Standalone [ 18] trains local models alone without
collaboration. Particularly, to evaluate more fairly, we make all
algorithms distribute rewards based on client contributions rather
than the calculated reputations.
Hyper-Parameters. We tune all hyper-parameters in datasets
by using grid search with FedAvg [ 19] and subsequently apply the
optimal parameters obtained from the validation dataset. The batch
size is𝐵= 64 for SVHN and 𝐵= 32 for both FashionMNIST andCIFAR10. The optimal parameters for Cifar10, SVHN, and Fashion-
MNIST of six scenarios are 𝐸= {15, 20},𝜂= {0.03, 0.05}, 𝛽= {2, 3,
5, 8};𝐸= 20,𝜂= {0.03, 0.05}, 𝛽= {3, 5, 10}, and 𝐸= {15, 20},𝜂= {
0.05, 0.1},𝛽= {1, 10, 20, 25}, respectively. For comparison, we select
the best fairness achieved by each method. The effects of hyper-
parameter𝛽on FedSAC are showed in Appendix D (the smaller 𝛽
is, the higher the accuracy achieved by FedSAC). More details about
the hyper-parameters results ( 𝜂,𝐸, and𝛽) are put in Appendix1F.
Implementation. All experiments are run on a 64 GB-RAM
Ubuntu 18.04.6 server with Intel(R) Xeon(R) CPU E5-2630 v4 @
2.20GHz and 1 NVidia(R) 2080Ti GPUs.
5.2 Experimental Results
Fairness (RQ1). To evaluate the FedSAC fairness, we compared
it with a few baselines on three datasets. Table 1 shows the fair-
ness metrics according to 𝐷𝑒𝑓𝑖𝑛𝑖𝑡𝑖𝑜𝑛 1. Standalone [ 18] trains local
models alone without collaboration, which represents the clients’
contributions. Table 1 indicates that our proposed dynamic sub-
model allocation mechanism achieves a fairness score above 95.73%
on all datasets, while the FedAvg performs poorly with the lowest
fairness score of -19.83%. On three datasets, the fairness of algo-
rithms (i.e., CFFL, CGSV, and FedAVE) exceeds 73.65% for the POW
and CLA scene. In these scenarios, the client’s contribution varies
greatly and is mainly related to the amount of data or diverse labels
of data. For the DIR scene, the data distribution among clients is sig-
nificantly uneven, resulting in a high degree of non-iid settings and
clients with relatively similar contributions. Consequently, CFFL,
CGSV, and FedAVE show low fairness, as the rewards received by
clients tend to be indistinguishable. In particular, in DIR (1.0) of
CIFAR10, our method outperforms CFFL, CGSV and FedAVE by
69.24%, 35.85%, and 40.71%, respectively.
Table 1 demonstrates that the proposed FedSAC outperforms
the state-of-the-art approaches in fairness, and validated the ef-
fectiveness of our method: high-contributing clients obtain high-
performance models. Figure 3 shows the comparison results of
overall performance to achieve bounded collaborative fairness with
state-of-the-art methods in CIFAR10 (left), SVHN (middle), and
Fashion MNIST (right). Obviously, FedSAC outperforms all base-
lines in terms of fairness.
Predictive performance (RQ2). To effectively assess the pre-
dictive performance of algorithms, we present our highest test ac-
curacies in comparison with all baseline methods in Table 2. These
results demonstrate the ability of the algorithms to reward high-
contributing clients with high-performance. First, comparing the
accuracy of FedSAC with Standalone (i.e., contribution) reveals that
FedSAC significantly outperforms Standalone. Second, among the
POW scene, FedSAC achieves the highest performance in CIFAR10,
SVHN and Fashion MNIST with accuracies of 48.61%, 74.84%, and
87.88%, respectively. Third, for the CLA scene on three datasets, the
highest accuracy is obtained by FedSAC, surpassing FedAvg by at
least 0.19%. In addition, in the extremely non-iid setting (e.g., DIR
(1.0) of SVHN), our method outperforms CFFL, CGSV, and FedAVE
by 2.17%, 5.06%, and 4.62%, respectively. Finally, for the DIR(2.0),
and DIR (3.0) scenes, FedSAC achieves comparable performance
to baseline methods in terms of accuracy. Specially, the notably
poor accuracy of q-FFL appears attributed to its mechanism that of-
fers the same reward to all clients, without adapting these rewards
3305KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
Dataset CIF
AR10 SVHN Fashion
MNIST
No
. Clients 10 10 10
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Fe
dAvg[19] 14.55±25.4
84.68±3.5 10.77±6.3 15.06±21.2 68.93±8.1 -19.24±26.7
71.42±13.0 58.56±4.0 50.95±13.5 31.43±19.0 -19.83±24.1
78.44±4.3 17.94±7.3 60.36±16.3 76.09±29.5
q-FFL[13] 34.91±15.1
98.74±0.5
90.89±1.5
89.01±1.5
73.02±2.5 67.97±11.6
93.05±1.0 82.11±1.8 86.87±4.7
89.41±2.1 42.69±9.8
88.99±0.3
73.19±6.0 83.93±0.7 79.99±7.3
CFFL[18] 93.55±1.3
89.99±0.8 29.90±3.6 81.82±0.8 59.86±3.0 96.38±1.5
95.63±0.4
46.91±5.7 38.58±2.5 31.35±8.3 90.94±0.5
86.50±0.7 85.90±0.9
85.09±1.3 71.10±0.3
CGSV[33] 90.78±0.6
91.04±0.8 63.29±3.6 84.59±1.6 84.75±0.2 90.99±0.4
87.22±0.6 72.09±0.2 72.19±0.2 76.31±0.2 95.34±0.3
73.65±2.6 82.91±3.6 82.91 ±3.6 84.95±1.3
Fe
dAVE[31] 85.50±0.8
92.80±1.2 58.43±0.9 85.82±0.7 88.61 ±1.3 92.68±0.4
92.77±0.8 82.42±1.2
64.83±1.0 79.38±0.8 86.98±0.5
86.36±1.1 79.51±1.3 87.99 ±0.5
67.62±0.3
Ours 98.80±0.2 99.06±0.3 99.14±0.6 95.73±0.5 97.01±0.7 99.44±0.3 99.74±0.1 96.09±0.3 96.48±0.2 98.32±0.8 96.35±0.2 98.93±0.7 99.23±0.3 97.71±0.2 98.62±0.3
Table 1: Comparison results of fairness 𝜌∈[− 100,100]with state-of-the-art methods on three datasets. The reported results are
averaged over 5 runs with different random seeds. (A higher value indicates better fairness. The best average result is marked
in bold. The second-best result is underlined. These notes are the same to others.)
Dataset CIF
AR10 SVHN Fashion
MNIST
No
. Clients 10 10 10
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Standalone 41.23±0.1
37.49±0.2 33.78±0.2 33.54±0.1 31.89±0.1 60.02±0.2
52.05±0.3 41.41±0.2 58.07±0.2 61.11±0.2 84.36±0.2
82.52±0.3 64.19±0.3 67.39±0.1 74.29±0.2
FedAvg[19] 48.36±0.2
42.64±0.5
48.84±0.1
49.49±0.5
49.72±0.5 74.16±0.2
68.25±0.1 77.75±0.2
78.17±0.1
81.35±0.1 87.64±0.2
85.42±0.0
87.32±0.1
87.27±0.2
88.25±0.2
q-FFL[13] 46.22±1.5
41.40±0.2 35.00±0.9 37.41±1.1 37.81±0.4 69.61±0.6
54.71±1.52 34.15±0.6 44.69±1.3 56.92±1.1 85.44±0.2
82.93±0.5 65.51±2.4 69.61±2.1 78.96±0.8
CFFL[18] 47.94±0.6
42.12±0.3 44.44±0.9 48.44±0.3 47.56±1.2 72.68±0.2
69.66±0.7
75.89±0.9 75.21±0.3 77.01±0.9 87.40±0.4
85.29±0.4 86.16±1.0 87.23±0.2 87.11±0.8
CGSV[33] 34.60±0.7
39.33±0.5 47.22±0.6 44.12±0.5 39.31±0.7 65.92±0.6
65.96±0.3 73.00±1.9 75.16±0.2 77.42±0.5 83.16±0.3
82.41±1.3 85.03±2.9 84.73±3.2 76.51±4.2
FedAVE[31] 46.51±0.2
35.18±1.5 46.60±0.6 39.20±1.2 40.60±1.8 70.80±0.5
65.10±0.6 73.44±0.6 73.43±0.3 75.48±0.7 86.18±0.6
79.86±1.2 76.66±0.8 80.90±1.3 67.74±0.8
Ours 48.61±0.2 44.16±0.2 49.06±0.6 50.01±0.2 49.85±0.3 74.84±0.2 70.51±0.8 78.06±0.1 78.55±0.1 81.95±0.4 87.88±0.2 85.61±0.3 87.85±0.1 87.54±0.4 88.38±0.4
Table 2: Comparison results of the maximum test accuracy (%) with state-of-the-art methods on three datasets. The reported
results are averaged over 5 runs with different random seeds. (A higher value indicates better accuracy.)
20 40 60 80 100
Fairness35.037.540.042.545.047.550.0The maximum test accuracyCIFAR10
FedSAC
20
 0 20 40 60 80 100
Fairness4050607080The maximum test accuracySVHN
Frameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSAC
20
 0 20 40 60 80 100
Fairness6570758085The maximum test accuracyFashion MNIST
Frameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSACFrameworks
FedAvg
q-FFL
CFFL
CGSV
FedAVE
FedSACScenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)Scenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)Scenes
POW
CLA
Dir(1.0)
Dir(2.0)
Dir(3.0)
Figure 3: Comparison results of overall performance to achieve bounded collaborative fairness with state-of-the-art methods
in CIFAR10 (left), SVHN (middle), and Fashion MNIST (right). (The closer the point is to the upper-right corner, the better the
performance.)
based on individual client contributions. Figure 4 illustrates the dis-
tributions of contributions and allocated rewards under scenes (i.e.,
DIR (1.0)) in CIFAR10 comparing FedSAC against baseline meth-
ods. It demonstrates that FedSAC not only guarantees bounded
collaborative fairness but also enables clients to receive rewards
that exceed their contributions (i.e., Standalone). More results un-
der different scenarios (i.e., POW and CLA scene) on CIFAR10 and
SVHN are presented in Appendix B. In short, FedSAC outperforms
all baselines in terms of accuracy.
Figure 5 illustrates the changes in clients’ test accuracy as the
number of communication rounds increases in the POW, and CLA
data partition of CIFAR10 and SVHN. Owing to the varying datasizes and diversity of labels owned by clients in FL, their contri-
butions to the system exhibit significant differences. As shown
in Figure 5, our proposed FedSAC, underpinned by a theoretical
guarantee, aims to reward high-contributing clients with high-
performance submodels by maintaining consistency in local mod-
els. As a result, each client will converge to a different model and
achieve varying levels of performance.
Ablation study (RQ3). To evaluate the effectiveness of two
proposed modules in FedSAC, a series of ablation experiments are
carried out on three public benchmarks with 10 clients, as shown
in Table 3 and 4. The operation of eliminating neuron importance,
denoted as𝑤/𝑜 𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛 , aims to treat all neurons equally and
3306FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Dataset CIF
AR10 SVHN Fashion
MNIST
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
𝑤/𝑜
𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛 79.99
96.47 10.93 43.81 -14.61 96.88
94.88 31.86 64.94 82.58 91.05
96.09 84.57 79.14 65.53
𝑤/𝑜𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛 98.64
98.13 99.22 84.81 83.59 99.00
99.20 95.66 96.05 87.65 93.72
97.45 97.32 27.40 91.43
𝐹𝑒𝑑𝑆𝐴𝐶 98.80
99.06 99.14 95.73 97.01 99.44
99.74 96.09 96.48 98.32 96.35
98.93 99.23 97.71 98.62
Table 3: Ablation studies on FedSAC for fairness 𝜌∈[− 100,100]on three public benchmarks. A higher 𝜌denotes better fairness.
Dataset CIF
AR10 SVHN Fashion
MNIST
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
𝑤/𝑜
𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛 47.68
42.46 40.02 47.00 44.09 69.98
66.95 66.25 68.14 74.02 87.48
85.00 70.99 73.13 83.43
𝑤/𝑜𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛 48.03
43.62 48.23 49.46 48.61 73.81
68.91 77.35 72.89 80.46 87.74
85.95 86.90 85.72 86.74
𝐹𝑒𝑑𝑆𝐴𝐶 48.61
44.16 49.06 50.01 49.85 74.84
70.51 78.06 78.55 81.95 87.88
85.61 87.85 87.54 88.38
Table 4: Ablation studies on FedSAC for the maximum test accuracy (%) on three public benchmarks.
00.20.40.6
1 2 3 4 5 6 7 8 9 10AccuracyCIFAR10
Contribution CFFL q-FFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10AccuracySVHN
Contribution CFFL q-FFL CGSV FedAVE FedSAC00.20.40.6
1 2 3 4 5 6 7 8 9 10AccuracyCIFAR10
Contribution q-FFL CFFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10AccuracySVHN
Contribution q-FFL CFFL CGSV FedAVE FedSAC
Figure 4: Comparison results of test accuracy using the data
partition of DIR (1.0) with state-of-the-art methods in CI-
FAR10 (up) and SVHN (down). Results of other scenes (i.e.,
POW and CLA) are in Appendix B.
allocate submodels based on their contributions. 𝑤/𝑜𝑎𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛
denotes removing the dynamic aggregation module, which uses
the traditional FedAvg aggregation method to train. The effective-
ness of the submodel allocation module is demonstrated in Table 3,
indicating that this module can reward high-contribution clients
to obtain high-performance models. In particular, our method has
significantly improved the fairness measure by 88.21% on the DIR
(1.0) scene of the CIFAR10 dataset. Table 4 shows the results of
the proposed dynamic aggregation module, which implies that this
module can effectively aggregate submodels with different sizes,
thereby further improving the overall performance of the local
0.20.30.40.5
0 50 100 150 200a. CIFAR10 -POW
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 10
0.10.20.30.40.5
0 50 100 150 200c. CIFAR10 -CLA
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 10
0.10.20.30.40.5
0 50 100 150 200e. CIFAR10 -DIR(2.0)
client 1 client 2 client 3 client 4 client 5
client 6 client 7 client 8 client 9 client 1000.20.40.60.8
0 100 200 300 400 500b. SVHN -POW
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10
00.20.40.60.8
0 100 200 300 400 500d. SVHN -CLA
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10Accuracy
Round sAccuracy
Round sAccuracy
Round s
Accuracy
Round sAccuracy
Round sAccuracy
Round s00.20.40.60.8
0 100 200 300 400 500f. SVHN -DIR(2.0)
client1 client2 client3 client4 client5
client6 client7 client8 client9 client10Figure 5: The test accuracy achieved by clients during train-
ing for CIFAR10 (left) and SVHN (right) in each round, under
the setting of POW and CLA.
models. Thus, the ablation study demonstrates that the two de-
signed modules in FedSAC are crucial and significant in enhancing
bounded collaborative fairness.
In summary, all experimental results show that both fairness
and model accuracy are of significance for bounded collaborative
fairness, and our FedSAC outperforms all baseline methods in both
fairness and model accuracy. More detailed results on the varying
numbers of clients (i.e., 20, 40, and 60) used are put in Appendix C.
6 CONCLUSION
In this work, we introduce a novel FL framework named FedSAC
that allocates submodels based on their contributions, thereby en-
suring bounded collaborative fairness and attaining superior local
accuracy while maintaining the consistency in local models. Our
method ensures that high-contributing clients can be rewarded
with high-performance submodels, which in turn enhances the
overall model accuracy. The experiments on three datasets show
that FedSAC exhibits a distinct advantage over baseline methods in
terms of fairness and accuracy. In the future, we aim to investigate
the implementation of FedSAC on large models.
7 ACKNOWLEDGMENTS
The research was supported by Natural Science Foundation of China
(62272403).
3307KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
REFERENCES
[1]Anish Agarwal et al .2019. A marketplace for data: An algorithmic solution. In
Proceedings of the 2019 ACM Conference on Economics and Computation. 701–726.
[2]Chen Chen et al .2022. Gear: a margin-based federated adversarial training ap-
proach. In International Workshop on Trustable, Verifiable, and Auditable Federated
Learning in Conjunction with AAAI, Vol. 2022.
[3]Dengsheng Chen, Jie Hu, Vince Junkai Tan, Xiaoming Wei, and Enhua Wu. 2023.
Elastic Aggregation for Federated Optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). 12187–12197.
[4]Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu.
2022. Feddc: Federated learning with non-iid data via local drift decoupling and
correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 10112–10121.
[5]Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. 2022. Efficient
Split-Mix Federated Learning for On-Demand and In-Situ Customization. In In
Proceedings of the International Conference on Learning Representations.
[6]Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos
Venieris, and Nicholas Lane. 2021. Fjord: Fair and accurate federated learning un-
der heterogeneous targets with ordered dropout. Advances in Neural Information
Processing Systems 34 (2021), 12876–12889.
[7]Wenke Huang, Mang Ye, and Bo Du. 2022. Learn From Others and Be Yourself in
Heterogeneous Federated Learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 10143–10153.
[8]Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. Master’s thesis, Department of Computer Science, University of
Toronto, 2009. (2009).
[9]Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[10] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. 2021. Fed-
mask: Joint computation and communication-efficient personalized federated
learning via heterogeneous masking. In Proceedings of the 19th ACM Conference
on Embedded Networked Sensor Systems. 42–55.
[11] Bo Li, Mikkel N. Schmidt, Tommy S. Alstrøm, and Sebastian U. Stich. 2023. On
the Effectiveness of Partial Variance Reduction in Federated Learning With
Heterogeneous Data. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 3964–3973.
[12] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine learning and systems 2 (2020), 429–450.
[13] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. 2019. Fair Re-
source Allocation in Federated Learning. In International Conference on Learning
Representations.
[14] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.
On the Convergence of FedAvg on Non-IID Data. In International Conference on
Learning Representations.
[15] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensemble
distillation for robust model fusion in federated learning. Advances in Neural
Information Processing Systems 33 (2020), 2351–2363.
[16] Jian-Hao Luo et al .2017. Thinet: A filter level pruning method for deep neural
network compression. In Proceedings of the IEEE international conference on
computer vision. 5058–5066.
[17] Lingjuan Lyu, Yitong Li, Karthik Nandakumar, Jiangshan Yu, and Xingjun Ma.
2020. How to democratise and protect AI: Fair and differentially private decen-
tralised deep learning. IEEE Transactions on Dependable and Secure Computing
(2020).
[18] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. 2020. Collaborative fairness in
federated learning. In Federated Learning. Springer, 189–204.
[19] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial Intelligence and Statistics . PMLR,
1273–1282.
[20] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019.
Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 11264–11272.
[21] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y Ng. 2011. Reading digits in natural images with unsupervised feature
learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011 (2011).
[22] Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,
Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Mar-
foq, Erum Mushtaq, et al .2022. FLamby: Datasets and Benchmarks for Cross-Silo
Federated Learning in Realistic Healthcare Settings. Advances in Neural Informa-
tion Processing Systems 35 (2022), 5315–5334.
[23] Adam Richardson et al .2020. Budget-bounded incentives for federated learning.
InFederated Learning. Springer, 176–188.[24] Yuxin Shi, Han Yu, and Cyril Leung. 2023. Towards fairness-aware federated
learning. IEEE Transactions on Neural Networks and Learning Systems (2023).
[25] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan
Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware
model rewards. In International Conference on Machine Learning. PMLR, 8927–
8936.
[26] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. Sparsified
SGD with memory. Advances in Neural Information Processing Systems 31 (2018).
[27] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.
2023. Federated learning on non-iid graphs via structural knowledge sharing. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 9953–9961.
[28] Sebastian Shenghong Tay, Xinyi Xu, Chuan Sheng Foo, and Bryan Kian Hsiang
Low. 2022. Incentivizing collaboration in machine learning via synthetic data
rewards. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.
9448–9456.
[29] Md Palash Uddin, Yong Xiang, Xuequan Lu, John Yearwood, and Longxiang Gao.
2020. Mutual information driven federated learning. IEEE Transactions on Parallel
and Distributed Systems 32, 7 (2020), 1526–1538.
[30] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. 2020. A
principled approach to data valuation for federated learning. Federated Learning:
Privacy and Incentive (2020), 153–167.
[31] Zihui Wang, Zhaopeng Peng, Xiaoliang Fan, Zheng Wang, Shangbin Wu, Rong-
shan Yu, Peizhen Yang, Chuanpan Zheng, and Cheng Wang. 2024. FedAVE:
Adaptive data value evaluation framework for collaborative fairness in federated
learning. Neurocomputing (2024), 127227.
[32] Xinyi Xu and Lingjuan Lyu. 2020. A reputation mechanism is all you need:
Collaborative fairness and adversarial robustness in federated learning. arXiv
preprint arXiv:2011.10464 (2020).
[33] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and
Bryan Kian Hsiang Low. 2021. Gradient driven rewards to guarantee fairness
in collaborative machine learning. Advances in Neural Information Processing
Systems 34 (2021), 16104–16117.
[34] Yuan-Yi Xu, Ci-Siang Lin, and Yu-Chiang Frank Wang. 2023. Bias-Eliminating
Augmentation Learning for Debiased Federated Learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 20442–
20452.
[35] Gang Yan, Hao Wang, Xu Yuan, and Jian Li. 2023. Criticalfl: A critical learning
periods augmented client selection framework for efficient federated learning.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2898–2907.
[36] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1–19.
[37] Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit
Niyato, and Qiang Yang. 2020. A fairness-aware incentive scheme for federated
learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.
393–399.
[38] Hao Yu, Sen Yang, and Shenghuo Zhu. 2019. Parallel restarted SGD with faster
convergence and less communication: Demystifying why model averaging works
for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 5693–5700.
[39] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han,
Mingfei Gao, Ching-Yung Lin, and Larry S Davis. 2018. Nisp: Pruning networks
using neuron importance score propagation. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 9194–9203.
[40] Yaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and Michael
Jordan. 2022. TCT: Convexifying federated learning using bootstrapped neural
tangent kernels. Advances in Neural Information Processing Systems 35 (2022),
30882–30897.
[41] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald,
Nghia Hoang, and Yasaman Khazaeni. 2019. Bayesian nonparametric federated
learning of neural networks. In International Conference on Machine Learning.
PMLR, 7252–7261.
[42] Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. 2021. A
survey of incentive mechanism design for federated learning. IEEE Transactions
on Emerging Topics in Computing (2021).
[43] Jingwen Zhang, Yuezhou Wu, and Rong Pan. 2021. Incentive mechanism for hori-
zontal federated learning based on reputation and reverse auction. In Proceedings
of the Web Conference 2021. 947–956.
[44] Yuchen Zhang, Martin J Wainwright, and John C Duchi. 2012. Communication-
efficient algorithms for statistical optimization. Advances in neural information
processing systems 25 (2012).
3308FedSAC: Dynamic Submodel Allocation for Collaborative Fairness
in Federated Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
A. PROOF OF THEOREM 2
Let𝐼𝐸be the set of global synchronization steps, i.e., 𝐼𝐸={𝑛𝐸|𝑛=
1,2,...}. For convenience, We define 𝑣𝑡+1
𝑖as the immediate result of
one step SGD update from 𝜃𝑡
𝑖, i.e.,𝑣𝑡+1
𝑖=𝜃𝑡
𝑖−𝜂𝑡∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖).¯𝑔𝑡=
Í𝑁
𝑖=1∇𝐹𝑖(𝜃𝑡
𝑖)
𝑝𝑖and𝑔𝑡=Í𝑁
𝑖=1∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)
𝑝𝑖. Therefore, ¯𝑣𝑡+1=¯𝜃𝑡−𝜂𝑡𝑔𝑡
and𝐸𝑔𝑡=¯𝑔𝑡.
Lemma 3. (Result of one step SGD). Assume ASSUMPTION 1 and
ASSUMPTION 2. If 𝜂𝑡⩽1
4𝐿, we have
𝐸∥¯𝑣𝑡+1−𝜃∗∥2⩽(1−𝜂𝑡𝜇)𝐸∥¯𝜃𝑡−𝜃∗∥2+𝜂2
𝑡𝐸∥𝑔𝑡−¯𝑔𝑡∥2
+6𝐿𝜂2
𝑡Γ+2𝐸𝑁∑︁
𝑖=1∥¯𝜃𝑡−𝜃𝑡
𝑖∥2
𝑝𝑖,(20)
where Γ=𝐹∗−Í𝑁
𝑖=1𝐹∗
𝑖
𝑝𝑖. LAMMA 3 has been made by [14].
For Assumption 3, the variance of stochastic gradients within
device𝑖is constrained by 𝜎2
𝑖. Consequently,
𝐸∥𝑔𝑡−¯𝑔𝑡∥2=𝐸∥𝑁∑︁
𝑖=11
𝑝𝑖(∇𝐹𝑘(𝜃𝑡
𝑖,𝜉𝑡
𝑖)−∇𝐹𝑖(𝜃𝑡
𝑖))∥2
=𝑁∑︁
𝑖=11
𝑝2
𝑖𝐸∥∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)−∇𝐹𝑖(𝜃𝑡
𝑖)∥2
⩽𝑁∑︁
𝑖=11
𝑝2
𝑖𝜎2
𝑖.(21)
As FedSAC requires communication each 𝐸steps. We let 𝜂𝑡⩽
2𝜂𝑡+𝐸. Therefore, for any 𝑡≥0, there exists a 𝑡0⩽𝑡, such that
𝑡−𝑡0⩽𝐸−1and𝜃𝑡0
𝑖=¯𝜃𝑡0for all𝑘=1,2,...,𝑁 . Then
𝐸𝑁∑︁
𝑖=11
𝑝𝑖∥¯𝜃𝑡−𝜃𝑡
𝑖∥2=𝐸𝑁∑︁
𝑖=11
𝑝𝑖∥(𝜃𝑡
𝑖−¯𝜃𝑡0)−( ¯𝜃𝑡−¯𝜃𝑡0)∥2
⩽𝐸𝑁∑︁
𝑖=11
𝑝𝑖∥𝜃𝑡
𝑖−¯𝜃𝑡0∥2
⩽𝐸𝑡−1∑︁
𝑡=𝑡0(𝐸−1)𝜂2
𝑡∥∇𝐹𝑘(𝜃𝑡
𝑖,𝜉𝑡
𝑖)∥2
⩽𝑡−1∑︁
𝑡=𝑡0(𝐸−1)𝜂2
𝑡0𝐺2
⩽𝜂2
𝑡0(𝐸−1)2𝐺2
⩽4𝜂2
𝑡(𝐸−1)2𝐺2.(22)
Here in lines 1252-1256, we use 𝐸∥𝑋−𝐸𝑋∥2⩽𝐸∥𝑋∥2where
𝑋=𝜃𝑡
𝑖−¯𝜃𝑡0with probability1
𝑝𝑖. In the lines 1256-1259, we use
Jensen inequality:
∥𝜃𝑡
𝑖−¯𝜃𝑡0∥=∥𝑡−1∑︁
𝑡=𝑡0𝜂𝑡∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)∥2
⩽(𝑡−𝑡0)∑︁
𝑡−𝑡0𝑡−1∑︁
𝑡−𝑡0𝜂2
𝑡∥∇𝐹𝑖(𝜃𝑡
𝑖,𝜉𝑡
𝑖)∥2.(23)In lines 1259-1262, we utilize 𝜂𝑡⩽𝜂𝑡0for𝑡≥𝑡0and𝐸∥∇𝐹𝑘(𝜃𝑡
𝑖,𝜉𝑡
𝑖)∥2
⩽𝐺2for𝑖=1,2,...,𝑁 . In the lines 1263-1265, we use 𝜂𝑡0⩽
2𝜂𝑡0+𝐸⩽2𝜂𝑡for𝑡0⩽𝑡⩽𝑡0+𝐸.
Let△𝑡=𝐸∥¯𝜃𝑡−𝜃∗∥. From Eq.(20), Eq. (21), and Eq. (22), it
follows that
△𝑡+1⩽(1−𝜂𝑡𝜇)△𝑡+𝜂2
𝑡𝑁∑︁
𝑖=1𝜎2
𝑝2
𝑖+6𝐿𝜂2
𝑡Γ+8𝜂2
𝑡(𝐸−1)2𝐺2
⩽(1−𝜂𝑡𝜇)△𝑡+𝜂2
𝑡(𝑁∑︁
𝑖=1𝜎2
𝑝2
𝑖+6𝐿Γ+8(𝐸−1)2𝐺2)
|                                  {z                                  }
𝐵(24)
For a diminishing stepsize, 𝜂𝑡=𝜅
𝑡+𝛾for some𝜅>1
𝜇and𝛾>0
such that𝜂1⩽𝑚𝑖𝑛{1
𝜇,1
4𝐿}=1
4𝐿and𝜂𝑡⩽2𝜂𝑡+𝐸. We will prove
△⩽𝑣
𝛾+𝑡by induction, where 𝑣=𝑚𝑎𝑥{𝜅2𝐵
𝜅𝜇−1,(𝛾+1)△1}. Firstly,
the definition of 𝑣guarantees its applicability for 𝑡=1. Assuming
the conclusion holds for some 𝑡, it follows that
△𝑡+1⩽(1−𝜂𝑡𝜇)△𝑡+𝜂2
𝑡𝐵
⩽(1−𝜅𝜇
𝑡+𝛾)𝑣
𝑡+𝛾+𝜅2𝐵
(𝑡+𝛾)2
=𝑡+𝛾−1
(𝑡+𝛾)2𝑣+[𝜅2𝐵
(𝑡+𝛾)2−𝜅𝜇−1
(𝑡+𝛾)2𝑣]
⩽𝑡+𝛾−1
(𝑡+𝛾)2𝑣+𝜅2𝐵
(𝑡+𝛾)2−𝜅2𝐵
(𝑡+𝛾)2−𝜅𝜇−1
(𝑡+𝛾)2(𝛾+1)△1|                   {z                   }
⩽0
⩽𝑣
𝑡+𝛾−1(25)
Then by the 𝐿-smoothness (ASSUMPTION 1) of 𝐹,
𝐸[𝐹(¯𝜃𝑇)]−𝐹∗⩽(¯𝜃𝑇−𝜃∗)𝑇∇𝐹𝑖(𝜃∗)
|   {z   }
=0+𝐿
2∥¯𝜃𝑇−𝜃∗∥2
2
=𝐿
2△𝑇
⩽𝐿
2𝑣
𝛾+𝑇(26)
We let𝜅=2
𝜇,𝛾=𝑚𝑎𝑥{8𝐿
𝜇,𝐸}−1. In the lines 1293, we have
𝑣=𝑚𝑎𝑥{𝜅2𝐵
𝜅𝜇−1,(𝛾+1)△1}
⩽𝜅2𝐵
𝜅𝜇−1+(𝛾+1)△1
⩽4𝐵
𝜇2+(𝛾+1)△1(27)
Substituting Eq. 27 into Eq. 26, we obtain
0⩽lim
𝑇→∞𝐸[𝐹(¯𝜃𝑇)]−𝐹∗⩽lim
𝑇→∞[𝐿
𝛾+𝑇(2𝐵
𝜇2+𝛾+1
2△1)]=0
(28)
Therefore, lim
𝑇→∞𝐸[𝐹(¯𝜃𝑇)]−𝐹∗=0.
3309KDD ’24, August 25–29, 2024, Barcelona, Spain Zihui Wang et al.
Dataset CIF
AR10 Fashion
MNIST
No
. Clients 20 40 60 20
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) DIR(1.0)
DIR(2.0) DIR(1.0)
DIR(2.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Fe
dAvg[19] -40.52±3.3
83.57±1.4 18.41±16.3 77.52±1.5 44.13±16.0 79.53±13.9
54.37±11.1 51.64±17.8
55.22±6.8 -34.64±5.6
84.37±3.3 22.95±2.6 30.39±6.7 56.37±3.5
q-FFL[13] 14.10±2.8
98.09±0.2
83.98±2.0 86.14±2.9
90.00±0.5 87.81±1.6
80.46±1.1 80.65±2.9
85.76±1.2 29.24±5.3
98.44±2.6
81.75±4.8 77.45±5.6 72.89±3.9
CFFL[18] 81.45±1.0
95.93±0.8 76.72±2.1 76.09±1.3 63.79 ±0.5 50.33±1.2
49.61±0.6 86.59±1.3
87.52±0.5 88.02±0.4
92.29±2.5 72.74±2.4 78.40±1.2 75.36 ±1.6
CGSV[33] 83.30±1.8
96.80±0.1 79.85±0.9 79.73±1.3 85.72 ±0.4 82.90±0.4
80.47±0.7 85.91±1.2
77.91±0.8 83.85±0.4
94.04±0.8 88.32±2.5
83.68±1.1
74.15±1.7
FedAVE[31] 88.46±1.5
97.18±0.6 87.59±1.1
78.43±0.4 68.70±1.3 45.69±2.6
65.60±1.2 38.92±1.1
60.61±1.3 87.14±0.6
93.97±1.2 78.87±2.1 81.20±1.5 72.73±0.7
Ours 99.62±0.2 98.52±0.1 96.29±0.4 98.06±0.5 95.99±0.4 99.37±0.5 96.57±0.2 97.69±0.6 95.12±0.3 97.40±0.4 98.49±0.3 96.52±0.8 97.65±0.6 96.47±0.7
Table 5: Comparison results of fairness 𝜌∈[− 100,100]with state-of-the-art methods on CIFAR10 and Fashion MNIST. The
reported results are averaged over 5 runs with different random seeds. (A higher value indicates better fairness. The best
average result is marked in bold. The second-best result is underlined. These notes are the same to others.)
Dataset CIF
AR10 Fashion
MNIST
No
. Clients 20 40 60 20
Scene PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0) DIR(1.0)
DIR(2.0) DIR(1.0)
DIR(2.0) PO
W CLA DIR(1.0) DIR(2.0) DIR(3.0)
Standalone 37.15±0.0
35.25±0.3 27.82±0.0 31.83±0.1 33.56±0.1 32.13±0.1
28.86±0.2 28.58±0.2
27.84±0.1 82.36±0.1
81.17±0.2 66.71±0.2 68.00±0.2 73.63±0.3
FedAvg[19] 46.94±0.2
41.14±0.6 48.31±0.5
49.00±0.5
50.13±0.3 48.19±0.3
49.57±0.3 49.06±0.1
49.30±0.3 87.29±0.3
84.67±0.2 87.46±0.0
88.06±0.1
88.04±0.1
q-FFL[13] 46.87±0.2
41.56±0.3
33.77±0.1 38.17±0.9 43.91±0.6 38.64±1.5
39.37±0.4 34.01±0.1
36.77±0.5 85.79±0.2
81.10±0.1 68.30±1.7 78.59±1.8 77.69±1.1
CFFL[18] 46.06±0.0
39.43±0.3 45.76±0.3 48.57±0.2 48.42±0.3 39.54±0.3
39.16±0.6 42.33±0.4
41.06±0.6 85.88±1.8
81.75±2.1 81.69±0.6 84.16±0.4 87.34±0.4
CGSV[33] 46.29±0.2
37.75±1.4 46.72±1.0 48.45±0.3 49.24±0.2 46.75±0.2
48.32±0.1 49.11±0.7
48.58±0.3 87.21±0.2
84.25±0.2 85.17±0.7 86.85±0.3 88.07 ±0.2
Fe
dAVE[31] 46.43±0.6
40.99±0.2 46.64±0.6 48.34±0.5 48.13±0.6 46.58±0.4
46.73±0.3 47.29±0.8
49.21±0.6 87.44±0.1
84.79±0.5
79.53±0.9 84.47±0.4 86.83±0.1
Ours 48.60±0.7 43.39±0.2 49.41±0.1 49.09±0.1 50.48±0.1 48.64±0.3 49.68±0.4 49.23±0.1 49.34±0.2 87.60±0.1 84.99±0.3 87.57±0.6 88.08±0.3 88.17±0.3
Table 6: Comparison results of the maximum test accuracy (%) with state-of-the-art methods on CIFAR10 and Fashion MNIST.
The reported results are averaged over 5 runs with different random seeds. (A higher value indicates better accuracy.)
00.20.40.6
12345678910Accuracya. CIFAR10-POW
Contributionq-FFLCFFLCGSVFedAVEFedSAC
00.20.40.6
12345678910Accuracye. CIFAR10-DIR(2.0)
Contributionq-FFLCFFLCGSVFedSAEFedSAC00.20.40.60.8
12345678910Accuracyb. SVHN-POW
Contributionq-FFLCFFLCGSVFedAVEFedSAC
00.20.40.60.8
12345678910Accuracyb. SVHN-CLA
Contribution q-FFL CFFL CGSV FedAVE FedSAC
00.20.40.60.8
1 2 3 4 5 6 7 8 9 10Accuracyb. SVHN -DIR(2.0)
Contribution q-FFL CFFL CGSV FedAVE FedSAC00.20.40.6
1 2 3 4 5 6 7 8 9 10Accuracyc. CIFAR10 -CLA
Contribution q-FFL CFFL CGSV FedAVE FedSAC
Figure 6: Comparison results of test accuracy using the scene
of POW and CLA with state-of-the-art methods in CIFAR10
(left) and SVHN (right). FedSAC exhibits the highest level of
consistency with the contribution.
B. FINAL REWARDS OF CLIENTS
Figure 6 shows the distributions of contributions and the final
rewards of clients under different scenes (i.e., POW and CLA in CI-
FAR10 and SVHN) by FedSAC and the compared methods. FedSAC
effectively differentiates the rewards it received, thereby ensuring
the collaborative fairness in FL.
C. VARYING NUMBERS OF CLIENTS
To verify the effectiveness of FedSAC in scenarios with varying
numbers of clients, we conduct experiments by increasing the num-
ber of local clients to 20, 40, and 60, respectively. Table 5 presentsScene PO
W DIR(1.0) DIR(2.0) DIR(3.0)
𝐹
𝑒𝑑𝑆𝐴𝐶(𝛽=2) 49.23(44.77)
49.66(39.53) 50.60(37.87) 51.37(43.19)
𝐹𝑒𝑑𝑆𝐴𝐶(𝛽=5) 48.43(37.73)
49.10(35.76) 50.01(30.91) 49.85(38.46)
𝐹𝑒𝑑𝑆𝐴𝐶(𝛽=10) 48.61(30.91)
49.06(31.36) 47.08(23.72) 49.04(28.54)
𝐹𝑒𝑑𝑆𝐴𝐶(𝛽=20) 46.53(24.10)
46.98(28.41) 43.50(20.17) 46.03(25.70)
Table 7: The maximum test accuracy (%) achieved by FedSAC
across different 𝛽, given a fairness threshold of 𝜌>95%,
on CIFAR10. Values in the middle brackets represent the
minimum test accuracy (%) among 10 clients.
the fairness results achieved by FedSAC and the compared meth-
ods, while Table 6 shows the maximum local model performance
achieved by these methods. In the large client number settings (No.
Clients = 20, 40, and 60), FedSAC outperforms all baseline methods
in terms of fairness (refer to Table 5) and accuracy (refer to Table 6).
The results demonstrate that FedSAC can effectively implement
BCF in scenarios with varying numbers of clients.
D. THE IMPACT OF 𝛽ON THE EXPERIMENT
In Table 7, we present the performance of FedSAC with different
values of𝛽on each scene of CIFAR10. The experiments demon-
strate that as 𝛽increases, the maximum test accuracy will gradually
decrease. This is because the size of the submodels downloaded by
clients increases as 𝛽decreases. When 𝛽is small, the submodels of
low-contribution clients contain more neurons, enabling effective
training to enhance all local model performance.
3310