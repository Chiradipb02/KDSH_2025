Hyper-Local Deformable Transformers for Text Spotting on
Historical Maps
Yijun Lin∗
University of Minnesota, Twin Cities
Minneapolis, MN, USA
lin00786@umn.eduYao-Yi Chiang
University of Minnesota, Twin Cities
Minneapolis, MN, USA
yaoyi@umn.edu
Abstract
Text on historical maps contains valuable information providing
georeferenced historical, political, and cultural contexts. However,
text extraction from historical maps has been challenging due to
the lack of (1) effective methods and (2) training data. Previous
approaches use ad-hoc steps tailored to only specific map styles.
Recent machine learning-based text spotters (e.g., for scene images)
have the potential to solve these challenges because of their flexi-
bility in supporting various types of text instances. However, these
methods remain challenges in extracting precise image features for
predicting every sub-component (boundary points and characters)
in a text instance. This is critical because map text can be lengthy
and highly rotated with complex backgrounds, posing difficulties
in detecting relevant image features from a rough text region. This
paper proposes PaLeTTe, an end-to-end text spotter for scanned
historical maps of a wide variety. PaLeTTe introduces a novel hyper-
local sampling module to explicitly learn localized image features
around the target boundary points and characters of a text instance
for detection and recognition. PaLeTTe also enables hyper-local po-
sitional embeddings to learn spatial interactions between boundary
points and characters within and across text instances. In addition,
this paper presents a novel approach to automatically generate
synthetic map images, SynthMap+, for training text spotters for
historical maps. The experiment shows that PaLeTTe with Syn-
thMap+ outperforms SOTA text spotters on two new benchmark
datasets of historical maps, particularly for long and angled text. We
have deployed PaLeTTe with SynthMap+ to process over 60,000
maps in the David Rumsey Historical Map collection and generated
over 100 million text labels to support map searching.
CCS Concepts
•Applied computing →Document management and text
processing; •Information systems →Digital libraries and
archives.
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671589Keywords
historical maps; text detection and recognition; text spotting; syn-
thetic map data
ACM Reference Format:
Yijun Lin and Yao-Yi Chiang. 2024. Hyper-Local Deformable Transformers
for Text Spotting on Historical Maps. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671589
1 Introduction
Detecting and recognizing text from scanned documents is essential
for effectively managing them and facilitating analysis and acces-
sibility to their content. This is particularly important for histori-
cal and cultural documents, such as historical manuscripts, books,
building & infrastructure plans, engineering drawings, handwritten
records, and maps. Historical maps, for example, offer valuable in-
formation about how territories, landscapes, and civilizations have
evolved over time, which are extremely crucial resources required
in various scientific domains, including humanity, geography, urban
planning, cartography, and environmental science [4, 5, 7].
This paper focuses on text spotting on scanned historical maps,
an important data source having wide varieties (e.g., in cartographic
designs and scanned quality) but lacking (1) effective approaches,
(2)text labels for training, and (3) benchmark datasets. Text on his-
torical maps, including names of places, labels for physical features
(e.g., building types), and descriptions of areas or landmarks, pro-
vides unique insights on georeferenced historical, political, and
cultural contexts [ 5,7]. For example, text on historical geological
maps indicates locations of rock formations, faults, folds, and min-
eral deposits, for which digital versions of this information are not
available at the scale and coverage of the historical maps.1
Numerous types of scanned historical maps are accessible on-
line, such as the US Geological Survey topographic maps [ 10], the
Sanborn maps [ 25], the Ordnance Survey maps [ 14], and a vast
range of maps in the David Rumsey Historical Map Collection [ 42].
Accessing desired scanned historical maps typically requires search-
ing through map metadata, which are often limited or unavailable.
Generating comprehensive metadata for individual map scans re-
quires expert knowledge and extensive manual work. Therefore,
accurately and automatically extracting text from scanned histori-
cal maps is crucial for enhancing their searchability and facilitating
the generation of map metadata [4].
Previous attempts to extract map text involve ad-hoc steps to tai-
lor and finetune existing text detection and recognition approaches
1See the recent AI for Critical Mineral Assessment Competition from DARPA and
USGS: https://criticalminerals.darpa.mil/.
5387
KDD ’24, August 25–29, 2024, Barcelona, Spain Yijun Lin & Yao-Yi Chiang
for a specific map style [ 1,6,27] or rely on external place name
databases [ 29,50]. Recent text spotting approaches have shown
promising results on benchmark datasets (mostly scene images),
indicating their potential for historical maps. One category of these
methods requires a careful design of the region of interest (RoI)
operation to extract image features within the detected regions
for recognition [ 12,32–34]. Another category needs additional
post-processing to group the results, such as segmentation-based
approaches [ 15,31,48]. These methods are either poor in recog-
nition due to imprecise detected regions or difficult to generalize
for text instances with wide varieties of curvatures, sizes, spacings,
orientations, and placement, e.g., historical map text.
The third category mitigates these issues by detecting and recog-
nizing text in parallel, for example, utilizing a deformable DETR [ 55]
with a separate character decoder on top of the boundary point
decoder [ 53,54]. However, there are two remaining challenges
in handling lengthy, curved, and highly rotated text on historical
maps. First, deformable DETR and its variant spotting models learn
image features for detection and recognition around a coarse ref-
erence point that roughly indicates the position of a text instance
(e.g., the instance center). This works well for object detection, for
which the networks predict one object category for a detected ob-
ject. However, text spotting requires further prediction on multiple
sub-components (boundary points and characters) of a detected ob-
ject (text instance). The reference point(s) and, consequentially, the
sampled image features can be misaligned with the target bound-
ary points or characters, especially for text instances that are long,
winding, or with extra-large character spacing. For example, Fig-
ure 1(a) shows that the deformable DETR-based text spotter, TESTR,
samples features from a fixed location (the text center as •) that can
be distant from the target characters. Second, all boundary points
and characters of a text instance share the same position informa-
tion (e.g., the text center coordinate), which cannot well capture the
inter- and intra- spatial relations between these sub-components
within and across text instances.
Furthermore, training these machine learning models requires
a large number of text labels, while creating text annotations re-
quires extensive manual work. Although there are existing synthetic
datasets [ 18,49] that are inexpensive and scalable alternatives and
provide ground-truth annotations, they are typically scene images
that differ significantly from historical maps in terms of text and
background styles. Thus, the lack of text labels for training is still a
challenge for text spotting on historical maps.
This paper presents PaLeTTe2
with SynthMap+ for extract-
ing text from historical maps.3PaLeTTe is an end-to-end text spot-
ter for historical maps with text instances of arbitrary shapes and
can be long & highly-rotated. The main idea of PaLeTTe is to explic-
itly model the individual boundary points and characters of a text
instance as the target objects for spotting. PaLeTTe progressively
refines the position of boundary points and character centers and
uses them as (1) the new reference points to sample image features
around the target sub-components, i.e., hyper-local sampling (see
Figrue 1(b)), and (2) position information to describe the inter- and
intra- spatial relations between sub-components, i.e., hyper-local
2PaLeTTe refers to hyPer-locAL dEformable Transformers for TExt spotting
3Code & Data: https://github.com/knowledge-computing/mapkurator-palette
(a) TESTR’s sampling locations ( •) are around the text center ( •) for predicting the
first character (left) and the last character (right)
(b)PaLeTTe uses character locations ( •) to sample hyper-local features ( •) for predict-
ing the first character (left) and the last character (right)
Figure 1: Comparison between reference points ( •) and sam-
pling locations (•) in TESTR and PaLeTTe when querying
the first character (left figure) and the last character (right
figure) . Darker blue dots have higher attention scores.
positional embeddings. In addition, we propose SynthMap+, a syn-
thetic dataset of historical-styled map images to facilitate training
text spotters for historical maps. SynthMap+ follows cartographic
rules to place text labels of various styles and integrates with the
background extracted from real historical maps.
We have deployed PaLeTTe with SynthMap+ in the mapKurator
system [ 21], a complete pipeline for extracting and linking text
from historical maps. The system has processed over 60,000 maps
in the David Rumsey Historical Map collection and the extracted
text labels have been incorporated into the metadata platform to
support map searching by Luna Imaging.4The main contributions
of the paper include the following,
•We propose PaLeTTe that introduces (1) hyper-local sampling to
learn precise image features for predicting boundary points and
characters, and (2) hyper-local position embeddings to capture
spatial relations between boundary points and characters within
and across text instances.
•We propose an approach that automatically generates SynthMap+
of various styles with text labels for training text spotters for his-
torical maps. Additionally, we introduce a new, highly diversified
benchmark dataset for evaluating text spotting performance on
real historical maps.
•PaLeTTe with SynthMap+ is the first text spotter capable of
handling a wide variety of scanned historical maps, thus opening
up a vast asset of valuable information in tens of thousands of
historical maps.
4Search by Text-on-Maps: https://mailchi.mp/stanford/apr2023-ai-advancements-in-
map-studies
5388Hyper-Local Deformable Transformers for Text Spotting on Historical Maps KDD ’24, August 25–29, 2024, Barcelona, Spain
2PaLeTTe
Figure 2 shows the network architecture of PaLeTTe. PaLeTTe
uses a CNN backbone followed by a Deformable Transformer en-
coder to extract multi-scale feature maps from the input image.
PaLeTTe uses these image features to identify the top 𝑄box pro-
posals indicating the approximate locations of text instances. In
the decoder, the first layer uses the proposal centers as the initial
reference points to sample the image features for predicting bound-
ary points. Then, a character center predictor takes the detected
boundary points and character information (initialized as zeros) as
input to predict character centers, serving as the reference points
for recognizing characters. The rest of the layers iteratively update
the position of the boundary points and character centers as the
new reference points to sample localized image features for text
detection and recognition.
Figure 2: The network architecture of PaLeTTe (top). The
Hyper-Local Deformable Transformer decoder contains mul-
tiple layers (bottom). PaLeTTe progressively refines the con-
tent queries of boundary points and characters, and their
reference points layer by layer.
2.1 Deformable Attention in Text Spotting
DETR [ 3] is known for its high accuracy in object detection tasks.
However, DETR computes the attention weight between every pair
of features in the entire feature maps, which can include feature
information that is not related to the target query and is compu-
tationally expensive. Deformable DETR [ 55] addresses this issueby using a data-dependent sparse attention mechanism that aims
to sample only relevant features for attention. Specifically, given
a set of𝐿-level multi-scale feature maps [ 13], denoted as{x𝑙}𝐿
𝑙=1,
where each level x𝑙∈R𝐶×𝐻𝑙×𝑊𝑙. Let p(𝑞)be the normalized coor-
dinates of the reference point to sample image features for a content
query𝑞. The multi-scale deformable attention works as,
MSDeformAttn(𝑞,p(𝑞),{x𝑙}𝐿
𝑙=1)=
𝐻∑︁
ℎ=1𝑊ℎ{𝐿∑︁
𝑙=1𝐾∑︁
𝑘=1𝐴ℎ𝑙𝑘(𝑞)·𝑊′
ℎx𝑙[𝜙𝑙(p(𝑞))+Δpℎ𝑙𝑘(𝑞)]}(1)
whereℎ,𝑙,𝑘refer to the indices of the attention head, the feature
level, and the sample points. 𝐴ℎ𝑙𝑘denotes the attention weight for
query𝑞, normalized across 𝐿×𝐾sample points. The function 𝜙𝑙
rescales the normalized coordinates p(𝑞)to the𝑙-th level feature
map and Δpcomputes the sample offset from p(𝑞)for the content
query, and their sum decides the sampling locations.
Using Deformable DETR for text spotting [ 53,54] typically starts
with a proposal generator to generate top 𝑄box proposals, repre-
sented as{(𝑥,𝑦,𝑤,ℎ)𝑖}𝑄
𝑖=1, where(𝑥,𝑦)are the center coordinates
and(𝑤,ℎ)are the box scale. The centers of these proposals serve as
reference points in the decoder. The decoder samples image features
from reference points with learned offsets and refines the content
queries to predict boundary points and characters layer by layer.
2.2 Hyper-Local Deformable Transformer
For a target instance, sharing the reference point (e.g., using the
proposal center) across all content queries has a limitation in that
the reference point indicates the coarse position of a text instance
and is not closely related to its sub-components, i.e., the 𝑁bound-
ary points and 𝑀characters.5This will lead to imprecise posi-
tion information and sample locations for extracting image fea-
tures. To overcome this issue, we propose hyper-local sampling that
generates features from localized reference points and hyper-local
positional embedding that indicates the precise location for each
sub-component of the instance.
For a proposal 𝑖,PaLeTTe initializes its reference points for the
boundary point content queries (𝑞(𝑖,1)
𝑛,...,𝑞(𝑖,𝑁)
𝑛)using the pro-
posal center coordinates (𝑥,𝑦). The positional embeddings of the
boundary point content queries are also initially identical, 𝜑((𝑥,𝑦)),
where𝜑is a sinusoidal positional encoding function. The 𝑁bound-
ary point composite queries for instance 𝑖are formulated as,
{ˆ𝑞(𝑖,𝑗)
𝑛}𝑁
𝑗=1={𝑞(𝑖,𝑗)
𝑛+𝜑((𝑥,𝑦)(𝑖))}𝑁
𝑗=1
In Figure 2 1, the boundary point decoder layer leverages de-
formable attention to learn feature maps from sampled image fea-
tures around the reference points (Eq. 1). The output represents
the updated information about the boundary points by looking at
the multi-scale image features. PaLeTTe further refines the out-
put by adding the positional embeddings again and conducting
self-attention between the boundary points within an instance and
between instances. This step enables the boundary points to adjust
their content in accordance with each other.
5We use∗𝑛as the symbols related to boundary points and ∗𝑚for characters.
5389KDD ’24, August 25–29, 2024, Barcelona, Spain Yijun Lin & Yao-Yi Chiang
The final output becomes the new boundary point content queries
for predicting the boundary points, (¯𝑥(𝑗)
𝑛,¯𝑦(𝑗)
𝑛)𝑁
𝑗=1. Then PaLeTTe
updates the reference points using the predicted boundary points,
aiming to move the sampling locations close to the target bound-
ary points for hyper-local sampling in the next layers. In addition,
PaLeTTe computes the new positional embeddings from the pre-
dicted boundary points, enabling hyper-local position information.
The new composite queries are the addition of the updated content
queries and the new positional embeddings:
{ˆ𝑞(𝑖,𝑗)
𝑛}𝑁
𝑗=1={𝑞(𝑖,𝑗)
𝑛+𝜑((¯𝑥(𝑖,𝑗)
𝑛,¯𝑦(𝑖,𝑗)
𝑛))}𝑁
𝑗=1
In this way, PaLeTTe progressively refines the boundary points,
consequentially, the reference points and positional embeddings
towards the target boundary points layer by layer to learn localized
and precise image features for text detection.
PaLeTTe also applies the hyper-local strategy to the character
decoder layer in Figure 2 3. To retrieve localized character in-
formation, PaLeTTe predicts the center of each character as the
reference point, indicating the precise position of the characters.
Specifically, for a proposal 𝑖,PaLeTTe computes the composite
queries for𝑀characters by adding the character content queries,
(𝑞(𝑖,1)
𝑚,...,𝑞(𝑖,𝑀)
𝑚 ), and their positional embeddings of the charac-
ter centers,(¯𝑥(𝑖,𝑗)
𝑚,¯𝑦(𝑖,𝑗)
𝑚)𝑀
𝑗=1. Then PaLeTTe samples hyper-local
image features around the character centers for queries using defor-
mation attention. PaLeTTe further refines the output by adding the
positional embeddings of the character center locations and adopt-
ing self-attention between the characters within a text instance.
This enables PaLeTTe to capture the relationships between char-
acters based on their content and locations. The output is used to
update the character content queries and predict characters. Here,
PaLeTTe introduces a new module to predict character centers
using boundary points and character information (Section 2.3).
The hyper-local strategy has two main advantages: (1) each con-
tent query has its own explicit positional prior, which allows for
sampling localized image features around target sub-components
(boundary points and characters) of a text instance. This strategy
enables extracting high-quality image features and results in supe-
rior training convergence since it localizes the sampling region; (2)
the positional queries encode the locations of individual reference
points for sub-components, which benefits the intra-class attention
between boundary points/characters within the text instance and
the inter-class attention across text instances.
2.3 Character Center Predictor
In Figure 2 2, we propose a character center predictor to generate
the centers and update the character reference points accordingly,
providing precise guidance on the sampling locations in the char-
acter decoder layer. Typically, the center of a character is highly
related to the character content and the text instance boundary.
Leveraging this assumption, the predictor takes character content
queries and boundary point content queries with their respective
position information to predict character centers.
For a text instance 𝑖,6the predictor uses the character content
queries{𝑞(𝑗)
𝑚}𝑀
𝑗=1with their positional embeddings {p(𝑞(𝑗)
𝑚)}𝑀
𝑗=1as
6Her
e we drop(𝑖)in𝑞(𝑖,𝑗)for simplicity.new queries (Q), and the boundary point content queries {𝑞(𝑗)
𝑛}𝑁
𝑗=1
with their positional embeddings {p(𝑞(𝑗)
𝑛)}𝑁
𝑗=1to form key and
value sets (K ,V), and predicts character centers as follows,
Q=
wq·(𝑞(𝑗)
𝑚+p(𝑞(𝑗)
𝑚))	𝑀
𝑗=1
K=
wk·(𝑞(𝑘)
𝑛+p(𝑞(𝑘)
𝑛))	𝑁
𝑘=1
V=
wv·𝑞(𝑘)
𝑛	𝑁
𝑘=1
A=
Q𝑗·K𝑘	𝑀,𝑁
𝑗=1,𝑘=1
𝐶𝑐𝑜𝑜𝑟𝑑𝑠 =
𝑀𝐿𝑃(𝑁∑︁
𝑘=1A𝑗,𝑘·V𝑘)	𝑀
𝑗=1
where wq,wk, and wvare linear projections. Ais the attention map
containing𝑁×𝑀elements, and 𝐶𝑐𝑜𝑜𝑟𝑑𝑠 are the character center
coordinates predicted using cross attention between characters and
boundary points followed by a 3-layer 𝑀𝐿𝑃 .
This module not only predicts the character centers to support
hyper-local sampling and positional embeddings, but also bridges
the gap between two decoders and enables the collaboration be-
tween the boundary points and characters of a text instance. The
predicted centers will be supervised by the truth character centers
during training. If a text instance is shorter than the predefined
maximum text length, we set the rest centers as the middle of the
last point of the top curve and the first point of the bottom curve,
i.e., the center line tail. We discuss the details in Section 2.5.
2.4 Optimization
2.4.1 Bipartite Matching. After obtaining the prediction set ˆ𝑌,
PaLeTTe uses the Hungarian algorithm [ 24] to achieve an opti-
mal matching between ˆ𝑌and the ground truth set 𝑌that minimizes
the matching costC:
arg min𝜑𝐺∑︁
𝑔=1C(𝑌(𝑔),ˆ𝑌(𝜑(𝑔)))
where𝐺is the number of ground truth and 𝜑(𝑔)is the predicted
instance index matched to the 𝑔-th ground truth instance.
Regarding the cost C, previous work [ 54] uses the boundary
point class and position similarity. However, pairs with similar
boundary point positions could increase optimization difficulties [ 52].
DeepSolo [ 52] introduces character similarity in the cost. However,
character recognition performance at the early training stage might
be much poorer than detection, potentially affecting the matching
process. PaLeTTe adds the comparison between the predicted char-
acter centers with the ground truth centers, making it robust in the
matching process. Thus, for the 𝑔-th ground truth instance and a
target instance, the cost function is:
C=𝜆𝑐𝑙𝑠𝐹𝐿(ˆ𝑏(𝜑(𝑔)))+𝜆𝑐𝑜𝑜𝑟𝑑𝑁−1∑︁
𝑛=0∥𝑝(𝑔)
𝑛−ˆ𝑝(𝜑(𝑔))
𝑛∥
+𝜆𝑐𝑒𝑛𝑡𝑒𝑟
|𝐶|𝐶−1∑︁
𝑐=0∥𝑝(𝑔)
𝑐−ˆ𝑝(𝜑(𝑔)
)
𝑐∥
where𝜆𝑐𝑙𝑠,𝜆𝑐𝑜𝑜𝑟𝑑 ,𝜆𝑐𝑒𝑛𝑡𝑒𝑟 are the hyper-parameters, ˆ𝑏(𝜑(𝑔))is
the probability for the text-only instance class [ 55],𝑝𝑛and𝑝𝑐are
5390Hyper-Local Deformable Transformers for Text Spotting on Historical Maps KDD ’24, August 25–29, 2024, Barcelona, Spain
the coordinates of 𝑛-th boundary point and 𝑐-th character center,
respectively, and|𝐶|is the number of characters.
2.4.2 Overall Loss. PaLeTTe uses the same encoder losses for pro-
posals as [ 52–54], denoted asL𝑒𝑛𝑐. For the𝑗-th query in the decoder,
the loss function is:
L(𝑗)
𝑑𝑒𝑐=𝜆𝑐𝑙𝑠L(𝑗)
𝑐𝑙𝑠+𝜆𝑐𝑜𝑜𝑟𝑑L(𝑗)
𝑐𝑜𝑜𝑟𝑑+𝜆𝑐𝑡L(𝑗)
𝑐𝑡+𝜆𝑐ℎ𝑎𝑟L(𝑗)
𝑐ℎ𝑎𝑟
whereL(𝑖)
𝑐𝑙𝑠andL(𝑖)
𝑐𝑜𝑜𝑟𝑑are the classification loss and boundary
points loss,L(𝑗)
𝑐𝑡is the𝐿1loss for character centers, and L(𝑗)
𝑐ℎ𝑎𝑟is the𝐶𝐸loss for character recognition. We also introduce the
intermediate decoder loss to regularize the output of each decoder
layer. Thus, the overall loss L=L𝑒𝑛𝑐+Í
𝑗L(𝑗)
𝑑𝑒𝑐.
2.5 Iterative Training
Although character centers are available or can be easily obtained
in synthetic datasets, retrieving character centers in human an-
notations is non-trivial. Inspired by [ 51], we propose an iterative
training method for PaLeTTe to mitigate the requirement for char-
acter centers in using human-annotated data. During pretraining,
we leverage massive synthetic data with low-cost character center
information and a small number of human annotations to train
PaLeTTe in a weakly supervised manner, i.e., PaLeTTe does not
compute the character center loss if centers are unavailable. Now,
the pretrained PaLeTTe has the capability to predict character
centers using the character center predictor.
During finetuning, we iteratively add the “correct” character
center predictions into training samples and use these centers for
training. Figure 3 provides an example to illustrate how PaLeTTe
gradually identifies the “correct” character centers for real-world
images. If all center locations of the characters in a word (i.e., yel-
low dots) are within the ground truth text boundary (i.e., black
polylines), we consider the text instance having a “correct” charac-
ter center prediction (e.g., “SCHOOL”). Otherwise, we only keep
the fake centers (i.e., blue dots) for the rest empty characters (e.g.,
“B.F.DAY”). Then, we add these correctly predicted centers to the
training data. After the first round of finetuning, we apply the fine-
tuned model on the images again to retrieve new “correct” character
centers. We repeat this process until the number of “correct” char-
acter center predictions is stable. This iterative training strategy on
PaLeTTe alleviates the requirement on expensive human-annotated
character centers, instead leveraging the predicted centers and train-
ing the model in a weakly supervised manner.
3SynthMap+
Manual annotations on real-world images (e.g., scene images and
scanned documents) are prohibitively expensive. Therefore, exist-
ing text spotters leverage synthetic datasets for training. However,
existing synthetic datasets [ 11,33] mainly focus on scene images
and do not work well for historical maps because map text can have
various cartographic styles, character spacing sizes, and rotation
angles. For example, map text typically follows the corresponding
geographic features (e.g., rivers) in a complex line-styled back-
ground (e.g., roads, contour lines). Li et al . [30] propose to generate
synthetic map images for text detection on historical maps. How-
ever, the dataset lacks text transcriptions and features only one
Figure 3: An example of training PaLeTTe in a weakly super-
vised way. PaLeTTe iteratively adds the correctly predicted
character centers (e.g., •) for training. Black polylines are
true text boundaries; red dots are true boundary points; yel-
low dots are predicted character centers; and blue dots are
fake centers for the rest empty characters.
map style (i.e., Ordnance Survey), limiting its adaptability to train
text spotting models for diverse map styles in practice.
We propose a novel approach that automatically generates map
images of historical styles along with text annotations, named Syn-
thMap+, to train text spotters for historical maps. Figure 4 shows
the workflow for generating SynthMap+ images and text labels. We
first collect a large number of location names, geographic features
(lines and polygons7), and their types (e.g., rivers and industrial
areas) from OpenStreetMap8. Our approach contains two modules,
text render andbackground render, to produce synthetic map im-
ages of a wide variety.
Figure 4: The workflow of generating synthetic map images
and text labels, including word-level boundary points, char-
acter centers, and transcription.
The text render (TR) uses the QGIS label placement API [ 26] to
draw location names according to the shapes of geographic features
on an empty canvas. For line features, the text instances follow the
line curvature (e.g., roads and railways). For polygon features, the
7We exclude point features because they do not provide diverse geometry shapes.
8https://www.openstreetmap.org/
5391KDD ’24, August 25–29, 2024, Barcelona, Spain Yijun Lin & Yao-Yi Chiang
text instances follow the contour of the polygon boundary (e.g.,
lakes). TR randomly sets text font, size, letter spacing, and word
spacing for text labels, providing various styles for placement. Then,
TR retrieves the bounding box of each character and groups them to
produce the boundary polygon, character centers, and transcription
for each text instance.
The background render (BR) aims to create realistic scanned map
backgrounds in a wide variety. The idea is to identify representative
background regions from real historical maps and use them as new
map backgrounds. Specifically, BR first detects text regions from
maps in the David Rumsey Historical Map Collection and creates
a rectangle buffer around the detection results to retrieve pixels
around text. Instead of using pixels, BR constructs grid cells of 8 ×8
pixels as a unit to preserve the local pattern of the texture and
color. The red cells in Figure 4 (a)show the extracted text regions.
BR adopts K-means to classify these cells into background and
foreground classes based on their colors, see Figure 4 (b). Then BR
uses a second K-means to group these background cells into clusters
as the representative map style profiles and use them to render
new maps. When creating a synthetic map image, BR randomly
selects one map style and its background cells will be reconstructed
into an image, serving as the color and texture. Our approach also
uses QGIS API to paint geographic features of various types using
predefined cartographic rules on these backgrounds. Finally, our
approach merges the output from TR and BR to create synthetic
map images. Figure 5 shows two examples in SynthMap+.
Figure 5: Examples of SynthMap+ images. Red boundaries
are word-level annotations.
4 Experiments and Results
4.1 Training Datasets
4.1.1 SynthText. Following [ 11,33], we create a new synthetic
dataset that contains character centers extracted from character
bounding polygons. We add background images of ancient canvas
styles from WikiArt [ 46] and the OSM location names for text
rendering. SynthText consists of 84,182 and 48,000 images of
mostly straight and curved text, respectively.
4.1.2 SynthMap+. We create a synthetic map dataset containing
a total of 73,657 images with 1,102,940 annotations.
4.1.3 External Human-Annotated Datasets. We add human anno-
tations of existing real-world scene images to facilitate training:
ICDAR15 [19], Total-Text [8], and MLT [36].4.2 Evaluation Map Datasets
We evaluate the text detection and recognition performance on two
manually annotated historical map datasets, following the standard
evaluation protocols [33].
4.2.1 Grinnell-UMass-31. Ray et al . [39] annotated 31 scanned
historical maps from 9 atlases in the David Rumsey Map Collection.
We crop the maps into patches of 1,000 ×1,000 pixels and select 500
images with the largest number of text instances. The text instances
truncated at the image edge or having all numeric characters are
excluded during the evaluation (i.e., treated as “DON’T CARE”).
The final dataset contains 15,143 non-numeric text instances.
4.2.2 Rumsey-309. We create a large and diversified historical map
benchmark dataset by randomly selecting and annotating 309 image
patches (1,000×1,000 pixels) from 54 diverse historical maps in the
David Rumsey Historical Map Collection. We follow the annotation
guideline [ 2] and generate ground truth annotations. The final
dataset contains 13,145 non-numeric text labels.
4.3 Experimental Settings
Following the convention for training text spotters, we pretrain
PaLeTTe (and baselines) with synthetic and human-annotated
datasets and finetune models with Total-Text [ 8]. We choose Total-
Text because (1) we do not have enough historical map images
for both training and testing, and (2) we find that existing text
spotters finetuned with Total-Text usually perform the best on our
evaluation datasets. We provide details of network architecture,
hyper-parameters, and training settings in Appendix A.
4.4 Results and Discussion
4.4.1 Overall Performance. We compare PaLeTTe with several
SOTA spotting models: ABCNet-v2 [ 34], SWINTS [ 15], TESTR [ 54],
and DeepSolo [ 53]. Table 1 reports the spotting performance of
pretrained and finetuned models. Pretrained PaLeTTe outperforms
baselines with E2E (no lexicon) F-score gains of ≥3.3% on Grinnell-
UMass-31 and≥5.8% on Rumsey-309. These results highlight the ef-
fectiveness of PaLeTTe in historical map text spotting. Additionally,
finetuning models with Total-Text generally enhances the perfor-
mance. The improvement on Grinnell-UMass-31 is less significant
compared to Rumsey-309. This might be because Grinnell-UMass-31
contains only nine map styles, making finetuning on diverse scene
images less impactful. Also, finetuned PaLeTTe models achieve
less improvement compared to the pretrained ones because, even
with iterative training, there are some text instances that PaLeTTe
cannot correctly locate their character centers, limiting its training
ability. The overall results demonstrate the robustness of PaLeTTe.
We further explore the model performance on example challenging
cases, including lengthy and highly rotated text instances.
4.4.2 Text Length. Table 2 presents the E2E recall on text instances
of lengths larger than 7 and 10. PaLeTTe outperforms baselines
with significant enhancements, e.g., 8% on Grinnell-UMass-31 and
2% on Rumsey-309 when text length≥10. The results demonstrate
that the use of a single fixed reference point or sampling reference
points from the center line is insufficient for long text instances
compared to the hyper-local strategy in PaLeTTe .
5392Hyper-Local Deformable Transformers for Text Spotting on Historical Maps KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Spotting performance of pretrained and finetuned models on Grinnell-UMass-31 andRumsey-309. “None” refer to no
lexicon. “Full” uses the lexicon of all words in the test set. The best results are in bold, and the second best are underlined .
Grinnell-UMass-31 Rumsey-309
MethodDetection E2E Detection E2E
P R F None Full P R F None Full
PretrainedABCNet 82.48 81.35 81.91 54.46 66.50 80.16 78.36 79.25 44.83 57.49
SWINTS 88.90 72.46 79.89 58.14 68.95 86.93 71.52 78.48 56.97 68.31
TESTR 87.53 82.30 84.84 68.05 77.94 85.72 79.07 82.26 65.99 75.59
DeepSolo 87.15 74.57 80.37 67.54 74.69 78.13 68.1 72.77 60.11 67.27
PaLeTTe 89.20 83.09 86.04 (↑1.4%) 70.27 (↑3.3%) 79.13 88.01 81.77 84.77 (↑3.1%) 69.82 (↑5.8%) 78.75
FinetunedABCNet 84.46 81.56 82.98 53.84 65.03 87.59 83.63 85.56 45.91 58.44
SWINTS 88.38 74.29 80.73 59.40 68.33 91.92 77.28 83.97 60.45 71.50
TESTR 89.27 83.40 86.24 69.95 78.09 92.25 86.36 89.21 73.44 81.54
DeepSolo 89.44 76.02 82.18 67.92 75.24 89.18 78.11 83.28 68.52 76.26
PaLeTTe 89.98 83.53 86.64 (↑0.5%) 71.67 (↑2.5%) 78.94 93.56 87.28 90.31 (↑1.2%) 75.87 (↑3.3%) 83.63
Table 2: E2E recall of pretrained models on the text length
(𝑙) :𝑙≥7and𝑙≥10
Grinnell-UMass-31 Rumsey-309
𝑙≥7𝑙≥10𝑙≥7𝑙≥10
# Text 4242 826 2436 393
ABCNet-v2 57.02 49.93 43.22 36.41
SWINTS 57.22 42.63 52.20 39.94
TESTR 71.31 62.98 66.14 58.26
DeepSolo 72.51 62.36 63.21 55.64
PaLeTTe 74.74 68.08 68.04 59.29
4.4.3 Text Orientation. Table 3 shows the E2E recall on the text
instances of rotation angles in [30◦, 60◦) and [60◦, 90◦]. The text
instances within these ranges constitute approximately 25% of the
datasets, and Rumsey-309 has 11% more instances with large an-
gles than Grinnell-UMass-31. The baselines (ABCNet-v2 and Deep-
Solo) that rely on Bezier curves as boundary representations per-
form worse than those directly using polygon points (TESTR and
PaLeTTe) when spotting text with high rotation angles, especially
[60◦, 90◦].PaLeTTe outperforms baselines on both datasets, espe-
cially having an improvement of 4.8% in [60◦, 90◦] on Rumsey-309,
which further demonstrates the effectiveness of the hyper-local
sampling strategy in PaLeTTe .
Table 3: E2E recall of pretrained models on two ranges of
text orientation degrees: [30◦, 60◦) and [60◦, 90◦]
Grinnell-UMass-31 Rumsey-309
[30, 60) [60, 90] (30, 60] (60, 90]
# Text 1873 1275 1981 1534
ABCNet-v2 22.67 15.94 20.51 9.84
SWINTS 40.69 37.12 45.50 47.06
TESTR 52.65 46.18 57.98 53.38
DeepSolo 46.22 36.14 50.97 44.84
PaLeTTe 54.74 47.17 62.58 55.224.4.4 Effect of SynthMap+ .Table 4 shows that introducing Syn-
thMap+ significantly improves performance in both PaLeTTe and
the baseline models. SynthMap+ enables text spotters to train from
a wide variety of map styles in text and background, boosting their
performance compared to only using SynthText. For example,
incorporating SynthMap+ during pretraining leads to a remark-
able 24.8% increase in E2E performance for PaLeTTe. We show the
results for other baselines in Appendix B.
Table 4: Effect of SynthMap+. Models are pretrained with:
SynthText (S), human-annotations ( H) (ICDAR, Total-Text,
MLT), and SynthMap+ (M). The spotting performance are
reported on Rumsey-309.
Datasets Det-P Det-R Det-F E2E-None
TESTRS+H 77.1 75.7 76.4 53.0
S+M 84.0 73.9 78.6 61.0
S+H+M 85.7 79.0 82.2 65.9
PaLeTTeS+H 79.2 77.1 78.1 55.9
S+M 86.7 76.9 81.5 64.6
S+H+M 88.0 81.7 84.7 69.8
4.4.5 Ablation Studies. We conduct ablation studies to further
analyze (a) the effect of hyper-local sampling, (b) the effect of
hyper-local positional embeddings, and (c) the effect of incorpo-
rating character center distance in the cost of bipartite matching.
For (a), we remove hyper-local sampling for detecting boundary
points (PaLeTTe- wo-HLD ) and recognizing character (PaLeTTe-
wo-HLR ) respectively by replacing the dynamic reference points
with the fix proposal center. For (b), we replace the hyper-local
positional embeddings with the positional embedding of the center
point (PaLeTTe- wo-HLPE ). For (c), we omit the character center
distance in the cost of bipartite matching (PaLeTTe- wo-B ). Table 5
presents several insights: (a) without hyper-local sampling results
in a 2.9% reduction in detection F1 (first row) and 5.1% in recognition
performance (second row). This finding highlights the effectiveness
of using boundary points and character centers as the respective
5393KDD ’24, August 25–29, 2024, Barcelona, Spain Yijun Lin & Yao-Yi Chiang
reference points to sample localized image features for the spotting
tasks. (b) PaLeTTe exhibits a marginal decrease in detection and
recognition performance when hyper-local positional embeddings
are excluded, demonstrating the necessity of using localized po-
sition information for predicting the target sub-components. (c)
incorporating character center distance in bipartite matching leads
to a slight improvement, indicating that the character center dis-
tance enables a robust bipartite matching between the predictions
and ground truth by considering the character positions.
Table 5: Ablation Studies of PaLeTTe onRumsey-309
Det-P Det-R Det-F E2E-None
PaLeTTe-wo-HLD 86.3 79.1 82.3 67.9
PaLeTTe-wo-HLR 86.9 80.9 83.8 66.4
PaLeTTe-wo-HLPE 87.4 78.9 83.0 68.3
PaLeTTe-wo-B 87.9 80.0 83.2 69.1
PaLeTTe 88.0 81.7 84.7 69.8
4.4.6 Visual Analysis. Figure 6 presents several example results
showing that PaLeTTe can accurately detect and recognize complex
text instance arrangements and styles. However, PaLeTTe faces
difficulties when the text size is extremely large and characters in a
text instance are far away, e.g., the character “A” in map image 4,
when some other text instances or linear features overlap the target
text instance (e.g., the character “J” is separated from “JEFFERS” by
“Newport”), and when many linear features exist on/around a text
instance. We believe that more diverse types of synthetic training
data could help alleviate this problem.
5 Deployment
We have deployed a version of PaLeTTe trained with SynthMap+,
SynthText, and human-annotated datasets in the mapKurator
system [ 21]. The mapKurator system is a complete pipeline for effi-
ciently extracting and linking text from large-dimension historical
maps.9The deployment at the University of Minnesota has pro-
cessed over 60,000 maps in the David Rumsey Historical Map Col-
lection and generated more than 100 million text labels, which have
been incorporated into the metadata and visualization platform by
Luna Imaging that enables the “Search by Text-on-Maps” capabil-
ity on the David Rumsey Historical Map Collection website.10We
provide two examples in Figure 7 of searching “Minnesota” (2,099
words found in the map collection) and “Minneapolis” (1,429 words
in total). The searching function will show all the text regions and
the corresponding maps containing the target word. This function
supported by PaLeTTe andSynthMap+ largely facilitates search-
ing through tens of thousands of historical map scans, promoting
the studies such as the geographic name changes [ 37]. mapKura-
tor with PaLeTTe is also deployed at the Center for GIS, RCHSS,
Academia Sinica, Taiwan and the Institute of Disaster Mitigation
for Urban Cultural Heritage, Ritsumeikan University, Japan for text
extraction from various types of historical maps.
9https://github.com/knowledge-computing/mapkurator-system
10https://www.davidrumsey.com/home
Figur
e 6: Visualization of spotting results
Figure 7: The “Search by Text-on-Maps” interface of search-
ing “Minnesota” (top) and “Minneapolis” (bottom).
5394Hyper-Local Deformable Transformers for Text Spotting on Historical Maps KDD ’24, August 25–29, 2024, Barcelona, Spain
6 Related Work
6.1 Text Spotting on Scanned Documents
Text detection and recognition on scanned documents are impor-
tant to understand the contents of documents, including scanned
receipts [ 17], historical newspapers [ 45], manuscripts [ 43] and
maps [ 2,14,41]. Traditional techniques usually involve careful
design with heuristics for a specific document style [ 7,27], e.g.,
separating text and background based on color [ 6]. Deep neural
networks have gained significant attention for automatically lo-
calizing and recognizing text from documents [ 29,35]. DAN is
a transformer-based method for recognizing and comprehending
handwritten text [ 9]. However, the map text can be in arbitrary
shapes and various styles. Weinman et al . [50] propose a CNN-based
text detection model to localize the text and then recognize the de-
tected text regions using CRNN [ 44]. However, the detection results
can limit the recognition performance in two-stage approaches.
6.2 Text Spotting on Scene Images
Recent end-to-end text spotting approaches mainly focus on scene
images and demonstrate promising results on benchmark datasets [ 8,
19]. Classical spotting models adopt a detect-then-recognize ar-
chitecture that relies on RoI-based connectors [ 12,32,33,40] or
shape transform modules [ 38,47]. FOTS [ 32] proposes RoIRotate
to extract oriented text regions from convolutional features, while
ABCNet [ 33,34] proposes BezierAlign to handle curved text using
parameterized Bezier curves. SwinTextSpotter [ 15] injects the de-
tection features into the recognition stage by predicting tight masks
to suppress background noise and generate recognition features.
However, these methods require careful design on RoI-based op-
erations, which can significantly impact recognition performance.
In addition, segmentation-based methods typically need heuristic
grouping and post-processing [31, 48, 51].
Inspired by DETR [ 3] and Deformable DETR [ 55], recent text
spotters [ 16,22,23,52–54] explore the DETR-based framework
without RoI operations and complicated postprocessing. TESTR [ 54]
adopts Deformable DETR with a dual decoder structure to refine
the boundary points and recognize characters layer by layer. Deep-
Solo [ 52,53] proposes to sample reference points from Bezier center
curves and use a single transformer decoder to predict boundary
points and text. Although these methods show promising results on
scene images, map text is typically lengthy, curved, and highly ro-
tated. Thus, using coarse reference point(s) (e.g., the proposal center
or sampled points from the center line) to sample image features can
be inaccurate for the target sub-components (i.e., boundary points
and characters) in a text instance. PaLeTTe builds on Deformable
DETR but explicitly models the boundary points and characters
as the sub-components, and learn from localized image features
towards the the sub-components for spotting, showing outstanding
performance on map text.
Another recent direction for text detection and recognition is to
incorporate the capability of language models. TrOCR [ 28] crops
images into small patches as tokens and inputs to a text Trans-
former as training LMs for text recognition. ESTextSpotter [ 16]
introduces a vision-language communication module, enabling in-
teraction between visual features from DETR and semantic infor-
mation from a language model. UNITS [ 20] addresses the challengewhen having multiple types of annotations (e.g., point, polygon)
by proposing a language modeling approach to model detection
formats as sequences of text, improving the flexibility of adopting
various types of annotations for text spotting. PaLeTTe leverages
the self-attention between character embeddings within a word to
capture the relationships between characters, which is also inspired
by the idea of the language model at the character level.
6.3 Synthetic Datasets for Text Spotting
Synthetic datasets [ 18,49] alleviate the need for sufficient text an-
notations for training text spotters. Gupta et al . [11] propose a
CNN-based method to create synthetic images by blending text
from various styles in existing natural scenes. Liu et al . [33] fur-
ther diversifies the shapes, corpus, and styles of the text. However,
these synthetic images differ significantly from historical maps
regarding text and background styles. Weinman et al . [50] propose
to dynamically synthesize images on-the-fly for training by ren-
dering text on historical maps with noises, but the images are for
recognition purpose. Li et al . [30] propose to create synthetic map
images using CycleGAN to transfer a contemporary map style (e.g.,
OpenStreetMap) into a historical style (e.g., Ordnance Survey) and
place text labels on them. However, this synthetic dataset contains
only one background style and lacks text transcription. In con-
trast, SynthMap+ provides numerous synthetic map images that
simulate text placement and background styles of real maps, cou-
pled with text labels (including boundary points and transcription),
facilitating text spotting on historical maps.
7 Limitations, Conclusion, and Future Work
One limitation of PaLeTTe is the use of expensive character center
information. Nonetheless, we showed that the requirement can be
alleviated using iterative training. This paper presented a promising
end-to-end text spotting method PaLeTTe and a novel synthetic
map generation approach with synthetic map images, SynthMap+,
to detect and recognize text on historical maps. We also introduced
a new benchmark dataset, Rumsey309, for text spotting evaluation
on historical maps. These resources could inspire and facilitate
future research and interdisciplinary collaborations. We plan to
extend our work to multilingual historical maps and group the
word-level spotting results into place phrases.
Acknowledgments
This material is based on work supported in part by the gift from
David and Abby Rumsey to the University of Minnesota Foundation,
the University of Minnesota (UMN), and Kartta Foundation. The
authors thank the MapKurator developers for deploying PaLeTTe
in the system. We also thank the students from the Knowledge Lab
at UMN for annotating the benchmark dataset, Rumsey309.
References
[1]Samantha T Arundel, Trenton P Morgan, and Phillip T Thiem. 2022. Deep
Learning Detection and Recognition of Spot Elevations on Historical Topographic
Maps. Frontiers in Environmental Science (2022), 117.
[2]Larry Boateng Asante, David Cambronero Sanchez, Ravi Chande, Dylan Gumm,
and Jerod Weinman. 2017. A data set of annotated historical maps. Technical
Report. Technical report, Grinnell College, Grinnell, Iowa.
5395KDD ’24, August 25–29, 2024, Barcelona, Spain Yijun Lin & Yao-Yi Chiang
[3]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-
der Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with
transformers. In European conference on computer vision. Springer, 213–229.
[4]Yao-Yi Chiang, Muhao Chen, Weiwei Duan, Jina Kim, Craig A Knoblock, Stefan
Leyk, Zekun Li, Yijun Lin, Min Namgung, Basel Shbita, et al .2023. GeoAI for the
Digitization of Historical Maps. In Handbook of Geospatial Artificial Intelligence.
CRC Press, 217–247.
[5]Yao-Yi Chiang, Weiwei Duan, Stefan Leyk, Johannes H Uhl, and Craig A Knoblock.
2020. Using historical maps in scientific studies: Applications, challenges, and best
practices. Springer.
[6]Yao-Yi Chiang and Craig A. Knoblock. 2011. Recognition of Multi-oriented, Multi-
sized, and Curved Text. In 2011 International Conference on Document Analysis
and Recognition. 1399–1403. https://doi.org/10.1109/ICDAR.2011.281
[7]Yao-Yi Chiang, Stefan Leyk, and Craig A Knoblock. 2014. A survey of digital map
processing techniques. ACM Computing Surveys (CSUR) 47, 1 (2014), 1–44.
[8]Chee-Kheng Ch’ng, Chee Seng Chan, and Cheng-Lin Liu. 2020. Total-text: toward
orientation robustness in scene text detection. International Journal on Document
Analysis and Recognition (IJDAR) 23, 1 (2020), 31–52.
[9]Denis Coquenet, Clément Chatelain, and Thierry Paquet. 2023. DAN: a
segmentation-free document attention network for handwritten document recog-
nition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).
[10] William Morris Davis. 1893. The topographic maps of the United States Geological
Survey. Science 534 (1893), 225–227.
[11] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. 2016. Synthetic data
for text localisation in natural images. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 2315–2324.
[12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn.
InProceedings of the IEEE international conference on computer vision. 2961–2969.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[14] Rachel Hewitt. 2011. Map of a nation: A biography of the Ordnance Survey. Granta
Publications.
[15] Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao
Zhu, Nicholas Yuan, Kai Ding, and Lianwen Jin. 2022. SwinTextSpotter: Scene
Text Spotting via Better Synergy between Text Detection and Text Recognition. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
4593–4603.
[16] Mingxin Huang, Jiaxin Zhang, Dezhi Peng, Hao Lu, Can Huang, Yuliang Liu,
Xiang Bai, and Lianwen Jin. 2023. Estextspotter: Towards better scene text
spotting with explicit synergy in transformer. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 19495–19505.
[17] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian
Lu, and CV Jawahar. 2019. Icdar2019 competition on scanned receipt ocr and
information extraction. In 2019 International Conference on Document Analysis
and Recognition (ICDAR). IEEE, 1516–1520.
[18] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014.
Synthetic data and artificial neural networks for natural scene text recognition.
arXiv preprint arXiv:1406.2227 (2014).
[19] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh,
Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ra-
maseshan Chandrasekhar, Shijian Lu, et al .2015. ICDAR 2015 competition on
robust reading. In 2015 13th international conference on document analysis and
recognition (ICDAR). IEEE, 1156–1160.
[20] Taeho Kil, Seonghyeon Kim, Sukmin Seo, Yoonsik Kim, and Daehee Kim. 2023.
Towards unified scene text spotting based on sequence generation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15223–
15232.
[21] Jina Kim, Zekun Li, Yijun Lin, Min Namgung, Leeje Jang, and Yao-Yi Chiang.
2023. The mapKurator System: A Complete Pipeline for Extracting and Linking
Text from Historical Maps. arXiv preprint arXiv:2306.17059 (2023).
[22] Seonghyeon Kim, Seung Shin, Yoonsik Kim, Han-Cheol Cho, Taeho Kil, Jae-
heung Surh, Seunghyun Park, Bado Lee, and Youngmin Baek. 2022. DEER:
Detection-agnostic End-to-End Recognizer for Scene Text Spotting. arXiv preprint
arXiv:2203.05122 (2022).
[23] Yair Kittenplon, Inbal Lavi, Sharon Fogel, Yarin Bar, R Manmatha, and Pietro
Perona. 2022. Towards weakly-supervised text spotting using a multi-task trans-
former. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 4604–4613.
[24] Harold W Kuhn. 1955. The Hungarian method for the assignment problem. Naval
research logistics quarterly 2, 1-2 (1955), 83–97.
[25] Robert A Lamb. 1961. The Sanborn map: a tool for the geographer. (1961).
[26] Joel Lawhead. 2017. QGIS python programming cookbook. Packt Publishing Ltd.
[27] Huali Li, Jun Liu, and Xiran Zhou. 2018. Intelligent map reader: A framework for
topographic map understanding with deep learning and gazetteer. IEEE Access 6
(2018), 25363–25376.[28] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha
Zhang, Zhoujun Li, and Furu Wei. 2023. Trocr: Transformer-based optical char-
acter recognition with pre-trained models. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 37. 13094–13102.
[29] Zekun Li, Yao-Yi Chiang, Sasan Tavakkol, Basel Shbita, Johannes H Uhl, Stefan
Leyk, and Craig A Knoblock. 2020. An automatic approach for generating rich,
linked geo-metadata from historical map images. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 3290–
3298.
[30] Zekun Li, Runyu Guan, Qianmu Yu, Yao-Yi Chiang, and Craig A Knoblock. 2021.
Synthetic Map Generation to Provide Unlimited Training Data for Historical
Map Text Detection. In Proceedings of the 4th ACM SIGSPATIAL International
Workshop on AI for Geographic Knowledge Discovery. 17–26.
[31] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. 2020. Mask
textspotter v3: Segmentation proposal network for robust scene text spotting.
InComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August
23–28, 2020, Proceedings, Part XI 16. Springer, 706–722.
[32] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan. 2018. Fots:
Fast oriented text spotting with a unified network. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 5676–5685.
[33] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei
Wang. 2020. Abcnet: Real-time scene text spotting with adaptive bezier-curve
network. In proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 9809–9818.
[34] Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu,
and Hao Chen. 2021. Abcnet v2: Adaptive bezier-curve network for real-time
end-to-end text spotting. arXiv preprint arXiv:2105.03620 (2021).
[35] Francesco Lombardi and Simone Marinai. 2020. Deep learning for historical
document analysis and recognition—A survey. Journal of Imaging 6, 10 (2020),
110.
[36] Nibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan Feng, Dimosthenis Karatzas,
Zhenbo Luo, Umapada Pal, Christophe Rigaud, Joseph Chazalon, et al .2017. Ic-
dar2017 robust reading challenge on multi-lingual scene text detection and script
identification-rrc-mlt. In 2017 14th IAPR international conference on document
analysis and recognition (ICDAR), Vol. 1. IEEE, 1454–1459.
[37] Rhett M Olson, Jina Kim, and Yao-Yi Chiang. 2023. An Automatic Approach to
Finding Geographic Name Changes on Historical Maps. In Proceedings of the 31st
ACM International Conference on Advances in Geographic Information Systems.
1–2.
[38] Liang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, and Fei
Wu. 2020. Text perceptron: Towards end-to-end arbitrary-shaped text spotting. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 11899–11907.
[39] Archan Ray, Ziwen Chen, Ben Gafford, Nathan Gifford, J Jai Kumar, Abyaya Lam-
sal, Liam Niehus-Staab, Jerod Weinman, and Erik Learned-Miller. 2018. Historical
map annotations for text detection and recognition. Grinnell College, Grinnell,
Iowa, Tech. Rep (2018).
[40] Roi Ronen, Shahar Tsiper, Oron Anschel, Inbal Lavi, Amir Markovitz, and R
Manmatha. 2022. Glass: Global to local attention for scene-text spotting. In
Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
23–27, 2022, Proceedings, Part XXVIII. Springer, 249–266.
[41] David Rumsey. 2002. David Rumsey map collection. Cartography Associates.
[42] David Rumsey. 2008. David Rumsey historical map collection.
[43] Joan Andreu Sánchez, Verónica Romero, Alejandro H Toselli, Mauricio Villegas,
and Enrique Vidal. 2019. A set of benchmarks for handwritten text recognition
on historical documents. Pattern Recognition 94 (2019), 122–134.
[44] Baoguang Shi, Xiang Bai, and Cong Yao. 2016. An end-to-end trainable neural
network for image-based sequence recognition and its application to scene text
recognition. IEEE transactions on pattern analysis and machine intelligence 39, 11
(2016), 2298–2304.
[45] Takahiro Shima, Kengo Terasawa, and Toshio Kawashima. 2011. Image Process-
ing for Historical Newspaper Archives. In Proceedings of the 2011 Workshop on
Historical Document Imaging and Processing (HIP ’11). Association for Computing
Machinery, New York, NY, USA, 127–132.
[46] Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi Tanaka. 2019.
Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork.
IEEE Transactions on Image Processing 28, 1 (2019), 394–409. https://doi.org/10.
1109/TIP.2018.2866698
[47] Hao Wang, Pu Lu, Hui Zhang, Mingkun Yang, Xiang Bai, Yongchao Xu, Mengchao
He, Yongpan Wang, and Wenyu Liu. 2020. All you need is boundary: Toward
arbitrary-shaped text spotting. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 34. 12160–12167.
[48] Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang,
Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, and Guangming Shi. 2021.
Pgnet: Real-time arbitrarily-shaped text spotting with point gathering network.
InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 2782–2790.
[49] Tao Wang, David J Wu, Adam Coates, and Andrew Y Ng. 2012. End-to-end
text recognition with convolutional neural networks. In Proceedings of the 21st
international conference on pattern recognition (ICPR2012). IEEE, 3304–3308.
5396Hyper-Local Deformable Transformers for Text Spotting on Historical Maps KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 6: Effect of SynthMap+ (Continue). Models are pre-
trained with: SynthText (S), human-annotations ( H) (IC-
DAR, Total-Text, MLT), and SynthMap+ (M). The spotting
performance are reported on Rumsey-309.
Datasets Det-P Det-R Det-F E2E-None
ABCNet-v2S+H 78.6 66.2 71.9 43.6
S+M 78.1 70.0 73.8 43.8
S+H+M 82.4 81.3 81.9 44.8
SWINTSS+H 87.9 72.5 79.4 52.3
S+M 76.0 56.2 64.6 46.1
S+H+M 86.9 71.5 78.4 56.9
DeepSoloS+H 82.7 64.5 72.5 56.7
S+M 72.3 63.6 67.7 51.8
S+H+M 78.1 68.1 72.7 60.1
[50] Jerod Weinman, Ziwen Chen, Ben Gafford, Nathan Gifford, Abyaya Lamsal, and
Liam Niehus-Staab. 2019. Deep neural networks for text detection and recognition
in historical maps. In 2019 International Conference on Document Analysis and
Recognition (ICDAR). IEEE, 902–909.
[51] Linjie Xing, Zhi Tian, Weilin Huang, and Matthew R Scott. 2019. Convolutional
character networks. In Proceedings of the IEEE/CVF international conference on
computer vision. 9126–9136.
[52] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, and
Dacheng Tao. 2022. DeepSolo: Let Transformer Decoder with Explicit Points
Solo for Text Spotting. arXiv preprint arXiv:2211.10772 (2022).
[53] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, and
Dacheng Tao. 2023. Deepsolo: Let transformer decoder with explicit points solo
for text spotting. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 19348–19357.
[54] Xiang Zhang, Yongwen Su, Subarna Tripathi, and Zhuowen Tu. 2022. Text
Spotting Transformers. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 9519–9528.[55] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020.
Deformable detr: Deformable transformers for end-to-end object detection. arXiv
preprint arXiv:2010.04159 (2020).
A Experimental Settings
We adopt ResNet-50 [ 13] as the backbone to extract multi-scale
image features for PaLeTTe and baselines. We use 6 layers in the
encoder and decoder. We set the number of heads as 8 and the
number of sampling points as 4 for deformable attention. We select
the top 100 proposals from the encoder as queries in the decoder.
The number of boundary points is 16, the maximum text length is
25, and the vocabulary size is 96.
We train PaLeTTe and baseline models on 4 NVIDIA A100 (40GB)
GPUs with an image batch size of 4. The total number of iterations
for pretraining is 400,000, the initial learning rate for pretraining
is1×10−4, and it is decayed by a factor of 0.1 in the 300,000th
iteration. We finetune the models for 10,000th iterations with a
learning rate of 1×10−5. We incorporate the data augmentation of
random resizing and cropping during training.
B Effect of SynthMap+
Table 6 shows the spotting performance of training without
and with SynthMap+ for the rest baselines in addition to Table 4.
We observe that training with SynthMap+ generally improves de-
tection and recognition performance. However, unlike PaLeTTe
or TESTR, training with synthetic data only (SynthText +Syn-
thMap+) achieves poorer performance than SynthText plus hu-
man annotations in SWINTS and DeepSolo, which requires further
investigation, but this indicates the necessity of training with real-
world images and human annotations.
5397