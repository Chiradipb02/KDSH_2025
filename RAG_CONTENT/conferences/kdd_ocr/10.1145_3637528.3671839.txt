QGRL: Quaternion Graph Representation Learning for
Heterogeneous Feature Data Clustering
Junyang Chen∗
School of Computer Science and
Technology, Guangdong University of
Technology, Guangzhou, China
jychen117@gmail.comYuzhu Ji∗
School of Computer Science and
Technology, Guangdong University of
Technology, Guangzhou, China
yuzhu.ji@gdut.edu.cnRong Zou
Department of Computer Science,
Hong Kong Baptist University,
Hong Kong SAR, China
rongzou@comp.hkbu.edu.hk
Yiqun Zhang†
School of Computer Science and
Technology, Guangdong University of
Technology, Guangzhou, China
yqzhang@gdut.edu.cnYiu-ming Cheung†
Department of Computer Science,
Hong Kong Baptist University,
Hong Kong SAR, China
ymc@comp.hkbu.edu.hk
ABSTRACT
Clustering is one of the most commonly used techniques for unsu-
pervised data analysis. As real data sets are usually composed of
numerical and categorical features that are heterogeneous in na-
ture, the heterogeneity in the distance metric and feature coupling
prevents deep representation learning from achieving satisfactory
clustering accuracy. Currently, supervised Quaternion Representa-
tion Learning (QRL) has achieved remarkable success in efficiently
learning informative representations of coupled features from mul-
tiple views derived endogenously from the original data. To inherit
the advantages of QRL for unsupervised heterogeneous feature
representation learning, we propose a deep QRL model that works
in an encoder-decoder manner. To ensure that the implicit cou-
plings of heterogeneous feature data can be well characterized by
representation learning, a hierarchical coupling encoding strategy
is designed to convert the data set into an attributed graph to be
the input of QRL. We also integrate the clustering objective into
the model training to facilitate a joint optimization of the represen-
tation and clustering. Extensive experimental evaluations illustrate
the superiority of the proposed Quaternion Graph Representation
Learning (QGRL) method in terms of clustering accuracy and ro-
bustness to various data sets composed of arbitrary combinations
of numerical and categorical features. The source code is opened at
https://github.com/Juny-Chen/QGRL.git.
CCS CONCEPTS
•Computing methodologies →Neural networks; Learning
latent representations; Mixture modeling; Cluster analysis.
∗Co-first author.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671839KEYWORDS
Heterogeneous features, Graph neural network, Quaternion repre-
sentation learning, Spectral clustering
ACM Reference Format:
Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, and Yiu-ming Cheung.
2024. QGRL: Quaternion Graph Representation Learning for Heterogeneous
Feature Data Clustering. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671839
1 INTRODUCTION
Clustering is one of the most fundamental techniques in knowl-
edge discovery and data mining tasks. It explores the potential
distributions of data objects reflected by the feature values in an un-
supervised way. With the explosive growth of various data, data sets
composed of both numerical and categorical features are very com-
mon, which can be easily found in medical data analysis systems
[24], citation relationship databases [ 6], to name a few. Exploring
data object distributions jointly reflected by the numerical and cat-
egorical features is difficult due to the feature heterogeneity. That
is, the heterogeneous numerical and categorical features are with
quantitative and qualitative values, respectively, which describe
object distributions in completely different ways [ 2,39]. The loss
of critical representation information across the heterogeneous fea-
tures surely degrades the effectiveness of representation learning
and the accuracy of downstream clustering.
Recent heterogeneous feature data clustering approaches at-
tempt to develop similarity measures that take into account more
data statistics and prior knowledge, including the occurrence fre-
quency and semantic ordinal relationship of feature values, inter-
dependence among features, etc. Compared with the conventional
one-hot encoding [ 3] and Hamming distance [ 4] that simply con-
sider the matching between two values for similarity representation,
more advanced metrics [ 8,20] that define distance structures for
features by considering the occurrence frequency of intra-feature
values have been proposed. Most recent works [ 39,41,42,47] fur-
ther exploit the statistical prior of inter-feature-coupling to achieve
a more informative representation of categorical data. However,
 
297
KDD ’24, August 25–29, 2024, Barcelona, Spain Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, & Yiu-ming Cheung
these methods are based on the quantification of feature-level simi-
larity without considering the relationship among data objects, and
thus overlook the impact of object-level similarity to the clustering.
Thanks to the powerful ability of deep graph convolutional net-
works in revealing the relationship among graph nodes, deep graph
representation learning-based clustering has attracted much atten-
tion and achieved competitive clustering performance [ 21,35]. In
the graph representation learning field, the dominant Graph Convo-
lutional Network (GCN) [ 16] and its variants simultaneously embed
the graph structure and feature values to obtain a more comprehen-
sive data representation. Later, Graph Auto-Encoder (GAE) and its
variants [ 15,26,35,37] have also been specially developed for unsu-
pervised representation learning of graph data. By adopting graph
convolution layers as encoders, they considerably enhanced the per-
formance of graph data clustering. Theoretically, the representation
ability can be further improved by stacking more graph convolution
layers. However, the embeddings tend to be homogeneous due to
the common over-smoothing effect of stacking graph convolution
layers. Accordingly, most graph representation learning models are
restricted to a shallow graph convolutional network, preventing
them from aggregating node relationship beyond local distributions.
As a result, they fail to produce noise-robust embeddings, and will
somewhat influence the clustering performance.
Therefore, in this work, a new graph representation learning
method named Quaternion Graph Representation Learning (QGRL)
has been proposed for heterogeneous feature data clustering. QGRL
first constructs a graph on the heterogeneous feature data to capture
the implicit value-level, feature-level, and object-level couplings,
and then introduces a powerful quaternion representation learn-
ing mechanism [ 29] to circumvent the over-smoothing effect of
the graph representation learning. More specifically, an adjacency
matrix is derived from the data to form a graph structure, which is
called Heterogeneous Data Graph (HDG). To ensure an informative
graph construction, divergent statistical information of the data
is encoded through the designed Hierarchical Coupling Encoding
(HCE) strategy for the adjacency matrix computation. HDG acts
to bridge the information pathway between heterogeneous fea-
ture data and the following representation learning. By generating
four-view encodings from the constructed graph, the Hamilton
product of Quaternion Representation Learning (QRL) can facilitate
an efficient rotation of global features, bringing a higher degree of
freedom to the representation learning. This compensates for the
shallow graph convolutional network structure, and thus mitigates
the over-smoothing of the learned node embeddings. By integrating
the graph reconstruction and spectral clustering losses, the model is
urged to learn clustering-friendly representations in the generated
quaternion latent space. Extensive experiments on various hetero-
geneous feature data sets verify the superiority of the proposed
method in terms of both representation learning and clustering.
The main contributions are summarized into three-fold:
•A novel QRL framework is proposed for accurate and ro-
bust heterogeneous feature data clustering. It bridges the
information pathway between heterogeneous features and
representation learning using constructed graph, and also
bridges the representation learning and clustering task by a
joint learning scheme.•To provide a high information fidelity basis for the represen-
tation learning, an encoding strategy is carefully designed to
combine the statistical prior of data, including intra-feature
probabilities, inter-feature dependencies, and inter-object
distances computed by a metric unified on the heterogeneous
features.
•This is the first attempt to introduce quaternion into unsuper-
vised representation learning. Through our model design, an
efficient decoupling for the representation of heterogeneous
feature data has been formed, which is also of great refer-
ence value for applying quaternion to other unsupervised
learning tasks.
2 RELATED WORK
This section overviews the related existing works in the fields of het-
erogeneous feature data clustering, graph representation learning,
and quaternion representation learning.
2.1 Heterogeneous Feature Data Clustering
Existing approaches for heterogeneous feature data clustering can
be roughly categorized into two types: 1) define a distance mea-
sure for categorical features for clustering, and 2) encode data into
numerical data for clustering.
For the former type, a developing trend is to exploit the statisti-
cal information of data for more reasonable distance computation.
Some studies [ 1,14,18] understand the similarity between two
values from the perspective of probability. That is, if the probabil-
ity of randomly picking two different values at the same time is
higher, then they are considered more similar. Some other methods
[20,40,41] compute the information entropy based on the probabili-
ties, and judge the dissimilarity from the perspective of information
theory. To extend the above ideas by taking into account the inter-
dependence between features, conditional probability distributions
(CPDs) corresponding to two possible values obtained from another
feature are widely adopted to reflect the value-level distance by the
existing methods [ 13,23,43]. By further exploiting the semantic
order of categorical feature values [ 44], the method [ 39] defines
distance metrics that are unified on numerical, nominal, and ordinal
features for more universal clustering.
The latter type of method converts categorical values into nu-
merical ones for clustering. As the conventional one-hot encoding
overlooks the couplings within data, a more advanced encoding
strategy that utilizes the adjacency matrix of data objects as the
encoding has been proposed [ 31]. To adapt the encoding strategies
to clustering, some recent advances [ 38,42,47] that make the en-
coding process learnable w.r.t. clustering objective have also been
proposed. However, all the above-mentioned encoding strategies
rely much on prior domain knowledge, which thus limits their effi-
cacy. For the encoded data, conventional K-Means-type algorithms
[12] or spectral clustering algorithms [ 22] can be directly applied
to obtain the clustering results.
2.2 Deep Graph Representation Learning for
Clustering
Inspired by the powerful feature extraction capability of convo-
lutional neural networks [ 17], GCNs [ 16] have been proposed to
 
298QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain
generalize the convolution operation to graph data, thereby integrat-
ing the graph structure and feature information for representation
learning. Inheriting the powerful encoder-decoder representation
learning backbone of AE [ 34] and VAE [ 7], Kipf et al. [15] pro-
posed GAE and VGAE that project the inputs to a low-dimensional
space and reconstruct the graph structure in a learnable manner to
capture the key data features.
The variants of GAE-based methods [ 26,28,35,37] further in-
troduce different encoding enhancement mechanisms to improve
the capability of embedding learning. The DAEGC [ 35] introduces
the attention mechanism to integrate attribute information and
graph architecture for more comprehensive representation learn-
ing. To further achieve joint clustering and representation learning,
the work [ 37] relaxes the clustering objective and combines it into
the training process of GAE. Later, to realize a more robust data
representation learning, R-GAE [ 26] has been proposed to relieve
the influence brought by the noise features, feature drifts, and
feature randomness from a mathematics perspective. Although
the above-mentioned GCN-based methods achieve considerable
improvements in clustering, they still suffer from the intrinsic over-
smoothing effect of graph convolution operation and have not taken
into account the common issue of feature heterogeneity.
2.3 Quaternion Representation Learning
A quaternion is a hyper-complex number composed of four parts,
and the Hamilton product of two quaternions can be viewed as their
efficient rotation in the space spanned by the orthogonal imaginary
axes. To leverage the efficient quaternion product in representation
learning, some recent studies [ 9,27,29,45,48] have extended the
feature encoding from real-value field to quaternion-value field for
more sufficient feature coupling learning. The Quaternion Neural
Networks (QNNs) [ 48] have demonstrated a great feature extrac-
tion ability in various supervised tasks, e.g., few-shot segmentation
[45], image classification [ 11], and speech recognition [ 30]. QCLNet
[45] introduces quaternion representation learning to alleviate the
computation burden brought by the high-dimensional correlated
tensors, and also to explore the latent interactions between query
and support images. The work [ 11] regards each RGB image as
a quaternion, and embeds it with a learnable weight quaternion
through the Hamilton product to achieve a more powerful represen-
tation learning. Benefiting from the orthogonal imaginary axes and
the rotation nature of quaternion algebra, quaternion facilitates effi-
cient feature coupling learning and thus is promising in enhancing
representation learning of features with complex relationships.
3 PRELIMINARIES
This section introduces the definition of heterogeneous feature data
and the problem setting of its clustering. Then the basic quaternion
algebra is presented. Table 1 sorts out the frequently used notations
and symbols in this paper.
A heterogeneous feature data set Sis represented as a triplet
S=<X,A,O>. The data object set X={𝒙𝑙|𝑙=1,2,...,𝑛}con-
tains𝑛objects, and each object 𝒙𝑙=[𝑥1
𝑙,𝑥2
𝑙,...,𝑥𝑑
𝑙]⊤is represented
by values from 𝑑featuresA={𝑨𝑟|𝑟=1,2,...,𝑑}. For a heteroge-
neous feature data set, it is assumed that there are 𝑑{𝑐}categorical
and𝑑{𝑢}numerical features, and we have A=A{𝑐}∪A{𝑢}withTable 1: Frequently used notations and symbols.
Notations
and Symbols Explanations
Subscript e
.g., “𝑖” of𝒙𝑖 Element
index
Superscript e.g., “𝑟” of𝑨𝑟andO𝑟Attribute
index
Curly-bracketed superscript e.g., “{𝑐}” of𝑑{𝑐}Annotation
Calligraphic
letters e.g.,A,O, andX Set
Upp
ercase, calligraphic font e.g.,R𝑟
𝑖𝑗Space
Upp
ercase, bold font, e.g.,AandW𝑖 Matrix
Bold
font, italic e.g.,𝒙𝑙and𝑨𝑟V
ector
𝑑=𝑑{𝑐}+𝑑{𝑢}, whereA{𝑐}andA{𝑢}are the categorical and
numerical feature set, respectively. Each feature can be written
as an𝑛-value vector 𝑨𝑟=[𝑎𝑟
1,𝑎𝑟
2,···,𝑎𝑟𝑛], and for a categorical
feature 𝑨𝑟∈A{𝑐}, its𝑛values are distributed on a limited num-
ber (i.e .,𝜐𝑟for𝑨𝑟) of possible values, which can be written as a
unique value setO𝑟={𝑜𝑟
1,𝑜𝑟
2,...,𝑜𝑟
𝜐𝑟}withO𝑟∈O. Our research
goal is to perform QRL on the above-mentioned heterogeneous
feature data sets to obtain satisfactory clustering performance. In
this work, we focus on the common crisp partitional clustering
task to divide a whole data set into a certain number of compact
subsects containing closely distributed data objects.
The following introduces the quaternion operation rules in QRL.
Quaternion 𝑄is a type of hyper-complex number in the field H,
which can be represented as:
𝑄=𝑟+𝑥i+𝑦j+𝑧k, (1)
where𝑟is for the real part and 𝑥i+𝑦𝒋+𝑧krepresents the imaginary
parts. In H, an orthogonal relationship holds for the imaginary
parts, i.e.,i2=j2=k2=ijk=−1. Then we present the quaternion
algebra involved in this paper: i) addition, ii) scale multiple, and iii)
Hamilton product, in the following.
i) Addition: Given two quaternions 𝑄1and𝑄2, the addition op-
eration adds up their corresponding parts by:
𝑄1+𝑄2=(𝑟1+𝑟2)+(𝑥1+𝑥2)i
+(𝑦1+𝑦2)j+(𝑧1+𝑧2)k.(2)
ii) Scale Multiple: A quaternion 𝑄can be scaled by a scalar 𝜆as:
𝜆𝑄=𝜆𝑟+𝜆𝑥i+𝜆𝑦j+𝜆𝑧k. (3)
iii) Hamilton Product: The interaction between two quaternions
𝑄1and𝑄2is specified by the Hamilton product known as the quater-
nion transformation. More specifically, 𝑄1can be transformed by
rotating it based on the quaternion 𝑄2by:
𝑄1⊗𝑄2=(𝑟1𝑟2−𝑥1𝑥2−𝑦1𝑦2−𝑧1𝑧2)
+(𝑟 1𝑥2+𝑥1𝑟2+𝑦1𝑧2−𝑧1𝑦2)i
+(𝑟 1𝑦2−𝑥1𝑧2+𝑦1𝑟2+𝑧1𝑥2)j
+(𝑟 1𝑧2+𝑥1𝑦2−𝑦1𝑥2+𝑧1𝑟2)k.(4)
Such an operation can form efficient interaction among feature
components in the quaternion field, and can thus be utilized to facil-
itate a higher degree of freedom for learning models in representing
complex data couplings.
 
299KDD ’24, August 25–29, 2024, Barcelona, Spain Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, & Yiu-ming Cheung
ĂͿ,ŝĞƌĂƌĐŚŝĐĂůŽƵƉůŝŶŐŶĐŽĚŝŶŐYƵĂƚĞƌŶŝŽŶ^ƉĂĐĞ
Quaternion  
Projection
yxz
r

ďͿYƵĂƚĞƌŶŝŽŶZĞƉƌĞƐĞŶƚĂƚŝŽŶ>ĞĂƌŶŝŶŐ
Quaternion Graph Encoders
ˆAM W
Quaternion 
EmbeddingQuaternion 
Featurerz
y
n
ˆdx
M
Process
Explanation
Hamilton 
ProductOutputs
ˆ
ˆi
Qt i
iHCE Features ˆ
ˆAdjacency Matrix Aˆ (, )A
 ˆ
 Heterogeneous Data Graph
1ro
rroX
1
Categoricalr
no
1ro
NumericalValue-level
Heterogeneous DFeature-level P
Object-level A1x
2x
4x
5x6x3x
u
A;
 ;
(| |)ˆ
klKLAA 
 (
kl
KL
 ) ( ˆ Tr( )eΞ DAΞ    
 (
(
e
 
Tr(
1C2C
3C
C
3
C
3
2
C
2
yxz
r
ˆ
ˆi
i
Hamilton 
Product
M
MRaw InputsdA1A}
1x
nx
ĐͿ:ŽŝŶƚ>ŽƐƐĂŶĚ^ƉĞĐƚƌĂůůƵƐƚĞƌŝŶŐ
Figure 1: Overview of the proposed QGRL. Heterogeneous data is first encoded into a more informative attributed graph
G={A,ˆX}called Heterogeneous Data Graph (HDG) through the proposed Hierarchical Coupling Encoding (HCE) strategy. Then
a multi-view projection is performed to convert the attributes of HDG into the quaternion space for quaternion representation
learning. The obtained quaternion embedding Ξis reconstructed to form adjacency matrix ¯Aas the decoding operation. Finally,
Ξoutput by the trained QGRL model is utilized for spectral clustering.
4 PROPOSED METHOD
In this section, we first introduce Hierarchical Coupling Encoding
(HCE) strategies to comprehensively encode the complex relation of
heterogeneous feature data, and then present the proposed Quater-
nion Graph Representation Learning (QGRL) for clustering. The
overall pipeline of QGRL is shown in Figure 1.
4.1 HCE: Hierarchical Coupling Encoding
There are four types of coupling of heterogeneous feature data: 1)
value-level coupling, i.e., the coupling among possible values within
a categorical feature, 2) feature-level coupling, i.e., the coupling
between interdependent features, 3) heterogeneous coupling, i.e.,
the coupling between different types of features, and 4) object-
level coupling, i.e., the coupling among data objects reflected by
their similarities. By properly encoding these couplings, a coupling
learning can be facilitated with a deep representation framework.
In this subsection, we introduce the proposed coupling encoding
strategies in the above-mentioned hierarchies.
4.1.1 Value-level coupling. The occurrence probabilities of possible
valuesO𝑟={𝑜𝑟
1,𝑜𝑟
2,...,𝑜𝑟
𝜐𝑟}of a categorical feature 𝑨𝑟can be
viewed as a series of probabilities:
I𝑟={𝑃𝑟
𝑖|𝑖=1,...,𝜐𝑟} (5)where𝑃𝑟
𝑖is the occurrence probability of possible value 𝑜𝑟
𝑖in𝑨𝑟:
𝑃𝑟
𝑖=𝛿({𝑨𝑟}𝑛
1=𝑜𝑟
𝑖)
𝛿({𝑨𝑟}𝑛
1≠Null). (6)
Here,𝛿({𝑨𝑟}𝑛
1=𝑜𝑟
𝑖)counts the occurrence frequency of 𝑜𝑟
𝑖in
feature value set{𝑨𝑟}𝑛
1, and𝛿({𝑨𝑟}𝑛
1≠Null) counts the number
of non-empty values in {𝑨𝑟}𝑛
1which is usually equal to 𝑛. Note
that we use the uppercase 𝑃𝑟
𝑖here to distinguish the occurrence
probability of a value from the conditional probability that will be
presented in the following. Since the probabilities satisfy:
𝜐𝑟∑︁
𝑖=1𝑃𝑟
𝑖=1,
encoding the feature values by the occurrence probabilities of the
corresponding possible values can surely capture the value-level
couplings within each feature.
4.1.2 Feature-level coupling. The original features of a real data set
are usually interdependent to a certain extent. To represent such
inter-feature relations, we also define the Conditional Probability
Distribution (CPD) of a feature 𝑨𝑚given a possible value 𝑜𝑟
𝑖from
another feature 𝑨𝑟as a𝜐𝑚-dimensional vector:
𝑷𝑚|𝑟
𝑖=[𝑝(𝑜𝑚
1|𝑜𝑟
𝑖),𝑝(𝑜𝑚
2|𝑜𝑟
𝑖),...,𝑝(𝑜𝑚
𝜐𝑚|𝑜𝑟
𝑖)]⊤, (7)
 
300QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain
where the conditional probability 𝑝(𝑜𝑚
𝑗|𝑜𝑟
𝑖)is computed by:
𝑝(𝑜𝑚
𝑗|𝑜𝑟
𝑖)=𝜎(X𝑚
𝑗∩X𝑟
𝑖)
𝜎(X𝑟
𝑖). (8)
Here,X𝑟
𝑖={𝒙𝑙|𝑥𝑟
𝑙=𝑜𝑟
𝑖,𝑙=1,2,···,𝑛}is a subset ofX, which
contains all the data objects with their 𝑟-th values equal to 𝑜𝑟
𝑖. The
function𝜎(·)counts the cardinality of a set. With 𝑷𝑚|𝑟
𝑖, we can
encode a value 𝑜𝑟
𝑖according to different features 𝑨𝑚∈A{𝑐}to
preserve the inter-feature dependence.
4.1.3 Heterogeneous coupling. The above feature-level coupling
encoding treats categorical features uniformly according to the
CPDs. However, for heterogeneous feature data, heterogeneity
of the distance structures of numerical and categorical features
have not been represented yet. To effectively connect heteroge-
neous features while preserving their intrinsic distance structure,
we propose to project categorical feature values onto a series of
one-dimensional spaces, and then encode the categorical values
according to their locations after the projection.
Remark 1. Connection of heterogeneous features. The reason
to project the categorical values onto one-dimensional spaces is to
unify the categorical and numerical features by letting them reflect
distances in the same manner. In this way, the basis of appropriately
representing the coupling of the heterogeneous features is formed.
The projection is performed according to a commonly used cat-
egorical feature distance metric based on CPDs defined in Sec-
tion 4.1.2, where the distance between two possible values 𝑜𝑟
𝑖and
𝑜𝑟
𝑗of a feature 𝑨𝑟is computed according to each of the categorical
featuresA{𝑐}, which can be written as:
𝑑(𝑜𝑟
𝑖,𝑜𝑟
𝑗)=∑︁
𝑨𝑚∈A{𝑐}𝑷𝑚|𝑟
𝑖−𝑷𝑚|𝑟
𝑗. (9)
With this distance definition, we can project all the 𝜐𝑟possible
values of feature 𝑨𝑟onto each of the 𝜐𝑟(𝜐𝑟−1)/2one-dimensional
spaces spanned by the corresponding pairs of possible values. That
is, given a one-dimensional space R𝑟
𝑖𝑗spanned by two possible
values𝑜𝑟
𝑖and𝑜𝑟
𝑗, projection point of a value 𝑜𝑟
𝑡can be determined
by computing its distance to 𝑜𝑟
𝑖(or to𝑜𝑟
𝑗) in the spaceR𝑟
𝑖𝑗by:
𝜙(𝑜𝑟
𝑡,𝑜𝑟
𝑖;R𝑟
𝑖𝑗)=|𝑑(𝑜𝑟
𝑡,𝑜𝑟
𝑖)2−𝑑(𝑜𝑟
𝑡,𝑜𝑟
𝑗)2+𝑑(𝑜𝑟
𝑖,𝑜𝑟
𝑗)2|
2·𝑑(𝑜𝑟
𝑖,𝑜𝑟
𝑗)(10)
following the Pythagorean theorem. For more projection details,
readers can refer to [ 42]. After projecting all the 𝜐𝑟possible values,
distance between each pair of the possible values in R𝑟
𝑖𝑗is obtained,
and we organize the distances as a symmetric matrix D𝑟
𝑖𝑗∈R𝜐𝑟×𝜐𝑟
with its(𝑡,𝑙)-th entry D𝑟
𝑖𝑗(𝑡,𝑙)indicating the distance between 𝑜𝑟
𝑡
and𝑜𝑟
𝑙in the projection space R𝑟
𝑖𝑗.
Remark 2. Comprehensiveness of projection. Each categorical
feature 𝑨𝑟is represented as a series of 𝜐𝑟(𝜐𝑟−1)/2one-dimensional
distance structures from different endogenous views formed by dif-
ferent pairs of possible values. Combined with Remark 1, we know
that the projection informatively preserves the intrinsic relationship
among possible values, while the form of one-dimensional embedding
ensures a homogeneous connection with numerical features.4.1.4 Encoding of couplings. With the above-mentioned three types
of coupling encoding, all the categorical features A{𝑐}are repre-
sented to higher dimensions. Specifically, given the 𝑙-th value of a
categorical feature 𝑨𝑟satisfying𝑎𝑟
𝑙=𝑜𝑟
𝑖,𝑎𝑟
𝑙will be encoded into a
vector by concatenating its three types of coupling encoding as:
ˆ𝒂𝑟
𝑙=[𝑃𝑟
𝑖,𝑷1|𝑟
𝑖,𝑷2|𝑟
𝑖,...
|          {z          }
𝑑{𝑐},D𝑟
11(𝑖,·),D𝑟
12(𝑖,·),...
|                     {z                     }
𝜐𝑟(𝜐𝑟−1)/2] (11)
where𝑃𝑟
𝑖indicates the value-level coupling, 𝑷1|𝑟
𝑖,𝑷2|𝑟
𝑖,... is the
feature-level coupling, and D𝑟
11(𝑖,·),D𝑟
12(𝑖,·),... stands for the het-
erogeneous coupling. Notation D𝑟
𝑖𝑗(𝑡,·)indicates the 𝑡-th row of
matrix D𝑟
𝑖𝑗defined in Section 4.1.3. By encoding each feature value
inA{𝑐}, the encoded categorical feature set can be denoted as
ˆA{𝑐}, and the whole feature set is updated to ˆA=ˆA{𝑐}∪A{𝑢}.
Accordingly, we denote the object set corresponding to ˆAasˆX.
We have presented the encoding of the three types of couplings
so far, i.e., value-level, feature-level, and heterogeneous couplings.
The last object-level coupling encoding is performed by construct-
ing a fully connected graph on the data objects, which will be
separately discussed in the next subsection.
4.2 HDG: Heterogeneous Data Graph
Construction
To construct an HDG on the data objects, we first define the object-
level distance between two objects 𝒙𝑎and𝒙𝑏as L2 norm by:
Ψ(𝒙𝑎,𝒙𝑏)=[Φ1(𝑥1
𝑎,𝑥1
𝑏),Φ2(𝑥2
𝑎,𝑥2
𝑏),...,Φ𝑑(𝑥𝑑
𝑎,𝑥𝑑
𝑏)]⊤2,(12)
where Φ𝑟(𝑥𝑟𝑎,𝑥𝑟
𝑏)is the distance reflected by the 𝑟-th feature. To
achieve a more reasonable distance measurement on heterogeneous
feature data, we adopt graph-based unified dissimilarity proposed
in [39] to compute Φ𝑟(𝑥𝑟𝑎,𝑥𝑟
𝑏). Suppose𝑥𝑟𝑎=𝑜𝑟
𝑖and𝑥𝑟
𝑏=𝑜𝑟
𝑗for
𝑨𝑟∈A{𝑐}, then the distance Φ𝑟(𝑥𝑟𝑎,𝑥𝑟
𝑏)can be written as:
Φ𝑟(𝑥𝑟
𝑎,𝑥𝑟
𝑏)= 
𝑑∑︁
𝑚=1𝜙𝑟|𝑚(𝑜𝑟
𝑖,𝑜𝑟
𝑗)·𝜔𝑟|𝑚,if𝑨r∈A{c}
|𝑥𝑟
𝑎−𝑥𝑟
𝑏|, if𝑨r∈A{u}(13)
where
𝜙𝑟|𝑚(𝑜𝑟
𝑖,𝑜𝑟
𝑗)=∥|𝑷𝑚|𝑟
𝑖−𝑷𝑚|𝑟
𝑗|∥1
2(14)
is the dissimilarity between 𝑜𝑟
𝑖and𝑜𝑟
𝑗reflected by 𝑨𝑚, and𝜔𝑟|𝑚
weights the importance of 𝑨𝑚in indicating the distances of val-
ues from 𝑨𝑟, which can be specified by users, and can also be
computed according to the interdependence between 𝑨𝑚and𝑨𝑟.
Following [ 39], we discretize each of the numerical features into
five equal-length intervals, and then treat the discretized features as
categorical features to complete the computation of Eq. (14), as 𝑨𝑚
could be a numerical feature. Although the value-level distances
in the two cases are with different formats in Eq. (13), they are
unified from the perspective of transformation cost computed by
the Earth Mover’s Distance (EMD). Due to space limitation, we
refer the readers to [ 39] for more details about the unification and
weights computation.
 
301KDD ’24, August 25–29, 2024, Barcelona, Spain Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, & Yiu-ming Cheung
Remark 3. Rationality of HDG construction. In Section 4.1.4,
an informative coupling encoding ˆXhas been obtained, which can be
utilized to compute object-level distances based on Euclidean distance.
The reasons why we instead adopt the graph-based unified dissimilar-
ity upon the original Xfor HDG construction are two-fold: 1) it treats
the heterogeneous numerical and categorical features in a unified
way to avoid information loss, and 2) categorical features have been
greatly expanded in ˆX, directly calculating distances upon it will lead
to overemphasis on the categorical features.
After computing the distances between each pair of objects by
Eq. (12), an adjacency matrix A∈R𝑛×𝑛is obtained with its (𝑖,𝑗)-th
entry equals to Ψ(𝒙𝑖,𝒙𝑗). So far we have completed the encoding
for all four types of couplings, which can be compactly represented
by the constructed HDG G={A,ˆX}. ThenGis treated as the
input of the proposed representation learning model, which will be
detailed in the following subsections.
4.3 QGRL: Quaternion Graph Representation
Learning
To convert the attributes ˆXof the constructed attributed graph G
into quaternion space, a learnable quaternion projection module is
designed to project ˆXinto quaternion-value space via:
F𝑖(ˆX;WP
𝑖,BP
𝑖)=WP
𝑖ˆX+BP
𝑖(15)
whereF𝑖(·)is the linear projection function w.r.t. different quater-
nion components, i.e.,𝑖∈{𝑟,𝑥,𝑦,𝑧}. Here we use the superscript
Pto distinguish the learnable parameters of the projection phase
from the following parameters of the quaternion encoding phase in-
dicated by H. Instead of compressing the features in the real-value
space, the quaternion projection aims to informatively convert
features into the four-view quaternion-value space Hto facilitate
representation learning with a higher degree of freedom. After the
projection, the encoded quaternion feature is formulated as:
M=M𝑟+M𝑥i+M𝑦j+M𝑧k, (16)
where M∈H𝑛×(4×ˆ𝑑)denotes the quaternion feature matrix.
To learn the interdependencies between different quaternion
components, we propose to use a quaternion graph representation
encoder to capture the relations between quaternion embeddings:
Hℎ(ˆA,Mℎ;WH
ℎ)=𝜑(ˆA·Mℎ⊗WH
ℎ), (17)
where𝜑(·)is the ReLU function, ˆAdenotes the normalized Laplacian
matrix of A,ℎindexes to the number of encoder layers, and the
symbol⊗indicates the Hamilton product, which can be defined as:
M⊗WH=M𝑟
M𝑥
M𝑦
M𝑧⊤WH𝑟−WH𝑥−WH𝑦−WH𝑧
WH𝑥 WH𝑟−WH𝑧 WH𝑦
WH𝑦 WH𝑧 WH𝑟−WH𝑥
WH𝑧−WH𝑦 WH𝑥 WH𝑟⊤
,(18)
where WHdenotes the learnable parameters. For simplicity, we
omit the subscript ℎofMandWHin the Eq. (18).
After propagating the feature representations with relations be-
tween quaternion components, the quaternion feature embeddings
are further aggregated into a single feature matrix, which is uti-
lized to calculate the loss of Graph reconstruction and clusteringin the next stage. In practice, the quaternion feature embedding
aggregation process can be formulated as:
𝚵=Re(M𝐿)⊛Im(M𝐿) (19)
where Re(·)and Im(·)are the real and imaginary parts of M𝐿,
respectively. M𝐿is the output of the last encoder layer H𝐿(·), and
the symbol ⊛indicates quaternion fusion operation, which takes
an average of the four quaternion embedding components to form
a compact embedding for the downstream graph construction and
clustering. Then, we reconstruct the adjacency matrix by:
¯A=𝚵·𝚵⊤(20)
where ¯Adenotes the reconstructed matrix, which will be directly
utilized to compute the training loss.
4.4 Joint Optimization for Graph
Reconstruction and Clustering
Inspired by [ 19], we propose to jointly optimize unsupervised het-
erogeneous feature representation learning and spectral clustering
by integrating the Kullback-Leibler (KL) divergence and relaxed
spectral clustering objective as the loss function. Intuitively, the KL
loss encourages the model to learn feature embeddings by recov-
ering the original graph connectivity. The clustering loss aims to
learn discriminative embeddings and facilitate clustering analysis
by preserving the similarity between close objects in the embedding
space. Concretely, the overall loss function is defined as:
L=L𝑘𝑙+𝛼L𝑟𝑒𝑔+𝛽L𝑒, (21)
where𝛼and𝛽are the hyper-parameters indicating loss weights.
L𝑟𝑒𝑔is the regularization term. In our implementation, L1 regular-
ization is adopted to penalize the complexity of the model. The KL
lossL𝑘𝑙is expressed as:
L𝑘𝑙=1
𝑛2𝑛∑︁
𝑖=1𝑛∑︁
𝑗=1ˆA𝒊𝒋log1
¯A𝒊𝒋, (22)
where ˆAand ¯Adenote the normalized Laplacian adjacency matrix
and the reconstructed adjacency matrix, respectively.
Similar to Graph Laplacian Eigenmaps [ 5], we introduce the loss
termL𝑒into the loss function to preserve the graph property and
penalize the quaternion embedding with higher similarity but lower
connectivity in the graph. Formally, L𝑒is defined as:
L𝑒=tr(Ξ⊤ D−¯AΞ), (23)
where Ddenotes the degree matrix, ¯Ais the reconstructed adjacency
matrix, Ξcontains the learned quaternion embeddings, and tr(·)
computes the trace of matrix.
The KL lossL𝑘𝑙encourages the consensus fusion of currently
learned embedding and the original graph structure information in
the process of reconstruction, while the eigenmap loss L𝑒makes
the model prefer sparse graph structure with higher feature simi-
larity of the connected nodes, which is consistent with the spectral
clustering objective, and can thus be treated as a relaxed spectral
clustering objective. In summary, they complement each other in
terms of informativeness and clustering friendliness of the learned
embeddings. Finally, the ultimate embeddings Ξoutput by the
trained QGRL model is treated as the input of spectral clustering to
obtain a certain number of clusters [22].
 
302QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Statistics of ten data sets, where 𝑛,𝑑𝑐,𝑑𝑢, and𝑘∗indi-
cate the number of objects, categorical features, numerical
features, and clusters, respectively.
No. Data set Abbrev. 𝑛𝑑𝑐𝑑𝑢𝑘∗
1 Heart Failure HF 299 5 7 2
2 Breast Cancer BC 286 5 4 2
3 Autism-Adolescent AA 104 7 2 2
4 Mammographic MM 961 4 1 2
5 Zoo ZO 101 16 0 7
6 Tic-Tac-Toe TTT 958 9 0 2
7 Glass Identification GI 214 0 9 6
8 Yeast YE 1484 0 8 10
9 Iris II 150 0 4 3
10 Wine WI 178 0 13 3
5 EXPERIMENTS
In this section, we first outline the experimental settings, and then
demonstrate experimental results with discussions.
5.1 Experimental Settings
5.1.1 Experiments Summary. In this section, we conduct four sets
of experiments to comprehensively evaluate our proposed cluster-
ing framework. The experimental designs are as follows.
•Clustering Performance Evaluation: To demonstrate the su-
perior performance of our proposed clustering framework,
we compare the clustering performance of ten methods that
include traditional, representative, and advanced counter-
parts on the heterogeneous feature data sets.
•Significance Test: To illustrate the superiority of our method,
we perform significance tests in comparison with the state-
of-the-art methods.
•Ablation Study: To demonstrate the effectiveness of the pro-
posed hierarchical coupling encoding (HCE) and quaternion
graph representation learning (QGRL) for heterogeneous
data clustering, we conduct ablation studies by incremen-
tally adding the proposed modules to a baseline model.
•Visualization Analysis: To intuitively demonstrate the effec-
tiveness of QGRL, 𝑡-SNE is utilized to compare the cluster
effect of embeddings generated by different models.
5.1.2 Data sets. Ten widely used public data sets are employed in
the experiments. Most data sets are with a mixture of categorical
and numerical features, and we also adopt pure categorical and
pure numerical data sets for more comprehensive evaluation. All
the data sets are from the UCI machine learning repository [ 25],
and their detailed statistics are provided in Table 2.
5.1.3 Compared Methods and Implementation Details. Our method
was compared with ten clustering counterparts, which can be
broadly categorized into: 1) Traditional methods: K-Means [ 12] and
Spectral clustering (SC) [ 22], 2) Representation learning methods:
GAE [ 15] and VGAE [ 15], 3) State-of-the-art graph representation
learning methods: ARGAE/ARVGAE [ 28], CCGC [ 36], DFCN [ 32],
DAEGC [ 35], and the EGAE [ 37]. Some GCN methods require graph
structure as input. For these methods, we apply one-hot encodingto the categorical features, thereby expanding the original features.
We then construct the adjacency matrix based on the Euclidean
distance between the expanded features. Our QGRL use the HDG
G={A,ˆX}constructed by our HCE as input. The quaternion graph
encoder is implemented by stacking two graph convolutional layers.
For hyper-parameter settings, the length of latent vectors produced
by the two layers is set at 1024 and 512, respectively. In practice,
QGRL is pre-trained with only KL divergence and regularization
loss and then trained in 50 epochs to obtain the representation and
clustering results. After the training of each epoch, we perform
matrix decomposition on the reconstructed adjacency matrix, and
then extract the final clustering results from the matrix. To mitigate
the effects of randomness, the mean and standard deviation are
computed after running each method 10 times.
The settings of the compared methods are briefly described be-
low. For GAE and VGA, we perform 200 epochs of unsupervised
training, then use K-Means to cluster the generated embedding.
The ARGAE, ARVGAE, CCGC, DFCN, DAEGC, and EGAE are im-
plemented following their source code and original settings. All the
experiments are implemented in PyTorch 1.8.0 and performed on a
machine with an NVIDIA A5000 GPU, 64GB RAM.
5.1.4 Evaluation Metrics. In this paper, the clustering performance
of the methods is evaluated by using three metrics [ 46],i.e., Accu-
racy (ACC), Normalized Mutual Information (NMI), and Adjusted
Rand Index (ARI). For the significance test discussed in Section 5.3,
Wilcoxon signed-ranks test[ 10] is adopted for pairwise comparison
of the counterparts.
5.2 Clustering Performance Evaluation
Based on the results in Table 3, our observations are as follows.
1) The proposed QGRL outperforms nearly all listed methods in
three metrics across ten data sets, with an average ranking of 1.1,
which illustrates its effectiveness in clustering in general.
2) On the most challenging heterogeneous feature data sets, i.e.,
HF, BC, AA, and MM data sets, QGRL obviously outperforms its
counterparts. Particularly, on the MM data set, QGRL achieves
great improvements in comparison with the other methods. This
indicates that the HCE encoding can effectively represent complex
relation information of the heterogeneous features.
3) Although QGRL focuses on representation learning of hetero-
geneous feature data, its clustering performance is still extremely
competitive on pure categorical data sets, i.e., ZO and TTT. This is
because the proposed HCE comprehensively performs four types of
hierarchical encoding. For categorical data, encoding of the value-
level, feature-level, and object-level couplings still acts to informa-
tively represent the implicit complex relation within the categorical
data, thus boosting the clustering performance.
4) QGRL outperforms the other counterparts on the TTT data set.
TTT contains samples indicating the chessboard status in tic-tac-
toe game, and there are two meaningful clusters in this data set, i.e.,
winning and losing. QGRL performs well because its HCE consid-
ers the correlation between categorical features (i.e . Section 4.1.2).
Since each placement position on the chessboard is a feature in the
data set, the encoding of feature couplings is equivalent to consider-
ing the correlation between chessboard placement positions, which
is crucial to distinguishing between winning and losing. Moreover,
 
303KDD ’24, August 25–29, 2024, Barcelona, Spain Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, & Yiu-ming Cheung
Table 3: Average clustering performance on ten heterogeneous benchmark data sets evaluated by ACC, NMI, and ARI (Mean ±Std),
with best results bolded and Average Performance Ranks (AR) noted.
Data
set Metric K
-Means SC GAE VGAE ARGAE ARVGAE CCGC DFCN DAEGC EGAE QGRL
(ours)
HFA
CC 0.6221±0.00
0.5485±0.00 0.6361±0.01 0.5438±0.03 0.6408±0.01 0.6890±0.00 0.6355±0.00 0.5742±0.02 0.6789±0.01 0.6900±0.02 0.7137±0.03
NMI 0.0025±0.00
0.0020±0.00 0.0074±0.01 0.0028±0.01 0.0035±0.01 0.0267±0.00 0.0075±0.00 0.0002±0.00 0.0101±0.01 0.0296±0.03 0.1092±0.03
ARI 0.0175±0.00
0.0042±0.00 0.0336±0.00 -0.0004±0.01 0.0220±0.00 0.0331±0.00 0.0337±0.00 -0.0004±0.00 0.0085±0.01 0.0263±3.36 0.1663±0.04
BCA
CC 0.5140±0.00
0.5210±0.00 0.7098±0.00 0.5430±0.05 0.7227±0.01 0.7154±0.01 0.7098±0.01 0.5531±0.05 0.6993±0.00 0.6647±0.06 0.7203±0.00
NMI 0.0021±0.00
0.0014±0.00 0.0684±0.00 0.0048±0.01 0.0826±0.01 0.0756±0.01 0.0753±0.01 0.0166±0.02 0.0039±0.00 0.0266±0.03 0.0866±0.01
ARI -0.0025±0.00
-0.0012±0.00 0.1468±0.00 0.0020±0.02 0.1688±0.01 0.1572±0.01 0.1524±0.01 0.0164±0.04 -0.0040±0.00 0.0362±6.01 0.1700±0.00
AAA
CC 0.5096±0.00
0.6058±0.00 0.5192±0.00 0.5481±0.02 0.5663±0.01 0.5394±0.01 0.5788±0.02 0.5144±0.02 0.6125±0.01 0.6192±0.01 0.6356±0.01
NMI 0.0006±0.00
0.0274±0.00 0.0014±0.00 0.0102±0.01 0.0066±0.01 0.0002±0.00 0.0154±0.00 0.0006±0.00 0.0250±0.00 0.0271±0.01 0.0471±0.01
ARI -0.0090±0.00
0.0353±0.00 -0.0127±0.00 -0.0063±0.01 0.0041±0.01 -0.0087±0.01 0.0148±0.01 -0.0081±0.01 0.0080±0.69 0.0162±0.02 0.0375±0.02
MMA
CC 0.6855±0.00
0.5398±0.00 0.5337±0.00 0.5119±0.01 0.6306±0.05 0.5559±0.01 0.6831±0.02 0.6855±0.00 0.5306±0.05 0.6852±0.01 0.8296±0.00
NMI 0.1102±0.00
0.0328±0.00 0.0076±0.00 0.0007±0.00 0.0577±0.04 0.0103±0.01 0.1009±0.01 0.1065±0.00 0.0105±0.03 0.1030±0.01 0.3487±0.01
ARI 0.1367±0.00
0.0054±0.00 0.0035±0.00 -0.0003±0.01 0.0752±0.05 0.0117±0.01 0.1338±0.02 0.1367±0.00 0.0109±0.03 0.1362±0.01 0.4340±0.01
ZOA
CC 0.7238±0.07
0.6109±0.01 0.5960±0.01 0.3772±0.03 0.7624±0.00 0.7634±0.01 0.6842±0.03 0.7168±0.08 0.3647±0.03 0.6891±0.03 0.8020±0.02
NMI 0.7761±0.05
0.7237±0.01 0.5479±0.02 0.2357±0.03 0.7299±0.00 0.7315±0.03 0.6340±0.02 0.7068±0.05 0.1534±0.07 0.7111±0.03 0.8212±0.02
ARI 0.6755±0.11
0.5515±0.01 0.3811±0.01 0.0797±0.03 0.6416±0.00 0.6446±0.02 0.5015±0.04 0.5744±0.10 -0.0410±0.01 0.5687±0.05 0.7733±0.03
T
TTA
CC 0.5783±0.00
0.5167±0.00 0.5992±0.00 0.5295±0.01 0.6069±0.03 0.5461±0.01 0.6033±0.02 0.5335±0.01 0.6277±0.06 0.6531±0.01 0.9574±0.03
NMI 0.0074±0.00
0.0002±0.00 0.0110±0.00 0.0016±0.01 0.0151±0.01 0.0011±0.01 0.0126±0.01 0.0025±0.00 0.0014±0.00 0.0084±0.01 0.7633±0.10
ARI 0.0195±0.00
-0.0001±0.00 0.0308±0.00 0.0027±0.01 0.0388±0.02 0.0045±0.00 0.0343±0.01 0.0040±0.01 -0.0014±0.00 0.0092±0.02 0.8381±0.09
GIA
CC 0.5421±0.00
0.4720±0.00 0.4346±0.00 0.2785±0.02 0.4650±0.01 0.3902±0.01 0.4477±0.01 0.4556±0.01 0.3617±0.03 0.5505±0.01 0.5509±0.03
NMI 0.4207±0.01
0.2914±0.00 0.3016±0.00 0.0405±0.01 0.3168±0.01 0.1949±0.01 0.3003±0.02 0.3148±0.02 0.0615±0.06 0.3763±0.01 0.5004±0.06
ARI 0.2655±0.01
0.1594±0.00 0.1993±0.00 0.0029±0.01 0.2180±0.01 0.1289±0.00 0.1728±0.03 0.1770±0.01 0.0132±0.01 0.2587±0.01 0.3476±0.08
YEA
CC 0.3620±0.01
0.30.93±0.00 0.2323±0.01 0.1495±0.01 0.2573±0.01 0.2849±0.00 0.2240±0.03 0.2174±0.03 0.3127±0.00 0.3638±0.01 0.4422±0.01
NMI 0.2642±0.01 0.0082±0.00
0.0874±0.00 0.0144±0.01 0.0914±0.01 0.0895±0.00 0.0758±0.02 0.1614±0.01 0.0110±0.00 0.2575±0.01 0.2182±0.01
ARI 0.1395±0.01
-0.0025±0.00 0.0416±0.00 -0.0005±0.00 0.0474±0.00 0.0515±0.00 0.0326±0.02 0.0578±0.01 0.0005±0.00 0.1395±0.01 0.1431±0.01
IIA
CC 0.8933±0.00
0.9067±0.00 0.8000±0.00 0.3947±0.03 0.8367±0.03 0.8827±0.04 0.9253±0.01 0.8373±0.01 0.3422±0.01 0.9167±0.01 0.9627±0.02
NMI 0.7582±0.00
0.8057±0.00 0.5684±0.00 0.0254±0.03 0.7235±0.03 0.7654±0.05 0.7756±0.02 0.7012±0.01 0.0267±0.01 0.7786±0.02 0.8695±0.04
ARI 0.7302±0.00
0.7592±0.00 0.5475±0.00 0.0076±0.02 0.6427±0.05 0.7185±0.08 0.7951±0.01 0.6354±0.02 0.0003±0.00 0.7795±0.03 0.8923±0.04
WIA
CC 0.4933±0.01
0.4944±0.00 0.3837±0.01 0.4062±0.03 0.5034±0.02 0.4382±0.00 0.5775±0.01 0.5000±0.01 0.3933±0.02 0.7472±0.00 0.9646±0.00
NMI 0.1123±0.01
0.1412±0.00 0.0279±0.01 0.0266±0.02 0.0923±0.01 0.0500±0.00 0.1629±0.03 0.0875±0.01 0.0221±0.01 0.3655±0.01 0.8742±0.01
ARI 0.1169±0.01
0.1250±0.00 -0.0089±0.00 0.0062±0.03 0.1094±0.01 0.0399±0.00 0.1720±0.02 0.1052±0.00 -0.0016±0.00 0.3899±0.00 0.8945±0.01
AR - 5.43
7.13 7.53 9.53 4.93 6.26 5.06 6.80 8.40 3.60 1.10
a unified distance metric is adopted to perform a more reasonable
graph construction (i.e . Section 4.2), which closely connects dif-
ferent chessboard statuses corresponding to the same cluster (i.e .
winning or losing) to reduce the difficulty of clustering.
5) On pure numerical data sets, i.e., GI, YE, II, and WI, QGRL still
demonstrates its superiority as the adopted quaternion representa-
tion learning works well in learning the relationship among features
and provides a discriminative data representation for clustering.
5.3 Significance Study
We conduct significance tests between our method and other meth-
ods using the Wilcoxon signed rank test based on the clustering
performance reported in Table 3. We show the p-values of compar-
isons in Figure 2, and a darker color represents a smaller p-value. It
can be observed that all comparisons demonstrate the significant
superiority of our method at a 99% confidence level. It is notewor-
thy that the Wilcoxon signed rank test is performed based on the
performance ranking of methods, so the p-values have a relatively
large granularity. For example, since our method ranks first in ARI
performance across all the data sets, the calculated p-value is always
consistent at 0.00195 in Figure 2.
5.4 Ablation Study of QGRL
The results of ablation studies are illustrated in Table 4, where “Base-
line” refers to the GAE model proposed in [ 15] combined with the
KL loss (Section 4.4), one-hot encoding (Section 5.1.3), and Euclidean
0.00195 0.00586 0.001950.00195 0.00195 0.001950.00195 0.00195 0.001950.00195 0.00195 0.001950.00391 0.00195 0.001950.00195 0.00195 0.001950.00195 0.00195 0.001950.00195 0.00195 0.001950.00195 0.00195 0.001950.00195 0.00586 0.00195
ACC NMI ARIK-MeansSCGAEVGAEARGAEARVGAECCGCDFCNDAEGCEGAE
p-value0.0019400.0027240.0035080.0042920.0050760.005860Figure 2:𝑝-values of Wilcoxon signed rank test in comparing
our method against the other methods on three metrics.
distance-based graph construction (Remark 3). Three models are
further considered, including 1) “Baseline+HCE”: Baseline with the
HCE proposed in Section 4.1 and 4.2, 2) “Baseline+QRL”: Baseline
with the proposed QRL presented in Section 4.3, and 3) The full
model of our proposed QGRL. The discussions are as follows.
 
304QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Clustering performance comparison of ablated ver-
sions of the proposed QGRL model.
Data
set Metric Baseline
Baseline+HCE Baseline+QRL QGRL (ours)
HFA
CC 0.6261±0.01
0.7080±0.00 0.6528±0.02 0.7137±0.03
NMI 0.0218±0.00
0.0606±0.00 0.0404±0.02 0.1092±0.03
ARI 0.0513±0.01
0.1310±0.00 0.0811±0.03 0.1663±0.04
BCA
CC 0.7028±0.00
0.5245±0.00 0.5462±0.01 0.7203±0.00
NMI 0.0627±0.00
0.0181±0.00 0.0085±0.01 0.0866±0.01
ARI 0.1366±0.00
-0.0077±0.00 0.0060±0.01 0.1700±0.00
AAA
CC 0.5202±0.01
0.5347±0.02 0.6346±0.01 0.6356±0.01
NMI 0.0013±0.00
0.0073±0.01 0.0449±0.02 0.0471±0.01
ARI -0.0123±0.00
0.0052±0.01 0.0386±0.02 0.0375±0.02
MMA
CC 0.5499±0.00
0.5346±0.02 0.6819±0.00 0.8296±0.00
NMI 0.0077±0.00
0.0073±0.01 0.0997±0.00 0.3487±0.01
ARI 0.0089±0.00
0.0052±0.01 0.1313±0.00 0.4340±0.01
ZOA
CC 0.6505±0.01
0.5357±0.02 0.7030±0.03 0.8020±0.02
NMI 0.6345±0.04
0.0075±0.01 0.7571±0.02 0.8212±0.02
ARI 0.4988±0.03
0.0053±0.01 0.5617±0.02 0.7733±0.03
T
TTA
CC 0.6034±0.01
0.6138±0.00 0.6153±0.02 0.9574±0.03
NMI 0.0124±0.00
0.0190±0.00 0.0461±0.03 0.7633±0.10
ARI 0.0340±0.01
0.0444±0.00 0.0525±0.02 0.8381±0.09
1) Baseline+HCE does not achieve obvious improvements in
comparison with Baseline. This is because HCE focuses on repre-
senting hierarchical couplings, which is the basis for quaternion
representation. By only combining with the baseline GAE, the cou-
pling relationships cannot be adequately learned, and may thus
lead to redundant information in the final representation.
2) Baseline+QRL is observed to deliver superior results on some
data sets, such as AA and MM, which shows that the proposed
QRL can learn rich feature representations when dealing with data
of limited information richness. However, the model performance
deteriorated on the BC data set. This could be attributed to the
inadequate encoding of the hierarchical coupling for heterogeneous
features without HCE, consequently, preventing the proper learning
of relationships and patterns in QRL.
3) The advantage of QGRL is not obvious compared to Base-
line+QRL on AA data set. The reason is that most of the categorical
features of AA are Boolean-valued and are relatively independent
of each other. This makes the HCE of QGRL have no significant im-
pact. In addition, the QRL module commonly adopted by QGRL and
Baseline+QRL dominates the performance on AA dataset as QRL
acts to enhance the degree of representation learning freedom. This
is why Baseline+QRL and QGRL have similar performance, and
why they both significantly outperform the two versions without
QRL, i.e. Baseline and Baseline+HCE, on AA data set.
4) The complete QGRL achieves significant improvements on
almost all the data sets, which indicates that the proposed HCE and
QRL modules can cooperate to achieve better representation. That
is, HCE provides an informative encoding basis as the model input,
while QRL demonstrates powerful coupling learning on the basis.
As a result, a comprehensive and discriminative representation is
obtained for accurate clustering.
5.5 Visualization Analysis
In this part, we visualize the cluster distribution of embeddings
obtained by two representative methods, i.e., CCGC and EGAE,
QGRL (ours) EGAE CCGCFigure 3:𝑡-SNE visualization of representations obtained by
CCGC, EGAE, and the proposed QGRL on MM data set. Pink
and blue mark the objects with the “true” cluster labels.
and the proposed QGRL via 𝑡-SNE [ 33], to intuitively show the
effectiveness and superiority of QGRL. From Figure 3, it can be seen
that QGRL better reveals the intrinsic clustering distributions than
the two compared methods.
6 CONCLUDING REMARKS
This paper proposes a novel quaternion graph representation learn-
ing method called QGRL for heterogeneous feature data clustering.
To more comprehensively learn the representations of heteroge-
neous numerical and categorical features, the complex hierarchical
couplings are carefully encoded into the form of attributed graphs
to uncover the value-level, feature-level, heterogeneous, and object-
level relation in the data. To obtain a more concise form of the data
representations for clustering, we integrate the powerful quater-
nion representation learning and spectral clustering objective to
perform unsupervised representation learning. It turns out that
clustering-friendly representations can be adequately learned on
the basis of informative heterogeneous feature encoding. As QGRL
is designed for representing heterogeneous features, it is surely
feasible for any-feature-type data, including numerical data, cate-
gorical data, and heterogeneous data. Extensive comparison and
ablation studies illustrate the superiority of QGRL in clustering.
While QGRL demonstrates its effectiveness, it is not exempt
from limitations, including the requirement of a given number of
clusters, relatively high computation cost, inflexibility in processing
dynamic or streaming data, etc. Our further research will focus on
deriving a lightweight network structure from QGRL, proposing a
continual learning strategy for the model training, and integrating
the search for an optimal number of clusters into the learning
process. Moreover, transferring QGRL to other tasks driven by
heterogeneous feature data, e.g., classification, anomaly detection,
dimensionality reduction, etc., is also promising.
ACKNOWLEDGEMENTS
This work was supported in part by the National Natural Sci-
ence Foundation of China (NSFC) under grants: 62102097 and
62302104, the NSFC/Research Grants Council (RGC) Joint Research
Scheme under the grant N_HKBU214/21, the Natural Science Foun-
dation of Guangdong Province under grants: 2023A1515012855,
2023A1515012884, and 2022A1515011592, the General Research
Fund of RGC under grants: 12201321, 12202622, and 12201323, the
RGC Senior Research Fellow Scheme under grant SRFS2324-2S02,
and the Science and Technology Program of Guangzhou under
grant SL2023A04J01625.
 
305KDD ’24, August 25–29, 2024, Barcelona, Spain Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, & Yiu-ming Cheung
REFERENCES
[1]Amir Ahmad and Lipika Dey. 2007. A method to compute distance between two
categorical values of same attribute in unsupervised learning for categorical data
set.Pattern Recognition Letters 28, 1 (2007), 110–118.
[2]Amir Ahmad and Shehroz S. Khan. 2019. Survey of State-of-the-Art Mixed Data
Clustering Algorithms. IEEE Access 7 (2019), 31883–31902.
[3]Madhavi Alamuri, Bapi Raju Surampudi, and Atul Negi. Jul. 2014. A survey of dis-
tance/similarity measures for categorical data. In Processings of the International
Joint Conference on Neural Networks. 1907–1914.
[4]Vladimir Batagelj, Hans-Hermann Bock, Anuska Ferligoj, and Ales Ziberna (Eds.).
2006. Data Science and Classification.
[5]Mikhail Belkin and Partha Niyogi. 2001. Laplacian Eigenmaps and Spectral
Techniques for Embedding and Clustering. In Proceedings of the 14th International
Conference on Neural Information Processing Systems. 585–591.
[6]Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. Apr. 2020.
Structural Deep Clustering Network. In Proceedings of The Web Conference 2020.
1400–1410.
[7]Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz,
and Samy Bengio. 2015. Generating sentences from a continuous space. arXiv
preprint arXiv:1511.06349 (2015).
[8]Yiu-ming Cheung and Hong Jia. 2013. Categorical-and-numerical-attribute data
clustering based on a unified similarity metric without knowing cluster number.
Pattern Recognition 46, 8 (2013), 2228–2238.
[9]Danilo Comminiello, Marco Lella, Simone Scardapane, and Aurelio Uncini. Oct.
2019. Quaternion Convolutional Neural Networks for Detection and Localization
of 3D Sound Events. In Processings of the Conference on Acoustics, Speech and
Signal Processing. 8533–8537.
[10] Janez Demsar. 2006. Statistical Comparisons of Classifiers over Multiple Data
Sets. Journal of Machine Learning Research 7 (2006), 1–30.
[11] Chase J. Gaudet and Anthony S. Maida. Jul. 2018. Deep Quaternion Networks. In
Processings of the International Joint Conference on Neural Networks. 1–8.
[12] Greg Hamerly and Charles Elkan. 2003. Learning the k in k-means. In Proceedings
of the 16th International Conference on Neural Information Processing Systems.
281–288.
[13] Dino Ienco, Ruggero G. Pensa, and Rosa Meo. Sep. 2009. Context-Based Distance
Learning for Categorical Data Clustering. In Processings of the 8th International
Symposium on Intelligent Data Analysis, Vol. 5772. 83–94.
[14] Hong Jia, Yiu-ming Cheung, and Jiming Liu. 2016. A New Distance Metric for
Unsupervised Learning of Categorical Data. IEEE Transactions on Neural Networks
and Learning Systems 27, 5 (2016), 1065–1079.
[15] T. N. Kipf. 2016. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308
(2016).
[16] Thomas N. Kipf and Max Welling. Apr. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In Processings of the 5th International Conference
on Learning Representations.
[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Dec. 2012. ImageNet
Classification with Deep Convolutional Neural Networks. In Proceedings of the
26th International Conference on Neural Information Processing Systems. 1106–
1114.
[18] Si Quang Le and Tu Bao Ho. 2005. An association-based dissimilarity measure
for categorical data. Pattern Recognition Letters 26, 16 (2005), 2549–2557.
[19] Xuelong Li, Hongyuan Zhang, and Rui Zhang. 2022. Adaptive Graph Auto-
Encoder for General Data Clustering. IEEE Transactions on Pattern Analysis and
Machine Intelligence 44, 12 (2022), 9725–9732.
[20] Dekang Lin. Jul. 1998. An Information-Theoretic Definition of Similarity. In
Processings of the 15th International Conference Machine Learning. 296–304.
[21] Yue Liu, Jun Xia, Sihang Zhou, Siwei Wang, Xifeng Guo, Xihong Yang, Ke Liang,
Wenxuan Tu, Stan Z. Li, and Xinwang Liu. 2022. A Survey of Deep Graph
Clustering: Taxonomy, Challenge, and Application. CoRR abs/2211.12875 (2022).
[22] Ulrike Luxburg. 2007. A tutorial on spectral clustering. Statistics and Computing
17 (2007), 395–416.
[23] Panagiotis Mandros, David Kaltenpoth, Mario Boley, and Jilles Vreeken. 2020.
Discovering Functional Dependencies from Mixed-Type Data. In Processings of
the 26th Conference on Knowledge Discovery and Data Mining. 1404–1414.
[24] Razvan V. Marinescu, Arman Eshaghi, Marco Lorenzi, Alexandra L. Young, Neil P.
Oxtoby, Sara Garbarino, Sebastian J. Crutch, and Daniel C. Alexander. 2019. DIVE:
A spatiotemporal progression model of brain pathology in neurodegenerative
disorders. NeuroImage 192 (2019), 166–177.
[25] Kolby Nottingham Markelle Kelly, Rachel Longjohn. 2023. The UCI Machine
Learning Repository. https://archive.ics.uci.edu.
[26] Nairouz Mrabah, Mohamed Bouguessa, Mohamed Fawzi Touati, and Riadh Ksan-
tini. 2023. Rethinking Graph Auto-Encoder Models for Attributed Graph Cluster-
ing.IEEE Transactions on Knowledge and Data Engineering 35, 9 (2023), 9037–9053.
[27] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Q. Phung. Nov. 2021. Quater-
nion Graph Neural Networks. In Processings of the Asian Conference on Machine
Learning, Vol. 157. 236–251.[28] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang.
Jul. 2018. Adversarially Regularized Graph Autoencoder for Graph Embedding.
InProcessings of the 27th International Joint Conference on Artificial Intelligence.
2609–2615.
[29] Titouan Parcollet, Mohamed Morchid, and Georges Linarès. 2020. A survey of
quaternion neural networks. Artificial Intelligence Review 53, 4 (2020), 2957–2982.
[30] Titouan Parcollet, Ying Zhang, Mohamed Morchid, Chiheb Trabelsi, Georges
Linarès, Renato de Mori, and Yoshua Bengio. Sep. 2018. Quaternion Convolutional
Neural Networks for End-to-End Automatic Speech Recognition. In Processings
of the 19th Conference of the International Speech Communication Association.
22–26.
[31] Yuhua Qian, Feijiang Li, Jiye Liang, Bing Liu, and Chuangyin Dang. 2016. Space
structure and clustering of categorical data. IEEE Transactions on Neural Networks
Learning Systems 27, 10 (2016), 2047–2059.
[32] Wenxuan Tu, Sihang Zhou, Xinwang Liu, Xifeng Guo, Zhiping Cai, En Zhu, and
Jieren Cheng. 2021. Deep Fusion Clustering Network. In Proceedings of the 35th
AAAI Conference on Artificial Intelligence. 9978–9987.
[33] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of Machine Learning Research 9, 86 (2008), 2579–2605.
[34] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
2008. Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th International Conference on Machine Learning. 1096–1103.
[35] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi
Zhang. Aug. 2019. Attributed Graph Clustering: A Deep Attentional Embedding
Approach. In Processings of the 28th International Joint Conference on Artificial
Intelligence. 3670–3676.
[36] Xihong Yang, Yue Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Qun Zheng,
Xinwang Liu, Liming Fang, and En Zhu. Feb. 2023. Cluster-Guided Contrastive
Graph Clustering Network. In Processings of the 37th AAAI Conference on Artificial
Intelligence. 10834–10842.
[37] Hongyuan Zhang, Pei Li, Rui Zhang, and Xuelong Li. 2023. Embedding Graph
Auto-Encoder for Graph Clustering. IEEE Transactions on Neural Networks and
Learning Systems 34, 11 (2023), 9352–9362.
[38] Yiqun Zhang and Yiu-ming Cheung. 2022. Learnable Weighting of Intra-Attribute
Distances for Categorical Data Clustering with Nominal and Ordinal Attributes.
IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7 (2022), 3560–
3576.
[39] Yiqun Zhang and Yiu-ming Cheung. 2023. Graph-Based Dissimilarity Measure-
ment for Cluster Analysis of Any-Type-Attributed Data. IEEE Transactions on
Neural Networks and Learning Systems 34, 9 (2023), 6530–6544.
[40] Yiqun Zhang and Yiu-ming Cheung. Oct. 2018. Exploiting Order Information
Embedded in Ordered Categories for Ordinal Data Clustering. In Processings of the
24th International Symposium on Methodologies for Intelligent Systems, Vol. 11177.
247–257.
[41] Yiqun Zhang, Yiu-ming Cheung, and Kay Chen Tan. 2020. A Unified Entropy-
Based Distance Metric for Ordinal-and-Nominal-Attribute Data Clustering. IEEE
Transactions on Neural Networks and Learning Systems 31, 1 (2020), 39–52.
[42] Yiqun Zhang, Yiu-ming Cheung, and An Zeng. Jul. 2022. Het2Hom: Repre-
sentation of Heterogeneous Attributes into Homogeneous Concept Spaces for
Categorical-and-Numerical-Attribute Data Clustering. In Processings of the 31st
International Joint Conference on Artificial Intelligence. 3758–3765.
[43] Yiqun Zhang and Yiu-ming Cheung. 2020. A new distance metric exploiting
heterogeneous interattribute relationship for ordinal-and-nominal-attribute data
clustering. IEEE Transactions on Cybernetics 52, 2 (2020), 758–771.
[44] Yiqun Zhang and Yiu-ming Cheung. 2020. An ordinal data clustering algorithm
with automated distance learning. In Processings of the 34th AAAI Conference on
Artificial Intelligence, Vol. 34. 6869–6876.
[45] Zewen Zheng, Guoheng Huang, Xiaochen Yuan, Chi-Man Pun, Hongrui Liu, and
Wing-Kuen Ling. 2023. Quaternion-Valued Correlation Learning for Few-Shot
Semantic Segmentation. IEEE Transactions on Circuits and Systems for Video
Technology 33, 5 (2023), 2102–2115.
[46] Sheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Zhao Li, Jiajun Bu,
Jia Wu, Xin Wang, Wenwu Zhu, and Martin Ester. 2022. A Comprehensive
Survey on Deep Clustering: Taxonomy, Challenges, and Future Directions. CoRR
abs/2206.07579 (2022).
[47] Chengzhang Zhu, Longbing Cao, and Jianping Yin. 2022. Unsupervised Hetero-
geneous Coupling Learning for Categorical Representation. IEEE Transactions on
Pattern Analysis and Machine Intelligence 44, 1 (2022), 533–549.
[48] Xuanyu Zhu, Yi Xu, Hongteng Xu, and Changjian Chen. Sep. 2018. Quaternion
Convolutional Neural Networks. In Processings of the 15th European Conference
on Computer Vision, Vol. 11212. 645–661.
 
306