Neural Optimization with Adaptive Heuristics for Intelligent
Marketing System
Changshuai Wei∗
LinkedIn Corporation
Seattle, USA
chawei@linkedin.comBenjamin Zelditch
LinkedIn Corporation
New York, USA
bzelditch@linkedin.comJoyce Chen
LinkedIn Corporation
Sunnyvale, USA
joychen@linkedin.com
Andre Assuncao Silva T Ribeiro
LinkedIn Corporation
Sunnyvale, USA
aribeiro@linkedin.comJingyi Kenneth Tay
LinkedIn Corporation
Sunnyvale, USA
ktay@linkedin.comBorja Ocejo Elizondo
LinkedIn Corporation
Sunnyvale, USA
bocejo@linkedin.com
Sathiya Keerthi Selvaraj
LinkedIn Corporation
Sunnyvale, USA
keselvaraj@linkedin.comAman Gupta
LinkedIn Corporation
Sunnyvale, USA
amagupta@linkedin.comLicurgo Benemann De Almeida
LinkedIn Corporation
New York, USA
lalmeida@linkedin.com
ABSTRACT
Computational marketing has become increasingly important in
today’s digital world, facing challenges such as massive hetero-
geneous data, multi-channel customer journeys, and limited mar-
keting budgets. In this paper, we propose a general framework
for marketing AI systems, the Neural Optimization with Adaptive
Heuristics (NOAH) framework. NOAH is the first general frame-
work for marketing optimization that considers both to-business
(2B) and to-consumer (2C) products, as well as both owned and
paid channels. We describe key modules of the NOAH framework,
including prediction, optimization, and adaptive heuristics, provid-
ing examples for bidding and content optimization. We then detail
the successful application of NOAH to LinkedIn’s email marketing
system, showcasing significant wins over the legacy ranking sys-
tem. Additionally, we share details and insights that are broadly
useful, particularly on: (i) addressing delayed feedback with life-
time value, (ii) performing large-scale linear programming with
randomization, (iii) improving retrieval with audience expansion,
(iv) reducing signal dilution in targeting tests, and (v) handling
zero-inflated heavy-tail metrics in statistical testing.
CCS CONCEPTS
•Computing methodologies →Machine learning; •Applied
computing→Marketing; •Mathematics of computing →
Probability and statistics ;Mathematical optimization .
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671591KEYWORDS
Computational Marketing, Linear Programming, Lifetime Value,
Audience Expansion
ACM Reference Format:
Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T
Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Sathiya Keerthi Selvaraj,
Aman Gupta, and Licurgo Benemann De Almeida. 2024. Neural Optimization
with Adaptive Heuristics for Intelligent Marketing System. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671591
1 INTRODUCTION
Compared with traditional media marketing, online digital mar-
keting has become increasingly important for advertisers and mar-
keters. With online digital marketing, marketers can reach cus-
tomers more directly with personalized marketing messages. Many
of the current major internet companies build advertising plat-
forms to better connect advertisers/marketers to customers, typi-
cally through auction or bidding systems. From a marketer’s per-
spective, there are paid social channels (e.g., Facebook) and paid
search channels (e.g., Google). Besides these paid channels, there are
also owned channels like email, where marketers can connect with
customers directly. There is much literature on building machine
learning (ML) systems from ads platform’s perspective; however,
few papers have discussed ML and optimization systematically
from the marketer’s perspective. In fact, there are many unique
challenges when building an intelligent marketing system from the
marketer’s perspective:
(1)The data can be massive and heterogeneous. Since there
are multiple marketing channels, the granularity of data
from different channels are highly heterogeneous. For owned
channels like email, marketers can collect member-level user
feedback, whereas for paid channels like Google Search, mar-
keters can only collect aggregate-level information, e.g., at
the keyword or campaign level.
5938
KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
(2)The objective can be challenging to define. The mar-
keting life-cycle can be quite different across products. For
many to-consumer (2C) products, the interaction is typically
online and self-served, so the life-cycle from receiving the
marketing message to conversion can be just a few days.
For to-business (2B) products, the life-cycle can span weeks
to months, including activities such as lead generation and
sales involvement. As a result, these ML models face issues
associated with delayed feedback.
(3)The action space is complex. There are different types of
actions to optimize the marketing outcome. These include
(but are not limited to) at what time and frequency the mar-
keting message should be sent, what content (e.g., picture,
language) to use for the message, which audience to target,
how much to bid for the marketing campaign(s) on vari-
ous ads platforms. Unifying these actions under a general
optimization framework would help build a more efficient
marketing system.
(4)There are various types of constraints. Marketing typi-
cally comes with cost. Some costs are explicit (e.g., billing
from paid channels), while others are implicit (e.g., com-
plaints or unsubscriptions from users on owned channels).
Besides these cost constraints, there are also other opera-
tional constraints within marketing systems, e.g., capping
on message frequency, minimum volume for various type
of products. An intelligent marketing system should be able
to determine optimal actions while respecting these con-
straints.
To address these challenges, we propose Neural Optimization
with Adaptive Heuristics (NOAH), a general framework for intelli-
gent marketing systems. Before we introduce the framework, we
briefly review related ML and optimization works in the marketing
industry and for general recommender systems.
1.1 Related Work
1.1.1 ML in Marketing Industry. The use of ML in marketing has
been fertile over the last decade, given the wealth of data naturally
produced by marketing and the capacity of ML tools/methods to
transform data into information that can be used for marketing
optimization and decision-making. ML solutions have been pro-
posed and/or applied to multiple domains, for instance: customer
segmentation in industries such as retail [ 14,32], hospitality [ 1,44]
and banking [ 33,45]; customer lifetime value (LTV) [ 19] in indus-
tries ranging from gaming [ 11], to e-commerce [ 22], to subscription
services [ 28]; marketing attribution, in both multi-touch attribution
[38] and marketing mix modeling [ 23]; the optimization of different
marketing channels, such as email [ 31,40], paid search bidding
[16,18], display advertising [ 36,41], among others. For a compre-
hensive review of the use of ML in marketing, we recommend the
work presented in [26, 29].
1.1.2 Optimization in Recommender Systems. A commonly used
optimization approach for Recommender systems (RecSys) is the
ranking heuristic, where items are ranked by their relevance scores
and top-ranked items are shown/sent to users. To balance different
(and sometimes conflicting) objectives, the relevance score is often
a linear combination of multiple predicted objective values. While alarge amount of research effort has gone into applying ML and AI to
predicting the individual objective values, less work has been done
on how to choose the weights for an optimized linear combination.
In recent years, Linear Programming (LP) has been emerging in
RecSys for its capability of trading off multiple objectives.
LP has been studied closely since the 1940s, and several commer-
cial solvers such as Gurobi [ 20] and GLPK [ 30] can give high-quality
solutions for small-to-moderate instances in reasonable time frames.
However, they cannot scale to industrial-scale problems with bil-
lions or trillions of variables due to memory or computational
constraints. One approach to handle large-scale LPs is to devise
algorithms where certain key quantities can be computed in a dis-
tributed manner. The alternating direction method of multipliers
(ADMM) [ 7] is one such approach; [ 46] details how ADMM can be
implemented successfully for industrial-scale problems. Another
approach is the primal-dual hybrid gradient (PDHG) method [ 9] for
a class of problems in convex optimization. [ 2] applies the PDHG
method to a saddle-point formulation of LP and adds enhancements
such as diagonal preconditioning, presolving, adaptive step sizes
and adaptive restarting to make the algorithm more efficient in
practice.
1.2 Our Approach
NOAH is a general framework for intelligent marketing systems
which consists of three major modules: 1) prediction module, 2)
optimization module and 3) adaptive heuristics module. The predic-
tion module provides predictions of business metrics given possible
marketing actions. The optimization module chooses marketing
actions that optimize the relevant business metrics. The adaptive
heuristics module connects various components of the system to
form a better feedback loop. We first go over more details of the
NOAH framework in Section 2, including the three NOAH modules
and some illustrative high-level examples. We then walk through a
large scale application to LinkedIn’s marketing engine in Section 3.
We also share production A/B test win results with some learnings
in Section 4 and conclude in Section 5. We would like to highlight
two major contributions of the paper:
(1)As far as we know, this is the first general framework for
marketing optimization that takes into account both 2B and
2C products, as well as both owned and paid channels.
(2)This is a large-scale application to LinkedIn marketing that
led to a major business win, and we report details of our
learnings and experiences in this paper.
These contributions imply that our approach offers great scope for
improving marketing optimization in general and hence is broadly
useful.
2 NOAH FRAMEWORK
In this section, we will first give a brief overview of LinkedIn’s mar-
keting system. We then introduce the details of different modules
and components of the NOAH framework, followed by applica-
tion guidelines and high-level examples for bidding and content
optimization.
5939Neural Optimization with Adaptive Heuristics for Intelligent Marketing System KDD ’24, August 25–29, 2024, Barcelona, Spain
2.1 Marketing Ecosystem at LinkedIn
LinkedIn is the world’s largest professional network and has prod-
ucts that serve both 2C and 2B needs. As a result, the marketing
system (Figure 1) needs to promote not only 2C products like Pre-
mium but also various 2B products in Talents, Learning, Ads and
Sales lines of business. The marketing system is a heterogeneous
system that involves both a first-party system and integration with
third-party tools. Marketers interact with the marketing system to
decide the optimal marketing actions for these 2B and 2C products,
including whom the marketing message should be sent to, the best
timing and frequencies for the messages to be sent, what bidding
price or target the keywords or campaigns should be set at (on paid
channels). Once the actions are decided from the Marketing Engine,
the marketing messages are delivered via various owned or paid
channels.
Paid channels include paid search, paid social and other display
ads on third-party platforms, where LinkedIn participates as an ad-
vertiser in ad bidding through various auction mechanisms. Mean-
while, with owned channels, LinkedIn can promote its products on
LinkedIn itself or use the email channel to have direct connections
with customers. When a user clicks on a marketing message from
one of these channels, they are directed to an optimized landing
page where more detailed product information is shown. For 2C
products, the landing page is typically on LinkedIn’s main website
and the purchase can be typically completed in a self-serve manner.
On the other hand, for 2B products the landing page is typically
Linkedin’s 2B micro-site (e.g., https://business.linkedin.com/) where
an inquiry form can be generated for LinkedIn sales personnel to
follow up.
Figure 1: Overview of LinkedIn’s Marketing System.
2.2 NOAH Modules
In order to have a general framework that can handle different types
of entities and possible actions in owned and paid channels, we first
abstract out two definitions: marketing units andmarketing actions .
Amarketing unit is an entity that a marketing action can be applied
on. For example, in email marketing where we can directly connect
with members and collect engagement data, the marketing unit is
a member and a marketing action (to be optimized) can be whether
to send a particular email to the member. In paid search channels,
assuming marketers can get keyword-level data, the marketing unit
can be a keyword, and a marketing action can be the bid amount
for the keyword.Figure 2 shows the overall diagram of the NOAH system. The
core engine (inside the large dashed green box) consists of three key
modules: (i) prediction, (ii) optimization, and (iii) adaptive heuristics.
We first introduce the components outside the core engine so that
the context, inputs and outputs around the core engine are clear.
NOAH is a human-in-the-loop system. Marketers with expert
knowledge define appropriate marketing units and marketing ac-
tions in the relevant databases. A candidate retrieval process re-
trieves the appropriate set of marketing units and actions from
the databases and feeds them to the core engine. Marketers also
provide input parameters via an UI that can guide the core engine’s
behavior. User engagement feedback and other contextual informa-
tion are also collected as input to the core engine. The core engine
processes these inputs through the three key modules to generate
optimized marketing actions.
Figure 2: NOAH Framework.
2.2.1 Prediction Module. In the prediction module, we want to
predict performance metrics for each marketing unit given some
marketing action. Denoting the feature vector for marketing unit 𝑢
as𝑥𝑢, feature vector for marketing action 𝑎as𝑥𝑎and a metric as 𝑦,
the prediction module builds a functional mapping:
𝑦=𝑓(𝑥𝑢,𝑥𝑎,𝑥𝑐),
where𝑥𝑐represents a feature vector for contextual information
other than marketing unit and action (e.g., location, device). The
metrics are typically marketing objectives and constraints related
to user feedback. For example, we can build a functional mapping
for four metrics: click through rate (CTR), conversion rate, profit
per conversion and cost per click (CPC). With these metrics, we
can use the chain rule to connect these conditional probabilities to
estimate the profit per impression and cost per impression, which
can be further used in the optimization module. The functional
mapping𝑓(·)can be learnt from user feedback data. Depending on
the amount of data available and the maturity of the system, we can
use different supervised learning methods to estimate 𝑓(·). If the
amount of data is small and the project is at the initial stage, a simple
5940KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
regression model is often enough. If more data is available and the
project is at the initial stage, an off-the-shelf gradient boosted tree
[17] or XGBoost model [ 12] can give decent performance. If data is
abundant and the project is in the iteration phase, we have found
that deep neural networks give the best performance due to their
flexible architecture. For example, transformer architectures can be
used to generate embeddings for historical marketing interaction
with users. If multiple metrics are being modeled, multi-task learn-
ing architectures like multi-gate mixture-of-experts (MMOE [ 25])
or entire space multi-task model (ESMM [ 27]) can be employed to
exploit the relationship among multiple metrics during training.
One key characteristic of the prediction module is the inclusion
of the marketing action. If the metric prediction is conditioned
on the marketing action, the prediction module can then generate
metric estimates for all possible marketing actions, from which the
optimization module can choose the best action. However, historical
data may not contain all possible marketing actions for each unit,
which may generate bias in inference [ 10]. To reduce the bias,
various causal ML methods can be used so that the 𝑓(·)learnt
better reflects the causal relationship [15, 21, 39].
2.2.2 Optimization Module. With the functional mapping of mar-
keting actions and performance metrics, we then need to choose
the best action for a given marketing unit in the optimization mod-
ule. The simplest, widely adopted approach is a ranking approach:
choose the action with the largest 𝑦value, i.e., the largest estimated
metric. While it can often work, this ranking approach ignores
many limitations or constraints that can lead to sub-optimal and
sometimes unacceptable performance.
To address various constraints across marketing units, we can
frame the problem as a constrained optimization problem. For each
metric𝑘=0,1,...,𝐾 , let𝑦𝑘=𝑓𝑘(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐)denote the estimated
value of metric 𝑘for marketing unit 𝑢under action 𝑎𝑢. Without
loss of generality, let 𝑦0be the primary metric to be optimized, and
let the remaining 𝑦𝑘(𝑘≥1) be metrics that are subject to various
limitations or business constraints with upper bound constraints
𝐶𝑘. Assuming no interaction effects across marketing units, we can
formulate the optimization problem:
max𝑎𝑢∑︁
𝑢𝑓0(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐), (1)
s.t.∑︁
𝑢𝑓𝑘(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐)≤𝐶𝑘,∀1≤𝑘≤𝐾. (2)
The above optimization problem can be relaxed and solved with
efficient linear programming (LP) algorithms in many cases; we
detail some examples in Sections 2.3 and 3.3. When latency is a
strong requirement, some compromises on accuracy need to be
made, for example, to use yesterday’s dual solution for today’s pri-
mal calculation. Fortunately, for marketing AI use cases, latency is
not a major problem as the systems are mostly offline. For exam-
ple, in email marketing, marketers control the time and frequency
and typically limit the frequency to avoid spamming users. In paid
media, marketers only need to change the non-impression-level
bid, e.g., cost-per-click (CPC) bid as the impression-level response
is taken care of by the ads platform. For these reasons, LP is well
suited for marketing AI use cases.Besides the marketing cost and business constraints on the
"space" dimensions, there are implicit constraints on the "time"
dimension, particularly for each marketing unit. The time window
to attract and convert a potential user is typically limited. Moreover,
a marketing action taken at a given time point can impact the status
and best marketing action to take at the next time point. We can
leverage Reinforcement Learning (RL) to address optimization on
"time" dimension. Contextual bandits can be used to balance the
exploration and exploitation. Q-Learning can be used to help find
an optimal sequence of marketing actions to maximize value for a
given marketing unit.
To summarize, the optimization module addresses the resource
limitation problem. As a general principle, we recommend LP when
the resource limitation across marketing units is strict and there
are less frequent marketing interactions, and recommend RL when
there are more frequent marketing interactions and less constraints
across marketing units (or these constraints can be taken care of by
other parts of the system). We note that these two techniques are not
mutually exclusive and can be combined. For example, we can focus
on "space" constraints and use an LP formulation, while simplifying
the time horizons resolution to an LTV prediction. We can also
focus on "time" horizons and use an Q-learning formulation, while
transforming the cost constraints implicitly to reward shaping. We
argue that the former combination is often preferred in marketing,
since marketing use cases are typically less frequent interaction
with strict cost constraints.
2.2.3 Adaptive Heuristics Module. Adaptive heuristics (AH) are
lightweight components that help the whole system run stably and
coherently, each of which may have some prediction and optimiza-
tion functionality. We here briefly discuss two main classes of AHs
and provide more details in Appendix B.
One important class of AH is Feedback Loop Controllers for busi-
ness constraints (green boxes as shown in Figure 2). Since the mar-
ket is dynamic and the marketing AI system can have errors (e.g.,
prediction error), the observed cost 𝐶′may be greater (violation) or
much less (under-utilization) than the budget 𝐶. To minimize under-
or over-utilization, we can continuously monitor the performance
and build a lightweight controller, 𝐶∗=𝑔(𝐶,𝐶′), to update𝐶∗as
the input to the optimization module instead of the original 𝐶.
Another important class of AH is Hierarchical Optimization, so
that the aggregate-level constraints propagate to lower level. For ex-
ample, a macro-level optimization with a forecasting model and an
LP can be built to allocate marketing budget to different marketing
channels and products.
2.3 Guidelines and Examples
NOAH is a general framework for marketing AI systems. Ideally,
we would like to perform joint optimization for multiple types of
marketing actions, e.g., product, timing, frequencies and content.
However, this is often infeasible due to reasons such as infrastruc-
ture constraints, operational convenience, or limitations in the ML
models or optimization solvers. As a workaround, we can use the
same NOAH framework and perform optimization in a cascad-
ing/sequential manner for different types of marketing actions. For
example, we can first decide the product for each marketing unit,
then optimize the timing and frequencies, and finally choose the
5941Neural Optimization with Adaptive Heuristics for Intelligent Marketing System KDD ’24, August 25–29, 2024, Barcelona, Spain
best content. Another benefit of this muti-stage design is that it al-
lows for a flexible combination of different optimization techniques
for different actions. Here, we give examples for two different mar-
keting actions: bidding and content optimization.
2.3.1 Bidding Optimization Example. For bidding, advertisers can
receive hourly or daily ads winning performance metrics 𝑥𝑝, for
example, average position for each keyword, or impression winning
percentage for each campaign. One can build a series of models
to predict: number of clicks 𝑦clicks=𝑓clicks(𝑥𝑢,𝑥𝑝), cost per click
𝑦cpc=𝑓cpc(𝑥𝑢,𝑥𝑝), conversion rate 𝑦cvr=𝑓cvr(𝑥𝑢)and revenue
per conversion 𝑦rpc=𝑓rpc(𝑥𝑢). Note that we can assume 𝑦cvrand
𝑦rpcdo not depend on ads winning performance as they happen
after users click the ad and land on the landing page. Typically,
advertisers want to maximize revenue while maintaining a certain
level of return-on-ad-spend (ROAS). The corresponding constrained
optimization formulation could be:
max
𝑝(𝑢)∑︁
𝑢𝑦rev
𝑢,𝑝(𝑢),
s.t.Í
𝑢𝑦rev
𝑢,𝑝(𝑢)Í
𝑢𝑦cost
𝑢,𝑝(𝑢)≥𝐶𝑅𝑂𝐴𝑆,
where𝑝represents the index for categorized winning performance,
and
𝑦rev
𝑢,𝑝=𝑓clicks(𝑥𝑢,𝑥𝑝)𝑓cvr(𝑥𝑢)𝑓rpc(𝑥𝑢),
𝑦cost
𝑢,𝑝=𝑓clicks(𝑥𝑢,𝑥𝑝)𝑓cpc(𝑥𝑢,𝑥𝑝).
The constrained optimization problem can be solved with the ef-
ficient algorithm in Appendix A.1 if the scale is large. Once the
desired optimal 𝑥∗𝑝is obtained, we can calculate the optimal CPC
bid as𝑎∗𝑢=𝑓cpc(𝑥𝑢,𝑥∗𝑝), which can be directly used in first-price
auction platforms, or as the expected-cost-per-click (ECPC) bid in
second-price auction platforms.
2.3.2 Content Optimization Example. Content optimization can
happen at different stages across different marketing channels.
Without loss of generality, assume there is a slot on the landing
page where we can show different creatives 𝑎to different users 𝑢.
We can build an online high-dimensional contextual bandit model
based on following regression model:
𝜌−1(𝑦)=𝑥𝑇𝑤+𝜖, 𝜖∼𝑁(0,𝛽2),
where𝑥=(𝑥𝑢,𝑥𝑎,𝑥𝑢,𝑎)is feature vector that includes features for
user𝑥𝑢, features for creative 𝑥𝑎, and features for the user-creative
interaction𝑥𝑢,𝑎,𝜌(·)is a link function, and 𝑦is the observed reward.
Online updates for the posterior distribution of 𝑤𝑖∼𝑁(𝜇𝑤𝑖,𝜎2𝑤𝑖)
can be computed efficiently using factor graph and expectation
propagation (see algorithm details in Appendix A.2). Thompson
sampling can then be employed to obtain predicted rewards for
different creatives, and the user is shown the creative with the
largest reward on the landing pages.
3 EMAIL MARKETING APPLICATION
In this section, we report the details of a large-scale application
of the NOAH framework to LinkedIn’s email marketing engine,
including important innovations and learnings.3.1 Email System
We can think of the email marketing system as a two-stage recom-
mender system: candidate retrieval followed by email optimization
(Figure 5 in Appendix). In the candidate retrieval stage, marketers
create email marketing campaigns for different products and define
the audience list for each campaign. In the email optimization stage,
ML models are used to find the best marketing emails for a selected
group of members, so as to optimize metrics while minimizing
negative member experiences like spamming. The whole system
runs in batches, with emails sent out at regular time intervals (e.g.,
weekly). The system seeks to maximize value for LinkedIn while re-
specting multiple business constraints. Some constraints are global,
e.g., the total number of user unsubscriptions should be less than
a certain value each week, while other constraints are local, e.g.,
each member cannot receive more than certain number of emails
each week.
3.2 Prediction with LTV
LinkedIn faces distinct challenges in predicting marketing perfor-
mance due to the different nature of 2C and 2B segments, particu-
larly the extended decision-making processes and delayed feedback
for 2B products. To overcome this, we create separate models for
predicting short-term metrics and LTV, then integrate the results
to estimate the long-term metric. Here, the long-term metric 𝑞𝑡
is a numeric measure defined by marketers reflecting marketing
performance over an extended period 𝑡, with short-term metrics
representing observable metrics within a brief timeframe, like con-
version and unsubscription. Notably, conversion is typically defined
differently for 2B and 2C products. In 2B, it often refers to the sub-
mission of a qualified lead, whereas in 2C, a conversion can be
marked by a customer signing up for a free trial.
To predict the short-term metrics, we develop predictive models
forming a functional mapping:
𝑦conv
𝑢,𝑎=𝑓conv(𝑥𝑢,𝑥𝑎,𝑥𝑐),
𝑦unsub
𝑢,𝑎 =𝑓unsub(𝑥𝑢,𝑥𝑎,𝑥𝑐),
where𝑦convand𝑦unsubrepresent the predicted conversion prob-
abilities and unsubscription probabilities respectively, 𝑥𝑢denotes
the feature vectors of users capturing user profiles, demographics,
and other behavioral data, 𝑥𝑎symbolizes the marketing actions un-
dertaken, including the decision to send specific marketing emails
to certain members, and 𝑥𝑐represents other relevant contextual
information that might influence the metrics.
We have a dedicated model to predict the LTV associated with a
given conversion event. We calculate the LTVs as
𝑦ltv
𝑢,𝑝=𝑓ltv(𝑥𝑢,𝑥𝑝,𝑥𝑐)
where𝑝indexes the product that email campaign 𝑎tries to promote.
We model the real-valued 𝑦ltvusing regression equipped with a
gamma loss function, defined as
𝐿=∑︁
𝑖h
𝜇𝑖𝑘+𝑞12
𝑖exp(−𝜇𝑖)𝑘i
where𝑘is the shape parameter, 𝑖indexes the observations in the
training data, 𝑞12
𝑖is the observed 12-month long-term metric (the
ground truth value) and 𝜇𝑖=𝑙𝑜𝑔(𝑦ltv
𝑖). This is a long time window
5942KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
that makes recent positive engagement unavailable for training
(i.e., censored observations). In order to overcome this issue, we
make auxiliary LTV predictions at shorter time windows (e.g., 1,3,6
months) and use the output of those models as features for the main
12-month LTV prediction. We found that this strategy increases pre-
diction accuracy and allows the model to respond faster to evolving
business trends.
3.3 Constrained and Multi-Objective
Optimization
3.3.1 Formulating the Constrained Optimization Problem. In our
legacy system, a ranking heuristic was used in the email optimiza-
tion stage. For each member, all candidate emails were ranked by a
linear combination of 𝑦convand𝑦unsubwith fixed coefficient 𝜅, i.e.,
𝑦conv−𝜅𝑦unsub, and top-ranked emails were sent to the member
with a fixed frequency cap. This approach did not optimize the busi-
ness goal directly, and business constraints were only considered
implicitly.
Here we apply the constrained optimization formulation in Sec-
tion 2.2.2 to this problem. Let 𝑢=1,...,𝑈 and𝑗=1,...,𝐽 in-
dex the member and campaigns respectively, and let 𝑎𝑢,𝑗∈{0,1}
denote the binary action decision of whether to send email 𝑗to
member𝑢. We can estimate the total expected long-term metric
byÍ
𝑢,𝑗𝑎𝑢,𝑗𝑦conv
𝑢,𝑗𝑦ltv
𝑢,𝑝(𝑗), and the total expected unsubscriptions by
Í
𝑢,𝑗𝑎𝑢,𝑗𝑦unsub
𝑢,𝑗.
Assuming that (i) we allow a maximum of 𝐶fcapemails to be sent
to each member, (ii) we must send at least 𝐶2Band𝐶2Cemails for 2B
and 2C products respectively, and (iii) we must have no more than
𝐶unsub unsubscriptions, we can form the constrained optimization
problem as follows:
min𝑎𝑢,𝑗∑︁
𝑢,𝑗−𝑦conv
𝑢,𝑗𝑦ltv
𝑢,𝑝(𝑗)𝑎𝑢,𝑗,
s.t.∑︁
𝑢,𝑗𝑎𝑢,𝑗𝑦unsub
𝑢,𝑗≤𝐶unsub,
∑︁
𝑢∑︁
𝑗∈J2B−𝑎𝑢,𝑗≤−𝐶2B,
∑︁
𝑢∑︁
𝑗∈J2C−𝑎𝑢,𝑗≤−𝐶2C,
∑︁
𝑗𝑎𝑢,𝑗≤𝐶fcap,∀𝑢,
𝑎𝑢,𝑗∈{0,1},∀𝑢,𝑗.
where, J2BandJ2Cdenote the email campaigns for 2B and 2C
products respectively.
We recognize the optimization problem as an integer linear pro-
gram (ILP), which can be combinatorially difficult to solve [ 37].
Instead, we relax the constraints on the 𝑎𝑢,𝑗’s to allow them to take
on values in the interval [0,1]. The resulting problem is a linear
program (LP) which is tractable (details in Section 3.3.2). We can
interpret non-integer 𝑎𝑢,𝑗as the probability with which we should
send email𝑗to member𝑢.
3.3.2 Solving the Large-Scale LP. While there are open-source and
commercial solvers for solving LPs, the size of the LP for our ap-
plication is too large for these solvers. Recall that the optimizationvariable𝑎has dimension 𝑈𝐽, the product of the number of mem-
bers and the number of campaigns. For a large social network like
LinkedIn, we could have 10-100 millions of members and hundreds
of email campaigns, resulting in tens of billions of variables. To
solve the LP, we turn to DuaLip [4,34,35], a large-scale LP solver
that we developed at LinkedIn. We briefly outline the algorithm
here.
The LP in 3.3.1 can be written in the following form:
min𝑎𝑦⊤𝑎 𝑠.𝑡. 𝐷𝑎≤𝑏, 𝑎𝑢∈B𝑢, 𝑢∈{1,...,𝑈}, (3)
where𝑎=(𝑎1,1,···,𝑎1,𝐽,···,𝑎𝑈,𝐽)∈R𝑈𝐽is the vector of decision
variables,𝑦is the coefficient vector for the decision variables in
the objective, 𝐷is the design matrix in the constraints, 𝑏is the
constraint budget vector, and B𝑢are additional "simple" constraints,
i.e., it is efficient to compute the Euclidean projection onto B𝑢.
In particular, 𝑏=(𝐶unsub,−𝐶2B,−𝐶2C), and𝐷𝑎corresponds to
the left-hand side of the first three constraints from the previous
section.B𝑢corresponds to the remaining two constraints, i.e., box-
cut constraintsB𝑢={𝑎𝑢,𝑗: 0≤𝑎𝑢,𝑗≤1∀𝑗,Í
𝑗𝑎𝑢,𝑗≤𝐶fcap}.
Instead of solving the LP (3)directly, we solve a perturbed version
of the LP, which is a quadratic program (QP):
min𝑎𝑦⊤𝑎+𝛾
2𝑎⊤𝑎 𝑠.𝑡. 𝐷𝑎≤𝑏, 𝑎𝑢∈B𝑢, 𝑢∈{1,...,𝑈},(4)
where𝛾>0is a regularization hyperparameter. Dualizing just the
polyhedral constraint 𝐷𝑎≤𝑏leads to the partial Lagrangian dual
𝑔𝛾(𝜆)=min
𝑎∈Bn
𝑦⊤𝑎+𝛾
2𝑎⊤𝑎+𝜆⊤(𝐷𝑎−𝑏)o
,
whereB=Î𝑈
𝑢=1B𝑢. The problem is transformed to calculate
max𝜆𝑔𝛾(𝜆)such that𝜆≥0, assuming strong duality holds. Apply-
ing Danskin’s theorem [ 6], we know that 𝑔𝛾is differentiable and
can explicitly compute its derivative:
∇𝑔𝛾(𝜆)=𝐷ˆ𝑎(𝜆)−𝑏,
ˆ𝑎𝑢(𝜆)=ΠB𝑢
−1
𝛾(𝐷⊤𝜆+𝑦)𝑢
, (5)
where ΠB𝑢(·)is the Euclidean projection operator onto B𝑢, and
for a vector 𝑧with the same dimensions as decision variable 𝑎,𝑧𝑢
denotes the sub-vector with indices corresponding to member 𝑢.
Notice that the computation of ˆ𝑎(𝜆)can be parallelized across mem-
bers𝑢and the box-cut projection ΠB𝑢can be computed efficiently
by leveraging Wolfe’s algorithm [ 43]. As a result,∇𝑔𝛾(𝜆)can be
computed efficiently. This allows us to use first-order methods such
as accelerated gradient ascent [ 5] and L-BFGS [ 24] to obtain𝜆∗𝛾, the
dual value that optimizes the dual function 𝑔𝛾. From this optimal
dual value, we can obtain optimal primal values 𝑎∗𝛾via (5):
(𝑎∗
𝛾)𝑢=ˆ𝑎𝑢(𝜆∗)=ΠB𝑢
−1
𝛾(𝐷⊤𝜆∗+𝑦)𝑢
, 𝑢=1,...,𝑈.
While we obtain the optimal solution for the perturbed prob-
lem and not the original one, [ 4] shows that for sufficiently small
𝛾values, the perturbed solution is also optimal for the original
problem.
5943Neural Optimization with Adaptive Heuristics for Intelligent Marketing System KDD ’24, August 25–29, 2024, Barcelona, Spain
3.3.3 Practical Tips for Implementation. We describe a couple of
tips that make DuaLip converge much faster. (A) The choice of
𝛾is associated with a critical trade-off: DuaLip’s solving of (4)is
faster if𝛾is larger, whereas DuaLip’s approximate solution of (4)
in a given number of iterations is closer to the solution of (3)if𝛾
is smaller. For one run, we try a grid of 𝛾values and choose the
largest𝛾value such that the regularizer part of the objective is
small enough, i.e., the ratio,𝛾𝑎⊤𝑎
2|𝑦⊤𝑎|is smaller than some threshold,
say10−3. After finding such a 𝛾value, we fix it and use it for all
future production runs. (B) If 𝐷is ill-conditioned (which typically
happens when different constraints are on different scales), we can
re-scale the constraints, i.e., pre-multiply 𝐷and𝑏by a suitably
chosen diagonal matrix.
With Dualip, the primal solution will fall on a vertex of B𝑢with
high probability [ 35] when𝛾is sufficiently small. This means most
of the(𝑎∗𝛾)𝑢’s are already binary, i.e., in {0,1}. However, we still
need to handle the cases when they are between 0 and 1. Simple
rounding may violate the hard constraint 𝐶fcap. We devise a ran-
domization algorithm, basically sampling the campaign indexes
with probability calculated from the primal solution (Appendix A.3).
The randomization also provides the additional benefit of balancing
exploration and exploitation.
3.4 Audience Expansion
In the email marketing lifecycle, dedicated marketing teams from
each business unit provide a list of candidate members for each
email campaign. Historically, we have found these candidate mem-
ber lists conservative in scope. To address this, we augment the
provided lists with additional members through an Audience Ex-
pansion (AE) module in which we retrieve additional members
who resemble the members provided by the marketing teams and
append them to the candidate lists. We find such members via an
approximate nearest neighbors (ANN) algorithm.
Formally, letUdenote the overall member base at LinkedIn, let
Jdenote the set of marketing campaigns, and let Uo
𝑗⊂U denote
the set of original candidate members provided by a marketing
team for email marketing campaign 𝑗∈J. We leverage ANN via
locality-sensitive hashing [13] to produce a set
Uexpansion
𝑗⊆n
𝑢∈U−
𝑗:∃𝑢′∈Uo
𝑗,s.t.∥𝑥𝑢−𝑥𝑢′∥2≤𝛿o
,
where,U−
𝑗=U\Uo
𝑗and𝛿is a hyperparameter which controls the
size of the expansion audience. Ultimately, we combine the original
audience with expansion audience for each campaign and input
the expanded audience Ueinto our optimization engine, where
Ue=Ð
𝑗∈JUo
𝑗∪Uexpansion
𝑗.
We have found that as we relax 𝛿, there is a trade-off between
wider campaign reach through increased expansion audience size
and diminished incremental expansion audience quality. If 𝛿is
too large, a portion of the incremental expansion members added
through AE are so dissimilar to the campaign’s original members
that they are never chosen by the LP to receive the marketing
campaign. Moreover, the computational runtime of both Audience
Expansion and the LP optimization increase in 𝛿, so choosing 𝛿too
large can lead to additional runtime for minimal gain. This latter
concern about computational efficiency is especially importantat LinkedIn’s scale. On the other hand, if 𝛿is too small then the
expansion audience itself will be too small to produce a significant
gain in downstream metrics of interest. We tune 𝛿through offline
evaluation to balance these trade-offs.
4 EXPERIMENTS AND RESULTS
4.1 Offline Analysis
We performed extensive offline analysis of prediction models, LP
and AE. Due to space limitation, we only highlight the summary of
offline analysis here and share details of the results in Appendix D
of the arXiv version of the paper [42].
•Evaluation of prediction models. We evaluated the clas-
sification and calibration performance of the two propensity
models𝑓conv (AUC=0.895) and 𝑓unsub (AUC=0.715). We also
evaluated the LTV model 𝑓ltvand found that: (i) gamma loss
with auxiliary tasks can respond faster to market changes,
(ii) it performs better than an alternative modeling approach
using survival loss.
•LP analysis. We found that smaller 𝛾values lead to better
optimality but can take longer to converge. We also validated
the high probability of binary primal solutions. More impor-
tantly, we investigated the fallback strategy of using the
previous dual solution for the current LP, and found that we
can maintain 99% of the objective value if data distribution
is stable.
•AE analysis. We evaluated the impact of 𝛿with respect to
the trade-off between audience size, quality and runtime.
•Pre-A/B test simulation. We find that (i) NOAH can make
efficient "not-send" decisions through LP whereas legacy
ranking system cannot, and (ii) the combination of AE and
LP helps realize the full potential of the optimization engine.
4.2 Online Test Design
We designed an online A/B test to compare the performance of the
NOAH email system with that of the legacy system. In the test, we
randomly assigned members to the treatment arm (NOAH system)
or the control arm (legacy system). In addition, we included a third
experimentation arm for the legacy system equipped with AE to
isolate the impact of increasing marketing email volume on its own.
The flow of the test is visualized in Figure 3.
Figure 3: A/B Test Flow.
The email marketing channel only covers a small percentage
of the member base. If the randomization splits on the set of all
members, the treatment effect could be diluted. We therefore focus
on members that are "available" to receive marketing emails. In
particular, we first randomize all members to 3 buckets, Usplit
𝑏,
5944KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
where𝑏∈{1,2,3}. For each week 𝑡of the experiment, we can
expand the original audience U𝑜
𝑡toU𝑒
𝑡with AE. Assuming the
experiment runs for 𝑇weeks, the set of members for experiment
arm𝑏can be obtained asU𝑏,𝑇=Ð
𝑡≤𝑇Usplit
𝑏∩U𝑒
𝑡. Note here that
for control arm 𝑏=1, the legacy system will rank and send emails
only for members in original audience, i.e., over set of U𝑜
𝑡∩Usplit
1
for week𝑡.
Since it is not realistic to wait for one year to obtain the long-
term metric 𝑞12, we define a long-term surrogate index [ 3]Y𝑢=Í
𝑝∈P𝑢𝑦LTV𝑢,𝑝for each member 𝑢, whereP𝑢denote the set of prod-
ucts that member 𝑢converted during the test period, and 𝑦LTV𝑢,𝑝
denotes the expected 1-year long-term metric associated with the
conversion on product 𝑝for member 𝑢. It is worth noting that
although each observation consists of potentially multiple conver-
sions, through offline analysis we have discovered that the majority
of members convert at most one time during the test period.
Besides the above primary metric, we also monitor a few sec-
ondary metrics, e.g., email volume, conversions (Conv) as well
as observed long-term metric. In addition, we have other metrics
corresponding to the LP constraints, the key one being unsubscrip-
tions (Unsub), a binary metric indicating whether or not a user
unsubscribed during the test period.
We compare the means of the above metrics from different arms
for statistical testing of treatment effects. It is worth noting that
long-term metrics exhibit characteristics of a zero-inflated heavy-
tail distribution (Appendix C.1). We performed simulations on Type
I error and power of the 𝑡-test against these distributions, and
found that the standard 𝑡-test is actually robust (Appendix C.2).
We believe this result is broadly useful, as alternatives do not suit
the needs of such testing: (1) Winsorization removes the large data
points that actually matter (if not more), (2) Non-parametric tests
(e.g., Mann-Whitney U test) lack a corresponding treatment effect
estimate.
4.3 Online Test Results
4.3.1 Overview. The A/B test resulted in major wins, one of the
largest in Linkedin algorithmic marketing thus far. An overview
of the results is presented in Table 1. All the numbers in the tables
represent relative difference over the control arm, i.e., ratio of the
difference (between the treatment and control arms) to the mean
value in control. Results that are not statistically significant are
marked either "neutral" or "not stat sig".
Table 1: Overview of A/B Test Result
Marketing System Surrogate Index Unsub Email Volume
Legacy+AE neutral +8.08% +9.42%
NOAH +2.16%1+1.52% +8.44%
1All percentages are relative to the control arm.
As shown in Table 1, NOAH produces a statistically significant
positive lift in expected 1-year long-term metric (Surrogate Index)
at the cost of a moderate uptick ( +1.52%) in total unsubscription.
Given that Email Volume is increased by 8.44%, the unsubscription
per email sent is actually reduced, indicating that more relevantTable 2: Product Type Breakdown
Product Type Surrogate Index 3-Mth Metric1Conv
2C not stat sig neutral neutral
2B +13.21% +23.66% +13.30%
1Observed 3-month long-term metric 𝑞3after test concluded.
emails were sent. By comparison, the legacy ranking system with
AE (Legacy+AE) does not produce a statistically significant effect
on the long-term metric, and it produces a large Unsub increment
(+8.08%), a magnitude similar as Email Volume increment ( +9.42%).
4.3.2 Product Type Breakdown. Historically, we have faced chal-
lenges in developing a unified marketing system which optimizes
marketing emails for a heterogeneous suite of 2C and 2B products.
Our legacy marketing system tends to optimize for quick sign-up
2C products in spite of these products’ relatively small lifetime
values. On the other hand, campaigns associated with high-LTV 2B
products tend to be neglected.
A breakdown of comparison between NOAH and control into
2C and 2B product types is presented in Table 2. We observe that
NOAH is able to drive a large positive lift in both Surrogate Index
(+13.21%) and total Conv ( +13.30%) for high-LTV 2B products with-
out sacrificing these metrics for 2C products. It achieves this lift by
increasing total 2B conversions and shifting the distribution 2B con-
versions toward those with higher Surrogate Index, as visualized
in Figure 7 in the Appendix.
Due to the nature of delayed feedback, we tracked the long-
term metric of conversions that happened during A/B test even
after the test concluded. We find that NOAH is able to drive a
significant lift (+23.66%) in observed 3-month long-term metric
(3-Mth Metric) for 2B products. As with the Surrogate Index, we
observed a distributional shift toward high-value conversions in
3-Mth Metrics, as shown in Figure 8 in the Appendix.
The breakdown analysis demonstrates that NOAH is able to lift
high value long-term metrics for 2B products while simultaneously
maintaining 2C acquisition metrics and controlling unsubscriptions.
This holistic behavior is essential for a unified marketing system
which must support a wide range of campaigns having different
downstream values.
5 CONCLUSION
In this paper, we introduce various challenges faced when building
an intelligent marketing system and propose a general framework,
NOAH, to address these challenges. We give guidelines on how to
apply the NOAH framework and provide a few high-level exam-
ples. In addition, we report a large-scale application of NOAH to
LinkedIn’s email marketing use case. We discuss important innova-
tions and learnings in the implementation and share results of the
major wins for the corresponding online A/B test.
ACKNOWLEDGMENTS
We thank Shruti Sharma, Maggie Zhang, Divyakumar Menghani,
Aarthi Jayaram, and Rehan Khan for their support of the work. We
also thank our colleagues Michael Ingley, Grinishkumar Engineer,
5945Neural Optimization with Adaptive Heuristics for Intelligent Marketing System KDD ’24, August 25–29, 2024, Barcelona, Spain
Artem Baranov, Sumukh Sagar Manjunath, Carlos Flores, Shawn
Bianchi, Kim Foo, Kyla Falkenhagen for their collaboration on the
email application and A/B test. Additionally, we’d like to acknowl-
edge Zhiqi Guo, Ming Wu, Miao Cheng and Faisal Farooq for their
contributions and support during the early phase of the work.
REFERENCES
[1]Elaheh Yadegaridehkordi Louis Sanzogni A. Rashid Tarik Kathy Knox Sarminah
Samad Othman Ibrahim Ali Ahani, Mehrbakhsh Nilashi. 2002. Revealing cus-
tomers’ satisfaction and preferences through online review analysis: The case of
Canary Islands hotels. Journal of Retailing and Consumer Services 51, 3 (2002),
331–343.
[2]David Applegate, Mateo Díaz, Oliver Hinder, Haihao Lu, Miles Lubin, Brendan
O’Donoghue, and Warren Schudy. 2021. Practical Large-Scale Linear Program-
ming using Primal-Dual Hybrid Gradient. 20243–20257.
[3]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. The
surrogate index: Combining short-term proxies to estimate long-term treatment
effects more rapidly and precisely. Technical Report. National Bureau of Economic
Research.
[4]Kinjal Basu, Amol Ghoting, Rahul Mazumder, and Yao Pan. 2020. ECLIPSE: An
Extreme-Scale Linear Program Solver for Web-Applications. Proceedings of the
37th International Conference on Machine Learning, 704–714.
[5]Amir Beck and Marc Teboulle. 2009. A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences 2
(2009), 183–202. Issue 1. https://doi.org/10.1137/080716542
[6]Dimitri P Bertsekas. 1997. Nonlinear programming. Journal of the Operational
Research Society 48, 3 (1997), 334–334.
[7]Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011.
Distributed optimization and statistical learning via the alternating direction
method of multipliers. Foundations and Trends in Machine Learning 3 (2011),
1–122. Issue 1. https://doi.org/10.1561/2200000016
[8]Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D Sculley. 2017. The
ML test score: A rubric for ML production readiness and technical debt reduction.
In2017 IEEE International Conference on Big Data (Big Data). IEEE, 1123–1132.
[9]Antonin Chambolle and Thomas Pock. 2011. A first-order primal-dual algorithm
for convex problems with applications to imaging. Journal of Mathematical
Imaging and Vision 40 (5 2011), 120–145. Issue 1. https://doi.org/10.1007/s10851-
010-0251-1
[10] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan
He. 2023. Bias and debias in recommender system: A survey and future directions.
ACM Transactions on Information Systems 41, 3 (2023), 1–39.
[11] Pei Pei Chen, Anna Guitart, Ana Fernández del Río, and Africa Periánez. 2018.
Customer lifetime value in video games using deep learning and parametric
models. In 2018 IEEE international conference on big data (big data). IEEE, 2134–
2140.
[12] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785–794.
[13] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-
sensitive hashing scheme based on p-stable distributions. Proceedings of the
twentieth annual symposium on Computational geometry (2004), 253–262.
[14] Michelle Roehm Derrick S. Boone. 2002. Retail segmentation using artificial
neural networks. International Journal of Research in Marketing 19, 3 (2002),
287–301.
[15] Max H Farrell, Tengyuan Liang, and Sanjog Misra. 2020. Deep learning for
individual heterogeneity: An automatic inference framework. arXiv preprint
arXiv:2010.14694 (2020).
[16] Zhe Feng, Swati Padmanabhan, and Di Wang. 2023. Online Bidding Algorithms
for Return-on-Spend Constrained Advertisers. In Proceedings of the ACM Web
Conference 2023. 3550–3560.
[17] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189–1232.
[18] Hui Gao and Yihan Yang. 2022. Multi-Head Online Learning for Delayed Feedback
Modeling. arXiv preprint arXiv:2205.12406 (2022).
[19] Sunil Gupta, Dominique Hanssens, Bruce Hardie, Wiliam Kahn, V Kumar,
Nathaniel Lin, Nalini Ravishanker, and S Sriram. 2006. Modeling customer
lifetime value. Journal of service research 9, 2 (2006), 139–155.
[20] Gurobi Optimization, LLC. 2023. Gurobi Optimizer Reference Manual. https:
//www.gurobi.com
[21] Daniel Jacob. 2021. Cate meets ml: Conditional average treatment effect and
machine learning. Digital Finance 3, 2 (2021), 99–148.
[22] Lukáš Kakalejčík, Jozef Bucko, and Martin Vejačka. 2019. Differences in buyer
journey between high-and low-value customers of e-commerce business. Journal
of theoretical and applied electronic commerce research 14, 2 (2019), 47–58.[23] Ajay Kumar, Ravi Shankar, and Naif Radi Aljohani. 2020. A big data driven frame-
work for demand-driven forecasting with effects of marketing-mix variables.
Industrial marketing management 90 (2020), 493–507.
[24] Dong C. Liu and Jorge Nocedal. 1989. On the Limited Memory BFGS Method for
Large Scale Optimization. Mathematical Programming 45 (1989), 503–528.
[25] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining. 1930–1939.
[26] Liye Ma and Baohong Sun. 2020. Machine learning and AI in marketing–
Connecting computing power to human insights. International Journal of Research
in Marketing 37, 3 (2020), 481–504.
[27] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. 1137–1140.
[28] Edward C Malthouse and Robert C Blattberg. 2005. Can we predict customer
lifetime value? Journal of interactive marketing 19, 1 (2005), 2–16.
[29] Eric WT Ngai and Yuanyuan Wu. 2022. Machine learning in marketing: A
literature review, conceptual framework, and research agenda. Journal of Business
Research 145 (2022), 35–48.
[30] Eiji Oki. 2012. GLPK (GNU Linear Programming Kit). https://api.semanticscholar.
org/CorpusID:63578694
[31] M Paulo, Vera L Miguéis, and Ivo Pereira. 2022. Leveraging email marketing:
Using the subject line to anticipate the open rate. Expert systems with applications
207 (2022), 117974.
[32] Musthofa Galih Pradana and Hoang Thi Ha. 2021. Maximizing strategy improve-
ment in mall customer segmentation using k-means clustering. Journal of Applied
Data Sciences 2, 1 (2021), 19–25.
[33] Omri Raiter. 2021. Segmentation of bank consumers for artificial intelligence
marketing. International Journal of Contemporary Financial Issues 1, 1 (2021),
39–54.
[34] Rohan Ramanath, Sathiya S. Keerthi, Kinjal Basu, Konstantin Salomatin, Pan
Yao, Amol Ghoting, and Miao Cheng. 2022. DuaLip: Dual Decomposition based
Linear Program Solver, version 2.0.0. https://github.com/linkedin/dualip
[35] Rohan Ramanath, S. Sathiya Keerthi, Yao Pan, Konstantin Salomatin, and Kinjal
Basu. 2021. Efficient Vertex-Oriented Polytopic Projection for Web-scale Appli-
cations. arXiv preprint arXiv:2103.05277, 1–13. http://arxiv.org/abs/2103.05277
[36] Kan Ren, Weinan Zhang, Ke Chang, Yifei Rong, Yong Yu, and Jun Wang. 2017.
Bidding machine: Learning to bid for directly optimizing profits in display ad-
vertising. IEEE Transactions on Knowledge and Data Engineering 30, 4 (2017),
645–659.
[37] A. Schrijver. 1986. Theory of Linear and Integer programming. Wiley-Interscience.
[38] Xuhui Shao and Lexin Li. 2011. Data-driven multi-touch attribution models.
InProceedings of the 17th ACM SIGKDD international conference on Knowledge
discovery and data mining. 258–264.
[39] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for
the estimation of treatment effects. Advances in neural information processing
systems 32 (2019).
[40] Duncan Simester, Artem Timoshenko, and Spyros I Zoumpoulis. 2020. Targeting
prospective customers: Robustness of machine-learning methods to typical data
challenges. Management Science 66, 6 (2020), 2495–2522.
[41] Vinay Singh, Brijesh Nanavati, Arpan Kumar Kar, and Agam Gupta. 2023. How to
maximize clicks for display advertisement in digital marketing? A reinforcement
learning approach. Information Systems Frontiers 25, 4 (2023), 1621–1638.
[42] Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro,
Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, and
Licurgo Benemann De Almeida. 2024. Neural Optimization with Adaptive Heuris-
tics for Intelligent Marketing System. arXiv:2405.10490
[43] Philip Wolfe. 1976. Finding the nearest point in a polytope. Mathematical
Programming 11 (1976), 128–149.
[44] Elaheh Yadegaridehkordi, Mehrbakhsh Nilashi, Mohd Hairul Nizam Bin Md Nasir,
Saeedeh Momtazi, Sarminah Samad, Eko Supriyanto, and Fahad Ghabban. 2021.
Customers segmentation in eco-friendly hotels using multi-criteria and machine
learning techniques. Technology in Society 65 (2021), 101528.
[45] Danuta Zakrzewska and Jan Murlewski. 2005. Clustering algorithms for bank
customer segmentation. In 5th International Conference on Intelligent Systems
Design and Applications (ISDA’05). IEEE, 197–202.
[46] Jun Zhou, Yang Bao, Daohong Jian, and Hua Wu. 2023. PDAS: A Practical
Distributed ADMM System for Large-Scale Linear Programming Problems at
Alipay. Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, 5717–5727. https://doi.org/10.1145/3580305.3599883
5946KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
A ALGORITHMS
A.1 Bidding Optimization with Lagrangian
Relaxation
We introduce the constraint into the objective with Lagrangian
relaxation:
𝑔(𝜆)= max
𝑝(𝑢)∈{1,...,𝑃}∑︁
𝑢𝑦rev
𝑢,𝑝(𝑢)−𝜆 
𝐶𝑅𝑂𝐴𝑆∑︁
𝑢𝑦cost
𝑢,𝑝(𝑢)−∑︁
𝑢𝑦rev
𝑢,𝑝(𝑢)!
=∑︁
𝑢max𝑝(𝑢)∈{1,...,𝑃}h
(1+𝜆)𝑦rev
𝑢,𝑝−𝜆𝐶𝑅𝑂𝐴𝑆𝑦cost
𝑢,𝑝i
.
For any fixed 𝜆=𝜆′, we can get the optimal 𝑝(𝑢)simply by finding
the maximum among 𝑃discrete values for each 𝑢, i.e.,
𝑝𝜆′(𝑢)=argmax
𝑝{(1+𝜆′)𝑦rev
𝑢,𝑝−𝜆′𝐶𝑅𝑂𝐴𝑆𝑦cost
𝑢,𝑝},∀𝑢.
Then, we can compute the subgradient with respect to 𝜆,
𝜕𝑔(𝜆)|𝜆=𝜆′=∑︁
𝑢𝑦rev
𝑢,𝑝𝜆′(𝑢)−𝐶𝑅𝑂𝐴𝑆∑︁
𝑢𝑦cost
𝑢,𝑝𝜆′(𝑢)
and use the sign of the subgradient to guide where to find 𝜆that
minimize𝑔(𝜆).
Algorithm 1 Bidding Optimization with Lagrangian Relaxation
Input:𝑦rev𝑢,𝑝,𝑦cost𝑢,𝑝,𝐶ROAS ,𝜆init>0, and 0<𝜖≪𝜆init
Output:𝑝∗(𝑢)for each𝑢
set𝜆𝑚𝑖𝑛←0and𝜆𝑚𝑎𝑥←𝜆init
if𝜕𝑔(𝜆)|𝜆=0≥0then
return𝑝0(𝑢)
else
while𝜕𝑔(𝜆)|𝜆=𝜆𝑚𝑎𝑥<0do
set𝜆𝑚𝑖𝑛←𝜆𝑚𝑎𝑥 and𝜆𝑚𝑎𝑥←2𝜆𝑚𝑎𝑥
end while
end if
while𝜆𝑚𝑎𝑥−𝜆𝑚𝑖𝑛>𝜖do
set𝜆′←𝜆𝑚𝑖𝑛+𝜆𝑚𝑎𝑥
2
if𝜕𝑔(𝜆)|𝜆=𝜆′>0then
set𝜆𝑚𝑎𝑥←𝜆′
else if𝜕𝑔(𝜆)|𝜆=𝜆′<0then
set𝜆𝑚𝑖𝑛←𝜆′
else
break
end if
end while
return𝑝𝜆′(𝑢)
A.2 Content Optimization with Online
Contextual Bandit
For efficient Bayesian online learning, we will assume the priors
for all weights are independent and Gaussian-distributed. The joint
distribution can be written in the following form:
𝑝(𝑦,𝑡,𝑠,𝑤 1,𝑤2,···,𝑤𝑘)=ℎ(𝑦,𝑡)𝑎(𝑡,𝑠)𝑔(𝑤1,𝑤2,···,𝑤𝑘,𝑠)Ö
𝑖𝑟𝑖(𝑤𝑖),where
𝑟𝑖(𝑤𝑖)=N(𝑤𝑖;𝜇𝑖,𝜎2
𝑖),
𝑔(𝑤1,···,𝑤𝐾,𝑠)=𝛿(𝑥𝑇𝑤=𝑠),
𝑎(𝑠,𝑡)=N(𝑡;𝑠,𝛽2),
ℎ(𝑦,𝑡)=𝛿(𝜌(𝑡)=𝑦),
and𝛿(·)is the indicator function.
In the factor graph, the variable nodes {𝑤𝑖,𝑠,𝑡,𝑦}are connected
by the factor nodes {𝑟𝑖,𝑔,𝑎,ℎ}. We can calculate the posterior dis-
tribution𝑝(𝑤𝑖|𝑥,𝑦)by message passing in factor graph:
(1) Consider 𝑡as root node.
(2)Perform message passing from leaf to root and obtain 𝑝(𝑡)=
N(𝑡;𝜂,𝜏2)ℎ(𝑦,𝑡), where𝜂=Í
𝑖𝑥𝑖𝜇𝑖, and𝜏2=Í
𝑖𝑥2
𝑖𝜎2
𝑖+𝛽2.
(3)If𝑝(𝑡)is not Gaussian, we use moment matching to obtain
ˆ𝑝(𝑡)=N(𝑡;𝜇𝑡,𝜎2
𝑡), where𝜇𝑡=𝜂+Δand𝜎2
𝑡=𝜏2𝛾.
(4)Perform message passing from root to leaf and obtain poste-
rior of model weights,
𝑝(𝑤𝑖)=N(𝑤𝑖;𝜇𝑤𝑖,𝜎2
𝑤𝑖),
where,
 
𝜎2𝑤𝑖=𝜎2
𝑖[1−𝑥2
𝑖𝜎2
𝑖
𝜏2(1−𝛾)]
𝜇𝑤𝑖=𝜇𝑖+𝑥𝑖𝜎2
𝑖
𝜏2Δ
With the above, we can have following online contextual bandit
algorithm that works for high-dimensional settings. The algorithm
can be easily parallelized or set up for on-device computing.
Algorithm 2 Content Optimization with Online Contextual Bandit
Input:𝜇𝑖,𝜎2
𝑖,𝛽2
fortime step𝑗=1,2,···,𝐽do
Sample ˆ𝑤𝑖from𝑁(𝜇𝑤𝑖,𝜎2𝑤𝑖)for all𝑖;
forcreative𝑎=1,2,···,𝐴do
Sample ˆ𝜖𝑎from𝑁(0,𝛽2)
Calculate latent reward ˜𝑡𝑗,𝑎=ˆ𝜖𝑎+Í
𝑖𝑥𝑖,𝑗,𝑎ˆ𝑤𝑖
end for
Choose creative 𝑎𝑗=arg max𝑎˜𝑡𝑗,𝑎and show it to user;
observe user engagement and receive reward 𝑦𝑗
Update𝜇𝑤𝑖←𝜇𝑤𝑖+𝑥𝑖𝜎2
𝑤𝑖
𝜏2Δ;
Update𝜎2𝑤𝑖←𝜎2𝑤𝑖[1−𝑥2
𝑖𝜎2
𝑤𝑖
𝜏2(1−𝛾)]
end for
Here, the value of 𝛾andΔdepends on the link function. For
linear regression, 𝛾=0andΔ=𝑦−𝑥𝑇𝑤. For probit regression,
𝛾=1−𝜛(𝑦𝜂
𝜏)andΔ=𝑦𝜏𝜗(𝑦𝜂
𝜏), where,𝜗(𝑡)=N(𝑡;0,1)
Φ(𝑡;0,1)and
𝜛(𝑡)=𝜗(𝑡)(𝜗(𝑡)+𝑡).
A.3 Sampling from Primal Solution
The IP to LP relaxation can be interpreted as transforming the
deterministic integer decision to the expected value of a random
integer decision, i.e., the probability of sending some marketing
email. We can add randomization after the LP solution to "recover"
5947Neural Optimization with Adaptive Heuristics for Intelligent Marketing System KDD ’24, August 25–29, 2024, Barcelona, Spain
the random integer decision 𝑎′
𝑢,𝑗. Formally, we want to implement
the sampling so that:
𝐸(𝑎′
𝑢,𝑗)∝𝑎𝑢,𝑗∑︁
𝑗𝑎′
𝑢,𝑗≤𝐶fcap.
We can follow the following algorithm to "recover" the random
integer decision 𝑎′
𝑢,𝑗:
(1) Calculate sampling probabilities 𝑝𝑢,𝑗=𝑎𝑢,𝑗Í
𝑗𝑎𝑢,𝑗
(2)Sample a setJ′𝑢of size⌊Í
𝑗𝑎𝑢,𝑗+0.5⌋without replacement,
from set of all index J𝑢with non-uniform distribution of
𝑝𝑢,𝑗
(3)We obtain𝑎′
𝑢,𝑗=𝐼(𝑗∈J′𝑢), where𝐼(·)is indicator function.
B ADAPTIVE HEURISTICS
There are two main classes of AH: 1) Feedback Loop Controller and
2) Hierarchical Optimization. At a high-level, these two classes of
AH help optimize and stabilize the system on the "time" and "space"
dimensions respectively.
Besides the two main classes of AHs, there are other simple
AHs, for example: cool-off rules so that a member cannot receive
more than some number marketing messages over a fixed time
window, and 𝜖-greedy exploration so that selection bias is measured
and taken into account in modeling. Most of these heuristics are
common to RecSys, and we refer readers to standard practice in
[8, 10].
Figure 4: Feedback Loop Controller.
B.1 Feedback Loop Controller
Controllers can observe the feedback from the marketing units, and
is able to adjust input to the main optimization module, so as to
stabilize it when there are deviations on the constraints. This is
particularly useful when we have strict constraints, e.g., marketing
budgets/cost. Technically, this can be achieved either by a Propor-
tional–integral–derivative (PID) controller and/or a lightweight
reinforcement learning (RL) model. Figure 4 shows the overall flow.
Without loss of generality, assume there is a marketing cost
metric𝐶1we want to monitor and control from the constraint
formulation (2) in Section 2.2.2. The controller takes observed cost
𝐶′
1, and other historical data 𝑥𝐻, and outputs the cost target 𝐶∗
1
which should be used as the budget in the linear program, i.e.,
𝐶∗
1=𝑔(𝐶1,𝐶′
1,𝜙(𝑥𝐻)).For the case of a PID controller, 𝜙(𝑥𝐻)represents the three quanti-
ties, proportion, integral, and derivative, which are derived from the
time series of error term 𝑒𝑡(𝑥𝐻)=𝐶′
1,𝑡−𝐶1,𝑡. Once𝐶∗
1is obtained,
we can use it for main optimization module:
max𝑎𝑢∑︁
𝑢𝑓0(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐),
s.t.∑︁
𝑢𝑓1(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐)≤𝐶∗
1,
∑︁
𝑢𝑓𝑘(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐)≤𝐶𝑘,∀2≤𝑘≤𝐾,
Beside PID controllers, we can also use RL to learn more complex
𝑔(.)and𝜙(.)functions.
B.2 Hierarchical Optimization
The formulation of Hierarchical Optimization is similar to the LP
formulations in Section 2.2.2, except that 1) we use aggregated data
as input, 2) the prediction model can be light-weight, and 3) the
optimization problem is small-scale.
The main use case for hierarchical optimization is macro-level
marketing budget allocation. With slight abuse of notation from
Section 2.2.2, we let 𝑢index unit of budget allocation (e.g., com-
bination of marketing channel and lines of business), and let 𝑎𝑢
denote the level of budget to be assigned to unit 𝑢. The resulting
optimization formulation is the same as (1) and (2).
The model used in the objective is typically the revenue on bud-
get changes 𝑦0=𝑓0(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐), where either traditional Media
Mixed Models (MMM) or modern causal ML methods can be used.
Among the constraints, the cost model 𝑦1=𝑓1(𝑥𝑢,𝑥𝑎𝑢,𝑥𝑐)can ei-
ther use learned or rule-based 𝑓1(.)(e.g., assuming cost equals fixed
proportion of budget) depending on the maturity of the optimiza-
tion engine. The constrained optimization problem is small-scale,
so we can typically use off-the-shelf solvers to solve the original
integer program without LP relaxation.
C ROBUSTNESS OF T-TEST
C.1 Zero-Inflated Heavy-Tail Metric
We can see that in Figure 6, there is a significant probability mass at
zero and right side of the distribution is heavy-tailed for Surrogate
Index. Note that the counts on histogram plot is 𝑙𝑜𝑔10scale.
Figure 6: Distribution of Surrogate Index.
C.2 Simulation Result for 𝑡-Test Robustness
We simulate the control and treatment based on zero-inflated heavy-
tail data. The control is simulated by 𝑋=𝑍𝑥𝑋′, where𝑍𝑥is random
5948KDD ’24, August 25–29, 2024, Barcelona, Spain Changshuai Wei et al.
Figure 5: Two-stage Email Marketing System with NOAH
binary variable with E[𝑍𝑥]=𝑝0, and𝑋′is sampled from a heavy-
tail distribution. The treatment is simulated by 𝑌=𝑍𝑦𝑌′, where
E[𝑍𝑦]=𝑝0+𝑝ΔandE[log(𝑌′)]=E[log(𝑋′)]+Δ.
Table 3: Type I errors of 𝑡test against zero-inflated
heavy-tail distribution.
𝛼1𝑋′∼𝑃obs 𝑋′∼𝑃ln
𝑝0=0.02𝑝0=0.8𝑝0=0.02𝑝0=0.8
0.05 0.03038 0.04837 0.02464 0.03917
0.01 0.00227 0.00858 0.00168 0.00524
0.005 0.00072 0.00376 0.00048 0.00211
0.001 0.00002 0.0005 0.00002 0.00031
1Size of the test.
Table 4: Power of 𝑡test against
zero-inflated heavy-tail distribu-
tion.
𝑝Δ Δ𝑋′∼𝑃obs𝑋′∼𝑃ln
0 1.0 0.451 0.153
0 2.0 0.909 0.496
0.005 1.0 0.673 0.295
0.005 2.0 0.976 0.637
The null model is simulated by setting Δ=0and𝑝Δ=0. Besides
varying the size of the test ( 𝛼), we also (i) vary 𝑝0at 0.02 and at
0.8, (ii) vary distribution of 𝑋′at𝑃obsand𝑃ln. Here𝑃obsis the
empirical distribution of the positive Surrogate Index in the A/B
test, and𝑃lnis the lognormal distribution with 𝜇(log(𝑋′))=2and
𝜎(log(𝑋′))=2. We perform t-test on the simulated samples andcalculated the estimated Type I errors. We can see Type I errors are
well controlled as shown in Table 3.
The alternative model is simulated by varying the values of Δ
and𝑝Δ. For simplicity, we fix 𝛼=0.05,𝑝0=0.2and vary the
distribution of 𝑋′at𝑃obsand𝑃ln. We can see in Table 4 that power
increases as 𝑝Δincreases or as Δincreases.
D ADDTIONAL A/B TEST RESULTS
D.1 Distribution of Long-Term Metrics on 2B
Conversions
Figure 7: Distribution of Surrogate Index for 2B Conversions.
Figure 8: Distribution of 3-Month Metric for 2B conversions.
5949