CASA: Clustered Federated Learning with Asynchronous Clients
Boyi Liu
SKLCCSE Lab
Beihang University
Beijing, China
boyliu@buaa.edu.cnYiming Ma
SKLCCSE Lab
Beihang University
Beijing, China
yimingma@buaa.edu.cnZimu Zhou
School of Data Science
City University of Hong Kong
Hong Kong, China
zimuzhou@cityu.edu.hk
Yexuan Shi
SKLCCSE Lab
Beihang University
Beijing, China
skyxuan@buaa.edu.cnShuyuan Li
SKLCCSE Lab
Beihang University
Beijing, China
lishuyuan@buaa.edu.cnYongxin Tong
SKLCCSE Lab
Beihang University
Beijing, China
yxtong@buaa.edu.cn
Abstract
Clustered Federated Learning (CFL) is an emerging paradigm to
extract insights from data on IoT devices. Through iterative client
clustering and model aggregation, CFL adeptly manages data het-
erogeneity, ensures privacy, and delivers personalized models to
heterogeneous devices. Traditional CFL approaches, which operate
synchronously, suffer from prolonged latency for waiting slow de-
vices during clustering and aggregation. This paper advocates a shift
to asynchronous CFL, allowing the server to process client updates
as they arrive. This shift enhances training efficiency yet introduces
complexities to the iterative training cycle. To this end, we present
CASA, a novel CFL scheme for Clustering-Aggregation Synergy un-
der Asynchrony. Built upon a holistic theoretical understanding of
asynchrony’s impact on CFL, CASA adopts a bi-level asynchronous
aggregation method and a buffer-aided dynamic clustering strat-
egy to harmonize between clustering and aggregation. Extensive
evaluations on standard benchmarks show that CASA outperforms
representative baselines in model accuracy and achieves 2.28-6.49×
higher convergence speed.
CCS Concepts
•Computing methodologies →Learning paradigms.
Keywords
Clustered Federated Learning; Asynchronous Federated Learning;
Sparse Training
ACM Reference Format:
Boyi Liu, Yiming Ma, Zimu Zhou, Yexuan Shi, Shuyuan Li, and Yongxin Tong.
2024. CASA: Clustered Federated Learning with Asynchronous Clients. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671979
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671979
𝑡!,𝑡",…globaldata……𝑡𝑖𝑚𝑒=0initial𝑡𝑖𝑚𝑒=10accurate
straggler(a) Synchronous CFL
…𝑡𝑖𝑚𝑒=0initial𝑡𝑖𝑚𝑒=10mis-clustering𝑡"𝑡!,𝑡#𝑡$,𝑡%𝑡&𝑡'partialdatastraggler… (b) Asynchronous CFL
Figure 1: Asynchrony can lead to mis-clustering, which may
further impair federated training of cluster-wise models.
1 Introduction
Clustered federated learning (CFL) is a promising solution to har-
ness the decentralized, heterogeneous data of IoT devices for collec-
tive intelligence. In CFL, devices (clients ) with similar data distribu-
tions are grouped to train cluster-wise models under server coor-
dination, while keeping their datasets localized [ 13,23,26,28,36].
This strategy is particularly effective in IoT applications, where
the data is often heterogeneous yet exhibits natural clusterability
[18,26]. For instance, human activities vary across users but may
share notable spatiotemporal similarities [ 29]. By leveraging the
inherent clusterability of heterogeneous data, CFL not only simpli-
fies operations but also creates accurate, personalized models, with
widespread applicability in smart homes [ 43], mobile healthcare
[26], and intelligent transportation [34].
Despite CFL’s efficacy in handling data heterogeneity, it strug-
gles when confronting system heterogeneity inherent in the diverse
computation and communication capabilities of IoT devices [ 44].
This problem stems from the synchronous operations, where the
server awaits simultaneous updates from all clients for clustering
and training [ 13,23,26,28,36]. Consequently, slow devices, often
termed stragglers, force the server into waiting states, leading to
extended training latency . For example, synchronous aggregation
may take 3-12×longer to reach the target accuracy in presence of
stragglers that are 5×slower than others [19].
A promising solution is to integrate asynchrony into CFL, where
the server processes client updates as they arrive, which have been
adopted in federated training of a single model [ 5,6,12,19,41].
 
1851
KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
Although such asynchronous mode eliminates long waiting time, it
may severely undermine the effectiveness of CFL. As illustrated in
Fig. 1, integrating asynchrony means clustering clients with partial
and outdated information, which not only induces clustering errors,
but also deteriorate the training of cluster-wise models. Specifically,
asynchronous CFL faces the following challenges.
•How to guarantee effective CFL with asynchronous clients? CFL
iteratively optimizes client clustering and model aggregation
[13,23,26,28,36], which converges in the synchronous
setting [ 23]. Yet it is unknown whether such convergence
still holds in asynchronous environments.
•How to adapt client clustering and model aggregation for asyn-
chronous CFL? Asynchrony necessitates staleness manage-
ment in both clustering and aggregation. Although staleness
control has been studied in federated learning without clus-
tering, e.g., by decaying outdated updates [ 7,19,27,41], these
designs are inapplicable to CFL due to the extra interplay
between clustering and aggregation.
In this paper, we present CASA (Clustering-Aggregation Synergy
under Asynchrony), a new CFL scheme for asynchronous clients.
We analyze the impact of asynchrony on client clustering and model
aggregation separately, as well as on their interplay, via a unified an-
alytical framework. We further derive the conditions for convergent
CFL in asynchronous environments. On this basis, we develop a
buffer-aided dynamic clustering algorithm and a bi-level asynchro-
nous aggregation scheme for effective and efficient asynchronous
CFL. In addition, we also harness sparse training to actively mitigate
the impact of stragglers. Evaluations on standard benchmarks show
that our methods achieve comparable accuracies to synchronous
CFL methods but converge faster by 2.28-6.49×. Compared with
prior asynchronous schemes, we improve the accuracy by up to
30.34%and are up to 39.11×faster to reach the target accuracy.
Our main contributions are as follows:
•To our knowledge, this is one of the first work on clustered
federated learning in a fully asynchronous context, promot-
ing the CFL deployment to heterogeneous IoT devices.
•We present CASA, which features both theoretical analysis
and practical server-side designs for fast and accurate CFL
in asynchronous environments.
•Evaluations show that CASA is both more accurate and faster
than the state-of-arts, which holds promise to simultaneously
address data and system heterogeneity in federated learning.
2 Related Work
Clustered Federated Learning. CFL alleviates data heterogeneity
in FL by grouping clients with similar data distributions, thereby
enhancing homogeneous learning within clusters [ 13,23,26,28,36].
Due to its simplicity and efficacy, this tactic prevails in personaliza-
tion in federated learning [ 4,35], among other strategies [ 11,33,46].
Research on CFL explores client similarity measures [ 22,26,28] and
clustering algorithms [ 2,18,22,36,42] to improve clustering accu-
racy and enhance its interplay with model training [ 13,23,42]. Com-
mon client similarity metrics include cosine similarity [ 28,36,42],
Euclidean distance [ 2,22], KL divergence [ 26], etc. The clustering
algorithms span from naive k-means [ 10,22,26] to hierarchical
clustering [ 2,18,20,28,42]. For instance, CFL [ 28] bi-partitionsclients into clusters until the training stabilizes. IFCA [ 13] itera-
tively refines the clusters via training loss minimization. ICFL [ 42]
adopts both incremental clustering and spectral clustering to dy-
namically discover the clustering structure. These studies operate
insynchronous settings, leaving the coherence between clustering
and training in the asynchronous context unexplored.
Our work adopts the standard cosine distance of model weights
to measure client similarity [ 28,36,42], and focus on the clustering
algorithms that function under asynchrony. A few proposals ex-
plored the semi-asynchronous case [ 49]. However, a comprehensive
analysis on the interactions between clustering and training in fully
asynchronous environments is missing.
Asynchronous Federated Learning. AFL enhances the efficiency
of FL in face of system heterogeneity. While classic FL relies on syn-
chronous model aggregation, introducing considerable latency due
to delays from stragglers, AFL activates model aggregation upon
receiving client updates, effectively minimizing idle wait times
[5,6,12,19,41]. Semi-asynchronous variants have also been pro-
posed to further reduce the client-server communication overhead
[31,40,49]. However, asynchronous aggregation introduces stale
gradients, which can destabilize learning or even cause model di-
vergence [ 5,19,40,41]. Accordingly, AFL algorithms often decay
stale updates during model aggregation [ 7,19,27,41] to mitigate
their impacts on convergence. For example, FedAsync [ 41] first
introduces the decay function to combat stale updates. FedBuff
[25] buffers recent updates and aggregates the averaged gradients
into global model. PORT [ 30] considers both divergence of model
updates and staleness when updating model parameters. TimelyFL
[47] adjusts the partial training rate to boost slow devices and re-
duce staleness. FedASMU [ 19] distributes the fresh global model
to clients during training to control staleness, accompanied with a
time-aware decay function to ensure convergence.
Our work also focuses on managing staleness, but for CFL rather
than the generic FL that merely trains a single global model. Due
to the unique interplay between clustering and training, we design
a new decay function that benefits clustering and training and
ensures their synergy. We also hitchhike the decay function for
sparse training, which actively controls the impact of stragglers.
3 Problem Statement
Clustered federated learning [ 13,23,26,28,36]is a pivotal solution
to handle non-IID data in federated learning. It groups clients based
on the similarity of their data distribution. Within each cluster,
clients collaboratively learn a shared model while the raw data
remains locally on device.
CFL Workflow. In a typical CFL framework, clients learn cluster-
wise models via bi-level optimization, iteratively minimizing the
clustering error andtraining loss till model convergence [ 22,23,42].
This process operates in three steps per round (see Fig. 2): (i) Local
Training (L-phase), in which clients perform local model training
and upload the local updates to the server; (ii) Client Clustering
(C-phase), where the server groups clients into clusters according
to their uploaded model weights; (iii) Model Aggregation (A-phase),
wherein client model weights are aggregated within each cluster,
followed by sending the updated global (cluster-wise) model to
 
1852CASA: Clustered Federated Learning with Asynchronous Clients KDD ’24, August 25–29, 2024, Barcelona, Spain
…L-phaselocal updateC-phaseclient clusteringA-phase model aggregation𝒕𝒕−𝝉𝒊𝒕−𝝉𝒋
𝒘𝒈(𝒕)𝒘𝒈(𝒕%𝟏)
Figure 2: A typical CFL framework, which consists of lo-
cal update (L-phase), client clustering (C-phase), and model
aggregation (A-phase). Asynchrony induces outdated local
updates to both the C- and A-phases.
clients. Importantly, the workflow is synchronous, where the server
waits updates from all clients for clustering and aggregation.
Formally, assume 𝑛clients{𝑐1,𝑐2,···,𝑐𝑛}with local dataset
{𝐷1,𝐷2,···,𝐷𝑛}are grouped into 𝐾clusters{𝑢1,𝑢2,···,𝑢𝐾}. All
clients in cluster 𝑢𝑘share and collaboratively train a global model
𝑤𝑔,𝑘to minimize the following training objectiveP:
min𝑤𝑔,1,...,𝑤𝑔,𝐾P=𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈C𝑘|𝐷𝑖|
|𝐷|E[L(𝑤𝑔,𝑘;𝐷𝑐)] (1)
whereC𝑘denotes the clients in cluster 𝑢𝑘. Meanwhile, CFL also
minimizes the following clustering errorH:
min𝑤𝑔,1,...,𝑤𝑔,𝐾,𝑤1,...,𝑤𝑛H=𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈C𝑘|𝐷𝑖|
|𝐷|∥𝑤𝑖−𝑤𝑔,𝑘∥2
2(2)
where𝑤𝑔,𝑘is the global model of cluster 𝑢𝑘, and𝑤𝑖is the local
parameters hold by client 𝑐𝑖.
CFL under Asynchrony. As mentioned in Sec. 1, we address
asynchronous CFL to enhance training efficiency amidst stragglers.
Adapting to asynchrony calls for significant modifications to both
clustering and aggregation.
•Client Clustering (C-phase) : Prior CFL methods [ 13,23,26,
28,36] base clustering decisions on full client availability.
The asynchronous clients invalidate this assumption, neces-
sitating new strategies for accurate and timely clustering.
•Model Aggregation (A-phase) : Stale updates from slower clients
hinders the training convergence and accuracy [ 5,19,40,41].
Asynchronous aggregation can manage staleness via weight
decay [ 7,19,27,41]. Yet its effectiveness remains unexplored
when an extra clustering stage is involved.
Assumptions and Scope. We focus on server-side solutions to
minimize interventions to clients. We use FedAvg [ 24] as the basic
optimizer given its pervasive adoption, but our strategies should
also apply to other optimizers e.g., FedProx [ 17]. While privacy is
vital in CFL, our primary goal is to improve training efficiency and
accuracy, keeping privacy at levels akin to FedAvg. Explorations
on specific attacks and defenses are beyond our scope.
4 Method
This section presents CASA (Clustering-Aggregation Synergy un-
derAsynchrony), an asynchronous CFL scheme. We analyze the
A-phasedecay coefficient αC-phasedynamic cluster size |𝒞!|high mis-clustering rateextra decay coefficient boundFigure 3: Impact of asynchrony on CFL.
impact of asynchrony on the generic CFL framework (Sec. 4.1), and
propose new designs on model aggregation (Sec. 4.2) and client clus-
tering (Sec. 4.3) to retain their synergy. Finally, we further enhance
CASA by incorporating sparse training (Sec. 4.4).
4.1 Understanding Asynchronous CFL
Although the bi-level optimization for CFL (Sec. 3) converges in the
synchronous setting [ 23], it is unknown whether such clustering-
aggregation synergy holds in asynchronous environments. As de-
picted in Fig. 3, asynchrony affects both the A- and C-phases as well
as their interplay. This necessitates the exploration of new require-
ments on clustering and aggregation to preserve their coherence.
Direct Impact. Asynchrony in CFL brings critical modifications
to both client clustering and model aggregation:
•C-phase : Measuring client similarity becomes complex with
partial client information. In synchronous settings, similar-
ity between clients 𝑐𝑖and𝑐𝑗in round𝑡might be computed
as𝐴𝑖𝑗=cos(𝑤(𝑡)
𝑖,𝑤(𝑡)
𝑗). However, with asynchronous up-
dates, this measure shifts to 𝐴′
𝑖𝑗=cos(𝑤(𝑡−𝜏𝑖)
𝑖,𝑤(𝑡−𝜏𝑗)
𝑗),
based on model parameters received in different rounds, i.e.,
𝑡−𝜏𝑖≠𝑡−𝜏𝑗. This leads to a deviation from the actual
similarity of local data distributions between 𝑐𝑖and𝑐𝑗due
to misaligned global model 𝑤(𝑡−𝜏𝑖)
𝑔,𝑘and𝑤(𝑡−𝜏𝑗)
𝑔,𝑘.
•A-phase : In asynchronous aggregation, a decay coefficient 𝛼
is introduced to manage staleness [ 7,19,27,41]. The aggre-
gation process becomes:
𝑤(𝑡+1)
𝑔,𝑘=(1−𝛼)𝑤(𝑡)
𝑔,𝑘+𝛼𝑤(𝑡−𝜏)
𝑖(3)
where the decay coefficient 𝛼is crucial for the convergence
of model training.
Compound Impact. The changes in individual steps also affect the
bi-level framework’s ability to optimize the training and clustering
objectives i.e., Eq. (1) and Eq. (2), as our analysis shows.
Theorem 1. (Clustering Error under Asynchrony). When cluster-
ing relies on a similarity matrix 𝐴′derived with asynchronous model
parameters, the mis-clustering rate 𝑝is bounded by:
𝑝=O(𝜆𝛼vut𝑛∑︁
𝑖=1(𝑛∑︁
𝑗=1∥𝜏𝑖−𝜏𝑗∥2)) (4)
where𝜆=𝜂𝑄𝜃𝑈 , and𝜂is the learning rate, 𝑄is the local training
steps,𝑈is the upper bound of gradient, 𝜃is the upper bound of
staleness (details in Appendix A.1.1).
Theorem 1 depicts the impact of asynchronous aggregation on
clustering accuracy. In the synchronous case, ∥𝜏𝑖−𝜏𝑗∥is close to
 
1853KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
zero, resulting in negligible clustering error. Conversely, in asyn-
chronous settings,∥𝜏𝑖−𝜏𝑗∥may be large due to client heterogeneity,
thus impacting clustering accuracy. Also, the mis-clustering rate 𝑝
is upper-bounded by the decay coefficient 𝛼.
Theorem 2. (Convergence of Training Objective). The training
objectivePdecreases monotonically, and thus the CFL framework
converges under asynchrony, if the following condition is met:
𝛼≤Ω(𝑡)ℎ𝑖
|C𝑘|(5)
where|C𝑘|is the size of cluster 𝑢𝑘,ℎ𝑖is the computational capacity of
𝑐𝑖, andΩ(𝑡)is a time-decreasing function (details in Appendix A.1.2).
Theorem 2 implies that clustering affects the design of model
aggregation. Specifically, the decay coefficient 𝛼should adapt to
the cluster size|C𝑘|. This adjustment is reasonable because larger
clusters often exhibit greater staleness [ 9]. Moreover, Theorem 2
suggests that 𝛼should be dynamic and client-specific, given that
bothΩ(𝑡)and|C𝑘|varies over time, and ℎ𝑖differs across clients.
Summary. We make the following observations.
•The CFL framework remains effective under asynchrony
with new requirements on clustering and aggregation.
•Clustering must be adaptable to asynchronous data arrival
and manage mis-clustering effectively.
•Aggregation should account for both global information e.g.,
cluster scale and round, and individual client characteristics
when managing stale weights.
Algorithm 1 illustrates our CASA framework, which follows the
standard L-C-A workflow. Next, we introduce practical designs to
fulfill the new requirements on clustering and aggregation. We start
with aggregation since the clustering algorithm is built upon the
decay coefficient. All the proofs are in Appendix A.1.
4.2 Bi-Level Asynchronous Aggregation
Principles. Stale model updates are often decayed by 𝛼(see Eq. (3))
during asynchronous aggregation for convergent training of generic
FL [5,19,40,41]. Prior research [ 27,41] mainly sets 𝛼based on
staleness alone, which is oversimplified for CFL. As highlighted in
Sec. 4.1, the decay coefficient should be configured considering
various cluster- andclient-specific factors to enhance clustering ac-
curacy and training convergence. To this end, we suggest a bi-level
decay coefficient design that strategically separates and manages
the complex dependencies on these factors.
Cluster-Level Decay. Following Theorem 2 but ignoring the client-
specific factors, we set the cluster-wise decay for cluster 𝑢𝑘as:
𝛼(𝑡)
𝑐,𝑘=𝛼0Ω(𝑡)
log(|C𝑘|)(6)
where𝛼0is the initial value of 𝛼. The rationales are three-fold.
•We expect the cluster-level decay to penalize the average
staleness within the cluster, whereas the stragglers would
receive extra penalty by the client-level decay. The cluster-
level decay also facilitates clustering decisions (see Sec. 4.3).
•From Theorem 2, 𝛼should be bounded by a time-decreasing
function Ω(𝑡). This is because 𝛼is bounded by the expecta-
tion of local gradients (details in Appendix A.1.2). The localAlgorithm 1: CASA
Input: Clients’ model parameters 𝑤1,...,𝑤𝑛, the single
global model parameters 𝑤𝑔,1
Output: Clients’ model parameters 𝑤1,...,𝑤𝑛, global model
parameters after clustering 𝑤𝑔,1,...,𝑤𝑔,𝐾
1Server Process:
2forasynchronous round 𝑡do
3 Receive update 𝑤(𝑡−𝜏𝑖)
𝑖from client𝑐𝑖
4 if𝑐𝑖is new client then
5𝑘←argmax(𝑟𝑒𝑝𝑘,𝑤(𝑡−𝜏𝑖)
𝑖)as Eq. (15)
6 Assign𝑐𝑖to cluster𝐶𝑘
7 //C-phase
8 Update similarity matrix 𝐴and cluster based on
Algorithm 3
9 Update buffer space based on Algorithm 2
10 //A-phase
11 Asynchronous aggregation as Eq. (9)
12Client Process:
13while True do
14 Receive global model 𝑤(𝑡)
𝑔,𝑘
15 //L-phase
16 Local training 𝑤(𝑡)
𝑖←𝑤(𝑡)
𝑔,𝑘−∇L(𝑤(𝑡)
𝑔,𝑘,𝐷𝑖)
17 Upload weight 𝑤(𝑡)
𝑖to server asynchronously
gradients are expected to diminish in convergent training,
which is characterized by the round-decaying function Ω(𝑡).
Motivated by learning rate scheduling [ 45], we instantiate
Ω(𝑡)as an exponential decay. Uniquely, we show that the
decay coefficient 𝛼should depend on not only the relative
staleness, but also the absolute rounds in CFL.
•From Theorem 2, 𝛼should decrease as the cluster size |C𝑘|
increases. We set 𝛼as inversely proportional to log(|C𝑘|)
because directly applying |C𝑘|may make𝛼susceptible to
dynamic clustering, which affects training stability.
Client-Level Decay. The client-level decay handles the client-
specific staleness on top of the cluster-level decay. From Theorem 2,
the client-specific staleness is captured by ℎ𝑖, which is the propor-
tion of local updates completed by a client per round. However, it
is difficult for the server to monitor the resources of clients and
accurately estimate ℎ𝑖[31]. Alternatively, we only impose extra
decay when the client staleness 𝜏𝑖exceeds a threshold 𝑟(𝑡)
𝑐:
𝛼(𝑡)
𝑖=(
𝛼(𝑡)
𝑐,𝑘, if𝜏𝑖≤𝑟(𝑡)
𝑐
𝛼(𝑡)
𝑐,𝑘/√𝜏𝑖,if𝜏𝑖>𝑟(𝑡)
𝑐(7)
Such binary classification of staleness is feasible since our proof
on convergent training only assumes bounded staleness (details in
Appendix A.1.2). Unlike previous work [ 41] that pre-defines the
maximal tolerable staleness, we heuristically set 𝑟(𝑡)
𝑐,𝑘as
𝑟(𝑡)
𝑐,𝑘=|C𝑘|(2−Ω(𝑡)). (8)
 
1854CASA: Clustered Federated Learning with Asynchronous Clients KDD ’24, August 25–29, 2024, Barcelona, Spain
upload𝒃𝒖𝒇𝒋𝒃𝒖𝒇𝒌𝒃𝒖𝒇𝑵insert…𝑨=1⋯0.7⋮⋱⋮0.7⋯1𝒃𝒖𝒇𝒊Xlarge gapXrecurring𝑤!,𝑤"𝑤#𝑤$,𝑤"𝒘𝒌𝒃𝒖𝒇𝒊,𝒘𝑵𝒃𝒖𝒇𝒊update bufferupdatesimilarity𝒄𝒊𝒘𝒊(𝒕*𝝉)
𝒄𝒋…
𝒄𝒌𝒄𝑵
Figure 4: Buffer-aided similarity calculation.
This is because the staleness tends to increase with cluster scale
[9], while its impact on training decreases over rounds [ 38]. Finally,
the traditional asynchronous aggregation, i.e., Eq. (3) becomes:
𝑤(𝑡+1)
𝑔,𝑘=𝛼(𝑡)
𝑖𝑤(𝑡−𝜏𝑖)
𝑖+(1−𝛼(𝑡)
𝑖)𝑤(𝑡)
𝑔,𝑘. (9)
4.3 Buffer-Aided Dynamic Clustering
Principles. Clustering in CFL aims to group clients with similar
local data distributions for effective training of cluster-wise models.
We measure client similarity via the cosine distance of their model
weights, as common in CFL literature [ 2,28,36,42], and focus
on the clustering algorithms suited for the asynchronous environ-
ments. As pointed out in Sec. 4.1, asynchrony leads to misaligned
model weights for similarity calculation, necessitates incremental
clustering with partial information, and enlarges clustering error
due to asynchronous aggregation. To address these challenges, we
propose a buffer-aided similarity calculation scheme, and utilize a
multi-partitioning iterative clustering approach, where the partition-
ing criterion is staleness-dependent. These designs enable timely
and accurate clustering (see case studies in Appendix 5.2.3).
Buffer-Aided Similarity Calculation. From Theorem 1, the clus-
tering error is bounded by the overall misalignment between model
weights in time, i.e.,Í𝑛
𝑖=1Í𝑛
𝑗=1∥𝜏𝑖−𝜏𝑗∥2. An intuitive solution is
tobuffer the model weights for each client and calculate the client
similarity using the most aligned model versions rather than the
most recent ones. Specifically, on receiving an update 𝑤(𝑡)
𝑖from
client𝑐𝑖, we select clients 𝑐𝑗,...,𝑐𝑘whose start time is close to that
of𝑐𝑖, and insert𝑤(𝑡)
𝑖into their buffer space 𝑏𝑢𝑓𝑗,...,𝑏𝑢𝑓𝑘. Then we
use model parameters inside buffer 𝑏𝑢𝑓𝑖for similarity calculation:
𝐴𝑖,𝑗=cos(𝑤(𝑡)
𝑖,𝑤𝑏𝑢𝑓𝑖
𝑗). (10)
Finally, we clear the buffer space of 𝑐𝑖, and wait for new updates.
To support the buffer-aided similarity calculation, one needs to
store allhistorical client updates, which is unscalable in practice.
Accordingly, we optimize the buffer storage below (see Fig. 4).
•Redundant Update Pruning. The server directly discards re-
dundant updates without buffering them. Client 𝑐𝑖’s model
parameters𝑤(𝑡−𝜏𝑖)
𝑖are considered redundant to client 𝑐𝑗in
two cases: (i) large gap :𝑡−𝜏𝑖is within the range 𝑟𝑐of client
𝑐𝑗’s starting round, i.e.,𝜏𝑗−𝜏𝑖≥𝑟𝑐, since large gap leads to
high clustering error (see Theorem 1). (ii) recurring : bufferAlgorithm 2: Prioritized buffer allocation.
Input: Received client update 𝑤(𝑡−𝜏𝑖)
𝑖
Output: New buffer for clients 𝑏𝑢𝑓1,...,𝑏𝑢𝑓𝑛
1forclient𝑐𝑗∈𝐶𝑘do
2 if𝑤(𝑡−𝜏𝑖)
𝑖not redundant then
3 Add𝑤(𝑡−𝜏𝑖)
𝑖to𝑏𝑢𝑓𝑗
4ifÍ𝑛
𝑖=1|𝑏𝑢𝑓𝑖|>𝑀then
5 Sort all buffers based on
6 uncomputed or not as the primary key,
7 decay function 𝛼𝑖as the secondary key,
8 Delete lastÍ𝑛
𝑖=1|𝑏𝑢𝑓𝑖|−𝑀items
𝑏𝑢𝑓𝑗already contains client 𝑐𝑖’s model parameters from a
previous instance.
•Prioritized Buffer Allocation. Given a buffer budget 𝑀, we
optimize buffer allocation as follows:
max𝑛∑︁
𝑖=1∑︁
𝑗∈𝑏𝑢𝑓𝑖1{𝑐𝑎𝑙𝑖,𝑗=1}
𝛼(𝑡)
𝑖,s.t.𝑛∑︁
𝑖=1|𝑏𝑢𝑓𝑖|≤𝑀 (11)
where 1{𝑐𝑎𝑙𝑖,𝑗=1}indicates whether the similarity between
𝑐𝑖and𝑐𝑗has already been computed, and 𝛼(𝑡)
𝑖is the client-
level decay defined in Eq. (7). This is because (i)we need all
the pairwise similarities to make clustering decisions, and
(ii)slower clients are more outdated and have less chances
to update the similarity matrix, thus of higher priority for
updating. Eq. (11) is an online knapsack problem and can
be solved in two steps: (i)Sort clients’ buffers following the
order of two keywords, i.e., # of similarities not computed,
and𝛼(𝑡)
𝑖.(ii)Greedily select the first 𝑀buffers. Algorithm 2
illustrates our buffer allocation scheme.
Multi-Partitioning Iterative Clustering. Most CFL methods as-
sume a fixed number of clusters [ 13,22], which is unfit for dynamics
in the asynchronous environments. One solution is to incrementally
adjust the number of clusters via iterative bi-partitioning [ 28,42].
Yet the bi-partitioning can be slow to converge. To boost the clus-
tering efficiency, we apply a multi-partitioning-based approach.
Specifically, we consider clients in a cluster as a graph 𝐺(𝑉,𝐸),
with clients as vertex set 𝑉and their pairwise similarities as edges
set𝐸. Our goal is to decide whether there is an appropriate gap in the
current graph 𝐺for partitioning into 𝑅subgraphs{𝐺1,𝐺2,...,𝐺𝑅}.
To estimate the number of subgraphs, we calculate the maximum
eigengap of 𝐺with the Laplacian matrix of its similarity matrix 𝐴:
𝐿=𝐼−𝐷−1/2𝐴𝐷−1/2(12)
Let{𝜆1,𝜆2,...,𝜆𝑛}be the eigenvalue of 𝐿. We select the maximum
eigengap to retrieve the best 𝑅partitions [37].
𝑅=arg max
𝑘𝜆𝑘+1−𝜆𝑘,s.t.{𝜆1,𝜆2,...,𝜆𝑛}←𝑆𝑉𝐷(𝐿) (13)
From Theorem 1, we should only perform clustering when the
(cluster-level) decay 𝛼𝑐,𝑘is not too large. Accordingly, we partition
clients into𝑅clusters only when:
𝛼𝑐,𝑘<(𝜆𝑅+1−𝜆𝑅)𝛾(14)
 
1855KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
Algorithm 3: Multi-partitioning iterative clustering.
Input: Similarity matrix 𝐴, cluster-level aggregation
parameter𝛼(𝑡)
𝑐,𝑘, received client update 𝑤(𝑡−𝜏𝑖)
𝑖
Output: New cluster list 𝑢1,...,𝑢𝐾
1forclient𝑐𝑗∈𝑏𝑢𝑓𝑖do
2 Update similarity matrix 𝐴𝑖,𝑗←cos(𝑤(𝑡−𝜏𝑖)
𝑖,𝑤𝑏𝑢𝑓𝑖
𝑗)
3Laplacian matrix 𝐿←𝐼−𝐷−1/2𝐴𝐷−1/2
4𝜆1,𝜆2,...𝜆𝑛←𝑆𝑉𝐷(𝐿)
5𝑅←arg max𝑘𝜆𝑘+1−𝜆𝑘
6if𝛼(𝑡)
𝑐,𝑘<(𝜆𝑅+1−𝜆𝑅)𝛾then
7𝑢𝑘1,𝑢𝑘2,...,𝑢𝑘𝑅←𝑆𝑝𝑒𝑐𝑡𝑟𝑎𝑙𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑖𝑛𝑔 (𝑢𝑘,𝑅)
8 Replace𝑢𝑘with𝑢𝑘1,𝑢𝑘2,...,𝑢𝑘𝑅
where𝜆𝑅+1−𝜆𝑅is the maximum eigengap of similarity matrix 𝐴,
and𝛾is a hyperparameter to scale the eigengap comparable to 𝛼𝑐,𝑘.
Algorithm 3 shows the overall clustering workflow.
Newcomer Assignment. Correctly clustering new clients in the
context of asynchronous clustered federated learning presents a
challenging problem. The reasons include:
•Weight gap : The inconsistent update frequency among clients
means that, rather than being assigned to a cluster more
similar in terms of data, a new client might be more easily
assigned to a cluster that has been updated less frequently.
•Center shift : In asynchronous cases, the global model does
not effectively represent the information of all clients within
a cluster, as it is dominated by fast clients.
To address these issues, we leverage historical information to assist
the clustering of newcomers. For each cluster 𝑢𝑘, we filter out the
historical information of clients in the newcomer’s sampled buffer
𝑏𝑢𝑓𝑖. We aggregate historical information in 𝑏𝑢𝑓𝑖∩𝑢𝑘as Eq. (15) to
obtain a representative information of the cluster, and then select an
appropriate cluster for the newcomer based on this representation.
𝑟𝑒𝑝𝑘=∑︁
𝑐𝑗∈𝑏𝑢𝑓𝑖∩𝑢𝑘|𝐷𝑗|
|𝐷𝑏𝑢𝑓𝑖∩𝑢𝑘|𝑤𝑏𝑢𝑓𝑖
𝑗(15)
Discussions. We make the following notes on our buffer-aided
dynamic clustering scheme.
•Buffers have been utilized for stable training of a single
model (without clustering) in asynchronous FL [ 25,31,40].
Our work is orthogonal as we apply buffers in clustering.
Our clustering scheme could be integrated with buffer-based
model aggregation for better training.
•Although our clustering algorithm is built upon prior multi-
partitioning methods, the key novelty is to trigger clustering
via Eq. (14). It allows high clustering accuracy guarantee
with Theorem 1, and is noise-resilient since the maximum
eigengap is typically large [37].
4.4 Mitigating Staleness via Sparse Training
Principles. We further improve CASA by actively mitigating stale-
ness, i.e., allocating lower training workload to slower clients. Thebasic idea is to incorporate sparse training [1,48], where person-
alized masks are assigned to clients to reduce their local training
workload, thus enhancing the stability of the model convergence.
Designs. We implement the idea as CASA+, which has two designs:
Firstly, during asynchronous aggregation, we only mix the masked
weights into the global model:
𝑤(𝑡+1)
𝑔,𝑘⊙𝑚(𝑡−𝜏𝑖)
𝑖=((1−𝛼(𝑡)
𝑖)𝑤(𝑡)
𝑔,𝑘+𝛼(𝑡)
𝑖𝑤(𝑡−𝜏𝑖)
𝑖)⊙𝑚(𝑡−𝜏𝑖)
𝑖(16)
where𝑚(𝑡−𝜏𝑖)
𝑖is the mask for client 𝑐𝑖.𝑚(𝑡−𝜏𝑖)
𝑖is obtained by
masking the smallest S(𝑡)
𝑖-proportion of local parameters 𝑤(𝑡)
𝑖.
Eq. (16) limits the influence of stale updates on global model with
aggregating partial parameters, which also boosts convergence.
Secondly, we configure the sparsity rate S(𝑡)
𝑖as:
S(𝑡)
𝑖=S0
2(𝛼(𝑡)
𝑐,𝑘−𝛼(𝑡)
𝑖)(1+cos(((𝑡−1)𝜋
𝑅𝑒𝑛𝑑))) (17)
which applies a cosine annealing function [ 21] to the divergence
of cluster- and client-level decay coefficient, where 𝑅𝑒𝑛𝑑is prede-
fined total rounds. This ensures that fast clients are unlikely to be
masked, because local 𝛼(𝑡)
𝑖equals to cluster-level decay. The reason
to control the sparsity rate via the decay coefficient is as follows. (i)
Clients with higher staleness will receive a higher degree of sparsity.
(ii)Sparsification impairs similarity calculation, and sparsity rate
before clustering should be less than that after clustering.
5 Experiments
5.1 Experimental Setup
Compared Methods. We compare CASA with representative CFL
and AFL methods. We also extend the CFL baselines to the asyn-
chronous setting. Specifically, we apply decay function in model
aggregation of these CFL baselines, and buffer the recent model
parameters from each client for similarity calculation. The server
clusters or check clustering condition per asynchronous round.
•Standalone: Each client trains its model with its local dataset
only without federated training.
•FedAvg [24]: generic FL that trains a global model via weighted
averaging of local model parameters.
•FedProx [17]: generic FL that applies a proximal to boost
the convergence.
•FedAsync [41]: classic AFL that aggregates model with
staleness-based weight decay.
•FedBuff [25]: AFL that applies buffer to aggregate the most
recent𝐾gradients.
•CFL [28]: synchronous CFL that adopts bi-partitioning clus-
tering based on mean and maximum norm of gradients.
•CFL-Async: asynchronous extension of CFL [28].
•ICFL [42]: synchronous CFL that adopts bi-partitioning and
incremental clustering.
•ICFL-Async: asynchronous extension of ICFL [42].
•IFCA [13]: synchronous CFL that iteratively clusters clients
based on minimizing loss.
•IFCA-Async: asynchronous extension of IFCA [13].
 
1856CASA: Clustered Federated Learning with Asynchronous Clients KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Overall performance. Accis the overall accuracy at convergence, Time is the time to reach the target accuracy. “/”
means that the method fails to reach the target accuracy. For IFCA, the item in parentheses means its pre-set cluster number 𝑘.
Type MethodMNIST CIFAR-10 FEMNIST IMU HARBox
Acc Time Acc Time Acc Time Acc Time Acc Time
N/A Standalone 98.26 4.9 82.6 35.2 93.95 / 90.00 41.9 69.48 /
SyncFedAvg 97.92 80.24 69.49 / 98.34 85.01 85.71 89.08 82.90 251.42
FedProx 97.31 104.68 65.83 / 98.14 114.34 87.57 96.05 81.80 390.96
CFL 99.42 30.63 90.50 209.55 98.75 75.14 86.29 94.93 85.06 170.99
IFCA(k)99.40(3) 11.21 89.10(3) 209.62 97.77(2) 145.34 94.28(2) 80.73 83.73(2) 334.34
99.50(4) 12.8 90.88(4) 77.88 96.59(5) 237.46 92.67(3) 73.28 85.06(4) 226.53
99.48(5) 5.61 91.14(5) 80.39 95.37(8) 279.85 91.81(4) 148.6 87.09(6) 220.98
ICFL 98.68 12.18 84.19 36.49 95.35 121.65 93.23 30.45 82.58 126.43
AsyncFedAsync 97.46 109.53 67.66 / 96.97 183.57 86.86 64.9 78.72 158.97
FedBuff 99.25 53.31 61.11 / 98.21 82.63 80.00 282.9 81.62 55.7
CFL-Async 99.23 9.54 89.97 145.8 98.68 36.67 87.71 69.3 82.43 59.8
IFCA-Async(k)99.28(3) 9.53 83.41(3) 195.30 98.39(2) 81.07 89.61(2) 143.1 77.58(2) 252.60
98.88(4) 8.83 88.99(4) 80.70 97.78(5) 118.27 85.62(3) 143.1 78.13(4) 215.20
99.32(5) 8.57 87.98(5) 83.20 97.62(8) 126.77 89.14(4) 87.77 76.81(6) 236.30
ICFL-Async 98.82 5 83.30 25.3 94.66 / 91.52 81.83 79.65 92.00
OursCASA 99.52 2.80 91.45 23.4 98.97 36.2 95.33 37.47 87.38 54.8
CASA+ 99.34 4.80 90.64 20.3 98.53 35.03 94.57 22.71 87.21 53.6
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost97.598.098.599.099.5AccuracyMNIST
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost60708090AccuracyCIFAR-10
0.6 0.8 1.0 1.2
Normalized Time Cost949596979899AccuracyFEMNIST
0.5 1.0 1.5 2.0 2.5
Normalized Time Cost808590AccuracyIMU
0.2 0.4 0.6 0.8 1.0
Normalized Time Cost70758085AccuracyHARBoxLocal
FedAvg
FedProx
CFL
ICFL
IFCA
FedAsync
FedBuff
Ours
Figure 5: Model accuracy vs. normalized time cost.
Datasets. We test two tasks. (i)image classification (IC): MNIST
[16], CIFAR10 [15], and FEMNIST [3]; (ii)human activity recogni-
tion (HAR): IMU [26], and HARBox [26].
Metrics. We assess the methods with three metrics: (i) Accuracy :
the accuracy on local datasets when converged; (ii) Time to Target
Accuracy : the latency to reach a given accuracy i.e., 95% for MNIST,
80% for CIFAR-10, 95% for FEMNIST, 80% for IMU, 75% for HARBox;
(iii) Time to Convergence : the time required to convergence.
Other details on experimental setups are in Appendix A.2.5.2 Main Results
5.2.1 Time-to-Accuracy. Table. 1 summarizes the time to reach a
target accuracy and the accuracy at convergence. Fig. 5 highlights
the trade-off between model accuracy and normalized latency (w.r.t.
FedAvg) at convergence. We make the following observations.
•Gains over Synchronous FL & CFL. Synchronous CFL outper-
forms the generic counterpart in both accuracy and latency
in most cases, particularly with label-skew, e.g., on MNIST
and CIFAR-10. We achieve higher accuracy than generic FL,
e.g.FedAvg (0.63% to 21.96%), and FedProx (0.83% to 30.34%),
and comparable accuracy to the CFL baselines, even though
we rely on partial information for clustering. However, our
methods are significantly faster, reducing the time to reach
the target accuracy by 2.00-37.39×on MNIST, 1.79-10.33×on
CIFAR-10, 2.15-7.99×on FEMNIST, 1.34-4.23×on IMU, and
2.36-7.29×on HARBox. From Fig. 5, our CASA reduces up
to 3.12×, 3.40×, 2.28×, 2.41×, 6.49×convergence time while
maintaining a comparable and even higher accuracy than
the synchronous CFL baselines.
•Gains over Asynchronous FL & CFL. AFL methods (without
clustering) suffer from low accuracy and long latency on
highly non-IID data. Concretely, the accuracy of our methods
is2.00-23.79%higher than FedAsync, and 0.27-30.34%higher
than FedBuff. They also take more time to reach the target
accuracy, e.g.up to 39.11×on MNIST, 5.25×on FEMNIST,
12,76×on IMU, and 2.97×on HARBox, and they fail to
reach the target accuracy on CIFAR-10. Our methods are also
faster than the asynchronous CFL baselines, and improves
the accuracy by up to 2.68%, 27.53%, 2.56%, 7.72%, 6.83%at
convergence on the five datasets.
5.2.2 Training Curves. This experiment tracks the training dynam-
ics of asynchronous CFL methods. From Fig. 6, CASA converges
faster and achieves a higher test accuracy than other methods.
IFCA-Async managed to capture the correct clustering structures
on label-skewed datasets like MNIST and CIFAR-10, but fails on
 
1857KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
(a) MNIST
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (b) CIFAR-10
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (c) FEMNIST
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (d) IMU
0 10 20 30 40 50
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time102030405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time20406080100Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time30405060708090Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA
0 50 100 150 200 250 300
Time4050607080Accuracy
CFL-Async
ICFL-Async
IFCA-Async
CASA (e) HARBox
Figure 6: Training time and test accuracy of asynchronous CFL methods.
0 10 20 30 40 50 60 70 80 90
Client id0
10
20
30
40
50
60
70
80
90Client id
0.750.800.850.900.951.00
(a) Sims at𝑡=100
0 10 20 30 40 50 60 70 80 90
Client id0
10
20
30
40
50
60
70
80
90Client id
0.600.650.700.750.800.850.900.951.00 (b) Sims at𝑡=300
0 50 100 150 200 250 300
Time020406080100Accuracy
 (c) Accuracy at two stages
Figure 7: Analysis of effectiveness of clustering.
feature-skewed datasets like FEMNIST, IMU and HARBox. This is
because IFCA-Async determines cluster identity based on training
loss, yet the difference in loss is small in feature-skewed datasets.
Furthermore, the global model in IFCA-Async can be biased due
to asynchrony (see Fig. 10b). CFL-Async performs well in part of
datasets. This is because 𝛼affects clustering in the asynchronous
scenarios, and such impact differs across datasets (see Fig. 10a).
5.2.3 Effectiveness of Clustering. This experiment assesses the ef-
fectiveness of CASA’s clustering. We test on CIFAR-10 and ob-
serve the similarity matrix of clients’ model parameters clients at
different times. The data partitioning strategy is the same as in
Appendix A.2.4, with 4clusters, each holding a few unique labels.
As shown in Fig. 7a, at 𝑡=100, a fairly clear clustering structure
has emerged, but the inner relationship within a cluster has not
been discovered yet. As shown in Fig. 7b, by 𝑡=300, we further
capture the data distributions within each cluster, resulting in finer-
grained and more accurate clusters.
Fig. 7c further illustrates the accuracy during training. At 𝑡=100
when there is clear but not fine-grained clustering relationship, the
overall accuracy reaches 86.5%. With training and mining of clients’
similarity, the large clusters are partitioned into sub-clusters with
higher cluster similarities. The partitioning boosts the accuracy,
with a final accuracy of 91.6%.
5.2.4 Impact of System Heterogeneity. This experiment tests an-
other two system heterogeneity setups on HARBox. Scenario A: It is
a general case where the maximum speed gap is 5, and the clients’
local training latencies are randomly sampled from this range [ 19].
Scenario B : It is the case with severe system heterogeneity, with
30%clients taking 10×the training time of fast clients.
Table. 2 summarizes the convergence accuracy and training la-
tency. CASA still outperforms the baselines. Compared with the
general case, CASA achieves a comparable accuracy with merely
1.08×higher latency, when facing severe system heterogeneity.Table 2: Impact of system heterogeneity on HARBox.
MethodScenario A (General) Scenario B (Severe)
Acc Time Acc Time
Standalone 70.47 498.9 70.43 531.2
FedAvg 83.18 1115.93 81.15 2015.19
FedProx 81.32 1190.16 80.19 2056.94
CFL 83.27 977.43 84.17 1614.84
IFCA 84.45 707.24 85.85 1687.42
ICFL 82.09 1019.7 82.12 2095.65
FedAsync 80.23 419.1 79.32 489.3
FedBuff 82.14 400.1 80.32 528.8
Ours 87.61 346.6 87.81 374.6
0 50 100 150 200 250
Time2030405060708090Accuracy
CASA
CASA-D
CASA-C
CASA+
0 50 100 150 200 250 300
Time50607080Accuracy
CASA
CASA-D
CASA-C
CASA+
(a) CIFAR-10
0 50 100 150 200 250
Time2030405060708090Accuracy
CASA
CASA-D
CASA-C
CASA+
0 50 100 150 200 250 300
Time50607080Accuracy
CASA
CASA-D
CASA-C
CASA+ (b) HARBox
Figure 8: Contributions of individual components.
Conversely, other baselines (except standalone, which only trains
locally) are 1.17×to2.39×shower in Scenario B. Thus the results
show that CASA is robust to severe system heterogeneity.
5.3 Ablation Study
5.3.1 Contributions of Individual Components. In addition to CASA
and CASA+, we test the following variants to understand the ef-
fective of different designs: (i)CASA-C, which is CASA without
clustering; and (ii)CASA-D, which is CASA without bi-level 𝛼.
As shown in Fig. 8, both CASA and CASA+ outperform CASA-C
and CASA-D. Due to lack of clustering, CASA-C fails to generate
personalized models adapted to local data distributions, leading to
lower model accuracy. Without the dynamical decay coefficient,
CASA-D not only induces erroneous clustering but also suffers
from slow convergence, hence exhibiting the poorest performance.
Both CASA and CASA+ outperforms CASA-D in accuracy, e.g.up
to34.23%in CIFAR-10, up to 12.37%in HARBox. CASA-C performs
better than CASA-D, but still worse than CASA and CASA+, with
23.62%and7.74%lower accuracy, respectively.
 
1858CASA: Clustered Federated Learning with Asynchronous Clients KDD ’24, August 25–29, 2024, Barcelona, Spain
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy
(a)
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy (b)
5 10 20 50 75 100
Buffer Size020406080Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity05101520Time to T arget Accuracy
0 0.3 0.4 0.5 0.6 0.7 0.8
Sparsity020406080Accuracy (c)
Figure 9: Hyperparameter tests: impact of (a) buffer size on
accuracy; impact of sparsity rate on (b) latency to target ac-
curacy and (c) accuracy at convergence.
5.3.2 Impact of Buffer Size. This experiment shows the effective-
ness of prioritized buffer allocation scheme (Sec. 4.3) by recording
accuracy with 100clients on CIFAR-10 given different buffer sizes.
From Fig. 9a, the accuracy rises to 91.21%with a buffer size of
75. Recall from Table. 1, the highest accuracy of asynchronous CFL
baselines is 89.97%. That is, CASA surpasses other asynchronous
CFL methods even when the buffer size is 75. Note that methods
such as CFL-Async and ICFL-Async need a buffer at least equal
to the client scale i.e.,𝑛=100to store the most recent model pa-
rameters of each client and calculate the pair-wise client similarity.
Therefore, our prioritized buffer allocation mechanism yields higher
accuracy at lower storage overhead.
5.3.3 Impact of Sparsity Rate. This experiment shows the impact
of sparsity rate on CASA+ (Sec. 4.4), which mitigates staleness with
sparse training. We measure the latency to the target accuracy and
the accuracy at convergence at various sparsity rate. Note that
S0=0means no sparse training, i.e., CASA.
From Fig. 9b, introducing sparse training with a initial sparse
rateS0=0.3results in 1.09×shorter latency to target accuracy
compared with CASA (the first bar). As we increase the sparsity rate
S0, the reduction in latency of CASA+ is 1.19-1.32×. However, when
the sparsity rate increases, the time required to reach the target
accuracy does not always decrease. This is because an over-sparse
model may not effectively contribute to the global model. With
faster convergence, the accuracy of CASA+ only drops 0.29-1.28%
as shown in Fig. 9c, which is acceptable.
5.3.4 Impact of Asynchronous Aggregation on CFL. This experi-
ment tests the impact of asynchronous aggregation on CFL schemes
to highlight the challenges when shifting to asynchronous CFL. We
select CFL [ 28] and IFCA [ 13], two representative synchronous CFL
methods that adopt a hierarchical and a dynamic clustering strategy,
respectively.
•Impact on Hierarchical Clustering. From Theorem 1, a larger
𝛼leads to a higher mis-clustering probability, particularly for
naive methods such as hierarchical clustering. We validate
this by testing CFL-Async on CIFAR-10 under various 𝛼val-
ues. We measure the clustering performance its purity [ 42]
which stands for the accuracy of clustering and the training
performance by the test accuracy. Fig. 10a shows the results.
As𝛼increases, the clustering error occurs. The mis-clustered
clients will then introduce unwanted data heterogeneity into
the cluster, which impairs training accuracy. For example,
0.3 0.5 0.7 0.9
Aggregation Decay 
0.00.20.40.60.81.0Purity
020406080
Accuracy(a) Hierarchical clustering
0 50 100 150 200 250 300
Time2030405060708090Accuracy
IFCA-Async(4)
IFCA(4)
IFCA-Async(3)
IFCA(3) (b) Dynamic clustering
Figure 10: Impact of asynchrony on clustering.
when𝛼=0.7, CFL-Async suffers high mis-clustering rate,
with an accuracy drop of 10.09%on CIFAR-10, respectively.
Note that CASA avoid these problems because (i)clustering
is only activated when 𝛼is small, which explicitly controls
clustering errors, (ii)with buffer limiting 𝜏𝑖−𝜏𝑗, according
to Theorem 1, CASA is more robust with respect to 𝛼.
•Impact on Dynamic Clustering. Asynchrony makes dynamic
clustering e.g., IFCA sensitive to client arrival orders. This is
because the global model is biased towards the most recent
local updates due to weight decay. This would exert chal-
lenges to assign clients to the correct clusters. To validate
this, we compare IFCA and IFCA-Async on CIFAR-10 with
cluster number set to 𝑘=3and𝑘=4. As shown in Fig. 10b,
IFCA-Async experiences more unstable convergence than
IFCA. IFCA-Async reaches the target accuracy only 1.09×
faster than IFCA when 𝑘=4, and IFCA is even faster than
IFCA-Async when 𝑘=3. Therefore, the shift to asynchro-
nous CFL does not necessarily improve the convergence
speed, which motivates our designs.
6 Conclusion
This paper presents CASA, a new CFL scheme for asynchronous
clients to boost training efficiency under both data and system het-
erogeneity. We systematically analyze the impact of asynchrony
on CFL. We further propose a bi-level asynchronous aggregation
method and a buffer-aided dynamic clustering strategy to per-
tain the synergy between client clustering and model aggregation
in asynchronous environments. Extensive evaluations show that
CASA outperforms existing CFL and AFL baselines in accuracy and
achieves 2.28-6.49×faster convergence speed. We envision CASA
as a practical solution for swift and personalized federated learning
with heterogeneous IoT devices.
Acknowledgments
We are grateful to anonymous reviewers for their constructive com-
ments. This work is partially supported by National Science Foun-
dation of China (NSFC) under Grant No. U21A20516 and 62336003,
Beijing Natural Science Foundation No. Z230001, Beihang Univer-
sity Basic Research Funding No. YWF-22-L-531, Didi Collaborative
Research Program NO2231122-00047, and CityU APRC grant No.
9610633. Yongxin Tong is the corresponding author.
 
1859KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
References
[1]Sameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. 2022. Feder-
ated dynamic sparse training: Computing less, communicating less, yet learning
better. In Proceedings of AAAI, Vol. 36. 6080–6088.
[2]Christopher Briggs, Zhong Fan, and Peter Andras. 2020. Federated learning with
hierarchical clustering of local updates to improve training on non-IID data. In
Proceedings of IJCNN. 1–9.
[3]Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn `y,
H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: A
benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).
[4]Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin Ding. 2022.
pFL-bench: A comprehensive benchmark for personalized federated learning.
Advances in Neural Information Processing Systems 35 (2022), 9344–9360.
[5]Ming Chen, Bingcheng Mao, and Tianyi Ma. 2019. Efficient and robust asynchro-
nous federated learning with stragglers. In Proceedings of ICLR.
[6]Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. 2020. Asynchro-
nous online federated learning for edge devices with non-iid data. In Proceedings
of Big Data. 15–24.
[7]Yang Chen, Xiaoyan Sun, and Yaochu Jin. 2019. Communication-efficient feder-
ated deep learning with layerwise asynchronous model update and temporally
weighted aggregation. IEEE Transactions on Neural Networks and Learning Systems
31, 10 (2019), 4229–4238.
[8]Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. 2021.
Exploiting shared representations for personalized federated learning. In Pro-
ceedings of ICML. 2089–2099.
[9]Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al .2012. Large
scale distributed deep networks. Advances in Neural Information Processing
Systems 25 (2012).
[10] Yiqun Diao, Qinbin Li, and Bingsheng He. 2023. Exploiting Label Skews in
Federated Learning with Model Concatenation. arXiv preprint arXiv:2312.06290
(2023).
[11] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized fed-
erated learning with theoretical guarantees: A model-agnostic meta-learning
approach. Advances in Neural Information Processing Systems 33 (2020), 3557–
3568.
[12] Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. 2023. A general
theory for federated optimization with asynchronous and heterogeneous clients
updates. Journal of Machine Learning Research 24, 110 (2023), 1–43.
[13] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. 2020. An
efficient framework for clustered federated learning. Advances in Neural Infor-
mation Processing Systems 33 (2020), 19586–19597.
[14] Ling Huang, Donghui Yan, Nina Taft, and Michael Jordan. 2008. Spectral cluster-
ing with perturbed data. Advances in Neural Information Processing Systems 21
(2008).
[15] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[16] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[17] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of MLSys 2 (2020), 429–450.
[18] Youpeng Li, Xuyu Wang, and Lingling An. 2023. Hierarchical Clustering-based
Personalized Federated Learning for Robust and Fair Human Activity Recognition.
Proceedings of IMWUT 7, 1 (2023), 1–38.
[19] Ji Liu, Juncheng Jia, Tianshi Che, Chao Huo, Jiaxiang Ren, Yang Zhou, Huaiyu Dai,
and Dejing Dou. 2023. FedASMU: Efficient Asynchronous Federated Learning
with Dynamic Staleness-aware Model Update. arXiv preprint arXiv:2312.05770
(2023).
[20] Jiachen Liu, Fan Lai, Yinwei Dai, Aditya Akella, Harsha V Madhyastha, and
Mosharaf Chowdhury. 2023. Auxo: Efficient Federated Learning via Scalable
Client Clustering. In Proceedings of SoCC. 125–141.
[21] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021.
Do we actually need dense over-parameterization? in-time over-parameterization
in sparse training. In Proceedings of ICML. 6989–7000.
[22] Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang.
2023. Multi-center federated learning: clients clustering for better personalization.
World Wide Web 26, 1 (2023), 481–500.
[23] Jie Ma, Guodong Long, Tianyi Zhou, Jing Jiang, and Chengqi Zhang. 2022. On
the convergence of clustered federated learning. arXiv preprint arXiv:2202.06187
(2022).
[24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Proceedings of AISTATS. 1273–1282.
[25] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rab-
bat, Mani Malek, and Dzmitry Huba. 2022. Federated learning with bufferedasynchronous aggregation. In Proceedings of AISTATS. 3581–3607.
[26] Xiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, and Guoliang Xing.
2021. Clusterfl: a similarity-aware federated learning system for human activity
recognition. In Proceedings of MobiSys. 54–66.
[27] Jungwuk Park, Dong-Jun Han, Minseok Choi, and Jaekyun Moon. 2021. Sageflow:
Robust federated learning against both stragglers and adversaries. Advances in
Neural Information Processing Systems 34 (2021), 840–851.
[28] Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. 2020. Clustered feder-
ated learning: Model-agnostic distributed multitask optimization under privacy
constraints. IEEE Transactions on Neural Networks and Learning Systems 32, 8
(2020), 3710–3722.
[29] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,
Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.
2015. Smart devices are different: Assessing and mitigating mobile sensing
heterogeneities for activity recognition. In Proceedings of SenSys. 127–140.
[30] Ningxin Su and Baochun Li. 2022. How Asynchronous can Federated Learning
Be?. In Proceedings of IWQoS. 1–11.
[31] Jingwei Sun, Ang Li, Lin Duan, Samiul Alam, Xuliang Deng, Xin Guo, Haim-
ing Wang, Maria Gorlatova, Mi Zhang, Hai Li, et al .2022. FedSEA: A Semi-
Asynchronous Federated Learning Framework for Extremely Heterogeneous
Devices. In Proceedings of SenSys. 106–119.
[32] Ahmet Ali Süzen, Burhan Duman, and Betül Şen. 2020. Benchmark analysis of
jetson tx2, jetson nano and raspberry pi using deep-cnn. In Proceedings of HORA.
1–5.
[33] Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020. Personalized federated
learning with moreau envelopes. Advances in Neural Information Processing
Systems 33 (2020), 21394–21405.
[34] Afaf Taik, Zoubeir Mlika, and Soumaya Cherkaoui. 2022. Clustered vehicular
federated learning: Process and optimization. IEEE Transactions on Intelligent
Transportation Systems 23, 12 (2022), 25371–25383.
[35] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-
alized federated learning. IEEE Transactions on Neural Networks and Learning
Systems (2022).
[36] Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, Chen
Chen, Mubarak Shah, and Bill Lin. 2023. Efficient distribution similarity identifi-
cation in clustered federated learning via principal angles between client data
subspaces. In Proceedings of AAAI, Vol. 37. 10043–10052.
[37] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17 (2007), 395–416.
[38] Haoming Wang and Wei Gao. 2023. Tackling the Unlimited Staleness in Federated
Learning with Intertwined Data and Device Heterogeneities. arXiv preprint
arXiv:2309.13536 (2023).
[39] Yansheng Wang, Yongxin Tong, Zimu Zhou, Ruisheng Zhang, Sinno Jialin Pan,
Lixin Fan, and Qiang Yang. 2023. Distribution-Regularized Federated Learning
on Non-IID Data. In Proceedings of ICDE. 2113–2125.
[40] Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, and Stephen Jarvis.
2020. SAFA: A semi-asynchronous protocol for fast federated learning with low
overhead. IEEE Trans. Comput. 70, 5 (2020), 655–668.
[41] Cong Xie, Sanmi Koyejo, and Indranil Gupta. 2019. Asynchronous federated
optimization. arXiv preprint arXiv:1903.03934 (2019).
[42] Yihan Yan, Xiaojun Tong, and Shen Wang. 2023. Clustered Federated Learning in
Heterogeneous Environment. IEEE Transactions on Neural Networks and Learning
Systems (2023).
[43] Zhikai Yang, Yaping Liu, Shuo Zhang, and Keshen Zhou. 2023. Personalized feder-
ated learning with model interpolation among client clusters and its application
in smart home. World Wide Web (2023), 1–26.
[44] Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and Dacheng Tao. 2023. Heteroge-
neous Federated Learning: State-of-the-art and Research Challenges. Comput.
Surveys (2023).
[45] Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I Jordan. 2019.
How does learning rate decay help modern neural networks? arXiv preprint
arXiv:1908.01878 (2019).
[46] Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie
Wu. 2021. Parameterized knowledge transfer for personalized federated learning.
Advances in Neural Information Processing Systems 34 (2021), 10092–10104.
[47] Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. 2023. Time-
lyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive
Partial Training. In Proceedings of CVPR. 5063–5072.
[48] Wenhao Zhang, Zimu Zhou, Yansheng Wang, and Yongxin Tong. 2023. Dm-pfl:
Hitchhiking generic federated learning for efficient shift-robust personalization.
InProceedings of SIGKDD. 3396–3408.
[49] Yu Zhang, Duo Liu, Moming Duan, Li Li, Xianzhang Chen, Ao Ren, Yujuan Tan,
and Chengliang Wang. 2023. FedMDS: An Efficient Model Discrepancy-Aware
Semi-Asynchronous Clustered Federated Learning Framework. IEEE Transactions
on Parallel and Distributed Systems 34 (2023), 1007–1019.
 
1860CASA: Clustered Federated Learning with Asynchronous Clients KDD ’24, August 25–29, 2024, Barcelona, Spain
A appendix
A.1 Proofs
A.1.1 Proof of Theorem 1. We make the following assumptions as
in [13, 19, 23, 39, 41].
Assumption 1. (Unbiased gradient estimator and Bounded gradi-
ents). The expectation of stochastic gradient ∇𝑙(𝑤𝑖,𝜉𝑖)is an unbiased
estimator of the local gradient for each client, and expectation of L2
norm of∇𝑙(𝑤𝑖,𝜉𝑖)is bounded by a constant U:
E𝜉𝑖∼𝐷𝑖[∥∇𝑙(𝑤𝑖,𝜉)∥2]≤𝑈 (18)
Then we prove Theorem 1, which provides an upper-bound for
the mis-clustering rate 𝑝.
Proof. In the synchronous setting, the similarity between two
clients is calculated as 𝐴𝑖𝑗=cos(𝑤(𝑡)
𝑖,𝑤(𝑡)
𝑗)in round𝑡. In asyn-
chronous setting, for client 𝑐𝑖starting at round 𝑡−𝜏𝑖and client
𝑐𝑗starting at round 𝑡−𝜏𝑗, the similarity calculation becomes
𝐴′
𝑖𝑗=cos(𝑤(𝑡−𝜏𝑖)
𝑖,𝑤(𝑡−𝜏𝑗)
𝑗). The divergence between 𝐴𝑖𝑗and𝐴′
𝑖𝑗
is:
𝐴′
𝑖𝑗−𝐴𝑖𝑗=≤cos(𝑤(𝑡−𝜏𝑖)
𝑗,𝑤(𝑡−𝜏𝑗)
𝑗)=O(∥𝑤(𝑡−𝜏𝑖)
𝑔−𝑤(𝑡−𝜏𝑗)
𝑔∥)
(19)
From [41], the gap of global model parameters is bounded as
∥𝑤(𝑡)
𝑔−𝑤(𝑡−𝜏)
𝑔∥≤𝛼𝛾𝐾𝐻𝑚𝑎𝑥O(𝑉2) (20)
where𝐾is upper bound of staleness, 𝛾is learning rate, 𝐻𝑚𝑎𝑥 is
maximum of local steps, 𝑉2is upper bound of gradients’ norm.
Take Eq. (20) into our problem, we have:
𝐴′
𝑖𝑗−𝐴𝑖𝑗=O((𝜏𝑖−𝜏𝑗)𝛼𝜂𝑄𝜃𝑈)=O(𝜆𝛼(𝜏𝑖−𝜏𝑗)) (21)
where𝑄refers to the maximum of local steps, 𝜂is the learning rate,
and𝜃is the upper bound of staleness. For simplicity, we use 𝜆to
represent𝜆=𝜂𝑄𝜃𝑈 .
The Frobenius-norm of 𝐴′−𝐴can be bounded by Eq. (21):
∥𝐴′−𝐴∥𝐹=vut𝑛∑︁
𝑖=1𝑛∑︁
𝑗=1∥𝐴′
𝑖𝑗−𝐴𝑖𝑗∥2=O(𝜆𝛼vut𝑛∑︁
𝑖=1𝑛∑︁
𝑗=1∥𝜏𝑖−𝜏𝑗∥2)
(22)
Given degree matrix 𝐷′as𝐷′
𝑖𝑖=Í𝑛
𝑗=1cos(𝑤(𝑡−𝜏𝑖)
𝑖,𝑤(𝑡−𝜏𝑗)
𝑗),
the divergence of Forbenius-norm is
∥𝐷′−𝐷∥𝐹=vt𝑛∑︁
𝑖=1(𝐷′
𝑖𝑖−𝐷𝑖𝑖)2=O(𝜆𝛼vut𝑛∑︁
𝑖=1(𝑛∑︁
𝑗=1∥𝜏𝑖−𝜏𝑗∥2))
(23)
The Frobenius-norm of matrix 𝐿′−𝐿can be rewritten as ∥𝐴′−𝐴−
(𝐷′−𝐷)∥𝐹. For item that meets 𝑖≠𝑗, there is𝐴′
𝑖𝑗−𝐴𝑖𝑗=𝐿′
𝑖𝑗−𝐿𝑖𝑗.
The only difference lies in the elements on the diagonal. For an
arbitrary𝑖, as𝐴′
𝑖𝑖−𝐴𝑖𝑖=1−1=0, there exists:
∥𝐿′−𝐿∥𝐹=√︃
∥𝐴′−𝐴∥2
𝐹+∥𝐷′−𝐷∥2
𝐹=O(𝜆𝛼vut𝑛∑︁
𝑖=1(𝑛∑︁
𝑗=1∥𝜏𝑖−𝜏𝑗∥2))
(24)As proved in [ 14], the mis-clustering rate 𝑝of the spectral parti-
tioning algorithm satisfies
𝑝≤𝛿2=∥˜v2−v2∥2≤4∥L′−L∥F
v−√
2∥L′−L∥F(25)
where v2is the unit-length second eignvectors of matrix 𝐿.
Theorem 1 is proved by telescoping all equations above. □
A.1.2 Proof of Theorem 2. For simplicity, we use 𝜆𝑖to represent
client𝑐𝑖’s weight|𝐷𝑖|
|𝐷|, andP𝑡,𝐶,P𝑡,𝐴,P𝑡,𝐿denote the training
objectivePof C-, A-, L-phase in round 𝑡, respectively. To prove the
convergence of CASA, we firstly make assumptions below.
Assumption 2. (Convex). Each loss function 𝑙orLis convex.
𝑙(𝑦)≥𝑙(𝑥)+⟨∇𝑙(𝑥),𝑦−𝑥⟩ (26)
Assumption 3. (Lipschitz Smooth). Each loss function 𝑙orLis
𝛽-smooth.
𝑙(𝑦)≤𝑙(𝑥)+⟨∇𝑙(𝑥),𝑦−𝑥⟩+𝛽
2∥𝑦−𝑥∥2
2(27)
Assumption 4. (Bounded gradient variance). The variance of
stochastic gradient∇𝑙(𝑤𝑖,𝜉𝑖)is bounded by 𝜎2,
E𝜉𝑖∼𝐷𝑖[∥∇𝑙(𝑤𝑖,𝜉𝑖)−∇𝑙(𝑤𝑖)∥2
2]=E[∥∇𝑙(𝑤𝑖,𝜉𝑖)∥2
2]−∥∇𝑙(𝑤𝑖)∥2
2≤𝜎2
(28)
Assumption 5. (Bounded staleness). At asynchronous training
round t, the server received an update 𝑤(𝑡−𝜏𝑖)
𝑖from client𝑐𝑖. The
update staleness is bounded, meeting 𝜏𝑖≤𝜃.
Lemma 1. From C-phase to A-phase in an arbitrary round 𝑡, we
have
P𝑡,𝐴≤P𝑡,𝐶+𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈𝑢𝑘𝜆𝑖𝜂(𝑡−𝜏𝑖)
𝑖𝑄𝑈(𝛼(𝑡−𝜏𝑖)
𝑖𝜃O(𝑈)+𝑈)(29)
Proof. The distance between cluster 𝑢𝑘’s global model and re-
ceived parameters from client 𝑐𝑖should be:
∥𝑤(𝑡)
𝑔,𝑘−𝑤(𝑡−𝜏𝑖)
𝑖∥2≤∥𝑤(𝑡)
𝑔,𝑘−𝑤(𝑡−𝜏𝑖)
𝑔,𝑘∥2+∥𝑤(𝑡−𝜏𝑖)
𝑔,𝑘−𝑤(𝑡−𝜏𝑖)
𝑖∥2
≤𝛼(𝑡−𝜏𝑖)
𝑖𝜂(𝑡−𝜏𝑖)
𝑖𝜃𝑄O(𝑈)+𝜂(𝑡−𝜏𝑖)
𝑖𝑄𝑈
(30)
For an arbitrary cluster 𝑢𝑘, we have:
∑︁
𝑐𝑖∈𝑢𝑘I(𝑞𝑒𝑛𝑑
𝑖=𝑄)𝜆𝑖(L(𝑤(𝑡+1)
𝑔,𝑘,𝐷𝑖)−L(𝑤(𝑡−𝜏𝑖)
𝑖,𝐷𝑖))
≤∑︁
𝑐𝑖∈𝑢𝑘I(𝑞𝑒𝑛𝑑
𝑖=𝑄)𝜆𝑖(⟨∇L(𝑤(𝑡+1)
𝑔,𝑘,𝐷𝑖),𝑤(𝑡+1)
𝑔,𝑘−𝑤(𝑡−𝜏𝑖)
𝑖⟩)
≤∑︁
𝑐𝑖∈𝑢𝑘𝜆𝑖𝜂(𝑡−𝜏𝑖)
𝑖𝑄𝑈(𝛼(𝑡−𝜏𝑖)
𝑖𝜃O(𝑈)+𝑈)(31)
Lemma 1 is satisfied with
P𝑡,𝐴−P𝑡,𝐶=𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈𝑢𝑘I(𝑞𝑒𝑛𝑑
𝑖=𝑄)𝜆𝑖(L(𝑤(𝑡+1)
𝑔,𝑘,𝐷𝑖)−L(𝑤(𝑡−𝜏𝑖)
𝑖,𝐷𝑖))
(32)
□
 
1861KDD ’24, August 25–29, 2024, Barcelona, Spain Boyi Liu et al.
Lemma 2. From the A-phase to L-phase in arbitrary communica-
tion round, we have:
E[P𝑡,𝐿]−P𝑡,𝐴
≤𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈𝑢𝑘𝜆𝑖𝑄−1∑︁
𝑞=0((𝛽(𝜂(𝑡)
𝑖)2
2−𝜂(𝑡)
𝑖)E[∥∇L(𝑤𝑞
𝑖)∥2
2]+𝛽(𝜂(𝑡)
𝑖)2
2𝜎2)
(33)
Proof. For client𝑐𝑖in local training, from step 𝑞to step𝑞+1:
L(𝑤𝑞+1
𝑖)−L(𝑤𝑞
𝑖)≤⟨∇L(𝑤𝑞
𝑖),𝑤𝑞+1
𝑖−𝑤𝑞
𝑖⟩+𝛽
2∥𝑤𝑞+1
𝑖−𝑤𝑞
𝑖∥2
2
=−𝜂(𝑡)
𝑖⟨∇L(𝑤𝑞
𝑖),∇L(𝑤𝑞
𝑖,𝜉𝑞
𝑖)⟩+𝛽(𝜂(𝑡)
𝑖)2
2∥∇L(𝑤𝑞
𝑖,𝜉𝑞
𝑖)∥2
2
(34)
Take expectation on both sides for a random batch 𝜉𝑞
𝑖at step𝑞,
E[L(𝑤𝑞+1
𝑖)]−L(𝑤𝑞
𝑖)≤(𝛽(𝜂(𝑡)
𝑖)2
2−𝜂(𝑡)
𝑖)∥∇L(𝑤𝑞
𝑖)∥2
2+𝛽(𝜂(𝑡)
𝑖)2
2𝜎2
(35)
For an arbitrary client 𝑐𝑖starting L-phase at step 𝑞𝑖, and ends at
step𝑞′
𝑖this round, we telescope them and get:
E[L(𝑤𝑞′
𝑖
𝑖)]−L(𝑤𝑞𝑖
𝑖)≤𝑞′
𝑖∑︁
𝑞=𝑞𝑖((𝛽(𝜂(𝑡)
𝑖)2
2−𝜂(𝑡)
𝑖)∥∇L(𝑤𝑞
𝑖)∥2
2+𝛽(𝜂(𝑡)
𝑖)2
2𝜎2)
(36)
Lemma 2 is satisfied telescoping all clients. □
Now we start to prove Theorem 2.
Proof. From L-phase in asynchronous round 𝑡−1to C-phase
in asynchronous round 𝑡−1, the overall loss does not change, for
the model parameters does not change, so:
P𝑡−1,𝐿=P𝑡,𝐶(37)
According to Lemma 1 and Lemma 2 we can get:
E[P𝑡,𝐿]−P𝑡−1,𝐿≤𝐾∑︁
𝑘=1∑︁
𝑐𝑖∈𝑢𝑘𝜆𝑖𝑞′
𝑖∑︁
𝑞=𝑞𝑖(𝜂(𝑡−𝜏𝑖)
𝑖𝑄𝑈(𝛼(𝑡−𝜏𝑖)
𝑖𝜃O(𝑈)+𝑈)
ℎ𝑛
𝑖
+(𝛽(𝜂(𝑡)
𝑖)2
2−𝜂𝑖)E[∥∇L(𝑤𝑞
𝑖)∥2
2]+𝛽(𝜂(𝑡)
𝑖)2
2𝜎2)
(38)
Givenℎ𝑖=𝑞′
𝑖−𝑞𝑖
𝑄, the right side of Eq. (38) is always negative
when
𝛼(𝑡)
𝑡≤(2−𝛽𝜂(𝑡)
𝑖)E[∥∇L(𝑤𝑞
𝑖)∥2
2]−𝛽𝜂(𝑡)
𝑖𝜎2
2𝜃𝑄𝑈O(𝑈)·ℎ𝑖−𝑈
𝜃O(𝑈)(39)
In the ideal case, the expectation of local gradient E[∥∇L(𝑤𝑞
𝑖)∥2
2]
gradually decreases with time, which can be represented by a time-
decay function. As the upper bound of staleness 𝜃is related to the
cluster size in an asynchronous scenario [ 9], for simplicity, we can
rewrite Eq. (39) as follows:
𝛼(𝑡)
𝑖≤Ω(𝑡)ℎ𝑖
|C𝑘|(40)
where|C𝑘|refers to the scale cluster 𝑢𝑘,ℎ𝑖indicates the system
resource of device 𝑖, andΩ(𝑡)is a function that monotonically
decreases over time. □A.2 Additional Experimental Settings
A.2.1 Experimental Environment. We conduct experiments on a
machine with AMD Ryzen 9 5950X 16-Core Processor CPU, and a
NVIDIA GeForce RTX 3090 GPU. The code are implemented with
Python 3.9.13 and PyTorch 1.13.1.
A.2.2 Configuration of Federation. We set client number 𝑛=100
for MNIST, CIFAR-10, FEMNIST, HARBox, and 𝑛=7for IMU. For
FEMNIST, we select the top 100clients with the largest size of local
data. For HARBox, we select the first 100clients from the 120clients.
We apply a CNN to MNIST, CIFAR-10 and FEMNIST as [ 8]; and a
two-layer fully connected network for IMU and HARBox as [26].
The wall-clock time is simulated following [ 30]. By default, we
set the local training latency of slow devices as 5×that of normal
devices, with 30% slow devices among all clients. This is because
the speed of mainstream IoT platforms may differ by 5×,e.g., be-
tween Raspberry Pi and NVIDIA Nano [ 32]. Since the concept of
round differs in synchronous and asynchronous settings, we train
for500rounds in synchronous settings, and for 50,000rounds in
asynchronous settings.
A.2.3 Hyperparameters. For all the datasets, we set the batch size
to10, and the learning rate 𝜂to0.01.
For all asynchronous methods, the basic decay coefficient is set
as𝛼=0.3. Other method-specific hyperparameters are set as below.
•FedProx [17]: proximal regularization term 𝜇=0.05.
•FedAsync [41]: hinge version, with 𝑎=1and𝑏=4.
•FedBuff [25]: buffer size 𝐾=10.
•CFL [ 28]: mean gradient bound 𝜀1=0.4; max gradient bound
𝜀2=0.7.
•ICFL [ 42]: start value 𝛼∗(0)=0.85, increasing factor 𝜀=4.0.
•CFL-Async: the max gradient bound is set higher than the
synchronous version to avoid frequent triggering of the clus-
tering condition. We set 𝜀2=1.
•ICFL-Async: same as ICFL, except that the time decay rate
becomes×0.01due to the asynchrony nature.
For our CASA, the basic decay coefficient is set 𝛼0=2, and the
time function is set as Ω(𝑡)=(𝑒
2.8)𝑘𝑡, with𝑘set to 0.001for most
cases,𝛾=0.15. To ensure 𝛼(𝑡)
𝑖is strictly less than 1,|C𝑘|is actually
set to|C𝑘|+3. And to ensure clustering quality, we select the first
𝑛=10eigenvalues, and calculate eigengap from them.
A.2.4 Configuration of Datasets. We experiment with two types
of clustering relationships:
•Feature-skew based. We select one IC dataset FEMNIST [ 3]
and two HAR datasets IMU and HARBox [ 26] as the feature-
skew case. These datasets possess realistic feature correla-
tions, such as the writing habits in FEMNIST and the different
activity postures in HAR.
•Label-skew based. We partition two IC datasets MNIST [16]
and CIFAR-10 [ 15] with labels manually. We separate 𝑛=100
clients into 4 groups, with a proportion of {0.2,0.2,0.3,0.3}
following [ 42]. The labels are split following the same pro-
portion. For instance, label 0 and label 1 are only held by
group 0. To simulate the non-IID setting, we set Dirichlet
distribution with 𝛼=1inside each cluster.
 
1862