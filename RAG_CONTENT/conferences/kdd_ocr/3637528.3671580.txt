Enhancing Pre-Ranking Performance: Tackling Intermediary
Challenges in Multi-Stage Cascading Recommendation Systems
Jianping Wei∗†
Ant Group
HangZhou, China
pinger.wjp@antgroup.comYujie Zhou∗
Ant Group
HangZhou, China
zhouyujie.zyj@antgroup.com
Zhengwei Wu
Ant Group
HangZhou, China
zejun.wzw@antgroup.comZiqi Liu
Ant Group
HangZhou, China
ziqiliu@antgroup.com
ABSTRACT
Large-scale search engines and recommendation systems utilize a
three-stage cascading architecture—recall, pre-ranking, and rank-
ing—to deliver relevant results within stringent latency limits. The
pre-ranking stage is crucial for filtering a large number of recalled
items into a manageable set for the ranking stage, greatly affect-
ing the system’s performance. Pre-ranking faces two intermediary
challenges: Sample Selection Bias (SSB) arises when training is
based on ranking stage feedback but the evaluation is on a broader
recall dataset. Also, compared to the ranking stage, simpler pre-
rank models may perform worse and less consistently. Traditional
methods to tackle SSB issues include using all recall results and
treating unexposed portions as negatives for training, which can be
costly and noisy. To boost performance and consistency, some pre-
ranking feature interaction enhancers don’t fully fix consistency
issues, while methods like knowledge distillation in ranking models
ignore exposure bias. Our proposed framework targets these issues
with three integral modules: Sample Selection, Domain Adaptation,
and Unbiased Distillation. Sample Selection filters recall results
to mitigate SSB and compute costs. Domain Adaptation enhances
model robustness by assigning pseudo-labels to unexposed sam-
ples. Unbiased Distillation uses exposure-independent scores from
Domain Adaptation to implement unbiased distillation for the pre-
ranking model. The framework focuses on optimizing pre-ranking
while maintaining training efficiency. We introduce new metrics
for pre-ranking evaluation, while experiments confirm the effec-
tiveness of our framework. Our framework is also deployed in real
industrial systems.
CCS CONCEPTS
•Information systems →Retrieval models and ranking; Data
mining; •Computing methodologies →Machine learning;
Artificial intelligence.
* Equal contribution.
†Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671580KEYWORDS
Pre-ranking, Multi-Stage Cascading System, Intermediary Chal-
lenges
ACM Reference Format:
Jianping Wei, Yujie Zhou, Zhengwei Wu, and Ziqi Liu. 2024. Enhancing Pre-
Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cas-
cading Recommendation Systems. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 9 pages. https:
//doi.org/10.1145/3637528.3671580
1 INTRODUCTION
Large-scale search engines and recommendation systems need to
swiftly serve users with relevant results from a vast pool of items.
These systems must adhere to strict latency constraints, which
make it impractical to evaluate a large number of items using a
single-stage complex model [ 9,15,19,20,32]. To maintain a balance
between efficiency and effectiveness, a typical solution employs
a three-stage cascading architecture [ 3,7,13,16,22,23,29,30],
which progresses through recall, pre-ranking, and ranking stages,
as shown in Figure 1. During the pre-ranking stage, the system
effectively refines the thousands of items from the recall stage to
a manageable subset, setting the stage for the final ranking. The
pre-ranking stage acts as a pivotal element that determines the
overall effectiveness of the ranking process.
Figure 1: Recommendation systems commonly use a three-
stage cascading process: firstly, the recall stage filters the
entire corpus to thousands of items using basic models or
rules; secondly, the pre-ranking stage narrows this down to
a few hundred candidates with more precise models; and
finally, the ranking stage selects the most relevant items for
the user with advanced models. The pre-ranking stage acts
as a critical bridge between recall and the final ranking stage.
5950
KDD ’24, August 25–29, 2024, Barcelona, Spain Jianping Wei, Yujie Zhou, Zhengwei Wu, & Ziqi Liu
In its intermediary role connecting the recall and final ranking
stages, the pre-ranking stage encounters two primary challenges1.
First, the pre-ranking stage depends on a training dataset with feed-
back information derived from the outcomes of the ranking stage.
Correspondingly, the pre-ranking stage needs to be evaluated on a
larger dataset derived from the outcomes of the recall stage. The sig-
nificant difference in the scope of these datasets precipitates issues
related to Sample Selection Bias (SSB). Second, simpler pre-ranking
models usually perform worse than the ranking models, causing
inconsistency in candidate set rankings between both stages. Since
pre-ranking lays a foundation for the final candidate selection in
the ranking stage, its accuracy is crucial. While the impact of the
pre-ranking stage on ultimate business outcomes is indirect, it is
significantly influenced by the subsequent ranking stage. There-
fore, ensuring consistency between the pre-ranking stage and the
ranking stage is essential.
To address the first challenge, current methods [ 22] integrate the
recall results directly into the training process of pre-ranking and
assign negative labels by default to samples without feedback (the
unexposed portions). To tackle the second challenge, initial research
[12,13,18,30] have concentrated on enhancing the precision of pre-
ranking models through the refinement of feature interaction and
selection mechanisms. Although these strategies yield performance
gains, they do not fully address the consistent issue inherent in
ranking tasks. The current approaches [ 22,25] employ ranking
models for knowledge distillation, aiming to improve efficacy while
ensuring consistency across ranking stage outputs.
Nonetheless, several issues persist with these methods [ 12,13,
18,22,25,30]. Initially, in addressing the first challenge, two critical
aspects were overlooked: the sizable recall result set generates sub-
stantial computational costs and inhibits efficient training, while
the presumption that all unexposed items are negative introduces
inadvertent noise into the training procedure. When confronting
the second challenge, existing methods fail to consider exposure
bias. As ranking models are tailored to the exposed dataset, their
performance often deteriorates on unexposed items, further com-
pounding exposure bias. These methods employ undiscriminating
distillation, neglecting to assess which aspects of the precision distil-
lation model merit the most attention. Furthermore, most methods
[12,13,18,25,30] tackle only a single intermediate challenge. There
are also methods [ 22] that can address two concurrently, but they
require simultaneous optimization in three stages, and the complex-
ity of this process makes comprehensive optimization difficult in
real-world scenarios. Therefore, the autonomy of each stage and the
efficacy of pre-ranking training are also of paramount importance
in multi-stage cascading recommendation systems.
To address these intermediary challenges in current recommen-
dation systems, we introduce a comprehensive framework SIDA
composed of three integral modules: Sample Selection, Domain
Adaptation, and Unbiased Distillation. The Sample Selection mod-
ule, which addresses Sample Selection Bias (SSB), employs a refined
process to identify and prioritize hard-to-distinguish samples from
the recall set. This method addresses SSB and enhances the effi-
ciency of model training. Additionally, our framework incorporates
1The subsequent discussion will refer to these two challenges as intermediary
challenges.aDomain Adaptation module, which assigns pseudo-labels to
samples that have not been exposed. This strategy mitigates the
lack of immediate feedback and contributes to the robustness of
the model. The strength of our Domain Adaptation module lies in
its ability to negate the effects of exposure, enabling the generation
of exposure-independent scores. Consequently, the Unbiased Dis-
tillation module leverages these exposure-independent scores as
a constraint when implementing ranking models for distillation,
thus facilitating exposure-unbiased distillation. Our framework’s
design is such that it circumvents the complexity of three-stage
joint optimization by focusing solely on the optimization of the pre-
ranking stage, as opposed to optimizing recall and ranking stages.
This confers a significant simplification to the optimization pro-
cess. Furthermore, while the three modules function as a cohesive
unit, they are trained independently. Sample Selection and Domain
Adaptation modules provide tailored samples and corresponding
scores to the Unbiased Distillation module upon completion of their
respective training phases. The pre-ranking model is then refined
within the Unbiased Distillation module. Compared to traditional
pre-ranking models, our approach incorporates supplementary sam-
ple and label data, yet it does not compromise training efficiency.
Our framework, encompassing three novel modules, is engineered
to enhance the performance of current pre-ranking models. It uti-
lizes information from the recall stage as well as the ranking stage
and actively anticipates and mitigates issues stemming from the
incorporation of new information. This methodology is crafted to
deliver performance gains while concurrently preserving efficiency.
Our paper has the following contributions.
•Problem: We have thoroughly analyzed the intermediary
challenges faced by the pre-ranking stage of recommenda-
tion systems and potential issues faced in addressing these
challenges.
•Framework: To address these challenges, we have devised
an innovative framework that integrates three key modules:
Sample Selection, Domain Adaptation, and Unbiased Distilla-
tion. This framework is designed to effectively leverage data
from both the recall and ranking stages, ultimately enhanc-
ing the pre-ranking stage by merging insights from both
processes coherently.
•Evaluation: The effectiveness of our framework is corrobo-
rated through a comprehensive suite of experiments, which
validate the performance improvements brought about by
our approach. We have proposed new evaluation indicators
and analysis methods to study the impact of pre-ranking.
2 RELATED WORK
In research on recommendation systems, the pre-ranking stage has
gained particular attention as a vital component within the cascad-
ing structure of these systems, especially on commercial platforms.
Pre-ranking is essential for improving the effectiveness and compu-
tational efficiency of the system as it filters the vast number of can-
didate items before they are processed by the downstream ranking
models. Recognizing the importance of pre-ranking, there has been
substantial research aimed at enhancing its accuracy. This body of
work includes advancements in feature interaction and selection
mechanisms [ 12,13,18,30], demonstrating the field’s commitment
5951Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
to optimizing the trade-off between performance and computa-
tional load. Specifically, considering that previous models often
reduced interactions to simple vector products, potentially leading
to notable drops in performance, an alternative line of research has
focused on enhancing higher-order feature interactions while also
reducing the latency of online operations. Innovations include the
introduction of fine-grained and early-stage feature interactions
[13], the use of dense layers, Squeeze-and-Excitation blocks, and
optimization techniques to improve engineering efficiency [ 30].
Additionally, methods like the feature selection methodology based
on feature complexity and variational dropout (FSCD) [ 18], as well
as network architecture search (NAS) for optimizing the feature set
and architecture for pre-ranking models [ 14], represent significant
advances in addressing the computational challenges while striving
for high model performance.
While initial research has mainly focused on the precision of pre-
ranking models, it has often overlooked how these models interact
with the recall and final ranking stages, disregarding the inherent
interdependencies within the recommendation process. More recent
studies have attempted to integrate insights from these adjacent
stages into pre-ranking. For example, research has been conducted
on the impact of ranking models on the accuracy of pre-ranking
[25], and on the development of joint training frameworks that align
the different stages [ 22]. Despite these advancements, there is still a
gap in understanding the complexities that emerge when deploying
these models in real-world industrial environments. Challenges
such as heightened operational costs and the introduction of noise
and bias persist, raising concerns about the practical efficiency
and accuracy of pre-ranking models when implemented in actual
settings.
3 APPROACH
3.1 Preliminaries
Recommender systems learn user preferences through interactions
denoted by quadruplets {(𝑢,𝑖,𝑟,𝑒)}. Here,𝑢represents a user from
a user set𝑈,𝑖is an item from an item set 𝐼, and𝑒is the exposure
variable with 𝑒=1for the item presented to the user and 𝑒=0
otherwise. The binary variable 𝑟indicates a click ( 𝑟=1) or no click
(𝑟=0), given𝑒=1.
Our study employs a three-stage cascading architecture. In the
recall stage, a candidate list 𝐾𝑟={(𝑢,𝑖,𝑠𝑟)}of top𝑁𝑟𝑒𝑐𝑎𝑙𝑙 items
is generated by the recall stage model for each user from all items
𝐼, with𝑠𝑟representing the score of the recall stage model. This is
followed by the pre-ranking stage, which narrows down the list
to the top𝑁𝑝𝑟𝑒candidates𝐾𝑝={(𝑢,𝑖,𝑠𝑝)}from𝐾𝑟, where𝑠𝑝
is the pre-ranking model score for the item to each user. Finally,
during the ranking stage, the pre-ranked items are further ranked
to identify the top 𝑁𝑟𝑎𝑛𝑘 candidates𝐾𝑓={(𝑢,𝑖,𝑠𝑓)}from𝐾𝑝,
with𝑠𝑓being the score from the final ranking model for each user.
The top𝑁𝑟𝑎𝑛𝑘 items after the ranking stage are recommended to
users. User interactions with these recommendations are recorded
as a sample set 𝑆𝐸={(𝑢,𝑖,𝑟,𝑒 =1)}. Additionally, we define the
set𝑆𝑁𝐸as𝑆𝑁𝐸=𝐾𝑟(𝑢,𝑖,𝑒 =0)=𝐾𝑟−𝑆𝐸, which represents the
unexposed portion of 𝐾𝑟and constitutes the majority of 𝐾𝑟.3.2 The Framework of Our Approach
Figure 2: The Framework of Our Approach
To better serve as a bridge between the recall and the ranking
stage, we propose the framework for the pre-ranking stage shown
in Figure 2.
To mitigate the Sample Selection Bias (SSB) between the feed-
back sample 𝑆𝐸and the evaluation sample 𝐾𝑟in the pre-ranking
model, it is necessary to incorporate the 𝑆𝑁𝐸. However, 𝑆𝑁𝐸is
often redundant and voluminous. To efficiently manage this, we
introduce the Sample Selection Module, which leverages informa-
tion from the exposed samples 𝑆𝐸to selectively downsample the
unexposed data 𝑆𝑁𝐸. This process yields a refined subset of unex-
posed samples, denoted by 𝑆′
𝑁𝐸, which is more advantageous for
the model. Consequently, we compile a novel model sample set
𝑆=𝑆𝐸∪𝑆′
𝑁𝐸.
To label the unexposed component 𝑆′
𝑁𝐸, we introduce a Domain
Adaptation Module that creates a set of samples 𝐷={𝐷𝑠,𝐷𝑡}.
Here,𝐷𝑠represents the source domain samples derived from 𝑆𝐸,
and𝐷𝑡consists of samples taken from 𝑆′
𝑁𝐸. By employing do-
main adaptation techniques [ 8], we mitigate the effects of exposure
bias, facilitating the transfer of information from exposed samples
𝐷𝑠to unexposed samples 𝐷𝑡. Consequently, we obtain exposure-
independent scores 𝑠𝑑, which can serve as pseudo labels for 𝑆′
𝑁𝐸
and as constraints to correct exposure bias during distillation. We
use existing ranking models to score sample 𝑆and obtain the cor-
responding score 𝑠𝑓for distillation.
At this stage, we have expanded the sample set 𝑆, along with the
exposure-agnostic score 𝑠𝑑for each sample, and the prediction score
𝑠𝑓provided by the ranking model for 𝑆. We propose the Unbiased
Distillation Module to integrate this information and derive the
final pre-ranking score.
3.3 Sample Selection
𝑆𝑁𝐸is difficult to filter because there is no feedback information.
Existing methods [ 5,6,11,26–28,31] are trained based on exposure
(feedback) information, which has low accuracy for 𝑆𝑁𝐸. There-
fore, model-based sampling methods are not feasible. Furthermore,
within𝑆𝑁𝐸, a majority of samples tend to be negative and can be
considered redundant in the context of model training. This ob-
servation leads us to the concept of hard-to-distinguish samples
[5,31]. Although unexposed samples are typically treated as nega-
tive samples, there exist certain samples that are close to positive
samples and are situated on the classification boundary. These sam-
ples can provide significant information gain for the model and are
often referred to as hard-to-distinguish samples. Consequently, our
5952KDD ’24, August 25–29, 2024, Barcelona, Spain Jianping Wei, Yujie Zhou, Zhengwei Wu, & Ziqi Liu
Figure 3: The Sample Selection Module
approach incorporates this principle to strategically select these
challenging samples. We define the degree to which the samples
are hard-to-distinguish as ℎ, and we hope to select samples with
a largerℎ. Inspired by the insights presented by [ 1], we propose a
graph-based approach that is inherently model-agnostic. Specifi-
cally, we correlate ℎwith the topology of the graph, which means
if the connectivity between 𝑢and𝑖is stronger on the graph, then
theℎof a sample composed of 𝑢and𝑖is also larger. Intuitively, the
stronger the connectivity between 𝑢and𝑖, the more likely 𝑢is to
click on𝑖, and therefore the sample of 𝑢and𝑖will be closer to the
positive sample. The proof of relationship between connectivity
andℎcan be found in [ 1]. This graph-based perspective serves to
counter the lack of adequate exposure information at the individ-
ual user-item pair level by exploiting the multi-hop relationships
inherent in the network structure. It allows for the computation of
connectivity between users and items, even in the absence of direct
links, which is particularly crucial for samples that are inherently
hard-to-distinguish characteristics.
As shown in Figure 3, we construct a bipartite graph 𝐺=(𝑈,𝐼,𝐸)
using the historical click behavior 𝑆𝐸,𝑟=1=(𝑢,𝑖,𝑟 =1,𝑒=1), where
𝑈is the user set, 𝐼is the item set, and 𝐸represents the click-based
connections between users and items. Edge weight 𝑤𝑢𝑖reflects the
click count of user 𝑢on item𝑖. We explore user-item connectivity
via random walks to predict the likelihood of navigating from a
given user𝑢to various items 𝑖. The transition probability from user
𝑢to item𝑖is defined as:
𝑃(𝑢→𝑖)=𝑤𝑢𝑖Í
𝑗∈𝐼(𝑢)𝑤𝑢𝑗,
where𝐼(𝑢)denotes the items to which user 𝑢is connected. The
reverse transition from item 𝑖to user𝑢is given by:
𝑃(𝑖→𝑢)=𝑤𝑢𝑖Í
𝑣∈𝑈(𝑖)𝑤𝑣𝑖,
At each random walk step, the subsequent node is chosen based on
the transition probabilities 𝑃(𝑢→𝑖)or𝑃(𝑖→𝑢), depending on
whether the current node is a user or an item, respectively.
After𝑇steps, the frequency of reaching each item node 𝑖from
user node𝑢is recorded. During the random walk, let 𝐶(𝑢,𝑖,𝑡)
represent the count of visits to item 𝑖by user𝑢in𝑡steps, calculatedusing the indicator function:
𝐶(𝑢,𝑖,𝑡)=𝑡∑︁
𝜏=11𝑋𝜏=𝑖,
where 1𝑋𝜏=𝑖is 1 if the random walk is at state 𝑖at time𝜏, otherwise
0. Finally, to obtain the probability distribution, the counts 𝐶(𝑢,𝑖,𝑇)
are normalized:
ℎ=ℎ𝑢,𝑖=𝐶(𝑢,𝑖,𝑇)Í
𝑖′∈𝐼𝐶(𝑢,𝑖′,𝑇),
Then we filter 𝑆𝑁𝐸throughℎand select samples with ℎabove a
certain value as 𝑆′
𝑁𝐸by implementing a set sampling ratio. Finally,
we can get the raw sample set 𝑆=𝑆𝐸∪𝑆′
𝑁𝐸for the pre-ranking
model in the Unbiased Distillation Module.
3.4 Domain Adaptation
Figure 4: The Domain Adaptation Module
The𝑆′
𝑁𝐸obtained from the Sample Selection Module is unlabeled.
In this section, we generate pseudo labels for the unlabeled samples.
The main difference between 𝑆𝐸and𝑆′
𝑁𝐸comes from whether they
are exposed or not, so we construct a domain adaptation model [ 8]
to remove the influence of exposure and transfer 𝑆𝐸’s information
to𝑆′
𝑁𝐸. As shown in Figure 4, we sample 𝑆𝐸as the source domain
sample as𝐷𝑠and𝑆′
𝑁𝐸as the target domain sample as 𝐷𝑡. Firstly,
we map input 𝐷𝑠and𝐷𝑡to D-dimensional feature vectors 𝑓𝑠and
𝑓𝑡using the same feature extractor 𝐺𝑓.
𝑓𝑠,𝑓𝑡=𝐺𝑓(𝐷𝑠,𝐷𝑡)
To eliminate serious domain discrepancy between the source sample
and the target sample, we introduce the adaptive embedding shift
to the source sample as:
𝑓′,𝑠=𝑓𝑠+𝑒𝑠
where𝑒𝑠is an initialization learnable parameter embedding, with
the same dimension as 𝑓𝑠. Then𝑓′,𝑠is mapped by a label predictor
𝐺𝑦to the label𝑟, and are optimized using the loss function 𝐿𝑠𝑟:
ˆ𝑟𝑠=𝐺𝑦(𝑓′,𝑠)
𝐿𝑠
𝑟=−1
𝑇∑︁
𝑇(𝑟𝑙𝑜𝑔(ˆ𝑟𝑠)+(1−𝑟)𝑙𝑜𝑔(1−ˆ𝑟𝑠))
To remove exposure bias, we need to make the features 𝑓′,𝑠
and𝑓𝑡exposure-invariant. Therefore, 𝑓′,𝑠and𝑓𝑡are mapped by
a Domain Classifier 𝐺𝑑to the domain label 𝑑where𝑑=1when
the sample is from the source domain and 𝑑=0when the sample
is from the target domain. And a Gradient Reverse Layer [ 8] has
5953Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
been added, with no parameters to learn, to make it impossible to
distinguish between source and domain in domain prediction. This
part is optimized using the loss function 𝐿𝑑:
ˆ𝑑=(ˆ𝑑𝑠,ˆ𝑑𝑡)=𝐺𝑑(𝑓′,𝑠,𝑓𝑡)
𝐿𝑑=−1
𝑇∑︁
𝑇(𝑑𝑙𝑜𝑔(ˆ𝑑)+(1−𝑑)𝑙𝑜𝑔(1−ˆ𝑑))
The final loss function is 𝐿=𝛼1𝐿𝑠𝑟+𝛼2𝐿𝑑, where𝛼1and𝛼2are
the hyperparameter. In the end, we can obtain the score 𝑠𝑑on the
𝑆sample.
3.5 Unbiased Distillation
Figure 5: The Unbiased Distillation Module
Here we will train the final pre-ranking model. Unlike conven-
tional pre-ranking models that use 𝑆𝐸={(𝑢,𝑖,𝑟,𝑒 =1)}samples
as input, our model uses the sample set 𝑆obtained by the Sample
Selection module, where the labels 𝑦={𝑟,𝑠𝑑}, in addition to the
clicked𝑟labels of the exposed parts 𝑆𝐸, also include the pseudo
labels𝑠𝑑in the exposed parts 𝑆′
𝑁𝐸obtained by the Domain Adap-
tation module. In addition, we also have scores 𝑠𝑓of the ranking
model and𝑠𝑑on the𝑆sample set, which are used for distillation
and constrained distillation, respectively.
After inputting user and item features, we can obtain the final
predicted value ˆ𝑦through the Click Model2. The model is optimized
through the following loss function:
𝐿𝑦𝑙=−1
𝑇∑︁
𝑇(𝑦𝑙𝑜𝑔(ˆ𝑦)+(1−𝑦)𝑙𝑜𝑔(1−ˆ𝑦))
𝐿𝑑𝑖𝑠=𝑤𝑖1
𝑛𝑛∑︁
𝑖=1(𝑠𝑓
𝑖−ˆ𝑦𝑖)2
𝑤𝑖=[𝜏(𝑠𝑓>𝑝3|𝑠𝑑>𝑝1)+𝜏(𝑠𝑓<𝑝3|𝑠𝑑<𝑝2)]𝛾
|𝑠𝑓−𝑠𝑑|
+𝜏(𝑠𝑓>=0|𝑝2<𝑠𝑑<𝑝1)
where𝑤𝑖is a weight calculated based on 𝑆𝑓and𝑆𝑑, used to
constrain distillation information. Here, 𝑝1,𝑝2, and𝑝3are three
confidence thresholds. 𝑝1and𝑝2are used to select 𝑠𝑑of high con-
fidence.𝑝3is used to distinguish positive and negative samples,
usually set to 0.5. 𝜏(𝑎|𝑏)=1if condition 𝑎and condition 𝑏is satis-
fied.𝛾is used to control the magnitude of the constraint force.
The final loss function is 𝐿=𝛽1𝐿𝑦𝑙+𝛽2𝐿𝑑𝑖𝑠, where𝛽1and𝛽2
are the hyperparameter.
2It can be any pre-ranking model3.6 Deployment
Figure 6: The Deployment of Our Framework
Although we added multiple modules during the pre-ranking
stage, we did not increase the online response time because these
modules solely serve offline training. In reality, only one well-
trained pre-ranking model is used for online services. In addition,
we also utilize some computational acceleration operations online.
The Deployment is shown in Figure 6.
3.6.1 Offline. Upon user interaction with the online recommenda-
tion system, the platform captures and records a series of logs at
various stages. These logs detail the outcomes of distinct stages, en-
compassing recall results, pre-ranking results, ranking results, final
exposure data, and user feedback. These logs will be stored stored
in Maxcompute3. Feedback provided by users and historical behav-
ioral data, when combined with initial recall outcomes, are ingested
by the Sample Selection module to identify hard-to-distinguish ex-
amples. Subsequently, these hard-to-distinguish examples, together
with exposure data, undergo processing in the Domain Adaptation
module to deduce pseudo labels. Then we will deploy the trained
model files to the online serving platform. Concomitant with the
deployment of the pre-ranking model, a lookup table is constructed
with item IDs as the key and item vector representation as the
value. This lookup table is crucial for accelerating computations
and will be explained in detail later in the subsequent sections.
Moreover, user features are extracted offline from the accumulated
logs and subsequently integrated in real-time with the pre-ranking
model and the item lookup directory to enhance recommendation
accuracy and performance.
3.6.2 Online. During real-time requests, due to the need to sort
multiple items for each user, a single set of user features encounters
multiple sets of item features. Meanwhile, in comparison with rank-
ing models, pre-ranking models are tasked with processing a signif-
icantly larger set of candidates in an online environment. Querying
and computing multiple-item features can be time-consuming. To
address this, item representation vectors are efficiently retrieved
3https://www.alibabacloud.com/product/maxcompute
5954KDD ’24, August 25–29, 2024, Barcelona, Spain Jianping Wei, Yujie Zhou, Zhengwei Wu, & Ziqi Liu
during online inference by querying an item lookup table using
item IDs [ 3], thereby circumventing these time-intensive processes.
It is important to note that, unlike the relatively static nature of item
features, user features can change frequently. However, since user
features require only a single query and computation per request,
their influence on the overall processing time remains negligible.
As a result, user features are consistently obtained and the user
representation vector is calculated in real time. Subsequently, in-
teractions between user and item vectors are established through
either a dot product or by concatenation, followed by processing
through subsequent neural network layers.
4 EXPERIMENT
In this section, we first set up the experiments, then report the
results and give a detailed analysis.
4.1 Experimental Setup
4.1.1 Datasets. To assess the performance of our proposed frame-
work, we conducted a series of experiments using two benchmark
datasets derived from recommendation systems and web search
contexts, namely MovieLens-1M (ML-1M)4and Tmall5. Addition-
ally, we implemented an online A/B test on a proprietary industrial
dataset in real industrial environments.
•The ML-1M dataset is composed of user-provided movie rat-
ings and includes data about 6,040 users and 3,706 movies.
Recognizing the ubiquity of implicit feedback in real-world
scenarios, we classified ratings of four and above as positive
signals of user preference, whereas ratings below this thresh-
old are regarded as negative signals. For data preparation, we
treated interactions with ratings as exposed data and pairs
of users and items without ratings as unexposed data.
•Originating from the Alibaba Group, the Tmall dataset cap-
tures user purchase behavior on the Tmall e-commerce plat-
form within the period from May 2015 to November 2015. It
reflects the transactions of 424,170 users involving 1,090,390
distinct items. We consider these historical transactions as
positive examples. In alignment with the practices outlined
in the RankFlow [ 22], we randomly selected a subset of non-
transacted instances as negative samples. Consequently, we
generated a balanced dataset comprising both positive and
negative samples, identified as exposed data. The remainder
of the user-item pairs were categorized as unexposed data.
All the above datasets are split into train, validation, and test
sets using timestamps [20, 21].
4.1.2 Compared Methods. Within the framework of the recommen-
dation pipeline, which encompasses three sequential stages—recall,
pre-ranking, and ranking—we elected to employ dual-tower ar-
chitectures for the recall stage, particularly the Deep Structured
Semantic Model (DSSM) [ 12], which are adept at producing vector
embeddings for both user queries and documents, facilitating effi-
cient retrieval of relevant items. During the inference process, the
recall stage is conducted using a maximum inner product search,
4https://grouplens.org/datasets/movielens/1m/
5https://tianchi.aliyun.com/dataset/42which is efficiently carried out by employing the FAISS library6.
For the ensuing pre-ranking and ranking stages, a range of well-
established models are utilized. This suite includes LR [ 4], FM [ 24],
DeepFM [10], WDL [2].
In our research, we implemented models for the recall stage,
pre-ranking stage, and ranking stage. Our main focus is on op-
timizing the pre-ranking model. To enable fair comparisons, we
have kept the recall and ranking models consistent, allowing any
differences to be attributed solely to changes in the pre-ranking
model. As large-scale cascading recommendation pipelines are un-
derexplored academically, we draw from [ 1] and use two classic
cascade structures for baseline comparisons: the widely-used Inde-
pendent Cascade Component (ICC) and Rankflow which processes
both unexposed and exposed data. Therefore, we conduct three
comparative experiments in this context:
•ICC: The pre-ranking model is trained in isolation, without
any interaction with other stages of the architecture.
•Rankflow: This approach entails a joint training framework
that encompasses all three stages. The models are optimized
concurrently; however, for our analysis, we extract and eval-
uate only the pre-ranking model.
•Our proposed SIDA framework incorporates a preliminary
pre-ranking model informed by three distinct modules.
4.1.3 Evaluation Metrics. In addition to commonly used metrics in
traditional recommendation systems, we introduce supplemental
metrics to ascertain the validity of our proposed approach.
•Our primary focus is on the efficacy of the ranking stage, con-
sidering its direct impact on business operations. To evaluate
this stage, we have selected key performance metrics: Hit Ra-
tio at rank𝐾(HR@K), Normalized Discounted Cumulative
Gain at rank 𝐾(NDCG@K), and Mean Average Precision at
rank𝐾(MAP@K)7. These metrics are specifically chosen to
assess the impact of optimizing the pre-ranking model on
the performance of the ranking model.
•To demonstrate that our method mitigates the SSB issue, we
introduce the penetration rate of unexposed samples at 𝐾
(PRU@K), i.e., the proportion of unexposed samples in the
pre-ranking result. PRU is defined as
𝑃𝑅𝑈=𝑁𝑢
𝐾
where𝑁𝑢is the number of unexposed samples in the top 𝐾
pre-ranking results.
•To quantify the consistency between the pre-ranking and
ranking stage, we establish the metric overlap at 𝐾(over-
lap@K), which represents the degree of overlap between the
top𝐾items scored by the pre-ranking and the top K items
scored by the ranking. It is calculated as follows:
𝑜𝑣𝑒𝑟𝑙𝑎𝑝 @𝐾=𝑟𝑎𝑛𝑘@𝐾∩𝑝𝑟𝑒𝑟𝑎𝑛𝑘 @𝐾
𝐾
where𝑟𝑎𝑛𝑘@𝐾is the top𝐾items scored by the ranking
model,𝑝𝑟𝑒𝑟𝑎𝑛𝑘 @𝐾is the top𝐾items scored by the pre-
ranking model.
6https://faiss.ai
7Formula calculation can refer to [17]
5955Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Experimental results on ML-1M dataset.
Stage Pre-ranking Ranking
Group of cascading systemMethodMetricAUC PRU@10 Overlap@10 HR@5 NDCG@5 MAP@5
DSSM+LR+FMICC 0.7809 0.9261 0.2780 0.1956 0.0869 0.0607
Rankflow 0.7793 0.9148 0.2551 0.1955 0.0867 0.0606
SIDA 0.7840 0.9271 0.2824 0.1956 0.0873 0.0611
DSSM+LR+DeepFMICC 0.7809 0.9261 0.2444 0.2104 0.0938 0.0661
Rankflow 0.7794 0.9148 0.2482 0.2104 0.0929 0.0650
SIDA 0.7847 0.9353 0.2656 0.2106 0.0945 0.0667
DSSM+LR+WDLICC 0.7809 0.9261 0.3097 0.1685 0.0711 0.0479
Rankflow 0.7794 0.9148 0.3069 0.1692 0.0709 0.0477
SIDA 0.7978 0.9405 0.3257 0.1685 0.0718 0.0487
Table 2: Experimental results on Tmall dataset.
Stage Pre-ranking Ranking
Group of cascading systemMethodMetricAUC PRU@10 Overlap@10 HR@5 NDCG@5 MAP@5
DSSM+LR+FMICC 0.7900 0.9878 0.2040 0.0553 0.0366 0.0308
Rankflow 0.7899 0.9879 0.2039 0.0552 0.0365 0.0308
SIDA 0.8059 0.9885 0.2073 0.0528 0.0363 0.0311
DSSM+LR+DeepFMICC 0.7899 0.9884 0.1396 0.0598 0.0386 0.0323
Rankflow 0.7900 0.9884 0.1395 0.0598 0.0386 0.0323
SIDA 0.8175 0.9892 0.1384 0.0582 0.0387 0.0330
DSSM+LR+WDLICC 0.7899 0.9895 0.2131 0.0502 0.0346 0.0299
Rankflow 0.7899 0.9895 0.2131 0.0503 0.0347 0.0299
SIDA 0.8168 0.9898 0.2099 0.0515 0.0352 0.0303
Table 3: Experimental results on different groups of result
number.
result numberMethodMetricHR@5 NDCG@5 MAP@5
[3000, 300, 30]ICC 0.1956 0.0869 0.0607
Rankflow 0.1954 0.0867 0.0605
SIDA 0.1956 0.0873 0.0611
[1000, 100, 10]ICC 0.1954 0.1099 0.0844
Rankflow 0.1954 0.1098 0.0844
SIDA 0.1958 0.1100 0.0845
[100, 50, 10]ICC 0.1956 0.1098 0.0843
Rankflow 0.1961 0.1100 0.0844
SIDA 0.1962 0.1101 0.0845
•As mentioned earlier, improving the accuracy of the pre-
ranking model is also very important. We have used the Area
Under the Curve (AUC) here to measure the improvement
in the accuracy of the pre-ranking model.
4.2 Offline Evaluation
To assess the effectiveness of our framework to final recommenda-
tion outputs, which are ultimately presented to the users, we em-
ployed a range of metrics such as HR@K, NDCG@K, and MAP@K
for the ranking stage when the pre-ranking is modified. Meanwhile,
we compared AUC across the pre-ranking outputs, alongside Over-
lap at𝐾(Overlap@K) for outcomes between the pre-ranking andTable 4: The Ablation Study.
MethodMetricAUC
LR 0.7809
LR+Sample+Negative 0.7796
LR+Sample+0.5 0.7812
LR+Sample+Pseudo 0.7814
LR+Sample+Pseudo+Dis 0.7847
ranking stages. We did not compare AUC on the ranking results
because the ranking model remained the same across each group
of experiments, making it meaningless to compare these metrics
in the ranking stage. Considering that unexposed samples are not
necessarily negative samples, we hope to recall more unexposed
samples with the same accuracy. Therefore, PRU@K is used to eval-
uate the pre-ranking stage. To comply with the assumption that the
pre-ranking model is simpler than ranking, we choose LR as the
model for the pre-ranking stage, and the remaining FM, DeepFM,
and WDL as the models for the ranking stage. The number of items
returned in each stage is 𝑁𝑟𝑒𝑐𝑎𝑙𝑙 =3000,𝑁𝑝𝑟𝑒=300,𝑁𝑟𝑎𝑛𝑘=30.
Table 1 shows the performances on the ML-1M dataset. The re-
sults of each group of experiments on the Tmall dataset are shown in
Table 2. From the experimental results, we can observe that although
there are differences in the combination of different three-stage
models, our SIDA framework always achieves the best performance.
5956KDD ’24, August 25–29, 2024, Barcelona, Spain Jianping Wei, Yujie Zhou, Zhengwei Wu, & Ziqi Liu
4.3 The Analysis and Ablation Study
In the cascading system, the number of items returned in each stage
can have a significant impact on the final recommendation effects.
In section 4.2, we set 𝑁𝑟𝑒𝑐𝑎𝑙𝑙 =3000,𝑁𝑝𝑟𝑒=300,𝑁𝑟𝑎𝑛𝑘=30, and com-
pared the effects under different groups of cascading systems. In
this section, we select DSSM+LR+FM as the cascading system and
conduct experiments with different groups of returned item num-
bers on the ML-1M dataset. To observe the final effectiveness of the
recommendations, we focus on evaluation metrics in the ranking
stage. We set the returned item number of the cascading system
as [𝑁𝑟𝑒𝑐𝑎𝑙𝑙 ,𝑁𝑝𝑟𝑒,𝑁𝑟𝑎𝑛𝑘]. The experiment result is shown in Table
3. It can be observed that even within the same method, there is a
significant difference in the evaluation results when the result num-
bers are varied. Nevertheless, even in comparison with different
groups of result numbers, SIDA outperforms other methods.
As mentioned earlier, unexposed data may not necessarily be
negative samples, and models based on exposed data tend to score
unexposed data negatively. Our Sample Selection module selects
hard-to-distinguish samples from the unexposed samples, which are
samples close to the 0-1 boundary. The Domain Adaptation module
will generate pseudo labels for this part by removing the influence
of the exposure domain. Figure 7 shows the scoring distribution
on our selected unexposed samples. We chose the Deepfm model
Figure 7: The Scoring Distribution On Unexposed Samples
with the highest AUC as the comparison model. From the solid
blue line, it can be seen that the score of Deepfm on this unexposed
sample is generally biased towards 0. Correspondingly, the solid
yellow line shows that the score of the Domain Adaptation model
is more uniform. Among them, we screened out the samples with
top hard-to-distinguish scores from the unexposed samples, and it
can be seen that the Domain Adaptation model can recognize that
these samples are generally more inclined toward 1.
To evaluate the contribution of each module in SIDA, we con-
ducted ablation experiments as shown in Table 4. Among them, LR
is the most primitive model, trained on exposed samples without
distillation. LR+Sample+Negative is based on the LR model, whichadds the unexposed samples filtered by the Sample Selection mod-
ule. The label of these samples is set to 0, and their AUC slightly
decreases. It can be seen that treating unexposed samples as neg-
ative samples will introduce noise and affect the effectiveness of
the model. LR+Sample+0.5 sets the label of the filtered unexposed
samples to 0.5, which further enhances their AUC. 0.5 is between
0 and 1, which also verifies our hypothesis that unexposed sam-
ples are not pure negative samples. The improvement in effective-
ness also proves the effectiveness of the Sample Selection module.
LR+Sample+Pseudo assigns pseudo labels to the unexposed samples
filtered out through the Domain Adaptation module. It can be seen
that AUC has also been further improved, proving the effective-
ness of the Domain Adaptation module. LR+Sample+Pseudo+Dis
is based on LR+Sample+Pseudo and introduces an Unbiased Dis-
tillation module for distillation. The effectiveness of the Unbiased
Distillation module can be seen from the improvement of AUC.
4.4 Online A/B Test
To evaluate the efficacy of our proposed method, we implemented
an A/B test within a real recommendation system over the period
from October 2023 to November 2023. We conducted the A/B test
involving over ten million users and more than one billion expo-
sure samples, our method achieved a 7.12% confidence8increase in
Click-Through Rate (CTR). This outcome substantiates the poten-
tial of our method to enhance the actual business impact. Owing
to its remarkable performance in an actual business environment,
our method has been fully integrated into the recommendation
system and has established itself as the new standard, catering to
the primary user traffic.
5 CONCLUSION
This paper introduces a novel framework comprised of three main
components: Sample Selection, Domain Adaptation, and Unbiased
Distillation, aimed at addressing the intermediary challenges faced
during the pre-ranking stage of large-scale search engines and
recommendation systems. It tackles Sample Selection Bias and con-
sistency issues, solves the limitations of current methods, and offers
a holistic solution. The Sample Selection module efficiently identi-
fies hard-to-distinguish samples to mitigate SSB and enhance the
efficiency of model training, while the Domain Adaptation module
enhances robustness against exposure bias through pseudo-labeling.
The Unbiased Distillation module employs these adaptations to fa-
cilitate fair knowledge transfer. Our framework simplifies the opti-
mization by concentrating on the pre-ranking stage alone, avoiding
the complexities of a joint three-stage optimization. The effective-
ness of our approach is confirmed through extensive experiments,
which also introduce novel evaluation metrics and analytical tech-
niques for the pre-ranking stage.
ACKNOWLEDGMENTS
We sincerely appreciate the colleagues from different departments
of Ant Group, including but not limited to Jianping Wei, Yujie Zhou,
Zhengwei Wu, Ziqi Liu, for their valuable discussions, efforts, and
support. We thank all the reviewers and chairs for their time and
constructive comments.
8https://en.wikipedia.org/wiki/Confidence_interval
5957Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems KDD ’24, August 25–29, 2024, Barcelona, Spain
REFERENCES
[1]Xiaohui Chen, Jiankai Sun, Taiqing Wang, Ruocheng Guo, Li-Ping Liu, and Aonan
Zhang. 2023. Graph-Based Model-Agnostic Data Subsampling for Recommenda-
tion Systems. arXiv preprint arXiv:2305.16391 (2023).
[2]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[3]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191–198.
[4]David R. Cox. 1958. The Regression Analysis of Binary Sequences (with Discus-
sion). J Roy Stat Soc B 20 (1958), 215–242.
[5]Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, and Depeng Jin. 2020. Simplify
and robustify negative sampling for implicit collaborative filtering. Advances in
Neural Information Processing Systems 33 (2020), 1094–1105.
[6]William Fithian and Trevor Hastie. 2015. Local case-control sampling: Efficient
subsampling in imbalanced data sets. Quality control and applied statistics 60, 3
(2015), 187–190.
[7]Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, and J Shane Culpepper. 2019.
Joint optimization of cascade ranking models. In Proceedings of the twelfth ACM
international conference on web search and data mining. 15–23.
[8]Yaroslav Ganin and Victor Lempitsky. 2015. Unsupervised domain adaptation by
backpropagation. In International conference on machine learning. PMLR, 1180–
1189.
[9]Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang
He. 2021. An embedding learning framework for numerical features in ctr
prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 2910–2918.
[10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[11] Lei Han, Kean Ming Tan, Ting Yang, and Tong Zhang. 2020. Local uncertainty
sampling for large-scale multiclass logistic regression. (2020).
[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM international conference on
Information & Knowledge Management. 2333–2338.
[13] Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long,
Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, et al .2022. IntTower: the Next
Generation of Two-Tower Model for Pre-Ranking System. In Proceedings of the
31st ACM International Conference on Information & Knowledge Management.
3292–3301.
[14] Xiang Li, Xiaojiang Zhou, Yao Xiao, Peihao Huang, Dayao Chen, Sheng Chen,
and Yunsen Xian. 2022. AutoFAS: Automatic Feature and Architecture Selection
for Pre-Ranking System. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 3241–3249.
[15] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xi-
uqiang He, Zhenguo Li, and Yong Yu. 2020. Autofis: Automatic feature interaction
selection in factorization models for click-through rate prediction. In proceedings
of the 26th ACM SIGKDD international conference on knowledge discovery & data
mining. 2636–2645.
[16] Shichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. 2017. Cascade ranking for opera-
tional e-commerce search. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 1557–1565.[17] Tie-Yan Liu et al .2009. Learning to rank for information retrieval. Foundations
and Trends® in Information Retrieval 3, 3 (2009), 225–331.
[18] Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin, Kuang-
Chih Lee, Jian Xu, and Bo Zheng. 2021. Towards a Better Tradeoff between
Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based
Approach. In Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval. 2036–2040.
[19] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong
sequential behavior data for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management.
2685–2692.
[20] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang,
Xiuqiang He, and Yong Yu. 2021. Retrieval & interaction machine for tabular
data prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1379–1389.
[21] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020.
User behavior retrieval for click-through rate prediction. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 2347–2356.
[22] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui
Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of Multi-
Stage Cascade Ranking Systems as Flows. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
814–824.
[23] Vikas C Raykar, Balaji Krishnapuram, and Shipeng Yu. 2010. Designing efficient
cascaded classifiers: tradeoff between accuracy and cost. In Proceedings of the 16th
ACM SIGKDD international conference on Knowledge discovery and data mining.
853–860.
[24] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference
on data mining. IEEE, 995–1000.
[25] Jiaxi Tang and Ke Wang. 2018. Ranking distillation: Learning compact ranking
models with high performance for recommender system. In Proceedings of the
24th ACM SIGKDD international conference on knowledge discovery & data mining .
2289–2298.
[26] Daniel Ting and Eric Brochu. 2018. Optimal subsampling with influence functions.
Advances in neural information processing systems 31 (2018).
[27] HaiYing Wang. 2020. Logistic regression for massive data with rare events. In
International Conference on Machine Learning. PMLR, 9829–9836.
[28] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform negative
sampling and log odds correction with rare events data. Advances in Neural
Information Processing Systems 34 (2021), 19847–19859.
[29] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A cascade ranking model
for efficient ranked retrieval. In Proceedings of the 34th international ACM SIGIR
conference on Research and development in Information Retrieval. 105–114.
[30] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai.
2020. Cold: Towards the next generation of pre-ranking system. arXiv preprint
arXiv:2007.16122 (2020).
[31] Weinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013. Optimizing top-n
collaborative filtering via dynamic negative item sampling. In Proceedings of
the 36th international ACM SIGIR conference on Research and development in
information retrieval. 785–788.
[32] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059–1068.
5958