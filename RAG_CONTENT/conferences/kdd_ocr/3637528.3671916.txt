Going Where, by Whom, and at What Time: Next Location
Prediction Considering User Preference and Temporal Regularity
Tianao Sun
tianaosun@mail.sdu.edu.cn
School of Software
Shandong University
Jinan, Shandong, ChinaKe Fu
ke_fu@mail.sdu.edu.cn
School of Software
Shandong University
Jinan, Shandong, ChinaWeiming Huang
weiming.huang@ntu.edu.sg
School of Computer Science and
Engineering
Nanyang Technological University
Singapore
Kai Zhao
kzhao4@gsu.edu
Robinson College of Business
Georgia State University
Atlanta, Georgia, USAYongshun Gong
ysgong@sdu.edu.cn
School of Software
Shandong University
Jinan, Shandong, ChinaMeng Chen∗
mchen@sdu.edu.cn
School of Software
Shandong University
Jinan, Shandong, China
ABSTRACT
Next location prediction is a crucial task in human mobility model-
ing, and is pivotal for many downstream applications like location-
based recommendation and transportation planning. Although
there has been a large body of research tackling this problem, the
usefulness of user preference and temporal regularity remains un-
derrepresented. Specifically, previous studies usually neglect the
explicit user preference information entailed from human trajecto-
ries and fall short in utilizing the arrival time of next location, as a
key determinant on next location. To address these limitations, we
propose a Multi-Context aware Location Prediction model (MCLP)
to predict next locations for individuals, where it explicitly models
user preference and the next arrival time as context. First, we uti-
lize a topic model to extract user preferences for different types of
locations from historical human trajectories. Second, we develop
an arrival time estimator to construct a robust arrival time em-
bedding based on the multi-head attention mechanism. The two
components provide pivotal contextual information for the subse-
quent prediction. Finally, we utilize the Transformer architecture
to mine sequential patterns and integrate multiple contextual in-
formation to predict the next locations. Experimental results on
two real-world mobility datasets show that our proposed MCLP
outperforms baseline methods.
CCS CONCEPTS
•Information systems →Spatial-temporal systems; Data
mining.
∗Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671916KEYWORDS
Next Location Prediction; Human Mobility; User Preference; Tem-
poral Regularity
ACM Reference Format:
Tianao Sun, Ke Fu, Weiming Huang, Kai Zhao, Yongshun Gong, and Meng
Chen. 2024. Going Where, by Whom, and at What Time: Next Location
Prediction Considering User Preference and Temporal Regularity. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671916
1 INTRODUCTION
Today’s boom in location-based services has fostered an increas-
ingly deep understanding of how people move in our cities. The
massive human trajectories, collected from a variety of sources, e.g.,
mobile phone and traffic camera records, reflect meaningful human
activity patterns. Specifically, once we largely omit the overly de-
tailed intermediate routing points, the places where people engage
in meaningful activities like working, shopping, and studying, can
be revealed. This process results in the activity location sequences
of users, illustrated in Figure 1. Modeling mobility patterns from
activity sequences and thereby predicting individuals’ next activ-
ity locations play a pivotal role in various applications such as
location-based recommendation [ 20], traffic optimization [ 9,18],
and epidemic control [14, 26].
Next activity location prediction at the individual level is gener-
ally challenging given the inherent randomness of human behavior.
In this context, a large body of research has focused on tackling
this problem, primarily by mining sequential patterns from large-
scale human mobility data sets. Early studies capture and model
movement patterns using statistical models like matrix factoriza-
tion methods [ 17] or creating location transition matrices based
on Markov chain models [ 12]. In the past years, deep learning
techniques including recurrent neural networks (RNNs) and Trans-
formers have made remarkable strides in this field [ 28,31,35].
Further, the researchers not only extract long-term spatiotemporal
dependencies using a deep sequential model, but also model ad-
ditional information to achieve improved prediction performance.
2784
KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianao Sun et al.
Time
07:30 09:00 18:00 20:00Activity
Sequence
Mobility
RecordsResiding Working Eating Shopping
?
Figure 1: Illustration of human mobility records and the
corresponding activity sequence.
For example, [ 8,22] utilize aggregation mechanisms to take ac-
count of the deep interactions among different timestamps in each
sequence; [ 36] leverages auxiliary tasks such as next category pre-
diction to improve location prediction accuracy; [ 6,27] explore to
use social group associations to solve sparsity and enhance model
performance.
Although previous approaches have been proven to be effective
to some extent, they are still limited in the following aspects:
•Personalization is crucial for effective prediction, while pre-
viously the user preference is encoded by either (1) a unique,
learnable, and randomly initialized embedding that is di-
rectly used for prediction [ 13,21,33], or (2) hidden states
learned from the user’s trajectory within a deep learning
model [ 24,28]. The former aims at distinguishing between
users, which barely reflects each user’s preference. The lat-
ter then learns users’ preferences from scratch, falling short
in capturing the explicit user preference already entailed
from human trajectories. Specifically, users’ preference is
largely embedded in the activity sequences through the lens
of the places where they engage in activities. For example,
one could observe that a user is a fan of shopping and of-
ten travels to various shopping venues, while another user
is passionate about learning, and often goes to university
campuses and libraries. Personal preference could largely
inform the next location prediction, e.g., the next location for
a shopping fan would more likely be a shopping mall than
a university. This type of preference can be mined through
looking at users’ activities in an overall manner.
•The effect of temporal regularity is underrepresented. Where
a user is going can be greatly informed by when the user
is going to that place. For example, if a user’s next activity
is around noon, she is more likely to go to a place to have
lunch. Previously, the usefulness of arrival time was either
not explored or considered implicitly, taking arrival time as
an auxiliary task in a next location prediction framework
[10,36]. This method neglects the fact that predicting the
time for the next activity of a user is a much easier task
than predicting the next location. This can be ascribed to the
stronger regularity exhibited in people’s routine times, and
the much smaller search space in the temporal dimension
compared to the spatial dimension [ 15] – for example, there
are only 24 possible choices in a scenario regarding each
hour as a time slot. In this context, we argue that arrivaltime estimation should serve as a direct determinant (con-
text information) for next location prediction, rather than
modeling it as an auxiliary task with only indirect effect.
In this paper, to better utilize the rich context information from
the perspective of user preference and temporal regularity, we pro-
pose a Multi-Context aware next Location Prediction model (MCLP),
which mainly consists of three components: user preference mining,
arrival time estimating, and sequential pattern mining. In the first
component, we model user-location cooccurrences with a proba-
bilistic topic model to generate the user preference embedding. In
the second component, we develop an arrival time estimator to
construct a robust arrival time embedding based on the multi-head
attention mechanism, which provides context information for the
final prediction. In the third component, we capture the sequential
patterns from users’ activity location sequences to generate the
sequential embedding. We ultimately combine these contextual
embeddings and feed them into a fully connected layer to predict
the next activity location.
Our contributions are summarized as follows:
•We propose a new next location prediction model, which
predicts the next activity location by jointly modeling user
preference, temporal regularity, and sequential patterns. Our
method is among the pioneering studies that explicitly cap-
ture user preference and arrival time as context information
for location prediction.
•We employ a probabilistic topic model to learn user-topic
distributions as the prior knowledge of user preference. We
develop an arrival time estimator based on the multi-head
attention mechanism to generate the arrival time embedding
as the context for location prediction. Our approaches shed
new light on how to incorporate user preference and tempo-
ral regularity into mobility prediction methods to enhance
performance.
•We conduct extensive experiments to evaluate MCLP with
real-world mobility datasets. The results demonstrate that
MCLP exhibits significant performance gains over baseline
methods based on the paired t-test. Data and source code
are available at https://github.com/SUNSTARK/MCLP.
2 RELATED WORK
Our work is related to a broad spectrum of human mobility pre-
diction. We mainly introduce the recent next location prediction
methods and the impact of context information modeling.
2.1 Next Location Prediction
The next location prediction problem has been widely studied in
the last decade, mainly falling into two categories: traditional math-
ematical methods and DL methods. Traditional approaches mainly
utilize mathematical prediction models. For instance, methods in-
clude deriving recommendations from user-location matrix factor-
ization [ 17], constructing location transition probability matrices
using Markov Chains (MC) [ 12], or integrating both to develop per-
sonalized MCs [ 25]. Bayesian theory is also employed in forecasting
the next location [4].
Conversely, DL methods address the shortcomings of traditional
approaches in sequential transition modeling by enhancing their
2785Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference
and Temporal Regularity KDD ’24, August 25–29, 2024, Barcelona, Spain.
ability to recognize temporal and spatial dependencies. The foun-
dational frameworks encompass Convolutional Neural Networks
(CNNs) [ 5,41], RNNs [ 21,24,35], and Transformers [ 1,33,36].
For instance, CEM [ 5] predicts the next location by simulating
the relative order of locations using one-dimensional convolution.
Flashback [ 35] designs a special hidden state aggregation mecha-
nism for RNN models to predict the next location. GETNext [ 36]
enhances the Transformer by incorporating a global trajectory flow
map to model common sequential transition information.
2.2 Impact of Context Information Modeling
The selection of the next location by most individuals is not ar-
bitrary but influenced by contextual factors at a particular time.
The time geography theory [ 23] posits that human activities are
confined by both time and space. Hence, certain contexts act as
potential determinants in the selection of future visited locations
for users. In the domain of location prediction, context information
spans a wide range of users themselves and spatiotemporal details.
This includes user preference [ 24,32], location text [ 3,10], friend-
ship [7, 27], and more, all extensively explored in the literature.
Regarding user preference, existing approaches predominantly
represent them as individual embeddings primarily utilized for user
identification or delegate their acquisition to deep sequence mod-
els, which learn them from the ground up. For instance, LSTPM
[28] utilizes LSTM to learn the user’s long-term and short-term
preference from the location sequences to make the next location
prediction. Graph-Flashback [ 24] constructs a user-location graph
and then obtains user preference through location representation
updates. Literature [ 13] additionally initializes travel mode embed-
dings to characterize users. Regarding temporal regularity, existing
methods (e.g., BSDA [ 19]) directly integrate the actual visit time of
the next location as a context, although this may not be suitable
for all application scenarios as it involves using future information
in advance.
To summarize, recognizing and exploiting multiple contextual
information inherent in human mobility sequences, particularly
the interplay of user preference and temporal regularity, could
enhance next location prediction. In this context, on one hand, we
directly capture user preference from historical mobility data as
prior knowledge to generate the user preference embedding; on
the other hand, we estimate the next arrival time based on the
temporal regularity of human mobility and regard it as the context
information for next location prediction.
3 PRELIMINARY
In this section, we first introduce the preliminary concepts and
problem statement which are used in this paper, and then provide
the empirical data analysis.
3.1 Problem Statement
Definition 1 (Mobility Record). Mobility records are com-
monly acquired through electronic devices and stored as spatial-
temporal tuples. Each mobility record (𝑢,𝑙,𝑡)comprises a user 𝑢,
a location𝑙, and a timestamp 𝑡, denoting that user 𝑢visits location 𝑙
at time𝑡.Table 1: Dataset statistics
T
raffic Camera Data Mobile
Phone Data
#
Users 7,800 10,000
#
Activity Locations 2,418 20,607
#
Mobility Records 1,115,619 1,594,551
#
Days 61 75
/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a
/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000033/uni00000027/uni00000029/uni00000037/uni00000055/uni00000044/uni00000049/uni00000049/uni0000004c/uni00000046/uni00000003/uni00000026/uni00000044/uni00000050/uni00000048/uni00000055/uni00000044
/uni00000030/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000003/uni00000033/uni0000004b/uni00000052/uni00000051/uni00000048
(a) Merely Activity Locations
/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a
/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000033/uni00000027/uni00000029/uni00000037/uni00000055/uni00000044/uni00000049/uni00000049/uni0000004c/uni00000046/uni00000003/uni00000026/uni00000044/uni00000050/uni00000048/uni00000055/uni00000044
/uni00000030/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000003/uni00000033/uni0000004b/uni00000052/uni00000051/uni00000048 (b) Adding Temporal Context
Figure 2: Influence of temporal context on mobility entropy.
Definition 2 (Mobility Trajectory). Given all the mobility
records of𝑢𝑖, we sort them according to the timestamp and define
the mobility trajectory 𝑇𝑢𝑖as{(𝑙1,𝑡1),···,(𝑙𝑗,𝑡𝑗),(𝑙𝑗+1,𝑡𝑗+1),···},
where𝑡𝑗<𝑡𝑗+1.
Definition 3 (Activity Location). Given a user’s trajectory
𝑇𝑢𝑖, a location𝑙𝑗stands as a proxy of activity location if 𝑢𝑖stays at
location𝑙𝑗for a specified time threshold (e.g., 60 minutes), indicating
engagement in an activity (e.g., working, shopping, or residing).
Definition 4 (Activity Seqence). Given a user’s trajectory
𝑇𝑢𝑖, we identify all the activity locations within it and generate an
activity sequence 𝑆𝑢𝑖, which is a subset of 𝑇𝑢𝑖that merely focuses
on users’ activities instead of intermediate route information of the
trajectory.
Definition 5 (Next Location Prediction). Given a user’s ac-
tivity sequence 𝑆𝑢𝑖𝑛={(𝑙𝑛−𝑚+1,𝑡𝑛−𝑚+1),···,(𝑙𝑛,𝑡𝑛)}over a specific
time window from 𝑚to𝑛, we aim to forecast the next activity location
𝑙𝑛+1associated with user 𝑢𝑖within the upcoming time step.
3.2 Data and Motivation
The datasets used in this work are derived from urban traffic cam-
eras [38] (referred to as the Traffic Camera Dataset) and mobile
phone applications [ 34] (referred to as the Mobile Phone Dataset).
The former contains the vehicle’s mobility records (including its
location and the corresponding timestamps) between March 2021
and April 2021, detailing people’s daily trips, while the latter tracks
app users’ movements and corresponding timestamps, where move-
ments are aggregated into spatial representations of 500 meters ×
500 meters grid cells (200 ×200 grid total) to safeguard individual
privacy. The latitude and longitude information in both datasets
are not available to protect the privacy of users. To generate ac-
tivity sequences, we identify a location as an activity location if a
user stays at the location for over one hour and then connect these
activity locations into a sequence. Table 1 shows the statistics of
the after-preprocessed datasets.
We posit that the inclusion of the arrival time and temporal
regularity can enhance the location prediction performance. In
2786KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianao Sun et al.
𝒆𝑡𝑢𝑖𝒆𝑙𝑢𝑖Preference
embedding
𝑙𝑛+1𝒆𝑛𝑎𝑡
Arrival time
embeddingመ𝑙𝑛+1𝒆𝑝𝑟𝑒𝑢𝑖
𝑢𝑖Add
+𝒆𝑛𝑠𝑒𝑞𝒆𝑡
Sequential
embedding
Multi -Head 
AttentionFeed 
Forward
N×Time 
embedding
Location
embedding𝒆𝑙𝑙𝑘𝑡𝑘
×𝑛−𝑚+1User embedding
𝒆𝑡User -location 
cooccurrence
𝑊𝑪𝑢𝑖𝜃𝛼
𝛽MLP Layer
𝑪𝑢𝑖
Cross 
Entropy 
Loss 𝒆𝑙𝑢𝑖(𝑢1,𝑙1,𝑡1)
(𝑢1,𝑙2,𝑡2)
(𝑢𝑖,𝑙𝑘,𝑡𝑘)
……
Location sequenceHistorical data
User informationFC Residual Layer
Allocation
Arrival Time
EstimatorUser embedding 
(time)User Preference Mining
Arrival Time Estimating
Sequential Pattern Mining
Figure 3: The framework of MCLP.
this context, we have undertaken an analysis from the perspec-
tive of mobility entropy to ascertain whether it is effective to take
the arrival time as contextual information for location prediction.
Specifically, we first focus solely on the sequence of activity loca-
tions and compute the mobility entropy [39] as follows,
𝐸(𝑆𝑢𝑖)=−𝑁𝑢𝑖∑︁
𝑖=1𝑝(𝑙𝑖)log2𝑝(𝑙𝑖), (1)
where𝑆𝑢𝑖represents the activity location sequence of user 𝑢𝑖,𝑁𝑢𝑖
denotes the number of unique locations visited by 𝑢𝑖in𝑆𝑢𝑖, and
𝑝(𝑙𝑖)signifies the frequency of location 𝑙𝑖occurring in 𝑆𝑢𝑖.
Further, we introduce temporal regularity into the mobility en-
tropy through categorizing activity locations visited by users into
their respective timeslots, and then compute the mean mobility en-
tropy after accounting for temporal regularity using the following
formula,
𝐸′(𝑆𝑢𝑖)=−1
𝑁𝑡𝑁𝑡∑︁
𝑡=1𝑁𝑢𝑖
𝑡∑︁
𝑖=1𝑝(𝑙𝑖,𝑡)log2𝑝(𝑙𝑖,𝑡), (2)
where𝑁𝑡represents the number of timeslots, 𝑁𝑢𝑖
𝑡is the count of
unique locations visited by user 𝑢𝑖during timeslot 𝑡, and𝑝(𝑙𝑖,𝑡)
denotes the frequency of location 𝑙𝑖appearing during timeslot 𝑡.
The results of the mobility entropy for modeling merely activity
sequences and modeling both activity sequences and temporal
context are depicted in Figure 2. Intriguingly, when users’ activity
locations are classified into corresponding arrival timeslots, themobility entropy in both datasets demonstrates a reduction. This
reduction suggests that determining the arrival time for the next
activity locations can theoretically lead to lower entropy, ultimately
resulting in better prediction accuracy (higher predictability).
4 METHOD
Figure 3 illustrates the framework of our proposed MCLP, which
consists of three components responsible for (1) user preference
mining, (2) arrival time estimating, and (3) sequential pattern min-
ing. First, we create a user-location co-occurrence matrix using
historical activity sequences and utilize a topic model to extract
prior knowledge regarding user preference. Second, we develop
an Arrival Time Estimator to flexibly capture temporal transition
patterns, generating an aggregated embedding representing the
expected arrival time for the next activity location. Third, we use
a Transformer architecture with residual-like join operations to
mine movement patterns from activity sequences to obtain the se-
quential embedding. In the end, we concatenate multiple contextual
embeddings and model them using a fully connected residual layer
to predict the next activity location.
4.1 User Preference Mining
We encode users’ location preferences based on their historical ac-
tivity sequences, as such information has proven to be very effective
for predicting the user’s choice [ 32]. Instead of letting the model
directly learn itself, we chose to inject the user’s preference for
the activity locations as prior knowledge. Leveraging the success
2787Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference
and Temporal Regularity KDD ’24, August 25–29, 2024, Barcelona, Spain.
of probabilistic topic models in natural language processing, we
employ the Latent Dirichlet Allocation (LDA) model [ 2] for global
personalization.
Specifically, users are analogized to documents and activity lo-
cations to words within those documents. The LDA model is then
applied to generate mixed topics for each user based on their activ-
ity location multisets with each topic characterized by an activity
location distribution. The formulation to derive the prior activity
location preference for each user is
v𝑢𝑖=
𝑐1,···,𝑐𝑗,···,𝑐|P|
,
C𝑢𝑖=LDAh
v𝑢1,···,v𝑢|U|iT
,(3)
where v𝑢𝑖represents a bag-of-locations vector for each user, with
𝑐𝑗being a count variable of user 𝑢𝑖visiting to the activity location
𝑙𝑗∈P, andPis the set of locations. The matrix [v𝑢1,···,v𝑢|U|]T
can be seen as a user-location co-occurrence matrix, where Uis
the set of users. C𝑢𝑖∈R𝑁𝑜signifies the latent topic distribution for
user𝑢𝑖, where𝑁𝑜predefines the number of topics for LDA. Once
user preference has been encoded into C, we consider updating its
semantics further with an MLP layer,
e𝑢𝑖
𝑝𝑟𝑒=MLP
C𝑢𝑖
, (4)
where e𝑢𝑖
𝑝𝑟𝑒represents the user preference embedding. The function
MLP(·)refers to a small, adaptable MLP-based network, comprising
two linear layers with a residual connection and layer normaliza-
tion. These layers are interspersed with a ReLU activation function
and a dropout layer to promote non-linear learning and prevent
overfitting.
The user preference embeddings mainly capture prior knowl-
edge from the global perspective, where users with similar location
co-occurrence patterns tend to have similar embeddings. Mean-
while, we also need to profile users according to the local sequential
patterns. In this context, we additionally construct a unique user
embedding e𝑢𝑖
𝑙for each user 𝑢𝑖based on
e𝑢𝑖
𝑙=ℎ𝑙
𝑢(𝑢𝑖,W𝑙
𝑢), (5)
whereℎ𝑙𝑢(·,·)denotes the embedding operation and W𝑙𝑢denotes
the parameter matrix optimized during training.
4.2 Arrival Time Estimating
Previous research [ 40] has validated that the next visit time plays a
pivotal role in influencing daily behavioral patterns and locational
choices. Intuitively, knowing a user’s probable arrival time of the
next location and considering it as the context to predict the next
location could improve prediction performance. Thus we aim to
estimate a user’s next arrival time based on its activity sequence
𝑆𝑢𝑖𝑛and build the arrival time embedding as the context feature.
To estimate the next arrival time, one simple idea is to construct
the time transition matrix B∈R𝑁𝑡×𝑁𝑡based on historical user
activity sequences, where the entry B𝑖𝑗denotes the transition fre-
quency from time 𝑡𝑖to𝑡𝑗and𝑁𝑡denotes the number of timeslots,
and take the maximum element in B𝑛as the predicted arrival time
if a user has arrived at location 𝑙𝑛at time𝑡𝑛. Considering that peo-
ple’s activities often span broader time frames, such as commuting
MatMulScale MaskSoftmaxMatMul
𝑸′𝑲′𝑽′𝑨𝒕𝒕𝒏
𝑸
AllocationLinear Linear LinearScaled Attention
Dot-ProductFC Layer
ConcatenationArrival time
embedding
𝑲 𝑽
All time 
embeddings𝒆𝑡𝑢𝑖𝒆𝑛𝑡
User embedding 
(time)Current time 
embedding 𝒆𝑛𝑎𝑡
𝒆𝑡Figure 4: Structure of the arrival time estimator.
between 7 a.m. and 9 a.m. in the morning, we argue that relying on
one specific timeslot as the context becomes impractical when the
next arrival time cannot be precisely determined. As a result, we
anticipate that the context vector for arrival time should aggregate
multiple representations of timeslots to accommodate this variabil-
ity. In this context, given current time 𝑡𝑛, we acquire its transition
frequency vector B𝑛to all candidate timeslots and compute the
arrival time embedding as
e𝑎𝑡
𝑛=B𝑛·(e𝑡
1,···,e𝑡
𝑁𝑡)T,
e𝑡
𝑘=ℎ𝑡(𝑡𝑘,W𝑡),(6)
where e𝑡
𝑘∈R𝑑is the raw representation of the corresponding
𝑘-th timeslot. ℎ𝑡(·,·)denotes the embedding operation, W𝑡is the
parameter matrix, and 𝑑denotes the embedding dimension. Note
that we segment time 𝑡into 24 timeslots for granularity.
Unfortunately, such a method generates the static time transi-
tion matrix Bwithout considering personalized user preference,
which results in a fixed time context for the current time, proving
inadequate in adapting to the dynamic shifts in user behavior pat-
terns. Therefore, we intricately capture user preference for travel
time and develop an Arrival Time Estimator to construct a robust
arrival time embedding based on the multi-head attention mecha-
nism. Firstly, we formulate a user-time embedding e𝑢𝑖
𝑡to identify
the personalized time preference,
e𝑢𝑖
𝑡=ℎ𝑡
𝑢(𝑢𝑖,W𝑡
𝑢), (7)
whereℎ𝑡𝑢(·,·)denotes the embedding operation and W𝑡𝑢signifies
the parameter matrix.
We commence by considering the user-time embedding e𝑢𝑖
𝑡, the
current time embedding e𝑡𝑛, and all-time embeddings e𝑡, as depicted
in Figure 4. An allocation layer is utilized to produce the necessary
2788KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianao Sun et al.
query, key, and value vectors:
Q=e𝑢𝑖
𝑡⊕e𝑡
𝑛,
K=V=e𝑡
𝑘, 𝑘∈{1,2,···,𝑁𝑡},(8)
where⊕denotes the concatenation operation. Subsequently, the
multi-head self-attention operation processes these inputs, linearly
projecting them into subspaces through distinct matrices, and then
applies𝛽attention functions in parallel to produce output repre-
sentations. Eventually, the arrival time embedding e𝑎𝑡𝑛is obtained
by concatenating these results through a unified projection,
Q′
𝑖=QW𝑄
𝑖,K′
𝑖=KW𝐾
𝑖,V′
𝑖=VW𝑉
𝑖,
Attn𝑖=softmax 
Q′
𝑖K′
iT
√𝐷𝑖!
V′
𝑖,
e𝑎𝑡
𝑛=FC
Attn 1⊕ ···⊕ Attn𝛽
,(9)
where W𝑄
𝑖,W𝐾
𝑖, and W𝑉
𝑖represent the learnable linear projection
matrices tasked with mapping the input into a corresponding sub-
space.𝛽is the number of attention heads and 𝐷𝑖represents the
dimension of the 𝑖-th head. FC(·)is a fully connected layer to unify
every attention head output.
4.3 Sequential Pattern Mining
We then capture the sequential patterns from users’ activity location
sequences. As the Transformer architecture [ 13,29] has shown the
powerful capacity to encode sequential dependencies from time-
series data, we employ it as the backbone network to model users’
location sequences to generate the sequential embeddings.
Given a location sequence (𝑙𝑛−𝑚+1,𝑡𝑛−𝑚+1),···,(𝑙𝑛,𝑡𝑛)of𝑢𝑖,
we randomly initialize the locations and time as raw representations
e𝑙∈R𝑑ande𝑡∈R𝑑, where𝑑denotes the embedding dimension.
Formally, we compute the representation for a location 𝑙𝑘as
e𝑙
𝑘=ℎ𝑙(𝑙𝑘,W𝑙), (10)
whereℎ𝑙(·,·)denotes the embedding operation and W𝑙is the pa-
rameter matrix. e𝑡is the same as that in Equation (6).
Taking the location and time embeddings as input, we then use
the stacked Transformer layers to capture the sequential patterns.
Each layer contains two sub-layers: a multi-head self-attention layer
and a point-wise feed-forward network. Both layers incorporate
residual connections and normalization. The Transformer layer
outputs the sequential context value h𝑜𝑢𝑡𝑛dynamically, relying on
attention scores that measure the associations between query-key
pairs derived from the initial representation X. The entire process
can be summarized as
X𝑘=e𝑙
𝑘+e𝑡
𝑘+PE𝑘,𝑘∈{𝑛−𝑚+1,···,𝑛},
h𝑛=LayerNorm(X+MHSA(X)),
h𝑜𝑢𝑡
𝑛=LayerNorm(h𝑛+FFN(h𝑛)),(11)
where the embedding Xdenotes the input of the Transformer en-
coder along with vanilla Positional Encoding (PE). h𝑛denotes the
hidden states, and h𝑜𝑢𝑡𝑛represents the output. LayerNorm(·)signi-
fies the layer normalization for facilitating learning; MHSA(·)con-
stitutes a multi-head attention operation integrated with a future-
masking mechanism, preventing the Transformer encoder fromaccessing future information; FFN(·)denotes the feed-forward net-
work consisting of two linear projections sandwiched by a Gaussian
Error Linear Unit (GELU) activation function.
It is noteworthy that various studies emphasize the necessity of
the current location and time information for predicting the next
location [ 30]. Therefore, we utilize an addition operation akin to the
residual connection approach to aid the model in recalling current
spatial-temporal information. The final sequential embedding is
presented as
e𝑠𝑒𝑞
𝑛=h𝑜𝑢𝑡
𝑛+e𝑙
𝑛+e𝑡
𝑛. (12)
4.4 Next Location Prediction
Once the four contextual embeddings are generated, they will be
concatenated into a fully connected layer. This layer serves to
compute the user’s transition probability for each candidate activity
location utilizing the softmax function,
𝑃(ˆ𝑙𝑛+1)=softmax
FC
e𝑢𝑖
𝑝𝑟𝑒⊕e𝑢𝑖
𝑙⊕e𝑎𝑡
𝑛⊕e𝑠𝑒𝑞
𝑛
, (13)
where FC(·)is a fully connected layer that incorporates residual con-
nections, structured with two linear layers and a ReLU activation
function to process and transform the concatenated embeddings.
When training the model, we treat the next location prediction
as a classification task across the entire location set P. The loss
function for location prediction is a multi-class cross-entropy loss,
L𝑆𝑢𝑖
𝑛=−|P|∑︁
𝑗=1𝑃(𝑙𝑛+1)𝑗log𝑃(ˆ𝑙𝑛+1)𝑗, (14)
where𝑃(𝑙𝑛+1)𝑗corresponds to the ground truth in one-hot encod-
ing. Specifically, 𝑃(𝑙𝑛+1)𝑗=1if the next activity location is the
𝑗-th location. Meanwhile, 𝑃(ˆ𝑙𝑛+1)𝑗denotes the model’s predicted
probability for user activity at location 𝑗.
5 EXPERIMENTS
5.1 Experiment Setup
5.1.1 Datasets. We adopt the Traffic Camera Dataset [ 38] and the
Mobile Phone Dataset [ 34] (cf. Section 3.2) to train our model. We
conducte a common preprocessing step, initially filtering users with
location sequences less than a length of 100. Subsequently, we divide
these sequences into subsequences, each with a fixed length of 20,
following established practices in previous studies [ 19,35]. For
both datasets, we partition each user’s subsequences into training,
validating, and testing sets, with the first 70% dedicated to training,
the next 10% for validating, and the remaining 20% allocated for
testing.
5.1.2 Baselines. We compare the performance of the proposed
model with classical methods and deep learning (DL) models.
•Markov Chain [12]. A classical model introduces the Mobility
Markov Chain (MMC), where states represent locations and tran-
sitions depict moves between locations. We utilize a first-order
Markov (1-MMC) due to observations that increasing the order
does not enhance performance.
•DeepMove [11]. An attentional recurrent network is designed
for mobility prediction using current and historical trajectories.
2789Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference
and Temporal Regularity KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 2: The performance of all the methods for next location prediction, where the variance is in parentheses and the best and
second performing results are represented in bold and underlined , respectively.
Metho
dT
raffic Camera Data Mobile Phone Data
A
cc@1 Acc@3 Acc@5 Acc@10 MRR Acc@1 Acc@3 Acc@5 Acc@10 MRR
1-MMC 23.61
39.50 44.43 48.29 32.42 29.48 45.68 49.54 52.46 38.21
DeepMove 35.89
(0.07) 51.60 (0.07) 57.72 (0.07) 65.15 (0.04) 46.08 (0.05) 37.38 (0.04) 56.84 (0.03) 63.10 (0.02) 69.88 (0.03) 49.11 (0.02)
Flashback 34.89
(0.06) 54.92 (0.09) 62.88 (0.04) 71.00 (0.07) 47.33 (0.04) 37.39 (0.03) 59.64 (0.05) 65.96 (0.04) 72.01 (0.06)
50.22 (0.02)
Graph-Flashback 35.69
(0.03) 55.64 (0.08) 63.73 (0.05) 72.25 (0.05)
48.18 (0.03) 37.61 (0.03) 59.62 (0.02) 65.86 (0.02) 71.88 (0.03) 50.31 (0.02)
STAN 29.92
(0.10) 49.70 (0.12) 57.81 (0.08) 66.24 (0.10) 42.39 (0.08) 36.40 (0.09) 56.43 (0.10) 62.15 (0.12) 67.77 (0.12) 48.06 (0.09)
GETNext 35.53
(0.11) 54.26 (0.14) 61.27 (0.21) 68.64 (0.12) 47.15 (0.11) 36.93 (0.09) 59.44 (0.11) 65.75 (0.40) 71.80 (0.64) 49.89 (0.09)
SNPM 36.43
(0.05) 56.18 (0.02)
63.74 (0.04)
71.45 (0.02) 48.58 (0.01)
37.99 (0.07) 59.89 (0.02) 66.03 (0.04) 71.92 (0.01) 50.60 (0.04)
T
rans-Aux 36.69
(0.12) 53.97 (0.13) 60.38 (0.11) 67.50 (0.07) 47.52 (0.09) 38.52 (0.31)
56.66 (0.11) 61.76 (0.18) 67.28 (0.18) 49.24 (0.12)
CSLSL 36.96 (0.12)
55.02 (0.13) 61.67 (0.11) 68.79 (0.07) 48.17 (0.09) 37.86 (0.16) 60.22 (0.07)
66.52 (0.02)
71.94 (0.01) 50.51 (0.11)
MCLP
(LSTM) 39.90
(0.06) 58.32 (0.07) 65.14 (0.07) 72.43 (0.07) 51.28 (0.05) 39.69 (0.07) 61.02 (0.07) 67.12 (0.07) 73.11 (0.09) 52.05 (0.06)
MCLP 40.11 (0.05) 58.44 (0.05) 65.30 (0.04) 72.58 (0.05) 51.46 (0.02) 39.79 (0.05) 61.02 (0.06) 67.13 (0.06)
73.10 (0.09) 52.11 (0.04)
Impr
ovement (%) 8.52
4.02 2.45 0.46 5.93 3.30 1.33 0.92 1.53 2.98
•Flashback [35]. Based on an RNN, it incorporates the concept
of flashback, utilizing sparse semantic trajectories to predict the
next location by seeking similar trajectories concerning temporal
characteristics.
•Graph-Flashback [24]. This approach utilizes Graph Convolu-
tional Networks (GCNs) on a learned location transition graph
to capture their representations, which are then input into RNN-
based models for location prediction.
•STAN [22]. It proposes a bi-attention architecture for person-
alized item frequency. Its first layer aggregates spatiotemporal
information, while the second layer matches the target to all
check-ins.
•GETNext [36]. It constructs a trajectory flow map to capture
common visiting-order transition information and encode it into
the location embedding. In our setting, due to the lack of category
information, we focus solely on predicting the next location and
visit time.
•SNPM [37]. This model uses the idea of RotatE to describe users’
check-in data to construct a transition graph and obtain the
vector representation of locations. It can dynamically search
neighbors to aggregate useful contextual information to make a
recommendation for the next location.
•Trans-Aux [13]. This model employs a transformer decoder-
based neural network to predict an individual’s next location by
utilizing historical location data, time, and travel mode. Originally,
the travel mode was treated as an auxiliary task. In our adaptation,
we modify this aspect to focus on predicting the next arrival time.
•CSLSL [16]. It introduces a causal structure along with consis-
tency constraints based on multi-task learning to explicitly model
the “when→what→where” decision logic. By fully exploiting the
temporal and categorized information, it enhances the accuracy
of location predictions.
•MCLP (LSTM). It is a variant of our MCLP model, where the
Transformer encoder is substituted with a Long Short-Term Mem-
ory (LSTM) network for extracting sequential information. All
other components of the model remain the same as MCLP.
Note that for models requiring latitude and longitude informa-
tion for spatial distance calculations (e.g., Flashback and STAN),these operations have been omitted in our study due to the unavail-
ability of such data.
5.1.3 Evaluation Metrics. We employ the following metrics to as-
sess the performance of various methods.
1)Accuracy (Acc): It gauges the overall correctness of the
model’s next location prediction. We utilize 𝐴𝑐𝑐@𝐾metric, where
𝐾∈{1,3,5,10}. A score of 1 is assigned if the top 𝐾predicted
locations encompass the ground truth location, and 0 otherwise.
2)Mean reciprocal rank (MRR): It calculates the average
reciprocal of the rank at which the correct location appears among
the predicted candidate locations, MRR =(Í𝑁𝑡𝑒𝑠𝑡
𝑖=11/rank𝑖)/𝑁𝑡𝑒𝑠𝑡,
where𝑁𝑡𝑒𝑠𝑡represents the number of test samples, and rank𝑖is the
ranking of the ground truth of the 𝑖-th test sample in the prediction
vector𝑃(ˆ𝑙𝑛+1).
5.1.4 Model Settings. We perform model optimization using the
Adam optimizer, initialized with a learning rate of 1𝑒−2and an
L2 regularization penalty of 1𝑒−6. For Transformer, we stack two
encoder layers with a 0.1dropout rate, and use four attention heads
in the multi-headed attention module. For MCLP, we set the dimen-
sions of representation vectors to 16 for both datasets and run the
model 50 epochs with batch size 256. We choose the optimal model
parameters via a grid search with an adaptive step size strategy.
5.2 Performance Comparison with Baselines
Table 2 shows the location prediction performance of different meth-
ods. We train each DL model 5 times with different random seeds
and report the mean and the standard deviation of the respective
performance indicators. From Table 2 we observe that
•The 1-MMC model exhibits comparatively weaker performance
when contrasted with DL models across both datasets. This obser-
vation underscores the limitation of the Markov property in fully
capturing the long-term spatiotemporal dependencies within
activity sequences.
•DeepMove, Flashback, Graph-Flashback and SNPM achieve sat-
isfactory and similar performance, as they all could capture the
complex sequential patterns with the RNN-based network.
2790KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianao Sun et al.
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000016/uni00000016/uni00000016/uni00000018/uni00000016/uni0000001a/uni00000016/uni0000001c/uni00000017/uni00000014/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000024/uni00000046/uni00000046/uni00000023/uni00000018
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000019/uni00000018/uni00000019/uni0000001a/uni00000019/uni0000001c/uni0000001a/uni00000014/uni0000001a/uni00000016/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014/uni00000013
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000017/uni00000017/uni00000017/uni00000019/uni00000017/uni0000001b/uni00000018/uni00000013/uni00000018/uni00000015/uni00000030/uni00000035/uni00000035
Figure 5: Ablation study on the Traffic Camera Dataset.
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000016/uni00000016/uni00000016/uni00000018/uni00000016/uni0000001a/uni00000016/uni0000001c/uni00000017/uni00000014/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000019/uni00000019/uni00000019/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000023/uni00000018
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000019/uni00000019/uni00000019/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni00000015/uni0000001a/uni00000017/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014/uni00000013
/uni00000025/uni00000044/uni00000056/uni00000048+/uni00000033/uni00000055/uni00000048+/uni00000024/uni00000057/uni00000030/uni00000026/uni0000002f/uni00000033/uni00000017/uni00000018/uni00000017/uni0000001a/uni00000017/uni0000001c/uni00000018/uni00000014/uni00000018/uni00000016/uni00000030/uni00000035/uni00000035
Figure 6: Ablation study on the Mobile Phone Dataset.
/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014 /uni00000030/uni00000035/uni00000035/uni00000016/uni00000019/uni00000016/uni0000001c/uni00000017/uni00000015/uni00000017/uni00000018/uni00000017/uni0000001b/uni00000018/uni00000014/uni00000036/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000046
/uni00000030/uni00000026/uni0000002f/uni00000033
/uni00000037/uni00000055/uni00000058/uni00000048
(a) Traffic Camera Dataset
/uni00000024/uni00000046/uni00000046/uni00000023/uni00000014 /uni00000030/uni00000035/uni00000035/uni00000016/uni00000019/uni00000016/uni0000001c/uni00000017/uni00000015/uni00000017/uni00000018/uni00000017/uni0000001b/uni00000018/uni00000014/uni00000036/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000046
/uni00000030/uni00000026/uni0000002f/uni00000033
/uni00000037/uni00000055/uni00000058/uni00000048 (b) Mobile Phone Dataset
Figure 7: Comparison of different arrival time estimators.
•STAN exhibits relatively poorer performance compared with
other DL models. Notably, the model’s performance appears to be
influenced by user selection during training, which may account
for its suboptimal results in our experiments.
•GETNext, Trans-Aux and CSLSL do not show obvious improve-
ment compared with these RNN-based methods (e.g., DeepMove,
Flashback, Graph-Flashback and SNPM), although it takes the
next arrival time prediction as an auxiliary task. This indicates
that implicitly modeling the arrival time of the next visited loca-
tion may be not effective enough for improving location predic-
tion.
•The proposed MCLP and MCLP (LSTM) obtain similar perfor-
mance and both perform better than these baselines. We know
that 1) LSTM and Transformer both could well capture the se-
quential patterns from mobility data and 2) considering the user
preference embedding and the arrival time embedding as the con-
text could largely improve the prediction performance. Compared
with the best results of the baseline methods, MCLP achieves an
improvement of 5.93% on the Traffic Camera Dataset and 2.98%
on the Mobile Phone Dataset in terms of MRR. Furthermore, the
superiority paired t-test results show that the improvement of
MCLP over these baselines is of practical significance with 𝑝
value <0.01.5.3 Ablation Study and Parameter Analysis
5.3.1 Study of Different Variants. Our MCLP is built upon three key
components: (1) user preference mining, (2) arrival time estimating,
and (3) sequential pattern mining. To assess the contribution of each
component to the overall performance, we carry out the ablation
study. Specifically, we conduct three variants: 1) the Base model
containing the sequential pattern mining component and the user
embedding e𝑢𝑖
𝑙, 2) the+𝑃𝑟𝑒model which adds the user preference
mining component to the Base model, and 3) the +𝐴𝑡model which
adds the arrival time estimating component to the Base model. The
results of these models, conducted on both datasets, are illustrated
in Figures 5 and 6.
Compared with the Base model, the +𝑃𝑟𝑒model performs better,
as it efficiently distills the user’s historical mobility data, accurately
capturing users’ personalized transition trends and particular incli-
nations for specific activity locations. Additionally, adding the ar-
rival time estimating component improves the model performance,
as it encapsulates a probabilistic assessment of the user’s potential
next arrival time, serving as the context to predict the next locations.
Finally, incorporating the user preference mining and the arrival
time estimating components into the Base model yields the best
performance, validating the effectiveness and essentials of learning
user preference and temporal regularity.
5.3.2 Study of Different Arrival Time Estimators. We also investi-
gate the effect of different arrival time estimators on the prediction
performance. As introduced in Section 4.2, we first present a naive
arrival time estimator, which constructs the time transition matrix
and generates the arrival time embedding based on Equation (7).
The naive method is named Static, as the arrival time embedding is
fixed with the same current time, without considering individual
dynamic travel habits. Meanwhile, we include a method named
True which directly uses the actual arrival time to build the time
embedding, serving as the benchmark of model performance when
considering the arrival time in the prediction model.
Figure 7 illustrates the performance of MCLP and the two al-
ternative approaches. MCLP outperforms the Static method, as it
2791Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference
and Temporal Regularity KDD ’24, August 25–29, 2024, Barcelona, Spain.
/uni00000014/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000017/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni00000018/uni00000018/uni00000013/uni00000019/uni00000013/uni00000013/uni00000019/uni00000018/uni00000013/uni0000001a/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001c/uni00000013/uni00000013
/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000052/uni00000053/uni0000004c/uni00000046/uni00000056/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000018/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000018/uni00000018/uni00000015/uni00000011/uni00000013/uni00000030/uni00000035/uni00000035
(a) Traffic Camera Dataset
/uni00000014/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000017/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni00000018/uni00000018/uni00000013/uni00000019/uni00000013/uni00000013/uni00000019/uni00000018/uni00000013/uni0000001a/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni0000001b/uni00000013/uni00000013/uni0000001b/uni00000018/uni00000013/uni0000001c/uni00000013/uni00000013
/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000052/uni00000053/uni0000004c/uni00000046/uni00000056/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000018/uni00000018/uni00000015/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000016/uni00000011/uni00000013/uni00000030/uni00000035/uni00000035
(b) Mobile Phone Dataset
Figure 8: Effect of topic numbers.
captures user preference for travel time and employs the multi-
head attention mechanism to generate the arrival time embedding.
Furthermore, we observe that there is a gap between MCLP and the
True method, indicating that it remains an opportunity to enhance
the prediction accuracy by refining the arrival time embedding
generation.
5.3.3 Study of Parameter Sensitivity. We explore the influence of
varying the number of latent topics ( 𝑁𝑜) used in LDA for learning
user-topic distributions. As depicted in Figure 8, the model perfor-
mance on both datasets first improves when we increase 𝑁𝑜from
10 to 400 and then stabilizes when we further increase it. Conse-
quently, we select 𝑁𝑜=400for the Traffic Camera Dataset and
𝑁𝑜=450for the Mobile Phone Dataset.
6 CONCLUSION
In this paper, we present a multi-context aware location prediction
model named MCLP based on user preference and temporal regu-
larity learning to predict a user’s next activity location. The MCLP
mainly consists of three components: a user preference mining com-
ponent to condense the prior knowledge from the user-location
cooccurrence matrix into the user preference embeddings; an ar-
rival time estimating component to capture the users’ temporal
transition regularity to generate arrival time embeddings; and a
sequential pattern mining component to encode the mobility pat-
terns into the sequential embeddings based on the Transformer
encoder. We integrate these context embeddings to predict the next
location with a fully connected residual layer. We conduct compre-
hensive experiments with two real mobility datasets to validate the
effectiveness of the proposed MCLP. Experimental results validate
that incorporating user preference and temporal regularity into the
prediction model could significantly improve the performance of
the next location prediction.ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant No. 61906107, 62202270, and
42101421, the Young Scholars Program of Shandong University.
W.H. was supported by the Knut and Alice Wallenberg Foundation.
REFERENCES
[1]Zain Ul Abideen, Heli Sun, Zhou Yang, Rana Zeeshan Ahmad, Adnan Iftekhar, and
Amir Ali. 2020. Deep Wide Spatial-temporal based Transformer Networks Mod-
eling for the Next Destination According to the Taxi Driver Behavior Prediction.
Applied Sciences 11, 1 (2020), 17.
[2]David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Alloca-
tion. Journal of Machine Learning Research 3, Jan (2003), 993–1022.
[3]Buru Chang, Yonggyu Park, Donghyeon Park, Seongsoon Kim, and Jaewoo
Kang. 2018. Content-aware Hierarchical Point-of-Interest Embedding Model for
Successive POI Recommendation. In Proceedings of the 27th International Joint
Conference on Artificial Intelligence. 3301–3307.
[4]Meng Chen, Qingjie Liu, Weiming Huang, Teng Zhang, Yixuan Zuo, and Xiaohui
Yu. 2021. Origin-Aware Location Prediction based on Historical Vehicle Tra-
jectories. ACM Transactions on Intelligent Systems and Technology 13, 1 (2021),
1–18.
[5]Meng Chen, Yixuan Zuo, Xiaoyi Jia, Yang Liu, Xiaohui Yu, and Kai Zheng. 2020.
CEM: A Convolutional Embedding Model for Predicting Next Locations. ACM
Transactions on Intelligent Systems and Technology 22, 6 (2020), 3349–3358.
[6]Yile Chen, Cheng Long, Gao Cong, and Chenliang Li. 2020. Context-aware
Deep Model for Joint Mobility and Time Prediction. In Proceedings of the 13th
International Conference on Web Search and Data Mining. 106–114.
[7]Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011. Friendship and Mobility:
User Movement in Location-based Social Networks. In Proceedings of the 17th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1082–1090.
[8]Xiaoliang Fan, Lei Guo, Ning Han, Yujie Wang, Jia Shi, and Yongna Yuan. 2018.
A Deep Learning Approach for Next Location Prediction. In Proceedings of the
IEEE 22nd International Conference on Computer Supported Cooperative Work in
Design. 69–74.
[9]Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. 2021. Spatial-
temporal Graph ODE Networks for Traffic Flow Forecasting. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 364–373.
[10] Jie Feng, Yong Li, Zeyu Yang, Qiang Qiu, and Depeng Jin. 2020. Predicting
Human Mobility with Semantic Motivation via Multi-task Attentional Recurrent
Networks. IEEE Transactions on Knowledge and Data Engineering 34, 5 (2020),
2360–2374.
[11] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng
Jin. 2018. DeepMove: Predicting Human Mobility with Attentional Recurrent
Networks. In Proceedings of the 2018 World Wide Web Conference. 1459–1468.
[12] Sébastien Gambs, Marc-Olivier Killijian, and Miguel Núñez del Prado Cortez.
2012. Next Place Prediction Using Mobility Markov Chains. In Proceedings of the
1st Workshop on Measurement, Privacy, and Mobility. 1–6.
[13] Ye Hong, Henry Martin, and Martin Raubal. 2022. How Do You Go Where?
Improving Next Location Prediction by Learning Travel Mode Information Using
Transformers. In Proceedings of the 30th International Conference on Advances in
Geographic Information Systems. 1–10.
[14] Jizhou Huang, Haifeng Wang, Miao Fan, An Zhuo, Yibo Sun, and Ying Li. 2020.
Understanding the Impact of the COVID-19 Pandemic on Transportation-related
Behaviors with Human Mobility Data. In Proceedings of the 26th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3443–3450.
[15] Wei Huang, Songnian Li, Xintao Liu, and Yifang Ban. 2015. Predicting Human
Mobility with Activity Changes. International Journal of Geographical Information
Science 29, 9 (2015), 1569–1587.
[16] Zongyuan Huang, Shengyuan Xu, Menghan Wang, Hansi Wu, Yanyan Xu, and
Yaohui Jin. 2024. Human Mobility Prediction with Causal and Spatial-constrained
Multi-task Network. EPJ Data Science 13, 1 (2024), 22.
[17] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech-
niques for Recommender Systems. Computer 42, 8 (2009), 30–37.
[18] Xiaoliang Lei, Hao Mei, Bin Shi, and Hua Wei. 2022. Modeling Network-level
Traffic Flow Transitions on Sparse Data. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 835–845.
[19] Xixi Li, Ruimin Hu, Zheng Wang, and Toshihiko Yamasaki. 2021. Location
Predicts You: Location Prediction via Bi-direction Speculation and Dual-level
Association. In Proceedings of the 30th International Joint Conference on Artificial
Intelligence. 529–536.
[20] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geography-
aware Sequential Location Recommendation. In Proceedings of the 26th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 2009–2019.
[21] Nicholas Lim, Bryan Hooi, See-Kiong Ng, Yong Liang Goh, Renrong Weng, and
Rui Tan. 2022. Hierarchical Multi-task Graph Recurrent Network for Next POI
2792KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianao Sun et al.
Recommendation. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 1133–1143.
[22] Yingtao Luo, Qiang Liu, and Zhaocheng Liu. 2021. STAN: Spatio-temporal
Attention Network for Next Location Recommendation. In Proceedings of the
Web Conference 2021. 2177–2185.
[23] Harvey J Miller. 2005. A Measurement Theory for Time Geography. Geographical
analysis 37, 1 (2005), 17–45.
[24] Xuan Rao, Lisi Chen, Yong Liu, Shuo Shang, Bin Yao, and Peng Han. 2022. Graph-
Flashback Network for Next Location Recommendation. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1463–1471.
[25] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-
izing Personalized Markov Chains for Next-Basket Recommendation. In Proceed-
ings of the 19th International Conference on World Wide Web. 811–820.
[26] Amray Schwabe, Joel Persson, and Stefan Feuerriegel. 2021. Predicting COVID-19
Spread from Large-scale Mobility Data. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3531–3539.
[27] Young-Duk Seo, Young-Gab Kim, Euijong Lee, and Doo-Kwon Baik. 2017. Per-
sonalized Recommender System based on Friendship Strength in Social Network
Services. Expert Systems with Applications 69 (2017), 135–148.
[28] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and
Hongzhi Yin. 2020. Where to Go Next: Modeling Long-and Short-term User
Preferences for Point-of-Interest Recommendation. In Proceedings of the 34th
AAAI Conference on Artificial Intelligence. 214–221.
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30, 6000–6010.
[30] Senzhang Wang, Jiannong Cao, and S Yu Philip. 2020. Deep Learning for Spatio-
temporal Data Mining: A Survey. IEEE Transactions on Knowledge and Data
Engineering 34, 8 (2020), 3681–3700.
[31] Zhaobo Wang, Yanmin Zhu, Qiaomei Zhang, Haobing Liu, Chunyang Wang,
and Tong Liu. 2022. Graph-enhanced Spatial-temporal Network for Next POI
Recommendation. ACM Transactions on Knowledge Discovery from Data 16, 6
(2022), 1–21.[32] Yuxia Wu, Ke Li, Guoshuai Zhao, and Xueming Qian. 2020. Personalized Long-and
Short-term Preference Learning for Next POI Recommendation. IEEE Transactions
on Knowledge and Data Engineering 34, 4 (2020), 1944–1957.
[33] Hao Xue, Flora Salim, Yongli Ren, and Nuria Oliver. 2021. MobTCast: Leveraging
Auxiliary Trajectory Forecasting for Human Mobility Prediction. In Advances in
Neural Information Processing Systems 34, 30380–30391.
[34] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki,
Esteban Moro, and Alex Pentland. 2023. Metropolitan Scale and Longitudinal
Dataset of Anonymized Human Mobility Trajectories. arXiv:2307.03401
[35] Dingqi Yang, Benjamin Fankhauser, Paolo Rosso, and Philippe Cudre-Mauroux.
2020. Location Prediction over Sparse User Mobility Traces Using RNNs: Flash-
back in Hidden States!. In Proceedings of the 29th International Joint Conference
on Artificial Intelligence. 2184–2190.
[36] Song Yang, Jiamou Liu, and Kaiqi Zhao. 2022. GETNext: Trajectory Flow Map
Enhanced Transformer for Next POI Recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 1144–1153.
[37] Feiyu Yin, Yong Liu, Zhiqi Shen, Lisi Chen, Shuo Shang, and Peng Han. 2023.
Next POI Recommendation with Dynamic Graph and Explicit Dependency. In
Proceedings of the 27th AAAI Conference on Artificial Intelligence. 4827–4834.
[38] Fudan Yu, Huan Yan, Rui Chen, Guozhen Zhang, Yu Liu, Meng Chen, and Yong Li.
2023. City-Scale Vehicle Trajectory Data from Traffic Camera Videos. Scientific
Data 10, 1 (2023), 711.
[39] Chao Zhang, Kai Zhao, and Meng Chen. 2022. Beyond the Limits of Predictability
in Human Mobility Prediction: Context-transition Predictability. IEEE Transac-
tions on Knowledge and Data Engineering 35, 5 (2022), 4514–4526.
[40] Jia-Dong Zhang and Chi-Yin Chow. 2015. Ticrec: A Probabilistic Framework to
Utilize Temporal Influence Correlations for Time-aware Location Recommenda-
tions. IEEE Transactions on Services Computing 9, 4 (2015), 633–646.
[41] Ting Zhong, Shengming Zhang, Fan Zhou, Kunpeng Zhang, Goce Trajcevski, and
Jin Wu. 2020. Hybrid Graph Convolutional Networks with Multi-Head Attention
for Location Recommendation. World Wide Web 23 (2020), 3125–3151.
2793