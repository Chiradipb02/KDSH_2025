Certified Robustness on Visual Graph Matching via Searching
Optimal Smoothing Range
Huaqing Shao
Department of CSE and MoE Key Lab
of AI, Shanghai Jiao Tong University
Shanghai, China
sdytshq@sjtu.edu.cnLanjun Wang‚àó
SNMC, Tianjin University
Tianjin, China
wanglanjun@tju.edu.cnYongwei Wang
SIAS and College of Computer
Science, Zhejiang University
Hangzhou, China
yongwei.wang@zju.edu.cn
Qibing Ren
Department of CSE, Shanghai Jiao
Tong University
Shanghai, China
renqibing@sjtu.edu.cnJunchi Yan‚àó
School of AI and Department of CSE,
Shanghai Jiao Tong University
Shanghai, China
yanjunchi@sjtu.edu.cn
ABSTRACT
Deep visual graph matching (GM) is a challenging combinatorial
task that involves finding a permutation matrix that indicates the
correspondence between keypoints from a pair of images. Like
many learning systems, empirical studies have shown that visual
GM is susceptible to adversarial attacks, with reliability issues in
downstream applications. To the best of our knowledge, certifying
robustness for deep visual GM remains an open challenge with
two main difficulties: how to handle the paired inputs together
with the heavily non-linear permutation output space (especially
at large scale), and how to balance the trade-off between certified
robustness and matching performance.
Inspired by the randomized smoothing (RS) technique, we pro-
pose the Certified Robustness based on the Optimal Smoothing
Range Search (CR-OSRS) technique to fulfill the robustness guaran-
tee for deep visual GM. First, unlike conventional RS methods that
use isotropic Gaussian distributions for smoothing, we build the
smoothed model with paired joint Gaussian distributions, which
capture the structural information among keypoints, and mitigate
the performance degradation caused by smoothing. For the vast
space of the permutation output, we devise a similarity-based par-
titioning method that can lower the computational complexity and
certification difficulty. We then derive a stringent robustness guar-
antee that links the certified space of inputs to their corresponding
fixed outputs. Second, we design a global optimization method to
search for optimal joint Gaussian distributions and facilitate a larger
certified space and better performance. Third, we apply data aug-
mentation and a similarity-based regularizer in training to enhance
smoothed model performance. Lastly, for the high-dimensional and
‚àóCorrespondence authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671852multivariable nature of the certified space, we propose two methods
(sampling and marginal radii) to evaluate it. Experimental results on
public benchmarks show that our method achieves state-of-the-art
certified robustness.
CCS CONCEPTS
‚Ä¢Security and privacy;
KEYWORDS
Certified Robustness; Visual Graph Matching; Optimal Smoothing
ACM Reference Format:
Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi
Yan. 2024. Certified Robustness on Visual Graph Matching via Search-
ing Optimal Smoothing Range. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24), August
25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671852
1 INTRODUCTION
As an essential and popular combinatorial problem, graph match-
ing (GM) has attracted extensive attention over the decades with
wide applications in vision [ 33], text [ 37], graphics [ 32], pattern
recognition [ 31], machine learning [ 44] etc. It refers to establishing
correspondences among two [ 7] or multiple graphs [ 15], and due to
the implicit nature of graphs in many real-world scenarios, visual
graph matching is receiving increasing attention by jointly model-
ing visual perception and the matching procedure by which CNN
and GNN are often both used for node/edge feature extraction and
matching, respectively [35, 40].
On the other hand, studies on the robustness of machine learning
models have attracted intense attention, while the robustness of
combinatorial solvers (especially with machine learning) remains
a crucial, yet largely unexplored area [ 12,24]. Under the deep vi-
sual GM paradigm, recent work [ 27] demonstrates that visual GM
models are susceptible to perturbations applied to keypoints and
image pixels, which suggests the frailness of both the visual per-
ception CNN model as well as the backend decision model for
combinatorial matching, and the authors propose an empirical
defense method based on an appearance-aware regularizer. How-
ever, there is still a lack of a principled certified defense to provide
 
2596
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
theoretical robustness guarantees for GM (let alone other combi-
natorial problems, especially in the practical context of prediction-
and-optimization [13]).
Certified robustness and empirical robustness are two distinct
concepts in the context of adversarial learning. Certified robustness
provides a rigorous verification of the model‚Äôs output invariance un-
der a bounded perturbation set, regardless of the attacks employed.
However, empirical robustness lacks such a theoretical guarantee
and only evaluates the model‚Äôs defense capabilities against exist-
ing attack methods, which may not generalize to future unseen
attacks. Existing certified robustness mechanisms (including ran-
domized smoothing, which we focus on in this study) in the graph
domain [ 3,14,29,46] are confined to the unconstrained node-level
or graph-level classification/prediction task with a single graph
input and are not readily applicable to address visual GM problems
with cross-graph and structured output (permutation matrix).
Certified robustness strives to design solvers whose prediction
for any input ùë•is verifiably invariant within some set around the
input [ 36]. Randomized smoothing (RS) [ 8,18] is a promising ap-
proach to achieving certified robustness of large-scale neural net-
works against arbitrary attacks. Given an input ùë•and a base func-
tion, RS constructs a smoothed function that is certifiably robust
within the region induced by ùë•and the smoothing distribution D
(usually an isotropic Gaussian distribution). RS has been widely
applied to certify various models, e.g., image classification [ 39]
and object detection in vision [ 5], which motivates us to develop
RS-based certified robustness for visual GM.
Applying RS to visual GM poses several challenges. C1: paired
inputs. The input of visual GM consists of paired images and key-
point position matrices, which means that perturbations are also in
pairs and mutually constrained in the certified space. This differs
from the single input setting of previous certification problems.
C2: dependency of keypoints. The graph structure derived from
Delaunay triangulation of keypoint positions as a whole conveys
important structural information. It is an essential intermediate
result for the visual GM model, which motivates us to preserve the
original graph structure during the smoothing process to maintain
the matching performance. C3: large permutation output space.
The output of visual GM is a 0‚àí1permutation matrix, which has an
exponential number of theoretical possibilities. For a matching task
withùëõkeypoints, the output is an ùëõ√óùëõmatrix, and there are ùëõ!theo-
retically possible outputs. This means that we cannot directly apply
the existing RS definition, which estimates the occurrence proba-
bility for each possible output and would cause a computational
explosion. Furthermore, this method is prone to cause certification
failure or an exceedingly small certified space without a dominant
output (an output with a markedly high probability of occurrence).
C4: performance degradation caused by smoothing. Smooth-
ing can influence model performance, as corroborated by prior
studies. Although data augmentation is a customary method to
enhance performance, it is not tailored for visual GM and thus its
effect is inadequate if applied directly.
To address these challenges, we propose Certified Robustness
based on Optimal Smoothing Range Search (CR-OSRS), a novel
robustness certification method for visual GM. Specifically, we as-
sume that the two perturbations against paired inputs belong to the
joint input space and derive a certification result that adheres to theinter-pair constraints (C1). We design a smoothed model by joint
Gaussian distributions that captures the correlation of keypoints
and employ global optimization to determine the optimal correla-
tion parameters that enhance certified robustness. The rationale of
this design is to preserve the difference and avoid confusion among
keypoints under perturbations as much as possible (C2). Further-
more, we delineate a subspace of the output space by a similarity
threshold and characterize the certified robustness as the output
that is invariably within the subspace under perturbations. This
eliminates the need to count the probability of each possible output
and only requires calculating the probability that the output falls
into the subspace (C3). Additionally, we propose a data augmenta-
tion method for visual GM using joint Gaussian noise and an output
similarity-based regularizer, which improves both the matching
accuracy and certified robustness (C4).
The contributions of this paper are as follows:
1) We propose a certification method for visual GM, CR-OSRS,
providing the rigorous robustness guarantee by characterizing ‚Ñì2
and‚Ñì1norm certified input space (see Theorem 4.1 and Appen-
dix. A). The robustness guarantee means that when the perturba-
tion is within the certified input space, the smoothed model always
predicts the output within the output subspace.
2) Specifically, we devise a smoothed model by joint Gaussian
distributions and globally optimize the correlation parameters of
the distributions, which can capture the connection of keypoints
to enhance the anti-disturbance ability of the model (see Sec. 4.2).
We also apply data augmentation with joint Gaussian noise and
the output similarity-based regularizer during the training phase
to further improve the model performance (see Sec. 4.3).
3) We devise two methods, sampling and marginal radii respec-
tively, to measure the certified space for quantitative analysis (see
Sec. 4.4). We evaluate our approach on the Pascal VOC dataset [ 11]
with Berkeley annotations [ 4], the Willow ObjectClass dataset [ 6]
and SPair-71k dataset [ 25] for six representative GM solvers. The re-
sults show that CR-OSRS provides robustness guarantees for visual
GM, and the CR-OSRS mechanism performs better than directly ap-
plying RS [ 8] to visual GM, which we denote as RS-GM. Moreover,
our designed training methods are also effective (see Sec. 5).
2 RELATED WORKS
We review studies on certified robustness through RS and discuss
the GM solver and its robustness.
2.1 Certified Robustness by Randomized
Smoothing
Randomized smoothing is proposed in Lecuyer et al . [18] as a cer-
tified adversarial defense and used to train the pioneering certi-
fiably robust classifier on ImageNet. However, it provides loose
guarantees. Cohen et al . [8] shows that Gaussian noise addition
provides a tight ‚Ñì2certification radius, with subsequent works on
new RS-type techniques, e.g. techniques using smoothing distribu-
tions at different norms [ 20,21,39], and techniques for different
tasks [ 5,14,17,30]. However, all previous smoothing distributions
lack favorable prior knowledge, which mainly refers to the keypoint
 
2597Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
positions and graph structure in visual GM. Moreover, they only cer-
tify a single image or graph and do not consider the combinatorial
nature inherent in visual GM.
2.2 Graph Matching and its Robustness
Approximate GM solvers have evolved from traditional methods
without learning [ 10] to learning-based [ 38]. In practice, GM meth-
ods are often closely related to visual data for matching, whereby
the model needs to consider both visual point features and con-
strained combinatorial matching. Seminal work [ 42] proposes a
deep neural network pipeline for visual GM, in which image fea-
tures are learned through CNN, with subsequent variants [ 28,33],
among which a major improvement is to exploit structural infor-
mation using different techniques, for example GNN, rather than
only using appearance for node/edge attributes as done in [ 42]. Our
work, which uses the RS-type technique, treats the GM solver as a
black box that guarantees the generality of our method, irrespective
of its learning-based or non-learning-based nature.
There are also works on adversarial attacks and defense on (deep)
GM. Previous work [ 41] proposes a robust GM model against per-
turbations, e.g., distortion, rotation, outliers, and noise. Zhang et al .
[45] propose an adversarial attack model for deep GM networks
that employs kernel density estimation to create dense regions
where neighboring nodes are indiscernible. The work [ 26] devises
two specific topology attacks in GM: inter-graph dispersion and
intra-graph combination attacks, and proposes a resilient defense
model. The recent effort Lin et al . [22] integrates the momentum
distillation strategy to balance the quadratic contrastive loss and
reduce the impact of bi-level noisy correspondence. However, these
defense methods are all heuristic and lack robustness certification
under unseen attacks.
3 PRELIMINARIES
3.1 Randomized Smoothing
The RS in [ 8] transforms an arbitrary base classifier ùëìinto a smoothed
classifierùëîthat is certifiably robust under ‚Ñì2norm. For any single in-
putùë•, the smoothed classifier ùëîreturns the most probable prediction
ofùëìfor the random variable N(ùë•;ùúé2ùêº):
ùëî(ùë•)=arg max
ùëê‚ààYùëÉ(ùëì(ùë•+ùúÄ)=ùëê), (1)
whereùúÄ‚àº N 0,ùúé2ùêºis an isotropic Gaussian noise. Then the
certified radius within which the output is unchanged for ùëî(ùë•+ùõø)=
ùëêùê¥that measures the certified robustness is:
‚à•ùõø‚à•2<ùúé
2
Œ¶‚àí1
ùëùùê¥
‚àíŒ¶‚àí1 ùëùùêµ
, (2)
where the most probable class ùëêùê¥is returned with probability ùëùùê¥
and the ‚Äúrunner-up‚Äù class is returned with probability ùëùùêµ.ùëùùê¥and
ùëùùêµare the lower bound and upper bound of ùëùùê¥andùëùùêµ, respec-
tively, and Œ¶‚àí1is the inverse of the standard Gaussian cumulative
distribution function.
3.2 Visual Graph Matching
We consider the visual GM task ùëìwhich is a comprehensive setting
allowing for both visual appearance and structural perturbation:
(c1,c2,z1,z2)‚Üí X, where(c1,c2)is the image pair with keypointposition pair(z1‚ààRùëõ1√ó2,z2‚ààRùëõ2√ó2),ùëõ1andùëõ2are the numbers
of keypoints, X‚àà{0,1}ùëõ1√óùëõ2represents a 0-1 permutation matrix.
Recent deep GM methods tackle images with keypoints as inputs
in an end-to-end manner [ 28,33,34,42] and typically comprise
three components: keypoint feature extractor, affinity learning,
and final correspondence solver. First, two graphs G1andG2are
constructed by Delaunay triangulation [ 19]. Then node features
are obtained via a feature extractor based on the keypoint positions.
Afterward, edge features are constituted based on node features
and topology information of G1andG2. Features are obtained by
bilinear interpolation on the feature map. Based on these node
and edge features, the affinity matrix K‚ààRùëõ1ùëõ2√óùëõ1ùëõ2is initialized
which is then fed to the affinity learning layer to learn the node-
to-node and edge-to-edge correspondence similarity. Finally, the
correspondence solver outputs the predicted permutation matrix X
by solving quadratic assignment problem (QAP) [ 23] which aims
to maximize the overall affinity score ùêΩ:
max
XùêΩ(X)=vec(X)‚ä§Kvec(X),
s.t.X‚àà{0,1}ùëõ1√óùëõ2,X1ùëõ1=1ùëõ1,X‚ä§1ùëõ2‚â§1ùëõ2,(3)
where vec(X)denotes the column-wise matrix of Xwhich is a
partial permutation matrix if ùëõ1<ùëõ2.
As discussed above, image pixels affect the extracted node and
edge features, while keypoint positions affect the extracted node
features by influencing the bilinear interpolation and the graph
structure extracted by Delaunay triangulation. However, the key-
point positions are inherently vulnerable due to the randomness
of human labeling or keypoint detectors (which are used in the
pre-processing step to locate key objects in an image [4]), and the
image pixels are extremely sensitive to various noises impercep-
tible to humans as in other image tasks. Therefore, in this study,
we consider the robustness of visual GM under two types of per-
turbations: perturbations on image pixels and keypoint positions
respectively as in Ren et al . [27] . As these two perturbations belong
to different spaces and exhibit different effects on GM models, we
devise different certification schemes for them. We examine the
certified robustness of GM models under perturbations on image
pixels with fixed keypoint positions and under perturbations on
keypoint positions with fixed pixel values.
4 METHODOLOGY
This section introduces the methodology of this work that com-
prises four parts: (1) the definition of a smoothed GM model and the
theoretical framework developed for certified robustness analysis
(Sec. 4.1); (2) the construction of the joint Gaussian distribution
and an optimization method that assists in searching the optimal
correlation parameter to optimize the trade-off between certified
robustness and model performance (Sec. 4.2); (3) a training method
that incorporates data augmentation with joint Gaussian noise and
an output similarity-based regularizer that limits the discrepancies
among smoothed outputs (Sec. 4.3); (4) two methods (sampling and
marginal radii) for quantifying the robustness certification (Sec. 4.4).
The pipeline is shown in Fig. 1 with the process detailed in Alg. 1.
 
2598KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
Original Input
Copy Input 1
Copy Input nDeep GMDoubly-stochastic matrix 0
Doubly-stochastic matrix 1
Doubly-stochastic matrix n Ground-truth matching
Average Cross EntropySimilarity-based RegularizationNoise 0
Noise 1
Noise n
Keypoint position
Joint Gaussian noise
Data AugmentationPermutation matrix 0
Permutation matrix 1
Permutation matrix n
Total Loss
Original InputKeypoint PositionCorrelation matrixBGlobal Optimization
Trained Model
RS Framework Subspaceùìß‚Ä≤
Certified SpaceTraining Phase Test Phase
Original Smoothing
parameterœÉ
Figure 1: The pipeline of this work consists of two phases: training and testing. In the training phase, we enhance the certified
robustness and matching accuracy of the model simultaneously by applying data augmentation and a regularizer as defined
in Eq. 11. In the testing stage, we first construct joint Gaussian distributions and employ the global optimization in Eq. 10
to search for the optimal smoothing range. Moreover, we use the optimization results and the trained model to construct a
smoothed model, and then compute the output subspace and input certified space following the procedure in Sec. 4.1. It depicts
a sample pipeline of robustness certification under keypoint position perturbations.
4.1 Robustness Guarantee for Visual GM
As discussed in Sec. 3, we certify the robustness under two types of
perturbations: keypoint position perturbations and image pixel per-
turbations respectively. In this subsection, we focus on the certified
robustness under keypoint position perturbations, and the certified
robustness under image perturbations can be derived similarly.
As stated in Sec. 1, visual GM poses a challenge for certified
robustness due to its large permutation output space. Previous re-
search e.g. [ 8] aims to certify that the output remains unchanged
under fixed range perturbations, which may result in a failed cer-
tification or an extremely restricted certified space for visual GM
due to the lack of a dominant output ‚Äì the probability difference
between the most probable output and the ‚Äúrunner-up‚Äù output is
small. Furthermore, it is computationally intractable to enumerate
the probabilities of all possible outputs. Therefore, we propose a
novel certified robustness definition that guarantees the output
always belongs to an output subspace centered on the core output.
We first define a core output Xùëê. When queried at(c1,c2,z1,z2),
Xùëêis a more likely output of base GM function ùëìwhen(z1,z2)is
perturbed by joint Gaussian noise:
Xùëê=ùêª(ùëÜ(‚àëÔ∏Å
ùëì
c1,c2,z1+ùúÄ1,z2+ùúÄ2
)),
whereùúÄ1‚àºN(0,Œ£1),ùúÄ2‚àºN(0,Œ£2),(4)
where the smoothing noise ùúÄ1andùúÄ2follow joint Gaussian dis-
tributions with covariance matrices Œ£1andŒ£2, which represent
constraints between keypoints z1andz2respectively (for solving
C1).ùëÜis the Sinkhorn operator that converts the vertex score matrix
into a doubly-stochastic matrix and ùêªis the Hungarian operatorthat transforms a doubly-stochastic matrix into a permutation ma-
trix. The computation of Eq. 4 takes into account the ‚Äúmajority
decision" of RS while only needing to store the sum of matching
matrices rather than the statistics of each possible matrix. Note that
Xùëêis not the output we aim to certify; it is merely the center point
of the subspace to be constructed, and thus there is no necessity to
consider whether this computation process is provably robust.
Next, we define a subspace X‚Ä≤of the entire output space Xby a
similarity threshold ùë†‚àà[0,1]. The similarity between the points in
X‚Ä≤and the core output Xùëêis no less than ùë†(for solving C3).
X‚Ä≤=(
XùëñXùëñ¬∑Xùëê
Xùëê¬∑Xùëê‚â•ùë†,Xùëñ‚ààX)
, (5)
where we employ a dot product Xùëñ¬∑Xùëêto measure the number
of identical matching keypoints in XùëñandXùëê, because all outputs
are 0-1 permutation matrices. Similarly, Xùëê¬∑Xùëêcalculates the total
number of keypoints to be matched.
By the above definition, we construct a new base function ùëì0
based onùëì. Specifically, we partition the entire output space into
two parts by Eq. 5, then assign all points inside X‚Ä≤with 1and the
rests with 0, and finally convert ùëìto a binary function ùëì0as:
ùëì0
c1,c2,z1,z2
=1,ifùëì(c1,c2,z1,z2)‚ààX‚Ä≤
0,otherwise. (6)
Then we build a smoothed function ùëî0fromùëì0. When queried
at the input c1,c2,z1,z2with fixed(c1,c2),ùëî0outputs the binary
 
2599Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
labels when(z1,z2)is perturbed by joint Gaussian noise:
ùëî0
c1,c2,z1,z2
=1,ifùëÉ(ùëì(c1,c2,z1+ùúÄ1,z2+ùúÄ2)‚ààX‚Ä≤)‚â•1/2
0,otherwise,
whereùúÄ1‚àºN(0,Œ£1),ùúÄ2‚àºN(0,Œ£2).
(7)
Theorem 4.1 ( ‚Ñì2norm certified space for visual GM).
Letùëìbe a matching function, ùëì0andùëî0be defined as in Eq. 6 and
Eq. 7,ùúÄ1‚àºN(0,Œ£1),ùúÄ2‚àºN(0,Œ£2). Supposeùëù‚àà(1
2,1]satisfy:
ùëÉ(ùëì0
c1,c2,z1+ùúÄ1,z2+ùúÄ2
=1)=
ùëÉ(ùëì(c1,c2,z1+ùúÄ1,z2+ùúÄ2)‚ààX‚Ä≤)=ùëù‚â•ùëù.(8)
Then we obtain the ‚Ñì2norm certified space for the perturbation pair
(ùõø1,ùõø2):
ùõø‚ä§
1Œ£‚àí1
1ùõø1+ùõø‚ä§
2Œ£‚àí1
2ùõø2
‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•<Œ¶‚àí1
ùëù
, (9)
which guarantees ùëî0 c1,c2,z1+ùõø1,z2+ùõø2=1.B1‚ààRùëõ1√óùëõ1and
B2‚ààRùëõ2√óùëõ2are full rank and real symmetric matrics based on the
keypoint correlation of keypoint position matrices z1andz2, satisfying
B1‚ä§B1=Œ£1andB2‚ä§B2=Œ£2.
Finally, we formulate a robustness guarantee of ùëî0that ensures
the similarity between the matching matrix and Xùëêbeing no less
thanùë†, that is, the matching matrix always belongs to the subspace
X‚Ä≤. We present and illustrate the detailed settings as well as the
properties of B1andB2in Sec. 4.2. The complete proof of Theo-
rem 4.1 is provided in Appendix D. The theorem for ‚Ñì1certification
is shown in Appendix. A.
4.2 Joint Smoothing Distribution
This subsection presents the detailed settings and properties of B1
andB2under keypoint position perturbations. Additionally, we
introduce an optimization method to search the optimal smoothing
range for enhancing robustness.
As stated in Sec. 3, keypoint positions influence the extracted
features through bilinear interpolation and directly determine the
graph structure. If the smoothing noise for each keypoint position
is completely independent, the perturbed keypoint set may exhibit
partial overlaps or high similarities. This may cause the extracted
features to overlap and thus degrade the matching performance.
Therefore, our objective is to design a smoothing distribution that
can preserve the diversity of keypoints (for solving C2).
To construct the correlation matrices B1andB2, we use a cor-
relation parameter ùëè. The diagonals of B1andB2are original ùúéas
in RS [ 8], the off-diagonal elements adjacent to the main diagonal
areùúé√óùëè, and the remaining elements are 0. This setting not only
maintains the correlation between keypoints but also allows ùëèand
ùúéto be global parameters that can be optimized. We devise an opti-
mization method that aims to maximize the volume of the certified
space through the proxy radius, which will be defined in Sec. 4.4.
We impose a constraint on ùëèin the optimization method to keep it
within a reasonable range, as a large ùëèmay enhance the matching
performance but diminish the certified space. The optimizationproblem can be written as:
arg max
ùúé,ùëèŒ¶‚àí1
ùëù‚àëÔ∏Å
ùëñ=1,2¬©¬≠
¬´2ùëöùëñvutùëöùëñ√ñ
ùëóùúÜùëñùëó¬™¬Æ
¬¨+ùúÖùëè, (10)
whereùúÖ‚ààR+is a hyperparameter, ùúÜùëñùëóis theùëó-th eigenvalue of
Œ£ùëñ, andùëöùëñis the eigenvalue number of Œ£ùëñ. This optimization idea
is inspired by the framework in [ 1,9], but deviates considerably
from them: their optimization is for individual input test points,
while our optimization method is a global optimization for the
entire dataset. Consequently, our method circumvents the data
independence problem in [1, 9].
4.3 Training with Data Augmentation and an
Output Similarity-based Regularizer
As noted in the previous RS methods [ 8,18], the smoothing noise
influences the model performance. Therefore, to improve both the
matching performance and the provable robustness, we adopt two
strategies (for solving C4). The first one is data augmentation, which
is motivated by [ 8]. A joint Gaussian distribution is used to generate
augmented data, which matches the smoothing distribution used
for building the smoothed model. The second one is a regularizer
based on the similarity among smoothed outputs. Since RS operates
on the principle of ‚Äúmajority decision", minimizing the loss between
each smoothed output and the true matching result is not adequate.
We also need to ensure that smoothed outputs are as consistent
as possible for a given input under multiple smoothing noises.
To achieve this, we replicate the same input ùëõtimes, apply data
augmentation to the ùëõdata points, calculate their respective outputs,
and then use the regularizer that penalizes the discrepancy among
theùëõsmoothed outputs. The total loss can be written as:
L=1
ùëõùëõ‚àëÔ∏Å
ùëñLùê∫ùëÄ Xùëñ,Xùëîùë°+ùõΩùëõ‚àëÔ∏Å
ùëñ,ùëó
1‚àíXùëñ¬∑Xùëó
Xùëîùë°¬∑Xùëîùë°
, (11)
whereùõΩ‚ààR+is a hyperparameter, Xùëîùë°is the true matching
for input(c1,c2,z1,z2),XùëñandXùëóare the smoothed outputs for
ùëì(c1,c2,z1+ùúÄ1,z2+ùúÄ2)when(ùúÄ1,ùúÄ2)are sampled by the ùëñ-th and
ùëó-th times, respectively. Lùê∫ùëÄis the original loss function in GM
methods, such as binary cross-entropy [ 34] and pixel offset regres-
sion [ 42]. The first term in Eq. 11 is average matching loss, which
leverages data augmentation to boost the matching accuracy under
perturbations. The second term is a regularizer that enforces a sim-
ilarity constraint among smoothed outputs, which contributes to
increasingùëùin Eq. 8 and thus improving the provable robustness.
4.4 Quantify Certification
In Sec. 4.1, we derive Eq. 9 to characterize the certified space with
paired perturbations, which is, however, challenging to quantify and
compare. Therefore, we propose two quantity metrics to measure
the certified robustness: sampling and marginal radii.
4.4.1 Sampling. According to Eq. 9, the certified robustness of ùëî0
increases when the certified space becomes larger, which means
that more pairs of(ùõø1,ùõø2)satisfy the certified space. However, it is
impractical to determine how many and which pairs of (ùõø1,ùõø2)sat-
isfy Eq. 9, so we propose using a sampling approach to approximate
the certified robustness. Specifically, we sample the perturbation
 
2600KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
Algorithm 1 Certified robustness for deep visual GM (CR-OSRS).
Input: c1,c2,z1,z2; base function ùëì; originalùúé; sample times
ùëò0; similarity threshold ùë†; number of copies ùëõ; regularization
hyperparameter ùõΩ.
Output: Core output Xùëê; evaluation results.
1:Use the data augmentation and regularizer in Sec. 4.3 to train a
visual GM model, and then obtain function ùëì0.
2:Obtain B1,B2,Œ£1,Œ£2described in Sec. 4.2 for perturbing key-
point position pair or image pair using an optimization method,
and then obtain function ùëî0.
3:Sampleùëò0number of input samples. For example, when
perturbing keypoint position pair, we obtain the series:
{(z1‚Ä≤
1,z2‚Ä≤
1),...,(z1‚Ä≤
ùëò0,z2‚Ä≤
ùëò0)}, where z1‚Ä≤
ùëñ‚àº N z1,Œ£1andz2‚Ä≤
ùëñ‚àº
N z2,Œ£2.
4:Predict the core output Xùëêand obtain the subspace X‚Ä≤using
the first sampling series.
5:Sampleùëò=10ùëò0number of input samples. For example,
when perturbing the keypoint position pair, we obtain the
series:{(z1
1,z2
1),...,(z1
ùëò,z2
ùëò)}, where z1
ùëñ‚àºN z1,Œ£1andz2
ùëñ‚àº
N z2,Œ£2.
6:Calculate one-sided confidence lower bound ùëùin Eq. 8 using
the second sampling series.
7:ifùëù<1
2then
8: This input cannot be robustly certified.
9:else
10: Obtain the sampling evaluation result and marginal radii
evaluation result as in Sec. 4.4.
11:end if
12:return Xùëê, evaluation results.
pairs(ùõø1,ùõø2)and verify if they satisfy Eq. 9. The approximate cer-
tified robustness of ùëî0is given by the probability of perturbation
samples that satisfy Eq. 9.
4.4.2 Marginal Radii. Moreover, by fixing one of ùõø1andùõø2, we
simplify the joint space in Eq. 9 to a marginal space, which facilitates
robustness evaluation. Specifically, we set one of ùõø1andùõø2to be a
zero matrix and derive a simple expression for Eq. 9. As an example,
we consider the case of setting ùõø2to a zero matrix as follows:
‚à•ùõø‚ä§
1B‚àí1‚à•<
Œ¶‚àí1
ùëù
. (12)
Lemma 4.2 (Eigenvalue Comparison). For a real symmetric
matrix A‚ààRùëõ√óùëõ, withùúÜmaxandùúÜminas its maximum and minimum
of eigenvalues, then ‚àÄX‚ààRùëõ,ùúÜminX‚ä§X‚â§X‚ä§AX‚â§ùúÜmaxX‚ä§X.
Using Lemma 4.2, we have1
ùúÜ1maxùõø‚ä§
1ùõø1‚â§ùõø‚ä§
1Œ£‚àí1
1ùõø1‚â§1
ùúÜ1minùõø‚ä§
1ùõø1
and derive minimum and maximum ‚Ñì2norm radii from Eq. 12:
‚à•ùõø1‚à•lower=‚àöÔ∏Å
ùúÜ1min
Œ¶‚àí1
ùëù
, (13)
‚à•ùõø1‚à•upper=‚àöÔ∏Å
ùúÜ1max
Œ¶‚àí1
ùëù
, (14)
whereùúÜ1min andùúÜ1max are the minimum and maximum eigenvalue
ofŒ£1. Inspired by [ 9], we can also use the ellipsoidal volume to
measure the certified space. The volume of the ellipsoid is given by:V(R)=ùëüùëö‚àö
ùúãùëö/Œì(ùëö/2+1)√éùëö
ùëñ=1ùúâùëñ[16], which we use to obtain
a proxy‚Ñì2norm radius from Eq. 12:
‚à•ùõø1‚à•volume =
Œ¶‚àí1
ùëù ‚àöùúã/ùëö‚àöÔ∏Å
Œì(ùëö/2+1)
2ùëövtùëö√ñ
ùëñùúÜ1ùëñ,(15)
whereùúÜ1ùëñis theùëñ-th eigenvalue of Œ£1, andùëöis the number of eigen-
values. In summary, the certified space of Eq. 12 can be regarded
as a hyperellipsoid with three radii: ‚à•ùõø1‚à•lower as the minor axis,
‚à•ùõø1‚à•upper as the major axis, and ‚à•ùõø1‚à•volume as a proxy radius of a
hypersphere whose volume is proportional to the volume of this
hyperellipsoid. Eq. 13, Eq. 14 and Eq. 15 are all quantifiable forms:
Eq. 13 is the lower bound radius that guarantees robustness against
the worst-case adversaries, Eq. 14 is the upper bound radius that
indicates the maximum potential to resist adversaries, and Eq. 15 is
the closest assessment to the certified space. Similarly, by setting ùõø1
as a zero matrix, we obtain the three radii of ùõø2in the same manner.
5 EXPERIMENTS
This section presents the experimental settings, including datasets,
GM solvers, parameter settings, and evaluation criteria. It then
evaluates the robustness certification and matching performance of
CR-OSRS and RS-GM for six common GM solvers using sampling
evaluation and marginal radii evaluation as described in Sec. 4.4.
Moreover, it performs ablation studies to illustrate the impact of
different hyperparameters on the outcomes.
5.1 Experiments Settings
In this section, we apply CR-OSRS and RS-GM to transform base
solvers into smoothed ones with certified robustness for compari-
son and analysis. Note that the original RS is not directly applicable
for obtaining robustness certification of functions with paired in-
puts and structured outputs. For comparison, we propose to use
RS-GM, a variant of RS that follows Theorem 4.1, with the only
modification being the replacement of the smoothing distribution
with an isotropic Gaussian distribution.
Following the GM literature [ 34], we evaluate our method on
the Pascal VOC dataset [ 11] with Berkeley annotations [ 4], the Wil-
low ObjectClass dataset [ 6] and SPair-71k dataset [ 25] for six GM
solvers, which are: GMN [ 42], PCA-GM [ 33], CIE-H [ 40], NGMv2 [ 34],
ASAR [ 27], COMMON [ 22]. Unless otherwise specified, we use the
same data processing and hyperparameter settings as in Wang et al .
[34]. All the experiments are conducted on CPU (Intel(R) Core(TM)
i7-7820X CPU @ 3.60GHz) and GPU (GTX 2080 Ti GPU).
5.2 Robustness Certification Evaluation
This section reports the results on Pascal VOC, SPair-71k, and
Willow ObjectClass under keypoint position perturbations. The
results under pixel perturbations are presented in Appendix C.
5.2.1 Sampling Evaluation. We use the sampling method presented
in Sec. 4.4 to estimate the size of the certified space, where a larger
space signifies stronger certified robustness. Specifically, we first
randomly generate 1,000 pairs of (ùõø1,ùõø2)from a uniform distribu-
tionU(ùúé,ùúé). Then we insert the pairs into Eq. 9 and calculate the
probability of pairs that satisfy Eq. 9. This probability for CR-OSRS
with data augmentation is 83.5%and is 40.7%for RS-GM with data
 
2601Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Table 1: ACR of CR-OSRS and RS-GM for six GM solvers on Pascal VOC under keypoint position perturbations. ‚ÄúAUG‚Äù denotes
data augmentation and ‚ÄúREG‚Äù denotes the regularizer. It shows the result for ùúé=0.5,ùë†=0.9,ùõΩ=0.01,ùëõ=2,ùúÖ=1
200andùëò=1000.
CR-OSRS+AUG+REG RS-GM+AUG+REG
‚à•ùõø‚à•lower‚à•ùõø‚à•upper‚à•ùõø‚à•volume‚à•ùõø‚à•lower‚à•ùõø‚à•upper‚à•ùõø‚à•volume
COMMON [22] 1.550 1.751 1.900 0.952 0.952 1.069
ASAR [27] 1.541 1.648 1.968 0.683 0.683 0.841
NGMv2 [34] 1.425 1.586 1.934 0.778 0.778 1.010
CIE-H [40] 0.987 1.167 1.354 0.572 0.572 0.731
PCA-GM [33] 0.954 1.158 1.340 0.546 0.546 0.686
GMN [42] 0.899 1.076 1.253 0.514 0.514 0.617
(a) CA and‚à•ùõø‚à•lower
 (b) CA and‚à•ùõø‚à•upper
 (c) CA and‚à•ùõø‚à•volume
Figure 2: CA of CR-OSRS and RS-GM for NGMv2 on Pascal VOC when perturbing keypoint positions. ‚ÄúAUG‚Äù: data augmentation;
‚ÄúREG‚Äù: the regularizer in Eq. 11. It shows the result for ùúé=0.5,ùë†=0.9,ùõΩ=0.01,ùëõ=2,ùúÖ=1
200andùëò=1000.
(a) CA and‚à•ùõø‚à•lower
 (b) CA and‚à•ùõø‚à•upper
 (c) CA and‚à•ùõø‚à•volume
Figure 3: CA of RS-GM and CR-OSRS for NGMv2 on SPair-71k dataset when perturbing keypoint positions. It shows the result
forùúé=0.5,ùë†=0.9,ùõΩ=0.01,ùëõ=2,ùúÖ=1
200andùëò=1000.
augmentation when ùúé=0.5,ùë†=0.9in Eq. 5,ùõΩ=0.01andùëõ=2in
Eq. 11,ùúÖ=1
200in Eq. 10, and ùëò=1000 in Alg. 1. This indicates that
the certified space derived by CR-OSRS is larger than that derived
by RS-GM, i.e., CR-OSRS achieves a better robustness guarantee.
5.2.2 Marginal Radii Evaluation. To evaluate the three marginal
radii (‚à•ùõø‚à•lower ,‚à•ùõø‚à•upper , and‚à•ùõø‚à•volume ) proposed in Sec. 4.4, we
propose two evaluation criteria: certified accuracy (CA) and average
certified radius (ACR). Inspired by CA for classification [ 8], wedefine CA for GM as follows:
ùê∂ùê¥(ùëÖ)=E(ùë•,Xùëîùë°)
I(‚à•ùõø1‚à•‚â•ùëÖ)I(‚à•ùõø2‚à•‚â•ùëÖ)I(ùëî0(ùë•)=1)I(Xc=Xùëîùë°)
,
(16)
where Iis an indicator function, ‚à•ùõø1‚à•and‚à•ùõø2‚à•denote the radii
calculated by Eq. 13, Eq. 14, or Eq. 15, ùëÖis the scale, ùëî0represents
the smoothed function defined in Eq. 7, ùë•denotes an element in
the test set. Meanwhile, to measure the certified robustness of the
entire test set, we refer to the ACR mentioned in Zhai et al . [43] to
 
2602KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
(a) CA and‚à•ùõø‚à•lower
 (b) CA and‚à•ùõø‚à•upper
 (c) CA and‚à•ùõø‚à•volume
Figure 4: CA achieved by RS-GM and CR-OSRS for NGMv2 on Willow ObjectClass dataset when perturbing keypoint positions.
Fig. 4 shows the result with ùúé=0.5,ùë†=0.9,ùõΩ=0.01,ùëõ=2,ùúÖ=1
200andùëò=1000.
(a) CA and‚à•ùõø‚à•lower when varying original ùúé.ùúédeter-
mines the scale of Œ£1andŒ£2that controls the trade-off
between certified robustness and certified accuracy.
(b) CA and‚à•ùõø‚à•lower when varying the original ùë†. Reduc-
ingùë†enhances the certified robustness, as it enlarges the
output subspace in Eq. 5
(c) CA and‚à•ùõø‚à•lower when varying ùúÖ.ùúÖhas little overall
influence on the outcomes.
Figure 5: Projections for the certified accuracy given larger or smaller original ùúé, similarity threshold ùë†in Eq. 5 and the constraint
hyperparameter ùúÖin Eq. 10. It shows the result of CR-OSRS trained by the data augmentation and regularizer for NGMv2 on
Pascal VOC.
propose the ACR for GM as follows:
ùê¥ùê∂ùëÖ=E(ùë•,Xùëîùë°)
‚à•ùõø1‚à•‚à•ùõø2‚à•I(ùëî0(ùë•)=1)I(Xc=Xùëîùë°)
. (17)
We examine the relationship between CA and three marginal
radii in Fig. 2 on the Pascal VOC dataset, Fig. 3 on the SPair-71k
dataset, and Fig. 4 on the Willow dataset. Specifically, we evalu-
ate the performance of RS-GM and CR-OSRS under three training
conditions: without data augmentation and regularizer, with data
augmentation only, and with both data augmentation and regu-
larizer, as defined in Eq. 11. Note that the performance under the
training conditions with only the regularizer is identical to that
without the data augmentation and regularizer. This is because,
in the absence of data augmentation, the outputs corresponding
to all copy data described in Sec. 4.3 are the same, and thus the
regularizer is always zero. Consequently, the certification result is
consistent with the baseline case RS-GM and CR-OSRS.
In Fig. 2, the curve of CR-OSRS is almost always above RS-GM,
implying higher certified robustness and matching accuracy. At thesame time, it also demonstrates that the proposed data augmenta-
tion and regularizer are effective. In Fig. 3, the curve of CR-OSRS is
also almost always above RS-GM, implying greater certified robust-
ness and matching accuracy. However, we notice that applying both
data augmentation and regularizer to RS-GM worsens the baseline
RS-CM result without them. We hypothesize that this phenomenon
may result from the excessive dispersion of the smoothed outputs,
which prevents the loss after adding the regularizer from converg-
ing properly. In Fig. 4, the curve of CR-OSRS is almost always above
RS-GM, indicating that CR-OSRS corresponds to larger radii for the
same certified accuracy and corresponds to higher accuracy for the
same radii, which implies greater certified robustness. However,
we observe that the improvement of model performance by data
augmentation and regularizer is not as significant as on the Pascal
VOC dataset. We conjecture that this is because the Willow dataset
is less sensitive to perturbations for keypoint positions. Therefore,
data augmentation and regularizer have little effect on the ‚Äúmajority
decision‚Äù of RS and even cause the model to underfit. By integrating
 
2603Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
(a) CA and‚à•ùõø‚à•lower when varying ùëõ. Choosing appropri-
ateùëõcan improve the model performance.
(b) CA and‚à•ùõø‚à•lower when varying ùõΩ. Choosing appro-
priateùõΩhelps balance the trade-off between matching
performance and certified robustness.
(c) CA and‚à•ùõø‚à•lower when varying ùëò. Largerùëòimproves
the robustness certification.
Figure 6: Projections for the certified accuracy given the larger or smaller number of copies ùëõand regularization hyperparameter
ùõΩin Eq. 11, and ùëòfor Monte Carlo sampling in Alg. 1. It shows the result for CR-OSRS trained by the data augmentation and
regularizer for NGMv2 on Pascal VOC.
Fig. 2, Fig. 3, Fig. 4 and Table 1, we conclude that our method is
applicable to various datasets and GM solvers.
To assess the provable robustness of the entire dataset, we com-
pute the ACR of CR-OSRS and RS-GM for six GM solvers in Table 1.
It is evident that the ACR of CR-OSRS is higher than that of RS-GM
and hence the overall provable robustness of CR-OSRS is superior
to that of RS-GM for various GM solvers. Furthermore, we observe
a positive correlation between the performance of the base and
the smoothed models: the smoothed model demonstrates higher
certified robustness as the performance of the base model improves.
5.3 Hyperparameter Analysis
Our method introduces the following hyperparameters: original
ùúé, similarity threshold ùë†for subspace construction as defined in
Eq. 5, the constraint hyperparameter ùúÖ, number of copies ùëõand
regularization hyperparameter ùõΩas shown in Eq. 11 as well as ùëòfor
Monte Carlo sampling. ùúéis varied from ùúé‚àà{0.5,1.0,1.5,2.0}and
the certified accuracy with each ùúéis plotted in Fig. 5(a). Generally, a
lowerùúéresults in higher certified accuracy and lower certified radii,
while a higher ùúéallows for larger certified radii but lower certified
accuracy.ùë†is varied from ùë†‚àà{0.6,0.7,0.8,0.9,1.0}and the certified
accuracy achieved by CR-OSRS with each ùë†is plotted in Fig. 5(b).
Whenùë†=1, the subspace in Eq. 5 degenerates into a single matrix,
which implies a stringent robustness guarantee that the output
remains invariant under any perturbation. However, as shown in
Fig. 5(b), when ùë†=1, the accuracy is always zero, in line with the
discussion in Sec. 4.1. The certification may fail or yield a small
certified space due to the absence of a dominant output. ùúÖis varied
fromùúÖ‚àà{0,1
300,1
200,1
100}the certified accuracy with each ùúÖis plot-
ted in Fig. 5(c). ùúÖhas little overall influence on the outcomes, but a
largerùúÖresults in a smaller ‚à•ùõø‚à•lower .ùëõis varied from ùëõ‚àà{1,2,3,4}
and the certified accuracy with each ùëõis plotted in Fig. 6(a) which
indicates that choosing appropriate values of ùëõis crucial for improv-
ing the model performance. ùõΩis varied from ùõΩ‚àà{0.005,0.01,0.02}
and the certified accuracy with each ùõΩis plotted in Fig. 6(b) which
indicates that choosing appropriate values of ùõΩhelps balance thetrade-off between matching performance and certified robustness.
Furthermore, ùëòis varied from ùëò‚àà{1000,2000,3000,4000,5000}
and the certified accuracy with each ùëòis plotted in Fig. 6(c) (under
the assumption ùëò=10ùëò0). We observe that when ùëòincreases, the
robustness can be certified to be stronger, which is influenced by
the Monte Carlo sampling algorithm.
6 CONCLUSION AND OUTLOOK
This paper has introduced the first definition of certified robustness
for visual graph matching and proposes a novel method, named
CR-OSRS. This method uses the correlation between keypoints to
construct a joint smoothing distribution and devises a global opti-
mization method to determine the optimal smoothing range that
balances provable robustness and matching performance. Further-
more, it presents a data augmentation technique based on the joint
Gaussian distribution and a regularizer based on output similarity
to improve model performance during the training phase. Then
it derives‚Ñì2-norm and‚Ñì1-norm certified space and suggests two
quantitative methods (sampling and marginal radii) to address the
challenge of quantifying the certified space. Finally, it conducts
experiments on different GM solvers and datasets and achieves
state-of-the-art robustness certification.
Future work. A significant direction is to enable robustness
certification on combinatorial solvers whereby GM is a special one
of such cases. We expect that this work can inspire subsequent
research in this promising area i.e. learning for combinatorial op-
timization where theoretical results are welcomed given recent
intensive empirical studies, e.g., [2, 38].
ACKNOWLEDGMENTS
This work was in part supported by the National Natural Science
Foundation of China (92370201, 62202329, 62441605), the Shanghai
Municipal Science and Technology Major Project (2021SHZDZX0102),
the foundation of MoE Key Lab of Artificial Intelligence, and the
Shanghai Pujiang Program (23PJ1412100).
 
2604KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
REFERENCES
[1]Motasem Alfarra, Adel Bibi, Philip Torr, and Bernard Ghanem. 2022. Data Depen-
dent Randomized Smoothing. In The 38th Conference on Uncertainty in Artificial
Intelligence.
[2]Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for
combinatorial optimization: a methodological tour d‚Äôhorizon. European Journal
of Operational Research 290, 2 (2021), 405‚Äì421.
[3]Aleksandar Bojchevski, Johannes Klicpera, and Stephan G√ºnnemann. 2020. Effi-
cient robustness certificates for discrete data: Sparsity-aware randomized smooth-
ing for graphs, images and more. In International Conference on Machine Learning.
PMLR, 1003‚Äì1013.
[4]Lubomir Bourdev and Jitendra Malik. 2009. Poselets: Body part detectors trained
using 3d human pose annotations. In 2009 IEEE 12th International Conference on
Computer Vision. IEEE, 1365‚Äì1372.
[5]Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John
Dickerson, and Tom Goldstein. 2020. Detection as regression: Certified object
detection with median smoothing. Advances in Neural Information Processing
Systems 33 (2020), 1275‚Äì1286.
[6]Minsu Cho, Karteek Alahari, and Jean Ponce. 2013. Learning Graphs to Match.
InProceedings of the IEEE International Conference on Computer Vision (ICCV).
[7]Minsu Cho, Jungmin Lee, and Kyoung Mu Lee. 2010. Reweighted random walks
for graph matching. In European conference on Computer vision. Springer, 492‚Äì
505.
[8]Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified adversarial
robustness via randomized smoothing. In International Conference on Machine
Learning. PMLR, 1310‚Äì1320.
[9]Francisco Eiras, Motasem Alfarra, M Pawan Kumar, Philip HS Torr, Puneet K
Dokania, Bernard Ghanem, and Adel Bibi. 2021. Ancer: Anisotropic certification
via sample-wise volume maximization. arXiv preprint arXiv:2107.04570 (2021).
[10] Frank Emmert-Streib, Matthias Dehmer, and Yongtang Shi. 2016. Fifty years
of graph matching, network alignment and network comparison. Information
sciences 346 (2016), 180‚Äì197.
[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge. Inter-
national journal of computer vision 88, 2 (2010), 303‚Äì338.
[12] Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, and
Stephan G√ºnnemann. 2021. Generalization of Neural Combinatorial Solvers
Through the Lens of Adversarial Robustness. arXiv preprint arXiv:2110.10942
(2021).
[13] H Geng, H Ruan, R Wang, Y Li, Y Wang, L Chen, and J Yan. 2023. Rethinking and
Benchmarking Predict-then-Optimize Paradigm for Combinatorial Optimization
Problems. arXiv preprint arXiv:2311.07633 (2023).
[14] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhenqiang Gong. 2020. Certified
robustness of community detection against adversarial structural perturbation
via randomized smoothing. In Proceedings of The Web Conference 2020. 2718‚Äì2724.
[15] Zetian Jiang, Tianzhe Wang, and Junchi Yan. 2021. Unifying Offline and Online
Multi-graph Matching via Finding Shortest Paths on Supergraph. TPAMI 43, 10
(2021), 3648‚Äì3663.
[16] Maurice G Kendall. 2004. A Course in the Geometry of n Dimensions. Courier
Corporation.
[17] Aounon Kumar and Tom Goldstein. 2021. Center Smoothing: Certified Robustness
for Networks with Structured Outputs. Advances in Neural Information Processing
Systems 34 (2021), 5560‚Äì5575.
[18] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certified robustness to adversarial examples with differential privacy.
In2019 IEEE Symposium on Security and Privacy (SP). IEEE, 656‚Äì672.
[19] D. T. Lee and B. J. Schachter. 1980. Two Algorithms for Constructing a Delaunay
Triangulation. International Journal of Parallel Programming 9, 3 (1980), 219‚Äì242.
[20] Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. 2019. Tight
certificates of adversarial robustness for randomly smoothed classifiers. Advances
in Neural Information Processing Systems 32 (2019).
[21] Alexander J Levine and Soheil Feizi. 2021. Improved, deterministic smoothing for
L_1 certified robustness. In International Conference on Machine Learning. PMLR,
6254‚Äì6264.
[22] Yijie Lin, Mouxing Yang, Jun Yu, Peng Hu, Changqing Zhang, and Xi Peng.
2023. Graph matching with bi-level noisy correspondence. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 23362‚Äì23371.
[23] Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Oswaldo Boaventura-Netto,
Peter Hahn, and Tania Querido. 2007. A survey for the quadratic assignment
problem. European journal of operational research 176, 2 (2007), 657‚Äì690.
[24] Han Lu, Zenan Li, Runzhong Wang, Qibing Ren, Junchi Yan, and Xiaokang Yang.
2021. Mind Your Solver! On Adversarial Attack and Defense for CombinatorialOptimization. arXiv preprint arXiv:2201.00402 (2021).
[25] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. 2019. Spair-71k: A large-
scale benchmark for semantic correspondence. arXiv preprint arXiv:1908.10543
(2019).
[26] Jiaxiang Ren, Zijie Zhang, Jiayin Jin, Xin Zhao, Sixing Wu, Yang Zhou, Yelong
Shen, Tianshi Che, Ruoming Jin, and Dejing Dou. 2021. Integrated defense for
resilient graph matching. In International Conference on Machine Learning. PMLR,
8982‚Äì8997.
[27] Qibing Ren, Qingquan Bao, Runzhong Wang, and Junchi Yan. 2022. Appearance
and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and
Beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 15263‚Äì15272.
[28] Michal Rol√≠nek, V√≠t Musil, Anselm Paulus, Marin Vlastelica, Claudio Michaelis,
and Georg Martius. 2020. Optimizing rank-based metrics with blackbox differen-
tiation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 7620‚Äì7630.
[29] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge:
Towards deep graph convolutional networks on node classification. arXiv preprint
arXiv:1907.10903 (2019).
[30] Huaqing Shao, Lanjun Wang, and Junchi Yan. 2023. Robustness Certification
for Structured Prediction with General Inputs via Safe Region Modeling in the
Semimetric Output Space. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 2010‚Äì2022.
[31] M. Vento. 2015. A long trip in the charming world of graphs for Pattern Recogni-
tion. Pattern Recognition (2015).
[32] Vladimir, G., Kim, Wilmot, Li, Niloy, J., Mitra, Stephen, and DiVerdi. 2012. Explor-
ing collections of 3D models using fuzzy correspondences. ACM Transactions on
Graphics (TOG) - SIGGRAPH 2012 Conference Proceedings 31, 4 (2012), 1‚Äì11.
[33] Runzhong Wang, Junchi Yan, and Xiaokang Yang. 2019. Learning combinatorial
embedding networks for deep graph matching. In Proceedings of the IEEE/CVF
international conference on computer vision. 3056‚Äì3065.
[34] Runzhong Wang, Junchi Yan, and Xiaokang Yang. 2021. Neural graph matching
network: Learning lawler‚Äôs quadratic assignment problem with extension to
hypergraph and multiple-graph matching. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2021).
[35] Runzhong Wang, Junchi Yan, and Xiaokang Yang. 2023. Combinatorial Learning
of Robust Deep Graph Matching: an Embedding based Approach. TPAMI 45, 6
(2023), 6984‚Äì7000.
[36] Eric Wong and Zico Kolter. 2018. Provable defenses against adversarial examples
via the convex outer adversarial polytope. In International Conference on Machine
Learning. PMLR, 5286‚Äì5295.
[37] Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei Zhou, and Dahua Lin.
2019. A graph-based framework to bridge movies and synopses. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 4592‚Äì4601.
[38] Junchi Yan, Shuang Yang, and Edwin Hancock. 2020. Learning Graph Matching
and Related Combinatorial Optimization Problems. In IJCAI.
[39] Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li.
2020. Randomized smoothing of all shapes and sizes. In International Conference
on Machine Learning. PMLR, 10693‚Äì10705.
[40] Tianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. 2019. Learning deep
graph matching with channel-independent embedding and hungarian attention.
InInternational conference on learning representations.
[41] Yu-Feng Yu, Guoxia Xu, Min Jiang, Hu Zhu, Dao-Qing Dai, and Hong Yan. 2019.
Joint Transformation Learning via the L 2, 1-Norm Metric for Robust Graph
Matching. IEEE transactions on cybernetics 51, 2 (2019), 521‚Äì533.
[42] Andrei Zanfir and Cristian Sminchisescu. 2018. Deep learning of graph matching.
InProceedings of the IEEE conference on computer vision and pattern recognition.
2684‚Äì2693.
[43] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar,
Cho-Jui Hsieh, and Liwei Wang. 2020. Macer: Attack-free and scalable robust
training via maximizing certified radius. arXiv preprint arXiv:2001.02378 (2020).
[44] Shaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xi-
aokang Yang, and Pinyan Lu. 2022. M-Mix: Generating Hard Negatives via
Multi-sample Mixing for Contrastive Learning. In Proceedings of Knowledge Dis-
covery and Data Mining Conference.
[45] Zijie Zhang, Zeru Zhang, Yang Zhou, Yelong Shen, Ruoming Jin, and Dejing
Dou. 2020. Adversarial attacks on deep graph matching. Advances in Neural
Information Processing Systems 33 (2020), 20834‚Äì20851.
[46] Daniel Z√ºgner and Stephan G√ºnnemann. 2020. Certifiable robustness of graph
convolutional networks under structure perturbations. In Proceedings of the 26th
ACM SIGKDD international conference on knowledge discovery & data mining.
1656‚Äì1665.
 
2605Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Algorithm 2 Algorithm for optimization.
1:Input:ùêødata; base function ùëì; originalùúé; originalùëè; iteration
timesùêæ.
2:Output: B1,B2,Œ£1,Œ£2.
3:Initialize:ùúé0‚Üêùúé,ùëè0‚Üêùëè.
4:forùëò=0...ùêæ‚àí1do
5: Calculate Bùëò
1,Bùëò
2,Œ£ùëò
1,Œ£ùëò
2usingùúéùëòandùëèùëòaccording to Sec. 4.
6: Initialize the sum of optimization goal ùëÇ.
7: forùëô=0...ùêø‚àí1do
8: Initializeùëòùë°‚Ñédata.
9: SampleùúÄ1‚àºN( 0,Bùëò
1),ùúÄ2‚àºN( 0,Bùëò
2).
10: Calculateùëùaccording to Eq. 8 and eigenvalues of Bùëò
1,Bùëò
2,
then calculate the optimization goal ùëÇùëôas in Eq. 10.
11:ùëÇ‚ÜêùëÇ+ùëÇùëô.
12: end for
13:ùúéùëò+1,ùëèùëò+1‚Üê‚àáùúéùëò,ùëèùëòùëÇ.
14:end for
15:Calculate B1,B2,Œ£1,Œ£2usingùúéùêæ‚àí1andùëèùêæ‚àí1according to
Sec. 4.
16:return B1,B2,Œ£1,Œ£2.
A CERTIFIED ROBUSTNESS FOR ‚Ñì1NORM
We show the robustness guarantee for the ‚Ñì1norm as follows.
Theorem A.1 ( ‚Ñì1norm certified space for visual GM).
Letùëìbe a matching function, ùëì0andùëî0be defined as in Eq. 6 and
Eq. 7 whereùúÄ1‚àºL(ùúÜ1),ùúÄ2‚àºL(ùúÜ2). Supposeùëù‚àà(1
2,1]satisfy:
ùëÉ(ùëì0
c1,c2,z1+ùúÄ1,z2+ùúÄ2
=1)=
ùëÉ(ùëì(c1,c2,z1+ùúÄ1,z2+ùúÄ2)‚ààX‚Ä≤)=ùëù‚â•ùëù.(18)
Then we obtain the ‚Ñì1norm certified space for the perturbation pair
(ùõø1,ùõø2):
‚à•ùõø1‚à•1
ùúÜ1+‚à•ùõø2‚à•1
ùúÜ2‚â§‚àílogh
2
1‚àíùëùi
, (19)
which guarantees ùëî0 c1,c2,z1+ùõø1,z2+ùõø2=1.
By fixing one of ùõø1andùõø2which is similar to in Sec. 4.4.2, we
simplify the joint space in Eq. 19 to a marginal space, which facil-
itates robustness evaluation. Specifically, we set one of ùõø1andùõø2
to be a zero matrix and derive a simple expression for Eq. 19. As
an example, we consider the case of setting ùõø2to a zero matrix as
follows:
‚à•ùõø1‚à•1‚â§‚àíùúÜ1logh
2
1‚àíùëùi
. (20)
B METHODOLOGY SUPPLEMENT
In this section, we provide a supplement to the method described in
Sec. 4. We first present the algorithm of the entire process, and then
explain the construction and optimization of the joint Gaussian
distribution. Alg. 1 consists of training and testing parts. In the
training part, we use data augmentation and a regularizer based
on the output similarity as introduced in Sec. 4.3 to train a model.
In the testing part, we employ Monte Carlo sampling to estimate
the certification result in practice. First, we construct and optimize
the smoothing joint Gaussian distribution according to Sec. 4.2 andconstruct the smoothed model ùëî0. Second, we sample (ùúÄ1,ùúÄ2)with
ùëò0times and obtain the core output Xùëêin Eq. 4 and subspace X‚Ä≤in
Eq. 5. Then we sample (ùúÄ1,ùúÄ2)withùëòtimes, and count how many
outputs fall into the subspace X‚Ä≤to obtain the probability ùëùin Eq. 8
and the certified space in Eq. 9. Finally, we use two quantitative
methods as in Sec. 4.4 to obtain evaluation results.
Alg. 2 summarizes the updates for optimizing by solving Eq. 10
withùêæsteps of stochastic gradient ascent. ùëùis approximated by the
Monte Carlo sampling algorithm in the subsequent certification
process, as described in Alg. 1, but we simplify its approximation
in the optimization method. As we only require a favorable trend
rather than a very accurate ùëùvalue here, we estimate it by sam-
pling once. This method enhances the efficiency of the optimization
method and also reduces the high variability in the gradient esti-
mation due to multiple sampling. We fix the number of iterations
for optimization to ùêæ=10, the size of data used for optimization
toùêø=100, and set the original correlation parameter ùëè=0.01.
Therefore, the entire optimization process is relatively fast and can
be relatively easily applied to various visual GM models.
C EXPERIMENTAL RESULTS ON IMAGE
PIXEL PERTURBATIONS
For perturbing image pixels, we plot the relationship of certified
accuracy (CA) and three marginal radii ( ‚à•ùõø‚à•lower ,‚à•ùõø‚à•upper , and
‚à•ùõø‚à•volume ) in Fig. 7 with the original ùúé=0.5,ùõΩ=0.01andùëõ=2.
Constructing a correlation matrix between pixels is computation-
ally expensive due to the large number of image pixels. Moreover,
it is challenging to extract the correlation between pixels. Hence,
we employ RS-GM to achieve robustness certification under pixel
perturbations. Fig. 7 demonstrates the effectiveness of data augmen-
tation and regularizer. Data augmentation has a significant effect,
but the regularizer does not improve performance in this case. We
hypothesize that this outcome results from the low variability of the
output distribution of a fixed input under multiple perturbations,
which renders the regularizer insignificant.
D PROOFS FOR ‚Ñì2NORM ROBUSTNESS
GUARANTEE
In this section, we present the full proofs for Theorem. 4.1. The
main tool for our proofs is the Neyman-Pearson lemma for two
variables, which we establish in Appendix D.1. Based on this lemma,
we obtain the certified result in Appendix D.2.
D.1 Neyman-Pearson for Two Variables
First, we propose Lemma D.1, which states the Neyman-Pearson
lemma for two variables.
Lemma D.1 (Neyman-Pearson for two variables). Letùëã1and
ùëå1be random variables in Rùëëwith densities ùúáùëã1andùúáùëå1. Then, let
ùëã2andùëå2be random variables in Rùëëwith densities ùúáùëã2andùúáùëå2. Let
‚Ñé:Rùëë√óRùëë‚Üí{0,1}be any deterministic or random function with
an input pair. Then:
1. IfS1√óS2=n
ùëß1‚ààRùëë,ùëß2‚ààRùëë:ùúáùëå1(ùëß1)ùúáùëå2(ùëß2)
ùúáùëã1(ùëß1)ùúáùëã2(ùëß2)‚â§ùë°o
for someùë°>
0andùëÉ(‚Ñé(ùëã1,ùëã2)=1)‚â•ùëÉ((ùëã1,ùëã2)‚ààS 1√óS2), thenùëÉ(‚Ñé(ùëå1,ùëå2)=
1)‚â•ùëÉ((ùëå1,ùëå2)‚ààS 1√óS 2).
 
2606KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, and Junchi Yan
(a) CA and‚à•ùõø‚à•lower
 (b) CA and‚à•ùõø‚à•upper
 (c) CA and‚à•ùõø‚à•volume
Figure 7: CA achieved by RS-GM for NGMv2 on Pascal VOC under pixel perturbations.
2. IfS1√óS2=n
ùëß1‚ààRùëë,ùëß2‚ààRùëë:ùúáùëå1(ùëß1)ùúáùëå2(ùëß2)
ùúáùëã1(ùëß1)ùúáùëã2(ùëß2)‚â•ùë°o
for someùë°>
0andùëÉ(‚Ñé(ùëã1,ùëã2)=1)‚â§ùëÉ((ùëã1,ùëã2)‚ààS 1√óS2), thenùëÉ(‚Ñé(ùëå1,ùëå2)=
1)‚â§ùëÉ((ùëå1,ùëå2)‚ààS 1√óS 2).
Next, we prove Lemma D.2, which is a special case of Lemma D.1
and states the Neyman-Pearson lemma for two joint Gaussian noise
variables.
Lemma D.2 (Neyman-Pearson for Two Joint Gaussian Noise).
Letùëã1‚àºN(ùë•1,Œ£1),ùëã2‚àºN(ùë•2,Œ£2)andùëå1‚àºN(ùë•1+ùõø1,Œ£1),
ùëå2‚àºN(ùë•2+ùõø2,Œ£2). Let‚Ñé:Rùëë√óRùëë‚Üí{0,1}be any deterministic
or random function. Then:
1. IfS1√óS 2=n
ùëß1‚ààRùëë,ùëß2‚ààRùëë:ùõø‚ä§
1Œ£‚àí1
1ùëß1+ùõø‚ä§
2Œ£‚àí1
2ùëß2‚â§ùõΩo
for someùõΩandùëÉ(‚Ñé(ùëã1,ùëã2)=1)‚â•ùëÉ((ùëã1,ùëã2)‚ààS 1√óS 2), then
ùëÉ(‚Ñé(ùëå1,ùëå2)=1)‚â•ùëÉ((ùëå1,ùëå2)‚ààS 1√óS 2).
2. IfS1√óS 2=n
ùëß1‚ààRùëë,ùëß2‚ààRùëë:ùõø‚ä§
1Œ£‚àí1
1ùëß1+ùõø‚ä§
2Œ£‚àí1
2ùëß2‚â•ùõΩo
for someùõΩandùëÉ(‚Ñé(ùëã1,ùëã2)=1)‚â§ùëÉ((ùëã1,ùëã2)‚ààS 1√óS 2), then
ùëÉ(‚Ñé(ùëå1,ùëå2)=1)‚â§ùëÉ((ùëå1,ùëå2)‚ààS 1√óS 2).
D.2 Proof of the Certified Robustness
This subsection presents the logic for proving robustness guarantees
and derives the certified spaces for these guarantees in Eq. 9.
To show that ùëî0 c1,c2,z1+ùõø1,z2+ùõø2=1, it follows from the
definition of ùëî0that we need to show that:
ùëÉ(ùëì
c1,c2,z1+ùúÄ1+ùõø1,z2+ùúÄ2+ùõø2
‚ààX‚Ä≤)
‚â•ùëÉ(ùëì
c1,c2,z1+ùúÄ1+ùõø1,z2+ùúÄ2+ùõø2
‚àâX‚Ä≤).
We define two random variables:
ùêº:=
c1,c2,z1+ùúÄ1,z2+ùúÄ2
=
c1,c2,N
z1,Œ£1
,N
z2,Œ£2
ùëÇ:=
c1,c2,z1+ùúÄ1+ùõø1,z2+ùúÄ2+ùõø2
=
c1,c2,N
z1+ùõø1,Œ£1
,N
z2+ùõø2,Œ£2
.We know that:
ùëÉ(ùëì(ùêº)‚ààX‚Ä≤)‚â•ùëù. (21)
Our goal is to show that
ùëÉ(ùëì(ùëÇ)‚ààX‚Ä≤)>ùëÉ(ùëì(ùëÇ)‚àâX‚Ä≤). (22)
According to lemma D.2, we can define the half-spaces:
A={ùëß1,ùëß2:ùõø‚ä§
1Œ£‚àí1
1(ùëß1‚àíz1)+ùõø‚ä§
2Œ£‚àí1
2(ùëß2‚àíz2)
‚â§‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•Œ¶‚àí1
ùëù
},
B={ùëß1,ùëß2:ùõø‚ä§
1Œ£‚àí1
1(ùëß1‚àíz1)+ùõø‚ä§
2Œ£‚àí1
2(ùëß2‚àíz2)
‚â•‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•Œ¶‚àí1
ùëù
}.
Similar as in [ 8] we can obtain that ùëÉ(ùêº‚ààA) =ùëù, therefore
we can obtain ùëÉ(ùëì(ùêº)‚ààX‚Ä≤)‚â•ùëÉ(ùêº‚ààA) . Hence we may apply
Lemma D.2 to conclude:
ùëÉ(ùëì(ùëÇ)‚ààX‚Ä≤)‚â•ùëÉ(ùëÇ‚ààA). (23)
Similarly, we obtain ùëÉ(ùëì(ùêº)‚àâX‚Ä≤)‚â§ùëÉ(ùêº‚ààB) . Hence we may
apply Lemma D.2 to conclude:
ùëÉ(ùëì(ùëÇ)‚àâX‚Ä≤)‚â§ùëÉ(ùëÇ‚ààB). (24)
Combining Eq. 23 and 24, we can obtain the conditions of Eq. 22:
ùëÉ(ùëì(ùëÇ)‚ààX‚Ä≤)‚â•ùëÉ(ùëÇ‚ààA) >ùëÉ(ùëÇ‚ààB)‚â•ùëÉ(ùëì(ùëÇ)‚àâX‚Ä≤).(25)
Then, we can obtain ùëÉ(ùëÇ‚ààA) andùëÉ(ùëÇ‚ààB) as:
ùëÉ(ùëÇ‚ààA)=Œ¶ 
Œ¶‚àí1
ùëù
‚àíùõø‚ä§
1Œ£‚àí1
1ùõø1+ùõø‚ä§
2Œ£‚àí1
2ùõø2
‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•!
,
ùëÉ(ùëÇ‚ààB)=Œ¶ 
‚àíŒ¶‚àí1
ùëù
+ùõø‚ä§
1Œ£‚àí1
1ùõø1+ùõø‚ä§
2Œ£‚àí1
2ùõø2
‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•!
.(26)
Finally, we obtain that ùëÉ(ùëÇ‚ààA) >ùëÉ(ùëÇ‚ààB) if and only if:
ùõø‚ä§
1Œ£‚àí1
1ùõø1+ùõø‚ä§
2Œ£‚àí1
2ùõø2
‚à•ùõø‚ä§
1Œ£‚àí1
1B1+ùõø‚ä§
2Œ£‚àí1
2B2‚à•<Œ¶‚àí1
ùëù)
.
 
2607