A Tutorial on Multi-Armed Bandit Applications for Large
Language Models
Djallel Bouneffouf
IBM Research
New York, USA
raphael.feraud@orange.comRaphaël Féraud
Orange Innovation
Lannion, 22300, France
raphael.feraud@orange.com
ABSTRACT
This tutorial offers a comprehensive guide on using multi-armed
bandit (MAB) algorithms to improve Large Language Models (LLMs).
As Natural Language Processing (NLP) tasks grow, efficient and
adaptive language generation systems are increasingly needed.
MAB algorithms, which balance exploration and exploitation under
uncertainty, are promising for enhancing LLMs.
The tutorial covers foundational MAB concepts, including the
exploration-exploitation trade-off and strategies like epsilon-greedy,
UCB (Upper Confidence Bound), and Thompson Sampling. It then
explores integrating MAB with LLMs, focusing on designing ar-
chitectures that treat text generation options as arms in a bandit
problem. Practical aspects like reward design, exploration policies,
and scalability are discussed.
Real-world case studies demonstrate the benefits of MAB-augmented
LLMs in content recommendation, dialogue generation, and per-
sonalized content creation, showing how these techniques improve
relevance, diversity, and user engagement.
KEYWORDS
Large Language Models, Bandits, Exploration, Counter-factual esti-
mation
ACM Reference Format:
Djallel Bouneffouf and Raphaël Féraud. 2024. A Tutorial on Multi-Armed
Bandit Applications for Large Language Models. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 2 pages.
https://doi.org/10.1145/3637528.3671440
1 TUTORIAL OUTLINE
The tutorial begins with an introduction to multi-armed bandits [ 1],
which provides a conceptual overview and the definitions of main
concepts, which compares them to classical reinforcement learning
by outlining the different types of bandit problems, and which
discusses their applications in decision- making and optimization
[2].
Next, we delve into the motivation for large language models
[3,4,4], explaining what they are, the challenges involved in their
training and optimization, and the importance of efficient resource
allocation.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671440We then deepen the understanding of the bandit framework
[1], covering the exploration-exploitation trade-off, the formaliza-
tion of the main bandit problems, and some impossibility results.
We detail some of the classical bandit algorithms, such as epsilon-
greedy, Upper Confidence Bound, and Thompson Sampling, before
highlighting algorithms and problems that are particularly relevant
for LLM: contextual and structured bandits [ 5–7], non-stationary
bandits [ 8,9], dueling bandits [ 10], and counter-factual estimation
[11].
We will also discuss the application of bandits in language mod-
eling [ 12–16], including resource allocation for training data, hy-
perparameter optimization, and model selection and evaluation.
The implementation and tools section will provide an overview
of bandit libraries and frameworks, a hands-on demonstration of im-
plementing a simple bandit algorithm, and guidance on integrating
with existing language model architectures .
Best practices and tips will be shared, covering the design of
effective bandit experiments, monitoring and evaluation metrics,
and ethical considerations and bias mitigation.
The tutorial will also cover future directions and open challenges,
such as emerging trends in bandit-based optimization, scalability
and efficiency improvements, and addressing limitations and over-
coming obstacles.
In the conclusion, we will recap key concepts, emphasize the
importance of multi-armed bandits in language modeling, and call
to action for implementing bandit strategies in practice. The tuto-
rial will end with a Q and A session, providing an open floor for
questions and discussions, additional resources and references, and
closing remarks.
This structured outline aims to cover the fundamentals of multi-
armed bandits and their application in optimizing large language
models, along with practical insights, case studies, and opportuni-
ties for hands-on learning and discussion.
This outline provides a structured framework for covering the
fundamentals of Multi-armed Bandits and their application in opti-
mizing large language models, along with practical insights, case
studies, and opportunities for hands-on learning and discussion.
2 SOCIETAL IMPACTS
The tutorial "Multi-Armed Bandit Applications for Large Language
Models" presents strategies for optimizing the exploration-exploitation
trade-off in the context of language model deployment. These strate-
gies can have several societal impacts:
Improved User Experience: By efficiently selecting the most
relevant responses or actions, language models can enhance user
experience across various applications such as customer service
chatbots, educational platforms, or recommendation systems. Users
6412
KDD ’24, August 25–29, 2024, Barcelona, Spain Djallel Bouneffouf and Raphaël Féraud
are more likely to receive accurate and helpful information, leading
to increased satisfaction and engagement.
Resource Efficiency: Optimizing language model interactions
using multi-armed bandit algorithms can lead to more efficient
resource utilization. This means that computational resources are
allocated more effectively, reducing energy consumption and po-
tentially lowering costs for service providers. This efficiency could
have positive environmental impacts by reducing overall energy
consumption.
Personalization and Accessibility: Language models can tailor
responses or actions based on user preferences and behaviors, lead-
ing to more personalized interactions. This personalization can
improve accessibility for individuals with diverse needs, includ-
ing those with disabilities, by providing tailored assistance and
information.
Ethical Considerations: As with any AI application, there are
ethical considerations surrounding the deployment of language
models. The tutorial emphasizes the importance of fairness and
transparency in algorithmic decision-making. By using multi-armed
bandit algorithms, developers can mitigate biases and ensure fair
treatment across different user groups, thereby promoting equity
and inclusivity.
Economic Implications: Efficient language model interactions
can have economic implications by increasing productivity and
streamlining processes in various industries. For example, in e-
commerce platforms, optimizing product recommendations can
lead to higher conversion rates and increased sales. Additionally,
by providing more accurate information and assistance, language
models can support decision-making processes in fields such as
finance and healthcare, potentially leading to better outcomes and
cost savings.
Privacy Concerns: The tutorial acknowledges the importance
of privacy protection in language model deployment. While opti-
mizing interactions can improve user experience, it’s essential to
prioritize user privacy and data security. Developers must imple-
ment robust privacy measures to safeguard sensitive information
and ensure compliance with relevant regulations such as GDPR or
CCPA.
3 TUTORS
3.1 Djallel Bouneffouf
Short bio: Djallel Bouneffoufworked for many years in the field of
online machine learning, with main research interest in building
autonomous systems that can learn to be competent in uncertain
environments. Dr. Djallel Bouneffouf conducted his research in both
public and private sector. He spent 5 years at Nomalys, a Mobile
App Development Company (Paris, France), where he developed
the first risk-aware recommender system, one year at Orange labs
(Lannion, France), where he proposed to model active learning as
a contextual bandit, 2 years at the BC Cancer Agency (Vancouver,
Canada), where he came up with a very fast clustering algorithm
helping the biologist analyzing all the collected and unstructured
data and during his 6 years at (USA and Ireland) he proposed the
first reinforcement learning model that can mimic a different brain
disorder and proposed a novel model of attention based on multi-
armed bandit algorithm. According to Google Scholar, Dr. DjallelBouneffouf is the 10th most cited scientist in his field, he has more
than 100 publications published in top-tier conferences, has over
2000 citations, and has served as a PC in more than 20 conferences.
3.2 Raphaël Féraud
Short bio: Raphaël Féraud is a research scientist at Orange Inno-
vation. He received his PhD in 1997 from Rennes I university, and
his HDR (French post-doctoral degree allowing its holder to super-
vise PhD students) in 2023 from Paris-Saclay university. During
his career in industry, he worked on many applications of machine
learning: face detection, churn, queuing optimization, marketing
optimization, big data, IoT, cloud optimization, smart grid, recom-
mendation... His research concerned many fields of machine learn-
ing: neural networks, data mining, stream mining, non-stationary
data, reinforcement learning. His current research focuses both on
theoretical and application aspects of bandit algorithms.
REFERENCES
[1] Tor Lattimore and Csaba Szepesvari. Bandit algorithms. 2017.
[2]Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications
of multi-armed and contextual bandits. In 2020 IEEE Congress on Evolutionary
Computation (CEC), pages 1–8. IEEE, 2020.
[3]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
Advances in neural information processing systems, 30, 2017.
[4]Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
models, 2021.
[5]Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with
linear payoff functions. In Proceedings of the Fourteenth International Conference
on Artificial Intelligence and Statistics, pages 208–214, 2011.
[6]Djallel Bouneffouf, Irina Rish, Guillermo A Cecchi, and Raphaël Féraud. Context
attentive bandits: Contextual bandit with restricted context. arXiv preprint
arXiv:1705.03821, 2017.
[7]Djallel Bouneffouf, Raphael Feraud, Sohini Upadhyay, Irina Rish, and Yasaman
Khazaeni. Toward optimal solution for the context-attentive bandit problem. In
IJCAI, pages 3493–3500, 2021.
[8]Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The
nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 2002.
[9]Robin Allesiardo, Raphaël Féraud, and Odalric-Ambrym Maillard. The Non-
stationary Stochastic Multi-armed Bandit Problem. International Journal of Data
Science and Analytics, 2017.
[10] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, and Sami Naamane. Generic
exploration and k-armed voting bandits. In ICML, 2013.
[11] Houssam Zenati, Eustache Diemert, Matthieu Martin, Julien Mairal, and Pierre
Gaillard. Sequential counterfactual risk minimization. In ICML, 2023.
[12] Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A Rossi, Sungchul Kim, and Shuai
Li. Which llm to play? convergence-aware online model selection with time-
increasing bandits. arXiv preprint arXiv:2403.07213, 2024.
[13] Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin
Van Roy. Efficient exploration for llms. arXiv preprint arXiv:2402.00396, 2024.
[14] Chengshuai Shi, Kun Yang, Jing Yang, and Cong Shen. Best arm identification for
prompt learning under a limited budget. arXiv preprint arXiv:2402.09723, 2024.
[15] Lixiang Li, Bharat Bhargava, Alina Nesen, and Nagender Aneja. Sentimentpulse:
Temporal-aware custom language models vs. gpt-3.5 for consumer sentiment.
[16] Djallel Bouneffouf, Oznur Alkan, Raphaël Féraud, and Baihan Lin. Question an-
swering system with sparse and noisy feedback. In IEEE International Conference
on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece,
June 4-10, 2023, pages 1–5. IEEE, 2023.
6413