SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape
Hua Zheng∗
Northeastern University
Boston, MA, USA
zheng.hua1@northeastern.eduKuang-Hung Liu
Meta
Menlo Park, CA, USA
khliu@meta.comIgor Fedorov
Meta
Menlo Park, CA, USA
ifedorov@meta.com
Xin Zhang
Meta
Menlo Park, CA, USA
xinzhang5@meta.comWen-Yen Chen
Meta
Menlo Park, CA, USA
wychen@meta.comWei Wen
Meta
Menlo Park, CA, USA
wewen@meta.com
Abstract
Neural Architecture Search (NAS) has become a widely used tool
for automating neural network design. While one-shot NAS meth-
ods have successfully reduced computational requirements, they
often require extensive training. On the other hand, zero-shot NAS
utilizes training-free proxies to evaluate a candidate architecture’s
test performance but has two limitations: (1) inability to use the
information gained as a network improves with training and (2)
unreliable performance, particularly in complex domains like Rec-
Sys, due to the multi-modal data inputs and complex architecture
configurations. To synthesize the benefits of both methods, we in-
troduce a “sub-one-shot” paradigm that serves as a bridge between
zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is
trained using only a small subset of the training data, a phase we
refer to as “warm-up.” Within this framework, we present SiGeo,
a proxy founded on a novel theoretical framework that connects
the supernet warm-up with the efficacy of the proxy. Extensive
experiments have consistently shown that SiGeo, when properly
warmed up, surpasses state-of-the-art NAS proxies in many es-
tablished NAS benchmarks in the computer vision domain. Fur-
thermore, when tested on recommendation system benchmarks,
SiGeo demonstrates its ability to match the performance of state-of-
the-art weight-sharing one-shot NAS methods while significantly
reducing computational costs by approximately 60%.
CCS Concepts
•Information systems →Information retrieval ;•Computing
methodologies→Machine learning.
Keywords
Neural Architecture Search, Zero-shot NAS, Sub-one-shot NAS,
Loss Landscape, Recommendation System
∗This work was done when the first author was an intern at Meta.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671712ACM Reference Format:
Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen,
and Wei Wen. 2024. SiGeo: Sub-One-Shot NAS via Geometry of Loss Land-
scape. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671712
1 Introduction
In recent years, Neural Architecture Search (NAS) has emerged as
a pivotal paradigm for automating the design of neural networks.
One-shot NAS simplifies the process of designing neural networks
by using a single, comprehensive supernet (i.e. search space), which
contains a versatile choice of candidate architectures. This approach
allows for quick evaluation of various network architectures, saving
both time and computational resources. It has been successfully
applied to the vision domain [ 6,15,33,40] and RecSys domain
[23,43,62]. One-shot NAS speeds up the process of finding the
best neural network design, but it still requires training [ 28]. To
mitigate this, zero-shot Neural Architecture Search (zero-shot NAS)
has been developed with the aim of bypassing the need for extensive
training and evaluation. The core of zero-shot NAS is to use a zero-
cost metric to quickly evaluate the actual performance of candidate
architectures, While a wide range of zero-shot proxies have been
introduced, a recent study [ 56] has suggested that many of them
do not fully utilize the information gained as a network improves
with training. As a result, the pretraining of a supernet does not
necessarily translate into enhanced search performance, leaving an
unexplored opportunity for improvement.
Despite significant advances in zero-shot NAS in the computer
vision domain, its application to other complex domains, such as
recommendation systems (RecSys), remains largely underexplored.
RecSys models introduce unique challenges, primarily owing to
their multi-modal data inputs and the diversity in their architec-
tural configurations [ 62]. Unlike vision models, which generally
rely on homogeneous 3D tensors, RecSys handles multi-modal fea-
tures, a blend of 2D and 3D tensors. Additionally, vision models
predominantly utilize convolutional layers, whereas RecSys models
are heterogeneous within each stage of the model using a variety
of building blocks, including but not limited to Sum, Gating, Dot-
Product, and Multi-Head Attention. This added complexity makes
the naive deployment of state-of-the-art (SOTA) zero-shot NAS
approaches less effective in the recommendation context.
To mitigate the limitations of existing NAS methods, this work
presents a novel sub-one-shot search strategy. As a setting between
4536
KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
0.2 0.4 0.6 0.8 1.0
GPU Days0.4400.4410.4420.4430.444Log Loss
(a) Criteo
0.2 0.4 0.6 0.8 1.0
GPU Days0.3740.3750.3760.3770.3780.3790.380Log Loss (b) Avazu
0.2 0.4 0.6 0.8 1.0
GPU Days0.14850.14900.14950.15000.15050.15100.15150.1520Log LossDNAS
PROFIT
AutoCTR
Random Search @ NASRec-Small
Random Search @ NASRec-Full 
NASRecNet @ NASRec-Small
NASRecNet @ NASRec-Full
ZiCo @ NASRec-Small (zero-shot)
ZiCo @ NASRec-Full (zero-shot)
SiGeo @ NASRec-Small (sub-one-shot) (c) KDD 2012
Figure 1: Performance summary of SiGeo-NAS on CTR Predictions Tasks. NASRec-Small and NASRec-Full are the two search
spaces, and NASRecNet is the NAS method from [62]. Here ★represents SiGeo under NASRec-Small search space.
zero-shot and one-shot, sub-one-shot NAS allows for limited train-
ing of supernet with a small portion of data while still employing a
training-free proxy for performance prediction of sampled subnets,
as illustrated in Figure 2. Therefore, the sub-one-shot NAS con-
siders a trade-off between computational efficiency and predictive
performance. In order to effectively utilize the additional informa-
tion inherited from the pretrained supernet, we further develop a
newSub-one-shot proxy based on the Fisher information matrix
andGeometry of loss landscape, named SiGeo.
We evaluate our proposed method on three popular CTR bench-
marks and demonstrate significant improvements compared to
both hand-crafted models and NAS-crafted models. Specifically, we
adapted the implementation of the SOTA one-shot weight-sharing
NAS, NASRec [ 62], and tailored it to a sub-one-shot version us-
ing SiGeo as a proxy, called SiGeo-NAS. In comparison to other
hand-crafted models and NAS-crafted models, both NASRec and
SiGeo-NAS crafted models show superior performance, achieving
significant log loss reductions. However, when evaluating computa-
tional cost, SiGeo stands out as it achieves a significant reduction in
the computational cost. Furthermore, when compared to the SOTA
zero-shot NAS approach, ZiCo [ 28], the SiGeo-NAS crafted model
achieves lower log loss while maintaining comparable computa-
tional efficiency. This highlights SiGeo’s significance in the balance
between performance and computational resources.
In sum, this work makes the following contributions:
•We introduce SiGeo, a novel proxy proven to be effective in
NAS when the candidate architectures are warmed up.
•We theoretically analyze the geometry of loss landscapes
and demonstrate the connection between the warm-up of
supernet and the effectiveness of the SiGeo.
•Sub-one-shot setting is proposed to bridge the gap between
zero-shot and one-shot NAS.
•Extensive experiments are conducted to assess the effective-
ness of SiGeo on both CV and RecSys tasks. The results
show that as we increase the warm-up level, SiGeo’s scores
align more closely with test accuracies. Compared with the
weight-sharing one-shot NAS, SiGeo shows comparable per-
formance but with a significant reduction in computational
costs, as shown in Figure 1 (Note that a 0.1% increase in AUC
is significant for CTR prediction [44, 53]).2 Related Work
One-Shot NAS. One-shot NAS reduces initial training costs by
using a supernet encompassing the entire search space, allowing
subnet evaluation with shared weights. Numerous methods have
been proposed for CV [ 7,9–11,15,27,45], and natural language
processing tasks [42, 52].
Zero-Shot NAS. Zero-Shot NAS employs a training-free proxy to
evaluate architectures, eliminating the necessity for costly model
training. Some proxies are developed to measure the expressivity of
a deep neural network [ 3,8,32,36] while many rely on gradients of
network parameters [ 1,26,34,47,50]. ZiCo, a zero-shot proxy based
on the relative standard deviation of the gradient, was introduced
[28] and shown to be better than other zero-shot proxies. Recently,
MeCo simplified the evaluation of architectures by analyzing the
Pearson correlation matrix of feature maps [ 20]. The zero-shot NAS
is less explored in the recommendation system domain.
Other NAS. Another noteworthy approach is predictor-assisted
NAS [ 46,54,55,55] where performance predictors are used to
accelerate architecture search, efficiently finding high-performing
networks with lower computational costs. Recently, [ 51] proposed
PreNAS to reduce the search space by a zero-cost selector before
performing the weight-sharing one-shot training on the selected
architectures to alleviate gradient update conflicts.
NAS for Recommendation System. Despite the notable achieve-
ments in computer vision (CV) and natural language processing
(NLP) tasks, the study of one-shot NAS in recommender systems is
relatively limited [ 16,23,43]. This is partly due to the unique and
diverse architectural requirements of recommender systems, which
are specifically designed to handle multi-modal data. Recently, a
novel approach, NASRec [ 62], has been developed to address this
limitation. It leverages weight-sharing NAS to fully enable NAS for
recommender systems, even in the presence of data modality and
architectural heterogeneity. Notably, NASRec has achieved SOTA
performance in three well-established benchmarks.
3 Problem Description
One-Shot NAS One-shot NAS algorithms [ 5] train a weight sharing
supernet, which jointly optimizes subnets in neural architecture
search space. Then this supernet is used to guide the selection of
candidate architectures. Let Adenote the architecture search space
4537SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape KDD ’24, August 25–29, 2024, Barcelona, Spain
andΘas all learnable weights. Let ℓdenote the loss function. The
goal of one-shot training is to solve the following optimization
problem under the given resource constraint 𝑟:
Θ★=arg min
ΘE𝒂∼A[ℓ(𝒂|Θ𝒂,D)]
𝒂★=arg max
𝒂∈AP𝑠𝑐𝑜𝑟𝑒(𝒂|Θ★
𝒂;D𝑣𝑎𝑙)s.t.𝑅(𝒂)≤𝑟
where 𝒂is the candidate architecture (i.e. subnet) sampled from
the search spaceA,Θ𝒂∈Θis the corresponding part of inherited
weights (i.e. subnet) from a supernet and Θ★𝒂represents the weights
of the subnet 𝒂inherited from a well-trained supernet. 𝑅(·)rep-
resents the resource consumption, i.e. latency and FLOPs. P𝑠𝑐𝑜𝑟𝑒
evaluates performance on the validation dataset.
Zero-Shot NAS Zero-shot NAS proxies have been introduced,
aiming to predict the quality of candidate architectures without the
need for any training. Zero-shot NAS can be formulated as
𝒂★=arg max
𝒂∈AP𝑧𝑒𝑟𝑜(𝒂|Θ𝒂)s.t.𝑅(𝒂)≤𝑟
whereP𝑧𝑒𝑟𝑜 is a zero-shot proxy that can quickly evaluate the
performance of any given candidate architecture without training.
Sub-One-Shot NAS. For the weight-sharing NAS, one key differ-
ence between one-shot and zero-shot settings is the warm-up phase.
In a one-shot NAS, the supernet is trained using a large dataset.
However, as noted by [ 51], “training within a huge sample space
damages the performance of individual subnets and requires more
computation to search". Conversely, the zero-shot setting omits this
warm-up phase, leading to a more efficient process. Nonetheless,
the performance of zero-shot proxies is often less reliable and sub-
ject to task-specific limitations. To bridge the gap, we propose a
new setting called “sub-one-shot", where the supernet is allowed
to be warmed up by a small portion of training data (e.g. 1% train-
ing samples) prior to the search process. Our goal is to propose a
new proxy that achieves one-shot NAS level performance but with
reduced search cost in sub-one-shot settings; see Figure 2a.
Remark 1. Compared with other zero-shot proxies, SiGeo allows
for the supernet warm-up. In practice, the warm-up with 1-10%
data is enough to make SiGeo achieve the SOTA performance.
Information Matrix and Fisher-Rao Norm Following the same
setup as Thomas et al . [48] , we have access to a set of samples
(𝒙,𝒚)∈X×Y where 𝒙is the input and 𝒚the target. Fisher infor-
mation matrix 𝑭and the Hessian 𝑯are define as,
𝑭(𝜽)=E𝑝𝜽𝜕2ℓ(𝜽;𝒙,𝒚)
𝜕𝜽𝜕𝜽⊤
=E𝑝𝜽𝜕
𝜕𝜽ℓ(𝜽;𝒙,𝒚)𝜕
𝜕𝜽ℓ(𝜽;𝒙,𝒚)⊤
𝑯(𝜽)=E𝑞𝜕2ℓ(𝜽;𝒙,𝒚)
𝜕𝜽𝜕𝜽⊤
where𝑞and𝑝𝜃represent data and model distributions respectively.
It has been shown that Fisher and Hessian matrices are identi-
cal when the learned model distribution is the same as the true
sampling distribution [ 21,48]. Thomas et al . [48] has shown that
these two matrices are empirically close as training progresses. The
Fisher-Rao (FR) norm, defined as ∥𝜽∥𝑓𝑟:=𝜽 𝑭(𝜽)𝜽, has been pro-
posed as a measure of the generalization capacity of deep neural
networks (DNNs) [30].
Notation. Here we briefly summarize the notations in the paper.
For any matrix 𝑨, we use∥𝑨∥, and∥𝑨∥𝐹to denote its operatornorm and Frobenius norm, respectively. In addition, the eigenvalues
𝜆𝑖(𝑨)of matrix 𝑨are sorted in non-ascending order, i.e. 𝜆1≥𝜆2≥
...≥𝜆𝑑. Unless otherwise specified, we use E[·]to denote the
expectation over the joint distribution of all samples accumulated
in the training process to date, often referred to as the filtration in
stochastic approximation literature. In what follows, we use ⊘to
denote the element-wise division and ∇to represent∇𝜽.
4 SiGeo Proxy Design
We discuss developing SiGeo for a candidate architecture in the
sub-one-shot setting, where the weights of the candidate architec-
tures are inherited from the warmed-up supernet. In contrast to
zero-shot NAS, after the warm-up of the supernet, the inherited
weights of subnets will escape the saddle points and reside near
a certain local minimum denoted by ˆ𝜽★[14,58]. This critical in-
sight enables us to focus on a small convex region surrounding the
local minimum (Assumption A.2). Leveraging the local convexity
property (outlined in Assumption A.4) and the loss decomposition
(detailed in Section 4.1), we develop SiGeo, a proxy for test loss,
utilizing estimators for training loss and generalization error. These
estimators are further discussed in Section 4.5 and Section 4.4, pro-
viding a framework for evaluating the efficiency and effectiveness
of candidate architectures in the sub-one-shot setting.
4.1 Losses for Candidate Architectures
For each candidate architecture 𝒂associated with the weight 𝜽:=
Θ𝒂∈Θ, consider a supervised learning problem of predicting
outputs 𝒚∈Y from inputs 𝒙∈X. Let ˆ𝜽★denote the local minimum
near which the weight of subnet is located. Given 𝑁i.i.d training
samplesD={(𝒙𝑛,𝒚𝑛);𝑛=1,...,𝑁}, the training objective is to
minimize the empirical training loss starting with an initial weight
𝜽0inherited from the pretrained supernet as follows
ˆ𝜽★∈arg min
𝜽∈R𝑑ℓ(𝜽;D):=Eˆ𝑞[ℓ(𝜽;𝒙,𝒚)]=1
𝑁𝑁∑︁
𝑛=1ℓ(𝜽;𝒙𝑛,𝒚𝑛)(1)
where ˆ𝑞is the empirical data distribution and ℓ(𝜽;𝒙,𝒚)is the loss
function. We consider stochastic gradient update of the form
𝜽𝑘+1←𝜽𝑘−𝜂𝑘𝑩−1
𝑘∇ℓ(𝜽𝑘;D𝑘) (2)
where 𝑩𝑘is a curvature matrix and
∇ℓ(𝜽𝑘;D𝑘)=1
|D𝑘|∑︁
(𝒙𝑛,𝒚𝑛)∈D 𝑘∇ℓ(𝜽𝑘;𝒙𝑛,𝒚𝑛)
is a sample gradient estimate from a uniformly sampled mini-batch
D𝑘⊆D at iteration 𝑘. Eq. (2) becomes a first-order stochastic
gradient descent (SGD) if the curvature function is an identity
matrix, 𝑩𝑘=𝑰. It turns out to be a natural gradient descent if the
curvature function is the exact Fisher information matrix 𝑩𝑘=𝑭
and second-order optimization if 𝑩𝑘is the Hessian [35].
LetL(𝜽)=E𝑞[ℓ(𝜽;𝒙,𝒚)]denote the expected loss with respect
to the true data distribution 𝑞(𝑥,𝑦). Since𝑞is unknown, in practice,
we rely on validation/training loss to evaluate the performance
of each fine-tuned subnet albeit at a considerable computational
expense. This loss can be decomposed at a true local minimum
4538KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
(a) Illustration of SiGeo-NAS under different NAS settings.
෠𝜃⋆
ℒ෠𝜃⋆After warming up the 
supernet, the candidate 
architecture is here
𝜃0Optimality 
Gap 𝒬𝜃𝑘Generalization 
Error 𝒢𝜃𝑘Training Loss
Test Loss
ℒ𝜃𝑘 (b) SiGeo: Loss landscape geometry.
Figure 2: (a) SiGeo-NAS samples candidate architectures with a light warm-up. In comparison to the one-shot NAS, this strategy
offers an efficient means of evaluating subnets. In contrast to the zero-shot approach, SiGeo-NAS attains improved performance
while incurring only a minimal warm-up cost. (b) The connection between SiGeo and the geometry of loss landscape around
the local minimum. The test error is approximated by the minimum achievable training loss and the generalization error.
𝜽★=arg min𝜽∈R𝑑L(𝜽):
L(𝜽★)=E𝑞h
ℓ(𝜽★;𝒙,𝒚)−ℓ(ˆ𝜽★;𝒙,𝒚)i
|                                 {z                                 }
Excess Risk
+E𝑞h
ℓ(ˆ𝜽★;𝒙,𝒚)i
−Eˆ𝑞h
ℓ(ˆ𝜽★;𝒙,𝒚)i
|                                        {z                                        }
Generalization Error+Eˆ𝑞h
ℓ(ˆ𝜽★;𝒙,𝒚)i
|              {z              }
Training Loss(3)
where the first term represents the excess risk, the difference be-
tween test error and the minimum possible error. The second term
isgeneralization error measuring the difference between train and
test error and indicating the extent to which the classifier may be
overfitted to a specific training set. The last term is the training loss.
The loss decomposition serves as a bridge between the test loss
L(ˆ𝜽★), the generalization error, and the training loss. This connec-
tion inspires us to develop a new proxy for the test loss, leveraging
training-free estimators of both the training loss and generalization
error. Nevertheless, crafting predictors for the minimum achievable
training loss and generalization error in non-convex optimization
is challenging. To overcome this obstacle, we adopt the supernet
warm-up approach as a means to collectively initialize weights for
all candidate architectures, ensuring that the initialized networks
are located around favorable local minima. As shown in Figure 2b,
this approach enables us to focus on a convex region around a single
local minimum where it becomes possible to derive a predictor for
the minimum achievable training loss using Taylor’s approximation
(Theorem 1) and a predictor for the generalization error by using
the established correlation between low generalization error and
flat minima (refer to Section 4.5).
4.2 Regularity Conditions
We summarize the assumptions for our analysis. The justification
for the assumption can be found in Appendix A.A.1 (Realizability) The true data-generating distribution is in the
model class.
A.2 (Initiation) The initial weight of the candidate architecture,
𝜽0, (probably after the warm-up) is located within a compact
setBcentered at ˆ𝜽★, such that∥𝜽0−ˆ𝜽★∥1≤𝑀.
A.3 (Differetiability) The loss function of candidate network
ℓ(𝜽;𝒙,𝒚)is differetiable almost everywhere on 𝜽∈Bfor all
(𝒙,𝒚)∈X×Y .
A.4 (Invertibility) The Hessian, Fisher’s information matrix and
covariance matrix of the gradient are positive definite in the
parametric space Bsuch that they are invertible.
4.3 On Convergence of Training Loss
Now let’s analyze the convergence rate for a given candidate ar-
chitecture and investigate the impact of gradient variance. Let
Q(𝜽):=L(𝜽)−L( ˆ𝜽★)=E[ℓ(𝜽;𝒙,𝒚)]−E[ℓ(ˆ𝜽★;𝒙,𝒚)]denote
the (local) optimality gap from the local minimum loss. The follow-
ing theorem presents the rate of convergence by using a similar
technique as Garrigos and Gower [17, Theorem 5.3.].
Proposition 1. Assume A.1-A.4. Consider{𝜽𝑘}𝑘∈Za sequence
generated by the first-order (SGD) algorithm (2), with a decreasing
sequence of stepsizes satisfying 𝜂𝑘>0. Let𝜎2
𝑘:=Var[∇ℓ(𝜽𝑘;D𝑘)]
denote the variance of sample gradient, where the variance is taken
over the joint distribution of all the training samples until the 𝑘-th
iteration. Then It holds that
E[L(¯𝜽𝑘)]−L( ˆ𝜽★)≤∥𝜽0−ˆ𝜽★∥
2Í𝑘−1
𝑖=0𝜂𝑖+1
2𝑘−1∑︁
𝑖=0𝜎2
𝑘
The proof of Proposition 1 is provided in Appendix B. The key
insight from this theorem is the relationship between the optimality
gap and the gradient variance. In particular, the smaller the gradient
variance across different training samples, the lower the training
loss the model converges to; i.e., the network converges at a faster
4539SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape KDD ’24, August 25–29, 2024, Barcelona, Spain
rate. In Section 4.5, we’ll discuss how the gradient variance is related
to the Hessian matrix around local minima, thereby representing
the curvature of the local minimum as shown in [48].
4.4 Lower Bound of Minimum Achievable
Training Loss
This section examines the loss landscape to provide insights into
predicting the minimum achievable training loss for a candidate
architecture. The discussion follows the similar analysis technique
used by [35]. Applying Taylor’s approximation to L(𝜽)gives
L(𝜽𝑘)−L( ˆ𝜽★)=1
2(𝜽𝑘−ˆ𝜽★)⊤𝑯(ˆ𝜽★)(𝜽𝑘−ˆ𝜽★)
+∇L( ˆ𝜽★)⊤(𝜽𝑘−ˆ𝜽★)+O
(𝜽𝑘−ˆ𝜽★)3
=1
2(𝜽𝑘−ˆ𝜽★)⊤𝑯(ˆ𝜽★)(𝜽𝑘−ˆ𝜽★)+O
(𝜽𝑘−ˆ𝜽★)3
(4)
where the last inequality holds due to ∇L( ˆ𝜽★)=0. By taking
the expectation over the joint distribution of all samples used for
training until 𝑘-th iteration, the optimality gap (4) becomes
Q𝑘:=E[L(𝜽𝑘)]−L( ˆ𝜽★)
=1
2Eh
(𝜽𝑘−ˆ𝜽★)⊤𝑯(ˆ𝜽★)(𝜽𝑘−ˆ𝜽★)i
+Eh
O
(𝜽𝑘−ˆ𝜽★)3i
.(5)
By assuming the local convexity (A.4) and some mild conditions,
it holds Eh
∥𝜽𝑘−ˆ𝜽★∥2i
=O(1
𝑘); see details in Lacoste-Julien et al .
[25, Theorem 4 ]or Bottou and Le Cun [4, Theorem A3 ]. It implies
that the higher order term Eh
(𝜽𝑘−ˆ𝜽★)3i
would shrink faster, that
is,Eh
(𝜽𝑘−ˆ𝜽★)3i
=𝑜(1
𝑘)[35]. As a result, it is sufficient to focus on
a quadratic loss and then present the lower bound of the minimum
achievable training loss; see the proof of Theorem 1 in Appendix C.
Theorem 1. Assume A.1-A.4. Let 𝜇𝑘=Í𝑘
𝑖=0E[∥∇ℓ(𝜽𝑖;D𝑖)∥1]
denote the sum of the expected absolute value of gradient across mini-
batch samples, denoted by D𝑖, where the gradient norm is
∥∇ℓ(𝜽𝑖;D𝑖)∥1=𝑑∑︁
𝑗=1∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;D𝑖),
∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;D𝑖)=1
|D𝑖||D𝑖|∑︁
𝑛=1∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;𝒙𝑛,𝒚𝑛).
Under some regularity conditions such that Eh
(𝜽𝑘−ˆ𝜽★)3i
=𝑜(1
𝑘),
it holds
L(ˆ𝜽★)≥E[L(𝜽𝑘)]−1
2Eh
𝜽⊤
𝑘𝑭(ˆ𝜽★)𝜽𝑘i
−𝜂𝜇𝑘∥𝑯(ˆ𝜽★)ˆ𝜽★∥∞
−1
2(ˆ𝜽★−2𝜽0)⊤𝑯(ˆ𝜽★)ˆ𝜽★+𝑜1
𝑘
.
Theorem 1 presents a lower bound of the minimum achiev-
able training loss. More precisely, a reduced minimum achiev-
able training loss is attainable when the expected absolute sam-
ple gradients 𝜇𝑘and the FR norm E[𝜽⊤
𝑘𝑭(ˆ𝜽★)𝜽𝑘]are high, or theexpected current training loss E[L(𝜽𝑘)]is low. As an approxi-
mation of the Fisher [ 41], the empirical Fisher information ma-
trix (EFIM) is used in the implementation, which is defined as
ˆ𝑭(𝜽)=1
𝑁Í𝑁
𝑛=1𝜕
𝜕𝜽ℓ(𝜽;𝒙𝑛,𝒚𝑛)𝜕
𝜕𝜽ℓ(𝜽;𝒙𝑛,𝒚𝑛)⊤.
4.5 Generalization Error
The decomposition of expected test loss (3) underscores the sig-
nificance of generalization error. The association of local flatness
of the loss landscape with better generalization in DNNs has been
widely embraced [ 22,30,57,59]. This view shows that flat min-
ima, requiring less information to characterize, should outperform
sharp minima in terms of generalization [ 19]. In assessing flatness,
most measurements approximate the local curvature of the loss
landscape, characterizing flat minima as those with reduced Hes-
sian eigenvalues [ 57]. As Hessians are very expensive to calculate,
the sample gradient variance is used to measure local curvature
properties. A rigorous discussion can be found in [ 28, Section 3.2.2].
4.6 Zero-Shot Metric
Inspired by the theoretical insights, we propose SiGeo, a test loss
predictor that jointly considers both the minimum achievable train-
ing loss and generalization error. The generalization error can be
approximated by the gradient variance as discussed in Section 4.5
and the minimum achievable training loss can be approximated by
absolute sample gradients, FR norm and current training loss, as
depicted in Theorem 1. With experiments of different combinations
and weights, we consider the following proxy formulation.
Definition 1. Let𝜽(𝑚)denote the parameters of the 𝑚-th module,
i.e.𝜽(𝑚)⊆𝜽. Let𝑘denote the number of batches used to compute
SiGeo. Given a neural network with 𝑀modules (containing trainable
parameters), the zero-shot from information theory and geometry of
loss landscape (SiGeo) is defined as follows:
𝑆𝑖𝐺𝑒𝑜 =𝑀∑︁
𝑚=1𝜆1log𝝁(𝑚)
𝑘⊘𝜎𝜎𝜎(𝑚)
𝑘1
+𝜆2log
𝜽⊤
𝑘ˆ𝑭(𝜽𝑘)𝜽𝑘
−𝜆3ℓ(𝜽𝑘;D𝑘) (6)
where 𝝁(𝑚)
𝑘=1
𝑘Í𝑘
𝑖=0|∇𝜽(𝑚)ℓ(𝜽𝑖;D𝑖)|is the average of absolute
gradient estimate with respect to 𝜽(𝑚)and
𝜎𝜎𝜎(𝑚)
𝑘=vuut
1
𝑘𝑘∑︁
𝑖=0 
∇𝜽(𝑚)ℓ(𝜽𝑖;D𝑖)−1
𝑘𝑘∑︁
𝑖=0∇𝜽(𝑚)ℓ(𝜽𝑖;D𝑖)!2
is the standard deviation of gradient across batches. See Appendix 4.7
for the implementation details of SiGeo.
For the first term, we could consider a neuron-wise formula-
tion𝜆1log ∥𝝁𝑘⊘𝜎𝜎𝜎𝑘∥1as an alternative to the module-wise one,
which is adapted from ZiCo. As depicted in Figure 3, the second
and third terms in Eq. (6) are important in utilizing the information
accrued during the training process.
Remark 2. Based on our theoretical framework, various formu-
lations for the zero-shot proxy can be chosen. In particular, when
both𝜆2and𝜆3are set to zero, SiGeo simplifies to ZiCo. Likewise, if
we allow for complete warming up of the supernet and fine-tuning
4540KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
of the subnet during the search, SiGeo becomes equivalent to one-
shot NAS when 𝜆1and𝜆2are set to zero. In practice, SiGeo reaches
SOTA performance using just four batches (see Section 5). Thus,
we compute SiGeo with only four input batches ( 𝑘=4); this makes
SiGeo computationally efficient.
4.7 Practical Implementation
Two key adjustments from weight-sharing NAS are needed: (1)
conducting a supernet warm-up by randomly selecting a small
portion of data from the training set, and (2) evaluating a stand-
alone subnet’s performance using the SiGeo proxy instead of relying
on the training accuracy after the post-training fine-tuning.
Proxy Weights. In all experiments in Section 5, we set the value of
𝜆1to 1. For candidate architectures without warm-up, we adjust our
proxy by setting 𝜆2=1and𝜆3=0to account for significant noise
in training loss values and the reduced effectiveness of the Fisher-
Rao (FR) norm. Conversely, if a warm-up procedure is performed,
we opt for 𝜆2=𝜆3=1in RecSys search spaces, while for CV
benchmarks, we use 𝜆2=50and𝜆3=1. The larger value of 𝜆2
for CV benchmarks is due to the observation that the first term in
SiGeo tends to exhibit a notably higher magnitude for convolutional
neural network models compared to those found within the NASRec
search space. Generally, the choice of proxy weights is contingent
upon the warm-up levels, where higher warm-up levels correspond
to increased weights for 𝜆2and𝜆3.
5 Experiments
We conduct 4 sets of experiments: (1) Validation of the theoretical
findings; (2) Evaluation of the SiGeo on zero-shot NAS benchmarks;
(3) Evaluation of the SiGeo on NAS benchmarks under various
warm-up levels; (4) Evaluation of SiGeo on click-through-rate (CTR)
benchmarks. An ablation study is performed to assess the efficacy of
each key component of SiGeo in Section 5.5. The implementation is
publicly available at https://github.com/zhenghuazx/NasRec-SiGeo.
5.1 Setup
In Experiment (1), our goal is to validate Theorems 1. We con-
structed two-layer MLP with rectified linear unit (ReLU) activation
(MLP-ReLU) networks with varying hidden dimensions, ranging
from 2 to 48 in increments of 2. Networks were trained on the
MNIST dataset for 3 epochs using the SGD optimizer with a learn-
ing rate of 0.02. The batch size was set as 128.
In Experiment (2), we compare SiGeo with other zero-shot prox-
ies on three NAS benchmarks, including (i) NAS-Bench-101 [ 60]
(ii) NAS-Bench-201 [ 12] and (iii) NAS-Bench-301 [ 61], a surrogate
benchmark for the DARTS search space [ 33] and (iv) TransBench-
101-Micro [ 13]. Experiments are performed in the zero-shot setting
and results are presented in Section 5.3.1. In Experiment (3), we
compare our method against ZiCo under various warm-up levels on
the same search spaces and tasks as used in Experiment (2). Results
are provided in Section 5.3.2.
In Experiment (4), we demonstrate empirical evaluations on
three popular RecSys benchmarks for Click-Through Rates (CTR)
1.75 
1.50 
V'I 
� 1.25 
.....J 1.00 .., 
V'I 
� 0.75 
0.50 
0.25 Spearman's rho: 0.30 
kendall's tau: 0.21 
0% Warm-up 
Spearman's rho: 0. 71 
kendall's tau: 0.52 
1.15 10% Warm-up 
1.50 
V'I 
� 1.25 
.....J 
.., 1.00 
V'I 
� 0.75 
0.50 
0.25 
Spearman's rho: 0. 79 
kendall's tau: 0.61 
1.15 20% Warm-up • •• 
J. 1.50 
V'I 
� 1.25 
.....J 
.., 1.00 
V'I 
� 0.75 
0.50 
0.25 ••·r••
..... a'! ...
. ' .,4. 
Spearman's rho: 0.80 
kendall's tau: 0.61 
1.15 40% Warm-up • • ·­. ··-
1.50 
V'I 
� 1.25 
.....J 
.., 1.00 
V'I 
� 0.75 
0.50 
0.25 
1.75 
1.50 
V'I 
� 1.25 
.....J 
.., 1.00 
V'I 
� 0.75 
0.50 
0.25 --�=" ,., ..• 
I,. ••• • ...
Spearman's rho: 0.82 
kendall's tau: 0.63 
60% Warm-up • '•. :r .. ,..,. •• Spearman's rho: -0.31 
kendall's tau: -0.21 
• 0% Warm-up 
• • 
• 
Spearman's rho: -0.61 
kendall's tau: -0.44 
10% Warm-up • 
Spearman's rho: -0. 70 
kendall's tau: -0.52 
20% Warm-up 
Spearman's rho: -0.71 
kendall's tau: -0.53 
40% Warm-up 
Spearman's rho: -0.71 
kendall's tau: -0.52 
60% Warm-up Spearman's rho: -0.48 
kendall's tau: -0.34 
0% Warm-up 
Spearman's rho: -0.54 
kendall's tau: -0.40 
10% Warm-up 
Spearman's rho: -0.57 
kendall's tau: -0.42 
, ... -· ••••20% Warm-up 
• • 
• 
SI 1 • 
Spearman's rho: -0.58 
kendall's tau: -0.43 
40% Warm-up 
�-
£ sec • 
Spearman's rho: -0.58 
kendall's tau: -0.44 
60% Warm-up 
Current Training Loss Fisher-Rao Norm Mean Absolute GradientsFigure 3: Test losses vs. statistics described in Theorem 1.
Results are generated by optimizing two-layer MLP-ReLU
networks with varying hidden dimensions, ranging from 2 to
48 in increments of 2. Statistics computed after the warm-up
with 0-100% data. The performance improvement in the FR
norm is observed to cease after 40% warm-up.
prediction: Criteo1, Avazu2and KDD Cup 20123. To identify the
optimal child subnet within the NASRec search space – encompass-
ing NASRec Small and NASRec-Full [ 62] – we employ the effective
regularized evolution technique [ 39]. All three datasets are pre-
processed in the same fashion as AutoCTR [ 43]. We conduct our
experiments under three supernet warm-up levels: 0%, 1%, and 100%,
corresponding to zero-shot, sub-one-shot, and one-shot settings.
5.2 Empirical Justification for the Theory
Our investigation starts with the validation of Theorem 1. The
effectiveness of sample standard deviation of gradients (see Propo-
sition 1) has been well validated by [ 28]. Therefore, we focus on the
correlations between training loss and test loss in relation to the
other three key statistics: (i) current training loss, (ii) FR norm, and
(iii) mean absolute gradients. Specifically, we conduct assessments
on three different warm-up levels, i.e. from 0%, 10% to 100%. The
concrete training configures are described in Appendix D.1.
1https://www.kaggle.com/competitions/criteo-display-ad-challenge/data
2https://www.kaggle.com/competitions/avazu-ctr-prediction/data
3https://www.kaggle.com/competitions/kddcup2012-track2/data
4541SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The correlation coefficients between various zero-cost proxies and two naive proxies (#params and FLOPs) vs. test
accuracy on various NAS benchmarks. SPR and Kendall represent Spearman’s 𝜌and Kendall’s 𝜏, respectively. Experiments are
run in the zero-shot setting. The best results are shown with bold fonts.
Proxy NameNB101-CF10 NB201-CF10 NB201-CF100 NB201-IMGNT NB301-CF10 TNB101-Object
SPR Kendall SPR Kendall SPR Kendall SPR Kendall SPR Kendall SPR Kendall
epe-nas [34] 0.00 0.00 0.70 0.52 0.60 0.43 0.33 0.23 0.00 0.00 0.39 0.26
fisher [49] -0.28 -0.20 0.50 0.37 0.54 0.40 0.48 0.36 -0.28 -0.19 0.44 0.29
FLOPs [38] 0.36 0.25 0.69 0.50 0.71 0.52 0.67 0.48 0.42 0.29 0.46 0.31
grad-norm [1] -0.25 -0.17 0.58 0.42 0.63 0.47 0.57 0.42 -0.04 -0.03 0.39 0.27
grasp [50] 0.27 0.18 0.51 0.35 0.54 0.38 0.55 0.39 0.34 0.23 -0.22 -0.14
jacov [36] -0.29 -0.20 0.75 0.57 0.71 0.54 0.71 0.54 -0.04 -0.03 0.51 0.36
l2-norm [1] 0.50 0.35 0.68 0.49 0.72 0.52 0.69 0.50 0.45 0.31 0.32 0.25
NASWOT [36] 0.31 0.21 0.77 0.58 0.80 0.62 0.77 0.59 0.47 0.32 0.39 0.32
#params [38] 0.37 0.25 0.72 0.54 0.73 0.55 0.69 0.52 0.46 0.31 0.45 0.32
plain [1] -0.32 -0.22 -0.26 -0.18 -0.21 -0.14 -0.22 -0.15 -0.32 -0.22 0..34 0.23
snip [26] -0.19 -0.14 0.58 0.43 0.63 0.47 0.57 0.42 -0.05 -0.03 0.45 0.35
synflow [47] 0.31 0.21 0.73 0.54 0.76 0.57 0.75 0.56 0.18 0.12 0.48 0.34
Zen [31] 0.59 0.42 0.35 0.27 0.35 0.28 0.39 0.29 0.43 0.30 0.54 0.40
ZiCo [28] 0.63 0.46 0.74 0.54 0.78 0.58 0.79 0.60 0.65 0.48 0.66 0.32
MeCo opt[20] 0.44 - 0.90 - 0.89 - 0.85 - 0.71 - 0.59 -
SiGeo (Ours) 0.63 0.46 0.78 0.58 0.82 0.62 0.80 0.61 0.65 0.48 0.66 0.32
Table 2: The correlation coefficients of SiGeo and ZiCo vs. test accuracy on various warm-up levels. SPR and Kendall represent
Spearman’s 𝜌and Kendall’s 𝜏, respectively.
Benchmark NB101-CF10 NB201-CF10 NB201-CF100 NB201-IMGNT NB301-CF10 TNB101-Object
Method Warm-up Level SPR Kendall SPR Kendall SPR Kendall SPR Kendall SPR Kendall SPR Kendall
ZiCo 0% 0.63 0.46 0.74 0.54 0.78 0.58 0.79 0.60 0.50 0.35 0.66 0.47
ZiCo 10% 0.63 0.46 0.78 0.58 0.81 0.61 0.80 0.60 0.51 0.36 0.67 0.48
ZiCo 20% 0.64 0.46 0.77 0.57 0.81 0.62 0.79 0.59 0.51 0.36 0.68 0.49
ZiCo 40% 0.64 0.46 0.78 0.58 0.80 0.61 0.79 0.59 0.52 0.36 0.68 0.50
ZiCo 60% 0.64 0.47 0.78 0.58 0.81 0.62 0.79 0.59 0.53 0.38 0.67 0.49
ZiCo 100% 0.63 0.46 0.77 0.57 0.80 0.61 0.78 0.59 0.53 0.37 0.67 0/48
SiGeo 0% 0.63 0.46 0.78 0.58 0.82 0.62 0.80 0.61 0.5 0.35 0.66 0.47
SiGeo 10% 0.68 0.48 0.83 0.64 0.85 0.66 0.85 0.67 0.53 0.37 0.68 0.49
SiGeo 20% 0.69 0.51 0.84 0.65 0.87 0.69 0.86 0.68 0.55 0.40 0.70 0.50
SiGeo 40% 0.70 0.52 0.83 0.64 0.89 0.71 0.87 0.69 0.56 0.41 0.71 0.52
SiGeo 60% 0.69 0.51 0.82 0.64 0.87 0.70 0.87 0.69 0.57 0.41 0.71 0.52
SiGeo 100% 0.68 0.50 0.82 0.63 0.88 0.71 0.86 0.68 0.56 0.41 0.71 0.51
Figure 3 shows that networks with higher mean absolute gradi-
ents, FR norm values, or lower current training loss tend to have
lower test and training loss. These results coincide with the conclu-
sion drawn from Theorem 1. In addition, we observe a significant
trend: as the warm-up level grows, there’s a marked improvement
in the ranking correlation between the current training loss and the
FR norm with the final training/test loss. In contrast, the correlation
of the mean absolute gradients remains fairly stable throughout
this process. Given that the ZiCo proxy relies solely on the gradient
standard deviation and the mean absolute gradients, the improved
correlation coefficients of the FR norm and current training loss
offer insights into why SiGeo outperforms the ZiCo method, par-
ticularly in the sub-one-shot setting.5.3 CV Benchmark Results
We assess the performance of SiGeo under various warm-up levels
across a range of well-established computer vision tasks.
5.3.1 Comparing SiGeo with Other Proxies under the Zero-Shot
Setting. We compute the correlation coefficients between proxies
and test accuracy on several datasets. Specifically, we use the CI-
FAR10 (CF10) dataset from NASBench-101 (NB101), NASBench-201
(NB201), and NASBench-301 (NB301); the CIFAR100 (CF100) dataset
from NB201; the ImageNet16-120 (IMGNT) dataset from NB201.
Due to the space limit, we only include the object classification
benchmark (TNB101-Object) from TransBench-101-Micro [ 13]. The
experiments in this section are evaluated with NAS-Bench-Suite-
Zero [ 24] to ensure a fair and comprehensive comparison with a
range of tasks with other SOTA methods. As presented in Table 1,
4542KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
Table 3: Performance of SiGeo-NAS on CTR Predictions Tasks. NASRec-Small and NASRec-Full are the two search spaces, and
NASRecNet is the NAS method from [62].
Setting MethodCriteo Avazu KDD Cup 2012 Search Cost
(GPU days)Log Loss AUC Log Loss AUC Log Loss AUC
Hand-crafted ArtsDLRM [37] 0.4436 0.8085 0.3814 0.7766 0.1523 0.8004 -
xDeepFM [29] 0.4418 0.8052 - - - - -
AutoInt+ [44] 0.4427 0.8090 0.3813 0.7772 0.1523 0.8002 -
DeepFM [18] 0.4432 0.8086 0.3816 0.7757 0.1529 0.7974 -
NAS-crafted Arts
(one-shot)
(100% warm-up)DNAS [23] 0.4442 - - - - - ∼0.28
PROFIT [16] 0.4427 0.8095 0.3735 0.7883 - - ∼0.5
AutoCTR [43] 0.4413 0.8104 0.3800 0.7791 0.1520 0.8011 ∼0.75
Random Search @ NASRec-Small 0.4411 0.8105 0.3748 0.7885 0.1500 0.8123 1.0
Random Search @ NASRec-Full 0.4418 0.8098 0.3767 0.7853 0.1509 0.8071 1.0
NASRecNet @ NASRec-Small 0.4395 0.8119 0.3741 0.7897 0.1489 0.8161 ∼0.25
NASRecNet @ NASRec-Full 0.4404 0.8109 0.3736 0.7905 0.1487 0.8170 ∼0.3
SiGeo @ NASRec-Small 0.4399 0.8115 0.3743 0.7894 0.1487 0.8171 ∼0.1
SiGeo @ NASRec-Full 0.4403 0.8110 0.3741 0.7898 0.1484 0.8187 ∼0.12
NAS-crafted Arts
(sub-one-shot)
(1% warm-up)ZiCo @ NASRec-Small 0.4404 0.8109 0.3754 0.7876 0.1490 0.8164 ∼0.1
ZiCo @ NASRec-Full 0.4403 0.8100 0.3762 0.7860 0.1486 0.8174 ∼0.12
SiGeo @ NASRec-Small 0.4396 0.8117 0.3741 0.7898 0.1484 0.8185 ∼0.1
SiGeo @ NASRec-Full 0.4397 0.8116 0.3754 0.7876 0.1485 0.8178 ∼0.12
NAS-crafted Arts
(zero-shot)
(0% warm-up)ZiCo @ NASRec-Small 0.4408 0.8105 0.3770 0.7849 0.1491 0.8156 ∼0.09
ZiCo @ NASRec-Full 0.4404 0.8108 0.3772 0.7845 0.1486 0.8177 ∼0.11
SiGeo @ NASRec-Small 0.4404 0.8109 0.3750 0.7882 0.1489 0.8165 ∼0.09
SiGeo @ NASRec-Full 0.4404 0.8108 0.3765 0.7856 0.1486 0.8177 ∼0.11
SiGeo shows a high correlation with the true test accuracy com-
pared to other zero-shot proxies while MeCo, a very recent proxy,
exhibits higher performance in certain settings. Note that experi-
ments are conducted without warm-up of candidate architectures.
5.3.2 Comparing SiGeo with ZiCo on various Warm-up Levels. We
conduct experiments to compare SiGeo against the SOTA proxy
ZiCo under a sub-one-shot setting. Experiments are performed
under the same setting of Section 5.3.1 with two key differences:
(1) candidate architectures are warmed up before calculating the
proxy scores; (2) we set 𝜆2=50and𝜆3=1when the warm-up level
is greater than zero. The results in Table 2 show (1) the ranking
correlation of ZiCo does not improve much with more warm-up; (2)
conversely, SiGeo’s ranking correlation enhances significantly as
the warm-up level increases, particularly when the warm-up level
is below 40%. These results are consistent with the results in Section
5.2, underscoring the importance of the Fisher-Rao (FR) norm and
current training loss in predicting the network performance when
the candidate architectures are warmed up.
5.4 RecSys Benchmark Results
We use three RecSys benchmarks to validate the performance of
SiGeo under different warm-up levels when compared with ZiCo
[28] and one-shot NAS approaches as well as hand-crafted models.
5.4.1 Experiment Configuration. We first show the training set-
tings and details of regularized evolution and model selection strate-
gies that SiGeo NAS employs during architecture search, and final
evaluation. The search space configuration and policy are the same
as NASRec [62]; see Zhang et al. [62] for the details.Supernet Training Settings During this process, we train the
supernet for a single epoch, employing the Adagrad optimizer, an
initial learning rate of 0.12, and a cosine learning rate schedule on
target RecSys benchmarks.
Regularized Evolution. Following [ 62], we employ an efficient
configuration of regularized evolution to find the optimal subnets
from the supernet. Specifically, we keep 128 architectures in a
population and run the regularized evolution algorithm for 240 iter-
ations. In each iteration, we select the best design from 64 sampled
architectures as the parent architecture, then create 8 new child
architectures to update the population.
Model Selection. we adhere to the evaluation protocols outlined
in AutoCTR [ 43] and NASRec [ 62]. Each dataset is partitioned
into three sets: training (80%), validation (10%), and testing (10%).
During the search, the supernet undergoes training on the training
set, and the top-15 subnets are selected based on performance on
the validation set. Subsequently, these top-15 models are trained
from scratch, and the most optimal subnet is chosen as the final
architecture, named SiGeo, short for SiGeo-based NASRec.
5.4.2 State-of-the-art Performance. We evaluate our SiGeo-NAS
against SOTA one-shot NAS baselines (DNAS[ 23], PROFIT[ 16],
AutoCTR[ 43], NASRecNaet [ 62]), and SOTA zero-shot NAS baseline
(ZiCo [ 28]) on three benchmark datasets: Criteo, Avazu, and KDD
Cup 2012. NASRecNaet, ZiCo-crafted model and SiGeo-crafted
model are all trained in the NASRec-Full and NASRec-Small search
spaces [ 62]. Figure 1 provides a visual comparison between the
SiGeo-crafted model and other baselines, while comprehensive
results can be found in Table 3. It should be noted that a 0.1%
increase in AUC is significant for CTR prediction [44, 53].
4543SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape KDD ’24, August 25–29, 2024, Barcelona, Spain
Compared to manually crafted CTR models [ 18,29,37,44], SiGeo-
NAS demonstrates significantly improved performance, as high-
lighted in Table 3 and Figure 1. Furthermore, we assess the SiGeo-
crafted model against the latest NAS-crafted models. Based on Table
3, we observe that both NASRecNet [ 62] and SiGeo consistently
outperform various hand-crafted and NAS-crafted architectures
across three CTR prediction tasks. After warming up the supernet
using only 1% of the training data (sub-one-shot setting), SiGeo, im-
plemented on both NASRec-Small and NASRec-Full search spaces,
shows remarkable performance in comparison to established one-
shot SOTA benchmark, NASRecNet, with 2-3times less computa-
tion time. For instance, in the Criteo dataset, SiGeo on NASRec-
small achieves a log loss of 0.4396 and an AUC of 0.8117 with 0.1
GPU days, while NASRecNet on the same dataset and search space
yields a log loss of 0.4396 and an AUC of 0.8119 using 0.25 GPU
days. Similarly, in the KDD Cup 2021 dataset, SiGeo on NASRec-
Small achieves a log loss of 0.1484 and an AUC of 0.8185, outper-
forming NASRecNet, which yields a log loss of 0.1489 and an AUC
of 0.8161 with a search cost reduction of 0.15 GPU days. These
concrete performance metrics demonstrate that SiGeo effectively
utilizes the NASRec search space [ 62] to generate competitive mod-
els, achieving comparable or superior results while also offering
reduced computational costs.
5.5 Ablation Study
To evaluate the effects of two key components, namely ZiCo and the
FR norm, on SiGeo’s performance, we carried out an ablation study.
Specifically, we compare the best subnets identified using ZiCo and
the FR norm as proxies. All experiments are performed in a sub-one-
shot setting with 1% warm-up, following the same configuration as
outlined in Section 5.4. The top-15 models selected by each proxy
are trained from scratch and their test accuracies are illustrated as
boxplots in Figure 4. The results reveal that the exclusion of any
terms from SiGeo detrimentally affects performance.
FR Norm Zico SiGeo0.43950.44000.44050.44100.44150.4420T est LossNASRec-Small
Best reported in 
Zhang et al. (2023)
FR Norm Zico SiGeo0.43950.44000.44050.44100.44150.4420
NASRec-Full
Best reported in 
Zhang et al. (2023)
Figure 4: Evaluating the performance of two critical compo-
nents (FR norm and ZiCo).
We varied𝜆1and𝜆2(keeping𝜆3=1) to assess their impact on
performance across different warm-up levels, testing sensitivity
to hyper-parameter selection. The results indicate that changes
in𝜆2generally yield more significant variations in performance,
particularly in the NB201-CF10 dataset, suggesting a relatively
higher sensitivity to the FR norm under the tested conditions.
6 Conclusion
In this paper, we introduce a “sub-one-shot" NAS paradigm that
serves as a bridge between zero-shot and one-shot NAS. In thisTable 4: Ablation Study for 𝜆1and𝜆2
𝜆1𝜆2Warm-up
LevelNB101-CF10 NB201-CF10
Spearman Kendall Spearman Kendall
1 50 10% 0.68 0.48 0.83 0.64
1 50 20% 0.69 0.51 0.84 0.65
1 50 40% 0.70 0.52 0.83 0.64
1 100 10% 0.68 0.48 0.86 0.68
1 100 20% 0.71 0.52 0.86 0.68
1 100 40% 0.72 0.53 0.84 0.65
1 1 10% 0.65 0.47 0.79 0.59
1 1 20% 0.66 0.48 0.80 0.61
1 1 40% 0.66 0.48 0.80 0.60
10 1 10% 0.63 0.46 0.78 0.58
10 1 20% 0.64 0.46 0.77 0.57
10 1 40% 0.64 0.46 0.78 0.58
setting, the supernet can be warmed up with a small portion of
training data to collectively initialize weights for all candidate ar-
chitectures. Then, within this framework, we propose SiGeo, a new
proxy proven to be increasingly effective when the candidate ar-
chitectures continue to warm up. The warm-up strategy and our
proposed proxy are justified by novel theoretical results that express
minimal achievable training loss and generalization error in terms
of gradient variance, Fisher-Rao norm, and gradient mean. Exten-
sive experimental results have demonstrated that SiGeo achieves
remarkable performance on three RecSys tasks (Criteo/Avazu/KDD-
2012) with significantly lower search costs. In addition, we also
validate SiGeo on various established NAS benchmarks (NASBench-
101/NASBench-201/NASBench-301).
References
[1]Mohamed S. Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D.
Lane. 2021. Zero-Cost Proxies for Lightweight NAS. In International Conference
on Learning Representations (ICLR).
[2]Shun-Ichi Amari. 1998. Natural gradient works efficiently in learning. Neural
computation 10, 2 (1998), 251–276.
[3]Kartikeya Bhardwaj, James Ward, Caleb Tung, Dibakar Gope, Lingchuan Meng,
Igor Fedorov, Alex Chalfin, Paul Whatmough, and Danny Loh. 2022. Restruc-
turable activation networks. arXiv preprint arXiv:2208.08562 (2022).
[4]Léon Bottou and Yann Le Cun. 2005. On-line learning for very large data sets.
Applied stochastic models in business and industry 21, 2 (2005), 137–151.
[5]Andrew Brock, Theo Lim, J.M. Ritchie, and Nick Weston. 2018. SMASH: One-Shot
Model Architecture Search through HyperNetworks. In International Conference
on Learning Representations.
[6]Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2020. Once
for All: Train One Network and Specialize it for Efficient Deployment. In Inter-
national Conference on Learning Representations.
[7]Han Cai, Ligeng Zhu, and Song Han. 2019. ProxylessNAS: Direct Neural Ar-
chitecture Search on Target Task and Hardware. In International Conference on
Learning Representations.
[8]Wuyang Chen, Xinyu Gong, and Zhangyang Wang. 2021. Neural Architecture
Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective.
InInternational Conference on Learning Representations.
[9]Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. 2019. Progressive differentiable
architecture search: Bridging the depth gap between search and evaluation. In
Proceedings of the IEEE/CVF international conference on computer vision. 1294–
1303.
[10] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. 2021. Fairnas: Rethinking evalua-
tion fairness of weight sharing neural architecture search. In Proceedings of the
IEEE/CVF International Conference on computer vision. 12239–12248.
[11] Xuanyi Dong and Yi Yang. 2019. Searching for a robust neural architecture in
four gpu hours. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 1761–1770.
[12] Xuanyi Dong and Yi Yang. 2020. NAS-Bench-201: Extending the Scope of Re-
producible Neural Architecture Search. In International Conference on Learning
4544KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
Representations (ICLR).
[13] Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang, and
Zhenguo Li. 2021. Transnas-bench-101: Improving transferability and generaliz-
ability of cross-task neural architecture search. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 5251–5260.
[14] Cong Fang, Zhouchen Lin, and Tong Zhang. 2019. Sharp analysis for nonconvex
sgd escaping from saddle points. In Conference on Learning Theory. PMLR, 1192–
1234.
[15] Igor Fedorov, Ramon Matas, Hokchhay Tann, Chuteng Zhou, Matthew Mattina,
and Paul Whatmough. 2022. UDC: Unified DNAS for Compressible TinyML
Models for Neural Processing Units. In Advances in Neural Information Processing
Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh
(Eds.), Vol. 35. Curran Associates, Inc., 18456–18471.
[16] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, and Yong Li. 2021. Progressive
Feature Interaction Search for Deep Sparse Network. In Advances in Neural
Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 392–403.
[17] Guillaume Garrigos and Robert M Gower. 2023. Handbook of convergence
theorems for (stochastic) gradient methods. arXiv preprint arXiv:2301.11235
(2023).
[18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.
InProceedings of the Twenty-Sixth International Joint Conference on Artificial
Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, Carles Sierra
(Ed.). ijcai.org, 1725–1731.
[19] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. Neural computation
9, 1 (1997), 1–42.
[20] Tangyu Jiang, Haodi Wang, and Rongfang Bie. 2023. MeCo: Zero-Shot NAS with
One Data and Single Forward Pass via Minimum Eigenvalue of Correlation. In
Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc.,
61020–61047.
[21] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. 2019. Universal statistics of
fisher information in deep neural networks: Mean field approach. In The 22nd
International Conference on Artificial Intelligence and Statistics. PMLR, 1032–1041.
[22] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. 2017. On Large-Batch Training for Deep Learning:
Generalization Gap and Sharp Minima. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net.
[23] Ravi Krishna, Aravind Kalaiah, Bichen Wu, Maxim Naumov, Dheevatsa Mudigere,
Misha Smelyanskiy, and Kurt Keutzer. 2021. Differentiable NAS Framework and
Application to Ads CTR Prediction. arXiv preprint arXiv:2110.14812 (2021).
[24] Arjun Krishnakumar, Colin White, Arber Zela, Renbo Tu, Mahmoud Safari, and
Frank Hutter. 2022. NAS-Bench-Suite-Zero: Accelerating Research on Zero
Cost Proxies. In Advances in Neural Information Processing Systems, S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran
Associates, Inc., 28037–28051.
[25] Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. 2012. A simpler approach
to obtaining an O (1/t) convergence rate for the projected stochastic subgradient
method. arXiv preprint arXiv:1212.2002 (2012).
[26] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. 2019. Snip: single-
Shot Network Pruning based on Connection sensitivity. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net.
[27] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang
Lin, and Xiaojun Chang. 2020. Block-wisely supervised neural architecture
search with knowledge distillation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 1989–1998.
[28] Guihong Li, Yuedong Yang, Kartikeya Bhardwaj, and Radu Marculescu. 2023.
ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients. In The
Eleventh International Conference on Learning Representations.
[29] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 1754–1763.
[30] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. 2019.
Fisher-rao metric, geometry, and complexity of neural networks. In The 22nd
international conference on artificial intelligence and statistics. PMLR, 888–896.
[31] Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao
Li, and Rong Jin. 2021. Zen-NAS: A Zero-Shot NAS for High-Performance Deep
Image Recognition. In 2021 IEEE/CVF International Conference on Computer Vision,
ICCV 2021.
[32] Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao
Li, and Rong Jin. 2021. Zen-nas: A zero-shot nas for high-performance image
recognition. In Proceedings of the IEEE/CVF International Conference on Computer
Vision. 347–356.[33] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable ar-
chitecture search. In International Conference on Learning Representations (ICLR).
[34] Vasco Lopes, Saeid Alirezazadeh, and Luís A Alexandre. 2021. Epe-nas: Effi-
cient performance estimation without training for neural architecture search. In
International Conference on Artificial Neural Networks. Springer, 552–563.
[35] James Martens. 2020. New insights and perspectives on the natural gradient
method. The Journal of Machine Learning Research 21, 1 (2020), 5776–5851.
[36] Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. 2021. Neural archi-
tecture search without training. In International Conference on Machine Learning.
PMLR, 7588–7598.
[37] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-
Jean Wu, Alisson G Azzolini, et al .2019. Deep learning recommendation model
for personalization and recommendation systems. arXiv preprint arXiv:1906.00091
(2019).
[38] Xuefei Ning, Changcheng Tang, Wenshuo Li, Zixuan Zhou, Shuang Liang,
Huazhong Yang, and Yu Wang. 2021. Evaluating Efficient Performance Esti-
mators of Neural Architectures. In Advances in Neural Information Processing
Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 12265–12277.
[39] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. 2019. Regular-
ized Evolution for Image Classifier Architecture Search. In Proceedings of the
Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innova-
tive Applications of Artificial Intelligence Conference and Ninth AAAI Sympo-
sium on Educational Advances in Artificial Intelligence (Honolulu, Hawaii, USA)
(AAAI’19/IAAI’19/EAAI’19). AAAI Press, Article 587, 10 pages.
[40] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Sue-
matsu, Jie Tan, Quoc V Le, and Alexey Kurakin. 2017. Large-scale evolution of
image classifiers. In International conference on machine learning . PMLR, 2902–
2911.
[41] Nicol N Schraudolph. 2002. Fast curvature matrix-vector products for second-
order gradient descent. Neural computation 14, 7 (2002), 1723–1738.
[42] David So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In Interna-
tional conference on machine learning. PMLR, 5877–5886.
[43] Qingquan Song, Dehua Cheng, Hanning Zhou, Jiyan Yang, Yuandong Tian,
and Xia Hu. 2020. Towards Automated Neural Interaction Discovery for Click-
Through Rate Prediction. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA)
(KDD ’20). Association for Computing Machinery, New York, NY, USA, 945–955.
[44] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-
Attentive Neural Networks. In Proceedings of the 28th ACM International Con-
ference on Information and Knowledge Management (Beijing, China) (CIKM ’19).
Association for Computing Machinery, New York, NY, USA, 1161–1170.
[45] Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi
Priyantha, Jie Liu, and Diana Marculescu. 2019. Single-path nas: Designing
hardware-efficient convnets in less than 4 hours. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. Springer, 481–497.
[46] Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen, and Mengjie
Zhang. 2019. Surrogate-assisted evolutionary deep learning using an end-to-end
random forest-based performance predictor. IEEE Transactions on Evolutionary
Computation 24, 2 (2019), 350–364.
[47] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. 2020.
Pruning neural networks without any data by iteratively conserving synaptic flow.
InAdvances in Neural Information Processing Systems, H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 6377–
6389.
[48] Valentin Thomas, Fabian Pedregosa, Bart Merriënboer, Pierre-Antoine Manzagol,
Yoshua Bengio, and Nicolas Le Roux. 2020. On the interplay between noise
and curvature and its effect on optimization and generalization. In International
Conference on Artificial Intelligence and Statistics. PMLR, 3503–3513.
[49] Jack Turner, Elliot J. Crowley, Michael O’Boyle, Amos Storkey, and Gavin Gray.
2020. BlockSwap: Fisher-guided Block Substitution for Network Compression
on a Budget. In International Conference on Learning Representations.
[50] Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020. Picking Winning Tickets
Before Training by Preserving Gradient Flow. In International Conference on
Learning Representations.
[51] Haibin Wang, Ce Ge, Hesen Chen, and Xiuyu Sun. 2023. PreNAS: Preferred One-
Shot Learning Towards Efficient Neural Architecture Search. In International
Conference on Machine Learning (ICML).
[52] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan,
and Song Han. 2020. HAT: Hardware-Aware Transformers for Efficient Natural
Language Processing. In Annual Conference of the Association for Computational
Linguistics.
[53] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the web conference 2021.
1785–1797.
4545SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape KDD ’24, August 25–29, 2024, Barcelona, Spain
[54] Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, and Jimin Liang.
2023. NPENAS: Neural Predictor Guided Evolution for Neural Architecture
Search. IEEE Transactions on Neural Networks and Learning Systems 34, 11 (2023),
8441–8455.
[55] Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, and Pieter-Jan Kin-
dermans. 2020. Neural predictor for neural architecture search. In European
Conference on computer vision. Springer, 660–676.
[56] Colin White, Mikhail Khodak, Renbo Tu, Shital Shah, Sébastien Bubeck, and
Debadeepta Dey. 2022. A deeper look at zero-cost proxies for lightweight nas.
ICLR Blog Track (2022).
[57] Lei Wu, Zhanxing Zhu, et al .2017. Towards understanding generalization of
deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239
(2017).
[58] Yi Xu, Rong Jin, and Tianbao Yang. 2018. First-order Stochastic Algorithms
for Escaping From Saddle Points in Almost Linear Time. In Advances in Neural
Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc.
[59] Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. 2018.
Hessian-based Analysis of Large Batch Training and Robustness to Adversaries.
InAdvances in Neural Information Processing Systems, S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran
Associates, Inc.
[60] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and
Frank Hutter. 2019. Nas-bench-101: Towards reproducible neural architecture
search. In International conference on machine learning. PMLR, 7105–7114.
[61] Arber Zela, Julien Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and
Frank Hutter. 2021. Surrogate NAS benchmarks: Going beyond the limited
search spaces of tabular NAS benchmarks. In International Conference on Learning
Representations.
[62] Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang
Xiong, Feng Yan, Hai Li, Yiran Chen, and Wei Wen. 2023. NASRec: Weight
Sharing Neural Architecture Search for Recommender Systems. In Proceedings
of the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23). Association for
Computing Machinery, New York, NY, USA, 1199–1207.
A Assumption Justification
The assumption A.1is proposed by [ 2] to show the Fisher efficiency
result. This assumption has been widely used by many studies
[21,48] to suggest that the model is powerful enough to capture
the training distribution at 𝜽=ˆ𝜽★. We emphasize that Assumption
A.1serves solely as a theoretical justification for substituting the
Hessian matrix with the Fisher Information Matrix. Even in cases
where this assumption fails to hold, the study by [ 48] has shown
that the matrices tend to converge as the model training progresses.
Our theoretical analysis relies on Assumption A.2, which is sup-
ported by recent theoretical work on SGD. Specifically, it posits
that after a warm-up phase using a limited dataset, the supernet’s
weights should avoid saddle points and approximate a local min-
imum [ 14,58]. Practically, this can be achieved by adding more
samples if needed to ensure the supernet is adequately warmed up.
In most deep learning literature, the almost everywhere differen-
tiability assumption A.3typically holds due to the need to calculate
the gradient. Lastly, A.4is the regularity condition that provides a
foundational basis for performing mathematical operations that rely
on the existence of the inverse of these matrices. This assumption
implies the convexity of the loss function in the space B.
B Proof of Proposition 1
Lemma 1. Assume A.4. Ifℓis differentiable, for any (𝒙,𝒚)∈X×Y ,
it holds
ℓ(𝜽1)≥ℓ(𝜽2)+⟨∇ℓ(𝜽2;𝒙,𝒚),𝜽1−𝜽2⟩,for all 𝜽1,𝜽2∈B
Proof. The positive definite Hessian in Assumption A.4directly
implies the convexity of loss function ℓwith respect to 𝜽for all
(𝒙,𝒚)∈X×Y . Then the conclusion immediately follow Garrigos
and Gower [17, Lemma 2.8]. □Proposition 1.Assume A.1-A.4. Consider{𝜽𝑘}𝑘∈Za sequence
generated by the first-order (SGD) algorithm (2), with a decreasing
sequence of stepsizes satisfying 𝜂𝑘>0. Let𝜎2
𝑘:=Var[∇ℓ(𝜽𝑘;D𝑘)]
denote the variance of sample gradient, where the variance is taken
over the joint distribution of all the training samples until the 𝑘-
th iteration (also known as filtration in stochastic approximation
literature). Then It holds that
E[L(¯𝜽𝑘)]−L( ˆ𝜽★)≤∥𝜽0−ˆ𝜽★∥
2Í𝑘−1
𝑡=0𝜂𝑡+1
2𝑘−1∑︁
𝑖=0𝜎2
𝑘
Proof. For a given weight 𝜽𝑘, the sample gradient estimate is
an unbiased estimate of expected gradient, i.e. E[∇ℓ(𝜽𝑘;D𝑘)|𝜽𝑘]=
E[1
|D𝑘|Í
(𝒙𝑛,𝒚𝑛)∈D 𝑘∇ℓ(𝜽𝑘;𝒙𝑛,𝒚𝑛)|𝜽𝑘]=∇L(𝜽𝑘)under some
mild conditions. We will note E𝑘[·]instead of E[·|𝜽𝑘], and Var𝑘[·]
instead of Var[·|𝜽𝑘]for simplicity. We have the gradient variance
Var𝑘[∇ℓ(𝜽𝑘;D𝑘)]=E𝑘
∥∇ℓ(𝜽𝑘;D𝑘)∥2
−∥E𝑘[∇ℓ(𝜽𝑘;D𝑘)]∥2
=E𝑘
∥∇ℓ(𝜽𝑘;D𝑘)∥2
. (7)
Let us start by analyzing the behavior of ∥𝜽𝑘−ˆ𝜽★∥2. By expand-
ing the squares, we obtain
∥𝜽𝑘+1−ˆ𝜽★∥=∥𝜽𝑘−𝜽★∥−2𝜂𝑘⟨∇ℓ(𝜽𝑘;D𝑘),𝜽𝑘−𝜽★⟩
+𝜂2
𝑘∥∇ℓ(𝜽𝑘;D𝑘)∥2
Hence, after taking the expectation conditioned on 𝜽𝑘, we can use
the convexity of ℓto obtain:
E𝑘h
∥𝜽𝑘+1−ˆ𝜽★∥i
=∥𝜽𝑘−ˆ𝜽★∥2−2𝜂𝑘⟨E𝑘[∇ℓ(𝜽𝑘;D𝑘)],𝜽𝑘−𝜽★⟩
+𝜂2
𝑘E𝑘
∥∇ℓ(𝜽𝑘;D𝑘)∥2
≤∥𝜽𝑘−ˆ𝜽★∥2+2𝜂𝑘⟨∇L( 𝜽𝑘),𝜽★−𝜽𝑘⟩
+𝜂2
𝑘E𝑘
∥∇ℓ(𝜽𝑘;D𝑘)∥2
≤∥𝜽𝑘−ˆ𝜽★∥2+2𝜂𝑘 L(𝜽★)−L( 𝜽𝑘)
+𝜂2
𝑘Var𝑘[∇ℓ(𝜽𝑘;D𝑘)] (8)
where the last equation holds due to Lemma 1 and Eq. (7). Rearrang-
ing and taking the full expectation of Eq. (8) over all past training
samples until iteration 𝑘, we have
2𝜂𝑘 E[L(𝜽𝑘)]−L( 𝜽★)≤Eh
∥𝜽𝑘−ˆ𝜽★∥i
−Eh
∥𝜽𝑘+1−ˆ𝜽★∥i
+𝜂2
𝑘E[Var𝑘[∇ℓ(𝜽𝑘;D𝑘)]] (9)
Due to law of total variance, it holds
𝜎2=Var[∇ℓ(𝜽𝑘;D𝑘)]
=E[Var[∇ℓ(𝜽𝑘;D𝑘|𝜽𝑘)]]+Var[E[∇ℓ(𝜽𝑘;D𝑘)|𝜽𝑘]]
≥E[Var𝑘[∇ℓ(𝜽𝑘;D𝑘)]].
Then Eq. (9) becomes
2𝜂𝑘 E[L(𝜽𝑘)]−L( 𝜽★)≤Eh
∥𝜽𝑘−ˆ𝜽★∥i
−Eh
∥𝜽𝑘+1−ˆ𝜽★∥i
+𝜂2
𝑘𝜎2
Summing over 𝑖=0,1,...,𝑘−1and using telescopic cancellation
gives
2𝑘−1∑︁
𝑖=0𝜂𝑖 E[L(𝜽𝑖)]−L( 𝜽★)≤∥𝜽0−ˆ𝜽★∥−Eh
∥𝜽𝑘−ˆ𝜽★∥i
+𝑘∑︁
𝑖=0𝜂2
𝑖𝜎2
4546KDD ’24, August 25–29, 2024, Barcelona, Spain Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, and Wei Wen
SinceEh
∥𝜽𝑘−ˆ𝜽★∥i
≥0, dividing both sides by 2Í𝑘−1
𝑡=0𝜂𝑡gives:
𝑘−1∑︁
𝑖=0𝜂𝑖Í𝑘−1
𝑡=0𝜂𝑡 E[L(𝜽𝑖)]−L( 𝜽★)≤∥𝜽0−ˆ𝜽★∥
2Í𝑘−1
𝑡=0𝜂𝑡+𝑘∑︁
𝑖=0𝜂2
𝑖
2Í𝑘−1
𝑡=0𝜂𝑡𝜎2
Define𝑝𝑘,𝑖:=𝜂2
𝑖Í𝑘−1
𝑡=0𝜂𝑡for𝑖=0,1,...,𝑘−1and observe that
𝑝𝑘,𝑖≥0andÍ𝑘−1
𝑖=0𝑝𝑘,𝑖=1. This allows us to treat the {𝑝𝑘,𝑖}as
probabilities. Then using the fact that ℓis convex together with
Jensen’s inequality gives
E[L(¯𝜽𝑘)]−L( ˆ𝜽★)≤𝑘−1∑︁
𝑖=0𝜂𝑖Í𝑘−1
𝑡=0𝜂𝑡 E[L(𝜽𝑖)]−L( 𝜽★)
≤∥𝜽0−ˆ𝜽★∥
2Í𝑘−1
𝑡=0𝜂𝑡+𝑘∑︁
𝑖=0𝜂2
𝑖
2Í𝑘−1
𝑡=0𝜂𝑡Var[∇ℓ(𝜽𝑖;D𝑖)]
≤∥𝜽0−ˆ𝜽★∥
2Í𝑘−1
𝑡=0𝜂𝑡+1
2𝑘−1∑︁
𝑖=0𝜎2
𝑘(10)
where the last inequality holds because 𝑝𝑘,𝑖≤1. □
C Proof of Theorem 1
Theorem 1Assume A.1-A.4. Let 𝜇𝑘=Í𝑘
𝑖=0E[∥∇ℓ(𝜽𝑖;D𝑖)∥1]de-
note the sum of the expected absolute value of gradient across mini-
batch samples, denoted by D𝑖, where the gradient norm is
∥∇ℓ(𝜽𝑖;D𝑖)∥1=𝑑∑︁
𝑗=1∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;D𝑖),
where
∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;D𝑖)=1
|D𝑖||D𝑖|∑︁
𝑛=1∇𝜃(𝑗)
𝑖ℓ(𝜽𝑖;𝒙𝑛,𝒚𝑛).
Under some regularity conditions such that Eh
(𝜽𝑘−ˆ𝜽★)3i
=𝑜(1
𝑘),
it holds
L(ˆ𝜽★)≥E[L(𝜽𝑘)]−1
2Eh
𝜽⊤
𝑘𝑭(ˆ𝜽★)𝜽𝑘i
−𝜂𝜇𝑘∥𝑯(ˆ𝜽★)ˆ𝜽★∥∞
−1
2(ˆ𝜽★−2𝜽0)⊤𝑯(ˆ𝜽★)ˆ𝜽★+𝑜1
𝑘
.
Proof. Notice that the optimality gap is 𝐺(𝜽𝑘):=E[L(𝜽𝑘)]−
L(ˆ𝜽★). By applying Taylor approximation up to second order in
Eq. (5) and taking expectation over the filtration, we have
𝐺(𝜽𝑘)=1
2Eh
𝜽⊤
𝑘𝑯(ˆ𝜽★)𝜽𝑘−2𝜽⊤
𝑘𝑯(ˆ𝜽★)ˆ𝜽★+𝜽★⊤𝑯(ˆ𝜽★)ˆ𝜽★i
+Eh
O
(𝜽𝑘−ˆ𝜽★)3i
=1
2Eh
𝜽⊤
𝑘𝑯(ˆ𝜽★)𝜽𝑘i
+𝜂E"𝑘∑︁
𝑖=0𝒈(𝜽𝑖)#⊤
𝑯(ˆ𝜽★)ˆ𝜽★
+1
2(𝜽★−2𝜽0)⊤𝑯(ˆ𝜽★)ˆ𝜽★+𝑜1
𝑘
.
Then by replacing Hessian with Fisher information matrix in the
first term, the optimality gap is bounded as follows
𝐺(𝜽𝑘)≤𝐸h
𝜽⊤
𝑘𝑭(ˆ𝜽★)𝜽𝑘i
2+𝜂E"𝑘∑︁
𝑖=0𝒈(𝜽𝑖)#⊤
𝑯(ˆ𝜽★)ˆ𝜽★
+(𝜽★−2𝜽0)⊤𝑯(ˆ𝜽★)ˆ𝜽★
2+𝑜1
𝑘
(11)By applying Hölder’s inequality and Minkowski’s Inequality, the
second term in Eq. (11) is
E"𝑘∑︁
𝑖=0𝒈(𝜽𝑖)#⊤
𝑯(ˆ𝜽★)ˆ𝜽★≤E"𝑘∑︁
𝑖=0𝒈(𝜽𝑖)
1#𝑯(ˆ𝜽★)ˆ𝜽★∞
≤𝑘∑︁
𝑖=0E
∥𝒈(𝜽𝑖)∥1𝑯(ˆ𝜽★)ˆ𝜽★∞.
Therefore, since 𝜇𝑘=Í𝑘
𝑖=0E[∥𝒈(𝜽𝑖)∥1]we have
𝐺(𝜽𝑘)≤1
2Eh
𝜽⊤
𝑘𝑭(ˆ𝜽★)𝜽𝑘i
+𝜂𝜇𝑘∥𝑯(ˆ𝜽★)ˆ𝜽★∥∞
+1
2(𝜽★−2𝜽0)⊤𝑯(ˆ𝜽★)ˆ𝜽★+𝑜1
𝑘
from which the conclusion immediately follows. □
D Implementation Details
D.1 Experiment (1): Justification for Theorem 1
We conducted training using a two-layer MLP equipped with ReLU
activation functions across the complete MNIST training dataset for
three epochs. The weights were updated through the application
of gradient descent as per Eq. (2), employing a batch size of 128.
Throughout training, these networks were optimized using the
SGD optimizer with a learning rate of 0.01. In the preprocessing
phase, all image tensors were normalized using a mean of (0.4914,
0.4822, 0.4465) and a standard deviation of (0.2023, 0.1994, 0.2010).
Then SiGeo scores were computed using four training batches after
the candidate architectures were trained on 0, 48, and 192 batches,
corresponding to 0%, 10%, 40% and 60% warm-up levels. Then we
visualize the relationship of the training loss and test loss after
three training epochs vs. (i) current training loss, (ii) FR norm and
(iii) mean absolute sample gradients in Figure 3.
D.2 Experiment (2) and (3): Comparing SiGeo
with Other Proxies on NAS Benchmarks
To calculate the correlations, we utilize the NAS-Bench-Suite-Zero
[24]. This suite encompasses a comprehensive set of commonly
used zero-shot proxies, complete with their official codebases re-
leased by the original authors, enabling us to accurately obtain
the proxy values. Since the ZiCo proxy was not incorporated into
NAS-Bench-Suite-Zero, we employed its official code to compute
the correlations for ZiCo. In the zero-shot setting, we adjust our
proxy by setting 𝜆2=1and𝜆3=0. If a warm-up procedure is
performed, we set we use 𝜆2=50and𝜆3=1.
D.3 Experiment (4): RecSys Benchmark Results
We employ regularized evolution, as detailed in [ 39], to identify the
optimal child subnet within the NASRec search space. The configu-
ration remains consistent with NASRec [ 62]. For comprehensive
details regarding the search space, search policy, and training strat-
egy, we direct readers to [62].
4547