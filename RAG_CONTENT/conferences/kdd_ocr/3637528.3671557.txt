Improving Ego-Cluster for Network Effect Measurement
Wentao Su
LinkedIn Corporation
Sunnyvale, CA, USA
wesu@linkedin.comWeitao Duan
LinkedIn Corporation
Sunnyvale, CA, USA
wduan@linkedin.com
Abstract
The network effect, wherein one user’s activity impacts another
user, is common in social network platforms. Many new features
in social networks are specifically designed to create a network
effect, enhancing user engagement. For instance, content creators
tend to produce more when their articles and posts receive positive
feedback from followers. This paper discusses a new cluster-level
experimentation methodology for measuring creator-side metrics
in the context of A/B experiments. It is a crucial part of LinkedIn’s
overall strategy to foster a robust creator community and ecosys-
tem. The method is developed based on widely-cited research at
LinkedIn but significantly improves the efficiency and flexibility of
the clustering algorithm. This improvement results in a stronger
capability for measuring creator-side metrics and an increased ve-
locity for creator-related experiments.
CCS Concepts
•General and reference →Experimentation; Empirical studies .
Keywords
A/B Testing, Clustering, Creator Measurement Metrics
ACM Reference Format:
Wentao Su and Weitao Duan. 2024. Improving Ego-Cluster for Network
Effect Measurement. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671557
1 Introduction
Randomized controlled experiments, or A/B tests, are the gold stan-
dard for evaluating the effect of new product features. In the tech-
nology industry, experimentation is adopted by many companies
to measure the impact of new features [ 4,14,15,24,28,34]. They
rely on the “Stable Unit Treatment Values Assumption” (SUTVA)
which states that treatment only affects treated users and does not
spill over to their friends.
Violations of SUTVA are not uncommon in features that ex-
hibit network effects. In social networks, such as Facebook [ 30] or
LinkedIn [ 29], connected users can interact with one another. If
user A and user B are connected, by changing the ranking of items
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671557on user A’s homepage feed, we impact their engagement with their
feed and indirectly change the items that appear on user B’s feed.
In marketplaces, such as Ads marketplace [25], delivery networks
[21], and ride hailing services [ 7], treatment units (marketers, dri-
vers, customers, etc) interfere with control units through market
competition.
The spillover of treatment effects between units violates SUTVA
and leads to inaccurate treatment effect estimates. For instance,
in social networks, it is not uncommon to observe low-quality
"viral" content having positive engagement effects on users but
ultimately having a negative overall impact. On the seller side of a
marketplace, if a treatment enhances the competitiveness of certain
sellers, unless all buyers still have an untapped budget to purchase
more, the increased appeal of treatment sellers results in relatively
less appeal for control sellers, causing control sales to decrease as
treatment sales increase.
As LinkedIn endeavors to cultivate a robust creator community
and ecosystem, it’s crucial that we comprehend the impact of the
features we develop for content creators and monitor our progress
in aligning with our vision for creators. For instance, when the
AI team constructs the feed recommendation model suggesting
creator content to users, we must assess not only the impact of
the AI model on viewer-side metrics, such as the number of viral
actions (likes, comments, and reshares), but also on creator-side
metrics like the creator’s retention rate, that is, how likely creators
are to continue generating in the next few days due to feedback
from their viewers, ensuring the evolution of a more dynamic feed
network.
Evaluating the impact on viewer-side metrics can be easily achieved
through traditional viewer-side A/B tests, wherein some viewers are
randomly assigned to the new AI model and others to the existing
model. However, measuring creator-side metrics poses challenges
due to the network effect [ 13,29] introduced by different units
of randomization and measurement. After a creator posts in the
feed, the feed ranking model positions the post in their viewers’
feeds. To test the new feed ranking model, the creator’s viewers
are randomly assigned to either the new model or the old model.
Nevertheless, attributing any change in the creator’s subsequent
actions to the new model becomes challenging, as about half of the
creator’s viewers are in treatment, and the other half are in control.
The spillover effect in the creator’s audience makes detecting any
lift in creator-side metrics under this experimental setup extremely
difficult. Therefore, a customized experimentation method is often
required to measure creator-side metrics in this social network.
2 Review on Controlled Experiments
In this section, we give a brief review on the evolution of controlled
experiment with a focus on network interference. The foundation
of experimentation was introduced by Sir Ronald A. Fisher at the
5713
KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Su and Weitao Duan
Rothamsted Agricultural Station in England in the 1920s with a fo-
cus on agriculture [ 6]. While the theory is simple, many researchers
have studied and extended Fisher’s work in textbooks and papers
[31] and controlled experiment has gained its popularity beyond
the original agricultural field [ 22,23]. Experiment practitioners
in many fields leverage the theory and conduct experiments to
evaluate new ideas [9, 32]. Deployment and analysis of controlled
experiments are done at large scale, presenting unique challenges
and pitfalls. Many researchers and experiment practitioners have
described the challenges, pitfalls and novel solutions [ 8,10,12,34].
One shared challenge, among many, is to measure the treatment
effect when interference exists among experimental units.
Before we dive into our solution to the interference problem, we
want to review the set up and notation for controlled experiments
and lay the foundation for the rest of paper.
Suppose we have one treatment feature 𝑇and one control expe-
rience𝐶, the metric of interest for user 𝑖is𝑌𝑖and the assignment
for user𝑖is𝑊𝑖, where
𝑊𝑖=(
1if user𝑖is in treatment group
0if user𝑖is in control group(1)
Following Rubin Causal Model or the potential outcome frame-
work set up [ 16,19,27], each unit’s potential outcome is defined as
a function of the entire assignment vector W∈{0,1}𝑁with𝑁of
units to treatment buckets: 𝑌𝑖(W).
The Average Treatment Effect (ATE) is defined as:
𝜇𝑌=1
𝑁𝑁∑︁
𝑖=1𝑌𝑖(WT)−1
𝑁𝑁∑︁
𝑖=𝑖𝑌𝑖(Wc) (2)
where WT=1,...,1andWC=0,...,0.
If Stable Unit Treatment Value Assumption (SUTVA) holds, the
realized outcome and the potential outcome have the following
relationship:
𝑌𝑖=(
𝑌𝑖(0)if𝑊𝑖=0
𝑌𝑖(1)if𝑊𝑖=1(3)
In a typical controlled experiment, suppose there are 𝑁𝐶units in
the control group and 𝑁𝑇units in the treatment group, the ATE is
given by the average difference between the treatment and control
group:
𝜇′
𝑌=1
𝑁𝑇∑︁
𝑖∈𝑇𝑌𝑖(1)−1
𝑁𝐶∑︁
𝑖∈𝐶𝑌𝑖(0) (4)
With interference, SUTVA no long holds and 𝜇′
𝑌is usually a bi-
ased estimator of 𝜇𝑌. So alternative experiment design and analysis
approaches needs to be adopted to properly measure the ATE.
3 Existing Solutions
To estimate the ATE in a social network, numerous researchers
have put forth various design and analysis methodologies. The
proposed solutions follow two major directions.
The first direction places a greater emphasis on experimental de-
signs [ 1,11,17,18,33]. It begins by creating clusters between users
in the network with the hope that most interference occurs within
the cluster rather than between them. Cluster-level randomization
is then utilized to estimate the ATE. Meta has adopted clustering
randomization in their network experiments [ 20]. At LinkedIn, weinitially attempted to directly cluster users and run A/B tests at
the cluster level [ 30]. However, due to the low number of clusters
and high interference between them, this approach did not yield
much success. Subsequently, we developed a more efficient cluster
algorithm, Ego Cluster v1 [ 29], which generates numerous small
clusters and measures the 1-hop (from viewers to creators) network
effect. Although a significant breakthrough for its model-agnostic
approach, we found that this tool encounters several issues that
impede the velocity of creator-related experiments. The algorithm
is a generic method demonstrating that clustering is an efficient
means of measuring network effects but it lacks certain flexibility.
For instance, to avoid clustering directly in a densely connected
graph, the tool clusters in an active member-only graph. Moreover,
it cannot target specific types of content creators. The network
type forming the clusters is hard-coded using the default network
based on the past 90 days’ feed impression count and is not easily
adaptable to other network types. Importantly, the tool requires
manual tuning of numerous network parameters, a time-consuming
process lasting several days. Finally, since LinkedIn typically has
multiple feed ranking models to be experimented concurrently, the
Ego Cluster experiment tends to absorb a significant proportion
of engaged members, leading to poor-quality leftover traffic and
biased estimates for other viewer measurement experiments [ 29],
although this bias has not been quantified in prior research.
The second direction leverages a model or surrogate metric to
estimate network interference based on the treatment assignment
of other users in the network [ 2,3,5]. [34] discussed a simple model
where the interference portion of ATE follows a linear relationship
with the number of treated friends. To reduce the cost of estimating
network effects, we have also tested a few downstream attribution
surrogate metrics in a traditional A/B test setting. These metrics
attempt to explicitly correlate a viewer-side action with a creator-
side impact by leveraging our internal tracking to link the two
together. One example is the creator love metric, which tracks
feedback from a specific viewer to a creator, allowing us to infer how
creators would respond to the feedback. While these attribution
metrics are easy to use, they are much less accurate compared to
the cluster-level method because they rely on strong modeling
assumptions. It is likely that the number of feedback from a viewer
to a creator is positively but not linearly related to the likelihood of
that creator creating more posts. However, quantifying this dynamic
relationship empirically proves challenging.
To accurately measure the network effect while simultaneously
enhancing the velocity of experiments, we have decided to intensify
our focus on the Ego Cluster idea but completely revamp the clus-
tering algorithm to make the tool scalable, efficient, and accurate
for creator measurement experiments.
4 A New Ego Cluster Tool
4.1 Ego Cluster Experimentation Process
Before delving into the details of our clustering algorithm, it is
worthwhile to review the general process of cluster-level experi-
mentation for content creator measurement at LinkedIn.
Figure 1 illustrates four major steps. The first step involves the
random and iterative assignment of egos (creators in the feed ex-
ample) to the treatment or control group. Each alter (viewers in the
5714Improving Ego-Cluster for Network Effect Measurement KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 1: cluster-level experimentation process
feed example) can be attached to one ego. If an alter can be attached
to multiple egos, it will be attached to the ego based on clustering
algorithm. Once clusters are created, alters automatically receive
the same treatment labels as the egos to which they are attached.
Following this, we reassign all egos to the control group and treat
the alters with treatment and control features in the experiment.
The final experiment readout involves comparing the metrics from
egos between the two variant groups. Since all egos receive the
control feature of the test, any statistical significance in the met-
rics can be attributed to the different levels of feedback from their
audiences between the treatment and control groups. The success
of the process relies on how well clusters are separated: the less
spillover across the clusters, the more robust the measurement will
be. The red line highlighted in first three steps is an example show-
ing one alter is algorithmically assigned to a control ego but is also
connected to a treatment ego, causing the spillover effect.
4.2 New Tool Architecture
The new Ego Cluster tool consolidates the network preparation,
clustering process, and diagnostic analysis modules, streamlining
the entire running process. Figure 2 shows the overall new tool
architecture design.
The network preparation relies on LinkedIn network relation-
ship database, defining six main types of network graphs1. The
creator dataset is also useful if users wish to create clusters based
on specific type of creators. The tool accepts a custom-defined
member list as the center of the cluster, as long as it generates
enough samples, ideally over 100k. Alongside the key quality met-
rics discussed below for selecting the best network type for cluster
formation, manual judgment is involved based on each business use.
For instance, to test the creator-side impact of a new feed relevance
model, the viral action or similar network type is recommended
because viral actions imposed by viewers often have significant
consequences on the retention rate of creators. For a notification
relevance model measurement, the impression type is preferred
as the notification network is generally broader, requiring a more
extensive definition of the network type. In the original version
1For every interaction between a viewer and a creator, impressions, clicks, likes,
reshares, comments, and viral actions are recorded.of Ego Cluster , the connection network type was also proposed,
but it faced challenges as the connection cluster was too loosely-
formed, leading to larger overlapping interference, coupled with
high implementation costs, as it is not conveniently defined in our
network relationship database.
The clustering process is the core part of the tool and is discussed
in section 4.3. Emphasizing the importance of feedback relation-
ships, we have designed an algorithm named "one-degree label
propagation" to capture the direct network bounding between egos
and alters, based on the original idea of label propagation [26].
The last module is the diagnostic analysis, where we measure
the quality of the clustering to ensure its robustness and qualifica-
tion for use in A/B tests. We define two types of metrics: network
stability rate and Ego Cluster loss rate, along with the remaining
traffic comparison analysis, enabling users to be aware of potential
measurement bias for other feed experiments conducted simultane-
ously.
Upon completion of the flow, the tool generates the member as-
signment list for use in Custom Selector on T-Rex, the LinkedIn A/B
test platform. Subsequently, testing output is generated, containing
creator metric measurements.
4.3 Clustering Algorithm
The most significant improvement in the latest version of Ego Clus-
ter, v2, lies in its clustering algorithm. It utilizes the one-degree label
propagation to generate ego-clusters, resulting in approximately
5 times more cluster samples while maintaining a similar cluster
quality.
As the network effect stems from viewer feedback, we believe
that how we assign viewers (alters) to specific creators (egos) is cru-
cial for creating robust clusters. The algorithm’s advantage over its
predecessor lies in its simplicity and time efficiency. The algorithm
utilizes the network structure to guide its progress and does not
optimize any specifically chosen measure of community strengths.
The algorithm involves four key steps:
(1)Randomly assign all egos to either the treatment or control
variant group.
(2) Obtain each alter’s network with their network weight.
5715KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Su and Weitao Duan
Figure 2: new tool architecture
(3)Assign the alter to the variant group based on the aggre-
gated weighted network count, with ties broken uniformly
randomly.
(4)Further assign the alter to one ego in the same variant group
based on its weight for clustering quality diagnostic.
Following the new algorithm, Figure 3 illustrates an example of
how we run the algorithm. This alter has viewed (or engaged based
on other network type actions) 5 egos — 3 treatment egos and 2
control egos. The alter will be assigned to the control variant group
because its total impression count is 11, exceeding the count of 6
for the treatment ego group. Subsequently, the alter will be further
assigned to the top ego among this two-ego control group, as it has
an impression count of 6, which is more than the count of 5 for the
other control ego. This process establishes a one-to-one map from
one alter to one ego, enabling us to group by egos and form the
ego-cluster solution for all eligible population.
Figure 3: alter assignment based on network aggregation
count
In Table 1 below, we discuss the comparison between Ego Cluster
V1 and V2. The new version of Ego Cluster generates about 5 times
more sample size, depending on the network types, without sacri-
ficing clustering quality. This enhancement significantly boosts thepower of the experiment. The primary reason for this improvement
is our ability to treat creators as the center of the cluster and resolve
conflicts of creator-sharing-viewers by aggregating network action
counts. Leveraging the existing labels of creators flexibly ensures
that most of the clusters generated from the tool can be utilized in
the A/B test.
Additionally, by focusing on building this one-to-one map from
alters to egos, we can implement Spark parallelism computation
instead of the sequential assignment used in V1, resulting in a 70%
reduction in the work flow running time. Moreover, by leveraging
a consistent programming language with strong scalability, we are
able to significantly reduce tool maintenance time.
While the new tool has seen significant improvement on multiple
fronts, the current tool relies on a few key assumptions:
(1)Low spillover: we assume that the measurement bias from
the Ego Cluster experiment is controlled at a reasonable level
during the experiment.
𝜇′
𝑌=1
𝑁𝑇∑︁
𝑎𝑙𝑡𝑒𝑟 𝑖∈𝑇𝑌𝑒𝑔𝑜(1)−1
𝑁𝐶∑︁
𝑎𝑙𝑡𝑒𝑟 𝑖∈𝐶𝑌𝑒𝑔𝑜(0) (5)
where we treat alter i in the randomized experiment and
measure the ATE for its connected ego’s metric Y. Bias will
always exist 𝜇𝑌−𝜇′
𝑌, the difference between real network
effect and what we obtain from the experiment, because al-
ters could still be connected to egos from different treatment
group. By improving clustering algorithm, we hope to reduce
this error to a reasonable level.
(2)Network stability: since the Ego Cluster solution is built
upon the historical network data and fixed during the A/B
test period, we assume that our network structure is rela-
tively stable so that network effect can be captured by the
experiment setup. In other words, we assume the evolving
network within a few weeks has relatively mild effect on the
variance of 𝜇𝑌−𝜇′
𝑌.
(3)Network predictability: the selected network type of Ego
Cluster can predict the impact of the new AI feed model. We
can fulfill this assumption by selecting the network type that
best reflects the impact of the new AI model.
4.4 Diagnostic Analysis
4.4.1 cluster quality metrics. The outputs of the tool consist of a
custom treatment assignment file required for the LinkedIn experi-
mentation platform for member triggering and diagnostic files for
users to select the best cluster solution based on multiple network
choices.
Two key metrics are employed to measure the quality of the
clustering: the loss rate metric and the stability rate metric. The
loss rate measures the spillover degree between clusters across
different variant groups. For each ego-cluster, it is calculated as the
ratio of the total number of weighted alters that are also connected
with other egos with different variant groups to the total number of
alters in this ego network. Aggregating each ego’s loss rate provides
the overall loss rate of the clusters. Applying (6), the loss rate in
ego A is 3/8 assuming all alters have the same impressions upon
egos in Figure 4, because only alter D, E and F can cause spillover
on Ego A since they are connected to egos from different variant
5716Improving Ego-Cluster for Network Effect Measurement KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparison between Ego Cluster V1 and V2
Improvement Area Version 1 Version 2
Code Maintenance Apache Pig + Java with Map-Reduce Spark
Scalability Sequentially fill up the sorted egos with
poor scalabilityParallel clustering and easily scalable
User Input Flexibility Does not accept pre-defined labels or
network typesFlexible network type and creator type
defined from LinkedIn network graph
Feed Traffic Occupation 15-30% Similar
Remaining Traffic Quality Lower than average traffic quality in
terms of connection and session countRemains low, but more LinkedIn mem-
bers improves testing power
Sample Size 200k 1M
Flow Running Time 8 hours 2 hours
Loss Rate (Quality of Cluster) 30% 20%
groups. See Supplement A for detailed proof on why this algorithm
minimizes the spillover of Ego Cluster solution.
Figure 4: an example for loss rate calculation
𝑙𝑜𝑠𝑠𝑟𝑎𝑡𝑒 =𝑡𝑜𝑡𝑎𝑙𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛𝑒𝑑𝑎𝑙𝑡𝑒𝑟𝑠
𝑡𝑜𝑡𝑎𝑙𝑎𝑙𝑡𝑒𝑟𝑠𝑖𝑛𝑒𝑎𝑐ℎ𝑒𝑔𝑜𝑛𝑒𝑡𝑤𝑜𝑟𝑘(6)
The loss rate, as defined earlier, specifically measures the spillover
degree atthe time of Ego Cluster creation, not during the experi-
ment when new members could continually join the network. To
address this issue, we introduce another loss rate definition that
covers the spillover degree in the next 14 days after the Ego Cluster
is created.
Throughout the experiment, there are two types of spillover
effects:
(1)The existing alters from the Ego Cluster solution have con-
nected to egos with a different treatment group.
(2)There are new alters that are not in the Ego Cluster solution
but connected to the egos.The first type of spillover effect is already captured in the formula
mentioned earlier. For the second type, since the new alters can be
assigned to any variant group during the experiment, assumptions
need to be made about the type of variant groups that new alters
will be assigned to during the test that generates the feedback
impact.
The worst-case scenario assumes that all newly joined alters
come from different variant groups besides the existing treatment
and control variants, and all alters’ weights are considered as "loss,"
making the loss rate the most conservative estimate. In other words,
all new alters are assumed to be actively polluting the experiments.
The second-scenario assumption is based on the fact that ego-
Cluster traffic accounts for about 10% of total active members in
the past experiments so that we assume newly joined alters have
about equal chance to be interacted with egos. In this case, we
assume 10% of these newly joined alters are assigned to the creator
measurement experiments, and the rest of the alters are most likely
to receive viewer measurement experiments, which is similar to
control variant. Based on this assignment, the spillover rate is then
calculated. This scenario is believed to be more proxy to the real
situation.
A third-scenario assumption is similar to the second-scenario
where we assume 10% of new alters that show up in the Ego Clus-
ter experiments are receiving Ego Cluster variants, either in the
treatment or control group, and the rest 90% are receiving viewer
measurement experiment variants. The difference is that we as-
sume this large proportion of alters is orthogonal to the Ego Cluster
experiment and has no spillover impact at all. This would be the
optimistic assumption.
The actual loss rate during the experiment should fall between
the most conservative and optimistic scenarios, likely to be close
to the second scenario.
𝑙𝑜𝑠𝑠𝑟𝑎𝑡𝑒 𝑓𝑜𝑟 14𝑑𝑎𝑦𝑠=
𝑒𝑥𝑖𝑠𝑡𝑖𝑛𝑔𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛𝑒𝑑𝑎𝑙𝑡𝑒𝑟𝑠 +𝑛𝑒𝑤𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛𝑒𝑑𝑎𝑙𝑡𝑒𝑟𝑠
𝑡𝑜𝑡𝑎𝑙𝑎𝑙𝑡𝑒𝑟𝑠𝑖𝑛𝑒𝑎𝑐ℎ𝑒𝑔𝑜𝑛𝑒𝑡𝑤𝑜𝑟𝑘
5717KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Su and Weitao Duan
Table 2: Loss rate definition for newly joined alters
Scenario Definition
scenario 1 - most conservative Assume all alters come from other variant groups that cause total spillover
effect to the Ego Clusters
scenario 2 - most close to real experiment Assume Ego Cluster ramps 10% of feed traffic, meaning 10% of new alters
will receive treatment or control variant of Ego Cluster experiment and
the other 90% will receive other feed ranking model variants, most likely
control variants
scenario 3 - optimistic estimate Same assignment assumption as scenario 2, but assume that 90% of the
traffic assigned to viewer measurement experiments is orthogonal to Ego
Cluster experiment and there is no spillover effect at all. This is a very
optimistic estimate.
where there are different calculations of the spillover for “new
misaligned alters” across clusters based on three scenarios men-
tioned above (see Table 4).
We will obtain three values for the loss rate over the next 14 days,
providing a range with upper and lower bounds for the spillover
effect. As mentioned earlier, the actual spillover effect is likely to
be closer to the second scenario, based on the calculation of traffic
assignment. This metric is important to gauge whether our Ego
Cluster solution meets the assumption of a "low spillover".
Another quality metric, stability rate, measures the degree of
stability by which the ego network endures. While our cluster solu-
tion remains consistent over a 2-3 week experiment, the network
undergoes dynamic evolution during the same period. It is crucial
that we select a network type with a relatively stable clustering
solution. This metric works as robustness check to make sure the
network stability assumption is satisfied.
𝑠𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦𝑟𝑎𝑡𝑒 =𝑡𝑜𝑡𝑎𝑙𝑇 0𝑒𝑑𝑔𝑒𝑠𝑡ℎ𝑎𝑡𝑠𝑡𝑖𝑙𝑙𝑒𝑥𝑖𝑠𝑡𝑖𝑛𝑇 14
𝑡𝑜𝑡𝑎𝑙𝑇 0𝑒𝑑𝑔𝑒𝑠𝑖𝑛𝑒𝑔𝑜𝑛𝑒𝑡𝑤𝑜𝑟𝑘(7)
4.4.2 network type summary table. Table 3 shows an example of
output for each network type solution from Ego Cluster v2.
We focus on two main types of network types: impression and
viral actions. With each network type, we have different aggrega-
tions of the network count based on the past 28, 45, and 90 days of
member interactions. So, in the end, we have 6 final Ego Cluster
solutions. In this example, using the past 90 days of viral action
interactions as the network type offers the best overall clusters.
It generates about 1.1M cluster samples with a 17.4% loss rate at
T0—the spillover rate when Ego Cluster is created. Additionally,
we calculate what the spillover degree would be given there will be
new interactions formed across clusters for the next 14 days in a
real A/B experiment environment. The loss rate at T14 in scenario
2 is at 22%, indicating that despite the network is evolving with
more interactions between members, the overall spillover degree is
manageable.4.5 Viewer-side Experiment Measurement
Correction
4.5.1 Ego Cluster leftover traffic quality. As our creator ecosystem
is an interconnected network where we conduct both creator mea-
surement experiments and viewer measurement experiments, the
use of the Ego Cluster tool for creator measurement experiments
inevitably absorbs some highly engaged traffic. This is because
active individuals in the network are more likely to be included in
our clustering solution. Consequently, this reduction in traffic for
viewer measurement experiments, often conducted concurrently,
results in what we term "Ego Cluster leftover traffic." This leftover
traffic, composed of less engaged users, can cause measurement
bias for viewer measurement experiments, as it does not accurately
represent the overall user population.
Figure 6 and 7 compare the traffic quality between the Ego Clus-
ter traffic and the leftover traffic in terms of user sessions and net-
work connection count. There is a noticeable drop in quality for the
remaining traffic as Ego Clusters absorb the most engaged members.
Therefore, the issue that shows up in previous Ego Cluster solution
persists in the new tool for feed-related experiments. For other AI
models, the extent of the issue depends on the scope of the total
experiment population and the quality and size of the Ego Cluster
sample. We are proposing a more robust method for correcting
metric measurements in other viewer measurement experiments
on leftover traffic.
4.5.2 model-free bias correction method. To correct the measure-
ment bias on the viewer measurement experiment, we design the
following setup illustrated in Figure 5. Once we create the Ego Clus-
ter traffic, including both egos and alters, we set aside a reserved
population p𝑅and put it into the viewer measurement experiment
so that the eligible traffic for viewer measurement experiments now
becomes p𝑅+ p1.
We define𝑛𝑅as the total reserved population, 𝑛𝐸as the total Ego
Cluster traffic, and 𝑛1as the total Ego Cluster leftover traffic, which
now also includes the reserved part 𝑛𝑅. We have the following de-
rived formulas to calculate the bias-corrected metric measurement
by including the "corrected" weights on metric performance.
5718Improving Ego-Cluster for Network Effect Measurement KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: the main cluster result for each network type
network type cluster size loss rate at T0 loss rate at T14
- scenario 1loss rate at T14
- scenario 2loss rate at T14
- scenario 3
past 28 days impressions 1,457,513 35.3% 38.5% 32.8% 27.0%
past 45 days impressions 1,569,492 36.7% 36.1% 31.4% 26.7%
past 90 days impressions 1,713,031 38.7% 33.9% 30.1% 26.3%
past 28 days viral actions 576,519 11.1% 49.3% 29.6% 9.9%
past 45 days viral actions 759,877 13.4% 41.2% 26.1% 10.9%
past 90 days viral actions 1,081,143 17.4% 32.0% 22.0% 12.0%
Figure 5: Ego Cluster leftover traffic bias correction method
setup
𝑀𝑒𝑎𝑛𝑇=𝑀𝑒𝑎𝑛(𝑟𝑒𝑠𝑒𝑟𝑣𝑒)𝑇∗𝑛𝐸
𝑛𝐸+𝑛1+
𝑀𝑒𝑎𝑛(𝑙𝑒𝑓𝑡𝑜𝑣𝑒𝑟)𝑇∗𝑛1
𝑛𝐸+𝑛1(8)
𝑀𝑒𝑎𝑛𝐶=𝑀𝑒𝑎𝑛(𝑟𝑒𝑠𝑒𝑟𝑣𝑒)𝐶∗𝑛𝐸
𝑛𝐸+𝑛1+
𝑀𝑒𝑎𝑛(𝑙𝑒𝑓𝑡𝑜𝑣𝑒𝑟)𝐶∗𝑛1
𝑛𝐸+𝑛1(9)
Δ%=(𝑀𝑒𝑎𝑛𝑇−𝑀𝑒𝑎𝑛𝐶)/𝑀𝑒𝑎𝑛𝐶 (10)
𝑉𝑎𝑟𝑇=𝑉𝑎𝑟(𝑟𝑒𝑠𝑒𝑟𝑣𝑒)𝑇∗(𝑛𝐸
𝑛𝐸+𝑛1)2+
𝑉𝑎𝑟(𝑙𝑒𝑓𝑡𝑜𝑣𝑒𝑟)𝑇∗(𝑛1
𝑛𝐸+𝑛1)2(11)
𝑉𝑎𝑟𝐶=𝑉𝑎𝑟(𝑟𝑒𝑠𝑒𝑟𝑣𝑒)𝐶∗(𝑛𝐸
𝑛𝐸+𝑛1)2+
𝑉𝑎𝑟(𝑙𝑒𝑓𝑡𝑜𝑣𝑒𝑟)𝐶∗(𝑛1
𝑛𝐸+𝑛1)2(12)𝑉𝑎𝑟Δ%=𝑉𝑎𝑟𝑇
𝑀𝑒𝑎𝑛2
𝐶∗𝑛𝑇+𝑀𝑒𝑎𝑛2
𝑇∗𝑉𝑎𝑟𝐶
𝑀𝑒𝑎𝑛4
𝐶∗𝑛𝐶(13)
Through the metric lift change and the variance of the metric lift
change, we can then calculate the p-value based on the classic two-
sample t-test formula. To validate the effectiveness of the method,
we conducted a back test using past feed AI model experiments.
Since this experiment is already completed, and we know the true
north minimum detectable effect (MDE) of key metrics, we ran
the Ego Cluster tool to create Ego Cluster traffic and its leftover
traffic among the LinkedIn feed population. We then applied the
bias correction method to re-calculate the MDE as if we were to
run this experiment on leftover plus reserve traffic only.
The result shows that the MDE of Daily Content Seeker increases
from 0.13% to 0.19%, with the relative lift approximately the same,
showing the unbiasedness of this method. The other metric, Unique
Content Creator, shows similar results. This offline analysis demon-
strates that while the MDE increases due to a smaller sample size,
the estimate is unbiased as a result of the method correction.
Figure 6: user session # distributional difference between Ego
Cluster and its leftover traffic
5 Application and Results
After the tool was deployed to production, we utilized it to assess
the impact of a new feed ranking model on the content creator’s
5719KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Su and Weitao Duan
Table 4: bias correction method result
Unique Content Viewer
MDE bias-corrected MDE Relative lift Bias-corrected lift
0.13% 0.19% 0.21% 0.22%
Unique Content Creator
MDE bias-corrected MDE Relative lift Bias-corrected lift
0.57% 0.97% 0.56% 0.58%
Figure 7: connection # distributional difference between Ego
Cluster and its leftover traffic
retention rate. The objective of the new AI model was to enhance
the creator’s retention rate by prioritizing their posts that would
receive more feedback from their viewers. We selected the past 45
days’ viral actions as the network type to construct the Ego Clusters,
resulting in 1.6M cluster samples. Subsequently, we assigned half
of the clusters’ alters to the new AI model, while the other half
used the old model. After running the experiment for 3 weeks, we
compared the egos in the treatment group with their control group
counterparts.
The results indicated that the creator love metric, a surrogacy
metric measuring actions previously found to be correlated with
future creator retention, showed a 5% lift with statistical signifi-
cance. Another noteworthy outcome was the increase of 0.7% in
the number of creator moders, a type of LinkedIn content creators,
signifying that the positive network effect stimulates members to
actively contribute to the social network. However, a few other
metrics, such as unique content creator, remained neutral. The ex-
periment demonstrates solid evidence that the new model encour-
ages feedback from viewers, and this feedback motivates creators
to continue producing content. Unfortunately, due to traffic com-
petition from other experiments, we had to halt the experiment
after 3 weeks. A more prolonged ramp-up or a larger sample size
accumulation may reveal a stronger signal.
6 Conclusion
This paper introduces an enhanced Ego Cluster algorithm designed
to measure creator-side metrics in the context of social networks.Proven as an effective method for network effect measurement in
A/B experiments, the new version of Ego Cluster utilizes one-degree
label propagation to significantly increase the sample size by more
than 5 times, accompanied by a substantial reduction in manual
model tuning. Moreover, it can adapt to different network types,
providing diverse clustering options. The implementation of this
algorithm greatly enhanced the efficiency of existing cluster-level
experiments which assess the impact of new creator-related AI
models at LinkedIn.
Several areas for future research and improvement are identi-
fied. The first area involves exploring ways to make clustering
more robust with less spillover effect during A/B experiments. Em-
phasis can be placed on enhancing the longevity of Ego Cluster
solution in this dynamically evolving network. The second area
focuses on the Ego Cluster leftover traffic for other viewer mea-
surement experiments. Studying methodologies to customize the
algorithm for optimizing traffic between viewer measurement and
creator measurement experiments could help minimize the impact
of measurement bias when simultaneously conducting both types
of experiments. The third area is the derivation of "site-wide" im-
pact from any Ego Cluster experiment so that we not only know
the impact of new feature from Ego Cluster traffic but how the
company’s bottom line going to change once this feature is rolled
out universally.
Acknowledgments
We thank our researchers and scientists in Data Science Applied
Research and Flagship Data Science for their feedback.
References
[1]Peter M Aronow and Joel A Middleton. 2013. A class of unbiased estimators
of the average treatment effect in randomized experiments. Journal of Causal
Inference 1, 1 (2013), 135–154.
[2]Peter M Aronow and Cyrus Samii. 2017. Estimating average causal effects under
general interference, with application to a social network experiment. (2017).
[3]Susan Athey, Dean Eckles, and Guido W Imbens. 2018. Exact p-values for network
interference. J. Amer. Statist. Assoc. 113, 521 (2018), 230–240.
[4]Eytan Bakshy, Dean Eckles, and Michael S. Bernstein. 2014. Designing and
deploying online field experiments. In Proceedings of the 23rd international con-
ference on World wide web (WWW ’14). Association for Computing Machinery,
Seoul, Korea, 283–292. https://doi.org/10.1145/2566486.2567967
[5]Guillaume W Basse and Edoardo M Airoldi. 2018. Model-assisted design of
experiments in the presence of network-correlated outcomes. Biometrika 105, 4
(2018), 849–858.
[6]Joan Fisher Box. 1980. R.A. Fisher and the Design of Experiments, 1922–1926.
The American Statistician 34, 1 (Feb. 1980), 1–7. https://doi.org/10.1080/00031305.
1980.10482701
[7]Nicholas Chamandy. 2016. Experimentation in a Ridesharing Marketplace. https:
//eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e
5720Improving Ego-Cluster for Network Effect Measurement KDD ’24, August 25–29, 2024, Barcelona, Spain
[8]Nanyu Chen, Min Liu, and Ya Xu. 2018. Automatic Detection and Diagnosis of
Biased Online Experiments. arXiv:1808.00114 [stat] (July 2018). http://arxiv.org/
abs/1808.00114 arXiv: 1808.00114.
[9]Alex Deng, Ya Xu, Ron Kohavi, and Toby Walker. 2013. Improving the sensitivity
of online controlled experiments by utilizing pre-experiment data. In Proceedings
of the sixth ACM international conference on Web search and data mining - WSDM
’13. ACM Press, Rome, Italy, 123. https://doi.org/10.1145/2433396.2433413
[10] Drew Dimmery, Eytan Bakshy, and Jasjeet Sekhon. 2019. Shrinkage Estimators
in Online Experiments. Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining - KDD ’19 (2019), 2914–2922.
https://doi.org/10.1145/3292500.3330771 arXiv: 1904.12918.
[11] Dean Eckles, Brian Karrer, and Johan Ugander. 2017. Design and analysis of
experiments in networks: Reducing bias from interference. Journal of Causal
Inference 5, 1 (2017), 20150021.
[12] Aleksander Fabijan, Jayant Gupchup, Somit Gupta, Jeff Omhover, Wen Qin, Lukas
Vermeer, and Pavel Dmitriev. 2019. Diagnosing Sample Ratio Mismatch in Online
Controlled Experiments: A Taxonomy and Rules of Thumb for Practitioners.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining - KDD ’19. ACM Press, Anchorage, AK, USA, 2156–2164.
https://doi.org/10.1145/3292500.3330722
[13] Huan Gui, Ya Xu, Anmol Bhasin, and Jiawei Han. 2015. Network A/B Testing:
From Sampling to Estimation. In Proceedings of the 24th International Conference
on World Wide Web (Florence, Italy) (WWW ’15). International World Wide Web
Conferences Steering Committee, Republic and Canton of Geneva, CHE, 399–409.
https://doi.org/10.1145/2736277.2741081
[14] Somit Gupta, Ronny Kohavi, Diane Tang, Ya Xu, Reid Andersen, Eytan Bakshy,
Niall Cardin, Sumita Chandran, Nanyu Chen, Dominic Coey, Mike Curtis, Alex
Deng, Weitao Duan, Peter Forbes, Brian Frasca, Tommy Guy, Guido W. Imbens,
Guillaume Saint Jacques, Pranav Kantawala, Ilya Katsev, Moshe Katzwer, Mikael
Konutgan, Elena Kunakova, Minyong Lee, MJ Lee, Joseph Liu, James McQueen,
Amir Najmi, Brent Smith, Vivek Trehan, Lukas Vermeer, Toby Walker, Jeffrey
Wong, and Igor Yashkov. 2019. Top Challenges from the first Practical Online
Controlled Experiments Summit. ACM SIGKDD Explorations Newsletter 21, 1
(May 2019), 20–35. https://doi.org/10.1145/3331651.3331655
[15] Henning Hohnhold, Deirdre O’Brien, and Diane Tang. 2015. Focusing on the
Long-term: It’s Good for Users and Business. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD
’15. ACM Press, Sydney, NSW, Australia, 1849–1858. https://doi.org/10.1145/
2783258.2788583
[16] Paul W Holland. 1986. Statistics and causal inference. Journal of the American
statistical Association 81, 396 (1986), 945–960.
[17] David Holtz, Ruben Lobel, Inessa Liskovich, and Sinan Aral. 2020. Reducing
interference bias in online marketplace pricing experiments. arXiv preprint
arXiv:2004.12489 (2020).
[18] Michael G Hudgens and M Elizabeth Halloran. 2008. Toward causal inference
with interference. J. Amer. Statist. Assoc. 103, 482 (2008), 832–842.
[19] Guido W Imbens and Donald B Rubin. 2015. Causal inference in statistics, social,
and biomedical sciences. Cambridge University Press.
[20] Brian Karrer, Liang Shi, Monica Bhole, Matt Goldman, Tyrone Palmer, Charlie
Gelman, Mikael Konutgan, and Feng Sun. 2021. Network Experimentation at
Scale. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining (Virtual Event, Singapore) (KDD ’21). Association for Computing
Machinery, New York, NY, USA, 3106–3116. https://doi.org/10.1145/3447548.
3467091
[21] David Kastelman and Raghav Ramesh. [n.d.]. Switchback Tests and Random-
ized Experimentation Under Network Effects at DoorDash | by DoorDash |
Medium. https://medium.com/@DoorDash/switchback-tests-and-randomized-
experimentation-under-network-effects-at-doordash-f1d938ab7c2a
[22] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.
2013. Online controlled experiments at large scale. In Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining - KDD
’13. ACM Press, Chicago, Illinois, USA, 1168. https://doi.org/10.1145/2487575.
2488217
[23] Ron Kohavi, Alex Deng, Roger Longbotham, and Ya Xu. 2014. Seven rules
of thumb for web site experimenters. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining - KDD ’14. ACM
Press, New York, New York, USA, 1857–1866. https://doi.org/10.1145/2623330.
2623341
[24] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M. Henne. 2009.
Controlled experiments on the web: survey and practical guide. Data Mining and
Knowledge Discovery 18, 1 (Feb. 2009), 140–181. https://doi.org/10.1007/s10618-
008-0114-1
[25] Min Liu, Jialiang Mao, and Kang Kang. 2021. Trustworthy and Powerful Online
Marketplace Experimentation with Budget-split Design. In Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. ACM,
Virtual Event Singapore, 3319–3329. https://doi.org/10.1145/3447548.3467193
[26] Usha Nandini Raghavan, Ré ka Albert, and Soundar Kumara. 2007. Near linear
time algorithm to detect community structures in large-scale networks. PhysicalReview E 76, 3 (sep 2007). https://doi.org/10.1103/physreve.76.036106
[27] Donald B. Rubin. 1974. Estimating causal effects of treatments in randomized and
nonrandomized studies. Journal of Educational Psychology 66, 5 (1974), 688–701.
https://doi.org/10.1037/h0037350
[28] Dmitriy Ryaboy. 2015. Twitter experimentation: technical overview.
https://blog.twitter.com/engineering/en_us/a/2015/twitter-experimentation-
technical-overview.html
[29] Guillaume Saint-Jacques, Maneesh Varshney, Jeremy Simpson, and Ya Xu. 2019.
Using Ego-Clusters to Measure Network Effects at LinkedIn. https://doi.org/10.
48550/ARXIV.1903.08755
[30] Martin Saveski, Jean Pouget-Abadie, Guillaume Saint-Jacques, Weitao Duan,
Souvik Ghosh, Ya Xu, and Edoardo M. Airoldi. 2017. Detecting Network Effects:
Randomizing Over Randomized Experiments. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
’17). Association for Computing Machinery, New York, NY, USA, 1027–1035.
https://doi.org/10.1145/3097983.3098192
[31] Ajit C. Tamhane. 2009. Statistical Analysis of Designed Experiments: Theory and
Applications. John Wiley & Sons. Google-Books-ID: 5fHm_OWUQmQC.
[32] Diane Tang, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. 2010. Overlap-
ping experiment infrastructure: More, better, faster experimentation. In Proceed-
ings of the 16th ACM SIGKDD international conference on Knowledge discovery
and data mining. 17–26.
[33] Johan Ugander, Brian Karrer, Lars Backstrom, and Jon Kleinberg. 2013. Graph
cluster randomization: network exposure to multiple universes. In Proceedings of
the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (Chicago, Illinois, USA) (KDD ’13). Association for Computing Machinery,
New York, NY, USA, 329–337. https://doi.org/10.1145/2487575.2487695
[34] Ya Xu, Nanyu Chen, Addrian Fernandez, Omar Sinno, and Anmol Bhasin. 2015.
From Infrastructure to Culture: A/B Testing Challenges in Large Scale Social
Networks. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining - KDD ’15. ACM Press, Sydney, NSW,
Australia, 2227–2236. https://doi.org/10.1145/2783258.2788602
A Proof
Lemma A.1. Given a directed social network graph G, the algo-
rithm discussed in section 4.3 will minimize the spillover effect for
cluster creations in an A/B experiment.
Proof. Assume the graph G = (V, E) where 𝑉is the ego as vertex
and𝐸is the network edge from alter to ego. Corresponding to
Ego Cluster solution, each individual ego i is denoted as 𝑣𝑖and
each individual alter j is denoted as 𝑎𝑗. The network action from
alter j to ego i, the edge value, is denoted as 𝑒𝑗,𝑖.𝑒𝑗,𝑖is zero when
there is no network action occurred between alter j and ego i. The
proposed algorithm aggregates the edge values from each alter
to its connected egos based on ego’s treatment or control variant
group and assign the variant label to whichever variant that has
higher edge aggregation value.
alter j variant is
𝑎𝑙𝑡𝑒𝑟 𝑗𝑣𝑎𝑟𝑖𝑎𝑛𝑡 ={𝑣𝑎𝑟𝑖𝑎𝑛𝑡𝐴 :∑︁
𝑖∈𝑣𝑎𝑟𝑖𝑎𝑛𝑡 𝐴𝑒𝑗,𝑖>∑︁
𝑖∈𝑣𝑎𝑟𝑖𝑎𝑛𝑡 𝐵𝑒𝑗,𝑖}
(14)
loss rate of an ego from t is
𝐿𝑜𝑠𝑠𝑅𝑎𝑡𝑒𝑖∈𝑡=Í
𝑗∈𝑐𝑒𝑗,𝑖Í
𝑖∈𝑐𝑒𝑗,𝑖+Í
𝑖∈𝑡𝑒𝑗,𝑖(15)
where t refers to treatment variant and c refers to control variant.
As shown in Figure 4, the loss rate for an ego is the ratio of the
total edge value of misaligned alters assigned to different variant
ego to the total edge value of the ego. And the final loss rate of the
Ego Cluster solution is the simple average of loss rate of all egos.
𝐿𝑜𝑠𝑠𝑅𝑎𝑡𝑒𝑒𝑔𝑜𝐶𝑙𝑢𝑠𝑡𝑒𝑟 =Í
𝑖∈𝑐,𝑡𝐿𝑜𝑠𝑠𝑅𝑎𝑡𝑒𝑖
𝑁𝑐+𝑁𝑡(16)
Without loss of generality, assume alter k, who would be as-
signed treatment variant group based on the proposed algorithm,
5721KDD ’24, August 25–29, 2024, Barcelona, Spain Wentao Su and Weitao Duan
is now assigned as control variant group. Since we know that the
aggregated edge value towards treatment egos from the alter k is
larger, we have∑︁
𝑖∈𝑐𝑒𝑘,𝑖<∑︁
𝑖∈𝑡𝑒𝑘,𝑖 (17)
From the loss rate calculation (15), the total loss that the alter
k makes would beÍ
𝑖∈𝑐𝑒𝑘,𝑖if it is assigned to treatment group,
because that is the total negative impact that alter k will make
towards the different variant egos. The loss will increase toÍ
𝑖∈𝑡𝑒𝑘,𝑖
if alter k is assigned to control group. With all other condition
unchanged, the overall Ego Cluster loss rate will increase due to
overall higher loss impact from the new alter assignment.
The conclusion is that any other treatment assignment of alters
from the current proposal would increase the total loss in the Ego
Cluster system.
□
B Pseudocode of clustering algorithm
(1) Obtain LinkedIn Active Member Network Data with prede-
termined candidate egos ( 𝑉𝑖). For example, egos can be all
users who post more than 1 article in the past 90 days.(2)Randomly assign egos ( 𝑉𝑖) into Treatment (T) and Control
(C) groups
(3) For every alter ( 𝐴𝑗)
-For every connected ego/alter pair, compute weighted
network count:
w(𝐴𝑗,𝑉𝑖, T) if𝐴𝑗, and𝑉𝑖are connected, and 𝑉𝑖is in treatment
w(𝐴𝑗,𝑉𝑖, C) if𝐴𝑗, and𝑉𝑖are connected, and 𝑉𝑖is in control
-Aggregate weighted network count with connected egos
by treatment assignment:
𝑊(𝐴𝑗,𝑇)=Í
𝑎𝑙𝑙𝑉 𝑖∈𝑇𝑤(𝐴𝑗,𝑉𝑖,𝑇)
𝑊(𝐴𝑗,𝐶)=Í
𝑎𝑙𝑙 𝑉 𝑖∈𝐶𝑤(𝐴𝑗,𝑉𝑖,𝐶)
-Treatment assignment of 𝐴𝑗= argmax W( 𝐴𝑗, X), where
X = T or C. This assignment will be used in A/B test.
-Ego assignment of 𝐴𝑗= argmax W( 𝐴𝑗,𝑉𝑖, X), where X
is determined in above step. This will be used for diagnostic
analysis.
(4)By now, we generate a mapping relationship from each alter
(𝐴𝑗) to each ego ( 𝑉𝑖) and ego-centered cluster can be created.
5722