DARE to Diversify: DAta Driven and Diverse LLM REd Teaming
Manish Nagireddy
IBM Research
Cambridge, MA, United States
manish.nagireddy@ibm.comBernat Guillén Pegueroles
Google
Zurich, Switzerland
bernatgp@google.comIoana Baldini
IBM Research
NY, United States
ioana@us.ibm.com
ABSTRACT
Large language models (LLMs) have been rapidly adopted, as show-
cased by ChatGPT’s overnight popularity, and are integrated in
products used by millions of people every day, such as search en-
gines and productivity suites. Yet the societal impact of LLMs, en-
compassing both benefits and harms, is not well understood. In-
spired by cybersecurity practices, red-teaming is emerging as a
technique to uncover model vulnerabilities. Despite increasing at-
tention from industry, academia, and government centered around
red-teaming LLMs, such efforts are still limited in the diversity of
the red-teaming focus, approaches and participants. Importantly,
given that LLMs are becoming ubiquitous, it is imperative that
red-teaming efforts are scaled out to include large segments of the
research, practitioners and the people whom are directly affected by
the deployment of these systems. The goal of this tutorial is two fold.
First, we introduce the topic of LLM red-teaming by reviewing the
state of the art for red-teaming practices, from participatory events
to automatic AI-focused approaches, exposing the gaps in both the
techniques and coverage of the targeted harms. Second, we plan to
engage the audience in a hands-on and interactive exercise in LLM
red-teaming to showcase the ease (or difficulty) of exposing model
vulnerabilities, contingent on both the targeted harm and model
capabilities. We believe that the KDD community of researchers
and practitioners are in a unique position to address the existing
gaps in red-teaming approaches, given their longstanding research
and practice of extracting knowledge from data.
CCS CONCEPTS
•Computing methodologies →Natural language generation.
KEYWORDS
socio-technical harms; auditing; red-teaming; large language mod-
els
ACM Reference Format:
Manish Nagireddy, Bernat Guillén Pegueroles, and Ioana Baldini. 2024.
DARE to Diversify: DAta Driven and Diverse LLM REd Teaming. In Pro-
ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 2 pages. https://doi.org/10.1145/3637528.3671444
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36714441 WHY THIS TUTORIAL?
Nowadays, access to a Large Language Model (LLM) with billions
of parameters is a website away. Given the demonstrated high
performance of LLMs on a variety of tasks (e.g., summarization,
content creation, etc.), there is widespread excitement to use these
systems in a myriad of downstream applications. Concurrently,
there are growing efforts in understanding and categorizing the
risks associated with LLMs. Red-teaming, or interactive probing, is
one such area of focus. In order to most effectively uncover potential
risks via red-teaming, we strongly believe that a participatory effort
is paramount. In particular, with this tutorial, we seek to leverage
the diverse set of skills and lived experiences of KDD conference
attendees in order to discover LLM failures. In doing so, we hope to
affirm the notion that red-teaming is not a point-in-time endeavor,
rather it is a never-ending process. Effective red-teaming is the
result of interactive exploration and iterative improvement.
2 TARGET AUDIENCE AND PREREQUISITES
Intended Audience: Everyone.
Prerequisites: An interest in LLMs. Previous exploration or inter-
action with an LLM-based chat bot may be a plus.
Learning Outcomes: Upon completion of this tutorial, both the
participants and the organizers will collectively gain a deeper un-
derstanding of what it takes to effectively red-team large language
models for a broad and diverse set of target harms.
3 TUTORIAL OUTLINE (3 HOURS)
The tutorial is split into two parts: an introduction to red-teaming
and a hands-on session with several red-teaming exercises. In the
second part, we plan to offer several types of exercises that showcase
the ease (or difficulty) of exposing model failure depending on the
model and/or targeted harm. The second part of the tutorial is
highly interactive: we will engage the audience in sharing their
experience both in the types of harms they are interested in and in
the techniques used to uncover model failure.
The following outline contains the main points that will be dis-
cussed during the tutorial, with an emphasis on some of the refer-
ences that will be covered.
3.1 Part I: Introduction to Red Teaming (1.5h)
•Why Red Teaming?
–The strengths, benefits and dangers of LLMs [2, 11]
–Risks from generative output [10, 17]
–Brief introduction to policy and regulations [6, 14, 16]
•Participatory Red-teaming
–DefCon31: Generative AI Red-teaming Challenge [ 15,18]
–Red-teaming LLMs for resilience to scientific disinforma-
tion [23]
6420
KDD ’24, August 25–29, 2024, Barcelona, Spain Manish Nagireddy, Bernat Guillén Pegueroles, and Ioana Baldini
•State-of-the-art in Red-teaming Research
–Attack strategies: affirmative completion [ 24], context
switching [ 21], contextual interaction attack [ 4], instruc-
tion indirection [ 19], ciphers [ 25], role play [ 22] and per-
suasion [26]
–Automatic/AI-based approaches [3, 8, 9, 12, 13, 20]
–Red-teaming tooling [1, 5, 7, 12]
3.2 Part II: Hands-on LLM Red Teaming (1.5h)
•Exercise introduction: Infrastructure and exercise descrip-
tion
•First hands-on exercise: Exploratory exercise with mini-
mal guidance
•Interactive discussion: Best tricks/approaches that exposed
model failures
•Second hands-on exercise: Exploratory exercise taking
into account findings from first round
•Interactive discussion: Model failures uncovered by par-
ticipants
•Call to action: Reiterate over red-teaming gaps and research
directions
4 SPEAKER BIOGRAPHIES
Manish Nagireddy is a Research Software Engineer at IBM Research
AI and the MIT-IBM Watson AI Lab. His main research goal is
to build trustworthy AI solutions. His research interests encom-
pass several areas in machine learning and artificial intelligence
from classical ML methods to natural language processing and
the generative context. His current work focuses on use-case cen-
tered algorithmic auditing and evaluation, in the context of large
language models. He is a core maintainer for many open-source
trustworthy AI toolkits - AI Fairness 360, AI Explainability 360, and
Uncertainty Quantification 360.
Bernat Guillén Pegueroles is an applied mathematician and data
scientist at Google Trust and Safety. He is currently the tech lead for
data driven T&S operations research. Other work includes modeling
ecosystems of abuse/counterabuse dynamics, analysis of human
eval reliability, and before that, studying nature’s abusers (brood
parasitism and interspecies competition).
Ioana Baldini is a Senior Research Scientist at IBM Research AI.
She is the Tech Lead for social bias auditing and red-teaming of
language models at IBM Research. She is currently representing
IBM in the US AI Safety Institute Consortium in the Red Teaming
Working Group. She has a proven research record in different areas
that span computer architecture, runtime systems, cloud infrastruc-
ture and applied natural language processing. She enjoys working
in large projects with multi-disciplinary teams.
ACKNOWLEDGMENTS
We thank Kush R. Varshney (IBM Fellow, AI Governance) for his
guidance and support of this work.
REFERENCES
[1]. 2024. Python Risk Identification Tool for generative AI (PyRIT). Retrieved
March 27, 2024 from "https://github.com/Azure/PyRIT"
[2]Samuel Bowman. 2022. The Dangers of Underclaiming: Reasons for Caution
When Reporting How NLP Systems Fail. In ACL.[3]Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell.
2023. Explore, Establish, Exploit: Red Teaming Language Models from Scratch.
arXiv:2306.09442
[4]Yixin Cheng, Markos Georgopoulos, Volkan Cevher, and Grigorios G. Chrysos.
2024. Leveraging the Context through Multi-Round Interactions for Jailbreaking
Attacks. arXiv:2402.09177 [cs.LG]
[5]Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos,
Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
Gonzalez, and Ion Stoica. 2024. Chatbot Arena: An Open Platform for Evaluating
LLMs by Human Preference. arXiv:2403.04132 [cs.AI]
[6]European Commission. 2023. AI Act. Retrieved March 27, 2024 from https:
//digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai
[7]Leon Derczynski, Erick Galinkin, and Subho Majumdar. 2024. garak: A Framework
for Large Language Model Red Teaming. https://garak.ai.
[8]Ethan Perez et al. 2022. Red Teaming Language Models with Language Models.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022.
[9]George Kour et al. 2023. Unveiling Safety Vulnerabilities of Large Language
Models. arXiv:2311.04124 [cs.CL]
[10] Laura Weidinger et al. 2022. Taxonomy of Risks posed by Language Models. In
Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Trans-
parency.
[11] Laura Weidinger et al. 2023. Sociotechnical Safety Evaluation of Generative AI
Systems. arXiv:2310.11986
[12] Mantas Mazeika et al. 2024. HarmBench: A Standardized Evaluation Framework
for Automated Red Teaming and Robust Refusal. arXiv:2402.04249 [cs.LG]
[13] Zhang-Wei Hong et al. 2024. Curiosity-driven Red-teaming for Large Language
Models.
[14] White House. 2022. Blueprint for an AI Bill of Rights. Retrieved March 27, 2024
from https://www.whitehouse.gov/ostp/ai-bill-of-rights/
[15] White House. 2023. Red-Teaming Large Language Models to Identify Novel AI
Risks. Retrieved March 27, 2024 from https://www.whitehouse.gov/ostp/news-
updates/2023/08/29/red-teaming-large-language-models-to-identify-novel-ai-
risks/
[16] White House. 2024. FACT SHEET: Vice President Harris Announces
OMB Policy to Advance Governance, Innovation, and Risk Management
in Federal Agencies Use of Artificial Intelligence. Retrieved March
28, 2024 from "https://www.whitehouse.gov/briefing-room/statements-
releases/2024/03/28/fact-sheet-vice-president-harris-announces-omb-policy-
to-advance-governance-innovation-and-risk-management-in-federal-
agencies-use-of-artificial-intelligence/"
[17] IBM. 2024. AI risk atlas. Retrieved March 27, 2024 from
https://dataplatform.cloud.ibm.com/docs/content/wsj/ai-risk-atlas/ai-risk-
atlas.html?context=wx&audience=wdp#
[18] Humane Intelligence, Seed AI, and DefCon AI Village. 2024. GENERATIVE AI
RED TEAMING CHALLENGE: TRANSPARENCY REPORT. https://drive.google.
com/file/d/1JqpbIP6DNomkb32umLoiEPombK2-0Rc-/view.
[19] Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubrama-
nian, Bo Li, and Radha Poovendran. 2024. ArtPrompt: ASCII Art-based Jailbreak
Attacks against Aligned LLMs. In ACL.
[20] Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, and Preethi Lahoti. 2023.
AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-
powered Applications. In EMNLP: Industry Track, Mingxuan Wang and Imed
Zitouni (Eds.).
[21] Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chen-
glei Si, Svetlina Anati, Valen Tagliabue, Anson Kost, Christopher Carnahan, and
Jordan Boyd-Graber. 2023. Ignore This Title and HackAPrompt: Exposing Sys-
temic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition.
InProceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing.
[22] Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen
Casper, and Javier Rando. 2023. Scalable and Transferable Black-Box Jailbreaks
for Language Models via Persona Modulation. arXiv:2311.03348 [cs.CL]
[23] The Royal Society and Humane Intelligence. 2024. Red teaming large language
models (LLMs) for resilience to scientific disinformation.
[24] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How
Does LLM Safety Training Fail?. In Advances in Neural Information Processing
Systems.
[25] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shum-
ing Shi, and Zhaopeng Tu. 2024. GPT-4 Is Too Smart To Be Safe: Stealthy Chat
with LLMs via Cipher. In ICLR.
[26] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.
2024. How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion
to Challenge AI Safety by Humanizing LLMs. arXiv:2401.06373 [cs.CL]
6421