FedNLR: Federated Learning with Neuron-wise Learning Rates
Haozhao Wang
School of Computer Science and
Technology, Huazhong University of
Science and Technology
Wuhan, ChinaPeirong Zheng∗
Department of Computing, The Hong
Kong Polytechnic University
Hongkong, ChinaXingshuo Han
Nanyang Technological University
Singapore, Singapore
Wenchao Xu
Department of Computing, The Hong
Kong Polytechnic University
Hongkong, ChinaRuixuan Li†
School of Computer Science and
Technology, Huazhong University of
Science and Technology
Wuhan, ChinaTianwei Zhang
Nanyang Technological University
Singapore, Singapore
Abstract
Federated Learning (FL) suffers from severe performance degrada-
tion due to the data heterogeneity among clients. Some existing
work suggests that the fundamental reason is that data heterogene-
ity can cause local model drift, and therefore proposes to calibrate
the direction of local updates to solve this problem. Though effective,
existing methods generally take the model as a whole, which lacks
a deep understanding of how the neurons within deep classification
models evolve during local training to form model drift. In this
paper, we bridge this gap by performing an intuitive and theoretical
analysis of the activation changes of each neuron during local train-
ing. Our analysis shows that the high activation of some neurons on
the samples of a certain class will be reduced during local training
when these samples are not included in the client, which we call
neuron drift, thus leading to the performance reduction of this class.
Motivated by this, we propose a novel and simple algorithm called
FedNLR, which utilizes N euron-wise L earning R ates during the FL
local training process. The principle behind this is to enhance the
learning of neurons bound to local classes on local data knowledge
while reducing the decay of non-local classes knowledge stored in
neurons. Experimental results demonstrate that FedNLR achieves
state-of-the-art performance on federated learning with popular
deep neural networks.
CCS Concepts
•Computer systems organization →Distributed architec-
tures.
Keywords
Federated Learning; Neuron-wise Learning Rates; NonIID
∗Haozhao Wang and Peirong Zheng contributed equally to this research.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672042ACM Reference Format:
Haozhao Wang, Peirong Zheng, Xingshuo Han, Wenchao Xu, Ruixuan
Li, and Tianwei Zhang. 2024. FedNLR : Federated Learning with Neuron-
wise Learning Rates. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3672042
1 Introduction
Federated learning (FL) is emerging as a prominent framework to
train deep neural networks (DNNs) via the collaboration among
clients without sharing their original dataset [ 25,32,38,42], and
has been widely adopted in various applications such as medical
image processing [ 9,27,44] and recommendation [ 2,35]. The ba-
sic steps of FL [ 32] is to iteratively run the local training of the
models in multiple clients separately and the global aggregation
of all updated models in the server. Although the process of FL is
easy to implement, the data among clients is usually statistically
heterogeneous (i.e., not independently and identically distributed,
NonIID), greatly degrading the performance.
Many works have been proposed to solve the NonIID problem.
One of the representative categories is to explore the essence of per-
formance degradation from the perspective of optimization, which
considers the client drift (i.e., model drift ) caused by the NonIID as
the main reason for performance degradation. More specifically,
Karimireddy et al. [ 17] claim that the optimum of multiple clients
are different from each other and are also far away from the global
optimum due to the NonIID data, thus resulting in the drift of opti-
mization direction. Considering this, a series of approaches seek to
achieve consistency of the local models across clients. For example,
some methods add regularization on the local loss function to facil-
itate the local model to approach the global optimal model [ 1,23].
Other works consider that the local gradient is biased and then
correct the local gradient to align the global gradient [ 7,17,37]
There are also some works that seek to calibrate the output of the
last layer or the last-second layer to align them with the global
model to achieve consistency [ 19,22,47]. Although these methods
are effective, they generally view the deep neural network (DNN)
as a whole, which lacks a deep understanding of how the neurons
within the DNN evolve to form the issue of model drift.
In this paper, we seek to tackle the NonIID challenge of FL from
the microscopic level by decomposing the DNN as a set of neurons
3069
KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.
to fully unleash FL performance potential. Specifically, we find
that the set of high-activation neurons within a well-converged
global DNN over each class is distinct from other classes, where
these neurons we call bound neurons of this class. However, during
the local training process, each client tends to overuse the DNN
neurons by homogenizing as many neurons as possible to obtain
high activation over its local data. Due to the data heterogeneity
among clients, this may lead to the activation reduction of bound
neurons of those data classes that are not included in this client,
which we call neuron drift. As a consequence, how to prevent the
activation of bound neurons over classes that are not included in
the client from being reduced is the key to the global model learning
the global data distribution.
Motivated by this discovery, we propose an effective yet simple
method named FedNLR , which allows each client to locally train
the deep neural network with neuron-wise learning rates. More
specifically, FedNLR sets a specific learning rate for the parameters
of each neuron based on their activation over the local data. The
learning rate of a neuron is set to be smaller if its activation over
the local data is smaller. The principle behind this is to enhance the
learning of neurons bound to local classes on local data knowledge
while restricting the decay of non-local classes knowledge stored in
neurons. We conduct extensive experiments on various DNNs and
datasets of which the experimental results show that our method
outperforms existing methods. Besides, as far as we know, our
method is the first to solve the NonIID challenge from the perspective
of adapting the local learning rate, which is orthogonal to existing
approaches. Our experimental results show that the performance of
existing methods receives further improvement as combined with
our method. Our contributions are:
•We are the first to investigate how the FL data heterogeneity
affects the trained DNNs by decomposing the DNN into a set
of neurons instead of viewing it as a whole. We identify that
the activation value of a neuron on some certain class will
be reduced during the local training process of a client if this
client has no access to the samples of this class, resulting in
performance degradation.
•We propose a novel method that allows each client to restrict
the update of neurons bound to the other clients and en-
hances the learning of neurons bound to its local data during
the local training process via leveraging neuron-wise learn-
ing rates. The learning rate for each neuron is adaptively set
based on its activation over the local data.
•We provide a theoretical analysis that guarantees the conver-
gence of the proposed method. Besides, the experiments con-
ducted on various configurations such as different learning
rates and different numbers of clients consistently demon-
strate the effectiveness of our method.
2 Related Work
To train a global model, many previous efforts have been proposed
to solve the NonIID challenge in FL [ 12,14,39]. There are mainly
two types of work which include either adding a regularization
item to the local loss or calibrating the local model update with the
global information to achieve consistency across clients.
Regularization-based Consistency Some methods propose
adding a regularization on the local loss function to achieve theconsistency of local updated models [ 1,23]. FedProx [ 23] penalizes
the proximal term on the local objective to force the local update
towards both the local optimum and the last global model. [ 1] pro-
posed a dynamic regularizer that is based on the current local model
and the received global model to achieve the same stationary point
across all clients. [ 8] combines the local training process with the
primal-dual algorithm to enhance the consistency among different
variable models. FedSpeed [ 37] considers that the prox-term will
also introduce the bias and then applies the prox-correction term
on the current local updates to efficiently reduce the bias. [ 40] show
that the hyper-parameters in the local update process also have an
impact on the consistency and then regularize the local update. By
encompassing the proximal term, [ 3] further propose a surrogate
loss for the quadratic models and show that the local learning rate
decay can balance the trade-off between the convergence rate and
the inconsistency. [ 31] applies a local fixed-point to implicitly con-
trol the convergence of the local model. Based on the inconsistency
of local update, [ 16] propose adaptively tuning the global step size
via computing a regularized term of all local updates.
Calibration-based Consistency Other works consider that the
local gradient is biased and then correct the local gradient to align
the global gradient [ 13,17,37,43]. [17] firstly demonstrates the
drift of local update in FL and then propose Scaffold that exploits
the bias of the local gradient to the global gradient to mitigate these
drifts. [ 7] consider the drift between the local optimal model and
the global optimal model essentially exists and propose learning the
drift term to compensate local gradient. [ 29] propose incorporating
the information of global gradient in the local training process such
that the local bias can be mitigated. Some studies also introduce the
momentum to the FL where the momentum also contains the infor-
mation of past gradients from other clients such that the drift can be
implicitly calibrated [ 18,36,41,45]. Considering the last layer cares
most about the classification, FedRS [ 26] utilizes an asymmetric
loss function to calibrate the bias of the last-layer parameters of
different classes. Similarly, there are also some works that seek to
calibrate the output of the last layer or the last-second layer to align
them with the global model to achieve consistency [19, 22, 47].
This paper focuses on the drift problem of local models across
clients in federated learning. Different from previous works that
usually view the training DNN as a whole and consider the drift of
the whole local update, we aim to solve the neuron drift problem
which is a fine-grained phenomenon of the client drift. To tackle
this challenge, we adopt neuron-wise learning rates during the local
training process, which is orthogonal to existing works.
3 Problem Formulation and Preliminaries
Problem Formulation. Our goal is to collectively train a global
model in Federated Learning (FL) for a total of 𝐾clients. Each client
𝑘has exclusive access to their private local dataset D𝑘which con-
sists of𝐷𝑘data samples(𝑥𝑛
𝑖,𝑦𝑖), where𝑥𝑖represents the 𝑖-th input
data sample and 𝑦𝑖belongs to a set of possible labels {1,2,...,𝐶}
with𝐶denoting the total number of classes. The primary objective
is to train a global model wthat minimizes the total empirical loss
across all local datasets. The loss function is formulated as:
minw𝐹(w):=𝐾∑︁
𝑘=1𝐷𝑘
𝐷𝐹𝑘(w),where𝐹𝑘(w)=1
𝐷𝑘𝐷𝑘∑︁
𝑖=1𝑓𝑘(w;𝑥𝑖,𝑦𝑖),(1)
3070FedNLR: Federated Learning with Neuron-wise Learning Rates KDD ’24, August 25–29, 2024, Barcelona, Spain
2023/11/12
1𝑤1,1,1
=1Server
1
2𝑥1𝑦=1
𝑡=1 𝑡=2Client 1
Client 2𝑤1,2,2
=1𝑤2,1,1
=10/9
𝑤2,2,2
=10/9 ℎ1,2ℎ1,1
2/32/3
𝑦=2Rounds
𝑐=1
𝑐=2𝑥2
𝑥1𝑥2
011
1/31
31/3
1/3𝑦=1𝑦=21
2
1
2
(a) Data Distribution
2023/11/12
1𝑤1,1,1
=1Server
1
2𝑥1𝑦=1
𝑡=1 𝑡=2Client 1
Client 2𝑤1,2,2
=1𝑤2,1,1
=10/9
𝑤2,2,2
=10/9 ℎ1,2ℎ1,1
2/32/3
𝑦=2Rounds
𝑐=1
𝑐=2𝑥2
𝑥1𝑥2
011
1/31
31/3
1/3𝑦=1𝑦=21
2
1
2 (b) Training Process
Figure 1: An example of two-client federated learning. One client is sampled in each round and each client only contains
samples of one single label, i.e., client 1 with class 1 and client 2 with class 2. The data of classes 1 and 2 are denoted by triangle
and circle respectively. The upward arrow represents that the value of the parameter is improved and the downward arrow
has the opposite meaning. As can be seen, after training the model in client 1, the parameters corresponding to the bonded
neurons of class 2 may be improperly tuned, leading to their reduced activation magnitude over the data of class 2. A similar
phenomenon also exists in the client 2, which we call neuron drift.
where𝐷=Í𝐾
𝑘=1𝐷𝑘denotes the total size of all local datasets,
𝐹𝑘(w)represents the local loss for the 𝑘-th client, and 𝑓𝑘(·)usu-
ally adopts the cross-entropy loss which measures the discrepancy
between the model’s predictions and the actual ground truth labels.
Basics of deep neural network. We consider a deep neural net-
work comprising 𝐿layers, where each layer 𝑙consists of𝑀𝑙neu-
rons. The model’s weight parameters are represented as wand the
parameters of the 𝑙-th layer are denoted by w𝑙. For each neuron
indexed as𝑖in the𝑙-th layer, we compute its activation output as
ℎ𝑙,𝑖=𝜎(w𝑇
𝑙,𝑖h𝑙−1). In this expression, we use the commonly used
activation function ReLU to signify 𝜎(·), while w𝑙,𝑖corresponds
to the parameters specific to this neuron. Furthermore, hl−1de-
notes the outputs of all neurons in the preceding 𝑙−1-th layer,
characterized as hl−1=[ℎ𝑙−1,1,...,ℎ𝑙−1,𝑚𝑙−1].
4 Neuron Drift
To make predictions, deep neural networks activate different neu-
rons for the data of different classes. In particular, given a well-
converged model, each class is bound to a specific subset of neu-
rons, where the activation magnitude of these neurons over this
class is larger than other classes. Formally, these bound neurons
are defined as follows.
Definition 1. Considering a neuron 𝑖, if its activation magnitude
ℎ𝑐
𝑖over the data of class 𝑐is larger than the activation ℎ𝑐′
𝑖over data
of other classes 𝑐′, i.e., min𝑖∈D𝑐ℎ𝑐
𝑖>max𝑖∈D𝑐′ℎ𝑐′
𝑖,∀𝑐′∈[𝐶],𝑐′≠𝑐,
then the neuron 𝑖is defined as the bound neuron of the class 𝑐.
Now, based on the theories of neural collapse [ 6,10,28], we show
that each data class corresponds to a subset of bounded neurons in
converged neural networks by the following theorem.
Theorem 1. Considering the neural network has reached the opti-
mal solution of (1) over the balanced-class dataset, if the loss function
𝑓(·)is cross-entropy (CE) or mean-square-error (MSE) and the ac-
tivation function is ReLU, then there exists a non-empty subset H𝑐
consisting of bound neurons for each class 𝑐.Proof. We denote the parameters of the last classifier layer
corresponding to the class 𝑐asw𝐿,𝑐, and denote the activation of
the last-second layer over the data of class 𝑐ash𝑐
𝐿−1. Based on the
theories of neural collapse, it can be concluded that
w𝐿,𝑐=𝐴h𝑐
𝐿−1, (2)
when the model achieves the optimal solution for CE loss (Theo-
rem 1 of [ 6]) and for MSE loss (Theorem 3 of [ 10]), where𝐴>0is a
constant. Since the optimal model minimizes the objective function
(1), it makes correct predictions over the training dataset. Hence, for
any data sample of the class 𝑐, its prediction score 𝑝𝑐corresponding
to the class 𝑐is larger than other classes 𝑐′, i.e.,𝑝𝑐>𝑝𝑐′. The pre-
diction score 𝑝𝑐for each class 𝑐is calculated based on the softmax
𝑝𝑐=exp(w𝑇
𝐿,𝑐h𝑐
𝐿−1/𝑇)
Í𝐶
𝑖=1exp(w𝑇
𝐿,𝑖h𝑐
𝐿−1/𝑇). Based on this definition, the prediction
scores of different classes for the sample 𝑥of class𝑐satisfy
𝑝𝑐=exp(w𝑇
𝐿,𝑐h𝑐
𝐿−1/𝑇)
Í𝐶
𝑖=1exp(w𝑇
𝐿,𝑖h𝑐
𝐿−1/𝑇)>𝑝𝑐′=exp(w𝑇
𝐿,𝑐′h𝑐
𝐿−1/𝑇)
Í𝐶
𝑖=1exp(w𝑇
𝐿,𝑖h𝑐
𝐿−1/𝑇)(3)
for any class 𝑐′∈[𝐶],𝑐′≠𝑐. By eliminating the denominators of
both sides of (3), we get that
w𝑇
𝐿,𝑐h𝑐
𝐿−1>w𝑇
𝐿,𝑐′h𝑐
𝐿−1. (4)
Bringing (2) to (4), we have
𝐴(h𝑐
𝐿−1)2>(h𝑐′
𝐿−1)𝑇h𝑐
𝐿−1, (5)
which directly derives the following inequality
(h𝑐
𝐿−1−h𝑐′
𝐿−1)𝑇h𝑐
𝐿−1>0⇔𝑀𝐿−1∑︁
𝑖=1(ℎ𝑐
𝐿−1,𝑖−ℎ𝑐′
𝐿−1,𝑖)ℎ𝑐
𝐿−1,𝑖>0,(6)
which indicates that there exists a subset of neurons H𝑐⊂[𝑀𝐿−1],
the following inequality holds
(ℎ𝑐
𝐿−1,𝑖−ℎ𝑐′
𝐿−1,𝑖)ℎ𝑐
𝐿−1,𝑖>0. (7)
3071KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.
for each𝑖∈H𝑐. Since the activation output by the ReLU func-
tion is always non-negative, i.e., ℎ𝑐
𝐿−1,𝑖≥0,∀𝑖∈[𝑀𝐿−1], we can
immediately obtain that
ℎ𝑐
𝐿−1,𝑖−ℎ𝑐′
𝐿−1,𝑖>0. (8)
for each𝑖∈H𝑐. The proof is done. □
Noting that the activation function is ReLU(w𝑇
𝑙,𝑖h𝑙−1)and the
prediction probability 𝑝𝑐for each class 𝑐is𝑝𝑐=exp(w𝑇
𝐿,𝑐h𝑐
𝐿−1/𝑇)
Í𝐶
𝑖=1exp(w𝑇
𝐿,𝑖h𝑐
𝐿−1/𝑇),
indicating that high activation magnitude strongly contributes to
the classification result. In the following text, for ease of expression,
we also call the neurons subset H𝑐as the bound neurons of the
class𝑐. Obviously, reducing the activation magnitude of bound neu-
rons corresponding to some given class 𝑐will inevitably decrease
its prediction probability and thus decrease the prediction accuracy.
Unfortunately, in FL scenario, we show that the activation magni-
tude of the bound neurons of some data classes may be severely
reduced during the training process due to the data heterogeneity
across clients. We show this by specifying the following example.
Example 1. We consider training a two-layer neural network over
two clients, as shown in Figure 1. The task is the binary classification
with the label 𝑦=1or𝑦=2corresponding to samples distributed
around(1,1/3)and around(1/3,1)respectively. By simulating the
data heterogeneity, we consider each client has only access to the
samples of one single class ( 𝑦=1for client 1and𝑦=2for client 2).
The initialized parameters of the neural network are presented in
the server of Figure 1a. The bound neurons for the class 𝑦=2are
denoted by gold color, i.e., the second hidden neuron ℎ1,2which
has a high activation over the data of class 2. Now, we show that
the expected activation of all neurons over the class 2is reduced
after local training in client 1:
Δℎ1,1=ℎ2
1,1−˜ℎ2
1,1=0.67−0.73=−0.06,
Δℎ1,2=ℎ2
1,2−˜ℎ2
1,2=1.11−1.04=0.07,
Δ𝑝1=𝑝1−˜𝑝1=0.45−0.63=−0.18,
Δ𝑝2=𝑝2−˜𝑝2=0.55−0.37=0.18,(9)
whereℎ2
1,2and ˜ℎ2
1,2represent the activation of class 2before and
after training an iteration in client 1with the learning rate 0.5.𝑝2
and𝑝′
2represent the probability score over the data of the class 2
before and after local training. By observing the value Δℎ1,2, we
can conclude that the activation of bound neurons over class 2is
greatly reduced after local training in client 1. This also leads to a
probability reduction as presented by Δ𝑝2, decreasing the accuracy
of the class 2. As a comparison, the values of Δℎ1,1andΔ𝑝1indicate
that the risk of class 2data being misclassified into class 1increases.
We call this phenomenon neuron drift, which is the essence of neural
networks occurring client drift [1, 17].
The example shows that each client tends to train the neural
network to maximize the probability score over its local data. The
client realizes its objective by increasing the activation magnitude
of neurons over its local data classes. When the bound neurons of
a class missed by the client are included in these updated neurons,
the phenomenon of neuron drift occurs. In fact, the phenomenon
of neuron drift presented in Example 1 exists in general two-layer
neural networks.Theorem 2. Considering a two-layer neural network has reached
the optimal solution of (1) over the balanced-class dataset of the bi-
nary classification task, and its activation function is ReLU, if the
neural network is trained a local step in a client 𝑘without the data of
class𝑐, then the activation magnitude of the bonded neurons H𝑐
over the class 𝑐data𝑥𝑐
𝑖∈D𝑐are reduced as 𝑥𝑇
𝑘𝑥𝑐
𝑖>0. More
specifically, the activation reduction Δℎ1,𝑚of the𝑚-th neuron of
the hidden layer is w𝑇
1,𝑚𝑥𝑐
𝑖asw𝑇
1,𝑚𝑥𝑐
𝑖−𝜂Í
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−
1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚≤0, and otherwise, the activation reduction
is𝜂Í
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚.
Proof. Consider the class 𝑐and one of its bonded neurons
ℎ1,𝑚∈H𝑐in the hidden layer. The updated formula of the pa-
rameter connected to this neuron is:
˜𝑤1,𝑚,𝑗=𝑤1,𝑚,𝑗−𝜂∑︁
𝑥𝑘∈D𝑘𝑥𝑗
𝑘 (𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚,
where ˜𝑤1,𝑚,𝑗denotes the parameter after local training in the client
𝑘,𝑝𝑐(𝑥𝑘)denotes the probability score of 𝑥𝑘over the class 𝑐, and𝑥𝑗
𝑘
denotes the 𝑗-th dimension of the sample 𝑥𝑘. Considering the model
has achieved convergence of neural collapse, based on equation (2),
i.e.,w2,𝑐=𝐴h𝑐
1,∀𝑐∈[𝐶], we have
(𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚 (10)
=−𝑝𝑐(𝑥𝑘)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚=𝑝𝑐(𝑥𝑘)(𝐴ℎ𝑐
1,𝑚−𝐴ℎ𝑐′
1,𝑚)>0,
where the last inequality is based on the Definition 1 for the bonded
neurons. Now, for each data sample 𝑥𝑐
𝑖∈D𝑐of the class 𝑐, when
𝑥𝑇
𝑘𝑥𝑐
𝑖>0for all sample 𝑥𝑘∈D𝑘of client𝑘, we can compute the
change of the activation ℎ1,𝑚of the bonded neuron before and after
local updating in the client 𝑘:
Δℎ1,𝑚=ReLU(w𝑇
1,𝑚𝑥𝑐
𝑖)−ReLU(˜w𝑇
1,𝑚𝑥𝑐
𝑖)=ReLU(w𝑇
1,𝑚𝑥𝑐
𝑖)
−ReLU
w𝑇
1,𝑚𝑥𝑐
𝑖−𝜂∑︁
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚
≥0, (11)
where𝑑is the dimension of the data space. The last inequality is de-
rived from (10). More specifically, as w𝑇
1,𝑚𝑥𝑐
𝑖−𝜂Í
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−
1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚≤0, the activation reduction is
Δℎ1,𝑚=ReLU(w𝑇
1,𝑚𝑥𝑐
𝑖)−0=w𝑇
1,𝑚𝑥𝑐
𝑖, (12)
and otherwise,
Δℎ1,𝑚
=w𝑇
1,𝑚𝑥𝑐
𝑖−w𝑇
1,𝑚𝑥𝑐
𝑖+𝜂∑︁
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚
=𝜂∑︁
𝑥𝑘∈D𝑘𝑥𝑇
𝑘𝑥𝑐
𝑖 (𝑝𝑐′(𝑥𝑘)−1)𝑤2,𝑐′,𝑚+𝑝𝑐(𝑥𝑘)𝑤2,𝑐,𝑚. (13)
The proof is done. □
Although our formal analysis is only based on the two-layer
neural network, it also provides some insights into the deeper
neural networks. Intuitively, we can view the deep neural network
as a series of two-layer neural networks. Except for the last two-
layer neural network, which uses real labels as supervision, all
others use the activation of the intermediate layer as supervision. In
fact, existing studies have shown that the separability of activation
3072FedNLR: Federated Learning with Neuron-wise Learning Rates KDD ’24, August 25–29, 2024, Barcelona, Spain
between classes also exists and progressively increases from shallow
to deep layers at a linear rate [ 11,15,34], which indicates that
these intermediate activation can be viewed as variants of real
label. As a consequence, the neuron drift may also exist in deep
neural networks, but in different degrees in different layers. The
deeper the layer, the more serious the neuron drift. This intuition is
also consistent with the experimental observations in [ 30], where
activation similarity among clients linearly reduces with the growth
of layers. Experimental verfications can be found in Appendix A.
5 Methodology
In this section, we propose a simple yet effective method named
FedNLR to mitigate the neuron drift. Specifically, the core idea is to
adopt neuron-level learning rates during the local training process.
The algorithm workflow is presented in Algorithm 1.
As analyzed above, the activation of bound neurons over some
classes may be reduced after training the neural network in the
client without the data of these classes. The principle behind this
is that the client tends to increase the activation of these bound
neurons over its local data. To solve this problem, a direct way is to
identify the bound neurons using these missed classes and then, set
small learning rates for the parameters of these neurons to restrict
their activation reduction.
However, such a way is impractical because the client has no
access to the data of these missed classes. Considering this, we
propose an approximation algorithm, which sets learning rates
according to the activation magnitude of the global model over
the local data. The learning rates of parameters of low-activation
neurons are set to be small. The principle is that the bound neurons
are included in the set of neurons with low activation magnitude
over local data. Therefore, restricting them can also mitigate the
activation reduction of bound neurons over missed data classes.
Consider the local training process of client 𝑘in each round 𝑡.
To adapt the neuron-level learning rates, before performing local
training steps, each client first computes the activation ℎ𝑙,𝑚for
each neuron of the received global model w𝑡over its local dataset
D𝑘and then computes the activation average ¯ℎ𝑙,𝑚over all samples:
¯ℎ𝑙,𝑚=1
𝐷𝑘∑︁
𝑥𝑖∈D𝑘ℎ𝑙,𝑚(𝑥𝑖). (14)
Since the activation values of neurons in different layers are dif-
ferent, the client sets the learning rates for neurons layer by layer.
Specifically, for each layer 𝑙, the learning rate ˆ𝜂𝑙,𝑚of the𝑚-th
neuron parameters is set to be:
𝑞𝑙,𝑚=𝑒¯ℎ𝑙,𝑚/𝑇𝑙
Í𝑀𝑙
𝑖=1𝑒¯ℎ𝑙,𝑖/𝑇𝑙,ˆ𝜂𝑙,𝑚=𝜂·𝑀𝑙𝑞𝑙,𝑚
Í𝑀𝑙
𝑖=1𝑞𝑙,𝑚, (15)
where𝜂is the basic learning rate used in existing methods and 𝑞𝑙,𝑚
denotes the scale of the learning rate. The temperature parameter 𝑇𝑙
controls the discrepancy between the maximized and the minimized
learning rate for the neurons of the 𝑙-th layer. A larger 𝑇𝑙indicates
a larger discrepancy. Specifically, to avoid excessively high or low
learning rates, we use a hyper-parameter 𝜇𝑙=ˆ𝜂𝑙,max
ˆ𝜂𝑙,minto be the ratio
between the maximized and the minimized learning rate for the
neurons of the 𝑙-th layer, where ˆ𝜂𝑙,max=max(ˆ𝜂𝑙,1,..., ˆ𝜂𝑙,𝑀𝑙)andAlgorithm 1: Algorithm workflow of FedNLR
Input :𝑇: communication round; 𝐾: client number; 𝜂:
basic learning rate; 𝜇0,𝑎1,𝑎2: discrepancy rate
between maximum and minimum learning rates;
1Initialize the parameter w1;
2for𝑡=1to𝑇do
3 Randomly select 𝐾𝑡clients and send the global model
𝑤𝑡to them;
4 foreach selected client 𝑘in parallel do
5 Compute activation average ¯ℎ𝑙,𝑚with (14);
6 Compute the discrepancy ratio 𝜇𝑙with (17);
7 Compute the temperature 𝑇𝑙with (16);
8 Compute the learning rate 𝜂𝑙,𝑚via (15);
9 Update local model w𝑘for𝐸iterations with (18);
10 Send the model w𝑡,𝐸
𝑘to the server;
11 end
12 Aggregate local models with (19);
13end
ˆ𝜂𝑙,min=min(ˆ𝜂𝑙,1,..., ˆ𝜂𝑙,𝑀𝑙). Then, we can obtain
𝜇𝑙=ˆ𝜂𝑙,max
ˆ𝜂𝑙,min=𝑒(¯ℎ𝑙,max−¯ℎ𝑙,min)/𝑇𝑙⇒𝑇𝑙=¯ℎ𝑙,max−¯ℎ𝑙,min
ln(𝜇𝑙).(16)
Setting𝜇𝑙for each layer of the neural network requires signifi-
cant human costs, we propose a heuristic formula to make configu-
rations for all layers as:
𝜇𝑙=𝜇0+𝑎1·𝑙/𝐿+𝑎2·log𝑀𝑙, (17)
where𝜇0,𝑎1, and𝑎2are positive constants. The configuration of
𝜇𝑙is mainly based on the depth and the width of the neural net-
work layer. The intuition behind this design is based on the sharing
degree of high-activation neurons between different data classes.
Even though the bound neurons of some missed classes are re-
duced by the client, these classes can still achieve high probability
scores with the shared neurons. Hence, the ratio is lower to relax
the restriction when different classes share more high-activation
neurons. Generally, shallow layers reveal the common knowledge
shared among classes and they may activate the similar neurons.
Conversely, deep layers extract different knowledge from different
classes, and each class tends to activate distinct neurons. Therefore,
we set lower ratios for shallow layers. A similar case also exists in
the layer width. When the number of neurons in a layer is small,
these classes have to share them to improve their probability scores.
Hence, we set smaller ratios for narrow layers.
Finally, with the learning rate 𝜂𝑙,𝑚for the parameters w𝑙,𝑚of
each𝑚-th neuron of the 𝑙-th layer, the client 𝑘runs update:
w𝑘
𝑙,𝑚=w𝑘
𝑙,𝑚−ˆ𝜂𝑙,𝑚∇w𝑘
𝑙,𝑚𝑓𝑘(w𝑘), (18)
where∇w𝑘
𝑙,𝑚𝑓𝑘(w𝑘)denotes the gradient over a mini-batch of sam-
ples randomly sampled from the local dataset D𝑘. After performing
𝐸local iterations, the client 𝑘uploads the locally updated model
w𝑘to the server, and the server aggregates received local models
3073KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.
into the global model w:
w=𝐾𝑡∑︁
𝑘=1𝐷𝑘Í𝐾𝑡
𝑘=1𝐷𝑘w𝑘, (19)
where𝐾𝑡is the number of the 𝑡-th round selected clients.
6 Theoretical Analysis
Besides, we make the following assumptions for these objectives
which are widely adopted in FL [5, 40].
Assumption 1. (L-smoothness). The objective function 𝐹𝑛is L-
smooth with Lipschitz constant 𝐿>0, i.e.,∥∇𝐹𝑛(w)−∇𝐹𝑛(w′)∥2≤
𝐿∥w−w′∥2for all w,w′.
Assumption 2. (Bounded Variance). For all parameters w, the
variance of the local stochastic gradient in each client is bounded by
𝜎2
𝑙:E(∥∇𝑓𝑛(w)−∇𝐹𝑛(w)∥2)≤𝜎2
𝑙. Besides, the norm of gradient is
bounded by 𝑀:∥∇𝐹𝑛(w)∥2≤𝑀2.
Based on the above assumptions, we have the following theory
for the convergence of the proposed algorithm.
Theorem 3. Consider problem (1) under Assumption 1 and 2. If
the learning rate 𝜂diminishes withO(√︃
𝐾
𝑇), then the global model
w𝑡obtained by Algorithm 1 achieves asymptotic convergence, i.e.,
1
𝑇𝑇∑︁
𝑡=1E∥∇𝐹(w𝑡)∥2
2≤2(𝐹(w1)−𝐹∗))√︁
𝜇max(𝑀max−1)+1
√
𝐾𝑇
+𝜇2max(𝜇max(𝑀max−1)+1)𝜎2
𝑙𝐿
(𝜇max+𝑀min−1)2𝑟√
𝐾𝑇+4𝜇3max(𝜇max(𝑀max−1)+1)2𝐸2𝑀2
(𝜇max+𝑀min−1)3√
𝑇
+(1−𝑟)𝜇2max(𝜇max(𝑀max−1)+1)𝑀2𝐿
(𝜇max+𝑀min−1)2𝑟√
𝑇, (20)
where𝑟is the ratio of participated clients in each round.
With Theorem 3, we ensure the convergence of the global model.
Although the convergence rate does not receive improvement over
baselines, empirical results demonstrate the effectiveness of our
method. The details of the proof can be found in Appendix B.
7 Experiment
In this section, we evaluate the accuracy of FedNLR and compare
it with several advanced methods in various datasets and NonIID
settings. Due to the page limitation, more details are available in
the supplementary materials.
7.1 Setup
Dataset. We explore 3 benchmark datasets: CIFAR-10[ 20], CIFAR-
100[20], CINIC[ 4]. For both of them, we use the two NonIID data
settings: the Dirichlet distribution[ 46] and the shards-based seg-
mentation [ 33]. In the Dirichlet (𝑛,𝛼) function, smaller 𝛼leads to
more NonIID level. In the Shards (𝑛,𝑆) function,𝑆represents the
number of classes in one client. For example, Shards (20,2) with
CIFAR-10 dataset will allocate each client with 2classes of images.
Notice that both functions are unbalanced, allocating un-uniform
size of data across clients. Furthermore, we choose different scenar-
ios of data distribution by setting 𝛼and𝑆in [2, 4, 6, 8] to simulate
different levels of real-world NonIID scenarios.Baseline. Baselines include FedAvg[ 32], FedProx[ 24], Scaffold[ 17],
FedNova[40], FedRS[26], FedLC[47], FedDyn[1], CCVR[30].
Hyper-parameter Settings. The hyper-parameters for each method
were set according to their respective original papers. For instance,
the hyper-parameter for FedProx is set at 0.01, for FedDyn is 0.1,
for FedLC is 1, for FedRS is 0.5, and for FedNLR , it is self-adjusted
using an empirical formula as outlined in Eq. (17). The fine-tuned
empirical formula dynamically adjusts 𝜇𝑙based on layer depth and
neuron count. The process of fine-tuning the specific factors is
further explained in Sec. 7.2.
Configurations. Unless otherwise mentioned, we set the number
of local training epoch 𝐸=2, communication round 𝑇=500,
number of clients 𝑛=20, participation ration 0.4in each round;
batch size 64, learning rate 𝜂=0.01. We develop a customized
Stochastic Gradient Descent (SGD) optimizer, capable of handling
vector-form learning rates. All methods run without momentum or
weight decay, except FedDyn requiring a weight decay of 0.1 and
running 200 rounds. These experiments are run on the hardware of
4 NVIDIA GeForce RTX 3090 GPUs and the software framework of
PyTorch 2.0. Each experimental setting is run twice, with the final 5
rounds’ ACC(accuracy) averaged and standard variance calculated.
We employ VGG-9 [ 21] as the basic model architecture, initialized
using the Kaiming uniform function for convolutional layers and
the Xavier normal function for linear layers.
7.2 Result and Analysis
Better Performance. As shown in Tab. 1 and Tab. 2, under all
NonIID conditions, FedNLR or "FedNLR + FedDyn" demonstrates
the best performance on both datasets. Although in a few cases,
FedNLR ’s performance is not as good as FedDyn’s, the combination
with FedDyn leads to significant improvement, achieving the opti-
mal state. In the CIFAR-100-Dirichlet(20, 0.05),FedNLR achieves a
3.9% promotion compared with FedAvg, while other methods only
raise around 1%. After enough rounds of training, other methods
don’t show an improvement because they are not designed to focus
on convergence accuracy. Even FedDyn has made great improve-
ments in the CIFAR-10-Dirichlet(20, 0.5), the addition of FedNLR still
shows gains of 1.1%. Moreover, the improvements of FedNLR grow
with the degree of NonIID, as proved by the CIFAR-100-Dirichlet(20,
0.05)and the less NonIID CIFAR-100-Dirichlet(20, 0.5).
Robustness on heterogeneous data. We find that FedNLR exhib-
ited greater performance improvements not only in situations with
higher NonIID degrees but also in a larger number of categories,
inferring from the comparison of the CIFAR-100-Dirichlet(20, 0.05)
and the fewer categories of CIFAR-10-Dirichlet(20, 0.05). This indi-
cates that FedNLR has significant advantages in handling different
types and complexities of data distributions. On the other hand,
we have run the experiment settings in different learning rates
as shown in Fig. 2, in which the resulting improvements are not
affected by the learning rate. Furthermore, when the client par-
ticipation ratio decreases, the NonIID influences more strongly
because of the unbalanced local datasets. We use 100 clients and
choose 10% participation in each round and calculate the average
and variance of the last 30 rounds. As shown in Tab. 3, FedNLR
presents a consistent improvement across different NonIID levels.
In the CIFAR-10-Shards(100, 4), FedDyn improves 4.57% and can
3074FedNLR: Federated Learning with Neuron-wise Learning Rates KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The comparison of final test accuracy on the two datasets. The best result in each setting is bolded.
Metho
d CIF
AR-10(%) CIF
AR-100(%)
Shar
ds(𝑛,𝑆) (20,
2) (20, 4) (20, 6) (20, 8) (20,
20) (20, 40) (20, 60) (20, 80)
Fe
dAvg 72.11±0.61
78.10±0.52 81.26±0.29 81.38±0.19 45.51±0.59
49.40±0.13 49.64±0.45 50.44±0.09
FedProx 71.78±0.32
78.90±0.28 80.99±0.17 81.42±0.14 45.17±0.37
48.43±0.34 49.53±0.36 50.71±0.06
Scaffold 71.76±0.20
78.11±0.46 81.19±0.15 81.13±0.34 45.64±0.32
48.92±0.08 49.78±0.20 51.81±0.32
FedNova 71.64±0.28
77.82±0.33 81.94±0.12 81.61±0.16 46.02±0.44
48.80±0.20 49.88±0.25 50.48±0.39
FedRS 72.13±0.82
78.53±0.28 81.99±0.15 80.95±0.17 46.21±0.59
49.41±0.17 50.75±0.37 51.34±0.19
FedLC 71.24±0.18
78.30±0.33 81.53±0.24 81.74±0.27 46.04±0.53
49.35±0.20 50.24±0.16 51.60±0.24
FedDyn 68.33±1.52
80.65±0.59 84.71±0.40 85.35±0.21 28.88±0.26
31.18±0.30 31.32±0.23 34.09±0.20
FedNLR 74.11±0.48 80.10±0.37 82.77±0.26 82.24±0.12 47.93±0.49 50.20±0.28 51.79±0.62 52.46±0.17
Fe
dDyn + FedNLR 73.27±0.33 81.93±0.30 85.67±0.26 85.89±0.08 35.03±0.15 33.86±0.28 33.44±0.42 35.99±0.30
Dirichlet (𝑛,𝛼) (20,
0.05) (20, 0.1) (20, 0.3) (20, 0.5) (20,
0.05) (20, 0.1) (20, 0.3) (20, 0.5)
Fe
dAvg 65.42±1.14
72.66±0.20 79.05±0.20 80.13±0.24 40.71±0.55
44.12±0.15 48.89±0.44 50.62±0.55
FedProx 64.02±2.04
72.51±0.76 78.49±0.20 80.20±1.27 41.10±0.62
43.82±0.31 48.49±0.18 50.14±0.77
Scaffold 63.22±3.49
71.84±0.48 78.94±0.17 79.86±0.66 41.86±0.19
44.76±0.63 49.20±0.61 50.26±0.36
FedNova 63.14±1.08
72.58±0.44 79.13±0.42 79.30±2.67 41.49±0.51
44.00±0.48 49.40±0.18 50.72±0.21
FedRS 64.98±0.81
71.54±0.26 78.70±0.21 80.07±0.27 41.25±0.55
44.91±0.36 49.53±0.23 51.26±0.17
FedLC 65.06±1.07
72.31±0.38 78.92±0.23 80.95±0.28 41.74±0.30
44.83±0.59 49.10±0.41 50.20±0.62
FedDyn 51.06±1.22
72.15±0.64 83.26±0.13 84.70±0.30 29.09±0.31
30.64±0.18 33.10±0.19 35.57±0.27
FedNLR 66.11±0.89 73.92±0.60 80.61±0.11 81.65±0.22 44.67±0.48 47.14±0.48 51.29±0.28 51.77±0.15
Fe
dDyn + FedNLR 48.29±2.14 75.02±0.48 84.17±0.36 85.82±0.31 34.37±0.73 34.78±0.48 35.59±0.33 36.52±0.25
Table 2: The accuracy on the CINIC dataset.
MethodShards(S) Dir(𝛼)
2 4 0.05 0.1
FedAvg 50.88 53.03 43.37 47.51
FedProx 52.12 52.75 43.17 46.74
Scaffold 52.07 52.42 43.33 46.91
FedNova 51.07 52.54 41.55 46.63
FedRS 51.19 52.13 44.47 46.72
FedLC 51.49 52.81 43.64 47.67
CCVR 50.59 51.31 42.66 46.93
FedDyn 47.56 55.99 42.83 50.22
FedNLR 52.17 52.30 43.37 47.81
FedDyn+FedNLR 49.49 56.24 44.69 51.93
gain an additional 2.91% when integrated with FedNLR . Combining
FedDyn with FedNLR performs the best in these NonIID situations.
Orthogonal Combination with other algorithms. FedNLR can
be divided into two methods: the aggregation method, which can
be discarded, and the learning rate scheduler, which can be inte-
grated with many other methods, such as FedRS, FedLC, FedNova,
and FedDyn. With only a few lines of code, the customized SGD
is capable of handling a vector-form learning rate for one layer.
Using the customized SGD, other methods are compatible with the
learning rate scheduler. The integration of FedDyn with FedNLR
demonstrates the capability to achieve state-of-the-art accuracy,
particularly under conditions of low Non-IID levels. This suggests
a synergistic effect between the two methods, enhancing model
performance in scenarios characterized by more homogeneous data
distributions. Because the accuracy of a model is directly correlated
with the utility of the generated activation map: a more accurate
model yields a more informative activation map. Consequently, aTable 3: 10% clients participate in each round. The compari-
son of final test accuracy on the two datasets.
Method CIFAR-10(%)
Shards (𝑛,𝑆) (100, 4) (100, 8)
FedAvg 72.94±1.33 75.03±1.27
FedProx 72.33±1.28 75.27±1.19
Scaffold 72.60±1.35 75.63±0.97
FedNova 72.61±1.44 74.65±1.13
FedRS 72.83±1.16 74.88±1.00
FedLC 72.72±1.25 74.32±1.07
FedDyn 77.51±1.84 83.32±0.51
FedNLR 73.38±1.48 76.14±1.31
FedDyn + FedNLR 80.42±1.59 85.26±0.57
Dirichlet (𝑛,𝛼) (100, 0.1) (100, 1)
FedAvg 63.30±2.31 77.83±0.95
FedProx 62.42±2.37 77.03±1.71
Scaffold 61.79±2.34 77.27±0.71
FedNova 55.49±5.80 78.21±0.97
FedRS 63.58±2.42 77.98±0.92
FedLC 63.17±2.59 77.97±0.66
FedDyn 63.98±2.39 85.22±0.34
FedNLR 63.40±2.51 78.73±1.47
FedDyn + FedNLR 64.16±2.37 86.10±0.39
learning rate scheduler derived from this enhanced activation map
can provide more insightful guidance for model training.
7.3 Hyper-Parameter Sensitivity Analysis and
Ablation Study
Scale Factor Normalization with Range Control. The purpose
3075KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.
0.005 0.01 0.05
Learning Rate404244464850Test Accuracy (%)
40.0443.05
40.7144.6745.3948.78FedAvg
FedNLR
Figure 2: Results of FedNLR using different learning rates with
CIFAR-100-Dirichlet(20, 0.05).
0 1 2 3 4 5 6 7 8
Layer Index2.53.03.54.04.55.05.56.06.5Temperature
Figure 3: The temperature 𝑇𝑙for each layer 𝑙.
of the𝜇is to limit the scale factor for learning rates of a weighted
layer in an interval where the largest scale factor equals the mul-
tiplication of 𝜇and the smallest scale factor. Also, the average
value of the scale factor in one layer is 1. As depicted in Fig. 3, the
temperature of the SoftMax function is used for the scale factor
normalization. The variance of temperature shows the crucial role
of𝜇in reducing manual setting efforts. Empirically, the second
layer typically requires the highest temperature, because it consists
of the most neurons across all layers.
Ablation Experiment of Scaling Each Layer. Fig. 4 shows the
ablation experiment results, proving each layer’s performance im-
provement. Most points are above the red line of FedAvg, proving
the hyperparameter’s insensitivity. There is not a constant num-
ber capable of continuously improving performance, but in most
situations, 1.75 and 2 show improvements. And the hidden layers’
variance is bigger than the shallow layers and the classifier. This
is evidence that hidden layers’ neuron values vary most concern-
ing the change of classes. Intuitively, if we simply sum up all the
improvements of all layers with manually chosen 𝜇, then the final
accuracy should be best. However, Fig. 5 proves that the setting
of the math formula Eq. (17) calculated 𝜇is better, while manu-
ally chosen 𝜇doesn’t perform well due to the interaction attribute
among all layers inside neural networks. Also, if we only con-
sider the width or depth, then the performance is not comparable
with the best result. Owing to constraints in computational power,
we have identified an optimal combination of hyper-parameters
within a limited range. As the two parameters increase from 1 to
0 1 2 3 4 5 6 7 8
Layer Index38.0040.0040.7142.0044.00Accuracy (%)=1.25
=1.5
=1.75
=2
=2.25
=2.5
=2.75
=3
Figure 4: Experimental result of 9 layers, with individ-
ual hyper-parameter 𝜇, under the setting of CIFAR-100-
Dirichlet(20, 0.05).
Manual1.75
(0.45,0)(1,0)(0,0.3)(0,1)
(0.45,0.3)(0.5,0.5)(1,1)(2,2)(4,4)(6,6)
(a1,a2) of the Emperical Formula424344454647Test Accuracy (%)
42.742.8
42.2543.81
42.4744.544.67
43.7945.845.746.23
46.0
Figure 5: Performance improvements of FedNLR with differ-
ent𝜇under the setting of CIFAR-100-Dirichlet(20, 0.05).
4 and 6, the average accuracy increases to 46.23, and then slightly
decreases, but the overall performance is still optimal. This may
indicate that the moderate increase of depth and width parameters
can improve the model performance, but the effect will decrease
after exceeding a certain range. The main experiment was initially
conducted using(0.45,0.3). However, potential improvements are
expected if this is changed to (1,1). Consequently, we configure the
equation Eq. (17) as the following empirical mathematical formula:
𝜇𝑙=1+𝑙
𝐿+log10(𝑀𝑙), which also applies to other models simply.
8 Conclusion
This paper conducts research on the problem of model drift in fed-
erated learning for training deep neural networks. We provide a deep
understanding of the formation process of model drift. Specifically,
instead of viewing the neural network as a whole, we decompose it
into a set of neurons and track the changes of each neuron during
local training. We identify that each class is bound to a specific
set of neurons and their activation can be reduced during the local
training process when this class is not included by the client. Based
on this motivation, we propose a simple yet effective method named
FedNLR which adopts neuron-wise learning rates to mitigate the
3076FedNLR: Federated Learning with Neuron-wise Learning Rates KDD ’24, August 25–29, 2024, Barcelona, Spain
drift of neurons. Our theoretical results guarantee the convergence
ofFedNLR and empirical results also demonstrate its effectiveness.
Despite the analysis for understanding the evolution of specific
neurons, this paper is limited to the two-layer neural network which
has great potential to be unleashed. Recently, many explanation
methods for deep neural networks have been proposed, which may
shed light on expand the analysis. For example, we may utilize
the Shapley value to identify accurate bound neurons to specific
samples. We leave them as the future work.
Acknowledgments
This work is supported in part by National Natural Science Founda-
tion of China under grants 62302184, 62376103, 62206102, Science
and Technology Support Program of Hubei Province under grant
2022BAA046, Ant Group through CCF-Ant Research Fund,and
two grant from the Research Grants Council of the Hong Kong
Special Administrative Region, China (Project No. PolyU15222621,
PolyU15225023). Besides, the research is supported under the Na-
tional Key R&D Program of China (2022ZD0160201) and the RIE2020
Industry Alignment Fund - Industry Collaboration Projects (IAF-
ICP) Funding Initiative, as well as cash and in-kind contributions
from the industry partner(s).
References
[1]Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina,
Paul N. Whatmough, and Venkatesh Saligrama. 2021. Federated Learning Based
on Dynamic Regularization. In 9th International Conference on Learning Repre-
sentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[2]Muhammad Ammad-ud-din, Elena Ivannikova, Suleiman A. Khan, Were Oyomno,
Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. 2019. Federated Collaborative
Filtering for Privacy-Preserving Personalized Recommendation System. CoRR
abs/1901.09888 (2019).
[3]Zachary Charles and Jakub Konečný. 2021. Convergence and Accuracy Trade-Offs
in Federated Learning and Meta-Learning. In The 24th International Conference
on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual
Event. 2575–2583.
[4]Luke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey.
2018. CINIC-10 is not ImageNet or CIFAR-10. CoRR abs/1810.03505 (2018).
[5]Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. 2020. Personalized
Federated Learning with Moreau Envelopes. In Proceedings of Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems, NeurIPS.
[6]Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. 2021. Exploring deep neu-
ral networks via layer-peeled model: Minority collapse in imbalanced training.
Proceedings of the National Academy of Sciences 118, 43 (2021), e2103091118.
[7]Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. 2022.
FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and
Correction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2022, New Orleans, LA, USA, June 18-24. 10102–10111.
[8]Yonghai Gong, Yichuan Li, and Nikolaos M. Freris. 2022. FedADMM: A Robust
Federated Deep Learning Framework with Adaptivity to System Heterogeneity.
In38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala
Lumpur, Malaysia, May 9-12, 2022. 2575–2587.
[9]Pengfei Guo, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, and Vishal M. Patel.
[n. d.]. Multi-Institutional Collaborations for Improving Deep Learning-Based
Magnetic Resonance Image Reconstruction Using Federated Learning. In IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June
19-25, 2021. 2423–2432.
[10] X. Y. Han, Vardan Papyan, and David L. Donoho. 2022. Neural Collapse Un-
der MSE Loss: Proximity to and Dynamics on the Central Path. In The Tenth
International Conference on Learning Representations, ICLR, Virtual, April 25-29,
2022.
[11] Hangfeng He and Weijie J. Su. 2022. A Law of Data Separation in Deep Learning.
CoRR abs/2210.17020 (2022).
[12] Ming Hu, Yue Cao, Anran Li, Zhiming Li, Chengwei Liu, Tianlin Li, Mingsong
Chen, and Yang Liu. 2024. FedMut: Generalized Federated Learning via Stochastic
Mutation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38.
12528–12537.[13] Ming Hu, Zeke Xia, Dengke Yan, Zhihao Yue, Jun Xia, Yihao Huang, Yang Liu,
and Mingsong Chen. 2023. GitFL: Uncertainty-Aware Real-Time Asynchronous
Federated Learning Using Version Control. In In Proceedings of IEEE Real-Time
Systems Symposium (RTSS). IEEE, 145–157.
[14] Ming Hu, Peiheng Zhou, Zhihao Yue, Zhiwei Ling, Yihao Huang, Anran Li,
Yang Liu, Xiang Lian, and Mingsong Chen. 2024. FedCross: Towards Accurate
Federated Learning via Multi-Model Cross-Aggregation. In IEEE International
Conference on Data Engineering (ICDE). IEEE, 2137–2150.
[15] Like Hui, Mikhail Belkin, and Preetum Nakkiran. 2022. Limitations of Neural Col-
lapse for Understanding Generalization in Deep Learning. CoRR abs/2202.08384
(2022).
[16] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. 2023. FedExP: Speed-
ing Up Federated Averaging via Extrapolation. In The Eleventh International
Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5.
[17] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Se-
bastian U. Stich, and Ananda Theertha Suresh. [n. d.]. SCAFFOLD: Stochastic
Controlled Averaging for Federated Learning. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event.
5132–5143.
[18] Geeho Kim, Jinkyu Kim, and Bohyung Han. 2022. Communication-Efficient
Federated Learning with Acceleration of Global Momentum. CoRR abs/2201.03172
(2022).
[19] Jinkyu Kim, Geeho Kim, and Bohyung Han. [n. d.]. Multi-Level Branched Regular-
ization for Federated Learning. In International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA. 11058–11073.
[20] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[21] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022. Federated Learning
on Non-IID Data Silos: An Experimental Study. In ICDE. IEEE, 965–978.
[22] Qinbin Li, Bingsheng He, and Dawn Song. [n. d.]. Model-Contrastive Federated
Learning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2021, virtual, June 19-25, 2021. 10713–10722.
[23] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020. Federated Optimization in Heterogeneous Networks. In
Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA,
March 2-4, 2020.
[24] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,
and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
Proceedings of Machine Learning and Systems 2 (2020), 429–450.
[25] Xin-Chun Li, Yi-Chu Xu, Shaoming Song, Bingshuai Li, Yinchuan Li, Yunfeng
Shao, and De-Chuan Zhan. [n. d.]. Federated Learning with Position-Aware
Neurons. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. 10072–10081.
[26] Xin-Chun Li and De-Chuan Zhan. 2021. FedRS: Federated Learning with Re-
stricted Softmax for Label Distribution Non-IID Data. In KDD ’21: The 27th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,
Singapore, August 14-18, 2021. 995–1005.
[27] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. [n. d.]. FedDG:
Federated Domain Generalization on Medical Image Segmentation via Episodic
Learning in Continuous Frequency Space. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. 1013–1023.
[28] Jianfeng Lu and Stefan Steinerberger. 2020. Neural Collapse with Cross-Entropy
Loss. CoRR abs/2012.08465 (2020).
[29] Kangyang Luo, Xiang Li, Yunshi Lan, and Ming Gao. 2023. GradMA: A Gradient-
Memory-based Accelerated Federated Learning with Alleviated Catastrophic
Forgetting. The IEEE/CVF Conference on Computer Vision and Pattern Recognition,
(CVPR 2023), Vancouver, Canada (2023).
[30] Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. 2021.
No Fear of Heterogeneity: Classifier Calibration for Federated Learning with
Non-IID Data. In Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual. 5972–5984.
[31] Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter
Richtárik. 2020. From Local SGD to Local Fixed-Point Methods for Federated
Learning. In Proceedings of the 37th International Conference on Machine Learning,
ICML, 13-18 July, Virtual Event. 6692–6701.
[32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics. 1273–1282.
[33] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas. 2017. Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In Proceedings of the 20th International Conference
on Artificial Intelligence and Statistics, AISTATS.
[34] Vardan Papyan. [n. d.]. Traces of Class/Cross-Class Structure Pervade Deep
Learning Spectra. Journal of Machine Learning Research, 2020, 252(21):1–64.
([n. d.]).
[35] Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays.
2019. Federated Learning for Emoji Prediction in a Mobile Keyboard. CoRR
3077KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.
abs/1906.04329 (2019).
[36] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021. Adaptive Fed-
erated Optimization. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7.
[37] Yan Sun, Li Shen, Tiansheng Huang, Liang Ding, and Dacheng Tao. 2023. Fed-
Speed: Larger Local Interval, Less Communication Round, and Higher General-
ization Accuracy. In The Eleventh International Conference on Learning Represen-
tations, ICLR 2023, Kigali, Rwanda, May 1-5.
[38] Chunnan Wang, Xiang Chen, Junzhe Wang, and Hongzhi Wang. June 18-24, 2022.
ATPFL: Automatic Trajectory Prediction Model Design under Federated Learning
Framework. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR, New Orleans, LA, USA. 6553–6562.
[39] Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng Zhan, and Zhi-
gang Zeng. 2023. DaFKD: Domain-aware Federated Knowledge Distillation. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023,
Vancouver, BC, Canada, June 17-24, 2023. IEEE, 20412–20421.
[40] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. 2020.
Tackling the Objective Inconsistency Problem in Heterogeneous Federated Op-
timization. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, Decem-
ber 6-12, 2020, virtual.
[41] Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael G. Rabbat. 2020.
SlowMo: Improving Communication-Efficient Distributed SGD with Slow Mo-
mentum. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020.
[42] Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing Gao. 2023. Anchor
Sampling for Federated Learning with Partial Client Participation. In Proceedings
of the 40th International Conference on Machine Learning. 37379–37416.
[43] Feijie Wu, Song Guo, Haozhao Wang, Haobo Zhang, Zhihao Qu, Jie Zhang, and
Ziming Liu. 2023. From Deterioration to Acceleration: A Calibration Approach
to Rehabilitating Step Asynchronism in Federated Optimization. IEEE Trans.
Parallel Distributed Syst. 34, 5 (2023), 1548–1559.
[44] An Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger Roth, Ali Hatamizadeh, Can
Zhao, Daguang Xu, Heng Huang, and Ziyue Xu. [n. d.]. Closing the Generalization
Gap of Cross-silo Federated Medical Image Segmentation. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA,
June 18-24, 2022. 20834–20843.
[45] Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. 2021. FedCM:
Federated Learning with Client-level Momentum. CoRR abs/2106.10874 (2021).
[46] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald,
Nghia Hoang, and Yasaman Khazaeni. 2019. Bayesian Nonparametric Federated
Learning of Neural Networks. In Proceedings of the 36th International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika
Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 7252–7261.
[47] Jie Zhang, Zhiqi Li, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Chao Wu.
[n. d.]. Federated Learning with Label Distribution Skew via Logits Calibration.
InInternational Conference on Machine Learning, ICML 2022, 17-23 July 2022,
Baltimore, Maryland, USA. 26311–26329.
A Verification Experiment
To validate the motivations, we propose training a 2-layer MLP with
25 hidden neurons on the Mnist dataset, facilitating the visualization
of neuron activations. In this experiment, each client is assigned
a single data category to create a NonIID setting. Moreover, we
demonstrate the change in activation states before and after local
training on clients 1 and 2 by selecting a model from an intermediate
training phase, where the chosen model has reached an accuracy of
88%. Below, we verify our viewpoints by comparing the activation
values of the model’s neurons on the data of clients 1 and 2, both
before local training and post-local training; these activation values
are presented in Table 4.
The existence of bounded neurons. Based on the original activation
values on both clients, it can be seen that there is a significant
difference in the bounded neurons between client1 and client2, thus
validating our theory about bounded neurons.
The existence of neuron drift. It can be observed that the activation
values of the model’s bounded neurons on client2 decrease after
local training on client1 using FedAvg.Our method mitigates neuron drift. Based on the activation values
after local training on client1 using FedNLR , it can be seen that the
activation values of the bounded neurons also decrease on client2,
while the extent of the decrease is smaller compared to FedAvg,
thereby demonstrating that our method alleviates neuron drift.
B Proof of Theorem 3
B.1 Lemmas
Lemma 4. We denote ˆ𝜂𝑘by the learning rates for all parame-
tersˆ𝜂𝑘=[ˆ𝜂𝑘
1,1,..., ˆ𝜂𝑘
1,𝑀1,..., ˆ𝜂𝑘
𝐿,𝑀𝐿]𝑇. Let𝜇max=max(𝜇1,...,𝜇𝐿),
𝑀min=min{𝑀1,...,𝑀𝐿}, and𝑀max=max{𝑀1,...,𝑀𝐿}. Ifˆ𝜂max
is the maximum learning rate for all parameters of all clients, i.e.,
ˆ𝜂max=max(max(ˆ𝜂1),..., max(ˆ𝜂𝐾)),
then its upper bound is ˆ𝜂max≤𝜇max𝜂/(𝜇max+𝑀min−1). Ifˆ𝜂minis
the minimum learning rate for all parameters of all clients, i.e., ˆ𝜂min=
min(min(ˆ𝜂1),..., min(ˆ𝜂𝐾)), then its lower bound is 𝜂/(𝜇max(𝑀max−
1)+1)≤ ˆ𝜂min.
Proof : Based on the definition of learning rate ˆ𝜂in (15), i.e.,
𝑞𝑙,𝑚=𝑒¯ℎ𝑙,𝑚/𝑇𝑙
Í𝑀𝑙
𝑖=1𝑒¯ℎ𝑙,𝑖/𝑇𝑙,ˆ𝜂𝑙,𝑚=𝜂·𝑞𝑙,𝑚, (21)
we can directly obtain thatÍ𝑀𝑙
𝑚=1ˆ𝜂𝑙,𝑚=𝜂for each layer 𝑙. Further,
based on the definition of 𝜇𝑙in (16), i.e.,𝜇𝑙=ˆ𝜂𝑙,max
ˆ𝜂𝑙,min, we can compute
the upper bound of ˆ𝜂𝑙,maxas
ˆ𝜂𝑙,max≤𝜇𝑙𝜂/(𝜇𝑙+𝑀𝑙−1) (22)
when the learning rates of all 𝑀𝑙−1neurons are ˆ𝜂𝑙,min. By denoting
𝜇max=max𝜇1,...,𝜇𝐿and𝑀min=min𝑀1,...,𝑀𝐿, we have the
upper bound of the learning rate ˆ𝜂𝑙,maxas
ˆ𝜂max≤𝜇max𝜂/(𝜇max+𝑀min−1). (23)
Similarly, we have the lower bound of the learning rate ˆ𝜂𝑙,minas
𝜂/(𝜇max(𝑀max−1)+1)≤ ˆ𝜂min, (24)
which completes the proof.
Lemma 5. The difference between the partially averaged gradient
and the global gradient is bounded, i.e.,
E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)−1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2≤ˆ𝜂2
max𝑀21−𝑟
𝑟.
Proof : We denote 1(𝑘)by the indicator function where 1(𝑘)=1
when the𝑘-th client is selected and otherwise, 1(𝑘)=0. We have
E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)−1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
=E1
𝐾𝑟𝐾∑︁
𝑘=11(𝑘)ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)−1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
=E1
𝐾𝐾∑︁
𝑘=1(1−1(𝑘)
𝑟)ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
3078FedNLR: Federated Learning with Neuron-wise Learning Rates KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: The activation of hidden neurons. The activation of bolded fonts represent bounded neurons. Activation change
denotes the value change of bounded neurons on data of client 2 (C-2) between before and after training on client 1 (C-1).
Neur
onID 1-8 9-16 17-25
Original
activation 0.39
0.16 0.12 0.24 0.17 0.85 0.1
0.49 0.02
0.32 0.34 0.02 0.07
on data of C-1 0.02
0 0 0.01 0
0.02 0.31 0.24 0.35
0.29 0.46 0.21 /
Original
activation 0.07
0.33 0.04 0.06 0
0.1 0.23 0.14 0.64 0.22
0 0.2 0.07
on data of C-2 0.56 0
0 0.31 0
0.01 0.23 0.01 0
0.19 0.09 0.52 /
A
ctivation on C-2 0.08
0.31 0.04 0.08 0
0.18 0.19 0.18 0.61 0.25
0 0.17 0.07
After C-1 via FedAvg 0.50 0
0 0.31 0
0.01 0.22 0.01 0.01
0.14 0.11 0.46 /
A
ctivation Change -0.03 -0.06 -0.06
A
ctivation on C-2 0.08
0.31 0.04 0.08 0
0.19 0.19 0.19 0.63 0.25
0 0.17 0.07
After C-1 via FedNLR 0.53 0
0 0.31 0
0.01 0.22 0.01 0.01
0.15 0.11 0.48 /
A
ctivation Change -0.01 -0.03 -0.04
≤1
𝐾𝐾∑︁
𝑘=1E(1−1(𝑘)
𝑟)2ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
≤ˆ𝜂2max
𝐾𝐾∑︁
𝑘=1E(1−1(𝑘)
𝑟)2∥∇𝐹𝑘(w𝑘
𝑡∗)∥2
2
=ˆ𝜂2max
𝐾𝐾∑︁
𝑘=1Eh
𝑟(1−1
𝑟)2∥∇𝐹𝑘(w𝑘
𝑡∗)∥2
2+(1−𝑟)∥∇𝐹𝑘(w𝑘
𝑡∗)∥2
2i
≤ˆ𝜂2
max𝑀21−𝑟
𝑟, (25)
which completes the proof.
B.2 Proof of Theorem 3
Without losing generality, we consider the data size 𝐷𝑖=𝐷𝑗, for
any two clients 𝑖and𝑗. We denote the local iteration number 𝑒
of the𝑡-th round as 𝑡∗=𝑡𝐸+𝑒. We also consider the number of
participated clients K𝑡remains unchanged across different rounds
with a ratio 𝑟from total𝐾clients. Besides, we introduce another
model for ease of analysis as
v𝑡∗=1
𝐾𝑟∑︁
𝑘∈K𝑡w𝑘
𝑡∗=v𝑡∗−1−1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗−1),(26)
where⊙denotes the element-wise product. ˆ𝜂𝑘denotes the vec-
tor of the learning rates for the parameters of all neurons ˆ𝜂𝑘=
[ˆ𝜂𝑘
1,1,..., ˆ𝜂𝑘
1,𝑀1,..., ˆ𝜂𝑘
𝐿,𝑀𝐿]𝑇.Obviously, we have v𝑡∗=w𝑡∗as𝑡∗≡
0(mod𝐸). Then, based on Assumption 1, we have
E𝐹(v𝑡∗+1)≤E𝐹(v𝑡∗)+E<∇𝐹𝑘(v𝑡∗),v𝑡∗+1−v𝑡∗>+𝐿
2E∥v𝑡∗+1−v𝑡∗∥2
2.(27)
According to the updated formula (26), we have
E∥v𝑡∗+1−v𝑡∗∥2
2=E∥1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗)∥2
2
=E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗)−1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)
+1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2=
(𝑎)E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2+E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗)−1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
=
(𝑏)1
𝐾2𝑟2E∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙ ∇𝑓𝑘(w𝑘
𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)2
2
+E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2=
(𝑐)E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
+1
𝐾2𝑟E𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙ ∇𝑓𝑘(w𝑘
𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)2
2
≤
(𝑑)ˆ𝜂2max
𝐾2𝑟E𝐾∑︁
𝑘=1∇𝑓𝑘(w𝑘
𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)2
2+E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
≤
(𝑒)ˆ𝜂2max𝜎2
𝑙
𝐾𝑟+E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2=
(𝑓)E1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
+ˆ𝜂2max𝜎2
𝑙
𝐾𝑟+E1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)−1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
≤
(𝑔)ˆ𝜂2max𝜎2
𝑙
𝐾𝑟+ˆ𝜂2
max𝑀21−𝑟
𝑟+E1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2. (28)
where (a) and (f) holds because E∥𝑎∥2
2=E∥𝑎−E𝑎∥2
2+∥E𝑎∥2
2. (b)
is due to E𝑓𝑘(w𝑘
𝑡∗)=∇𝐹𝑘(w𝑘
𝑡∗). (c) holds because EÍ
𝑘∈K𝑡𝑎𝑘=
𝑟EÍ𝐾
𝑘=1𝑎𝑘.ˆ𝜂maxin the inequality (d) denotes the upper bound of
the learning rate provided in Lemma 4. (e) is derived by Assump-
tion 2. (g) is based on Lemma 5. We further note that
E<∇𝐹(v𝑡∗),v𝑡∗+1−v𝑡∗>
=E<∇𝐹(v𝑡∗),−1
𝐾𝑟∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗)>
=−E𝐿∑︁
𝑙=1𝑀𝑙∑︁
𝑚=11
1
𝐾Í𝐾
𝑘=1ˆ𝜂𝑘
𝑙,𝑚<1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹(v𝑡∗)𝑙,𝑚,
1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹𝑘(w𝑘
𝑡∗)𝑙,𝑚>=−1
2E𝐿∑︁
𝑙=1𝑀𝑙∑︁
𝑚=11
1
𝐾Í𝐾
𝑘=1ˆ𝜂𝑘
𝑙,𝑚
3079KDD ’24, August 25–29, 2024, Barcelona, Spain Haozhao Wang et al.

∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹(v𝑡∗)𝑙,𝑚∥2
2+∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹𝑘(w𝑘
𝑡∗)𝑙,𝑚∥2
2
−∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹(v𝑡∗)𝑙,𝑚−1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹𝑘(w𝑘
𝑡∗)𝑙,𝑚∥2
2
=−1
2𝐾E𝐿∑︁
𝑙=1𝑀𝑙∑︁
𝑚=1𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∥∇𝐹(v𝑡∗)𝑙,𝑚∥2
2
−1
2E𝐿∑︁
𝑙=1𝑀𝑙∑︁
𝑚=11
1
𝐾Í𝐾
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∇𝐹𝑘(w𝑘
𝑡∗)𝑙,𝑚∥2
2
+E𝐿∑︁
𝑙=1𝑀𝑙∑︁
𝑚=11
2
𝐾Í𝐾
𝑘=1ˆ𝜂𝑘
𝑙,𝑚∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘
𝑙,𝑚 ∇𝐹(v𝑡∗)𝑙,𝑚−∇𝐹𝑘(w𝑘
𝑡∗)𝑙,𝑚∥2
2
≤−ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2−1
2ˆ𝜂maxE∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)∥2
2
+1
2ˆ𝜂min∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙ ∇𝐹(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)∥2
2. (29)
Bringing (29) and (29) back to (27), we have
E𝐹(v𝑡∗+1)≤E𝐹(v𝑡∗)−1
2ˆ𝜂maxE∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)∥2
2
−ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2+1
2ˆ𝜂min∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙ ∇𝐹(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)∥2
2
+ˆ𝜂2max𝜎2
𝑙𝐿
2𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
2𝑟+𝐿
2E1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)2
2
≤E𝐹(v𝑡∗)+ 𝐿
2−1
2ˆ𝜂maxE∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙∇𝐹𝑘(w𝑘
𝑡∗)∥2
2
−ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2+ˆ𝜂2max𝜎2
𝑙𝐿
2𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
2𝑟
+1
2ˆ𝜂min∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙ ∇𝐹(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)∥2
2(30)
≤E𝐹(v𝑡∗)−ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2+ˆ𝜂2max𝜎2
𝑙𝐿
2𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
2𝑟
+1
2ˆ𝜂min∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙ ∇𝐹(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)∥2
2≤E𝐹(v𝑡∗)
−ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2+2(ˆ𝜂𝑘max)3𝐸2𝑀2
ˆ𝜂min+ˆ𝜂2max𝜎2
𝑙𝐿
2𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
2𝑟,
where the last-second inequality holds when𝐿
2−1
2ˆ𝜂max<0, i.e.,
ˆ𝜂max<1
𝐿. The last inequality holds because
E∥1
𝐾𝐾∑︁
𝑘=1ˆ𝜂𝑘⊙(∇𝐹(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗))∥2
2≤ˆ𝜂2max
𝐾𝐾∑︁
𝑘=1E∥∇𝐹𝑘(v𝑡∗)−∇𝐹𝑘(w𝑘
𝑡∗)∥2
2≤ˆ𝜂2max𝐿2
𝐾𝐾∑︁
𝑘=1E∥v𝑡∗−w𝑘
𝑡∗∥2
2
=ˆ𝜂2max𝐿2
𝐾𝐾∑︁
𝑘=1E1
𝐾𝑟𝑒∑︁
𝜏=0∑︁
𝑘∈K𝑡ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)−𝑒∑︁
𝜏=0ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)2
2
≤2ˆ𝜂2max𝐿2𝑒
𝐾𝑟𝐾∑︁
𝑘=1𝑒∑︁
𝜏=0E∑︁
𝑘∈K𝑡∥ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)∥2
2
+2ˆ𝜂2max𝐿2𝑒
𝐾𝐾∑︁
𝑘=1𝑒∑︁
𝜏=0E∥ˆ𝜂𝑘⊙∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)∥2
2
≤2(ˆ𝜂max)3𝑒
𝐾𝑒∑︁
𝜏=0𝐾∑︁
𝑘=1E∥∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)∥2
2
+2(ˆ𝜂max)3𝑒
𝐾𝑒∑︁
𝜏=0𝐾∑︁
𝑘=1E∥∇𝑓𝑘(w𝑘
𝑡∗−𝑒+𝜏)∥2
2≤4(ˆ𝜂𝑘
max)3𝐸2𝑀2.(31)
Since ˆ𝜂min∥∇𝐹(v𝑡∗)∥2
2≤∥√︃
ˆ𝜂𝑘⊙∇𝐹𝑘(v𝑡∗)∥2
2, re-arranging (30)
obtains
ˆ𝜂min
2𝐾E𝐾∑︁
𝑘=1∥∇𝐹(v𝑡∗)∥2
2≤E𝐹(v𝑡∗)−E𝐹(v𝑡∗+1)
+2(ˆ𝜂𝑘max)3𝐸2𝑀2
ˆ𝜂min+ˆ𝜂2max𝜎2
𝑙𝐿
2𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
2𝑟. (32)
By taking a sum of both sides of (32) from 𝑡∗=1to𝑇∗, we have
𝑇∗∑︁
𝑡∗=1E∥∇𝐹(v𝑡∗)∥2
2≤2(E𝐹(v1)−E𝐹(v𝑇∗))
ˆ𝜂min
+4(ˆ𝜂𝑘max)3𝐸2𝑀2𝑇
ˆ𝜂2
min+ˆ𝜂2max𝜎2
𝑙𝐿𝑇
ˆ𝜂min𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿𝑇
ˆ𝜂min𝑟. (33)
Denote𝐹∗as the global optima. SinceÍ𝑇
𝑡=1E∥∇𝐹(w𝑡)∥2
2≤Í𝑇∗
𝑡∗=1E∥∇𝐹(v𝑡∗)∥2
2,
by setting𝜂=√︃
𝐾
𝑇, based on Lemma 4, we have
1
𝑇𝑇∑︁
𝑡=1E∥∇𝐹(w𝑡)∥2
2≤2(𝐹(w1)−𝐹∗))
ˆ𝜂min𝑇+4(ˆ𝜂𝑘max)3𝐸2𝑀2
ˆ𝜂2
min
+ˆ𝜂2max𝜎2
𝑙𝐿
ˆ𝜂min𝐾𝑟+(1−𝑟)ˆ𝜂2max𝑀2𝐿
ˆ𝜂min𝑟
≤2(𝐹(w1)−𝐹∗))√︁
𝜇max(𝑀max−1)+1
√
𝐾𝑇
+4𝜇3max(𝜇max(𝑀max−1)+1)2𝐸2𝑀2
(𝜇max+𝑀min−1)3√
𝑇
+𝜇2max(𝜇max(𝑀max−1)+1)𝜎2
𝑙𝐿
(𝜇max+𝑀min−1)2𝑟√
𝐾𝑇
+(1−𝑟)𝜇2max(𝜇max(𝑀max−1)+1)𝑀2𝐿
(𝜇max+𝑀min−1)2𝑟√
𝑇, (34)
which completes the proof.
3080