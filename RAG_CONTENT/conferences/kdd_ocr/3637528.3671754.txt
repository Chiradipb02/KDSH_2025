CoMAL: Contrastive Active Learning for Multi-Label Text
Classification
Cheng Peng
State Key Lab of Blockchain and Data
Security, Zhejiang University
Hangzhou, China
chengchng@zju.edu.cnHaobo Wang∗
School of Software Technology,
Zhejiang University
Ningbo, China
wanghaobo@zju.edu.cnKe Chen∗
State Key Lab of Blockchain and Data
Security, Zhejiang University
Hangzhou, China
chenk@zju.edu.cn
Lidan Shou
State Key Lab of Blockchain and Data
Security, Zhejiang University
Hangzhou, China
should@zju.edu.cnChang Yao
School of Software Technology,
Zhejiang University
Ningbo, China
changy@zju.edu.cnRunze Wu
Fuxi AI Lab, NetEase Corp.
Hangzhou, China
wurunze1@corp.netease.com
Gang Chen
State Key Lab of Blockchain and Data
Security, Zhejiang University
Hangzhou, China
cg@zju.edu.cn
ABSTRACT
Multi-label text classification (MLTC) allows a given text to be as-
sociated with multiple labels, which well suits many real-world
data mining scenarios. However, the annotation effort of MLTC is
inevitably expensive and time-consuming. Although multi-label ac-
tive learning provides a cost-effective solution, it still faces two ma-
jor challenges: (i) constructing decent feature space to distinguish
the confusing semantics of different labels; (ii) defining proper sam-
pling criteria to measure a sample’s joint effect over the entire label
space. To bridge these gaps, we propose a Contrastive Multi-label
Active Learning framework (CoMAL) that gives an effective data ac-
quisition strategy. Specifically, a contrastive decoupling mechanism
is introduced to fully release the semantic information of multiple
labels into the latent space. Then, we devise a hybrid criterion that
balances two data value measures: (i) similarity-enhanced label car-
dinality inconsistency reflects the uncertainty of data predictions.
(ii)positive feature diversity evaluates the positive-propensity se-
mantic diversity to handle the label sparsity. Extensive experiments
demonstrate that our CoMAL outperforms the current state-of-
the-art multi-label active learning approaches. Code for CoMAL is
available at https://github.com/chengzju/CoMAL.
∗Both authors are the corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671754CCS CONCEPTS
•Computing methodologies →Natural language processing;
Active learning settings.
KEYWORDS
Multi-label Learning; Active Learning; Contrastive Learning
ACM Reference Format:
Cheng Peng, Haobo Wang, Ke Chen, Lidan Shou, Chang Yao, Runze Wu,
and Gang Chen. 2024. CoMAL: Contrastive Active Learning for Multi-Label
Text Classification. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671754
1 INTRODUCTION
Multi-label text classification (MLTC) is a fundamental NLP task
that allows each text to be tagged with multiple labels, which suits
many real-world data annotation scenarios under large label space
[10,33,43]. Nowadays, MLTC has received extensive attention and
promoted various applications such as sentiment analysis [ 20,21],
E-commerce dynamic search advertising [ 14,24], article annotating
[3,15], and so on. However, under a potentially large and sparse
label space, associating multiple non-exclusive labels for massive
samples will inevitably suffer prohibitive annotation costs.
Active learning (AL) [ 28] is a pivotal learning paradigm that
substantially reduces the manual annotation burden without com-
promising performance. AL iteratively queries the most valuable
samples and exploits the ever-increasing labeled pool to improve
model performance. Informativeness [32,38] and representativeness
[4,13] are two widely used value criteria: the former evaluates
the potential of samples to reduce the model’s generalization error,
while the latter prefers samples that can well reflect the overall
data distribution. Arguably, a properly devised data acquisition
 
2364
KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
Negative samples
Positive samples
(a) Latent space of CoMAL w/o cl
Negative samples
Positive samples
Prototypes (b) Latent space of CoMAL
prototype 1
prototype 2prototype 3
prototype 4prototype 5
prototype 6Points selected
   for class 3
Points near the
prototype 3 are
not selected for
samplingSelected samples
Not-selected samples
Prototypes (c) Data acquisition
Figure 1: T-SNE visualization of latent feature embeddings on AAPD. Based on sub-figure (b), sub-figure (c) shows the embedding
distribution of instances sampled or not sampled by CoMAL.
Embedding Space
Encoder
TextEncoder
(Feature Decoupling)
Text
Text-wise Embedding Text-label pair -wise EmbeddingPos.Neg.
Figure 2: Conventional text-wise embedding space and text-
label pair-wise embedding space (ours), which is better for
exploring the joint semantics of multiple labels.
should ensure that the model maintains promising performance
under limited supervision resources.
Despite the promise, applying the AL paradigm in the Multi-
Label scenario (MLAL) inevitably encounters two thorny obstacles.
The first crucial challenge is to construct a decent feature represen-
tation space that can reflect the joint semantics of multiple labels.
Most current works [ 1,26,29] mostly use a single holistic embed-
ding to represent each multi-label sample (as shown in the left
of Figure 2), which cannot effectively distinguish different seman-
tic information, resulting confusion on joint label semantics. The
second challenge is to define proper sampling criteria to measure
multi-label samples. Unlike single-label tasks, the value of each
sample depends on the comprehensive value gain brought by over-
all text-label pairs. In our words, it is required to sample data points
that can improve generalization on all labels (informativeness) and
can cover as many label semantics as possible (representativeness).
Notably, these two issues are interrelated —- a well-defined feature
space ensures a clearer definition of sampling criteria, while more
accurate sampling also promotes feature learning.
To bridge the gaps, we break away from the reliance on a sin-
gle representation and propose a Contrastive Multi-label ActiveLearning framework (CoMAL) to learn label-wise representations
for boosted active sampling. Specifically, CoMAL pioneers the lever-
age of multi-label contrastive learning to adequately unleash the
potential semantics of limited data into a label-wise feature space,
which then facilitates a hybrid data value measurement. To bet-
ter contrastively learn the latent feature space, we employ the
multi-label decoupling mechanism to decompose the original text-
level representation into multiple text-label pair-level ones and
construct positive pairs for contrastive comparison. Based on the
feature space, a novel prototype-based active sampling strategy is
devised, which considers both informativeness and representative-
ness. The informativeness is formulated as the similarity-enhanced
label cardinality inconsistency, which measures the inconsistency
between similarity-based prediction and the label cardinality. Con-
sidering the label sparsity, the representativeness is measured as
thepositive feature diversity, which reveals the semantic feature
difference between the instance and the labeled pool. Combining
the two criteria, we obtain the final hybrid acquisition function to
sample instances with high uncertainty and feature diversity.
As shown in Figure 1, the visualization results indicate that our
CoMAL can generate a label-wise feature space with high-class
discriminability, and the devised prototype-based value measure-
ment can carry out effective data acquisition (detailed discussion
in Section 4.8). The main contributions are summarized as follows:
•To the best of our knowledge, our paper pioneers the ex-
ploration of contrastive learning for MLAL and proposes
a novel framework. As an integral part of our algorithm, a
prototype-based data sampling strategy is introduced, that
profits from the contrastively learned space.
•CoMAL measures both informativeness and representative-
ness for more comprehensively exploiting the potential in-
formation contrastively released from limited data to assist
an effective data acquisition. Specifically, two criteria, i.e.
similarity-enhanced label cardinality inconsistency andposi-
tive feature diversity, ensure high prediction uncertainty and
feature diversity of the selected data.
•Empirically, CoMAL establishes the state-of-the-art perfor-
mance on four MLTC benchmark datasets, which illustrates
the effectiveness of our data sampling strategy.
 
2365CoMAL: Contrastive Active Learning for Multi-Label Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain.
Feature
vectorSub-feature
vectorLatent
vectorDecoded
vectorReconstructed
feature vectorMulti -label
predictionLossMLP
or FCInput
textBERT -
modelEncoder
Classifier
LclsTarget MLTC Model
QLabeled
Pool
Unlabeled
PoolSampled
dataOracle
Sampling
strategyC AGG ClassifierM1 En De
M2 En De
MC En De
Reconstruction ClassificationLrec Lcls
Latent spaceLscl
......
...
DecouplingContrastive Learning Module
Pull
Push
Class a b c
PrototypesPull
Push
Class a b c
Prototypes
Figure 3: Illustration of CoMAL in the AL loop, which attaches a contrastive learning module to a target MLTC model and
presents a sampling strategy Q.
2 PRELIMINARIES
2.1 Multi-label Text Classification
Multi-label text classification (MLTC) aims to assign each text with
relevant labels. Suppose the training set as D={(𝒙𝑖,𝒚𝑖)|𝒙𝑖∈X,𝒚𝑖∈
Y}𝑁
𝑖=1,𝒙𝑖is an input text and 𝒚𝑖=[𝑦𝑖,1,𝑦𝑖,2,...,𝑦 𝑖,C],𝑦𝑖,𝑗∈{0,1}is
aC-dimensional multi-hot vector, where the sign 𝑦𝑖,𝑗=1indicates
that 𝒙𝑖belongs to the class 𝑗, otherwise 𝑦𝑖,𝑗=0. MLTC can be
generalized to learn a decision function 𝑓:X↦→Y. Specifically, 𝑓
can be further refined as 𝑓=ℎ(𝑔(𝒙)), where𝑔(·)is the encoding
function and ℎ(·)is the classification function. Finally, the binary
cross-entropy (BCE) loss can be used to train the MLTC task as:
L𝑐𝑙𝑠=−C∑︁
𝑗=1 𝑦𝑖,𝑗log𝑓𝑗(𝒙𝑖) +( 1−𝑦𝑖,𝑗)log(1−𝑓𝑗(𝒙𝑖)), (1)
where𝑓gives the sigmoid output and we denote 𝑓𝑗as its𝑗-th entry.
In this paper, We omit the sample index 𝑖when the context is clear.
2.2 Pool-based Active Learning
Pool-based active learning (PAL) [ 28] operates an iterative process
to obtain better models by sampling valuable samples. PAL assumes
a small labeled pool D𝐿and a fairly large unlabeled pool D𝑈as a
starting point. First, the target model Mis trained withD𝐿. Based
onM, a sampling strategy Qmines the most valuable data subset
D𝑆fromD𝑈and queries their labels from oracle. Then, D𝑆are
added toD𝐿and removed from D𝑈and the modelMis retrained
with the updatedD𝐿. Therefore, each round of data acquisition in
AL can be formulated as:
D𝐿←D 𝐿∪D∗
𝑆andD𝑈←D 𝑈\D∗
𝑆where
D∗
𝑆=arg max
V(D 𝑆)arg min
L(M)Q(D 𝑈;M(D 𝐿)),(2)where functionsVandLevaluate the value of D𝑆and general-
ization error ofM, respectively. Thereafter, model training and
data acquisition are repeated until the model Mattains promising
performance or the maximum number of cycles Tis reached.
Arguably, the key to a virtuous AL cycle is the effective sampling
strategyQ. It is worth mentioning that the above iterative paradigm
still works in the multi-label active learning scenario. However, the
above iterative paradigm is more intractable in the multi-label sce-
narios since the sampling strategy Qneeds to accurately measure
the instance’s value over the entire label space.
3 METHODOLOGY
As shown in Figure 3, CoMAL attaches a well-designed contrastive
learning moduleM𝑐𝑙to the target MLTC model M𝑚𝑙𝑡𝑐 (see Ap-
pendix A) and derives a prototype-based sampling strategy Q.
3.1 Contrastive Learning Module
Severely scarce data poses intractable obstacles to potential informa-
tion mining. Therefore, we couple the label-wise feature decoupling
with a contrastive term to effectively unleash the multi-label po-
tential into an embedding space. Afterward, we further introduce
label supervision to impel the reconstructed representation to be
discriminative.
Contrastive multi-label feature decoupling. Supervised con-
trastive learning (SCL) [ 16], as an effective way to obtain discrimi-
native representations, achieves improved performance in various
supervised learning tasks [ 17,34]. However, the conventional SCL
is for single-label classification and follows an example-level learn-
ing paradigm by operating unified features of samples, which is
not applicable in the multi-label scenario (ML). Considering that
multiple samples with the same label can still be assigned other
 
2366KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
inconsistent labels, the example-level learning paradigm will lead to
confusion between consistent and inconsistent features. Therefore,
we develop a label-level contrastive learning method to facilitate
the adaptation of SCL in ML.
The crucial challenge is how to construct the reasonable pos-
itive setPfor the multi-label samples. To this end, we employ
the multi-label decoupling mechanism to transform the original
multi-label problem into multiple single-label ones. Specifically,
after the target network M𝑚𝑙𝑡𝑐 encodes an input text 𝒙𝑖as a text-
level feature vector 𝒗𝑖∈R𝑑𝑣, the moduleM𝑐𝑙takes the vector 𝒗𝑖
as its input. The feature vector 𝒗𝑖is first decoupled into multiple
label-specific sub-feature vectors {𝒖𝑖,𝑗∈R𝑑𝑢}C
𝑗=1viaCindepen-
dent MLPs{𝑚𝑗(·)}C
𝑗=1, whereCis the number of labels. Then, we
further map each 𝒖𝑖,𝑗to a latent embedding 𝒛𝑖,𝑗∈R𝑑𝑧in the latent
spaceS𝑧via a shared projection network 𝑒(·).
After obtaining the decoupled latent embeddings, the next step is
to construct the positive set P(𝒛𝑖,𝑗)for each label-wise embedding
𝒛𝑖,𝑗to apply contrastive learning. Given a mini-batch of Bexamples,
all latent embeddings can be integrated into a set Z={𝒛𝑖,𝑗|𝑖∈
{1,2,...,B},𝑗∈{1,2,...,C}}as the contrastive embedding pool
AandA(𝒛𝑖,𝑗)=A\{𝒛𝑖,𝑗}. Considering that some embeddings
correspond to labels with negative polarity, we redefine a new
label for each embedding 𝒛𝑖,𝑗as𝑙𝑖,𝑗=I[𝑦𝑖,𝑗=1]𝑗+I[𝑦𝑖,𝑗=0](C+ 1),
where I[·]is the indicator function. Specifically, if 𝒛𝑖,𝑗is a positive
sample of class 𝑗, then its corresponding new label is 𝑗; If𝒛𝑖,𝑗is
a negative sample of class 𝑗, then its corresponding new label is
C+1. Considering that negative embeddings of any class should
not have semantic information, we cluster them into one new class
(i.e. classC+1).
Therefore, the positive set of each anchor 𝒛𝑖,𝑗can be expressed as
P(𝒛𝑖,𝑗)={𝒛𝑖′,𝑗′∈Z|𝑙𝑖′,𝑗′=𝑙𝑖,𝑗}. In other words, embeddings with
the same positive label are positively paired with each other, and all
embeddings with different negative labels are totally grouped into
a common positive set. Given an embedding 𝒛𝑖,𝑗, the per-anchor
contrastive loss is defined by contrasting it with the remainder of
the poolAas:
L𝑚𝑠𝑐𝑙=−1
|P(𝒛𝑖,𝑗)|∑︁
𝒛+∈P(𝒛𝑖,𝑗) logexp 𝒛𝑖,𝑗⊤𝒛+/𝜏
Í
𝒛′∈A( 𝒛𝑖,𝑗)exp 𝒛𝑖,𝑗⊤𝒛′/𝜏,
(3)
where𝜏∈R+is the scalar temperature.
We keep a prototype embedding vector 𝒑𝑗∈R𝑑𝑧corresponding
to each class 𝑗∈{1,2,...,C}, which can be deemed as a set of
representative embedding vectors. Specifically, after the training,
we calculate the prototype by averaging all embedding vectors of
class𝑗as𝒑𝑗=Mean({𝒛𝑖,𝑗|𝑙𝑖,𝑗=𝑗}), where Mean(·)denotes the
Mean-Pooling operation. Admittedly, these prototypes {𝒑𝑗}C
𝑗=1will
be key to the effectiveness of our devised sampling strategy.
Feature reconstruction and classification. To further lever-
age the supervision information to guide the learning of M𝑐𝑙, es-
pecially to attain a better latent S𝑧, we restore the original text
features for multi-label classification. Specifically, we first transform
each latent embedding 𝒛𝑖,𝑗back to a decoded vector ˜𝒖𝑖,𝑗∈R𝑑𝑢via
a shared decoder 𝑑(·). Then, all decoded vectors {˜𝒖𝑖,𝑗}C
𝑗=1of sample
𝒙𝑖are concatenated and re-projected to a reconstructed text-levelAlgorithm 1 The operation procedure of the contrastive learning
module
Input: A batch of text-level features {𝒗𝑖}B
𝑖=1
Output: The final loss 𝐿of theM𝑐𝑙
1:Perform label-wise feature decoupling to obtain label-specific
sub-features{{𝒖𝑖,𝑗}B
𝑖=1}C
𝑗=1
2:Project all sub-features into a latent space to obtain latent
embeddings{{𝒛𝑖,𝑗}B
𝑖=1}C
𝑗=1with a shared encoder 𝑒(·)
3:/* Contrastive Learning Loss */
4:Redefine the new label 𝑙𝑖,𝑗for each latent embedding 𝒛𝑖,𝑗
5:Obtain the contrastive positive set P(𝑧𝑖,𝑗)for each embedding
𝒛𝑖,𝑗
6:Calculate the contrastive learning loss using Equation (3)
7:/* Reconstruction Loss */
8:Decode the latent embeddings to decoded vectors
{{˜𝒖𝑖,𝑗}B
𝑖=1}C
𝑗=1with a shared decoder 𝑑(·)
9:Obtain the reconstructed text-level features {˜𝒗𝑖}B
𝑖=1from con-
catenated features{[𝒖𝑖,1;...;𝒖𝑖,C]}B
𝑖=1with a aggregation net-
work𝑎(·)
10:Calculate L2 reconstruction loss using Equation (4)
11:/* Classification Loss */
12:Obtain the multi-label prediction {˜𝒚𝑖}B
𝑖=1with network ℎ′(·)
13:Calculate BCE loss using Equation (5)
14:Obtain the final loss using Equation (6)
feature vector as ˜𝒗𝑖=𝑎([˜𝒖𝑖,1;˜𝒖𝑖,2,..., ˜𝒖𝑖,C])∈R𝑑𝑣, where𝑎(·)is
an aggregation network. To ensure that the reconstructed feature
vector ˜𝒗𝑖can be correctly re-projected into the original feature
space to maximize the preservation of all information from the
original instance, we introduce an L2 reconstruction loss as:
L𝑟𝑒𝑐=∥𝒗𝑖−˜𝒗𝑖∥2. (4)
Due to AL relying only on a small amount of annotated data for
model training, the reconstruction loss can promote the stability
of representation, maximize the retention of category semantics,
and promote AL. Then, we employ a network ℎ′(·)to implement
the multi-label classification on the vector ˜𝒗𝑖and obtain the multi-
label prediction ˜𝒚𝑖∈[0,1]C. Finally, to enable the reconstructed
text-wise features to learn correct label polarity, we compute the
BCE loss as:
L𝑐𝑙𝑠=−C∑︁
𝑗=1 𝑦𝑖,𝑗log(ℎ′𝑗(˜𝒗𝑖))+( 1−𝑦𝑖,𝑗)log(1−ℎ′𝑗(˜𝒗𝑖)).
(5)
Combining all losses (3, 4, 5), the overall loss of M𝑐𝑙is defined as:
L=L𝑚𝑠𝑐𝑙+𝛼L𝑟𝑒𝑐+𝛽L𝑐𝑙𝑠, (6)
where𝛼,𝛽are the weight coefficients. It is worth noting that the
moduleM𝑐𝑙is trained along with the model M𝑚𝑙𝑡𝑐, which means
no additional learning phase is introduced. Furthermore, since other
AL methods do not employ our M𝑐𝑙branch, for fair comparison,
we stop the gradient back-propagation from M𝑐𝑙toM𝑚𝑙𝑡𝑐 to avoid
performance interference. Experimentally, we found that allowing
gradient propagation doesn’t necessarily affect the performance
(see Section 4.6). It can be said that M𝑐𝑙is not intended to directly
 
2367CoMAL: Contrastive Active Learning for Multi-Label Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain.
Instance 1 Instance 2 Instance 3 Instance 4
High SLCI
High PFDLow SLCI
High PFDHigh SLCI
Low PFDLow SLCI
Low PFD
Far distanceNear distance Given that the label cardinality is  set to 2：  
Latent embedding space
SLCI/PFD value Sampled/Unsampled Instance Positive/Negative embedding Label prototype 
 neighbor
Figure 4: An intuitive illustration of sampling strategy Qin a hypothetical toy example. The size of the label space is six and
ellipses of different colors represent embeddings belonging to different label categories. The dark-color black-outline ellipse
indicates an embedding with a positive corresponding label, while a light-color gray-outline ellipse indicates an embedding
with a negative corresponding label. Instance 1 is the only one sampled by Qdue to its high SLCI and high PFD.
affect representation learning of the backbone model but to build a
better AL sampler. The pseudo-code of the CL module’s procedure
is shown in Algorithm 1.
Remark. In contrast to SCL for single-label classification problems,
our text-label pair embeddings explore the pattern on different labels.
This allows us to evaluate the value of different labels individually
before aggregated data evaluation. Also, it brings convenience to
achieve diverse data sampling according to label-aware patterns.
3.2 Prototype-based Active Sampling Strategy
As we mentioned, the module M𝑐𝑙attains a latent embedding
space with high-class discriminability. As a collaborative algorithm,
we introduce a novel prototype-based active sampling strategy Q.
Briefly, the sampling strategy Qfollows the PAL learning paradigm
and presents a hybrid data acquisition function that effectively
balances prediction uncertainty and feature diversity.
Informativeness measure. Uncertainty, as a straightforward
criterion to reflect the confidence of the model on the predicted
results, is widely adopted in current AL works [ 6,9,32]. A canonical
implementation of uncertainty-based sampling (e.g. Entropy [ 28])
is to select the most obfuscated instances w.r.t. the learned decision
boundary. However, scarce data often induce inaccurate decision
boundaries, which in turn fail to give a credible value measure, thus
greatly reducing the effect of data sampling. Fortunately, since both
labeled and unlabeled instances are drawn from the same under-
lying distribution, they often share common statistical properties
in the output label space. Therefore, we attempt to leverage the
inconsistency between the number of predicted positive labels and
the label cardinality of the labeled set to measure the uncertainty
of the unlabeled samples.
We propose a novel uncertainty-based criterion called Similarity-
enhanced Label Cardinality Inconsistency (SLCI), which exploresthe feature distribution information of the latent space S𝑧in the
moduleM𝑐𝑙. Specifically, we exploit the feature similarity between
samples and label prototypes. Remember we maintain a prototype
set as{𝒑𝑗}C
𝑗=1after training. Firstly, for an unlabeled instance 𝒙,
we calculate the similarity between each vector 𝒛𝑗and its corre-
sponding label prototype as 𝑘𝑗=𝑠(𝒛𝑗,𝒑𝑗), where𝑠(·)denotes an
arbitrary measure of similarity (e.g. the cosine similarity). As for the
labeled poolD𝐿, we can obtain the mean similarity for each label 𝑗
in the same way as ¯𝑘𝑗=Mean({𝑘𝑖,𝑗=𝑠(𝒛𝑖,𝑗,𝒑𝑗)}|D𝐿|
𝑖=1). Then, we
adopt a projection condition 𝑘𝑗>¯𝑘𝑗to make the satisfied label
positive. Finally, we define the similarity-enhanced LCI (SLCI) as
the inconsistency between the projection and the label cardinality
of the labeled setD𝐿as:
𝑆𝐿𝐶𝐼(𝒙)=∥C∑︁
𝑗=1𝐼[𝑘𝑗>¯𝑘𝑗]−1
|D𝐿||D𝐿|∑︁
𝑖=1C∑︁
𝑗=1𝐼[𝑦𝑖,𝑗=1]∥2. (7)
Representativeness measure. To further leverage the feature
distribution information in the latent space S𝑧, we propose a crite-
rion to measure sample representativeness from the perspective of
feature diversity. Specifically, we define the Positive Feature Diver-
sity (PFD) to represent the semantic feature difference between the
unlabeled instance 𝒙and the labeled pool D𝐿. It should be noted
that considering the sparsity of positive labels in the multi-label
scenario, this strategy only applies to those feature vectors whose
corresponding labels are judged as positive. Here, we still use the
similarity measure of 𝑠(·), where a small 𝑘𝑗value means that the
feature vector 𝒛𝑗is far away from the prototype 𝒑𝑗, further indi-
cating that the instance 𝒙contains more features that are not seen
in the labeled pool D𝐿. Thus the PFD criterion is formulated as:
𝑃𝐹𝐷(𝒙)=∑︁C
𝑗=1𝐼[˜𝑦𝑗=1](1−𝑘𝑗). (8)
 
2368KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
Algorithm 2 The active learning cycle for CoMAL
Input: Unlabeled data pool D𝑈, target modelM𝑚𝑙𝑡𝑐 with param-
eterΘ𝑚𝑙𝑡𝑐, moduleM𝑐𝑙with parameter Θ𝑐𝑙
Output: Labeled data poolD𝐿, target model parameters Θ𝑚𝑙𝑡𝑐
1:Initialize labeled pool D𝐿with ˆ𝑁samples
2:while not reach maximum cycle number Tdo
3: /* Training on the labeled pool D𝐿*/
4:whileM𝑚𝑙𝑡𝑐 andM𝑐𝑙do not converge do
5: Update parameters Θ𝑚𝑙𝑡𝑐,Θ𝑐𝑙using Equation (10), (6)
6:end while
7: /* Calculating label prototypes */
8: Calculate label prototypes {𝒑𝑗}C
𝑗=1in the latent space S𝑧
9: /* Data acquisition on the unlabeled pool D𝑈*/
10: foreach unlabeled sample 𝒙inD𝑈do
11: Calculate𝑆𝐿𝐶𝐼(𝒙),𝑃𝐹𝐷(𝒙)using Equation (7), (8)
12: Obtain the hybrid data value Q(𝒙)using Equation (9)
13: end for
14: Sample unlabeled subset D∗
𝑆fromD𝑈according toQ(𝒙)
15: Update data pool by D𝐿←D 𝐿∪D∗
𝑆,D𝑈←D 𝑈\D∗
𝑆
16:end while
17:return Labeled data poolD𝐿and target model with Θ𝑚𝑙𝑡𝑐
Combining the above two criteria, we finally attain the hybrid
active sampling strategy Qas:
Q(𝒙)=𝑆𝐿𝐶𝐼(𝒙)𝛾·𝑃𝐹𝐷(𝒙)1−𝛾, (9)
where𝛾is a trade-off parameter. Experimentally, we uniformly
set𝛾to 0.5. Then we select the most valuable samples D𝑆via
𝒙∗=arg max𝒙∈D𝑈Q(𝒙)with the limited budget 𝜂.
To more intuitively interpret the sampling strategy Q, we present
a toy example with four hypothetical instances in Figure 4. Among
4 instances, instance 1 is sampled preferentially due to its high SLCI
and high PFD, while others are not. A high SLCI value implies that
the label cardinality differs significantly from the number of labels
associated with the instance (e.g. instances 1 and 3). A high PFD
value means that the instance (e.g. instances 1 and 2) has more
unique semantic features (expressed as the ’far distance’ between
the embedding and the prototype). The pseudo-code of CoMAL is
shown in Algorithm 2.
4 EXPERIMENTS
4.1 Setup
Datasets and Evaluation Metrics. We use four benchmark MLTC
datasets to evaluate the performance of our CoMAL: We two stan-
dard metrics, i.e. micro-F1 score and precision at top K ( 𝑃@𝑘) [15],
to compare performance.
•AAPD (Arxiv Academic Paper Dataset) [40] collects the
abstract and the corresponding subjects of more than 55K
papers from arXiv in the field of computer science.
•RCV1 (Reuters Corpus Volume I) [18] is composed of
more than 800K manually categorized newswire stories for
research purposes.•JustDance1contains about 10K real user comments on a
series of entertainment games from YouTube.
•EURLex-4K [22] is a collection of documents about Euro-
pean Union law belonging to about 4K subjects.
For each dataset, we select only a portion of the data for the
experiments. In particular, the original EURLex-4K dataset is an
extreme multi-label dataset with massive rare labels. However, the
extreme long-tail problem is not the main focus of MLAL and
is seldom touched by existing MLAL works. To enable all labels
to effectively participate in active learning, we remove extremely
rare labels with a label frequency of less than 40. The statistics of
four datasets are listed in Table 1. We use two standard metrics,
i.e. micro-F1 score and precision at top K ( 𝑃@𝑘) [15], to compare
performance.
Baselines. We employ the following baselines:
(1)Random Sampling acquires samples uniformly at random
from the unlabeled data. (2) BADGE chooses high-magnitude and
disparate samples in the hallucinated gradient space [ 1]. (3) Core-
Setacquires samples that best cover the dataset and uses the greedy
farthest-first traversal algorithm to approximate this optimization
objective [ 27]. (4) LLoss selects samples with the highest target
losses with a loss prediction module [ 41]. (5) MMC samples in-
stances that maximize the expected model loss reduction rate and
use logistic regression to predict the number of labels for a new in-
stance [ 39]. (6) Adaptive considers both the prediction separation
margin and the label cardinality inconsistency, then combines these
two parts for data sampling [ 19]. (7)AUDI improves the cardinality
inconsistency measure via label ranking, where a dummy label
is used to separate positive and negative labels [ 13]. (8) CVIRS
combines two uncertainty measures based on the magnitude of
the difference margin in predictions and the inconsistency of a
predicted label set [ 26]. (9) GP-B2Mselects samples by combining
both the feature uncertainty and label covariance by quantifying a
sample’s overall contribution to a correlated label space [29].
Implementation Details. In our experiments, the labeled set D𝐿
is initialized with ˆ𝑁samples, and the rest samples of the training
datasetD𝑡𝑟𝑎𝑖𝑛 are collected as the unlabeled set D𝑈. We keep a
minimum of 1 positive instance per label in the initial D𝐿and
randomly select the rest samples. All the baseline models share the
same copy of the initial D𝐿to make a fair comparison. In each AL
cycle, all methods acquire 𝜂samples from theD𝑈and retrieve their
labels. Then the model is retrained from scratch with the expanded
D𝐿. The AL process ends up at the maximum cycle T=50.
For all experiments, we migrate all AL methods to the same
basic multi-label model for a fair comparison. We adopt the pre-
trained language model (BERT-base model: 12 layers and 768 hidden
dimensions) [ 37] as the encoding backbone and the 2-layer MLP as
the final classifier. We freeze the first 9 layers of BERT and fine-tune
the final 3 layers as well as the MLP. For the model parameters,
we set𝑑𝑣=768,𝑑𝑢=512on all datasets, 𝑑𝑧=256on AAPD and
RCV1 while 𝑑𝑧=64on JustDance. We employ the AdamW [ 37]
optimizer with the initial learning rate 1𝑒−4and set𝛼,𝛽=1. The
batch sizeBis set to 64, and training stops until it meets 6-epoch
patience. We uniformly set the hyperparameter 𝛾to 0.5 on the four
datasets. For better statistical robustness, we use the mean and
1https://linktr.ee/justdanceproject(Attribution 4.0 International (CC BY 4.0))
 
2369CoMAL: Contrastive Active Learning for Multi-Label Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain.
0 10 20 30 40 50
Number of active cycle0.850.860.870.880.890.900.910.920.930.94P@1RCV1
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.720.740.760.780.800.82P@1AAPD
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.540.560.580.600.620.640.66P@1JustDance
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.660.680.700.720.740.760.780.800.82P@1EURLex-4K
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.660.680.700.720.740.76P@3RCV1
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.520.530.540.550.560.570.580.59P@3AAPD
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.460.480.500.520.540.56P@3JustDance
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.500.520.540.560.580.600.620.64P@3EURLex-4K
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.620.640.660.680.700.720.740.76Micro F1RCV1
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.580.600.620.640.660.68Micro F1AAPD
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.400.420.440.460.480.500.520.54Micro F1JustDance
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.450.480.500.530.550.580.600.620.65Micro F1EURLex-4K
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMAL
Figure 5: Performance comparison on RCV1, AAPD, JustDance and EURLex-4K. All results are averaged on 5 runs. The shaded
area around each curve denotes the respective standard deviation of each method.
Table 1: Statistics of the datasets and hyperparameters. 𝑁train,
𝑁valid and𝑁testare the numbers of training, validation and
test samples respectively, 𝐿is the number of labels, ¯𝐿is the
average number of labels per sample. ˆ𝑁is the initial size of
D𝐿and𝜂is the sampling budget of each cycle.
Datasets 𝑁train𝑁valid𝑁test𝐿 ¯𝐿 ˆ𝑁
𝜂
RCV1 23,149
2,000 10,000 101 3 .2200
100
AAPD 45,840 5,000 5,000 54 2 .4100
100
JustDance 7,910 800 1 ,500 38 5 .940
20
EURLex-4K 13,060 800 2 ,000 461 3 .7500
200
standard deviation from 5 runs of randomly initialized experiments.
All experiments are conducted with one NVIDIA A100 GPU. Other
detailed hyperparameters are shown in Table 1.
4.2 Performance Comparison
As shown in Figure 5, CoMAL outperforms all the rivals on all
datasets. Specifically, on RCV1, we improve upon 𝑃@1of the sec-
ond best baseline by 1.9%, 2.1%, and 1.8% where |D𝐿|=1000,1500,
and2000, respectively. The experimental results also reveal evi-
dence confirming that the effectiveness of CoMAL benefits from
the comprehensive consideration of prediction uncertainty and
feature diversity: (1) Among all competitive methods, the superior
performance of BADGE strongly suggests that incorporating both
uncertainty and diversity is advantageous for selecting valuable
samples. (2) Although Adaptive is a strong competitor using LCI
to measure sample uncertainty, the superior CoMAL demonstratesthe significance of exploiting feature diversity to assist uncertainty
measurement for efficient data acquisition.
Another observation is that the Random Sampling is a moder-
ate player, and in most cases outperforms LLoss and CVIRS, and
sometimes even beats MMC. This shows the challenging nature
of the MLAL problem — methods specifically designed for MLAL
(e.g. CVIRS and MMC) and single-label methods adapted to the
multi-label scenario (e.g. LLoss) may easily fall behind Random —
which justifies the motivation of our work.
It needs to be admitted that, on JustDance, although the advan-
tages of CoMAL are not as obvious as on other datasets over other
baselines, CoMAL still maintains the optimal performance. This is
mainly because on a smaller-scale dataset, the target task becomes
relatively easy and the newly selected samples via different sam-
pling methods can consistently maintain good performance, leading
to a small performance gap. Since machine learning practitioners
are more inclined to use AL for processing large unlabeled data,
the above finding still favors the significance of CoMAL. Other
comparison results are shown in Appendix B.1.
4.3 Ablation Study
We conduct the ablation study to further demonstrate the effect
of each component of CoMAL with the following ablated variants:
(a) CoMAL without contrastive learning (w/o cl), which learns
the latent spaceS𝑧withoutL𝑠𝑐𝑙and retains the original sam-
pling strategy. (b) CoMAL without reconstruction and classification
(w/o rc), which removes the process after projecting latent embed-
dings inM𝑐𝑙to illustrate the effectiveness of using L𝑟𝑒𝑐andL𝑠𝑐𝑙
 
2370KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
0 10 20 30 40 50
Number of active cycle0.880.890.900.910.920.930.94P@1RCV1
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
(a) Ablation test on RCV1
0 10 20 30 40 50
Number of active cycle0.750.760.770.780.790.800.810.82P@1AAPD
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL (b) Ablation test on AAPD
0 10 20 30 40 50
Number of active cycle0.880.890.900.910.920.930.94P@1RCV1
0.1
0.3
0.5
0.7
0.9 (c) Effect of 𝛾on RCV1
0 10 20 30 40 50
Number of active cycle0.750.760.770.780.790.800.810.82P@1AAPD
0.1
0.3
0.5
0.7
0.9 (d) Effect of 𝛾on AAPD
Figure 6: Ablation study (a)-(b) and the impact of 𝛾(c)-(d) on RCV1 and AAPD.
0 10 20 30 40 50
Number of active cycle0.880.890.900.910.920.930.94P@1RCV1
CoMAL w/o NEG
CoMAL w/o LSF
CoMAL
(a)
0 10 20 30 40 50
Number of active cycle0.720.740.760.780.800.82P@1AAPD
CoMAL w/o NEG
CoMAL w/o LSF
CoMAL (b)
0 10 20 30 40 50
Number of active cycle0.880.890.900.910.920.930.94P@1RCV1
CoMAL w/o SG
CoMAL w/ SG (c)
0 10 20 30 40 50
Number of active cycle0.750.760.770.780.790.800.810.82P@1AAPD
CoMAL w/o SG
CoMAL w/ SG (d)
Figure 7: Performance comparison with different CL implementations (a)-(b). Performance comparison with/without stopping
gradient backpropagation from the module M𝑐𝑙to the modelM𝑚𝑙𝑡𝑐(c)-(d).
0 10 20 30 40 50
Number of active cycle0.700.720.740.760.780.800.820.84P@1AAPD
Adaptive (50)
Adaptive (100)
Adaptive (200)
GP-B2M (50)
GP-B2M (100)GP-B2M (200)
CoMAL (50)
CoMAL (100)
CoMAL (200)
(a) Methods with different 𝜂
100 500 900 1300 1700 2100 2500
Number of sampled examples0.660.680.700.720.740.760.780.80P@1AAPD
=50
=100
=200
 (b) CoMAL with different 𝜂
Figure 8: Impact of 𝜂on different methods.
to introduce more supervisory information. (c) CoMAL-SLCI and
CoMAL-PFD, which respectively use SLCI and PFD to implement
data sampling.
Figure 6 (a)-(b) compare the performance with different CoMAL
variants on RCV1 and AAPD. (1) The performance of CoMAL w/o cl
drops below CoMAL, which signifies the effectiveness of contrastive
learning in producing disambiguation representations to facilitate
the selection of valuable samples. (2) The results of CoMAL versus
CoMAL w/o rc show the effect of introducing feature reconstruction
and classification in module M𝑐𝑙, which imperceptibly incorporates
more supervisory information into the learning of space S𝑧. (3)
CoMAL outperforms CoMAL-SLCI andCoMAL-PFD, which reveals
that both prediction uncertainty and feature diversity of unlabeled
samples are indispensable for robust sampling, and also reflect the
complementary effect of SLCI and PFD. In summary, all experiments
prove that every component of CoMAL is indispensable. Detailed
ablation results are shown in Appendix B.2.4.4 Effect of the Parameter 𝛾
Parameter𝛾, as the trade-off coefficient in Qwhich balances SLCI
and PFD, is bound to affect the data acquisition. In Figure 6 (c)-
(d), we exhibit the performance comparison of setting 𝛾values
in[0.1,0.3,0.5,0.7,0.9]. On RCV1 and AAPD, the 𝛾value of 0.5
achieves the best results and performance deteriorates as 𝛾ap-
proaches 1 or 0, which illustrates the importance of considering
both the uncertainty and diversity of the unlabeled samples. There-
fore, we set 𝛾to 0.5 on all datasets to ensure that Qequally consid-
ers data informativeness and representativeness. Other results are
shown in Appendix.
4.5 Effect of Different CL Implementations
To illustrate the rationality of our CL implementation, we compare
CoMAL with other variants using different CL implementations:
(a) CoMAL without negative (w/o NEG), which no longer builds a
positive set for those negative label-specific sub-features. (b) Co-
MAL without label-specific sub-features (w/o LSF), which removes
the multi-label decoupling process and directly uses the text-level
features to perform CL. As shown in Figure 7 (a)-(b), on RCV1 and
AAPD, CoMAL obtains significant performance advantages over
both CoMAL w/o NEG andCoMAL w/o LSF. The comparison with
CoMAL w/o NEG indicates that setting up an additional positive
cluster for negative label-specific features can learn a better latent
space with high-class discriminability and facilitate our acquisition
function. On the other hand, without feature decoupling, it is hard
for a single feature that aggregates multi-label information to sep-
arate discriminative features of different classes. a single feature
makes it hard to separate discriminative features of different classes.
 
2371CoMAL: Contrastive Active Learning for Multi-Label Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain.
4.6 Comparison of CoMAL with/without
Gradient Backpropagation from M𝑐𝑙
As discussed in Section 3.1, we block gradient backpropagation
from the CL module M𝑐𝑙to the target model M𝑚𝑙𝑡𝑐. In this way,
we maintain a fair comparison with other baseline methods, so that
the training of the CL module does not affect the backbone network.
Here, we provide additional experiments to better demonstrate the
impact of stopping gradient on performance. As shown in Figure
7 (c)-(d), whether the gradient is stopped or not, the performance
of the target model M𝑚𝑙𝑡𝑐 will not be greatly affected. Therefore,
our moduleM𝑐𝑙is not intended to directly affect representation
learning of the backbone model but to build a better AL sampler.
4.7 Effect of the Sampling Budget 𝜂
To better illustrate the impact of budget size, we compare the perfor-
mance under different 𝜂values. As shown in Figure 8 (a), regardless
of the budget size ( 𝜂∈[50,100,200]), our CoMAL still maintains
optimal performance, which proves the stability of its effect. Fig-
ure 8 (b) shows the performance comparison after sampling equal
amounts of data with different budget sizes. It can be observed that
the performance difference with different 𝜂is not significant, which
indirectly reflects the robustness of our method.
4.8 Visualization
To illustrate the data acquisition effect of CoMAL, we use t-SNE [ 31]
to visualize the embedding distribution of S𝑧in Figure 1 (more in
Appendix B.3). Different colors represent embeddings with different
positive labels, while embeddings with negative label polarity are
represented in gray. We plot the embeddings on AAPD with |D𝐿|=
1000 of three scenarios: (a) the variant CoMAL w/o cl, (b) our method
CoMAL, and (c) one round of sampling results of CoMAL.
In Figure 1 (a), without CL, the positive and negative embed-
dings are hardly distinguishable, and the embeddings of the same
label are relatively dispersed. Contrastly, in Figure 1 (b), CoMAL
produces well-clustered and more distinguishable embeddings, con-
firming the effectiveness of CL. After sorting the unlabeled pool
D𝑈viaQ, we plot the first 100 instances sampled by CoMAL in
one AL iteration (colored triangle) and the last 100 instances not
selected (inverted triangle of the same color) in Figure 1 (c). Co-
MAL prioritizes the instance whose feature embeddings are distant
from their respective prototypes. In other words, CoMAL selects
samples whose features are mistakenly mixed with the negative
features and leverages CL to normalize (pull) them towards positive
feature clusters in the subsequent training iteration. Iteratively, the
feature diversity of the labeled pool D𝐿is continuously enriched.
Therefore, a complementary collaboration between contrastive rep-
resentation learning and data acquisition in CoMAL ensured the
feature diversity of the final sampled data.
5 RELATED WORK
Active Learning (AL) is a learning paradigm that actively selects
valuable data from an unlabeled data pool to reduce annotation
cost [ 25]. Current AL approaches can be roughly categorized as
follows: (a) Informativeness-based methods [ 13,32,38] measure
the ability of a sample to reduce model prediction uncertainty orimprove model generalization performance. (b) Representativeness-
based methods [ 4,8,13] measure how well a sample represents an
unlabeled data set to prevent sampling bias in data distribution. (c)
The hybrid methods [ 9,13,30,35] combine the two measures to
obtain more robust acquisition functions.
Mutli-label Active Learning (MLAL), as a derivative of AL in the
multi-label (ML) scenario, considers a value measure on the entire
label space. Based on the binary relevance [ 42], MMC [ 39] trains an
individual SVM for each label and uses the loss reduction to select
data. To achieve more robust data sampling, many works attempt
to combine multiple metrics [ 19,26,29,36]. QUIRE [ 12] combines
informativeness and representativeness in a min-max view of AL,
and further improves the selection via the label correlation. ADAP-
TIVE [ 19] derives a maximum correntropy-based AL method by
merging uncertainty and representativeness. Despite various en-
deavors, unreliable value evaluation with limited data remains a
critical issue. To this end, we attempt to release the information
of limited data into a latent space and derive effective measures to
select valuable samples.
Contrastive Learning (CL) [ 5,11,23] is a self-supervised learning
framework that learns discriminative representations via instance
similarity or dissimilarity. Recently, supervised contrastive learning
(SCL) [ 16], an approach that leverages label information to aggre-
gate data from the same class as the positive set, obtains improved
performance on various supervised learning tasks. Nowadays, CL
is introduced into multi-label learning [ 2,7]. However, to date, no
pioneers have applied CL to multi-label learning in the AL scenario.
In this work, we leverage the CL technique to learn a reasonable
and effective sample value measure for the MLAL task.
6 CONCLUSION
In this work, we propose a novel Contrastive Multi-label Active
learning framework, CoMAL. The key idea is to fully release the
potential information of limited labeled data into a latent space by
using the contrastive learning technique. Based on the contrastively
learned space, two value criteria, i.e. similarity-enhanced label car-
dinality inconsistency (SLCI) and positive feature diversity (PFD),
which measure informativeness and representativeness of unla-
beled samples respectively, are combined into a more robust data
acquisition strategy. Empirically, extensive experiments show that
CoMAL establishes state-of-the-art performance. For future work,
we will extend CoMAL to further reduce the annotation cost.
ACKNOWLEDGMENTS
This work is supported by the Pioneer R&D Program of Zhejiang
(No.2024C01021) and the CCF-NetEase ThunderFire Innovation
Research Funding (NO.202305). This paper is also sponsored by the
Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and
Data Security.
REFERENCES
[1]Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and
Alekh Agarwal. 2020. Deep Batch Active Learning by Diverse, Uncertain Gradient
Lower Bounds. In ICLR.
[2]Junwen Bai, Shufeng Kong, and Carla P Gomes. 2022. Gaussian mixture vari-
ational autoencoder with contrastive learning for multi-label classification. In
ICML.
 
2372KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
[3]Wei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and Inderjit S Dhillon.
2020. Taming pretrained transformers for extreme multi-label text classification.
InProceedings of the 26th ACM SIGKDD international conference on knowledge
discovery & data mining. 3163–3171.
[4]Rita Chattopadhyay, Zheng Wang, Wei Fan, Ian Davidson, Sethuraman Pan-
chanathan, and Jieping Ye. 2013. Batch mode active sampling based on marginal
probability distribution matching. ACM Trans. Knowl. Discov. Data (2013).
[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In ICML.
[6]Everton Alvares Cherman, Yannis Papanikolaou, Grigorios Tsoumakas, and
Maria Carolina Monard. 2019. Multi-label active learning: key issues and a
novel query strategy. Evol. Syst. (2019).
[7]Son D Dao, Zhao Ethan, Phung Dinh, and Cai Jianfei. 2021. Contrast learning
visual attention for multi label classification. arXiv preprint arXiv:2107.11626
(2021).
[8]Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical sampling for active learning.
InICML.
[9]Bo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, and Dacheng Tao. 2017.
Robust and discriminative labeling for multi-label active learning based on maxi-
mum correntropy criterion. IEEE Trans. Image Process. (2017).
[10] Hao Fei, Yue Zhang, Yafeng Ren, and Donghong Ji. 2020. Latent emotion memory
for multi-label emotion classification. In AAAI. 7692–7699.
[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
mentum contrast for unsupervised visual representation learning. In CVPR.
[12] Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. 2010. Active learning by querying
informative and representative examples. In NIPS.
[13] Sheng-Jun Huang and Zhi-Hua Zhou. 2013. Active query driven by uncertainty
and diversity for incremental multi-label learning. In ICDM.
[14] Himanshu Jain, Venkatesh Balasubramanian, Bhanu Chunduri, and Manik Varma.
2019. Slice: Scalable linear extreme classifiers trained on 100 million labels for
related searches. In WSDM.
[15] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. 2016. Extreme multi-
label loss functions for recommendation, tagging, ranking & other missing label
applications. In SIGKDD.
[16] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive
learning. In NeurIPS.
[17] Minseon Kim, Jihoon Tack, and Sung Ju Hwang. 2020. Adversarial self-supervised
contrastive learning. In NeurIPS.
[18] David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. 2004. Rcv1: A new
benchmark collection for text categorization research. J. Mach. Learn. Res. (2004).
[19] Xin Li and Yuhong Guo. 2013. Active learning with multi-label SVM classification..
InIJCAI.
[20] Xin Li, Haoran Xie, Yanghui Rao, Yanjia Chen, Xuebo Liu, Huan Huang, and
Fu Lee Wang. 2016. Weighted multi-label classification model for sentiment
analysis of online news. In BigComp.
[21] Shuhua Monica Liu and Jiun-Hung Chen. 2015. A multi-label classification based
approach for sentiment classification. Expert Syst. Appl. (2015).
[22] Eneldo Loza Mencia and Johannes Fürnkranz. 2008. Efficient pairwise multilabel
classification for large-scale problems in the legal domain. In Machine Learning
and Knowledge Discovery in Databases, Vol. 5212. Springer, 50–65.
[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[24] Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik
Varma. 2018. Parabel: Partitioned label trees for extreme classification with
application to dynamic search advertising. In WWW.
[25] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta,
Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active learning. CSUR 54,
9 (2021), 1–40.
[26] Oscar Reyes, Carlos Morell, and Sebastián Ventura. 2018. Effective active learning
strategy for multi-label learning. Neurocomputing (2018).
[27] Ozan Sener and Silvio Savarese. 2018. Active learning for convolutional neural
networks: A core-set approach. In ICLR.
[28] Burr Settles. 2009. Active learning literature survey. Technical report (2009).
[29] Weishi Shi, Dayou Yu, and Qi Yu. 2021. A Gaussian process-Bayesian Bernoulli
mixture model for multi-label active learning. NeurIps (2021).
[30] Ying-Peng Tang and Sheng-Jun Huang. 2019. Self-paced active learning: Query
the right thing at the right time. In AAAI.
[31] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research (2008).
[32] Deepak Vasisht, Andreas Damianou, Manik Varma, and Ashish Kapoor. 2014.
Active learning for sparse bayesian multilabel classification. In SIGKDD.
[33] Dongkai Wang and Shiliang Zhang. 2020. Unsupervised person re-identification
via multi-label classification. In CVPR. 10981–10990.
[34] Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc
Van Gool. 2021. Exploring cross-image pixel contrast for semantic segmentation.
InICCV.
[35] Zheng Wang and Jieping Ye. 2015. Querying discriminative and representative
samples for batch mode active learning. ACM Trans. Knowl. Discov. Data (2015).[36] Lukas Wertz, Jasmina Bogojeska, Katsiaryna Mirylenka, and Jonas Kuhn. 2023.
Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text
Classification. In Findings of the Association for Computational Linguistics: ACL
2023. 10959–10977.
[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al .
2019. Huggingface’s transformers: State-of-the-art natural language processing.
arXiv preprint arXiv:1910.03771 (2019).
[38] Yifan Yan and Sheng-Jun Huang. 2018. Cost-effective active learning for hierar-
chical multi-label classification.. In IJCAI.
[39] Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng Chen. 2009. Effective
multi-label active learning for text classification. In SIGKDD.
[40] Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, and Houfeng Wang.
2018. SGM: sequence generation model for multi-label classification. In COLING.
[41] Donggeun Yoo and In So Kweon. 2019. Learning loss for active learning. In
CVPR.
[42] Min-Ling Zhang, Yu-Kun Li, Xu-Ying Liu, and Xin Geng. 2018. Binary relevance
for multi-label learning: an overview. Frontiers of Computer Science 12 (2018),
191–202.
[43] Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel neural networks with
applications to functional genomics and text categorization. TKDE 18, 10 (2006),
1338–1351.
A THE IMPLEMENTATION OF TARGET
MODEL
Here we provide a detailed description of the target network M𝑚𝑙𝑡𝑐.
As described in Section 2.1, the network M𝑚𝑙𝑡𝑐 follows the straight-
forward BR [ 42] approach to transform the original multi-label
classification problem into multiple binary ones. As shown in Fig-
ure 3, we first feed each text 𝒙𝑖into the encoder 𝑔(·)(BERT-base
model [ 37]) to generate feature vector 𝒗𝑖∈R𝑑𝑣. Then we employ
a classification network ℎ(·)to obtain the multi-label prediction
ˆ𝒚𝑖∈[0,1]C. Finally, we use the binary cross entropy (BCE) as the
objection function for fine-tuning M𝑚𝑙𝑡𝑐 as follows:
L𝑚𝑙𝑡𝑐
𝑐𝑙𝑠=−∑︁C
𝑗=1 𝑦𝑖,𝑗logℎ𝑗(𝑔(𝒙𝑖))+
(1−𝑦𝑖,𝑗)log(1−ℎ𝑗(𝑔(𝒙𝑖))).(10)
It should be reiterated that the training of network M𝑚𝑙𝑡𝑐 will not
be disturbed by the module M𝑐𝑙. ModelM𝑚𝑙𝑡𝑐 will only provide
moduleM𝑐𝑙with an intermediate representation 𝒗𝑖of each text 𝒙𝑖,
but will cut off the gradient back-propagation from module M𝑐𝑙
when the weights of model M𝑚𝑙𝑡𝑐 are updated. In addition, when
the training of model M𝑚𝑙𝑡𝑐 reaches convergence, the training of
modelM𝑚𝑙𝑡𝑐 will be finished and only the learning of module M𝑐𝑙
will be maintained.
B EXPERIMENTS AND RESULTS
B.1 Performance Comparison
Here, we add two additional evaluation metrics, i.e. NDCG@K and
Macro-F1, widely used in multi-label works to comprehensively
compare the performance of different methods. From the experi-
mental results in Figure 9, it can be seen that our method still has a
performance advantage compared to other competitive baselines.
B.2 Ablation Studies
We investigate the effect of each component of CoMAL on four
datasets. Figure 11 shows the performance comparison between
CoMAL and different variants. It can be seen that CoMAL main-
tains high performance on all datasets, which demonstrates the
rationality of combining different components.
 
2373CoMAL: Contrastive Active Learning for Multi-Label Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain.
0 10 20 30 40 50
Number of active cycle0.150.200.250.300.350.400.45Macro F1RCV1
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.720.740.760.780.800.820.840.86N@3RCV1
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMA
0 10 20 30 40 50
Number of active cycle0.250.300.350.400.450.50Macro F1AAPD
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
GP-B2M
CoMAL
0 10 20 30 40 50
Number of active cycle0.660.680.700.720.740.760.78N@3AAPD
Random
MMC
Adaptive
CVIRS
Core-SetLLoss
BADGE
AUDI
GP-B2M
CoMA
Figur
e 9: Comparison results of Macro-F1 and NDCG@3 on RCV1 and AAPD.
0 5 10 15 20 25 30
Number of active cycle1.002.003.004.005.006.00secondsAAPD training time cost
Random
MMC
Adaptive
Core-Set
BADGE
GP-B2M
CoMAL
(
a) Traing time
0 5 10 15 20 25 30
Number of active cycle2.102.202.302.402.502.60seconds (log10)AAPD sampling time cost
Random
MMC
Adaptive
Core-Set
BADGE
GP-B2M
CoMAL (
b) Sampling time
0 10 20 30 40 50
Number of active cycle0.670.680.690.700.710.720.730.740.75P@3RCV1
0.1
0.3
0.5
0.7
0.9 (
c) Effect of 𝛾on RCV1
0 10 20 30 40 50
Number of active cycle0.540.550.560.570.580.59P@3AAPD
0.1
0.3
0.5
0.7
0.9 (
d) Effect of 𝛾on AAPD
Figure 10: Comparison of training and sampling time cost on AAPD (a)-(b). The impact of 𝛾on RCV1 and AAPD (c)-(d).
0 10 20 30 40 50
Number of active cycle0.660.680.700.720.740.76P@3RCV1
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
0 10 20 30 40 50
Number of active cycle0.540.550.560.570.580.59P@3AAPD
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
0 10 20 30 40 50
Number of active cycle0.520.540.560.580.600.620.640.66P@1JustDance
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
0 10 20 30 40 50
Number of active cycle0.460.480.500.520.540.56P@3JustDance
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
0 10 20 30 40 50
Number of active cycle0.650.680.700.720.750.780.800.82P@1EURLex-4K
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
0 10 20 30 40 50
Number of active cycle0.500.520.540.560.580.600.620.640.66P@3EURLex-4K
CoMAL w/o cl
CoMAL w/o rc
CoMAL-SLCI
CoMAL-PFD
CoMAL
Figure 11: Results of the ablation study on RCV1, AAPD,
JustDance and EURLex-4K datasets.B.3 Visualization
To better demonstrate the feature distribution of latent space S𝑧
in moduleM𝑐𝑙and the acquisition effect of our designed active
data sampling strategy, we exhibit t-SNE embeddings on the RCV1
dataset in Figures 12. We present the results of iterations 5, 15, 25,
35, 45 in the active learning process, which shows the embedding
distribution of 6 classes. It can be observed that CoMAL can attain
well-clustered and distinguishable embeddings with contrastive
learning in any round of iteration. CoMAL always gives prioritizes
to the instance whose feature embeddings are far from their re-
spective prototypes, especially those samples that are mistakenly
mixed with negative features. In the subsequent training iterations,
CoMAL leverages contrastive learning to re-cluster the feature em-
beddings of the enlarged labeled dataset D𝐿, thereby normalizing
(pulling) the newly sampled instances to the corresponding posi-
tive feature clusters. Iteratively, the feature diversity of the labeled
poolD𝐿is continuously enriched. Therefore, a complementary and
cooperative relationship is formed between contrastive representa-
tion learning and data acquisition to ensure the feature diversity of
the final sampled data.
B.4 Time Cost Comparison
To better illustrate the computational cost of our method, we com-
pared the time cost of different methods during model training and
data sampling. As shown in Figure ??, both in model training and
data acquisition, the time cost of CoMAL versus the other methods
is not obvious, which indirectly indicates the cost-effectiveness of
our method.
 
2374KDD ’24, August 25–29, 2024, Barcelona, Spain. Cheng Peng et al.
Negative samples
Positive samples
Prototypes
(a) Latent space (iteration 5 round)
Selected samples
Not-selected samples
Prototypes (b) Data acquisition (iteration 5 round)
Negative samples
Positive samples
Prototypes
(c) Latent space (iteration 15 round)
Selected samples
Not-selected samples
Prototypes (d) Data acquisition (iteration 15 round)
Negative samples
Positive samples
Prototypes
(e) Latent space (iteration 25 round)
Selected samples
Not-selected samples
Prototypes (f) Data acquisition (iteration 25 round)
Negative samples
Positive samples
Prototypes
(g) Latent space (iteration 35 round)
Selected samples
Not-selected samples
Prototypes (h) Data acquisition (iteration 35 round)
Negative samples
Positive samples
Prototypes
(i) Latent space (iteration 45 round)
Selected samples
Not-selected samples
Prototypes (j) Latent space (iteration 45 round)
Figure 12: T-SNE visualization of latent feature embeddings on RCV1 via CoMAL. The sub-figures on the right are the embedding
distribution of instances sampled or not sampled corresponding to the sub-figures on the left.
 
2375