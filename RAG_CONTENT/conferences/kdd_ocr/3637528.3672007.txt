ReCDA: Concept Drift Adaptation with Representation
Enhancement for Network Intrusion Detection
Shuo Yang∗
shuoyang.ee@gmail.com
The University of Hong Kong
HongKong SAR, ChinaXinran Zheng∗
zhengxr23@mails.tsinghua.edu.cn
Tsinghua University
Beijing, ChinaJinze Li
lijinze-hku@connect.hku.hk
The University of Hong Kong
HongKong SAR, China
Jinfeng Xu
jinfeng@connect.hku.hk
The University of Hong Kong
HongKong SAR, ChinaXingjun Wang
wangxingjun@tsinghua.edu.cn
Tsinghua University
Beijing, ChinaEdith C. H. Ngai†
chngai@eee.hku.hk
The University of Hong Kong
HongKong SAR, China
ABSTRACT
The deployment of learning-based models to detect malicious activ-
ities in network traffic flows is significantly challenged by concept
drift. With evolving attack technology and dynamic attack behav-
iors, the underlying data distribution of recently arrived traffic flows
deviates from historical empirical distributions over time. Existing
approaches depend on a significant amount of labeled drifting sam-
ples to facilitate the deep model to handle concept drift, which faces
labor-intensive manual labeling and the risk of label noise. In this pa-
per, we propose ReCDA, a Concept Drift Adaptation method with
Representation enhancement, which consists of a self-supervised
representation enhancement stage and a weakly-supervised classi-
fier tuning stage. Specifically, in the initial stage, ReCDA introduces
drift-aware perturbation and representation alignment to facilitate
the model in acquiring robust representations from drift-aware and
drift-invariant perspectives. Moreover, in the subsequent stage, a
meticulously crafted instructive sampling strategy and a robust
representation constraint encourage the model to learn discrim-
inative knowledge about benign and malicious activities during
fine-tuning, thereby enhancing performance further. We conduct
comprehensive evaluations on several benchmark datasets under
varying degrees of concept drift. The experiment results demon-
strate the superior adaptability and robustness of the proposed
method.
CCS CONCEPTS
•Security and privacy →Intrusion detection systems.
KEYWORDS
Intrusion Detection, Network Security, Concept Drift
∗The first two authors contributed equally to this research.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672007ACM Reference Format:
Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith
C. H. Ngai. 2024. ReCDA: Concept Drift Adaptation with Representation
Enhancement for Network Intrusion Detection. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3672007
1 INTRODUCTION
Network Intrusion Detection (NID) plays an essential role in ensur-
ing security by continuously identifying and discarding suspicious
activities in network traffic [ 8,47]. Deep Learning (DL) [ 1] has
emerged as a promising approach for intrusion detection due to its
capability to learn complex representations from raw traffic flows.
In recent years, DL-based intrusion detection systems have demon-
strated state-of-the-art performance [15, 31] and have found wide
applications in diverse domains [11, 17, 28, 52].
In today’s interconnected digital landscape, the relentless evo-
lution of cyber threats [ 25] demands robust intrusion detection
systems capable of adapting to the ever-changing nature of concept
drift. Concept drift refers to the phenomenon in which the statisti-
cal properties of monitored data change over time [ 6,33], posing
significant challenges to intrusion detection systems that rely on
static models [ 42,50]. Failure to adapt to concept drift may have
severe consequences, including increased false positives or missed
intrusions [ 4], potentially resulting in compromised systems and
sensitive data breaches [40].
Traditional ensemble techniques [ 34,51] for addressing concept
drift in intrusion detection have limitations in adaptability and
robustness. These approaches often struggle to keep pace with
the dynamic and complex nature of modern attacks, since they
fail to capture the nuanced patterns and subtle changes that occur
over time [ 44]. To bridge this gap, concept drift adaptation through
incremental learning or continuous learning [ 3,7,53,54] shows
promise by introducing new knowledge into the classifier to reduce
the cumulative prediction error. Although these approaches can
bring some performance improvement, they come at the cost of
increased manual labeling. Some existing work [ 3,26,48] used
pseudo-labels to update the model, which is more vulnerable to label
noise and hence susceptible to drastic performance deterioration
by self-poisoning [ 27]. Consequently, there is an urgent need for
3818
KDD ’24, August 25–29, 2024, Barcelona, Spain Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith C. H. Ngai
Source TCP sequence numberDensityDoS
Fuzzers
Destination to source time to liveDensityDoS
Fuzzers
The state and its dependent protocol DensityDoS
Fuzzers
Figure 1: Feature Distribution of DoS and Fuzzers Attacks.
novel and effective concept drift adaptation techniques to enhance
the robustness and reliability of intrusion detection systems.
In this paper, to address the aforementioned challenges, we pro-
posed ReCDA: a self-supervised concept drift adaptation method
with representation enhancement. Different from existing work [ 3,
26,50], we leverage the inherent characteristics of collected traf-
fic flows to guide the model in extracting both drift-aware and
drift-invariant representation, thus circumventing labor-intensive
manual labeling and the risk of noisy labels. Technically, we ini-
tially employ perturbation techniques to generate the drift view of
the original traffic flows, inspired by the observation that changes
in local features contribute to concept drift. As illustrated in Fig-
ure 1, discrepancies between attack variants are often manifested
in the feature distribution. Subsequently, the original flow and its
perturbed version are fed into a shared encoder network optimized
by contrastive loss to align the representations. This allows us to
acquire a robust feature extractor capable of narrowing the gap
between historical and drift traffic flows while maintaining the sep-
arability of instances. To impart discriminative knowledge about
benign and malicious activities, we sample some instructive in-
stances from the historical dataset. Given the robustness of the
representations obtained in the representation enhancement stage,
we utilize consistency regularization to constrain the expansion
of the representation space, thus mitigating the risk of overfitting
historical distribution. Through the above process, the model can
obtain a representation that is both conscious of concept drift and
resilient to it, thereby improving the classification performance and
robustness of the model in concept drift scenarios.
The contributions of this paper are as follows:
(1) We propose an advanced concept drift adaptation method
with representation enhancement. Our method benefits from the
representations with drift-aware and drift-invariant, thereby en-
hancing the performance of the intrusion detection model.
(2) We provide a novel perspective on concept drift adaptation
through feature perturbation. To the best of our knowledge, it is
the first self-supervised concept drift adaptation method tailored to
network intrusion detection, circumventing labor-intensive manual
labeling and the risk of label noise.
(3) We highlight the mild and insufficient nature of existing
evaluations of concept drift adaptation methods, which have only
been tested under limited degrees of drift. In contrast, we propose
a more rigorous evaluation setting.
(4) We conduct extensive experiments on several benchmark
datasets under varying degrees of concept drift. The experiment
results demonstrate the superior adaptability and robustness of the
proposed method.2 RELATED WORK
2.1 Network Intrusion Detection
Network intrusion detection is a potent technology for detecting
and responding to malicious and unauthorized activities within
networks. Recently, numerous learning-based methods [ 12,14,18,
19,35,39,55] have been proposed to safeguard networks against
advanced attacks. Qiu et al. [39] observed entangled distributions
of flow features and proposed a two-step feature disentanglement
approach along with a dynamic graph diffusion scheme to identify
various attacks. Mirsky et al. [35] introduced Kitsune, an unsuper-
vised online network intrusion detection method. It employs an
ensemble of autoencoders to collectively distinguish between be-
nign and malicious traffic patterns. Fu et al. [19] utilized frequency
domain features to develop a real-time machine learning-based
malicious traffic detection system that ensures high detection accu-
racy and throughput. Considering the increase in attack variants
and the blurring of the boundary between malicious and benign
activity. Yue et al. [55] designed heuristic contrastive tasks to mine
semantic relationships among samples. Diallo et al. [12] utilized
cluster centers to expand the features of a given dataset, which
improved the robustness and the generalization abilities of detec-
tion models. Despite the superior performance of these approaches,
they rely on the assumption of data stationarity in a static intrusion
detection environment [ 24]. Consequently, they cannot be applied
in dynamic environments experiencing concept drift.
2.2 Concept Drift Adaptation
Concept drift occurs due to the changes in statistical properties of
the data features over time [ 33], resulting in a gradual decline in the
performance of models well-trained on historical data. To address
this challenge, concept drift detection and adaptation techniques
are required. Concept drift detection aims to identify when concept
drift occurs in the data. The detection methods typically monitor
the model’s performance [ 38] or track the statistical properties of
the incoming data [ 13,29,50]. When significant deviations from
the expected behavior are observed, it indicates the presence of
concept drift. For example, CADE [ 50] uses contrastive learning to
map the data samples into a low-dimensional space and learns a
distance function to measure dissimilarity between samples, which
presented satisfactory detection performance in security applica-
tions. After concept drift is detected, adaptation strategies need to
be employed to update the model and mitigate its impact, which is
crucial to improving model robustness.
The goal of adaptation is to make the model resilient to the chang-
ing data distribution. The concept drift adaptation challenge can be
addressed using two potential solutions: incremental learning [ 23]
and ensemble learning [ 34], to effectively deal with the drift and en-
able the model to adapt quickly to the new data patterns. Ensemble
learning involves combining multiple base learners to construct an
ensemble model with better generalization ability. Liu et al. [30] de-
veloped an instance-based ensemble learning algorithm, which can
dynamically select different classifiers in varying concept drift situ-
ations. To cope with concept drift issues in imbalanced data streams,
Halder et al. [21] proposed an autonomic active learning strategy
with a cluster-based ensemble classifier. The selection and gener-
ation of a sub-classifier require elaborate strategies, which may
3819ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 2: The Overview of Proposed ReCDA.
not be suitable for complex network intrusion detection tasks [ 44].
In contrast, incremental learning methods address concept drift
by retraining or altering the model [ 23,45] to accommodate both
drift data and historical data. Chen et al. [7] proposed a continual
learning method to combat the concept drift problem of Android
malware classifiers, which apply similarity-based uncertainty to
select new samples for analysts to label and retrain the classifier.
To alleviate intensive manual labeling, Andresini et al. [3] utilized
the nearest centroid neighbor strategy to generate pseudo-labels,
achieving satisfactory performance on network intrusion detection
tasks. However, these approaches are quite vulnerable to label noise
and prone to self-poisoning of pseudo-labels [27].
3 METHODOLOGY
3.1 Preliminaries
3.1.1 Notation. In the sequel, we use uppercase and lowercase
letters to denote matrices and vectors, respectively. Besides, |D|
represents the total number of elements in set D.
3.1.2 Problem Definition. Letx∈R𝑑be a network traffic flow
comprising 𝑑attributes or dimensions. Consider xℎ∈R𝑑and
x𝑟∈R𝑑represent previously and recently collected samples, re-
spectively.𝑦∈R2:{0,1}is the corresponding binary label of x
(i.e. benign or malicious). Given a labeled historical dataset Dℎ=
{(xℎ
1,𝑦ℎ
1),(xℎ
2,𝑦ℎ
2),...,(xℎ
|Dℎ|,𝑦ℎ
|Dℎ|)}, an unlabeled recent dataset
D𝑟={x𝑟
1,x𝑟
2,...,x𝑟
|D𝑟|}, and a hold-out test dataset D𝑡={(x𝑡
1,𝑦𝑡
1),
(x𝑡
2,𝑦𝑡
2),...,(x𝑡
|D𝑡|,𝑦𝑡
|D𝑡|)}. Let𝑃(xℎ)and𝑃(x𝑟)represent distribu-
tions ofDℎandD𝑟, respectively, andD𝑡shares a similar distribu-
tion withDℎ∪D𝑟. Under the concept drift problem setting [ 33],
D𝑟is considered to drift from Dℎ, which means 𝑃(xℎ)≠𝑃(x𝑟), re-
sulting in the well-trained model on Dℎfailing to generalize on D𝑡.
Due to the difference between the two distributions, the goal of con-
cept drift adaptation is to learn a statistical model M(𝜃):R𝑑→R2
using all the given data in DℎandD𝑟to minimize the prediction
errorÍ|D𝑡|
𝑖=1|ˆ𝑦𝑡
𝑖−𝑦𝑡
𝑖|, where ˆ𝑦𝑡
𝑖is the predicted label of the 𝑖-th testinstance by the model M(𝜃), and𝑦𝑡
𝑖is the corresponding ground
truth.M(𝜃)can be expressed as 𝑓◦𝑔, where𝑓is a feature extrac-
tor mapping input xto its latent representation and 𝑔coherently
generates binary predictions based on the representation.
3.1.3 Architecture Overview. We illustrate the proposed ReCDA
in Figure 2, which is a robust concept drift adaptation method
with representation enhancement for concept drift without relying
on any manual labeling in a network environment subject to the
continuous emergence of attack variants. ReCDA consists of two
primary stages:
(1) Self-supervised representation enhancement. During this
stage, carefully designed drift-aware perturbation and representa-
tion alignment modules are employed to map traffic flows from the
original space to the latent space, ensuring drift-aware and drift-
invariant. Here’s a detailed explanation of these two properties:
•Drift-aware: ReCDA can integrate information from both
the historical and drift traffic flows, allowing it to adjust its
representation accordingly.
•Drift-invariant: ReCDA remains stable and unaffected by
changes in underlying data distribution, thus ensuring ro-
bustness to concept drift.
(2) Weakly-supervised classifier tuning. During this stage, we
propose an instructive sample selection module and a classifier fine-
tuning module based on representation constraint to efficiently
incorporate discriminative knowledge into the model, further en-
hancing predictive robustness under concept drift scenarios. More
technical details are described as follows.
3.2 Drift-aware Perturbation
According to the problem definition, concept drift refers to distribu-
tion shifting between the historical dataset and the recent dataset.
While the intuitive would be to minimize the prediction error by
narrowing the gap between the distributions of the historical and
drifted data, estimating the distribution of drifted samples is chal-
lenging in practice due to constantly emerging attack variants and
3820KDD ’24, August 25–29, 2024, Barcelona, Spain Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith C. H. Ngai
the risk of model collapse [ 5] caused by strict distribution consis-
tency. Our proposed method is based on a key observation: for
traffic flows, distribution drift corresponds to feature drift in the
original space, wherein local changes in feature values drive dis-
tribution drift. Therefore, we expect that the model can perceive
changes in the features of unlabeled drift flows and capture the
relationship between the historical traffic flows and them.
To implement this idea, we propose a drift-aware perturbation
method that involves randomly sampling the marginal distribution
of features in the drift sample and perturbing the historical sample
with a certain probability. Unlike common perturbation methods
such as adding random noise [ 22,43] or replacing features with
meaningless values [ 20,49,55], our approach aims to maintain
semantic retention of features while introducing randomness to
encourage the model to perceive feature changes in unlabeled drift
flows and capture their correlation with the original flow samples.
As shown in Figure 2, a mask vector m=[𝑚1,𝑚2,...,𝑚𝑑]𝑇is
generated, where the vector elements 𝑚𝑘∈{0,1},𝑘∈[1,𝑑]are
extracted from a Bernoulli distribution with a certain probability 𝜎.
Subsequently, a sample xℎ
𝑖from the historical dataset and the mask
vector are used jointly as inputs. Drift-aware perturbation samples
are generated as follows:
˜x𝑖=m⊙x𝑟
𝑖+(1−m)⊙ xℎ
𝑖. (1)
Using Eq. (1), ˜x𝑖can be obtained by sampling the feature’s mar-
ginal distribution of the drift sample. The process bridges the gap
between xℎandx𝑟, allowing for the smooth elimination of the
distribution discrepancies under the control of 𝜎. The parameter
𝜎can be considered a perturbed rate that regulates the degree
of fusion between the two distributions. The generated samples
gain awareness of drift while retaining the semantic information of
the original flow. The randomness introduced by the perturbation
increases the diversity of the traffic views, which indicates that
our method can better simulate feature drift in real concept drift
scenarios.
3.3 Representation Alignment
Following perturbation, we obtain the drift-aware perspective ˜x𝑖
of the selected original sample xℎ
𝑖. After that, we aim for the model
to map drift samples and historical samples to adjacent latent sub-
spaces and learn representations invariant to drift. To achieve this
goal, we employ the encoder network as a feature extractor and
utilize contrastive learning [ 37] to encourage the representation of
˜x𝑖andxℎ
𝑖to be close. Specifically, for each historical sample x𝑖in a
mini-batchB, we generate its perturbed view ˜x𝑖using Eq. (1). Then
we feed them to the encoder network 𝑓, and the resulting outputs
are passed to a projection head ℎto obtain the corresponding latent
representations z𝑖=ℎ◦𝑓(x𝑖)and˜z𝑖=ℎ◦𝑓(˜x𝑖). The contrastive
loss can be expressed as:
L𝑟𝑎(x𝑖)=−log©­
«exp(z𝑖·˜z𝑖/𝜏)
Í|B|
𝑗=1exp z𝑖·˜z𝑗/𝜏ª®
¬, (2)
where𝜏>0is the temperature parameter. Minimizing Eq. (2)
ensures that z𝑖becomes closer to ˜z𝑖and be farther from ˜z𝑗for
𝑖≠𝑗, thereby yielding a more generalized representation withdrift-aware and drift-invariant. Thus, the parameter optimization
of the encoder in the representation enhancement stage can be
formalized as follows:
𝜃∗
𝑓=min
𝜃𝑓h
L𝑟𝑎
x𝑖|𝜃𝑓i
, (3)
where𝜃∗
𝑓denote updated parameters of encoder network 𝑓.
3.4 Instructive Sampling strategy
In the preceding stage, although we obtained a drift-robust fea-
ture extractor, its direct utilization for detecting malicious activities
within the network is impeded by a crucial limitation—it lacks
knowledge in distinguishing between benign and malicious net-
work activities. To address this limitation and facilitate the attain-
ment of a robust classifier, we consider using labeled samples from
the historical dataset to fine-tune both the feature extractor and
classifier. This refinement injects discriminative knowledge into the
model, enhancing its ability to discern between benign and mali-
cious network activities. However, incorporating a large number of
labeled samples from Dℎduring fine-tuning tends to exacerbate the
risk of overfitting to the historical distribution 𝑃(xℎ), consequently
yielding a vanilla classifier that inadequately recognizes drifting
traffic flows. To mitigate this issue, we advocate for the selective use
of instructive historical instances as the source of discriminative
knowledge. Here, we introduce the instructive sampling strategy,
delineated by the following two principles:
•Instructive samples should be distant from drift samples in
original space.
•Instructive samples should resemble the representation of
the drift sample in latent space.
The original space encompasses samples directly extracted from
the dataset, while the latent space denotes the embedding space of
the feature extractor output. These principles guide the selection
process, ensuring that the chosen instructive samples possess char-
acteristics conducive to model training and drift adaptation. The
degradation of the model caused by concept drift can generally be at-
tributed to the classifier’s challenge in effectively detecting attacks
based on the representations generated by the feature extractor.
Specifically, the features of recently arrived malicious samples drift,
exhibiting confusion with benign samples. Distinguishing these
samples in the original space is challenging because they corre-
spond to similar feature values and are usually assigned to the same
category. Hence, our intuition is to select labeled historical samples
located at the potential decision boundary, as their representations
contain richer category-discriminative knowledge. To facilitate the
model in capturing the generalized difference between benign and
malicious activities under the guidance of such a sub-dataset, we
introduce the instructive matrix 𝐺∈R|Dℎ|×2for determining the
selection of samples. It is defined as follows:
𝐺𝑖,𝑗=𝑠𝑖𝑚(xℎ
𝑖,¯x𝑟
𝑗|¯y𝑗)
𝑠𝑖𝑚(𝑓(xℎ
𝑖),𝑓(¯x𝑟
𝑗)|¯y𝑗)+𝜖. (4)
Here,𝑠𝑖𝑚(·)represents the distance measurement function, with
the cosine distance being used due to its effectiveness in measur-
ing the similarity between high-dimensional vectors. ¯x𝑟
𝑗denotes
clustering centroids of samples belonging to the category ¯y𝑗in the
3821ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection KDD ’24, August 25–29, 2024, Barcelona, Spain
original space. Specifically, binary clustering is performed on the
unlabeled recent dataset to obtain the clustering centroids involved
in the distance calculation, which accelerates computation and re-
duces storage requirements. Additionally, an extra small value 𝜖is
added to the denominator for numerical stability.
According to Eq. (4), higher instructive metrics indicate more
influential samples in constructing the decision boundary. Hence,
the top-𝛿historical samples will be selected to participate in the
constrained classifier tuning phase.
3.5 Constrained Classifier Tuning
To train a classifier for network intrusion detection, we integrate a
classification head 𝑔to the encoder network 𝑓, which takes the out-
put of𝑓as the input and predicts the label of the instance. During
the classifier tuning stage, we expect that the robust feature extrac-
tor that has been generated will not lose the valuable knowledge
that has been gained. Therefore, we constrain the variation of the
representation for subsequent learning to maintain the effective-
ness of the robust feature extractor. The consistency regularization
loss is defined as:
L𝑐𝑟(x𝑖)=𝑓(x𝑖|𝜃𝑓)−𝑓(x𝑖|𝜃∗
𝑓)2
. (5)
L𝑐𝑟aims to encourage the tuned feature extractor 𝑓(𝜃∗
𝑓)to re-
turn a similar output distribution learned during the representation
enhancement stage. In addition, we employ cross-entropy loss to
guide the classifier in acquiring category-discriminative knowledge.
LetL𝑐𝑒(x𝑖,𝑦𝑖)represent the supervised loss, the parameters opti-
mization of the feature extractor and classifier in the fine-tuning
stage is formalized as follows:
𝜃∗
𝑔,𝜃∗
𝑓=min
𝜃𝑔,𝜃𝑓h
L𝑐𝑒
x𝑖,𝑦𝑖|𝜃𝑔,𝜃𝑓
+𝜆L𝑐𝑟
x𝑖|𝜃𝑓i
. (6)
Here,𝜃∗𝑔denotes updated parameters of classifier 𝑔, and𝜆is a
balanced coefficient to control the regularization strength. By opti-
mizing Eq. (6), the model obtains category-discriminative knowl-
edge, while preserving the drift awareness and drift invariance
representation obtained during the representation enhancement
stage. This results in a generalized model capable of robust concept
drift adaptation. The algorithm of ReCDA is shown in Algorithm 1.
4 EXPERIMENTS
This section outlines evaluation settings, which provide a more
challenging evaluation compared with common settings. Following
the settings, we conduct a comparative analysis to gauge the effec-
tiveness and robustness of the proposed concept drift adaptation
method against existing approaches.
4.1 Evaluation Setting
Previous studies [ 13,29,50] predominantly conducted experiments
under the one-vs-rest setting, wherein only one type of attack is
deemed as drift. However, we observe that this evaluation approach
is conservative and often yields an overly optimistic assessment
of performance. To address this limitation, we advocate for a more
rigorous evaluation method that substantially intensifies the con-
cept drift by concurrently considering multiple attack categoriesAlgorithm 1 Algorithm of ReCDA
Input: historical datasetDℎ, recent datasetD𝑟, batch size|B|,
perturbation rate 𝜎, temperature 𝜏, constant𝜖, coefficient 𝜆
encoder network 𝑓, projection head ℎ, classifier𝑔.
Output: encoder network 𝑓and classifier 𝑔
1:let𝑑be the dimension of input samples.
2:let𝜃and𝜃∗be model parameters before and after one iteration.
3:forsampled mini-batch {xℎ
𝑖}|B|
𝑖=1⊆Dℎdo
4:for𝑖=1,...,|B|do
5: generate m∈R𝑑where𝑚𝑘∈[0,1]and𝑝(𝑚𝑘=1)=𝜎.
6: sample x𝑟
𝑖fromD𝑟.
7: let˜x𝑖=m⊙x𝑟
𝑖+(1−m)⊙ xℎ
𝑖.
8: letz𝑖=ℎ◦𝑓(xℎ
𝑖),˜z𝑖=ℎ◦𝑓(˜x𝑖).
9:end for
10: letL𝑟𝑎:=−1
|B|Í|B|
𝑖=1log
exp(z𝑖·˜z𝑖/𝜏)Í|B|
𝑗=1exp(z𝑖·˜z𝑗/𝜏)
.
11: update𝑓andℎto minimizeL𝑟𝑎.
12:end for
13:clusterD𝑟to get centroids ¯y∈[0,1].
14:generate𝐺∈R|Dℎ|×2, where𝐺𝑖,𝑗=𝑠𝑖𝑚(xℎ
𝑖,¯x𝑟
𝑗|¯y𝑗)
𝑠𝑖𝑚(𝑓(xℎ
𝑖),𝑓(¯x𝑟
𝑗)|¯y𝑗)+𝜖.
15:select top-𝛿samples fromDℎaccording𝐺to generate sub-
datasetDℎ
𝑠𝑢𝑏.
16:forsampled mini-batch {xℎ
𝑖}|B|
𝑖=1⊆Dℎ
𝑠𝑢𝑏do
17: letL𝑐𝑟:=1
|B|Í|B|
𝑖=1𝑓(x𝑖|𝜃𝑓)−𝑓(x𝑖|𝜃∗
𝑓)2
.
18: update𝑓and𝑔to minimizeL𝑐𝑒+𝜆L𝑐𝑟.
19:end for
as drift data. Furthermore, evaluating model stability across vary-
ing degrees of drift is challenging when using a non-uniform test
dataset. To ensure a fair evaluation, we propose adhering to the
original dataset partition or adopting a fixed training and test set
split method. We deal with two widely used network intrusion
detection datasets to simulate the real concept drift environment.
The details are as follows.
4.1.1 UNSW-NB15. The detailed description of the UNSW-NB15
dataset [ 36] can be found in Appendix A.1. To evaluate our method,
we iteratively select samples from different sets of malicious cat-
egories to serve as drift data. In this way, we split the original
training set into various combinations of historical dataset Dℎand
recent datasetD𝑟, withDℎbeing labeled andD𝑟unlabeled. Here,
we define the drift index to quantify the degree of drift:
C#𝑖:𝑖=|𝐿(Dℎ∪D𝑟)−𝐿(Dℎ)|, (7)
where𝐿(Dℎ)and𝐿(D𝑟)represent the number of categories in
the historical dataset and recent dataset, respectively. For example,
C#2 indicates that the Dℎcomprises four categories of malicious
activity, while the remaining two types appear in the D𝑟. For any𝑖,
there are𝐶𝑖
6=6!
𝑖!(6−𝑖)!data partitioning cases, which can be viewed
as a combination problem. For better comparison, the reported
results in the sequel are averaged across all data partitioning cases.
4.1.2 CICIDS-2017. The detailed description of the CICIDS-2017
dataset [ 16] can be found in Appendix A.2. CICIDS-2017 provides
3822KDD ’24, August 25–29, 2024, Barcelona, Spain Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith C. H. Ngai
(a) UNSW-NB15
 (b) CICIDS-2017
Figure 3: Optimal Transport Dataset Distance for UNSW-
NB15 Dataset and CICIDS-2017 Dataset.
timestamps for traffic flows so we can obtain drift data more consis-
tently with real-world scenarios. Initially, we partition each day’s
data into the training and test sets with a ratio of 8:2. The entire
traffic flow sequence in the training set can be formalized as D=
(X𝑡,𝑌𝑡):𝑡=1,2,3,4,5, whereX𝑡represent the traffic trace col-
lected in the 𝑡-th day,𝑌𝑡is the corresponding label of X𝑡. Then we di-
vide theDinto the historical dataset Dℎ=(X1,𝑌1),...,(X𝑖+1,𝑌𝑖+1)
and recent datasetD𝑟=(X𝑖+2,𝑌𝑖+2),...,(X5,𝑌5). Time-based split-
ting implies that the recent dataset contains sample features absent
from the historical dataset, thus inducing concept drift. Accordingly,
the drift index can be defined as T@i, where for 𝑡>𝑖+1the label
of traffic instance is not available in the training phase.
We use the Optimal Transport Dataset Distance (OTDD) [ 2] to
explore the distribution differences over categories or timestamps.
OTDD is a geometric method for calculating distances between
probability distributions to compare datasets. For better illustration,
a min-max scale is applied to the calculated distance matrix. As
depicted in Figure 3, whether the dataset is split over categories or
timestamps, it introduces varying degrees of drift into the evalua-
tion. For example, the distribution of flows collected on Wednesday
in the CICIDS-2017 dataset exhibits more significant differences
compared to other days, posing a greater challenge for adapting to
concept drift in the case of T@1.
4.2 Experimental Setup
We present the experimental setup in this subsection, including
baseline methods, evaluation metrics, and parameter settings.
4.2.1 Baseline Methods. We compare the proposed method against
three common machine learning methods: Logistic Regression (LR),
K-Nearest Neighbors (KNN) [ 10], and Decision Tree (DT) [ 32].
We also include the latest method [ 12,18,55] for network traf-
fic classification and two state-of-the-art concept drift adaption
methods [ 3,50] for network intrusion detection. Here is a brief
introduction to them.
LEXNet [18] is a lightweight, efficient, and explainable convo-
lutional neural network designed for network traffic classification.
It relies on a new residual block and prototype layer and shows
superior performance on a commercial-grade dataset.
CLEID [55] is an intrusion detection framework based on con-
trastive learning, which utilizes a heuristic method to constructsample pairs based on random masking effectively. Semantic re-
lationships among different samples are extracted to enhance the
robustness of the model.
ACID [12] is a classifier-agnostic and highly effective intrusion
detection system. It introduces supervised adaptive clustering tech-
niques to learn cluster centers that can be used as extensions of the
input features, which improves the robustness against outliers and
the generalization abilities of detection models. We followed their
disclosed implementation, i.e. feed-forward networks are employed.
CADE [50] is a representative method for adapting to concept
drift by labeling the detected drifting samples. Specifically, CADE
maps the data samples into a low-dimensional space and learns
a distance function to measure dissimilarity between samples. To
ensure a fair comparison, we followed their disclosure setting and
adapted the implementation to binary classification. It should be
noted that CADE is primarily designed for drift detection, so we
calculate the union of correct drift detection and correct classifica-
tion to evaluate its performance. In other words, for a drift sample,
as long as CADE can detect its drift, even if the classification result
is wrong, it still contributes to final accuracy.
INSOMNIA [3] is a semi-supervised intrusion detector that
updates the base model as network traffic characteristics are affected
by concept drift. They also consider the cost of labeling, so in
the model update phase, they use Nearest Centroid (NC)-based
strategies to generate pseudo-labels for drift adaptation. It is worth
noting that our focus is on robust concept drift adaptation methods
aimed at achieving good generalization on drift traffic flows, rather
than continuous incremental learning. Therefore, the base model of
INSOMNIA is well trained on Dℎ, andD𝑟is provided to INSOMNIA
for model update at once.
4.2.2 Evaluation Metrics. We utilize Accuracy and F1-score to
quantify the performance of the proposed method and the com-
petitors. Accuracy measures overall correctness, while the F1-score
balances precision and recall. These metrics provide a comprehen-
sive evaluation of classification accuracy and robustness in concept
drift adaptation.
4.2.3 Parameter Settings. In the representation enhancement stage,
we use two fully connected layers as the feature extractor, followed
by a linear head consisting of a fully connected layer. The hidden
layer size is all set to 16. In the classifier tuning stage, we initial-
ize a single fully connected layer as a classifier to perform binary
classification, i.e. distinguish between benign and malicious traffic,
which has the same input dimension as the output of the feature
extractor. Both stages are trained with Adam optimizer using the
initial learning rate of 0.0001 and batch size of 128. The represen-
tation model and classifier model are trained for 100 epochs and
50 epochs, respectively. All experiments are run on NVIDIA 4090
GPUs for fair comparisons.
4.3 Experiment Results
In this section, we first compare the overall performance of ReCDA
with other baselines and evaluate the robustness of the model to
drift adaptation under varying degrees of drift. Subsequently, we
delve into a more detailed investigation under a fixed drift index.
3823ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparisons on UNSW-NB15 Dataset.
MethodC#1 C#2 C#3 C#4 C#5
ACC F1 ACC F1 ACC F1 ACC F1 ACC F1
LR 0.81±0.03 0.80±0.04 0.80±0.06 0.79±0.06 0.78±0.08 0.78±0.08 0.75±0.09 0.74±0.09 0.69±0.09 0.68±0.10
KNN 0.84±0.03 0.84±0.04 0.83±0.05 0.83±0.05 0.81±0.09 0.80±0.09 0.77±0.07 0.77±0.07 0.70±0.07 0.69±0.07
DT 0.87±0.03 0.87±0.03 0.87±0.05 0.87±0.05 0.84±0.10 0.84±0.10 0.81±0.11 0.81±0.12 0.72±0.13 0.71±0.14
LEXNet 0.86±0.05 0.88±0.04 0.86±0.06 0.87±0.07 0.81±0.09 0.83±0.08 0.74±0.14 0.67±0.29 0.63±0.12 0.46±0.26
ACID 0.86±0.03 0.85±0.03 0.84±0.06 0.84±0.06 0.82±0.07 0.81±0.07 0.78±0.07 0.78±0.07 0.72±0.06 0.71±0.06
CLEID 0.87±0.02 0.87±0.02 0.87±0.03 0.87±0.03 0.85±0.05 0.84±0.05 0.81±0.06 0.81±0.06 0.73±0.09 0.72±0.09
INSOMNIA 0.88±0.05 0.86±0.05 0.85±0.06 0.84±0.06 0.82±0.07 0.80±0.09 0.75±0.09 0.70±0.15 0.70±0.11 0.52±0.28
CADE 0.84±0.05 0.88±0.04 0.82±0.05 0.86±0.03 0.79±0.03 0.84±0.02 0.80±0.03 0.84±0.02 0.79±0.03 0.84±0.02
ReCDA 0.91±0.01 0.91±0.01 0.91±0.03 0.90±0.03 0.89±0.04 0.89±0.04 0.87±0.06 0.87±0.06 0.86±0.05 0.86±0.04
Table 2: Comparisons on CICIDS-2017 Dataset.
MethodT@1 T@2 T@3
ACC F1 ACC F1 ACC F1
LR 0.596 0.577 0.761 0.747 0.768 0.756
KNN 0.641 0.615 0.814 0.808 0.778 0.773
DT 0.508 0.351 0.710 0.684 0.711 0.685
LEXNet 0.511 0.674 0.808 0.832 0.761 0.783
ACID 0.652 0.619 0.814 0.808 0.814 0.813
CLEID 0.640 0.605 0.816 0.809 0.814 0.808
INSOMNIA 0.854 0.851 0.808 0.767 0.808 0.768
CADE 0.850 0.835 0.952 0.950 0.963 0.961
ReCDA 0.872 0.872 0.954 0.954 0.963 0.963
4.3.1 Overall Evaluation. In Table 1, we report the experimental
results on the UNSW-NB15 dataset. As described in Section 4.1.1, for
drift index C#i, we iteratively considered 𝑖type(s) of attacks among
the6malicious categories as drift data. The results presented are the
averages and standard deviations under all category combinations.
From the results in Table 1, it is evident that our method exhibits
excellent adaptability and stability in handling concept drift across
different degrees of drift. Particularly in the case of extreme drift
(C#5), our method maintains an accuracy that is 7% higher than
that of the sub-optimal method.
For all considered machine learning models and LEXNet, we
observe performance degradation as the degree of drift increases,
emphasizing the inability of these intrusion detection models to
cope with changes in data distribution resulting from concept drift.
For example, the accuracy of LEXNet drops from 0.86at C#1 to
0.63at C#5, performing even worse than LR. This indicates that
while the elaborated DL-based model fits the historical dataset
well, it struggles to generalize in concept drift scenarios. Regarding
ACID and CLEID designed to improve the generalization ability of
intrusion detectors, the results show their limited drift adaptability,
which is vulnerable under more rigorous evaluation settings.
C#1 is widely used as the benchmark setting for concept drift,
i.e. only a type of attack is unknown in the training phase. How-
ever, we have found that this setting tends to give an illusion ofhigh performance, as the performance deteriorates rapidly with an
increase in the degree of drift. For example, INSOMNIA reaches a
competitive accuracy of 0.88at C#1 but suffers from significant per-
formance degradation by 6% and 18% when shifted to C#3 and C#5,
accompanied by a steep increase in standard deviation. Recall that
INSOMNIA applies NC-based strategies to generate pseudo-labels
for drift adaptation. The experiment results show that the estima-
tion of pseudo-labels is impractical when lack of sufficient neighbor
samples, and the performance of the model is easily affected by the
label noise, which leads to the phenomenon of self-poisoning. In
comparison to INSOMNIA, CADE and ReCDA demonstrate greater
resilience to increasing degrees of drift, with ReCDA exhibiting an
accuracy 10% and 7% higher than that of CADE at C#3 and C#5,
respectively.
In practical applications, the dynamic changes of network envi-
ronments and user behaviors cause traffic flows to undergo concept
drift, resulting in the deteriorating performance of intrusion detec-
tion systems over time. Therefore, we evaluate the drift adaptability
of models by partitioning the CICIDS-2017 dataset based on times-
tamps. From the experimental results in Table 2, it can be seen
that ReCDA obtains the best performance for all situations. While
CADE closely approximates the performance of our method at T@2
and T@3, it is important to note that, as mentioned in Section 4.1.2,
we have adopted a tolerant measure for CADE.
Additionally, for T@1, the performances of DT and LEXNet
are approximately equivalent to random guessing, with 0.511and
0.508of accuracy, respectively. Despite having parameters and
complexity that far exceed baseline models like LR or KNN, LEXNet,
as an intrusion detector, exhibits extremely poor generalization
performance on concept drift data. When the distribution in the
historical dataset is uncorrelated with that in the drift dataset, the
model tends to learn specific biases from the historical dataset,
making generalization and adaptation to concept drift challenging.
Overall, the experimental results reveal the rapid degradation
of traditional models as the degree of drift increases, underscoring
their unreliability. Furthermore, these experiments also highlight
the issues of the current concept drift adaptation methods and
demonstrate the robustness of our proposed approach under vary-
ing degrees of drift.
3824KDD ’24, August 25–29, 2024, Barcelona, Spain Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith C. H. Ngai
LR KNN DT LEXNet ACID CLEID INSOMINA CADE ReCDA
Method0.50.60.70.80.91.0Accuracy
Figure 4: The Detailed Results at C#2 on UNSW-NB15 Dataset.
Table 3: The Influence of Perturbation Rate.
𝜎 0.3 0.4 0.5 0.6 0.7 0.8
UNSW-NB15 0.819 0.845 0.839 0.847 0.831 0.813
CICIDS-2017 0.833 0.854 0.876 0.878 0.897 0.849
4.3.2 Detailed Evaluation. We take a closer look at the detailed per-
formance at C#2 on the UNSW-NB15 dataset. The results depicted
in Figure 4 reveal that various combinations of malicious activities
exert different degrees of impact on the intrusion detection model,
a phenomenon linked to the similarity of their feature distributions.
As shown in Figure 3(a), certain types of attacks exhibit similar
distances, enabling the model to generalize effectively to other cat-
egories even when labels for one category are unavailable during
training. Nevertheless, our method consistently achieves high accu-
racy and outperforms the other methods across all cases. Moreover,
the area occupied by ReCDA is notably smaller than that of its
competitors, indicating that our method exhibits consistency at
various degrees of drift. This superior property shows that ReCDA
has strong adaptability and robustness.
5 FURTHER ANALYSIS
In this section, we conduct further analysis of ReCDA at T@2 and
C#1, where Generic attack in UNSW-NB15 is regarded as drift.
5.1 Parameter Study
We investigate the influence of hyper-parameters by varying their
values. Our experiment approach involves fixing one parameter
while altering the other.
5.1.1 The influence of 𝜎.To assess the impact of perturbation rate
𝜎, we followed the settings outlined in Section 4.2.3, except for
employing a frozen encoder during the classifier tuning stage. We
vary𝜎from 0.3 to 0.8. As shown in Table 3, the optimal performance
on UNSW-NB15 dataset is attained when 𝜎=0.6, while for CICIDS-
2017 dataset, the value is 𝜎=0.7.𝜎serves as the control parameter
determining the fusion degree of historical and drift samples, where
the contrastive loss evaluates the similarity between the original
flow and its perturbed version. Therefore, excessively large values
of𝜎may destroy the semantics of traffic flows, whereas overly
small values may lead to insufficient awareness of concept drift.Table 4: The Influence of 𝛿and𝜆on UNSW-NB15 Dataset.
𝜆𝛿25% 50% 75% 100%
0.1 0.893 0.901 0.888 0.887
0.5 0.881 0.899 0.913 0.908
0.8 0.869 0.885 0.885 0.898
1.0 0.850 0.866 0.874 0.876
Table 5: The Influence of 𝛿and𝜆on CICIDS-2017 Dataset.
𝜆𝛿25% 50% 75% 100%
0.1 0.856 0.854 0.855 0.853
0.5 0.880 0.893 0.893 0.893
0.8 0.943 0.954 0.950 0.943
1.0 0.907 0.918 0.913 0.898
Table 6: Ablation Study of ReCDA.
MethodUNSW-NB15 CICIDS-2017
ACC F1 ACC F1
ReCDA 0.913 0.911 0.954 0.954
ReCDA w/o IS 0.908 0.906 0.943 0.943
ReCDA w/o IS, CT 0.895 0.869 0.849 0.848
ReCDA w/o IS, CT, RA 0.751 0.716 0.761 0.747
5.1.2 The influence of 𝛿and𝜆.The classifier tuning stage includes
two hyper-parameters, the instructive sample selection rate 𝛿, and
a factor𝜆to regulate the regularization strength.
We respectively tune the two hyper-parameters in a range of
𝛿∈{25%,50%,75%,100%}and𝜆∈{0.1,0.5,0.8,1.0}. The results
are illustrated in Table 4 and Table 5. According to the results, the
accuracy fluctuates with the change of 𝛿. A large𝛿may lead to
overfitting on the historical dataset, diminishing awareness of drift
data, whereas a smaller 𝛿may not furnish the classifier with suffi-
cient discriminative insights. Selection of 𝜆is dataset-dependent;
for example, UNSW-NB15 favors a smaller value ( 𝜆=0.5), whereas
CICIDS-2017 shows preference for a larger one ( 𝜆=0.8).
5.2 Ablation Study
In this section, we conduct an ablation study to analyze the per-
formance gain of each component in ReCDA by removing them
gradually. RA, IS, and CT are the abbreviations of representation
alignment, instructive sampling, and constrained tuning, respec-
tively. The results are shown in Table 6.
The results consistently demonstrate that ReCDA outperforms
its variants. Each component in ReCDA contributes to enhancing
the predictive model’s performance, with the optimal performance
achieved when they collaborate in our unified framework. We note
that the removal of CT leads to a more significant performance
drop in the CICIDS-2017 dataset compared to the UNSW-NB15
dataset. This difference can be attributed to the more severe data
3825ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection KDD ’24, August 25–29, 2024, Barcelona, Spain
UNSW-NB15
H-BenignH-Attack
CICIDS-2017
R-BenignR-Attack
(
a) Orginal Space
 (
b) Latent Space (Baseline)
 (
c) Latent Space (Directly)
 (
d) Latent Space (ReCDA)
Figure 5: t-SNE Visualizations on UNSW-NB15 and CICIDS-2017 Dataset. We sample part of the traffic flows from DℎandD𝑟,
denoted H- and R-, respectively. (a) We visualize the samples in the original space. (b) We train a 3-layer MLP as a baseline. (c)
We directly generate the representation using the feature extractor of ReCDA. (d) We use the entire ReCDA.
shift in CICIDS-2017, resulting in the feature extractor and predic-
tive model being prone to overfitting historical traffic flows while
losing awareness of the drift flows. This observation aligns with
the findings in Section 5.1.2. In addition, we observe that the pro-
posed representation alignment significantly enhances the deep
representations, and the followed instructive sampling strategy fur-
ther improves the classification performance even further, which
justifies our claims.
5.3 Visualization
We employ the t-SNE technique [ 41] to visualize original space and
latent space. The visualization results on UNSW-NB15 and CICIDS-
2017 datasets are presented in Figure 5. It is observed that both
benign and attack flows in recent arrivals deviate from the original
data distribution, making the decision boundary of a well-trained
model on the historical dataset that is challenging to generalize on
drifting data. For example, column (b) shows the baseline can fit
H-Benign and H-Attack on the CICIDS-2017 datasets perfectly but
struggles with R-Attack. Recall that our method maps historical
data and drift data from the separated original distribution to a
unified latent distribution through drift-aware perturbation and
representation alignment. Columns (c) and (d) demonstrate that
H-Attack and R-Attack instances are notably closer in the enhanced
representation of ReCDA compared to columns (a) and (b), indi-
cating the drift-aware of our approach. Furthermore, our proposed
instructive sampling strategy selects those representative instances
near the fuzzy boundary to provide the classifier with discrimi-
native knowledge, facilitating the construction of a drift-robust
decision boundary.
6 CONCLUSION
The paper introduces ReCDA, a novel method aimed at addressing
the challenges of concept drift adaptation in network intrusion
detection. ReCDA incorporates drift-aware perturbation and self-
supervised representation alignment to facilitate the learning of
robust representations that are both drift-aware and drift-invariant.
This approach avoids the need of labor-intensive manual labelingand mitigates the risk of label noise. Additionally, ReCDA integrates
an instructive sampling strategy that is meticulously designed to
enhance the classifier’s discriminatory capabilities between be-
nign and malicious activities, leveraging the robust representations
extracted. To comprehensively evaluate ReCDA, we surpassed con-
ventional settings, devising a more realistic and challenging evalu-
ation scenario. The experimental findings underscore the superior
adaptability and robustness exhibited by ReCDA, positioning it
as a promising candidate for addressing concept drift in network
intrusion detection. Future work could focus on extending our
method to other security domains, such as malware detection [ 9]
and encrypted traffic classification [46].
ACKNOWLEDGMENTS
This work was supported by the Hong Kong UGC General Research
Fund no. 17203320 and 17209822, and the project grants from the
HKU-SCF FinTech Academy.
REFERENCES
[1]Arwa Aldweesh, Abdelouahid Derhab, and Ahmed Z Emam. 2020. Deep learning
approaches for anomaly-based intrusion detection systems: A survey, taxonomy,
and open issues. Knowledge-Based Systems 189 (2020), 105124.
[2]David Alvarez-Melis and Nicolo Fusi. 2020. Geometric dataset distances via
optimal transport. Advances in Neural Information Processing Systems 33 (2020),
21428–21439.
[3]Giuseppina Andresini, Feargus Pendlebury, Fabio Pierazzi, Corrado Loglisci,
Annalisa Appice, and Lorenzo Cavallaro. 2021. Insomnia: Towards concept-
drift robustness in network intrusion detection. In Proceedings of the 14th ACM
workshop on artificial intelligence and security. 111–122.
[4]Federico Barbero, Feargus Pendlebury, Fabio Pierazzi, and Lorenzo Cavallaro.
2022. Transcending transcend: Revisiting malware classification in the presence
of concept drift. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE,
805–823.
[5]Adrien Bardes, Jean Ponce, and Yann LeCun. 2022. VICReg: Variance-Invariance-
Covariance Regularization for Self-Supervised Learning. arXiv:2105.04906
[6]Firas Bayram, Bestoun S Ahmed, and Andreas Kassler. 2022. From concept
drift to model degradation: An overview on performance-aware drift detectors.
Knowledge-Based Systems 245 (2022), 108632.
[7]Yizheng Chen, Zhoujie Ding, and David Wagner. 2023. Continuous learning
for android malware detection. In 32nd USENIX Security Symposium (USENIX
Security 23). 1127–1144.
[8]Dylan Chou and Meng Jiang. 2021. A survey on data-driven network intrusion
detection. ACM Computing Surveys (CSUR) 54, 9 (2021), 1–36.
3826KDD ’24, August 25–29, 2024, Barcelona, Spain Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, and Edith C. H. Ngai
[9]Theo Chow, Zeliang Kan, Lorenz Linhardt, Lorenzo Cavallaro, Daniel Arp, and
Fabio Pierazzi. 2023. Drift Forensics of Malware Classifiers. In Proceedings of the
16th ACM Workshop on Artificial Intelligence and Security. 197–207.
[10] Cristiano Antonio De Souza, Carlos Becker Westphall, Renato Bobsin Machado,
João Bosco Mangueira Sobral, and Gustavo dos Santos Vieira. 2020. Hybrid ap-
proach to intrusion detection in fog-based IoT environments. Computer Networks
180 (2020), 107417.
[11] Abdelouahid Derhab, Mohamed Belaoued, Irfan Mohiuddin, Fajri Kurniawan,
and Muhammad Khurram Khan. 2021. Histogram-based intrusion detection and
filtering framework for secure and safe in-vehicle networks. IEEE Transactions
on Intelligent Transportation Systems 23, 3 (2021), 2366–2379.
[12] Alec F Diallo and Paul Patras. 2021. Adaptive clustering-based malicious traffic
classification at the network edge. In IEEE INFOCOM 2021-IEEE Conference on
Computer Communications. IEEE, 1–10.
[13] Mirabelle Dib, Sadegh Torabi, Elias Bou-Harb, Nizar Bouguila, and Chadi Assi.
2022. EVOLIoT: A self-supervised contrastive learning framework for detecting
and characterizing evolving IoT malware variants. In Proceedings of the 2022 ACM
on Asia Conference on Computer and Communications Security. 452–466.
[14] Hongwei Ding, Yu Sun, Nana Huang, Zhidong Shen, and Xiaohui Cui. 2023.
TMG-GAN: Generative Adversarial Networks-Based Imbalanced Learning for
Network Intrusion Detection. IEEE Transactions on Information Forensics and
Security 19 (2023), 1156–1167.
[15] Zhaoxu Ding, Guoqiang Zhong, Xianping Qin, Qingyang Li, Zhenlin Fan,
Zhaoyang Deng, Xiao Ling, and Wei Xiang. 2024. MF-Net: Multi-frequency
intrusion detection network for Internet traffic data. Pattern Recognition 146
(2024), 109999.
[16] Gints Engelen, Vera Rimmer, and Wouter Joosen. 2021. Troubleshooting an
intrusion detection dataset: the CICIDS2017 case study. In 2021 IEEE Security and
Privacy Workshops (SPW). IEEE, 7–12.
[17] Mojtaba Eskandari, Zaffar Haider Janjua, Massimo Vecchio, and Fabio Antonelli.
2020. Passban IDS: An intelligent anomaly-based intrusion detection system for
IoT edge devices. IEEE Internet of Things Journal 7, 8 (2020), 6882–6897.
[18] Kevin Fauvel, Fuxing Chen, and Dario Rossi. 2023. A lightweight, efficient and
explainable-by-design convolutional neural network for internet traffic classifica-
tion. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 4013–4023.
[19] Chuanpu Fu, Qi Li, Meng Shen, and Ke Xu. 2021. Realtime robust malicious
traffic detection via frequency domain analysis. In Proceedings of the 2021 ACM
SIGSAC Conference on Computer and Communications Security. 3431–3446.
[20] Jie Gu and Shan Lu. 2021. An effective intrusion detection approach using SVM
with naïve Bayes feature embedding. Computers & Security 103 (2021), 102158.
[21] Bohnishikha Halder, KM Azharul Hasan, Toshiyuki Amagasa, and Md Manjur
Ahmed. 2023. Autonomic Active Learning Strategy using Cluster-based Ensemble
Classifier for Concept Drifts in Imbalanced Data Stream. Expert Systems with
Applications (2023), 120578.
[22] Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu,
Xingang Shi, and Xia Yin. 2021. Evaluating and improving adversarial robustness
of machine learning-based network intrusion detectors. IEEE Journal on Selected
Areas in Communications 39, 8 (2021), 2632–2647.
[23] Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu. 2020. Incremental
learning in online scenario. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. 13926–13935.
[24] Dhruba Jyoti Kalita, Vibhav Prakash Singh, and Vinay Kumar. 2023. A novel
adaptive optimization framework for SVM hyper-parameters tuning in non-
stationary environment: A case study on intrusion detection system. Expert
Systems with Applications 213 (2023), 119189.
[25] Nektaria Kaloudi and Jingyue Li. 2020. The ai-based cyber threat landscape: A
survey. ACM Computing Surveys (CSUR) 53, 1 (2020), 1–34.
[26] Zeliang Kan, Feargus Pendlebury, Fabio Pierazzi, and Lorenzo Cavallaro. 2021.
Investigating labelless drift adaptation for malware detection. In Proceedings of
the 14th ACM Workshop on Artificial Intelligence and Security. 123–134.
[27] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang
Chiu, Supun Samarasekera, and Nazanin Rahnavard. 2023. C-SFDA: A Curricu-
lum Learning Aided Self-Training Framework for Efficient Source Free Domain
Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 24120–24131.
[28] Shu-Yu Kuo, Fan-Hsun Tseng, and Yao-Hsin Chou. 2023. Metaverse intrusion
detection of wormhole attacks based on a novel statistical mechanism. Future
Generation Computer Systems 143 (2023), 179–190.
[29] Aditya Kuppa and Nhien-An Le-Khac. 2022. Learn to adapt: Robust drift detection
in security domain. Computers and Electrical Engineering 102 (2022), 108239.
[30] Anjin Liu, Jie Lu, and Guangquan Zhang. 2020. Diverse instance-weighting
ensemble based on region drift disagreement for concept drift adaptation. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 293–307.
[31] Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher, and Marius
Portmann. 2022. E-graphsage: A graph neural network based intrusion detection
system for iot. In NOMS 2022-2022 IEEE/IFIP Network Operations and Management
Symposium. IEEE, 1–9.[32] Maya Hilda Lestari Louk and Bayu Adhi Tama. 2023. Dual-IDS: A bagging-based
gradient boosting decision tree model for network anomaly intrusion detection
system. Expert Systems with Applications 213 (2023), 119030.
[33] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018.
Learning under concept drift: A review. IEEE transactions on knowledge and data
engineering 31, 12 (2018), 2346–2363.
[34] Leandro L Minku and Xin Yao. 2011. DDD: A new ensemble approach for dealing
with concept drift. IEEE transactions on knowledge and data engineering 24, 4
(2011), 619–633.
[35] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. 2018. Kit-
sune: An Ensemble of Autoencoders for Online Network Intrusion Detection. In
Network and Distributed Systems Security (NDSS) Symposium.
[36] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for
network intrusion detection systems (UNSW-NB15 network data set). In 2015
military communications and information systems conference (MilCIS). IEEE, 1–6.
[37] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[38] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and
Lorenzo Cavallaro. 2019. TESSERACT: Eliminating experimental bias in malware
classification across space and time. In 28th USENIX security symposium. 729–746.
[39] Chenyang Qiu, Yingsheng Geng, Junrui Lu, Kaida Chen, Shitong Zhu, Ya Su,
Guoshun Nan, Can Zhang, Junsong Fu, Qimei Cui, et al .2023. 3D-IDS: Dou-
bly Disentangled Dynamic Intrusion Detection. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 1965–1977.
[40] Xiaokui Shu, Danfeng Yao, and Elisa Bertino. 2015. Privacy-preserving detection
of sensitive data exposure. IEEE transactions on information forensics and security
10, 5 (2015), 1092–1103.
[41] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[42] Omar Abdel Wahab. 2022. Intrusion detection in the iot under data and concept
drifts: Online deep learning approach. IEEE Internet of Things Journal 9, 20 (2022),
19706–19716.
[43] Ning Wang, Yimin Chen, Yang Xiao, Yang Hu, Wenjing Lou, and Y Thomas Hou.
2022. Manda: On adversarial example detection for network intrusion detection
system. IEEE Transactions on Dependable and Secure Computing 20, 2 (2022),
1139–1153.
[44] Xian Wang. 2022. ENIDrift: A Fast and Adaptive Ensemble System for Network
Intrusion Detection under Real-world Drift. In Proceedings of the 38th Annual
Computer Security Applications Conference. 785–798.
[45] Yinjun Wu, Edgar Dobriban, and Susan Davidson. 2020. Deltagrad: Rapid retrain-
ing of machine learning models. In International Conference on Machine Learning.
PMLR, 10355–10366.
[46] Renjie Xie, Jiahao Cao, Enhuan Dong, Mingwei Xu, Kun Sun, Qi Li, Licheng
Shen, and Menghao Zhang. 2023. Rosetta: Enabling Robust TLS Encrypted
Traffic Classification in Diverse Network Environments with TCP-Aware Traffic
Augmentation. In 32nd USENIX Security Symposium. 625–642.
[47] Congyuan Xu, Jizhong Shen, and Xin Du. 2020. A method of few-shot network
intrusion detection based on meta-learning framework. IEEE Transactions on
Information Forensics and Security 15 (2020), 3540–3552.
[48] Ke Xu, Yingjiu Li, Robert Deng, Kai Chen, and Jiayun Xu. 2019. Droidevolver: Self-
evolving android malware detection system. In 2019 IEEE European Symposium
on Security and Privacy (EuroS&P). IEEE, 47–62.
[49] Haonan Yan, Xiaoguang Li, Wenjing Zhang, Rui Wang, Hui Li, Xingwen Zhao,
Fenghua Li, and Xiaodong Lin. 2023. Automatic evasion of machine learning-
based network intrusion detection systems. IEEE Transactions on Dependable and
Secure Computing (2023).
[50] Limin Yang, Wenbo Guo, Qingying Hao, Arridhana Ciptadi, Ali Ahmadzadeh,
Xinyu Xing, and Gang Wang. 2021. CADE: Detecting and explaining concept drift
samples for security applications. In 30th USENIX Security Symposium (USENIX
Security 21). 2327–2344.
[51] Li Yang, Dimitrios Michael Manias, and Abdallah Shami. 2021. Pwpae: An
ensemble framework for concept drift adaptation in iot data streams. In 2021
IEEE Global Communications Conference (GLOBECOM). IEEE, 01–06.
[52] Shuo Yang, Xinran Zheng, Zhengzhuo Xu, and Xingjun Wang. 2023. A Light-
weight Approach for Network Intrusion Detection based on Self-Knowledge
Distillation. In ICC 2023-IEEE International Conference on Communications. IEEE,
3000–3005.
[53] Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. 2019.
Adaptive deep models for incremental learning: Considering capacity scalability
and sustainability. In Proceedings of the 25th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining. 74–82.
[54] Susik Yoon, Youngjun Lee, Jae-Gil Lee, and Byung Suk Lee. 2022. Adaptive
Model Pooling for Online Deep Anomaly Detection from a Complex Evolving
Data Stream. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2347–2357.
[55] Yawei Yue, Xingshu Chen, Zhenhui Han, Xuemei Zeng, and Yi Zhu. 2022. Con-
trastive learning enhanced intrusion detection. IEEE Transactions on Network
and Service Management 19, 4 (2022), 4232–4247.
3827ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection KDD ’24, August 25–29, 2024, Barcelona, Spain
A DESCRIPTION OF DATASETS
A.1 UNSW-NB15 Dataset
Table 7: The Statistics of UNSW-NB15 Dataset.
Category Training Set Test Set
Benign 56000 37000
DoS 12264 4089
Exploits 33393 11132
Fuzzers 18184 6062
Generic 40000 18871
Reconnaissance 10491 3496
OthersAnalysis 2000 677
Backdoors 1746 583
Worms 130 44
Shellcode 1133 378
Total 175341 82332
The raw network packets of the UNSW-NB15 dataset [ 36] were
collected by the IXIA PerfectStorm tool in the Cyber Range Lab at
the University of New South Wales (UNSW), Australia. The dataset
is designed to simulate real-world network traffic and includes a
diverse range of network traffic features, encompassing both be-
nign and nine malicious activities, namely, DoS, Exploits, Fuzzers,
Generic, Reconnaissance, Analysis, Backdoors, Worms, and Shell-
code. For reliable evaluation, we maintain the original dataset split:
175,341 training data instances and 82,332 testing data instances, in
which the test set contains all nine types of attacks. Given that the
drift of the less prevalent attack categories has minimal impact on
the evaluation, we merge the four attacks denoted as "Others", the
statistics of categories in the training and test dataset are shown in
Table 7.A.2 CICIDS-2017 Dataset
Table 8: The Statistics of CICIDS-2017 Dataset.
Day Benign Malicious Total # of Categories
Monday 371689 0 371689 1
Tuesday 315008 6953 321961 4
Wednesday 319186 171790 490976 6
Thursday 359973 222 360195 5
Friday 291213 254884 546097 4
The CICIDS-2017 dataset, developed by the Canadian Institute
for Cybersecurity (CIC) at the University of New Brunswick, is a
comprehensive and widely used benchmark dataset for network
intrusion detection research. Following the recommendation in
INSOMINA [ 3], we use the refined version of CICIDS-2017 [ 16].
The new version reconstructed or relabelled 20 percent of the orig-
inal traffic traces to rectify issues related to traffic generation, flow
construction, feature extraction, and labeling. Consequently, the
dataset comprises 2,524,767 timestamped flows and 72 features, cov-
ering 5 days and encompassing 15 malicious categories, including
brute force attacks, Heartbleed, botnet communication, DoS and
DDoS variants, infiltration, and web-related threats.
3828