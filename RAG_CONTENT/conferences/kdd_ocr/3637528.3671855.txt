Generative Pretrained Hierarchical Transformer for
Time Series Forecasting
Zhiding Liu
University of Science and Technology
of China
State Key Laboratory of Cognitive
Intelligence
Hefei, Anhui, China
zhiding@mail.ustc.edu.cnJiqian Yang
University of Science and Technology
of China
State Key Laboratory of Cognitive
Intelligence
Hefei, Anhui, China
yangjq@mail.ustc.edu.cnMingyue Cheng∗
University of Science and Technology
of China
State Key Laboratory of Cognitive
Intelligence
Hefei, Anhui, China
mycheng@ustc.edu.cn
Yucong Luo
University of Science and Technology
of China
State Key Laboratory of Cognitive
Intelligence
Hefei, Anhui, China
prime666@mail.ustc.edu.cnZhi Li
Shenzhen International Graduate
School, Tsinghua University
Shenzhen, Guangdong, China
zhilizl@sz.tsinghua.edu.cn
Abstract
Recenteffortshavebeendedicatedtoenhancingtimeseriesfore-
castingaccuracybyintroducingadvancednetworkarchitectures
andself-supervisedpretrainingstrategies.Nevertheless,existing
approaches still exhibit two critical drawbacks. Firstly, these meth-
ods often rely on a single dataset for training, limiting the model’s
generalizability due to the restricted scale of the training data. Sec-
ondly,theone-stepgenerationschemaiswidelyfollowed,which
necessitates a customized forecasting head and overlooks the tem-
poraldependenciesintheoutputseries,andalsoleadstoincreased
training costs under different horizon length settings.
Toaddresstheseissues,weproposeanovelgenerativepretrained
hierarchicaltransformerarchitectureforforecasting,named GPHT.
There are two aspects of key designs in GPHT. On the one hand,
weadvocateforconstructingamixeddatasetunderthechannel-
independent assumption for pretraining our model, comprising
various datasets from diverse data scenarios. This approach signif-
icantly expands the scale of training data, allowing our model to
uncovercommonalitiesintimeseriesdataandfacilitatingimproved
transfer to specific datasets. On the other hand, GPHT employs an
auto-regressive forecasting approach, effectively modeling tempo-
raldependenciesintheoutputseries.Importantly,nocustomized
forecastingheadisrequired,enabling asinglemodeltoforecastatar-
bitraryhorizonsettings. Weconductsufficientexperimentsoneight
datasets withmainstream self-supervisedpretraining modelsand
∗Mingyue Cheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671855supervised models. The results demonstrated that GPHT surpasses
thebaselinemodelsacrossvariousfine-tuningandzero/few-shot
learning settings in the traditional long-term forecasting task, pro-
vidingsupportforverifyingthefeasibilityofpretrainingtimeseries
large models. We make our codes publicly available1.
CCS Concepts
•Mathematics of computing →Time series analysis;•Com-
puting methodologies →Artificial intelligence.
Keywords
Time series forecasting; deep learning; pretraining
ACM Reference Format:
Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, and Zhi Li. 2024.
GenerativePretrainedHierarchicalTransformerforTimeSeriesForecasting.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
andDataMining(KDD’24),August25–29,2024,Barcelona,Spain. ACM,New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671855
1 Introduction
Time series forecasting is one of the fundamental tasks in time
seriesanalysis,garneringsignificantattentionoverthepastseveral
years[35,41].Preciseforecastingplaysacrucialroleinassisting
various real-world applications, including climate report [ 1,50],
patientvitalsignassessments[ 30],urbancomputing[ 24]andstock
prediction[ 23]etc.Variouseffortshavebeendevotedtothisareafor
moreaccurateforecasting.Notably,deep-learning-basedmethods
have achieved great success due to their capability to capture both
temporal and cross-dimension dependencies [28, 31, 51].
On the other hand, inspired by recent significant advancements
inpretrainingmethodsinboththeNLPandCVfields[ 3,12,18,20],
various pretraining-based time series analysis methods have been
proposed [ 49]. The contrastive learning technique is widely em-
ployedinthediscriminativepretrainingmethods,wherethemodels
1https://github.com/icantnamemyself/GPHT
 
2003
KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhiding Liu et al.
areexpectedtolearnrepresentationsfromconstructedpositiveand
negative pairs [ 42,45]. Furthermore, the incorporation of gener-
ative targets, such as masked time-series modeling, into the pre-
training task has also been well-studied, with the aim of extracting
general knowledge during the reconstruction process [ 7,13,38].
Additionally, considering the shared characteristics between time
series and natural languages, some recent studies have emerged
to adapt pre-trained language models into accurate forecasters
through prompting or fine-tuning [ 19,54]. All these methods have
achieved significant success, even competing effectively with su-
pervised forecasting approaches.
Despitetheireffectiveness,thereremainsignificantchallenges
in promoting the performance of the pretrained forecasters. Firstly,thelimitedscaleofthedatasetisacriticalissue.Thestandardprac-
ticeofthesemethodsinvolvespretrainingonasinglereal-world
orsyntheticdataset,andevaluatingitsperformanceonthisdataset
orotherdatasetsthroughtransferlearning[ 10,14].However,the
amount of training instances in a single dataset is often limited
[17],anditsinherentpatternsmayfailtoencompassthecomplex
scenarios of other datasets, leading to suboptimal forecasting accu-
racyandtransferability.Secondly,nearlyallforecastingapproaches
adhere to the one-step generation schema [ 52], which implies that
the predictions for all future time steps are generated through a
singleforwardpasswithacustomizedforecastingheaddetermined
by a specified horizon length. The drawbacks of this paradigm are
multifaceted.On theone hand,the temporaldependencieswithin
thepredictedseriesareinevitablyoverlooked,potentiallyleadingto
an inferior result. On the other hand, the tailored forecasting head
hindersthegeneralizabilityofthepretrainedmodels,asmultiple
models are required for different horizon length settings.
Toalleviatetheabovechallenges,wearemotivatedtoexplore
pretraining a single unified forecasting model that generalizes
well across diverse data scenarios and forecasting settings with
anovelgenerativehierarchicaltransformerarchitecture,namely
GPHT. Firstly for the dataset construction, we extend the channel-
independentassumption[ 46]intomultipledatascenarios,simply
mixing time series originating from various scopes as a whole
withoutconsideringextrainformation,whichprovidesvastcharac-
teristicssuchasdiverseperiodicitiesthatbenefitthepertainingpro-cedure.Besides,tobettercapturethecommonalitiesandspecialties
within the mixed dataset, we naturally introduce a novel hierar-
chical transformer architecture as the backbone model [ 4,37]. Fur-
thermore,weformulatetheforecastingtaskasastandardlanguage
modeling task with the patchingtechnique [ 8,31], which projects
time series into token-level representations, and an auto-regressive
optimization function is applied in our pretraining procedure to re-
place the conventional one-step generating schema. Consequently,
themodelcanwellmodelthetemporaldependenciesintheforecast-inghorizonsatatoken-wiselevelthroughauto-regressiveinference
and can be seamlessly adapted to diverse datasets with varying
horizon length settings without any modification. We compare our
model’s performance with state-of-the-art supervised and pretrain-
ing methods under various fine-tuning and zero/few-shot learning
settings. The results demonstrate the superiority and generalizabil-
ity of the proposed GPHT.
In summary, our contributions are as follows:•We explore pretraining a single unified forecasting model
thatgeneralizeswellacrossdiversescenarioswiththepre-
training dataset constructed under the channel-independent
assumption, which allows for the easy creation of diverse,
large-scale datasets, forming the foundation for the general-
izability across data scenarios of the forecasting model.
•WeintroduceGPHT,anovelhierarchicaltransformerfore-
castinginanauto-regressivemanner.Thisdesigninherently
aids in modeling both the commonalities and specialties
withinthemixeddataset,guaranteeinguniversalityunder
various forecasting settings.
•Weconductsufficientexperimentson 8widelyusedbench-
markdatasets,comparingourproposedGPHTwithmain-stream supervised and pretraining methods. The results
showthatourmodelsurpassesthebaselinemodelsacross
various fine-tuning and zero/few-shot learning settings.
2 Related Works
2.1 Time Series Forecasting
Time series forecasting is a crucial task with broad applications,
garnering significant attention in recent years. Early research pre-
dominantly focuses on statistical methods suchas ARIMA [ 2,48],
whichbuildsanauto-regressivemodelandforecastsinamoving
average fashion. However, these methods may encounter limita-
tionsinlong-termforecastingsettings.Theadventofdeeplearning
has led to the development of numerous models capturing both
temporal and cross-dimensional dependencies in multivariate time
series, utilizing modern architectures [9, 26, 29, 34, 36].
Transformer-based and MLP-based approaches have emerged
asresearchhotspotsduetotheiroutstandingperformance[ 5,27,
46]. Beyond model architecture, several customized techniques
rooted in time series analysis have been established. These in-
cludetrend-seasonaldecomposition[ 44],time-frequencyconver-
sion[53],seriesstabilization[ 25],andpatching[ 31].Theintegra-
tionoftheseadvancedstudiesenablescontemporaryforecasters
to achieve unprecedented accuracy in predictions across diverse
scenarios through supervised training.
2.2 Self-supervised Pretraining in Time Series
Modeling
2.2.1 Discriminativemethods. Contrastivelearningiswidelyuti-
lizedinthediscriminativetimeseriesmodelingapproaches,aiming
to derive crucial representations from pre-defined positive and
negativepairs.Akeychallengeliesineffectivelyconstructingin-
formative instance pairs.
Inpractice,TNC[ 39]takesadvantageofthelocalsmoothness
of a signal’s generative process to define neighborhoods in time
with stationary properties, and TS2Vec [ 45] proposes to employ
contrastive learning on bothinstance level and patch level ina hi-
erarchicalwayforrobustcontextualrepresentationlearning.More-
over, TS-TCC [ 15] introduces a new contrastive learning task of
cross-viewprediction.Subsequently,CoST[ 42]comprisesbothtime
domain and frequency domain contrastive losses to learn discrimi-
nativetrendandseasonalrepresentations.Notethatdiscriminative
methods primarily concentrate on coarse-grained instance-level
 
2004Generative Pretrained Hierarchical Transformer for
Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain.
information, leading to unsatisfactory performance in forecasting
tasks where fine-grained temporal features are essential.
2.2.2 Generativemethods. Generativepretrainingmethodsusually
followaparadigmofreconstruction.Inthecontextoftimeseries
analysis,theobjectiveofmaskedtime-seriesmodelinghasbeenex-
tensivelyexplored.TST[ 47]pioneerstheuseoftraditionalmasked
modeling,aimingtopredicttheremovedtimeseriespointsbased
on the remaining ones. Subsequently, STEP [ 38] and PatchTST
[31] expand on this concept to a sub-series level, where local infor-
mationismoreeffectivelycaptured,andcomputationalcostsare
significantly reduced. Furthermore, a recent study [ 13] achieves
superiorfine-tuningperformancebyintroducinganovelmasked
modeling task, involving the reconstruction of the original time
seriesfrommultiplerandomlymaskedseries.Besides,TimeMAE
[7]significantlysurpassespreviouscompetitivebaselinesinclas-
sificationtasksbyleveragingdecoupledmaskedautoencodersto
learn robust representations through two pretext tasks: masked
codeword classification and masked representation regression.
On the other hand, the forecast-as-pretraining schema has also
been a subject of recent research. ForecastPFN [ 14] introduces a
prior-data fitted network trained on synthetic data and achievesaccurate zero-shot forecasting on univariate time series. Besides,both TimeGPT-1 [
16] and PreDcT [ 10] explore the potential of
trainingafoundationmodelforforecasting,yieldingzero-shotfore-castingcapabilityunderrelativelyshorthorizonlengths.Moreover,
there is another related line of work focusing on adapting tradi-tional pretrained generative language models to the time series
domain,eitherthroughprompting[ 19]orfine-tuning[ 54].These
approacheshavedemonstratedcompetitiveresultswhencompared
to the traditional supervised approaches.
Despitetheeffectiveness,existingmethodsexhibittwokeyshort-
comings. Firstly, these models are typically trained on a single
dataset with limited scale and patterns, impeding their ability to
generalizetodiverseforecastingscenarios.Secondly,bothsuper-
visedandself-supervisedapproachesoftennecessitateacustomized
forecasting head, incurring multiple training costs under different
horizon length settings. Regarding our proposed method, GPHT, it
standsoutasagenerativeself-supervisedpretrainingapproach,dis-
tinguishingitselffromexistingmethodsbyeffectivelyaddressing
theseissues.Indetail,weinvestigatethefeasibilityofconstructinga
mixed dataset for training our model in an auto-regressive manner.
This approach ensures the superior generalizability of GPHT, al-
lowingittobeseamlesslyadaptedtoanydataset,includingunseen
datasets, and forecast at arbitrary horizon lengths. Notably, experi-
ments demonstrate that our method surpasses the baseline models
acrossvariousfine-tuningandzero/few-shotlearningsettingsin
the traditional long-term forecasting task.
3 Proposed Method
Inthissection,wewilldelveintothespecificsoftheproposedGPHT
method illustrated in Figure 1, demonstrating its capacity to effi-
ciently acquire precise time series forecasting through pretraining
on the mixed dataset.3.1 Problem Definition
Given an input series X∈R𝐶×𝐿, a time series forecasting model is
tasked with precisely predicting future values Y∈R𝐶×𝐻. Here, 𝐿,
𝐻, and𝐶represent the lookback window length, horizon length,
and the number of channels, respectively.
GPHT adopts the channel-independent assumption [ 31,46],
treating each multivariate time series as multiple independent uni-
variate time series. In essence, GPHT conducts individual fore-
casting oneach variate withinthe input series,and the resultant
forecasts are concatenated to generate the final predictions.
Moreover, we extend this methodology to the construction of
the mixed pretraining dataset, where the heterogeneity of each
variableisdiscardedandnoextrainformationistakenintoaccount.
It can be therefore seamlessly applied to more diverse scenarios
where the covariate information may be missing and the data itself
maybesynthetic.Inpractice,weconcatenatethetrainingsegments
fromvariousreal-worlddatasetstoconstitutethetrainingsetofthemixeddataset.Thisprocessissimilarlyappliedtothevalidationandtestingportions.OurapproachensuresthatGPHTispretrainedona
richvarietyoftemporalpatterns,therebyenhancingitsadaptability
and generalization capabilities across diverse time series domains.
3.2 Series Tokenization
Giventhatthemajorityoftimeseriesoriginatefromsignalscap-
tured by real-world sensors, the data inherently carry noise, and
information is often sparsely distributed across the time points.
Consequently,employingauto-regressivetrainingonpoint-wise
time series might yield suboptimal performance due to the risk of
overfitting outliersand theassociatederror accumulation[ 52], in
addition to incurring high computation costs.
To address these challenges, we employ the series tokenization
technique, a proven effective approach in time series modeling
[22,31,51]. Specifically, we adopt a non-overlapping tokenization
strategy,reshapingtheinputseries Xintoasequenceoftimeseries
tokens, 𝑥∈R𝐶×𝐿/prime×𝑇, where 𝑇×𝐿/prime=𝐿,𝑇represents the token
length, and 𝐿/primecan be considered as the sequence length. The to-
kenization strategy not only helps mitigate the impact of noise
and sparse information distribution but also enhances the model’s
abilitytobettercapturethelocalsemantics,ultimatelycontributing
to more robust and accurate time series forecasting.
Furthermore,recentresearchhashighlightedaprevalentissue
in time series data, characterized by a distribution shift [ 11,28,33]
which means that the mean and variance of time series changes
over time. This challenge significantly hinders the generalizability
of deep-learning-based forecasters. The impact of this phenome-non is even more pronounced in the context of pretraining on a
mixeddataset,whereseriesinherentlystemfromdistincttempo-
raldistributions.Tomitigatethisissue,weintroduceanInstance
Normalization layer [ 25], designed to address the distribution shift
problem by normalizing the input series using the formula:
𝑥𝑖𝑛=𝑥−𝜇
𝜎+𝜖and 𝜇=E[X],𝜎2=Var[X].(1)
Here, 𝜖is a small constant, and 𝜇,𝜎represent the instance-specific
mean and standard deviation of X, respectively. Subsequently, 𝑥0
𝑖𝑛
is fed into the model for further feature extraction. This normal-
ization step enhances the model’s robustness to distribution shifts,
 
2005KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhiding Liu et al.
Instance Normalization & Series TokenizationReverse Instance Normalization
ߤǡߪMulti-stage Hierarchical Transformer Blocks
ݔ௜௡௜Down SamplingForecast Head & Up Sampling
Causal Multi-head AttentionAdd & Layer NormFeed-forward LayerAdd & Layer NormZero
Padding
M ൈN ൈݔ௢௨௧௜ݔ௜௡௜ାଵ Iterative Residual
Mixed Dataset
Figure 1: Illustration of the proposed GPHT model with two key features: (a) GPHT forecasts in an auto-regressive manner on
the time series tokens. (b) Pretrained on the mixed dataset with the multi-stage hierarchical transformer blocks, GPHT excels
in capturing the commonalities among time series originating from various data scenarios.
promoting more effective learning and improved generalization
across diverse temporal patterns.
3.3 Hierarchical Transformer Blocks
Multi-scale representationlearning has demonstrated its effective-
nessinvarioustimeseriesmodelingtasks[ 6,37],giventhemultiple
periodic characteristics commonly found in real-world time series
data. Furthermore, to better discover commonalities hidden within
mixed datasets comprising various data scenarios, we posit the
indispensability of a hierarchical encoder.
In practice, drawing inspiration from the multi-rate sampling
strategy [ 4], we introduce a token-level multi-stage representa-
tion learning approach within our hierarchical transformer blocks:
Suppose 𝑥𝑖istheinputseriesaftertokenizationofstage 𝑖,amax-
pooling operation with a kernel size of 𝑘𝑖is applied on each token,
down-sampling the original data into 𝑥𝑖
𝑖𝑛∈R𝐶×𝐿/prime×𝑇
𝑘𝑖. This opera-
tionretainsacoarse-grainedportionoftheoriginaldata,compelling
theencodernetworktofocusonmodelingcoarsepatterns.After-
ward, a standard multi-layer transformer network is employed for
representation learning [40]:
ℎ𝑖𝑛
𝑖=𝑃𝐸𝑖(𝑥𝑖𝑛
𝑖)+𝐸𝑚𝑏 𝑖(𝑥𝑖𝑛
𝑖)
ℎ𝑜𝑢𝑡
𝑖=𝑇𝑅𝑀 𝑖(ℎ𝑖𝑛𝑖).(2)
Here, 𝑃𝐸representsaregularpositionembeddinglayer,and 𝐸𝑚𝑏is
a token projection layer projecting a time series token into the hid-
den space of the transformer. Besides, we employ the decoder-only
transformers as the backbone, which incorporates causal attention
maskstopreventinformationleakageduringtheauto-regressive
generating process.Finally,thelearnedhiddenstates ℎ𝑜𝑢𝑡
𝑖arefedintotheforecasting
head,whichisalinearlayer,topredictfuturevaluesforeachtoken.
Inthisregard,weadoptanup-samplingoperation,whichcanbe
either linear interpolation or MLP, to map the predictions back
totheoriginaltimeseriestokens,yieldingthepredictions 𝑥𝑖
𝑜𝑢𝑡of
stage 𝑖. This process is represented as:
𝑥𝑖
𝑜𝑢𝑡=𝑈𝑝𝑆𝑎 𝑚𝑝𝑙𝑖𝑛 𝑔 (𝐹𝑜𝑟𝑒𝑐𝑎𝑠𝑡(ℎ𝑜𝑢𝑡
𝑖)). (3)
Thiscomprehensiveapproachtomulti-stagerepresentationlearn-
ingandforecastingallowsforthecapturingofintricatetemporal
patternsatdifferentscales,contributingtothemodel’seffectiveness
in handling diverse time series scenarios.
3.4 Iterative Residual Learning
Utilizing the multi-stage hierarchical transformer blocks, GPHTisexpectedtolearncoarse-to-finerepresentationseffectively.To
fullyleveragetheserepresentations,weproposeanoveliterative
residual learning strategy [ 21,32], transforming the forecasting
process into an iterative approach.
Specifically,asillustratedinFigure1,theinputofstage 𝑖+1is
the residual of stage 𝑖’s input and output, defined as:
𝑥𝑖+1
𝑖𝑛=𝑥𝑖
𝑖𝑛−𝑃𝑎𝑑𝐹𝑖𝑟𝑠𝑡𝑇𝑜𝑘𝑒𝑛 (𝑥𝑖
𝑜𝑢𝑡). (4)
Takingadvantageoftheauto-regressivetrainingschema,each
tokenin 𝑥𝑖
𝑜𝑢𝑡canbeconsideredasthepredictedvalueofthenextto-
kencorrespondingtothetokenin 𝑥𝑖
𝑖𝑛.Fortheregressionofthefirst
token, we adopt a zero-padding for simplicity. Therefore, Equation
4 effectively refines the input for the next stage, eliminating
the redundant information in the series.
 
2006Generative Pretrained Hierarchical Transformer for
Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain.
Intuitively,thepoolingoperationwithinthehierarchicaltrans-
formers allows the model to concentrate on specific patterns at
lower frequencies. Additionally, the task of deeper blocks is simpli-
fiedastheshallowerlayersfilteroutthewell-approximatedsignals.
Therefore,theiterativeresiduallearningstrategyenablesthemodel
tofocusonrefiningthefinerdetails,allowingfortheprogressive
enhancement of the predictive accuracy. Moreover, the strategy
naturallysuitsthediversepatternsinthemixedpretrainingdataset
withadaptabilitytovarioustemporalstructures,thusguaranteeing
promising generalizability.
3.5 Optimization Target
Let𝑆represent the number of stages, the intermediate forecasting
result of GPHT is the sum of all the outputs of each hierarchical
transformer block, defined as:
𝑦𝑝𝑟𝑒𝑑=𝑆/summationdisplay.1
𝑖=1𝑥𝑖
𝑜𝑢𝑡. (5)
Besides,aReversedInstanceNormalizationlayerisappliedto
the intermediate result through de-normalization, restoring the
characteristics of the input series for better accuracy [25]:
Y𝑝𝑟𝑒𝑑=𝑦𝑝𝑟𝑒𝑑·(𝜎+𝜖)+𝜇. (6)
To fully leverage the mixed dataset and better capture tempo-
raldependencies,weformulatethepretrainingtaskasastandard
languagemodelingtask,employingatoken-wiseauto-regressive
loss function as the optimization target (which is also a forecasting
task).UsingthesamenotationsasinSection3.1,andletting 𝐻=𝑇,
the optimization target of GPHT is:
𝐿=𝑀𝑆𝐸(Y𝑝𝑟𝑒𝑑,𝐶𝑜𝑛𝑐𝑎𝑡(X[:,𝑇:],Y)) (7)
Here,weusethestandardMeanSquaredError(MSE)asthenext-
token-predictionlossfunction,astimeseriestokensarestillcon-
tinuous numerical values.
3.6 Inference
Giventhatthepretrainingtaskcanbeconsideredaforecastingtask,
thepre-trainedGPHTcanbedirectlyappliedtodownstream
forecastingtaskswithoutanymodification.Thissetsourap-
proach apart from mainstream pretraining methods [ 7,13,45,54],
where a fine-tuning procedure is typically required.
On the other hand, we argue that the performance of GPHT can
also be further enhanced through fine-tuning. In practice, to strike
a balance between maintaining generalizability and improving per-
formance on a specific dataset, we adopt a parameter-efficient tun-
ingstrategy.Specifically,onlytheforecastingheadsdescribedin
Equation3areupdatedduringthefine-tuningprocess.Importantly,these forecasting head parameters account for less than 0.5% of the
entire model.
Duringinference,benefitingfromtheaforementionedtraining
schemaandthechannel-independentassumption,GPHTistheo-
retically capable of conducting universal forecasting on any input
multi-variate time series, regardless of arbitrary horizon lengths.
The forecasting process is akin to the decoding process of a lan-
guage model. Given any input X, our model can initially predict
thenextfirsttoken.Thispredictedtokenisthenconcatenatedto
theendoftheinputseriestogeneratepredictionsforthesecondtoken. Note that a max input length 𝐿𝑚exists in our model due
to the positional embeddings and heavy computation cost when
addressing long sequences. Consequently, only the most recent 𝐿𝑚
tokens are fed into the model for forecasting.
4 Experiments
Inthis section,we conductsufficientexperiments on8widely useddatasetsincomparisonwithmainstreamself-supervisedpretraining
methodsand supervisedmethodsto illustratetheeffectiveness of
our proposed GPHT.
4.1 Experimental Setup
4.1.1 Datasets. Table 1 provides detailed descriptions of the used
datasets, covering various data scenarios and scales. Among them,
bothETT2andElectricity3mainlyrecordstheconsumptionon
electricity,andETTcanbefurtherdividedinto4subsetsaccording
tothefrequency.Besides, Exchange4collectsthedailyexchange
rate among 8 countries, Traffic5contains the data of traffic load
sensors, and Weather6is made up of 21 climate indicators like
air temperature. Following the standard protocol, we split each
datasetintotraining,validationandtestingsetsaccordingtothe
chronological order. The split ratio is 6:2:2 for the ETT dataset and
7:1:2 for the other datasets [44].
Table 1: The Statistics of Each Dataset.
Dataset Variables Frequency Length Scope
ETTh1/ETTh2 7 1 Hour 17420 Energy
ETTm1/ETTm2 7 15 Minutes 69680 Energy
Electricity 321 1 Hour 26304 Energy
Exchange 8 1 Day 7588 Finance
Traffic 862 1 Hour 17544 Transportation
Weather 21 10 Minutes 52696 Weather
4.1.2 Compared Baselines. We select various state-of-the-art mod-
elsasthebaselinemodelsintheexperiments,encompassingboth
self-supervised and supervised approaches.
Among them, FPT [54]introduces a framework that utilizes
parameter-efficient fine-tuning on pretrained generative language
models, adapting them to time series tasks. SimMTM [ 13]re-
frames the standard masked time series modeling target into re-
covering masked time points through the weighted aggregation of
multipleneighbors. Additionally, TimeMAE[ 7]leveragesdecou-
pled masked autoencoders to learn robust representations through
masked codeword classification and masked representation regres-
sion. On the other hand, we include superior supervised mod-
els to better demonstrate the effectiveness of GPHT, including
Transformer-based models such as PatchTST [ 31]andiTrans-
former [ 27], a linear model-based approach DLinear [ 46], and
2https://github.com/zhouhaoyi/ETDataset
3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
4https://github.com/laiguokun/multivariate-time-series-data
5http://pems.dot.ca.gov
6https://www.bgc-jena.mpg.de/wetter/
 
2007KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhiding Liu et al.
Table2:MultivariatetimeseriesforecastingresultscomparingGPHTwithbothSOTAself-supervisedapproachesandsupervised
approaches. The best results are in bold and the second best are underlined .
Type Ours Self-supervised Supervised
Methods GPHT* GPHT PatchTST FPT SimMTM TimeMAE PatchTST iTransformer TimesNet DLinear
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEElectricity960.128 0.219 0.128 0.219 0.1320.225 0.139 0.238 0.133 0.223 0.133 0.230 0.138 0.233 0.132 0.228 0.177 0.281 0.141 0.238
1920.1470.236 0.146 0.236 0.148 0.241 0.155 0.252 0.147 0.2370.150 0.246 0.153 0.247 0.154 0.249 0.193 0.295 0.154 0.251
3360.165 0.255 0.165 0.255 0.167 0.260 0.170 0.267 0.166 0.265 0.166 0.2650.170 0.263 0.172 0.267 0.206 0.306 0.170 0.269
7200.2060.2920.2070.2920.2050.2920.208 0.299 0.203 0.2970.1990.2960.206 0.295 0.204 0.296 0.223 0.320 0.205 0.302Exchange960.096 0.216 0.087 0.207 0.0880.2070.098 0.222 0.100 0.226 0.229 0.352 0.094 0.216 0.099 0.225 0.166 0.305 0.0870.217
1920.183 0.304 0.172 0.2960.186 0.308 0.209 0.327 0.210 0.332 0.653 0.581 0.191 0.311 0.206 0.329 0.303 0.413 0.1640.298
3360.3220.4100.309 0.400 0.374 0.446 0.398 0.463 0.389 0.460 1.524 0.887 0.343 0.427 0.370 0.448 0.445 0.511 0.333 0.437
7200.8330.6850.808 0.669 0.857 0.692 1.010 0.747 1.104 0.800 2.525 1.193 0.888 0.706 0.963 0.746 1.389 0.899 0.988 0.749Traffic960.3480.2360.346 0.234 0.382 0.262 0.388 0.279 0.368 0.262 0.365 0.252 0.395 0.272 0.361 0.266 0.600 0.323 0.411 0.284
1920.374 0.248 0.371 0.246 0.385 0.261 0.411 0.287 0.373 0.251 0.383 0.260 0.411 0.278 0.378 0.271 0.612 0.327 0.423 0.289
3360.392 0.259 0.3880.2560.409 0.275 0.423 0.293 0.395 0.2540.399 0.269 0.424 0.284 0.390 0.274 0.628 0.344 0.437 0.297
7200.428 0.284 0.423 0.279 0.438 0.291 0.449 0.307 0.432 0.290 0.438 0.291 0.453 0.300 0.424 0.291 0.657 0.349 0.467 0.316Weather960.1550.1960.1540.1960.1480.1960.152 0.201 0.152 0.201 0.151 0.208 0.1470.1970.162 0.212 0.168 0.225 0.176 0.236
1920.2030.2400.2010.2400.1930.2400.197 0.244 0.198 0.245 0.198 0.256 0.191 0.240 0.205 0.251 0.218 0.268 0.217 0.275
3360.259 0.283 0.257 0.283 0.244 0.279 0.252 0.287 0.249 0.285 0.246 0.2940.2440.2820.257 0.291 0.269 0.301 0.264 0.315
7200.338 0.337 0.335 0.337 0.3210.3340.329 0.340 0.324 0.335 0.3160.3510.3200.3340.325 0.337 0.340 0.350 0.325 0.364ETTh1960.378 0.388 0.363 0.382 0.384 0.401 0.388 0.405 0.383 0.411 0.431 0.450 0.382 0.403 0.405 0.419 0.421 0.438 0.375 0.396
1920.425 0.416 0.405 0.408 0.427 0.431 0.422 0.423 0.417 0.432 0.484 0.486 0.4160.423 0.448 0.447 0.482 0.479 0.428 0.437
3360.456 0.432 0.4300.4230.461 0.450 0.442 0.435 0.4250.439 0.515 0.507 0.441 0.440 0.482 0.470 0.528 0.505 0.448 0.449
7200.454 0.449 0.414 0.435 0.460 0.465 0.469 0.473 0.437 0.456 0.595 0.577 0.470 0.475 0.560 0.537 0.527 0.510 0.505 0.514ETTh2960.307 0.347 0.296 0.3400.297 0.354 0.291 0.349 0.298 0.350 0.294 0.358 0.2860.3420.305 0.361 0.355 0.408 0.296 0.360
1920.373 0.389 0.363 0.3840.388 0.406 0.356 0.390 0.360 0.388 0.3520.3970.357 0.389 0.391 0.412 0.403 0.434 0.391 0.423
3360.399 0.414 0.392 0.410 0.392 0.413 0.387 0.418 0.388 0.410 0.394 0.427 0.377 0.409 0.418 0.433 0.398 0.434 0.445 0.460
7200.412 0.429 0.4070.4270.413 0.442 0.415 0.448 0.412 0.435 0.539 0.510 0.4060.440 0.437 0.455 0.443 0.465 0.700 0.592ETTm1960.301 0.345 0.291 0.3390.2810.3410.2900.346 0.296 0.349 0.301 0.348 0.298 0.345 0.306 0.360 0.331 0.372 0.303 0.346
1920.347 0.374 0.337 0.3680.3260.372 0.330 0.3710.334 0.373 0.351 0.383 0.339 0.374 0.345 0.382 0.435 0.421 0.338 0.368
3360.388 0.401 0.377 0.393 0.348 0.384 0.3660.3930.371 0.398 0.390 0.408 0.381 0.401 0.378 0.402 0.457· 0.445 0.373 0.393
7200.465 0.441 0.452 0.433 0.399 0.418 0.4160.4210.418 0.425 0.457 0.446 0.428 0.431 0.443 0.439 0.526 0.481 0.428 0.423ETTm2960.179 0.257 0.170 0.250 0.1710.2570.1710.261 0.173 0.264 0.180 0.267 0.174 0.261 0.174 0.266 0.190 0.276 0.1700.264
1920.242 0.298 0.230 0.291 0.236 0.304 0.231 0.3020.2300.299 0.243 0.312 0.238 0.307 0.247 0.315 0.244 0.311 0.233 0.311
3360.300 0.334 0.285 0.3270.291 0.344 0.288 0.343 0.2820.3320.308 0.355 0.293 0.346 0.292 0.343 0.302 0.349 0.298 0.358
7200.400 0.393 0.380 0.3860.388 0.404 0.389 0.406 0.374 0.3900.395 0.407 0.3730.401 0.375 0.395 0.406 0.406 0.423 0.437
#1 Counts 84 1 13 0 4 3 10 0 0 4
aconvolution-basedmodel TimesNet[ 43],whichisalsoamulti-
scalemodel.Theperformanceofthesemethodseffectivelyrepre-
sentstheutmostaccuracyachievablebycurrentforecastingmodels.
4.1.3 Implementation Details. We employ ADAM as the default
optimizerthroughoutalltheexperimentsandusemeansquareder-
ror(MSE)andmeanabsoluteerror(MAE)astheevaluationmetrics.
AlowerMSE/MAEindicatesbetterperformance.Forthebaseline
models, we implement them with official codes and recommended
parameter settings.
As for GPHT, we set the token length 𝑇to 48, and the max
inputlength 𝐿𝑚issetto7toaccommodatethelookbackwindow
length.Themodelcomprises4stagesofhierarchicaltransformer
blocks,eachwiththree-layerdecoder-onlytransformers.Thedown-
sampling ratio for each stage is set to [8,4,2,1]respectively. Allexperimentsareconductedforthreerunswithafixedrandomseed
on a single NVIDIA RTX 4090 24GB GPU.
4.2 Main Results
We report the long-term multi-variate forecasting results in Ta-
ble2.Forafaircomparison,thelookbackwindowlength 𝐿isset
to 336 for every model and dataset, and the horizon length 𝐻is
[96,192,336,720]following the standard protocol. Here, GPHT*
denotes our model pretrained on the mixed dataset without anymodification, while GPHT represents the fine-tuned version for
each dataset, as described in Section 3.6.
Asshowninthetable,wecandrawsomeinterestingconclusions.
Firstly,self-supervisedmethodsshowcasecompetitiveperformance
with their supervised counterparts, underscoring the efficacy of
 
2008Generative Pretrained Hierarchical Transformer for
Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain.
advanced pretraining techniques. Secondly, the tokenization tech-
nique emerges as a crucial factor in achieving precise forecast-
ing, as evidenced by the outstanding performance of both FPT
andpatchTSTamongthebaselinemodels.Finally,itappearsthat
modernforecastershaveapproachedtheprecisionlimitsofsome
benchmarkdatasets,likelyduetoinherentnoiseandunpredictabledistributionshiftsinthedata.Thisphenomenonresultsinminimal
differences in outcomes among various models.
On the other hand, in terms of our proposed GPHT, it consis-
tently surpasses its competitors across most experimental settings.
The tuned GPHT model establishes the most accurate forecasting
underover 65%caseswithitspowerfulcounterparts.Specifically,
GPHT exhibits an average MSE reduction of 9.23%on the Ex-
change dataset, 1.60%on the Traffic dataset, and 3.00%on the
ETTh1dataset,incomparisonwiththebestbaselineunderallex-
perimental forecasting lengths. Regarding MAE evaluation, the
improvements are more pronounced at 5.30%, 3.97%, and 5.07%
respectively. Although the relative improvements under certain
settingsmaynotbeassubstantial,asweclaimedbefore,thecon-
sistently superior performance provides strong evidence for the
effectivenessofourapproach.Thisfurtherdemonstratesthatour
model can better capture temporal dependencies in various data
scenarios, benefiting from its pretraining on the mixed dataset.
Besides,itisnoteworthythatourmodelexhibitssuperiorper-
formance at relatively shorter horizon lengths. Specifically, GPHT
surpasses PatchTST on every dataset when 𝐻=96, resulting in an
average MAE reduction of 4.55%. We attribute this performance
boost to the explicit modeling of temporal dependencies in the
output series.On the otherhand, dueto the inevitableerror accu-
mulationissuecausedbytheauto-regressiveforecastingschema,
the superiority of GPHT decreases with longer 𝐻s and the the
average reduction of MAE comes to 3.38%when 𝐻=720.
Even more surprisingly, GPHT*, representing the model after
pretraining without any fine-tuning or modification, proves
to be competitive with the baseline models. Specifically, GPHT*
achievesthebestorthesecond-bestperformancein 26outofall
64 settings. When compared to a single model, it outperforms FPT
andsupervisedPatchTSTunder 44and40experimentalsettings,
respectively.Thisresultvalidatesthefeasibilityoflearningthecom-
monalities of different time series by training on the mixed dataset
with the channel-independent assumption. It also underscores the
incredible generalizability of our proposed model, primarily owing
to the specially designed multi-stage hierarchical blocks and the
iterative residual learning strategy.
4.3 Zero-shot Evaluation
TofurtherhighlightGPHT’scapacitytolearngeneralknowledge
and discover common patterns from the mixed dataset, we con-
duct zero-shot forecasting experiments on the Exchange, Weather,
andTrafficdatasets, originatingfrom variousdatascenarios with
distinct scales. The zero-shot forecasting task is conceptually chal-
lengingformethodsthatmodelcross-variabledependencies,hence,
only the channel-independentmodels are considered as baseline
models. It is important to note that ForecastPFN [ 14] is specifically
designedforzero-shotforecasting,butitsperformanceisheavily
contingent on how the training data is synthesized, and as such, itisnotincludedinthisexperiment.Themodelsaretrainedonthe
mixeddatasetcomprisedoftheremaining7datasetsandevaluated
directly on the target dataset. The resultsare presented in Table 3.
Table 3: Comparison on zero-shot forecasting task. The best
results are highlighted in bold.
Methods GPHT FPT PatchTST DLinear
Metric MSE MAE MSE MAE MSE MAE MSE MAEExchange960.098 0.219 0.104 0.226 0.102 0.227 0.169 0.316
1920.183 0.305 0.218 0.333 0.205 0.325 0.230 0.374
3360.321 0.411 0.391 0.460 0.362 0.440 0.334 0.444
7200.824 0.682 0.978 0.734 0.991 0.745 0.560 0.591Traffic960.411 0.291 0.447 0.331 0.433 0.314 0.453 0.328
1920.435 0.302 0.461 0.335 0.447 0.319 0.464 0.330
3360.460 0.316 0.477 0.343 0.465 0.329 0.481 0.340
7200.521 0.353 0.5030.3560.504 0.354 0.5060.351Weather960.202 0.244 0.216 0.264 0.207 0.259 0.239 0.297
1920.248 0.283 0.260 0.301 0.257 0.299 0.275 0.325
3360.306 0.324 0.328 0.351 0.340 0.350 0.323 0.360
7200.389 0.377 0.414 0.403 0.414 0.402 0.392 0.405
Clearly,GPHTconsistentlyoutperformsothermodelsacrossvar-
ious settings,showcasing pronounced relativeimprovements. We
attributethissuccessprimarilytothemulti-stagehierarchicaltrans-
former blocks, designed to capture diverse temporal patterns with
different resolutions. Consequently, GPHT exhibits better trans-ferability to unseen time series. Additionally, all models achieve
forecastingaccuracyatanacceptableerrorlevel,withDLineareven
demonstratingunprecedentedprecisionontheExchangedataset
when 𝐻=720. These results strongly validate the feasibility of
our approach to learning commonalities in time series through
pretraining on a mixed dataset.
4.4 Few-shot Evaluation
In real-world applications, the initial observation of time seriesmay be of a limited size, posing challenges for training accurate
forecasters. To evaluatethe representation powerof GPHTundersuchcircumstances,weconductfew-shotevaluationsontheETT
andElectricitydataset.Indetail,onlyaportion(10%or5%,following
existingwork[ 54])ofthetraininginstancesareusedfortraining
the models, and we evaluate their MSE and MAE on the full test
set. The results are reported in Table 4.
In comparison to the selected baseline models, GPHT consis-
tently achieves superior performance across various experimental
settings, particularly on small-scale datasets. Specifically, when
comparedtothebest-performingbaselineinthe10%setting,GPHTdemonstratesarelativeaverageMSEreductionof 16.02%and7.02%
on the ETTh1 and ETTh2 datasets, respectively. With a further re-
duction in the training data to only 5%, the relative improvements
becomeevenmorepronounced,reaching 30.19%and12.49%.How-
ever, it is noteworthy that GPHT exhibitspoor forecasting perfor-
mance on the ETTm1 dataset. We believe this observation may
beattributedtothepotentialheterogeneityofETTm1compared
 
2009KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhiding Liu et al.
Table 4: Multivariate forecasting results under few-shot learning settings. The best results are highlighted in bold.
Portion 5% 10%
Methods GPHT FPT SimMTM PatchTST iTransformer GPHT FPT SimMTM PatchTST iTransformer
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEElectricity960.143 0.237 0.148 0.246 0.152 0.255 0.188 0.292 0.155 0.256 0.140 0.233 0.149 0.248 0.146 0.246 0.147 0.245 0.148 0.247
1920.162 0.254 0.163 0.259 0.167 0.268 0.202 0.304 0.172 0.272 0.159 0.250 0.164 0.261 0.163 0.262 0.162 0.258 0.167 0.266
3360.1840.275 0.181 0.277 0.187 0.287 0.219 0.318 0.197 0.295 0.180 0.271 0.183 0.280 0.184 0.280 0.181 0.276 0.192 0.290
7200.238 0.321 0.231 0.315 0.240 0.326 0.264 0.351 0.261 0.344 0.2310.3130.234 0.318 0.242 0.325 0.2300.315 0.244 0.329ETTh1960.383 0.390 0.478 0.474 0.537 0.502 0.505 0.481 0.580 0.520 0.382 0.391 0.453 0.454 0.482 0.467 0.450 0.448 0.557 0.514
1920.426 0.416 0.705 0.577 0.580 0.525 0.576 0.514 0.670 0.557 0.424 0.418 0.522 0.494 0.532 0.498 0.523 0.489 0.668 0.562
3360.453 0.430 0.736 0.571 0.603 0.543 0.672 0.554 0.726 0.577 0.450 0.443 0.571 0.522 0.561 0.523 0.523 0.494 0.684 0.559
7200.433 0.440 0.718 0.579 0.708 0.597 0.759 0.625 0.802 0.626 0.427 0.442 0.574 0.535 0.734 0.617 0.508 0.502 0.709 0.587ETTh2960.298 0.343 0.476 0.457 0.381 0.401 0.502 0.475 0.395 0.420 0.298 0.343 0.330 0.371 0.332 0.373 0.320 0.366 0.365 0.398
1920.368 0.386 0.714 0.573 0.435 0.435 0.569 0.511 0.448 0.453 0.367 0.387 0.419 0.422 0.391 0.411 0.400 0.416 0.432 0.439
3360.402 0.412 0.683 0.573 0.431 0.441 0.540 0.506 0.453 0.462 0.396 0.413 0.419 0.434 0.410 0.429 0.405 0.425 0.437 0.450
7200.417 0.429 0.648 0.557 0.450 0.459 0.506 0.494 0.483 0.484 0.409 0.428 0.506 0.485 0.448 0.460 0.483 0.474 0.463 0.471ETTm1960.513 0.438 0.395 0.409 0.446 0.434 0.376 0.395 0.434 0.436 0.506 0.427 0.403 0.411 0.442 0.430 0.386 0.401 0.420 0.427
1920.552 0.464 0.410 0.417 0.461 0.436 0.391 0.402 0.472 0.456 0.563 0.458 0.430 0.426 0.454 0.431 0.406 0.413 0.472 0.456
3360.609 0.491 0.453 0.440 0.500 0.455 0.438 0.431 0.531 0.486 0.634 0.492 0.472 0.443 0.552 0.469 0.438 0.429 0.530 0.486
7200.685 0.581 0.742 0.566 0.590 0.503 0.549 0.495 0.615 0.527 0.721 0.534 0.665 0.526 0.715 0.539 0.499 0.464 0.629 0.533ETTm2960.186 0.271 0.196 0.278 0.216 0.293 0.196 0.276 0.211 0.295 0.173 0.256 0.198 0.275 0.203 0.283 0.191 0.270 0.198 0.284
1920.248 0.311 0.263 0.316 0.267 0.324 0.258 0.315 0.269 0.322 0.234 0.297 0.263 0.315 0.256 0.315 0.252 0.308 0.254 0.318
3360.307 0.347 0.336 0.363 0.315 0.356 0.318 0.353 0.325 0.370 0.293 0.335 0.320 0.350 0.305 0.345 0.310 0.345 0.305 0.352
7200.412 0.409 0.453 0.430 0.406 0.406 0.447 0.427 0.441 0.434 0.3980.3950.426 0.412 0.3970.398 0.398 0.397 0.405 0.408
to other datasets, where the pretraining procedure might compro-
misegeneralizabilitywheninsufficientdataisavailable.Addressing
how to identify and leverage dataset heterogeneities for enhanced
pretraining remains a subject for future exploration.
4.5 Ablation Study
4.5.1 Hierarchical Architecture. In this section, we explore the in-
fluenceofhierarchicaltransformer blocksonGPHT’sperformance.
We present the averaged MSE and MAE evaluations for GPHT
with varying stages of hierarchical transformer blocks across all
benchmark datasets, considering a forecasting horizon of 𝐻=720
(see Figure 2). As the number of stages increases, GPHT is theoret-
ically better equipped to capture diverse temporal dependencies
withinthemixeddataset,suchasdifferentperiodicities.Theresults
strongly affirm our hypothesis, as the 4-stage GPHT surpasses the
1-stageGPHT(withouthierarchicalstructures),achievinganotable
2.86%reduction in MSE.
4.5.2 OntheEffectofPretraining. Inadditiontothearchitecture
design, another key aspect is the pretraining procedure on the
mixed dataset.Does it pose positiveeffects on theforecasting per-
formance?WeprovidequantifiedresultsinFigure3.Specifically,we
comparetheperformanceoffine-tunedGPHTwiththeonetrainedfromscratch.TheMAEevaluationsaveragedonthehorizonlength
(i.e.,𝐻=96,192,336,720)arepresented.Fromtheresults,wecan
infer that pretraining on the mixed dataset enables the model to
leverage commonalities among time series, facilitating better trans-
fertospecificdatasets.ComparedtoGPHTtrainedfromscratch,
pretrainingresultsinanaverageMAEreductionof 5.75%,reaching
as high as 9.65%on the ETTm2 dataset.0.4080.4100.4120.4140.4160.4180.4200.4220.4240.4260.428
0.4350.4400.4450.4500.4550.460
12345
MAEMSE
Number of the stages of hierarchical transformer blocksMSE MAE
Figure 2:Performancecomparison betweenGHPT withdif-
ferent stages of hierarchical transformer blocks.
0.0000.0500.1000.1500.2000.2500.3000.3500.4000.4500.500MAEw/ pretrain w/o pretrain
Figure3:MAEevaluationbetweenGPHTandGPHTwithout
pretraining on benchmark datasets.
 
2010Generative Pretrained Hierarchical Transformer for
Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain.
5 Conclusion
In this work, we proposed a generative pretrained hierarchical
transformer model, namely GPHT, for time series forecasting. It
standsoutintwokeyaspects.Conceptually,weexploredtrainingasingleunifiedforecastingmodelthatgeneralizeswellacrossdiverse
datascenariosandforecastingsettings.Technically,weproposed
asimpleyeteffectiveparadigmthattreatstimeseriesoriginating
fromvariousscopesasawhole,discardingtheheterogeneityand
concatenatingthevaluesofeachvariablefromdifferentdatasetsto form the mixed dataset for pretraining. Besides, we replaced
conventionalone-stepgenerating,whichisadoptedbymostrecentforecasting methods, withauto-regressive decoding for better flexi-
bility andperformance. Wealso introducedthe hierarchicalstruc-
ture better to capture the diverse patterns in the mixed dataset. Weconductedsufficientexperimentson8widelyuseddatasetsincom-
parisonwithmainstreamself-supervisedpretrainingmodelsand
supervised models, the results demonstrated that GPHT surpasses
thebaselinemodelsacrossvariousfine-tuningandzero/few-shot
learning settings in the traditional long-term forecasting task.
Acknowledgments
This research was supported by grants from the Joint Research
ProjectoftheScienceandTechnologyInnovationCommunityin
Yangtze River Delta (No. 2023CSJZN0200), and the Fundamental
ResearchFundsfortheCentralUniversities.Thisworkalsothanked
to the support of funding MAI2022C007.
References
[1]KaifengBi,LingxiXie,HenghengZhang,XinChen,XiaotaoGu,andQiTian.2023.
Accurate medium-range global weather forecasting with 3D neural networks.
Nature(2023), 1–6.
[2]GeorgeEPBoxandGwilymMJenkins.1968. Somerecentadvancesinforecasting
andcontrol. Journalofthe RoyalStatisticalSociety.SeriesC (AppliedStatistics) 17,
2 (1968), 91–109.
[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,etal .2020. Languagemodelsarefew-shotlearners. Advancesinneural
information processing systems 33 (2020), 1877–1901.
[4]Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max
Mergenthaler-Canseco, and Artur Dubrawski. 2022. N-HiTS: Neural Hierar-
chical Interpolation for Time Series Forecasting. arXiv:2201.12886 [cs.LG]
[5]WeiqiChen,WenweiWang,BingqingPeng,QingsongWen,TianZhou,andLiangSun.2022. Learningtorotate:Quaterniontransformerforcomplicatedperiodical
time series forecasting. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 146–156.
[6]MingyueCheng,QiLiu,ZhidingLiu,ZhiLi,YucongLuo,andEnhongChen.2023.
FormerTime: Hierarchical Multi-Scale Representations for Multivariate Time
SeriesClassification.In ProceedingsoftheACMWebConference2023.1437–1445.
[7]Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, and Enhong
Chen. 2023. TimeMAE: Self-Supervised Representations of Time Series with
Decoupled Masked Autoencoders. arXiv preprint arXiv:2303.00320 (2023).
[8]Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, and Chenyi
Lei. 2024. Learning Transferable Time Series Classifier with Cross-Domain
Pre-training from Language Model. arXiv preprint arXiv:2403.12372 (2024).
[9]MingyueCheng,JiqianYang,TingyuePan,QiLiu,andZhiLi.2024. Convtimenet:
Adeephierarchicalfullyconvolutionalmodelformultivariatetimeseriesanalysis.
arXiv preprint arXiv:2403.01493 (2024).
[10]AbhimanyuDas,WeihaoKong,RajatSen,andYichenZhou.2023. Adecoder-only
foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688
(2023).
[11]Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2021. St-
norm:Spatialandtemporalnormalizationformulti-variatetimeseriesforecasting.
InProceedingsofthe27thACMSIGKDDconferenceonknowledgediscovery&data
mining. 269–278.
[12]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXiv
preprint arXiv:1810.04805 (2018).[13]JiaxiangDong,HaixuWu,HaoranZhang,LiZhang,JianminWang,andMing-
sheng Long. 2023. SimMTM: A Simple Pre-Training Framework for Masked
Time-Series Modeling. arXiv preprint arXiv:2302.00861 (2023).
[14]SamuelDooley,GurnoorSinghKhurana,ChiragMohapatra,SiddarthaVenkat
Naidu, and Colin White. 2023. ForecastPFN: Synthetically-Trained Zero-Shot
Forecasting.In Thirty-seventhConferenceonNeuralInformationProcessingSys-
tems.
[15]Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong
Kwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning via
temporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021).
[16]AzulGarzaandMaxMergenthaler-Canseco.2023. TimeGPT-1. arXivpreprint
arXiv:2310.03589 (2023).
[17]Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman,
and Pablo Montero-Manso. 2021. Monash time series forecasting archive. arXiv
preprint arXiv:2105.06643 (2021).
[18]Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, PierreRichemond,ElenaBuchatskaya,CarlDoersch,BernardoAvilaPires,Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural information processing
systems33 (2020), 21271–21284.
[19]NateGruver,MarcAntonFinzi,ShikaiQiu,andAndrewGordonWilson.2023.
LargeLanguageModelsAreZero-ShotTimeSeriesForecasters.In Thirty-seventh
Conference on Neural Information Processing Systems.
[20]KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.
2022. Masked autoencoders are scalable vision learners. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 16000–16009.
[21]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[22]Wenqiang He, Mingyue Cheng, Qi Liu, and Zhi Li. 2023. ShapeWordNet: An
Interpretable Shapelet Neural Network for Physiological Signal Classification. In
InternationalConferenceonDatabaseSystemsforAdvancedApplications.Springer,
353–369.
[23]Junji Jiang, Likang Wu, Hongke Zhao, Hengshu Zhu, and Wei Zhang. 2023.
Forecastingmovementsofstocktimeseriesbasedonhiddenstateguideddeep
learning approach. Information Processing & Management 60, 3 (2023), 103328.
[24]Guangyin Jin, Yuxuan Liang, Yuchen Fang, Zezhi Shao, Jincai Huang, Junbo
Zhang,andYuZheng.2023.Spatio-temporalgraphneuralnetworksforpredictive
learning in urban computing: A survey. IEEE Transactions on Knowledge and
Data Engineering (2023).
[25]Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and
Jaegul Choo. 2021. Reversible instance normalization for accurate time-series
forecasting against distribution shift. In International Conference on Learning
Representations.
[26]Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, andQiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with SampleConvolution and Interaction. Thirty-sixth Conference on Neural Information
Processing Systems (NeurIPS), 2022 (2022).
[27]YongLiu,TenggeHu,HaoranZhang,HaixuWu,ShiyuWang,LintaoMa,and
MingshengLong.2023. itransformer:Invertedtransformersareeffectivefortime
series forecasting. arXiv preprint arXiv:2310.06625 (2023).
[28]Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, andEnhong Chen. 2023. Adaptive Normalization for Non-stationary Time Series
Forecasting:ATemporalSlicePerspective.In Thirty-seventhConferenceonNeural
Information Processing Systems.
[29]Yiwei Lou, Yu Huang, Xuliang Xing, Yongzhi Cao, and Hanpin Wang. 2022.Mts-lstdm: multi-time-scale long short-term double memory for power load
forecasting. Journal of systems architecture 125 (2022), 102443.
[30]Feng Lu, Wei Li, Zhiqiang Zhou, Cheng Song, Yifei Sun, Yuwei Zhang, YufeiRen, Xiaofei Liao, Hai Jin, Ailin Luo, et al
.2023. A composite multi-attention
frameworkforintraoperativehypotensionearlywarning.In Proceedingsofthe
AAAI Conference on Artificial Intelligence, Vol. 37. 14374–14381.
[31]YuqiNie,NamHNguyen,PhanwadeeSinthong,andJayantKalagnanam.2022.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In
The Eleventh International Conference on Learning Representations.
[32]BorisNOreshkin,DmitriCarpov,NicolasChapados,andYoshuaBengio.2019. N-
BEATS: Neural basis expansion analysis for interpretable time series forecasting.
InInternational Conference on Learning Representations.
[33]Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and
Alexandros Iosifidis. 2019. Deep adaptive input normalization for time series
forecasting. IEEEtransactionsonneuralnetworksandlearningsystems 31,9(2019),
3760–3765.
[34]Gábor Petneházi. 2019. Recurrent neural networks for time series forecasting.
arXiv preprint arXiv:1901.00069 (2019).
[35]Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied
Babai, Devon K Barrow, Souhaib Ben Taieb, Christoph Bergmeir, Ricardo J Bessa,
Jakub Bijak, John E Boylan, et al .2022. Forecasting: theory and practice. Interna-
tional Journal of Forecasting (2022).
 
2011KDD ’24, August 25–29, 2024, Barcelona, Spain. Zhiding Liu et al.
[36]David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.
DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-
national Journal of Forecasting 36, 3 (2020), 1181–1191.
[37]MohammadAminShabani,AmirHAbdi,LiliMeng,andTristanSylvain.2022.
Scaleformer:IterativeMulti-scaleRefiningTransformersforTimeSeriesFore-
casting. In The Eleventh International Conference on Learning Representations.
[38]ZezhiShao,ZhaoZhang,FeiWang,andYongjunXu.2022. Pre-trainingenhanced
spatial-temporalgraphneuralnetworkformultivariatetimeseriesforecasting.
InProceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and
Data Mining. 1567–1577.
[39]Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. 2020. Unsupervised
RepresentationLearningforTimeSerieswithTemporalNeighborhoodCoding.
InInternational Conference on Learning Representations.
[40]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. Advances in neural information processing systems 30 (2017).
[41]QingsongWen,TianZhou,ChaoliZhang,WeiqiChen,ZiqingMa,JunchiYan,
and Liang Sun. 2022. Transformers in time series: A survey. arXiv preprint
arXiv:2202.07125 (2022).
[42]Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2021.
CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for
TimeSeriesForecasting.In InternationalConferenceonLearningRepresentations.
[43]Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. In The Eleventh International Conference on Learning Representations.
[44]HaixuWu,JiehuiXu,JianminWang,andMingshengLong.2021. Autoformer:De-
composition transformers with auto-correlation for long-term series forecasting.
Advances in Neural Information Processing Systems 34 (2021), 22419–22430.
[45]Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,
YunhaiTong,andBixiongXu.2022. Ts2vec:Towardsuniversalrepresentationof
timeseries.In ProceedingsoftheAAAIConferenceonArtificialIntelligence ,Vol.36.
8980–8987.
[46]Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers
Effective for Time Series Forecasting? Proceedings of the AAAI Conference on
Artificial Intelligence.
[47]George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,
andCarstenEickhoff.2021. Atransformer-basedframeworkformultivariatetime
seriesrepresentationlearning.In Proceedingsofthe27thACMSIGKDDconference
on knowledge discovery & data mining. 2114–2124.
[48]G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neural
network model. Neurocomputing 50 (2003), 159–175.
[49]KexinZhang,QingsongWen,ChaoliZhang,RongyaoCai,MingJin,YongLiu,
James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al .2023. Self-
SupervisedLearningforTimeSeriesAnalysis:Taxonomy,Progress,andProspects.
arXiv preprint arXiv:2306.10125 (2023).
[50]YuchenZhang,MingshengLong,KaiyuanChen,LanxiangXing,RonghuaJin,
Michael I Jordan, and Jianmin Wang. 2023. Skilful nowcasting of extreme precip-
itation with NowcastNet. Nature619, 7970 (2023), 526–532.
[51]YunhaoZhangandJunchiYan.2022. Crossformer:Transformerutilizingcross-
dimensiondependencyformultivariatetimeseriesforecasting.In TheEleventh
International Conference on Learning Representations.
[52]Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quencetime-seriesforecasting.In ProceedingsoftheAAAIConferenceonArtificial
Intelligence, Vol. 35. 11106–11115.[53]TianZhou,ZiqingMa,QingsongWen,XueWang,LiangSun,andRongJin.2022.
FEDformer:Frequencyenhanceddecomposedtransformerforlong-termseries
forecasting. In Proc. 39th International Conference on Machine Learning (ICML
2022)(Baltimore, Maryland).
[54]Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al .2023. One fits all: Power
general time series analysis by pretrained lm. Advances in neural information
processing systems 36 (2023), 43322–43355.
A Complexity Comparison
To betterillustrate the proposedGPHT model’s computationcost,
we provide quantitative results on the Electricity dataset under
lookback window 𝐿=336 and forecasting horizon 𝐻=720 in
Table5.Insummary,theproposedGPHTmodelismedium-sized
compared to the baseline models. Thanks to its straightforward
optimization objective, both the pretraining and finetuning pro-cesses of GPHT are quite efficient and do not require too much
time,especiallycomparedwiththepretraining-basedapproaches.
A key drawback of our model might be the inference speed, which
is naturally limited by the auto-regressive decoding schema.
B Qualitative Evaluation
Inthissection, weprovidevisualizationsoflong-termforecasting
results to better demonstrate the performance of GPHT and the
effectiveness of the hierarchical transformer architecture.
We plot aforecasting samplein Figure4, illustrating how GPHT
forecastswiththehierarchicalarchitectureandtheiterativeresidual
schema. It can be referred that the initial stages predominantlyfocus on extracting the general periodic patterns from the input
series and the latter stages can therefore pay more attention to the
specializedtrends,since theauto-regressiveforecasting resultsof
stage 3 align more closely with the input series, albeit with less
periodicitythantheprecedingstages.Theresultsstronglyverify
ourassumptionthatthehierarchicalarchitecturecanbettercapture
the commonalities and specialties of the mixed pertaining dataset,
andtheiterativeresidualschemaeffectivelyrefinestheinputfor
the next stage, eliminating the redundant information in the series.
Besides, we provide qualitative comparison between GPHT and
mainstream supervised forecasting methods in Figure 5 on various
datasets. Benefiting from the auto-regressive pertaining and thehierarchical architecture, GPHT can better capture the temporal
dependencies so as to achieve better performance.
 
2012Generative Pretrained Hierarchical Transformer for
Time Series Forecasting KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 5: Computation cost between GPHT and mainstream forecasting approaches.
Methods Params TrainingTime(perepoch) InferenceSpeed(itr/s)
GPHT 37.98M(pretraining)/98.50K(finetuning) 20min(pretraining)/254.1s(finetuning) 0.34
FPT 105.20M(24.00M trainable) 3858.8s(finetuning) 0.69
SimMTM 62.14M(pretraining)/7.76M(finetuning) 73min(pretraining)/946.5s(finetuning) 5.98
PatchTST 4.27M 128.9s 9.02
iTransformer 5.28M 24.7s 26.39
TimesNet 150.64M 1179.6s 1.51
Figure 4: Visualization of the input andcorresponding output series of GPHT’s multiple stages on asample from the ETTh1
dataset.
(a) GPHT (b) PatchTST (c) iTransformer (d) DLinear
ETTh1
Exchange
 Traffic
Figure 5: Illustration of forecasting showcases comparing GPHT and baseline models. The lookback window is set to 336 and
the forecasting horizon is set to 336, 192, 96 for the Exchange, Traffic, and ETTh1 dataset respectively.
 
2013