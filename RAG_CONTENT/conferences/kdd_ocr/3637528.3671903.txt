BivariateDecisionTrees: Smaller,Interpretable,More Accur ate
RasulKairgeldin
rkairgeldin@ucmerced.edu
University of California, Merced
Merced, CA,USAMiguelÁ. Carreira-Perpiñán
mcarreira-perpinan@ucmerced.edu
University ofCalifornia, Merced
Merced, CA,USA
Abstract
Univariate decisiontrees, commonlyusedsince the1950s,predict
by asking questions about a single feature in each decision node.
Whiletheyareinterpretable,theyoftenlackcompetitivepredictive
accuracy due to their inability to model feature correlations. Mul-
tivariate(oblique)treesusemultiplefeaturesineachnode,captur-
ing high-dimensional correlations better, but sometimes they can
bediﬃculttointerpret.Weadvocateforamodelthatstrikesause-
fulmiddleground:bivariatedecisiontrees,whichusetwofeatures
ineach node.This typicallyproducestreesthatnotonlyaremore
accuratethanunivariatetrees,butmuchsmaller,whichoﬀsetsthe
small increase in node complexity and keeps them interpretable.
They also help data mining by constructing new features that are
useful for discrimination, and by providing a form of supervised,
hierarchical 2Dvisualizationthatreveals patternssuchasclusters
or linear structure. We give two new algorithms to learn bivari-
ate trees: a fast one based on CART; and a slower one based on
alternatingoptimizationwithafeatureregularizationterm,which
producesthebesttrees whilestill scalingtolargedatasets.
CCS Concepts
•Computingmethodologies →Classiﬁcationandregression
trees.
Keywords
decision trees;interpretability; pairwise interactions
ACMReference Format:
RasulKairgeldinandMiguelÁ.Carreira-Perpiñán.2024.BivariateDecision
Trees:Smaller,Interpretable,MoreAccurate.In Proceedingsofthe30thACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671903
1 Introduction
Inmanyways,decisiontreesstandaloneinthestatisticalmachine
learning literature. They use conditional computation by design:
an input instance follows a single root-leaf path, without using
therestofthetreeparameters.Hence, inference timeisextremely
fast(logarithmiconthenumberofpathsifthetreewascomplete).
Theyhandlethemulticlasscasedirectly,withouttheneedforone-
vs-allorone-vs-oneapproaches,aseachleafcanbelabeledwitha
This work is licensed under a Creative Commons Attribu-
tio
n International 4.0 License.
KDD’24,August25–29,2024,Barcelona, Spain
© 2024 Copyrightheld by the owner/author(s).
ACM ISBN979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671903particularclass.And,althoughoutsideourscopeinthis paper,en-
sembles of trees are among the most powerful classiﬁers, at least
for tabular data, while being far simpler to train and tune com-
paredtoneural networks.Finally, decisiontreesareperhapsmost
valued because of their interpretability, together with a handful
ofmodels(suchaslogisticregression, generalizedadditivemodels
and scorecards). The way they achieve a prediction follows a se-
quenceofsimplequestionsabouttheinputfeatureswhichisquite
closetohumanreasoning.Adecisiontreecanbeexplainedandau-
dited at a global level by simple inspection. For a particular input
instance, one canexplain thetree predictionbyfollowingthe cor-
responding root-leaf path, or solve a counterfactual explanation
to ﬁnd the minimal change to the input features that results in a
desiredprediction[7,17].Indeed,someapplicationsofautomated
models are required to provide an explanation of their decision.
A well-known example in the US are adverse actions by ﬁnancial
institutions (such as denial of a loan application), where “each ap-
plicant against whom adverse action is taken shall be entitled to
a statement of reasons for such action from the creditor” (Equal
CreditOpportunityAct,1974).Incaseslikethese,explainablemod-
els suchas decisiontrees are essential.
Theone seriousdisadvantage of decisiontrees is thatthey usu-
ally result in low predictive accuracy. There are two reasons for
that. The ﬁrst one is a poor data model . A typical decision tree is
oftheaxis-alignedorunivariatetype,whichmeansthateachdeci-
sionnodeorsplitoperatesonasingleinputfeature(e.g.“if /u1D4657>10
go right, otherwise go left”). When features are correlated, which
is the usual situation in practice, this requires many zigzagging
splitsinordertoconstructanobliquedecisionboundary(i.e.,alin-
ear combination of features; see ﬁg. 1). Thus, the resulting tree is
big,which makes itharder tointerpret,andwillgeneralizepoorly
anyway. Anothermodelshortcomingis thathaving each nodeex-
amineonlyonefeaturelimitsthetotalnumberoffeaturesthatcan
be examined along a path or on the whole tree. And the number
ofnodesinthetreeisitselfmuchsmallerthanthesamplesize,be-
cause a leaf cannot be empty of training instances. This can force
thetreetoignore importantfeatures.
Univ.:Δ=8,19leaves, 4%error Biv.: Δ=3,5 leaves, 2%error
/u1D4651/u1D4652
/u1D4651/u1D4652
Figure 1: Partitioning by a univariate (left) and bivariate
tree(right).Byallowingsomefeaturecorrelations,bivariate
treesachievebetterperformancewith muchsmaller trees.
 
1336
KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
The second reason is a poor optimization . The hard decisions
thatenableconditionalcomputationinthetreealsomakeitapiece-
wise constant function whose gradient with respect to its param-
eters is either zero or undeﬁned. Optimizing a loss function over
the tree is thus a very nonconvex problem. The most widespread
way to train trees is the greedy recursive partitioning approach.
Starting at the root, a node’s split (feature and threshold) is ﬁxed
based on a local “purity” criterion, and the process is repeated re-
cursively until a stopping criterion holds, possibly followed by a
pruning procedure intended to reduce overﬁtting. CART [5] and
C4.5 [31] are the foremost representative algorithms, diﬀering on
minordetails(suchasthechoiceofpuritycriterion).Thisapproach
does not optimize a global objective function over the tree (such
as the 0/1 loss, cross-entropy or hinge loss), and indeed it results
inquitesuboptimaltrees[19].Itdoestwothingswell,though:itis
abletosearch fortreestructurestosomeextent (itself a very diﬃ-
cultproblem),anditisabletosolvethelocalsplitproblemexactly
byenumerating all possiblefeatures and thresholds (thelatterare
the midpoints between scalar feature values). Indeed, CART and
C4.5 remain in widespread practicaluse.
However, CART-likeprocedureswere never successfulforobli-
que(multivariate) trees,becausetheobliquesplitcannot beeasily
optimizedby enumeration (indeed, it is NP-hard). Whilesome ap-
proximationshavebeenproposed(suchasalocalsearch[5,25]),a
suboptimal split has a cascade eﬀect downstream that results in a
big tree [19]. Since now each split uses all features, the tree is un-
wieldyandfarfrominterpretable,whichmakesitnotworthusing
even if some(oftensmall) improvement occursintheaccuracy.
A recent algorithm, Tree Alternating Optimization (TAO) [8],
has made it possible to train univariate and (sparse) oblique trees
eﬀectively. Unlike recursive partitioning, TAO operates on a well-
deﬁnedparametrictreemodelandobjectivefunction.Ateachiter-
ation,itupdateseachnode’sparameterssotheobjectivedecreases.
It has been shown to produce univariate trees of higher accuracy
thanCART,C4.5and otheralgorithms[38],and obliquetreesthat
signiﬁcantly exceed univariate trees in accuracy (as is to be ex-
pected, since they can directly model many-feature correlations)
[8].However,aunivariatetree(eventrainedbyTAO)isstillapoor
data modelin many cases, while a (sparse) obliquetreecan some-
times behard tointerpret if it uses many features in each node.
Ourgoalhereis todesigndecisiontreesthataremoreaccurate
thanunivariatetreeswhileremaininghighlyinterpretableandeﬃ-
cienttotrain.Wepropose bivariatetrees ,whereeachdecisionnode
has zero, one or two features at most. Allowing for two features
ratherthanonesigniﬁcantlyincreasesthepredictiveaccuracyand
decreases the tree size even more signiﬁcantly, improving inter-
pretability,asshown inourexperiments. Thisis madepossibleby
our new algorithms to learn bivariate trees, of which we propose
two: a very fast one, bivariate CART , based on greedy recursive
partitioning,whichcanbeconvenientlyimplementedviaaprepro-
cessing step for univariate CART; and a slower but better one, bi-
variateTAO ,basedonalternatingoptimization.Inthelatter,wein-
troduceanew regularizationtermthatpenalizesdiﬀerentially the
useofzero features (which makes thenoderedundantand results
inautomaticallypruningthetree),onefeature(univariatesplit)or
twofeatures(bivariatesplit);andwegiveagood,approximatebut
eﬃcient solutiontothebivariatesplitstepthatarises withinTAO.We believe bivariate trees strike a good tradeoﬀthat can make
them very practical. We explain their relation with pairwise-inte-
ractionmodelsandotherworkinsections2and3,andourtraining
algorithmsinsection4.Ourexperimentsinsection5compareuni-
variate,bivariateandobliquetreesintermsofaccuracy,modelsize
and interpretability.
2 Other pairwise-interaction models
Interpretable models based on pairwise interactions have a long
historyinmachinelearning,dataminingandstatistics.Infact,the
ideaofbivariatetreesisnotnew(seerelatedwork),butithasnever
succeeded in practice, due to the lack of an eﬀective optimization
algorithm. We provide such algorithms in this paper and demon-
stratehow bivariatetrees areindeed useful models.
Two widely used pairwise-interaction models are Generalized
AdditiveModelswithpairwiseinteractions(GA2M)andFactoriza-
tionMachines(FM).AGA2M[18,sec.9.5],[23]consistsofasumof
univariateandbivariatecomponentfunctions.AFM[32]is alow-
rankbilinear model(thesumoflinear andquadraticterms,where
the latter’s matrix is low-rank). These models are interpretable in
that one can inspect the individual terms (e.g. with a 2D heatmap
for each GA2M pairwise term). The FM is especially useful with
one-hotencodedcategoricaldata,whereeachpairwiseweightcor-
respondstotheco-ocurrenceoftwo categories.
In both cases, the total number of pairwise interactions with
/u1D437features isO(/u1D4372), so it is critical to restrict the actual num-
ber of pairwise interactions. This is for interpretability but also
to keep the number of parameters small, so the model can learn
fromlimitedsamplesizes,oftenaprobleminpractice.InFMsthis
isachievedbyusingamatrixofrank /u1D43E</u1D437forthequadraticterm;
inGA2M,bygreedilyselectingasmallsubsetofinteractions.Inbi-
variatetrees,wecontrolthisbygloballyoptimizingthelossplusa
regularizationtermthatpenalizesthenumberofnodes,whosehy-
perparametercanbecross-validatedorselectedbyhandtoachieve
a desired tree size. Note that, unlike FMs and GA2Ms, bivariate
trees areabletomodelmorecomplexinteractions duetothehier-
archicalstructureofthetree(atthecostofmakingthetreedeeper).
Howinterpretablearebivariatetrees? Ontheonehand,eachsplit
involves now two features, so it is more complex. On the other
hand, the ability to learn oblique splits (even with just two fea-
tures) greatly reduces the number of nodes and depth of the tree
compared to a univariate one, and hence the number of rules ex-
tracted from the tree. This can often make a bivariate tree more
interpretable than a univariate one—a large univariate tree is not
only complex, but examining it requires constant pan and zoom.
A bivariate tree is also more accurate, particularlywhen the label
dependsonfeaturecorrelations.Itcanalsohappenthatthebivari-
atesplitcanbeunderstoodasanew,meaningfulfeatureonitsown
right.Finally,bivariatetreesbringanotheradvantageoverunivari-
ate trees: we can show a 2D scatterplot at each decision node on
its two selected features, which behaves like a hierarchical 2D lin-
ear discriminant analysis . This shows information between-class
(how speciﬁc classes are separated from each other), as well as
within-class (such as clustering or linear structure). We convinc-
ingly demonstrate this our experiments by comparing bivariate
and univariate trees in terms of accuracy and tree size, and by ex-
ploringin detailtwo casestudies.
 
1337BivariateDecision Trees: Smaller,Interpretable, MoreAccur ate KDD ’24,August25–29, 2024,Barcelona,Spain
3 Relatedwork
Learning trees has long been dominated by greedy recursive par-
titioning, of which countless versions exist [5, 22, 31, 33]. This
has been more successful with univariate trees than oblique ones
[5, 26], which are rarely used. Indeed, popular types of decision
forests use univariate trees, such as random forests [4] or gradi-
ent boostingforests [10, 20].Many implementations ofunivariate
trees exist, from scikit-learn to commercial packages such as
SAS,SPSS,MatlaborevenExcel.Mostunivariatetreeshaveacon-
stantpredictionattheleaves(classlabelorregressionoutput),but
there also exist more complex leaf models, such as linear [31], al-
though,again, theseare rarelyused.
Wehavefoundonlythreepapersexploringtheideaofbivariate
trees, all of them in the context of greedy recursive partitioning,
diﬀeringinhowtheyadapttheunivariatesplitsearchtoabivariate
one.Twoearlyapproaches[2,24]usedavariationofCART’slocal
search for obliquesplits. A recent one [3] used an exhaustive and
expensive search for the bivariate split using branch-and-bound.
Because of the underlying greedy recursive partitioning, this still
results inlarge, suboptimaltrees.
A parallel line of work on tree learning has focused on non-
greedyapproaches usingexact,brute-forcesearch.This hastaken
diﬀerent forms: mixed-integer optimization [1, 34], dynamic pro-
gramming in various forms [11] and other forms of combinato-
rial optimization [27, 28]. While cleverly designed, all these ap-
proacheshavethefundamentaldisadvantagethattheirworst-case
complexityisexponentialontheproblemsize,sothattheybecome
infeasible for a nontrivial tree depth and number of nodes, or a
nontrivialsamplesizeordimensionality.Noneoftheseapproaches
have considered bivariatesplits.
A ﬁnal line of work is the use of alternating optimization over
the tree nodes, the TAO algorithm [8]. This has made it possible
to optimize a given objective function over all the parameters of
aﬁxed-structuretree,fortasks suchas clustering[12],dimension-
ality reduction [36], semi-supervised learning [37] or imbalanced
classiﬁcation [15]. The resulting trees are smaller but more accu-
rate than traditional ones [38], and similar advantages carry over
to the forest setting [6, 9, 13, 14, 35]. Optimally updating a deci-
sion node given all other nodes (the reduced problem) takes the
formofacertain0/1lossbinaryclassiﬁcationproblem.ThisisNP-
hardforobliquesplits,butcanbeapproximatedbyasurrogateloss,
although this can often produceinadequate results. Here, we cap-
italize on the possibility to use partial enumeration in a 2D space
of lineorientations toﬁnda goodbivariate spliteﬃciently.
Finally, it is possibleto constructa bivariate tree byﬁrst apply-
ingfeatureselectiontotheproblemtoselecttwofeaturesglobally
andthenﬁttingatree.Thisgivesabivariatetreethatusesthe same
twofeaturesineachnode,whichwillresultinapooraccuracy.We
seek bivariatetrees where each nodecanuse anytwofeatures.
4 Learning bivariate trees
We consider classiﬁcation (the extension to regression is straight-
forwardandgiven later).Consider a /u1D43E-classproblemwithdataset
ofsize/u1D441with/u1D437-dimensionalinputfeatures {(x/u1D45B,/u1D466.alt/u1D45B)}/u1D441
/u1D45B=1⊂R/u1D437×
{1,...,/u1D43E}.Considerabinarydecisiontree(eachdecisionnodehas
exactly2children)withasetofdecisionnodes Ndec,asetofleavesNleaf,andN=Ndec∪Nleaf.Wedeﬁnea routingfunctionineach
decision node /u1D456∈Ndecas/u1D453/u1D456(x;/u1D73D/u1D456):R/u1D437→{left/u1D456,right/u1D456}⊂N
which sends a sample xto either its left or right child. We use
bivariate decision nodes where routing function makes hard de-
cisions/u1D453/u1D456(x;/u1D73D/u1D456)=left/u1D456if/u1D464/u1D456/u1D457/u1D465/u1D457+/u1D464/u1D456/u1D458/u1D465/u1D458+/u1D44F/u1D456<0, otherwise
right/u1D456, and the learnable parameters are /u1D73D/u1D456={w/u1D456,/u1D44F/u1D456}, where
/bardblw/u1D456/bardbl0≤2 ensures splits of no more than 2 features. Each leaf
/u1D456∈Nleafcontains a constant label classiﬁer that outputs a single
class/u1D450/u1D456∈{1,...,/u1D43E}. We collectively deﬁne the parameters of all
nodes as /u1D6AF={(w/u1D456,/u1D44F/u1D456)}/u1D456∈Ndec∪{/u1D450/u1D457}/u1D457∈Nleaf. The predictive func-
tion of the entire tree /u1D447(x;/u1D6AF)guides a sample xalong a single
pathfromtherootthroughasequenceofbivariatedecisionnodes
toexactlyoneleaf,which provides theclassiﬁcationoutput.
Weconsiderthefollowingobjectivefunctionoveralltheparam-
eters ofa treeofgiven structure,where /u1D43F(·,·)is the0/1loss:
/u1D438(/u1D6AF)=/u1D441/summationdisplay.1
/u1D45B=1/u1D43F(/u1D466.alt/u1D45B,/u1D447(x/u1D45B;/u1D6AF))+/u1D706/summationdisplay.1
/u1D456∈Ndec/u1D719(w/u1D456)s.t./braceleftBigg
/bardblw/u1D456/bardbl0≤2,
/u1D456∈Ndec(1)
and weintroducethefollowing, new typeof regularization:
/u1D719(w/u1D456)=/braceleftBigg
/u1D436, if/bardblw/u1D456/bardbl0=2
/bardblw/u1D456/bardbl0,if/bardblw/u1D456/bardbl0<2.(2)
Both theℓ0constraint and the penalty /u1D719are diﬀerent from the
sparseobliquetreeformulationof[8],whichonlyhadan ℓ1penalty;
thisdoesencourageobliquenodeswithfew features,butdoesnot
achieve bivariatenodes,asshowninourexperiments.The feature
costregularization term (2) is critical in allowing not just bivariate
splitsbut also univariate ones and pruningnodes . It imposes a cost
of0,1or/u1D436foreachzero-,uni-orbivariatenode(using0,1or2fea-
tures)in thetree, respectively.Using a cost /u1D436>1means bivariate
splitsarepreferabletounivariateones onlyiftheyreducetheloss
suﬃciently.Ifanodeusesnofeatures( /bardblw/u1D456/bardbl0=0)thenitsendsall
pointseitherrightorleft(dependingon /u1D44F/u1D456),soitisredundantand
canbeprunedattheendofthealgorithm.Sinceusingnofeatures
has cost zero, the regularization term in (1) counts the eﬀective
number of nodes in the tree (with a weight of 1 or /u1D436if using 1 or
2features,resp.).Hence,theusercancontrolthenumberofnodes
inthetreevia /u1D706,and howuni- orbivariatethetreeis via /u1D436.
4.1 The better algorithm: bivariate TAO
In equation (1), the loss function is additively separable over the
training samples, and theregularization term is additively separa-
bleoverthenodes.Thisrendersitsuitableforalternatingoptimiza-
tionoverthetreenodes(onatreeofgivenstructure),whichresults
in monotonic descent and convergence in a ﬁnite number of iter-
ations. Wefollow an argument as in [8]. Theidea is based ontwo
theorems: separability condition and reduced problem (RP) over
a node. First, deﬁne the reduced setR/u1D456⊂{1,...,/u1D441}as the train-
ing instances that reach the node /u1D456∈N.Separability condition
implies that equation (1) can be separated and optimized over pa-
rametersofanynon-descendantnodes(locatedonthesamedepth)
independently and inparallel.This is a result of treemaking hard
decisions(R/u1D456∩R/u1D457=∅foranynon-descendant nodes /u1D456and/u1D457).Re-
duced problem over a node states that optimizing equation (1)
 
1338KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
over parameters of thegiven node /u1D456∈Nreduces to simpler, well-
deﬁned problem involving its reduced set R/u1D456. RP is diﬀerent for
decision nodesand leaves:
LeafEquivalent to optimizing the top-level objective (1) over pa-
rameter/u1D450/u1D456onR/u1D456. Exact solution: the majority class of the
samples inR/u1D456:/u1D450/u1D456=argmax/u1D458∈{1,...,/u1D43E}/summationtext.1
/u1D45B∈R/u1D456/u1D43F(/u1D466.alt/u1D45B,/u1D458).
Decisionnode The objective (1) can be equivalently reduced to
thefollowing0/1loss binaryclassiﬁcation problem :
/u1D438/u1D456(w/u1D456,/u1D44F/u1D456)=/summationdisplay.1
/u1D45B∈R/u1D456/u1D43F(¯/u1D466.alt/u1D45B,/u1D453/u1D456(x/u1D45B;w/u1D456,/u1D44F/u1D456))+/u1D706/u1D719(w/u1D456)s.t./bardblw/u1D456/bardbl0≤2(3)
where/u1D43Fisthe0/1lossand¯/u1D466.alt/u1D45B∈{left,right}isapseudola-
belassignedtotraininginstance /u1D465/u1D45Btoindicatethechildthat
yieldsalowerlossvalue.Thelossiscomputedbypropagat-
ingx/u1D45Bdownthecorrespondingchild.
4.1.1 Solvingthereducedproblemoveradecisionnode. Problem(3)
arises with sparse oblique trees [8] and is NP-hard with arbitrary
linear decision nodes, so there it is approximated via a surrogate
loss. However, with the constraint /bardblw/u1D456/bardbl0≤2, the problem can
besolvedexactlyin O(/u1D4413/u1D4372)byenumeratingeverypossiblesplit
(= linear dichotomy on the /u1D441instances) over all/parenleftbig/u1D437
2/parenrightbigfeature com-
binations. Unfortunately, this is very costly. We propose a faster,
approximatesolutionwhich shows goodresults onpractice.
Solution 1: /u1D43Fbiv(bivariate solution) of eq. (3) s.t. /bardblw/u1D456/bardbl0=2,
/u1D44F/u1D456∈Ris achieved at /u1D73Dbiv
/u1D456={wbiv
/u1D456,/u1D44Fbiv
/u1D456}.Deﬁne a small,ﬁxedsub-
set of line orientations W∈R2×/u1D43Bsampled uniformly in two di-
mensionsbyrotatingitaroundtheorigin( /u1D43Btimes)withinarange
of 0 to 180 degrees. For points in the reduced set R/u1D456we select a
pairoffeaturesandprojectthemontoeachorientation w/u1D459∈W.It
can be vectorized Xbiv
/u1D456=X/u1D456SWwhereX/u1D456∈R|R/u1D456|×/u1D437is a matrix
containing all instances in the reduced set R/u1D456, andS∈R/u1D437×2is a
matrix with each column consisting of a 1in the row correspond-
ing to the selected feature and zeros everywhere else. Vectoriza-
tionallowsaGPUutilizationforfastercomputation.Thesolution
can becomputedusingthresholding over features of Xbiv
/u1D456.There-
sulting bias can be interpreted as an optimal split of the points
projectedontoselectedorientation w/u1D459tominimizenumberofmis-
classiﬁed instances in R/u1D456. We repeat this process for each pair of
features and ﬁnd best solution wbiv
/u1D456,/u1D44Fbiv
/u1D456, wherewbiv
/u1D456is a sparse
vector.Pseudocodeis in Fig. 3and an illustrationin Fig. 2.
Solution 2: /u1D43Funiv(univariate solution) of eq. (3) s.t. /bardblw/u1D456/bardbl0=1,
/u1D44F/u1D456∈Ris achieved at /u1D73Duniv
/u1D456={(0,/u1D464univ
/u1D456)/u1D447,/u1D44Funiv
/u1D456}. It is computed
simplybythresholdingoveroriginalfeatures.Forthedecisionnode
/u1D456∈Ndecalgorithmselectsonefeaturetosplitpointsin R/u1D456inorder
tominimize eq.(3).
Solution 3: /u1D43F0(zero-variate solution) of eq. (3) s.t. /bardblw/u1D456/bardbl0=0,
/u1D44F/u1D456∈{−1,+1}is achieved at /u1D73D0
/u1D456={0,/u1D44F0
/u1D456}. This indicates that all
samples inR/u1D456are sent totheleft( /u1D44F0
/u1D456=−1)ortheright ( /u1D44F0
/u1D456=1).
Thesolutionof theRPcan besummarized as follows:
/u1D73D∗
/u1D456= 
/u1D73Dbiv
/u1D456,if/u1D43Fbiv+/u1D706/u1D436<min(/u1D43Funiv+/u1D706,/u1D43F0)
/u1D73Duniv
/u1D456,if/u1D43Funiv+/u1D706<min(/u1D43Fbiv+/u1D706/u1D436,/u1D43F0)
/u1D73D0
/u1D456,if/u1D43F0≤min(/u1D43Fbiv+/u1D706/u1D436,/u1D43Funiv+/u1D706)
Webreakthetiesalwaysinfavorofamodelwithlowernumberof
parameters. The separability condition allows us to optimize /u1D438in
parallelfortheparametersofanygroupofnodesthatdonothave/u1D465/u1D456/u1D465/u1D457
4po
ssible
directions
globaloptimumapprox. optimum
Figure 2: Illustration of our approximate solution of the re-
ducedproblematadecisionnodeassumingaselectedpairof
features(/u1D465/u1D456,/u1D465/u1D457).Theinstancesinthereducedsetofthenode
arelabeledaccording to their pseudolabels(preferredchild,
left◦orright×).Theoptimum(in0/1loss)linearclassiﬁeris
thethickblueline(onemisclassiﬁcation).Theapproximate
optimumfoundusingthe /u1D43B=4possibledirections(inset)is
thethickredline(twomisclassiﬁcations).Thethinredlines
are all the possible thresholds (passing through midpoints
betweenprojectedinstances)for theredorientation.
aparent-childrelationship,whilekeepingtheparametersofthere-
maining nodes ﬁxed.Weoptimizenodeparameters in thereverse
breadth-ﬁrstsearchorderstartingwithdeepestnodesandmoving
allthewaytotheroot.Thisresultsinamonotonicdecreaseinthe
objective function /u1D438. It is important to note that after each itera-
tion, the subset used for training each classiﬁer or leaf predictor
is modiﬁed. Some decision nodes will opt for not using a feature,
hence being redundant (since it directs all input instances to the
same child) and candidates for eventual removal in a postprocess-
ing step at the end of thetraining. This means that bivariate TAO
indeed achieves pruning, or equivalently learns the tree structure
(subject tobeing asubset oftheinitial tree’s structure).
4.1.2 Regression. This requires just two modiﬁcations. First, the
reducedproblemoveraleafissolvedbycomputingthemeanofthe
samplesinitsreducedset.Second,theobjective(1)inthedecision
node is now reduced to a weighted binary classiﬁcation (instead
of eq. (3)), where the weights are obtained by sending the sample
downeach child subtreeand computingtheir respectiveloss.
4.1.3 Regularizationpath. For/u1D43E-classclassiﬁcationproblemwith
/u1D441training samples deﬁne /u1D4411≥/u1D4412≥ ··· ≥/u1D441/u1D43E, where/u1D4411
is a number of samples in the most populous class. Considering
that empty feature solution of the RP we can derive /u1D706/u1D719(w/u1D456) ≥
/u1D43F0−/u1D43Fbiv∈{0,1,...,/u1D441−/u1D4411}. Since bivariate split generally pro-
duceslower 0/1losswetypicallyset /u1D436≥1.Furthermore,consider-
ingthatvaluesof /u1D43Fareinteger(numberofmisclassiﬁedpoints)itis
enoughtorunthealgorithmfor /u1D706={0,1,...,/u1D441−/u1D4411}tocompute
fullregularizationpath.At /u1D706=/u1D441−/u1D4411treeconsistsofasingleleaf
withthelabelofsamplesin /u1D4411.Wecomputethefullregularization
pathandselectatreewithbestvalidationerror.Nodeswithempty
features are retained until the end of the algorithm since their so-
lutionsmaybecomenonempty at lateriterations ora larger /u1D706.
In this context, /u1D706/u1D719(w/u1D456)can be interpreted as the maximum al-
lowed number of misclassiﬁed samples by a decision node. When
this threshold is exceeded, the node is pruned. For most datasets,
 
1339BivariateDecision Trees: Smaller,Interpretable, MoreAccur ate KDD ’24,August25–29, 2024,Barcelona,Spain
inputtraining set{x/u1D45B,/u1D466.alt/u1D45B}/u1D441
/u1D45B=1,
binaryaxis-aligned tree /u1D447withgiven structure and
parameters /u1D6AFat thenodesN=Ndec∪Nleaf
repeat
for/u1D456∈N
R/u1D456←reduced setofnode /u1D456
end if
for/u1D451=Δdownto0do
for/u1D456∈nodesat depth /u1D451(can bedonein parallel)
if/u1D456∈Nleaf
/u1D450/u1D456←majority class inR/u1D456
else
solution ofreduced problemeq. (3)for decision node /u1D456∈Ndec
end if
if/u1D43Fbiv+/u1D706/u1D436<min(/u1D43Funiv+/u1D706,/u1D43F0)
w/u1D456,b/u1D456←/u1D73Dbiv
/u1D456
else if/u1D43Funiv+/u1D706<min(/u1D43Fbiv+/u1D706/u1D436,/u1D43F0)
w/u1D456,b/u1D456←/u1D73Duniv
/u1D456
else if/u1D43F0≤min(/u1D43Fbiv+/u1D706/u1D436,/u1D43Funiv+/u1D706)
w/u1D456,b/u1D456←/u1D73D0
/u1D456
end if
end for
end for
until/u1D438(/u1D6AF)does not strictlydecrease
remove redundant nodes (empty featuressolution)
returntrained/u1D447
inputtrainingset{x/u1D45B,¯/u1D466.alt/u1D45B}/u1D45B∈R/u1D456o f decision node /u1D456∈Ndec,
matrix of orientations W∈R2×/u1D43B
foreach pair of features /u1D457,/u1D458∈/u1D437
forw/u1D459∈W
x/u1D457,/u1D458
/u1D459←project selectedfeatures of
each sample inR/u1D456onto/u1D464/u1D459
/u1D44F/u1D457,/u1D458
/u1D459←optimal thresholdingover x/u1D457,/u1D458
/u1D459
if/u1D457,/u1D458,w/u1D459,/u1D44F/u1D457,/u1D458
/u1D459produce lowestvalueof eq. (3)
/u1D73D/u1D44F/u1D456/u1D463
/u1D456←{w∗,/u1D44F/u1D457,/u1D458
/u1D459}, wherew∗isa sparse vector
of allzeros withcorresponding value of w/u1D459
at/u1D457,/u1D458
end if
end for
end for
return/u1D73D/u1D44F/u1D456/u1D463
/u1D456
Figure 3: Pseudocode: bivariate TAO algorithm (top); solu-
tio
n to thereducedproblem in adecisionnode /u1D456(bottom).
there exists a range of /u1D706values where tree parameters does not
change.Knowingtheminimumdiﬀerencein 0/1lossbetweenempty
featuresolutionand currentbestsolution(univariateorbivariate)
for each decisionnodewecan calculatenext signiﬁcant /u1D706inregu-
larizationpath.Thiswayfullregularizationpathcanbecomputed
much faster.
4.1.4 Computationalcomplexity. Assume/u1D441trainingsamplesand
acompletetreeofdepth Δ(having2Δ−1decisionnodes).Call R/u1D456
the reduced set of node /u1D456. At the start of each iteration we update
R/u1D456for each node by propagating each training instance to its cor-
responding leaf, which is O(Δ/u1D441)with space complexity of O(/u1D441)
tostoreindices.WeoptimizenodeparametersinreverseBFSorderstartingwith
leaves. For each leaf /u1D456∈ Nleafwe compute label using majority
vote inO(|R/u1D456|). For each decision node /u1D456∈ Ndecwe ﬁrst com-
putepseudolabelsbysendingsamplesofreducessettotheleftand
rightchildatO(2Δ|R/u1D456|).Forunivariatesplit,wesortinstancefea-
ture values{/u1D465/u1D45B/u1D451}/u1D45B∈R/u1D456before thresholding. In the ﬁnal step, we
determine the loss for every bias value similar to how the purest
split is computed within recursive partitioning. It can be done ex-
actly and eﬃciently through an incremental method, where we
calculate the loss for the next bias value based on the loss of the
current value. The total computational complexity for /u1D437features
isO(/u1D437Δ|R/u1D456|)+Θ(/u1D437|R/u1D456|log|R/u1D456|). For bivariate split the process
is computationally similar to ﬁnding univariate split on X/u1D44F/u1D456/u1D463
/u1D456(de-
scribedin 4.1) for every pair of features. Since /u1D43Bis constant, total
computationscomplexityis O(/u1D4372Δ|R/u1D456|2)+Θ(/u1D4372|R/u1D456|2log|R/u1D456|).
Thetotalcostoftrainingisdominatedbydecisionnodes.Atany
depththeunionofallreducedsetsisthewholetrainingset.Inthis
casethetotalcomputationalcostisupperboundedby Θ(Δ2/u1D4372/u1D4412+
Δ/u1D4372/u1D4412log/u1D441).It isworthmentioning thatdecisionnodes ateach
depthcanbetrainedinparallel,whichdrasticallyreducesrunning
timeofthealgorithm.
4.2 The fasteralgorithm: bivariate CART
Our idea above of partial enumeration over the bivariate splits
can be combined with greedy recursive partitioning (in particu-
lar CART). This does not anymore optimize any global objective
function and it produces worse trees than bivariate TAO, but it is
muchfaster.Itcanbedonein twoways.One, moreeﬃcient, is by
modifyingtheCARTsplitstep(basedontheGiniindex)tousethe
partial enumeration, in a similar way to the TAO decision node
reduced problem above. Another, less eﬃcient (in memory), is to
constructanew,augmentedtrainingsetwith ≤/u1D437+/parenleftbig/u1D437
2/parenrightbig|/u1D43B|features
in advance and simply runthe usual, univariate CART on it. This
algorithm works quite well, as seen in our experiments, resulting
inbivariate trees that are quitesmaller and generalize betterthan
univariate CART. Note this algorithm is diﬀerent from previous
workonbivariate trees.
4.3 Interaction of /u1D436and/u1D706: a phasediagram
Here we give a qualitative understanding of the eﬀect of the fea-
turecost/u1D436andtheregularizationhyperparameter /u1D706onthesizeof
the tree and the number of features it uses. It can be plotted as a
“phase diagram” in (/u1D706,/u1D436)-space such as that in ﬁg. 4. Inspection
of the objective function shows that, since the loss is always less
than/u1D441(where/u1D441is the sample size), there exists a critical value
0</u1D706∗</u1D441such thattheoptimaltreefor /u1D706≥/u1D706∗consists ofa sin-
gleleafnodewithaconstantlabel(zero-variatetree).Likewise,for
small enough /u1D706, all nodes are bivariate if /u1D436≤1(since univariate
splitsareno cheaperand less powerful);and thereexists a critical
value1</u1D436∗≤/u1D441such that all nodes are univariate if /u1D436≥/u1D436∗
(since even a single bivariate node is too costly). Trees with both
uni- and bivariate nodes exist fora region (0,/u1D706∗]×(1,/u1D436∗], which
contains the practically useful models, where we allow the objec-
tivefunctiontodeterminetheoptimaluni/bivariateratio.Thisalso
suggests asimpliﬁedtraining strategywhere weﬁx /u1D436toavaluea
bitover one(onpractice, /u1D436∈[1.1,1.5]) and cross-validate /u1D706.
 
1340KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
% bivariatenodes #decision nodes test error
0  10 1003  
2.5
2  
1.5
1  
0.5
00.20.40.60.81
/u1D706/u1D436univariate
bivariate
zero-variateunivariatebivariate
0  10 1003  
2.5
2  
1.5
1  
0.5
0102030405060
/u1D7060  10 
1003  
2.5
2  
1.5
1  
0.5
100101102
/u1D706
Fig
ure 4: Phase diagram (/u1D706,/u1D436)for the Segment dataset.
We plot: the proportion of bivariate vs univariate decision
nodes (indicating the regions of pure zero-, uni- and bivari-
ate trees); the number of decision nodes; and the test error
(%).Theellipseindicatesthe regionof best-errortrees.
0 1 2 3 4 5 600.51.01.52.02.5
/u1D441×105CA
RT
C5.
0BivariateTAO#dec.nodes×103
0 1 2 3 4 5 620.621.221.822.423
/u1D441×105/u1D438test(%)
0 1 2 3 4 5 6100101102103
/u1D441×105Runtime(s)
Figure 5: Number of nodes, test error and training time for
univariateCARTandC5.0andbivariateTAOtreesasafunc-
tionofthesamplesize(subsampledfromtheSUSYdataset).
5 Experiments
We provide all necessary details about hyperparameters, imple-
mentations ofdiﬀerent algorithms,etc.intheappendix.
Accuracy, tree size and number of features. Table 1 shows that,
almostwithoutexceptionacross several datasetsofvarying types,
our bivariate TAO tree is both most accurate and smallest com-
pared to any other univariate tree (CART [5], C5.0 [31]) or bivari-
ate tree (in particular, BiDT [3]). This is as expected: it uses the
more ﬂexible model and the better optimization. Next best is our
other,moreapproximatealgorithm,bivariateCART.Theimprove-
ment inaccuracyover univariate treesis consistent and varies de-
pending on the dataset (usually a few percentage points). The re-
duction in depth and number of nodes, and the corresponding sim-
pliﬁcation of the tree, is drastic, typically several times fewer nodes
(17 times smaller on the MiniBooNE dataset). In terms of training
time, althoughnotaslightning-fast asunivariate trees,ourbivari-
atetreesscalewelltolargedatasets(unlikeBiDT,whichtimesout,
asexpectedfromitsbrute-forcesearch).Whencomparedwiththe
oblique trees, bivariate TAO trees are less accurate (as expected)
butthereisoftenlittlediﬀerence. However,obliquetreesusequite
a lotoffeaturespernode,whichmakes themmorecomplex.Simi-
lar conclusions followfrom theresults forregression (table2).
Dependenceonsamplesize. Itiswellknownthatunivariatetrees
(CART, C5.0) grow in size proportionally to the sample size, in-
dicating a suboptimal training and pruning, as a consequence of
theirgreedyrecursivepartitioning[29].Thisisconﬁrmedinﬁg.5,
wheretheycontinuetogrowevenwhentheiraccuracyplateausaf-
ter reaching≈50%of the total.In contrast, ourbivariate TAO tree
slowlygrowsinsizebuteventuallystops,indicatingithasreached  Train
  Test0-1Loss ( %)
100
100101
101102
102Best model  Train
  Test
10−110−2100
1 00101
101102
102103 0Best model#decisionnodes
10010110230
20
10
0
10−110−21001 0110210330
20
10
00#non-zero weights
1001011021015
5
20
/u1D70610−110−21001 011021031015
5
200
/u1D706
Fi
gure 6: 0/1 loss, number of nodes and average number of
fea
tures per decision node for bivariate (left) and oblique
trees(right)overtheir regularizationpath (Segmentdataset).
a suﬃcient model size—all the while with a better accuracy and
muchsmallersize( ≈3–5times)thantheunivariatetrees.Withour
unoptimizedcode,bivariateTAOis much slower totrainthanthe
(extremelyfast)CARTandC5.0,butitisstillgoodforpracticaluse.
Obliquetreesusemanyfeatures. Hereweshow thatonecannot
achieve a bivariate tree by overpenalizing a sparse oblique tree.
The latter,proposedin [8],minimizes the sum of the0/1 loss plus
anℓ1penalty/u1D706/summationtext.1
/u1D456/bardblw/u1D456/bardbl1on the weight vector of each decision
node/u1D456in the tree, with hyperparameter /u1D706≥0. Fig. 6 shows the
entire regularizationpathfor asparseobliquetreeand a bivariate
tree on the Segment dataset. While the bivariate tree has an aver-
ageofatmost2featurespernodethroughoutitspath,theoblique
onereachesanaverageof2onlyforveryhigh /u1D706,atwhichpointthe
tree is very small and has an enormous error. The reason for this
is that, while increasing /u1D706does encourage sparsity of weight vec-
torsover thewholetree,this sparsity resultsin somenodes being
pruned(whenever w/u1D456=0)whileothernodesusemorefeatures.In
this dataset, the best trees have about the same error ( ≈2.5%) and
numberofnodes(≈16)forbothbivariateandoblique,butthelatter
using≈7features pernode.
5.1 Interpretability ofbivariate trees
The small size of a (well-optimized) bivariate tree and the ability
tovisualizeitsdecisionnodesmakesithighlyinterpretable,aswe
demonstratehere in twoexamples.
5.1.1 Breast Cancer UCI dataset. This is a binary classiﬁcation
task into malignant and bening tumors. Each input instance con-
tains 30 features (listed in appendix table 4) extracted from a col-
lection of cells, speciﬁcally, geometric features about size, shape,
etc.(measuredfromsnake-generated cellnucleiboundaries).Each
suchfeatureisveryinformativeonitsownandcanbereadilyiden-
tiﬁedand understoodbyaradiologist.
 
1341BivariateDecision Trees: Smaller,Interpretable, MoreAccur ate KDD ’24,August25–29, 2024,Barcelona,Spain
Table 1: Comparison between bivariate, univariate and oblique trees for classiﬁcation. Each tree was selected by cross-
validating its hyperparameter and this was repeated 3 times over random training/validation sets. We report training and
testaccuracy( %±stdev);averagedepth Δ,nodecount /u1D45Bandnumberoffeatures /u1D453pernode(weomit /u1D453=1and2forunivariate
andbivariatetrees,respectively);andaverageruntime(secondsor“timeout”).Weindicatewith color greenbestand bluesec-
ond b
estresultfor testaccuracyandnodecount overthe univariateandbivariatetrees(ignoringthe obliquetrees).
Dataset ( /u1D441tra in,/u1D437,/u1D43E) .............bivariate............. ......univariate...... . oblique .
TAO CART BiDT CART C5.0 TAO
BreastCancer (455,30, 2)training (%) 96.04±1.53 99.12±0. 00 96.99±0.1098.61±0.41 98.9±0. 4898.21±0.79
tes
t(%) 98.25±0.4398.00±0 .00 97.66±0. 1094.73±0.00 95.6±0. 6797.71±1.04
Δ/#
nodes//u1D4531/3 3.0/9 1.6/5 4.0/16 5.3/12 3/15/10.3
runtim
e (s) 4 6 2 0.1 0.1 5
Seg
ment (1963,19, 7)training (%) 98.47±0.35 97.30±0. 00 97.58±0.0198.76±0.00 98.9±0. 1099.48±0.21
tes
t(%) 97.41±0.1496.73±0 .14 96.06±0. 1496.01±0.47 96.3±0. 4897.58±1.31
Δ/#
nodes//u1D45311.0/13 11.0/25 9.0/21 15.0/128 12.25/77 8/271/8.5
runtim
e (s) 30 13 27 0.1 0.1 20
Spa
mbase (3910,57, 2)training (%) 96.39±0.08 97.49±0. 14 95.34±1.8597.86±2.91 96.16±0. 1496.55±0.47
tes
t(%) 93.34±0.07 92.19±0. 0592.71±0.5392.18±0.31 92.2±0. 4294.31±1.22
Δ/#
nodes//u1D45314/53 10.0/77 16/161 24.7/362 14.7/77 4/30/42.1
runtim
e (s) 120 284 208 0.3 0.3 60
Hou
se 16H (11464,16, 2)training (%) 87.1±1.55 89.45±0. 00 90.42±0.2386.2±0.0 91.98±0. 5586.55±1.10
tes
t(%) 85.6±0.0784.73±0 .0585.6±0 .1783.4±0.0 83.06±0. 3285.47±0.51
Δ/#
nodes//u1D4537/35 10/107 10/115 8/75 15.05/245 4/13/14.9
runtim
e (s) 30 21 15 0.2 0.1 24
Lette
r (16000,16,26)training (%) 100±1.37 100±0. 01 98.40±1.7694.30±0.01 98.66±0. 0795.43±0.29
tes
t(%) 87.25±0.1187.25±0 .00 86.80±0. 3786.04±0.04 86.76±0. 3390.41±0.31
Δ/#
nodes//u1D45335/1314 35.0/2121 37.6/2596 28/3888 16.85/2817 11/2155/8.5
runtim
e (s) 300 73 12 0.3 0.9 77
Ele
ctricity (32702, 8, 2)training (%) 98.97±2.80 95.80±0. 80 96.14±1.2099.10±0.00 95.04±0. 4398.1±1.8
tes
t(%) 89.38±0.12 86.05±0. 05 87.91±0.0687.80±0.1688 .64±0.4290.23±0.19
Δ/#
nodes//u1D45323.0/1083 24.0/1741 22.3/1881 30.0/6366 17.25/2615 10/249/6.8
runtim
e (s) 300 81 393 0.9 0.9 134
MiniBo
oNE (62048,50, 2)training (%) 92.36±0.00 96.02±0. 02 - 96.61±0.02 95.88±0. 0791.98±0.15
tes
t(%) 91.16±0.0090.68±0 .03 - 90.25±0.03 89.84±0. 1091.43±0.12
Δ/#
nodes//u1D45311.0/105 15/831 - 19.3/2012 15.65/1787 10/133/16.8
runtim
e (s) 1200 1000 timeout 5.2 6.4 3000
SUS
Y (600000,18, 2)training (%) 80.71±0.00 81.35±0. 00 - 81.45±0.00 80.90±0. 0081.10±0.00
tes
t(%) 79.51±0.00 79.01±0. 00 - 78.90±0.0079 .10±0.0080.3±0.00
Δ/#
nodes//u1D45317.0/1077 21/2780 - 24/4389 16.25/3227 12/983
runtim
e (s)≈2h≈1h timeout 40.2 35.2 ≈2h
Table 2: Like table 1 but for regression, reporting RMSE
±std
evover3runs.
Dataset (/u1D441tra in,/u1D437,/u1D43E)......bivariate...... univariate oblique
TAO CART CART TAO
airfoil (1002, 5, 1)0.34±0.13 0.01±0. 000.52±0.103.02±0.29
2.46±0.432.72±0 .022.75±0.623.13±0.38
16/252 22/2133 15/479 8/147/3
15 5 1.3 13
aba
lone (2506, 8, 1)2.14±0.03 0.39±0. 122.32±0.112.11±0.02
2.24±0.13 2.95±0. 382.34±0.592.18±0.05
2/7 23/903 7/39 6/58.6/6
21 10 0.9 19
cpu
act (4915,21, 1)1.84±0.53 1.74±0. 432.10±0.712.47±0.07
3.11±0.103.33±0 .313.97±1.322.71±0.04
10/191 14/327 13/373 6/52.7/13
27 17 1.3 13
aile
rons (7154,40, 1)1.88±0.03 0.71±0. 001.81±0.121.65±0.02
2.03±0.002.21±0 .002.25±0.631.76±0.05
5/21 15/50 21/105 6/60.2/13
27 20 1.3 21Fig. 9 (appendix) shows how our bivariate TAO trees dominate
the
univariate CART trees by a signiﬁcant margin over the range
oftreesizes.Fig.7showsthebesttreesintermsofcross-validation.
TheunivariateCARTtreehasdepth7,16leavesand6.4%error.The
bivariate TAO tree has depth 3, 5 leaves and 2% error. Not only is
thebivariatemuchmoreaccurate,butitssmallsizemakesiteasier
tounderstand bysimpleinspection.Itresults inonly5rulesvs 16
rulesfortheunivariate tree.
Globallevelstructure. Thetreestructureshowsthebivariatetree
can represent the whole dataset only in 5 very simple IF-THEN-
ELSErules.Itisnotablethatthetreeperformsasigniﬁcantfeature
selection. It uses only 6 features (out of 30). Three of them con-
cern theradiusmeasurements of nuclei boundaries (mean value,
largestvalueandstandarderror),andobviouslycapture sizeinfor-
mation about the cells. Two of them concern contour concavities
(mean value and standard error), and obviously capture shapein-
formationaboutthecells (essentially, whether they are circle-like
or have indentations). The last feature is the texture (mean value)
of the cell nuclei. The tree ignores features about smoothness of
thecontour,area of thecells, perimeter of thecell contour,fractal
 
1342KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
univariate CARTtree: Δ=7,16leaves,/u1D438test=6.4% bivariateTAO tree: Δ=3,5leaves,/u1D438test=2.0%
 
 
  
1
 
 
01
 
10 0
 
 
0100
10
 
 
  
 
01
 
  
1
 
1
 
10
 
 0
10
 
1:if(/u1D4657>0.05)&(/u1D4652 0>0.09)&(/u1D46510>0.58)
&(/u1D46524>2.00)&(/u1D46514>−1.19)&(/u1D4651>0.17)thenPREDICT1 endif
2:if(/u1D4657>0.0
5)&(/u1D46520>0.09)&(/u1D46510>0.58)
&(/u1D46524>2.00)&(/u1D46514>−1.19)&(/u1D4651≤0.17)thenPREDICT0 endif
3:if(/u1D4657≤0.0
5)&(/u1D46527≤0.50)&(/u1D46516≤3.20)thenPREDICT1 endif
4:if(/u1D4657≤0.0
5)&(/u1D46527≤0.50)&(/u1D46516>3.20)thenPREDICT0 end if
5:..
.//12MORERULES ...1:if(/u1D4657+0.3/u1D46510>−0.1)&(/u1D4651+9.8/u1D46520>2.3)thenPREDICT1 endif
2:if(/u1D4657+0.3/u1D4651
0>−0.1)&(/u1D4651+9.8/u1D46520≤2.3)thenPREDICT0 end if
3:if(/u1D4657+0.3/u1D4651
0≤−0.1)&(/u1D46520+1.2/u1D46527≤0.7)thenPREDICT0 end if
4:if(/u1D4657+0.3/u1D4651
0≤−0.1)&
(/u1D46520+1.2/u1D46527>0.7)&(/u1D4650+3.9/u1D4651>1.0)thenPREDICT1
5:elsePREDICT0 e nd if
Fig
ure7: BestunivariateCART treeandbivariateTAO treewiththeir setsof rules(BreastCancerdataset).
dimension,etc.Thismakessensesincesomeofthosearelikelycor-
related with size and shape as given by the radius and concavity,
and thus areredundant.
Local level structure. At a local level in the tree, we can look
at speciﬁc nodes. The pairwise combination of features learned
in each decision node can be seen as a new, constructed feature,
which is quite meaningful and further improves interpretability.
Forexample,therootofthetree(whichusesasfeaturesthelargest
cell radius and the mean concavity) can be understood as detect-
ing a cell collectionwhere the cell nuclei boundaries areirregular
orunusuallylarge.Theroot’srightchilduses asfeatures thestan-
dard error of radius and concavity, which means it detects high
variance in the shape and size of the cell collection (as opposed
toa collectionthat has uniform shapeand size). Taking these two
nodestogether,thetreepredictsmalignancy(rightmostpath).This
canbedonewiththeother4leaves, eachofwhich isapopulation
(malignant orbenign) characterizedbyashortpathorruleinvolv-
ingveryfewconstructedfeaturesthataremeaningful. Asanother
example, thedeepest decisionnodeclassiﬁes thecell collectionas
malignantbasedonacombinationofaveragesizeandtexturevari-
ance irregularities.
5.1.2 Segmentdataset. Considernowﬁg.8,whichshowsabivari-
ate TAO tree on the Segment dataset, a standard UCI benchmark
forclassiﬁcation.Ithas1963instances training,19continuousfea-
turesand7classes(balanced).Eachclassisanoriginalcolorimage
and each feature is a localdescriptor based on 3 ×3patches (listed
in the appendix, table 5). The bivariate tree has 25 nodes (12 de-
cision nodes, 13 leaves), depth 10, and a training and test error of
1.53%and2.59%,respectively.TheunivariateCARTtreewithlow-
est testerror(notshown) ismuchbigger: 128nodes,depth15and
testerror3.99 %.IfwetuneCARTpost-pruningtoachieve25nodes,
the error increases to 7.5 %. Inspecting the bivariate tree reveals a
wealth of information about the data,as we discuss next. Such in-
sight wouldnotbepossiblewithblack-boxmodelssuchasforests
orneural nets.Globallevelstructure. Thetreeisheavilylopsided,havingdepth
10 but only 13 leaves. This means for most decision nodes one or
bothchildrenisaleaf,sotheIF-THEN-ELSErulescorrespondingto
thetreearealmostperfectlytail-recursive.Thismakes themmore
interpretable,andfasteratinference(manyleaveshavelowdepth).
A consequence of the lopsided structure is that several classes,
accounting forover 50%of thedataset, are processed in a cascade
way,and thetreemakes early decisions forthem (with very short
rules). For example, class 3 is decided at the root (node A) after a
singlebivariate decision,withzero training error.Classes 5,4and
0 are also decided in a single root-leaf path each after 2, 3 and 5
decision nodes (one of them univariate), respectively, with near-
zero training error. Classes 1, 2 and 6 are harder to separate and
requiredeeperrules.
The tree has performed a signiﬁcant feature selection : it uses
only 10 features (0 1 5 6 7 9 10 12 15 18) out of the total 19. Since
the tree has 12 decision nodes, it could use up to 24 distinct fea-
tures, but it uses fewer. This is driven by the optimization we ap-
ply, which globally updates all the tree parameters. In contrast, a
univariate tree with 12 decision nodes would be forced to use at
most 12 features no matter what, because it can have only one in
each node. In order tousesuﬃciently many features, a univariate
treehas tobeoverly deep,which limits their interpretability.
Locallevelstructure. Weshowascatterplotateachdecisionnode
projectingtheinstancesreachingitontoitstwofeaturesandshow-
ing the boundary of the split. The scatterplots show the power of
bivariate splits (most obviously in nodes E, G, J, etc.). Using uni-
variate splits would require a deep sequence of splits. More im-
portantly, thescatterplotsprovideaformofsuperviseddimensional-
ityreduction,like a lineardiscriminantanalysisbut tree-structured,
whichisveryhelpfulforvisualization .Itisalsodirectlyinterpretable
because the projection is directly on two original features (rather
thana complexfunctionofallthefeatures). Forexample, nodesC
and E show a distinct cluster structure in classes 4 and 0, respec-
tively.AlthoughwewerenotabletoobtainoriginalimagesofSeg-
ment dataset, we conjecture that these clusters might correspond
 
1343BivariateDecision Trees: Smaller,Interpretable, MoreAccur ate KDD ’24,August25–29, 2024,Barcelona,Spain
-1.6 0.1 1.8
x0-0.11.32.7-0.9 -0.55 -0.2
x10-0.9-0.55-0.2x12-1 1 3
x9-202x00 5 10
x5-101x10
-0.7 -0.45 -0.2
x18-1.701.7x0-1 -0.6 -0.2
x9-1-0.6-0.2x12
-2 -0.8 0.5
x1-0.81.94x7
-0.9 -0.8 -0.7
x9-0.5-0.4-0.3
-1 -0.6 -0.2
x10-1-0.6-0.2x12-2 0 2
x1-202x15
-1 0 1
x1-101x9
-1 -0.6 -0.2
x10-1-0.6-0.2x12|R|:284
/u1D43F:0
cl
ass:3
|R|:281
/u1D43F:0
class:5
|R|:281
/u1D43F:0
class:4|R|:278
/u1D43F:1
class:0
|R|:172
/u1D43F:2
class:2|R|:20
/u1D43F:0
class:2|R|:235
/u1D43F:0
class:1|R|:118
/u1D43F:8
class:6
|R|:42
/u1D43F:2
class:1|R|:64
/u1D43F:7
class:2|R|:97
/u1D43F:6
class:6
|R|:24
/u1D43F:4
class:2|R|:67
/u1D43F:3
class:6|R|:1963/u1D43F:33
|R|:1679/u1D43F:33
|R|:1398/u1D43F:33|R|:1117/u1D43F:33
|R|:862/u1D43F:33
|R|:584/u1D43F:32|R|:412/u1D43F:30
|R|:251/u1D43F:17
|R|:133/u1D43F:9
|R|:91/u1D43F:7|R|:161/u1D43F:13
|
R|:255/u1D43F:0A
B
CD
E F
GH
I J
K
L
F
igure 8: A bivariate TAO tree on the Segmentdataset, plotted in 3 columns so it ﬁts. Each decision node shows a scatterplot
of itsinstancesonto itstwo features.Eachnodeshows thenumberof traininginstancesreaching it( |R|andtheir0/1loss ( /u1D43F).
to objects of diﬀerent color. Nodes E, G and K all use a bivariate
split on features (10,12) and show a clear linear, parallel structure
over multiple classes. Features 10 and 12 correspond to average
amount of red and green color of the region. Bivariate split can
beinterpreted as a mixtureof thesetwocolors.Samples of class 0
in node E can be separated from other samples by measuring the
amount ofthis mixtureand thresholdingit usingbias.
All nodes use bivariate splits except nodes B and F, which use
a univariate split. For plotting purposes only (so we can show a
2D scatterplot), we include feature 0 (region centroid column) as
the vertical axis in those nodes. This has the serendipitous eﬀect
of showing thatallclasses show a linear structurealongfeature0.
Notehowsomebivariatesplitsareabletoseparateoutoneclass
from the others cleanly (e.g. nodes A, B, C and E). But,in general,
thisisnotpossiblewithalineardecisionboundaryanditrequires
further partitioning down the tree. For example, nodes D and E
show a sequence of two splits that separate class 0 from the rest.
Thus, we should generally expect that a given decision node will
have someclasses straddlingbothsides oftheboundary.
Thebivariatedecisionfunctionscanusuallybeunderstoodasnew,
meaningfulfeatures .Forexample,splitinnodeDcanbeinterpreted
asacombinationofaverageamountofredcolorintheregionand
a contrastofadjacent pixels.Weassumeit canhelptodistinguish
presence ofverticaledgesofspeciﬁccolortone.Somefeatures are
repeatedly used (sometimes the same pair of features), indicating
their relevance to separating some group of classes. For example,
features 1(rowofthecenterpixeloftheregion) and9(average in-
tensityoftheregion)arecriticaltoseparatecleanlyﬁrstclass3andthen class 4 from the rest. This make sense since it indicates posi-
tion of the 3 by 3 patch and its color the intensity. Some features
areusedonlyonce,deepdownthetreeandaﬀectingasmallsubset
ofinstances, indicating theyareneededtomakeaﬁnedistinction.
Forexample,features0and6appearonlyonceandtogetherinthe
deepest node (L). Some bivariate splits could be turned into uni-
variate ones (eg nodes A and C) but at a higher loss. For example
making univariate split at node A results in 15 moremisclassiﬁed
points.
6 Conclusion
We have proposed bivariate decision trees as a practically useful
tradeoﬀbetweenunivariatetreesandobliquetrees.Theyarehighly
interpretable because they use two features at most in each deci-
sion node, unlike oblique trees, which use all or many features.
Comparedtounivariatetrees,bivariatetreesaremuchsmallerbut
signiﬁcantly more accurate. They also can reveal insights about
thedatabyconstructingnew,bivariatefeaturesthatareusefulfor
discrimination; and by providing a form of supervised, hierarchi-
cal 2Dvisualization at each decision node, which reveals patterns
in the data such as clusters or linear structure. To learn a bivari-
ate tree, we have given two algorithms. One is very fast, based
ongreedyrecursivepartitioning.Theotherisslowerbutproduces
muchbettertrees,byusingalternatingoptimizationofthelossand
thenodefeatures’ costgloballyover all thetreeparameters.
Acknowledgments. Work supportedby NSF award IIS–2007147.
Wethank M.Gabidolla forhelp withtheTAO implementation.
 
1344KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
References
[1] DimitrisBertsimasandJackDunn.2017. OptimalClassiﬁcationTrees. Machine
Learning 106, 7(July 2017), 1039–1082.
[2] Jan C. Bioch, Onno van der Meer, and Rob Potharst. 1997. Bivariate Decision
Trees.In Proc.EuropeanSymposiumonPrinciplesofDataMiningandKnowledge
Discovery(PKDD1997) ,J.Komorowskiand J.Zytkow (Eds.). 232–242.
[3] FerdinandBollwein andStephan Westphal.2021. ABranch&BoundAlgorithm
ToDetermineOptimalBivariateSplitsforObliqueDecisionTreeInduction. Ap-
plied Intelligence 51,10 (Oct. 2021),7552–7572.
[4] Leo Breiman.2001. Random Forests. Machine Learning 45, 1(Oct. 2001), 5–32.
[5] Leo J. Breiman, Jerome H. Friedman, R. A. Olshen, and Charles J. Stone. 1984.
Classiﬁcationand RegressionTrees . Wadsworth, Belmont, Calif.
[6] MiguelÁ.Carreira-Perpiñán,MagzhanGabidolla,andArmanZharmagambetov.
2023. TowardsBetterDecisionForests:ForestAlternatingOptimization.In Proc.
ofthe2023IEEEComputerSocietyConf.ComputerVisionandPatternRecognition
(CVPR’23) .Vancouver,Canada,7589–7598.
[7] Miguel Á. Carreira-Perpiñánand SuryabhanSingh Hada. 2021. Counterfactual
Explanations for Oblique Decision Trees: Exact, Eﬃcient Algorithms. In Proc.
of the 35thAAAIConferenceon ArtiﬁcialIntelligence (AAAI2021) .Online, 6903–
6911.
[8] Miguel Á. Carreira-Perpiñán and Pooya Tavallali. 2018. Alternating Optimiza-
tion of Decision Trees, with Application to Learning Sparse Oblique Trees. In
AdvancesinNeuralInformationProcessingSystems(NEURIPS) ,S.Bengio,H.Wal-
lach,H. Larochelle, K.Grauman,N.Cesa-Bianchi,and R.Garnett (Eds.), Vol. 31.
MITPress,Cambridge,MA,1211–1221.
[9] MiguelÁ. Carreira-Perpiñánand ArmanZharmagambetov.2020. Ensemblesof
Bagged TAO Trees Consistently Improve over Random Forests, AdaBoost and
Gradient Boosting. In Proc. of the 2020 ACM-IMS Foundations of Data Science
Conference (FODS2020) .Seattle, WA, 35–46.
[10] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proc. of the 22nd ACM SIGKDD Int. Conf. Knowledge Discovery and
Data Mining(SIGKDD2016) .SanFrancisco,CA, 785–794.
[11] Emir Demirović, Anna Lukina, Emmanuel Hebrard, Jeﬀrey Chan, James Bai-
ley, Christopher Leckie, Kotagiri Ramamohanarao, and Peter J. Stuckey. 2022.
MurTree:OptimalDecisionTreesviaDynamicProgrammingandSearch. J.Ma-
chine Learning Research 23, 26(2022), 1–47.
[12] Magzhan Gabidolla and Miguel Á. Carreira-Perpiñán. 2022. Optimal Inter-
pretable Clustering Using Oblique Decision Trees. In Proc. of the 28th ACM
SIGKDDInt. Conf.Knowledge Discoveryand DataMining(SIGKDD 2022) .Wash-
ington, DC,400–410.
[13] Magzhan Gabidolla and Miguel Á. Carreira-Perpiñán.2022. Pushing the Enve-
lopeofGradientBoostingForestsviaGlobally-OptimizedObliqueTrees.In Proc.
ofthe2022IEEEComputerSocietyConf.ComputerVisionandPatternRecognition
(CVPR’22) .NewOrleans,LA,285–294.
[14] MagzhanGabidolla,ArmanZharmagambetov,andMiguelÁ.Carreira-Perpiñán.
2022. Improved Multiclass AdaBoost Using Sparse Oblique Decision Trees. In
Int.J. Conf.Neural Networks(IJCNN’22) .Padua,Italy.
[15] MagzhanGabidolla,ArmanZharmagambetov,andMiguelÁ.Carreira-Perpiñán.
2024. Beyond the ROCCurve:ClassiﬁcationTreesUsing Cost-Optimal Curves,
with Application to Imbalanced Datasets.In Proc.of the 41th Int. Conf. Machine
Learning (ICML2024) .Vienna,Austria.
[16] Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. 2022. Why Do Tree-
Based Models Still Outperform Deep Learning on Tabular Data?. In Advances
in Neural Information Processing Systems (NEURIPS) , S. Koyejo, S. Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.MITPress,Cambridge,
MA,507–520.
[17] SuryabhanSinghHadaandMiguelÁ.Carreira-Perpiñán.2021. ExploringCoun-
terfactual Explanations for Classiﬁcationand Regression trees. In ECML PKDD
3rdInt.WorkshopandTutorialoneXplainable Knowledge DiscoveryinDataMin-
ing(XKDD 2021) .489–504.
[18] Trevor J. Hastie and Robert J. Tibshirani. 1990. Generalized Additive Models .
Number 43 in Monographs on Statistics and Applied Probability. Chapman &
Hall, London, NewYork.
[19] Trevor J. Hastie, Robert J. Tibshirani, and Jerome H. Friedman. 2009. The Ele-
mentsof StatisticalLearning—Data Mining,Inferenceand Prediction (second ed.).
Springer-Verlag.
[20] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
QiweiYe, and Tie-YanLiu.2017. LightGBM:A Highly Eﬃcient GradientBoost-
ing Decision Tree.In Advances in Neural Information ProcessingSystems (NIPS) ,
I.Guyon, U.v.Luxburg,S.Bengio, H.Wallach,R.Fergus,S.Vishwanathan, and
R.Garnett (Eds.), Vol. 30. MITPress,Cambridge,MA,3146–3154.
[21] M.Lichman.2013. UCIMachineLearningRepository. http://archive.ics.uci.edu/
ml.
[22] Wei-Yin Loh. 2002. Regression Trees with Unbiased Variable Selection and In-
teractionDetection. StatisticaSinica 12,2 (April 2002), 361–386.
[23] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate
Intelligible ModelswithPairwiseInteractions.In Proc.ofthe19thACMSIGKDDInt.Conf.KnowledgeDiscoveryandDataMining(SIGKDD2013) .Chicago,IL,623–
631.
[24] DavidLubinsky.1994. ClassiﬁcationTreeswith BivariateSplits. Applied Intelli-
gence4, 3(July 1994), 283–296.
[25] S. K. Murthy,S.Kasif,and S. Salzberg.1994. A System forInduction of Oblique
DecisionTrees. J.Artiﬁcial Intelligence Research 2 (1994), 1–32.
[26] Sreerama K. Murthy, Simon Kasif, Steven Salzberg, and Richard Beigel. 1993.
OC1:A Randomized Algorithm for Building Oblique Decision Trees.In Proc.of
the11thAAAIConferenceonArtiﬁcialIntelligence(AAAI1993) .Washington,DC,
322–327.
[27] Nina Narodytska,Alexey Ignatiev,Filipe Pereira,and Joao Marques-Silva.2003.
Learning Optimal Decision Trees with SAT. In Proc. of the 18th Int. Joint Conf.
ArtiﬁcialIntelligence (IJCAI’03) .Acapulco, Mexico, 1362–1368.
[28] Siegfried Nijssen and Elisa Fromont. 2010. Optimal Constraint-Based Decision
TreeInduction fromItemsetLattices. DataMiningand Knowledge Discovery 21,
1(July 2010), 9–51.
[29] TimOatesandDavidJensen.1997. TheEﬀectsofTrainingSet Sizeon Decision
TreeComplexity.In Proc.ofthe14thInt.Conf.MachineLearning(ICML’97) ,D.H.
Fisher(Ed.). Nashville,TN,254–262.
[30] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss,VincentDubourg,JakeVanderplas,AlexandrePassos,DavidCournapeau,
MatthieuBrucher,MatthieuPerrot,and ÉdouardDuchesnay.2011. Scikit-learn:
MachineLearninginPython. J.MachineLearningResearch 12(Oct.2011),2825–
2830. Available online athttps://scikit-learn.org.
[31] J.RossQuinlan.1993. C4.5:ProgramsforMachineLearning . MorganKaufmann.
[32] Steﬀen Rendle. 2010. Factorization Machines.In Proc.of the 10thIEEEInt. Conf.
Data Mining(ICDM2010) .Sydney,Australia,995–1000.
[33] Lior Rokachand Oded Maimon.2015. Data Mining With DecisionTrees.Theory
and Applications (second ed.). Number 81 in Series in Machine Perception and
ArtiﬁcialIntelligence. World Scientiﬁc.
[34] Sicco Verwerand YingqianZhang. 2019. LearningOptimalClassiﬁcationTrees
UsingaBinaryLinearProgramFormulation.In Proc.ofthe33rdAAAIConference
on ArtiﬁcialIntelligence (AAAI2019) .Honolulu, HI, 1625–1632.
[35] ArmanZharmagambetovandMiguelÁ.Carreira-Perpiñán.2020. Smaller,More
Accurate Regression Forests Using Tree Alternating Optimization. In Proc. of
the37thInt.Conf.MachineLearning(ICML2020) ,HalDauméIIIandAartiSingh
(Eds.). Online, 11398–11408.
[36] Arman Zharmagambetovand Miguel Á. Carreira-Perpiñán.2022. Learning In-
terpretable,Tree-BasedProjectionMappingsforNonlinearEmbeddings.In Proc.
of the 25th Int. Conf. Artiﬁcial Intelligence and Statistics (AISTATS 2022) . Online,
9550–9570.
[37] Arman Zharmagambetov and Miguel Á. Carreira-Perpiñán. 2022. Semi-
Supervised Learning with Decision Trees: Graph Laplacian Tree Alternating
Optimization. In Advances in Neural Information Processing Systems (NEURIPS) ,
S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.
MIT Press,Cambridge,MA, 2392–2405.
[38] Arman Zharmagambetov, Suryabhan Singh Hada, Magzhan Gabidolla, and
Miguel Á. Carreira-Perpiñán.2021. Non-Greedy Algorithms for Decision Tree
Optimization: An Experimental Comparison. In Int. J. Conf. Neural Networks
(IJCNN’21) .Virtualevent.
0
5 10 15 20 25 30 350246810121416
totalnumber of nodesuni
variate CART
bivariateTAO/u1D438test(%)
Figure9:Testerror(ontheBreastCancerdataset)ofunivari-
ateCARTandbivariateTAOtreesasafunctionofthenum-
ber of nodes (obtained from the regularization path over
theirhyperparameter).
 
1345BivariateDecision Trees: Smaller,Interpretable, MoreAccur ate KDD ’24,August25–29, 2024,Barcelona,Spain
Table 3: Comparison between bivariate, univariate and oblique trees for classiﬁcation. Each tree was selected by cross-
validating its hyperparameter and this was repeated 3 times over random training/validation sets. We report training and
testaccuracy( %±stdev);averagedepth Δ,nodecount /u1D45Bandnumberoffeatures /u1D453pernode(weomit /u1D453=1and2forunivariate
andbivariatetrees,respectively);andaverageruntime(secondsor“timeout”).Weindicatewith color greenbestand bluesec-
ond b
estresultfor testaccuracyandnodecount overthe univariateandbivariatetrees(ignoringthe obliquetrees).
Dataset ( /u1D441tra in,/u1D437,/u1D43E) .............bivariate............. ......univariate...... ..oblique..
TAO CART BiDT CART C5.0 TAO
Breast Cancer (455,30, 2)training (%) 96.04±1.53 99.12±0. 00 96.99±0.1098.61±0.41 98.9±0. 4898.21±0.79
tes
t(%) 98.25±0.4398.00±0 .00 97.66±0. 1094.73±0.00 95.6±0. 6797.71±1.04
Δ/#
nodes//u1D4531/3 3.0/9 1.6/5 4.0/16 5.3/12 3/15/10.3
runtim
e (s) 4 6 2 0.1 0.1 5
Seg
ment (1963,19, 7)training (%) 98.47±0.35 97.30±0. 00 97.58±0.0198.76±0.00 98.9±0. 1099.48±0.21
tes
t(%) 97.41±0.1496.73±0 .14 96.06±0. 1496.01±0.47 96.3±0. 4897.58±1.31
Δ/#
nodes//u1D45311.0/13 11.0/25 9.0/21 15.0/128 12.25/77 8/271/8.5
runtim
e (s) 30 13 27 0.1 0.1 20
Optic
al recog. (3823,62,10)training (%) 97.28±0.36 97.65±0. 01 98.70±0.5098.32±1.18 98.5±0. 1897.68±0.59
tes
t(%) 88.48±0.39 87.03±0. 0288.10±0.3285.32±0.22 86.8±0. 2391.27±1.74
Δ/#
nodes//u1D4539.0/54 14.0/149 16.0/188 15.0/358 13.65/335 7/115.8/15.0
runtim
e (s) 30 233 17 0.3 0.3 10
Spa
mbase (3910,57, 2)training (%) 96.39±0.08 97.49±0. 14 95.34±1.8597.86±2.91 96.16±0. 1496.55±0.47
tes
t(%) 93.34±0.07 92.19±0. 0592.71±0.5392.18±0.31 92.2±0. 4294.31±1.22
Δ/#
nodes//u1D45314/53 10.0/77 16/161 24.7/362 14.7/77 4/30/42.1
runtim
e (s) 120 284 208 0.3 0.3 60
Pag
eblock (4652,10, 5)training (%) 98.39±0.00 98.41±0. 00 99.19±0.4099.74±0.01 97.7±0. 0995.96±0.12
tes
t(%) 97.93±0096.83±0 .01 96.38±0. 0696.52±0.17 96.54±0. 1396.35±0.41
Δ/#
nodes//u1D4538.0/25 18.0/159 4.3/14 18/268 10.05/90 6/39/7.5
runtim
e (s) 30 21 15 0.2 0.1 39
Hou
se 16H (11464,16, 2)training (%) 87.1±1.55 89.45±0. 00 90.42±0.2386.2±0.0 91.98±0. 5586.55±1.10
tes
t(%) 85.6±0.0784.73±0 .0585.6±0 .1783.4±0.0 83.06±0. 3285.47±0.51
Δ/#
nodes//u1D4537/35 10/107 10/115 8/75 15.05/245 4/13/14.9
runtim
e (s) 30 21 15 0.2 0.1 24
Lette
r (16000,16,26)training (%) 100±1.37 100±0. 01 98.40±1.7694.30±0.01 98.66±0. 0795.43±0.29
tes
t(%) 87.25±0.1187.25±0 .00 86.80±0. 3786.04±0.04 86.76±0. 3390.41±0.31
Δ/#
nodes//u1D45335/1314 35.0/2121 37.6/2596 28/3888 16.85/2817 11/2155/8.5
runtim
e (s) 300 73 12∗0.3 0.9 77
Ele
ctricity (32702, 8, 2)training (%) 98.97±2.80 95.80±0. 80 96.14±1.2099.10±0.00 95.04±0. 4398.1±1.8
tes
t(%) 89.38±0.12 86.05±0. 05 87.91±0.0687.80±0.1688 .64±0.4290.23±0.19
Δ/#
nodes//u1D45323.0/1083 24.0/1741 22.3/1881 30.0/6366 17.25/2615 10/249/6.8
runtim
e (s) 300 81 393 0.9 0.9 134
MiniBo
oNE (62048,50, 2)training (%) 92.36±0.00 96.02±0. 02 - 96.61±0.02 95.88±0. 0791.98±0.15
tes
t(%) 91.16±0.0090.68±0 .03 - 90.25±0.03 89.84±0. 1091.43±0.12
Δ/#
nodes//u1D45311.0/105 15/831 - 19.3/2012 15.65/1787 10/133/16.8
runtim
e (s) 1200 1000 timeout 5.2 6.4 3000
SUS
Y (600000,18, 2)training (%) 80.71±0.00 81.35±0. 00 - 81.45±0.00 80.90±0. 0081.10±0.00
tes
t(%) 79.51±0.00 79.01±0. 00 - 78.90±0.0079 .10±0.0080.3±0.00
Δ/#
nodes//u1D45317.0/1077 21/2780 - 24/4389 16.25/3227 12/983
runtim
e (s)≈2h≈1h timeout 40.2 35.2 ≈2h
A Experiment setup
We u
se the CART Python implementation in scikit-learn [30].
We grow the tree to its maximum depth by setting the minsplit
parameterto1andthe ccp alpha complexityparameterto0,and
determine theoptimalpruningparameter onthehold-outset.
Weemployanopen-sourcesingle-threadedLinuxversionofthe
C5.0 implementation in C (https://rulequest.com/download.html).
Tooptimizemodelperformanceforeachdataset,weconductagrid
search on the hold-out set to identify the most suitable parame-
ters. We mainly ﬁne-tune two parameters: -c CF, which governs
thepruningseverity,and -m cases ,whichspeciﬁestheminimum
number of data points required to split a node. It’s worth notingthatwemaintaindefaultconﬁgurationsforallothermodelparam-
eters. Remarkably, our ﬁndings indicate that the tuned parameter
values closelyalign with thedefaultsettings in many instances.
Weuse an open-sourcemultithreaded implementation of BiDT
by [3] written in C++ with Python interface (https://github.com/
fbollwein/OptimizedDecisionTrees). We enable both univari-
ate and bivariate splits. The resulting tree grows fully and is then
prunedeitherusingreducederrorpruningorminimumcost-complexity
pruningusing thehold-outset.
We implement bivariate TAO in C++ with parallel processing
and ﬁne-tune regularization parameters /u1D436and/u1D706. For each experi-
ment we start with /u1D436=1and increase in with a step of 0.25until
the tree becomes fully univariate. Through empirical analysis, it
wasfoundthatalgorithmgeneralizes wellwhen /u1D436fallswithinthe
 
1346KDD ’24,August25–29, 2024,Barcelona,Spain Rasu lKairgeldinandMiguel Á.Carreira-Perpiñán
Table5:FeaturedescriptionofSegmentdataset.Description
provided from the oﬃcial UCI web page. Index column is
usedtoindicatefeaturesinﬁg. 8in themainpaper.
Index Description
/u1D4650thecolumnof thecenter pixel oftheregion
/u1D4651th
erow ofthecenter pixelof theregion
/u1D4652#pixelsin aregion =9
/u1D4653#lines of lowcontrastthat gothrough theregion
/u1D4654#lines of high contrast,greater than5
/u1D4655contrastof horizontallyadjacent pixelsin theregion
/u1D4656standard deviation of prev.feature
/u1D4657contrastof verticallyadjacent pixels
/u1D4658standard deviation of prev.feature
/u1D4659average over theregion of (R+ G + B)/3
/u1D46510average over theregion of theR
/u1D46511average over theregion of theB
/u1D46512average over theregion of theG
/u1D46513excess red:(2R - (G + B))
/u1D46514excess blue:(2B - (G + R))
/u1D46515excess green: (2G - (R+ B))
/u1D46516Valueof HSV
/u1D46517SaturationofHSV
/u1D46518HueofHSV
Table 6: Comparison between bivariate CART and univari-
ate
CART initialization for bivariate TAO. We report train
andtestaccuracy( %,±stdevover3runs),averagedepth Δof
the treeandaveragenodecount.
Dataset ( /u1D441tra in,/u1D437,/u1D43E) bivariate univariate
CARTinit. CARTinit.
Breast Cancer (455,30, 2)training (%) 96.04±1.53 99.12±0. 01
test(%) 98.25±0.43 97.37±0. 12
Δ/#nodes 1/3 3.1/9.5
Seg
ment (1963,19, 7)training (%) 98.47±0.35 99.03±0. 30
test(%) 97.41±0.14 97.12±1. 10
Δ/#nodes 11.0/13 12.0/45.3
Optic
al recog. (3823,62,10)training (%) 97.28±0.36 97.17±0. 05
test(%) 88.48±0.39 87.65±0. 11
Δ/#nodes 14.0/61 11.0/123
Spa
mbase (3910,57, 2)training (%) 96.78±0.08 96.39±0. 08
test(%) 92.32±0.0793 .34±0.07
Δ/#
nodes 11/28 14.0/53
Pag
eblock (4652,10, 5)training (%) 98.39±0.00 98.24±0. 01
test(%) 97.93±00 97.81±0. 01
Δ/#nodes 8.0/25 5.0/23
House 16H (11464,16, 2)training (%) 91.42±1.55 87.1±1. 55
test(%) 84.93±0.0785 .6±0.07
Δ/#
nodes 11/83 7/35
Letter (16000,16,26)training (%) 98.3±1.37 97.25±1. 37
test(%) 87.25±0.11 86.99±0. 11
Δ/#nodes 35/1314 28/2005
Ele
ctricity (32702, 8, 2)training (%) 98.97±2.80 96.80±0. 80
test(%) 89.38±0.12 87.05±0. 05
Δ/#nodes 23.0/1083 24.0/1141
MiniBo
oNE (62048,50, 2)training (%) 94.72±0.00 92.36±0. 00
test(%) 91.31±0.00 91.16±0. 00
Δ/#nodes 13.0/225 11/105Table4:ListoffeaturesofBreastCancerdataset.Description
p
rovided from the oﬃcial UCI web page. Index column is
usedtoindicatefeaturesin ﬁg.7in themain paper.
radius1 radius2 radius3
tex
ture1 texture2 texture3
per
imeter1 perimeter2 perimeter3
are
a1 area2 area3
smoo
thness1 smoothness2 smoothness3
comp
actness1 compactness2 compactness3
con
cavity1 concavity2 concavity3
con
cave points1 concavepoints2 concave points3
symme
try1 symmetry2 symmetry3
fra
ctaldimension1 fractal dimension2 fractaldimension3
rangeof1and2.5acrossmostdatasets.Sincerunningalgorithm for
eachvalueof /u1D706iscomputationallycostlyweperformcoarsesearch
withﬁnerstepscloserto0.Asitwasdiscussedinsection3.2inthe
mainpaper,everynextvalueof /u1D706canbecalculatedforgiven /u1D436.We
generate a ﬁxed subset of line inclinations /u1D43Buniformly between
0 and 180 degrees and for most experiments /u1D43Bis set between 30
and90. We use save values of /u1D43Bfor bivariate CART. The initial
treestructureis obtained fromfullygrownCART tree(univariate
orbivariate).
We run our experiments on datasets from UCI ML repository
[21] and [16]. For datasets that does not have separate test set we
randomlysubsample20 %ofthewholedatasetfortesting. Weran-
domlyselect 10 %of samples as hold-outset for validation. For ex-
periments, wereportaverage over 3 runs with stdev for trainand
test accuracy. We set a timeout of 2 hours for all algorithms on
all datasets except for SUSY ( ≈600k samples), for which it was in-
creasedto4hours.OurhardwaresetupisIntelXeonCPUE5-2699
v3@ 2.30GHzwith256GB RAM.
B Additional experimental results
Table6presentsresultsofdiﬀerentinitializationforbivariateTAO.
We use initial tree structureproducedby univariate and bivariate
CART proposed in section 3.4. Initializing from bivariate CART
generally results in smaller and better performing ﬁnal trees. In
the main paper we use both initializations and pick the best one
onthevalidationset.
 
1347