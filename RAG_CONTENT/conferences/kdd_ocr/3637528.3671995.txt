Quantifying and Estimating the Predictability Upper Bound of
Univariate Numeric Time Series
Jamal Mohammed
Universiy of Zurich
Zurich, Switzerland
mjamal@ifi.uzh.chMichael H.B√∂hlen
Universiy of Zurich
Zurich, Switzerland
boehlen@ifi.uzh.chSven Helmer
Universiy of Zurich
Zurich, Switzerland
helmer@ifi.uzh.ch
ABSTRACT
The intrinsic predictability of a given time series indicates how well
an (ideal) algorithm could potentially predict it when trained on the
time series data. Being able to compute the intrinsic predictability
helps the developers of prediction algorithms immensely in decid-
ing whether there is further optimization potential, as it tells them
how close they are to what is (theoretically) achievable. We call
the intrinsic predictability the predictability upper bound Œ†ùëöùëéùë•
and propose a novel method for quantifying and estimating it for
univariate numeric time series. So far, this has only been done for
symbolic time series, even though most real-world time series are
numeric by nature. We base our technique on the close relationship
between entropy and predictability, utilizing the entropy rate of a
time series to compute Œ†ùëöùëéùë•. Since existing entropy rate estima-
tors, such as those based on the Lempel-Ziv compression algorithm,
only work for symbolic data, we develop new estimators using
tolerance thresholds for matching numeric values. We demonstrate
thatŒ†ùëöùëéùë•is an effective upper bound that characterizes the in-
trinsic predictability of a time series. We give formal proofs and
we validate our arguments experimentally by comparing Œ†ùëöùëéùë•
with the prediction accuracy of different state-of-the-art models on
various real-world datasets from different domains.
CCS CONCEPTS
‚Ä¢Mathematics of computing ‚ÜíTime series analysis ;Infor-
mation theory.
KEYWORDS
Prediction, Information Theory, Time series
ACM Reference Format:
Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer. 2024. Quantifying
and Estimating the Predictability Upper Bound of Univariate Numeric Time
Series. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671995
1 INTRODUCTION
Predicting (or forecasting) future events from the past plays an
important role in many application fields, e.g. sales and weather
forecasts, portfolio management, and prediction of (human) mobil-
ity. Our goal is to quantify the limits of predictability of a given time
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671995series, i.e., providing an upper bound for the ratio of potentially
achievable correct predictions to all predictions for a particular
dataset. When computing this upper bound, we only rely on the
intrinsic properties of the data without utilizing any additional
information. That is why this upper bound is also called intrinsic
predictability [19], in contrast to realized predictability, which only
tells us about the performance of specific prediction algorithms.
Intrinsic predictability is a powerful tool when it comes to judging
whether we have poor realized predictability due to a suboptimal
prediction algorithm/model or because the time series is generally
hard to predict [ 24]. For instance, a small gap between the upper
bound and the currently used prediction algorithm indicates that
there is little room for improvement and that it is unlikely that we
can find a better algorithm. Additionally, intrinsic predictability can
be utilized to benchmark prediction algorithms against an absolute
measure rather than relatively against each other.
We base our technique on the entropy rate, which has been
shown to be suitable for measuring how difficult it is to predict
the next value in a time series [ 7]. The notion that entropy and
predictability are related (i.e., the higher the entropy, the lower
the predictability and vice versa) has been around for a long time.
While many different variants of entropy have been proposed for
measuring the complexity of data, e.g. approximate entropy [ 25],
sample entropy [ 26], or permutation entropy [ 3], none of these
methods have been used to quantify the predictability. Most of
the studies investigating these entropies show various rates of
correlation between entropy and predictability or metrics such as
the mean squared error [ 5,6]. To the best of our knowledge, Song
et al. [ 31] were the first to propose a method to compute a concrete
number: the maximum predictability Œ†max, which serves as an
upper bound for the predictability. However, their approach only
works for symbolic time series; they map location information to a
fixed alphabet. Many real-world time series are numeric, though.
We show that discretizing numeric values in a straightforward way
(as, for instance, done by Smith et al. [ 30]) does not work well, up to
the point of Œ†maxceasing to be an upper bound for predictability.
We propose a new way to compute Œ†max, avoiding discretization
by introducing a tolerance ùúñ. Two values ùë•andùë¶are considered
equal if their distance is within the tolerance ùúñ:|ùë•‚àíùë¶|‚â§ùúñ. This
preserves proximity relations between numeric values much better
than the original discretization approach and leads to more accurate
values for Œ†max. Note that a tolerance also affects the implementa-
tion of the algorithm to compute Œ†max. We show how to calculate
Œ†maxusing a novel numeric Lempel-Ziv algorithm. In summary,
our main contributions are
 
2236
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
‚Ä¢We propose a new upper bound for the predictability of
numeric time series given its entropy rate and a tolerance ùúñ,
and prove the correctness of our approach.
‚Ä¢We implement a novel algorithm for estimating the entropy
rate of a numeric time series.
‚Ä¢In an experimental evaluation we demonstrate the effective-
ness of our technique.
2 RELATED WORK
The related work section is divided into two parts. First, we have
a look at entropy-based approaches for computing or estimating
the predictability of time series. In the second part, we review
techniques for implementing approaches to estimate entropy rates.
Entropy is a well-known concept applied to estimating pre-
dictability [ 7]. Song et al. propose an approach based on the entropy
rate of a (human mobility) time series to compute an upper bound
of predictability [ 31]. Although criticized by Smith et al. [ 30] and
Xu et al. [ 34] for using a coarse spatial granularity resulting in a
loose upper bound, in principle the approach by Song et al. works.
However, it can only be applied to symbolic time series data. This
is one of the reasons why a relatively coarse granularity was used:
mapping spatial areas to symbols loses the spatial relationships be-
tween the different areas. Generalizing an entropy-based approach
from a discrete domain to a continuous domain is far from trivial.
As we will show later, the naive discretization applied by Smith et
al. causes problems in terms of the accuracy. The straightforward
approach of replacing the sum in the formula for entropy with
an integral (as originally suggested by Shannon [ 27]) leads to the
so-called differential entropy [4,7], which has several issues, such as
easily becoming infinite for continuous probability distributions [ 2]
and not being invariant under coordinate transformations [4].
Since the seminal work of Shannon [ 27,28], many different
notions of entropy, such as approximate entropy[ 25], fuzzy en-
tropy [ 5], sample entropy[ 26], and permutation entropy[ 3], have
been proposed. An advantage of these entropy types is that they can
be applied to continuous domains. For instance, Chen et al. [ 5] have
estimated the predictability of numerical time series with fuzzy
entropy, approximate entropy, and sample entropy, while Garland
et al [ 10] have done so with permutation entropy. Garland et al.
demonstrate empirically that their weighted permutation entropy
(WPE) is correlated to the mean absolute scaled error (MASE). The
method is limited to correlation, though, as WPE and MASE are two
completely different measures and there is no direct connection
between the two. This lack of being directly comparable with en-
tropy is a general issue with measures for forecast accuracy, such as
MASE (even though MASE is widely considered to be the preferred
measure [ 15]). We are not aware of any method to compute an
upper bound of the best achievable forecast accuracy for a given
numerical time series. Summarizing the entropy-based state-of-the-
art methods, we can say that on the one hand, there are techniques
that compute an upper bound for the intrinsic predictability of a
time series but only work for discrete time series [ 31]. On the other
hand, there are approaches that can be applied to numeric time
series, but which only establish a correlation between forecasting
accuracy and entropy and do not provide an upper bound for theintrinsic predictability. With our method, we can compute an upper
bound for the intrinsic predictability for numeric time series.
We now turn to the second part of the related work section,
implementing the entropy rate computation of a time series. First
of all, we point out that calculating the exact value of the entropy
rate is usually not possible, as the exact probability distribution
of the underlying generating process is not known (we assume
that we are only given the observed values). Even if we had ac-
cess to the concrete probabilities, it would still be infeasible to
compute the exact entropy rate, since this has high runtime costs.
Consequently, the entropy rate of a time series is usually estimated
rather than computed exactly. For an overview and a discussion,
see [ 9], in which Feutrill distinguishes between parametric estima-
tors, which assume that the data generation follows a certain model,
such as a Gaussian process [ 7] or a Markov process [ 7], and non-
parametric ones, which do not make this assumption. We disregard
parametric estimators, as we do not make any assumptions about
the underlying model. For non-parametric estimators, such as the
Lempel-Ziv-based algorithm [ 35], the close relationship between
entropy and data compression algorithms is exploited. Intuitively,
the entropy rate measures how many bits of novel information are
introduced by each new sample emitted from a source. When using
a compression algorithm on these samples, the compression rate
will be higher if less novel information is encountered and vice
versa. The Lempel-Ziv (LZ) algorithm is of particular interest, as it
is auniversal source code for ergodic, stationary sources. A code for
a sourceùëã(e.g. a random variable) is a mapping from the domain
ofùëãto a set of symbols. For a universal source code, the limit of the
expected code length (for the number of samples going to infinity)
reaches the entropy rate and the mapping is independent of the
particular source [ 1]. The original LZ algorithm is only suitable for
symbolic data, though. We implement an entropy estimator based
on the Lempel-Ziv algorithm for numeric time series.
3 PRELIMINARIES
Before we begin describing our approach, we introduce some basic
definitions of time series and entropy-related concepts.
Definition 1. Atime series T is a time-ordered sequence ùëá=
(ùë•1,ùë•2,...,ùë•ùëõ), whereùë•ùë°is the value at time point ùë°and n is the
length of time series ùëá.
Definition 2. Atime series data generating process is de-
fined via a stochastic process ùëã, which consists of a sequence of ran-
dom variables ùëã1,ùëã2,...,ùëãùëõ. The index refers to the point in time,
i.e.,ùëãùë°is the random variable for time point ùë°.
For the moment, we make the following assumption on the time
seriesùëáand the stochastic process ùëã:
‚Ä¢Discrete, equi-spaced intervals of time: for ùë°‚àà{1,2,...,ùëõ}
the delta between any consecutive observations ùë•ùë°andùë•ùë°+1
is constant; so, ùëãis a discrete time stochastic process.
‚Ä¢Boundedness of values in ùëá:ùë•ùë°‚àà[ùëöùëñùëõ(ùëá),ùëöùëéùë•(ùëá)].
‚Ä¢Stationarity of ùëã: the statistical properties of ùëádo not change
over time. More formally, the probabilistic behavior of a
sequence(ùë•ùëñ,ùë•ùëñ+1,...,ùë•ùëñ+ùëò)is the same as that of a shifted
sequence(ùë•ùëñ+‚Ñé,ùë•ùëñ+‚Ñé+1,...,ùë•ùëñ+‚Ñé+ùëò)[29].
 
2237Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
‚Ä¢Ergodicity of ùëã: the average over ensembles and time are
the same (ensembles are essentially a collection of possible
states). For our case, this means that the average over po-
tential subsequences (for a time period) is the same as the
average over a single very long time period [23].
As we cover numeric time series, the random variables ùëãùë°can
be interpreted as continuous random variables. The definitions
of entropy below assume discrete random variables, though. We
resolve this seeming contradiction in Sections 5.1 and 5.2.
Definition 3. Entropy measures the information content, quan-
tifying the uncertainty of a random variable. For random variable ùëå
over domain ùúô, the entropy is defined as:
ùêª(ùëå)=‚àí√ï
ùë¶‚ààùúôùëù(ùë¶)log2(ùëù(ùë¶)) (1)
ùëù(ùë¶)is the probability that ùëåtakes valueùë¶‚ààùúô. Entropy is usually
measured in bits, thus we use the logarithm to the base 2.
Entropy is always greater or equal to 0 (assuming that 0log2(0)=
0) and is maximized for a uniformly distributed random variable.
Additionally, entropy always refers to a single random variable.
As a time series is defined by a stochastic process made up of a
sequence of random variables ùëã1,ùëã2,...,ùëãùëõ, we need to modify
how to measure the uncertainty of the process. Instead of using
entropy, we use the entropy rate, which intuitively measures the
average uncertainty per symbol in the generated time series.
Definition 4. Theentropy rateHof a stochastic process ùëã
consisting of the random variables ùëã1,ùëã2,...,ùëãùëõis computed with
the help of conditional entropy and the limit as ùëõtends to infinity:
H(ùëã)=limùëõ‚Üí‚àû1
ùëõùëõ√ï
ùë°=1ùêª(ùëãùë°|ùëãùë°‚àí1...ùëã1) (2)
We should mention that the limit may not exist if the generating
processùëãis non-stationary or non-ergodic. Moreover, for t=1 the
above equation is reduced to ùêª(ùëã1)[7].
Definition 5. Theconditional entropy measures the amount
of information provided by the outcome of a random variable ùëågiven
the value of another random variable ùëç.
ùêª(ùëå|ùëç)=√ï
ùëß‚ààùúÅùëù(ùëß)ùêª(ùëå|ùëç=ùëß)
=‚àí√ï
ùëß‚ààùúÅ√ï
ùë¶‚ààùúôùëù(ùë¶,ùëß)log2ùëù(ùë¶|ùëß)(3)
In our proofs, we use the following theorems on conditional
entropy. The proofs of these theorems can be found in [ 7], along
with more details about information theory and entropy.
Theorem 1. Chain rule for joint and conditional entropies:
ùêª(ùëã,ùëå)=ùêª(ùëã)+ùêª(ùëå|ùëã) (4)
Corollary 2. A corollary of this rule is
ùêª(ùëã,ùëå|ùëç)=ùêª(ùëã|ùëç)+ùêª(ùëå|ùëã,ùëç) (5)
Theorem 3. Conditionality reduces entropy:
ùêª(ùëã|ùëå)‚â§ùêª(ùëã) (6)4 CURRENT CHALLENGES
While Song et al. managed to connect predictability directly to en-
tropy, they sidestepped the whole issue of symbolic versus numeric
data by assuming that the data is already discretized [ 31]. Smith et
al. continued where Song et al. had left off and explicitly discretized
the geolocation data by placing a grid on top of maps [ 30]. The
numerical value domain was essentially divided into bins and all
values contained in a bin were assigned the same symbol.
Figure 1: Binning time series values
Although binning makes it easy to determine whether a predic-
tion is considered acceptable or not (two values sharing the same
symbol match), there are still problems regarding proximity. For
example, given a value of 6 and two predictions 5 and 8, intuitively
5 is a better prediction, since it is closer. However, binning rarely
captures the proximity correctly. In Figure 1, we have a bin width of
2.4, but for this bin configuration, 8 would be considered a matching
prediction, while 5 would not be. Assuming that two values are
considered close enough when they are within distance 1.2 of each
other, we have a total of 78 potentially matching pairs in Figure 1,
25 of which are within range of each other. However, using the bin-
ning depicted in the figure with dashed lines, we have 24 matching
pairs, 7 of which are false positives, while missing 8 false negatives.
Figure 2: Proximity Issues due to binning
This is not only counterintuitive, but also leads to scenarios in
which Œ†ùëöùëéùë•ceases to be an upper bound. We generated a synthetic
series of integers using a first order Markov model with a total of 50
unique values and trained a model on this data. Then we computed
Œ†ùëöùëéùë•for the time series, discretizing the values using different bin
widths. Figure 2 shows the results. Not only does the model (labeled
Pimarkov) have predictability better than the upper bound (labeled
Pimax), the upper bound does not grow monotonically with the
bin width (we would expect higher predictability for wider bins,
due to a decrease in entropy), but it fluctuates depending on where
the bounds of the bins are located. This observation highlights the
failure of numeric value discretization as a method for computing
the predictability upper bound of numeric sequences.
 
2238KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
5 OUR APPROACH
Our goal is to develop a method considering the proximity of values,
allowing us to compute the intrinsic predictability of a time series
directly from its entropy rate. We utilize a similarity measure, i.e.,
we compare the time series consisting of the predicted values to
the actually measured time series by determining their similarity.
5.1 Defining Predictability
First, we need to determine whether a predicted value matches
the actually measured one. (We assume a forecast horizon of one,
i.e., given the measured values so far, we predict the next value.)
After predicting a value, we get access to the next true value, which
can then be used for the following prediction. We first define pre-
dictability formally and come back to its implementation later.
Definition 6. Given the stochastic process ùëã=(ùëã1,ùëã2,...,ùëãùëõ)
generating the time series ùëá, let ÀÜùëã=(ÀÜùëã1,ÀÜùëã2,..., ÀÜùëãùëõ)be the process
generating the predictions. Given a predefined threshold ùúñ, we say that
there is a (prediction) error , defined by the discrete random variable
ùê∏ùë°, at timestamp ùë°, if
ùê∏ùë°=(
1|ùë•ùë°‚àíÀÜùë•ùë°|>ùúñ
0ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí(7)
In the following, when we state that ùëãùë°‚âàùúñÀÜùëãùë°, we mean that|ùë•ùë°‚àí
ÀÜùë•ùë°|‚â§ùúñ(thus,ùëãùë°Ã∏‚âàùúñÀÜùëãùë°means|ùë•ùë°‚àíÀÜùë•ùë°|>ùúñ). Applying‚âàùúñto a se-
quence of values means that ùëãùëñ,ùëãùëñ+1,...,ùëãùëñ+ùëô‚âàùúñÀÜùëãùëñ,ÀÜùëãùëñ+1,..., ÀÜùëãùëñ+ùëô
is true, iff√ìùëô
ùëó=0(ùëãùëñ+ùëó‚âàùúñÀÜùëãùëñ+ùëó)is true.
Compared to binning, this is a subtle but crucial difference. Us-
ing a tolerance threshold for the distance between values avoids
misclassifications: all pairs within distance ùúñwill be matched, while
those that are further apart will not. We use the definition of a pre-
diction error to define the predictability of a given model (described
by its stochastic process ÀÜùëã) for a particular time series.
Definition 7. We distinguish different levels of predictability.
‚Ä¢The predictability Œ†ÀÜùëã
ùë°ofÀÜùëãfor a single step ùë°is equal to
Œ†ÀÜùëã
ùë°=ùëù(ùê∏ùë°=0)=1‚àíùëù(ùê∏ùë°=1) (8)
‚Ä¢Calculating the average predictability for the process up to
timestampùëõboils down to
Œ†ÀÜùëã(ùëõ)=1
ùëõùëõ√ï
ùë°=1Œ†ÀÜùëã
ùë° (9)
‚Ä¢The overall predictability of ÀÜùëãis the limit of Equation (10)
withùëõtending to infinity:
Œ†ÀÜùëã=limùëõ‚Üí‚àû1
ùëõùëõ√ï
ùë°=1Œ†ÀÜùëã
ùë° (10)
An open question remains when it comes to applying Equa-
tion(7)to determine whether two values match or not: what value
should we set ùúñto? When using it in the context of a similarity
measure, the time series are normalized to obtain consistent mea-
surements [ 11]. Morse and Patel recommend using one quarter
of the standard deviation of the normalized time series for ùúñ[22].
While we could do this as well, we think a user should set thevalue ofùúñ. This makes a lot of sense for numerical time series, as
there may be requirements from the application side concerning
an acceptable margin of error of predictions, i.e., which distance of
a prediction to the actual value can still be tolerated. Essentially,
our approach determines the intrinsic predictability of a time series
for a given margin of error and allows a user to investigate the
tradeoffs between predictability and different margins of error.
5.2 Upper Bound for Predictability
First, we look at the uncertainty introduced by predicting time
series values. More precisely, given the prediction ÀÜùëãùë°for timestamp
ùë°, how much information do we need to determine the actual value
ùëãùë°? In fact, we also have to figure out if ÀÜùëãùë°is a correct value or
if we have made a prediction error. More formally, we have to
determine the conditional entropy ùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°). Applying the chain
rule (Equation (5)), we get
ùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°)=ùêª(ùê∏ùë°|ÀÜùëãùë°)+ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°) (11)
Intuitively, ùêª(ùê∏ùë°|ÀÜùëãùë°)represents the information needed to com-
municate that an error was made, while ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°)stands for
the information needed to determine ùëãùë°given the prediction ÀÜùëãùë°
and the knowledge that an error occurred at time ùë°. In its current
form, Equation (11)is difficult, if not impossible, to compute. We
would need access to a (large) number of joint and conditional
probabilities and, more importantly, the random variables ùëãùë°and
ÀÜùëãùë°are, strictly speaking, continuous. Nevertheless, we can provide
an upper bound, essentially treating ùëãùë°and ÀÜùëãùë°as discrete random
variables covering a range of values in the continuous domain.
First, we look at ùêª(ùê∏ùë°|ÀÜùëãùë°). Since conditionality reduces entropy
(Equation (6)), we know that
ùêª(ùê∏ùë°|ÀÜùëãùë°) ‚â§ùêª(ùê∏ùë°) (12)
=‚àíùëù(ùê∏ùë°=0)log2ùëù(ùê∏ùë°=0)‚àíùëù(ùê∏ùë°=1)log2ùëù(ùê∏ùë°=1)
ùê∏ùë°is a discrete random variable, so we are able to apply Equation (1).
We distinguish two cases for the right-most part of Equation (11):
ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°)=ùëù(ùê∏ùë°=0)ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=0)+
ùëù(ùê∏ùë°=1)ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=1)
=ùëù(ùê∏ùë°=1)ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=1) (13)
The first part for ùëù(ùê∏ùë°=0)drops out, because if we make a
correct prediction, then ùëãùë°‚âàùúñÀÜùëãùë°and we do not need any ad-
ditional information to determine the correct value, which means
ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=0)is equal toùêª(ùëãùë°|ùëãùë°)=ùêª(ÀÜùëãùë°|ÀÜùëãùë°), which is equal
to zero.
That leaves us with the second part ùëù(ùê∏ùë°=1)ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=1).
We know that we made an error, so ùëãùë°Ã∏‚âàùúñÀÜùëãùë°, which, according to
Definition 6, means that the difference between the predicted value
ÀÜùë•ùë°and the correct value ùë•ùë°is greater than ùúñ. We divide the range
of all possible values into sub-intervals of width ùúñstarting from ÀÜùë•ùë°.
Figure 3 illustrates this: the correct value ùë•ùë°will not be found in
the gray sub-intervals (otherwise, ùë•ùë°‚âàùúñÀÜùë•ùë°would be true), but in
one of the other intervals.
We now have to determine the number of sub-intervals. Assume
that we know the smallest and largest theoretically possible value
of the domain of ùëãùë°, denoted by ùë•minandùë•max, respectively. If we
handle predictions ÀÜùëãùë°falling outside of the range as being equal to
 
2239Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
... ...ÀÜùë•ùë°
| 
  {z   }|   {z   }
ùúñùúñ| 
  {z   } |   {z   }
ùúñ ùúñ| 
  {z   } |   {z   }
ùúñ ùúñ
Figur
e 3: Making an incorrect prediction
ùë•minorùë•max, then we also guarantee that ÀÜùëãùë°has the same domain
asùëãùë°. An upper bound for the maximum number ùëÅof distinct
intervals of width ùúñ(in the range[ùë•min,ùë•max]) is
ùëÅ=ùë•max+ùúñ‚àí(ùë•min‚àíùúñ)
ùúñ(14)
Consequently, knowing that we have ùëÅintervals and that the
twoùúñ-intervals neighboring ÀÜùë•ùë°are blocked, ùëãùë°has to fall into one of
the other (of at most) ùëÅ‚àí2ranges. The entropy becomes maximal
for a uniform distribution, so we can determine an upper bound:
ùêª(ùëãùë°|ÀÜùëãùë°,ùê∏ùë°=1)‚â§ùëÅ‚àí2√ï
ùëñ=11
ùëÅ‚àí2log21
ùëÅ‚àí2=log2(ùëÅ‚àí2) (15)
Finally, we connect the entropy ùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°)to the entropy rate
H(ùëã)of the time series. Since the prediction model uses the past
observations to make a prediction, i.e., ÀÜùëãùë°=ùëì(ùëã1,ùëã2,...,ùëãùë°‚àí1),
ùêª(ùëãùë°|ÀÜùëãùë°)=ùêª(ùëãùë°|ùëãùë°‚àí1,...,ùëã 1). Applying the chain rule differ-
ently toùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°), we obtain
ùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°)=ùêª(ùëãùë°|ÀÜùëãùë°)+ùêª(ùê∏ùë°|ùëãùë°,ÀÜùëãùë°) (16)
However, the entropy ùêª(ùê∏ùë°|ùëãùë°,ÀÜùëãùë°)is equal to zero, since know-
ingùëãùë°and ÀÜùëãùë°, we can directly deduce ùê∏ùë°without any additional
information, so
ùêª(ùê∏ùë°,ùëãùë°|ÀÜùëãùë°)=ùêª(ùëãùë°|ÀÜùëãùë°)=ùêª(ùëãùë°|ùëãùë°‚àí1,...,ùëã 1) (17)
Computing the average entropy ùêª(ùëã|ÀÜùëã)of predicting the whole
time series then boils down to
ùêª(ùëã|ÀÜùëã)=1
ùëõùëõ√ï
ùë°=1ùêª(ùëãùë°|ùëãùë°‚àí1,...,ùëã 1) (18)
This expression is equivalent to the one found in the entropy rate
(cf. Definition 4). Combining all the previous results (Equations (11),
(12), (13), (15), and (18)) yields:
H(ùëã)‚â§‚àíŒ†ÀÜùëãlog2Œ†ÀÜùëã‚àí(1‚àíŒ†ÀÜùëã)
log2(1‚àíŒ†ÀÜùëã)‚àílog2(ùëÅ‚àí2)
(19)
This means, we have now connected the entropy rate of a time
series directly with its predictability. The inequality in Equation (19)
holds for an arbitrary prediction model ÀÜùëãùë°. We are interested in an
upper bound for the predictability, which means finding the largest
possible value for ÀÜùëãùë°satisfying (19). We denote this upper bound
byŒ†maxand compute its value by determining the entropy rate
H(ùëã)of the time series and then (numerically) solving the equation
H(ùëã)=‚àíŒ†maxlog2Œ†max‚àí(1‚àíŒ†max)(log2(1‚àíŒ†max)+log2(ùëÅ‚àí
2))forŒ†max. Any other value for Œ†ÀÜùëãwill result in a higher entropy
and therefore a lower predictability. How to determine the entropy
rate of a time series is the topic of the following section.6 ENTROPY RATE ESTIMATION
Equation (19)makes it clear that being able to accurately estimate
the entropy rate is crucial for our approach. As already indicated,
there are numerous different ways to estimate the entropy rate of a
time series. Many of these methods converge to the true entropy
rate when the length ùëõof the time series goes to infinity. In this
case, the differences between those estimators do not play a role, as
their limit is always the true entropy rate. However, when applying
different entropy rate estimators to finite time series, the differences
do matter. Surprisingly, work on entropy rate estimation for finite
time series is few and far between. The most detailed (empirical)
study we found was by Lesne et al. [ 17]. The authors investigate
two block entropy and two Lempel-Ziv variants. However, as it is
not possible to generalize block entropy estimators to numerical
data, we focus on Lempel-Ziv (LZ) estimators.
Approaches based on Lempel-Ziv use the compressibility of a
time series to estimate the entropy rate. This is done by parsing
the time series and partitioning it into a set of distinct words. For
the computation of the entropy rate, we are particularly interested
in the number of words created by the parsing: the higher/lower
the number of partitions, the higher/lower the entropy. Two dif-
ferent parsing approaches have been proposed, which, for sake of
simplicity, we call LZ1 and LZ2.
6.1 Numeric Lempel-Ziv 1 (NLZ1)
LZ1 is based on the 1978 paper by Lempel and Ziv [ 36]. Cover and
Thomas call this a tree-structured Lempel-Ziv algorithm, as it can
be viewed as building a (tree-structured) dictionary [ 7]. Essentially,
the parsing searches for the shortest prefix of the part of the time
series not parsed yet that is not in the dictionary. For symbolic
data, matching values during the parsing is straightforward: we can
compare the values directly. For numeric data, we have to use our
comparison operator ‚âàùúñ. Algorithm 1 shows pseudocode for the
numeric LZ1 estimator (NLZ1), returning the number of partitions;
ùë§[ùëñ,ùëó]denotes the subsequence ùë•ùëñ,ùë•ùëñ+1,...,ùë•ùëóof time series ùëá(if
ùëñ>ùëó, thenùë§[ùëñ,ùëó]is the empty word).
Algorithm 1: NLZ1(ùëá=(ùë•1,ùë•2,...,ùë•ùëõ))
1ùëñ‚Üê1,ùê∑‚Üê‚àÖ;
2whileùëñ‚â§ùëõdo
3 find the smallest ùëó‚â•ùëñsuch that there is a word
ùë§[ùëé,ùëè]‚ààùê∑withùë§[ùëñ,ùëó‚àí1]‚âàùúñùë§[ùëé,ùëè]and no word
ùë§[ùëé,ùëê]‚ààùê∑withùë§[ùëñ,ùëó]‚âàùúñùë§[ùëé,ùëê];
4ùê∑‚Üêùê∑‚à™{ùë§[ùëñ,ùëó]};
5ùëñ‚Üêùëó+1;
6end
7return|ùê∑|
We now have to convert the number of words, ùëê(ùëõ)=|ùê∑|, returned
by the algorithm above for time series ùëáwith length ùëõinto an
entropy rate. Cover and Thomas [7] show that
Hùê∂(ùëá)=ùëê(ùëõ)(log2(ùëê(ùëõ))+1)
ùëõ‚ÜíH(ùëã) (20)
forùëõ‚Üí‚àû for a stationary ergodic sequence (their proof is based
on a proof by Wyner and Ziv [33]).
 
2240KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
When implementing NLZ1, the most expensive part is finding
a matching word in the dictionary. For symbolic data, this is just
a lookup in a hash table. However, due to using the more com-
plicated comparison operator ‚âàùúñ, we cannot just do a hash table
lookup. Clearly, we do not want to iterate through all the keys in
the dictionary one by one. Consequently, we divide the dictionary
into sub-dictionaries: one for each length, i.e., we have one sub-
dictionary for words of length one, one for sequences of length
two, and so on. In this way, we only have to look at a subset of
the words stored in the dictionary. We provide details about the
implementation and complexity in the appendix.
6.2 Numeric Lempel-Ziv 2 (NLZ2)
LZ2 appears in a paper by Wyner and Ziv[ 32] and looks for the
shortest prefix of the part of the time series not parsed yet that has
not been encountered anywhere in the parsed subsequence. Since
we use the estimator by Kontoyiannis et al. [ 16], we compute the
lengths of all words for every starting position within ùëá. Again,
we have to make changes to be able to deal with numerical data,
utilizing the operator ‚âàùúñ. Algorithm 2 shows pseudocode for NLZ2.
Algorithm 2: NLZ2(ùëá=(ùë•1,ùë•2,...,ùë•ùëõ))
1ùëñ‚Üê1, allocate list Œõ‚Üê(ùúÜ1,ùúÜ2,...,ùúÜùëõ);
2whileùëñ‚â§ùëõdo
3 find the smallest ùëó‚â•ùëñsuch that there is a word
ùë§[ùëé,ùëè]‚àà(ùë•1,ùë•2,...,ùë•ùëñ‚àí1)withùë§[ùëñ,ùëó‚àí1]‚âàùúñùë§[ùëé,ùëè]
and no word ùë§[ùëé,ùëê]‚àà(ùë•1,ùë•2,...,ùë•ùëñ‚àí1)with
ùë§[ùëñ,ùëó]‚âàùúñùë§[ùëé,ùëê];
4ùúÜùëñ‚Üêùëó‚àíùëñ+1;
5ùëñ++;
6end
7return Œõ
Again, we have to turn the returned value into an entropy rate.
Kontoyiannis et al. use the following formula for doing so:
Hùêæ(ùëá)=log2(ùëõ)
1
ùëõ√çùëõ
ùëñ=1ùúÜùëñ(21)
They show that for ùëõ‚Üí‚àû ,Hùêæ(ùëá)converges toH(ùëã)for station-
ary ergodic processes. The motivation for computing ùúÜùëñfor every
position from 1 to ùëõand averaging the values is to decrease the
variance.
In NLZ2, we have to find a matching word in the part of the
time series we already parsed, which means doing threshold-based
comparisons again. In a first step, we build an inverted index for
faster lookup of matching values. For this purpose, we traverse the
time seriesùëáand for each distinct value we find, we create a list of
positions where this value is found in ùëáand store the information
in a dictionary. In a final step, we create a sorted array of all the
keys in the dictionary that acts as a directory for the inverted index.
After building the index, we traverse the time series value by
value and look up all previously parsed positions that match the cur-
rent value. For each of these positions, we find the longest matching
subsequence (up to the current position). The maximum length of
all these subsequences plus one (because we are looking for theshortest unseen subsequence) becomes the value of the current ùúÜ.
Once we have determined the values of all ùúÜùëñinŒõ, we can estimate
the entropy rate by using Equation (21).
Finally, we describe how lookups in the index work. In a first step,
we have to find all values in the directory of the index that match
the current value in the time series. Since the entries in the directory
are sorted, we use binary search to find the first matching value
(and then traverse back and forth from there). For every matching
value, we look up the positions in the dictionary and return those
that have already been parsed. Further details are provided in the
appendix.
7 EXPERIMENTAL EVALUATION
We run several experiments to evaluate our approach empirically.1
The first experiments are conducted in an ideal setting, i.e., first, the
time series satisfy all the assumptions made in Section 5 (covering
the theoretical results) and, second, we can determine the exact
entropy rate of the time series. We demonstrate that Œ†ùëöùëéùë•is indeed
an upper bound for the predictability of numeric univariate time
series and that the entropy rate can also be estimated accurately
in an ideal scenario. Then, we move on to experiments using real-
world data, showing that our approach is also applicable if the
underlying assumptions do not hold.
7.1 Experimental setup and Datasets
We use the first 80% of each time series (ùë•1,ùë•2, ...,ùë•ùëü)to train
different prediction models and to estimate Œ†ùëöùëéùë•. The remaining
20% of the time series (ùë•ùëü+1,ùë•ùëü+2,...,ùë•ùëõ)is used to evaluate the
accuracy Œ†ùëöùëúùëëùëíùëôof the models. Œ†ùëöùëúùëëùëíùëôis computed as shown in
Formula (10), i.e., the proportion of correct predictions among all
predictions. We use a one-step ahead forecasting method [ 15], i.e.,
the next value is predicted based on past observations up to this
point. Letùë•ùë°be the actual value and ÀÜùë•ùë°=ùëì(ùë•1,ùë•2,...,ùë•ùë°‚àí1)be
the prediction of a model for time step ùë°, respectively. Similar to
Formula (7), the correctness of a prediction of a model using an
error is
ùê∏ùëöùëúùëëùëíùëô =(
1|ùë•ùë°‚àíÀÜùë•ùë°|>ùúñ
0ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí(22)
Thus, the prediction accuracy of a model boils down to
Œ†ùëöùëúùëëùëíùëô=1‚àí√çùëõ
ùë°=ùëü+1ùê∏ùëöùëúùëëùëíùëô
ùëõ‚àíùëü(23)
We use the following datasets for the experiments:
‚Ä¢Synthetic Datasets are based on First Order Markov Mod-
els. In the experiments, we label the datasets using the fol-
lowing schema: ùëÅùë•ùê∑ùë¶ , whereùë•represents the number of
states in the model, meaning the number of distinct values
that are generated, and ùë¶specifies the distribution of the
transition probabilities. We use the following distributions:
(1)ùë¶=ùë¢, uniform distribution, (2) ùë¶=ùë†, skewed distribu-
tion, i.e., Zipf distribution with ùõº=2, and (3)ùë¶=ùëñ, where
ùëñ‚àà{0,1,2,...,ùëò}. The parameter ùëñdetermines the transi-
tion probabilities in the following way: ùëÅ‚àí1transitions,
called weak transitions, have probability ùëùùëñùë§=1
10ùëñùëÅ, and
the remaining transition, called dominant transition, has the
1The code and data can be accessed on: https://github.com/JamalSZ/QEPUBUNTS
 
2241Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
probabilityùëùùëñ
ùëë=1‚àí(ùëÅ‚àí1)ùëùùëñùë§. Forùëñ=0, we have a uniform
distribution; for larger ùëñ, we have more skew.
‚Ä¢TheS&P 5002open price dataset provides a collection of
daily opening prices for individual stocks. The python pan-
das_datareader package with parameters symbol=‚ÄôGSPC‚Äô,
start date=04-01-1990, end date= 31-10-2021 and data_source
= ‚Äôyahoo‚Äô was used to load the data from Yahoo! Finance.
‚Ä¢TheTemperature3dataset provides hourly temperature
values of the city of Zurich Switzerland.
‚Ä¢ETTh1 (Electricity Transformer Temperature hourly) is a
dataset about the long-term deployment of electric power,
consisting of two-year hourly oil temperatures of electric
household power generators. [18].
‚Ä¢ThePrecipitation4dataset (CPC-CONUS) contains daily
rain measurements. For the experiments, we use historical
records of location (39‚ó¶N, 109‚ó¶W) from 1948 until 2024.
We use the real-world datasets to show that our approach also
works when assumptions such as stationarity do not hold and the
time series show a trend or seasonality. We ran the Augmented
Dickey-Fuller5(ADF) stationarity test [ 20,21], indicating how much
each dataset deviates from stationarity (ADF critical values are:
‚àí3.431for 1%,‚àí2.862for 5%, and‚àí2.567for 10%). Based on the
p-values and the ADF statistic (see Table 1 for an overview of ADF
results and trend/seasonality for all datasets), the ETTh1 dataset
can be considered marginally stationary at a 1% significance level
and stationary at the 5% level and shows seasonality, but no trend.
The S&P 500 dataset does not meet any of the ADF criteria and
is therefore clearly non-stationary, showing only a trend. For the
Temperature dataset, the p-value and the ADF statistics are lower
than the 1% critical value, indicating stationarity. It shows seasonal-
ity, but no trend. Finally, the Precipitation dataset has a p-value and
an ADF statistic far below the critical values, strongly indicating
stationarity.
ADF Statistic p-value trend seasonality
ETTh1 -3.416 0.010 ‚úì
S&P 500 0.856 0.992 ‚úì
Temperature -3.732 0.004 ‚úì
Precipitation -114.624 0.000 ‚úì ‚úì
Table 1: Stationarity Test
We implemented our algorithms from scratch in Python and
conducted the experiments on a machine with an Intel Xeon Silver
4214 CPU with 48 cores at 1.000GHz, running Debian GNU/Linux
10 (buster)ùë•86_64as operating system. Solving Equation (19)for
Œ†maxis done using the numerical solver scipy.optimize.fsolve().6
2https://finance.yahoo.com/quote/%5EGSPC/history/
3https://data.stadt-zuerich.ch/dataset/ugz_meteodaten_stundenmittelwerte (Accessed
on 20-11-2021)
4Copernicus Climate Change Service, Climate Data Store, (2021): Temperature and
precipitation gridded data for global and regional domains derived from in-situ and
satellite observations. Copernicus Climate Change Service (C3S) Climate Data Store
(CDS). DOI: 10.24381/cds.11dedf0c (Accessed on 11-06-2024)
5https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html
6https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html7.2 Ideal Scenario
For the ideal scenarios, we use synthetic datasets and set the tol-
erance level ùúñto 0 for the moment to ensure that all theoretical
assumptions are met and that we can compute the exact entropy
rate.
7.2.1 Œ†ùëöùëéùë•is an Upper Bound. We generated 20 synthetic datasets
NxDy varying the number of states ùë•and the probability distribu-
tionsùë¶for the transitions: ùë•‚àà{5,10,15,..., 50}andùë¶‚àà{ùë¢,ùë†}. The
case forùë¶=ùë†is highly predictable, because every state has one very
likely transition, while the case for ùë¶=ùë¢is unpredictable, since
every transition probability is the same. We trained two models
on the generated time series: ARIMA of order 1 (AR(1)) and a first-
order Markov model (FOMM). We cannot do better than FOMM, as
it follows the underlying generating process perfectly.
(a) Uniform Œ†ùëöùëéùë•,ùëávsŒ†ùëöùëúùëëùëíùëô
(b) Skewed Œ†ùëöùëéùë•,ùëávsŒ†ùëöùëúùëëùëíùëô
Figure 4: First-Order-Markov-Model Synthetic
Figure 4 compares the predictabilities of the models and Œ†ùëöùëéùë•
with each other. On the x-axis, we vary the number of different
states of the Markov model and the y-axis measures the predictabil-
ity (as defined in Equation 23). We make the following observations.
First,Œ†ùëöùëéùë•is indeed an upper bound for these datasets, its curve
is always above the ones for AR(1) and FOMM. Additionally, the
general shape of the curves for FOMM and Œ†ùëöùëéùë•is very similar (in
particular for the more predictable dataset with ùë¶=ùë†), indicating
thatŒ†ùëöùëéùë•captures the potential predictability of the time series
well. This is remarkable, because Œ†ùëöùëéùë•does not make any assump-
tions about the probability distribution underlying the generating
process. Finally, as expected, the upper bound Œ†ùëöùëéùë•is also smaller
for the less predictable datasets ( ùë¶=ùë¢) and higher for the more
predictable ones ( ùë¶=ùë†).
7.2.2 Entropy Rate Estimation. As already mentioned before, the
accurate estimation of entropy rates is crucial for us. While most
proposed estimators converge to the true value for time series of
infinite lengths, for finite time series the situation is more complex.
For this experiment, we generated eight datasets ùëÅùë•ùê∑ùëñ with
ùë•=10,ùëñ=0,1,2,..., 7for figure 5a and 13 datasets ùëÅùë•ùê∑ùë¶ with
ùë•=2,3,4,5,10,15,..., 50andùë¶=ùë¢for figure 5b. In Figure 5, we
compare the quality of NLZ1 and NLZ2 with the exact entropy rate
for the generated datasets. On the x-axis, we vary ùëñ, depicting the
weak transition probability ùëùùëñùë§(Figure 5a) and the number of states
(Figure 5b). On the y-axis we show the entropy rate. We actually
display the plot twice in Figure 5a: the upper half uses a linear scale,
while the lower half uses a logarithmic one. Let us first look at
 
2242KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
(a)ùêªùëíùë•ùëéùëêùë° /ùêªùëÅ ùêøùëç 1/ùêªùëÅ ùêøùëç 2
 (b)ùêªùëíùë•ùëéùëêùë° /ùêªùëÅ ùêøùëç 1/ùêªùëÅ ùêøùëç 2
Figure 5: Entropy Estimation for different entropy levels
the NLZ1 estimator: in Figure 5a it overestimates the true entropy
rate, but gets closer for large values of ùëùùëñùë§(high level of entropy);
in Figure 5b it starts out by overestimating the true rate for small
values ofùë•(i.e., lower level of entropy) and then gradually shifts
to underestimating the rate for large values of ùë•(i.e., higher level
of entropy). This confirms empirical results by Lesne et al. in [17],
who demonstrated that the accuracy of entropy rate estimators
depends on the entropy level of the dataset. The classic Lempel-Ziv
estimator, called LZ76 in [ 17], overestimated the entropy rate for
low entropy levels and underestimated it for high entropy levels.
We now turn to the NLZ2 estimator. When Kontoyiannis et al.
developed their estimator [ 16], they assumed that standard Lempel-
Ziv estimators tend to overestimate the true entropy rate based on
experiments conducted on a dataset consisting of four Jane Austen
novels. While this assumption holds for English text, which has a
relatively low entropy rate of 1.6 bits per character, Kontoyiannis
et al. did not consider other datasets with higher levels of entropy,
missing the fact that Lempel-Ziv estimators do not always over-
estimate the true entropy. Their goal was to create an estimator
that, while converging to the true entropy for time series of infinite
lengths, produces estimates that are lower than those of NLZ1 for
finite ones. We confirm this in Figures 5a and 5b: NLZ2 generally
underestimates the true entropy rate, except for datasets with very
low levels of entropy (see lower left corners of figures). For our
purposes, the NLZ2 estimator is usually the better estimator, as
an underestimation of the true entropy rate will guarantee that
Œ†ùëöùëéùë•is an upper bound. We come back to the discussion of the
entropy level of datasets when investigating real-world data sets in
Section 7.3. We also studied the convergence rate of the estimators:
there is no substantial difference between NLZ1 and NLZ2 (the
results can be found in the appendix).
7.3 Real-World Time Series
We now investigate the four real-world datasets ETTh1, S&P 500,
Temperature, and Precipitation. Figures 6a, 6b, 6c, and 6d show
Œ†ùëöùëéùë•, computed with NLZ1 and NLZ2, and different models trained
on the data: SciNet[ 18], ARIMA[ 14], LSTM[ 13], and CNNLSTM[ 8].
On the x-axis we vary the tolerance threshold ùúñ(low values of ùúñ
mean a high level of entropy and vice versa) and on the y-axis we
display the predictability.
For the experiments, we varied the threshold level from ùúñùëöùëéùë•=
ùëöùëéùë•(ùëá)‚àíùëöùëñùëõ(ùëá)
2, where all values match each other, i.e., we havean entropy of 0, down to ùúñùëöùëñùëõ=ùëöùëñùëõ(ùë•ùëñ‚àíùë•ùëó)
2,‚àÄùë•ùëñ,ùë•ùëó‚ààùúí(withùúí
being the set of unique values in T), where non-identical values
never match, i.e., we have the highest possible entropy rate. For
practical purposes, a user would have to choose a value that reflects
the margin of error that is still acceptable for an application.
We observe that the models perform differently for the various
datasets, but the most important question is: is Œ†ùëöùëéùë•still an upper
bound for predictability for real-world datasets when many of
the assumptions made for the proofs do not hold anymore (see
Section 7.1). If we use the NLZ1 estimator, the answer seems to
be no, as for high values of ùúñ, the predictability of many models
surpasses Œ†ùëöùëéùë•. It is actually more intricate, as the differences
between NLZ1 and NLZ2 heavily depend on the entropy level.
Large values of ùúñmean that we are more lenient when it comes
to matching values and this has a direct impact on the entropy
rate: the larger ùúñ, the lower the entropy rate, which can be seen in
Figures 6e, 6f,6g, and 6h for the different datasets. For low levels
of entropy (i.e., large values of ùúñ), as we have seen in Section 7.2.2,
the NLZ1 estimator overestimates the true entropy rate and, conse-
quently, we end up with an underestimation for Œ†ùëöùëéùë•. Significantly
overestimating the true entropy rate leads to a situation in which
Œ†ùëöùëéùë•ceases to be an upper bound, while underestimating the true
entropy rate leads to a looser bound for Œ†ùëöùëéùë•(however, Œ†ùëöùëéùë•is
still an upper bound). The NLZ2 estimator was specifically built to
provide lower estimates than NLZ1, converging to the true value
from below, and, therefore, is better suited for low levels of entropy.
For high levels of entropy (i.e., small values of ùúñ), we are faced with
a different situation: at some point (e.g., in Figure 5b this was for an
entropy rate between 3 and 4), NLZ1 switches from overestimating
to underestimating the true entropy rate. Around this breakeven
point, NLZ1 provides a very accurate estimation of the entropy
rate and if we move to higher levels of entropy, NLZ1 gives us
better results than NLZ2, which underestimates the true entropy
rate more and more (resulting in a looser upper bound for Œ†ùëöùëéùë•).
Consequently, for low levels of entropy, NLZ2 should be used, while
for high levels of entropy, NLZ1 is the better estimator.
For very low levels of entropy, NLZ2 starts to overestimate the
true entropy rate as well. This the case for very large values of ùúñfor
real-world datasets, where Œ†ùëöùëéùë•may cease to be an upper bound.
Nevertheless, we argue that for practical purposes, these cases are
irrelevant, since the predictability reaches values close to one, which
means that we know that these scenarios are highly predictable.
Additionally, very large values for the tolerance threshold ùúñare
useless from the application point of view: for instance, assuming a
stock price with a value of $200, making predictions within a range
of +$100/-$100 of the true value is not useful.
8 CONCLUSION AND FUTURE WORK
We developed a technique for quantifying an upper bound for the
predictability of univariate numeric time series and implemented
an algorithm to estimate this upper bound. Unlike conventional
approaches for adapting discrete algorithms to numeric data by
discretizing the data, we introduce a tolerance threshold ùúñ, which
allows us to avoid the proximity issues caused by discretization.
In an empirical study, we validate the effectiveness of our ap-
proach and demonstrate that it can also be applied in real-world
 
2243Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
(a) ETTh1
 (b) S&P 500 Open Price
 (c) City of Z√ºrich Temperature
 (d) Precipitation
(e) ETTh1
 (f) S&P 500 Open Price
 (g) City of Z√ºrich Temperature
 (h) Precipitation
Figure 6: (a), (b), (c), (d) Œ†ùëöùëéùë•vsŒ†ùëöùëúùëëùëíùëôand (e), (f), (g), (h) ùêªùëÅùêøùëç 1vsùêªùëÅùêøùëç 1(real-world datasets)
settings, which do not always adhere to the conditions assumed by
the theoretical framework we use. We showcase the utility of our
method across a range of scenarios and datasets.
While the computation of the upper bound Œ†ùëöùëéùë•itself is robust
and stable, it relies on sufficiently accurate estimations of the en-
tropy rate of a given time series. While the theoretical properties
of entropy rate estimators are largely understood, especially in
scenarios modeling infinite time series, investigations into finite
time series (for real-world applications) are few and far between.
We plan to analyze the behavior and properties of entropy rate
estimators for various finite time series with the goal of developing
more robust estimators that consider the overall entropy level of a
dataset, allowing us to unlock the full potential of our approach.
REFERENCES
[1]Peter Andreasen. 2001. Universal Source Coding. Master‚Äôs thesis. University of
Copenhagen, Copenhagen, Denmark.
[2]Valentina Baccetti and Matt Visser. 2012. Infinite Shannon entropy. Journal of
Statistical Mechanics: Theory and Experiment 2013 (12 2012). https://doi.org/10.
1088/1742-5468/2013/04/P04010
[3]Christoph Bandt and Bernd Pompe. 2002. Permutation Entropy: A Natural
Complexity Measure for Time Series. Phys. Rev. Lett. 88 (Apr 2002), 174102. Issue
17. https://doi.org/10.1103/PhysRevLett.88.174102
[4]Richard E. Blahut. 2002. 25 - Information Theory and Coding. In Reference Data for
Engineers (Ninth Edition) (ninth edition ed.), Wendy M. Middleton and Mac E. Van
Valkenburg (Eds.). Newnes, Woburn, 25‚Äì1‚Äì25‚Äì31. https://doi.org/10.1016/B978-
075067291-7/50027-3
[5]Weiting Chen, Jun Zhuang, Wangxin Yu, and Zhizhong Wang. 2009. Measuring
complexity using FuzzyEn, ApEn, and SampEn. Medical Engineering & Physics
31, 1 (2009), 61‚Äì68. https://doi.org/10.1016/j.medengphy.2008.04.005
[6]Zhe Chen, Ya‚Äôan Li, Hongtao Liang, and Jing Yu. 2019. Improved Permutation
Entropy for Measuring Complexity of Time Series under Noisy Condition. Com-
plexity 2019 (2019), 1403829:1‚Äì1403829:12. https://doi.org/10.1155/2019/1403829
[7]Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information Theory (2nd
edition ed.). John Wiley & Sons, Inc.
[8]Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Sub-
hashini Venugopalan, Trevor Darrell, and Kate Saenko. 2015. Long-term recurrent
convolutional networks for visual recognition and description. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June
7-12, 2015. IEEE Computer Society, 2625‚Äì2634. https://doi.org/10.1109/CVPR.
2015.7298878[9] Andrew Feutrill. 2023. Characterisation and Estimation of Entropy Rate for Long
Range Dependent Processes. Ph. D. Dissertation. University of Adelaide, Adelaide,
Australia.
[10] Joshua Garland, Ryan James, and Elizabeth Bradley. 2014. Model-free quantifi-
cation of time-series predictability. Phys. Rev. E 90 (Nov 2014), 052910. Issue 5.
https://doi.org/10.1103/PhysRevE.90.052910
[11] Dina Q. Goldin and Paris C. Kanellakis. 1995. On Similarity Queries for Time-
Series Data: Constraint Specification and Implementation. In 1st Int. Conf. on
Principles and Practice of Constraint Programming (CP‚Äô95). Cassis, France, 137‚Äì153.
https://doi.org/10.1007/3-540-60299-2_9
[12] Yanjun Han, Jiantao Jiao, Chuan-Zheng Lee, Tsachy Weissman, Yihong Wu, and
Tiancheng Yu. 2018. Entropy Rate Estimation for Markov Chains with Large
State Space. arXiv:1802.07889 [cs.LG]
[13] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (11 1997), 1735‚Äì1780. https://doi.org/10.1162/neco.1997.
9.8.1735
[14] R.J. Hyndman and G. Athanasopoulos. 2021. Forecasting: principles and practice
(3rd edition ed.). OTexts: Melbourne, Australia. OTexts.com/fpp3.
[15] Rob J. Hyndman and Anne B. Koehler. 2006. Another look at measures of
forecast accuracy. International Journal of Forecasting 22, 4 (2006), 679‚Äì688.
https://doi.org/10.1016/j.ijforecast.2006.03.001
[16] Kontoyiannis I., Algoet P. H., Suhov Yu. M., and Wyner A. J. 1998. Nonpara-
metric Entropy Estimation for Stationary Processes and Random Fields, with
Applications to English Text. IEEE Transactions on Information Theory 44 (1998),
1319‚Äì1327.
[17] Annick Lesne, Jean-Luc Blanc, and Laurent Pezard. 2009. Entropy estimation
of very short symbolic sequences. Phys. Rev. E 79 (Apr 2009), 046208. Issue 4.
https://doi.org/10.1103/PhysRevE.79.046208
[18] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and
Qiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with Sample
Convolution and Interaction. arXiv:2106.09305 [cs.LG]
[19] E.N. Lorenz. 1995. Predictability: a problem partly solved. In Seminar on Pre-
dictability. ECMWF, Reading, United Kingdom, 1‚Äì18.
[20] James G MacKinnon. 1994. Approximate asymptotic distribution functions for
unit-root and cointegration tests. Journal of Business & Economic Statistics 12, 2
(1994), 167‚Äì176.
[21] James G MacKinnon. 2010. Critical values for cointegration tests. Technical Report.
Queen‚Äôs Economics Department Working Paper.
[22] Michael D. Morse and Jignesh M. Patel. 2007. An efficient and accurate method
for evaluating time series similarity. In Proc. of the ACM SIGMOD Int. Conf. on
Management of Data. ACM, Beijing, China, 569‚Äì580. https://doi.org/10.1145/
1247480.1247544
[23] Athanasios Papoulis and S. Unnikrishna Pillai. 2002. Probability, Random Variables,
and Stochastic Processes (4th ed.). McGraw Hill, Boston.
[24] Frank Pennekamp, Alison C. Iles, Joshua Garland, Georgina Brennan, Ulrich Brose,
Ursula Gaedke, Ute Jacob, Pavel Kratina, Blake Matthews, Stephan Munch, Mark
Novak, Gian Marco Palamara, Bj√∂rn C. Rall, Benjamin Rosenbaum, Andrea Tabi,
 
2244KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
Colette Ward, Richard Williams, Hao Ye, and Owen L. Petchey. 2019. The intrinsic
predictability of ecological time series and its potential to guide forecasting.
Ecological Monographs 89, 2 (2019), e01359. https://doi.org/10.1002/ecm.1359
[25] Steven M. Pincus. 1991. Approximate entropy as a measure of system complexity.
Proc. Natl. Acad. Sci. 88, 6 (1991), 2297‚Äì2301. https://doi.org/10.1073/pnas.88.6.
2297
[26] Joshua S. Richman and J. Randall Moorman. 2000. Physiological time-series
analysis using approximate entropy and sample entropy. American Journal of
Physiology-Heart and Circulatory Physiology 278, 6 (2000), H2039‚ÄìH2049. https:
//doi.org/10.1152/ajpheart.2000.278.6.H2039
[27] Claude E. Shannon. 1948. A mathematical theory of communication. Bell Syst.
Tech. J. 27, 4 (1948), 623‚Äì656. https://doi.org/10.1002/j.1538-7305.1948.tb00917.x
[28] Claude E. Shannon. 1948. A mathematical theory of communication. Bell Syst.
Tech. J. 27, 3 (1948), 379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x
[29] Robert H. Shumway and David S. Stoffer. 2017. Time Series Analysis and Its
Applications (4th edition ed.). Springer. https://doi.org/10.1007/978-3-319-52452-
8
[30] Gavin Smith, Romain Wieser, James Goulding, and Duncan Barrack. 2014. A
refined limit on the predictability of human mobility. 2014 IEEE International
Conference on Pervasive Computing and Communications, PerCom 2014, 88‚Äì94.
https://doi.org/10.1109/PerCom.2014.6813948
[31] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-L√°szl√≥ Barab√°si. 2010.
Limits of Predictability in Human Mobility. Science 327(5968) (February 2010),
1018‚Äì1021.
[32] Aaron D. Wyner and Jacob Ziv. 1989. Some asymptotic properties of the entropy
of a stationary ergodic data source with applications to data compression. IEEE
Trans. Inf. Theory 35, 6 (1989), 1250‚Äì1258. https://doi.org/10.1109/18.45281
[33] Aaron D. Wyner and Jacob Ziv. 1991. Fixed data base version of the Lempel-
Ziv data compression algorithm. IEEE Trans. Inf. Theory 37, 3 (1991), 878‚Äì880.
https://doi.org/10.1109/18.79955
[34] Paiheng Xu, Likang Yin, ZhongTao Yue, and Tao Zhou. 2019. On predictability
of time series. Physica A-statistical Mechanics and Its Applications 523 (2019),
345‚Äì351.
[35] Jacob Ziv and Abraham Lempel. 1977. A Universal Algorithm for Sequential
Data Compression. IEEE Transactions on Information Theory 23 (1977), 337‚Äì343.
[36] Jacob Ziv and Abraham Lempel. 1978. Compression of individual sequences
via variable-rate coding. IEEE Trans. Inf. Theory 24, 5 (1978), 530‚Äì536. https:
//doi.org/10.1109/TIT.1978.1055934
A PROPERTIES OF MARKOV CHAINS
Definition 8. A sequence of random variables ùëã1,ùëã2,...ùëãùëõare
said to form a (first order) Markov chain if the conditional distribution
of anyùëãùëñdepends only on ùëãùëñ‚àí1and is conditionally independent of
ùëã1..ùëãùëñ‚àí2. Specifically, ùëã1,ùëã2,...ùëãùëõform a Markov chain if the joint
probability mass function can be written as:
ùëù(ùë•1,ùë•2,..ùë•ùëõ)=ùëù(ùë•1)¬∑ùëù(ùë•2|ùë•1)¬∑¬∑¬∑¬∑¬∑ùëù(ùë•ùëõ|ùë•ùëõ‚àí1) (24)
Theorem 4. Letùëã={ùëãùëñ}be a stationary Markov chain with ùëÅ
unique states, stationary distribution ùúáùëÅand transition probability
matrixùëÉùëÅ. Letùëã1‚àºùúáùëÅ. Then the exact entropy rate is:
Hùëíùë•ùëéùëêùë°(ùëã)=‚àí√ï
ùëñùëóùúáùëñ¬∑ùëÉùëñùëóùëôùëúùëîùëÉùëñùëó. (25)
where the stationary distribution is the solution of the following equa-
tion
ùúáùëÅ=ùúáùëÅ¬∑ùëÉùëÅ (26)
Example 1. Here we show how we compute the exact entropy rate
of a Markov chain. Let T = {0,1,0,1,0,1,0,1} with transition probability
matrix
ùëÉ=0.5 0.5
0.5 0.5
In the general case, to compute ùúáùëÅ, we first initialize ùúá0with
arbitrary probabilities (summing up to one), e.g. ùúá0=[0.01,0.99]. We
then iteratively compute ùúáùëñusing Equation (26)(ùúáùëñ=ùúáùëñ‚àí1¬∑ùëÉ) until
ùúáùëñ‚âÉùúáùëñ‚àí1. Since T is a uniformly distributed time series, the steady
stationary distribution of T is ùúáùëÅ={0.5,0.5}. Finally, we calculatethe entropy rate using Equation (25), obtaining
Hùëíùë•ùëéùëêùë°(ùëá)=‚àí√ç2
ùëñ=1√ç2
ùëó=1ùúáùëñ¬∑ùëÉùëñùëóùëôùëúùëîùëÉùëñùëó=‚àí4‚àó0.5‚àó0.5¬∑ùëôùëúùëî20.5=1
B IMPLEMENTATION AND PSEUDOCODE
Figure 7 shows the pipeline of operations needed to compute Œ†max.
Calculating the value for ùëÅis straightforward, we determine the
minimum and maximum value occurring in ùëáand then apply Equa-
tion(14). Solving Equation (19)forŒ†maxis done using a numerical
solver scipy.optimize.fsolve()7function which finds the roots of
a nonlinear equation. As the most challenging part is estimating
the entropy rate, we provide more details on the implementation
of NLZ1 and NLZ2 here.
Input:
ùëáandùúñ
Estimating
Entropy RateComputeùëÅ
Solve
forŒ†maxOutput: Œ†max
Figure 7: Framework Pipeline
B.1 Implementing NLZ1 and NLZ2
Algorithm 3 shows pseudocode for our implementation of the NLZ1
estimator and Algorithm 4 provides details about the lookup func-
tion we use.
In the NLZ2 algorithm, we first build an inverted index for faster
lookup of matching values, see Algorithm 5 for pseudocode. Algo-
rithm 6 shows pseudocode for the implementation of the actual
NLZ2 estimator. Finally, we describe how lookups in the index work.
We have to find all values in the directory of the index that match
the current value in the time series. Since the entries in the directory
are sorted, we use binary search to find the first matching value
(and then traverse back and forth from there). For every matching
value, we look up the positions in the dictionary and return those
that have already been parsed. Algorithm 7 shows pseudocode for
the lookup code.
B.2 Run Time Complexity
The run time of the algorithms presented in the previous sections
depends heavily on the distribution of the values within a time
series, which makes it difficult to analyze their complexity. Con-
sequently, we look at the extreme cases, i.e., the most expensive
and the least expensive one, all the other run times fall between
these two extremes. We focus on the complexity of Algorithm 6, as
it is more complicated than Algorithm 3. For two arbitrary values
ùëáùëñ,ùëáùëó‚ààùëá,0‚â§ùëñ‚â†ùëó<ùëõ, the extreme cases are:
‚Ä¢Case 1:‚àÄùëñ,ùëó(ùëñ‚â†ùëó):ùëáùëñÃ∏‚âàùúñùëáùëó, i.e.,ùëáùëñandùëáùëónever overlap
‚Ä¢Case 2:‚àÄùëñ,ùëó(ùëñ‚â†ùëó):ùëáùëñ‚âàùúñùëáùëó, i.e.,ùëáùëñandùëáùëóalways overlap
7https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html
 
2245Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Algorithm 3: NLZ1(ùëá=(ùëá1,ùëá2,...,ùëáùëõ),ùúñ)
1ùëñ‚Üê1,ùê∑‚Üê‚àÖ;
2whileùëñ‚â§ùëõdo
3 curSeq‚Üêùëáùëñ;
4 while LZ1lookup(curSeq, ùê∑,ùúñ)do
5ùëñ++;
6 ifùëñ‚â§ùëõthen
7 appendùëáùëñto curSeq;
8 else
9 break;
10 end
11 ifnot len(curSeq) in ùê∑then
12 ùê∑[len(curSeq)][ curSeq]‚Üê 1;
13 ùëñ++;
14 end
15 end
16end
17forùëñ‚Üê1to len(ùê∑)do
18ùëêùëõ‚Üêùëêùëõ+len(ùê∑[ùëñ]);
19end
20returnùëêùëõ(log2(ùëêùëõ)+1)
ùëõ
Algorithm 4: NLZ1lookup(curSeq, D, ùúñ)
1found‚Üêfalse;
2forseq inùê∑[len(curSeq)] do
3ùëñ‚Üê1;
4 while(ùëñ‚â§len(curSeq))‚àß( curSeq[ùëñ]‚âàùúñseq[ùëñ])do
5ùëñ++;
6 end
7 ifùëñ>len(curSeq) then
8 found‚Üêtrue;
9 break;
10 end
11end
12return found
Algorithm 5: NLZ2buildIdx( ùëá)
1ùê∑‚Üê‚àÖ;
2forùëñ‚Üê1to len(ùëá)do
3ùê∑[ùëá[ùëñ]].append(ùëñ);
4end
5Dir‚Üêsorted array of keys in ùê∑;
6return InvIdx(Dir,ùê∑)
Case 1 .In this scenario, the loop in line 6 of Algorithm 6 will
never be executed, as the set stored in the variable matches will al-
ways be empty. The lookup in line 5 (see also Algorithm 7), although
yielding no matches (since ùëáùëñÃ∏‚âàùúñùëáùëó, has to search the dictionary.
As the size of the dictionary is at most ùëõand we use binary search,
the run time for one search is ùëÇ(logùëõ). In Algorithm 6, the lookupAlgorithm 6: NLZ2(ùëá=(ùëá1,ùëá2,...,ùëáùëõ),ùúñ)
1Œõ‚Üê{ùúÜ1...ùúÜùëõ}; // initialize all ùúÜùëñwith 0
2Idx‚ÜêLZ2buildIIdx( ùëá) ;
3forcur‚Üê1 toùëõdo
4ùúÜmax‚Üê0;
5 matches‚ÜêLZ2lookup(Idx, ùëá, cur,ùúñ);
6 forùëñin matches do
7ùúÜ‚Üê0;
8 while(cur+ùúÜ<ùëõ)‚àß(ùëñ+ùúÜ<cur)do
9 ifùëá[ùëñ+ùúÜ]‚âàùúñùëá[cur+ùúÜ]then
10 ùúÜ++;
11 end
12 end
13 ifcur+ùúÜ=ùëõthen
14 return Œõ
15 end
16 ifùúÜ>ùúÜmaxthen
17 ùúÜmax‚ÜêùúÜ;
18 end
19 end
20ùúÜùëêùë¢ùëü‚ÜêùúÜmax+1;
21end
22return Œõ
is invokedùëõtimes (see line 3), thus the overall run time complexity
of Case 1 is ùëÇ(ùëõlogùëõ).
Case 2 .In this case, because the current value ùëáùëêùë¢ùëüof the time
series matches all previously seen ones ùëá1,ùëá2,...,ùëáùëêùë¢ùëü‚àí1, the size
of the set returned by the lookup (and stored in matches ) is equal
toùëêùë¢ùëü‚àí1. Subsequently, the loop in line 6 of Algorithm 6 will be
invokedùëêùë¢ùëü‚àí1times. The number of comparisons performed in
the loop starting at line 8is inversely proportional to the value of
ùëñ: the larger ùëñ, the fewer comparisons we need to make ( ùëêùë¢ùëü‚àí1
comparisons for ùëñ=1,ùëêùë¢ùëü‚àí2comparisons for ùëñ=2, and so forth).
Thus, in total, we have to make
ùëêùë¢ùëü‚àí1√ï
ùëñ=1ùëêùë¢ùëü‚àíùëñ=ùëêùë¢ùëü‚àí1√ï
ùëñ=1ùëêùë¢ùëü‚àíùëêùë¢ùëü‚àí1√ï
ùëñ=1ùëñ (27)
=ùëêùë¢ùëü2‚àí1
2(ùëêùë¢ùëü‚àí1)ùëêùë¢ùëü (28)
=1
2ùëêùë¢ùëü(ùëêùë¢ùëü+1) (29)
comparisons in the inner loops (line 6 and line 8), which are exe-
cuted for each iteration of the outermost loop in line 3. The out-
ermost loop stops at cur =ùëõ
2, since we reach the end of the time
series when searching for the longest match (see condition in line
8). So, the overall number of iterations is
ùëõ
2√ï
ùëêùë¢ùëü=11
2ùëêùë¢ùëü(ùëêùë¢ùëü+1) (30)
 
2246KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Jamal Mohammed, Michael H.B√∂hlen, and Sven Helmer
SinceùëÇ(1
2ùëêùë¢ùëü(ùëêùë¢ùëü+1))=ùëÇ(ùëêùë¢ùëü2)andùëÇ(ùëõ
2)=ùëÇ(ùëõ), we obtain
ùëõ√ï
ùëêùë¢ùëü=1ùëêùë¢ùëü2=ùëõ(ùëõ+1)(2ùëõ+1)
6=ùëÇ(ùëõ3) (31)
via the square pyramidal number of ùëõ.
Therefore, the worst-case complexity of Case 2 is ùëÇ(ùëõ3). Strictly
speaking, there is still the cost ùëÇ(ùëõlogùëõ)of looking up the matches
in line 5, but this cost is dominated by ùëÇ(ùëõ3).
Algorithm 7: NLZ2lookup(Idx, ùëá, cur,ùúñ)
1matches‚Üê‚àÖ;
2forallùëñsuch that Idx.Dir[ ùëñ]‚âàùúñùëá[ùëêùë¢ùëü]do
3 forpos in Idx.ùê∑[Idx.Dir[ùëñ]]do
4 ifpos<curthen
5 matches‚Üêmatches‚à™{ pos };
6 end
7 end
8end
9return matches
C CONVERGENCE OF THE ENTROPY RATE
ESTIMATION
As we are estimating the entropy rate of finite time series, the rate
of convergence of the estimation becomes a crucial aspect in deter-
mining the stability of the value Œ†ùëöùëéùë•for increasing time series
lengths. It is usually difficult for real-world datasets to determine
this convergence rate, as several factors influence it. Nevertheless,
it would be of great help to get an indication that the length of the
time series is sufficiently large to ensure a reliable value for Œ†ùëöùëéùë•.
Han et al. claim that a length of ùëõ‚â´ùëÅ2, whereùëÅis the alphabet
size, is sufficient [ 12]. We found two issues with this approach. First
of all, what is the exact meaning of ‚Äú ‚â´‚Äù, i.e., when is ùëõmuch greater
thanùëÅ2in terms of concrete values? Second, the inequality given
in [12] is too simplistic, as the convergence rate does not depend on
the alphabet size alone. It is possible to have different convergence
rates for the same value of ùëÅ, but different levels of entropy [17].
We propose an approach based on measuring the differences of
the entropy rate taken from neighboring (equi-distant) points in
time. More formally, if we, for instance, estimate the entropy rate for
ùëõ=1000,2000,3000,... (whereùëõdenotes the length of the prefix of
a time series we use for the estimation), then Œî(ùëõ1,ùëõ2)=H2000‚àí
H1000measures the first difference, Œî(ùëõ2,ùëõ3)=H3000‚àíH 2000the
second one, and so on (for computing Hwe use Algorithm 1 or 2).
However, we found that using absolute values for the difference can
be misleading: depending on the level of entropy, the differences
vary. For high levels of entropy, the entropy rate starts out with
a large value and tends to drop steeply. In contrast, for low levels
of entropy, the values of the entropy rate start out smaller and
decrease more slowly. Hence, we measure the relative difference:
Œîùëü(ùëõùëñ,ùëõùëñ+1)=Hùëõùëñ+1‚àíHùëõùëñ
max(Hùëõùëñ+1‚àíHùëõùëñ). (32)When the relative difference drops below a certain threshold, we
know that the obtained estimates are stable enough for a reliable
estimation of Œ†ùëöùëéùë•.
(a) Convergence Threshold = 1% N=10
(b) Convergence Threshold = 1% N=50
Figure 8: Convergence of H with increasing n for different
values ofùúñ
We ran some experiments investigating the convergence by gen-
erating two datasets ùëÅùë•ùê∑ùë¶ withùë•={10,50}andùë¶=ùë¢looking
at the convergence rate of the entropy rate estimators (here we
show the results for the NLZ2 estimator, the ones for NLZ1 are
very similar). Like in the previous experiments, we also expected
the entropy level of the data to have an effect. Figures 8a and 8b
illustrate the results for ùë•=10andùë•=50, respectively. On the
x-axis, we vary ùëõ, the length of the time series, and on the y-axis, we
varyùúñ, the tolerance threshold. The numbers in the heatmaps are
computed by subtracting the entropy for ùëõfrom the value for the
previousùëõaccording to Equation (32)(forùëõ=1000, we subtract the
value from that for ùëõ=500, which is not shown). Again, we observe
that the level of entropy of the time series has an impact. For high
levels of entropy, i.e., small values of ùúñ, the estimators converge
fairly quickly, while for low levels of entropy, i.e., large values of
ùúñ, the estimators take longer. This is an important observation, as
it means that it is not sufficient to just make sure that ùëõ‚â´ùëÅ2
as claimed by Han et al. in [ 12], but that we also need to consider
the level of entropy of the data. For very low levels of entropy, the
estimators can take quite some time to converge (bottom rows in
Figures 8a and 8b). However, for practical purposes, these cases are
not important; we also discuss this matter in Section 7.3.
 
2247