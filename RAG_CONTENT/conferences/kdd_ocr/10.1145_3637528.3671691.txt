Meta Clustering of Neural Bandits
Yikun Ban∗
yikunb2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USAYunzhe Qi∗
yunzheq2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USATianxin Wei
twei10@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USA
Lihui Liu
lihuil2@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USAJingrui He
jingrui@illinois.edu
University of Illinois at
Urbana-Champaign
Urbana, IL, USA
Abstract
The contextual bandit has been identified as a powerful frame-
work to formulate the recommendation process as a sequential
decision-making process, where each item is regarded as an arm
and the objective is to minimize the regret of 𝑇rounds. In this
paper, we study a new problem, Clustering of Neural Bandits, by
extending previous work to the arbitrary reward function, to strike
a balance between user heterogeneity and user correlations in the
recommender system. To solve this problem, we propose a novel
algorithm called M-CNB, which utilizes a meta-learner to represent
and rapidly adapt to dynamic clusters, along with an informative
Upper Confidence Bound (UCB)-based exploration strategy. We
provide an instance-dependent performance guarantee for the pro-
posed algorithm that withstands the adversarial context, and we
further prove the guarantee is at least as good as state-of-the-art
(SOTA) approaches under the same assumptions. In extensive exper-
iments conducted in both recommendation and online classification
scenarios, M-CNB outperforms SOTA baselines. This shows the
effectiveness of the proposed approach in improving online recom-
mendation and online classification performance.
CCS Concepts
•Theory of computation →Online learning algorithms; •
Information systems →Personalization.
Keywords
Neural Contextual Bandits; Recommendation; User Modeling; Meta
Learning
ACM Reference Format:
Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He. 2024. Meta
Clustering of Neural Bandits. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
∗Both authors contributed equally to this paper.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.367169125–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671691
1 Introduction
Recommender systems play an integral role in various online busi-
nesses, including e-commerce platforms and online streaming ser-
vices. They leverage user correlations to assist the perception of user
preferences, a field of study spanning several decades. In the past,
considerable effort has been directed toward supervised-learning-
based collaborative filtering methods within relatively static envi-
ronments [ 26,52]. However, the ideal recommender systems should
adapt over time to consistently meet user interests. Consequently,
it is natural to formulate the recommendation process as a sequen-
tial decision-making process. In this paradigm, the recommender
engages with users, observes their online feedback (i.e., rewards),
and optimizes the user experience for long-term benefits, rather
than fitting a model on the collected static data based on super-
vised learning [ 13,22,60]. Based on this idea, this paper focuses on
the formulation of contextual bandits, where each item is treated
as an arm (context) in a recommendation round, and the primary
objective is to minimize the cumulative regret over 𝑇rounds and
tackle the dilemma of exploitation and exploration in the sequential
decision-making process [1, 3, 4, 8, 23, 24, 39, 39, 40, 43, 46, 47].
Linear contextual bandits model a user’s preference through a
linear reward function based on arm contexts [ 1,16,38]. However,
given the substantial growth of users in recommender systems, it
can be overly ambitious to represent all user preferences with a
single reward function, and it may overlook the user correlations if
each user is modeled as a single bandit. To address this challenge, a
series of methods known as clustering of linear bandits [ 4,23,24,39,
40] have emerged, which represent each cluster of users as a reward
function, achieving a balance between user heterogeneity and user
correlations. Note that the cluster information is unknown in this
problem setting. In essence, with each user being treated as a linear
contextual bandit, these methods adopt graph-based techniques to
dynamically cluster users, and leverage user correlations for making
arm recommendations. However, it is crucial to acknowledge the
limitations of this line of works: they all rely on linear reward
functions, and user clusters are represented as linear combinations
of individual bandit parameters. The assumptions of linearity in
reward functions and the linear representation of clusters may not
hold up well in real-world applications [53, 65].
 
95
KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
In relaxation of the assumption on reward mapping functions,
inspired by recent advances in the single neural bandit [ 64,65]
where a neural network is assigned to learn anunknown reward
function, we study the new problem of Clustering of Neural Bandits
(CNB) in this paper. Different from the single neural bandit [ 64,65]
and clustering of linear bandits [ 4,23,24,39,40], CNB introduces
the bandit clusters built upon the arbitrary reward functions, which
can be either linear or non-linear. Meanwhile, we note that the
underlying clusters are usually not static over specific arm contexts
[39]. For example, in the personalized recommendation task, two
users (bandits) may both like "country music", but can have differ-
ent opinions on "rock music". Therefore, adapting to arm-specific
"relative clusters" in a dynamic environment is one of the main
challenges in this problem. We propose a novel algorithm, Meta
Clustering of Neural Bandits ( M-CNB ), to solve the CNB problem.
Next, we will summarize our key ideas and contributions.
Methodology. To address the CNB problem, we must confront
three key challenges: (1) Efficiently determining a user’s relative
group: Our approach involves employing a neural network, named
the "user learner," to estimate each user’s preferences. By group-
ing users with similar preferences, we efficiently create clusters
with a process taking O(𝑛)time, where 𝑛is the number of bandits
(users). (2) Effective parametric representation of dynamic clusters:
Inspired by advancements in meta-learning [ 21,62], we introduce
a meta-learner capable of representing and swiftly adapting to
evolving clusters. In each round 𝑡, the meta-learner leverages its
perceived knowledge from prior rounds {1,...,𝑡−1}to rapidly
adapt to new clusters via a few samples. This enables the rapid
acquisition of nonlinear cluster representations, marking our first
main contribution. (3) Balancing exploitation and exploration with
relative bandit clusters: Our second main contribution is proposing
an informative UCB-type exploration strategy, which takes into
account both user-side and meta-side information for balancing the
exploration and exploitation. By addressing these three main chal-
lenges, our approach manages to solve the CNB problem effectively
and efficiently.
Theoretical analysis. To obtain a regret upper bound for the
proposed algorithm, we need to tackle the following three chal-
lenges: (1) Analyzing neural meta-learner in bandit framework:
To finish the analysis, we must build a confidence ellipsoid for the
meta-learner approximation, which is one of the main research gaps.
To deal with this gap, we bridge the meta-learner and user-learner
via the Neural Tangent Kernel (NTK) regression and build the con-
fidence ellipsoid upon the user-learner, which allows us to achieve
a more comprehensive understanding of the meta-learner’s behav-
ior. (2) Reducing the naive eO(√
𝑛𝑇)regret upper bound: eO(√
𝑇)is
roughly the regret effort to learn a single neural bandit, and thus
eO(√
𝑛𝑇)are the regret efforts to learn 𝑛neural bandits for 𝑛users.
We reduce the eO(√
𝑛𝑇)efforts to eO(√︁
𝑞𝑇), where𝑞is the expected
number of clusters. This also indicates the proposed algorithm
can leverage the collaborative effects among users. (3) Adversarial
attack on contexts: In most neural bandit works, a common as-
sumption is that the NTK matrix is non-singular, requiring that
no two observed contexts (items) are identical or parallel [ 64,65].
This vulnerability makes their regret analysis susceptible to adver-
sarial attacks and less practical in real-world scenarios. In face ofthis challenge, we provide an instance-dependent regret analysis
that withstands the context attack, and allows the contexts to be
repeatedly observed. Furthermore, under the same assumptions as
in existing works, we demonstrate that our regret upper bound is at
least as good as SOTA approaches. The above efforts to address the
challenges in the theoretical analysis is our third main contribution.
Evaluations. We evaluate the proposed algorithm in two scenar-
ios: Online recommendation and Online classification with bandit
feedback. For the first scenario, which naturally lends itself to CNB,
we assess the algorithm’s performance on four recommendation
datasets. Since online classification has been widely used to evaluate
neural bandits [ 6,64,65], we evaluate the algorithms on eight clas-
sification datasets where each class can be considered as a bandit
(user), and correlations among classes are expected to be exploited.
We compare the proposed algorithm with 8 strong baselines and
show the superior performance of the proposed algorithm. Addi-
tionally, we offer the empirical analysis of the algorithm’s time
complexity, and conduct extensive sensitivity studies to investi-
gate the impact of critical hyperparameters. The above empirical
evaluation is our fourth main contribution.
Next, detailed discussion regarding related works is placed in
Section 2. After introducing the problem definition in Section 3, we
present the proposed algorithm, M-CNB , in Section 4 together with
theoretical analysis in Section 5. Then, we provide the experimental
results in Section 6 and conclude the paper in Section 7.
2 Related Work
In this section, we briefly review the related works, including clus-
tering of bandits and neural bandits.
Clustering of bandits. CLUB [ 23] first studies collaborative
effects among users in contextual bandits where each user hosts
an unknown vector to represent the behavior based on the linear
reward function. CLUB formulates user similarity on an evolving
graph and selects an arm leveraging the clustered groups. Then,
Gentile et al. [24], Li et al. [39] propose to cluster users based on
specific contents and select arms leveraging the aggregated infor-
mation of conditioned groups. Li et al. [40] improves the clustering
procedure by allowing groups to split and merge. Ban and He [4]
uses seed-based local clustering to find overlapping groups, differ-
ent from global clustering on graphs. Korda et al. [33], Wu et al.
[57], Yang et al. [61] also study clustering of bandits with various
settings in recommender systems. However, all these works are
based on the linear reward assumption, which may fail in many
real-world applications.
Neural bandits . Lipton et al. [41], Riquelme et al. [49] adapt the
Thompson Sampling (TS) to the last layer of deep neural networks
to select an action. However, these approaches do not provide re-
gret analysis. Zhou et al. [65] and Zhang et al. [64] first provide the
regret analysis of UCB-based and TS-based neural bandits, where
they apply ridge regression on the space of gradients. Ban et al.
[5]studies a multi-facet bandit problem with a UCB-based explo-
ration. Jia et al. [30]perturbs the training samples for incorporating
both exploitation and exploration. EE-Net [ 6,9] proposes to use
another neural network for exploration with applications on active
learning [ 7,10] and meta-learning [ 48]. [59] combines the last-
layer neural network embedding with linear UCB to improve the
 
96Meta Clustering of Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
computation efficiency. Dutta et al. [20] uses an off-the-shelf meta-
learning approach to solve the contextual bandit problem in which
the expected reward is formulated as Q-function. Santana et al. [50]
proposes a Hierarchical Reinforcement Learning framework for
recommendation in the dynamic experiments, where a meta-bandit
is used for the selected independent recommender system. Kassraie
and Krause [31] revisit Neural-UCB type algorithms and shows
theeO(√
𝑇)regret bound without the restrictive assumptions on
the context. Hong et al. [27], Maillard and Mannor [42] study the
latent bandit problem where the reward distribution of arms are
conditioned on some unknown discrete latent state and prove the
eO(√
𝑇)regret bound for their algorithm as well. Federated bandits
[15] consider dealing with multiple bandits (agents) while preserv-
ing the privacy of each bandit. Deb et al. [17] reduce the contextual
bandits to neural online regression for tighter regret upper bound.
Qi et al. [47] propose to use graph to formulate user correlations
with the adoption of graph neural networks. However, the above
works either focus on the different problem settings or overlook
the clustering of bandits.
Other related works. [ 35,51] study meta-learning in Thomp-
son sampling and Hong et al. [28], Wan et al. [54] aims to exploit
the hierarchical knowledge among hierarchical Bayesian bandits.
However, they focus on the Bayesian or non-contextual bandits.
3 Problem: Clustering of Neural Bandits
In this section, we introduce the CNB problem, motivated by learn-
ing correlations among bandits with arbitrary reward functions.
Next, we will use the scenarios of personalized recommendation to
state the problem setting.
Suppose there are 𝑛users (bandits), 𝑁={1,...,𝑛}, to serve on
a platform. In the 𝑡thround, the platform receives a user 𝑢𝑡∈𝑁
(unique ID for this user) and prepares the corresponding 𝐾candi-
date arms X𝑡={x𝑡,1,x𝑡,2,..., x𝑡,𝐾}. Each arm is represented by
its𝑑-dimensional feature vector x𝑡,𝑖∈R𝑑,𝑖∈[𝐾]={1,...,𝐾},
which will encode the information from both the user side and
the arm side [ 38]. Then, the learner is expected to select an arm
x𝑡∈X𝑡and recommend it to 𝑢𝑡, where𝑢𝑡refers to the target or
served user. In response to this action, 𝑢𝑡will provide the platform
with a corresponding reward (feedback) 𝑟𝑡. Here, since different
users may generate different rewards towards the same arm, we
use𝑟𝑡,𝑖|𝑢𝑡to represent the reward produced by 𝑢𝑡given x𝑡,𝑖. The
formal definition of arm reward is below.
Given𝑢𝑡∈𝑁, the reward 𝑟𝑡,𝑖for each candidate arm x𝑡,𝑖∈X𝑡
is assumed to be governed by an unknown function by
𝑟𝑡,𝑖|𝑢𝑡=ℎ𝑢𝑡(x𝑡,𝑖)+𝜁𝑡,𝑖, (1)
whereℎ𝑢𝑡is an unknown reward function associated with 𝑢𝑡, and
it can be either linear or non-linear. 𝜁𝑡,𝑖is a noise term with zero
expectation E[𝜁𝑡,𝑖]=0. We also assume the reward 𝑟𝑡,𝑖∈[0,1]is
bounded, as in many existing works [ 4,23,24]. Note that previous
works on clustering of linear bandits all assume ℎ𝑢𝑡is a linear
function with respect to arm x𝑡,𝑖[4, 23, 24, 39, 40].
Meanwhile, users may exhibit clustering behavior. Inspired by
[24,39], we consider the cluster behavior to be item-varying, i.e.,
the users who have the same preference on a certain item may have
different opinions on another item. Therefore, we formulate a set ofusers with the same opinions on a certain item as a relative cluster,
with the following definition.
Definition 3.1 (Relative Cluster). In round𝑡, given an arm x𝑡,𝑖∈
X𝑡, a relative clusterN(x𝑡,𝑖)⊆𝑁with respect to x𝑡,𝑖satisfies
(1)∀𝑢,𝑢′∈N( x𝑡,𝑖),E[𝑟𝑡,𝑖|𝑢]=E[𝑟𝑡,𝑖|𝑢′]
(2)N′⊆𝑁,s.t.N′satisfies(1)andN(x𝑡,𝑖)⊂N′.
The condition(2)is to guarantee that no other clusters contains
N(x𝑡,𝑖). This cluster definition allows users to agree on certain
items while disagree on others, which is consistent with the real-
world scenario. Since the users from different clusters are expected
to have distinct behavior with respect to x𝑡,𝑖, we provide the fol-
lowing constraint among relative clusters.
Definition 3.2 (𝛾-gap). Given two different cluster N(x𝑡,𝑖),N′(x𝑡,𝑖),
there exists a constant 𝛾>0, such that
∀𝑢∈N( x𝑡,𝑖),𝑢′∈N′(x𝑡,𝑖),|E[𝑟𝑡,𝑖|𝑢]−E[𝑟𝑡,𝑖|𝑢′]|≥𝛾.
For any two clusters in 𝑁, we assume that they satisfy the 𝛾-
gap constraint. Note that such an assumption is standard in the
literature of online clustering of bandit to differentiate clusters
[4,23,24,39,40]. As a result, given an arm x𝑡,𝑖, the bandit pool 𝑁
can be divided into 𝑞𝑡,𝑖non-overlapping clusters:N1(x𝑡,𝑖),N2(x𝑡,𝑖),
...,N𝑞𝑡,𝑖(x𝑡,𝑖), where𝑞𝑡,𝑖≪𝑛. Note that the cluster information
isunknown in the platform.
For the CNB problem, the goal of the learner is to minimize the
pseudo regret of 𝑇rounds:
R𝑇=𝑇∑︁
𝑡=1E[𝑟∗
𝑡−𝑟𝑡|𝑢𝑡,X𝑡], (2)
where𝑟𝑡is the reward received in round 𝑡, and E[𝑟∗
𝑡|𝑢𝑡,X𝑡]=
max x𝑡,𝑖∈X𝑡ℎ𝑢𝑡(x𝑡,𝑖).
Notations. Let x𝑡be the arm selected in round 𝑡, and𝑟𝑡be
the corresponding reward received in round 𝑡. We use∥x𝑡∥2to
represent the Euclidean norm. For each user 𝑢∈𝑁, let𝜇𝑢
𝑡be the
number of rounds that user 𝑢’ learner has been served up to round
𝑡, andT𝑢
𝑡be all of𝑢’s historical data up to round 𝑡.𝑚is the width
of neural network and 𝐿is depth of neural network in the proposed
approach. Given a group N, all its data up to round 𝑡can be denoted
by{T𝑢
𝑡}𝑢∈N={T𝑢
𝑡|𝑢∈N} . We use standardOandΩnotation
to hide constants.
4 Proposed Algorithm
In this section, we present our proposed algorithm, denoted as
M-CNB , to address the formulated CNB problem. M-CNB lever-
ages the potential correlations among bandits, and aims to rapidly
acquire a representation for dynamic relative clusters.
ForM-CNB , we utilize a meta-learner, denoted as Θ, to rapidly
adapt to clusters, as well as represent the behavior of a cluster. Addi-
tionally, there are 𝑛user-learners, denoted by {𝜃𝑢}𝑢∈𝑁, responsible
for learning the preference ℎ𝑢(·)for each user 𝑢∈𝑁. In terms of
the workflow, the primary role of the meta-learner is to determine
recommended arms, while the user-learners are primarily utilized
for clustering purposes. The meta-learner and user-learners share
the same neural network structure, denoted as 𝑓. And the workflow
ofM-CNB is divided into three main components: User clustering,
 
97KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
Θ!"#Θ!∇𝐿!!∇𝐿!"∇𝐿!#𝑢!(1) Clustering(2) Mata Adaptation
Figure 1: Clustering and Meta Adaptation: Given 𝑢𝑡and an
arm x𝑡,𝑖, (1)M-CNB identifies cluster bN𝑢𝑡(x𝑡,𝑖), and then (2)
meta-learner Θ𝑡−1rapidly adapt to this cluster, proceeding
to (3) the UCB exploration.
Meta adaptation, and UCB-based selection. Then, we proceed to
elaborate their details.
User clustering. Recall that in Section 3, each user 𝑢∈𝑁
is governed by an unknown function ℎ𝑢. In this case, we use a
neural network 𝑓(·;𝜃𝑢), to estimate ℎ𝑢. In round𝑡∈[𝑇], let𝑢𝑡
be the user to serve. Given 𝑢𝑡’s past data up to round 𝑡−1, i.e.,
T𝑢𝑡
𝑡−1, we can train parameters 𝜃𝑢𝑡by minimizing the following loss:
L(𝜃𝑢𝑡)=Í
(x,𝑟)∈T𝑢𝑡
𝑡−1(𝑓(x;𝜃𝑢𝑡)−𝑟)2/2.Let𝜃𝑢𝑡
𝑡−1represent𝜃𝑢𝑡
trained onT𝑢𝑡
𝑡−1in round𝑡−1by stochastic gradient descent (SGD).
Therefore, for each 𝑢∈𝑁, we can obtain the trained parameters
𝜃𝑢
𝑡−1. Then, given 𝑢𝑡and an arm x𝑡,𝑖, we return 𝑢𝑡’s estimated
cluster with respect to arm x𝑡,𝑖by
bN𝑢𝑡(x𝑡,𝑖)=
𝑢∈𝑁|𝑓(x𝑡,𝑖;𝜃𝑢
𝑡−1)−𝑓(x𝑡,𝑖;𝜃𝑢𝑡
𝑡−1)|≤𝜈−1
𝜈𝛾	
.
(3)
where𝛾∈(0,1)represents the assumed 𝛾-gap and𝜈>1is a tuning
parameter to for the exploration of cluster members.
Meta adaptation. We employ one meta-learner Θto represent
and adapt to the behavior of dynamic clusters. In meta-learning,
the meta-learner is trained based on a number of different tasks
and can quickly adapt to new tasks with a small amount of new
data [ 21]. Here, we consider a cluster N𝑢𝑡(x𝑡,𝑖)as a task and its
collected data as the task distribution. As a result, M-CNB has two
adaptation phases: meta adaptation, and user adaptation.
Meta adaptation. In the 𝑡thround, given a cluster bN𝑢𝑡(x𝑡,𝑖), we
have the available "task distributions" {T𝑢
𝑡−1}𝑢∈bN𝑢𝑡(x𝑡,𝑖). The goal
of the meta-learner is to quickly adapt to the bandit cluster. Thus, we
randomly draw a few samples from {T𝑢
𝑡−1}𝑢∈bN𝑢𝑡(x𝑡,𝑖)and update
Θin round𝑡using SGD, denoted by Θ𝑡,𝑖, based on Θ𝑡−1that is
continuously trained on the collected interactions to incorporate
the knowledge of past 𝑡−1rounds. The workflow is described in
Figure 1 and Algorithm 2.
User adaptation. In the 𝑡thround, given 𝑢𝑡, after receiving the
reward𝑟𝑡, we have available data (x𝑡,𝑟𝑡). Then, the user leaner
𝜃𝑢𝑡is updated in round 𝑡to have a refined clustering capability,
denoted by𝜃𝑢𝑡
𝑡. As the users in a cluster share the same or similar
preferences on a certain item, we update all the user learners in
this cluster, described in Algorithm 1 Lines 14-18.
Note that for the clustering of linear bandits works [ 4,23,24,39,
40], they represent the cluster behavior Θby the linear combination
of bandit-learners, e.g., Θ=1
|bN𝑢𝑡(x𝑡,𝑖)|Í
𝑢∈bN𝑢𝑡(x𝑡,𝑖)𝜃𝑢
𝑡. This canAlgorithm 1 M-CNB
1:Input:𝑇(number of rounds), 𝛾,𝜈(cluster exploration parame-
ter),𝑆(norm parameter), 𝛿(confidence level) , 𝜂1,𝜂2(learning
rate),𝑚(width of neural network).
2:Initialize Θ0;𝜃𝑢
0=Θ0,𝜇𝑢
0=0,T𝑢
0=∅,∀𝑢∈𝑁
3:Observe one data for each 𝑢∈𝑁
4:for𝑡=1,2,...,𝑇 do
5: Receive a target user 𝑢𝑡∈𝑁and observe 𝑘arms X𝑡=
{x𝑡,1,..., x𝑡,𝑘}
6:for𝑖∈[𝑘]do
7: Determine bN𝑢𝑡(x𝑡,𝑖)={𝑢∈𝑁| |𝑓(x𝑡,𝑖;𝜃𝑢
𝑡−1) −
𝑓(x𝑡,𝑖;𝜃𝑢𝑡
𝑡−1)|≤𝜈−1
𝜈𝛾}.
8: Θ𝑡,𝑖=SGD_Meta
bN𝑢𝑡(x𝑡,𝑖),Θ𝑡−1
9: U𝑡,𝑖=𝑓(x𝑡,𝑖;Θ𝑡,𝑖) +∥∇Θ𝑓(x𝑡,𝑖;Θ𝑡,𝑖)−∇ 𝜃𝑓(x𝑡,𝑖;𝜃𝑢𝑡
0)∥2
𝑚1/4+
√︃
𝑆+1
2𝜇𝑢
𝑡+√︂
2 log(1/𝛿)
𝜇𝑢
𝑡
10: end for
11:b𝑖=arg𝑖∈[𝑘]maxU𝑡,𝑖
12: Play x𝑡,b𝑖and observe reward 𝑟𝑡,b𝑖
13: x𝑡=x𝑡,b𝑖, 𝑟𝑡=𝑟𝑡,b𝑖,Θ𝑡=Θ𝑡,b𝑖
14: for𝑢∈bN𝑢𝑡(x𝑡)do
15:L𝑡 𝜃𝑢
𝑡=(𝑓(x𝑡;𝜃𝑢
𝑡)−𝑟𝑡)2/2
16:𝜃𝑢
𝑡=𝜃𝑢
𝑡−𝜂1▽𝜃𝑢
𝑡L𝑡 𝜃𝑢
𝑡# User Adaptation
17:𝜇𝑢
𝑡=𝜇𝑢
𝑡−1+1,T𝑢
𝑡=T𝑢
𝑡−1∪{(x𝑡,𝑟𝑡)}
18: end for
19: for𝑢∉bN𝑢𝑡(x𝑡)do
20:𝜃𝑢
𝑡=𝜃𝑢
𝑡−1,𝜇𝑢
𝑡=𝜇𝑢
𝑡−1,T𝑢
𝑡=T𝑢
𝑡−1
21: end for
22:end for
Algorithm 2 SGD_Meta ( bN𝑢𝑡(x𝑡,𝑖),Θ𝑡−1)
bN=bN𝑢𝑡(x𝑡,𝑖)
for𝑢∈bNdo
Randomly draw(x𝑢,𝑟𝑢)fromT𝑢
𝑡−1
L𝑢(Θ𝑡−1)=(𝑓(x𝑢;Θ𝑡−1)−𝑟𝑢)2/2
end for
L𝑡−1(bN)=1
|bN|Í
𝑢∈bNL𝑢(Θ𝑡−1)
Θ𝑡,𝑖=Θ𝑡−1−𝜂2∇Θ𝑡−1L𝑡−1(bN) # Meta Adaptation
Return: Θ𝑡,𝑖
lead to limited representation power of the cluster learner, and
their linear reward assumptions may not necessarily hold for real
world settings [ 65]. Instead, we use the meta adaptation to update
the meta-learner Θ𝑡−1according to bN𝑢𝑡(x𝑡,𝑖), which can represent
non-linear combinations of user-learners [21, 55].
UCB-based Exploration. To balance the trade-off between
the exploitation of the currently available information and the
exploration of new matches, we introduce the following UCB-based
selection criterion. Based on Lemma A.12, the cumulative error
induced by meta-learner is controlled by
 
98Meta Clustering of Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
𝑇∑︁
𝑡=1E
𝑟𝑡|x𝑡
|𝑓(x𝑡;Θ𝑡)−𝑟𝑡|𝑢𝑡
≤𝑇∑︁
𝑡=1O(∥∇ Θ𝑓(x𝑡;Θ𝑡)−∇𝜃𝑓(x𝑡;𝜃𝑢𝑡
0)∥2)
𝑚1/4
|                                           {z                                           }
Meta-side info
+∑︁
𝑢∈𝑁𝜇𝑢
𝑇"
O √︄
𝑆+1
2𝜇𝑢
𝑇!
+√︄
2 log(1/𝛿)
𝜇𝑢
𝑇|                               {z                               }
User-side info#
,
where∇Θ𝑓(x𝑡;Θ𝑡)incorporates the discriminative information
of meta-learner acquired from the correlations within the relative
cluster bN𝑢𝑡(x𝑡)andO(1√𝜇𝑢
𝑇)shows the shrinking confidence in-
terval of user-learner to a specific user 𝑢. Then, we select an arm
according to: x𝑡=argx𝑡,𝑖∈X𝑡maxU𝑡,𝑖( where U𝑡,𝑖is calculated in
Line 9).
In summary, Algorithm 1 depicts the workflow of M-CNB . In
each round𝑡, given a target user and a pool of candidate arms, we
compute the meta-learner and its bound for each relative cluster
(Line 6-10). Then, we choose the arm according to the UCB-type
strategy (Line 11). After receiving the reward, we update the user-
learners. Note that the meta-learner has been updated in Line 8.
Then, we discuss the time complexity of Algorithm 1. Here, with
𝑛being the number of users, M-CNB will takeO(𝑛)to find the
cluster for the served user. Given the detected cluster bN, it takes
O(|bN|) to update the meta-learner by SGD. Suppose E[|bN|]=
𝑛/ˆ𝑞and𝑛/ˆ𝑞≪𝑛. Therefore, the overall test time complexity of
Algorithm 1 isO(𝐾(𝑛+𝑛/ˆ𝑞)). To scale M-CNB for deployment
in large recommender systems, we can rely on the assistance of
pre-processing tools: Pre-clustering of users and Pre-selection of
items. On the one hand, we can perform pre-clustering of users
based on the user features or other information. Then, let a pre-
cluster (instead of a single user) hold a neural network, which will
significantly reduce 𝑛. On the other hand, we can conduct the pre-
selection of items based on item and user features, to reduce 𝐾
substantially. For instance, we only consider the restaurants that
are near the serving user for the restaurant recommendation task.
Furthermore, we can also control the magnitude of 𝑛/ˆ𝑞by tuning
the hyperparameter 𝜈based on the actual application scenario.
Consequently, M-CNB can effectively serve as a core component
of large-scale recommender systems.
5 Regret Analysis
In this section, we provide the performance guarantee of M-CNB ,
which is built in the over-parameterized neural networks regime.
As the standard setting in contextual bandits, all arms are nor-
malized to the unit length. Given an arm x𝑡,𝑖∈R𝑑with∥x𝑡,𝑖∥2=1,
𝑡∈[𝑇],𝑖∈[𝐾], without loss of generality, we define 𝑓as a fully-
connected network with depth 𝐿≥2and width𝑚:
𝑓(x𝑡,𝑖;𝜃orΘ)=W𝐿𝜎(W𝐿−1𝜎(W𝐿−2...𝜎(W1x𝑡,𝑖))) (4)
where𝜎is the ReLU activation function, W1∈R𝑚×𝑑,W𝑙∈R𝑚×𝑚,
for2≤𝑙≤𝐿−1,W𝐿∈R1×𝑚, and
𝜃,Θ=[vec(W1)⊤,vec(W2)⊤,..., vec(W𝐿)⊤]⊤∈R𝑝.Note that our analysis results can also be readily generalized to other
neural architectures such as CNNs and ResNet [ 2,18]. Then, we
employ the following initialization [ 12] for𝜃andΘ: For𝑙∈[𝐿−1],
each entry of W𝑙is drawn from the normal distribution N(0,2/𝑚);
Each entry of W𝐿is drawn from the normal distribution N(0,1/𝑚).
Here, given 𝑅>0, we define the following function class:
𝐵(𝜃0,𝑅)={𝜃∈R𝑝:∥𝜃−𝜃0∥2≤𝑅/𝑚1/4}. (5)
The term𝐵(𝜃0,𝑅)defines a function class ball centered at the ran-
dom initialization point 𝜃0and with a radius of 𝑅. This defini-
tion was originally introduced in the context of analyzing over-
parameterized neural networks, and it can be found in the works
of [12] and [ 2]. Recall that 𝑞𝑡,𝑖represents the number of clusters
given x𝑡,𝑖. For the simplicity of analysis, we assume E[𝑞𝑡,𝑖]=𝑞,𝑡∈
[𝑇],𝑖∈[𝐾]. Let{(x𝑡,𝑟𝑡)}𝑇𝐾
𝑡=1represent all the data in 𝑇rounds
and define the squared loss L𝑡(𝜃)=(𝑓(x𝑡;𝜃)−𝑟𝑡)2/2. Then, we
provide the instance-dependent regret upper bound for M-CNB
with the following theorem.
Theorem 5.1. Given the number of rounds 𝑇and𝛾, for any𝛿∈
(0,1),𝑅>0, suppose𝑚≥eΩ(poly(𝑇,𝐿,𝑅)·𝐾𝑛log(1/𝛿)),𝜂1=𝜂2=
𝑅2√𝑚, andE[|N𝑢𝑡(x𝑡)|]=𝑛
𝑞,𝑡∈[𝑇]. Then, with probability at least
1−𝛿over the initialization, Algorithm 1 achieves the following regret
upper bound:
R𝑇≤√︃
𝑞𝑇·𝑆∗
𝑇𝐾+O( 1)+O(√︁
2𝑞𝑇log(O(1)/𝛿)).
where𝑆∗
𝑇𝐾= inf
𝜃∈𝐵(𝜃0,𝑅)Í𝑇𝐾
𝑡=1L𝑡(𝜃).
Theorem 5.1 provides a regret bound for M-CNB , which consists
of two main terms. The first term is instance-dependent and relates
to the squared error achieved by the function class 𝐵(𝜃0,𝑅)on the
data. The second term is a standard large-deviation error term.
There are some noteworthy properties regarding Theorem 5.1.
One important aspect is that it depends on the parameter 𝑞, which
represents the expected number of clusters, rather than the number
of users𝑛. Specifically, eO(√
𝑇)corresponds to the regret effort for
learning a single bandit, and thus eO(√
𝑛𝑇)is an estimate of the
regret effort for learning 𝑛bandits. However, Theorem 5.1 refines
this naive bound to eO(√︁
𝑞𝑇), linking the regret effort to the actual
underlying clusters among users.
Another advantage of Theorem 5.1 is that it makes no assump-
tions about the contexts {x𝑡}𝑇𝐾
𝑡=1used in the problem. This makes
Theorem 5.1 robust against adversarial attacks on the contexts and
allows the observed contexts to contain repeated items. In contrast,
existing neural bandit algorithms like [ 31,64,65] rely on Assump-
tion 5.1 for the contexts, and their regret upper bounds can be
disrupted by straightforward adversarial attacks, e.g., creating two
identical contexts with different rewards.
The term𝑆∗
𝑇𝐾reflects the "regression difficulty" of fitting all the
data using a given function class, while the radius 𝑅controls the
richness or complexity of that function class. It’s important to note
that the choice of 𝑅is flexible, although it’s not without constraints:
specifically, the value of 𝑚must be larger than a polynomial of
𝑅. When𝑅is set to a larger value, it expands the function class
𝐵(𝜃0,𝑅), which means it can potentially fit a wider range of data.
Consequently, this tends to make 𝑆∗
𝑇𝐾smaller. Recent advances in
 
99KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
the convergence of neural networks, as demonstrated by [ 2] and
[18], have shown that there is an optimal region around the initial-
ization point in over-parameterized neural networks. This suggests
that, with the proper choice of 𝑅, term𝑆∗
𝑇𝐾can be constrained to a
small constant value.
Next, we show the common assumption made on existing neural
bandits, and prove that Theorem 5.1 is no worse than their regret
bounds under the same assumption. The analysis is associated with
the Neural Tangent Kernel (NTK) matrix as follows:
Definition 5.2 (NTK [ 29,56]).LetNdenote the normal distribu-
tion. Given the data instances {x𝑡}𝑇
𝑡=1, for all𝑖,𝑗∈[𝑇], define
H0
𝑖,𝑗=Σ0
𝑖,𝑗=⟨x𝑖,x𝑗⟩,A𝑙
𝑖,𝑗= 
Σ𝑙
𝑖,𝑖Σ𝑙
𝑖,𝑗
Σ𝑙
𝑗,𝑖Σ𝑙
𝑗,𝑗!
Σ𝑙
𝑖,𝑗=2E𝑎,𝑏∼N( 0,A𝑙−1
𝑖,𝑗)[𝜎(𝑎)𝜎(𝑏)],
H𝑙
𝑖,𝑗=2H𝑙−1
𝑖,𝑗E𝑎,𝑏∼N( 0,A𝑙−1
𝑖,𝑗)[𝜎′(𝑎)𝜎′(𝑏)]+Σ𝑙
𝑖,𝑗.
Then, the NTK matrix is defined as H=(H𝐿+Σ𝐿)/2.
Assumption 5.1. There exists 𝜆0>0, such that H⪰𝜆0I
The assumption 5.1 is generally held in the literature of neural
bandits [ 5,6,15,30,59,64,65] to ensure the existence of a solution
for NTK regression. This assumption holds true when any two
contexts in{x𝑡}𝑇𝐾
𝑡=1are not linearly dependent or parallel. Then,
the SOTA regret upper bound for a single neural bandit ( 𝑛=1)
[5, 15, 64, 65] is as follows:
eO(√︁
e𝑑𝑇(𝑆+√︁
e𝑑)). (6)
There are two complexity terms in the regret bounds [ 6,65]. The
first complexity term is 𝑆=√
h⊤H−1h, where
h=[ℎ𝑢1(x1),ℎ𝑢1(x2),...,ℎ𝑢𝑇(x𝑇𝐾)]⊤∈R𝑇𝐾.
The purpose of the term 𝑆is to provide an upper bound on the
optimal parameters in the context of NTK regression. However,
it’s important to note that the value of 𝑆becomes unbounded (i.e.,
∞) when the matrix Hbecomes singular. This singularity can be
induced by an adversary who creates two identical or parallel con-
texts, causing problems in their analysis.
The second complexity term is the effective dimension ˜𝑑, defined
ase𝑑=log det(I+H)
log(1+𝑇𝐾), which describes the actual underlying dimen-
sion in the RKHS space spanned by NTK. The following lemma is
to show an upper bound of 𝑆∗
𝑇𝐾under the same assumption.
Lemma 5.3. Suppose Assumption 5.1 and conditions in Theorem 5.1
holds where 𝑚≥eΩ(poly(𝑇,𝐿)·𝐾𝑛𝜆−1
0log(1/𝛿)). With probability
at least 1−𝛿over the initialization, there exists 𝜃′∈𝐵(𝜃0,eΩ(𝑇3/2)),
such that
E[𝑆∗
𝑇𝐾]≤E[𝑇𝐾∑︁
𝑡=1L𝑡(𝜃′)]≤eO√︁
e𝑑+𝑆2
·e𝑑.
Lemma 5.3 provides an upper bound for 𝑆∗
𝑇𝐾by setting𝑅=
eΩ(𝑇3/2). Subsequently, by applying the Hoeffding-Azuma inequal-
ity over𝑆∗
𝑇𝐾and replacing 𝑆∗
𝑇𝐾with this upper bound, Theorem 5.1
can be reformulated as eO(√︁
e𝑑𝑇(𝑆+√︁
e𝑑))for a single neural banditoreO(√︃
𝑞e𝑑𝑇(𝑆+√︁
e𝑑))for𝑛users (CNB problem). This transforma-
tion implies that Theorem 5.1 is at least as good as the SOTA upper
bounds represented by Eq. (6).
6 Experiments
In this section, we evaluate M-CNB ’s empirical performance on
both online recommendation and classification scenarios. Our source
code are anonymously available at https://anonymous.4open.science/
r/Mn-C35C/ .
Recommendation datasets. We use four public datasets, Ama-
zon [ 45], Facebook [ 37], Movielens [ 25], and Yelp1, to evaluate
M-CNB ’s ability in discovering and exploiting user clusters to im-
prove the recommendation performance. Amazon is an E-commerce
recommendation dataset consisting of 883636 review ratings. Face-
book is a social recommendation dataset with 88234 links. Movie-
Lens is a movie recommendation dataset consisting of 25million
reviews between 1.6×105users and 6×104movies. Yelp is a shop
recommendation dataset released in the Yelp dataset challenge,
composed of 4.7 million review entries made by 1.18million users
towards 1.57×105merchants. For these four datasets, we extract
ratings in the reviews and build the rating matrix by selecting the
top10000 users and top 10000 items (friends, movies, shops) with
the most rating records. Then, we use the singular-value decompo-
sition (SVD) to extract a normalized 10-dimensional feature vector
for each user and item. The goal of this problem is to select the item
with good ratings. Given an item and a specific user, we generate
the reward by using the user’s rating stars for this item. If the user’s
rating is more than 4 stars (5 stars total), its reward is 1; Otherwise,
its reward is 0. Here, we use pre-clustering (K-means) to form the
user pool with 50 users (pre-clusters). Then, in each round, a user
𝑢𝑡is randomly drawn from the user pool. For the arm pool, we
randomly choose one restaurant (movie) rated by 𝑢𝑡with reward
1and randomly pick the other 9restaurants (movies) rated by 𝑢𝑡
with 0reward. With each restaurant or movie corresponding to an
arm, the goal for the learner is to pick the arm with the highest
reward.
Classification datasets. In our online classification with bandit
feedback experiments, we utilized a range of well-known classifi-
cation datasets, including Mnist [ 36], Notmnist [ 11], Cifar10 [ 34],
Emnist (Letter) [ 14], Fashion [ 58], as well as the Shuttle, Mush-
room, and MagicTelescope (MT) datasets [ 19]. Here, we provide
some preliminaries for this setup. In the round 𝑡∈ [𝑇], given
an instance x𝑡∈R𝑑drawn from some distribution, we aim to
classify x𝑡among𝐾classes. x𝑡is first transformed into 𝐾long
vectors: x𝑡,1=(x⊤,0,..., 0)⊤,x𝑡,2=(0,x⊤,..., 0)⊤,..., x𝑡,𝐾=
(0,0,..., x⊤)⊤∈R𝑑𝐾, matching𝐾classes respectively. The index
of the arm that the learner selects is the class predicted by the
learner. Then, the reward is defined as 1ifx𝑡belongs to this class;
otherwise, the reward is 0. In other words, each arm represents a
specific class. For example, x𝑡,1is only presented to Class 1; x𝑡,2
is only presented to Class 2. This problem has been studied in al-
most all the neural bandit works [ 6,31,64,65]. Compared to these
works, we aim to learn the correlations among classes to improve
performance. Thus, we formulate one class as a user (bandit) (i.e.,
1https://www.yelp.com/dataset
 
100Meta Clustering of Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 2: Regret comparison on recommendation datasets.
a user in the recommendation scenario) and all the samples be-
longing to this class are deemed as the data of this user. This set
of experiments aims to evaluate M-CNB ’s ability to learn various
non-linear reward functions, as well as the ability of discovering
and exploiting the correlations among classes. Additionally, we
extended the evaluation by combining the Mnist and Notmnist
datasets to simulate a more challenging application scenario, given
that both datasets involve 10-class classification problems.
Baselines. We compare M-CNB with SOTA baselines as follows:
(1) CLUB [ 23] clusters users based on the connected components
in the user graph and refines the groups incrementally; (2) COFIBA
[39] clusters on both the user and arm sides based on the evolving
graph, and chooses arms using a UCB-based exploration strategy;
(3) SCLUB [ 40] improves the algorithm CLUB by allowing groups to
merge and split, to enhance the group representation; (4) LOCB [ 4]
uses the seed-based clustering and allows groups to be overlapped.
Then, it chooses the best group candidates for arm selection; (5)
NeuUCB-ONE [ 65] uses one neural network to formulate all users,
and selects arms via a UCB-based recommendation; (6) NeuUCB-
IND [ 65] uses one neural network to formulate one user separately
(totally𝑛networks) and applies the same strategy to choose arms.
(7) NeuA+U: we concatenate the arm features and user features
together and treat them as the input for the neural network. Note
that the user features are only available on Movielens and Yelp
datasets. Thus, we only report the results on these two datasets for
NeuA+U. (8) NeuralLinear: following the existing work [ 44,63]. A
shared neural network is built for all users to get an embedding for
each arm. which is fed into the linear bandit with the clustering pro-
cedure. Since LinUCB [ 38] and KernalUCB [ 53] are outperformed
by the above baselines, we will not include them for comparison.
Configurations. We run all experiments on a server with the
NVIDIA Tesla V100 SXM2 GPU. For all the baselines, they all have
two parameters: 𝜆that is to tune the regularization at initialization
and𝛼which is to adjust the UCB value. To find their best perfor-
mance, we conduct the grid search for 𝜆and𝛼over(0.01,0.1,1)
and(0.0001,0.001,0.01,0.1)respectively. For LOCB, the number
of random seeds is set as 20following their default setting. For
M-CNB , we set𝜈as5and𝛾as0.4to tune the cluster, and 𝑆is set
to1. To ensure fair comparison, for all neural methods, we use the
same simple neural network with 2fully-connected layers, and the
Figure 3: Regret comparison on Mnist and Notmnist, Cifar10,
EMNIST(Letter), and Shuttle.
width𝑚is set as 100. To save the running time, we train the neural
networks every 10 rounds in the first 1000 rounds and train the neu-
ral networks every 100 rounds afterwards. In our implementation,
we use Adam [ 32] for SGD. In the end, we choose the best results
for the comparison and report the mean and standard deviation
(shadows in figures) of 10runs for all methods.
Results. Figure 2-4 reports the average regrets of all the methods
on the recommendation and classification datasets. Figure 2 displays
the regret curves for all the methods evaluated on the MovieLens
and Yelp datasets. In these experiments, M-CNB consistently out-
performs all the baseline methods, showcasing its effectiveness.
Specifically, M-CNB improves performance by 5.8% on Amazon,
7.7 % on Facebook, 8.1 % on MovieLens, and 2.0 % on Yelp, com-
pared to the best-performing baseline. These superior results can
be attributed to two specific advantages that M-CNB offers over
the two types of baseline methods. In contrast to conventional lin-
ear clustering of bandits (CLUB, COFIBA, SCLUB, LOCB), M-CNB
has the capability to learn non-linear reward functions. This flexi-
bility allows M-CNB to excel in scenarios where user preferences
exhibit non-linearity in terms of arm contexts. In comparison to neu-
ral bandits (NeuUCB-ONE, NeuUCB-IND, NeuA+U, NeuralLinear),
M-CNB takes advantage of user clustering and leverages the corre-
lations within these clusters, as captured by the meta-learner. This
exploitation of inter-user correlations enables M-CNB to enhance
recommendation performance. By combining these advantages,
M-CNB achieves substantial improvements over the MovieLens
and Yelp datasets, demonstrating its prowess in addressing collab-
orative neural bandit problems and enhancing recommendation
systems. Note M-CNB ’s regret rate decreases on these four datasets,
even though the "linear-like" behavior in Figure 2.
Figures 3 and 4 show the regret comparison on ML datasets,
where M-CNB outperforms all the baselines. Here, each class can be
thought of as a user in these datasets. The ML datasets exhibit non-
linear reward functions concerning the arms, making them challeng-
ing for conventional clustering of linear bandits (CLUB, COFIBA,
SCLUB, LOCB). These methods may struggle to capture the non-
linearity of the reward functions, resulting in sub-optimal perfor-
mance. Among the neural baselines, NeuUCB-ONE benefits from
the representation power of neural networks. However, it treats all
users (classes) as a single cluster, overlooking the variations and
 
101KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
Figure 4: Regret comparison on Mnist, Fashion-Mnist, Mush-
room, and MagicTelescope.
correlations among them. On the other hand, NeuUCB-IND deals
with users individually, neglecting the potential benefits of leverag-
ing collaborative knowledge among users. NeuralLinear uses one
shared embedding (neural network) for all users, which may not be
the optimal solution given the user heterogeneity. M-CNB ’s advan-
tage lies in its ability to exploit shared knowledge within clusters
of classes that exhibit strong correlations. It leverages this common
knowledge to improve its performances across different tasks, as it
can efficiently adapt its meta-learner based on past clusters. More
discussions with baselines are placed in Appendix B.1.
1000 2000 3000 4000 5000
Running Time(s)45005000550060006500700075008000RegretMovieLens
'CLUB'
'COFIBA'
'SLUCB'
'LOCB'
'NeuUCB-IND'
'NeuUCB-ONE'
'NeuA+U'
'NeuralLinear'
'M-CNB'
2000 4000 6000 8000 10000
Running Time(s)300040005000600070008000RegretMnist
'CLUB'
'COFIBA'
'SLUCB'
'LOCB'
'NeuUCB-IND'
'NeuUCB-ONE'
'NeuralLinear'
'M-CNB'
Figure 5: Running time vs. Performance for all methods.
Running time analysis. Figure 5 demonstrates the trade-off
between running time and cumulative regret on both the Movie-
lens and Mnist datasets, where the unit of the x-axis is seconds. As
M-CNB is under the framework of neural bandits, we use NeuUCB-
ONE as the baseline (1.0). The results indicate that M-CNB takes
comparable computation costs (1 .6×on Movielens and 2.9×on
Mnist) to NeuUCB-ONE while substantially improving performance.
This suggests that M-CNB can be deployed to significantly enhance
performance when the user correlation is a crucial factor (e.g.,
recommendation tasks), with only a moderate increase in computa-
tional overhead.
Now, let us delve into the analysis of the running time for M-
CNB. Specifically, we can break down the computational cost of
M-CNB into three main components: (1) Clustering : to form the
user cluster (Line 7 in Algorithm 1); (2) Meta adaptation : to train
a meta-model (Algorithm 2); (3) User-learner training : to train the
user-learners (Lines 14-18 in Algorithm 1).
Table 1 provides the breakdown of the time cost for the three
main components of M-CNB . Clustering: This part’s time cost
grows linearly with the number of users 𝑛because it has a timeTable 1: Breakdown time cost for M-CNB in a round (seconds)
with different number of users on MovieLens.
n
=500 n
= 5000 n
= 10000 n
= 20000
Clustering 0.006 0.057 0.113 0.228
Meta
adaptation 0.003 0.002 0.003 0.003
User-learner
training 0.067 0.068 0.096 0.078
complexity of 𝑂(𝑛)for clustering. As discussed previously, lever-
aging pre-clustering techniques can significantly reduce this cost.
It is also important to note that all clustering methods inherently
have this time cost, and it is challenging to further reduce it. Meta
adaptation: Due to the benefits of meta-learning, this part requires
only a few steps of gradient descent to train a model with good
performances. Consequently, the time cost for meta-adaptation is
relatively trivial. User-learner training: While this part may require
more SGD steps to converge, it is important to recognize that it is
primarily used for clustering purposes. Therefore, the frequency of
training user-learners can be reduced to decrease the cost. In sum-
mary, M-CNB aims to achieve the clustering of neural bandits and
can manage to strike a good balance between the computational
cost and the model performance.
0 2000 4000 6000 8000 10000
Rounds010002000300040005000RegretAblation Study on MovieLens
 = 1.1
 = 1.5
 = 3
 = 5
 = 10
0 2000 4000 6000 8000 10000
Rounds010002000300040005000RegretAblation Study on MovieLens
 = 0.1
 = 0.2
 = 0.4
 = 0.8
Figure 6: Sensitivity study for 𝜈and𝛾on MovieLens Dataset.
Study for𝜈and𝛾. Figure 6 illustrates the performance variation
ofM-CNB concerning the parameters 𝜈and𝛾. For the sake of
discussion, we will focus on 𝜈but note that 𝛾plays a similar role in
terms of controlling clustering. When 𝜈is set to a value like 1.1, the
exploration range of clusters becomes very narrow. In this case, the
inferred cluster size in each round, |bN𝑢𝑡(x𝑡,𝑖)|, tends to be small.
This means that the inferred cluster bN𝑢𝑡(x𝑡,𝑖)is more likely to
consist of true members of 𝑢𝑡’s relative cluster. However, there is a
drawback regarding this narrow exploration range: it might result
in missing out on potential cluster members in the initial phases of
learning. On the other hand, setting 𝜈to a larger value, like 𝜈=5,
widens the exploration range of clusters. This means that there
are more opportunities to include a larger number of members in
the inferred cluster. However, continuously increasing 𝜈does not
necessarily lead to improved performances, because excessively
large values of 𝜈might result in inferred clusters that include non-
collaborative users and clustering noise. Therefore, in practice, we
recommend to set 𝜈to a relatively large number (e.g., 𝜈=5) that
strikes a balance between the exploration and exploitation.
Study for𝑆. Figure 7 provides insight into the sensitivity of
M-CNB concerning the parameter 𝑆in Algorithm 1. It is evident
that M-CNB exhibits robust performance across a range of values
for𝑆. This robustness can be attributed to the strong discriminabil-
ity of the meta-learner and the derived upper bound. Even with
varying𝑆values, the relative order of arms ranked by M-CNB ex-
periences only slightly changes. This consistency in arm rankings
 
102Meta Clustering of Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
0 2000 4000 6000 8000 10000
Rounds0500100015002000250030003500RegretAblation Study for S
S = 0.1
S = 1
S = 10
S = 100
Figure 7: Sensitivity study for 𝑆on Mnist Dataset.
demonstrates that M-CNB is capable of maintaining the robust
performance, which in turn reduces the need for extensive hyper-
parameter tuning.
7 Conclusion
In this paper, we study the Cluster of Neural Bandits problem to
incorporate correlation in bandits with generic reward assumptions.
Then, we propose a novel algorithm, M-CNB , to solve this problem,
where a meta-learner is assigned to represent and rapidly adapt to
dynamic clusters, along with an informative UCB-type exploration
strategy. Moreover, we provide the instance-dependent regret anal-
ysis for M-CNB . In the end, to demonstrate the effectiveness of
M-CNB , we conduct extensive experiments to evaluate its empiri-
cal performance against strong baselines on recommendation and
classification datasets.
Acknowledgement
This work is supported by National Science Foundation under
Award No. IIS-2002540, and Agriculture and Food Research Initia-
tive (AFRI) grant no. 2020-67021-32799/project accession no.1024178
from the USDA National Institute of Food and Agriculture. The
views and conclusions are those of the authors and should not
be interpreted as representing the official policies of the funding
agencies or the government.
References
[1]Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems, pages
2312–2320, 2011.
[2]Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning, pages
242–252. PMLR, 2019.
[3]Y. Ban and J. He. Generic outlier detection in multi-armed bandit. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 913–923, 2020.
[4]Y. Ban and J. He. Local clustering in contextual multi-armed bandits. In Proceed-
ings of the Web Conference 2021, pages 2335–2346, 2021.
[5]Y. Ban, J. He, and C. B. Cook. Multi-facet contextual bandits: A neural network
perspective. In The 27th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 35–45, 2021.
[6]Y. Ban, Y. Yan, A. Banerjee, and J. He. Ee-net: Exploitation-exploration neural
networks in contextual bandits. arXiv preprint arXiv:2110.03177, 2021.
[7]Y. Ban, Y. Zhang, H. Tong, A. Banerjee, and J. He. Improved algorithms for
neural active learning. Advances in Neural Information Processing Systems, 35:
27497–27509, 2022.
[8]Y. Ban, Y. Qi, and J. He. Neural contextual bandits for personalized recommenda-
tion. arXiv preprint arXiv:2312.14037, 2023.
[9]Y. Ban, Y. Yan, A. Banerjee, and J. He. Neural exploitation and exploration of
contextual bandits. arXiv preprint arXiv:2305.03784, 2023.
[10] Y. Ban, I. Agarwal, Z. Wu, Y. Zhu, K. Weldemariam, H. Tong, and J. He. Neural
active learning beyond bandits. arXiv preprint arXiv:2404.12522, 2024.
[11] Y. Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:
http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2, 2011.[12] Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide
and deep neural networks. Advances in Neural Information Processing Systems,
32:10836–10846, 2019.
[13] M. Chen, C. Xu, V. Gatto, D. Jain, A. Kumar, and E. Chi. Off-policy actor-critic
for recommender systems. In Proceedings of the 16th ACM Conference on Recom-
mender Systems, pages 338–349, 2022.
[14] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 international joint conference on neural networks
(IJCNN), pages 2921–2926. IEEE, 2017.
[15] Z. Dai, Y. Shu, A. Verma, F. X. Fan, B. K. H. Low, and P. Jaillet. Federated neural
bandit. arXiv preprint arXiv:2205.14309, 2022.
[16] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under
bandit feedback. 2008.
[17] R. Deb, Y. Ban, S. Zuo, J. He, and A. Banerjee. Contextual bandits with online
neural regression. arXiv preprint arXiv:2312.07145, 2023.
[18] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima
of deep neural networks. In International Conference on Machine Learning, pages
1675–1685. PMLR, 2019.
[19] D. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
[20] P. Dutta, M. Kit, J. S. Kim, M. Mascaro, et al. Automl for contextual bandits. arXiv
preprint arXiv:1909.03212, 2019.
[21] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pages 1126–
1135. PMLR, 2017.
[22] C. Gao, K. Huang, J. Chen, Y. Zhang, B. Li, P. Jiang, S. Wang, Z. Zhang, and
X. He. Alleviating matthew effect of offline reinforcement learning in interactive
recommendation. arXiv preprint arXiv:2307.04571, 2023.
[23] C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In International
Conference on Machine Learning, pages 757–765, 2014.
[24] C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, and E. Etrue. On context-
dependent clustering of bandits. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 1253–1262. JMLR. org, 2017.
[25] F. M. Harper and J. A. Konstan. The movielens datasets: History and context.
Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015.
[26] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. Neural collaborative
filtering. In Proceedings of the 26th international conference on world wide web,
pages 173–182, 2017.
[27] J. Hong, B. Kveton, M. Zaheer, Y. Chow, A. Ahmed, and C. Boutilier. Latent bandits
revisited. Advances in Neural Information Processing Systems, 33:13423–13433,
2020.
[28] J. Hong, B. Kveton, M. Zaheer, and M. Ghavamzadeh. Hierarchical bayesian
bandits. In International Conference on Artificial Intelligence and Statistics , pages
7724–7741. PMLR, 2022.
[29] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing
systems, pages 8571–8580, 2018.
[30] Y. Jia, W. Zhang, D. Zhou, Q. Gu, and H. Wang. Learning neural contextual
bandits through perturbed rewards. In International Conference on Learning
Representations, 2022.
[31] P. Kassraie and A. Krause. Neural contextual bandits without regret. In Interna-
tional Conference on Artificial Intelligence and Statistics, pages 240–278. PMLR,
2022.
[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[33] N. Korda, B. Szörényi, and L. Shuai. Distributed clustering of linear bandits
in peer to peer networks. In Journal of machine learning research workshop
and conference proceedings, volume 48, pages 1301–1309. International Machine
Learning Societ, 2016.
[34] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
[35] B. Kveton, M. Konobeev, M. Zaheer, C.-w. Hsu, M. Mladenov, C. Boutilier, and
C. Szepesvari. Meta-thompson sampling. In International Conference on Machine
Learning, pages 5884–5893. PMLR, 2021.
[36] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[37] J. Leskovec and J. Mcauley. Learning to discover social circles in ego networks.
Advances in neural information processing systems, 25, 2012.
[38] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international
conference on World wide web, pages 661–670, 2010.
[39] S. Li, A. Karatzoglou, and C. Gentile. Collaborative filtering bandits. In Proceedings
of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval, pages 539–548, 2016.
[40] S. Li, W. Chen, S. Li, and K.-S. Leung. Improved algorithm on online clustering
of bandits. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence, pages 2923–2929. AAAI Press, 2019.
 
103KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
[41] Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng. Bbq-networks: Efficient
exploration in deep reinforcement learning for task-oriented dialogue systems.
InProceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[42] O.-A. Maillard and S. Mannor. Latent bandits. In International Conference on
Machine Learning, pages 136–144. PMLR, 2014.
[43] T. M. McDonald, L. Maystre, M. Lalmas, D. Russo, and K. Ciosek. Impatient bandits:
Optimizing recommendations for the long-term without delay. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
pages 1687–1697, 2023.
[44] O. Nabati, T. Zahavy, and S. Mannor. Online limited memory neural-linear bandits
with likelihood matching. In International Conference on Machine Learning, pages
7905–7915. PMLR, 2021.
[45] J. Ni, J. Li, and J. McAuley. Justifying recommendations using distantly-labeled
reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical
methods in natural language processing and the 9th international joint conference
on natural language processing (EMNLP-IJCNLP), pages 188–197, 2019.
[46] Y. Qi, Y. Ban, and J. He. Neural bandit with arm group graph. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
pages 1379–1389, 2022.
[47] Y. Qi, Y. Ban, and J. He. Graph neural bandits. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1920–1931,
2023.
[48] Y. Qi, Y. Ban, T. Wei, J. Zou, H. Yao, and J. He. Meta-learning with neural bandit
scheduler. Advances in Neural Information Processing Systems, 36, 2023.
[49] C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: An
empirical comparison of bayesian deep networks for thompson sampling. arXiv
preprint arXiv:1802.09127, 2018.
[50] M. R. Santana, L. C. Melo, F. H. Camargo, B. Brandão, A. Soares, R. M. Oliveira,
and S. Caetano. Contextual meta-bandit for recommender systems selection. In
Fourteenth ACM Conference on Recommender Systems, pages 444–449, 2020.
[51] M. Simchowitz, C. Tosh, A. Krishnamurthy, D. J. Hsu, T. Lykouris, M. Dudik,
and R. E. Schapire. Bayesian decision-making under misspecified priors with
applications to meta-learning. Advances in Neural Information Processing Systems,
34:26382–26394, 2021.
[52] X. Su and T. M. Khoshgoftaar. A survey of collaborative filtering techniques.
Advances in artificial intelligence, 2009, 2009.
[53] M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis
of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
[54] R. Wan, L. Ge, and R. Song. Metadata-based multi-task bandits with bayesian
hierarchical models. Advances in Neural Information Processing Systems, 34:
29655–29668, 2021.
[55] L. Wang, Q. Cai, Z. Yang, and Z. Wang. On the global optimality of model-agnostic
meta-learning. In International Conference on Machine Learning, pages 9837–9846.
PMLR, 2020.
[56] Z. Wang, P. Awasthi, C. Dann, A. Sekhari, and C. Gentile. Neural active learning
with performance guarantees. Advances in Neural Information Processing Systems,
34:7510–7521, 2021.
[57] J. Wu, C. Zhao, T. Yu, J. Li, and S. Li. Clustering of conversational bandits for user
preference learning and elicitation. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management, pages 2129–2139, 2021.
[58] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747,
2017.
[59] P. Xu, Z. Wen, H. Zhao, and Q. Gu. Neural contextual bandits with deep repre-
sentation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020.
[60] W. Xue, Q. Cai, R. Zhan, D. Zheng, P. Jiang, and B. An. Resact: Reinforcing
long-term engagement in sequential recommendation with residual actor. arXiv
preprint arXiv:2206.02620, 2022.
[61] L. Yang, B. Liu, L. Lin, F. Xia, K. Chen, and Q. Yang. Exploring clustering of
bandits for online recommendation system. In Fourteenth ACM Conference on
Recommender Systems, pages 120–129, 2020.
[62] H. Yao, Y. Wei, J. Huang, and Z. Li. Hierarchically structured meta-learning. In
International Conference on Machine Learning, pages 7045–7054. PMLR, 2019.
[63] T. Zahavy and S. Mannor. Deep neural linear bandits: Overcoming catastrophic
forgetting through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
[64] W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International
Conference on Learning Representations, 2021.
[65] D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration.
InInternational Conference on Machine Learning, pages 11492–11502. PMLR, 2020.
A Proof Details of Theorem 5.1
Our proof technique is different from related works. [ 4,23,24,39,
40] are built on the classic linear bandit framework and [ 31,64,65]
utilize the kernel-based analysis in the NTK regime. In contrast,
we use the generalization bound of user-learner to bound the errorincurred in each round and bridge meta-learner with user-learner
by bounding their distance, which leads to our final regret bound.
Specifically, we decompose the regret of 𝑇rounds into three key
terms, where the first term is the error induced by user learner
𝜃𝑢, the second term is the distance between user learner and meta
learner, and the third term is the error induced by the meta learner
Θ. Then, Lemma A.10 provides an upper bound for the first term.
Lemma A.10 is an extension of Lemma A.7, which is the key to
removing the input dimension. Lemma A.7 has two terms with
the complexityO(√
𝑇), where the first term is the training error
induced by a class of functions around initialization, the second term
is the deviation induced by concentration inequality for 𝑓(·;𝜃𝑢).
Lemma A.11 bounds the distance between user-learner and meta-
learner. Lemma A.12 bounds the error induced by the meta learner
using triangle inequality bridged by the user learner. Bounding
these three terms completes the proof.
A.1 Analysis for user-learner
Following [ 2,12], given an instance x∈E𝑑with∥x∥2=1, we
define the outputs of hidden layers of the neural network (Eq. (4)):
h0=x,h𝑙=𝜎(W𝑙h𝑙−1),𝑙∈[𝐿−1].
Then, we define the binary diagonal matrix functioning as ReLU:
D𝑙=diag( 1{(W𝑙h𝑙−1)1},..., 1{(W𝑙h𝑙−1)𝑚}),𝑙∈[𝐿−1].
Accordingly, given an input x, the neural network (Eq. (4)) is repre-
sented by
𝑓(x𝑡;𝜃orΘ)=W𝐿(𝐿−1Ö
𝑙=1D𝑙W𝑙)x,
and
∇W𝑙𝑓=(
[h𝑙−1W𝐿(Î𝐿−1
𝜏=𝑙+1D𝜏W𝜏)]⊤,𝑙∈[𝐿−1]
h⊤
𝐿−1,𝑙=𝐿.
Given a reward 𝑟∈[0,1], defineL(𝜃)=(𝑓(x;𝜃)−𝑟)2/2. Then,
we have the following auxiliary lemmas.
Lemma A.1. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem
5.1. With probability at least 1−O(𝑇𝐾𝐿)·exp(−Ω(𝑚𝜔2/3𝐿))over
the random initialization, for all 𝑡∈[𝑇],𝑖∈[𝑘],𝜃(orΘ) satisfying
∥𝜃−𝜃0∥2≤𝜔with𝜔≤O(𝐿−9/2[log𝑚]−3), it holds uniformly that
(1),|𝑓(x;𝜃)|≤O( 1).
(2),∥∇𝜃𝑓(x;𝜃)∥2≤O(√
𝐿).
(3),∥∇𝜃L(𝜃)∥2≤O(√
𝐿)
Lemma A.2. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem 5.1.
With probability at least 1−O(𝑇𝐾𝐿)·exp(−Ω(𝑚𝜔2/3𝐿)), for all
𝑡∈[𝑇],𝑖∈[𝑘],𝜃,𝜃′(orΘ,Θ′) satisfying∥𝜃−𝜃0∥2,∥𝜃′−𝜃0∥2≤𝜔
with𝜔≤O(𝐿−9/2[log𝑚]−3), it holds uniformly that
|𝑓(x;𝜃)−𝑓(x;𝜃′)−⟨▽𝜃′𝑓(x;𝜃′),𝜃−𝜃′⟩|≤O(𝜔1/3𝐿2√︁
log𝑚)∥𝜃−𝜃′∥2.
Lemma A.3. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem
5.1. With probability at least 1−O(𝑇𝐾𝐿)·exp(−Ω(𝑚𝜔2/3𝐿)), for
all𝑡∈[𝑇],𝑖∈[𝑘],𝜃,𝜃′satisfying∥𝜃−𝜃0∥2,∥𝜃′−𝜃0∥2≤𝜔with
𝜔≤O(𝐿−9/2[log𝑚]−3), it holds uniformly that
(1) |𝑓(x;𝜃)−𝑓(x;𝜃′)|≤O(𝜔√
𝐿)+O(𝜔4/3𝐿2√︁
log𝑚)
(7)
 
104Meta Clustering of Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
Lemma A.4 (Almost Convexity). LetL𝑡(𝜃)=(𝑓(x𝑡;𝜃)−𝑟𝑡)2/2.
Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem 5.1. For any 𝜖>
0, with probability at least 1−O(𝑇𝐾𝐿2)exp[−Ω(𝑚𝜔2/3𝐿)]over
randomness of 𝜃1, for all𝑡∈[𝑇], and𝜃,𝜃′satisfying∥𝜃−𝜃0∥2≤𝜔
and∥𝜃′−𝜃0∥2≤𝜔with𝜔≤ O(𝐿−6[log𝑚]−3/2𝜖3/4), it holds
uniformly that
L𝑡(𝜃′)≥L𝑡(𝜃)+⟨∇𝜃L𝑡(𝜃),𝜃′−𝜃⟩−𝜖.
Lemma A.5 (User Trajectory Ball). Suppose𝑚,𝜂 1,𝜂2satisfy the
conditions in Theorem 5.1. With probability at least 1−O(𝑇𝐾𝐿2)
exp[−Ω(𝑚𝜔2/3𝐿)]over randomness of 𝜃0, for any𝑅>0, it holds
uniformly that
∥𝜃𝑡−𝜃0∥2≤O(𝑅/𝑚1/4)≤O(𝐿−6[log𝑚]−3/2𝑇−3/4),𝑡∈[𝑇].
Lemma A.6 (Instance-dependent Loss Bound). LetL𝑡(𝜃)=(𝑓(x𝑡;𝜃)−
𝑟𝑡)2/2. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem 5.1. With
probability at least 1−O(𝑇𝐾𝐿2)exp[−Ω(𝑚𝜔2/3𝐿)]over randomness
of𝜃0, given any𝑅>0it holds that
𝑇∑︁
𝑡=1L𝑡(𝜃𝑡)≤𝑇∑︁
𝑡=1L𝑡(𝜃∗)+O( 1)+𝑇𝐿𝑅2
√𝑚. (8)
where𝜃∗=arg inf𝜃′∈𝐵(𝜃0,𝑅)Í𝑇
𝑡=1L𝑡(𝜃′).
Lemma A.7. For any𝛿∈(0,1),𝑅>0, and𝑚,𝜂 1,𝜂2satisfy the
conditions in Theorem 5.1. In a round 𝜏where𝑢∈𝑁is serving
user, let𝑥𝜏be the selected arm and 𝑟𝜏is the corresponding received
reward. Then, with probability at least 1−𝛿over the randomness of
initialization, the cumulative regret induced by 𝑢up to round 𝑇is
upper bounded by:
1
𝜇𝑢
𝑇∑︁
(x𝜏,𝑟𝜏)∈T𝑢
𝑇E
𝑟𝜏|x𝜏[|𝑓(x𝜏;𝜃𝑢
𝜏−1)−𝑟𝜏|]
≤√︄
𝑆∗
𝑇(𝑢)+O( 1)
𝜇𝑢
𝑇+O √︄
2 log(O(1)/𝛿)
𝜇𝑢
𝑇!
.
where𝑆∗
𝑇(𝑢)= inf
𝜃∈𝐵(𝜃0,𝑅)Í
(x𝜏,𝑟𝜏)∈T𝑢
𝑇𝐿𝜏(𝜃).
Lemma A.8. For any𝛿∈(0,1),𝑅>0, and𝑚,𝜂 1,𝜂2satisfy the
conditions in Theorem 5.1. Suppose bN𝑢𝑡(x𝑡)=N𝑢𝑡(x𝑡),∀𝑡∈[𝑇].
After𝑇rounds, with probability 1−𝛿over the random initialization,
the cumulative error induced by the bandit-learners is upper bounded
by
𝑇∑︁
𝑡=11
|N𝑢𝑡(x𝑡)|∑︁
𝑢𝑡,𝑖∈N𝑢𝑡(x𝑡)E
𝑟𝑡|x𝑡[|𝑓(x𝑡;𝜃𝑢𝑡,𝑖
𝑡−1)−𝑟𝑡|]
≤√︃
𝑞𝑇·𝑆∗
𝑇𝑘log(O(𝛿−1))+O( 1)+O(√︃
2𝑞𝑇log(O(𝛿−1))),
where𝑆∗
𝑇𝑘= inf
𝜃∈𝐵(𝜃0,𝑅)Í𝑇𝑘
𝑡=1L𝑡(𝜃).
Corollary A.9. For any𝛿∈(0,1),𝑅>0, and𝑚,𝜂 1,𝜂2satisfy the
conditions in Theorem 5.1. In a round 𝜏where𝑢∈𝑁is the serving
user, let𝑥∗𝜏be the arm selected according to Bayes-optimal policy 𝜋∗:
𝑥∗
𝜏=arg max
x𝜏,𝑖,𝑖∈[𝑘]ℎ𝑢(x𝜏,𝑖),
and𝑟∗𝜏is the corresponding reward. Then, with probability at least
1−𝛿over the randomness of initialization, after 𝑡∈[𝑇]rounds, thecumulative regret induced by 𝑢with policy𝜋∗is upper bounded by:
1
𝜇𝑢
𝑡∑︁
(x∗𝜏,𝑟∗𝜏)∈T𝑢,∗
𝑡E
𝑟∗𝜏|x∗𝜏[|𝑓(x∗
𝜏;𝜃𝑢,∗
𝜏−1)−𝑟∗
𝜏||𝜋∗]
≤√︄
𝑆∗
𝑇(𝑢)+O( 1)
𝜇𝑢
𝑡+O √︄
2 log(O(1)/𝛿)
𝜇𝑢
𝑡!
.
where𝑆∗
𝑇(𝑢)= inf
𝜃∈𝐵(𝜃0,𝑅)Í
(x∗𝜏,𝑟∗𝜏)∈T𝑢,∗
𝑡𝐿𝜏(𝜃), andT𝑢,∗
𝑡are stored
Bayes-optimal pairs up to round 𝑡for𝑢, and𝜃𝑢,∗
𝜏−1are the parameters
trained onT𝑢,∗
𝜏−1according to SGD_User in round 𝜏−1.
Corollary A.10. For any𝛿∈(0,1),𝑅>0, and𝑚,𝜂 1,𝜂2satisfy the
conditions in Theorem 5.1. In round 𝑡∈[𝑇], given𝑢∈𝑁, let
𝑥∗
𝑡=arg max
x𝑡,𝑖,𝑖∈[𝑘]ℎ𝑢(x𝑡,𝑖)
the Bayes-optimal arm for 𝑢and𝑟∗
𝑡is the corresponding reward. Then,
with probability at least 1−𝛿over the random initialization, after
𝑇rounds, with probability 1−𝛿over the random initialization, the
cumulative error induced by the bandit-learners is upper bounded by:
𝑇∑︁
𝑡=1E
𝑟∗
𝑡|x∗
𝑡[|𝑓(x∗
𝑡;𝜃𝑢𝑡,∗
𝑡−1)−𝑟∗
𝑡|]
≤√︃
𝑞𝑇·𝑆∗
𝑇𝑘+O( 1)+O(√︁
2𝑞𝑇log(O(1)/𝛿)).
where the expectation is taken over 𝑟∗𝜏conditioned on x∗𝜏and𝜃𝑢𝑡,∗
𝑡−1
are the parameters trained on T𝑢𝑡,∗
𝑡−1according to SGD in round 𝑡−1.
A.2 Bridge Meta-learner and User-learner
For brevity, we use 𝑔(x𝑡;𝜃𝑢
𝑡−1)to represent the gradient ∇𝜃𝑓(x𝑡;𝜃𝑢
𝑡−1).
Lemma A.11. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem
5.1. With probability at least 1−O(𝑛𝑇𝐾𝐿2)exp[−Ω(𝑚𝜔2/3𝐿)]over
randomness of Θ0, for all𝑡∈[𝑇], any𝑢∈𝑁andΘ𝑡returned by
Algorithm 2, for any ∥x𝑡∥2=1, it holds uniformly for Algorithms
1-2 that
|𝑓(x𝑡;𝜃𝑢
𝑡−1)−𝑓(x𝑡;Θ𝑡)|≤𝑅∥∇Θ𝑓(x𝑡;Θ𝑡)−∇𝜃𝑓(x𝑡;𝜃𝑢
0)∥2
𝑚1/4+𝑍,
(9)
where
𝑍=O(𝑅𝐿2√︁
log𝑚)
𝑚1/3+O(𝐿7/3𝑅2√︁
log𝑚)
𝑚1/2+O(2𝑅√
𝐿)
𝑚1/4.
Lemma A.12. Suppose𝑚,𝜂 1,𝜂2satisfy the conditions in Theorem
5.1. Then, with probability at least 1−𝛿over the random initialization,
for any𝛿∈(0,1),𝑅>0, after𝑡rounds, the error induced by meta-
learner is upper bounded by:
𝑇∑︁
𝑡=1E
𝑟𝑡|x𝑡[|𝑓(x𝑡;Θ𝑡)−𝑟𝑡||𝑢𝑡]
≤𝑇∑︁
𝑡=1𝑅∥𝑔(x𝑡;Θ𝑡)−𝑔(x𝑡;𝜃𝑢𝑡
0)∥2
𝑚1/4+∑︁
𝑢∈𝑁𝜇𝑢
𝑇"
O©­­
«√︃
𝑆∗
𝑇𝑘+O( 1)
√︃
2𝜇𝑢
𝑇ª®®
¬
+√︄
2 log(O(1)/𝛿)
𝜇𝑢
𝑇#
.
(10)
where the expectation is taken over 𝑟𝑡conditioned on 𝑥𝑡.
 
105KDD ’24, August 25–29, 2024, Barcelona, Spain Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He
NeuUCB-ONE NeuUCB-IND M-CNB
2 layers 6964.1 6911.7 6773.4
4 layers 6944.7 6942.5 6803.2
8 layers 7012.8 6932.2 6878.3
10 layers 6992.4 6987.4 6854.9
Table 4: The cumulative regret of 10000 rounds on Yelp with
the different number of layers.
B Connections with Neural Tangent Kernel
Lemma B.1 (Lemma 5.3 Restated). Suppose𝑚satisfies the condi-
tions in Theorem 5.1. With probability at least 1−𝛿over the initial-
ization, there exists 𝜃′∈𝐵(𝜃0,eΩ(𝑇3/2)), such that
E[𝑆∗
𝑇𝑘]≤𝑇𝐾∑︁
𝑡=1E[(𝑟𝑡−𝑓(x𝑡;𝜃′))2/2]
≤O√︃
e𝑑log(1+𝑇𝐾)−2 log𝛿+𝑆+12
·e𝑑log(1+𝑇𝐾).
Lemma B.2. Suppose𝑚satisfies the conditions in Theorem 5.1.
With probability at least 1−𝛿over the initialization, there exists
𝜃′∈𝐵(𝜃0,eΩ(𝑇3/2))for all𝑡∈[𝑇], such that
|ℎ(x𝑡)−𝑓(x𝑡;𝜃′)|
≤O©­
«√︄
logdet(A𝑡)
det(I)
−2 log𝛿+𝑆+1ª®
¬∥𝑔(x𝑡;𝜃0)∥A−1
𝑡
+O 
𝑇2𝐿3√︁
log𝑚
𝑚1/3!
B.1 Additional Results
Convergence rate on MovieLens and Yelp. Table 2 illustrates the aver-
age cumulative regrets of M-CNB for different time step intervals
of 2000 rounds on the MovieLens and Yelp datasets. It’s noticeable
that the average regret per round of M-CNB decreases as more
time steps are considered. One plausible explanation for the "linear-
like" curves in the cumulative regrets is that both the MovieLensand Yelp datasets used for recommendation tasks contain inher-
ent noise. This noise can make it challenging for algorithms to
accurately learn the underlying reward mapping function. Con-
sequently, achieving substantial experimental improvements on
these datasets can be quite challenging. In essence, the presence
of noise in the data can lead to fluctuations in the regret curves,
making it appear as if progress is linear rather than exponential
or logarithmic. Despite these challenges, the algorithm, M-CNB ,
continues to make progress in minimizing regret over time.
Rounds 2000 4000 6000 8000 10000
MovieLens 0.4717 0.4555 0.4475 0.4452 0.4442
Yelp 0.7532 0.7395 0.7358 0.7306 0.7269
Table 2: Convergence rate of M-CNB on MovieLens and Yelp
NeuUCB-ONE NeuUCB-IND M-CNB
2 layers 4839.6 7491.0 4447.1
4 layers 5017.2 7503.8 4498.3
8 layers 5033.5 7764.5 4696.1
10 layers 4808.3 7697.4 4624.7
Table 3: The cumulative regret of 10000 rounds on MovieLens
with the different number of layers.
Ablation Study for Network Layers. We run the experiments on
MovieLens and Yelp datasets with the different number of layers
of neural networks and report the results in Table 3 and 4. M-CNB
achieves the best performance in most cases. In this paper, we try to
propose a generic framework to combine meta-learning and bandits
with the neural network approximation. Since the UCB in M-CNB
only depends on the gradient, the neural network can be easily
replaced by other different structures.
 
106